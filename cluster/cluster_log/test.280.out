Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=280, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15680-15735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818977
Iteration 2/25 | Loss: 0.00107923
Iteration 3/25 | Loss: 0.00075804
Iteration 4/25 | Loss: 0.00068964
Iteration 5/25 | Loss: 0.00067668
Iteration 6/25 | Loss: 0.00067463
Iteration 7/25 | Loss: 0.00067448
Iteration 8/25 | Loss: 0.00067448
Iteration 9/25 | Loss: 0.00067448
Iteration 10/25 | Loss: 0.00067448
Iteration 11/25 | Loss: 0.00067448
Iteration 12/25 | Loss: 0.00067448
Iteration 13/25 | Loss: 0.00067448
Iteration 14/25 | Loss: 0.00067448
Iteration 15/25 | Loss: 0.00067448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006744839483872056, 0.0006744839483872056, 0.0006744839483872056, 0.0006744839483872056, 0.0006744839483872056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006744839483872056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42153788
Iteration 2/25 | Loss: 0.00031915
Iteration 3/25 | Loss: 0.00031914
Iteration 4/25 | Loss: 0.00031914
Iteration 5/25 | Loss: 0.00031914
Iteration 6/25 | Loss: 0.00031914
Iteration 7/25 | Loss: 0.00031914
Iteration 8/25 | Loss: 0.00031914
Iteration 9/25 | Loss: 0.00031914
Iteration 10/25 | Loss: 0.00031914
Iteration 11/25 | Loss: 0.00031914
Iteration 12/25 | Loss: 0.00031914
Iteration 13/25 | Loss: 0.00031914
Iteration 14/25 | Loss: 0.00031914
Iteration 15/25 | Loss: 0.00031914
Iteration 16/25 | Loss: 0.00031914
Iteration 17/25 | Loss: 0.00031914
Iteration 18/25 | Loss: 0.00031914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00031913642305880785, 0.00031913642305880785, 0.00031913642305880785, 0.00031913642305880785, 0.00031913642305880785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031913642305880785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031914
Iteration 2/1000 | Loss: 0.00003475
Iteration 3/1000 | Loss: 0.00002640
Iteration 4/1000 | Loss: 0.00002316
Iteration 5/1000 | Loss: 0.00002167
Iteration 6/1000 | Loss: 0.00002078
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001939
Iteration 10/1000 | Loss: 0.00001905
Iteration 11/1000 | Loss: 0.00001896
Iteration 12/1000 | Loss: 0.00001878
Iteration 13/1000 | Loss: 0.00001871
Iteration 14/1000 | Loss: 0.00001871
Iteration 15/1000 | Loss: 0.00001868
Iteration 16/1000 | Loss: 0.00001867
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001847
Iteration 19/1000 | Loss: 0.00001844
Iteration 20/1000 | Loss: 0.00001843
Iteration 21/1000 | Loss: 0.00001843
Iteration 22/1000 | Loss: 0.00001843
Iteration 23/1000 | Loss: 0.00001843
Iteration 24/1000 | Loss: 0.00001843
Iteration 25/1000 | Loss: 0.00001842
Iteration 26/1000 | Loss: 0.00001842
Iteration 27/1000 | Loss: 0.00001842
Iteration 28/1000 | Loss: 0.00001842
Iteration 29/1000 | Loss: 0.00001841
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00001840
Iteration 32/1000 | Loss: 0.00001839
Iteration 33/1000 | Loss: 0.00001839
Iteration 34/1000 | Loss: 0.00001839
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001838
Iteration 37/1000 | Loss: 0.00001837
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001836
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001833
Iteration 52/1000 | Loss: 0.00001833
Iteration 53/1000 | Loss: 0.00001832
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001832
Iteration 56/1000 | Loss: 0.00001832
Iteration 57/1000 | Loss: 0.00001832
Iteration 58/1000 | Loss: 0.00001831
Iteration 59/1000 | Loss: 0.00001831
Iteration 60/1000 | Loss: 0.00001831
Iteration 61/1000 | Loss: 0.00001831
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001830
Iteration 65/1000 | Loss: 0.00001830
Iteration 66/1000 | Loss: 0.00001830
Iteration 67/1000 | Loss: 0.00001830
Iteration 68/1000 | Loss: 0.00001830
Iteration 69/1000 | Loss: 0.00001830
Iteration 70/1000 | Loss: 0.00001830
Iteration 71/1000 | Loss: 0.00001830
Iteration 72/1000 | Loss: 0.00001830
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001829
Iteration 75/1000 | Loss: 0.00001829
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001829
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001828
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001827
Iteration 100/1000 | Loss: 0.00001827
Iteration 101/1000 | Loss: 0.00001827
Iteration 102/1000 | Loss: 0.00001827
Iteration 103/1000 | Loss: 0.00001827
Iteration 104/1000 | Loss: 0.00001827
Iteration 105/1000 | Loss: 0.00001827
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001827
Iteration 108/1000 | Loss: 0.00001827
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001826
Iteration 115/1000 | Loss: 0.00001826
Iteration 116/1000 | Loss: 0.00001826
Iteration 117/1000 | Loss: 0.00001826
Iteration 118/1000 | Loss: 0.00001826
Iteration 119/1000 | Loss: 0.00001826
Iteration 120/1000 | Loss: 0.00001825
Iteration 121/1000 | Loss: 0.00001825
Iteration 122/1000 | Loss: 0.00001825
Iteration 123/1000 | Loss: 0.00001825
Iteration 124/1000 | Loss: 0.00001825
Iteration 125/1000 | Loss: 0.00001825
Iteration 126/1000 | Loss: 0.00001825
Iteration 127/1000 | Loss: 0.00001825
Iteration 128/1000 | Loss: 0.00001825
Iteration 129/1000 | Loss: 0.00001825
Iteration 130/1000 | Loss: 0.00001825
Iteration 131/1000 | Loss: 0.00001825
Iteration 132/1000 | Loss: 0.00001825
Iteration 133/1000 | Loss: 0.00001825
Iteration 134/1000 | Loss: 0.00001825
Iteration 135/1000 | Loss: 0.00001825
Iteration 136/1000 | Loss: 0.00001824
Iteration 137/1000 | Loss: 0.00001824
Iteration 138/1000 | Loss: 0.00001824
Iteration 139/1000 | Loss: 0.00001824
Iteration 140/1000 | Loss: 0.00001824
Iteration 141/1000 | Loss: 0.00001824
Iteration 142/1000 | Loss: 0.00001824
Iteration 143/1000 | Loss: 0.00001824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.8243605154566467e-05, 1.8243605154566467e-05, 1.8243605154566467e-05, 1.8243605154566467e-05, 1.8243605154566467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8243605154566467e-05

Optimization complete. Final v2v error: 3.6099934577941895 mm

Highest mean error: 4.790685176849365 mm for frame 164

Lowest mean error: 3.0771729946136475 mm for frame 217

Saving results

Total time: 43.08985614776611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810453
Iteration 2/25 | Loss: 0.00113692
Iteration 3/25 | Loss: 0.00079259
Iteration 4/25 | Loss: 0.00073673
Iteration 5/25 | Loss: 0.00071756
Iteration 6/25 | Loss: 0.00070977
Iteration 7/25 | Loss: 0.00071554
Iteration 8/25 | Loss: 0.00072156
Iteration 9/25 | Loss: 0.00070496
Iteration 10/25 | Loss: 0.00069611
Iteration 11/25 | Loss: 0.00069489
Iteration 12/25 | Loss: 0.00069482
Iteration 13/25 | Loss: 0.00069481
Iteration 14/25 | Loss: 0.00069481
Iteration 15/25 | Loss: 0.00069481
Iteration 16/25 | Loss: 0.00069481
Iteration 17/25 | Loss: 0.00069481
Iteration 18/25 | Loss: 0.00069480
Iteration 19/25 | Loss: 0.00069480
Iteration 20/25 | Loss: 0.00069480
Iteration 21/25 | Loss: 0.00069480
Iteration 22/25 | Loss: 0.00069480
Iteration 23/25 | Loss: 0.00069480
Iteration 24/25 | Loss: 0.00069480
Iteration 25/25 | Loss: 0.00069480

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44894660
Iteration 2/25 | Loss: 0.00025618
Iteration 3/25 | Loss: 0.00025618
Iteration 4/25 | Loss: 0.00025617
Iteration 5/25 | Loss: 0.00025617
Iteration 6/25 | Loss: 0.00025617
Iteration 7/25 | Loss: 0.00025617
Iteration 8/25 | Loss: 0.00025617
Iteration 9/25 | Loss: 0.00025617
Iteration 10/25 | Loss: 0.00025617
Iteration 11/25 | Loss: 0.00025617
Iteration 12/25 | Loss: 0.00025617
Iteration 13/25 | Loss: 0.00025617
Iteration 14/25 | Loss: 0.00025617
Iteration 15/25 | Loss: 0.00025617
Iteration 16/25 | Loss: 0.00025617
Iteration 17/25 | Loss: 0.00025617
Iteration 18/25 | Loss: 0.00025617
Iteration 19/25 | Loss: 0.00025617
Iteration 20/25 | Loss: 0.00025617
Iteration 21/25 | Loss: 0.00025617
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002561716246418655, 0.0002561716246418655, 0.0002561716246418655, 0.0002561716246418655, 0.0002561716246418655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002561716246418655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025617
Iteration 2/1000 | Loss: 0.00004178
Iteration 3/1000 | Loss: 0.00003353
Iteration 4/1000 | Loss: 0.00002723
Iteration 5/1000 | Loss: 0.00002528
Iteration 6/1000 | Loss: 0.00002396
Iteration 7/1000 | Loss: 0.00002327
Iteration 8/1000 | Loss: 0.00002262
Iteration 9/1000 | Loss: 0.00002228
Iteration 10/1000 | Loss: 0.00002220
Iteration 11/1000 | Loss: 0.00002203
Iteration 12/1000 | Loss: 0.00002192
Iteration 13/1000 | Loss: 0.00002191
Iteration 14/1000 | Loss: 0.00002185
Iteration 15/1000 | Loss: 0.00002182
Iteration 16/1000 | Loss: 0.00002182
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002176
Iteration 22/1000 | Loss: 0.00002175
Iteration 23/1000 | Loss: 0.00002173
Iteration 24/1000 | Loss: 0.00002172
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002167
Iteration 27/1000 | Loss: 0.00002167
Iteration 28/1000 | Loss: 0.00002167
Iteration 29/1000 | Loss: 0.00002167
Iteration 30/1000 | Loss: 0.00002166
Iteration 31/1000 | Loss: 0.00002166
Iteration 32/1000 | Loss: 0.00002165
Iteration 33/1000 | Loss: 0.00002165
Iteration 34/1000 | Loss: 0.00002165
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002165
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002164
Iteration 39/1000 | Loss: 0.00002164
Iteration 40/1000 | Loss: 0.00002164
Iteration 41/1000 | Loss: 0.00002164
Iteration 42/1000 | Loss: 0.00002163
Iteration 43/1000 | Loss: 0.00002163
Iteration 44/1000 | Loss: 0.00002163
Iteration 45/1000 | Loss: 0.00002162
Iteration 46/1000 | Loss: 0.00002162
Iteration 47/1000 | Loss: 0.00002162
Iteration 48/1000 | Loss: 0.00002162
Iteration 49/1000 | Loss: 0.00002162
Iteration 50/1000 | Loss: 0.00002162
Iteration 51/1000 | Loss: 0.00002162
Iteration 52/1000 | Loss: 0.00002162
Iteration 53/1000 | Loss: 0.00002162
Iteration 54/1000 | Loss: 0.00002162
Iteration 55/1000 | Loss: 0.00002161
Iteration 56/1000 | Loss: 0.00002161
Iteration 57/1000 | Loss: 0.00002161
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002161
Iteration 64/1000 | Loss: 0.00002161
Iteration 65/1000 | Loss: 0.00002160
Iteration 66/1000 | Loss: 0.00002160
Iteration 67/1000 | Loss: 0.00002160
Iteration 68/1000 | Loss: 0.00002160
Iteration 69/1000 | Loss: 0.00002160
Iteration 70/1000 | Loss: 0.00002160
Iteration 71/1000 | Loss: 0.00002160
Iteration 72/1000 | Loss: 0.00002160
Iteration 73/1000 | Loss: 0.00002160
Iteration 74/1000 | Loss: 0.00002160
Iteration 75/1000 | Loss: 0.00002160
Iteration 76/1000 | Loss: 0.00002160
Iteration 77/1000 | Loss: 0.00002159
Iteration 78/1000 | Loss: 0.00002159
Iteration 79/1000 | Loss: 0.00002159
Iteration 80/1000 | Loss: 0.00002159
Iteration 81/1000 | Loss: 0.00002159
Iteration 82/1000 | Loss: 0.00002159
Iteration 83/1000 | Loss: 0.00002159
Iteration 84/1000 | Loss: 0.00002159
Iteration 85/1000 | Loss: 0.00002159
Iteration 86/1000 | Loss: 0.00002159
Iteration 87/1000 | Loss: 0.00002159
Iteration 88/1000 | Loss: 0.00002158
Iteration 89/1000 | Loss: 0.00002158
Iteration 90/1000 | Loss: 0.00002158
Iteration 91/1000 | Loss: 0.00002158
Iteration 92/1000 | Loss: 0.00002158
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002158
Iteration 95/1000 | Loss: 0.00002157
Iteration 96/1000 | Loss: 0.00002157
Iteration 97/1000 | Loss: 0.00002157
Iteration 98/1000 | Loss: 0.00002157
Iteration 99/1000 | Loss: 0.00002157
Iteration 100/1000 | Loss: 0.00002157
Iteration 101/1000 | Loss: 0.00002157
Iteration 102/1000 | Loss: 0.00002157
Iteration 103/1000 | Loss: 0.00002157
Iteration 104/1000 | Loss: 0.00002157
Iteration 105/1000 | Loss: 0.00002157
Iteration 106/1000 | Loss: 0.00002157
Iteration 107/1000 | Loss: 0.00002157
Iteration 108/1000 | Loss: 0.00002157
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002157
Iteration 111/1000 | Loss: 0.00002157
Iteration 112/1000 | Loss: 0.00002157
Iteration 113/1000 | Loss: 0.00002157
Iteration 114/1000 | Loss: 0.00002157
Iteration 115/1000 | Loss: 0.00002157
Iteration 116/1000 | Loss: 0.00002157
Iteration 117/1000 | Loss: 0.00002157
Iteration 118/1000 | Loss: 0.00002157
Iteration 119/1000 | Loss: 0.00002157
Iteration 120/1000 | Loss: 0.00002157
Iteration 121/1000 | Loss: 0.00002157
Iteration 122/1000 | Loss: 0.00002157
Iteration 123/1000 | Loss: 0.00002157
Iteration 124/1000 | Loss: 0.00002157
Iteration 125/1000 | Loss: 0.00002157
Iteration 126/1000 | Loss: 0.00002157
Iteration 127/1000 | Loss: 0.00002157
Iteration 128/1000 | Loss: 0.00002157
Iteration 129/1000 | Loss: 0.00002157
Iteration 130/1000 | Loss: 0.00002157
Iteration 131/1000 | Loss: 0.00002157
Iteration 132/1000 | Loss: 0.00002157
Iteration 133/1000 | Loss: 0.00002157
Iteration 134/1000 | Loss: 0.00002157
Iteration 135/1000 | Loss: 0.00002157
Iteration 136/1000 | Loss: 0.00002157
Iteration 137/1000 | Loss: 0.00002157
Iteration 138/1000 | Loss: 0.00002157
Iteration 139/1000 | Loss: 0.00002157
Iteration 140/1000 | Loss: 0.00002157
Iteration 141/1000 | Loss: 0.00002157
Iteration 142/1000 | Loss: 0.00002157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.1571870092884637e-05, 2.1571870092884637e-05, 2.1571870092884637e-05, 2.1571870092884637e-05, 2.1571870092884637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1571870092884637e-05

Optimization complete. Final v2v error: 3.854418992996216 mm

Highest mean error: 4.12048864364624 mm for frame 80

Lowest mean error: 3.65054988861084 mm for frame 13

Saving results

Total time: 42.272547006607056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825443
Iteration 2/25 | Loss: 0.00079154
Iteration 3/25 | Loss: 0.00062438
Iteration 4/25 | Loss: 0.00059912
Iteration 5/25 | Loss: 0.00059208
Iteration 6/25 | Loss: 0.00059056
Iteration 7/25 | Loss: 0.00059039
Iteration 8/25 | Loss: 0.00059037
Iteration 9/25 | Loss: 0.00059037
Iteration 10/25 | Loss: 0.00059037
Iteration 11/25 | Loss: 0.00059037
Iteration 12/25 | Loss: 0.00059037
Iteration 13/25 | Loss: 0.00059037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005903710843995214, 0.0005903710843995214, 0.0005903710843995214, 0.0005903710843995214, 0.0005903710843995214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005903710843995214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46257889
Iteration 2/25 | Loss: 0.00026849
Iteration 3/25 | Loss: 0.00026848
Iteration 4/25 | Loss: 0.00026848
Iteration 5/25 | Loss: 0.00026848
Iteration 6/25 | Loss: 0.00026848
Iteration 7/25 | Loss: 0.00026848
Iteration 8/25 | Loss: 0.00026848
Iteration 9/25 | Loss: 0.00026848
Iteration 10/25 | Loss: 0.00026848
Iteration 11/25 | Loss: 0.00026848
Iteration 12/25 | Loss: 0.00026848
Iteration 13/25 | Loss: 0.00026848
Iteration 14/25 | Loss: 0.00026848
Iteration 15/25 | Loss: 0.00026848
Iteration 16/25 | Loss: 0.00026848
Iteration 17/25 | Loss: 0.00026848
Iteration 18/25 | Loss: 0.00026848
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002684802748262882, 0.0002684802748262882, 0.0002684802748262882, 0.0002684802748262882, 0.0002684802748262882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002684802748262882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026848
Iteration 2/1000 | Loss: 0.00001915
Iteration 3/1000 | Loss: 0.00001272
Iteration 4/1000 | Loss: 0.00001196
Iteration 5/1000 | Loss: 0.00001127
Iteration 6/1000 | Loss: 0.00001101
Iteration 7/1000 | Loss: 0.00001074
Iteration 8/1000 | Loss: 0.00001065
Iteration 9/1000 | Loss: 0.00001063
Iteration 10/1000 | Loss: 0.00001059
Iteration 11/1000 | Loss: 0.00001059
Iteration 12/1000 | Loss: 0.00001057
Iteration 13/1000 | Loss: 0.00001055
Iteration 14/1000 | Loss: 0.00001054
Iteration 15/1000 | Loss: 0.00001054
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001051
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001050
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001046
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001044
Iteration 35/1000 | Loss: 0.00001044
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001041
Iteration 38/1000 | Loss: 0.00001040
Iteration 39/1000 | Loss: 0.00001040
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001039
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001037
Iteration 47/1000 | Loss: 0.00001037
Iteration 48/1000 | Loss: 0.00001036
Iteration 49/1000 | Loss: 0.00001036
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001035
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001033
Iteration 56/1000 | Loss: 0.00001033
Iteration 57/1000 | Loss: 0.00001033
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001033
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001032
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001030
Iteration 65/1000 | Loss: 0.00001030
Iteration 66/1000 | Loss: 0.00001029
Iteration 67/1000 | Loss: 0.00001029
Iteration 68/1000 | Loss: 0.00001029
Iteration 69/1000 | Loss: 0.00001028
Iteration 70/1000 | Loss: 0.00001028
Iteration 71/1000 | Loss: 0.00001028
Iteration 72/1000 | Loss: 0.00001026
Iteration 73/1000 | Loss: 0.00001024
Iteration 74/1000 | Loss: 0.00001024
Iteration 75/1000 | Loss: 0.00001024
Iteration 76/1000 | Loss: 0.00001023
Iteration 77/1000 | Loss: 0.00001023
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001021
Iteration 82/1000 | Loss: 0.00001021
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001021
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001021
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001020
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001020
Iteration 95/1000 | Loss: 0.00001020
Iteration 96/1000 | Loss: 0.00001020
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001020
Iteration 99/1000 | Loss: 0.00001020
Iteration 100/1000 | Loss: 0.00001020
Iteration 101/1000 | Loss: 0.00001020
Iteration 102/1000 | Loss: 0.00001019
Iteration 103/1000 | Loss: 0.00001019
Iteration 104/1000 | Loss: 0.00001019
Iteration 105/1000 | Loss: 0.00001019
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001019
Iteration 108/1000 | Loss: 0.00001019
Iteration 109/1000 | Loss: 0.00001019
Iteration 110/1000 | Loss: 0.00001019
Iteration 111/1000 | Loss: 0.00001019
Iteration 112/1000 | Loss: 0.00001019
Iteration 113/1000 | Loss: 0.00001019
Iteration 114/1000 | Loss: 0.00001018
Iteration 115/1000 | Loss: 0.00001018
Iteration 116/1000 | Loss: 0.00001018
Iteration 117/1000 | Loss: 0.00001017
Iteration 118/1000 | Loss: 0.00001017
Iteration 119/1000 | Loss: 0.00001017
Iteration 120/1000 | Loss: 0.00001017
Iteration 121/1000 | Loss: 0.00001017
Iteration 122/1000 | Loss: 0.00001016
Iteration 123/1000 | Loss: 0.00001016
Iteration 124/1000 | Loss: 0.00001016
Iteration 125/1000 | Loss: 0.00001016
Iteration 126/1000 | Loss: 0.00001016
Iteration 127/1000 | Loss: 0.00001016
Iteration 128/1000 | Loss: 0.00001016
Iteration 129/1000 | Loss: 0.00001016
Iteration 130/1000 | Loss: 0.00001016
Iteration 131/1000 | Loss: 0.00001016
Iteration 132/1000 | Loss: 0.00001016
Iteration 133/1000 | Loss: 0.00001015
Iteration 134/1000 | Loss: 0.00001015
Iteration 135/1000 | Loss: 0.00001015
Iteration 136/1000 | Loss: 0.00001015
Iteration 137/1000 | Loss: 0.00001015
Iteration 138/1000 | Loss: 0.00001015
Iteration 139/1000 | Loss: 0.00001015
Iteration 140/1000 | Loss: 0.00001015
Iteration 141/1000 | Loss: 0.00001015
Iteration 142/1000 | Loss: 0.00001015
Iteration 143/1000 | Loss: 0.00001014
Iteration 144/1000 | Loss: 0.00001014
Iteration 145/1000 | Loss: 0.00001014
Iteration 146/1000 | Loss: 0.00001014
Iteration 147/1000 | Loss: 0.00001014
Iteration 148/1000 | Loss: 0.00001014
Iteration 149/1000 | Loss: 0.00001014
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001013
Iteration 152/1000 | Loss: 0.00001013
Iteration 153/1000 | Loss: 0.00001013
Iteration 154/1000 | Loss: 0.00001013
Iteration 155/1000 | Loss: 0.00001013
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001013
Iteration 158/1000 | Loss: 0.00001012
Iteration 159/1000 | Loss: 0.00001012
Iteration 160/1000 | Loss: 0.00001012
Iteration 161/1000 | Loss: 0.00001012
Iteration 162/1000 | Loss: 0.00001012
Iteration 163/1000 | Loss: 0.00001012
Iteration 164/1000 | Loss: 0.00001012
Iteration 165/1000 | Loss: 0.00001012
Iteration 166/1000 | Loss: 0.00001011
Iteration 167/1000 | Loss: 0.00001011
Iteration 168/1000 | Loss: 0.00001011
Iteration 169/1000 | Loss: 0.00001011
Iteration 170/1000 | Loss: 0.00001011
Iteration 171/1000 | Loss: 0.00001011
Iteration 172/1000 | Loss: 0.00001011
Iteration 173/1000 | Loss: 0.00001010
Iteration 174/1000 | Loss: 0.00001010
Iteration 175/1000 | Loss: 0.00001010
Iteration 176/1000 | Loss: 0.00001010
Iteration 177/1000 | Loss: 0.00001010
Iteration 178/1000 | Loss: 0.00001010
Iteration 179/1000 | Loss: 0.00001010
Iteration 180/1000 | Loss: 0.00001010
Iteration 181/1000 | Loss: 0.00001010
Iteration 182/1000 | Loss: 0.00001010
Iteration 183/1000 | Loss: 0.00001009
Iteration 184/1000 | Loss: 0.00001009
Iteration 185/1000 | Loss: 0.00001009
Iteration 186/1000 | Loss: 0.00001009
Iteration 187/1000 | Loss: 0.00001009
Iteration 188/1000 | Loss: 0.00001009
Iteration 189/1000 | Loss: 0.00001009
Iteration 190/1000 | Loss: 0.00001009
Iteration 191/1000 | Loss: 0.00001009
Iteration 192/1000 | Loss: 0.00001009
Iteration 193/1000 | Loss: 0.00001009
Iteration 194/1000 | Loss: 0.00001008
Iteration 195/1000 | Loss: 0.00001008
Iteration 196/1000 | Loss: 0.00001008
Iteration 197/1000 | Loss: 0.00001008
Iteration 198/1000 | Loss: 0.00001008
Iteration 199/1000 | Loss: 0.00001008
Iteration 200/1000 | Loss: 0.00001008
Iteration 201/1000 | Loss: 0.00001008
Iteration 202/1000 | Loss: 0.00001008
Iteration 203/1000 | Loss: 0.00001008
Iteration 204/1000 | Loss: 0.00001008
Iteration 205/1000 | Loss: 0.00001008
Iteration 206/1000 | Loss: 0.00001008
Iteration 207/1000 | Loss: 0.00001008
Iteration 208/1000 | Loss: 0.00001008
Iteration 209/1000 | Loss: 0.00001008
Iteration 210/1000 | Loss: 0.00001008
Iteration 211/1000 | Loss: 0.00001008
Iteration 212/1000 | Loss: 0.00001008
Iteration 213/1000 | Loss: 0.00001008
Iteration 214/1000 | Loss: 0.00001008
Iteration 215/1000 | Loss: 0.00001008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.0080229913000949e-05, 1.0080229913000949e-05, 1.0080229913000949e-05, 1.0080229913000949e-05, 1.0080229913000949e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0080229913000949e-05

Optimization complete. Final v2v error: 2.671717882156372 mm

Highest mean error: 2.874034881591797 mm for frame 80

Lowest mean error: 2.530841827392578 mm for frame 175

Saving results

Total time: 36.522470474243164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969384
Iteration 2/25 | Loss: 0.00131790
Iteration 3/25 | Loss: 0.00098280
Iteration 4/25 | Loss: 0.00081395
Iteration 5/25 | Loss: 0.00081255
Iteration 6/25 | Loss: 0.00073458
Iteration 7/25 | Loss: 0.00070733
Iteration 8/25 | Loss: 0.00070109
Iteration 9/25 | Loss: 0.00070628
Iteration 10/25 | Loss: 0.00068921
Iteration 11/25 | Loss: 0.00067883
Iteration 12/25 | Loss: 0.00067710
Iteration 13/25 | Loss: 0.00068111
Iteration 14/25 | Loss: 0.00067920
Iteration 15/25 | Loss: 0.00067715
Iteration 16/25 | Loss: 0.00067894
Iteration 17/25 | Loss: 0.00067546
Iteration 18/25 | Loss: 0.00067546
Iteration 19/25 | Loss: 0.00067546
Iteration 20/25 | Loss: 0.00067546
Iteration 21/25 | Loss: 0.00067546
Iteration 22/25 | Loss: 0.00067546
Iteration 23/25 | Loss: 0.00067546
Iteration 24/25 | Loss: 0.00067546
Iteration 25/25 | Loss: 0.00067546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56743670
Iteration 2/25 | Loss: 0.00030432
Iteration 3/25 | Loss: 0.00030429
Iteration 4/25 | Loss: 0.00030429
Iteration 5/25 | Loss: 0.00030429
Iteration 6/25 | Loss: 0.00030429
Iteration 7/25 | Loss: 0.00030429
Iteration 8/25 | Loss: 0.00030429
Iteration 9/25 | Loss: 0.00030429
Iteration 10/25 | Loss: 0.00030429
Iteration 11/25 | Loss: 0.00030429
Iteration 12/25 | Loss: 0.00030429
Iteration 13/25 | Loss: 0.00030429
Iteration 14/25 | Loss: 0.00030429
Iteration 15/25 | Loss: 0.00030429
Iteration 16/25 | Loss: 0.00030429
Iteration 17/25 | Loss: 0.00030429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00030429181060753763, 0.00030429181060753763, 0.00030429181060753763, 0.00030429181060753763, 0.00030429181060753763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030429181060753763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030429
Iteration 2/1000 | Loss: 0.00003512
Iteration 3/1000 | Loss: 0.00007690
Iteration 4/1000 | Loss: 0.00002334
Iteration 5/1000 | Loss: 0.00006266
Iteration 6/1000 | Loss: 0.00002141
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00007747
Iteration 9/1000 | Loss: 0.00001997
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001961
Iteration 12/1000 | Loss: 0.00001960
Iteration 13/1000 | Loss: 0.00001942
Iteration 14/1000 | Loss: 0.00001940
Iteration 15/1000 | Loss: 0.00001935
Iteration 16/1000 | Loss: 0.00001931
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001920
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001920
Iteration 22/1000 | Loss: 0.00001919
Iteration 23/1000 | Loss: 0.00001919
Iteration 24/1000 | Loss: 0.00001918
Iteration 25/1000 | Loss: 0.00001917
Iteration 26/1000 | Loss: 0.00001917
Iteration 27/1000 | Loss: 0.00001917
Iteration 28/1000 | Loss: 0.00001916
Iteration 29/1000 | Loss: 0.00001916
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001915
Iteration 33/1000 | Loss: 0.00001915
Iteration 34/1000 | Loss: 0.00001915
Iteration 35/1000 | Loss: 0.00001914
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001914
Iteration 38/1000 | Loss: 0.00001914
Iteration 39/1000 | Loss: 0.00001914
Iteration 40/1000 | Loss: 0.00001913
Iteration 41/1000 | Loss: 0.00001913
Iteration 42/1000 | Loss: 0.00001913
Iteration 43/1000 | Loss: 0.00001913
Iteration 44/1000 | Loss: 0.00001913
Iteration 45/1000 | Loss: 0.00001913
Iteration 46/1000 | Loss: 0.00001912
Iteration 47/1000 | Loss: 0.00001912
Iteration 48/1000 | Loss: 0.00001912
Iteration 49/1000 | Loss: 0.00001912
Iteration 50/1000 | Loss: 0.00001912
Iteration 51/1000 | Loss: 0.00001911
Iteration 52/1000 | Loss: 0.00001911
Iteration 53/1000 | Loss: 0.00001911
Iteration 54/1000 | Loss: 0.00001911
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001911
Iteration 57/1000 | Loss: 0.00001911
Iteration 58/1000 | Loss: 0.00001911
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001911
Iteration 63/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.9108552805846557e-05, 1.9108552805846557e-05, 1.9108552805846557e-05, 1.9108552805846557e-05, 1.9108552805846557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9108552805846557e-05

Optimization complete. Final v2v error: 3.700733184814453 mm

Highest mean error: 4.756660461425781 mm for frame 39

Lowest mean error: 3.1604599952697754 mm for frame 200

Saving results

Total time: 53.50525236129761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052379
Iteration 2/25 | Loss: 0.00192416
Iteration 3/25 | Loss: 0.00164729
Iteration 4/25 | Loss: 0.00156245
Iteration 5/25 | Loss: 0.00144729
Iteration 6/25 | Loss: 0.00102304
Iteration 7/25 | Loss: 0.00085751
Iteration 8/25 | Loss: 0.00081241
Iteration 9/25 | Loss: 0.00080840
Iteration 10/25 | Loss: 0.00080757
Iteration 11/25 | Loss: 0.00080757
Iteration 12/25 | Loss: 0.00080757
Iteration 13/25 | Loss: 0.00080757
Iteration 14/25 | Loss: 0.00080757
Iteration 15/25 | Loss: 0.00080757
Iteration 16/25 | Loss: 0.00080757
Iteration 17/25 | Loss: 0.00080757
Iteration 18/25 | Loss: 0.00080757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008075670339167118, 0.0008075670339167118, 0.0008075670339167118, 0.0008075670339167118, 0.0008075670339167118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008075670339167118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41811132
Iteration 2/25 | Loss: 0.00028153
Iteration 3/25 | Loss: 0.00028152
Iteration 4/25 | Loss: 0.00028152
Iteration 5/25 | Loss: 0.00028152
Iteration 6/25 | Loss: 0.00028152
Iteration 7/25 | Loss: 0.00028152
Iteration 8/25 | Loss: 0.00028152
Iteration 9/25 | Loss: 0.00028152
Iteration 10/25 | Loss: 0.00028152
Iteration 11/25 | Loss: 0.00028152
Iteration 12/25 | Loss: 0.00028152
Iteration 13/25 | Loss: 0.00028152
Iteration 14/25 | Loss: 0.00028152
Iteration 15/25 | Loss: 0.00028152
Iteration 16/25 | Loss: 0.00028152
Iteration 17/25 | Loss: 0.00028152
Iteration 18/25 | Loss: 0.00028152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002815165789797902, 0.0002815165789797902, 0.0002815165789797902, 0.0002815165789797902, 0.0002815165789797902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002815165789797902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028152
Iteration 2/1000 | Loss: 0.00004114
Iteration 3/1000 | Loss: 0.00003023
Iteration 4/1000 | Loss: 0.00002708
Iteration 5/1000 | Loss: 0.00002593
Iteration 6/1000 | Loss: 0.00002514
Iteration 7/1000 | Loss: 0.00002463
Iteration 8/1000 | Loss: 0.00002432
Iteration 9/1000 | Loss: 0.00002410
Iteration 10/1000 | Loss: 0.00002409
Iteration 11/1000 | Loss: 0.00002397
Iteration 12/1000 | Loss: 0.00002391
Iteration 13/1000 | Loss: 0.00002391
Iteration 14/1000 | Loss: 0.00002390
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002384
Iteration 17/1000 | Loss: 0.00002380
Iteration 18/1000 | Loss: 0.00002376
Iteration 19/1000 | Loss: 0.00002376
Iteration 20/1000 | Loss: 0.00002376
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002375
Iteration 23/1000 | Loss: 0.00002375
Iteration 24/1000 | Loss: 0.00002375
Iteration 25/1000 | Loss: 0.00002375
Iteration 26/1000 | Loss: 0.00002375
Iteration 27/1000 | Loss: 0.00002375
Iteration 28/1000 | Loss: 0.00002373
Iteration 29/1000 | Loss: 0.00002373
Iteration 30/1000 | Loss: 0.00002373
Iteration 31/1000 | Loss: 0.00002372
Iteration 32/1000 | Loss: 0.00002372
Iteration 33/1000 | Loss: 0.00002371
Iteration 34/1000 | Loss: 0.00002371
Iteration 35/1000 | Loss: 0.00002370
Iteration 36/1000 | Loss: 0.00002370
Iteration 37/1000 | Loss: 0.00002370
Iteration 38/1000 | Loss: 0.00002369
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002369
Iteration 41/1000 | Loss: 0.00002368
Iteration 42/1000 | Loss: 0.00002367
Iteration 43/1000 | Loss: 0.00002367
Iteration 44/1000 | Loss: 0.00002367
Iteration 45/1000 | Loss: 0.00002367
Iteration 46/1000 | Loss: 0.00002366
Iteration 47/1000 | Loss: 0.00002366
Iteration 48/1000 | Loss: 0.00002366
Iteration 49/1000 | Loss: 0.00002366
Iteration 50/1000 | Loss: 0.00002366
Iteration 51/1000 | Loss: 0.00002366
Iteration 52/1000 | Loss: 0.00002365
Iteration 53/1000 | Loss: 0.00002365
Iteration 54/1000 | Loss: 0.00002365
Iteration 55/1000 | Loss: 0.00002364
Iteration 56/1000 | Loss: 0.00002364
Iteration 57/1000 | Loss: 0.00002364
Iteration 58/1000 | Loss: 0.00002363
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00002362
Iteration 61/1000 | Loss: 0.00002361
Iteration 62/1000 | Loss: 0.00002361
Iteration 63/1000 | Loss: 0.00002361
Iteration 64/1000 | Loss: 0.00002360
Iteration 65/1000 | Loss: 0.00002360
Iteration 66/1000 | Loss: 0.00002360
Iteration 67/1000 | Loss: 0.00002359
Iteration 68/1000 | Loss: 0.00002359
Iteration 69/1000 | Loss: 0.00002358
Iteration 70/1000 | Loss: 0.00002358
Iteration 71/1000 | Loss: 0.00002358
Iteration 72/1000 | Loss: 0.00002358
Iteration 73/1000 | Loss: 0.00002358
Iteration 74/1000 | Loss: 0.00002358
Iteration 75/1000 | Loss: 0.00002357
Iteration 76/1000 | Loss: 0.00002357
Iteration 77/1000 | Loss: 0.00002357
Iteration 78/1000 | Loss: 0.00002357
Iteration 79/1000 | Loss: 0.00002357
Iteration 80/1000 | Loss: 0.00002357
Iteration 81/1000 | Loss: 0.00002357
Iteration 82/1000 | Loss: 0.00002357
Iteration 83/1000 | Loss: 0.00002356
Iteration 84/1000 | Loss: 0.00002355
Iteration 85/1000 | Loss: 0.00002355
Iteration 86/1000 | Loss: 0.00002354
Iteration 87/1000 | Loss: 0.00002354
Iteration 88/1000 | Loss: 0.00002354
Iteration 89/1000 | Loss: 0.00002354
Iteration 90/1000 | Loss: 0.00002353
Iteration 91/1000 | Loss: 0.00002352
Iteration 92/1000 | Loss: 0.00002352
Iteration 93/1000 | Loss: 0.00002352
Iteration 94/1000 | Loss: 0.00002352
Iteration 95/1000 | Loss: 0.00002352
Iteration 96/1000 | Loss: 0.00002352
Iteration 97/1000 | Loss: 0.00002352
Iteration 98/1000 | Loss: 0.00002352
Iteration 99/1000 | Loss: 0.00002352
Iteration 100/1000 | Loss: 0.00002351
Iteration 101/1000 | Loss: 0.00002351
Iteration 102/1000 | Loss: 0.00002351
Iteration 103/1000 | Loss: 0.00002351
Iteration 104/1000 | Loss: 0.00002351
Iteration 105/1000 | Loss: 0.00002350
Iteration 106/1000 | Loss: 0.00002350
Iteration 107/1000 | Loss: 0.00002350
Iteration 108/1000 | Loss: 0.00002350
Iteration 109/1000 | Loss: 0.00002349
Iteration 110/1000 | Loss: 0.00002349
Iteration 111/1000 | Loss: 0.00002349
Iteration 112/1000 | Loss: 0.00002349
Iteration 113/1000 | Loss: 0.00002349
Iteration 114/1000 | Loss: 0.00002349
Iteration 115/1000 | Loss: 0.00002349
Iteration 116/1000 | Loss: 0.00002349
Iteration 117/1000 | Loss: 0.00002348
Iteration 118/1000 | Loss: 0.00002348
Iteration 119/1000 | Loss: 0.00002348
Iteration 120/1000 | Loss: 0.00002348
Iteration 121/1000 | Loss: 0.00002348
Iteration 122/1000 | Loss: 0.00002347
Iteration 123/1000 | Loss: 0.00002347
Iteration 124/1000 | Loss: 0.00002347
Iteration 125/1000 | Loss: 0.00002347
Iteration 126/1000 | Loss: 0.00002347
Iteration 127/1000 | Loss: 0.00002347
Iteration 128/1000 | Loss: 0.00002347
Iteration 129/1000 | Loss: 0.00002347
Iteration 130/1000 | Loss: 0.00002347
Iteration 131/1000 | Loss: 0.00002347
Iteration 132/1000 | Loss: 0.00002347
Iteration 133/1000 | Loss: 0.00002347
Iteration 134/1000 | Loss: 0.00002347
Iteration 135/1000 | Loss: 0.00002347
Iteration 136/1000 | Loss: 0.00002347
Iteration 137/1000 | Loss: 0.00002347
Iteration 138/1000 | Loss: 0.00002347
Iteration 139/1000 | Loss: 0.00002347
Iteration 140/1000 | Loss: 0.00002346
Iteration 141/1000 | Loss: 0.00002346
Iteration 142/1000 | Loss: 0.00002346
Iteration 143/1000 | Loss: 0.00002346
Iteration 144/1000 | Loss: 0.00002346
Iteration 145/1000 | Loss: 0.00002346
Iteration 146/1000 | Loss: 0.00002346
Iteration 147/1000 | Loss: 0.00002346
Iteration 148/1000 | Loss: 0.00002345
Iteration 149/1000 | Loss: 0.00002345
Iteration 150/1000 | Loss: 0.00002345
Iteration 151/1000 | Loss: 0.00002345
Iteration 152/1000 | Loss: 0.00002345
Iteration 153/1000 | Loss: 0.00002345
Iteration 154/1000 | Loss: 0.00002345
Iteration 155/1000 | Loss: 0.00002345
Iteration 156/1000 | Loss: 0.00002345
Iteration 157/1000 | Loss: 0.00002344
Iteration 158/1000 | Loss: 0.00002344
Iteration 159/1000 | Loss: 0.00002344
Iteration 160/1000 | Loss: 0.00002344
Iteration 161/1000 | Loss: 0.00002344
Iteration 162/1000 | Loss: 0.00002344
Iteration 163/1000 | Loss: 0.00002344
Iteration 164/1000 | Loss: 0.00002344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.344254608033225e-05, 2.344254608033225e-05, 2.344254608033225e-05, 2.344254608033225e-05, 2.344254608033225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.344254608033225e-05

Optimization complete. Final v2v error: 4.109565258026123 mm

Highest mean error: 4.465981960296631 mm for frame 181

Lowest mean error: 3.947171449661255 mm for frame 44

Saving results

Total time: 49.08368253707886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083478
Iteration 2/25 | Loss: 0.01083477
Iteration 3/25 | Loss: 0.01083477
Iteration 4/25 | Loss: 0.01083477
Iteration 5/25 | Loss: 0.00213310
Iteration 6/25 | Loss: 0.00141537
Iteration 7/25 | Loss: 0.00114843
Iteration 8/25 | Loss: 0.00103262
Iteration 9/25 | Loss: 0.00101603
Iteration 10/25 | Loss: 0.00088891
Iteration 11/25 | Loss: 0.00085106
Iteration 12/25 | Loss: 0.00078513
Iteration 13/25 | Loss: 0.00081245
Iteration 14/25 | Loss: 0.00075463
Iteration 15/25 | Loss: 0.00073515
Iteration 16/25 | Loss: 0.00073899
Iteration 17/25 | Loss: 0.00072984
Iteration 18/25 | Loss: 0.00072004
Iteration 19/25 | Loss: 0.00071956
Iteration 20/25 | Loss: 0.00071478
Iteration 21/25 | Loss: 0.00071268
Iteration 22/25 | Loss: 0.00071181
Iteration 23/25 | Loss: 0.00071095
Iteration 24/25 | Loss: 0.00071067
Iteration 25/25 | Loss: 0.00071032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61291265
Iteration 2/25 | Loss: 0.00040668
Iteration 3/25 | Loss: 0.00038022
Iteration 4/25 | Loss: 0.00038021
Iteration 5/25 | Loss: 0.00038021
Iteration 6/25 | Loss: 0.00038021
Iteration 7/25 | Loss: 0.00038021
Iteration 8/25 | Loss: 0.00038021
Iteration 9/25 | Loss: 0.00038021
Iteration 10/25 | Loss: 0.00038021
Iteration 11/25 | Loss: 0.00038021
Iteration 12/25 | Loss: 0.00038021
Iteration 13/25 | Loss: 0.00038021
Iteration 14/25 | Loss: 0.00038021
Iteration 15/25 | Loss: 0.00038021
Iteration 16/25 | Loss: 0.00038021
Iteration 17/25 | Loss: 0.00038021
Iteration 18/25 | Loss: 0.00038021
Iteration 19/25 | Loss: 0.00038021
Iteration 20/25 | Loss: 0.00038021
Iteration 21/25 | Loss: 0.00038021
Iteration 22/25 | Loss: 0.00038021
Iteration 23/25 | Loss: 0.00038021
Iteration 24/25 | Loss: 0.00038021
Iteration 25/25 | Loss: 0.00038021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038021
Iteration 2/1000 | Loss: 0.00005942
Iteration 3/1000 | Loss: 0.00010423
Iteration 4/1000 | Loss: 0.00006716
Iteration 5/1000 | Loss: 0.00002094
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00007725
Iteration 8/1000 | Loss: 0.00007129
Iteration 9/1000 | Loss: 0.00002380
Iteration 10/1000 | Loss: 0.00001862
Iteration 11/1000 | Loss: 0.00002527
Iteration 12/1000 | Loss: 0.00002935
Iteration 13/1000 | Loss: 0.00001885
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00002182
Iteration 16/1000 | Loss: 0.00001887
Iteration 17/1000 | Loss: 0.00001816
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00011059
Iteration 20/1000 | Loss: 0.00023596
Iteration 21/1000 | Loss: 0.00002597
Iteration 22/1000 | Loss: 0.00002648
Iteration 23/1000 | Loss: 0.00001809
Iteration 24/1000 | Loss: 0.00001803
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001799
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001793
Iteration 31/1000 | Loss: 0.00001792
Iteration 32/1000 | Loss: 0.00009434
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001785
Iteration 35/1000 | Loss: 0.00001784
Iteration 36/1000 | Loss: 0.00001784
Iteration 37/1000 | Loss: 0.00001784
Iteration 38/1000 | Loss: 0.00001784
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001783
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 54. Stopping optimization.
Last 5 losses: [1.7825215763878077e-05, 1.7825215763878077e-05, 1.7825215763878077e-05, 1.7825215763878077e-05, 1.7825215763878077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7825215763878077e-05

Optimization complete. Final v2v error: 3.4740843772888184 mm

Highest mean error: 9.537582397460938 mm for frame 14

Lowest mean error: 2.8933680057525635 mm for frame 219

Saving results

Total time: 83.40944528579712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917186
Iteration 2/25 | Loss: 0.00128838
Iteration 3/25 | Loss: 0.00094019
Iteration 4/25 | Loss: 0.00084463
Iteration 5/25 | Loss: 0.00081040
Iteration 6/25 | Loss: 0.00080548
Iteration 7/25 | Loss: 0.00082049
Iteration 8/25 | Loss: 0.00081524
Iteration 9/25 | Loss: 0.00079277
Iteration 10/25 | Loss: 0.00078520
Iteration 11/25 | Loss: 0.00077717
Iteration 12/25 | Loss: 0.00077004
Iteration 13/25 | Loss: 0.00076309
Iteration 14/25 | Loss: 0.00075746
Iteration 15/25 | Loss: 0.00075598
Iteration 16/25 | Loss: 0.00075759
Iteration 17/25 | Loss: 0.00076147
Iteration 18/25 | Loss: 0.00075506
Iteration 19/25 | Loss: 0.00075323
Iteration 20/25 | Loss: 0.00075223
Iteration 21/25 | Loss: 0.00075061
Iteration 22/25 | Loss: 0.00074796
Iteration 23/25 | Loss: 0.00074643
Iteration 24/25 | Loss: 0.00074535
Iteration 25/25 | Loss: 0.00074477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.33269835
Iteration 2/25 | Loss: 0.00068549
Iteration 3/25 | Loss: 0.00068549
Iteration 4/25 | Loss: 0.00068549
Iteration 5/25 | Loss: 0.00068549
Iteration 6/25 | Loss: 0.00068549
Iteration 7/25 | Loss: 0.00068548
Iteration 8/25 | Loss: 0.00068548
Iteration 9/25 | Loss: 0.00068548
Iteration 10/25 | Loss: 0.00068548
Iteration 11/25 | Loss: 0.00068548
Iteration 12/25 | Loss: 0.00068548
Iteration 13/25 | Loss: 0.00068548
Iteration 14/25 | Loss: 0.00068548
Iteration 15/25 | Loss: 0.00068548
Iteration 16/25 | Loss: 0.00068548
Iteration 17/25 | Loss: 0.00068548
Iteration 18/25 | Loss: 0.00068548
Iteration 19/25 | Loss: 0.00068548
Iteration 20/25 | Loss: 0.00068548
Iteration 21/25 | Loss: 0.00068548
Iteration 22/25 | Loss: 0.00068548
Iteration 23/25 | Loss: 0.00068548
Iteration 24/25 | Loss: 0.00068548
Iteration 25/25 | Loss: 0.00068548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068548
Iteration 2/1000 | Loss: 0.00053383
Iteration 3/1000 | Loss: 0.00025917
Iteration 4/1000 | Loss: 0.00006879
Iteration 5/1000 | Loss: 0.00004693
Iteration 6/1000 | Loss: 0.00003950
Iteration 7/1000 | Loss: 0.00003706
Iteration 8/1000 | Loss: 0.00025223
Iteration 9/1000 | Loss: 0.00005441
Iteration 10/1000 | Loss: 0.00004622
Iteration 11/1000 | Loss: 0.00004381
Iteration 12/1000 | Loss: 0.00004236
Iteration 13/1000 | Loss: 0.00024013
Iteration 14/1000 | Loss: 0.00017255
Iteration 15/1000 | Loss: 0.00019043
Iteration 16/1000 | Loss: 0.00015871
Iteration 17/1000 | Loss: 0.00012130
Iteration 18/1000 | Loss: 0.00014248
Iteration 19/1000 | Loss: 0.00015677
Iteration 20/1000 | Loss: 0.00022784
Iteration 21/1000 | Loss: 0.00005083
Iteration 22/1000 | Loss: 0.00004099
Iteration 23/1000 | Loss: 0.00003813
Iteration 24/1000 | Loss: 0.00003628
Iteration 25/1000 | Loss: 0.00003455
Iteration 26/1000 | Loss: 0.00003353
Iteration 27/1000 | Loss: 0.00003289
Iteration 28/1000 | Loss: 0.00003236
Iteration 29/1000 | Loss: 0.00003192
Iteration 30/1000 | Loss: 0.00003154
Iteration 31/1000 | Loss: 0.00003121
Iteration 32/1000 | Loss: 0.00003082
Iteration 33/1000 | Loss: 0.00019388
Iteration 34/1000 | Loss: 0.00026942
Iteration 35/1000 | Loss: 0.00017967
Iteration 36/1000 | Loss: 0.00007793
Iteration 37/1000 | Loss: 0.00005453
Iteration 38/1000 | Loss: 0.00003928
Iteration 39/1000 | Loss: 0.00003548
Iteration 40/1000 | Loss: 0.00003337
Iteration 41/1000 | Loss: 0.00005386
Iteration 42/1000 | Loss: 0.00004739
Iteration 43/1000 | Loss: 0.00004990
Iteration 44/1000 | Loss: 0.00004407
Iteration 45/1000 | Loss: 0.00004496
Iteration 46/1000 | Loss: 0.00003575
Iteration 47/1000 | Loss: 0.00003346
Iteration 48/1000 | Loss: 0.00003215
Iteration 49/1000 | Loss: 0.00003158
Iteration 50/1000 | Loss: 0.00003068
Iteration 51/1000 | Loss: 0.00003008
Iteration 52/1000 | Loss: 0.00027214
Iteration 53/1000 | Loss: 0.00008484
Iteration 54/1000 | Loss: 0.00003919
Iteration 55/1000 | Loss: 0.00003183
Iteration 56/1000 | Loss: 0.00003004
Iteration 57/1000 | Loss: 0.00002928
Iteration 58/1000 | Loss: 0.00002901
Iteration 59/1000 | Loss: 0.00021621
Iteration 60/1000 | Loss: 0.00005592
Iteration 61/1000 | Loss: 0.00003023
Iteration 62/1000 | Loss: 0.00005595
Iteration 63/1000 | Loss: 0.00005305
Iteration 64/1000 | Loss: 0.00020770
Iteration 65/1000 | Loss: 0.00006025
Iteration 66/1000 | Loss: 0.00002961
Iteration 67/1000 | Loss: 0.00020088
Iteration 68/1000 | Loss: 0.00004228
Iteration 69/1000 | Loss: 0.00003330
Iteration 70/1000 | Loss: 0.00003198
Iteration 71/1000 | Loss: 0.00003119
Iteration 72/1000 | Loss: 0.00005919
Iteration 73/1000 | Loss: 0.00003464
Iteration 74/1000 | Loss: 0.00003167
Iteration 75/1000 | Loss: 0.00003068
Iteration 76/1000 | Loss: 0.00002994
Iteration 77/1000 | Loss: 0.00002937
Iteration 78/1000 | Loss: 0.00002897
Iteration 79/1000 | Loss: 0.00002869
Iteration 80/1000 | Loss: 0.00005005
Iteration 81/1000 | Loss: 0.00003434
Iteration 82/1000 | Loss: 0.00003065
Iteration 83/1000 | Loss: 0.00002932
Iteration 84/1000 | Loss: 0.00002848
Iteration 85/1000 | Loss: 0.00002835
Iteration 86/1000 | Loss: 0.00003274
Iteration 87/1000 | Loss: 0.00002818
Iteration 88/1000 | Loss: 0.00003537
Iteration 89/1000 | Loss: 0.00002814
Iteration 90/1000 | Loss: 0.00003773
Iteration 91/1000 | Loss: 0.00002810
Iteration 92/1000 | Loss: 0.00003507
Iteration 93/1000 | Loss: 0.00002811
Iteration 94/1000 | Loss: 0.00003798
Iteration 95/1000 | Loss: 0.00002763
Iteration 96/1000 | Loss: 0.00002759
Iteration 97/1000 | Loss: 0.00002738
Iteration 98/1000 | Loss: 0.00002711
Iteration 99/1000 | Loss: 0.00002687
Iteration 100/1000 | Loss: 0.00002686
Iteration 101/1000 | Loss: 0.00025571
Iteration 102/1000 | Loss: 0.00004598
Iteration 103/1000 | Loss: 0.00003106
Iteration 104/1000 | Loss: 0.00002890
Iteration 105/1000 | Loss: 0.00002745
Iteration 106/1000 | Loss: 0.00002681
Iteration 107/1000 | Loss: 0.00002634
Iteration 108/1000 | Loss: 0.00002611
Iteration 109/1000 | Loss: 0.00002599
Iteration 110/1000 | Loss: 0.00002599
Iteration 111/1000 | Loss: 0.00002598
Iteration 112/1000 | Loss: 0.00002597
Iteration 113/1000 | Loss: 0.00002597
Iteration 114/1000 | Loss: 0.00002596
Iteration 115/1000 | Loss: 0.00002596
Iteration 116/1000 | Loss: 0.00002596
Iteration 117/1000 | Loss: 0.00002596
Iteration 118/1000 | Loss: 0.00002596
Iteration 119/1000 | Loss: 0.00002596
Iteration 120/1000 | Loss: 0.00002596
Iteration 121/1000 | Loss: 0.00002596
Iteration 122/1000 | Loss: 0.00002596
Iteration 123/1000 | Loss: 0.00002595
Iteration 124/1000 | Loss: 0.00002595
Iteration 125/1000 | Loss: 0.00002595
Iteration 126/1000 | Loss: 0.00002595
Iteration 127/1000 | Loss: 0.00002595
Iteration 128/1000 | Loss: 0.00002595
Iteration 129/1000 | Loss: 0.00002595
Iteration 130/1000 | Loss: 0.00002595
Iteration 131/1000 | Loss: 0.00002595
Iteration 132/1000 | Loss: 0.00002595
Iteration 133/1000 | Loss: 0.00002595
Iteration 134/1000 | Loss: 0.00002595
Iteration 135/1000 | Loss: 0.00002595
Iteration 136/1000 | Loss: 0.00002595
Iteration 137/1000 | Loss: 0.00002594
Iteration 138/1000 | Loss: 0.00002594
Iteration 139/1000 | Loss: 0.00002594
Iteration 140/1000 | Loss: 0.00002594
Iteration 141/1000 | Loss: 0.00002594
Iteration 142/1000 | Loss: 0.00002594
Iteration 143/1000 | Loss: 0.00002594
Iteration 144/1000 | Loss: 0.00002594
Iteration 145/1000 | Loss: 0.00002594
Iteration 146/1000 | Loss: 0.00002594
Iteration 147/1000 | Loss: 0.00002594
Iteration 148/1000 | Loss: 0.00002594
Iteration 149/1000 | Loss: 0.00002594
Iteration 150/1000 | Loss: 0.00002594
Iteration 151/1000 | Loss: 0.00002594
Iteration 152/1000 | Loss: 0.00002594
Iteration 153/1000 | Loss: 0.00002594
Iteration 154/1000 | Loss: 0.00002594
Iteration 155/1000 | Loss: 0.00002594
Iteration 156/1000 | Loss: 0.00002594
Iteration 157/1000 | Loss: 0.00002594
Iteration 158/1000 | Loss: 0.00002594
Iteration 159/1000 | Loss: 0.00002594
Iteration 160/1000 | Loss: 0.00002594
Iteration 161/1000 | Loss: 0.00002594
Iteration 162/1000 | Loss: 0.00002594
Iteration 163/1000 | Loss: 0.00002594
Iteration 164/1000 | Loss: 0.00002594
Iteration 165/1000 | Loss: 0.00002594
Iteration 166/1000 | Loss: 0.00002594
Iteration 167/1000 | Loss: 0.00002594
Iteration 168/1000 | Loss: 0.00002594
Iteration 169/1000 | Loss: 0.00002594
Iteration 170/1000 | Loss: 0.00002594
Iteration 171/1000 | Loss: 0.00002594
Iteration 172/1000 | Loss: 0.00002594
Iteration 173/1000 | Loss: 0.00002594
Iteration 174/1000 | Loss: 0.00002594
Iteration 175/1000 | Loss: 0.00002594
Iteration 176/1000 | Loss: 0.00002594
Iteration 177/1000 | Loss: 0.00002594
Iteration 178/1000 | Loss: 0.00002594
Iteration 179/1000 | Loss: 0.00002594
Iteration 180/1000 | Loss: 0.00002594
Iteration 181/1000 | Loss: 0.00002594
Iteration 182/1000 | Loss: 0.00002594
Iteration 183/1000 | Loss: 0.00002594
Iteration 184/1000 | Loss: 0.00002594
Iteration 185/1000 | Loss: 0.00002594
Iteration 186/1000 | Loss: 0.00002594
Iteration 187/1000 | Loss: 0.00002594
Iteration 188/1000 | Loss: 0.00002594
Iteration 189/1000 | Loss: 0.00002594
Iteration 190/1000 | Loss: 0.00002594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.5936666133929975e-05, 2.5936666133929975e-05, 2.5936666133929975e-05, 2.5936666133929975e-05, 2.5936666133929975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5936666133929975e-05

Optimization complete. Final v2v error: 4.198453426361084 mm

Highest mean error: 6.596127986907959 mm for frame 98

Lowest mean error: 3.4778478145599365 mm for frame 134

Saving results

Total time: 206.74832224845886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811138
Iteration 2/25 | Loss: 0.00071788
Iteration 3/25 | Loss: 0.00061851
Iteration 4/25 | Loss: 0.00059982
Iteration 5/25 | Loss: 0.00059527
Iteration 6/25 | Loss: 0.00059421
Iteration 7/25 | Loss: 0.00059387
Iteration 8/25 | Loss: 0.00059387
Iteration 9/25 | Loss: 0.00059387
Iteration 10/25 | Loss: 0.00059387
Iteration 11/25 | Loss: 0.00059387
Iteration 12/25 | Loss: 0.00059387
Iteration 13/25 | Loss: 0.00059387
Iteration 14/25 | Loss: 0.00059387
Iteration 15/25 | Loss: 0.00059387
Iteration 16/25 | Loss: 0.00059387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000593869888689369, 0.000593869888689369, 0.000593869888689369, 0.000593869888689369, 0.000593869888689369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000593869888689369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50077939
Iteration 2/25 | Loss: 0.00028548
Iteration 3/25 | Loss: 0.00028548
Iteration 4/25 | Loss: 0.00028548
Iteration 5/25 | Loss: 0.00028548
Iteration 6/25 | Loss: 0.00028548
Iteration 7/25 | Loss: 0.00028548
Iteration 8/25 | Loss: 0.00028547
Iteration 9/25 | Loss: 0.00028547
Iteration 10/25 | Loss: 0.00028547
Iteration 11/25 | Loss: 0.00028547
Iteration 12/25 | Loss: 0.00028547
Iteration 13/25 | Loss: 0.00028547
Iteration 14/25 | Loss: 0.00028547
Iteration 15/25 | Loss: 0.00028547
Iteration 16/25 | Loss: 0.00028547
Iteration 17/25 | Loss: 0.00028547
Iteration 18/25 | Loss: 0.00028547
Iteration 19/25 | Loss: 0.00028547
Iteration 20/25 | Loss: 0.00028547
Iteration 21/25 | Loss: 0.00028547
Iteration 22/25 | Loss: 0.00028547
Iteration 23/25 | Loss: 0.00028547
Iteration 24/25 | Loss: 0.00028547
Iteration 25/25 | Loss: 0.00028547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0002854744088836014, 0.0002854744088836014, 0.0002854744088836014, 0.0002854744088836014, 0.0002854744088836014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002854744088836014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028547
Iteration 2/1000 | Loss: 0.00002653
Iteration 3/1000 | Loss: 0.00001882
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001435
Iteration 6/1000 | Loss: 0.00001382
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001319
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00001298
Iteration 14/1000 | Loss: 0.00001298
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001287
Iteration 20/1000 | Loss: 0.00001286
Iteration 21/1000 | Loss: 0.00001286
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001285
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001284
Iteration 27/1000 | Loss: 0.00001283
Iteration 28/1000 | Loss: 0.00001283
Iteration 29/1000 | Loss: 0.00001283
Iteration 30/1000 | Loss: 0.00001283
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001283
Iteration 34/1000 | Loss: 0.00001282
Iteration 35/1000 | Loss: 0.00001282
Iteration 36/1000 | Loss: 0.00001281
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001279
Iteration 40/1000 | Loss: 0.00001279
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001278
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001277
Iteration 45/1000 | Loss: 0.00001277
Iteration 46/1000 | Loss: 0.00001276
Iteration 47/1000 | Loss: 0.00001276
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001275
Iteration 50/1000 | Loss: 0.00001275
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001272
Iteration 63/1000 | Loss: 0.00001272
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001271
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001269
Iteration 69/1000 | Loss: 0.00001269
Iteration 70/1000 | Loss: 0.00001269
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001269
Iteration 77/1000 | Loss: 0.00001269
Iteration 78/1000 | Loss: 0.00001269
Iteration 79/1000 | Loss: 0.00001269
Iteration 80/1000 | Loss: 0.00001269
Iteration 81/1000 | Loss: 0.00001269
Iteration 82/1000 | Loss: 0.00001269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.2688502465607598e-05, 1.2688502465607598e-05, 1.2688502465607598e-05, 1.2688502465607598e-05, 1.2688502465607598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2688502465607598e-05

Optimization complete. Final v2v error: 3.0413756370544434 mm

Highest mean error: 3.181943655014038 mm for frame 113

Lowest mean error: 2.9142704010009766 mm for frame 43

Saving results

Total time: 27.759759664535522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107425
Iteration 2/25 | Loss: 0.00206112
Iteration 3/25 | Loss: 0.00166183
Iteration 4/25 | Loss: 0.00135121
Iteration 5/25 | Loss: 0.00104873
Iteration 6/25 | Loss: 0.00087414
Iteration 7/25 | Loss: 0.00082028
Iteration 8/25 | Loss: 0.00082278
Iteration 9/25 | Loss: 0.00079579
Iteration 10/25 | Loss: 0.00078311
Iteration 11/25 | Loss: 0.00077982
Iteration 12/25 | Loss: 0.00077609
Iteration 13/25 | Loss: 0.00077130
Iteration 14/25 | Loss: 0.00075890
Iteration 15/25 | Loss: 0.00076144
Iteration 16/25 | Loss: 0.00076696
Iteration 17/25 | Loss: 0.00075734
Iteration 18/25 | Loss: 0.00075823
Iteration 19/25 | Loss: 0.00075836
Iteration 20/25 | Loss: 0.00075697
Iteration 21/25 | Loss: 0.00075561
Iteration 22/25 | Loss: 0.00075563
Iteration 23/25 | Loss: 0.00075373
Iteration 24/25 | Loss: 0.00075228
Iteration 25/25 | Loss: 0.00075388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44935131
Iteration 2/25 | Loss: 0.00041493
Iteration 3/25 | Loss: 0.00041493
Iteration 4/25 | Loss: 0.00041493
Iteration 5/25 | Loss: 0.00041493
Iteration 6/25 | Loss: 0.00041493
Iteration 7/25 | Loss: 0.00041493
Iteration 8/25 | Loss: 0.00041493
Iteration 9/25 | Loss: 0.00041493
Iteration 10/25 | Loss: 0.00041493
Iteration 11/25 | Loss: 0.00041492
Iteration 12/25 | Loss: 0.00041492
Iteration 13/25 | Loss: 0.00041492
Iteration 14/25 | Loss: 0.00041492
Iteration 15/25 | Loss: 0.00041492
Iteration 16/25 | Loss: 0.00041492
Iteration 17/25 | Loss: 0.00041492
Iteration 18/25 | Loss: 0.00041492
Iteration 19/25 | Loss: 0.00041492
Iteration 20/25 | Loss: 0.00041492
Iteration 21/25 | Loss: 0.00041492
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004149249289184809, 0.0004149249289184809, 0.0004149249289184809, 0.0004149249289184809, 0.0004149249289184809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004149249289184809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041492
Iteration 2/1000 | Loss: 0.00252973
Iteration 3/1000 | Loss: 0.00025956
Iteration 4/1000 | Loss: 0.00016019
Iteration 5/1000 | Loss: 0.00012518
Iteration 6/1000 | Loss: 0.00011640
Iteration 7/1000 | Loss: 0.00025425
Iteration 8/1000 | Loss: 0.00014493
Iteration 9/1000 | Loss: 0.00027747
Iteration 10/1000 | Loss: 0.00115443
Iteration 11/1000 | Loss: 0.00060584
Iteration 12/1000 | Loss: 0.00108807
Iteration 13/1000 | Loss: 0.00011504
Iteration 14/1000 | Loss: 0.00008739
Iteration 15/1000 | Loss: 0.00006735
Iteration 16/1000 | Loss: 0.00006590
Iteration 17/1000 | Loss: 0.00009155
Iteration 18/1000 | Loss: 0.00009437
Iteration 19/1000 | Loss: 0.00006684
Iteration 20/1000 | Loss: 0.00009140
Iteration 21/1000 | Loss: 0.00007695
Iteration 22/1000 | Loss: 0.00012794
Iteration 23/1000 | Loss: 0.00003439
Iteration 24/1000 | Loss: 0.00002710
Iteration 25/1000 | Loss: 0.00002599
Iteration 26/1000 | Loss: 0.00002515
Iteration 27/1000 | Loss: 0.00002469
Iteration 28/1000 | Loss: 0.00002422
Iteration 29/1000 | Loss: 0.00002376
Iteration 30/1000 | Loss: 0.00002342
Iteration 31/1000 | Loss: 0.00016475
Iteration 32/1000 | Loss: 0.00002706
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002200
Iteration 35/1000 | Loss: 0.00002137
Iteration 36/1000 | Loss: 0.00002080
Iteration 37/1000 | Loss: 0.00002051
Iteration 38/1000 | Loss: 0.00002049
Iteration 39/1000 | Loss: 0.00002043
Iteration 40/1000 | Loss: 0.00002038
Iteration 41/1000 | Loss: 0.00002037
Iteration 42/1000 | Loss: 0.00002028
Iteration 43/1000 | Loss: 0.00002019
Iteration 44/1000 | Loss: 0.00002019
Iteration 45/1000 | Loss: 0.00002018
Iteration 46/1000 | Loss: 0.00002018
Iteration 47/1000 | Loss: 0.00002018
Iteration 48/1000 | Loss: 0.00002017
Iteration 49/1000 | Loss: 0.00002017
Iteration 50/1000 | Loss: 0.00002016
Iteration 51/1000 | Loss: 0.00002016
Iteration 52/1000 | Loss: 0.00002015
Iteration 53/1000 | Loss: 0.00002015
Iteration 54/1000 | Loss: 0.00002015
Iteration 55/1000 | Loss: 0.00002014
Iteration 56/1000 | Loss: 0.00002014
Iteration 57/1000 | Loss: 0.00002014
Iteration 58/1000 | Loss: 0.00002013
Iteration 59/1000 | Loss: 0.00002013
Iteration 60/1000 | Loss: 0.00002013
Iteration 61/1000 | Loss: 0.00002012
Iteration 62/1000 | Loss: 0.00002012
Iteration 63/1000 | Loss: 0.00002012
Iteration 64/1000 | Loss: 0.00002012
Iteration 65/1000 | Loss: 0.00002012
Iteration 66/1000 | Loss: 0.00002011
Iteration 67/1000 | Loss: 0.00002011
Iteration 68/1000 | Loss: 0.00002011
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00002011
Iteration 71/1000 | Loss: 0.00002011
Iteration 72/1000 | Loss: 0.00002010
Iteration 73/1000 | Loss: 0.00002010
Iteration 74/1000 | Loss: 0.00002010
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002008
Iteration 79/1000 | Loss: 0.00002008
Iteration 80/1000 | Loss: 0.00002008
Iteration 81/1000 | Loss: 0.00002008
Iteration 82/1000 | Loss: 0.00002008
Iteration 83/1000 | Loss: 0.00002008
Iteration 84/1000 | Loss: 0.00002008
Iteration 85/1000 | Loss: 0.00002008
Iteration 86/1000 | Loss: 0.00002007
Iteration 87/1000 | Loss: 0.00002007
Iteration 88/1000 | Loss: 0.00002007
Iteration 89/1000 | Loss: 0.00002007
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002007
Iteration 92/1000 | Loss: 0.00002007
Iteration 93/1000 | Loss: 0.00002007
Iteration 94/1000 | Loss: 0.00002007
Iteration 95/1000 | Loss: 0.00002007
Iteration 96/1000 | Loss: 0.00002007
Iteration 97/1000 | Loss: 0.00002007
Iteration 98/1000 | Loss: 0.00002007
Iteration 99/1000 | Loss: 0.00002007
Iteration 100/1000 | Loss: 0.00002007
Iteration 101/1000 | Loss: 0.00002007
Iteration 102/1000 | Loss: 0.00002007
Iteration 103/1000 | Loss: 0.00002007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.007251350732986e-05, 2.007251350732986e-05, 2.007251350732986e-05, 2.007251350732986e-05, 2.007251350732986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.007251350732986e-05

Optimization complete. Final v2v error: 3.6676385402679443 mm

Highest mean error: 9.164151191711426 mm for frame 135

Lowest mean error: 3.1101720333099365 mm for frame 7

Saving results

Total time: 105.85603761672974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043642
Iteration 2/25 | Loss: 0.00247510
Iteration 3/25 | Loss: 0.00196154
Iteration 4/25 | Loss: 0.00191628
Iteration 5/25 | Loss: 0.00171652
Iteration 6/25 | Loss: 0.00162269
Iteration 7/25 | Loss: 0.00159358
Iteration 8/25 | Loss: 0.00153372
Iteration 9/25 | Loss: 0.00149398
Iteration 10/25 | Loss: 0.00146464
Iteration 11/25 | Loss: 0.00139221
Iteration 12/25 | Loss: 0.00139076
Iteration 13/25 | Loss: 0.00136999
Iteration 14/25 | Loss: 0.00135428
Iteration 15/25 | Loss: 0.00135023
Iteration 16/25 | Loss: 0.00136374
Iteration 17/25 | Loss: 0.00134513
Iteration 18/25 | Loss: 0.00123952
Iteration 19/25 | Loss: 0.00120276
Iteration 20/25 | Loss: 0.00116408
Iteration 21/25 | Loss: 0.00113099
Iteration 22/25 | Loss: 0.00111820
Iteration 23/25 | Loss: 0.00111070
Iteration 24/25 | Loss: 0.00110712
Iteration 25/25 | Loss: 0.00110089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43609023
Iteration 2/25 | Loss: 0.01301689
Iteration 3/25 | Loss: 0.00278474
Iteration 4/25 | Loss: 0.00261378
Iteration 5/25 | Loss: 0.00261378
Iteration 6/25 | Loss: 0.00261378
Iteration 7/25 | Loss: 0.00261378
Iteration 8/25 | Loss: 0.00261378
Iteration 9/25 | Loss: 0.00261378
Iteration 10/25 | Loss: 0.00261378
Iteration 11/25 | Loss: 0.00261378
Iteration 12/25 | Loss: 0.00261378
Iteration 13/25 | Loss: 0.00261378
Iteration 14/25 | Loss: 0.00261378
Iteration 15/25 | Loss: 0.00261378
Iteration 16/25 | Loss: 0.00261378
Iteration 17/25 | Loss: 0.00261378
Iteration 18/25 | Loss: 0.00261378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002613778691738844, 0.002613778691738844, 0.002613778691738844, 0.002613778691738844, 0.002613778691738844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002613778691738844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261378
Iteration 2/1000 | Loss: 0.01541008
Iteration 3/1000 | Loss: 0.00130645
Iteration 4/1000 | Loss: 0.00140922
Iteration 5/1000 | Loss: 0.00163523
Iteration 6/1000 | Loss: 0.00116554
Iteration 7/1000 | Loss: 0.00147856
Iteration 8/1000 | Loss: 0.00137360
Iteration 9/1000 | Loss: 0.00168923
Iteration 10/1000 | Loss: 0.00123016
Iteration 11/1000 | Loss: 0.00183281
Iteration 12/1000 | Loss: 0.00178485
Iteration 13/1000 | Loss: 0.00230963
Iteration 14/1000 | Loss: 0.00058248
Iteration 15/1000 | Loss: 0.00094194
Iteration 16/1000 | Loss: 0.00074399
Iteration 17/1000 | Loss: 0.00047477
Iteration 18/1000 | Loss: 0.00047183
Iteration 19/1000 | Loss: 0.00043396
Iteration 20/1000 | Loss: 0.00039665
Iteration 21/1000 | Loss: 0.00060411
Iteration 22/1000 | Loss: 0.00037738
Iteration 23/1000 | Loss: 0.00133318
Iteration 24/1000 | Loss: 0.00009837
Iteration 25/1000 | Loss: 0.00011351
Iteration 26/1000 | Loss: 0.00175069
Iteration 27/1000 | Loss: 0.00564593
Iteration 28/1000 | Loss: 0.00380650
Iteration 29/1000 | Loss: 0.00489614
Iteration 30/1000 | Loss: 0.00066303
Iteration 31/1000 | Loss: 0.00025662
Iteration 32/1000 | Loss: 0.00050266
Iteration 33/1000 | Loss: 0.00091043
Iteration 34/1000 | Loss: 0.00014120
Iteration 35/1000 | Loss: 0.00011183
Iteration 36/1000 | Loss: 0.00010708
Iteration 37/1000 | Loss: 0.00014818
Iteration 38/1000 | Loss: 0.00015633
Iteration 39/1000 | Loss: 0.00010660
Iteration 40/1000 | Loss: 0.00017337
Iteration 41/1000 | Loss: 0.00005176
Iteration 42/1000 | Loss: 0.00004493
Iteration 43/1000 | Loss: 0.00013131
Iteration 44/1000 | Loss: 0.00008398
Iteration 45/1000 | Loss: 0.00029295
Iteration 46/1000 | Loss: 0.00004563
Iteration 47/1000 | Loss: 0.00003916
Iteration 48/1000 | Loss: 0.00041484
Iteration 49/1000 | Loss: 0.00075768
Iteration 50/1000 | Loss: 0.00005164
Iteration 51/1000 | Loss: 0.00004188
Iteration 52/1000 | Loss: 0.00015821
Iteration 53/1000 | Loss: 0.00032301
Iteration 54/1000 | Loss: 0.00023939
Iteration 55/1000 | Loss: 0.00019448
Iteration 56/1000 | Loss: 0.00038750
Iteration 57/1000 | Loss: 0.00039984
Iteration 58/1000 | Loss: 0.00025659
Iteration 59/1000 | Loss: 0.00019903
Iteration 60/1000 | Loss: 0.00028271
Iteration 61/1000 | Loss: 0.00026479
Iteration 62/1000 | Loss: 0.00028964
Iteration 63/1000 | Loss: 0.00043834
Iteration 64/1000 | Loss: 0.00009065
Iteration 65/1000 | Loss: 0.00004300
Iteration 66/1000 | Loss: 0.00003117
Iteration 67/1000 | Loss: 0.00015720
Iteration 68/1000 | Loss: 0.00002887
Iteration 69/1000 | Loss: 0.00004305
Iteration 70/1000 | Loss: 0.00017022
Iteration 71/1000 | Loss: 0.00004776
Iteration 72/1000 | Loss: 0.00003755
Iteration 73/1000 | Loss: 0.00002631
Iteration 74/1000 | Loss: 0.00003995
Iteration 75/1000 | Loss: 0.00003017
Iteration 76/1000 | Loss: 0.00002786
Iteration 77/1000 | Loss: 0.00007056
Iteration 78/1000 | Loss: 0.00005283
Iteration 79/1000 | Loss: 0.00005355
Iteration 80/1000 | Loss: 0.00003695
Iteration 81/1000 | Loss: 0.00003752
Iteration 82/1000 | Loss: 0.00003620
Iteration 83/1000 | Loss: 0.00004383
Iteration 84/1000 | Loss: 0.00004921
Iteration 85/1000 | Loss: 0.00004074
Iteration 86/1000 | Loss: 0.00003548
Iteration 87/1000 | Loss: 0.00003847
Iteration 88/1000 | Loss: 0.00003481
Iteration 89/1000 | Loss: 0.00003917
Iteration 90/1000 | Loss: 0.00070923
Iteration 91/1000 | Loss: 0.00009566
Iteration 92/1000 | Loss: 0.00003826
Iteration 93/1000 | Loss: 0.00004281
Iteration 94/1000 | Loss: 0.00008882
Iteration 95/1000 | Loss: 0.00004845
Iteration 96/1000 | Loss: 0.00006490
Iteration 97/1000 | Loss: 0.00004503
Iteration 98/1000 | Loss: 0.00011686
Iteration 99/1000 | Loss: 0.00021581
Iteration 100/1000 | Loss: 0.00008832
Iteration 101/1000 | Loss: 0.00006730
Iteration 102/1000 | Loss: 0.00005342
Iteration 103/1000 | Loss: 0.00004769
Iteration 104/1000 | Loss: 0.00018767
Iteration 105/1000 | Loss: 0.00010429
Iteration 106/1000 | Loss: 0.00009967
Iteration 107/1000 | Loss: 0.00003855
Iteration 108/1000 | Loss: 0.00004705
Iteration 109/1000 | Loss: 0.00004273
Iteration 110/1000 | Loss: 0.00003756
Iteration 111/1000 | Loss: 0.00003663
Iteration 112/1000 | Loss: 0.00003561
Iteration 113/1000 | Loss: 0.00003461
Iteration 114/1000 | Loss: 0.00004097
Iteration 115/1000 | Loss: 0.00002455
Iteration 116/1000 | Loss: 0.00002355
Iteration 117/1000 | Loss: 0.00002320
Iteration 118/1000 | Loss: 0.00002283
Iteration 119/1000 | Loss: 0.00002266
Iteration 120/1000 | Loss: 0.00002265
Iteration 121/1000 | Loss: 0.00002264
Iteration 122/1000 | Loss: 0.00002263
Iteration 123/1000 | Loss: 0.00002262
Iteration 124/1000 | Loss: 0.00002261
Iteration 125/1000 | Loss: 0.00002258
Iteration 126/1000 | Loss: 0.00002258
Iteration 127/1000 | Loss: 0.00002257
Iteration 128/1000 | Loss: 0.00002257
Iteration 129/1000 | Loss: 0.00002256
Iteration 130/1000 | Loss: 0.00002256
Iteration 131/1000 | Loss: 0.00002255
Iteration 132/1000 | Loss: 0.00002253
Iteration 133/1000 | Loss: 0.00002252
Iteration 134/1000 | Loss: 0.00002252
Iteration 135/1000 | Loss: 0.00002252
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002250
Iteration 139/1000 | Loss: 0.00002248
Iteration 140/1000 | Loss: 0.00002238
Iteration 141/1000 | Loss: 0.00002237
Iteration 142/1000 | Loss: 0.00002236
Iteration 143/1000 | Loss: 0.00002236
Iteration 144/1000 | Loss: 0.00017814
Iteration 145/1000 | Loss: 0.00010562
Iteration 146/1000 | Loss: 0.00003712
Iteration 147/1000 | Loss: 0.00003555
Iteration 148/1000 | Loss: 0.00002961
Iteration 149/1000 | Loss: 0.00011682
Iteration 150/1000 | Loss: 0.00005551
Iteration 151/1000 | Loss: 0.00002626
Iteration 152/1000 | Loss: 0.00002498
Iteration 153/1000 | Loss: 0.00024873
Iteration 154/1000 | Loss: 0.00002343
Iteration 155/1000 | Loss: 0.00002304
Iteration 156/1000 | Loss: 0.00002263
Iteration 157/1000 | Loss: 0.00015443
Iteration 158/1000 | Loss: 0.00004194
Iteration 159/1000 | Loss: 0.00013274
Iteration 160/1000 | Loss: 0.00002296
Iteration 161/1000 | Loss: 0.00002217
Iteration 162/1000 | Loss: 0.00007083
Iteration 163/1000 | Loss: 0.00002289
Iteration 164/1000 | Loss: 0.00010210
Iteration 165/1000 | Loss: 0.00002656
Iteration 166/1000 | Loss: 0.00005104
Iteration 167/1000 | Loss: 0.00002699
Iteration 168/1000 | Loss: 0.00011012
Iteration 169/1000 | Loss: 0.00009401
Iteration 170/1000 | Loss: 0.00008077
Iteration 171/1000 | Loss: 0.00011809
Iteration 172/1000 | Loss: 0.00002186
Iteration 173/1000 | Loss: 0.00002156
Iteration 174/1000 | Loss: 0.00002148
Iteration 175/1000 | Loss: 0.00002146
Iteration 176/1000 | Loss: 0.00002145
Iteration 177/1000 | Loss: 0.00002145
Iteration 178/1000 | Loss: 0.00002139
Iteration 179/1000 | Loss: 0.00002137
Iteration 180/1000 | Loss: 0.00002136
Iteration 181/1000 | Loss: 0.00002135
Iteration 182/1000 | Loss: 0.00002135
Iteration 183/1000 | Loss: 0.00002134
Iteration 184/1000 | Loss: 0.00002134
Iteration 185/1000 | Loss: 0.00002134
Iteration 186/1000 | Loss: 0.00002133
Iteration 187/1000 | Loss: 0.00002133
Iteration 188/1000 | Loss: 0.00002133
Iteration 189/1000 | Loss: 0.00002133
Iteration 190/1000 | Loss: 0.00002133
Iteration 191/1000 | Loss: 0.00002132
Iteration 192/1000 | Loss: 0.00002132
Iteration 193/1000 | Loss: 0.00002132
Iteration 194/1000 | Loss: 0.00002132
Iteration 195/1000 | Loss: 0.00002131
Iteration 196/1000 | Loss: 0.00002131
Iteration 197/1000 | Loss: 0.00002130
Iteration 198/1000 | Loss: 0.00002130
Iteration 199/1000 | Loss: 0.00002130
Iteration 200/1000 | Loss: 0.00002129
Iteration 201/1000 | Loss: 0.00002129
Iteration 202/1000 | Loss: 0.00002129
Iteration 203/1000 | Loss: 0.00002129
Iteration 204/1000 | Loss: 0.00002128
Iteration 205/1000 | Loss: 0.00002128
Iteration 206/1000 | Loss: 0.00002128
Iteration 207/1000 | Loss: 0.00015224
Iteration 208/1000 | Loss: 0.00002670
Iteration 209/1000 | Loss: 0.00010704
Iteration 210/1000 | Loss: 0.00002140
Iteration 211/1000 | Loss: 0.00006715
Iteration 212/1000 | Loss: 0.00002833
Iteration 213/1000 | Loss: 0.00003273
Iteration 214/1000 | Loss: 0.00002132
Iteration 215/1000 | Loss: 0.00002130
Iteration 216/1000 | Loss: 0.00002128
Iteration 217/1000 | Loss: 0.00002127
Iteration 218/1000 | Loss: 0.00002127
Iteration 219/1000 | Loss: 0.00002127
Iteration 220/1000 | Loss: 0.00002126
Iteration 221/1000 | Loss: 0.00002126
Iteration 222/1000 | Loss: 0.00002126
Iteration 223/1000 | Loss: 0.00002124
Iteration 224/1000 | Loss: 0.00002124
Iteration 225/1000 | Loss: 0.00002124
Iteration 226/1000 | Loss: 0.00002123
Iteration 227/1000 | Loss: 0.00002123
Iteration 228/1000 | Loss: 0.00002123
Iteration 229/1000 | Loss: 0.00002123
Iteration 230/1000 | Loss: 0.00002123
Iteration 231/1000 | Loss: 0.00002123
Iteration 232/1000 | Loss: 0.00002123
Iteration 233/1000 | Loss: 0.00002123
Iteration 234/1000 | Loss: 0.00002123
Iteration 235/1000 | Loss: 0.00002122
Iteration 236/1000 | Loss: 0.00002122
Iteration 237/1000 | Loss: 0.00002122
Iteration 238/1000 | Loss: 0.00002122
Iteration 239/1000 | Loss: 0.00002122
Iteration 240/1000 | Loss: 0.00002122
Iteration 241/1000 | Loss: 0.00002122
Iteration 242/1000 | Loss: 0.00002122
Iteration 243/1000 | Loss: 0.00002121
Iteration 244/1000 | Loss: 0.00002121
Iteration 245/1000 | Loss: 0.00002121
Iteration 246/1000 | Loss: 0.00002121
Iteration 247/1000 | Loss: 0.00002121
Iteration 248/1000 | Loss: 0.00002121
Iteration 249/1000 | Loss: 0.00002121
Iteration 250/1000 | Loss: 0.00002121
Iteration 251/1000 | Loss: 0.00002121
Iteration 252/1000 | Loss: 0.00002121
Iteration 253/1000 | Loss: 0.00002121
Iteration 254/1000 | Loss: 0.00002121
Iteration 255/1000 | Loss: 0.00002121
Iteration 256/1000 | Loss: 0.00002121
Iteration 257/1000 | Loss: 0.00002121
Iteration 258/1000 | Loss: 0.00002121
Iteration 259/1000 | Loss: 0.00002121
Iteration 260/1000 | Loss: 0.00002121
Iteration 261/1000 | Loss: 0.00002121
Iteration 262/1000 | Loss: 0.00002121
Iteration 263/1000 | Loss: 0.00002121
Iteration 264/1000 | Loss: 0.00002121
Iteration 265/1000 | Loss: 0.00002121
Iteration 266/1000 | Loss: 0.00002121
Iteration 267/1000 | Loss: 0.00002121
Iteration 268/1000 | Loss: 0.00002121
Iteration 269/1000 | Loss: 0.00002121
Iteration 270/1000 | Loss: 0.00002121
Iteration 271/1000 | Loss: 0.00002121
Iteration 272/1000 | Loss: 0.00002121
Iteration 273/1000 | Loss: 0.00002121
Iteration 274/1000 | Loss: 0.00002121
Iteration 275/1000 | Loss: 0.00002121
Iteration 276/1000 | Loss: 0.00002121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [2.120538556482643e-05, 2.120538556482643e-05, 2.120538556482643e-05, 2.120538556482643e-05, 2.120538556482643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.120538556482643e-05

Optimization complete. Final v2v error: 3.856036424636841 mm

Highest mean error: 4.890820026397705 mm for frame 238

Lowest mean error: 3.20888614654541 mm for frame 38

Saving results

Total time: 307.32002878189087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064970
Iteration 2/25 | Loss: 0.00309806
Iteration 3/25 | Loss: 0.00227465
Iteration 4/25 | Loss: 0.00195759
Iteration 5/25 | Loss: 0.00178326
Iteration 6/25 | Loss: 0.00163996
Iteration 7/25 | Loss: 0.00153487
Iteration 8/25 | Loss: 0.00139913
Iteration 9/25 | Loss: 0.00139621
Iteration 10/25 | Loss: 0.00133413
Iteration 11/25 | Loss: 0.00120181
Iteration 12/25 | Loss: 0.00118221
Iteration 13/25 | Loss: 0.00112389
Iteration 14/25 | Loss: 0.00109760
Iteration 15/25 | Loss: 0.00107670
Iteration 16/25 | Loss: 0.00106357
Iteration 17/25 | Loss: 0.00106911
Iteration 18/25 | Loss: 0.00104438
Iteration 19/25 | Loss: 0.00103372
Iteration 20/25 | Loss: 0.00102730
Iteration 21/25 | Loss: 0.00102724
Iteration 22/25 | Loss: 0.00102542
Iteration 23/25 | Loss: 0.00102191
Iteration 24/25 | Loss: 0.00101977
Iteration 25/25 | Loss: 0.00102563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43234146
Iteration 2/25 | Loss: 0.00638433
Iteration 3/25 | Loss: 0.00428567
Iteration 4/25 | Loss: 0.00428420
Iteration 5/25 | Loss: 0.00428420
Iteration 6/25 | Loss: 0.00428420
Iteration 7/25 | Loss: 0.00428420
Iteration 8/25 | Loss: 0.00428420
Iteration 9/25 | Loss: 0.00428420
Iteration 10/25 | Loss: 0.00428420
Iteration 11/25 | Loss: 0.00428420
Iteration 12/25 | Loss: 0.00428420
Iteration 13/25 | Loss: 0.00428420
Iteration 14/25 | Loss: 0.00428420
Iteration 15/25 | Loss: 0.00428420
Iteration 16/25 | Loss: 0.00428420
Iteration 17/25 | Loss: 0.00428420
Iteration 18/25 | Loss: 0.00428420
Iteration 19/25 | Loss: 0.00428420
Iteration 20/25 | Loss: 0.00428420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004284200258553028, 0.004284200258553028, 0.004284200258553028, 0.004284200258553028, 0.004284200258553028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004284200258553028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00428420
Iteration 2/1000 | Loss: 0.00107496
Iteration 3/1000 | Loss: 0.00119841
Iteration 4/1000 | Loss: 0.00080074
Iteration 5/1000 | Loss: 0.00066153
Iteration 6/1000 | Loss: 0.00049924
Iteration 7/1000 | Loss: 0.00154375
Iteration 8/1000 | Loss: 0.00313618
Iteration 9/1000 | Loss: 0.00252198
Iteration 10/1000 | Loss: 0.00147273
Iteration 11/1000 | Loss: 0.00029784
Iteration 12/1000 | Loss: 0.00069347
Iteration 13/1000 | Loss: 0.00128044
Iteration 14/1000 | Loss: 0.00047482
Iteration 15/1000 | Loss: 0.00043581
Iteration 16/1000 | Loss: 0.00034660
Iteration 17/1000 | Loss: 0.00029716
Iteration 18/1000 | Loss: 0.00125337
Iteration 19/1000 | Loss: 0.00237468
Iteration 20/1000 | Loss: 0.00049593
Iteration 21/1000 | Loss: 0.00050781
Iteration 22/1000 | Loss: 0.00030689
Iteration 23/1000 | Loss: 0.00022325
Iteration 24/1000 | Loss: 0.00021414
Iteration 25/1000 | Loss: 0.00047808
Iteration 26/1000 | Loss: 0.00020571
Iteration 27/1000 | Loss: 0.00105552
Iteration 28/1000 | Loss: 0.00044989
Iteration 29/1000 | Loss: 0.00041963
Iteration 30/1000 | Loss: 0.00020503
Iteration 31/1000 | Loss: 0.00068119
Iteration 32/1000 | Loss: 0.00043617
Iteration 33/1000 | Loss: 0.00052405
Iteration 34/1000 | Loss: 0.00019487
Iteration 35/1000 | Loss: 0.00019221
Iteration 36/1000 | Loss: 0.00019042
Iteration 37/1000 | Loss: 0.00030821
Iteration 38/1000 | Loss: 0.00186790
Iteration 39/1000 | Loss: 0.00351479
Iteration 40/1000 | Loss: 0.00511393
Iteration 41/1000 | Loss: 0.00356134
Iteration 42/1000 | Loss: 0.00394978
Iteration 43/1000 | Loss: 0.00573017
Iteration 44/1000 | Loss: 0.00227031
Iteration 45/1000 | Loss: 0.00044181
Iteration 46/1000 | Loss: 0.00063438
Iteration 47/1000 | Loss: 0.00021219
Iteration 48/1000 | Loss: 0.00019194
Iteration 49/1000 | Loss: 0.00018657
Iteration 50/1000 | Loss: 0.00032770
Iteration 51/1000 | Loss: 0.00018114
Iteration 52/1000 | Loss: 0.00017934
Iteration 53/1000 | Loss: 0.00017779
Iteration 54/1000 | Loss: 0.00017576
Iteration 55/1000 | Loss: 0.00052332
Iteration 56/1000 | Loss: 0.00017306
Iteration 57/1000 | Loss: 0.00017102
Iteration 58/1000 | Loss: 0.00045317
Iteration 59/1000 | Loss: 0.00031004
Iteration 60/1000 | Loss: 0.00017617
Iteration 61/1000 | Loss: 0.00016906
Iteration 62/1000 | Loss: 0.00040869
Iteration 63/1000 | Loss: 0.00032922
Iteration 64/1000 | Loss: 0.00019107
Iteration 65/1000 | Loss: 0.00015530
Iteration 66/1000 | Loss: 0.00034429
Iteration 67/1000 | Loss: 0.00033474
Iteration 68/1000 | Loss: 0.00014785
Iteration 69/1000 | Loss: 0.00068444
Iteration 70/1000 | Loss: 0.00080620
Iteration 71/1000 | Loss: 0.00021848
Iteration 72/1000 | Loss: 0.00014305
Iteration 73/1000 | Loss: 0.00014424
Iteration 74/1000 | Loss: 0.00036691
Iteration 75/1000 | Loss: 0.00018998
Iteration 76/1000 | Loss: 0.00013930
Iteration 77/1000 | Loss: 0.00015534
Iteration 78/1000 | Loss: 0.00052098
Iteration 79/1000 | Loss: 0.00016552
Iteration 80/1000 | Loss: 0.00012542
Iteration 81/1000 | Loss: 0.00037895
Iteration 82/1000 | Loss: 0.00057027
Iteration 83/1000 | Loss: 0.00013254
Iteration 84/1000 | Loss: 0.00012244
Iteration 85/1000 | Loss: 0.00011623
Iteration 86/1000 | Loss: 0.00011147
Iteration 87/1000 | Loss: 0.00010903
Iteration 88/1000 | Loss: 0.00010698
Iteration 89/1000 | Loss: 0.00010575
Iteration 90/1000 | Loss: 0.00030161
Iteration 91/1000 | Loss: 0.00047211
Iteration 92/1000 | Loss: 0.00020571
Iteration 93/1000 | Loss: 0.00013902
Iteration 94/1000 | Loss: 0.00010638
Iteration 95/1000 | Loss: 0.00010036
Iteration 96/1000 | Loss: 0.00023330
Iteration 97/1000 | Loss: 0.00009638
Iteration 98/1000 | Loss: 0.00018034
Iteration 99/1000 | Loss: 0.00009527
Iteration 100/1000 | Loss: 0.00009447
Iteration 101/1000 | Loss: 0.00009379
Iteration 102/1000 | Loss: 0.00026381
Iteration 103/1000 | Loss: 0.00010408
Iteration 104/1000 | Loss: 0.00009540
Iteration 105/1000 | Loss: 0.00009340
Iteration 106/1000 | Loss: 0.00009309
Iteration 107/1000 | Loss: 0.00026633
Iteration 108/1000 | Loss: 0.00010199
Iteration 109/1000 | Loss: 0.00009738
Iteration 110/1000 | Loss: 0.00020304
Iteration 111/1000 | Loss: 0.00009423
Iteration 112/1000 | Loss: 0.00009287
Iteration 113/1000 | Loss: 0.00009282
Iteration 114/1000 | Loss: 0.00009269
Iteration 115/1000 | Loss: 0.00009262
Iteration 116/1000 | Loss: 0.00009262
Iteration 117/1000 | Loss: 0.00009262
Iteration 118/1000 | Loss: 0.00009261
Iteration 119/1000 | Loss: 0.00009261
Iteration 120/1000 | Loss: 0.00009259
Iteration 121/1000 | Loss: 0.00009259
Iteration 122/1000 | Loss: 0.00009259
Iteration 123/1000 | Loss: 0.00009259
Iteration 124/1000 | Loss: 0.00009259
Iteration 125/1000 | Loss: 0.00009259
Iteration 126/1000 | Loss: 0.00009259
Iteration 127/1000 | Loss: 0.00009257
Iteration 128/1000 | Loss: 0.00009257
Iteration 129/1000 | Loss: 0.00009257
Iteration 130/1000 | Loss: 0.00009257
Iteration 131/1000 | Loss: 0.00009256
Iteration 132/1000 | Loss: 0.00009256
Iteration 133/1000 | Loss: 0.00009256
Iteration 134/1000 | Loss: 0.00009256
Iteration 135/1000 | Loss: 0.00009255
Iteration 136/1000 | Loss: 0.00009255
Iteration 137/1000 | Loss: 0.00009255
Iteration 138/1000 | Loss: 0.00009250
Iteration 139/1000 | Loss: 0.00009249
Iteration 140/1000 | Loss: 0.00009248
Iteration 141/1000 | Loss: 0.00009247
Iteration 142/1000 | Loss: 0.00009247
Iteration 143/1000 | Loss: 0.00009246
Iteration 144/1000 | Loss: 0.00009246
Iteration 145/1000 | Loss: 0.00009246
Iteration 146/1000 | Loss: 0.00009245
Iteration 147/1000 | Loss: 0.00009237
Iteration 148/1000 | Loss: 0.00009237
Iteration 149/1000 | Loss: 0.00009236
Iteration 150/1000 | Loss: 0.00009236
Iteration 151/1000 | Loss: 0.00009236
Iteration 152/1000 | Loss: 0.00009236
Iteration 153/1000 | Loss: 0.00009236
Iteration 154/1000 | Loss: 0.00009235
Iteration 155/1000 | Loss: 0.00009235
Iteration 156/1000 | Loss: 0.00009235
Iteration 157/1000 | Loss: 0.00009235
Iteration 158/1000 | Loss: 0.00009234
Iteration 159/1000 | Loss: 0.00009234
Iteration 160/1000 | Loss: 0.00009234
Iteration 161/1000 | Loss: 0.00009233
Iteration 162/1000 | Loss: 0.00009233
Iteration 163/1000 | Loss: 0.00009233
Iteration 164/1000 | Loss: 0.00009232
Iteration 165/1000 | Loss: 0.00009232
Iteration 166/1000 | Loss: 0.00009232
Iteration 167/1000 | Loss: 0.00009232
Iteration 168/1000 | Loss: 0.00009232
Iteration 169/1000 | Loss: 0.00009232
Iteration 170/1000 | Loss: 0.00009232
Iteration 171/1000 | Loss: 0.00009232
Iteration 172/1000 | Loss: 0.00009232
Iteration 173/1000 | Loss: 0.00009231
Iteration 174/1000 | Loss: 0.00009231
Iteration 175/1000 | Loss: 0.00009231
Iteration 176/1000 | Loss: 0.00009231
Iteration 177/1000 | Loss: 0.00009231
Iteration 178/1000 | Loss: 0.00009231
Iteration 179/1000 | Loss: 0.00009231
Iteration 180/1000 | Loss: 0.00009231
Iteration 181/1000 | Loss: 0.00009231
Iteration 182/1000 | Loss: 0.00009230
Iteration 183/1000 | Loss: 0.00009230
Iteration 184/1000 | Loss: 0.00009230
Iteration 185/1000 | Loss: 0.00009230
Iteration 186/1000 | Loss: 0.00009229
Iteration 187/1000 | Loss: 0.00009229
Iteration 188/1000 | Loss: 0.00009228
Iteration 189/1000 | Loss: 0.00009228
Iteration 190/1000 | Loss: 0.00009228
Iteration 191/1000 | Loss: 0.00009228
Iteration 192/1000 | Loss: 0.00009228
Iteration 193/1000 | Loss: 0.00009227
Iteration 194/1000 | Loss: 0.00009227
Iteration 195/1000 | Loss: 0.00009227
Iteration 196/1000 | Loss: 0.00009227
Iteration 197/1000 | Loss: 0.00009227
Iteration 198/1000 | Loss: 0.00009227
Iteration 199/1000 | Loss: 0.00009227
Iteration 200/1000 | Loss: 0.00009227
Iteration 201/1000 | Loss: 0.00009226
Iteration 202/1000 | Loss: 0.00009226
Iteration 203/1000 | Loss: 0.00009226
Iteration 204/1000 | Loss: 0.00009226
Iteration 205/1000 | Loss: 0.00009226
Iteration 206/1000 | Loss: 0.00009226
Iteration 207/1000 | Loss: 0.00009226
Iteration 208/1000 | Loss: 0.00009226
Iteration 209/1000 | Loss: 0.00009226
Iteration 210/1000 | Loss: 0.00009226
Iteration 211/1000 | Loss: 0.00009226
Iteration 212/1000 | Loss: 0.00009226
Iteration 213/1000 | Loss: 0.00009226
Iteration 214/1000 | Loss: 0.00009226
Iteration 215/1000 | Loss: 0.00009225
Iteration 216/1000 | Loss: 0.00009225
Iteration 217/1000 | Loss: 0.00009225
Iteration 218/1000 | Loss: 0.00009225
Iteration 219/1000 | Loss: 0.00009225
Iteration 220/1000 | Loss: 0.00009225
Iteration 221/1000 | Loss: 0.00009225
Iteration 222/1000 | Loss: 0.00009225
Iteration 223/1000 | Loss: 0.00009225
Iteration 224/1000 | Loss: 0.00009225
Iteration 225/1000 | Loss: 0.00009225
Iteration 226/1000 | Loss: 0.00009224
Iteration 227/1000 | Loss: 0.00009224
Iteration 228/1000 | Loss: 0.00009224
Iteration 229/1000 | Loss: 0.00009224
Iteration 230/1000 | Loss: 0.00009224
Iteration 231/1000 | Loss: 0.00009224
Iteration 232/1000 | Loss: 0.00009224
Iteration 233/1000 | Loss: 0.00009224
Iteration 234/1000 | Loss: 0.00009224
Iteration 235/1000 | Loss: 0.00009224
Iteration 236/1000 | Loss: 0.00009224
Iteration 237/1000 | Loss: 0.00009224
Iteration 238/1000 | Loss: 0.00009224
Iteration 239/1000 | Loss: 0.00009224
Iteration 240/1000 | Loss: 0.00009224
Iteration 241/1000 | Loss: 0.00009224
Iteration 242/1000 | Loss: 0.00009224
Iteration 243/1000 | Loss: 0.00009224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [9.223707456840202e-05, 9.223707456840202e-05, 9.223707456840202e-05, 9.223707456840202e-05, 9.223707456840202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.223707456840202e-05

Optimization complete. Final v2v error: 5.2199225425720215 mm

Highest mean error: 11.179825782775879 mm for frame 54

Lowest mean error: 3.5527889728546143 mm for frame 149

Saving results

Total time: 209.74251890182495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016020
Iteration 2/25 | Loss: 0.00131084
Iteration 3/25 | Loss: 0.00092914
Iteration 4/25 | Loss: 0.00086503
Iteration 5/25 | Loss: 0.00082841
Iteration 6/25 | Loss: 0.00082267
Iteration 7/25 | Loss: 0.00084318
Iteration 8/25 | Loss: 0.00081088
Iteration 9/25 | Loss: 0.00079218
Iteration 10/25 | Loss: 0.00078364
Iteration 11/25 | Loss: 0.00077536
Iteration 12/25 | Loss: 0.00073720
Iteration 13/25 | Loss: 0.00073864
Iteration 14/25 | Loss: 0.00073922
Iteration 15/25 | Loss: 0.00071078
Iteration 16/25 | Loss: 0.00071041
Iteration 17/25 | Loss: 0.00073321
Iteration 18/25 | Loss: 0.00073385
Iteration 19/25 | Loss: 0.00070494
Iteration 20/25 | Loss: 0.00069132
Iteration 21/25 | Loss: 0.00069886
Iteration 22/25 | Loss: 0.00067978
Iteration 23/25 | Loss: 0.00067705
Iteration 24/25 | Loss: 0.00067687
Iteration 25/25 | Loss: 0.00067684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04398561
Iteration 2/25 | Loss: 0.00053895
Iteration 3/25 | Loss: 0.00053894
Iteration 4/25 | Loss: 0.00053894
Iteration 5/25 | Loss: 0.00053894
Iteration 6/25 | Loss: 0.00053894
Iteration 7/25 | Loss: 0.00053893
Iteration 8/25 | Loss: 0.00053893
Iteration 9/25 | Loss: 0.00053893
Iteration 10/25 | Loss: 0.00053893
Iteration 11/25 | Loss: 0.00053893
Iteration 12/25 | Loss: 0.00053893
Iteration 13/25 | Loss: 0.00053893
Iteration 14/25 | Loss: 0.00053893
Iteration 15/25 | Loss: 0.00053893
Iteration 16/25 | Loss: 0.00053893
Iteration 17/25 | Loss: 0.00053893
Iteration 18/25 | Loss: 0.00053893
Iteration 19/25 | Loss: 0.00053893
Iteration 20/25 | Loss: 0.00053893
Iteration 21/25 | Loss: 0.00053893
Iteration 22/25 | Loss: 0.00053893
Iteration 23/25 | Loss: 0.00053893
Iteration 24/25 | Loss: 0.00053893
Iteration 25/25 | Loss: 0.00053893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053893
Iteration 2/1000 | Loss: 0.00006124
Iteration 3/1000 | Loss: 0.00004574
Iteration 4/1000 | Loss: 0.00004123
Iteration 5/1000 | Loss: 0.00050475
Iteration 6/1000 | Loss: 0.00102006
Iteration 7/1000 | Loss: 0.00145129
Iteration 8/1000 | Loss: 0.00075428
Iteration 9/1000 | Loss: 0.00113651
Iteration 10/1000 | Loss: 0.00027948
Iteration 11/1000 | Loss: 0.00053789
Iteration 12/1000 | Loss: 0.00027968
Iteration 13/1000 | Loss: 0.00124166
Iteration 14/1000 | Loss: 0.00009684
Iteration 15/1000 | Loss: 0.00112979
Iteration 16/1000 | Loss: 0.00074134
Iteration 17/1000 | Loss: 0.00004447
Iteration 18/1000 | Loss: 0.00003462
Iteration 19/1000 | Loss: 0.00003169
Iteration 20/1000 | Loss: 0.00002924
Iteration 21/1000 | Loss: 0.00002604
Iteration 22/1000 | Loss: 0.00002395
Iteration 23/1000 | Loss: 0.00002256
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002089
Iteration 26/1000 | Loss: 0.00002049
Iteration 27/1000 | Loss: 0.00002021
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001992
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001963
Iteration 32/1000 | Loss: 0.00001961
Iteration 33/1000 | Loss: 0.00001961
Iteration 34/1000 | Loss: 0.00001960
Iteration 35/1000 | Loss: 0.00001959
Iteration 36/1000 | Loss: 0.00001959
Iteration 37/1000 | Loss: 0.00001959
Iteration 38/1000 | Loss: 0.00001958
Iteration 39/1000 | Loss: 0.00001958
Iteration 40/1000 | Loss: 0.00001957
Iteration 41/1000 | Loss: 0.00001956
Iteration 42/1000 | Loss: 0.00001955
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001950
Iteration 46/1000 | Loss: 0.00001949
Iteration 47/1000 | Loss: 0.00001948
Iteration 48/1000 | Loss: 0.00001948
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001942
Iteration 51/1000 | Loss: 0.00001941
Iteration 52/1000 | Loss: 0.00001941
Iteration 53/1000 | Loss: 0.00001939
Iteration 54/1000 | Loss: 0.00001938
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001935
Iteration 61/1000 | Loss: 0.00001935
Iteration 62/1000 | Loss: 0.00001935
Iteration 63/1000 | Loss: 0.00001934
Iteration 64/1000 | Loss: 0.00001934
Iteration 65/1000 | Loss: 0.00001933
Iteration 66/1000 | Loss: 0.00001933
Iteration 67/1000 | Loss: 0.00001933
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001931
Iteration 71/1000 | Loss: 0.00001931
Iteration 72/1000 | Loss: 0.00001931
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001928
Iteration 81/1000 | Loss: 0.00001928
Iteration 82/1000 | Loss: 0.00001928
Iteration 83/1000 | Loss: 0.00001927
Iteration 84/1000 | Loss: 0.00001927
Iteration 85/1000 | Loss: 0.00001927
Iteration 86/1000 | Loss: 0.00001926
Iteration 87/1000 | Loss: 0.00001926
Iteration 88/1000 | Loss: 0.00001926
Iteration 89/1000 | Loss: 0.00001926
Iteration 90/1000 | Loss: 0.00001925
Iteration 91/1000 | Loss: 0.00001925
Iteration 92/1000 | Loss: 0.00001925
Iteration 93/1000 | Loss: 0.00001925
Iteration 94/1000 | Loss: 0.00001924
Iteration 95/1000 | Loss: 0.00001924
Iteration 96/1000 | Loss: 0.00001924
Iteration 97/1000 | Loss: 0.00001924
Iteration 98/1000 | Loss: 0.00001923
Iteration 99/1000 | Loss: 0.00001923
Iteration 100/1000 | Loss: 0.00001923
Iteration 101/1000 | Loss: 0.00001923
Iteration 102/1000 | Loss: 0.00001923
Iteration 103/1000 | Loss: 0.00001922
Iteration 104/1000 | Loss: 0.00001922
Iteration 105/1000 | Loss: 0.00001922
Iteration 106/1000 | Loss: 0.00001921
Iteration 107/1000 | Loss: 0.00001921
Iteration 108/1000 | Loss: 0.00001921
Iteration 109/1000 | Loss: 0.00001921
Iteration 110/1000 | Loss: 0.00001921
Iteration 111/1000 | Loss: 0.00001920
Iteration 112/1000 | Loss: 0.00001920
Iteration 113/1000 | Loss: 0.00001920
Iteration 114/1000 | Loss: 0.00001919
Iteration 115/1000 | Loss: 0.00001919
Iteration 116/1000 | Loss: 0.00001919
Iteration 117/1000 | Loss: 0.00001919
Iteration 118/1000 | Loss: 0.00001919
Iteration 119/1000 | Loss: 0.00001919
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001918
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001916
Iteration 129/1000 | Loss: 0.00001916
Iteration 130/1000 | Loss: 0.00001916
Iteration 131/1000 | Loss: 0.00001915
Iteration 132/1000 | Loss: 0.00001915
Iteration 133/1000 | Loss: 0.00001914
Iteration 134/1000 | Loss: 0.00001914
Iteration 135/1000 | Loss: 0.00001913
Iteration 136/1000 | Loss: 0.00001913
Iteration 137/1000 | Loss: 0.00001913
Iteration 138/1000 | Loss: 0.00057359
Iteration 139/1000 | Loss: 0.00071452
Iteration 140/1000 | Loss: 0.00114219
Iteration 141/1000 | Loss: 0.00058149
Iteration 142/1000 | Loss: 0.00003318
Iteration 143/1000 | Loss: 0.00097966
Iteration 144/1000 | Loss: 0.00002371
Iteration 145/1000 | Loss: 0.00001970
Iteration 146/1000 | Loss: 0.00001776
Iteration 147/1000 | Loss: 0.00001617
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001452
Iteration 150/1000 | Loss: 0.00001416
Iteration 151/1000 | Loss: 0.00001408
Iteration 152/1000 | Loss: 0.00001386
Iteration 153/1000 | Loss: 0.00001386
Iteration 154/1000 | Loss: 0.00001371
Iteration 155/1000 | Loss: 0.00001371
Iteration 156/1000 | Loss: 0.00001370
Iteration 157/1000 | Loss: 0.00001369
Iteration 158/1000 | Loss: 0.00001369
Iteration 159/1000 | Loss: 0.00001361
Iteration 160/1000 | Loss: 0.00001358
Iteration 161/1000 | Loss: 0.00001358
Iteration 162/1000 | Loss: 0.00001358
Iteration 163/1000 | Loss: 0.00001357
Iteration 164/1000 | Loss: 0.00001357
Iteration 165/1000 | Loss: 0.00001356
Iteration 166/1000 | Loss: 0.00001355
Iteration 167/1000 | Loss: 0.00001355
Iteration 168/1000 | Loss: 0.00001354
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001353
Iteration 175/1000 | Loss: 0.00001353
Iteration 176/1000 | Loss: 0.00001353
Iteration 177/1000 | Loss: 0.00001353
Iteration 178/1000 | Loss: 0.00001353
Iteration 179/1000 | Loss: 0.00001353
Iteration 180/1000 | Loss: 0.00001353
Iteration 181/1000 | Loss: 0.00001353
Iteration 182/1000 | Loss: 0.00001353
Iteration 183/1000 | Loss: 0.00001352
Iteration 184/1000 | Loss: 0.00001352
Iteration 185/1000 | Loss: 0.00001352
Iteration 186/1000 | Loss: 0.00001352
Iteration 187/1000 | Loss: 0.00001351
Iteration 188/1000 | Loss: 0.00001351
Iteration 189/1000 | Loss: 0.00001351
Iteration 190/1000 | Loss: 0.00001351
Iteration 191/1000 | Loss: 0.00001351
Iteration 192/1000 | Loss: 0.00001351
Iteration 193/1000 | Loss: 0.00001351
Iteration 194/1000 | Loss: 0.00001350
Iteration 195/1000 | Loss: 0.00001350
Iteration 196/1000 | Loss: 0.00001350
Iteration 197/1000 | Loss: 0.00001350
Iteration 198/1000 | Loss: 0.00001349
Iteration 199/1000 | Loss: 0.00001349
Iteration 200/1000 | Loss: 0.00001349
Iteration 201/1000 | Loss: 0.00001349
Iteration 202/1000 | Loss: 0.00001349
Iteration 203/1000 | Loss: 0.00001349
Iteration 204/1000 | Loss: 0.00001348
Iteration 205/1000 | Loss: 0.00001348
Iteration 206/1000 | Loss: 0.00001348
Iteration 207/1000 | Loss: 0.00001348
Iteration 208/1000 | Loss: 0.00001348
Iteration 209/1000 | Loss: 0.00001348
Iteration 210/1000 | Loss: 0.00001348
Iteration 211/1000 | Loss: 0.00001348
Iteration 212/1000 | Loss: 0.00001348
Iteration 213/1000 | Loss: 0.00001347
Iteration 214/1000 | Loss: 0.00001347
Iteration 215/1000 | Loss: 0.00001347
Iteration 216/1000 | Loss: 0.00001346
Iteration 217/1000 | Loss: 0.00001346
Iteration 218/1000 | Loss: 0.00001346
Iteration 219/1000 | Loss: 0.00001346
Iteration 220/1000 | Loss: 0.00001346
Iteration 221/1000 | Loss: 0.00001346
Iteration 222/1000 | Loss: 0.00001346
Iteration 223/1000 | Loss: 0.00001346
Iteration 224/1000 | Loss: 0.00001346
Iteration 225/1000 | Loss: 0.00001346
Iteration 226/1000 | Loss: 0.00001345
Iteration 227/1000 | Loss: 0.00001345
Iteration 228/1000 | Loss: 0.00001345
Iteration 229/1000 | Loss: 0.00001345
Iteration 230/1000 | Loss: 0.00001345
Iteration 231/1000 | Loss: 0.00001345
Iteration 232/1000 | Loss: 0.00001345
Iteration 233/1000 | Loss: 0.00001345
Iteration 234/1000 | Loss: 0.00001345
Iteration 235/1000 | Loss: 0.00001345
Iteration 236/1000 | Loss: 0.00001345
Iteration 237/1000 | Loss: 0.00001344
Iteration 238/1000 | Loss: 0.00001344
Iteration 239/1000 | Loss: 0.00001344
Iteration 240/1000 | Loss: 0.00001344
Iteration 241/1000 | Loss: 0.00001344
Iteration 242/1000 | Loss: 0.00001344
Iteration 243/1000 | Loss: 0.00001344
Iteration 244/1000 | Loss: 0.00001344
Iteration 245/1000 | Loss: 0.00001344
Iteration 246/1000 | Loss: 0.00001344
Iteration 247/1000 | Loss: 0.00001344
Iteration 248/1000 | Loss: 0.00001344
Iteration 249/1000 | Loss: 0.00001344
Iteration 250/1000 | Loss: 0.00001344
Iteration 251/1000 | Loss: 0.00001344
Iteration 252/1000 | Loss: 0.00001344
Iteration 253/1000 | Loss: 0.00001343
Iteration 254/1000 | Loss: 0.00001343
Iteration 255/1000 | Loss: 0.00001343
Iteration 256/1000 | Loss: 0.00001343
Iteration 257/1000 | Loss: 0.00001343
Iteration 258/1000 | Loss: 0.00001343
Iteration 259/1000 | Loss: 0.00001343
Iteration 260/1000 | Loss: 0.00001343
Iteration 261/1000 | Loss: 0.00001343
Iteration 262/1000 | Loss: 0.00001343
Iteration 263/1000 | Loss: 0.00001343
Iteration 264/1000 | Loss: 0.00001343
Iteration 265/1000 | Loss: 0.00001343
Iteration 266/1000 | Loss: 0.00001343
Iteration 267/1000 | Loss: 0.00001343
Iteration 268/1000 | Loss: 0.00001343
Iteration 269/1000 | Loss: 0.00001343
Iteration 270/1000 | Loss: 0.00001343
Iteration 271/1000 | Loss: 0.00001343
Iteration 272/1000 | Loss: 0.00001343
Iteration 273/1000 | Loss: 0.00001343
Iteration 274/1000 | Loss: 0.00001343
Iteration 275/1000 | Loss: 0.00001343
Iteration 276/1000 | Loss: 0.00001343
Iteration 277/1000 | Loss: 0.00001343
Iteration 278/1000 | Loss: 0.00001343
Iteration 279/1000 | Loss: 0.00001343
Iteration 280/1000 | Loss: 0.00001343
Iteration 281/1000 | Loss: 0.00001343
Iteration 282/1000 | Loss: 0.00001342
Iteration 283/1000 | Loss: 0.00001342
Iteration 284/1000 | Loss: 0.00001342
Iteration 285/1000 | Loss: 0.00001342
Iteration 286/1000 | Loss: 0.00001342
Iteration 287/1000 | Loss: 0.00001342
Iteration 288/1000 | Loss: 0.00001342
Iteration 289/1000 | Loss: 0.00001342
Iteration 290/1000 | Loss: 0.00001342
Iteration 291/1000 | Loss: 0.00001342
Iteration 292/1000 | Loss: 0.00001342
Iteration 293/1000 | Loss: 0.00001342
Iteration 294/1000 | Loss: 0.00001342
Iteration 295/1000 | Loss: 0.00001342
Iteration 296/1000 | Loss: 0.00001342
Iteration 297/1000 | Loss: 0.00001342
Iteration 298/1000 | Loss: 0.00001342
Iteration 299/1000 | Loss: 0.00001342
Iteration 300/1000 | Loss: 0.00001342
Iteration 301/1000 | Loss: 0.00001342
Iteration 302/1000 | Loss: 0.00001342
Iteration 303/1000 | Loss: 0.00001342
Iteration 304/1000 | Loss: 0.00001342
Iteration 305/1000 | Loss: 0.00001342
Iteration 306/1000 | Loss: 0.00001342
Iteration 307/1000 | Loss: 0.00001342
Iteration 308/1000 | Loss: 0.00001342
Iteration 309/1000 | Loss: 0.00001342
Iteration 310/1000 | Loss: 0.00001342
Iteration 311/1000 | Loss: 0.00001342
Iteration 312/1000 | Loss: 0.00001342
Iteration 313/1000 | Loss: 0.00001342
Iteration 314/1000 | Loss: 0.00001342
Iteration 315/1000 | Loss: 0.00001342
Iteration 316/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 316. Stopping optimization.
Last 5 losses: [1.34240281113307e-05, 1.34240281113307e-05, 1.34240281113307e-05, 1.34240281113307e-05, 1.34240281113307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.34240281113307e-05

Optimization complete. Final v2v error: 3.090292453765869 mm

Highest mean error: 4.716865539550781 mm for frame 88

Lowest mean error: 2.5732672214508057 mm for frame 31

Saving results

Total time: 121.78757405281067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879292
Iteration 2/25 | Loss: 0.00127934
Iteration 3/25 | Loss: 0.00091691
Iteration 4/25 | Loss: 0.00083934
Iteration 5/25 | Loss: 0.00082106
Iteration 6/25 | Loss: 0.00081396
Iteration 7/25 | Loss: 0.00082920
Iteration 8/25 | Loss: 0.00079019
Iteration 9/25 | Loss: 0.00077335
Iteration 10/25 | Loss: 0.00073788
Iteration 11/25 | Loss: 0.00073172
Iteration 12/25 | Loss: 0.00073131
Iteration 13/25 | Loss: 0.00073128
Iteration 14/25 | Loss: 0.00073127
Iteration 15/25 | Loss: 0.00073127
Iteration 16/25 | Loss: 0.00073127
Iteration 17/25 | Loss: 0.00073127
Iteration 18/25 | Loss: 0.00073127
Iteration 19/25 | Loss: 0.00073127
Iteration 20/25 | Loss: 0.00073127
Iteration 21/25 | Loss: 0.00073127
Iteration 22/25 | Loss: 0.00073126
Iteration 23/25 | Loss: 0.00073126
Iteration 24/25 | Loss: 0.00073126
Iteration 25/25 | Loss: 0.00073126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99853408
Iteration 2/25 | Loss: 0.00026090
Iteration 3/25 | Loss: 0.00026089
Iteration 4/25 | Loss: 0.00026089
Iteration 5/25 | Loss: 0.00026089
Iteration 6/25 | Loss: 0.00026089
Iteration 7/25 | Loss: 0.00026089
Iteration 8/25 | Loss: 0.00026089
Iteration 9/25 | Loss: 0.00026089
Iteration 10/25 | Loss: 0.00026089
Iteration 11/25 | Loss: 0.00026089
Iteration 12/25 | Loss: 0.00026089
Iteration 13/25 | Loss: 0.00026089
Iteration 14/25 | Loss: 0.00026089
Iteration 15/25 | Loss: 0.00026089
Iteration 16/25 | Loss: 0.00026089
Iteration 17/25 | Loss: 0.00026089
Iteration 18/25 | Loss: 0.00026089
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00026088638696819544, 0.00026088638696819544, 0.00026088638696819544, 0.00026088638696819544, 0.00026088638696819544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026088638696819544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026089
Iteration 2/1000 | Loss: 0.00003610
Iteration 3/1000 | Loss: 0.00002847
Iteration 4/1000 | Loss: 0.00002677
Iteration 5/1000 | Loss: 0.00002532
Iteration 6/1000 | Loss: 0.00002441
Iteration 7/1000 | Loss: 0.00002348
Iteration 8/1000 | Loss: 0.00002308
Iteration 9/1000 | Loss: 0.00002283
Iteration 10/1000 | Loss: 0.00002266
Iteration 11/1000 | Loss: 0.00002260
Iteration 12/1000 | Loss: 0.00002260
Iteration 13/1000 | Loss: 0.00002260
Iteration 14/1000 | Loss: 0.00002259
Iteration 15/1000 | Loss: 0.00002256
Iteration 16/1000 | Loss: 0.00002256
Iteration 17/1000 | Loss: 0.00002247
Iteration 18/1000 | Loss: 0.00002247
Iteration 19/1000 | Loss: 0.00002247
Iteration 20/1000 | Loss: 0.00002246
Iteration 21/1000 | Loss: 0.00002245
Iteration 22/1000 | Loss: 0.00002245
Iteration 23/1000 | Loss: 0.00002245
Iteration 24/1000 | Loss: 0.00002245
Iteration 25/1000 | Loss: 0.00002245
Iteration 26/1000 | Loss: 0.00002245
Iteration 27/1000 | Loss: 0.00002244
Iteration 28/1000 | Loss: 0.00002244
Iteration 29/1000 | Loss: 0.00002244
Iteration 30/1000 | Loss: 0.00002244
Iteration 31/1000 | Loss: 0.00002244
Iteration 32/1000 | Loss: 0.00002243
Iteration 33/1000 | Loss: 0.00002243
Iteration 34/1000 | Loss: 0.00002243
Iteration 35/1000 | Loss: 0.00002243
Iteration 36/1000 | Loss: 0.00002242
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002242
Iteration 39/1000 | Loss: 0.00002242
Iteration 40/1000 | Loss: 0.00002242
Iteration 41/1000 | Loss: 0.00002242
Iteration 42/1000 | Loss: 0.00002242
Iteration 43/1000 | Loss: 0.00002241
Iteration 44/1000 | Loss: 0.00002241
Iteration 45/1000 | Loss: 0.00002240
Iteration 46/1000 | Loss: 0.00002240
Iteration 47/1000 | Loss: 0.00002240
Iteration 48/1000 | Loss: 0.00002239
Iteration 49/1000 | Loss: 0.00002239
Iteration 50/1000 | Loss: 0.00002239
Iteration 51/1000 | Loss: 0.00002239
Iteration 52/1000 | Loss: 0.00002239
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002239
Iteration 55/1000 | Loss: 0.00002239
Iteration 56/1000 | Loss: 0.00002238
Iteration 57/1000 | Loss: 0.00002238
Iteration 58/1000 | Loss: 0.00002238
Iteration 59/1000 | Loss: 0.00002237
Iteration 60/1000 | Loss: 0.00002237
Iteration 61/1000 | Loss: 0.00002236
Iteration 62/1000 | Loss: 0.00002236
Iteration 63/1000 | Loss: 0.00002236
Iteration 64/1000 | Loss: 0.00002236
Iteration 65/1000 | Loss: 0.00002236
Iteration 66/1000 | Loss: 0.00002236
Iteration 67/1000 | Loss: 0.00002236
Iteration 68/1000 | Loss: 0.00002236
Iteration 69/1000 | Loss: 0.00002236
Iteration 70/1000 | Loss: 0.00002235
Iteration 71/1000 | Loss: 0.00002235
Iteration 72/1000 | Loss: 0.00002235
Iteration 73/1000 | Loss: 0.00002234
Iteration 74/1000 | Loss: 0.00002234
Iteration 75/1000 | Loss: 0.00002234
Iteration 76/1000 | Loss: 0.00002234
Iteration 77/1000 | Loss: 0.00002234
Iteration 78/1000 | Loss: 0.00002234
Iteration 79/1000 | Loss: 0.00002234
Iteration 80/1000 | Loss: 0.00002234
Iteration 81/1000 | Loss: 0.00002234
Iteration 82/1000 | Loss: 0.00002234
Iteration 83/1000 | Loss: 0.00002234
Iteration 84/1000 | Loss: 0.00002234
Iteration 85/1000 | Loss: 0.00002234
Iteration 86/1000 | Loss: 0.00002234
Iteration 87/1000 | Loss: 0.00002233
Iteration 88/1000 | Loss: 0.00002233
Iteration 89/1000 | Loss: 0.00002233
Iteration 90/1000 | Loss: 0.00002233
Iteration 91/1000 | Loss: 0.00002233
Iteration 92/1000 | Loss: 0.00002233
Iteration 93/1000 | Loss: 0.00002233
Iteration 94/1000 | Loss: 0.00002233
Iteration 95/1000 | Loss: 0.00002233
Iteration 96/1000 | Loss: 0.00002232
Iteration 97/1000 | Loss: 0.00002232
Iteration 98/1000 | Loss: 0.00002232
Iteration 99/1000 | Loss: 0.00002232
Iteration 100/1000 | Loss: 0.00002232
Iteration 101/1000 | Loss: 0.00002232
Iteration 102/1000 | Loss: 0.00002232
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002232
Iteration 107/1000 | Loss: 0.00002232
Iteration 108/1000 | Loss: 0.00002232
Iteration 109/1000 | Loss: 0.00002232
Iteration 110/1000 | Loss: 0.00002232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.2323705707094632e-05, 2.2323705707094632e-05, 2.2323705707094632e-05, 2.2323705707094632e-05, 2.2323705707094632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2323705707094632e-05

Optimization complete. Final v2v error: 4.006382942199707 mm

Highest mean error: 4.438755512237549 mm for frame 80

Lowest mean error: 3.6267716884613037 mm for frame 44

Saving results

Total time: 41.61981225013733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961281
Iteration 2/25 | Loss: 0.00117594
Iteration 3/25 | Loss: 0.00075987
Iteration 4/25 | Loss: 0.00073326
Iteration 5/25 | Loss: 0.00072642
Iteration 6/25 | Loss: 0.00072438
Iteration 7/25 | Loss: 0.00072421
Iteration 8/25 | Loss: 0.00072421
Iteration 9/25 | Loss: 0.00072421
Iteration 10/25 | Loss: 0.00072421
Iteration 11/25 | Loss: 0.00072421
Iteration 12/25 | Loss: 0.00072421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007242059800773859, 0.0007242059800773859, 0.0007242059800773859, 0.0007242059800773859, 0.0007242059800773859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007242059800773859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73891723
Iteration 2/25 | Loss: 0.00042240
Iteration 3/25 | Loss: 0.00042239
Iteration 4/25 | Loss: 0.00042239
Iteration 5/25 | Loss: 0.00042239
Iteration 6/25 | Loss: 0.00042239
Iteration 7/25 | Loss: 0.00042239
Iteration 8/25 | Loss: 0.00042239
Iteration 9/25 | Loss: 0.00042239
Iteration 10/25 | Loss: 0.00042239
Iteration 11/25 | Loss: 0.00042238
Iteration 12/25 | Loss: 0.00042238
Iteration 13/25 | Loss: 0.00042238
Iteration 14/25 | Loss: 0.00042238
Iteration 15/25 | Loss: 0.00042238
Iteration 16/25 | Loss: 0.00042238
Iteration 17/25 | Loss: 0.00042238
Iteration 18/25 | Loss: 0.00042238
Iteration 19/25 | Loss: 0.00042238
Iteration 20/25 | Loss: 0.00042238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004223849100526422, 0.0004223849100526422, 0.0004223849100526422, 0.0004223849100526422, 0.0004223849100526422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004223849100526422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042238
Iteration 2/1000 | Loss: 0.00003144
Iteration 3/1000 | Loss: 0.00002167
Iteration 4/1000 | Loss: 0.00002016
Iteration 5/1000 | Loss: 0.00001931
Iteration 6/1000 | Loss: 0.00001879
Iteration 7/1000 | Loss: 0.00001850
Iteration 8/1000 | Loss: 0.00001824
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001801
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001799
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001796
Iteration 20/1000 | Loss: 0.00001796
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001791
Iteration 23/1000 | Loss: 0.00001790
Iteration 24/1000 | Loss: 0.00001786
Iteration 25/1000 | Loss: 0.00001786
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001783
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001782
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001780
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001778
Iteration 41/1000 | Loss: 0.00001778
Iteration 42/1000 | Loss: 0.00001778
Iteration 43/1000 | Loss: 0.00001778
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001777
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001776
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001776
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001775
Iteration 56/1000 | Loss: 0.00001775
Iteration 57/1000 | Loss: 0.00001775
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001775
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001774
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001773
Iteration 67/1000 | Loss: 0.00001773
Iteration 68/1000 | Loss: 0.00001773
Iteration 69/1000 | Loss: 0.00001773
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001772
Iteration 76/1000 | Loss: 0.00001772
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001772
Iteration 86/1000 | Loss: 0.00001772
Iteration 87/1000 | Loss: 0.00001772
Iteration 88/1000 | Loss: 0.00001772
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001771
Iteration 95/1000 | Loss: 0.00001771
Iteration 96/1000 | Loss: 0.00001771
Iteration 97/1000 | Loss: 0.00001771
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001771
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001771
Iteration 102/1000 | Loss: 0.00001771
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.7707483493722975e-05, 1.7707483493722975e-05, 1.7707483493722975e-05, 1.7707483493722975e-05, 1.7707483493722975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7707483493722975e-05

Optimization complete. Final v2v error: 3.5041611194610596 mm

Highest mean error: 4.878941059112549 mm for frame 4

Lowest mean error: 2.8283936977386475 mm for frame 226

Saving results

Total time: 35.288328647613525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544699
Iteration 2/25 | Loss: 0.00111725
Iteration 3/25 | Loss: 0.00069301
Iteration 4/25 | Loss: 0.00068152
Iteration 5/25 | Loss: 0.00061906
Iteration 6/25 | Loss: 0.00061549
Iteration 7/25 | Loss: 0.00061430
Iteration 8/25 | Loss: 0.00061395
Iteration 9/25 | Loss: 0.00061386
Iteration 10/25 | Loss: 0.00061386
Iteration 11/25 | Loss: 0.00061386
Iteration 12/25 | Loss: 0.00061386
Iteration 13/25 | Loss: 0.00061386
Iteration 14/25 | Loss: 0.00061386
Iteration 15/25 | Loss: 0.00061386
Iteration 16/25 | Loss: 0.00061386
Iteration 17/25 | Loss: 0.00061386
Iteration 18/25 | Loss: 0.00061386
Iteration 19/25 | Loss: 0.00061386
Iteration 20/25 | Loss: 0.00061386
Iteration 21/25 | Loss: 0.00061386
Iteration 22/25 | Loss: 0.00061386
Iteration 23/25 | Loss: 0.00061386
Iteration 24/25 | Loss: 0.00061386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006138637545518577, 0.0006138637545518577, 0.0006138637545518577, 0.0006138637545518577, 0.0006138637545518577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006138637545518577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.31794643
Iteration 2/25 | Loss: 0.00037166
Iteration 3/25 | Loss: 0.00037164
Iteration 4/25 | Loss: 0.00037164
Iteration 5/25 | Loss: 0.00037164
Iteration 6/25 | Loss: 0.00037164
Iteration 7/25 | Loss: 0.00037164
Iteration 8/25 | Loss: 0.00037164
Iteration 9/25 | Loss: 0.00037164
Iteration 10/25 | Loss: 0.00037164
Iteration 11/25 | Loss: 0.00037164
Iteration 12/25 | Loss: 0.00037164
Iteration 13/25 | Loss: 0.00037164
Iteration 14/25 | Loss: 0.00037164
Iteration 15/25 | Loss: 0.00037164
Iteration 16/25 | Loss: 0.00037164
Iteration 17/25 | Loss: 0.00037164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003716390929184854, 0.0003716390929184854, 0.0003716390929184854, 0.0003716390929184854, 0.0003716390929184854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003716390929184854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037164
Iteration 2/1000 | Loss: 0.00002363
Iteration 3/1000 | Loss: 0.00001417
Iteration 4/1000 | Loss: 0.00001294
Iteration 5/1000 | Loss: 0.00001221
Iteration 6/1000 | Loss: 0.00001182
Iteration 7/1000 | Loss: 0.00002847
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001148
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001146
Iteration 12/1000 | Loss: 0.00001146
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001146
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001146
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001143
Iteration 19/1000 | Loss: 0.00002101
Iteration 20/1000 | Loss: 0.00001134
Iteration 21/1000 | Loss: 0.00001128
Iteration 22/1000 | Loss: 0.00001127
Iteration 23/1000 | Loss: 0.00001126
Iteration 24/1000 | Loss: 0.00001126
Iteration 25/1000 | Loss: 0.00001126
Iteration 26/1000 | Loss: 0.00001126
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001125
Iteration 30/1000 | Loss: 0.00001124
Iteration 31/1000 | Loss: 0.00001124
Iteration 32/1000 | Loss: 0.00001124
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001118
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00003039
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001109
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001108
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001108
Iteration 61/1000 | Loss: 0.00001107
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001107
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001106
Iteration 66/1000 | Loss: 0.00001105
Iteration 67/1000 | Loss: 0.00001105
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001100
Iteration 70/1000 | Loss: 0.00001100
Iteration 71/1000 | Loss: 0.00001100
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001100
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00002858
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001096
Iteration 86/1000 | Loss: 0.00001096
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00002629
Iteration 98/1000 | Loss: 0.00001098
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001093
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001091
Iteration 115/1000 | Loss: 0.00001091
Iteration 116/1000 | Loss: 0.00001091
Iteration 117/1000 | Loss: 0.00001091
Iteration 118/1000 | Loss: 0.00001091
Iteration 119/1000 | Loss: 0.00001090
Iteration 120/1000 | Loss: 0.00001090
Iteration 121/1000 | Loss: 0.00001090
Iteration 122/1000 | Loss: 0.00001090
Iteration 123/1000 | Loss: 0.00001090
Iteration 124/1000 | Loss: 0.00001090
Iteration 125/1000 | Loss: 0.00001090
Iteration 126/1000 | Loss: 0.00001090
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001090
Iteration 129/1000 | Loss: 0.00001090
Iteration 130/1000 | Loss: 0.00001090
Iteration 131/1000 | Loss: 0.00001090
Iteration 132/1000 | Loss: 0.00001090
Iteration 133/1000 | Loss: 0.00001090
Iteration 134/1000 | Loss: 0.00001089
Iteration 135/1000 | Loss: 0.00001089
Iteration 136/1000 | Loss: 0.00001089
Iteration 137/1000 | Loss: 0.00001089
Iteration 138/1000 | Loss: 0.00001089
Iteration 139/1000 | Loss: 0.00001089
Iteration 140/1000 | Loss: 0.00001089
Iteration 141/1000 | Loss: 0.00001089
Iteration 142/1000 | Loss: 0.00001089
Iteration 143/1000 | Loss: 0.00001088
Iteration 144/1000 | Loss: 0.00001088
Iteration 145/1000 | Loss: 0.00001088
Iteration 146/1000 | Loss: 0.00001088
Iteration 147/1000 | Loss: 0.00001088
Iteration 148/1000 | Loss: 0.00001088
Iteration 149/1000 | Loss: 0.00001088
Iteration 150/1000 | Loss: 0.00001088
Iteration 151/1000 | Loss: 0.00001088
Iteration 152/1000 | Loss: 0.00001088
Iteration 153/1000 | Loss: 0.00001088
Iteration 154/1000 | Loss: 0.00001088
Iteration 155/1000 | Loss: 0.00001088
Iteration 156/1000 | Loss: 0.00001088
Iteration 157/1000 | Loss: 0.00001088
Iteration 158/1000 | Loss: 0.00001087
Iteration 159/1000 | Loss: 0.00001087
Iteration 160/1000 | Loss: 0.00001087
Iteration 161/1000 | Loss: 0.00001087
Iteration 162/1000 | Loss: 0.00001086
Iteration 163/1000 | Loss: 0.00001086
Iteration 164/1000 | Loss: 0.00001086
Iteration 165/1000 | Loss: 0.00001086
Iteration 166/1000 | Loss: 0.00001086
Iteration 167/1000 | Loss: 0.00001086
Iteration 168/1000 | Loss: 0.00001086
Iteration 169/1000 | Loss: 0.00001086
Iteration 170/1000 | Loss: 0.00001086
Iteration 171/1000 | Loss: 0.00001085
Iteration 172/1000 | Loss: 0.00001085
Iteration 173/1000 | Loss: 0.00001085
Iteration 174/1000 | Loss: 0.00001085
Iteration 175/1000 | Loss: 0.00001085
Iteration 176/1000 | Loss: 0.00001085
Iteration 177/1000 | Loss: 0.00001085
Iteration 178/1000 | Loss: 0.00001085
Iteration 179/1000 | Loss: 0.00001085
Iteration 180/1000 | Loss: 0.00001085
Iteration 181/1000 | Loss: 0.00001085
Iteration 182/1000 | Loss: 0.00001085
Iteration 183/1000 | Loss: 0.00001084
Iteration 184/1000 | Loss: 0.00001084
Iteration 185/1000 | Loss: 0.00001084
Iteration 186/1000 | Loss: 0.00001084
Iteration 187/1000 | Loss: 0.00001084
Iteration 188/1000 | Loss: 0.00001084
Iteration 189/1000 | Loss: 0.00001084
Iteration 190/1000 | Loss: 0.00001084
Iteration 191/1000 | Loss: 0.00001084
Iteration 192/1000 | Loss: 0.00001084
Iteration 193/1000 | Loss: 0.00001084
Iteration 194/1000 | Loss: 0.00001084
Iteration 195/1000 | Loss: 0.00001083
Iteration 196/1000 | Loss: 0.00001083
Iteration 197/1000 | Loss: 0.00001083
Iteration 198/1000 | Loss: 0.00001083
Iteration 199/1000 | Loss: 0.00001083
Iteration 200/1000 | Loss: 0.00001082
Iteration 201/1000 | Loss: 0.00001082
Iteration 202/1000 | Loss: 0.00001082
Iteration 203/1000 | Loss: 0.00001082
Iteration 204/1000 | Loss: 0.00001082
Iteration 205/1000 | Loss: 0.00001082
Iteration 206/1000 | Loss: 0.00001082
Iteration 207/1000 | Loss: 0.00001082
Iteration 208/1000 | Loss: 0.00001082
Iteration 209/1000 | Loss: 0.00001082
Iteration 210/1000 | Loss: 0.00001082
Iteration 211/1000 | Loss: 0.00001082
Iteration 212/1000 | Loss: 0.00001082
Iteration 213/1000 | Loss: 0.00001082
Iteration 214/1000 | Loss: 0.00001082
Iteration 215/1000 | Loss: 0.00001082
Iteration 216/1000 | Loss: 0.00001082
Iteration 217/1000 | Loss: 0.00001082
Iteration 218/1000 | Loss: 0.00001082
Iteration 219/1000 | Loss: 0.00001082
Iteration 220/1000 | Loss: 0.00001082
Iteration 221/1000 | Loss: 0.00001082
Iteration 222/1000 | Loss: 0.00001082
Iteration 223/1000 | Loss: 0.00001082
Iteration 224/1000 | Loss: 0.00001082
Iteration 225/1000 | Loss: 0.00001082
Iteration 226/1000 | Loss: 0.00001082
Iteration 227/1000 | Loss: 0.00001081
Iteration 228/1000 | Loss: 0.00001081
Iteration 229/1000 | Loss: 0.00001081
Iteration 230/1000 | Loss: 0.00001081
Iteration 231/1000 | Loss: 0.00001081
Iteration 232/1000 | Loss: 0.00001081
Iteration 233/1000 | Loss: 0.00001081
Iteration 234/1000 | Loss: 0.00001081
Iteration 235/1000 | Loss: 0.00001081
Iteration 236/1000 | Loss: 0.00001081
Iteration 237/1000 | Loss: 0.00001081
Iteration 238/1000 | Loss: 0.00001081
Iteration 239/1000 | Loss: 0.00001081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.0814452252816409e-05, 1.0814452252816409e-05, 1.0814452252816409e-05, 1.0814452252816409e-05, 1.0814452252816409e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0814452252816409e-05

Optimization complete. Final v2v error: 2.7637505531311035 mm

Highest mean error: 3.4742939472198486 mm for frame 166

Lowest mean error: 2.413116216659546 mm for frame 143

Saving results

Total time: 54.76773428916931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01134142
Iteration 2/25 | Loss: 0.00218572
Iteration 3/25 | Loss: 0.00176522
Iteration 4/25 | Loss: 0.00223654
Iteration 5/25 | Loss: 0.00214512
Iteration 6/25 | Loss: 0.00146811
Iteration 7/25 | Loss: 0.00120144
Iteration 8/25 | Loss: 0.00102588
Iteration 9/25 | Loss: 0.00093768
Iteration 10/25 | Loss: 0.00089950
Iteration 11/25 | Loss: 0.00087782
Iteration 12/25 | Loss: 0.00086900
Iteration 13/25 | Loss: 0.00085814
Iteration 14/25 | Loss: 0.00085657
Iteration 15/25 | Loss: 0.00084935
Iteration 16/25 | Loss: 0.00084554
Iteration 17/25 | Loss: 0.00084097
Iteration 18/25 | Loss: 0.00083754
Iteration 19/25 | Loss: 0.00083664
Iteration 20/25 | Loss: 0.00083759
Iteration 21/25 | Loss: 0.00083925
Iteration 22/25 | Loss: 0.00083848
Iteration 23/25 | Loss: 0.00083581
Iteration 24/25 | Loss: 0.00083590
Iteration 25/25 | Loss: 0.00083604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30813658
Iteration 2/25 | Loss: 0.00019383
Iteration 3/25 | Loss: 0.00019383
Iteration 4/25 | Loss: 0.00019383
Iteration 5/25 | Loss: 0.00019383
Iteration 6/25 | Loss: 0.00019383
Iteration 7/25 | Loss: 0.00019383
Iteration 8/25 | Loss: 0.00019383
Iteration 9/25 | Loss: 0.00019383
Iteration 10/25 | Loss: 0.00019383
Iteration 11/25 | Loss: 0.00019383
Iteration 12/25 | Loss: 0.00019383
Iteration 13/25 | Loss: 0.00019383
Iteration 14/25 | Loss: 0.00019383
Iteration 15/25 | Loss: 0.00019383
Iteration 16/25 | Loss: 0.00019383
Iteration 17/25 | Loss: 0.00019383
Iteration 18/25 | Loss: 0.00019383
Iteration 19/25 | Loss: 0.00019383
Iteration 20/25 | Loss: 0.00019383
Iteration 21/25 | Loss: 0.00019383
Iteration 22/25 | Loss: 0.00019383
Iteration 23/25 | Loss: 0.00019383
Iteration 24/25 | Loss: 0.00019383
Iteration 25/25 | Loss: 0.00019383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019383
Iteration 2/1000 | Loss: 0.00007735
Iteration 3/1000 | Loss: 0.00007975
Iteration 4/1000 | Loss: 0.00006960
Iteration 5/1000 | Loss: 0.00008798
Iteration 6/1000 | Loss: 0.00009495
Iteration 7/1000 | Loss: 0.00010615
Iteration 8/1000 | Loss: 0.00008010
Iteration 9/1000 | Loss: 0.00008340
Iteration 10/1000 | Loss: 0.00008852
Iteration 11/1000 | Loss: 0.00009771
Iteration 12/1000 | Loss: 0.00008837
Iteration 13/1000 | Loss: 0.00008192
Iteration 14/1000 | Loss: 0.00008207
Iteration 15/1000 | Loss: 0.00008637
Iteration 16/1000 | Loss: 0.00009360
Iteration 17/1000 | Loss: 0.00010118
Iteration 18/1000 | Loss: 0.00009627
Iteration 19/1000 | Loss: 0.00009886
Iteration 20/1000 | Loss: 0.00010260
Iteration 21/1000 | Loss: 0.00009178
Iteration 22/1000 | Loss: 0.00009164
Iteration 23/1000 | Loss: 0.00008516
Iteration 24/1000 | Loss: 0.00007976
Iteration 25/1000 | Loss: 0.00007295
Iteration 26/1000 | Loss: 0.00008882
Iteration 27/1000 | Loss: 0.00008741
Iteration 28/1000 | Loss: 0.00009206
Iteration 29/1000 | Loss: 0.00008302
Iteration 30/1000 | Loss: 0.00007780
Iteration 31/1000 | Loss: 0.00007881
Iteration 32/1000 | Loss: 0.00010270
Iteration 33/1000 | Loss: 0.00008510
Iteration 34/1000 | Loss: 0.00009160
Iteration 35/1000 | Loss: 0.00008578
Iteration 36/1000 | Loss: 0.00008722
Iteration 37/1000 | Loss: 0.00007940
Iteration 38/1000 | Loss: 0.00008914
Iteration 39/1000 | Loss: 0.00008965
Iteration 40/1000 | Loss: 0.00009464
Iteration 41/1000 | Loss: 0.00007636
Iteration 42/1000 | Loss: 0.00006428
Iteration 43/1000 | Loss: 0.00010064
Iteration 44/1000 | Loss: 0.00008771
Iteration 45/1000 | Loss: 0.00008471
Iteration 46/1000 | Loss: 0.00008182
Iteration 47/1000 | Loss: 0.00007995
Iteration 48/1000 | Loss: 0.00007740
Iteration 49/1000 | Loss: 0.00009034
Iteration 50/1000 | Loss: 0.00008293
Iteration 51/1000 | Loss: 0.00008711
Iteration 52/1000 | Loss: 0.00008190
Iteration 53/1000 | Loss: 0.00008089
Iteration 54/1000 | Loss: 0.00008796
Iteration 55/1000 | Loss: 0.00008485
Iteration 56/1000 | Loss: 0.00008403
Iteration 57/1000 | Loss: 0.00008334
Iteration 58/1000 | Loss: 0.00008774
Iteration 59/1000 | Loss: 0.00009029
Iteration 60/1000 | Loss: 0.00009261
Iteration 61/1000 | Loss: 0.00009173
Iteration 62/1000 | Loss: 0.00008237
Iteration 63/1000 | Loss: 0.00008444
Iteration 64/1000 | Loss: 0.00008265
Iteration 65/1000 | Loss: 0.00008383
Iteration 66/1000 | Loss: 0.00008240
Iteration 67/1000 | Loss: 0.00008359
Iteration 68/1000 | Loss: 0.00008403
Iteration 69/1000 | Loss: 0.00008144
Iteration 70/1000 | Loss: 0.00008325
Iteration 71/1000 | Loss: 0.00008360
Iteration 72/1000 | Loss: 0.00008292
Iteration 73/1000 | Loss: 0.00008143
Iteration 74/1000 | Loss: 0.00008636
Iteration 75/1000 | Loss: 0.00008646
Iteration 76/1000 | Loss: 0.00008260
Iteration 77/1000 | Loss: 0.00008252
Iteration 78/1000 | Loss: 0.00008038
Iteration 79/1000 | Loss: 0.00007911
Iteration 80/1000 | Loss: 0.00008771
Iteration 81/1000 | Loss: 0.00008253
Iteration 82/1000 | Loss: 0.00008439
Iteration 83/1000 | Loss: 0.00008244
Iteration 84/1000 | Loss: 0.00008144
Iteration 85/1000 | Loss: 0.00008001
Iteration 86/1000 | Loss: 0.00008055
Iteration 87/1000 | Loss: 0.00007516
Iteration 88/1000 | Loss: 0.00008322
Iteration 89/1000 | Loss: 0.00007799
Iteration 90/1000 | Loss: 0.00007668
Iteration 91/1000 | Loss: 0.00008618
Iteration 92/1000 | Loss: 0.00007886
Iteration 93/1000 | Loss: 0.00008189
Iteration 94/1000 | Loss: 0.00007611
Iteration 95/1000 | Loss: 0.00007996
Iteration 96/1000 | Loss: 0.00007664
Iteration 97/1000 | Loss: 0.00009397
Iteration 98/1000 | Loss: 0.00005168
Iteration 99/1000 | Loss: 0.00006458
Iteration 100/1000 | Loss: 0.00007372
Iteration 101/1000 | Loss: 0.00006650
Iteration 102/1000 | Loss: 0.00007962
Iteration 103/1000 | Loss: 0.00007873
Iteration 104/1000 | Loss: 0.00009100
Iteration 105/1000 | Loss: 0.00007508
Iteration 106/1000 | Loss: 0.00009507
Iteration 107/1000 | Loss: 0.00008833
Iteration 108/1000 | Loss: 0.00009756
Iteration 109/1000 | Loss: 0.00007246
Iteration 110/1000 | Loss: 0.00009205
Iteration 111/1000 | Loss: 0.00007801
Iteration 112/1000 | Loss: 0.00008209
Iteration 113/1000 | Loss: 0.00007741
Iteration 114/1000 | Loss: 0.00007803
Iteration 115/1000 | Loss: 0.00008777
Iteration 116/1000 | Loss: 0.00009135
Iteration 117/1000 | Loss: 0.00007357
Iteration 118/1000 | Loss: 0.00007364
Iteration 119/1000 | Loss: 0.00008113
Iteration 120/1000 | Loss: 0.00008251
Iteration 121/1000 | Loss: 0.00008501
Iteration 122/1000 | Loss: 0.00009038
Iteration 123/1000 | Loss: 0.00008138
Iteration 124/1000 | Loss: 0.00008498
Iteration 125/1000 | Loss: 0.00008410
Iteration 126/1000 | Loss: 0.00008806
Iteration 127/1000 | Loss: 0.00009543
Iteration 128/1000 | Loss: 0.00008959
Iteration 129/1000 | Loss: 0.00008616
Iteration 130/1000 | Loss: 0.00008277
Iteration 131/1000 | Loss: 0.00007627
Iteration 132/1000 | Loss: 0.00008593
Iteration 133/1000 | Loss: 0.00007938
Iteration 134/1000 | Loss: 0.00008454
Iteration 135/1000 | Loss: 0.00008124
Iteration 136/1000 | Loss: 0.00008575
Iteration 137/1000 | Loss: 0.00008060
Iteration 138/1000 | Loss: 0.00008451
Iteration 139/1000 | Loss: 0.00008512
Iteration 140/1000 | Loss: 0.00008490
Iteration 141/1000 | Loss: 0.00009182
Iteration 142/1000 | Loss: 0.00009081
Iteration 143/1000 | Loss: 0.00008049
Iteration 144/1000 | Loss: 0.00008229
Iteration 145/1000 | Loss: 0.00008037
Iteration 146/1000 | Loss: 0.00007824
Iteration 147/1000 | Loss: 0.00008630
Iteration 148/1000 | Loss: 0.00008681
Iteration 149/1000 | Loss: 0.00008220
Iteration 150/1000 | Loss: 0.00007939
Iteration 151/1000 | Loss: 0.00006598
Iteration 152/1000 | Loss: 0.00007895
Iteration 153/1000 | Loss: 0.00008059
Iteration 154/1000 | Loss: 0.00007966
Iteration 155/1000 | Loss: 0.00008826
Iteration 156/1000 | Loss: 0.00008872
Iteration 157/1000 | Loss: 0.00007101
Iteration 158/1000 | Loss: 0.00006162
Iteration 159/1000 | Loss: 0.00007971
Iteration 160/1000 | Loss: 0.00007638
Iteration 161/1000 | Loss: 0.00008011
Iteration 162/1000 | Loss: 0.00008596
Iteration 163/1000 | Loss: 0.00007709
Iteration 164/1000 | Loss: 0.00009008
Iteration 165/1000 | Loss: 0.00008168
Iteration 166/1000 | Loss: 0.00008651
Iteration 167/1000 | Loss: 0.00007809
Iteration 168/1000 | Loss: 0.00008668
Iteration 169/1000 | Loss: 0.00009446
Iteration 170/1000 | Loss: 0.00008693
Iteration 171/1000 | Loss: 0.00008446
Iteration 172/1000 | Loss: 0.00009080
Iteration 173/1000 | Loss: 0.00008684
Iteration 174/1000 | Loss: 0.00008571
Iteration 175/1000 | Loss: 0.00006347
Iteration 176/1000 | Loss: 0.00009561
Iteration 177/1000 | Loss: 0.00006804
Iteration 178/1000 | Loss: 0.00005337
Iteration 179/1000 | Loss: 0.00005994
Iteration 180/1000 | Loss: 0.00007622
Iteration 181/1000 | Loss: 0.00008338
Iteration 182/1000 | Loss: 0.00008258
Iteration 183/1000 | Loss: 0.00008329
Iteration 184/1000 | Loss: 0.00007905
Iteration 185/1000 | Loss: 0.00006689
Iteration 186/1000 | Loss: 0.00007360
Iteration 187/1000 | Loss: 0.00008219
Iteration 188/1000 | Loss: 0.00008744
Iteration 189/1000 | Loss: 0.00008149
Iteration 190/1000 | Loss: 0.00008510
Iteration 191/1000 | Loss: 0.00008470
Iteration 192/1000 | Loss: 0.00010330
Iteration 193/1000 | Loss: 0.00008063
Iteration 194/1000 | Loss: 0.00008573
Iteration 195/1000 | Loss: 0.00008084
Iteration 196/1000 | Loss: 0.00008877
Iteration 197/1000 | Loss: 0.00006306
Iteration 198/1000 | Loss: 0.00006531
Iteration 199/1000 | Loss: 0.00008137
Iteration 200/1000 | Loss: 0.00008172
Iteration 201/1000 | Loss: 0.00008533
Iteration 202/1000 | Loss: 0.00008309
Iteration 203/1000 | Loss: 0.00009339
Iteration 204/1000 | Loss: 0.00009387
Iteration 205/1000 | Loss: 0.00007270
Iteration 206/1000 | Loss: 0.00008266
Iteration 207/1000 | Loss: 0.00007779
Iteration 208/1000 | Loss: 0.00008014
Iteration 209/1000 | Loss: 0.00008196
Iteration 210/1000 | Loss: 0.00008100
Iteration 211/1000 | Loss: 0.00008846
Iteration 212/1000 | Loss: 0.00005917
Iteration 213/1000 | Loss: 0.00007980
Iteration 214/1000 | Loss: 0.00008347
Iteration 215/1000 | Loss: 0.00008346
Iteration 216/1000 | Loss: 0.00008820
Iteration 217/1000 | Loss: 0.00010524
Iteration 218/1000 | Loss: 0.00008811
Iteration 219/1000 | Loss: 0.00007721
Iteration 220/1000 | Loss: 0.00007144
Iteration 221/1000 | Loss: 0.00010306
Iteration 222/1000 | Loss: 0.00009389
Iteration 223/1000 | Loss: 0.00008333
Iteration 224/1000 | Loss: 0.00005857
Iteration 225/1000 | Loss: 0.00008627
Iteration 226/1000 | Loss: 0.00009620
Iteration 227/1000 | Loss: 0.00007491
Iteration 228/1000 | Loss: 0.00005188
Iteration 229/1000 | Loss: 0.00008096
Iteration 230/1000 | Loss: 0.00009658
Iteration 231/1000 | Loss: 0.00009887
Iteration 232/1000 | Loss: 0.00007139
Iteration 233/1000 | Loss: 0.00007655
Iteration 234/1000 | Loss: 0.00007335
Iteration 235/1000 | Loss: 0.00008092
Iteration 236/1000 | Loss: 0.00007298
Iteration 237/1000 | Loss: 0.00006531
Iteration 238/1000 | Loss: 0.00008825
Iteration 239/1000 | Loss: 0.00008410
Iteration 240/1000 | Loss: 0.00009031
Iteration 241/1000 | Loss: 0.00007952
Iteration 242/1000 | Loss: 0.00008529
Iteration 243/1000 | Loss: 0.00008011
Iteration 244/1000 | Loss: 0.00010145
Iteration 245/1000 | Loss: 0.00008083
Iteration 246/1000 | Loss: 0.00008212
Iteration 247/1000 | Loss: 0.00008777
Iteration 248/1000 | Loss: 0.00008220
Iteration 249/1000 | Loss: 0.00008901
Iteration 250/1000 | Loss: 0.00008450
Iteration 251/1000 | Loss: 0.00007494
Iteration 252/1000 | Loss: 0.00007712
Iteration 253/1000 | Loss: 0.00010094
Iteration 254/1000 | Loss: 0.00008634
Iteration 255/1000 | Loss: 0.00008683
Iteration 256/1000 | Loss: 0.00008814
Iteration 257/1000 | Loss: 0.00008874
Iteration 258/1000 | Loss: 0.00009412
Iteration 259/1000 | Loss: 0.00009084
Iteration 260/1000 | Loss: 0.00009065
Iteration 261/1000 | Loss: 0.00006677
Iteration 262/1000 | Loss: 0.00007889
Iteration 263/1000 | Loss: 0.00009015
Iteration 264/1000 | Loss: 0.00007453
Iteration 265/1000 | Loss: 0.00009010
Iteration 266/1000 | Loss: 0.00008932
Iteration 267/1000 | Loss: 0.00008552
Iteration 268/1000 | Loss: 0.00009286
Iteration 269/1000 | Loss: 0.00008716
Iteration 270/1000 | Loss: 0.00009174
Iteration 271/1000 | Loss: 0.00007497
Iteration 272/1000 | Loss: 0.00007606
Iteration 273/1000 | Loss: 0.00006145
Iteration 274/1000 | Loss: 0.00004393
Iteration 275/1000 | Loss: 0.00006937
Iteration 276/1000 | Loss: 0.00007114
Iteration 277/1000 | Loss: 0.00006823
Iteration 278/1000 | Loss: 0.00007273
Iteration 279/1000 | Loss: 0.00007279
Iteration 280/1000 | Loss: 0.00007387
Iteration 281/1000 | Loss: 0.00007234
Iteration 282/1000 | Loss: 0.00007568
Iteration 283/1000 | Loss: 0.00007421
Iteration 284/1000 | Loss: 0.00008726
Iteration 285/1000 | Loss: 0.00008943
Iteration 286/1000 | Loss: 0.00008730
Iteration 287/1000 | Loss: 0.00007643
Iteration 288/1000 | Loss: 0.00007304
Iteration 289/1000 | Loss: 0.00007401
Iteration 290/1000 | Loss: 0.00008590
Iteration 291/1000 | Loss: 0.00008337
Iteration 292/1000 | Loss: 0.00005595
Iteration 293/1000 | Loss: 0.00007196
Iteration 294/1000 | Loss: 0.00008134
Iteration 295/1000 | Loss: 0.00008306
Iteration 296/1000 | Loss: 0.00008326
Iteration 297/1000 | Loss: 0.00007932
Iteration 298/1000 | Loss: 0.00008395
Iteration 299/1000 | Loss: 0.00008289
Iteration 300/1000 | Loss: 0.00005509
Iteration 301/1000 | Loss: 0.00007060
Iteration 302/1000 | Loss: 0.00004144
Iteration 303/1000 | Loss: 0.00005415
Iteration 304/1000 | Loss: 0.00005171
Iteration 305/1000 | Loss: 0.00005138
Iteration 306/1000 | Loss: 0.00004825
Iteration 307/1000 | Loss: 0.00004905
Iteration 308/1000 | Loss: 0.00003727
Iteration 309/1000 | Loss: 0.00004717
Iteration 310/1000 | Loss: 0.00005037
Iteration 311/1000 | Loss: 0.00005237
Iteration 312/1000 | Loss: 0.00006084
Iteration 313/1000 | Loss: 0.00004716
Iteration 314/1000 | Loss: 0.00005481
Iteration 315/1000 | Loss: 0.00004674
Iteration 316/1000 | Loss: 0.00005311
Iteration 317/1000 | Loss: 0.00005610
Iteration 318/1000 | Loss: 0.00006455
Iteration 319/1000 | Loss: 0.00004564
Iteration 320/1000 | Loss: 0.00003889
Iteration 321/1000 | Loss: 0.00004128
Iteration 322/1000 | Loss: 0.00004638
Iteration 323/1000 | Loss: 0.00004414
Iteration 324/1000 | Loss: 0.00003673
Iteration 325/1000 | Loss: 0.00004165
Iteration 326/1000 | Loss: 0.00003232
Iteration 327/1000 | Loss: 0.00004141
Iteration 328/1000 | Loss: 0.00003793
Iteration 329/1000 | Loss: 0.00004418
Iteration 330/1000 | Loss: 0.00004175
Iteration 331/1000 | Loss: 0.00003413
Iteration 332/1000 | Loss: 0.00004216
Iteration 333/1000 | Loss: 0.00004280
Iteration 334/1000 | Loss: 0.00004363
Iteration 335/1000 | Loss: 0.00004213
Iteration 336/1000 | Loss: 0.00004017
Iteration 337/1000 | Loss: 0.00004051
Iteration 338/1000 | Loss: 0.00004228
Iteration 339/1000 | Loss: 0.00004276
Iteration 340/1000 | Loss: 0.00004241
Iteration 341/1000 | Loss: 0.00004211
Iteration 342/1000 | Loss: 0.00004130
Iteration 343/1000 | Loss: 0.00005170
Iteration 344/1000 | Loss: 0.00004163
Iteration 345/1000 | Loss: 0.00004775
Iteration 346/1000 | Loss: 0.00004105
Iteration 347/1000 | Loss: 0.00004090
Iteration 348/1000 | Loss: 0.00004055
Iteration 349/1000 | Loss: 0.00003887
Iteration 350/1000 | Loss: 0.00004879
Iteration 351/1000 | Loss: 0.00003244
Iteration 352/1000 | Loss: 0.00003001
Iteration 353/1000 | Loss: 0.00002882
Iteration 354/1000 | Loss: 0.00002805
Iteration 355/1000 | Loss: 0.00002755
Iteration 356/1000 | Loss: 0.00002726
Iteration 357/1000 | Loss: 0.00002720
Iteration 358/1000 | Loss: 0.00002716
Iteration 359/1000 | Loss: 0.00002710
Iteration 360/1000 | Loss: 0.00002710
Iteration 361/1000 | Loss: 0.00002709
Iteration 362/1000 | Loss: 0.00002709
Iteration 363/1000 | Loss: 0.00002709
Iteration 364/1000 | Loss: 0.00002709
Iteration 365/1000 | Loss: 0.00002709
Iteration 366/1000 | Loss: 0.00002709
Iteration 367/1000 | Loss: 0.00002709
Iteration 368/1000 | Loss: 0.00002709
Iteration 369/1000 | Loss: 0.00002709
Iteration 370/1000 | Loss: 0.00002709
Iteration 371/1000 | Loss: 0.00002709
Iteration 372/1000 | Loss: 0.00002709
Iteration 373/1000 | Loss: 0.00002708
Iteration 374/1000 | Loss: 0.00002708
Iteration 375/1000 | Loss: 0.00002707
Iteration 376/1000 | Loss: 0.00002705
Iteration 377/1000 | Loss: 0.00002705
Iteration 378/1000 | Loss: 0.00002705
Iteration 379/1000 | Loss: 0.00002705
Iteration 380/1000 | Loss: 0.00002705
Iteration 381/1000 | Loss: 0.00002705
Iteration 382/1000 | Loss: 0.00002705
Iteration 383/1000 | Loss: 0.00002705
Iteration 384/1000 | Loss: 0.00002704
Iteration 385/1000 | Loss: 0.00002702
Iteration 386/1000 | Loss: 0.00002700
Iteration 387/1000 | Loss: 0.00002699
Iteration 388/1000 | Loss: 0.00002699
Iteration 389/1000 | Loss: 0.00002698
Iteration 390/1000 | Loss: 0.00002698
Iteration 391/1000 | Loss: 0.00002697
Iteration 392/1000 | Loss: 0.00002697
Iteration 393/1000 | Loss: 0.00002697
Iteration 394/1000 | Loss: 0.00002697
Iteration 395/1000 | Loss: 0.00002696
Iteration 396/1000 | Loss: 0.00002696
Iteration 397/1000 | Loss: 0.00002696
Iteration 398/1000 | Loss: 0.00002696
Iteration 399/1000 | Loss: 0.00002695
Iteration 400/1000 | Loss: 0.00002695
Iteration 401/1000 | Loss: 0.00002695
Iteration 402/1000 | Loss: 0.00002695
Iteration 403/1000 | Loss: 0.00002695
Iteration 404/1000 | Loss: 0.00002695
Iteration 405/1000 | Loss: 0.00002695
Iteration 406/1000 | Loss: 0.00002694
Iteration 407/1000 | Loss: 0.00002694
Iteration 408/1000 | Loss: 0.00002693
Iteration 409/1000 | Loss: 0.00002693
Iteration 410/1000 | Loss: 0.00002693
Iteration 411/1000 | Loss: 0.00002693
Iteration 412/1000 | Loss: 0.00002693
Iteration 413/1000 | Loss: 0.00002693
Iteration 414/1000 | Loss: 0.00002692
Iteration 415/1000 | Loss: 0.00002692
Iteration 416/1000 | Loss: 0.00002691
Iteration 417/1000 | Loss: 0.00002691
Iteration 418/1000 | Loss: 0.00002690
Iteration 419/1000 | Loss: 0.00002690
Iteration 420/1000 | Loss: 0.00002689
Iteration 421/1000 | Loss: 0.00002689
Iteration 422/1000 | Loss: 0.00002688
Iteration 423/1000 | Loss: 0.00002688
Iteration 424/1000 | Loss: 0.00002684
Iteration 425/1000 | Loss: 0.00002684
Iteration 426/1000 | Loss: 0.00002683
Iteration 427/1000 | Loss: 0.00002683
Iteration 428/1000 | Loss: 0.00002682
Iteration 429/1000 | Loss: 0.00002682
Iteration 430/1000 | Loss: 0.00002682
Iteration 431/1000 | Loss: 0.00002681
Iteration 432/1000 | Loss: 0.00002681
Iteration 433/1000 | Loss: 0.00002681
Iteration 434/1000 | Loss: 0.00002681
Iteration 435/1000 | Loss: 0.00002681
Iteration 436/1000 | Loss: 0.00002681
Iteration 437/1000 | Loss: 0.00002681
Iteration 438/1000 | Loss: 0.00002680
Iteration 439/1000 | Loss: 0.00002680
Iteration 440/1000 | Loss: 0.00002680
Iteration 441/1000 | Loss: 0.00002680
Iteration 442/1000 | Loss: 0.00002679
Iteration 443/1000 | Loss: 0.00002679
Iteration 444/1000 | Loss: 0.00002679
Iteration 445/1000 | Loss: 0.00002679
Iteration 446/1000 | Loss: 0.00002679
Iteration 447/1000 | Loss: 0.00002679
Iteration 448/1000 | Loss: 0.00002679
Iteration 449/1000 | Loss: 0.00002679
Iteration 450/1000 | Loss: 0.00002679
Iteration 451/1000 | Loss: 0.00002679
Iteration 452/1000 | Loss: 0.00002679
Iteration 453/1000 | Loss: 0.00002679
Iteration 454/1000 | Loss: 0.00002679
Iteration 455/1000 | Loss: 0.00002679
Iteration 456/1000 | Loss: 0.00002679
Iteration 457/1000 | Loss: 0.00002679
Iteration 458/1000 | Loss: 0.00002679
Iteration 459/1000 | Loss: 0.00002679
Iteration 460/1000 | Loss: 0.00002679
Iteration 461/1000 | Loss: 0.00002679
Iteration 462/1000 | Loss: 0.00002679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 462. Stopping optimization.
Last 5 losses: [2.6787400202010758e-05, 2.6787400202010758e-05, 2.6787400202010758e-05, 2.6787400202010758e-05, 2.6787400202010758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6787400202010758e-05

Optimization complete. Final v2v error: 4.110921859741211 mm

Highest mean error: 4.921914100646973 mm for frame 222

Lowest mean error: 3.8252179622650146 mm for frame 0

Saving results

Total time: 618.5716948509216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398198
Iteration 2/25 | Loss: 0.00083930
Iteration 3/25 | Loss: 0.00066872
Iteration 4/25 | Loss: 0.00065163
Iteration 5/25 | Loss: 0.00064368
Iteration 6/25 | Loss: 0.00064232
Iteration 7/25 | Loss: 0.00064232
Iteration 8/25 | Loss: 0.00064232
Iteration 9/25 | Loss: 0.00064232
Iteration 10/25 | Loss: 0.00064232
Iteration 11/25 | Loss: 0.00064232
Iteration 12/25 | Loss: 0.00064232
Iteration 13/25 | Loss: 0.00064232
Iteration 14/25 | Loss: 0.00064232
Iteration 15/25 | Loss: 0.00064232
Iteration 16/25 | Loss: 0.00064232
Iteration 17/25 | Loss: 0.00064232
Iteration 18/25 | Loss: 0.00064232
Iteration 19/25 | Loss: 0.00064232
Iteration 20/25 | Loss: 0.00064232
Iteration 21/25 | Loss: 0.00064232
Iteration 22/25 | Loss: 0.00064232
Iteration 23/25 | Loss: 0.00064232
Iteration 24/25 | Loss: 0.00064232
Iteration 25/25 | Loss: 0.00064232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73419297
Iteration 2/25 | Loss: 0.00032375
Iteration 3/25 | Loss: 0.00032371
Iteration 4/25 | Loss: 0.00032371
Iteration 5/25 | Loss: 0.00032371
Iteration 6/25 | Loss: 0.00032371
Iteration 7/25 | Loss: 0.00032371
Iteration 8/25 | Loss: 0.00032371
Iteration 9/25 | Loss: 0.00032371
Iteration 10/25 | Loss: 0.00032371
Iteration 11/25 | Loss: 0.00032371
Iteration 12/25 | Loss: 0.00032371
Iteration 13/25 | Loss: 0.00032371
Iteration 14/25 | Loss: 0.00032371
Iteration 15/25 | Loss: 0.00032371
Iteration 16/25 | Loss: 0.00032371
Iteration 17/25 | Loss: 0.00032371
Iteration 18/25 | Loss: 0.00032371
Iteration 19/25 | Loss: 0.00032371
Iteration 20/25 | Loss: 0.00032371
Iteration 21/25 | Loss: 0.00032371
Iteration 22/25 | Loss: 0.00032371
Iteration 23/25 | Loss: 0.00032371
Iteration 24/25 | Loss: 0.00032371
Iteration 25/25 | Loss: 0.00032371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032371
Iteration 2/1000 | Loss: 0.00002446
Iteration 3/1000 | Loss: 0.00001565
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001378
Iteration 6/1000 | Loss: 0.00001328
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001263
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001235
Iteration 12/1000 | Loss: 0.00001235
Iteration 13/1000 | Loss: 0.00001235
Iteration 14/1000 | Loss: 0.00001235
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00001229
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001225
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001221
Iteration 33/1000 | Loss: 0.00001221
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.213869109051302e-05, 1.213869109051302e-05, 1.213869109051302e-05, 1.213869109051302e-05, 1.213869109051302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.213869109051302e-05

Optimization complete. Final v2v error: 2.9793052673339844 mm

Highest mean error: 3.1889727115631104 mm for frame 3

Lowest mean error: 2.795313835144043 mm for frame 195

Saving results

Total time: 33.3237144947052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00662659
Iteration 2/25 | Loss: 0.00124625
Iteration 3/25 | Loss: 0.00079706
Iteration 4/25 | Loss: 0.00071734
Iteration 5/25 | Loss: 0.00069647
Iteration 6/25 | Loss: 0.00069158
Iteration 7/25 | Loss: 0.00069389
Iteration 8/25 | Loss: 0.00069370
Iteration 9/25 | Loss: 0.00069231
Iteration 10/25 | Loss: 0.00068928
Iteration 11/25 | Loss: 0.00068734
Iteration 12/25 | Loss: 0.00068697
Iteration 13/25 | Loss: 0.00068674
Iteration 14/25 | Loss: 0.00068672
Iteration 15/25 | Loss: 0.00068671
Iteration 16/25 | Loss: 0.00068670
Iteration 17/25 | Loss: 0.00068670
Iteration 18/25 | Loss: 0.00068670
Iteration 19/25 | Loss: 0.00068669
Iteration 20/25 | Loss: 0.00068669
Iteration 21/25 | Loss: 0.00068669
Iteration 22/25 | Loss: 0.00068669
Iteration 23/25 | Loss: 0.00068669
Iteration 24/25 | Loss: 0.00068669
Iteration 25/25 | Loss: 0.00068669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95949602
Iteration 2/25 | Loss: 0.00059661
Iteration 3/25 | Loss: 0.00059661
Iteration 4/25 | Loss: 0.00059661
Iteration 5/25 | Loss: 0.00059661
Iteration 6/25 | Loss: 0.00059661
Iteration 7/25 | Loss: 0.00059661
Iteration 8/25 | Loss: 0.00059661
Iteration 9/25 | Loss: 0.00059661
Iteration 10/25 | Loss: 0.00059661
Iteration 11/25 | Loss: 0.00059661
Iteration 12/25 | Loss: 0.00059661
Iteration 13/25 | Loss: 0.00059661
Iteration 14/25 | Loss: 0.00059661
Iteration 15/25 | Loss: 0.00059661
Iteration 16/25 | Loss: 0.00059661
Iteration 17/25 | Loss: 0.00059661
Iteration 18/25 | Loss: 0.00059661
Iteration 19/25 | Loss: 0.00059661
Iteration 20/25 | Loss: 0.00059661
Iteration 21/25 | Loss: 0.00059661
Iteration 22/25 | Loss: 0.00059661
Iteration 23/25 | Loss: 0.00059661
Iteration 24/25 | Loss: 0.00059661
Iteration 25/25 | Loss: 0.00059661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000596606289036572, 0.000596606289036572, 0.000596606289036572, 0.000596606289036572, 0.000596606289036572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000596606289036572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059661
Iteration 2/1000 | Loss: 0.00006511
Iteration 3/1000 | Loss: 0.00004726
Iteration 4/1000 | Loss: 0.00004127
Iteration 5/1000 | Loss: 0.00005632
Iteration 6/1000 | Loss: 0.00004005
Iteration 7/1000 | Loss: 0.00003218
Iteration 8/1000 | Loss: 0.00003049
Iteration 9/1000 | Loss: 0.00002947
Iteration 10/1000 | Loss: 0.00003139
Iteration 11/1000 | Loss: 0.00002995
Iteration 12/1000 | Loss: 0.00002859
Iteration 13/1000 | Loss: 0.00003330
Iteration 14/1000 | Loss: 0.00002822
Iteration 15/1000 | Loss: 0.00002796
Iteration 16/1000 | Loss: 0.00002771
Iteration 17/1000 | Loss: 0.00002766
Iteration 18/1000 | Loss: 0.00002766
Iteration 19/1000 | Loss: 0.00002765
Iteration 20/1000 | Loss: 0.00002765
Iteration 21/1000 | Loss: 0.00002764
Iteration 22/1000 | Loss: 0.00002764
Iteration 23/1000 | Loss: 0.00002762
Iteration 24/1000 | Loss: 0.00002748
Iteration 25/1000 | Loss: 0.00002726
Iteration 26/1000 | Loss: 0.00002712
Iteration 27/1000 | Loss: 0.00004454
Iteration 28/1000 | Loss: 0.00002680
Iteration 29/1000 | Loss: 0.00070495
Iteration 30/1000 | Loss: 0.00004555
Iteration 31/1000 | Loss: 0.00066067
Iteration 32/1000 | Loss: 0.00004914
Iteration 33/1000 | Loss: 0.00080966
Iteration 34/1000 | Loss: 0.00004987
Iteration 35/1000 | Loss: 0.00004462
Iteration 36/1000 | Loss: 0.00004055
Iteration 37/1000 | Loss: 0.00003096
Iteration 38/1000 | Loss: 0.00074769
Iteration 39/1000 | Loss: 0.00004154
Iteration 40/1000 | Loss: 0.00003364
Iteration 41/1000 | Loss: 0.00003152
Iteration 42/1000 | Loss: 0.00026217
Iteration 43/1000 | Loss: 0.00007024
Iteration 44/1000 | Loss: 0.00003629
Iteration 45/1000 | Loss: 0.00003120
Iteration 46/1000 | Loss: 0.00072193
Iteration 47/1000 | Loss: 0.00060684
Iteration 48/1000 | Loss: 0.00003361
Iteration 49/1000 | Loss: 0.00004662
Iteration 50/1000 | Loss: 0.00002527
Iteration 51/1000 | Loss: 0.00002350
Iteration 52/1000 | Loss: 0.00002176
Iteration 53/1000 | Loss: 0.00002087
Iteration 54/1000 | Loss: 0.00002931
Iteration 55/1000 | Loss: 0.00002031
Iteration 56/1000 | Loss: 0.00002630
Iteration 57/1000 | Loss: 0.00002015
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00002014
Iteration 60/1000 | Loss: 0.00002013
Iteration 61/1000 | Loss: 0.00002013
Iteration 62/1000 | Loss: 0.00002013
Iteration 63/1000 | Loss: 0.00001997
Iteration 64/1000 | Loss: 0.00001986
Iteration 65/1000 | Loss: 0.00001979
Iteration 66/1000 | Loss: 0.00001975
Iteration 67/1000 | Loss: 0.00001971
Iteration 68/1000 | Loss: 0.00001970
Iteration 69/1000 | Loss: 0.00001968
Iteration 70/1000 | Loss: 0.00001967
Iteration 71/1000 | Loss: 0.00001967
Iteration 72/1000 | Loss: 0.00001966
Iteration 73/1000 | Loss: 0.00001966
Iteration 74/1000 | Loss: 0.00001965
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001964
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001960
Iteration 91/1000 | Loss: 0.00001959
Iteration 92/1000 | Loss: 0.00001959
Iteration 93/1000 | Loss: 0.00001959
Iteration 94/1000 | Loss: 0.00001959
Iteration 95/1000 | Loss: 0.00001959
Iteration 96/1000 | Loss: 0.00001959
Iteration 97/1000 | Loss: 0.00001959
Iteration 98/1000 | Loss: 0.00001959
Iteration 99/1000 | Loss: 0.00001959
Iteration 100/1000 | Loss: 0.00001959
Iteration 101/1000 | Loss: 0.00001959
Iteration 102/1000 | Loss: 0.00001958
Iteration 103/1000 | Loss: 0.00004690
Iteration 104/1000 | Loss: 0.00001964
Iteration 105/1000 | Loss: 0.00001956
Iteration 106/1000 | Loss: 0.00001955
Iteration 107/1000 | Loss: 0.00001954
Iteration 108/1000 | Loss: 0.00001954
Iteration 109/1000 | Loss: 0.00001954
Iteration 110/1000 | Loss: 0.00001953
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001953
Iteration 115/1000 | Loss: 0.00001953
Iteration 116/1000 | Loss: 0.00001952
Iteration 117/1000 | Loss: 0.00001952
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001952
Iteration 120/1000 | Loss: 0.00001952
Iteration 121/1000 | Loss: 0.00001952
Iteration 122/1000 | Loss: 0.00001952
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001952
Iteration 128/1000 | Loss: 0.00001951
Iteration 129/1000 | Loss: 0.00001951
Iteration 130/1000 | Loss: 0.00001951
Iteration 131/1000 | Loss: 0.00001951
Iteration 132/1000 | Loss: 0.00001951
Iteration 133/1000 | Loss: 0.00001950
Iteration 134/1000 | Loss: 0.00001950
Iteration 135/1000 | Loss: 0.00001950
Iteration 136/1000 | Loss: 0.00001950
Iteration 137/1000 | Loss: 0.00001950
Iteration 138/1000 | Loss: 0.00001950
Iteration 139/1000 | Loss: 0.00001950
Iteration 140/1000 | Loss: 0.00001950
Iteration 141/1000 | Loss: 0.00001950
Iteration 142/1000 | Loss: 0.00001950
Iteration 143/1000 | Loss: 0.00001949
Iteration 144/1000 | Loss: 0.00001949
Iteration 145/1000 | Loss: 0.00001949
Iteration 146/1000 | Loss: 0.00001949
Iteration 147/1000 | Loss: 0.00001949
Iteration 148/1000 | Loss: 0.00001949
Iteration 149/1000 | Loss: 0.00001949
Iteration 150/1000 | Loss: 0.00001948
Iteration 151/1000 | Loss: 0.00001948
Iteration 152/1000 | Loss: 0.00001948
Iteration 153/1000 | Loss: 0.00001948
Iteration 154/1000 | Loss: 0.00001948
Iteration 155/1000 | Loss: 0.00001948
Iteration 156/1000 | Loss: 0.00001948
Iteration 157/1000 | Loss: 0.00001948
Iteration 158/1000 | Loss: 0.00001948
Iteration 159/1000 | Loss: 0.00001948
Iteration 160/1000 | Loss: 0.00001948
Iteration 161/1000 | Loss: 0.00001948
Iteration 162/1000 | Loss: 0.00001947
Iteration 163/1000 | Loss: 0.00001947
Iteration 164/1000 | Loss: 0.00001947
Iteration 165/1000 | Loss: 0.00001947
Iteration 166/1000 | Loss: 0.00001947
Iteration 167/1000 | Loss: 0.00001947
Iteration 168/1000 | Loss: 0.00001946
Iteration 169/1000 | Loss: 0.00001946
Iteration 170/1000 | Loss: 0.00001946
Iteration 171/1000 | Loss: 0.00001946
Iteration 172/1000 | Loss: 0.00001945
Iteration 173/1000 | Loss: 0.00001944
Iteration 174/1000 | Loss: 0.00001944
Iteration 175/1000 | Loss: 0.00001944
Iteration 176/1000 | Loss: 0.00001943
Iteration 177/1000 | Loss: 0.00001943
Iteration 178/1000 | Loss: 0.00001943
Iteration 179/1000 | Loss: 0.00001943
Iteration 180/1000 | Loss: 0.00004166
Iteration 181/1000 | Loss: 0.00002076
Iteration 182/1000 | Loss: 0.00001942
Iteration 183/1000 | Loss: 0.00001942
Iteration 184/1000 | Loss: 0.00001941
Iteration 185/1000 | Loss: 0.00001941
Iteration 186/1000 | Loss: 0.00001941
Iteration 187/1000 | Loss: 0.00001941
Iteration 188/1000 | Loss: 0.00001941
Iteration 189/1000 | Loss: 0.00001941
Iteration 190/1000 | Loss: 0.00001941
Iteration 191/1000 | Loss: 0.00001941
Iteration 192/1000 | Loss: 0.00001941
Iteration 193/1000 | Loss: 0.00001940
Iteration 194/1000 | Loss: 0.00001940
Iteration 195/1000 | Loss: 0.00001940
Iteration 196/1000 | Loss: 0.00001940
Iteration 197/1000 | Loss: 0.00001940
Iteration 198/1000 | Loss: 0.00001940
Iteration 199/1000 | Loss: 0.00001940
Iteration 200/1000 | Loss: 0.00001940
Iteration 201/1000 | Loss: 0.00001940
Iteration 202/1000 | Loss: 0.00001940
Iteration 203/1000 | Loss: 0.00001940
Iteration 204/1000 | Loss: 0.00001940
Iteration 205/1000 | Loss: 0.00001940
Iteration 206/1000 | Loss: 0.00001940
Iteration 207/1000 | Loss: 0.00001940
Iteration 208/1000 | Loss: 0.00001940
Iteration 209/1000 | Loss: 0.00001940
Iteration 210/1000 | Loss: 0.00001940
Iteration 211/1000 | Loss: 0.00001940
Iteration 212/1000 | Loss: 0.00001940
Iteration 213/1000 | Loss: 0.00001940
Iteration 214/1000 | Loss: 0.00001940
Iteration 215/1000 | Loss: 0.00001940
Iteration 216/1000 | Loss: 0.00001940
Iteration 217/1000 | Loss: 0.00001940
Iteration 218/1000 | Loss: 0.00001940
Iteration 219/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.9400824385229498e-05, 1.9400824385229498e-05, 1.9400824385229498e-05, 1.9400824385229498e-05, 1.9400824385229498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9400824385229498e-05

Optimization complete. Final v2v error: 3.473240375518799 mm

Highest mean error: 12.796283721923828 mm for frame 9

Lowest mean error: 2.8600142002105713 mm for frame 202

Saving results

Total time: 125.80870294570923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059875
Iteration 2/25 | Loss: 0.00212470
Iteration 3/25 | Loss: 0.00280360
Iteration 4/25 | Loss: 0.00171413
Iteration 5/25 | Loss: 0.00114409
Iteration 6/25 | Loss: 0.00088923
Iteration 7/25 | Loss: 0.00088508
Iteration 8/25 | Loss: 0.00090446
Iteration 9/25 | Loss: 0.00086824
Iteration 10/25 | Loss: 0.00083783
Iteration 11/25 | Loss: 0.00080851
Iteration 12/25 | Loss: 0.00077215
Iteration 13/25 | Loss: 0.00075967
Iteration 14/25 | Loss: 0.00075568
Iteration 15/25 | Loss: 0.00075448
Iteration 16/25 | Loss: 0.00074797
Iteration 17/25 | Loss: 0.00074386
Iteration 18/25 | Loss: 0.00074115
Iteration 19/25 | Loss: 0.00073865
Iteration 20/25 | Loss: 0.00073677
Iteration 21/25 | Loss: 0.00073922
Iteration 22/25 | Loss: 0.00073670
Iteration 23/25 | Loss: 0.00074240
Iteration 24/25 | Loss: 0.00074111
Iteration 25/25 | Loss: 0.00073943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52468884
Iteration 2/25 | Loss: 0.00183955
Iteration 3/25 | Loss: 0.00070330
Iteration 4/25 | Loss: 0.00070326
Iteration 5/25 | Loss: 0.00070326
Iteration 6/25 | Loss: 0.00070326
Iteration 7/25 | Loss: 0.00070326
Iteration 8/25 | Loss: 0.00070326
Iteration 9/25 | Loss: 0.00070326
Iteration 10/25 | Loss: 0.00070326
Iteration 11/25 | Loss: 0.00070326
Iteration 12/25 | Loss: 0.00070326
Iteration 13/25 | Loss: 0.00070326
Iteration 14/25 | Loss: 0.00070326
Iteration 15/25 | Loss: 0.00070326
Iteration 16/25 | Loss: 0.00070326
Iteration 17/25 | Loss: 0.00070326
Iteration 18/25 | Loss: 0.00070326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007032583234831691, 0.0007032583234831691, 0.0007032583234831691, 0.0007032583234831691, 0.0007032583234831691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007032583234831691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070326
Iteration 2/1000 | Loss: 0.00137991
Iteration 3/1000 | Loss: 0.00031429
Iteration 4/1000 | Loss: 0.00036216
Iteration 5/1000 | Loss: 0.00090976
Iteration 6/1000 | Loss: 0.00029731
Iteration 7/1000 | Loss: 0.00020769
Iteration 8/1000 | Loss: 0.00036793
Iteration 9/1000 | Loss: 0.00065824
Iteration 10/1000 | Loss: 0.00003614
Iteration 11/1000 | Loss: 0.00084037
Iteration 12/1000 | Loss: 0.00072778
Iteration 13/1000 | Loss: 0.00009220
Iteration 14/1000 | Loss: 0.00029258
Iteration 15/1000 | Loss: 0.00047809
Iteration 16/1000 | Loss: 0.00052266
Iteration 17/1000 | Loss: 0.00184577
Iteration 18/1000 | Loss: 0.00102771
Iteration 19/1000 | Loss: 0.00058328
Iteration 20/1000 | Loss: 0.00117897
Iteration 21/1000 | Loss: 0.00019878
Iteration 22/1000 | Loss: 0.00011166
Iteration 23/1000 | Loss: 0.00012397
Iteration 24/1000 | Loss: 0.00004179
Iteration 25/1000 | Loss: 0.00009946
Iteration 26/1000 | Loss: 0.00008057
Iteration 27/1000 | Loss: 0.00004773
Iteration 28/1000 | Loss: 0.00016683
Iteration 29/1000 | Loss: 0.00007439
Iteration 30/1000 | Loss: 0.00024721
Iteration 31/1000 | Loss: 0.00011563
Iteration 32/1000 | Loss: 0.00019810
Iteration 33/1000 | Loss: 0.00011705
Iteration 34/1000 | Loss: 0.00003876
Iteration 35/1000 | Loss: 0.00010276
Iteration 36/1000 | Loss: 0.00002962
Iteration 37/1000 | Loss: 0.00006316
Iteration 38/1000 | Loss: 0.00006394
Iteration 39/1000 | Loss: 0.00007507
Iteration 40/1000 | Loss: 0.00006433
Iteration 41/1000 | Loss: 0.00007325
Iteration 42/1000 | Loss: 0.00005901
Iteration 43/1000 | Loss: 0.00004827
Iteration 44/1000 | Loss: 0.00005922
Iteration 45/1000 | Loss: 0.00004537
Iteration 46/1000 | Loss: 0.00004632
Iteration 47/1000 | Loss: 0.00002777
Iteration 48/1000 | Loss: 0.00005100
Iteration 49/1000 | Loss: 0.00008274
Iteration 50/1000 | Loss: 0.00004969
Iteration 51/1000 | Loss: 0.00007631
Iteration 52/1000 | Loss: 0.00006379
Iteration 53/1000 | Loss: 0.00006958
Iteration 54/1000 | Loss: 0.00006018
Iteration 55/1000 | Loss: 0.00007151
Iteration 56/1000 | Loss: 0.00004292
Iteration 57/1000 | Loss: 0.00005580
Iteration 58/1000 | Loss: 0.00012727
Iteration 59/1000 | Loss: 0.00007185
Iteration 60/1000 | Loss: 0.00006824
Iteration 61/1000 | Loss: 0.00023684
Iteration 62/1000 | Loss: 0.00006525
Iteration 63/1000 | Loss: 0.00006560
Iteration 64/1000 | Loss: 0.00024730
Iteration 65/1000 | Loss: 0.00008177
Iteration 66/1000 | Loss: 0.00013408
Iteration 67/1000 | Loss: 0.00007779
Iteration 68/1000 | Loss: 0.00012289
Iteration 69/1000 | Loss: 0.00019034
Iteration 70/1000 | Loss: 0.00013128
Iteration 71/1000 | Loss: 0.00002701
Iteration 72/1000 | Loss: 0.00002258
Iteration 73/1000 | Loss: 0.00002136
Iteration 74/1000 | Loss: 0.00002074
Iteration 75/1000 | Loss: 0.00002034
Iteration 76/1000 | Loss: 0.00002005
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001950
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001935
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001924
Iteration 84/1000 | Loss: 0.00001923
Iteration 85/1000 | Loss: 0.00001920
Iteration 86/1000 | Loss: 0.00001918
Iteration 87/1000 | Loss: 0.00001918
Iteration 88/1000 | Loss: 0.00001918
Iteration 89/1000 | Loss: 0.00001918
Iteration 90/1000 | Loss: 0.00001918
Iteration 91/1000 | Loss: 0.00001918
Iteration 92/1000 | Loss: 0.00001918
Iteration 93/1000 | Loss: 0.00001917
Iteration 94/1000 | Loss: 0.00001917
Iteration 95/1000 | Loss: 0.00001917
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001916
Iteration 98/1000 | Loss: 0.00001916
Iteration 99/1000 | Loss: 0.00001916
Iteration 100/1000 | Loss: 0.00001915
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001914
Iteration 107/1000 | Loss: 0.00001914
Iteration 108/1000 | Loss: 0.00001914
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001913
Iteration 111/1000 | Loss: 0.00001913
Iteration 112/1000 | Loss: 0.00001913
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00001910
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001909
Iteration 118/1000 | Loss: 0.00001909
Iteration 119/1000 | Loss: 0.00001909
Iteration 120/1000 | Loss: 0.00001908
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001905
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001900
Iteration 131/1000 | Loss: 0.00001900
Iteration 132/1000 | Loss: 0.00001899
Iteration 133/1000 | Loss: 0.00001899
Iteration 134/1000 | Loss: 0.00001898
Iteration 135/1000 | Loss: 0.00001898
Iteration 136/1000 | Loss: 0.00001898
Iteration 137/1000 | Loss: 0.00001897
Iteration 138/1000 | Loss: 0.00001897
Iteration 139/1000 | Loss: 0.00001897
Iteration 140/1000 | Loss: 0.00001897
Iteration 141/1000 | Loss: 0.00001897
Iteration 142/1000 | Loss: 0.00001897
Iteration 143/1000 | Loss: 0.00001896
Iteration 144/1000 | Loss: 0.00001896
Iteration 145/1000 | Loss: 0.00001896
Iteration 146/1000 | Loss: 0.00001896
Iteration 147/1000 | Loss: 0.00001895
Iteration 148/1000 | Loss: 0.00001895
Iteration 149/1000 | Loss: 0.00001895
Iteration 150/1000 | Loss: 0.00001895
Iteration 151/1000 | Loss: 0.00001895
Iteration 152/1000 | Loss: 0.00001895
Iteration 153/1000 | Loss: 0.00001895
Iteration 154/1000 | Loss: 0.00001895
Iteration 155/1000 | Loss: 0.00001895
Iteration 156/1000 | Loss: 0.00001895
Iteration 157/1000 | Loss: 0.00001895
Iteration 158/1000 | Loss: 0.00001895
Iteration 159/1000 | Loss: 0.00001895
Iteration 160/1000 | Loss: 0.00001895
Iteration 161/1000 | Loss: 0.00001895
Iteration 162/1000 | Loss: 0.00001895
Iteration 163/1000 | Loss: 0.00001894
Iteration 164/1000 | Loss: 0.00001894
Iteration 165/1000 | Loss: 0.00001894
Iteration 166/1000 | Loss: 0.00001894
Iteration 167/1000 | Loss: 0.00001894
Iteration 168/1000 | Loss: 0.00001894
Iteration 169/1000 | Loss: 0.00001894
Iteration 170/1000 | Loss: 0.00001894
Iteration 171/1000 | Loss: 0.00001894
Iteration 172/1000 | Loss: 0.00001894
Iteration 173/1000 | Loss: 0.00001894
Iteration 174/1000 | Loss: 0.00001894
Iteration 175/1000 | Loss: 0.00001894
Iteration 176/1000 | Loss: 0.00001894
Iteration 177/1000 | Loss: 0.00001894
Iteration 178/1000 | Loss: 0.00001893
Iteration 179/1000 | Loss: 0.00001893
Iteration 180/1000 | Loss: 0.00001893
Iteration 181/1000 | Loss: 0.00001893
Iteration 182/1000 | Loss: 0.00001893
Iteration 183/1000 | Loss: 0.00001893
Iteration 184/1000 | Loss: 0.00001893
Iteration 185/1000 | Loss: 0.00001893
Iteration 186/1000 | Loss: 0.00001893
Iteration 187/1000 | Loss: 0.00001893
Iteration 188/1000 | Loss: 0.00001893
Iteration 189/1000 | Loss: 0.00001893
Iteration 190/1000 | Loss: 0.00001893
Iteration 191/1000 | Loss: 0.00001893
Iteration 192/1000 | Loss: 0.00001892
Iteration 193/1000 | Loss: 0.00001892
Iteration 194/1000 | Loss: 0.00001892
Iteration 195/1000 | Loss: 0.00001892
Iteration 196/1000 | Loss: 0.00001892
Iteration 197/1000 | Loss: 0.00001892
Iteration 198/1000 | Loss: 0.00001892
Iteration 199/1000 | Loss: 0.00001892
Iteration 200/1000 | Loss: 0.00001892
Iteration 201/1000 | Loss: 0.00001891
Iteration 202/1000 | Loss: 0.00001891
Iteration 203/1000 | Loss: 0.00001891
Iteration 204/1000 | Loss: 0.00001891
Iteration 205/1000 | Loss: 0.00001891
Iteration 206/1000 | Loss: 0.00001891
Iteration 207/1000 | Loss: 0.00001891
Iteration 208/1000 | Loss: 0.00001891
Iteration 209/1000 | Loss: 0.00001891
Iteration 210/1000 | Loss: 0.00001891
Iteration 211/1000 | Loss: 0.00001891
Iteration 212/1000 | Loss: 0.00001891
Iteration 213/1000 | Loss: 0.00001891
Iteration 214/1000 | Loss: 0.00001891
Iteration 215/1000 | Loss: 0.00001891
Iteration 216/1000 | Loss: 0.00001891
Iteration 217/1000 | Loss: 0.00001890
Iteration 218/1000 | Loss: 0.00001890
Iteration 219/1000 | Loss: 0.00001890
Iteration 220/1000 | Loss: 0.00001890
Iteration 221/1000 | Loss: 0.00001890
Iteration 222/1000 | Loss: 0.00001890
Iteration 223/1000 | Loss: 0.00001890
Iteration 224/1000 | Loss: 0.00001890
Iteration 225/1000 | Loss: 0.00001890
Iteration 226/1000 | Loss: 0.00001890
Iteration 227/1000 | Loss: 0.00001890
Iteration 228/1000 | Loss: 0.00001890
Iteration 229/1000 | Loss: 0.00001890
Iteration 230/1000 | Loss: 0.00001890
Iteration 231/1000 | Loss: 0.00001890
Iteration 232/1000 | Loss: 0.00001890
Iteration 233/1000 | Loss: 0.00001890
Iteration 234/1000 | Loss: 0.00001890
Iteration 235/1000 | Loss: 0.00001890
Iteration 236/1000 | Loss: 0.00001890
Iteration 237/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.8897393601946533e-05, 1.8897393601946533e-05, 1.8897393601946533e-05, 1.8897393601946533e-05, 1.8897393601946533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8897393601946533e-05

Optimization complete. Final v2v error: 3.602572441101074 mm

Highest mean error: 5.033022403717041 mm for frame 107

Lowest mean error: 2.9681737422943115 mm for frame 182

Saving results

Total time: 189.20049166679382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774654
Iteration 2/25 | Loss: 0.00097680
Iteration 3/25 | Loss: 0.00074177
Iteration 4/25 | Loss: 0.00069747
Iteration 5/25 | Loss: 0.00069160
Iteration 6/25 | Loss: 0.00069133
Iteration 7/25 | Loss: 0.00069133
Iteration 8/25 | Loss: 0.00069133
Iteration 9/25 | Loss: 0.00069133
Iteration 10/25 | Loss: 0.00069133
Iteration 11/25 | Loss: 0.00069133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006913296529091895, 0.0006913296529091895, 0.0006913296529091895, 0.0006913296529091895, 0.0006913296529091895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006913296529091895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45184660
Iteration 2/25 | Loss: 0.00029707
Iteration 3/25 | Loss: 0.00029704
Iteration 4/25 | Loss: 0.00029704
Iteration 5/25 | Loss: 0.00029704
Iteration 6/25 | Loss: 0.00029704
Iteration 7/25 | Loss: 0.00029704
Iteration 8/25 | Loss: 0.00029703
Iteration 9/25 | Loss: 0.00029703
Iteration 10/25 | Loss: 0.00029703
Iteration 11/25 | Loss: 0.00029703
Iteration 12/25 | Loss: 0.00029703
Iteration 13/25 | Loss: 0.00029703
Iteration 14/25 | Loss: 0.00029703
Iteration 15/25 | Loss: 0.00029703
Iteration 16/25 | Loss: 0.00029703
Iteration 17/25 | Loss: 0.00029703
Iteration 18/25 | Loss: 0.00029703
Iteration 19/25 | Loss: 0.00029703
Iteration 20/25 | Loss: 0.00029703
Iteration 21/25 | Loss: 0.00029703
Iteration 22/25 | Loss: 0.00029703
Iteration 23/25 | Loss: 0.00029703
Iteration 24/25 | Loss: 0.00029703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00029703418840654194, 0.00029703418840654194, 0.00029703418840654194, 0.00029703418840654194, 0.00029703418840654194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029703418840654194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029703
Iteration 2/1000 | Loss: 0.00005069
Iteration 3/1000 | Loss: 0.00003384
Iteration 4/1000 | Loss: 0.00003064
Iteration 5/1000 | Loss: 0.00002800
Iteration 6/1000 | Loss: 0.00002664
Iteration 7/1000 | Loss: 0.00002564
Iteration 8/1000 | Loss: 0.00002488
Iteration 9/1000 | Loss: 0.00002450
Iteration 10/1000 | Loss: 0.00002424
Iteration 11/1000 | Loss: 0.00002397
Iteration 12/1000 | Loss: 0.00002384
Iteration 13/1000 | Loss: 0.00002377
Iteration 14/1000 | Loss: 0.00002362
Iteration 15/1000 | Loss: 0.00002361
Iteration 16/1000 | Loss: 0.00002359
Iteration 17/1000 | Loss: 0.00002359
Iteration 18/1000 | Loss: 0.00002359
Iteration 19/1000 | Loss: 0.00002358
Iteration 20/1000 | Loss: 0.00002358
Iteration 21/1000 | Loss: 0.00002358
Iteration 22/1000 | Loss: 0.00002357
Iteration 23/1000 | Loss: 0.00002357
Iteration 24/1000 | Loss: 0.00002357
Iteration 25/1000 | Loss: 0.00002357
Iteration 26/1000 | Loss: 0.00002357
Iteration 27/1000 | Loss: 0.00002356
Iteration 28/1000 | Loss: 0.00002356
Iteration 29/1000 | Loss: 0.00002356
Iteration 30/1000 | Loss: 0.00002356
Iteration 31/1000 | Loss: 0.00002356
Iteration 32/1000 | Loss: 0.00002356
Iteration 33/1000 | Loss: 0.00002356
Iteration 34/1000 | Loss: 0.00002356
Iteration 35/1000 | Loss: 0.00002356
Iteration 36/1000 | Loss: 0.00002355
Iteration 37/1000 | Loss: 0.00002355
Iteration 38/1000 | Loss: 0.00002355
Iteration 39/1000 | Loss: 0.00002355
Iteration 40/1000 | Loss: 0.00002355
Iteration 41/1000 | Loss: 0.00002355
Iteration 42/1000 | Loss: 0.00002354
Iteration 43/1000 | Loss: 0.00002354
Iteration 44/1000 | Loss: 0.00002354
Iteration 45/1000 | Loss: 0.00002354
Iteration 46/1000 | Loss: 0.00002354
Iteration 47/1000 | Loss: 0.00002354
Iteration 48/1000 | Loss: 0.00002354
Iteration 49/1000 | Loss: 0.00002354
Iteration 50/1000 | Loss: 0.00002354
Iteration 51/1000 | Loss: 0.00002354
Iteration 52/1000 | Loss: 0.00002354
Iteration 53/1000 | Loss: 0.00002354
Iteration 54/1000 | Loss: 0.00002353
Iteration 55/1000 | Loss: 0.00002353
Iteration 56/1000 | Loss: 0.00002353
Iteration 57/1000 | Loss: 0.00002353
Iteration 58/1000 | Loss: 0.00002353
Iteration 59/1000 | Loss: 0.00002353
Iteration 60/1000 | Loss: 0.00002353
Iteration 61/1000 | Loss: 0.00002353
Iteration 62/1000 | Loss: 0.00002353
Iteration 63/1000 | Loss: 0.00002353
Iteration 64/1000 | Loss: 0.00002352
Iteration 65/1000 | Loss: 0.00002352
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00002352
Iteration 68/1000 | Loss: 0.00002352
Iteration 69/1000 | Loss: 0.00002352
Iteration 70/1000 | Loss: 0.00002352
Iteration 71/1000 | Loss: 0.00002352
Iteration 72/1000 | Loss: 0.00002352
Iteration 73/1000 | Loss: 0.00002352
Iteration 74/1000 | Loss: 0.00002352
Iteration 75/1000 | Loss: 0.00002352
Iteration 76/1000 | Loss: 0.00002352
Iteration 77/1000 | Loss: 0.00002352
Iteration 78/1000 | Loss: 0.00002352
Iteration 79/1000 | Loss: 0.00002352
Iteration 80/1000 | Loss: 0.00002352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.3516708097304218e-05, 2.3516708097304218e-05, 2.3516708097304218e-05, 2.3516708097304218e-05, 2.3516708097304218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3516708097304218e-05

Optimization complete. Final v2v error: 4.026164531707764 mm

Highest mean error: 4.350686073303223 mm for frame 156

Lowest mean error: 3.7382853031158447 mm for frame 215

Saving results

Total time: 34.383113622665405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879782
Iteration 2/25 | Loss: 0.00094547
Iteration 3/25 | Loss: 0.00080845
Iteration 4/25 | Loss: 0.00077744
Iteration 5/25 | Loss: 0.00076551
Iteration 6/25 | Loss: 0.00076302
Iteration 7/25 | Loss: 0.00076259
Iteration 8/25 | Loss: 0.00076259
Iteration 9/25 | Loss: 0.00076259
Iteration 10/25 | Loss: 0.00076259
Iteration 11/25 | Loss: 0.00076259
Iteration 12/25 | Loss: 0.00076259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007625942234881222, 0.0007625942234881222, 0.0007625942234881222, 0.0007625942234881222, 0.0007625942234881222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007625942234881222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77623272
Iteration 2/25 | Loss: 0.00158898
Iteration 3/25 | Loss: 0.00158898
Iteration 4/25 | Loss: 0.00158898
Iteration 5/25 | Loss: 0.00158897
Iteration 6/25 | Loss: 0.00158897
Iteration 7/25 | Loss: 0.00158897
Iteration 8/25 | Loss: 0.00158897
Iteration 9/25 | Loss: 0.00158897
Iteration 10/25 | Loss: 0.00158897
Iteration 11/25 | Loss: 0.00158897
Iteration 12/25 | Loss: 0.00158897
Iteration 13/25 | Loss: 0.00158897
Iteration 14/25 | Loss: 0.00158897
Iteration 15/25 | Loss: 0.00158897
Iteration 16/25 | Loss: 0.00158897
Iteration 17/25 | Loss: 0.00158897
Iteration 18/25 | Loss: 0.00158897
Iteration 19/25 | Loss: 0.00158897
Iteration 20/25 | Loss: 0.00158897
Iteration 21/25 | Loss: 0.00158897
Iteration 22/25 | Loss: 0.00158897
Iteration 23/25 | Loss: 0.00158897
Iteration 24/25 | Loss: 0.00158897
Iteration 25/25 | Loss: 0.00158897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158897
Iteration 2/1000 | Loss: 0.00003086
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00002082
Iteration 5/1000 | Loss: 0.00001961
Iteration 6/1000 | Loss: 0.00001872
Iteration 7/1000 | Loss: 0.00001813
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001735
Iteration 12/1000 | Loss: 0.00001735
Iteration 13/1000 | Loss: 0.00001734
Iteration 14/1000 | Loss: 0.00001719
Iteration 15/1000 | Loss: 0.00001718
Iteration 16/1000 | Loss: 0.00001716
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001704
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001704
Iteration 23/1000 | Loss: 0.00001704
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001703
Iteration 27/1000 | Loss: 0.00001703
Iteration 28/1000 | Loss: 0.00001703
Iteration 29/1000 | Loss: 0.00001703
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001697
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001696
Iteration 38/1000 | Loss: 0.00001695
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001694
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001692
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001690
Iteration 47/1000 | Loss: 0.00001690
Iteration 48/1000 | Loss: 0.00001688
Iteration 49/1000 | Loss: 0.00001687
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001684
Iteration 52/1000 | Loss: 0.00001684
Iteration 53/1000 | Loss: 0.00001684
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001684
Iteration 57/1000 | Loss: 0.00001684
Iteration 58/1000 | Loss: 0.00001684
Iteration 59/1000 | Loss: 0.00001684
Iteration 60/1000 | Loss: 0.00001683
Iteration 61/1000 | Loss: 0.00001683
Iteration 62/1000 | Loss: 0.00001683
Iteration 63/1000 | Loss: 0.00001680
Iteration 64/1000 | Loss: 0.00001680
Iteration 65/1000 | Loss: 0.00001679
Iteration 66/1000 | Loss: 0.00001679
Iteration 67/1000 | Loss: 0.00001679
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001678
Iteration 70/1000 | Loss: 0.00001678
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001677
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001676
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001675
Iteration 79/1000 | Loss: 0.00001674
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001673
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00001672
Iteration 86/1000 | Loss: 0.00001671
Iteration 87/1000 | Loss: 0.00001671
Iteration 88/1000 | Loss: 0.00001671
Iteration 89/1000 | Loss: 0.00001671
Iteration 90/1000 | Loss: 0.00001671
Iteration 91/1000 | Loss: 0.00001671
Iteration 92/1000 | Loss: 0.00001671
Iteration 93/1000 | Loss: 0.00001671
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001671
Iteration 96/1000 | Loss: 0.00001670
Iteration 97/1000 | Loss: 0.00001670
Iteration 98/1000 | Loss: 0.00001670
Iteration 99/1000 | Loss: 0.00001670
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001669
Iteration 105/1000 | Loss: 0.00001669
Iteration 106/1000 | Loss: 0.00001668
Iteration 107/1000 | Loss: 0.00001668
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001668
Iteration 110/1000 | Loss: 0.00001668
Iteration 111/1000 | Loss: 0.00001668
Iteration 112/1000 | Loss: 0.00001668
Iteration 113/1000 | Loss: 0.00001668
Iteration 114/1000 | Loss: 0.00001667
Iteration 115/1000 | Loss: 0.00001667
Iteration 116/1000 | Loss: 0.00001667
Iteration 117/1000 | Loss: 0.00001667
Iteration 118/1000 | Loss: 0.00001667
Iteration 119/1000 | Loss: 0.00001667
Iteration 120/1000 | Loss: 0.00001667
Iteration 121/1000 | Loss: 0.00001667
Iteration 122/1000 | Loss: 0.00001666
Iteration 123/1000 | Loss: 0.00001666
Iteration 124/1000 | Loss: 0.00001666
Iteration 125/1000 | Loss: 0.00001666
Iteration 126/1000 | Loss: 0.00001666
Iteration 127/1000 | Loss: 0.00001666
Iteration 128/1000 | Loss: 0.00001666
Iteration 129/1000 | Loss: 0.00001666
Iteration 130/1000 | Loss: 0.00001666
Iteration 131/1000 | Loss: 0.00001666
Iteration 132/1000 | Loss: 0.00001666
Iteration 133/1000 | Loss: 0.00001666
Iteration 134/1000 | Loss: 0.00001665
Iteration 135/1000 | Loss: 0.00001665
Iteration 136/1000 | Loss: 0.00001665
Iteration 137/1000 | Loss: 0.00001665
Iteration 138/1000 | Loss: 0.00001665
Iteration 139/1000 | Loss: 0.00001665
Iteration 140/1000 | Loss: 0.00001665
Iteration 141/1000 | Loss: 0.00001665
Iteration 142/1000 | Loss: 0.00001665
Iteration 143/1000 | Loss: 0.00001665
Iteration 144/1000 | Loss: 0.00001665
Iteration 145/1000 | Loss: 0.00001665
Iteration 146/1000 | Loss: 0.00001665
Iteration 147/1000 | Loss: 0.00001665
Iteration 148/1000 | Loss: 0.00001665
Iteration 149/1000 | Loss: 0.00001665
Iteration 150/1000 | Loss: 0.00001665
Iteration 151/1000 | Loss: 0.00001665
Iteration 152/1000 | Loss: 0.00001665
Iteration 153/1000 | Loss: 0.00001664
Iteration 154/1000 | Loss: 0.00001664
Iteration 155/1000 | Loss: 0.00001664
Iteration 156/1000 | Loss: 0.00001664
Iteration 157/1000 | Loss: 0.00001664
Iteration 158/1000 | Loss: 0.00001664
Iteration 159/1000 | Loss: 0.00001664
Iteration 160/1000 | Loss: 0.00001664
Iteration 161/1000 | Loss: 0.00001664
Iteration 162/1000 | Loss: 0.00001664
Iteration 163/1000 | Loss: 0.00001664
Iteration 164/1000 | Loss: 0.00001664
Iteration 165/1000 | Loss: 0.00001664
Iteration 166/1000 | Loss: 0.00001664
Iteration 167/1000 | Loss: 0.00001664
Iteration 168/1000 | Loss: 0.00001664
Iteration 169/1000 | Loss: 0.00001664
Iteration 170/1000 | Loss: 0.00001664
Iteration 171/1000 | Loss: 0.00001664
Iteration 172/1000 | Loss: 0.00001664
Iteration 173/1000 | Loss: 0.00001663
Iteration 174/1000 | Loss: 0.00001663
Iteration 175/1000 | Loss: 0.00001663
Iteration 176/1000 | Loss: 0.00001663
Iteration 177/1000 | Loss: 0.00001663
Iteration 178/1000 | Loss: 0.00001663
Iteration 179/1000 | Loss: 0.00001663
Iteration 180/1000 | Loss: 0.00001663
Iteration 181/1000 | Loss: 0.00001663
Iteration 182/1000 | Loss: 0.00001663
Iteration 183/1000 | Loss: 0.00001663
Iteration 184/1000 | Loss: 0.00001663
Iteration 185/1000 | Loss: 0.00001663
Iteration 186/1000 | Loss: 0.00001663
Iteration 187/1000 | Loss: 0.00001663
Iteration 188/1000 | Loss: 0.00001663
Iteration 189/1000 | Loss: 0.00001662
Iteration 190/1000 | Loss: 0.00001662
Iteration 191/1000 | Loss: 0.00001662
Iteration 192/1000 | Loss: 0.00001662
Iteration 193/1000 | Loss: 0.00001662
Iteration 194/1000 | Loss: 0.00001662
Iteration 195/1000 | Loss: 0.00001662
Iteration 196/1000 | Loss: 0.00001662
Iteration 197/1000 | Loss: 0.00001662
Iteration 198/1000 | Loss: 0.00001662
Iteration 199/1000 | Loss: 0.00001662
Iteration 200/1000 | Loss: 0.00001662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.6622861949144863e-05, 1.6622861949144863e-05, 1.6622861949144863e-05, 1.6622861949144863e-05, 1.6622861949144863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6622861949144863e-05

Optimization complete. Final v2v error: 3.5314180850982666 mm

Highest mean error: 4.06767463684082 mm for frame 59

Lowest mean error: 3.091451644897461 mm for frame 147

Saving results

Total time: 39.27453017234802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424314
Iteration 2/25 | Loss: 0.00106151
Iteration 3/25 | Loss: 0.00092735
Iteration 4/25 | Loss: 0.00090814
Iteration 5/25 | Loss: 0.00090144
Iteration 6/25 | Loss: 0.00090023
Iteration 7/25 | Loss: 0.00090012
Iteration 8/25 | Loss: 0.00090012
Iteration 9/25 | Loss: 0.00090012
Iteration 10/25 | Loss: 0.00090012
Iteration 11/25 | Loss: 0.00090012
Iteration 12/25 | Loss: 0.00090012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009001237922348082, 0.0009001237922348082, 0.0009001237922348082, 0.0009001237922348082, 0.0009001237922348082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009001237922348082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72422481
Iteration 2/25 | Loss: 0.00170762
Iteration 3/25 | Loss: 0.00170762
Iteration 4/25 | Loss: 0.00170762
Iteration 5/25 | Loss: 0.00170762
Iteration 6/25 | Loss: 0.00170761
Iteration 7/25 | Loss: 0.00170761
Iteration 8/25 | Loss: 0.00170761
Iteration 9/25 | Loss: 0.00170761
Iteration 10/25 | Loss: 0.00170761
Iteration 11/25 | Loss: 0.00170761
Iteration 12/25 | Loss: 0.00170761
Iteration 13/25 | Loss: 0.00170761
Iteration 14/25 | Loss: 0.00170761
Iteration 15/25 | Loss: 0.00170761
Iteration 16/25 | Loss: 0.00170761
Iteration 17/25 | Loss: 0.00170761
Iteration 18/25 | Loss: 0.00170761
Iteration 19/25 | Loss: 0.00170761
Iteration 20/25 | Loss: 0.00170761
Iteration 21/25 | Loss: 0.00170761
Iteration 22/25 | Loss: 0.00170761
Iteration 23/25 | Loss: 0.00170761
Iteration 24/25 | Loss: 0.00170761
Iteration 25/25 | Loss: 0.00170761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170761
Iteration 2/1000 | Loss: 0.00007154
Iteration 3/1000 | Loss: 0.00004879
Iteration 4/1000 | Loss: 0.00004404
Iteration 5/1000 | Loss: 0.00004174
Iteration 6/1000 | Loss: 0.00003946
Iteration 7/1000 | Loss: 0.00003852
Iteration 8/1000 | Loss: 0.00003764
Iteration 9/1000 | Loss: 0.00003712
Iteration 10/1000 | Loss: 0.00003659
Iteration 11/1000 | Loss: 0.00003628
Iteration 12/1000 | Loss: 0.00003603
Iteration 13/1000 | Loss: 0.00003587
Iteration 14/1000 | Loss: 0.00003587
Iteration 15/1000 | Loss: 0.00003586
Iteration 16/1000 | Loss: 0.00003578
Iteration 17/1000 | Loss: 0.00003578
Iteration 18/1000 | Loss: 0.00003575
Iteration 19/1000 | Loss: 0.00003569
Iteration 20/1000 | Loss: 0.00003568
Iteration 21/1000 | Loss: 0.00003562
Iteration 22/1000 | Loss: 0.00003561
Iteration 23/1000 | Loss: 0.00003561
Iteration 24/1000 | Loss: 0.00003561
Iteration 25/1000 | Loss: 0.00003559
Iteration 26/1000 | Loss: 0.00003559
Iteration 27/1000 | Loss: 0.00003556
Iteration 28/1000 | Loss: 0.00003554
Iteration 29/1000 | Loss: 0.00003554
Iteration 30/1000 | Loss: 0.00003551
Iteration 31/1000 | Loss: 0.00003551
Iteration 32/1000 | Loss: 0.00003550
Iteration 33/1000 | Loss: 0.00003550
Iteration 34/1000 | Loss: 0.00003550
Iteration 35/1000 | Loss: 0.00003549
Iteration 36/1000 | Loss: 0.00003549
Iteration 37/1000 | Loss: 0.00003547
Iteration 38/1000 | Loss: 0.00003546
Iteration 39/1000 | Loss: 0.00003546
Iteration 40/1000 | Loss: 0.00003546
Iteration 41/1000 | Loss: 0.00003545
Iteration 42/1000 | Loss: 0.00003545
Iteration 43/1000 | Loss: 0.00003544
Iteration 44/1000 | Loss: 0.00003544
Iteration 45/1000 | Loss: 0.00003544
Iteration 46/1000 | Loss: 0.00003543
Iteration 47/1000 | Loss: 0.00003543
Iteration 48/1000 | Loss: 0.00003542
Iteration 49/1000 | Loss: 0.00003542
Iteration 50/1000 | Loss: 0.00003542
Iteration 51/1000 | Loss: 0.00003542
Iteration 52/1000 | Loss: 0.00003542
Iteration 53/1000 | Loss: 0.00003541
Iteration 54/1000 | Loss: 0.00003541
Iteration 55/1000 | Loss: 0.00003541
Iteration 56/1000 | Loss: 0.00003541
Iteration 57/1000 | Loss: 0.00003540
Iteration 58/1000 | Loss: 0.00003540
Iteration 59/1000 | Loss: 0.00003539
Iteration 60/1000 | Loss: 0.00003539
Iteration 61/1000 | Loss: 0.00003539
Iteration 62/1000 | Loss: 0.00003539
Iteration 63/1000 | Loss: 0.00003538
Iteration 64/1000 | Loss: 0.00003538
Iteration 65/1000 | Loss: 0.00003538
Iteration 66/1000 | Loss: 0.00003538
Iteration 67/1000 | Loss: 0.00003538
Iteration 68/1000 | Loss: 0.00003538
Iteration 69/1000 | Loss: 0.00003538
Iteration 70/1000 | Loss: 0.00003538
Iteration 71/1000 | Loss: 0.00003537
Iteration 72/1000 | Loss: 0.00003537
Iteration 73/1000 | Loss: 0.00003537
Iteration 74/1000 | Loss: 0.00003537
Iteration 75/1000 | Loss: 0.00003537
Iteration 76/1000 | Loss: 0.00003537
Iteration 77/1000 | Loss: 0.00003537
Iteration 78/1000 | Loss: 0.00003537
Iteration 79/1000 | Loss: 0.00003536
Iteration 80/1000 | Loss: 0.00003536
Iteration 81/1000 | Loss: 0.00003536
Iteration 82/1000 | Loss: 0.00003536
Iteration 83/1000 | Loss: 0.00003536
Iteration 84/1000 | Loss: 0.00003536
Iteration 85/1000 | Loss: 0.00003535
Iteration 86/1000 | Loss: 0.00003535
Iteration 87/1000 | Loss: 0.00003535
Iteration 88/1000 | Loss: 0.00003535
Iteration 89/1000 | Loss: 0.00003535
Iteration 90/1000 | Loss: 0.00003535
Iteration 91/1000 | Loss: 0.00003535
Iteration 92/1000 | Loss: 0.00003535
Iteration 93/1000 | Loss: 0.00003535
Iteration 94/1000 | Loss: 0.00003534
Iteration 95/1000 | Loss: 0.00003534
Iteration 96/1000 | Loss: 0.00003534
Iteration 97/1000 | Loss: 0.00003534
Iteration 98/1000 | Loss: 0.00003534
Iteration 99/1000 | Loss: 0.00003534
Iteration 100/1000 | Loss: 0.00003534
Iteration 101/1000 | Loss: 0.00003534
Iteration 102/1000 | Loss: 0.00003533
Iteration 103/1000 | Loss: 0.00003533
Iteration 104/1000 | Loss: 0.00003533
Iteration 105/1000 | Loss: 0.00003533
Iteration 106/1000 | Loss: 0.00003533
Iteration 107/1000 | Loss: 0.00003533
Iteration 108/1000 | Loss: 0.00003533
Iteration 109/1000 | Loss: 0.00003533
Iteration 110/1000 | Loss: 0.00003533
Iteration 111/1000 | Loss: 0.00003533
Iteration 112/1000 | Loss: 0.00003533
Iteration 113/1000 | Loss: 0.00003533
Iteration 114/1000 | Loss: 0.00003533
Iteration 115/1000 | Loss: 0.00003533
Iteration 116/1000 | Loss: 0.00003533
Iteration 117/1000 | Loss: 0.00003533
Iteration 118/1000 | Loss: 0.00003533
Iteration 119/1000 | Loss: 0.00003533
Iteration 120/1000 | Loss: 0.00003533
Iteration 121/1000 | Loss: 0.00003532
Iteration 122/1000 | Loss: 0.00003532
Iteration 123/1000 | Loss: 0.00003532
Iteration 124/1000 | Loss: 0.00003532
Iteration 125/1000 | Loss: 0.00003532
Iteration 126/1000 | Loss: 0.00003531
Iteration 127/1000 | Loss: 0.00003531
Iteration 128/1000 | Loss: 0.00003531
Iteration 129/1000 | Loss: 0.00003531
Iteration 130/1000 | Loss: 0.00003531
Iteration 131/1000 | Loss: 0.00003531
Iteration 132/1000 | Loss: 0.00003531
Iteration 133/1000 | Loss: 0.00003531
Iteration 134/1000 | Loss: 0.00003531
Iteration 135/1000 | Loss: 0.00003531
Iteration 136/1000 | Loss: 0.00003531
Iteration 137/1000 | Loss: 0.00003531
Iteration 138/1000 | Loss: 0.00003530
Iteration 139/1000 | Loss: 0.00003530
Iteration 140/1000 | Loss: 0.00003530
Iteration 141/1000 | Loss: 0.00003530
Iteration 142/1000 | Loss: 0.00003530
Iteration 143/1000 | Loss: 0.00003530
Iteration 144/1000 | Loss: 0.00003530
Iteration 145/1000 | Loss: 0.00003529
Iteration 146/1000 | Loss: 0.00003529
Iteration 147/1000 | Loss: 0.00003529
Iteration 148/1000 | Loss: 0.00003529
Iteration 149/1000 | Loss: 0.00003529
Iteration 150/1000 | Loss: 0.00003529
Iteration 151/1000 | Loss: 0.00003529
Iteration 152/1000 | Loss: 0.00003529
Iteration 153/1000 | Loss: 0.00003529
Iteration 154/1000 | Loss: 0.00003529
Iteration 155/1000 | Loss: 0.00003528
Iteration 156/1000 | Loss: 0.00003528
Iteration 157/1000 | Loss: 0.00003528
Iteration 158/1000 | Loss: 0.00003528
Iteration 159/1000 | Loss: 0.00003528
Iteration 160/1000 | Loss: 0.00003528
Iteration 161/1000 | Loss: 0.00003528
Iteration 162/1000 | Loss: 0.00003528
Iteration 163/1000 | Loss: 0.00003528
Iteration 164/1000 | Loss: 0.00003528
Iteration 165/1000 | Loss: 0.00003528
Iteration 166/1000 | Loss: 0.00003528
Iteration 167/1000 | Loss: 0.00003528
Iteration 168/1000 | Loss: 0.00003528
Iteration 169/1000 | Loss: 0.00003528
Iteration 170/1000 | Loss: 0.00003528
Iteration 171/1000 | Loss: 0.00003528
Iteration 172/1000 | Loss: 0.00003528
Iteration 173/1000 | Loss: 0.00003528
Iteration 174/1000 | Loss: 0.00003528
Iteration 175/1000 | Loss: 0.00003528
Iteration 176/1000 | Loss: 0.00003528
Iteration 177/1000 | Loss: 0.00003528
Iteration 178/1000 | Loss: 0.00003528
Iteration 179/1000 | Loss: 0.00003528
Iteration 180/1000 | Loss: 0.00003528
Iteration 181/1000 | Loss: 0.00003528
Iteration 182/1000 | Loss: 0.00003528
Iteration 183/1000 | Loss: 0.00003528
Iteration 184/1000 | Loss: 0.00003528
Iteration 185/1000 | Loss: 0.00003528
Iteration 186/1000 | Loss: 0.00003528
Iteration 187/1000 | Loss: 0.00003528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [3.5276749258628115e-05, 3.5276749258628115e-05, 3.5276749258628115e-05, 3.5276749258628115e-05, 3.5276749258628115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5276749258628115e-05

Optimization complete. Final v2v error: 5.066184043884277 mm

Highest mean error: 5.555055141448975 mm for frame 108

Lowest mean error: 4.547678470611572 mm for frame 35

Saving results

Total time: 40.975255727767944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603793
Iteration 2/25 | Loss: 0.00090075
Iteration 3/25 | Loss: 0.00079914
Iteration 4/25 | Loss: 0.00078076
Iteration 5/25 | Loss: 0.00077385
Iteration 6/25 | Loss: 0.00077238
Iteration 7/25 | Loss: 0.00077227
Iteration 8/25 | Loss: 0.00077227
Iteration 9/25 | Loss: 0.00077227
Iteration 10/25 | Loss: 0.00077227
Iteration 11/25 | Loss: 0.00077227
Iteration 12/25 | Loss: 0.00077227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007722688606008887, 0.0007722688606008887, 0.0007722688606008887, 0.0007722688606008887, 0.0007722688606008887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007722688606008887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89196372
Iteration 2/25 | Loss: 0.00161637
Iteration 3/25 | Loss: 0.00161637
Iteration 4/25 | Loss: 0.00161637
Iteration 5/25 | Loss: 0.00161637
Iteration 6/25 | Loss: 0.00161637
Iteration 7/25 | Loss: 0.00161637
Iteration 8/25 | Loss: 0.00161637
Iteration 9/25 | Loss: 0.00161637
Iteration 10/25 | Loss: 0.00161637
Iteration 11/25 | Loss: 0.00161637
Iteration 12/25 | Loss: 0.00161637
Iteration 13/25 | Loss: 0.00161637
Iteration 14/25 | Loss: 0.00161637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001616370864212513, 0.001616370864212513, 0.001616370864212513, 0.001616370864212513, 0.001616370864212513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001616370864212513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161637
Iteration 2/1000 | Loss: 0.00003066
Iteration 3/1000 | Loss: 0.00002226
Iteration 4/1000 | Loss: 0.00002041
Iteration 5/1000 | Loss: 0.00001949
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001815
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001788
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001758
Iteration 14/1000 | Loss: 0.00001757
Iteration 15/1000 | Loss: 0.00001757
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001754
Iteration 19/1000 | Loss: 0.00001753
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001750
Iteration 22/1000 | Loss: 0.00001749
Iteration 23/1000 | Loss: 0.00001749
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001748
Iteration 26/1000 | Loss: 0.00001747
Iteration 27/1000 | Loss: 0.00001747
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001738
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001736
Iteration 32/1000 | Loss: 0.00001736
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001735
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001733
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001733
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001730
Iteration 44/1000 | Loss: 0.00001728
Iteration 45/1000 | Loss: 0.00001728
Iteration 46/1000 | Loss: 0.00001727
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001727
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001725
Iteration 52/1000 | Loss: 0.00001725
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001720
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001718
Iteration 66/1000 | Loss: 0.00001718
Iteration 67/1000 | Loss: 0.00001718
Iteration 68/1000 | Loss: 0.00001717
Iteration 69/1000 | Loss: 0.00001717
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001715
Iteration 77/1000 | Loss: 0.00001715
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001713
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001712
Iteration 84/1000 | Loss: 0.00001712
Iteration 85/1000 | Loss: 0.00001712
Iteration 86/1000 | Loss: 0.00001712
Iteration 87/1000 | Loss: 0.00001712
Iteration 88/1000 | Loss: 0.00001711
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001710
Iteration 93/1000 | Loss: 0.00001710
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001708
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001708
Iteration 108/1000 | Loss: 0.00001708
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001707
Iteration 114/1000 | Loss: 0.00001707
Iteration 115/1000 | Loss: 0.00001707
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001707
Iteration 120/1000 | Loss: 0.00001707
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001706
Iteration 123/1000 | Loss: 0.00001706
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001705
Iteration 131/1000 | Loss: 0.00001705
Iteration 132/1000 | Loss: 0.00001705
Iteration 133/1000 | Loss: 0.00001705
Iteration 134/1000 | Loss: 0.00001705
Iteration 135/1000 | Loss: 0.00001705
Iteration 136/1000 | Loss: 0.00001705
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001704
Iteration 139/1000 | Loss: 0.00001704
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001704
Iteration 143/1000 | Loss: 0.00001704
Iteration 144/1000 | Loss: 0.00001704
Iteration 145/1000 | Loss: 0.00001704
Iteration 146/1000 | Loss: 0.00001704
Iteration 147/1000 | Loss: 0.00001704
Iteration 148/1000 | Loss: 0.00001704
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001703
Iteration 154/1000 | Loss: 0.00001703
Iteration 155/1000 | Loss: 0.00001703
Iteration 156/1000 | Loss: 0.00001703
Iteration 157/1000 | Loss: 0.00001703
Iteration 158/1000 | Loss: 0.00001703
Iteration 159/1000 | Loss: 0.00001703
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001703
Iteration 165/1000 | Loss: 0.00001703
Iteration 166/1000 | Loss: 0.00001703
Iteration 167/1000 | Loss: 0.00001703
Iteration 168/1000 | Loss: 0.00001703
Iteration 169/1000 | Loss: 0.00001703
Iteration 170/1000 | Loss: 0.00001703
Iteration 171/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.702675399428699e-05, 1.702675399428699e-05, 1.702675399428699e-05, 1.702675399428699e-05, 1.702675399428699e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.702675399428699e-05

Optimization complete. Final v2v error: 3.5562028884887695 mm

Highest mean error: 3.90708589553833 mm for frame 93

Lowest mean error: 3.1618216037750244 mm for frame 121

Saving results

Total time: 35.427582025527954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376461
Iteration 2/25 | Loss: 0.00087081
Iteration 3/25 | Loss: 0.00079584
Iteration 4/25 | Loss: 0.00078128
Iteration 5/25 | Loss: 0.00077533
Iteration 6/25 | Loss: 0.00077407
Iteration 7/25 | Loss: 0.00077407
Iteration 8/25 | Loss: 0.00077407
Iteration 9/25 | Loss: 0.00077407
Iteration 10/25 | Loss: 0.00077407
Iteration 11/25 | Loss: 0.00077407
Iteration 12/25 | Loss: 0.00077407
Iteration 13/25 | Loss: 0.00077407
Iteration 14/25 | Loss: 0.00077407
Iteration 15/25 | Loss: 0.00077407
Iteration 16/25 | Loss: 0.00077407
Iteration 17/25 | Loss: 0.00077407
Iteration 18/25 | Loss: 0.00077407
Iteration 19/25 | Loss: 0.00077407
Iteration 20/25 | Loss: 0.00077407
Iteration 21/25 | Loss: 0.00077407
Iteration 22/25 | Loss: 0.00077407
Iteration 23/25 | Loss: 0.00077407
Iteration 24/25 | Loss: 0.00077407
Iteration 25/25 | Loss: 0.00077407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.21607089
Iteration 2/25 | Loss: 0.00165663
Iteration 3/25 | Loss: 0.00165663
Iteration 4/25 | Loss: 0.00165663
Iteration 5/25 | Loss: 0.00165663
Iteration 6/25 | Loss: 0.00165663
Iteration 7/25 | Loss: 0.00165663
Iteration 8/25 | Loss: 0.00165663
Iteration 9/25 | Loss: 0.00165663
Iteration 10/25 | Loss: 0.00165663
Iteration 11/25 | Loss: 0.00165662
Iteration 12/25 | Loss: 0.00165662
Iteration 13/25 | Loss: 0.00165662
Iteration 14/25 | Loss: 0.00165662
Iteration 15/25 | Loss: 0.00165662
Iteration 16/25 | Loss: 0.00165662
Iteration 17/25 | Loss: 0.00165662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001656624604947865, 0.001656624604947865, 0.001656624604947865, 0.001656624604947865, 0.001656624604947865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001656624604947865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165662
Iteration 2/1000 | Loss: 0.00003392
Iteration 3/1000 | Loss: 0.00002314
Iteration 4/1000 | Loss: 0.00002095
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001829
Iteration 7/1000 | Loss: 0.00001768
Iteration 8/1000 | Loss: 0.00001738
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001700
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001682
Iteration 13/1000 | Loss: 0.00001680
Iteration 14/1000 | Loss: 0.00001678
Iteration 15/1000 | Loss: 0.00001677
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001665
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001660
Iteration 23/1000 | Loss: 0.00001658
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001657
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001650
Iteration 29/1000 | Loss: 0.00001650
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001644
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001643
Iteration 37/1000 | Loss: 0.00001643
Iteration 38/1000 | Loss: 0.00001643
Iteration 39/1000 | Loss: 0.00001643
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001638
Iteration 46/1000 | Loss: 0.00001638
Iteration 47/1000 | Loss: 0.00001637
Iteration 48/1000 | Loss: 0.00001637
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001636
Iteration 51/1000 | Loss: 0.00001635
Iteration 52/1000 | Loss: 0.00001635
Iteration 53/1000 | Loss: 0.00001633
Iteration 54/1000 | Loss: 0.00001633
Iteration 55/1000 | Loss: 0.00001633
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001633
Iteration 62/1000 | Loss: 0.00001632
Iteration 63/1000 | Loss: 0.00001632
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001631
Iteration 67/1000 | Loss: 0.00001630
Iteration 68/1000 | Loss: 0.00001630
Iteration 69/1000 | Loss: 0.00001629
Iteration 70/1000 | Loss: 0.00001629
Iteration 71/1000 | Loss: 0.00001628
Iteration 72/1000 | Loss: 0.00001628
Iteration 73/1000 | Loss: 0.00001628
Iteration 74/1000 | Loss: 0.00001627
Iteration 75/1000 | Loss: 0.00001627
Iteration 76/1000 | Loss: 0.00001627
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001623
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00001621
Iteration 104/1000 | Loss: 0.00001621
Iteration 105/1000 | Loss: 0.00001621
Iteration 106/1000 | Loss: 0.00001621
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001621
Iteration 110/1000 | Loss: 0.00001620
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001620
Iteration 117/1000 | Loss: 0.00001619
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001619
Iteration 126/1000 | Loss: 0.00001619
Iteration 127/1000 | Loss: 0.00001619
Iteration 128/1000 | Loss: 0.00001619
Iteration 129/1000 | Loss: 0.00001619
Iteration 130/1000 | Loss: 0.00001619
Iteration 131/1000 | Loss: 0.00001619
Iteration 132/1000 | Loss: 0.00001619
Iteration 133/1000 | Loss: 0.00001619
Iteration 134/1000 | Loss: 0.00001619
Iteration 135/1000 | Loss: 0.00001619
Iteration 136/1000 | Loss: 0.00001619
Iteration 137/1000 | Loss: 0.00001619
Iteration 138/1000 | Loss: 0.00001619
Iteration 139/1000 | Loss: 0.00001619
Iteration 140/1000 | Loss: 0.00001619
Iteration 141/1000 | Loss: 0.00001619
Iteration 142/1000 | Loss: 0.00001619
Iteration 143/1000 | Loss: 0.00001619
Iteration 144/1000 | Loss: 0.00001619
Iteration 145/1000 | Loss: 0.00001619
Iteration 146/1000 | Loss: 0.00001619
Iteration 147/1000 | Loss: 0.00001619
Iteration 148/1000 | Loss: 0.00001619
Iteration 149/1000 | Loss: 0.00001619
Iteration 150/1000 | Loss: 0.00001619
Iteration 151/1000 | Loss: 0.00001619
Iteration 152/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.6187392247957177e-05, 1.6187392247957177e-05, 1.6187392247957177e-05, 1.6187392247957177e-05, 1.6187392247957177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6187392247957177e-05

Optimization complete. Final v2v error: 3.5000407695770264 mm

Highest mean error: 3.7438180446624756 mm for frame 52

Lowest mean error: 3.2929396629333496 mm for frame 143

Saving results

Total time: 40.74918079376221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774402
Iteration 2/25 | Loss: 0.00109472
Iteration 3/25 | Loss: 0.00091435
Iteration 4/25 | Loss: 0.00088691
Iteration 5/25 | Loss: 0.00087532
Iteration 6/25 | Loss: 0.00087332
Iteration 7/25 | Loss: 0.00087300
Iteration 8/25 | Loss: 0.00087300
Iteration 9/25 | Loss: 0.00087300
Iteration 10/25 | Loss: 0.00087300
Iteration 11/25 | Loss: 0.00087300
Iteration 12/25 | Loss: 0.00087300
Iteration 13/25 | Loss: 0.00087300
Iteration 14/25 | Loss: 0.00087300
Iteration 15/25 | Loss: 0.00087300
Iteration 16/25 | Loss: 0.00087300
Iteration 17/25 | Loss: 0.00087300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008729971013963223, 0.0008729971013963223, 0.0008729971013963223, 0.0008729971013963223, 0.0008729971013963223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008729971013963223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.43576097
Iteration 2/25 | Loss: 0.00152398
Iteration 3/25 | Loss: 0.00152395
Iteration 4/25 | Loss: 0.00152395
Iteration 5/25 | Loss: 0.00152395
Iteration 6/25 | Loss: 0.00152395
Iteration 7/25 | Loss: 0.00152395
Iteration 8/25 | Loss: 0.00152395
Iteration 9/25 | Loss: 0.00152395
Iteration 10/25 | Loss: 0.00152395
Iteration 11/25 | Loss: 0.00152395
Iteration 12/25 | Loss: 0.00152395
Iteration 13/25 | Loss: 0.00152395
Iteration 14/25 | Loss: 0.00152395
Iteration 15/25 | Loss: 0.00152395
Iteration 16/25 | Loss: 0.00152395
Iteration 17/25 | Loss: 0.00152395
Iteration 18/25 | Loss: 0.00152395
Iteration 19/25 | Loss: 0.00152395
Iteration 20/25 | Loss: 0.00152395
Iteration 21/25 | Loss: 0.00152395
Iteration 22/25 | Loss: 0.00152395
Iteration 23/25 | Loss: 0.00152395
Iteration 24/25 | Loss: 0.00152395
Iteration 25/25 | Loss: 0.00152395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015239492058753967, 0.0015239492058753967, 0.0015239492058753967, 0.0015239492058753967, 0.0015239492058753967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015239492058753967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152395
Iteration 2/1000 | Loss: 0.00004738
Iteration 3/1000 | Loss: 0.00003088
Iteration 4/1000 | Loss: 0.00002743
Iteration 5/1000 | Loss: 0.00002606
Iteration 6/1000 | Loss: 0.00002510
Iteration 7/1000 | Loss: 0.00002455
Iteration 8/1000 | Loss: 0.00002413
Iteration 9/1000 | Loss: 0.00002370
Iteration 10/1000 | Loss: 0.00002342
Iteration 11/1000 | Loss: 0.00002319
Iteration 12/1000 | Loss: 0.00002295
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002282
Iteration 15/1000 | Loss: 0.00002275
Iteration 16/1000 | Loss: 0.00002270
Iteration 17/1000 | Loss: 0.00002269
Iteration 18/1000 | Loss: 0.00002269
Iteration 19/1000 | Loss: 0.00002266
Iteration 20/1000 | Loss: 0.00002266
Iteration 21/1000 | Loss: 0.00002265
Iteration 22/1000 | Loss: 0.00002264
Iteration 23/1000 | Loss: 0.00002264
Iteration 24/1000 | Loss: 0.00002263
Iteration 25/1000 | Loss: 0.00002263
Iteration 26/1000 | Loss: 0.00002262
Iteration 27/1000 | Loss: 0.00002262
Iteration 28/1000 | Loss: 0.00002261
Iteration 29/1000 | Loss: 0.00002257
Iteration 30/1000 | Loss: 0.00002256
Iteration 31/1000 | Loss: 0.00002255
Iteration 32/1000 | Loss: 0.00002254
Iteration 33/1000 | Loss: 0.00002253
Iteration 34/1000 | Loss: 0.00002252
Iteration 35/1000 | Loss: 0.00002251
Iteration 36/1000 | Loss: 0.00002247
Iteration 37/1000 | Loss: 0.00002241
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002238
Iteration 40/1000 | Loss: 0.00002238
Iteration 41/1000 | Loss: 0.00002238
Iteration 42/1000 | Loss: 0.00002237
Iteration 43/1000 | Loss: 0.00002237
Iteration 44/1000 | Loss: 0.00002235
Iteration 45/1000 | Loss: 0.00002233
Iteration 46/1000 | Loss: 0.00002232
Iteration 47/1000 | Loss: 0.00002232
Iteration 48/1000 | Loss: 0.00002231
Iteration 49/1000 | Loss: 0.00002231
Iteration 50/1000 | Loss: 0.00002231
Iteration 51/1000 | Loss: 0.00002231
Iteration 52/1000 | Loss: 0.00002231
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002230
Iteration 55/1000 | Loss: 0.00002230
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002229
Iteration 60/1000 | Loss: 0.00002229
Iteration 61/1000 | Loss: 0.00002229
Iteration 62/1000 | Loss: 0.00002229
Iteration 63/1000 | Loss: 0.00002228
Iteration 64/1000 | Loss: 0.00002228
Iteration 65/1000 | Loss: 0.00002228
Iteration 66/1000 | Loss: 0.00002228
Iteration 67/1000 | Loss: 0.00002228
Iteration 68/1000 | Loss: 0.00002228
Iteration 69/1000 | Loss: 0.00002228
Iteration 70/1000 | Loss: 0.00002228
Iteration 71/1000 | Loss: 0.00002228
Iteration 72/1000 | Loss: 0.00002227
Iteration 73/1000 | Loss: 0.00002227
Iteration 74/1000 | Loss: 0.00002227
Iteration 75/1000 | Loss: 0.00002227
Iteration 76/1000 | Loss: 0.00002226
Iteration 77/1000 | Loss: 0.00002226
Iteration 78/1000 | Loss: 0.00002226
Iteration 79/1000 | Loss: 0.00002226
Iteration 80/1000 | Loss: 0.00002226
Iteration 81/1000 | Loss: 0.00002226
Iteration 82/1000 | Loss: 0.00002226
Iteration 83/1000 | Loss: 0.00002226
Iteration 84/1000 | Loss: 0.00002226
Iteration 85/1000 | Loss: 0.00002226
Iteration 86/1000 | Loss: 0.00002226
Iteration 87/1000 | Loss: 0.00002226
Iteration 88/1000 | Loss: 0.00002226
Iteration 89/1000 | Loss: 0.00002226
Iteration 90/1000 | Loss: 0.00002226
Iteration 91/1000 | Loss: 0.00002226
Iteration 92/1000 | Loss: 0.00002226
Iteration 93/1000 | Loss: 0.00002226
Iteration 94/1000 | Loss: 0.00002226
Iteration 95/1000 | Loss: 0.00002226
Iteration 96/1000 | Loss: 0.00002226
Iteration 97/1000 | Loss: 0.00002226
Iteration 98/1000 | Loss: 0.00002226
Iteration 99/1000 | Loss: 0.00002226
Iteration 100/1000 | Loss: 0.00002226
Iteration 101/1000 | Loss: 0.00002226
Iteration 102/1000 | Loss: 0.00002226
Iteration 103/1000 | Loss: 0.00002226
Iteration 104/1000 | Loss: 0.00002226
Iteration 105/1000 | Loss: 0.00002226
Iteration 106/1000 | Loss: 0.00002226
Iteration 107/1000 | Loss: 0.00002226
Iteration 108/1000 | Loss: 0.00002226
Iteration 109/1000 | Loss: 0.00002226
Iteration 110/1000 | Loss: 0.00002226
Iteration 111/1000 | Loss: 0.00002226
Iteration 112/1000 | Loss: 0.00002226
Iteration 113/1000 | Loss: 0.00002226
Iteration 114/1000 | Loss: 0.00002226
Iteration 115/1000 | Loss: 0.00002226
Iteration 116/1000 | Loss: 0.00002226
Iteration 117/1000 | Loss: 0.00002226
Iteration 118/1000 | Loss: 0.00002226
Iteration 119/1000 | Loss: 0.00002226
Iteration 120/1000 | Loss: 0.00002226
Iteration 121/1000 | Loss: 0.00002226
Iteration 122/1000 | Loss: 0.00002226
Iteration 123/1000 | Loss: 0.00002226
Iteration 124/1000 | Loss: 0.00002226
Iteration 125/1000 | Loss: 0.00002226
Iteration 126/1000 | Loss: 0.00002226
Iteration 127/1000 | Loss: 0.00002226
Iteration 128/1000 | Loss: 0.00002226
Iteration 129/1000 | Loss: 0.00002226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.2259651814238168e-05, 2.2259651814238168e-05, 2.2259651814238168e-05, 2.2259651814238168e-05, 2.2259651814238168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2259651814238168e-05

Optimization complete. Final v2v error: 4.103261470794678 mm

Highest mean error: 4.507434368133545 mm for frame 113

Lowest mean error: 3.6629512310028076 mm for frame 6

Saving results

Total time: 37.0675835609436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866776
Iteration 2/25 | Loss: 0.00096194
Iteration 3/25 | Loss: 0.00078925
Iteration 4/25 | Loss: 0.00076328
Iteration 5/25 | Loss: 0.00075596
Iteration 6/25 | Loss: 0.00075364
Iteration 7/25 | Loss: 0.00075345
Iteration 8/25 | Loss: 0.00075345
Iteration 9/25 | Loss: 0.00075345
Iteration 10/25 | Loss: 0.00075345
Iteration 11/25 | Loss: 0.00075345
Iteration 12/25 | Loss: 0.00075345
Iteration 13/25 | Loss: 0.00075345
Iteration 14/25 | Loss: 0.00075345
Iteration 15/25 | Loss: 0.00075345
Iteration 16/25 | Loss: 0.00075345
Iteration 17/25 | Loss: 0.00075345
Iteration 18/25 | Loss: 0.00075345
Iteration 19/25 | Loss: 0.00075345
Iteration 20/25 | Loss: 0.00075345
Iteration 21/25 | Loss: 0.00075345
Iteration 22/25 | Loss: 0.00075345
Iteration 23/25 | Loss: 0.00075345
Iteration 24/25 | Loss: 0.00075345
Iteration 25/25 | Loss: 0.00075345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15584207
Iteration 2/25 | Loss: 0.00151433
Iteration 3/25 | Loss: 0.00151431
Iteration 4/25 | Loss: 0.00151431
Iteration 5/25 | Loss: 0.00151431
Iteration 6/25 | Loss: 0.00151431
Iteration 7/25 | Loss: 0.00151431
Iteration 8/25 | Loss: 0.00151431
Iteration 9/25 | Loss: 0.00151431
Iteration 10/25 | Loss: 0.00151431
Iteration 11/25 | Loss: 0.00151431
Iteration 12/25 | Loss: 0.00151431
Iteration 13/25 | Loss: 0.00151431
Iteration 14/25 | Loss: 0.00151431
Iteration 15/25 | Loss: 0.00151431
Iteration 16/25 | Loss: 0.00151431
Iteration 17/25 | Loss: 0.00151431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015143087366595864, 0.0015143087366595864, 0.0015143087366595864, 0.0015143087366595864, 0.0015143087366595864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015143087366595864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151431
Iteration 2/1000 | Loss: 0.00002912
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001608
Iteration 9/1000 | Loss: 0.00001582
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001523
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00001521
Iteration 18/1000 | Loss: 0.00001520
Iteration 19/1000 | Loss: 0.00001520
Iteration 20/1000 | Loss: 0.00001520
Iteration 21/1000 | Loss: 0.00001519
Iteration 22/1000 | Loss: 0.00001519
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001519
Iteration 25/1000 | Loss: 0.00001519
Iteration 26/1000 | Loss: 0.00001518
Iteration 27/1000 | Loss: 0.00001518
Iteration 28/1000 | Loss: 0.00001517
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001516
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001515
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001515
Iteration 37/1000 | Loss: 0.00001514
Iteration 38/1000 | Loss: 0.00001514
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001513
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001511
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001509
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001508
Iteration 52/1000 | Loss: 0.00001508
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001507
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001504
Iteration 58/1000 | Loss: 0.00001504
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001503
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001498
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001498
Iteration 65/1000 | Loss: 0.00001498
Iteration 66/1000 | Loss: 0.00001496
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001493
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001492
Iteration 73/1000 | Loss: 0.00001492
Iteration 74/1000 | Loss: 0.00001490
Iteration 75/1000 | Loss: 0.00001490
Iteration 76/1000 | Loss: 0.00001490
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001486
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00001486
Iteration 92/1000 | Loss: 0.00001486
Iteration 93/1000 | Loss: 0.00001485
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001485
Iteration 99/1000 | Loss: 0.00001485
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001484
Iteration 102/1000 | Loss: 0.00001484
Iteration 103/1000 | Loss: 0.00001483
Iteration 104/1000 | Loss: 0.00001483
Iteration 105/1000 | Loss: 0.00001483
Iteration 106/1000 | Loss: 0.00001483
Iteration 107/1000 | Loss: 0.00001483
Iteration 108/1000 | Loss: 0.00001483
Iteration 109/1000 | Loss: 0.00001483
Iteration 110/1000 | Loss: 0.00001483
Iteration 111/1000 | Loss: 0.00001483
Iteration 112/1000 | Loss: 0.00001483
Iteration 113/1000 | Loss: 0.00001483
Iteration 114/1000 | Loss: 0.00001483
Iteration 115/1000 | Loss: 0.00001483
Iteration 116/1000 | Loss: 0.00001483
Iteration 117/1000 | Loss: 0.00001483
Iteration 118/1000 | Loss: 0.00001483
Iteration 119/1000 | Loss: 0.00001483
Iteration 120/1000 | Loss: 0.00001483
Iteration 121/1000 | Loss: 0.00001483
Iteration 122/1000 | Loss: 0.00001483
Iteration 123/1000 | Loss: 0.00001483
Iteration 124/1000 | Loss: 0.00001483
Iteration 125/1000 | Loss: 0.00001483
Iteration 126/1000 | Loss: 0.00001483
Iteration 127/1000 | Loss: 0.00001483
Iteration 128/1000 | Loss: 0.00001483
Iteration 129/1000 | Loss: 0.00001483
Iteration 130/1000 | Loss: 0.00001483
Iteration 131/1000 | Loss: 0.00001483
Iteration 132/1000 | Loss: 0.00001483
Iteration 133/1000 | Loss: 0.00001483
Iteration 134/1000 | Loss: 0.00001483
Iteration 135/1000 | Loss: 0.00001483
Iteration 136/1000 | Loss: 0.00001483
Iteration 137/1000 | Loss: 0.00001483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.482704101363197e-05, 1.482704101363197e-05, 1.482704101363197e-05, 1.482704101363197e-05, 1.482704101363197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.482704101363197e-05

Optimization complete. Final v2v error: 3.2974419593811035 mm

Highest mean error: 4.061403274536133 mm for frame 30

Lowest mean error: 2.890188694000244 mm for frame 6

Saving results

Total time: 36.41318416595459
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048533
Iteration 2/25 | Loss: 0.00270737
Iteration 3/25 | Loss: 0.00159512
Iteration 4/25 | Loss: 0.00136838
Iteration 5/25 | Loss: 0.00129704
Iteration 6/25 | Loss: 0.00120592
Iteration 7/25 | Loss: 0.00119227
Iteration 8/25 | Loss: 0.00116727
Iteration 9/25 | Loss: 0.00111758
Iteration 10/25 | Loss: 0.00112213
Iteration 11/25 | Loss: 0.00108608
Iteration 12/25 | Loss: 0.00105245
Iteration 13/25 | Loss: 0.00103546
Iteration 14/25 | Loss: 0.00100188
Iteration 15/25 | Loss: 0.00100867
Iteration 16/25 | Loss: 0.00099367
Iteration 17/25 | Loss: 0.00098198
Iteration 18/25 | Loss: 0.00099159
Iteration 19/25 | Loss: 0.00096778
Iteration 20/25 | Loss: 0.00097709
Iteration 21/25 | Loss: 0.00097652
Iteration 22/25 | Loss: 0.00097284
Iteration 23/25 | Loss: 0.00096871
Iteration 24/25 | Loss: 0.00095359
Iteration 25/25 | Loss: 0.00097292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24608898
Iteration 2/25 | Loss: 0.00464364
Iteration 3/25 | Loss: 0.00368637
Iteration 4/25 | Loss: 0.00368614
Iteration 5/25 | Loss: 0.00368614
Iteration 6/25 | Loss: 0.00368614
Iteration 7/25 | Loss: 0.00368614
Iteration 8/25 | Loss: 0.00368614
Iteration 9/25 | Loss: 0.00368614
Iteration 10/25 | Loss: 0.00368614
Iteration 11/25 | Loss: 0.00368614
Iteration 12/25 | Loss: 0.00368614
Iteration 13/25 | Loss: 0.00368614
Iteration 14/25 | Loss: 0.00368614
Iteration 15/25 | Loss: 0.00368614
Iteration 16/25 | Loss: 0.00368614
Iteration 17/25 | Loss: 0.00368614
Iteration 18/25 | Loss: 0.00368614
Iteration 19/25 | Loss: 0.00368614
Iteration 20/25 | Loss: 0.00368614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003686140989884734, 0.003686140989884734, 0.003686140989884734, 0.003686140989884734, 0.003686140989884734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003686140989884734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00368614
Iteration 2/1000 | Loss: 0.00253755
Iteration 3/1000 | Loss: 0.00223273
Iteration 4/1000 | Loss: 0.00244194
Iteration 5/1000 | Loss: 0.00134817
Iteration 6/1000 | Loss: 0.00108944
Iteration 7/1000 | Loss: 0.00237077
Iteration 8/1000 | Loss: 0.00408717
Iteration 9/1000 | Loss: 0.00206073
Iteration 10/1000 | Loss: 0.00137405
Iteration 11/1000 | Loss: 0.00143627
Iteration 12/1000 | Loss: 0.00222358
Iteration 13/1000 | Loss: 0.00181732
Iteration 14/1000 | Loss: 0.00591085
Iteration 15/1000 | Loss: 0.00506340
Iteration 16/1000 | Loss: 0.00158096
Iteration 17/1000 | Loss: 0.00165044
Iteration 18/1000 | Loss: 0.00180182
Iteration 19/1000 | Loss: 0.00206113
Iteration 20/1000 | Loss: 0.00373230
Iteration 21/1000 | Loss: 0.00181919
Iteration 22/1000 | Loss: 0.00191763
Iteration 23/1000 | Loss: 0.00177054
Iteration 24/1000 | Loss: 0.00161705
Iteration 25/1000 | Loss: 0.00264715
Iteration 26/1000 | Loss: 0.00173608
Iteration 27/1000 | Loss: 0.00704019
Iteration 28/1000 | Loss: 0.00574740
Iteration 29/1000 | Loss: 0.00346416
Iteration 30/1000 | Loss: 0.00434451
Iteration 31/1000 | Loss: 0.00125763
Iteration 32/1000 | Loss: 0.00114127
Iteration 33/1000 | Loss: 0.00126235
Iteration 34/1000 | Loss: 0.00091017
Iteration 35/1000 | Loss: 0.00094629
Iteration 36/1000 | Loss: 0.00329281
Iteration 37/1000 | Loss: 0.00162125
Iteration 38/1000 | Loss: 0.00352591
Iteration 39/1000 | Loss: 0.00226488
Iteration 40/1000 | Loss: 0.00250195
Iteration 41/1000 | Loss: 0.00230770
Iteration 42/1000 | Loss: 0.00202893
Iteration 43/1000 | Loss: 0.00206751
Iteration 44/1000 | Loss: 0.00240331
Iteration 45/1000 | Loss: 0.00128402
Iteration 46/1000 | Loss: 0.00213725
Iteration 47/1000 | Loss: 0.00244941
Iteration 48/1000 | Loss: 0.00178254
Iteration 49/1000 | Loss: 0.00198549
Iteration 50/1000 | Loss: 0.00193529
Iteration 51/1000 | Loss: 0.00194521
Iteration 52/1000 | Loss: 0.00194704
Iteration 53/1000 | Loss: 0.00178013
Iteration 54/1000 | Loss: 0.00269426
Iteration 55/1000 | Loss: 0.00243969
Iteration 56/1000 | Loss: 0.00131982
Iteration 57/1000 | Loss: 0.00191211
Iteration 58/1000 | Loss: 0.00168049
Iteration 59/1000 | Loss: 0.00175798
Iteration 60/1000 | Loss: 0.00210805
Iteration 61/1000 | Loss: 0.00208052
Iteration 62/1000 | Loss: 0.00210213
Iteration 63/1000 | Loss: 0.00164590
Iteration 64/1000 | Loss: 0.00093637
Iteration 65/1000 | Loss: 0.00092175
Iteration 66/1000 | Loss: 0.00249547
Iteration 67/1000 | Loss: 0.00147687
Iteration 68/1000 | Loss: 0.00101887
Iteration 69/1000 | Loss: 0.00210424
Iteration 70/1000 | Loss: 0.00137728
Iteration 71/1000 | Loss: 0.00109377
Iteration 72/1000 | Loss: 0.00121281
Iteration 73/1000 | Loss: 0.00187405
Iteration 74/1000 | Loss: 0.00208113
Iteration 75/1000 | Loss: 0.00155376
Iteration 76/1000 | Loss: 0.00180252
Iteration 77/1000 | Loss: 0.00155826
Iteration 78/1000 | Loss: 0.00083053
Iteration 79/1000 | Loss: 0.00084729
Iteration 80/1000 | Loss: 0.00284700
Iteration 81/1000 | Loss: 0.00367134
Iteration 82/1000 | Loss: 0.00107607
Iteration 83/1000 | Loss: 0.00116164
Iteration 84/1000 | Loss: 0.00236839
Iteration 85/1000 | Loss: 0.00137331
Iteration 86/1000 | Loss: 0.00183080
Iteration 87/1000 | Loss: 0.00360219
Iteration 88/1000 | Loss: 0.00129959
Iteration 89/1000 | Loss: 0.00107925
Iteration 90/1000 | Loss: 0.00108864
Iteration 91/1000 | Loss: 0.00228336
Iteration 92/1000 | Loss: 0.00148803
Iteration 93/1000 | Loss: 0.00227849
Iteration 94/1000 | Loss: 0.00133527
Iteration 95/1000 | Loss: 0.00225385
Iteration 96/1000 | Loss: 0.00184911
Iteration 97/1000 | Loss: 0.00123810
Iteration 98/1000 | Loss: 0.00121770
Iteration 99/1000 | Loss: 0.00269198
Iteration 100/1000 | Loss: 0.00525629
Iteration 101/1000 | Loss: 0.00424782
Iteration 102/1000 | Loss: 0.00198533
Iteration 103/1000 | Loss: 0.00116954
Iteration 104/1000 | Loss: 0.00183349
Iteration 105/1000 | Loss: 0.00250698
Iteration 106/1000 | Loss: 0.00182177
Iteration 107/1000 | Loss: 0.00326959
Iteration 108/1000 | Loss: 0.00230062
Iteration 109/1000 | Loss: 0.00159921
Iteration 110/1000 | Loss: 0.00155308
Iteration 111/1000 | Loss: 0.00394512
Iteration 112/1000 | Loss: 0.00271692
Iteration 113/1000 | Loss: 0.00192271
Iteration 114/1000 | Loss: 0.00125722
Iteration 115/1000 | Loss: 0.00198857
Iteration 116/1000 | Loss: 0.00161846
Iteration 117/1000 | Loss: 0.00153846
Iteration 118/1000 | Loss: 0.00218124
Iteration 119/1000 | Loss: 0.00343685
Iteration 120/1000 | Loss: 0.00202842
Iteration 121/1000 | Loss: 0.00210422
Iteration 122/1000 | Loss: 0.00355544
Iteration 123/1000 | Loss: 0.00249188
Iteration 124/1000 | Loss: 0.00143562
Iteration 125/1000 | Loss: 0.00177268
Iteration 126/1000 | Loss: 0.00295743
Iteration 127/1000 | Loss: 0.00570727
Iteration 128/1000 | Loss: 0.00389467
Iteration 129/1000 | Loss: 0.00238611
Iteration 130/1000 | Loss: 0.00160258
Iteration 131/1000 | Loss: 0.00119468
Iteration 132/1000 | Loss: 0.00118024
Iteration 133/1000 | Loss: 0.00135706
Iteration 134/1000 | Loss: 0.00223759
Iteration 135/1000 | Loss: 0.00185893
Iteration 136/1000 | Loss: 0.00327466
Iteration 137/1000 | Loss: 0.00291426
Iteration 138/1000 | Loss: 0.00362799
Iteration 139/1000 | Loss: 0.00119974
Iteration 140/1000 | Loss: 0.00288842
Iteration 141/1000 | Loss: 0.00124121
Iteration 142/1000 | Loss: 0.00128656
Iteration 143/1000 | Loss: 0.00107226
Iteration 144/1000 | Loss: 0.00135930
Iteration 145/1000 | Loss: 0.00120957
Iteration 146/1000 | Loss: 0.00109045
Iteration 147/1000 | Loss: 0.00120960
Iteration 148/1000 | Loss: 0.00108547
Iteration 149/1000 | Loss: 0.00118712
Iteration 150/1000 | Loss: 0.00183848
Iteration 151/1000 | Loss: 0.00123057
Iteration 152/1000 | Loss: 0.00098135
Iteration 153/1000 | Loss: 0.00405627
Iteration 154/1000 | Loss: 0.00147323
Iteration 155/1000 | Loss: 0.00098559
Iteration 156/1000 | Loss: 0.00326246
Iteration 157/1000 | Loss: 0.00078818
Iteration 158/1000 | Loss: 0.00103304
Iteration 159/1000 | Loss: 0.00092813
Iteration 160/1000 | Loss: 0.00163905
Iteration 161/1000 | Loss: 0.00058543
Iteration 162/1000 | Loss: 0.00123077
Iteration 163/1000 | Loss: 0.00096820
Iteration 164/1000 | Loss: 0.00099744
Iteration 165/1000 | Loss: 0.00107443
Iteration 166/1000 | Loss: 0.00259586
Iteration 167/1000 | Loss: 0.00111831
Iteration 168/1000 | Loss: 0.00083948
Iteration 169/1000 | Loss: 0.00107120
Iteration 170/1000 | Loss: 0.00097624
Iteration 171/1000 | Loss: 0.00076830
Iteration 172/1000 | Loss: 0.00094554
Iteration 173/1000 | Loss: 0.00066357
Iteration 174/1000 | Loss: 0.00186029
Iteration 175/1000 | Loss: 0.00231437
Iteration 176/1000 | Loss: 0.00073309
Iteration 177/1000 | Loss: 0.00124042
Iteration 178/1000 | Loss: 0.00093873
Iteration 179/1000 | Loss: 0.00138952
Iteration 180/1000 | Loss: 0.00067439
Iteration 181/1000 | Loss: 0.00047683
Iteration 182/1000 | Loss: 0.00096943
Iteration 183/1000 | Loss: 0.00086097
Iteration 184/1000 | Loss: 0.00095037
Iteration 185/1000 | Loss: 0.00086347
Iteration 186/1000 | Loss: 0.00061814
Iteration 187/1000 | Loss: 0.00063712
Iteration 188/1000 | Loss: 0.00060166
Iteration 189/1000 | Loss: 0.00113034
Iteration 190/1000 | Loss: 0.00074720
Iteration 191/1000 | Loss: 0.00069689
Iteration 192/1000 | Loss: 0.00085288
Iteration 193/1000 | Loss: 0.00065276
Iteration 194/1000 | Loss: 0.00070187
Iteration 195/1000 | Loss: 0.00078084
Iteration 196/1000 | Loss: 0.00116911
Iteration 197/1000 | Loss: 0.00094752
Iteration 198/1000 | Loss: 0.00135435
Iteration 199/1000 | Loss: 0.00118973
Iteration 200/1000 | Loss: 0.00081461
Iteration 201/1000 | Loss: 0.00084020
Iteration 202/1000 | Loss: 0.00078572
Iteration 203/1000 | Loss: 0.00102316
Iteration 204/1000 | Loss: 0.00093404
Iteration 205/1000 | Loss: 0.00090822
Iteration 206/1000 | Loss: 0.00128385
Iteration 207/1000 | Loss: 0.00123141
Iteration 208/1000 | Loss: 0.00136553
Iteration 209/1000 | Loss: 0.00109594
Iteration 210/1000 | Loss: 0.00269041
Iteration 211/1000 | Loss: 0.00116712
Iteration 212/1000 | Loss: 0.00174765
Iteration 213/1000 | Loss: 0.00228212
Iteration 214/1000 | Loss: 0.00117595
Iteration 215/1000 | Loss: 0.00307182
Iteration 216/1000 | Loss: 0.00144262
Iteration 217/1000 | Loss: 0.00100955
Iteration 218/1000 | Loss: 0.00085742
Iteration 219/1000 | Loss: 0.00096831
Iteration 220/1000 | Loss: 0.00131385
Iteration 221/1000 | Loss: 0.00100177
Iteration 222/1000 | Loss: 0.00175589
Iteration 223/1000 | Loss: 0.00083810
Iteration 224/1000 | Loss: 0.00099304
Iteration 225/1000 | Loss: 0.00163945
Iteration 226/1000 | Loss: 0.00106369
Iteration 227/1000 | Loss: 0.00170928
Iteration 228/1000 | Loss: 0.00113509
Iteration 229/1000 | Loss: 0.00150052
Iteration 230/1000 | Loss: 0.00087722
Iteration 231/1000 | Loss: 0.00075287
Iteration 232/1000 | Loss: 0.00081010
Iteration 233/1000 | Loss: 0.00069260
Iteration 234/1000 | Loss: 0.00095988
Iteration 235/1000 | Loss: 0.00069876
Iteration 236/1000 | Loss: 0.00088788
Iteration 237/1000 | Loss: 0.00066437
Iteration 238/1000 | Loss: 0.00098012
Iteration 239/1000 | Loss: 0.00123846
Iteration 240/1000 | Loss: 0.00168651
Iteration 241/1000 | Loss: 0.00094669
Iteration 242/1000 | Loss: 0.00172444
Iteration 243/1000 | Loss: 0.00088929
Iteration 244/1000 | Loss: 0.00101003
Iteration 245/1000 | Loss: 0.00071922
Iteration 246/1000 | Loss: 0.00077774
Iteration 247/1000 | Loss: 0.00110679
Iteration 248/1000 | Loss: 0.00101363
Iteration 249/1000 | Loss: 0.00091387
Iteration 250/1000 | Loss: 0.00101646
Iteration 251/1000 | Loss: 0.00100422
Iteration 252/1000 | Loss: 0.00176479
Iteration 253/1000 | Loss: 0.00094220
Iteration 254/1000 | Loss: 0.00127588
Iteration 255/1000 | Loss: 0.00427118
Iteration 256/1000 | Loss: 0.00162947
Iteration 257/1000 | Loss: 0.00100405
Iteration 258/1000 | Loss: 0.00142323
Iteration 259/1000 | Loss: 0.00086203
Iteration 260/1000 | Loss: 0.00079975
Iteration 261/1000 | Loss: 0.00065245
Iteration 262/1000 | Loss: 0.00100790
Iteration 263/1000 | Loss: 0.00079598
Iteration 264/1000 | Loss: 0.00083477
Iteration 265/1000 | Loss: 0.00127978
Iteration 266/1000 | Loss: 0.00190896
Iteration 267/1000 | Loss: 0.00230156
Iteration 268/1000 | Loss: 0.00270777
Iteration 269/1000 | Loss: 0.00191827
Iteration 270/1000 | Loss: 0.00319853
Iteration 271/1000 | Loss: 0.00220275
Iteration 272/1000 | Loss: 0.00140252
Iteration 273/1000 | Loss: 0.00116724
Iteration 274/1000 | Loss: 0.00124505
Iteration 275/1000 | Loss: 0.00345014
Iteration 276/1000 | Loss: 0.00246547
Iteration 277/1000 | Loss: 0.00166939
Iteration 278/1000 | Loss: 0.00414998
Iteration 279/1000 | Loss: 0.00082649
Iteration 280/1000 | Loss: 0.00217813
Iteration 281/1000 | Loss: 0.00088477
Iteration 282/1000 | Loss: 0.00073026
Iteration 283/1000 | Loss: 0.00195222
Iteration 284/1000 | Loss: 0.00120635
Iteration 285/1000 | Loss: 0.00104079
Iteration 286/1000 | Loss: 0.00084092
Iteration 287/1000 | Loss: 0.00093448
Iteration 288/1000 | Loss: 0.00106950
Iteration 289/1000 | Loss: 0.00063454
Iteration 290/1000 | Loss: 0.00117833
Iteration 291/1000 | Loss: 0.00057468
Iteration 292/1000 | Loss: 0.00100083
Iteration 293/1000 | Loss: 0.00074299
Iteration 294/1000 | Loss: 0.00049942
Iteration 295/1000 | Loss: 0.00063905
Iteration 296/1000 | Loss: 0.00090359
Iteration 297/1000 | Loss: 0.00169463
Iteration 298/1000 | Loss: 0.00075763
Iteration 299/1000 | Loss: 0.00129307
Iteration 300/1000 | Loss: 0.00082981
Iteration 301/1000 | Loss: 0.00074666
Iteration 302/1000 | Loss: 0.00076007
Iteration 303/1000 | Loss: 0.00049809
Iteration 304/1000 | Loss: 0.00053062
Iteration 305/1000 | Loss: 0.00060165
Iteration 306/1000 | Loss: 0.00055574
Iteration 307/1000 | Loss: 0.00135216
Iteration 308/1000 | Loss: 0.00095459
Iteration 309/1000 | Loss: 0.00106084
Iteration 310/1000 | Loss: 0.00283981
Iteration 311/1000 | Loss: 0.00110033
Iteration 312/1000 | Loss: 0.00079168
Iteration 313/1000 | Loss: 0.00064898
Iteration 314/1000 | Loss: 0.00065551
Iteration 315/1000 | Loss: 0.00077005
Iteration 316/1000 | Loss: 0.00109213
Iteration 317/1000 | Loss: 0.00080636
Iteration 318/1000 | Loss: 0.00060557
Iteration 319/1000 | Loss: 0.00072155
Iteration 320/1000 | Loss: 0.00073480
Iteration 321/1000 | Loss: 0.00077127
Iteration 322/1000 | Loss: 0.00073530
Iteration 323/1000 | Loss: 0.00064493
Iteration 324/1000 | Loss: 0.00111705
Iteration 325/1000 | Loss: 0.00096636
Iteration 326/1000 | Loss: 0.00080151
Iteration 327/1000 | Loss: 0.00055338
Iteration 328/1000 | Loss: 0.00045154
Iteration 329/1000 | Loss: 0.00043352
Iteration 330/1000 | Loss: 0.00071657
Iteration 331/1000 | Loss: 0.00078070
Iteration 332/1000 | Loss: 0.00069486
Iteration 333/1000 | Loss: 0.00060021
Iteration 334/1000 | Loss: 0.00102635
Iteration 335/1000 | Loss: 0.00059870
Iteration 336/1000 | Loss: 0.00074596
Iteration 337/1000 | Loss: 0.00064724
Iteration 338/1000 | Loss: 0.00063588
Iteration 339/1000 | Loss: 0.00090453
Iteration 340/1000 | Loss: 0.00134460
Iteration 341/1000 | Loss: 0.00036507
Iteration 342/1000 | Loss: 0.00074105
Iteration 343/1000 | Loss: 0.00066979
Iteration 344/1000 | Loss: 0.00048648
Iteration 345/1000 | Loss: 0.00039152
Iteration 346/1000 | Loss: 0.00036037
Iteration 347/1000 | Loss: 0.00070030
Iteration 348/1000 | Loss: 0.00073347
Iteration 349/1000 | Loss: 0.00054175
Iteration 350/1000 | Loss: 0.00090021
Iteration 351/1000 | Loss: 0.00071943
Iteration 352/1000 | Loss: 0.00067447
Iteration 353/1000 | Loss: 0.00075922
Iteration 354/1000 | Loss: 0.00066737
Iteration 355/1000 | Loss: 0.00074934
Iteration 356/1000 | Loss: 0.00062685
Iteration 357/1000 | Loss: 0.00066154
Iteration 358/1000 | Loss: 0.00068492
Iteration 359/1000 | Loss: 0.00079924
Iteration 360/1000 | Loss: 0.00014084
Iteration 361/1000 | Loss: 0.00016519
Iteration 362/1000 | Loss: 0.00012923
Iteration 363/1000 | Loss: 0.00054544
Iteration 364/1000 | Loss: 0.00047409
Iteration 365/1000 | Loss: 0.00145530
Iteration 366/1000 | Loss: 0.00055903
Iteration 367/1000 | Loss: 0.00030463
Iteration 368/1000 | Loss: 0.00037895
Iteration 369/1000 | Loss: 0.00031371
Iteration 370/1000 | Loss: 0.00033113
Iteration 371/1000 | Loss: 0.00035176
Iteration 372/1000 | Loss: 0.00027871
Iteration 373/1000 | Loss: 0.00115279
Iteration 374/1000 | Loss: 0.00040484
Iteration 375/1000 | Loss: 0.00055323
Iteration 376/1000 | Loss: 0.00055832
Iteration 377/1000 | Loss: 0.00045404
Iteration 378/1000 | Loss: 0.00038962
Iteration 379/1000 | Loss: 0.00058602
Iteration 380/1000 | Loss: 0.00059552
Iteration 381/1000 | Loss: 0.00057475
Iteration 382/1000 | Loss: 0.00055155
Iteration 383/1000 | Loss: 0.00130660
Iteration 384/1000 | Loss: 0.00035179
Iteration 385/1000 | Loss: 0.00033586
Iteration 386/1000 | Loss: 0.00024394
Iteration 387/1000 | Loss: 0.00035357
Iteration 388/1000 | Loss: 0.00040319
Iteration 389/1000 | Loss: 0.00039947
Iteration 390/1000 | Loss: 0.00053300
Iteration 391/1000 | Loss: 0.00072100
Iteration 392/1000 | Loss: 0.00052834
Iteration 393/1000 | Loss: 0.00046750
Iteration 394/1000 | Loss: 0.00059269
Iteration 395/1000 | Loss: 0.00060937
Iteration 396/1000 | Loss: 0.00060281
Iteration 397/1000 | Loss: 0.00044355
Iteration 398/1000 | Loss: 0.00039993
Iteration 399/1000 | Loss: 0.00031610
Iteration 400/1000 | Loss: 0.00041832
Iteration 401/1000 | Loss: 0.00035489
Iteration 402/1000 | Loss: 0.00028079
Iteration 403/1000 | Loss: 0.00049204
Iteration 404/1000 | Loss: 0.00031930
Iteration 405/1000 | Loss: 0.00027054
Iteration 406/1000 | Loss: 0.00027744
Iteration 407/1000 | Loss: 0.00025947
Iteration 408/1000 | Loss: 0.00041299
Iteration 409/1000 | Loss: 0.00081625
Iteration 410/1000 | Loss: 0.00138722
Iteration 411/1000 | Loss: 0.00087713
Iteration 412/1000 | Loss: 0.00064117
Iteration 413/1000 | Loss: 0.00044411
Iteration 414/1000 | Loss: 0.00034997
Iteration 415/1000 | Loss: 0.00032016
Iteration 416/1000 | Loss: 0.00029976
Iteration 417/1000 | Loss: 0.00023477
Iteration 418/1000 | Loss: 0.00111272
Iteration 419/1000 | Loss: 0.00035745
Iteration 420/1000 | Loss: 0.00051070
Iteration 421/1000 | Loss: 0.00022221
Iteration 422/1000 | Loss: 0.00032483
Iteration 423/1000 | Loss: 0.00035713
Iteration 424/1000 | Loss: 0.00028551
Iteration 425/1000 | Loss: 0.00029335
Iteration 426/1000 | Loss: 0.00048245
Iteration 427/1000 | Loss: 0.00061612
Iteration 428/1000 | Loss: 0.00173292
Iteration 429/1000 | Loss: 0.00059852
Iteration 430/1000 | Loss: 0.00095350
Iteration 431/1000 | Loss: 0.00077534
Iteration 432/1000 | Loss: 0.00040073
Iteration 433/1000 | Loss: 0.00034073
Iteration 434/1000 | Loss: 0.00040648
Iteration 435/1000 | Loss: 0.00030615
Iteration 436/1000 | Loss: 0.00026835
Iteration 437/1000 | Loss: 0.00053007
Iteration 438/1000 | Loss: 0.00042115
Iteration 439/1000 | Loss: 0.00045367
Iteration 440/1000 | Loss: 0.00042018
Iteration 441/1000 | Loss: 0.00055749
Iteration 442/1000 | Loss: 0.00028219
Iteration 443/1000 | Loss: 0.00050812
Iteration 444/1000 | Loss: 0.00036746
Iteration 445/1000 | Loss: 0.00039482
Iteration 446/1000 | Loss: 0.00038641
Iteration 447/1000 | Loss: 0.00038525
Iteration 448/1000 | Loss: 0.00035382
Iteration 449/1000 | Loss: 0.00037393
Iteration 450/1000 | Loss: 0.00038612
Iteration 451/1000 | Loss: 0.00052690
Iteration 452/1000 | Loss: 0.00053078
Iteration 453/1000 | Loss: 0.00034611
Iteration 454/1000 | Loss: 0.00029438
Iteration 455/1000 | Loss: 0.00044092
Iteration 456/1000 | Loss: 0.00023156
Iteration 457/1000 | Loss: 0.00049882
Iteration 458/1000 | Loss: 0.00027053
Iteration 459/1000 | Loss: 0.00091226
Iteration 460/1000 | Loss: 0.00089014
Iteration 461/1000 | Loss: 0.00056229
Iteration 462/1000 | Loss: 0.00056542
Iteration 463/1000 | Loss: 0.00064459
Iteration 464/1000 | Loss: 0.00045428
Iteration 465/1000 | Loss: 0.00038467
Iteration 466/1000 | Loss: 0.00043796
Iteration 467/1000 | Loss: 0.00035427
Iteration 468/1000 | Loss: 0.00039194
Iteration 469/1000 | Loss: 0.00035764
Iteration 470/1000 | Loss: 0.00040977
Iteration 471/1000 | Loss: 0.00023928
Iteration 472/1000 | Loss: 0.00023624
Iteration 473/1000 | Loss: 0.00021238
Iteration 474/1000 | Loss: 0.00028491
Iteration 475/1000 | Loss: 0.00041158
Iteration 476/1000 | Loss: 0.00042159
Iteration 477/1000 | Loss: 0.00054376
Iteration 478/1000 | Loss: 0.00061417
Iteration 479/1000 | Loss: 0.00057236
Iteration 480/1000 | Loss: 0.00050319
Iteration 481/1000 | Loss: 0.00061706
Iteration 482/1000 | Loss: 0.00044258
Iteration 483/1000 | Loss: 0.00057117
Iteration 484/1000 | Loss: 0.00043287
Iteration 485/1000 | Loss: 0.00034513
Iteration 486/1000 | Loss: 0.00041260
Iteration 487/1000 | Loss: 0.00050613
Iteration 488/1000 | Loss: 0.00040259
Iteration 489/1000 | Loss: 0.00053955
Iteration 490/1000 | Loss: 0.00052672
Iteration 491/1000 | Loss: 0.00022869
Iteration 492/1000 | Loss: 0.00042437
Iteration 493/1000 | Loss: 0.00031623
Iteration 494/1000 | Loss: 0.00098494
Iteration 495/1000 | Loss: 0.00079096
Iteration 496/1000 | Loss: 0.00042896
Iteration 497/1000 | Loss: 0.00038843
Iteration 498/1000 | Loss: 0.00039976
Iteration 499/1000 | Loss: 0.00038234
Iteration 500/1000 | Loss: 0.00036888
Iteration 501/1000 | Loss: 0.00112848
Iteration 502/1000 | Loss: 0.00053398
Iteration 503/1000 | Loss: 0.00039948
Iteration 504/1000 | Loss: 0.00045029
Iteration 505/1000 | Loss: 0.00021302
Iteration 506/1000 | Loss: 0.00046449
Iteration 507/1000 | Loss: 0.00075970
Iteration 508/1000 | Loss: 0.00047280
Iteration 509/1000 | Loss: 0.00037778
Iteration 510/1000 | Loss: 0.00028195
Iteration 511/1000 | Loss: 0.00029101
Iteration 512/1000 | Loss: 0.00018020
Iteration 513/1000 | Loss: 0.00012975
Iteration 514/1000 | Loss: 0.00036280
Iteration 515/1000 | Loss: 0.00019903
Iteration 516/1000 | Loss: 0.00019225
Iteration 517/1000 | Loss: 0.00026175
Iteration 518/1000 | Loss: 0.00024906
Iteration 519/1000 | Loss: 0.00022041
Iteration 520/1000 | Loss: 0.00036073
Iteration 521/1000 | Loss: 0.00051282
Iteration 522/1000 | Loss: 0.00024896
Iteration 523/1000 | Loss: 0.00024712
Iteration 524/1000 | Loss: 0.00007382
Iteration 525/1000 | Loss: 0.00013271
Iteration 526/1000 | Loss: 0.00015742
Iteration 527/1000 | Loss: 0.00018929
Iteration 528/1000 | Loss: 0.00039758
Iteration 529/1000 | Loss: 0.00024633
Iteration 530/1000 | Loss: 0.00018187
Iteration 531/1000 | Loss: 0.00038560
Iteration 532/1000 | Loss: 0.00032467
Iteration 533/1000 | Loss: 0.00029148
Iteration 534/1000 | Loss: 0.00030064
Iteration 535/1000 | Loss: 0.00036050
Iteration 536/1000 | Loss: 0.00047442
Iteration 537/1000 | Loss: 0.00016792
Iteration 538/1000 | Loss: 0.00042167
Iteration 539/1000 | Loss: 0.00048543
Iteration 540/1000 | Loss: 0.00049265
Iteration 541/1000 | Loss: 0.00047118
Iteration 542/1000 | Loss: 0.00051920
Iteration 543/1000 | Loss: 0.00105967
Iteration 544/1000 | Loss: 0.00035055
Iteration 545/1000 | Loss: 0.00017142
Iteration 546/1000 | Loss: 0.00045560
Iteration 547/1000 | Loss: 0.00070635
Iteration 548/1000 | Loss: 0.00044910
Iteration 549/1000 | Loss: 0.00028138
Iteration 550/1000 | Loss: 0.00058632
Iteration 551/1000 | Loss: 0.00074341
Iteration 552/1000 | Loss: 0.00054519
Iteration 553/1000 | Loss: 0.00081709
Iteration 554/1000 | Loss: 0.00059139
Iteration 555/1000 | Loss: 0.00051110
Iteration 556/1000 | Loss: 0.00062770
Iteration 557/1000 | Loss: 0.00048842
Iteration 558/1000 | Loss: 0.00034673
Iteration 559/1000 | Loss: 0.00020419
Iteration 560/1000 | Loss: 0.00021478
Iteration 561/1000 | Loss: 0.00017297
Iteration 562/1000 | Loss: 0.00048481
Iteration 563/1000 | Loss: 0.00030849
Iteration 564/1000 | Loss: 0.00026296
Iteration 565/1000 | Loss: 0.00010214
Iteration 566/1000 | Loss: 0.00033591
Iteration 567/1000 | Loss: 0.00043981
Iteration 568/1000 | Loss: 0.00044273
Iteration 569/1000 | Loss: 0.00036814
Iteration 570/1000 | Loss: 0.00037380
Iteration 571/1000 | Loss: 0.00051952
Iteration 572/1000 | Loss: 0.00028898
Iteration 573/1000 | Loss: 0.00031710
Iteration 574/1000 | Loss: 0.00122315
Iteration 575/1000 | Loss: 0.00067120
Iteration 576/1000 | Loss: 0.00101873
Iteration 577/1000 | Loss: 0.00029929
Iteration 578/1000 | Loss: 0.00051024
Iteration 579/1000 | Loss: 0.00033577
Iteration 580/1000 | Loss: 0.00029108
Iteration 581/1000 | Loss: 0.00050929
Iteration 582/1000 | Loss: 0.00025628
Iteration 583/1000 | Loss: 0.00021197
Iteration 584/1000 | Loss: 0.00035127
Iteration 585/1000 | Loss: 0.00013816
Iteration 586/1000 | Loss: 0.00016162
Iteration 587/1000 | Loss: 0.00010209
Iteration 588/1000 | Loss: 0.00016129
Iteration 589/1000 | Loss: 0.00012797
Iteration 590/1000 | Loss: 0.00023376
Iteration 591/1000 | Loss: 0.00068654
Iteration 592/1000 | Loss: 0.00038720
Iteration 593/1000 | Loss: 0.00023482
Iteration 594/1000 | Loss: 0.00047287
Iteration 595/1000 | Loss: 0.00038891
Iteration 596/1000 | Loss: 0.00021495
Iteration 597/1000 | Loss: 0.00017583
Iteration 598/1000 | Loss: 0.00020373
Iteration 599/1000 | Loss: 0.00023310
Iteration 600/1000 | Loss: 0.00023374
Iteration 601/1000 | Loss: 0.00022219
Iteration 602/1000 | Loss: 0.00017878
Iteration 603/1000 | Loss: 0.00044611
Iteration 604/1000 | Loss: 0.00019512
Iteration 605/1000 | Loss: 0.00007005
Iteration 606/1000 | Loss: 0.00030296
Iteration 607/1000 | Loss: 0.00009229
Iteration 608/1000 | Loss: 0.00010851
Iteration 609/1000 | Loss: 0.00026742
Iteration 610/1000 | Loss: 0.00029527
Iteration 611/1000 | Loss: 0.00034157
Iteration 612/1000 | Loss: 0.00020927
Iteration 613/1000 | Loss: 0.00021008
Iteration 614/1000 | Loss: 0.00038831
Iteration 615/1000 | Loss: 0.00034407
Iteration 616/1000 | Loss: 0.00038257
Iteration 617/1000 | Loss: 0.00040267
Iteration 618/1000 | Loss: 0.00131357
Iteration 619/1000 | Loss: 0.00039508
Iteration 620/1000 | Loss: 0.00033683
Iteration 621/1000 | Loss: 0.00036416
Iteration 622/1000 | Loss: 0.00021590
Iteration 623/1000 | Loss: 0.00062573
Iteration 624/1000 | Loss: 0.00054913
Iteration 625/1000 | Loss: 0.00012657
Iteration 626/1000 | Loss: 0.00008462
Iteration 627/1000 | Loss: 0.00026508
Iteration 628/1000 | Loss: 0.00022445
Iteration 629/1000 | Loss: 0.00033800
Iteration 630/1000 | Loss: 0.00013675
Iteration 631/1000 | Loss: 0.00017563
Iteration 632/1000 | Loss: 0.00011746
Iteration 633/1000 | Loss: 0.00023003
Iteration 634/1000 | Loss: 0.00017786
Iteration 635/1000 | Loss: 0.00014360
Iteration 636/1000 | Loss: 0.00013768
Iteration 637/1000 | Loss: 0.00019494
Iteration 638/1000 | Loss: 0.00024636
Iteration 639/1000 | Loss: 0.00044240
Iteration 640/1000 | Loss: 0.00029620
Iteration 641/1000 | Loss: 0.00042723
Iteration 642/1000 | Loss: 0.00019481
Iteration 643/1000 | Loss: 0.00012750
Iteration 644/1000 | Loss: 0.00017881
Iteration 645/1000 | Loss: 0.00012454
Iteration 646/1000 | Loss: 0.00014242
Iteration 647/1000 | Loss: 0.00019761
Iteration 648/1000 | Loss: 0.00020806
Iteration 649/1000 | Loss: 0.00023693
Iteration 650/1000 | Loss: 0.00041367
Iteration 651/1000 | Loss: 0.00021146
Iteration 652/1000 | Loss: 0.00029553
Iteration 653/1000 | Loss: 0.00012967
Iteration 654/1000 | Loss: 0.00020254
Iteration 655/1000 | Loss: 0.00036535
Iteration 656/1000 | Loss: 0.00026829
Iteration 657/1000 | Loss: 0.00035852
Iteration 658/1000 | Loss: 0.00029504
Iteration 659/1000 | Loss: 0.00009535
Iteration 660/1000 | Loss: 0.00011847
Iteration 661/1000 | Loss: 0.00014386
Iteration 662/1000 | Loss: 0.00009996
Iteration 663/1000 | Loss: 0.00017638
Iteration 664/1000 | Loss: 0.00009640
Iteration 665/1000 | Loss: 0.00015471
Iteration 666/1000 | Loss: 0.00011358
Iteration 667/1000 | Loss: 0.00008153
Iteration 668/1000 | Loss: 0.00019996
Iteration 669/1000 | Loss: 0.00017903
Iteration 670/1000 | Loss: 0.00012866
Iteration 671/1000 | Loss: 0.00012284
Iteration 672/1000 | Loss: 0.00011476
Iteration 673/1000 | Loss: 0.00012208
Iteration 674/1000 | Loss: 0.00013245
Iteration 675/1000 | Loss: 0.00018543
Iteration 676/1000 | Loss: 0.00013411
Iteration 677/1000 | Loss: 0.00049839
Iteration 678/1000 | Loss: 0.00011972
Iteration 679/1000 | Loss: 0.00005434
Iteration 680/1000 | Loss: 0.00004326
Iteration 681/1000 | Loss: 0.00005279
Iteration 682/1000 | Loss: 0.00014774
Iteration 683/1000 | Loss: 0.00019009
Iteration 684/1000 | Loss: 0.00015952
Iteration 685/1000 | Loss: 0.00016999
Iteration 686/1000 | Loss: 0.00013987
Iteration 687/1000 | Loss: 0.00010620
Iteration 688/1000 | Loss: 0.00029541
Iteration 689/1000 | Loss: 0.00007187
Iteration 690/1000 | Loss: 0.00008116
Iteration 691/1000 | Loss: 0.00010809
Iteration 692/1000 | Loss: 0.00008121
Iteration 693/1000 | Loss: 0.00007195
Iteration 694/1000 | Loss: 0.00028992
Iteration 695/1000 | Loss: 0.00024316
Iteration 696/1000 | Loss: 0.00020469
Iteration 697/1000 | Loss: 0.00019802
Iteration 698/1000 | Loss: 0.00019082
Iteration 699/1000 | Loss: 0.00017667
Iteration 700/1000 | Loss: 0.00016755
Iteration 701/1000 | Loss: 0.00029596
Iteration 702/1000 | Loss: 0.00033113
Iteration 703/1000 | Loss: 0.00031423
Iteration 704/1000 | Loss: 0.00032779
Iteration 705/1000 | Loss: 0.00015035
Iteration 706/1000 | Loss: 0.00006752
Iteration 707/1000 | Loss: 0.00009775
Iteration 708/1000 | Loss: 0.00013762
Iteration 709/1000 | Loss: 0.00015201
Iteration 710/1000 | Loss: 0.00026391
Iteration 711/1000 | Loss: 0.00012007
Iteration 712/1000 | Loss: 0.00021260
Iteration 713/1000 | Loss: 0.00010238
Iteration 714/1000 | Loss: 0.00015207
Iteration 715/1000 | Loss: 0.00013064
Iteration 716/1000 | Loss: 0.00017749
Iteration 717/1000 | Loss: 0.00017924
Iteration 718/1000 | Loss: 0.00013165
Iteration 719/1000 | Loss: 0.00013295
Iteration 720/1000 | Loss: 0.00017688
Iteration 721/1000 | Loss: 0.00023964
Iteration 722/1000 | Loss: 0.00028698
Iteration 723/1000 | Loss: 0.00021286
Iteration 724/1000 | Loss: 0.00030015
Iteration 725/1000 | Loss: 0.00024895
Iteration 726/1000 | Loss: 0.00023716
Iteration 727/1000 | Loss: 0.00043246
Iteration 728/1000 | Loss: 0.00028760
Iteration 729/1000 | Loss: 0.00023439
Iteration 730/1000 | Loss: 0.00023168
Iteration 731/1000 | Loss: 0.00012435
Iteration 732/1000 | Loss: 0.00022690
Iteration 733/1000 | Loss: 0.00018665
Iteration 734/1000 | Loss: 0.00019393
Iteration 735/1000 | Loss: 0.00038555
Iteration 736/1000 | Loss: 0.00044283
Iteration 737/1000 | Loss: 0.00031604
Iteration 738/1000 | Loss: 0.00027580
Iteration 739/1000 | Loss: 0.00012040
Iteration 740/1000 | Loss: 0.00015116
Iteration 741/1000 | Loss: 0.00011288
Iteration 742/1000 | Loss: 0.00029394
Iteration 743/1000 | Loss: 0.00067775
Iteration 744/1000 | Loss: 0.00041671
Iteration 745/1000 | Loss: 0.00070855
Iteration 746/1000 | Loss: 0.00021070
Iteration 747/1000 | Loss: 0.00052810
Iteration 748/1000 | Loss: 0.00077164
Iteration 749/1000 | Loss: 0.00097083
Iteration 750/1000 | Loss: 0.00035885
Iteration 751/1000 | Loss: 0.00046510
Iteration 752/1000 | Loss: 0.00020698
Iteration 753/1000 | Loss: 0.00097287
Iteration 754/1000 | Loss: 0.00043490
Iteration 755/1000 | Loss: 0.00058788
Iteration 756/1000 | Loss: 0.00053095
Iteration 757/1000 | Loss: 0.00029369
Iteration 758/1000 | Loss: 0.00020038
Iteration 759/1000 | Loss: 0.00004669
Iteration 760/1000 | Loss: 0.00005224
Iteration 761/1000 | Loss: 0.00004101
Iteration 762/1000 | Loss: 0.00009710
Iteration 763/1000 | Loss: 0.00009220
Iteration 764/1000 | Loss: 0.00052064
Iteration 765/1000 | Loss: 0.00026972
Iteration 766/1000 | Loss: 0.00023493
Iteration 767/1000 | Loss: 0.00015994
Iteration 768/1000 | Loss: 0.00023388
Iteration 769/1000 | Loss: 0.00044069
Iteration 770/1000 | Loss: 0.00059210
Iteration 771/1000 | Loss: 0.00032394
Iteration 772/1000 | Loss: 0.00022155
Iteration 773/1000 | Loss: 0.00018581
Iteration 774/1000 | Loss: 0.00019449
Iteration 775/1000 | Loss: 0.00016109
Iteration 776/1000 | Loss: 0.00019619
Iteration 777/1000 | Loss: 0.00019624
Iteration 778/1000 | Loss: 0.00027924
Iteration 779/1000 | Loss: 0.00021832
Iteration 780/1000 | Loss: 0.00014881
Iteration 781/1000 | Loss: 0.00013429
Iteration 782/1000 | Loss: 0.00012845
Iteration 783/1000 | Loss: 0.00054436
Iteration 784/1000 | Loss: 0.00083435
Iteration 785/1000 | Loss: 0.00052616
Iteration 786/1000 | Loss: 0.00066042
Iteration 787/1000 | Loss: 0.00038341
Iteration 788/1000 | Loss: 0.00019043
Iteration 789/1000 | Loss: 0.00054007
Iteration 790/1000 | Loss: 0.00032692
Iteration 791/1000 | Loss: 0.00034139
Iteration 792/1000 | Loss: 0.00047133
Iteration 793/1000 | Loss: 0.00069041
Iteration 794/1000 | Loss: 0.00042245
Iteration 795/1000 | Loss: 0.00032365
Iteration 796/1000 | Loss: 0.00025255
Iteration 797/1000 | Loss: 0.00032658
Iteration 798/1000 | Loss: 0.00027759
Iteration 799/1000 | Loss: 0.00034408
Iteration 800/1000 | Loss: 0.00009591
Iteration 801/1000 | Loss: 0.00011280
Iteration 802/1000 | Loss: 0.00009945
Iteration 803/1000 | Loss: 0.00019144
Iteration 804/1000 | Loss: 0.00012619
Iteration 805/1000 | Loss: 0.00032633
Iteration 806/1000 | Loss: 0.00015966
Iteration 807/1000 | Loss: 0.00021502
Iteration 808/1000 | Loss: 0.00017515
Iteration 809/1000 | Loss: 0.00022748
Iteration 810/1000 | Loss: 0.00022720
Iteration 811/1000 | Loss: 0.00016469
Iteration 812/1000 | Loss: 0.00015999
Iteration 813/1000 | Loss: 0.00016636
Iteration 814/1000 | Loss: 0.00016165
Iteration 815/1000 | Loss: 0.00015631
Iteration 816/1000 | Loss: 0.00019404
Iteration 817/1000 | Loss: 0.00011933
Iteration 818/1000 | Loss: 0.00021210
Iteration 819/1000 | Loss: 0.00025432
Iteration 820/1000 | Loss: 0.00005198
Iteration 821/1000 | Loss: 0.00006307
Iteration 822/1000 | Loss: 0.00004786
Iteration 823/1000 | Loss: 0.00006086
Iteration 824/1000 | Loss: 0.00004395
Iteration 825/1000 | Loss: 0.00011672
Iteration 826/1000 | Loss: 0.00016402
Iteration 827/1000 | Loss: 0.00042326
Iteration 828/1000 | Loss: 0.00012513
Iteration 829/1000 | Loss: 0.00009447
Iteration 830/1000 | Loss: 0.00013926
Iteration 831/1000 | Loss: 0.00018526
Iteration 832/1000 | Loss: 0.00012616
Iteration 833/1000 | Loss: 0.00010130
Iteration 834/1000 | Loss: 0.00012704
Iteration 835/1000 | Loss: 0.00011928
Iteration 836/1000 | Loss: 0.00037315
Iteration 837/1000 | Loss: 0.00037347
Iteration 838/1000 | Loss: 0.00016200
Iteration 839/1000 | Loss: 0.00051764
Iteration 840/1000 | Loss: 0.00014103
Iteration 841/1000 | Loss: 0.00051196
Iteration 842/1000 | Loss: 0.00018762
Iteration 843/1000 | Loss: 0.00052907
Iteration 844/1000 | Loss: 0.00020401
Iteration 845/1000 | Loss: 0.00028774
Iteration 846/1000 | Loss: 0.00054752
Iteration 847/1000 | Loss: 0.00065380
Iteration 848/1000 | Loss: 0.00033397
Iteration 849/1000 | Loss: 0.00064755
Iteration 850/1000 | Loss: 0.00015371
Iteration 851/1000 | Loss: 0.00016944
Iteration 852/1000 | Loss: 0.00011416
Iteration 853/1000 | Loss: 0.00011065
Iteration 854/1000 | Loss: 0.00020616
Iteration 855/1000 | Loss: 0.00026757
Iteration 856/1000 | Loss: 0.00012391
Iteration 857/1000 | Loss: 0.00013079
Iteration 858/1000 | Loss: 0.00025842
Iteration 859/1000 | Loss: 0.00020937
Iteration 860/1000 | Loss: 0.00020766
Iteration 861/1000 | Loss: 0.00022186
Iteration 862/1000 | Loss: 0.00021849
Iteration 863/1000 | Loss: 0.00019886
Iteration 864/1000 | Loss: 0.00020420
Iteration 865/1000 | Loss: 0.00018767
Iteration 866/1000 | Loss: 0.00024415
Iteration 867/1000 | Loss: 0.00030464
Iteration 868/1000 | Loss: 0.00026157
Iteration 869/1000 | Loss: 0.00027643
Iteration 870/1000 | Loss: 0.00024037
Iteration 871/1000 | Loss: 0.00028573
Iteration 872/1000 | Loss: 0.00015691
Iteration 873/1000 | Loss: 0.00014077
Iteration 874/1000 | Loss: 0.00035704
Iteration 875/1000 | Loss: 0.00018292
Iteration 876/1000 | Loss: 0.00012288
Iteration 877/1000 | Loss: 0.00004752
Iteration 878/1000 | Loss: 0.00013024
Iteration 879/1000 | Loss: 0.00052136
Iteration 880/1000 | Loss: 0.00014559
Iteration 881/1000 | Loss: 0.00026172
Iteration 882/1000 | Loss: 0.00034651
Iteration 883/1000 | Loss: 0.00028215
Iteration 884/1000 | Loss: 0.00055354
Iteration 885/1000 | Loss: 0.00023186
Iteration 886/1000 | Loss: 0.00022829
Iteration 887/1000 | Loss: 0.00030524
Iteration 888/1000 | Loss: 0.00027547
Iteration 889/1000 | Loss: 0.00028819
Iteration 890/1000 | Loss: 0.00029890
Iteration 891/1000 | Loss: 0.00012339
Iteration 892/1000 | Loss: 0.00008507
Iteration 893/1000 | Loss: 0.00041378
Iteration 894/1000 | Loss: 0.00051280
Iteration 895/1000 | Loss: 0.00054309
Iteration 896/1000 | Loss: 0.00085457
Iteration 897/1000 | Loss: 0.00057071
Iteration 898/1000 | Loss: 0.00050013
Iteration 899/1000 | Loss: 0.00058946
Iteration 900/1000 | Loss: 0.00013420
Iteration 901/1000 | Loss: 0.00008467
Iteration 902/1000 | Loss: 0.00004724
Iteration 903/1000 | Loss: 0.00002913
Iteration 904/1000 | Loss: 0.00002795
Iteration 905/1000 | Loss: 0.00012759
Iteration 906/1000 | Loss: 0.00009330
Iteration 907/1000 | Loss: 0.00012274
Iteration 908/1000 | Loss: 0.00003159
Iteration 909/1000 | Loss: 0.00003015
Iteration 910/1000 | Loss: 0.00002920
Iteration 911/1000 | Loss: 0.00014627
Iteration 912/1000 | Loss: 0.00011985
Iteration 913/1000 | Loss: 0.00018460
Iteration 914/1000 | Loss: 0.00005633
Iteration 915/1000 | Loss: 0.00032208
Iteration 916/1000 | Loss: 0.00055670
Iteration 917/1000 | Loss: 0.00077662
Iteration 918/1000 | Loss: 0.00016244
Iteration 919/1000 | Loss: 0.00012772
Iteration 920/1000 | Loss: 0.00010279
Iteration 921/1000 | Loss: 0.00011571
Iteration 922/1000 | Loss: 0.00010287
Iteration 923/1000 | Loss: 0.00015604
Iteration 924/1000 | Loss: 0.00008056
Iteration 925/1000 | Loss: 0.00008080
Iteration 926/1000 | Loss: 0.00010736
Iteration 927/1000 | Loss: 0.00010532
Iteration 928/1000 | Loss: 0.00016394
Iteration 929/1000 | Loss: 0.00026426
Iteration 930/1000 | Loss: 0.00010021
Iteration 931/1000 | Loss: 0.00010967
Iteration 932/1000 | Loss: 0.00012846
Iteration 933/1000 | Loss: 0.00011187
Iteration 934/1000 | Loss: 0.00012043
Iteration 935/1000 | Loss: 0.00005160
Iteration 936/1000 | Loss: 0.00007590
Iteration 937/1000 | Loss: 0.00007900
Iteration 938/1000 | Loss: 0.00008388
Iteration 939/1000 | Loss: 0.00003581
Iteration 940/1000 | Loss: 0.00015096
Iteration 941/1000 | Loss: 0.00011615
Iteration 942/1000 | Loss: 0.00010787
Iteration 943/1000 | Loss: 0.00014169
Iteration 944/1000 | Loss: 0.00013921
Iteration 945/1000 | Loss: 0.00009567
Iteration 946/1000 | Loss: 0.00011392
Iteration 947/1000 | Loss: 0.00013602
Iteration 948/1000 | Loss: 0.00019438
Iteration 949/1000 | Loss: 0.00011991
Iteration 950/1000 | Loss: 0.00020950
Iteration 951/1000 | Loss: 0.00005458
Iteration 952/1000 | Loss: 0.00009825
Iteration 953/1000 | Loss: 0.00025154
Iteration 954/1000 | Loss: 0.00008509
Iteration 955/1000 | Loss: 0.00006906
Iteration 956/1000 | Loss: 0.00008537
Iteration 957/1000 | Loss: 0.00002864
Iteration 958/1000 | Loss: 0.00010041
Iteration 959/1000 | Loss: 0.00012731
Iteration 960/1000 | Loss: 0.00007755
Iteration 961/1000 | Loss: 0.00007291
Iteration 962/1000 | Loss: 0.00008143
Iteration 963/1000 | Loss: 0.00006688
Iteration 964/1000 | Loss: 0.00007755
Iteration 965/1000 | Loss: 0.00011018
Iteration 966/1000 | Loss: 0.00005612
Iteration 967/1000 | Loss: 0.00003019
Iteration 968/1000 | Loss: 0.00002719
Iteration 969/1000 | Loss: 0.00007525
Iteration 970/1000 | Loss: 0.00006664
Iteration 971/1000 | Loss: 0.00008551
Iteration 972/1000 | Loss: 0.00015007
Iteration 973/1000 | Loss: 0.00007968
Iteration 974/1000 | Loss: 0.00015915
Iteration 975/1000 | Loss: 0.00009857
Iteration 976/1000 | Loss: 0.00010630
Iteration 977/1000 | Loss: 0.00003871
Iteration 978/1000 | Loss: 0.00002649
Iteration 979/1000 | Loss: 0.00009581
Iteration 980/1000 | Loss: 0.00007493
Iteration 981/1000 | Loss: 0.00005318
Iteration 982/1000 | Loss: 0.00002928
Iteration 983/1000 | Loss: 0.00020063
Iteration 984/1000 | Loss: 0.00011493
Iteration 985/1000 | Loss: 0.00002679
Iteration 986/1000 | Loss: 0.00002449
Iteration 987/1000 | Loss: 0.00002406
Iteration 988/1000 | Loss: 0.00002369
Iteration 989/1000 | Loss: 0.00002352
Iteration 990/1000 | Loss: 0.00002342
Iteration 991/1000 | Loss: 0.00002340
Iteration 992/1000 | Loss: 0.00006434
Iteration 993/1000 | Loss: 0.00009892
Iteration 994/1000 | Loss: 0.00012909
Iteration 995/1000 | Loss: 0.00011120
Iteration 996/1000 | Loss: 0.00006239
Iteration 997/1000 | Loss: 0.00011745
Iteration 998/1000 | Loss: 0.00028447
Iteration 999/1000 | Loss: 0.00002657
Iteration 1000/1000 | Loss: 0.00017512

Optimization complete. Final v2v error: 4.03369140625 mm

Highest mean error: 57.11688995361328 mm for frame 68

Lowest mean error: 3.0048916339874268 mm for frame 86

Saving results

Total time: 1593.1783020496368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782985
Iteration 2/25 | Loss: 0.00209552
Iteration 3/25 | Loss: 0.00135537
Iteration 4/25 | Loss: 0.00116050
Iteration 5/25 | Loss: 0.00114345
Iteration 6/25 | Loss: 0.00103387
Iteration 7/25 | Loss: 0.00097907
Iteration 8/25 | Loss: 0.00095825
Iteration 9/25 | Loss: 0.00093317
Iteration 10/25 | Loss: 0.00093664
Iteration 11/25 | Loss: 0.00090008
Iteration 12/25 | Loss: 0.00089263
Iteration 13/25 | Loss: 0.00087764
Iteration 14/25 | Loss: 0.00087104
Iteration 15/25 | Loss: 0.00086863
Iteration 16/25 | Loss: 0.00086811
Iteration 17/25 | Loss: 0.00087093
Iteration 18/25 | Loss: 0.00086532
Iteration 19/25 | Loss: 0.00086503
Iteration 20/25 | Loss: 0.00086497
Iteration 21/25 | Loss: 0.00086497
Iteration 22/25 | Loss: 0.00086497
Iteration 23/25 | Loss: 0.00086497
Iteration 24/25 | Loss: 0.00086497
Iteration 25/25 | Loss: 0.00086497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.89998245
Iteration 2/25 | Loss: 0.00139269
Iteration 3/25 | Loss: 0.00139255
Iteration 4/25 | Loss: 0.00139255
Iteration 5/25 | Loss: 0.00139255
Iteration 6/25 | Loss: 0.00139255
Iteration 7/25 | Loss: 0.00139255
Iteration 8/25 | Loss: 0.00139255
Iteration 9/25 | Loss: 0.00139255
Iteration 10/25 | Loss: 0.00139255
Iteration 11/25 | Loss: 0.00139255
Iteration 12/25 | Loss: 0.00139255
Iteration 13/25 | Loss: 0.00139255
Iteration 14/25 | Loss: 0.00139255
Iteration 15/25 | Loss: 0.00139255
Iteration 16/25 | Loss: 0.00139255
Iteration 17/25 | Loss: 0.00139255
Iteration 18/25 | Loss: 0.00139255
Iteration 19/25 | Loss: 0.00139255
Iteration 20/25 | Loss: 0.00139255
Iteration 21/25 | Loss: 0.00139255
Iteration 22/25 | Loss: 0.00139255
Iteration 23/25 | Loss: 0.00139255
Iteration 24/25 | Loss: 0.00139255
Iteration 25/25 | Loss: 0.00139255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139255
Iteration 2/1000 | Loss: 0.00003919
Iteration 3/1000 | Loss: 0.00003080
Iteration 4/1000 | Loss: 0.00002872
Iteration 5/1000 | Loss: 0.00002744
Iteration 6/1000 | Loss: 0.00002670
Iteration 7/1000 | Loss: 0.00018606
Iteration 8/1000 | Loss: 0.00002953
Iteration 9/1000 | Loss: 0.00002655
Iteration 10/1000 | Loss: 0.00002514
Iteration 11/1000 | Loss: 0.00002383
Iteration 12/1000 | Loss: 0.00002332
Iteration 13/1000 | Loss: 0.00002309
Iteration 14/1000 | Loss: 0.00002295
Iteration 15/1000 | Loss: 0.00002293
Iteration 16/1000 | Loss: 0.00002276
Iteration 17/1000 | Loss: 0.00002259
Iteration 18/1000 | Loss: 0.00002257
Iteration 19/1000 | Loss: 0.00002241
Iteration 20/1000 | Loss: 0.00002239
Iteration 21/1000 | Loss: 0.00002237
Iteration 22/1000 | Loss: 0.00002237
Iteration 23/1000 | Loss: 0.00002233
Iteration 24/1000 | Loss: 0.00002233
Iteration 25/1000 | Loss: 0.00002233
Iteration 26/1000 | Loss: 0.00002232
Iteration 27/1000 | Loss: 0.00002232
Iteration 28/1000 | Loss: 0.00002232
Iteration 29/1000 | Loss: 0.00002230
Iteration 30/1000 | Loss: 0.00002229
Iteration 31/1000 | Loss: 0.00002227
Iteration 32/1000 | Loss: 0.00002226
Iteration 33/1000 | Loss: 0.00002226
Iteration 34/1000 | Loss: 0.00002225
Iteration 35/1000 | Loss: 0.00002225
Iteration 36/1000 | Loss: 0.00002224
Iteration 37/1000 | Loss: 0.00002224
Iteration 38/1000 | Loss: 0.00002223
Iteration 39/1000 | Loss: 0.00002223
Iteration 40/1000 | Loss: 0.00002222
Iteration 41/1000 | Loss: 0.00002222
Iteration 42/1000 | Loss: 0.00002222
Iteration 43/1000 | Loss: 0.00002222
Iteration 44/1000 | Loss: 0.00002221
Iteration 45/1000 | Loss: 0.00002221
Iteration 46/1000 | Loss: 0.00002220
Iteration 47/1000 | Loss: 0.00002220
Iteration 48/1000 | Loss: 0.00002220
Iteration 49/1000 | Loss: 0.00002219
Iteration 50/1000 | Loss: 0.00002218
Iteration 51/1000 | Loss: 0.00002218
Iteration 52/1000 | Loss: 0.00002218
Iteration 53/1000 | Loss: 0.00002218
Iteration 54/1000 | Loss: 0.00002217
Iteration 55/1000 | Loss: 0.00002217
Iteration 56/1000 | Loss: 0.00002217
Iteration 57/1000 | Loss: 0.00002217
Iteration 58/1000 | Loss: 0.00002216
Iteration 59/1000 | Loss: 0.00002216
Iteration 60/1000 | Loss: 0.00002216
Iteration 61/1000 | Loss: 0.00002216
Iteration 62/1000 | Loss: 0.00002215
Iteration 63/1000 | Loss: 0.00002215
Iteration 64/1000 | Loss: 0.00002215
Iteration 65/1000 | Loss: 0.00002215
Iteration 66/1000 | Loss: 0.00002215
Iteration 67/1000 | Loss: 0.00002215
Iteration 68/1000 | Loss: 0.00002214
Iteration 69/1000 | Loss: 0.00002214
Iteration 70/1000 | Loss: 0.00002214
Iteration 71/1000 | Loss: 0.00002213
Iteration 72/1000 | Loss: 0.00002213
Iteration 73/1000 | Loss: 0.00002213
Iteration 74/1000 | Loss: 0.00002213
Iteration 75/1000 | Loss: 0.00002212
Iteration 76/1000 | Loss: 0.00002212
Iteration 77/1000 | Loss: 0.00002212
Iteration 78/1000 | Loss: 0.00002211
Iteration 79/1000 | Loss: 0.00002211
Iteration 80/1000 | Loss: 0.00002210
Iteration 81/1000 | Loss: 0.00002210
Iteration 82/1000 | Loss: 0.00002210
Iteration 83/1000 | Loss: 0.00002210
Iteration 84/1000 | Loss: 0.00002209
Iteration 85/1000 | Loss: 0.00002209
Iteration 86/1000 | Loss: 0.00002209
Iteration 87/1000 | Loss: 0.00002208
Iteration 88/1000 | Loss: 0.00002208
Iteration 89/1000 | Loss: 0.00002207
Iteration 90/1000 | Loss: 0.00002207
Iteration 91/1000 | Loss: 0.00002207
Iteration 92/1000 | Loss: 0.00002206
Iteration 93/1000 | Loss: 0.00002206
Iteration 94/1000 | Loss: 0.00002206
Iteration 95/1000 | Loss: 0.00002206
Iteration 96/1000 | Loss: 0.00002205
Iteration 97/1000 | Loss: 0.00002205
Iteration 98/1000 | Loss: 0.00002205
Iteration 99/1000 | Loss: 0.00002204
Iteration 100/1000 | Loss: 0.00002204
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002204
Iteration 103/1000 | Loss: 0.00002204
Iteration 104/1000 | Loss: 0.00002204
Iteration 105/1000 | Loss: 0.00002204
Iteration 106/1000 | Loss: 0.00002204
Iteration 107/1000 | Loss: 0.00002204
Iteration 108/1000 | Loss: 0.00002204
Iteration 109/1000 | Loss: 0.00002203
Iteration 110/1000 | Loss: 0.00002203
Iteration 111/1000 | Loss: 0.00002203
Iteration 112/1000 | Loss: 0.00002203
Iteration 113/1000 | Loss: 0.00002203
Iteration 114/1000 | Loss: 0.00002203
Iteration 115/1000 | Loss: 0.00002202
Iteration 116/1000 | Loss: 0.00002202
Iteration 117/1000 | Loss: 0.00002202
Iteration 118/1000 | Loss: 0.00002202
Iteration 119/1000 | Loss: 0.00002202
Iteration 120/1000 | Loss: 0.00002202
Iteration 121/1000 | Loss: 0.00002202
Iteration 122/1000 | Loss: 0.00002202
Iteration 123/1000 | Loss: 0.00002202
Iteration 124/1000 | Loss: 0.00002202
Iteration 125/1000 | Loss: 0.00002202
Iteration 126/1000 | Loss: 0.00002202
Iteration 127/1000 | Loss: 0.00002202
Iteration 128/1000 | Loss: 0.00002202
Iteration 129/1000 | Loss: 0.00002201
Iteration 130/1000 | Loss: 0.00002201
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002200
Iteration 138/1000 | Loss: 0.00002200
Iteration 139/1000 | Loss: 0.00002200
Iteration 140/1000 | Loss: 0.00002200
Iteration 141/1000 | Loss: 0.00002200
Iteration 142/1000 | Loss: 0.00002200
Iteration 143/1000 | Loss: 0.00002200
Iteration 144/1000 | Loss: 0.00002200
Iteration 145/1000 | Loss: 0.00002200
Iteration 146/1000 | Loss: 0.00002200
Iteration 147/1000 | Loss: 0.00002200
Iteration 148/1000 | Loss: 0.00002200
Iteration 149/1000 | Loss: 0.00002200
Iteration 150/1000 | Loss: 0.00002200
Iteration 151/1000 | Loss: 0.00002200
Iteration 152/1000 | Loss: 0.00002199
Iteration 153/1000 | Loss: 0.00002199
Iteration 154/1000 | Loss: 0.00002199
Iteration 155/1000 | Loss: 0.00002199
Iteration 156/1000 | Loss: 0.00002199
Iteration 157/1000 | Loss: 0.00002199
Iteration 158/1000 | Loss: 0.00002199
Iteration 159/1000 | Loss: 0.00002199
Iteration 160/1000 | Loss: 0.00002199
Iteration 161/1000 | Loss: 0.00002199
Iteration 162/1000 | Loss: 0.00002199
Iteration 163/1000 | Loss: 0.00002199
Iteration 164/1000 | Loss: 0.00002199
Iteration 165/1000 | Loss: 0.00002199
Iteration 166/1000 | Loss: 0.00002198
Iteration 167/1000 | Loss: 0.00002198
Iteration 168/1000 | Loss: 0.00002198
Iteration 169/1000 | Loss: 0.00002198
Iteration 170/1000 | Loss: 0.00002198
Iteration 171/1000 | Loss: 0.00002198
Iteration 172/1000 | Loss: 0.00002197
Iteration 173/1000 | Loss: 0.00002197
Iteration 174/1000 | Loss: 0.00002197
Iteration 175/1000 | Loss: 0.00002197
Iteration 176/1000 | Loss: 0.00002197
Iteration 177/1000 | Loss: 0.00002197
Iteration 178/1000 | Loss: 0.00002196
Iteration 179/1000 | Loss: 0.00002196
Iteration 180/1000 | Loss: 0.00002196
Iteration 181/1000 | Loss: 0.00002196
Iteration 182/1000 | Loss: 0.00002196
Iteration 183/1000 | Loss: 0.00002196
Iteration 184/1000 | Loss: 0.00002196
Iteration 185/1000 | Loss: 0.00002196
Iteration 186/1000 | Loss: 0.00002195
Iteration 187/1000 | Loss: 0.00002195
Iteration 188/1000 | Loss: 0.00002195
Iteration 189/1000 | Loss: 0.00002195
Iteration 190/1000 | Loss: 0.00002195
Iteration 191/1000 | Loss: 0.00002195
Iteration 192/1000 | Loss: 0.00002195
Iteration 193/1000 | Loss: 0.00002194
Iteration 194/1000 | Loss: 0.00002194
Iteration 195/1000 | Loss: 0.00002194
Iteration 196/1000 | Loss: 0.00002194
Iteration 197/1000 | Loss: 0.00002194
Iteration 198/1000 | Loss: 0.00002194
Iteration 199/1000 | Loss: 0.00002194
Iteration 200/1000 | Loss: 0.00002194
Iteration 201/1000 | Loss: 0.00002194
Iteration 202/1000 | Loss: 0.00002193
Iteration 203/1000 | Loss: 0.00002193
Iteration 204/1000 | Loss: 0.00002193
Iteration 205/1000 | Loss: 0.00002193
Iteration 206/1000 | Loss: 0.00002193
Iteration 207/1000 | Loss: 0.00002193
Iteration 208/1000 | Loss: 0.00002193
Iteration 209/1000 | Loss: 0.00002193
Iteration 210/1000 | Loss: 0.00002193
Iteration 211/1000 | Loss: 0.00002192
Iteration 212/1000 | Loss: 0.00002192
Iteration 213/1000 | Loss: 0.00002192
Iteration 214/1000 | Loss: 0.00002192
Iteration 215/1000 | Loss: 0.00002192
Iteration 216/1000 | Loss: 0.00002191
Iteration 217/1000 | Loss: 0.00002191
Iteration 218/1000 | Loss: 0.00002191
Iteration 219/1000 | Loss: 0.00002191
Iteration 220/1000 | Loss: 0.00002191
Iteration 221/1000 | Loss: 0.00002191
Iteration 222/1000 | Loss: 0.00002191
Iteration 223/1000 | Loss: 0.00002191
Iteration 224/1000 | Loss: 0.00002191
Iteration 225/1000 | Loss: 0.00002190
Iteration 226/1000 | Loss: 0.00002190
Iteration 227/1000 | Loss: 0.00002190
Iteration 228/1000 | Loss: 0.00002190
Iteration 229/1000 | Loss: 0.00002190
Iteration 230/1000 | Loss: 0.00002190
Iteration 231/1000 | Loss: 0.00002190
Iteration 232/1000 | Loss: 0.00002190
Iteration 233/1000 | Loss: 0.00002190
Iteration 234/1000 | Loss: 0.00002190
Iteration 235/1000 | Loss: 0.00002190
Iteration 236/1000 | Loss: 0.00002189
Iteration 237/1000 | Loss: 0.00002189
Iteration 238/1000 | Loss: 0.00002189
Iteration 239/1000 | Loss: 0.00002189
Iteration 240/1000 | Loss: 0.00002189
Iteration 241/1000 | Loss: 0.00002189
Iteration 242/1000 | Loss: 0.00002189
Iteration 243/1000 | Loss: 0.00002189
Iteration 244/1000 | Loss: 0.00002189
Iteration 245/1000 | Loss: 0.00002189
Iteration 246/1000 | Loss: 0.00002189
Iteration 247/1000 | Loss: 0.00002189
Iteration 248/1000 | Loss: 0.00002189
Iteration 249/1000 | Loss: 0.00002188
Iteration 250/1000 | Loss: 0.00002188
Iteration 251/1000 | Loss: 0.00002188
Iteration 252/1000 | Loss: 0.00002188
Iteration 253/1000 | Loss: 0.00002188
Iteration 254/1000 | Loss: 0.00002188
Iteration 255/1000 | Loss: 0.00002188
Iteration 256/1000 | Loss: 0.00002188
Iteration 257/1000 | Loss: 0.00002188
Iteration 258/1000 | Loss: 0.00002188
Iteration 259/1000 | Loss: 0.00002188
Iteration 260/1000 | Loss: 0.00002188
Iteration 261/1000 | Loss: 0.00002188
Iteration 262/1000 | Loss: 0.00002188
Iteration 263/1000 | Loss: 0.00002188
Iteration 264/1000 | Loss: 0.00002188
Iteration 265/1000 | Loss: 0.00002188
Iteration 266/1000 | Loss: 0.00002188
Iteration 267/1000 | Loss: 0.00002188
Iteration 268/1000 | Loss: 0.00002188
Iteration 269/1000 | Loss: 0.00002188
Iteration 270/1000 | Loss: 0.00002188
Iteration 271/1000 | Loss: 0.00002188
Iteration 272/1000 | Loss: 0.00002188
Iteration 273/1000 | Loss: 0.00002188
Iteration 274/1000 | Loss: 0.00002188
Iteration 275/1000 | Loss: 0.00002188
Iteration 276/1000 | Loss: 0.00002188
Iteration 277/1000 | Loss: 0.00002188
Iteration 278/1000 | Loss: 0.00002188
Iteration 279/1000 | Loss: 0.00002188
Iteration 280/1000 | Loss: 0.00002188
Iteration 281/1000 | Loss: 0.00002188
Iteration 282/1000 | Loss: 0.00002188
Iteration 283/1000 | Loss: 0.00002188
Iteration 284/1000 | Loss: 0.00002188
Iteration 285/1000 | Loss: 0.00002188
Iteration 286/1000 | Loss: 0.00002188
Iteration 287/1000 | Loss: 0.00002188
Iteration 288/1000 | Loss: 0.00002188
Iteration 289/1000 | Loss: 0.00002188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [2.187728023272939e-05, 2.187728023272939e-05, 2.187728023272939e-05, 2.187728023272939e-05, 2.187728023272939e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.187728023272939e-05

Optimization complete. Final v2v error: 4.041450500488281 mm

Highest mean error: 4.674164772033691 mm for frame 129

Lowest mean error: 3.5984280109405518 mm for frame 81

Saving results

Total time: 76.2398145198822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854874
Iteration 2/25 | Loss: 0.00130612
Iteration 3/25 | Loss: 0.00095623
Iteration 4/25 | Loss: 0.00090811
Iteration 5/25 | Loss: 0.00088731
Iteration 6/25 | Loss: 0.00090723
Iteration 7/25 | Loss: 0.00090577
Iteration 8/25 | Loss: 0.00089141
Iteration 9/25 | Loss: 0.00087985
Iteration 10/25 | Loss: 0.00088310
Iteration 11/25 | Loss: 0.00087544
Iteration 12/25 | Loss: 0.00085695
Iteration 13/25 | Loss: 0.00084546
Iteration 14/25 | Loss: 0.00084284
Iteration 15/25 | Loss: 0.00084179
Iteration 16/25 | Loss: 0.00084158
Iteration 17/25 | Loss: 0.00084146
Iteration 18/25 | Loss: 0.00084133
Iteration 19/25 | Loss: 0.00084127
Iteration 20/25 | Loss: 0.00084126
Iteration 21/25 | Loss: 0.00084126
Iteration 22/25 | Loss: 0.00084126
Iteration 23/25 | Loss: 0.00084126
Iteration 24/25 | Loss: 0.00084125
Iteration 25/25 | Loss: 0.00084125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.92578125
Iteration 2/25 | Loss: 0.00153194
Iteration 3/25 | Loss: 0.00153187
Iteration 4/25 | Loss: 0.00153186
Iteration 5/25 | Loss: 0.00153186
Iteration 6/25 | Loss: 0.00153186
Iteration 7/25 | Loss: 0.00153186
Iteration 8/25 | Loss: 0.00153186
Iteration 9/25 | Loss: 0.00153186
Iteration 10/25 | Loss: 0.00153186
Iteration 11/25 | Loss: 0.00153186
Iteration 12/25 | Loss: 0.00153186
Iteration 13/25 | Loss: 0.00153186
Iteration 14/25 | Loss: 0.00153186
Iteration 15/25 | Loss: 0.00153186
Iteration 16/25 | Loss: 0.00153186
Iteration 17/25 | Loss: 0.00153186
Iteration 18/25 | Loss: 0.00153186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015318626537919044, 0.0015318626537919044, 0.0015318626537919044, 0.0015318626537919044, 0.0015318626537919044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015318626537919044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153186
Iteration 2/1000 | Loss: 0.00003408
Iteration 3/1000 | Loss: 0.00002656
Iteration 4/1000 | Loss: 0.00002514
Iteration 5/1000 | Loss: 0.00002413
Iteration 6/1000 | Loss: 0.00002348
Iteration 7/1000 | Loss: 0.00002299
Iteration 8/1000 | Loss: 0.00002272
Iteration 9/1000 | Loss: 0.00002247
Iteration 10/1000 | Loss: 0.00002239
Iteration 11/1000 | Loss: 0.00002215
Iteration 12/1000 | Loss: 0.00002209
Iteration 13/1000 | Loss: 0.00002203
Iteration 14/1000 | Loss: 0.00002199
Iteration 15/1000 | Loss: 0.00002191
Iteration 16/1000 | Loss: 0.00002184
Iteration 17/1000 | Loss: 0.00002176
Iteration 18/1000 | Loss: 0.00002174
Iteration 19/1000 | Loss: 0.00002169
Iteration 20/1000 | Loss: 0.00002166
Iteration 21/1000 | Loss: 0.00002165
Iteration 22/1000 | Loss: 0.00002162
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002151
Iteration 25/1000 | Loss: 0.00002150
Iteration 26/1000 | Loss: 0.00002149
Iteration 27/1000 | Loss: 0.00002148
Iteration 28/1000 | Loss: 0.00002148
Iteration 29/1000 | Loss: 0.00002146
Iteration 30/1000 | Loss: 0.00002145
Iteration 31/1000 | Loss: 0.00002145
Iteration 32/1000 | Loss: 0.00002144
Iteration 33/1000 | Loss: 0.00002144
Iteration 34/1000 | Loss: 0.00002142
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002141
Iteration 40/1000 | Loss: 0.00002141
Iteration 41/1000 | Loss: 0.00002141
Iteration 42/1000 | Loss: 0.00002140
Iteration 43/1000 | Loss: 0.00002140
Iteration 44/1000 | Loss: 0.00002140
Iteration 45/1000 | Loss: 0.00002140
Iteration 46/1000 | Loss: 0.00002139
Iteration 47/1000 | Loss: 0.00002139
Iteration 48/1000 | Loss: 0.00002138
Iteration 49/1000 | Loss: 0.00002138
Iteration 50/1000 | Loss: 0.00002138
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002137
Iteration 54/1000 | Loss: 0.00002137
Iteration 55/1000 | Loss: 0.00002137
Iteration 56/1000 | Loss: 0.00002137
Iteration 57/1000 | Loss: 0.00002137
Iteration 58/1000 | Loss: 0.00002137
Iteration 59/1000 | Loss: 0.00002136
Iteration 60/1000 | Loss: 0.00002136
Iteration 61/1000 | Loss: 0.00002135
Iteration 62/1000 | Loss: 0.00002135
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002134
Iteration 65/1000 | Loss: 0.00002134
Iteration 66/1000 | Loss: 0.00002134
Iteration 67/1000 | Loss: 0.00002134
Iteration 68/1000 | Loss: 0.00002134
Iteration 69/1000 | Loss: 0.00002134
Iteration 70/1000 | Loss: 0.00002134
Iteration 71/1000 | Loss: 0.00002133
Iteration 72/1000 | Loss: 0.00002133
Iteration 73/1000 | Loss: 0.00002133
Iteration 74/1000 | Loss: 0.00002133
Iteration 75/1000 | Loss: 0.00002133
Iteration 76/1000 | Loss: 0.00002132
Iteration 77/1000 | Loss: 0.00002132
Iteration 78/1000 | Loss: 0.00002132
Iteration 79/1000 | Loss: 0.00002132
Iteration 80/1000 | Loss: 0.00002132
Iteration 81/1000 | Loss: 0.00002132
Iteration 82/1000 | Loss: 0.00002132
Iteration 83/1000 | Loss: 0.00002132
Iteration 84/1000 | Loss: 0.00002132
Iteration 85/1000 | Loss: 0.00002132
Iteration 86/1000 | Loss: 0.00002132
Iteration 87/1000 | Loss: 0.00002132
Iteration 88/1000 | Loss: 0.00002131
Iteration 89/1000 | Loss: 0.00002131
Iteration 90/1000 | Loss: 0.00002131
Iteration 91/1000 | Loss: 0.00002131
Iteration 92/1000 | Loss: 0.00002131
Iteration 93/1000 | Loss: 0.00002131
Iteration 94/1000 | Loss: 0.00002130
Iteration 95/1000 | Loss: 0.00002130
Iteration 96/1000 | Loss: 0.00002130
Iteration 97/1000 | Loss: 0.00002130
Iteration 98/1000 | Loss: 0.00002130
Iteration 99/1000 | Loss: 0.00002130
Iteration 100/1000 | Loss: 0.00002130
Iteration 101/1000 | Loss: 0.00002130
Iteration 102/1000 | Loss: 0.00002129
Iteration 103/1000 | Loss: 0.00002129
Iteration 104/1000 | Loss: 0.00002129
Iteration 105/1000 | Loss: 0.00002129
Iteration 106/1000 | Loss: 0.00002129
Iteration 107/1000 | Loss: 0.00002129
Iteration 108/1000 | Loss: 0.00002128
Iteration 109/1000 | Loss: 0.00002128
Iteration 110/1000 | Loss: 0.00002128
Iteration 111/1000 | Loss: 0.00002128
Iteration 112/1000 | Loss: 0.00002128
Iteration 113/1000 | Loss: 0.00002128
Iteration 114/1000 | Loss: 0.00002128
Iteration 115/1000 | Loss: 0.00002128
Iteration 116/1000 | Loss: 0.00002128
Iteration 117/1000 | Loss: 0.00002127
Iteration 118/1000 | Loss: 0.00002127
Iteration 119/1000 | Loss: 0.00002126
Iteration 120/1000 | Loss: 0.00002126
Iteration 121/1000 | Loss: 0.00002126
Iteration 122/1000 | Loss: 0.00002126
Iteration 123/1000 | Loss: 0.00002126
Iteration 124/1000 | Loss: 0.00002126
Iteration 125/1000 | Loss: 0.00002126
Iteration 126/1000 | Loss: 0.00002126
Iteration 127/1000 | Loss: 0.00002126
Iteration 128/1000 | Loss: 0.00002126
Iteration 129/1000 | Loss: 0.00002126
Iteration 130/1000 | Loss: 0.00002126
Iteration 131/1000 | Loss: 0.00002126
Iteration 132/1000 | Loss: 0.00002126
Iteration 133/1000 | Loss: 0.00002126
Iteration 134/1000 | Loss: 0.00002125
Iteration 135/1000 | Loss: 0.00002125
Iteration 136/1000 | Loss: 0.00002125
Iteration 137/1000 | Loss: 0.00002125
Iteration 138/1000 | Loss: 0.00002125
Iteration 139/1000 | Loss: 0.00002125
Iteration 140/1000 | Loss: 0.00002125
Iteration 141/1000 | Loss: 0.00002125
Iteration 142/1000 | Loss: 0.00002125
Iteration 143/1000 | Loss: 0.00002125
Iteration 144/1000 | Loss: 0.00002125
Iteration 145/1000 | Loss: 0.00002124
Iteration 146/1000 | Loss: 0.00002124
Iteration 147/1000 | Loss: 0.00002124
Iteration 148/1000 | Loss: 0.00002124
Iteration 149/1000 | Loss: 0.00002124
Iteration 150/1000 | Loss: 0.00002124
Iteration 151/1000 | Loss: 0.00002124
Iteration 152/1000 | Loss: 0.00002124
Iteration 153/1000 | Loss: 0.00002124
Iteration 154/1000 | Loss: 0.00002124
Iteration 155/1000 | Loss: 0.00002124
Iteration 156/1000 | Loss: 0.00002124
Iteration 157/1000 | Loss: 0.00002123
Iteration 158/1000 | Loss: 0.00002123
Iteration 159/1000 | Loss: 0.00002123
Iteration 160/1000 | Loss: 0.00002123
Iteration 161/1000 | Loss: 0.00002123
Iteration 162/1000 | Loss: 0.00002123
Iteration 163/1000 | Loss: 0.00002123
Iteration 164/1000 | Loss: 0.00002123
Iteration 165/1000 | Loss: 0.00002123
Iteration 166/1000 | Loss: 0.00002123
Iteration 167/1000 | Loss: 0.00002123
Iteration 168/1000 | Loss: 0.00002123
Iteration 169/1000 | Loss: 0.00002123
Iteration 170/1000 | Loss: 0.00002123
Iteration 171/1000 | Loss: 0.00002123
Iteration 172/1000 | Loss: 0.00002123
Iteration 173/1000 | Loss: 0.00002123
Iteration 174/1000 | Loss: 0.00002123
Iteration 175/1000 | Loss: 0.00002123
Iteration 176/1000 | Loss: 0.00002123
Iteration 177/1000 | Loss: 0.00002123
Iteration 178/1000 | Loss: 0.00002123
Iteration 179/1000 | Loss: 0.00002123
Iteration 180/1000 | Loss: 0.00002123
Iteration 181/1000 | Loss: 0.00002123
Iteration 182/1000 | Loss: 0.00002123
Iteration 183/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.122526893799659e-05, 2.122526893799659e-05, 2.122526893799659e-05, 2.122526893799659e-05, 2.122526893799659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.122526893799659e-05

Optimization complete. Final v2v error: 3.978271484375 mm

Highest mean error: 4.653858661651611 mm for frame 102

Lowest mean error: 3.0089409351348877 mm for frame 170

Saving results

Total time: 63.75624752044678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00333185
Iteration 2/25 | Loss: 0.00093879
Iteration 3/25 | Loss: 0.00080839
Iteration 4/25 | Loss: 0.00078902
Iteration 5/25 | Loss: 0.00078452
Iteration 6/25 | Loss: 0.00078385
Iteration 7/25 | Loss: 0.00078385
Iteration 8/25 | Loss: 0.00078385
Iteration 9/25 | Loss: 0.00078385
Iteration 10/25 | Loss: 0.00078385
Iteration 11/25 | Loss: 0.00078385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000783851370215416, 0.000783851370215416, 0.000783851370215416, 0.000783851370215416, 0.000783851370215416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000783851370215416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54719806
Iteration 2/25 | Loss: 0.00168720
Iteration 3/25 | Loss: 0.00168720
Iteration 4/25 | Loss: 0.00168720
Iteration 5/25 | Loss: 0.00168720
Iteration 6/25 | Loss: 0.00168720
Iteration 7/25 | Loss: 0.00168720
Iteration 8/25 | Loss: 0.00168720
Iteration 9/25 | Loss: 0.00168720
Iteration 10/25 | Loss: 0.00168720
Iteration 11/25 | Loss: 0.00168720
Iteration 12/25 | Loss: 0.00168720
Iteration 13/25 | Loss: 0.00168720
Iteration 14/25 | Loss: 0.00168720
Iteration 15/25 | Loss: 0.00168720
Iteration 16/25 | Loss: 0.00168720
Iteration 17/25 | Loss: 0.00168720
Iteration 18/25 | Loss: 0.00168720
Iteration 19/25 | Loss: 0.00168720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016871967818588018, 0.0016871967818588018, 0.0016871967818588018, 0.0016871967818588018, 0.0016871967818588018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016871967818588018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168720
Iteration 2/1000 | Loss: 0.00003870
Iteration 3/1000 | Loss: 0.00002537
Iteration 4/1000 | Loss: 0.00002275
Iteration 5/1000 | Loss: 0.00002146
Iteration 6/1000 | Loss: 0.00002059
Iteration 7/1000 | Loss: 0.00002001
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001911
Iteration 10/1000 | Loss: 0.00001882
Iteration 11/1000 | Loss: 0.00001858
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001825
Iteration 14/1000 | Loss: 0.00001819
Iteration 15/1000 | Loss: 0.00001818
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001814
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001810
Iteration 20/1000 | Loss: 0.00001809
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001800
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001789
Iteration 27/1000 | Loss: 0.00001788
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001787
Iteration 30/1000 | Loss: 0.00001787
Iteration 31/1000 | Loss: 0.00001787
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001786
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001777
Iteration 85/1000 | Loss: 0.00001777
Iteration 86/1000 | Loss: 0.00001777
Iteration 87/1000 | Loss: 0.00001777
Iteration 88/1000 | Loss: 0.00001776
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00001775
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001775
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001774
Iteration 101/1000 | Loss: 0.00001774
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001773
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001773
Iteration 106/1000 | Loss: 0.00001773
Iteration 107/1000 | Loss: 0.00001773
Iteration 108/1000 | Loss: 0.00001773
Iteration 109/1000 | Loss: 0.00001773
Iteration 110/1000 | Loss: 0.00001773
Iteration 111/1000 | Loss: 0.00001772
Iteration 112/1000 | Loss: 0.00001772
Iteration 113/1000 | Loss: 0.00001772
Iteration 114/1000 | Loss: 0.00001772
Iteration 115/1000 | Loss: 0.00001772
Iteration 116/1000 | Loss: 0.00001772
Iteration 117/1000 | Loss: 0.00001772
Iteration 118/1000 | Loss: 0.00001772
Iteration 119/1000 | Loss: 0.00001772
Iteration 120/1000 | Loss: 0.00001772
Iteration 121/1000 | Loss: 0.00001772
Iteration 122/1000 | Loss: 0.00001772
Iteration 123/1000 | Loss: 0.00001772
Iteration 124/1000 | Loss: 0.00001772
Iteration 125/1000 | Loss: 0.00001772
Iteration 126/1000 | Loss: 0.00001772
Iteration 127/1000 | Loss: 0.00001772
Iteration 128/1000 | Loss: 0.00001772
Iteration 129/1000 | Loss: 0.00001772
Iteration 130/1000 | Loss: 0.00001772
Iteration 131/1000 | Loss: 0.00001772
Iteration 132/1000 | Loss: 0.00001772
Iteration 133/1000 | Loss: 0.00001772
Iteration 134/1000 | Loss: 0.00001772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.7716101865516976e-05, 1.7716101865516976e-05, 1.7716101865516976e-05, 1.7716101865516976e-05, 1.7716101865516976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7716101865516976e-05

Optimization complete. Final v2v error: 3.629399538040161 mm

Highest mean error: 4.0355939865112305 mm for frame 75

Lowest mean error: 3.1927900314331055 mm for frame 178

Saving results

Total time: 42.977699756622314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980570
Iteration 2/25 | Loss: 0.00251707
Iteration 3/25 | Loss: 0.00197989
Iteration 4/25 | Loss: 0.00176252
Iteration 5/25 | Loss: 0.00152923
Iteration 6/25 | Loss: 0.00185934
Iteration 7/25 | Loss: 0.00176082
Iteration 8/25 | Loss: 0.00145541
Iteration 9/25 | Loss: 0.00120706
Iteration 10/25 | Loss: 0.00111158
Iteration 11/25 | Loss: 0.00107207
Iteration 12/25 | Loss: 0.00104842
Iteration 13/25 | Loss: 0.00100697
Iteration 14/25 | Loss: 0.00098660
Iteration 15/25 | Loss: 0.00097896
Iteration 16/25 | Loss: 0.00096807
Iteration 17/25 | Loss: 0.00095394
Iteration 18/25 | Loss: 0.00094124
Iteration 19/25 | Loss: 0.00094153
Iteration 20/25 | Loss: 0.00093464
Iteration 21/25 | Loss: 0.00093410
Iteration 22/25 | Loss: 0.00092901
Iteration 23/25 | Loss: 0.00093270
Iteration 24/25 | Loss: 0.00093161
Iteration 25/25 | Loss: 0.00092659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15128946
Iteration 2/25 | Loss: 0.00277886
Iteration 3/25 | Loss: 0.00276862
Iteration 4/25 | Loss: 0.00276862
Iteration 5/25 | Loss: 0.00276862
Iteration 6/25 | Loss: 0.00276862
Iteration 7/25 | Loss: 0.00276862
Iteration 8/25 | Loss: 0.00276862
Iteration 9/25 | Loss: 0.00276862
Iteration 10/25 | Loss: 0.00276862
Iteration 11/25 | Loss: 0.00276862
Iteration 12/25 | Loss: 0.00276862
Iteration 13/25 | Loss: 0.00276862
Iteration 14/25 | Loss: 0.00276862
Iteration 15/25 | Loss: 0.00276862
Iteration 16/25 | Loss: 0.00276862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002768619917333126, 0.002768619917333126, 0.002768619917333126, 0.002768619917333126, 0.002768619917333126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002768619917333126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276862
Iteration 2/1000 | Loss: 0.00027057
Iteration 3/1000 | Loss: 0.00013365
Iteration 4/1000 | Loss: 0.00011073
Iteration 5/1000 | Loss: 0.00042857
Iteration 6/1000 | Loss: 0.00010302
Iteration 7/1000 | Loss: 0.00008130
Iteration 8/1000 | Loss: 0.00007387
Iteration 9/1000 | Loss: 0.00007016
Iteration 10/1000 | Loss: 0.00006801
Iteration 11/1000 | Loss: 0.00006644
Iteration 12/1000 | Loss: 0.00083683
Iteration 13/1000 | Loss: 0.00226937
Iteration 14/1000 | Loss: 0.00009272
Iteration 15/1000 | Loss: 0.00006725
Iteration 16/1000 | Loss: 0.00005511
Iteration 17/1000 | Loss: 0.00004206
Iteration 18/1000 | Loss: 0.00003265
Iteration 19/1000 | Loss: 0.00002924
Iteration 20/1000 | Loss: 0.00002706
Iteration 21/1000 | Loss: 0.00002527
Iteration 22/1000 | Loss: 0.00002424
Iteration 23/1000 | Loss: 0.00002370
Iteration 24/1000 | Loss: 0.00002326
Iteration 25/1000 | Loss: 0.00002305
Iteration 26/1000 | Loss: 0.00002288
Iteration 27/1000 | Loss: 0.00002277
Iteration 28/1000 | Loss: 0.00002270
Iteration 29/1000 | Loss: 0.00002269
Iteration 30/1000 | Loss: 0.00002264
Iteration 31/1000 | Loss: 0.00002261
Iteration 32/1000 | Loss: 0.00002261
Iteration 33/1000 | Loss: 0.00002260
Iteration 34/1000 | Loss: 0.00002257
Iteration 35/1000 | Loss: 0.00002257
Iteration 36/1000 | Loss: 0.00002257
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002256
Iteration 42/1000 | Loss: 0.00002256
Iteration 43/1000 | Loss: 0.00002256
Iteration 44/1000 | Loss: 0.00002256
Iteration 45/1000 | Loss: 0.00002256
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00002256
Iteration 48/1000 | Loss: 0.00002256
Iteration 49/1000 | Loss: 0.00002256
Iteration 50/1000 | Loss: 0.00002256
Iteration 51/1000 | Loss: 0.00002254
Iteration 52/1000 | Loss: 0.00002253
Iteration 53/1000 | Loss: 0.00002252
Iteration 54/1000 | Loss: 0.00002246
Iteration 55/1000 | Loss: 0.00002246
Iteration 56/1000 | Loss: 0.00002243
Iteration 57/1000 | Loss: 0.00002243
Iteration 58/1000 | Loss: 0.00002243
Iteration 59/1000 | Loss: 0.00002242
Iteration 60/1000 | Loss: 0.00002242
Iteration 61/1000 | Loss: 0.00002239
Iteration 62/1000 | Loss: 0.00002239
Iteration 63/1000 | Loss: 0.00002239
Iteration 64/1000 | Loss: 0.00002239
Iteration 65/1000 | Loss: 0.00002239
Iteration 66/1000 | Loss: 0.00002239
Iteration 67/1000 | Loss: 0.00002239
Iteration 68/1000 | Loss: 0.00002239
Iteration 69/1000 | Loss: 0.00002239
Iteration 70/1000 | Loss: 0.00002239
Iteration 71/1000 | Loss: 0.00002239
Iteration 72/1000 | Loss: 0.00002238
Iteration 73/1000 | Loss: 0.00002237
Iteration 74/1000 | Loss: 0.00002237
Iteration 75/1000 | Loss: 0.00002237
Iteration 76/1000 | Loss: 0.00002237
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002236
Iteration 80/1000 | Loss: 0.00002236
Iteration 81/1000 | Loss: 0.00002236
Iteration 82/1000 | Loss: 0.00002236
Iteration 83/1000 | Loss: 0.00002236
Iteration 84/1000 | Loss: 0.00002236
Iteration 85/1000 | Loss: 0.00002236
Iteration 86/1000 | Loss: 0.00002236
Iteration 87/1000 | Loss: 0.00002236
Iteration 88/1000 | Loss: 0.00002236
Iteration 89/1000 | Loss: 0.00002235
Iteration 90/1000 | Loss: 0.00002235
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002234
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002234
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002232
Iteration 101/1000 | Loss: 0.00002232
Iteration 102/1000 | Loss: 0.00002232
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002232
Iteration 107/1000 | Loss: 0.00002232
Iteration 108/1000 | Loss: 0.00002232
Iteration 109/1000 | Loss: 0.00002231
Iteration 110/1000 | Loss: 0.00002231
Iteration 111/1000 | Loss: 0.00002231
Iteration 112/1000 | Loss: 0.00002231
Iteration 113/1000 | Loss: 0.00002230
Iteration 114/1000 | Loss: 0.00002230
Iteration 115/1000 | Loss: 0.00002230
Iteration 116/1000 | Loss: 0.00002230
Iteration 117/1000 | Loss: 0.00002230
Iteration 118/1000 | Loss: 0.00002230
Iteration 119/1000 | Loss: 0.00002230
Iteration 120/1000 | Loss: 0.00002230
Iteration 121/1000 | Loss: 0.00002230
Iteration 122/1000 | Loss: 0.00002230
Iteration 123/1000 | Loss: 0.00002230
Iteration 124/1000 | Loss: 0.00002230
Iteration 125/1000 | Loss: 0.00002229
Iteration 126/1000 | Loss: 0.00002229
Iteration 127/1000 | Loss: 0.00002229
Iteration 128/1000 | Loss: 0.00002229
Iteration 129/1000 | Loss: 0.00002229
Iteration 130/1000 | Loss: 0.00002229
Iteration 131/1000 | Loss: 0.00002228
Iteration 132/1000 | Loss: 0.00002228
Iteration 133/1000 | Loss: 0.00002228
Iteration 134/1000 | Loss: 0.00002228
Iteration 135/1000 | Loss: 0.00002227
Iteration 136/1000 | Loss: 0.00002227
Iteration 137/1000 | Loss: 0.00002227
Iteration 138/1000 | Loss: 0.00002227
Iteration 139/1000 | Loss: 0.00002227
Iteration 140/1000 | Loss: 0.00002226
Iteration 141/1000 | Loss: 0.00002226
Iteration 142/1000 | Loss: 0.00002226
Iteration 143/1000 | Loss: 0.00002226
Iteration 144/1000 | Loss: 0.00002226
Iteration 145/1000 | Loss: 0.00002226
Iteration 146/1000 | Loss: 0.00002226
Iteration 147/1000 | Loss: 0.00002226
Iteration 148/1000 | Loss: 0.00002226
Iteration 149/1000 | Loss: 0.00002226
Iteration 150/1000 | Loss: 0.00002225
Iteration 151/1000 | Loss: 0.00002225
Iteration 152/1000 | Loss: 0.00002225
Iteration 153/1000 | Loss: 0.00002225
Iteration 154/1000 | Loss: 0.00002225
Iteration 155/1000 | Loss: 0.00002225
Iteration 156/1000 | Loss: 0.00002225
Iteration 157/1000 | Loss: 0.00002225
Iteration 158/1000 | Loss: 0.00002225
Iteration 159/1000 | Loss: 0.00002225
Iteration 160/1000 | Loss: 0.00002225
Iteration 161/1000 | Loss: 0.00002225
Iteration 162/1000 | Loss: 0.00002225
Iteration 163/1000 | Loss: 0.00002225
Iteration 164/1000 | Loss: 0.00002225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.2246773369261064e-05, 2.2246773369261064e-05, 2.2246773369261064e-05, 2.2246773369261064e-05, 2.2246773369261064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2246773369261064e-05

Optimization complete. Final v2v error: 4.059645652770996 mm

Highest mean error: 5.734908103942871 mm for frame 115

Lowest mean error: 3.7426962852478027 mm for frame 0

Saving results

Total time: 90.95931267738342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044937
Iteration 2/25 | Loss: 0.00257698
Iteration 3/25 | Loss: 0.00174577
Iteration 4/25 | Loss: 0.00155263
Iteration 5/25 | Loss: 0.00149800
Iteration 6/25 | Loss: 0.00138706
Iteration 7/25 | Loss: 0.00125940
Iteration 8/25 | Loss: 0.00115341
Iteration 9/25 | Loss: 0.00113732
Iteration 10/25 | Loss: 0.00111953
Iteration 11/25 | Loss: 0.00110456
Iteration 12/25 | Loss: 0.00108384
Iteration 13/25 | Loss: 0.00107821
Iteration 14/25 | Loss: 0.00107325
Iteration 15/25 | Loss: 0.00107358
Iteration 16/25 | Loss: 0.00106799
Iteration 17/25 | Loss: 0.00106671
Iteration 18/25 | Loss: 0.00106625
Iteration 19/25 | Loss: 0.00106605
Iteration 20/25 | Loss: 0.00106589
Iteration 21/25 | Loss: 0.00106577
Iteration 22/25 | Loss: 0.00106562
Iteration 23/25 | Loss: 0.00106552
Iteration 24/25 | Loss: 0.00106544
Iteration 25/25 | Loss: 0.00106543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11719489
Iteration 2/25 | Loss: 0.00334080
Iteration 3/25 | Loss: 0.00315543
Iteration 4/25 | Loss: 0.00309371
Iteration 5/25 | Loss: 0.00309371
Iteration 6/25 | Loss: 0.00309371
Iteration 7/25 | Loss: 0.00309371
Iteration 8/25 | Loss: 0.00309370
Iteration 9/25 | Loss: 0.00309370
Iteration 10/25 | Loss: 0.00309370
Iteration 11/25 | Loss: 0.00309370
Iteration 12/25 | Loss: 0.00309370
Iteration 13/25 | Loss: 0.00309370
Iteration 14/25 | Loss: 0.00309370
Iteration 15/25 | Loss: 0.00309370
Iteration 16/25 | Loss: 0.00309370
Iteration 17/25 | Loss: 0.00309370
Iteration 18/25 | Loss: 0.00309370
Iteration 19/25 | Loss: 0.00309370
Iteration 20/25 | Loss: 0.00309370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0030937015544623137, 0.0030937015544623137, 0.0030937015544623137, 0.0030937015544623137, 0.0030937015544623137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030937015544623137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309370
Iteration 2/1000 | Loss: 0.00117327
Iteration 3/1000 | Loss: 0.00031719
Iteration 4/1000 | Loss: 0.00021175
Iteration 5/1000 | Loss: 0.00025896
Iteration 6/1000 | Loss: 0.00025683
Iteration 7/1000 | Loss: 0.00013962
Iteration 8/1000 | Loss: 0.00030236
Iteration 9/1000 | Loss: 0.00011732
Iteration 10/1000 | Loss: 0.00031515
Iteration 11/1000 | Loss: 0.00032987
Iteration 12/1000 | Loss: 0.00040169
Iteration 13/1000 | Loss: 0.00026218
Iteration 14/1000 | Loss: 0.00052796
Iteration 15/1000 | Loss: 0.00075275
Iteration 16/1000 | Loss: 0.00108917
Iteration 17/1000 | Loss: 0.00246845
Iteration 18/1000 | Loss: 0.00037520
Iteration 19/1000 | Loss: 0.00024380
Iteration 20/1000 | Loss: 0.00009070
Iteration 21/1000 | Loss: 0.00031685
Iteration 22/1000 | Loss: 0.00005764
Iteration 23/1000 | Loss: 0.00004342
Iteration 24/1000 | Loss: 0.00028045
Iteration 25/1000 | Loss: 0.00003491
Iteration 26/1000 | Loss: 0.00003126
Iteration 27/1000 | Loss: 0.00002891
Iteration 28/1000 | Loss: 0.00015510
Iteration 29/1000 | Loss: 0.00018339
Iteration 30/1000 | Loss: 0.00028888
Iteration 31/1000 | Loss: 0.00036163
Iteration 32/1000 | Loss: 0.00030231
Iteration 33/1000 | Loss: 0.00013822
Iteration 34/1000 | Loss: 0.00023720
Iteration 35/1000 | Loss: 0.00004192
Iteration 36/1000 | Loss: 0.00003262
Iteration 37/1000 | Loss: 0.00014939
Iteration 38/1000 | Loss: 0.00002557
Iteration 39/1000 | Loss: 0.00002509
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002390
Iteration 42/1000 | Loss: 0.00002358
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002315
Iteration 45/1000 | Loss: 0.00002312
Iteration 46/1000 | Loss: 0.00002299
Iteration 47/1000 | Loss: 0.00002296
Iteration 48/1000 | Loss: 0.00002295
Iteration 49/1000 | Loss: 0.00002288
Iteration 50/1000 | Loss: 0.00002287
Iteration 51/1000 | Loss: 0.00002287
Iteration 52/1000 | Loss: 0.00002286
Iteration 53/1000 | Loss: 0.00002286
Iteration 54/1000 | Loss: 0.00002285
Iteration 55/1000 | Loss: 0.00002284
Iteration 56/1000 | Loss: 0.00002283
Iteration 57/1000 | Loss: 0.00002282
Iteration 58/1000 | Loss: 0.00002282
Iteration 59/1000 | Loss: 0.00002282
Iteration 60/1000 | Loss: 0.00002281
Iteration 61/1000 | Loss: 0.00002281
Iteration 62/1000 | Loss: 0.00002281
Iteration 63/1000 | Loss: 0.00002280
Iteration 64/1000 | Loss: 0.00002280
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002279
Iteration 67/1000 | Loss: 0.00002279
Iteration 68/1000 | Loss: 0.00002279
Iteration 69/1000 | Loss: 0.00002279
Iteration 70/1000 | Loss: 0.00002279
Iteration 71/1000 | Loss: 0.00002279
Iteration 72/1000 | Loss: 0.00002278
Iteration 73/1000 | Loss: 0.00002278
Iteration 74/1000 | Loss: 0.00002278
Iteration 75/1000 | Loss: 0.00002278
Iteration 76/1000 | Loss: 0.00002278
Iteration 77/1000 | Loss: 0.00002278
Iteration 78/1000 | Loss: 0.00002278
Iteration 79/1000 | Loss: 0.00002278
Iteration 80/1000 | Loss: 0.00002278
Iteration 81/1000 | Loss: 0.00002278
Iteration 82/1000 | Loss: 0.00002278
Iteration 83/1000 | Loss: 0.00002278
Iteration 84/1000 | Loss: 0.00002278
Iteration 85/1000 | Loss: 0.00002277
Iteration 86/1000 | Loss: 0.00002277
Iteration 87/1000 | Loss: 0.00002277
Iteration 88/1000 | Loss: 0.00002277
Iteration 89/1000 | Loss: 0.00002277
Iteration 90/1000 | Loss: 0.00002277
Iteration 91/1000 | Loss: 0.00002277
Iteration 92/1000 | Loss: 0.00002277
Iteration 93/1000 | Loss: 0.00002277
Iteration 94/1000 | Loss: 0.00002277
Iteration 95/1000 | Loss: 0.00002276
Iteration 96/1000 | Loss: 0.00002276
Iteration 97/1000 | Loss: 0.00002276
Iteration 98/1000 | Loss: 0.00002276
Iteration 99/1000 | Loss: 0.00002276
Iteration 100/1000 | Loss: 0.00002276
Iteration 101/1000 | Loss: 0.00002276
Iteration 102/1000 | Loss: 0.00002276
Iteration 103/1000 | Loss: 0.00002276
Iteration 104/1000 | Loss: 0.00002276
Iteration 105/1000 | Loss: 0.00002276
Iteration 106/1000 | Loss: 0.00002276
Iteration 107/1000 | Loss: 0.00002276
Iteration 108/1000 | Loss: 0.00002276
Iteration 109/1000 | Loss: 0.00002276
Iteration 110/1000 | Loss: 0.00002275
Iteration 111/1000 | Loss: 0.00002275
Iteration 112/1000 | Loss: 0.00002275
Iteration 113/1000 | Loss: 0.00002275
Iteration 114/1000 | Loss: 0.00002275
Iteration 115/1000 | Loss: 0.00002275
Iteration 116/1000 | Loss: 0.00002275
Iteration 117/1000 | Loss: 0.00002275
Iteration 118/1000 | Loss: 0.00002275
Iteration 119/1000 | Loss: 0.00002275
Iteration 120/1000 | Loss: 0.00002275
Iteration 121/1000 | Loss: 0.00002275
Iteration 122/1000 | Loss: 0.00002275
Iteration 123/1000 | Loss: 0.00002275
Iteration 124/1000 | Loss: 0.00002274
Iteration 125/1000 | Loss: 0.00002274
Iteration 126/1000 | Loss: 0.00002274
Iteration 127/1000 | Loss: 0.00002274
Iteration 128/1000 | Loss: 0.00002274
Iteration 129/1000 | Loss: 0.00002274
Iteration 130/1000 | Loss: 0.00002274
Iteration 131/1000 | Loss: 0.00002274
Iteration 132/1000 | Loss: 0.00002274
Iteration 133/1000 | Loss: 0.00002274
Iteration 134/1000 | Loss: 0.00002274
Iteration 135/1000 | Loss: 0.00002274
Iteration 136/1000 | Loss: 0.00002274
Iteration 137/1000 | Loss: 0.00002274
Iteration 138/1000 | Loss: 0.00002273
Iteration 139/1000 | Loss: 0.00002273
Iteration 140/1000 | Loss: 0.00002273
Iteration 141/1000 | Loss: 0.00002273
Iteration 142/1000 | Loss: 0.00002273
Iteration 143/1000 | Loss: 0.00002273
Iteration 144/1000 | Loss: 0.00002273
Iteration 145/1000 | Loss: 0.00002273
Iteration 146/1000 | Loss: 0.00002273
Iteration 147/1000 | Loss: 0.00002273
Iteration 148/1000 | Loss: 0.00002273
Iteration 149/1000 | Loss: 0.00002273
Iteration 150/1000 | Loss: 0.00002273
Iteration 151/1000 | Loss: 0.00002273
Iteration 152/1000 | Loss: 0.00002273
Iteration 153/1000 | Loss: 0.00002272
Iteration 154/1000 | Loss: 0.00002272
Iteration 155/1000 | Loss: 0.00002272
Iteration 156/1000 | Loss: 0.00002272
Iteration 157/1000 | Loss: 0.00002272
Iteration 158/1000 | Loss: 0.00002272
Iteration 159/1000 | Loss: 0.00002272
Iteration 160/1000 | Loss: 0.00002271
Iteration 161/1000 | Loss: 0.00002271
Iteration 162/1000 | Loss: 0.00002271
Iteration 163/1000 | Loss: 0.00002271
Iteration 164/1000 | Loss: 0.00002271
Iteration 165/1000 | Loss: 0.00002271
Iteration 166/1000 | Loss: 0.00002271
Iteration 167/1000 | Loss: 0.00002271
Iteration 168/1000 | Loss: 0.00002271
Iteration 169/1000 | Loss: 0.00002271
Iteration 170/1000 | Loss: 0.00002271
Iteration 171/1000 | Loss: 0.00002270
Iteration 172/1000 | Loss: 0.00002270
Iteration 173/1000 | Loss: 0.00002270
Iteration 174/1000 | Loss: 0.00002270
Iteration 175/1000 | Loss: 0.00002270
Iteration 176/1000 | Loss: 0.00002270
Iteration 177/1000 | Loss: 0.00002270
Iteration 178/1000 | Loss: 0.00002270
Iteration 179/1000 | Loss: 0.00002270
Iteration 180/1000 | Loss: 0.00002270
Iteration 181/1000 | Loss: 0.00002270
Iteration 182/1000 | Loss: 0.00002270
Iteration 183/1000 | Loss: 0.00002270
Iteration 184/1000 | Loss: 0.00002270
Iteration 185/1000 | Loss: 0.00002270
Iteration 186/1000 | Loss: 0.00002270
Iteration 187/1000 | Loss: 0.00002270
Iteration 188/1000 | Loss: 0.00002270
Iteration 189/1000 | Loss: 0.00002270
Iteration 190/1000 | Loss: 0.00002270
Iteration 191/1000 | Loss: 0.00002270
Iteration 192/1000 | Loss: 0.00002270
Iteration 193/1000 | Loss: 0.00002270
Iteration 194/1000 | Loss: 0.00002270
Iteration 195/1000 | Loss: 0.00002270
Iteration 196/1000 | Loss: 0.00002270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.270115328428801e-05, 2.270115328428801e-05, 2.270115328428801e-05, 2.270115328428801e-05, 2.270115328428801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.270115328428801e-05

Optimization complete. Final v2v error: 4.111943244934082 mm

Highest mean error: 4.528198719024658 mm for frame 46

Lowest mean error: 3.7493419647216797 mm for frame 193

Saving results

Total time: 130.88909697532654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041166
Iteration 2/25 | Loss: 0.00193099
Iteration 3/25 | Loss: 0.00125537
Iteration 4/25 | Loss: 0.00113148
Iteration 5/25 | Loss: 0.00113967
Iteration 6/25 | Loss: 0.00111611
Iteration 7/25 | Loss: 0.00107436
Iteration 8/25 | Loss: 0.00103208
Iteration 9/25 | Loss: 0.00101698
Iteration 10/25 | Loss: 0.00101066
Iteration 11/25 | Loss: 0.00101071
Iteration 12/25 | Loss: 0.00102277
Iteration 13/25 | Loss: 0.00100382
Iteration 14/25 | Loss: 0.00100331
Iteration 15/25 | Loss: 0.00099805
Iteration 16/25 | Loss: 0.00099442
Iteration 17/25 | Loss: 0.00099355
Iteration 18/25 | Loss: 0.00099365
Iteration 19/25 | Loss: 0.00099329
Iteration 20/25 | Loss: 0.00099326
Iteration 21/25 | Loss: 0.00099326
Iteration 22/25 | Loss: 0.00099326
Iteration 23/25 | Loss: 0.00099326
Iteration 24/25 | Loss: 0.00099326
Iteration 25/25 | Loss: 0.00099326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12202120
Iteration 2/25 | Loss: 0.00292915
Iteration 3/25 | Loss: 0.00270210
Iteration 4/25 | Loss: 0.00270210
Iteration 5/25 | Loss: 0.00270210
Iteration 6/25 | Loss: 0.00270210
Iteration 7/25 | Loss: 0.00270210
Iteration 8/25 | Loss: 0.00270210
Iteration 9/25 | Loss: 0.00270210
Iteration 10/25 | Loss: 0.00270210
Iteration 11/25 | Loss: 0.00270210
Iteration 12/25 | Loss: 0.00270210
Iteration 13/25 | Loss: 0.00270210
Iteration 14/25 | Loss: 0.00270210
Iteration 15/25 | Loss: 0.00270210
Iteration 16/25 | Loss: 0.00270210
Iteration 17/25 | Loss: 0.00270210
Iteration 18/25 | Loss: 0.00270210
Iteration 19/25 | Loss: 0.00270210
Iteration 20/25 | Loss: 0.00270210
Iteration 21/25 | Loss: 0.00270210
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002702096477150917, 0.002702096477150917, 0.002702096477150917, 0.002702096477150917, 0.002702096477150917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002702096477150917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270210
Iteration 2/1000 | Loss: 0.00176846
Iteration 3/1000 | Loss: 0.00056104
Iteration 4/1000 | Loss: 0.00028333
Iteration 5/1000 | Loss: 0.00016439
Iteration 6/1000 | Loss: 0.00014931
Iteration 7/1000 | Loss: 0.00027125
Iteration 8/1000 | Loss: 0.00044701
Iteration 9/1000 | Loss: 0.00009100
Iteration 10/1000 | Loss: 0.00006354
Iteration 11/1000 | Loss: 0.00037560
Iteration 12/1000 | Loss: 0.00013367
Iteration 13/1000 | Loss: 0.00006683
Iteration 14/1000 | Loss: 0.00042985
Iteration 15/1000 | Loss: 0.00081616
Iteration 16/1000 | Loss: 0.00010812
Iteration 17/1000 | Loss: 0.00016681
Iteration 18/1000 | Loss: 0.00004441
Iteration 19/1000 | Loss: 0.00005371
Iteration 20/1000 | Loss: 0.00003065
Iteration 21/1000 | Loss: 0.00008810
Iteration 22/1000 | Loss: 0.00002694
Iteration 23/1000 | Loss: 0.00002796
Iteration 24/1000 | Loss: 0.00002494
Iteration 25/1000 | Loss: 0.00006108
Iteration 26/1000 | Loss: 0.00002836
Iteration 27/1000 | Loss: 0.00002949
Iteration 28/1000 | Loss: 0.00002761
Iteration 29/1000 | Loss: 0.00004803
Iteration 30/1000 | Loss: 0.00002583
Iteration 31/1000 | Loss: 0.00002297
Iteration 32/1000 | Loss: 0.00004396
Iteration 33/1000 | Loss: 0.00002261
Iteration 34/1000 | Loss: 0.00002968
Iteration 35/1000 | Loss: 0.00002249
Iteration 36/1000 | Loss: 0.00002230
Iteration 37/1000 | Loss: 0.00002230
Iteration 38/1000 | Loss: 0.00002230
Iteration 39/1000 | Loss: 0.00002229
Iteration 40/1000 | Loss: 0.00002229
Iteration 41/1000 | Loss: 0.00002229
Iteration 42/1000 | Loss: 0.00002228
Iteration 43/1000 | Loss: 0.00002228
Iteration 44/1000 | Loss: 0.00002228
Iteration 45/1000 | Loss: 0.00002228
Iteration 46/1000 | Loss: 0.00002228
Iteration 47/1000 | Loss: 0.00002228
Iteration 48/1000 | Loss: 0.00002228
Iteration 49/1000 | Loss: 0.00002227
Iteration 50/1000 | Loss: 0.00002705
Iteration 51/1000 | Loss: 0.00002224
Iteration 52/1000 | Loss: 0.00002213
Iteration 53/1000 | Loss: 0.00002212
Iteration 54/1000 | Loss: 0.00002211
Iteration 55/1000 | Loss: 0.00002210
Iteration 56/1000 | Loss: 0.00002210
Iteration 57/1000 | Loss: 0.00002210
Iteration 58/1000 | Loss: 0.00002210
Iteration 59/1000 | Loss: 0.00002210
Iteration 60/1000 | Loss: 0.00002209
Iteration 61/1000 | Loss: 0.00002209
Iteration 62/1000 | Loss: 0.00002209
Iteration 63/1000 | Loss: 0.00002209
Iteration 64/1000 | Loss: 0.00002209
Iteration 65/1000 | Loss: 0.00002209
Iteration 66/1000 | Loss: 0.00002209
Iteration 67/1000 | Loss: 0.00002209
Iteration 68/1000 | Loss: 0.00002209
Iteration 69/1000 | Loss: 0.00002209
Iteration 70/1000 | Loss: 0.00002209
Iteration 71/1000 | Loss: 0.00002209
Iteration 72/1000 | Loss: 0.00002209
Iteration 73/1000 | Loss: 0.00002209
Iteration 74/1000 | Loss: 0.00002209
Iteration 75/1000 | Loss: 0.00002209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.2088235709816217e-05, 2.2088235709816217e-05, 2.2088235709816217e-05, 2.2088235709816217e-05, 2.2088235709816217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2088235709816217e-05

Optimization complete. Final v2v error: 4.098240375518799 mm

Highest mean error: 4.424107074737549 mm for frame 78

Lowest mean error: 3.418492317199707 mm for frame 1

Saving results

Total time: 99.68728685379028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873506
Iteration 2/25 | Loss: 0.00122909
Iteration 3/25 | Loss: 0.00092727
Iteration 4/25 | Loss: 0.00088495
Iteration 5/25 | Loss: 0.00086845
Iteration 6/25 | Loss: 0.00086230
Iteration 7/25 | Loss: 0.00086058
Iteration 8/25 | Loss: 0.00085950
Iteration 9/25 | Loss: 0.00085915
Iteration 10/25 | Loss: 0.00085915
Iteration 11/25 | Loss: 0.00085915
Iteration 12/25 | Loss: 0.00085915
Iteration 13/25 | Loss: 0.00085915
Iteration 14/25 | Loss: 0.00085915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008591450750827789, 0.0008591450750827789, 0.0008591450750827789, 0.0008591450750827789, 0.0008591450750827789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008591450750827789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20372748
Iteration 2/25 | Loss: 0.00162725
Iteration 3/25 | Loss: 0.00162724
Iteration 4/25 | Loss: 0.00162724
Iteration 5/25 | Loss: 0.00162724
Iteration 6/25 | Loss: 0.00162724
Iteration 7/25 | Loss: 0.00162724
Iteration 8/25 | Loss: 0.00162724
Iteration 9/25 | Loss: 0.00162724
Iteration 10/25 | Loss: 0.00162724
Iteration 11/25 | Loss: 0.00162724
Iteration 12/25 | Loss: 0.00162724
Iteration 13/25 | Loss: 0.00162724
Iteration 14/25 | Loss: 0.00162724
Iteration 15/25 | Loss: 0.00162724
Iteration 16/25 | Loss: 0.00162724
Iteration 17/25 | Loss: 0.00162724
Iteration 18/25 | Loss: 0.00162724
Iteration 19/25 | Loss: 0.00162724
Iteration 20/25 | Loss: 0.00162724
Iteration 21/25 | Loss: 0.00162724
Iteration 22/25 | Loss: 0.00162724
Iteration 23/25 | Loss: 0.00162724
Iteration 24/25 | Loss: 0.00162724
Iteration 25/25 | Loss: 0.00162724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162724
Iteration 2/1000 | Loss: 0.00006549
Iteration 3/1000 | Loss: 0.00003757
Iteration 4/1000 | Loss: 0.00003064
Iteration 5/1000 | Loss: 0.00002901
Iteration 6/1000 | Loss: 0.00002813
Iteration 7/1000 | Loss: 0.00002726
Iteration 8/1000 | Loss: 0.00002673
Iteration 9/1000 | Loss: 0.00002629
Iteration 10/1000 | Loss: 0.00002589
Iteration 11/1000 | Loss: 0.00002561
Iteration 12/1000 | Loss: 0.00002539
Iteration 13/1000 | Loss: 0.00002519
Iteration 14/1000 | Loss: 0.00002493
Iteration 15/1000 | Loss: 0.00002474
Iteration 16/1000 | Loss: 0.00002473
Iteration 17/1000 | Loss: 0.00002472
Iteration 18/1000 | Loss: 0.00002466
Iteration 19/1000 | Loss: 0.00002465
Iteration 20/1000 | Loss: 0.00002464
Iteration 21/1000 | Loss: 0.00002461
Iteration 22/1000 | Loss: 0.00002457
Iteration 23/1000 | Loss: 0.00002457
Iteration 24/1000 | Loss: 0.00002457
Iteration 25/1000 | Loss: 0.00002456
Iteration 26/1000 | Loss: 0.00002455
Iteration 27/1000 | Loss: 0.00002455
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002454
Iteration 30/1000 | Loss: 0.00002453
Iteration 31/1000 | Loss: 0.00002452
Iteration 32/1000 | Loss: 0.00002451
Iteration 33/1000 | Loss: 0.00002450
Iteration 34/1000 | Loss: 0.00002449
Iteration 35/1000 | Loss: 0.00002449
Iteration 36/1000 | Loss: 0.00002449
Iteration 37/1000 | Loss: 0.00002448
Iteration 38/1000 | Loss: 0.00002448
Iteration 39/1000 | Loss: 0.00002447
Iteration 40/1000 | Loss: 0.00002447
Iteration 41/1000 | Loss: 0.00002447
Iteration 42/1000 | Loss: 0.00002446
Iteration 43/1000 | Loss: 0.00002446
Iteration 44/1000 | Loss: 0.00002445
Iteration 45/1000 | Loss: 0.00002445
Iteration 46/1000 | Loss: 0.00002445
Iteration 47/1000 | Loss: 0.00002445
Iteration 48/1000 | Loss: 0.00002445
Iteration 49/1000 | Loss: 0.00002444
Iteration 50/1000 | Loss: 0.00002444
Iteration 51/1000 | Loss: 0.00002444
Iteration 52/1000 | Loss: 0.00002444
Iteration 53/1000 | Loss: 0.00002443
Iteration 54/1000 | Loss: 0.00002443
Iteration 55/1000 | Loss: 0.00002443
Iteration 56/1000 | Loss: 0.00002443
Iteration 57/1000 | Loss: 0.00002443
Iteration 58/1000 | Loss: 0.00002442
Iteration 59/1000 | Loss: 0.00002442
Iteration 60/1000 | Loss: 0.00002442
Iteration 61/1000 | Loss: 0.00002442
Iteration 62/1000 | Loss: 0.00002442
Iteration 63/1000 | Loss: 0.00002441
Iteration 64/1000 | Loss: 0.00002441
Iteration 65/1000 | Loss: 0.00002441
Iteration 66/1000 | Loss: 0.00002441
Iteration 67/1000 | Loss: 0.00002441
Iteration 68/1000 | Loss: 0.00002441
Iteration 69/1000 | Loss: 0.00002441
Iteration 70/1000 | Loss: 0.00002440
Iteration 71/1000 | Loss: 0.00002440
Iteration 72/1000 | Loss: 0.00002440
Iteration 73/1000 | Loss: 0.00002440
Iteration 74/1000 | Loss: 0.00002439
Iteration 75/1000 | Loss: 0.00002439
Iteration 76/1000 | Loss: 0.00002439
Iteration 77/1000 | Loss: 0.00002439
Iteration 78/1000 | Loss: 0.00002438
Iteration 79/1000 | Loss: 0.00002438
Iteration 80/1000 | Loss: 0.00002438
Iteration 81/1000 | Loss: 0.00002437
Iteration 82/1000 | Loss: 0.00002437
Iteration 83/1000 | Loss: 0.00002437
Iteration 84/1000 | Loss: 0.00002437
Iteration 85/1000 | Loss: 0.00002436
Iteration 86/1000 | Loss: 0.00002436
Iteration 87/1000 | Loss: 0.00002436
Iteration 88/1000 | Loss: 0.00002436
Iteration 89/1000 | Loss: 0.00002436
Iteration 90/1000 | Loss: 0.00002436
Iteration 91/1000 | Loss: 0.00002435
Iteration 92/1000 | Loss: 0.00002435
Iteration 93/1000 | Loss: 0.00002435
Iteration 94/1000 | Loss: 0.00002435
Iteration 95/1000 | Loss: 0.00002434
Iteration 96/1000 | Loss: 0.00002434
Iteration 97/1000 | Loss: 0.00002434
Iteration 98/1000 | Loss: 0.00002434
Iteration 99/1000 | Loss: 0.00002434
Iteration 100/1000 | Loss: 0.00002434
Iteration 101/1000 | Loss: 0.00002434
Iteration 102/1000 | Loss: 0.00002434
Iteration 103/1000 | Loss: 0.00002434
Iteration 104/1000 | Loss: 0.00002434
Iteration 105/1000 | Loss: 0.00002433
Iteration 106/1000 | Loss: 0.00002433
Iteration 107/1000 | Loss: 0.00002433
Iteration 108/1000 | Loss: 0.00002433
Iteration 109/1000 | Loss: 0.00002433
Iteration 110/1000 | Loss: 0.00002433
Iteration 111/1000 | Loss: 0.00002432
Iteration 112/1000 | Loss: 0.00002432
Iteration 113/1000 | Loss: 0.00002432
Iteration 114/1000 | Loss: 0.00002432
Iteration 115/1000 | Loss: 0.00002432
Iteration 116/1000 | Loss: 0.00002432
Iteration 117/1000 | Loss: 0.00002432
Iteration 118/1000 | Loss: 0.00002432
Iteration 119/1000 | Loss: 0.00002431
Iteration 120/1000 | Loss: 0.00002431
Iteration 121/1000 | Loss: 0.00002431
Iteration 122/1000 | Loss: 0.00002431
Iteration 123/1000 | Loss: 0.00002431
Iteration 124/1000 | Loss: 0.00002431
Iteration 125/1000 | Loss: 0.00002430
Iteration 126/1000 | Loss: 0.00002430
Iteration 127/1000 | Loss: 0.00002430
Iteration 128/1000 | Loss: 0.00002430
Iteration 129/1000 | Loss: 0.00002430
Iteration 130/1000 | Loss: 0.00002430
Iteration 131/1000 | Loss: 0.00002430
Iteration 132/1000 | Loss: 0.00002430
Iteration 133/1000 | Loss: 0.00002430
Iteration 134/1000 | Loss: 0.00002430
Iteration 135/1000 | Loss: 0.00002429
Iteration 136/1000 | Loss: 0.00002429
Iteration 137/1000 | Loss: 0.00002429
Iteration 138/1000 | Loss: 0.00002429
Iteration 139/1000 | Loss: 0.00002429
Iteration 140/1000 | Loss: 0.00002429
Iteration 141/1000 | Loss: 0.00002429
Iteration 142/1000 | Loss: 0.00002429
Iteration 143/1000 | Loss: 0.00002428
Iteration 144/1000 | Loss: 0.00002428
Iteration 145/1000 | Loss: 0.00002428
Iteration 146/1000 | Loss: 0.00002428
Iteration 147/1000 | Loss: 0.00002428
Iteration 148/1000 | Loss: 0.00002428
Iteration 149/1000 | Loss: 0.00002428
Iteration 150/1000 | Loss: 0.00002427
Iteration 151/1000 | Loss: 0.00002427
Iteration 152/1000 | Loss: 0.00002427
Iteration 153/1000 | Loss: 0.00002427
Iteration 154/1000 | Loss: 0.00002427
Iteration 155/1000 | Loss: 0.00002427
Iteration 156/1000 | Loss: 0.00002427
Iteration 157/1000 | Loss: 0.00002427
Iteration 158/1000 | Loss: 0.00002427
Iteration 159/1000 | Loss: 0.00002427
Iteration 160/1000 | Loss: 0.00002427
Iteration 161/1000 | Loss: 0.00002426
Iteration 162/1000 | Loss: 0.00002426
Iteration 163/1000 | Loss: 0.00002426
Iteration 164/1000 | Loss: 0.00002426
Iteration 165/1000 | Loss: 0.00002426
Iteration 166/1000 | Loss: 0.00002426
Iteration 167/1000 | Loss: 0.00002426
Iteration 168/1000 | Loss: 0.00002426
Iteration 169/1000 | Loss: 0.00002426
Iteration 170/1000 | Loss: 0.00002426
Iteration 171/1000 | Loss: 0.00002426
Iteration 172/1000 | Loss: 0.00002425
Iteration 173/1000 | Loss: 0.00002425
Iteration 174/1000 | Loss: 0.00002425
Iteration 175/1000 | Loss: 0.00002425
Iteration 176/1000 | Loss: 0.00002425
Iteration 177/1000 | Loss: 0.00002425
Iteration 178/1000 | Loss: 0.00002425
Iteration 179/1000 | Loss: 0.00002425
Iteration 180/1000 | Loss: 0.00002425
Iteration 181/1000 | Loss: 0.00002425
Iteration 182/1000 | Loss: 0.00002425
Iteration 183/1000 | Loss: 0.00002425
Iteration 184/1000 | Loss: 0.00002425
Iteration 185/1000 | Loss: 0.00002425
Iteration 186/1000 | Loss: 0.00002425
Iteration 187/1000 | Loss: 0.00002425
Iteration 188/1000 | Loss: 0.00002424
Iteration 189/1000 | Loss: 0.00002424
Iteration 190/1000 | Loss: 0.00002424
Iteration 191/1000 | Loss: 0.00002424
Iteration 192/1000 | Loss: 0.00002424
Iteration 193/1000 | Loss: 0.00002424
Iteration 194/1000 | Loss: 0.00002424
Iteration 195/1000 | Loss: 0.00002424
Iteration 196/1000 | Loss: 0.00002424
Iteration 197/1000 | Loss: 0.00002424
Iteration 198/1000 | Loss: 0.00002424
Iteration 199/1000 | Loss: 0.00002424
Iteration 200/1000 | Loss: 0.00002424
Iteration 201/1000 | Loss: 0.00002423
Iteration 202/1000 | Loss: 0.00002423
Iteration 203/1000 | Loss: 0.00002423
Iteration 204/1000 | Loss: 0.00002423
Iteration 205/1000 | Loss: 0.00002423
Iteration 206/1000 | Loss: 0.00002423
Iteration 207/1000 | Loss: 0.00002423
Iteration 208/1000 | Loss: 0.00002423
Iteration 209/1000 | Loss: 0.00002423
Iteration 210/1000 | Loss: 0.00002423
Iteration 211/1000 | Loss: 0.00002423
Iteration 212/1000 | Loss: 0.00002423
Iteration 213/1000 | Loss: 0.00002423
Iteration 214/1000 | Loss: 0.00002423
Iteration 215/1000 | Loss: 0.00002423
Iteration 216/1000 | Loss: 0.00002423
Iteration 217/1000 | Loss: 0.00002423
Iteration 218/1000 | Loss: 0.00002423
Iteration 219/1000 | Loss: 0.00002423
Iteration 220/1000 | Loss: 0.00002423
Iteration 221/1000 | Loss: 0.00002423
Iteration 222/1000 | Loss: 0.00002423
Iteration 223/1000 | Loss: 0.00002423
Iteration 224/1000 | Loss: 0.00002423
Iteration 225/1000 | Loss: 0.00002423
Iteration 226/1000 | Loss: 0.00002423
Iteration 227/1000 | Loss: 0.00002423
Iteration 228/1000 | Loss: 0.00002423
Iteration 229/1000 | Loss: 0.00002423
Iteration 230/1000 | Loss: 0.00002423
Iteration 231/1000 | Loss: 0.00002423
Iteration 232/1000 | Loss: 0.00002423
Iteration 233/1000 | Loss: 0.00002423
Iteration 234/1000 | Loss: 0.00002423
Iteration 235/1000 | Loss: 0.00002423
Iteration 236/1000 | Loss: 0.00002423
Iteration 237/1000 | Loss: 0.00002423
Iteration 238/1000 | Loss: 0.00002423
Iteration 239/1000 | Loss: 0.00002423
Iteration 240/1000 | Loss: 0.00002423
Iteration 241/1000 | Loss: 0.00002423
Iteration 242/1000 | Loss: 0.00002423
Iteration 243/1000 | Loss: 0.00002423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.4230164854088798e-05, 2.4230164854088798e-05, 2.4230164854088798e-05, 2.4230164854088798e-05, 2.4230164854088798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4230164854088798e-05

Optimization complete. Final v2v error: 4.223163604736328 mm

Highest mean error: 6.677056312561035 mm for frame 71

Lowest mean error: 3.1608753204345703 mm for frame 0

Saving results

Total time: 48.574817180633545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038941
Iteration 2/25 | Loss: 0.00147468
Iteration 3/25 | Loss: 0.00114530
Iteration 4/25 | Loss: 0.00107034
Iteration 5/25 | Loss: 0.00104309
Iteration 6/25 | Loss: 0.00103685
Iteration 7/25 | Loss: 0.00103427
Iteration 8/25 | Loss: 0.00103424
Iteration 9/25 | Loss: 0.00103424
Iteration 10/25 | Loss: 0.00103424
Iteration 11/25 | Loss: 0.00103424
Iteration 12/25 | Loss: 0.00103424
Iteration 13/25 | Loss: 0.00103424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010342412861064076, 0.0010342412861064076, 0.0010342412861064076, 0.0010342412861064076, 0.0010342412861064076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010342412861064076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96344006
Iteration 2/25 | Loss: 0.00150619
Iteration 3/25 | Loss: 0.00150598
Iteration 4/25 | Loss: 0.00150598
Iteration 5/25 | Loss: 0.00150598
Iteration 6/25 | Loss: 0.00150598
Iteration 7/25 | Loss: 0.00150598
Iteration 8/25 | Loss: 0.00150598
Iteration 9/25 | Loss: 0.00150598
Iteration 10/25 | Loss: 0.00150598
Iteration 11/25 | Loss: 0.00150598
Iteration 12/25 | Loss: 0.00150598
Iteration 13/25 | Loss: 0.00150598
Iteration 14/25 | Loss: 0.00150598
Iteration 15/25 | Loss: 0.00150598
Iteration 16/25 | Loss: 0.00150598
Iteration 17/25 | Loss: 0.00150598
Iteration 18/25 | Loss: 0.00150598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015059823635965586, 0.0015059823635965586, 0.0015059823635965586, 0.0015059823635965586, 0.0015059823635965586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015059823635965586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150598
Iteration 2/1000 | Loss: 0.00009711
Iteration 3/1000 | Loss: 0.00007412
Iteration 4/1000 | Loss: 0.00006445
Iteration 5/1000 | Loss: 0.00005892
Iteration 6/1000 | Loss: 0.00005629
Iteration 7/1000 | Loss: 0.00005418
Iteration 8/1000 | Loss: 0.00005263
Iteration 9/1000 | Loss: 0.00005154
Iteration 10/1000 | Loss: 0.00005045
Iteration 11/1000 | Loss: 0.00004978
Iteration 12/1000 | Loss: 0.00004919
Iteration 13/1000 | Loss: 0.00004878
Iteration 14/1000 | Loss: 0.00004836
Iteration 15/1000 | Loss: 0.00004803
Iteration 16/1000 | Loss: 0.00004777
Iteration 17/1000 | Loss: 0.00004754
Iteration 18/1000 | Loss: 0.00004734
Iteration 19/1000 | Loss: 0.00004718
Iteration 20/1000 | Loss: 0.00004714
Iteration 21/1000 | Loss: 0.00004713
Iteration 22/1000 | Loss: 0.00004712
Iteration 23/1000 | Loss: 0.00004708
Iteration 24/1000 | Loss: 0.00004708
Iteration 25/1000 | Loss: 0.00004701
Iteration 26/1000 | Loss: 0.00004700
Iteration 27/1000 | Loss: 0.00004699
Iteration 28/1000 | Loss: 0.00004698
Iteration 29/1000 | Loss: 0.00004697
Iteration 30/1000 | Loss: 0.00004697
Iteration 31/1000 | Loss: 0.00004696
Iteration 32/1000 | Loss: 0.00004696
Iteration 33/1000 | Loss: 0.00004696
Iteration 34/1000 | Loss: 0.00004695
Iteration 35/1000 | Loss: 0.00004695
Iteration 36/1000 | Loss: 0.00004695
Iteration 37/1000 | Loss: 0.00004694
Iteration 38/1000 | Loss: 0.00004694
Iteration 39/1000 | Loss: 0.00004694
Iteration 40/1000 | Loss: 0.00004694
Iteration 41/1000 | Loss: 0.00004694
Iteration 42/1000 | Loss: 0.00004694
Iteration 43/1000 | Loss: 0.00004693
Iteration 44/1000 | Loss: 0.00004693
Iteration 45/1000 | Loss: 0.00004691
Iteration 46/1000 | Loss: 0.00004691
Iteration 47/1000 | Loss: 0.00004691
Iteration 48/1000 | Loss: 0.00004690
Iteration 49/1000 | Loss: 0.00004690
Iteration 50/1000 | Loss: 0.00004690
Iteration 51/1000 | Loss: 0.00004689
Iteration 52/1000 | Loss: 0.00004689
Iteration 53/1000 | Loss: 0.00004689
Iteration 54/1000 | Loss: 0.00004689
Iteration 55/1000 | Loss: 0.00004689
Iteration 56/1000 | Loss: 0.00004689
Iteration 57/1000 | Loss: 0.00004689
Iteration 58/1000 | Loss: 0.00004689
Iteration 59/1000 | Loss: 0.00004689
Iteration 60/1000 | Loss: 0.00004689
Iteration 61/1000 | Loss: 0.00004689
Iteration 62/1000 | Loss: 0.00004688
Iteration 63/1000 | Loss: 0.00004688
Iteration 64/1000 | Loss: 0.00004688
Iteration 65/1000 | Loss: 0.00004688
Iteration 66/1000 | Loss: 0.00004688
Iteration 67/1000 | Loss: 0.00004688
Iteration 68/1000 | Loss: 0.00004688
Iteration 69/1000 | Loss: 0.00004688
Iteration 70/1000 | Loss: 0.00004688
Iteration 71/1000 | Loss: 0.00004687
Iteration 72/1000 | Loss: 0.00004687
Iteration 73/1000 | Loss: 0.00004687
Iteration 74/1000 | Loss: 0.00004686
Iteration 75/1000 | Loss: 0.00004686
Iteration 76/1000 | Loss: 0.00004686
Iteration 77/1000 | Loss: 0.00004686
Iteration 78/1000 | Loss: 0.00004686
Iteration 79/1000 | Loss: 0.00004686
Iteration 80/1000 | Loss: 0.00004686
Iteration 81/1000 | Loss: 0.00004685
Iteration 82/1000 | Loss: 0.00004685
Iteration 83/1000 | Loss: 0.00004685
Iteration 84/1000 | Loss: 0.00004684
Iteration 85/1000 | Loss: 0.00004684
Iteration 86/1000 | Loss: 0.00004684
Iteration 87/1000 | Loss: 0.00004683
Iteration 88/1000 | Loss: 0.00004683
Iteration 89/1000 | Loss: 0.00004683
Iteration 90/1000 | Loss: 0.00004682
Iteration 91/1000 | Loss: 0.00004682
Iteration 92/1000 | Loss: 0.00004682
Iteration 93/1000 | Loss: 0.00004682
Iteration 94/1000 | Loss: 0.00004682
Iteration 95/1000 | Loss: 0.00004682
Iteration 96/1000 | Loss: 0.00004681
Iteration 97/1000 | Loss: 0.00004681
Iteration 98/1000 | Loss: 0.00004681
Iteration 99/1000 | Loss: 0.00004681
Iteration 100/1000 | Loss: 0.00004681
Iteration 101/1000 | Loss: 0.00004681
Iteration 102/1000 | Loss: 0.00004680
Iteration 103/1000 | Loss: 0.00004680
Iteration 104/1000 | Loss: 0.00004680
Iteration 105/1000 | Loss: 0.00004680
Iteration 106/1000 | Loss: 0.00004680
Iteration 107/1000 | Loss: 0.00004679
Iteration 108/1000 | Loss: 0.00004679
Iteration 109/1000 | Loss: 0.00004679
Iteration 110/1000 | Loss: 0.00004678
Iteration 111/1000 | Loss: 0.00004678
Iteration 112/1000 | Loss: 0.00004678
Iteration 113/1000 | Loss: 0.00004678
Iteration 114/1000 | Loss: 0.00004678
Iteration 115/1000 | Loss: 0.00004678
Iteration 116/1000 | Loss: 0.00004678
Iteration 117/1000 | Loss: 0.00004678
Iteration 118/1000 | Loss: 0.00004678
Iteration 119/1000 | Loss: 0.00004677
Iteration 120/1000 | Loss: 0.00004677
Iteration 121/1000 | Loss: 0.00004677
Iteration 122/1000 | Loss: 0.00004677
Iteration 123/1000 | Loss: 0.00004677
Iteration 124/1000 | Loss: 0.00004676
Iteration 125/1000 | Loss: 0.00004676
Iteration 126/1000 | Loss: 0.00004676
Iteration 127/1000 | Loss: 0.00004676
Iteration 128/1000 | Loss: 0.00004675
Iteration 129/1000 | Loss: 0.00004675
Iteration 130/1000 | Loss: 0.00004675
Iteration 131/1000 | Loss: 0.00004675
Iteration 132/1000 | Loss: 0.00004675
Iteration 133/1000 | Loss: 0.00004674
Iteration 134/1000 | Loss: 0.00004674
Iteration 135/1000 | Loss: 0.00004674
Iteration 136/1000 | Loss: 0.00004673
Iteration 137/1000 | Loss: 0.00004672
Iteration 138/1000 | Loss: 0.00004672
Iteration 139/1000 | Loss: 0.00004672
Iteration 140/1000 | Loss: 0.00004672
Iteration 141/1000 | Loss: 0.00004672
Iteration 142/1000 | Loss: 0.00004672
Iteration 143/1000 | Loss: 0.00004672
Iteration 144/1000 | Loss: 0.00004672
Iteration 145/1000 | Loss: 0.00004672
Iteration 146/1000 | Loss: 0.00004672
Iteration 147/1000 | Loss: 0.00004672
Iteration 148/1000 | Loss: 0.00004671
Iteration 149/1000 | Loss: 0.00004671
Iteration 150/1000 | Loss: 0.00004671
Iteration 151/1000 | Loss: 0.00004671
Iteration 152/1000 | Loss: 0.00004671
Iteration 153/1000 | Loss: 0.00004671
Iteration 154/1000 | Loss: 0.00004670
Iteration 155/1000 | Loss: 0.00004670
Iteration 156/1000 | Loss: 0.00004670
Iteration 157/1000 | Loss: 0.00004670
Iteration 158/1000 | Loss: 0.00004670
Iteration 159/1000 | Loss: 0.00004670
Iteration 160/1000 | Loss: 0.00004670
Iteration 161/1000 | Loss: 0.00004670
Iteration 162/1000 | Loss: 0.00004670
Iteration 163/1000 | Loss: 0.00004670
Iteration 164/1000 | Loss: 0.00004669
Iteration 165/1000 | Loss: 0.00004669
Iteration 166/1000 | Loss: 0.00004669
Iteration 167/1000 | Loss: 0.00004669
Iteration 168/1000 | Loss: 0.00004669
Iteration 169/1000 | Loss: 0.00004669
Iteration 170/1000 | Loss: 0.00004669
Iteration 171/1000 | Loss: 0.00004668
Iteration 172/1000 | Loss: 0.00004668
Iteration 173/1000 | Loss: 0.00004668
Iteration 174/1000 | Loss: 0.00004668
Iteration 175/1000 | Loss: 0.00004667
Iteration 176/1000 | Loss: 0.00004667
Iteration 177/1000 | Loss: 0.00004667
Iteration 178/1000 | Loss: 0.00004667
Iteration 179/1000 | Loss: 0.00004667
Iteration 180/1000 | Loss: 0.00004667
Iteration 181/1000 | Loss: 0.00004667
Iteration 182/1000 | Loss: 0.00004667
Iteration 183/1000 | Loss: 0.00004666
Iteration 184/1000 | Loss: 0.00004666
Iteration 185/1000 | Loss: 0.00004666
Iteration 186/1000 | Loss: 0.00004665
Iteration 187/1000 | Loss: 0.00004665
Iteration 188/1000 | Loss: 0.00004665
Iteration 189/1000 | Loss: 0.00004665
Iteration 190/1000 | Loss: 0.00004665
Iteration 191/1000 | Loss: 0.00004664
Iteration 192/1000 | Loss: 0.00004664
Iteration 193/1000 | Loss: 0.00004664
Iteration 194/1000 | Loss: 0.00004664
Iteration 195/1000 | Loss: 0.00004664
Iteration 196/1000 | Loss: 0.00004664
Iteration 197/1000 | Loss: 0.00004664
Iteration 198/1000 | Loss: 0.00004663
Iteration 199/1000 | Loss: 0.00004663
Iteration 200/1000 | Loss: 0.00004663
Iteration 201/1000 | Loss: 0.00004663
Iteration 202/1000 | Loss: 0.00004663
Iteration 203/1000 | Loss: 0.00004663
Iteration 204/1000 | Loss: 0.00004663
Iteration 205/1000 | Loss: 0.00004662
Iteration 206/1000 | Loss: 0.00004662
Iteration 207/1000 | Loss: 0.00004662
Iteration 208/1000 | Loss: 0.00004662
Iteration 209/1000 | Loss: 0.00004662
Iteration 210/1000 | Loss: 0.00004662
Iteration 211/1000 | Loss: 0.00004661
Iteration 212/1000 | Loss: 0.00004661
Iteration 213/1000 | Loss: 0.00004661
Iteration 214/1000 | Loss: 0.00004661
Iteration 215/1000 | Loss: 0.00004661
Iteration 216/1000 | Loss: 0.00004660
Iteration 217/1000 | Loss: 0.00004660
Iteration 218/1000 | Loss: 0.00004660
Iteration 219/1000 | Loss: 0.00004660
Iteration 220/1000 | Loss: 0.00004660
Iteration 221/1000 | Loss: 0.00004660
Iteration 222/1000 | Loss: 0.00004660
Iteration 223/1000 | Loss: 0.00004660
Iteration 224/1000 | Loss: 0.00004660
Iteration 225/1000 | Loss: 0.00004660
Iteration 226/1000 | Loss: 0.00004660
Iteration 227/1000 | Loss: 0.00004660
Iteration 228/1000 | Loss: 0.00004660
Iteration 229/1000 | Loss: 0.00004660
Iteration 230/1000 | Loss: 0.00004660
Iteration 231/1000 | Loss: 0.00004660
Iteration 232/1000 | Loss: 0.00004660
Iteration 233/1000 | Loss: 0.00004660
Iteration 234/1000 | Loss: 0.00004660
Iteration 235/1000 | Loss: 0.00004660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [4.6598492190241814e-05, 4.6598492190241814e-05, 4.6598492190241814e-05, 4.6598492190241814e-05, 4.6598492190241814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6598492190241814e-05

Optimization complete. Final v2v error: 5.726583003997803 mm

Highest mean error: 6.3249382972717285 mm for frame 182

Lowest mean error: 5.1445817947387695 mm for frame 151

Saving results

Total time: 61.77866983413696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403665
Iteration 2/25 | Loss: 0.00093726
Iteration 3/25 | Loss: 0.00081214
Iteration 4/25 | Loss: 0.00079522
Iteration 5/25 | Loss: 0.00078934
Iteration 6/25 | Loss: 0.00078810
Iteration 7/25 | Loss: 0.00078805
Iteration 8/25 | Loss: 0.00078805
Iteration 9/25 | Loss: 0.00078805
Iteration 10/25 | Loss: 0.00078805
Iteration 11/25 | Loss: 0.00078805
Iteration 12/25 | Loss: 0.00078805
Iteration 13/25 | Loss: 0.00078805
Iteration 14/25 | Loss: 0.00078805
Iteration 15/25 | Loss: 0.00078805
Iteration 16/25 | Loss: 0.00078805
Iteration 17/25 | Loss: 0.00078805
Iteration 18/25 | Loss: 0.00078805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007880458724685013, 0.0007880458724685013, 0.0007880458724685013, 0.0007880458724685013, 0.0007880458724685013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007880458724685013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.17166710
Iteration 2/25 | Loss: 0.00161613
Iteration 3/25 | Loss: 0.00161612
Iteration 4/25 | Loss: 0.00161612
Iteration 5/25 | Loss: 0.00161612
Iteration 6/25 | Loss: 0.00161612
Iteration 7/25 | Loss: 0.00161612
Iteration 8/25 | Loss: 0.00161612
Iteration 9/25 | Loss: 0.00161612
Iteration 10/25 | Loss: 0.00161612
Iteration 11/25 | Loss: 0.00161612
Iteration 12/25 | Loss: 0.00161612
Iteration 13/25 | Loss: 0.00161612
Iteration 14/25 | Loss: 0.00161612
Iteration 15/25 | Loss: 0.00161612
Iteration 16/25 | Loss: 0.00161612
Iteration 17/25 | Loss: 0.00161612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016161160310730338, 0.0016161160310730338, 0.0016161160310730338, 0.0016161160310730338, 0.0016161160310730338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016161160310730338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161612
Iteration 2/1000 | Loss: 0.00003279
Iteration 3/1000 | Loss: 0.00002462
Iteration 4/1000 | Loss: 0.00002299
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00002068
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001989
Iteration 9/1000 | Loss: 0.00001961
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001936
Iteration 12/1000 | Loss: 0.00001932
Iteration 13/1000 | Loss: 0.00001919
Iteration 14/1000 | Loss: 0.00001908
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001899
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001895
Iteration 19/1000 | Loss: 0.00001891
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001890
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001884
Iteration 25/1000 | Loss: 0.00001881
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001879
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00001879
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001878
Iteration 34/1000 | Loss: 0.00001878
Iteration 35/1000 | Loss: 0.00001878
Iteration 36/1000 | Loss: 0.00001877
Iteration 37/1000 | Loss: 0.00001877
Iteration 38/1000 | Loss: 0.00001877
Iteration 39/1000 | Loss: 0.00001877
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001874
Iteration 46/1000 | Loss: 0.00001873
Iteration 47/1000 | Loss: 0.00001873
Iteration 48/1000 | Loss: 0.00001873
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001871
Iteration 52/1000 | Loss: 0.00001871
Iteration 53/1000 | Loss: 0.00001870
Iteration 54/1000 | Loss: 0.00001870
Iteration 55/1000 | Loss: 0.00001870
Iteration 56/1000 | Loss: 0.00001869
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001868
Iteration 60/1000 | Loss: 0.00001868
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001866
Iteration 75/1000 | Loss: 0.00001866
Iteration 76/1000 | Loss: 0.00001866
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001865
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001865
Iteration 86/1000 | Loss: 0.00001865
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001864
Iteration 96/1000 | Loss: 0.00001864
Iteration 97/1000 | Loss: 0.00001864
Iteration 98/1000 | Loss: 0.00001864
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001864
Iteration 101/1000 | Loss: 0.00001864
Iteration 102/1000 | Loss: 0.00001864
Iteration 103/1000 | Loss: 0.00001864
Iteration 104/1000 | Loss: 0.00001864
Iteration 105/1000 | Loss: 0.00001864
Iteration 106/1000 | Loss: 0.00001864
Iteration 107/1000 | Loss: 0.00001864
Iteration 108/1000 | Loss: 0.00001864
Iteration 109/1000 | Loss: 0.00001863
Iteration 110/1000 | Loss: 0.00001863
Iteration 111/1000 | Loss: 0.00001863
Iteration 112/1000 | Loss: 0.00001863
Iteration 113/1000 | Loss: 0.00001863
Iteration 114/1000 | Loss: 0.00001863
Iteration 115/1000 | Loss: 0.00001863
Iteration 116/1000 | Loss: 0.00001863
Iteration 117/1000 | Loss: 0.00001863
Iteration 118/1000 | Loss: 0.00001863
Iteration 119/1000 | Loss: 0.00001863
Iteration 120/1000 | Loss: 0.00001863
Iteration 121/1000 | Loss: 0.00001863
Iteration 122/1000 | Loss: 0.00001863
Iteration 123/1000 | Loss: 0.00001863
Iteration 124/1000 | Loss: 0.00001863
Iteration 125/1000 | Loss: 0.00001863
Iteration 126/1000 | Loss: 0.00001863
Iteration 127/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.8631388229550794e-05, 1.8631388229550794e-05, 1.8631388229550794e-05, 1.8631388229550794e-05, 1.8631388229550794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8631388229550794e-05

Optimization complete. Final v2v error: 3.757082939147949 mm

Highest mean error: 4.109035491943359 mm for frame 95

Lowest mean error: 3.4849488735198975 mm for frame 116

Saving results

Total time: 35.010247230529785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049890
Iteration 2/25 | Loss: 0.00184670
Iteration 3/25 | Loss: 0.00128011
Iteration 4/25 | Loss: 0.00110898
Iteration 5/25 | Loss: 0.00105462
Iteration 6/25 | Loss: 0.00108081
Iteration 7/25 | Loss: 0.00100435
Iteration 8/25 | Loss: 0.00091007
Iteration 9/25 | Loss: 0.00085100
Iteration 10/25 | Loss: 0.00084031
Iteration 11/25 | Loss: 0.00083316
Iteration 12/25 | Loss: 0.00082766
Iteration 13/25 | Loss: 0.00081638
Iteration 14/25 | Loss: 0.00082443
Iteration 15/25 | Loss: 0.00082680
Iteration 16/25 | Loss: 0.00081888
Iteration 17/25 | Loss: 0.00081572
Iteration 18/25 | Loss: 0.00081185
Iteration 19/25 | Loss: 0.00081128
Iteration 20/25 | Loss: 0.00080978
Iteration 21/25 | Loss: 0.00080495
Iteration 22/25 | Loss: 0.00081146
Iteration 23/25 | Loss: 0.00080905
Iteration 24/25 | Loss: 0.00080930
Iteration 25/25 | Loss: 0.00081601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21702051
Iteration 2/25 | Loss: 0.00217569
Iteration 3/25 | Loss: 0.00217569
Iteration 4/25 | Loss: 0.00217569
Iteration 5/25 | Loss: 0.00217569
Iteration 6/25 | Loss: 0.00217569
Iteration 7/25 | Loss: 0.00217569
Iteration 8/25 | Loss: 0.00217569
Iteration 9/25 | Loss: 0.00217569
Iteration 10/25 | Loss: 0.00217569
Iteration 11/25 | Loss: 0.00217569
Iteration 12/25 | Loss: 0.00217569
Iteration 13/25 | Loss: 0.00217569
Iteration 14/25 | Loss: 0.00217569
Iteration 15/25 | Loss: 0.00217569
Iteration 16/25 | Loss: 0.00217569
Iteration 17/25 | Loss: 0.00217569
Iteration 18/25 | Loss: 0.00217569
Iteration 19/25 | Loss: 0.00217569
Iteration 20/25 | Loss: 0.00217569
Iteration 21/25 | Loss: 0.00217569
Iteration 22/25 | Loss: 0.00217569
Iteration 23/25 | Loss: 0.00217569
Iteration 24/25 | Loss: 0.00217569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0021756868809461594, 0.0021756868809461594, 0.0021756868809461594, 0.0021756868809461594, 0.0021756868809461594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021756868809461594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217569
Iteration 2/1000 | Loss: 0.00037535
Iteration 3/1000 | Loss: 0.00053067
Iteration 4/1000 | Loss: 0.00017745
Iteration 5/1000 | Loss: 0.00006884
Iteration 6/1000 | Loss: 0.00009020
Iteration 7/1000 | Loss: 0.00014062
Iteration 8/1000 | Loss: 0.00011297
Iteration 9/1000 | Loss: 0.00015095
Iteration 10/1000 | Loss: 0.00067448
Iteration 11/1000 | Loss: 0.00013960
Iteration 12/1000 | Loss: 0.00105111
Iteration 13/1000 | Loss: 0.00027865
Iteration 14/1000 | Loss: 0.00019904
Iteration 15/1000 | Loss: 0.00005550
Iteration 16/1000 | Loss: 0.00004539
Iteration 17/1000 | Loss: 0.00004315
Iteration 18/1000 | Loss: 0.00003414
Iteration 19/1000 | Loss: 0.00003025
Iteration 20/1000 | Loss: 0.00002858
Iteration 21/1000 | Loss: 0.00006080
Iteration 22/1000 | Loss: 0.00002848
Iteration 23/1000 | Loss: 0.00002420
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002033
Iteration 26/1000 | Loss: 0.00001937
Iteration 27/1000 | Loss: 0.00001853
Iteration 28/1000 | Loss: 0.00001774
Iteration 29/1000 | Loss: 0.00034187
Iteration 30/1000 | Loss: 0.00017134
Iteration 31/1000 | Loss: 0.00035640
Iteration 32/1000 | Loss: 0.00004020
Iteration 33/1000 | Loss: 0.00002783
Iteration 34/1000 | Loss: 0.00002200
Iteration 35/1000 | Loss: 0.00001993
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001640
Iteration 39/1000 | Loss: 0.00001599
Iteration 40/1000 | Loss: 0.00001568
Iteration 41/1000 | Loss: 0.00001543
Iteration 42/1000 | Loss: 0.00001516
Iteration 43/1000 | Loss: 0.00001495
Iteration 44/1000 | Loss: 0.00002480
Iteration 45/1000 | Loss: 0.00001501
Iteration 46/1000 | Loss: 0.00001441
Iteration 47/1000 | Loss: 0.00001427
Iteration 48/1000 | Loss: 0.00001423
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001419
Iteration 53/1000 | Loss: 0.00001416
Iteration 54/1000 | Loss: 0.00001416
Iteration 55/1000 | Loss: 0.00001414
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00001412
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001411
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001411
Iteration 63/1000 | Loss: 0.00001411
Iteration 64/1000 | Loss: 0.00001411
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001410
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001407
Iteration 72/1000 | Loss: 0.00001407
Iteration 73/1000 | Loss: 0.00001407
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001402
Iteration 78/1000 | Loss: 0.00001402
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001392
Iteration 83/1000 | Loss: 0.00001392
Iteration 84/1000 | Loss: 0.00001390
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001387
Iteration 101/1000 | Loss: 0.00001387
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001387
Iteration 104/1000 | Loss: 0.00001387
Iteration 105/1000 | Loss: 0.00001387
Iteration 106/1000 | Loss: 0.00001387
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001386
Iteration 109/1000 | Loss: 0.00001386
Iteration 110/1000 | Loss: 0.00001386
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001386
Iteration 113/1000 | Loss: 0.00001386
Iteration 114/1000 | Loss: 0.00001385
Iteration 115/1000 | Loss: 0.00001385
Iteration 116/1000 | Loss: 0.00001385
Iteration 117/1000 | Loss: 0.00001385
Iteration 118/1000 | Loss: 0.00001384
Iteration 119/1000 | Loss: 0.00001384
Iteration 120/1000 | Loss: 0.00001384
Iteration 121/1000 | Loss: 0.00001384
Iteration 122/1000 | Loss: 0.00001384
Iteration 123/1000 | Loss: 0.00001383
Iteration 124/1000 | Loss: 0.00001383
Iteration 125/1000 | Loss: 0.00001383
Iteration 126/1000 | Loss: 0.00001383
Iteration 127/1000 | Loss: 0.00001383
Iteration 128/1000 | Loss: 0.00001383
Iteration 129/1000 | Loss: 0.00001383
Iteration 130/1000 | Loss: 0.00001383
Iteration 131/1000 | Loss: 0.00001383
Iteration 132/1000 | Loss: 0.00001383
Iteration 133/1000 | Loss: 0.00001383
Iteration 134/1000 | Loss: 0.00001382
Iteration 135/1000 | Loss: 0.00001382
Iteration 136/1000 | Loss: 0.00001382
Iteration 137/1000 | Loss: 0.00001382
Iteration 138/1000 | Loss: 0.00001382
Iteration 139/1000 | Loss: 0.00001382
Iteration 140/1000 | Loss: 0.00001382
Iteration 141/1000 | Loss: 0.00001382
Iteration 142/1000 | Loss: 0.00001382
Iteration 143/1000 | Loss: 0.00001382
Iteration 144/1000 | Loss: 0.00001382
Iteration 145/1000 | Loss: 0.00001382
Iteration 146/1000 | Loss: 0.00001382
Iteration 147/1000 | Loss: 0.00001382
Iteration 148/1000 | Loss: 0.00001382
Iteration 149/1000 | Loss: 0.00001382
Iteration 150/1000 | Loss: 0.00001382
Iteration 151/1000 | Loss: 0.00001382
Iteration 152/1000 | Loss: 0.00001382
Iteration 153/1000 | Loss: 0.00001381
Iteration 154/1000 | Loss: 0.00001381
Iteration 155/1000 | Loss: 0.00001381
Iteration 156/1000 | Loss: 0.00001381
Iteration 157/1000 | Loss: 0.00001381
Iteration 158/1000 | Loss: 0.00001381
Iteration 159/1000 | Loss: 0.00001381
Iteration 160/1000 | Loss: 0.00001381
Iteration 161/1000 | Loss: 0.00001381
Iteration 162/1000 | Loss: 0.00001381
Iteration 163/1000 | Loss: 0.00001381
Iteration 164/1000 | Loss: 0.00001381
Iteration 165/1000 | Loss: 0.00001381
Iteration 166/1000 | Loss: 0.00001381
Iteration 167/1000 | Loss: 0.00001381
Iteration 168/1000 | Loss: 0.00001381
Iteration 169/1000 | Loss: 0.00001380
Iteration 170/1000 | Loss: 0.00001380
Iteration 171/1000 | Loss: 0.00001380
Iteration 172/1000 | Loss: 0.00001380
Iteration 173/1000 | Loss: 0.00001380
Iteration 174/1000 | Loss: 0.00001380
Iteration 175/1000 | Loss: 0.00001380
Iteration 176/1000 | Loss: 0.00001380
Iteration 177/1000 | Loss: 0.00001380
Iteration 178/1000 | Loss: 0.00001380
Iteration 179/1000 | Loss: 0.00001380
Iteration 180/1000 | Loss: 0.00001380
Iteration 181/1000 | Loss: 0.00001380
Iteration 182/1000 | Loss: 0.00001380
Iteration 183/1000 | Loss: 0.00001380
Iteration 184/1000 | Loss: 0.00001380
Iteration 185/1000 | Loss: 0.00001380
Iteration 186/1000 | Loss: 0.00001380
Iteration 187/1000 | Loss: 0.00001380
Iteration 188/1000 | Loss: 0.00001380
Iteration 189/1000 | Loss: 0.00001380
Iteration 190/1000 | Loss: 0.00001380
Iteration 191/1000 | Loss: 0.00001380
Iteration 192/1000 | Loss: 0.00001380
Iteration 193/1000 | Loss: 0.00001380
Iteration 194/1000 | Loss: 0.00001380
Iteration 195/1000 | Loss: 0.00001380
Iteration 196/1000 | Loss: 0.00001380
Iteration 197/1000 | Loss: 0.00001380
Iteration 198/1000 | Loss: 0.00001380
Iteration 199/1000 | Loss: 0.00001380
Iteration 200/1000 | Loss: 0.00001380
Iteration 201/1000 | Loss: 0.00001380
Iteration 202/1000 | Loss: 0.00001380
Iteration 203/1000 | Loss: 0.00001380
Iteration 204/1000 | Loss: 0.00001380
Iteration 205/1000 | Loss: 0.00001380
Iteration 206/1000 | Loss: 0.00001380
Iteration 207/1000 | Loss: 0.00001380
Iteration 208/1000 | Loss: 0.00001380
Iteration 209/1000 | Loss: 0.00001380
Iteration 210/1000 | Loss: 0.00001380
Iteration 211/1000 | Loss: 0.00001380
Iteration 212/1000 | Loss: 0.00001380
Iteration 213/1000 | Loss: 0.00001380
Iteration 214/1000 | Loss: 0.00001380
Iteration 215/1000 | Loss: 0.00001380
Iteration 216/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.3798417057842016e-05, 1.3798417057842016e-05, 1.3798417057842016e-05, 1.3798417057842016e-05, 1.3798417057842016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3798417057842016e-05

Optimization complete. Final v2v error: 3.138739585876465 mm

Highest mean error: 3.532653570175171 mm for frame 27

Lowest mean error: 2.9299564361572266 mm for frame 121

Saving results

Total time: 118.00877094268799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069117
Iteration 2/25 | Loss: 0.00301210
Iteration 3/25 | Loss: 0.00238007
Iteration 4/25 | Loss: 0.00157396
Iteration 5/25 | Loss: 0.00143565
Iteration 6/25 | Loss: 0.00136610
Iteration 7/25 | Loss: 0.00145249
Iteration 8/25 | Loss: 0.00144614
Iteration 9/25 | Loss: 0.00159546
Iteration 10/25 | Loss: 0.00170905
Iteration 11/25 | Loss: 0.00159647
Iteration 12/25 | Loss: 0.00149469
Iteration 13/25 | Loss: 0.00143243
Iteration 14/25 | Loss: 0.00137666
Iteration 15/25 | Loss: 0.00133251
Iteration 16/25 | Loss: 0.00131546
Iteration 17/25 | Loss: 0.00127584
Iteration 18/25 | Loss: 0.00126547
Iteration 19/25 | Loss: 0.00127028
Iteration 20/25 | Loss: 0.00123820
Iteration 21/25 | Loss: 0.00125035
Iteration 22/25 | Loss: 0.00123576
Iteration 23/25 | Loss: 0.00132007
Iteration 24/25 | Loss: 0.00134106
Iteration 25/25 | Loss: 0.00139992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34986615
Iteration 2/25 | Loss: 0.00542291
Iteration 3/25 | Loss: 0.00535042
Iteration 4/25 | Loss: 0.00535042
Iteration 5/25 | Loss: 0.00535042
Iteration 6/25 | Loss: 0.00535042
Iteration 7/25 | Loss: 0.00535042
Iteration 8/25 | Loss: 0.00535042
Iteration 9/25 | Loss: 0.00535042
Iteration 10/25 | Loss: 0.00535042
Iteration 11/25 | Loss: 0.00535042
Iteration 12/25 | Loss: 0.00535042
Iteration 13/25 | Loss: 0.00535042
Iteration 14/25 | Loss: 0.00535042
Iteration 15/25 | Loss: 0.00535042
Iteration 16/25 | Loss: 0.00535042
Iteration 17/25 | Loss: 0.00535042
Iteration 18/25 | Loss: 0.00535042
Iteration 19/25 | Loss: 0.00535042
Iteration 20/25 | Loss: 0.00535042
Iteration 21/25 | Loss: 0.00535042
Iteration 22/25 | Loss: 0.00535042
Iteration 23/25 | Loss: 0.00535042
Iteration 24/25 | Loss: 0.00535042
Iteration 25/25 | Loss: 0.00535042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.005350416526198387, 0.005350416526198387, 0.005350416526198387, 0.005350416526198387, 0.005350416526198387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005350416526198387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00535042
Iteration 2/1000 | Loss: 0.00124612
Iteration 3/1000 | Loss: 0.00108450
Iteration 4/1000 | Loss: 0.00067933
Iteration 5/1000 | Loss: 0.00078431
Iteration 6/1000 | Loss: 0.00054912
Iteration 7/1000 | Loss: 0.00084776
Iteration 8/1000 | Loss: 0.00059996
Iteration 9/1000 | Loss: 0.00079472
Iteration 10/1000 | Loss: 0.00210918
Iteration 11/1000 | Loss: 0.00295093
Iteration 12/1000 | Loss: 0.00201604
Iteration 13/1000 | Loss: 0.00190579
Iteration 14/1000 | Loss: 0.00149112
Iteration 15/1000 | Loss: 0.00148252
Iteration 16/1000 | Loss: 0.00166285
Iteration 17/1000 | Loss: 0.00234465
Iteration 18/1000 | Loss: 0.00132475
Iteration 19/1000 | Loss: 0.00131843
Iteration 20/1000 | Loss: 0.00092474
Iteration 21/1000 | Loss: 0.00098915
Iteration 22/1000 | Loss: 0.00096187
Iteration 23/1000 | Loss: 0.00078921
Iteration 24/1000 | Loss: 0.00105616
Iteration 25/1000 | Loss: 0.00111499
Iteration 26/1000 | Loss: 0.00103042
Iteration 27/1000 | Loss: 0.00083997
Iteration 28/1000 | Loss: 0.00072785
Iteration 29/1000 | Loss: 0.00162552
Iteration 30/1000 | Loss: 0.00150734
Iteration 31/1000 | Loss: 0.00085633
Iteration 32/1000 | Loss: 0.00077213
Iteration 33/1000 | Loss: 0.00128654
Iteration 34/1000 | Loss: 0.00135244
Iteration 35/1000 | Loss: 0.00130680
Iteration 36/1000 | Loss: 0.00067301
Iteration 37/1000 | Loss: 0.00038261
Iteration 38/1000 | Loss: 0.00087520
Iteration 39/1000 | Loss: 0.00043078
Iteration 40/1000 | Loss: 0.00054963
Iteration 41/1000 | Loss: 0.00059042
Iteration 42/1000 | Loss: 0.00024925
Iteration 43/1000 | Loss: 0.00037066
Iteration 44/1000 | Loss: 0.00030957
Iteration 45/1000 | Loss: 0.00036056
Iteration 46/1000 | Loss: 0.00042022
Iteration 47/1000 | Loss: 0.00028049
Iteration 48/1000 | Loss: 0.00057484
Iteration 49/1000 | Loss: 0.00083186
Iteration 50/1000 | Loss: 0.00033657
Iteration 51/1000 | Loss: 0.00033969
Iteration 52/1000 | Loss: 0.00032372
Iteration 53/1000 | Loss: 0.00051859
Iteration 54/1000 | Loss: 0.00025624
Iteration 55/1000 | Loss: 0.00040644
Iteration 56/1000 | Loss: 0.00038091
Iteration 57/1000 | Loss: 0.00047438
Iteration 58/1000 | Loss: 0.00027387
Iteration 59/1000 | Loss: 0.00030856
Iteration 60/1000 | Loss: 0.00015617
Iteration 61/1000 | Loss: 0.00045625
Iteration 62/1000 | Loss: 0.00108705
Iteration 63/1000 | Loss: 0.00089282
Iteration 64/1000 | Loss: 0.00104943
Iteration 65/1000 | Loss: 0.00073608
Iteration 66/1000 | Loss: 0.00073121
Iteration 67/1000 | Loss: 0.00084257
Iteration 68/1000 | Loss: 0.00052276
Iteration 69/1000 | Loss: 0.00045133
Iteration 70/1000 | Loss: 0.00049914
Iteration 71/1000 | Loss: 0.00055232
Iteration 72/1000 | Loss: 0.00054324
Iteration 73/1000 | Loss: 0.00053330
Iteration 74/1000 | Loss: 0.00098525
Iteration 75/1000 | Loss: 0.00055659
Iteration 76/1000 | Loss: 0.00040399
Iteration 77/1000 | Loss: 0.00074746
Iteration 78/1000 | Loss: 0.00086298
Iteration 79/1000 | Loss: 0.00090378
Iteration 80/1000 | Loss: 0.00066562
Iteration 81/1000 | Loss: 0.00044948
Iteration 82/1000 | Loss: 0.00098393
Iteration 83/1000 | Loss: 0.00080245
Iteration 84/1000 | Loss: 0.00071055
Iteration 85/1000 | Loss: 0.00090183
Iteration 86/1000 | Loss: 0.00113383
Iteration 87/1000 | Loss: 0.00114151
Iteration 88/1000 | Loss: 0.00098326
Iteration 89/1000 | Loss: 0.00122705
Iteration 90/1000 | Loss: 0.00123092
Iteration 91/1000 | Loss: 0.00156317
Iteration 92/1000 | Loss: 0.00108942
Iteration 93/1000 | Loss: 0.00091562
Iteration 94/1000 | Loss: 0.00060945
Iteration 95/1000 | Loss: 0.00059487
Iteration 96/1000 | Loss: 0.00063936
Iteration 97/1000 | Loss: 0.00079517
Iteration 98/1000 | Loss: 0.00128044
Iteration 99/1000 | Loss: 0.00241887
Iteration 100/1000 | Loss: 0.00200769
Iteration 101/1000 | Loss: 0.00109536
Iteration 102/1000 | Loss: 0.00098987
Iteration 103/1000 | Loss: 0.00078964
Iteration 104/1000 | Loss: 0.00048249
Iteration 105/1000 | Loss: 0.00089665
Iteration 106/1000 | Loss: 0.00093194
Iteration 107/1000 | Loss: 0.00090706
Iteration 108/1000 | Loss: 0.00065838
Iteration 109/1000 | Loss: 0.00051489
Iteration 110/1000 | Loss: 0.00073886
Iteration 111/1000 | Loss: 0.00193936
Iteration 112/1000 | Loss: 0.00158058
Iteration 113/1000 | Loss: 0.00137348
Iteration 114/1000 | Loss: 0.00109261
Iteration 115/1000 | Loss: 0.00060086
Iteration 116/1000 | Loss: 0.00095786
Iteration 117/1000 | Loss: 0.00098448
Iteration 118/1000 | Loss: 0.00091091
Iteration 119/1000 | Loss: 0.00084530
Iteration 120/1000 | Loss: 0.00117549
Iteration 121/1000 | Loss: 0.00081033
Iteration 122/1000 | Loss: 0.00119669
Iteration 123/1000 | Loss: 0.00151552
Iteration 124/1000 | Loss: 0.00128028
Iteration 125/1000 | Loss: 0.00119939
Iteration 126/1000 | Loss: 0.00128646
Iteration 127/1000 | Loss: 0.00122165
Iteration 128/1000 | Loss: 0.00118724
Iteration 129/1000 | Loss: 0.00149519
Iteration 130/1000 | Loss: 0.00151348
Iteration 131/1000 | Loss: 0.00117498
Iteration 132/1000 | Loss: 0.00152812
Iteration 133/1000 | Loss: 0.00104390
Iteration 134/1000 | Loss: 0.00136935
Iteration 135/1000 | Loss: 0.00139236
Iteration 136/1000 | Loss: 0.00098665
Iteration 137/1000 | Loss: 0.00081796
Iteration 138/1000 | Loss: 0.00084247
Iteration 139/1000 | Loss: 0.00049811
Iteration 140/1000 | Loss: 0.00063066
Iteration 141/1000 | Loss: 0.00091256
Iteration 142/1000 | Loss: 0.00105302
Iteration 143/1000 | Loss: 0.00121073
Iteration 144/1000 | Loss: 0.00116647
Iteration 145/1000 | Loss: 0.00167986
Iteration 146/1000 | Loss: 0.00173436
Iteration 147/1000 | Loss: 0.00117348
Iteration 148/1000 | Loss: 0.00099497
Iteration 149/1000 | Loss: 0.00080996
Iteration 150/1000 | Loss: 0.00089496
Iteration 151/1000 | Loss: 0.00161155
Iteration 152/1000 | Loss: 0.00195010
Iteration 153/1000 | Loss: 0.00137953
Iteration 154/1000 | Loss: 0.00138661
Iteration 155/1000 | Loss: 0.00122722
Iteration 156/1000 | Loss: 0.00118428
Iteration 157/1000 | Loss: 0.00122794
Iteration 158/1000 | Loss: 0.00088855
Iteration 159/1000 | Loss: 0.00108921
Iteration 160/1000 | Loss: 0.00113082
Iteration 161/1000 | Loss: 0.00124937
Iteration 162/1000 | Loss: 0.00146654
Iteration 163/1000 | Loss: 0.00135012
Iteration 164/1000 | Loss: 0.00123842
Iteration 165/1000 | Loss: 0.00084004
Iteration 166/1000 | Loss: 0.00126020
Iteration 167/1000 | Loss: 0.00096582
Iteration 168/1000 | Loss: 0.00096897
Iteration 169/1000 | Loss: 0.00096463
Iteration 170/1000 | Loss: 0.00071847
Iteration 171/1000 | Loss: 0.00068008
Iteration 172/1000 | Loss: 0.00091900
Iteration 173/1000 | Loss: 0.00091263
Iteration 174/1000 | Loss: 0.00077458
Iteration 175/1000 | Loss: 0.00068689
Iteration 176/1000 | Loss: 0.00071484
Iteration 177/1000 | Loss: 0.00061458
Iteration 178/1000 | Loss: 0.00056734
Iteration 179/1000 | Loss: 0.00115150
Iteration 180/1000 | Loss: 0.00101238
Iteration 181/1000 | Loss: 0.00122205
Iteration 182/1000 | Loss: 0.00137178
Iteration 183/1000 | Loss: 0.00149359
Iteration 184/1000 | Loss: 0.00113604
Iteration 185/1000 | Loss: 0.00084281
Iteration 186/1000 | Loss: 0.00073701
Iteration 187/1000 | Loss: 0.00115774
Iteration 188/1000 | Loss: 0.00063857
Iteration 189/1000 | Loss: 0.00103460
Iteration 190/1000 | Loss: 0.00039149
Iteration 191/1000 | Loss: 0.00043513
Iteration 192/1000 | Loss: 0.00060748
Iteration 193/1000 | Loss: 0.00070432
Iteration 194/1000 | Loss: 0.00053609
Iteration 195/1000 | Loss: 0.00032453
Iteration 196/1000 | Loss: 0.00026784
Iteration 197/1000 | Loss: 0.00052167
Iteration 198/1000 | Loss: 0.00017381
Iteration 199/1000 | Loss: 0.00056188
Iteration 200/1000 | Loss: 0.00100677
Iteration 201/1000 | Loss: 0.00064354
Iteration 202/1000 | Loss: 0.00029109
Iteration 203/1000 | Loss: 0.00091264
Iteration 204/1000 | Loss: 0.00023562
Iteration 205/1000 | Loss: 0.00049411
Iteration 206/1000 | Loss: 0.00037383
Iteration 207/1000 | Loss: 0.00038145
Iteration 208/1000 | Loss: 0.00023736
Iteration 209/1000 | Loss: 0.00097147
Iteration 210/1000 | Loss: 0.00072804
Iteration 211/1000 | Loss: 0.00023182
Iteration 212/1000 | Loss: 0.00020934
Iteration 213/1000 | Loss: 0.00017917
Iteration 214/1000 | Loss: 0.00029129
Iteration 215/1000 | Loss: 0.00014701
Iteration 216/1000 | Loss: 0.00040694
Iteration 217/1000 | Loss: 0.00025482
Iteration 218/1000 | Loss: 0.00037553
Iteration 219/1000 | Loss: 0.00035472
Iteration 220/1000 | Loss: 0.00023364
Iteration 221/1000 | Loss: 0.00042216
Iteration 222/1000 | Loss: 0.00047769
Iteration 223/1000 | Loss: 0.00031662
Iteration 224/1000 | Loss: 0.00050812
Iteration 225/1000 | Loss: 0.00054713
Iteration 226/1000 | Loss: 0.00025830
Iteration 227/1000 | Loss: 0.00039993
Iteration 228/1000 | Loss: 0.00038167
Iteration 229/1000 | Loss: 0.00018419
Iteration 230/1000 | Loss: 0.00032826
Iteration 231/1000 | Loss: 0.00012669
Iteration 232/1000 | Loss: 0.00012209
Iteration 233/1000 | Loss: 0.00014688
Iteration 234/1000 | Loss: 0.00010758
Iteration 235/1000 | Loss: 0.00012421
Iteration 236/1000 | Loss: 0.00012599
Iteration 237/1000 | Loss: 0.00011231
Iteration 238/1000 | Loss: 0.00012249
Iteration 239/1000 | Loss: 0.00013655
Iteration 240/1000 | Loss: 0.00013636
Iteration 241/1000 | Loss: 0.00015039
Iteration 242/1000 | Loss: 0.00023779
Iteration 243/1000 | Loss: 0.00028738
Iteration 244/1000 | Loss: 0.00013853
Iteration 245/1000 | Loss: 0.00012040
Iteration 246/1000 | Loss: 0.00011936
Iteration 247/1000 | Loss: 0.00013226
Iteration 248/1000 | Loss: 0.00009038
Iteration 249/1000 | Loss: 0.00025786
Iteration 250/1000 | Loss: 0.00023375
Iteration 251/1000 | Loss: 0.00021786
Iteration 252/1000 | Loss: 0.00026005
Iteration 253/1000 | Loss: 0.00023454
Iteration 254/1000 | Loss: 0.00062117
Iteration 255/1000 | Loss: 0.00052919
Iteration 256/1000 | Loss: 0.00046289
Iteration 257/1000 | Loss: 0.00044814
Iteration 258/1000 | Loss: 0.00014102
Iteration 259/1000 | Loss: 0.00011467
Iteration 260/1000 | Loss: 0.00029997
Iteration 261/1000 | Loss: 0.00012583
Iteration 262/1000 | Loss: 0.00010361
Iteration 263/1000 | Loss: 0.00026600
Iteration 264/1000 | Loss: 0.00034451
Iteration 265/1000 | Loss: 0.00028741
Iteration 266/1000 | Loss: 0.00037933
Iteration 267/1000 | Loss: 0.00028519
Iteration 268/1000 | Loss: 0.00024954
Iteration 269/1000 | Loss: 0.00015820
Iteration 270/1000 | Loss: 0.00018028
Iteration 271/1000 | Loss: 0.00008505
Iteration 272/1000 | Loss: 0.00009447
Iteration 273/1000 | Loss: 0.00010575
Iteration 274/1000 | Loss: 0.00043819
Iteration 275/1000 | Loss: 0.00026256
Iteration 276/1000 | Loss: 0.00019887
Iteration 277/1000 | Loss: 0.00011298
Iteration 278/1000 | Loss: 0.00009451
Iteration 279/1000 | Loss: 0.00012361
Iteration 280/1000 | Loss: 0.00011522
Iteration 281/1000 | Loss: 0.00010808
Iteration 282/1000 | Loss: 0.00012433
Iteration 283/1000 | Loss: 0.00012940
Iteration 284/1000 | Loss: 0.00012113
Iteration 285/1000 | Loss: 0.00012454
Iteration 286/1000 | Loss: 0.00012316
Iteration 287/1000 | Loss: 0.00013011
Iteration 288/1000 | Loss: 0.00066423
Iteration 289/1000 | Loss: 0.00045855
Iteration 290/1000 | Loss: 0.00012415
Iteration 291/1000 | Loss: 0.00015192
Iteration 292/1000 | Loss: 0.00014418
Iteration 293/1000 | Loss: 0.00015841
Iteration 294/1000 | Loss: 0.00011415
Iteration 295/1000 | Loss: 0.00026065
Iteration 296/1000 | Loss: 0.00051722
Iteration 297/1000 | Loss: 0.00011448
Iteration 298/1000 | Loss: 0.00026058
Iteration 299/1000 | Loss: 0.00023489
Iteration 300/1000 | Loss: 0.00025722
Iteration 301/1000 | Loss: 0.00036071
Iteration 302/1000 | Loss: 0.00028206
Iteration 303/1000 | Loss: 0.00011840
Iteration 304/1000 | Loss: 0.00009649
Iteration 305/1000 | Loss: 0.00009067
Iteration 306/1000 | Loss: 0.00011992
Iteration 307/1000 | Loss: 0.00024355
Iteration 308/1000 | Loss: 0.00013104
Iteration 309/1000 | Loss: 0.00011247
Iteration 310/1000 | Loss: 0.00012710
Iteration 311/1000 | Loss: 0.00051409
Iteration 312/1000 | Loss: 0.00018003
Iteration 313/1000 | Loss: 0.00020283
Iteration 314/1000 | Loss: 0.00023014
Iteration 315/1000 | Loss: 0.00008127
Iteration 316/1000 | Loss: 0.00025087
Iteration 317/1000 | Loss: 0.00008950
Iteration 318/1000 | Loss: 0.00034737
Iteration 319/1000 | Loss: 0.00081979
Iteration 320/1000 | Loss: 0.00017759
Iteration 321/1000 | Loss: 0.00013118
Iteration 322/1000 | Loss: 0.00010520
Iteration 323/1000 | Loss: 0.00020833
Iteration 324/1000 | Loss: 0.00008821
Iteration 325/1000 | Loss: 0.00035809
Iteration 326/1000 | Loss: 0.00009020
Iteration 327/1000 | Loss: 0.00010771
Iteration 328/1000 | Loss: 0.00024398
Iteration 329/1000 | Loss: 0.00021958
Iteration 330/1000 | Loss: 0.00073555
Iteration 331/1000 | Loss: 0.00095435
Iteration 332/1000 | Loss: 0.00080640
Iteration 333/1000 | Loss: 0.00162068
Iteration 334/1000 | Loss: 0.00210676
Iteration 335/1000 | Loss: 0.00230429
Iteration 336/1000 | Loss: 0.00201330
Iteration 337/1000 | Loss: 0.00120077
Iteration 338/1000 | Loss: 0.00163505
Iteration 339/1000 | Loss: 0.00083114
Iteration 340/1000 | Loss: 0.00095140
Iteration 341/1000 | Loss: 0.00135263
Iteration 342/1000 | Loss: 0.00087847
Iteration 343/1000 | Loss: 0.00059250
Iteration 344/1000 | Loss: 0.00181170
Iteration 345/1000 | Loss: 0.00060891
Iteration 346/1000 | Loss: 0.00032562
Iteration 347/1000 | Loss: 0.00087463
Iteration 348/1000 | Loss: 0.00062367
Iteration 349/1000 | Loss: 0.00069412
Iteration 350/1000 | Loss: 0.00050050
Iteration 351/1000 | Loss: 0.00080099
Iteration 352/1000 | Loss: 0.00052974
Iteration 353/1000 | Loss: 0.00045799
Iteration 354/1000 | Loss: 0.00061744
Iteration 355/1000 | Loss: 0.00044112
Iteration 356/1000 | Loss: 0.00077800
Iteration 357/1000 | Loss: 0.00087110
Iteration 358/1000 | Loss: 0.00109793
Iteration 359/1000 | Loss: 0.00026388
Iteration 360/1000 | Loss: 0.00101269
Iteration 361/1000 | Loss: 0.00115167
Iteration 362/1000 | Loss: 0.00134725
Iteration 363/1000 | Loss: 0.00150730
Iteration 364/1000 | Loss: 0.00111812
Iteration 365/1000 | Loss: 0.00147257
Iteration 366/1000 | Loss: 0.00124398
Iteration 367/1000 | Loss: 0.00035591
Iteration 368/1000 | Loss: 0.00087121
Iteration 369/1000 | Loss: 0.00132603
Iteration 370/1000 | Loss: 0.00014563
Iteration 371/1000 | Loss: 0.00051173
Iteration 372/1000 | Loss: 0.00012816
Iteration 373/1000 | Loss: 0.00025669
Iteration 374/1000 | Loss: 0.00031584
Iteration 375/1000 | Loss: 0.00008261
Iteration 376/1000 | Loss: 0.00010113
Iteration 377/1000 | Loss: 0.00026969
Iteration 378/1000 | Loss: 0.00008619
Iteration 379/1000 | Loss: 0.00007657
Iteration 380/1000 | Loss: 0.00007605
Iteration 381/1000 | Loss: 0.00008438
Iteration 382/1000 | Loss: 0.00010479
Iteration 383/1000 | Loss: 0.00006587
Iteration 384/1000 | Loss: 0.00009825
Iteration 385/1000 | Loss: 0.00022470
Iteration 386/1000 | Loss: 0.00011609
Iteration 387/1000 | Loss: 0.00010675
Iteration 388/1000 | Loss: 0.00009519
Iteration 389/1000 | Loss: 0.00010217
Iteration 390/1000 | Loss: 0.00010941
Iteration 391/1000 | Loss: 0.00009298
Iteration 392/1000 | Loss: 0.00008692
Iteration 393/1000 | Loss: 0.00008459
Iteration 394/1000 | Loss: 0.00009335
Iteration 395/1000 | Loss: 0.00009085
Iteration 396/1000 | Loss: 0.00022107
Iteration 397/1000 | Loss: 0.00008386
Iteration 398/1000 | Loss: 0.00023427
Iteration 399/1000 | Loss: 0.00023940
Iteration 400/1000 | Loss: 0.00019326
Iteration 401/1000 | Loss: 0.00020251
Iteration 402/1000 | Loss: 0.00023246
Iteration 403/1000 | Loss: 0.00009433
Iteration 404/1000 | Loss: 0.00010122
Iteration 405/1000 | Loss: 0.00016142
Iteration 406/1000 | Loss: 0.00009157
Iteration 407/1000 | Loss: 0.00020530
Iteration 408/1000 | Loss: 0.00008669
Iteration 409/1000 | Loss: 0.00009622
Iteration 410/1000 | Loss: 0.00007815
Iteration 411/1000 | Loss: 0.00010615
Iteration 412/1000 | Loss: 0.00008556
Iteration 413/1000 | Loss: 0.00009184
Iteration 414/1000 | Loss: 0.00008945
Iteration 415/1000 | Loss: 0.00010236
Iteration 416/1000 | Loss: 0.00009834
Iteration 417/1000 | Loss: 0.00010263
Iteration 418/1000 | Loss: 0.00006429
Iteration 419/1000 | Loss: 0.00019496
Iteration 420/1000 | Loss: 0.00015093
Iteration 421/1000 | Loss: 0.00017859
Iteration 422/1000 | Loss: 0.00010883
Iteration 423/1000 | Loss: 0.00008398
Iteration 424/1000 | Loss: 0.00005382
Iteration 425/1000 | Loss: 0.00018261
Iteration 426/1000 | Loss: 0.00005714
Iteration 427/1000 | Loss: 0.00005617
Iteration 428/1000 | Loss: 0.00019021
Iteration 429/1000 | Loss: 0.00072388
Iteration 430/1000 | Loss: 0.00056906
Iteration 431/1000 | Loss: 0.00005086
Iteration 432/1000 | Loss: 0.00018302
Iteration 433/1000 | Loss: 0.00037818
Iteration 434/1000 | Loss: 0.00034459
Iteration 435/1000 | Loss: 0.00017227
Iteration 436/1000 | Loss: 0.00006010
Iteration 437/1000 | Loss: 0.00005380
Iteration 438/1000 | Loss: 0.00026839
Iteration 439/1000 | Loss: 0.00006749
Iteration 440/1000 | Loss: 0.00005513
Iteration 441/1000 | Loss: 0.00005049
Iteration 442/1000 | Loss: 0.00038135
Iteration 443/1000 | Loss: 0.00026819
Iteration 444/1000 | Loss: 0.00018331
Iteration 445/1000 | Loss: 0.00005564
Iteration 446/1000 | Loss: 0.00004981
Iteration 447/1000 | Loss: 0.00004664
Iteration 448/1000 | Loss: 0.00004448
Iteration 449/1000 | Loss: 0.00006120
Iteration 450/1000 | Loss: 0.00004465
Iteration 451/1000 | Loss: 0.00005640
Iteration 452/1000 | Loss: 0.00005776
Iteration 453/1000 | Loss: 0.00005232
Iteration 454/1000 | Loss: 0.00005410
Iteration 455/1000 | Loss: 0.00004147
Iteration 456/1000 | Loss: 0.00024977
Iteration 457/1000 | Loss: 0.00005245
Iteration 458/1000 | Loss: 0.00004596
Iteration 459/1000 | Loss: 0.00004329
Iteration 460/1000 | Loss: 0.00004223
Iteration 461/1000 | Loss: 0.00004169
Iteration 462/1000 | Loss: 0.00004117
Iteration 463/1000 | Loss: 0.00004068
Iteration 464/1000 | Loss: 0.00016232
Iteration 465/1000 | Loss: 0.00099551
Iteration 466/1000 | Loss: 0.00196897
Iteration 467/1000 | Loss: 0.00139593
Iteration 468/1000 | Loss: 0.00095415
Iteration 469/1000 | Loss: 0.00022122
Iteration 470/1000 | Loss: 0.00016574
Iteration 471/1000 | Loss: 0.00006434
Iteration 472/1000 | Loss: 0.00005514
Iteration 473/1000 | Loss: 0.00005096
Iteration 474/1000 | Loss: 0.00018088
Iteration 475/1000 | Loss: 0.00005638
Iteration 476/1000 | Loss: 0.00004962
Iteration 477/1000 | Loss: 0.00004600
Iteration 478/1000 | Loss: 0.00004399
Iteration 479/1000 | Loss: 0.00004269
Iteration 480/1000 | Loss: 0.00004147
Iteration 481/1000 | Loss: 0.00017384
Iteration 482/1000 | Loss: 0.00017630
Iteration 483/1000 | Loss: 0.00017374
Iteration 484/1000 | Loss: 0.00017359
Iteration 485/1000 | Loss: 0.00015860
Iteration 486/1000 | Loss: 0.00008984
Iteration 487/1000 | Loss: 0.00011415
Iteration 488/1000 | Loss: 0.00013039
Iteration 489/1000 | Loss: 0.00004672
Iteration 490/1000 | Loss: 0.00004441
Iteration 491/1000 | Loss: 0.00004350
Iteration 492/1000 | Loss: 0.00029668
Iteration 493/1000 | Loss: 0.00031099
Iteration 494/1000 | Loss: 0.00056741
Iteration 495/1000 | Loss: 0.00052341
Iteration 496/1000 | Loss: 0.00026080
Iteration 497/1000 | Loss: 0.00026987
Iteration 498/1000 | Loss: 0.00024752
Iteration 499/1000 | Loss: 0.00006355
Iteration 500/1000 | Loss: 0.00005692
Iteration 501/1000 | Loss: 0.00005242
Iteration 502/1000 | Loss: 0.00018072
Iteration 503/1000 | Loss: 0.00018194
Iteration 504/1000 | Loss: 0.00031713
Iteration 505/1000 | Loss: 0.00026194
Iteration 506/1000 | Loss: 0.00014614
Iteration 507/1000 | Loss: 0.00007881
Iteration 508/1000 | Loss: 0.00005842
Iteration 509/1000 | Loss: 0.00018820
Iteration 510/1000 | Loss: 0.00062835
Iteration 511/1000 | Loss: 0.00022936
Iteration 512/1000 | Loss: 0.00016436
Iteration 513/1000 | Loss: 0.00006940
Iteration 514/1000 | Loss: 0.00005907
Iteration 515/1000 | Loss: 0.00032889
Iteration 516/1000 | Loss: 0.00056805
Iteration 517/1000 | Loss: 0.00007929
Iteration 518/1000 | Loss: 0.00019793
Iteration 519/1000 | Loss: 0.00031753
Iteration 520/1000 | Loss: 0.00023740
Iteration 521/1000 | Loss: 0.00040805
Iteration 522/1000 | Loss: 0.00024343
Iteration 523/1000 | Loss: 0.00027055
Iteration 524/1000 | Loss: 0.00006425
Iteration 525/1000 | Loss: 0.00005435
Iteration 526/1000 | Loss: 0.00006463
Iteration 527/1000 | Loss: 0.00004994
Iteration 528/1000 | Loss: 0.00018092
Iteration 529/1000 | Loss: 0.00018653
Iteration 530/1000 | Loss: 0.00005205
Iteration 531/1000 | Loss: 0.00004781
Iteration 532/1000 | Loss: 0.00004503
Iteration 533/1000 | Loss: 0.00004390
Iteration 534/1000 | Loss: 0.00004268
Iteration 535/1000 | Loss: 0.00013938
Iteration 536/1000 | Loss: 0.00017126
Iteration 537/1000 | Loss: 0.00020314
Iteration 538/1000 | Loss: 0.00046820
Iteration 539/1000 | Loss: 0.00005140
Iteration 540/1000 | Loss: 0.00004414
Iteration 541/1000 | Loss: 0.00004134
Iteration 542/1000 | Loss: 0.00029836
Iteration 543/1000 | Loss: 0.00005108
Iteration 544/1000 | Loss: 0.00004789
Iteration 545/1000 | Loss: 0.00018953
Iteration 546/1000 | Loss: 0.00014524
Iteration 547/1000 | Loss: 0.00004067
Iteration 548/1000 | Loss: 0.00003888
Iteration 549/1000 | Loss: 0.00003758
Iteration 550/1000 | Loss: 0.00019895
Iteration 551/1000 | Loss: 0.00028065
Iteration 552/1000 | Loss: 0.00030349
Iteration 553/1000 | Loss: 0.00026451
Iteration 554/1000 | Loss: 0.00006694
Iteration 555/1000 | Loss: 0.00015401
Iteration 556/1000 | Loss: 0.00094832
Iteration 557/1000 | Loss: 0.00031498
Iteration 558/1000 | Loss: 0.00018231
Iteration 559/1000 | Loss: 0.00025017
Iteration 560/1000 | Loss: 0.00018409
Iteration 561/1000 | Loss: 0.00010175
Iteration 562/1000 | Loss: 0.00021701
Iteration 563/1000 | Loss: 0.00013215
Iteration 564/1000 | Loss: 0.00005275
Iteration 565/1000 | Loss: 0.00015922
Iteration 566/1000 | Loss: 0.00026189
Iteration 567/1000 | Loss: 0.00013179
Iteration 568/1000 | Loss: 0.00008288
Iteration 569/1000 | Loss: 0.00004896
Iteration 570/1000 | Loss: 0.00004558
Iteration 571/1000 | Loss: 0.00004346
Iteration 572/1000 | Loss: 0.00004202
Iteration 573/1000 | Loss: 0.00018460
Iteration 574/1000 | Loss: 0.00033342
Iteration 575/1000 | Loss: 0.00017682
Iteration 576/1000 | Loss: 0.00005157
Iteration 577/1000 | Loss: 0.00004446
Iteration 578/1000 | Loss: 0.00004102
Iteration 579/1000 | Loss: 0.00003961
Iteration 580/1000 | Loss: 0.00003868
Iteration 581/1000 | Loss: 0.00003754
Iteration 582/1000 | Loss: 0.00003657
Iteration 583/1000 | Loss: 0.00017818
Iteration 584/1000 | Loss: 0.00013843
Iteration 585/1000 | Loss: 0.00015047
Iteration 586/1000 | Loss: 0.00003960
Iteration 587/1000 | Loss: 0.00029527
Iteration 588/1000 | Loss: 0.00045294
Iteration 589/1000 | Loss: 0.00026360
Iteration 590/1000 | Loss: 0.00031361
Iteration 591/1000 | Loss: 0.00010313
Iteration 592/1000 | Loss: 0.00005582
Iteration 593/1000 | Loss: 0.00008608
Iteration 594/1000 | Loss: 0.00004779
Iteration 595/1000 | Loss: 0.00004385
Iteration 596/1000 | Loss: 0.00004067
Iteration 597/1000 | Loss: 0.00024053
Iteration 598/1000 | Loss: 0.00027740
Iteration 599/1000 | Loss: 0.00034193
Iteration 600/1000 | Loss: 0.00024892
Iteration 601/1000 | Loss: 0.00018833
Iteration 602/1000 | Loss: 0.00010550
Iteration 603/1000 | Loss: 0.00018993
Iteration 604/1000 | Loss: 0.00007824
Iteration 605/1000 | Loss: 0.00004494
Iteration 606/1000 | Loss: 0.00027287
Iteration 607/1000 | Loss: 0.00019789
Iteration 608/1000 | Loss: 0.00010391
Iteration 609/1000 | Loss: 0.00005767
Iteration 610/1000 | Loss: 0.00008737
Iteration 611/1000 | Loss: 0.00022697
Iteration 612/1000 | Loss: 0.00037520
Iteration 613/1000 | Loss: 0.00042695
Iteration 614/1000 | Loss: 0.00026409
Iteration 615/1000 | Loss: 0.00016589
Iteration 616/1000 | Loss: 0.00007001
Iteration 617/1000 | Loss: 0.00004524
Iteration 618/1000 | Loss: 0.00016718
Iteration 619/1000 | Loss: 0.00020079
Iteration 620/1000 | Loss: 0.00006550
Iteration 621/1000 | Loss: 0.00006039
Iteration 622/1000 | Loss: 0.00004258
Iteration 623/1000 | Loss: 0.00017367
Iteration 624/1000 | Loss: 0.00017887
Iteration 625/1000 | Loss: 0.00020668
Iteration 626/1000 | Loss: 0.00005035
Iteration 627/1000 | Loss: 0.00004526
Iteration 628/1000 | Loss: 0.00004273
Iteration 629/1000 | Loss: 0.00004063
Iteration 630/1000 | Loss: 0.00003942
Iteration 631/1000 | Loss: 0.00003824
Iteration 632/1000 | Loss: 0.00025172
Iteration 633/1000 | Loss: 0.00019531
Iteration 634/1000 | Loss: 0.00017916
Iteration 635/1000 | Loss: 0.00017103
Iteration 636/1000 | Loss: 0.00004641
Iteration 637/1000 | Loss: 0.00026223
Iteration 638/1000 | Loss: 0.00026986
Iteration 639/1000 | Loss: 0.00004958
Iteration 640/1000 | Loss: 0.00021702
Iteration 641/1000 | Loss: 0.00010394
Iteration 642/1000 | Loss: 0.00030433
Iteration 643/1000 | Loss: 0.00005649
Iteration 644/1000 | Loss: 0.00004376
Iteration 645/1000 | Loss: 0.00003998
Iteration 646/1000 | Loss: 0.00003851
Iteration 647/1000 | Loss: 0.00003803
Iteration 648/1000 | Loss: 0.00003750
Iteration 649/1000 | Loss: 0.00003691
Iteration 650/1000 | Loss: 0.00003637
Iteration 651/1000 | Loss: 0.00022878
Iteration 652/1000 | Loss: 0.00054665
Iteration 653/1000 | Loss: 0.00102890
Iteration 654/1000 | Loss: 0.00020224
Iteration 655/1000 | Loss: 0.00011854
Iteration 656/1000 | Loss: 0.00006654
Iteration 657/1000 | Loss: 0.00005435
Iteration 658/1000 | Loss: 0.00004784
Iteration 659/1000 | Loss: 0.00004515
Iteration 660/1000 | Loss: 0.00004255
Iteration 661/1000 | Loss: 0.00003997
Iteration 662/1000 | Loss: 0.00030527
Iteration 663/1000 | Loss: 0.00017040
Iteration 664/1000 | Loss: 0.00025312
Iteration 665/1000 | Loss: 0.00013977
Iteration 666/1000 | Loss: 0.00029209
Iteration 667/1000 | Loss: 0.00011540
Iteration 668/1000 | Loss: 0.00019089
Iteration 669/1000 | Loss: 0.00011148
Iteration 670/1000 | Loss: 0.00016449
Iteration 671/1000 | Loss: 0.00010611
Iteration 672/1000 | Loss: 0.00023672
Iteration 673/1000 | Loss: 0.00018711
Iteration 674/1000 | Loss: 0.00018461
Iteration 675/1000 | Loss: 0.00019304
Iteration 676/1000 | Loss: 0.00016407
Iteration 677/1000 | Loss: 0.00018724
Iteration 678/1000 | Loss: 0.00045959
Iteration 679/1000 | Loss: 0.00029323
Iteration 680/1000 | Loss: 0.00005295
Iteration 681/1000 | Loss: 0.00010735
Iteration 682/1000 | Loss: 0.00011268
Iteration 683/1000 | Loss: 0.00010603
Iteration 684/1000 | Loss: 0.00011544
Iteration 685/1000 | Loss: 0.00008829
Iteration 686/1000 | Loss: 0.00003849
Iteration 687/1000 | Loss: 0.00003679
Iteration 688/1000 | Loss: 0.00003568
Iteration 689/1000 | Loss: 0.00003484
Iteration 690/1000 | Loss: 0.00017135
Iteration 691/1000 | Loss: 0.00015333
Iteration 692/1000 | Loss: 0.00015649
Iteration 693/1000 | Loss: 0.00013565
Iteration 694/1000 | Loss: 0.00014993
Iteration 695/1000 | Loss: 0.00014013
Iteration 696/1000 | Loss: 0.00012495
Iteration 697/1000 | Loss: 0.00011787
Iteration 698/1000 | Loss: 0.00012304
Iteration 699/1000 | Loss: 0.00004429
Iteration 700/1000 | Loss: 0.00011195
Iteration 701/1000 | Loss: 0.00006750
Iteration 702/1000 | Loss: 0.00004293
Iteration 703/1000 | Loss: 0.00008158
Iteration 704/1000 | Loss: 0.00005158
Iteration 705/1000 | Loss: 0.00006919
Iteration 706/1000 | Loss: 0.00005982
Iteration 707/1000 | Loss: 0.00005646
Iteration 708/1000 | Loss: 0.00005076
Iteration 709/1000 | Loss: 0.00005322
Iteration 710/1000 | Loss: 0.00005192
Iteration 711/1000 | Loss: 0.00004700
Iteration 712/1000 | Loss: 0.00004059
Iteration 713/1000 | Loss: 0.00004747
Iteration 714/1000 | Loss: 0.00004151
Iteration 715/1000 | Loss: 0.00004751
Iteration 716/1000 | Loss: 0.00004331
Iteration 717/1000 | Loss: 0.00005253
Iteration 718/1000 | Loss: 0.00004747
Iteration 719/1000 | Loss: 0.00005755
Iteration 720/1000 | Loss: 0.00004738
Iteration 721/1000 | Loss: 0.00016032
Iteration 722/1000 | Loss: 0.00012173
Iteration 723/1000 | Loss: 0.00008584
Iteration 724/1000 | Loss: 0.00007263
Iteration 725/1000 | Loss: 0.00005402
Iteration 726/1000 | Loss: 0.00003356
Iteration 727/1000 | Loss: 0.00003270
Iteration 728/1000 | Loss: 0.00003210
Iteration 729/1000 | Loss: 0.00003173
Iteration 730/1000 | Loss: 0.00014427
Iteration 731/1000 | Loss: 0.00005398
Iteration 732/1000 | Loss: 0.00005514
Iteration 733/1000 | Loss: 0.00016176
Iteration 734/1000 | Loss: 0.00013650
Iteration 735/1000 | Loss: 0.00003478
Iteration 736/1000 | Loss: 0.00003355
Iteration 737/1000 | Loss: 0.00003273
Iteration 738/1000 | Loss: 0.00003228
Iteration 739/1000 | Loss: 0.00016513
Iteration 740/1000 | Loss: 0.00003529
Iteration 741/1000 | Loss: 0.00016778
Iteration 742/1000 | Loss: 0.00017031
Iteration 743/1000 | Loss: 0.00004452
Iteration 744/1000 | Loss: 0.00003840
Iteration 745/1000 | Loss: 0.00003592
Iteration 746/1000 | Loss: 0.00003483
Iteration 747/1000 | Loss: 0.00003408
Iteration 748/1000 | Loss: 0.00003338
Iteration 749/1000 | Loss: 0.00003255
Iteration 750/1000 | Loss: 0.00015762
Iteration 751/1000 | Loss: 0.00003639
Iteration 752/1000 | Loss: 0.00003444
Iteration 753/1000 | Loss: 0.00003298
Iteration 754/1000 | Loss: 0.00003265
Iteration 755/1000 | Loss: 0.00003244
Iteration 756/1000 | Loss: 0.00003204
Iteration 757/1000 | Loss: 0.00003167
Iteration 758/1000 | Loss: 0.00119991
Iteration 759/1000 | Loss: 0.00020153
Iteration 760/1000 | Loss: 0.00015370
Iteration 761/1000 | Loss: 0.00012491
Iteration 762/1000 | Loss: 0.00051411
Iteration 763/1000 | Loss: 0.00012882
Iteration 764/1000 | Loss: 0.00013792
Iteration 765/1000 | Loss: 0.00019271
Iteration 766/1000 | Loss: 0.00022528
Iteration 767/1000 | Loss: 0.00005209
Iteration 768/1000 | Loss: 0.00022941
Iteration 769/1000 | Loss: 0.00011932
Iteration 770/1000 | Loss: 0.00004773
Iteration 771/1000 | Loss: 0.00010302
Iteration 772/1000 | Loss: 0.00018190
Iteration 773/1000 | Loss: 0.00021240
Iteration 774/1000 | Loss: 0.00005344
Iteration 775/1000 | Loss: 0.00035460
Iteration 776/1000 | Loss: 0.00005048
Iteration 777/1000 | Loss: 0.00004390
Iteration 778/1000 | Loss: 0.00004079
Iteration 779/1000 | Loss: 0.00003883
Iteration 780/1000 | Loss: 0.00016428
Iteration 781/1000 | Loss: 0.00004223
Iteration 782/1000 | Loss: 0.00003948
Iteration 783/1000 | Loss: 0.00004034
Iteration 784/1000 | Loss: 0.00005461
Iteration 785/1000 | Loss: 0.00003775
Iteration 786/1000 | Loss: 0.00005121
Iteration 787/1000 | Loss: 0.00004777
Iteration 788/1000 | Loss: 0.00009940
Iteration 789/1000 | Loss: 0.00015716
Iteration 790/1000 | Loss: 0.00004153
Iteration 791/1000 | Loss: 0.00003908
Iteration 792/1000 | Loss: 0.00036027
Iteration 793/1000 | Loss: 0.00041949
Iteration 794/1000 | Loss: 0.00034183
Iteration 795/1000 | Loss: 0.00066163
Iteration 796/1000 | Loss: 0.00031863
Iteration 797/1000 | Loss: 0.00019430
Iteration 798/1000 | Loss: 0.00012770
Iteration 799/1000 | Loss: 0.00012216
Iteration 800/1000 | Loss: 0.00004236
Iteration 801/1000 | Loss: 0.00003863
Iteration 802/1000 | Loss: 0.00018027
Iteration 803/1000 | Loss: 0.00018373
Iteration 804/1000 | Loss: 0.00004259
Iteration 805/1000 | Loss: 0.00003917
Iteration 806/1000 | Loss: 0.00003687
Iteration 807/1000 | Loss: 0.00016984
Iteration 808/1000 | Loss: 0.00004051
Iteration 809/1000 | Loss: 0.00027914
Iteration 810/1000 | Loss: 0.00004474
Iteration 811/1000 | Loss: 0.00004045
Iteration 812/1000 | Loss: 0.00003770
Iteration 813/1000 | Loss: 0.00015948
Iteration 814/1000 | Loss: 0.00021225
Iteration 815/1000 | Loss: 0.00009035
Iteration 816/1000 | Loss: 0.00004018
Iteration 817/1000 | Loss: 0.00004286
Iteration 818/1000 | Loss: 0.00003474
Iteration 819/1000 | Loss: 0.00003352
Iteration 820/1000 | Loss: 0.00003228
Iteration 821/1000 | Loss: 0.00003168
Iteration 822/1000 | Loss: 0.00017224
Iteration 823/1000 | Loss: 0.00003765
Iteration 824/1000 | Loss: 0.00003314
Iteration 825/1000 | Loss: 0.00003158
Iteration 826/1000 | Loss: 0.00014948
Iteration 827/1000 | Loss: 0.00003250
Iteration 828/1000 | Loss: 0.00003120
Iteration 829/1000 | Loss: 0.00003075
Iteration 830/1000 | Loss: 0.00003033
Iteration 831/1000 | Loss: 0.00003003
Iteration 832/1000 | Loss: 0.00002984
Iteration 833/1000 | Loss: 0.00002967
Iteration 834/1000 | Loss: 0.00002960
Iteration 835/1000 | Loss: 0.00002960
Iteration 836/1000 | Loss: 0.00002953
Iteration 837/1000 | Loss: 0.00002953
Iteration 838/1000 | Loss: 0.00002952
Iteration 839/1000 | Loss: 0.00002952
Iteration 840/1000 | Loss: 0.00002952
Iteration 841/1000 | Loss: 0.00002951
Iteration 842/1000 | Loss: 0.00025755
Iteration 843/1000 | Loss: 0.00025755
Iteration 844/1000 | Loss: 0.00024124
Iteration 845/1000 | Loss: 0.00004034
Iteration 846/1000 | Loss: 0.00017228
Iteration 847/1000 | Loss: 0.00010500
Iteration 848/1000 | Loss: 0.00003510
Iteration 849/1000 | Loss: 0.00003334
Iteration 850/1000 | Loss: 0.00026539
Iteration 851/1000 | Loss: 0.00003782
Iteration 852/1000 | Loss: 0.00016596
Iteration 853/1000 | Loss: 0.00003770
Iteration 854/1000 | Loss: 0.00003496
Iteration 855/1000 | Loss: 0.00003346
Iteration 856/1000 | Loss: 0.00018580
Iteration 857/1000 | Loss: 0.00012611
Iteration 858/1000 | Loss: 0.00030208
Iteration 859/1000 | Loss: 0.00016589
Iteration 860/1000 | Loss: 0.00007942
Iteration 861/1000 | Loss: 0.00003310
Iteration 862/1000 | Loss: 0.00017656
Iteration 863/1000 | Loss: 0.00003611
Iteration 864/1000 | Loss: 0.00120329
Iteration 865/1000 | Loss: 0.00007250
Iteration 866/1000 | Loss: 0.00005590
Iteration 867/1000 | Loss: 0.00004560
Iteration 868/1000 | Loss: 0.00004333
Iteration 869/1000 | Loss: 0.00019473
Iteration 870/1000 | Loss: 0.00008067
Iteration 871/1000 | Loss: 0.00009011
Iteration 872/1000 | Loss: 0.00003845
Iteration 873/1000 | Loss: 0.00003567
Iteration 874/1000 | Loss: 0.00003337
Iteration 875/1000 | Loss: 0.00003232
Iteration 876/1000 | Loss: 0.00003169
Iteration 877/1000 | Loss: 0.00003124
Iteration 878/1000 | Loss: 0.00003082
Iteration 879/1000 | Loss: 0.00003054
Iteration 880/1000 | Loss: 0.00015831
Iteration 881/1000 | Loss: 0.00004010
Iteration 882/1000 | Loss: 0.00003432
Iteration 883/1000 | Loss: 0.00003256
Iteration 884/1000 | Loss: 0.00003135
Iteration 885/1000 | Loss: 0.00003045
Iteration 886/1000 | Loss: 0.00002992
Iteration 887/1000 | Loss: 0.00002948
Iteration 888/1000 | Loss: 0.00002906
Iteration 889/1000 | Loss: 0.00002894
Iteration 890/1000 | Loss: 0.00002881
Iteration 891/1000 | Loss: 0.00002879
Iteration 892/1000 | Loss: 0.00002877
Iteration 893/1000 | Loss: 0.00002877
Iteration 894/1000 | Loss: 0.00002876
Iteration 895/1000 | Loss: 0.00002876
Iteration 896/1000 | Loss: 0.00002876
Iteration 897/1000 | Loss: 0.00002876
Iteration 898/1000 | Loss: 0.00002876
Iteration 899/1000 | Loss: 0.00002876
Iteration 900/1000 | Loss: 0.00002876
Iteration 901/1000 | Loss: 0.00002876
Iteration 902/1000 | Loss: 0.00002876
Iteration 903/1000 | Loss: 0.00002875
Iteration 904/1000 | Loss: 0.00002872
Iteration 905/1000 | Loss: 0.00002869
Iteration 906/1000 | Loss: 0.00002869
Iteration 907/1000 | Loss: 0.00002869
Iteration 908/1000 | Loss: 0.00002869
Iteration 909/1000 | Loss: 0.00002869
Iteration 910/1000 | Loss: 0.00002869
Iteration 911/1000 | Loss: 0.00002869
Iteration 912/1000 | Loss: 0.00002869
Iteration 913/1000 | Loss: 0.00002868
Iteration 914/1000 | Loss: 0.00002868
Iteration 915/1000 | Loss: 0.00002868
Iteration 916/1000 | Loss: 0.00002867
Iteration 917/1000 | Loss: 0.00002865
Iteration 918/1000 | Loss: 0.00002865
Iteration 919/1000 | Loss: 0.00002865
Iteration 920/1000 | Loss: 0.00002865
Iteration 921/1000 | Loss: 0.00002865
Iteration 922/1000 | Loss: 0.00002865
Iteration 923/1000 | Loss: 0.00002865
Iteration 924/1000 | Loss: 0.00002865
Iteration 925/1000 | Loss: 0.00002865
Iteration 926/1000 | Loss: 0.00002865
Iteration 927/1000 | Loss: 0.00002864
Iteration 928/1000 | Loss: 0.00002864
Iteration 929/1000 | Loss: 0.00002862
Iteration 930/1000 | Loss: 0.00002861
Iteration 931/1000 | Loss: 0.00002861
Iteration 932/1000 | Loss: 0.00002860
Iteration 933/1000 | Loss: 0.00002860
Iteration 934/1000 | Loss: 0.00002860
Iteration 935/1000 | Loss: 0.00002860
Iteration 936/1000 | Loss: 0.00002860
Iteration 937/1000 | Loss: 0.00002859
Iteration 938/1000 | Loss: 0.00002859
Iteration 939/1000 | Loss: 0.00002859
Iteration 940/1000 | Loss: 0.00002858
Iteration 941/1000 | Loss: 0.00002857
Iteration 942/1000 | Loss: 0.00002857
Iteration 943/1000 | Loss: 0.00002857
Iteration 944/1000 | Loss: 0.00002857
Iteration 945/1000 | Loss: 0.00002857
Iteration 946/1000 | Loss: 0.00002857
Iteration 947/1000 | Loss: 0.00002857
Iteration 948/1000 | Loss: 0.00002857
Iteration 949/1000 | Loss: 0.00002856
Iteration 950/1000 | Loss: 0.00002856
Iteration 951/1000 | Loss: 0.00002856
Iteration 952/1000 | Loss: 0.00002855
Iteration 953/1000 | Loss: 0.00002855
Iteration 954/1000 | Loss: 0.00002855
Iteration 955/1000 | Loss: 0.00002853
Iteration 956/1000 | Loss: 0.00002853
Iteration 957/1000 | Loss: 0.00002851
Iteration 958/1000 | Loss: 0.00002851
Iteration 959/1000 | Loss: 0.00002851
Iteration 960/1000 | Loss: 0.00002851
Iteration 961/1000 | Loss: 0.00002851
Iteration 962/1000 | Loss: 0.00002851
Iteration 963/1000 | Loss: 0.00002851
Iteration 964/1000 | Loss: 0.00002851
Iteration 965/1000 | Loss: 0.00002851
Iteration 966/1000 | Loss: 0.00002851
Iteration 967/1000 | Loss: 0.00002850
Iteration 968/1000 | Loss: 0.00002850
Iteration 969/1000 | Loss: 0.00002849
Iteration 970/1000 | Loss: 0.00002849
Iteration 971/1000 | Loss: 0.00002849
Iteration 972/1000 | Loss: 0.00002849
Iteration 973/1000 | Loss: 0.00002848
Iteration 974/1000 | Loss: 0.00002848
Iteration 975/1000 | Loss: 0.00002848
Iteration 976/1000 | Loss: 0.00002848
Iteration 977/1000 | Loss: 0.00002848
Iteration 978/1000 | Loss: 0.00002848
Iteration 979/1000 | Loss: 0.00002847
Iteration 980/1000 | Loss: 0.00002847
Iteration 981/1000 | Loss: 0.00002847
Iteration 982/1000 | Loss: 0.00002846
Iteration 983/1000 | Loss: 0.00002846
Iteration 984/1000 | Loss: 0.00002846
Iteration 985/1000 | Loss: 0.00002845
Iteration 986/1000 | Loss: 0.00002845
Iteration 987/1000 | Loss: 0.00002845
Iteration 988/1000 | Loss: 0.00002845
Iteration 989/1000 | Loss: 0.00002845
Iteration 990/1000 | Loss: 0.00002845
Iteration 991/1000 | Loss: 0.00002845
Iteration 992/1000 | Loss: 0.00002844
Iteration 993/1000 | Loss: 0.00002844
Iteration 994/1000 | Loss: 0.00002844
Iteration 995/1000 | Loss: 0.00002844
Iteration 996/1000 | Loss: 0.00002843
Iteration 997/1000 | Loss: 0.00002843
Iteration 998/1000 | Loss: 0.00002843
Iteration 999/1000 | Loss: 0.00002843
Iteration 1000/1000 | Loss: 0.00002843

Optimization complete. Final v2v error: 4.331470489501953 mm

Highest mean error: 7.474381446838379 mm for frame 237

Lowest mean error: 3.957705020904541 mm for frame 67

Saving results

Total time: 1441.3494923114777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991154
Iteration 2/25 | Loss: 0.00447071
Iteration 3/25 | Loss: 0.00302525
Iteration 4/25 | Loss: 0.00281343
Iteration 5/25 | Loss: 0.00244899
Iteration 6/25 | Loss: 0.00195704
Iteration 7/25 | Loss: 0.00152775
Iteration 8/25 | Loss: 0.00139682
Iteration 9/25 | Loss: 0.00126850
Iteration 10/25 | Loss: 0.00122513
Iteration 11/25 | Loss: 0.00118236
Iteration 12/25 | Loss: 0.00116508
Iteration 13/25 | Loss: 0.00116930
Iteration 14/25 | Loss: 0.00114221
Iteration 15/25 | Loss: 0.00113617
Iteration 16/25 | Loss: 0.00113368
Iteration 17/25 | Loss: 0.00113792
Iteration 18/25 | Loss: 0.00112710
Iteration 19/25 | Loss: 0.00112556
Iteration 20/25 | Loss: 0.00112488
Iteration 21/25 | Loss: 0.00112443
Iteration 22/25 | Loss: 0.00112387
Iteration 23/25 | Loss: 0.00112333
Iteration 24/25 | Loss: 0.00112257
Iteration 25/25 | Loss: 0.00112926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14124894
Iteration 2/25 | Loss: 0.00523657
Iteration 3/25 | Loss: 0.00395719
Iteration 4/25 | Loss: 0.00395566
Iteration 5/25 | Loss: 0.00395566
Iteration 6/25 | Loss: 0.00395566
Iteration 7/25 | Loss: 0.00395566
Iteration 8/25 | Loss: 0.00395566
Iteration 9/25 | Loss: 0.00395566
Iteration 10/25 | Loss: 0.00395566
Iteration 11/25 | Loss: 0.00395565
Iteration 12/25 | Loss: 0.00395565
Iteration 13/25 | Loss: 0.00395566
Iteration 14/25 | Loss: 0.00395565
Iteration 15/25 | Loss: 0.00395566
Iteration 16/25 | Loss: 0.00395565
Iteration 17/25 | Loss: 0.00395565
Iteration 18/25 | Loss: 0.00395565
Iteration 19/25 | Loss: 0.00395565
Iteration 20/25 | Loss: 0.00395565
Iteration 21/25 | Loss: 0.00395565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003955654799938202, 0.003955654799938202, 0.003955654799938202, 0.003955654799938202, 0.003955654799938202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003955654799938202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395565
Iteration 2/1000 | Loss: 0.00422265
Iteration 3/1000 | Loss: 0.00307931
Iteration 4/1000 | Loss: 0.00237805
Iteration 5/1000 | Loss: 0.00087048
Iteration 6/1000 | Loss: 0.00086128
Iteration 7/1000 | Loss: 0.00052728
Iteration 8/1000 | Loss: 0.00085533
Iteration 9/1000 | Loss: 0.00150669
Iteration 10/1000 | Loss: 0.00056380
Iteration 11/1000 | Loss: 0.00059632
Iteration 12/1000 | Loss: 0.00028779
Iteration 13/1000 | Loss: 0.00017879
Iteration 14/1000 | Loss: 0.00019827
Iteration 15/1000 | Loss: 0.00089077
Iteration 16/1000 | Loss: 0.00157919
Iteration 17/1000 | Loss: 0.00342280
Iteration 18/1000 | Loss: 0.00121025
Iteration 19/1000 | Loss: 0.00114716
Iteration 20/1000 | Loss: 0.00049147
Iteration 21/1000 | Loss: 0.00065808
Iteration 22/1000 | Loss: 0.00036025
Iteration 23/1000 | Loss: 0.00014204
Iteration 24/1000 | Loss: 0.00039221
Iteration 25/1000 | Loss: 0.00043479
Iteration 26/1000 | Loss: 0.00026500
Iteration 27/1000 | Loss: 0.00025491
Iteration 28/1000 | Loss: 0.00015624
Iteration 29/1000 | Loss: 0.00014615
Iteration 30/1000 | Loss: 0.00013210
Iteration 31/1000 | Loss: 0.00070733
Iteration 32/1000 | Loss: 0.00022061
Iteration 33/1000 | Loss: 0.00037166
Iteration 34/1000 | Loss: 0.00138105
Iteration 35/1000 | Loss: 0.00306190
Iteration 36/1000 | Loss: 0.00207710
Iteration 37/1000 | Loss: 0.00297845
Iteration 38/1000 | Loss: 0.00051572
Iteration 39/1000 | Loss: 0.00014022
Iteration 40/1000 | Loss: 0.00050776
Iteration 41/1000 | Loss: 0.00170704
Iteration 42/1000 | Loss: 0.00128835
Iteration 43/1000 | Loss: 0.00144215
Iteration 44/1000 | Loss: 0.00180584
Iteration 45/1000 | Loss: 0.00086648
Iteration 46/1000 | Loss: 0.00048475
Iteration 47/1000 | Loss: 0.00008436
Iteration 48/1000 | Loss: 0.00024727
Iteration 49/1000 | Loss: 0.00006142
Iteration 50/1000 | Loss: 0.00092192
Iteration 51/1000 | Loss: 0.00026454
Iteration 52/1000 | Loss: 0.00013702
Iteration 53/1000 | Loss: 0.00033437
Iteration 54/1000 | Loss: 0.00104264
Iteration 55/1000 | Loss: 0.00129295
Iteration 56/1000 | Loss: 0.00013249
Iteration 57/1000 | Loss: 0.00048235
Iteration 58/1000 | Loss: 0.00125918
Iteration 59/1000 | Loss: 0.00047922
Iteration 60/1000 | Loss: 0.00082435
Iteration 61/1000 | Loss: 0.00070796
Iteration 62/1000 | Loss: 0.00011436
Iteration 63/1000 | Loss: 0.00018110
Iteration 64/1000 | Loss: 0.00070020
Iteration 65/1000 | Loss: 0.00022957
Iteration 66/1000 | Loss: 0.00028500
Iteration 67/1000 | Loss: 0.00021862
Iteration 68/1000 | Loss: 0.00031185
Iteration 69/1000 | Loss: 0.00018508
Iteration 70/1000 | Loss: 0.00027981
Iteration 71/1000 | Loss: 0.00007973
Iteration 72/1000 | Loss: 0.00041598
Iteration 73/1000 | Loss: 0.00015708
Iteration 74/1000 | Loss: 0.00006159
Iteration 75/1000 | Loss: 0.00004227
Iteration 76/1000 | Loss: 0.00003932
Iteration 77/1000 | Loss: 0.00005020
Iteration 78/1000 | Loss: 0.00004917
Iteration 79/1000 | Loss: 0.00003609
Iteration 80/1000 | Loss: 0.00004241
Iteration 81/1000 | Loss: 0.00004172
Iteration 82/1000 | Loss: 0.00004670
Iteration 83/1000 | Loss: 0.00003924
Iteration 84/1000 | Loss: 0.00003634
Iteration 85/1000 | Loss: 0.00003506
Iteration 86/1000 | Loss: 0.00003431
Iteration 87/1000 | Loss: 0.00003384
Iteration 88/1000 | Loss: 0.00003355
Iteration 89/1000 | Loss: 0.00003336
Iteration 90/1000 | Loss: 0.00038321
Iteration 91/1000 | Loss: 0.00039597
Iteration 92/1000 | Loss: 0.00020473
Iteration 93/1000 | Loss: 0.00038604
Iteration 94/1000 | Loss: 0.00020587
Iteration 95/1000 | Loss: 0.00063675
Iteration 96/1000 | Loss: 0.00038794
Iteration 97/1000 | Loss: 0.00024856
Iteration 98/1000 | Loss: 0.00005972
Iteration 99/1000 | Loss: 0.00014775
Iteration 100/1000 | Loss: 0.00007681
Iteration 101/1000 | Loss: 0.00004001
Iteration 102/1000 | Loss: 0.00025360
Iteration 103/1000 | Loss: 0.00003116
Iteration 104/1000 | Loss: 0.00002792
Iteration 105/1000 | Loss: 0.00024010
Iteration 106/1000 | Loss: 0.00005766
Iteration 107/1000 | Loss: 0.00005300
Iteration 108/1000 | Loss: 0.00002550
Iteration 109/1000 | Loss: 0.00002471
Iteration 110/1000 | Loss: 0.00002414
Iteration 111/1000 | Loss: 0.00002393
Iteration 112/1000 | Loss: 0.00002377
Iteration 113/1000 | Loss: 0.00002359
Iteration 114/1000 | Loss: 0.00002353
Iteration 115/1000 | Loss: 0.00002350
Iteration 116/1000 | Loss: 0.00002343
Iteration 117/1000 | Loss: 0.00002341
Iteration 118/1000 | Loss: 0.00002341
Iteration 119/1000 | Loss: 0.00002338
Iteration 120/1000 | Loss: 0.00002338
Iteration 121/1000 | Loss: 0.00002332
Iteration 122/1000 | Loss: 0.00002331
Iteration 123/1000 | Loss: 0.00002330
Iteration 124/1000 | Loss: 0.00002330
Iteration 125/1000 | Loss: 0.00002329
Iteration 126/1000 | Loss: 0.00002329
Iteration 127/1000 | Loss: 0.00002328
Iteration 128/1000 | Loss: 0.00002328
Iteration 129/1000 | Loss: 0.00002328
Iteration 130/1000 | Loss: 0.00002327
Iteration 131/1000 | Loss: 0.00002327
Iteration 132/1000 | Loss: 0.00002327
Iteration 133/1000 | Loss: 0.00002327
Iteration 134/1000 | Loss: 0.00002327
Iteration 135/1000 | Loss: 0.00002327
Iteration 136/1000 | Loss: 0.00002327
Iteration 137/1000 | Loss: 0.00002327
Iteration 138/1000 | Loss: 0.00002327
Iteration 139/1000 | Loss: 0.00002327
Iteration 140/1000 | Loss: 0.00002326
Iteration 141/1000 | Loss: 0.00002326
Iteration 142/1000 | Loss: 0.00002326
Iteration 143/1000 | Loss: 0.00002326
Iteration 144/1000 | Loss: 0.00002326
Iteration 145/1000 | Loss: 0.00002325
Iteration 146/1000 | Loss: 0.00002325
Iteration 147/1000 | Loss: 0.00002325
Iteration 148/1000 | Loss: 0.00002324
Iteration 149/1000 | Loss: 0.00002324
Iteration 150/1000 | Loss: 0.00002324
Iteration 151/1000 | Loss: 0.00002324
Iteration 152/1000 | Loss: 0.00002324
Iteration 153/1000 | Loss: 0.00002324
Iteration 154/1000 | Loss: 0.00002324
Iteration 155/1000 | Loss: 0.00002324
Iteration 156/1000 | Loss: 0.00002324
Iteration 157/1000 | Loss: 0.00002323
Iteration 158/1000 | Loss: 0.00002323
Iteration 159/1000 | Loss: 0.00002323
Iteration 160/1000 | Loss: 0.00002323
Iteration 161/1000 | Loss: 0.00002323
Iteration 162/1000 | Loss: 0.00002323
Iteration 163/1000 | Loss: 0.00002323
Iteration 164/1000 | Loss: 0.00002323
Iteration 165/1000 | Loss: 0.00002323
Iteration 166/1000 | Loss: 0.00002323
Iteration 167/1000 | Loss: 0.00002323
Iteration 168/1000 | Loss: 0.00002323
Iteration 169/1000 | Loss: 0.00002323
Iteration 170/1000 | Loss: 0.00002323
Iteration 171/1000 | Loss: 0.00002323
Iteration 172/1000 | Loss: 0.00002323
Iteration 173/1000 | Loss: 0.00002323
Iteration 174/1000 | Loss: 0.00002323
Iteration 175/1000 | Loss: 0.00002323
Iteration 176/1000 | Loss: 0.00002323
Iteration 177/1000 | Loss: 0.00002323
Iteration 178/1000 | Loss: 0.00002323
Iteration 179/1000 | Loss: 0.00002323
Iteration 180/1000 | Loss: 0.00002323
Iteration 181/1000 | Loss: 0.00002323
Iteration 182/1000 | Loss: 0.00002323
Iteration 183/1000 | Loss: 0.00002323
Iteration 184/1000 | Loss: 0.00002323
Iteration 185/1000 | Loss: 0.00002323
Iteration 186/1000 | Loss: 0.00002323
Iteration 187/1000 | Loss: 0.00002323
Iteration 188/1000 | Loss: 0.00002323
Iteration 189/1000 | Loss: 0.00002323
Iteration 190/1000 | Loss: 0.00002323
Iteration 191/1000 | Loss: 0.00002323
Iteration 192/1000 | Loss: 0.00002323
Iteration 193/1000 | Loss: 0.00002323
Iteration 194/1000 | Loss: 0.00002323
Iteration 195/1000 | Loss: 0.00002323
Iteration 196/1000 | Loss: 0.00002323
Iteration 197/1000 | Loss: 0.00002323
Iteration 198/1000 | Loss: 0.00002323
Iteration 199/1000 | Loss: 0.00002323
Iteration 200/1000 | Loss: 0.00002323
Iteration 201/1000 | Loss: 0.00002323
Iteration 202/1000 | Loss: 0.00002323
Iteration 203/1000 | Loss: 0.00002323
Iteration 204/1000 | Loss: 0.00002323
Iteration 205/1000 | Loss: 0.00002323
Iteration 206/1000 | Loss: 0.00002323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.3226368284667842e-05, 2.3226368284667842e-05, 2.3226368284667842e-05, 2.3226368284667842e-05, 2.3226368284667842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3226368284667842e-05

Optimization complete. Final v2v error: 4.153235912322998 mm

Highest mean error: 4.463318824768066 mm for frame 4

Lowest mean error: 3.802130937576294 mm for frame 131

Saving results

Total time: 201.98903131484985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843943
Iteration 2/25 | Loss: 0.00119363
Iteration 3/25 | Loss: 0.00089202
Iteration 4/25 | Loss: 0.00084462
Iteration 5/25 | Loss: 0.00083846
Iteration 6/25 | Loss: 0.00083619
Iteration 7/25 | Loss: 0.00083618
Iteration 8/25 | Loss: 0.00083618
Iteration 9/25 | Loss: 0.00083618
Iteration 10/25 | Loss: 0.00083618
Iteration 11/25 | Loss: 0.00083618
Iteration 12/25 | Loss: 0.00083618
Iteration 13/25 | Loss: 0.00083618
Iteration 14/25 | Loss: 0.00083618
Iteration 15/25 | Loss: 0.00083618
Iteration 16/25 | Loss: 0.00083618
Iteration 17/25 | Loss: 0.00083618
Iteration 18/25 | Loss: 0.00083618
Iteration 19/25 | Loss: 0.00083618
Iteration 20/25 | Loss: 0.00083618
Iteration 21/25 | Loss: 0.00083618
Iteration 22/25 | Loss: 0.00083618
Iteration 23/25 | Loss: 0.00083618
Iteration 24/25 | Loss: 0.00083618
Iteration 25/25 | Loss: 0.00083618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06051433
Iteration 2/25 | Loss: 0.00160299
Iteration 3/25 | Loss: 0.00160298
Iteration 4/25 | Loss: 0.00160298
Iteration 5/25 | Loss: 0.00160298
Iteration 6/25 | Loss: 0.00160298
Iteration 7/25 | Loss: 0.00160298
Iteration 8/25 | Loss: 0.00160298
Iteration 9/25 | Loss: 0.00160298
Iteration 10/25 | Loss: 0.00160298
Iteration 11/25 | Loss: 0.00160298
Iteration 12/25 | Loss: 0.00160298
Iteration 13/25 | Loss: 0.00160298
Iteration 14/25 | Loss: 0.00160298
Iteration 15/25 | Loss: 0.00160298
Iteration 16/25 | Loss: 0.00160298
Iteration 17/25 | Loss: 0.00160298
Iteration 18/25 | Loss: 0.00160298
Iteration 19/25 | Loss: 0.00160298
Iteration 20/25 | Loss: 0.00160298
Iteration 21/25 | Loss: 0.00160298
Iteration 22/25 | Loss: 0.00160298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016029791440814734, 0.0016029791440814734, 0.0016029791440814734, 0.0016029791440814734, 0.0016029791440814734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016029791440814734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160298
Iteration 2/1000 | Loss: 0.00003616
Iteration 3/1000 | Loss: 0.00002718
Iteration 4/1000 | Loss: 0.00002539
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002350
Iteration 7/1000 | Loss: 0.00002285
Iteration 8/1000 | Loss: 0.00002247
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002196
Iteration 11/1000 | Loss: 0.00002173
Iteration 12/1000 | Loss: 0.00002168
Iteration 13/1000 | Loss: 0.00002167
Iteration 14/1000 | Loss: 0.00002167
Iteration 15/1000 | Loss: 0.00002166
Iteration 16/1000 | Loss: 0.00002166
Iteration 17/1000 | Loss: 0.00002148
Iteration 18/1000 | Loss: 0.00002135
Iteration 19/1000 | Loss: 0.00002130
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002125
Iteration 22/1000 | Loss: 0.00002124
Iteration 23/1000 | Loss: 0.00002124
Iteration 24/1000 | Loss: 0.00002122
Iteration 25/1000 | Loss: 0.00002121
Iteration 26/1000 | Loss: 0.00002121
Iteration 27/1000 | Loss: 0.00002118
Iteration 28/1000 | Loss: 0.00002118
Iteration 29/1000 | Loss: 0.00002118
Iteration 30/1000 | Loss: 0.00002117
Iteration 31/1000 | Loss: 0.00002117
Iteration 32/1000 | Loss: 0.00002110
Iteration 33/1000 | Loss: 0.00002110
Iteration 34/1000 | Loss: 0.00002106
Iteration 35/1000 | Loss: 0.00002105
Iteration 36/1000 | Loss: 0.00002105
Iteration 37/1000 | Loss: 0.00002105
Iteration 38/1000 | Loss: 0.00002105
Iteration 39/1000 | Loss: 0.00002105
Iteration 40/1000 | Loss: 0.00002105
Iteration 41/1000 | Loss: 0.00002105
Iteration 42/1000 | Loss: 0.00002105
Iteration 43/1000 | Loss: 0.00002105
Iteration 44/1000 | Loss: 0.00002103
Iteration 45/1000 | Loss: 0.00002103
Iteration 46/1000 | Loss: 0.00002103
Iteration 47/1000 | Loss: 0.00002103
Iteration 48/1000 | Loss: 0.00002103
Iteration 49/1000 | Loss: 0.00002103
Iteration 50/1000 | Loss: 0.00002103
Iteration 51/1000 | Loss: 0.00002103
Iteration 52/1000 | Loss: 0.00002102
Iteration 53/1000 | Loss: 0.00002102
Iteration 54/1000 | Loss: 0.00002102
Iteration 55/1000 | Loss: 0.00002102
Iteration 56/1000 | Loss: 0.00002102
Iteration 57/1000 | Loss: 0.00002102
Iteration 58/1000 | Loss: 0.00002102
Iteration 59/1000 | Loss: 0.00002102
Iteration 60/1000 | Loss: 0.00002102
Iteration 61/1000 | Loss: 0.00002102
Iteration 62/1000 | Loss: 0.00002102
Iteration 63/1000 | Loss: 0.00002102
Iteration 64/1000 | Loss: 0.00002102
Iteration 65/1000 | Loss: 0.00002102
Iteration 66/1000 | Loss: 0.00002102
Iteration 67/1000 | Loss: 0.00002101
Iteration 68/1000 | Loss: 0.00002101
Iteration 69/1000 | Loss: 0.00002101
Iteration 70/1000 | Loss: 0.00002100
Iteration 71/1000 | Loss: 0.00002100
Iteration 72/1000 | Loss: 0.00002100
Iteration 73/1000 | Loss: 0.00002099
Iteration 74/1000 | Loss: 0.00002099
Iteration 75/1000 | Loss: 0.00002099
Iteration 76/1000 | Loss: 0.00002098
Iteration 77/1000 | Loss: 0.00002098
Iteration 78/1000 | Loss: 0.00002098
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002096
Iteration 83/1000 | Loss: 0.00002096
Iteration 84/1000 | Loss: 0.00002096
Iteration 85/1000 | Loss: 0.00002096
Iteration 86/1000 | Loss: 0.00002095
Iteration 87/1000 | Loss: 0.00002095
Iteration 88/1000 | Loss: 0.00002095
Iteration 89/1000 | Loss: 0.00002095
Iteration 90/1000 | Loss: 0.00002095
Iteration 91/1000 | Loss: 0.00002095
Iteration 92/1000 | Loss: 0.00002095
Iteration 93/1000 | Loss: 0.00002095
Iteration 94/1000 | Loss: 0.00002095
Iteration 95/1000 | Loss: 0.00002095
Iteration 96/1000 | Loss: 0.00002095
Iteration 97/1000 | Loss: 0.00002095
Iteration 98/1000 | Loss: 0.00002095
Iteration 99/1000 | Loss: 0.00002095
Iteration 100/1000 | Loss: 0.00002095
Iteration 101/1000 | Loss: 0.00002095
Iteration 102/1000 | Loss: 0.00002095
Iteration 103/1000 | Loss: 0.00002095
Iteration 104/1000 | Loss: 0.00002095
Iteration 105/1000 | Loss: 0.00002095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.0953439161530696e-05, 2.0953439161530696e-05, 2.0953439161530696e-05, 2.0953439161530696e-05, 2.0953439161530696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0953439161530696e-05

Optimization complete. Final v2v error: 3.9589293003082275 mm

Highest mean error: 4.261192321777344 mm for frame 51

Lowest mean error: 3.6275970935821533 mm for frame 107

Saving results

Total time: 40.9221031665802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473113
Iteration 2/25 | Loss: 0.00090370
Iteration 3/25 | Loss: 0.00080172
Iteration 4/25 | Loss: 0.00078658
Iteration 5/25 | Loss: 0.00078041
Iteration 6/25 | Loss: 0.00077914
Iteration 7/25 | Loss: 0.00077909
Iteration 8/25 | Loss: 0.00077909
Iteration 9/25 | Loss: 0.00077909
Iteration 10/25 | Loss: 0.00077909
Iteration 11/25 | Loss: 0.00077909
Iteration 12/25 | Loss: 0.00077909
Iteration 13/25 | Loss: 0.00077909
Iteration 14/25 | Loss: 0.00077909
Iteration 15/25 | Loss: 0.00077909
Iteration 16/25 | Loss: 0.00077909
Iteration 17/25 | Loss: 0.00077909
Iteration 18/25 | Loss: 0.00077909
Iteration 19/25 | Loss: 0.00077909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007790850359015167, 0.0007790850359015167, 0.0007790850359015167, 0.0007790850359015167, 0.0007790850359015167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007790850359015167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46798921
Iteration 2/25 | Loss: 0.00151352
Iteration 3/25 | Loss: 0.00151351
Iteration 4/25 | Loss: 0.00151351
Iteration 5/25 | Loss: 0.00151351
Iteration 6/25 | Loss: 0.00151351
Iteration 7/25 | Loss: 0.00151351
Iteration 8/25 | Loss: 0.00151351
Iteration 9/25 | Loss: 0.00151351
Iteration 10/25 | Loss: 0.00151351
Iteration 11/25 | Loss: 0.00151351
Iteration 12/25 | Loss: 0.00151351
Iteration 13/25 | Loss: 0.00151351
Iteration 14/25 | Loss: 0.00151351
Iteration 15/25 | Loss: 0.00151351
Iteration 16/25 | Loss: 0.00151351
Iteration 17/25 | Loss: 0.00151351
Iteration 18/25 | Loss: 0.00151351
Iteration 19/25 | Loss: 0.00151351
Iteration 20/25 | Loss: 0.00151351
Iteration 21/25 | Loss: 0.00151351
Iteration 22/25 | Loss: 0.00151351
Iteration 23/25 | Loss: 0.00151351
Iteration 24/25 | Loss: 0.00151351
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015135095454752445, 0.0015135095454752445, 0.0015135095454752445, 0.0015135095454752445, 0.0015135095454752445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015135095454752445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151351
Iteration 2/1000 | Loss: 0.00003649
Iteration 3/1000 | Loss: 0.00002708
Iteration 4/1000 | Loss: 0.00002513
Iteration 5/1000 | Loss: 0.00002354
Iteration 6/1000 | Loss: 0.00002271
Iteration 7/1000 | Loss: 0.00002212
Iteration 8/1000 | Loss: 0.00002184
Iteration 9/1000 | Loss: 0.00002152
Iteration 10/1000 | Loss: 0.00002125
Iteration 11/1000 | Loss: 0.00002100
Iteration 12/1000 | Loss: 0.00002076
Iteration 13/1000 | Loss: 0.00002075
Iteration 14/1000 | Loss: 0.00002072
Iteration 15/1000 | Loss: 0.00002072
Iteration 16/1000 | Loss: 0.00002071
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00002057
Iteration 19/1000 | Loss: 0.00002053
Iteration 20/1000 | Loss: 0.00002052
Iteration 21/1000 | Loss: 0.00002051
Iteration 22/1000 | Loss: 0.00002049
Iteration 23/1000 | Loss: 0.00002048
Iteration 24/1000 | Loss: 0.00002048
Iteration 25/1000 | Loss: 0.00002047
Iteration 26/1000 | Loss: 0.00002046
Iteration 27/1000 | Loss: 0.00002041
Iteration 28/1000 | Loss: 0.00002037
Iteration 29/1000 | Loss: 0.00002037
Iteration 30/1000 | Loss: 0.00002036
Iteration 31/1000 | Loss: 0.00002035
Iteration 32/1000 | Loss: 0.00002035
Iteration 33/1000 | Loss: 0.00002034
Iteration 34/1000 | Loss: 0.00002034
Iteration 35/1000 | Loss: 0.00002034
Iteration 36/1000 | Loss: 0.00002033
Iteration 37/1000 | Loss: 0.00002033
Iteration 38/1000 | Loss: 0.00002032
Iteration 39/1000 | Loss: 0.00002032
Iteration 40/1000 | Loss: 0.00002031
Iteration 41/1000 | Loss: 0.00002031
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002030
Iteration 44/1000 | Loss: 0.00002030
Iteration 45/1000 | Loss: 0.00002027
Iteration 46/1000 | Loss: 0.00002027
Iteration 47/1000 | Loss: 0.00002027
Iteration 48/1000 | Loss: 0.00002027
Iteration 49/1000 | Loss: 0.00002027
Iteration 50/1000 | Loss: 0.00002027
Iteration 51/1000 | Loss: 0.00002027
Iteration 52/1000 | Loss: 0.00002027
Iteration 53/1000 | Loss: 0.00002027
Iteration 54/1000 | Loss: 0.00002027
Iteration 55/1000 | Loss: 0.00002027
Iteration 56/1000 | Loss: 0.00002027
Iteration 57/1000 | Loss: 0.00002026
Iteration 58/1000 | Loss: 0.00002026
Iteration 59/1000 | Loss: 0.00002026
Iteration 60/1000 | Loss: 0.00002026
Iteration 61/1000 | Loss: 0.00002026
Iteration 62/1000 | Loss: 0.00002025
Iteration 63/1000 | Loss: 0.00002024
Iteration 64/1000 | Loss: 0.00002024
Iteration 65/1000 | Loss: 0.00002024
Iteration 66/1000 | Loss: 0.00002023
Iteration 67/1000 | Loss: 0.00002023
Iteration 68/1000 | Loss: 0.00002023
Iteration 69/1000 | Loss: 0.00002023
Iteration 70/1000 | Loss: 0.00002023
Iteration 71/1000 | Loss: 0.00002023
Iteration 72/1000 | Loss: 0.00002023
Iteration 73/1000 | Loss: 0.00002022
Iteration 74/1000 | Loss: 0.00002022
Iteration 75/1000 | Loss: 0.00002022
Iteration 76/1000 | Loss: 0.00002022
Iteration 77/1000 | Loss: 0.00002021
Iteration 78/1000 | Loss: 0.00002021
Iteration 79/1000 | Loss: 0.00002020
Iteration 80/1000 | Loss: 0.00002020
Iteration 81/1000 | Loss: 0.00002020
Iteration 82/1000 | Loss: 0.00002020
Iteration 83/1000 | Loss: 0.00002020
Iteration 84/1000 | Loss: 0.00002020
Iteration 85/1000 | Loss: 0.00002020
Iteration 86/1000 | Loss: 0.00002020
Iteration 87/1000 | Loss: 0.00002020
Iteration 88/1000 | Loss: 0.00002019
Iteration 89/1000 | Loss: 0.00002019
Iteration 90/1000 | Loss: 0.00002019
Iteration 91/1000 | Loss: 0.00002019
Iteration 92/1000 | Loss: 0.00002019
Iteration 93/1000 | Loss: 0.00002019
Iteration 94/1000 | Loss: 0.00002019
Iteration 95/1000 | Loss: 0.00002019
Iteration 96/1000 | Loss: 0.00002019
Iteration 97/1000 | Loss: 0.00002019
Iteration 98/1000 | Loss: 0.00002019
Iteration 99/1000 | Loss: 0.00002018
Iteration 100/1000 | Loss: 0.00002018
Iteration 101/1000 | Loss: 0.00002018
Iteration 102/1000 | Loss: 0.00002018
Iteration 103/1000 | Loss: 0.00002018
Iteration 104/1000 | Loss: 0.00002017
Iteration 105/1000 | Loss: 0.00002017
Iteration 106/1000 | Loss: 0.00002017
Iteration 107/1000 | Loss: 0.00002017
Iteration 108/1000 | Loss: 0.00002017
Iteration 109/1000 | Loss: 0.00002017
Iteration 110/1000 | Loss: 0.00002017
Iteration 111/1000 | Loss: 0.00002017
Iteration 112/1000 | Loss: 0.00002017
Iteration 113/1000 | Loss: 0.00002017
Iteration 114/1000 | Loss: 0.00002017
Iteration 115/1000 | Loss: 0.00002016
Iteration 116/1000 | Loss: 0.00002016
Iteration 117/1000 | Loss: 0.00002016
Iteration 118/1000 | Loss: 0.00002016
Iteration 119/1000 | Loss: 0.00002016
Iteration 120/1000 | Loss: 0.00002016
Iteration 121/1000 | Loss: 0.00002015
Iteration 122/1000 | Loss: 0.00002015
Iteration 123/1000 | Loss: 0.00002015
Iteration 124/1000 | Loss: 0.00002015
Iteration 125/1000 | Loss: 0.00002015
Iteration 126/1000 | Loss: 0.00002015
Iteration 127/1000 | Loss: 0.00002015
Iteration 128/1000 | Loss: 0.00002015
Iteration 129/1000 | Loss: 0.00002015
Iteration 130/1000 | Loss: 0.00002015
Iteration 131/1000 | Loss: 0.00002015
Iteration 132/1000 | Loss: 0.00002015
Iteration 133/1000 | Loss: 0.00002014
Iteration 134/1000 | Loss: 0.00002014
Iteration 135/1000 | Loss: 0.00002014
Iteration 136/1000 | Loss: 0.00002014
Iteration 137/1000 | Loss: 0.00002014
Iteration 138/1000 | Loss: 0.00002014
Iteration 139/1000 | Loss: 0.00002014
Iteration 140/1000 | Loss: 0.00002014
Iteration 141/1000 | Loss: 0.00002014
Iteration 142/1000 | Loss: 0.00002014
Iteration 143/1000 | Loss: 0.00002014
Iteration 144/1000 | Loss: 0.00002014
Iteration 145/1000 | Loss: 0.00002014
Iteration 146/1000 | Loss: 0.00002014
Iteration 147/1000 | Loss: 0.00002013
Iteration 148/1000 | Loss: 0.00002013
Iteration 149/1000 | Loss: 0.00002013
Iteration 150/1000 | Loss: 0.00002013
Iteration 151/1000 | Loss: 0.00002013
Iteration 152/1000 | Loss: 0.00002013
Iteration 153/1000 | Loss: 0.00002013
Iteration 154/1000 | Loss: 0.00002013
Iteration 155/1000 | Loss: 0.00002013
Iteration 156/1000 | Loss: 0.00002013
Iteration 157/1000 | Loss: 0.00002013
Iteration 158/1000 | Loss: 0.00002013
Iteration 159/1000 | Loss: 0.00002013
Iteration 160/1000 | Loss: 0.00002013
Iteration 161/1000 | Loss: 0.00002013
Iteration 162/1000 | Loss: 0.00002012
Iteration 163/1000 | Loss: 0.00002012
Iteration 164/1000 | Loss: 0.00002012
Iteration 165/1000 | Loss: 0.00002012
Iteration 166/1000 | Loss: 0.00002012
Iteration 167/1000 | Loss: 0.00002012
Iteration 168/1000 | Loss: 0.00002012
Iteration 169/1000 | Loss: 0.00002012
Iteration 170/1000 | Loss: 0.00002012
Iteration 171/1000 | Loss: 0.00002012
Iteration 172/1000 | Loss: 0.00002012
Iteration 173/1000 | Loss: 0.00002012
Iteration 174/1000 | Loss: 0.00002012
Iteration 175/1000 | Loss: 0.00002012
Iteration 176/1000 | Loss: 0.00002012
Iteration 177/1000 | Loss: 0.00002012
Iteration 178/1000 | Loss: 0.00002011
Iteration 179/1000 | Loss: 0.00002011
Iteration 180/1000 | Loss: 0.00002011
Iteration 181/1000 | Loss: 0.00002011
Iteration 182/1000 | Loss: 0.00002011
Iteration 183/1000 | Loss: 0.00002011
Iteration 184/1000 | Loss: 0.00002011
Iteration 185/1000 | Loss: 0.00002011
Iteration 186/1000 | Loss: 0.00002011
Iteration 187/1000 | Loss: 0.00002011
Iteration 188/1000 | Loss: 0.00002011
Iteration 189/1000 | Loss: 0.00002011
Iteration 190/1000 | Loss: 0.00002011
Iteration 191/1000 | Loss: 0.00002011
Iteration 192/1000 | Loss: 0.00002011
Iteration 193/1000 | Loss: 0.00002010
Iteration 194/1000 | Loss: 0.00002010
Iteration 195/1000 | Loss: 0.00002010
Iteration 196/1000 | Loss: 0.00002010
Iteration 197/1000 | Loss: 0.00002010
Iteration 198/1000 | Loss: 0.00002010
Iteration 199/1000 | Loss: 0.00002010
Iteration 200/1000 | Loss: 0.00002010
Iteration 201/1000 | Loss: 0.00002010
Iteration 202/1000 | Loss: 0.00002010
Iteration 203/1000 | Loss: 0.00002010
Iteration 204/1000 | Loss: 0.00002010
Iteration 205/1000 | Loss: 0.00002010
Iteration 206/1000 | Loss: 0.00002010
Iteration 207/1000 | Loss: 0.00002010
Iteration 208/1000 | Loss: 0.00002010
Iteration 209/1000 | Loss: 0.00002010
Iteration 210/1000 | Loss: 0.00002010
Iteration 211/1000 | Loss: 0.00002010
Iteration 212/1000 | Loss: 0.00002010
Iteration 213/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.0102381313336082e-05, 2.0102381313336082e-05, 2.0102381313336082e-05, 2.0102381313336082e-05, 2.0102381313336082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0102381313336082e-05

Optimization complete. Final v2v error: 3.8878872394561768 mm

Highest mean error: 4.416790962219238 mm for frame 111

Lowest mean error: 3.217517137527466 mm for frame 128

Saving results

Total time: 40.527435541152954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714933
Iteration 2/25 | Loss: 0.00119259
Iteration 3/25 | Loss: 0.00092056
Iteration 4/25 | Loss: 0.00087803
Iteration 5/25 | Loss: 0.00086998
Iteration 6/25 | Loss: 0.00086806
Iteration 7/25 | Loss: 0.00086801
Iteration 8/25 | Loss: 0.00086801
Iteration 9/25 | Loss: 0.00086801
Iteration 10/25 | Loss: 0.00086801
Iteration 11/25 | Loss: 0.00086801
Iteration 12/25 | Loss: 0.00086801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008680086466483772, 0.0008680086466483772, 0.0008680086466483772, 0.0008680086466483772, 0.0008680086466483772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008680086466483772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12245536
Iteration 2/25 | Loss: 0.00173856
Iteration 3/25 | Loss: 0.00173854
Iteration 4/25 | Loss: 0.00173854
Iteration 5/25 | Loss: 0.00173854
Iteration 6/25 | Loss: 0.00173854
Iteration 7/25 | Loss: 0.00173854
Iteration 8/25 | Loss: 0.00173854
Iteration 9/25 | Loss: 0.00173854
Iteration 10/25 | Loss: 0.00173854
Iteration 11/25 | Loss: 0.00173854
Iteration 12/25 | Loss: 0.00173854
Iteration 13/25 | Loss: 0.00173854
Iteration 14/25 | Loss: 0.00173854
Iteration 15/25 | Loss: 0.00173854
Iteration 16/25 | Loss: 0.00173854
Iteration 17/25 | Loss: 0.00173854
Iteration 18/25 | Loss: 0.00173854
Iteration 19/25 | Loss: 0.00173854
Iteration 20/25 | Loss: 0.00173854
Iteration 21/25 | Loss: 0.00173854
Iteration 22/25 | Loss: 0.00173854
Iteration 23/25 | Loss: 0.00173854
Iteration 24/25 | Loss: 0.00173854
Iteration 25/25 | Loss: 0.00173854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173854
Iteration 2/1000 | Loss: 0.00003787
Iteration 3/1000 | Loss: 0.00002888
Iteration 4/1000 | Loss: 0.00002609
Iteration 5/1000 | Loss: 0.00002502
Iteration 6/1000 | Loss: 0.00002422
Iteration 7/1000 | Loss: 0.00002366
Iteration 8/1000 | Loss: 0.00002318
Iteration 9/1000 | Loss: 0.00002280
Iteration 10/1000 | Loss: 0.00002245
Iteration 11/1000 | Loss: 0.00002215
Iteration 12/1000 | Loss: 0.00002197
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002176
Iteration 15/1000 | Loss: 0.00002169
Iteration 16/1000 | Loss: 0.00002161
Iteration 17/1000 | Loss: 0.00002161
Iteration 18/1000 | Loss: 0.00002160
Iteration 19/1000 | Loss: 0.00002157
Iteration 20/1000 | Loss: 0.00002157
Iteration 21/1000 | Loss: 0.00002156
Iteration 22/1000 | Loss: 0.00002156
Iteration 23/1000 | Loss: 0.00002155
Iteration 24/1000 | Loss: 0.00002155
Iteration 25/1000 | Loss: 0.00002148
Iteration 26/1000 | Loss: 0.00002148
Iteration 27/1000 | Loss: 0.00002147
Iteration 28/1000 | Loss: 0.00002146
Iteration 29/1000 | Loss: 0.00002146
Iteration 30/1000 | Loss: 0.00002146
Iteration 31/1000 | Loss: 0.00002145
Iteration 32/1000 | Loss: 0.00002144
Iteration 33/1000 | Loss: 0.00002143
Iteration 34/1000 | Loss: 0.00002143
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00002140
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002139
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002139
Iteration 42/1000 | Loss: 0.00002139
Iteration 43/1000 | Loss: 0.00002139
Iteration 44/1000 | Loss: 0.00002139
Iteration 45/1000 | Loss: 0.00002139
Iteration 46/1000 | Loss: 0.00002139
Iteration 47/1000 | Loss: 0.00002138
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002135
Iteration 55/1000 | Loss: 0.00002135
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002135
Iteration 59/1000 | Loss: 0.00002134
Iteration 60/1000 | Loss: 0.00002134
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002134
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002132
Iteration 68/1000 | Loss: 0.00002132
Iteration 69/1000 | Loss: 0.00002132
Iteration 70/1000 | Loss: 0.00002132
Iteration 71/1000 | Loss: 0.00002131
Iteration 72/1000 | Loss: 0.00002131
Iteration 73/1000 | Loss: 0.00002131
Iteration 74/1000 | Loss: 0.00002130
Iteration 75/1000 | Loss: 0.00002130
Iteration 76/1000 | Loss: 0.00002130
Iteration 77/1000 | Loss: 0.00002130
Iteration 78/1000 | Loss: 0.00002130
Iteration 79/1000 | Loss: 0.00002130
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002127
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002126
Iteration 89/1000 | Loss: 0.00002126
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002125
Iteration 92/1000 | Loss: 0.00002125
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002124
Iteration 96/1000 | Loss: 0.00002124
Iteration 97/1000 | Loss: 0.00002124
Iteration 98/1000 | Loss: 0.00002124
Iteration 99/1000 | Loss: 0.00002124
Iteration 100/1000 | Loss: 0.00002124
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00002124
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002124
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002124
Iteration 108/1000 | Loss: 0.00002124
Iteration 109/1000 | Loss: 0.00002124
Iteration 110/1000 | Loss: 0.00002124
Iteration 111/1000 | Loss: 0.00002124
Iteration 112/1000 | Loss: 0.00002123
Iteration 113/1000 | Loss: 0.00002123
Iteration 114/1000 | Loss: 0.00002123
Iteration 115/1000 | Loss: 0.00002123
Iteration 116/1000 | Loss: 0.00002123
Iteration 117/1000 | Loss: 0.00002123
Iteration 118/1000 | Loss: 0.00002123
Iteration 119/1000 | Loss: 0.00002122
Iteration 120/1000 | Loss: 0.00002122
Iteration 121/1000 | Loss: 0.00002122
Iteration 122/1000 | Loss: 0.00002122
Iteration 123/1000 | Loss: 0.00002122
Iteration 124/1000 | Loss: 0.00002122
Iteration 125/1000 | Loss: 0.00002122
Iteration 126/1000 | Loss: 0.00002121
Iteration 127/1000 | Loss: 0.00002121
Iteration 128/1000 | Loss: 0.00002121
Iteration 129/1000 | Loss: 0.00002121
Iteration 130/1000 | Loss: 0.00002121
Iteration 131/1000 | Loss: 0.00002121
Iteration 132/1000 | Loss: 0.00002121
Iteration 133/1000 | Loss: 0.00002121
Iteration 134/1000 | Loss: 0.00002121
Iteration 135/1000 | Loss: 0.00002121
Iteration 136/1000 | Loss: 0.00002121
Iteration 137/1000 | Loss: 0.00002121
Iteration 138/1000 | Loss: 0.00002121
Iteration 139/1000 | Loss: 0.00002121
Iteration 140/1000 | Loss: 0.00002121
Iteration 141/1000 | Loss: 0.00002121
Iteration 142/1000 | Loss: 0.00002121
Iteration 143/1000 | Loss: 0.00002121
Iteration 144/1000 | Loss: 0.00002120
Iteration 145/1000 | Loss: 0.00002120
Iteration 146/1000 | Loss: 0.00002120
Iteration 147/1000 | Loss: 0.00002120
Iteration 148/1000 | Loss: 0.00002120
Iteration 149/1000 | Loss: 0.00002120
Iteration 150/1000 | Loss: 0.00002120
Iteration 151/1000 | Loss: 0.00002120
Iteration 152/1000 | Loss: 0.00002120
Iteration 153/1000 | Loss: 0.00002120
Iteration 154/1000 | Loss: 0.00002120
Iteration 155/1000 | Loss: 0.00002120
Iteration 156/1000 | Loss: 0.00002120
Iteration 157/1000 | Loss: 0.00002120
Iteration 158/1000 | Loss: 0.00002120
Iteration 159/1000 | Loss: 0.00002120
Iteration 160/1000 | Loss: 0.00002120
Iteration 161/1000 | Loss: 0.00002120
Iteration 162/1000 | Loss: 0.00002120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.1197996829869226e-05, 2.1197996829869226e-05, 2.1197996829869226e-05, 2.1197996829869226e-05, 2.1197996829869226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1197996829869226e-05

Optimization complete. Final v2v error: 4.013242721557617 mm

Highest mean error: 4.309491157531738 mm for frame 160

Lowest mean error: 3.72916841506958 mm for frame 49

Saving results

Total time: 45.91005873680115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455866
Iteration 2/25 | Loss: 0.00103619
Iteration 3/25 | Loss: 0.00086519
Iteration 4/25 | Loss: 0.00084774
Iteration 5/25 | Loss: 0.00084053
Iteration 6/25 | Loss: 0.00083948
Iteration 7/25 | Loss: 0.00083946
Iteration 8/25 | Loss: 0.00083946
Iteration 9/25 | Loss: 0.00083946
Iteration 10/25 | Loss: 0.00083946
Iteration 11/25 | Loss: 0.00083946
Iteration 12/25 | Loss: 0.00083946
Iteration 13/25 | Loss: 0.00083946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008394617470912635, 0.0008394617470912635, 0.0008394617470912635, 0.0008394617470912635, 0.0008394617470912635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008394617470912635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71735358
Iteration 2/25 | Loss: 0.00173952
Iteration 3/25 | Loss: 0.00173951
Iteration 4/25 | Loss: 0.00173950
Iteration 5/25 | Loss: 0.00173950
Iteration 6/25 | Loss: 0.00173950
Iteration 7/25 | Loss: 0.00173950
Iteration 8/25 | Loss: 0.00173950
Iteration 9/25 | Loss: 0.00173950
Iteration 10/25 | Loss: 0.00173950
Iteration 11/25 | Loss: 0.00173950
Iteration 12/25 | Loss: 0.00173950
Iteration 13/25 | Loss: 0.00173950
Iteration 14/25 | Loss: 0.00173950
Iteration 15/25 | Loss: 0.00173950
Iteration 16/25 | Loss: 0.00173950
Iteration 17/25 | Loss: 0.00173950
Iteration 18/25 | Loss: 0.00173950
Iteration 19/25 | Loss: 0.00173950
Iteration 20/25 | Loss: 0.00173950
Iteration 21/25 | Loss: 0.00173950
Iteration 22/25 | Loss: 0.00173950
Iteration 23/25 | Loss: 0.00173950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017395021859556437, 0.0017395021859556437, 0.0017395021859556437, 0.0017395021859556437, 0.0017395021859556437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017395021859556437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173950
Iteration 2/1000 | Loss: 0.00004391
Iteration 3/1000 | Loss: 0.00002920
Iteration 4/1000 | Loss: 0.00002465
Iteration 5/1000 | Loss: 0.00002331
Iteration 6/1000 | Loss: 0.00002246
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002117
Iteration 9/1000 | Loss: 0.00002081
Iteration 10/1000 | Loss: 0.00002050
Iteration 11/1000 | Loss: 0.00002029
Iteration 12/1000 | Loss: 0.00002015
Iteration 13/1000 | Loss: 0.00002003
Iteration 14/1000 | Loss: 0.00002001
Iteration 15/1000 | Loss: 0.00001997
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001992
Iteration 18/1000 | Loss: 0.00001988
Iteration 19/1000 | Loss: 0.00001984
Iteration 20/1000 | Loss: 0.00001982
Iteration 21/1000 | Loss: 0.00001972
Iteration 22/1000 | Loss: 0.00001965
Iteration 23/1000 | Loss: 0.00001960
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001956
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00001954
Iteration 30/1000 | Loss: 0.00001954
Iteration 31/1000 | Loss: 0.00001954
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001953
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001951
Iteration 37/1000 | Loss: 0.00001951
Iteration 38/1000 | Loss: 0.00001951
Iteration 39/1000 | Loss: 0.00001950
Iteration 40/1000 | Loss: 0.00001950
Iteration 41/1000 | Loss: 0.00001949
Iteration 42/1000 | Loss: 0.00001949
Iteration 43/1000 | Loss: 0.00001948
Iteration 44/1000 | Loss: 0.00001948
Iteration 45/1000 | Loss: 0.00001947
Iteration 46/1000 | Loss: 0.00001947
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001946
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001945
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001943
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001941
Iteration 57/1000 | Loss: 0.00001941
Iteration 58/1000 | Loss: 0.00001941
Iteration 59/1000 | Loss: 0.00001941
Iteration 60/1000 | Loss: 0.00001941
Iteration 61/1000 | Loss: 0.00001941
Iteration 62/1000 | Loss: 0.00001941
Iteration 63/1000 | Loss: 0.00001940
Iteration 64/1000 | Loss: 0.00001940
Iteration 65/1000 | Loss: 0.00001940
Iteration 66/1000 | Loss: 0.00001940
Iteration 67/1000 | Loss: 0.00001940
Iteration 68/1000 | Loss: 0.00001939
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001938
Iteration 71/1000 | Loss: 0.00001938
Iteration 72/1000 | Loss: 0.00001938
Iteration 73/1000 | Loss: 0.00001937
Iteration 74/1000 | Loss: 0.00001937
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001936
Iteration 77/1000 | Loss: 0.00001935
Iteration 78/1000 | Loss: 0.00001935
Iteration 79/1000 | Loss: 0.00001935
Iteration 80/1000 | Loss: 0.00001934
Iteration 81/1000 | Loss: 0.00001934
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001933
Iteration 84/1000 | Loss: 0.00001933
Iteration 85/1000 | Loss: 0.00001933
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001931
Iteration 90/1000 | Loss: 0.00001931
Iteration 91/1000 | Loss: 0.00001931
Iteration 92/1000 | Loss: 0.00001931
Iteration 93/1000 | Loss: 0.00001931
Iteration 94/1000 | Loss: 0.00001930
Iteration 95/1000 | Loss: 0.00001930
Iteration 96/1000 | Loss: 0.00001930
Iteration 97/1000 | Loss: 0.00001929
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001929
Iteration 100/1000 | Loss: 0.00001929
Iteration 101/1000 | Loss: 0.00001929
Iteration 102/1000 | Loss: 0.00001928
Iteration 103/1000 | Loss: 0.00001928
Iteration 104/1000 | Loss: 0.00001928
Iteration 105/1000 | Loss: 0.00001928
Iteration 106/1000 | Loss: 0.00001927
Iteration 107/1000 | Loss: 0.00001927
Iteration 108/1000 | Loss: 0.00001927
Iteration 109/1000 | Loss: 0.00001927
Iteration 110/1000 | Loss: 0.00001927
Iteration 111/1000 | Loss: 0.00001927
Iteration 112/1000 | Loss: 0.00001927
Iteration 113/1000 | Loss: 0.00001926
Iteration 114/1000 | Loss: 0.00001926
Iteration 115/1000 | Loss: 0.00001925
Iteration 116/1000 | Loss: 0.00001924
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001924
Iteration 120/1000 | Loss: 0.00001924
Iteration 121/1000 | Loss: 0.00001924
Iteration 122/1000 | Loss: 0.00001924
Iteration 123/1000 | Loss: 0.00001924
Iteration 124/1000 | Loss: 0.00001924
Iteration 125/1000 | Loss: 0.00001924
Iteration 126/1000 | Loss: 0.00001924
Iteration 127/1000 | Loss: 0.00001923
Iteration 128/1000 | Loss: 0.00001923
Iteration 129/1000 | Loss: 0.00001923
Iteration 130/1000 | Loss: 0.00001923
Iteration 131/1000 | Loss: 0.00001923
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001923
Iteration 134/1000 | Loss: 0.00001921
Iteration 135/1000 | Loss: 0.00001921
Iteration 136/1000 | Loss: 0.00001921
Iteration 137/1000 | Loss: 0.00001920
Iteration 138/1000 | Loss: 0.00001920
Iteration 139/1000 | Loss: 0.00001920
Iteration 140/1000 | Loss: 0.00001919
Iteration 141/1000 | Loss: 0.00001919
Iteration 142/1000 | Loss: 0.00001919
Iteration 143/1000 | Loss: 0.00001919
Iteration 144/1000 | Loss: 0.00001919
Iteration 145/1000 | Loss: 0.00001919
Iteration 146/1000 | Loss: 0.00001919
Iteration 147/1000 | Loss: 0.00001918
Iteration 148/1000 | Loss: 0.00001918
Iteration 149/1000 | Loss: 0.00001918
Iteration 150/1000 | Loss: 0.00001917
Iteration 151/1000 | Loss: 0.00001917
Iteration 152/1000 | Loss: 0.00001917
Iteration 153/1000 | Loss: 0.00001917
Iteration 154/1000 | Loss: 0.00001917
Iteration 155/1000 | Loss: 0.00001916
Iteration 156/1000 | Loss: 0.00001916
Iteration 157/1000 | Loss: 0.00001916
Iteration 158/1000 | Loss: 0.00001916
Iteration 159/1000 | Loss: 0.00001916
Iteration 160/1000 | Loss: 0.00001916
Iteration 161/1000 | Loss: 0.00001916
Iteration 162/1000 | Loss: 0.00001916
Iteration 163/1000 | Loss: 0.00001916
Iteration 164/1000 | Loss: 0.00001916
Iteration 165/1000 | Loss: 0.00001916
Iteration 166/1000 | Loss: 0.00001915
Iteration 167/1000 | Loss: 0.00001915
Iteration 168/1000 | Loss: 0.00001915
Iteration 169/1000 | Loss: 0.00001915
Iteration 170/1000 | Loss: 0.00001915
Iteration 171/1000 | Loss: 0.00001915
Iteration 172/1000 | Loss: 0.00001915
Iteration 173/1000 | Loss: 0.00001915
Iteration 174/1000 | Loss: 0.00001915
Iteration 175/1000 | Loss: 0.00001914
Iteration 176/1000 | Loss: 0.00001914
Iteration 177/1000 | Loss: 0.00001914
Iteration 178/1000 | Loss: 0.00001914
Iteration 179/1000 | Loss: 0.00001914
Iteration 180/1000 | Loss: 0.00001914
Iteration 181/1000 | Loss: 0.00001914
Iteration 182/1000 | Loss: 0.00001914
Iteration 183/1000 | Loss: 0.00001914
Iteration 184/1000 | Loss: 0.00001914
Iteration 185/1000 | Loss: 0.00001914
Iteration 186/1000 | Loss: 0.00001914
Iteration 187/1000 | Loss: 0.00001914
Iteration 188/1000 | Loss: 0.00001914
Iteration 189/1000 | Loss: 0.00001913
Iteration 190/1000 | Loss: 0.00001913
Iteration 191/1000 | Loss: 0.00001913
Iteration 192/1000 | Loss: 0.00001913
Iteration 193/1000 | Loss: 0.00001913
Iteration 194/1000 | Loss: 0.00001913
Iteration 195/1000 | Loss: 0.00001913
Iteration 196/1000 | Loss: 0.00001913
Iteration 197/1000 | Loss: 0.00001913
Iteration 198/1000 | Loss: 0.00001913
Iteration 199/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.913215237436816e-05, 1.913215237436816e-05, 1.913215237436816e-05, 1.913215237436816e-05, 1.913215237436816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.913215237436816e-05

Optimization complete. Final v2v error: 3.810436248779297 mm

Highest mean error: 4.677083492279053 mm for frame 172

Lowest mean error: 3.52718448638916 mm for frame 15

Saving results

Total time: 50.27214765548706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937459
Iteration 2/25 | Loss: 0.00107596
Iteration 3/25 | Loss: 0.00090552
Iteration 4/25 | Loss: 0.00087011
Iteration 5/25 | Loss: 0.00086357
Iteration 6/25 | Loss: 0.00086225
Iteration 7/25 | Loss: 0.00086225
Iteration 8/25 | Loss: 0.00086225
Iteration 9/25 | Loss: 0.00086225
Iteration 10/25 | Loss: 0.00086225
Iteration 11/25 | Loss: 0.00086225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008622503373771906, 0.0008622503373771906, 0.0008622503373771906, 0.0008622503373771906, 0.0008622503373771906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008622503373771906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17469597
Iteration 2/25 | Loss: 0.00196748
Iteration 3/25 | Loss: 0.00196745
Iteration 4/25 | Loss: 0.00196744
Iteration 5/25 | Loss: 0.00196744
Iteration 6/25 | Loss: 0.00196744
Iteration 7/25 | Loss: 0.00196744
Iteration 8/25 | Loss: 0.00196744
Iteration 9/25 | Loss: 0.00196744
Iteration 10/25 | Loss: 0.00196744
Iteration 11/25 | Loss: 0.00196744
Iteration 12/25 | Loss: 0.00196744
Iteration 13/25 | Loss: 0.00196744
Iteration 14/25 | Loss: 0.00196744
Iteration 15/25 | Loss: 0.00196744
Iteration 16/25 | Loss: 0.00196744
Iteration 17/25 | Loss: 0.00196744
Iteration 18/25 | Loss: 0.00196744
Iteration 19/25 | Loss: 0.00196744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001967441523447633, 0.001967441523447633, 0.001967441523447633, 0.001967441523447633, 0.001967441523447633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001967441523447633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196744
Iteration 2/1000 | Loss: 0.00005668
Iteration 3/1000 | Loss: 0.00003662
Iteration 4/1000 | Loss: 0.00003118
Iteration 5/1000 | Loss: 0.00002918
Iteration 6/1000 | Loss: 0.00002853
Iteration 7/1000 | Loss: 0.00002787
Iteration 8/1000 | Loss: 0.00002717
Iteration 9/1000 | Loss: 0.00002679
Iteration 10/1000 | Loss: 0.00002649
Iteration 11/1000 | Loss: 0.00002620
Iteration 12/1000 | Loss: 0.00002591
Iteration 13/1000 | Loss: 0.00002564
Iteration 14/1000 | Loss: 0.00002548
Iteration 15/1000 | Loss: 0.00002548
Iteration 16/1000 | Loss: 0.00002544
Iteration 17/1000 | Loss: 0.00002543
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00002540
Iteration 20/1000 | Loss: 0.00002540
Iteration 21/1000 | Loss: 0.00002540
Iteration 22/1000 | Loss: 0.00002540
Iteration 23/1000 | Loss: 0.00002540
Iteration 24/1000 | Loss: 0.00002540
Iteration 25/1000 | Loss: 0.00002540
Iteration 26/1000 | Loss: 0.00002540
Iteration 27/1000 | Loss: 0.00002540
Iteration 28/1000 | Loss: 0.00002540
Iteration 29/1000 | Loss: 0.00002539
Iteration 30/1000 | Loss: 0.00002539
Iteration 31/1000 | Loss: 0.00002538
Iteration 32/1000 | Loss: 0.00002538
Iteration 33/1000 | Loss: 0.00002536
Iteration 34/1000 | Loss: 0.00002535
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00002534
Iteration 37/1000 | Loss: 0.00002533
Iteration 38/1000 | Loss: 0.00002533
Iteration 39/1000 | Loss: 0.00002532
Iteration 40/1000 | Loss: 0.00002531
Iteration 41/1000 | Loss: 0.00002531
Iteration 42/1000 | Loss: 0.00002531
Iteration 43/1000 | Loss: 0.00002531
Iteration 44/1000 | Loss: 0.00002531
Iteration 45/1000 | Loss: 0.00002531
Iteration 46/1000 | Loss: 0.00002531
Iteration 47/1000 | Loss: 0.00002530
Iteration 48/1000 | Loss: 0.00002530
Iteration 49/1000 | Loss: 0.00002530
Iteration 50/1000 | Loss: 0.00002530
Iteration 51/1000 | Loss: 0.00002529
Iteration 52/1000 | Loss: 0.00002529
Iteration 53/1000 | Loss: 0.00002528
Iteration 54/1000 | Loss: 0.00002528
Iteration 55/1000 | Loss: 0.00002528
Iteration 56/1000 | Loss: 0.00002528
Iteration 57/1000 | Loss: 0.00002527
Iteration 58/1000 | Loss: 0.00002527
Iteration 59/1000 | Loss: 0.00002526
Iteration 60/1000 | Loss: 0.00002526
Iteration 61/1000 | Loss: 0.00002526
Iteration 62/1000 | Loss: 0.00002526
Iteration 63/1000 | Loss: 0.00002525
Iteration 64/1000 | Loss: 0.00002525
Iteration 65/1000 | Loss: 0.00002525
Iteration 66/1000 | Loss: 0.00002525
Iteration 67/1000 | Loss: 0.00002525
Iteration 68/1000 | Loss: 0.00002525
Iteration 69/1000 | Loss: 0.00002525
Iteration 70/1000 | Loss: 0.00002525
Iteration 71/1000 | Loss: 0.00002525
Iteration 72/1000 | Loss: 0.00002525
Iteration 73/1000 | Loss: 0.00002524
Iteration 74/1000 | Loss: 0.00002524
Iteration 75/1000 | Loss: 0.00002524
Iteration 76/1000 | Loss: 0.00002524
Iteration 77/1000 | Loss: 0.00002524
Iteration 78/1000 | Loss: 0.00002523
Iteration 79/1000 | Loss: 0.00002523
Iteration 80/1000 | Loss: 0.00002523
Iteration 81/1000 | Loss: 0.00002523
Iteration 82/1000 | Loss: 0.00002523
Iteration 83/1000 | Loss: 0.00002522
Iteration 84/1000 | Loss: 0.00002522
Iteration 85/1000 | Loss: 0.00002522
Iteration 86/1000 | Loss: 0.00002522
Iteration 87/1000 | Loss: 0.00002522
Iteration 88/1000 | Loss: 0.00002522
Iteration 89/1000 | Loss: 0.00002521
Iteration 90/1000 | Loss: 0.00002521
Iteration 91/1000 | Loss: 0.00002521
Iteration 92/1000 | Loss: 0.00002520
Iteration 93/1000 | Loss: 0.00002520
Iteration 94/1000 | Loss: 0.00002520
Iteration 95/1000 | Loss: 0.00002519
Iteration 96/1000 | Loss: 0.00002519
Iteration 97/1000 | Loss: 0.00002519
Iteration 98/1000 | Loss: 0.00002519
Iteration 99/1000 | Loss: 0.00002519
Iteration 100/1000 | Loss: 0.00002519
Iteration 101/1000 | Loss: 0.00002519
Iteration 102/1000 | Loss: 0.00002519
Iteration 103/1000 | Loss: 0.00002518
Iteration 104/1000 | Loss: 0.00002518
Iteration 105/1000 | Loss: 0.00002518
Iteration 106/1000 | Loss: 0.00002518
Iteration 107/1000 | Loss: 0.00002518
Iteration 108/1000 | Loss: 0.00002518
Iteration 109/1000 | Loss: 0.00002518
Iteration 110/1000 | Loss: 0.00002518
Iteration 111/1000 | Loss: 0.00002518
Iteration 112/1000 | Loss: 0.00002518
Iteration 113/1000 | Loss: 0.00002517
Iteration 114/1000 | Loss: 0.00002517
Iteration 115/1000 | Loss: 0.00002517
Iteration 116/1000 | Loss: 0.00002517
Iteration 117/1000 | Loss: 0.00002517
Iteration 118/1000 | Loss: 0.00002517
Iteration 119/1000 | Loss: 0.00002517
Iteration 120/1000 | Loss: 0.00002517
Iteration 121/1000 | Loss: 0.00002517
Iteration 122/1000 | Loss: 0.00002516
Iteration 123/1000 | Loss: 0.00002516
Iteration 124/1000 | Loss: 0.00002516
Iteration 125/1000 | Loss: 0.00002516
Iteration 126/1000 | Loss: 0.00002516
Iteration 127/1000 | Loss: 0.00002516
Iteration 128/1000 | Loss: 0.00002516
Iteration 129/1000 | Loss: 0.00002516
Iteration 130/1000 | Loss: 0.00002516
Iteration 131/1000 | Loss: 0.00002516
Iteration 132/1000 | Loss: 0.00002516
Iteration 133/1000 | Loss: 0.00002516
Iteration 134/1000 | Loss: 0.00002516
Iteration 135/1000 | Loss: 0.00002516
Iteration 136/1000 | Loss: 0.00002516
Iteration 137/1000 | Loss: 0.00002516
Iteration 138/1000 | Loss: 0.00002516
Iteration 139/1000 | Loss: 0.00002516
Iteration 140/1000 | Loss: 0.00002516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.5158469725283794e-05, 2.5158469725283794e-05, 2.5158469725283794e-05, 2.5158469725283794e-05, 2.5158469725283794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5158469725283794e-05

Optimization complete. Final v2v error: 4.300382137298584 mm

Highest mean error: 4.667691707611084 mm for frame 139

Lowest mean error: 3.7159502506256104 mm for frame 0

Saving results

Total time: 37.90138602256775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_1149/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_1149/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422961
Iteration 2/25 | Loss: 0.00096455
Iteration 3/25 | Loss: 0.00083796
Iteration 4/25 | Loss: 0.00080276
Iteration 5/25 | Loss: 0.00079584
Iteration 6/25 | Loss: 0.00079442
Iteration 7/25 | Loss: 0.00079418
Iteration 8/25 | Loss: 0.00079418
Iteration 9/25 | Loss: 0.00079418
Iteration 10/25 | Loss: 0.00079418
Iteration 11/25 | Loss: 0.00079418
Iteration 12/25 | Loss: 0.00079418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007941783987917006, 0.0007941783987917006, 0.0007941783987917006, 0.0007941783987917006, 0.0007941783987917006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007941783987917006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27252328
Iteration 2/25 | Loss: 0.00166338
Iteration 3/25 | Loss: 0.00166338
Iteration 4/25 | Loss: 0.00166338
Iteration 5/25 | Loss: 0.00166338
Iteration 6/25 | Loss: 0.00166338
Iteration 7/25 | Loss: 0.00166338
Iteration 8/25 | Loss: 0.00166338
Iteration 9/25 | Loss: 0.00166338
Iteration 10/25 | Loss: 0.00166338
Iteration 11/25 | Loss: 0.00166338
Iteration 12/25 | Loss: 0.00166338
Iteration 13/25 | Loss: 0.00166338
Iteration 14/25 | Loss: 0.00166338
Iteration 15/25 | Loss: 0.00166338
Iteration 16/25 | Loss: 0.00166338
Iteration 17/25 | Loss: 0.00166338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001663376810029149, 0.001663376810029149, 0.001663376810029149, 0.001663376810029149, 0.001663376810029149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001663376810029149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166338
Iteration 2/1000 | Loss: 0.00004674
Iteration 3/1000 | Loss: 0.00002563
Iteration 4/1000 | Loss: 0.00002070
Iteration 5/1000 | Loss: 0.00001960
Iteration 6/1000 | Loss: 0.00001865
Iteration 7/1000 | Loss: 0.00001801
Iteration 8/1000 | Loss: 0.00001753
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001675
Iteration 13/1000 | Loss: 0.00001666
Iteration 14/1000 | Loss: 0.00001657
Iteration 15/1000 | Loss: 0.00001648
Iteration 16/1000 | Loss: 0.00001645
Iteration 17/1000 | Loss: 0.00001644
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001637
Iteration 20/1000 | Loss: 0.00001636
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001631
Iteration 25/1000 | Loss: 0.00001629
Iteration 26/1000 | Loss: 0.00001629
Iteration 27/1000 | Loss: 0.00001627
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001625
Iteration 31/1000 | Loss: 0.00001625
Iteration 32/1000 | Loss: 0.00001624
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001622
Iteration 38/1000 | Loss: 0.00001622
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001621
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001618
Iteration 43/1000 | Loss: 0.00001618
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001618
Iteration 46/1000 | Loss: 0.00001618
Iteration 47/1000 | Loss: 0.00001618
Iteration 48/1000 | Loss: 0.00001618
Iteration 49/1000 | Loss: 0.00001616
Iteration 50/1000 | Loss: 0.00001616
Iteration 51/1000 | Loss: 0.00001616
Iteration 52/1000 | Loss: 0.00001616
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001616
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001615
Iteration 58/1000 | Loss: 0.00001615
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001615
Iteration 61/1000 | Loss: 0.00001615
Iteration 62/1000 | Loss: 0.00001615
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001613
Iteration 69/1000 | Loss: 0.00001612
Iteration 70/1000 | Loss: 0.00001612
Iteration 71/1000 | Loss: 0.00001612
Iteration 72/1000 | Loss: 0.00001611
Iteration 73/1000 | Loss: 0.00001611
Iteration 74/1000 | Loss: 0.00001611
Iteration 75/1000 | Loss: 0.00001611
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001610
Iteration 78/1000 | Loss: 0.00001609
Iteration 79/1000 | Loss: 0.00001609
Iteration 80/1000 | Loss: 0.00001609
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001608
Iteration 83/1000 | Loss: 0.00001608
Iteration 84/1000 | Loss: 0.00001608
Iteration 85/1000 | Loss: 0.00001608
Iteration 86/1000 | Loss: 0.00001608
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001608
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001604
Iteration 96/1000 | Loss: 0.00001604
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001603
Iteration 99/1000 | Loss: 0.00001603
Iteration 100/1000 | Loss: 0.00001603
Iteration 101/1000 | Loss: 0.00001602
Iteration 102/1000 | Loss: 0.00001602
Iteration 103/1000 | Loss: 0.00001601
Iteration 104/1000 | Loss: 0.00001601
Iteration 105/1000 | Loss: 0.00001601
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001599
Iteration 112/1000 | Loss: 0.00001599
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001598
Iteration 120/1000 | Loss: 0.00001598
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001597
Iteration 128/1000 | Loss: 0.00001597
Iteration 129/1000 | Loss: 0.00001597
Iteration 130/1000 | Loss: 0.00001596
Iteration 131/1000 | Loss: 0.00001596
Iteration 132/1000 | Loss: 0.00001596
Iteration 133/1000 | Loss: 0.00001596
Iteration 134/1000 | Loss: 0.00001596
Iteration 135/1000 | Loss: 0.00001596
Iteration 136/1000 | Loss: 0.00001596
Iteration 137/1000 | Loss: 0.00001595
Iteration 138/1000 | Loss: 0.00001595
Iteration 139/1000 | Loss: 0.00001595
Iteration 140/1000 | Loss: 0.00001595
Iteration 141/1000 | Loss: 0.00001595
Iteration 142/1000 | Loss: 0.00001595
Iteration 143/1000 | Loss: 0.00001594
Iteration 144/1000 | Loss: 0.00001594
Iteration 145/1000 | Loss: 0.00001594
Iteration 146/1000 | Loss: 0.00001594
Iteration 147/1000 | Loss: 0.00001594
Iteration 148/1000 | Loss: 0.00001594
Iteration 149/1000 | Loss: 0.00001594
Iteration 150/1000 | Loss: 0.00001593
Iteration 151/1000 | Loss: 0.00001593
Iteration 152/1000 | Loss: 0.00001593
Iteration 153/1000 | Loss: 0.00001593
Iteration 154/1000 | Loss: 0.00001593
Iteration 155/1000 | Loss: 0.00001593
Iteration 156/1000 | Loss: 0.00001593
Iteration 157/1000 | Loss: 0.00001593
Iteration 158/1000 | Loss: 0.00001593
Iteration 159/1000 | Loss: 0.00001593
Iteration 160/1000 | Loss: 0.00001593
Iteration 161/1000 | Loss: 0.00001593
Iteration 162/1000 | Loss: 0.00001593
Iteration 163/1000 | Loss: 0.00001593
Iteration 164/1000 | Loss: 0.00001592
Iteration 165/1000 | Loss: 0.00001592
Iteration 166/1000 | Loss: 0.00001592
Iteration 167/1000 | Loss: 0.00001592
Iteration 168/1000 | Loss: 0.00001592
Iteration 169/1000 | Loss: 0.00001592
Iteration 170/1000 | Loss: 0.00001592
Iteration 171/1000 | Loss: 0.00001592
Iteration 172/1000 | Loss: 0.00001592
Iteration 173/1000 | Loss: 0.00001592
Iteration 174/1000 | Loss: 0.00001592
Iteration 175/1000 | Loss: 0.00001592
Iteration 176/1000 | Loss: 0.00001592
Iteration 177/1000 | Loss: 0.00001592
Iteration 178/1000 | Loss: 0.00001591
Iteration 179/1000 | Loss: 0.00001591
Iteration 180/1000 | Loss: 0.00001591
Iteration 181/1000 | Loss: 0.00001591
Iteration 182/1000 | Loss: 0.00001591
Iteration 183/1000 | Loss: 0.00001591
Iteration 184/1000 | Loss: 0.00001591
Iteration 185/1000 | Loss: 0.00001591
Iteration 186/1000 | Loss: 0.00001591
Iteration 187/1000 | Loss: 0.00001591
Iteration 188/1000 | Loss: 0.00001591
Iteration 189/1000 | Loss: 0.00001591
Iteration 190/1000 | Loss: 0.00001591
Iteration 191/1000 | Loss: 0.00001591
Iteration 192/1000 | Loss: 0.00001591
Iteration 193/1000 | Loss: 0.00001591
Iteration 194/1000 | Loss: 0.00001591
Iteration 195/1000 | Loss: 0.00001591
Iteration 196/1000 | Loss: 0.00001591
Iteration 197/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.5906560292933136e-05, 1.5906560292933136e-05, 1.5906560292933136e-05, 1.5906560292933136e-05, 1.5906560292933136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5906560292933136e-05

Optimization complete. Final v2v error: 3.3841872215270996 mm

Highest mean error: 3.741323709487915 mm for frame 5

Lowest mean error: 3.0699822902679443 mm for frame 31

Saving results

Total time: 40.19964575767517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393337
Iteration 2/25 | Loss: 0.00088197
Iteration 3/25 | Loss: 0.00063650
Iteration 4/25 | Loss: 0.00059982
Iteration 5/25 | Loss: 0.00059362
Iteration 6/25 | Loss: 0.00059223
Iteration 7/25 | Loss: 0.00059181
Iteration 8/25 | Loss: 0.00059181
Iteration 9/25 | Loss: 0.00059181
Iteration 10/25 | Loss: 0.00059181
Iteration 11/25 | Loss: 0.00059181
Iteration 12/25 | Loss: 0.00059181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005918072420172393, 0.0005918072420172393, 0.0005918072420172393, 0.0005918072420172393, 0.0005918072420172393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005918072420172393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45100176
Iteration 2/25 | Loss: 0.00023880
Iteration 3/25 | Loss: 0.00023879
Iteration 4/25 | Loss: 0.00023879
Iteration 5/25 | Loss: 0.00023879
Iteration 6/25 | Loss: 0.00023879
Iteration 7/25 | Loss: 0.00023879
Iteration 8/25 | Loss: 0.00023879
Iteration 9/25 | Loss: 0.00023879
Iteration 10/25 | Loss: 0.00023879
Iteration 11/25 | Loss: 0.00023879
Iteration 12/25 | Loss: 0.00023879
Iteration 13/25 | Loss: 0.00023879
Iteration 14/25 | Loss: 0.00023879
Iteration 15/25 | Loss: 0.00023879
Iteration 16/25 | Loss: 0.00023879
Iteration 17/25 | Loss: 0.00023879
Iteration 18/25 | Loss: 0.00023879
Iteration 19/25 | Loss: 0.00023879
Iteration 20/25 | Loss: 0.00023879
Iteration 21/25 | Loss: 0.00023879
Iteration 22/25 | Loss: 0.00023879
Iteration 23/25 | Loss: 0.00023879
Iteration 24/25 | Loss: 0.00023879
Iteration 25/25 | Loss: 0.00023879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023879
Iteration 2/1000 | Loss: 0.00001948
Iteration 3/1000 | Loss: 0.00001482
Iteration 4/1000 | Loss: 0.00001347
Iteration 5/1000 | Loss: 0.00001269
Iteration 6/1000 | Loss: 0.00001223
Iteration 7/1000 | Loss: 0.00001191
Iteration 8/1000 | Loss: 0.00001178
Iteration 9/1000 | Loss: 0.00001178
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001171
Iteration 14/1000 | Loss: 0.00001164
Iteration 15/1000 | Loss: 0.00001156
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001152
Iteration 18/1000 | Loss: 0.00001151
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001150
Iteration 22/1000 | Loss: 0.00001149
Iteration 23/1000 | Loss: 0.00001147
Iteration 24/1000 | Loss: 0.00001146
Iteration 25/1000 | Loss: 0.00001145
Iteration 26/1000 | Loss: 0.00001144
Iteration 27/1000 | Loss: 0.00001143
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001140
Iteration 34/1000 | Loss: 0.00001140
Iteration 35/1000 | Loss: 0.00001140
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001138
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001138
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001137
Iteration 47/1000 | Loss: 0.00001137
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001136
Iteration 51/1000 | Loss: 0.00001136
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001135
Iteration 54/1000 | Loss: 0.00001135
Iteration 55/1000 | Loss: 0.00001135
Iteration 56/1000 | Loss: 0.00001135
Iteration 57/1000 | Loss: 0.00001135
Iteration 58/1000 | Loss: 0.00001135
Iteration 59/1000 | Loss: 0.00001134
Iteration 60/1000 | Loss: 0.00001134
Iteration 61/1000 | Loss: 0.00001134
Iteration 62/1000 | Loss: 0.00001134
Iteration 63/1000 | Loss: 0.00001133
Iteration 64/1000 | Loss: 0.00001133
Iteration 65/1000 | Loss: 0.00001133
Iteration 66/1000 | Loss: 0.00001132
Iteration 67/1000 | Loss: 0.00001132
Iteration 68/1000 | Loss: 0.00001132
Iteration 69/1000 | Loss: 0.00001131
Iteration 70/1000 | Loss: 0.00001131
Iteration 71/1000 | Loss: 0.00001131
Iteration 72/1000 | Loss: 0.00001131
Iteration 73/1000 | Loss: 0.00001130
Iteration 74/1000 | Loss: 0.00001130
Iteration 75/1000 | Loss: 0.00001130
Iteration 76/1000 | Loss: 0.00001130
Iteration 77/1000 | Loss: 0.00001129
Iteration 78/1000 | Loss: 0.00001129
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001129
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001127
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001124
Iteration 121/1000 | Loss: 0.00001124
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Iteration 128/1000 | Loss: 0.00001124
Iteration 129/1000 | Loss: 0.00001124
Iteration 130/1000 | Loss: 0.00001123
Iteration 131/1000 | Loss: 0.00001123
Iteration 132/1000 | Loss: 0.00001123
Iteration 133/1000 | Loss: 0.00001123
Iteration 134/1000 | Loss: 0.00001122
Iteration 135/1000 | Loss: 0.00001122
Iteration 136/1000 | Loss: 0.00001122
Iteration 137/1000 | Loss: 0.00001122
Iteration 138/1000 | Loss: 0.00001122
Iteration 139/1000 | Loss: 0.00001121
Iteration 140/1000 | Loss: 0.00001121
Iteration 141/1000 | Loss: 0.00001121
Iteration 142/1000 | Loss: 0.00001121
Iteration 143/1000 | Loss: 0.00001121
Iteration 144/1000 | Loss: 0.00001121
Iteration 145/1000 | Loss: 0.00001120
Iteration 146/1000 | Loss: 0.00001120
Iteration 147/1000 | Loss: 0.00001120
Iteration 148/1000 | Loss: 0.00001120
Iteration 149/1000 | Loss: 0.00001120
Iteration 150/1000 | Loss: 0.00001120
Iteration 151/1000 | Loss: 0.00001120
Iteration 152/1000 | Loss: 0.00001120
Iteration 153/1000 | Loss: 0.00001120
Iteration 154/1000 | Loss: 0.00001120
Iteration 155/1000 | Loss: 0.00001120
Iteration 156/1000 | Loss: 0.00001120
Iteration 157/1000 | Loss: 0.00001120
Iteration 158/1000 | Loss: 0.00001119
Iteration 159/1000 | Loss: 0.00001119
Iteration 160/1000 | Loss: 0.00001119
Iteration 161/1000 | Loss: 0.00001119
Iteration 162/1000 | Loss: 0.00001119
Iteration 163/1000 | Loss: 0.00001119
Iteration 164/1000 | Loss: 0.00001118
Iteration 165/1000 | Loss: 0.00001118
Iteration 166/1000 | Loss: 0.00001118
Iteration 167/1000 | Loss: 0.00001118
Iteration 168/1000 | Loss: 0.00001118
Iteration 169/1000 | Loss: 0.00001118
Iteration 170/1000 | Loss: 0.00001118
Iteration 171/1000 | Loss: 0.00001118
Iteration 172/1000 | Loss: 0.00001118
Iteration 173/1000 | Loss: 0.00001118
Iteration 174/1000 | Loss: 0.00001117
Iteration 175/1000 | Loss: 0.00001117
Iteration 176/1000 | Loss: 0.00001117
Iteration 177/1000 | Loss: 0.00001117
Iteration 178/1000 | Loss: 0.00001117
Iteration 179/1000 | Loss: 0.00001117
Iteration 180/1000 | Loss: 0.00001117
Iteration 181/1000 | Loss: 0.00001117
Iteration 182/1000 | Loss: 0.00001117
Iteration 183/1000 | Loss: 0.00001117
Iteration 184/1000 | Loss: 0.00001116
Iteration 185/1000 | Loss: 0.00001116
Iteration 186/1000 | Loss: 0.00001116
Iteration 187/1000 | Loss: 0.00001116
Iteration 188/1000 | Loss: 0.00001116
Iteration 189/1000 | Loss: 0.00001116
Iteration 190/1000 | Loss: 0.00001116
Iteration 191/1000 | Loss: 0.00001116
Iteration 192/1000 | Loss: 0.00001116
Iteration 193/1000 | Loss: 0.00001116
Iteration 194/1000 | Loss: 0.00001116
Iteration 195/1000 | Loss: 0.00001116
Iteration 196/1000 | Loss: 0.00001116
Iteration 197/1000 | Loss: 0.00001116
Iteration 198/1000 | Loss: 0.00001116
Iteration 199/1000 | Loss: 0.00001116
Iteration 200/1000 | Loss: 0.00001116
Iteration 201/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.116201383410953e-05, 1.116201383410953e-05, 1.116201383410953e-05, 1.116201383410953e-05, 1.116201383410953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.116201383410953e-05

Optimization complete. Final v2v error: 2.8660287857055664 mm

Highest mean error: 2.9322669506073 mm for frame 120

Lowest mean error: 2.7712037563323975 mm for frame 97

Saving results

Total time: 35.478764295578
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941629
Iteration 2/25 | Loss: 0.00148752
Iteration 3/25 | Loss: 0.00087504
Iteration 4/25 | Loss: 0.00079968
Iteration 5/25 | Loss: 0.00077213
Iteration 6/25 | Loss: 0.00076560
Iteration 7/25 | Loss: 0.00076464
Iteration 8/25 | Loss: 0.00076459
Iteration 9/25 | Loss: 0.00076459
Iteration 10/25 | Loss: 0.00076459
Iteration 11/25 | Loss: 0.00076459
Iteration 12/25 | Loss: 0.00076459
Iteration 13/25 | Loss: 0.00076459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007645945297554135, 0.0007645945297554135, 0.0007645945297554135, 0.0007645945297554135, 0.0007645945297554135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007645945297554135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09178805
Iteration 2/25 | Loss: 0.00022753
Iteration 3/25 | Loss: 0.00022751
Iteration 4/25 | Loss: 0.00022751
Iteration 5/25 | Loss: 0.00022751
Iteration 6/25 | Loss: 0.00022751
Iteration 7/25 | Loss: 0.00022751
Iteration 8/25 | Loss: 0.00022751
Iteration 9/25 | Loss: 0.00022751
Iteration 10/25 | Loss: 0.00022751
Iteration 11/25 | Loss: 0.00022751
Iteration 12/25 | Loss: 0.00022751
Iteration 13/25 | Loss: 0.00022751
Iteration 14/25 | Loss: 0.00022751
Iteration 15/25 | Loss: 0.00022751
Iteration 16/25 | Loss: 0.00022751
Iteration 17/25 | Loss: 0.00022751
Iteration 18/25 | Loss: 0.00022751
Iteration 19/25 | Loss: 0.00022751
Iteration 20/25 | Loss: 0.00022751
Iteration 21/25 | Loss: 0.00022751
Iteration 22/25 | Loss: 0.00022751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002275081060361117, 0.0002275081060361117, 0.0002275081060361117, 0.0002275081060361117, 0.0002275081060361117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002275081060361117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022751
Iteration 2/1000 | Loss: 0.00004817
Iteration 3/1000 | Loss: 0.00003638
Iteration 4/1000 | Loss: 0.00003410
Iteration 5/1000 | Loss: 0.00003201
Iteration 6/1000 | Loss: 0.00003055
Iteration 7/1000 | Loss: 0.00002979
Iteration 8/1000 | Loss: 0.00002877
Iteration 9/1000 | Loss: 0.00002822
Iteration 10/1000 | Loss: 0.00002790
Iteration 11/1000 | Loss: 0.00002769
Iteration 12/1000 | Loss: 0.00002754
Iteration 13/1000 | Loss: 0.00002751
Iteration 14/1000 | Loss: 0.00002737
Iteration 15/1000 | Loss: 0.00002737
Iteration 16/1000 | Loss: 0.00002733
Iteration 17/1000 | Loss: 0.00002733
Iteration 18/1000 | Loss: 0.00002733
Iteration 19/1000 | Loss: 0.00002733
Iteration 20/1000 | Loss: 0.00002728
Iteration 21/1000 | Loss: 0.00002727
Iteration 22/1000 | Loss: 0.00002727
Iteration 23/1000 | Loss: 0.00002727
Iteration 24/1000 | Loss: 0.00002726
Iteration 25/1000 | Loss: 0.00002725
Iteration 26/1000 | Loss: 0.00002724
Iteration 27/1000 | Loss: 0.00002723
Iteration 28/1000 | Loss: 0.00002723
Iteration 29/1000 | Loss: 0.00002723
Iteration 30/1000 | Loss: 0.00002722
Iteration 31/1000 | Loss: 0.00002722
Iteration 32/1000 | Loss: 0.00002722
Iteration 33/1000 | Loss: 0.00002722
Iteration 34/1000 | Loss: 0.00002720
Iteration 35/1000 | Loss: 0.00002720
Iteration 36/1000 | Loss: 0.00002720
Iteration 37/1000 | Loss: 0.00002719
Iteration 38/1000 | Loss: 0.00002719
Iteration 39/1000 | Loss: 0.00002718
Iteration 40/1000 | Loss: 0.00002718
Iteration 41/1000 | Loss: 0.00002718
Iteration 42/1000 | Loss: 0.00002717
Iteration 43/1000 | Loss: 0.00002717
Iteration 44/1000 | Loss: 0.00002715
Iteration 45/1000 | Loss: 0.00002715
Iteration 46/1000 | Loss: 0.00002715
Iteration 47/1000 | Loss: 0.00002715
Iteration 48/1000 | Loss: 0.00002715
Iteration 49/1000 | Loss: 0.00002715
Iteration 50/1000 | Loss: 0.00002715
Iteration 51/1000 | Loss: 0.00002715
Iteration 52/1000 | Loss: 0.00002715
Iteration 53/1000 | Loss: 0.00002714
Iteration 54/1000 | Loss: 0.00002714
Iteration 55/1000 | Loss: 0.00002712
Iteration 56/1000 | Loss: 0.00002710
Iteration 57/1000 | Loss: 0.00002709
Iteration 58/1000 | Loss: 0.00002707
Iteration 59/1000 | Loss: 0.00002706
Iteration 60/1000 | Loss: 0.00002706
Iteration 61/1000 | Loss: 0.00002706
Iteration 62/1000 | Loss: 0.00002705
Iteration 63/1000 | Loss: 0.00002705
Iteration 64/1000 | Loss: 0.00002703
Iteration 65/1000 | Loss: 0.00002703
Iteration 66/1000 | Loss: 0.00002703
Iteration 67/1000 | Loss: 0.00002702
Iteration 68/1000 | Loss: 0.00002702
Iteration 69/1000 | Loss: 0.00002702
Iteration 70/1000 | Loss: 0.00002702
Iteration 71/1000 | Loss: 0.00002702
Iteration 72/1000 | Loss: 0.00002702
Iteration 73/1000 | Loss: 0.00002701
Iteration 74/1000 | Loss: 0.00002701
Iteration 75/1000 | Loss: 0.00002701
Iteration 76/1000 | Loss: 0.00002700
Iteration 77/1000 | Loss: 0.00002700
Iteration 78/1000 | Loss: 0.00002699
Iteration 79/1000 | Loss: 0.00002699
Iteration 80/1000 | Loss: 0.00002699
Iteration 81/1000 | Loss: 0.00002699
Iteration 82/1000 | Loss: 0.00002698
Iteration 83/1000 | Loss: 0.00002697
Iteration 84/1000 | Loss: 0.00002697
Iteration 85/1000 | Loss: 0.00002696
Iteration 86/1000 | Loss: 0.00002695
Iteration 87/1000 | Loss: 0.00002693
Iteration 88/1000 | Loss: 0.00002693
Iteration 89/1000 | Loss: 0.00002693
Iteration 90/1000 | Loss: 0.00002692
Iteration 91/1000 | Loss: 0.00002692
Iteration 92/1000 | Loss: 0.00002691
Iteration 93/1000 | Loss: 0.00002690
Iteration 94/1000 | Loss: 0.00002690
Iteration 95/1000 | Loss: 0.00002690
Iteration 96/1000 | Loss: 0.00002689
Iteration 97/1000 | Loss: 0.00002689
Iteration 98/1000 | Loss: 0.00002689
Iteration 99/1000 | Loss: 0.00002689
Iteration 100/1000 | Loss: 0.00002689
Iteration 101/1000 | Loss: 0.00002689
Iteration 102/1000 | Loss: 0.00002688
Iteration 103/1000 | Loss: 0.00002688
Iteration 104/1000 | Loss: 0.00002688
Iteration 105/1000 | Loss: 0.00002688
Iteration 106/1000 | Loss: 0.00002688
Iteration 107/1000 | Loss: 0.00002688
Iteration 108/1000 | Loss: 0.00002686
Iteration 109/1000 | Loss: 0.00002686
Iteration 110/1000 | Loss: 0.00002686
Iteration 111/1000 | Loss: 0.00002685
Iteration 112/1000 | Loss: 0.00002685
Iteration 113/1000 | Loss: 0.00002685
Iteration 114/1000 | Loss: 0.00002685
Iteration 115/1000 | Loss: 0.00002685
Iteration 116/1000 | Loss: 0.00002684
Iteration 117/1000 | Loss: 0.00002684
Iteration 118/1000 | Loss: 0.00002684
Iteration 119/1000 | Loss: 0.00002684
Iteration 120/1000 | Loss: 0.00002683
Iteration 121/1000 | Loss: 0.00002683
Iteration 122/1000 | Loss: 0.00002683
Iteration 123/1000 | Loss: 0.00002683
Iteration 124/1000 | Loss: 0.00002682
Iteration 125/1000 | Loss: 0.00002682
Iteration 126/1000 | Loss: 0.00002682
Iteration 127/1000 | Loss: 0.00002682
Iteration 128/1000 | Loss: 0.00002682
Iteration 129/1000 | Loss: 0.00002681
Iteration 130/1000 | Loss: 0.00002681
Iteration 131/1000 | Loss: 0.00002681
Iteration 132/1000 | Loss: 0.00002681
Iteration 133/1000 | Loss: 0.00002681
Iteration 134/1000 | Loss: 0.00002681
Iteration 135/1000 | Loss: 0.00002681
Iteration 136/1000 | Loss: 0.00002681
Iteration 137/1000 | Loss: 0.00002681
Iteration 138/1000 | Loss: 0.00002681
Iteration 139/1000 | Loss: 0.00002681
Iteration 140/1000 | Loss: 0.00002680
Iteration 141/1000 | Loss: 0.00002680
Iteration 142/1000 | Loss: 0.00002680
Iteration 143/1000 | Loss: 0.00002680
Iteration 144/1000 | Loss: 0.00002680
Iteration 145/1000 | Loss: 0.00002680
Iteration 146/1000 | Loss: 0.00002680
Iteration 147/1000 | Loss: 0.00002680
Iteration 148/1000 | Loss: 0.00002680
Iteration 149/1000 | Loss: 0.00002680
Iteration 150/1000 | Loss: 0.00002680
Iteration 151/1000 | Loss: 0.00002680
Iteration 152/1000 | Loss: 0.00002679
Iteration 153/1000 | Loss: 0.00002679
Iteration 154/1000 | Loss: 0.00002679
Iteration 155/1000 | Loss: 0.00002679
Iteration 156/1000 | Loss: 0.00002679
Iteration 157/1000 | Loss: 0.00002679
Iteration 158/1000 | Loss: 0.00002679
Iteration 159/1000 | Loss: 0.00002679
Iteration 160/1000 | Loss: 0.00002679
Iteration 161/1000 | Loss: 0.00002679
Iteration 162/1000 | Loss: 0.00002679
Iteration 163/1000 | Loss: 0.00002679
Iteration 164/1000 | Loss: 0.00002679
Iteration 165/1000 | Loss: 0.00002679
Iteration 166/1000 | Loss: 0.00002679
Iteration 167/1000 | Loss: 0.00002678
Iteration 168/1000 | Loss: 0.00002678
Iteration 169/1000 | Loss: 0.00002678
Iteration 170/1000 | Loss: 0.00002678
Iteration 171/1000 | Loss: 0.00002678
Iteration 172/1000 | Loss: 0.00002678
Iteration 173/1000 | Loss: 0.00002678
Iteration 174/1000 | Loss: 0.00002678
Iteration 175/1000 | Loss: 0.00002678
Iteration 176/1000 | Loss: 0.00002677
Iteration 177/1000 | Loss: 0.00002677
Iteration 178/1000 | Loss: 0.00002677
Iteration 179/1000 | Loss: 0.00002677
Iteration 180/1000 | Loss: 0.00002677
Iteration 181/1000 | Loss: 0.00002677
Iteration 182/1000 | Loss: 0.00002677
Iteration 183/1000 | Loss: 0.00002677
Iteration 184/1000 | Loss: 0.00002677
Iteration 185/1000 | Loss: 0.00002677
Iteration 186/1000 | Loss: 0.00002677
Iteration 187/1000 | Loss: 0.00002677
Iteration 188/1000 | Loss: 0.00002677
Iteration 189/1000 | Loss: 0.00002677
Iteration 190/1000 | Loss: 0.00002677
Iteration 191/1000 | Loss: 0.00002677
Iteration 192/1000 | Loss: 0.00002677
Iteration 193/1000 | Loss: 0.00002677
Iteration 194/1000 | Loss: 0.00002677
Iteration 195/1000 | Loss: 0.00002677
Iteration 196/1000 | Loss: 0.00002677
Iteration 197/1000 | Loss: 0.00002677
Iteration 198/1000 | Loss: 0.00002677
Iteration 199/1000 | Loss: 0.00002677
Iteration 200/1000 | Loss: 0.00002677
Iteration 201/1000 | Loss: 0.00002677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.6766099836095236e-05, 2.6766099836095236e-05, 2.6766099836095236e-05, 2.6766099836095236e-05, 2.6766099836095236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6766099836095236e-05

Optimization complete. Final v2v error: 4.2281270027160645 mm

Highest mean error: 5.403688907623291 mm for frame 103

Lowest mean error: 3.4560391902923584 mm for frame 39

Saving results

Total time: 44.68783617019653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035099
Iteration 2/25 | Loss: 0.00267430
Iteration 3/25 | Loss: 0.00180315
Iteration 4/25 | Loss: 0.00160511
Iteration 5/25 | Loss: 0.00161730
Iteration 6/25 | Loss: 0.00127018
Iteration 7/25 | Loss: 0.00096915
Iteration 8/25 | Loss: 0.00084143
Iteration 9/25 | Loss: 0.00079956
Iteration 10/25 | Loss: 0.00079200
Iteration 11/25 | Loss: 0.00077042
Iteration 12/25 | Loss: 0.00076030
Iteration 13/25 | Loss: 0.00075862
Iteration 14/25 | Loss: 0.00075682
Iteration 15/25 | Loss: 0.00075368
Iteration 16/25 | Loss: 0.00075437
Iteration 17/25 | Loss: 0.00075364
Iteration 18/25 | Loss: 0.00075465
Iteration 19/25 | Loss: 0.00075581
Iteration 20/25 | Loss: 0.00075497
Iteration 21/25 | Loss: 0.00075458
Iteration 22/25 | Loss: 0.00075502
Iteration 23/25 | Loss: 0.00075363
Iteration 24/25 | Loss: 0.00075427
Iteration 25/25 | Loss: 0.00075634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43824315
Iteration 2/25 | Loss: 0.00033465
Iteration 3/25 | Loss: 0.00033465
Iteration 4/25 | Loss: 0.00033465
Iteration 5/25 | Loss: 0.00033465
Iteration 6/25 | Loss: 0.00033465
Iteration 7/25 | Loss: 0.00033465
Iteration 8/25 | Loss: 0.00033465
Iteration 9/25 | Loss: 0.00033465
Iteration 10/25 | Loss: 0.00033465
Iteration 11/25 | Loss: 0.00033465
Iteration 12/25 | Loss: 0.00033465
Iteration 13/25 | Loss: 0.00033465
Iteration 14/25 | Loss: 0.00033465
Iteration 15/25 | Loss: 0.00033465
Iteration 16/25 | Loss: 0.00033465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00033464538864791393, 0.00033464538864791393, 0.00033464538864791393, 0.00033464538864791393, 0.00033464538864791393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033464538864791393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033465
Iteration 2/1000 | Loss: 0.00006572
Iteration 3/1000 | Loss: 0.00006490
Iteration 4/1000 | Loss: 0.00004100
Iteration 5/1000 | Loss: 0.00006328
Iteration 6/1000 | Loss: 0.00004398
Iteration 7/1000 | Loss: 0.00005552
Iteration 8/1000 | Loss: 0.00005537
Iteration 9/1000 | Loss: 0.00006997
Iteration 10/1000 | Loss: 0.00006122
Iteration 11/1000 | Loss: 0.00005985
Iteration 12/1000 | Loss: 0.00006056
Iteration 13/1000 | Loss: 0.00006239
Iteration 14/1000 | Loss: 0.00006972
Iteration 15/1000 | Loss: 0.00005965
Iteration 16/1000 | Loss: 0.00007393
Iteration 17/1000 | Loss: 0.00006748
Iteration 18/1000 | Loss: 0.00007674
Iteration 19/1000 | Loss: 0.00005397
Iteration 20/1000 | Loss: 0.00006288
Iteration 21/1000 | Loss: 0.00003411
Iteration 22/1000 | Loss: 0.00003743
Iteration 23/1000 | Loss: 0.00003371
Iteration 24/1000 | Loss: 0.00004364
Iteration 25/1000 | Loss: 0.00003687
Iteration 26/1000 | Loss: 0.00003427
Iteration 27/1000 | Loss: 0.00003256
Iteration 28/1000 | Loss: 0.00004839
Iteration 29/1000 | Loss: 0.00003920
Iteration 30/1000 | Loss: 0.00003725
Iteration 31/1000 | Loss: 0.00004005
Iteration 32/1000 | Loss: 0.00005814
Iteration 33/1000 | Loss: 0.00004857
Iteration 34/1000 | Loss: 0.00004240
Iteration 35/1000 | Loss: 0.00004320
Iteration 36/1000 | Loss: 0.00004669
Iteration 37/1000 | Loss: 0.00004592
Iteration 38/1000 | Loss: 0.00005588
Iteration 39/1000 | Loss: 0.00003518
Iteration 40/1000 | Loss: 0.00003447
Iteration 41/1000 | Loss: 0.00002620
Iteration 42/1000 | Loss: 0.00002204
Iteration 43/1000 | Loss: 0.00002705
Iteration 44/1000 | Loss: 0.00003288
Iteration 45/1000 | Loss: 0.00003184
Iteration 46/1000 | Loss: 0.00003364
Iteration 47/1000 | Loss: 0.00003098
Iteration 48/1000 | Loss: 0.00003017
Iteration 49/1000 | Loss: 0.00002494
Iteration 50/1000 | Loss: 0.00002847
Iteration 51/1000 | Loss: 0.00002863
Iteration 52/1000 | Loss: 0.00003643
Iteration 53/1000 | Loss: 0.00002953
Iteration 54/1000 | Loss: 0.00003496
Iteration 55/1000 | Loss: 0.00004025
Iteration 56/1000 | Loss: 0.00003666
Iteration 57/1000 | Loss: 0.00002792
Iteration 58/1000 | Loss: 0.00003868
Iteration 59/1000 | Loss: 0.00003076
Iteration 60/1000 | Loss: 0.00002668
Iteration 61/1000 | Loss: 0.00004495
Iteration 62/1000 | Loss: 0.00002769
Iteration 63/1000 | Loss: 0.00002367
Iteration 64/1000 | Loss: 0.00002586
Iteration 65/1000 | Loss: 0.00004630
Iteration 66/1000 | Loss: 0.00002524
Iteration 67/1000 | Loss: 0.00002085
Iteration 68/1000 | Loss: 0.00001970
Iteration 69/1000 | Loss: 0.00001924
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001909
Iteration 72/1000 | Loss: 0.00001907
Iteration 73/1000 | Loss: 0.00001905
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001900
Iteration 76/1000 | Loss: 0.00001900
Iteration 77/1000 | Loss: 0.00001900
Iteration 78/1000 | Loss: 0.00001899
Iteration 79/1000 | Loss: 0.00001897
Iteration 80/1000 | Loss: 0.00001897
Iteration 81/1000 | Loss: 0.00001896
Iteration 82/1000 | Loss: 0.00001896
Iteration 83/1000 | Loss: 0.00001895
Iteration 84/1000 | Loss: 0.00001895
Iteration 85/1000 | Loss: 0.00001894
Iteration 86/1000 | Loss: 0.00001894
Iteration 87/1000 | Loss: 0.00001893
Iteration 88/1000 | Loss: 0.00001893
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001890
Iteration 103/1000 | Loss: 0.00001889
Iteration 104/1000 | Loss: 0.00001889
Iteration 105/1000 | Loss: 0.00001889
Iteration 106/1000 | Loss: 0.00001888
Iteration 107/1000 | Loss: 0.00001888
Iteration 108/1000 | Loss: 0.00001887
Iteration 109/1000 | Loss: 0.00001887
Iteration 110/1000 | Loss: 0.00001886
Iteration 111/1000 | Loss: 0.00001886
Iteration 112/1000 | Loss: 0.00001886
Iteration 113/1000 | Loss: 0.00001886
Iteration 114/1000 | Loss: 0.00001885
Iteration 115/1000 | Loss: 0.00001885
Iteration 116/1000 | Loss: 0.00001885
Iteration 117/1000 | Loss: 0.00001884
Iteration 118/1000 | Loss: 0.00001884
Iteration 119/1000 | Loss: 0.00001884
Iteration 120/1000 | Loss: 0.00001884
Iteration 121/1000 | Loss: 0.00001883
Iteration 122/1000 | Loss: 0.00001883
Iteration 123/1000 | Loss: 0.00001883
Iteration 124/1000 | Loss: 0.00001883
Iteration 125/1000 | Loss: 0.00001883
Iteration 126/1000 | Loss: 0.00001883
Iteration 127/1000 | Loss: 0.00001883
Iteration 128/1000 | Loss: 0.00001883
Iteration 129/1000 | Loss: 0.00001882
Iteration 130/1000 | Loss: 0.00001882
Iteration 131/1000 | Loss: 0.00001882
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001882
Iteration 136/1000 | Loss: 0.00001882
Iteration 137/1000 | Loss: 0.00001881
Iteration 138/1000 | Loss: 0.00001881
Iteration 139/1000 | Loss: 0.00001881
Iteration 140/1000 | Loss: 0.00001881
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001881
Iteration 143/1000 | Loss: 0.00001880
Iteration 144/1000 | Loss: 0.00001880
Iteration 145/1000 | Loss: 0.00001880
Iteration 146/1000 | Loss: 0.00001880
Iteration 147/1000 | Loss: 0.00001880
Iteration 148/1000 | Loss: 0.00001880
Iteration 149/1000 | Loss: 0.00001880
Iteration 150/1000 | Loss: 0.00001880
Iteration 151/1000 | Loss: 0.00001880
Iteration 152/1000 | Loss: 0.00001880
Iteration 153/1000 | Loss: 0.00001880
Iteration 154/1000 | Loss: 0.00001879
Iteration 155/1000 | Loss: 0.00001879
Iteration 156/1000 | Loss: 0.00001879
Iteration 157/1000 | Loss: 0.00001879
Iteration 158/1000 | Loss: 0.00001879
Iteration 159/1000 | Loss: 0.00001879
Iteration 160/1000 | Loss: 0.00001879
Iteration 161/1000 | Loss: 0.00001879
Iteration 162/1000 | Loss: 0.00001879
Iteration 163/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.879474802990444e-05, 1.879474802990444e-05, 1.879474802990444e-05, 1.879474802990444e-05, 1.879474802990444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.879474802990444e-05

Optimization complete. Final v2v error: 3.547935724258423 mm

Highest mean error: 4.29094123840332 mm for frame 152

Lowest mean error: 3.314096689224243 mm for frame 122

Saving results

Total time: 169.17703771591187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380833
Iteration 2/25 | Loss: 0.00072513
Iteration 3/25 | Loss: 0.00061876
Iteration 4/25 | Loss: 0.00060051
Iteration 5/25 | Loss: 0.00059382
Iteration 6/25 | Loss: 0.00059246
Iteration 7/25 | Loss: 0.00059235
Iteration 8/25 | Loss: 0.00059235
Iteration 9/25 | Loss: 0.00059235
Iteration 10/25 | Loss: 0.00059235
Iteration 11/25 | Loss: 0.00059235
Iteration 12/25 | Loss: 0.00059235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000592352356761694, 0.000592352356761694, 0.000592352356761694, 0.000592352356761694, 0.000592352356761694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000592352356761694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46075201
Iteration 2/25 | Loss: 0.00028075
Iteration 3/25 | Loss: 0.00028075
Iteration 4/25 | Loss: 0.00028075
Iteration 5/25 | Loss: 0.00028075
Iteration 6/25 | Loss: 0.00028075
Iteration 7/25 | Loss: 0.00028075
Iteration 8/25 | Loss: 0.00028075
Iteration 9/25 | Loss: 0.00028075
Iteration 10/25 | Loss: 0.00028075
Iteration 11/25 | Loss: 0.00028075
Iteration 12/25 | Loss: 0.00028075
Iteration 13/25 | Loss: 0.00028075
Iteration 14/25 | Loss: 0.00028075
Iteration 15/25 | Loss: 0.00028075
Iteration 16/25 | Loss: 0.00028075
Iteration 17/25 | Loss: 0.00028075
Iteration 18/25 | Loss: 0.00028075
Iteration 19/25 | Loss: 0.00028075
Iteration 20/25 | Loss: 0.00028075
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00028074567671865225, 0.00028074567671865225, 0.00028074567671865225, 0.00028074567671865225, 0.00028074567671865225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028074567671865225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028075
Iteration 2/1000 | Loss: 0.00002517
Iteration 3/1000 | Loss: 0.00001562
Iteration 4/1000 | Loss: 0.00001456
Iteration 5/1000 | Loss: 0.00001374
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001295
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001264
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001264
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001252
Iteration 25/1000 | Loss: 0.00001251
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001238
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001235
Iteration 44/1000 | Loss: 0.00001235
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001233
Iteration 48/1000 | Loss: 0.00001232
Iteration 49/1000 | Loss: 0.00001232
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001227
Iteration 58/1000 | Loss: 0.00001227
Iteration 59/1000 | Loss: 0.00001226
Iteration 60/1000 | Loss: 0.00001226
Iteration 61/1000 | Loss: 0.00001226
Iteration 62/1000 | Loss: 0.00001226
Iteration 63/1000 | Loss: 0.00001225
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001217
Iteration 85/1000 | Loss: 0.00001217
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001215
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001215
Iteration 113/1000 | Loss: 0.00001215
Iteration 114/1000 | Loss: 0.00001215
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001215
Iteration 118/1000 | Loss: 0.00001215
Iteration 119/1000 | Loss: 0.00001215
Iteration 120/1000 | Loss: 0.00001215
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Iteration 128/1000 | Loss: 0.00001214
Iteration 129/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.214358962897677e-05, 1.214358962897677e-05, 1.214358962897677e-05, 1.214358962897677e-05, 1.214358962897677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.214358962897677e-05

Optimization complete. Final v2v error: 2.970421552658081 mm

Highest mean error: 3.261091709136963 mm for frame 98

Lowest mean error: 2.8321175575256348 mm for frame 121

Saving results

Total time: 37.15065264701843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883066
Iteration 2/25 | Loss: 0.00112847
Iteration 3/25 | Loss: 0.00081047
Iteration 4/25 | Loss: 0.00074751
Iteration 5/25 | Loss: 0.00074016
Iteration 6/25 | Loss: 0.00073945
Iteration 7/25 | Loss: 0.00073945
Iteration 8/25 | Loss: 0.00073945
Iteration 9/25 | Loss: 0.00073945
Iteration 10/25 | Loss: 0.00073945
Iteration 11/25 | Loss: 0.00073945
Iteration 12/25 | Loss: 0.00073945
Iteration 13/25 | Loss: 0.00073945
Iteration 14/25 | Loss: 0.00073945
Iteration 15/25 | Loss: 0.00073945
Iteration 16/25 | Loss: 0.00073945
Iteration 17/25 | Loss: 0.00073945
Iteration 18/25 | Loss: 0.00073945
Iteration 19/25 | Loss: 0.00073945
Iteration 20/25 | Loss: 0.00073945
Iteration 21/25 | Loss: 0.00073945
Iteration 22/25 | Loss: 0.00073945
Iteration 23/25 | Loss: 0.00073945
Iteration 24/25 | Loss: 0.00073945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007394540589302778, 0.0007394540589302778, 0.0007394540589302778, 0.0007394540589302778, 0.0007394540589302778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007394540589302778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99294829
Iteration 2/25 | Loss: 0.00024679
Iteration 3/25 | Loss: 0.00024678
Iteration 4/25 | Loss: 0.00024678
Iteration 5/25 | Loss: 0.00024678
Iteration 6/25 | Loss: 0.00024678
Iteration 7/25 | Loss: 0.00024678
Iteration 8/25 | Loss: 0.00024678
Iteration 9/25 | Loss: 0.00024678
Iteration 10/25 | Loss: 0.00024678
Iteration 11/25 | Loss: 0.00024678
Iteration 12/25 | Loss: 0.00024678
Iteration 13/25 | Loss: 0.00024678
Iteration 14/25 | Loss: 0.00024678
Iteration 15/25 | Loss: 0.00024678
Iteration 16/25 | Loss: 0.00024678
Iteration 17/25 | Loss: 0.00024678
Iteration 18/25 | Loss: 0.00024678
Iteration 19/25 | Loss: 0.00024678
Iteration 20/25 | Loss: 0.00024678
Iteration 21/25 | Loss: 0.00024678
Iteration 22/25 | Loss: 0.00024678
Iteration 23/25 | Loss: 0.00024678
Iteration 24/25 | Loss: 0.00024678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002467803133185953, 0.0002467803133185953, 0.0002467803133185953, 0.0002467803133185953, 0.0002467803133185953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002467803133185953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024678
Iteration 2/1000 | Loss: 0.00003961
Iteration 3/1000 | Loss: 0.00003113
Iteration 4/1000 | Loss: 0.00002882
Iteration 5/1000 | Loss: 0.00002711
Iteration 6/1000 | Loss: 0.00002623
Iteration 7/1000 | Loss: 0.00002538
Iteration 8/1000 | Loss: 0.00002499
Iteration 9/1000 | Loss: 0.00002468
Iteration 10/1000 | Loss: 0.00002451
Iteration 11/1000 | Loss: 0.00002447
Iteration 12/1000 | Loss: 0.00002442
Iteration 13/1000 | Loss: 0.00002442
Iteration 14/1000 | Loss: 0.00002441
Iteration 15/1000 | Loss: 0.00002430
Iteration 16/1000 | Loss: 0.00002429
Iteration 17/1000 | Loss: 0.00002429
Iteration 18/1000 | Loss: 0.00002429
Iteration 19/1000 | Loss: 0.00002429
Iteration 20/1000 | Loss: 0.00002429
Iteration 21/1000 | Loss: 0.00002429
Iteration 22/1000 | Loss: 0.00002429
Iteration 23/1000 | Loss: 0.00002429
Iteration 24/1000 | Loss: 0.00002429
Iteration 25/1000 | Loss: 0.00002428
Iteration 26/1000 | Loss: 0.00002428
Iteration 27/1000 | Loss: 0.00002428
Iteration 28/1000 | Loss: 0.00002427
Iteration 29/1000 | Loss: 0.00002426
Iteration 30/1000 | Loss: 0.00002425
Iteration 31/1000 | Loss: 0.00002425
Iteration 32/1000 | Loss: 0.00002424
Iteration 33/1000 | Loss: 0.00002424
Iteration 34/1000 | Loss: 0.00002424
Iteration 35/1000 | Loss: 0.00002423
Iteration 36/1000 | Loss: 0.00002423
Iteration 37/1000 | Loss: 0.00002423
Iteration 38/1000 | Loss: 0.00002423
Iteration 39/1000 | Loss: 0.00002422
Iteration 40/1000 | Loss: 0.00002422
Iteration 41/1000 | Loss: 0.00002421
Iteration 42/1000 | Loss: 0.00002421
Iteration 43/1000 | Loss: 0.00002421
Iteration 44/1000 | Loss: 0.00002421
Iteration 45/1000 | Loss: 0.00002421
Iteration 46/1000 | Loss: 0.00002421
Iteration 47/1000 | Loss: 0.00002421
Iteration 48/1000 | Loss: 0.00002420
Iteration 49/1000 | Loss: 0.00002420
Iteration 50/1000 | Loss: 0.00002418
Iteration 51/1000 | Loss: 0.00002417
Iteration 52/1000 | Loss: 0.00002417
Iteration 53/1000 | Loss: 0.00002417
Iteration 54/1000 | Loss: 0.00002416
Iteration 55/1000 | Loss: 0.00002416
Iteration 56/1000 | Loss: 0.00002416
Iteration 57/1000 | Loss: 0.00002415
Iteration 58/1000 | Loss: 0.00002415
Iteration 59/1000 | Loss: 0.00002415
Iteration 60/1000 | Loss: 0.00002415
Iteration 61/1000 | Loss: 0.00002415
Iteration 62/1000 | Loss: 0.00002415
Iteration 63/1000 | Loss: 0.00002415
Iteration 64/1000 | Loss: 0.00002415
Iteration 65/1000 | Loss: 0.00002415
Iteration 66/1000 | Loss: 0.00002414
Iteration 67/1000 | Loss: 0.00002414
Iteration 68/1000 | Loss: 0.00002414
Iteration 69/1000 | Loss: 0.00002414
Iteration 70/1000 | Loss: 0.00002414
Iteration 71/1000 | Loss: 0.00002414
Iteration 72/1000 | Loss: 0.00002414
Iteration 73/1000 | Loss: 0.00002414
Iteration 74/1000 | Loss: 0.00002414
Iteration 75/1000 | Loss: 0.00002414
Iteration 76/1000 | Loss: 0.00002414
Iteration 77/1000 | Loss: 0.00002414
Iteration 78/1000 | Loss: 0.00002414
Iteration 79/1000 | Loss: 0.00002414
Iteration 80/1000 | Loss: 0.00002414
Iteration 81/1000 | Loss: 0.00002414
Iteration 82/1000 | Loss: 0.00002414
Iteration 83/1000 | Loss: 0.00002414
Iteration 84/1000 | Loss: 0.00002414
Iteration 85/1000 | Loss: 0.00002414
Iteration 86/1000 | Loss: 0.00002414
Iteration 87/1000 | Loss: 0.00002414
Iteration 88/1000 | Loss: 0.00002414
Iteration 89/1000 | Loss: 0.00002414
Iteration 90/1000 | Loss: 0.00002413
Iteration 91/1000 | Loss: 0.00002413
Iteration 92/1000 | Loss: 0.00002413
Iteration 93/1000 | Loss: 0.00002413
Iteration 94/1000 | Loss: 0.00002413
Iteration 95/1000 | Loss: 0.00002413
Iteration 96/1000 | Loss: 0.00002413
Iteration 97/1000 | Loss: 0.00002413
Iteration 98/1000 | Loss: 0.00002413
Iteration 99/1000 | Loss: 0.00002413
Iteration 100/1000 | Loss: 0.00002413
Iteration 101/1000 | Loss: 0.00002413
Iteration 102/1000 | Loss: 0.00002413
Iteration 103/1000 | Loss: 0.00002413
Iteration 104/1000 | Loss: 0.00002413
Iteration 105/1000 | Loss: 0.00002413
Iteration 106/1000 | Loss: 0.00002413
Iteration 107/1000 | Loss: 0.00002413
Iteration 108/1000 | Loss: 0.00002413
Iteration 109/1000 | Loss: 0.00002413
Iteration 110/1000 | Loss: 0.00002413
Iteration 111/1000 | Loss: 0.00002413
Iteration 112/1000 | Loss: 0.00002413
Iteration 113/1000 | Loss: 0.00002413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.413363108644262e-05, 2.413363108644262e-05, 2.413363108644262e-05, 2.413363108644262e-05, 2.413363108644262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.413363108644262e-05

Optimization complete. Final v2v error: 4.1438093185424805 mm

Highest mean error: 4.328246593475342 mm for frame 72

Lowest mean error: 4.036439895629883 mm for frame 149

Saving results

Total time: 30.647814989089966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410330
Iteration 2/25 | Loss: 0.00075463
Iteration 3/25 | Loss: 0.00063502
Iteration 4/25 | Loss: 0.00061649
Iteration 5/25 | Loss: 0.00061134
Iteration 6/25 | Loss: 0.00061013
Iteration 7/25 | Loss: 0.00060973
Iteration 8/25 | Loss: 0.00060971
Iteration 9/25 | Loss: 0.00060971
Iteration 10/25 | Loss: 0.00060971
Iteration 11/25 | Loss: 0.00060971
Iteration 12/25 | Loss: 0.00060971
Iteration 13/25 | Loss: 0.00060971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006097114528529346, 0.0006097114528529346, 0.0006097114528529346, 0.0006097114528529346, 0.0006097114528529346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006097114528529346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81087458
Iteration 2/25 | Loss: 0.00027075
Iteration 3/25 | Loss: 0.00027075
Iteration 4/25 | Loss: 0.00027075
Iteration 5/25 | Loss: 0.00027075
Iteration 6/25 | Loss: 0.00027075
Iteration 7/25 | Loss: 0.00027075
Iteration 8/25 | Loss: 0.00027075
Iteration 9/25 | Loss: 0.00027075
Iteration 10/25 | Loss: 0.00027075
Iteration 11/25 | Loss: 0.00027075
Iteration 12/25 | Loss: 0.00027075
Iteration 13/25 | Loss: 0.00027075
Iteration 14/25 | Loss: 0.00027075
Iteration 15/25 | Loss: 0.00027075
Iteration 16/25 | Loss: 0.00027075
Iteration 17/25 | Loss: 0.00027075
Iteration 18/25 | Loss: 0.00027075
Iteration 19/25 | Loss: 0.00027075
Iteration 20/25 | Loss: 0.00027075
Iteration 21/25 | Loss: 0.00027075
Iteration 22/25 | Loss: 0.00027075
Iteration 23/25 | Loss: 0.00027075
Iteration 24/25 | Loss: 0.00027075
Iteration 25/25 | Loss: 0.00027075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027075
Iteration 2/1000 | Loss: 0.00003017
Iteration 3/1000 | Loss: 0.00002020
Iteration 4/1000 | Loss: 0.00001674
Iteration 5/1000 | Loss: 0.00001599
Iteration 6/1000 | Loss: 0.00001518
Iteration 7/1000 | Loss: 0.00001476
Iteration 8/1000 | Loss: 0.00001437
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001399
Iteration 13/1000 | Loss: 0.00001390
Iteration 14/1000 | Loss: 0.00001385
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001384
Iteration 18/1000 | Loss: 0.00001383
Iteration 19/1000 | Loss: 0.00001382
Iteration 20/1000 | Loss: 0.00001381
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001370
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001365
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001359
Iteration 40/1000 | Loss: 0.00001359
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001356
Iteration 53/1000 | Loss: 0.00001356
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001355
Iteration 56/1000 | Loss: 0.00001355
Iteration 57/1000 | Loss: 0.00001355
Iteration 58/1000 | Loss: 0.00001354
Iteration 59/1000 | Loss: 0.00001354
Iteration 60/1000 | Loss: 0.00001354
Iteration 61/1000 | Loss: 0.00001354
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001353
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001351
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001351
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001350
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001349
Iteration 84/1000 | Loss: 0.00001349
Iteration 85/1000 | Loss: 0.00001349
Iteration 86/1000 | Loss: 0.00001348
Iteration 87/1000 | Loss: 0.00001348
Iteration 88/1000 | Loss: 0.00001348
Iteration 89/1000 | Loss: 0.00001348
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001345
Iteration 105/1000 | Loss: 0.00001345
Iteration 106/1000 | Loss: 0.00001345
Iteration 107/1000 | Loss: 0.00001344
Iteration 108/1000 | Loss: 0.00001344
Iteration 109/1000 | Loss: 0.00001344
Iteration 110/1000 | Loss: 0.00001344
Iteration 111/1000 | Loss: 0.00001344
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001344
Iteration 114/1000 | Loss: 0.00001343
Iteration 115/1000 | Loss: 0.00001343
Iteration 116/1000 | Loss: 0.00001343
Iteration 117/1000 | Loss: 0.00001343
Iteration 118/1000 | Loss: 0.00001343
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001342
Iteration 121/1000 | Loss: 0.00001342
Iteration 122/1000 | Loss: 0.00001342
Iteration 123/1000 | Loss: 0.00001342
Iteration 124/1000 | Loss: 0.00001342
Iteration 125/1000 | Loss: 0.00001342
Iteration 126/1000 | Loss: 0.00001342
Iteration 127/1000 | Loss: 0.00001342
Iteration 128/1000 | Loss: 0.00001342
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001342
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001342
Iteration 134/1000 | Loss: 0.00001342
Iteration 135/1000 | Loss: 0.00001342
Iteration 136/1000 | Loss: 0.00001342
Iteration 137/1000 | Loss: 0.00001342
Iteration 138/1000 | Loss: 0.00001342
Iteration 139/1000 | Loss: 0.00001342
Iteration 140/1000 | Loss: 0.00001342
Iteration 141/1000 | Loss: 0.00001342
Iteration 142/1000 | Loss: 0.00001342
Iteration 143/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.3421205039776396e-05, 1.3421205039776396e-05, 1.3421205039776396e-05, 1.3421205039776396e-05, 1.3421205039776396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3421205039776396e-05

Optimization complete. Final v2v error: 3.0980656147003174 mm

Highest mean error: 3.8825011253356934 mm for frame 75

Lowest mean error: 2.7951300144195557 mm for frame 96

Saving results

Total time: 35.58744215965271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822032
Iteration 2/25 | Loss: 0.00077998
Iteration 3/25 | Loss: 0.00063120
Iteration 4/25 | Loss: 0.00060539
Iteration 5/25 | Loss: 0.00059943
Iteration 6/25 | Loss: 0.00059802
Iteration 7/25 | Loss: 0.00059789
Iteration 8/25 | Loss: 0.00059789
Iteration 9/25 | Loss: 0.00059789
Iteration 10/25 | Loss: 0.00059789
Iteration 11/25 | Loss: 0.00059789
Iteration 12/25 | Loss: 0.00059789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005978886038064957, 0.0005978886038064957, 0.0005978886038064957, 0.0005978886038064957, 0.0005978886038064957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005978886038064957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46093976
Iteration 2/25 | Loss: 0.00027517
Iteration 3/25 | Loss: 0.00027517
Iteration 4/25 | Loss: 0.00027517
Iteration 5/25 | Loss: 0.00027517
Iteration 6/25 | Loss: 0.00027517
Iteration 7/25 | Loss: 0.00027517
Iteration 8/25 | Loss: 0.00027517
Iteration 9/25 | Loss: 0.00027517
Iteration 10/25 | Loss: 0.00027517
Iteration 11/25 | Loss: 0.00027517
Iteration 12/25 | Loss: 0.00027517
Iteration 13/25 | Loss: 0.00027517
Iteration 14/25 | Loss: 0.00027517
Iteration 15/25 | Loss: 0.00027517
Iteration 16/25 | Loss: 0.00027517
Iteration 17/25 | Loss: 0.00027517
Iteration 18/25 | Loss: 0.00027517
Iteration 19/25 | Loss: 0.00027517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00027516871341504157, 0.00027516871341504157, 0.00027516871341504157, 0.00027516871341504157, 0.00027516871341504157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027516871341504157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027517
Iteration 2/1000 | Loss: 0.00002171
Iteration 3/1000 | Loss: 0.00001344
Iteration 4/1000 | Loss: 0.00001233
Iteration 5/1000 | Loss: 0.00001161
Iteration 6/1000 | Loss: 0.00001122
Iteration 7/1000 | Loss: 0.00001095
Iteration 8/1000 | Loss: 0.00001080
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001074
Iteration 12/1000 | Loss: 0.00001071
Iteration 13/1000 | Loss: 0.00001070
Iteration 14/1000 | Loss: 0.00001070
Iteration 15/1000 | Loss: 0.00001069
Iteration 16/1000 | Loss: 0.00001068
Iteration 17/1000 | Loss: 0.00001064
Iteration 18/1000 | Loss: 0.00001062
Iteration 19/1000 | Loss: 0.00001062
Iteration 20/1000 | Loss: 0.00001061
Iteration 21/1000 | Loss: 0.00001060
Iteration 22/1000 | Loss: 0.00001059
Iteration 23/1000 | Loss: 0.00001059
Iteration 24/1000 | Loss: 0.00001058
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001057
Iteration 27/1000 | Loss: 0.00001057
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001057
Iteration 30/1000 | Loss: 0.00001057
Iteration 31/1000 | Loss: 0.00001054
Iteration 32/1000 | Loss: 0.00001051
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001050
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001049
Iteration 37/1000 | Loss: 0.00001049
Iteration 38/1000 | Loss: 0.00001049
Iteration 39/1000 | Loss: 0.00001048
Iteration 40/1000 | Loss: 0.00001048
Iteration 41/1000 | Loss: 0.00001047
Iteration 42/1000 | Loss: 0.00001047
Iteration 43/1000 | Loss: 0.00001047
Iteration 44/1000 | Loss: 0.00001046
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001045
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001043
Iteration 49/1000 | Loss: 0.00001043
Iteration 50/1000 | Loss: 0.00001043
Iteration 51/1000 | Loss: 0.00001043
Iteration 52/1000 | Loss: 0.00001042
Iteration 53/1000 | Loss: 0.00001042
Iteration 54/1000 | Loss: 0.00001042
Iteration 55/1000 | Loss: 0.00001042
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001039
Iteration 65/1000 | Loss: 0.00001039
Iteration 66/1000 | Loss: 0.00001039
Iteration 67/1000 | Loss: 0.00001038
Iteration 68/1000 | Loss: 0.00001038
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001038
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001038
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001033
Iteration 78/1000 | Loss: 0.00001033
Iteration 79/1000 | Loss: 0.00001032
Iteration 80/1000 | Loss: 0.00001031
Iteration 81/1000 | Loss: 0.00001031
Iteration 82/1000 | Loss: 0.00001031
Iteration 83/1000 | Loss: 0.00001031
Iteration 84/1000 | Loss: 0.00001030
Iteration 85/1000 | Loss: 0.00001030
Iteration 86/1000 | Loss: 0.00001030
Iteration 87/1000 | Loss: 0.00001030
Iteration 88/1000 | Loss: 0.00001030
Iteration 89/1000 | Loss: 0.00001030
Iteration 90/1000 | Loss: 0.00001030
Iteration 91/1000 | Loss: 0.00001030
Iteration 92/1000 | Loss: 0.00001030
Iteration 93/1000 | Loss: 0.00001029
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001029
Iteration 98/1000 | Loss: 0.00001029
Iteration 99/1000 | Loss: 0.00001029
Iteration 100/1000 | Loss: 0.00001029
Iteration 101/1000 | Loss: 0.00001029
Iteration 102/1000 | Loss: 0.00001029
Iteration 103/1000 | Loss: 0.00001029
Iteration 104/1000 | Loss: 0.00001029
Iteration 105/1000 | Loss: 0.00001029
Iteration 106/1000 | Loss: 0.00001029
Iteration 107/1000 | Loss: 0.00001028
Iteration 108/1000 | Loss: 0.00001028
Iteration 109/1000 | Loss: 0.00001028
Iteration 110/1000 | Loss: 0.00001028
Iteration 111/1000 | Loss: 0.00001028
Iteration 112/1000 | Loss: 0.00001028
Iteration 113/1000 | Loss: 0.00001028
Iteration 114/1000 | Loss: 0.00001028
Iteration 115/1000 | Loss: 0.00001028
Iteration 116/1000 | Loss: 0.00001028
Iteration 117/1000 | Loss: 0.00001028
Iteration 118/1000 | Loss: 0.00001028
Iteration 119/1000 | Loss: 0.00001028
Iteration 120/1000 | Loss: 0.00001028
Iteration 121/1000 | Loss: 0.00001027
Iteration 122/1000 | Loss: 0.00001027
Iteration 123/1000 | Loss: 0.00001027
Iteration 124/1000 | Loss: 0.00001027
Iteration 125/1000 | Loss: 0.00001027
Iteration 126/1000 | Loss: 0.00001026
Iteration 127/1000 | Loss: 0.00001026
Iteration 128/1000 | Loss: 0.00001026
Iteration 129/1000 | Loss: 0.00001026
Iteration 130/1000 | Loss: 0.00001026
Iteration 131/1000 | Loss: 0.00001026
Iteration 132/1000 | Loss: 0.00001026
Iteration 133/1000 | Loss: 0.00001025
Iteration 134/1000 | Loss: 0.00001025
Iteration 135/1000 | Loss: 0.00001025
Iteration 136/1000 | Loss: 0.00001025
Iteration 137/1000 | Loss: 0.00001025
Iteration 138/1000 | Loss: 0.00001024
Iteration 139/1000 | Loss: 0.00001024
Iteration 140/1000 | Loss: 0.00001024
Iteration 141/1000 | Loss: 0.00001024
Iteration 142/1000 | Loss: 0.00001024
Iteration 143/1000 | Loss: 0.00001024
Iteration 144/1000 | Loss: 0.00001024
Iteration 145/1000 | Loss: 0.00001024
Iteration 146/1000 | Loss: 0.00001024
Iteration 147/1000 | Loss: 0.00001024
Iteration 148/1000 | Loss: 0.00001024
Iteration 149/1000 | Loss: 0.00001024
Iteration 150/1000 | Loss: 0.00001024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.0237084097752813e-05, 1.0237084097752813e-05, 1.0237084097752813e-05, 1.0237084097752813e-05, 1.0237084097752813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0237084097752813e-05

Optimization complete. Final v2v error: 2.6909282207489014 mm

Highest mean error: 2.8531129360198975 mm for frame 85

Lowest mean error: 2.551126003265381 mm for frame 159

Saving results

Total time: 32.64440703392029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997316
Iteration 2/25 | Loss: 0.00248742
Iteration 3/25 | Loss: 0.00173219
Iteration 4/25 | Loss: 0.00122108
Iteration 5/25 | Loss: 0.00113446
Iteration 6/25 | Loss: 0.00111005
Iteration 7/25 | Loss: 0.00106050
Iteration 8/25 | Loss: 0.00102108
Iteration 9/25 | Loss: 0.00102842
Iteration 10/25 | Loss: 0.00099791
Iteration 11/25 | Loss: 0.00097696
Iteration 12/25 | Loss: 0.00096785
Iteration 13/25 | Loss: 0.00096054
Iteration 14/25 | Loss: 0.00095222
Iteration 15/25 | Loss: 0.00094988
Iteration 16/25 | Loss: 0.00094394
Iteration 17/25 | Loss: 0.00094069
Iteration 18/25 | Loss: 0.00093575
Iteration 19/25 | Loss: 0.00093737
Iteration 20/25 | Loss: 0.00093868
Iteration 21/25 | Loss: 0.00092984
Iteration 22/25 | Loss: 0.00092600
Iteration 23/25 | Loss: 0.00092324
Iteration 24/25 | Loss: 0.00092266
Iteration 25/25 | Loss: 0.00092177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.50980425
Iteration 2/25 | Loss: 0.00246702
Iteration 3/25 | Loss: 0.00246702
Iteration 4/25 | Loss: 0.00246701
Iteration 5/25 | Loss: 0.00246701
Iteration 6/25 | Loss: 0.00246701
Iteration 7/25 | Loss: 0.00246701
Iteration 8/25 | Loss: 0.00246701
Iteration 9/25 | Loss: 0.00246701
Iteration 10/25 | Loss: 0.00246701
Iteration 11/25 | Loss: 0.00246701
Iteration 12/25 | Loss: 0.00246701
Iteration 13/25 | Loss: 0.00246701
Iteration 14/25 | Loss: 0.00246701
Iteration 15/25 | Loss: 0.00246701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0024670131970196962, 0.0024670131970196962, 0.0024670131970196962, 0.0024670131970196962, 0.0024670131970196962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024670131970196962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246701
Iteration 2/1000 | Loss: 0.00211472
Iteration 3/1000 | Loss: 0.00458489
Iteration 4/1000 | Loss: 0.00297394
Iteration 5/1000 | Loss: 0.00298326
Iteration 6/1000 | Loss: 0.00039695
Iteration 7/1000 | Loss: 0.00111794
Iteration 8/1000 | Loss: 0.00066331
Iteration 9/1000 | Loss: 0.00017105
Iteration 10/1000 | Loss: 0.00084892
Iteration 11/1000 | Loss: 0.00094567
Iteration 12/1000 | Loss: 0.00011768
Iteration 13/1000 | Loss: 0.00009542
Iteration 14/1000 | Loss: 0.00109989
Iteration 15/1000 | Loss: 0.00230261
Iteration 16/1000 | Loss: 0.00090341
Iteration 17/1000 | Loss: 0.00119309
Iteration 18/1000 | Loss: 0.00131785
Iteration 19/1000 | Loss: 0.00090954
Iteration 20/1000 | Loss: 0.00109604
Iteration 21/1000 | Loss: 0.00091022
Iteration 22/1000 | Loss: 0.00125338
Iteration 23/1000 | Loss: 0.00058759
Iteration 24/1000 | Loss: 0.00118230
Iteration 25/1000 | Loss: 0.00104616
Iteration 26/1000 | Loss: 0.00062201
Iteration 27/1000 | Loss: 0.00321210
Iteration 28/1000 | Loss: 0.00364339
Iteration 29/1000 | Loss: 0.00168822
Iteration 30/1000 | Loss: 0.00085047
Iteration 31/1000 | Loss: 0.00042197
Iteration 32/1000 | Loss: 0.00009077
Iteration 33/1000 | Loss: 0.00058572
Iteration 34/1000 | Loss: 0.00143198
Iteration 35/1000 | Loss: 0.00123022
Iteration 36/1000 | Loss: 0.00030452
Iteration 37/1000 | Loss: 0.00059940
Iteration 38/1000 | Loss: 0.00156848
Iteration 39/1000 | Loss: 0.00160667
Iteration 40/1000 | Loss: 0.00011805
Iteration 41/1000 | Loss: 0.00106475
Iteration 42/1000 | Loss: 0.00106707
Iteration 43/1000 | Loss: 0.00122780
Iteration 44/1000 | Loss: 0.00180744
Iteration 45/1000 | Loss: 0.00140280
Iteration 46/1000 | Loss: 0.00078495
Iteration 47/1000 | Loss: 0.00047843
Iteration 48/1000 | Loss: 0.00013604
Iteration 49/1000 | Loss: 0.00043453
Iteration 50/1000 | Loss: 0.00078904
Iteration 51/1000 | Loss: 0.00112568
Iteration 52/1000 | Loss: 0.00060190
Iteration 53/1000 | Loss: 0.00044281
Iteration 54/1000 | Loss: 0.00241822
Iteration 55/1000 | Loss: 0.00118128
Iteration 56/1000 | Loss: 0.00101437
Iteration 57/1000 | Loss: 0.00054079
Iteration 58/1000 | Loss: 0.00020359
Iteration 59/1000 | Loss: 0.00032188
Iteration 60/1000 | Loss: 0.00073783
Iteration 61/1000 | Loss: 0.00042064
Iteration 62/1000 | Loss: 0.00005437
Iteration 63/1000 | Loss: 0.00032388
Iteration 64/1000 | Loss: 0.00048110
Iteration 65/1000 | Loss: 0.00005757
Iteration 66/1000 | Loss: 0.00004574
Iteration 67/1000 | Loss: 0.00004103
Iteration 68/1000 | Loss: 0.00069498
Iteration 69/1000 | Loss: 0.00005093
Iteration 70/1000 | Loss: 0.00003839
Iteration 71/1000 | Loss: 0.00004350
Iteration 72/1000 | Loss: 0.00002861
Iteration 73/1000 | Loss: 0.00002680
Iteration 74/1000 | Loss: 0.00002535
Iteration 75/1000 | Loss: 0.00002426
Iteration 76/1000 | Loss: 0.00002360
Iteration 77/1000 | Loss: 0.00002298
Iteration 78/1000 | Loss: 0.00002231
Iteration 79/1000 | Loss: 0.00002187
Iteration 80/1000 | Loss: 0.00002164
Iteration 81/1000 | Loss: 0.00002143
Iteration 82/1000 | Loss: 0.00002134
Iteration 83/1000 | Loss: 0.00002134
Iteration 84/1000 | Loss: 0.00002116
Iteration 85/1000 | Loss: 0.00002114
Iteration 86/1000 | Loss: 0.00002109
Iteration 87/1000 | Loss: 0.00002109
Iteration 88/1000 | Loss: 0.00002108
Iteration 89/1000 | Loss: 0.00002108
Iteration 90/1000 | Loss: 0.00002106
Iteration 91/1000 | Loss: 0.00002106
Iteration 92/1000 | Loss: 0.00002106
Iteration 93/1000 | Loss: 0.00002106
Iteration 94/1000 | Loss: 0.00002106
Iteration 95/1000 | Loss: 0.00002106
Iteration 96/1000 | Loss: 0.00002106
Iteration 97/1000 | Loss: 0.00002105
Iteration 98/1000 | Loss: 0.00002105
Iteration 99/1000 | Loss: 0.00002103
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002102
Iteration 102/1000 | Loss: 0.00002102
Iteration 103/1000 | Loss: 0.00002101
Iteration 104/1000 | Loss: 0.00002101
Iteration 105/1000 | Loss: 0.00002100
Iteration 106/1000 | Loss: 0.00002100
Iteration 107/1000 | Loss: 0.00002100
Iteration 108/1000 | Loss: 0.00002100
Iteration 109/1000 | Loss: 0.00002099
Iteration 110/1000 | Loss: 0.00002099
Iteration 111/1000 | Loss: 0.00002098
Iteration 112/1000 | Loss: 0.00002098
Iteration 113/1000 | Loss: 0.00002097
Iteration 114/1000 | Loss: 0.00002096
Iteration 115/1000 | Loss: 0.00002096
Iteration 116/1000 | Loss: 0.00002096
Iteration 117/1000 | Loss: 0.00002095
Iteration 118/1000 | Loss: 0.00002095
Iteration 119/1000 | Loss: 0.00002094
Iteration 120/1000 | Loss: 0.00002094
Iteration 121/1000 | Loss: 0.00002094
Iteration 122/1000 | Loss: 0.00002094
Iteration 123/1000 | Loss: 0.00002094
Iteration 124/1000 | Loss: 0.00002094
Iteration 125/1000 | Loss: 0.00002094
Iteration 126/1000 | Loss: 0.00002093
Iteration 127/1000 | Loss: 0.00002093
Iteration 128/1000 | Loss: 0.00002093
Iteration 129/1000 | Loss: 0.00002093
Iteration 130/1000 | Loss: 0.00002093
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002092
Iteration 133/1000 | Loss: 0.00002092
Iteration 134/1000 | Loss: 0.00002092
Iteration 135/1000 | Loss: 0.00002092
Iteration 136/1000 | Loss: 0.00002092
Iteration 137/1000 | Loss: 0.00002092
Iteration 138/1000 | Loss: 0.00002092
Iteration 139/1000 | Loss: 0.00002092
Iteration 140/1000 | Loss: 0.00002091
Iteration 141/1000 | Loss: 0.00002091
Iteration 142/1000 | Loss: 0.00002091
Iteration 143/1000 | Loss: 0.00002091
Iteration 144/1000 | Loss: 0.00002091
Iteration 145/1000 | Loss: 0.00002091
Iteration 146/1000 | Loss: 0.00002091
Iteration 147/1000 | Loss: 0.00002091
Iteration 148/1000 | Loss: 0.00002091
Iteration 149/1000 | Loss: 0.00002090
Iteration 150/1000 | Loss: 0.00002090
Iteration 151/1000 | Loss: 0.00002090
Iteration 152/1000 | Loss: 0.00002090
Iteration 153/1000 | Loss: 0.00002090
Iteration 154/1000 | Loss: 0.00002090
Iteration 155/1000 | Loss: 0.00002089
Iteration 156/1000 | Loss: 0.00002089
Iteration 157/1000 | Loss: 0.00002089
Iteration 158/1000 | Loss: 0.00002089
Iteration 159/1000 | Loss: 0.00002089
Iteration 160/1000 | Loss: 0.00002089
Iteration 161/1000 | Loss: 0.00002089
Iteration 162/1000 | Loss: 0.00002089
Iteration 163/1000 | Loss: 0.00002089
Iteration 164/1000 | Loss: 0.00002088
Iteration 165/1000 | Loss: 0.00002088
Iteration 166/1000 | Loss: 0.00002088
Iteration 167/1000 | Loss: 0.00002088
Iteration 168/1000 | Loss: 0.00002088
Iteration 169/1000 | Loss: 0.00002088
Iteration 170/1000 | Loss: 0.00002088
Iteration 171/1000 | Loss: 0.00002088
Iteration 172/1000 | Loss: 0.00002088
Iteration 173/1000 | Loss: 0.00002088
Iteration 174/1000 | Loss: 0.00002088
Iteration 175/1000 | Loss: 0.00002087
Iteration 176/1000 | Loss: 0.00002087
Iteration 177/1000 | Loss: 0.00002087
Iteration 178/1000 | Loss: 0.00002087
Iteration 179/1000 | Loss: 0.00002087
Iteration 180/1000 | Loss: 0.00002087
Iteration 181/1000 | Loss: 0.00002086
Iteration 182/1000 | Loss: 0.00002086
Iteration 183/1000 | Loss: 0.00002086
Iteration 184/1000 | Loss: 0.00002086
Iteration 185/1000 | Loss: 0.00002086
Iteration 186/1000 | Loss: 0.00002086
Iteration 187/1000 | Loss: 0.00002086
Iteration 188/1000 | Loss: 0.00002086
Iteration 189/1000 | Loss: 0.00002086
Iteration 190/1000 | Loss: 0.00002085
Iteration 191/1000 | Loss: 0.00002085
Iteration 192/1000 | Loss: 0.00002085
Iteration 193/1000 | Loss: 0.00002085
Iteration 194/1000 | Loss: 0.00002085
Iteration 195/1000 | Loss: 0.00002085
Iteration 196/1000 | Loss: 0.00002084
Iteration 197/1000 | Loss: 0.00002084
Iteration 198/1000 | Loss: 0.00002084
Iteration 199/1000 | Loss: 0.00002084
Iteration 200/1000 | Loss: 0.00002084
Iteration 201/1000 | Loss: 0.00002084
Iteration 202/1000 | Loss: 0.00002084
Iteration 203/1000 | Loss: 0.00002084
Iteration 204/1000 | Loss: 0.00002084
Iteration 205/1000 | Loss: 0.00002084
Iteration 206/1000 | Loss: 0.00002084
Iteration 207/1000 | Loss: 0.00002084
Iteration 208/1000 | Loss: 0.00002084
Iteration 209/1000 | Loss: 0.00002084
Iteration 210/1000 | Loss: 0.00002084
Iteration 211/1000 | Loss: 0.00002084
Iteration 212/1000 | Loss: 0.00002083
Iteration 213/1000 | Loss: 0.00002083
Iteration 214/1000 | Loss: 0.00002083
Iteration 215/1000 | Loss: 0.00002083
Iteration 216/1000 | Loss: 0.00002083
Iteration 217/1000 | Loss: 0.00002083
Iteration 218/1000 | Loss: 0.00002083
Iteration 219/1000 | Loss: 0.00007368
Iteration 220/1000 | Loss: 0.00004919
Iteration 221/1000 | Loss: 0.00002417
Iteration 222/1000 | Loss: 0.00016063
Iteration 223/1000 | Loss: 0.00011377
Iteration 224/1000 | Loss: 0.00003397
Iteration 225/1000 | Loss: 0.00006552
Iteration 226/1000 | Loss: 0.00019440
Iteration 227/1000 | Loss: 0.00015461
Iteration 228/1000 | Loss: 0.00020104
Iteration 229/1000 | Loss: 0.00005037
Iteration 230/1000 | Loss: 0.00013027
Iteration 231/1000 | Loss: 0.00017241
Iteration 232/1000 | Loss: 0.00018345
Iteration 233/1000 | Loss: 0.00018256
Iteration 234/1000 | Loss: 0.00012686
Iteration 235/1000 | Loss: 0.00005570
Iteration 236/1000 | Loss: 0.00010144
Iteration 237/1000 | Loss: 0.00016384
Iteration 238/1000 | Loss: 0.00012652
Iteration 239/1000 | Loss: 0.00009741
Iteration 240/1000 | Loss: 0.00017669
Iteration 241/1000 | Loss: 0.00012962
Iteration 242/1000 | Loss: 0.00011395
Iteration 243/1000 | Loss: 0.00017809
Iteration 244/1000 | Loss: 0.00016804
Iteration 245/1000 | Loss: 0.00008638
Iteration 246/1000 | Loss: 0.00016690
Iteration 247/1000 | Loss: 0.00017159
Iteration 248/1000 | Loss: 0.00017486
Iteration 249/1000 | Loss: 0.00003016
Iteration 250/1000 | Loss: 0.00002706
Iteration 251/1000 | Loss: 0.00008508
Iteration 252/1000 | Loss: 0.00009750
Iteration 253/1000 | Loss: 0.00002173
Iteration 254/1000 | Loss: 0.00002120
Iteration 255/1000 | Loss: 0.00002092
Iteration 256/1000 | Loss: 0.00002090
Iteration 257/1000 | Loss: 0.00002089
Iteration 258/1000 | Loss: 0.00002089
Iteration 259/1000 | Loss: 0.00002088
Iteration 260/1000 | Loss: 0.00002088
Iteration 261/1000 | Loss: 0.00002088
Iteration 262/1000 | Loss: 0.00002087
Iteration 263/1000 | Loss: 0.00002087
Iteration 264/1000 | Loss: 0.00002087
Iteration 265/1000 | Loss: 0.00002087
Iteration 266/1000 | Loss: 0.00002087
Iteration 267/1000 | Loss: 0.00002087
Iteration 268/1000 | Loss: 0.00002087
Iteration 269/1000 | Loss: 0.00002087
Iteration 270/1000 | Loss: 0.00002087
Iteration 271/1000 | Loss: 0.00002087
Iteration 272/1000 | Loss: 0.00002087
Iteration 273/1000 | Loss: 0.00002087
Iteration 274/1000 | Loss: 0.00002087
Iteration 275/1000 | Loss: 0.00002087
Iteration 276/1000 | Loss: 0.00002086
Iteration 277/1000 | Loss: 0.00002086
Iteration 278/1000 | Loss: 0.00002086
Iteration 279/1000 | Loss: 0.00002086
Iteration 280/1000 | Loss: 0.00002086
Iteration 281/1000 | Loss: 0.00002086
Iteration 282/1000 | Loss: 0.00002086
Iteration 283/1000 | Loss: 0.00002086
Iteration 284/1000 | Loss: 0.00002086
Iteration 285/1000 | Loss: 0.00002086
Iteration 286/1000 | Loss: 0.00002086
Iteration 287/1000 | Loss: 0.00002086
Iteration 288/1000 | Loss: 0.00002086
Iteration 289/1000 | Loss: 0.00002086
Iteration 290/1000 | Loss: 0.00002086
Iteration 291/1000 | Loss: 0.00002086
Iteration 292/1000 | Loss: 0.00002086
Iteration 293/1000 | Loss: 0.00002086
Iteration 294/1000 | Loss: 0.00002086
Iteration 295/1000 | Loss: 0.00002086
Iteration 296/1000 | Loss: 0.00002086
Iteration 297/1000 | Loss: 0.00002086
Iteration 298/1000 | Loss: 0.00002086
Iteration 299/1000 | Loss: 0.00002086
Iteration 300/1000 | Loss: 0.00002086
Iteration 301/1000 | Loss: 0.00002086
Iteration 302/1000 | Loss: 0.00002086
Iteration 303/1000 | Loss: 0.00002086
Iteration 304/1000 | Loss: 0.00002086
Iteration 305/1000 | Loss: 0.00002086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [2.0858746211160906e-05, 2.0858746211160906e-05, 2.0858746211160906e-05, 2.0858746211160906e-05, 2.0858746211160906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0858746211160906e-05

Optimization complete. Final v2v error: 3.652898073196411 mm

Highest mean error: 8.609068870544434 mm for frame 73

Lowest mean error: 2.8500943183898926 mm for frame 129

Saving results

Total time: 222.05310702323914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00606293
Iteration 2/25 | Loss: 0.00133894
Iteration 3/25 | Loss: 0.00081148
Iteration 4/25 | Loss: 0.00072457
Iteration 5/25 | Loss: 0.00071889
Iteration 6/25 | Loss: 0.00071811
Iteration 7/25 | Loss: 0.00071795
Iteration 8/25 | Loss: 0.00071795
Iteration 9/25 | Loss: 0.00071795
Iteration 10/25 | Loss: 0.00071795
Iteration 11/25 | Loss: 0.00071795
Iteration 12/25 | Loss: 0.00071795
Iteration 13/25 | Loss: 0.00071795
Iteration 14/25 | Loss: 0.00071795
Iteration 15/25 | Loss: 0.00071795
Iteration 16/25 | Loss: 0.00071795
Iteration 17/25 | Loss: 0.00071795
Iteration 18/25 | Loss: 0.00071795
Iteration 19/25 | Loss: 0.00071795
Iteration 20/25 | Loss: 0.00071795
Iteration 21/25 | Loss: 0.00071795
Iteration 22/25 | Loss: 0.00071795
Iteration 23/25 | Loss: 0.00071795
Iteration 24/25 | Loss: 0.00071795
Iteration 25/25 | Loss: 0.00071795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44999027
Iteration 2/25 | Loss: 0.00035251
Iteration 3/25 | Loss: 0.00035248
Iteration 4/25 | Loss: 0.00035248
Iteration 5/25 | Loss: 0.00035248
Iteration 6/25 | Loss: 0.00035248
Iteration 7/25 | Loss: 0.00035248
Iteration 8/25 | Loss: 0.00035248
Iteration 9/25 | Loss: 0.00035248
Iteration 10/25 | Loss: 0.00035248
Iteration 11/25 | Loss: 0.00035248
Iteration 12/25 | Loss: 0.00035248
Iteration 13/25 | Loss: 0.00035248
Iteration 14/25 | Loss: 0.00035248
Iteration 15/25 | Loss: 0.00035248
Iteration 16/25 | Loss: 0.00035248
Iteration 17/25 | Loss: 0.00035248
Iteration 18/25 | Loss: 0.00035248
Iteration 19/25 | Loss: 0.00035248
Iteration 20/25 | Loss: 0.00035248
Iteration 21/25 | Loss: 0.00035248
Iteration 22/25 | Loss: 0.00035248
Iteration 23/25 | Loss: 0.00035248
Iteration 24/25 | Loss: 0.00035248
Iteration 25/25 | Loss: 0.00035248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035248
Iteration 2/1000 | Loss: 0.00003710
Iteration 3/1000 | Loss: 0.00002244
Iteration 4/1000 | Loss: 0.00001849
Iteration 5/1000 | Loss: 0.00001713
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001600
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001571
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001568
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001566
Iteration 15/1000 | Loss: 0.00001566
Iteration 16/1000 | Loss: 0.00001566
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001566
Iteration 19/1000 | Loss: 0.00001565
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001565
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001564
Iteration 24/1000 | Loss: 0.00001560
Iteration 25/1000 | Loss: 0.00001556
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001541
Iteration 33/1000 | Loss: 0.00001541
Iteration 34/1000 | Loss: 0.00001540
Iteration 35/1000 | Loss: 0.00001540
Iteration 36/1000 | Loss: 0.00001540
Iteration 37/1000 | Loss: 0.00001540
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001539
Iteration 40/1000 | Loss: 0.00001538
Iteration 41/1000 | Loss: 0.00001538
Iteration 42/1000 | Loss: 0.00001538
Iteration 43/1000 | Loss: 0.00001538
Iteration 44/1000 | Loss: 0.00001538
Iteration 45/1000 | Loss: 0.00001538
Iteration 46/1000 | Loss: 0.00001538
Iteration 47/1000 | Loss: 0.00001538
Iteration 48/1000 | Loss: 0.00001536
Iteration 49/1000 | Loss: 0.00001536
Iteration 50/1000 | Loss: 0.00001536
Iteration 51/1000 | Loss: 0.00001536
Iteration 52/1000 | Loss: 0.00001536
Iteration 53/1000 | Loss: 0.00001536
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001535
Iteration 56/1000 | Loss: 0.00001534
Iteration 57/1000 | Loss: 0.00001534
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001534
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001534
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001533
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001533
Iteration 71/1000 | Loss: 0.00001533
Iteration 72/1000 | Loss: 0.00001532
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001530
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001528
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001527
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001526
Iteration 104/1000 | Loss: 0.00001526
Iteration 105/1000 | Loss: 0.00001526
Iteration 106/1000 | Loss: 0.00001526
Iteration 107/1000 | Loss: 0.00001526
Iteration 108/1000 | Loss: 0.00001526
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001526
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001525
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001524
Iteration 123/1000 | Loss: 0.00001524
Iteration 124/1000 | Loss: 0.00001524
Iteration 125/1000 | Loss: 0.00001524
Iteration 126/1000 | Loss: 0.00001524
Iteration 127/1000 | Loss: 0.00001524
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001524
Iteration 131/1000 | Loss: 0.00001523
Iteration 132/1000 | Loss: 0.00001523
Iteration 133/1000 | Loss: 0.00001523
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001523
Iteration 139/1000 | Loss: 0.00001523
Iteration 140/1000 | Loss: 0.00001523
Iteration 141/1000 | Loss: 0.00001523
Iteration 142/1000 | Loss: 0.00001523
Iteration 143/1000 | Loss: 0.00001523
Iteration 144/1000 | Loss: 0.00001523
Iteration 145/1000 | Loss: 0.00001523
Iteration 146/1000 | Loss: 0.00001523
Iteration 147/1000 | Loss: 0.00001523
Iteration 148/1000 | Loss: 0.00001523
Iteration 149/1000 | Loss: 0.00001523
Iteration 150/1000 | Loss: 0.00001523
Iteration 151/1000 | Loss: 0.00001523
Iteration 152/1000 | Loss: 0.00001523
Iteration 153/1000 | Loss: 0.00001523
Iteration 154/1000 | Loss: 0.00001523
Iteration 155/1000 | Loss: 0.00001523
Iteration 156/1000 | Loss: 0.00001523
Iteration 157/1000 | Loss: 0.00001523
Iteration 158/1000 | Loss: 0.00001523
Iteration 159/1000 | Loss: 0.00001523
Iteration 160/1000 | Loss: 0.00001523
Iteration 161/1000 | Loss: 0.00001523
Iteration 162/1000 | Loss: 0.00001523
Iteration 163/1000 | Loss: 0.00001523
Iteration 164/1000 | Loss: 0.00001523
Iteration 165/1000 | Loss: 0.00001523
Iteration 166/1000 | Loss: 0.00001523
Iteration 167/1000 | Loss: 0.00001523
Iteration 168/1000 | Loss: 0.00001523
Iteration 169/1000 | Loss: 0.00001523
Iteration 170/1000 | Loss: 0.00001523
Iteration 171/1000 | Loss: 0.00001523
Iteration 172/1000 | Loss: 0.00001523
Iteration 173/1000 | Loss: 0.00001523
Iteration 174/1000 | Loss: 0.00001523
Iteration 175/1000 | Loss: 0.00001523
Iteration 176/1000 | Loss: 0.00001523
Iteration 177/1000 | Loss: 0.00001523
Iteration 178/1000 | Loss: 0.00001523
Iteration 179/1000 | Loss: 0.00001523
Iteration 180/1000 | Loss: 0.00001523
Iteration 181/1000 | Loss: 0.00001523
Iteration 182/1000 | Loss: 0.00001523
Iteration 183/1000 | Loss: 0.00001523
Iteration 184/1000 | Loss: 0.00001523
Iteration 185/1000 | Loss: 0.00001523
Iteration 186/1000 | Loss: 0.00001523
Iteration 187/1000 | Loss: 0.00001523
Iteration 188/1000 | Loss: 0.00001523
Iteration 189/1000 | Loss: 0.00001523
Iteration 190/1000 | Loss: 0.00001523
Iteration 191/1000 | Loss: 0.00001523
Iteration 192/1000 | Loss: 0.00001523
Iteration 193/1000 | Loss: 0.00001523
Iteration 194/1000 | Loss: 0.00001523
Iteration 195/1000 | Loss: 0.00001523
Iteration 196/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.523307764728088e-05, 1.523307764728088e-05, 1.523307764728088e-05, 1.523307764728088e-05, 1.523307764728088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.523307764728088e-05

Optimization complete. Final v2v error: 3.321911334991455 mm

Highest mean error: 3.509518623352051 mm for frame 11

Lowest mean error: 2.991461753845215 mm for frame 5

Saving results

Total time: 34.23241353034973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810864
Iteration 2/25 | Loss: 0.00106229
Iteration 3/25 | Loss: 0.00078350
Iteration 4/25 | Loss: 0.00071404
Iteration 5/25 | Loss: 0.00069608
Iteration 6/25 | Loss: 0.00069125
Iteration 7/25 | Loss: 0.00069072
Iteration 8/25 | Loss: 0.00069072
Iteration 9/25 | Loss: 0.00069072
Iteration 10/25 | Loss: 0.00069072
Iteration 11/25 | Loss: 0.00069072
Iteration 12/25 | Loss: 0.00069072
Iteration 13/25 | Loss: 0.00069072
Iteration 14/25 | Loss: 0.00069072
Iteration 15/25 | Loss: 0.00069072
Iteration 16/25 | Loss: 0.00069072
Iteration 17/25 | Loss: 0.00069072
Iteration 18/25 | Loss: 0.00069072
Iteration 19/25 | Loss: 0.00069072
Iteration 20/25 | Loss: 0.00069072
Iteration 21/25 | Loss: 0.00069072
Iteration 22/25 | Loss: 0.00069072
Iteration 23/25 | Loss: 0.00069072
Iteration 24/25 | Loss: 0.00069072
Iteration 25/25 | Loss: 0.00069072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45450771
Iteration 2/25 | Loss: 0.00035982
Iteration 3/25 | Loss: 0.00035982
Iteration 4/25 | Loss: 0.00035981
Iteration 5/25 | Loss: 0.00035981
Iteration 6/25 | Loss: 0.00035981
Iteration 7/25 | Loss: 0.00035981
Iteration 8/25 | Loss: 0.00035981
Iteration 9/25 | Loss: 0.00035981
Iteration 10/25 | Loss: 0.00035981
Iteration 11/25 | Loss: 0.00035981
Iteration 12/25 | Loss: 0.00035981
Iteration 13/25 | Loss: 0.00035981
Iteration 14/25 | Loss: 0.00035981
Iteration 15/25 | Loss: 0.00035981
Iteration 16/25 | Loss: 0.00035981
Iteration 17/25 | Loss: 0.00035981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000359812198439613, 0.000359812198439613, 0.000359812198439613, 0.000359812198439613, 0.000359812198439613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000359812198439613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035981
Iteration 2/1000 | Loss: 0.00003567
Iteration 3/1000 | Loss: 0.00002650
Iteration 4/1000 | Loss: 0.00002470
Iteration 5/1000 | Loss: 0.00002327
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002183
Iteration 8/1000 | Loss: 0.00002134
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002050
Iteration 13/1000 | Loss: 0.00002047
Iteration 14/1000 | Loss: 0.00002035
Iteration 15/1000 | Loss: 0.00002032
Iteration 16/1000 | Loss: 0.00002032
Iteration 17/1000 | Loss: 0.00002032
Iteration 18/1000 | Loss: 0.00002029
Iteration 19/1000 | Loss: 0.00002029
Iteration 20/1000 | Loss: 0.00002028
Iteration 21/1000 | Loss: 0.00002028
Iteration 22/1000 | Loss: 0.00002028
Iteration 23/1000 | Loss: 0.00002028
Iteration 24/1000 | Loss: 0.00002028
Iteration 25/1000 | Loss: 0.00002027
Iteration 26/1000 | Loss: 0.00002027
Iteration 27/1000 | Loss: 0.00002027
Iteration 28/1000 | Loss: 0.00002027
Iteration 29/1000 | Loss: 0.00002027
Iteration 30/1000 | Loss: 0.00002026
Iteration 31/1000 | Loss: 0.00002026
Iteration 32/1000 | Loss: 0.00002026
Iteration 33/1000 | Loss: 0.00002025
Iteration 34/1000 | Loss: 0.00002025
Iteration 35/1000 | Loss: 0.00002025
Iteration 36/1000 | Loss: 0.00002024
Iteration 37/1000 | Loss: 0.00002024
Iteration 38/1000 | Loss: 0.00002024
Iteration 39/1000 | Loss: 0.00002024
Iteration 40/1000 | Loss: 0.00002024
Iteration 41/1000 | Loss: 0.00002024
Iteration 42/1000 | Loss: 0.00002023
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002023
Iteration 45/1000 | Loss: 0.00002022
Iteration 46/1000 | Loss: 0.00002022
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002021
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002020
Iteration 54/1000 | Loss: 0.00002020
Iteration 55/1000 | Loss: 0.00002019
Iteration 56/1000 | Loss: 0.00002019
Iteration 57/1000 | Loss: 0.00002018
Iteration 58/1000 | Loss: 0.00002018
Iteration 59/1000 | Loss: 0.00002018
Iteration 60/1000 | Loss: 0.00002018
Iteration 61/1000 | Loss: 0.00002018
Iteration 62/1000 | Loss: 0.00002018
Iteration 63/1000 | Loss: 0.00002018
Iteration 64/1000 | Loss: 0.00002018
Iteration 65/1000 | Loss: 0.00002018
Iteration 66/1000 | Loss: 0.00002017
Iteration 67/1000 | Loss: 0.00002016
Iteration 68/1000 | Loss: 0.00002016
Iteration 69/1000 | Loss: 0.00002015
Iteration 70/1000 | Loss: 0.00002015
Iteration 71/1000 | Loss: 0.00002015
Iteration 72/1000 | Loss: 0.00002015
Iteration 73/1000 | Loss: 0.00002015
Iteration 74/1000 | Loss: 0.00002014
Iteration 75/1000 | Loss: 0.00002014
Iteration 76/1000 | Loss: 0.00002014
Iteration 77/1000 | Loss: 0.00002013
Iteration 78/1000 | Loss: 0.00002013
Iteration 79/1000 | Loss: 0.00002013
Iteration 80/1000 | Loss: 0.00002013
Iteration 81/1000 | Loss: 0.00002012
Iteration 82/1000 | Loss: 0.00002012
Iteration 83/1000 | Loss: 0.00002012
Iteration 84/1000 | Loss: 0.00002012
Iteration 85/1000 | Loss: 0.00002012
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002011
Iteration 90/1000 | Loss: 0.00002011
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002011
Iteration 93/1000 | Loss: 0.00002010
Iteration 94/1000 | Loss: 0.00002010
Iteration 95/1000 | Loss: 0.00002010
Iteration 96/1000 | Loss: 0.00002010
Iteration 97/1000 | Loss: 0.00002010
Iteration 98/1000 | Loss: 0.00002010
Iteration 99/1000 | Loss: 0.00002010
Iteration 100/1000 | Loss: 0.00002010
Iteration 101/1000 | Loss: 0.00002010
Iteration 102/1000 | Loss: 0.00002010
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00002009
Iteration 105/1000 | Loss: 0.00002009
Iteration 106/1000 | Loss: 0.00002009
Iteration 107/1000 | Loss: 0.00002009
Iteration 108/1000 | Loss: 0.00002009
Iteration 109/1000 | Loss: 0.00002009
Iteration 110/1000 | Loss: 0.00002009
Iteration 111/1000 | Loss: 0.00002009
Iteration 112/1000 | Loss: 0.00002008
Iteration 113/1000 | Loss: 0.00002008
Iteration 114/1000 | Loss: 0.00002008
Iteration 115/1000 | Loss: 0.00002008
Iteration 116/1000 | Loss: 0.00002008
Iteration 117/1000 | Loss: 0.00002008
Iteration 118/1000 | Loss: 0.00002008
Iteration 119/1000 | Loss: 0.00002008
Iteration 120/1000 | Loss: 0.00002008
Iteration 121/1000 | Loss: 0.00002008
Iteration 122/1000 | Loss: 0.00002008
Iteration 123/1000 | Loss: 0.00002007
Iteration 124/1000 | Loss: 0.00002007
Iteration 125/1000 | Loss: 0.00002007
Iteration 126/1000 | Loss: 0.00002007
Iteration 127/1000 | Loss: 0.00002007
Iteration 128/1000 | Loss: 0.00002007
Iteration 129/1000 | Loss: 0.00002007
Iteration 130/1000 | Loss: 0.00002006
Iteration 131/1000 | Loss: 0.00002006
Iteration 132/1000 | Loss: 0.00002006
Iteration 133/1000 | Loss: 0.00002006
Iteration 134/1000 | Loss: 0.00002006
Iteration 135/1000 | Loss: 0.00002006
Iteration 136/1000 | Loss: 0.00002006
Iteration 137/1000 | Loss: 0.00002005
Iteration 138/1000 | Loss: 0.00002005
Iteration 139/1000 | Loss: 0.00002005
Iteration 140/1000 | Loss: 0.00002005
Iteration 141/1000 | Loss: 0.00002005
Iteration 142/1000 | Loss: 0.00002005
Iteration 143/1000 | Loss: 0.00002005
Iteration 144/1000 | Loss: 0.00002005
Iteration 145/1000 | Loss: 0.00002005
Iteration 146/1000 | Loss: 0.00002005
Iteration 147/1000 | Loss: 0.00002004
Iteration 148/1000 | Loss: 0.00002004
Iteration 149/1000 | Loss: 0.00002004
Iteration 150/1000 | Loss: 0.00002004
Iteration 151/1000 | Loss: 0.00002004
Iteration 152/1000 | Loss: 0.00002004
Iteration 153/1000 | Loss: 0.00002004
Iteration 154/1000 | Loss: 0.00002004
Iteration 155/1000 | Loss: 0.00002004
Iteration 156/1000 | Loss: 0.00002004
Iteration 157/1000 | Loss: 0.00002004
Iteration 158/1000 | Loss: 0.00002004
Iteration 159/1000 | Loss: 0.00002004
Iteration 160/1000 | Loss: 0.00002004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.003930785576813e-05, 2.003930785576813e-05, 2.003930785576813e-05, 2.003930785576813e-05, 2.003930785576813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.003930785576813e-05

Optimization complete. Final v2v error: 3.770622968673706 mm

Highest mean error: 4.474778652191162 mm for frame 155

Lowest mean error: 3.3642327785491943 mm for frame 236

Saving results

Total time: 43.473751068115234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914517
Iteration 2/25 | Loss: 0.00133022
Iteration 3/25 | Loss: 0.00087870
Iteration 4/25 | Loss: 0.00081647
Iteration 5/25 | Loss: 0.00072710
Iteration 6/25 | Loss: 0.00069892
Iteration 7/25 | Loss: 0.00068340
Iteration 8/25 | Loss: 0.00067299
Iteration 9/25 | Loss: 0.00067196
Iteration 10/25 | Loss: 0.00067143
Iteration 11/25 | Loss: 0.00067073
Iteration 12/25 | Loss: 0.00067012
Iteration 13/25 | Loss: 0.00067210
Iteration 14/25 | Loss: 0.00066872
Iteration 15/25 | Loss: 0.00066739
Iteration 16/25 | Loss: 0.00066683
Iteration 17/25 | Loss: 0.00066651
Iteration 18/25 | Loss: 0.00066648
Iteration 19/25 | Loss: 0.00066648
Iteration 20/25 | Loss: 0.00066647
Iteration 21/25 | Loss: 0.00066647
Iteration 22/25 | Loss: 0.00066647
Iteration 23/25 | Loss: 0.00066647
Iteration 24/25 | Loss: 0.00066647
Iteration 25/25 | Loss: 0.00066647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56081200
Iteration 2/25 | Loss: 0.00029221
Iteration 3/25 | Loss: 0.00029221
Iteration 4/25 | Loss: 0.00029221
Iteration 5/25 | Loss: 0.00029221
Iteration 6/25 | Loss: 0.00029221
Iteration 7/25 | Loss: 0.00029221
Iteration 8/25 | Loss: 0.00029221
Iteration 9/25 | Loss: 0.00029221
Iteration 10/25 | Loss: 0.00029221
Iteration 11/25 | Loss: 0.00029221
Iteration 12/25 | Loss: 0.00029221
Iteration 13/25 | Loss: 0.00029221
Iteration 14/25 | Loss: 0.00029221
Iteration 15/25 | Loss: 0.00029221
Iteration 16/25 | Loss: 0.00029221
Iteration 17/25 | Loss: 0.00029221
Iteration 18/25 | Loss: 0.00029221
Iteration 19/25 | Loss: 0.00029221
Iteration 20/25 | Loss: 0.00029221
Iteration 21/25 | Loss: 0.00029221
Iteration 22/25 | Loss: 0.00029221
Iteration 23/25 | Loss: 0.00029221
Iteration 24/25 | Loss: 0.00029221
Iteration 25/25 | Loss: 0.00029221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029221
Iteration 2/1000 | Loss: 0.00002413
Iteration 3/1000 | Loss: 0.00001943
Iteration 4/1000 | Loss: 0.00001828
Iteration 5/1000 | Loss: 0.00001755
Iteration 6/1000 | Loss: 0.00001712
Iteration 7/1000 | Loss: 0.00001681
Iteration 8/1000 | Loss: 0.00001664
Iteration 9/1000 | Loss: 0.00001663
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001645
Iteration 12/1000 | Loss: 0.00001642
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001631
Iteration 15/1000 | Loss: 0.00001631
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001630
Iteration 18/1000 | Loss: 0.00001630
Iteration 19/1000 | Loss: 0.00001625
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001624
Iteration 22/1000 | Loss: 0.00001622
Iteration 23/1000 | Loss: 0.00001621
Iteration 24/1000 | Loss: 0.00001621
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001618
Iteration 28/1000 | Loss: 0.00001617
Iteration 29/1000 | Loss: 0.00001616
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001616
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001615
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001615
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001614
Iteration 40/1000 | Loss: 0.00001613
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001612
Iteration 44/1000 | Loss: 0.00001611
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001611
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001608
Iteration 56/1000 | Loss: 0.00001608
Iteration 57/1000 | Loss: 0.00001607
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001605
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001605
Iteration 64/1000 | Loss: 0.00001605
Iteration 65/1000 | Loss: 0.00001604
Iteration 66/1000 | Loss: 0.00001604
Iteration 67/1000 | Loss: 0.00001604
Iteration 68/1000 | Loss: 0.00001603
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001602
Iteration 71/1000 | Loss: 0.00001601
Iteration 72/1000 | Loss: 0.00001601
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001601
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001600
Iteration 79/1000 | Loss: 0.00001600
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001599
Iteration 82/1000 | Loss: 0.00001598
Iteration 83/1000 | Loss: 0.00001598
Iteration 84/1000 | Loss: 0.00001598
Iteration 85/1000 | Loss: 0.00001598
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001597
Iteration 89/1000 | Loss: 0.00001597
Iteration 90/1000 | Loss: 0.00001597
Iteration 91/1000 | Loss: 0.00001597
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001596
Iteration 95/1000 | Loss: 0.00001596
Iteration 96/1000 | Loss: 0.00001596
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001595
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Iteration 101/1000 | Loss: 0.00001595
Iteration 102/1000 | Loss: 0.00001595
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001594
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001594
Iteration 117/1000 | Loss: 0.00001594
Iteration 118/1000 | Loss: 0.00001594
Iteration 119/1000 | Loss: 0.00001594
Iteration 120/1000 | Loss: 0.00001594
Iteration 121/1000 | Loss: 0.00001594
Iteration 122/1000 | Loss: 0.00001594
Iteration 123/1000 | Loss: 0.00001594
Iteration 124/1000 | Loss: 0.00001594
Iteration 125/1000 | Loss: 0.00001594
Iteration 126/1000 | Loss: 0.00001594
Iteration 127/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5938032447593287e-05, 1.5938032447593287e-05, 1.5938032447593287e-05, 1.5938032447593287e-05, 1.5938032447593287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5938032447593287e-05

Optimization complete. Final v2v error: 3.3193700313568115 mm

Highest mean error: 3.73403263092041 mm for frame 200

Lowest mean error: 2.793456792831421 mm for frame 0

Saving results

Total time: 61.37033653259277
