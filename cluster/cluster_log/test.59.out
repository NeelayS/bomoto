Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=59, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3304-3359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490172
Iteration 2/25 | Loss: 0.00143131
Iteration 3/25 | Loss: 0.00136418
Iteration 4/25 | Loss: 0.00135351
Iteration 5/25 | Loss: 0.00134913
Iteration 6/25 | Loss: 0.00134909
Iteration 7/25 | Loss: 0.00134909
Iteration 8/25 | Loss: 0.00134909
Iteration 9/25 | Loss: 0.00134909
Iteration 10/25 | Loss: 0.00134909
Iteration 11/25 | Loss: 0.00134909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013490905985236168, 0.0013490905985236168, 0.0013490905985236168, 0.0013490905985236168, 0.0013490905985236168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013490905985236168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46461260
Iteration 2/25 | Loss: 0.00090327
Iteration 3/25 | Loss: 0.00090325
Iteration 4/25 | Loss: 0.00090325
Iteration 5/25 | Loss: 0.00090325
Iteration 6/25 | Loss: 0.00090325
Iteration 7/25 | Loss: 0.00090325
Iteration 8/25 | Loss: 0.00090325
Iteration 9/25 | Loss: 0.00090325
Iteration 10/25 | Loss: 0.00090325
Iteration 11/25 | Loss: 0.00090325
Iteration 12/25 | Loss: 0.00090325
Iteration 13/25 | Loss: 0.00090325
Iteration 14/25 | Loss: 0.00090325
Iteration 15/25 | Loss: 0.00090325
Iteration 16/25 | Loss: 0.00090325
Iteration 17/25 | Loss: 0.00090325
Iteration 18/25 | Loss: 0.00090325
Iteration 19/25 | Loss: 0.00090325
Iteration 20/25 | Loss: 0.00090325
Iteration 21/25 | Loss: 0.00090325
Iteration 22/25 | Loss: 0.00090325
Iteration 23/25 | Loss: 0.00090325
Iteration 24/25 | Loss: 0.00090325
Iteration 25/25 | Loss: 0.00090325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090325
Iteration 2/1000 | Loss: 0.00004119
Iteration 3/1000 | Loss: 0.00002402
Iteration 4/1000 | Loss: 0.00002069
Iteration 5/1000 | Loss: 0.00001958
Iteration 6/1000 | Loss: 0.00001886
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001814
Iteration 9/1000 | Loss: 0.00001782
Iteration 10/1000 | Loss: 0.00001751
Iteration 11/1000 | Loss: 0.00001732
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001718
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001699
Iteration 17/1000 | Loss: 0.00001685
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001684
Iteration 20/1000 | Loss: 0.00001678
Iteration 21/1000 | Loss: 0.00001677
Iteration 22/1000 | Loss: 0.00001677
Iteration 23/1000 | Loss: 0.00001677
Iteration 24/1000 | Loss: 0.00001677
Iteration 25/1000 | Loss: 0.00001677
Iteration 26/1000 | Loss: 0.00001671
Iteration 27/1000 | Loss: 0.00001670
Iteration 28/1000 | Loss: 0.00001669
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001660
Iteration 32/1000 | Loss: 0.00001656
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001648
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001640
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001637
Iteration 41/1000 | Loss: 0.00001636
Iteration 42/1000 | Loss: 0.00001635
Iteration 43/1000 | Loss: 0.00001635
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001635
Iteration 46/1000 | Loss: 0.00001635
Iteration 47/1000 | Loss: 0.00001634
Iteration 48/1000 | Loss: 0.00001634
Iteration 49/1000 | Loss: 0.00001634
Iteration 50/1000 | Loss: 0.00001634
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001634
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001634
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001632
Iteration 59/1000 | Loss: 0.00001632
Iteration 60/1000 | Loss: 0.00001632
Iteration 61/1000 | Loss: 0.00001632
Iteration 62/1000 | Loss: 0.00001632
Iteration 63/1000 | Loss: 0.00001632
Iteration 64/1000 | Loss: 0.00001631
Iteration 65/1000 | Loss: 0.00001631
Iteration 66/1000 | Loss: 0.00001631
Iteration 67/1000 | Loss: 0.00001631
Iteration 68/1000 | Loss: 0.00001631
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001629
Iteration 78/1000 | Loss: 0.00001629
Iteration 79/1000 | Loss: 0.00001628
Iteration 80/1000 | Loss: 0.00001628
Iteration 81/1000 | Loss: 0.00001628
Iteration 82/1000 | Loss: 0.00001627
Iteration 83/1000 | Loss: 0.00001627
Iteration 84/1000 | Loss: 0.00001627
Iteration 85/1000 | Loss: 0.00001627
Iteration 86/1000 | Loss: 0.00001626
Iteration 87/1000 | Loss: 0.00001626
Iteration 88/1000 | Loss: 0.00001626
Iteration 89/1000 | Loss: 0.00001626
Iteration 90/1000 | Loss: 0.00001626
Iteration 91/1000 | Loss: 0.00001626
Iteration 92/1000 | Loss: 0.00001626
Iteration 93/1000 | Loss: 0.00001625
Iteration 94/1000 | Loss: 0.00001625
Iteration 95/1000 | Loss: 0.00001625
Iteration 96/1000 | Loss: 0.00001624
Iteration 97/1000 | Loss: 0.00001624
Iteration 98/1000 | Loss: 0.00001624
Iteration 99/1000 | Loss: 0.00001624
Iteration 100/1000 | Loss: 0.00001624
Iteration 101/1000 | Loss: 0.00001624
Iteration 102/1000 | Loss: 0.00001624
Iteration 103/1000 | Loss: 0.00001624
Iteration 104/1000 | Loss: 0.00001624
Iteration 105/1000 | Loss: 0.00001624
Iteration 106/1000 | Loss: 0.00001623
Iteration 107/1000 | Loss: 0.00001623
Iteration 108/1000 | Loss: 0.00001623
Iteration 109/1000 | Loss: 0.00001623
Iteration 110/1000 | Loss: 0.00001623
Iteration 111/1000 | Loss: 0.00001623
Iteration 112/1000 | Loss: 0.00001623
Iteration 113/1000 | Loss: 0.00001623
Iteration 114/1000 | Loss: 0.00001623
Iteration 115/1000 | Loss: 0.00001623
Iteration 116/1000 | Loss: 0.00001623
Iteration 117/1000 | Loss: 0.00001623
Iteration 118/1000 | Loss: 0.00001622
Iteration 119/1000 | Loss: 0.00001622
Iteration 120/1000 | Loss: 0.00001622
Iteration 121/1000 | Loss: 0.00001622
Iteration 122/1000 | Loss: 0.00001622
Iteration 123/1000 | Loss: 0.00001622
Iteration 124/1000 | Loss: 0.00001622
Iteration 125/1000 | Loss: 0.00001622
Iteration 126/1000 | Loss: 0.00001621
Iteration 127/1000 | Loss: 0.00001621
Iteration 128/1000 | Loss: 0.00001621
Iteration 129/1000 | Loss: 0.00001621
Iteration 130/1000 | Loss: 0.00001621
Iteration 131/1000 | Loss: 0.00001621
Iteration 132/1000 | Loss: 0.00001621
Iteration 133/1000 | Loss: 0.00001621
Iteration 134/1000 | Loss: 0.00001621
Iteration 135/1000 | Loss: 0.00001620
Iteration 136/1000 | Loss: 0.00001620
Iteration 137/1000 | Loss: 0.00001620
Iteration 138/1000 | Loss: 0.00001620
Iteration 139/1000 | Loss: 0.00001620
Iteration 140/1000 | Loss: 0.00001620
Iteration 141/1000 | Loss: 0.00001620
Iteration 142/1000 | Loss: 0.00001620
Iteration 143/1000 | Loss: 0.00001620
Iteration 144/1000 | Loss: 0.00001620
Iteration 145/1000 | Loss: 0.00001620
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001619
Iteration 151/1000 | Loss: 0.00001619
Iteration 152/1000 | Loss: 0.00001619
Iteration 153/1000 | Loss: 0.00001619
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001619
Iteration 158/1000 | Loss: 0.00001619
Iteration 159/1000 | Loss: 0.00001618
Iteration 160/1000 | Loss: 0.00001618
Iteration 161/1000 | Loss: 0.00001618
Iteration 162/1000 | Loss: 0.00001618
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001617
Iteration 167/1000 | Loss: 0.00001617
Iteration 168/1000 | Loss: 0.00001617
Iteration 169/1000 | Loss: 0.00001617
Iteration 170/1000 | Loss: 0.00001617
Iteration 171/1000 | Loss: 0.00001617
Iteration 172/1000 | Loss: 0.00001617
Iteration 173/1000 | Loss: 0.00001617
Iteration 174/1000 | Loss: 0.00001617
Iteration 175/1000 | Loss: 0.00001617
Iteration 176/1000 | Loss: 0.00001617
Iteration 177/1000 | Loss: 0.00001616
Iteration 178/1000 | Loss: 0.00001616
Iteration 179/1000 | Loss: 0.00001616
Iteration 180/1000 | Loss: 0.00001616
Iteration 181/1000 | Loss: 0.00001616
Iteration 182/1000 | Loss: 0.00001616
Iteration 183/1000 | Loss: 0.00001616
Iteration 184/1000 | Loss: 0.00001616
Iteration 185/1000 | Loss: 0.00001616
Iteration 186/1000 | Loss: 0.00001616
Iteration 187/1000 | Loss: 0.00001616
Iteration 188/1000 | Loss: 0.00001616
Iteration 189/1000 | Loss: 0.00001616
Iteration 190/1000 | Loss: 0.00001616
Iteration 191/1000 | Loss: 0.00001616
Iteration 192/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.6157740901689976e-05, 1.6157740901689976e-05, 1.6157740901689976e-05, 1.6157740901689976e-05, 1.6157740901689976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6157740901689976e-05

Optimization complete. Final v2v error: 3.3938770294189453 mm

Highest mean error: 3.834027051925659 mm for frame 104

Lowest mean error: 3.189884901046753 mm for frame 224

Saving results

Total time: 49.55017304420471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983599
Iteration 2/25 | Loss: 0.00983599
Iteration 3/25 | Loss: 0.00983599
Iteration 4/25 | Loss: 0.00983599
Iteration 5/25 | Loss: 0.00983599
Iteration 6/25 | Loss: 0.00270789
Iteration 7/25 | Loss: 0.00191765
Iteration 8/25 | Loss: 0.00183804
Iteration 9/25 | Loss: 0.00158853
Iteration 10/25 | Loss: 0.00154496
Iteration 11/25 | Loss: 0.00148646
Iteration 12/25 | Loss: 0.00144047
Iteration 13/25 | Loss: 0.00142664
Iteration 14/25 | Loss: 0.00139668
Iteration 15/25 | Loss: 0.00137741
Iteration 16/25 | Loss: 0.00137084
Iteration 17/25 | Loss: 0.00136915
Iteration 18/25 | Loss: 0.00136662
Iteration 19/25 | Loss: 0.00136853
Iteration 20/25 | Loss: 0.00137046
Iteration 21/25 | Loss: 0.00136621
Iteration 22/25 | Loss: 0.00136387
Iteration 23/25 | Loss: 0.00136258
Iteration 24/25 | Loss: 0.00135898
Iteration 25/25 | Loss: 0.00135964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39865100
Iteration 2/25 | Loss: 0.00114347
Iteration 3/25 | Loss: 0.00114347
Iteration 4/25 | Loss: 0.00114347
Iteration 5/25 | Loss: 0.00114347
Iteration 6/25 | Loss: 0.00114347
Iteration 7/25 | Loss: 0.00114346
Iteration 8/25 | Loss: 0.00114346
Iteration 9/25 | Loss: 0.00114346
Iteration 10/25 | Loss: 0.00114346
Iteration 11/25 | Loss: 0.00109877
Iteration 12/25 | Loss: 0.00109877
Iteration 13/25 | Loss: 0.00109877
Iteration 14/25 | Loss: 0.00109877
Iteration 15/25 | Loss: 0.00109877
Iteration 16/25 | Loss: 0.00109877
Iteration 17/25 | Loss: 0.00109877
Iteration 18/25 | Loss: 0.00109877
Iteration 19/25 | Loss: 0.00109877
Iteration 20/25 | Loss: 0.00109877
Iteration 21/25 | Loss: 0.00109877
Iteration 22/25 | Loss: 0.00109877
Iteration 23/25 | Loss: 0.00109877
Iteration 24/25 | Loss: 0.00109877
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010987683199346066, 0.0010987683199346066, 0.0010987683199346066, 0.0010987683199346066, 0.0010987683199346066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010987683199346066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109877
Iteration 2/1000 | Loss: 0.00011034
Iteration 3/1000 | Loss: 0.00079162
Iteration 4/1000 | Loss: 0.00069586
Iteration 5/1000 | Loss: 0.00059635
Iteration 6/1000 | Loss: 0.00050732
Iteration 7/1000 | Loss: 0.00047289
Iteration 8/1000 | Loss: 0.00026421
Iteration 9/1000 | Loss: 0.00023365
Iteration 10/1000 | Loss: 0.00007314
Iteration 11/1000 | Loss: 0.00011991
Iteration 12/1000 | Loss: 0.00015660
Iteration 13/1000 | Loss: 0.00007856
Iteration 14/1000 | Loss: 0.00027891
Iteration 15/1000 | Loss: 0.00031018
Iteration 16/1000 | Loss: 0.00018624
Iteration 17/1000 | Loss: 0.00017963
Iteration 18/1000 | Loss: 0.00011747
Iteration 19/1000 | Loss: 0.00008867
Iteration 20/1000 | Loss: 0.00003710
Iteration 21/1000 | Loss: 0.00003522
Iteration 22/1000 | Loss: 0.00015321
Iteration 23/1000 | Loss: 0.00007299
Iteration 24/1000 | Loss: 0.00003693
Iteration 25/1000 | Loss: 0.00004841
Iteration 26/1000 | Loss: 0.00004373
Iteration 27/1000 | Loss: 0.00003918
Iteration 28/1000 | Loss: 0.00006775
Iteration 29/1000 | Loss: 0.00004603
Iteration 30/1000 | Loss: 0.00006452
Iteration 31/1000 | Loss: 0.00006877
Iteration 32/1000 | Loss: 0.00011760
Iteration 33/1000 | Loss: 0.00005865
Iteration 34/1000 | Loss: 0.00003764
Iteration 35/1000 | Loss: 0.00003992
Iteration 36/1000 | Loss: 0.00004093
Iteration 37/1000 | Loss: 0.00008041
Iteration 38/1000 | Loss: 0.00005054
Iteration 39/1000 | Loss: 0.00005193
Iteration 40/1000 | Loss: 0.00005603
Iteration 41/1000 | Loss: 0.00016483
Iteration 42/1000 | Loss: 0.00006080
Iteration 43/1000 | Loss: 0.00007125
Iteration 44/1000 | Loss: 0.00014252
Iteration 45/1000 | Loss: 0.00004246
Iteration 46/1000 | Loss: 0.00017769
Iteration 47/1000 | Loss: 0.00037763
Iteration 48/1000 | Loss: 0.00032322
Iteration 49/1000 | Loss: 0.00038114
Iteration 50/1000 | Loss: 0.00045384
Iteration 51/1000 | Loss: 0.00033105
Iteration 52/1000 | Loss: 0.00031063
Iteration 53/1000 | Loss: 0.00028815
Iteration 54/1000 | Loss: 0.00046374
Iteration 55/1000 | Loss: 0.00039705
Iteration 56/1000 | Loss: 0.00058716
Iteration 57/1000 | Loss: 0.00009743
Iteration 58/1000 | Loss: 0.00008492
Iteration 59/1000 | Loss: 0.00007893
Iteration 60/1000 | Loss: 0.00004282
Iteration 61/1000 | Loss: 0.00003951
Iteration 62/1000 | Loss: 0.00004268
Iteration 63/1000 | Loss: 0.00002754
Iteration 64/1000 | Loss: 0.00002986
Iteration 65/1000 | Loss: 0.00002634
Iteration 66/1000 | Loss: 0.00003712
Iteration 67/1000 | Loss: 0.00003558
Iteration 68/1000 | Loss: 0.00003624
Iteration 69/1000 | Loss: 0.00003592
Iteration 70/1000 | Loss: 0.00003575
Iteration 71/1000 | Loss: 0.00003602
Iteration 72/1000 | Loss: 0.00003879
Iteration 73/1000 | Loss: 0.00003778
Iteration 74/1000 | Loss: 0.00003422
Iteration 75/1000 | Loss: 0.00004596
Iteration 76/1000 | Loss: 0.00004829
Iteration 77/1000 | Loss: 0.00002829
Iteration 78/1000 | Loss: 0.00002992
Iteration 79/1000 | Loss: 0.00002526
Iteration 80/1000 | Loss: 0.00002718
Iteration 81/1000 | Loss: 0.00002570
Iteration 82/1000 | Loss: 0.00002413
Iteration 83/1000 | Loss: 0.00002585
Iteration 84/1000 | Loss: 0.00002472
Iteration 85/1000 | Loss: 0.00002352
Iteration 86/1000 | Loss: 0.00002485
Iteration 87/1000 | Loss: 0.00002586
Iteration 88/1000 | Loss: 0.00004841
Iteration 89/1000 | Loss: 0.00004841
Iteration 90/1000 | Loss: 0.00017641
Iteration 91/1000 | Loss: 0.00010339
Iteration 92/1000 | Loss: 0.00003440
Iteration 93/1000 | Loss: 0.00002657
Iteration 94/1000 | Loss: 0.00003295
Iteration 95/1000 | Loss: 0.00003402
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002312
Iteration 98/1000 | Loss: 0.00002288
Iteration 99/1000 | Loss: 0.00002279
Iteration 100/1000 | Loss: 0.00002273
Iteration 101/1000 | Loss: 0.00002273
Iteration 102/1000 | Loss: 0.00002273
Iteration 103/1000 | Loss: 0.00002273
Iteration 104/1000 | Loss: 0.00002273
Iteration 105/1000 | Loss: 0.00002273
Iteration 106/1000 | Loss: 0.00002272
Iteration 107/1000 | Loss: 0.00002272
Iteration 108/1000 | Loss: 0.00002272
Iteration 109/1000 | Loss: 0.00002272
Iteration 110/1000 | Loss: 0.00002330
Iteration 111/1000 | Loss: 0.00002270
Iteration 112/1000 | Loss: 0.00002269
Iteration 113/1000 | Loss: 0.00002268
Iteration 114/1000 | Loss: 0.00002268
Iteration 115/1000 | Loss: 0.00002268
Iteration 116/1000 | Loss: 0.00002279
Iteration 117/1000 | Loss: 0.00002267
Iteration 118/1000 | Loss: 0.00002266
Iteration 119/1000 | Loss: 0.00002266
Iteration 120/1000 | Loss: 0.00002266
Iteration 121/1000 | Loss: 0.00002266
Iteration 122/1000 | Loss: 0.00002266
Iteration 123/1000 | Loss: 0.00002266
Iteration 124/1000 | Loss: 0.00002265
Iteration 125/1000 | Loss: 0.00002265
Iteration 126/1000 | Loss: 0.00002264
Iteration 127/1000 | Loss: 0.00002264
Iteration 128/1000 | Loss: 0.00002263
Iteration 129/1000 | Loss: 0.00002263
Iteration 130/1000 | Loss: 0.00002263
Iteration 131/1000 | Loss: 0.00002263
Iteration 132/1000 | Loss: 0.00002263
Iteration 133/1000 | Loss: 0.00002263
Iteration 134/1000 | Loss: 0.00002262
Iteration 135/1000 | Loss: 0.00002262
Iteration 136/1000 | Loss: 0.00002262
Iteration 137/1000 | Loss: 0.00002261
Iteration 138/1000 | Loss: 0.00002261
Iteration 139/1000 | Loss: 0.00002261
Iteration 140/1000 | Loss: 0.00002260
Iteration 141/1000 | Loss: 0.00002260
Iteration 142/1000 | Loss: 0.00002260
Iteration 143/1000 | Loss: 0.00002260
Iteration 144/1000 | Loss: 0.00002259
Iteration 145/1000 | Loss: 0.00002258
Iteration 146/1000 | Loss: 0.00002258
Iteration 147/1000 | Loss: 0.00002258
Iteration 148/1000 | Loss: 0.00002257
Iteration 149/1000 | Loss: 0.00002257
Iteration 150/1000 | Loss: 0.00002257
Iteration 151/1000 | Loss: 0.00002257
Iteration 152/1000 | Loss: 0.00002256
Iteration 153/1000 | Loss: 0.00002256
Iteration 154/1000 | Loss: 0.00002256
Iteration 155/1000 | Loss: 0.00002255
Iteration 156/1000 | Loss: 0.00002255
Iteration 157/1000 | Loss: 0.00002254
Iteration 158/1000 | Loss: 0.00002254
Iteration 159/1000 | Loss: 0.00002254
Iteration 160/1000 | Loss: 0.00002254
Iteration 161/1000 | Loss: 0.00002254
Iteration 162/1000 | Loss: 0.00002253
Iteration 163/1000 | Loss: 0.00002253
Iteration 164/1000 | Loss: 0.00002253
Iteration 165/1000 | Loss: 0.00002253
Iteration 166/1000 | Loss: 0.00002253
Iteration 167/1000 | Loss: 0.00002253
Iteration 168/1000 | Loss: 0.00002253
Iteration 169/1000 | Loss: 0.00002253
Iteration 170/1000 | Loss: 0.00002252
Iteration 171/1000 | Loss: 0.00002252
Iteration 172/1000 | Loss: 0.00002252
Iteration 173/1000 | Loss: 0.00002252
Iteration 174/1000 | Loss: 0.00002252
Iteration 175/1000 | Loss: 0.00002252
Iteration 176/1000 | Loss: 0.00002251
Iteration 177/1000 | Loss: 0.00002251
Iteration 178/1000 | Loss: 0.00002251
Iteration 179/1000 | Loss: 0.00002251
Iteration 180/1000 | Loss: 0.00002251
Iteration 181/1000 | Loss: 0.00002251
Iteration 182/1000 | Loss: 0.00002251
Iteration 183/1000 | Loss: 0.00002251
Iteration 184/1000 | Loss: 0.00002250
Iteration 185/1000 | Loss: 0.00002250
Iteration 186/1000 | Loss: 0.00002250
Iteration 187/1000 | Loss: 0.00002250
Iteration 188/1000 | Loss: 0.00002250
Iteration 189/1000 | Loss: 0.00002250
Iteration 190/1000 | Loss: 0.00002250
Iteration 191/1000 | Loss: 0.00002250
Iteration 192/1000 | Loss: 0.00002249
Iteration 193/1000 | Loss: 0.00002249
Iteration 194/1000 | Loss: 0.00002249
Iteration 195/1000 | Loss: 0.00002249
Iteration 196/1000 | Loss: 0.00002249
Iteration 197/1000 | Loss: 0.00002249
Iteration 198/1000 | Loss: 0.00002249
Iteration 199/1000 | Loss: 0.00002248
Iteration 200/1000 | Loss: 0.00002248
Iteration 201/1000 | Loss: 0.00002248
Iteration 202/1000 | Loss: 0.00002248
Iteration 203/1000 | Loss: 0.00002247
Iteration 204/1000 | Loss: 0.00002247
Iteration 205/1000 | Loss: 0.00002247
Iteration 206/1000 | Loss: 0.00002246
Iteration 207/1000 | Loss: 0.00002246
Iteration 208/1000 | Loss: 0.00002246
Iteration 209/1000 | Loss: 0.00002246
Iteration 210/1000 | Loss: 0.00002246
Iteration 211/1000 | Loss: 0.00002246
Iteration 212/1000 | Loss: 0.00002246
Iteration 213/1000 | Loss: 0.00002245
Iteration 214/1000 | Loss: 0.00002245
Iteration 215/1000 | Loss: 0.00002245
Iteration 216/1000 | Loss: 0.00002245
Iteration 217/1000 | Loss: 0.00002245
Iteration 218/1000 | Loss: 0.00002245
Iteration 219/1000 | Loss: 0.00002245
Iteration 220/1000 | Loss: 0.00002245
Iteration 221/1000 | Loss: 0.00002245
Iteration 222/1000 | Loss: 0.00002245
Iteration 223/1000 | Loss: 0.00002245
Iteration 224/1000 | Loss: 0.00002244
Iteration 225/1000 | Loss: 0.00002244
Iteration 226/1000 | Loss: 0.00002244
Iteration 227/1000 | Loss: 0.00002244
Iteration 228/1000 | Loss: 0.00002244
Iteration 229/1000 | Loss: 0.00002244
Iteration 230/1000 | Loss: 0.00002244
Iteration 231/1000 | Loss: 0.00002244
Iteration 232/1000 | Loss: 0.00002244
Iteration 233/1000 | Loss: 0.00002244
Iteration 234/1000 | Loss: 0.00002244
Iteration 235/1000 | Loss: 0.00002244
Iteration 236/1000 | Loss: 0.00002244
Iteration 237/1000 | Loss: 0.00002244
Iteration 238/1000 | Loss: 0.00002244
Iteration 239/1000 | Loss: 0.00002243
Iteration 240/1000 | Loss: 0.00002243
Iteration 241/1000 | Loss: 0.00002243
Iteration 242/1000 | Loss: 0.00002243
Iteration 243/1000 | Loss: 0.00002243
Iteration 244/1000 | Loss: 0.00002243
Iteration 245/1000 | Loss: 0.00002243
Iteration 246/1000 | Loss: 0.00002243
Iteration 247/1000 | Loss: 0.00002243
Iteration 248/1000 | Loss: 0.00002243
Iteration 249/1000 | Loss: 0.00002243
Iteration 250/1000 | Loss: 0.00002243
Iteration 251/1000 | Loss: 0.00002243
Iteration 252/1000 | Loss: 0.00002243
Iteration 253/1000 | Loss: 0.00002243
Iteration 254/1000 | Loss: 0.00002243
Iteration 255/1000 | Loss: 0.00002243
Iteration 256/1000 | Loss: 0.00002243
Iteration 257/1000 | Loss: 0.00002243
Iteration 258/1000 | Loss: 0.00002243
Iteration 259/1000 | Loss: 0.00002243
Iteration 260/1000 | Loss: 0.00002243
Iteration 261/1000 | Loss: 0.00002243
Iteration 262/1000 | Loss: 0.00002243
Iteration 263/1000 | Loss: 0.00002243
Iteration 264/1000 | Loss: 0.00002243
Iteration 265/1000 | Loss: 0.00002243
Iteration 266/1000 | Loss: 0.00002243
Iteration 267/1000 | Loss: 0.00002243
Iteration 268/1000 | Loss: 0.00002243
Iteration 269/1000 | Loss: 0.00002243
Iteration 270/1000 | Loss: 0.00002243
Iteration 271/1000 | Loss: 0.00002243
Iteration 272/1000 | Loss: 0.00002243
Iteration 273/1000 | Loss: 0.00002243
Iteration 274/1000 | Loss: 0.00002243
Iteration 275/1000 | Loss: 0.00002243
Iteration 276/1000 | Loss: 0.00002243
Iteration 277/1000 | Loss: 0.00002243
Iteration 278/1000 | Loss: 0.00002243
Iteration 279/1000 | Loss: 0.00002243
Iteration 280/1000 | Loss: 0.00002243
Iteration 281/1000 | Loss: 0.00002243
Iteration 282/1000 | Loss: 0.00002243
Iteration 283/1000 | Loss: 0.00002243
Iteration 284/1000 | Loss: 0.00002243
Iteration 285/1000 | Loss: 0.00002243
Iteration 286/1000 | Loss: 0.00002243
Iteration 287/1000 | Loss: 0.00002243
Iteration 288/1000 | Loss: 0.00002243
Iteration 289/1000 | Loss: 0.00002243
Iteration 290/1000 | Loss: 0.00002243
Iteration 291/1000 | Loss: 0.00002243
Iteration 292/1000 | Loss: 0.00002243
Iteration 293/1000 | Loss: 0.00002243
Iteration 294/1000 | Loss: 0.00002243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.2427679141401313e-05, 2.2427679141401313e-05, 2.2427679141401313e-05, 2.2427679141401313e-05, 2.2427679141401313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2427679141401313e-05

Optimization complete. Final v2v error: 4.001902103424072 mm

Highest mean error: 5.5493059158325195 mm for frame 89

Lowest mean error: 3.3484137058258057 mm for frame 64

Saving results

Total time: 210.55561661720276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788825
Iteration 2/25 | Loss: 0.00190398
Iteration 3/25 | Loss: 0.00147440
Iteration 4/25 | Loss: 0.00143344
Iteration 5/25 | Loss: 0.00142723
Iteration 6/25 | Loss: 0.00142613
Iteration 7/25 | Loss: 0.00142580
Iteration 8/25 | Loss: 0.00142580
Iteration 9/25 | Loss: 0.00142580
Iteration 10/25 | Loss: 0.00142580
Iteration 11/25 | Loss: 0.00142580
Iteration 12/25 | Loss: 0.00142580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014258010778576136, 0.0014258010778576136, 0.0014258010778576136, 0.0014258010778576136, 0.0014258010778576136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014258010778576136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26149404
Iteration 2/25 | Loss: 0.00076838
Iteration 3/25 | Loss: 0.00076834
Iteration 4/25 | Loss: 0.00076834
Iteration 5/25 | Loss: 0.00076834
Iteration 6/25 | Loss: 0.00076834
Iteration 7/25 | Loss: 0.00076834
Iteration 8/25 | Loss: 0.00076834
Iteration 9/25 | Loss: 0.00076834
Iteration 10/25 | Loss: 0.00076834
Iteration 11/25 | Loss: 0.00076834
Iteration 12/25 | Loss: 0.00076834
Iteration 13/25 | Loss: 0.00076834
Iteration 14/25 | Loss: 0.00076834
Iteration 15/25 | Loss: 0.00076834
Iteration 16/25 | Loss: 0.00076834
Iteration 17/25 | Loss: 0.00076834
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007683407166041434, 0.0007683407166041434, 0.0007683407166041434, 0.0007683407166041434, 0.0007683407166041434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007683407166041434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076834
Iteration 2/1000 | Loss: 0.00005257
Iteration 3/1000 | Loss: 0.00003596
Iteration 4/1000 | Loss: 0.00003029
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00002748
Iteration 7/1000 | Loss: 0.00002684
Iteration 8/1000 | Loss: 0.00002627
Iteration 9/1000 | Loss: 0.00002592
Iteration 10/1000 | Loss: 0.00002559
Iteration 11/1000 | Loss: 0.00002533
Iteration 12/1000 | Loss: 0.00002532
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002514
Iteration 15/1000 | Loss: 0.00002506
Iteration 16/1000 | Loss: 0.00002497
Iteration 17/1000 | Loss: 0.00002497
Iteration 18/1000 | Loss: 0.00002488
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002482
Iteration 21/1000 | Loss: 0.00002482
Iteration 22/1000 | Loss: 0.00002482
Iteration 23/1000 | Loss: 0.00002482
Iteration 24/1000 | Loss: 0.00002481
Iteration 25/1000 | Loss: 0.00002481
Iteration 26/1000 | Loss: 0.00002481
Iteration 27/1000 | Loss: 0.00002481
Iteration 28/1000 | Loss: 0.00002481
Iteration 29/1000 | Loss: 0.00002481
Iteration 30/1000 | Loss: 0.00002481
Iteration 31/1000 | Loss: 0.00002480
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002479
Iteration 34/1000 | Loss: 0.00002478
Iteration 35/1000 | Loss: 0.00002478
Iteration 36/1000 | Loss: 0.00002478
Iteration 37/1000 | Loss: 0.00002477
Iteration 38/1000 | Loss: 0.00002477
Iteration 39/1000 | Loss: 0.00002477
Iteration 40/1000 | Loss: 0.00002476
Iteration 41/1000 | Loss: 0.00002476
Iteration 42/1000 | Loss: 0.00002475
Iteration 43/1000 | Loss: 0.00002475
Iteration 44/1000 | Loss: 0.00002475
Iteration 45/1000 | Loss: 0.00002474
Iteration 46/1000 | Loss: 0.00002474
Iteration 47/1000 | Loss: 0.00002474
Iteration 48/1000 | Loss: 0.00002473
Iteration 49/1000 | Loss: 0.00002473
Iteration 50/1000 | Loss: 0.00002472
Iteration 51/1000 | Loss: 0.00002472
Iteration 52/1000 | Loss: 0.00002472
Iteration 53/1000 | Loss: 0.00002472
Iteration 54/1000 | Loss: 0.00002472
Iteration 55/1000 | Loss: 0.00002472
Iteration 56/1000 | Loss: 0.00002472
Iteration 57/1000 | Loss: 0.00002472
Iteration 58/1000 | Loss: 0.00002472
Iteration 59/1000 | Loss: 0.00002472
Iteration 60/1000 | Loss: 0.00002471
Iteration 61/1000 | Loss: 0.00002470
Iteration 62/1000 | Loss: 0.00002470
Iteration 63/1000 | Loss: 0.00002470
Iteration 64/1000 | Loss: 0.00002469
Iteration 65/1000 | Loss: 0.00002469
Iteration 66/1000 | Loss: 0.00002469
Iteration 67/1000 | Loss: 0.00002469
Iteration 68/1000 | Loss: 0.00002469
Iteration 69/1000 | Loss: 0.00002469
Iteration 70/1000 | Loss: 0.00002469
Iteration 71/1000 | Loss: 0.00002469
Iteration 72/1000 | Loss: 0.00002468
Iteration 73/1000 | Loss: 0.00002468
Iteration 74/1000 | Loss: 0.00002468
Iteration 75/1000 | Loss: 0.00002468
Iteration 76/1000 | Loss: 0.00002468
Iteration 77/1000 | Loss: 0.00002468
Iteration 78/1000 | Loss: 0.00002468
Iteration 79/1000 | Loss: 0.00002468
Iteration 80/1000 | Loss: 0.00002468
Iteration 81/1000 | Loss: 0.00002468
Iteration 82/1000 | Loss: 0.00002468
Iteration 83/1000 | Loss: 0.00002468
Iteration 84/1000 | Loss: 0.00002467
Iteration 85/1000 | Loss: 0.00002467
Iteration 86/1000 | Loss: 0.00002467
Iteration 87/1000 | Loss: 0.00002467
Iteration 88/1000 | Loss: 0.00002467
Iteration 89/1000 | Loss: 0.00002467
Iteration 90/1000 | Loss: 0.00002467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.4674542146385647e-05, 2.4674542146385647e-05, 2.4674542146385647e-05, 2.4674542146385647e-05, 2.4674542146385647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4674542146385647e-05

Optimization complete. Final v2v error: 4.1507487297058105 mm

Highest mean error: 4.451478481292725 mm for frame 117

Lowest mean error: 3.882361888885498 mm for frame 45

Saving results

Total time: 32.79883790016174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788550
Iteration 2/25 | Loss: 0.00193540
Iteration 3/25 | Loss: 0.00149063
Iteration 4/25 | Loss: 0.00143151
Iteration 5/25 | Loss: 0.00142366
Iteration 6/25 | Loss: 0.00142251
Iteration 7/25 | Loss: 0.00142251
Iteration 8/25 | Loss: 0.00142251
Iteration 9/25 | Loss: 0.00142251
Iteration 10/25 | Loss: 0.00142251
Iteration 11/25 | Loss: 0.00142251
Iteration 12/25 | Loss: 0.00142251
Iteration 13/25 | Loss: 0.00142251
Iteration 14/25 | Loss: 0.00142251
Iteration 15/25 | Loss: 0.00142251
Iteration 16/25 | Loss: 0.00142251
Iteration 17/25 | Loss: 0.00142251
Iteration 18/25 | Loss: 0.00142251
Iteration 19/25 | Loss: 0.00142251
Iteration 20/25 | Loss: 0.00142251
Iteration 21/25 | Loss: 0.00142251
Iteration 22/25 | Loss: 0.00142251
Iteration 23/25 | Loss: 0.00142251
Iteration 24/25 | Loss: 0.00142251
Iteration 25/25 | Loss: 0.00142251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27029240
Iteration 2/25 | Loss: 0.00075092
Iteration 3/25 | Loss: 0.00075090
Iteration 4/25 | Loss: 0.00075090
Iteration 5/25 | Loss: 0.00075090
Iteration 6/25 | Loss: 0.00075090
Iteration 7/25 | Loss: 0.00075090
Iteration 8/25 | Loss: 0.00075089
Iteration 9/25 | Loss: 0.00075089
Iteration 10/25 | Loss: 0.00075089
Iteration 11/25 | Loss: 0.00075089
Iteration 12/25 | Loss: 0.00075089
Iteration 13/25 | Loss: 0.00075089
Iteration 14/25 | Loss: 0.00075089
Iteration 15/25 | Loss: 0.00075089
Iteration 16/25 | Loss: 0.00075089
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007508942508138716, 0.0007508942508138716, 0.0007508942508138716, 0.0007508942508138716, 0.0007508942508138716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007508942508138716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075089
Iteration 2/1000 | Loss: 0.00004563
Iteration 3/1000 | Loss: 0.00003165
Iteration 4/1000 | Loss: 0.00002886
Iteration 5/1000 | Loss: 0.00002771
Iteration 6/1000 | Loss: 0.00002669
Iteration 7/1000 | Loss: 0.00002626
Iteration 8/1000 | Loss: 0.00002580
Iteration 9/1000 | Loss: 0.00002530
Iteration 10/1000 | Loss: 0.00002504
Iteration 11/1000 | Loss: 0.00002478
Iteration 12/1000 | Loss: 0.00002455
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00002431
Iteration 15/1000 | Loss: 0.00002430
Iteration 16/1000 | Loss: 0.00002430
Iteration 17/1000 | Loss: 0.00002427
Iteration 18/1000 | Loss: 0.00002427
Iteration 19/1000 | Loss: 0.00002426
Iteration 20/1000 | Loss: 0.00002426
Iteration 21/1000 | Loss: 0.00002425
Iteration 22/1000 | Loss: 0.00002424
Iteration 23/1000 | Loss: 0.00002424
Iteration 24/1000 | Loss: 0.00002423
Iteration 25/1000 | Loss: 0.00002421
Iteration 26/1000 | Loss: 0.00002421
Iteration 27/1000 | Loss: 0.00002421
Iteration 28/1000 | Loss: 0.00002421
Iteration 29/1000 | Loss: 0.00002421
Iteration 30/1000 | Loss: 0.00002421
Iteration 31/1000 | Loss: 0.00002421
Iteration 32/1000 | Loss: 0.00002420
Iteration 33/1000 | Loss: 0.00002420
Iteration 34/1000 | Loss: 0.00002420
Iteration 35/1000 | Loss: 0.00002420
Iteration 36/1000 | Loss: 0.00002420
Iteration 37/1000 | Loss: 0.00002420
Iteration 38/1000 | Loss: 0.00002420
Iteration 39/1000 | Loss: 0.00002420
Iteration 40/1000 | Loss: 0.00002420
Iteration 41/1000 | Loss: 0.00002419
Iteration 42/1000 | Loss: 0.00002418
Iteration 43/1000 | Loss: 0.00002417
Iteration 44/1000 | Loss: 0.00002417
Iteration 45/1000 | Loss: 0.00002416
Iteration 46/1000 | Loss: 0.00002416
Iteration 47/1000 | Loss: 0.00002416
Iteration 48/1000 | Loss: 0.00002415
Iteration 49/1000 | Loss: 0.00002415
Iteration 50/1000 | Loss: 0.00002415
Iteration 51/1000 | Loss: 0.00002413
Iteration 52/1000 | Loss: 0.00002412
Iteration 53/1000 | Loss: 0.00002412
Iteration 54/1000 | Loss: 0.00002411
Iteration 55/1000 | Loss: 0.00002410
Iteration 56/1000 | Loss: 0.00002409
Iteration 57/1000 | Loss: 0.00002409
Iteration 58/1000 | Loss: 0.00002409
Iteration 59/1000 | Loss: 0.00002409
Iteration 60/1000 | Loss: 0.00002408
Iteration 61/1000 | Loss: 0.00002408
Iteration 62/1000 | Loss: 0.00002408
Iteration 63/1000 | Loss: 0.00002408
Iteration 64/1000 | Loss: 0.00002408
Iteration 65/1000 | Loss: 0.00002408
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002407
Iteration 68/1000 | Loss: 0.00002407
Iteration 69/1000 | Loss: 0.00002407
Iteration 70/1000 | Loss: 0.00002407
Iteration 71/1000 | Loss: 0.00002407
Iteration 72/1000 | Loss: 0.00002407
Iteration 73/1000 | Loss: 0.00002407
Iteration 74/1000 | Loss: 0.00002407
Iteration 75/1000 | Loss: 0.00002407
Iteration 76/1000 | Loss: 0.00002406
Iteration 77/1000 | Loss: 0.00002406
Iteration 78/1000 | Loss: 0.00002406
Iteration 79/1000 | Loss: 0.00002406
Iteration 80/1000 | Loss: 0.00002406
Iteration 81/1000 | Loss: 0.00002406
Iteration 82/1000 | Loss: 0.00002406
Iteration 83/1000 | Loss: 0.00002406
Iteration 84/1000 | Loss: 0.00002406
Iteration 85/1000 | Loss: 0.00002406
Iteration 86/1000 | Loss: 0.00002406
Iteration 87/1000 | Loss: 0.00002406
Iteration 88/1000 | Loss: 0.00002406
Iteration 89/1000 | Loss: 0.00002406
Iteration 90/1000 | Loss: 0.00002406
Iteration 91/1000 | Loss: 0.00002406
Iteration 92/1000 | Loss: 0.00002406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.405560553597752e-05, 2.405560553597752e-05, 2.405560553597752e-05, 2.405560553597752e-05, 2.405560553597752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.405560553597752e-05

Optimization complete. Final v2v error: 4.090481281280518 mm

Highest mean error: 4.450728416442871 mm for frame 125

Lowest mean error: 3.867372989654541 mm for frame 222

Saving results

Total time: 38.26883888244629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407059
Iteration 2/25 | Loss: 0.00134000
Iteration 3/25 | Loss: 0.00127021
Iteration 4/25 | Loss: 0.00126541
Iteration 5/25 | Loss: 0.00126452
Iteration 6/25 | Loss: 0.00126452
Iteration 7/25 | Loss: 0.00126452
Iteration 8/25 | Loss: 0.00126452
Iteration 9/25 | Loss: 0.00126452
Iteration 10/25 | Loss: 0.00126452
Iteration 11/25 | Loss: 0.00126452
Iteration 12/25 | Loss: 0.00126452
Iteration 13/25 | Loss: 0.00126452
Iteration 14/25 | Loss: 0.00126452
Iteration 15/25 | Loss: 0.00126452
Iteration 16/25 | Loss: 0.00126452
Iteration 17/25 | Loss: 0.00126452
Iteration 18/25 | Loss: 0.00126452
Iteration 19/25 | Loss: 0.00126452
Iteration 20/25 | Loss: 0.00126452
Iteration 21/25 | Loss: 0.00126452
Iteration 22/25 | Loss: 0.00126452
Iteration 23/25 | Loss: 0.00126452
Iteration 24/25 | Loss: 0.00126452
Iteration 25/25 | Loss: 0.00126452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39783990
Iteration 2/25 | Loss: 0.00088374
Iteration 3/25 | Loss: 0.00088374
Iteration 4/25 | Loss: 0.00088374
Iteration 5/25 | Loss: 0.00088374
Iteration 6/25 | Loss: 0.00088374
Iteration 7/25 | Loss: 0.00088374
Iteration 8/25 | Loss: 0.00088374
Iteration 9/25 | Loss: 0.00088374
Iteration 10/25 | Loss: 0.00088374
Iteration 11/25 | Loss: 0.00088374
Iteration 12/25 | Loss: 0.00088374
Iteration 13/25 | Loss: 0.00088373
Iteration 14/25 | Loss: 0.00088373
Iteration 15/25 | Loss: 0.00088373
Iteration 16/25 | Loss: 0.00088373
Iteration 17/25 | Loss: 0.00088373
Iteration 18/25 | Loss: 0.00088373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008837349596433342, 0.0008837349596433342, 0.0008837349596433342, 0.0008837349596433342, 0.0008837349596433342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008837349596433342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088373
Iteration 2/1000 | Loss: 0.00002693
Iteration 3/1000 | Loss: 0.00001864
Iteration 4/1000 | Loss: 0.00001519
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001346
Iteration 7/1000 | Loss: 0.00001284
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001230
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001229
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001190
Iteration 25/1000 | Loss: 0.00001190
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001189
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001170
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001159
Iteration 39/1000 | Loss: 0.00001158
Iteration 40/1000 | Loss: 0.00001158
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001156
Iteration 44/1000 | Loss: 0.00001156
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001147
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001144
Iteration 57/1000 | Loss: 0.00001144
Iteration 58/1000 | Loss: 0.00001144
Iteration 59/1000 | Loss: 0.00001144
Iteration 60/1000 | Loss: 0.00001144
Iteration 61/1000 | Loss: 0.00001144
Iteration 62/1000 | Loss: 0.00001144
Iteration 63/1000 | Loss: 0.00001144
Iteration 64/1000 | Loss: 0.00001144
Iteration 65/1000 | Loss: 0.00001144
Iteration 66/1000 | Loss: 0.00001143
Iteration 67/1000 | Loss: 0.00001143
Iteration 68/1000 | Loss: 0.00001143
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001142
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001141
Iteration 74/1000 | Loss: 0.00001141
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001140
Iteration 77/1000 | Loss: 0.00001140
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001138
Iteration 82/1000 | Loss: 0.00001138
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001137
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001134
Iteration 93/1000 | Loss: 0.00001133
Iteration 94/1000 | Loss: 0.00001132
Iteration 95/1000 | Loss: 0.00001132
Iteration 96/1000 | Loss: 0.00001132
Iteration 97/1000 | Loss: 0.00001131
Iteration 98/1000 | Loss: 0.00001131
Iteration 99/1000 | Loss: 0.00001131
Iteration 100/1000 | Loss: 0.00001131
Iteration 101/1000 | Loss: 0.00001131
Iteration 102/1000 | Loss: 0.00001131
Iteration 103/1000 | Loss: 0.00001131
Iteration 104/1000 | Loss: 0.00001131
Iteration 105/1000 | Loss: 0.00001131
Iteration 106/1000 | Loss: 0.00001131
Iteration 107/1000 | Loss: 0.00001131
Iteration 108/1000 | Loss: 0.00001131
Iteration 109/1000 | Loss: 0.00001130
Iteration 110/1000 | Loss: 0.00001130
Iteration 111/1000 | Loss: 0.00001130
Iteration 112/1000 | Loss: 0.00001130
Iteration 113/1000 | Loss: 0.00001129
Iteration 114/1000 | Loss: 0.00001128
Iteration 115/1000 | Loss: 0.00001128
Iteration 116/1000 | Loss: 0.00001128
Iteration 117/1000 | Loss: 0.00001127
Iteration 118/1000 | Loss: 0.00001127
Iteration 119/1000 | Loss: 0.00001126
Iteration 120/1000 | Loss: 0.00001126
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001125
Iteration 124/1000 | Loss: 0.00001125
Iteration 125/1000 | Loss: 0.00001125
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001125
Iteration 131/1000 | Loss: 0.00001125
Iteration 132/1000 | Loss: 0.00001125
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001124
Iteration 137/1000 | Loss: 0.00001123
Iteration 138/1000 | Loss: 0.00001123
Iteration 139/1000 | Loss: 0.00001123
Iteration 140/1000 | Loss: 0.00001123
Iteration 141/1000 | Loss: 0.00001123
Iteration 142/1000 | Loss: 0.00001123
Iteration 143/1000 | Loss: 0.00001123
Iteration 144/1000 | Loss: 0.00001122
Iteration 145/1000 | Loss: 0.00001122
Iteration 146/1000 | Loss: 0.00001122
Iteration 147/1000 | Loss: 0.00001122
Iteration 148/1000 | Loss: 0.00001122
Iteration 149/1000 | Loss: 0.00001122
Iteration 150/1000 | Loss: 0.00001122
Iteration 151/1000 | Loss: 0.00001122
Iteration 152/1000 | Loss: 0.00001122
Iteration 153/1000 | Loss: 0.00001122
Iteration 154/1000 | Loss: 0.00001122
Iteration 155/1000 | Loss: 0.00001122
Iteration 156/1000 | Loss: 0.00001122
Iteration 157/1000 | Loss: 0.00001122
Iteration 158/1000 | Loss: 0.00001121
Iteration 159/1000 | Loss: 0.00001121
Iteration 160/1000 | Loss: 0.00001121
Iteration 161/1000 | Loss: 0.00001121
Iteration 162/1000 | Loss: 0.00001121
Iteration 163/1000 | Loss: 0.00001121
Iteration 164/1000 | Loss: 0.00001121
Iteration 165/1000 | Loss: 0.00001121
Iteration 166/1000 | Loss: 0.00001121
Iteration 167/1000 | Loss: 0.00001121
Iteration 168/1000 | Loss: 0.00001121
Iteration 169/1000 | Loss: 0.00001120
Iteration 170/1000 | Loss: 0.00001120
Iteration 171/1000 | Loss: 0.00001120
Iteration 172/1000 | Loss: 0.00001120
Iteration 173/1000 | Loss: 0.00001120
Iteration 174/1000 | Loss: 0.00001120
Iteration 175/1000 | Loss: 0.00001120
Iteration 176/1000 | Loss: 0.00001120
Iteration 177/1000 | Loss: 0.00001120
Iteration 178/1000 | Loss: 0.00001120
Iteration 179/1000 | Loss: 0.00001120
Iteration 180/1000 | Loss: 0.00001119
Iteration 181/1000 | Loss: 0.00001119
Iteration 182/1000 | Loss: 0.00001119
Iteration 183/1000 | Loss: 0.00001119
Iteration 184/1000 | Loss: 0.00001119
Iteration 185/1000 | Loss: 0.00001119
Iteration 186/1000 | Loss: 0.00001119
Iteration 187/1000 | Loss: 0.00001119
Iteration 188/1000 | Loss: 0.00001119
Iteration 189/1000 | Loss: 0.00001119
Iteration 190/1000 | Loss: 0.00001119
Iteration 191/1000 | Loss: 0.00001119
Iteration 192/1000 | Loss: 0.00001118
Iteration 193/1000 | Loss: 0.00001118
Iteration 194/1000 | Loss: 0.00001118
Iteration 195/1000 | Loss: 0.00001118
Iteration 196/1000 | Loss: 0.00001118
Iteration 197/1000 | Loss: 0.00001118
Iteration 198/1000 | Loss: 0.00001118
Iteration 199/1000 | Loss: 0.00001118
Iteration 200/1000 | Loss: 0.00001118
Iteration 201/1000 | Loss: 0.00001118
Iteration 202/1000 | Loss: 0.00001118
Iteration 203/1000 | Loss: 0.00001118
Iteration 204/1000 | Loss: 0.00001118
Iteration 205/1000 | Loss: 0.00001117
Iteration 206/1000 | Loss: 0.00001117
Iteration 207/1000 | Loss: 0.00001117
Iteration 208/1000 | Loss: 0.00001117
Iteration 209/1000 | Loss: 0.00001117
Iteration 210/1000 | Loss: 0.00001117
Iteration 211/1000 | Loss: 0.00001117
Iteration 212/1000 | Loss: 0.00001117
Iteration 213/1000 | Loss: 0.00001117
Iteration 214/1000 | Loss: 0.00001117
Iteration 215/1000 | Loss: 0.00001117
Iteration 216/1000 | Loss: 0.00001117
Iteration 217/1000 | Loss: 0.00001117
Iteration 218/1000 | Loss: 0.00001117
Iteration 219/1000 | Loss: 0.00001117
Iteration 220/1000 | Loss: 0.00001117
Iteration 221/1000 | Loss: 0.00001117
Iteration 222/1000 | Loss: 0.00001117
Iteration 223/1000 | Loss: 0.00001117
Iteration 224/1000 | Loss: 0.00001117
Iteration 225/1000 | Loss: 0.00001117
Iteration 226/1000 | Loss: 0.00001117
Iteration 227/1000 | Loss: 0.00001117
Iteration 228/1000 | Loss: 0.00001117
Iteration 229/1000 | Loss: 0.00001117
Iteration 230/1000 | Loss: 0.00001117
Iteration 231/1000 | Loss: 0.00001117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.1174433893756941e-05, 1.1174433893756941e-05, 1.1174433893756941e-05, 1.1174433893756941e-05, 1.1174433893756941e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1174433893756941e-05

Optimization complete. Final v2v error: 2.8810532093048096 mm

Highest mean error: 2.9079134464263916 mm for frame 80

Lowest mean error: 2.8424224853515625 mm for frame 8

Saving results

Total time: 38.87728977203369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001005
Iteration 2/25 | Loss: 0.00253238
Iteration 3/25 | Loss: 0.00295282
Iteration 4/25 | Loss: 0.00157419
Iteration 5/25 | Loss: 0.00152776
Iteration 6/25 | Loss: 0.00147842
Iteration 7/25 | Loss: 0.00144774
Iteration 8/25 | Loss: 0.00142229
Iteration 9/25 | Loss: 0.00140256
Iteration 10/25 | Loss: 0.00139772
Iteration 11/25 | Loss: 0.00138571
Iteration 12/25 | Loss: 0.00136917
Iteration 13/25 | Loss: 0.00136731
Iteration 14/25 | Loss: 0.00136685
Iteration 15/25 | Loss: 0.00136728
Iteration 16/25 | Loss: 0.00135314
Iteration 17/25 | Loss: 0.00134753
Iteration 18/25 | Loss: 0.00134428
Iteration 19/25 | Loss: 0.00134313
Iteration 20/25 | Loss: 0.00134689
Iteration 21/25 | Loss: 0.00134913
Iteration 22/25 | Loss: 0.00135875
Iteration 23/25 | Loss: 0.00135804
Iteration 24/25 | Loss: 0.00135642
Iteration 25/25 | Loss: 0.00135032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50401103
Iteration 2/25 | Loss: 0.00126935
Iteration 3/25 | Loss: 0.00126935
Iteration 4/25 | Loss: 0.00126935
Iteration 5/25 | Loss: 0.00126935
Iteration 6/25 | Loss: 0.00126935
Iteration 7/25 | Loss: 0.00126935
Iteration 8/25 | Loss: 0.00126935
Iteration 9/25 | Loss: 0.00126935
Iteration 10/25 | Loss: 0.00126935
Iteration 11/25 | Loss: 0.00126935
Iteration 12/25 | Loss: 0.00126935
Iteration 13/25 | Loss: 0.00126935
Iteration 14/25 | Loss: 0.00126935
Iteration 15/25 | Loss: 0.00126935
Iteration 16/25 | Loss: 0.00126935
Iteration 17/25 | Loss: 0.00126935
Iteration 18/25 | Loss: 0.00126935
Iteration 19/25 | Loss: 0.00126935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012693466851487756, 0.0012693466851487756, 0.0012693466851487756, 0.0012693466851487756, 0.0012693466851487756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012693466851487756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126935
Iteration 2/1000 | Loss: 0.00150027
Iteration 3/1000 | Loss: 0.00075585
Iteration 4/1000 | Loss: 0.00043907
Iteration 5/1000 | Loss: 0.00037451
Iteration 6/1000 | Loss: 0.00039162
Iteration 7/1000 | Loss: 0.00022596
Iteration 8/1000 | Loss: 0.00020106
Iteration 9/1000 | Loss: 0.00022320
Iteration 10/1000 | Loss: 0.00220734
Iteration 11/1000 | Loss: 0.00027023
Iteration 12/1000 | Loss: 0.00025501
Iteration 13/1000 | Loss: 0.00050273
Iteration 14/1000 | Loss: 0.00006365
Iteration 15/1000 | Loss: 0.00007005
Iteration 16/1000 | Loss: 0.00006757
Iteration 17/1000 | Loss: 0.00006500
Iteration 18/1000 | Loss: 0.00003255
Iteration 19/1000 | Loss: 0.00024407
Iteration 20/1000 | Loss: 0.00016067
Iteration 21/1000 | Loss: 0.00032264
Iteration 22/1000 | Loss: 0.00020929
Iteration 23/1000 | Loss: 0.00002866
Iteration 24/1000 | Loss: 0.00024265
Iteration 25/1000 | Loss: 0.00023333
Iteration 26/1000 | Loss: 0.00037296
Iteration 27/1000 | Loss: 0.00054701
Iteration 28/1000 | Loss: 0.00011922
Iteration 29/1000 | Loss: 0.00003280
Iteration 30/1000 | Loss: 0.00004650
Iteration 31/1000 | Loss: 0.00002973
Iteration 32/1000 | Loss: 0.00160121
Iteration 33/1000 | Loss: 0.00009090
Iteration 34/1000 | Loss: 0.00026199
Iteration 35/1000 | Loss: 0.00003581
Iteration 36/1000 | Loss: 0.00003023
Iteration 37/1000 | Loss: 0.00057021
Iteration 38/1000 | Loss: 0.00003618
Iteration 39/1000 | Loss: 0.00002982
Iteration 40/1000 | Loss: 0.00002659
Iteration 41/1000 | Loss: 0.00002605
Iteration 42/1000 | Loss: 0.00029966
Iteration 43/1000 | Loss: 0.00024617
Iteration 44/1000 | Loss: 0.00027929
Iteration 45/1000 | Loss: 0.00030359
Iteration 46/1000 | Loss: 0.00026879
Iteration 47/1000 | Loss: 0.00019404
Iteration 48/1000 | Loss: 0.00003652
Iteration 49/1000 | Loss: 0.00003068
Iteration 50/1000 | Loss: 0.00002558
Iteration 51/1000 | Loss: 0.00002516
Iteration 52/1000 | Loss: 0.00002476
Iteration 53/1000 | Loss: 0.00032243
Iteration 54/1000 | Loss: 0.00003354
Iteration 55/1000 | Loss: 0.00002934
Iteration 56/1000 | Loss: 0.00049698
Iteration 57/1000 | Loss: 0.00039527
Iteration 58/1000 | Loss: 0.00038454
Iteration 59/1000 | Loss: 0.00037328
Iteration 60/1000 | Loss: 0.00072010
Iteration 61/1000 | Loss: 0.00050163
Iteration 62/1000 | Loss: 0.00036126
Iteration 63/1000 | Loss: 0.00033098
Iteration 64/1000 | Loss: 0.00010882
Iteration 65/1000 | Loss: 0.00126426
Iteration 66/1000 | Loss: 0.00028128
Iteration 67/1000 | Loss: 0.00003268
Iteration 68/1000 | Loss: 0.00002895
Iteration 69/1000 | Loss: 0.00002676
Iteration 70/1000 | Loss: 0.00028302
Iteration 71/1000 | Loss: 0.00005395
Iteration 72/1000 | Loss: 0.00003525
Iteration 73/1000 | Loss: 0.00002877
Iteration 74/1000 | Loss: 0.00015020
Iteration 75/1000 | Loss: 0.00002587
Iteration 76/1000 | Loss: 0.00002422
Iteration 77/1000 | Loss: 0.00002353
Iteration 78/1000 | Loss: 0.00002281
Iteration 79/1000 | Loss: 0.00002232
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002132
Iteration 82/1000 | Loss: 0.00002093
Iteration 83/1000 | Loss: 0.00002489
Iteration 84/1000 | Loss: 0.00002131
Iteration 85/1000 | Loss: 0.00002024
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00001939
Iteration 88/1000 | Loss: 0.00001922
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001886
Iteration 91/1000 | Loss: 0.00001883
Iteration 92/1000 | Loss: 0.00028366
Iteration 93/1000 | Loss: 0.00003158
Iteration 94/1000 | Loss: 0.00002402
Iteration 95/1000 | Loss: 0.00002169
Iteration 96/1000 | Loss: 0.00002053
Iteration 97/1000 | Loss: 0.00001990
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001876
Iteration 100/1000 | Loss: 0.00025380
Iteration 101/1000 | Loss: 0.00024160
Iteration 102/1000 | Loss: 0.00005838
Iteration 103/1000 | Loss: 0.00020175
Iteration 104/1000 | Loss: 0.00009431
Iteration 105/1000 | Loss: 0.00001937
Iteration 106/1000 | Loss: 0.00001865
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001843
Iteration 109/1000 | Loss: 0.00001843
Iteration 110/1000 | Loss: 0.00001842
Iteration 111/1000 | Loss: 0.00001842
Iteration 112/1000 | Loss: 0.00001842
Iteration 113/1000 | Loss: 0.00001841
Iteration 114/1000 | Loss: 0.00001841
Iteration 115/1000 | Loss: 0.00001840
Iteration 116/1000 | Loss: 0.00001839
Iteration 117/1000 | Loss: 0.00001839
Iteration 118/1000 | Loss: 0.00001839
Iteration 119/1000 | Loss: 0.00001838
Iteration 120/1000 | Loss: 0.00001838
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00029184
Iteration 123/1000 | Loss: 0.00014249
Iteration 124/1000 | Loss: 0.00023159
Iteration 125/1000 | Loss: 0.00003103
Iteration 126/1000 | Loss: 0.00002455
Iteration 127/1000 | Loss: 0.00002179
Iteration 128/1000 | Loss: 0.00003907
Iteration 129/1000 | Loss: 0.00078937
Iteration 130/1000 | Loss: 0.00002286
Iteration 131/1000 | Loss: 0.00001946
Iteration 132/1000 | Loss: 0.00001865
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001785
Iteration 135/1000 | Loss: 0.00001757
Iteration 136/1000 | Loss: 0.00001738
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001715
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001712
Iteration 142/1000 | Loss: 0.00001712
Iteration 143/1000 | Loss: 0.00001711
Iteration 144/1000 | Loss: 0.00001709
Iteration 145/1000 | Loss: 0.00001709
Iteration 146/1000 | Loss: 0.00001709
Iteration 147/1000 | Loss: 0.00001709
Iteration 148/1000 | Loss: 0.00001709
Iteration 149/1000 | Loss: 0.00001709
Iteration 150/1000 | Loss: 0.00001709
Iteration 151/1000 | Loss: 0.00001708
Iteration 152/1000 | Loss: 0.00001708
Iteration 153/1000 | Loss: 0.00001708
Iteration 154/1000 | Loss: 0.00001708
Iteration 155/1000 | Loss: 0.00001708
Iteration 156/1000 | Loss: 0.00001708
Iteration 157/1000 | Loss: 0.00001707
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001704
Iteration 160/1000 | Loss: 0.00001704
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001702
Iteration 165/1000 | Loss: 0.00001702
Iteration 166/1000 | Loss: 0.00001702
Iteration 167/1000 | Loss: 0.00001701
Iteration 168/1000 | Loss: 0.00001701
Iteration 169/1000 | Loss: 0.00001700
Iteration 170/1000 | Loss: 0.00001700
Iteration 171/1000 | Loss: 0.00001699
Iteration 172/1000 | Loss: 0.00001699
Iteration 173/1000 | Loss: 0.00001699
Iteration 174/1000 | Loss: 0.00001698
Iteration 175/1000 | Loss: 0.00001697
Iteration 176/1000 | Loss: 0.00001696
Iteration 177/1000 | Loss: 0.00001696
Iteration 178/1000 | Loss: 0.00001696
Iteration 179/1000 | Loss: 0.00001696
Iteration 180/1000 | Loss: 0.00001696
Iteration 181/1000 | Loss: 0.00001696
Iteration 182/1000 | Loss: 0.00001696
Iteration 183/1000 | Loss: 0.00001696
Iteration 184/1000 | Loss: 0.00001696
Iteration 185/1000 | Loss: 0.00001696
Iteration 186/1000 | Loss: 0.00001696
Iteration 187/1000 | Loss: 0.00001695
Iteration 188/1000 | Loss: 0.00001695
Iteration 189/1000 | Loss: 0.00001695
Iteration 190/1000 | Loss: 0.00001695
Iteration 191/1000 | Loss: 0.00001695
Iteration 192/1000 | Loss: 0.00001695
Iteration 193/1000 | Loss: 0.00001695
Iteration 194/1000 | Loss: 0.00001695
Iteration 195/1000 | Loss: 0.00001695
Iteration 196/1000 | Loss: 0.00001695
Iteration 197/1000 | Loss: 0.00001695
Iteration 198/1000 | Loss: 0.00001694
Iteration 199/1000 | Loss: 0.00001694
Iteration 200/1000 | Loss: 0.00001694
Iteration 201/1000 | Loss: 0.00001694
Iteration 202/1000 | Loss: 0.00001694
Iteration 203/1000 | Loss: 0.00001693
Iteration 204/1000 | Loss: 0.00001693
Iteration 205/1000 | Loss: 0.00001693
Iteration 206/1000 | Loss: 0.00001693
Iteration 207/1000 | Loss: 0.00001693
Iteration 208/1000 | Loss: 0.00001693
Iteration 209/1000 | Loss: 0.00001693
Iteration 210/1000 | Loss: 0.00001693
Iteration 211/1000 | Loss: 0.00001693
Iteration 212/1000 | Loss: 0.00001693
Iteration 213/1000 | Loss: 0.00001693
Iteration 214/1000 | Loss: 0.00001693
Iteration 215/1000 | Loss: 0.00001692
Iteration 216/1000 | Loss: 0.00001692
Iteration 217/1000 | Loss: 0.00001692
Iteration 218/1000 | Loss: 0.00001692
Iteration 219/1000 | Loss: 0.00001692
Iteration 220/1000 | Loss: 0.00001692
Iteration 221/1000 | Loss: 0.00001692
Iteration 222/1000 | Loss: 0.00001692
Iteration 223/1000 | Loss: 0.00001692
Iteration 224/1000 | Loss: 0.00001692
Iteration 225/1000 | Loss: 0.00001692
Iteration 226/1000 | Loss: 0.00001691
Iteration 227/1000 | Loss: 0.00001691
Iteration 228/1000 | Loss: 0.00001691
Iteration 229/1000 | Loss: 0.00001691
Iteration 230/1000 | Loss: 0.00001691
Iteration 231/1000 | Loss: 0.00001691
Iteration 232/1000 | Loss: 0.00001691
Iteration 233/1000 | Loss: 0.00001691
Iteration 234/1000 | Loss: 0.00001691
Iteration 235/1000 | Loss: 0.00001691
Iteration 236/1000 | Loss: 0.00001691
Iteration 237/1000 | Loss: 0.00001691
Iteration 238/1000 | Loss: 0.00001691
Iteration 239/1000 | Loss: 0.00001691
Iteration 240/1000 | Loss: 0.00001691
Iteration 241/1000 | Loss: 0.00001691
Iteration 242/1000 | Loss: 0.00001691
Iteration 243/1000 | Loss: 0.00001691
Iteration 244/1000 | Loss: 0.00001690
Iteration 245/1000 | Loss: 0.00001690
Iteration 246/1000 | Loss: 0.00001690
Iteration 247/1000 | Loss: 0.00001690
Iteration 248/1000 | Loss: 0.00001690
Iteration 249/1000 | Loss: 0.00001690
Iteration 250/1000 | Loss: 0.00001690
Iteration 251/1000 | Loss: 0.00001690
Iteration 252/1000 | Loss: 0.00001690
Iteration 253/1000 | Loss: 0.00001690
Iteration 254/1000 | Loss: 0.00001690
Iteration 255/1000 | Loss: 0.00001690
Iteration 256/1000 | Loss: 0.00001690
Iteration 257/1000 | Loss: 0.00001690
Iteration 258/1000 | Loss: 0.00001690
Iteration 259/1000 | Loss: 0.00001690
Iteration 260/1000 | Loss: 0.00001690
Iteration 261/1000 | Loss: 0.00001690
Iteration 262/1000 | Loss: 0.00001690
Iteration 263/1000 | Loss: 0.00001690
Iteration 264/1000 | Loss: 0.00001690
Iteration 265/1000 | Loss: 0.00001690
Iteration 266/1000 | Loss: 0.00001690
Iteration 267/1000 | Loss: 0.00001690
Iteration 268/1000 | Loss: 0.00001690
Iteration 269/1000 | Loss: 0.00001690
Iteration 270/1000 | Loss: 0.00001690
Iteration 271/1000 | Loss: 0.00001690
Iteration 272/1000 | Loss: 0.00001690
Iteration 273/1000 | Loss: 0.00001690
Iteration 274/1000 | Loss: 0.00001690
Iteration 275/1000 | Loss: 0.00001690
Iteration 276/1000 | Loss: 0.00001690
Iteration 277/1000 | Loss: 0.00001690
Iteration 278/1000 | Loss: 0.00001690
Iteration 279/1000 | Loss: 0.00001690
Iteration 280/1000 | Loss: 0.00001690
Iteration 281/1000 | Loss: 0.00001690
Iteration 282/1000 | Loss: 0.00001690
Iteration 283/1000 | Loss: 0.00001690
Iteration 284/1000 | Loss: 0.00001690
Iteration 285/1000 | Loss: 0.00001690
Iteration 286/1000 | Loss: 0.00001690
Iteration 287/1000 | Loss: 0.00001690
Iteration 288/1000 | Loss: 0.00001690
Iteration 289/1000 | Loss: 0.00001690
Iteration 290/1000 | Loss: 0.00001690
Iteration 291/1000 | Loss: 0.00001690
Iteration 292/1000 | Loss: 0.00001690
Iteration 293/1000 | Loss: 0.00001690
Iteration 294/1000 | Loss: 0.00001690
Iteration 295/1000 | Loss: 0.00001690
Iteration 296/1000 | Loss: 0.00001690
Iteration 297/1000 | Loss: 0.00001690
Iteration 298/1000 | Loss: 0.00001690
Iteration 299/1000 | Loss: 0.00001690
Iteration 300/1000 | Loss: 0.00001690
Iteration 301/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.6895282897166908e-05, 1.6895282897166908e-05, 1.6895282897166908e-05, 1.6895282897166908e-05, 1.6895282897166908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6895282897166908e-05

Optimization complete. Final v2v error: 3.344327211380005 mm

Highest mean error: 5.469192028045654 mm for frame 101

Lowest mean error: 2.916041612625122 mm for frame 125

Saving results

Total time: 216.39255452156067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00359980
Iteration 2/25 | Loss: 0.00130890
Iteration 3/25 | Loss: 0.00124803
Iteration 4/25 | Loss: 0.00123961
Iteration 5/25 | Loss: 0.00123659
Iteration 6/25 | Loss: 0.00123623
Iteration 7/25 | Loss: 0.00123623
Iteration 8/25 | Loss: 0.00123623
Iteration 9/25 | Loss: 0.00123623
Iteration 10/25 | Loss: 0.00123623
Iteration 11/25 | Loss: 0.00123623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012362285051494837, 0.0012362285051494837, 0.0012362285051494837, 0.0012362285051494837, 0.0012362285051494837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012362285051494837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40676081
Iteration 2/25 | Loss: 0.00086366
Iteration 3/25 | Loss: 0.00086366
Iteration 4/25 | Loss: 0.00086365
Iteration 5/25 | Loss: 0.00086365
Iteration 6/25 | Loss: 0.00086365
Iteration 7/25 | Loss: 0.00086365
Iteration 8/25 | Loss: 0.00086365
Iteration 9/25 | Loss: 0.00086365
Iteration 10/25 | Loss: 0.00086365
Iteration 11/25 | Loss: 0.00086365
Iteration 12/25 | Loss: 0.00086365
Iteration 13/25 | Loss: 0.00086365
Iteration 14/25 | Loss: 0.00086365
Iteration 15/25 | Loss: 0.00086365
Iteration 16/25 | Loss: 0.00086365
Iteration 17/25 | Loss: 0.00086365
Iteration 18/25 | Loss: 0.00086365
Iteration 19/25 | Loss: 0.00086365
Iteration 20/25 | Loss: 0.00086365
Iteration 21/25 | Loss: 0.00086365
Iteration 22/25 | Loss: 0.00086365
Iteration 23/25 | Loss: 0.00086365
Iteration 24/25 | Loss: 0.00086365
Iteration 25/25 | Loss: 0.00086365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086365
Iteration 2/1000 | Loss: 0.00002224
Iteration 3/1000 | Loss: 0.00001463
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001260
Iteration 6/1000 | Loss: 0.00001197
Iteration 7/1000 | Loss: 0.00001158
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001128
Iteration 10/1000 | Loss: 0.00001110
Iteration 11/1000 | Loss: 0.00001106
Iteration 12/1000 | Loss: 0.00001101
Iteration 13/1000 | Loss: 0.00001099
Iteration 14/1000 | Loss: 0.00001098
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001098
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001097
Iteration 20/1000 | Loss: 0.00001087
Iteration 21/1000 | Loss: 0.00001084
Iteration 22/1000 | Loss: 0.00001081
Iteration 23/1000 | Loss: 0.00001081
Iteration 24/1000 | Loss: 0.00001076
Iteration 25/1000 | Loss: 0.00001071
Iteration 26/1000 | Loss: 0.00001066
Iteration 27/1000 | Loss: 0.00001065
Iteration 28/1000 | Loss: 0.00001065
Iteration 29/1000 | Loss: 0.00001065
Iteration 30/1000 | Loss: 0.00001065
Iteration 31/1000 | Loss: 0.00001065
Iteration 32/1000 | Loss: 0.00001064
Iteration 33/1000 | Loss: 0.00001064
Iteration 34/1000 | Loss: 0.00001063
Iteration 35/1000 | Loss: 0.00001063
Iteration 36/1000 | Loss: 0.00001063
Iteration 37/1000 | Loss: 0.00001063
Iteration 38/1000 | Loss: 0.00001062
Iteration 39/1000 | Loss: 0.00001062
Iteration 40/1000 | Loss: 0.00001062
Iteration 41/1000 | Loss: 0.00001062
Iteration 42/1000 | Loss: 0.00001062
Iteration 43/1000 | Loss: 0.00001061
Iteration 44/1000 | Loss: 0.00001061
Iteration 45/1000 | Loss: 0.00001061
Iteration 46/1000 | Loss: 0.00001060
Iteration 47/1000 | Loss: 0.00001059
Iteration 48/1000 | Loss: 0.00001059
Iteration 49/1000 | Loss: 0.00001058
Iteration 50/1000 | Loss: 0.00001058
Iteration 51/1000 | Loss: 0.00001058
Iteration 52/1000 | Loss: 0.00001058
Iteration 53/1000 | Loss: 0.00001058
Iteration 54/1000 | Loss: 0.00001057
Iteration 55/1000 | Loss: 0.00001057
Iteration 56/1000 | Loss: 0.00001057
Iteration 57/1000 | Loss: 0.00001057
Iteration 58/1000 | Loss: 0.00001057
Iteration 59/1000 | Loss: 0.00001057
Iteration 60/1000 | Loss: 0.00001057
Iteration 61/1000 | Loss: 0.00001056
Iteration 62/1000 | Loss: 0.00001056
Iteration 63/1000 | Loss: 0.00001056
Iteration 64/1000 | Loss: 0.00001055
Iteration 65/1000 | Loss: 0.00001055
Iteration 66/1000 | Loss: 0.00001052
Iteration 67/1000 | Loss: 0.00001052
Iteration 68/1000 | Loss: 0.00001052
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001050
Iteration 72/1000 | Loss: 0.00001050
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001050
Iteration 75/1000 | Loss: 0.00001050
Iteration 76/1000 | Loss: 0.00001049
Iteration 77/1000 | Loss: 0.00001048
Iteration 78/1000 | Loss: 0.00001048
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001041
Iteration 81/1000 | Loss: 0.00001041
Iteration 82/1000 | Loss: 0.00001041
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001040
Iteration 85/1000 | Loss: 0.00001040
Iteration 86/1000 | Loss: 0.00001040
Iteration 87/1000 | Loss: 0.00001040
Iteration 88/1000 | Loss: 0.00001040
Iteration 89/1000 | Loss: 0.00001040
Iteration 90/1000 | Loss: 0.00001040
Iteration 91/1000 | Loss: 0.00001040
Iteration 92/1000 | Loss: 0.00001039
Iteration 93/1000 | Loss: 0.00001039
Iteration 94/1000 | Loss: 0.00001039
Iteration 95/1000 | Loss: 0.00001038
Iteration 96/1000 | Loss: 0.00001038
Iteration 97/1000 | Loss: 0.00001038
Iteration 98/1000 | Loss: 0.00001038
Iteration 99/1000 | Loss: 0.00001037
Iteration 100/1000 | Loss: 0.00001037
Iteration 101/1000 | Loss: 0.00001037
Iteration 102/1000 | Loss: 0.00001036
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001036
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001035
Iteration 109/1000 | Loss: 0.00001035
Iteration 110/1000 | Loss: 0.00001034
Iteration 111/1000 | Loss: 0.00001034
Iteration 112/1000 | Loss: 0.00001034
Iteration 113/1000 | Loss: 0.00001034
Iteration 114/1000 | Loss: 0.00001034
Iteration 115/1000 | Loss: 0.00001034
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001033
Iteration 118/1000 | Loss: 0.00001033
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001033
Iteration 121/1000 | Loss: 0.00001033
Iteration 122/1000 | Loss: 0.00001033
Iteration 123/1000 | Loss: 0.00001033
Iteration 124/1000 | Loss: 0.00001033
Iteration 125/1000 | Loss: 0.00001033
Iteration 126/1000 | Loss: 0.00001033
Iteration 127/1000 | Loss: 0.00001032
Iteration 128/1000 | Loss: 0.00001032
Iteration 129/1000 | Loss: 0.00001032
Iteration 130/1000 | Loss: 0.00001032
Iteration 131/1000 | Loss: 0.00001032
Iteration 132/1000 | Loss: 0.00001031
Iteration 133/1000 | Loss: 0.00001031
Iteration 134/1000 | Loss: 0.00001031
Iteration 135/1000 | Loss: 0.00001031
Iteration 136/1000 | Loss: 0.00001031
Iteration 137/1000 | Loss: 0.00001031
Iteration 138/1000 | Loss: 0.00001031
Iteration 139/1000 | Loss: 0.00001031
Iteration 140/1000 | Loss: 0.00001031
Iteration 141/1000 | Loss: 0.00001031
Iteration 142/1000 | Loss: 0.00001031
Iteration 143/1000 | Loss: 0.00001031
Iteration 144/1000 | Loss: 0.00001031
Iteration 145/1000 | Loss: 0.00001031
Iteration 146/1000 | Loss: 0.00001031
Iteration 147/1000 | Loss: 0.00001031
Iteration 148/1000 | Loss: 0.00001031
Iteration 149/1000 | Loss: 0.00001031
Iteration 150/1000 | Loss: 0.00001031
Iteration 151/1000 | Loss: 0.00001031
Iteration 152/1000 | Loss: 0.00001031
Iteration 153/1000 | Loss: 0.00001030
Iteration 154/1000 | Loss: 0.00001030
Iteration 155/1000 | Loss: 0.00001030
Iteration 156/1000 | Loss: 0.00001030
Iteration 157/1000 | Loss: 0.00001030
Iteration 158/1000 | Loss: 0.00001030
Iteration 159/1000 | Loss: 0.00001030
Iteration 160/1000 | Loss: 0.00001030
Iteration 161/1000 | Loss: 0.00001030
Iteration 162/1000 | Loss: 0.00001030
Iteration 163/1000 | Loss: 0.00001030
Iteration 164/1000 | Loss: 0.00001029
Iteration 165/1000 | Loss: 0.00001029
Iteration 166/1000 | Loss: 0.00001029
Iteration 167/1000 | Loss: 0.00001029
Iteration 168/1000 | Loss: 0.00001029
Iteration 169/1000 | Loss: 0.00001029
Iteration 170/1000 | Loss: 0.00001029
Iteration 171/1000 | Loss: 0.00001029
Iteration 172/1000 | Loss: 0.00001029
Iteration 173/1000 | Loss: 0.00001029
Iteration 174/1000 | Loss: 0.00001029
Iteration 175/1000 | Loss: 0.00001029
Iteration 176/1000 | Loss: 0.00001029
Iteration 177/1000 | Loss: 0.00001029
Iteration 178/1000 | Loss: 0.00001028
Iteration 179/1000 | Loss: 0.00001028
Iteration 180/1000 | Loss: 0.00001028
Iteration 181/1000 | Loss: 0.00001028
Iteration 182/1000 | Loss: 0.00001028
Iteration 183/1000 | Loss: 0.00001028
Iteration 184/1000 | Loss: 0.00001028
Iteration 185/1000 | Loss: 0.00001028
Iteration 186/1000 | Loss: 0.00001028
Iteration 187/1000 | Loss: 0.00001028
Iteration 188/1000 | Loss: 0.00001028
Iteration 189/1000 | Loss: 0.00001028
Iteration 190/1000 | Loss: 0.00001027
Iteration 191/1000 | Loss: 0.00001027
Iteration 192/1000 | Loss: 0.00001027
Iteration 193/1000 | Loss: 0.00001027
Iteration 194/1000 | Loss: 0.00001026
Iteration 195/1000 | Loss: 0.00001026
Iteration 196/1000 | Loss: 0.00001026
Iteration 197/1000 | Loss: 0.00001026
Iteration 198/1000 | Loss: 0.00001026
Iteration 199/1000 | Loss: 0.00001026
Iteration 200/1000 | Loss: 0.00001026
Iteration 201/1000 | Loss: 0.00001026
Iteration 202/1000 | Loss: 0.00001026
Iteration 203/1000 | Loss: 0.00001026
Iteration 204/1000 | Loss: 0.00001026
Iteration 205/1000 | Loss: 0.00001026
Iteration 206/1000 | Loss: 0.00001026
Iteration 207/1000 | Loss: 0.00001026
Iteration 208/1000 | Loss: 0.00001026
Iteration 209/1000 | Loss: 0.00001026
Iteration 210/1000 | Loss: 0.00001026
Iteration 211/1000 | Loss: 0.00001026
Iteration 212/1000 | Loss: 0.00001026
Iteration 213/1000 | Loss: 0.00001026
Iteration 214/1000 | Loss: 0.00001026
Iteration 215/1000 | Loss: 0.00001025
Iteration 216/1000 | Loss: 0.00001025
Iteration 217/1000 | Loss: 0.00001025
Iteration 218/1000 | Loss: 0.00001025
Iteration 219/1000 | Loss: 0.00001025
Iteration 220/1000 | Loss: 0.00001025
Iteration 221/1000 | Loss: 0.00001025
Iteration 222/1000 | Loss: 0.00001025
Iteration 223/1000 | Loss: 0.00001025
Iteration 224/1000 | Loss: 0.00001025
Iteration 225/1000 | Loss: 0.00001025
Iteration 226/1000 | Loss: 0.00001025
Iteration 227/1000 | Loss: 0.00001024
Iteration 228/1000 | Loss: 0.00001024
Iteration 229/1000 | Loss: 0.00001024
Iteration 230/1000 | Loss: 0.00001024
Iteration 231/1000 | Loss: 0.00001024
Iteration 232/1000 | Loss: 0.00001024
Iteration 233/1000 | Loss: 0.00001024
Iteration 234/1000 | Loss: 0.00001024
Iteration 235/1000 | Loss: 0.00001024
Iteration 236/1000 | Loss: 0.00001024
Iteration 237/1000 | Loss: 0.00001024
Iteration 238/1000 | Loss: 0.00001024
Iteration 239/1000 | Loss: 0.00001024
Iteration 240/1000 | Loss: 0.00001024
Iteration 241/1000 | Loss: 0.00001024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.0239624316454865e-05, 1.0239624316454865e-05, 1.0239624316454865e-05, 1.0239624316454865e-05, 1.0239624316454865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0239624316454865e-05

Optimization complete. Final v2v error: 2.7573976516723633 mm

Highest mean error: 2.8932700157165527 mm for frame 131

Lowest mean error: 2.7067811489105225 mm for frame 120

Saving results

Total time: 40.81097173690796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865752
Iteration 2/25 | Loss: 0.00138265
Iteration 3/25 | Loss: 0.00131210
Iteration 4/25 | Loss: 0.00129877
Iteration 5/25 | Loss: 0.00129486
Iteration 6/25 | Loss: 0.00129422
Iteration 7/25 | Loss: 0.00129422
Iteration 8/25 | Loss: 0.00129422
Iteration 9/25 | Loss: 0.00129422
Iteration 10/25 | Loss: 0.00129422
Iteration 11/25 | Loss: 0.00129422
Iteration 12/25 | Loss: 0.00129422
Iteration 13/25 | Loss: 0.00129422
Iteration 14/25 | Loss: 0.00129422
Iteration 15/25 | Loss: 0.00129422
Iteration 16/25 | Loss: 0.00129422
Iteration 17/25 | Loss: 0.00129422
Iteration 18/25 | Loss: 0.00129422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012942195171490312, 0.0012942195171490312, 0.0012942195171490312, 0.0012942195171490312, 0.0012942195171490312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012942195171490312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46004856
Iteration 2/25 | Loss: 0.00084452
Iteration 3/25 | Loss: 0.00084451
Iteration 4/25 | Loss: 0.00084451
Iteration 5/25 | Loss: 0.00084451
Iteration 6/25 | Loss: 0.00084451
Iteration 7/25 | Loss: 0.00084451
Iteration 8/25 | Loss: 0.00084451
Iteration 9/25 | Loss: 0.00084451
Iteration 10/25 | Loss: 0.00084451
Iteration 11/25 | Loss: 0.00084451
Iteration 12/25 | Loss: 0.00084451
Iteration 13/25 | Loss: 0.00084450
Iteration 14/25 | Loss: 0.00084450
Iteration 15/25 | Loss: 0.00084450
Iteration 16/25 | Loss: 0.00084450
Iteration 17/25 | Loss: 0.00084450
Iteration 18/25 | Loss: 0.00084450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008445048006251454, 0.0008445048006251454, 0.0008445048006251454, 0.0008445048006251454, 0.0008445048006251454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008445048006251454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084450
Iteration 2/1000 | Loss: 0.00003390
Iteration 3/1000 | Loss: 0.00002376
Iteration 4/1000 | Loss: 0.00002050
Iteration 5/1000 | Loss: 0.00001918
Iteration 6/1000 | Loss: 0.00001837
Iteration 7/1000 | Loss: 0.00001795
Iteration 8/1000 | Loss: 0.00001749
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001669
Iteration 12/1000 | Loss: 0.00001649
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001617
Iteration 19/1000 | Loss: 0.00001617
Iteration 20/1000 | Loss: 0.00001617
Iteration 21/1000 | Loss: 0.00001615
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001594
Iteration 28/1000 | Loss: 0.00001593
Iteration 29/1000 | Loss: 0.00001592
Iteration 30/1000 | Loss: 0.00001592
Iteration 31/1000 | Loss: 0.00001592
Iteration 32/1000 | Loss: 0.00001591
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001590
Iteration 35/1000 | Loss: 0.00001590
Iteration 36/1000 | Loss: 0.00001589
Iteration 37/1000 | Loss: 0.00001589
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001588
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001585
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001585
Iteration 48/1000 | Loss: 0.00001584
Iteration 49/1000 | Loss: 0.00001584
Iteration 50/1000 | Loss: 0.00001584
Iteration 51/1000 | Loss: 0.00001583
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001582
Iteration 54/1000 | Loss: 0.00001579
Iteration 55/1000 | Loss: 0.00001579
Iteration 56/1000 | Loss: 0.00001578
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001574
Iteration 59/1000 | Loss: 0.00001571
Iteration 60/1000 | Loss: 0.00001571
Iteration 61/1000 | Loss: 0.00001571
Iteration 62/1000 | Loss: 0.00001571
Iteration 63/1000 | Loss: 0.00001570
Iteration 64/1000 | Loss: 0.00001570
Iteration 65/1000 | Loss: 0.00001570
Iteration 66/1000 | Loss: 0.00001570
Iteration 67/1000 | Loss: 0.00001570
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001570
Iteration 70/1000 | Loss: 0.00001570
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001569
Iteration 73/1000 | Loss: 0.00001569
Iteration 74/1000 | Loss: 0.00001569
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001568
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001567
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001566
Iteration 85/1000 | Loss: 0.00001566
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001565
Iteration 88/1000 | Loss: 0.00001565
Iteration 89/1000 | Loss: 0.00001565
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001563
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001562
Iteration 100/1000 | Loss: 0.00001562
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001561
Iteration 106/1000 | Loss: 0.00001561
Iteration 107/1000 | Loss: 0.00001561
Iteration 108/1000 | Loss: 0.00001561
Iteration 109/1000 | Loss: 0.00001560
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001559
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001559
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001558
Iteration 125/1000 | Loss: 0.00001558
Iteration 126/1000 | Loss: 0.00001558
Iteration 127/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5583427739329636e-05, 1.5583427739329636e-05, 1.5583427739329636e-05, 1.5583427739329636e-05, 1.5583427739329636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5583427739329636e-05

Optimization complete. Final v2v error: 3.3499083518981934 mm

Highest mean error: 3.612898349761963 mm for frame 84

Lowest mean error: 3.139197587966919 mm for frame 15

Saving results

Total time: 37.640591621398926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536020
Iteration 2/25 | Loss: 0.00152186
Iteration 3/25 | Loss: 0.00143544
Iteration 4/25 | Loss: 0.00141970
Iteration 5/25 | Loss: 0.00141582
Iteration 6/25 | Loss: 0.00141582
Iteration 7/25 | Loss: 0.00141582
Iteration 8/25 | Loss: 0.00141582
Iteration 9/25 | Loss: 0.00141582
Iteration 10/25 | Loss: 0.00141582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014158155536279082, 0.0014158155536279082, 0.0014158155536279082, 0.0014158155536279082, 0.0014158155536279082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014158155536279082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77346712
Iteration 2/25 | Loss: 0.00076666
Iteration 3/25 | Loss: 0.00076665
Iteration 4/25 | Loss: 0.00076665
Iteration 5/25 | Loss: 0.00076665
Iteration 6/25 | Loss: 0.00076665
Iteration 7/25 | Loss: 0.00076665
Iteration 8/25 | Loss: 0.00076665
Iteration 9/25 | Loss: 0.00076665
Iteration 10/25 | Loss: 0.00076665
Iteration 11/25 | Loss: 0.00076665
Iteration 12/25 | Loss: 0.00076665
Iteration 13/25 | Loss: 0.00076665
Iteration 14/25 | Loss: 0.00076665
Iteration 15/25 | Loss: 0.00076665
Iteration 16/25 | Loss: 0.00076665
Iteration 17/25 | Loss: 0.00076665
Iteration 18/25 | Loss: 0.00076665
Iteration 19/25 | Loss: 0.00076665
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007666483870707452, 0.0007666483870707452, 0.0007666483870707452, 0.0007666483870707452, 0.0007666483870707452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007666483870707452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076665
Iteration 2/1000 | Loss: 0.00004870
Iteration 3/1000 | Loss: 0.00004301
Iteration 4/1000 | Loss: 0.00004087
Iteration 5/1000 | Loss: 0.00003951
Iteration 6/1000 | Loss: 0.00003904
Iteration 7/1000 | Loss: 0.00003856
Iteration 8/1000 | Loss: 0.00003800
Iteration 9/1000 | Loss: 0.00003745
Iteration 10/1000 | Loss: 0.00003691
Iteration 11/1000 | Loss: 0.00003660
Iteration 12/1000 | Loss: 0.00003633
Iteration 13/1000 | Loss: 0.00003593
Iteration 14/1000 | Loss: 0.00003570
Iteration 15/1000 | Loss: 0.00003549
Iteration 16/1000 | Loss: 0.00003542
Iteration 17/1000 | Loss: 0.00003531
Iteration 18/1000 | Loss: 0.00003522
Iteration 19/1000 | Loss: 0.00003520
Iteration 20/1000 | Loss: 0.00003519
Iteration 21/1000 | Loss: 0.00003518
Iteration 22/1000 | Loss: 0.00003517
Iteration 23/1000 | Loss: 0.00003517
Iteration 24/1000 | Loss: 0.00003516
Iteration 25/1000 | Loss: 0.00003516
Iteration 26/1000 | Loss: 0.00003513
Iteration 27/1000 | Loss: 0.00003512
Iteration 28/1000 | Loss: 0.00003510
Iteration 29/1000 | Loss: 0.00003510
Iteration 30/1000 | Loss: 0.00003510
Iteration 31/1000 | Loss: 0.00003509
Iteration 32/1000 | Loss: 0.00003509
Iteration 33/1000 | Loss: 0.00003508
Iteration 34/1000 | Loss: 0.00003508
Iteration 35/1000 | Loss: 0.00003507
Iteration 36/1000 | Loss: 0.00003507
Iteration 37/1000 | Loss: 0.00003507
Iteration 38/1000 | Loss: 0.00003506
Iteration 39/1000 | Loss: 0.00003506
Iteration 40/1000 | Loss: 0.00003506
Iteration 41/1000 | Loss: 0.00003506
Iteration 42/1000 | Loss: 0.00003506
Iteration 43/1000 | Loss: 0.00003506
Iteration 44/1000 | Loss: 0.00003506
Iteration 45/1000 | Loss: 0.00003506
Iteration 46/1000 | Loss: 0.00003506
Iteration 47/1000 | Loss: 0.00003505
Iteration 48/1000 | Loss: 0.00003505
Iteration 49/1000 | Loss: 0.00003505
Iteration 50/1000 | Loss: 0.00003505
Iteration 51/1000 | Loss: 0.00003504
Iteration 52/1000 | Loss: 0.00003504
Iteration 53/1000 | Loss: 0.00003503
Iteration 54/1000 | Loss: 0.00003503
Iteration 55/1000 | Loss: 0.00003503
Iteration 56/1000 | Loss: 0.00003503
Iteration 57/1000 | Loss: 0.00003503
Iteration 58/1000 | Loss: 0.00003503
Iteration 59/1000 | Loss: 0.00003503
Iteration 60/1000 | Loss: 0.00003503
Iteration 61/1000 | Loss: 0.00003503
Iteration 62/1000 | Loss: 0.00003503
Iteration 63/1000 | Loss: 0.00003503
Iteration 64/1000 | Loss: 0.00003502
Iteration 65/1000 | Loss: 0.00003502
Iteration 66/1000 | Loss: 0.00003502
Iteration 67/1000 | Loss: 0.00003502
Iteration 68/1000 | Loss: 0.00003502
Iteration 69/1000 | Loss: 0.00003502
Iteration 70/1000 | Loss: 0.00003502
Iteration 71/1000 | Loss: 0.00003502
Iteration 72/1000 | Loss: 0.00003501
Iteration 73/1000 | Loss: 0.00003501
Iteration 74/1000 | Loss: 0.00003501
Iteration 75/1000 | Loss: 0.00003500
Iteration 76/1000 | Loss: 0.00003500
Iteration 77/1000 | Loss: 0.00003500
Iteration 78/1000 | Loss: 0.00003500
Iteration 79/1000 | Loss: 0.00003499
Iteration 80/1000 | Loss: 0.00003499
Iteration 81/1000 | Loss: 0.00003499
Iteration 82/1000 | Loss: 0.00003498
Iteration 83/1000 | Loss: 0.00003498
Iteration 84/1000 | Loss: 0.00003498
Iteration 85/1000 | Loss: 0.00003497
Iteration 86/1000 | Loss: 0.00003497
Iteration 87/1000 | Loss: 0.00003497
Iteration 88/1000 | Loss: 0.00003497
Iteration 89/1000 | Loss: 0.00003497
Iteration 90/1000 | Loss: 0.00003496
Iteration 91/1000 | Loss: 0.00003496
Iteration 92/1000 | Loss: 0.00003496
Iteration 93/1000 | Loss: 0.00003496
Iteration 94/1000 | Loss: 0.00003496
Iteration 95/1000 | Loss: 0.00003496
Iteration 96/1000 | Loss: 0.00003496
Iteration 97/1000 | Loss: 0.00003496
Iteration 98/1000 | Loss: 0.00003496
Iteration 99/1000 | Loss: 0.00003496
Iteration 100/1000 | Loss: 0.00003496
Iteration 101/1000 | Loss: 0.00003496
Iteration 102/1000 | Loss: 0.00003496
Iteration 103/1000 | Loss: 0.00003496
Iteration 104/1000 | Loss: 0.00003496
Iteration 105/1000 | Loss: 0.00003496
Iteration 106/1000 | Loss: 0.00003496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [3.495663986541331e-05, 3.495663986541331e-05, 3.495663986541331e-05, 3.495663986541331e-05, 3.495663986541331e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.495663986541331e-05

Optimization complete. Final v2v error: 4.7115583419799805 mm

Highest mean error: 4.745739459991455 mm for frame 24

Lowest mean error: 4.669212818145752 mm for frame 56

Saving results

Total time: 42.872949838638306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462583
Iteration 2/25 | Loss: 0.00152552
Iteration 3/25 | Loss: 0.00137416
Iteration 4/25 | Loss: 0.00136059
Iteration 5/25 | Loss: 0.00135888
Iteration 6/25 | Loss: 0.00135888
Iteration 7/25 | Loss: 0.00135888
Iteration 8/25 | Loss: 0.00135888
Iteration 9/25 | Loss: 0.00135888
Iteration 10/25 | Loss: 0.00135888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013588794972747564, 0.0013588794972747564, 0.0013588794972747564, 0.0013588794972747564, 0.0013588794972747564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013588794972747564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37552476
Iteration 2/25 | Loss: 0.00087316
Iteration 3/25 | Loss: 0.00087315
Iteration 4/25 | Loss: 0.00087315
Iteration 5/25 | Loss: 0.00087315
Iteration 6/25 | Loss: 0.00087315
Iteration 7/25 | Loss: 0.00087315
Iteration 8/25 | Loss: 0.00087315
Iteration 9/25 | Loss: 0.00087315
Iteration 10/25 | Loss: 0.00087315
Iteration 11/25 | Loss: 0.00087315
Iteration 12/25 | Loss: 0.00087315
Iteration 13/25 | Loss: 0.00087315
Iteration 14/25 | Loss: 0.00087315
Iteration 15/25 | Loss: 0.00087315
Iteration 16/25 | Loss: 0.00087315
Iteration 17/25 | Loss: 0.00087315
Iteration 18/25 | Loss: 0.00087315
Iteration 19/25 | Loss: 0.00087315
Iteration 20/25 | Loss: 0.00087315
Iteration 21/25 | Loss: 0.00087315
Iteration 22/25 | Loss: 0.00087315
Iteration 23/25 | Loss: 0.00087315
Iteration 24/25 | Loss: 0.00087315
Iteration 25/25 | Loss: 0.00087315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087315
Iteration 2/1000 | Loss: 0.00003699
Iteration 3/1000 | Loss: 0.00002433
Iteration 4/1000 | Loss: 0.00001826
Iteration 5/1000 | Loss: 0.00001675
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001568
Iteration 8/1000 | Loss: 0.00001533
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00001517
Iteration 11/1000 | Loss: 0.00001512
Iteration 12/1000 | Loss: 0.00001488
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001467
Iteration 15/1000 | Loss: 0.00001464
Iteration 16/1000 | Loss: 0.00001463
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001449
Iteration 21/1000 | Loss: 0.00001448
Iteration 22/1000 | Loss: 0.00001448
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001432
Iteration 27/1000 | Loss: 0.00001424
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001423
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001417
Iteration 32/1000 | Loss: 0.00001417
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00001414
Iteration 37/1000 | Loss: 0.00001411
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001399
Iteration 42/1000 | Loss: 0.00001399
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001399
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001396
Iteration 56/1000 | Loss: 0.00001396
Iteration 57/1000 | Loss: 0.00001396
Iteration 58/1000 | Loss: 0.00001396
Iteration 59/1000 | Loss: 0.00001396
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001387
Iteration 98/1000 | Loss: 0.00001387
Iteration 99/1000 | Loss: 0.00001387
Iteration 100/1000 | Loss: 0.00001387
Iteration 101/1000 | Loss: 0.00001387
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001386
Iteration 104/1000 | Loss: 0.00001386
Iteration 105/1000 | Loss: 0.00001386
Iteration 106/1000 | Loss: 0.00001386
Iteration 107/1000 | Loss: 0.00001385
Iteration 108/1000 | Loss: 0.00001385
Iteration 109/1000 | Loss: 0.00001385
Iteration 110/1000 | Loss: 0.00001385
Iteration 111/1000 | Loss: 0.00001385
Iteration 112/1000 | Loss: 0.00001385
Iteration 113/1000 | Loss: 0.00001385
Iteration 114/1000 | Loss: 0.00001385
Iteration 115/1000 | Loss: 0.00001385
Iteration 116/1000 | Loss: 0.00001384
Iteration 117/1000 | Loss: 0.00001384
Iteration 118/1000 | Loss: 0.00001383
Iteration 119/1000 | Loss: 0.00001383
Iteration 120/1000 | Loss: 0.00001383
Iteration 121/1000 | Loss: 0.00001383
Iteration 122/1000 | Loss: 0.00001383
Iteration 123/1000 | Loss: 0.00001383
Iteration 124/1000 | Loss: 0.00001383
Iteration 125/1000 | Loss: 0.00001383
Iteration 126/1000 | Loss: 0.00001383
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001382
Iteration 130/1000 | Loss: 0.00001382
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001379
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001379
Iteration 153/1000 | Loss: 0.00001379
Iteration 154/1000 | Loss: 0.00001379
Iteration 155/1000 | Loss: 0.00001379
Iteration 156/1000 | Loss: 0.00001379
Iteration 157/1000 | Loss: 0.00001379
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001378
Iteration 165/1000 | Loss: 0.00001378
Iteration 166/1000 | Loss: 0.00001378
Iteration 167/1000 | Loss: 0.00001378
Iteration 168/1000 | Loss: 0.00001378
Iteration 169/1000 | Loss: 0.00001378
Iteration 170/1000 | Loss: 0.00001378
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001378
Iteration 175/1000 | Loss: 0.00001378
Iteration 176/1000 | Loss: 0.00001378
Iteration 177/1000 | Loss: 0.00001378
Iteration 178/1000 | Loss: 0.00001378
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001378
Iteration 189/1000 | Loss: 0.00001378
Iteration 190/1000 | Loss: 0.00001378
Iteration 191/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.3783286703983322e-05, 1.3783286703983322e-05, 1.3783286703983322e-05, 1.3783286703983322e-05, 1.3783286703983322e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3783286703983322e-05

Optimization complete. Final v2v error: 3.2159998416900635 mm

Highest mean error: 3.401404619216919 mm for frame 94

Lowest mean error: 3.0549747943878174 mm for frame 198

Saving results

Total time: 40.529520988464355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022980
Iteration 2/25 | Loss: 0.01022980
Iteration 3/25 | Loss: 0.01022980
Iteration 4/25 | Loss: 0.00206998
Iteration 5/25 | Loss: 0.00163765
Iteration 6/25 | Loss: 0.00153551
Iteration 7/25 | Loss: 0.00156525
Iteration 8/25 | Loss: 0.00153109
Iteration 9/25 | Loss: 0.00150107
Iteration 10/25 | Loss: 0.00148412
Iteration 11/25 | Loss: 0.00148118
Iteration 12/25 | Loss: 0.00147499
Iteration 13/25 | Loss: 0.00147163
Iteration 14/25 | Loss: 0.00145677
Iteration 15/25 | Loss: 0.00144775
Iteration 16/25 | Loss: 0.00144289
Iteration 17/25 | Loss: 0.00144504
Iteration 18/25 | Loss: 0.00144451
Iteration 19/25 | Loss: 0.00144253
Iteration 20/25 | Loss: 0.00144012
Iteration 21/25 | Loss: 0.00143730
Iteration 22/25 | Loss: 0.00143420
Iteration 23/25 | Loss: 0.00143493
Iteration 24/25 | Loss: 0.00143398
Iteration 25/25 | Loss: 0.00143793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68082106
Iteration 2/25 | Loss: 0.00153614
Iteration 3/25 | Loss: 0.00140915
Iteration 4/25 | Loss: 0.00140915
Iteration 5/25 | Loss: 0.00140915
Iteration 6/25 | Loss: 0.00140915
Iteration 7/25 | Loss: 0.00140915
Iteration 8/25 | Loss: 0.00140915
Iteration 9/25 | Loss: 0.00140915
Iteration 10/25 | Loss: 0.00140915
Iteration 11/25 | Loss: 0.00140914
Iteration 12/25 | Loss: 0.00140914
Iteration 13/25 | Loss: 0.00140914
Iteration 14/25 | Loss: 0.00140914
Iteration 15/25 | Loss: 0.00140914
Iteration 16/25 | Loss: 0.00140914
Iteration 17/25 | Loss: 0.00140914
Iteration 18/25 | Loss: 0.00140914
Iteration 19/25 | Loss: 0.00140914
Iteration 20/25 | Loss: 0.00140914
Iteration 21/25 | Loss: 0.00140914
Iteration 22/25 | Loss: 0.00140914
Iteration 23/25 | Loss: 0.00140914
Iteration 24/25 | Loss: 0.00140914
Iteration 25/25 | Loss: 0.00140914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140914
Iteration 2/1000 | Loss: 0.00037694
Iteration 3/1000 | Loss: 0.00041587
Iteration 4/1000 | Loss: 0.00026682
Iteration 5/1000 | Loss: 0.00009600
Iteration 6/1000 | Loss: 0.00011390
Iteration 7/1000 | Loss: 0.00020251
Iteration 8/1000 | Loss: 0.00020693
Iteration 9/1000 | Loss: 0.00023832
Iteration 10/1000 | Loss: 0.00010416
Iteration 11/1000 | Loss: 0.00021120
Iteration 12/1000 | Loss: 0.00016332
Iteration 13/1000 | Loss: 0.00012835
Iteration 14/1000 | Loss: 0.00013629
Iteration 15/1000 | Loss: 0.00009106
Iteration 16/1000 | Loss: 0.00026838
Iteration 17/1000 | Loss: 0.00021529
Iteration 18/1000 | Loss: 0.00006867
Iteration 19/1000 | Loss: 0.00018660
Iteration 20/1000 | Loss: 0.00016272
Iteration 21/1000 | Loss: 0.00026815
Iteration 22/1000 | Loss: 0.00019375
Iteration 23/1000 | Loss: 0.00022538
Iteration 24/1000 | Loss: 0.00019808
Iteration 25/1000 | Loss: 0.00018620
Iteration 26/1000 | Loss: 0.00016848
Iteration 27/1000 | Loss: 0.00011343
Iteration 28/1000 | Loss: 0.00021206
Iteration 29/1000 | Loss: 0.00021749
Iteration 30/1000 | Loss: 0.00017885
Iteration 31/1000 | Loss: 0.00019157
Iteration 32/1000 | Loss: 0.00035239
Iteration 33/1000 | Loss: 0.00033334
Iteration 34/1000 | Loss: 0.00018317
Iteration 35/1000 | Loss: 0.00012486
Iteration 36/1000 | Loss: 0.00010262
Iteration 37/1000 | Loss: 0.00005122
Iteration 38/1000 | Loss: 0.00013763
Iteration 39/1000 | Loss: 0.00013889
Iteration 40/1000 | Loss: 0.00014582
Iteration 41/1000 | Loss: 0.00012376
Iteration 42/1000 | Loss: 0.00029955
Iteration 43/1000 | Loss: 0.00035454
Iteration 44/1000 | Loss: 0.00008069
Iteration 45/1000 | Loss: 0.00010015
Iteration 46/1000 | Loss: 0.00010598
Iteration 47/1000 | Loss: 0.00004777
Iteration 48/1000 | Loss: 0.00020943
Iteration 49/1000 | Loss: 0.00020031
Iteration 50/1000 | Loss: 0.00006263
Iteration 51/1000 | Loss: 0.00005561
Iteration 52/1000 | Loss: 0.00007506
Iteration 53/1000 | Loss: 0.00004270
Iteration 54/1000 | Loss: 0.00004813
Iteration 55/1000 | Loss: 0.00004429
Iteration 56/1000 | Loss: 0.00004136
Iteration 57/1000 | Loss: 0.00003163
Iteration 58/1000 | Loss: 0.00004447
Iteration 59/1000 | Loss: 0.00004134
Iteration 60/1000 | Loss: 0.00004051
Iteration 61/1000 | Loss: 0.00004064
Iteration 62/1000 | Loss: 0.00003408
Iteration 63/1000 | Loss: 0.00003182
Iteration 64/1000 | Loss: 0.00003146
Iteration 65/1000 | Loss: 0.00003954
Iteration 66/1000 | Loss: 0.00004573
Iteration 67/1000 | Loss: 0.00003130
Iteration 68/1000 | Loss: 0.00002739
Iteration 69/1000 | Loss: 0.00002630
Iteration 70/1000 | Loss: 0.00002555
Iteration 71/1000 | Loss: 0.00002508
Iteration 72/1000 | Loss: 0.00003568
Iteration 73/1000 | Loss: 0.00002641
Iteration 74/1000 | Loss: 0.00002535
Iteration 75/1000 | Loss: 0.00002472
Iteration 76/1000 | Loss: 0.00002437
Iteration 77/1000 | Loss: 0.00002435
Iteration 78/1000 | Loss: 0.00002426
Iteration 79/1000 | Loss: 0.00002421
Iteration 80/1000 | Loss: 0.00002421
Iteration 81/1000 | Loss: 0.00002421
Iteration 82/1000 | Loss: 0.00002421
Iteration 83/1000 | Loss: 0.00002420
Iteration 84/1000 | Loss: 0.00002420
Iteration 85/1000 | Loss: 0.00002417
Iteration 86/1000 | Loss: 0.00002416
Iteration 87/1000 | Loss: 0.00002415
Iteration 88/1000 | Loss: 0.00002415
Iteration 89/1000 | Loss: 0.00002414
Iteration 90/1000 | Loss: 0.00002413
Iteration 91/1000 | Loss: 0.00002410
Iteration 92/1000 | Loss: 0.00002410
Iteration 93/1000 | Loss: 0.00002405
Iteration 94/1000 | Loss: 0.00002401
Iteration 95/1000 | Loss: 0.00002401
Iteration 96/1000 | Loss: 0.00002401
Iteration 97/1000 | Loss: 0.00002400
Iteration 98/1000 | Loss: 0.00002400
Iteration 99/1000 | Loss: 0.00002399
Iteration 100/1000 | Loss: 0.00002399
Iteration 101/1000 | Loss: 0.00002397
Iteration 102/1000 | Loss: 0.00002397
Iteration 103/1000 | Loss: 0.00002395
Iteration 104/1000 | Loss: 0.00002395
Iteration 105/1000 | Loss: 0.00002395
Iteration 106/1000 | Loss: 0.00002394
Iteration 107/1000 | Loss: 0.00002394
Iteration 108/1000 | Loss: 0.00002394
Iteration 109/1000 | Loss: 0.00002394
Iteration 110/1000 | Loss: 0.00002394
Iteration 111/1000 | Loss: 0.00002394
Iteration 112/1000 | Loss: 0.00002393
Iteration 113/1000 | Loss: 0.00002393
Iteration 114/1000 | Loss: 0.00002393
Iteration 115/1000 | Loss: 0.00002393
Iteration 116/1000 | Loss: 0.00002393
Iteration 117/1000 | Loss: 0.00002393
Iteration 118/1000 | Loss: 0.00002392
Iteration 119/1000 | Loss: 0.00002392
Iteration 120/1000 | Loss: 0.00002392
Iteration 121/1000 | Loss: 0.00002392
Iteration 122/1000 | Loss: 0.00002392
Iteration 123/1000 | Loss: 0.00002392
Iteration 124/1000 | Loss: 0.00002392
Iteration 125/1000 | Loss: 0.00002392
Iteration 126/1000 | Loss: 0.00002391
Iteration 127/1000 | Loss: 0.00002391
Iteration 128/1000 | Loss: 0.00002391
Iteration 129/1000 | Loss: 0.00002391
Iteration 130/1000 | Loss: 0.00002391
Iteration 131/1000 | Loss: 0.00002391
Iteration 132/1000 | Loss: 0.00002391
Iteration 133/1000 | Loss: 0.00002391
Iteration 134/1000 | Loss: 0.00002391
Iteration 135/1000 | Loss: 0.00002391
Iteration 136/1000 | Loss: 0.00002391
Iteration 137/1000 | Loss: 0.00002391
Iteration 138/1000 | Loss: 0.00002391
Iteration 139/1000 | Loss: 0.00002390
Iteration 140/1000 | Loss: 0.00002390
Iteration 141/1000 | Loss: 0.00002390
Iteration 142/1000 | Loss: 0.00002390
Iteration 143/1000 | Loss: 0.00002390
Iteration 144/1000 | Loss: 0.00002390
Iteration 145/1000 | Loss: 0.00002390
Iteration 146/1000 | Loss: 0.00002390
Iteration 147/1000 | Loss: 0.00002390
Iteration 148/1000 | Loss: 0.00002390
Iteration 149/1000 | Loss: 0.00002390
Iteration 150/1000 | Loss: 0.00002390
Iteration 151/1000 | Loss: 0.00002389
Iteration 152/1000 | Loss: 0.00002389
Iteration 153/1000 | Loss: 0.00002389
Iteration 154/1000 | Loss: 0.00002389
Iteration 155/1000 | Loss: 0.00002389
Iteration 156/1000 | Loss: 0.00002389
Iteration 157/1000 | Loss: 0.00002389
Iteration 158/1000 | Loss: 0.00002389
Iteration 159/1000 | Loss: 0.00002388
Iteration 160/1000 | Loss: 0.00002388
Iteration 161/1000 | Loss: 0.00002388
Iteration 162/1000 | Loss: 0.00002387
Iteration 163/1000 | Loss: 0.00002387
Iteration 164/1000 | Loss: 0.00002387
Iteration 165/1000 | Loss: 0.00002387
Iteration 166/1000 | Loss: 0.00002387
Iteration 167/1000 | Loss: 0.00002387
Iteration 168/1000 | Loss: 0.00002387
Iteration 169/1000 | Loss: 0.00002387
Iteration 170/1000 | Loss: 0.00002387
Iteration 171/1000 | Loss: 0.00002386
Iteration 172/1000 | Loss: 0.00002386
Iteration 173/1000 | Loss: 0.00002386
Iteration 174/1000 | Loss: 0.00002386
Iteration 175/1000 | Loss: 0.00002385
Iteration 176/1000 | Loss: 0.00002385
Iteration 177/1000 | Loss: 0.00002385
Iteration 178/1000 | Loss: 0.00002385
Iteration 179/1000 | Loss: 0.00002385
Iteration 180/1000 | Loss: 0.00002385
Iteration 181/1000 | Loss: 0.00002385
Iteration 182/1000 | Loss: 0.00002385
Iteration 183/1000 | Loss: 0.00002385
Iteration 184/1000 | Loss: 0.00002384
Iteration 185/1000 | Loss: 0.00002384
Iteration 186/1000 | Loss: 0.00002384
Iteration 187/1000 | Loss: 0.00002384
Iteration 188/1000 | Loss: 0.00002384
Iteration 189/1000 | Loss: 0.00002384
Iteration 190/1000 | Loss: 0.00002383
Iteration 191/1000 | Loss: 0.00002382
Iteration 192/1000 | Loss: 0.00002382
Iteration 193/1000 | Loss: 0.00002382
Iteration 194/1000 | Loss: 0.00002382
Iteration 195/1000 | Loss: 0.00002382
Iteration 196/1000 | Loss: 0.00002382
Iteration 197/1000 | Loss: 0.00002382
Iteration 198/1000 | Loss: 0.00002382
Iteration 199/1000 | Loss: 0.00002382
Iteration 200/1000 | Loss: 0.00002382
Iteration 201/1000 | Loss: 0.00002381
Iteration 202/1000 | Loss: 0.00002381
Iteration 203/1000 | Loss: 0.00002381
Iteration 204/1000 | Loss: 0.00002381
Iteration 205/1000 | Loss: 0.00002380
Iteration 206/1000 | Loss: 0.00002380
Iteration 207/1000 | Loss: 0.00002380
Iteration 208/1000 | Loss: 0.00002380
Iteration 209/1000 | Loss: 0.00002380
Iteration 210/1000 | Loss: 0.00002380
Iteration 211/1000 | Loss: 0.00002379
Iteration 212/1000 | Loss: 0.00002379
Iteration 213/1000 | Loss: 0.00002379
Iteration 214/1000 | Loss: 0.00002379
Iteration 215/1000 | Loss: 0.00002379
Iteration 216/1000 | Loss: 0.00002379
Iteration 217/1000 | Loss: 0.00002379
Iteration 218/1000 | Loss: 0.00002379
Iteration 219/1000 | Loss: 0.00002379
Iteration 220/1000 | Loss: 0.00002379
Iteration 221/1000 | Loss: 0.00002379
Iteration 222/1000 | Loss: 0.00002378
Iteration 223/1000 | Loss: 0.00002378
Iteration 224/1000 | Loss: 0.00002378
Iteration 225/1000 | Loss: 0.00002378
Iteration 226/1000 | Loss: 0.00002378
Iteration 227/1000 | Loss: 0.00002377
Iteration 228/1000 | Loss: 0.00002377
Iteration 229/1000 | Loss: 0.00002377
Iteration 230/1000 | Loss: 0.00002377
Iteration 231/1000 | Loss: 0.00002377
Iteration 232/1000 | Loss: 0.00002377
Iteration 233/1000 | Loss: 0.00002377
Iteration 234/1000 | Loss: 0.00002377
Iteration 235/1000 | Loss: 0.00002377
Iteration 236/1000 | Loss: 0.00002377
Iteration 237/1000 | Loss: 0.00002377
Iteration 238/1000 | Loss: 0.00002377
Iteration 239/1000 | Loss: 0.00002376
Iteration 240/1000 | Loss: 0.00002376
Iteration 241/1000 | Loss: 0.00002376
Iteration 242/1000 | Loss: 0.00002376
Iteration 243/1000 | Loss: 0.00002376
Iteration 244/1000 | Loss: 0.00002376
Iteration 245/1000 | Loss: 0.00002376
Iteration 246/1000 | Loss: 0.00002376
Iteration 247/1000 | Loss: 0.00002376
Iteration 248/1000 | Loss: 0.00002375
Iteration 249/1000 | Loss: 0.00002375
Iteration 250/1000 | Loss: 0.00002375
Iteration 251/1000 | Loss: 0.00002375
Iteration 252/1000 | Loss: 0.00002375
Iteration 253/1000 | Loss: 0.00002375
Iteration 254/1000 | Loss: 0.00002375
Iteration 255/1000 | Loss: 0.00002375
Iteration 256/1000 | Loss: 0.00002374
Iteration 257/1000 | Loss: 0.00002374
Iteration 258/1000 | Loss: 0.00002374
Iteration 259/1000 | Loss: 0.00002374
Iteration 260/1000 | Loss: 0.00002374
Iteration 261/1000 | Loss: 0.00002374
Iteration 262/1000 | Loss: 0.00002374
Iteration 263/1000 | Loss: 0.00002373
Iteration 264/1000 | Loss: 0.00002373
Iteration 265/1000 | Loss: 0.00002373
Iteration 266/1000 | Loss: 0.00002373
Iteration 267/1000 | Loss: 0.00002373
Iteration 268/1000 | Loss: 0.00002373
Iteration 269/1000 | Loss: 0.00002373
Iteration 270/1000 | Loss: 0.00002373
Iteration 271/1000 | Loss: 0.00002373
Iteration 272/1000 | Loss: 0.00002373
Iteration 273/1000 | Loss: 0.00002373
Iteration 274/1000 | Loss: 0.00002373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [2.3731674446025863e-05, 2.3731674446025863e-05, 2.3731674446025863e-05, 2.3731674446025863e-05, 2.3731674446025863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3731674446025863e-05

Optimization complete. Final v2v error: 4.05551815032959 mm

Highest mean error: 10.8452787399292 mm for frame 1

Lowest mean error: 3.736809253692627 mm for frame 63

Saving results

Total time: 184.8396816253662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822459
Iteration 2/25 | Loss: 0.00135228
Iteration 3/25 | Loss: 0.00127925
Iteration 4/25 | Loss: 0.00126974
Iteration 5/25 | Loss: 0.00126615
Iteration 6/25 | Loss: 0.00126573
Iteration 7/25 | Loss: 0.00126573
Iteration 8/25 | Loss: 0.00126573
Iteration 9/25 | Loss: 0.00126573
Iteration 10/25 | Loss: 0.00126573
Iteration 11/25 | Loss: 0.00126573
Iteration 12/25 | Loss: 0.00126573
Iteration 13/25 | Loss: 0.00126573
Iteration 14/25 | Loss: 0.00126573
Iteration 15/25 | Loss: 0.00126573
Iteration 16/25 | Loss: 0.00126573
Iteration 17/25 | Loss: 0.00126573
Iteration 18/25 | Loss: 0.00126573
Iteration 19/25 | Loss: 0.00126573
Iteration 20/25 | Loss: 0.00126573
Iteration 21/25 | Loss: 0.00126573
Iteration 22/25 | Loss: 0.00126573
Iteration 23/25 | Loss: 0.00126573
Iteration 24/25 | Loss: 0.00126573
Iteration 25/25 | Loss: 0.00126573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47957671
Iteration 2/25 | Loss: 0.00088099
Iteration 3/25 | Loss: 0.00088099
Iteration 4/25 | Loss: 0.00088099
Iteration 5/25 | Loss: 0.00088099
Iteration 6/25 | Loss: 0.00088099
Iteration 7/25 | Loss: 0.00088099
Iteration 8/25 | Loss: 0.00088099
Iteration 9/25 | Loss: 0.00088099
Iteration 10/25 | Loss: 0.00088099
Iteration 11/25 | Loss: 0.00088099
Iteration 12/25 | Loss: 0.00088099
Iteration 13/25 | Loss: 0.00088099
Iteration 14/25 | Loss: 0.00088099
Iteration 15/25 | Loss: 0.00088099
Iteration 16/25 | Loss: 0.00088099
Iteration 17/25 | Loss: 0.00088099
Iteration 18/25 | Loss: 0.00088099
Iteration 19/25 | Loss: 0.00088099
Iteration 20/25 | Loss: 0.00088099
Iteration 21/25 | Loss: 0.00088099
Iteration 22/25 | Loss: 0.00088099
Iteration 23/25 | Loss: 0.00088099
Iteration 24/25 | Loss: 0.00088099
Iteration 25/25 | Loss: 0.00088099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088099
Iteration 2/1000 | Loss: 0.00002231
Iteration 3/1000 | Loss: 0.00001637
Iteration 4/1000 | Loss: 0.00001469
Iteration 5/1000 | Loss: 0.00001404
Iteration 6/1000 | Loss: 0.00001361
Iteration 7/1000 | Loss: 0.00001315
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001281
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001255
Iteration 13/1000 | Loss: 0.00001232
Iteration 14/1000 | Loss: 0.00001229
Iteration 15/1000 | Loss: 0.00001221
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001213
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001212
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001201
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001186
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001181
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001179
Iteration 46/1000 | Loss: 0.00001178
Iteration 47/1000 | Loss: 0.00001177
Iteration 48/1000 | Loss: 0.00001177
Iteration 49/1000 | Loss: 0.00001177
Iteration 50/1000 | Loss: 0.00001177
Iteration 51/1000 | Loss: 0.00001177
Iteration 52/1000 | Loss: 0.00001177
Iteration 53/1000 | Loss: 0.00001176
Iteration 54/1000 | Loss: 0.00001176
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001174
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001172
Iteration 66/1000 | Loss: 0.00001172
Iteration 67/1000 | Loss: 0.00001171
Iteration 68/1000 | Loss: 0.00001171
Iteration 69/1000 | Loss: 0.00001171
Iteration 70/1000 | Loss: 0.00001170
Iteration 71/1000 | Loss: 0.00001170
Iteration 72/1000 | Loss: 0.00001170
Iteration 73/1000 | Loss: 0.00001170
Iteration 74/1000 | Loss: 0.00001170
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001169
Iteration 77/1000 | Loss: 0.00001168
Iteration 78/1000 | Loss: 0.00001168
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001159
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001157
Iteration 97/1000 | Loss: 0.00001157
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001154
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001152
Iteration 121/1000 | Loss: 0.00001152
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001150
Iteration 126/1000 | Loss: 0.00001150
Iteration 127/1000 | Loss: 0.00001150
Iteration 128/1000 | Loss: 0.00001150
Iteration 129/1000 | Loss: 0.00001150
Iteration 130/1000 | Loss: 0.00001150
Iteration 131/1000 | Loss: 0.00001150
Iteration 132/1000 | Loss: 0.00001150
Iteration 133/1000 | Loss: 0.00001150
Iteration 134/1000 | Loss: 0.00001149
Iteration 135/1000 | Loss: 0.00001149
Iteration 136/1000 | Loss: 0.00001149
Iteration 137/1000 | Loss: 0.00001149
Iteration 138/1000 | Loss: 0.00001149
Iteration 139/1000 | Loss: 0.00001149
Iteration 140/1000 | Loss: 0.00001149
Iteration 141/1000 | Loss: 0.00001149
Iteration 142/1000 | Loss: 0.00001149
Iteration 143/1000 | Loss: 0.00001148
Iteration 144/1000 | Loss: 0.00001148
Iteration 145/1000 | Loss: 0.00001148
Iteration 146/1000 | Loss: 0.00001148
Iteration 147/1000 | Loss: 0.00001148
Iteration 148/1000 | Loss: 0.00001148
Iteration 149/1000 | Loss: 0.00001148
Iteration 150/1000 | Loss: 0.00001148
Iteration 151/1000 | Loss: 0.00001148
Iteration 152/1000 | Loss: 0.00001148
Iteration 153/1000 | Loss: 0.00001148
Iteration 154/1000 | Loss: 0.00001148
Iteration 155/1000 | Loss: 0.00001147
Iteration 156/1000 | Loss: 0.00001147
Iteration 157/1000 | Loss: 0.00001147
Iteration 158/1000 | Loss: 0.00001147
Iteration 159/1000 | Loss: 0.00001147
Iteration 160/1000 | Loss: 0.00001147
Iteration 161/1000 | Loss: 0.00001147
Iteration 162/1000 | Loss: 0.00001147
Iteration 163/1000 | Loss: 0.00001147
Iteration 164/1000 | Loss: 0.00001147
Iteration 165/1000 | Loss: 0.00001147
Iteration 166/1000 | Loss: 0.00001147
Iteration 167/1000 | Loss: 0.00001147
Iteration 168/1000 | Loss: 0.00001147
Iteration 169/1000 | Loss: 0.00001147
Iteration 170/1000 | Loss: 0.00001147
Iteration 171/1000 | Loss: 0.00001147
Iteration 172/1000 | Loss: 0.00001147
Iteration 173/1000 | Loss: 0.00001147
Iteration 174/1000 | Loss: 0.00001147
Iteration 175/1000 | Loss: 0.00001147
Iteration 176/1000 | Loss: 0.00001147
Iteration 177/1000 | Loss: 0.00001146
Iteration 178/1000 | Loss: 0.00001146
Iteration 179/1000 | Loss: 0.00001146
Iteration 180/1000 | Loss: 0.00001146
Iteration 181/1000 | Loss: 0.00001146
Iteration 182/1000 | Loss: 0.00001146
Iteration 183/1000 | Loss: 0.00001146
Iteration 184/1000 | Loss: 0.00001146
Iteration 185/1000 | Loss: 0.00001146
Iteration 186/1000 | Loss: 0.00001146
Iteration 187/1000 | Loss: 0.00001146
Iteration 188/1000 | Loss: 0.00001146
Iteration 189/1000 | Loss: 0.00001146
Iteration 190/1000 | Loss: 0.00001146
Iteration 191/1000 | Loss: 0.00001146
Iteration 192/1000 | Loss: 0.00001146
Iteration 193/1000 | Loss: 0.00001146
Iteration 194/1000 | Loss: 0.00001146
Iteration 195/1000 | Loss: 0.00001146
Iteration 196/1000 | Loss: 0.00001146
Iteration 197/1000 | Loss: 0.00001146
Iteration 198/1000 | Loss: 0.00001146
Iteration 199/1000 | Loss: 0.00001146
Iteration 200/1000 | Loss: 0.00001146
Iteration 201/1000 | Loss: 0.00001146
Iteration 202/1000 | Loss: 0.00001146
Iteration 203/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.1457897016953211e-05, 1.1457897016953211e-05, 1.1457897016953211e-05, 1.1457897016953211e-05, 1.1457897016953211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1457897016953211e-05

Optimization complete. Final v2v error: 2.897700786590576 mm

Highest mean error: 3.3683865070343018 mm for frame 89

Lowest mean error: 2.721743106842041 mm for frame 11

Saving results

Total time: 38.69051933288574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047089
Iteration 2/25 | Loss: 0.00170385
Iteration 3/25 | Loss: 0.00144172
Iteration 4/25 | Loss: 0.00142449
Iteration 5/25 | Loss: 0.00140468
Iteration 6/25 | Loss: 0.00140318
Iteration 7/25 | Loss: 0.00139917
Iteration 8/25 | Loss: 0.00140255
Iteration 9/25 | Loss: 0.00139868
Iteration 10/25 | Loss: 0.00139863
Iteration 11/25 | Loss: 0.00139863
Iteration 12/25 | Loss: 0.00139862
Iteration 13/25 | Loss: 0.00139862
Iteration 14/25 | Loss: 0.00139862
Iteration 15/25 | Loss: 0.00139862
Iteration 16/25 | Loss: 0.00139862
Iteration 17/25 | Loss: 0.00139862
Iteration 18/25 | Loss: 0.00139861
Iteration 19/25 | Loss: 0.00139859
Iteration 20/25 | Loss: 0.00140114
Iteration 21/25 | Loss: 0.00139926
Iteration 22/25 | Loss: 0.00139853
Iteration 23/25 | Loss: 0.00139853
Iteration 24/25 | Loss: 0.00139853
Iteration 25/25 | Loss: 0.00139853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54283476
Iteration 2/25 | Loss: 0.00087553
Iteration 3/25 | Loss: 0.00087550
Iteration 4/25 | Loss: 0.00087550
Iteration 5/25 | Loss: 0.00087550
Iteration 6/25 | Loss: 0.00087550
Iteration 7/25 | Loss: 0.00087550
Iteration 8/25 | Loss: 0.00087550
Iteration 9/25 | Loss: 0.00087550
Iteration 10/25 | Loss: 0.00087550
Iteration 11/25 | Loss: 0.00087550
Iteration 12/25 | Loss: 0.00087550
Iteration 13/25 | Loss: 0.00087550
Iteration 14/25 | Loss: 0.00087550
Iteration 15/25 | Loss: 0.00087550
Iteration 16/25 | Loss: 0.00087550
Iteration 17/25 | Loss: 0.00087550
Iteration 18/25 | Loss: 0.00087550
Iteration 19/25 | Loss: 0.00087550
Iteration 20/25 | Loss: 0.00087550
Iteration 21/25 | Loss: 0.00087550
Iteration 22/25 | Loss: 0.00087550
Iteration 23/25 | Loss: 0.00087550
Iteration 24/25 | Loss: 0.00087550
Iteration 25/25 | Loss: 0.00087550

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087550
Iteration 2/1000 | Loss: 0.00004486
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002704
Iteration 5/1000 | Loss: 0.00002558
Iteration 6/1000 | Loss: 0.00002482
Iteration 7/1000 | Loss: 0.00002429
Iteration 8/1000 | Loss: 0.00002371
Iteration 9/1000 | Loss: 0.00002337
Iteration 10/1000 | Loss: 0.00002308
Iteration 11/1000 | Loss: 0.00002288
Iteration 12/1000 | Loss: 0.00002268
Iteration 13/1000 | Loss: 0.00002253
Iteration 14/1000 | Loss: 0.00002252
Iteration 15/1000 | Loss: 0.00002232
Iteration 16/1000 | Loss: 0.00002221
Iteration 17/1000 | Loss: 0.00002221
Iteration 18/1000 | Loss: 0.00002220
Iteration 19/1000 | Loss: 0.00002219
Iteration 20/1000 | Loss: 0.00002218
Iteration 21/1000 | Loss: 0.00002218
Iteration 22/1000 | Loss: 0.00002217
Iteration 23/1000 | Loss: 0.00002216
Iteration 24/1000 | Loss: 0.00002212
Iteration 25/1000 | Loss: 0.00002212
Iteration 26/1000 | Loss: 0.00002211
Iteration 27/1000 | Loss: 0.00002210
Iteration 28/1000 | Loss: 0.00002210
Iteration 29/1000 | Loss: 0.00002209
Iteration 30/1000 | Loss: 0.00002208
Iteration 31/1000 | Loss: 0.00002207
Iteration 32/1000 | Loss: 0.00002206
Iteration 33/1000 | Loss: 0.00002206
Iteration 34/1000 | Loss: 0.00002206
Iteration 35/1000 | Loss: 0.00002205
Iteration 36/1000 | Loss: 0.00002204
Iteration 37/1000 | Loss: 0.00002202
Iteration 38/1000 | Loss: 0.00002202
Iteration 39/1000 | Loss: 0.00002201
Iteration 40/1000 | Loss: 0.00002199
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002198
Iteration 44/1000 | Loss: 0.00002198
Iteration 45/1000 | Loss: 0.00002198
Iteration 46/1000 | Loss: 0.00002197
Iteration 47/1000 | Loss: 0.00002196
Iteration 48/1000 | Loss: 0.00002196
Iteration 49/1000 | Loss: 0.00002196
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002196
Iteration 52/1000 | Loss: 0.00002196
Iteration 53/1000 | Loss: 0.00002196
Iteration 54/1000 | Loss: 0.00002195
Iteration 55/1000 | Loss: 0.00002195
Iteration 56/1000 | Loss: 0.00002195
Iteration 57/1000 | Loss: 0.00002195
Iteration 58/1000 | Loss: 0.00002194
Iteration 59/1000 | Loss: 0.00002194
Iteration 60/1000 | Loss: 0.00002193
Iteration 61/1000 | Loss: 0.00002193
Iteration 62/1000 | Loss: 0.00002192
Iteration 63/1000 | Loss: 0.00002192
Iteration 64/1000 | Loss: 0.00002192
Iteration 65/1000 | Loss: 0.00002192
Iteration 66/1000 | Loss: 0.00002191
Iteration 67/1000 | Loss: 0.00002191
Iteration 68/1000 | Loss: 0.00002190
Iteration 69/1000 | Loss: 0.00002189
Iteration 70/1000 | Loss: 0.00002189
Iteration 71/1000 | Loss: 0.00002189
Iteration 72/1000 | Loss: 0.00002189
Iteration 73/1000 | Loss: 0.00002188
Iteration 74/1000 | Loss: 0.00002188
Iteration 75/1000 | Loss: 0.00002188
Iteration 76/1000 | Loss: 0.00002188
Iteration 77/1000 | Loss: 0.00002188
Iteration 78/1000 | Loss: 0.00002188
Iteration 79/1000 | Loss: 0.00002187
Iteration 80/1000 | Loss: 0.00002187
Iteration 81/1000 | Loss: 0.00002187
Iteration 82/1000 | Loss: 0.00002187
Iteration 83/1000 | Loss: 0.00002187
Iteration 84/1000 | Loss: 0.00002187
Iteration 85/1000 | Loss: 0.00002187
Iteration 86/1000 | Loss: 0.00002186
Iteration 87/1000 | Loss: 0.00002186
Iteration 88/1000 | Loss: 0.00002186
Iteration 89/1000 | Loss: 0.00002186
Iteration 90/1000 | Loss: 0.00002186
Iteration 91/1000 | Loss: 0.00002186
Iteration 92/1000 | Loss: 0.00002186
Iteration 93/1000 | Loss: 0.00002186
Iteration 94/1000 | Loss: 0.00002186
Iteration 95/1000 | Loss: 0.00002186
Iteration 96/1000 | Loss: 0.00002185
Iteration 97/1000 | Loss: 0.00002185
Iteration 98/1000 | Loss: 0.00002185
Iteration 99/1000 | Loss: 0.00002185
Iteration 100/1000 | Loss: 0.00002185
Iteration 101/1000 | Loss: 0.00002185
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002185
Iteration 107/1000 | Loss: 0.00002185
Iteration 108/1000 | Loss: 0.00002185
Iteration 109/1000 | Loss: 0.00002185
Iteration 110/1000 | Loss: 0.00002185
Iteration 111/1000 | Loss: 0.00002185
Iteration 112/1000 | Loss: 0.00002185
Iteration 113/1000 | Loss: 0.00002185
Iteration 114/1000 | Loss: 0.00002185
Iteration 115/1000 | Loss: 0.00002185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.1845766241312958e-05, 2.1845766241312958e-05, 2.1845766241312958e-05, 2.1845766241312958e-05, 2.1845766241312958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1845766241312958e-05

Optimization complete. Final v2v error: 3.8630082607269287 mm

Highest mean error: 4.187776565551758 mm for frame 12

Lowest mean error: 3.6052136421203613 mm for frame 51

Saving results

Total time: 48.26109576225281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836390
Iteration 2/25 | Loss: 0.00151009
Iteration 3/25 | Loss: 0.00135794
Iteration 4/25 | Loss: 0.00134052
Iteration 5/25 | Loss: 0.00133481
Iteration 6/25 | Loss: 0.00133413
Iteration 7/25 | Loss: 0.00133413
Iteration 8/25 | Loss: 0.00133413
Iteration 9/25 | Loss: 0.00133413
Iteration 10/25 | Loss: 0.00133413
Iteration 11/25 | Loss: 0.00133413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013341320445761085, 0.0013341320445761085, 0.0013341320445761085, 0.0013341320445761085, 0.0013341320445761085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013341320445761085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93390411
Iteration 2/25 | Loss: 0.00062486
Iteration 3/25 | Loss: 0.00062486
Iteration 4/25 | Loss: 0.00062486
Iteration 5/25 | Loss: 0.00062486
Iteration 6/25 | Loss: 0.00062486
Iteration 7/25 | Loss: 0.00062486
Iteration 8/25 | Loss: 0.00062486
Iteration 9/25 | Loss: 0.00062486
Iteration 10/25 | Loss: 0.00062486
Iteration 11/25 | Loss: 0.00062486
Iteration 12/25 | Loss: 0.00062486
Iteration 13/25 | Loss: 0.00062486
Iteration 14/25 | Loss: 0.00062486
Iteration 15/25 | Loss: 0.00062486
Iteration 16/25 | Loss: 0.00062486
Iteration 17/25 | Loss: 0.00062486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006248551071621478, 0.0006248551071621478, 0.0006248551071621478, 0.0006248551071621478, 0.0006248551071621478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006248551071621478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062486
Iteration 2/1000 | Loss: 0.00004147
Iteration 3/1000 | Loss: 0.00003438
Iteration 4/1000 | Loss: 0.00003158
Iteration 5/1000 | Loss: 0.00003013
Iteration 6/1000 | Loss: 0.00002921
Iteration 7/1000 | Loss: 0.00002880
Iteration 8/1000 | Loss: 0.00002851
Iteration 9/1000 | Loss: 0.00002823
Iteration 10/1000 | Loss: 0.00002819
Iteration 11/1000 | Loss: 0.00002805
Iteration 12/1000 | Loss: 0.00002804
Iteration 13/1000 | Loss: 0.00002801
Iteration 14/1000 | Loss: 0.00002798
Iteration 15/1000 | Loss: 0.00002786
Iteration 16/1000 | Loss: 0.00002785
Iteration 17/1000 | Loss: 0.00002785
Iteration 18/1000 | Loss: 0.00002785
Iteration 19/1000 | Loss: 0.00002785
Iteration 20/1000 | Loss: 0.00002785
Iteration 21/1000 | Loss: 0.00002785
Iteration 22/1000 | Loss: 0.00002785
Iteration 23/1000 | Loss: 0.00002785
Iteration 24/1000 | Loss: 0.00002785
Iteration 25/1000 | Loss: 0.00002784
Iteration 26/1000 | Loss: 0.00002784
Iteration 27/1000 | Loss: 0.00002784
Iteration 28/1000 | Loss: 0.00002784
Iteration 29/1000 | Loss: 0.00002781
Iteration 30/1000 | Loss: 0.00002774
Iteration 31/1000 | Loss: 0.00002773
Iteration 32/1000 | Loss: 0.00002772
Iteration 33/1000 | Loss: 0.00002772
Iteration 34/1000 | Loss: 0.00002769
Iteration 35/1000 | Loss: 0.00002768
Iteration 36/1000 | Loss: 0.00002768
Iteration 37/1000 | Loss: 0.00002767
Iteration 38/1000 | Loss: 0.00002767
Iteration 39/1000 | Loss: 0.00002767
Iteration 40/1000 | Loss: 0.00002767
Iteration 41/1000 | Loss: 0.00002767
Iteration 42/1000 | Loss: 0.00002767
Iteration 43/1000 | Loss: 0.00002767
Iteration 44/1000 | Loss: 0.00002767
Iteration 45/1000 | Loss: 0.00002767
Iteration 46/1000 | Loss: 0.00002767
Iteration 47/1000 | Loss: 0.00002767
Iteration 48/1000 | Loss: 0.00002767
Iteration 49/1000 | Loss: 0.00002767
Iteration 50/1000 | Loss: 0.00002767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [2.7670823328662664e-05, 2.7670823328662664e-05, 2.7670823328662664e-05, 2.7670823328662664e-05, 2.7670823328662664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7670823328662664e-05

Optimization complete. Final v2v error: 4.414879322052002 mm

Highest mean error: 4.522753715515137 mm for frame 43

Lowest mean error: 4.280496120452881 mm for frame 1

Saving results

Total time: 28.113823890686035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780966
Iteration 2/25 | Loss: 0.00138777
Iteration 3/25 | Loss: 0.00129757
Iteration 4/25 | Loss: 0.00129045
Iteration 5/25 | Loss: 0.00128995
Iteration 6/25 | Loss: 0.00128995
Iteration 7/25 | Loss: 0.00128995
Iteration 8/25 | Loss: 0.00128995
Iteration 9/25 | Loss: 0.00128995
Iteration 10/25 | Loss: 0.00128995
Iteration 11/25 | Loss: 0.00128995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00128994754049927, 0.00128994754049927, 0.00128994754049927, 0.00128994754049927, 0.00128994754049927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00128994754049927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38412380
Iteration 2/25 | Loss: 0.00071133
Iteration 3/25 | Loss: 0.00071131
Iteration 4/25 | Loss: 0.00071131
Iteration 5/25 | Loss: 0.00071130
Iteration 6/25 | Loss: 0.00071130
Iteration 7/25 | Loss: 0.00071130
Iteration 8/25 | Loss: 0.00071130
Iteration 9/25 | Loss: 0.00071130
Iteration 10/25 | Loss: 0.00071130
Iteration 11/25 | Loss: 0.00071130
Iteration 12/25 | Loss: 0.00071130
Iteration 13/25 | Loss: 0.00071130
Iteration 14/25 | Loss: 0.00071130
Iteration 15/25 | Loss: 0.00071130
Iteration 16/25 | Loss: 0.00071130
Iteration 17/25 | Loss: 0.00071130
Iteration 18/25 | Loss: 0.00071130
Iteration 19/25 | Loss: 0.00071130
Iteration 20/25 | Loss: 0.00071130
Iteration 21/25 | Loss: 0.00071130
Iteration 22/25 | Loss: 0.00071130
Iteration 23/25 | Loss: 0.00071130
Iteration 24/25 | Loss: 0.00071130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007113026804290712, 0.0007113026804290712, 0.0007113026804290712, 0.0007113026804290712, 0.0007113026804290712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007113026804290712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071130
Iteration 2/1000 | Loss: 0.00002854
Iteration 3/1000 | Loss: 0.00002238
Iteration 4/1000 | Loss: 0.00001975
Iteration 5/1000 | Loss: 0.00001836
Iteration 6/1000 | Loss: 0.00001767
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001674
Iteration 9/1000 | Loss: 0.00001637
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001536
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001497
Iteration 17/1000 | Loss: 0.00001495
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001469
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001455
Iteration 22/1000 | Loss: 0.00001451
Iteration 23/1000 | Loss: 0.00001451
Iteration 24/1000 | Loss: 0.00001450
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001449
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001443
Iteration 31/1000 | Loss: 0.00001443
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001442
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001434
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001433
Iteration 43/1000 | Loss: 0.00001433
Iteration 44/1000 | Loss: 0.00001432
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001432
Iteration 47/1000 | Loss: 0.00001432
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001432
Iteration 54/1000 | Loss: 0.00001432
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001431
Iteration 58/1000 | Loss: 0.00001431
Iteration 59/1000 | Loss: 0.00001431
Iteration 60/1000 | Loss: 0.00001431
Iteration 61/1000 | Loss: 0.00001431
Iteration 62/1000 | Loss: 0.00001431
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001431
Iteration 66/1000 | Loss: 0.00001431
Iteration 67/1000 | Loss: 0.00001431
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001430
Iteration 76/1000 | Loss: 0.00001430
Iteration 77/1000 | Loss: 0.00001430
Iteration 78/1000 | Loss: 0.00001430
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001429
Iteration 84/1000 | Loss: 0.00001429
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001428
Iteration 93/1000 | Loss: 0.00001428
Iteration 94/1000 | Loss: 0.00001428
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001428
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.4281278708949685e-05, 1.4281278708949685e-05, 1.4281278708949685e-05, 1.4281278708949685e-05, 1.4281278708949685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4281278708949685e-05

Optimization complete. Final v2v error: 3.178006410598755 mm

Highest mean error: 3.5313220024108887 mm for frame 62

Lowest mean error: 2.8424930572509766 mm for frame 27

Saving results

Total time: 37.63861560821533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407825
Iteration 2/25 | Loss: 0.00132373
Iteration 3/25 | Loss: 0.00126643
Iteration 4/25 | Loss: 0.00125679
Iteration 5/25 | Loss: 0.00125379
Iteration 6/25 | Loss: 0.00125379
Iteration 7/25 | Loss: 0.00125379
Iteration 8/25 | Loss: 0.00125379
Iteration 9/25 | Loss: 0.00125379
Iteration 10/25 | Loss: 0.00125379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012537870788946748, 0.0012537870788946748, 0.0012537870788946748, 0.0012537870788946748, 0.0012537870788946748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012537870788946748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18309689
Iteration 2/25 | Loss: 0.00080663
Iteration 3/25 | Loss: 0.00080663
Iteration 4/25 | Loss: 0.00080663
Iteration 5/25 | Loss: 0.00080663
Iteration 6/25 | Loss: 0.00080663
Iteration 7/25 | Loss: 0.00080663
Iteration 8/25 | Loss: 0.00080663
Iteration 9/25 | Loss: 0.00080663
Iteration 10/25 | Loss: 0.00080663
Iteration 11/25 | Loss: 0.00080663
Iteration 12/25 | Loss: 0.00080663
Iteration 13/25 | Loss: 0.00080663
Iteration 14/25 | Loss: 0.00080663
Iteration 15/25 | Loss: 0.00080663
Iteration 16/25 | Loss: 0.00080663
Iteration 17/25 | Loss: 0.00080663
Iteration 18/25 | Loss: 0.00080663
Iteration 19/25 | Loss: 0.00080663
Iteration 20/25 | Loss: 0.00080663
Iteration 21/25 | Loss: 0.00080663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008066257578320801, 0.0008066257578320801, 0.0008066257578320801, 0.0008066257578320801, 0.0008066257578320801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008066257578320801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080663
Iteration 2/1000 | Loss: 0.00002421
Iteration 3/1000 | Loss: 0.00001721
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001477
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001357
Iteration 9/1000 | Loss: 0.00001329
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00001307
Iteration 13/1000 | Loss: 0.00001302
Iteration 14/1000 | Loss: 0.00001287
Iteration 15/1000 | Loss: 0.00001286
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001263
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001258
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001253
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001252
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001249
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001245
Iteration 66/1000 | Loss: 0.00001244
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001239
Iteration 85/1000 | Loss: 0.00001239
Iteration 86/1000 | Loss: 0.00001239
Iteration 87/1000 | Loss: 0.00001239
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001238
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001237
Iteration 97/1000 | Loss: 0.00001237
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001230
Iteration 118/1000 | Loss: 0.00001230
Iteration 119/1000 | Loss: 0.00001229
Iteration 120/1000 | Loss: 0.00001229
Iteration 121/1000 | Loss: 0.00001229
Iteration 122/1000 | Loss: 0.00001229
Iteration 123/1000 | Loss: 0.00001229
Iteration 124/1000 | Loss: 0.00001229
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001228
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001227
Iteration 132/1000 | Loss: 0.00001227
Iteration 133/1000 | Loss: 0.00001227
Iteration 134/1000 | Loss: 0.00001227
Iteration 135/1000 | Loss: 0.00001227
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001226
Iteration 138/1000 | Loss: 0.00001226
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001226
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001224
Iteration 149/1000 | Loss: 0.00001224
Iteration 150/1000 | Loss: 0.00001224
Iteration 151/1000 | Loss: 0.00001224
Iteration 152/1000 | Loss: 0.00001224
Iteration 153/1000 | Loss: 0.00001224
Iteration 154/1000 | Loss: 0.00001223
Iteration 155/1000 | Loss: 0.00001223
Iteration 156/1000 | Loss: 0.00001223
Iteration 157/1000 | Loss: 0.00001223
Iteration 158/1000 | Loss: 0.00001223
Iteration 159/1000 | Loss: 0.00001223
Iteration 160/1000 | Loss: 0.00001223
Iteration 161/1000 | Loss: 0.00001223
Iteration 162/1000 | Loss: 0.00001223
Iteration 163/1000 | Loss: 0.00001223
Iteration 164/1000 | Loss: 0.00001223
Iteration 165/1000 | Loss: 0.00001223
Iteration 166/1000 | Loss: 0.00001223
Iteration 167/1000 | Loss: 0.00001223
Iteration 168/1000 | Loss: 0.00001223
Iteration 169/1000 | Loss: 0.00001223
Iteration 170/1000 | Loss: 0.00001223
Iteration 171/1000 | Loss: 0.00001223
Iteration 172/1000 | Loss: 0.00001223
Iteration 173/1000 | Loss: 0.00001223
Iteration 174/1000 | Loss: 0.00001223
Iteration 175/1000 | Loss: 0.00001223
Iteration 176/1000 | Loss: 0.00001223
Iteration 177/1000 | Loss: 0.00001223
Iteration 178/1000 | Loss: 0.00001223
Iteration 179/1000 | Loss: 0.00001223
Iteration 180/1000 | Loss: 0.00001223
Iteration 181/1000 | Loss: 0.00001223
Iteration 182/1000 | Loss: 0.00001223
Iteration 183/1000 | Loss: 0.00001223
Iteration 184/1000 | Loss: 0.00001223
Iteration 185/1000 | Loss: 0.00001223
Iteration 186/1000 | Loss: 0.00001223
Iteration 187/1000 | Loss: 0.00001223
Iteration 188/1000 | Loss: 0.00001223
Iteration 189/1000 | Loss: 0.00001223
Iteration 190/1000 | Loss: 0.00001223
Iteration 191/1000 | Loss: 0.00001223
Iteration 192/1000 | Loss: 0.00001223
Iteration 193/1000 | Loss: 0.00001223
Iteration 194/1000 | Loss: 0.00001223
Iteration 195/1000 | Loss: 0.00001223
Iteration 196/1000 | Loss: 0.00001223
Iteration 197/1000 | Loss: 0.00001223
Iteration 198/1000 | Loss: 0.00001223
Iteration 199/1000 | Loss: 0.00001223
Iteration 200/1000 | Loss: 0.00001223
Iteration 201/1000 | Loss: 0.00001223
Iteration 202/1000 | Loss: 0.00001223
Iteration 203/1000 | Loss: 0.00001223
Iteration 204/1000 | Loss: 0.00001223
Iteration 205/1000 | Loss: 0.00001223
Iteration 206/1000 | Loss: 0.00001223
Iteration 207/1000 | Loss: 0.00001223
Iteration 208/1000 | Loss: 0.00001223
Iteration 209/1000 | Loss: 0.00001223
Iteration 210/1000 | Loss: 0.00001223
Iteration 211/1000 | Loss: 0.00001223
Iteration 212/1000 | Loss: 0.00001223
Iteration 213/1000 | Loss: 0.00001223
Iteration 214/1000 | Loss: 0.00001223
Iteration 215/1000 | Loss: 0.00001223
Iteration 216/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.222970058734063e-05, 1.222970058734063e-05, 1.222970058734063e-05, 1.222970058734063e-05, 1.222970058734063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.222970058734063e-05

Optimization complete. Final v2v error: 2.9911441802978516 mm

Highest mean error: 3.4552853107452393 mm for frame 211

Lowest mean error: 2.876312017440796 mm for frame 77

Saving results

Total time: 44.13729524612427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428617
Iteration 2/25 | Loss: 0.00136973
Iteration 3/25 | Loss: 0.00130362
Iteration 4/25 | Loss: 0.00128923
Iteration 5/25 | Loss: 0.00128505
Iteration 6/25 | Loss: 0.00128416
Iteration 7/25 | Loss: 0.00128391
Iteration 8/25 | Loss: 0.00128391
Iteration 9/25 | Loss: 0.00128391
Iteration 10/25 | Loss: 0.00128391
Iteration 11/25 | Loss: 0.00128391
Iteration 12/25 | Loss: 0.00128391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012839145492762327, 0.0012839145492762327, 0.0012839145492762327, 0.0012839145492762327, 0.0012839145492762327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012839145492762327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56706023
Iteration 2/25 | Loss: 0.00084862
Iteration 3/25 | Loss: 0.00084862
Iteration 4/25 | Loss: 0.00084862
Iteration 5/25 | Loss: 0.00084862
Iteration 6/25 | Loss: 0.00084862
Iteration 7/25 | Loss: 0.00084862
Iteration 8/25 | Loss: 0.00084862
Iteration 9/25 | Loss: 0.00084862
Iteration 10/25 | Loss: 0.00084862
Iteration 11/25 | Loss: 0.00084862
Iteration 12/25 | Loss: 0.00084861
Iteration 13/25 | Loss: 0.00084861
Iteration 14/25 | Loss: 0.00084861
Iteration 15/25 | Loss: 0.00084861
Iteration 16/25 | Loss: 0.00084861
Iteration 17/25 | Loss: 0.00084861
Iteration 18/25 | Loss: 0.00084861
Iteration 19/25 | Loss: 0.00084861
Iteration 20/25 | Loss: 0.00084861
Iteration 21/25 | Loss: 0.00084861
Iteration 22/25 | Loss: 0.00084861
Iteration 23/25 | Loss: 0.00084861
Iteration 24/25 | Loss: 0.00084861
Iteration 25/25 | Loss: 0.00084861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084861
Iteration 2/1000 | Loss: 0.00003208
Iteration 3/1000 | Loss: 0.00002228
Iteration 4/1000 | Loss: 0.00001933
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001760
Iteration 7/1000 | Loss: 0.00001725
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001644
Iteration 10/1000 | Loss: 0.00001622
Iteration 11/1000 | Loss: 0.00001591
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001555
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001552
Iteration 16/1000 | Loss: 0.00001551
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001549
Iteration 19/1000 | Loss: 0.00001541
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001538
Iteration 22/1000 | Loss: 0.00001537
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001534
Iteration 27/1000 | Loss: 0.00001533
Iteration 28/1000 | Loss: 0.00001533
Iteration 29/1000 | Loss: 0.00001532
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001522
Iteration 34/1000 | Loss: 0.00001522
Iteration 35/1000 | Loss: 0.00001521
Iteration 36/1000 | Loss: 0.00001520
Iteration 37/1000 | Loss: 0.00001520
Iteration 38/1000 | Loss: 0.00001520
Iteration 39/1000 | Loss: 0.00001520
Iteration 40/1000 | Loss: 0.00001519
Iteration 41/1000 | Loss: 0.00001519
Iteration 42/1000 | Loss: 0.00001519
Iteration 43/1000 | Loss: 0.00001519
Iteration 44/1000 | Loss: 0.00001519
Iteration 45/1000 | Loss: 0.00001519
Iteration 46/1000 | Loss: 0.00001518
Iteration 47/1000 | Loss: 0.00001518
Iteration 48/1000 | Loss: 0.00001517
Iteration 49/1000 | Loss: 0.00001517
Iteration 50/1000 | Loss: 0.00001517
Iteration 51/1000 | Loss: 0.00001517
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001517
Iteration 55/1000 | Loss: 0.00001517
Iteration 56/1000 | Loss: 0.00001517
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001513
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001512
Iteration 61/1000 | Loss: 0.00001510
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001506
Iteration 64/1000 | Loss: 0.00001506
Iteration 65/1000 | Loss: 0.00001506
Iteration 66/1000 | Loss: 0.00001506
Iteration 67/1000 | Loss: 0.00001506
Iteration 68/1000 | Loss: 0.00001506
Iteration 69/1000 | Loss: 0.00001506
Iteration 70/1000 | Loss: 0.00001506
Iteration 71/1000 | Loss: 0.00001506
Iteration 72/1000 | Loss: 0.00001505
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001498
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001498
Iteration 88/1000 | Loss: 0.00001497
Iteration 89/1000 | Loss: 0.00001497
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001495
Iteration 96/1000 | Loss: 0.00001494
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001494
Iteration 101/1000 | Loss: 0.00001494
Iteration 102/1000 | Loss: 0.00001494
Iteration 103/1000 | Loss: 0.00001494
Iteration 104/1000 | Loss: 0.00001494
Iteration 105/1000 | Loss: 0.00001493
Iteration 106/1000 | Loss: 0.00001493
Iteration 107/1000 | Loss: 0.00001493
Iteration 108/1000 | Loss: 0.00001492
Iteration 109/1000 | Loss: 0.00001492
Iteration 110/1000 | Loss: 0.00001491
Iteration 111/1000 | Loss: 0.00001491
Iteration 112/1000 | Loss: 0.00001491
Iteration 113/1000 | Loss: 0.00001490
Iteration 114/1000 | Loss: 0.00001490
Iteration 115/1000 | Loss: 0.00001490
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001488
Iteration 119/1000 | Loss: 0.00001488
Iteration 120/1000 | Loss: 0.00001488
Iteration 121/1000 | Loss: 0.00001488
Iteration 122/1000 | Loss: 0.00001488
Iteration 123/1000 | Loss: 0.00001487
Iteration 124/1000 | Loss: 0.00001487
Iteration 125/1000 | Loss: 0.00001487
Iteration 126/1000 | Loss: 0.00001487
Iteration 127/1000 | Loss: 0.00001487
Iteration 128/1000 | Loss: 0.00001487
Iteration 129/1000 | Loss: 0.00001487
Iteration 130/1000 | Loss: 0.00001486
Iteration 131/1000 | Loss: 0.00001486
Iteration 132/1000 | Loss: 0.00001486
Iteration 133/1000 | Loss: 0.00001486
Iteration 134/1000 | Loss: 0.00001486
Iteration 135/1000 | Loss: 0.00001486
Iteration 136/1000 | Loss: 0.00001486
Iteration 137/1000 | Loss: 0.00001485
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001485
Iteration 143/1000 | Loss: 0.00001485
Iteration 144/1000 | Loss: 0.00001485
Iteration 145/1000 | Loss: 0.00001485
Iteration 146/1000 | Loss: 0.00001485
Iteration 147/1000 | Loss: 0.00001485
Iteration 148/1000 | Loss: 0.00001485
Iteration 149/1000 | Loss: 0.00001484
Iteration 150/1000 | Loss: 0.00001484
Iteration 151/1000 | Loss: 0.00001484
Iteration 152/1000 | Loss: 0.00001484
Iteration 153/1000 | Loss: 0.00001484
Iteration 154/1000 | Loss: 0.00001484
Iteration 155/1000 | Loss: 0.00001484
Iteration 156/1000 | Loss: 0.00001484
Iteration 157/1000 | Loss: 0.00001484
Iteration 158/1000 | Loss: 0.00001484
Iteration 159/1000 | Loss: 0.00001484
Iteration 160/1000 | Loss: 0.00001484
Iteration 161/1000 | Loss: 0.00001484
Iteration 162/1000 | Loss: 0.00001484
Iteration 163/1000 | Loss: 0.00001484
Iteration 164/1000 | Loss: 0.00001484
Iteration 165/1000 | Loss: 0.00001484
Iteration 166/1000 | Loss: 0.00001484
Iteration 167/1000 | Loss: 0.00001484
Iteration 168/1000 | Loss: 0.00001484
Iteration 169/1000 | Loss: 0.00001484
Iteration 170/1000 | Loss: 0.00001484
Iteration 171/1000 | Loss: 0.00001483
Iteration 172/1000 | Loss: 0.00001483
Iteration 173/1000 | Loss: 0.00001483
Iteration 174/1000 | Loss: 0.00001483
Iteration 175/1000 | Loss: 0.00001483
Iteration 176/1000 | Loss: 0.00001483
Iteration 177/1000 | Loss: 0.00001483
Iteration 178/1000 | Loss: 0.00001483
Iteration 179/1000 | Loss: 0.00001483
Iteration 180/1000 | Loss: 0.00001483
Iteration 181/1000 | Loss: 0.00001483
Iteration 182/1000 | Loss: 0.00001483
Iteration 183/1000 | Loss: 0.00001483
Iteration 184/1000 | Loss: 0.00001483
Iteration 185/1000 | Loss: 0.00001483
Iteration 186/1000 | Loss: 0.00001483
Iteration 187/1000 | Loss: 0.00001483
Iteration 188/1000 | Loss: 0.00001483
Iteration 189/1000 | Loss: 0.00001482
Iteration 190/1000 | Loss: 0.00001482
Iteration 191/1000 | Loss: 0.00001482
Iteration 192/1000 | Loss: 0.00001482
Iteration 193/1000 | Loss: 0.00001482
Iteration 194/1000 | Loss: 0.00001482
Iteration 195/1000 | Loss: 0.00001482
Iteration 196/1000 | Loss: 0.00001482
Iteration 197/1000 | Loss: 0.00001482
Iteration 198/1000 | Loss: 0.00001482
Iteration 199/1000 | Loss: 0.00001481
Iteration 200/1000 | Loss: 0.00001481
Iteration 201/1000 | Loss: 0.00001481
Iteration 202/1000 | Loss: 0.00001481
Iteration 203/1000 | Loss: 0.00001481
Iteration 204/1000 | Loss: 0.00001481
Iteration 205/1000 | Loss: 0.00001481
Iteration 206/1000 | Loss: 0.00001481
Iteration 207/1000 | Loss: 0.00001481
Iteration 208/1000 | Loss: 0.00001481
Iteration 209/1000 | Loss: 0.00001481
Iteration 210/1000 | Loss: 0.00001481
Iteration 211/1000 | Loss: 0.00001481
Iteration 212/1000 | Loss: 0.00001481
Iteration 213/1000 | Loss: 0.00001481
Iteration 214/1000 | Loss: 0.00001481
Iteration 215/1000 | Loss: 0.00001481
Iteration 216/1000 | Loss: 0.00001481
Iteration 217/1000 | Loss: 0.00001481
Iteration 218/1000 | Loss: 0.00001481
Iteration 219/1000 | Loss: 0.00001481
Iteration 220/1000 | Loss: 0.00001481
Iteration 221/1000 | Loss: 0.00001481
Iteration 222/1000 | Loss: 0.00001481
Iteration 223/1000 | Loss: 0.00001481
Iteration 224/1000 | Loss: 0.00001481
Iteration 225/1000 | Loss: 0.00001481
Iteration 226/1000 | Loss: 0.00001481
Iteration 227/1000 | Loss: 0.00001481
Iteration 228/1000 | Loss: 0.00001481
Iteration 229/1000 | Loss: 0.00001481
Iteration 230/1000 | Loss: 0.00001481
Iteration 231/1000 | Loss: 0.00001481
Iteration 232/1000 | Loss: 0.00001481
Iteration 233/1000 | Loss: 0.00001481
Iteration 234/1000 | Loss: 0.00001481
Iteration 235/1000 | Loss: 0.00001481
Iteration 236/1000 | Loss: 0.00001481
Iteration 237/1000 | Loss: 0.00001481
Iteration 238/1000 | Loss: 0.00001481
Iteration 239/1000 | Loss: 0.00001481
Iteration 240/1000 | Loss: 0.00001481
Iteration 241/1000 | Loss: 0.00001481
Iteration 242/1000 | Loss: 0.00001481
Iteration 243/1000 | Loss: 0.00001481
Iteration 244/1000 | Loss: 0.00001481
Iteration 245/1000 | Loss: 0.00001481
Iteration 246/1000 | Loss: 0.00001481
Iteration 247/1000 | Loss: 0.00001481
Iteration 248/1000 | Loss: 0.00001481
Iteration 249/1000 | Loss: 0.00001481
Iteration 250/1000 | Loss: 0.00001481
Iteration 251/1000 | Loss: 0.00001481
Iteration 252/1000 | Loss: 0.00001481
Iteration 253/1000 | Loss: 0.00001481
Iteration 254/1000 | Loss: 0.00001481
Iteration 255/1000 | Loss: 0.00001481
Iteration 256/1000 | Loss: 0.00001481
Iteration 257/1000 | Loss: 0.00001481
Iteration 258/1000 | Loss: 0.00001481
Iteration 259/1000 | Loss: 0.00001481
Iteration 260/1000 | Loss: 0.00001481
Iteration 261/1000 | Loss: 0.00001481
Iteration 262/1000 | Loss: 0.00001481
Iteration 263/1000 | Loss: 0.00001481
Iteration 264/1000 | Loss: 0.00001481
Iteration 265/1000 | Loss: 0.00001481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.4809903404966462e-05, 1.4809903404966462e-05, 1.4809903404966462e-05, 1.4809903404966462e-05, 1.4809903404966462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4809903404966462e-05

Optimization complete. Final v2v error: 3.2971677780151367 mm

Highest mean error: 3.7255728244781494 mm for frame 89

Lowest mean error: 3.1612393856048584 mm for frame 123

Saving results

Total time: 44.912144899368286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676512
Iteration 2/25 | Loss: 0.00178075
Iteration 3/25 | Loss: 0.00153062
Iteration 4/25 | Loss: 0.00149163
Iteration 5/25 | Loss: 0.00148275
Iteration 6/25 | Loss: 0.00148066
Iteration 7/25 | Loss: 0.00148066
Iteration 8/25 | Loss: 0.00148066
Iteration 9/25 | Loss: 0.00148066
Iteration 10/25 | Loss: 0.00148066
Iteration 11/25 | Loss: 0.00148066
Iteration 12/25 | Loss: 0.00148066
Iteration 13/25 | Loss: 0.00148066
Iteration 14/25 | Loss: 0.00148066
Iteration 15/25 | Loss: 0.00148066
Iteration 16/25 | Loss: 0.00148066
Iteration 17/25 | Loss: 0.00148066
Iteration 18/25 | Loss: 0.00148066
Iteration 19/25 | Loss: 0.00148066
Iteration 20/25 | Loss: 0.00148066
Iteration 21/25 | Loss: 0.00148066
Iteration 22/25 | Loss: 0.00148066
Iteration 23/25 | Loss: 0.00148066
Iteration 24/25 | Loss: 0.00148066
Iteration 25/25 | Loss: 0.00148066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001480660866945982, 0.001480660866945982, 0.001480660866945982, 0.001480660866945982, 0.001480660866945982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001480660866945982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20045614
Iteration 2/25 | Loss: 0.00113771
Iteration 3/25 | Loss: 0.00113771
Iteration 4/25 | Loss: 0.00113770
Iteration 5/25 | Loss: 0.00113770
Iteration 6/25 | Loss: 0.00113770
Iteration 7/25 | Loss: 0.00113770
Iteration 8/25 | Loss: 0.00113770
Iteration 9/25 | Loss: 0.00113770
Iteration 10/25 | Loss: 0.00113770
Iteration 11/25 | Loss: 0.00113770
Iteration 12/25 | Loss: 0.00113770
Iteration 13/25 | Loss: 0.00113770
Iteration 14/25 | Loss: 0.00113770
Iteration 15/25 | Loss: 0.00113770
Iteration 16/25 | Loss: 0.00113770
Iteration 17/25 | Loss: 0.00113770
Iteration 18/25 | Loss: 0.00113770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011377019109204412, 0.0011377019109204412, 0.0011377019109204412, 0.0011377019109204412, 0.0011377019109204412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011377019109204412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113770
Iteration 2/1000 | Loss: 0.00008073
Iteration 3/1000 | Loss: 0.00004925
Iteration 4/1000 | Loss: 0.00004394
Iteration 5/1000 | Loss: 0.00004082
Iteration 6/1000 | Loss: 0.00003841
Iteration 7/1000 | Loss: 0.00003685
Iteration 8/1000 | Loss: 0.00003556
Iteration 9/1000 | Loss: 0.00003484
Iteration 10/1000 | Loss: 0.00003433
Iteration 11/1000 | Loss: 0.00003391
Iteration 12/1000 | Loss: 0.00003353
Iteration 13/1000 | Loss: 0.00003322
Iteration 14/1000 | Loss: 0.00003310
Iteration 15/1000 | Loss: 0.00003291
Iteration 16/1000 | Loss: 0.00003274
Iteration 17/1000 | Loss: 0.00003270
Iteration 18/1000 | Loss: 0.00003263
Iteration 19/1000 | Loss: 0.00003258
Iteration 20/1000 | Loss: 0.00003249
Iteration 21/1000 | Loss: 0.00003246
Iteration 22/1000 | Loss: 0.00003246
Iteration 23/1000 | Loss: 0.00003244
Iteration 24/1000 | Loss: 0.00003241
Iteration 25/1000 | Loss: 0.00003239
Iteration 26/1000 | Loss: 0.00003238
Iteration 27/1000 | Loss: 0.00003236
Iteration 28/1000 | Loss: 0.00003233
Iteration 29/1000 | Loss: 0.00003232
Iteration 30/1000 | Loss: 0.00003232
Iteration 31/1000 | Loss: 0.00003232
Iteration 32/1000 | Loss: 0.00003231
Iteration 33/1000 | Loss: 0.00003229
Iteration 34/1000 | Loss: 0.00003228
Iteration 35/1000 | Loss: 0.00003227
Iteration 36/1000 | Loss: 0.00003227
Iteration 37/1000 | Loss: 0.00003227
Iteration 38/1000 | Loss: 0.00003227
Iteration 39/1000 | Loss: 0.00003227
Iteration 40/1000 | Loss: 0.00003227
Iteration 41/1000 | Loss: 0.00003227
Iteration 42/1000 | Loss: 0.00003227
Iteration 43/1000 | Loss: 0.00003227
Iteration 44/1000 | Loss: 0.00003226
Iteration 45/1000 | Loss: 0.00003226
Iteration 46/1000 | Loss: 0.00003226
Iteration 47/1000 | Loss: 0.00003226
Iteration 48/1000 | Loss: 0.00003226
Iteration 49/1000 | Loss: 0.00003226
Iteration 50/1000 | Loss: 0.00003226
Iteration 51/1000 | Loss: 0.00003226
Iteration 52/1000 | Loss: 0.00003226
Iteration 53/1000 | Loss: 0.00003226
Iteration 54/1000 | Loss: 0.00003225
Iteration 55/1000 | Loss: 0.00003225
Iteration 56/1000 | Loss: 0.00003224
Iteration 57/1000 | Loss: 0.00003223
Iteration 58/1000 | Loss: 0.00003222
Iteration 59/1000 | Loss: 0.00003222
Iteration 60/1000 | Loss: 0.00003222
Iteration 61/1000 | Loss: 0.00003222
Iteration 62/1000 | Loss: 0.00003222
Iteration 63/1000 | Loss: 0.00003222
Iteration 64/1000 | Loss: 0.00003222
Iteration 65/1000 | Loss: 0.00003221
Iteration 66/1000 | Loss: 0.00003221
Iteration 67/1000 | Loss: 0.00003220
Iteration 68/1000 | Loss: 0.00003219
Iteration 69/1000 | Loss: 0.00003219
Iteration 70/1000 | Loss: 0.00003218
Iteration 71/1000 | Loss: 0.00003218
Iteration 72/1000 | Loss: 0.00003217
Iteration 73/1000 | Loss: 0.00003217
Iteration 74/1000 | Loss: 0.00003217
Iteration 75/1000 | Loss: 0.00003216
Iteration 76/1000 | Loss: 0.00003216
Iteration 77/1000 | Loss: 0.00003216
Iteration 78/1000 | Loss: 0.00003215
Iteration 79/1000 | Loss: 0.00003215
Iteration 80/1000 | Loss: 0.00003214
Iteration 81/1000 | Loss: 0.00003214
Iteration 82/1000 | Loss: 0.00003214
Iteration 83/1000 | Loss: 0.00003213
Iteration 84/1000 | Loss: 0.00003213
Iteration 85/1000 | Loss: 0.00003213
Iteration 86/1000 | Loss: 0.00003213
Iteration 87/1000 | Loss: 0.00003213
Iteration 88/1000 | Loss: 0.00003213
Iteration 89/1000 | Loss: 0.00003213
Iteration 90/1000 | Loss: 0.00003212
Iteration 91/1000 | Loss: 0.00003212
Iteration 92/1000 | Loss: 0.00003212
Iteration 93/1000 | Loss: 0.00003212
Iteration 94/1000 | Loss: 0.00003211
Iteration 95/1000 | Loss: 0.00003211
Iteration 96/1000 | Loss: 0.00003211
Iteration 97/1000 | Loss: 0.00003211
Iteration 98/1000 | Loss: 0.00003211
Iteration 99/1000 | Loss: 0.00003211
Iteration 100/1000 | Loss: 0.00003211
Iteration 101/1000 | Loss: 0.00003211
Iteration 102/1000 | Loss: 0.00003211
Iteration 103/1000 | Loss: 0.00003210
Iteration 104/1000 | Loss: 0.00003210
Iteration 105/1000 | Loss: 0.00003210
Iteration 106/1000 | Loss: 0.00003210
Iteration 107/1000 | Loss: 0.00003210
Iteration 108/1000 | Loss: 0.00003210
Iteration 109/1000 | Loss: 0.00003210
Iteration 110/1000 | Loss: 0.00003210
Iteration 111/1000 | Loss: 0.00003210
Iteration 112/1000 | Loss: 0.00003210
Iteration 113/1000 | Loss: 0.00003210
Iteration 114/1000 | Loss: 0.00003209
Iteration 115/1000 | Loss: 0.00003209
Iteration 116/1000 | Loss: 0.00003209
Iteration 117/1000 | Loss: 0.00003209
Iteration 118/1000 | Loss: 0.00003209
Iteration 119/1000 | Loss: 0.00003209
Iteration 120/1000 | Loss: 0.00003209
Iteration 121/1000 | Loss: 0.00003209
Iteration 122/1000 | Loss: 0.00003208
Iteration 123/1000 | Loss: 0.00003208
Iteration 124/1000 | Loss: 0.00003208
Iteration 125/1000 | Loss: 0.00003208
Iteration 126/1000 | Loss: 0.00003208
Iteration 127/1000 | Loss: 0.00003208
Iteration 128/1000 | Loss: 0.00003208
Iteration 129/1000 | Loss: 0.00003208
Iteration 130/1000 | Loss: 0.00003208
Iteration 131/1000 | Loss: 0.00003208
Iteration 132/1000 | Loss: 0.00003208
Iteration 133/1000 | Loss: 0.00003208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [3.207663758075796e-05, 3.207663758075796e-05, 3.207663758075796e-05, 3.207663758075796e-05, 3.207663758075796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.207663758075796e-05

Optimization complete. Final v2v error: 4.753043174743652 mm

Highest mean error: 5.1832194328308105 mm for frame 88

Lowest mean error: 4.2977495193481445 mm for frame 187

Saving results

Total time: 48.16120791435242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011970
Iteration 2/25 | Loss: 0.00177389
Iteration 3/25 | Loss: 0.00149713
Iteration 4/25 | Loss: 0.00147183
Iteration 5/25 | Loss: 0.00146345
Iteration 6/25 | Loss: 0.00146149
Iteration 7/25 | Loss: 0.00146149
Iteration 8/25 | Loss: 0.00146149
Iteration 9/25 | Loss: 0.00146149
Iteration 10/25 | Loss: 0.00146149
Iteration 11/25 | Loss: 0.00146149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014614876126870513, 0.0014614876126870513, 0.0014614876126870513, 0.0014614876126870513, 0.0014614876126870513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014614876126870513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96038383
Iteration 2/25 | Loss: 0.00125241
Iteration 3/25 | Loss: 0.00125241
Iteration 4/25 | Loss: 0.00125241
Iteration 5/25 | Loss: 0.00125241
Iteration 6/25 | Loss: 0.00125241
Iteration 7/25 | Loss: 0.00125241
Iteration 8/25 | Loss: 0.00125241
Iteration 9/25 | Loss: 0.00125241
Iteration 10/25 | Loss: 0.00125241
Iteration 11/25 | Loss: 0.00125241
Iteration 12/25 | Loss: 0.00125241
Iteration 13/25 | Loss: 0.00125241
Iteration 14/25 | Loss: 0.00125241
Iteration 15/25 | Loss: 0.00125241
Iteration 16/25 | Loss: 0.00125241
Iteration 17/25 | Loss: 0.00125241
Iteration 18/25 | Loss: 0.00125241
Iteration 19/25 | Loss: 0.00125241
Iteration 20/25 | Loss: 0.00125241
Iteration 21/25 | Loss: 0.00125241
Iteration 22/25 | Loss: 0.00125241
Iteration 23/25 | Loss: 0.00125241
Iteration 24/25 | Loss: 0.00125241
Iteration 25/25 | Loss: 0.00125241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012524076737463474, 0.0012524076737463474, 0.0012524076737463474, 0.0012524076737463474, 0.0012524076737463474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012524076737463474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125241
Iteration 2/1000 | Loss: 0.00009780
Iteration 3/1000 | Loss: 0.00004653
Iteration 4/1000 | Loss: 0.00003741
Iteration 5/1000 | Loss: 0.00003493
Iteration 6/1000 | Loss: 0.00003367
Iteration 7/1000 | Loss: 0.00003273
Iteration 8/1000 | Loss: 0.00003187
Iteration 9/1000 | Loss: 0.00003132
Iteration 10/1000 | Loss: 0.00003092
Iteration 11/1000 | Loss: 0.00003058
Iteration 12/1000 | Loss: 0.00003033
Iteration 13/1000 | Loss: 0.00003013
Iteration 14/1000 | Loss: 0.00002990
Iteration 15/1000 | Loss: 0.00002972
Iteration 16/1000 | Loss: 0.00002968
Iteration 17/1000 | Loss: 0.00002956
Iteration 18/1000 | Loss: 0.00002951
Iteration 19/1000 | Loss: 0.00002940
Iteration 20/1000 | Loss: 0.00002936
Iteration 21/1000 | Loss: 0.00002936
Iteration 22/1000 | Loss: 0.00002933
Iteration 23/1000 | Loss: 0.00002933
Iteration 24/1000 | Loss: 0.00002931
Iteration 25/1000 | Loss: 0.00002930
Iteration 26/1000 | Loss: 0.00002930
Iteration 27/1000 | Loss: 0.00002929
Iteration 28/1000 | Loss: 0.00002928
Iteration 29/1000 | Loss: 0.00002928
Iteration 30/1000 | Loss: 0.00002924
Iteration 31/1000 | Loss: 0.00002924
Iteration 32/1000 | Loss: 0.00002924
Iteration 33/1000 | Loss: 0.00002921
Iteration 34/1000 | Loss: 0.00002921
Iteration 35/1000 | Loss: 0.00002920
Iteration 36/1000 | Loss: 0.00002920
Iteration 37/1000 | Loss: 0.00002920
Iteration 38/1000 | Loss: 0.00002920
Iteration 39/1000 | Loss: 0.00002920
Iteration 40/1000 | Loss: 0.00002920
Iteration 41/1000 | Loss: 0.00002920
Iteration 42/1000 | Loss: 0.00002920
Iteration 43/1000 | Loss: 0.00002920
Iteration 44/1000 | Loss: 0.00002920
Iteration 45/1000 | Loss: 0.00002919
Iteration 46/1000 | Loss: 0.00002919
Iteration 47/1000 | Loss: 0.00002919
Iteration 48/1000 | Loss: 0.00002916
Iteration 49/1000 | Loss: 0.00002916
Iteration 50/1000 | Loss: 0.00002916
Iteration 51/1000 | Loss: 0.00002916
Iteration 52/1000 | Loss: 0.00002915
Iteration 53/1000 | Loss: 0.00002915
Iteration 54/1000 | Loss: 0.00002915
Iteration 55/1000 | Loss: 0.00002915
Iteration 56/1000 | Loss: 0.00002915
Iteration 57/1000 | Loss: 0.00002915
Iteration 58/1000 | Loss: 0.00002915
Iteration 59/1000 | Loss: 0.00002915
Iteration 60/1000 | Loss: 0.00002915
Iteration 61/1000 | Loss: 0.00002915
Iteration 62/1000 | Loss: 0.00002915
Iteration 63/1000 | Loss: 0.00002914
Iteration 64/1000 | Loss: 0.00002914
Iteration 65/1000 | Loss: 0.00002914
Iteration 66/1000 | Loss: 0.00002914
Iteration 67/1000 | Loss: 0.00002913
Iteration 68/1000 | Loss: 0.00002913
Iteration 69/1000 | Loss: 0.00002913
Iteration 70/1000 | Loss: 0.00002913
Iteration 71/1000 | Loss: 0.00002912
Iteration 72/1000 | Loss: 0.00002912
Iteration 73/1000 | Loss: 0.00002912
Iteration 74/1000 | Loss: 0.00002911
Iteration 75/1000 | Loss: 0.00002910
Iteration 76/1000 | Loss: 0.00002910
Iteration 77/1000 | Loss: 0.00002909
Iteration 78/1000 | Loss: 0.00002909
Iteration 79/1000 | Loss: 0.00002908
Iteration 80/1000 | Loss: 0.00002908
Iteration 81/1000 | Loss: 0.00002907
Iteration 82/1000 | Loss: 0.00002907
Iteration 83/1000 | Loss: 0.00002907
Iteration 84/1000 | Loss: 0.00002907
Iteration 85/1000 | Loss: 0.00002907
Iteration 86/1000 | Loss: 0.00002906
Iteration 87/1000 | Loss: 0.00002906
Iteration 88/1000 | Loss: 0.00002906
Iteration 89/1000 | Loss: 0.00002905
Iteration 90/1000 | Loss: 0.00002905
Iteration 91/1000 | Loss: 0.00002905
Iteration 92/1000 | Loss: 0.00002905
Iteration 93/1000 | Loss: 0.00002905
Iteration 94/1000 | Loss: 0.00002905
Iteration 95/1000 | Loss: 0.00002905
Iteration 96/1000 | Loss: 0.00002905
Iteration 97/1000 | Loss: 0.00002905
Iteration 98/1000 | Loss: 0.00002904
Iteration 99/1000 | Loss: 0.00002904
Iteration 100/1000 | Loss: 0.00002904
Iteration 101/1000 | Loss: 0.00002904
Iteration 102/1000 | Loss: 0.00002903
Iteration 103/1000 | Loss: 0.00002903
Iteration 104/1000 | Loss: 0.00002903
Iteration 105/1000 | Loss: 0.00002903
Iteration 106/1000 | Loss: 0.00002903
Iteration 107/1000 | Loss: 0.00002903
Iteration 108/1000 | Loss: 0.00002903
Iteration 109/1000 | Loss: 0.00002902
Iteration 110/1000 | Loss: 0.00002902
Iteration 111/1000 | Loss: 0.00002902
Iteration 112/1000 | Loss: 0.00002902
Iteration 113/1000 | Loss: 0.00002901
Iteration 114/1000 | Loss: 0.00002901
Iteration 115/1000 | Loss: 0.00002901
Iteration 116/1000 | Loss: 0.00002901
Iteration 117/1000 | Loss: 0.00002901
Iteration 118/1000 | Loss: 0.00002901
Iteration 119/1000 | Loss: 0.00002900
Iteration 120/1000 | Loss: 0.00002900
Iteration 121/1000 | Loss: 0.00002900
Iteration 122/1000 | Loss: 0.00002900
Iteration 123/1000 | Loss: 0.00002900
Iteration 124/1000 | Loss: 0.00002900
Iteration 125/1000 | Loss: 0.00002900
Iteration 126/1000 | Loss: 0.00002900
Iteration 127/1000 | Loss: 0.00002900
Iteration 128/1000 | Loss: 0.00002900
Iteration 129/1000 | Loss: 0.00002900
Iteration 130/1000 | Loss: 0.00002900
Iteration 131/1000 | Loss: 0.00002900
Iteration 132/1000 | Loss: 0.00002900
Iteration 133/1000 | Loss: 0.00002900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.8997474146308377e-05, 2.8997474146308377e-05, 2.8997474146308377e-05, 2.8997474146308377e-05, 2.8997474146308377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8997474146308377e-05

Optimization complete. Final v2v error: 4.429540634155273 mm

Highest mean error: 5.270477294921875 mm for frame 51

Lowest mean error: 3.7468132972717285 mm for frame 27

Saving results

Total time: 42.75396656990051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758816
Iteration 2/25 | Loss: 0.00169529
Iteration 3/25 | Loss: 0.00142369
Iteration 4/25 | Loss: 0.00137366
Iteration 5/25 | Loss: 0.00136648
Iteration 6/25 | Loss: 0.00136451
Iteration 7/25 | Loss: 0.00136451
Iteration 8/25 | Loss: 0.00136451
Iteration 9/25 | Loss: 0.00136451
Iteration 10/25 | Loss: 0.00136451
Iteration 11/25 | Loss: 0.00136451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013645064318552613, 0.0013645064318552613, 0.0013645064318552613, 0.0013645064318552613, 0.0013645064318552613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013645064318552613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44515777
Iteration 2/25 | Loss: 0.00065219
Iteration 3/25 | Loss: 0.00065219
Iteration 4/25 | Loss: 0.00065219
Iteration 5/25 | Loss: 0.00065219
Iteration 6/25 | Loss: 0.00065219
Iteration 7/25 | Loss: 0.00065219
Iteration 8/25 | Loss: 0.00065219
Iteration 9/25 | Loss: 0.00065219
Iteration 10/25 | Loss: 0.00065219
Iteration 11/25 | Loss: 0.00065219
Iteration 12/25 | Loss: 0.00065219
Iteration 13/25 | Loss: 0.00065219
Iteration 14/25 | Loss: 0.00065219
Iteration 15/25 | Loss: 0.00065219
Iteration 16/25 | Loss: 0.00065219
Iteration 17/25 | Loss: 0.00065219
Iteration 18/25 | Loss: 0.00065219
Iteration 19/25 | Loss: 0.00065219
Iteration 20/25 | Loss: 0.00065219
Iteration 21/25 | Loss: 0.00065219
Iteration 22/25 | Loss: 0.00065219
Iteration 23/25 | Loss: 0.00065219
Iteration 24/25 | Loss: 0.00065219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006521879695355892, 0.0006521879695355892, 0.0006521879695355892, 0.0006521879695355892, 0.0006521879695355892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006521879695355892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065219
Iteration 2/1000 | Loss: 0.00007249
Iteration 3/1000 | Loss: 0.00004966
Iteration 4/1000 | Loss: 0.00003931
Iteration 5/1000 | Loss: 0.00003756
Iteration 6/1000 | Loss: 0.00003621
Iteration 7/1000 | Loss: 0.00003501
Iteration 8/1000 | Loss: 0.00003431
Iteration 9/1000 | Loss: 0.00003375
Iteration 10/1000 | Loss: 0.00003334
Iteration 11/1000 | Loss: 0.00003304
Iteration 12/1000 | Loss: 0.00003276
Iteration 13/1000 | Loss: 0.00003250
Iteration 14/1000 | Loss: 0.00003233
Iteration 15/1000 | Loss: 0.00003232
Iteration 16/1000 | Loss: 0.00003220
Iteration 17/1000 | Loss: 0.00003203
Iteration 18/1000 | Loss: 0.00003188
Iteration 19/1000 | Loss: 0.00003178
Iteration 20/1000 | Loss: 0.00003178
Iteration 21/1000 | Loss: 0.00003178
Iteration 22/1000 | Loss: 0.00003175
Iteration 23/1000 | Loss: 0.00003175
Iteration 24/1000 | Loss: 0.00003172
Iteration 25/1000 | Loss: 0.00003172
Iteration 26/1000 | Loss: 0.00003171
Iteration 27/1000 | Loss: 0.00003170
Iteration 28/1000 | Loss: 0.00003170
Iteration 29/1000 | Loss: 0.00003169
Iteration 30/1000 | Loss: 0.00003168
Iteration 31/1000 | Loss: 0.00003168
Iteration 32/1000 | Loss: 0.00003167
Iteration 33/1000 | Loss: 0.00003167
Iteration 34/1000 | Loss: 0.00003166
Iteration 35/1000 | Loss: 0.00003166
Iteration 36/1000 | Loss: 0.00003166
Iteration 37/1000 | Loss: 0.00003165
Iteration 38/1000 | Loss: 0.00003165
Iteration 39/1000 | Loss: 0.00003165
Iteration 40/1000 | Loss: 0.00003165
Iteration 41/1000 | Loss: 0.00003164
Iteration 42/1000 | Loss: 0.00003164
Iteration 43/1000 | Loss: 0.00003163
Iteration 44/1000 | Loss: 0.00003163
Iteration 45/1000 | Loss: 0.00003163
Iteration 46/1000 | Loss: 0.00003163
Iteration 47/1000 | Loss: 0.00003162
Iteration 48/1000 | Loss: 0.00003162
Iteration 49/1000 | Loss: 0.00003162
Iteration 50/1000 | Loss: 0.00003161
Iteration 51/1000 | Loss: 0.00003161
Iteration 52/1000 | Loss: 0.00003161
Iteration 53/1000 | Loss: 0.00003161
Iteration 54/1000 | Loss: 0.00003161
Iteration 55/1000 | Loss: 0.00003160
Iteration 56/1000 | Loss: 0.00003160
Iteration 57/1000 | Loss: 0.00003160
Iteration 58/1000 | Loss: 0.00003160
Iteration 59/1000 | Loss: 0.00003160
Iteration 60/1000 | Loss: 0.00003160
Iteration 61/1000 | Loss: 0.00003160
Iteration 62/1000 | Loss: 0.00003160
Iteration 63/1000 | Loss: 0.00003159
Iteration 64/1000 | Loss: 0.00003159
Iteration 65/1000 | Loss: 0.00003159
Iteration 66/1000 | Loss: 0.00003159
Iteration 67/1000 | Loss: 0.00003159
Iteration 68/1000 | Loss: 0.00003159
Iteration 69/1000 | Loss: 0.00003159
Iteration 70/1000 | Loss: 0.00003159
Iteration 71/1000 | Loss: 0.00003158
Iteration 72/1000 | Loss: 0.00003158
Iteration 73/1000 | Loss: 0.00003158
Iteration 74/1000 | Loss: 0.00003158
Iteration 75/1000 | Loss: 0.00003158
Iteration 76/1000 | Loss: 0.00003158
Iteration 77/1000 | Loss: 0.00003158
Iteration 78/1000 | Loss: 0.00003158
Iteration 79/1000 | Loss: 0.00003158
Iteration 80/1000 | Loss: 0.00003158
Iteration 81/1000 | Loss: 0.00003158
Iteration 82/1000 | Loss: 0.00003158
Iteration 83/1000 | Loss: 0.00003157
Iteration 84/1000 | Loss: 0.00003157
Iteration 85/1000 | Loss: 0.00003157
Iteration 86/1000 | Loss: 0.00003157
Iteration 87/1000 | Loss: 0.00003157
Iteration 88/1000 | Loss: 0.00003157
Iteration 89/1000 | Loss: 0.00003157
Iteration 90/1000 | Loss: 0.00003157
Iteration 91/1000 | Loss: 0.00003157
Iteration 92/1000 | Loss: 0.00003157
Iteration 93/1000 | Loss: 0.00003157
Iteration 94/1000 | Loss: 0.00003157
Iteration 95/1000 | Loss: 0.00003157
Iteration 96/1000 | Loss: 0.00003157
Iteration 97/1000 | Loss: 0.00003157
Iteration 98/1000 | Loss: 0.00003157
Iteration 99/1000 | Loss: 0.00003157
Iteration 100/1000 | Loss: 0.00003157
Iteration 101/1000 | Loss: 0.00003157
Iteration 102/1000 | Loss: 0.00003157
Iteration 103/1000 | Loss: 0.00003157
Iteration 104/1000 | Loss: 0.00003157
Iteration 105/1000 | Loss: 0.00003157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [3.157242827001028e-05, 3.157242827001028e-05, 3.157242827001028e-05, 3.157242827001028e-05, 3.157242827001028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.157242827001028e-05

Optimization complete. Final v2v error: 4.667116165161133 mm

Highest mean error: 6.291055679321289 mm for frame 26

Lowest mean error: 3.5724198818206787 mm for frame 179

Saving results

Total time: 40.553409814834595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442600
Iteration 2/25 | Loss: 0.00151911
Iteration 3/25 | Loss: 0.00135274
Iteration 4/25 | Loss: 0.00133720
Iteration 5/25 | Loss: 0.00133504
Iteration 6/25 | Loss: 0.00133459
Iteration 7/25 | Loss: 0.00133459
Iteration 8/25 | Loss: 0.00133459
Iteration 9/25 | Loss: 0.00133459
Iteration 10/25 | Loss: 0.00133459
Iteration 11/25 | Loss: 0.00133459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013345873448997736, 0.0013345873448997736, 0.0013345873448997736, 0.0013345873448997736, 0.0013345873448997736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013345873448997736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.30906534
Iteration 2/25 | Loss: 0.00086717
Iteration 3/25 | Loss: 0.00086717
Iteration 4/25 | Loss: 0.00086717
Iteration 5/25 | Loss: 0.00086717
Iteration 6/25 | Loss: 0.00086717
Iteration 7/25 | Loss: 0.00086717
Iteration 8/25 | Loss: 0.00086717
Iteration 9/25 | Loss: 0.00086717
Iteration 10/25 | Loss: 0.00086717
Iteration 11/25 | Loss: 0.00086717
Iteration 12/25 | Loss: 0.00086717
Iteration 13/25 | Loss: 0.00086717
Iteration 14/25 | Loss: 0.00086717
Iteration 15/25 | Loss: 0.00086717
Iteration 16/25 | Loss: 0.00086717
Iteration 17/25 | Loss: 0.00086717
Iteration 18/25 | Loss: 0.00086717
Iteration 19/25 | Loss: 0.00086717
Iteration 20/25 | Loss: 0.00086717
Iteration 21/25 | Loss: 0.00086717
Iteration 22/25 | Loss: 0.00086717
Iteration 23/25 | Loss: 0.00086717
Iteration 24/25 | Loss: 0.00086717
Iteration 25/25 | Loss: 0.00086717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086717
Iteration 2/1000 | Loss: 0.00003734
Iteration 3/1000 | Loss: 0.00002377
Iteration 4/1000 | Loss: 0.00002069
Iteration 5/1000 | Loss: 0.00001971
Iteration 6/1000 | Loss: 0.00001894
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001770
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001725
Iteration 11/1000 | Loss: 0.00001705
Iteration 12/1000 | Loss: 0.00001695
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001672
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001662
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001653
Iteration 20/1000 | Loss: 0.00001652
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001649
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001646
Iteration 25/1000 | Loss: 0.00001645
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001637
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00001636
Iteration 32/1000 | Loss: 0.00001635
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00001634
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001632
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001630
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001621
Iteration 85/1000 | Loss: 0.00001621
Iteration 86/1000 | Loss: 0.00001620
Iteration 87/1000 | Loss: 0.00001620
Iteration 88/1000 | Loss: 0.00001620
Iteration 89/1000 | Loss: 0.00001620
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001620
Iteration 95/1000 | Loss: 0.00001619
Iteration 96/1000 | Loss: 0.00001619
Iteration 97/1000 | Loss: 0.00001619
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001618
Iteration 106/1000 | Loss: 0.00001618
Iteration 107/1000 | Loss: 0.00001618
Iteration 108/1000 | Loss: 0.00001618
Iteration 109/1000 | Loss: 0.00001617
Iteration 110/1000 | Loss: 0.00001617
Iteration 111/1000 | Loss: 0.00001617
Iteration 112/1000 | Loss: 0.00001617
Iteration 113/1000 | Loss: 0.00001617
Iteration 114/1000 | Loss: 0.00001617
Iteration 115/1000 | Loss: 0.00001616
Iteration 116/1000 | Loss: 0.00001616
Iteration 117/1000 | Loss: 0.00001616
Iteration 118/1000 | Loss: 0.00001616
Iteration 119/1000 | Loss: 0.00001616
Iteration 120/1000 | Loss: 0.00001616
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001615
Iteration 123/1000 | Loss: 0.00001615
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001615
Iteration 126/1000 | Loss: 0.00001615
Iteration 127/1000 | Loss: 0.00001615
Iteration 128/1000 | Loss: 0.00001615
Iteration 129/1000 | Loss: 0.00001615
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001615
Iteration 136/1000 | Loss: 0.00001615
Iteration 137/1000 | Loss: 0.00001615
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.6148675058502704e-05, 1.6148675058502704e-05, 1.6148675058502704e-05, 1.6148675058502704e-05, 1.6148675058502704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6148675058502704e-05

Optimization complete. Final v2v error: 3.388397216796875 mm

Highest mean error: 4.128089427947998 mm for frame 71

Lowest mean error: 2.965941905975342 mm for frame 101

Saving results

Total time: 36.848509073257446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800989
Iteration 2/25 | Loss: 0.00142113
Iteration 3/25 | Loss: 0.00133872
Iteration 4/25 | Loss: 0.00131635
Iteration 5/25 | Loss: 0.00130898
Iteration 6/25 | Loss: 0.00130715
Iteration 7/25 | Loss: 0.00130687
Iteration 8/25 | Loss: 0.00130687
Iteration 9/25 | Loss: 0.00130687
Iteration 10/25 | Loss: 0.00130687
Iteration 11/25 | Loss: 0.00130687
Iteration 12/25 | Loss: 0.00130687
Iteration 13/25 | Loss: 0.00130687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013068672269582748, 0.0013068672269582748, 0.0013068672269582748, 0.0013068672269582748, 0.0013068672269582748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013068672269582748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38058388
Iteration 2/25 | Loss: 0.00106405
Iteration 3/25 | Loss: 0.00106405
Iteration 4/25 | Loss: 0.00106405
Iteration 5/25 | Loss: 0.00106405
Iteration 6/25 | Loss: 0.00106405
Iteration 7/25 | Loss: 0.00106405
Iteration 8/25 | Loss: 0.00106405
Iteration 9/25 | Loss: 0.00106405
Iteration 10/25 | Loss: 0.00106405
Iteration 11/25 | Loss: 0.00106405
Iteration 12/25 | Loss: 0.00106405
Iteration 13/25 | Loss: 0.00106405
Iteration 14/25 | Loss: 0.00106405
Iteration 15/25 | Loss: 0.00106405
Iteration 16/25 | Loss: 0.00106405
Iteration 17/25 | Loss: 0.00106405
Iteration 18/25 | Loss: 0.00106405
Iteration 19/25 | Loss: 0.00106405
Iteration 20/25 | Loss: 0.00106405
Iteration 21/25 | Loss: 0.00106405
Iteration 22/25 | Loss: 0.00106405
Iteration 23/25 | Loss: 0.00106405
Iteration 24/25 | Loss: 0.00106405
Iteration 25/25 | Loss: 0.00106405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106405
Iteration 2/1000 | Loss: 0.00005739
Iteration 3/1000 | Loss: 0.00003787
Iteration 4/1000 | Loss: 0.00003051
Iteration 5/1000 | Loss: 0.00002874
Iteration 6/1000 | Loss: 0.00002750
Iteration 7/1000 | Loss: 0.00002656
Iteration 8/1000 | Loss: 0.00002580
Iteration 9/1000 | Loss: 0.00002526
Iteration 10/1000 | Loss: 0.00002491
Iteration 11/1000 | Loss: 0.00002460
Iteration 12/1000 | Loss: 0.00002432
Iteration 13/1000 | Loss: 0.00002415
Iteration 14/1000 | Loss: 0.00002393
Iteration 15/1000 | Loss: 0.00002392
Iteration 16/1000 | Loss: 0.00002378
Iteration 17/1000 | Loss: 0.00002377
Iteration 18/1000 | Loss: 0.00002374
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002373
Iteration 21/1000 | Loss: 0.00002373
Iteration 22/1000 | Loss: 0.00002369
Iteration 23/1000 | Loss: 0.00002369
Iteration 24/1000 | Loss: 0.00002367
Iteration 25/1000 | Loss: 0.00002367
Iteration 26/1000 | Loss: 0.00002366
Iteration 27/1000 | Loss: 0.00002366
Iteration 28/1000 | Loss: 0.00002365
Iteration 29/1000 | Loss: 0.00002365
Iteration 30/1000 | Loss: 0.00002363
Iteration 31/1000 | Loss: 0.00002363
Iteration 32/1000 | Loss: 0.00002362
Iteration 33/1000 | Loss: 0.00002362
Iteration 34/1000 | Loss: 0.00002361
Iteration 35/1000 | Loss: 0.00002361
Iteration 36/1000 | Loss: 0.00002361
Iteration 37/1000 | Loss: 0.00002360
Iteration 38/1000 | Loss: 0.00002360
Iteration 39/1000 | Loss: 0.00002360
Iteration 40/1000 | Loss: 0.00002359
Iteration 41/1000 | Loss: 0.00002359
Iteration 42/1000 | Loss: 0.00002359
Iteration 43/1000 | Loss: 0.00002358
Iteration 44/1000 | Loss: 0.00002358
Iteration 45/1000 | Loss: 0.00002358
Iteration 46/1000 | Loss: 0.00002358
Iteration 47/1000 | Loss: 0.00002358
Iteration 48/1000 | Loss: 0.00002357
Iteration 49/1000 | Loss: 0.00002357
Iteration 50/1000 | Loss: 0.00002357
Iteration 51/1000 | Loss: 0.00002356
Iteration 52/1000 | Loss: 0.00002356
Iteration 53/1000 | Loss: 0.00002356
Iteration 54/1000 | Loss: 0.00002356
Iteration 55/1000 | Loss: 0.00002356
Iteration 56/1000 | Loss: 0.00002355
Iteration 57/1000 | Loss: 0.00002355
Iteration 58/1000 | Loss: 0.00002355
Iteration 59/1000 | Loss: 0.00002355
Iteration 60/1000 | Loss: 0.00002355
Iteration 61/1000 | Loss: 0.00002354
Iteration 62/1000 | Loss: 0.00002354
Iteration 63/1000 | Loss: 0.00002354
Iteration 64/1000 | Loss: 0.00002354
Iteration 65/1000 | Loss: 0.00002354
Iteration 66/1000 | Loss: 0.00002354
Iteration 67/1000 | Loss: 0.00002353
Iteration 68/1000 | Loss: 0.00002353
Iteration 69/1000 | Loss: 0.00002353
Iteration 70/1000 | Loss: 0.00002352
Iteration 71/1000 | Loss: 0.00002352
Iteration 72/1000 | Loss: 0.00002351
Iteration 73/1000 | Loss: 0.00002351
Iteration 74/1000 | Loss: 0.00002351
Iteration 75/1000 | Loss: 0.00002351
Iteration 76/1000 | Loss: 0.00002350
Iteration 77/1000 | Loss: 0.00002350
Iteration 78/1000 | Loss: 0.00002350
Iteration 79/1000 | Loss: 0.00002349
Iteration 80/1000 | Loss: 0.00002349
Iteration 81/1000 | Loss: 0.00002349
Iteration 82/1000 | Loss: 0.00002349
Iteration 83/1000 | Loss: 0.00002349
Iteration 84/1000 | Loss: 0.00002348
Iteration 85/1000 | Loss: 0.00002348
Iteration 86/1000 | Loss: 0.00002348
Iteration 87/1000 | Loss: 0.00002347
Iteration 88/1000 | Loss: 0.00002347
Iteration 89/1000 | Loss: 0.00002347
Iteration 90/1000 | Loss: 0.00002347
Iteration 91/1000 | Loss: 0.00002347
Iteration 92/1000 | Loss: 0.00002347
Iteration 93/1000 | Loss: 0.00002347
Iteration 94/1000 | Loss: 0.00002347
Iteration 95/1000 | Loss: 0.00002347
Iteration 96/1000 | Loss: 0.00002347
Iteration 97/1000 | Loss: 0.00002347
Iteration 98/1000 | Loss: 0.00002347
Iteration 99/1000 | Loss: 0.00002347
Iteration 100/1000 | Loss: 0.00002347
Iteration 101/1000 | Loss: 0.00002347
Iteration 102/1000 | Loss: 0.00002347
Iteration 103/1000 | Loss: 0.00002347
Iteration 104/1000 | Loss: 0.00002347
Iteration 105/1000 | Loss: 0.00002347
Iteration 106/1000 | Loss: 0.00002347
Iteration 107/1000 | Loss: 0.00002347
Iteration 108/1000 | Loss: 0.00002347
Iteration 109/1000 | Loss: 0.00002347
Iteration 110/1000 | Loss: 0.00002347
Iteration 111/1000 | Loss: 0.00002347
Iteration 112/1000 | Loss: 0.00002347
Iteration 113/1000 | Loss: 0.00002347
Iteration 114/1000 | Loss: 0.00002347
Iteration 115/1000 | Loss: 0.00002347
Iteration 116/1000 | Loss: 0.00002347
Iteration 117/1000 | Loss: 0.00002347
Iteration 118/1000 | Loss: 0.00002347
Iteration 119/1000 | Loss: 0.00002347
Iteration 120/1000 | Loss: 0.00002347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.3466496713808738e-05, 2.3466496713808738e-05, 2.3466496713808738e-05, 2.3466496713808738e-05, 2.3466496713808738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3466496713808738e-05

Optimization complete. Final v2v error: 4.098024845123291 mm

Highest mean error: 4.828903675079346 mm for frame 60

Lowest mean error: 3.8388445377349854 mm for frame 106

Saving results

Total time: 37.67350459098816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371399
Iteration 2/25 | Loss: 0.00130859
Iteration 3/25 | Loss: 0.00126605
Iteration 4/25 | Loss: 0.00125818
Iteration 5/25 | Loss: 0.00125620
Iteration 6/25 | Loss: 0.00125620
Iteration 7/25 | Loss: 0.00125620
Iteration 8/25 | Loss: 0.00125620
Iteration 9/25 | Loss: 0.00125620
Iteration 10/25 | Loss: 0.00125620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012562008341774344, 0.0012562008341774344, 0.0012562008341774344, 0.0012562008341774344, 0.0012562008341774344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012562008341774344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65175533
Iteration 2/25 | Loss: 0.00088691
Iteration 3/25 | Loss: 0.00088691
Iteration 4/25 | Loss: 0.00088691
Iteration 5/25 | Loss: 0.00088691
Iteration 6/25 | Loss: 0.00088691
Iteration 7/25 | Loss: 0.00088691
Iteration 8/25 | Loss: 0.00088691
Iteration 9/25 | Loss: 0.00088691
Iteration 10/25 | Loss: 0.00088691
Iteration 11/25 | Loss: 0.00088691
Iteration 12/25 | Loss: 0.00088691
Iteration 13/25 | Loss: 0.00088691
Iteration 14/25 | Loss: 0.00088691
Iteration 15/25 | Loss: 0.00088691
Iteration 16/25 | Loss: 0.00088691
Iteration 17/25 | Loss: 0.00088691
Iteration 18/25 | Loss: 0.00088691
Iteration 19/25 | Loss: 0.00088691
Iteration 20/25 | Loss: 0.00088691
Iteration 21/25 | Loss: 0.00088691
Iteration 22/25 | Loss: 0.00088691
Iteration 23/25 | Loss: 0.00088691
Iteration 24/25 | Loss: 0.00088691
Iteration 25/25 | Loss: 0.00088691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088691
Iteration 2/1000 | Loss: 0.00001933
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001335
Iteration 5/1000 | Loss: 0.00001289
Iteration 6/1000 | Loss: 0.00001242
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001184
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001160
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001148
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001144
Iteration 15/1000 | Loss: 0.00001143
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001135
Iteration 20/1000 | Loss: 0.00001134
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001127
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001126
Iteration 26/1000 | Loss: 0.00001126
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001115
Iteration 32/1000 | Loss: 0.00001115
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001111
Iteration 36/1000 | Loss: 0.00001111
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001105
Iteration 48/1000 | Loss: 0.00001105
Iteration 49/1000 | Loss: 0.00001105
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001105
Iteration 52/1000 | Loss: 0.00001104
Iteration 53/1000 | Loss: 0.00001102
Iteration 54/1000 | Loss: 0.00001102
Iteration 55/1000 | Loss: 0.00001102
Iteration 56/1000 | Loss: 0.00001102
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001102
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001100
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001099
Iteration 66/1000 | Loss: 0.00001098
Iteration 67/1000 | Loss: 0.00001098
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001097
Iteration 70/1000 | Loss: 0.00001097
Iteration 71/1000 | Loss: 0.00001097
Iteration 72/1000 | Loss: 0.00001097
Iteration 73/1000 | Loss: 0.00001097
Iteration 74/1000 | Loss: 0.00001096
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001095
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001095
Iteration 79/1000 | Loss: 0.00001095
Iteration 80/1000 | Loss: 0.00001094
Iteration 81/1000 | Loss: 0.00001094
Iteration 82/1000 | Loss: 0.00001094
Iteration 83/1000 | Loss: 0.00001094
Iteration 84/1000 | Loss: 0.00001093
Iteration 85/1000 | Loss: 0.00001093
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001092
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001090
Iteration 101/1000 | Loss: 0.00001090
Iteration 102/1000 | Loss: 0.00001090
Iteration 103/1000 | Loss: 0.00001090
Iteration 104/1000 | Loss: 0.00001090
Iteration 105/1000 | Loss: 0.00001090
Iteration 106/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.0902693247771822e-05, 1.0902693247771822e-05, 1.0902693247771822e-05, 1.0902693247771822e-05, 1.0902693247771822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0902693247771822e-05

Optimization complete. Final v2v error: 2.8477671146392822 mm

Highest mean error: 3.246694326400757 mm for frame 133

Lowest mean error: 2.7668099403381348 mm for frame 212

Saving results

Total time: 36.45434236526489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042140
Iteration 2/25 | Loss: 0.01042140
Iteration 3/25 | Loss: 0.01042140
Iteration 4/25 | Loss: 0.01042140
Iteration 5/25 | Loss: 0.01042139
Iteration 6/25 | Loss: 0.01042139
Iteration 7/25 | Loss: 0.01042139
Iteration 8/25 | Loss: 0.01042139
Iteration 9/25 | Loss: 0.01042139
Iteration 10/25 | Loss: 0.01042139
Iteration 11/25 | Loss: 0.01042138
Iteration 12/25 | Loss: 0.01042138
Iteration 13/25 | Loss: 0.01042138
Iteration 14/25 | Loss: 0.00535153
Iteration 15/25 | Loss: 0.00407512
Iteration 16/25 | Loss: 0.00315966
Iteration 17/25 | Loss: 0.00267190
Iteration 18/25 | Loss: 0.00244171
Iteration 19/25 | Loss: 0.00242007
Iteration 20/25 | Loss: 0.00248320
Iteration 21/25 | Loss: 0.00214603
Iteration 22/25 | Loss: 0.00202047
Iteration 23/25 | Loss: 0.00189908
Iteration 24/25 | Loss: 0.00182920
Iteration 25/25 | Loss: 0.00174603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68839020
Iteration 2/25 | Loss: 0.00367200
Iteration 3/25 | Loss: 0.00367199
Iteration 4/25 | Loss: 0.00367198
Iteration 5/25 | Loss: 0.00367198
Iteration 6/25 | Loss: 0.00367198
Iteration 7/25 | Loss: 0.00309805
Iteration 8/25 | Loss: 0.00258587
Iteration 9/25 | Loss: 0.00258582
Iteration 10/25 | Loss: 0.00258582
Iteration 11/25 | Loss: 0.00258582
Iteration 12/25 | Loss: 0.00258582
Iteration 13/25 | Loss: 0.00258582
Iteration 14/25 | Loss: 0.00258582
Iteration 15/25 | Loss: 0.00258582
Iteration 16/25 | Loss: 0.00258582
Iteration 17/25 | Loss: 0.00258582
Iteration 18/25 | Loss: 0.00258582
Iteration 19/25 | Loss: 0.00258582
Iteration 20/25 | Loss: 0.00258582
Iteration 21/25 | Loss: 0.00258582
Iteration 22/25 | Loss: 0.00258582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002585820620879531, 0.002585820620879531, 0.002585820620879531, 0.002585820620879531, 0.002585820620879531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002585820620879531

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258582
Iteration 2/1000 | Loss: 0.00126155
Iteration 3/1000 | Loss: 0.00158693
Iteration 4/1000 | Loss: 0.00157090
Iteration 5/1000 | Loss: 0.00103866
Iteration 6/1000 | Loss: 0.00068794
Iteration 7/1000 | Loss: 0.00030397
Iteration 8/1000 | Loss: 0.00039752
Iteration 9/1000 | Loss: 0.00025572
Iteration 10/1000 | Loss: 0.00031714
Iteration 11/1000 | Loss: 0.00016552
Iteration 12/1000 | Loss: 0.00017238
Iteration 13/1000 | Loss: 0.00040091
Iteration 14/1000 | Loss: 0.00008731
Iteration 15/1000 | Loss: 0.00006794
Iteration 16/1000 | Loss: 0.00006248
Iteration 17/1000 | Loss: 0.00005736
Iteration 18/1000 | Loss: 0.00005439
Iteration 19/1000 | Loss: 0.00012517
Iteration 20/1000 | Loss: 0.00033917
Iteration 21/1000 | Loss: 0.00035580
Iteration 22/1000 | Loss: 0.00010465
Iteration 23/1000 | Loss: 0.00025609
Iteration 24/1000 | Loss: 0.00019264
Iteration 25/1000 | Loss: 0.00013705
Iteration 26/1000 | Loss: 0.00004997
Iteration 27/1000 | Loss: 0.00023705
Iteration 28/1000 | Loss: 0.00005175
Iteration 29/1000 | Loss: 0.00020039
Iteration 30/1000 | Loss: 0.00025363
Iteration 31/1000 | Loss: 0.00022084
Iteration 32/1000 | Loss: 0.00009823
Iteration 33/1000 | Loss: 0.00023503
Iteration 34/1000 | Loss: 0.00005474
Iteration 35/1000 | Loss: 0.00004774
Iteration 36/1000 | Loss: 0.00004460
Iteration 37/1000 | Loss: 0.00021681
Iteration 38/1000 | Loss: 0.00010036
Iteration 39/1000 | Loss: 0.00041718
Iteration 40/1000 | Loss: 0.00021006
Iteration 41/1000 | Loss: 0.00006806
Iteration 42/1000 | Loss: 0.00005179
Iteration 43/1000 | Loss: 0.00004578
Iteration 44/1000 | Loss: 0.00004124
Iteration 45/1000 | Loss: 0.00003925
Iteration 46/1000 | Loss: 0.00003729
Iteration 47/1000 | Loss: 0.00014614
Iteration 48/1000 | Loss: 0.00003994
Iteration 49/1000 | Loss: 0.00003517
Iteration 50/1000 | Loss: 0.00003330
Iteration 51/1000 | Loss: 0.00003074
Iteration 52/1000 | Loss: 0.00002960
Iteration 53/1000 | Loss: 0.00002856
Iteration 54/1000 | Loss: 0.00002799
Iteration 55/1000 | Loss: 0.00002774
Iteration 56/1000 | Loss: 0.00002753
Iteration 57/1000 | Loss: 0.00002750
Iteration 58/1000 | Loss: 0.00002749
Iteration 59/1000 | Loss: 0.00002749
Iteration 60/1000 | Loss: 0.00002730
Iteration 61/1000 | Loss: 0.00002722
Iteration 62/1000 | Loss: 0.00002716
Iteration 63/1000 | Loss: 0.00002715
Iteration 64/1000 | Loss: 0.00002715
Iteration 65/1000 | Loss: 0.00002713
Iteration 66/1000 | Loss: 0.00002711
Iteration 67/1000 | Loss: 0.00002711
Iteration 68/1000 | Loss: 0.00002711
Iteration 69/1000 | Loss: 0.00002711
Iteration 70/1000 | Loss: 0.00002711
Iteration 71/1000 | Loss: 0.00002711
Iteration 72/1000 | Loss: 0.00002711
Iteration 73/1000 | Loss: 0.00002711
Iteration 74/1000 | Loss: 0.00002710
Iteration 75/1000 | Loss: 0.00002710
Iteration 76/1000 | Loss: 0.00002710
Iteration 77/1000 | Loss: 0.00002710
Iteration 78/1000 | Loss: 0.00002710
Iteration 79/1000 | Loss: 0.00002710
Iteration 80/1000 | Loss: 0.00002710
Iteration 81/1000 | Loss: 0.00002708
Iteration 82/1000 | Loss: 0.00002707
Iteration 83/1000 | Loss: 0.00002707
Iteration 84/1000 | Loss: 0.00002707
Iteration 85/1000 | Loss: 0.00002707
Iteration 86/1000 | Loss: 0.00002706
Iteration 87/1000 | Loss: 0.00002706
Iteration 88/1000 | Loss: 0.00002706
Iteration 89/1000 | Loss: 0.00002706
Iteration 90/1000 | Loss: 0.00002706
Iteration 91/1000 | Loss: 0.00002706
Iteration 92/1000 | Loss: 0.00002706
Iteration 93/1000 | Loss: 0.00002706
Iteration 94/1000 | Loss: 0.00002706
Iteration 95/1000 | Loss: 0.00002706
Iteration 96/1000 | Loss: 0.00002706
Iteration 97/1000 | Loss: 0.00002705
Iteration 98/1000 | Loss: 0.00002705
Iteration 99/1000 | Loss: 0.00002704
Iteration 100/1000 | Loss: 0.00002704
Iteration 101/1000 | Loss: 0.00002704
Iteration 102/1000 | Loss: 0.00002704
Iteration 103/1000 | Loss: 0.00002704
Iteration 104/1000 | Loss: 0.00002704
Iteration 105/1000 | Loss: 0.00002704
Iteration 106/1000 | Loss: 0.00002704
Iteration 107/1000 | Loss: 0.00002704
Iteration 108/1000 | Loss: 0.00002704
Iteration 109/1000 | Loss: 0.00002704
Iteration 110/1000 | Loss: 0.00002704
Iteration 111/1000 | Loss: 0.00002703
Iteration 112/1000 | Loss: 0.00002703
Iteration 113/1000 | Loss: 0.00002703
Iteration 114/1000 | Loss: 0.00002703
Iteration 115/1000 | Loss: 0.00002703
Iteration 116/1000 | Loss: 0.00002702
Iteration 117/1000 | Loss: 0.00002701
Iteration 118/1000 | Loss: 0.00002701
Iteration 119/1000 | Loss: 0.00002700
Iteration 120/1000 | Loss: 0.00002700
Iteration 121/1000 | Loss: 0.00002700
Iteration 122/1000 | Loss: 0.00002700
Iteration 123/1000 | Loss: 0.00002700
Iteration 124/1000 | Loss: 0.00002700
Iteration 125/1000 | Loss: 0.00002700
Iteration 126/1000 | Loss: 0.00002700
Iteration 127/1000 | Loss: 0.00002699
Iteration 128/1000 | Loss: 0.00002699
Iteration 129/1000 | Loss: 0.00002699
Iteration 130/1000 | Loss: 0.00002699
Iteration 131/1000 | Loss: 0.00002698
Iteration 132/1000 | Loss: 0.00002698
Iteration 133/1000 | Loss: 0.00002698
Iteration 134/1000 | Loss: 0.00002698
Iteration 135/1000 | Loss: 0.00002698
Iteration 136/1000 | Loss: 0.00002698
Iteration 137/1000 | Loss: 0.00002698
Iteration 138/1000 | Loss: 0.00002698
Iteration 139/1000 | Loss: 0.00002698
Iteration 140/1000 | Loss: 0.00002698
Iteration 141/1000 | Loss: 0.00002698
Iteration 142/1000 | Loss: 0.00002698
Iteration 143/1000 | Loss: 0.00002698
Iteration 144/1000 | Loss: 0.00002698
Iteration 145/1000 | Loss: 0.00002698
Iteration 146/1000 | Loss: 0.00002698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.698127536859829e-05, 2.698127536859829e-05, 2.698127536859829e-05, 2.698127536859829e-05, 2.698127536859829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.698127536859829e-05

Optimization complete. Final v2v error: 4.113703727722168 mm

Highest mean error: 10.44098949432373 mm for frame 117

Lowest mean error: 3.8208577632904053 mm for frame 32

Saving results

Total time: 129.2637791633606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390573
Iteration 2/25 | Loss: 0.00141386
Iteration 3/25 | Loss: 0.00130930
Iteration 4/25 | Loss: 0.00128980
Iteration 5/25 | Loss: 0.00128355
Iteration 6/25 | Loss: 0.00128228
Iteration 7/25 | Loss: 0.00128219
Iteration 8/25 | Loss: 0.00128219
Iteration 9/25 | Loss: 0.00128219
Iteration 10/25 | Loss: 0.00128219
Iteration 11/25 | Loss: 0.00128219
Iteration 12/25 | Loss: 0.00128219
Iteration 13/25 | Loss: 0.00128219
Iteration 14/25 | Loss: 0.00128219
Iteration 15/25 | Loss: 0.00128219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012821877608075738, 0.0012821877608075738, 0.0012821877608075738, 0.0012821877608075738, 0.0012821877608075738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012821877608075738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36319792
Iteration 2/25 | Loss: 0.00082325
Iteration 3/25 | Loss: 0.00082325
Iteration 4/25 | Loss: 0.00082325
Iteration 5/25 | Loss: 0.00082325
Iteration 6/25 | Loss: 0.00082325
Iteration 7/25 | Loss: 0.00082325
Iteration 8/25 | Loss: 0.00082325
Iteration 9/25 | Loss: 0.00082325
Iteration 10/25 | Loss: 0.00082325
Iteration 11/25 | Loss: 0.00082325
Iteration 12/25 | Loss: 0.00082325
Iteration 13/25 | Loss: 0.00082325
Iteration 14/25 | Loss: 0.00082325
Iteration 15/25 | Loss: 0.00082325
Iteration 16/25 | Loss: 0.00082325
Iteration 17/25 | Loss: 0.00082325
Iteration 18/25 | Loss: 0.00082325
Iteration 19/25 | Loss: 0.00082325
Iteration 20/25 | Loss: 0.00082325
Iteration 21/25 | Loss: 0.00082325
Iteration 22/25 | Loss: 0.00082325
Iteration 23/25 | Loss: 0.00082325
Iteration 24/25 | Loss: 0.00082325
Iteration 25/25 | Loss: 0.00082325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082325
Iteration 2/1000 | Loss: 0.00005603
Iteration 3/1000 | Loss: 0.00003770
Iteration 4/1000 | Loss: 0.00003006
Iteration 5/1000 | Loss: 0.00002667
Iteration 6/1000 | Loss: 0.00002491
Iteration 7/1000 | Loss: 0.00002374
Iteration 8/1000 | Loss: 0.00002250
Iteration 9/1000 | Loss: 0.00002176
Iteration 10/1000 | Loss: 0.00002126
Iteration 11/1000 | Loss: 0.00002089
Iteration 12/1000 | Loss: 0.00002060
Iteration 13/1000 | Loss: 0.00002050
Iteration 14/1000 | Loss: 0.00002028
Iteration 15/1000 | Loss: 0.00002010
Iteration 16/1000 | Loss: 0.00002004
Iteration 17/1000 | Loss: 0.00002003
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001990
Iteration 21/1000 | Loss: 0.00001986
Iteration 22/1000 | Loss: 0.00001984
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001980
Iteration 25/1000 | Loss: 0.00001978
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001977
Iteration 28/1000 | Loss: 0.00001976
Iteration 29/1000 | Loss: 0.00001975
Iteration 30/1000 | Loss: 0.00001975
Iteration 31/1000 | Loss: 0.00001974
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00001969
Iteration 34/1000 | Loss: 0.00001969
Iteration 35/1000 | Loss: 0.00001966
Iteration 36/1000 | Loss: 0.00001966
Iteration 37/1000 | Loss: 0.00001962
Iteration 38/1000 | Loss: 0.00001961
Iteration 39/1000 | Loss: 0.00001961
Iteration 40/1000 | Loss: 0.00001961
Iteration 41/1000 | Loss: 0.00001959
Iteration 42/1000 | Loss: 0.00001959
Iteration 43/1000 | Loss: 0.00001959
Iteration 44/1000 | Loss: 0.00001958
Iteration 45/1000 | Loss: 0.00001958
Iteration 46/1000 | Loss: 0.00001958
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00001957
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001956
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001956
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001955
Iteration 58/1000 | Loss: 0.00001955
Iteration 59/1000 | Loss: 0.00001955
Iteration 60/1000 | Loss: 0.00001955
Iteration 61/1000 | Loss: 0.00001954
Iteration 62/1000 | Loss: 0.00001954
Iteration 63/1000 | Loss: 0.00001954
Iteration 64/1000 | Loss: 0.00001954
Iteration 65/1000 | Loss: 0.00001954
Iteration 66/1000 | Loss: 0.00001954
Iteration 67/1000 | Loss: 0.00001954
Iteration 68/1000 | Loss: 0.00001954
Iteration 69/1000 | Loss: 0.00001954
Iteration 70/1000 | Loss: 0.00001954
Iteration 71/1000 | Loss: 0.00001954
Iteration 72/1000 | Loss: 0.00001954
Iteration 73/1000 | Loss: 0.00001954
Iteration 74/1000 | Loss: 0.00001953
Iteration 75/1000 | Loss: 0.00001953
Iteration 76/1000 | Loss: 0.00001953
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001952
Iteration 79/1000 | Loss: 0.00001952
Iteration 80/1000 | Loss: 0.00001950
Iteration 81/1000 | Loss: 0.00001950
Iteration 82/1000 | Loss: 0.00001950
Iteration 83/1000 | Loss: 0.00001950
Iteration 84/1000 | Loss: 0.00001950
Iteration 85/1000 | Loss: 0.00001950
Iteration 86/1000 | Loss: 0.00001950
Iteration 87/1000 | Loss: 0.00001950
Iteration 88/1000 | Loss: 0.00001949
Iteration 89/1000 | Loss: 0.00001948
Iteration 90/1000 | Loss: 0.00001948
Iteration 91/1000 | Loss: 0.00001948
Iteration 92/1000 | Loss: 0.00001947
Iteration 93/1000 | Loss: 0.00001947
Iteration 94/1000 | Loss: 0.00001947
Iteration 95/1000 | Loss: 0.00001947
Iteration 96/1000 | Loss: 0.00001946
Iteration 97/1000 | Loss: 0.00001946
Iteration 98/1000 | Loss: 0.00001946
Iteration 99/1000 | Loss: 0.00001946
Iteration 100/1000 | Loss: 0.00001946
Iteration 101/1000 | Loss: 0.00001946
Iteration 102/1000 | Loss: 0.00001945
Iteration 103/1000 | Loss: 0.00001945
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001945
Iteration 106/1000 | Loss: 0.00001944
Iteration 107/1000 | Loss: 0.00001944
Iteration 108/1000 | Loss: 0.00001944
Iteration 109/1000 | Loss: 0.00001944
Iteration 110/1000 | Loss: 0.00001943
Iteration 111/1000 | Loss: 0.00001943
Iteration 112/1000 | Loss: 0.00001943
Iteration 113/1000 | Loss: 0.00001943
Iteration 114/1000 | Loss: 0.00001943
Iteration 115/1000 | Loss: 0.00001943
Iteration 116/1000 | Loss: 0.00001943
Iteration 117/1000 | Loss: 0.00001943
Iteration 118/1000 | Loss: 0.00001942
Iteration 119/1000 | Loss: 0.00001942
Iteration 120/1000 | Loss: 0.00001942
Iteration 121/1000 | Loss: 0.00001942
Iteration 122/1000 | Loss: 0.00001942
Iteration 123/1000 | Loss: 0.00001942
Iteration 124/1000 | Loss: 0.00001942
Iteration 125/1000 | Loss: 0.00001941
Iteration 126/1000 | Loss: 0.00001941
Iteration 127/1000 | Loss: 0.00001941
Iteration 128/1000 | Loss: 0.00001941
Iteration 129/1000 | Loss: 0.00001940
Iteration 130/1000 | Loss: 0.00001940
Iteration 131/1000 | Loss: 0.00001940
Iteration 132/1000 | Loss: 0.00001940
Iteration 133/1000 | Loss: 0.00001940
Iteration 134/1000 | Loss: 0.00001939
Iteration 135/1000 | Loss: 0.00001939
Iteration 136/1000 | Loss: 0.00001939
Iteration 137/1000 | Loss: 0.00001939
Iteration 138/1000 | Loss: 0.00001939
Iteration 139/1000 | Loss: 0.00001939
Iteration 140/1000 | Loss: 0.00001939
Iteration 141/1000 | Loss: 0.00001938
Iteration 142/1000 | Loss: 0.00001938
Iteration 143/1000 | Loss: 0.00001938
Iteration 144/1000 | Loss: 0.00001938
Iteration 145/1000 | Loss: 0.00001938
Iteration 146/1000 | Loss: 0.00001938
Iteration 147/1000 | Loss: 0.00001938
Iteration 148/1000 | Loss: 0.00001938
Iteration 149/1000 | Loss: 0.00001938
Iteration 150/1000 | Loss: 0.00001938
Iteration 151/1000 | Loss: 0.00001938
Iteration 152/1000 | Loss: 0.00001938
Iteration 153/1000 | Loss: 0.00001938
Iteration 154/1000 | Loss: 0.00001938
Iteration 155/1000 | Loss: 0.00001938
Iteration 156/1000 | Loss: 0.00001938
Iteration 157/1000 | Loss: 0.00001938
Iteration 158/1000 | Loss: 0.00001938
Iteration 159/1000 | Loss: 0.00001938
Iteration 160/1000 | Loss: 0.00001938
Iteration 161/1000 | Loss: 0.00001938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.938031527970452e-05, 1.938031527970452e-05, 1.938031527970452e-05, 1.938031527970452e-05, 1.938031527970452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.938031527970452e-05

Optimization complete. Final v2v error: 3.725501775741577 mm

Highest mean error: 4.433548450469971 mm for frame 69

Lowest mean error: 3.2494282722473145 mm for frame 83

Saving results

Total time: 41.02808904647827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512391
Iteration 2/25 | Loss: 0.00157967
Iteration 3/25 | Loss: 0.00139851
Iteration 4/25 | Loss: 0.00138081
Iteration 5/25 | Loss: 0.00137476
Iteration 6/25 | Loss: 0.00137266
Iteration 7/25 | Loss: 0.00137263
Iteration 8/25 | Loss: 0.00137263
Iteration 9/25 | Loss: 0.00137263
Iteration 10/25 | Loss: 0.00137263
Iteration 11/25 | Loss: 0.00137263
Iteration 12/25 | Loss: 0.00137263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013726325705647469, 0.0013726325705647469, 0.0013726325705647469, 0.0013726325705647469, 0.0013726325705647469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013726325705647469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10738337
Iteration 2/25 | Loss: 0.00106620
Iteration 3/25 | Loss: 0.00106619
Iteration 4/25 | Loss: 0.00106619
Iteration 5/25 | Loss: 0.00106619
Iteration 6/25 | Loss: 0.00106619
Iteration 7/25 | Loss: 0.00106618
Iteration 8/25 | Loss: 0.00106618
Iteration 9/25 | Loss: 0.00106618
Iteration 10/25 | Loss: 0.00106618
Iteration 11/25 | Loss: 0.00106618
Iteration 12/25 | Loss: 0.00106618
Iteration 13/25 | Loss: 0.00106618
Iteration 14/25 | Loss: 0.00106618
Iteration 15/25 | Loss: 0.00106618
Iteration 16/25 | Loss: 0.00106618
Iteration 17/25 | Loss: 0.00106618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001066183322109282, 0.001066183322109282, 0.001066183322109282, 0.001066183322109282, 0.001066183322109282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001066183322109282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106618
Iteration 2/1000 | Loss: 0.00007477
Iteration 3/1000 | Loss: 0.00004316
Iteration 4/1000 | Loss: 0.00003395
Iteration 5/1000 | Loss: 0.00003176
Iteration 6/1000 | Loss: 0.00003011
Iteration 7/1000 | Loss: 0.00002909
Iteration 8/1000 | Loss: 0.00002828
Iteration 9/1000 | Loss: 0.00002751
Iteration 10/1000 | Loss: 0.00002705
Iteration 11/1000 | Loss: 0.00002661
Iteration 12/1000 | Loss: 0.00002628
Iteration 13/1000 | Loss: 0.00002606
Iteration 14/1000 | Loss: 0.00002584
Iteration 15/1000 | Loss: 0.00002561
Iteration 16/1000 | Loss: 0.00002545
Iteration 17/1000 | Loss: 0.00002535
Iteration 18/1000 | Loss: 0.00002534
Iteration 19/1000 | Loss: 0.00002528
Iteration 20/1000 | Loss: 0.00002518
Iteration 21/1000 | Loss: 0.00002516
Iteration 22/1000 | Loss: 0.00002515
Iteration 23/1000 | Loss: 0.00002515
Iteration 24/1000 | Loss: 0.00002513
Iteration 25/1000 | Loss: 0.00002512
Iteration 26/1000 | Loss: 0.00002512
Iteration 27/1000 | Loss: 0.00002510
Iteration 28/1000 | Loss: 0.00002507
Iteration 29/1000 | Loss: 0.00002506
Iteration 30/1000 | Loss: 0.00002506
Iteration 31/1000 | Loss: 0.00002506
Iteration 32/1000 | Loss: 0.00002505
Iteration 33/1000 | Loss: 0.00002503
Iteration 34/1000 | Loss: 0.00002503
Iteration 35/1000 | Loss: 0.00002503
Iteration 36/1000 | Loss: 0.00002502
Iteration 37/1000 | Loss: 0.00002502
Iteration 38/1000 | Loss: 0.00002502
Iteration 39/1000 | Loss: 0.00002501
Iteration 40/1000 | Loss: 0.00002501
Iteration 41/1000 | Loss: 0.00002500
Iteration 42/1000 | Loss: 0.00002500
Iteration 43/1000 | Loss: 0.00002499
Iteration 44/1000 | Loss: 0.00002499
Iteration 45/1000 | Loss: 0.00002499
Iteration 46/1000 | Loss: 0.00002498
Iteration 47/1000 | Loss: 0.00002498
Iteration 48/1000 | Loss: 0.00002498
Iteration 49/1000 | Loss: 0.00002497
Iteration 50/1000 | Loss: 0.00002497
Iteration 51/1000 | Loss: 0.00002497
Iteration 52/1000 | Loss: 0.00002496
Iteration 53/1000 | Loss: 0.00002496
Iteration 54/1000 | Loss: 0.00002496
Iteration 55/1000 | Loss: 0.00002495
Iteration 56/1000 | Loss: 0.00002495
Iteration 57/1000 | Loss: 0.00002495
Iteration 58/1000 | Loss: 0.00002494
Iteration 59/1000 | Loss: 0.00002494
Iteration 60/1000 | Loss: 0.00002494
Iteration 61/1000 | Loss: 0.00002493
Iteration 62/1000 | Loss: 0.00002493
Iteration 63/1000 | Loss: 0.00002493
Iteration 64/1000 | Loss: 0.00002492
Iteration 65/1000 | Loss: 0.00002492
Iteration 66/1000 | Loss: 0.00002492
Iteration 67/1000 | Loss: 0.00002491
Iteration 68/1000 | Loss: 0.00002491
Iteration 69/1000 | Loss: 0.00002491
Iteration 70/1000 | Loss: 0.00002490
Iteration 71/1000 | Loss: 0.00002490
Iteration 72/1000 | Loss: 0.00002490
Iteration 73/1000 | Loss: 0.00002490
Iteration 74/1000 | Loss: 0.00002490
Iteration 75/1000 | Loss: 0.00002490
Iteration 76/1000 | Loss: 0.00002490
Iteration 77/1000 | Loss: 0.00002490
Iteration 78/1000 | Loss: 0.00002489
Iteration 79/1000 | Loss: 0.00002489
Iteration 80/1000 | Loss: 0.00002489
Iteration 81/1000 | Loss: 0.00002489
Iteration 82/1000 | Loss: 0.00002489
Iteration 83/1000 | Loss: 0.00002489
Iteration 84/1000 | Loss: 0.00002488
Iteration 85/1000 | Loss: 0.00002488
Iteration 86/1000 | Loss: 0.00002488
Iteration 87/1000 | Loss: 0.00002488
Iteration 88/1000 | Loss: 0.00002488
Iteration 89/1000 | Loss: 0.00002487
Iteration 90/1000 | Loss: 0.00002487
Iteration 91/1000 | Loss: 0.00002487
Iteration 92/1000 | Loss: 0.00002487
Iteration 93/1000 | Loss: 0.00002487
Iteration 94/1000 | Loss: 0.00002486
Iteration 95/1000 | Loss: 0.00002486
Iteration 96/1000 | Loss: 0.00002486
Iteration 97/1000 | Loss: 0.00002486
Iteration 98/1000 | Loss: 0.00002485
Iteration 99/1000 | Loss: 0.00002485
Iteration 100/1000 | Loss: 0.00002485
Iteration 101/1000 | Loss: 0.00002485
Iteration 102/1000 | Loss: 0.00002485
Iteration 103/1000 | Loss: 0.00002485
Iteration 104/1000 | Loss: 0.00002485
Iteration 105/1000 | Loss: 0.00002485
Iteration 106/1000 | Loss: 0.00002484
Iteration 107/1000 | Loss: 0.00002484
Iteration 108/1000 | Loss: 0.00002484
Iteration 109/1000 | Loss: 0.00002484
Iteration 110/1000 | Loss: 0.00002484
Iteration 111/1000 | Loss: 0.00002484
Iteration 112/1000 | Loss: 0.00002484
Iteration 113/1000 | Loss: 0.00002484
Iteration 114/1000 | Loss: 0.00002484
Iteration 115/1000 | Loss: 0.00002483
Iteration 116/1000 | Loss: 0.00002483
Iteration 117/1000 | Loss: 0.00002483
Iteration 118/1000 | Loss: 0.00002483
Iteration 119/1000 | Loss: 0.00002483
Iteration 120/1000 | Loss: 0.00002483
Iteration 121/1000 | Loss: 0.00002483
Iteration 122/1000 | Loss: 0.00002483
Iteration 123/1000 | Loss: 0.00002483
Iteration 124/1000 | Loss: 0.00002483
Iteration 125/1000 | Loss: 0.00002483
Iteration 126/1000 | Loss: 0.00002483
Iteration 127/1000 | Loss: 0.00002483
Iteration 128/1000 | Loss: 0.00002483
Iteration 129/1000 | Loss: 0.00002483
Iteration 130/1000 | Loss: 0.00002483
Iteration 131/1000 | Loss: 0.00002483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.483106254658196e-05, 2.483106254658196e-05, 2.483106254658196e-05, 2.483106254658196e-05, 2.483106254658196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.483106254658196e-05

Optimization complete. Final v2v error: 4.111341953277588 mm

Highest mean error: 5.2261152267456055 mm for frame 103

Lowest mean error: 3.0141468048095703 mm for frame 72

Saving results

Total time: 49.448527097702026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781392
Iteration 2/25 | Loss: 0.00150392
Iteration 3/25 | Loss: 0.00136323
Iteration 4/25 | Loss: 0.00134475
Iteration 5/25 | Loss: 0.00133345
Iteration 6/25 | Loss: 0.00133274
Iteration 7/25 | Loss: 0.00133044
Iteration 8/25 | Loss: 0.00132760
Iteration 9/25 | Loss: 0.00132617
Iteration 10/25 | Loss: 0.00132578
Iteration 11/25 | Loss: 0.00132566
Iteration 12/25 | Loss: 0.00132565
Iteration 13/25 | Loss: 0.00132565
Iteration 14/25 | Loss: 0.00132565
Iteration 15/25 | Loss: 0.00132565
Iteration 16/25 | Loss: 0.00132565
Iteration 17/25 | Loss: 0.00132565
Iteration 18/25 | Loss: 0.00132565
Iteration 19/25 | Loss: 0.00132565
Iteration 20/25 | Loss: 0.00132565
Iteration 21/25 | Loss: 0.00132565
Iteration 22/25 | Loss: 0.00132565
Iteration 23/25 | Loss: 0.00132565
Iteration 24/25 | Loss: 0.00132565
Iteration 25/25 | Loss: 0.00132565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63592410
Iteration 2/25 | Loss: 0.00088168
Iteration 3/25 | Loss: 0.00088168
Iteration 4/25 | Loss: 0.00088168
Iteration 5/25 | Loss: 0.00088167
Iteration 6/25 | Loss: 0.00088167
Iteration 7/25 | Loss: 0.00088167
Iteration 8/25 | Loss: 0.00088167
Iteration 9/25 | Loss: 0.00088167
Iteration 10/25 | Loss: 0.00088167
Iteration 11/25 | Loss: 0.00088167
Iteration 12/25 | Loss: 0.00088167
Iteration 13/25 | Loss: 0.00088167
Iteration 14/25 | Loss: 0.00088167
Iteration 15/25 | Loss: 0.00088167
Iteration 16/25 | Loss: 0.00088167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008816725458018482, 0.0008816725458018482, 0.0008816725458018482, 0.0008816725458018482, 0.0008816725458018482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008816725458018482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088167
Iteration 2/1000 | Loss: 0.00003025
Iteration 3/1000 | Loss: 0.00002216
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001970
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001877
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001824
Iteration 10/1000 | Loss: 0.00001802
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001788
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00001784
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001778
Iteration 17/1000 | Loss: 0.00001772
Iteration 18/1000 | Loss: 0.00001770
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001769
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001757
Iteration 23/1000 | Loss: 0.00001756
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001755
Iteration 27/1000 | Loss: 0.00001754
Iteration 28/1000 | Loss: 0.00001754
Iteration 29/1000 | Loss: 0.00001754
Iteration 30/1000 | Loss: 0.00001753
Iteration 31/1000 | Loss: 0.00001753
Iteration 32/1000 | Loss: 0.00001752
Iteration 33/1000 | Loss: 0.00001752
Iteration 34/1000 | Loss: 0.00001752
Iteration 35/1000 | Loss: 0.00001752
Iteration 36/1000 | Loss: 0.00001751
Iteration 37/1000 | Loss: 0.00001751
Iteration 38/1000 | Loss: 0.00001751
Iteration 39/1000 | Loss: 0.00001751
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001749
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001749
Iteration 49/1000 | Loss: 0.00001749
Iteration 50/1000 | Loss: 0.00001749
Iteration 51/1000 | Loss: 0.00001749
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001748
Iteration 57/1000 | Loss: 0.00001748
Iteration 58/1000 | Loss: 0.00001748
Iteration 59/1000 | Loss: 0.00001748
Iteration 60/1000 | Loss: 0.00001748
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001747
Iteration 64/1000 | Loss: 0.00001747
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001747
Iteration 68/1000 | Loss: 0.00001747
Iteration 69/1000 | Loss: 0.00001747
Iteration 70/1000 | Loss: 0.00001747
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001746
Iteration 74/1000 | Loss: 0.00001746
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001746
Iteration 79/1000 | Loss: 0.00001746
Iteration 80/1000 | Loss: 0.00001745
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001745
Iteration 83/1000 | Loss: 0.00001745
Iteration 84/1000 | Loss: 0.00001745
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001744
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001742
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001741
Iteration 99/1000 | Loss: 0.00001741
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001741
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001740
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001739
Iteration 118/1000 | Loss: 0.00001739
Iteration 119/1000 | Loss: 0.00001739
Iteration 120/1000 | Loss: 0.00001739
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001738
Iteration 124/1000 | Loss: 0.00001738
Iteration 125/1000 | Loss: 0.00001738
Iteration 126/1000 | Loss: 0.00001738
Iteration 127/1000 | Loss: 0.00001738
Iteration 128/1000 | Loss: 0.00001738
Iteration 129/1000 | Loss: 0.00001738
Iteration 130/1000 | Loss: 0.00001738
Iteration 131/1000 | Loss: 0.00001737
Iteration 132/1000 | Loss: 0.00001737
Iteration 133/1000 | Loss: 0.00001737
Iteration 134/1000 | Loss: 0.00001737
Iteration 135/1000 | Loss: 0.00001737
Iteration 136/1000 | Loss: 0.00001737
Iteration 137/1000 | Loss: 0.00001737
Iteration 138/1000 | Loss: 0.00001737
Iteration 139/1000 | Loss: 0.00001737
Iteration 140/1000 | Loss: 0.00001737
Iteration 141/1000 | Loss: 0.00001737
Iteration 142/1000 | Loss: 0.00001737
Iteration 143/1000 | Loss: 0.00001737
Iteration 144/1000 | Loss: 0.00001737
Iteration 145/1000 | Loss: 0.00001737
Iteration 146/1000 | Loss: 0.00001737
Iteration 147/1000 | Loss: 0.00001737
Iteration 148/1000 | Loss: 0.00001737
Iteration 149/1000 | Loss: 0.00001737
Iteration 150/1000 | Loss: 0.00001737
Iteration 151/1000 | Loss: 0.00001737
Iteration 152/1000 | Loss: 0.00001737
Iteration 153/1000 | Loss: 0.00001737
Iteration 154/1000 | Loss: 0.00001737
Iteration 155/1000 | Loss: 0.00001737
Iteration 156/1000 | Loss: 0.00001737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.7374351955368184e-05, 1.7374351955368184e-05, 1.7374351955368184e-05, 1.7374351955368184e-05, 1.7374351955368184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7374351955368184e-05

Optimization complete. Final v2v error: 3.5154600143432617 mm

Highest mean error: 4.1525044441223145 mm for frame 172

Lowest mean error: 3.093830108642578 mm for frame 52

Saving results

Total time: 50.84903621673584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794050
Iteration 2/25 | Loss: 0.00165938
Iteration 3/25 | Loss: 0.00147257
Iteration 4/25 | Loss: 0.00142306
Iteration 5/25 | Loss: 0.00140027
Iteration 6/25 | Loss: 0.00139907
Iteration 7/25 | Loss: 0.00139072
Iteration 8/25 | Loss: 0.00138311
Iteration 9/25 | Loss: 0.00138153
Iteration 10/25 | Loss: 0.00138026
Iteration 11/25 | Loss: 0.00137984
Iteration 12/25 | Loss: 0.00137962
Iteration 13/25 | Loss: 0.00137940
Iteration 14/25 | Loss: 0.00137923
Iteration 15/25 | Loss: 0.00137913
Iteration 16/25 | Loss: 0.00137905
Iteration 17/25 | Loss: 0.00137892
Iteration 18/25 | Loss: 0.00137882
Iteration 19/25 | Loss: 0.00137880
Iteration 20/25 | Loss: 0.00137879
Iteration 21/25 | Loss: 0.00137879
Iteration 22/25 | Loss: 0.00137879
Iteration 23/25 | Loss: 0.00137879
Iteration 24/25 | Loss: 0.00137879
Iteration 25/25 | Loss: 0.00137879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82591200
Iteration 2/25 | Loss: 0.00123119
Iteration 3/25 | Loss: 0.00122602
Iteration 4/25 | Loss: 0.00122602
Iteration 5/25 | Loss: 0.00122602
Iteration 6/25 | Loss: 0.00122602
Iteration 7/25 | Loss: 0.00122602
Iteration 8/25 | Loss: 0.00122602
Iteration 9/25 | Loss: 0.00122602
Iteration 10/25 | Loss: 0.00122602
Iteration 11/25 | Loss: 0.00122602
Iteration 12/25 | Loss: 0.00122602
Iteration 13/25 | Loss: 0.00122602
Iteration 14/25 | Loss: 0.00122602
Iteration 15/25 | Loss: 0.00122602
Iteration 16/25 | Loss: 0.00122602
Iteration 17/25 | Loss: 0.00122602
Iteration 18/25 | Loss: 0.00122602
Iteration 19/25 | Loss: 0.00122602
Iteration 20/25 | Loss: 0.00122602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012260184157639742, 0.0012260184157639742, 0.0012260184157639742, 0.0012260184157639742, 0.0012260184157639742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012260184157639742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122602
Iteration 2/1000 | Loss: 0.00007468
Iteration 3/1000 | Loss: 0.00008854
Iteration 4/1000 | Loss: 0.00003717
Iteration 5/1000 | Loss: 0.00003329
Iteration 6/1000 | Loss: 0.00006410
Iteration 7/1000 | Loss: 0.00003032
Iteration 8/1000 | Loss: 0.00002986
Iteration 9/1000 | Loss: 0.00002921
Iteration 10/1000 | Loss: 0.00002886
Iteration 11/1000 | Loss: 0.00002852
Iteration 12/1000 | Loss: 0.00002828
Iteration 13/1000 | Loss: 0.00002797
Iteration 14/1000 | Loss: 0.00002797
Iteration 15/1000 | Loss: 0.00002795
Iteration 16/1000 | Loss: 0.00002775
Iteration 17/1000 | Loss: 0.00006488
Iteration 18/1000 | Loss: 0.00003106
Iteration 19/1000 | Loss: 0.00002965
Iteration 20/1000 | Loss: 0.00003154
Iteration 21/1000 | Loss: 0.00002728
Iteration 22/1000 | Loss: 0.00002714
Iteration 23/1000 | Loss: 0.00002712
Iteration 24/1000 | Loss: 0.00002710
Iteration 25/1000 | Loss: 0.00002709
Iteration 26/1000 | Loss: 0.00002709
Iteration 27/1000 | Loss: 0.00002708
Iteration 28/1000 | Loss: 0.00002708
Iteration 29/1000 | Loss: 0.00002706
Iteration 30/1000 | Loss: 0.00002705
Iteration 31/1000 | Loss: 0.00002704
Iteration 32/1000 | Loss: 0.00002703
Iteration 33/1000 | Loss: 0.00002700
Iteration 34/1000 | Loss: 0.00006861
Iteration 35/1000 | Loss: 0.00002749
Iteration 36/1000 | Loss: 0.00002691
Iteration 37/1000 | Loss: 0.00002686
Iteration 38/1000 | Loss: 0.00002681
Iteration 39/1000 | Loss: 0.00002681
Iteration 40/1000 | Loss: 0.00002681
Iteration 41/1000 | Loss: 0.00002680
Iteration 42/1000 | Loss: 0.00002672
Iteration 43/1000 | Loss: 0.00002671
Iteration 44/1000 | Loss: 0.00002670
Iteration 45/1000 | Loss: 0.00002670
Iteration 46/1000 | Loss: 0.00002668
Iteration 47/1000 | Loss: 0.00002664
Iteration 48/1000 | Loss: 0.00008170
Iteration 49/1000 | Loss: 0.00002814
Iteration 50/1000 | Loss: 0.00002668
Iteration 51/1000 | Loss: 0.00002647
Iteration 52/1000 | Loss: 0.00002643
Iteration 53/1000 | Loss: 0.00002643
Iteration 54/1000 | Loss: 0.00002643
Iteration 55/1000 | Loss: 0.00002643
Iteration 56/1000 | Loss: 0.00002643
Iteration 57/1000 | Loss: 0.00002643
Iteration 58/1000 | Loss: 0.00002643
Iteration 59/1000 | Loss: 0.00002643
Iteration 60/1000 | Loss: 0.00002642
Iteration 61/1000 | Loss: 0.00002642
Iteration 62/1000 | Loss: 0.00002642
Iteration 63/1000 | Loss: 0.00002642
Iteration 64/1000 | Loss: 0.00002642
Iteration 65/1000 | Loss: 0.00002642
Iteration 66/1000 | Loss: 0.00002642
Iteration 67/1000 | Loss: 0.00002642
Iteration 68/1000 | Loss: 0.00002641
Iteration 69/1000 | Loss: 0.00002641
Iteration 70/1000 | Loss: 0.00002641
Iteration 71/1000 | Loss: 0.00002640
Iteration 72/1000 | Loss: 0.00002640
Iteration 73/1000 | Loss: 0.00002640
Iteration 74/1000 | Loss: 0.00002640
Iteration 75/1000 | Loss: 0.00002639
Iteration 76/1000 | Loss: 0.00002639
Iteration 77/1000 | Loss: 0.00002639
Iteration 78/1000 | Loss: 0.00002639
Iteration 79/1000 | Loss: 0.00002638
Iteration 80/1000 | Loss: 0.00002638
Iteration 81/1000 | Loss: 0.00002638
Iteration 82/1000 | Loss: 0.00002638
Iteration 83/1000 | Loss: 0.00002637
Iteration 84/1000 | Loss: 0.00002637
Iteration 85/1000 | Loss: 0.00002637
Iteration 86/1000 | Loss: 0.00002637
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002636
Iteration 89/1000 | Loss: 0.00002636
Iteration 90/1000 | Loss: 0.00002636
Iteration 91/1000 | Loss: 0.00002636
Iteration 92/1000 | Loss: 0.00002636
Iteration 93/1000 | Loss: 0.00002635
Iteration 94/1000 | Loss: 0.00002635
Iteration 95/1000 | Loss: 0.00002635
Iteration 96/1000 | Loss: 0.00002635
Iteration 97/1000 | Loss: 0.00002635
Iteration 98/1000 | Loss: 0.00002635
Iteration 99/1000 | Loss: 0.00002635
Iteration 100/1000 | Loss: 0.00002635
Iteration 101/1000 | Loss: 0.00002635
Iteration 102/1000 | Loss: 0.00002635
Iteration 103/1000 | Loss: 0.00002635
Iteration 104/1000 | Loss: 0.00002634
Iteration 105/1000 | Loss: 0.00002634
Iteration 106/1000 | Loss: 0.00002634
Iteration 107/1000 | Loss: 0.00002634
Iteration 108/1000 | Loss: 0.00002633
Iteration 109/1000 | Loss: 0.00002633
Iteration 110/1000 | Loss: 0.00002633
Iteration 111/1000 | Loss: 0.00002633
Iteration 112/1000 | Loss: 0.00002632
Iteration 113/1000 | Loss: 0.00002632
Iteration 114/1000 | Loss: 0.00002632
Iteration 115/1000 | Loss: 0.00002632
Iteration 116/1000 | Loss: 0.00002631
Iteration 117/1000 | Loss: 0.00002631
Iteration 118/1000 | Loss: 0.00002631
Iteration 119/1000 | Loss: 0.00002631
Iteration 120/1000 | Loss: 0.00002631
Iteration 121/1000 | Loss: 0.00002631
Iteration 122/1000 | Loss: 0.00002631
Iteration 123/1000 | Loss: 0.00002630
Iteration 124/1000 | Loss: 0.00002630
Iteration 125/1000 | Loss: 0.00002630
Iteration 126/1000 | Loss: 0.00002630
Iteration 127/1000 | Loss: 0.00002630
Iteration 128/1000 | Loss: 0.00002629
Iteration 129/1000 | Loss: 0.00002629
Iteration 130/1000 | Loss: 0.00002629
Iteration 131/1000 | Loss: 0.00002628
Iteration 132/1000 | Loss: 0.00002628
Iteration 133/1000 | Loss: 0.00002627
Iteration 134/1000 | Loss: 0.00002627
Iteration 135/1000 | Loss: 0.00002627
Iteration 136/1000 | Loss: 0.00002626
Iteration 137/1000 | Loss: 0.00002626
Iteration 138/1000 | Loss: 0.00002626
Iteration 139/1000 | Loss: 0.00002626
Iteration 140/1000 | Loss: 0.00002626
Iteration 141/1000 | Loss: 0.00002626
Iteration 142/1000 | Loss: 0.00002626
Iteration 143/1000 | Loss: 0.00002626
Iteration 144/1000 | Loss: 0.00002626
Iteration 145/1000 | Loss: 0.00002626
Iteration 146/1000 | Loss: 0.00002626
Iteration 147/1000 | Loss: 0.00002626
Iteration 148/1000 | Loss: 0.00002626
Iteration 149/1000 | Loss: 0.00002626
Iteration 150/1000 | Loss: 0.00002626
Iteration 151/1000 | Loss: 0.00002626
Iteration 152/1000 | Loss: 0.00002626
Iteration 153/1000 | Loss: 0.00002626
Iteration 154/1000 | Loss: 0.00002625
Iteration 155/1000 | Loss: 0.00002624
Iteration 156/1000 | Loss: 0.00002624
Iteration 157/1000 | Loss: 0.00002624
Iteration 158/1000 | Loss: 0.00002624
Iteration 159/1000 | Loss: 0.00002624
Iteration 160/1000 | Loss: 0.00002624
Iteration 161/1000 | Loss: 0.00002624
Iteration 162/1000 | Loss: 0.00002624
Iteration 163/1000 | Loss: 0.00002624
Iteration 164/1000 | Loss: 0.00002624
Iteration 165/1000 | Loss: 0.00002624
Iteration 166/1000 | Loss: 0.00002624
Iteration 167/1000 | Loss: 0.00002624
Iteration 168/1000 | Loss: 0.00002623
Iteration 169/1000 | Loss: 0.00002623
Iteration 170/1000 | Loss: 0.00002623
Iteration 171/1000 | Loss: 0.00002623
Iteration 172/1000 | Loss: 0.00002623
Iteration 173/1000 | Loss: 0.00002623
Iteration 174/1000 | Loss: 0.00002623
Iteration 175/1000 | Loss: 0.00002623
Iteration 176/1000 | Loss: 0.00002623
Iteration 177/1000 | Loss: 0.00002623
Iteration 178/1000 | Loss: 0.00002623
Iteration 179/1000 | Loss: 0.00002622
Iteration 180/1000 | Loss: 0.00002622
Iteration 181/1000 | Loss: 0.00002622
Iteration 182/1000 | Loss: 0.00002622
Iteration 183/1000 | Loss: 0.00002622
Iteration 184/1000 | Loss: 0.00002622
Iteration 185/1000 | Loss: 0.00002621
Iteration 186/1000 | Loss: 0.00002621
Iteration 187/1000 | Loss: 0.00002620
Iteration 188/1000 | Loss: 0.00002620
Iteration 189/1000 | Loss: 0.00002620
Iteration 190/1000 | Loss: 0.00002620
Iteration 191/1000 | Loss: 0.00002620
Iteration 192/1000 | Loss: 0.00002619
Iteration 193/1000 | Loss: 0.00002619
Iteration 194/1000 | Loss: 0.00002619
Iteration 195/1000 | Loss: 0.00002619
Iteration 196/1000 | Loss: 0.00002619
Iteration 197/1000 | Loss: 0.00002619
Iteration 198/1000 | Loss: 0.00002619
Iteration 199/1000 | Loss: 0.00002619
Iteration 200/1000 | Loss: 0.00002619
Iteration 201/1000 | Loss: 0.00002619
Iteration 202/1000 | Loss: 0.00002619
Iteration 203/1000 | Loss: 0.00002618
Iteration 204/1000 | Loss: 0.00002618
Iteration 205/1000 | Loss: 0.00002618
Iteration 206/1000 | Loss: 0.00002617
Iteration 207/1000 | Loss: 0.00002617
Iteration 208/1000 | Loss: 0.00002617
Iteration 209/1000 | Loss: 0.00002616
Iteration 210/1000 | Loss: 0.00002616
Iteration 211/1000 | Loss: 0.00002616
Iteration 212/1000 | Loss: 0.00002616
Iteration 213/1000 | Loss: 0.00002616
Iteration 214/1000 | Loss: 0.00002615
Iteration 215/1000 | Loss: 0.00002615
Iteration 216/1000 | Loss: 0.00002615
Iteration 217/1000 | Loss: 0.00002614
Iteration 218/1000 | Loss: 0.00002614
Iteration 219/1000 | Loss: 0.00002614
Iteration 220/1000 | Loss: 0.00002614
Iteration 221/1000 | Loss: 0.00002613
Iteration 222/1000 | Loss: 0.00002613
Iteration 223/1000 | Loss: 0.00002613
Iteration 224/1000 | Loss: 0.00002613
Iteration 225/1000 | Loss: 0.00002612
Iteration 226/1000 | Loss: 0.00002612
Iteration 227/1000 | Loss: 0.00002612
Iteration 228/1000 | Loss: 0.00002612
Iteration 229/1000 | Loss: 0.00002612
Iteration 230/1000 | Loss: 0.00002612
Iteration 231/1000 | Loss: 0.00002611
Iteration 232/1000 | Loss: 0.00002611
Iteration 233/1000 | Loss: 0.00002611
Iteration 234/1000 | Loss: 0.00002611
Iteration 235/1000 | Loss: 0.00002611
Iteration 236/1000 | Loss: 0.00002611
Iteration 237/1000 | Loss: 0.00002611
Iteration 238/1000 | Loss: 0.00002611
Iteration 239/1000 | Loss: 0.00002611
Iteration 240/1000 | Loss: 0.00002611
Iteration 241/1000 | Loss: 0.00002611
Iteration 242/1000 | Loss: 0.00002611
Iteration 243/1000 | Loss: 0.00002611
Iteration 244/1000 | Loss: 0.00002611
Iteration 245/1000 | Loss: 0.00002611
Iteration 246/1000 | Loss: 0.00002611
Iteration 247/1000 | Loss: 0.00002611
Iteration 248/1000 | Loss: 0.00002611
Iteration 249/1000 | Loss: 0.00002611
Iteration 250/1000 | Loss: 0.00002611
Iteration 251/1000 | Loss: 0.00002611
Iteration 252/1000 | Loss: 0.00002611
Iteration 253/1000 | Loss: 0.00002611
Iteration 254/1000 | Loss: 0.00002611
Iteration 255/1000 | Loss: 0.00002611
Iteration 256/1000 | Loss: 0.00002611
Iteration 257/1000 | Loss: 0.00002611
Iteration 258/1000 | Loss: 0.00002611
Iteration 259/1000 | Loss: 0.00002611
Iteration 260/1000 | Loss: 0.00002611
Iteration 261/1000 | Loss: 0.00002611
Iteration 262/1000 | Loss: 0.00002611
Iteration 263/1000 | Loss: 0.00002611
Iteration 264/1000 | Loss: 0.00002611
Iteration 265/1000 | Loss: 0.00002611
Iteration 266/1000 | Loss: 0.00002611
Iteration 267/1000 | Loss: 0.00002611
Iteration 268/1000 | Loss: 0.00002611
Iteration 269/1000 | Loss: 0.00002611
Iteration 270/1000 | Loss: 0.00002611
Iteration 271/1000 | Loss: 0.00002611
Iteration 272/1000 | Loss: 0.00002611
Iteration 273/1000 | Loss: 0.00002611
Iteration 274/1000 | Loss: 0.00002611
Iteration 275/1000 | Loss: 0.00002611
Iteration 276/1000 | Loss: 0.00002611
Iteration 277/1000 | Loss: 0.00002611
Iteration 278/1000 | Loss: 0.00002611
Iteration 279/1000 | Loss: 0.00002611
Iteration 280/1000 | Loss: 0.00002611
Iteration 281/1000 | Loss: 0.00002611
Iteration 282/1000 | Loss: 0.00002611
Iteration 283/1000 | Loss: 0.00002611
Iteration 284/1000 | Loss: 0.00002611
Iteration 285/1000 | Loss: 0.00002611
Iteration 286/1000 | Loss: 0.00002611
Iteration 287/1000 | Loss: 0.00002611
Iteration 288/1000 | Loss: 0.00002611
Iteration 289/1000 | Loss: 0.00002611
Iteration 290/1000 | Loss: 0.00002611
Iteration 291/1000 | Loss: 0.00002611
Iteration 292/1000 | Loss: 0.00002611
Iteration 293/1000 | Loss: 0.00002611
Iteration 294/1000 | Loss: 0.00002611
Iteration 295/1000 | Loss: 0.00002611
Iteration 296/1000 | Loss: 0.00002611
Iteration 297/1000 | Loss: 0.00002611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [2.610625779198017e-05, 2.610625779198017e-05, 2.610625779198017e-05, 2.610625779198017e-05, 2.610625779198017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.610625779198017e-05

Optimization complete. Final v2v error: 3.946547031402588 mm

Highest mean error: 12.132116317749023 mm for frame 48

Lowest mean error: 3.41788911819458 mm for frame 131

Saving results

Total time: 85.35036945343018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041030
Iteration 2/25 | Loss: 0.00289121
Iteration 3/25 | Loss: 0.00247749
Iteration 4/25 | Loss: 0.00207471
Iteration 5/25 | Loss: 0.00173343
Iteration 6/25 | Loss: 0.00160602
Iteration 7/25 | Loss: 0.00158795
Iteration 8/25 | Loss: 0.00152920
Iteration 9/25 | Loss: 0.00145108
Iteration 10/25 | Loss: 0.00140800
Iteration 11/25 | Loss: 0.00137111
Iteration 12/25 | Loss: 0.00135314
Iteration 13/25 | Loss: 0.00135030
Iteration 14/25 | Loss: 0.00134474
Iteration 15/25 | Loss: 0.00134017
Iteration 16/25 | Loss: 0.00133771
Iteration 17/25 | Loss: 0.00133670
Iteration 18/25 | Loss: 0.00133641
Iteration 19/25 | Loss: 0.00133637
Iteration 20/25 | Loss: 0.00133636
Iteration 21/25 | Loss: 0.00133636
Iteration 22/25 | Loss: 0.00133636
Iteration 23/25 | Loss: 0.00133636
Iteration 24/25 | Loss: 0.00133636
Iteration 25/25 | Loss: 0.00133636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44407690
Iteration 2/25 | Loss: 0.00115835
Iteration 3/25 | Loss: 0.00115834
Iteration 4/25 | Loss: 0.00115834
Iteration 5/25 | Loss: 0.00089033
Iteration 6/25 | Loss: 0.00089029
Iteration 7/25 | Loss: 0.00089029
Iteration 8/25 | Loss: 0.00089029
Iteration 9/25 | Loss: 0.00089029
Iteration 10/25 | Loss: 0.00089029
Iteration 11/25 | Loss: 0.00089029
Iteration 12/25 | Loss: 0.00089029
Iteration 13/25 | Loss: 0.00089028
Iteration 14/25 | Loss: 0.00089028
Iteration 15/25 | Loss: 0.00089028
Iteration 16/25 | Loss: 0.00089028
Iteration 17/25 | Loss: 0.00089028
Iteration 18/25 | Loss: 0.00089028
Iteration 19/25 | Loss: 0.00089028
Iteration 20/25 | Loss: 0.00089028
Iteration 21/25 | Loss: 0.00089028
Iteration 22/25 | Loss: 0.00089028
Iteration 23/25 | Loss: 0.00089028
Iteration 24/25 | Loss: 0.00089028
Iteration 25/25 | Loss: 0.00089028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089028
Iteration 2/1000 | Loss: 0.00083993
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00095888
Iteration 5/1000 | Loss: 0.00044243
Iteration 6/1000 | Loss: 0.00003599
Iteration 7/1000 | Loss: 0.00002820
Iteration 8/1000 | Loss: 0.00002409
Iteration 9/1000 | Loss: 0.00002319
Iteration 10/1000 | Loss: 0.00002245
Iteration 11/1000 | Loss: 0.00002199
Iteration 12/1000 | Loss: 0.00002157
Iteration 13/1000 | Loss: 0.00002127
Iteration 14/1000 | Loss: 0.00052498
Iteration 15/1000 | Loss: 0.00123273
Iteration 16/1000 | Loss: 0.00036356
Iteration 17/1000 | Loss: 0.00002549
Iteration 18/1000 | Loss: 0.00002217
Iteration 19/1000 | Loss: 0.00025328
Iteration 20/1000 | Loss: 0.00002520
Iteration 21/1000 | Loss: 0.00002032
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001861
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001835
Iteration 27/1000 | Loss: 0.00001831
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001802
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001784
Iteration 34/1000 | Loss: 0.00001783
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001781
Iteration 38/1000 | Loss: 0.00001781
Iteration 39/1000 | Loss: 0.00001781
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001780
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001779
Iteration 46/1000 | Loss: 0.00001779
Iteration 47/1000 | Loss: 0.00001779
Iteration 48/1000 | Loss: 0.00001779
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001776
Iteration 56/1000 | Loss: 0.00001775
Iteration 57/1000 | Loss: 0.00001775
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001774
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001774
Iteration 64/1000 | Loss: 0.00001774
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001773
Iteration 67/1000 | Loss: 0.00001773
Iteration 68/1000 | Loss: 0.00001773
Iteration 69/1000 | Loss: 0.00001773
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001773
Iteration 77/1000 | Loss: 0.00001773
Iteration 78/1000 | Loss: 0.00001773
Iteration 79/1000 | Loss: 0.00001773
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001772
Iteration 86/1000 | Loss: 0.00001772
Iteration 87/1000 | Loss: 0.00001772
Iteration 88/1000 | Loss: 0.00001772
Iteration 89/1000 | Loss: 0.00001772
Iteration 90/1000 | Loss: 0.00001772
Iteration 91/1000 | Loss: 0.00001772
Iteration 92/1000 | Loss: 0.00001772
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001771
Iteration 95/1000 | Loss: 0.00001771
Iteration 96/1000 | Loss: 0.00001771
Iteration 97/1000 | Loss: 0.00001771
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001771
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001771
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001770
Iteration 104/1000 | Loss: 0.00001770
Iteration 105/1000 | Loss: 0.00001770
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001769
Iteration 113/1000 | Loss: 0.00001769
Iteration 114/1000 | Loss: 0.00001769
Iteration 115/1000 | Loss: 0.00001769
Iteration 116/1000 | Loss: 0.00001769
Iteration 117/1000 | Loss: 0.00001769
Iteration 118/1000 | Loss: 0.00001769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.7692444089334458e-05, 1.7692444089334458e-05, 1.7692444089334458e-05, 1.7692444089334458e-05, 1.7692444089334458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7692444089334458e-05

Optimization complete. Final v2v error: 3.567939043045044 mm

Highest mean error: 4.094267845153809 mm for frame 12

Lowest mean error: 3.117816686630249 mm for frame 122

Saving results

Total time: 76.2028284072876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00711011
Iteration 2/25 | Loss: 0.00240416
Iteration 3/25 | Loss: 0.00161118
Iteration 4/25 | Loss: 0.00150748
Iteration 5/25 | Loss: 0.00142006
Iteration 6/25 | Loss: 0.00140638
Iteration 7/25 | Loss: 0.00134467
Iteration 8/25 | Loss: 0.00131126
Iteration 9/25 | Loss: 0.00127692
Iteration 10/25 | Loss: 0.00127319
Iteration 11/25 | Loss: 0.00126716
Iteration 12/25 | Loss: 0.00126423
Iteration 13/25 | Loss: 0.00126700
Iteration 14/25 | Loss: 0.00126288
Iteration 15/25 | Loss: 0.00126483
Iteration 16/25 | Loss: 0.00126278
Iteration 17/25 | Loss: 0.00126278
Iteration 18/25 | Loss: 0.00126278
Iteration 19/25 | Loss: 0.00126277
Iteration 20/25 | Loss: 0.00126277
Iteration 21/25 | Loss: 0.00126277
Iteration 22/25 | Loss: 0.00126277
Iteration 23/25 | Loss: 0.00126277
Iteration 24/25 | Loss: 0.00126276
Iteration 25/25 | Loss: 0.00126276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71814692
Iteration 2/25 | Loss: 0.00095017
Iteration 3/25 | Loss: 0.00092295
Iteration 4/25 | Loss: 0.00092295
Iteration 5/25 | Loss: 0.00092295
Iteration 6/25 | Loss: 0.00092295
Iteration 7/25 | Loss: 0.00092295
Iteration 8/25 | Loss: 0.00092295
Iteration 9/25 | Loss: 0.00092295
Iteration 10/25 | Loss: 0.00092295
Iteration 11/25 | Loss: 0.00092295
Iteration 12/25 | Loss: 0.00092295
Iteration 13/25 | Loss: 0.00092295
Iteration 14/25 | Loss: 0.00092295
Iteration 15/25 | Loss: 0.00092295
Iteration 16/25 | Loss: 0.00092295
Iteration 17/25 | Loss: 0.00092295
Iteration 18/25 | Loss: 0.00092294
Iteration 19/25 | Loss: 0.00092294
Iteration 20/25 | Loss: 0.00092294
Iteration 21/25 | Loss: 0.00092294
Iteration 22/25 | Loss: 0.00092294
Iteration 23/25 | Loss: 0.00092294
Iteration 24/25 | Loss: 0.00092294
Iteration 25/25 | Loss: 0.00092294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092294
Iteration 2/1000 | Loss: 0.00003132
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00003122
Iteration 5/1000 | Loss: 0.00002995
Iteration 6/1000 | Loss: 0.00004859
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001638
Iteration 9/1000 | Loss: 0.00001518
Iteration 10/1000 | Loss: 0.00001494
Iteration 11/1000 | Loss: 0.00001470
Iteration 12/1000 | Loss: 0.00001446
Iteration 13/1000 | Loss: 0.00001437
Iteration 14/1000 | Loss: 0.00003099
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00045722
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00002063
Iteration 20/1000 | Loss: 0.00001647
Iteration 21/1000 | Loss: 0.00002035
Iteration 22/1000 | Loss: 0.00001324
Iteration 23/1000 | Loss: 0.00002683
Iteration 24/1000 | Loss: 0.00001262
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00002202
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00002844
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001225
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001224
Iteration 56/1000 | Loss: 0.00001224
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00002603
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001217
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001213
Iteration 81/1000 | Loss: 0.00001213
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001210
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001209
Iteration 89/1000 | Loss: 0.00001209
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001209
Iteration 95/1000 | Loss: 0.00001209
Iteration 96/1000 | Loss: 0.00001209
Iteration 97/1000 | Loss: 0.00001209
Iteration 98/1000 | Loss: 0.00001209
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001208
Iteration 101/1000 | Loss: 0.00001208
Iteration 102/1000 | Loss: 0.00001208
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00002184
Iteration 106/1000 | Loss: 0.00001449
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001204
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001204
Iteration 126/1000 | Loss: 0.00001204
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001204
Iteration 129/1000 | Loss: 0.00001204
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.203966803586809e-05, 1.203966803586809e-05, 1.203966803586809e-05, 1.203966803586809e-05, 1.203966803586809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.203966803586809e-05

Optimization complete. Final v2v error: 2.9913768768310547 mm

Highest mean error: 3.95558500289917 mm for frame 194

Lowest mean error: 2.8203072547912598 mm for frame 210

Saving results

Total time: 81.33352637290955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444243
Iteration 2/25 | Loss: 0.00134266
Iteration 3/25 | Loss: 0.00127566
Iteration 4/25 | Loss: 0.00126458
Iteration 5/25 | Loss: 0.00126194
Iteration 6/25 | Loss: 0.00126194
Iteration 7/25 | Loss: 0.00126194
Iteration 8/25 | Loss: 0.00126194
Iteration 9/25 | Loss: 0.00126194
Iteration 10/25 | Loss: 0.00126194
Iteration 11/25 | Loss: 0.00126194
Iteration 12/25 | Loss: 0.00126194
Iteration 13/25 | Loss: 0.00126194
Iteration 14/25 | Loss: 0.00126194
Iteration 15/25 | Loss: 0.00126194
Iteration 16/25 | Loss: 0.00126194
Iteration 17/25 | Loss: 0.00126194
Iteration 18/25 | Loss: 0.00126194
Iteration 19/25 | Loss: 0.00126194
Iteration 20/25 | Loss: 0.00126194
Iteration 21/25 | Loss: 0.00126194
Iteration 22/25 | Loss: 0.00126194
Iteration 23/25 | Loss: 0.00126194
Iteration 24/25 | Loss: 0.00126194
Iteration 25/25 | Loss: 0.00126194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.64764738
Iteration 2/25 | Loss: 0.00083943
Iteration 3/25 | Loss: 0.00083943
Iteration 4/25 | Loss: 0.00083942
Iteration 5/25 | Loss: 0.00083942
Iteration 6/25 | Loss: 0.00083942
Iteration 7/25 | Loss: 0.00083942
Iteration 8/25 | Loss: 0.00083942
Iteration 9/25 | Loss: 0.00083942
Iteration 10/25 | Loss: 0.00083942
Iteration 11/25 | Loss: 0.00083942
Iteration 12/25 | Loss: 0.00083942
Iteration 13/25 | Loss: 0.00083942
Iteration 14/25 | Loss: 0.00083942
Iteration 15/25 | Loss: 0.00083942
Iteration 16/25 | Loss: 0.00083942
Iteration 17/25 | Loss: 0.00083942
Iteration 18/25 | Loss: 0.00083942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008394215838052332, 0.0008394215838052332, 0.0008394215838052332, 0.0008394215838052332, 0.0008394215838052332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008394215838052332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083942
Iteration 2/1000 | Loss: 0.00002381
Iteration 3/1000 | Loss: 0.00001782
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001582
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001491
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001417
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001385
Iteration 12/1000 | Loss: 0.00001366
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001351
Iteration 15/1000 | Loss: 0.00001344
Iteration 16/1000 | Loss: 0.00001340
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001315
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001314
Iteration 24/1000 | Loss: 0.00001313
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001307
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001300
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001292
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001291
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001290
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001283
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001281
Iteration 50/1000 | Loss: 0.00001279
Iteration 51/1000 | Loss: 0.00001279
Iteration 52/1000 | Loss: 0.00001279
Iteration 53/1000 | Loss: 0.00001279
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001278
Iteration 56/1000 | Loss: 0.00001278
Iteration 57/1000 | Loss: 0.00001278
Iteration 58/1000 | Loss: 0.00001278
Iteration 59/1000 | Loss: 0.00001278
Iteration 60/1000 | Loss: 0.00001278
Iteration 61/1000 | Loss: 0.00001278
Iteration 62/1000 | Loss: 0.00001278
Iteration 63/1000 | Loss: 0.00001277
Iteration 64/1000 | Loss: 0.00001277
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001276
Iteration 67/1000 | Loss: 0.00001276
Iteration 68/1000 | Loss: 0.00001275
Iteration 69/1000 | Loss: 0.00001275
Iteration 70/1000 | Loss: 0.00001275
Iteration 71/1000 | Loss: 0.00001275
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001275
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001274
Iteration 81/1000 | Loss: 0.00001274
Iteration 82/1000 | Loss: 0.00001274
Iteration 83/1000 | Loss: 0.00001274
Iteration 84/1000 | Loss: 0.00001274
Iteration 85/1000 | Loss: 0.00001274
Iteration 86/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.2735214113490656e-05, 1.2735214113490656e-05, 1.2735214113490656e-05, 1.2735214113490656e-05, 1.2735214113490656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2735214113490656e-05

Optimization complete. Final v2v error: 3.080798625946045 mm

Highest mean error: 3.3843822479248047 mm for frame 44

Lowest mean error: 2.865065574645996 mm for frame 27

Saving results

Total time: 40.302753925323486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974652
Iteration 2/25 | Loss: 0.00297260
Iteration 3/25 | Loss: 0.00201200
Iteration 4/25 | Loss: 0.00175227
Iteration 5/25 | Loss: 0.00169649
Iteration 6/25 | Loss: 0.00163580
Iteration 7/25 | Loss: 0.00163884
Iteration 8/25 | Loss: 0.00162618
Iteration 9/25 | Loss: 0.00160511
Iteration 10/25 | Loss: 0.00157006
Iteration 11/25 | Loss: 0.00156041
Iteration 12/25 | Loss: 0.00154748
Iteration 13/25 | Loss: 0.00154718
Iteration 14/25 | Loss: 0.00153668
Iteration 15/25 | Loss: 0.00152279
Iteration 16/25 | Loss: 0.00152035
Iteration 17/25 | Loss: 0.00151080
Iteration 18/25 | Loss: 0.00150593
Iteration 19/25 | Loss: 0.00150109
Iteration 20/25 | Loss: 0.00149976
Iteration 21/25 | Loss: 0.00149950
Iteration 22/25 | Loss: 0.00149889
Iteration 23/25 | Loss: 0.00149887
Iteration 24/25 | Loss: 0.00149898
Iteration 25/25 | Loss: 0.00149897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37467217
Iteration 2/25 | Loss: 0.00238911
Iteration 3/25 | Loss: 0.00238911
Iteration 4/25 | Loss: 0.00195555
Iteration 5/25 | Loss: 0.00193043
Iteration 6/25 | Loss: 0.00193043
Iteration 7/25 | Loss: 0.00193043
Iteration 8/25 | Loss: 0.00193043
Iteration 9/25 | Loss: 0.00193043
Iteration 10/25 | Loss: 0.00193043
Iteration 11/25 | Loss: 0.00193043
Iteration 12/25 | Loss: 0.00193043
Iteration 13/25 | Loss: 0.00193043
Iteration 14/25 | Loss: 0.00193043
Iteration 15/25 | Loss: 0.00193043
Iteration 16/25 | Loss: 0.00193043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019304297165945172, 0.0019304297165945172, 0.0019304297165945172, 0.0019304297165945172, 0.0019304297165945172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019304297165945172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193043
Iteration 2/1000 | Loss: 0.00072687
Iteration 3/1000 | Loss: 0.00171060
Iteration 4/1000 | Loss: 0.00078891
Iteration 5/1000 | Loss: 0.00025607
Iteration 6/1000 | Loss: 0.00015309
Iteration 7/1000 | Loss: 0.00039153
Iteration 8/1000 | Loss: 0.00031853
Iteration 9/1000 | Loss: 0.00021984
Iteration 10/1000 | Loss: 0.00013139
Iteration 11/1000 | Loss: 0.00039865
Iteration 12/1000 | Loss: 0.00024145
Iteration 13/1000 | Loss: 0.00034025
Iteration 14/1000 | Loss: 0.00038586
Iteration 15/1000 | Loss: 0.00132044
Iteration 16/1000 | Loss: 0.00051655
Iteration 17/1000 | Loss: 0.00018991
Iteration 18/1000 | Loss: 0.00014009
Iteration 19/1000 | Loss: 0.00034096
Iteration 20/1000 | Loss: 0.00013637
Iteration 21/1000 | Loss: 0.00020084
Iteration 22/1000 | Loss: 0.00015557
Iteration 23/1000 | Loss: 0.00017512
Iteration 24/1000 | Loss: 0.00020191
Iteration 25/1000 | Loss: 0.00017292
Iteration 26/1000 | Loss: 0.00016948
Iteration 27/1000 | Loss: 0.00013637
Iteration 28/1000 | Loss: 0.00008468
Iteration 29/1000 | Loss: 0.00008302
Iteration 30/1000 | Loss: 0.00024919
Iteration 31/1000 | Loss: 0.00066007
Iteration 32/1000 | Loss: 0.00324803
Iteration 33/1000 | Loss: 0.00288351
Iteration 34/1000 | Loss: 0.00034284
Iteration 35/1000 | Loss: 0.00050194
Iteration 36/1000 | Loss: 0.00038879
Iteration 37/1000 | Loss: 0.00027703
Iteration 38/1000 | Loss: 0.00013669
Iteration 39/1000 | Loss: 0.00010841
Iteration 40/1000 | Loss: 0.00036003
Iteration 41/1000 | Loss: 0.00007817
Iteration 42/1000 | Loss: 0.00028539
Iteration 43/1000 | Loss: 0.00004509
Iteration 44/1000 | Loss: 0.00014399
Iteration 45/1000 | Loss: 0.00005821
Iteration 46/1000 | Loss: 0.00010935
Iteration 47/1000 | Loss: 0.00004430
Iteration 48/1000 | Loss: 0.00003175
Iteration 49/1000 | Loss: 0.00007702
Iteration 50/1000 | Loss: 0.00003461
Iteration 51/1000 | Loss: 0.00006035
Iteration 52/1000 | Loss: 0.00002688
Iteration 53/1000 | Loss: 0.00005380
Iteration 54/1000 | Loss: 0.00017254
Iteration 55/1000 | Loss: 0.00003660
Iteration 56/1000 | Loss: 0.00003465
Iteration 57/1000 | Loss: 0.00003086
Iteration 58/1000 | Loss: 0.00004008
Iteration 59/1000 | Loss: 0.00002478
Iteration 60/1000 | Loss: 0.00004675
Iteration 61/1000 | Loss: 0.00002676
Iteration 62/1000 | Loss: 0.00003270
Iteration 63/1000 | Loss: 0.00003719
Iteration 64/1000 | Loss: 0.00003142
Iteration 65/1000 | Loss: 0.00003265
Iteration 66/1000 | Loss: 0.00002318
Iteration 67/1000 | Loss: 0.00002793
Iteration 68/1000 | Loss: 0.00002903
Iteration 69/1000 | Loss: 0.00002292
Iteration 70/1000 | Loss: 0.00002467
Iteration 71/1000 | Loss: 0.00002289
Iteration 72/1000 | Loss: 0.00002279
Iteration 73/1000 | Loss: 0.00002279
Iteration 74/1000 | Loss: 0.00002279
Iteration 75/1000 | Loss: 0.00002279
Iteration 76/1000 | Loss: 0.00002279
Iteration 77/1000 | Loss: 0.00002279
Iteration 78/1000 | Loss: 0.00002279
Iteration 79/1000 | Loss: 0.00002490
Iteration 80/1000 | Loss: 0.00003193
Iteration 81/1000 | Loss: 0.00003672
Iteration 82/1000 | Loss: 0.00002303
Iteration 83/1000 | Loss: 0.00002541
Iteration 84/1000 | Loss: 0.00004081
Iteration 85/1000 | Loss: 0.00005511
Iteration 86/1000 | Loss: 0.00002300
Iteration 87/1000 | Loss: 0.00002343
Iteration 88/1000 | Loss: 0.00002284
Iteration 89/1000 | Loss: 0.00002259
Iteration 90/1000 | Loss: 0.00002259
Iteration 91/1000 | Loss: 0.00002259
Iteration 92/1000 | Loss: 0.00002259
Iteration 93/1000 | Loss: 0.00002259
Iteration 94/1000 | Loss: 0.00002259
Iteration 95/1000 | Loss: 0.00002259
Iteration 96/1000 | Loss: 0.00002259
Iteration 97/1000 | Loss: 0.00002259
Iteration 98/1000 | Loss: 0.00002259
Iteration 99/1000 | Loss: 0.00002259
Iteration 100/1000 | Loss: 0.00002259
Iteration 101/1000 | Loss: 0.00002259
Iteration 102/1000 | Loss: 0.00002259
Iteration 103/1000 | Loss: 0.00002259
Iteration 104/1000 | Loss: 0.00002259
Iteration 105/1000 | Loss: 0.00002259
Iteration 106/1000 | Loss: 0.00002259
Iteration 107/1000 | Loss: 0.00002259
Iteration 108/1000 | Loss: 0.00002259
Iteration 109/1000 | Loss: 0.00002259
Iteration 110/1000 | Loss: 0.00002259
Iteration 111/1000 | Loss: 0.00002259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.2594262190978043e-05, 2.2594262190978043e-05, 2.2594262190978043e-05, 2.2594262190978043e-05, 2.2594262190978043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2594262190978043e-05

Optimization complete. Final v2v error: 3.690917730331421 mm

Highest mean error: 10.500761985778809 mm for frame 184

Lowest mean error: 3.1180601119995117 mm for frame 135

Saving results

Total time: 169.52701783180237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404099
Iteration 2/25 | Loss: 0.00134270
Iteration 3/25 | Loss: 0.00127509
Iteration 4/25 | Loss: 0.00126519
Iteration 5/25 | Loss: 0.00126188
Iteration 6/25 | Loss: 0.00126167
Iteration 7/25 | Loss: 0.00126167
Iteration 8/25 | Loss: 0.00126167
Iteration 9/25 | Loss: 0.00126167
Iteration 10/25 | Loss: 0.00126167
Iteration 11/25 | Loss: 0.00126167
Iteration 12/25 | Loss: 0.00126167
Iteration 13/25 | Loss: 0.00126167
Iteration 14/25 | Loss: 0.00126167
Iteration 15/25 | Loss: 0.00126167
Iteration 16/25 | Loss: 0.00126167
Iteration 17/25 | Loss: 0.00126167
Iteration 18/25 | Loss: 0.00126167
Iteration 19/25 | Loss: 0.00126167
Iteration 20/25 | Loss: 0.00126167
Iteration 21/25 | Loss: 0.00126167
Iteration 22/25 | Loss: 0.00126167
Iteration 23/25 | Loss: 0.00126167
Iteration 24/25 | Loss: 0.00126167
Iteration 25/25 | Loss: 0.00126167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.64496565
Iteration 2/25 | Loss: 0.00086638
Iteration 3/25 | Loss: 0.00086637
Iteration 4/25 | Loss: 0.00086637
Iteration 5/25 | Loss: 0.00086637
Iteration 6/25 | Loss: 0.00086637
Iteration 7/25 | Loss: 0.00086637
Iteration 8/25 | Loss: 0.00086637
Iteration 9/25 | Loss: 0.00086637
Iteration 10/25 | Loss: 0.00086637
Iteration 11/25 | Loss: 0.00086637
Iteration 12/25 | Loss: 0.00086637
Iteration 13/25 | Loss: 0.00086637
Iteration 14/25 | Loss: 0.00086637
Iteration 15/25 | Loss: 0.00086637
Iteration 16/25 | Loss: 0.00086637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008663716726005077, 0.0008663716726005077, 0.0008663716726005077, 0.0008663716726005077, 0.0008663716726005077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008663716726005077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086637
Iteration 2/1000 | Loss: 0.00002343
Iteration 3/1000 | Loss: 0.00001765
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001439
Iteration 7/1000 | Loss: 0.00001402
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001302
Iteration 16/1000 | Loss: 0.00001298
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001287
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001271
Iteration 30/1000 | Loss: 0.00001271
Iteration 31/1000 | Loss: 0.00001270
Iteration 32/1000 | Loss: 0.00001267
Iteration 33/1000 | Loss: 0.00001267
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001266
Iteration 36/1000 | Loss: 0.00001260
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001260
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001259
Iteration 42/1000 | Loss: 0.00001258
Iteration 43/1000 | Loss: 0.00001256
Iteration 44/1000 | Loss: 0.00001256
Iteration 45/1000 | Loss: 0.00001256
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001255
Iteration 50/1000 | Loss: 0.00001255
Iteration 51/1000 | Loss: 0.00001255
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001255
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001254
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001253
Iteration 62/1000 | Loss: 0.00001253
Iteration 63/1000 | Loss: 0.00001253
Iteration 64/1000 | Loss: 0.00001253
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001253
Iteration 67/1000 | Loss: 0.00001253
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001252
Iteration 72/1000 | Loss: 0.00001252
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001252
Iteration 78/1000 | Loss: 0.00001252
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001250
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001250
Iteration 85/1000 | Loss: 0.00001249
Iteration 86/1000 | Loss: 0.00001249
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001248
Iteration 89/1000 | Loss: 0.00001248
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001247
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001246
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001245
Iteration 98/1000 | Loss: 0.00001245
Iteration 99/1000 | Loss: 0.00001244
Iteration 100/1000 | Loss: 0.00001244
Iteration 101/1000 | Loss: 0.00001243
Iteration 102/1000 | Loss: 0.00001243
Iteration 103/1000 | Loss: 0.00001243
Iteration 104/1000 | Loss: 0.00001243
Iteration 105/1000 | Loss: 0.00001243
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001240
Iteration 113/1000 | Loss: 0.00001240
Iteration 114/1000 | Loss: 0.00001240
Iteration 115/1000 | Loss: 0.00001240
Iteration 116/1000 | Loss: 0.00001240
Iteration 117/1000 | Loss: 0.00001240
Iteration 118/1000 | Loss: 0.00001240
Iteration 119/1000 | Loss: 0.00001240
Iteration 120/1000 | Loss: 0.00001240
Iteration 121/1000 | Loss: 0.00001240
Iteration 122/1000 | Loss: 0.00001240
Iteration 123/1000 | Loss: 0.00001240
Iteration 124/1000 | Loss: 0.00001240
Iteration 125/1000 | Loss: 0.00001240
Iteration 126/1000 | Loss: 0.00001240
Iteration 127/1000 | Loss: 0.00001240
Iteration 128/1000 | Loss: 0.00001240
Iteration 129/1000 | Loss: 0.00001240
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001239
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001239
Iteration 144/1000 | Loss: 0.00001239
Iteration 145/1000 | Loss: 0.00001239
Iteration 146/1000 | Loss: 0.00001239
Iteration 147/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.2390704796416685e-05, 1.2390704796416685e-05, 1.2390704796416685e-05, 1.2390704796416685e-05, 1.2390704796416685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2390704796416685e-05

Optimization complete. Final v2v error: 3.0125954151153564 mm

Highest mean error: 3.3772101402282715 mm for frame 134

Lowest mean error: 2.823042392730713 mm for frame 167

Saving results

Total time: 38.43941831588745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499840
Iteration 2/25 | Loss: 0.00142623
Iteration 3/25 | Loss: 0.00133155
Iteration 4/25 | Loss: 0.00131024
Iteration 5/25 | Loss: 0.00130498
Iteration 6/25 | Loss: 0.00130351
Iteration 7/25 | Loss: 0.00130339
Iteration 8/25 | Loss: 0.00130339
Iteration 9/25 | Loss: 0.00130339
Iteration 10/25 | Loss: 0.00130339
Iteration 11/25 | Loss: 0.00130339
Iteration 12/25 | Loss: 0.00130339
Iteration 13/25 | Loss: 0.00130339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013033885043114424, 0.0013033885043114424, 0.0013033885043114424, 0.0013033885043114424, 0.0013033885043114424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013033885043114424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01492786
Iteration 2/25 | Loss: 0.00069831
Iteration 3/25 | Loss: 0.00069826
Iteration 4/25 | Loss: 0.00069825
Iteration 5/25 | Loss: 0.00069825
Iteration 6/25 | Loss: 0.00069825
Iteration 7/25 | Loss: 0.00069825
Iteration 8/25 | Loss: 0.00069825
Iteration 9/25 | Loss: 0.00069825
Iteration 10/25 | Loss: 0.00069825
Iteration 11/25 | Loss: 0.00069825
Iteration 12/25 | Loss: 0.00069825
Iteration 13/25 | Loss: 0.00069825
Iteration 14/25 | Loss: 0.00069825
Iteration 15/25 | Loss: 0.00069825
Iteration 16/25 | Loss: 0.00069825
Iteration 17/25 | Loss: 0.00069825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006982522318139672, 0.0006982522318139672, 0.0006982522318139672, 0.0006982522318139672, 0.0006982522318139672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006982522318139672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069825
Iteration 2/1000 | Loss: 0.00005080
Iteration 3/1000 | Loss: 0.00003339
Iteration 4/1000 | Loss: 0.00002746
Iteration 5/1000 | Loss: 0.00002596
Iteration 6/1000 | Loss: 0.00002477
Iteration 7/1000 | Loss: 0.00002385
Iteration 8/1000 | Loss: 0.00002315
Iteration 9/1000 | Loss: 0.00002274
Iteration 10/1000 | Loss: 0.00002236
Iteration 11/1000 | Loss: 0.00002201
Iteration 12/1000 | Loss: 0.00002168
Iteration 13/1000 | Loss: 0.00002139
Iteration 14/1000 | Loss: 0.00002098
Iteration 15/1000 | Loss: 0.00002071
Iteration 16/1000 | Loss: 0.00002050
Iteration 17/1000 | Loss: 0.00002042
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002042
Iteration 20/1000 | Loss: 0.00002042
Iteration 21/1000 | Loss: 0.00002041
Iteration 22/1000 | Loss: 0.00002041
Iteration 23/1000 | Loss: 0.00002041
Iteration 24/1000 | Loss: 0.00002041
Iteration 25/1000 | Loss: 0.00002041
Iteration 26/1000 | Loss: 0.00002041
Iteration 27/1000 | Loss: 0.00002041
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00002040
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002039
Iteration 32/1000 | Loss: 0.00002039
Iteration 33/1000 | Loss: 0.00002035
Iteration 34/1000 | Loss: 0.00002032
Iteration 35/1000 | Loss: 0.00002022
Iteration 36/1000 | Loss: 0.00002022
Iteration 37/1000 | Loss: 0.00002021
Iteration 38/1000 | Loss: 0.00002021
Iteration 39/1000 | Loss: 0.00002021
Iteration 40/1000 | Loss: 0.00002020
Iteration 41/1000 | Loss: 0.00002020
Iteration 42/1000 | Loss: 0.00002020
Iteration 43/1000 | Loss: 0.00002020
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002020
Iteration 46/1000 | Loss: 0.00002020
Iteration 47/1000 | Loss: 0.00002019
Iteration 48/1000 | Loss: 0.00002019
Iteration 49/1000 | Loss: 0.00002019
Iteration 50/1000 | Loss: 0.00002019
Iteration 51/1000 | Loss: 0.00002019
Iteration 52/1000 | Loss: 0.00002019
Iteration 53/1000 | Loss: 0.00002019
Iteration 54/1000 | Loss: 0.00002018
Iteration 55/1000 | Loss: 0.00002018
Iteration 56/1000 | Loss: 0.00002018
Iteration 57/1000 | Loss: 0.00002018
Iteration 58/1000 | Loss: 0.00002017
Iteration 59/1000 | Loss: 0.00002017
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00002017
Iteration 62/1000 | Loss: 0.00002017
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00002014
Iteration 65/1000 | Loss: 0.00002014
Iteration 66/1000 | Loss: 0.00002014
Iteration 67/1000 | Loss: 0.00002014
Iteration 68/1000 | Loss: 0.00002013
Iteration 69/1000 | Loss: 0.00002013
Iteration 70/1000 | Loss: 0.00002013
Iteration 71/1000 | Loss: 0.00002013
Iteration 72/1000 | Loss: 0.00002013
Iteration 73/1000 | Loss: 0.00002013
Iteration 74/1000 | Loss: 0.00002013
Iteration 75/1000 | Loss: 0.00002013
Iteration 76/1000 | Loss: 0.00002013
Iteration 77/1000 | Loss: 0.00002013
Iteration 78/1000 | Loss: 0.00002013
Iteration 79/1000 | Loss: 0.00002013
Iteration 80/1000 | Loss: 0.00002013
Iteration 81/1000 | Loss: 0.00002013
Iteration 82/1000 | Loss: 0.00002012
Iteration 83/1000 | Loss: 0.00002012
Iteration 84/1000 | Loss: 0.00002012
Iteration 85/1000 | Loss: 0.00002012
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002012
Iteration 90/1000 | Loss: 0.00002012
Iteration 91/1000 | Loss: 0.00002012
Iteration 92/1000 | Loss: 0.00002012
Iteration 93/1000 | Loss: 0.00002012
Iteration 94/1000 | Loss: 0.00002012
Iteration 95/1000 | Loss: 0.00002011
Iteration 96/1000 | Loss: 0.00002011
Iteration 97/1000 | Loss: 0.00002011
Iteration 98/1000 | Loss: 0.00002011
Iteration 99/1000 | Loss: 0.00002011
Iteration 100/1000 | Loss: 0.00002011
Iteration 101/1000 | Loss: 0.00002011
Iteration 102/1000 | Loss: 0.00002011
Iteration 103/1000 | Loss: 0.00002011
Iteration 104/1000 | Loss: 0.00002011
Iteration 105/1000 | Loss: 0.00002011
Iteration 106/1000 | Loss: 0.00002011
Iteration 107/1000 | Loss: 0.00002011
Iteration 108/1000 | Loss: 0.00002011
Iteration 109/1000 | Loss: 0.00002010
Iteration 110/1000 | Loss: 0.00002010
Iteration 111/1000 | Loss: 0.00002010
Iteration 112/1000 | Loss: 0.00002010
Iteration 113/1000 | Loss: 0.00002010
Iteration 114/1000 | Loss: 0.00002010
Iteration 115/1000 | Loss: 0.00002010
Iteration 116/1000 | Loss: 0.00002010
Iteration 117/1000 | Loss: 0.00002010
Iteration 118/1000 | Loss: 0.00002010
Iteration 119/1000 | Loss: 0.00002010
Iteration 120/1000 | Loss: 0.00002009
Iteration 121/1000 | Loss: 0.00002009
Iteration 122/1000 | Loss: 0.00002009
Iteration 123/1000 | Loss: 0.00002009
Iteration 124/1000 | Loss: 0.00002009
Iteration 125/1000 | Loss: 0.00002009
Iteration 126/1000 | Loss: 0.00002009
Iteration 127/1000 | Loss: 0.00002009
Iteration 128/1000 | Loss: 0.00002009
Iteration 129/1000 | Loss: 0.00002008
Iteration 130/1000 | Loss: 0.00002008
Iteration 131/1000 | Loss: 0.00002008
Iteration 132/1000 | Loss: 0.00002008
Iteration 133/1000 | Loss: 0.00002008
Iteration 134/1000 | Loss: 0.00002008
Iteration 135/1000 | Loss: 0.00002008
Iteration 136/1000 | Loss: 0.00002008
Iteration 137/1000 | Loss: 0.00002008
Iteration 138/1000 | Loss: 0.00002008
Iteration 139/1000 | Loss: 0.00002008
Iteration 140/1000 | Loss: 0.00002008
Iteration 141/1000 | Loss: 0.00002007
Iteration 142/1000 | Loss: 0.00002007
Iteration 143/1000 | Loss: 0.00002007
Iteration 144/1000 | Loss: 0.00002007
Iteration 145/1000 | Loss: 0.00002007
Iteration 146/1000 | Loss: 0.00002007
Iteration 147/1000 | Loss: 0.00002007
Iteration 148/1000 | Loss: 0.00002007
Iteration 149/1000 | Loss: 0.00002007
Iteration 150/1000 | Loss: 0.00002007
Iteration 151/1000 | Loss: 0.00002007
Iteration 152/1000 | Loss: 0.00002007
Iteration 153/1000 | Loss: 0.00002007
Iteration 154/1000 | Loss: 0.00002006
Iteration 155/1000 | Loss: 0.00002006
Iteration 156/1000 | Loss: 0.00002006
Iteration 157/1000 | Loss: 0.00002006
Iteration 158/1000 | Loss: 0.00002006
Iteration 159/1000 | Loss: 0.00002006
Iteration 160/1000 | Loss: 0.00002006
Iteration 161/1000 | Loss: 0.00002006
Iteration 162/1000 | Loss: 0.00002006
Iteration 163/1000 | Loss: 0.00002006
Iteration 164/1000 | Loss: 0.00002006
Iteration 165/1000 | Loss: 0.00002006
Iteration 166/1000 | Loss: 0.00002006
Iteration 167/1000 | Loss: 0.00002006
Iteration 168/1000 | Loss: 0.00002006
Iteration 169/1000 | Loss: 0.00002006
Iteration 170/1000 | Loss: 0.00002006
Iteration 171/1000 | Loss: 0.00002006
Iteration 172/1000 | Loss: 0.00002005
Iteration 173/1000 | Loss: 0.00002005
Iteration 174/1000 | Loss: 0.00002005
Iteration 175/1000 | Loss: 0.00002005
Iteration 176/1000 | Loss: 0.00002005
Iteration 177/1000 | Loss: 0.00002005
Iteration 178/1000 | Loss: 0.00002005
Iteration 179/1000 | Loss: 0.00002005
Iteration 180/1000 | Loss: 0.00002005
Iteration 181/1000 | Loss: 0.00002005
Iteration 182/1000 | Loss: 0.00002005
Iteration 183/1000 | Loss: 0.00002005
Iteration 184/1000 | Loss: 0.00002005
Iteration 185/1000 | Loss: 0.00002005
Iteration 186/1000 | Loss: 0.00002005
Iteration 187/1000 | Loss: 0.00002005
Iteration 188/1000 | Loss: 0.00002005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.0047637008246966e-05, 2.0047637008246966e-05, 2.0047637008246966e-05, 2.0047637008246966e-05, 2.0047637008246966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0047637008246966e-05

Optimization complete. Final v2v error: 3.785247564315796 mm

Highest mean error: 3.8143510818481445 mm for frame 5

Lowest mean error: 3.75696063041687 mm for frame 41

Saving results

Total time: 41.92508816719055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033647
Iteration 2/25 | Loss: 0.01033647
Iteration 3/25 | Loss: 0.01033647
Iteration 4/25 | Loss: 0.00210257
Iteration 5/25 | Loss: 0.00148344
Iteration 6/25 | Loss: 0.00138973
Iteration 7/25 | Loss: 0.00134918
Iteration 8/25 | Loss: 0.00132427
Iteration 9/25 | Loss: 0.00129271
Iteration 10/25 | Loss: 0.00127506
Iteration 11/25 | Loss: 0.00126725
Iteration 12/25 | Loss: 0.00125906
Iteration 13/25 | Loss: 0.00125673
Iteration 14/25 | Loss: 0.00125745
Iteration 15/25 | Loss: 0.00125582
Iteration 16/25 | Loss: 0.00125622
Iteration 17/25 | Loss: 0.00125548
Iteration 18/25 | Loss: 0.00125548
Iteration 19/25 | Loss: 0.00125548
Iteration 20/25 | Loss: 0.00125548
Iteration 21/25 | Loss: 0.00125548
Iteration 22/25 | Loss: 0.00125548
Iteration 23/25 | Loss: 0.00125548
Iteration 24/25 | Loss: 0.00125547
Iteration 25/25 | Loss: 0.00125547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46234524
Iteration 2/25 | Loss: 0.00108132
Iteration 3/25 | Loss: 0.00103007
Iteration 4/25 | Loss: 0.00103007
Iteration 5/25 | Loss: 0.00103007
Iteration 6/25 | Loss: 0.00103007
Iteration 7/25 | Loss: 0.00103007
Iteration 8/25 | Loss: 0.00103006
Iteration 9/25 | Loss: 0.00103006
Iteration 10/25 | Loss: 0.00103006
Iteration 11/25 | Loss: 0.00103006
Iteration 12/25 | Loss: 0.00103006
Iteration 13/25 | Loss: 0.00103006
Iteration 14/25 | Loss: 0.00103006
Iteration 15/25 | Loss: 0.00103006
Iteration 16/25 | Loss: 0.00103006
Iteration 17/25 | Loss: 0.00103006
Iteration 18/25 | Loss: 0.00103006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001030063722282648, 0.001030063722282648, 0.001030063722282648, 0.001030063722282648, 0.001030063722282648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001030063722282648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103006
Iteration 2/1000 | Loss: 0.00012721
Iteration 3/1000 | Loss: 0.00011187
Iteration 4/1000 | Loss: 0.00002402
Iteration 5/1000 | Loss: 0.00014177
Iteration 6/1000 | Loss: 0.00002209
Iteration 7/1000 | Loss: 0.00008489
Iteration 8/1000 | Loss: 0.00007502
Iteration 9/1000 | Loss: 0.00002294
Iteration 10/1000 | Loss: 0.00001661
Iteration 11/1000 | Loss: 0.00005029
Iteration 12/1000 | Loss: 0.00071989
Iteration 13/1000 | Loss: 0.00050854
Iteration 14/1000 | Loss: 0.00002703
Iteration 15/1000 | Loss: 0.00070656
Iteration 16/1000 | Loss: 0.00039777
Iteration 17/1000 | Loss: 0.00005849
Iteration 18/1000 | Loss: 0.00004398
Iteration 19/1000 | Loss: 0.00083394
Iteration 20/1000 | Loss: 0.00074492
Iteration 21/1000 | Loss: 0.00021244
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00002310
Iteration 24/1000 | Loss: 0.00066684
Iteration 25/1000 | Loss: 0.00003738
Iteration 26/1000 | Loss: 0.00003739
Iteration 27/1000 | Loss: 0.00003526
Iteration 28/1000 | Loss: 0.00004452
Iteration 29/1000 | Loss: 0.00003667
Iteration 30/1000 | Loss: 0.00007155
Iteration 31/1000 | Loss: 0.00002066
Iteration 32/1000 | Loss: 0.00002551
Iteration 33/1000 | Loss: 0.00001656
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001209
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001801
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00001180
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00002643
Iteration 52/1000 | Loss: 0.00009422
Iteration 53/1000 | Loss: 0.00001308
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001162
Iteration 69/1000 | Loss: 0.00001162
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001161
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001160
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001159
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001159
Iteration 94/1000 | Loss: 0.00001159
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001158
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001154
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001152
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001152
Iteration 121/1000 | Loss: 0.00001152
Iteration 122/1000 | Loss: 0.00001152
Iteration 123/1000 | Loss: 0.00001152
Iteration 124/1000 | Loss: 0.00001152
Iteration 125/1000 | Loss: 0.00001152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.1517055099830031e-05, 1.1517055099830031e-05, 1.1517055099830031e-05, 1.1517055099830031e-05, 1.1517055099830031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1517055099830031e-05

Optimization complete. Final v2v error: 2.9369421005249023 mm

Highest mean error: 4.164337158203125 mm for frame 119

Lowest mean error: 2.757025718688965 mm for frame 47

Saving results

Total time: 87.97421169281006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00691321
Iteration 2/25 | Loss: 0.00160676
Iteration 3/25 | Loss: 0.00145834
Iteration 4/25 | Loss: 0.00136758
Iteration 5/25 | Loss: 0.00132138
Iteration 6/25 | Loss: 0.00130873
Iteration 7/25 | Loss: 0.00130552
Iteration 8/25 | Loss: 0.00130338
Iteration 9/25 | Loss: 0.00129924
Iteration 10/25 | Loss: 0.00129909
Iteration 11/25 | Loss: 0.00129924
Iteration 12/25 | Loss: 0.00129871
Iteration 13/25 | Loss: 0.00129870
Iteration 14/25 | Loss: 0.00129870
Iteration 15/25 | Loss: 0.00129870
Iteration 16/25 | Loss: 0.00129870
Iteration 17/25 | Loss: 0.00129870
Iteration 18/25 | Loss: 0.00129870
Iteration 19/25 | Loss: 0.00129870
Iteration 20/25 | Loss: 0.00129870
Iteration 21/25 | Loss: 0.00129870
Iteration 22/25 | Loss: 0.00129870
Iteration 23/25 | Loss: 0.00129869
Iteration 24/25 | Loss: 0.00129869
Iteration 25/25 | Loss: 0.00129869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71277750
Iteration 2/25 | Loss: 0.00102950
Iteration 3/25 | Loss: 0.00101880
Iteration 4/25 | Loss: 0.00101879
Iteration 5/25 | Loss: 0.00101879
Iteration 6/25 | Loss: 0.00101879
Iteration 7/25 | Loss: 0.00101879
Iteration 8/25 | Loss: 0.00101879
Iteration 9/25 | Loss: 0.00101879
Iteration 10/25 | Loss: 0.00101879
Iteration 11/25 | Loss: 0.00101879
Iteration 12/25 | Loss: 0.00101879
Iteration 13/25 | Loss: 0.00101879
Iteration 14/25 | Loss: 0.00101879
Iteration 15/25 | Loss: 0.00101879
Iteration 16/25 | Loss: 0.00101879
Iteration 17/25 | Loss: 0.00101879
Iteration 18/25 | Loss: 0.00101879
Iteration 19/25 | Loss: 0.00101879
Iteration 20/25 | Loss: 0.00101879
Iteration 21/25 | Loss: 0.00101879
Iteration 22/25 | Loss: 0.00101879
Iteration 23/25 | Loss: 0.00101879
Iteration 24/25 | Loss: 0.00101879
Iteration 25/25 | Loss: 0.00101879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101879
Iteration 2/1000 | Loss: 0.00004588
Iteration 3/1000 | Loss: 0.00001940
Iteration 4/1000 | Loss: 0.00003658
Iteration 5/1000 | Loss: 0.00002206
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00004228
Iteration 8/1000 | Loss: 0.00001643
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001567
Iteration 12/1000 | Loss: 0.00001555
Iteration 13/1000 | Loss: 0.00001549
Iteration 14/1000 | Loss: 0.00001547
Iteration 15/1000 | Loss: 0.00001545
Iteration 16/1000 | Loss: 0.00001535
Iteration 17/1000 | Loss: 0.00001531
Iteration 18/1000 | Loss: 0.00001530
Iteration 19/1000 | Loss: 0.00004160
Iteration 20/1000 | Loss: 0.00001518
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001511
Iteration 25/1000 | Loss: 0.00001511
Iteration 26/1000 | Loss: 0.00001510
Iteration 27/1000 | Loss: 0.00001510
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001509
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001507
Iteration 37/1000 | Loss: 0.00001507
Iteration 38/1000 | Loss: 0.00001507
Iteration 39/1000 | Loss: 0.00001507
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001506
Iteration 42/1000 | Loss: 0.00001505
Iteration 43/1000 | Loss: 0.00001505
Iteration 44/1000 | Loss: 0.00001505
Iteration 45/1000 | Loss: 0.00001504
Iteration 46/1000 | Loss: 0.00001504
Iteration 47/1000 | Loss: 0.00001503
Iteration 48/1000 | Loss: 0.00001503
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001502
Iteration 52/1000 | Loss: 0.00001500
Iteration 53/1000 | Loss: 0.00001500
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001499
Iteration 56/1000 | Loss: 0.00001498
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001497
Iteration 60/1000 | Loss: 0.00001496
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001495
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001495
Iteration 65/1000 | Loss: 0.00001495
Iteration 66/1000 | Loss: 0.00001495
Iteration 67/1000 | Loss: 0.00001495
Iteration 68/1000 | Loss: 0.00001495
Iteration 69/1000 | Loss: 0.00001495
Iteration 70/1000 | Loss: 0.00001494
Iteration 71/1000 | Loss: 0.00001494
Iteration 72/1000 | Loss: 0.00001494
Iteration 73/1000 | Loss: 0.00001493
Iteration 74/1000 | Loss: 0.00001493
Iteration 75/1000 | Loss: 0.00001493
Iteration 76/1000 | Loss: 0.00001493
Iteration 77/1000 | Loss: 0.00001492
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001491
Iteration 81/1000 | Loss: 0.00001491
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001489
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001488
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001486
Iteration 100/1000 | Loss: 0.00001486
Iteration 101/1000 | Loss: 0.00001486
Iteration 102/1000 | Loss: 0.00001486
Iteration 103/1000 | Loss: 0.00001486
Iteration 104/1000 | Loss: 0.00001486
Iteration 105/1000 | Loss: 0.00001486
Iteration 106/1000 | Loss: 0.00001486
Iteration 107/1000 | Loss: 0.00001486
Iteration 108/1000 | Loss: 0.00001486
Iteration 109/1000 | Loss: 0.00001486
Iteration 110/1000 | Loss: 0.00001486
Iteration 111/1000 | Loss: 0.00001485
Iteration 112/1000 | Loss: 0.00001485
Iteration 113/1000 | Loss: 0.00001485
Iteration 114/1000 | Loss: 0.00001485
Iteration 115/1000 | Loss: 0.00001485
Iteration 116/1000 | Loss: 0.00001485
Iteration 117/1000 | Loss: 0.00001484
Iteration 118/1000 | Loss: 0.00001484
Iteration 119/1000 | Loss: 0.00001484
Iteration 120/1000 | Loss: 0.00001484
Iteration 121/1000 | Loss: 0.00001484
Iteration 122/1000 | Loss: 0.00001484
Iteration 123/1000 | Loss: 0.00001483
Iteration 124/1000 | Loss: 0.00001483
Iteration 125/1000 | Loss: 0.00001483
Iteration 126/1000 | Loss: 0.00001483
Iteration 127/1000 | Loss: 0.00001483
Iteration 128/1000 | Loss: 0.00001483
Iteration 129/1000 | Loss: 0.00001483
Iteration 130/1000 | Loss: 0.00001483
Iteration 131/1000 | Loss: 0.00001483
Iteration 132/1000 | Loss: 0.00001482
Iteration 133/1000 | Loss: 0.00001482
Iteration 134/1000 | Loss: 0.00001482
Iteration 135/1000 | Loss: 0.00001482
Iteration 136/1000 | Loss: 0.00001482
Iteration 137/1000 | Loss: 0.00001482
Iteration 138/1000 | Loss: 0.00001482
Iteration 139/1000 | Loss: 0.00001482
Iteration 140/1000 | Loss: 0.00001481
Iteration 141/1000 | Loss: 0.00001481
Iteration 142/1000 | Loss: 0.00001481
Iteration 143/1000 | Loss: 0.00001481
Iteration 144/1000 | Loss: 0.00001481
Iteration 145/1000 | Loss: 0.00001481
Iteration 146/1000 | Loss: 0.00001481
Iteration 147/1000 | Loss: 0.00001481
Iteration 148/1000 | Loss: 0.00001480
Iteration 149/1000 | Loss: 0.00001480
Iteration 150/1000 | Loss: 0.00001480
Iteration 151/1000 | Loss: 0.00001480
Iteration 152/1000 | Loss: 0.00001480
Iteration 153/1000 | Loss: 0.00001480
Iteration 154/1000 | Loss: 0.00001480
Iteration 155/1000 | Loss: 0.00001480
Iteration 156/1000 | Loss: 0.00001480
Iteration 157/1000 | Loss: 0.00001480
Iteration 158/1000 | Loss: 0.00001480
Iteration 159/1000 | Loss: 0.00001479
Iteration 160/1000 | Loss: 0.00001479
Iteration 161/1000 | Loss: 0.00001479
Iteration 162/1000 | Loss: 0.00001479
Iteration 163/1000 | Loss: 0.00001479
Iteration 164/1000 | Loss: 0.00001479
Iteration 165/1000 | Loss: 0.00001479
Iteration 166/1000 | Loss: 0.00001479
Iteration 167/1000 | Loss: 0.00001479
Iteration 168/1000 | Loss: 0.00001479
Iteration 169/1000 | Loss: 0.00001479
Iteration 170/1000 | Loss: 0.00001479
Iteration 171/1000 | Loss: 0.00001479
Iteration 172/1000 | Loss: 0.00001479
Iteration 173/1000 | Loss: 0.00001478
Iteration 174/1000 | Loss: 0.00001478
Iteration 175/1000 | Loss: 0.00001478
Iteration 176/1000 | Loss: 0.00001478
Iteration 177/1000 | Loss: 0.00001478
Iteration 178/1000 | Loss: 0.00001478
Iteration 179/1000 | Loss: 0.00001478
Iteration 180/1000 | Loss: 0.00001478
Iteration 181/1000 | Loss: 0.00001478
Iteration 182/1000 | Loss: 0.00001478
Iteration 183/1000 | Loss: 0.00001478
Iteration 184/1000 | Loss: 0.00001478
Iteration 185/1000 | Loss: 0.00001478
Iteration 186/1000 | Loss: 0.00001477
Iteration 187/1000 | Loss: 0.00001477
Iteration 188/1000 | Loss: 0.00001477
Iteration 189/1000 | Loss: 0.00001477
Iteration 190/1000 | Loss: 0.00001477
Iteration 191/1000 | Loss: 0.00001477
Iteration 192/1000 | Loss: 0.00001476
Iteration 193/1000 | Loss: 0.00001476
Iteration 194/1000 | Loss: 0.00001476
Iteration 195/1000 | Loss: 0.00001476
Iteration 196/1000 | Loss: 0.00001476
Iteration 197/1000 | Loss: 0.00001476
Iteration 198/1000 | Loss: 0.00001476
Iteration 199/1000 | Loss: 0.00001476
Iteration 200/1000 | Loss: 0.00001476
Iteration 201/1000 | Loss: 0.00001476
Iteration 202/1000 | Loss: 0.00001476
Iteration 203/1000 | Loss: 0.00001476
Iteration 204/1000 | Loss: 0.00001476
Iteration 205/1000 | Loss: 0.00001476
Iteration 206/1000 | Loss: 0.00001476
Iteration 207/1000 | Loss: 0.00001476
Iteration 208/1000 | Loss: 0.00001476
Iteration 209/1000 | Loss: 0.00001476
Iteration 210/1000 | Loss: 0.00001476
Iteration 211/1000 | Loss: 0.00001476
Iteration 212/1000 | Loss: 0.00001476
Iteration 213/1000 | Loss: 0.00001476
Iteration 214/1000 | Loss: 0.00001476
Iteration 215/1000 | Loss: 0.00001476
Iteration 216/1000 | Loss: 0.00001476
Iteration 217/1000 | Loss: 0.00001476
Iteration 218/1000 | Loss: 0.00001476
Iteration 219/1000 | Loss: 0.00001476
Iteration 220/1000 | Loss: 0.00001476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.476178385928506e-05, 1.476178385928506e-05, 1.476178385928506e-05, 1.476178385928506e-05, 1.476178385928506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.476178385928506e-05

Optimization complete. Final v2v error: 3.264047861099243 mm

Highest mean error: 3.7732338905334473 mm for frame 83

Lowest mean error: 2.9057047367095947 mm for frame 37

Saving results

Total time: 56.178250312805176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839522
Iteration 2/25 | Loss: 0.00152316
Iteration 3/25 | Loss: 0.00138559
Iteration 4/25 | Loss: 0.00136639
Iteration 5/25 | Loss: 0.00135908
Iteration 6/25 | Loss: 0.00135716
Iteration 7/25 | Loss: 0.00135675
Iteration 8/25 | Loss: 0.00135675
Iteration 9/25 | Loss: 0.00135675
Iteration 10/25 | Loss: 0.00135675
Iteration 11/25 | Loss: 0.00135675
Iteration 12/25 | Loss: 0.00135675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013567466521635652, 0.0013567466521635652, 0.0013567466521635652, 0.0013567466521635652, 0.0013567466521635652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013567466521635652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41301394
Iteration 2/25 | Loss: 0.00099175
Iteration 3/25 | Loss: 0.00099175
Iteration 4/25 | Loss: 0.00099175
Iteration 5/25 | Loss: 0.00099175
Iteration 6/25 | Loss: 0.00099175
Iteration 7/25 | Loss: 0.00099175
Iteration 8/25 | Loss: 0.00099175
Iteration 9/25 | Loss: 0.00099175
Iteration 10/25 | Loss: 0.00099175
Iteration 11/25 | Loss: 0.00099175
Iteration 12/25 | Loss: 0.00099175
Iteration 13/25 | Loss: 0.00099175
Iteration 14/25 | Loss: 0.00099175
Iteration 15/25 | Loss: 0.00099175
Iteration 16/25 | Loss: 0.00099175
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009917505085468292, 0.0009917505085468292, 0.0009917505085468292, 0.0009917505085468292, 0.0009917505085468292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009917505085468292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099175
Iteration 2/1000 | Loss: 0.00006239
Iteration 3/1000 | Loss: 0.00003950
Iteration 4/1000 | Loss: 0.00003010
Iteration 5/1000 | Loss: 0.00002634
Iteration 6/1000 | Loss: 0.00002490
Iteration 7/1000 | Loss: 0.00002363
Iteration 8/1000 | Loss: 0.00002302
Iteration 9/1000 | Loss: 0.00002245
Iteration 10/1000 | Loss: 0.00002188
Iteration 11/1000 | Loss: 0.00002156
Iteration 12/1000 | Loss: 0.00002133
Iteration 13/1000 | Loss: 0.00002107
Iteration 14/1000 | Loss: 0.00002089
Iteration 15/1000 | Loss: 0.00002070
Iteration 16/1000 | Loss: 0.00002066
Iteration 17/1000 | Loss: 0.00002063
Iteration 18/1000 | Loss: 0.00002062
Iteration 19/1000 | Loss: 0.00002062
Iteration 20/1000 | Loss: 0.00002061
Iteration 21/1000 | Loss: 0.00002054
Iteration 22/1000 | Loss: 0.00002052
Iteration 23/1000 | Loss: 0.00002045
Iteration 24/1000 | Loss: 0.00002043
Iteration 25/1000 | Loss: 0.00002043
Iteration 26/1000 | Loss: 0.00002041
Iteration 27/1000 | Loss: 0.00002038
Iteration 28/1000 | Loss: 0.00002037
Iteration 29/1000 | Loss: 0.00002028
Iteration 30/1000 | Loss: 0.00002025
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002022
Iteration 34/1000 | Loss: 0.00002022
Iteration 35/1000 | Loss: 0.00002022
Iteration 36/1000 | Loss: 0.00002022
Iteration 37/1000 | Loss: 0.00002022
Iteration 38/1000 | Loss: 0.00002022
Iteration 39/1000 | Loss: 0.00002022
Iteration 40/1000 | Loss: 0.00002022
Iteration 41/1000 | Loss: 0.00002021
Iteration 42/1000 | Loss: 0.00002021
Iteration 43/1000 | Loss: 0.00002020
Iteration 44/1000 | Loss: 0.00002019
Iteration 45/1000 | Loss: 0.00002019
Iteration 46/1000 | Loss: 0.00002019
Iteration 47/1000 | Loss: 0.00002019
Iteration 48/1000 | Loss: 0.00002018
Iteration 49/1000 | Loss: 0.00002018
Iteration 50/1000 | Loss: 0.00002018
Iteration 51/1000 | Loss: 0.00002018
Iteration 52/1000 | Loss: 0.00002018
Iteration 53/1000 | Loss: 0.00002018
Iteration 54/1000 | Loss: 0.00002018
Iteration 55/1000 | Loss: 0.00002018
Iteration 56/1000 | Loss: 0.00002018
Iteration 57/1000 | Loss: 0.00002017
Iteration 58/1000 | Loss: 0.00002017
Iteration 59/1000 | Loss: 0.00002017
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00002017
Iteration 62/1000 | Loss: 0.00002016
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00002015
Iteration 65/1000 | Loss: 0.00002015
Iteration 66/1000 | Loss: 0.00002015
Iteration 67/1000 | Loss: 0.00002015
Iteration 68/1000 | Loss: 0.00002014
Iteration 69/1000 | Loss: 0.00002014
Iteration 70/1000 | Loss: 0.00002014
Iteration 71/1000 | Loss: 0.00002014
Iteration 72/1000 | Loss: 0.00002014
Iteration 73/1000 | Loss: 0.00002013
Iteration 74/1000 | Loss: 0.00002013
Iteration 75/1000 | Loss: 0.00002013
Iteration 76/1000 | Loss: 0.00002013
Iteration 77/1000 | Loss: 0.00002013
Iteration 78/1000 | Loss: 0.00002013
Iteration 79/1000 | Loss: 0.00002013
Iteration 80/1000 | Loss: 0.00002013
Iteration 81/1000 | Loss: 0.00002012
Iteration 82/1000 | Loss: 0.00002012
Iteration 83/1000 | Loss: 0.00002012
Iteration 84/1000 | Loss: 0.00002012
Iteration 85/1000 | Loss: 0.00002012
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002011
Iteration 89/1000 | Loss: 0.00002011
Iteration 90/1000 | Loss: 0.00002011
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002010
Iteration 93/1000 | Loss: 0.00002010
Iteration 94/1000 | Loss: 0.00002010
Iteration 95/1000 | Loss: 0.00002010
Iteration 96/1000 | Loss: 0.00002010
Iteration 97/1000 | Loss: 0.00002010
Iteration 98/1000 | Loss: 0.00002009
Iteration 99/1000 | Loss: 0.00002009
Iteration 100/1000 | Loss: 0.00002009
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00002008
Iteration 105/1000 | Loss: 0.00002008
Iteration 106/1000 | Loss: 0.00002008
Iteration 107/1000 | Loss: 0.00002008
Iteration 108/1000 | Loss: 0.00002008
Iteration 109/1000 | Loss: 0.00002007
Iteration 110/1000 | Loss: 0.00002007
Iteration 111/1000 | Loss: 0.00002007
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002006
Iteration 114/1000 | Loss: 0.00002006
Iteration 115/1000 | Loss: 0.00002006
Iteration 116/1000 | Loss: 0.00002005
Iteration 117/1000 | Loss: 0.00002005
Iteration 118/1000 | Loss: 0.00002005
Iteration 119/1000 | Loss: 0.00002005
Iteration 120/1000 | Loss: 0.00002005
Iteration 121/1000 | Loss: 0.00002004
Iteration 122/1000 | Loss: 0.00002004
Iteration 123/1000 | Loss: 0.00002004
Iteration 124/1000 | Loss: 0.00002004
Iteration 125/1000 | Loss: 0.00002004
Iteration 126/1000 | Loss: 0.00002003
Iteration 127/1000 | Loss: 0.00002003
Iteration 128/1000 | Loss: 0.00002003
Iteration 129/1000 | Loss: 0.00002003
Iteration 130/1000 | Loss: 0.00002003
Iteration 131/1000 | Loss: 0.00002003
Iteration 132/1000 | Loss: 0.00002003
Iteration 133/1000 | Loss: 0.00002003
Iteration 134/1000 | Loss: 0.00002003
Iteration 135/1000 | Loss: 0.00002002
Iteration 136/1000 | Loss: 0.00002002
Iteration 137/1000 | Loss: 0.00002002
Iteration 138/1000 | Loss: 0.00002002
Iteration 139/1000 | Loss: 0.00002002
Iteration 140/1000 | Loss: 0.00002002
Iteration 141/1000 | Loss: 0.00002002
Iteration 142/1000 | Loss: 0.00002001
Iteration 143/1000 | Loss: 0.00002001
Iteration 144/1000 | Loss: 0.00002001
Iteration 145/1000 | Loss: 0.00002001
Iteration 146/1000 | Loss: 0.00002001
Iteration 147/1000 | Loss: 0.00002001
Iteration 148/1000 | Loss: 0.00002001
Iteration 149/1000 | Loss: 0.00002000
Iteration 150/1000 | Loss: 0.00002000
Iteration 151/1000 | Loss: 0.00002000
Iteration 152/1000 | Loss: 0.00002000
Iteration 153/1000 | Loss: 0.00002000
Iteration 154/1000 | Loss: 0.00002000
Iteration 155/1000 | Loss: 0.00002000
Iteration 156/1000 | Loss: 0.00002000
Iteration 157/1000 | Loss: 0.00001999
Iteration 158/1000 | Loss: 0.00001999
Iteration 159/1000 | Loss: 0.00001999
Iteration 160/1000 | Loss: 0.00001999
Iteration 161/1000 | Loss: 0.00001999
Iteration 162/1000 | Loss: 0.00001999
Iteration 163/1000 | Loss: 0.00001999
Iteration 164/1000 | Loss: 0.00001999
Iteration 165/1000 | Loss: 0.00001998
Iteration 166/1000 | Loss: 0.00001998
Iteration 167/1000 | Loss: 0.00001998
Iteration 168/1000 | Loss: 0.00001998
Iteration 169/1000 | Loss: 0.00001998
Iteration 170/1000 | Loss: 0.00001998
Iteration 171/1000 | Loss: 0.00001998
Iteration 172/1000 | Loss: 0.00001998
Iteration 173/1000 | Loss: 0.00001998
Iteration 174/1000 | Loss: 0.00001998
Iteration 175/1000 | Loss: 0.00001998
Iteration 176/1000 | Loss: 0.00001997
Iteration 177/1000 | Loss: 0.00001997
Iteration 178/1000 | Loss: 0.00001997
Iteration 179/1000 | Loss: 0.00001996
Iteration 180/1000 | Loss: 0.00001996
Iteration 181/1000 | Loss: 0.00001996
Iteration 182/1000 | Loss: 0.00001996
Iteration 183/1000 | Loss: 0.00001996
Iteration 184/1000 | Loss: 0.00001996
Iteration 185/1000 | Loss: 0.00001995
Iteration 186/1000 | Loss: 0.00001995
Iteration 187/1000 | Loss: 0.00001995
Iteration 188/1000 | Loss: 0.00001995
Iteration 189/1000 | Loss: 0.00001995
Iteration 190/1000 | Loss: 0.00001995
Iteration 191/1000 | Loss: 0.00001995
Iteration 192/1000 | Loss: 0.00001995
Iteration 193/1000 | Loss: 0.00001994
Iteration 194/1000 | Loss: 0.00001994
Iteration 195/1000 | Loss: 0.00001994
Iteration 196/1000 | Loss: 0.00001994
Iteration 197/1000 | Loss: 0.00001994
Iteration 198/1000 | Loss: 0.00001994
Iteration 199/1000 | Loss: 0.00001994
Iteration 200/1000 | Loss: 0.00001994
Iteration 201/1000 | Loss: 0.00001994
Iteration 202/1000 | Loss: 0.00001994
Iteration 203/1000 | Loss: 0.00001994
Iteration 204/1000 | Loss: 0.00001993
Iteration 205/1000 | Loss: 0.00001993
Iteration 206/1000 | Loss: 0.00001993
Iteration 207/1000 | Loss: 0.00001993
Iteration 208/1000 | Loss: 0.00001993
Iteration 209/1000 | Loss: 0.00001993
Iteration 210/1000 | Loss: 0.00001993
Iteration 211/1000 | Loss: 0.00001993
Iteration 212/1000 | Loss: 0.00001993
Iteration 213/1000 | Loss: 0.00001993
Iteration 214/1000 | Loss: 0.00001993
Iteration 215/1000 | Loss: 0.00001993
Iteration 216/1000 | Loss: 0.00001993
Iteration 217/1000 | Loss: 0.00001993
Iteration 218/1000 | Loss: 0.00001992
Iteration 219/1000 | Loss: 0.00001992
Iteration 220/1000 | Loss: 0.00001992
Iteration 221/1000 | Loss: 0.00001992
Iteration 222/1000 | Loss: 0.00001992
Iteration 223/1000 | Loss: 0.00001992
Iteration 224/1000 | Loss: 0.00001992
Iteration 225/1000 | Loss: 0.00001992
Iteration 226/1000 | Loss: 0.00001992
Iteration 227/1000 | Loss: 0.00001992
Iteration 228/1000 | Loss: 0.00001992
Iteration 229/1000 | Loss: 0.00001992
Iteration 230/1000 | Loss: 0.00001991
Iteration 231/1000 | Loss: 0.00001991
Iteration 232/1000 | Loss: 0.00001991
Iteration 233/1000 | Loss: 0.00001991
Iteration 234/1000 | Loss: 0.00001991
Iteration 235/1000 | Loss: 0.00001991
Iteration 236/1000 | Loss: 0.00001991
Iteration 237/1000 | Loss: 0.00001991
Iteration 238/1000 | Loss: 0.00001991
Iteration 239/1000 | Loss: 0.00001991
Iteration 240/1000 | Loss: 0.00001991
Iteration 241/1000 | Loss: 0.00001991
Iteration 242/1000 | Loss: 0.00001991
Iteration 243/1000 | Loss: 0.00001990
Iteration 244/1000 | Loss: 0.00001990
Iteration 245/1000 | Loss: 0.00001990
Iteration 246/1000 | Loss: 0.00001990
Iteration 247/1000 | Loss: 0.00001990
Iteration 248/1000 | Loss: 0.00001990
Iteration 249/1000 | Loss: 0.00001990
Iteration 250/1000 | Loss: 0.00001990
Iteration 251/1000 | Loss: 0.00001989
Iteration 252/1000 | Loss: 0.00001989
Iteration 253/1000 | Loss: 0.00001989
Iteration 254/1000 | Loss: 0.00001989
Iteration 255/1000 | Loss: 0.00001989
Iteration 256/1000 | Loss: 0.00001989
Iteration 257/1000 | Loss: 0.00001989
Iteration 258/1000 | Loss: 0.00001989
Iteration 259/1000 | Loss: 0.00001989
Iteration 260/1000 | Loss: 0.00001989
Iteration 261/1000 | Loss: 0.00001989
Iteration 262/1000 | Loss: 0.00001989
Iteration 263/1000 | Loss: 0.00001989
Iteration 264/1000 | Loss: 0.00001989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.98942707356764e-05, 1.98942707356764e-05, 1.98942707356764e-05, 1.98942707356764e-05, 1.98942707356764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.98942707356764e-05

Optimization complete. Final v2v error: 3.7066497802734375 mm

Highest mean error: 5.844699382781982 mm for frame 70

Lowest mean error: 2.980816602706909 mm for frame 127

Saving results

Total time: 48.388548851013184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527182
Iteration 2/25 | Loss: 0.00151869
Iteration 3/25 | Loss: 0.00133997
Iteration 4/25 | Loss: 0.00132254
Iteration 5/25 | Loss: 0.00131935
Iteration 6/25 | Loss: 0.00131884
Iteration 7/25 | Loss: 0.00131884
Iteration 8/25 | Loss: 0.00131884
Iteration 9/25 | Loss: 0.00131884
Iteration 10/25 | Loss: 0.00131884
Iteration 11/25 | Loss: 0.00131884
Iteration 12/25 | Loss: 0.00131884
Iteration 13/25 | Loss: 0.00131884
Iteration 14/25 | Loss: 0.00131884
Iteration 15/25 | Loss: 0.00131884
Iteration 16/25 | Loss: 0.00131884
Iteration 17/25 | Loss: 0.00131884
Iteration 18/25 | Loss: 0.00131884
Iteration 19/25 | Loss: 0.00131884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013188355369493365, 0.0013188355369493365, 0.0013188355369493365, 0.0013188355369493365, 0.0013188355369493365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013188355369493365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60946465
Iteration 2/25 | Loss: 0.00077094
Iteration 3/25 | Loss: 0.00077093
Iteration 4/25 | Loss: 0.00077093
Iteration 5/25 | Loss: 0.00077093
Iteration 6/25 | Loss: 0.00077093
Iteration 7/25 | Loss: 0.00077093
Iteration 8/25 | Loss: 0.00077093
Iteration 9/25 | Loss: 0.00077093
Iteration 10/25 | Loss: 0.00077093
Iteration 11/25 | Loss: 0.00077093
Iteration 12/25 | Loss: 0.00077093
Iteration 13/25 | Loss: 0.00077093
Iteration 14/25 | Loss: 0.00077093
Iteration 15/25 | Loss: 0.00077093
Iteration 16/25 | Loss: 0.00077093
Iteration 17/25 | Loss: 0.00077093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000770926708355546, 0.000770926708355546, 0.000770926708355546, 0.000770926708355546, 0.000770926708355546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000770926708355546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077093
Iteration 2/1000 | Loss: 0.00003207
Iteration 3/1000 | Loss: 0.00002102
Iteration 4/1000 | Loss: 0.00001907
Iteration 5/1000 | Loss: 0.00001796
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001647
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001557
Iteration 12/1000 | Loss: 0.00001542
Iteration 13/1000 | Loss: 0.00001523
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001497
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001476
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001465
Iteration 28/1000 | Loss: 0.00001463
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001461
Iteration 31/1000 | Loss: 0.00001461
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001460
Iteration 35/1000 | Loss: 0.00001460
Iteration 36/1000 | Loss: 0.00001460
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001456
Iteration 47/1000 | Loss: 0.00001456
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001455
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001455
Iteration 52/1000 | Loss: 0.00001454
Iteration 53/1000 | Loss: 0.00001454
Iteration 54/1000 | Loss: 0.00001454
Iteration 55/1000 | Loss: 0.00001453
Iteration 56/1000 | Loss: 0.00001453
Iteration 57/1000 | Loss: 0.00001452
Iteration 58/1000 | Loss: 0.00001452
Iteration 59/1000 | Loss: 0.00001451
Iteration 60/1000 | Loss: 0.00001451
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001447
Iteration 67/1000 | Loss: 0.00001447
Iteration 68/1000 | Loss: 0.00001447
Iteration 69/1000 | Loss: 0.00001447
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001447
Iteration 77/1000 | Loss: 0.00001447
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001446
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001444
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001443
Iteration 91/1000 | Loss: 0.00001443
Iteration 92/1000 | Loss: 0.00001443
Iteration 93/1000 | Loss: 0.00001443
Iteration 94/1000 | Loss: 0.00001442
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001442
Iteration 97/1000 | Loss: 0.00001442
Iteration 98/1000 | Loss: 0.00001441
Iteration 99/1000 | Loss: 0.00001441
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001439
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001439
Iteration 127/1000 | Loss: 0.00001439
Iteration 128/1000 | Loss: 0.00001439
Iteration 129/1000 | Loss: 0.00001439
Iteration 130/1000 | Loss: 0.00001439
Iteration 131/1000 | Loss: 0.00001439
Iteration 132/1000 | Loss: 0.00001439
Iteration 133/1000 | Loss: 0.00001439
Iteration 134/1000 | Loss: 0.00001439
Iteration 135/1000 | Loss: 0.00001439
Iteration 136/1000 | Loss: 0.00001439
Iteration 137/1000 | Loss: 0.00001439
Iteration 138/1000 | Loss: 0.00001439
Iteration 139/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.4392675439012237e-05, 1.4392675439012237e-05, 1.4392675439012237e-05, 1.4392675439012237e-05, 1.4392675439012237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4392675439012237e-05

Optimization complete. Final v2v error: 3.217329978942871 mm

Highest mean error: 3.5258381366729736 mm for frame 12

Lowest mean error: 2.8628089427948 mm for frame 104

Saving results

Total time: 44.78253698348999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795214
Iteration 2/25 | Loss: 0.00149589
Iteration 3/25 | Loss: 0.00132337
Iteration 4/25 | Loss: 0.00129781
Iteration 5/25 | Loss: 0.00129170
Iteration 6/25 | Loss: 0.00129041
Iteration 7/25 | Loss: 0.00129041
Iteration 8/25 | Loss: 0.00129041
Iteration 9/25 | Loss: 0.00129041
Iteration 10/25 | Loss: 0.00129041
Iteration 11/25 | Loss: 0.00129041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012904118048027158, 0.0012904118048027158, 0.0012904118048027158, 0.0012904118048027158, 0.0012904118048027158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012904118048027158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42667091
Iteration 2/25 | Loss: 0.00096753
Iteration 3/25 | Loss: 0.00096753
Iteration 4/25 | Loss: 0.00096753
Iteration 5/25 | Loss: 0.00096753
Iteration 6/25 | Loss: 0.00096753
Iteration 7/25 | Loss: 0.00096753
Iteration 8/25 | Loss: 0.00096753
Iteration 9/25 | Loss: 0.00096753
Iteration 10/25 | Loss: 0.00096753
Iteration 11/25 | Loss: 0.00096753
Iteration 12/25 | Loss: 0.00096753
Iteration 13/25 | Loss: 0.00096753
Iteration 14/25 | Loss: 0.00096753
Iteration 15/25 | Loss: 0.00096753
Iteration 16/25 | Loss: 0.00096753
Iteration 17/25 | Loss: 0.00096753
Iteration 18/25 | Loss: 0.00096753
Iteration 19/25 | Loss: 0.00096753
Iteration 20/25 | Loss: 0.00096753
Iteration 21/25 | Loss: 0.00096753
Iteration 22/25 | Loss: 0.00096753
Iteration 23/25 | Loss: 0.00096753
Iteration 24/25 | Loss: 0.00096753
Iteration 25/25 | Loss: 0.00096753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096753
Iteration 2/1000 | Loss: 0.00004822
Iteration 3/1000 | Loss: 0.00003027
Iteration 4/1000 | Loss: 0.00002330
Iteration 5/1000 | Loss: 0.00002115
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001778
Iteration 9/1000 | Loss: 0.00001729
Iteration 10/1000 | Loss: 0.00001699
Iteration 11/1000 | Loss: 0.00001669
Iteration 12/1000 | Loss: 0.00001646
Iteration 13/1000 | Loss: 0.00001640
Iteration 14/1000 | Loss: 0.00001635
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001625
Iteration 17/1000 | Loss: 0.00001622
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001612
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00001607
Iteration 25/1000 | Loss: 0.00001606
Iteration 26/1000 | Loss: 0.00001601
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001600
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001598
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001596
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001595
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001594
Iteration 49/1000 | Loss: 0.00001594
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001593
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001592
Iteration 58/1000 | Loss: 0.00001592
Iteration 59/1000 | Loss: 0.00001592
Iteration 60/1000 | Loss: 0.00001592
Iteration 61/1000 | Loss: 0.00001592
Iteration 62/1000 | Loss: 0.00001592
Iteration 63/1000 | Loss: 0.00001592
Iteration 64/1000 | Loss: 0.00001591
Iteration 65/1000 | Loss: 0.00001591
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001590
Iteration 68/1000 | Loss: 0.00001590
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001590
Iteration 72/1000 | Loss: 0.00001590
Iteration 73/1000 | Loss: 0.00001590
Iteration 74/1000 | Loss: 0.00001590
Iteration 75/1000 | Loss: 0.00001590
Iteration 76/1000 | Loss: 0.00001590
Iteration 77/1000 | Loss: 0.00001590
Iteration 78/1000 | Loss: 0.00001590
Iteration 79/1000 | Loss: 0.00001590
Iteration 80/1000 | Loss: 0.00001590
Iteration 81/1000 | Loss: 0.00001590
Iteration 82/1000 | Loss: 0.00001590
Iteration 83/1000 | Loss: 0.00001590
Iteration 84/1000 | Loss: 0.00001590
Iteration 85/1000 | Loss: 0.00001590
Iteration 86/1000 | Loss: 0.00001590
Iteration 87/1000 | Loss: 0.00001590
Iteration 88/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.5902080122032203e-05, 1.5902080122032203e-05, 1.5902080122032203e-05, 1.5902080122032203e-05, 1.5902080122032203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5902080122032203e-05

Optimization complete. Final v2v error: 3.407092809677124 mm

Highest mean error: 3.861959934234619 mm for frame 79

Lowest mean error: 3.007399797439575 mm for frame 154

Saving results

Total time: 34.8121235370636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387180
Iteration 2/25 | Loss: 0.00134014
Iteration 3/25 | Loss: 0.00129894
Iteration 4/25 | Loss: 0.00129649
Iteration 5/25 | Loss: 0.00129649
Iteration 6/25 | Loss: 0.00129649
Iteration 7/25 | Loss: 0.00129649
Iteration 8/25 | Loss: 0.00129649
Iteration 9/25 | Loss: 0.00129649
Iteration 10/25 | Loss: 0.00129649
Iteration 11/25 | Loss: 0.00129649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012964893830940127, 0.0012964893830940127, 0.0012964893830940127, 0.0012964893830940127, 0.0012964893830940127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012964893830940127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55287206
Iteration 2/25 | Loss: 0.00078859
Iteration 3/25 | Loss: 0.00078854
Iteration 4/25 | Loss: 0.00078854
Iteration 5/25 | Loss: 0.00078854
Iteration 6/25 | Loss: 0.00078854
Iteration 7/25 | Loss: 0.00078854
Iteration 8/25 | Loss: 0.00078854
Iteration 9/25 | Loss: 0.00078854
Iteration 10/25 | Loss: 0.00078854
Iteration 11/25 | Loss: 0.00078854
Iteration 12/25 | Loss: 0.00078854
Iteration 13/25 | Loss: 0.00078854
Iteration 14/25 | Loss: 0.00078854
Iteration 15/25 | Loss: 0.00078854
Iteration 16/25 | Loss: 0.00078854
Iteration 17/25 | Loss: 0.00078854
Iteration 18/25 | Loss: 0.00078854
Iteration 19/25 | Loss: 0.00078854
Iteration 20/25 | Loss: 0.00078854
Iteration 21/25 | Loss: 0.00078854
Iteration 22/25 | Loss: 0.00078854
Iteration 23/25 | Loss: 0.00078854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007885366794653237, 0.0007885366794653237, 0.0007885366794653237, 0.0007885366794653237, 0.0007885366794653237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007885366794653237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078854
Iteration 2/1000 | Loss: 0.00002889
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001772
Iteration 5/1000 | Loss: 0.00001691
Iteration 6/1000 | Loss: 0.00001625
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001525
Iteration 10/1000 | Loss: 0.00001512
Iteration 11/1000 | Loss: 0.00001491
Iteration 12/1000 | Loss: 0.00001486
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001444
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001435
Iteration 17/1000 | Loss: 0.00001434
Iteration 18/1000 | Loss: 0.00001422
Iteration 19/1000 | Loss: 0.00001422
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001418
Iteration 22/1000 | Loss: 0.00001417
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001416
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001411
Iteration 27/1000 | Loss: 0.00001411
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001411
Iteration 30/1000 | Loss: 0.00001411
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001408
Iteration 34/1000 | Loss: 0.00001407
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001397
Iteration 43/1000 | Loss: 0.00001393
Iteration 44/1000 | Loss: 0.00001393
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001387
Iteration 50/1000 | Loss: 0.00001386
Iteration 51/1000 | Loss: 0.00001385
Iteration 52/1000 | Loss: 0.00001385
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001375
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001368
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001365
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001364
Iteration 79/1000 | Loss: 0.00001364
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001362
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001361
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001360
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001357
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001355
Iteration 104/1000 | Loss: 0.00001355
Iteration 105/1000 | Loss: 0.00001355
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001351
Iteration 114/1000 | Loss: 0.00001351
Iteration 115/1000 | Loss: 0.00001351
Iteration 116/1000 | Loss: 0.00001351
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001350
Iteration 122/1000 | Loss: 0.00001350
Iteration 123/1000 | Loss: 0.00001350
Iteration 124/1000 | Loss: 0.00001350
Iteration 125/1000 | Loss: 0.00001349
Iteration 126/1000 | Loss: 0.00001349
Iteration 127/1000 | Loss: 0.00001349
Iteration 128/1000 | Loss: 0.00001348
Iteration 129/1000 | Loss: 0.00001348
Iteration 130/1000 | Loss: 0.00001348
Iteration 131/1000 | Loss: 0.00001348
Iteration 132/1000 | Loss: 0.00001348
Iteration 133/1000 | Loss: 0.00001348
Iteration 134/1000 | Loss: 0.00001347
Iteration 135/1000 | Loss: 0.00001347
Iteration 136/1000 | Loss: 0.00001347
Iteration 137/1000 | Loss: 0.00001347
Iteration 138/1000 | Loss: 0.00001347
Iteration 139/1000 | Loss: 0.00001347
Iteration 140/1000 | Loss: 0.00001347
Iteration 141/1000 | Loss: 0.00001347
Iteration 142/1000 | Loss: 0.00001347
Iteration 143/1000 | Loss: 0.00001347
Iteration 144/1000 | Loss: 0.00001347
Iteration 145/1000 | Loss: 0.00001347
Iteration 146/1000 | Loss: 0.00001347
Iteration 147/1000 | Loss: 0.00001347
Iteration 148/1000 | Loss: 0.00001347
Iteration 149/1000 | Loss: 0.00001347
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001347
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001346
Iteration 154/1000 | Loss: 0.00001346
Iteration 155/1000 | Loss: 0.00001346
Iteration 156/1000 | Loss: 0.00001346
Iteration 157/1000 | Loss: 0.00001346
Iteration 158/1000 | Loss: 0.00001346
Iteration 159/1000 | Loss: 0.00001346
Iteration 160/1000 | Loss: 0.00001346
Iteration 161/1000 | Loss: 0.00001346
Iteration 162/1000 | Loss: 0.00001346
Iteration 163/1000 | Loss: 0.00001346
Iteration 164/1000 | Loss: 0.00001346
Iteration 165/1000 | Loss: 0.00001345
Iteration 166/1000 | Loss: 0.00001345
Iteration 167/1000 | Loss: 0.00001345
Iteration 168/1000 | Loss: 0.00001345
Iteration 169/1000 | Loss: 0.00001345
Iteration 170/1000 | Loss: 0.00001345
Iteration 171/1000 | Loss: 0.00001345
Iteration 172/1000 | Loss: 0.00001345
Iteration 173/1000 | Loss: 0.00001345
Iteration 174/1000 | Loss: 0.00001345
Iteration 175/1000 | Loss: 0.00001345
Iteration 176/1000 | Loss: 0.00001345
Iteration 177/1000 | Loss: 0.00001345
Iteration 178/1000 | Loss: 0.00001345
Iteration 179/1000 | Loss: 0.00001345
Iteration 180/1000 | Loss: 0.00001345
Iteration 181/1000 | Loss: 0.00001345
Iteration 182/1000 | Loss: 0.00001345
Iteration 183/1000 | Loss: 0.00001345
Iteration 184/1000 | Loss: 0.00001345
Iteration 185/1000 | Loss: 0.00001345
Iteration 186/1000 | Loss: 0.00001345
Iteration 187/1000 | Loss: 0.00001345
Iteration 188/1000 | Loss: 0.00001345
Iteration 189/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.3450953701976687e-05, 1.3450953701976687e-05, 1.3450953701976687e-05, 1.3450953701976687e-05, 1.3450953701976687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3450953701976687e-05

Optimization complete. Final v2v error: 3.11527419090271 mm

Highest mean error: 3.5027992725372314 mm for frame 129

Lowest mean error: 2.83180570602417 mm for frame 153

Saving results

Total time: 48.51066875457764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436845
Iteration 2/25 | Loss: 0.00141750
Iteration 3/25 | Loss: 0.00133689
Iteration 4/25 | Loss: 0.00132350
Iteration 5/25 | Loss: 0.00131883
Iteration 6/25 | Loss: 0.00131848
Iteration 7/25 | Loss: 0.00131848
Iteration 8/25 | Loss: 0.00131848
Iteration 9/25 | Loss: 0.00131848
Iteration 10/25 | Loss: 0.00131848
Iteration 11/25 | Loss: 0.00131848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013184804702177644, 0.0013184804702177644, 0.0013184804702177644, 0.0013184804702177644, 0.0013184804702177644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013184804702177644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81614029
Iteration 2/25 | Loss: 0.00090757
Iteration 3/25 | Loss: 0.00090757
Iteration 4/25 | Loss: 0.00090757
Iteration 5/25 | Loss: 0.00090757
Iteration 6/25 | Loss: 0.00090757
Iteration 7/25 | Loss: 0.00090757
Iteration 8/25 | Loss: 0.00090757
Iteration 9/25 | Loss: 0.00090757
Iteration 10/25 | Loss: 0.00090757
Iteration 11/25 | Loss: 0.00090757
Iteration 12/25 | Loss: 0.00090757
Iteration 13/25 | Loss: 0.00090757
Iteration 14/25 | Loss: 0.00090757
Iteration 15/25 | Loss: 0.00090757
Iteration 16/25 | Loss: 0.00090757
Iteration 17/25 | Loss: 0.00090757
Iteration 18/25 | Loss: 0.00090757
Iteration 19/25 | Loss: 0.00090757
Iteration 20/25 | Loss: 0.00090757
Iteration 21/25 | Loss: 0.00090757
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009075695998035371, 0.0009075695998035371, 0.0009075695998035371, 0.0009075695998035371, 0.0009075695998035371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009075695998035371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090757
Iteration 2/1000 | Loss: 0.00003815
Iteration 3/1000 | Loss: 0.00002615
Iteration 4/1000 | Loss: 0.00002340
Iteration 5/1000 | Loss: 0.00002234
Iteration 6/1000 | Loss: 0.00002174
Iteration 7/1000 | Loss: 0.00002123
Iteration 8/1000 | Loss: 0.00002081
Iteration 9/1000 | Loss: 0.00002050
Iteration 10/1000 | Loss: 0.00002012
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001963
Iteration 15/1000 | Loss: 0.00001951
Iteration 16/1000 | Loss: 0.00001944
Iteration 17/1000 | Loss: 0.00001941
Iteration 18/1000 | Loss: 0.00001939
Iteration 19/1000 | Loss: 0.00001939
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001937
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001929
Iteration 24/1000 | Loss: 0.00001928
Iteration 25/1000 | Loss: 0.00001922
Iteration 26/1000 | Loss: 0.00001922
Iteration 27/1000 | Loss: 0.00001922
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001917
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001915
Iteration 33/1000 | Loss: 0.00001915
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00001914
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001914
Iteration 38/1000 | Loss: 0.00001914
Iteration 39/1000 | Loss: 0.00001914
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001911
Iteration 42/1000 | Loss: 0.00001910
Iteration 43/1000 | Loss: 0.00001910
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001910
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001905
Iteration 54/1000 | Loss: 0.00001905
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001903
Iteration 59/1000 | Loss: 0.00001903
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001902
Iteration 62/1000 | Loss: 0.00001902
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001901
Iteration 65/1000 | Loss: 0.00001900
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001900
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001899
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001898
Iteration 76/1000 | Loss: 0.00001898
Iteration 77/1000 | Loss: 0.00001897
Iteration 78/1000 | Loss: 0.00001897
Iteration 79/1000 | Loss: 0.00001897
Iteration 80/1000 | Loss: 0.00001897
Iteration 81/1000 | Loss: 0.00001897
Iteration 82/1000 | Loss: 0.00001897
Iteration 83/1000 | Loss: 0.00001897
Iteration 84/1000 | Loss: 0.00001897
Iteration 85/1000 | Loss: 0.00001896
Iteration 86/1000 | Loss: 0.00001896
Iteration 87/1000 | Loss: 0.00001896
Iteration 88/1000 | Loss: 0.00001896
Iteration 89/1000 | Loss: 0.00001896
Iteration 90/1000 | Loss: 0.00001896
Iteration 91/1000 | Loss: 0.00001896
Iteration 92/1000 | Loss: 0.00001896
Iteration 93/1000 | Loss: 0.00001896
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001895
Iteration 100/1000 | Loss: 0.00001894
Iteration 101/1000 | Loss: 0.00001894
Iteration 102/1000 | Loss: 0.00001894
Iteration 103/1000 | Loss: 0.00001894
Iteration 104/1000 | Loss: 0.00001893
Iteration 105/1000 | Loss: 0.00001893
Iteration 106/1000 | Loss: 0.00001893
Iteration 107/1000 | Loss: 0.00001893
Iteration 108/1000 | Loss: 0.00001893
Iteration 109/1000 | Loss: 0.00001893
Iteration 110/1000 | Loss: 0.00001893
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001893
Iteration 113/1000 | Loss: 0.00001892
Iteration 114/1000 | Loss: 0.00001892
Iteration 115/1000 | Loss: 0.00001892
Iteration 116/1000 | Loss: 0.00001892
Iteration 117/1000 | Loss: 0.00001892
Iteration 118/1000 | Loss: 0.00001892
Iteration 119/1000 | Loss: 0.00001892
Iteration 120/1000 | Loss: 0.00001892
Iteration 121/1000 | Loss: 0.00001892
Iteration 122/1000 | Loss: 0.00001892
Iteration 123/1000 | Loss: 0.00001892
Iteration 124/1000 | Loss: 0.00001891
Iteration 125/1000 | Loss: 0.00001891
Iteration 126/1000 | Loss: 0.00001891
Iteration 127/1000 | Loss: 0.00001891
Iteration 128/1000 | Loss: 0.00001891
Iteration 129/1000 | Loss: 0.00001890
Iteration 130/1000 | Loss: 0.00001890
Iteration 131/1000 | Loss: 0.00001890
Iteration 132/1000 | Loss: 0.00001890
Iteration 133/1000 | Loss: 0.00001890
Iteration 134/1000 | Loss: 0.00001889
Iteration 135/1000 | Loss: 0.00001889
Iteration 136/1000 | Loss: 0.00001889
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001889
Iteration 141/1000 | Loss: 0.00001889
Iteration 142/1000 | Loss: 0.00001889
Iteration 143/1000 | Loss: 0.00001889
Iteration 144/1000 | Loss: 0.00001889
Iteration 145/1000 | Loss: 0.00001889
Iteration 146/1000 | Loss: 0.00001889
Iteration 147/1000 | Loss: 0.00001889
Iteration 148/1000 | Loss: 0.00001889
Iteration 149/1000 | Loss: 0.00001889
Iteration 150/1000 | Loss: 0.00001888
Iteration 151/1000 | Loss: 0.00001888
Iteration 152/1000 | Loss: 0.00001888
Iteration 153/1000 | Loss: 0.00001888
Iteration 154/1000 | Loss: 0.00001888
Iteration 155/1000 | Loss: 0.00001888
Iteration 156/1000 | Loss: 0.00001888
Iteration 157/1000 | Loss: 0.00001888
Iteration 158/1000 | Loss: 0.00001888
Iteration 159/1000 | Loss: 0.00001888
Iteration 160/1000 | Loss: 0.00001888
Iteration 161/1000 | Loss: 0.00001888
Iteration 162/1000 | Loss: 0.00001888
Iteration 163/1000 | Loss: 0.00001888
Iteration 164/1000 | Loss: 0.00001888
Iteration 165/1000 | Loss: 0.00001888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.8880762581829913e-05, 1.8880762581829913e-05, 1.8880762581829913e-05, 1.8880762581829913e-05, 1.8880762581829913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8880762581829913e-05

Optimization complete. Final v2v error: 3.6961801052093506 mm

Highest mean error: 3.9586591720581055 mm for frame 30

Lowest mean error: 3.4708945751190186 mm for frame 21

Saving results

Total time: 39.0883584022522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00727037
Iteration 2/25 | Loss: 0.00192728
Iteration 3/25 | Loss: 0.00158563
Iteration 4/25 | Loss: 0.00149002
Iteration 5/25 | Loss: 0.00150382
Iteration 6/25 | Loss: 0.00148836
Iteration 7/25 | Loss: 0.00140617
Iteration 8/25 | Loss: 0.00140124
Iteration 9/25 | Loss: 0.00138863
Iteration 10/25 | Loss: 0.00138181
Iteration 11/25 | Loss: 0.00138060
Iteration 12/25 | Loss: 0.00137497
Iteration 13/25 | Loss: 0.00136598
Iteration 14/25 | Loss: 0.00136524
Iteration 15/25 | Loss: 0.00136840
Iteration 16/25 | Loss: 0.00136741
Iteration 17/25 | Loss: 0.00136883
Iteration 18/25 | Loss: 0.00136554
Iteration 19/25 | Loss: 0.00136343
Iteration 20/25 | Loss: 0.00136163
Iteration 21/25 | Loss: 0.00136128
Iteration 22/25 | Loss: 0.00136119
Iteration 23/25 | Loss: 0.00136119
Iteration 24/25 | Loss: 0.00136119
Iteration 25/25 | Loss: 0.00136119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.13396025
Iteration 2/25 | Loss: 0.00105665
Iteration 3/25 | Loss: 0.00105658
Iteration 4/25 | Loss: 0.00105657
Iteration 5/25 | Loss: 0.00105657
Iteration 6/25 | Loss: 0.00105657
Iteration 7/25 | Loss: 0.00105657
Iteration 8/25 | Loss: 0.00105657
Iteration 9/25 | Loss: 0.00105657
Iteration 10/25 | Loss: 0.00105657
Iteration 11/25 | Loss: 0.00105657
Iteration 12/25 | Loss: 0.00105657
Iteration 13/25 | Loss: 0.00105657
Iteration 14/25 | Loss: 0.00105657
Iteration 15/25 | Loss: 0.00105657
Iteration 16/25 | Loss: 0.00105657
Iteration 17/25 | Loss: 0.00105657
Iteration 18/25 | Loss: 0.00105657
Iteration 19/25 | Loss: 0.00105657
Iteration 20/25 | Loss: 0.00105657
Iteration 21/25 | Loss: 0.00105657
Iteration 22/25 | Loss: 0.00105657
Iteration 23/25 | Loss: 0.00105657
Iteration 24/25 | Loss: 0.00105657
Iteration 25/25 | Loss: 0.00105657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010565724223852158, 0.0010565724223852158, 0.0010565724223852158, 0.0010565724223852158, 0.0010565724223852158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010565724223852158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105657
Iteration 2/1000 | Loss: 0.00007129
Iteration 3/1000 | Loss: 0.00004585
Iteration 4/1000 | Loss: 0.00003617
Iteration 5/1000 | Loss: 0.00003277
Iteration 6/1000 | Loss: 0.00003043
Iteration 7/1000 | Loss: 0.00004292
Iteration 8/1000 | Loss: 0.00003077
Iteration 9/1000 | Loss: 0.00002920
Iteration 10/1000 | Loss: 0.00002805
Iteration 11/1000 | Loss: 0.00002678
Iteration 12/1000 | Loss: 0.00002571
Iteration 13/1000 | Loss: 0.00002534
Iteration 14/1000 | Loss: 0.00002503
Iteration 15/1000 | Loss: 0.00002500
Iteration 16/1000 | Loss: 0.00002484
Iteration 17/1000 | Loss: 0.00002470
Iteration 18/1000 | Loss: 0.00002469
Iteration 19/1000 | Loss: 0.00002467
Iteration 20/1000 | Loss: 0.00002466
Iteration 21/1000 | Loss: 0.00002465
Iteration 22/1000 | Loss: 0.00002464
Iteration 23/1000 | Loss: 0.00002462
Iteration 24/1000 | Loss: 0.00002459
Iteration 25/1000 | Loss: 0.00002458
Iteration 26/1000 | Loss: 0.00002451
Iteration 27/1000 | Loss: 0.00002444
Iteration 28/1000 | Loss: 0.00033705
Iteration 29/1000 | Loss: 0.00018698
Iteration 30/1000 | Loss: 0.00003762
Iteration 31/1000 | Loss: 0.00002758
Iteration 32/1000 | Loss: 0.00002556
Iteration 33/1000 | Loss: 0.00020791
Iteration 34/1000 | Loss: 0.00003156
Iteration 35/1000 | Loss: 0.00002705
Iteration 36/1000 | Loss: 0.00002523
Iteration 37/1000 | Loss: 0.00002427
Iteration 38/1000 | Loss: 0.00002337
Iteration 39/1000 | Loss: 0.00002273
Iteration 40/1000 | Loss: 0.00002232
Iteration 41/1000 | Loss: 0.00002213
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002192
Iteration 44/1000 | Loss: 0.00002191
Iteration 45/1000 | Loss: 0.00002176
Iteration 46/1000 | Loss: 0.00002173
Iteration 47/1000 | Loss: 0.00002170
Iteration 48/1000 | Loss: 0.00002165
Iteration 49/1000 | Loss: 0.00002165
Iteration 50/1000 | Loss: 0.00002164
Iteration 51/1000 | Loss: 0.00002163
Iteration 52/1000 | Loss: 0.00002163
Iteration 53/1000 | Loss: 0.00002162
Iteration 54/1000 | Loss: 0.00002162
Iteration 55/1000 | Loss: 0.00002161
Iteration 56/1000 | Loss: 0.00002161
Iteration 57/1000 | Loss: 0.00002160
Iteration 58/1000 | Loss: 0.00002160
Iteration 59/1000 | Loss: 0.00002159
Iteration 60/1000 | Loss: 0.00002159
Iteration 61/1000 | Loss: 0.00002158
Iteration 62/1000 | Loss: 0.00002158
Iteration 63/1000 | Loss: 0.00002158
Iteration 64/1000 | Loss: 0.00002157
Iteration 65/1000 | Loss: 0.00002157
Iteration 66/1000 | Loss: 0.00002156
Iteration 67/1000 | Loss: 0.00002156
Iteration 68/1000 | Loss: 0.00002155
Iteration 69/1000 | Loss: 0.00002155
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002154
Iteration 72/1000 | Loss: 0.00002154
Iteration 73/1000 | Loss: 0.00002154
Iteration 74/1000 | Loss: 0.00002153
Iteration 75/1000 | Loss: 0.00002153
Iteration 76/1000 | Loss: 0.00002152
Iteration 77/1000 | Loss: 0.00002152
Iteration 78/1000 | Loss: 0.00002152
Iteration 79/1000 | Loss: 0.00002151
Iteration 80/1000 | Loss: 0.00002151
Iteration 81/1000 | Loss: 0.00002151
Iteration 82/1000 | Loss: 0.00002150
Iteration 83/1000 | Loss: 0.00002150
Iteration 84/1000 | Loss: 0.00002149
Iteration 85/1000 | Loss: 0.00002149
Iteration 86/1000 | Loss: 0.00002148
Iteration 87/1000 | Loss: 0.00002148
Iteration 88/1000 | Loss: 0.00002148
Iteration 89/1000 | Loss: 0.00002147
Iteration 90/1000 | Loss: 0.00002147
Iteration 91/1000 | Loss: 0.00002147
Iteration 92/1000 | Loss: 0.00002146
Iteration 93/1000 | Loss: 0.00002146
Iteration 94/1000 | Loss: 0.00002146
Iteration 95/1000 | Loss: 0.00002146
Iteration 96/1000 | Loss: 0.00002145
Iteration 97/1000 | Loss: 0.00002145
Iteration 98/1000 | Loss: 0.00002145
Iteration 99/1000 | Loss: 0.00002145
Iteration 100/1000 | Loss: 0.00002145
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002144
Iteration 104/1000 | Loss: 0.00002144
Iteration 105/1000 | Loss: 0.00002144
Iteration 106/1000 | Loss: 0.00002144
Iteration 107/1000 | Loss: 0.00002144
Iteration 108/1000 | Loss: 0.00002144
Iteration 109/1000 | Loss: 0.00002144
Iteration 110/1000 | Loss: 0.00002144
Iteration 111/1000 | Loss: 0.00002143
Iteration 112/1000 | Loss: 0.00002143
Iteration 113/1000 | Loss: 0.00002143
Iteration 114/1000 | Loss: 0.00002143
Iteration 115/1000 | Loss: 0.00002143
Iteration 116/1000 | Loss: 0.00002143
Iteration 117/1000 | Loss: 0.00002143
Iteration 118/1000 | Loss: 0.00002143
Iteration 119/1000 | Loss: 0.00002143
Iteration 120/1000 | Loss: 0.00002142
Iteration 121/1000 | Loss: 0.00002142
Iteration 122/1000 | Loss: 0.00002142
Iteration 123/1000 | Loss: 0.00002142
Iteration 124/1000 | Loss: 0.00002142
Iteration 125/1000 | Loss: 0.00002142
Iteration 126/1000 | Loss: 0.00002142
Iteration 127/1000 | Loss: 0.00002142
Iteration 128/1000 | Loss: 0.00002142
Iteration 129/1000 | Loss: 0.00002141
Iteration 130/1000 | Loss: 0.00002141
Iteration 131/1000 | Loss: 0.00002141
Iteration 132/1000 | Loss: 0.00002141
Iteration 133/1000 | Loss: 0.00002141
Iteration 134/1000 | Loss: 0.00002141
Iteration 135/1000 | Loss: 0.00002140
Iteration 136/1000 | Loss: 0.00002140
Iteration 137/1000 | Loss: 0.00002140
Iteration 138/1000 | Loss: 0.00002140
Iteration 139/1000 | Loss: 0.00002140
Iteration 140/1000 | Loss: 0.00002140
Iteration 141/1000 | Loss: 0.00002140
Iteration 142/1000 | Loss: 0.00002140
Iteration 143/1000 | Loss: 0.00002140
Iteration 144/1000 | Loss: 0.00002139
Iteration 145/1000 | Loss: 0.00002139
Iteration 146/1000 | Loss: 0.00002139
Iteration 147/1000 | Loss: 0.00002139
Iteration 148/1000 | Loss: 0.00002139
Iteration 149/1000 | Loss: 0.00002139
Iteration 150/1000 | Loss: 0.00002138
Iteration 151/1000 | Loss: 0.00002138
Iteration 152/1000 | Loss: 0.00002138
Iteration 153/1000 | Loss: 0.00002138
Iteration 154/1000 | Loss: 0.00002138
Iteration 155/1000 | Loss: 0.00002138
Iteration 156/1000 | Loss: 0.00002138
Iteration 157/1000 | Loss: 0.00002138
Iteration 158/1000 | Loss: 0.00002138
Iteration 159/1000 | Loss: 0.00002138
Iteration 160/1000 | Loss: 0.00002137
Iteration 161/1000 | Loss: 0.00002137
Iteration 162/1000 | Loss: 0.00002137
Iteration 163/1000 | Loss: 0.00002137
Iteration 164/1000 | Loss: 0.00002137
Iteration 165/1000 | Loss: 0.00002137
Iteration 166/1000 | Loss: 0.00002137
Iteration 167/1000 | Loss: 0.00002137
Iteration 168/1000 | Loss: 0.00002137
Iteration 169/1000 | Loss: 0.00002137
Iteration 170/1000 | Loss: 0.00002136
Iteration 171/1000 | Loss: 0.00002136
Iteration 172/1000 | Loss: 0.00002136
Iteration 173/1000 | Loss: 0.00002136
Iteration 174/1000 | Loss: 0.00002136
Iteration 175/1000 | Loss: 0.00002136
Iteration 176/1000 | Loss: 0.00002136
Iteration 177/1000 | Loss: 0.00002135
Iteration 178/1000 | Loss: 0.00002135
Iteration 179/1000 | Loss: 0.00002135
Iteration 180/1000 | Loss: 0.00002135
Iteration 181/1000 | Loss: 0.00002135
Iteration 182/1000 | Loss: 0.00002135
Iteration 183/1000 | Loss: 0.00002135
Iteration 184/1000 | Loss: 0.00002135
Iteration 185/1000 | Loss: 0.00002135
Iteration 186/1000 | Loss: 0.00002135
Iteration 187/1000 | Loss: 0.00002135
Iteration 188/1000 | Loss: 0.00002135
Iteration 189/1000 | Loss: 0.00002135
Iteration 190/1000 | Loss: 0.00002135
Iteration 191/1000 | Loss: 0.00002135
Iteration 192/1000 | Loss: 0.00002135
Iteration 193/1000 | Loss: 0.00002135
Iteration 194/1000 | Loss: 0.00002135
Iteration 195/1000 | Loss: 0.00002134
Iteration 196/1000 | Loss: 0.00002134
Iteration 197/1000 | Loss: 0.00002134
Iteration 198/1000 | Loss: 0.00002134
Iteration 199/1000 | Loss: 0.00002134
Iteration 200/1000 | Loss: 0.00002134
Iteration 201/1000 | Loss: 0.00002134
Iteration 202/1000 | Loss: 0.00002134
Iteration 203/1000 | Loss: 0.00002134
Iteration 204/1000 | Loss: 0.00002133
Iteration 205/1000 | Loss: 0.00002133
Iteration 206/1000 | Loss: 0.00002133
Iteration 207/1000 | Loss: 0.00002133
Iteration 208/1000 | Loss: 0.00002133
Iteration 209/1000 | Loss: 0.00002133
Iteration 210/1000 | Loss: 0.00002133
Iteration 211/1000 | Loss: 0.00002132
Iteration 212/1000 | Loss: 0.00002132
Iteration 213/1000 | Loss: 0.00002132
Iteration 214/1000 | Loss: 0.00002132
Iteration 215/1000 | Loss: 0.00002132
Iteration 216/1000 | Loss: 0.00002132
Iteration 217/1000 | Loss: 0.00002132
Iteration 218/1000 | Loss: 0.00002132
Iteration 219/1000 | Loss: 0.00002132
Iteration 220/1000 | Loss: 0.00002132
Iteration 221/1000 | Loss: 0.00002132
Iteration 222/1000 | Loss: 0.00002132
Iteration 223/1000 | Loss: 0.00002132
Iteration 224/1000 | Loss: 0.00002132
Iteration 225/1000 | Loss: 0.00002132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.1322006432455964e-05, 2.1322006432455964e-05, 2.1322006432455964e-05, 2.1322006432455964e-05, 2.1322006432455964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1322006432455964e-05

Optimization complete. Final v2v error: 3.841578722000122 mm

Highest mean error: 5.026793479919434 mm for frame 105

Lowest mean error: 3.0250566005706787 mm for frame 148

Saving results

Total time: 111.94865560531616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750943
Iteration 2/25 | Loss: 0.00158089
Iteration 3/25 | Loss: 0.00130394
Iteration 4/25 | Loss: 0.00128714
Iteration 5/25 | Loss: 0.00128394
Iteration 6/25 | Loss: 0.00128366
Iteration 7/25 | Loss: 0.00128366
Iteration 8/25 | Loss: 0.00128366
Iteration 9/25 | Loss: 0.00128366
Iteration 10/25 | Loss: 0.00128366
Iteration 11/25 | Loss: 0.00128366
Iteration 12/25 | Loss: 0.00128366
Iteration 13/25 | Loss: 0.00128366
Iteration 14/25 | Loss: 0.00128366
Iteration 15/25 | Loss: 0.00128366
Iteration 16/25 | Loss: 0.00128366
Iteration 17/25 | Loss: 0.00128366
Iteration 18/25 | Loss: 0.00128366
Iteration 19/25 | Loss: 0.00128366
Iteration 20/25 | Loss: 0.00128366
Iteration 21/25 | Loss: 0.00128366
Iteration 22/25 | Loss: 0.00128366
Iteration 23/25 | Loss: 0.00128366
Iteration 24/25 | Loss: 0.00128366
Iteration 25/25 | Loss: 0.00128366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38871372
Iteration 2/25 | Loss: 0.00066355
Iteration 3/25 | Loss: 0.00066355
Iteration 4/25 | Loss: 0.00066355
Iteration 5/25 | Loss: 0.00066355
Iteration 6/25 | Loss: 0.00066355
Iteration 7/25 | Loss: 0.00066354
Iteration 8/25 | Loss: 0.00066354
Iteration 9/25 | Loss: 0.00066354
Iteration 10/25 | Loss: 0.00066354
Iteration 11/25 | Loss: 0.00066354
Iteration 12/25 | Loss: 0.00066354
Iteration 13/25 | Loss: 0.00066354
Iteration 14/25 | Loss: 0.00066354
Iteration 15/25 | Loss: 0.00066354
Iteration 16/25 | Loss: 0.00066354
Iteration 17/25 | Loss: 0.00066354
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006635439931415021, 0.0006635439931415021, 0.0006635439931415021, 0.0006635439931415021, 0.0006635439931415021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006635439931415021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066354
Iteration 2/1000 | Loss: 0.00003284
Iteration 3/1000 | Loss: 0.00002582
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00001994
Iteration 7/1000 | Loss: 0.00001918
Iteration 8/1000 | Loss: 0.00001851
Iteration 9/1000 | Loss: 0.00001806
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001721
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001672
Iteration 15/1000 | Loss: 0.00001649
Iteration 16/1000 | Loss: 0.00001642
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001625
Iteration 19/1000 | Loss: 0.00001621
Iteration 20/1000 | Loss: 0.00001620
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001615
Iteration 23/1000 | Loss: 0.00001613
Iteration 24/1000 | Loss: 0.00001612
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001606
Iteration 28/1000 | Loss: 0.00001603
Iteration 29/1000 | Loss: 0.00001603
Iteration 30/1000 | Loss: 0.00001600
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001588
Iteration 44/1000 | Loss: 0.00001586
Iteration 45/1000 | Loss: 0.00001586
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001585
Iteration 48/1000 | Loss: 0.00001584
Iteration 49/1000 | Loss: 0.00001581
Iteration 50/1000 | Loss: 0.00001580
Iteration 51/1000 | Loss: 0.00001580
Iteration 52/1000 | Loss: 0.00001579
Iteration 53/1000 | Loss: 0.00001578
Iteration 54/1000 | Loss: 0.00001578
Iteration 55/1000 | Loss: 0.00001578
Iteration 56/1000 | Loss: 0.00001578
Iteration 57/1000 | Loss: 0.00001578
Iteration 58/1000 | Loss: 0.00001577
Iteration 59/1000 | Loss: 0.00001577
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001572
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001571
Iteration 75/1000 | Loss: 0.00001571
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001570
Iteration 78/1000 | Loss: 0.00001570
Iteration 79/1000 | Loss: 0.00001570
Iteration 80/1000 | Loss: 0.00001570
Iteration 81/1000 | Loss: 0.00001570
Iteration 82/1000 | Loss: 0.00001569
Iteration 83/1000 | Loss: 0.00001569
Iteration 84/1000 | Loss: 0.00001569
Iteration 85/1000 | Loss: 0.00001569
Iteration 86/1000 | Loss: 0.00001568
Iteration 87/1000 | Loss: 0.00001568
Iteration 88/1000 | Loss: 0.00001567
Iteration 89/1000 | Loss: 0.00001567
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001565
Iteration 92/1000 | Loss: 0.00001565
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001565
Iteration 95/1000 | Loss: 0.00001565
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001564
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001563
Iteration 106/1000 | Loss: 0.00001563
Iteration 107/1000 | Loss: 0.00001562
Iteration 108/1000 | Loss: 0.00001562
Iteration 109/1000 | Loss: 0.00001562
Iteration 110/1000 | Loss: 0.00001561
Iteration 111/1000 | Loss: 0.00001561
Iteration 112/1000 | Loss: 0.00001561
Iteration 113/1000 | Loss: 0.00001561
Iteration 114/1000 | Loss: 0.00001561
Iteration 115/1000 | Loss: 0.00001561
Iteration 116/1000 | Loss: 0.00001560
Iteration 117/1000 | Loss: 0.00001560
Iteration 118/1000 | Loss: 0.00001560
Iteration 119/1000 | Loss: 0.00001560
Iteration 120/1000 | Loss: 0.00001560
Iteration 121/1000 | Loss: 0.00001560
Iteration 122/1000 | Loss: 0.00001560
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001558
Iteration 130/1000 | Loss: 0.00001558
Iteration 131/1000 | Loss: 0.00001558
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001557
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001557
Iteration 140/1000 | Loss: 0.00001557
Iteration 141/1000 | Loss: 0.00001557
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001556
Iteration 144/1000 | Loss: 0.00001556
Iteration 145/1000 | Loss: 0.00001556
Iteration 146/1000 | Loss: 0.00001555
Iteration 147/1000 | Loss: 0.00001555
Iteration 148/1000 | Loss: 0.00001555
Iteration 149/1000 | Loss: 0.00001555
Iteration 150/1000 | Loss: 0.00001555
Iteration 151/1000 | Loss: 0.00001555
Iteration 152/1000 | Loss: 0.00001554
Iteration 153/1000 | Loss: 0.00001554
Iteration 154/1000 | Loss: 0.00001554
Iteration 155/1000 | Loss: 0.00001554
Iteration 156/1000 | Loss: 0.00001554
Iteration 157/1000 | Loss: 0.00001554
Iteration 158/1000 | Loss: 0.00001554
Iteration 159/1000 | Loss: 0.00001554
Iteration 160/1000 | Loss: 0.00001553
Iteration 161/1000 | Loss: 0.00001553
Iteration 162/1000 | Loss: 0.00001553
Iteration 163/1000 | Loss: 0.00001553
Iteration 164/1000 | Loss: 0.00001553
Iteration 165/1000 | Loss: 0.00001552
Iteration 166/1000 | Loss: 0.00001552
Iteration 167/1000 | Loss: 0.00001552
Iteration 168/1000 | Loss: 0.00001552
Iteration 169/1000 | Loss: 0.00001552
Iteration 170/1000 | Loss: 0.00001552
Iteration 171/1000 | Loss: 0.00001552
Iteration 172/1000 | Loss: 0.00001552
Iteration 173/1000 | Loss: 0.00001552
Iteration 174/1000 | Loss: 0.00001551
Iteration 175/1000 | Loss: 0.00001551
Iteration 176/1000 | Loss: 0.00001551
Iteration 177/1000 | Loss: 0.00001551
Iteration 178/1000 | Loss: 0.00001551
Iteration 179/1000 | Loss: 0.00001551
Iteration 180/1000 | Loss: 0.00001550
Iteration 181/1000 | Loss: 0.00001550
Iteration 182/1000 | Loss: 0.00001550
Iteration 183/1000 | Loss: 0.00001550
Iteration 184/1000 | Loss: 0.00001550
Iteration 185/1000 | Loss: 0.00001550
Iteration 186/1000 | Loss: 0.00001550
Iteration 187/1000 | Loss: 0.00001550
Iteration 188/1000 | Loss: 0.00001550
Iteration 189/1000 | Loss: 0.00001550
Iteration 190/1000 | Loss: 0.00001550
Iteration 191/1000 | Loss: 0.00001550
Iteration 192/1000 | Loss: 0.00001550
Iteration 193/1000 | Loss: 0.00001550
Iteration 194/1000 | Loss: 0.00001550
Iteration 195/1000 | Loss: 0.00001550
Iteration 196/1000 | Loss: 0.00001550
Iteration 197/1000 | Loss: 0.00001549
Iteration 198/1000 | Loss: 0.00001549
Iteration 199/1000 | Loss: 0.00001549
Iteration 200/1000 | Loss: 0.00001549
Iteration 201/1000 | Loss: 0.00001549
Iteration 202/1000 | Loss: 0.00001549
Iteration 203/1000 | Loss: 0.00001549
Iteration 204/1000 | Loss: 0.00001549
Iteration 205/1000 | Loss: 0.00001549
Iteration 206/1000 | Loss: 0.00001549
Iteration 207/1000 | Loss: 0.00001549
Iteration 208/1000 | Loss: 0.00001549
Iteration 209/1000 | Loss: 0.00001549
Iteration 210/1000 | Loss: 0.00001549
Iteration 211/1000 | Loss: 0.00001549
Iteration 212/1000 | Loss: 0.00001549
Iteration 213/1000 | Loss: 0.00001549
Iteration 214/1000 | Loss: 0.00001549
Iteration 215/1000 | Loss: 0.00001549
Iteration 216/1000 | Loss: 0.00001549
Iteration 217/1000 | Loss: 0.00001548
Iteration 218/1000 | Loss: 0.00001548
Iteration 219/1000 | Loss: 0.00001548
Iteration 220/1000 | Loss: 0.00001548
Iteration 221/1000 | Loss: 0.00001548
Iteration 222/1000 | Loss: 0.00001548
Iteration 223/1000 | Loss: 0.00001548
Iteration 224/1000 | Loss: 0.00001548
Iteration 225/1000 | Loss: 0.00001548
Iteration 226/1000 | Loss: 0.00001548
Iteration 227/1000 | Loss: 0.00001548
Iteration 228/1000 | Loss: 0.00001548
Iteration 229/1000 | Loss: 0.00001548
Iteration 230/1000 | Loss: 0.00001548
Iteration 231/1000 | Loss: 0.00001548
Iteration 232/1000 | Loss: 0.00001548
Iteration 233/1000 | Loss: 0.00001548
Iteration 234/1000 | Loss: 0.00001548
Iteration 235/1000 | Loss: 0.00001548
Iteration 236/1000 | Loss: 0.00001548
Iteration 237/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.5479532521567307e-05, 1.5479532521567307e-05, 1.5479532521567307e-05, 1.5479532521567307e-05, 1.5479532521567307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5479532521567307e-05

Optimization complete. Final v2v error: 3.284104108810425 mm

Highest mean error: 5.38407039642334 mm for frame 0

Lowest mean error: 2.8841793537139893 mm for frame 144

Saving results

Total time: 54.64721393585205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715052
Iteration 2/25 | Loss: 0.00171153
Iteration 3/25 | Loss: 0.00151603
Iteration 4/25 | Loss: 0.00146681
Iteration 5/25 | Loss: 0.00145952
Iteration 6/25 | Loss: 0.00144356
Iteration 7/25 | Loss: 0.00142956
Iteration 8/25 | Loss: 0.00142754
Iteration 9/25 | Loss: 0.00142740
Iteration 10/25 | Loss: 0.00142738
Iteration 11/25 | Loss: 0.00142738
Iteration 12/25 | Loss: 0.00142738
Iteration 13/25 | Loss: 0.00142738
Iteration 14/25 | Loss: 0.00142737
Iteration 15/25 | Loss: 0.00142737
Iteration 16/25 | Loss: 0.00142737
Iteration 17/25 | Loss: 0.00142737
Iteration 18/25 | Loss: 0.00142737
Iteration 19/25 | Loss: 0.00142737
Iteration 20/25 | Loss: 0.00142737
Iteration 21/25 | Loss: 0.00142737
Iteration 22/25 | Loss: 0.00142737
Iteration 23/25 | Loss: 0.00142736
Iteration 24/25 | Loss: 0.00142736
Iteration 25/25 | Loss: 0.00142736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50354624
Iteration 2/25 | Loss: 0.00095630
Iteration 3/25 | Loss: 0.00095623
Iteration 4/25 | Loss: 0.00095622
Iteration 5/25 | Loss: 0.00095622
Iteration 6/25 | Loss: 0.00095622
Iteration 7/25 | Loss: 0.00095622
Iteration 8/25 | Loss: 0.00095622
Iteration 9/25 | Loss: 0.00095622
Iteration 10/25 | Loss: 0.00095622
Iteration 11/25 | Loss: 0.00095622
Iteration 12/25 | Loss: 0.00095622
Iteration 13/25 | Loss: 0.00095622
Iteration 14/25 | Loss: 0.00095622
Iteration 15/25 | Loss: 0.00095622
Iteration 16/25 | Loss: 0.00095622
Iteration 17/25 | Loss: 0.00095622
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009562222985550761, 0.0009562222985550761, 0.0009562222985550761, 0.0009562222985550761, 0.0009562222985550761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009562222985550761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095622
Iteration 2/1000 | Loss: 0.00004995
Iteration 3/1000 | Loss: 0.00003347
Iteration 4/1000 | Loss: 0.00002964
Iteration 5/1000 | Loss: 0.00002801
Iteration 6/1000 | Loss: 0.00002676
Iteration 7/1000 | Loss: 0.00002607
Iteration 8/1000 | Loss: 0.00002550
Iteration 9/1000 | Loss: 0.00002511
Iteration 10/1000 | Loss: 0.00002484
Iteration 11/1000 | Loss: 0.00002461
Iteration 12/1000 | Loss: 0.00002440
Iteration 13/1000 | Loss: 0.00002431
Iteration 14/1000 | Loss: 0.00002417
Iteration 15/1000 | Loss: 0.00002417
Iteration 16/1000 | Loss: 0.00002411
Iteration 17/1000 | Loss: 0.00002410
Iteration 18/1000 | Loss: 0.00002410
Iteration 19/1000 | Loss: 0.00002408
Iteration 20/1000 | Loss: 0.00002408
Iteration 21/1000 | Loss: 0.00002407
Iteration 22/1000 | Loss: 0.00002407
Iteration 23/1000 | Loss: 0.00002403
Iteration 24/1000 | Loss: 0.00002401
Iteration 25/1000 | Loss: 0.00002401
Iteration 26/1000 | Loss: 0.00002399
Iteration 27/1000 | Loss: 0.00002399
Iteration 28/1000 | Loss: 0.00002399
Iteration 29/1000 | Loss: 0.00002399
Iteration 30/1000 | Loss: 0.00002399
Iteration 31/1000 | Loss: 0.00002399
Iteration 32/1000 | Loss: 0.00002399
Iteration 33/1000 | Loss: 0.00002398
Iteration 34/1000 | Loss: 0.00002398
Iteration 35/1000 | Loss: 0.00002398
Iteration 36/1000 | Loss: 0.00002398
Iteration 37/1000 | Loss: 0.00002398
Iteration 38/1000 | Loss: 0.00002398
Iteration 39/1000 | Loss: 0.00002397
Iteration 40/1000 | Loss: 0.00002397
Iteration 41/1000 | Loss: 0.00002397
Iteration 42/1000 | Loss: 0.00002395
Iteration 43/1000 | Loss: 0.00002395
Iteration 44/1000 | Loss: 0.00002394
Iteration 45/1000 | Loss: 0.00002394
Iteration 46/1000 | Loss: 0.00002394
Iteration 47/1000 | Loss: 0.00002393
Iteration 48/1000 | Loss: 0.00002393
Iteration 49/1000 | Loss: 0.00002393
Iteration 50/1000 | Loss: 0.00002393
Iteration 51/1000 | Loss: 0.00002392
Iteration 52/1000 | Loss: 0.00002392
Iteration 53/1000 | Loss: 0.00002391
Iteration 54/1000 | Loss: 0.00002391
Iteration 55/1000 | Loss: 0.00002391
Iteration 56/1000 | Loss: 0.00002391
Iteration 57/1000 | Loss: 0.00002391
Iteration 58/1000 | Loss: 0.00002390
Iteration 59/1000 | Loss: 0.00002390
Iteration 60/1000 | Loss: 0.00002389
Iteration 61/1000 | Loss: 0.00002389
Iteration 62/1000 | Loss: 0.00002389
Iteration 63/1000 | Loss: 0.00002389
Iteration 64/1000 | Loss: 0.00002389
Iteration 65/1000 | Loss: 0.00002389
Iteration 66/1000 | Loss: 0.00002389
Iteration 67/1000 | Loss: 0.00002389
Iteration 68/1000 | Loss: 0.00002389
Iteration 69/1000 | Loss: 0.00002388
Iteration 70/1000 | Loss: 0.00002388
Iteration 71/1000 | Loss: 0.00002388
Iteration 72/1000 | Loss: 0.00002388
Iteration 73/1000 | Loss: 0.00002388
Iteration 74/1000 | Loss: 0.00002388
Iteration 75/1000 | Loss: 0.00002388
Iteration 76/1000 | Loss: 0.00002388
Iteration 77/1000 | Loss: 0.00002388
Iteration 78/1000 | Loss: 0.00002388
Iteration 79/1000 | Loss: 0.00002387
Iteration 80/1000 | Loss: 0.00002387
Iteration 81/1000 | Loss: 0.00002387
Iteration 82/1000 | Loss: 0.00002387
Iteration 83/1000 | Loss: 0.00002387
Iteration 84/1000 | Loss: 0.00002387
Iteration 85/1000 | Loss: 0.00002386
Iteration 86/1000 | Loss: 0.00002386
Iteration 87/1000 | Loss: 0.00002386
Iteration 88/1000 | Loss: 0.00002386
Iteration 89/1000 | Loss: 0.00002385
Iteration 90/1000 | Loss: 0.00002385
Iteration 91/1000 | Loss: 0.00002385
Iteration 92/1000 | Loss: 0.00002385
Iteration 93/1000 | Loss: 0.00002385
Iteration 94/1000 | Loss: 0.00002384
Iteration 95/1000 | Loss: 0.00002384
Iteration 96/1000 | Loss: 0.00002384
Iteration 97/1000 | Loss: 0.00002384
Iteration 98/1000 | Loss: 0.00002384
Iteration 99/1000 | Loss: 0.00002384
Iteration 100/1000 | Loss: 0.00002384
Iteration 101/1000 | Loss: 0.00002383
Iteration 102/1000 | Loss: 0.00002383
Iteration 103/1000 | Loss: 0.00002383
Iteration 104/1000 | Loss: 0.00002383
Iteration 105/1000 | Loss: 0.00002383
Iteration 106/1000 | Loss: 0.00002383
Iteration 107/1000 | Loss: 0.00002383
Iteration 108/1000 | Loss: 0.00002383
Iteration 109/1000 | Loss: 0.00002383
Iteration 110/1000 | Loss: 0.00002383
Iteration 111/1000 | Loss: 0.00002382
Iteration 112/1000 | Loss: 0.00002382
Iteration 113/1000 | Loss: 0.00002382
Iteration 114/1000 | Loss: 0.00002382
Iteration 115/1000 | Loss: 0.00002382
Iteration 116/1000 | Loss: 0.00002382
Iteration 117/1000 | Loss: 0.00002381
Iteration 118/1000 | Loss: 0.00002381
Iteration 119/1000 | Loss: 0.00002381
Iteration 120/1000 | Loss: 0.00002381
Iteration 121/1000 | Loss: 0.00002381
Iteration 122/1000 | Loss: 0.00002381
Iteration 123/1000 | Loss: 0.00002381
Iteration 124/1000 | Loss: 0.00002381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.381049307587091e-05, 2.381049307587091e-05, 2.381049307587091e-05, 2.381049307587091e-05, 2.381049307587091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.381049307587091e-05

Optimization complete. Final v2v error: 4.051897048950195 mm

Highest mean error: 4.953010559082031 mm for frame 23

Lowest mean error: 3.4470760822296143 mm for frame 198

Saving results

Total time: 50.91189789772034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395765
Iteration 2/25 | Loss: 0.00139561
Iteration 3/25 | Loss: 0.00131025
Iteration 4/25 | Loss: 0.00129058
Iteration 5/25 | Loss: 0.00128272
Iteration 6/25 | Loss: 0.00128161
Iteration 7/25 | Loss: 0.00128161
Iteration 8/25 | Loss: 0.00128161
Iteration 9/25 | Loss: 0.00128161
Iteration 10/25 | Loss: 0.00128161
Iteration 11/25 | Loss: 0.00128161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012816147645935416, 0.0012816147645935416, 0.0012816147645935416, 0.0012816147645935416, 0.0012816147645935416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012816147645935416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38158727
Iteration 2/25 | Loss: 0.00094794
Iteration 3/25 | Loss: 0.00094794
Iteration 4/25 | Loss: 0.00094794
Iteration 5/25 | Loss: 0.00094794
Iteration 6/25 | Loss: 0.00094794
Iteration 7/25 | Loss: 0.00094794
Iteration 8/25 | Loss: 0.00094794
Iteration 9/25 | Loss: 0.00094794
Iteration 10/25 | Loss: 0.00094794
Iteration 11/25 | Loss: 0.00094794
Iteration 12/25 | Loss: 0.00094794
Iteration 13/25 | Loss: 0.00094794
Iteration 14/25 | Loss: 0.00094794
Iteration 15/25 | Loss: 0.00094794
Iteration 16/25 | Loss: 0.00094794
Iteration 17/25 | Loss: 0.00094794
Iteration 18/25 | Loss: 0.00094794
Iteration 19/25 | Loss: 0.00094794
Iteration 20/25 | Loss: 0.00094794
Iteration 21/25 | Loss: 0.00094794
Iteration 22/25 | Loss: 0.00094794
Iteration 23/25 | Loss: 0.00094794
Iteration 24/25 | Loss: 0.00094794
Iteration 25/25 | Loss: 0.00094794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094794
Iteration 2/1000 | Loss: 0.00004987
Iteration 3/1000 | Loss: 0.00003684
Iteration 4/1000 | Loss: 0.00003029
Iteration 5/1000 | Loss: 0.00002800
Iteration 6/1000 | Loss: 0.00002641
Iteration 7/1000 | Loss: 0.00002542
Iteration 8/1000 | Loss: 0.00002472
Iteration 9/1000 | Loss: 0.00002435
Iteration 10/1000 | Loss: 0.00002398
Iteration 11/1000 | Loss: 0.00002370
Iteration 12/1000 | Loss: 0.00002349
Iteration 13/1000 | Loss: 0.00002329
Iteration 14/1000 | Loss: 0.00002320
Iteration 15/1000 | Loss: 0.00002318
Iteration 16/1000 | Loss: 0.00002317
Iteration 17/1000 | Loss: 0.00002312
Iteration 18/1000 | Loss: 0.00002309
Iteration 19/1000 | Loss: 0.00002308
Iteration 20/1000 | Loss: 0.00002308
Iteration 21/1000 | Loss: 0.00002308
Iteration 22/1000 | Loss: 0.00002308
Iteration 23/1000 | Loss: 0.00002308
Iteration 24/1000 | Loss: 0.00002307
Iteration 25/1000 | Loss: 0.00002307
Iteration 26/1000 | Loss: 0.00002306
Iteration 27/1000 | Loss: 0.00002305
Iteration 28/1000 | Loss: 0.00002305
Iteration 29/1000 | Loss: 0.00002304
Iteration 30/1000 | Loss: 0.00002304
Iteration 31/1000 | Loss: 0.00002304
Iteration 32/1000 | Loss: 0.00002303
Iteration 33/1000 | Loss: 0.00002303
Iteration 34/1000 | Loss: 0.00002303
Iteration 35/1000 | Loss: 0.00002302
Iteration 36/1000 | Loss: 0.00002302
Iteration 37/1000 | Loss: 0.00002301
Iteration 38/1000 | Loss: 0.00002301
Iteration 39/1000 | Loss: 0.00002300
Iteration 40/1000 | Loss: 0.00002297
Iteration 41/1000 | Loss: 0.00002296
Iteration 42/1000 | Loss: 0.00002296
Iteration 43/1000 | Loss: 0.00002295
Iteration 44/1000 | Loss: 0.00002294
Iteration 45/1000 | Loss: 0.00002294
Iteration 46/1000 | Loss: 0.00002293
Iteration 47/1000 | Loss: 0.00002293
Iteration 48/1000 | Loss: 0.00002293
Iteration 49/1000 | Loss: 0.00002291
Iteration 50/1000 | Loss: 0.00002291
Iteration 51/1000 | Loss: 0.00002291
Iteration 52/1000 | Loss: 0.00002291
Iteration 53/1000 | Loss: 0.00002291
Iteration 54/1000 | Loss: 0.00002291
Iteration 55/1000 | Loss: 0.00002290
Iteration 56/1000 | Loss: 0.00002290
Iteration 57/1000 | Loss: 0.00002289
Iteration 58/1000 | Loss: 0.00002289
Iteration 59/1000 | Loss: 0.00002289
Iteration 60/1000 | Loss: 0.00002288
Iteration 61/1000 | Loss: 0.00002288
Iteration 62/1000 | Loss: 0.00002288
Iteration 63/1000 | Loss: 0.00002287
Iteration 64/1000 | Loss: 0.00002287
Iteration 65/1000 | Loss: 0.00002287
Iteration 66/1000 | Loss: 0.00002286
Iteration 67/1000 | Loss: 0.00002286
Iteration 68/1000 | Loss: 0.00002286
Iteration 69/1000 | Loss: 0.00002285
Iteration 70/1000 | Loss: 0.00002285
Iteration 71/1000 | Loss: 0.00002285
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002284
Iteration 75/1000 | Loss: 0.00002284
Iteration 76/1000 | Loss: 0.00002284
Iteration 77/1000 | Loss: 0.00002284
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00002283
Iteration 80/1000 | Loss: 0.00002283
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002282
Iteration 83/1000 | Loss: 0.00002282
Iteration 84/1000 | Loss: 0.00002282
Iteration 85/1000 | Loss: 0.00002282
Iteration 86/1000 | Loss: 0.00002282
Iteration 87/1000 | Loss: 0.00002282
Iteration 88/1000 | Loss: 0.00002282
Iteration 89/1000 | Loss: 0.00002282
Iteration 90/1000 | Loss: 0.00002281
Iteration 91/1000 | Loss: 0.00002281
Iteration 92/1000 | Loss: 0.00002280
Iteration 93/1000 | Loss: 0.00002280
Iteration 94/1000 | Loss: 0.00002280
Iteration 95/1000 | Loss: 0.00002280
Iteration 96/1000 | Loss: 0.00002279
Iteration 97/1000 | Loss: 0.00002279
Iteration 98/1000 | Loss: 0.00002279
Iteration 99/1000 | Loss: 0.00002278
Iteration 100/1000 | Loss: 0.00002278
Iteration 101/1000 | Loss: 0.00002278
Iteration 102/1000 | Loss: 0.00002278
Iteration 103/1000 | Loss: 0.00002277
Iteration 104/1000 | Loss: 0.00002277
Iteration 105/1000 | Loss: 0.00002277
Iteration 106/1000 | Loss: 0.00002277
Iteration 107/1000 | Loss: 0.00002277
Iteration 108/1000 | Loss: 0.00002276
Iteration 109/1000 | Loss: 0.00002276
Iteration 110/1000 | Loss: 0.00002276
Iteration 111/1000 | Loss: 0.00002276
Iteration 112/1000 | Loss: 0.00002275
Iteration 113/1000 | Loss: 0.00002275
Iteration 114/1000 | Loss: 0.00002275
Iteration 115/1000 | Loss: 0.00002275
Iteration 116/1000 | Loss: 0.00002275
Iteration 117/1000 | Loss: 0.00002274
Iteration 118/1000 | Loss: 0.00002274
Iteration 119/1000 | Loss: 0.00002274
Iteration 120/1000 | Loss: 0.00002274
Iteration 121/1000 | Loss: 0.00002274
Iteration 122/1000 | Loss: 0.00002273
Iteration 123/1000 | Loss: 0.00002273
Iteration 124/1000 | Loss: 0.00002273
Iteration 125/1000 | Loss: 0.00002273
Iteration 126/1000 | Loss: 0.00002273
Iteration 127/1000 | Loss: 0.00002273
Iteration 128/1000 | Loss: 0.00002273
Iteration 129/1000 | Loss: 0.00002273
Iteration 130/1000 | Loss: 0.00002273
Iteration 131/1000 | Loss: 0.00002273
Iteration 132/1000 | Loss: 0.00002273
Iteration 133/1000 | Loss: 0.00002272
Iteration 134/1000 | Loss: 0.00002272
Iteration 135/1000 | Loss: 0.00002272
Iteration 136/1000 | Loss: 0.00002272
Iteration 137/1000 | Loss: 0.00002272
Iteration 138/1000 | Loss: 0.00002272
Iteration 139/1000 | Loss: 0.00002272
Iteration 140/1000 | Loss: 0.00002272
Iteration 141/1000 | Loss: 0.00002272
Iteration 142/1000 | Loss: 0.00002272
Iteration 143/1000 | Loss: 0.00002272
Iteration 144/1000 | Loss: 0.00002272
Iteration 145/1000 | Loss: 0.00002272
Iteration 146/1000 | Loss: 0.00002272
Iteration 147/1000 | Loss: 0.00002272
Iteration 148/1000 | Loss: 0.00002272
Iteration 149/1000 | Loss: 0.00002272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.272107667522505e-05, 2.272107667522505e-05, 2.272107667522505e-05, 2.272107667522505e-05, 2.272107667522505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.272107667522505e-05

Optimization complete. Final v2v error: 3.927326202392578 mm

Highest mean error: 4.903486728668213 mm for frame 103

Lowest mean error: 2.982980489730835 mm for frame 3

Saving results

Total time: 43.1072154045105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00301231
Iteration 2/25 | Loss: 0.00139329
Iteration 3/25 | Loss: 0.00126072
Iteration 4/25 | Loss: 0.00124724
Iteration 5/25 | Loss: 0.00124366
Iteration 6/25 | Loss: 0.00124276
Iteration 7/25 | Loss: 0.00124247
Iteration 8/25 | Loss: 0.00124247
Iteration 9/25 | Loss: 0.00124247
Iteration 10/25 | Loss: 0.00124247
Iteration 11/25 | Loss: 0.00124247
Iteration 12/25 | Loss: 0.00124247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012424684828147292, 0.0012424684828147292, 0.0012424684828147292, 0.0012424684828147292, 0.0012424684828147292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012424684828147292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36723936
Iteration 2/25 | Loss: 0.00081017
Iteration 3/25 | Loss: 0.00081017
Iteration 4/25 | Loss: 0.00081017
Iteration 5/25 | Loss: 0.00081017
Iteration 6/25 | Loss: 0.00081017
Iteration 7/25 | Loss: 0.00081017
Iteration 8/25 | Loss: 0.00081017
Iteration 9/25 | Loss: 0.00081017
Iteration 10/25 | Loss: 0.00081017
Iteration 11/25 | Loss: 0.00081017
Iteration 12/25 | Loss: 0.00081017
Iteration 13/25 | Loss: 0.00081017
Iteration 14/25 | Loss: 0.00081017
Iteration 15/25 | Loss: 0.00081017
Iteration 16/25 | Loss: 0.00081017
Iteration 17/25 | Loss: 0.00081017
Iteration 18/25 | Loss: 0.00081017
Iteration 19/25 | Loss: 0.00081017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008101653074845672, 0.0008101653074845672, 0.0008101653074845672, 0.0008101653074845672, 0.0008101653074845672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008101653074845672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081017
Iteration 2/1000 | Loss: 0.00004727
Iteration 3/1000 | Loss: 0.00002541
Iteration 4/1000 | Loss: 0.00002183
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001907
Iteration 7/1000 | Loss: 0.00001820
Iteration 8/1000 | Loss: 0.00001754
Iteration 9/1000 | Loss: 0.00001699
Iteration 10/1000 | Loss: 0.00001651
Iteration 11/1000 | Loss: 0.00001629
Iteration 12/1000 | Loss: 0.00001609
Iteration 13/1000 | Loss: 0.00001602
Iteration 14/1000 | Loss: 0.00001597
Iteration 15/1000 | Loss: 0.00001596
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001593
Iteration 19/1000 | Loss: 0.00001592
Iteration 20/1000 | Loss: 0.00001592
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001589
Iteration 27/1000 | Loss: 0.00001589
Iteration 28/1000 | Loss: 0.00001589
Iteration 29/1000 | Loss: 0.00001588
Iteration 30/1000 | Loss: 0.00001588
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001587
Iteration 35/1000 | Loss: 0.00001586
Iteration 36/1000 | Loss: 0.00001586
Iteration 37/1000 | Loss: 0.00001586
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00001585
Iteration 40/1000 | Loss: 0.00001584
Iteration 41/1000 | Loss: 0.00001584
Iteration 42/1000 | Loss: 0.00001584
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001584
Iteration 45/1000 | Loss: 0.00001584
Iteration 46/1000 | Loss: 0.00001583
Iteration 47/1000 | Loss: 0.00001582
Iteration 48/1000 | Loss: 0.00001582
Iteration 49/1000 | Loss: 0.00001581
Iteration 50/1000 | Loss: 0.00001580
Iteration 51/1000 | Loss: 0.00001579
Iteration 52/1000 | Loss: 0.00001579
Iteration 53/1000 | Loss: 0.00001579
Iteration 54/1000 | Loss: 0.00001578
Iteration 55/1000 | Loss: 0.00001578
Iteration 56/1000 | Loss: 0.00001578
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001577
Iteration 59/1000 | Loss: 0.00001577
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001576
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001575
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001575
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001574
Iteration 79/1000 | Loss: 0.00001574
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001573
Iteration 82/1000 | Loss: 0.00001573
Iteration 83/1000 | Loss: 0.00001573
Iteration 84/1000 | Loss: 0.00001573
Iteration 85/1000 | Loss: 0.00001572
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001569
Iteration 93/1000 | Loss: 0.00001569
Iteration 94/1000 | Loss: 0.00001569
Iteration 95/1000 | Loss: 0.00001568
Iteration 96/1000 | Loss: 0.00001568
Iteration 97/1000 | Loss: 0.00001568
Iteration 98/1000 | Loss: 0.00001567
Iteration 99/1000 | Loss: 0.00001567
Iteration 100/1000 | Loss: 0.00001567
Iteration 101/1000 | Loss: 0.00001566
Iteration 102/1000 | Loss: 0.00001566
Iteration 103/1000 | Loss: 0.00001566
Iteration 104/1000 | Loss: 0.00001566
Iteration 105/1000 | Loss: 0.00001565
Iteration 106/1000 | Loss: 0.00001565
Iteration 107/1000 | Loss: 0.00001565
Iteration 108/1000 | Loss: 0.00001565
Iteration 109/1000 | Loss: 0.00001564
Iteration 110/1000 | Loss: 0.00001564
Iteration 111/1000 | Loss: 0.00001564
Iteration 112/1000 | Loss: 0.00001564
Iteration 113/1000 | Loss: 0.00001563
Iteration 114/1000 | Loss: 0.00001563
Iteration 115/1000 | Loss: 0.00001563
Iteration 116/1000 | Loss: 0.00001563
Iteration 117/1000 | Loss: 0.00001563
Iteration 118/1000 | Loss: 0.00001563
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001563
Iteration 122/1000 | Loss: 0.00001563
Iteration 123/1000 | Loss: 0.00001563
Iteration 124/1000 | Loss: 0.00001563
Iteration 125/1000 | Loss: 0.00001563
Iteration 126/1000 | Loss: 0.00001563
Iteration 127/1000 | Loss: 0.00001563
Iteration 128/1000 | Loss: 0.00001563
Iteration 129/1000 | Loss: 0.00001563
Iteration 130/1000 | Loss: 0.00001563
Iteration 131/1000 | Loss: 0.00001563
Iteration 132/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.5628251276211813e-05, 1.5628251276211813e-05, 1.5628251276211813e-05, 1.5628251276211813e-05, 1.5628251276211813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5628251276211813e-05

Optimization complete. Final v2v error: 3.378386974334717 mm

Highest mean error: 3.601051092147827 mm for frame 135

Lowest mean error: 3.1563045978546143 mm for frame 41

Saving results

Total time: 40.26485276222229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567854
Iteration 2/25 | Loss: 0.00133736
Iteration 3/25 | Loss: 0.00127534
Iteration 4/25 | Loss: 0.00126666
Iteration 5/25 | Loss: 0.00126366
Iteration 6/25 | Loss: 0.00126348
Iteration 7/25 | Loss: 0.00126348
Iteration 8/25 | Loss: 0.00126348
Iteration 9/25 | Loss: 0.00126348
Iteration 10/25 | Loss: 0.00126348
Iteration 11/25 | Loss: 0.00126348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012634772574529052, 0.0012634772574529052, 0.0012634772574529052, 0.0012634772574529052, 0.0012634772574529052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012634772574529052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.88762355
Iteration 2/25 | Loss: 0.00087450
Iteration 3/25 | Loss: 0.00087450
Iteration 4/25 | Loss: 0.00087450
Iteration 5/25 | Loss: 0.00087450
Iteration 6/25 | Loss: 0.00087449
Iteration 7/25 | Loss: 0.00087449
Iteration 8/25 | Loss: 0.00087449
Iteration 9/25 | Loss: 0.00087449
Iteration 10/25 | Loss: 0.00087449
Iteration 11/25 | Loss: 0.00087449
Iteration 12/25 | Loss: 0.00087449
Iteration 13/25 | Loss: 0.00087449
Iteration 14/25 | Loss: 0.00087449
Iteration 15/25 | Loss: 0.00087449
Iteration 16/25 | Loss: 0.00087449
Iteration 17/25 | Loss: 0.00087449
Iteration 18/25 | Loss: 0.00087449
Iteration 19/25 | Loss: 0.00087449
Iteration 20/25 | Loss: 0.00087449
Iteration 21/25 | Loss: 0.00087449
Iteration 22/25 | Loss: 0.00087449
Iteration 23/25 | Loss: 0.00087449
Iteration 24/25 | Loss: 0.00087449
Iteration 25/25 | Loss: 0.00087449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087449
Iteration 2/1000 | Loss: 0.00002166
Iteration 3/1000 | Loss: 0.00001555
Iteration 4/1000 | Loss: 0.00001443
Iteration 5/1000 | Loss: 0.00001374
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001292
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001243
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001219
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001203
Iteration 19/1000 | Loss: 0.00001196
Iteration 20/1000 | Loss: 0.00001195
Iteration 21/1000 | Loss: 0.00001194
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001188
Iteration 27/1000 | Loss: 0.00001187
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001182
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001167
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001164
Iteration 70/1000 | Loss: 0.00001164
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001161
Iteration 83/1000 | Loss: 0.00001161
Iteration 84/1000 | Loss: 0.00001161
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001160
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001158
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001152
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001152
Iteration 129/1000 | Loss: 0.00001152
Iteration 130/1000 | Loss: 0.00001152
Iteration 131/1000 | Loss: 0.00001152
Iteration 132/1000 | Loss: 0.00001152
Iteration 133/1000 | Loss: 0.00001152
Iteration 134/1000 | Loss: 0.00001151
Iteration 135/1000 | Loss: 0.00001151
Iteration 136/1000 | Loss: 0.00001151
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001151
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001150
Iteration 144/1000 | Loss: 0.00001150
Iteration 145/1000 | Loss: 0.00001150
Iteration 146/1000 | Loss: 0.00001150
Iteration 147/1000 | Loss: 0.00001150
Iteration 148/1000 | Loss: 0.00001150
Iteration 149/1000 | Loss: 0.00001150
Iteration 150/1000 | Loss: 0.00001150
Iteration 151/1000 | Loss: 0.00001150
Iteration 152/1000 | Loss: 0.00001150
Iteration 153/1000 | Loss: 0.00001149
Iteration 154/1000 | Loss: 0.00001149
Iteration 155/1000 | Loss: 0.00001149
Iteration 156/1000 | Loss: 0.00001149
Iteration 157/1000 | Loss: 0.00001149
Iteration 158/1000 | Loss: 0.00001149
Iteration 159/1000 | Loss: 0.00001149
Iteration 160/1000 | Loss: 0.00001149
Iteration 161/1000 | Loss: 0.00001149
Iteration 162/1000 | Loss: 0.00001149
Iteration 163/1000 | Loss: 0.00001149
Iteration 164/1000 | Loss: 0.00001149
Iteration 165/1000 | Loss: 0.00001149
Iteration 166/1000 | Loss: 0.00001149
Iteration 167/1000 | Loss: 0.00001149
Iteration 168/1000 | Loss: 0.00001148
Iteration 169/1000 | Loss: 0.00001148
Iteration 170/1000 | Loss: 0.00001148
Iteration 171/1000 | Loss: 0.00001148
Iteration 172/1000 | Loss: 0.00001148
Iteration 173/1000 | Loss: 0.00001148
Iteration 174/1000 | Loss: 0.00001148
Iteration 175/1000 | Loss: 0.00001148
Iteration 176/1000 | Loss: 0.00001148
Iteration 177/1000 | Loss: 0.00001148
Iteration 178/1000 | Loss: 0.00001148
Iteration 179/1000 | Loss: 0.00001148
Iteration 180/1000 | Loss: 0.00001148
Iteration 181/1000 | Loss: 0.00001148
Iteration 182/1000 | Loss: 0.00001148
Iteration 183/1000 | Loss: 0.00001147
Iteration 184/1000 | Loss: 0.00001147
Iteration 185/1000 | Loss: 0.00001147
Iteration 186/1000 | Loss: 0.00001147
Iteration 187/1000 | Loss: 0.00001147
Iteration 188/1000 | Loss: 0.00001147
Iteration 189/1000 | Loss: 0.00001147
Iteration 190/1000 | Loss: 0.00001147
Iteration 191/1000 | Loss: 0.00001147
Iteration 192/1000 | Loss: 0.00001147
Iteration 193/1000 | Loss: 0.00001147
Iteration 194/1000 | Loss: 0.00001147
Iteration 195/1000 | Loss: 0.00001147
Iteration 196/1000 | Loss: 0.00001147
Iteration 197/1000 | Loss: 0.00001147
Iteration 198/1000 | Loss: 0.00001147
Iteration 199/1000 | Loss: 0.00001147
Iteration 200/1000 | Loss: 0.00001147
Iteration 201/1000 | Loss: 0.00001147
Iteration 202/1000 | Loss: 0.00001147
Iteration 203/1000 | Loss: 0.00001147
Iteration 204/1000 | Loss: 0.00001147
Iteration 205/1000 | Loss: 0.00001147
Iteration 206/1000 | Loss: 0.00001147
Iteration 207/1000 | Loss: 0.00001147
Iteration 208/1000 | Loss: 0.00001147
Iteration 209/1000 | Loss: 0.00001147
Iteration 210/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.1471709512989037e-05, 1.1471709512989037e-05, 1.1471709512989037e-05, 1.1471709512989037e-05, 1.1471709512989037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1471709512989037e-05

Optimization complete. Final v2v error: 2.9243087768554688 mm

Highest mean error: 3.3156721591949463 mm for frame 118

Lowest mean error: 2.809887647628784 mm for frame 2

Saving results

Total time: 38.75002384185791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394400
Iteration 2/25 | Loss: 0.00137953
Iteration 3/25 | Loss: 0.00129833
Iteration 4/25 | Loss: 0.00129098
Iteration 5/25 | Loss: 0.00128892
Iteration 6/25 | Loss: 0.00128892
Iteration 7/25 | Loss: 0.00128892
Iteration 8/25 | Loss: 0.00128892
Iteration 9/25 | Loss: 0.00128892
Iteration 10/25 | Loss: 0.00128892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001288918312638998, 0.001288918312638998, 0.001288918312638998, 0.001288918312638998, 0.001288918312638998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001288918312638998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52747273
Iteration 2/25 | Loss: 0.00085785
Iteration 3/25 | Loss: 0.00085785
Iteration 4/25 | Loss: 0.00085785
Iteration 5/25 | Loss: 0.00085785
Iteration 6/25 | Loss: 0.00085785
Iteration 7/25 | Loss: 0.00085785
Iteration 8/25 | Loss: 0.00085785
Iteration 9/25 | Loss: 0.00085785
Iteration 10/25 | Loss: 0.00085785
Iteration 11/25 | Loss: 0.00085785
Iteration 12/25 | Loss: 0.00085785
Iteration 13/25 | Loss: 0.00085785
Iteration 14/25 | Loss: 0.00085785
Iteration 15/25 | Loss: 0.00085785
Iteration 16/25 | Loss: 0.00085785
Iteration 17/25 | Loss: 0.00085785
Iteration 18/25 | Loss: 0.00085785
Iteration 19/25 | Loss: 0.00085785
Iteration 20/25 | Loss: 0.00085785
Iteration 21/25 | Loss: 0.00085785
Iteration 22/25 | Loss: 0.00085785
Iteration 23/25 | Loss: 0.00085785
Iteration 24/25 | Loss: 0.00085785
Iteration 25/25 | Loss: 0.00085785
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008578476845286787, 0.0008578476845286787, 0.0008578476845286787, 0.0008578476845286787, 0.0008578476845286787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008578476845286787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085785
Iteration 2/1000 | Loss: 0.00003350
Iteration 3/1000 | Loss: 0.00002141
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001581
Iteration 6/1000 | Loss: 0.00001501
Iteration 7/1000 | Loss: 0.00001435
Iteration 8/1000 | Loss: 0.00001391
Iteration 9/1000 | Loss: 0.00001361
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001296
Iteration 12/1000 | Loss: 0.00001293
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001273
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001243
Iteration 53/1000 | Loss: 0.00001242
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001240
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001234
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001230
Iteration 74/1000 | Loss: 0.00001230
Iteration 75/1000 | Loss: 0.00001230
Iteration 76/1000 | Loss: 0.00001230
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001229
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001226
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001226
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001223
Iteration 96/1000 | Loss: 0.00001222
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001221
Iteration 100/1000 | Loss: 0.00001221
Iteration 101/1000 | Loss: 0.00001221
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001220
Iteration 105/1000 | Loss: 0.00001220
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001219
Iteration 108/1000 | Loss: 0.00001219
Iteration 109/1000 | Loss: 0.00001219
Iteration 110/1000 | Loss: 0.00001219
Iteration 111/1000 | Loss: 0.00001218
Iteration 112/1000 | Loss: 0.00001217
Iteration 113/1000 | Loss: 0.00001217
Iteration 114/1000 | Loss: 0.00001217
Iteration 115/1000 | Loss: 0.00001217
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001215
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Iteration 128/1000 | Loss: 0.00001213
Iteration 129/1000 | Loss: 0.00001213
Iteration 130/1000 | Loss: 0.00001213
Iteration 131/1000 | Loss: 0.00001213
Iteration 132/1000 | Loss: 0.00001213
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001213
Iteration 136/1000 | Loss: 0.00001213
Iteration 137/1000 | Loss: 0.00001213
Iteration 138/1000 | Loss: 0.00001213
Iteration 139/1000 | Loss: 0.00001213
Iteration 140/1000 | Loss: 0.00001212
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001212
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001211
Iteration 146/1000 | Loss: 0.00001211
Iteration 147/1000 | Loss: 0.00001210
Iteration 148/1000 | Loss: 0.00001210
Iteration 149/1000 | Loss: 0.00001210
Iteration 150/1000 | Loss: 0.00001210
Iteration 151/1000 | Loss: 0.00001210
Iteration 152/1000 | Loss: 0.00001210
Iteration 153/1000 | Loss: 0.00001210
Iteration 154/1000 | Loss: 0.00001210
Iteration 155/1000 | Loss: 0.00001210
Iteration 156/1000 | Loss: 0.00001209
Iteration 157/1000 | Loss: 0.00001209
Iteration 158/1000 | Loss: 0.00001209
Iteration 159/1000 | Loss: 0.00001208
Iteration 160/1000 | Loss: 0.00001208
Iteration 161/1000 | Loss: 0.00001207
Iteration 162/1000 | Loss: 0.00001207
Iteration 163/1000 | Loss: 0.00001207
Iteration 164/1000 | Loss: 0.00001206
Iteration 165/1000 | Loss: 0.00001206
Iteration 166/1000 | Loss: 0.00001206
Iteration 167/1000 | Loss: 0.00001206
Iteration 168/1000 | Loss: 0.00001206
Iteration 169/1000 | Loss: 0.00001205
Iteration 170/1000 | Loss: 0.00001205
Iteration 171/1000 | Loss: 0.00001205
Iteration 172/1000 | Loss: 0.00001205
Iteration 173/1000 | Loss: 0.00001205
Iteration 174/1000 | Loss: 0.00001204
Iteration 175/1000 | Loss: 0.00001204
Iteration 176/1000 | Loss: 0.00001204
Iteration 177/1000 | Loss: 0.00001204
Iteration 178/1000 | Loss: 0.00001204
Iteration 179/1000 | Loss: 0.00001204
Iteration 180/1000 | Loss: 0.00001204
Iteration 181/1000 | Loss: 0.00001203
Iteration 182/1000 | Loss: 0.00001203
Iteration 183/1000 | Loss: 0.00001203
Iteration 184/1000 | Loss: 0.00001203
Iteration 185/1000 | Loss: 0.00001203
Iteration 186/1000 | Loss: 0.00001203
Iteration 187/1000 | Loss: 0.00001203
Iteration 188/1000 | Loss: 0.00001203
Iteration 189/1000 | Loss: 0.00001203
Iteration 190/1000 | Loss: 0.00001203
Iteration 191/1000 | Loss: 0.00001203
Iteration 192/1000 | Loss: 0.00001203
Iteration 193/1000 | Loss: 0.00001203
Iteration 194/1000 | Loss: 0.00001202
Iteration 195/1000 | Loss: 0.00001202
Iteration 196/1000 | Loss: 0.00001202
Iteration 197/1000 | Loss: 0.00001202
Iteration 198/1000 | Loss: 0.00001202
Iteration 199/1000 | Loss: 0.00001202
Iteration 200/1000 | Loss: 0.00001202
Iteration 201/1000 | Loss: 0.00001201
Iteration 202/1000 | Loss: 0.00001201
Iteration 203/1000 | Loss: 0.00001201
Iteration 204/1000 | Loss: 0.00001201
Iteration 205/1000 | Loss: 0.00001201
Iteration 206/1000 | Loss: 0.00001201
Iteration 207/1000 | Loss: 0.00001201
Iteration 208/1000 | Loss: 0.00001201
Iteration 209/1000 | Loss: 0.00001201
Iteration 210/1000 | Loss: 0.00001200
Iteration 211/1000 | Loss: 0.00001200
Iteration 212/1000 | Loss: 0.00001200
Iteration 213/1000 | Loss: 0.00001200
Iteration 214/1000 | Loss: 0.00001200
Iteration 215/1000 | Loss: 0.00001200
Iteration 216/1000 | Loss: 0.00001200
Iteration 217/1000 | Loss: 0.00001200
Iteration 218/1000 | Loss: 0.00001200
Iteration 219/1000 | Loss: 0.00001199
Iteration 220/1000 | Loss: 0.00001199
Iteration 221/1000 | Loss: 0.00001199
Iteration 222/1000 | Loss: 0.00001199
Iteration 223/1000 | Loss: 0.00001199
Iteration 224/1000 | Loss: 0.00001199
Iteration 225/1000 | Loss: 0.00001199
Iteration 226/1000 | Loss: 0.00001198
Iteration 227/1000 | Loss: 0.00001198
Iteration 228/1000 | Loss: 0.00001198
Iteration 229/1000 | Loss: 0.00001198
Iteration 230/1000 | Loss: 0.00001198
Iteration 231/1000 | Loss: 0.00001198
Iteration 232/1000 | Loss: 0.00001198
Iteration 233/1000 | Loss: 0.00001198
Iteration 234/1000 | Loss: 0.00001198
Iteration 235/1000 | Loss: 0.00001198
Iteration 236/1000 | Loss: 0.00001198
Iteration 237/1000 | Loss: 0.00001198
Iteration 238/1000 | Loss: 0.00001198
Iteration 239/1000 | Loss: 0.00001198
Iteration 240/1000 | Loss: 0.00001198
Iteration 241/1000 | Loss: 0.00001198
Iteration 242/1000 | Loss: 0.00001198
Iteration 243/1000 | Loss: 0.00001198
Iteration 244/1000 | Loss: 0.00001198
Iteration 245/1000 | Loss: 0.00001198
Iteration 246/1000 | Loss: 0.00001198
Iteration 247/1000 | Loss: 0.00001198
Iteration 248/1000 | Loss: 0.00001198
Iteration 249/1000 | Loss: 0.00001198
Iteration 250/1000 | Loss: 0.00001198
Iteration 251/1000 | Loss: 0.00001198
Iteration 252/1000 | Loss: 0.00001198
Iteration 253/1000 | Loss: 0.00001198
Iteration 254/1000 | Loss: 0.00001198
Iteration 255/1000 | Loss: 0.00001198
Iteration 256/1000 | Loss: 0.00001198
Iteration 257/1000 | Loss: 0.00001198
Iteration 258/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.1979173905274365e-05, 1.1979173905274365e-05, 1.1979173905274365e-05, 1.1979173905274365e-05, 1.1979173905274365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1979173905274365e-05

Optimization complete. Final v2v error: 2.9751193523406982 mm

Highest mean error: 3.30409574508667 mm for frame 145

Lowest mean error: 2.798892021179199 mm for frame 67

Saving results

Total time: 50.1471688747406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865014
Iteration 2/25 | Loss: 0.00145483
Iteration 3/25 | Loss: 0.00136223
Iteration 4/25 | Loss: 0.00134338
Iteration 5/25 | Loss: 0.00133668
Iteration 6/25 | Loss: 0.00133546
Iteration 7/25 | Loss: 0.00133546
Iteration 8/25 | Loss: 0.00133546
Iteration 9/25 | Loss: 0.00133546
Iteration 10/25 | Loss: 0.00133546
Iteration 11/25 | Loss: 0.00133546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013354639522731304, 0.0013354639522731304, 0.0013354639522731304, 0.0013354639522731304, 0.0013354639522731304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013354639522731304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40226269
Iteration 2/25 | Loss: 0.00088312
Iteration 3/25 | Loss: 0.00088312
Iteration 4/25 | Loss: 0.00088312
Iteration 5/25 | Loss: 0.00088312
Iteration 6/25 | Loss: 0.00088312
Iteration 7/25 | Loss: 0.00088312
Iteration 8/25 | Loss: 0.00088312
Iteration 9/25 | Loss: 0.00088312
Iteration 10/25 | Loss: 0.00088312
Iteration 11/25 | Loss: 0.00088312
Iteration 12/25 | Loss: 0.00088312
Iteration 13/25 | Loss: 0.00088312
Iteration 14/25 | Loss: 0.00088312
Iteration 15/25 | Loss: 0.00088312
Iteration 16/25 | Loss: 0.00088312
Iteration 17/25 | Loss: 0.00088312
Iteration 18/25 | Loss: 0.00088312
Iteration 19/25 | Loss: 0.00088312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008831191225908697, 0.0008831191225908697, 0.0008831191225908697, 0.0008831191225908697, 0.0008831191225908697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008831191225908697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088312
Iteration 2/1000 | Loss: 0.00005540
Iteration 3/1000 | Loss: 0.00003503
Iteration 4/1000 | Loss: 0.00002840
Iteration 5/1000 | Loss: 0.00002627
Iteration 6/1000 | Loss: 0.00002512
Iteration 7/1000 | Loss: 0.00002416
Iteration 8/1000 | Loss: 0.00002355
Iteration 9/1000 | Loss: 0.00002295
Iteration 10/1000 | Loss: 0.00002256
Iteration 11/1000 | Loss: 0.00002231
Iteration 12/1000 | Loss: 0.00002212
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002192
Iteration 15/1000 | Loss: 0.00002190
Iteration 16/1000 | Loss: 0.00002189
Iteration 17/1000 | Loss: 0.00002188
Iteration 18/1000 | Loss: 0.00002188
Iteration 19/1000 | Loss: 0.00002187
Iteration 20/1000 | Loss: 0.00002172
Iteration 21/1000 | Loss: 0.00002171
Iteration 22/1000 | Loss: 0.00002163
Iteration 23/1000 | Loss: 0.00002157
Iteration 24/1000 | Loss: 0.00002156
Iteration 25/1000 | Loss: 0.00002155
Iteration 26/1000 | Loss: 0.00002150
Iteration 27/1000 | Loss: 0.00002150
Iteration 28/1000 | Loss: 0.00002148
Iteration 29/1000 | Loss: 0.00002148
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002147
Iteration 32/1000 | Loss: 0.00002146
Iteration 33/1000 | Loss: 0.00002144
Iteration 34/1000 | Loss: 0.00002142
Iteration 35/1000 | Loss: 0.00002142
Iteration 36/1000 | Loss: 0.00002140
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002140
Iteration 39/1000 | Loss: 0.00002140
Iteration 40/1000 | Loss: 0.00002140
Iteration 41/1000 | Loss: 0.00002139
Iteration 42/1000 | Loss: 0.00002139
Iteration 43/1000 | Loss: 0.00002139
Iteration 44/1000 | Loss: 0.00002139
Iteration 45/1000 | Loss: 0.00002139
Iteration 46/1000 | Loss: 0.00002138
Iteration 47/1000 | Loss: 0.00002138
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002135
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002134
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002132
Iteration 61/1000 | Loss: 0.00002132
Iteration 62/1000 | Loss: 0.00002132
Iteration 63/1000 | Loss: 0.00002131
Iteration 64/1000 | Loss: 0.00002131
Iteration 65/1000 | Loss: 0.00002131
Iteration 66/1000 | Loss: 0.00002130
Iteration 67/1000 | Loss: 0.00002130
Iteration 68/1000 | Loss: 0.00002130
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002129
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002127
Iteration 76/1000 | Loss: 0.00002127
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002125
Iteration 81/1000 | Loss: 0.00002125
Iteration 82/1000 | Loss: 0.00002125
Iteration 83/1000 | Loss: 0.00002125
Iteration 84/1000 | Loss: 0.00002125
Iteration 85/1000 | Loss: 0.00002125
Iteration 86/1000 | Loss: 0.00002125
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002124
Iteration 90/1000 | Loss: 0.00002124
Iteration 91/1000 | Loss: 0.00002124
Iteration 92/1000 | Loss: 0.00002124
Iteration 93/1000 | Loss: 0.00002124
Iteration 94/1000 | Loss: 0.00002124
Iteration 95/1000 | Loss: 0.00002124
Iteration 96/1000 | Loss: 0.00002124
Iteration 97/1000 | Loss: 0.00002124
Iteration 98/1000 | Loss: 0.00002124
Iteration 99/1000 | Loss: 0.00002124
Iteration 100/1000 | Loss: 0.00002124
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00002123
Iteration 103/1000 | Loss: 0.00002123
Iteration 104/1000 | Loss: 0.00002123
Iteration 105/1000 | Loss: 0.00002123
Iteration 106/1000 | Loss: 0.00002123
Iteration 107/1000 | Loss: 0.00002122
Iteration 108/1000 | Loss: 0.00002122
Iteration 109/1000 | Loss: 0.00002122
Iteration 110/1000 | Loss: 0.00002122
Iteration 111/1000 | Loss: 0.00002122
Iteration 112/1000 | Loss: 0.00002122
Iteration 113/1000 | Loss: 0.00002122
Iteration 114/1000 | Loss: 0.00002122
Iteration 115/1000 | Loss: 0.00002121
Iteration 116/1000 | Loss: 0.00002121
Iteration 117/1000 | Loss: 0.00002121
Iteration 118/1000 | Loss: 0.00002121
Iteration 119/1000 | Loss: 0.00002120
Iteration 120/1000 | Loss: 0.00002120
Iteration 121/1000 | Loss: 0.00002120
Iteration 122/1000 | Loss: 0.00002120
Iteration 123/1000 | Loss: 0.00002120
Iteration 124/1000 | Loss: 0.00002119
Iteration 125/1000 | Loss: 0.00002119
Iteration 126/1000 | Loss: 0.00002119
Iteration 127/1000 | Loss: 0.00002118
Iteration 128/1000 | Loss: 0.00002118
Iteration 129/1000 | Loss: 0.00002118
Iteration 130/1000 | Loss: 0.00002118
Iteration 131/1000 | Loss: 0.00002118
Iteration 132/1000 | Loss: 0.00002117
Iteration 133/1000 | Loss: 0.00002117
Iteration 134/1000 | Loss: 0.00002117
Iteration 135/1000 | Loss: 0.00002116
Iteration 136/1000 | Loss: 0.00002116
Iteration 137/1000 | Loss: 0.00002116
Iteration 138/1000 | Loss: 0.00002116
Iteration 139/1000 | Loss: 0.00002116
Iteration 140/1000 | Loss: 0.00002115
Iteration 141/1000 | Loss: 0.00002115
Iteration 142/1000 | Loss: 0.00002115
Iteration 143/1000 | Loss: 0.00002115
Iteration 144/1000 | Loss: 0.00002115
Iteration 145/1000 | Loss: 0.00002115
Iteration 146/1000 | Loss: 0.00002115
Iteration 147/1000 | Loss: 0.00002115
Iteration 148/1000 | Loss: 0.00002114
Iteration 149/1000 | Loss: 0.00002114
Iteration 150/1000 | Loss: 0.00002114
Iteration 151/1000 | Loss: 0.00002113
Iteration 152/1000 | Loss: 0.00002113
Iteration 153/1000 | Loss: 0.00002113
Iteration 154/1000 | Loss: 0.00002113
Iteration 155/1000 | Loss: 0.00002113
Iteration 156/1000 | Loss: 0.00002113
Iteration 157/1000 | Loss: 0.00002113
Iteration 158/1000 | Loss: 0.00002113
Iteration 159/1000 | Loss: 0.00002113
Iteration 160/1000 | Loss: 0.00002112
Iteration 161/1000 | Loss: 0.00002112
Iteration 162/1000 | Loss: 0.00002112
Iteration 163/1000 | Loss: 0.00002112
Iteration 164/1000 | Loss: 0.00002112
Iteration 165/1000 | Loss: 0.00002112
Iteration 166/1000 | Loss: 0.00002112
Iteration 167/1000 | Loss: 0.00002112
Iteration 168/1000 | Loss: 0.00002112
Iteration 169/1000 | Loss: 0.00002112
Iteration 170/1000 | Loss: 0.00002112
Iteration 171/1000 | Loss: 0.00002112
Iteration 172/1000 | Loss: 0.00002112
Iteration 173/1000 | Loss: 0.00002112
Iteration 174/1000 | Loss: 0.00002112
Iteration 175/1000 | Loss: 0.00002112
Iteration 176/1000 | Loss: 0.00002112
Iteration 177/1000 | Loss: 0.00002111
Iteration 178/1000 | Loss: 0.00002111
Iteration 179/1000 | Loss: 0.00002111
Iteration 180/1000 | Loss: 0.00002111
Iteration 181/1000 | Loss: 0.00002111
Iteration 182/1000 | Loss: 0.00002111
Iteration 183/1000 | Loss: 0.00002111
Iteration 184/1000 | Loss: 0.00002111
Iteration 185/1000 | Loss: 0.00002111
Iteration 186/1000 | Loss: 0.00002111
Iteration 187/1000 | Loss: 0.00002111
Iteration 188/1000 | Loss: 0.00002111
Iteration 189/1000 | Loss: 0.00002111
Iteration 190/1000 | Loss: 0.00002111
Iteration 191/1000 | Loss: 0.00002110
Iteration 192/1000 | Loss: 0.00002110
Iteration 193/1000 | Loss: 0.00002110
Iteration 194/1000 | Loss: 0.00002110
Iteration 195/1000 | Loss: 0.00002110
Iteration 196/1000 | Loss: 0.00002110
Iteration 197/1000 | Loss: 0.00002110
Iteration 198/1000 | Loss: 0.00002110
Iteration 199/1000 | Loss: 0.00002110
Iteration 200/1000 | Loss: 0.00002110
Iteration 201/1000 | Loss: 0.00002110
Iteration 202/1000 | Loss: 0.00002110
Iteration 203/1000 | Loss: 0.00002110
Iteration 204/1000 | Loss: 0.00002110
Iteration 205/1000 | Loss: 0.00002110
Iteration 206/1000 | Loss: 0.00002110
Iteration 207/1000 | Loss: 0.00002110
Iteration 208/1000 | Loss: 0.00002110
Iteration 209/1000 | Loss: 0.00002110
Iteration 210/1000 | Loss: 0.00002110
Iteration 211/1000 | Loss: 0.00002110
Iteration 212/1000 | Loss: 0.00002110
Iteration 213/1000 | Loss: 0.00002110
Iteration 214/1000 | Loss: 0.00002110
Iteration 215/1000 | Loss: 0.00002110
Iteration 216/1000 | Loss: 0.00002110
Iteration 217/1000 | Loss: 0.00002110
Iteration 218/1000 | Loss: 0.00002110
Iteration 219/1000 | Loss: 0.00002110
Iteration 220/1000 | Loss: 0.00002110
Iteration 221/1000 | Loss: 0.00002110
Iteration 222/1000 | Loss: 0.00002110
Iteration 223/1000 | Loss: 0.00002110
Iteration 224/1000 | Loss: 0.00002110
Iteration 225/1000 | Loss: 0.00002110
Iteration 226/1000 | Loss: 0.00002110
Iteration 227/1000 | Loss: 0.00002110
Iteration 228/1000 | Loss: 0.00002110
Iteration 229/1000 | Loss: 0.00002110
Iteration 230/1000 | Loss: 0.00002110
Iteration 231/1000 | Loss: 0.00002110
Iteration 232/1000 | Loss: 0.00002110
Iteration 233/1000 | Loss: 0.00002110
Iteration 234/1000 | Loss: 0.00002110
Iteration 235/1000 | Loss: 0.00002110
Iteration 236/1000 | Loss: 0.00002110
Iteration 237/1000 | Loss: 0.00002110
Iteration 238/1000 | Loss: 0.00002110
Iteration 239/1000 | Loss: 0.00002110
Iteration 240/1000 | Loss: 0.00002110
Iteration 241/1000 | Loss: 0.00002110
Iteration 242/1000 | Loss: 0.00002110
Iteration 243/1000 | Loss: 0.00002110
Iteration 244/1000 | Loss: 0.00002110
Iteration 245/1000 | Loss: 0.00002110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [2.1103909602970816e-05, 2.1103909602970816e-05, 2.1103909602970816e-05, 2.1103909602970816e-05, 2.1103909602970816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1103909602970816e-05

Optimization complete. Final v2v error: 3.8449552059173584 mm

Highest mean error: 5.342970371246338 mm for frame 67

Lowest mean error: 3.2496116161346436 mm for frame 98

Saving results

Total time: 42.94541025161743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461228
Iteration 2/25 | Loss: 0.00148512
Iteration 3/25 | Loss: 0.00136496
Iteration 4/25 | Loss: 0.00135698
Iteration 5/25 | Loss: 0.00135487
Iteration 6/25 | Loss: 0.00135420
Iteration 7/25 | Loss: 0.00135420
Iteration 8/25 | Loss: 0.00135420
Iteration 9/25 | Loss: 0.00135420
Iteration 10/25 | Loss: 0.00135420
Iteration 11/25 | Loss: 0.00135420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013542022788897157, 0.0013542022788897157, 0.0013542022788897157, 0.0013542022788897157, 0.0013542022788897157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013542022788897157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46815312
Iteration 2/25 | Loss: 0.00090137
Iteration 3/25 | Loss: 0.00090136
Iteration 4/25 | Loss: 0.00090136
Iteration 5/25 | Loss: 0.00090136
Iteration 6/25 | Loss: 0.00090136
Iteration 7/25 | Loss: 0.00090136
Iteration 8/25 | Loss: 0.00090136
Iteration 9/25 | Loss: 0.00090136
Iteration 10/25 | Loss: 0.00090136
Iteration 11/25 | Loss: 0.00090136
Iteration 12/25 | Loss: 0.00090136
Iteration 13/25 | Loss: 0.00090136
Iteration 14/25 | Loss: 0.00090136
Iteration 15/25 | Loss: 0.00090136
Iteration 16/25 | Loss: 0.00090136
Iteration 17/25 | Loss: 0.00090136
Iteration 18/25 | Loss: 0.00090136
Iteration 19/25 | Loss: 0.00090136
Iteration 20/25 | Loss: 0.00090136
Iteration 21/25 | Loss: 0.00090136
Iteration 22/25 | Loss: 0.00090136
Iteration 23/25 | Loss: 0.00090136
Iteration 24/25 | Loss: 0.00090136
Iteration 25/25 | Loss: 0.00090136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090136
Iteration 2/1000 | Loss: 0.00004523
Iteration 3/1000 | Loss: 0.00003003
Iteration 4/1000 | Loss: 0.00002341
Iteration 5/1000 | Loss: 0.00002205
Iteration 6/1000 | Loss: 0.00002106
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00001986
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001922
Iteration 11/1000 | Loss: 0.00001899
Iteration 12/1000 | Loss: 0.00001878
Iteration 13/1000 | Loss: 0.00001865
Iteration 14/1000 | Loss: 0.00001850
Iteration 15/1000 | Loss: 0.00001835
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001819
Iteration 19/1000 | Loss: 0.00001816
Iteration 20/1000 | Loss: 0.00001812
Iteration 21/1000 | Loss: 0.00001812
Iteration 22/1000 | Loss: 0.00001812
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001811
Iteration 25/1000 | Loss: 0.00001808
Iteration 26/1000 | Loss: 0.00001808
Iteration 27/1000 | Loss: 0.00001808
Iteration 28/1000 | Loss: 0.00001808
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001808
Iteration 33/1000 | Loss: 0.00001808
Iteration 34/1000 | Loss: 0.00001808
Iteration 35/1000 | Loss: 0.00001808
Iteration 36/1000 | Loss: 0.00001807
Iteration 37/1000 | Loss: 0.00001807
Iteration 38/1000 | Loss: 0.00001807
Iteration 39/1000 | Loss: 0.00001807
Iteration 40/1000 | Loss: 0.00001807
Iteration 41/1000 | Loss: 0.00001807
Iteration 42/1000 | Loss: 0.00001806
Iteration 43/1000 | Loss: 0.00001806
Iteration 44/1000 | Loss: 0.00001804
Iteration 45/1000 | Loss: 0.00001804
Iteration 46/1000 | Loss: 0.00001803
Iteration 47/1000 | Loss: 0.00001803
Iteration 48/1000 | Loss: 0.00001803
Iteration 49/1000 | Loss: 0.00001802
Iteration 50/1000 | Loss: 0.00001802
Iteration 51/1000 | Loss: 0.00001800
Iteration 52/1000 | Loss: 0.00001798
Iteration 53/1000 | Loss: 0.00001798
Iteration 54/1000 | Loss: 0.00001797
Iteration 55/1000 | Loss: 0.00001796
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001795
Iteration 59/1000 | Loss: 0.00001795
Iteration 60/1000 | Loss: 0.00001794
Iteration 61/1000 | Loss: 0.00001794
Iteration 62/1000 | Loss: 0.00001794
Iteration 63/1000 | Loss: 0.00001794
Iteration 64/1000 | Loss: 0.00001794
Iteration 65/1000 | Loss: 0.00001794
Iteration 66/1000 | Loss: 0.00001794
Iteration 67/1000 | Loss: 0.00001794
Iteration 68/1000 | Loss: 0.00001794
Iteration 69/1000 | Loss: 0.00001793
Iteration 70/1000 | Loss: 0.00001792
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001789
Iteration 77/1000 | Loss: 0.00001789
Iteration 78/1000 | Loss: 0.00001789
Iteration 79/1000 | Loss: 0.00001788
Iteration 80/1000 | Loss: 0.00001788
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001784
Iteration 89/1000 | Loss: 0.00001784
Iteration 90/1000 | Loss: 0.00001784
Iteration 91/1000 | Loss: 0.00001783
Iteration 92/1000 | Loss: 0.00001783
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001781
Iteration 96/1000 | Loss: 0.00001781
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001781
Iteration 99/1000 | Loss: 0.00001781
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001780
Iteration 103/1000 | Loss: 0.00001780
Iteration 104/1000 | Loss: 0.00001780
Iteration 105/1000 | Loss: 0.00001780
Iteration 106/1000 | Loss: 0.00001780
Iteration 107/1000 | Loss: 0.00001779
Iteration 108/1000 | Loss: 0.00001779
Iteration 109/1000 | Loss: 0.00001779
Iteration 110/1000 | Loss: 0.00001779
Iteration 111/1000 | Loss: 0.00001779
Iteration 112/1000 | Loss: 0.00001779
Iteration 113/1000 | Loss: 0.00001779
Iteration 114/1000 | Loss: 0.00001778
Iteration 115/1000 | Loss: 0.00001778
Iteration 116/1000 | Loss: 0.00001778
Iteration 117/1000 | Loss: 0.00001778
Iteration 118/1000 | Loss: 0.00001778
Iteration 119/1000 | Loss: 0.00001778
Iteration 120/1000 | Loss: 0.00001778
Iteration 121/1000 | Loss: 0.00001778
Iteration 122/1000 | Loss: 0.00001778
Iteration 123/1000 | Loss: 0.00001778
Iteration 124/1000 | Loss: 0.00001778
Iteration 125/1000 | Loss: 0.00001778
Iteration 126/1000 | Loss: 0.00001778
Iteration 127/1000 | Loss: 0.00001778
Iteration 128/1000 | Loss: 0.00001778
Iteration 129/1000 | Loss: 0.00001778
Iteration 130/1000 | Loss: 0.00001778
Iteration 131/1000 | Loss: 0.00001778
Iteration 132/1000 | Loss: 0.00001778
Iteration 133/1000 | Loss: 0.00001778
Iteration 134/1000 | Loss: 0.00001778
Iteration 135/1000 | Loss: 0.00001778
Iteration 136/1000 | Loss: 0.00001778
Iteration 137/1000 | Loss: 0.00001778
Iteration 138/1000 | Loss: 0.00001778
Iteration 139/1000 | Loss: 0.00001778
Iteration 140/1000 | Loss: 0.00001778
Iteration 141/1000 | Loss: 0.00001778
Iteration 142/1000 | Loss: 0.00001778
Iteration 143/1000 | Loss: 0.00001778
Iteration 144/1000 | Loss: 0.00001778
Iteration 145/1000 | Loss: 0.00001778
Iteration 146/1000 | Loss: 0.00001778
Iteration 147/1000 | Loss: 0.00001778
Iteration 148/1000 | Loss: 0.00001778
Iteration 149/1000 | Loss: 0.00001778
Iteration 150/1000 | Loss: 0.00001778
Iteration 151/1000 | Loss: 0.00001778
Iteration 152/1000 | Loss: 0.00001778
Iteration 153/1000 | Loss: 0.00001778
Iteration 154/1000 | Loss: 0.00001778
Iteration 155/1000 | Loss: 0.00001778
Iteration 156/1000 | Loss: 0.00001778
Iteration 157/1000 | Loss: 0.00001778
Iteration 158/1000 | Loss: 0.00001778
Iteration 159/1000 | Loss: 0.00001778
Iteration 160/1000 | Loss: 0.00001778
Iteration 161/1000 | Loss: 0.00001778
Iteration 162/1000 | Loss: 0.00001778
Iteration 163/1000 | Loss: 0.00001778
Iteration 164/1000 | Loss: 0.00001778
Iteration 165/1000 | Loss: 0.00001778
Iteration 166/1000 | Loss: 0.00001778
Iteration 167/1000 | Loss: 0.00001778
Iteration 168/1000 | Loss: 0.00001778
Iteration 169/1000 | Loss: 0.00001778
Iteration 170/1000 | Loss: 0.00001778
Iteration 171/1000 | Loss: 0.00001778
Iteration 172/1000 | Loss: 0.00001778
Iteration 173/1000 | Loss: 0.00001778
Iteration 174/1000 | Loss: 0.00001778
Iteration 175/1000 | Loss: 0.00001778
Iteration 176/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.7776281310943887e-05, 1.7776281310943887e-05, 1.7776281310943887e-05, 1.7776281310943887e-05, 1.7776281310943887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7776281310943887e-05

Optimization complete. Final v2v error: 3.491360664367676 mm

Highest mean error: 4.001179218292236 mm for frame 15

Lowest mean error: 2.7785468101501465 mm for frame 0

Saving results

Total time: 40.85848927497864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821540
Iteration 2/25 | Loss: 0.00153321
Iteration 3/25 | Loss: 0.00132448
Iteration 4/25 | Loss: 0.00129823
Iteration 5/25 | Loss: 0.00129383
Iteration 6/25 | Loss: 0.00129310
Iteration 7/25 | Loss: 0.00129310
Iteration 8/25 | Loss: 0.00129310
Iteration 9/25 | Loss: 0.00129310
Iteration 10/25 | Loss: 0.00129310
Iteration 11/25 | Loss: 0.00129310
Iteration 12/25 | Loss: 0.00129310
Iteration 13/25 | Loss: 0.00129310
Iteration 14/25 | Loss: 0.00129310
Iteration 15/25 | Loss: 0.00129310
Iteration 16/25 | Loss: 0.00129310
Iteration 17/25 | Loss: 0.00129310
Iteration 18/25 | Loss: 0.00129310
Iteration 19/25 | Loss: 0.00129310
Iteration 20/25 | Loss: 0.00129310
Iteration 21/25 | Loss: 0.00129310
Iteration 22/25 | Loss: 0.00129310
Iteration 23/25 | Loss: 0.00129310
Iteration 24/25 | Loss: 0.00129310
Iteration 25/25 | Loss: 0.00129310

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40041006
Iteration 2/25 | Loss: 0.00084111
Iteration 3/25 | Loss: 0.00084111
Iteration 4/25 | Loss: 0.00084111
Iteration 5/25 | Loss: 0.00084111
Iteration 6/25 | Loss: 0.00084111
Iteration 7/25 | Loss: 0.00084111
Iteration 8/25 | Loss: 0.00084111
Iteration 9/25 | Loss: 0.00084111
Iteration 10/25 | Loss: 0.00084111
Iteration 11/25 | Loss: 0.00084111
Iteration 12/25 | Loss: 0.00084111
Iteration 13/25 | Loss: 0.00084111
Iteration 14/25 | Loss: 0.00084111
Iteration 15/25 | Loss: 0.00084111
Iteration 16/25 | Loss: 0.00084111
Iteration 17/25 | Loss: 0.00084111
Iteration 18/25 | Loss: 0.00084111
Iteration 19/25 | Loss: 0.00084111
Iteration 20/25 | Loss: 0.00084111
Iteration 21/25 | Loss: 0.00084111
Iteration 22/25 | Loss: 0.00084111
Iteration 23/25 | Loss: 0.00084111
Iteration 24/25 | Loss: 0.00084111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008411102462559938, 0.0008411102462559938, 0.0008411102462559938, 0.0008411102462559938, 0.0008411102462559938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008411102462559938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084111
Iteration 2/1000 | Loss: 0.00003911
Iteration 3/1000 | Loss: 0.00002373
Iteration 4/1000 | Loss: 0.00001990
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001744
Iteration 7/1000 | Loss: 0.00001650
Iteration 8/1000 | Loss: 0.00001605
Iteration 9/1000 | Loss: 0.00001564
Iteration 10/1000 | Loss: 0.00001530
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001482
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001458
Iteration 22/1000 | Loss: 0.00001453
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001450
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001449
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001447
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001445
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001444
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001443
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001442
Iteration 48/1000 | Loss: 0.00001442
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001441
Iteration 51/1000 | Loss: 0.00001441
Iteration 52/1000 | Loss: 0.00001440
Iteration 53/1000 | Loss: 0.00001440
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001439
Iteration 56/1000 | Loss: 0.00001439
Iteration 57/1000 | Loss: 0.00001439
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001438
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001437
Iteration 65/1000 | Loss: 0.00001437
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001436
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001433
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001432
Iteration 81/1000 | Loss: 0.00001432
Iteration 82/1000 | Loss: 0.00001432
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001431
Iteration 86/1000 | Loss: 0.00001430
Iteration 87/1000 | Loss: 0.00001430
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001430
Iteration 90/1000 | Loss: 0.00001430
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001425
Iteration 109/1000 | Loss: 0.00001425
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001424
Iteration 112/1000 | Loss: 0.00001424
Iteration 113/1000 | Loss: 0.00001424
Iteration 114/1000 | Loss: 0.00001424
Iteration 115/1000 | Loss: 0.00001424
Iteration 116/1000 | Loss: 0.00001424
Iteration 117/1000 | Loss: 0.00001424
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001423
Iteration 120/1000 | Loss: 0.00001423
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001422
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001420
Iteration 138/1000 | Loss: 0.00001420
Iteration 139/1000 | Loss: 0.00001420
Iteration 140/1000 | Loss: 0.00001420
Iteration 141/1000 | Loss: 0.00001420
Iteration 142/1000 | Loss: 0.00001420
Iteration 143/1000 | Loss: 0.00001420
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001418
Iteration 153/1000 | Loss: 0.00001418
Iteration 154/1000 | Loss: 0.00001418
Iteration 155/1000 | Loss: 0.00001417
Iteration 156/1000 | Loss: 0.00001417
Iteration 157/1000 | Loss: 0.00001417
Iteration 158/1000 | Loss: 0.00001417
Iteration 159/1000 | Loss: 0.00001417
Iteration 160/1000 | Loss: 0.00001417
Iteration 161/1000 | Loss: 0.00001417
Iteration 162/1000 | Loss: 0.00001417
Iteration 163/1000 | Loss: 0.00001417
Iteration 164/1000 | Loss: 0.00001416
Iteration 165/1000 | Loss: 0.00001416
Iteration 166/1000 | Loss: 0.00001416
Iteration 167/1000 | Loss: 0.00001416
Iteration 168/1000 | Loss: 0.00001416
Iteration 169/1000 | Loss: 0.00001416
Iteration 170/1000 | Loss: 0.00001416
Iteration 171/1000 | Loss: 0.00001416
Iteration 172/1000 | Loss: 0.00001416
Iteration 173/1000 | Loss: 0.00001416
Iteration 174/1000 | Loss: 0.00001416
Iteration 175/1000 | Loss: 0.00001416
Iteration 176/1000 | Loss: 0.00001416
Iteration 177/1000 | Loss: 0.00001416
Iteration 178/1000 | Loss: 0.00001416
Iteration 179/1000 | Loss: 0.00001416
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Iteration 183/1000 | Loss: 0.00001415
Iteration 184/1000 | Loss: 0.00001415
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001415
Iteration 187/1000 | Loss: 0.00001415
Iteration 188/1000 | Loss: 0.00001415
Iteration 189/1000 | Loss: 0.00001415
Iteration 190/1000 | Loss: 0.00001414
Iteration 191/1000 | Loss: 0.00001414
Iteration 192/1000 | Loss: 0.00001414
Iteration 193/1000 | Loss: 0.00001414
Iteration 194/1000 | Loss: 0.00001414
Iteration 195/1000 | Loss: 0.00001414
Iteration 196/1000 | Loss: 0.00001414
Iteration 197/1000 | Loss: 0.00001414
Iteration 198/1000 | Loss: 0.00001413
Iteration 199/1000 | Loss: 0.00001413
Iteration 200/1000 | Loss: 0.00001413
Iteration 201/1000 | Loss: 0.00001413
Iteration 202/1000 | Loss: 0.00001413
Iteration 203/1000 | Loss: 0.00001413
Iteration 204/1000 | Loss: 0.00001413
Iteration 205/1000 | Loss: 0.00001412
Iteration 206/1000 | Loss: 0.00001412
Iteration 207/1000 | Loss: 0.00001412
Iteration 208/1000 | Loss: 0.00001412
Iteration 209/1000 | Loss: 0.00001411
Iteration 210/1000 | Loss: 0.00001411
Iteration 211/1000 | Loss: 0.00001411
Iteration 212/1000 | Loss: 0.00001411
Iteration 213/1000 | Loss: 0.00001411
Iteration 214/1000 | Loss: 0.00001411
Iteration 215/1000 | Loss: 0.00001411
Iteration 216/1000 | Loss: 0.00001411
Iteration 217/1000 | Loss: 0.00001411
Iteration 218/1000 | Loss: 0.00001411
Iteration 219/1000 | Loss: 0.00001411
Iteration 220/1000 | Loss: 0.00001411
Iteration 221/1000 | Loss: 0.00001411
Iteration 222/1000 | Loss: 0.00001411
Iteration 223/1000 | Loss: 0.00001411
Iteration 224/1000 | Loss: 0.00001411
Iteration 225/1000 | Loss: 0.00001411
Iteration 226/1000 | Loss: 0.00001411
Iteration 227/1000 | Loss: 0.00001410
Iteration 228/1000 | Loss: 0.00001410
Iteration 229/1000 | Loss: 0.00001410
Iteration 230/1000 | Loss: 0.00001410
Iteration 231/1000 | Loss: 0.00001410
Iteration 232/1000 | Loss: 0.00001410
Iteration 233/1000 | Loss: 0.00001410
Iteration 234/1000 | Loss: 0.00001410
Iteration 235/1000 | Loss: 0.00001410
Iteration 236/1000 | Loss: 0.00001410
Iteration 237/1000 | Loss: 0.00001410
Iteration 238/1000 | Loss: 0.00001410
Iteration 239/1000 | Loss: 0.00001410
Iteration 240/1000 | Loss: 0.00001410
Iteration 241/1000 | Loss: 0.00001410
Iteration 242/1000 | Loss: 0.00001410
Iteration 243/1000 | Loss: 0.00001410
Iteration 244/1000 | Loss: 0.00001410
Iteration 245/1000 | Loss: 0.00001410
Iteration 246/1000 | Loss: 0.00001410
Iteration 247/1000 | Loss: 0.00001410
Iteration 248/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.4101382475928403e-05, 1.4101382475928403e-05, 1.4101382475928403e-05, 1.4101382475928403e-05, 1.4101382475928403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4101382475928403e-05

Optimization complete. Final v2v error: 3.208073139190674 mm

Highest mean error: 3.961637496948242 mm for frame 114

Lowest mean error: 2.827636241912842 mm for frame 33

Saving results

Total time: 47.84530973434448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022826
Iteration 2/25 | Loss: 0.00291219
Iteration 3/25 | Loss: 0.00251611
Iteration 4/25 | Loss: 0.00245269
Iteration 5/25 | Loss: 0.00220823
Iteration 6/25 | Loss: 0.00193680
Iteration 7/25 | Loss: 0.00174527
Iteration 8/25 | Loss: 0.00168618
Iteration 9/25 | Loss: 0.00165636
Iteration 10/25 | Loss: 0.00160150
Iteration 11/25 | Loss: 0.00157997
Iteration 12/25 | Loss: 0.00156181
Iteration 13/25 | Loss: 0.00155531
Iteration 14/25 | Loss: 0.00155804
Iteration 15/25 | Loss: 0.00154486
Iteration 16/25 | Loss: 0.00153100
Iteration 17/25 | Loss: 0.00152548
Iteration 18/25 | Loss: 0.00153006
Iteration 19/25 | Loss: 0.00154712
Iteration 20/25 | Loss: 0.00150193
Iteration 21/25 | Loss: 0.00148665
Iteration 22/25 | Loss: 0.00149545
Iteration 23/25 | Loss: 0.00151064
Iteration 24/25 | Loss: 0.00153039
Iteration 25/25 | Loss: 0.00147969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34155619
Iteration 2/25 | Loss: 0.00248612
Iteration 3/25 | Loss: 0.00155709
Iteration 4/25 | Loss: 0.00155708
Iteration 5/25 | Loss: 0.00155708
Iteration 6/25 | Loss: 0.00155708
Iteration 7/25 | Loss: 0.00155708
Iteration 8/25 | Loss: 0.00155708
Iteration 9/25 | Loss: 0.00155708
Iteration 10/25 | Loss: 0.00155708
Iteration 11/25 | Loss: 0.00155708
Iteration 12/25 | Loss: 0.00155708
Iteration 13/25 | Loss: 0.00155708
Iteration 14/25 | Loss: 0.00155708
Iteration 15/25 | Loss: 0.00155708
Iteration 16/25 | Loss: 0.00155708
Iteration 17/25 | Loss: 0.00155708
Iteration 18/25 | Loss: 0.00155708
Iteration 19/25 | Loss: 0.00155708
Iteration 20/25 | Loss: 0.00155708
Iteration 21/25 | Loss: 0.00155708
Iteration 22/25 | Loss: 0.00155708
Iteration 23/25 | Loss: 0.00155708
Iteration 24/25 | Loss: 0.00155708
Iteration 25/25 | Loss: 0.00155708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155708
Iteration 2/1000 | Loss: 0.00040842
Iteration 3/1000 | Loss: 0.00113789
Iteration 4/1000 | Loss: 0.00103563
Iteration 5/1000 | Loss: 0.00041559
Iteration 6/1000 | Loss: 0.00043687
Iteration 7/1000 | Loss: 0.00013947
Iteration 8/1000 | Loss: 0.00020207
Iteration 9/1000 | Loss: 0.00045007
Iteration 10/1000 | Loss: 0.00009460
Iteration 11/1000 | Loss: 0.00042783
Iteration 12/1000 | Loss: 0.00040984
Iteration 13/1000 | Loss: 0.00067337
Iteration 14/1000 | Loss: 0.00026365
Iteration 15/1000 | Loss: 0.00028853
Iteration 16/1000 | Loss: 0.00052862
Iteration 17/1000 | Loss: 0.00144219
Iteration 18/1000 | Loss: 0.00055549
Iteration 19/1000 | Loss: 0.00057997
Iteration 20/1000 | Loss: 0.00014390
Iteration 21/1000 | Loss: 0.00005502
Iteration 22/1000 | Loss: 0.00015951
Iteration 23/1000 | Loss: 0.00045844
Iteration 24/1000 | Loss: 0.00011705
Iteration 25/1000 | Loss: 0.00017333
Iteration 26/1000 | Loss: 0.00010349
Iteration 27/1000 | Loss: 0.00011900
Iteration 28/1000 | Loss: 0.00016406
Iteration 29/1000 | Loss: 0.00033501
Iteration 30/1000 | Loss: 0.00018767
Iteration 31/1000 | Loss: 0.00073296
Iteration 32/1000 | Loss: 0.00023176
Iteration 33/1000 | Loss: 0.00074106
Iteration 34/1000 | Loss: 0.00061531
Iteration 35/1000 | Loss: 0.00041900
Iteration 36/1000 | Loss: 0.00006370
Iteration 37/1000 | Loss: 0.00019726
Iteration 38/1000 | Loss: 0.00011090
Iteration 39/1000 | Loss: 0.00004772
Iteration 40/1000 | Loss: 0.00004667
Iteration 41/1000 | Loss: 0.00011003
Iteration 42/1000 | Loss: 0.00035973
Iteration 43/1000 | Loss: 0.00017023
Iteration 44/1000 | Loss: 0.00050996
Iteration 45/1000 | Loss: 0.00008205
Iteration 46/1000 | Loss: 0.00014965
Iteration 47/1000 | Loss: 0.00006414
Iteration 48/1000 | Loss: 0.00019443
Iteration 49/1000 | Loss: 0.00006957
Iteration 50/1000 | Loss: 0.00060993
Iteration 51/1000 | Loss: 0.00023968
Iteration 52/1000 | Loss: 0.00027357
Iteration 53/1000 | Loss: 0.00030019
Iteration 54/1000 | Loss: 0.00030295
Iteration 55/1000 | Loss: 0.00023957
Iteration 56/1000 | Loss: 0.00017283
Iteration 57/1000 | Loss: 0.00021949
Iteration 58/1000 | Loss: 0.00010091
Iteration 59/1000 | Loss: 0.00054044
Iteration 60/1000 | Loss: 0.00010740
Iteration 61/1000 | Loss: 0.00004778
Iteration 62/1000 | Loss: 0.00028021
Iteration 63/1000 | Loss: 0.00020348
Iteration 64/1000 | Loss: 0.00012272
Iteration 65/1000 | Loss: 0.00006526
Iteration 66/1000 | Loss: 0.00032547
Iteration 67/1000 | Loss: 0.00009431
Iteration 68/1000 | Loss: 0.00006609
Iteration 69/1000 | Loss: 0.00005772
Iteration 70/1000 | Loss: 0.00006340
Iteration 71/1000 | Loss: 0.00049117
Iteration 72/1000 | Loss: 0.00020881
Iteration 73/1000 | Loss: 0.00047793
Iteration 74/1000 | Loss: 0.00083397
Iteration 75/1000 | Loss: 0.00021032
Iteration 76/1000 | Loss: 0.00060565
Iteration 77/1000 | Loss: 0.00024114
Iteration 78/1000 | Loss: 0.00029385
Iteration 79/1000 | Loss: 0.00016747
Iteration 80/1000 | Loss: 0.00021079
Iteration 81/1000 | Loss: 0.00008211
Iteration 82/1000 | Loss: 0.00011061
Iteration 83/1000 | Loss: 0.00004307
Iteration 84/1000 | Loss: 0.00034214
Iteration 85/1000 | Loss: 0.00023069
Iteration 86/1000 | Loss: 0.00120576
Iteration 87/1000 | Loss: 0.00025751
Iteration 88/1000 | Loss: 0.00018959
Iteration 89/1000 | Loss: 0.00089545
Iteration 90/1000 | Loss: 0.00008018
Iteration 91/1000 | Loss: 0.00031910
Iteration 92/1000 | Loss: 0.00004375
Iteration 93/1000 | Loss: 0.00024609
Iteration 94/1000 | Loss: 0.00050670
Iteration 95/1000 | Loss: 0.00009382
Iteration 96/1000 | Loss: 0.00010191
Iteration 97/1000 | Loss: 0.00012932
Iteration 98/1000 | Loss: 0.00003613
Iteration 99/1000 | Loss: 0.00042770
Iteration 100/1000 | Loss: 0.00003614
Iteration 101/1000 | Loss: 0.00020654
Iteration 102/1000 | Loss: 0.00049954
Iteration 103/1000 | Loss: 0.00015487
Iteration 104/1000 | Loss: 0.00043366
Iteration 105/1000 | Loss: 0.00011176
Iteration 106/1000 | Loss: 0.00007017
Iteration 107/1000 | Loss: 0.00004614
Iteration 108/1000 | Loss: 0.00017765
Iteration 109/1000 | Loss: 0.00007149
Iteration 110/1000 | Loss: 0.00004958
Iteration 111/1000 | Loss: 0.00022702
Iteration 112/1000 | Loss: 0.00009143
Iteration 113/1000 | Loss: 0.00023007
Iteration 114/1000 | Loss: 0.00011155
Iteration 115/1000 | Loss: 0.00015124
Iteration 116/1000 | Loss: 0.00020443
Iteration 117/1000 | Loss: 0.00015681
Iteration 118/1000 | Loss: 0.00004169
Iteration 119/1000 | Loss: 0.00004123
Iteration 120/1000 | Loss: 0.00003786
Iteration 121/1000 | Loss: 0.00005028
Iteration 122/1000 | Loss: 0.00004437
Iteration 123/1000 | Loss: 0.00003942
Iteration 124/1000 | Loss: 0.00021409
Iteration 125/1000 | Loss: 0.00005634
Iteration 126/1000 | Loss: 0.00003912
Iteration 127/1000 | Loss: 0.00006890
Iteration 128/1000 | Loss: 0.00028819
Iteration 129/1000 | Loss: 0.00004873
Iteration 130/1000 | Loss: 0.00004716
Iteration 131/1000 | Loss: 0.00005545
Iteration 132/1000 | Loss: 0.00006537
Iteration 133/1000 | Loss: 0.00012661
Iteration 134/1000 | Loss: 0.00030953
Iteration 135/1000 | Loss: 0.00011692
Iteration 136/1000 | Loss: 0.00011031
Iteration 137/1000 | Loss: 0.00006934
Iteration 138/1000 | Loss: 0.00014652
Iteration 139/1000 | Loss: 0.00007900
Iteration 140/1000 | Loss: 0.00004654
Iteration 141/1000 | Loss: 0.00020708
Iteration 142/1000 | Loss: 0.00030945
Iteration 143/1000 | Loss: 0.00014733
Iteration 144/1000 | Loss: 0.00021610
Iteration 145/1000 | Loss: 0.00015789
Iteration 146/1000 | Loss: 0.00019653
Iteration 147/1000 | Loss: 0.00013753
Iteration 148/1000 | Loss: 0.00006880
Iteration 149/1000 | Loss: 0.00003730
Iteration 150/1000 | Loss: 0.00006226
Iteration 151/1000 | Loss: 0.00004746
Iteration 152/1000 | Loss: 0.00003492
Iteration 153/1000 | Loss: 0.00009974
Iteration 154/1000 | Loss: 0.00003454
Iteration 155/1000 | Loss: 0.00016846
Iteration 156/1000 | Loss: 0.00008594
Iteration 157/1000 | Loss: 0.00007115
Iteration 158/1000 | Loss: 0.00004108
Iteration 159/1000 | Loss: 0.00031509
Iteration 160/1000 | Loss: 0.00006476
Iteration 161/1000 | Loss: 0.00023546
Iteration 162/1000 | Loss: 0.00012592
Iteration 163/1000 | Loss: 0.00015919
Iteration 164/1000 | Loss: 0.00012157
Iteration 165/1000 | Loss: 0.00008242
Iteration 166/1000 | Loss: 0.00009668
Iteration 167/1000 | Loss: 0.00042562
Iteration 168/1000 | Loss: 0.00014309
Iteration 169/1000 | Loss: 0.00012676
Iteration 170/1000 | Loss: 0.00027777
Iteration 171/1000 | Loss: 0.00011713
Iteration 172/1000 | Loss: 0.00004006
Iteration 173/1000 | Loss: 0.00008855
Iteration 174/1000 | Loss: 0.00011169
Iteration 175/1000 | Loss: 0.00004182
Iteration 176/1000 | Loss: 0.00005794
Iteration 177/1000 | Loss: 0.00003375
Iteration 178/1000 | Loss: 0.00004246
Iteration 179/1000 | Loss: 0.00008999
Iteration 180/1000 | Loss: 0.00003921
Iteration 181/1000 | Loss: 0.00024593
Iteration 182/1000 | Loss: 0.00032165
Iteration 183/1000 | Loss: 0.00007659
Iteration 184/1000 | Loss: 0.00008692
Iteration 185/1000 | Loss: 0.00003533
Iteration 186/1000 | Loss: 0.00003156
Iteration 187/1000 | Loss: 0.00004503
Iteration 188/1000 | Loss: 0.00003117
Iteration 189/1000 | Loss: 0.00007547
Iteration 190/1000 | Loss: 0.00011491
Iteration 191/1000 | Loss: 0.00006521
Iteration 192/1000 | Loss: 0.00003150
Iteration 193/1000 | Loss: 0.00003220
Iteration 194/1000 | Loss: 0.00003098
Iteration 195/1000 | Loss: 0.00003098
Iteration 196/1000 | Loss: 0.00003097
Iteration 197/1000 | Loss: 0.00003096
Iteration 198/1000 | Loss: 0.00003096
Iteration 199/1000 | Loss: 0.00003324
Iteration 200/1000 | Loss: 0.00003091
Iteration 201/1000 | Loss: 0.00003087
Iteration 202/1000 | Loss: 0.00003087
Iteration 203/1000 | Loss: 0.00003087
Iteration 204/1000 | Loss: 0.00003087
Iteration 205/1000 | Loss: 0.00003087
Iteration 206/1000 | Loss: 0.00004757
Iteration 207/1000 | Loss: 0.00003090
Iteration 208/1000 | Loss: 0.00003088
Iteration 209/1000 | Loss: 0.00003087
Iteration 210/1000 | Loss: 0.00003085
Iteration 211/1000 | Loss: 0.00003085
Iteration 212/1000 | Loss: 0.00003085
Iteration 213/1000 | Loss: 0.00003085
Iteration 214/1000 | Loss: 0.00003085
Iteration 215/1000 | Loss: 0.00003084
Iteration 216/1000 | Loss: 0.00003084
Iteration 217/1000 | Loss: 0.00003084
Iteration 218/1000 | Loss: 0.00003084
Iteration 219/1000 | Loss: 0.00003084
Iteration 220/1000 | Loss: 0.00003084
Iteration 221/1000 | Loss: 0.00003084
Iteration 222/1000 | Loss: 0.00003084
Iteration 223/1000 | Loss: 0.00003084
Iteration 224/1000 | Loss: 0.00003084
Iteration 225/1000 | Loss: 0.00003084
Iteration 226/1000 | Loss: 0.00003084
Iteration 227/1000 | Loss: 0.00003084
Iteration 228/1000 | Loss: 0.00003084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [3.084028503508307e-05, 3.084028503508307e-05, 3.084028503508307e-05, 3.084028503508307e-05, 3.084028503508307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.084028503508307e-05

Optimization complete. Final v2v error: 4.023492813110352 mm

Highest mean error: 11.455548286437988 mm for frame 38

Lowest mean error: 3.615905284881592 mm for frame 6

Saving results

Total time: 307.6397354602814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459754
Iteration 2/25 | Loss: 0.00155096
Iteration 3/25 | Loss: 0.00136181
Iteration 4/25 | Loss: 0.00134727
Iteration 5/25 | Loss: 0.00134598
Iteration 6/25 | Loss: 0.00134598
Iteration 7/25 | Loss: 0.00134598
Iteration 8/25 | Loss: 0.00134598
Iteration 9/25 | Loss: 0.00134598
Iteration 10/25 | Loss: 0.00134598
Iteration 11/25 | Loss: 0.00134598
Iteration 12/25 | Loss: 0.00134598
Iteration 13/25 | Loss: 0.00134598
Iteration 14/25 | Loss: 0.00134598
Iteration 15/25 | Loss: 0.00134598
Iteration 16/25 | Loss: 0.00134598
Iteration 17/25 | Loss: 0.00134598
Iteration 18/25 | Loss: 0.00134598
Iteration 19/25 | Loss: 0.00134598
Iteration 20/25 | Loss: 0.00134598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013459841720759869, 0.0013459841720759869, 0.0013459841720759869, 0.0013459841720759869, 0.0013459841720759869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013459841720759869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37757814
Iteration 2/25 | Loss: 0.00066460
Iteration 3/25 | Loss: 0.00066460
Iteration 4/25 | Loss: 0.00066460
Iteration 5/25 | Loss: 0.00066460
Iteration 6/25 | Loss: 0.00066460
Iteration 7/25 | Loss: 0.00066460
Iteration 8/25 | Loss: 0.00066460
Iteration 9/25 | Loss: 0.00066460
Iteration 10/25 | Loss: 0.00066460
Iteration 11/25 | Loss: 0.00066460
Iteration 12/25 | Loss: 0.00066460
Iteration 13/25 | Loss: 0.00066460
Iteration 14/25 | Loss: 0.00066460
Iteration 15/25 | Loss: 0.00066460
Iteration 16/25 | Loss: 0.00066460
Iteration 17/25 | Loss: 0.00066460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006646013935096562, 0.0006646013935096562, 0.0006646013935096562, 0.0006646013935096562, 0.0006646013935096562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006646013935096562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066460
Iteration 2/1000 | Loss: 0.00003774
Iteration 3/1000 | Loss: 0.00002943
Iteration 4/1000 | Loss: 0.00002716
Iteration 5/1000 | Loss: 0.00002587
Iteration 6/1000 | Loss: 0.00002472
Iteration 7/1000 | Loss: 0.00002373
Iteration 8/1000 | Loss: 0.00002320
Iteration 9/1000 | Loss: 0.00002253
Iteration 10/1000 | Loss: 0.00002225
Iteration 11/1000 | Loss: 0.00002208
Iteration 12/1000 | Loss: 0.00002206
Iteration 13/1000 | Loss: 0.00002196
Iteration 14/1000 | Loss: 0.00002196
Iteration 15/1000 | Loss: 0.00002195
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002194
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002192
Iteration 20/1000 | Loss: 0.00002192
Iteration 21/1000 | Loss: 0.00002192
Iteration 22/1000 | Loss: 0.00002191
Iteration 23/1000 | Loss: 0.00002191
Iteration 24/1000 | Loss: 0.00002191
Iteration 25/1000 | Loss: 0.00002189
Iteration 26/1000 | Loss: 0.00002189
Iteration 27/1000 | Loss: 0.00002189
Iteration 28/1000 | Loss: 0.00002188
Iteration 29/1000 | Loss: 0.00002188
Iteration 30/1000 | Loss: 0.00002188
Iteration 31/1000 | Loss: 0.00002187
Iteration 32/1000 | Loss: 0.00002187
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002187
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002186
Iteration 37/1000 | Loss: 0.00002186
Iteration 38/1000 | Loss: 0.00002185
Iteration 39/1000 | Loss: 0.00002185
Iteration 40/1000 | Loss: 0.00002183
Iteration 41/1000 | Loss: 0.00002183
Iteration 42/1000 | Loss: 0.00002183
Iteration 43/1000 | Loss: 0.00002182
Iteration 44/1000 | Loss: 0.00002181
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002178
Iteration 47/1000 | Loss: 0.00002177
Iteration 48/1000 | Loss: 0.00002177
Iteration 49/1000 | Loss: 0.00002176
Iteration 50/1000 | Loss: 0.00002176
Iteration 51/1000 | Loss: 0.00002176
Iteration 52/1000 | Loss: 0.00002175
Iteration 53/1000 | Loss: 0.00002175
Iteration 54/1000 | Loss: 0.00002175
Iteration 55/1000 | Loss: 0.00002175
Iteration 56/1000 | Loss: 0.00002175
Iteration 57/1000 | Loss: 0.00002175
Iteration 58/1000 | Loss: 0.00002175
Iteration 59/1000 | Loss: 0.00002175
Iteration 60/1000 | Loss: 0.00002175
Iteration 61/1000 | Loss: 0.00002175
Iteration 62/1000 | Loss: 0.00002173
Iteration 63/1000 | Loss: 0.00002173
Iteration 64/1000 | Loss: 0.00002173
Iteration 65/1000 | Loss: 0.00002172
Iteration 66/1000 | Loss: 0.00002172
Iteration 67/1000 | Loss: 0.00002171
Iteration 68/1000 | Loss: 0.00002170
Iteration 69/1000 | Loss: 0.00002170
Iteration 70/1000 | Loss: 0.00002169
Iteration 71/1000 | Loss: 0.00002169
Iteration 72/1000 | Loss: 0.00002168
Iteration 73/1000 | Loss: 0.00002168
Iteration 74/1000 | Loss: 0.00002168
Iteration 75/1000 | Loss: 0.00002167
Iteration 76/1000 | Loss: 0.00002166
Iteration 77/1000 | Loss: 0.00002166
Iteration 78/1000 | Loss: 0.00002165
Iteration 79/1000 | Loss: 0.00002165
Iteration 80/1000 | Loss: 0.00002164
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002164
Iteration 83/1000 | Loss: 0.00002164
Iteration 84/1000 | Loss: 0.00002164
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00002163
Iteration 87/1000 | Loss: 0.00002163
Iteration 88/1000 | Loss: 0.00002163
Iteration 89/1000 | Loss: 0.00002162
Iteration 90/1000 | Loss: 0.00002162
Iteration 91/1000 | Loss: 0.00002161
Iteration 92/1000 | Loss: 0.00002161
Iteration 93/1000 | Loss: 0.00002161
Iteration 94/1000 | Loss: 0.00002161
Iteration 95/1000 | Loss: 0.00002160
Iteration 96/1000 | Loss: 0.00002160
Iteration 97/1000 | Loss: 0.00002160
Iteration 98/1000 | Loss: 0.00002160
Iteration 99/1000 | Loss: 0.00002160
Iteration 100/1000 | Loss: 0.00002160
Iteration 101/1000 | Loss: 0.00002159
Iteration 102/1000 | Loss: 0.00002159
Iteration 103/1000 | Loss: 0.00002159
Iteration 104/1000 | Loss: 0.00002159
Iteration 105/1000 | Loss: 0.00002159
Iteration 106/1000 | Loss: 0.00002159
Iteration 107/1000 | Loss: 0.00002158
Iteration 108/1000 | Loss: 0.00002158
Iteration 109/1000 | Loss: 0.00002158
Iteration 110/1000 | Loss: 0.00002157
Iteration 111/1000 | Loss: 0.00002157
Iteration 112/1000 | Loss: 0.00002157
Iteration 113/1000 | Loss: 0.00002157
Iteration 114/1000 | Loss: 0.00002157
Iteration 115/1000 | Loss: 0.00002157
Iteration 116/1000 | Loss: 0.00002157
Iteration 117/1000 | Loss: 0.00002157
Iteration 118/1000 | Loss: 0.00002157
Iteration 119/1000 | Loss: 0.00002156
Iteration 120/1000 | Loss: 0.00002156
Iteration 121/1000 | Loss: 0.00002156
Iteration 122/1000 | Loss: 0.00002156
Iteration 123/1000 | Loss: 0.00002156
Iteration 124/1000 | Loss: 0.00002156
Iteration 125/1000 | Loss: 0.00002156
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002155
Iteration 128/1000 | Loss: 0.00002155
Iteration 129/1000 | Loss: 0.00002155
Iteration 130/1000 | Loss: 0.00002155
Iteration 131/1000 | Loss: 0.00002154
Iteration 132/1000 | Loss: 0.00002154
Iteration 133/1000 | Loss: 0.00002154
Iteration 134/1000 | Loss: 0.00002154
Iteration 135/1000 | Loss: 0.00002154
Iteration 136/1000 | Loss: 0.00002153
Iteration 137/1000 | Loss: 0.00002153
Iteration 138/1000 | Loss: 0.00002153
Iteration 139/1000 | Loss: 0.00002153
Iteration 140/1000 | Loss: 0.00002153
Iteration 141/1000 | Loss: 0.00002153
Iteration 142/1000 | Loss: 0.00002153
Iteration 143/1000 | Loss: 0.00002152
Iteration 144/1000 | Loss: 0.00002152
Iteration 145/1000 | Loss: 0.00002152
Iteration 146/1000 | Loss: 0.00002152
Iteration 147/1000 | Loss: 0.00002152
Iteration 148/1000 | Loss: 0.00002152
Iteration 149/1000 | Loss: 0.00002152
Iteration 150/1000 | Loss: 0.00002151
Iteration 151/1000 | Loss: 0.00002151
Iteration 152/1000 | Loss: 0.00002151
Iteration 153/1000 | Loss: 0.00002151
Iteration 154/1000 | Loss: 0.00002151
Iteration 155/1000 | Loss: 0.00002151
Iteration 156/1000 | Loss: 0.00002151
Iteration 157/1000 | Loss: 0.00002150
Iteration 158/1000 | Loss: 0.00002150
Iteration 159/1000 | Loss: 0.00002150
Iteration 160/1000 | Loss: 0.00002150
Iteration 161/1000 | Loss: 0.00002149
Iteration 162/1000 | Loss: 0.00002149
Iteration 163/1000 | Loss: 0.00002149
Iteration 164/1000 | Loss: 0.00002149
Iteration 165/1000 | Loss: 0.00002149
Iteration 166/1000 | Loss: 0.00002149
Iteration 167/1000 | Loss: 0.00002149
Iteration 168/1000 | Loss: 0.00002148
Iteration 169/1000 | Loss: 0.00002148
Iteration 170/1000 | Loss: 0.00002148
Iteration 171/1000 | Loss: 0.00002148
Iteration 172/1000 | Loss: 0.00002148
Iteration 173/1000 | Loss: 0.00002148
Iteration 174/1000 | Loss: 0.00002148
Iteration 175/1000 | Loss: 0.00002148
Iteration 176/1000 | Loss: 0.00002148
Iteration 177/1000 | Loss: 0.00002148
Iteration 178/1000 | Loss: 0.00002147
Iteration 179/1000 | Loss: 0.00002147
Iteration 180/1000 | Loss: 0.00002146
Iteration 181/1000 | Loss: 0.00002146
Iteration 182/1000 | Loss: 0.00002146
Iteration 183/1000 | Loss: 0.00002146
Iteration 184/1000 | Loss: 0.00002146
Iteration 185/1000 | Loss: 0.00002146
Iteration 186/1000 | Loss: 0.00002146
Iteration 187/1000 | Loss: 0.00002145
Iteration 188/1000 | Loss: 0.00002145
Iteration 189/1000 | Loss: 0.00002145
Iteration 190/1000 | Loss: 0.00002145
Iteration 191/1000 | Loss: 0.00002145
Iteration 192/1000 | Loss: 0.00002145
Iteration 193/1000 | Loss: 0.00002145
Iteration 194/1000 | Loss: 0.00002145
Iteration 195/1000 | Loss: 0.00002145
Iteration 196/1000 | Loss: 0.00002145
Iteration 197/1000 | Loss: 0.00002145
Iteration 198/1000 | Loss: 0.00002145
Iteration 199/1000 | Loss: 0.00002145
Iteration 200/1000 | Loss: 0.00002145
Iteration 201/1000 | Loss: 0.00002145
Iteration 202/1000 | Loss: 0.00002144
Iteration 203/1000 | Loss: 0.00002144
Iteration 204/1000 | Loss: 0.00002144
Iteration 205/1000 | Loss: 0.00002143
Iteration 206/1000 | Loss: 0.00002143
Iteration 207/1000 | Loss: 0.00002143
Iteration 208/1000 | Loss: 0.00002143
Iteration 209/1000 | Loss: 0.00002143
Iteration 210/1000 | Loss: 0.00002143
Iteration 211/1000 | Loss: 0.00002143
Iteration 212/1000 | Loss: 0.00002143
Iteration 213/1000 | Loss: 0.00002143
Iteration 214/1000 | Loss: 0.00002143
Iteration 215/1000 | Loss: 0.00002143
Iteration 216/1000 | Loss: 0.00002142
Iteration 217/1000 | Loss: 0.00002142
Iteration 218/1000 | Loss: 0.00002142
Iteration 219/1000 | Loss: 0.00002142
Iteration 220/1000 | Loss: 0.00002142
Iteration 221/1000 | Loss: 0.00002142
Iteration 222/1000 | Loss: 0.00002142
Iteration 223/1000 | Loss: 0.00002141
Iteration 224/1000 | Loss: 0.00002141
Iteration 225/1000 | Loss: 0.00002141
Iteration 226/1000 | Loss: 0.00002141
Iteration 227/1000 | Loss: 0.00002140
Iteration 228/1000 | Loss: 0.00002140
Iteration 229/1000 | Loss: 0.00002140
Iteration 230/1000 | Loss: 0.00002140
Iteration 231/1000 | Loss: 0.00002140
Iteration 232/1000 | Loss: 0.00002140
Iteration 233/1000 | Loss: 0.00002140
Iteration 234/1000 | Loss: 0.00002139
Iteration 235/1000 | Loss: 0.00002139
Iteration 236/1000 | Loss: 0.00002139
Iteration 237/1000 | Loss: 0.00002139
Iteration 238/1000 | Loss: 0.00002139
Iteration 239/1000 | Loss: 0.00002139
Iteration 240/1000 | Loss: 0.00002139
Iteration 241/1000 | Loss: 0.00002139
Iteration 242/1000 | Loss: 0.00002139
Iteration 243/1000 | Loss: 0.00002139
Iteration 244/1000 | Loss: 0.00002138
Iteration 245/1000 | Loss: 0.00002138
Iteration 246/1000 | Loss: 0.00002138
Iteration 247/1000 | Loss: 0.00002138
Iteration 248/1000 | Loss: 0.00002138
Iteration 249/1000 | Loss: 0.00002138
Iteration 250/1000 | Loss: 0.00002138
Iteration 251/1000 | Loss: 0.00002138
Iteration 252/1000 | Loss: 0.00002138
Iteration 253/1000 | Loss: 0.00002138
Iteration 254/1000 | Loss: 0.00002138
Iteration 255/1000 | Loss: 0.00002138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.1381903934525326e-05, 2.1381903934525326e-05, 2.1381903934525326e-05, 2.1381903934525326e-05, 2.1381903934525326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1381903934525326e-05

Optimization complete. Final v2v error: 3.887484312057495 mm

Highest mean error: 4.057175636291504 mm for frame 0

Lowest mean error: 3.718726634979248 mm for frame 178

Saving results

Total time: 42.18599486351013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438741
Iteration 2/25 | Loss: 0.00139814
Iteration 3/25 | Loss: 0.00131757
Iteration 4/25 | Loss: 0.00130779
Iteration 5/25 | Loss: 0.00130520
Iteration 6/25 | Loss: 0.00130455
Iteration 7/25 | Loss: 0.00130423
Iteration 8/25 | Loss: 0.00130418
Iteration 9/25 | Loss: 0.00130418
Iteration 10/25 | Loss: 0.00130418
Iteration 11/25 | Loss: 0.00130418
Iteration 12/25 | Loss: 0.00130418
Iteration 13/25 | Loss: 0.00130418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013041812926530838, 0.0013041812926530838, 0.0013041812926530838, 0.0013041812926530838, 0.0013041812926530838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013041812926530838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50302792
Iteration 2/25 | Loss: 0.00091205
Iteration 3/25 | Loss: 0.00091205
Iteration 4/25 | Loss: 0.00091205
Iteration 5/25 | Loss: 0.00091205
Iteration 6/25 | Loss: 0.00091205
Iteration 7/25 | Loss: 0.00091205
Iteration 8/25 | Loss: 0.00091205
Iteration 9/25 | Loss: 0.00091205
Iteration 10/25 | Loss: 0.00091205
Iteration 11/25 | Loss: 0.00091205
Iteration 12/25 | Loss: 0.00091205
Iteration 13/25 | Loss: 0.00091205
Iteration 14/25 | Loss: 0.00091205
Iteration 15/25 | Loss: 0.00091205
Iteration 16/25 | Loss: 0.00091205
Iteration 17/25 | Loss: 0.00091205
Iteration 18/25 | Loss: 0.00091205
Iteration 19/25 | Loss: 0.00091205
Iteration 20/25 | Loss: 0.00091205
Iteration 21/25 | Loss: 0.00091205
Iteration 22/25 | Loss: 0.00091205
Iteration 23/25 | Loss: 0.00091205
Iteration 24/25 | Loss: 0.00091205
Iteration 25/25 | Loss: 0.00091205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091205
Iteration 2/1000 | Loss: 0.00003466
Iteration 3/1000 | Loss: 0.00002518
Iteration 4/1000 | Loss: 0.00002116
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001950
Iteration 7/1000 | Loss: 0.00001901
Iteration 8/1000 | Loss: 0.00001848
Iteration 9/1000 | Loss: 0.00001806
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001767
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001692
Iteration 17/1000 | Loss: 0.00001676
Iteration 18/1000 | Loss: 0.00001675
Iteration 19/1000 | Loss: 0.00001674
Iteration 20/1000 | Loss: 0.00001674
Iteration 21/1000 | Loss: 0.00001673
Iteration 22/1000 | Loss: 0.00001673
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001665
Iteration 25/1000 | Loss: 0.00001664
Iteration 26/1000 | Loss: 0.00001664
Iteration 27/1000 | Loss: 0.00001661
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001660
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001659
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001655
Iteration 42/1000 | Loss: 0.00001655
Iteration 43/1000 | Loss: 0.00001655
Iteration 44/1000 | Loss: 0.00001655
Iteration 45/1000 | Loss: 0.00001654
Iteration 46/1000 | Loss: 0.00001654
Iteration 47/1000 | Loss: 0.00001654
Iteration 48/1000 | Loss: 0.00001653
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001652
Iteration 51/1000 | Loss: 0.00001652
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001644
Iteration 55/1000 | Loss: 0.00001644
Iteration 56/1000 | Loss: 0.00001644
Iteration 57/1000 | Loss: 0.00001643
Iteration 58/1000 | Loss: 0.00001642
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001638
Iteration 69/1000 | Loss: 0.00001638
Iteration 70/1000 | Loss: 0.00001637
Iteration 71/1000 | Loss: 0.00001637
Iteration 72/1000 | Loss: 0.00001637
Iteration 73/1000 | Loss: 0.00001637
Iteration 74/1000 | Loss: 0.00001637
Iteration 75/1000 | Loss: 0.00001637
Iteration 76/1000 | Loss: 0.00001636
Iteration 77/1000 | Loss: 0.00001636
Iteration 78/1000 | Loss: 0.00001636
Iteration 79/1000 | Loss: 0.00001636
Iteration 80/1000 | Loss: 0.00001636
Iteration 81/1000 | Loss: 0.00001635
Iteration 82/1000 | Loss: 0.00001635
Iteration 83/1000 | Loss: 0.00001635
Iteration 84/1000 | Loss: 0.00001635
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001634
Iteration 87/1000 | Loss: 0.00001634
Iteration 88/1000 | Loss: 0.00001633
Iteration 89/1000 | Loss: 0.00001633
Iteration 90/1000 | Loss: 0.00001633
Iteration 91/1000 | Loss: 0.00001633
Iteration 92/1000 | Loss: 0.00001633
Iteration 93/1000 | Loss: 0.00001633
Iteration 94/1000 | Loss: 0.00001633
Iteration 95/1000 | Loss: 0.00001633
Iteration 96/1000 | Loss: 0.00001632
Iteration 97/1000 | Loss: 0.00001632
Iteration 98/1000 | Loss: 0.00001632
Iteration 99/1000 | Loss: 0.00001632
Iteration 100/1000 | Loss: 0.00001632
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001631
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001631
Iteration 105/1000 | Loss: 0.00001631
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001630
Iteration 108/1000 | Loss: 0.00001630
Iteration 109/1000 | Loss: 0.00001630
Iteration 110/1000 | Loss: 0.00001630
Iteration 111/1000 | Loss: 0.00001630
Iteration 112/1000 | Loss: 0.00001630
Iteration 113/1000 | Loss: 0.00001630
Iteration 114/1000 | Loss: 0.00001629
Iteration 115/1000 | Loss: 0.00001629
Iteration 116/1000 | Loss: 0.00001629
Iteration 117/1000 | Loss: 0.00001629
Iteration 118/1000 | Loss: 0.00001629
Iteration 119/1000 | Loss: 0.00001629
Iteration 120/1000 | Loss: 0.00001628
Iteration 121/1000 | Loss: 0.00001628
Iteration 122/1000 | Loss: 0.00001628
Iteration 123/1000 | Loss: 0.00001628
Iteration 124/1000 | Loss: 0.00001628
Iteration 125/1000 | Loss: 0.00001628
Iteration 126/1000 | Loss: 0.00001628
Iteration 127/1000 | Loss: 0.00001628
Iteration 128/1000 | Loss: 0.00001628
Iteration 129/1000 | Loss: 0.00001628
Iteration 130/1000 | Loss: 0.00001627
Iteration 131/1000 | Loss: 0.00001627
Iteration 132/1000 | Loss: 0.00001627
Iteration 133/1000 | Loss: 0.00001627
Iteration 134/1000 | Loss: 0.00001627
Iteration 135/1000 | Loss: 0.00001627
Iteration 136/1000 | Loss: 0.00001627
Iteration 137/1000 | Loss: 0.00001627
Iteration 138/1000 | Loss: 0.00001626
Iteration 139/1000 | Loss: 0.00001626
Iteration 140/1000 | Loss: 0.00001626
Iteration 141/1000 | Loss: 0.00001626
Iteration 142/1000 | Loss: 0.00001626
Iteration 143/1000 | Loss: 0.00001626
Iteration 144/1000 | Loss: 0.00001626
Iteration 145/1000 | Loss: 0.00001626
Iteration 146/1000 | Loss: 0.00001626
Iteration 147/1000 | Loss: 0.00001625
Iteration 148/1000 | Loss: 0.00001625
Iteration 149/1000 | Loss: 0.00001625
Iteration 150/1000 | Loss: 0.00001625
Iteration 151/1000 | Loss: 0.00001625
Iteration 152/1000 | Loss: 0.00001625
Iteration 153/1000 | Loss: 0.00001625
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001625
Iteration 157/1000 | Loss: 0.00001625
Iteration 158/1000 | Loss: 0.00001625
Iteration 159/1000 | Loss: 0.00001625
Iteration 160/1000 | Loss: 0.00001625
Iteration 161/1000 | Loss: 0.00001625
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001625
Iteration 165/1000 | Loss: 0.00001625
Iteration 166/1000 | Loss: 0.00001624
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001624
Iteration 169/1000 | Loss: 0.00001624
Iteration 170/1000 | Loss: 0.00001624
Iteration 171/1000 | Loss: 0.00001624
Iteration 172/1000 | Loss: 0.00001624
Iteration 173/1000 | Loss: 0.00001624
Iteration 174/1000 | Loss: 0.00001624
Iteration 175/1000 | Loss: 0.00001624
Iteration 176/1000 | Loss: 0.00001624
Iteration 177/1000 | Loss: 0.00001624
Iteration 178/1000 | Loss: 0.00001624
Iteration 179/1000 | Loss: 0.00001624
Iteration 180/1000 | Loss: 0.00001624
Iteration 181/1000 | Loss: 0.00001624
Iteration 182/1000 | Loss: 0.00001624
Iteration 183/1000 | Loss: 0.00001624
Iteration 184/1000 | Loss: 0.00001624
Iteration 185/1000 | Loss: 0.00001624
Iteration 186/1000 | Loss: 0.00001624
Iteration 187/1000 | Loss: 0.00001624
Iteration 188/1000 | Loss: 0.00001624
Iteration 189/1000 | Loss: 0.00001624
Iteration 190/1000 | Loss: 0.00001624
Iteration 191/1000 | Loss: 0.00001624
Iteration 192/1000 | Loss: 0.00001624
Iteration 193/1000 | Loss: 0.00001623
Iteration 194/1000 | Loss: 0.00001623
Iteration 195/1000 | Loss: 0.00001623
Iteration 196/1000 | Loss: 0.00001623
Iteration 197/1000 | Loss: 0.00001623
Iteration 198/1000 | Loss: 0.00001623
Iteration 199/1000 | Loss: 0.00001623
Iteration 200/1000 | Loss: 0.00001623
Iteration 201/1000 | Loss: 0.00001623
Iteration 202/1000 | Loss: 0.00001623
Iteration 203/1000 | Loss: 0.00001623
Iteration 204/1000 | Loss: 0.00001623
Iteration 205/1000 | Loss: 0.00001623
Iteration 206/1000 | Loss: 0.00001623
Iteration 207/1000 | Loss: 0.00001623
Iteration 208/1000 | Loss: 0.00001623
Iteration 209/1000 | Loss: 0.00001623
Iteration 210/1000 | Loss: 0.00001623
Iteration 211/1000 | Loss: 0.00001623
Iteration 212/1000 | Loss: 0.00001623
Iteration 213/1000 | Loss: 0.00001623
Iteration 214/1000 | Loss: 0.00001623
Iteration 215/1000 | Loss: 0.00001623
Iteration 216/1000 | Loss: 0.00001623
Iteration 217/1000 | Loss: 0.00001623
Iteration 218/1000 | Loss: 0.00001623
Iteration 219/1000 | Loss: 0.00001623
Iteration 220/1000 | Loss: 0.00001623
Iteration 221/1000 | Loss: 0.00001623
Iteration 222/1000 | Loss: 0.00001623
Iteration 223/1000 | Loss: 0.00001623
Iteration 224/1000 | Loss: 0.00001623
Iteration 225/1000 | Loss: 0.00001623
Iteration 226/1000 | Loss: 0.00001623
Iteration 227/1000 | Loss: 0.00001623
Iteration 228/1000 | Loss: 0.00001623
Iteration 229/1000 | Loss: 0.00001623
Iteration 230/1000 | Loss: 0.00001623
Iteration 231/1000 | Loss: 0.00001623
Iteration 232/1000 | Loss: 0.00001623
Iteration 233/1000 | Loss: 0.00001623
Iteration 234/1000 | Loss: 0.00001623
Iteration 235/1000 | Loss: 0.00001623
Iteration 236/1000 | Loss: 0.00001623
Iteration 237/1000 | Loss: 0.00001623
Iteration 238/1000 | Loss: 0.00001623
Iteration 239/1000 | Loss: 0.00001623
Iteration 240/1000 | Loss: 0.00001623
Iteration 241/1000 | Loss: 0.00001623
Iteration 242/1000 | Loss: 0.00001623
Iteration 243/1000 | Loss: 0.00001623
Iteration 244/1000 | Loss: 0.00001623
Iteration 245/1000 | Loss: 0.00001623
Iteration 246/1000 | Loss: 0.00001623
Iteration 247/1000 | Loss: 0.00001623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.6233667338383384e-05, 1.6233667338383384e-05, 1.6233667338383384e-05, 1.6233667338383384e-05, 1.6233667338383384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6233667338383384e-05

Optimization complete. Final v2v error: 3.4029030799865723 mm

Highest mean error: 4.676171779632568 mm for frame 47

Lowest mean error: 3.060560464859009 mm for frame 86

Saving results

Total time: 47.17364859580994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785826
Iteration 2/25 | Loss: 0.00135901
Iteration 3/25 | Loss: 0.00129338
Iteration 4/25 | Loss: 0.00127864
Iteration 5/25 | Loss: 0.00127472
Iteration 6/25 | Loss: 0.00127472
Iteration 7/25 | Loss: 0.00127472
Iteration 8/25 | Loss: 0.00127472
Iteration 9/25 | Loss: 0.00127472
Iteration 10/25 | Loss: 0.00127472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012747222790494561, 0.0012747222790494561, 0.0012747222790494561, 0.0012747222790494561, 0.0012747222790494561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012747222790494561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43207324
Iteration 2/25 | Loss: 0.00083145
Iteration 3/25 | Loss: 0.00083145
Iteration 4/25 | Loss: 0.00083144
Iteration 5/25 | Loss: 0.00083144
Iteration 6/25 | Loss: 0.00083144
Iteration 7/25 | Loss: 0.00083144
Iteration 8/25 | Loss: 0.00083144
Iteration 9/25 | Loss: 0.00083144
Iteration 10/25 | Loss: 0.00083144
Iteration 11/25 | Loss: 0.00083144
Iteration 12/25 | Loss: 0.00083144
Iteration 13/25 | Loss: 0.00083144
Iteration 14/25 | Loss: 0.00083144
Iteration 15/25 | Loss: 0.00083144
Iteration 16/25 | Loss: 0.00083144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008314427686855197, 0.0008314427686855197, 0.0008314427686855197, 0.0008314427686855197, 0.0008314427686855197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008314427686855197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083144
Iteration 2/1000 | Loss: 0.00002920
Iteration 3/1000 | Loss: 0.00002033
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001711
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001558
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001476
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001468
Iteration 14/1000 | Loss: 0.00001466
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001457
Iteration 18/1000 | Loss: 0.00001456
Iteration 19/1000 | Loss: 0.00001454
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001435
Iteration 22/1000 | Loss: 0.00001434
Iteration 23/1000 | Loss: 0.00001427
Iteration 24/1000 | Loss: 0.00001424
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001416
Iteration 33/1000 | Loss: 0.00001416
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001409
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001409
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001404
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001403
Iteration 53/1000 | Loss: 0.00001403
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001397
Iteration 64/1000 | Loss: 0.00001397
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001391
Iteration 74/1000 | Loss: 0.00001391
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001389
Iteration 78/1000 | Loss: 0.00001388
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001387
Iteration 83/1000 | Loss: 0.00001387
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001386
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001386
Iteration 91/1000 | Loss: 0.00001386
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001385
Iteration 96/1000 | Loss: 0.00001385
Iteration 97/1000 | Loss: 0.00001384
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001384
Iteration 104/1000 | Loss: 0.00001383
Iteration 105/1000 | Loss: 0.00001383
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001383
Iteration 112/1000 | Loss: 0.00001383
Iteration 113/1000 | Loss: 0.00001383
Iteration 114/1000 | Loss: 0.00001383
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001382
Iteration 117/1000 | Loss: 0.00001382
Iteration 118/1000 | Loss: 0.00001382
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001382
Iteration 123/1000 | Loss: 0.00001382
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001382
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001379
Iteration 141/1000 | Loss: 0.00001379
Iteration 142/1000 | Loss: 0.00001379
Iteration 143/1000 | Loss: 0.00001379
Iteration 144/1000 | Loss: 0.00001379
Iteration 145/1000 | Loss: 0.00001379
Iteration 146/1000 | Loss: 0.00001379
Iteration 147/1000 | Loss: 0.00001379
Iteration 148/1000 | Loss: 0.00001379
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001379
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001378
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001378
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001378
Iteration 159/1000 | Loss: 0.00001378
Iteration 160/1000 | Loss: 0.00001378
Iteration 161/1000 | Loss: 0.00001378
Iteration 162/1000 | Loss: 0.00001378
Iteration 163/1000 | Loss: 0.00001378
Iteration 164/1000 | Loss: 0.00001378
Iteration 165/1000 | Loss: 0.00001378
Iteration 166/1000 | Loss: 0.00001378
Iteration 167/1000 | Loss: 0.00001378
Iteration 168/1000 | Loss: 0.00001378
Iteration 169/1000 | Loss: 0.00001378
Iteration 170/1000 | Loss: 0.00001377
Iteration 171/1000 | Loss: 0.00001377
Iteration 172/1000 | Loss: 0.00001377
Iteration 173/1000 | Loss: 0.00001377
Iteration 174/1000 | Loss: 0.00001377
Iteration 175/1000 | Loss: 0.00001377
Iteration 176/1000 | Loss: 0.00001377
Iteration 177/1000 | Loss: 0.00001377
Iteration 178/1000 | Loss: 0.00001377
Iteration 179/1000 | Loss: 0.00001377
Iteration 180/1000 | Loss: 0.00001377
Iteration 181/1000 | Loss: 0.00001377
Iteration 182/1000 | Loss: 0.00001377
Iteration 183/1000 | Loss: 0.00001377
Iteration 184/1000 | Loss: 0.00001377
Iteration 185/1000 | Loss: 0.00001377
Iteration 186/1000 | Loss: 0.00001377
Iteration 187/1000 | Loss: 0.00001377
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001377
Iteration 192/1000 | Loss: 0.00001377
Iteration 193/1000 | Loss: 0.00001377
Iteration 194/1000 | Loss: 0.00001377
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.3770747500529978e-05, 1.3770747500529978e-05, 1.3770747500529978e-05, 1.3770747500529978e-05, 1.3770747500529978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3770747500529978e-05

Optimization complete. Final v2v error: 3.1667299270629883 mm

Highest mean error: 3.451724052429199 mm for frame 66

Lowest mean error: 3.0594005584716797 mm for frame 145

Saving results

Total time: 40.951393365859985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00284621
Iteration 2/25 | Loss: 0.00141191
Iteration 3/25 | Loss: 0.00129061
Iteration 4/25 | Loss: 0.00127977
Iteration 5/25 | Loss: 0.00127613
Iteration 6/25 | Loss: 0.00127485
Iteration 7/25 | Loss: 0.00127466
Iteration 8/25 | Loss: 0.00127466
Iteration 9/25 | Loss: 0.00127466
Iteration 10/25 | Loss: 0.00127466
Iteration 11/25 | Loss: 0.00127466
Iteration 12/25 | Loss: 0.00127466
Iteration 13/25 | Loss: 0.00127466
Iteration 14/25 | Loss: 0.00127466
Iteration 15/25 | Loss: 0.00127466
Iteration 16/25 | Loss: 0.00127466
Iteration 17/25 | Loss: 0.00127466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012746572028845549, 0.0012746572028845549, 0.0012746572028845549, 0.0012746572028845549, 0.0012746572028845549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012746572028845549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36883855
Iteration 2/25 | Loss: 0.00091296
Iteration 3/25 | Loss: 0.00091296
Iteration 4/25 | Loss: 0.00091296
Iteration 5/25 | Loss: 0.00091296
Iteration 6/25 | Loss: 0.00091296
Iteration 7/25 | Loss: 0.00091296
Iteration 8/25 | Loss: 0.00091296
Iteration 9/25 | Loss: 0.00091296
Iteration 10/25 | Loss: 0.00091296
Iteration 11/25 | Loss: 0.00091296
Iteration 12/25 | Loss: 0.00091296
Iteration 13/25 | Loss: 0.00091296
Iteration 14/25 | Loss: 0.00091296
Iteration 15/25 | Loss: 0.00091296
Iteration 16/25 | Loss: 0.00091296
Iteration 17/25 | Loss: 0.00091296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009129623067565262, 0.0009129623067565262, 0.0009129623067565262, 0.0009129623067565262, 0.0009129623067565262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009129623067565262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091296
Iteration 2/1000 | Loss: 0.00005007
Iteration 3/1000 | Loss: 0.00003011
Iteration 4/1000 | Loss: 0.00002249
Iteration 5/1000 | Loss: 0.00002090
Iteration 6/1000 | Loss: 0.00001990
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001844
Iteration 9/1000 | Loss: 0.00001793
Iteration 10/1000 | Loss: 0.00001759
Iteration 11/1000 | Loss: 0.00001732
Iteration 12/1000 | Loss: 0.00001715
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001682
Iteration 15/1000 | Loss: 0.00001673
Iteration 16/1000 | Loss: 0.00001672
Iteration 17/1000 | Loss: 0.00001669
Iteration 18/1000 | Loss: 0.00001668
Iteration 19/1000 | Loss: 0.00001664
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001663
Iteration 23/1000 | Loss: 0.00001661
Iteration 24/1000 | Loss: 0.00001660
Iteration 25/1000 | Loss: 0.00001660
Iteration 26/1000 | Loss: 0.00001660
Iteration 27/1000 | Loss: 0.00001659
Iteration 28/1000 | Loss: 0.00001659
Iteration 29/1000 | Loss: 0.00001659
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001657
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001657
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001656
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001656
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001656
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001655
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001655
Iteration 56/1000 | Loss: 0.00001655
Iteration 57/1000 | Loss: 0.00001655
Iteration 58/1000 | Loss: 0.00001655
Iteration 59/1000 | Loss: 0.00001654
Iteration 60/1000 | Loss: 0.00001654
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001653
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001653
Iteration 67/1000 | Loss: 0.00001653
Iteration 68/1000 | Loss: 0.00001653
Iteration 69/1000 | Loss: 0.00001653
Iteration 70/1000 | Loss: 0.00001653
Iteration 71/1000 | Loss: 0.00001652
Iteration 72/1000 | Loss: 0.00001652
Iteration 73/1000 | Loss: 0.00001652
Iteration 74/1000 | Loss: 0.00001652
Iteration 75/1000 | Loss: 0.00001652
Iteration 76/1000 | Loss: 0.00001652
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001651
Iteration 80/1000 | Loss: 0.00001651
Iteration 81/1000 | Loss: 0.00001651
Iteration 82/1000 | Loss: 0.00001651
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001651
Iteration 85/1000 | Loss: 0.00001650
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001650
Iteration 89/1000 | Loss: 0.00001650
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001649
Iteration 97/1000 | Loss: 0.00001649
Iteration 98/1000 | Loss: 0.00001649
Iteration 99/1000 | Loss: 0.00001649
Iteration 100/1000 | Loss: 0.00001649
Iteration 101/1000 | Loss: 0.00001649
Iteration 102/1000 | Loss: 0.00001648
Iteration 103/1000 | Loss: 0.00001648
Iteration 104/1000 | Loss: 0.00001648
Iteration 105/1000 | Loss: 0.00001648
Iteration 106/1000 | Loss: 0.00001648
Iteration 107/1000 | Loss: 0.00001648
Iteration 108/1000 | Loss: 0.00001648
Iteration 109/1000 | Loss: 0.00001648
Iteration 110/1000 | Loss: 0.00001648
Iteration 111/1000 | Loss: 0.00001648
Iteration 112/1000 | Loss: 0.00001648
Iteration 113/1000 | Loss: 0.00001648
Iteration 114/1000 | Loss: 0.00001648
Iteration 115/1000 | Loss: 0.00001648
Iteration 116/1000 | Loss: 0.00001648
Iteration 117/1000 | Loss: 0.00001647
Iteration 118/1000 | Loss: 0.00001647
Iteration 119/1000 | Loss: 0.00001647
Iteration 120/1000 | Loss: 0.00001647
Iteration 121/1000 | Loss: 0.00001647
Iteration 122/1000 | Loss: 0.00001646
Iteration 123/1000 | Loss: 0.00001646
Iteration 124/1000 | Loss: 0.00001646
Iteration 125/1000 | Loss: 0.00001646
Iteration 126/1000 | Loss: 0.00001646
Iteration 127/1000 | Loss: 0.00001646
Iteration 128/1000 | Loss: 0.00001646
Iteration 129/1000 | Loss: 0.00001646
Iteration 130/1000 | Loss: 0.00001646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.646391501708422e-05, 1.646391501708422e-05, 1.646391501708422e-05, 1.646391501708422e-05, 1.646391501708422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.646391501708422e-05

Optimization complete. Final v2v error: 3.4834325313568115 mm

Highest mean error: 3.9389798641204834 mm for frame 83

Lowest mean error: 3.0992836952209473 mm for frame 5

Saving results

Total time: 37.2711398601532
