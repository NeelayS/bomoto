Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=60, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3360-3415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886491
Iteration 2/25 | Loss: 0.00136367
Iteration 3/25 | Loss: 0.00130342
Iteration 4/25 | Loss: 0.00129404
Iteration 5/25 | Loss: 0.00129269
Iteration 6/25 | Loss: 0.00129269
Iteration 7/25 | Loss: 0.00129269
Iteration 8/25 | Loss: 0.00129269
Iteration 9/25 | Loss: 0.00129269
Iteration 10/25 | Loss: 0.00129269
Iteration 11/25 | Loss: 0.00129269
Iteration 12/25 | Loss: 0.00129269
Iteration 13/25 | Loss: 0.00129269
Iteration 14/25 | Loss: 0.00129269
Iteration 15/25 | Loss: 0.00129269
Iteration 16/25 | Loss: 0.00129269
Iteration 17/25 | Loss: 0.00129269
Iteration 18/25 | Loss: 0.00129269
Iteration 19/25 | Loss: 0.00129269
Iteration 20/25 | Loss: 0.00129269
Iteration 21/25 | Loss: 0.00129269
Iteration 22/25 | Loss: 0.00129269
Iteration 23/25 | Loss: 0.00129269
Iteration 24/25 | Loss: 0.00129269
Iteration 25/25 | Loss: 0.00129269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47518826
Iteration 2/25 | Loss: 0.00082716
Iteration 3/25 | Loss: 0.00082716
Iteration 4/25 | Loss: 0.00082716
Iteration 5/25 | Loss: 0.00082716
Iteration 6/25 | Loss: 0.00082716
Iteration 7/25 | Loss: 0.00082716
Iteration 8/25 | Loss: 0.00082716
Iteration 9/25 | Loss: 0.00082716
Iteration 10/25 | Loss: 0.00082716
Iteration 11/25 | Loss: 0.00082716
Iteration 12/25 | Loss: 0.00082716
Iteration 13/25 | Loss: 0.00082716
Iteration 14/25 | Loss: 0.00082716
Iteration 15/25 | Loss: 0.00082716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008271611295640469, 0.0008271611295640469, 0.0008271611295640469, 0.0008271611295640469, 0.0008271611295640469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008271611295640469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082716
Iteration 2/1000 | Loss: 0.00002410
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001841
Iteration 5/1000 | Loss: 0.00001763
Iteration 6/1000 | Loss: 0.00001717
Iteration 7/1000 | Loss: 0.00001686
Iteration 8/1000 | Loss: 0.00001644
Iteration 9/1000 | Loss: 0.00001618
Iteration 10/1000 | Loss: 0.00001618
Iteration 11/1000 | Loss: 0.00001614
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001605
Iteration 15/1000 | Loss: 0.00001596
Iteration 16/1000 | Loss: 0.00001594
Iteration 17/1000 | Loss: 0.00001591
Iteration 18/1000 | Loss: 0.00001573
Iteration 19/1000 | Loss: 0.00001572
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001570
Iteration 22/1000 | Loss: 0.00001565
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001560
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001553
Iteration 29/1000 | Loss: 0.00001553
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001549
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001549
Iteration 36/1000 | Loss: 0.00001549
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001543
Iteration 43/1000 | Loss: 0.00001543
Iteration 44/1000 | Loss: 0.00001543
Iteration 45/1000 | Loss: 0.00001543
Iteration 46/1000 | Loss: 0.00001543
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001542
Iteration 50/1000 | Loss: 0.00001542
Iteration 51/1000 | Loss: 0.00001542
Iteration 52/1000 | Loss: 0.00001542
Iteration 53/1000 | Loss: 0.00001542
Iteration 54/1000 | Loss: 0.00001542
Iteration 55/1000 | Loss: 0.00001540
Iteration 56/1000 | Loss: 0.00001540
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001535
Iteration 61/1000 | Loss: 0.00001535
Iteration 62/1000 | Loss: 0.00001535
Iteration 63/1000 | Loss: 0.00001534
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001534
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001530
Iteration 69/1000 | Loss: 0.00001530
Iteration 70/1000 | Loss: 0.00001530
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001529
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001529
Iteration 76/1000 | Loss: 0.00001529
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001528
Iteration 81/1000 | Loss: 0.00001528
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001522
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001522
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001518
Iteration 101/1000 | Loss: 0.00001518
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001518
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001517
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001517
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001517
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001516
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001516
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001516
Iteration 125/1000 | Loss: 0.00001516
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001516
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001515
Iteration 138/1000 | Loss: 0.00001515
Iteration 139/1000 | Loss: 0.00001514
Iteration 140/1000 | Loss: 0.00001514
Iteration 141/1000 | Loss: 0.00001514
Iteration 142/1000 | Loss: 0.00001514
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001513
Iteration 147/1000 | Loss: 0.00001513
Iteration 148/1000 | Loss: 0.00001513
Iteration 149/1000 | Loss: 0.00001513
Iteration 150/1000 | Loss: 0.00001513
Iteration 151/1000 | Loss: 0.00001513
Iteration 152/1000 | Loss: 0.00001513
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001513
Iteration 156/1000 | Loss: 0.00001513
Iteration 157/1000 | Loss: 0.00001513
Iteration 158/1000 | Loss: 0.00001513
Iteration 159/1000 | Loss: 0.00001513
Iteration 160/1000 | Loss: 0.00001513
Iteration 161/1000 | Loss: 0.00001513
Iteration 162/1000 | Loss: 0.00001513
Iteration 163/1000 | Loss: 0.00001513
Iteration 164/1000 | Loss: 0.00001513
Iteration 165/1000 | Loss: 0.00001513
Iteration 166/1000 | Loss: 0.00001513
Iteration 167/1000 | Loss: 0.00001513
Iteration 168/1000 | Loss: 0.00001513
Iteration 169/1000 | Loss: 0.00001513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.5133725355553906e-05, 1.5133725355553906e-05, 1.5133725355553906e-05, 1.5133725355553906e-05, 1.5133725355553906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5133725355553906e-05

Optimization complete. Final v2v error: 3.2993452548980713 mm

Highest mean error: 3.740540027618408 mm for frame 147

Lowest mean error: 3.215979814529419 mm for frame 203

Saving results

Total time: 43.917909383773804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463717
Iteration 2/25 | Loss: 0.00147882
Iteration 3/25 | Loss: 0.00135895
Iteration 4/25 | Loss: 0.00134344
Iteration 5/25 | Loss: 0.00133830
Iteration 6/25 | Loss: 0.00133743
Iteration 7/25 | Loss: 0.00133743
Iteration 8/25 | Loss: 0.00133743
Iteration 9/25 | Loss: 0.00133743
Iteration 10/25 | Loss: 0.00133743
Iteration 11/25 | Loss: 0.00133743
Iteration 12/25 | Loss: 0.00133743
Iteration 13/25 | Loss: 0.00133743
Iteration 14/25 | Loss: 0.00133743
Iteration 15/25 | Loss: 0.00133743
Iteration 16/25 | Loss: 0.00133743
Iteration 17/25 | Loss: 0.00133743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013374342815950513, 0.0013374342815950513, 0.0013374342815950513, 0.0013374342815950513, 0.0013374342815950513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013374342815950513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85972428
Iteration 2/25 | Loss: 0.00086564
Iteration 3/25 | Loss: 0.00086563
Iteration 4/25 | Loss: 0.00086563
Iteration 5/25 | Loss: 0.00086563
Iteration 6/25 | Loss: 0.00086563
Iteration 7/25 | Loss: 0.00086563
Iteration 8/25 | Loss: 0.00086563
Iteration 9/25 | Loss: 0.00086563
Iteration 10/25 | Loss: 0.00086563
Iteration 11/25 | Loss: 0.00086563
Iteration 12/25 | Loss: 0.00086563
Iteration 13/25 | Loss: 0.00086563
Iteration 14/25 | Loss: 0.00086563
Iteration 15/25 | Loss: 0.00086563
Iteration 16/25 | Loss: 0.00086563
Iteration 17/25 | Loss: 0.00086563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008656294085085392, 0.0008656294085085392, 0.0008656294085085392, 0.0008656294085085392, 0.0008656294085085392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008656294085085392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086563
Iteration 2/1000 | Loss: 0.00003369
Iteration 3/1000 | Loss: 0.00002518
Iteration 4/1000 | Loss: 0.00002233
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001926
Iteration 8/1000 | Loss: 0.00001887
Iteration 9/1000 | Loss: 0.00001852
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001786
Iteration 13/1000 | Loss: 0.00001764
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001731
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001710
Iteration 21/1000 | Loss: 0.00001710
Iteration 22/1000 | Loss: 0.00001710
Iteration 23/1000 | Loss: 0.00001710
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001710
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001708
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001700
Iteration 32/1000 | Loss: 0.00001700
Iteration 33/1000 | Loss: 0.00001699
Iteration 34/1000 | Loss: 0.00001699
Iteration 35/1000 | Loss: 0.00001699
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001698
Iteration 38/1000 | Loss: 0.00001697
Iteration 39/1000 | Loss: 0.00001697
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001697
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001697
Iteration 46/1000 | Loss: 0.00001697
Iteration 47/1000 | Loss: 0.00001696
Iteration 48/1000 | Loss: 0.00001696
Iteration 49/1000 | Loss: 0.00001696
Iteration 50/1000 | Loss: 0.00001696
Iteration 51/1000 | Loss: 0.00001695
Iteration 52/1000 | Loss: 0.00001695
Iteration 53/1000 | Loss: 0.00001694
Iteration 54/1000 | Loss: 0.00001694
Iteration 55/1000 | Loss: 0.00001693
Iteration 56/1000 | Loss: 0.00001693
Iteration 57/1000 | Loss: 0.00001692
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001689
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001684
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001683
Iteration 73/1000 | Loss: 0.00001683
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001683
Iteration 77/1000 | Loss: 0.00001683
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001681
Iteration 84/1000 | Loss: 0.00001681
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001681
Iteration 90/1000 | Loss: 0.00001681
Iteration 91/1000 | Loss: 0.00001681
Iteration 92/1000 | Loss: 0.00001681
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001681
Iteration 96/1000 | Loss: 0.00001681
Iteration 97/1000 | Loss: 0.00001681
Iteration 98/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.6810607121442445e-05, 1.6810607121442445e-05, 1.6810607121442445e-05, 1.6810607121442445e-05, 1.6810607121442445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6810607121442445e-05

Optimization complete. Final v2v error: 3.4780828952789307 mm

Highest mean error: 3.786635160446167 mm for frame 244

Lowest mean error: 3.326721668243408 mm for frame 178

Saving results

Total time: 44.234803676605225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518883
Iteration 2/25 | Loss: 0.00141892
Iteration 3/25 | Loss: 0.00134330
Iteration 4/25 | Loss: 0.00132494
Iteration 5/25 | Loss: 0.00132159
Iteration 6/25 | Loss: 0.00132129
Iteration 7/25 | Loss: 0.00132129
Iteration 8/25 | Loss: 0.00132129
Iteration 9/25 | Loss: 0.00132129
Iteration 10/25 | Loss: 0.00132129
Iteration 11/25 | Loss: 0.00132129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013212939957156777, 0.0013212939957156777, 0.0013212939957156777, 0.0013212939957156777, 0.0013212939957156777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013212939957156777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98380280
Iteration 2/25 | Loss: 0.00091250
Iteration 3/25 | Loss: 0.00091250
Iteration 4/25 | Loss: 0.00091250
Iteration 5/25 | Loss: 0.00091250
Iteration 6/25 | Loss: 0.00091250
Iteration 7/25 | Loss: 0.00091250
Iteration 8/25 | Loss: 0.00091250
Iteration 9/25 | Loss: 0.00091250
Iteration 10/25 | Loss: 0.00091250
Iteration 11/25 | Loss: 0.00091250
Iteration 12/25 | Loss: 0.00091250
Iteration 13/25 | Loss: 0.00091250
Iteration 14/25 | Loss: 0.00091250
Iteration 15/25 | Loss: 0.00091250
Iteration 16/25 | Loss: 0.00091250
Iteration 17/25 | Loss: 0.00091250
Iteration 18/25 | Loss: 0.00091250
Iteration 19/25 | Loss: 0.00091250
Iteration 20/25 | Loss: 0.00091250
Iteration 21/25 | Loss: 0.00091250
Iteration 22/25 | Loss: 0.00091250
Iteration 23/25 | Loss: 0.00091250
Iteration 24/25 | Loss: 0.00091250
Iteration 25/25 | Loss: 0.00091250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091250
Iteration 2/1000 | Loss: 0.00003146
Iteration 3/1000 | Loss: 0.00002336
Iteration 4/1000 | Loss: 0.00002155
Iteration 5/1000 | Loss: 0.00002027
Iteration 6/1000 | Loss: 0.00001948
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001807
Iteration 10/1000 | Loss: 0.00001781
Iteration 11/1000 | Loss: 0.00001756
Iteration 12/1000 | Loss: 0.00001751
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001741
Iteration 16/1000 | Loss: 0.00001740
Iteration 17/1000 | Loss: 0.00001735
Iteration 18/1000 | Loss: 0.00001732
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001731
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001716
Iteration 23/1000 | Loss: 0.00001716
Iteration 24/1000 | Loss: 0.00001716
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001716
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001716
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001714
Iteration 32/1000 | Loss: 0.00001714
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001713
Iteration 37/1000 | Loss: 0.00001713
Iteration 38/1000 | Loss: 0.00001712
Iteration 39/1000 | Loss: 0.00001712
Iteration 40/1000 | Loss: 0.00001712
Iteration 41/1000 | Loss: 0.00001711
Iteration 42/1000 | Loss: 0.00001711
Iteration 43/1000 | Loss: 0.00001711
Iteration 44/1000 | Loss: 0.00001711
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001710
Iteration 47/1000 | Loss: 0.00001709
Iteration 48/1000 | Loss: 0.00001708
Iteration 49/1000 | Loss: 0.00001708
Iteration 50/1000 | Loss: 0.00001708
Iteration 51/1000 | Loss: 0.00001708
Iteration 52/1000 | Loss: 0.00001708
Iteration 53/1000 | Loss: 0.00001707
Iteration 54/1000 | Loss: 0.00001707
Iteration 55/1000 | Loss: 0.00001707
Iteration 56/1000 | Loss: 0.00001707
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001707
Iteration 59/1000 | Loss: 0.00001706
Iteration 60/1000 | Loss: 0.00001706
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001703
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001702
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001701
Iteration 78/1000 | Loss: 0.00001700
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001699
Iteration 82/1000 | Loss: 0.00001699
Iteration 83/1000 | Loss: 0.00001699
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001694
Iteration 105/1000 | Loss: 0.00001694
Iteration 106/1000 | Loss: 0.00001693
Iteration 107/1000 | Loss: 0.00001693
Iteration 108/1000 | Loss: 0.00001693
Iteration 109/1000 | Loss: 0.00001693
Iteration 110/1000 | Loss: 0.00001692
Iteration 111/1000 | Loss: 0.00001692
Iteration 112/1000 | Loss: 0.00001692
Iteration 113/1000 | Loss: 0.00001692
Iteration 114/1000 | Loss: 0.00001692
Iteration 115/1000 | Loss: 0.00001691
Iteration 116/1000 | Loss: 0.00001691
Iteration 117/1000 | Loss: 0.00001691
Iteration 118/1000 | Loss: 0.00001691
Iteration 119/1000 | Loss: 0.00001691
Iteration 120/1000 | Loss: 0.00001690
Iteration 121/1000 | Loss: 0.00001690
Iteration 122/1000 | Loss: 0.00001690
Iteration 123/1000 | Loss: 0.00001689
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001689
Iteration 131/1000 | Loss: 0.00001689
Iteration 132/1000 | Loss: 0.00001689
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001688
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001685
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001685
Iteration 153/1000 | Loss: 0.00001685
Iteration 154/1000 | Loss: 0.00001685
Iteration 155/1000 | Loss: 0.00001685
Iteration 156/1000 | Loss: 0.00001685
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Iteration 160/1000 | Loss: 0.00001684
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001684
Iteration 163/1000 | Loss: 0.00001684
Iteration 164/1000 | Loss: 0.00001684
Iteration 165/1000 | Loss: 0.00001684
Iteration 166/1000 | Loss: 0.00001684
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001683
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Iteration 173/1000 | Loss: 0.00001683
Iteration 174/1000 | Loss: 0.00001683
Iteration 175/1000 | Loss: 0.00001683
Iteration 176/1000 | Loss: 0.00001683
Iteration 177/1000 | Loss: 0.00001683
Iteration 178/1000 | Loss: 0.00001683
Iteration 179/1000 | Loss: 0.00001683
Iteration 180/1000 | Loss: 0.00001683
Iteration 181/1000 | Loss: 0.00001683
Iteration 182/1000 | Loss: 0.00001683
Iteration 183/1000 | Loss: 0.00001683
Iteration 184/1000 | Loss: 0.00001683
Iteration 185/1000 | Loss: 0.00001683
Iteration 186/1000 | Loss: 0.00001683
Iteration 187/1000 | Loss: 0.00001683
Iteration 188/1000 | Loss: 0.00001683
Iteration 189/1000 | Loss: 0.00001683
Iteration 190/1000 | Loss: 0.00001683
Iteration 191/1000 | Loss: 0.00001683
Iteration 192/1000 | Loss: 0.00001683
Iteration 193/1000 | Loss: 0.00001683
Iteration 194/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.6831025277497247e-05, 1.6831025277497247e-05, 1.6831025277497247e-05, 1.6831025277497247e-05, 1.6831025277497247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6831025277497247e-05

Optimization complete. Final v2v error: 3.4734673500061035 mm

Highest mean error: 3.9652161598205566 mm for frame 74

Lowest mean error: 3.1960692405700684 mm for frame 216

Saving results

Total time: 45.964022636413574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393341
Iteration 2/25 | Loss: 0.00132259
Iteration 3/25 | Loss: 0.00127347
Iteration 4/25 | Loss: 0.00126589
Iteration 5/25 | Loss: 0.00126305
Iteration 6/25 | Loss: 0.00126298
Iteration 7/25 | Loss: 0.00126298
Iteration 8/25 | Loss: 0.00126298
Iteration 9/25 | Loss: 0.00126298
Iteration 10/25 | Loss: 0.00126298
Iteration 11/25 | Loss: 0.00126298
Iteration 12/25 | Loss: 0.00126298
Iteration 13/25 | Loss: 0.00126298
Iteration 14/25 | Loss: 0.00126298
Iteration 15/25 | Loss: 0.00126298
Iteration 16/25 | Loss: 0.00126298
Iteration 17/25 | Loss: 0.00126298
Iteration 18/25 | Loss: 0.00126298
Iteration 19/25 | Loss: 0.00126298
Iteration 20/25 | Loss: 0.00126298
Iteration 21/25 | Loss: 0.00126298
Iteration 22/25 | Loss: 0.00126298
Iteration 23/25 | Loss: 0.00126298
Iteration 24/25 | Loss: 0.00126298
Iteration 25/25 | Loss: 0.00126298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94153762
Iteration 2/25 | Loss: 0.00086439
Iteration 3/25 | Loss: 0.00086438
Iteration 4/25 | Loss: 0.00086438
Iteration 5/25 | Loss: 0.00086438
Iteration 6/25 | Loss: 0.00086438
Iteration 7/25 | Loss: 0.00086438
Iteration 8/25 | Loss: 0.00086438
Iteration 9/25 | Loss: 0.00086438
Iteration 10/25 | Loss: 0.00086438
Iteration 11/25 | Loss: 0.00086438
Iteration 12/25 | Loss: 0.00086438
Iteration 13/25 | Loss: 0.00086438
Iteration 14/25 | Loss: 0.00086438
Iteration 15/25 | Loss: 0.00086438
Iteration 16/25 | Loss: 0.00086438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008643804467283189, 0.0008643804467283189, 0.0008643804467283189, 0.0008643804467283189, 0.0008643804467283189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008643804467283189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086438
Iteration 2/1000 | Loss: 0.00002174
Iteration 3/1000 | Loss: 0.00001642
Iteration 4/1000 | Loss: 0.00001512
Iteration 5/1000 | Loss: 0.00001457
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001302
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001264
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001262
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001254
Iteration 19/1000 | Loss: 0.00001251
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001248
Iteration 22/1000 | Loss: 0.00001247
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001240
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001236
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001224
Iteration 56/1000 | Loss: 0.00001224
Iteration 57/1000 | Loss: 0.00001224
Iteration 58/1000 | Loss: 0.00001223
Iteration 59/1000 | Loss: 0.00001223
Iteration 60/1000 | Loss: 0.00001223
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001221
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001218
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001217
Iteration 79/1000 | Loss: 0.00001217
Iteration 80/1000 | Loss: 0.00001217
Iteration 81/1000 | Loss: 0.00001217
Iteration 82/1000 | Loss: 0.00001216
Iteration 83/1000 | Loss: 0.00001216
Iteration 84/1000 | Loss: 0.00001216
Iteration 85/1000 | Loss: 0.00001216
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001214
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001212
Iteration 96/1000 | Loss: 0.00001212
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001212
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001209
Iteration 108/1000 | Loss: 0.00001209
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001208
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001206
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001204
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001204
Iteration 136/1000 | Loss: 0.00001204
Iteration 137/1000 | Loss: 0.00001204
Iteration 138/1000 | Loss: 0.00001204
Iteration 139/1000 | Loss: 0.00001204
Iteration 140/1000 | Loss: 0.00001204
Iteration 141/1000 | Loss: 0.00001204
Iteration 142/1000 | Loss: 0.00001204
Iteration 143/1000 | Loss: 0.00001204
Iteration 144/1000 | Loss: 0.00001204
Iteration 145/1000 | Loss: 0.00001204
Iteration 146/1000 | Loss: 0.00001204
Iteration 147/1000 | Loss: 0.00001204
Iteration 148/1000 | Loss: 0.00001204
Iteration 149/1000 | Loss: 0.00001204
Iteration 150/1000 | Loss: 0.00001204
Iteration 151/1000 | Loss: 0.00001204
Iteration 152/1000 | Loss: 0.00001204
Iteration 153/1000 | Loss: 0.00001204
Iteration 154/1000 | Loss: 0.00001204
Iteration 155/1000 | Loss: 0.00001204
Iteration 156/1000 | Loss: 0.00001204
Iteration 157/1000 | Loss: 0.00001204
Iteration 158/1000 | Loss: 0.00001204
Iteration 159/1000 | Loss: 0.00001204
Iteration 160/1000 | Loss: 0.00001204
Iteration 161/1000 | Loss: 0.00001204
Iteration 162/1000 | Loss: 0.00001204
Iteration 163/1000 | Loss: 0.00001204
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001204
Iteration 166/1000 | Loss: 0.00001204
Iteration 167/1000 | Loss: 0.00001204
Iteration 168/1000 | Loss: 0.00001204
Iteration 169/1000 | Loss: 0.00001204
Iteration 170/1000 | Loss: 0.00001204
Iteration 171/1000 | Loss: 0.00001204
Iteration 172/1000 | Loss: 0.00001204
Iteration 173/1000 | Loss: 0.00001204
Iteration 174/1000 | Loss: 0.00001204
Iteration 175/1000 | Loss: 0.00001204
Iteration 176/1000 | Loss: 0.00001204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.2035982763336506e-05, 1.2035982763336506e-05, 1.2035982763336506e-05, 1.2035982763336506e-05, 1.2035982763336506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2035982763336506e-05

Optimization complete. Final v2v error: 2.980158567428589 mm

Highest mean error: 3.166865110397339 mm for frame 76

Lowest mean error: 2.8555493354797363 mm for frame 1

Saving results

Total time: 39.948092222213745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471844
Iteration 2/25 | Loss: 0.00161641
Iteration 3/25 | Loss: 0.00141426
Iteration 4/25 | Loss: 0.00138947
Iteration 5/25 | Loss: 0.00138581
Iteration 6/25 | Loss: 0.00138546
Iteration 7/25 | Loss: 0.00138546
Iteration 8/25 | Loss: 0.00138546
Iteration 9/25 | Loss: 0.00138546
Iteration 10/25 | Loss: 0.00138546
Iteration 11/25 | Loss: 0.00138546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013854613061994314, 0.0013854613061994314, 0.0013854613061994314, 0.0013854613061994314, 0.0013854613061994314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013854613061994314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39365923
Iteration 2/25 | Loss: 0.00095810
Iteration 3/25 | Loss: 0.00095808
Iteration 4/25 | Loss: 0.00095808
Iteration 5/25 | Loss: 0.00095808
Iteration 6/25 | Loss: 0.00095808
Iteration 7/25 | Loss: 0.00095808
Iteration 8/25 | Loss: 0.00095808
Iteration 9/25 | Loss: 0.00095808
Iteration 10/25 | Loss: 0.00095808
Iteration 11/25 | Loss: 0.00095808
Iteration 12/25 | Loss: 0.00095808
Iteration 13/25 | Loss: 0.00095808
Iteration 14/25 | Loss: 0.00095808
Iteration 15/25 | Loss: 0.00095808
Iteration 16/25 | Loss: 0.00095808
Iteration 17/25 | Loss: 0.00095808
Iteration 18/25 | Loss: 0.00095808
Iteration 19/25 | Loss: 0.00095808
Iteration 20/25 | Loss: 0.00095808
Iteration 21/25 | Loss: 0.00095808
Iteration 22/25 | Loss: 0.00095808
Iteration 23/25 | Loss: 0.00095808
Iteration 24/25 | Loss: 0.00095808
Iteration 25/25 | Loss: 0.00095808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095808
Iteration 2/1000 | Loss: 0.00005491
Iteration 3/1000 | Loss: 0.00003286
Iteration 4/1000 | Loss: 0.00002983
Iteration 5/1000 | Loss: 0.00002818
Iteration 6/1000 | Loss: 0.00002671
Iteration 7/1000 | Loss: 0.00002577
Iteration 8/1000 | Loss: 0.00002516
Iteration 9/1000 | Loss: 0.00002439
Iteration 10/1000 | Loss: 0.00002377
Iteration 11/1000 | Loss: 0.00002347
Iteration 12/1000 | Loss: 0.00002324
Iteration 13/1000 | Loss: 0.00002312
Iteration 14/1000 | Loss: 0.00002301
Iteration 15/1000 | Loss: 0.00002291
Iteration 16/1000 | Loss: 0.00002276
Iteration 17/1000 | Loss: 0.00002274
Iteration 18/1000 | Loss: 0.00002273
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002264
Iteration 21/1000 | Loss: 0.00002262
Iteration 22/1000 | Loss: 0.00002261
Iteration 23/1000 | Loss: 0.00002260
Iteration 24/1000 | Loss: 0.00002260
Iteration 25/1000 | Loss: 0.00002259
Iteration 26/1000 | Loss: 0.00002259
Iteration 27/1000 | Loss: 0.00002258
Iteration 28/1000 | Loss: 0.00002258
Iteration 29/1000 | Loss: 0.00002256
Iteration 30/1000 | Loss: 0.00002256
Iteration 31/1000 | Loss: 0.00002256
Iteration 32/1000 | Loss: 0.00002256
Iteration 33/1000 | Loss: 0.00002256
Iteration 34/1000 | Loss: 0.00002256
Iteration 35/1000 | Loss: 0.00002256
Iteration 36/1000 | Loss: 0.00002255
Iteration 37/1000 | Loss: 0.00002255
Iteration 38/1000 | Loss: 0.00002253
Iteration 39/1000 | Loss: 0.00002253
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002251
Iteration 44/1000 | Loss: 0.00002251
Iteration 45/1000 | Loss: 0.00002250
Iteration 46/1000 | Loss: 0.00002250
Iteration 47/1000 | Loss: 0.00002250
Iteration 48/1000 | Loss: 0.00002248
Iteration 49/1000 | Loss: 0.00002248
Iteration 50/1000 | Loss: 0.00002248
Iteration 51/1000 | Loss: 0.00002248
Iteration 52/1000 | Loss: 0.00002248
Iteration 53/1000 | Loss: 0.00002248
Iteration 54/1000 | Loss: 0.00002248
Iteration 55/1000 | Loss: 0.00002247
Iteration 56/1000 | Loss: 0.00002247
Iteration 57/1000 | Loss: 0.00002247
Iteration 58/1000 | Loss: 0.00002245
Iteration 59/1000 | Loss: 0.00002244
Iteration 60/1000 | Loss: 0.00002243
Iteration 61/1000 | Loss: 0.00002243
Iteration 62/1000 | Loss: 0.00002243
Iteration 63/1000 | Loss: 0.00002243
Iteration 64/1000 | Loss: 0.00002243
Iteration 65/1000 | Loss: 0.00002243
Iteration 66/1000 | Loss: 0.00002243
Iteration 67/1000 | Loss: 0.00002243
Iteration 68/1000 | Loss: 0.00002243
Iteration 69/1000 | Loss: 0.00002243
Iteration 70/1000 | Loss: 0.00002242
Iteration 71/1000 | Loss: 0.00002242
Iteration 72/1000 | Loss: 0.00002241
Iteration 73/1000 | Loss: 0.00002241
Iteration 74/1000 | Loss: 0.00002241
Iteration 75/1000 | Loss: 0.00002240
Iteration 76/1000 | Loss: 0.00002239
Iteration 77/1000 | Loss: 0.00002239
Iteration 78/1000 | Loss: 0.00002239
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00002238
Iteration 81/1000 | Loss: 0.00002238
Iteration 82/1000 | Loss: 0.00002238
Iteration 83/1000 | Loss: 0.00002238
Iteration 84/1000 | Loss: 0.00002238
Iteration 85/1000 | Loss: 0.00002238
Iteration 86/1000 | Loss: 0.00002237
Iteration 87/1000 | Loss: 0.00002237
Iteration 88/1000 | Loss: 0.00002237
Iteration 89/1000 | Loss: 0.00002236
Iteration 90/1000 | Loss: 0.00002236
Iteration 91/1000 | Loss: 0.00002236
Iteration 92/1000 | Loss: 0.00002235
Iteration 93/1000 | Loss: 0.00002235
Iteration 94/1000 | Loss: 0.00002235
Iteration 95/1000 | Loss: 0.00002235
Iteration 96/1000 | Loss: 0.00002234
Iteration 97/1000 | Loss: 0.00002234
Iteration 98/1000 | Loss: 0.00002234
Iteration 99/1000 | Loss: 0.00002232
Iteration 100/1000 | Loss: 0.00002232
Iteration 101/1000 | Loss: 0.00002232
Iteration 102/1000 | Loss: 0.00002232
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002231
Iteration 106/1000 | Loss: 0.00002231
Iteration 107/1000 | Loss: 0.00002230
Iteration 108/1000 | Loss: 0.00002230
Iteration 109/1000 | Loss: 0.00002230
Iteration 110/1000 | Loss: 0.00002230
Iteration 111/1000 | Loss: 0.00002230
Iteration 112/1000 | Loss: 0.00002230
Iteration 113/1000 | Loss: 0.00002230
Iteration 114/1000 | Loss: 0.00002230
Iteration 115/1000 | Loss: 0.00002230
Iteration 116/1000 | Loss: 0.00002229
Iteration 117/1000 | Loss: 0.00002229
Iteration 118/1000 | Loss: 0.00002229
Iteration 119/1000 | Loss: 0.00002228
Iteration 120/1000 | Loss: 0.00002228
Iteration 121/1000 | Loss: 0.00002228
Iteration 122/1000 | Loss: 0.00002228
Iteration 123/1000 | Loss: 0.00002228
Iteration 124/1000 | Loss: 0.00002228
Iteration 125/1000 | Loss: 0.00002228
Iteration 126/1000 | Loss: 0.00002227
Iteration 127/1000 | Loss: 0.00002227
Iteration 128/1000 | Loss: 0.00002227
Iteration 129/1000 | Loss: 0.00002227
Iteration 130/1000 | Loss: 0.00002227
Iteration 131/1000 | Loss: 0.00002227
Iteration 132/1000 | Loss: 0.00002227
Iteration 133/1000 | Loss: 0.00002226
Iteration 134/1000 | Loss: 0.00002226
Iteration 135/1000 | Loss: 0.00002226
Iteration 136/1000 | Loss: 0.00002226
Iteration 137/1000 | Loss: 0.00002226
Iteration 138/1000 | Loss: 0.00002226
Iteration 139/1000 | Loss: 0.00002226
Iteration 140/1000 | Loss: 0.00002226
Iteration 141/1000 | Loss: 0.00002225
Iteration 142/1000 | Loss: 0.00002225
Iteration 143/1000 | Loss: 0.00002225
Iteration 144/1000 | Loss: 0.00002225
Iteration 145/1000 | Loss: 0.00002225
Iteration 146/1000 | Loss: 0.00002224
Iteration 147/1000 | Loss: 0.00002224
Iteration 148/1000 | Loss: 0.00002224
Iteration 149/1000 | Loss: 0.00002224
Iteration 150/1000 | Loss: 0.00002224
Iteration 151/1000 | Loss: 0.00002224
Iteration 152/1000 | Loss: 0.00002224
Iteration 153/1000 | Loss: 0.00002224
Iteration 154/1000 | Loss: 0.00002224
Iteration 155/1000 | Loss: 0.00002224
Iteration 156/1000 | Loss: 0.00002223
Iteration 157/1000 | Loss: 0.00002223
Iteration 158/1000 | Loss: 0.00002223
Iteration 159/1000 | Loss: 0.00002223
Iteration 160/1000 | Loss: 0.00002223
Iteration 161/1000 | Loss: 0.00002223
Iteration 162/1000 | Loss: 0.00002223
Iteration 163/1000 | Loss: 0.00002223
Iteration 164/1000 | Loss: 0.00002223
Iteration 165/1000 | Loss: 0.00002223
Iteration 166/1000 | Loss: 0.00002223
Iteration 167/1000 | Loss: 0.00002222
Iteration 168/1000 | Loss: 0.00002222
Iteration 169/1000 | Loss: 0.00002222
Iteration 170/1000 | Loss: 0.00002222
Iteration 171/1000 | Loss: 0.00002222
Iteration 172/1000 | Loss: 0.00002222
Iteration 173/1000 | Loss: 0.00002222
Iteration 174/1000 | Loss: 0.00002222
Iteration 175/1000 | Loss: 0.00002222
Iteration 176/1000 | Loss: 0.00002222
Iteration 177/1000 | Loss: 0.00002222
Iteration 178/1000 | Loss: 0.00002222
Iteration 179/1000 | Loss: 0.00002222
Iteration 180/1000 | Loss: 0.00002221
Iteration 181/1000 | Loss: 0.00002221
Iteration 182/1000 | Loss: 0.00002221
Iteration 183/1000 | Loss: 0.00002221
Iteration 184/1000 | Loss: 0.00002221
Iteration 185/1000 | Loss: 0.00002221
Iteration 186/1000 | Loss: 0.00002221
Iteration 187/1000 | Loss: 0.00002221
Iteration 188/1000 | Loss: 0.00002221
Iteration 189/1000 | Loss: 0.00002221
Iteration 190/1000 | Loss: 0.00002221
Iteration 191/1000 | Loss: 0.00002221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.2208601876627654e-05, 2.2208601876627654e-05, 2.2208601876627654e-05, 2.2208601876627654e-05, 2.2208601876627654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2208601876627654e-05

Optimization complete. Final v2v error: 3.9350903034210205 mm

Highest mean error: 4.210322856903076 mm for frame 164

Lowest mean error: 3.5280518531799316 mm for frame 27

Saving results

Total time: 50.50605869293213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991464
Iteration 2/25 | Loss: 0.00343775
Iteration 3/25 | Loss: 0.00274967
Iteration 4/25 | Loss: 0.00235921
Iteration 5/25 | Loss: 0.00226683
Iteration 6/25 | Loss: 0.00226119
Iteration 7/25 | Loss: 0.00214731
Iteration 8/25 | Loss: 0.00202817
Iteration 9/25 | Loss: 0.00188031
Iteration 10/25 | Loss: 0.00179753
Iteration 11/25 | Loss: 0.00177285
Iteration 12/25 | Loss: 0.00176411
Iteration 13/25 | Loss: 0.00174099
Iteration 14/25 | Loss: 0.00174384
Iteration 15/25 | Loss: 0.00172375
Iteration 16/25 | Loss: 0.00173132
Iteration 17/25 | Loss: 0.00171243
Iteration 18/25 | Loss: 0.00170462
Iteration 19/25 | Loss: 0.00169803
Iteration 20/25 | Loss: 0.00169700
Iteration 21/25 | Loss: 0.00170113
Iteration 22/25 | Loss: 0.00169586
Iteration 23/25 | Loss: 0.00169530
Iteration 24/25 | Loss: 0.00169513
Iteration 25/25 | Loss: 0.00169512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40811241
Iteration 2/25 | Loss: 0.00391761
Iteration 3/25 | Loss: 0.00218830
Iteration 4/25 | Loss: 0.00218784
Iteration 5/25 | Loss: 0.00218784
Iteration 6/25 | Loss: 0.00218784
Iteration 7/25 | Loss: 0.00218784
Iteration 8/25 | Loss: 0.00218784
Iteration 9/25 | Loss: 0.00218784
Iteration 10/25 | Loss: 0.00218784
Iteration 11/25 | Loss: 0.00218783
Iteration 12/25 | Loss: 0.00218783
Iteration 13/25 | Loss: 0.00218783
Iteration 14/25 | Loss: 0.00218783
Iteration 15/25 | Loss: 0.00218783
Iteration 16/25 | Loss: 0.00218783
Iteration 17/25 | Loss: 0.00218783
Iteration 18/25 | Loss: 0.00218783
Iteration 19/25 | Loss: 0.00218783
Iteration 20/25 | Loss: 0.00218783
Iteration 21/25 | Loss: 0.00218783
Iteration 22/25 | Loss: 0.00218783
Iteration 23/25 | Loss: 0.00218783
Iteration 24/25 | Loss: 0.00218783
Iteration 25/25 | Loss: 0.00218783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218783
Iteration 2/1000 | Loss: 0.00154510
Iteration 3/1000 | Loss: 0.00497314
Iteration 4/1000 | Loss: 0.00020761
Iteration 5/1000 | Loss: 0.00058625
Iteration 6/1000 | Loss: 0.00017133
Iteration 7/1000 | Loss: 0.00131641
Iteration 8/1000 | Loss: 0.00144772
Iteration 9/1000 | Loss: 0.00402227
Iteration 10/1000 | Loss: 0.00026970
Iteration 11/1000 | Loss: 0.00057957
Iteration 12/1000 | Loss: 0.00115588
Iteration 13/1000 | Loss: 0.00080608
Iteration 14/1000 | Loss: 0.00047706
Iteration 15/1000 | Loss: 0.00024545
Iteration 16/1000 | Loss: 0.00015291
Iteration 17/1000 | Loss: 0.00046941
Iteration 18/1000 | Loss: 0.00056289
Iteration 19/1000 | Loss: 0.00017797
Iteration 20/1000 | Loss: 0.00089757
Iteration 21/1000 | Loss: 0.00126660
Iteration 22/1000 | Loss: 0.00295885
Iteration 23/1000 | Loss: 0.00318709
Iteration 24/1000 | Loss: 0.00067709
Iteration 25/1000 | Loss: 0.00018440
Iteration 26/1000 | Loss: 0.00020157
Iteration 27/1000 | Loss: 0.00020897
Iteration 28/1000 | Loss: 0.00020837
Iteration 29/1000 | Loss: 0.00079347
Iteration 30/1000 | Loss: 0.00038016
Iteration 31/1000 | Loss: 0.00044313
Iteration 32/1000 | Loss: 0.00050522
Iteration 33/1000 | Loss: 0.00020996
Iteration 34/1000 | Loss: 0.00034574
Iteration 35/1000 | Loss: 0.00016464
Iteration 36/1000 | Loss: 0.00017027
Iteration 37/1000 | Loss: 0.00033533
Iteration 38/1000 | Loss: 0.00013963
Iteration 39/1000 | Loss: 0.00013805
Iteration 40/1000 | Loss: 0.00013665
Iteration 41/1000 | Loss: 0.00013551
Iteration 42/1000 | Loss: 0.00073888
Iteration 43/1000 | Loss: 0.00190514
Iteration 44/1000 | Loss: 0.00404562
Iteration 45/1000 | Loss: 0.00133812
Iteration 46/1000 | Loss: 0.00063613
Iteration 47/1000 | Loss: 0.00021235
Iteration 48/1000 | Loss: 0.00013186
Iteration 49/1000 | Loss: 0.00013021
Iteration 50/1000 | Loss: 0.00027111
Iteration 51/1000 | Loss: 0.00012747
Iteration 52/1000 | Loss: 0.00012599
Iteration 53/1000 | Loss: 0.00012451
Iteration 54/1000 | Loss: 0.00054827
Iteration 55/1000 | Loss: 0.00012348
Iteration 56/1000 | Loss: 0.00012217
Iteration 57/1000 | Loss: 0.00012143
Iteration 58/1000 | Loss: 0.00060533
Iteration 59/1000 | Loss: 0.00012074
Iteration 60/1000 | Loss: 0.00011983
Iteration 61/1000 | Loss: 0.00011920
Iteration 62/1000 | Loss: 0.00011866
Iteration 63/1000 | Loss: 0.00052869
Iteration 64/1000 | Loss: 0.00120668
Iteration 65/1000 | Loss: 0.00201540
Iteration 66/1000 | Loss: 0.00193569
Iteration 67/1000 | Loss: 0.00036700
Iteration 68/1000 | Loss: 0.00085340
Iteration 69/1000 | Loss: 0.00018100
Iteration 70/1000 | Loss: 0.00087096
Iteration 71/1000 | Loss: 0.00012314
Iteration 72/1000 | Loss: 0.00106764
Iteration 73/1000 | Loss: 0.00050376
Iteration 74/1000 | Loss: 0.00073552
Iteration 75/1000 | Loss: 0.00007289
Iteration 76/1000 | Loss: 0.00006390
Iteration 77/1000 | Loss: 0.00036168
Iteration 78/1000 | Loss: 0.00005751
Iteration 79/1000 | Loss: 0.00041604
Iteration 80/1000 | Loss: 0.00005457
Iteration 81/1000 | Loss: 0.00056871
Iteration 82/1000 | Loss: 0.00102439
Iteration 83/1000 | Loss: 0.00026949
Iteration 84/1000 | Loss: 0.00005419
Iteration 85/1000 | Loss: 0.00005092
Iteration 86/1000 | Loss: 0.00004971
Iteration 87/1000 | Loss: 0.00049665
Iteration 88/1000 | Loss: 0.00039797
Iteration 89/1000 | Loss: 0.00024272
Iteration 90/1000 | Loss: 0.00004857
Iteration 91/1000 | Loss: 0.00004761
Iteration 92/1000 | Loss: 0.00022163
Iteration 93/1000 | Loss: 0.00020708
Iteration 94/1000 | Loss: 0.00005182
Iteration 95/1000 | Loss: 0.00004820
Iteration 96/1000 | Loss: 0.00004688
Iteration 97/1000 | Loss: 0.00004653
Iteration 98/1000 | Loss: 0.00004622
Iteration 99/1000 | Loss: 0.00004600
Iteration 100/1000 | Loss: 0.00004593
Iteration 101/1000 | Loss: 0.00004587
Iteration 102/1000 | Loss: 0.00004585
Iteration 103/1000 | Loss: 0.00004585
Iteration 104/1000 | Loss: 0.00004581
Iteration 105/1000 | Loss: 0.00004572
Iteration 106/1000 | Loss: 0.00004572
Iteration 107/1000 | Loss: 0.00004566
Iteration 108/1000 | Loss: 0.00004565
Iteration 109/1000 | Loss: 0.00004565
Iteration 110/1000 | Loss: 0.00004564
Iteration 111/1000 | Loss: 0.00004564
Iteration 112/1000 | Loss: 0.00004564
Iteration 113/1000 | Loss: 0.00004564
Iteration 114/1000 | Loss: 0.00004563
Iteration 115/1000 | Loss: 0.00004561
Iteration 116/1000 | Loss: 0.00004561
Iteration 117/1000 | Loss: 0.00004561
Iteration 118/1000 | Loss: 0.00004561
Iteration 119/1000 | Loss: 0.00004561
Iteration 120/1000 | Loss: 0.00004561
Iteration 121/1000 | Loss: 0.00004561
Iteration 122/1000 | Loss: 0.00004561
Iteration 123/1000 | Loss: 0.00004561
Iteration 124/1000 | Loss: 0.00004561
Iteration 125/1000 | Loss: 0.00004560
Iteration 126/1000 | Loss: 0.00004560
Iteration 127/1000 | Loss: 0.00004560
Iteration 128/1000 | Loss: 0.00004560
Iteration 129/1000 | Loss: 0.00004560
Iteration 130/1000 | Loss: 0.00004558
Iteration 131/1000 | Loss: 0.00004558
Iteration 132/1000 | Loss: 0.00004557
Iteration 133/1000 | Loss: 0.00004557
Iteration 134/1000 | Loss: 0.00004556
Iteration 135/1000 | Loss: 0.00004555
Iteration 136/1000 | Loss: 0.00004554
Iteration 137/1000 | Loss: 0.00004554
Iteration 138/1000 | Loss: 0.00004554
Iteration 139/1000 | Loss: 0.00004553
Iteration 140/1000 | Loss: 0.00004553
Iteration 141/1000 | Loss: 0.00004553
Iteration 142/1000 | Loss: 0.00004553
Iteration 143/1000 | Loss: 0.00004553
Iteration 144/1000 | Loss: 0.00004552
Iteration 145/1000 | Loss: 0.00004552
Iteration 146/1000 | Loss: 0.00004552
Iteration 147/1000 | Loss: 0.00004552
Iteration 148/1000 | Loss: 0.00004552
Iteration 149/1000 | Loss: 0.00004552
Iteration 150/1000 | Loss: 0.00004551
Iteration 151/1000 | Loss: 0.00004551
Iteration 152/1000 | Loss: 0.00004550
Iteration 153/1000 | Loss: 0.00004550
Iteration 154/1000 | Loss: 0.00004550
Iteration 155/1000 | Loss: 0.00004550
Iteration 156/1000 | Loss: 0.00004550
Iteration 157/1000 | Loss: 0.00004550
Iteration 158/1000 | Loss: 0.00004550
Iteration 159/1000 | Loss: 0.00004550
Iteration 160/1000 | Loss: 0.00004550
Iteration 161/1000 | Loss: 0.00004550
Iteration 162/1000 | Loss: 0.00004549
Iteration 163/1000 | Loss: 0.00004549
Iteration 164/1000 | Loss: 0.00004549
Iteration 165/1000 | Loss: 0.00004548
Iteration 166/1000 | Loss: 0.00004548
Iteration 167/1000 | Loss: 0.00004548
Iteration 168/1000 | Loss: 0.00004548
Iteration 169/1000 | Loss: 0.00004548
Iteration 170/1000 | Loss: 0.00004548
Iteration 171/1000 | Loss: 0.00004548
Iteration 172/1000 | Loss: 0.00004548
Iteration 173/1000 | Loss: 0.00004547
Iteration 174/1000 | Loss: 0.00004547
Iteration 175/1000 | Loss: 0.00004547
Iteration 176/1000 | Loss: 0.00004547
Iteration 177/1000 | Loss: 0.00004547
Iteration 178/1000 | Loss: 0.00004547
Iteration 179/1000 | Loss: 0.00004547
Iteration 180/1000 | Loss: 0.00004547
Iteration 181/1000 | Loss: 0.00004547
Iteration 182/1000 | Loss: 0.00004547
Iteration 183/1000 | Loss: 0.00004547
Iteration 184/1000 | Loss: 0.00004547
Iteration 185/1000 | Loss: 0.00004547
Iteration 186/1000 | Loss: 0.00004547
Iteration 187/1000 | Loss: 0.00004547
Iteration 188/1000 | Loss: 0.00004547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [4.5471377234207466e-05, 4.5471377234207466e-05, 4.5471377234207466e-05, 4.5471377234207466e-05, 4.5471377234207466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5471377234207466e-05

Optimization complete. Final v2v error: 4.944533824920654 mm

Highest mean error: 11.579209327697754 mm for frame 84

Lowest mean error: 3.9384472370147705 mm for frame 108

Saving results

Total time: 183.3186535835266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384297
Iteration 2/25 | Loss: 0.00142109
Iteration 3/25 | Loss: 0.00131155
Iteration 4/25 | Loss: 0.00128466
Iteration 5/25 | Loss: 0.00127792
Iteration 6/25 | Loss: 0.00127660
Iteration 7/25 | Loss: 0.00127660
Iteration 8/25 | Loss: 0.00127660
Iteration 9/25 | Loss: 0.00127660
Iteration 10/25 | Loss: 0.00127660
Iteration 11/25 | Loss: 0.00127660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012765969149768353, 0.0012765969149768353, 0.0012765969149768353, 0.0012765969149768353, 0.0012765969149768353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012765969149768353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37529874
Iteration 2/25 | Loss: 0.00074515
Iteration 3/25 | Loss: 0.00074514
Iteration 4/25 | Loss: 0.00074514
Iteration 5/25 | Loss: 0.00074514
Iteration 6/25 | Loss: 0.00074514
Iteration 7/25 | Loss: 0.00074514
Iteration 8/25 | Loss: 0.00074514
Iteration 9/25 | Loss: 0.00074514
Iteration 10/25 | Loss: 0.00074514
Iteration 11/25 | Loss: 0.00074514
Iteration 12/25 | Loss: 0.00074514
Iteration 13/25 | Loss: 0.00074514
Iteration 14/25 | Loss: 0.00074514
Iteration 15/25 | Loss: 0.00074514
Iteration 16/25 | Loss: 0.00074514
Iteration 17/25 | Loss: 0.00074514
Iteration 18/25 | Loss: 0.00074514
Iteration 19/25 | Loss: 0.00074514
Iteration 20/25 | Loss: 0.00074514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007451405981555581, 0.0007451405981555581, 0.0007451405981555581, 0.0007451405981555581, 0.0007451405981555581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007451405981555581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074514
Iteration 2/1000 | Loss: 0.00003605
Iteration 3/1000 | Loss: 0.00002453
Iteration 4/1000 | Loss: 0.00002247
Iteration 5/1000 | Loss: 0.00002101
Iteration 6/1000 | Loss: 0.00001990
Iteration 7/1000 | Loss: 0.00001930
Iteration 8/1000 | Loss: 0.00001892
Iteration 9/1000 | Loss: 0.00001855
Iteration 10/1000 | Loss: 0.00001832
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001819
Iteration 13/1000 | Loss: 0.00001819
Iteration 14/1000 | Loss: 0.00001817
Iteration 15/1000 | Loss: 0.00001814
Iteration 16/1000 | Loss: 0.00001808
Iteration 17/1000 | Loss: 0.00001797
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001792
Iteration 20/1000 | Loss: 0.00001788
Iteration 21/1000 | Loss: 0.00001785
Iteration 22/1000 | Loss: 0.00001777
Iteration 23/1000 | Loss: 0.00001777
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001774
Iteration 26/1000 | Loss: 0.00001771
Iteration 27/1000 | Loss: 0.00001771
Iteration 28/1000 | Loss: 0.00001770
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00001767
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001766
Iteration 34/1000 | Loss: 0.00001766
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001763
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001762
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001761
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001760
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001755
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001753
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001752
Iteration 53/1000 | Loss: 0.00001752
Iteration 54/1000 | Loss: 0.00001752
Iteration 55/1000 | Loss: 0.00001752
Iteration 56/1000 | Loss: 0.00001751
Iteration 57/1000 | Loss: 0.00001751
Iteration 58/1000 | Loss: 0.00001750
Iteration 59/1000 | Loss: 0.00001750
Iteration 60/1000 | Loss: 0.00001749
Iteration 61/1000 | Loss: 0.00001749
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001745
Iteration 70/1000 | Loss: 0.00001744
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001744
Iteration 73/1000 | Loss: 0.00001744
Iteration 74/1000 | Loss: 0.00001744
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001743
Iteration 79/1000 | Loss: 0.00001742
Iteration 80/1000 | Loss: 0.00001742
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001740
Iteration 84/1000 | Loss: 0.00001740
Iteration 85/1000 | Loss: 0.00001740
Iteration 86/1000 | Loss: 0.00001740
Iteration 87/1000 | Loss: 0.00001739
Iteration 88/1000 | Loss: 0.00001739
Iteration 89/1000 | Loss: 0.00001739
Iteration 90/1000 | Loss: 0.00001738
Iteration 91/1000 | Loss: 0.00001738
Iteration 92/1000 | Loss: 0.00001738
Iteration 93/1000 | Loss: 0.00001738
Iteration 94/1000 | Loss: 0.00001738
Iteration 95/1000 | Loss: 0.00001737
Iteration 96/1000 | Loss: 0.00001737
Iteration 97/1000 | Loss: 0.00001737
Iteration 98/1000 | Loss: 0.00001736
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001736
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001734
Iteration 105/1000 | Loss: 0.00001734
Iteration 106/1000 | Loss: 0.00001734
Iteration 107/1000 | Loss: 0.00001734
Iteration 108/1000 | Loss: 0.00001734
Iteration 109/1000 | Loss: 0.00001734
Iteration 110/1000 | Loss: 0.00001734
Iteration 111/1000 | Loss: 0.00001734
Iteration 112/1000 | Loss: 0.00001733
Iteration 113/1000 | Loss: 0.00001733
Iteration 114/1000 | Loss: 0.00001733
Iteration 115/1000 | Loss: 0.00001733
Iteration 116/1000 | Loss: 0.00001733
Iteration 117/1000 | Loss: 0.00001732
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001731
Iteration 126/1000 | Loss: 0.00001731
Iteration 127/1000 | Loss: 0.00001731
Iteration 128/1000 | Loss: 0.00001731
Iteration 129/1000 | Loss: 0.00001731
Iteration 130/1000 | Loss: 0.00001731
Iteration 131/1000 | Loss: 0.00001731
Iteration 132/1000 | Loss: 0.00001730
Iteration 133/1000 | Loss: 0.00001730
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001730
Iteration 136/1000 | Loss: 0.00001730
Iteration 137/1000 | Loss: 0.00001730
Iteration 138/1000 | Loss: 0.00001730
Iteration 139/1000 | Loss: 0.00001729
Iteration 140/1000 | Loss: 0.00001729
Iteration 141/1000 | Loss: 0.00001729
Iteration 142/1000 | Loss: 0.00001729
Iteration 143/1000 | Loss: 0.00001729
Iteration 144/1000 | Loss: 0.00001729
Iteration 145/1000 | Loss: 0.00001729
Iteration 146/1000 | Loss: 0.00001729
Iteration 147/1000 | Loss: 0.00001729
Iteration 148/1000 | Loss: 0.00001729
Iteration 149/1000 | Loss: 0.00001729
Iteration 150/1000 | Loss: 0.00001728
Iteration 151/1000 | Loss: 0.00001728
Iteration 152/1000 | Loss: 0.00001728
Iteration 153/1000 | Loss: 0.00001728
Iteration 154/1000 | Loss: 0.00001728
Iteration 155/1000 | Loss: 0.00001728
Iteration 156/1000 | Loss: 0.00001728
Iteration 157/1000 | Loss: 0.00001728
Iteration 158/1000 | Loss: 0.00001727
Iteration 159/1000 | Loss: 0.00001727
Iteration 160/1000 | Loss: 0.00001727
Iteration 161/1000 | Loss: 0.00001727
Iteration 162/1000 | Loss: 0.00001727
Iteration 163/1000 | Loss: 0.00001727
Iteration 164/1000 | Loss: 0.00001727
Iteration 165/1000 | Loss: 0.00001727
Iteration 166/1000 | Loss: 0.00001727
Iteration 167/1000 | Loss: 0.00001727
Iteration 168/1000 | Loss: 0.00001727
Iteration 169/1000 | Loss: 0.00001727
Iteration 170/1000 | Loss: 0.00001727
Iteration 171/1000 | Loss: 0.00001727
Iteration 172/1000 | Loss: 0.00001727
Iteration 173/1000 | Loss: 0.00001727
Iteration 174/1000 | Loss: 0.00001727
Iteration 175/1000 | Loss: 0.00001727
Iteration 176/1000 | Loss: 0.00001727
Iteration 177/1000 | Loss: 0.00001727
Iteration 178/1000 | Loss: 0.00001727
Iteration 179/1000 | Loss: 0.00001727
Iteration 180/1000 | Loss: 0.00001727
Iteration 181/1000 | Loss: 0.00001727
Iteration 182/1000 | Loss: 0.00001727
Iteration 183/1000 | Loss: 0.00001727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.72662221302744e-05, 1.72662221302744e-05, 1.72662221302744e-05, 1.72662221302744e-05, 1.72662221302744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.72662221302744e-05

Optimization complete. Final v2v error: 3.5413477420806885 mm

Highest mean error: 3.7767179012298584 mm for frame 145

Lowest mean error: 3.019080638885498 mm for frame 3

Saving results

Total time: 47.50892639160156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948090
Iteration 2/25 | Loss: 0.00284495
Iteration 3/25 | Loss: 0.00210102
Iteration 4/25 | Loss: 0.00170064
Iteration 5/25 | Loss: 0.00171512
Iteration 6/25 | Loss: 0.00165640
Iteration 7/25 | Loss: 0.00163905
Iteration 8/25 | Loss: 0.00157764
Iteration 9/25 | Loss: 0.00156613
Iteration 10/25 | Loss: 0.00154257
Iteration 11/25 | Loss: 0.00152971
Iteration 12/25 | Loss: 0.00153019
Iteration 13/25 | Loss: 0.00152842
Iteration 14/25 | Loss: 0.00151765
Iteration 15/25 | Loss: 0.00151212
Iteration 16/25 | Loss: 0.00150635
Iteration 17/25 | Loss: 0.00150639
Iteration 18/25 | Loss: 0.00151069
Iteration 19/25 | Loss: 0.00150975
Iteration 20/25 | Loss: 0.00150806
Iteration 21/25 | Loss: 0.00151138
Iteration 22/25 | Loss: 0.00150948
Iteration 23/25 | Loss: 0.00151245
Iteration 24/25 | Loss: 0.00150661
Iteration 25/25 | Loss: 0.00150649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41887379
Iteration 2/25 | Loss: 0.00219916
Iteration 3/25 | Loss: 0.00219916
Iteration 4/25 | Loss: 0.00219916
Iteration 5/25 | Loss: 0.00219916
Iteration 6/25 | Loss: 0.00219916
Iteration 7/25 | Loss: 0.00219916
Iteration 8/25 | Loss: 0.00219916
Iteration 9/25 | Loss: 0.00219916
Iteration 10/25 | Loss: 0.00219916
Iteration 11/25 | Loss: 0.00219916
Iteration 12/25 | Loss: 0.00219916
Iteration 13/25 | Loss: 0.00219916
Iteration 14/25 | Loss: 0.00219916
Iteration 15/25 | Loss: 0.00219916
Iteration 16/25 | Loss: 0.00219916
Iteration 17/25 | Loss: 0.00219916
Iteration 18/25 | Loss: 0.00219916
Iteration 19/25 | Loss: 0.00219916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0021991555113345385, 0.0021991555113345385, 0.0021991555113345385, 0.0021991555113345385, 0.0021991555113345385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021991555113345385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219916
Iteration 2/1000 | Loss: 0.00081775
Iteration 3/1000 | Loss: 0.00057653
Iteration 4/1000 | Loss: 0.00191948
Iteration 5/1000 | Loss: 0.00267717
Iteration 6/1000 | Loss: 0.00122867
Iteration 7/1000 | Loss: 0.00030404
Iteration 8/1000 | Loss: 0.00015420
Iteration 9/1000 | Loss: 0.00094896
Iteration 10/1000 | Loss: 0.00067742
Iteration 11/1000 | Loss: 0.00048166
Iteration 12/1000 | Loss: 0.00008534
Iteration 13/1000 | Loss: 0.00006956
Iteration 14/1000 | Loss: 0.00006241
Iteration 15/1000 | Loss: 0.00005648
Iteration 16/1000 | Loss: 0.00044012
Iteration 17/1000 | Loss: 0.00008571
Iteration 18/1000 | Loss: 0.00010243
Iteration 19/1000 | Loss: 0.00005300
Iteration 20/1000 | Loss: 0.00005494
Iteration 21/1000 | Loss: 0.00010659
Iteration 22/1000 | Loss: 0.00007225
Iteration 23/1000 | Loss: 0.00021471
Iteration 24/1000 | Loss: 0.00021718
Iteration 25/1000 | Loss: 0.00053303
Iteration 26/1000 | Loss: 0.00018943
Iteration 27/1000 | Loss: 0.00004649
Iteration 28/1000 | Loss: 0.00014372
Iteration 29/1000 | Loss: 0.00011919
Iteration 30/1000 | Loss: 0.00017812
Iteration 31/1000 | Loss: 0.00014274
Iteration 32/1000 | Loss: 0.00015991
Iteration 33/1000 | Loss: 0.00028090
Iteration 34/1000 | Loss: 0.00025678
Iteration 35/1000 | Loss: 0.00005862
Iteration 36/1000 | Loss: 0.00004762
Iteration 37/1000 | Loss: 0.00004334
Iteration 38/1000 | Loss: 0.00004136
Iteration 39/1000 | Loss: 0.00003996
Iteration 40/1000 | Loss: 0.00003847
Iteration 41/1000 | Loss: 0.00084817
Iteration 42/1000 | Loss: 0.00030944
Iteration 43/1000 | Loss: 0.00032250
Iteration 44/1000 | Loss: 0.00044666
Iteration 45/1000 | Loss: 0.00026980
Iteration 46/1000 | Loss: 0.00050439
Iteration 47/1000 | Loss: 0.00021249
Iteration 48/1000 | Loss: 0.00023584
Iteration 49/1000 | Loss: 0.00004996
Iteration 50/1000 | Loss: 0.00004471
Iteration 51/1000 | Loss: 0.00004158
Iteration 52/1000 | Loss: 0.00003897
Iteration 53/1000 | Loss: 0.00003719
Iteration 54/1000 | Loss: 0.00043667
Iteration 55/1000 | Loss: 0.00005225
Iteration 56/1000 | Loss: 0.00003642
Iteration 57/1000 | Loss: 0.00003385
Iteration 58/1000 | Loss: 0.00003268
Iteration 59/1000 | Loss: 0.00003150
Iteration 60/1000 | Loss: 0.00003060
Iteration 61/1000 | Loss: 0.00002994
Iteration 62/1000 | Loss: 0.00020147
Iteration 63/1000 | Loss: 0.00018760
Iteration 64/1000 | Loss: 0.00020642
Iteration 65/1000 | Loss: 0.00019443
Iteration 66/1000 | Loss: 0.00021579
Iteration 67/1000 | Loss: 0.00019116
Iteration 68/1000 | Loss: 0.00004608
Iteration 69/1000 | Loss: 0.00003343
Iteration 70/1000 | Loss: 0.00003112
Iteration 71/1000 | Loss: 0.00002961
Iteration 72/1000 | Loss: 0.00002800
Iteration 73/1000 | Loss: 0.00054489
Iteration 74/1000 | Loss: 0.00007358
Iteration 75/1000 | Loss: 0.00004231
Iteration 76/1000 | Loss: 0.00003037
Iteration 77/1000 | Loss: 0.00002681
Iteration 78/1000 | Loss: 0.00002494
Iteration 79/1000 | Loss: 0.00002392
Iteration 80/1000 | Loss: 0.00002311
Iteration 81/1000 | Loss: 0.00002224
Iteration 82/1000 | Loss: 0.00002173
Iteration 83/1000 | Loss: 0.00002141
Iteration 84/1000 | Loss: 0.00002134
Iteration 85/1000 | Loss: 0.00002131
Iteration 86/1000 | Loss: 0.00002130
Iteration 87/1000 | Loss: 0.00002129
Iteration 88/1000 | Loss: 0.00002128
Iteration 89/1000 | Loss: 0.00002128
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002122
Iteration 92/1000 | Loss: 0.00016317
Iteration 93/1000 | Loss: 0.00002835
Iteration 94/1000 | Loss: 0.00002429
Iteration 95/1000 | Loss: 0.00002275
Iteration 96/1000 | Loss: 0.00002144
Iteration 97/1000 | Loss: 0.00002051
Iteration 98/1000 | Loss: 0.00001986
Iteration 99/1000 | Loss: 0.00001944
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001917
Iteration 102/1000 | Loss: 0.00001915
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001912
Iteration 107/1000 | Loss: 0.00001909
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001895
Iteration 110/1000 | Loss: 0.00001895
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001893
Iteration 113/1000 | Loss: 0.00001893
Iteration 114/1000 | Loss: 0.00001892
Iteration 115/1000 | Loss: 0.00001887
Iteration 116/1000 | Loss: 0.00001887
Iteration 117/1000 | Loss: 0.00001886
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00001886
Iteration 120/1000 | Loss: 0.00001886
Iteration 121/1000 | Loss: 0.00001886
Iteration 122/1000 | Loss: 0.00001886
Iteration 123/1000 | Loss: 0.00001886
Iteration 124/1000 | Loss: 0.00001886
Iteration 125/1000 | Loss: 0.00001886
Iteration 126/1000 | Loss: 0.00001885
Iteration 127/1000 | Loss: 0.00001885
Iteration 128/1000 | Loss: 0.00001885
Iteration 129/1000 | Loss: 0.00001885
Iteration 130/1000 | Loss: 0.00001885
Iteration 131/1000 | Loss: 0.00001885
Iteration 132/1000 | Loss: 0.00001885
Iteration 133/1000 | Loss: 0.00001885
Iteration 134/1000 | Loss: 0.00001885
Iteration 135/1000 | Loss: 0.00001884
Iteration 136/1000 | Loss: 0.00001884
Iteration 137/1000 | Loss: 0.00001884
Iteration 138/1000 | Loss: 0.00001884
Iteration 139/1000 | Loss: 0.00001884
Iteration 140/1000 | Loss: 0.00001883
Iteration 141/1000 | Loss: 0.00001883
Iteration 142/1000 | Loss: 0.00001883
Iteration 143/1000 | Loss: 0.00001883
Iteration 144/1000 | Loss: 0.00001883
Iteration 145/1000 | Loss: 0.00001883
Iteration 146/1000 | Loss: 0.00001883
Iteration 147/1000 | Loss: 0.00001883
Iteration 148/1000 | Loss: 0.00001883
Iteration 149/1000 | Loss: 0.00001883
Iteration 150/1000 | Loss: 0.00001883
Iteration 151/1000 | Loss: 0.00001883
Iteration 152/1000 | Loss: 0.00001883
Iteration 153/1000 | Loss: 0.00001882
Iteration 154/1000 | Loss: 0.00001882
Iteration 155/1000 | Loss: 0.00001882
Iteration 156/1000 | Loss: 0.00001882
Iteration 157/1000 | Loss: 0.00001882
Iteration 158/1000 | Loss: 0.00001882
Iteration 159/1000 | Loss: 0.00001882
Iteration 160/1000 | Loss: 0.00001882
Iteration 161/1000 | Loss: 0.00001881
Iteration 162/1000 | Loss: 0.00001881
Iteration 163/1000 | Loss: 0.00001881
Iteration 164/1000 | Loss: 0.00001881
Iteration 165/1000 | Loss: 0.00001881
Iteration 166/1000 | Loss: 0.00001881
Iteration 167/1000 | Loss: 0.00001881
Iteration 168/1000 | Loss: 0.00001881
Iteration 169/1000 | Loss: 0.00001881
Iteration 170/1000 | Loss: 0.00001881
Iteration 171/1000 | Loss: 0.00001881
Iteration 172/1000 | Loss: 0.00001880
Iteration 173/1000 | Loss: 0.00001880
Iteration 174/1000 | Loss: 0.00001880
Iteration 175/1000 | Loss: 0.00001880
Iteration 176/1000 | Loss: 0.00001879
Iteration 177/1000 | Loss: 0.00001879
Iteration 178/1000 | Loss: 0.00001878
Iteration 179/1000 | Loss: 0.00001878
Iteration 180/1000 | Loss: 0.00001878
Iteration 181/1000 | Loss: 0.00001878
Iteration 182/1000 | Loss: 0.00001878
Iteration 183/1000 | Loss: 0.00001878
Iteration 184/1000 | Loss: 0.00001878
Iteration 185/1000 | Loss: 0.00001878
Iteration 186/1000 | Loss: 0.00001878
Iteration 187/1000 | Loss: 0.00001877
Iteration 188/1000 | Loss: 0.00001877
Iteration 189/1000 | Loss: 0.00001877
Iteration 190/1000 | Loss: 0.00001877
Iteration 191/1000 | Loss: 0.00001876
Iteration 192/1000 | Loss: 0.00001876
Iteration 193/1000 | Loss: 0.00001876
Iteration 194/1000 | Loss: 0.00001876
Iteration 195/1000 | Loss: 0.00001876
Iteration 196/1000 | Loss: 0.00001875
Iteration 197/1000 | Loss: 0.00001875
Iteration 198/1000 | Loss: 0.00001875
Iteration 199/1000 | Loss: 0.00001875
Iteration 200/1000 | Loss: 0.00001875
Iteration 201/1000 | Loss: 0.00001875
Iteration 202/1000 | Loss: 0.00001875
Iteration 203/1000 | Loss: 0.00001875
Iteration 204/1000 | Loss: 0.00001875
Iteration 205/1000 | Loss: 0.00001874
Iteration 206/1000 | Loss: 0.00001874
Iteration 207/1000 | Loss: 0.00001874
Iteration 208/1000 | Loss: 0.00001874
Iteration 209/1000 | Loss: 0.00001874
Iteration 210/1000 | Loss: 0.00001874
Iteration 211/1000 | Loss: 0.00001874
Iteration 212/1000 | Loss: 0.00001874
Iteration 213/1000 | Loss: 0.00001874
Iteration 214/1000 | Loss: 0.00001874
Iteration 215/1000 | Loss: 0.00001874
Iteration 216/1000 | Loss: 0.00001874
Iteration 217/1000 | Loss: 0.00001874
Iteration 218/1000 | Loss: 0.00001874
Iteration 219/1000 | Loss: 0.00001873
Iteration 220/1000 | Loss: 0.00001873
Iteration 221/1000 | Loss: 0.00001873
Iteration 222/1000 | Loss: 0.00001873
Iteration 223/1000 | Loss: 0.00001873
Iteration 224/1000 | Loss: 0.00001873
Iteration 225/1000 | Loss: 0.00001873
Iteration 226/1000 | Loss: 0.00001873
Iteration 227/1000 | Loss: 0.00001873
Iteration 228/1000 | Loss: 0.00001873
Iteration 229/1000 | Loss: 0.00001873
Iteration 230/1000 | Loss: 0.00001873
Iteration 231/1000 | Loss: 0.00001873
Iteration 232/1000 | Loss: 0.00001873
Iteration 233/1000 | Loss: 0.00001873
Iteration 234/1000 | Loss: 0.00001873
Iteration 235/1000 | Loss: 0.00001873
Iteration 236/1000 | Loss: 0.00001873
Iteration 237/1000 | Loss: 0.00001873
Iteration 238/1000 | Loss: 0.00001873
Iteration 239/1000 | Loss: 0.00001873
Iteration 240/1000 | Loss: 0.00001873
Iteration 241/1000 | Loss: 0.00001873
Iteration 242/1000 | Loss: 0.00001873
Iteration 243/1000 | Loss: 0.00001873
Iteration 244/1000 | Loss: 0.00001873
Iteration 245/1000 | Loss: 0.00001873
Iteration 246/1000 | Loss: 0.00001873
Iteration 247/1000 | Loss: 0.00001873
Iteration 248/1000 | Loss: 0.00001873
Iteration 249/1000 | Loss: 0.00001873
Iteration 250/1000 | Loss: 0.00001873
Iteration 251/1000 | Loss: 0.00001873
Iteration 252/1000 | Loss: 0.00001873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.8727278074948117e-05, 1.8727278074948117e-05, 1.8727278074948117e-05, 1.8727278074948117e-05, 1.8727278074948117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8727278074948117e-05

Optimization complete. Final v2v error: 3.5475785732269287 mm

Highest mean error: 4.629096031188965 mm for frame 72

Lowest mean error: 2.913026809692383 mm for frame 46

Saving results

Total time: 190.54682970046997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944587
Iteration 2/25 | Loss: 0.00296404
Iteration 3/25 | Loss: 0.00219070
Iteration 4/25 | Loss: 0.00202226
Iteration 5/25 | Loss: 0.00195656
Iteration 6/25 | Loss: 0.00185372
Iteration 7/25 | Loss: 0.00174843
Iteration 8/25 | Loss: 0.00169969
Iteration 9/25 | Loss: 0.00173469
Iteration 10/25 | Loss: 0.00171512
Iteration 11/25 | Loss: 0.00171134
Iteration 12/25 | Loss: 0.00163209
Iteration 13/25 | Loss: 0.00158821
Iteration 14/25 | Loss: 0.00156760
Iteration 15/25 | Loss: 0.00156082
Iteration 16/25 | Loss: 0.00155381
Iteration 17/25 | Loss: 0.00154723
Iteration 18/25 | Loss: 0.00155872
Iteration 19/25 | Loss: 0.00153226
Iteration 20/25 | Loss: 0.00152151
Iteration 21/25 | Loss: 0.00151986
Iteration 22/25 | Loss: 0.00153065
Iteration 23/25 | Loss: 0.00151813
Iteration 24/25 | Loss: 0.00150848
Iteration 25/25 | Loss: 0.00149575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39440429
Iteration 2/25 | Loss: 0.00203431
Iteration 3/25 | Loss: 0.00190658
Iteration 4/25 | Loss: 0.00190658
Iteration 5/25 | Loss: 0.00190658
Iteration 6/25 | Loss: 0.00190658
Iteration 7/25 | Loss: 0.00190657
Iteration 8/25 | Loss: 0.00190658
Iteration 9/25 | Loss: 0.00190658
Iteration 10/25 | Loss: 0.00190658
Iteration 11/25 | Loss: 0.00190657
Iteration 12/25 | Loss: 0.00190658
Iteration 13/25 | Loss: 0.00190658
Iteration 14/25 | Loss: 0.00190658
Iteration 15/25 | Loss: 0.00190658
Iteration 16/25 | Loss: 0.00190657
Iteration 17/25 | Loss: 0.00190657
Iteration 18/25 | Loss: 0.00190658
Iteration 19/25 | Loss: 0.00190658
Iteration 20/25 | Loss: 0.00190658
Iteration 21/25 | Loss: 0.00190658
Iteration 22/25 | Loss: 0.00190658
Iteration 23/25 | Loss: 0.00190658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00190657505299896, 0.00190657505299896, 0.00190657505299896, 0.00190657505299896, 0.00190657505299896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00190657505299896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190658
Iteration 2/1000 | Loss: 0.00078722
Iteration 3/1000 | Loss: 0.00125206
Iteration 4/1000 | Loss: 0.00314766
Iteration 5/1000 | Loss: 0.00164152
Iteration 6/1000 | Loss: 0.00158914
Iteration 7/1000 | Loss: 0.00062546
Iteration 8/1000 | Loss: 0.00048052
Iteration 9/1000 | Loss: 0.00025605
Iteration 10/1000 | Loss: 0.00108115
Iteration 11/1000 | Loss: 0.00013568
Iteration 12/1000 | Loss: 0.00041973
Iteration 13/1000 | Loss: 0.00032320
Iteration 14/1000 | Loss: 0.00011931
Iteration 15/1000 | Loss: 0.00010757
Iteration 16/1000 | Loss: 0.00043136
Iteration 17/1000 | Loss: 0.00016463
Iteration 18/1000 | Loss: 0.00116066
Iteration 19/1000 | Loss: 0.00020687
Iteration 20/1000 | Loss: 0.00013666
Iteration 21/1000 | Loss: 0.00090116
Iteration 22/1000 | Loss: 0.00131880
Iteration 23/1000 | Loss: 0.00028105
Iteration 24/1000 | Loss: 0.00036714
Iteration 25/1000 | Loss: 0.00015906
Iteration 26/1000 | Loss: 0.00014515
Iteration 27/1000 | Loss: 0.00010268
Iteration 28/1000 | Loss: 0.00009755
Iteration 29/1000 | Loss: 0.00016236
Iteration 30/1000 | Loss: 0.00010107
Iteration 31/1000 | Loss: 0.00008990
Iteration 32/1000 | Loss: 0.00014182
Iteration 33/1000 | Loss: 0.00008500
Iteration 34/1000 | Loss: 0.00013633
Iteration 35/1000 | Loss: 0.00019989
Iteration 36/1000 | Loss: 0.00012669
Iteration 37/1000 | Loss: 0.00007775
Iteration 38/1000 | Loss: 0.00007607
Iteration 39/1000 | Loss: 0.00007288
Iteration 40/1000 | Loss: 0.00026894
Iteration 41/1000 | Loss: 0.00015826
Iteration 42/1000 | Loss: 0.00008456
Iteration 43/1000 | Loss: 0.00007489
Iteration 44/1000 | Loss: 0.00007046
Iteration 45/1000 | Loss: 0.00006651
Iteration 46/1000 | Loss: 0.00006392
Iteration 47/1000 | Loss: 0.00006235
Iteration 48/1000 | Loss: 0.00006162
Iteration 49/1000 | Loss: 0.00006089
Iteration 50/1000 | Loss: 0.00006023
Iteration 51/1000 | Loss: 0.00005952
Iteration 52/1000 | Loss: 0.00005916
Iteration 53/1000 | Loss: 0.00005881
Iteration 54/1000 | Loss: 0.00005858
Iteration 55/1000 | Loss: 0.00006294
Iteration 56/1000 | Loss: 0.00005887
Iteration 57/1000 | Loss: 0.00005838
Iteration 58/1000 | Loss: 0.00005786
Iteration 59/1000 | Loss: 0.00005759
Iteration 60/1000 | Loss: 0.00005756
Iteration 61/1000 | Loss: 0.00040060
Iteration 62/1000 | Loss: 0.00007734
Iteration 63/1000 | Loss: 0.00006407
Iteration 64/1000 | Loss: 0.00006171
Iteration 65/1000 | Loss: 0.00013682
Iteration 66/1000 | Loss: 0.00007237
Iteration 67/1000 | Loss: 0.00005810
Iteration 68/1000 | Loss: 0.00010733
Iteration 69/1000 | Loss: 0.00005753
Iteration 70/1000 | Loss: 0.00005705
Iteration 71/1000 | Loss: 0.00005678
Iteration 72/1000 | Loss: 0.00005676
Iteration 73/1000 | Loss: 0.00005653
Iteration 74/1000 | Loss: 0.00005636
Iteration 75/1000 | Loss: 0.00007611
Iteration 76/1000 | Loss: 0.00012024
Iteration 77/1000 | Loss: 0.00007399
Iteration 78/1000 | Loss: 0.00005572
Iteration 79/1000 | Loss: 0.00005518
Iteration 80/1000 | Loss: 0.00005474
Iteration 81/1000 | Loss: 0.00005438
Iteration 82/1000 | Loss: 0.00005397
Iteration 83/1000 | Loss: 0.00005368
Iteration 84/1000 | Loss: 0.00010943
Iteration 85/1000 | Loss: 0.00005705
Iteration 86/1000 | Loss: 0.00005334
Iteration 87/1000 | Loss: 0.00005265
Iteration 88/1000 | Loss: 0.00005225
Iteration 89/1000 | Loss: 0.00005181
Iteration 90/1000 | Loss: 0.00005147
Iteration 91/1000 | Loss: 0.00005115
Iteration 92/1000 | Loss: 0.00005109
Iteration 93/1000 | Loss: 0.00005092
Iteration 94/1000 | Loss: 0.00005090
Iteration 95/1000 | Loss: 0.00005087
Iteration 96/1000 | Loss: 0.00005086
Iteration 97/1000 | Loss: 0.00005085
Iteration 98/1000 | Loss: 0.00005085
Iteration 99/1000 | Loss: 0.00005084
Iteration 100/1000 | Loss: 0.00005084
Iteration 101/1000 | Loss: 0.00005084
Iteration 102/1000 | Loss: 0.00005084
Iteration 103/1000 | Loss: 0.00005084
Iteration 104/1000 | Loss: 0.00005084
Iteration 105/1000 | Loss: 0.00005083
Iteration 106/1000 | Loss: 0.00005083
Iteration 107/1000 | Loss: 0.00005083
Iteration 108/1000 | Loss: 0.00005083
Iteration 109/1000 | Loss: 0.00005083
Iteration 110/1000 | Loss: 0.00005083
Iteration 111/1000 | Loss: 0.00005083
Iteration 112/1000 | Loss: 0.00005083
Iteration 113/1000 | Loss: 0.00005082
Iteration 114/1000 | Loss: 0.00005082
Iteration 115/1000 | Loss: 0.00005082
Iteration 116/1000 | Loss: 0.00005082
Iteration 117/1000 | Loss: 0.00005082
Iteration 118/1000 | Loss: 0.00005082
Iteration 119/1000 | Loss: 0.00005082
Iteration 120/1000 | Loss: 0.00005081
Iteration 121/1000 | Loss: 0.00005081
Iteration 122/1000 | Loss: 0.00005081
Iteration 123/1000 | Loss: 0.00005081
Iteration 124/1000 | Loss: 0.00005081
Iteration 125/1000 | Loss: 0.00005081
Iteration 126/1000 | Loss: 0.00005080
Iteration 127/1000 | Loss: 0.00005080
Iteration 128/1000 | Loss: 0.00005080
Iteration 129/1000 | Loss: 0.00005079
Iteration 130/1000 | Loss: 0.00005079
Iteration 131/1000 | Loss: 0.00005079
Iteration 132/1000 | Loss: 0.00005079
Iteration 133/1000 | Loss: 0.00005079
Iteration 134/1000 | Loss: 0.00005079
Iteration 135/1000 | Loss: 0.00005079
Iteration 136/1000 | Loss: 0.00005079
Iteration 137/1000 | Loss: 0.00005078
Iteration 138/1000 | Loss: 0.00005078
Iteration 139/1000 | Loss: 0.00005078
Iteration 140/1000 | Loss: 0.00005078
Iteration 141/1000 | Loss: 0.00005078
Iteration 142/1000 | Loss: 0.00005078
Iteration 143/1000 | Loss: 0.00005078
Iteration 144/1000 | Loss: 0.00005078
Iteration 145/1000 | Loss: 0.00005078
Iteration 146/1000 | Loss: 0.00005078
Iteration 147/1000 | Loss: 0.00005078
Iteration 148/1000 | Loss: 0.00005078
Iteration 149/1000 | Loss: 0.00005078
Iteration 150/1000 | Loss: 0.00005078
Iteration 151/1000 | Loss: 0.00005078
Iteration 152/1000 | Loss: 0.00005078
Iteration 153/1000 | Loss: 0.00005077
Iteration 154/1000 | Loss: 0.00005077
Iteration 155/1000 | Loss: 0.00005077
Iteration 156/1000 | Loss: 0.00005077
Iteration 157/1000 | Loss: 0.00005077
Iteration 158/1000 | Loss: 0.00005077
Iteration 159/1000 | Loss: 0.00005077
Iteration 160/1000 | Loss: 0.00005077
Iteration 161/1000 | Loss: 0.00005077
Iteration 162/1000 | Loss: 0.00005076
Iteration 163/1000 | Loss: 0.00005076
Iteration 164/1000 | Loss: 0.00005076
Iteration 165/1000 | Loss: 0.00005076
Iteration 166/1000 | Loss: 0.00005076
Iteration 167/1000 | Loss: 0.00005076
Iteration 168/1000 | Loss: 0.00005076
Iteration 169/1000 | Loss: 0.00005076
Iteration 170/1000 | Loss: 0.00005076
Iteration 171/1000 | Loss: 0.00005076
Iteration 172/1000 | Loss: 0.00005076
Iteration 173/1000 | Loss: 0.00005076
Iteration 174/1000 | Loss: 0.00005076
Iteration 175/1000 | Loss: 0.00005076
Iteration 176/1000 | Loss: 0.00005076
Iteration 177/1000 | Loss: 0.00005076
Iteration 178/1000 | Loss: 0.00005076
Iteration 179/1000 | Loss: 0.00005076
Iteration 180/1000 | Loss: 0.00005076
Iteration 181/1000 | Loss: 0.00005076
Iteration 182/1000 | Loss: 0.00005076
Iteration 183/1000 | Loss: 0.00005076
Iteration 184/1000 | Loss: 0.00005076
Iteration 185/1000 | Loss: 0.00005076
Iteration 186/1000 | Loss: 0.00005076
Iteration 187/1000 | Loss: 0.00005076
Iteration 188/1000 | Loss: 0.00005076
Iteration 189/1000 | Loss: 0.00005076
Iteration 190/1000 | Loss: 0.00005076
Iteration 191/1000 | Loss: 0.00005076
Iteration 192/1000 | Loss: 0.00005076
Iteration 193/1000 | Loss: 0.00005076
Iteration 194/1000 | Loss: 0.00005076
Iteration 195/1000 | Loss: 0.00005076
Iteration 196/1000 | Loss: 0.00005076
Iteration 197/1000 | Loss: 0.00005076
Iteration 198/1000 | Loss: 0.00005076
Iteration 199/1000 | Loss: 0.00005076
Iteration 200/1000 | Loss: 0.00005076
Iteration 201/1000 | Loss: 0.00005076
Iteration 202/1000 | Loss: 0.00005076
Iteration 203/1000 | Loss: 0.00005076
Iteration 204/1000 | Loss: 0.00005076
Iteration 205/1000 | Loss: 0.00005076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [5.0760074373101816e-05, 5.0760074373101816e-05, 5.0760074373101816e-05, 5.0760074373101816e-05, 5.0760074373101816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.0760074373101816e-05

Optimization complete. Final v2v error: 4.363819122314453 mm

Highest mean error: 10.803321838378906 mm for frame 127

Lowest mean error: 3.412304162979126 mm for frame 6

Saving results

Total time: 179.27564930915833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958455
Iteration 2/25 | Loss: 0.00160494
Iteration 3/25 | Loss: 0.00139847
Iteration 4/25 | Loss: 0.00137852
Iteration 5/25 | Loss: 0.00137652
Iteration 6/25 | Loss: 0.00137652
Iteration 7/25 | Loss: 0.00137652
Iteration 8/25 | Loss: 0.00137652
Iteration 9/25 | Loss: 0.00137652
Iteration 10/25 | Loss: 0.00137652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013765174662694335, 0.0013765174662694335, 0.0013765174662694335, 0.0013765174662694335, 0.0013765174662694335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013765174662694335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39653420
Iteration 2/25 | Loss: 0.00079859
Iteration 3/25 | Loss: 0.00079859
Iteration 4/25 | Loss: 0.00079859
Iteration 5/25 | Loss: 0.00079859
Iteration 6/25 | Loss: 0.00079859
Iteration 7/25 | Loss: 0.00079859
Iteration 8/25 | Loss: 0.00079859
Iteration 9/25 | Loss: 0.00079859
Iteration 10/25 | Loss: 0.00079859
Iteration 11/25 | Loss: 0.00079859
Iteration 12/25 | Loss: 0.00079859
Iteration 13/25 | Loss: 0.00079859
Iteration 14/25 | Loss: 0.00079859
Iteration 15/25 | Loss: 0.00079859
Iteration 16/25 | Loss: 0.00079859
Iteration 17/25 | Loss: 0.00079859
Iteration 18/25 | Loss: 0.00079859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007985907723195851, 0.0007985907723195851, 0.0007985907723195851, 0.0007985907723195851, 0.0007985907723195851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007985907723195851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079859
Iteration 2/1000 | Loss: 0.00003409
Iteration 3/1000 | Loss: 0.00002590
Iteration 4/1000 | Loss: 0.00002359
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00002238
Iteration 7/1000 | Loss: 0.00002194
Iteration 8/1000 | Loss: 0.00002164
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002136
Iteration 11/1000 | Loss: 0.00002105
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002055
Iteration 14/1000 | Loss: 0.00002041
Iteration 15/1000 | Loss: 0.00002041
Iteration 16/1000 | Loss: 0.00002041
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002039
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002037
Iteration 21/1000 | Loss: 0.00002036
Iteration 22/1000 | Loss: 0.00002034
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002018
Iteration 27/1000 | Loss: 0.00002018
Iteration 28/1000 | Loss: 0.00002018
Iteration 29/1000 | Loss: 0.00002018
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002018
Iteration 32/1000 | Loss: 0.00002018
Iteration 33/1000 | Loss: 0.00002016
Iteration 34/1000 | Loss: 0.00002015
Iteration 35/1000 | Loss: 0.00002015
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002014
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00002013
Iteration 40/1000 | Loss: 0.00002013
Iteration 41/1000 | Loss: 0.00002012
Iteration 42/1000 | Loss: 0.00002012
Iteration 43/1000 | Loss: 0.00002012
Iteration 44/1000 | Loss: 0.00002011
Iteration 45/1000 | Loss: 0.00002011
Iteration 46/1000 | Loss: 0.00002011
Iteration 47/1000 | Loss: 0.00002010
Iteration 48/1000 | Loss: 0.00002010
Iteration 49/1000 | Loss: 0.00002009
Iteration 50/1000 | Loss: 0.00002009
Iteration 51/1000 | Loss: 0.00002009
Iteration 52/1000 | Loss: 0.00002009
Iteration 53/1000 | Loss: 0.00002009
Iteration 54/1000 | Loss: 0.00002009
Iteration 55/1000 | Loss: 0.00002009
Iteration 56/1000 | Loss: 0.00002009
Iteration 57/1000 | Loss: 0.00002009
Iteration 58/1000 | Loss: 0.00002008
Iteration 59/1000 | Loss: 0.00002008
Iteration 60/1000 | Loss: 0.00002008
Iteration 61/1000 | Loss: 0.00002008
Iteration 62/1000 | Loss: 0.00002007
Iteration 63/1000 | Loss: 0.00002007
Iteration 64/1000 | Loss: 0.00002007
Iteration 65/1000 | Loss: 0.00002007
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002007
Iteration 71/1000 | Loss: 0.00002007
Iteration 72/1000 | Loss: 0.00002007
Iteration 73/1000 | Loss: 0.00002007
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002006
Iteration 76/1000 | Loss: 0.00002006
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002006
Iteration 79/1000 | Loss: 0.00002005
Iteration 80/1000 | Loss: 0.00002005
Iteration 81/1000 | Loss: 0.00002005
Iteration 82/1000 | Loss: 0.00002005
Iteration 83/1000 | Loss: 0.00002005
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002004
Iteration 87/1000 | Loss: 0.00002004
Iteration 88/1000 | Loss: 0.00002004
Iteration 89/1000 | Loss: 0.00002004
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002003
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002002
Iteration 102/1000 | Loss: 0.00002002
Iteration 103/1000 | Loss: 0.00002002
Iteration 104/1000 | Loss: 0.00002002
Iteration 105/1000 | Loss: 0.00002002
Iteration 106/1000 | Loss: 0.00002002
Iteration 107/1000 | Loss: 0.00002002
Iteration 108/1000 | Loss: 0.00002001
Iteration 109/1000 | Loss: 0.00002001
Iteration 110/1000 | Loss: 0.00002001
Iteration 111/1000 | Loss: 0.00002000
Iteration 112/1000 | Loss: 0.00002000
Iteration 113/1000 | Loss: 0.00002000
Iteration 114/1000 | Loss: 0.00002000
Iteration 115/1000 | Loss: 0.00001999
Iteration 116/1000 | Loss: 0.00001999
Iteration 117/1000 | Loss: 0.00001999
Iteration 118/1000 | Loss: 0.00001999
Iteration 119/1000 | Loss: 0.00001999
Iteration 120/1000 | Loss: 0.00001999
Iteration 121/1000 | Loss: 0.00001999
Iteration 122/1000 | Loss: 0.00001999
Iteration 123/1000 | Loss: 0.00001998
Iteration 124/1000 | Loss: 0.00001998
Iteration 125/1000 | Loss: 0.00001998
Iteration 126/1000 | Loss: 0.00001998
Iteration 127/1000 | Loss: 0.00001998
Iteration 128/1000 | Loss: 0.00001998
Iteration 129/1000 | Loss: 0.00001998
Iteration 130/1000 | Loss: 0.00001997
Iteration 131/1000 | Loss: 0.00001997
Iteration 132/1000 | Loss: 0.00001997
Iteration 133/1000 | Loss: 0.00001997
Iteration 134/1000 | Loss: 0.00001997
Iteration 135/1000 | Loss: 0.00001997
Iteration 136/1000 | Loss: 0.00001996
Iteration 137/1000 | Loss: 0.00001996
Iteration 138/1000 | Loss: 0.00001996
Iteration 139/1000 | Loss: 0.00001996
Iteration 140/1000 | Loss: 0.00001996
Iteration 141/1000 | Loss: 0.00001996
Iteration 142/1000 | Loss: 0.00001996
Iteration 143/1000 | Loss: 0.00001995
Iteration 144/1000 | Loss: 0.00001995
Iteration 145/1000 | Loss: 0.00001995
Iteration 146/1000 | Loss: 0.00001995
Iteration 147/1000 | Loss: 0.00001995
Iteration 148/1000 | Loss: 0.00001995
Iteration 149/1000 | Loss: 0.00001994
Iteration 150/1000 | Loss: 0.00001994
Iteration 151/1000 | Loss: 0.00001994
Iteration 152/1000 | Loss: 0.00001994
Iteration 153/1000 | Loss: 0.00001994
Iteration 154/1000 | Loss: 0.00001994
Iteration 155/1000 | Loss: 0.00001994
Iteration 156/1000 | Loss: 0.00001994
Iteration 157/1000 | Loss: 0.00001994
Iteration 158/1000 | Loss: 0.00001994
Iteration 159/1000 | Loss: 0.00001994
Iteration 160/1000 | Loss: 0.00001994
Iteration 161/1000 | Loss: 0.00001994
Iteration 162/1000 | Loss: 0.00001994
Iteration 163/1000 | Loss: 0.00001994
Iteration 164/1000 | Loss: 0.00001994
Iteration 165/1000 | Loss: 0.00001994
Iteration 166/1000 | Loss: 0.00001994
Iteration 167/1000 | Loss: 0.00001994
Iteration 168/1000 | Loss: 0.00001994
Iteration 169/1000 | Loss: 0.00001994
Iteration 170/1000 | Loss: 0.00001994
Iteration 171/1000 | Loss: 0.00001994
Iteration 172/1000 | Loss: 0.00001994
Iteration 173/1000 | Loss: 0.00001994
Iteration 174/1000 | Loss: 0.00001994
Iteration 175/1000 | Loss: 0.00001994
Iteration 176/1000 | Loss: 0.00001994
Iteration 177/1000 | Loss: 0.00001994
Iteration 178/1000 | Loss: 0.00001994
Iteration 179/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.9936682292609476e-05, 1.9936682292609476e-05, 1.9936682292609476e-05, 1.9936682292609476e-05, 1.9936682292609476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9936682292609476e-05

Optimization complete. Final v2v error: 3.7752456665039062 mm

Highest mean error: 3.923565149307251 mm for frame 113

Lowest mean error: 3.048492670059204 mm for frame 1

Saving results

Total time: 36.69764018058777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780116
Iteration 2/25 | Loss: 0.00152554
Iteration 3/25 | Loss: 0.00142848
Iteration 4/25 | Loss: 0.00142231
Iteration 5/25 | Loss: 0.00142195
Iteration 6/25 | Loss: 0.00142195
Iteration 7/25 | Loss: 0.00142195
Iteration 8/25 | Loss: 0.00142195
Iteration 9/25 | Loss: 0.00142195
Iteration 10/25 | Loss: 0.00142195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014219532022252679, 0.0014219532022252679, 0.0014219532022252679, 0.0014219532022252679, 0.0014219532022252679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014219532022252679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98226988
Iteration 2/25 | Loss: 0.00091454
Iteration 3/25 | Loss: 0.00091452
Iteration 4/25 | Loss: 0.00091451
Iteration 5/25 | Loss: 0.00091451
Iteration 6/25 | Loss: 0.00091451
Iteration 7/25 | Loss: 0.00091451
Iteration 8/25 | Loss: 0.00091451
Iteration 9/25 | Loss: 0.00091451
Iteration 10/25 | Loss: 0.00091451
Iteration 11/25 | Loss: 0.00091451
Iteration 12/25 | Loss: 0.00091451
Iteration 13/25 | Loss: 0.00091451
Iteration 14/25 | Loss: 0.00091451
Iteration 15/25 | Loss: 0.00091451
Iteration 16/25 | Loss: 0.00091451
Iteration 17/25 | Loss: 0.00091451
Iteration 18/25 | Loss: 0.00091451
Iteration 19/25 | Loss: 0.00091451
Iteration 20/25 | Loss: 0.00091451
Iteration 21/25 | Loss: 0.00091451
Iteration 22/25 | Loss: 0.00091451
Iteration 23/25 | Loss: 0.00091451
Iteration 24/25 | Loss: 0.00091451
Iteration 25/25 | Loss: 0.00091451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091451
Iteration 2/1000 | Loss: 0.00003526
Iteration 3/1000 | Loss: 0.00002563
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002289
Iteration 6/1000 | Loss: 0.00002247
Iteration 7/1000 | Loss: 0.00002222
Iteration 8/1000 | Loss: 0.00002188
Iteration 9/1000 | Loss: 0.00002160
Iteration 10/1000 | Loss: 0.00002127
Iteration 11/1000 | Loss: 0.00002103
Iteration 12/1000 | Loss: 0.00002096
Iteration 13/1000 | Loss: 0.00002075
Iteration 14/1000 | Loss: 0.00002072
Iteration 15/1000 | Loss: 0.00002067
Iteration 16/1000 | Loss: 0.00002066
Iteration 17/1000 | Loss: 0.00002066
Iteration 18/1000 | Loss: 0.00002064
Iteration 19/1000 | Loss: 0.00002057
Iteration 20/1000 | Loss: 0.00002052
Iteration 21/1000 | Loss: 0.00002040
Iteration 22/1000 | Loss: 0.00002033
Iteration 23/1000 | Loss: 0.00002033
Iteration 24/1000 | Loss: 0.00002033
Iteration 25/1000 | Loss: 0.00002032
Iteration 26/1000 | Loss: 0.00002032
Iteration 27/1000 | Loss: 0.00002032
Iteration 28/1000 | Loss: 0.00002032
Iteration 29/1000 | Loss: 0.00002029
Iteration 30/1000 | Loss: 0.00002029
Iteration 31/1000 | Loss: 0.00002027
Iteration 32/1000 | Loss: 0.00002019
Iteration 33/1000 | Loss: 0.00002019
Iteration 34/1000 | Loss: 0.00002016
Iteration 35/1000 | Loss: 0.00002015
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002014
Iteration 38/1000 | Loss: 0.00002013
Iteration 39/1000 | Loss: 0.00002011
Iteration 40/1000 | Loss: 0.00002010
Iteration 41/1000 | Loss: 0.00002010
Iteration 42/1000 | Loss: 0.00002010
Iteration 43/1000 | Loss: 0.00002009
Iteration 44/1000 | Loss: 0.00002009
Iteration 45/1000 | Loss: 0.00002006
Iteration 46/1000 | Loss: 0.00002006
Iteration 47/1000 | Loss: 0.00002006
Iteration 48/1000 | Loss: 0.00002006
Iteration 49/1000 | Loss: 0.00002006
Iteration 50/1000 | Loss: 0.00002006
Iteration 51/1000 | Loss: 0.00002006
Iteration 52/1000 | Loss: 0.00002006
Iteration 53/1000 | Loss: 0.00002006
Iteration 54/1000 | Loss: 0.00002006
Iteration 55/1000 | Loss: 0.00002006
Iteration 56/1000 | Loss: 0.00002005
Iteration 57/1000 | Loss: 0.00002005
Iteration 58/1000 | Loss: 0.00002005
Iteration 59/1000 | Loss: 0.00002005
Iteration 60/1000 | Loss: 0.00002004
Iteration 61/1000 | Loss: 0.00002003
Iteration 62/1000 | Loss: 0.00002003
Iteration 63/1000 | Loss: 0.00002003
Iteration 64/1000 | Loss: 0.00002003
Iteration 65/1000 | Loss: 0.00002003
Iteration 66/1000 | Loss: 0.00002002
Iteration 67/1000 | Loss: 0.00002002
Iteration 68/1000 | Loss: 0.00002002
Iteration 69/1000 | Loss: 0.00002002
Iteration 70/1000 | Loss: 0.00002002
Iteration 71/1000 | Loss: 0.00002002
Iteration 72/1000 | Loss: 0.00002001
Iteration 73/1000 | Loss: 0.00002001
Iteration 74/1000 | Loss: 0.00002001
Iteration 75/1000 | Loss: 0.00002001
Iteration 76/1000 | Loss: 0.00002001
Iteration 77/1000 | Loss: 0.00002000
Iteration 78/1000 | Loss: 0.00002000
Iteration 79/1000 | Loss: 0.00002000
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001999
Iteration 83/1000 | Loss: 0.00001999
Iteration 84/1000 | Loss: 0.00001999
Iteration 85/1000 | Loss: 0.00001999
Iteration 86/1000 | Loss: 0.00001999
Iteration 87/1000 | Loss: 0.00001999
Iteration 88/1000 | Loss: 0.00001999
Iteration 89/1000 | Loss: 0.00001999
Iteration 90/1000 | Loss: 0.00001999
Iteration 91/1000 | Loss: 0.00001999
Iteration 92/1000 | Loss: 0.00001998
Iteration 93/1000 | Loss: 0.00001998
Iteration 94/1000 | Loss: 0.00001998
Iteration 95/1000 | Loss: 0.00001998
Iteration 96/1000 | Loss: 0.00001998
Iteration 97/1000 | Loss: 0.00001998
Iteration 98/1000 | Loss: 0.00001998
Iteration 99/1000 | Loss: 0.00001998
Iteration 100/1000 | Loss: 0.00001998
Iteration 101/1000 | Loss: 0.00001998
Iteration 102/1000 | Loss: 0.00001998
Iteration 103/1000 | Loss: 0.00001998
Iteration 104/1000 | Loss: 0.00001998
Iteration 105/1000 | Loss: 0.00001998
Iteration 106/1000 | Loss: 0.00001998
Iteration 107/1000 | Loss: 0.00001998
Iteration 108/1000 | Loss: 0.00001997
Iteration 109/1000 | Loss: 0.00001997
Iteration 110/1000 | Loss: 0.00001997
Iteration 111/1000 | Loss: 0.00001997
Iteration 112/1000 | Loss: 0.00001997
Iteration 113/1000 | Loss: 0.00001997
Iteration 114/1000 | Loss: 0.00001997
Iteration 115/1000 | Loss: 0.00001997
Iteration 116/1000 | Loss: 0.00001997
Iteration 117/1000 | Loss: 0.00001997
Iteration 118/1000 | Loss: 0.00001997
Iteration 119/1000 | Loss: 0.00001997
Iteration 120/1000 | Loss: 0.00001997
Iteration 121/1000 | Loss: 0.00001996
Iteration 122/1000 | Loss: 0.00001996
Iteration 123/1000 | Loss: 0.00001996
Iteration 124/1000 | Loss: 0.00001996
Iteration 125/1000 | Loss: 0.00001996
Iteration 126/1000 | Loss: 0.00001996
Iteration 127/1000 | Loss: 0.00001996
Iteration 128/1000 | Loss: 0.00001995
Iteration 129/1000 | Loss: 0.00001995
Iteration 130/1000 | Loss: 0.00001995
Iteration 131/1000 | Loss: 0.00001995
Iteration 132/1000 | Loss: 0.00001995
Iteration 133/1000 | Loss: 0.00001995
Iteration 134/1000 | Loss: 0.00001995
Iteration 135/1000 | Loss: 0.00001995
Iteration 136/1000 | Loss: 0.00001994
Iteration 137/1000 | Loss: 0.00001994
Iteration 138/1000 | Loss: 0.00001994
Iteration 139/1000 | Loss: 0.00001994
Iteration 140/1000 | Loss: 0.00001994
Iteration 141/1000 | Loss: 0.00001994
Iteration 142/1000 | Loss: 0.00001994
Iteration 143/1000 | Loss: 0.00001994
Iteration 144/1000 | Loss: 0.00001994
Iteration 145/1000 | Loss: 0.00001994
Iteration 146/1000 | Loss: 0.00001994
Iteration 147/1000 | Loss: 0.00001994
Iteration 148/1000 | Loss: 0.00001994
Iteration 149/1000 | Loss: 0.00001994
Iteration 150/1000 | Loss: 0.00001994
Iteration 151/1000 | Loss: 0.00001994
Iteration 152/1000 | Loss: 0.00001994
Iteration 153/1000 | Loss: 0.00001994
Iteration 154/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.9938672267016955e-05, 1.9938672267016955e-05, 1.9938672267016955e-05, 1.9938672267016955e-05, 1.9938672267016955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9938672267016955e-05

Optimization complete. Final v2v error: 3.683452844619751 mm

Highest mean error: 4.117593765258789 mm for frame 172

Lowest mean error: 3.4092559814453125 mm for frame 58

Saving results

Total time: 44.160383224487305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00282790
Iteration 2/25 | Loss: 0.00148380
Iteration 3/25 | Loss: 0.00127408
Iteration 4/25 | Loss: 0.00124931
Iteration 5/25 | Loss: 0.00124304
Iteration 6/25 | Loss: 0.00124009
Iteration 7/25 | Loss: 0.00123928
Iteration 8/25 | Loss: 0.00123916
Iteration 9/25 | Loss: 0.00123916
Iteration 10/25 | Loss: 0.00123916
Iteration 11/25 | Loss: 0.00123916
Iteration 12/25 | Loss: 0.00123916
Iteration 13/25 | Loss: 0.00123916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012391594937071204, 0.0012391594937071204, 0.0012391594937071204, 0.0012391594937071204, 0.0012391594937071204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012391594937071204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38877857
Iteration 2/25 | Loss: 0.00090841
Iteration 3/25 | Loss: 0.00090841
Iteration 4/25 | Loss: 0.00090841
Iteration 5/25 | Loss: 0.00090841
Iteration 6/25 | Loss: 0.00090841
Iteration 7/25 | Loss: 0.00090841
Iteration 8/25 | Loss: 0.00090841
Iteration 9/25 | Loss: 0.00090841
Iteration 10/25 | Loss: 0.00090841
Iteration 11/25 | Loss: 0.00090841
Iteration 12/25 | Loss: 0.00090841
Iteration 13/25 | Loss: 0.00090841
Iteration 14/25 | Loss: 0.00090841
Iteration 15/25 | Loss: 0.00090841
Iteration 16/25 | Loss: 0.00090841
Iteration 17/25 | Loss: 0.00090841
Iteration 18/25 | Loss: 0.00090841
Iteration 19/25 | Loss: 0.00090841
Iteration 20/25 | Loss: 0.00090841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000908411166165024, 0.000908411166165024, 0.000908411166165024, 0.000908411166165024, 0.000908411166165024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000908411166165024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090841
Iteration 2/1000 | Loss: 0.00004110
Iteration 3/1000 | Loss: 0.00002242
Iteration 4/1000 | Loss: 0.00001815
Iteration 5/1000 | Loss: 0.00001657
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001413
Iteration 9/1000 | Loss: 0.00001378
Iteration 10/1000 | Loss: 0.00001339
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001258
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001257
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001252
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001250
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001245
Iteration 69/1000 | Loss: 0.00001245
Iteration 70/1000 | Loss: 0.00001245
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001245
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001245
Iteration 77/1000 | Loss: 0.00001245
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001245
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.2454827810870484e-05, 1.2454827810870484e-05, 1.2454827810870484e-05, 1.2454827810870484e-05, 1.2454827810870484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2454827810870484e-05

Optimization complete. Final v2v error: 3.06758975982666 mm

Highest mean error: 3.2251274585723877 mm for frame 225

Lowest mean error: 2.9166393280029297 mm for frame 140

Saving results

Total time: 37.43754172325134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009515
Iteration 2/25 | Loss: 0.00242619
Iteration 3/25 | Loss: 0.00186410
Iteration 4/25 | Loss: 0.00167594
Iteration 5/25 | Loss: 0.00161891
Iteration 6/25 | Loss: 0.00151243
Iteration 7/25 | Loss: 0.00148166
Iteration 8/25 | Loss: 0.00146405
Iteration 9/25 | Loss: 0.00143221
Iteration 10/25 | Loss: 0.00142791
Iteration 11/25 | Loss: 0.00142241
Iteration 12/25 | Loss: 0.00142418
Iteration 13/25 | Loss: 0.00142075
Iteration 14/25 | Loss: 0.00142069
Iteration 15/25 | Loss: 0.00142018
Iteration 16/25 | Loss: 0.00142015
Iteration 17/25 | Loss: 0.00142169
Iteration 18/25 | Loss: 0.00141868
Iteration 19/25 | Loss: 0.00141707
Iteration 20/25 | Loss: 0.00141807
Iteration 21/25 | Loss: 0.00141780
Iteration 22/25 | Loss: 0.00141764
Iteration 23/25 | Loss: 0.00141750
Iteration 24/25 | Loss: 0.00141775
Iteration 25/25 | Loss: 0.00141752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37587321
Iteration 2/25 | Loss: 0.00092258
Iteration 3/25 | Loss: 0.00091583
Iteration 4/25 | Loss: 0.00091583
Iteration 5/25 | Loss: 0.00091583
Iteration 6/25 | Loss: 0.00091583
Iteration 7/25 | Loss: 0.00091583
Iteration 8/25 | Loss: 0.00091583
Iteration 9/25 | Loss: 0.00091583
Iteration 10/25 | Loss: 0.00091583
Iteration 11/25 | Loss: 0.00091583
Iteration 12/25 | Loss: 0.00091583
Iteration 13/25 | Loss: 0.00091583
Iteration 14/25 | Loss: 0.00091583
Iteration 15/25 | Loss: 0.00091583
Iteration 16/25 | Loss: 0.00091583
Iteration 17/25 | Loss: 0.00091583
Iteration 18/25 | Loss: 0.00091583
Iteration 19/25 | Loss: 0.00091583
Iteration 20/25 | Loss: 0.00091583
Iteration 21/25 | Loss: 0.00091583
Iteration 22/25 | Loss: 0.00091583
Iteration 23/25 | Loss: 0.00091583
Iteration 24/25 | Loss: 0.00091583
Iteration 25/25 | Loss: 0.00091583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091583
Iteration 2/1000 | Loss: 0.00017231
Iteration 3/1000 | Loss: 0.00006767
Iteration 4/1000 | Loss: 0.00006149
Iteration 5/1000 | Loss: 0.00004716
Iteration 6/1000 | Loss: 0.00004723
Iteration 7/1000 | Loss: 0.00004785
Iteration 8/1000 | Loss: 0.00004937
Iteration 9/1000 | Loss: 0.00004125
Iteration 10/1000 | Loss: 0.00003896
Iteration 11/1000 | Loss: 0.00004217
Iteration 12/1000 | Loss: 0.00004444
Iteration 13/1000 | Loss: 0.00004564
Iteration 14/1000 | Loss: 0.00004990
Iteration 15/1000 | Loss: 0.00021554
Iteration 16/1000 | Loss: 0.00025869
Iteration 17/1000 | Loss: 0.00005738
Iteration 18/1000 | Loss: 0.00004002
Iteration 19/1000 | Loss: 0.00003415
Iteration 20/1000 | Loss: 0.00003664
Iteration 21/1000 | Loss: 0.00002904
Iteration 22/1000 | Loss: 0.00002962
Iteration 23/1000 | Loss: 0.00002705
Iteration 24/1000 | Loss: 0.00002642
Iteration 25/1000 | Loss: 0.00002574
Iteration 26/1000 | Loss: 0.00002516
Iteration 27/1000 | Loss: 0.00002485
Iteration 28/1000 | Loss: 0.00002456
Iteration 29/1000 | Loss: 0.00002428
Iteration 30/1000 | Loss: 0.00002414
Iteration 31/1000 | Loss: 0.00002413
Iteration 32/1000 | Loss: 0.00002409
Iteration 33/1000 | Loss: 0.00002408
Iteration 34/1000 | Loss: 0.00002408
Iteration 35/1000 | Loss: 0.00002407
Iteration 36/1000 | Loss: 0.00002406
Iteration 37/1000 | Loss: 0.00002406
Iteration 38/1000 | Loss: 0.00002405
Iteration 39/1000 | Loss: 0.00002405
Iteration 40/1000 | Loss: 0.00002403
Iteration 41/1000 | Loss: 0.00002403
Iteration 42/1000 | Loss: 0.00002402
Iteration 43/1000 | Loss: 0.00002400
Iteration 44/1000 | Loss: 0.00002399
Iteration 45/1000 | Loss: 0.00002399
Iteration 46/1000 | Loss: 0.00002399
Iteration 47/1000 | Loss: 0.00002399
Iteration 48/1000 | Loss: 0.00002398
Iteration 49/1000 | Loss: 0.00002398
Iteration 50/1000 | Loss: 0.00002398
Iteration 51/1000 | Loss: 0.00002398
Iteration 52/1000 | Loss: 0.00002398
Iteration 53/1000 | Loss: 0.00002398
Iteration 54/1000 | Loss: 0.00002397
Iteration 55/1000 | Loss: 0.00002396
Iteration 56/1000 | Loss: 0.00002395
Iteration 57/1000 | Loss: 0.00002395
Iteration 58/1000 | Loss: 0.00002395
Iteration 59/1000 | Loss: 0.00002394
Iteration 60/1000 | Loss: 0.00002394
Iteration 61/1000 | Loss: 0.00002394
Iteration 62/1000 | Loss: 0.00002394
Iteration 63/1000 | Loss: 0.00002394
Iteration 64/1000 | Loss: 0.00002393
Iteration 65/1000 | Loss: 0.00002393
Iteration 66/1000 | Loss: 0.00002392
Iteration 67/1000 | Loss: 0.00002392
Iteration 68/1000 | Loss: 0.00002392
Iteration 69/1000 | Loss: 0.00002392
Iteration 70/1000 | Loss: 0.00002391
Iteration 71/1000 | Loss: 0.00002391
Iteration 72/1000 | Loss: 0.00002391
Iteration 73/1000 | Loss: 0.00002391
Iteration 74/1000 | Loss: 0.00002390
Iteration 75/1000 | Loss: 0.00002390
Iteration 76/1000 | Loss: 0.00002390
Iteration 77/1000 | Loss: 0.00002390
Iteration 78/1000 | Loss: 0.00002390
Iteration 79/1000 | Loss: 0.00002390
Iteration 80/1000 | Loss: 0.00002390
Iteration 81/1000 | Loss: 0.00002390
Iteration 82/1000 | Loss: 0.00002390
Iteration 83/1000 | Loss: 0.00002390
Iteration 84/1000 | Loss: 0.00002389
Iteration 85/1000 | Loss: 0.00002389
Iteration 86/1000 | Loss: 0.00002389
Iteration 87/1000 | Loss: 0.00002389
Iteration 88/1000 | Loss: 0.00002389
Iteration 89/1000 | Loss: 0.00002389
Iteration 90/1000 | Loss: 0.00002389
Iteration 91/1000 | Loss: 0.00002389
Iteration 92/1000 | Loss: 0.00002389
Iteration 93/1000 | Loss: 0.00002389
Iteration 94/1000 | Loss: 0.00002389
Iteration 95/1000 | Loss: 0.00002389
Iteration 96/1000 | Loss: 0.00002389
Iteration 97/1000 | Loss: 0.00002389
Iteration 98/1000 | Loss: 0.00002388
Iteration 99/1000 | Loss: 0.00002388
Iteration 100/1000 | Loss: 0.00002388
Iteration 101/1000 | Loss: 0.00002388
Iteration 102/1000 | Loss: 0.00002388
Iteration 103/1000 | Loss: 0.00002388
Iteration 104/1000 | Loss: 0.00002387
Iteration 105/1000 | Loss: 0.00002387
Iteration 106/1000 | Loss: 0.00002387
Iteration 107/1000 | Loss: 0.00002387
Iteration 108/1000 | Loss: 0.00002387
Iteration 109/1000 | Loss: 0.00002387
Iteration 110/1000 | Loss: 0.00002387
Iteration 111/1000 | Loss: 0.00002387
Iteration 112/1000 | Loss: 0.00002387
Iteration 113/1000 | Loss: 0.00002387
Iteration 114/1000 | Loss: 0.00002387
Iteration 115/1000 | Loss: 0.00002387
Iteration 116/1000 | Loss: 0.00002387
Iteration 117/1000 | Loss: 0.00002387
Iteration 118/1000 | Loss: 0.00002387
Iteration 119/1000 | Loss: 0.00002387
Iteration 120/1000 | Loss: 0.00002387
Iteration 121/1000 | Loss: 0.00002387
Iteration 122/1000 | Loss: 0.00002386
Iteration 123/1000 | Loss: 0.00002386
Iteration 124/1000 | Loss: 0.00002386
Iteration 125/1000 | Loss: 0.00002386
Iteration 126/1000 | Loss: 0.00002386
Iteration 127/1000 | Loss: 0.00002386
Iteration 128/1000 | Loss: 0.00002386
Iteration 129/1000 | Loss: 0.00002386
Iteration 130/1000 | Loss: 0.00002386
Iteration 131/1000 | Loss: 0.00002386
Iteration 132/1000 | Loss: 0.00002386
Iteration 133/1000 | Loss: 0.00002385
Iteration 134/1000 | Loss: 0.00002385
Iteration 135/1000 | Loss: 0.00002385
Iteration 136/1000 | Loss: 0.00002385
Iteration 137/1000 | Loss: 0.00002385
Iteration 138/1000 | Loss: 0.00002385
Iteration 139/1000 | Loss: 0.00002385
Iteration 140/1000 | Loss: 0.00002385
Iteration 141/1000 | Loss: 0.00002385
Iteration 142/1000 | Loss: 0.00002385
Iteration 143/1000 | Loss: 0.00002385
Iteration 144/1000 | Loss: 0.00002385
Iteration 145/1000 | Loss: 0.00002385
Iteration 146/1000 | Loss: 0.00002385
Iteration 147/1000 | Loss: 0.00002385
Iteration 148/1000 | Loss: 0.00002385
Iteration 149/1000 | Loss: 0.00002384
Iteration 150/1000 | Loss: 0.00002384
Iteration 151/1000 | Loss: 0.00002384
Iteration 152/1000 | Loss: 0.00002384
Iteration 153/1000 | Loss: 0.00002384
Iteration 154/1000 | Loss: 0.00002384
Iteration 155/1000 | Loss: 0.00002384
Iteration 156/1000 | Loss: 0.00002384
Iteration 157/1000 | Loss: 0.00002384
Iteration 158/1000 | Loss: 0.00002384
Iteration 159/1000 | Loss: 0.00002384
Iteration 160/1000 | Loss: 0.00002384
Iteration 161/1000 | Loss: 0.00002384
Iteration 162/1000 | Loss: 0.00002384
Iteration 163/1000 | Loss: 0.00002384
Iteration 164/1000 | Loss: 0.00002384
Iteration 165/1000 | Loss: 0.00002384
Iteration 166/1000 | Loss: 0.00002384
Iteration 167/1000 | Loss: 0.00002384
Iteration 168/1000 | Loss: 0.00002384
Iteration 169/1000 | Loss: 0.00002384
Iteration 170/1000 | Loss: 0.00002384
Iteration 171/1000 | Loss: 0.00002384
Iteration 172/1000 | Loss: 0.00002384
Iteration 173/1000 | Loss: 0.00002384
Iteration 174/1000 | Loss: 0.00002384
Iteration 175/1000 | Loss: 0.00002384
Iteration 176/1000 | Loss: 0.00002384
Iteration 177/1000 | Loss: 0.00002384
Iteration 178/1000 | Loss: 0.00002384
Iteration 179/1000 | Loss: 0.00002384
Iteration 180/1000 | Loss: 0.00002384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.384038998570759e-05, 2.384038998570759e-05, 2.384038998570759e-05, 2.384038998570759e-05, 2.384038998570759e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.384038998570759e-05

Optimization complete. Final v2v error: 4.059028148651123 mm

Highest mean error: 4.456378936767578 mm for frame 218

Lowest mean error: 3.720919370651245 mm for frame 1

Saving results

Total time: 111.22810196876526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965693
Iteration 2/25 | Loss: 0.00154230
Iteration 3/25 | Loss: 0.00144386
Iteration 4/25 | Loss: 0.00143067
Iteration 5/25 | Loss: 0.00142770
Iteration 6/25 | Loss: 0.00142745
Iteration 7/25 | Loss: 0.00142745
Iteration 8/25 | Loss: 0.00142745
Iteration 9/25 | Loss: 0.00142745
Iteration 10/25 | Loss: 0.00142745
Iteration 11/25 | Loss: 0.00142745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001427450217306614, 0.001427450217306614, 0.001427450217306614, 0.001427450217306614, 0.001427450217306614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001427450217306614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66115546
Iteration 2/25 | Loss: 0.00124431
Iteration 3/25 | Loss: 0.00124430
Iteration 4/25 | Loss: 0.00124430
Iteration 5/25 | Loss: 0.00124430
Iteration 6/25 | Loss: 0.00124430
Iteration 7/25 | Loss: 0.00124430
Iteration 8/25 | Loss: 0.00124430
Iteration 9/25 | Loss: 0.00124430
Iteration 10/25 | Loss: 0.00124430
Iteration 11/25 | Loss: 0.00124430
Iteration 12/25 | Loss: 0.00124430
Iteration 13/25 | Loss: 0.00124430
Iteration 14/25 | Loss: 0.00124430
Iteration 15/25 | Loss: 0.00124430
Iteration 16/25 | Loss: 0.00124430
Iteration 17/25 | Loss: 0.00124430
Iteration 18/25 | Loss: 0.00124430
Iteration 19/25 | Loss: 0.00124430
Iteration 20/25 | Loss: 0.00124430
Iteration 21/25 | Loss: 0.00124430
Iteration 22/25 | Loss: 0.00124430
Iteration 23/25 | Loss: 0.00124430
Iteration 24/25 | Loss: 0.00124430
Iteration 25/25 | Loss: 0.00124430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124430
Iteration 2/1000 | Loss: 0.00004814
Iteration 3/1000 | Loss: 0.00003112
Iteration 4/1000 | Loss: 0.00002885
Iteration 5/1000 | Loss: 0.00002763
Iteration 6/1000 | Loss: 0.00002636
Iteration 7/1000 | Loss: 0.00002573
Iteration 8/1000 | Loss: 0.00002536
Iteration 9/1000 | Loss: 0.00002502
Iteration 10/1000 | Loss: 0.00002473
Iteration 11/1000 | Loss: 0.00002448
Iteration 12/1000 | Loss: 0.00002439
Iteration 13/1000 | Loss: 0.00002421
Iteration 14/1000 | Loss: 0.00002414
Iteration 15/1000 | Loss: 0.00002399
Iteration 16/1000 | Loss: 0.00002393
Iteration 17/1000 | Loss: 0.00002385
Iteration 18/1000 | Loss: 0.00002384
Iteration 19/1000 | Loss: 0.00002380
Iteration 20/1000 | Loss: 0.00002380
Iteration 21/1000 | Loss: 0.00002378
Iteration 22/1000 | Loss: 0.00002377
Iteration 23/1000 | Loss: 0.00002372
Iteration 24/1000 | Loss: 0.00002368
Iteration 25/1000 | Loss: 0.00002367
Iteration 26/1000 | Loss: 0.00002366
Iteration 27/1000 | Loss: 0.00002366
Iteration 28/1000 | Loss: 0.00002365
Iteration 29/1000 | Loss: 0.00002365
Iteration 30/1000 | Loss: 0.00002364
Iteration 31/1000 | Loss: 0.00002360
Iteration 32/1000 | Loss: 0.00002359
Iteration 33/1000 | Loss: 0.00002358
Iteration 34/1000 | Loss: 0.00002358
Iteration 35/1000 | Loss: 0.00002357
Iteration 36/1000 | Loss: 0.00002357
Iteration 37/1000 | Loss: 0.00002356
Iteration 38/1000 | Loss: 0.00002356
Iteration 39/1000 | Loss: 0.00002356
Iteration 40/1000 | Loss: 0.00002355
Iteration 41/1000 | Loss: 0.00002355
Iteration 42/1000 | Loss: 0.00002355
Iteration 43/1000 | Loss: 0.00002355
Iteration 44/1000 | Loss: 0.00002355
Iteration 45/1000 | Loss: 0.00002354
Iteration 46/1000 | Loss: 0.00002354
Iteration 47/1000 | Loss: 0.00002354
Iteration 48/1000 | Loss: 0.00002353
Iteration 49/1000 | Loss: 0.00002353
Iteration 50/1000 | Loss: 0.00002353
Iteration 51/1000 | Loss: 0.00002353
Iteration 52/1000 | Loss: 0.00002352
Iteration 53/1000 | Loss: 0.00002351
Iteration 54/1000 | Loss: 0.00002351
Iteration 55/1000 | Loss: 0.00002350
Iteration 56/1000 | Loss: 0.00002350
Iteration 57/1000 | Loss: 0.00002349
Iteration 58/1000 | Loss: 0.00002349
Iteration 59/1000 | Loss: 0.00002349
Iteration 60/1000 | Loss: 0.00002349
Iteration 61/1000 | Loss: 0.00002349
Iteration 62/1000 | Loss: 0.00002349
Iteration 63/1000 | Loss: 0.00002348
Iteration 64/1000 | Loss: 0.00002348
Iteration 65/1000 | Loss: 0.00002348
Iteration 66/1000 | Loss: 0.00002347
Iteration 67/1000 | Loss: 0.00002346
Iteration 68/1000 | Loss: 0.00002346
Iteration 69/1000 | Loss: 0.00002346
Iteration 70/1000 | Loss: 0.00002345
Iteration 71/1000 | Loss: 0.00002345
Iteration 72/1000 | Loss: 0.00002345
Iteration 73/1000 | Loss: 0.00002344
Iteration 74/1000 | Loss: 0.00002344
Iteration 75/1000 | Loss: 0.00002344
Iteration 76/1000 | Loss: 0.00002344
Iteration 77/1000 | Loss: 0.00002343
Iteration 78/1000 | Loss: 0.00002343
Iteration 79/1000 | Loss: 0.00002343
Iteration 80/1000 | Loss: 0.00002343
Iteration 81/1000 | Loss: 0.00002343
Iteration 82/1000 | Loss: 0.00002343
Iteration 83/1000 | Loss: 0.00002343
Iteration 84/1000 | Loss: 0.00002342
Iteration 85/1000 | Loss: 0.00002342
Iteration 86/1000 | Loss: 0.00002342
Iteration 87/1000 | Loss: 0.00002342
Iteration 88/1000 | Loss: 0.00002342
Iteration 89/1000 | Loss: 0.00002342
Iteration 90/1000 | Loss: 0.00002342
Iteration 91/1000 | Loss: 0.00002342
Iteration 92/1000 | Loss: 0.00002342
Iteration 93/1000 | Loss: 0.00002342
Iteration 94/1000 | Loss: 0.00002342
Iteration 95/1000 | Loss: 0.00002342
Iteration 96/1000 | Loss: 0.00002342
Iteration 97/1000 | Loss: 0.00002342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.3422588128596544e-05, 2.3422588128596544e-05, 2.3422588128596544e-05, 2.3422588128596544e-05, 2.3422588128596544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3422588128596544e-05

Optimization complete. Final v2v error: 4.049726486206055 mm

Highest mean error: 4.332669734954834 mm for frame 54

Lowest mean error: 3.7498013973236084 mm for frame 121

Saving results

Total time: 35.623204946517944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761218
Iteration 2/25 | Loss: 0.00136750
Iteration 3/25 | Loss: 0.00128381
Iteration 4/25 | Loss: 0.00127643
Iteration 5/25 | Loss: 0.00127643
Iteration 6/25 | Loss: 0.00127643
Iteration 7/25 | Loss: 0.00127643
Iteration 8/25 | Loss: 0.00127643
Iteration 9/25 | Loss: 0.00127643
Iteration 10/25 | Loss: 0.00127643
Iteration 11/25 | Loss: 0.00127643
Iteration 12/25 | Loss: 0.00127643
Iteration 13/25 | Loss: 0.00127643
Iteration 14/25 | Loss: 0.00127643
Iteration 15/25 | Loss: 0.00127643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001276431605219841, 0.001276431605219841, 0.001276431605219841, 0.001276431605219841, 0.001276431605219841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001276431605219841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39816403
Iteration 2/25 | Loss: 0.00077822
Iteration 3/25 | Loss: 0.00077821
Iteration 4/25 | Loss: 0.00077821
Iteration 5/25 | Loss: 0.00077821
Iteration 6/25 | Loss: 0.00077821
Iteration 7/25 | Loss: 0.00077821
Iteration 8/25 | Loss: 0.00077821
Iteration 9/25 | Loss: 0.00077821
Iteration 10/25 | Loss: 0.00077821
Iteration 11/25 | Loss: 0.00077821
Iteration 12/25 | Loss: 0.00077821
Iteration 13/25 | Loss: 0.00077821
Iteration 14/25 | Loss: 0.00077821
Iteration 15/25 | Loss: 0.00077821
Iteration 16/25 | Loss: 0.00077821
Iteration 17/25 | Loss: 0.00077821
Iteration 18/25 | Loss: 0.00077821
Iteration 19/25 | Loss: 0.00077821
Iteration 20/25 | Loss: 0.00077821
Iteration 21/25 | Loss: 0.00077821
Iteration 22/25 | Loss: 0.00077821
Iteration 23/25 | Loss: 0.00077821
Iteration 24/25 | Loss: 0.00077821
Iteration 25/25 | Loss: 0.00077821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077821
Iteration 2/1000 | Loss: 0.00002698
Iteration 3/1000 | Loss: 0.00001939
Iteration 4/1000 | Loss: 0.00001707
Iteration 5/1000 | Loss: 0.00001587
Iteration 6/1000 | Loss: 0.00001520
Iteration 7/1000 | Loss: 0.00001483
Iteration 8/1000 | Loss: 0.00001433
Iteration 9/1000 | Loss: 0.00001399
Iteration 10/1000 | Loss: 0.00001379
Iteration 11/1000 | Loss: 0.00001366
Iteration 12/1000 | Loss: 0.00001363
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001350
Iteration 15/1000 | Loss: 0.00001346
Iteration 16/1000 | Loss: 0.00001346
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001326
Iteration 20/1000 | Loss: 0.00001321
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001320
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001314
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001300
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001299
Iteration 36/1000 | Loss: 0.00001299
Iteration 37/1000 | Loss: 0.00001298
Iteration 38/1000 | Loss: 0.00001297
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001295
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001284
Iteration 51/1000 | Loss: 0.00001281
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001275
Iteration 61/1000 | Loss: 0.00001275
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001274
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001273
Iteration 67/1000 | Loss: 0.00001273
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001272
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001270
Iteration 76/1000 | Loss: 0.00001270
Iteration 77/1000 | Loss: 0.00001269
Iteration 78/1000 | Loss: 0.00001269
Iteration 79/1000 | Loss: 0.00001269
Iteration 80/1000 | Loss: 0.00001268
Iteration 81/1000 | Loss: 0.00001268
Iteration 82/1000 | Loss: 0.00001268
Iteration 83/1000 | Loss: 0.00001268
Iteration 84/1000 | Loss: 0.00001268
Iteration 85/1000 | Loss: 0.00001268
Iteration 86/1000 | Loss: 0.00001268
Iteration 87/1000 | Loss: 0.00001268
Iteration 88/1000 | Loss: 0.00001268
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001267
Iteration 91/1000 | Loss: 0.00001267
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001266
Iteration 95/1000 | Loss: 0.00001266
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001262
Iteration 123/1000 | Loss: 0.00001262
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001260
Iteration 135/1000 | Loss: 0.00001260
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001258
Iteration 142/1000 | Loss: 0.00001258
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001258
Iteration 145/1000 | Loss: 0.00001257
Iteration 146/1000 | Loss: 0.00001257
Iteration 147/1000 | Loss: 0.00001256
Iteration 148/1000 | Loss: 0.00001256
Iteration 149/1000 | Loss: 0.00001256
Iteration 150/1000 | Loss: 0.00001256
Iteration 151/1000 | Loss: 0.00001256
Iteration 152/1000 | Loss: 0.00001255
Iteration 153/1000 | Loss: 0.00001255
Iteration 154/1000 | Loss: 0.00001255
Iteration 155/1000 | Loss: 0.00001254
Iteration 156/1000 | Loss: 0.00001254
Iteration 157/1000 | Loss: 0.00001254
Iteration 158/1000 | Loss: 0.00001254
Iteration 159/1000 | Loss: 0.00001254
Iteration 160/1000 | Loss: 0.00001254
Iteration 161/1000 | Loss: 0.00001254
Iteration 162/1000 | Loss: 0.00001254
Iteration 163/1000 | Loss: 0.00001254
Iteration 164/1000 | Loss: 0.00001254
Iteration 165/1000 | Loss: 0.00001254
Iteration 166/1000 | Loss: 0.00001254
Iteration 167/1000 | Loss: 0.00001254
Iteration 168/1000 | Loss: 0.00001254
Iteration 169/1000 | Loss: 0.00001254
Iteration 170/1000 | Loss: 0.00001254
Iteration 171/1000 | Loss: 0.00001254
Iteration 172/1000 | Loss: 0.00001254
Iteration 173/1000 | Loss: 0.00001254
Iteration 174/1000 | Loss: 0.00001254
Iteration 175/1000 | Loss: 0.00001254
Iteration 176/1000 | Loss: 0.00001254
Iteration 177/1000 | Loss: 0.00001254
Iteration 178/1000 | Loss: 0.00001254
Iteration 179/1000 | Loss: 0.00001254
Iteration 180/1000 | Loss: 0.00001254
Iteration 181/1000 | Loss: 0.00001254
Iteration 182/1000 | Loss: 0.00001254
Iteration 183/1000 | Loss: 0.00001254
Iteration 184/1000 | Loss: 0.00001254
Iteration 185/1000 | Loss: 0.00001254
Iteration 186/1000 | Loss: 0.00001254
Iteration 187/1000 | Loss: 0.00001254
Iteration 188/1000 | Loss: 0.00001254
Iteration 189/1000 | Loss: 0.00001254
Iteration 190/1000 | Loss: 0.00001254
Iteration 191/1000 | Loss: 0.00001254
Iteration 192/1000 | Loss: 0.00001254
Iteration 193/1000 | Loss: 0.00001254
Iteration 194/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.2538929695438128e-05, 1.2538929695438128e-05, 1.2538929695438128e-05, 1.2538929695438128e-05, 1.2538929695438128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2538929695438128e-05

Optimization complete. Final v2v error: 3.008086919784546 mm

Highest mean error: 3.1901180744171143 mm for frame 140

Lowest mean error: 2.852851390838623 mm for frame 4

Saving results

Total time: 47.723498821258545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438088
Iteration 2/25 | Loss: 0.00135221
Iteration 3/25 | Loss: 0.00126820
Iteration 4/25 | Loss: 0.00125759
Iteration 5/25 | Loss: 0.00125453
Iteration 6/25 | Loss: 0.00125396
Iteration 7/25 | Loss: 0.00125396
Iteration 8/25 | Loss: 0.00125396
Iteration 9/25 | Loss: 0.00125396
Iteration 10/25 | Loss: 0.00125396
Iteration 11/25 | Loss: 0.00125396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012539594899863005, 0.0012539594899863005, 0.0012539594899863005, 0.0012539594899863005, 0.0012539594899863005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012539594899863005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.57824516
Iteration 2/25 | Loss: 0.00082286
Iteration 3/25 | Loss: 0.00082285
Iteration 4/25 | Loss: 0.00082285
Iteration 5/25 | Loss: 0.00082285
Iteration 6/25 | Loss: 0.00082285
Iteration 7/25 | Loss: 0.00082285
Iteration 8/25 | Loss: 0.00082285
Iteration 9/25 | Loss: 0.00082285
Iteration 10/25 | Loss: 0.00082285
Iteration 11/25 | Loss: 0.00082285
Iteration 12/25 | Loss: 0.00082285
Iteration 13/25 | Loss: 0.00082285
Iteration 14/25 | Loss: 0.00082285
Iteration 15/25 | Loss: 0.00082285
Iteration 16/25 | Loss: 0.00082285
Iteration 17/25 | Loss: 0.00082285
Iteration 18/25 | Loss: 0.00082285
Iteration 19/25 | Loss: 0.00082285
Iteration 20/25 | Loss: 0.00082285
Iteration 21/25 | Loss: 0.00082285
Iteration 22/25 | Loss: 0.00082285
Iteration 23/25 | Loss: 0.00082285
Iteration 24/25 | Loss: 0.00082285
Iteration 25/25 | Loss: 0.00082285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082285
Iteration 2/1000 | Loss: 0.00002816
Iteration 3/1000 | Loss: 0.00001967
Iteration 4/1000 | Loss: 0.00001796
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001630
Iteration 7/1000 | Loss: 0.00001577
Iteration 8/1000 | Loss: 0.00001543
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00001494
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001469
Iteration 14/1000 | Loss: 0.00001455
Iteration 15/1000 | Loss: 0.00001453
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001451
Iteration 19/1000 | Loss: 0.00001451
Iteration 20/1000 | Loss: 0.00001449
Iteration 21/1000 | Loss: 0.00001449
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001441
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001437
Iteration 29/1000 | Loss: 0.00001437
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001435
Iteration 32/1000 | Loss: 0.00001434
Iteration 33/1000 | Loss: 0.00001434
Iteration 34/1000 | Loss: 0.00001434
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00001428
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001426
Iteration 51/1000 | Loss: 0.00001425
Iteration 52/1000 | Loss: 0.00001425
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001424
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001424
Iteration 57/1000 | Loss: 0.00001423
Iteration 58/1000 | Loss: 0.00001423
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001419
Iteration 65/1000 | Loss: 0.00001418
Iteration 66/1000 | Loss: 0.00001416
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001408
Iteration 94/1000 | Loss: 0.00001408
Iteration 95/1000 | Loss: 0.00001408
Iteration 96/1000 | Loss: 0.00001407
Iteration 97/1000 | Loss: 0.00001407
Iteration 98/1000 | Loss: 0.00001407
Iteration 99/1000 | Loss: 0.00001407
Iteration 100/1000 | Loss: 0.00001407
Iteration 101/1000 | Loss: 0.00001406
Iteration 102/1000 | Loss: 0.00001406
Iteration 103/1000 | Loss: 0.00001406
Iteration 104/1000 | Loss: 0.00001406
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001405
Iteration 109/1000 | Loss: 0.00001405
Iteration 110/1000 | Loss: 0.00001405
Iteration 111/1000 | Loss: 0.00001405
Iteration 112/1000 | Loss: 0.00001405
Iteration 113/1000 | Loss: 0.00001405
Iteration 114/1000 | Loss: 0.00001404
Iteration 115/1000 | Loss: 0.00001404
Iteration 116/1000 | Loss: 0.00001404
Iteration 117/1000 | Loss: 0.00001404
Iteration 118/1000 | Loss: 0.00001404
Iteration 119/1000 | Loss: 0.00001404
Iteration 120/1000 | Loss: 0.00001404
Iteration 121/1000 | Loss: 0.00001404
Iteration 122/1000 | Loss: 0.00001404
Iteration 123/1000 | Loss: 0.00001404
Iteration 124/1000 | Loss: 0.00001403
Iteration 125/1000 | Loss: 0.00001403
Iteration 126/1000 | Loss: 0.00001403
Iteration 127/1000 | Loss: 0.00001403
Iteration 128/1000 | Loss: 0.00001403
Iteration 129/1000 | Loss: 0.00001403
Iteration 130/1000 | Loss: 0.00001403
Iteration 131/1000 | Loss: 0.00001403
Iteration 132/1000 | Loss: 0.00001403
Iteration 133/1000 | Loss: 0.00001403
Iteration 134/1000 | Loss: 0.00001403
Iteration 135/1000 | Loss: 0.00001403
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001402
Iteration 141/1000 | Loss: 0.00001402
Iteration 142/1000 | Loss: 0.00001401
Iteration 143/1000 | Loss: 0.00001401
Iteration 144/1000 | Loss: 0.00001401
Iteration 145/1000 | Loss: 0.00001401
Iteration 146/1000 | Loss: 0.00001401
Iteration 147/1000 | Loss: 0.00001401
Iteration 148/1000 | Loss: 0.00001401
Iteration 149/1000 | Loss: 0.00001401
Iteration 150/1000 | Loss: 0.00001401
Iteration 151/1000 | Loss: 0.00001401
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001400
Iteration 159/1000 | Loss: 0.00001400
Iteration 160/1000 | Loss: 0.00001400
Iteration 161/1000 | Loss: 0.00001400
Iteration 162/1000 | Loss: 0.00001400
Iteration 163/1000 | Loss: 0.00001400
Iteration 164/1000 | Loss: 0.00001400
Iteration 165/1000 | Loss: 0.00001400
Iteration 166/1000 | Loss: 0.00001400
Iteration 167/1000 | Loss: 0.00001400
Iteration 168/1000 | Loss: 0.00001400
Iteration 169/1000 | Loss: 0.00001400
Iteration 170/1000 | Loss: 0.00001400
Iteration 171/1000 | Loss: 0.00001400
Iteration 172/1000 | Loss: 0.00001400
Iteration 173/1000 | Loss: 0.00001400
Iteration 174/1000 | Loss: 0.00001400
Iteration 175/1000 | Loss: 0.00001400
Iteration 176/1000 | Loss: 0.00001400
Iteration 177/1000 | Loss: 0.00001400
Iteration 178/1000 | Loss: 0.00001400
Iteration 179/1000 | Loss: 0.00001400
Iteration 180/1000 | Loss: 0.00001400
Iteration 181/1000 | Loss: 0.00001400
Iteration 182/1000 | Loss: 0.00001400
Iteration 183/1000 | Loss: 0.00001400
Iteration 184/1000 | Loss: 0.00001400
Iteration 185/1000 | Loss: 0.00001400
Iteration 186/1000 | Loss: 0.00001400
Iteration 187/1000 | Loss: 0.00001400
Iteration 188/1000 | Loss: 0.00001400
Iteration 189/1000 | Loss: 0.00001400
Iteration 190/1000 | Loss: 0.00001400
Iteration 191/1000 | Loss: 0.00001400
Iteration 192/1000 | Loss: 0.00001400
Iteration 193/1000 | Loss: 0.00001400
Iteration 194/1000 | Loss: 0.00001400
Iteration 195/1000 | Loss: 0.00001400
Iteration 196/1000 | Loss: 0.00001400
Iteration 197/1000 | Loss: 0.00001400
Iteration 198/1000 | Loss: 0.00001400
Iteration 199/1000 | Loss: 0.00001400
Iteration 200/1000 | Loss: 0.00001400
Iteration 201/1000 | Loss: 0.00001400
Iteration 202/1000 | Loss: 0.00001400
Iteration 203/1000 | Loss: 0.00001400
Iteration 204/1000 | Loss: 0.00001400
Iteration 205/1000 | Loss: 0.00001400
Iteration 206/1000 | Loss: 0.00001400
Iteration 207/1000 | Loss: 0.00001400
Iteration 208/1000 | Loss: 0.00001400
Iteration 209/1000 | Loss: 0.00001400
Iteration 210/1000 | Loss: 0.00001400
Iteration 211/1000 | Loss: 0.00001400
Iteration 212/1000 | Loss: 0.00001400
Iteration 213/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.4000605915498454e-05, 1.4000605915498454e-05, 1.4000605915498454e-05, 1.4000605915498454e-05, 1.4000605915498454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4000605915498454e-05

Optimization complete. Final v2v error: 3.1821091175079346 mm

Highest mean error: 3.8885719776153564 mm for frame 89

Lowest mean error: 2.8501007556915283 mm for frame 39

Saving results

Total time: 40.67554521560669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018813
Iteration 2/25 | Loss: 0.00261059
Iteration 3/25 | Loss: 0.00218227
Iteration 4/25 | Loss: 0.00169395
Iteration 5/25 | Loss: 0.00163598
Iteration 6/25 | Loss: 0.00161513
Iteration 7/25 | Loss: 0.00148683
Iteration 8/25 | Loss: 0.00144379
Iteration 9/25 | Loss: 0.00140226
Iteration 10/25 | Loss: 0.00140427
Iteration 11/25 | Loss: 0.00139966
Iteration 12/25 | Loss: 0.00136852
Iteration 13/25 | Loss: 0.00135907
Iteration 14/25 | Loss: 0.00135492
Iteration 15/25 | Loss: 0.00135436
Iteration 16/25 | Loss: 0.00135425
Iteration 17/25 | Loss: 0.00135423
Iteration 18/25 | Loss: 0.00135423
Iteration 19/25 | Loss: 0.00135423
Iteration 20/25 | Loss: 0.00135423
Iteration 21/25 | Loss: 0.00135423
Iteration 22/25 | Loss: 0.00135423
Iteration 23/25 | Loss: 0.00135423
Iteration 24/25 | Loss: 0.00135423
Iteration 25/25 | Loss: 0.00135423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44528365
Iteration 2/25 | Loss: 0.00090717
Iteration 3/25 | Loss: 0.00090717
Iteration 4/25 | Loss: 0.00089943
Iteration 5/25 | Loss: 0.00089942
Iteration 6/25 | Loss: 0.00089942
Iteration 7/25 | Loss: 0.00089942
Iteration 8/25 | Loss: 0.00089941
Iteration 9/25 | Loss: 0.00089941
Iteration 10/25 | Loss: 0.00089941
Iteration 11/25 | Loss: 0.00089941
Iteration 12/25 | Loss: 0.00089941
Iteration 13/25 | Loss: 0.00089941
Iteration 14/25 | Loss: 0.00089941
Iteration 15/25 | Loss: 0.00089941
Iteration 16/25 | Loss: 0.00089941
Iteration 17/25 | Loss: 0.00089941
Iteration 18/25 | Loss: 0.00089941
Iteration 19/25 | Loss: 0.00089941
Iteration 20/25 | Loss: 0.00089941
Iteration 21/25 | Loss: 0.00089941
Iteration 22/25 | Loss: 0.00089941
Iteration 23/25 | Loss: 0.00089941
Iteration 24/25 | Loss: 0.00089941
Iteration 25/25 | Loss: 0.00089941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089941
Iteration 2/1000 | Loss: 0.00019125
Iteration 3/1000 | Loss: 0.00034222
Iteration 4/1000 | Loss: 0.00025756
Iteration 5/1000 | Loss: 0.00048180
Iteration 6/1000 | Loss: 0.00294306
Iteration 7/1000 | Loss: 0.00092439
Iteration 8/1000 | Loss: 0.00020449
Iteration 9/1000 | Loss: 0.00037916
Iteration 10/1000 | Loss: 0.00006332
Iteration 11/1000 | Loss: 0.00002708
Iteration 12/1000 | Loss: 0.00005225
Iteration 13/1000 | Loss: 0.00007425
Iteration 14/1000 | Loss: 0.00022705
Iteration 15/1000 | Loss: 0.00024684
Iteration 16/1000 | Loss: 0.00005453
Iteration 17/1000 | Loss: 0.00019325
Iteration 18/1000 | Loss: 0.00027519
Iteration 19/1000 | Loss: 0.00009168
Iteration 20/1000 | Loss: 0.00003018
Iteration 21/1000 | Loss: 0.00004420
Iteration 22/1000 | Loss: 0.00002752
Iteration 23/1000 | Loss: 0.00005391
Iteration 24/1000 | Loss: 0.00002610
Iteration 25/1000 | Loss: 0.00006501
Iteration 26/1000 | Loss: 0.00015776
Iteration 27/1000 | Loss: 0.00031381
Iteration 28/1000 | Loss: 0.00006249
Iteration 29/1000 | Loss: 0.00002988
Iteration 30/1000 | Loss: 0.00003424
Iteration 31/1000 | Loss: 0.00002439
Iteration 32/1000 | Loss: 0.00015278
Iteration 33/1000 | Loss: 0.00002436
Iteration 34/1000 | Loss: 0.00002395
Iteration 35/1000 | Loss: 0.00002389
Iteration 36/1000 | Loss: 0.00002376
Iteration 37/1000 | Loss: 0.00012623
Iteration 38/1000 | Loss: 0.00036280
Iteration 39/1000 | Loss: 0.00015733
Iteration 40/1000 | Loss: 0.00003317
Iteration 41/1000 | Loss: 0.00002817
Iteration 42/1000 | Loss: 0.00002373
Iteration 43/1000 | Loss: 0.00002350
Iteration 44/1000 | Loss: 0.00002342
Iteration 45/1000 | Loss: 0.00002342
Iteration 46/1000 | Loss: 0.00002341
Iteration 47/1000 | Loss: 0.00002341
Iteration 48/1000 | Loss: 0.00002340
Iteration 49/1000 | Loss: 0.00002339
Iteration 50/1000 | Loss: 0.00002336
Iteration 51/1000 | Loss: 0.00002334
Iteration 52/1000 | Loss: 0.00002334
Iteration 53/1000 | Loss: 0.00002334
Iteration 54/1000 | Loss: 0.00002334
Iteration 55/1000 | Loss: 0.00002334
Iteration 56/1000 | Loss: 0.00002333
Iteration 57/1000 | Loss: 0.00002333
Iteration 58/1000 | Loss: 0.00002333
Iteration 59/1000 | Loss: 0.00002332
Iteration 60/1000 | Loss: 0.00002332
Iteration 61/1000 | Loss: 0.00002332
Iteration 62/1000 | Loss: 0.00002331
Iteration 63/1000 | Loss: 0.00002329
Iteration 64/1000 | Loss: 0.00007451
Iteration 65/1000 | Loss: 0.00007284
Iteration 66/1000 | Loss: 0.00003295
Iteration 67/1000 | Loss: 0.00002862
Iteration 68/1000 | Loss: 0.00002332
Iteration 69/1000 | Loss: 0.00002330
Iteration 70/1000 | Loss: 0.00002328
Iteration 71/1000 | Loss: 0.00002327
Iteration 72/1000 | Loss: 0.00009891
Iteration 73/1000 | Loss: 0.00003946
Iteration 74/1000 | Loss: 0.00002336
Iteration 75/1000 | Loss: 0.00004459
Iteration 76/1000 | Loss: 0.00002675
Iteration 77/1000 | Loss: 0.00003012
Iteration 78/1000 | Loss: 0.00002413
Iteration 79/1000 | Loss: 0.00002328
Iteration 80/1000 | Loss: 0.00002328
Iteration 81/1000 | Loss: 0.00002366
Iteration 82/1000 | Loss: 0.00002363
Iteration 83/1000 | Loss: 0.00002327
Iteration 84/1000 | Loss: 0.00002327
Iteration 85/1000 | Loss: 0.00002327
Iteration 86/1000 | Loss: 0.00002327
Iteration 87/1000 | Loss: 0.00002327
Iteration 88/1000 | Loss: 0.00002327
Iteration 89/1000 | Loss: 0.00002327
Iteration 90/1000 | Loss: 0.00002327
Iteration 91/1000 | Loss: 0.00002327
Iteration 92/1000 | Loss: 0.00002327
Iteration 93/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.327002221136354e-05, 2.327002221136354e-05, 2.327002221136354e-05, 2.327002221136354e-05, 2.327002221136354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.327002221136354e-05

Optimization complete. Final v2v error: 3.832489252090454 mm

Highest mean error: 11.373494148254395 mm for frame 5

Lowest mean error: 3.280658483505249 mm for frame 82

Saving results

Total time: 120.9764997959137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825325
Iteration 2/25 | Loss: 0.00230786
Iteration 3/25 | Loss: 0.00172883
Iteration 4/25 | Loss: 0.00166653
Iteration 5/25 | Loss: 0.00165312
Iteration 6/25 | Loss: 0.00154862
Iteration 7/25 | Loss: 0.00152389
Iteration 8/25 | Loss: 0.00151782
Iteration 9/25 | Loss: 0.00152620
Iteration 10/25 | Loss: 0.00151718
Iteration 11/25 | Loss: 0.00151411
Iteration 12/25 | Loss: 0.00151080
Iteration 13/25 | Loss: 0.00150681
Iteration 14/25 | Loss: 0.00150460
Iteration 15/25 | Loss: 0.00150616
Iteration 16/25 | Loss: 0.00150444
Iteration 17/25 | Loss: 0.00150319
Iteration 18/25 | Loss: 0.00150279
Iteration 19/25 | Loss: 0.00150050
Iteration 20/25 | Loss: 0.00150162
Iteration 21/25 | Loss: 0.00149883
Iteration 22/25 | Loss: 0.00150003
Iteration 23/25 | Loss: 0.00149653
Iteration 24/25 | Loss: 0.00149928
Iteration 25/25 | Loss: 0.00149704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43545294
Iteration 2/25 | Loss: 0.00103262
Iteration 3/25 | Loss: 0.00103260
Iteration 4/25 | Loss: 0.00103260
Iteration 5/25 | Loss: 0.00103260
Iteration 6/25 | Loss: 0.00103260
Iteration 7/25 | Loss: 0.00103260
Iteration 8/25 | Loss: 0.00103260
Iteration 9/25 | Loss: 0.00103260
Iteration 10/25 | Loss: 0.00103260
Iteration 11/25 | Loss: 0.00103260
Iteration 12/25 | Loss: 0.00103260
Iteration 13/25 | Loss: 0.00103260
Iteration 14/25 | Loss: 0.00103260
Iteration 15/25 | Loss: 0.00103260
Iteration 16/25 | Loss: 0.00103260
Iteration 17/25 | Loss: 0.00103260
Iteration 18/25 | Loss: 0.00103260
Iteration 19/25 | Loss: 0.00103260
Iteration 20/25 | Loss: 0.00103260
Iteration 21/25 | Loss: 0.00103260
Iteration 22/25 | Loss: 0.00103260
Iteration 23/25 | Loss: 0.00103260
Iteration 24/25 | Loss: 0.00103260
Iteration 25/25 | Loss: 0.00103260
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010325978510081768, 0.0010325978510081768, 0.0010325978510081768, 0.0010325978510081768, 0.0010325978510081768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010325978510081768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103260
Iteration 2/1000 | Loss: 0.00014292
Iteration 3/1000 | Loss: 0.00018237
Iteration 4/1000 | Loss: 0.00014177
Iteration 5/1000 | Loss: 0.00013395
Iteration 6/1000 | Loss: 0.00017373
Iteration 7/1000 | Loss: 0.00014322
Iteration 8/1000 | Loss: 0.00013218
Iteration 9/1000 | Loss: 0.00012059
Iteration 10/1000 | Loss: 0.00009036
Iteration 11/1000 | Loss: 0.00012386
Iteration 12/1000 | Loss: 0.00014634
Iteration 13/1000 | Loss: 0.00017443
Iteration 14/1000 | Loss: 0.00011730
Iteration 15/1000 | Loss: 0.00016597
Iteration 16/1000 | Loss: 0.00017095
Iteration 17/1000 | Loss: 0.00014028
Iteration 18/1000 | Loss: 0.00017223
Iteration 19/1000 | Loss: 0.00019816
Iteration 20/1000 | Loss: 0.00022729
Iteration 21/1000 | Loss: 0.00021100
Iteration 22/1000 | Loss: 0.00018693
Iteration 23/1000 | Loss: 0.00017125
Iteration 24/1000 | Loss: 0.00018155
Iteration 25/1000 | Loss: 0.00015750
Iteration 26/1000 | Loss: 0.00026546
Iteration 27/1000 | Loss: 0.00017788
Iteration 28/1000 | Loss: 0.00016257
Iteration 29/1000 | Loss: 0.00012455
Iteration 30/1000 | Loss: 0.00017454
Iteration 31/1000 | Loss: 0.00015954
Iteration 32/1000 | Loss: 0.00011891
Iteration 33/1000 | Loss: 0.00008851
Iteration 34/1000 | Loss: 0.00012159
Iteration 35/1000 | Loss: 0.00014058
Iteration 36/1000 | Loss: 0.00015352
Iteration 37/1000 | Loss: 0.00016575
Iteration 38/1000 | Loss: 0.00014088
Iteration 39/1000 | Loss: 0.00012142
Iteration 40/1000 | Loss: 0.00011532
Iteration 41/1000 | Loss: 0.00010388
Iteration 42/1000 | Loss: 0.00010523
Iteration 43/1000 | Loss: 0.00013374
Iteration 44/1000 | Loss: 0.00015107
Iteration 45/1000 | Loss: 0.00015978
Iteration 46/1000 | Loss: 0.00014784
Iteration 47/1000 | Loss: 0.00010156
Iteration 48/1000 | Loss: 0.00010786
Iteration 49/1000 | Loss: 0.00015383
Iteration 50/1000 | Loss: 0.00013480
Iteration 51/1000 | Loss: 0.00011799
Iteration 52/1000 | Loss: 0.00010184
Iteration 53/1000 | Loss: 0.00010507
Iteration 54/1000 | Loss: 0.00012696
Iteration 55/1000 | Loss: 0.00012440
Iteration 56/1000 | Loss: 0.00012663
Iteration 57/1000 | Loss: 0.00010787
Iteration 58/1000 | Loss: 0.00011804
Iteration 59/1000 | Loss: 0.00012475
Iteration 60/1000 | Loss: 0.00013251
Iteration 61/1000 | Loss: 0.00015433
Iteration 62/1000 | Loss: 0.00015526
Iteration 63/1000 | Loss: 0.00007742
Iteration 64/1000 | Loss: 0.00013897
Iteration 65/1000 | Loss: 0.00013992
Iteration 66/1000 | Loss: 0.00012948
Iteration 67/1000 | Loss: 0.00013837
Iteration 68/1000 | Loss: 0.00012563
Iteration 69/1000 | Loss: 0.00014703
Iteration 70/1000 | Loss: 0.00015260
Iteration 71/1000 | Loss: 0.00016667
Iteration 72/1000 | Loss: 0.00012007
Iteration 73/1000 | Loss: 0.00010295
Iteration 74/1000 | Loss: 0.00011942
Iteration 75/1000 | Loss: 0.00014363
Iteration 76/1000 | Loss: 0.00016329
Iteration 77/1000 | Loss: 0.00013224
Iteration 78/1000 | Loss: 0.00009060
Iteration 79/1000 | Loss: 0.00008420
Iteration 80/1000 | Loss: 0.00010613
Iteration 81/1000 | Loss: 0.00011266
Iteration 82/1000 | Loss: 0.00012704
Iteration 83/1000 | Loss: 0.00012963
Iteration 84/1000 | Loss: 0.00010336
Iteration 85/1000 | Loss: 0.00011554
Iteration 86/1000 | Loss: 0.00010024
Iteration 87/1000 | Loss: 0.00012745
Iteration 88/1000 | Loss: 0.00014014
Iteration 89/1000 | Loss: 0.00012261
Iteration 90/1000 | Loss: 0.00008948
Iteration 91/1000 | Loss: 0.00011392
Iteration 92/1000 | Loss: 0.00010586
Iteration 93/1000 | Loss: 0.00008661
Iteration 94/1000 | Loss: 0.00011382
Iteration 95/1000 | Loss: 0.00012500
Iteration 96/1000 | Loss: 0.00013498
Iteration 97/1000 | Loss: 0.00013602
Iteration 98/1000 | Loss: 0.00010899
Iteration 99/1000 | Loss: 0.00010012
Iteration 100/1000 | Loss: 0.00009410
Iteration 101/1000 | Loss: 0.00010359
Iteration 102/1000 | Loss: 0.00010476
Iteration 103/1000 | Loss: 0.00011160
Iteration 104/1000 | Loss: 0.00008163
Iteration 105/1000 | Loss: 0.00007237
Iteration 106/1000 | Loss: 0.00008676
Iteration 107/1000 | Loss: 0.00008005
Iteration 108/1000 | Loss: 0.00007585
Iteration 109/1000 | Loss: 0.00009129
Iteration 110/1000 | Loss: 0.00012111
Iteration 111/1000 | Loss: 0.00012971
Iteration 112/1000 | Loss: 0.00011729
Iteration 113/1000 | Loss: 0.00011507
Iteration 114/1000 | Loss: 0.00011420
Iteration 115/1000 | Loss: 0.00011297
Iteration 116/1000 | Loss: 0.00019045
Iteration 117/1000 | Loss: 0.00012169
Iteration 118/1000 | Loss: 0.00008241
Iteration 119/1000 | Loss: 0.00009220
Iteration 120/1000 | Loss: 0.00011662
Iteration 121/1000 | Loss: 0.00018703
Iteration 122/1000 | Loss: 0.00010133
Iteration 123/1000 | Loss: 0.00008842
Iteration 124/1000 | Loss: 0.00007401
Iteration 125/1000 | Loss: 0.00012017
Iteration 126/1000 | Loss: 0.00012262
Iteration 127/1000 | Loss: 0.00010503
Iteration 128/1000 | Loss: 0.00010771
Iteration 129/1000 | Loss: 0.00010914
Iteration 130/1000 | Loss: 0.00008855
Iteration 131/1000 | Loss: 0.00009359
Iteration 132/1000 | Loss: 0.00008354
Iteration 133/1000 | Loss: 0.00007740
Iteration 134/1000 | Loss: 0.00007148
Iteration 135/1000 | Loss: 0.00010581
Iteration 136/1000 | Loss: 0.00010576
Iteration 137/1000 | Loss: 0.00009822
Iteration 138/1000 | Loss: 0.00008451
Iteration 139/1000 | Loss: 0.00011911
Iteration 140/1000 | Loss: 0.00009647
Iteration 141/1000 | Loss: 0.00007946
Iteration 142/1000 | Loss: 0.00010027
Iteration 143/1000 | Loss: 0.00008324
Iteration 144/1000 | Loss: 0.00007797
Iteration 145/1000 | Loss: 0.00007816
Iteration 146/1000 | Loss: 0.00008924
Iteration 147/1000 | Loss: 0.00008106
Iteration 148/1000 | Loss: 0.00008018
Iteration 149/1000 | Loss: 0.00013032
Iteration 150/1000 | Loss: 0.00009076
Iteration 151/1000 | Loss: 0.00010207
Iteration 152/1000 | Loss: 0.00008269
Iteration 153/1000 | Loss: 0.00009466
Iteration 154/1000 | Loss: 0.00007084
Iteration 155/1000 | Loss: 0.00009083
Iteration 156/1000 | Loss: 0.00007356
Iteration 157/1000 | Loss: 0.00008913
Iteration 158/1000 | Loss: 0.00008641
Iteration 159/1000 | Loss: 0.00008873
Iteration 160/1000 | Loss: 0.00008580
Iteration 161/1000 | Loss: 0.00008145
Iteration 162/1000 | Loss: 0.00006474
Iteration 163/1000 | Loss: 0.00014754
Iteration 164/1000 | Loss: 0.00007297
Iteration 165/1000 | Loss: 0.00005969
Iteration 166/1000 | Loss: 0.00007718
Iteration 167/1000 | Loss: 0.00006254
Iteration 168/1000 | Loss: 0.00009989
Iteration 169/1000 | Loss: 0.00008816
Iteration 170/1000 | Loss: 0.00010108
Iteration 171/1000 | Loss: 0.00007403
Iteration 172/1000 | Loss: 0.00009656
Iteration 173/1000 | Loss: 0.00009559
Iteration 174/1000 | Loss: 0.00009417
Iteration 175/1000 | Loss: 0.00007489
Iteration 176/1000 | Loss: 0.00006881
Iteration 177/1000 | Loss: 0.00006077
Iteration 178/1000 | Loss: 0.00006756
Iteration 179/1000 | Loss: 0.00007098
Iteration 180/1000 | Loss: 0.00008068
Iteration 181/1000 | Loss: 0.00007206
Iteration 182/1000 | Loss: 0.00009586
Iteration 183/1000 | Loss: 0.00009246
Iteration 184/1000 | Loss: 0.00011241
Iteration 185/1000 | Loss: 0.00010838
Iteration 186/1000 | Loss: 0.00005325
Iteration 187/1000 | Loss: 0.00008731
Iteration 188/1000 | Loss: 0.00010899
Iteration 189/1000 | Loss: 0.00008414
Iteration 190/1000 | Loss: 0.00012876
Iteration 191/1000 | Loss: 0.00009251
Iteration 192/1000 | Loss: 0.00011469
Iteration 193/1000 | Loss: 0.00008822
Iteration 194/1000 | Loss: 0.00008305
Iteration 195/1000 | Loss: 0.00007765
Iteration 196/1000 | Loss: 0.00009591
Iteration 197/1000 | Loss: 0.00008175
Iteration 198/1000 | Loss: 0.00010419
Iteration 199/1000 | Loss: 0.00009561
Iteration 200/1000 | Loss: 0.00006558
Iteration 201/1000 | Loss: 0.00006662
Iteration 202/1000 | Loss: 0.00006702
Iteration 203/1000 | Loss: 0.00008485
Iteration 204/1000 | Loss: 0.00006965
Iteration 205/1000 | Loss: 0.00007081
Iteration 206/1000 | Loss: 0.00006457
Iteration 207/1000 | Loss: 0.00006859
Iteration 208/1000 | Loss: 0.00006170
Iteration 209/1000 | Loss: 0.00007372
Iteration 210/1000 | Loss: 0.00005676
Iteration 211/1000 | Loss: 0.00006448
Iteration 212/1000 | Loss: 0.00007081
Iteration 213/1000 | Loss: 0.00007730
Iteration 214/1000 | Loss: 0.00006694
Iteration 215/1000 | Loss: 0.00007379
Iteration 216/1000 | Loss: 0.00007130
Iteration 217/1000 | Loss: 0.00007895
Iteration 218/1000 | Loss: 0.00006804
Iteration 219/1000 | Loss: 0.00006475
Iteration 220/1000 | Loss: 0.00010044
Iteration 221/1000 | Loss: 0.00007453
Iteration 222/1000 | Loss: 0.00006837
Iteration 223/1000 | Loss: 0.00007094
Iteration 224/1000 | Loss: 0.00007668
Iteration 225/1000 | Loss: 0.00004861
Iteration 226/1000 | Loss: 0.00005652
Iteration 227/1000 | Loss: 0.00006691
Iteration 228/1000 | Loss: 0.00007571
Iteration 229/1000 | Loss: 0.00006950
Iteration 230/1000 | Loss: 0.00006436
Iteration 231/1000 | Loss: 0.00005466
Iteration 232/1000 | Loss: 0.00005360
Iteration 233/1000 | Loss: 0.00005132
Iteration 234/1000 | Loss: 0.00005558
Iteration 235/1000 | Loss: 0.00005873
Iteration 236/1000 | Loss: 0.00006007
Iteration 237/1000 | Loss: 0.00006074
Iteration 238/1000 | Loss: 0.00007755
Iteration 239/1000 | Loss: 0.00007694
Iteration 240/1000 | Loss: 0.00008232
Iteration 241/1000 | Loss: 0.00007021
Iteration 242/1000 | Loss: 0.00006860
Iteration 243/1000 | Loss: 0.00007003
Iteration 244/1000 | Loss: 0.00006528
Iteration 245/1000 | Loss: 0.00004283
Iteration 246/1000 | Loss: 0.00004248
Iteration 247/1000 | Loss: 0.00004911
Iteration 248/1000 | Loss: 0.00003989
Iteration 249/1000 | Loss: 0.00004968
Iteration 250/1000 | Loss: 0.00004358
Iteration 251/1000 | Loss: 0.00004479
Iteration 252/1000 | Loss: 0.00004942
Iteration 253/1000 | Loss: 0.00003362
Iteration 254/1000 | Loss: 0.00003359
Iteration 255/1000 | Loss: 0.00004814
Iteration 256/1000 | Loss: 0.00004134
Iteration 257/1000 | Loss: 0.00004642
Iteration 258/1000 | Loss: 0.00003871
Iteration 259/1000 | Loss: 0.00003935
Iteration 260/1000 | Loss: 0.00004001
Iteration 261/1000 | Loss: 0.00003982
Iteration 262/1000 | Loss: 0.00003643
Iteration 263/1000 | Loss: 0.00004141
Iteration 264/1000 | Loss: 0.00004740
Iteration 265/1000 | Loss: 0.00003693
Iteration 266/1000 | Loss: 0.00003831
Iteration 267/1000 | Loss: 0.00004121
Iteration 268/1000 | Loss: 0.00003368
Iteration 269/1000 | Loss: 0.00003116
Iteration 270/1000 | Loss: 0.00004096
Iteration 271/1000 | Loss: 0.00003469
Iteration 272/1000 | Loss: 0.00003782
Iteration 273/1000 | Loss: 0.00004523
Iteration 274/1000 | Loss: 0.00004255
Iteration 275/1000 | Loss: 0.00004571
Iteration 276/1000 | Loss: 0.00004517
Iteration 277/1000 | Loss: 0.00003659
Iteration 278/1000 | Loss: 0.00004981
Iteration 279/1000 | Loss: 0.00003267
Iteration 280/1000 | Loss: 0.00003040
Iteration 281/1000 | Loss: 0.00004989
Iteration 282/1000 | Loss: 0.00004181
Iteration 283/1000 | Loss: 0.00004677
Iteration 284/1000 | Loss: 0.00003158
Iteration 285/1000 | Loss: 0.00003761
Iteration 286/1000 | Loss: 0.00004092
Iteration 287/1000 | Loss: 0.00003645
Iteration 288/1000 | Loss: 0.00004251
Iteration 289/1000 | Loss: 0.00004622
Iteration 290/1000 | Loss: 0.00005133
Iteration 291/1000 | Loss: 0.00004771
Iteration 292/1000 | Loss: 0.00005032
Iteration 293/1000 | Loss: 0.00004755
Iteration 294/1000 | Loss: 0.00003855
Iteration 295/1000 | Loss: 0.00004208
Iteration 296/1000 | Loss: 0.00004697
Iteration 297/1000 | Loss: 0.00004424
Iteration 298/1000 | Loss: 0.00004648
Iteration 299/1000 | Loss: 0.00004451
Iteration 300/1000 | Loss: 0.00004563
Iteration 301/1000 | Loss: 0.00004450
Iteration 302/1000 | Loss: 0.00004563
Iteration 303/1000 | Loss: 0.00004447
Iteration 304/1000 | Loss: 0.00005058
Iteration 305/1000 | Loss: 0.00003210
Iteration 306/1000 | Loss: 0.00007821
Iteration 307/1000 | Loss: 0.00004222
Iteration 308/1000 | Loss: 0.00004438
Iteration 309/1000 | Loss: 0.00004387
Iteration 310/1000 | Loss: 0.00004775
Iteration 311/1000 | Loss: 0.00004450
Iteration 312/1000 | Loss: 0.00004460
Iteration 313/1000 | Loss: 0.00005576
Iteration 314/1000 | Loss: 0.00005132
Iteration 315/1000 | Loss: 0.00004262
Iteration 316/1000 | Loss: 0.00005029
Iteration 317/1000 | Loss: 0.00005411
Iteration 318/1000 | Loss: 0.00004526
Iteration 319/1000 | Loss: 0.00005301
Iteration 320/1000 | Loss: 0.00005239
Iteration 321/1000 | Loss: 0.00004783
Iteration 322/1000 | Loss: 0.00003959
Iteration 323/1000 | Loss: 0.00003042
Iteration 324/1000 | Loss: 0.00002934
Iteration 325/1000 | Loss: 0.00002833
Iteration 326/1000 | Loss: 0.00002783
Iteration 327/1000 | Loss: 0.00002751
Iteration 328/1000 | Loss: 0.00002728
Iteration 329/1000 | Loss: 0.00002699
Iteration 330/1000 | Loss: 0.00002688
Iteration 331/1000 | Loss: 0.00002681
Iteration 332/1000 | Loss: 0.00002665
Iteration 333/1000 | Loss: 0.00002658
Iteration 334/1000 | Loss: 0.00002657
Iteration 335/1000 | Loss: 0.00002654
Iteration 336/1000 | Loss: 0.00002654
Iteration 337/1000 | Loss: 0.00002650
Iteration 338/1000 | Loss: 0.00002646
Iteration 339/1000 | Loss: 0.00002644
Iteration 340/1000 | Loss: 0.00002643
Iteration 341/1000 | Loss: 0.00002642
Iteration 342/1000 | Loss: 0.00002642
Iteration 343/1000 | Loss: 0.00002641
Iteration 344/1000 | Loss: 0.00002641
Iteration 345/1000 | Loss: 0.00002640
Iteration 346/1000 | Loss: 0.00002636
Iteration 347/1000 | Loss: 0.00002636
Iteration 348/1000 | Loss: 0.00002631
Iteration 349/1000 | Loss: 0.00002628
Iteration 350/1000 | Loss: 0.00002627
Iteration 351/1000 | Loss: 0.00002627
Iteration 352/1000 | Loss: 0.00002626
Iteration 353/1000 | Loss: 0.00003275
Iteration 354/1000 | Loss: 0.00003275
Iteration 355/1000 | Loss: 0.00003456
Iteration 356/1000 | Loss: 0.00004667
Iteration 357/1000 | Loss: 0.00004023
Iteration 358/1000 | Loss: 0.00003285
Iteration 359/1000 | Loss: 0.00005116
Iteration 360/1000 | Loss: 0.00003326
Iteration 361/1000 | Loss: 0.00002754
Iteration 362/1000 | Loss: 0.00003221
Iteration 363/1000 | Loss: 0.00005125
Iteration 364/1000 | Loss: 0.00002908
Iteration 365/1000 | Loss: 0.00002645
Iteration 366/1000 | Loss: 0.00002578
Iteration 367/1000 | Loss: 0.00002551
Iteration 368/1000 | Loss: 0.00002550
Iteration 369/1000 | Loss: 0.00002547
Iteration 370/1000 | Loss: 0.00002547
Iteration 371/1000 | Loss: 0.00002546
Iteration 372/1000 | Loss: 0.00002546
Iteration 373/1000 | Loss: 0.00002546
Iteration 374/1000 | Loss: 0.00002546
Iteration 375/1000 | Loss: 0.00002546
Iteration 376/1000 | Loss: 0.00002546
Iteration 377/1000 | Loss: 0.00002544
Iteration 378/1000 | Loss: 0.00002543
Iteration 379/1000 | Loss: 0.00002543
Iteration 380/1000 | Loss: 0.00002541
Iteration 381/1000 | Loss: 0.00002541
Iteration 382/1000 | Loss: 0.00002541
Iteration 383/1000 | Loss: 0.00002541
Iteration 384/1000 | Loss: 0.00002541
Iteration 385/1000 | Loss: 0.00002541
Iteration 386/1000 | Loss: 0.00002541
Iteration 387/1000 | Loss: 0.00002541
Iteration 388/1000 | Loss: 0.00002540
Iteration 389/1000 | Loss: 0.00002537
Iteration 390/1000 | Loss: 0.00002537
Iteration 391/1000 | Loss: 0.00002537
Iteration 392/1000 | Loss: 0.00002535
Iteration 393/1000 | Loss: 0.00002535
Iteration 394/1000 | Loss: 0.00002535
Iteration 395/1000 | Loss: 0.00002535
Iteration 396/1000 | Loss: 0.00002535
Iteration 397/1000 | Loss: 0.00002535
Iteration 398/1000 | Loss: 0.00002535
Iteration 399/1000 | Loss: 0.00002535
Iteration 400/1000 | Loss: 0.00002535
Iteration 401/1000 | Loss: 0.00002535
Iteration 402/1000 | Loss: 0.00002535
Iteration 403/1000 | Loss: 0.00002535
Iteration 404/1000 | Loss: 0.00002534
Iteration 405/1000 | Loss: 0.00002534
Iteration 406/1000 | Loss: 0.00002533
Iteration 407/1000 | Loss: 0.00002533
Iteration 408/1000 | Loss: 0.00002533
Iteration 409/1000 | Loss: 0.00002533
Iteration 410/1000 | Loss: 0.00002533
Iteration 411/1000 | Loss: 0.00002533
Iteration 412/1000 | Loss: 0.00002533
Iteration 413/1000 | Loss: 0.00002533
Iteration 414/1000 | Loss: 0.00002533
Iteration 415/1000 | Loss: 0.00002533
Iteration 416/1000 | Loss: 0.00002533
Iteration 417/1000 | Loss: 0.00002533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 417. Stopping optimization.
Last 5 losses: [2.5326604372821748e-05, 2.5326604372821748e-05, 2.5326604372821748e-05, 2.5326604372821748e-05, 2.5326604372821748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5326604372821748e-05

Optimization complete. Final v2v error: 4.2529802322387695 mm

Highest mean error: 4.926657676696777 mm for frame 195

Lowest mean error: 3.950787305831909 mm for frame 84

Saving results

Total time: 602.7796542644501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851334
Iteration 2/25 | Loss: 0.00136318
Iteration 3/25 | Loss: 0.00128658
Iteration 4/25 | Loss: 0.00127467
Iteration 5/25 | Loss: 0.00127187
Iteration 6/25 | Loss: 0.00127187
Iteration 7/25 | Loss: 0.00127187
Iteration 8/25 | Loss: 0.00127187
Iteration 9/25 | Loss: 0.00127187
Iteration 10/25 | Loss: 0.00127187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012718718498945236, 0.0012718718498945236, 0.0012718718498945236, 0.0012718718498945236, 0.0012718718498945236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012718718498945236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.09224796
Iteration 2/25 | Loss: 0.00082099
Iteration 3/25 | Loss: 0.00082099
Iteration 4/25 | Loss: 0.00082099
Iteration 5/25 | Loss: 0.00082098
Iteration 6/25 | Loss: 0.00082098
Iteration 7/25 | Loss: 0.00082098
Iteration 8/25 | Loss: 0.00082098
Iteration 9/25 | Loss: 0.00082098
Iteration 10/25 | Loss: 0.00082098
Iteration 11/25 | Loss: 0.00082098
Iteration 12/25 | Loss: 0.00082098
Iteration 13/25 | Loss: 0.00082098
Iteration 14/25 | Loss: 0.00082098
Iteration 15/25 | Loss: 0.00082098
Iteration 16/25 | Loss: 0.00082098
Iteration 17/25 | Loss: 0.00082098
Iteration 18/25 | Loss: 0.00082098
Iteration 19/25 | Loss: 0.00082098
Iteration 20/25 | Loss: 0.00082098
Iteration 21/25 | Loss: 0.00082098
Iteration 22/25 | Loss: 0.00082098
Iteration 23/25 | Loss: 0.00082098
Iteration 24/25 | Loss: 0.00082098
Iteration 25/25 | Loss: 0.00082098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082098
Iteration 2/1000 | Loss: 0.00002647
Iteration 3/1000 | Loss: 0.00002121
Iteration 4/1000 | Loss: 0.00001986
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001851
Iteration 7/1000 | Loss: 0.00001815
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001750
Iteration 10/1000 | Loss: 0.00001737
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001703
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001694
Iteration 15/1000 | Loss: 0.00001692
Iteration 16/1000 | Loss: 0.00001691
Iteration 17/1000 | Loss: 0.00001690
Iteration 18/1000 | Loss: 0.00001687
Iteration 19/1000 | Loss: 0.00001679
Iteration 20/1000 | Loss: 0.00001674
Iteration 21/1000 | Loss: 0.00001673
Iteration 22/1000 | Loss: 0.00001666
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001660
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001659
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00001656
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001654
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001649
Iteration 39/1000 | Loss: 0.00001649
Iteration 40/1000 | Loss: 0.00001646
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001643
Iteration 44/1000 | Loss: 0.00001643
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001639
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001638
Iteration 50/1000 | Loss: 0.00001638
Iteration 51/1000 | Loss: 0.00001638
Iteration 52/1000 | Loss: 0.00001637
Iteration 53/1000 | Loss: 0.00001637
Iteration 54/1000 | Loss: 0.00001637
Iteration 55/1000 | Loss: 0.00001636
Iteration 56/1000 | Loss: 0.00001636
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001635
Iteration 59/1000 | Loss: 0.00001635
Iteration 60/1000 | Loss: 0.00001635
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001635
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001635
Iteration 69/1000 | Loss: 0.00001635
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001634
Iteration 73/1000 | Loss: 0.00001634
Iteration 74/1000 | Loss: 0.00001634
Iteration 75/1000 | Loss: 0.00001634
Iteration 76/1000 | Loss: 0.00001634
Iteration 77/1000 | Loss: 0.00001634
Iteration 78/1000 | Loss: 0.00001633
Iteration 79/1000 | Loss: 0.00001633
Iteration 80/1000 | Loss: 0.00001633
Iteration 81/1000 | Loss: 0.00001633
Iteration 82/1000 | Loss: 0.00001633
Iteration 83/1000 | Loss: 0.00001633
Iteration 84/1000 | Loss: 0.00001633
Iteration 85/1000 | Loss: 0.00001633
Iteration 86/1000 | Loss: 0.00001633
Iteration 87/1000 | Loss: 0.00001633
Iteration 88/1000 | Loss: 0.00001633
Iteration 89/1000 | Loss: 0.00001633
Iteration 90/1000 | Loss: 0.00001633
Iteration 91/1000 | Loss: 0.00001632
Iteration 92/1000 | Loss: 0.00001632
Iteration 93/1000 | Loss: 0.00001632
Iteration 94/1000 | Loss: 0.00001632
Iteration 95/1000 | Loss: 0.00001632
Iteration 96/1000 | Loss: 0.00001632
Iteration 97/1000 | Loss: 0.00001632
Iteration 98/1000 | Loss: 0.00001632
Iteration 99/1000 | Loss: 0.00001632
Iteration 100/1000 | Loss: 0.00001632
Iteration 101/1000 | Loss: 0.00001632
Iteration 102/1000 | Loss: 0.00001632
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001631
Iteration 105/1000 | Loss: 0.00001631
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.6313708329107612e-05, 1.6313708329107612e-05, 1.6313708329107612e-05, 1.6313708329107612e-05, 1.6313708329107612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6313708329107612e-05

Optimization complete. Final v2v error: 3.433880567550659 mm

Highest mean error: 3.7444348335266113 mm for frame 199

Lowest mean error: 3.167444944381714 mm for frame 4

Saving results

Total time: 37.486801862716675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824585
Iteration 2/25 | Loss: 0.00151267
Iteration 3/25 | Loss: 0.00131171
Iteration 4/25 | Loss: 0.00127799
Iteration 5/25 | Loss: 0.00127292
Iteration 6/25 | Loss: 0.00127219
Iteration 7/25 | Loss: 0.00127219
Iteration 8/25 | Loss: 0.00127219
Iteration 9/25 | Loss: 0.00127219
Iteration 10/25 | Loss: 0.00127219
Iteration 11/25 | Loss: 0.00127219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012721867533400655, 0.0012721867533400655, 0.0012721867533400655, 0.0012721867533400655, 0.0012721867533400655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012721867533400655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02664876
Iteration 2/25 | Loss: 0.00053380
Iteration 3/25 | Loss: 0.00053379
Iteration 4/25 | Loss: 0.00053379
Iteration 5/25 | Loss: 0.00053379
Iteration 6/25 | Loss: 0.00053379
Iteration 7/25 | Loss: 0.00053379
Iteration 8/25 | Loss: 0.00053378
Iteration 9/25 | Loss: 0.00053378
Iteration 10/25 | Loss: 0.00053378
Iteration 11/25 | Loss: 0.00053378
Iteration 12/25 | Loss: 0.00053378
Iteration 13/25 | Loss: 0.00053378
Iteration 14/25 | Loss: 0.00053378
Iteration 15/25 | Loss: 0.00053378
Iteration 16/25 | Loss: 0.00053378
Iteration 17/25 | Loss: 0.00053378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005337840993888676, 0.0005337840993888676, 0.0005337840993888676, 0.0005337840993888676, 0.0005337840993888676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005337840993888676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053378
Iteration 2/1000 | Loss: 0.00004258
Iteration 3/1000 | Loss: 0.00003118
Iteration 4/1000 | Loss: 0.00002837
Iteration 5/1000 | Loss: 0.00002724
Iteration 6/1000 | Loss: 0.00002618
Iteration 7/1000 | Loss: 0.00002531
Iteration 8/1000 | Loss: 0.00002476
Iteration 9/1000 | Loss: 0.00002439
Iteration 10/1000 | Loss: 0.00002409
Iteration 11/1000 | Loss: 0.00002387
Iteration 12/1000 | Loss: 0.00002370
Iteration 13/1000 | Loss: 0.00002368
Iteration 14/1000 | Loss: 0.00002354
Iteration 15/1000 | Loss: 0.00002339
Iteration 16/1000 | Loss: 0.00002337
Iteration 17/1000 | Loss: 0.00002330
Iteration 18/1000 | Loss: 0.00002330
Iteration 19/1000 | Loss: 0.00002329
Iteration 20/1000 | Loss: 0.00002328
Iteration 21/1000 | Loss: 0.00002327
Iteration 22/1000 | Loss: 0.00002327
Iteration 23/1000 | Loss: 0.00002327
Iteration 24/1000 | Loss: 0.00002327
Iteration 25/1000 | Loss: 0.00002327
Iteration 26/1000 | Loss: 0.00002327
Iteration 27/1000 | Loss: 0.00002327
Iteration 28/1000 | Loss: 0.00002327
Iteration 29/1000 | Loss: 0.00002326
Iteration 30/1000 | Loss: 0.00002326
Iteration 31/1000 | Loss: 0.00002326
Iteration 32/1000 | Loss: 0.00002326
Iteration 33/1000 | Loss: 0.00002326
Iteration 34/1000 | Loss: 0.00002326
Iteration 35/1000 | Loss: 0.00002325
Iteration 36/1000 | Loss: 0.00002324
Iteration 37/1000 | Loss: 0.00002323
Iteration 38/1000 | Loss: 0.00002323
Iteration 39/1000 | Loss: 0.00002323
Iteration 40/1000 | Loss: 0.00002323
Iteration 41/1000 | Loss: 0.00002323
Iteration 42/1000 | Loss: 0.00002323
Iteration 43/1000 | Loss: 0.00002323
Iteration 44/1000 | Loss: 0.00002323
Iteration 45/1000 | Loss: 0.00002322
Iteration 46/1000 | Loss: 0.00002322
Iteration 47/1000 | Loss: 0.00002321
Iteration 48/1000 | Loss: 0.00002320
Iteration 49/1000 | Loss: 0.00002320
Iteration 50/1000 | Loss: 0.00002320
Iteration 51/1000 | Loss: 0.00002320
Iteration 52/1000 | Loss: 0.00002320
Iteration 53/1000 | Loss: 0.00002320
Iteration 54/1000 | Loss: 0.00002319
Iteration 55/1000 | Loss: 0.00002319
Iteration 56/1000 | Loss: 0.00002319
Iteration 57/1000 | Loss: 0.00002319
Iteration 58/1000 | Loss: 0.00002319
Iteration 59/1000 | Loss: 0.00002319
Iteration 60/1000 | Loss: 0.00002319
Iteration 61/1000 | Loss: 0.00002319
Iteration 62/1000 | Loss: 0.00002319
Iteration 63/1000 | Loss: 0.00002319
Iteration 64/1000 | Loss: 0.00002318
Iteration 65/1000 | Loss: 0.00002318
Iteration 66/1000 | Loss: 0.00002318
Iteration 67/1000 | Loss: 0.00002318
Iteration 68/1000 | Loss: 0.00002317
Iteration 69/1000 | Loss: 0.00002317
Iteration 70/1000 | Loss: 0.00002317
Iteration 71/1000 | Loss: 0.00002316
Iteration 72/1000 | Loss: 0.00002316
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00002316
Iteration 75/1000 | Loss: 0.00002316
Iteration 76/1000 | Loss: 0.00002316
Iteration 77/1000 | Loss: 0.00002315
Iteration 78/1000 | Loss: 0.00002315
Iteration 79/1000 | Loss: 0.00002315
Iteration 80/1000 | Loss: 0.00002314
Iteration 81/1000 | Loss: 0.00002314
Iteration 82/1000 | Loss: 0.00002314
Iteration 83/1000 | Loss: 0.00002313
Iteration 84/1000 | Loss: 0.00002313
Iteration 85/1000 | Loss: 0.00002313
Iteration 86/1000 | Loss: 0.00002313
Iteration 87/1000 | Loss: 0.00002313
Iteration 88/1000 | Loss: 0.00002313
Iteration 89/1000 | Loss: 0.00002313
Iteration 90/1000 | Loss: 0.00002312
Iteration 91/1000 | Loss: 0.00002312
Iteration 92/1000 | Loss: 0.00002312
Iteration 93/1000 | Loss: 0.00002312
Iteration 94/1000 | Loss: 0.00002312
Iteration 95/1000 | Loss: 0.00002312
Iteration 96/1000 | Loss: 0.00002312
Iteration 97/1000 | Loss: 0.00002312
Iteration 98/1000 | Loss: 0.00002311
Iteration 99/1000 | Loss: 0.00002311
Iteration 100/1000 | Loss: 0.00002311
Iteration 101/1000 | Loss: 0.00002311
Iteration 102/1000 | Loss: 0.00002310
Iteration 103/1000 | Loss: 0.00002310
Iteration 104/1000 | Loss: 0.00002309
Iteration 105/1000 | Loss: 0.00002309
Iteration 106/1000 | Loss: 0.00002308
Iteration 107/1000 | Loss: 0.00002308
Iteration 108/1000 | Loss: 0.00002308
Iteration 109/1000 | Loss: 0.00002308
Iteration 110/1000 | Loss: 0.00002307
Iteration 111/1000 | Loss: 0.00002307
Iteration 112/1000 | Loss: 0.00002307
Iteration 113/1000 | Loss: 0.00002307
Iteration 114/1000 | Loss: 0.00002305
Iteration 115/1000 | Loss: 0.00002305
Iteration 116/1000 | Loss: 0.00002305
Iteration 117/1000 | Loss: 0.00002305
Iteration 118/1000 | Loss: 0.00002305
Iteration 119/1000 | Loss: 0.00002305
Iteration 120/1000 | Loss: 0.00002305
Iteration 121/1000 | Loss: 0.00002305
Iteration 122/1000 | Loss: 0.00002305
Iteration 123/1000 | Loss: 0.00002305
Iteration 124/1000 | Loss: 0.00002305
Iteration 125/1000 | Loss: 0.00002305
Iteration 126/1000 | Loss: 0.00002305
Iteration 127/1000 | Loss: 0.00002305
Iteration 128/1000 | Loss: 0.00002305
Iteration 129/1000 | Loss: 0.00002305
Iteration 130/1000 | Loss: 0.00002305
Iteration 131/1000 | Loss: 0.00002305
Iteration 132/1000 | Loss: 0.00002305
Iteration 133/1000 | Loss: 0.00002305
Iteration 134/1000 | Loss: 0.00002305
Iteration 135/1000 | Loss: 0.00002305
Iteration 136/1000 | Loss: 0.00002305
Iteration 137/1000 | Loss: 0.00002305
Iteration 138/1000 | Loss: 0.00002305
Iteration 139/1000 | Loss: 0.00002305
Iteration 140/1000 | Loss: 0.00002305
Iteration 141/1000 | Loss: 0.00002305
Iteration 142/1000 | Loss: 0.00002305
Iteration 143/1000 | Loss: 0.00002305
Iteration 144/1000 | Loss: 0.00002305
Iteration 145/1000 | Loss: 0.00002305
Iteration 146/1000 | Loss: 0.00002305
Iteration 147/1000 | Loss: 0.00002305
Iteration 148/1000 | Loss: 0.00002305
Iteration 149/1000 | Loss: 0.00002305
Iteration 150/1000 | Loss: 0.00002305
Iteration 151/1000 | Loss: 0.00002305
Iteration 152/1000 | Loss: 0.00002305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.304756526427809e-05, 2.304756526427809e-05, 2.304756526427809e-05, 2.304756526427809e-05, 2.304756526427809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.304756526427809e-05

Optimization complete. Final v2v error: 4.089041709899902 mm

Highest mean error: 4.378994941711426 mm for frame 149

Lowest mean error: 3.912327289581299 mm for frame 26

Saving results

Total time: 37.74503469467163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791161
Iteration 2/25 | Loss: 0.00133837
Iteration 3/25 | Loss: 0.00123892
Iteration 4/25 | Loss: 0.00122935
Iteration 5/25 | Loss: 0.00122861
Iteration 6/25 | Loss: 0.00122861
Iteration 7/25 | Loss: 0.00122861
Iteration 8/25 | Loss: 0.00122861
Iteration 9/25 | Loss: 0.00122861
Iteration 10/25 | Loss: 0.00122861
Iteration 11/25 | Loss: 0.00122861
Iteration 12/25 | Loss: 0.00122861
Iteration 13/25 | Loss: 0.00122861
Iteration 14/25 | Loss: 0.00122861
Iteration 15/25 | Loss: 0.00122861
Iteration 16/25 | Loss: 0.00122861
Iteration 17/25 | Loss: 0.00122861
Iteration 18/25 | Loss: 0.00122861
Iteration 19/25 | Loss: 0.00122861
Iteration 20/25 | Loss: 0.00122861
Iteration 21/25 | Loss: 0.00122861
Iteration 22/25 | Loss: 0.00122861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012286071432754397, 0.0012286071432754397, 0.0012286071432754397, 0.0012286071432754397, 0.0012286071432754397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012286071432754397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42204428
Iteration 2/25 | Loss: 0.00065152
Iteration 3/25 | Loss: 0.00065152
Iteration 4/25 | Loss: 0.00065152
Iteration 5/25 | Loss: 0.00065152
Iteration 6/25 | Loss: 0.00065152
Iteration 7/25 | Loss: 0.00065152
Iteration 8/25 | Loss: 0.00065152
Iteration 9/25 | Loss: 0.00065152
Iteration 10/25 | Loss: 0.00065152
Iteration 11/25 | Loss: 0.00065152
Iteration 12/25 | Loss: 0.00065152
Iteration 13/25 | Loss: 0.00065152
Iteration 14/25 | Loss: 0.00065152
Iteration 15/25 | Loss: 0.00065152
Iteration 16/25 | Loss: 0.00065152
Iteration 17/25 | Loss: 0.00065152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006515179411508143, 0.0006515179411508143, 0.0006515179411508143, 0.0006515179411508143, 0.0006515179411508143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006515179411508143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065152
Iteration 2/1000 | Loss: 0.00002444
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001680
Iteration 5/1000 | Loss: 0.00001562
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001464
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001358
Iteration 12/1000 | Loss: 0.00001340
Iteration 13/1000 | Loss: 0.00001339
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001338
Iteration 16/1000 | Loss: 0.00001337
Iteration 17/1000 | Loss: 0.00001337
Iteration 18/1000 | Loss: 0.00001337
Iteration 19/1000 | Loss: 0.00001336
Iteration 20/1000 | Loss: 0.00001334
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001334
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001333
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001304
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001295
Iteration 33/1000 | Loss: 0.00001293
Iteration 34/1000 | Loss: 0.00001293
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 38. Stopping optimization.
Last 5 losses: [1.2930054253956769e-05, 1.2930054253956769e-05, 1.2930054253956769e-05, 1.2930054253956769e-05, 1.2930054253956769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2930054253956769e-05

Optimization complete. Final v2v error: 3.0826737880706787 mm

Highest mean error: 3.1631855964660645 mm for frame 131

Lowest mean error: 2.9581093788146973 mm for frame 1

Saving results

Total time: 31.004801273345947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819898
Iteration 2/25 | Loss: 0.00139237
Iteration 3/25 | Loss: 0.00127973
Iteration 4/25 | Loss: 0.00125797
Iteration 5/25 | Loss: 0.00125427
Iteration 6/25 | Loss: 0.00125370
Iteration 7/25 | Loss: 0.00125370
Iteration 8/25 | Loss: 0.00125370
Iteration 9/25 | Loss: 0.00125370
Iteration 10/25 | Loss: 0.00125370
Iteration 11/25 | Loss: 0.00125370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012537020957097411, 0.0012537020957097411, 0.0012537020957097411, 0.0012537020957097411, 0.0012537020957097411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012537020957097411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58547759
Iteration 2/25 | Loss: 0.00081875
Iteration 3/25 | Loss: 0.00081875
Iteration 4/25 | Loss: 0.00081875
Iteration 5/25 | Loss: 0.00081875
Iteration 6/25 | Loss: 0.00081875
Iteration 7/25 | Loss: 0.00081875
Iteration 8/25 | Loss: 0.00081875
Iteration 9/25 | Loss: 0.00081875
Iteration 10/25 | Loss: 0.00081875
Iteration 11/25 | Loss: 0.00081875
Iteration 12/25 | Loss: 0.00081875
Iteration 13/25 | Loss: 0.00081875
Iteration 14/25 | Loss: 0.00081875
Iteration 15/25 | Loss: 0.00081875
Iteration 16/25 | Loss: 0.00081875
Iteration 17/25 | Loss: 0.00081875
Iteration 18/25 | Loss: 0.00081875
Iteration 19/25 | Loss: 0.00081875
Iteration 20/25 | Loss: 0.00081875
Iteration 21/25 | Loss: 0.00081875
Iteration 22/25 | Loss: 0.00081875
Iteration 23/25 | Loss: 0.00081875
Iteration 24/25 | Loss: 0.00081875
Iteration 25/25 | Loss: 0.00081875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081875
Iteration 2/1000 | Loss: 0.00005743
Iteration 3/1000 | Loss: 0.00004116
Iteration 4/1000 | Loss: 0.00003322
Iteration 5/1000 | Loss: 0.00003073
Iteration 6/1000 | Loss: 0.00002935
Iteration 7/1000 | Loss: 0.00002830
Iteration 8/1000 | Loss: 0.00002763
Iteration 9/1000 | Loss: 0.00002696
Iteration 10/1000 | Loss: 0.00002652
Iteration 11/1000 | Loss: 0.00002623
Iteration 12/1000 | Loss: 0.00002590
Iteration 13/1000 | Loss: 0.00002573
Iteration 14/1000 | Loss: 0.00002548
Iteration 15/1000 | Loss: 0.00002526
Iteration 16/1000 | Loss: 0.00002507
Iteration 17/1000 | Loss: 0.00002498
Iteration 18/1000 | Loss: 0.00002492
Iteration 19/1000 | Loss: 0.00002490
Iteration 20/1000 | Loss: 0.00002490
Iteration 21/1000 | Loss: 0.00002478
Iteration 22/1000 | Loss: 0.00002474
Iteration 23/1000 | Loss: 0.00002470
Iteration 24/1000 | Loss: 0.00002470
Iteration 25/1000 | Loss: 0.00002469
Iteration 26/1000 | Loss: 0.00002467
Iteration 27/1000 | Loss: 0.00002467
Iteration 28/1000 | Loss: 0.00002467
Iteration 29/1000 | Loss: 0.00002467
Iteration 30/1000 | Loss: 0.00002466
Iteration 31/1000 | Loss: 0.00002466
Iteration 32/1000 | Loss: 0.00002466
Iteration 33/1000 | Loss: 0.00002466
Iteration 34/1000 | Loss: 0.00002466
Iteration 35/1000 | Loss: 0.00002465
Iteration 36/1000 | Loss: 0.00002464
Iteration 37/1000 | Loss: 0.00002463
Iteration 38/1000 | Loss: 0.00002463
Iteration 39/1000 | Loss: 0.00002463
Iteration 40/1000 | Loss: 0.00002462
Iteration 41/1000 | Loss: 0.00002462
Iteration 42/1000 | Loss: 0.00002462
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002461
Iteration 45/1000 | Loss: 0.00002461
Iteration 46/1000 | Loss: 0.00002461
Iteration 47/1000 | Loss: 0.00002461
Iteration 48/1000 | Loss: 0.00002461
Iteration 49/1000 | Loss: 0.00002460
Iteration 50/1000 | Loss: 0.00002460
Iteration 51/1000 | Loss: 0.00002460
Iteration 52/1000 | Loss: 0.00002459
Iteration 53/1000 | Loss: 0.00002459
Iteration 54/1000 | Loss: 0.00002459
Iteration 55/1000 | Loss: 0.00002458
Iteration 56/1000 | Loss: 0.00002458
Iteration 57/1000 | Loss: 0.00002457
Iteration 58/1000 | Loss: 0.00002457
Iteration 59/1000 | Loss: 0.00002457
Iteration 60/1000 | Loss: 0.00002456
Iteration 61/1000 | Loss: 0.00002456
Iteration 62/1000 | Loss: 0.00002456
Iteration 63/1000 | Loss: 0.00002455
Iteration 64/1000 | Loss: 0.00002455
Iteration 65/1000 | Loss: 0.00002455
Iteration 66/1000 | Loss: 0.00002455
Iteration 67/1000 | Loss: 0.00002455
Iteration 68/1000 | Loss: 0.00002455
Iteration 69/1000 | Loss: 0.00002455
Iteration 70/1000 | Loss: 0.00002454
Iteration 71/1000 | Loss: 0.00002454
Iteration 72/1000 | Loss: 0.00002454
Iteration 73/1000 | Loss: 0.00002454
Iteration 74/1000 | Loss: 0.00002454
Iteration 75/1000 | Loss: 0.00002454
Iteration 76/1000 | Loss: 0.00002454
Iteration 77/1000 | Loss: 0.00002454
Iteration 78/1000 | Loss: 0.00002454
Iteration 79/1000 | Loss: 0.00002454
Iteration 80/1000 | Loss: 0.00002454
Iteration 81/1000 | Loss: 0.00002453
Iteration 82/1000 | Loss: 0.00002452
Iteration 83/1000 | Loss: 0.00002452
Iteration 84/1000 | Loss: 0.00002451
Iteration 85/1000 | Loss: 0.00002451
Iteration 86/1000 | Loss: 0.00002451
Iteration 87/1000 | Loss: 0.00002450
Iteration 88/1000 | Loss: 0.00002450
Iteration 89/1000 | Loss: 0.00002450
Iteration 90/1000 | Loss: 0.00002450
Iteration 91/1000 | Loss: 0.00002450
Iteration 92/1000 | Loss: 0.00002450
Iteration 93/1000 | Loss: 0.00002450
Iteration 94/1000 | Loss: 0.00002450
Iteration 95/1000 | Loss: 0.00002450
Iteration 96/1000 | Loss: 0.00002450
Iteration 97/1000 | Loss: 0.00002450
Iteration 98/1000 | Loss: 0.00002450
Iteration 99/1000 | Loss: 0.00002449
Iteration 100/1000 | Loss: 0.00002449
Iteration 101/1000 | Loss: 0.00002449
Iteration 102/1000 | Loss: 0.00002449
Iteration 103/1000 | Loss: 0.00002449
Iteration 104/1000 | Loss: 0.00002448
Iteration 105/1000 | Loss: 0.00002448
Iteration 106/1000 | Loss: 0.00002448
Iteration 107/1000 | Loss: 0.00002448
Iteration 108/1000 | Loss: 0.00002448
Iteration 109/1000 | Loss: 0.00002448
Iteration 110/1000 | Loss: 0.00002447
Iteration 111/1000 | Loss: 0.00002447
Iteration 112/1000 | Loss: 0.00002447
Iteration 113/1000 | Loss: 0.00002447
Iteration 114/1000 | Loss: 0.00002447
Iteration 115/1000 | Loss: 0.00002447
Iteration 116/1000 | Loss: 0.00002447
Iteration 117/1000 | Loss: 0.00002447
Iteration 118/1000 | Loss: 0.00002447
Iteration 119/1000 | Loss: 0.00002447
Iteration 120/1000 | Loss: 0.00002447
Iteration 121/1000 | Loss: 0.00002446
Iteration 122/1000 | Loss: 0.00002446
Iteration 123/1000 | Loss: 0.00002446
Iteration 124/1000 | Loss: 0.00002446
Iteration 125/1000 | Loss: 0.00002446
Iteration 126/1000 | Loss: 0.00002445
Iteration 127/1000 | Loss: 0.00002445
Iteration 128/1000 | Loss: 0.00002445
Iteration 129/1000 | Loss: 0.00002445
Iteration 130/1000 | Loss: 0.00002445
Iteration 131/1000 | Loss: 0.00002445
Iteration 132/1000 | Loss: 0.00002445
Iteration 133/1000 | Loss: 0.00002444
Iteration 134/1000 | Loss: 0.00002444
Iteration 135/1000 | Loss: 0.00002444
Iteration 136/1000 | Loss: 0.00002444
Iteration 137/1000 | Loss: 0.00002444
Iteration 138/1000 | Loss: 0.00002444
Iteration 139/1000 | Loss: 0.00002443
Iteration 140/1000 | Loss: 0.00002443
Iteration 141/1000 | Loss: 0.00002443
Iteration 142/1000 | Loss: 0.00002443
Iteration 143/1000 | Loss: 0.00002443
Iteration 144/1000 | Loss: 0.00002443
Iteration 145/1000 | Loss: 0.00002442
Iteration 146/1000 | Loss: 0.00002442
Iteration 147/1000 | Loss: 0.00002442
Iteration 148/1000 | Loss: 0.00002442
Iteration 149/1000 | Loss: 0.00002442
Iteration 150/1000 | Loss: 0.00002442
Iteration 151/1000 | Loss: 0.00002442
Iteration 152/1000 | Loss: 0.00002441
Iteration 153/1000 | Loss: 0.00002441
Iteration 154/1000 | Loss: 0.00002441
Iteration 155/1000 | Loss: 0.00002441
Iteration 156/1000 | Loss: 0.00002441
Iteration 157/1000 | Loss: 0.00002441
Iteration 158/1000 | Loss: 0.00002440
Iteration 159/1000 | Loss: 0.00002440
Iteration 160/1000 | Loss: 0.00002440
Iteration 161/1000 | Loss: 0.00002440
Iteration 162/1000 | Loss: 0.00002440
Iteration 163/1000 | Loss: 0.00002440
Iteration 164/1000 | Loss: 0.00002440
Iteration 165/1000 | Loss: 0.00002439
Iteration 166/1000 | Loss: 0.00002439
Iteration 167/1000 | Loss: 0.00002439
Iteration 168/1000 | Loss: 0.00002439
Iteration 169/1000 | Loss: 0.00002439
Iteration 170/1000 | Loss: 0.00002438
Iteration 171/1000 | Loss: 0.00002438
Iteration 172/1000 | Loss: 0.00002438
Iteration 173/1000 | Loss: 0.00002438
Iteration 174/1000 | Loss: 0.00002438
Iteration 175/1000 | Loss: 0.00002437
Iteration 176/1000 | Loss: 0.00002437
Iteration 177/1000 | Loss: 0.00002437
Iteration 178/1000 | Loss: 0.00002437
Iteration 179/1000 | Loss: 0.00002437
Iteration 180/1000 | Loss: 0.00002437
Iteration 181/1000 | Loss: 0.00002437
Iteration 182/1000 | Loss: 0.00002436
Iteration 183/1000 | Loss: 0.00002436
Iteration 184/1000 | Loss: 0.00002436
Iteration 185/1000 | Loss: 0.00002436
Iteration 186/1000 | Loss: 0.00002436
Iteration 187/1000 | Loss: 0.00002436
Iteration 188/1000 | Loss: 0.00002436
Iteration 189/1000 | Loss: 0.00002435
Iteration 190/1000 | Loss: 0.00002435
Iteration 191/1000 | Loss: 0.00002435
Iteration 192/1000 | Loss: 0.00002435
Iteration 193/1000 | Loss: 0.00002435
Iteration 194/1000 | Loss: 0.00002435
Iteration 195/1000 | Loss: 0.00002435
Iteration 196/1000 | Loss: 0.00002435
Iteration 197/1000 | Loss: 0.00002435
Iteration 198/1000 | Loss: 0.00002435
Iteration 199/1000 | Loss: 0.00002435
Iteration 200/1000 | Loss: 0.00002435
Iteration 201/1000 | Loss: 0.00002435
Iteration 202/1000 | Loss: 0.00002435
Iteration 203/1000 | Loss: 0.00002434
Iteration 204/1000 | Loss: 0.00002434
Iteration 205/1000 | Loss: 0.00002434
Iteration 206/1000 | Loss: 0.00002434
Iteration 207/1000 | Loss: 0.00002434
Iteration 208/1000 | Loss: 0.00002434
Iteration 209/1000 | Loss: 0.00002434
Iteration 210/1000 | Loss: 0.00002434
Iteration 211/1000 | Loss: 0.00002434
Iteration 212/1000 | Loss: 0.00002434
Iteration 213/1000 | Loss: 0.00002434
Iteration 214/1000 | Loss: 0.00002434
Iteration 215/1000 | Loss: 0.00002434
Iteration 216/1000 | Loss: 0.00002433
Iteration 217/1000 | Loss: 0.00002433
Iteration 218/1000 | Loss: 0.00002433
Iteration 219/1000 | Loss: 0.00002433
Iteration 220/1000 | Loss: 0.00002433
Iteration 221/1000 | Loss: 0.00002433
Iteration 222/1000 | Loss: 0.00002433
Iteration 223/1000 | Loss: 0.00002433
Iteration 224/1000 | Loss: 0.00002433
Iteration 225/1000 | Loss: 0.00002433
Iteration 226/1000 | Loss: 0.00002433
Iteration 227/1000 | Loss: 0.00002433
Iteration 228/1000 | Loss: 0.00002433
Iteration 229/1000 | Loss: 0.00002433
Iteration 230/1000 | Loss: 0.00002433
Iteration 231/1000 | Loss: 0.00002433
Iteration 232/1000 | Loss: 0.00002433
Iteration 233/1000 | Loss: 0.00002432
Iteration 234/1000 | Loss: 0.00002432
Iteration 235/1000 | Loss: 0.00002432
Iteration 236/1000 | Loss: 0.00002432
Iteration 237/1000 | Loss: 0.00002432
Iteration 238/1000 | Loss: 0.00002432
Iteration 239/1000 | Loss: 0.00002432
Iteration 240/1000 | Loss: 0.00002432
Iteration 241/1000 | Loss: 0.00002432
Iteration 242/1000 | Loss: 0.00002432
Iteration 243/1000 | Loss: 0.00002432
Iteration 244/1000 | Loss: 0.00002432
Iteration 245/1000 | Loss: 0.00002432
Iteration 246/1000 | Loss: 0.00002432
Iteration 247/1000 | Loss: 0.00002432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [2.431959546811413e-05, 2.431959546811413e-05, 2.431959546811413e-05, 2.431959546811413e-05, 2.431959546811413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.431959546811413e-05

Optimization complete. Final v2v error: 4.032532691955566 mm

Highest mean error: 6.179235935211182 mm for frame 59

Lowest mean error: 2.933238983154297 mm for frame 46

Saving results

Total time: 47.80599045753479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386129
Iteration 2/25 | Loss: 0.00130141
Iteration 3/25 | Loss: 0.00124467
Iteration 4/25 | Loss: 0.00124009
Iteration 5/25 | Loss: 0.00123886
Iteration 6/25 | Loss: 0.00123852
Iteration 7/25 | Loss: 0.00123852
Iteration 8/25 | Loss: 0.00123852
Iteration 9/25 | Loss: 0.00123852
Iteration 10/25 | Loss: 0.00123852
Iteration 11/25 | Loss: 0.00123852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001238523400388658, 0.001238523400388658, 0.001238523400388658, 0.001238523400388658, 0.001238523400388658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001238523400388658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72584224
Iteration 2/25 | Loss: 0.00086609
Iteration 3/25 | Loss: 0.00086609
Iteration 4/25 | Loss: 0.00086609
Iteration 5/25 | Loss: 0.00086608
Iteration 6/25 | Loss: 0.00086608
Iteration 7/25 | Loss: 0.00086608
Iteration 8/25 | Loss: 0.00086608
Iteration 9/25 | Loss: 0.00086608
Iteration 10/25 | Loss: 0.00086608
Iteration 11/25 | Loss: 0.00086608
Iteration 12/25 | Loss: 0.00086608
Iteration 13/25 | Loss: 0.00086608
Iteration 14/25 | Loss: 0.00086608
Iteration 15/25 | Loss: 0.00086608
Iteration 16/25 | Loss: 0.00086608
Iteration 17/25 | Loss: 0.00086608
Iteration 18/25 | Loss: 0.00086608
Iteration 19/25 | Loss: 0.00086608
Iteration 20/25 | Loss: 0.00086608
Iteration 21/25 | Loss: 0.00086608
Iteration 22/25 | Loss: 0.00086608
Iteration 23/25 | Loss: 0.00086608
Iteration 24/25 | Loss: 0.00086608
Iteration 25/25 | Loss: 0.00086608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086608
Iteration 2/1000 | Loss: 0.00002578
Iteration 3/1000 | Loss: 0.00001632
Iteration 4/1000 | Loss: 0.00001420
Iteration 5/1000 | Loss: 0.00001332
Iteration 6/1000 | Loss: 0.00001271
Iteration 7/1000 | Loss: 0.00001228
Iteration 8/1000 | Loss: 0.00001211
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001204
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001180
Iteration 13/1000 | Loss: 0.00001176
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001161
Iteration 19/1000 | Loss: 0.00001161
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001156
Iteration 22/1000 | Loss: 0.00001155
Iteration 23/1000 | Loss: 0.00001154
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001153
Iteration 26/1000 | Loss: 0.00001153
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001151
Iteration 30/1000 | Loss: 0.00001151
Iteration 31/1000 | Loss: 0.00001150
Iteration 32/1000 | Loss: 0.00001150
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001145
Iteration 41/1000 | Loss: 0.00001145
Iteration 42/1000 | Loss: 0.00001144
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001144
Iteration 46/1000 | Loss: 0.00001144
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001143
Iteration 55/1000 | Loss: 0.00001143
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001142
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001141
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001141
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001140
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001139
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001139
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001139
Iteration 81/1000 | Loss: 0.00001139
Iteration 82/1000 | Loss: 0.00001139
Iteration 83/1000 | Loss: 0.00001139
Iteration 84/1000 | Loss: 0.00001139
Iteration 85/1000 | Loss: 0.00001139
Iteration 86/1000 | Loss: 0.00001139
Iteration 87/1000 | Loss: 0.00001139
Iteration 88/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.1387228369130753e-05, 1.1387228369130753e-05, 1.1387228369130753e-05, 1.1387228369130753e-05, 1.1387228369130753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1387228369130753e-05

Optimization complete. Final v2v error: 2.9104092121124268 mm

Highest mean error: 3.2833080291748047 mm for frame 84

Lowest mean error: 2.8109023571014404 mm for frame 164

Saving results

Total time: 30.53606390953064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951730
Iteration 2/25 | Loss: 0.00951729
Iteration 3/25 | Loss: 0.00951729
Iteration 4/25 | Loss: 0.00411104
Iteration 5/25 | Loss: 0.00237159
Iteration 6/25 | Loss: 0.00216206
Iteration 7/25 | Loss: 0.00214564
Iteration 8/25 | Loss: 0.00194438
Iteration 9/25 | Loss: 0.00181917
Iteration 10/25 | Loss: 0.00177354
Iteration 11/25 | Loss: 0.00172776
Iteration 12/25 | Loss: 0.00168667
Iteration 13/25 | Loss: 0.00164816
Iteration 14/25 | Loss: 0.00164097
Iteration 15/25 | Loss: 0.00161141
Iteration 16/25 | Loss: 0.00156076
Iteration 17/25 | Loss: 0.00153651
Iteration 18/25 | Loss: 0.00153069
Iteration 19/25 | Loss: 0.00151991
Iteration 20/25 | Loss: 0.00150591
Iteration 21/25 | Loss: 0.00149849
Iteration 22/25 | Loss: 0.00149652
Iteration 23/25 | Loss: 0.00149471
Iteration 24/25 | Loss: 0.00148979
Iteration 25/25 | Loss: 0.00148916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40533996
Iteration 2/25 | Loss: 0.00167534
Iteration 3/25 | Loss: 0.00165000
Iteration 4/25 | Loss: 0.00165000
Iteration 5/25 | Loss: 0.00165000
Iteration 6/25 | Loss: 0.00165000
Iteration 7/25 | Loss: 0.00165000
Iteration 8/25 | Loss: 0.00165000
Iteration 9/25 | Loss: 0.00165000
Iteration 10/25 | Loss: 0.00165000
Iteration 11/25 | Loss: 0.00165000
Iteration 12/25 | Loss: 0.00165000
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016500005731359124, 0.0016500005731359124, 0.0016500005731359124, 0.0016500005731359124, 0.0016500005731359124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016500005731359124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165000
Iteration 2/1000 | Loss: 0.00027295
Iteration 3/1000 | Loss: 0.00018157
Iteration 4/1000 | Loss: 0.00015377
Iteration 5/1000 | Loss: 0.00013688
Iteration 6/1000 | Loss: 0.00012882
Iteration 7/1000 | Loss: 0.00025650
Iteration 8/1000 | Loss: 0.00075480
Iteration 9/1000 | Loss: 0.00076816
Iteration 10/1000 | Loss: 0.00042915
Iteration 11/1000 | Loss: 0.00050748
Iteration 12/1000 | Loss: 0.00012468
Iteration 13/1000 | Loss: 0.00015797
Iteration 14/1000 | Loss: 0.00009084
Iteration 15/1000 | Loss: 0.00009191
Iteration 16/1000 | Loss: 0.00009475
Iteration 17/1000 | Loss: 0.00019034
Iteration 18/1000 | Loss: 0.00005250
Iteration 19/1000 | Loss: 0.00003215
Iteration 20/1000 | Loss: 0.00005939
Iteration 21/1000 | Loss: 0.00005302
Iteration 22/1000 | Loss: 0.00006057
Iteration 23/1000 | Loss: 0.00002855
Iteration 24/1000 | Loss: 0.00002518
Iteration 25/1000 | Loss: 0.00002141
Iteration 26/1000 | Loss: 0.00002069
Iteration 27/1000 | Loss: 0.00002035
Iteration 28/1000 | Loss: 0.00002007
Iteration 29/1000 | Loss: 0.00001986
Iteration 30/1000 | Loss: 0.00001966
Iteration 31/1000 | Loss: 0.00001962
Iteration 32/1000 | Loss: 0.00001958
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001945
Iteration 36/1000 | Loss: 0.00001945
Iteration 37/1000 | Loss: 0.00001943
Iteration 38/1000 | Loss: 0.00001942
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001940
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001936
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001936
Iteration 64/1000 | Loss: 0.00001936
Iteration 65/1000 | Loss: 0.00001936
Iteration 66/1000 | Loss: 0.00001936
Iteration 67/1000 | Loss: 0.00001936
Iteration 68/1000 | Loss: 0.00001936
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001936
Iteration 71/1000 | Loss: 0.00001936
Iteration 72/1000 | Loss: 0.00001936
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001936
Iteration 75/1000 | Loss: 0.00001936
Iteration 76/1000 | Loss: 0.00001936
Iteration 77/1000 | Loss: 0.00001936
Iteration 78/1000 | Loss: 0.00001936
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001936
Iteration 86/1000 | Loss: 0.00001936
Iteration 87/1000 | Loss: 0.00001936
Iteration 88/1000 | Loss: 0.00001936
Iteration 89/1000 | Loss: 0.00001936
Iteration 90/1000 | Loss: 0.00001936
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.935997534019407e-05, 1.935997534019407e-05, 1.935997534019407e-05, 1.935997534019407e-05, 1.935997534019407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.935997534019407e-05

Optimization complete. Final v2v error: 3.7397656440734863 mm

Highest mean error: 4.125368118286133 mm for frame 97

Lowest mean error: 3.3888285160064697 mm for frame 170

Saving results

Total time: 99.01526069641113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421378
Iteration 2/25 | Loss: 0.00157581
Iteration 3/25 | Loss: 0.00132216
Iteration 4/25 | Loss: 0.00128410
Iteration 5/25 | Loss: 0.00127712
Iteration 6/25 | Loss: 0.00127547
Iteration 7/25 | Loss: 0.00127531
Iteration 8/25 | Loss: 0.00127531
Iteration 9/25 | Loss: 0.00127531
Iteration 10/25 | Loss: 0.00127531
Iteration 11/25 | Loss: 0.00127531
Iteration 12/25 | Loss: 0.00127531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012753106420859694, 0.0012753106420859694, 0.0012753106420859694, 0.0012753106420859694, 0.0012753106420859694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012753106420859694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39829588
Iteration 2/25 | Loss: 0.00069720
Iteration 3/25 | Loss: 0.00069720
Iteration 4/25 | Loss: 0.00069720
Iteration 5/25 | Loss: 0.00069720
Iteration 6/25 | Loss: 0.00069720
Iteration 7/25 | Loss: 0.00069720
Iteration 8/25 | Loss: 0.00069720
Iteration 9/25 | Loss: 0.00069719
Iteration 10/25 | Loss: 0.00069719
Iteration 11/25 | Loss: 0.00069719
Iteration 12/25 | Loss: 0.00069719
Iteration 13/25 | Loss: 0.00069719
Iteration 14/25 | Loss: 0.00069719
Iteration 15/25 | Loss: 0.00069719
Iteration 16/25 | Loss: 0.00069719
Iteration 17/25 | Loss: 0.00069719
Iteration 18/25 | Loss: 0.00069719
Iteration 19/25 | Loss: 0.00069719
Iteration 20/25 | Loss: 0.00069719
Iteration 21/25 | Loss: 0.00069719
Iteration 22/25 | Loss: 0.00069719
Iteration 23/25 | Loss: 0.00069719
Iteration 24/25 | Loss: 0.00069719
Iteration 25/25 | Loss: 0.00069719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069719
Iteration 2/1000 | Loss: 0.00005410
Iteration 3/1000 | Loss: 0.00003094
Iteration 4/1000 | Loss: 0.00002333
Iteration 5/1000 | Loss: 0.00002138
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00001942
Iteration 8/1000 | Loss: 0.00001877
Iteration 9/1000 | Loss: 0.00001824
Iteration 10/1000 | Loss: 0.00001797
Iteration 11/1000 | Loss: 0.00001784
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001734
Iteration 17/1000 | Loss: 0.00001733
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001733
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001730
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001727
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001712
Iteration 30/1000 | Loss: 0.00001711
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001706
Iteration 37/1000 | Loss: 0.00001704
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00001702
Iteration 40/1000 | Loss: 0.00001702
Iteration 41/1000 | Loss: 0.00001702
Iteration 42/1000 | Loss: 0.00001702
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001701
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001701
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001700
Iteration 56/1000 | Loss: 0.00001700
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001698
Iteration 65/1000 | Loss: 0.00001698
Iteration 66/1000 | Loss: 0.00001698
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001696
Iteration 71/1000 | Loss: 0.00001696
Iteration 72/1000 | Loss: 0.00001696
Iteration 73/1000 | Loss: 0.00001696
Iteration 74/1000 | Loss: 0.00001696
Iteration 75/1000 | Loss: 0.00001696
Iteration 76/1000 | Loss: 0.00001696
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001695
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001693
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001693
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001692
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001689
Iteration 103/1000 | Loss: 0.00001689
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001687
Iteration 109/1000 | Loss: 0.00001686
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00001686
Iteration 114/1000 | Loss: 0.00001686
Iteration 115/1000 | Loss: 0.00001686
Iteration 116/1000 | Loss: 0.00001686
Iteration 117/1000 | Loss: 0.00001686
Iteration 118/1000 | Loss: 0.00001685
Iteration 119/1000 | Loss: 0.00001684
Iteration 120/1000 | Loss: 0.00001684
Iteration 121/1000 | Loss: 0.00001684
Iteration 122/1000 | Loss: 0.00001684
Iteration 123/1000 | Loss: 0.00001684
Iteration 124/1000 | Loss: 0.00001684
Iteration 125/1000 | Loss: 0.00001684
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001683
Iteration 128/1000 | Loss: 0.00001683
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001682
Iteration 131/1000 | Loss: 0.00001682
Iteration 132/1000 | Loss: 0.00001682
Iteration 133/1000 | Loss: 0.00001681
Iteration 134/1000 | Loss: 0.00001681
Iteration 135/1000 | Loss: 0.00001681
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001680
Iteration 139/1000 | Loss: 0.00001680
Iteration 140/1000 | Loss: 0.00001680
Iteration 141/1000 | Loss: 0.00001680
Iteration 142/1000 | Loss: 0.00001680
Iteration 143/1000 | Loss: 0.00001680
Iteration 144/1000 | Loss: 0.00001680
Iteration 145/1000 | Loss: 0.00001680
Iteration 146/1000 | Loss: 0.00001680
Iteration 147/1000 | Loss: 0.00001679
Iteration 148/1000 | Loss: 0.00001679
Iteration 149/1000 | Loss: 0.00001679
Iteration 150/1000 | Loss: 0.00001679
Iteration 151/1000 | Loss: 0.00001679
Iteration 152/1000 | Loss: 0.00001679
Iteration 153/1000 | Loss: 0.00001679
Iteration 154/1000 | Loss: 0.00001679
Iteration 155/1000 | Loss: 0.00001679
Iteration 156/1000 | Loss: 0.00001679
Iteration 157/1000 | Loss: 0.00001679
Iteration 158/1000 | Loss: 0.00001679
Iteration 159/1000 | Loss: 0.00001678
Iteration 160/1000 | Loss: 0.00001678
Iteration 161/1000 | Loss: 0.00001678
Iteration 162/1000 | Loss: 0.00001678
Iteration 163/1000 | Loss: 0.00001678
Iteration 164/1000 | Loss: 0.00001678
Iteration 165/1000 | Loss: 0.00001678
Iteration 166/1000 | Loss: 0.00001678
Iteration 167/1000 | Loss: 0.00001677
Iteration 168/1000 | Loss: 0.00001677
Iteration 169/1000 | Loss: 0.00001677
Iteration 170/1000 | Loss: 0.00001677
Iteration 171/1000 | Loss: 0.00001677
Iteration 172/1000 | Loss: 0.00001677
Iteration 173/1000 | Loss: 0.00001677
Iteration 174/1000 | Loss: 0.00001677
Iteration 175/1000 | Loss: 0.00001677
Iteration 176/1000 | Loss: 0.00001677
Iteration 177/1000 | Loss: 0.00001676
Iteration 178/1000 | Loss: 0.00001676
Iteration 179/1000 | Loss: 0.00001676
Iteration 180/1000 | Loss: 0.00001676
Iteration 181/1000 | Loss: 0.00001676
Iteration 182/1000 | Loss: 0.00001676
Iteration 183/1000 | Loss: 0.00001676
Iteration 184/1000 | Loss: 0.00001676
Iteration 185/1000 | Loss: 0.00001675
Iteration 186/1000 | Loss: 0.00001675
Iteration 187/1000 | Loss: 0.00001675
Iteration 188/1000 | Loss: 0.00001675
Iteration 189/1000 | Loss: 0.00001675
Iteration 190/1000 | Loss: 0.00001675
Iteration 191/1000 | Loss: 0.00001675
Iteration 192/1000 | Loss: 0.00001675
Iteration 193/1000 | Loss: 0.00001675
Iteration 194/1000 | Loss: 0.00001675
Iteration 195/1000 | Loss: 0.00001675
Iteration 196/1000 | Loss: 0.00001675
Iteration 197/1000 | Loss: 0.00001674
Iteration 198/1000 | Loss: 0.00001674
Iteration 199/1000 | Loss: 0.00001674
Iteration 200/1000 | Loss: 0.00001674
Iteration 201/1000 | Loss: 0.00001674
Iteration 202/1000 | Loss: 0.00001674
Iteration 203/1000 | Loss: 0.00001674
Iteration 204/1000 | Loss: 0.00001674
Iteration 205/1000 | Loss: 0.00001673
Iteration 206/1000 | Loss: 0.00001673
Iteration 207/1000 | Loss: 0.00001673
Iteration 208/1000 | Loss: 0.00001673
Iteration 209/1000 | Loss: 0.00001673
Iteration 210/1000 | Loss: 0.00001673
Iteration 211/1000 | Loss: 0.00001673
Iteration 212/1000 | Loss: 0.00001673
Iteration 213/1000 | Loss: 0.00001673
Iteration 214/1000 | Loss: 0.00001673
Iteration 215/1000 | Loss: 0.00001673
Iteration 216/1000 | Loss: 0.00001673
Iteration 217/1000 | Loss: 0.00001672
Iteration 218/1000 | Loss: 0.00001672
Iteration 219/1000 | Loss: 0.00001672
Iteration 220/1000 | Loss: 0.00001672
Iteration 221/1000 | Loss: 0.00001672
Iteration 222/1000 | Loss: 0.00001672
Iteration 223/1000 | Loss: 0.00001672
Iteration 224/1000 | Loss: 0.00001672
Iteration 225/1000 | Loss: 0.00001672
Iteration 226/1000 | Loss: 0.00001672
Iteration 227/1000 | Loss: 0.00001672
Iteration 228/1000 | Loss: 0.00001672
Iteration 229/1000 | Loss: 0.00001672
Iteration 230/1000 | Loss: 0.00001672
Iteration 231/1000 | Loss: 0.00001672
Iteration 232/1000 | Loss: 0.00001672
Iteration 233/1000 | Loss: 0.00001672
Iteration 234/1000 | Loss: 0.00001672
Iteration 235/1000 | Loss: 0.00001672
Iteration 236/1000 | Loss: 0.00001672
Iteration 237/1000 | Loss: 0.00001672
Iteration 238/1000 | Loss: 0.00001672
Iteration 239/1000 | Loss: 0.00001672
Iteration 240/1000 | Loss: 0.00001672
Iteration 241/1000 | Loss: 0.00001672
Iteration 242/1000 | Loss: 0.00001672
Iteration 243/1000 | Loss: 0.00001672
Iteration 244/1000 | Loss: 0.00001672
Iteration 245/1000 | Loss: 0.00001672
Iteration 246/1000 | Loss: 0.00001672
Iteration 247/1000 | Loss: 0.00001672
Iteration 248/1000 | Loss: 0.00001672
Iteration 249/1000 | Loss: 0.00001672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.672309372224845e-05, 1.672309372224845e-05, 1.672309372224845e-05, 1.672309372224845e-05, 1.672309372224845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.672309372224845e-05

Optimization complete. Final v2v error: 3.473038911819458 mm

Highest mean error: 4.2094855308532715 mm for frame 72

Lowest mean error: 2.998549222946167 mm for frame 13

Saving results

Total time: 43.97612500190735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397406
Iteration 2/25 | Loss: 0.00136128
Iteration 3/25 | Loss: 0.00126043
Iteration 4/25 | Loss: 0.00124175
Iteration 5/25 | Loss: 0.00123546
Iteration 6/25 | Loss: 0.00123451
Iteration 7/25 | Loss: 0.00123451
Iteration 8/25 | Loss: 0.00123451
Iteration 9/25 | Loss: 0.00123451
Iteration 10/25 | Loss: 0.00123451
Iteration 11/25 | Loss: 0.00123451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001234514289535582, 0.001234514289535582, 0.001234514289535582, 0.001234514289535582, 0.001234514289535582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001234514289535582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41384482
Iteration 2/25 | Loss: 0.00083577
Iteration 3/25 | Loss: 0.00083577
Iteration 4/25 | Loss: 0.00083577
Iteration 5/25 | Loss: 0.00083577
Iteration 6/25 | Loss: 0.00083577
Iteration 7/25 | Loss: 0.00083577
Iteration 8/25 | Loss: 0.00083577
Iteration 9/25 | Loss: 0.00083577
Iteration 10/25 | Loss: 0.00083577
Iteration 11/25 | Loss: 0.00083577
Iteration 12/25 | Loss: 0.00083577
Iteration 13/25 | Loss: 0.00083577
Iteration 14/25 | Loss: 0.00083577
Iteration 15/25 | Loss: 0.00083577
Iteration 16/25 | Loss: 0.00083577
Iteration 17/25 | Loss: 0.00083577
Iteration 18/25 | Loss: 0.00083577
Iteration 19/25 | Loss: 0.00083577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008357710321433842, 0.0008357710321433842, 0.0008357710321433842, 0.0008357710321433842, 0.0008357710321433842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008357710321433842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083577
Iteration 2/1000 | Loss: 0.00003865
Iteration 3/1000 | Loss: 0.00002702
Iteration 4/1000 | Loss: 0.00002455
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002177
Iteration 7/1000 | Loss: 0.00002096
Iteration 8/1000 | Loss: 0.00002047
Iteration 9/1000 | Loss: 0.00002007
Iteration 10/1000 | Loss: 0.00001965
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001913
Iteration 13/1000 | Loss: 0.00001899
Iteration 14/1000 | Loss: 0.00001896
Iteration 15/1000 | Loss: 0.00001892
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001889
Iteration 19/1000 | Loss: 0.00001889
Iteration 20/1000 | Loss: 0.00001889
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001888
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001886
Iteration 26/1000 | Loss: 0.00001883
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001877
Iteration 29/1000 | Loss: 0.00001877
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001868
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001866
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001866
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001864
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001863
Iteration 50/1000 | Loss: 0.00001863
Iteration 51/1000 | Loss: 0.00001863
Iteration 52/1000 | Loss: 0.00001862
Iteration 53/1000 | Loss: 0.00001862
Iteration 54/1000 | Loss: 0.00001862
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001859
Iteration 60/1000 | Loss: 0.00001859
Iteration 61/1000 | Loss: 0.00001859
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001857
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001856
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001855
Iteration 85/1000 | Loss: 0.00001855
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001855
Iteration 89/1000 | Loss: 0.00001855
Iteration 90/1000 | Loss: 0.00001855
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001855
Iteration 97/1000 | Loss: 0.00001855
Iteration 98/1000 | Loss: 0.00001855
Iteration 99/1000 | Loss: 0.00001855
Iteration 100/1000 | Loss: 0.00001855
Iteration 101/1000 | Loss: 0.00001855
Iteration 102/1000 | Loss: 0.00001855
Iteration 103/1000 | Loss: 0.00001855
Iteration 104/1000 | Loss: 0.00001854
Iteration 105/1000 | Loss: 0.00001854
Iteration 106/1000 | Loss: 0.00001854
Iteration 107/1000 | Loss: 0.00001854
Iteration 108/1000 | Loss: 0.00001854
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001854
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001853
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001852
Iteration 117/1000 | Loss: 0.00001852
Iteration 118/1000 | Loss: 0.00001852
Iteration 119/1000 | Loss: 0.00001852
Iteration 120/1000 | Loss: 0.00001852
Iteration 121/1000 | Loss: 0.00001852
Iteration 122/1000 | Loss: 0.00001852
Iteration 123/1000 | Loss: 0.00001852
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001852
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001852
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001851
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001851
Iteration 135/1000 | Loss: 0.00001851
Iteration 136/1000 | Loss: 0.00001851
Iteration 137/1000 | Loss: 0.00001851
Iteration 138/1000 | Loss: 0.00001851
Iteration 139/1000 | Loss: 0.00001851
Iteration 140/1000 | Loss: 0.00001851
Iteration 141/1000 | Loss: 0.00001851
Iteration 142/1000 | Loss: 0.00001851
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001850
Iteration 146/1000 | Loss: 0.00001850
Iteration 147/1000 | Loss: 0.00001850
Iteration 148/1000 | Loss: 0.00001850
Iteration 149/1000 | Loss: 0.00001850
Iteration 150/1000 | Loss: 0.00001850
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001849
Iteration 158/1000 | Loss: 0.00001849
Iteration 159/1000 | Loss: 0.00001849
Iteration 160/1000 | Loss: 0.00001849
Iteration 161/1000 | Loss: 0.00001849
Iteration 162/1000 | Loss: 0.00001849
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001848
Iteration 168/1000 | Loss: 0.00001848
Iteration 169/1000 | Loss: 0.00001848
Iteration 170/1000 | Loss: 0.00001848
Iteration 171/1000 | Loss: 0.00001848
Iteration 172/1000 | Loss: 0.00001848
Iteration 173/1000 | Loss: 0.00001848
Iteration 174/1000 | Loss: 0.00001848
Iteration 175/1000 | Loss: 0.00001848
Iteration 176/1000 | Loss: 0.00001847
Iteration 177/1000 | Loss: 0.00001847
Iteration 178/1000 | Loss: 0.00001847
Iteration 179/1000 | Loss: 0.00001847
Iteration 180/1000 | Loss: 0.00001847
Iteration 181/1000 | Loss: 0.00001847
Iteration 182/1000 | Loss: 0.00001847
Iteration 183/1000 | Loss: 0.00001846
Iteration 184/1000 | Loss: 0.00001846
Iteration 185/1000 | Loss: 0.00001846
Iteration 186/1000 | Loss: 0.00001846
Iteration 187/1000 | Loss: 0.00001846
Iteration 188/1000 | Loss: 0.00001845
Iteration 189/1000 | Loss: 0.00001845
Iteration 190/1000 | Loss: 0.00001845
Iteration 191/1000 | Loss: 0.00001845
Iteration 192/1000 | Loss: 0.00001845
Iteration 193/1000 | Loss: 0.00001844
Iteration 194/1000 | Loss: 0.00001844
Iteration 195/1000 | Loss: 0.00001844
Iteration 196/1000 | Loss: 0.00001844
Iteration 197/1000 | Loss: 0.00001844
Iteration 198/1000 | Loss: 0.00001844
Iteration 199/1000 | Loss: 0.00001844
Iteration 200/1000 | Loss: 0.00001844
Iteration 201/1000 | Loss: 0.00001844
Iteration 202/1000 | Loss: 0.00001844
Iteration 203/1000 | Loss: 0.00001844
Iteration 204/1000 | Loss: 0.00001843
Iteration 205/1000 | Loss: 0.00001843
Iteration 206/1000 | Loss: 0.00001843
Iteration 207/1000 | Loss: 0.00001843
Iteration 208/1000 | Loss: 0.00001843
Iteration 209/1000 | Loss: 0.00001843
Iteration 210/1000 | Loss: 0.00001843
Iteration 211/1000 | Loss: 0.00001843
Iteration 212/1000 | Loss: 0.00001843
Iteration 213/1000 | Loss: 0.00001843
Iteration 214/1000 | Loss: 0.00001842
Iteration 215/1000 | Loss: 0.00001842
Iteration 216/1000 | Loss: 0.00001842
Iteration 217/1000 | Loss: 0.00001842
Iteration 218/1000 | Loss: 0.00001842
Iteration 219/1000 | Loss: 0.00001842
Iteration 220/1000 | Loss: 0.00001842
Iteration 221/1000 | Loss: 0.00001842
Iteration 222/1000 | Loss: 0.00001842
Iteration 223/1000 | Loss: 0.00001842
Iteration 224/1000 | Loss: 0.00001842
Iteration 225/1000 | Loss: 0.00001842
Iteration 226/1000 | Loss: 0.00001842
Iteration 227/1000 | Loss: 0.00001842
Iteration 228/1000 | Loss: 0.00001842
Iteration 229/1000 | Loss: 0.00001842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.842000892793294e-05, 1.842000892793294e-05, 1.842000892793294e-05, 1.842000892793294e-05, 1.842000892793294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.842000892793294e-05

Optimization complete. Final v2v error: 3.59206223487854 mm

Highest mean error: 4.492429733276367 mm for frame 96

Lowest mean error: 3.0827136039733887 mm for frame 72

Saving results

Total time: 48.291627168655396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515340
Iteration 2/25 | Loss: 0.00150597
Iteration 3/25 | Loss: 0.00134672
Iteration 4/25 | Loss: 0.00132865
Iteration 5/25 | Loss: 0.00132077
Iteration 6/25 | Loss: 0.00132063
Iteration 7/25 | Loss: 0.00132063
Iteration 8/25 | Loss: 0.00132063
Iteration 9/25 | Loss: 0.00132063
Iteration 10/25 | Loss: 0.00132063
Iteration 11/25 | Loss: 0.00132063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013206318253651261, 0.0013206318253651261, 0.0013206318253651261, 0.0013206318253651261, 0.0013206318253651261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013206318253651261

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77517432
Iteration 2/25 | Loss: 0.00083227
Iteration 3/25 | Loss: 0.00083227
Iteration 4/25 | Loss: 0.00083227
Iteration 5/25 | Loss: 0.00083227
Iteration 6/25 | Loss: 0.00083227
Iteration 7/25 | Loss: 0.00083227
Iteration 8/25 | Loss: 0.00083227
Iteration 9/25 | Loss: 0.00083227
Iteration 10/25 | Loss: 0.00083227
Iteration 11/25 | Loss: 0.00083227
Iteration 12/25 | Loss: 0.00083227
Iteration 13/25 | Loss: 0.00083227
Iteration 14/25 | Loss: 0.00083227
Iteration 15/25 | Loss: 0.00083227
Iteration 16/25 | Loss: 0.00083227
Iteration 17/25 | Loss: 0.00083227
Iteration 18/25 | Loss: 0.00083227
Iteration 19/25 | Loss: 0.00083227
Iteration 20/25 | Loss: 0.00083227
Iteration 21/25 | Loss: 0.00083227
Iteration 22/25 | Loss: 0.00083227
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008322708890773356, 0.0008322708890773356, 0.0008322708890773356, 0.0008322708890773356, 0.0008322708890773356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008322708890773356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083227
Iteration 2/1000 | Loss: 0.00004881
Iteration 3/1000 | Loss: 0.00003281
Iteration 4/1000 | Loss: 0.00002956
Iteration 5/1000 | Loss: 0.00002798
Iteration 6/1000 | Loss: 0.00002703
Iteration 7/1000 | Loss: 0.00002615
Iteration 8/1000 | Loss: 0.00002547
Iteration 9/1000 | Loss: 0.00002508
Iteration 10/1000 | Loss: 0.00002465
Iteration 11/1000 | Loss: 0.00002422
Iteration 12/1000 | Loss: 0.00002390
Iteration 13/1000 | Loss: 0.00002361
Iteration 14/1000 | Loss: 0.00002340
Iteration 15/1000 | Loss: 0.00002324
Iteration 16/1000 | Loss: 0.00002309
Iteration 17/1000 | Loss: 0.00002298
Iteration 18/1000 | Loss: 0.00002297
Iteration 19/1000 | Loss: 0.00002296
Iteration 20/1000 | Loss: 0.00002294
Iteration 21/1000 | Loss: 0.00002294
Iteration 22/1000 | Loss: 0.00002291
Iteration 23/1000 | Loss: 0.00002291
Iteration 24/1000 | Loss: 0.00002288
Iteration 25/1000 | Loss: 0.00002288
Iteration 26/1000 | Loss: 0.00002285
Iteration 27/1000 | Loss: 0.00002284
Iteration 28/1000 | Loss: 0.00002277
Iteration 29/1000 | Loss: 0.00002276
Iteration 30/1000 | Loss: 0.00002273
Iteration 31/1000 | Loss: 0.00002273
Iteration 32/1000 | Loss: 0.00002272
Iteration 33/1000 | Loss: 0.00002272
Iteration 34/1000 | Loss: 0.00002271
Iteration 35/1000 | Loss: 0.00002270
Iteration 36/1000 | Loss: 0.00002270
Iteration 37/1000 | Loss: 0.00002268
Iteration 38/1000 | Loss: 0.00002268
Iteration 39/1000 | Loss: 0.00002263
Iteration 40/1000 | Loss: 0.00002263
Iteration 41/1000 | Loss: 0.00002263
Iteration 42/1000 | Loss: 0.00002263
Iteration 43/1000 | Loss: 0.00002263
Iteration 44/1000 | Loss: 0.00002263
Iteration 45/1000 | Loss: 0.00002262
Iteration 46/1000 | Loss: 0.00002262
Iteration 47/1000 | Loss: 0.00002262
Iteration 48/1000 | Loss: 0.00002261
Iteration 49/1000 | Loss: 0.00002261
Iteration 50/1000 | Loss: 0.00002261
Iteration 51/1000 | Loss: 0.00002261
Iteration 52/1000 | Loss: 0.00002261
Iteration 53/1000 | Loss: 0.00002261
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002260
Iteration 57/1000 | Loss: 0.00002260
Iteration 58/1000 | Loss: 0.00002259
Iteration 59/1000 | Loss: 0.00002259
Iteration 60/1000 | Loss: 0.00002259
Iteration 61/1000 | Loss: 0.00002259
Iteration 62/1000 | Loss: 0.00002259
Iteration 63/1000 | Loss: 0.00002258
Iteration 64/1000 | Loss: 0.00002258
Iteration 65/1000 | Loss: 0.00002258
Iteration 66/1000 | Loss: 0.00002257
Iteration 67/1000 | Loss: 0.00002257
Iteration 68/1000 | Loss: 0.00002257
Iteration 69/1000 | Loss: 0.00002257
Iteration 70/1000 | Loss: 0.00002257
Iteration 71/1000 | Loss: 0.00002256
Iteration 72/1000 | Loss: 0.00002256
Iteration 73/1000 | Loss: 0.00002256
Iteration 74/1000 | Loss: 0.00002255
Iteration 75/1000 | Loss: 0.00002254
Iteration 76/1000 | Loss: 0.00002254
Iteration 77/1000 | Loss: 0.00002254
Iteration 78/1000 | Loss: 0.00002253
Iteration 79/1000 | Loss: 0.00002253
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002252
Iteration 82/1000 | Loss: 0.00002252
Iteration 83/1000 | Loss: 0.00002252
Iteration 84/1000 | Loss: 0.00002252
Iteration 85/1000 | Loss: 0.00002252
Iteration 86/1000 | Loss: 0.00002251
Iteration 87/1000 | Loss: 0.00002251
Iteration 88/1000 | Loss: 0.00002251
Iteration 89/1000 | Loss: 0.00002251
Iteration 90/1000 | Loss: 0.00002251
Iteration 91/1000 | Loss: 0.00002250
Iteration 92/1000 | Loss: 0.00002250
Iteration 93/1000 | Loss: 0.00002250
Iteration 94/1000 | Loss: 0.00002250
Iteration 95/1000 | Loss: 0.00002249
Iteration 96/1000 | Loss: 0.00002249
Iteration 97/1000 | Loss: 0.00002249
Iteration 98/1000 | Loss: 0.00002248
Iteration 99/1000 | Loss: 0.00002248
Iteration 100/1000 | Loss: 0.00002248
Iteration 101/1000 | Loss: 0.00002248
Iteration 102/1000 | Loss: 0.00002248
Iteration 103/1000 | Loss: 0.00002247
Iteration 104/1000 | Loss: 0.00002247
Iteration 105/1000 | Loss: 0.00002246
Iteration 106/1000 | Loss: 0.00002246
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002246
Iteration 110/1000 | Loss: 0.00002246
Iteration 111/1000 | Loss: 0.00002245
Iteration 112/1000 | Loss: 0.00002245
Iteration 113/1000 | Loss: 0.00002245
Iteration 114/1000 | Loss: 0.00002245
Iteration 115/1000 | Loss: 0.00002245
Iteration 116/1000 | Loss: 0.00002245
Iteration 117/1000 | Loss: 0.00002244
Iteration 118/1000 | Loss: 0.00002242
Iteration 119/1000 | Loss: 0.00002242
Iteration 120/1000 | Loss: 0.00002241
Iteration 121/1000 | Loss: 0.00002241
Iteration 122/1000 | Loss: 0.00002241
Iteration 123/1000 | Loss: 0.00002240
Iteration 124/1000 | Loss: 0.00002240
Iteration 125/1000 | Loss: 0.00002240
Iteration 126/1000 | Loss: 0.00002239
Iteration 127/1000 | Loss: 0.00002238
Iteration 128/1000 | Loss: 0.00002238
Iteration 129/1000 | Loss: 0.00002237
Iteration 130/1000 | Loss: 0.00002237
Iteration 131/1000 | Loss: 0.00002236
Iteration 132/1000 | Loss: 0.00002235
Iteration 133/1000 | Loss: 0.00002234
Iteration 134/1000 | Loss: 0.00002234
Iteration 135/1000 | Loss: 0.00002234
Iteration 136/1000 | Loss: 0.00002233
Iteration 137/1000 | Loss: 0.00002233
Iteration 138/1000 | Loss: 0.00002233
Iteration 139/1000 | Loss: 0.00002232
Iteration 140/1000 | Loss: 0.00002232
Iteration 141/1000 | Loss: 0.00002232
Iteration 142/1000 | Loss: 0.00002232
Iteration 143/1000 | Loss: 0.00002232
Iteration 144/1000 | Loss: 0.00002231
Iteration 145/1000 | Loss: 0.00002231
Iteration 146/1000 | Loss: 0.00002231
Iteration 147/1000 | Loss: 0.00002230
Iteration 148/1000 | Loss: 0.00002230
Iteration 149/1000 | Loss: 0.00002230
Iteration 150/1000 | Loss: 0.00002230
Iteration 151/1000 | Loss: 0.00002229
Iteration 152/1000 | Loss: 0.00002229
Iteration 153/1000 | Loss: 0.00002229
Iteration 154/1000 | Loss: 0.00002229
Iteration 155/1000 | Loss: 0.00002229
Iteration 156/1000 | Loss: 0.00002229
Iteration 157/1000 | Loss: 0.00002229
Iteration 158/1000 | Loss: 0.00002228
Iteration 159/1000 | Loss: 0.00002228
Iteration 160/1000 | Loss: 0.00002228
Iteration 161/1000 | Loss: 0.00002228
Iteration 162/1000 | Loss: 0.00002228
Iteration 163/1000 | Loss: 0.00002228
Iteration 164/1000 | Loss: 0.00002228
Iteration 165/1000 | Loss: 0.00002228
Iteration 166/1000 | Loss: 0.00002227
Iteration 167/1000 | Loss: 0.00002227
Iteration 168/1000 | Loss: 0.00002227
Iteration 169/1000 | Loss: 0.00002227
Iteration 170/1000 | Loss: 0.00002227
Iteration 171/1000 | Loss: 0.00002227
Iteration 172/1000 | Loss: 0.00002227
Iteration 173/1000 | Loss: 0.00002226
Iteration 174/1000 | Loss: 0.00002226
Iteration 175/1000 | Loss: 0.00002226
Iteration 176/1000 | Loss: 0.00002226
Iteration 177/1000 | Loss: 0.00002226
Iteration 178/1000 | Loss: 0.00002226
Iteration 179/1000 | Loss: 0.00002226
Iteration 180/1000 | Loss: 0.00002225
Iteration 181/1000 | Loss: 0.00002225
Iteration 182/1000 | Loss: 0.00002225
Iteration 183/1000 | Loss: 0.00002225
Iteration 184/1000 | Loss: 0.00002224
Iteration 185/1000 | Loss: 0.00002224
Iteration 186/1000 | Loss: 0.00002224
Iteration 187/1000 | Loss: 0.00002224
Iteration 188/1000 | Loss: 0.00002224
Iteration 189/1000 | Loss: 0.00002224
Iteration 190/1000 | Loss: 0.00002223
Iteration 191/1000 | Loss: 0.00002223
Iteration 192/1000 | Loss: 0.00002223
Iteration 193/1000 | Loss: 0.00002223
Iteration 194/1000 | Loss: 0.00002223
Iteration 195/1000 | Loss: 0.00002223
Iteration 196/1000 | Loss: 0.00002223
Iteration 197/1000 | Loss: 0.00002223
Iteration 198/1000 | Loss: 0.00002223
Iteration 199/1000 | Loss: 0.00002223
Iteration 200/1000 | Loss: 0.00002222
Iteration 201/1000 | Loss: 0.00002222
Iteration 202/1000 | Loss: 0.00002222
Iteration 203/1000 | Loss: 0.00002222
Iteration 204/1000 | Loss: 0.00002222
Iteration 205/1000 | Loss: 0.00002222
Iteration 206/1000 | Loss: 0.00002222
Iteration 207/1000 | Loss: 0.00002222
Iteration 208/1000 | Loss: 0.00002222
Iteration 209/1000 | Loss: 0.00002222
Iteration 210/1000 | Loss: 0.00002222
Iteration 211/1000 | Loss: 0.00002222
Iteration 212/1000 | Loss: 0.00002222
Iteration 213/1000 | Loss: 0.00002221
Iteration 214/1000 | Loss: 0.00002221
Iteration 215/1000 | Loss: 0.00002221
Iteration 216/1000 | Loss: 0.00002221
Iteration 217/1000 | Loss: 0.00002221
Iteration 218/1000 | Loss: 0.00002220
Iteration 219/1000 | Loss: 0.00002220
Iteration 220/1000 | Loss: 0.00002220
Iteration 221/1000 | Loss: 0.00002220
Iteration 222/1000 | Loss: 0.00002220
Iteration 223/1000 | Loss: 0.00002220
Iteration 224/1000 | Loss: 0.00002220
Iteration 225/1000 | Loss: 0.00002220
Iteration 226/1000 | Loss: 0.00002220
Iteration 227/1000 | Loss: 0.00002220
Iteration 228/1000 | Loss: 0.00002220
Iteration 229/1000 | Loss: 0.00002220
Iteration 230/1000 | Loss: 0.00002220
Iteration 231/1000 | Loss: 0.00002220
Iteration 232/1000 | Loss: 0.00002220
Iteration 233/1000 | Loss: 0.00002220
Iteration 234/1000 | Loss: 0.00002219
Iteration 235/1000 | Loss: 0.00002219
Iteration 236/1000 | Loss: 0.00002219
Iteration 237/1000 | Loss: 0.00002219
Iteration 238/1000 | Loss: 0.00002219
Iteration 239/1000 | Loss: 0.00002219
Iteration 240/1000 | Loss: 0.00002219
Iteration 241/1000 | Loss: 0.00002219
Iteration 242/1000 | Loss: 0.00002219
Iteration 243/1000 | Loss: 0.00002219
Iteration 244/1000 | Loss: 0.00002219
Iteration 245/1000 | Loss: 0.00002219
Iteration 246/1000 | Loss: 0.00002219
Iteration 247/1000 | Loss: 0.00002219
Iteration 248/1000 | Loss: 0.00002219
Iteration 249/1000 | Loss: 0.00002219
Iteration 250/1000 | Loss: 0.00002219
Iteration 251/1000 | Loss: 0.00002218
Iteration 252/1000 | Loss: 0.00002218
Iteration 253/1000 | Loss: 0.00002218
Iteration 254/1000 | Loss: 0.00002218
Iteration 255/1000 | Loss: 0.00002218
Iteration 256/1000 | Loss: 0.00002218
Iteration 257/1000 | Loss: 0.00002218
Iteration 258/1000 | Loss: 0.00002218
Iteration 259/1000 | Loss: 0.00002218
Iteration 260/1000 | Loss: 0.00002218
Iteration 261/1000 | Loss: 0.00002218
Iteration 262/1000 | Loss: 0.00002218
Iteration 263/1000 | Loss: 0.00002218
Iteration 264/1000 | Loss: 0.00002218
Iteration 265/1000 | Loss: 0.00002218
Iteration 266/1000 | Loss: 0.00002218
Iteration 267/1000 | Loss: 0.00002218
Iteration 268/1000 | Loss: 0.00002218
Iteration 269/1000 | Loss: 0.00002218
Iteration 270/1000 | Loss: 0.00002218
Iteration 271/1000 | Loss: 0.00002218
Iteration 272/1000 | Loss: 0.00002218
Iteration 273/1000 | Loss: 0.00002218
Iteration 274/1000 | Loss: 0.00002218
Iteration 275/1000 | Loss: 0.00002218
Iteration 276/1000 | Loss: 0.00002218
Iteration 277/1000 | Loss: 0.00002218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [2.217520341218915e-05, 2.217520341218915e-05, 2.217520341218915e-05, 2.217520341218915e-05, 2.217520341218915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.217520341218915e-05

Optimization complete. Final v2v error: 3.9207382202148438 mm

Highest mean error: 4.402151107788086 mm for frame 197

Lowest mean error: 3.7189128398895264 mm for frame 51

Saving results

Total time: 56.475850105285645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490981
Iteration 2/25 | Loss: 0.00139300
Iteration 3/25 | Loss: 0.00131057
Iteration 4/25 | Loss: 0.00129478
Iteration 5/25 | Loss: 0.00128794
Iteration 6/25 | Loss: 0.00128785
Iteration 7/25 | Loss: 0.00128785
Iteration 8/25 | Loss: 0.00128785
Iteration 9/25 | Loss: 0.00128785
Iteration 10/25 | Loss: 0.00128785
Iteration 11/25 | Loss: 0.00128785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012878536945208907, 0.0012878536945208907, 0.0012878536945208907, 0.0012878536945208907, 0.0012878536945208907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012878536945208907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83403116
Iteration 2/25 | Loss: 0.00080595
Iteration 3/25 | Loss: 0.00080595
Iteration 4/25 | Loss: 0.00080595
Iteration 5/25 | Loss: 0.00080595
Iteration 6/25 | Loss: 0.00080595
Iteration 7/25 | Loss: 0.00080595
Iteration 8/25 | Loss: 0.00080595
Iteration 9/25 | Loss: 0.00080595
Iteration 10/25 | Loss: 0.00080595
Iteration 11/25 | Loss: 0.00080595
Iteration 12/25 | Loss: 0.00080595
Iteration 13/25 | Loss: 0.00080595
Iteration 14/25 | Loss: 0.00080595
Iteration 15/25 | Loss: 0.00080595
Iteration 16/25 | Loss: 0.00080595
Iteration 17/25 | Loss: 0.00080595
Iteration 18/25 | Loss: 0.00080595
Iteration 19/25 | Loss: 0.00080595
Iteration 20/25 | Loss: 0.00080595
Iteration 21/25 | Loss: 0.00080595
Iteration 22/25 | Loss: 0.00080595
Iteration 23/25 | Loss: 0.00080595
Iteration 24/25 | Loss: 0.00080595
Iteration 25/25 | Loss: 0.00080595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080595
Iteration 2/1000 | Loss: 0.00004643
Iteration 3/1000 | Loss: 0.00003076
Iteration 4/1000 | Loss: 0.00002793
Iteration 5/1000 | Loss: 0.00002610
Iteration 6/1000 | Loss: 0.00002479
Iteration 7/1000 | Loss: 0.00002398
Iteration 8/1000 | Loss: 0.00002328
Iteration 9/1000 | Loss: 0.00002275
Iteration 10/1000 | Loss: 0.00002236
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00002164
Iteration 13/1000 | Loss: 0.00002135
Iteration 14/1000 | Loss: 0.00002119
Iteration 15/1000 | Loss: 0.00002115
Iteration 16/1000 | Loss: 0.00002097
Iteration 17/1000 | Loss: 0.00002081
Iteration 18/1000 | Loss: 0.00002074
Iteration 19/1000 | Loss: 0.00002074
Iteration 20/1000 | Loss: 0.00002061
Iteration 21/1000 | Loss: 0.00002050
Iteration 22/1000 | Loss: 0.00002050
Iteration 23/1000 | Loss: 0.00002048
Iteration 24/1000 | Loss: 0.00002047
Iteration 25/1000 | Loss: 0.00002038
Iteration 26/1000 | Loss: 0.00002036
Iteration 27/1000 | Loss: 0.00002036
Iteration 28/1000 | Loss: 0.00002034
Iteration 29/1000 | Loss: 0.00002031
Iteration 30/1000 | Loss: 0.00002031
Iteration 31/1000 | Loss: 0.00002031
Iteration 32/1000 | Loss: 0.00002031
Iteration 33/1000 | Loss: 0.00002031
Iteration 34/1000 | Loss: 0.00002031
Iteration 35/1000 | Loss: 0.00002031
Iteration 36/1000 | Loss: 0.00002031
Iteration 37/1000 | Loss: 0.00002030
Iteration 38/1000 | Loss: 0.00002030
Iteration 39/1000 | Loss: 0.00002030
Iteration 40/1000 | Loss: 0.00002030
Iteration 41/1000 | Loss: 0.00002030
Iteration 42/1000 | Loss: 0.00002029
Iteration 43/1000 | Loss: 0.00002028
Iteration 44/1000 | Loss: 0.00002028
Iteration 45/1000 | Loss: 0.00002028
Iteration 46/1000 | Loss: 0.00002028
Iteration 47/1000 | Loss: 0.00002028
Iteration 48/1000 | Loss: 0.00002028
Iteration 49/1000 | Loss: 0.00002028
Iteration 50/1000 | Loss: 0.00002028
Iteration 51/1000 | Loss: 0.00002028
Iteration 52/1000 | Loss: 0.00002027
Iteration 53/1000 | Loss: 0.00002027
Iteration 54/1000 | Loss: 0.00002027
Iteration 55/1000 | Loss: 0.00002027
Iteration 56/1000 | Loss: 0.00002027
Iteration 57/1000 | Loss: 0.00002027
Iteration 58/1000 | Loss: 0.00002027
Iteration 59/1000 | Loss: 0.00002027
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002026
Iteration 62/1000 | Loss: 0.00002026
Iteration 63/1000 | Loss: 0.00002025
Iteration 64/1000 | Loss: 0.00002025
Iteration 65/1000 | Loss: 0.00002025
Iteration 66/1000 | Loss: 0.00002025
Iteration 67/1000 | Loss: 0.00002025
Iteration 68/1000 | Loss: 0.00002025
Iteration 69/1000 | Loss: 0.00002025
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00002025
Iteration 72/1000 | Loss: 0.00002025
Iteration 73/1000 | Loss: 0.00002025
Iteration 74/1000 | Loss: 0.00002024
Iteration 75/1000 | Loss: 0.00002024
Iteration 76/1000 | Loss: 0.00002024
Iteration 77/1000 | Loss: 0.00002024
Iteration 78/1000 | Loss: 0.00002024
Iteration 79/1000 | Loss: 0.00002024
Iteration 80/1000 | Loss: 0.00002024
Iteration 81/1000 | Loss: 0.00002024
Iteration 82/1000 | Loss: 0.00002024
Iteration 83/1000 | Loss: 0.00002024
Iteration 84/1000 | Loss: 0.00002024
Iteration 85/1000 | Loss: 0.00002023
Iteration 86/1000 | Loss: 0.00002023
Iteration 87/1000 | Loss: 0.00002023
Iteration 88/1000 | Loss: 0.00002022
Iteration 89/1000 | Loss: 0.00002022
Iteration 90/1000 | Loss: 0.00002022
Iteration 91/1000 | Loss: 0.00002022
Iteration 92/1000 | Loss: 0.00002022
Iteration 93/1000 | Loss: 0.00002022
Iteration 94/1000 | Loss: 0.00002021
Iteration 95/1000 | Loss: 0.00002020
Iteration 96/1000 | Loss: 0.00002020
Iteration 97/1000 | Loss: 0.00002019
Iteration 98/1000 | Loss: 0.00002019
Iteration 99/1000 | Loss: 0.00002018
Iteration 100/1000 | Loss: 0.00002017
Iteration 101/1000 | Loss: 0.00002016
Iteration 102/1000 | Loss: 0.00002016
Iteration 103/1000 | Loss: 0.00002016
Iteration 104/1000 | Loss: 0.00002015
Iteration 105/1000 | Loss: 0.00002015
Iteration 106/1000 | Loss: 0.00002015
Iteration 107/1000 | Loss: 0.00002015
Iteration 108/1000 | Loss: 0.00002014
Iteration 109/1000 | Loss: 0.00002014
Iteration 110/1000 | Loss: 0.00002014
Iteration 111/1000 | Loss: 0.00002014
Iteration 112/1000 | Loss: 0.00002013
Iteration 113/1000 | Loss: 0.00002013
Iteration 114/1000 | Loss: 0.00002013
Iteration 115/1000 | Loss: 0.00002013
Iteration 116/1000 | Loss: 0.00002013
Iteration 117/1000 | Loss: 0.00002013
Iteration 118/1000 | Loss: 0.00002012
Iteration 119/1000 | Loss: 0.00002012
Iteration 120/1000 | Loss: 0.00002012
Iteration 121/1000 | Loss: 0.00002011
Iteration 122/1000 | Loss: 0.00002011
Iteration 123/1000 | Loss: 0.00002011
Iteration 124/1000 | Loss: 0.00002010
Iteration 125/1000 | Loss: 0.00002010
Iteration 126/1000 | Loss: 0.00002010
Iteration 127/1000 | Loss: 0.00002010
Iteration 128/1000 | Loss: 0.00002009
Iteration 129/1000 | Loss: 0.00002009
Iteration 130/1000 | Loss: 0.00002009
Iteration 131/1000 | Loss: 0.00002009
Iteration 132/1000 | Loss: 0.00002008
Iteration 133/1000 | Loss: 0.00002008
Iteration 134/1000 | Loss: 0.00002008
Iteration 135/1000 | Loss: 0.00002007
Iteration 136/1000 | Loss: 0.00002007
Iteration 137/1000 | Loss: 0.00002007
Iteration 138/1000 | Loss: 0.00002007
Iteration 139/1000 | Loss: 0.00002007
Iteration 140/1000 | Loss: 0.00002007
Iteration 141/1000 | Loss: 0.00002007
Iteration 142/1000 | Loss: 0.00002006
Iteration 143/1000 | Loss: 0.00002006
Iteration 144/1000 | Loss: 0.00002006
Iteration 145/1000 | Loss: 0.00002006
Iteration 146/1000 | Loss: 0.00002006
Iteration 147/1000 | Loss: 0.00002005
Iteration 148/1000 | Loss: 0.00002005
Iteration 149/1000 | Loss: 0.00002004
Iteration 150/1000 | Loss: 0.00002004
Iteration 151/1000 | Loss: 0.00002004
Iteration 152/1000 | Loss: 0.00002004
Iteration 153/1000 | Loss: 0.00002004
Iteration 154/1000 | Loss: 0.00002004
Iteration 155/1000 | Loss: 0.00002004
Iteration 156/1000 | Loss: 0.00002004
Iteration 157/1000 | Loss: 0.00002004
Iteration 158/1000 | Loss: 0.00002004
Iteration 159/1000 | Loss: 0.00002004
Iteration 160/1000 | Loss: 0.00002003
Iteration 161/1000 | Loss: 0.00002003
Iteration 162/1000 | Loss: 0.00002003
Iteration 163/1000 | Loss: 0.00002003
Iteration 164/1000 | Loss: 0.00002003
Iteration 165/1000 | Loss: 0.00002003
Iteration 166/1000 | Loss: 0.00002003
Iteration 167/1000 | Loss: 0.00002003
Iteration 168/1000 | Loss: 0.00002002
Iteration 169/1000 | Loss: 0.00002002
Iteration 170/1000 | Loss: 0.00002002
Iteration 171/1000 | Loss: 0.00002002
Iteration 172/1000 | Loss: 0.00002002
Iteration 173/1000 | Loss: 0.00002002
Iteration 174/1000 | Loss: 0.00002002
Iteration 175/1000 | Loss: 0.00002002
Iteration 176/1000 | Loss: 0.00002002
Iteration 177/1000 | Loss: 0.00002002
Iteration 178/1000 | Loss: 0.00002002
Iteration 179/1000 | Loss: 0.00002002
Iteration 180/1000 | Loss: 0.00002001
Iteration 181/1000 | Loss: 0.00002001
Iteration 182/1000 | Loss: 0.00002001
Iteration 183/1000 | Loss: 0.00002001
Iteration 184/1000 | Loss: 0.00002001
Iteration 185/1000 | Loss: 0.00002000
Iteration 186/1000 | Loss: 0.00002000
Iteration 187/1000 | Loss: 0.00002000
Iteration 188/1000 | Loss: 0.00002000
Iteration 189/1000 | Loss: 0.00002000
Iteration 190/1000 | Loss: 0.00002000
Iteration 191/1000 | Loss: 0.00002000
Iteration 192/1000 | Loss: 0.00001999
Iteration 193/1000 | Loss: 0.00001999
Iteration 194/1000 | Loss: 0.00001999
Iteration 195/1000 | Loss: 0.00001999
Iteration 196/1000 | Loss: 0.00001999
Iteration 197/1000 | Loss: 0.00001999
Iteration 198/1000 | Loss: 0.00001999
Iteration 199/1000 | Loss: 0.00001999
Iteration 200/1000 | Loss: 0.00001998
Iteration 201/1000 | Loss: 0.00001998
Iteration 202/1000 | Loss: 0.00001998
Iteration 203/1000 | Loss: 0.00001997
Iteration 204/1000 | Loss: 0.00001997
Iteration 205/1000 | Loss: 0.00001997
Iteration 206/1000 | Loss: 0.00001997
Iteration 207/1000 | Loss: 0.00001997
Iteration 208/1000 | Loss: 0.00001997
Iteration 209/1000 | Loss: 0.00001997
Iteration 210/1000 | Loss: 0.00001996
Iteration 211/1000 | Loss: 0.00001996
Iteration 212/1000 | Loss: 0.00001996
Iteration 213/1000 | Loss: 0.00001996
Iteration 214/1000 | Loss: 0.00001996
Iteration 215/1000 | Loss: 0.00001996
Iteration 216/1000 | Loss: 0.00001995
Iteration 217/1000 | Loss: 0.00001995
Iteration 218/1000 | Loss: 0.00001995
Iteration 219/1000 | Loss: 0.00001995
Iteration 220/1000 | Loss: 0.00001995
Iteration 221/1000 | Loss: 0.00001995
Iteration 222/1000 | Loss: 0.00001995
Iteration 223/1000 | Loss: 0.00001995
Iteration 224/1000 | Loss: 0.00001995
Iteration 225/1000 | Loss: 0.00001995
Iteration 226/1000 | Loss: 0.00001995
Iteration 227/1000 | Loss: 0.00001994
Iteration 228/1000 | Loss: 0.00001994
Iteration 229/1000 | Loss: 0.00001994
Iteration 230/1000 | Loss: 0.00001994
Iteration 231/1000 | Loss: 0.00001994
Iteration 232/1000 | Loss: 0.00001994
Iteration 233/1000 | Loss: 0.00001994
Iteration 234/1000 | Loss: 0.00001994
Iteration 235/1000 | Loss: 0.00001994
Iteration 236/1000 | Loss: 0.00001994
Iteration 237/1000 | Loss: 0.00001994
Iteration 238/1000 | Loss: 0.00001994
Iteration 239/1000 | Loss: 0.00001994
Iteration 240/1000 | Loss: 0.00001994
Iteration 241/1000 | Loss: 0.00001994
Iteration 242/1000 | Loss: 0.00001994
Iteration 243/1000 | Loss: 0.00001994
Iteration 244/1000 | Loss: 0.00001994
Iteration 245/1000 | Loss: 0.00001994
Iteration 246/1000 | Loss: 0.00001994
Iteration 247/1000 | Loss: 0.00001994
Iteration 248/1000 | Loss: 0.00001994
Iteration 249/1000 | Loss: 0.00001994
Iteration 250/1000 | Loss: 0.00001994
Iteration 251/1000 | Loss: 0.00001994
Iteration 252/1000 | Loss: 0.00001994
Iteration 253/1000 | Loss: 0.00001994
Iteration 254/1000 | Loss: 0.00001994
Iteration 255/1000 | Loss: 0.00001994
Iteration 256/1000 | Loss: 0.00001994
Iteration 257/1000 | Loss: 0.00001994
Iteration 258/1000 | Loss: 0.00001994
Iteration 259/1000 | Loss: 0.00001994
Iteration 260/1000 | Loss: 0.00001994
Iteration 261/1000 | Loss: 0.00001994
Iteration 262/1000 | Loss: 0.00001994
Iteration 263/1000 | Loss: 0.00001994
Iteration 264/1000 | Loss: 0.00001994
Iteration 265/1000 | Loss: 0.00001994
Iteration 266/1000 | Loss: 0.00001994
Iteration 267/1000 | Loss: 0.00001994
Iteration 268/1000 | Loss: 0.00001994
Iteration 269/1000 | Loss: 0.00001994
Iteration 270/1000 | Loss: 0.00001994
Iteration 271/1000 | Loss: 0.00001994
Iteration 272/1000 | Loss: 0.00001994
Iteration 273/1000 | Loss: 0.00001994
Iteration 274/1000 | Loss: 0.00001994
Iteration 275/1000 | Loss: 0.00001994
Iteration 276/1000 | Loss: 0.00001994
Iteration 277/1000 | Loss: 0.00001994
Iteration 278/1000 | Loss: 0.00001994
Iteration 279/1000 | Loss: 0.00001994
Iteration 280/1000 | Loss: 0.00001994
Iteration 281/1000 | Loss: 0.00001994
Iteration 282/1000 | Loss: 0.00001994
Iteration 283/1000 | Loss: 0.00001994
Iteration 284/1000 | Loss: 0.00001994
Iteration 285/1000 | Loss: 0.00001994
Iteration 286/1000 | Loss: 0.00001994
Iteration 287/1000 | Loss: 0.00001994
Iteration 288/1000 | Loss: 0.00001994
Iteration 289/1000 | Loss: 0.00001994
Iteration 290/1000 | Loss: 0.00001994
Iteration 291/1000 | Loss: 0.00001994
Iteration 292/1000 | Loss: 0.00001994
Iteration 293/1000 | Loss: 0.00001994
Iteration 294/1000 | Loss: 0.00001994
Iteration 295/1000 | Loss: 0.00001994
Iteration 296/1000 | Loss: 0.00001994
Iteration 297/1000 | Loss: 0.00001994
Iteration 298/1000 | Loss: 0.00001994
Iteration 299/1000 | Loss: 0.00001994
Iteration 300/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.9938801415264606e-05, 1.9938801415264606e-05, 1.9938801415264606e-05, 1.9938801415264606e-05, 1.9938801415264606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9938801415264606e-05

Optimization complete. Final v2v error: 3.7804858684539795 mm

Highest mean error: 4.483592987060547 mm for frame 5

Lowest mean error: 3.5976614952087402 mm for frame 57

Saving results

Total time: 60.06095743179321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596682
Iteration 2/25 | Loss: 0.00160939
Iteration 3/25 | Loss: 0.00136840
Iteration 4/25 | Loss: 0.00134048
Iteration 5/25 | Loss: 0.00133587
Iteration 6/25 | Loss: 0.00133419
Iteration 7/25 | Loss: 0.00133416
Iteration 8/25 | Loss: 0.00133416
Iteration 9/25 | Loss: 0.00133416
Iteration 10/25 | Loss: 0.00133416
Iteration 11/25 | Loss: 0.00133416
Iteration 12/25 | Loss: 0.00133416
Iteration 13/25 | Loss: 0.00133416
Iteration 14/25 | Loss: 0.00133416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013341567246243358, 0.0013341567246243358, 0.0013341567246243358, 0.0013341567246243358, 0.0013341567246243358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013341567246243358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22028935
Iteration 2/25 | Loss: 0.00077495
Iteration 3/25 | Loss: 0.00077495
Iteration 4/25 | Loss: 0.00077495
Iteration 5/25 | Loss: 0.00077495
Iteration 6/25 | Loss: 0.00077495
Iteration 7/25 | Loss: 0.00077495
Iteration 8/25 | Loss: 0.00077495
Iteration 9/25 | Loss: 0.00077495
Iteration 10/25 | Loss: 0.00077495
Iteration 11/25 | Loss: 0.00077495
Iteration 12/25 | Loss: 0.00077495
Iteration 13/25 | Loss: 0.00077495
Iteration 14/25 | Loss: 0.00077495
Iteration 15/25 | Loss: 0.00077495
Iteration 16/25 | Loss: 0.00077495
Iteration 17/25 | Loss: 0.00077495
Iteration 18/25 | Loss: 0.00077495
Iteration 19/25 | Loss: 0.00077495
Iteration 20/25 | Loss: 0.00077495
Iteration 21/25 | Loss: 0.00077495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007749455398879945, 0.0007749455398879945, 0.0007749455398879945, 0.0007749455398879945, 0.0007749455398879945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007749455398879945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077495
Iteration 2/1000 | Loss: 0.00004350
Iteration 3/1000 | Loss: 0.00002744
Iteration 4/1000 | Loss: 0.00002279
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00002003
Iteration 8/1000 | Loss: 0.00001958
Iteration 9/1000 | Loss: 0.00001927
Iteration 10/1000 | Loss: 0.00001899
Iteration 11/1000 | Loss: 0.00001880
Iteration 12/1000 | Loss: 0.00001876
Iteration 13/1000 | Loss: 0.00001874
Iteration 14/1000 | Loss: 0.00001867
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001866
Iteration 18/1000 | Loss: 0.00001866
Iteration 19/1000 | Loss: 0.00001866
Iteration 20/1000 | Loss: 0.00001866
Iteration 21/1000 | Loss: 0.00001866
Iteration 22/1000 | Loss: 0.00001864
Iteration 23/1000 | Loss: 0.00001861
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001855
Iteration 26/1000 | Loss: 0.00001855
Iteration 27/1000 | Loss: 0.00001854
Iteration 28/1000 | Loss: 0.00001854
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001849
Iteration 32/1000 | Loss: 0.00001849
Iteration 33/1000 | Loss: 0.00001848
Iteration 34/1000 | Loss: 0.00001848
Iteration 35/1000 | Loss: 0.00001847
Iteration 36/1000 | Loss: 0.00001847
Iteration 37/1000 | Loss: 0.00001846
Iteration 38/1000 | Loss: 0.00001846
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001845
Iteration 42/1000 | Loss: 0.00001844
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001843
Iteration 46/1000 | Loss: 0.00001843
Iteration 47/1000 | Loss: 0.00001843
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001843
Iteration 50/1000 | Loss: 0.00001842
Iteration 51/1000 | Loss: 0.00001842
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001842
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001842
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001840
Iteration 59/1000 | Loss: 0.00001840
Iteration 60/1000 | Loss: 0.00001840
Iteration 61/1000 | Loss: 0.00001840
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001839
Iteration 66/1000 | Loss: 0.00001839
Iteration 67/1000 | Loss: 0.00001839
Iteration 68/1000 | Loss: 0.00001839
Iteration 69/1000 | Loss: 0.00001839
Iteration 70/1000 | Loss: 0.00001839
Iteration 71/1000 | Loss: 0.00001839
Iteration 72/1000 | Loss: 0.00001839
Iteration 73/1000 | Loss: 0.00001839
Iteration 74/1000 | Loss: 0.00001839
Iteration 75/1000 | Loss: 0.00001839
Iteration 76/1000 | Loss: 0.00001839
Iteration 77/1000 | Loss: 0.00001838
Iteration 78/1000 | Loss: 0.00001838
Iteration 79/1000 | Loss: 0.00001838
Iteration 80/1000 | Loss: 0.00001838
Iteration 81/1000 | Loss: 0.00001838
Iteration 82/1000 | Loss: 0.00001838
Iteration 83/1000 | Loss: 0.00001838
Iteration 84/1000 | Loss: 0.00001837
Iteration 85/1000 | Loss: 0.00001837
Iteration 86/1000 | Loss: 0.00001837
Iteration 87/1000 | Loss: 0.00001836
Iteration 88/1000 | Loss: 0.00001836
Iteration 89/1000 | Loss: 0.00001835
Iteration 90/1000 | Loss: 0.00001835
Iteration 91/1000 | Loss: 0.00001835
Iteration 92/1000 | Loss: 0.00001835
Iteration 93/1000 | Loss: 0.00001834
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001834
Iteration 96/1000 | Loss: 0.00001834
Iteration 97/1000 | Loss: 0.00001833
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001833
Iteration 100/1000 | Loss: 0.00001832
Iteration 101/1000 | Loss: 0.00001832
Iteration 102/1000 | Loss: 0.00001832
Iteration 103/1000 | Loss: 0.00001832
Iteration 104/1000 | Loss: 0.00001831
Iteration 105/1000 | Loss: 0.00001831
Iteration 106/1000 | Loss: 0.00001831
Iteration 107/1000 | Loss: 0.00001831
Iteration 108/1000 | Loss: 0.00001831
Iteration 109/1000 | Loss: 0.00001831
Iteration 110/1000 | Loss: 0.00001831
Iteration 111/1000 | Loss: 0.00001830
Iteration 112/1000 | Loss: 0.00001830
Iteration 113/1000 | Loss: 0.00001830
Iteration 114/1000 | Loss: 0.00001830
Iteration 115/1000 | Loss: 0.00001829
Iteration 116/1000 | Loss: 0.00001829
Iteration 117/1000 | Loss: 0.00001829
Iteration 118/1000 | Loss: 0.00001829
Iteration 119/1000 | Loss: 0.00001829
Iteration 120/1000 | Loss: 0.00001829
Iteration 121/1000 | Loss: 0.00001829
Iteration 122/1000 | Loss: 0.00001829
Iteration 123/1000 | Loss: 0.00001829
Iteration 124/1000 | Loss: 0.00001829
Iteration 125/1000 | Loss: 0.00001829
Iteration 126/1000 | Loss: 0.00001828
Iteration 127/1000 | Loss: 0.00001828
Iteration 128/1000 | Loss: 0.00001828
Iteration 129/1000 | Loss: 0.00001828
Iteration 130/1000 | Loss: 0.00001828
Iteration 131/1000 | Loss: 0.00001828
Iteration 132/1000 | Loss: 0.00001828
Iteration 133/1000 | Loss: 0.00001828
Iteration 134/1000 | Loss: 0.00001828
Iteration 135/1000 | Loss: 0.00001827
Iteration 136/1000 | Loss: 0.00001827
Iteration 137/1000 | Loss: 0.00001827
Iteration 138/1000 | Loss: 0.00001827
Iteration 139/1000 | Loss: 0.00001827
Iteration 140/1000 | Loss: 0.00001827
Iteration 141/1000 | Loss: 0.00001827
Iteration 142/1000 | Loss: 0.00001827
Iteration 143/1000 | Loss: 0.00001827
Iteration 144/1000 | Loss: 0.00001827
Iteration 145/1000 | Loss: 0.00001827
Iteration 146/1000 | Loss: 0.00001827
Iteration 147/1000 | Loss: 0.00001827
Iteration 148/1000 | Loss: 0.00001826
Iteration 149/1000 | Loss: 0.00001826
Iteration 150/1000 | Loss: 0.00001826
Iteration 151/1000 | Loss: 0.00001826
Iteration 152/1000 | Loss: 0.00001826
Iteration 153/1000 | Loss: 0.00001826
Iteration 154/1000 | Loss: 0.00001826
Iteration 155/1000 | Loss: 0.00001826
Iteration 156/1000 | Loss: 0.00001826
Iteration 157/1000 | Loss: 0.00001826
Iteration 158/1000 | Loss: 0.00001826
Iteration 159/1000 | Loss: 0.00001826
Iteration 160/1000 | Loss: 0.00001826
Iteration 161/1000 | Loss: 0.00001825
Iteration 162/1000 | Loss: 0.00001825
Iteration 163/1000 | Loss: 0.00001825
Iteration 164/1000 | Loss: 0.00001825
Iteration 165/1000 | Loss: 0.00001825
Iteration 166/1000 | Loss: 0.00001825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.825480103434529e-05, 1.825480103434529e-05, 1.825480103434529e-05, 1.825480103434529e-05, 1.825480103434529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.825480103434529e-05

Optimization complete. Final v2v error: 3.471637010574341 mm

Highest mean error: 4.97200345993042 mm for frame 54

Lowest mean error: 3.1261093616485596 mm for frame 137

Saving results

Total time: 35.90782356262207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854093
Iteration 2/25 | Loss: 0.00136684
Iteration 3/25 | Loss: 0.00129426
Iteration 4/25 | Loss: 0.00128043
Iteration 5/25 | Loss: 0.00127654
Iteration 6/25 | Loss: 0.00127611
Iteration 7/25 | Loss: 0.00127611
Iteration 8/25 | Loss: 0.00127611
Iteration 9/25 | Loss: 0.00127611
Iteration 10/25 | Loss: 0.00127611
Iteration 11/25 | Loss: 0.00127611
Iteration 12/25 | Loss: 0.00127611
Iteration 13/25 | Loss: 0.00127611
Iteration 14/25 | Loss: 0.00127611
Iteration 15/25 | Loss: 0.00127611
Iteration 16/25 | Loss: 0.00127611
Iteration 17/25 | Loss: 0.00127611
Iteration 18/25 | Loss: 0.00127611
Iteration 19/25 | Loss: 0.00127611
Iteration 20/25 | Loss: 0.00127611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012761072721332312, 0.0012761072721332312, 0.0012761072721332312, 0.0012761072721332312, 0.0012761072721332312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012761072721332312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39365304
Iteration 2/25 | Loss: 0.00095928
Iteration 3/25 | Loss: 0.00095926
Iteration 4/25 | Loss: 0.00095926
Iteration 5/25 | Loss: 0.00095926
Iteration 6/25 | Loss: 0.00095926
Iteration 7/25 | Loss: 0.00095926
Iteration 8/25 | Loss: 0.00095926
Iteration 9/25 | Loss: 0.00095926
Iteration 10/25 | Loss: 0.00095926
Iteration 11/25 | Loss: 0.00095926
Iteration 12/25 | Loss: 0.00095926
Iteration 13/25 | Loss: 0.00095926
Iteration 14/25 | Loss: 0.00095926
Iteration 15/25 | Loss: 0.00095926
Iteration 16/25 | Loss: 0.00095926
Iteration 17/25 | Loss: 0.00095926
Iteration 18/25 | Loss: 0.00095926
Iteration 19/25 | Loss: 0.00095926
Iteration 20/25 | Loss: 0.00095926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009592555579729378, 0.0009592555579729378, 0.0009592555579729378, 0.0009592555579729378, 0.0009592555579729378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009592555579729378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095926
Iteration 2/1000 | Loss: 0.00004332
Iteration 3/1000 | Loss: 0.00002476
Iteration 4/1000 | Loss: 0.00002069
Iteration 5/1000 | Loss: 0.00001912
Iteration 6/1000 | Loss: 0.00001799
Iteration 7/1000 | Loss: 0.00001717
Iteration 8/1000 | Loss: 0.00001654
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001611
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001595
Iteration 13/1000 | Loss: 0.00001594
Iteration 14/1000 | Loss: 0.00001593
Iteration 15/1000 | Loss: 0.00001592
Iteration 16/1000 | Loss: 0.00001591
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001533
Iteration 26/1000 | Loss: 0.00001533
Iteration 27/1000 | Loss: 0.00001532
Iteration 28/1000 | Loss: 0.00001532
Iteration 29/1000 | Loss: 0.00001532
Iteration 30/1000 | Loss: 0.00001531
Iteration 31/1000 | Loss: 0.00001530
Iteration 32/1000 | Loss: 0.00001529
Iteration 33/1000 | Loss: 0.00001529
Iteration 34/1000 | Loss: 0.00001529
Iteration 35/1000 | Loss: 0.00001529
Iteration 36/1000 | Loss: 0.00001528
Iteration 37/1000 | Loss: 0.00001528
Iteration 38/1000 | Loss: 0.00001528
Iteration 39/1000 | Loss: 0.00001528
Iteration 40/1000 | Loss: 0.00001528
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001527
Iteration 44/1000 | Loss: 0.00001527
Iteration 45/1000 | Loss: 0.00001527
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001526
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001523
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001521
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001520
Iteration 65/1000 | Loss: 0.00001520
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001520
Iteration 68/1000 | Loss: 0.00001519
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001518
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001517
Iteration 85/1000 | Loss: 0.00001517
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001516
Iteration 88/1000 | Loss: 0.00001516
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001516
Iteration 91/1000 | Loss: 0.00001516
Iteration 92/1000 | Loss: 0.00001516
Iteration 93/1000 | Loss: 0.00001516
Iteration 94/1000 | Loss: 0.00001516
Iteration 95/1000 | Loss: 0.00001515
Iteration 96/1000 | Loss: 0.00001515
Iteration 97/1000 | Loss: 0.00001515
Iteration 98/1000 | Loss: 0.00001515
Iteration 99/1000 | Loss: 0.00001514
Iteration 100/1000 | Loss: 0.00001514
Iteration 101/1000 | Loss: 0.00001514
Iteration 102/1000 | Loss: 0.00001514
Iteration 103/1000 | Loss: 0.00001514
Iteration 104/1000 | Loss: 0.00001514
Iteration 105/1000 | Loss: 0.00001513
Iteration 106/1000 | Loss: 0.00001513
Iteration 107/1000 | Loss: 0.00001513
Iteration 108/1000 | Loss: 0.00001513
Iteration 109/1000 | Loss: 0.00001513
Iteration 110/1000 | Loss: 0.00001513
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001512
Iteration 114/1000 | Loss: 0.00001512
Iteration 115/1000 | Loss: 0.00001512
Iteration 116/1000 | Loss: 0.00001512
Iteration 117/1000 | Loss: 0.00001512
Iteration 118/1000 | Loss: 0.00001512
Iteration 119/1000 | Loss: 0.00001512
Iteration 120/1000 | Loss: 0.00001512
Iteration 121/1000 | Loss: 0.00001512
Iteration 122/1000 | Loss: 0.00001511
Iteration 123/1000 | Loss: 0.00001511
Iteration 124/1000 | Loss: 0.00001511
Iteration 125/1000 | Loss: 0.00001511
Iteration 126/1000 | Loss: 0.00001511
Iteration 127/1000 | Loss: 0.00001511
Iteration 128/1000 | Loss: 0.00001511
Iteration 129/1000 | Loss: 0.00001511
Iteration 130/1000 | Loss: 0.00001511
Iteration 131/1000 | Loss: 0.00001511
Iteration 132/1000 | Loss: 0.00001511
Iteration 133/1000 | Loss: 0.00001511
Iteration 134/1000 | Loss: 0.00001511
Iteration 135/1000 | Loss: 0.00001511
Iteration 136/1000 | Loss: 0.00001511
Iteration 137/1000 | Loss: 0.00001510
Iteration 138/1000 | Loss: 0.00001510
Iteration 139/1000 | Loss: 0.00001510
Iteration 140/1000 | Loss: 0.00001510
Iteration 141/1000 | Loss: 0.00001509
Iteration 142/1000 | Loss: 0.00001509
Iteration 143/1000 | Loss: 0.00001509
Iteration 144/1000 | Loss: 0.00001509
Iteration 145/1000 | Loss: 0.00001509
Iteration 146/1000 | Loss: 0.00001509
Iteration 147/1000 | Loss: 0.00001509
Iteration 148/1000 | Loss: 0.00001509
Iteration 149/1000 | Loss: 0.00001509
Iteration 150/1000 | Loss: 0.00001509
Iteration 151/1000 | Loss: 0.00001509
Iteration 152/1000 | Loss: 0.00001509
Iteration 153/1000 | Loss: 0.00001509
Iteration 154/1000 | Loss: 0.00001509
Iteration 155/1000 | Loss: 0.00001509
Iteration 156/1000 | Loss: 0.00001509
Iteration 157/1000 | Loss: 0.00001509
Iteration 158/1000 | Loss: 0.00001509
Iteration 159/1000 | Loss: 0.00001509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.5089849512150977e-05, 1.5089849512150977e-05, 1.5089849512150977e-05, 1.5089849512150977e-05, 1.5089849512150977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5089849512150977e-05

Optimization complete. Final v2v error: 3.231412649154663 mm

Highest mean error: 4.2009196281433105 mm for frame 41

Lowest mean error: 2.747159481048584 mm for frame 84

Saving results

Total time: 36.84519362449646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771195
Iteration 2/25 | Loss: 0.00175514
Iteration 3/25 | Loss: 0.00144752
Iteration 4/25 | Loss: 0.00141327
Iteration 5/25 | Loss: 0.00140819
Iteration 6/25 | Loss: 0.00140719
Iteration 7/25 | Loss: 0.00140719
Iteration 8/25 | Loss: 0.00140719
Iteration 9/25 | Loss: 0.00140719
Iteration 10/25 | Loss: 0.00140719
Iteration 11/25 | Loss: 0.00140719
Iteration 12/25 | Loss: 0.00140719
Iteration 13/25 | Loss: 0.00140719
Iteration 14/25 | Loss: 0.00140719
Iteration 15/25 | Loss: 0.00140719
Iteration 16/25 | Loss: 0.00140719
Iteration 17/25 | Loss: 0.00140719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014071926707401872, 0.0014071926707401872, 0.0014071926707401872, 0.0014071926707401872, 0.0014071926707401872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014071926707401872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40979195
Iteration 2/25 | Loss: 0.00088471
Iteration 3/25 | Loss: 0.00088468
Iteration 4/25 | Loss: 0.00088467
Iteration 5/25 | Loss: 0.00088467
Iteration 6/25 | Loss: 0.00088467
Iteration 7/25 | Loss: 0.00088467
Iteration 8/25 | Loss: 0.00088467
Iteration 9/25 | Loss: 0.00088467
Iteration 10/25 | Loss: 0.00088467
Iteration 11/25 | Loss: 0.00088467
Iteration 12/25 | Loss: 0.00088467
Iteration 13/25 | Loss: 0.00088467
Iteration 14/25 | Loss: 0.00088467
Iteration 15/25 | Loss: 0.00088467
Iteration 16/25 | Loss: 0.00088467
Iteration 17/25 | Loss: 0.00088467
Iteration 18/25 | Loss: 0.00088467
Iteration 19/25 | Loss: 0.00088467
Iteration 20/25 | Loss: 0.00088467
Iteration 21/25 | Loss: 0.00088467
Iteration 22/25 | Loss: 0.00088467
Iteration 23/25 | Loss: 0.00088467
Iteration 24/25 | Loss: 0.00088467
Iteration 25/25 | Loss: 0.00088467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088467
Iteration 2/1000 | Loss: 0.00004879
Iteration 3/1000 | Loss: 0.00003626
Iteration 4/1000 | Loss: 0.00003218
Iteration 5/1000 | Loss: 0.00003111
Iteration 6/1000 | Loss: 0.00003017
Iteration 7/1000 | Loss: 0.00002942
Iteration 8/1000 | Loss: 0.00002881
Iteration 9/1000 | Loss: 0.00002843
Iteration 10/1000 | Loss: 0.00002814
Iteration 11/1000 | Loss: 0.00002781
Iteration 12/1000 | Loss: 0.00002753
Iteration 13/1000 | Loss: 0.00002727
Iteration 14/1000 | Loss: 0.00002704
Iteration 15/1000 | Loss: 0.00002702
Iteration 16/1000 | Loss: 0.00002686
Iteration 17/1000 | Loss: 0.00002683
Iteration 18/1000 | Loss: 0.00002683
Iteration 19/1000 | Loss: 0.00002683
Iteration 20/1000 | Loss: 0.00002680
Iteration 21/1000 | Loss: 0.00002680
Iteration 22/1000 | Loss: 0.00002678
Iteration 23/1000 | Loss: 0.00002678
Iteration 24/1000 | Loss: 0.00002677
Iteration 25/1000 | Loss: 0.00002677
Iteration 26/1000 | Loss: 0.00002676
Iteration 27/1000 | Loss: 0.00002676
Iteration 28/1000 | Loss: 0.00002676
Iteration 29/1000 | Loss: 0.00002676
Iteration 30/1000 | Loss: 0.00002676
Iteration 31/1000 | Loss: 0.00002676
Iteration 32/1000 | Loss: 0.00002676
Iteration 33/1000 | Loss: 0.00002676
Iteration 34/1000 | Loss: 0.00002676
Iteration 35/1000 | Loss: 0.00002676
Iteration 36/1000 | Loss: 0.00002675
Iteration 37/1000 | Loss: 0.00002675
Iteration 38/1000 | Loss: 0.00002675
Iteration 39/1000 | Loss: 0.00002675
Iteration 40/1000 | Loss: 0.00002675
Iteration 41/1000 | Loss: 0.00002675
Iteration 42/1000 | Loss: 0.00002675
Iteration 43/1000 | Loss: 0.00002675
Iteration 44/1000 | Loss: 0.00002675
Iteration 45/1000 | Loss: 0.00002675
Iteration 46/1000 | Loss: 0.00002674
Iteration 47/1000 | Loss: 0.00002674
Iteration 48/1000 | Loss: 0.00002674
Iteration 49/1000 | Loss: 0.00002674
Iteration 50/1000 | Loss: 0.00002674
Iteration 51/1000 | Loss: 0.00002673
Iteration 52/1000 | Loss: 0.00002673
Iteration 53/1000 | Loss: 0.00002673
Iteration 54/1000 | Loss: 0.00002673
Iteration 55/1000 | Loss: 0.00002672
Iteration 56/1000 | Loss: 0.00002672
Iteration 57/1000 | Loss: 0.00002672
Iteration 58/1000 | Loss: 0.00002669
Iteration 59/1000 | Loss: 0.00002669
Iteration 60/1000 | Loss: 0.00002668
Iteration 61/1000 | Loss: 0.00002668
Iteration 62/1000 | Loss: 0.00002668
Iteration 63/1000 | Loss: 0.00002668
Iteration 64/1000 | Loss: 0.00002668
Iteration 65/1000 | Loss: 0.00002668
Iteration 66/1000 | Loss: 0.00002668
Iteration 67/1000 | Loss: 0.00002667
Iteration 68/1000 | Loss: 0.00002667
Iteration 69/1000 | Loss: 0.00002666
Iteration 70/1000 | Loss: 0.00002666
Iteration 71/1000 | Loss: 0.00002665
Iteration 72/1000 | Loss: 0.00002665
Iteration 73/1000 | Loss: 0.00002665
Iteration 74/1000 | Loss: 0.00002665
Iteration 75/1000 | Loss: 0.00002665
Iteration 76/1000 | Loss: 0.00002664
Iteration 77/1000 | Loss: 0.00002664
Iteration 78/1000 | Loss: 0.00002664
Iteration 79/1000 | Loss: 0.00002664
Iteration 80/1000 | Loss: 0.00002664
Iteration 81/1000 | Loss: 0.00002661
Iteration 82/1000 | Loss: 0.00002661
Iteration 83/1000 | Loss: 0.00002661
Iteration 84/1000 | Loss: 0.00002660
Iteration 85/1000 | Loss: 0.00002660
Iteration 86/1000 | Loss: 0.00002659
Iteration 87/1000 | Loss: 0.00002659
Iteration 88/1000 | Loss: 0.00002659
Iteration 89/1000 | Loss: 0.00002658
Iteration 90/1000 | Loss: 0.00002658
Iteration 91/1000 | Loss: 0.00002658
Iteration 92/1000 | Loss: 0.00002658
Iteration 93/1000 | Loss: 0.00002657
Iteration 94/1000 | Loss: 0.00002657
Iteration 95/1000 | Loss: 0.00002657
Iteration 96/1000 | Loss: 0.00002657
Iteration 97/1000 | Loss: 0.00002657
Iteration 98/1000 | Loss: 0.00002657
Iteration 99/1000 | Loss: 0.00002656
Iteration 100/1000 | Loss: 0.00002656
Iteration 101/1000 | Loss: 0.00002656
Iteration 102/1000 | Loss: 0.00002656
Iteration 103/1000 | Loss: 0.00002656
Iteration 104/1000 | Loss: 0.00002656
Iteration 105/1000 | Loss: 0.00002656
Iteration 106/1000 | Loss: 0.00002656
Iteration 107/1000 | Loss: 0.00002655
Iteration 108/1000 | Loss: 0.00002655
Iteration 109/1000 | Loss: 0.00002655
Iteration 110/1000 | Loss: 0.00002655
Iteration 111/1000 | Loss: 0.00002655
Iteration 112/1000 | Loss: 0.00002655
Iteration 113/1000 | Loss: 0.00002655
Iteration 114/1000 | Loss: 0.00002655
Iteration 115/1000 | Loss: 0.00002655
Iteration 116/1000 | Loss: 0.00002655
Iteration 117/1000 | Loss: 0.00002655
Iteration 118/1000 | Loss: 0.00002655
Iteration 119/1000 | Loss: 0.00002655
Iteration 120/1000 | Loss: 0.00002655
Iteration 121/1000 | Loss: 0.00002655
Iteration 122/1000 | Loss: 0.00002655
Iteration 123/1000 | Loss: 0.00002655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.6549350877758116e-05, 2.6549350877758116e-05, 2.6549350877758116e-05, 2.6549350877758116e-05, 2.6549350877758116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6549350877758116e-05

Optimization complete. Final v2v error: 4.301375865936279 mm

Highest mean error: 4.532440662384033 mm for frame 76

Lowest mean error: 4.153339385986328 mm for frame 5

Saving results

Total time: 37.59080505371094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400112
Iteration 2/25 | Loss: 0.00134051
Iteration 3/25 | Loss: 0.00128137
Iteration 4/25 | Loss: 0.00126447
Iteration 5/25 | Loss: 0.00125998
Iteration 6/25 | Loss: 0.00125968
Iteration 7/25 | Loss: 0.00125968
Iteration 8/25 | Loss: 0.00125968
Iteration 9/25 | Loss: 0.00125968
Iteration 10/25 | Loss: 0.00125968
Iteration 11/25 | Loss: 0.00125968
Iteration 12/25 | Loss: 0.00125968
Iteration 13/25 | Loss: 0.00125968
Iteration 14/25 | Loss: 0.00125968
Iteration 15/25 | Loss: 0.00125968
Iteration 16/25 | Loss: 0.00125968
Iteration 17/25 | Loss: 0.00125968
Iteration 18/25 | Loss: 0.00125968
Iteration 19/25 | Loss: 0.00125968
Iteration 20/25 | Loss: 0.00125968
Iteration 21/25 | Loss: 0.00125968
Iteration 22/25 | Loss: 0.00125968
Iteration 23/25 | Loss: 0.00125968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012596771121025085, 0.0012596771121025085, 0.0012596771121025085, 0.0012596771121025085, 0.0012596771121025085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012596771121025085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32592893
Iteration 2/25 | Loss: 0.00087486
Iteration 3/25 | Loss: 0.00087486
Iteration 4/25 | Loss: 0.00087486
Iteration 5/25 | Loss: 0.00087486
Iteration 6/25 | Loss: 0.00087486
Iteration 7/25 | Loss: 0.00087486
Iteration 8/25 | Loss: 0.00087486
Iteration 9/25 | Loss: 0.00087486
Iteration 10/25 | Loss: 0.00087486
Iteration 11/25 | Loss: 0.00087486
Iteration 12/25 | Loss: 0.00087486
Iteration 13/25 | Loss: 0.00087486
Iteration 14/25 | Loss: 0.00087486
Iteration 15/25 | Loss: 0.00087486
Iteration 16/25 | Loss: 0.00087486
Iteration 17/25 | Loss: 0.00087486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000874856486916542, 0.000874856486916542, 0.000874856486916542, 0.000874856486916542, 0.000874856486916542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000874856486916542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087486
Iteration 2/1000 | Loss: 0.00002969
Iteration 3/1000 | Loss: 0.00002240
Iteration 4/1000 | Loss: 0.00002117
Iteration 5/1000 | Loss: 0.00002054
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001961
Iteration 8/1000 | Loss: 0.00001934
Iteration 9/1000 | Loss: 0.00001902
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001868
Iteration 12/1000 | Loss: 0.00001859
Iteration 13/1000 | Loss: 0.00001859
Iteration 14/1000 | Loss: 0.00001858
Iteration 15/1000 | Loss: 0.00001844
Iteration 16/1000 | Loss: 0.00001842
Iteration 17/1000 | Loss: 0.00001842
Iteration 18/1000 | Loss: 0.00001840
Iteration 19/1000 | Loss: 0.00001839
Iteration 20/1000 | Loss: 0.00001839
Iteration 21/1000 | Loss: 0.00001838
Iteration 22/1000 | Loss: 0.00001838
Iteration 23/1000 | Loss: 0.00001837
Iteration 24/1000 | Loss: 0.00001836
Iteration 25/1000 | Loss: 0.00001835
Iteration 26/1000 | Loss: 0.00001832
Iteration 27/1000 | Loss: 0.00001832
Iteration 28/1000 | Loss: 0.00001832
Iteration 29/1000 | Loss: 0.00001832
Iteration 30/1000 | Loss: 0.00001825
Iteration 31/1000 | Loss: 0.00001824
Iteration 32/1000 | Loss: 0.00001824
Iteration 33/1000 | Loss: 0.00001824
Iteration 34/1000 | Loss: 0.00001823
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00001821
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001820
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001818
Iteration 44/1000 | Loss: 0.00001815
Iteration 45/1000 | Loss: 0.00001815
Iteration 46/1000 | Loss: 0.00001815
Iteration 47/1000 | Loss: 0.00001815
Iteration 48/1000 | Loss: 0.00001815
Iteration 49/1000 | Loss: 0.00001815
Iteration 50/1000 | Loss: 0.00001815
Iteration 51/1000 | Loss: 0.00001815
Iteration 52/1000 | Loss: 0.00001815
Iteration 53/1000 | Loss: 0.00001815
Iteration 54/1000 | Loss: 0.00001815
Iteration 55/1000 | Loss: 0.00001815
Iteration 56/1000 | Loss: 0.00001815
Iteration 57/1000 | Loss: 0.00001815
Iteration 58/1000 | Loss: 0.00001815
Iteration 59/1000 | Loss: 0.00001815
Iteration 60/1000 | Loss: 0.00001815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.814749339246191e-05, 1.814749339246191e-05, 1.814749339246191e-05, 1.814749339246191e-05, 1.814749339246191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.814749339246191e-05

Optimization complete. Final v2v error: 3.6357665061950684 mm

Highest mean error: 3.9469282627105713 mm for frame 109

Lowest mean error: 3.4918036460876465 mm for frame 11

Saving results

Total time: 33.42954444885254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807473
Iteration 2/25 | Loss: 0.00143413
Iteration 3/25 | Loss: 0.00129128
Iteration 4/25 | Loss: 0.00128043
Iteration 5/25 | Loss: 0.00127697
Iteration 6/25 | Loss: 0.00127654
Iteration 7/25 | Loss: 0.00127654
Iteration 8/25 | Loss: 0.00127654
Iteration 9/25 | Loss: 0.00127654
Iteration 10/25 | Loss: 0.00127654
Iteration 11/25 | Loss: 0.00127654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012765424326062202, 0.0012765424326062202, 0.0012765424326062202, 0.0012765424326062202, 0.0012765424326062202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012765424326062202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.41255236
Iteration 2/25 | Loss: 0.00075443
Iteration 3/25 | Loss: 0.00075437
Iteration 4/25 | Loss: 0.00075437
Iteration 5/25 | Loss: 0.00075436
Iteration 6/25 | Loss: 0.00075436
Iteration 7/25 | Loss: 0.00075436
Iteration 8/25 | Loss: 0.00075436
Iteration 9/25 | Loss: 0.00075436
Iteration 10/25 | Loss: 0.00075436
Iteration 11/25 | Loss: 0.00075436
Iteration 12/25 | Loss: 0.00075436
Iteration 13/25 | Loss: 0.00075436
Iteration 14/25 | Loss: 0.00075436
Iteration 15/25 | Loss: 0.00075436
Iteration 16/25 | Loss: 0.00075436
Iteration 17/25 | Loss: 0.00075436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007543633691966534, 0.0007543633691966534, 0.0007543633691966534, 0.0007543633691966534, 0.0007543633691966534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007543633691966534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075436
Iteration 2/1000 | Loss: 0.00003703
Iteration 3/1000 | Loss: 0.00002551
Iteration 4/1000 | Loss: 0.00002381
Iteration 5/1000 | Loss: 0.00002253
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00002090
Iteration 8/1000 | Loss: 0.00002041
Iteration 9/1000 | Loss: 0.00001994
Iteration 10/1000 | Loss: 0.00001965
Iteration 11/1000 | Loss: 0.00001943
Iteration 12/1000 | Loss: 0.00001934
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001919
Iteration 15/1000 | Loss: 0.00001912
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001896
Iteration 19/1000 | Loss: 0.00001885
Iteration 20/1000 | Loss: 0.00001884
Iteration 21/1000 | Loss: 0.00001881
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00001877
Iteration 24/1000 | Loss: 0.00001876
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001875
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001871
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001865
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001863
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001860
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001859
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001855
Iteration 48/1000 | Loss: 0.00001854
Iteration 49/1000 | Loss: 0.00001854
Iteration 50/1000 | Loss: 0.00001853
Iteration 51/1000 | Loss: 0.00001853
Iteration 52/1000 | Loss: 0.00001852
Iteration 53/1000 | Loss: 0.00001852
Iteration 54/1000 | Loss: 0.00001852
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001852
Iteration 63/1000 | Loss: 0.00001852
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001849
Iteration 71/1000 | Loss: 0.00001849
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001848
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001847
Iteration 78/1000 | Loss: 0.00001847
Iteration 79/1000 | Loss: 0.00001846
Iteration 80/1000 | Loss: 0.00001846
Iteration 81/1000 | Loss: 0.00001846
Iteration 82/1000 | Loss: 0.00001846
Iteration 83/1000 | Loss: 0.00001846
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001845
Iteration 86/1000 | Loss: 0.00001845
Iteration 87/1000 | Loss: 0.00001845
Iteration 88/1000 | Loss: 0.00001845
Iteration 89/1000 | Loss: 0.00001844
Iteration 90/1000 | Loss: 0.00001844
Iteration 91/1000 | Loss: 0.00001844
Iteration 92/1000 | Loss: 0.00001844
Iteration 93/1000 | Loss: 0.00001844
Iteration 94/1000 | Loss: 0.00001844
Iteration 95/1000 | Loss: 0.00001843
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001843
Iteration 98/1000 | Loss: 0.00001843
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Iteration 101/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.8431754142511636e-05, 1.8431754142511636e-05, 1.8431754142511636e-05, 1.8431754142511636e-05, 1.8431754142511636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8431754142511636e-05

Optimization complete. Final v2v error: 3.6322543621063232 mm

Highest mean error: 4.6497673988342285 mm for frame 66

Lowest mean error: 3.074631452560425 mm for frame 173

Saving results

Total time: 42.38704800605774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414162
Iteration 2/25 | Loss: 0.00136162
Iteration 3/25 | Loss: 0.00126968
Iteration 4/25 | Loss: 0.00125593
Iteration 5/25 | Loss: 0.00125088
Iteration 6/25 | Loss: 0.00124985
Iteration 7/25 | Loss: 0.00124985
Iteration 8/25 | Loss: 0.00124985
Iteration 9/25 | Loss: 0.00124985
Iteration 10/25 | Loss: 0.00124985
Iteration 11/25 | Loss: 0.00124985
Iteration 12/25 | Loss: 0.00124985
Iteration 13/25 | Loss: 0.00124985
Iteration 14/25 | Loss: 0.00124985
Iteration 15/25 | Loss: 0.00124985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012498503783717752, 0.0012498503783717752, 0.0012498503783717752, 0.0012498503783717752, 0.0012498503783717752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012498503783717752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12381577
Iteration 2/25 | Loss: 0.00086529
Iteration 3/25 | Loss: 0.00086528
Iteration 4/25 | Loss: 0.00086528
Iteration 5/25 | Loss: 0.00086528
Iteration 6/25 | Loss: 0.00086527
Iteration 7/25 | Loss: 0.00086527
Iteration 8/25 | Loss: 0.00086527
Iteration 9/25 | Loss: 0.00086527
Iteration 10/25 | Loss: 0.00086527
Iteration 11/25 | Loss: 0.00086527
Iteration 12/25 | Loss: 0.00086527
Iteration 13/25 | Loss: 0.00086527
Iteration 14/25 | Loss: 0.00086527
Iteration 15/25 | Loss: 0.00086527
Iteration 16/25 | Loss: 0.00086527
Iteration 17/25 | Loss: 0.00086527
Iteration 18/25 | Loss: 0.00086527
Iteration 19/25 | Loss: 0.00086527
Iteration 20/25 | Loss: 0.00086527
Iteration 21/25 | Loss: 0.00086527
Iteration 22/25 | Loss: 0.00086527
Iteration 23/25 | Loss: 0.00086527
Iteration 24/25 | Loss: 0.00086527
Iteration 25/25 | Loss: 0.00086527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086527
Iteration 2/1000 | Loss: 0.00003356
Iteration 3/1000 | Loss: 0.00002216
Iteration 4/1000 | Loss: 0.00001903
Iteration 5/1000 | Loss: 0.00001778
Iteration 6/1000 | Loss: 0.00001687
Iteration 7/1000 | Loss: 0.00001626
Iteration 8/1000 | Loss: 0.00001581
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001525
Iteration 12/1000 | Loss: 0.00001497
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001479
Iteration 16/1000 | Loss: 0.00001472
Iteration 17/1000 | Loss: 0.00001469
Iteration 18/1000 | Loss: 0.00001468
Iteration 19/1000 | Loss: 0.00001468
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001462
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001459
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001456
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001453
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001451
Iteration 31/1000 | Loss: 0.00001450
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001441
Iteration 41/1000 | Loss: 0.00001441
Iteration 42/1000 | Loss: 0.00001440
Iteration 43/1000 | Loss: 0.00001440
Iteration 44/1000 | Loss: 0.00001440
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001437
Iteration 48/1000 | Loss: 0.00001437
Iteration 49/1000 | Loss: 0.00001437
Iteration 50/1000 | Loss: 0.00001436
Iteration 51/1000 | Loss: 0.00001436
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001435
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001434
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001433
Iteration 58/1000 | Loss: 0.00001433
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001432
Iteration 61/1000 | Loss: 0.00001432
Iteration 62/1000 | Loss: 0.00001431
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001430
Iteration 66/1000 | Loss: 0.00001430
Iteration 67/1000 | Loss: 0.00001429
Iteration 68/1000 | Loss: 0.00001429
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001427
Iteration 71/1000 | Loss: 0.00001427
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001426
Iteration 74/1000 | Loss: 0.00001426
Iteration 75/1000 | Loss: 0.00001426
Iteration 76/1000 | Loss: 0.00001425
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001423
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001421
Iteration 85/1000 | Loss: 0.00001421
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001418
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001417
Iteration 110/1000 | Loss: 0.00001417
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001415
Iteration 120/1000 | Loss: 0.00001415
Iteration 121/1000 | Loss: 0.00001415
Iteration 122/1000 | Loss: 0.00001415
Iteration 123/1000 | Loss: 0.00001415
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001414
Iteration 126/1000 | Loss: 0.00001413
Iteration 127/1000 | Loss: 0.00001413
Iteration 128/1000 | Loss: 0.00001413
Iteration 129/1000 | Loss: 0.00001413
Iteration 130/1000 | Loss: 0.00001412
Iteration 131/1000 | Loss: 0.00001412
Iteration 132/1000 | Loss: 0.00001412
Iteration 133/1000 | Loss: 0.00001412
Iteration 134/1000 | Loss: 0.00001412
Iteration 135/1000 | Loss: 0.00001412
Iteration 136/1000 | Loss: 0.00001412
Iteration 137/1000 | Loss: 0.00001412
Iteration 138/1000 | Loss: 0.00001412
Iteration 139/1000 | Loss: 0.00001412
Iteration 140/1000 | Loss: 0.00001411
Iteration 141/1000 | Loss: 0.00001411
Iteration 142/1000 | Loss: 0.00001411
Iteration 143/1000 | Loss: 0.00001411
Iteration 144/1000 | Loss: 0.00001411
Iteration 145/1000 | Loss: 0.00001411
Iteration 146/1000 | Loss: 0.00001410
Iteration 147/1000 | Loss: 0.00001410
Iteration 148/1000 | Loss: 0.00001410
Iteration 149/1000 | Loss: 0.00001410
Iteration 150/1000 | Loss: 0.00001409
Iteration 151/1000 | Loss: 0.00001409
Iteration 152/1000 | Loss: 0.00001409
Iteration 153/1000 | Loss: 0.00001409
Iteration 154/1000 | Loss: 0.00001409
Iteration 155/1000 | Loss: 0.00001409
Iteration 156/1000 | Loss: 0.00001409
Iteration 157/1000 | Loss: 0.00001408
Iteration 158/1000 | Loss: 0.00001408
Iteration 159/1000 | Loss: 0.00001408
Iteration 160/1000 | Loss: 0.00001408
Iteration 161/1000 | Loss: 0.00001408
Iteration 162/1000 | Loss: 0.00001408
Iteration 163/1000 | Loss: 0.00001408
Iteration 164/1000 | Loss: 0.00001407
Iteration 165/1000 | Loss: 0.00001407
Iteration 166/1000 | Loss: 0.00001407
Iteration 167/1000 | Loss: 0.00001407
Iteration 168/1000 | Loss: 0.00001407
Iteration 169/1000 | Loss: 0.00001406
Iteration 170/1000 | Loss: 0.00001406
Iteration 171/1000 | Loss: 0.00001406
Iteration 172/1000 | Loss: 0.00001406
Iteration 173/1000 | Loss: 0.00001406
Iteration 174/1000 | Loss: 0.00001406
Iteration 175/1000 | Loss: 0.00001406
Iteration 176/1000 | Loss: 0.00001406
Iteration 177/1000 | Loss: 0.00001406
Iteration 178/1000 | Loss: 0.00001405
Iteration 179/1000 | Loss: 0.00001405
Iteration 180/1000 | Loss: 0.00001405
Iteration 181/1000 | Loss: 0.00001405
Iteration 182/1000 | Loss: 0.00001405
Iteration 183/1000 | Loss: 0.00001405
Iteration 184/1000 | Loss: 0.00001405
Iteration 185/1000 | Loss: 0.00001405
Iteration 186/1000 | Loss: 0.00001405
Iteration 187/1000 | Loss: 0.00001405
Iteration 188/1000 | Loss: 0.00001405
Iteration 189/1000 | Loss: 0.00001405
Iteration 190/1000 | Loss: 0.00001405
Iteration 191/1000 | Loss: 0.00001404
Iteration 192/1000 | Loss: 0.00001404
Iteration 193/1000 | Loss: 0.00001404
Iteration 194/1000 | Loss: 0.00001404
Iteration 195/1000 | Loss: 0.00001404
Iteration 196/1000 | Loss: 0.00001404
Iteration 197/1000 | Loss: 0.00001404
Iteration 198/1000 | Loss: 0.00001404
Iteration 199/1000 | Loss: 0.00001404
Iteration 200/1000 | Loss: 0.00001404
Iteration 201/1000 | Loss: 0.00001404
Iteration 202/1000 | Loss: 0.00001403
Iteration 203/1000 | Loss: 0.00001403
Iteration 204/1000 | Loss: 0.00001403
Iteration 205/1000 | Loss: 0.00001403
Iteration 206/1000 | Loss: 0.00001403
Iteration 207/1000 | Loss: 0.00001403
Iteration 208/1000 | Loss: 0.00001402
Iteration 209/1000 | Loss: 0.00001402
Iteration 210/1000 | Loss: 0.00001402
Iteration 211/1000 | Loss: 0.00001402
Iteration 212/1000 | Loss: 0.00001402
Iteration 213/1000 | Loss: 0.00001402
Iteration 214/1000 | Loss: 0.00001402
Iteration 215/1000 | Loss: 0.00001402
Iteration 216/1000 | Loss: 0.00001401
Iteration 217/1000 | Loss: 0.00001401
Iteration 218/1000 | Loss: 0.00001401
Iteration 219/1000 | Loss: 0.00001401
Iteration 220/1000 | Loss: 0.00001401
Iteration 221/1000 | Loss: 0.00001401
Iteration 222/1000 | Loss: 0.00001401
Iteration 223/1000 | Loss: 0.00001401
Iteration 224/1000 | Loss: 0.00001401
Iteration 225/1000 | Loss: 0.00001401
Iteration 226/1000 | Loss: 0.00001401
Iteration 227/1000 | Loss: 0.00001401
Iteration 228/1000 | Loss: 0.00001401
Iteration 229/1000 | Loss: 0.00001401
Iteration 230/1000 | Loss: 0.00001401
Iteration 231/1000 | Loss: 0.00001400
Iteration 232/1000 | Loss: 0.00001400
Iteration 233/1000 | Loss: 0.00001400
Iteration 234/1000 | Loss: 0.00001400
Iteration 235/1000 | Loss: 0.00001400
Iteration 236/1000 | Loss: 0.00001400
Iteration 237/1000 | Loss: 0.00001399
Iteration 238/1000 | Loss: 0.00001399
Iteration 239/1000 | Loss: 0.00001399
Iteration 240/1000 | Loss: 0.00001399
Iteration 241/1000 | Loss: 0.00001399
Iteration 242/1000 | Loss: 0.00001399
Iteration 243/1000 | Loss: 0.00001399
Iteration 244/1000 | Loss: 0.00001399
Iteration 245/1000 | Loss: 0.00001399
Iteration 246/1000 | Loss: 0.00001399
Iteration 247/1000 | Loss: 0.00001399
Iteration 248/1000 | Loss: 0.00001399
Iteration 249/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.399055418005446e-05, 1.399055418005446e-05, 1.399055418005446e-05, 1.399055418005446e-05, 1.399055418005446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.399055418005446e-05

Optimization complete. Final v2v error: 3.207334041595459 mm

Highest mean error: 3.70821475982666 mm for frame 37

Lowest mean error: 2.9438400268554688 mm for frame 26

Saving results

Total time: 45.28675174713135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003720
Iteration 2/25 | Loss: 0.00188760
Iteration 3/25 | Loss: 0.00146242
Iteration 4/25 | Loss: 0.00139770
Iteration 5/25 | Loss: 0.00136246
Iteration 6/25 | Loss: 0.00135310
Iteration 7/25 | Loss: 0.00135080
Iteration 8/25 | Loss: 0.00135018
Iteration 9/25 | Loss: 0.00135011
Iteration 10/25 | Loss: 0.00135011
Iteration 11/25 | Loss: 0.00135011
Iteration 12/25 | Loss: 0.00135011
Iteration 13/25 | Loss: 0.00135011
Iteration 14/25 | Loss: 0.00135011
Iteration 15/25 | Loss: 0.00135011
Iteration 16/25 | Loss: 0.00135011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013501110952347517, 0.0013501110952347517, 0.0013501110952347517, 0.0013501110952347517, 0.0013501110952347517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013501110952347517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40950322
Iteration 2/25 | Loss: 0.00082109
Iteration 3/25 | Loss: 0.00082109
Iteration 4/25 | Loss: 0.00082109
Iteration 5/25 | Loss: 0.00082109
Iteration 6/25 | Loss: 0.00082109
Iteration 7/25 | Loss: 0.00082109
Iteration 8/25 | Loss: 0.00082109
Iteration 9/25 | Loss: 0.00082109
Iteration 10/25 | Loss: 0.00082109
Iteration 11/25 | Loss: 0.00082109
Iteration 12/25 | Loss: 0.00082109
Iteration 13/25 | Loss: 0.00082109
Iteration 14/25 | Loss: 0.00082109
Iteration 15/25 | Loss: 0.00082109
Iteration 16/25 | Loss: 0.00082109
Iteration 17/25 | Loss: 0.00082109
Iteration 18/25 | Loss: 0.00082109
Iteration 19/25 | Loss: 0.00082109
Iteration 20/25 | Loss: 0.00082109
Iteration 21/25 | Loss: 0.00082109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008210854139178991, 0.0008210854139178991, 0.0008210854139178991, 0.0008210854139178991, 0.0008210854139178991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008210854139178991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082109
Iteration 2/1000 | Loss: 0.00003840
Iteration 3/1000 | Loss: 0.00002900
Iteration 4/1000 | Loss: 0.00002694
Iteration 5/1000 | Loss: 0.00002616
Iteration 6/1000 | Loss: 0.00002565
Iteration 7/1000 | Loss: 0.00002523
Iteration 8/1000 | Loss: 0.00002517
Iteration 9/1000 | Loss: 0.00002498
Iteration 10/1000 | Loss: 0.00002482
Iteration 11/1000 | Loss: 0.00002473
Iteration 12/1000 | Loss: 0.00002470
Iteration 13/1000 | Loss: 0.00002469
Iteration 14/1000 | Loss: 0.00002469
Iteration 15/1000 | Loss: 0.00002469
Iteration 16/1000 | Loss: 0.00002459
Iteration 17/1000 | Loss: 0.00002459
Iteration 18/1000 | Loss: 0.00002454
Iteration 19/1000 | Loss: 0.00002450
Iteration 20/1000 | Loss: 0.00002450
Iteration 21/1000 | Loss: 0.00002450
Iteration 22/1000 | Loss: 0.00002449
Iteration 23/1000 | Loss: 0.00002449
Iteration 24/1000 | Loss: 0.00002449
Iteration 25/1000 | Loss: 0.00002449
Iteration 26/1000 | Loss: 0.00002449
Iteration 27/1000 | Loss: 0.00002449
Iteration 28/1000 | Loss: 0.00002449
Iteration 29/1000 | Loss: 0.00002449
Iteration 30/1000 | Loss: 0.00002446
Iteration 31/1000 | Loss: 0.00002446
Iteration 32/1000 | Loss: 0.00002446
Iteration 33/1000 | Loss: 0.00002445
Iteration 34/1000 | Loss: 0.00002445
Iteration 35/1000 | Loss: 0.00002445
Iteration 36/1000 | Loss: 0.00002444
Iteration 37/1000 | Loss: 0.00002443
Iteration 38/1000 | Loss: 0.00002443
Iteration 39/1000 | Loss: 0.00002443
Iteration 40/1000 | Loss: 0.00002442
Iteration 41/1000 | Loss: 0.00002442
Iteration 42/1000 | Loss: 0.00002442
Iteration 43/1000 | Loss: 0.00002442
Iteration 44/1000 | Loss: 0.00002442
Iteration 45/1000 | Loss: 0.00002442
Iteration 46/1000 | Loss: 0.00002442
Iteration 47/1000 | Loss: 0.00002442
Iteration 48/1000 | Loss: 0.00002442
Iteration 49/1000 | Loss: 0.00002442
Iteration 50/1000 | Loss: 0.00002442
Iteration 51/1000 | Loss: 0.00002442
Iteration 52/1000 | Loss: 0.00002441
Iteration 53/1000 | Loss: 0.00002441
Iteration 54/1000 | Loss: 0.00002441
Iteration 55/1000 | Loss: 0.00002441
Iteration 56/1000 | Loss: 0.00002441
Iteration 57/1000 | Loss: 0.00002441
Iteration 58/1000 | Loss: 0.00002441
Iteration 59/1000 | Loss: 0.00002441
Iteration 60/1000 | Loss: 0.00002440
Iteration 61/1000 | Loss: 0.00002440
Iteration 62/1000 | Loss: 0.00002440
Iteration 63/1000 | Loss: 0.00002440
Iteration 64/1000 | Loss: 0.00002440
Iteration 65/1000 | Loss: 0.00002440
Iteration 66/1000 | Loss: 0.00002440
Iteration 67/1000 | Loss: 0.00002440
Iteration 68/1000 | Loss: 0.00002440
Iteration 69/1000 | Loss: 0.00002440
Iteration 70/1000 | Loss: 0.00002439
Iteration 71/1000 | Loss: 0.00002439
Iteration 72/1000 | Loss: 0.00002439
Iteration 73/1000 | Loss: 0.00002439
Iteration 74/1000 | Loss: 0.00002439
Iteration 75/1000 | Loss: 0.00002439
Iteration 76/1000 | Loss: 0.00002439
Iteration 77/1000 | Loss: 0.00002439
Iteration 78/1000 | Loss: 0.00002439
Iteration 79/1000 | Loss: 0.00002439
Iteration 80/1000 | Loss: 0.00002439
Iteration 81/1000 | Loss: 0.00002439
Iteration 82/1000 | Loss: 0.00002438
Iteration 83/1000 | Loss: 0.00002438
Iteration 84/1000 | Loss: 0.00002438
Iteration 85/1000 | Loss: 0.00002438
Iteration 86/1000 | Loss: 0.00002438
Iteration 87/1000 | Loss: 0.00002438
Iteration 88/1000 | Loss: 0.00002438
Iteration 89/1000 | Loss: 0.00002438
Iteration 90/1000 | Loss: 0.00002438
Iteration 91/1000 | Loss: 0.00002438
Iteration 92/1000 | Loss: 0.00002438
Iteration 93/1000 | Loss: 0.00002438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.4381197363254614e-05, 2.4381197363254614e-05, 2.4381197363254614e-05, 2.4381197363254614e-05, 2.4381197363254614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4381197363254614e-05

Optimization complete. Final v2v error: 4.159358024597168 mm

Highest mean error: 4.353947162628174 mm for frame 23

Lowest mean error: 4.03169059753418 mm for frame 106

Saving results

Total time: 33.61690974235535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000275
Iteration 2/25 | Loss: 0.00292115
Iteration 3/25 | Loss: 0.00222440
Iteration 4/25 | Loss: 0.00202214
Iteration 5/25 | Loss: 0.00192916
Iteration 6/25 | Loss: 0.00193843
Iteration 7/25 | Loss: 0.00181799
Iteration 8/25 | Loss: 0.00162921
Iteration 9/25 | Loss: 0.00156728
Iteration 10/25 | Loss: 0.00154373
Iteration 11/25 | Loss: 0.00154013
Iteration 12/25 | Loss: 0.00150357
Iteration 13/25 | Loss: 0.00151811
Iteration 14/25 | Loss: 0.00149020
Iteration 15/25 | Loss: 0.00148558
Iteration 16/25 | Loss: 0.00148129
Iteration 17/25 | Loss: 0.00147759
Iteration 18/25 | Loss: 0.00146625
Iteration 19/25 | Loss: 0.00146789
Iteration 20/25 | Loss: 0.00146428
Iteration 21/25 | Loss: 0.00146651
Iteration 22/25 | Loss: 0.00145827
Iteration 23/25 | Loss: 0.00145680
Iteration 24/25 | Loss: 0.00145672
Iteration 25/25 | Loss: 0.00145672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53443980
Iteration 2/25 | Loss: 0.00420116
Iteration 3/25 | Loss: 0.00308314
Iteration 4/25 | Loss: 0.00308313
Iteration 5/25 | Loss: 0.00308313
Iteration 6/25 | Loss: 0.00308313
Iteration 7/25 | Loss: 0.00308313
Iteration 8/25 | Loss: 0.00308313
Iteration 9/25 | Loss: 0.00308313
Iteration 10/25 | Loss: 0.00308313
Iteration 11/25 | Loss: 0.00308313
Iteration 12/25 | Loss: 0.00308313
Iteration 13/25 | Loss: 0.00308313
Iteration 14/25 | Loss: 0.00308313
Iteration 15/25 | Loss: 0.00308313
Iteration 16/25 | Loss: 0.00308313
Iteration 17/25 | Loss: 0.00308313
Iteration 18/25 | Loss: 0.00308313
Iteration 19/25 | Loss: 0.00308313
Iteration 20/25 | Loss: 0.00308313
Iteration 21/25 | Loss: 0.00308313
Iteration 22/25 | Loss: 0.00308313
Iteration 23/25 | Loss: 0.00308313
Iteration 24/25 | Loss: 0.00308313
Iteration 25/25 | Loss: 0.00308313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00308313
Iteration 2/1000 | Loss: 0.00113542
Iteration 3/1000 | Loss: 0.00036905
Iteration 4/1000 | Loss: 0.00135543
Iteration 5/1000 | Loss: 0.00061457
Iteration 6/1000 | Loss: 0.00041940
Iteration 7/1000 | Loss: 0.00015459
Iteration 8/1000 | Loss: 0.00014059
Iteration 9/1000 | Loss: 0.00030445
Iteration 10/1000 | Loss: 0.00097670
Iteration 11/1000 | Loss: 0.00301326
Iteration 12/1000 | Loss: 0.00012902
Iteration 13/1000 | Loss: 0.00012415
Iteration 14/1000 | Loss: 0.00046645
Iteration 15/1000 | Loss: 0.00041737
Iteration 16/1000 | Loss: 0.00011942
Iteration 17/1000 | Loss: 0.00039977
Iteration 18/1000 | Loss: 0.00016862
Iteration 19/1000 | Loss: 0.00011325
Iteration 20/1000 | Loss: 0.00011050
Iteration 21/1000 | Loss: 0.00039527
Iteration 22/1000 | Loss: 0.00048967
Iteration 23/1000 | Loss: 0.00021630
Iteration 24/1000 | Loss: 0.00029216
Iteration 25/1000 | Loss: 0.00010501
Iteration 26/1000 | Loss: 0.00033873
Iteration 27/1000 | Loss: 0.00010407
Iteration 28/1000 | Loss: 0.00010241
Iteration 29/1000 | Loss: 0.00031202
Iteration 30/1000 | Loss: 0.00010164
Iteration 31/1000 | Loss: 0.00060235
Iteration 32/1000 | Loss: 0.00165849
Iteration 33/1000 | Loss: 0.00146852
Iteration 34/1000 | Loss: 0.00101710
Iteration 35/1000 | Loss: 0.00055472
Iteration 36/1000 | Loss: 0.00019844
Iteration 37/1000 | Loss: 0.00015608
Iteration 38/1000 | Loss: 0.00009998
Iteration 39/1000 | Loss: 0.00036801
Iteration 40/1000 | Loss: 0.00046432
Iteration 41/1000 | Loss: 0.00009559
Iteration 42/1000 | Loss: 0.00009149
Iteration 43/1000 | Loss: 0.00044345
Iteration 44/1000 | Loss: 0.00092924
Iteration 45/1000 | Loss: 0.00009140
Iteration 46/1000 | Loss: 0.00013468
Iteration 47/1000 | Loss: 0.00008936
Iteration 48/1000 | Loss: 0.00013098
Iteration 49/1000 | Loss: 0.00008853
Iteration 50/1000 | Loss: 0.00014888
Iteration 51/1000 | Loss: 0.00008762
Iteration 52/1000 | Loss: 0.00008733
Iteration 53/1000 | Loss: 0.00046812
Iteration 54/1000 | Loss: 0.00008703
Iteration 55/1000 | Loss: 0.00008656
Iteration 56/1000 | Loss: 0.00008638
Iteration 57/1000 | Loss: 0.00008620
Iteration 58/1000 | Loss: 0.00008610
Iteration 59/1000 | Loss: 0.00008606
Iteration 60/1000 | Loss: 0.00008606
Iteration 61/1000 | Loss: 0.00008600
Iteration 62/1000 | Loss: 0.00008596
Iteration 63/1000 | Loss: 0.00008595
Iteration 64/1000 | Loss: 0.00008595
Iteration 65/1000 | Loss: 0.00008595
Iteration 66/1000 | Loss: 0.00008595
Iteration 67/1000 | Loss: 0.00008595
Iteration 68/1000 | Loss: 0.00008591
Iteration 69/1000 | Loss: 0.00008591
Iteration 70/1000 | Loss: 0.00008589
Iteration 71/1000 | Loss: 0.00008585
Iteration 72/1000 | Loss: 0.00008584
Iteration 73/1000 | Loss: 0.00008584
Iteration 74/1000 | Loss: 0.00008584
Iteration 75/1000 | Loss: 0.00008583
Iteration 76/1000 | Loss: 0.00008580
Iteration 77/1000 | Loss: 0.00008579
Iteration 78/1000 | Loss: 0.00008579
Iteration 79/1000 | Loss: 0.00008578
Iteration 80/1000 | Loss: 0.00008578
Iteration 81/1000 | Loss: 0.00008578
Iteration 82/1000 | Loss: 0.00008578
Iteration 83/1000 | Loss: 0.00008578
Iteration 84/1000 | Loss: 0.00008578
Iteration 85/1000 | Loss: 0.00008578
Iteration 86/1000 | Loss: 0.00008577
Iteration 87/1000 | Loss: 0.00008577
Iteration 88/1000 | Loss: 0.00008577
Iteration 89/1000 | Loss: 0.00008577
Iteration 90/1000 | Loss: 0.00008577
Iteration 91/1000 | Loss: 0.00008577
Iteration 92/1000 | Loss: 0.00008576
Iteration 93/1000 | Loss: 0.00008576
Iteration 94/1000 | Loss: 0.00008576
Iteration 95/1000 | Loss: 0.00008576
Iteration 96/1000 | Loss: 0.00008576
Iteration 97/1000 | Loss: 0.00008576
Iteration 98/1000 | Loss: 0.00008576
Iteration 99/1000 | Loss: 0.00008575
Iteration 100/1000 | Loss: 0.00008575
Iteration 101/1000 | Loss: 0.00008575
Iteration 102/1000 | Loss: 0.00008574
Iteration 103/1000 | Loss: 0.00008574
Iteration 104/1000 | Loss: 0.00008574
Iteration 105/1000 | Loss: 0.00008574
Iteration 106/1000 | Loss: 0.00008574
Iteration 107/1000 | Loss: 0.00008573
Iteration 108/1000 | Loss: 0.00008573
Iteration 109/1000 | Loss: 0.00008572
Iteration 110/1000 | Loss: 0.00008572
Iteration 111/1000 | Loss: 0.00008572
Iteration 112/1000 | Loss: 0.00008572
Iteration 113/1000 | Loss: 0.00008572
Iteration 114/1000 | Loss: 0.00008572
Iteration 115/1000 | Loss: 0.00008571
Iteration 116/1000 | Loss: 0.00008571
Iteration 117/1000 | Loss: 0.00008571
Iteration 118/1000 | Loss: 0.00008571
Iteration 119/1000 | Loss: 0.00008571
Iteration 120/1000 | Loss: 0.00008571
Iteration 121/1000 | Loss: 0.00008571
Iteration 122/1000 | Loss: 0.00008570
Iteration 123/1000 | Loss: 0.00008570
Iteration 124/1000 | Loss: 0.00008570
Iteration 125/1000 | Loss: 0.00008570
Iteration 126/1000 | Loss: 0.00008570
Iteration 127/1000 | Loss: 0.00008570
Iteration 128/1000 | Loss: 0.00008569
Iteration 129/1000 | Loss: 0.00008569
Iteration 130/1000 | Loss: 0.00008569
Iteration 131/1000 | Loss: 0.00008569
Iteration 132/1000 | Loss: 0.00008569
Iteration 133/1000 | Loss: 0.00008568
Iteration 134/1000 | Loss: 0.00008568
Iteration 135/1000 | Loss: 0.00008568
Iteration 136/1000 | Loss: 0.00008568
Iteration 137/1000 | Loss: 0.00008567
Iteration 138/1000 | Loss: 0.00008567
Iteration 139/1000 | Loss: 0.00008567
Iteration 140/1000 | Loss: 0.00008566
Iteration 141/1000 | Loss: 0.00008566
Iteration 142/1000 | Loss: 0.00008566
Iteration 143/1000 | Loss: 0.00008566
Iteration 144/1000 | Loss: 0.00008566
Iteration 145/1000 | Loss: 0.00008566
Iteration 146/1000 | Loss: 0.00008565
Iteration 147/1000 | Loss: 0.00008565
Iteration 148/1000 | Loss: 0.00008565
Iteration 149/1000 | Loss: 0.00008565
Iteration 150/1000 | Loss: 0.00008565
Iteration 151/1000 | Loss: 0.00008565
Iteration 152/1000 | Loss: 0.00008565
Iteration 153/1000 | Loss: 0.00008565
Iteration 154/1000 | Loss: 0.00008565
Iteration 155/1000 | Loss: 0.00008564
Iteration 156/1000 | Loss: 0.00008564
Iteration 157/1000 | Loss: 0.00008564
Iteration 158/1000 | Loss: 0.00008564
Iteration 159/1000 | Loss: 0.00008564
Iteration 160/1000 | Loss: 0.00008564
Iteration 161/1000 | Loss: 0.00008564
Iteration 162/1000 | Loss: 0.00008564
Iteration 163/1000 | Loss: 0.00008564
Iteration 164/1000 | Loss: 0.00008564
Iteration 165/1000 | Loss: 0.00008564
Iteration 166/1000 | Loss: 0.00008563
Iteration 167/1000 | Loss: 0.00008563
Iteration 168/1000 | Loss: 0.00008563
Iteration 169/1000 | Loss: 0.00008563
Iteration 170/1000 | Loss: 0.00008563
Iteration 171/1000 | Loss: 0.00008563
Iteration 172/1000 | Loss: 0.00008563
Iteration 173/1000 | Loss: 0.00008563
Iteration 174/1000 | Loss: 0.00008562
Iteration 175/1000 | Loss: 0.00008562
Iteration 176/1000 | Loss: 0.00008562
Iteration 177/1000 | Loss: 0.00008562
Iteration 178/1000 | Loss: 0.00008562
Iteration 179/1000 | Loss: 0.00008562
Iteration 180/1000 | Loss: 0.00008562
Iteration 181/1000 | Loss: 0.00008562
Iteration 182/1000 | Loss: 0.00008562
Iteration 183/1000 | Loss: 0.00008562
Iteration 184/1000 | Loss: 0.00008562
Iteration 185/1000 | Loss: 0.00008562
Iteration 186/1000 | Loss: 0.00008562
Iteration 187/1000 | Loss: 0.00008562
Iteration 188/1000 | Loss: 0.00008562
Iteration 189/1000 | Loss: 0.00008562
Iteration 190/1000 | Loss: 0.00008562
Iteration 191/1000 | Loss: 0.00008561
Iteration 192/1000 | Loss: 0.00008561
Iteration 193/1000 | Loss: 0.00008561
Iteration 194/1000 | Loss: 0.00008561
Iteration 195/1000 | Loss: 0.00008561
Iteration 196/1000 | Loss: 0.00008561
Iteration 197/1000 | Loss: 0.00008561
Iteration 198/1000 | Loss: 0.00008561
Iteration 199/1000 | Loss: 0.00008561
Iteration 200/1000 | Loss: 0.00008561
Iteration 201/1000 | Loss: 0.00008561
Iteration 202/1000 | Loss: 0.00008561
Iteration 203/1000 | Loss: 0.00008561
Iteration 204/1000 | Loss: 0.00008560
Iteration 205/1000 | Loss: 0.00008560
Iteration 206/1000 | Loss: 0.00008560
Iteration 207/1000 | Loss: 0.00008560
Iteration 208/1000 | Loss: 0.00008560
Iteration 209/1000 | Loss: 0.00008560
Iteration 210/1000 | Loss: 0.00008560
Iteration 211/1000 | Loss: 0.00008560
Iteration 212/1000 | Loss: 0.00008560
Iteration 213/1000 | Loss: 0.00008560
Iteration 214/1000 | Loss: 0.00008560
Iteration 215/1000 | Loss: 0.00008560
Iteration 216/1000 | Loss: 0.00008560
Iteration 217/1000 | Loss: 0.00008560
Iteration 218/1000 | Loss: 0.00008560
Iteration 219/1000 | Loss: 0.00008560
Iteration 220/1000 | Loss: 0.00008560
Iteration 221/1000 | Loss: 0.00008560
Iteration 222/1000 | Loss: 0.00008560
Iteration 223/1000 | Loss: 0.00008560
Iteration 224/1000 | Loss: 0.00008560
Iteration 225/1000 | Loss: 0.00008560
Iteration 226/1000 | Loss: 0.00008560
Iteration 227/1000 | Loss: 0.00008560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [8.56006154208444e-05, 8.56006154208444e-05, 8.56006154208444e-05, 8.56006154208444e-05, 8.56006154208444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.56006154208444e-05

Optimization complete. Final v2v error: 5.092767715454102 mm

Highest mean error: 11.988269805908203 mm for frame 18

Lowest mean error: 3.5113766193389893 mm for frame 7

Saving results

Total time: 134.68595504760742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499728
Iteration 2/25 | Loss: 0.00137015
Iteration 3/25 | Loss: 0.00128732
Iteration 4/25 | Loss: 0.00126974
Iteration 5/25 | Loss: 0.00126573
Iteration 6/25 | Loss: 0.00126444
Iteration 7/25 | Loss: 0.00126444
Iteration 8/25 | Loss: 0.00126444
Iteration 9/25 | Loss: 0.00126444
Iteration 10/25 | Loss: 0.00126444
Iteration 11/25 | Loss: 0.00126444
Iteration 12/25 | Loss: 0.00126444
Iteration 13/25 | Loss: 0.00126444
Iteration 14/25 | Loss: 0.00126444
Iteration 15/25 | Loss: 0.00126444
Iteration 16/25 | Loss: 0.00126444
Iteration 17/25 | Loss: 0.00126444
Iteration 18/25 | Loss: 0.00126444
Iteration 19/25 | Loss: 0.00126444
Iteration 20/25 | Loss: 0.00126444
Iteration 21/25 | Loss: 0.00126444
Iteration 22/25 | Loss: 0.00126444
Iteration 23/25 | Loss: 0.00126444
Iteration 24/25 | Loss: 0.00126444
Iteration 25/25 | Loss: 0.00126444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03986144
Iteration 2/25 | Loss: 0.00061574
Iteration 3/25 | Loss: 0.00061568
Iteration 4/25 | Loss: 0.00061568
Iteration 5/25 | Loss: 0.00061568
Iteration 6/25 | Loss: 0.00061568
Iteration 7/25 | Loss: 0.00061568
Iteration 8/25 | Loss: 0.00061568
Iteration 9/25 | Loss: 0.00061568
Iteration 10/25 | Loss: 0.00061568
Iteration 11/25 | Loss: 0.00061568
Iteration 12/25 | Loss: 0.00061568
Iteration 13/25 | Loss: 0.00061568
Iteration 14/25 | Loss: 0.00061568
Iteration 15/25 | Loss: 0.00061568
Iteration 16/25 | Loss: 0.00061568
Iteration 17/25 | Loss: 0.00061568
Iteration 18/25 | Loss: 0.00061568
Iteration 19/25 | Loss: 0.00061568
Iteration 20/25 | Loss: 0.00061568
Iteration 21/25 | Loss: 0.00061568
Iteration 22/25 | Loss: 0.00061568
Iteration 23/25 | Loss: 0.00061568
Iteration 24/25 | Loss: 0.00061568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006156793097034097, 0.0006156793097034097, 0.0006156793097034097, 0.0006156793097034097, 0.0006156793097034097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006156793097034097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061568
Iteration 2/1000 | Loss: 0.00004992
Iteration 3/1000 | Loss: 0.00003302
Iteration 4/1000 | Loss: 0.00002722
Iteration 5/1000 | Loss: 0.00002571
Iteration 6/1000 | Loss: 0.00002452
Iteration 7/1000 | Loss: 0.00002367
Iteration 8/1000 | Loss: 0.00002298
Iteration 9/1000 | Loss: 0.00002250
Iteration 10/1000 | Loss: 0.00002203
Iteration 11/1000 | Loss: 0.00002203
Iteration 12/1000 | Loss: 0.00002202
Iteration 13/1000 | Loss: 0.00002177
Iteration 14/1000 | Loss: 0.00002141
Iteration 15/1000 | Loss: 0.00002102
Iteration 16/1000 | Loss: 0.00002089
Iteration 17/1000 | Loss: 0.00002052
Iteration 18/1000 | Loss: 0.00002031
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002017
Iteration 21/1000 | Loss: 0.00002017
Iteration 22/1000 | Loss: 0.00002013
Iteration 23/1000 | Loss: 0.00002012
Iteration 24/1000 | Loss: 0.00002012
Iteration 25/1000 | Loss: 0.00002011
Iteration 26/1000 | Loss: 0.00002010
Iteration 27/1000 | Loss: 0.00002010
Iteration 28/1000 | Loss: 0.00002009
Iteration 29/1000 | Loss: 0.00002009
Iteration 30/1000 | Loss: 0.00002000
Iteration 31/1000 | Loss: 0.00001999
Iteration 32/1000 | Loss: 0.00001999
Iteration 33/1000 | Loss: 0.00001999
Iteration 34/1000 | Loss: 0.00001999
Iteration 35/1000 | Loss: 0.00001999
Iteration 36/1000 | Loss: 0.00001999
Iteration 37/1000 | Loss: 0.00001999
Iteration 38/1000 | Loss: 0.00001991
Iteration 39/1000 | Loss: 0.00001991
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001989
Iteration 42/1000 | Loss: 0.00001989
Iteration 43/1000 | Loss: 0.00001988
Iteration 44/1000 | Loss: 0.00001988
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001987
Iteration 47/1000 | Loss: 0.00001987
Iteration 48/1000 | Loss: 0.00001987
Iteration 49/1000 | Loss: 0.00001987
Iteration 50/1000 | Loss: 0.00001986
Iteration 51/1000 | Loss: 0.00001986
Iteration 52/1000 | Loss: 0.00001986
Iteration 53/1000 | Loss: 0.00001983
Iteration 54/1000 | Loss: 0.00001983
Iteration 55/1000 | Loss: 0.00001983
Iteration 56/1000 | Loss: 0.00001983
Iteration 57/1000 | Loss: 0.00001983
Iteration 58/1000 | Loss: 0.00001983
Iteration 59/1000 | Loss: 0.00001983
Iteration 60/1000 | Loss: 0.00001983
Iteration 61/1000 | Loss: 0.00001983
Iteration 62/1000 | Loss: 0.00001983
Iteration 63/1000 | Loss: 0.00001983
Iteration 64/1000 | Loss: 0.00001983
Iteration 65/1000 | Loss: 0.00001983
Iteration 66/1000 | Loss: 0.00001983
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001983
Iteration 70/1000 | Loss: 0.00001983
Iteration 71/1000 | Loss: 0.00001983
Iteration 72/1000 | Loss: 0.00001983
Iteration 73/1000 | Loss: 0.00001983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.98283505596919e-05, 1.98283505596919e-05, 1.98283505596919e-05, 1.98283505596919e-05, 1.98283505596919e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.98283505596919e-05

Optimization complete. Final v2v error: 3.7685675621032715 mm

Highest mean error: 3.796849489212036 mm for frame 2

Lowest mean error: 3.7424750328063965 mm for frame 55

Saving results

Total time: 36.87692332267761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00587006
Iteration 2/25 | Loss: 0.00131825
Iteration 3/25 | Loss: 0.00127841
Iteration 4/25 | Loss: 0.00127370
Iteration 5/25 | Loss: 0.00127245
Iteration 6/25 | Loss: 0.00127245
Iteration 7/25 | Loss: 0.00127245
Iteration 8/25 | Loss: 0.00127245
Iteration 9/25 | Loss: 0.00127245
Iteration 10/25 | Loss: 0.00127245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012724504340440035, 0.0012724504340440035, 0.0012724504340440035, 0.0012724504340440035, 0.0012724504340440035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012724504340440035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.42981434
Iteration 2/25 | Loss: 0.00089134
Iteration 3/25 | Loss: 0.00089127
Iteration 4/25 | Loss: 0.00089127
Iteration 5/25 | Loss: 0.00089127
Iteration 6/25 | Loss: 0.00089127
Iteration 7/25 | Loss: 0.00089126
Iteration 8/25 | Loss: 0.00089126
Iteration 9/25 | Loss: 0.00089126
Iteration 10/25 | Loss: 0.00089126
Iteration 11/25 | Loss: 0.00089126
Iteration 12/25 | Loss: 0.00089126
Iteration 13/25 | Loss: 0.00089126
Iteration 14/25 | Loss: 0.00089126
Iteration 15/25 | Loss: 0.00089126
Iteration 16/25 | Loss: 0.00089126
Iteration 17/25 | Loss: 0.00089126
Iteration 18/25 | Loss: 0.00089126
Iteration 19/25 | Loss: 0.00089126
Iteration 20/25 | Loss: 0.00089126
Iteration 21/25 | Loss: 0.00089126
Iteration 22/25 | Loss: 0.00089126
Iteration 23/25 | Loss: 0.00089126
Iteration 24/25 | Loss: 0.00089126
Iteration 25/25 | Loss: 0.00089126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089126
Iteration 2/1000 | Loss: 0.00003053
Iteration 3/1000 | Loss: 0.00001895
Iteration 4/1000 | Loss: 0.00001662
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001460
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001413
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001384
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001357
Iteration 15/1000 | Loss: 0.00001348
Iteration 16/1000 | Loss: 0.00001346
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001341
Iteration 21/1000 | Loss: 0.00001341
Iteration 22/1000 | Loss: 0.00001341
Iteration 23/1000 | Loss: 0.00001341
Iteration 24/1000 | Loss: 0.00001341
Iteration 25/1000 | Loss: 0.00001340
Iteration 26/1000 | Loss: 0.00001340
Iteration 27/1000 | Loss: 0.00001340
Iteration 28/1000 | Loss: 0.00001340
Iteration 29/1000 | Loss: 0.00001339
Iteration 30/1000 | Loss: 0.00001339
Iteration 31/1000 | Loss: 0.00001339
Iteration 32/1000 | Loss: 0.00001337
Iteration 33/1000 | Loss: 0.00001337
Iteration 34/1000 | Loss: 0.00001336
Iteration 35/1000 | Loss: 0.00001336
Iteration 36/1000 | Loss: 0.00001335
Iteration 37/1000 | Loss: 0.00001335
Iteration 38/1000 | Loss: 0.00001335
Iteration 39/1000 | Loss: 0.00001335
Iteration 40/1000 | Loss: 0.00001334
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001333
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001331
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001328
Iteration 54/1000 | Loss: 0.00001326
Iteration 55/1000 | Loss: 0.00001326
Iteration 56/1000 | Loss: 0.00001326
Iteration 57/1000 | Loss: 0.00001326
Iteration 58/1000 | Loss: 0.00001326
Iteration 59/1000 | Loss: 0.00001325
Iteration 60/1000 | Loss: 0.00001325
Iteration 61/1000 | Loss: 0.00001325
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001325
Iteration 65/1000 | Loss: 0.00001325
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001322
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001321
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001319
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001319
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001319
Iteration 82/1000 | Loss: 0.00001319
Iteration 83/1000 | Loss: 0.00001318
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001318
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001315
Iteration 91/1000 | Loss: 0.00001315
Iteration 92/1000 | Loss: 0.00001314
Iteration 93/1000 | Loss: 0.00001314
Iteration 94/1000 | Loss: 0.00001314
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001309
Iteration 110/1000 | Loss: 0.00001309
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001307
Iteration 117/1000 | Loss: 0.00001307
Iteration 118/1000 | Loss: 0.00001307
Iteration 119/1000 | Loss: 0.00001307
Iteration 120/1000 | Loss: 0.00001307
Iteration 121/1000 | Loss: 0.00001307
Iteration 122/1000 | Loss: 0.00001307
Iteration 123/1000 | Loss: 0.00001306
Iteration 124/1000 | Loss: 0.00001306
Iteration 125/1000 | Loss: 0.00001306
Iteration 126/1000 | Loss: 0.00001306
Iteration 127/1000 | Loss: 0.00001306
Iteration 128/1000 | Loss: 0.00001306
Iteration 129/1000 | Loss: 0.00001306
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001305
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001305
Iteration 135/1000 | Loss: 0.00001305
Iteration 136/1000 | Loss: 0.00001305
Iteration 137/1000 | Loss: 0.00001305
Iteration 138/1000 | Loss: 0.00001305
Iteration 139/1000 | Loss: 0.00001305
Iteration 140/1000 | Loss: 0.00001305
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001304
Iteration 144/1000 | Loss: 0.00001304
Iteration 145/1000 | Loss: 0.00001304
Iteration 146/1000 | Loss: 0.00001304
Iteration 147/1000 | Loss: 0.00001304
Iteration 148/1000 | Loss: 0.00001303
Iteration 149/1000 | Loss: 0.00001303
Iteration 150/1000 | Loss: 0.00001303
Iteration 151/1000 | Loss: 0.00001302
Iteration 152/1000 | Loss: 0.00001302
Iteration 153/1000 | Loss: 0.00001302
Iteration 154/1000 | Loss: 0.00001302
Iteration 155/1000 | Loss: 0.00001302
Iteration 156/1000 | Loss: 0.00001302
Iteration 157/1000 | Loss: 0.00001302
Iteration 158/1000 | Loss: 0.00001302
Iteration 159/1000 | Loss: 0.00001302
Iteration 160/1000 | Loss: 0.00001302
Iteration 161/1000 | Loss: 0.00001302
Iteration 162/1000 | Loss: 0.00001301
Iteration 163/1000 | Loss: 0.00001301
Iteration 164/1000 | Loss: 0.00001301
Iteration 165/1000 | Loss: 0.00001301
Iteration 166/1000 | Loss: 0.00001301
Iteration 167/1000 | Loss: 0.00001301
Iteration 168/1000 | Loss: 0.00001301
Iteration 169/1000 | Loss: 0.00001300
Iteration 170/1000 | Loss: 0.00001300
Iteration 171/1000 | Loss: 0.00001300
Iteration 172/1000 | Loss: 0.00001300
Iteration 173/1000 | Loss: 0.00001300
Iteration 174/1000 | Loss: 0.00001300
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001300
Iteration 178/1000 | Loss: 0.00001299
Iteration 179/1000 | Loss: 0.00001299
Iteration 180/1000 | Loss: 0.00001299
Iteration 181/1000 | Loss: 0.00001299
Iteration 182/1000 | Loss: 0.00001299
Iteration 183/1000 | Loss: 0.00001299
Iteration 184/1000 | Loss: 0.00001299
Iteration 185/1000 | Loss: 0.00001299
Iteration 186/1000 | Loss: 0.00001299
Iteration 187/1000 | Loss: 0.00001299
Iteration 188/1000 | Loss: 0.00001299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.2990682080271654e-05, 1.2990682080271654e-05, 1.2990682080271654e-05, 1.2990682080271654e-05, 1.2990682080271654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2990682080271654e-05

Optimization complete. Final v2v error: 3.0591394901275635 mm

Highest mean error: 3.4016215801239014 mm for frame 104

Lowest mean error: 2.810462236404419 mm for frame 29

Saving results

Total time: 41.16766405105591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908933
Iteration 2/25 | Loss: 0.00157767
Iteration 3/25 | Loss: 0.00142441
Iteration 4/25 | Loss: 0.00139601
Iteration 5/25 | Loss: 0.00135298
Iteration 6/25 | Loss: 0.00135244
Iteration 7/25 | Loss: 0.00135414
Iteration 8/25 | Loss: 0.00134091
Iteration 9/25 | Loss: 0.00133497
Iteration 10/25 | Loss: 0.00133399
Iteration 11/25 | Loss: 0.00133372
Iteration 12/25 | Loss: 0.00133345
Iteration 13/25 | Loss: 0.00133267
Iteration 14/25 | Loss: 0.00133597
Iteration 15/25 | Loss: 0.00133630
Iteration 16/25 | Loss: 0.00133612
Iteration 17/25 | Loss: 0.00133574
Iteration 18/25 | Loss: 0.00133525
Iteration 19/25 | Loss: 0.00133219
Iteration 20/25 | Loss: 0.00133506
Iteration 21/25 | Loss: 0.00133611
Iteration 22/25 | Loss: 0.00133603
Iteration 23/25 | Loss: 0.00133586
Iteration 24/25 | Loss: 0.00133405
Iteration 25/25 | Loss: 0.00133567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35609651
Iteration 2/25 | Loss: 0.00083646
Iteration 3/25 | Loss: 0.00083646
Iteration 4/25 | Loss: 0.00083646
Iteration 5/25 | Loss: 0.00083646
Iteration 6/25 | Loss: 0.00083646
Iteration 7/25 | Loss: 0.00083645
Iteration 8/25 | Loss: 0.00083645
Iteration 9/25 | Loss: 0.00083645
Iteration 10/25 | Loss: 0.00083645
Iteration 11/25 | Loss: 0.00083645
Iteration 12/25 | Loss: 0.00083645
Iteration 13/25 | Loss: 0.00083645
Iteration 14/25 | Loss: 0.00083645
Iteration 15/25 | Loss: 0.00083645
Iteration 16/25 | Loss: 0.00083645
Iteration 17/25 | Loss: 0.00083645
Iteration 18/25 | Loss: 0.00083645
Iteration 19/25 | Loss: 0.00083645
Iteration 20/25 | Loss: 0.00083645
Iteration 21/25 | Loss: 0.00083645
Iteration 22/25 | Loss: 0.00083645
Iteration 23/25 | Loss: 0.00083645
Iteration 24/25 | Loss: 0.00083645
Iteration 25/25 | Loss: 0.00083645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008364533423446119, 0.0008364533423446119, 0.0008364533423446119, 0.0008364533423446119, 0.0008364533423446119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008364533423446119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083645
Iteration 2/1000 | Loss: 0.00012952
Iteration 3/1000 | Loss: 0.00011550
Iteration 4/1000 | Loss: 0.00014715
Iteration 5/1000 | Loss: 0.00121609
Iteration 6/1000 | Loss: 0.00018445
Iteration 7/1000 | Loss: 0.00014112
Iteration 8/1000 | Loss: 0.00015233
Iteration 9/1000 | Loss: 0.00007508
Iteration 10/1000 | Loss: 0.00006953
Iteration 11/1000 | Loss: 0.00007626
Iteration 12/1000 | Loss: 0.00017395
Iteration 13/1000 | Loss: 0.00015780
Iteration 14/1000 | Loss: 0.00011231
Iteration 15/1000 | Loss: 0.00008813
Iteration 16/1000 | Loss: 0.00016639
Iteration 17/1000 | Loss: 0.00017197
Iteration 18/1000 | Loss: 0.00019156
Iteration 19/1000 | Loss: 0.00202661
Iteration 20/1000 | Loss: 0.00026058
Iteration 21/1000 | Loss: 0.00003611
Iteration 22/1000 | Loss: 0.00003058
Iteration 23/1000 | Loss: 0.00002864
Iteration 24/1000 | Loss: 0.00002745
Iteration 25/1000 | Loss: 0.00002692
Iteration 26/1000 | Loss: 0.00002640
Iteration 27/1000 | Loss: 0.00002595
Iteration 28/1000 | Loss: 0.00002537
Iteration 29/1000 | Loss: 0.00002502
Iteration 30/1000 | Loss: 0.00002476
Iteration 31/1000 | Loss: 0.00002456
Iteration 32/1000 | Loss: 0.00002449
Iteration 33/1000 | Loss: 0.00002441
Iteration 34/1000 | Loss: 0.00002425
Iteration 35/1000 | Loss: 0.00002425
Iteration 36/1000 | Loss: 0.00002424
Iteration 37/1000 | Loss: 0.00002414
Iteration 38/1000 | Loss: 0.00002413
Iteration 39/1000 | Loss: 0.00002408
Iteration 40/1000 | Loss: 0.00002399
Iteration 41/1000 | Loss: 0.00002393
Iteration 42/1000 | Loss: 0.00002385
Iteration 43/1000 | Loss: 0.00002380
Iteration 44/1000 | Loss: 0.00002380
Iteration 45/1000 | Loss: 0.00002379
Iteration 46/1000 | Loss: 0.00002379
Iteration 47/1000 | Loss: 0.00002378
Iteration 48/1000 | Loss: 0.00002378
Iteration 49/1000 | Loss: 0.00002377
Iteration 50/1000 | Loss: 0.00002377
Iteration 51/1000 | Loss: 0.00002377
Iteration 52/1000 | Loss: 0.00002376
Iteration 53/1000 | Loss: 0.00002375
Iteration 54/1000 | Loss: 0.00002375
Iteration 55/1000 | Loss: 0.00002375
Iteration 56/1000 | Loss: 0.00002375
Iteration 57/1000 | Loss: 0.00002374
Iteration 58/1000 | Loss: 0.00002374
Iteration 59/1000 | Loss: 0.00002374
Iteration 60/1000 | Loss: 0.00002373
Iteration 61/1000 | Loss: 0.00002372
Iteration 62/1000 | Loss: 0.00002372
Iteration 63/1000 | Loss: 0.00002371
Iteration 64/1000 | Loss: 0.00002371
Iteration 65/1000 | Loss: 0.00002370
Iteration 66/1000 | Loss: 0.00002370
Iteration 67/1000 | Loss: 0.00002370
Iteration 68/1000 | Loss: 0.00002370
Iteration 69/1000 | Loss: 0.00002369
Iteration 70/1000 | Loss: 0.00002369
Iteration 71/1000 | Loss: 0.00002369
Iteration 72/1000 | Loss: 0.00002369
Iteration 73/1000 | Loss: 0.00002369
Iteration 74/1000 | Loss: 0.00002368
Iteration 75/1000 | Loss: 0.00002368
Iteration 76/1000 | Loss: 0.00002368
Iteration 77/1000 | Loss: 0.00002368
Iteration 78/1000 | Loss: 0.00002368
Iteration 79/1000 | Loss: 0.00002368
Iteration 80/1000 | Loss: 0.00002367
Iteration 81/1000 | Loss: 0.00002367
Iteration 82/1000 | Loss: 0.00002367
Iteration 83/1000 | Loss: 0.00002366
Iteration 84/1000 | Loss: 0.00002365
Iteration 85/1000 | Loss: 0.00002365
Iteration 86/1000 | Loss: 0.00002365
Iteration 87/1000 | Loss: 0.00002365
Iteration 88/1000 | Loss: 0.00002365
Iteration 89/1000 | Loss: 0.00002364
Iteration 90/1000 | Loss: 0.00002364
Iteration 91/1000 | Loss: 0.00002364
Iteration 92/1000 | Loss: 0.00002364
Iteration 93/1000 | Loss: 0.00002364
Iteration 94/1000 | Loss: 0.00002363
Iteration 95/1000 | Loss: 0.00002363
Iteration 96/1000 | Loss: 0.00002363
Iteration 97/1000 | Loss: 0.00002363
Iteration 98/1000 | Loss: 0.00002363
Iteration 99/1000 | Loss: 0.00002363
Iteration 100/1000 | Loss: 0.00002363
Iteration 101/1000 | Loss: 0.00002363
Iteration 102/1000 | Loss: 0.00002363
Iteration 103/1000 | Loss: 0.00002363
Iteration 104/1000 | Loss: 0.00002362
Iteration 105/1000 | Loss: 0.00002362
Iteration 106/1000 | Loss: 0.00002362
Iteration 107/1000 | Loss: 0.00002362
Iteration 108/1000 | Loss: 0.00002362
Iteration 109/1000 | Loss: 0.00002361
Iteration 110/1000 | Loss: 0.00002361
Iteration 111/1000 | Loss: 0.00002361
Iteration 112/1000 | Loss: 0.00002361
Iteration 113/1000 | Loss: 0.00002361
Iteration 114/1000 | Loss: 0.00002361
Iteration 115/1000 | Loss: 0.00002361
Iteration 116/1000 | Loss: 0.00002361
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00002360
Iteration 119/1000 | Loss: 0.00002360
Iteration 120/1000 | Loss: 0.00002360
Iteration 121/1000 | Loss: 0.00002360
Iteration 122/1000 | Loss: 0.00002359
Iteration 123/1000 | Loss: 0.00002359
Iteration 124/1000 | Loss: 0.00002359
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002359
Iteration 127/1000 | Loss: 0.00002358
Iteration 128/1000 | Loss: 0.00002358
Iteration 129/1000 | Loss: 0.00002358
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002357
Iteration 132/1000 | Loss: 0.00002357
Iteration 133/1000 | Loss: 0.00002357
Iteration 134/1000 | Loss: 0.00002357
Iteration 135/1000 | Loss: 0.00002357
Iteration 136/1000 | Loss: 0.00002357
Iteration 137/1000 | Loss: 0.00002357
Iteration 138/1000 | Loss: 0.00002356
Iteration 139/1000 | Loss: 0.00002356
Iteration 140/1000 | Loss: 0.00002356
Iteration 141/1000 | Loss: 0.00002356
Iteration 142/1000 | Loss: 0.00002355
Iteration 143/1000 | Loss: 0.00002355
Iteration 144/1000 | Loss: 0.00002355
Iteration 145/1000 | Loss: 0.00002355
Iteration 146/1000 | Loss: 0.00002355
Iteration 147/1000 | Loss: 0.00002355
Iteration 148/1000 | Loss: 0.00002355
Iteration 149/1000 | Loss: 0.00002355
Iteration 150/1000 | Loss: 0.00002355
Iteration 151/1000 | Loss: 0.00002355
Iteration 152/1000 | Loss: 0.00002355
Iteration 153/1000 | Loss: 0.00002354
Iteration 154/1000 | Loss: 0.00002354
Iteration 155/1000 | Loss: 0.00002354
Iteration 156/1000 | Loss: 0.00002354
Iteration 157/1000 | Loss: 0.00002354
Iteration 158/1000 | Loss: 0.00002353
Iteration 159/1000 | Loss: 0.00002353
Iteration 160/1000 | Loss: 0.00002353
Iteration 161/1000 | Loss: 0.00002353
Iteration 162/1000 | Loss: 0.00002353
Iteration 163/1000 | Loss: 0.00002353
Iteration 164/1000 | Loss: 0.00002353
Iteration 165/1000 | Loss: 0.00002353
Iteration 166/1000 | Loss: 0.00002353
Iteration 167/1000 | Loss: 0.00002352
Iteration 168/1000 | Loss: 0.00002352
Iteration 169/1000 | Loss: 0.00002352
Iteration 170/1000 | Loss: 0.00002352
Iteration 171/1000 | Loss: 0.00002352
Iteration 172/1000 | Loss: 0.00002352
Iteration 173/1000 | Loss: 0.00002352
Iteration 174/1000 | Loss: 0.00002351
Iteration 175/1000 | Loss: 0.00002351
Iteration 176/1000 | Loss: 0.00002351
Iteration 177/1000 | Loss: 0.00002351
Iteration 178/1000 | Loss: 0.00002351
Iteration 179/1000 | Loss: 0.00002351
Iteration 180/1000 | Loss: 0.00002351
Iteration 181/1000 | Loss: 0.00002351
Iteration 182/1000 | Loss: 0.00002351
Iteration 183/1000 | Loss: 0.00002351
Iteration 184/1000 | Loss: 0.00002351
Iteration 185/1000 | Loss: 0.00002351
Iteration 186/1000 | Loss: 0.00002351
Iteration 187/1000 | Loss: 0.00002350
Iteration 188/1000 | Loss: 0.00002350
Iteration 189/1000 | Loss: 0.00002350
Iteration 190/1000 | Loss: 0.00002350
Iteration 191/1000 | Loss: 0.00002350
Iteration 192/1000 | Loss: 0.00002350
Iteration 193/1000 | Loss: 0.00002350
Iteration 194/1000 | Loss: 0.00002350
Iteration 195/1000 | Loss: 0.00002350
Iteration 196/1000 | Loss: 0.00002350
Iteration 197/1000 | Loss: 0.00002350
Iteration 198/1000 | Loss: 0.00002350
Iteration 199/1000 | Loss: 0.00002350
Iteration 200/1000 | Loss: 0.00002350
Iteration 201/1000 | Loss: 0.00002350
Iteration 202/1000 | Loss: 0.00002350
Iteration 203/1000 | Loss: 0.00002350
Iteration 204/1000 | Loss: 0.00002350
Iteration 205/1000 | Loss: 0.00002350
Iteration 206/1000 | Loss: 0.00002350
Iteration 207/1000 | Loss: 0.00002350
Iteration 208/1000 | Loss: 0.00002350
Iteration 209/1000 | Loss: 0.00002350
Iteration 210/1000 | Loss: 0.00002349
Iteration 211/1000 | Loss: 0.00002349
Iteration 212/1000 | Loss: 0.00002349
Iteration 213/1000 | Loss: 0.00002349
Iteration 214/1000 | Loss: 0.00002349
Iteration 215/1000 | Loss: 0.00002349
Iteration 216/1000 | Loss: 0.00002349
Iteration 217/1000 | Loss: 0.00002349
Iteration 218/1000 | Loss: 0.00002349
Iteration 219/1000 | Loss: 0.00002349
Iteration 220/1000 | Loss: 0.00002349
Iteration 221/1000 | Loss: 0.00002349
Iteration 222/1000 | Loss: 0.00002349
Iteration 223/1000 | Loss: 0.00002349
Iteration 224/1000 | Loss: 0.00002349
Iteration 225/1000 | Loss: 0.00002349
Iteration 226/1000 | Loss: 0.00002349
Iteration 227/1000 | Loss: 0.00002349
Iteration 228/1000 | Loss: 0.00002349
Iteration 229/1000 | Loss: 0.00002349
Iteration 230/1000 | Loss: 0.00002348
Iteration 231/1000 | Loss: 0.00002348
Iteration 232/1000 | Loss: 0.00002348
Iteration 233/1000 | Loss: 0.00002348
Iteration 234/1000 | Loss: 0.00002348
Iteration 235/1000 | Loss: 0.00002348
Iteration 236/1000 | Loss: 0.00002348
Iteration 237/1000 | Loss: 0.00002348
Iteration 238/1000 | Loss: 0.00002348
Iteration 239/1000 | Loss: 0.00002348
Iteration 240/1000 | Loss: 0.00002348
Iteration 241/1000 | Loss: 0.00002348
Iteration 242/1000 | Loss: 0.00002348
Iteration 243/1000 | Loss: 0.00002348
Iteration 244/1000 | Loss: 0.00002348
Iteration 245/1000 | Loss: 0.00002348
Iteration 246/1000 | Loss: 0.00002348
Iteration 247/1000 | Loss: 0.00002347
Iteration 248/1000 | Loss: 0.00002347
Iteration 249/1000 | Loss: 0.00002347
Iteration 250/1000 | Loss: 0.00002347
Iteration 251/1000 | Loss: 0.00002347
Iteration 252/1000 | Loss: 0.00002347
Iteration 253/1000 | Loss: 0.00002347
Iteration 254/1000 | Loss: 0.00002347
Iteration 255/1000 | Loss: 0.00002347
Iteration 256/1000 | Loss: 0.00002347
Iteration 257/1000 | Loss: 0.00002347
Iteration 258/1000 | Loss: 0.00002347
Iteration 259/1000 | Loss: 0.00002347
Iteration 260/1000 | Loss: 0.00002347
Iteration 261/1000 | Loss: 0.00002347
Iteration 262/1000 | Loss: 0.00002347
Iteration 263/1000 | Loss: 0.00002347
Iteration 264/1000 | Loss: 0.00002347
Iteration 265/1000 | Loss: 0.00002347
Iteration 266/1000 | Loss: 0.00002347
Iteration 267/1000 | Loss: 0.00002347
Iteration 268/1000 | Loss: 0.00002347
Iteration 269/1000 | Loss: 0.00002347
Iteration 270/1000 | Loss: 0.00002347
Iteration 271/1000 | Loss: 0.00002347
Iteration 272/1000 | Loss: 0.00002347
Iteration 273/1000 | Loss: 0.00002347
Iteration 274/1000 | Loss: 0.00002347
Iteration 275/1000 | Loss: 0.00002347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [2.346532346564345e-05, 2.346532346564345e-05, 2.346532346564345e-05, 2.346532346564345e-05, 2.346532346564345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.346532346564345e-05

Optimization complete. Final v2v error: 4.075844764709473 mm

Highest mean error: 5.67093563079834 mm for frame 136

Lowest mean error: 3.5050089359283447 mm for frame 35

Saving results

Total time: 110.92815470695496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011779
Iteration 2/25 | Loss: 0.00373456
Iteration 3/25 | Loss: 0.00216539
Iteration 4/25 | Loss: 0.00186107
Iteration 5/25 | Loss: 0.00182448
Iteration 6/25 | Loss: 0.00169048
Iteration 7/25 | Loss: 0.00160457
Iteration 8/25 | Loss: 0.00153311
Iteration 9/25 | Loss: 0.00150320
Iteration 10/25 | Loss: 0.00146767
Iteration 11/25 | Loss: 0.00145239
Iteration 12/25 | Loss: 0.00144721
Iteration 13/25 | Loss: 0.00145514
Iteration 14/25 | Loss: 0.00143962
Iteration 15/25 | Loss: 0.00142976
Iteration 16/25 | Loss: 0.00143050
Iteration 17/25 | Loss: 0.00142792
Iteration 18/25 | Loss: 0.00142482
Iteration 19/25 | Loss: 0.00142764
Iteration 20/25 | Loss: 0.00142364
Iteration 21/25 | Loss: 0.00142350
Iteration 22/25 | Loss: 0.00142023
Iteration 23/25 | Loss: 0.00141970
Iteration 24/25 | Loss: 0.00141764
Iteration 25/25 | Loss: 0.00142029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37622929
Iteration 2/25 | Loss: 0.00321224
Iteration 3/25 | Loss: 0.00149726
Iteration 4/25 | Loss: 0.00149726
Iteration 5/25 | Loss: 0.00149726
Iteration 6/25 | Loss: 0.00149726
Iteration 7/25 | Loss: 0.00149726
Iteration 8/25 | Loss: 0.00149726
Iteration 9/25 | Loss: 0.00149726
Iteration 10/25 | Loss: 0.00149726
Iteration 11/25 | Loss: 0.00149726
Iteration 12/25 | Loss: 0.00149726
Iteration 13/25 | Loss: 0.00149726
Iteration 14/25 | Loss: 0.00149726
Iteration 15/25 | Loss: 0.00149726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014972598291933537, 0.0014972598291933537, 0.0014972598291933537, 0.0014972598291933537, 0.0014972598291933537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014972598291933537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149726
Iteration 2/1000 | Loss: 0.00038486
Iteration 3/1000 | Loss: 0.00043305
Iteration 4/1000 | Loss: 0.00026006
Iteration 5/1000 | Loss: 0.00115908
Iteration 6/1000 | Loss: 0.00226610
Iteration 7/1000 | Loss: 0.00059329
Iteration 8/1000 | Loss: 0.00055995
Iteration 9/1000 | Loss: 0.00038179
Iteration 10/1000 | Loss: 0.00151068
Iteration 11/1000 | Loss: 0.00115144
Iteration 12/1000 | Loss: 0.00131836
Iteration 13/1000 | Loss: 0.00078740
Iteration 14/1000 | Loss: 0.00091070
Iteration 15/1000 | Loss: 0.00093315
Iteration 16/1000 | Loss: 0.00062236
Iteration 17/1000 | Loss: 0.00025538
Iteration 18/1000 | Loss: 0.00008613
Iteration 19/1000 | Loss: 0.00007644
Iteration 20/1000 | Loss: 0.00014231
Iteration 21/1000 | Loss: 0.00007114
Iteration 22/1000 | Loss: 0.00043665
Iteration 23/1000 | Loss: 0.00025822
Iteration 24/1000 | Loss: 0.00030628
Iteration 25/1000 | Loss: 0.00015422
Iteration 26/1000 | Loss: 0.00011349
Iteration 27/1000 | Loss: 0.00012102
Iteration 28/1000 | Loss: 0.00005666
Iteration 29/1000 | Loss: 0.00064254
Iteration 30/1000 | Loss: 0.00028741
Iteration 31/1000 | Loss: 0.00030445
Iteration 32/1000 | Loss: 0.00020642
Iteration 33/1000 | Loss: 0.00043316
Iteration 34/1000 | Loss: 0.00070687
Iteration 35/1000 | Loss: 0.00040821
Iteration 36/1000 | Loss: 0.00041350
Iteration 37/1000 | Loss: 0.00027728
Iteration 38/1000 | Loss: 0.00036277
Iteration 39/1000 | Loss: 0.00008000
Iteration 40/1000 | Loss: 0.00099241
Iteration 41/1000 | Loss: 0.00136011
Iteration 42/1000 | Loss: 0.00196666
Iteration 43/1000 | Loss: 0.00098151
Iteration 44/1000 | Loss: 0.00015463
Iteration 45/1000 | Loss: 0.00010001
Iteration 46/1000 | Loss: 0.00004544
Iteration 47/1000 | Loss: 0.00003894
Iteration 48/1000 | Loss: 0.00051958
Iteration 49/1000 | Loss: 0.00095004
Iteration 50/1000 | Loss: 0.00041569
Iteration 51/1000 | Loss: 0.00003998
Iteration 52/1000 | Loss: 0.00003663
Iteration 53/1000 | Loss: 0.00002830
Iteration 54/1000 | Loss: 0.00002545
Iteration 55/1000 | Loss: 0.00002385
Iteration 56/1000 | Loss: 0.00002237
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00002111
Iteration 59/1000 | Loss: 0.00002053
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00007362
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001943
Iteration 64/1000 | Loss: 0.00006168
Iteration 65/1000 | Loss: 0.00002048
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001903
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001902
Iteration 72/1000 | Loss: 0.00001902
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001900
Iteration 76/1000 | Loss: 0.00001900
Iteration 77/1000 | Loss: 0.00001900
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001900
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001897
Iteration 86/1000 | Loss: 0.00001897
Iteration 87/1000 | Loss: 0.00001897
Iteration 88/1000 | Loss: 0.00001896
Iteration 89/1000 | Loss: 0.00001896
Iteration 90/1000 | Loss: 0.00001896
Iteration 91/1000 | Loss: 0.00001896
Iteration 92/1000 | Loss: 0.00001896
Iteration 93/1000 | Loss: 0.00001896
Iteration 94/1000 | Loss: 0.00001893
Iteration 95/1000 | Loss: 0.00001890
Iteration 96/1000 | Loss: 0.00001890
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001889
Iteration 100/1000 | Loss: 0.00001889
Iteration 101/1000 | Loss: 0.00001889
Iteration 102/1000 | Loss: 0.00001888
Iteration 103/1000 | Loss: 0.00001888
Iteration 104/1000 | Loss: 0.00001888
Iteration 105/1000 | Loss: 0.00001887
Iteration 106/1000 | Loss: 0.00001887
Iteration 107/1000 | Loss: 0.00001887
Iteration 108/1000 | Loss: 0.00001887
Iteration 109/1000 | Loss: 0.00001887
Iteration 110/1000 | Loss: 0.00001887
Iteration 111/1000 | Loss: 0.00001886
Iteration 112/1000 | Loss: 0.00001886
Iteration 113/1000 | Loss: 0.00001886
Iteration 114/1000 | Loss: 0.00001886
Iteration 115/1000 | Loss: 0.00001885
Iteration 116/1000 | Loss: 0.00001885
Iteration 117/1000 | Loss: 0.00001885
Iteration 118/1000 | Loss: 0.00001884
Iteration 119/1000 | Loss: 0.00001884
Iteration 120/1000 | Loss: 0.00001884
Iteration 121/1000 | Loss: 0.00001883
Iteration 122/1000 | Loss: 0.00001883
Iteration 123/1000 | Loss: 0.00001883
Iteration 124/1000 | Loss: 0.00001883
Iteration 125/1000 | Loss: 0.00001883
Iteration 126/1000 | Loss: 0.00001883
Iteration 127/1000 | Loss: 0.00001883
Iteration 128/1000 | Loss: 0.00001883
Iteration 129/1000 | Loss: 0.00001883
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00001882
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001882
Iteration 136/1000 | Loss: 0.00001881
Iteration 137/1000 | Loss: 0.00001881
Iteration 138/1000 | Loss: 0.00001881
Iteration 139/1000 | Loss: 0.00001880
Iteration 140/1000 | Loss: 0.00001880
Iteration 141/1000 | Loss: 0.00001880
Iteration 142/1000 | Loss: 0.00001880
Iteration 143/1000 | Loss: 0.00001879
Iteration 144/1000 | Loss: 0.00001879
Iteration 145/1000 | Loss: 0.00001879
Iteration 146/1000 | Loss: 0.00001878
Iteration 147/1000 | Loss: 0.00001878
Iteration 148/1000 | Loss: 0.00001877
Iteration 149/1000 | Loss: 0.00001877
Iteration 150/1000 | Loss: 0.00008991
Iteration 151/1000 | Loss: 0.00001883
Iteration 152/1000 | Loss: 0.00001871
Iteration 153/1000 | Loss: 0.00001871
Iteration 154/1000 | Loss: 0.00001871
Iteration 155/1000 | Loss: 0.00001871
Iteration 156/1000 | Loss: 0.00001870
Iteration 157/1000 | Loss: 0.00001870
Iteration 158/1000 | Loss: 0.00001870
Iteration 159/1000 | Loss: 0.00001870
Iteration 160/1000 | Loss: 0.00001870
Iteration 161/1000 | Loss: 0.00001870
Iteration 162/1000 | Loss: 0.00001870
Iteration 163/1000 | Loss: 0.00001870
Iteration 164/1000 | Loss: 0.00001870
Iteration 165/1000 | Loss: 0.00001869
Iteration 166/1000 | Loss: 0.00001869
Iteration 167/1000 | Loss: 0.00001869
Iteration 168/1000 | Loss: 0.00001869
Iteration 169/1000 | Loss: 0.00001869
Iteration 170/1000 | Loss: 0.00001869
Iteration 171/1000 | Loss: 0.00001868
Iteration 172/1000 | Loss: 0.00001868
Iteration 173/1000 | Loss: 0.00001868
Iteration 174/1000 | Loss: 0.00001867
Iteration 175/1000 | Loss: 0.00001867
Iteration 176/1000 | Loss: 0.00001867
Iteration 177/1000 | Loss: 0.00001866
Iteration 178/1000 | Loss: 0.00001866
Iteration 179/1000 | Loss: 0.00001866
Iteration 180/1000 | Loss: 0.00001866
Iteration 181/1000 | Loss: 0.00001866
Iteration 182/1000 | Loss: 0.00001866
Iteration 183/1000 | Loss: 0.00001866
Iteration 184/1000 | Loss: 0.00001866
Iteration 185/1000 | Loss: 0.00001866
Iteration 186/1000 | Loss: 0.00001866
Iteration 187/1000 | Loss: 0.00001866
Iteration 188/1000 | Loss: 0.00001866
Iteration 189/1000 | Loss: 0.00001865
Iteration 190/1000 | Loss: 0.00001865
Iteration 191/1000 | Loss: 0.00001865
Iteration 192/1000 | Loss: 0.00001865
Iteration 193/1000 | Loss: 0.00001865
Iteration 194/1000 | Loss: 0.00001865
Iteration 195/1000 | Loss: 0.00001865
Iteration 196/1000 | Loss: 0.00001865
Iteration 197/1000 | Loss: 0.00001865
Iteration 198/1000 | Loss: 0.00001865
Iteration 199/1000 | Loss: 0.00001865
Iteration 200/1000 | Loss: 0.00001864
Iteration 201/1000 | Loss: 0.00001864
Iteration 202/1000 | Loss: 0.00001864
Iteration 203/1000 | Loss: 0.00001864
Iteration 204/1000 | Loss: 0.00001864
Iteration 205/1000 | Loss: 0.00001864
Iteration 206/1000 | Loss: 0.00001864
Iteration 207/1000 | Loss: 0.00001863
Iteration 208/1000 | Loss: 0.00001863
Iteration 209/1000 | Loss: 0.00001863
Iteration 210/1000 | Loss: 0.00001863
Iteration 211/1000 | Loss: 0.00001862
Iteration 212/1000 | Loss: 0.00001862
Iteration 213/1000 | Loss: 0.00001862
Iteration 214/1000 | Loss: 0.00001862
Iteration 215/1000 | Loss: 0.00001862
Iteration 216/1000 | Loss: 0.00001862
Iteration 217/1000 | Loss: 0.00001861
Iteration 218/1000 | Loss: 0.00001861
Iteration 219/1000 | Loss: 0.00001861
Iteration 220/1000 | Loss: 0.00001861
Iteration 221/1000 | Loss: 0.00001861
Iteration 222/1000 | Loss: 0.00001861
Iteration 223/1000 | Loss: 0.00001861
Iteration 224/1000 | Loss: 0.00001861
Iteration 225/1000 | Loss: 0.00001861
Iteration 226/1000 | Loss: 0.00001860
Iteration 227/1000 | Loss: 0.00001860
Iteration 228/1000 | Loss: 0.00001860
Iteration 229/1000 | Loss: 0.00001860
Iteration 230/1000 | Loss: 0.00001860
Iteration 231/1000 | Loss: 0.00001860
Iteration 232/1000 | Loss: 0.00001860
Iteration 233/1000 | Loss: 0.00001860
Iteration 234/1000 | Loss: 0.00001860
Iteration 235/1000 | Loss: 0.00001859
Iteration 236/1000 | Loss: 0.00001859
Iteration 237/1000 | Loss: 0.00001859
Iteration 238/1000 | Loss: 0.00001859
Iteration 239/1000 | Loss: 0.00001859
Iteration 240/1000 | Loss: 0.00001859
Iteration 241/1000 | Loss: 0.00001859
Iteration 242/1000 | Loss: 0.00001859
Iteration 243/1000 | Loss: 0.00001859
Iteration 244/1000 | Loss: 0.00001859
Iteration 245/1000 | Loss: 0.00001859
Iteration 246/1000 | Loss: 0.00001859
Iteration 247/1000 | Loss: 0.00001859
Iteration 248/1000 | Loss: 0.00001859
Iteration 249/1000 | Loss: 0.00001859
Iteration 250/1000 | Loss: 0.00001859
Iteration 251/1000 | Loss: 0.00001859
Iteration 252/1000 | Loss: 0.00001859
Iteration 253/1000 | Loss: 0.00001859
Iteration 254/1000 | Loss: 0.00001859
Iteration 255/1000 | Loss: 0.00001859
Iteration 256/1000 | Loss: 0.00001859
Iteration 257/1000 | Loss: 0.00001859
Iteration 258/1000 | Loss: 0.00001859
Iteration 259/1000 | Loss: 0.00001859
Iteration 260/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.858562245615758e-05, 1.858562245615758e-05, 1.858562245615758e-05, 1.858562245615758e-05, 1.858562245615758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.858562245615758e-05

Optimization complete. Final v2v error: 3.617924213409424 mm

Highest mean error: 4.988425254821777 mm for frame 43

Lowest mean error: 3.0459465980529785 mm for frame 24

Saving results

Total time: 175.59407234191895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025343
Iteration 2/25 | Loss: 0.01025343
Iteration 3/25 | Loss: 0.01025343
Iteration 4/25 | Loss: 0.01025343
Iteration 5/25 | Loss: 0.01025342
Iteration 6/25 | Loss: 0.01025342
Iteration 7/25 | Loss: 0.01025342
Iteration 8/25 | Loss: 0.01025342
Iteration 9/25 | Loss: 0.01025342
Iteration 10/25 | Loss: 0.01025342
Iteration 11/25 | Loss: 0.01025342
Iteration 12/25 | Loss: 0.01025342
Iteration 13/25 | Loss: 0.01025342
Iteration 14/25 | Loss: 0.01025342
Iteration 15/25 | Loss: 0.01025341
Iteration 16/25 | Loss: 0.01025341
Iteration 17/25 | Loss: 0.01025341
Iteration 18/25 | Loss: 0.01025341
Iteration 19/25 | Loss: 0.01025341
Iteration 20/25 | Loss: 0.01025341
Iteration 21/25 | Loss: 0.01025341
Iteration 22/25 | Loss: 0.01025341
Iteration 23/25 | Loss: 0.01025341
Iteration 24/25 | Loss: 0.01025341
Iteration 25/25 | Loss: 0.01025340

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.09410357
Iteration 2/25 | Loss: 0.11746541
Iteration 3/25 | Loss: 0.11401557
Iteration 4/25 | Loss: 0.11272835
Iteration 5/25 | Loss: 0.11207774
Iteration 6/25 | Loss: 0.11207772
Iteration 7/25 | Loss: 0.11207772
Iteration 8/25 | Loss: 0.11207772
Iteration 9/25 | Loss: 0.11207770
Iteration 10/25 | Loss: 0.11207770
Iteration 11/25 | Loss: 0.11207770
Iteration 12/25 | Loss: 0.11207770
Iteration 13/25 | Loss: 0.11207770
Iteration 14/25 | Loss: 0.11207770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.11207769811153412, 0.11207769811153412, 0.11207769811153412, 0.11207769811153412, 0.11207769811153412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11207769811153412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11207770
Iteration 2/1000 | Loss: 0.00134133
Iteration 3/1000 | Loss: 0.00199994
Iteration 4/1000 | Loss: 0.00032469
Iteration 5/1000 | Loss: 0.00029156
Iteration 6/1000 | Loss: 0.00036421
Iteration 7/1000 | Loss: 0.00008978
Iteration 8/1000 | Loss: 0.00017562
Iteration 9/1000 | Loss: 0.00006638
Iteration 10/1000 | Loss: 0.00011439
Iteration 11/1000 | Loss: 0.00006042
Iteration 12/1000 | Loss: 0.00008346
Iteration 13/1000 | Loss: 0.00006302
Iteration 14/1000 | Loss: 0.00013662
Iteration 15/1000 | Loss: 0.00003744
Iteration 16/1000 | Loss: 0.00003598
Iteration 17/1000 | Loss: 0.00016102
Iteration 18/1000 | Loss: 0.00004178
Iteration 19/1000 | Loss: 0.00004337
Iteration 20/1000 | Loss: 0.00003516
Iteration 21/1000 | Loss: 0.00003432
Iteration 22/1000 | Loss: 0.00020513
Iteration 23/1000 | Loss: 0.00003062
Iteration 24/1000 | Loss: 0.00003662
Iteration 25/1000 | Loss: 0.00002890
Iteration 26/1000 | Loss: 0.00003421
Iteration 27/1000 | Loss: 0.00002763
Iteration 28/1000 | Loss: 0.00015964
Iteration 29/1000 | Loss: 0.00012821
Iteration 30/1000 | Loss: 0.00024680
Iteration 31/1000 | Loss: 0.00008117
Iteration 32/1000 | Loss: 0.00003160
Iteration 33/1000 | Loss: 0.00002588
Iteration 34/1000 | Loss: 0.00002700
Iteration 35/1000 | Loss: 0.00008734
Iteration 36/1000 | Loss: 0.00002523
Iteration 37/1000 | Loss: 0.00004694
Iteration 38/1000 | Loss: 0.00002468
Iteration 39/1000 | Loss: 0.00002911
Iteration 40/1000 | Loss: 0.00006483
Iteration 41/1000 | Loss: 0.00002470
Iteration 42/1000 | Loss: 0.00002769
Iteration 43/1000 | Loss: 0.00002443
Iteration 44/1000 | Loss: 0.00003495
Iteration 45/1000 | Loss: 0.00002412
Iteration 46/1000 | Loss: 0.00002410
Iteration 47/1000 | Loss: 0.00003911
Iteration 48/1000 | Loss: 0.00002392
Iteration 49/1000 | Loss: 0.00002390
Iteration 50/1000 | Loss: 0.00002381
Iteration 51/1000 | Loss: 0.00002381
Iteration 52/1000 | Loss: 0.00002379
Iteration 53/1000 | Loss: 0.00006194
Iteration 54/1000 | Loss: 0.00004782
Iteration 55/1000 | Loss: 0.00002551
Iteration 56/1000 | Loss: 0.00002365
Iteration 57/1000 | Loss: 0.00002364
Iteration 58/1000 | Loss: 0.00002362
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00002362
Iteration 61/1000 | Loss: 0.00002701
Iteration 62/1000 | Loss: 0.00003095
Iteration 63/1000 | Loss: 0.00002356
Iteration 64/1000 | Loss: 0.00003986
Iteration 65/1000 | Loss: 0.00002535
Iteration 66/1000 | Loss: 0.00002724
Iteration 67/1000 | Loss: 0.00006331
Iteration 68/1000 | Loss: 0.00004605
Iteration 69/1000 | Loss: 0.00002856
Iteration 70/1000 | Loss: 0.00002337
Iteration 71/1000 | Loss: 0.00002337
Iteration 72/1000 | Loss: 0.00002336
Iteration 73/1000 | Loss: 0.00002335
Iteration 74/1000 | Loss: 0.00002335
Iteration 75/1000 | Loss: 0.00002335
Iteration 76/1000 | Loss: 0.00002334
Iteration 77/1000 | Loss: 0.00002334
Iteration 78/1000 | Loss: 0.00002334
Iteration 79/1000 | Loss: 0.00002334
Iteration 80/1000 | Loss: 0.00002334
Iteration 81/1000 | Loss: 0.00002334
Iteration 82/1000 | Loss: 0.00002334
Iteration 83/1000 | Loss: 0.00002334
Iteration 84/1000 | Loss: 0.00002334
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002333
Iteration 87/1000 | Loss: 0.00002333
Iteration 88/1000 | Loss: 0.00002333
Iteration 89/1000 | Loss: 0.00002333
Iteration 90/1000 | Loss: 0.00002332
Iteration 91/1000 | Loss: 0.00002332
Iteration 92/1000 | Loss: 0.00002332
Iteration 93/1000 | Loss: 0.00002332
Iteration 94/1000 | Loss: 0.00002332
Iteration 95/1000 | Loss: 0.00002332
Iteration 96/1000 | Loss: 0.00002331
Iteration 97/1000 | Loss: 0.00002331
Iteration 98/1000 | Loss: 0.00002331
Iteration 99/1000 | Loss: 0.00002330
Iteration 100/1000 | Loss: 0.00002330
Iteration 101/1000 | Loss: 0.00002329
Iteration 102/1000 | Loss: 0.00002329
Iteration 103/1000 | Loss: 0.00002329
Iteration 104/1000 | Loss: 0.00002329
Iteration 105/1000 | Loss: 0.00002328
Iteration 106/1000 | Loss: 0.00002328
Iteration 107/1000 | Loss: 0.00002328
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002327
Iteration 110/1000 | Loss: 0.00005970
Iteration 111/1000 | Loss: 0.00002663
Iteration 112/1000 | Loss: 0.00002895
Iteration 113/1000 | Loss: 0.00002329
Iteration 114/1000 | Loss: 0.00002325
Iteration 115/1000 | Loss: 0.00002324
Iteration 116/1000 | Loss: 0.00002324
Iteration 117/1000 | Loss: 0.00002323
Iteration 118/1000 | Loss: 0.00002323
Iteration 119/1000 | Loss: 0.00002322
Iteration 120/1000 | Loss: 0.00002322
Iteration 121/1000 | Loss: 0.00002516
Iteration 122/1000 | Loss: 0.00002321
Iteration 123/1000 | Loss: 0.00002320
Iteration 124/1000 | Loss: 0.00002320
Iteration 125/1000 | Loss: 0.00002319
Iteration 126/1000 | Loss: 0.00003031
Iteration 127/1000 | Loss: 0.00005980
Iteration 128/1000 | Loss: 0.00004323
Iteration 129/1000 | Loss: 0.00002993
Iteration 130/1000 | Loss: 0.00003689
Iteration 131/1000 | Loss: 0.00004171
Iteration 132/1000 | Loss: 0.00002354
Iteration 133/1000 | Loss: 0.00002309
Iteration 134/1000 | Loss: 0.00002309
Iteration 135/1000 | Loss: 0.00002309
Iteration 136/1000 | Loss: 0.00002309
Iteration 137/1000 | Loss: 0.00002309
Iteration 138/1000 | Loss: 0.00002309
Iteration 139/1000 | Loss: 0.00002309
Iteration 140/1000 | Loss: 0.00002309
Iteration 141/1000 | Loss: 0.00002309
Iteration 142/1000 | Loss: 0.00002309
Iteration 143/1000 | Loss: 0.00002309
Iteration 144/1000 | Loss: 0.00002309
Iteration 145/1000 | Loss: 0.00002309
Iteration 146/1000 | Loss: 0.00002309
Iteration 147/1000 | Loss: 0.00002309
Iteration 148/1000 | Loss: 0.00002309
Iteration 149/1000 | Loss: 0.00002309
Iteration 150/1000 | Loss: 0.00002309
Iteration 151/1000 | Loss: 0.00002309
Iteration 152/1000 | Loss: 0.00002309
Iteration 153/1000 | Loss: 0.00002309
Iteration 154/1000 | Loss: 0.00002309
Iteration 155/1000 | Loss: 0.00002309
Iteration 156/1000 | Loss: 0.00002309
Iteration 157/1000 | Loss: 0.00002309
Iteration 158/1000 | Loss: 0.00002309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.30901478062151e-05, 2.30901478062151e-05, 2.30901478062151e-05, 2.30901478062151e-05, 2.30901478062151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.30901478062151e-05

Optimization complete. Final v2v error: 4.020785808563232 mm

Highest mean error: 5.458338260650635 mm for frame 192

Lowest mean error: 3.154954195022583 mm for frame 86

Saving results

Total time: 120.33568525314331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441836
Iteration 2/25 | Loss: 0.00147828
Iteration 3/25 | Loss: 0.00130931
Iteration 4/25 | Loss: 0.00130160
Iteration 5/25 | Loss: 0.00130031
Iteration 6/25 | Loss: 0.00130030
Iteration 7/25 | Loss: 0.00130030
Iteration 8/25 | Loss: 0.00130030
Iteration 9/25 | Loss: 0.00130030
Iteration 10/25 | Loss: 0.00130030
Iteration 11/25 | Loss: 0.00130030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001300295814871788, 0.001300295814871788, 0.001300295814871788, 0.001300295814871788, 0.001300295814871788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001300295814871788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.34015846
Iteration 2/25 | Loss: 0.00082786
Iteration 3/25 | Loss: 0.00082782
Iteration 4/25 | Loss: 0.00082782
Iteration 5/25 | Loss: 0.00082782
Iteration 6/25 | Loss: 0.00082782
Iteration 7/25 | Loss: 0.00082782
Iteration 8/25 | Loss: 0.00082782
Iteration 9/25 | Loss: 0.00082782
Iteration 10/25 | Loss: 0.00082782
Iteration 11/25 | Loss: 0.00082782
Iteration 12/25 | Loss: 0.00082782
Iteration 13/25 | Loss: 0.00082782
Iteration 14/25 | Loss: 0.00082782
Iteration 15/25 | Loss: 0.00082782
Iteration 16/25 | Loss: 0.00082782
Iteration 17/25 | Loss: 0.00082782
Iteration 18/25 | Loss: 0.00082782
Iteration 19/25 | Loss: 0.00082782
Iteration 20/25 | Loss: 0.00082782
Iteration 21/25 | Loss: 0.00082782
Iteration 22/25 | Loss: 0.00082782
Iteration 23/25 | Loss: 0.00082782
Iteration 24/25 | Loss: 0.00082782
Iteration 25/25 | Loss: 0.00082782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082782
Iteration 2/1000 | Loss: 0.00003628
Iteration 3/1000 | Loss: 0.00002522
Iteration 4/1000 | Loss: 0.00002075
Iteration 5/1000 | Loss: 0.00001944
Iteration 6/1000 | Loss: 0.00001872
Iteration 7/1000 | Loss: 0.00001813
Iteration 8/1000 | Loss: 0.00001748
Iteration 9/1000 | Loss: 0.00001727
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001681
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001636
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001628
Iteration 19/1000 | Loss: 0.00001627
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001625
Iteration 22/1000 | Loss: 0.00001623
Iteration 23/1000 | Loss: 0.00001621
Iteration 24/1000 | Loss: 0.00001619
Iteration 25/1000 | Loss: 0.00001619
Iteration 26/1000 | Loss: 0.00001618
Iteration 27/1000 | Loss: 0.00001618
Iteration 28/1000 | Loss: 0.00001616
Iteration 29/1000 | Loss: 0.00001616
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001616
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001615
Iteration 37/1000 | Loss: 0.00001615
Iteration 38/1000 | Loss: 0.00001615
Iteration 39/1000 | Loss: 0.00001615
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001611
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001611
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001610
Iteration 48/1000 | Loss: 0.00001610
Iteration 49/1000 | Loss: 0.00001610
Iteration 50/1000 | Loss: 0.00001609
Iteration 51/1000 | Loss: 0.00001609
Iteration 52/1000 | Loss: 0.00001608
Iteration 53/1000 | Loss: 0.00001608
Iteration 54/1000 | Loss: 0.00001608
Iteration 55/1000 | Loss: 0.00001607
Iteration 56/1000 | Loss: 0.00001607
Iteration 57/1000 | Loss: 0.00001606
Iteration 58/1000 | Loss: 0.00001606
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001605
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001604
Iteration 64/1000 | Loss: 0.00001604
Iteration 65/1000 | Loss: 0.00001604
Iteration 66/1000 | Loss: 0.00001604
Iteration 67/1000 | Loss: 0.00001604
Iteration 68/1000 | Loss: 0.00001603
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001602
Iteration 71/1000 | Loss: 0.00001602
Iteration 72/1000 | Loss: 0.00001601
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001601
Iteration 75/1000 | Loss: 0.00001601
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001600
Iteration 79/1000 | Loss: 0.00001598
Iteration 80/1000 | Loss: 0.00001598
Iteration 81/1000 | Loss: 0.00001598
Iteration 82/1000 | Loss: 0.00001597
Iteration 83/1000 | Loss: 0.00001597
Iteration 84/1000 | Loss: 0.00001597
Iteration 85/1000 | Loss: 0.00001597
Iteration 86/1000 | Loss: 0.00001597
Iteration 87/1000 | Loss: 0.00001597
Iteration 88/1000 | Loss: 0.00001597
Iteration 89/1000 | Loss: 0.00001596
Iteration 90/1000 | Loss: 0.00001596
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001594
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001593
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001592
Iteration 102/1000 | Loss: 0.00001592
Iteration 103/1000 | Loss: 0.00001592
Iteration 104/1000 | Loss: 0.00001591
Iteration 105/1000 | Loss: 0.00001591
Iteration 106/1000 | Loss: 0.00001591
Iteration 107/1000 | Loss: 0.00001591
Iteration 108/1000 | Loss: 0.00001590
Iteration 109/1000 | Loss: 0.00001590
Iteration 110/1000 | Loss: 0.00001590
Iteration 111/1000 | Loss: 0.00001590
Iteration 112/1000 | Loss: 0.00001590
Iteration 113/1000 | Loss: 0.00001589
Iteration 114/1000 | Loss: 0.00001589
Iteration 115/1000 | Loss: 0.00001589
Iteration 116/1000 | Loss: 0.00001589
Iteration 117/1000 | Loss: 0.00001589
Iteration 118/1000 | Loss: 0.00001589
Iteration 119/1000 | Loss: 0.00001589
Iteration 120/1000 | Loss: 0.00001589
Iteration 121/1000 | Loss: 0.00001588
Iteration 122/1000 | Loss: 0.00001588
Iteration 123/1000 | Loss: 0.00001588
Iteration 124/1000 | Loss: 0.00001588
Iteration 125/1000 | Loss: 0.00001588
Iteration 126/1000 | Loss: 0.00001588
Iteration 127/1000 | Loss: 0.00001588
Iteration 128/1000 | Loss: 0.00001588
Iteration 129/1000 | Loss: 0.00001587
Iteration 130/1000 | Loss: 0.00001587
Iteration 131/1000 | Loss: 0.00001587
Iteration 132/1000 | Loss: 0.00001587
Iteration 133/1000 | Loss: 0.00001587
Iteration 134/1000 | Loss: 0.00001587
Iteration 135/1000 | Loss: 0.00001587
Iteration 136/1000 | Loss: 0.00001587
Iteration 137/1000 | Loss: 0.00001587
Iteration 138/1000 | Loss: 0.00001587
Iteration 139/1000 | Loss: 0.00001587
Iteration 140/1000 | Loss: 0.00001587
Iteration 141/1000 | Loss: 0.00001587
Iteration 142/1000 | Loss: 0.00001587
Iteration 143/1000 | Loss: 0.00001587
Iteration 144/1000 | Loss: 0.00001587
Iteration 145/1000 | Loss: 0.00001587
Iteration 146/1000 | Loss: 0.00001587
Iteration 147/1000 | Loss: 0.00001587
Iteration 148/1000 | Loss: 0.00001587
Iteration 149/1000 | Loss: 0.00001587
Iteration 150/1000 | Loss: 0.00001587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.5867131878621876e-05, 1.5867131878621876e-05, 1.5867131878621876e-05, 1.5867131878621876e-05, 1.5867131878621876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5867131878621876e-05

Optimization complete. Final v2v error: 3.35859751701355 mm

Highest mean error: 4.084766864776611 mm for frame 71

Lowest mean error: 2.93743634223938 mm for frame 101

Saving results

Total time: 37.39203214645386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764854
Iteration 2/25 | Loss: 0.00168764
Iteration 3/25 | Loss: 0.00130107
Iteration 4/25 | Loss: 0.00127361
Iteration 5/25 | Loss: 0.00126535
Iteration 6/25 | Loss: 0.00126447
Iteration 7/25 | Loss: 0.00126239
Iteration 8/25 | Loss: 0.00126189
Iteration 9/25 | Loss: 0.00126176
Iteration 10/25 | Loss: 0.00126176
Iteration 11/25 | Loss: 0.00126176
Iteration 12/25 | Loss: 0.00126176
Iteration 13/25 | Loss: 0.00126176
Iteration 14/25 | Loss: 0.00126176
Iteration 15/25 | Loss: 0.00126176
Iteration 16/25 | Loss: 0.00126176
Iteration 17/25 | Loss: 0.00126176
Iteration 18/25 | Loss: 0.00126176
Iteration 19/25 | Loss: 0.00126176
Iteration 20/25 | Loss: 0.00126176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012617640895769, 0.0012617640895769, 0.0012617640895769, 0.0012617640895769, 0.0012617640895769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012617640895769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75238752
Iteration 2/25 | Loss: 0.00087233
Iteration 3/25 | Loss: 0.00087181
Iteration 4/25 | Loss: 0.00087181
Iteration 5/25 | Loss: 0.00087181
Iteration 6/25 | Loss: 0.00087181
Iteration 7/25 | Loss: 0.00087181
Iteration 8/25 | Loss: 0.00087181
Iteration 9/25 | Loss: 0.00087181
Iteration 10/25 | Loss: 0.00087181
Iteration 11/25 | Loss: 0.00087181
Iteration 12/25 | Loss: 0.00087181
Iteration 13/25 | Loss: 0.00087181
Iteration 14/25 | Loss: 0.00087181
Iteration 15/25 | Loss: 0.00087181
Iteration 16/25 | Loss: 0.00087181
Iteration 17/25 | Loss: 0.00087181
Iteration 18/25 | Loss: 0.00087181
Iteration 19/25 | Loss: 0.00087181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008718108874745667, 0.0008718108874745667, 0.0008718108874745667, 0.0008718108874745667, 0.0008718108874745667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008718108874745667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087181
Iteration 2/1000 | Loss: 0.00002913
Iteration 3/1000 | Loss: 0.00002285
Iteration 4/1000 | Loss: 0.00001849
Iteration 5/1000 | Loss: 0.00001701
Iteration 6/1000 | Loss: 0.00001627
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001588
Iteration 9/1000 | Loss: 0.00001530
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001497
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001490
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001476
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001469
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00001466
Iteration 25/1000 | Loss: 0.00001465
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001458
Iteration 28/1000 | Loss: 0.00001458
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001457
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001455
Iteration 33/1000 | Loss: 0.00001455
Iteration 34/1000 | Loss: 0.00001455
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001454
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001453
Iteration 41/1000 | Loss: 0.00001453
Iteration 42/1000 | Loss: 0.00001452
Iteration 43/1000 | Loss: 0.00001452
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001452
Iteration 47/1000 | Loss: 0.00001446
Iteration 48/1000 | Loss: 0.00001444
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001443
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001443
Iteration 69/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.4428469512495212e-05, 1.4428469512495212e-05, 1.4428469512495212e-05, 1.4428469512495212e-05, 1.4428469512495212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4428469512495212e-05

Optimization complete. Final v2v error: 3.2144668102264404 mm

Highest mean error: 3.553858757019043 mm for frame 138

Lowest mean error: 2.9679152965545654 mm for frame 57

Saving results

Total time: 44.66165375709534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410860
Iteration 2/25 | Loss: 0.00134906
Iteration 3/25 | Loss: 0.00125335
Iteration 4/25 | Loss: 0.00124337
Iteration 5/25 | Loss: 0.00124117
Iteration 6/25 | Loss: 0.00124104
Iteration 7/25 | Loss: 0.00124104
Iteration 8/25 | Loss: 0.00124104
Iteration 9/25 | Loss: 0.00124104
Iteration 10/25 | Loss: 0.00124104
Iteration 11/25 | Loss: 0.00124104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001241043908521533, 0.001241043908521533, 0.001241043908521533, 0.001241043908521533, 0.001241043908521533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001241043908521533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42178833
Iteration 2/25 | Loss: 0.00065094
Iteration 3/25 | Loss: 0.00065094
Iteration 4/25 | Loss: 0.00065094
Iteration 5/25 | Loss: 0.00065094
Iteration 6/25 | Loss: 0.00065094
Iteration 7/25 | Loss: 0.00065094
Iteration 8/25 | Loss: 0.00065094
Iteration 9/25 | Loss: 0.00065094
Iteration 10/25 | Loss: 0.00065094
Iteration 11/25 | Loss: 0.00065094
Iteration 12/25 | Loss: 0.00065094
Iteration 13/25 | Loss: 0.00065094
Iteration 14/25 | Loss: 0.00065094
Iteration 15/25 | Loss: 0.00065094
Iteration 16/25 | Loss: 0.00065094
Iteration 17/25 | Loss: 0.00065094
Iteration 18/25 | Loss: 0.00065094
Iteration 19/25 | Loss: 0.00065094
Iteration 20/25 | Loss: 0.00065094
Iteration 21/25 | Loss: 0.00065094
Iteration 22/25 | Loss: 0.00065094
Iteration 23/25 | Loss: 0.00065094
Iteration 24/25 | Loss: 0.00065094
Iteration 25/25 | Loss: 0.00065094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065094
Iteration 2/1000 | Loss: 0.00002931
Iteration 3/1000 | Loss: 0.00002071
Iteration 4/1000 | Loss: 0.00001907
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001653
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001587
Iteration 10/1000 | Loss: 0.00001573
Iteration 11/1000 | Loss: 0.00001563
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001551
Iteration 15/1000 | Loss: 0.00001549
Iteration 16/1000 | Loss: 0.00001543
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001530
Iteration 21/1000 | Loss: 0.00001519
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001516
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001513
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001501
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001499
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001487
Iteration 36/1000 | Loss: 0.00001485
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001482
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001480
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001479
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001478
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001477
Iteration 63/1000 | Loss: 0.00001477
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001474
Iteration 72/1000 | Loss: 0.00001474
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001473
Iteration 75/1000 | Loss: 0.00001473
Iteration 76/1000 | Loss: 0.00001473
Iteration 77/1000 | Loss: 0.00001473
Iteration 78/1000 | Loss: 0.00001472
Iteration 79/1000 | Loss: 0.00001472
Iteration 80/1000 | Loss: 0.00001471
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001470
Iteration 83/1000 | Loss: 0.00001470
Iteration 84/1000 | Loss: 0.00001470
Iteration 85/1000 | Loss: 0.00001470
Iteration 86/1000 | Loss: 0.00001470
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001468
Iteration 89/1000 | Loss: 0.00001468
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001467
Iteration 95/1000 | Loss: 0.00001467
Iteration 96/1000 | Loss: 0.00001467
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001467
Iteration 99/1000 | Loss: 0.00001467
Iteration 100/1000 | Loss: 0.00001467
Iteration 101/1000 | Loss: 0.00001467
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001466
Iteration 106/1000 | Loss: 0.00001466
Iteration 107/1000 | Loss: 0.00001466
Iteration 108/1000 | Loss: 0.00001465
Iteration 109/1000 | Loss: 0.00001465
Iteration 110/1000 | Loss: 0.00001465
Iteration 111/1000 | Loss: 0.00001465
Iteration 112/1000 | Loss: 0.00001465
Iteration 113/1000 | Loss: 0.00001465
Iteration 114/1000 | Loss: 0.00001465
Iteration 115/1000 | Loss: 0.00001465
Iteration 116/1000 | Loss: 0.00001465
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001464
Iteration 120/1000 | Loss: 0.00001464
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Iteration 133/1000 | Loss: 0.00001462
Iteration 134/1000 | Loss: 0.00001462
Iteration 135/1000 | Loss: 0.00001462
Iteration 136/1000 | Loss: 0.00001462
Iteration 137/1000 | Loss: 0.00001461
Iteration 138/1000 | Loss: 0.00001461
Iteration 139/1000 | Loss: 0.00001461
Iteration 140/1000 | Loss: 0.00001461
Iteration 141/1000 | Loss: 0.00001461
Iteration 142/1000 | Loss: 0.00001461
Iteration 143/1000 | Loss: 0.00001461
Iteration 144/1000 | Loss: 0.00001461
Iteration 145/1000 | Loss: 0.00001461
Iteration 146/1000 | Loss: 0.00001461
Iteration 147/1000 | Loss: 0.00001461
Iteration 148/1000 | Loss: 0.00001461
Iteration 149/1000 | Loss: 0.00001461
Iteration 150/1000 | Loss: 0.00001461
Iteration 151/1000 | Loss: 0.00001460
Iteration 152/1000 | Loss: 0.00001460
Iteration 153/1000 | Loss: 0.00001460
Iteration 154/1000 | Loss: 0.00001460
Iteration 155/1000 | Loss: 0.00001460
Iteration 156/1000 | Loss: 0.00001460
Iteration 157/1000 | Loss: 0.00001460
Iteration 158/1000 | Loss: 0.00001460
Iteration 159/1000 | Loss: 0.00001460
Iteration 160/1000 | Loss: 0.00001460
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001460
Iteration 163/1000 | Loss: 0.00001459
Iteration 164/1000 | Loss: 0.00001459
Iteration 165/1000 | Loss: 0.00001459
Iteration 166/1000 | Loss: 0.00001459
Iteration 167/1000 | Loss: 0.00001459
Iteration 168/1000 | Loss: 0.00001459
Iteration 169/1000 | Loss: 0.00001459
Iteration 170/1000 | Loss: 0.00001459
Iteration 171/1000 | Loss: 0.00001459
Iteration 172/1000 | Loss: 0.00001459
Iteration 173/1000 | Loss: 0.00001459
Iteration 174/1000 | Loss: 0.00001459
Iteration 175/1000 | Loss: 0.00001459
Iteration 176/1000 | Loss: 0.00001459
Iteration 177/1000 | Loss: 0.00001459
Iteration 178/1000 | Loss: 0.00001459
Iteration 179/1000 | Loss: 0.00001459
Iteration 180/1000 | Loss: 0.00001458
Iteration 181/1000 | Loss: 0.00001458
Iteration 182/1000 | Loss: 0.00001458
Iteration 183/1000 | Loss: 0.00001458
Iteration 184/1000 | Loss: 0.00001458
Iteration 185/1000 | Loss: 0.00001458
Iteration 186/1000 | Loss: 0.00001458
Iteration 187/1000 | Loss: 0.00001458
Iteration 188/1000 | Loss: 0.00001458
Iteration 189/1000 | Loss: 0.00001458
Iteration 190/1000 | Loss: 0.00001457
Iteration 191/1000 | Loss: 0.00001457
Iteration 192/1000 | Loss: 0.00001457
Iteration 193/1000 | Loss: 0.00001457
Iteration 194/1000 | Loss: 0.00001457
Iteration 195/1000 | Loss: 0.00001457
Iteration 196/1000 | Loss: 0.00001457
Iteration 197/1000 | Loss: 0.00001457
Iteration 198/1000 | Loss: 0.00001457
Iteration 199/1000 | Loss: 0.00001457
Iteration 200/1000 | Loss: 0.00001457
Iteration 201/1000 | Loss: 0.00001457
Iteration 202/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.4573776752513368e-05, 1.4573776752513368e-05, 1.4573776752513368e-05, 1.4573776752513368e-05, 1.4573776752513368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4573776752513368e-05

Optimization complete. Final v2v error: 3.2598373889923096 mm

Highest mean error: 3.498654842376709 mm for frame 115

Lowest mean error: 3.000128746032715 mm for frame 13

Saving results

Total time: 41.9642128944397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00352995
Iteration 2/25 | Loss: 0.00136558
Iteration 3/25 | Loss: 0.00125433
Iteration 4/25 | Loss: 0.00123416
Iteration 5/25 | Loss: 0.00122762
Iteration 6/25 | Loss: 0.00122603
Iteration 7/25 | Loss: 0.00122592
Iteration 8/25 | Loss: 0.00122592
Iteration 9/25 | Loss: 0.00122592
Iteration 10/25 | Loss: 0.00122592
Iteration 11/25 | Loss: 0.00122592
Iteration 12/25 | Loss: 0.00122592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012259215582162142, 0.0012259215582162142, 0.0012259215582162142, 0.0012259215582162142, 0.0012259215582162142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012259215582162142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50247025
Iteration 2/25 | Loss: 0.00079122
Iteration 3/25 | Loss: 0.00079122
Iteration 4/25 | Loss: 0.00079122
Iteration 5/25 | Loss: 0.00079122
Iteration 6/25 | Loss: 0.00079122
Iteration 7/25 | Loss: 0.00079122
Iteration 8/25 | Loss: 0.00079122
Iteration 9/25 | Loss: 0.00079122
Iteration 10/25 | Loss: 0.00079122
Iteration 11/25 | Loss: 0.00079122
Iteration 12/25 | Loss: 0.00079122
Iteration 13/25 | Loss: 0.00079122
Iteration 14/25 | Loss: 0.00079122
Iteration 15/25 | Loss: 0.00079122
Iteration 16/25 | Loss: 0.00079122
Iteration 17/25 | Loss: 0.00079122
Iteration 18/25 | Loss: 0.00079122
Iteration 19/25 | Loss: 0.00079122
Iteration 20/25 | Loss: 0.00079122
Iteration 21/25 | Loss: 0.00079122
Iteration 22/25 | Loss: 0.00079122
Iteration 23/25 | Loss: 0.00079122
Iteration 24/25 | Loss: 0.00079122
Iteration 25/25 | Loss: 0.00079122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079122
Iteration 2/1000 | Loss: 0.00004761
Iteration 3/1000 | Loss: 0.00003141
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00001930
Iteration 6/1000 | Loss: 0.00001809
Iteration 7/1000 | Loss: 0.00001700
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001579
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001514
Iteration 12/1000 | Loss: 0.00001493
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001471
Iteration 15/1000 | Loss: 0.00001455
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001448
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001446
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001433
Iteration 22/1000 | Loss: 0.00001433
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001430
Iteration 26/1000 | Loss: 0.00001430
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001430
Iteration 29/1000 | Loss: 0.00001429
Iteration 30/1000 | Loss: 0.00001429
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001428
Iteration 33/1000 | Loss: 0.00001428
Iteration 34/1000 | Loss: 0.00001427
Iteration 35/1000 | Loss: 0.00001427
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001426
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001426
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001425
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001422
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001422
Iteration 60/1000 | Loss: 0.00001422
Iteration 61/1000 | Loss: 0.00001422
Iteration 62/1000 | Loss: 0.00001422
Iteration 63/1000 | Loss: 0.00001421
Iteration 64/1000 | Loss: 0.00001421
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001421
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001420
Iteration 69/1000 | Loss: 0.00001420
Iteration 70/1000 | Loss: 0.00001420
Iteration 71/1000 | Loss: 0.00001420
Iteration 72/1000 | Loss: 0.00001419
Iteration 73/1000 | Loss: 0.00001419
Iteration 74/1000 | Loss: 0.00001419
Iteration 75/1000 | Loss: 0.00001419
Iteration 76/1000 | Loss: 0.00001419
Iteration 77/1000 | Loss: 0.00001419
Iteration 78/1000 | Loss: 0.00001419
Iteration 79/1000 | Loss: 0.00001419
Iteration 80/1000 | Loss: 0.00001418
Iteration 81/1000 | Loss: 0.00001418
Iteration 82/1000 | Loss: 0.00001418
Iteration 83/1000 | Loss: 0.00001418
Iteration 84/1000 | Loss: 0.00001417
Iteration 85/1000 | Loss: 0.00001417
Iteration 86/1000 | Loss: 0.00001417
Iteration 87/1000 | Loss: 0.00001417
Iteration 88/1000 | Loss: 0.00001417
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001416
Iteration 91/1000 | Loss: 0.00001416
Iteration 92/1000 | Loss: 0.00001416
Iteration 93/1000 | Loss: 0.00001416
Iteration 94/1000 | Loss: 0.00001416
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001416
Iteration 98/1000 | Loss: 0.00001416
Iteration 99/1000 | Loss: 0.00001416
Iteration 100/1000 | Loss: 0.00001415
Iteration 101/1000 | Loss: 0.00001415
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001414
Iteration 106/1000 | Loss: 0.00001414
Iteration 107/1000 | Loss: 0.00001414
Iteration 108/1000 | Loss: 0.00001414
Iteration 109/1000 | Loss: 0.00001414
Iteration 110/1000 | Loss: 0.00001414
Iteration 111/1000 | Loss: 0.00001413
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001413
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001413
Iteration 118/1000 | Loss: 0.00001413
Iteration 119/1000 | Loss: 0.00001413
Iteration 120/1000 | Loss: 0.00001413
Iteration 121/1000 | Loss: 0.00001412
Iteration 122/1000 | Loss: 0.00001412
Iteration 123/1000 | Loss: 0.00001412
Iteration 124/1000 | Loss: 0.00001412
Iteration 125/1000 | Loss: 0.00001412
Iteration 126/1000 | Loss: 0.00001412
Iteration 127/1000 | Loss: 0.00001412
Iteration 128/1000 | Loss: 0.00001412
Iteration 129/1000 | Loss: 0.00001412
Iteration 130/1000 | Loss: 0.00001412
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Iteration 138/1000 | Loss: 0.00001411
Iteration 139/1000 | Loss: 0.00001411
Iteration 140/1000 | Loss: 0.00001411
Iteration 141/1000 | Loss: 0.00001411
Iteration 142/1000 | Loss: 0.00001411
Iteration 143/1000 | Loss: 0.00001411
Iteration 144/1000 | Loss: 0.00001411
Iteration 145/1000 | Loss: 0.00001411
Iteration 146/1000 | Loss: 0.00001411
Iteration 147/1000 | Loss: 0.00001410
Iteration 148/1000 | Loss: 0.00001410
Iteration 149/1000 | Loss: 0.00001410
Iteration 150/1000 | Loss: 0.00001410
Iteration 151/1000 | Loss: 0.00001410
Iteration 152/1000 | Loss: 0.00001410
Iteration 153/1000 | Loss: 0.00001410
Iteration 154/1000 | Loss: 0.00001410
Iteration 155/1000 | Loss: 0.00001410
Iteration 156/1000 | Loss: 0.00001410
Iteration 157/1000 | Loss: 0.00001410
Iteration 158/1000 | Loss: 0.00001410
Iteration 159/1000 | Loss: 0.00001410
Iteration 160/1000 | Loss: 0.00001410
Iteration 161/1000 | Loss: 0.00001410
Iteration 162/1000 | Loss: 0.00001410
Iteration 163/1000 | Loss: 0.00001410
Iteration 164/1000 | Loss: 0.00001410
Iteration 165/1000 | Loss: 0.00001410
Iteration 166/1000 | Loss: 0.00001410
Iteration 167/1000 | Loss: 0.00001410
Iteration 168/1000 | Loss: 0.00001410
Iteration 169/1000 | Loss: 0.00001410
Iteration 170/1000 | Loss: 0.00001410
Iteration 171/1000 | Loss: 0.00001410
Iteration 172/1000 | Loss: 0.00001410
Iteration 173/1000 | Loss: 0.00001410
Iteration 174/1000 | Loss: 0.00001410
Iteration 175/1000 | Loss: 0.00001410
Iteration 176/1000 | Loss: 0.00001410
Iteration 177/1000 | Loss: 0.00001410
Iteration 178/1000 | Loss: 0.00001410
Iteration 179/1000 | Loss: 0.00001410
Iteration 180/1000 | Loss: 0.00001410
Iteration 181/1000 | Loss: 0.00001410
Iteration 182/1000 | Loss: 0.00001410
Iteration 183/1000 | Loss: 0.00001410
Iteration 184/1000 | Loss: 0.00001410
Iteration 185/1000 | Loss: 0.00001410
Iteration 186/1000 | Loss: 0.00001410
Iteration 187/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.4100627595325932e-05, 1.4100627595325932e-05, 1.4100627595325932e-05, 1.4100627595325932e-05, 1.4100627595325932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4100627595325932e-05

Optimization complete. Final v2v error: 3.2143521308898926 mm

Highest mean error: 3.4280905723571777 mm for frame 104

Lowest mean error: 3.043085813522339 mm for frame 16

Saving results

Total time: 40.71219515800476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00519053
Iteration 2/25 | Loss: 0.00143769
Iteration 3/25 | Loss: 0.00133048
Iteration 4/25 | Loss: 0.00132319
Iteration 5/25 | Loss: 0.00132228
Iteration 6/25 | Loss: 0.00132228
Iteration 7/25 | Loss: 0.00132228
Iteration 8/25 | Loss: 0.00132228
Iteration 9/25 | Loss: 0.00132228
Iteration 10/25 | Loss: 0.00132228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001322275958955288, 0.001322275958955288, 0.001322275958955288, 0.001322275958955288, 0.001322275958955288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001322275958955288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86581337
Iteration 2/25 | Loss: 0.00079453
Iteration 3/25 | Loss: 0.00079452
Iteration 4/25 | Loss: 0.00079452
Iteration 5/25 | Loss: 0.00079452
Iteration 6/25 | Loss: 0.00079452
Iteration 7/25 | Loss: 0.00079452
Iteration 8/25 | Loss: 0.00079452
Iteration 9/25 | Loss: 0.00079452
Iteration 10/25 | Loss: 0.00079452
Iteration 11/25 | Loss: 0.00079452
Iteration 12/25 | Loss: 0.00079452
Iteration 13/25 | Loss: 0.00079452
Iteration 14/25 | Loss: 0.00079452
Iteration 15/25 | Loss: 0.00079452
Iteration 16/25 | Loss: 0.00079452
Iteration 17/25 | Loss: 0.00079452
Iteration 18/25 | Loss: 0.00079452
Iteration 19/25 | Loss: 0.00079452
Iteration 20/25 | Loss: 0.00079452
Iteration 21/25 | Loss: 0.00079452
Iteration 22/25 | Loss: 0.00079452
Iteration 23/25 | Loss: 0.00079452
Iteration 24/25 | Loss: 0.00079452
Iteration 25/25 | Loss: 0.00079452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079452
Iteration 2/1000 | Loss: 0.00002694
Iteration 3/1000 | Loss: 0.00002055
Iteration 4/1000 | Loss: 0.00001889
Iteration 5/1000 | Loss: 0.00001803
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00001728
Iteration 8/1000 | Loss: 0.00001699
Iteration 9/1000 | Loss: 0.00001678
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001630
Iteration 12/1000 | Loss: 0.00001612
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001601
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001600
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001600
Iteration 19/1000 | Loss: 0.00001600
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001594
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001583
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001566
Iteration 37/1000 | Loss: 0.00001561
Iteration 38/1000 | Loss: 0.00001560
Iteration 39/1000 | Loss: 0.00001560
Iteration 40/1000 | Loss: 0.00001559
Iteration 41/1000 | Loss: 0.00001559
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001556
Iteration 44/1000 | Loss: 0.00001556
Iteration 45/1000 | Loss: 0.00001556
Iteration 46/1000 | Loss: 0.00001556
Iteration 47/1000 | Loss: 0.00001556
Iteration 48/1000 | Loss: 0.00001555
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001555
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00001548
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001547
Iteration 88/1000 | Loss: 0.00001547
Iteration 89/1000 | Loss: 0.00001547
Iteration 90/1000 | Loss: 0.00001547
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001546
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.546386192785576e-05, 1.546386192785576e-05, 1.546386192785576e-05, 1.546386192785576e-05, 1.546386192785576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.546386192785576e-05

Optimization complete. Final v2v error: 3.326265811920166 mm

Highest mean error: 3.376016855239868 mm for frame 2

Lowest mean error: 3.2933290004730225 mm for frame 69

Saving results

Total time: 37.9357168674469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928475
Iteration 2/25 | Loss: 0.00151551
Iteration 3/25 | Loss: 0.00137115
Iteration 4/25 | Loss: 0.00134144
Iteration 5/25 | Loss: 0.00133481
Iteration 6/25 | Loss: 0.00133413
Iteration 7/25 | Loss: 0.00133413
Iteration 8/25 | Loss: 0.00133413
Iteration 9/25 | Loss: 0.00133413
Iteration 10/25 | Loss: 0.00133413
Iteration 11/25 | Loss: 0.00133413
Iteration 12/25 | Loss: 0.00133413
Iteration 13/25 | Loss: 0.00133413
Iteration 14/25 | Loss: 0.00133413
Iteration 15/25 | Loss: 0.00133413
Iteration 16/25 | Loss: 0.00133413
Iteration 17/25 | Loss: 0.00133413
Iteration 18/25 | Loss: 0.00133413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013341320445761085, 0.0013341320445761085, 0.0013341320445761085, 0.0013341320445761085, 0.0013341320445761085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013341320445761085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09859371
Iteration 2/25 | Loss: 0.00113838
Iteration 3/25 | Loss: 0.00113833
Iteration 4/25 | Loss: 0.00113833
Iteration 5/25 | Loss: 0.00113833
Iteration 6/25 | Loss: 0.00113833
Iteration 7/25 | Loss: 0.00113833
Iteration 8/25 | Loss: 0.00113833
Iteration 9/25 | Loss: 0.00113833
Iteration 10/25 | Loss: 0.00113833
Iteration 11/25 | Loss: 0.00113833
Iteration 12/25 | Loss: 0.00113833
Iteration 13/25 | Loss: 0.00113832
Iteration 14/25 | Loss: 0.00113832
Iteration 15/25 | Loss: 0.00113832
Iteration 16/25 | Loss: 0.00113832
Iteration 17/25 | Loss: 0.00113832
Iteration 18/25 | Loss: 0.00113832
Iteration 19/25 | Loss: 0.00113832
Iteration 20/25 | Loss: 0.00113832
Iteration 21/25 | Loss: 0.00113832
Iteration 22/25 | Loss: 0.00113832
Iteration 23/25 | Loss: 0.00113832
Iteration 24/25 | Loss: 0.00113832
Iteration 25/25 | Loss: 0.00113832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011383246164768934, 0.0011383246164768934, 0.0011383246164768934, 0.0011383246164768934, 0.0011383246164768934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011383246164768934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113832
Iteration 2/1000 | Loss: 0.00004075
Iteration 3/1000 | Loss: 0.00002765
Iteration 4/1000 | Loss: 0.00002572
Iteration 5/1000 | Loss: 0.00002459
Iteration 6/1000 | Loss: 0.00002398
Iteration 7/1000 | Loss: 0.00002346
Iteration 8/1000 | Loss: 0.00002306
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002223
Iteration 11/1000 | Loss: 0.00002194
Iteration 12/1000 | Loss: 0.00002172
Iteration 13/1000 | Loss: 0.00002162
Iteration 14/1000 | Loss: 0.00002155
Iteration 15/1000 | Loss: 0.00002152
Iteration 16/1000 | Loss: 0.00002149
Iteration 17/1000 | Loss: 0.00002149
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002149
Iteration 20/1000 | Loss: 0.00002148
Iteration 21/1000 | Loss: 0.00002148
Iteration 22/1000 | Loss: 0.00002148
Iteration 23/1000 | Loss: 0.00002148
Iteration 24/1000 | Loss: 0.00002147
Iteration 25/1000 | Loss: 0.00002147
Iteration 26/1000 | Loss: 0.00002146
Iteration 27/1000 | Loss: 0.00002146
Iteration 28/1000 | Loss: 0.00002145
Iteration 29/1000 | Loss: 0.00002145
Iteration 30/1000 | Loss: 0.00002143
Iteration 31/1000 | Loss: 0.00002142
Iteration 32/1000 | Loss: 0.00002141
Iteration 33/1000 | Loss: 0.00002141
Iteration 34/1000 | Loss: 0.00002140
Iteration 35/1000 | Loss: 0.00002140
Iteration 36/1000 | Loss: 0.00002140
Iteration 37/1000 | Loss: 0.00002139
Iteration 38/1000 | Loss: 0.00002138
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002136
Iteration 57/1000 | Loss: 0.00002136
Iteration 58/1000 | Loss: 0.00002136
Iteration 59/1000 | Loss: 0.00002135
Iteration 60/1000 | Loss: 0.00002135
Iteration 61/1000 | Loss: 0.00002135
Iteration 62/1000 | Loss: 0.00002135
Iteration 63/1000 | Loss: 0.00002135
Iteration 64/1000 | Loss: 0.00002135
Iteration 65/1000 | Loss: 0.00002135
Iteration 66/1000 | Loss: 0.00002135
Iteration 67/1000 | Loss: 0.00002135
Iteration 68/1000 | Loss: 0.00002134
Iteration 69/1000 | Loss: 0.00002134
Iteration 70/1000 | Loss: 0.00002134
Iteration 71/1000 | Loss: 0.00002134
Iteration 72/1000 | Loss: 0.00002134
Iteration 73/1000 | Loss: 0.00002134
Iteration 74/1000 | Loss: 0.00002134
Iteration 75/1000 | Loss: 0.00002134
Iteration 76/1000 | Loss: 0.00002134
Iteration 77/1000 | Loss: 0.00002134
Iteration 78/1000 | Loss: 0.00002134
Iteration 79/1000 | Loss: 0.00002134
Iteration 80/1000 | Loss: 0.00002134
Iteration 81/1000 | Loss: 0.00002134
Iteration 82/1000 | Loss: 0.00002134
Iteration 83/1000 | Loss: 0.00002134
Iteration 84/1000 | Loss: 0.00002134
Iteration 85/1000 | Loss: 0.00002134
Iteration 86/1000 | Loss: 0.00002134
Iteration 87/1000 | Loss: 0.00002134
Iteration 88/1000 | Loss: 0.00002134
Iteration 89/1000 | Loss: 0.00002134
Iteration 90/1000 | Loss: 0.00002134
Iteration 91/1000 | Loss: 0.00002134
Iteration 92/1000 | Loss: 0.00002134
Iteration 93/1000 | Loss: 0.00002134
Iteration 94/1000 | Loss: 0.00002134
Iteration 95/1000 | Loss: 0.00002134
Iteration 96/1000 | Loss: 0.00002134
Iteration 97/1000 | Loss: 0.00002134
Iteration 98/1000 | Loss: 0.00002134
Iteration 99/1000 | Loss: 0.00002134
Iteration 100/1000 | Loss: 0.00002134
Iteration 101/1000 | Loss: 0.00002134
Iteration 102/1000 | Loss: 0.00002134
Iteration 103/1000 | Loss: 0.00002134
Iteration 104/1000 | Loss: 0.00002134
Iteration 105/1000 | Loss: 0.00002134
Iteration 106/1000 | Loss: 0.00002134
Iteration 107/1000 | Loss: 0.00002134
Iteration 108/1000 | Loss: 0.00002134
Iteration 109/1000 | Loss: 0.00002134
Iteration 110/1000 | Loss: 0.00002134
Iteration 111/1000 | Loss: 0.00002134
Iteration 112/1000 | Loss: 0.00002134
Iteration 113/1000 | Loss: 0.00002134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.134081114490982e-05, 2.134081114490982e-05, 2.134081114490982e-05, 2.134081114490982e-05, 2.134081114490982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.134081114490982e-05

Optimization complete. Final v2v error: 3.944789409637451 mm

Highest mean error: 4.334862232208252 mm for frame 44

Lowest mean error: 3.5488102436065674 mm for frame 135

Saving results

Total time: 33.05277872085571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967809
Iteration 2/25 | Loss: 0.00170388
Iteration 3/25 | Loss: 0.00140796
Iteration 4/25 | Loss: 0.00137528
Iteration 5/25 | Loss: 0.00136527
Iteration 6/25 | Loss: 0.00135428
Iteration 7/25 | Loss: 0.00136197
Iteration 8/25 | Loss: 0.00135489
Iteration 9/25 | Loss: 0.00134510
Iteration 10/25 | Loss: 0.00134387
Iteration 11/25 | Loss: 0.00134359
Iteration 12/25 | Loss: 0.00134359
Iteration 13/25 | Loss: 0.00134359
Iteration 14/25 | Loss: 0.00134358
Iteration 15/25 | Loss: 0.00134358
Iteration 16/25 | Loss: 0.00134358
Iteration 17/25 | Loss: 0.00134358
Iteration 18/25 | Loss: 0.00134358
Iteration 19/25 | Loss: 0.00134358
Iteration 20/25 | Loss: 0.00134358
Iteration 21/25 | Loss: 0.00134358
Iteration 22/25 | Loss: 0.00134358
Iteration 23/25 | Loss: 0.00134358
Iteration 24/25 | Loss: 0.00134358
Iteration 25/25 | Loss: 0.00134358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.61076927
Iteration 2/25 | Loss: 0.00144997
Iteration 3/25 | Loss: 0.00144991
Iteration 4/25 | Loss: 0.00144991
Iteration 5/25 | Loss: 0.00144991
Iteration 6/25 | Loss: 0.00144991
Iteration 7/25 | Loss: 0.00144991
Iteration 8/25 | Loss: 0.00144991
Iteration 9/25 | Loss: 0.00144991
Iteration 10/25 | Loss: 0.00144991
Iteration 11/25 | Loss: 0.00144991
Iteration 12/25 | Loss: 0.00144990
Iteration 13/25 | Loss: 0.00144990
Iteration 14/25 | Loss: 0.00144990
Iteration 15/25 | Loss: 0.00144990
Iteration 16/25 | Loss: 0.00144990
Iteration 17/25 | Loss: 0.00144990
Iteration 18/25 | Loss: 0.00144990
Iteration 19/25 | Loss: 0.00144990
Iteration 20/25 | Loss: 0.00144990
Iteration 21/25 | Loss: 0.00144990
Iteration 22/25 | Loss: 0.00144990
Iteration 23/25 | Loss: 0.00144990
Iteration 24/25 | Loss: 0.00144990
Iteration 25/25 | Loss: 0.00144990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144990
Iteration 2/1000 | Loss: 0.00015563
Iteration 3/1000 | Loss: 0.00016360
Iteration 4/1000 | Loss: 0.00009923
Iteration 5/1000 | Loss: 0.00004838
Iteration 6/1000 | Loss: 0.00007838
Iteration 7/1000 | Loss: 0.00005795
Iteration 8/1000 | Loss: 0.00005450
Iteration 9/1000 | Loss: 0.00006742
Iteration 10/1000 | Loss: 0.00004512
Iteration 11/1000 | Loss: 0.00005253
Iteration 12/1000 | Loss: 0.00004425
Iteration 13/1000 | Loss: 0.00004186
Iteration 14/1000 | Loss: 0.00004119
Iteration 15/1000 | Loss: 0.00004399
Iteration 16/1000 | Loss: 0.00002505
Iteration 17/1000 | Loss: 0.00003576
Iteration 18/1000 | Loss: 0.00004806
Iteration 19/1000 | Loss: 0.00003572
Iteration 20/1000 | Loss: 0.00002820
Iteration 21/1000 | Loss: 0.00003395
Iteration 22/1000 | Loss: 0.00002602
Iteration 23/1000 | Loss: 0.00002841
Iteration 24/1000 | Loss: 0.00003379
Iteration 25/1000 | Loss: 0.00030269
Iteration 26/1000 | Loss: 0.00003806
Iteration 27/1000 | Loss: 0.00004236
Iteration 28/1000 | Loss: 0.00005893
Iteration 29/1000 | Loss: 0.00004549
Iteration 30/1000 | Loss: 0.00003345
Iteration 31/1000 | Loss: 0.00003777
Iteration 32/1000 | Loss: 0.00004838
Iteration 33/1000 | Loss: 0.00005366
Iteration 34/1000 | Loss: 0.00004684
Iteration 35/1000 | Loss: 0.00004228
Iteration 36/1000 | Loss: 0.00003515
Iteration 37/1000 | Loss: 0.00005293
Iteration 38/1000 | Loss: 0.00004419
Iteration 39/1000 | Loss: 0.00004310
Iteration 40/1000 | Loss: 0.00004703
Iteration 41/1000 | Loss: 0.00003875
Iteration 42/1000 | Loss: 0.00004535
Iteration 43/1000 | Loss: 0.00004446
Iteration 44/1000 | Loss: 0.00004386
Iteration 45/1000 | Loss: 0.00003140
Iteration 46/1000 | Loss: 0.00002778
Iteration 47/1000 | Loss: 0.00003627
Iteration 48/1000 | Loss: 0.00004017
Iteration 49/1000 | Loss: 0.00003552
Iteration 50/1000 | Loss: 0.00003301
Iteration 51/1000 | Loss: 0.00004950
Iteration 52/1000 | Loss: 0.00005587
Iteration 53/1000 | Loss: 0.00004590
Iteration 54/1000 | Loss: 0.00003515
Iteration 55/1000 | Loss: 0.00004811
Iteration 56/1000 | Loss: 0.00003836
Iteration 57/1000 | Loss: 0.00002278
Iteration 58/1000 | Loss: 0.00002096
Iteration 59/1000 | Loss: 0.00001953
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001880
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001832
Iteration 65/1000 | Loss: 0.00001822
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001800
Iteration 68/1000 | Loss: 0.00001799
Iteration 69/1000 | Loss: 0.00001794
Iteration 70/1000 | Loss: 0.00001794
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001790
Iteration 73/1000 | Loss: 0.00001790
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001788
Iteration 80/1000 | Loss: 0.00001788
Iteration 81/1000 | Loss: 0.00001788
Iteration 82/1000 | Loss: 0.00001788
Iteration 83/1000 | Loss: 0.00001788
Iteration 84/1000 | Loss: 0.00001788
Iteration 85/1000 | Loss: 0.00001787
Iteration 86/1000 | Loss: 0.00001787
Iteration 87/1000 | Loss: 0.00001787
Iteration 88/1000 | Loss: 0.00001787
Iteration 89/1000 | Loss: 0.00001786
Iteration 90/1000 | Loss: 0.00001786
Iteration 91/1000 | Loss: 0.00001786
Iteration 92/1000 | Loss: 0.00001786
Iteration 93/1000 | Loss: 0.00001786
Iteration 94/1000 | Loss: 0.00001786
Iteration 95/1000 | Loss: 0.00001786
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001785
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001783
Iteration 107/1000 | Loss: 0.00001783
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001780
Iteration 123/1000 | Loss: 0.00001780
Iteration 124/1000 | Loss: 0.00001780
Iteration 125/1000 | Loss: 0.00001780
Iteration 126/1000 | Loss: 0.00001779
Iteration 127/1000 | Loss: 0.00001779
Iteration 128/1000 | Loss: 0.00001779
Iteration 129/1000 | Loss: 0.00001779
Iteration 130/1000 | Loss: 0.00001779
Iteration 131/1000 | Loss: 0.00001779
Iteration 132/1000 | Loss: 0.00001778
Iteration 133/1000 | Loss: 0.00001778
Iteration 134/1000 | Loss: 0.00001778
Iteration 135/1000 | Loss: 0.00001778
Iteration 136/1000 | Loss: 0.00001778
Iteration 137/1000 | Loss: 0.00001778
Iteration 138/1000 | Loss: 0.00001778
Iteration 139/1000 | Loss: 0.00001778
Iteration 140/1000 | Loss: 0.00001778
Iteration 141/1000 | Loss: 0.00001778
Iteration 142/1000 | Loss: 0.00001777
Iteration 143/1000 | Loss: 0.00001777
Iteration 144/1000 | Loss: 0.00001777
Iteration 145/1000 | Loss: 0.00001777
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001776
Iteration 148/1000 | Loss: 0.00001776
Iteration 149/1000 | Loss: 0.00001776
Iteration 150/1000 | Loss: 0.00001776
Iteration 151/1000 | Loss: 0.00001775
Iteration 152/1000 | Loss: 0.00001775
Iteration 153/1000 | Loss: 0.00001775
Iteration 154/1000 | Loss: 0.00001775
Iteration 155/1000 | Loss: 0.00001775
Iteration 156/1000 | Loss: 0.00001775
Iteration 157/1000 | Loss: 0.00001775
Iteration 158/1000 | Loss: 0.00001775
Iteration 159/1000 | Loss: 0.00001775
Iteration 160/1000 | Loss: 0.00001774
Iteration 161/1000 | Loss: 0.00001774
Iteration 162/1000 | Loss: 0.00001774
Iteration 163/1000 | Loss: 0.00001774
Iteration 164/1000 | Loss: 0.00001774
Iteration 165/1000 | Loss: 0.00001774
Iteration 166/1000 | Loss: 0.00001774
Iteration 167/1000 | Loss: 0.00001774
Iteration 168/1000 | Loss: 0.00001773
Iteration 169/1000 | Loss: 0.00001773
Iteration 170/1000 | Loss: 0.00001773
Iteration 171/1000 | Loss: 0.00001773
Iteration 172/1000 | Loss: 0.00001773
Iteration 173/1000 | Loss: 0.00001773
Iteration 174/1000 | Loss: 0.00001773
Iteration 175/1000 | Loss: 0.00001773
Iteration 176/1000 | Loss: 0.00001773
Iteration 177/1000 | Loss: 0.00001773
Iteration 178/1000 | Loss: 0.00001773
Iteration 179/1000 | Loss: 0.00001773
Iteration 180/1000 | Loss: 0.00001773
Iteration 181/1000 | Loss: 0.00001773
Iteration 182/1000 | Loss: 0.00001773
Iteration 183/1000 | Loss: 0.00001773
Iteration 184/1000 | Loss: 0.00001773
Iteration 185/1000 | Loss: 0.00001772
Iteration 186/1000 | Loss: 0.00001772
Iteration 187/1000 | Loss: 0.00001772
Iteration 188/1000 | Loss: 0.00001772
Iteration 189/1000 | Loss: 0.00001772
Iteration 190/1000 | Loss: 0.00001772
Iteration 191/1000 | Loss: 0.00001772
Iteration 192/1000 | Loss: 0.00001771
Iteration 193/1000 | Loss: 0.00001771
Iteration 194/1000 | Loss: 0.00001771
Iteration 195/1000 | Loss: 0.00001771
Iteration 196/1000 | Loss: 0.00001771
Iteration 197/1000 | Loss: 0.00001770
Iteration 198/1000 | Loss: 0.00001770
Iteration 199/1000 | Loss: 0.00001770
Iteration 200/1000 | Loss: 0.00001770
Iteration 201/1000 | Loss: 0.00001770
Iteration 202/1000 | Loss: 0.00001770
Iteration 203/1000 | Loss: 0.00001770
Iteration 204/1000 | Loss: 0.00001770
Iteration 205/1000 | Loss: 0.00001770
Iteration 206/1000 | Loss: 0.00001769
Iteration 207/1000 | Loss: 0.00001769
Iteration 208/1000 | Loss: 0.00001769
Iteration 209/1000 | Loss: 0.00001769
Iteration 210/1000 | Loss: 0.00001769
Iteration 211/1000 | Loss: 0.00001769
Iteration 212/1000 | Loss: 0.00001769
Iteration 213/1000 | Loss: 0.00001769
Iteration 214/1000 | Loss: 0.00001769
Iteration 215/1000 | Loss: 0.00001768
Iteration 216/1000 | Loss: 0.00001768
Iteration 217/1000 | Loss: 0.00001768
Iteration 218/1000 | Loss: 0.00001768
Iteration 219/1000 | Loss: 0.00001768
Iteration 220/1000 | Loss: 0.00001768
Iteration 221/1000 | Loss: 0.00001768
Iteration 222/1000 | Loss: 0.00001768
Iteration 223/1000 | Loss: 0.00001768
Iteration 224/1000 | Loss: 0.00001768
Iteration 225/1000 | Loss: 0.00001768
Iteration 226/1000 | Loss: 0.00001768
Iteration 227/1000 | Loss: 0.00001768
Iteration 228/1000 | Loss: 0.00001768
Iteration 229/1000 | Loss: 0.00001768
Iteration 230/1000 | Loss: 0.00001768
Iteration 231/1000 | Loss: 0.00001768
Iteration 232/1000 | Loss: 0.00001768
Iteration 233/1000 | Loss: 0.00001768
Iteration 234/1000 | Loss: 0.00001768
Iteration 235/1000 | Loss: 0.00001768
Iteration 236/1000 | Loss: 0.00001768
Iteration 237/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.7682306861388497e-05, 1.7682306861388497e-05, 1.7682306861388497e-05, 1.7682306861388497e-05, 1.7682306861388497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7682306861388497e-05

Optimization complete. Final v2v error: 3.483851671218872 mm

Highest mean error: 5.067947864532471 mm for frame 35

Lowest mean error: 2.921283483505249 mm for frame 81

Saving results

Total time: 120.38872480392456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551506
Iteration 2/25 | Loss: 0.00156186
Iteration 3/25 | Loss: 0.00139915
Iteration 4/25 | Loss: 0.00137724
Iteration 5/25 | Loss: 0.00137263
Iteration 6/25 | Loss: 0.00137260
Iteration 7/25 | Loss: 0.00137260
Iteration 8/25 | Loss: 0.00137260
Iteration 9/25 | Loss: 0.00137260
Iteration 10/25 | Loss: 0.00137260
Iteration 11/25 | Loss: 0.00137260
Iteration 12/25 | Loss: 0.00137260
Iteration 13/25 | Loss: 0.00137260
Iteration 14/25 | Loss: 0.00137260
Iteration 15/25 | Loss: 0.00137260
Iteration 16/25 | Loss: 0.00137260
Iteration 17/25 | Loss: 0.00137260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013726036995649338, 0.0013726036995649338, 0.0013726036995649338, 0.0013726036995649338, 0.0013726036995649338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013726036995649338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45478892
Iteration 2/25 | Loss: 0.00093455
Iteration 3/25 | Loss: 0.00093455
Iteration 4/25 | Loss: 0.00093455
Iteration 5/25 | Loss: 0.00093455
Iteration 6/25 | Loss: 0.00093455
Iteration 7/25 | Loss: 0.00093455
Iteration 8/25 | Loss: 0.00093455
Iteration 9/25 | Loss: 0.00093455
Iteration 10/25 | Loss: 0.00093455
Iteration 11/25 | Loss: 0.00093455
Iteration 12/25 | Loss: 0.00093455
Iteration 13/25 | Loss: 0.00093455
Iteration 14/25 | Loss: 0.00093455
Iteration 15/25 | Loss: 0.00093455
Iteration 16/25 | Loss: 0.00093455
Iteration 17/25 | Loss: 0.00093455
Iteration 18/25 | Loss: 0.00093455
Iteration 19/25 | Loss: 0.00093455
Iteration 20/25 | Loss: 0.00093455
Iteration 21/25 | Loss: 0.00093455
Iteration 22/25 | Loss: 0.00093455
Iteration 23/25 | Loss: 0.00093455
Iteration 24/25 | Loss: 0.00093455
Iteration 25/25 | Loss: 0.00093455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093455
Iteration 2/1000 | Loss: 0.00004680
Iteration 3/1000 | Loss: 0.00003480
Iteration 4/1000 | Loss: 0.00003216
Iteration 5/1000 | Loss: 0.00003083
Iteration 6/1000 | Loss: 0.00002991
Iteration 7/1000 | Loss: 0.00002918
Iteration 8/1000 | Loss: 0.00002888
Iteration 9/1000 | Loss: 0.00002856
Iteration 10/1000 | Loss: 0.00002833
Iteration 11/1000 | Loss: 0.00002817
Iteration 12/1000 | Loss: 0.00002806
Iteration 13/1000 | Loss: 0.00002805
Iteration 14/1000 | Loss: 0.00002803
Iteration 15/1000 | Loss: 0.00002803
Iteration 16/1000 | Loss: 0.00002794
Iteration 17/1000 | Loss: 0.00002788
Iteration 18/1000 | Loss: 0.00002785
Iteration 19/1000 | Loss: 0.00002784
Iteration 20/1000 | Loss: 0.00002783
Iteration 21/1000 | Loss: 0.00002782
Iteration 22/1000 | Loss: 0.00002782
Iteration 23/1000 | Loss: 0.00002782
Iteration 24/1000 | Loss: 0.00002782
Iteration 25/1000 | Loss: 0.00002781
Iteration 26/1000 | Loss: 0.00002781
Iteration 27/1000 | Loss: 0.00002781
Iteration 28/1000 | Loss: 0.00002780
Iteration 29/1000 | Loss: 0.00002780
Iteration 30/1000 | Loss: 0.00002780
Iteration 31/1000 | Loss: 0.00002780
Iteration 32/1000 | Loss: 0.00002780
Iteration 33/1000 | Loss: 0.00002780
Iteration 34/1000 | Loss: 0.00002780
Iteration 35/1000 | Loss: 0.00002780
Iteration 36/1000 | Loss: 0.00002780
Iteration 37/1000 | Loss: 0.00002780
Iteration 38/1000 | Loss: 0.00002779
Iteration 39/1000 | Loss: 0.00002779
Iteration 40/1000 | Loss: 0.00002779
Iteration 41/1000 | Loss: 0.00002779
Iteration 42/1000 | Loss: 0.00002779
Iteration 43/1000 | Loss: 0.00002778
Iteration 44/1000 | Loss: 0.00002778
Iteration 45/1000 | Loss: 0.00002778
Iteration 46/1000 | Loss: 0.00002778
Iteration 47/1000 | Loss: 0.00002778
Iteration 48/1000 | Loss: 0.00002777
Iteration 49/1000 | Loss: 0.00002777
Iteration 50/1000 | Loss: 0.00002777
Iteration 51/1000 | Loss: 0.00002777
Iteration 52/1000 | Loss: 0.00002777
Iteration 53/1000 | Loss: 0.00002777
Iteration 54/1000 | Loss: 0.00002777
Iteration 55/1000 | Loss: 0.00002776
Iteration 56/1000 | Loss: 0.00002776
Iteration 57/1000 | Loss: 0.00002776
Iteration 58/1000 | Loss: 0.00002776
Iteration 59/1000 | Loss: 0.00002776
Iteration 60/1000 | Loss: 0.00002776
Iteration 61/1000 | Loss: 0.00002776
Iteration 62/1000 | Loss: 0.00002776
Iteration 63/1000 | Loss: 0.00002776
Iteration 64/1000 | Loss: 0.00002776
Iteration 65/1000 | Loss: 0.00002776
Iteration 66/1000 | Loss: 0.00002776
Iteration 67/1000 | Loss: 0.00002776
Iteration 68/1000 | Loss: 0.00002776
Iteration 69/1000 | Loss: 0.00002776
Iteration 70/1000 | Loss: 0.00002775
Iteration 71/1000 | Loss: 0.00002775
Iteration 72/1000 | Loss: 0.00002775
Iteration 73/1000 | Loss: 0.00002775
Iteration 74/1000 | Loss: 0.00002775
Iteration 75/1000 | Loss: 0.00002775
Iteration 76/1000 | Loss: 0.00002775
Iteration 77/1000 | Loss: 0.00002775
Iteration 78/1000 | Loss: 0.00002775
Iteration 79/1000 | Loss: 0.00002775
Iteration 80/1000 | Loss: 0.00002775
Iteration 81/1000 | Loss: 0.00002775
Iteration 82/1000 | Loss: 0.00002775
Iteration 83/1000 | Loss: 0.00002775
Iteration 84/1000 | Loss: 0.00002775
Iteration 85/1000 | Loss: 0.00002775
Iteration 86/1000 | Loss: 0.00002775
Iteration 87/1000 | Loss: 0.00002774
Iteration 88/1000 | Loss: 0.00002774
Iteration 89/1000 | Loss: 0.00002774
Iteration 90/1000 | Loss: 0.00002774
Iteration 91/1000 | Loss: 0.00002774
Iteration 92/1000 | Loss: 0.00002774
Iteration 93/1000 | Loss: 0.00002774
Iteration 94/1000 | Loss: 0.00002774
Iteration 95/1000 | Loss: 0.00002774
Iteration 96/1000 | Loss: 0.00002774
Iteration 97/1000 | Loss: 0.00002774
Iteration 98/1000 | Loss: 0.00002774
Iteration 99/1000 | Loss: 0.00002774
Iteration 100/1000 | Loss: 0.00002774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.7743169994209893e-05, 2.7743169994209893e-05, 2.7743169994209893e-05, 2.7743169994209893e-05, 2.7743169994209893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7743169994209893e-05

Optimization complete. Final v2v error: 4.363202095031738 mm

Highest mean error: 4.959295272827148 mm for frame 88

Lowest mean error: 3.935938596725464 mm for frame 138

Saving results

Total time: 35.2505841255188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876074
Iteration 2/25 | Loss: 0.00214235
Iteration 3/25 | Loss: 0.00154185
Iteration 4/25 | Loss: 0.00151682
Iteration 5/25 | Loss: 0.00150973
Iteration 6/25 | Loss: 0.00150794
Iteration 7/25 | Loss: 0.00150794
Iteration 8/25 | Loss: 0.00150794
Iteration 9/25 | Loss: 0.00150794
Iteration 10/25 | Loss: 0.00150794
Iteration 11/25 | Loss: 0.00150794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001507944893091917, 0.001507944893091917, 0.001507944893091917, 0.001507944893091917, 0.001507944893091917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001507944893091917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92997694
Iteration 2/25 | Loss: 0.00083290
Iteration 3/25 | Loss: 0.00083290
Iteration 4/25 | Loss: 0.00083290
Iteration 5/25 | Loss: 0.00083290
Iteration 6/25 | Loss: 0.00083290
Iteration 7/25 | Loss: 0.00083290
Iteration 8/25 | Loss: 0.00083290
Iteration 9/25 | Loss: 0.00083290
Iteration 10/25 | Loss: 0.00083290
Iteration 11/25 | Loss: 0.00083290
Iteration 12/25 | Loss: 0.00083290
Iteration 13/25 | Loss: 0.00083290
Iteration 14/25 | Loss: 0.00083290
Iteration 15/25 | Loss: 0.00083290
Iteration 16/25 | Loss: 0.00083290
Iteration 17/25 | Loss: 0.00083290
Iteration 18/25 | Loss: 0.00083290
Iteration 19/25 | Loss: 0.00083290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008328956901095808, 0.0008328956901095808, 0.0008328956901095808, 0.0008328956901095808, 0.0008328956901095808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008328956901095808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083290
Iteration 2/1000 | Loss: 0.00008600
Iteration 3/1000 | Loss: 0.00006190
Iteration 4/1000 | Loss: 0.00005540
Iteration 5/1000 | Loss: 0.00005282
Iteration 6/1000 | Loss: 0.00005104
Iteration 7/1000 | Loss: 0.00004977
Iteration 8/1000 | Loss: 0.00004867
Iteration 9/1000 | Loss: 0.00004757
Iteration 10/1000 | Loss: 0.00004663
Iteration 11/1000 | Loss: 0.00004572
Iteration 12/1000 | Loss: 0.00004511
Iteration 13/1000 | Loss: 0.00004451
Iteration 14/1000 | Loss: 0.00004389
Iteration 15/1000 | Loss: 0.00004343
Iteration 16/1000 | Loss: 0.00004306
Iteration 17/1000 | Loss: 0.00004269
Iteration 18/1000 | Loss: 0.00004238
Iteration 19/1000 | Loss: 0.00004214
Iteration 20/1000 | Loss: 0.00004194
Iteration 21/1000 | Loss: 0.00004177
Iteration 22/1000 | Loss: 0.00004154
Iteration 23/1000 | Loss: 0.00004131
Iteration 24/1000 | Loss: 0.00004109
Iteration 25/1000 | Loss: 0.00004091
Iteration 26/1000 | Loss: 0.00004082
Iteration 27/1000 | Loss: 0.00004070
Iteration 28/1000 | Loss: 0.00004068
Iteration 29/1000 | Loss: 0.00004068
Iteration 30/1000 | Loss: 0.00004062
Iteration 31/1000 | Loss: 0.00004055
Iteration 32/1000 | Loss: 0.00004052
Iteration 33/1000 | Loss: 0.00004052
Iteration 34/1000 | Loss: 0.00004052
Iteration 35/1000 | Loss: 0.00004052
Iteration 36/1000 | Loss: 0.00004052
Iteration 37/1000 | Loss: 0.00004051
Iteration 38/1000 | Loss: 0.00004050
Iteration 39/1000 | Loss: 0.00004050
Iteration 40/1000 | Loss: 0.00004048
Iteration 41/1000 | Loss: 0.00004048
Iteration 42/1000 | Loss: 0.00004047
Iteration 43/1000 | Loss: 0.00004046
Iteration 44/1000 | Loss: 0.00004046
Iteration 45/1000 | Loss: 0.00004046
Iteration 46/1000 | Loss: 0.00004045
Iteration 47/1000 | Loss: 0.00004045
Iteration 48/1000 | Loss: 0.00004044
Iteration 49/1000 | Loss: 0.00004044
Iteration 50/1000 | Loss: 0.00004043
Iteration 51/1000 | Loss: 0.00004043
Iteration 52/1000 | Loss: 0.00004042
Iteration 53/1000 | Loss: 0.00004042
Iteration 54/1000 | Loss: 0.00004041
Iteration 55/1000 | Loss: 0.00004041
Iteration 56/1000 | Loss: 0.00004038
Iteration 57/1000 | Loss: 0.00004037
Iteration 58/1000 | Loss: 0.00004037
Iteration 59/1000 | Loss: 0.00004037
Iteration 60/1000 | Loss: 0.00004037
Iteration 61/1000 | Loss: 0.00004037
Iteration 62/1000 | Loss: 0.00004037
Iteration 63/1000 | Loss: 0.00004036
Iteration 64/1000 | Loss: 0.00004036
Iteration 65/1000 | Loss: 0.00004035
Iteration 66/1000 | Loss: 0.00004034
Iteration 67/1000 | Loss: 0.00004033
Iteration 68/1000 | Loss: 0.00004033
Iteration 69/1000 | Loss: 0.00004033
Iteration 70/1000 | Loss: 0.00004033
Iteration 71/1000 | Loss: 0.00004033
Iteration 72/1000 | Loss: 0.00004033
Iteration 73/1000 | Loss: 0.00004033
Iteration 74/1000 | Loss: 0.00004033
Iteration 75/1000 | Loss: 0.00004033
Iteration 76/1000 | Loss: 0.00004033
Iteration 77/1000 | Loss: 0.00004033
Iteration 78/1000 | Loss: 0.00004033
Iteration 79/1000 | Loss: 0.00004032
Iteration 80/1000 | Loss: 0.00004032
Iteration 81/1000 | Loss: 0.00004031
Iteration 82/1000 | Loss: 0.00004031
Iteration 83/1000 | Loss: 0.00004031
Iteration 84/1000 | Loss: 0.00004030
Iteration 85/1000 | Loss: 0.00004030
Iteration 86/1000 | Loss: 0.00004030
Iteration 87/1000 | Loss: 0.00004030
Iteration 88/1000 | Loss: 0.00004030
Iteration 89/1000 | Loss: 0.00004030
Iteration 90/1000 | Loss: 0.00004030
Iteration 91/1000 | Loss: 0.00004030
Iteration 92/1000 | Loss: 0.00004030
Iteration 93/1000 | Loss: 0.00004030
Iteration 94/1000 | Loss: 0.00004030
Iteration 95/1000 | Loss: 0.00004030
Iteration 96/1000 | Loss: 0.00004029
Iteration 97/1000 | Loss: 0.00004029
Iteration 98/1000 | Loss: 0.00004029
Iteration 99/1000 | Loss: 0.00004029
Iteration 100/1000 | Loss: 0.00004029
Iteration 101/1000 | Loss: 0.00004029
Iteration 102/1000 | Loss: 0.00004029
Iteration 103/1000 | Loss: 0.00004029
Iteration 104/1000 | Loss: 0.00004029
Iteration 105/1000 | Loss: 0.00004029
Iteration 106/1000 | Loss: 0.00004029
Iteration 107/1000 | Loss: 0.00004029
Iteration 108/1000 | Loss: 0.00004028
Iteration 109/1000 | Loss: 0.00004028
Iteration 110/1000 | Loss: 0.00004028
Iteration 111/1000 | Loss: 0.00004028
Iteration 112/1000 | Loss: 0.00004028
Iteration 113/1000 | Loss: 0.00004028
Iteration 114/1000 | Loss: 0.00004028
Iteration 115/1000 | Loss: 0.00004028
Iteration 116/1000 | Loss: 0.00004028
Iteration 117/1000 | Loss: 0.00004028
Iteration 118/1000 | Loss: 0.00004028
Iteration 119/1000 | Loss: 0.00004028
Iteration 120/1000 | Loss: 0.00004027
Iteration 121/1000 | Loss: 0.00004027
Iteration 122/1000 | Loss: 0.00004027
Iteration 123/1000 | Loss: 0.00004026
Iteration 124/1000 | Loss: 0.00004026
Iteration 125/1000 | Loss: 0.00004026
Iteration 126/1000 | Loss: 0.00004026
Iteration 127/1000 | Loss: 0.00004026
Iteration 128/1000 | Loss: 0.00004026
Iteration 129/1000 | Loss: 0.00004026
Iteration 130/1000 | Loss: 0.00004025
Iteration 131/1000 | Loss: 0.00004025
Iteration 132/1000 | Loss: 0.00004025
Iteration 133/1000 | Loss: 0.00004025
Iteration 134/1000 | Loss: 0.00004025
Iteration 135/1000 | Loss: 0.00004024
Iteration 136/1000 | Loss: 0.00004024
Iteration 137/1000 | Loss: 0.00004024
Iteration 138/1000 | Loss: 0.00004024
Iteration 139/1000 | Loss: 0.00004024
Iteration 140/1000 | Loss: 0.00004024
Iteration 141/1000 | Loss: 0.00004024
Iteration 142/1000 | Loss: 0.00004024
Iteration 143/1000 | Loss: 0.00004024
Iteration 144/1000 | Loss: 0.00004024
Iteration 145/1000 | Loss: 0.00004024
Iteration 146/1000 | Loss: 0.00004023
Iteration 147/1000 | Loss: 0.00004023
Iteration 148/1000 | Loss: 0.00004023
Iteration 149/1000 | Loss: 0.00004023
Iteration 150/1000 | Loss: 0.00004023
Iteration 151/1000 | Loss: 0.00004023
Iteration 152/1000 | Loss: 0.00004023
Iteration 153/1000 | Loss: 0.00004023
Iteration 154/1000 | Loss: 0.00004023
Iteration 155/1000 | Loss: 0.00004023
Iteration 156/1000 | Loss: 0.00004023
Iteration 157/1000 | Loss: 0.00004023
Iteration 158/1000 | Loss: 0.00004023
Iteration 159/1000 | Loss: 0.00004022
Iteration 160/1000 | Loss: 0.00004022
Iteration 161/1000 | Loss: 0.00004022
Iteration 162/1000 | Loss: 0.00004022
Iteration 163/1000 | Loss: 0.00004022
Iteration 164/1000 | Loss: 0.00004022
Iteration 165/1000 | Loss: 0.00004022
Iteration 166/1000 | Loss: 0.00004022
Iteration 167/1000 | Loss: 0.00004022
Iteration 168/1000 | Loss: 0.00004022
Iteration 169/1000 | Loss: 0.00004021
Iteration 170/1000 | Loss: 0.00004021
Iteration 171/1000 | Loss: 0.00004021
Iteration 172/1000 | Loss: 0.00004021
Iteration 173/1000 | Loss: 0.00004020
Iteration 174/1000 | Loss: 0.00004020
Iteration 175/1000 | Loss: 0.00004020
Iteration 176/1000 | Loss: 0.00004020
Iteration 177/1000 | Loss: 0.00004020
Iteration 178/1000 | Loss: 0.00004020
Iteration 179/1000 | Loss: 0.00004019
Iteration 180/1000 | Loss: 0.00004019
Iteration 181/1000 | Loss: 0.00004019
Iteration 182/1000 | Loss: 0.00004019
Iteration 183/1000 | Loss: 0.00004019
Iteration 184/1000 | Loss: 0.00004019
Iteration 185/1000 | Loss: 0.00004019
Iteration 186/1000 | Loss: 0.00004019
Iteration 187/1000 | Loss: 0.00004019
Iteration 188/1000 | Loss: 0.00004019
Iteration 189/1000 | Loss: 0.00004019
Iteration 190/1000 | Loss: 0.00004019
Iteration 191/1000 | Loss: 0.00004019
Iteration 192/1000 | Loss: 0.00004019
Iteration 193/1000 | Loss: 0.00004019
Iteration 194/1000 | Loss: 0.00004019
Iteration 195/1000 | Loss: 0.00004019
Iteration 196/1000 | Loss: 0.00004019
Iteration 197/1000 | Loss: 0.00004019
Iteration 198/1000 | Loss: 0.00004019
Iteration 199/1000 | Loss: 0.00004019
Iteration 200/1000 | Loss: 0.00004019
Iteration 201/1000 | Loss: 0.00004019
Iteration 202/1000 | Loss: 0.00004019
Iteration 203/1000 | Loss: 0.00004019
Iteration 204/1000 | Loss: 0.00004019
Iteration 205/1000 | Loss: 0.00004019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [4.018704203190282e-05, 4.018704203190282e-05, 4.018704203190282e-05, 4.018704203190282e-05, 4.018704203190282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.018704203190282e-05

Optimization complete. Final v2v error: 5.294241428375244 mm

Highest mean error: 5.854725360870361 mm for frame 15

Lowest mean error: 4.808778762817383 mm for frame 183

Saving results

Total time: 67.98022031784058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980366
Iteration 2/25 | Loss: 0.00252532
Iteration 3/25 | Loss: 0.00222944
Iteration 4/25 | Loss: 0.00197125
Iteration 5/25 | Loss: 0.00197823
Iteration 6/25 | Loss: 0.00203148
Iteration 7/25 | Loss: 0.00152173
Iteration 8/25 | Loss: 0.00143886
Iteration 9/25 | Loss: 0.00141100
Iteration 10/25 | Loss: 0.00139098
Iteration 11/25 | Loss: 0.00138389
Iteration 12/25 | Loss: 0.00137748
Iteration 13/25 | Loss: 0.00137948
Iteration 14/25 | Loss: 0.00136492
Iteration 15/25 | Loss: 0.00136079
Iteration 16/25 | Loss: 0.00135968
Iteration 17/25 | Loss: 0.00135933
Iteration 18/25 | Loss: 0.00136591
Iteration 19/25 | Loss: 0.00135786
Iteration 20/25 | Loss: 0.00135665
Iteration 21/25 | Loss: 0.00135641
Iteration 22/25 | Loss: 0.00135640
Iteration 23/25 | Loss: 0.00135639
Iteration 24/25 | Loss: 0.00135639
Iteration 25/25 | Loss: 0.00135639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41590452
Iteration 2/25 | Loss: 0.00116016
Iteration 3/25 | Loss: 0.00116016
Iteration 4/25 | Loss: 0.00116016
Iteration 5/25 | Loss: 0.00116016
Iteration 6/25 | Loss: 0.00116016
Iteration 7/25 | Loss: 0.00116016
Iteration 8/25 | Loss: 0.00116016
Iteration 9/25 | Loss: 0.00116016
Iteration 10/25 | Loss: 0.00116016
Iteration 11/25 | Loss: 0.00116016
Iteration 12/25 | Loss: 0.00116016
Iteration 13/25 | Loss: 0.00116016
Iteration 14/25 | Loss: 0.00116016
Iteration 15/25 | Loss: 0.00116016
Iteration 16/25 | Loss: 0.00116016
Iteration 17/25 | Loss: 0.00116016
Iteration 18/25 | Loss: 0.00116016
Iteration 19/25 | Loss: 0.00116016
Iteration 20/25 | Loss: 0.00116016
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011601573787629604, 0.0011601573787629604, 0.0011601573787629604, 0.0011601573787629604, 0.0011601573787629604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011601573787629604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116016
Iteration 2/1000 | Loss: 0.00041167
Iteration 3/1000 | Loss: 0.00019384
Iteration 4/1000 | Loss: 0.00012739
Iteration 5/1000 | Loss: 0.00007234
Iteration 6/1000 | Loss: 0.00006557
Iteration 7/1000 | Loss: 0.00005648
Iteration 8/1000 | Loss: 0.00004954
Iteration 9/1000 | Loss: 0.00004572
Iteration 10/1000 | Loss: 0.00004381
Iteration 11/1000 | Loss: 0.00004169
Iteration 12/1000 | Loss: 0.00004055
Iteration 13/1000 | Loss: 0.00101590
Iteration 14/1000 | Loss: 0.00023029
Iteration 15/1000 | Loss: 0.00006666
Iteration 16/1000 | Loss: 0.00004293
Iteration 17/1000 | Loss: 0.00003657
Iteration 18/1000 | Loss: 0.00003285
Iteration 19/1000 | Loss: 0.00003077
Iteration 20/1000 | Loss: 0.00002930
Iteration 21/1000 | Loss: 0.00002836
Iteration 22/1000 | Loss: 0.00002776
Iteration 23/1000 | Loss: 0.00002724
Iteration 24/1000 | Loss: 0.00002694
Iteration 25/1000 | Loss: 0.00002668
Iteration 26/1000 | Loss: 0.00002666
Iteration 27/1000 | Loss: 0.00002642
Iteration 28/1000 | Loss: 0.00002617
Iteration 29/1000 | Loss: 0.00002600
Iteration 30/1000 | Loss: 0.00002576
Iteration 31/1000 | Loss: 0.00002559
Iteration 32/1000 | Loss: 0.00002550
Iteration 33/1000 | Loss: 0.00002543
Iteration 34/1000 | Loss: 0.00002543
Iteration 35/1000 | Loss: 0.00002543
Iteration 36/1000 | Loss: 0.00002543
Iteration 37/1000 | Loss: 0.00002543
Iteration 38/1000 | Loss: 0.00002543
Iteration 39/1000 | Loss: 0.00002542
Iteration 40/1000 | Loss: 0.00002542
Iteration 41/1000 | Loss: 0.00002542
Iteration 42/1000 | Loss: 0.00002542
Iteration 43/1000 | Loss: 0.00002542
Iteration 44/1000 | Loss: 0.00002541
Iteration 45/1000 | Loss: 0.00002538
Iteration 46/1000 | Loss: 0.00002538
Iteration 47/1000 | Loss: 0.00002538
Iteration 48/1000 | Loss: 0.00002537
Iteration 49/1000 | Loss: 0.00002537
Iteration 50/1000 | Loss: 0.00002537
Iteration 51/1000 | Loss: 0.00002537
Iteration 52/1000 | Loss: 0.00002537
Iteration 53/1000 | Loss: 0.00002537
Iteration 54/1000 | Loss: 0.00002535
Iteration 55/1000 | Loss: 0.00002535
Iteration 56/1000 | Loss: 0.00002535
Iteration 57/1000 | Loss: 0.00002535
Iteration 58/1000 | Loss: 0.00002535
Iteration 59/1000 | Loss: 0.00002535
Iteration 60/1000 | Loss: 0.00002535
Iteration 61/1000 | Loss: 0.00002534
Iteration 62/1000 | Loss: 0.00002534
Iteration 63/1000 | Loss: 0.00002533
Iteration 64/1000 | Loss: 0.00002533
Iteration 65/1000 | Loss: 0.00002533
Iteration 66/1000 | Loss: 0.00002533
Iteration 67/1000 | Loss: 0.00002533
Iteration 68/1000 | Loss: 0.00002532
Iteration 69/1000 | Loss: 0.00002532
Iteration 70/1000 | Loss: 0.00002532
Iteration 71/1000 | Loss: 0.00002532
Iteration 72/1000 | Loss: 0.00002532
Iteration 73/1000 | Loss: 0.00002532
Iteration 74/1000 | Loss: 0.00002531
Iteration 75/1000 | Loss: 0.00002531
Iteration 76/1000 | Loss: 0.00002531
Iteration 77/1000 | Loss: 0.00002531
Iteration 78/1000 | Loss: 0.00002531
Iteration 79/1000 | Loss: 0.00002531
Iteration 80/1000 | Loss: 0.00002531
Iteration 81/1000 | Loss: 0.00002531
Iteration 82/1000 | Loss: 0.00002531
Iteration 83/1000 | Loss: 0.00002531
Iteration 84/1000 | Loss: 0.00002530
Iteration 85/1000 | Loss: 0.00002530
Iteration 86/1000 | Loss: 0.00002530
Iteration 87/1000 | Loss: 0.00002530
Iteration 88/1000 | Loss: 0.00002530
Iteration 89/1000 | Loss: 0.00002530
Iteration 90/1000 | Loss: 0.00002530
Iteration 91/1000 | Loss: 0.00002530
Iteration 92/1000 | Loss: 0.00002530
Iteration 93/1000 | Loss: 0.00002530
Iteration 94/1000 | Loss: 0.00002529
Iteration 95/1000 | Loss: 0.00002529
Iteration 96/1000 | Loss: 0.00002529
Iteration 97/1000 | Loss: 0.00002529
Iteration 98/1000 | Loss: 0.00002529
Iteration 99/1000 | Loss: 0.00002529
Iteration 100/1000 | Loss: 0.00002529
Iteration 101/1000 | Loss: 0.00002529
Iteration 102/1000 | Loss: 0.00002529
Iteration 103/1000 | Loss: 0.00002529
Iteration 104/1000 | Loss: 0.00002529
Iteration 105/1000 | Loss: 0.00002529
Iteration 106/1000 | Loss: 0.00002528
Iteration 107/1000 | Loss: 0.00002528
Iteration 108/1000 | Loss: 0.00002528
Iteration 109/1000 | Loss: 0.00002528
Iteration 110/1000 | Loss: 0.00002528
Iteration 111/1000 | Loss: 0.00002528
Iteration 112/1000 | Loss: 0.00002528
Iteration 113/1000 | Loss: 0.00002527
Iteration 114/1000 | Loss: 0.00002527
Iteration 115/1000 | Loss: 0.00002527
Iteration 116/1000 | Loss: 0.00002527
Iteration 117/1000 | Loss: 0.00002527
Iteration 118/1000 | Loss: 0.00002527
Iteration 119/1000 | Loss: 0.00002527
Iteration 120/1000 | Loss: 0.00002527
Iteration 121/1000 | Loss: 0.00002527
Iteration 122/1000 | Loss: 0.00002527
Iteration 123/1000 | Loss: 0.00002527
Iteration 124/1000 | Loss: 0.00002526
Iteration 125/1000 | Loss: 0.00002526
Iteration 126/1000 | Loss: 0.00002526
Iteration 127/1000 | Loss: 0.00002526
Iteration 128/1000 | Loss: 0.00002526
Iteration 129/1000 | Loss: 0.00002526
Iteration 130/1000 | Loss: 0.00002526
Iteration 131/1000 | Loss: 0.00002526
Iteration 132/1000 | Loss: 0.00002526
Iteration 133/1000 | Loss: 0.00002526
Iteration 134/1000 | Loss: 0.00002526
Iteration 135/1000 | Loss: 0.00002526
Iteration 136/1000 | Loss: 0.00002526
Iteration 137/1000 | Loss: 0.00002526
Iteration 138/1000 | Loss: 0.00002526
Iteration 139/1000 | Loss: 0.00002526
Iteration 140/1000 | Loss: 0.00002526
Iteration 141/1000 | Loss: 0.00002526
Iteration 142/1000 | Loss: 0.00002526
Iteration 143/1000 | Loss: 0.00002526
Iteration 144/1000 | Loss: 0.00002526
Iteration 145/1000 | Loss: 0.00002525
Iteration 146/1000 | Loss: 0.00002525
Iteration 147/1000 | Loss: 0.00002525
Iteration 148/1000 | Loss: 0.00002525
Iteration 149/1000 | Loss: 0.00002525
Iteration 150/1000 | Loss: 0.00002525
Iteration 151/1000 | Loss: 0.00002525
Iteration 152/1000 | Loss: 0.00002525
Iteration 153/1000 | Loss: 0.00002525
Iteration 154/1000 | Loss: 0.00002525
Iteration 155/1000 | Loss: 0.00002525
Iteration 156/1000 | Loss: 0.00002525
Iteration 157/1000 | Loss: 0.00002525
Iteration 158/1000 | Loss: 0.00002525
Iteration 159/1000 | Loss: 0.00002525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.5254132197005674e-05, 2.5254132197005674e-05, 2.5254132197005674e-05, 2.5254132197005674e-05, 2.5254132197005674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5254132197005674e-05

Optimization complete. Final v2v error: 3.782731056213379 mm

Highest mean error: 17.628223419189453 mm for frame 3

Lowest mean error: 3.602898597717285 mm for frame 98

Saving results

Total time: 85.55932974815369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883569
Iteration 2/25 | Loss: 0.00183897
Iteration 3/25 | Loss: 0.00155072
Iteration 4/25 | Loss: 0.00152570
Iteration 5/25 | Loss: 0.00152218
Iteration 6/25 | Loss: 0.00152218
Iteration 7/25 | Loss: 0.00152218
Iteration 8/25 | Loss: 0.00152218
Iteration 9/25 | Loss: 0.00152218
Iteration 10/25 | Loss: 0.00152218
Iteration 11/25 | Loss: 0.00152218
Iteration 12/25 | Loss: 0.00152218
Iteration 13/25 | Loss: 0.00152218
Iteration 14/25 | Loss: 0.00152218
Iteration 15/25 | Loss: 0.00152218
Iteration 16/25 | Loss: 0.00152218
Iteration 17/25 | Loss: 0.00152218
Iteration 18/25 | Loss: 0.00152218
Iteration 19/25 | Loss: 0.00152218
Iteration 20/25 | Loss: 0.00152218
Iteration 21/25 | Loss: 0.00152218
Iteration 22/25 | Loss: 0.00152218
Iteration 23/25 | Loss: 0.00152218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015221777139231563, 0.0015221777139231563, 0.0015221777139231563, 0.0015221777139231563, 0.0015221777139231563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015221777139231563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60184699
Iteration 2/25 | Loss: 0.00117188
Iteration 3/25 | Loss: 0.00117188
Iteration 4/25 | Loss: 0.00117188
Iteration 5/25 | Loss: 0.00117188
Iteration 6/25 | Loss: 0.00117188
Iteration 7/25 | Loss: 0.00117188
Iteration 8/25 | Loss: 0.00117188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.001171877491287887, 0.001171877491287887, 0.001171877491287887, 0.001171877491287887, 0.001171877491287887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001171877491287887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117188
Iteration 2/1000 | Loss: 0.00005662
Iteration 3/1000 | Loss: 0.00003924
Iteration 4/1000 | Loss: 0.00003588
Iteration 5/1000 | Loss: 0.00003428
Iteration 6/1000 | Loss: 0.00003330
Iteration 7/1000 | Loss: 0.00003262
Iteration 8/1000 | Loss: 0.00003207
Iteration 9/1000 | Loss: 0.00003164
Iteration 10/1000 | Loss: 0.00003138
Iteration 11/1000 | Loss: 0.00003116
Iteration 12/1000 | Loss: 0.00003110
Iteration 13/1000 | Loss: 0.00003096
Iteration 14/1000 | Loss: 0.00003095
Iteration 15/1000 | Loss: 0.00003095
Iteration 16/1000 | Loss: 0.00003094
Iteration 17/1000 | Loss: 0.00003094
Iteration 18/1000 | Loss: 0.00003087
Iteration 19/1000 | Loss: 0.00003086
Iteration 20/1000 | Loss: 0.00003086
Iteration 21/1000 | Loss: 0.00003086
Iteration 22/1000 | Loss: 0.00003085
Iteration 23/1000 | Loss: 0.00003085
Iteration 24/1000 | Loss: 0.00003085
Iteration 25/1000 | Loss: 0.00003084
Iteration 26/1000 | Loss: 0.00003084
Iteration 27/1000 | Loss: 0.00003084
Iteration 28/1000 | Loss: 0.00003084
Iteration 29/1000 | Loss: 0.00003084
Iteration 30/1000 | Loss: 0.00003083
Iteration 31/1000 | Loss: 0.00003083
Iteration 32/1000 | Loss: 0.00003083
Iteration 33/1000 | Loss: 0.00003082
Iteration 34/1000 | Loss: 0.00003082
Iteration 35/1000 | Loss: 0.00003082
Iteration 36/1000 | Loss: 0.00003082
Iteration 37/1000 | Loss: 0.00003082
Iteration 38/1000 | Loss: 0.00003082
Iteration 39/1000 | Loss: 0.00003081
Iteration 40/1000 | Loss: 0.00003081
Iteration 41/1000 | Loss: 0.00003081
Iteration 42/1000 | Loss: 0.00003081
Iteration 43/1000 | Loss: 0.00003081
Iteration 44/1000 | Loss: 0.00003081
Iteration 45/1000 | Loss: 0.00003081
Iteration 46/1000 | Loss: 0.00003081
Iteration 47/1000 | Loss: 0.00003081
Iteration 48/1000 | Loss: 0.00003081
Iteration 49/1000 | Loss: 0.00003081
Iteration 50/1000 | Loss: 0.00003081
Iteration 51/1000 | Loss: 0.00003080
Iteration 52/1000 | Loss: 0.00003080
Iteration 53/1000 | Loss: 0.00003080
Iteration 54/1000 | Loss: 0.00003080
Iteration 55/1000 | Loss: 0.00003080
Iteration 56/1000 | Loss: 0.00003080
Iteration 57/1000 | Loss: 0.00003080
Iteration 58/1000 | Loss: 0.00003080
Iteration 59/1000 | Loss: 0.00003080
Iteration 60/1000 | Loss: 0.00003080
Iteration 61/1000 | Loss: 0.00003080
Iteration 62/1000 | Loss: 0.00003080
Iteration 63/1000 | Loss: 0.00003079
Iteration 64/1000 | Loss: 0.00003079
Iteration 65/1000 | Loss: 0.00003079
Iteration 66/1000 | Loss: 0.00003079
Iteration 67/1000 | Loss: 0.00003079
Iteration 68/1000 | Loss: 0.00003079
Iteration 69/1000 | Loss: 0.00003079
Iteration 70/1000 | Loss: 0.00003079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [3.0791856261203066e-05, 3.0791856261203066e-05, 3.0791856261203066e-05, 3.0791856261203066e-05, 3.0791856261203066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0791856261203066e-05

Optimization complete. Final v2v error: 4.647012233734131 mm

Highest mean error: 4.940349578857422 mm for frame 98

Lowest mean error: 4.460300922393799 mm for frame 80

Saving results

Total time: 29.923279523849487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00690168
Iteration 2/25 | Loss: 0.00171213
Iteration 3/25 | Loss: 0.00139567
Iteration 4/25 | Loss: 0.00135265
Iteration 5/25 | Loss: 0.00133699
Iteration 6/25 | Loss: 0.00132483
Iteration 7/25 | Loss: 0.00131698
Iteration 8/25 | Loss: 0.00131092
Iteration 9/25 | Loss: 0.00130571
Iteration 10/25 | Loss: 0.00130532
Iteration 11/25 | Loss: 0.00130528
Iteration 12/25 | Loss: 0.00130528
Iteration 13/25 | Loss: 0.00130528
Iteration 14/25 | Loss: 0.00130528
Iteration 15/25 | Loss: 0.00130528
Iteration 16/25 | Loss: 0.00130528
Iteration 17/25 | Loss: 0.00130528
Iteration 18/25 | Loss: 0.00130527
Iteration 19/25 | Loss: 0.00130527
Iteration 20/25 | Loss: 0.00130527
Iteration 21/25 | Loss: 0.00130527
Iteration 22/25 | Loss: 0.00130527
Iteration 23/25 | Loss: 0.00130527
Iteration 24/25 | Loss: 0.00130527
Iteration 25/25 | Loss: 0.00130527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11489367
Iteration 2/25 | Loss: 0.00095838
Iteration 3/25 | Loss: 0.00095807
Iteration 4/25 | Loss: 0.00095807
Iteration 5/25 | Loss: 0.00095807
Iteration 6/25 | Loss: 0.00095807
Iteration 7/25 | Loss: 0.00095807
Iteration 8/25 | Loss: 0.00095807
Iteration 9/25 | Loss: 0.00095807
Iteration 10/25 | Loss: 0.00095807
Iteration 11/25 | Loss: 0.00095807
Iteration 12/25 | Loss: 0.00095807
Iteration 13/25 | Loss: 0.00095807
Iteration 14/25 | Loss: 0.00095807
Iteration 15/25 | Loss: 0.00095807
Iteration 16/25 | Loss: 0.00095807
Iteration 17/25 | Loss: 0.00095807
Iteration 18/25 | Loss: 0.00095807
Iteration 19/25 | Loss: 0.00095807
Iteration 20/25 | Loss: 0.00095807
Iteration 21/25 | Loss: 0.00095807
Iteration 22/25 | Loss: 0.00095807
Iteration 23/25 | Loss: 0.00095807
Iteration 24/25 | Loss: 0.00095807
Iteration 25/25 | Loss: 0.00095807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095807
Iteration 2/1000 | Loss: 0.00006633
Iteration 3/1000 | Loss: 0.00004470
Iteration 4/1000 | Loss: 0.00003548
Iteration 5/1000 | Loss: 0.00003206
Iteration 6/1000 | Loss: 0.00002981
Iteration 7/1000 | Loss: 0.00002849
Iteration 8/1000 | Loss: 0.00002757
Iteration 9/1000 | Loss: 0.00002673
Iteration 10/1000 | Loss: 0.00002622
Iteration 11/1000 | Loss: 0.00002578
Iteration 12/1000 | Loss: 0.00002545
Iteration 13/1000 | Loss: 0.00002514
Iteration 14/1000 | Loss: 0.00002494
Iteration 15/1000 | Loss: 0.00002485
Iteration 16/1000 | Loss: 0.00002469
Iteration 17/1000 | Loss: 0.00002462
Iteration 18/1000 | Loss: 0.00002456
Iteration 19/1000 | Loss: 0.00002452
Iteration 20/1000 | Loss: 0.00002451
Iteration 21/1000 | Loss: 0.00002450
Iteration 22/1000 | Loss: 0.00002446
Iteration 23/1000 | Loss: 0.00002443
Iteration 24/1000 | Loss: 0.00002442
Iteration 25/1000 | Loss: 0.00002441
Iteration 26/1000 | Loss: 0.00002438
Iteration 27/1000 | Loss: 0.00002435
Iteration 28/1000 | Loss: 0.00002434
Iteration 29/1000 | Loss: 0.00002433
Iteration 30/1000 | Loss: 0.00002432
Iteration 31/1000 | Loss: 0.00002431
Iteration 32/1000 | Loss: 0.00002431
Iteration 33/1000 | Loss: 0.00002430
Iteration 34/1000 | Loss: 0.00002430
Iteration 35/1000 | Loss: 0.00002429
Iteration 36/1000 | Loss: 0.00002428
Iteration 37/1000 | Loss: 0.00002428
Iteration 38/1000 | Loss: 0.00002425
Iteration 39/1000 | Loss: 0.00002425
Iteration 40/1000 | Loss: 0.00002422
Iteration 41/1000 | Loss: 0.00002422
Iteration 42/1000 | Loss: 0.00002420
Iteration 43/1000 | Loss: 0.00002419
Iteration 44/1000 | Loss: 0.00002419
Iteration 45/1000 | Loss: 0.00002418
Iteration 46/1000 | Loss: 0.00002418
Iteration 47/1000 | Loss: 0.00002418
Iteration 48/1000 | Loss: 0.00002417
Iteration 49/1000 | Loss: 0.00002417
Iteration 50/1000 | Loss: 0.00002417
Iteration 51/1000 | Loss: 0.00002416
Iteration 52/1000 | Loss: 0.00002416
Iteration 53/1000 | Loss: 0.00002415
Iteration 54/1000 | Loss: 0.00002414
Iteration 55/1000 | Loss: 0.00002413
Iteration 56/1000 | Loss: 0.00002413
Iteration 57/1000 | Loss: 0.00002412
Iteration 58/1000 | Loss: 0.00002411
Iteration 59/1000 | Loss: 0.00002410
Iteration 60/1000 | Loss: 0.00002409
Iteration 61/1000 | Loss: 0.00002408
Iteration 62/1000 | Loss: 0.00002408
Iteration 63/1000 | Loss: 0.00002408
Iteration 64/1000 | Loss: 0.00002407
Iteration 65/1000 | Loss: 0.00002407
Iteration 66/1000 | Loss: 0.00002406
Iteration 67/1000 | Loss: 0.00002406
Iteration 68/1000 | Loss: 0.00002406
Iteration 69/1000 | Loss: 0.00002406
Iteration 70/1000 | Loss: 0.00002406
Iteration 71/1000 | Loss: 0.00002406
Iteration 72/1000 | Loss: 0.00002406
Iteration 73/1000 | Loss: 0.00002406
Iteration 74/1000 | Loss: 0.00002406
Iteration 75/1000 | Loss: 0.00002405
Iteration 76/1000 | Loss: 0.00002405
Iteration 77/1000 | Loss: 0.00002405
Iteration 78/1000 | Loss: 0.00002405
Iteration 79/1000 | Loss: 0.00002405
Iteration 80/1000 | Loss: 0.00002405
Iteration 81/1000 | Loss: 0.00002405
Iteration 82/1000 | Loss: 0.00002405
Iteration 83/1000 | Loss: 0.00002405
Iteration 84/1000 | Loss: 0.00002404
Iteration 85/1000 | Loss: 0.00002404
Iteration 86/1000 | Loss: 0.00002404
Iteration 87/1000 | Loss: 0.00002403
Iteration 88/1000 | Loss: 0.00002403
Iteration 89/1000 | Loss: 0.00002403
Iteration 90/1000 | Loss: 0.00002402
Iteration 91/1000 | Loss: 0.00002402
Iteration 92/1000 | Loss: 0.00002402
Iteration 93/1000 | Loss: 0.00002401
Iteration 94/1000 | Loss: 0.00002401
Iteration 95/1000 | Loss: 0.00002401
Iteration 96/1000 | Loss: 0.00002401
Iteration 97/1000 | Loss: 0.00002400
Iteration 98/1000 | Loss: 0.00002400
Iteration 99/1000 | Loss: 0.00002400
Iteration 100/1000 | Loss: 0.00002400
Iteration 101/1000 | Loss: 0.00002400
Iteration 102/1000 | Loss: 0.00002399
Iteration 103/1000 | Loss: 0.00002399
Iteration 104/1000 | Loss: 0.00002399
Iteration 105/1000 | Loss: 0.00002398
Iteration 106/1000 | Loss: 0.00002398
Iteration 107/1000 | Loss: 0.00002398
Iteration 108/1000 | Loss: 0.00002397
Iteration 109/1000 | Loss: 0.00002397
Iteration 110/1000 | Loss: 0.00002397
Iteration 111/1000 | Loss: 0.00002396
Iteration 112/1000 | Loss: 0.00002396
Iteration 113/1000 | Loss: 0.00002396
Iteration 114/1000 | Loss: 0.00002396
Iteration 115/1000 | Loss: 0.00002395
Iteration 116/1000 | Loss: 0.00002394
Iteration 117/1000 | Loss: 0.00002394
Iteration 118/1000 | Loss: 0.00002394
Iteration 119/1000 | Loss: 0.00002393
Iteration 120/1000 | Loss: 0.00002393
Iteration 121/1000 | Loss: 0.00002393
Iteration 122/1000 | Loss: 0.00002393
Iteration 123/1000 | Loss: 0.00002392
Iteration 124/1000 | Loss: 0.00002392
Iteration 125/1000 | Loss: 0.00002392
Iteration 126/1000 | Loss: 0.00002392
Iteration 127/1000 | Loss: 0.00002392
Iteration 128/1000 | Loss: 0.00002392
Iteration 129/1000 | Loss: 0.00002392
Iteration 130/1000 | Loss: 0.00002392
Iteration 131/1000 | Loss: 0.00002392
Iteration 132/1000 | Loss: 0.00002392
Iteration 133/1000 | Loss: 0.00002392
Iteration 134/1000 | Loss: 0.00002392
Iteration 135/1000 | Loss: 0.00002392
Iteration 136/1000 | Loss: 0.00002391
Iteration 137/1000 | Loss: 0.00002391
Iteration 138/1000 | Loss: 0.00002391
Iteration 139/1000 | Loss: 0.00002391
Iteration 140/1000 | Loss: 0.00002391
Iteration 141/1000 | Loss: 0.00002390
Iteration 142/1000 | Loss: 0.00002390
Iteration 143/1000 | Loss: 0.00002390
Iteration 144/1000 | Loss: 0.00002390
Iteration 145/1000 | Loss: 0.00002389
Iteration 146/1000 | Loss: 0.00002389
Iteration 147/1000 | Loss: 0.00002389
Iteration 148/1000 | Loss: 0.00002389
Iteration 149/1000 | Loss: 0.00002389
Iteration 150/1000 | Loss: 0.00002389
Iteration 151/1000 | Loss: 0.00002389
Iteration 152/1000 | Loss: 0.00002389
Iteration 153/1000 | Loss: 0.00002389
Iteration 154/1000 | Loss: 0.00002389
Iteration 155/1000 | Loss: 0.00002389
Iteration 156/1000 | Loss: 0.00002389
Iteration 157/1000 | Loss: 0.00002389
Iteration 158/1000 | Loss: 0.00002389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.3890328520792536e-05, 2.3890328520792536e-05, 2.3890328520792536e-05, 2.3890328520792536e-05, 2.3890328520792536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3890328520792536e-05

Optimization complete. Final v2v error: 3.9638681411743164 mm

Highest mean error: 6.191696643829346 mm for frame 132

Lowest mean error: 3.0708117485046387 mm for frame 205

Saving results

Total time: 62.63515019416809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901368
Iteration 2/25 | Loss: 0.00260939
Iteration 3/25 | Loss: 0.00185780
Iteration 4/25 | Loss: 0.00176849
Iteration 5/25 | Loss: 0.00171023
Iteration 6/25 | Loss: 0.00167250
Iteration 7/25 | Loss: 0.00165630
Iteration 8/25 | Loss: 0.00164135
Iteration 9/25 | Loss: 0.00163865
Iteration 10/25 | Loss: 0.00163733
Iteration 11/25 | Loss: 0.00163660
Iteration 12/25 | Loss: 0.00163566
Iteration 13/25 | Loss: 0.00163468
Iteration 14/25 | Loss: 0.00163376
Iteration 15/25 | Loss: 0.00163286
Iteration 16/25 | Loss: 0.00163191
Iteration 17/25 | Loss: 0.00163117
Iteration 18/25 | Loss: 0.00163061
Iteration 19/25 | Loss: 0.00163027
Iteration 20/25 | Loss: 0.00163005
Iteration 21/25 | Loss: 0.00162989
Iteration 22/25 | Loss: 0.00162976
Iteration 23/25 | Loss: 0.00162967
Iteration 24/25 | Loss: 0.00162965
Iteration 25/25 | Loss: 0.00162965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45767117
Iteration 2/25 | Loss: 0.00442433
Iteration 3/25 | Loss: 0.00420744
Iteration 4/25 | Loss: 0.00411730
Iteration 5/25 | Loss: 0.00411730
Iteration 6/25 | Loss: 0.00411730
Iteration 7/25 | Loss: 0.00411730
Iteration 8/25 | Loss: 0.00411730
Iteration 9/25 | Loss: 0.00411730
Iteration 10/25 | Loss: 0.00411730
Iteration 11/25 | Loss: 0.00411730
Iteration 12/25 | Loss: 0.00411730
Iteration 13/25 | Loss: 0.00411730
Iteration 14/25 | Loss: 0.00411730
Iteration 15/25 | Loss: 0.00411730
Iteration 16/25 | Loss: 0.00411730
Iteration 17/25 | Loss: 0.00411730
Iteration 18/25 | Loss: 0.00411730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004117295611649752, 0.004117295611649752, 0.004117295611649752, 0.004117295611649752, 0.004117295611649752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004117295611649752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00411730
Iteration 2/1000 | Loss: 0.01319491
Iteration 3/1000 | Loss: 0.00201677
Iteration 4/1000 | Loss: 0.00063221
Iteration 5/1000 | Loss: 0.00205109
Iteration 6/1000 | Loss: 0.00024089
Iteration 7/1000 | Loss: 0.00029736
Iteration 8/1000 | Loss: 0.00015966
Iteration 9/1000 | Loss: 0.00030360
Iteration 10/1000 | Loss: 0.00041899
Iteration 11/1000 | Loss: 0.00019000
Iteration 12/1000 | Loss: 0.00015629
Iteration 13/1000 | Loss: 0.00017275
Iteration 14/1000 | Loss: 0.00030351
Iteration 15/1000 | Loss: 0.00010804
Iteration 16/1000 | Loss: 0.00031415
Iteration 17/1000 | Loss: 0.00016002
Iteration 18/1000 | Loss: 0.00018264
Iteration 19/1000 | Loss: 0.00029970
Iteration 20/1000 | Loss: 0.00008661
Iteration 21/1000 | Loss: 0.00007334
Iteration 22/1000 | Loss: 0.00006712
Iteration 23/1000 | Loss: 0.00015935
Iteration 24/1000 | Loss: 0.00006496
Iteration 25/1000 | Loss: 0.00006041
Iteration 26/1000 | Loss: 0.00005806
Iteration 27/1000 | Loss: 0.00005591
Iteration 28/1000 | Loss: 0.00005311
Iteration 29/1000 | Loss: 0.00005106
Iteration 30/1000 | Loss: 0.00082097
Iteration 31/1000 | Loss: 0.00367399
Iteration 32/1000 | Loss: 0.00300435
Iteration 33/1000 | Loss: 0.00136472
Iteration 34/1000 | Loss: 0.00037451
Iteration 35/1000 | Loss: 0.00008848
Iteration 36/1000 | Loss: 0.00006254
Iteration 37/1000 | Loss: 0.00005488
Iteration 38/1000 | Loss: 0.00305592
Iteration 39/1000 | Loss: 0.00117260
Iteration 40/1000 | Loss: 0.00030547
Iteration 41/1000 | Loss: 0.00015948
Iteration 42/1000 | Loss: 0.00013974
Iteration 43/1000 | Loss: 0.00017849
Iteration 44/1000 | Loss: 0.00010929
Iteration 45/1000 | Loss: 0.00019984
Iteration 46/1000 | Loss: 0.00030038
Iteration 47/1000 | Loss: 0.00006549
Iteration 48/1000 | Loss: 0.00006039
Iteration 49/1000 | Loss: 0.00035951
Iteration 50/1000 | Loss: 0.00011110
Iteration 51/1000 | Loss: 0.00022337
Iteration 52/1000 | Loss: 0.00015948
Iteration 53/1000 | Loss: 0.00015890
Iteration 54/1000 | Loss: 0.00006216
Iteration 55/1000 | Loss: 0.00007214
Iteration 56/1000 | Loss: 0.00005066
Iteration 57/1000 | Loss: 0.00004955
Iteration 58/1000 | Loss: 0.00004846
Iteration 59/1000 | Loss: 0.00004910
Iteration 60/1000 | Loss: 0.00004647
Iteration 61/1000 | Loss: 0.00004586
Iteration 62/1000 | Loss: 0.00004513
Iteration 63/1000 | Loss: 0.00016166
Iteration 64/1000 | Loss: 0.00004463
Iteration 65/1000 | Loss: 0.00004409
Iteration 66/1000 | Loss: 0.00004406
Iteration 67/1000 | Loss: 0.00004405
Iteration 68/1000 | Loss: 0.00004401
Iteration 69/1000 | Loss: 0.00004381
Iteration 70/1000 | Loss: 0.00004378
Iteration 71/1000 | Loss: 0.00004373
Iteration 72/1000 | Loss: 0.00004367
Iteration 73/1000 | Loss: 0.00004358
Iteration 74/1000 | Loss: 0.00004356
Iteration 75/1000 | Loss: 0.00004356
Iteration 76/1000 | Loss: 0.00004355
Iteration 77/1000 | Loss: 0.00004355
Iteration 78/1000 | Loss: 0.00004354
Iteration 79/1000 | Loss: 0.00004354
Iteration 80/1000 | Loss: 0.00004354
Iteration 81/1000 | Loss: 0.00004353
Iteration 82/1000 | Loss: 0.00004353
Iteration 83/1000 | Loss: 0.00004352
Iteration 84/1000 | Loss: 0.00064791
Iteration 85/1000 | Loss: 0.00526539
Iteration 86/1000 | Loss: 0.00330452
Iteration 87/1000 | Loss: 0.00414838
Iteration 88/1000 | Loss: 0.00303744
Iteration 89/1000 | Loss: 0.00183903
Iteration 90/1000 | Loss: 0.00076567
Iteration 91/1000 | Loss: 0.00111278
Iteration 92/1000 | Loss: 0.00273695
Iteration 93/1000 | Loss: 0.00183752
Iteration 94/1000 | Loss: 0.00173385
Iteration 95/1000 | Loss: 0.00132043
Iteration 96/1000 | Loss: 0.00051056
Iteration 97/1000 | Loss: 0.00112191
Iteration 98/1000 | Loss: 0.00072407
Iteration 99/1000 | Loss: 0.00028260
Iteration 100/1000 | Loss: 0.00043648
Iteration 101/1000 | Loss: 0.00083548
Iteration 102/1000 | Loss: 0.00022520
Iteration 103/1000 | Loss: 0.00019476
Iteration 104/1000 | Loss: 0.00007876
Iteration 105/1000 | Loss: 0.00006270
Iteration 106/1000 | Loss: 0.00008605
Iteration 107/1000 | Loss: 0.00005655
Iteration 108/1000 | Loss: 0.00050383
Iteration 109/1000 | Loss: 0.00029001
Iteration 110/1000 | Loss: 0.00030977
Iteration 111/1000 | Loss: 0.00005256
Iteration 112/1000 | Loss: 0.00006589
Iteration 113/1000 | Loss: 0.00033538
Iteration 114/1000 | Loss: 0.00096328
Iteration 115/1000 | Loss: 0.00114868
Iteration 116/1000 | Loss: 0.00080497
Iteration 117/1000 | Loss: 0.00018682
Iteration 118/1000 | Loss: 0.00042988
Iteration 119/1000 | Loss: 0.00005315
Iteration 120/1000 | Loss: 0.00004687
Iteration 121/1000 | Loss: 0.00009764
Iteration 122/1000 | Loss: 0.00034032
Iteration 123/1000 | Loss: 0.00011272
Iteration 124/1000 | Loss: 0.00004255
Iteration 125/1000 | Loss: 0.00053441
Iteration 126/1000 | Loss: 0.00057009
Iteration 127/1000 | Loss: 0.00023956
Iteration 128/1000 | Loss: 0.00061921
Iteration 129/1000 | Loss: 0.00064675
Iteration 130/1000 | Loss: 0.00051069
Iteration 131/1000 | Loss: 0.00062711
Iteration 132/1000 | Loss: 0.00096756
Iteration 133/1000 | Loss: 0.00036948
Iteration 134/1000 | Loss: 0.00004304
Iteration 135/1000 | Loss: 0.00003734
Iteration 136/1000 | Loss: 0.00007467
Iteration 137/1000 | Loss: 0.00003186
Iteration 138/1000 | Loss: 0.00009418
Iteration 139/1000 | Loss: 0.00002964
Iteration 140/1000 | Loss: 0.00012727
Iteration 141/1000 | Loss: 0.00017300
Iteration 142/1000 | Loss: 0.00017886
Iteration 143/1000 | Loss: 0.00013638
Iteration 144/1000 | Loss: 0.00025391
Iteration 145/1000 | Loss: 0.00002798
Iteration 146/1000 | Loss: 0.00010261
Iteration 147/1000 | Loss: 0.00011796
Iteration 148/1000 | Loss: 0.00011429
Iteration 149/1000 | Loss: 0.00002885
Iteration 150/1000 | Loss: 0.00002684
Iteration 151/1000 | Loss: 0.00002632
Iteration 152/1000 | Loss: 0.00002604
Iteration 153/1000 | Loss: 0.00002576
Iteration 154/1000 | Loss: 0.00016555
Iteration 155/1000 | Loss: 0.00002968
Iteration 156/1000 | Loss: 0.00002744
Iteration 157/1000 | Loss: 0.00002572
Iteration 158/1000 | Loss: 0.00002558
Iteration 159/1000 | Loss: 0.00002554
Iteration 160/1000 | Loss: 0.00002549
Iteration 161/1000 | Loss: 0.00002549
Iteration 162/1000 | Loss: 0.00002548
Iteration 163/1000 | Loss: 0.00014809
Iteration 164/1000 | Loss: 0.00006945
Iteration 165/1000 | Loss: 0.00002552
Iteration 166/1000 | Loss: 0.00002536
Iteration 167/1000 | Loss: 0.00014916
Iteration 168/1000 | Loss: 0.00006017
Iteration 169/1000 | Loss: 0.00002570
Iteration 170/1000 | Loss: 0.00005328
Iteration 171/1000 | Loss: 0.00004197
Iteration 172/1000 | Loss: 0.00040133
Iteration 173/1000 | Loss: 0.00017690
Iteration 174/1000 | Loss: 0.00002545
Iteration 175/1000 | Loss: 0.00002537
Iteration 176/1000 | Loss: 0.00002536
Iteration 177/1000 | Loss: 0.00002535
Iteration 178/1000 | Loss: 0.00002534
Iteration 179/1000 | Loss: 0.00002532
Iteration 180/1000 | Loss: 0.00029953
Iteration 181/1000 | Loss: 0.00004666
Iteration 182/1000 | Loss: 0.00002893
Iteration 183/1000 | Loss: 0.00002661
Iteration 184/1000 | Loss: 0.00019719
Iteration 185/1000 | Loss: 0.00002747
Iteration 186/1000 | Loss: 0.00002564
Iteration 187/1000 | Loss: 0.00002488
Iteration 188/1000 | Loss: 0.00015375
Iteration 189/1000 | Loss: 0.00002952
Iteration 190/1000 | Loss: 0.00002644
Iteration 191/1000 | Loss: 0.00002431
Iteration 192/1000 | Loss: 0.00002401
Iteration 193/1000 | Loss: 0.00002399
Iteration 194/1000 | Loss: 0.00016170
Iteration 195/1000 | Loss: 0.00008896
Iteration 196/1000 | Loss: 0.00002436
Iteration 197/1000 | Loss: 0.00002385
Iteration 198/1000 | Loss: 0.00013959
Iteration 199/1000 | Loss: 0.00004331
Iteration 200/1000 | Loss: 0.00014812
Iteration 201/1000 | Loss: 0.00002806
Iteration 202/1000 | Loss: 0.00002542
Iteration 203/1000 | Loss: 0.00002395
Iteration 204/1000 | Loss: 0.00002375
Iteration 205/1000 | Loss: 0.00002375
Iteration 206/1000 | Loss: 0.00002375
Iteration 207/1000 | Loss: 0.00002374
Iteration 208/1000 | Loss: 0.00002374
Iteration 209/1000 | Loss: 0.00002374
Iteration 210/1000 | Loss: 0.00002374
Iteration 211/1000 | Loss: 0.00002374
Iteration 212/1000 | Loss: 0.00002374
Iteration 213/1000 | Loss: 0.00002374
Iteration 214/1000 | Loss: 0.00002374
Iteration 215/1000 | Loss: 0.00002374
Iteration 216/1000 | Loss: 0.00002374
Iteration 217/1000 | Loss: 0.00002374
Iteration 218/1000 | Loss: 0.00002374
Iteration 219/1000 | Loss: 0.00002373
Iteration 220/1000 | Loss: 0.00002373
Iteration 221/1000 | Loss: 0.00002373
Iteration 222/1000 | Loss: 0.00002373
Iteration 223/1000 | Loss: 0.00002373
Iteration 224/1000 | Loss: 0.00002372
Iteration 225/1000 | Loss: 0.00002372
Iteration 226/1000 | Loss: 0.00002372
Iteration 227/1000 | Loss: 0.00002372
Iteration 228/1000 | Loss: 0.00002372
Iteration 229/1000 | Loss: 0.00002372
Iteration 230/1000 | Loss: 0.00002372
Iteration 231/1000 | Loss: 0.00002371
Iteration 232/1000 | Loss: 0.00002371
Iteration 233/1000 | Loss: 0.00002371
Iteration 234/1000 | Loss: 0.00002371
Iteration 235/1000 | Loss: 0.00002371
Iteration 236/1000 | Loss: 0.00002371
Iteration 237/1000 | Loss: 0.00002371
Iteration 238/1000 | Loss: 0.00002371
Iteration 239/1000 | Loss: 0.00002371
Iteration 240/1000 | Loss: 0.00002371
Iteration 241/1000 | Loss: 0.00002371
Iteration 242/1000 | Loss: 0.00002371
Iteration 243/1000 | Loss: 0.00002371
Iteration 244/1000 | Loss: 0.00002371
Iteration 245/1000 | Loss: 0.00002371
Iteration 246/1000 | Loss: 0.00002371
Iteration 247/1000 | Loss: 0.00002371
Iteration 248/1000 | Loss: 0.00002371
Iteration 249/1000 | Loss: 0.00002371
Iteration 250/1000 | Loss: 0.00002371
Iteration 251/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [2.3709621018497273e-05, 2.3709621018497273e-05, 2.3709621018497273e-05, 2.3709621018497273e-05, 2.3709621018497273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3709621018497273e-05

Optimization complete. Final v2v error: 3.698212146759033 mm

Highest mean error: 11.897941589355469 mm for frame 90

Lowest mean error: 3.2653791904449463 mm for frame 3

Saving results

Total time: 328.5091218948364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794662
Iteration 2/25 | Loss: 0.00135647
Iteration 3/25 | Loss: 0.00126771
Iteration 4/25 | Loss: 0.00126132
Iteration 5/25 | Loss: 0.00125987
Iteration 6/25 | Loss: 0.00125987
Iteration 7/25 | Loss: 0.00125987
Iteration 8/25 | Loss: 0.00125987
Iteration 9/25 | Loss: 0.00125987
Iteration 10/25 | Loss: 0.00125987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012598669854924083, 0.0012598669854924083, 0.0012598669854924083, 0.0012598669854924083, 0.0012598669854924083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012598669854924083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45786726
Iteration 2/25 | Loss: 0.00083340
Iteration 3/25 | Loss: 0.00083339
Iteration 4/25 | Loss: 0.00083339
Iteration 5/25 | Loss: 0.00083339
Iteration 6/25 | Loss: 0.00083339
Iteration 7/25 | Loss: 0.00083339
Iteration 8/25 | Loss: 0.00083339
Iteration 9/25 | Loss: 0.00083339
Iteration 10/25 | Loss: 0.00083339
Iteration 11/25 | Loss: 0.00083339
Iteration 12/25 | Loss: 0.00083339
Iteration 13/25 | Loss: 0.00083339
Iteration 14/25 | Loss: 0.00083339
Iteration 15/25 | Loss: 0.00083339
Iteration 16/25 | Loss: 0.00083339
Iteration 17/25 | Loss: 0.00083339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008333891746588051, 0.0008333891746588051, 0.0008333891746588051, 0.0008333891746588051, 0.0008333891746588051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008333891746588051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083339
Iteration 2/1000 | Loss: 0.00003475
Iteration 3/1000 | Loss: 0.00002385
Iteration 4/1000 | Loss: 0.00001922
Iteration 5/1000 | Loss: 0.00001756
Iteration 6/1000 | Loss: 0.00001631
Iteration 7/1000 | Loss: 0.00001561
Iteration 8/1000 | Loss: 0.00001514
Iteration 9/1000 | Loss: 0.00001486
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001442
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001426
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001424
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001418
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001415
Iteration 22/1000 | Loss: 0.00001412
Iteration 23/1000 | Loss: 0.00001411
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001407
Iteration 26/1000 | Loss: 0.00001407
Iteration 27/1000 | Loss: 0.00001406
Iteration 28/1000 | Loss: 0.00001406
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001405
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001402
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001402
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001397
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001395
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001393
Iteration 57/1000 | Loss: 0.00001392
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001392
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001391
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001390
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001382
Iteration 84/1000 | Loss: 0.00001382
Iteration 85/1000 | Loss: 0.00001381
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001381
Iteration 88/1000 | Loss: 0.00001380
Iteration 89/1000 | Loss: 0.00001380
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001379
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001374
Iteration 107/1000 | Loss: 0.00001374
Iteration 108/1000 | Loss: 0.00001374
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001371
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001370
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001368
Iteration 130/1000 | Loss: 0.00001368
Iteration 131/1000 | Loss: 0.00001368
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001367
Iteration 135/1000 | Loss: 0.00001367
Iteration 136/1000 | Loss: 0.00001367
Iteration 137/1000 | Loss: 0.00001367
Iteration 138/1000 | Loss: 0.00001367
Iteration 139/1000 | Loss: 0.00001366
Iteration 140/1000 | Loss: 0.00001366
Iteration 141/1000 | Loss: 0.00001366
Iteration 142/1000 | Loss: 0.00001366
Iteration 143/1000 | Loss: 0.00001366
Iteration 144/1000 | Loss: 0.00001366
Iteration 145/1000 | Loss: 0.00001366
Iteration 146/1000 | Loss: 0.00001365
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001364
Iteration 151/1000 | Loss: 0.00001364
Iteration 152/1000 | Loss: 0.00001364
Iteration 153/1000 | Loss: 0.00001363
Iteration 154/1000 | Loss: 0.00001363
Iteration 155/1000 | Loss: 0.00001363
Iteration 156/1000 | Loss: 0.00001363
Iteration 157/1000 | Loss: 0.00001363
Iteration 158/1000 | Loss: 0.00001363
Iteration 159/1000 | Loss: 0.00001363
Iteration 160/1000 | Loss: 0.00001362
Iteration 161/1000 | Loss: 0.00001362
Iteration 162/1000 | Loss: 0.00001362
Iteration 163/1000 | Loss: 0.00001362
Iteration 164/1000 | Loss: 0.00001362
Iteration 165/1000 | Loss: 0.00001362
Iteration 166/1000 | Loss: 0.00001361
Iteration 167/1000 | Loss: 0.00001361
Iteration 168/1000 | Loss: 0.00001360
Iteration 169/1000 | Loss: 0.00001360
Iteration 170/1000 | Loss: 0.00001360
Iteration 171/1000 | Loss: 0.00001359
Iteration 172/1000 | Loss: 0.00001359
Iteration 173/1000 | Loss: 0.00001359
Iteration 174/1000 | Loss: 0.00001359
Iteration 175/1000 | Loss: 0.00001359
Iteration 176/1000 | Loss: 0.00001358
Iteration 177/1000 | Loss: 0.00001358
Iteration 178/1000 | Loss: 0.00001358
Iteration 179/1000 | Loss: 0.00001358
Iteration 180/1000 | Loss: 0.00001358
Iteration 181/1000 | Loss: 0.00001358
Iteration 182/1000 | Loss: 0.00001357
Iteration 183/1000 | Loss: 0.00001357
Iteration 184/1000 | Loss: 0.00001357
Iteration 185/1000 | Loss: 0.00001357
Iteration 186/1000 | Loss: 0.00001357
Iteration 187/1000 | Loss: 0.00001356
Iteration 188/1000 | Loss: 0.00001356
Iteration 189/1000 | Loss: 0.00001356
Iteration 190/1000 | Loss: 0.00001356
Iteration 191/1000 | Loss: 0.00001356
Iteration 192/1000 | Loss: 0.00001356
Iteration 193/1000 | Loss: 0.00001355
Iteration 194/1000 | Loss: 0.00001355
Iteration 195/1000 | Loss: 0.00001355
Iteration 196/1000 | Loss: 0.00001355
Iteration 197/1000 | Loss: 0.00001355
Iteration 198/1000 | Loss: 0.00001355
Iteration 199/1000 | Loss: 0.00001355
Iteration 200/1000 | Loss: 0.00001355
Iteration 201/1000 | Loss: 0.00001355
Iteration 202/1000 | Loss: 0.00001355
Iteration 203/1000 | Loss: 0.00001354
Iteration 204/1000 | Loss: 0.00001354
Iteration 205/1000 | Loss: 0.00001354
Iteration 206/1000 | Loss: 0.00001354
Iteration 207/1000 | Loss: 0.00001354
Iteration 208/1000 | Loss: 0.00001354
Iteration 209/1000 | Loss: 0.00001354
Iteration 210/1000 | Loss: 0.00001354
Iteration 211/1000 | Loss: 0.00001354
Iteration 212/1000 | Loss: 0.00001354
Iteration 213/1000 | Loss: 0.00001354
Iteration 214/1000 | Loss: 0.00001354
Iteration 215/1000 | Loss: 0.00001353
Iteration 216/1000 | Loss: 0.00001353
Iteration 217/1000 | Loss: 0.00001353
Iteration 218/1000 | Loss: 0.00001353
Iteration 219/1000 | Loss: 0.00001353
Iteration 220/1000 | Loss: 0.00001353
Iteration 221/1000 | Loss: 0.00001353
Iteration 222/1000 | Loss: 0.00001353
Iteration 223/1000 | Loss: 0.00001353
Iteration 224/1000 | Loss: 0.00001353
Iteration 225/1000 | Loss: 0.00001353
Iteration 226/1000 | Loss: 0.00001353
Iteration 227/1000 | Loss: 0.00001353
Iteration 228/1000 | Loss: 0.00001353
Iteration 229/1000 | Loss: 0.00001352
Iteration 230/1000 | Loss: 0.00001352
Iteration 231/1000 | Loss: 0.00001352
Iteration 232/1000 | Loss: 0.00001352
Iteration 233/1000 | Loss: 0.00001352
Iteration 234/1000 | Loss: 0.00001351
Iteration 235/1000 | Loss: 0.00001351
Iteration 236/1000 | Loss: 0.00001351
Iteration 237/1000 | Loss: 0.00001351
Iteration 238/1000 | Loss: 0.00001351
Iteration 239/1000 | Loss: 0.00001351
Iteration 240/1000 | Loss: 0.00001351
Iteration 241/1000 | Loss: 0.00001351
Iteration 242/1000 | Loss: 0.00001351
Iteration 243/1000 | Loss: 0.00001351
Iteration 244/1000 | Loss: 0.00001351
Iteration 245/1000 | Loss: 0.00001351
Iteration 246/1000 | Loss: 0.00001351
Iteration 247/1000 | Loss: 0.00001351
Iteration 248/1000 | Loss: 0.00001351
Iteration 249/1000 | Loss: 0.00001351
Iteration 250/1000 | Loss: 0.00001351
Iteration 251/1000 | Loss: 0.00001351
Iteration 252/1000 | Loss: 0.00001351
Iteration 253/1000 | Loss: 0.00001351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.3509242307918612e-05, 1.3509242307918612e-05, 1.3509242307918612e-05, 1.3509242307918612e-05, 1.3509242307918612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3509242307918612e-05

Optimization complete. Final v2v error: 3.0752530097961426 mm

Highest mean error: 4.605257034301758 mm for frame 81

Lowest mean error: 2.730558395385742 mm for frame 191

Saving results

Total time: 48.7617347240448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394234
Iteration 2/25 | Loss: 0.00129365
Iteration 3/25 | Loss: 0.00124763
Iteration 4/25 | Loss: 0.00124035
Iteration 5/25 | Loss: 0.00123844
Iteration 6/25 | Loss: 0.00123844
Iteration 7/25 | Loss: 0.00123844
Iteration 8/25 | Loss: 0.00123844
Iteration 9/25 | Loss: 0.00123844
Iteration 10/25 | Loss: 0.00123844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012384394649416208, 0.0012384394649416208, 0.0012384394649416208, 0.0012384394649416208, 0.0012384394649416208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012384394649416208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44107580
Iteration 2/25 | Loss: 0.00080368
Iteration 3/25 | Loss: 0.00080367
Iteration 4/25 | Loss: 0.00080367
Iteration 5/25 | Loss: 0.00080367
Iteration 6/25 | Loss: 0.00080367
Iteration 7/25 | Loss: 0.00080367
Iteration 8/25 | Loss: 0.00080367
Iteration 9/25 | Loss: 0.00080367
Iteration 10/25 | Loss: 0.00080367
Iteration 11/25 | Loss: 0.00080367
Iteration 12/25 | Loss: 0.00080367
Iteration 13/25 | Loss: 0.00080367
Iteration 14/25 | Loss: 0.00080367
Iteration 15/25 | Loss: 0.00080367
Iteration 16/25 | Loss: 0.00080367
Iteration 17/25 | Loss: 0.00080367
Iteration 18/25 | Loss: 0.00080367
Iteration 19/25 | Loss: 0.00080367
Iteration 20/25 | Loss: 0.00080367
Iteration 21/25 | Loss: 0.00080367
Iteration 22/25 | Loss: 0.00080367
Iteration 23/25 | Loss: 0.00080367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008036715444177389, 0.0008036715444177389, 0.0008036715444177389, 0.0008036715444177389, 0.0008036715444177389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008036715444177389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080367
Iteration 2/1000 | Loss: 0.00002613
Iteration 3/1000 | Loss: 0.00001899
Iteration 4/1000 | Loss: 0.00001710
Iteration 5/1000 | Loss: 0.00001618
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001489
Iteration 9/1000 | Loss: 0.00001455
Iteration 10/1000 | Loss: 0.00001440
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001415
Iteration 17/1000 | Loss: 0.00001414
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001411
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00001403
Iteration 24/1000 | Loss: 0.00001400
Iteration 25/1000 | Loss: 0.00001400
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001400
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00001400
Iteration 30/1000 | Loss: 0.00001400
Iteration 31/1000 | Loss: 0.00001400
Iteration 32/1000 | Loss: 0.00001400
Iteration 33/1000 | Loss: 0.00001400
Iteration 34/1000 | Loss: 0.00001399
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001396
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001395
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001393
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001390
Iteration 44/1000 | Loss: 0.00001390
Iteration 45/1000 | Loss: 0.00001388
Iteration 46/1000 | Loss: 0.00001388
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001386
Iteration 49/1000 | Loss: 0.00001386
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001384
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001384
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001381
Iteration 62/1000 | Loss: 0.00001381
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001379
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00001379
Iteration 67/1000 | Loss: 0.00001379
Iteration 68/1000 | Loss: 0.00001379
Iteration 69/1000 | Loss: 0.00001379
Iteration 70/1000 | Loss: 0.00001379
Iteration 71/1000 | Loss: 0.00001379
Iteration 72/1000 | Loss: 0.00001379
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001378
Iteration 76/1000 | Loss: 0.00001378
Iteration 77/1000 | Loss: 0.00001377
Iteration 78/1000 | Loss: 0.00001377
Iteration 79/1000 | Loss: 0.00001377
Iteration 80/1000 | Loss: 0.00001377
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001376
Iteration 83/1000 | Loss: 0.00001376
Iteration 84/1000 | Loss: 0.00001376
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001369
Iteration 111/1000 | Loss: 0.00001369
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001368
Iteration 118/1000 | Loss: 0.00001368
Iteration 119/1000 | Loss: 0.00001368
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001367
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001366
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001365
Iteration 130/1000 | Loss: 0.00001365
Iteration 131/1000 | Loss: 0.00001365
Iteration 132/1000 | Loss: 0.00001364
Iteration 133/1000 | Loss: 0.00001364
Iteration 134/1000 | Loss: 0.00001364
Iteration 135/1000 | Loss: 0.00001364
Iteration 136/1000 | Loss: 0.00001363
Iteration 137/1000 | Loss: 0.00001363
Iteration 138/1000 | Loss: 0.00001363
Iteration 139/1000 | Loss: 0.00001363
Iteration 140/1000 | Loss: 0.00001363
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00001363
Iteration 143/1000 | Loss: 0.00001362
Iteration 144/1000 | Loss: 0.00001362
Iteration 145/1000 | Loss: 0.00001362
Iteration 146/1000 | Loss: 0.00001362
Iteration 147/1000 | Loss: 0.00001362
Iteration 148/1000 | Loss: 0.00001362
Iteration 149/1000 | Loss: 0.00001361
Iteration 150/1000 | Loss: 0.00001361
Iteration 151/1000 | Loss: 0.00001361
Iteration 152/1000 | Loss: 0.00001361
Iteration 153/1000 | Loss: 0.00001361
Iteration 154/1000 | Loss: 0.00001360
Iteration 155/1000 | Loss: 0.00001360
Iteration 156/1000 | Loss: 0.00001360
Iteration 157/1000 | Loss: 0.00001360
Iteration 158/1000 | Loss: 0.00001360
Iteration 159/1000 | Loss: 0.00001360
Iteration 160/1000 | Loss: 0.00001360
Iteration 161/1000 | Loss: 0.00001359
Iteration 162/1000 | Loss: 0.00001359
Iteration 163/1000 | Loss: 0.00001358
Iteration 164/1000 | Loss: 0.00001357
Iteration 165/1000 | Loss: 0.00001357
Iteration 166/1000 | Loss: 0.00001357
Iteration 167/1000 | Loss: 0.00001357
Iteration 168/1000 | Loss: 0.00001357
Iteration 169/1000 | Loss: 0.00001357
Iteration 170/1000 | Loss: 0.00001357
Iteration 171/1000 | Loss: 0.00001357
Iteration 172/1000 | Loss: 0.00001357
Iteration 173/1000 | Loss: 0.00001356
Iteration 174/1000 | Loss: 0.00001356
Iteration 175/1000 | Loss: 0.00001356
Iteration 176/1000 | Loss: 0.00001356
Iteration 177/1000 | Loss: 0.00001356
Iteration 178/1000 | Loss: 0.00001355
Iteration 179/1000 | Loss: 0.00001355
Iteration 180/1000 | Loss: 0.00001355
Iteration 181/1000 | Loss: 0.00001355
Iteration 182/1000 | Loss: 0.00001355
Iteration 183/1000 | Loss: 0.00001355
Iteration 184/1000 | Loss: 0.00001355
Iteration 185/1000 | Loss: 0.00001355
Iteration 186/1000 | Loss: 0.00001354
Iteration 187/1000 | Loss: 0.00001354
Iteration 188/1000 | Loss: 0.00001354
Iteration 189/1000 | Loss: 0.00001354
Iteration 190/1000 | Loss: 0.00001354
Iteration 191/1000 | Loss: 0.00001354
Iteration 192/1000 | Loss: 0.00001354
Iteration 193/1000 | Loss: 0.00001354
Iteration 194/1000 | Loss: 0.00001354
Iteration 195/1000 | Loss: 0.00001354
Iteration 196/1000 | Loss: 0.00001354
Iteration 197/1000 | Loss: 0.00001354
Iteration 198/1000 | Loss: 0.00001354
Iteration 199/1000 | Loss: 0.00001354
Iteration 200/1000 | Loss: 0.00001354
Iteration 201/1000 | Loss: 0.00001354
Iteration 202/1000 | Loss: 0.00001354
Iteration 203/1000 | Loss: 0.00001354
Iteration 204/1000 | Loss: 0.00001354
Iteration 205/1000 | Loss: 0.00001354
Iteration 206/1000 | Loss: 0.00001354
Iteration 207/1000 | Loss: 0.00001354
Iteration 208/1000 | Loss: 0.00001354
Iteration 209/1000 | Loss: 0.00001354
Iteration 210/1000 | Loss: 0.00001354
Iteration 211/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.3541558473662008e-05, 1.3541558473662008e-05, 1.3541558473662008e-05, 1.3541558473662008e-05, 1.3541558473662008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3541558473662008e-05

Optimization complete. Final v2v error: 3.128964900970459 mm

Highest mean error: 3.2512705326080322 mm for frame 181

Lowest mean error: 3.0624449253082275 mm for frame 204

Saving results

Total time: 44.07071900367737
