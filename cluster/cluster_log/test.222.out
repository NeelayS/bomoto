Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=222, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12432-12487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1064
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773878
Iteration 2/25 | Loss: 0.00197267
Iteration 3/25 | Loss: 0.00128493
Iteration 4/25 | Loss: 0.00118122
Iteration 5/25 | Loss: 0.00116892
Iteration 6/25 | Loss: 0.00115453
Iteration 7/25 | Loss: 0.00115331
Iteration 8/25 | Loss: 0.00111559
Iteration 9/25 | Loss: 0.00111680
Iteration 10/25 | Loss: 0.00110550
Iteration 11/25 | Loss: 0.00110837
Iteration 12/25 | Loss: 0.00110413
Iteration 13/25 | Loss: 0.00110371
Iteration 14/25 | Loss: 0.00110793
Iteration 15/25 | Loss: 0.00110556
Iteration 16/25 | Loss: 0.00110367
Iteration 17/25 | Loss: 0.00110364
Iteration 18/25 | Loss: 0.00110364
Iteration 19/25 | Loss: 0.00110364
Iteration 20/25 | Loss: 0.00110363
Iteration 21/25 | Loss: 0.00110363
Iteration 22/25 | Loss: 0.00110363
Iteration 23/25 | Loss: 0.00110363
Iteration 24/25 | Loss: 0.00110363
Iteration 25/25 | Loss: 0.00110363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41770077
Iteration 2/25 | Loss: 0.00093683
Iteration 3/25 | Loss: 0.00091567
Iteration 4/25 | Loss: 0.00091567
Iteration 5/25 | Loss: 0.00091567
Iteration 6/25 | Loss: 0.00091567
Iteration 7/25 | Loss: 0.00091567
Iteration 8/25 | Loss: 0.00091567
Iteration 9/25 | Loss: 0.00091567
Iteration 10/25 | Loss: 0.00091567
Iteration 11/25 | Loss: 0.00091566
Iteration 12/25 | Loss: 0.00091566
Iteration 13/25 | Loss: 0.00091566
Iteration 14/25 | Loss: 0.00091566
Iteration 15/25 | Loss: 0.00091566
Iteration 16/25 | Loss: 0.00091566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000915664539206773, 0.000915664539206773, 0.000915664539206773, 0.000915664539206773, 0.000915664539206773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000915664539206773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091566
Iteration 2/1000 | Loss: 0.00002204
Iteration 3/1000 | Loss: 0.00004985
Iteration 4/1000 | Loss: 0.00001381
Iteration 5/1000 | Loss: 0.00006607
Iteration 6/1000 | Loss: 0.00042995
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00011994
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001059
Iteration 13/1000 | Loss: 0.00001045
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001038
Iteration 16/1000 | Loss: 0.00001032
Iteration 17/1000 | Loss: 0.00001031
Iteration 18/1000 | Loss: 0.00001029
Iteration 19/1000 | Loss: 0.00001029
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00000996
Iteration 22/1000 | Loss: 0.00022044
Iteration 23/1000 | Loss: 0.00015450
Iteration 24/1000 | Loss: 0.00033975
Iteration 25/1000 | Loss: 0.00003778
Iteration 26/1000 | Loss: 0.00002159
Iteration 27/1000 | Loss: 0.00001017
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00009243
Iteration 30/1000 | Loss: 0.00000995
Iteration 31/1000 | Loss: 0.00002306
Iteration 32/1000 | Loss: 0.00021144
Iteration 33/1000 | Loss: 0.00001003
Iteration 34/1000 | Loss: 0.00004624
Iteration 35/1000 | Loss: 0.00000972
Iteration 36/1000 | Loss: 0.00000958
Iteration 37/1000 | Loss: 0.00005587
Iteration 38/1000 | Loss: 0.00043197
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00000958
Iteration 41/1000 | Loss: 0.00001938
Iteration 42/1000 | Loss: 0.00000954
Iteration 43/1000 | Loss: 0.00000954
Iteration 44/1000 | Loss: 0.00000954
Iteration 45/1000 | Loss: 0.00000954
Iteration 46/1000 | Loss: 0.00000954
Iteration 47/1000 | Loss: 0.00000953
Iteration 48/1000 | Loss: 0.00000953
Iteration 49/1000 | Loss: 0.00000953
Iteration 50/1000 | Loss: 0.00000952
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00000950
Iteration 53/1000 | Loss: 0.00000949
Iteration 54/1000 | Loss: 0.00000949
Iteration 55/1000 | Loss: 0.00000949
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000948
Iteration 59/1000 | Loss: 0.00000948
Iteration 60/1000 | Loss: 0.00000948
Iteration 61/1000 | Loss: 0.00000948
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000947
Iteration 64/1000 | Loss: 0.00000947
Iteration 65/1000 | Loss: 0.00000947
Iteration 66/1000 | Loss: 0.00000947
Iteration 67/1000 | Loss: 0.00000947
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000947
Iteration 71/1000 | Loss: 0.00000947
Iteration 72/1000 | Loss: 0.00000947
Iteration 73/1000 | Loss: 0.00000947
Iteration 74/1000 | Loss: 0.00000946
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000946
Iteration 78/1000 | Loss: 0.00000946
Iteration 79/1000 | Loss: 0.00000946
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000945
Iteration 84/1000 | Loss: 0.00000945
Iteration 85/1000 | Loss: 0.00003244
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000950
Iteration 88/1000 | Loss: 0.00000946
Iteration 89/1000 | Loss: 0.00000945
Iteration 90/1000 | Loss: 0.00000945
Iteration 91/1000 | Loss: 0.00000945
Iteration 92/1000 | Loss: 0.00000945
Iteration 93/1000 | Loss: 0.00000945
Iteration 94/1000 | Loss: 0.00000945
Iteration 95/1000 | Loss: 0.00000945
Iteration 96/1000 | Loss: 0.00000944
Iteration 97/1000 | Loss: 0.00000944
Iteration 98/1000 | Loss: 0.00000944
Iteration 99/1000 | Loss: 0.00000944
Iteration 100/1000 | Loss: 0.00000944
Iteration 101/1000 | Loss: 0.00000944
Iteration 102/1000 | Loss: 0.00000944
Iteration 103/1000 | Loss: 0.00000944
Iteration 104/1000 | Loss: 0.00000943
Iteration 105/1000 | Loss: 0.00000943
Iteration 106/1000 | Loss: 0.00000943
Iteration 107/1000 | Loss: 0.00000943
Iteration 108/1000 | Loss: 0.00000943
Iteration 109/1000 | Loss: 0.00000943
Iteration 110/1000 | Loss: 0.00000943
Iteration 111/1000 | Loss: 0.00000943
Iteration 112/1000 | Loss: 0.00000943
Iteration 113/1000 | Loss: 0.00000943
Iteration 114/1000 | Loss: 0.00000943
Iteration 115/1000 | Loss: 0.00000943
Iteration 116/1000 | Loss: 0.00000942
Iteration 117/1000 | Loss: 0.00000942
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000942
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000942
Iteration 125/1000 | Loss: 0.00000942
Iteration 126/1000 | Loss: 0.00000942
Iteration 127/1000 | Loss: 0.00000942
Iteration 128/1000 | Loss: 0.00000942
Iteration 129/1000 | Loss: 0.00000942
Iteration 130/1000 | Loss: 0.00000942
Iteration 131/1000 | Loss: 0.00000942
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000942
Iteration 135/1000 | Loss: 0.00000942
Iteration 136/1000 | Loss: 0.00000942
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000941
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000941
Iteration 143/1000 | Loss: 0.00000941
Iteration 144/1000 | Loss: 0.00000941
Iteration 145/1000 | Loss: 0.00000941
Iteration 146/1000 | Loss: 0.00000941
Iteration 147/1000 | Loss: 0.00000941
Iteration 148/1000 | Loss: 0.00000941
Iteration 149/1000 | Loss: 0.00000941
Iteration 150/1000 | Loss: 0.00000941
Iteration 151/1000 | Loss: 0.00000941
Iteration 152/1000 | Loss: 0.00000941
Iteration 153/1000 | Loss: 0.00000941
Iteration 154/1000 | Loss: 0.00000940
Iteration 155/1000 | Loss: 0.00000940
Iteration 156/1000 | Loss: 0.00000940
Iteration 157/1000 | Loss: 0.00000940
Iteration 158/1000 | Loss: 0.00000940
Iteration 159/1000 | Loss: 0.00000940
Iteration 160/1000 | Loss: 0.00000940
Iteration 161/1000 | Loss: 0.00000940
Iteration 162/1000 | Loss: 0.00000940
Iteration 163/1000 | Loss: 0.00000940
Iteration 164/1000 | Loss: 0.00000940
Iteration 165/1000 | Loss: 0.00000940
Iteration 166/1000 | Loss: 0.00000940
Iteration 167/1000 | Loss: 0.00000940
Iteration 168/1000 | Loss: 0.00000939
Iteration 169/1000 | Loss: 0.00000939
Iteration 170/1000 | Loss: 0.00000939
Iteration 171/1000 | Loss: 0.00000939
Iteration 172/1000 | Loss: 0.00000939
Iteration 173/1000 | Loss: 0.00000939
Iteration 174/1000 | Loss: 0.00000939
Iteration 175/1000 | Loss: 0.00000939
Iteration 176/1000 | Loss: 0.00000939
Iteration 177/1000 | Loss: 0.00000939
Iteration 178/1000 | Loss: 0.00000938
Iteration 179/1000 | Loss: 0.00000938
Iteration 180/1000 | Loss: 0.00000938
Iteration 181/1000 | Loss: 0.00000938
Iteration 182/1000 | Loss: 0.00000938
Iteration 183/1000 | Loss: 0.00000938
Iteration 184/1000 | Loss: 0.00000938
Iteration 185/1000 | Loss: 0.00000938
Iteration 186/1000 | Loss: 0.00000938
Iteration 187/1000 | Loss: 0.00000938
Iteration 188/1000 | Loss: 0.00000938
Iteration 189/1000 | Loss: 0.00000938
Iteration 190/1000 | Loss: 0.00000938
Iteration 191/1000 | Loss: 0.00000938
Iteration 192/1000 | Loss: 0.00000938
Iteration 193/1000 | Loss: 0.00000938
Iteration 194/1000 | Loss: 0.00000938
Iteration 195/1000 | Loss: 0.00000938
Iteration 196/1000 | Loss: 0.00000938
Iteration 197/1000 | Loss: 0.00000938
Iteration 198/1000 | Loss: 0.00000938
Iteration 199/1000 | Loss: 0.00000938
Iteration 200/1000 | Loss: 0.00000938
Iteration 201/1000 | Loss: 0.00000938
Iteration 202/1000 | Loss: 0.00000938
Iteration 203/1000 | Loss: 0.00000937
Iteration 204/1000 | Loss: 0.00000937
Iteration 205/1000 | Loss: 0.00000937
Iteration 206/1000 | Loss: 0.00000937
Iteration 207/1000 | Loss: 0.00000937
Iteration 208/1000 | Loss: 0.00000937
Iteration 209/1000 | Loss: 0.00000937
Iteration 210/1000 | Loss: 0.00000937
Iteration 211/1000 | Loss: 0.00000937
Iteration 212/1000 | Loss: 0.00000937
Iteration 213/1000 | Loss: 0.00000937
Iteration 214/1000 | Loss: 0.00000937
Iteration 215/1000 | Loss: 0.00000937
Iteration 216/1000 | Loss: 0.00000937
Iteration 217/1000 | Loss: 0.00000937
Iteration 218/1000 | Loss: 0.00000937
Iteration 219/1000 | Loss: 0.00000937
Iteration 220/1000 | Loss: 0.00000937
Iteration 221/1000 | Loss: 0.00000937
Iteration 222/1000 | Loss: 0.00000937
Iteration 223/1000 | Loss: 0.00000937
Iteration 224/1000 | Loss: 0.00000937
Iteration 225/1000 | Loss: 0.00000937
Iteration 226/1000 | Loss: 0.00000937
Iteration 227/1000 | Loss: 0.00000937
Iteration 228/1000 | Loss: 0.00000937
Iteration 229/1000 | Loss: 0.00000937
Iteration 230/1000 | Loss: 0.00000937
Iteration 231/1000 | Loss: 0.00000937
Iteration 232/1000 | Loss: 0.00000937
Iteration 233/1000 | Loss: 0.00000937
Iteration 234/1000 | Loss: 0.00000937
Iteration 235/1000 | Loss: 0.00000937
Iteration 236/1000 | Loss: 0.00000937
Iteration 237/1000 | Loss: 0.00000937
Iteration 238/1000 | Loss: 0.00000937
Iteration 239/1000 | Loss: 0.00000937
Iteration 240/1000 | Loss: 0.00000937
Iteration 241/1000 | Loss: 0.00000937
Iteration 242/1000 | Loss: 0.00000937
Iteration 243/1000 | Loss: 0.00000937
Iteration 244/1000 | Loss: 0.00000937
Iteration 245/1000 | Loss: 0.00000937
Iteration 246/1000 | Loss: 0.00000937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [9.365218829771038e-06, 9.365218829771038e-06, 9.365218829771038e-06, 9.365218829771038e-06, 9.365218829771038e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.365218829771038e-06

Optimization complete. Final v2v error: 2.6498160362243652 mm

Highest mean error: 2.966928720474243 mm for frame 57

Lowest mean error: 2.4677774906158447 mm for frame 148

Saving results

Total time: 2527.419330596924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1023
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832812
Iteration 2/25 | Loss: 0.00122439
Iteration 3/25 | Loss: 0.00112911
Iteration 4/25 | Loss: 0.00111992
Iteration 5/25 | Loss: 0.00111803
Iteration 6/25 | Loss: 0.00111778
Iteration 7/25 | Loss: 0.00111778
Iteration 8/25 | Loss: 0.00111778
Iteration 9/25 | Loss: 0.00111778
Iteration 10/25 | Loss: 0.00111778
Iteration 11/25 | Loss: 0.00111778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011177805718034506, 0.0011177805718034506, 0.0011177805718034506, 0.0011177805718034506, 0.0011177805718034506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011177805718034506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35323310
Iteration 2/25 | Loss: 0.00070453
Iteration 3/25 | Loss: 0.00070451
Iteration 4/25 | Loss: 0.00070451
Iteration 5/25 | Loss: 0.00070450
Iteration 6/25 | Loss: 0.00070450
Iteration 7/25 | Loss: 0.00070450
Iteration 8/25 | Loss: 0.00070450
Iteration 9/25 | Loss: 0.00070450
Iteration 10/25 | Loss: 0.00070450
Iteration 11/25 | Loss: 0.00070450
Iteration 12/25 | Loss: 0.00070450
Iteration 13/25 | Loss: 0.00070450
Iteration 14/25 | Loss: 0.00070450
Iteration 15/25 | Loss: 0.00070450
Iteration 16/25 | Loss: 0.00070450
Iteration 17/25 | Loss: 0.00070450
Iteration 18/25 | Loss: 0.00070450
Iteration 19/25 | Loss: 0.00070450
Iteration 20/25 | Loss: 0.00070450
Iteration 21/25 | Loss: 0.00070450
Iteration 22/25 | Loss: 0.00070450
Iteration 23/25 | Loss: 0.00070450
Iteration 24/25 | Loss: 0.00070450
Iteration 25/25 | Loss: 0.00070450

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070450
Iteration 2/1000 | Loss: 0.00001995
Iteration 3/1000 | Loss: 0.00001385
Iteration 4/1000 | Loss: 0.00001187
Iteration 5/1000 | Loss: 0.00001078
Iteration 6/1000 | Loss: 0.00001009
Iteration 7/1000 | Loss: 0.00000975
Iteration 8/1000 | Loss: 0.00000939
Iteration 9/1000 | Loss: 0.00000938
Iteration 10/1000 | Loss: 0.00000925
Iteration 11/1000 | Loss: 0.00000905
Iteration 12/1000 | Loss: 0.00000900
Iteration 13/1000 | Loss: 0.00000896
Iteration 14/1000 | Loss: 0.00000896
Iteration 15/1000 | Loss: 0.00000895
Iteration 16/1000 | Loss: 0.00000895
Iteration 17/1000 | Loss: 0.00000894
Iteration 18/1000 | Loss: 0.00000893
Iteration 19/1000 | Loss: 0.00000892
Iteration 20/1000 | Loss: 0.00000892
Iteration 21/1000 | Loss: 0.00000885
Iteration 22/1000 | Loss: 0.00000885
Iteration 23/1000 | Loss: 0.00000879
Iteration 24/1000 | Loss: 0.00000876
Iteration 25/1000 | Loss: 0.00000876
Iteration 26/1000 | Loss: 0.00000875
Iteration 27/1000 | Loss: 0.00000875
Iteration 28/1000 | Loss: 0.00000871
Iteration 29/1000 | Loss: 0.00000869
Iteration 30/1000 | Loss: 0.00000868
Iteration 31/1000 | Loss: 0.00000868
Iteration 32/1000 | Loss: 0.00000867
Iteration 33/1000 | Loss: 0.00000867
Iteration 34/1000 | Loss: 0.00000866
Iteration 35/1000 | Loss: 0.00000866
Iteration 36/1000 | Loss: 0.00000866
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000865
Iteration 39/1000 | Loss: 0.00000865
Iteration 40/1000 | Loss: 0.00000865
Iteration 41/1000 | Loss: 0.00000865
Iteration 42/1000 | Loss: 0.00000865
Iteration 43/1000 | Loss: 0.00000865
Iteration 44/1000 | Loss: 0.00000865
Iteration 45/1000 | Loss: 0.00000865
Iteration 46/1000 | Loss: 0.00000865
Iteration 47/1000 | Loss: 0.00000864
Iteration 48/1000 | Loss: 0.00000864
Iteration 49/1000 | Loss: 0.00000864
Iteration 50/1000 | Loss: 0.00000864
Iteration 51/1000 | Loss: 0.00000864
Iteration 52/1000 | Loss: 0.00000864
Iteration 53/1000 | Loss: 0.00000864
Iteration 54/1000 | Loss: 0.00000864
Iteration 55/1000 | Loss: 0.00000864
Iteration 56/1000 | Loss: 0.00000864
Iteration 57/1000 | Loss: 0.00000863
Iteration 58/1000 | Loss: 0.00000863
Iteration 59/1000 | Loss: 0.00000863
Iteration 60/1000 | Loss: 0.00000863
Iteration 61/1000 | Loss: 0.00000863
Iteration 62/1000 | Loss: 0.00000863
Iteration 63/1000 | Loss: 0.00000863
Iteration 64/1000 | Loss: 0.00000863
Iteration 65/1000 | Loss: 0.00000863
Iteration 66/1000 | Loss: 0.00000862
Iteration 67/1000 | Loss: 0.00000862
Iteration 68/1000 | Loss: 0.00000862
Iteration 69/1000 | Loss: 0.00000862
Iteration 70/1000 | Loss: 0.00000862
Iteration 71/1000 | Loss: 0.00000862
Iteration 72/1000 | Loss: 0.00000862
Iteration 73/1000 | Loss: 0.00000862
Iteration 74/1000 | Loss: 0.00000862
Iteration 75/1000 | Loss: 0.00000862
Iteration 76/1000 | Loss: 0.00000862
Iteration 77/1000 | Loss: 0.00000862
Iteration 78/1000 | Loss: 0.00000861
Iteration 79/1000 | Loss: 0.00000861
Iteration 80/1000 | Loss: 0.00000861
Iteration 81/1000 | Loss: 0.00000861
Iteration 82/1000 | Loss: 0.00000860
Iteration 83/1000 | Loss: 0.00000860
Iteration 84/1000 | Loss: 0.00000860
Iteration 85/1000 | Loss: 0.00000860
Iteration 86/1000 | Loss: 0.00000860
Iteration 87/1000 | Loss: 0.00000860
Iteration 88/1000 | Loss: 0.00000860
Iteration 89/1000 | Loss: 0.00000859
Iteration 90/1000 | Loss: 0.00000859
Iteration 91/1000 | Loss: 0.00000859
Iteration 92/1000 | Loss: 0.00000859
Iteration 93/1000 | Loss: 0.00000859
Iteration 94/1000 | Loss: 0.00000859
Iteration 95/1000 | Loss: 0.00000859
Iteration 96/1000 | Loss: 0.00000858
Iteration 97/1000 | Loss: 0.00000858
Iteration 98/1000 | Loss: 0.00000857
Iteration 99/1000 | Loss: 0.00000857
Iteration 100/1000 | Loss: 0.00000857
Iteration 101/1000 | Loss: 0.00000857
Iteration 102/1000 | Loss: 0.00000857
Iteration 103/1000 | Loss: 0.00000857
Iteration 104/1000 | Loss: 0.00000857
Iteration 105/1000 | Loss: 0.00000857
Iteration 106/1000 | Loss: 0.00000857
Iteration 107/1000 | Loss: 0.00000857
Iteration 108/1000 | Loss: 0.00000857
Iteration 109/1000 | Loss: 0.00000856
Iteration 110/1000 | Loss: 0.00000856
Iteration 111/1000 | Loss: 0.00000856
Iteration 112/1000 | Loss: 0.00000856
Iteration 113/1000 | Loss: 0.00000856
Iteration 114/1000 | Loss: 0.00000856
Iteration 115/1000 | Loss: 0.00000856
Iteration 116/1000 | Loss: 0.00000856
Iteration 117/1000 | Loss: 0.00000856
Iteration 118/1000 | Loss: 0.00000856
Iteration 119/1000 | Loss: 0.00000856
Iteration 120/1000 | Loss: 0.00000856
Iteration 121/1000 | Loss: 0.00000856
Iteration 122/1000 | Loss: 0.00000856
Iteration 123/1000 | Loss: 0.00000855
Iteration 124/1000 | Loss: 0.00000855
Iteration 125/1000 | Loss: 0.00000855
Iteration 126/1000 | Loss: 0.00000855
Iteration 127/1000 | Loss: 0.00000855
Iteration 128/1000 | Loss: 0.00000854
Iteration 129/1000 | Loss: 0.00000854
Iteration 130/1000 | Loss: 0.00000854
Iteration 131/1000 | Loss: 0.00000854
Iteration 132/1000 | Loss: 0.00000853
Iteration 133/1000 | Loss: 0.00000853
Iteration 134/1000 | Loss: 0.00000853
Iteration 135/1000 | Loss: 0.00000853
Iteration 136/1000 | Loss: 0.00000853
Iteration 137/1000 | Loss: 0.00000853
Iteration 138/1000 | Loss: 0.00000853
Iteration 139/1000 | Loss: 0.00000853
Iteration 140/1000 | Loss: 0.00000853
Iteration 141/1000 | Loss: 0.00000852
Iteration 142/1000 | Loss: 0.00000852
Iteration 143/1000 | Loss: 0.00000852
Iteration 144/1000 | Loss: 0.00000852
Iteration 145/1000 | Loss: 0.00000851
Iteration 146/1000 | Loss: 0.00000851
Iteration 147/1000 | Loss: 0.00000851
Iteration 148/1000 | Loss: 0.00000850
Iteration 149/1000 | Loss: 0.00000850
Iteration 150/1000 | Loss: 0.00000850
Iteration 151/1000 | Loss: 0.00000849
Iteration 152/1000 | Loss: 0.00000848
Iteration 153/1000 | Loss: 0.00000848
Iteration 154/1000 | Loss: 0.00000848
Iteration 155/1000 | Loss: 0.00000848
Iteration 156/1000 | Loss: 0.00000848
Iteration 157/1000 | Loss: 0.00000848
Iteration 158/1000 | Loss: 0.00000848
Iteration 159/1000 | Loss: 0.00000848
Iteration 160/1000 | Loss: 0.00000848
Iteration 161/1000 | Loss: 0.00000848
Iteration 162/1000 | Loss: 0.00000847
Iteration 163/1000 | Loss: 0.00000847
Iteration 164/1000 | Loss: 0.00000847
Iteration 165/1000 | Loss: 0.00000847
Iteration 166/1000 | Loss: 0.00000847
Iteration 167/1000 | Loss: 0.00000847
Iteration 168/1000 | Loss: 0.00000847
Iteration 169/1000 | Loss: 0.00000847
Iteration 170/1000 | Loss: 0.00000846
Iteration 171/1000 | Loss: 0.00000846
Iteration 172/1000 | Loss: 0.00000846
Iteration 173/1000 | Loss: 0.00000846
Iteration 174/1000 | Loss: 0.00000846
Iteration 175/1000 | Loss: 0.00000846
Iteration 176/1000 | Loss: 0.00000846
Iteration 177/1000 | Loss: 0.00000846
Iteration 178/1000 | Loss: 0.00000846
Iteration 179/1000 | Loss: 0.00000846
Iteration 180/1000 | Loss: 0.00000846
Iteration 181/1000 | Loss: 0.00000846
Iteration 182/1000 | Loss: 0.00000846
Iteration 183/1000 | Loss: 0.00000846
Iteration 184/1000 | Loss: 0.00000846
Iteration 185/1000 | Loss: 0.00000846
Iteration 186/1000 | Loss: 0.00000846
Iteration 187/1000 | Loss: 0.00000846
Iteration 188/1000 | Loss: 0.00000846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [8.456623618258163e-06, 8.456623618258163e-06, 8.456623618258163e-06, 8.456623618258163e-06, 8.456623618258163e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.456623618258163e-06

Optimization complete. Final v2v error: 2.509652614593506 mm

Highest mean error: 2.737391471862793 mm for frame 28

Lowest mean error: 2.3907666206359863 mm for frame 106

Saving results

Total time: 879.4408493041992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1024
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011901
Iteration 2/25 | Loss: 0.00228806
Iteration 3/25 | Loss: 0.00182901
Iteration 4/25 | Loss: 0.00168262
Iteration 5/25 | Loss: 0.00154907
Iteration 6/25 | Loss: 0.00147630
Iteration 7/25 | Loss: 0.00144875
Iteration 8/25 | Loss: 0.00143855
Iteration 9/25 | Loss: 0.00140443
Iteration 10/25 | Loss: 0.00137477
Iteration 11/25 | Loss: 0.00136419
Iteration 12/25 | Loss: 0.00134089
Iteration 13/25 | Loss: 0.00134541
Iteration 14/25 | Loss: 0.00131900
Iteration 15/25 | Loss: 0.00132741
Iteration 16/25 | Loss: 0.00131288
Iteration 17/25 | Loss: 0.00130283
Iteration 18/25 | Loss: 0.00130729
Iteration 19/25 | Loss: 0.00130822
Iteration 20/25 | Loss: 0.00131058
Iteration 21/25 | Loss: 0.00130312
Iteration 22/25 | Loss: 0.00129882
Iteration 23/25 | Loss: 0.00129714
Iteration 24/25 | Loss: 0.00129251
Iteration 25/25 | Loss: 0.00128887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41557884
Iteration 2/25 | Loss: 0.00146617
Iteration 3/25 | Loss: 0.00132684
Iteration 4/25 | Loss: 0.00132683
Iteration 5/25 | Loss: 0.00132683
Iteration 6/25 | Loss: 0.00132682
Iteration 7/25 | Loss: 0.00132682
Iteration 8/25 | Loss: 0.00132682
Iteration 9/25 | Loss: 0.00132682
Iteration 10/25 | Loss: 0.00132682
Iteration 11/25 | Loss: 0.00132682
Iteration 12/25 | Loss: 0.00132682
Iteration 13/25 | Loss: 0.00132682
Iteration 14/25 | Loss: 0.00132682
Iteration 15/25 | Loss: 0.00132682
Iteration 16/25 | Loss: 0.00132682
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013268237235024571, 0.0013268237235024571, 0.0013268237235024571, 0.0013268237235024571, 0.0013268237235024571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013268237235024571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132682
Iteration 2/1000 | Loss: 0.00011839
Iteration 3/1000 | Loss: 0.00026396
Iteration 4/1000 | Loss: 0.00021026
Iteration 5/1000 | Loss: 0.00142425
Iteration 6/1000 | Loss: 0.00008081
Iteration 7/1000 | Loss: 0.00006541
Iteration 8/1000 | Loss: 0.00015694
Iteration 9/1000 | Loss: 0.00031116
Iteration 10/1000 | Loss: 0.00126287
Iteration 11/1000 | Loss: 0.00006129
Iteration 12/1000 | Loss: 0.00035465
Iteration 13/1000 | Loss: 0.00025480
Iteration 14/1000 | Loss: 0.00005700
Iteration 15/1000 | Loss: 0.00006948
Iteration 16/1000 | Loss: 0.00005507
Iteration 17/1000 | Loss: 0.00035338
Iteration 18/1000 | Loss: 0.00022036
Iteration 19/1000 | Loss: 0.00006496
Iteration 20/1000 | Loss: 0.00005273
Iteration 21/1000 | Loss: 0.00180923
Iteration 22/1000 | Loss: 0.00070352
Iteration 23/1000 | Loss: 0.00013339
Iteration 24/1000 | Loss: 0.00016889
Iteration 25/1000 | Loss: 0.00041982
Iteration 26/1000 | Loss: 0.00011090
Iteration 27/1000 | Loss: 0.00007595
Iteration 28/1000 | Loss: 0.00028724
Iteration 29/1000 | Loss: 0.00063143
Iteration 30/1000 | Loss: 0.00003172
Iteration 31/1000 | Loss: 0.00011106
Iteration 32/1000 | Loss: 0.00008273
Iteration 33/1000 | Loss: 0.00045228
Iteration 34/1000 | Loss: 0.00004081
Iteration 35/1000 | Loss: 0.00003160
Iteration 36/1000 | Loss: 0.00007228
Iteration 37/1000 | Loss: 0.00029771
Iteration 38/1000 | Loss: 0.00235386
Iteration 39/1000 | Loss: 0.00041989
Iteration 40/1000 | Loss: 0.00005376
Iteration 41/1000 | Loss: 0.00002281
Iteration 42/1000 | Loss: 0.00012265
Iteration 43/1000 | Loss: 0.00026470
Iteration 44/1000 | Loss: 0.00002549
Iteration 45/1000 | Loss: 0.00002812
Iteration 46/1000 | Loss: 0.00002064
Iteration 47/1000 | Loss: 0.00003667
Iteration 48/1000 | Loss: 0.00010928
Iteration 49/1000 | Loss: 0.00002004
Iteration 50/1000 | Loss: 0.00019177
Iteration 51/1000 | Loss: 0.00002012
Iteration 52/1000 | Loss: 0.00001943
Iteration 53/1000 | Loss: 0.00001913
Iteration 54/1000 | Loss: 0.00001910
Iteration 55/1000 | Loss: 0.00007888
Iteration 56/1000 | Loss: 0.00014061
Iteration 57/1000 | Loss: 0.00006006
Iteration 58/1000 | Loss: 0.00004331
Iteration 59/1000 | Loss: 0.00032171
Iteration 60/1000 | Loss: 0.00003088
Iteration 61/1000 | Loss: 0.00005451
Iteration 62/1000 | Loss: 0.00009801
Iteration 63/1000 | Loss: 0.00002883
Iteration 64/1000 | Loss: 0.00002107
Iteration 65/1000 | Loss: 0.00002236
Iteration 66/1000 | Loss: 0.00002125
Iteration 67/1000 | Loss: 0.00005305
Iteration 68/1000 | Loss: 0.00002092
Iteration 69/1000 | Loss: 0.00005946
Iteration 70/1000 | Loss: 0.00002293
Iteration 71/1000 | Loss: 0.00005115
Iteration 72/1000 | Loss: 0.00002848
Iteration 73/1000 | Loss: 0.00002574
Iteration 74/1000 | Loss: 0.00002020
Iteration 75/1000 | Loss: 0.00002321
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001874
Iteration 80/1000 | Loss: 0.00001874
Iteration 81/1000 | Loss: 0.00001874
Iteration 82/1000 | Loss: 0.00001874
Iteration 83/1000 | Loss: 0.00001874
Iteration 84/1000 | Loss: 0.00001874
Iteration 85/1000 | Loss: 0.00001873
Iteration 86/1000 | Loss: 0.00001873
Iteration 87/1000 | Loss: 0.00001873
Iteration 88/1000 | Loss: 0.00001872
Iteration 89/1000 | Loss: 0.00001872
Iteration 90/1000 | Loss: 0.00001869
Iteration 91/1000 | Loss: 0.00001869
Iteration 92/1000 | Loss: 0.00001869
Iteration 93/1000 | Loss: 0.00001868
Iteration 94/1000 | Loss: 0.00001867
Iteration 95/1000 | Loss: 0.00001867
Iteration 96/1000 | Loss: 0.00002910
Iteration 97/1000 | Loss: 0.00001871
Iteration 98/1000 | Loss: 0.00001868
Iteration 99/1000 | Loss: 0.00001867
Iteration 100/1000 | Loss: 0.00001866
Iteration 101/1000 | Loss: 0.00001865
Iteration 102/1000 | Loss: 0.00001865
Iteration 103/1000 | Loss: 0.00001865
Iteration 104/1000 | Loss: 0.00001864
Iteration 105/1000 | Loss: 0.00001864
Iteration 106/1000 | Loss: 0.00001864
Iteration 107/1000 | Loss: 0.00001863
Iteration 108/1000 | Loss: 0.00001863
Iteration 109/1000 | Loss: 0.00001863
Iteration 110/1000 | Loss: 0.00001863
Iteration 111/1000 | Loss: 0.00001863
Iteration 112/1000 | Loss: 0.00001863
Iteration 113/1000 | Loss: 0.00001862
Iteration 114/1000 | Loss: 0.00001862
Iteration 115/1000 | Loss: 0.00007488
Iteration 116/1000 | Loss: 0.00005158
Iteration 117/1000 | Loss: 0.00005415
Iteration 118/1000 | Loss: 0.00002490
Iteration 119/1000 | Loss: 0.00007238
Iteration 120/1000 | Loss: 0.00002207
Iteration 121/1000 | Loss: 0.00002367
Iteration 122/1000 | Loss: 0.00006246
Iteration 123/1000 | Loss: 0.00001885
Iteration 124/1000 | Loss: 0.00004083
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001861
Iteration 127/1000 | Loss: 0.00001860
Iteration 128/1000 | Loss: 0.00001859
Iteration 129/1000 | Loss: 0.00001858
Iteration 130/1000 | Loss: 0.00001858
Iteration 131/1000 | Loss: 0.00001858
Iteration 132/1000 | Loss: 0.00001858
Iteration 133/1000 | Loss: 0.00001858
Iteration 134/1000 | Loss: 0.00001858
Iteration 135/1000 | Loss: 0.00001858
Iteration 136/1000 | Loss: 0.00001858
Iteration 137/1000 | Loss: 0.00001858
Iteration 138/1000 | Loss: 0.00001858
Iteration 139/1000 | Loss: 0.00001858
Iteration 140/1000 | Loss: 0.00001858
Iteration 141/1000 | Loss: 0.00001858
Iteration 142/1000 | Loss: 0.00001858
Iteration 143/1000 | Loss: 0.00001858
Iteration 144/1000 | Loss: 0.00001858
Iteration 145/1000 | Loss: 0.00001858
Iteration 146/1000 | Loss: 0.00001858
Iteration 147/1000 | Loss: 0.00001857
Iteration 148/1000 | Loss: 0.00001857
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001857
Iteration 151/1000 | Loss: 0.00001857
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001857
Iteration 154/1000 | Loss: 0.00001857
Iteration 155/1000 | Loss: 0.00001857
Iteration 156/1000 | Loss: 0.00001857
Iteration 157/1000 | Loss: 0.00001857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.8572276530903764e-05, 1.8572276530903764e-05, 1.8572276530903764e-05, 1.8572276530903764e-05, 1.8572276530903764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8572276530903764e-05

Optimization complete. Final v2v error: 3.612459421157837 mm

Highest mean error: 4.287055492401123 mm for frame 85

Lowest mean error: 3.24611496925354 mm for frame 38

Saving results

Total time: 3643.1350309848785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1065
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447068
Iteration 2/25 | Loss: 0.00141553
Iteration 3/25 | Loss: 0.00118131
Iteration 4/25 | Loss: 0.00115151
Iteration 5/25 | Loss: 0.00114693
Iteration 6/25 | Loss: 0.00114547
Iteration 7/25 | Loss: 0.00114547
Iteration 8/25 | Loss: 0.00114547
Iteration 9/25 | Loss: 0.00114547
Iteration 10/25 | Loss: 0.00114547
Iteration 11/25 | Loss: 0.00114547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011454708874225616, 0.0011454708874225616, 0.0011454708874225616, 0.0011454708874225616, 0.0011454708874225616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011454708874225616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46967614
Iteration 2/25 | Loss: 0.00074254
Iteration 3/25 | Loss: 0.00074254
Iteration 4/25 | Loss: 0.00074254
Iteration 5/25 | Loss: 0.00074254
Iteration 6/25 | Loss: 0.00074254
Iteration 7/25 | Loss: 0.00074254
Iteration 8/25 | Loss: 0.00074254
Iteration 9/25 | Loss: 0.00074254
Iteration 10/25 | Loss: 0.00074254
Iteration 11/25 | Loss: 0.00074254
Iteration 12/25 | Loss: 0.00074254
Iteration 13/25 | Loss: 0.00074254
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007425383082590997, 0.0007425383082590997, 0.0007425383082590997, 0.0007425383082590997, 0.0007425383082590997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007425383082590997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074254
Iteration 2/1000 | Loss: 0.00002638
Iteration 3/1000 | Loss: 0.00001743
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001413
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001296
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001279
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001264
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001263
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001263
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001256
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001254
Iteration 51/1000 | Loss: 0.00001254
Iteration 52/1000 | Loss: 0.00001253
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001253
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001252
Iteration 57/1000 | Loss: 0.00001252
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001250
Iteration 68/1000 | Loss: 0.00001250
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001249
Iteration 71/1000 | Loss: 0.00001249
Iteration 72/1000 | Loss: 0.00001249
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001248
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001245
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001244
Iteration 82/1000 | Loss: 0.00001244
Iteration 83/1000 | Loss: 0.00001243
Iteration 84/1000 | Loss: 0.00001243
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001243
Iteration 87/1000 | Loss: 0.00001243
Iteration 88/1000 | Loss: 0.00001242
Iteration 89/1000 | Loss: 0.00001242
Iteration 90/1000 | Loss: 0.00001242
Iteration 91/1000 | Loss: 0.00001242
Iteration 92/1000 | Loss: 0.00001242
Iteration 93/1000 | Loss: 0.00001242
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001241
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001240
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001240
Iteration 104/1000 | Loss: 0.00001240
Iteration 105/1000 | Loss: 0.00001240
Iteration 106/1000 | Loss: 0.00001239
Iteration 107/1000 | Loss: 0.00001239
Iteration 108/1000 | Loss: 0.00001239
Iteration 109/1000 | Loss: 0.00001239
Iteration 110/1000 | Loss: 0.00001239
Iteration 111/1000 | Loss: 0.00001239
Iteration 112/1000 | Loss: 0.00001239
Iteration 113/1000 | Loss: 0.00001239
Iteration 114/1000 | Loss: 0.00001239
Iteration 115/1000 | Loss: 0.00001239
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001238
Iteration 120/1000 | Loss: 0.00001238
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001238
Iteration 123/1000 | Loss: 0.00001238
Iteration 124/1000 | Loss: 0.00001238
Iteration 125/1000 | Loss: 0.00001238
Iteration 126/1000 | Loss: 0.00001238
Iteration 127/1000 | Loss: 0.00001238
Iteration 128/1000 | Loss: 0.00001238
Iteration 129/1000 | Loss: 0.00001238
Iteration 130/1000 | Loss: 0.00001237
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001237
Iteration 133/1000 | Loss: 0.00001237
Iteration 134/1000 | Loss: 0.00001237
Iteration 135/1000 | Loss: 0.00001237
Iteration 136/1000 | Loss: 0.00001237
Iteration 137/1000 | Loss: 0.00001237
Iteration 138/1000 | Loss: 0.00001237
Iteration 139/1000 | Loss: 0.00001237
Iteration 140/1000 | Loss: 0.00001236
Iteration 141/1000 | Loss: 0.00001236
Iteration 142/1000 | Loss: 0.00001236
Iteration 143/1000 | Loss: 0.00001236
Iteration 144/1000 | Loss: 0.00001236
Iteration 145/1000 | Loss: 0.00001236
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001235
Iteration 149/1000 | Loss: 0.00001235
Iteration 150/1000 | Loss: 0.00001235
Iteration 151/1000 | Loss: 0.00001235
Iteration 152/1000 | Loss: 0.00001235
Iteration 153/1000 | Loss: 0.00001235
Iteration 154/1000 | Loss: 0.00001235
Iteration 155/1000 | Loss: 0.00001235
Iteration 156/1000 | Loss: 0.00001235
Iteration 157/1000 | Loss: 0.00001235
Iteration 158/1000 | Loss: 0.00001235
Iteration 159/1000 | Loss: 0.00001235
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001234
Iteration 162/1000 | Loss: 0.00001234
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001234
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001234
Iteration 168/1000 | Loss: 0.00001233
Iteration 169/1000 | Loss: 0.00001233
Iteration 170/1000 | Loss: 0.00001233
Iteration 171/1000 | Loss: 0.00001233
Iteration 172/1000 | Loss: 0.00001233
Iteration 173/1000 | Loss: 0.00001233
Iteration 174/1000 | Loss: 0.00001233
Iteration 175/1000 | Loss: 0.00001233
Iteration 176/1000 | Loss: 0.00001233
Iteration 177/1000 | Loss: 0.00001233
Iteration 178/1000 | Loss: 0.00001233
Iteration 179/1000 | Loss: 0.00001233
Iteration 180/1000 | Loss: 0.00001233
Iteration 181/1000 | Loss: 0.00001233
Iteration 182/1000 | Loss: 0.00001233
Iteration 183/1000 | Loss: 0.00001233
Iteration 184/1000 | Loss: 0.00001233
Iteration 185/1000 | Loss: 0.00001233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.2326308024057653e-05, 1.2326308024057653e-05, 1.2326308024057653e-05, 1.2326308024057653e-05, 1.2326308024057653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2326308024057653e-05

Optimization complete. Final v2v error: 2.9549460411071777 mm

Highest mean error: 4.10432243347168 mm for frame 106

Lowest mean error: 2.5598669052124023 mm for frame 158

Saving results

Total time: 1199.088264465332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1069
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389014
Iteration 2/25 | Loss: 0.00119747
Iteration 3/25 | Loss: 0.00111404
Iteration 4/25 | Loss: 0.00110465
Iteration 5/25 | Loss: 0.00110210
Iteration 6/25 | Loss: 0.00110161
Iteration 7/25 | Loss: 0.00110161
Iteration 8/25 | Loss: 0.00110161
Iteration 9/25 | Loss: 0.00110161
Iteration 10/25 | Loss: 0.00110161
Iteration 11/25 | Loss: 0.00110161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011016104836016893, 0.0011016104836016893, 0.0011016104836016893, 0.0011016104836016893, 0.0011016104836016893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011016104836016893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37353194
Iteration 2/25 | Loss: 0.00091478
Iteration 3/25 | Loss: 0.00091477
Iteration 4/25 | Loss: 0.00091477
Iteration 5/25 | Loss: 0.00091477
Iteration 6/25 | Loss: 0.00091477
Iteration 7/25 | Loss: 0.00091477
Iteration 8/25 | Loss: 0.00091477
Iteration 9/25 | Loss: 0.00091477
Iteration 10/25 | Loss: 0.00091477
Iteration 11/25 | Loss: 0.00091477
Iteration 12/25 | Loss: 0.00091477
Iteration 13/25 | Loss: 0.00091477
Iteration 14/25 | Loss: 0.00091477
Iteration 15/25 | Loss: 0.00091477
Iteration 16/25 | Loss: 0.00091477
Iteration 17/25 | Loss: 0.00091477
Iteration 18/25 | Loss: 0.00091477
Iteration 19/25 | Loss: 0.00091477
Iteration 20/25 | Loss: 0.00091477
Iteration 21/25 | Loss: 0.00091477
Iteration 22/25 | Loss: 0.00091477
Iteration 23/25 | Loss: 0.00091477
Iteration 24/25 | Loss: 0.00091477
Iteration 25/25 | Loss: 0.00091477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091477
Iteration 2/1000 | Loss: 0.00002297
Iteration 3/1000 | Loss: 0.00001302
Iteration 4/1000 | Loss: 0.00001134
Iteration 5/1000 | Loss: 0.00001061
Iteration 6/1000 | Loss: 0.00001002
Iteration 7/1000 | Loss: 0.00000965
Iteration 8/1000 | Loss: 0.00000946
Iteration 9/1000 | Loss: 0.00000944
Iteration 10/1000 | Loss: 0.00000943
Iteration 11/1000 | Loss: 0.00000942
Iteration 12/1000 | Loss: 0.00000941
Iteration 13/1000 | Loss: 0.00000941
Iteration 14/1000 | Loss: 0.00000924
Iteration 15/1000 | Loss: 0.00000919
Iteration 16/1000 | Loss: 0.00000915
Iteration 17/1000 | Loss: 0.00000915
Iteration 18/1000 | Loss: 0.00000914
Iteration 19/1000 | Loss: 0.00000914
Iteration 20/1000 | Loss: 0.00000912
Iteration 21/1000 | Loss: 0.00000910
Iteration 22/1000 | Loss: 0.00000909
Iteration 23/1000 | Loss: 0.00000907
Iteration 24/1000 | Loss: 0.00000907
Iteration 25/1000 | Loss: 0.00000904
Iteration 26/1000 | Loss: 0.00000903
Iteration 27/1000 | Loss: 0.00000901
Iteration 28/1000 | Loss: 0.00000900
Iteration 29/1000 | Loss: 0.00000896
Iteration 30/1000 | Loss: 0.00000893
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000892
Iteration 33/1000 | Loss: 0.00000892
Iteration 34/1000 | Loss: 0.00000891
Iteration 35/1000 | Loss: 0.00000891
Iteration 36/1000 | Loss: 0.00000889
Iteration 37/1000 | Loss: 0.00000888
Iteration 38/1000 | Loss: 0.00000888
Iteration 39/1000 | Loss: 0.00000888
Iteration 40/1000 | Loss: 0.00000887
Iteration 41/1000 | Loss: 0.00000887
Iteration 42/1000 | Loss: 0.00000887
Iteration 43/1000 | Loss: 0.00000886
Iteration 44/1000 | Loss: 0.00000886
Iteration 45/1000 | Loss: 0.00000885
Iteration 46/1000 | Loss: 0.00000885
Iteration 47/1000 | Loss: 0.00000884
Iteration 48/1000 | Loss: 0.00000884
Iteration 49/1000 | Loss: 0.00000883
Iteration 50/1000 | Loss: 0.00000883
Iteration 51/1000 | Loss: 0.00000883
Iteration 52/1000 | Loss: 0.00000882
Iteration 53/1000 | Loss: 0.00000882
Iteration 54/1000 | Loss: 0.00000882
Iteration 55/1000 | Loss: 0.00000882
Iteration 56/1000 | Loss: 0.00000881
Iteration 57/1000 | Loss: 0.00000881
Iteration 58/1000 | Loss: 0.00000881
Iteration 59/1000 | Loss: 0.00000881
Iteration 60/1000 | Loss: 0.00000880
Iteration 61/1000 | Loss: 0.00000880
Iteration 62/1000 | Loss: 0.00000880
Iteration 63/1000 | Loss: 0.00000880
Iteration 64/1000 | Loss: 0.00000879
Iteration 65/1000 | Loss: 0.00000879
Iteration 66/1000 | Loss: 0.00000879
Iteration 67/1000 | Loss: 0.00000878
Iteration 68/1000 | Loss: 0.00000878
Iteration 69/1000 | Loss: 0.00000877
Iteration 70/1000 | Loss: 0.00000877
Iteration 71/1000 | Loss: 0.00000877
Iteration 72/1000 | Loss: 0.00000877
Iteration 73/1000 | Loss: 0.00000876
Iteration 74/1000 | Loss: 0.00000876
Iteration 75/1000 | Loss: 0.00000876
Iteration 76/1000 | Loss: 0.00000876
Iteration 77/1000 | Loss: 0.00000876
Iteration 78/1000 | Loss: 0.00000875
Iteration 79/1000 | Loss: 0.00000875
Iteration 80/1000 | Loss: 0.00000875
Iteration 81/1000 | Loss: 0.00000875
Iteration 82/1000 | Loss: 0.00000875
Iteration 83/1000 | Loss: 0.00000875
Iteration 84/1000 | Loss: 0.00000875
Iteration 85/1000 | Loss: 0.00000875
Iteration 86/1000 | Loss: 0.00000874
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000874
Iteration 89/1000 | Loss: 0.00000874
Iteration 90/1000 | Loss: 0.00000874
Iteration 91/1000 | Loss: 0.00000874
Iteration 92/1000 | Loss: 0.00000874
Iteration 93/1000 | Loss: 0.00000874
Iteration 94/1000 | Loss: 0.00000874
Iteration 95/1000 | Loss: 0.00000874
Iteration 96/1000 | Loss: 0.00000874
Iteration 97/1000 | Loss: 0.00000873
Iteration 98/1000 | Loss: 0.00000873
Iteration 99/1000 | Loss: 0.00000873
Iteration 100/1000 | Loss: 0.00000873
Iteration 101/1000 | Loss: 0.00000873
Iteration 102/1000 | Loss: 0.00000872
Iteration 103/1000 | Loss: 0.00000872
Iteration 104/1000 | Loss: 0.00000872
Iteration 105/1000 | Loss: 0.00000872
Iteration 106/1000 | Loss: 0.00000872
Iteration 107/1000 | Loss: 0.00000871
Iteration 108/1000 | Loss: 0.00000871
Iteration 109/1000 | Loss: 0.00000871
Iteration 110/1000 | Loss: 0.00000871
Iteration 111/1000 | Loss: 0.00000871
Iteration 112/1000 | Loss: 0.00000870
Iteration 113/1000 | Loss: 0.00000870
Iteration 114/1000 | Loss: 0.00000870
Iteration 115/1000 | Loss: 0.00000869
Iteration 116/1000 | Loss: 0.00000869
Iteration 117/1000 | Loss: 0.00000869
Iteration 118/1000 | Loss: 0.00000869
Iteration 119/1000 | Loss: 0.00000868
Iteration 120/1000 | Loss: 0.00000868
Iteration 121/1000 | Loss: 0.00000868
Iteration 122/1000 | Loss: 0.00000868
Iteration 123/1000 | Loss: 0.00000867
Iteration 124/1000 | Loss: 0.00000867
Iteration 125/1000 | Loss: 0.00000867
Iteration 126/1000 | Loss: 0.00000867
Iteration 127/1000 | Loss: 0.00000867
Iteration 128/1000 | Loss: 0.00000866
Iteration 129/1000 | Loss: 0.00000866
Iteration 130/1000 | Loss: 0.00000866
Iteration 131/1000 | Loss: 0.00000866
Iteration 132/1000 | Loss: 0.00000866
Iteration 133/1000 | Loss: 0.00000865
Iteration 134/1000 | Loss: 0.00000865
Iteration 135/1000 | Loss: 0.00000865
Iteration 136/1000 | Loss: 0.00000865
Iteration 137/1000 | Loss: 0.00000865
Iteration 138/1000 | Loss: 0.00000865
Iteration 139/1000 | Loss: 0.00000865
Iteration 140/1000 | Loss: 0.00000865
Iteration 141/1000 | Loss: 0.00000865
Iteration 142/1000 | Loss: 0.00000864
Iteration 143/1000 | Loss: 0.00000864
Iteration 144/1000 | Loss: 0.00000864
Iteration 145/1000 | Loss: 0.00000864
Iteration 146/1000 | Loss: 0.00000864
Iteration 147/1000 | Loss: 0.00000864
Iteration 148/1000 | Loss: 0.00000864
Iteration 149/1000 | Loss: 0.00000864
Iteration 150/1000 | Loss: 0.00000864
Iteration 151/1000 | Loss: 0.00000864
Iteration 152/1000 | Loss: 0.00000864
Iteration 153/1000 | Loss: 0.00000864
Iteration 154/1000 | Loss: 0.00000863
Iteration 155/1000 | Loss: 0.00000863
Iteration 156/1000 | Loss: 0.00000863
Iteration 157/1000 | Loss: 0.00000863
Iteration 158/1000 | Loss: 0.00000863
Iteration 159/1000 | Loss: 0.00000862
Iteration 160/1000 | Loss: 0.00000862
Iteration 161/1000 | Loss: 0.00000862
Iteration 162/1000 | Loss: 0.00000862
Iteration 163/1000 | Loss: 0.00000861
Iteration 164/1000 | Loss: 0.00000861
Iteration 165/1000 | Loss: 0.00000861
Iteration 166/1000 | Loss: 0.00000861
Iteration 167/1000 | Loss: 0.00000861
Iteration 168/1000 | Loss: 0.00000861
Iteration 169/1000 | Loss: 0.00000861
Iteration 170/1000 | Loss: 0.00000861
Iteration 171/1000 | Loss: 0.00000861
Iteration 172/1000 | Loss: 0.00000861
Iteration 173/1000 | Loss: 0.00000860
Iteration 174/1000 | Loss: 0.00000860
Iteration 175/1000 | Loss: 0.00000860
Iteration 176/1000 | Loss: 0.00000860
Iteration 177/1000 | Loss: 0.00000860
Iteration 178/1000 | Loss: 0.00000860
Iteration 179/1000 | Loss: 0.00000860
Iteration 180/1000 | Loss: 0.00000860
Iteration 181/1000 | Loss: 0.00000860
Iteration 182/1000 | Loss: 0.00000860
Iteration 183/1000 | Loss: 0.00000860
Iteration 184/1000 | Loss: 0.00000860
Iteration 185/1000 | Loss: 0.00000860
Iteration 186/1000 | Loss: 0.00000860
Iteration 187/1000 | Loss: 0.00000859
Iteration 188/1000 | Loss: 0.00000859
Iteration 189/1000 | Loss: 0.00000859
Iteration 190/1000 | Loss: 0.00000859
Iteration 191/1000 | Loss: 0.00000859
Iteration 192/1000 | Loss: 0.00000859
Iteration 193/1000 | Loss: 0.00000859
Iteration 194/1000 | Loss: 0.00000859
Iteration 195/1000 | Loss: 0.00000859
Iteration 196/1000 | Loss: 0.00000859
Iteration 197/1000 | Loss: 0.00000859
Iteration 198/1000 | Loss: 0.00000859
Iteration 199/1000 | Loss: 0.00000859
Iteration 200/1000 | Loss: 0.00000859
Iteration 201/1000 | Loss: 0.00000859
Iteration 202/1000 | Loss: 0.00000859
Iteration 203/1000 | Loss: 0.00000858
Iteration 204/1000 | Loss: 0.00000858
Iteration 205/1000 | Loss: 0.00000858
Iteration 206/1000 | Loss: 0.00000858
Iteration 207/1000 | Loss: 0.00000858
Iteration 208/1000 | Loss: 0.00000858
Iteration 209/1000 | Loss: 0.00000858
Iteration 210/1000 | Loss: 0.00000858
Iteration 211/1000 | Loss: 0.00000858
Iteration 212/1000 | Loss: 0.00000858
Iteration 213/1000 | Loss: 0.00000858
Iteration 214/1000 | Loss: 0.00000858
Iteration 215/1000 | Loss: 0.00000858
Iteration 216/1000 | Loss: 0.00000858
Iteration 217/1000 | Loss: 0.00000858
Iteration 218/1000 | Loss: 0.00000857
Iteration 219/1000 | Loss: 0.00000857
Iteration 220/1000 | Loss: 0.00000857
Iteration 221/1000 | Loss: 0.00000857
Iteration 222/1000 | Loss: 0.00000857
Iteration 223/1000 | Loss: 0.00000857
Iteration 224/1000 | Loss: 0.00000857
Iteration 225/1000 | Loss: 0.00000857
Iteration 226/1000 | Loss: 0.00000857
Iteration 227/1000 | Loss: 0.00000857
Iteration 228/1000 | Loss: 0.00000857
Iteration 229/1000 | Loss: 0.00000857
Iteration 230/1000 | Loss: 0.00000857
Iteration 231/1000 | Loss: 0.00000857
Iteration 232/1000 | Loss: 0.00000857
Iteration 233/1000 | Loss: 0.00000857
Iteration 234/1000 | Loss: 0.00000857
Iteration 235/1000 | Loss: 0.00000857
Iteration 236/1000 | Loss: 0.00000857
Iteration 237/1000 | Loss: 0.00000857
Iteration 238/1000 | Loss: 0.00000857
Iteration 239/1000 | Loss: 0.00000857
Iteration 240/1000 | Loss: 0.00000857
Iteration 241/1000 | Loss: 0.00000857
Iteration 242/1000 | Loss: 0.00000857
Iteration 243/1000 | Loss: 0.00000857
Iteration 244/1000 | Loss: 0.00000856
Iteration 245/1000 | Loss: 0.00000856
Iteration 246/1000 | Loss: 0.00000856
Iteration 247/1000 | Loss: 0.00000856
Iteration 248/1000 | Loss: 0.00000856
Iteration 249/1000 | Loss: 0.00000856
Iteration 250/1000 | Loss: 0.00000856
Iteration 251/1000 | Loss: 0.00000856
Iteration 252/1000 | Loss: 0.00000856
Iteration 253/1000 | Loss: 0.00000856
Iteration 254/1000 | Loss: 0.00000856
Iteration 255/1000 | Loss: 0.00000856
Iteration 256/1000 | Loss: 0.00000856
Iteration 257/1000 | Loss: 0.00000856
Iteration 258/1000 | Loss: 0.00000856
Iteration 259/1000 | Loss: 0.00000856
Iteration 260/1000 | Loss: 0.00000856
Iteration 261/1000 | Loss: 0.00000856
Iteration 262/1000 | Loss: 0.00000856
Iteration 263/1000 | Loss: 0.00000856
Iteration 264/1000 | Loss: 0.00000856
Iteration 265/1000 | Loss: 0.00000856
Iteration 266/1000 | Loss: 0.00000855
Iteration 267/1000 | Loss: 0.00000855
Iteration 268/1000 | Loss: 0.00000855
Iteration 269/1000 | Loss: 0.00000855
Iteration 270/1000 | Loss: 0.00000855
Iteration 271/1000 | Loss: 0.00000855
Iteration 272/1000 | Loss: 0.00000855
Iteration 273/1000 | Loss: 0.00000855
Iteration 274/1000 | Loss: 0.00000855
Iteration 275/1000 | Loss: 0.00000855
Iteration 276/1000 | Loss: 0.00000855
Iteration 277/1000 | Loss: 0.00000855
Iteration 278/1000 | Loss: 0.00000855
Iteration 279/1000 | Loss: 0.00000854
Iteration 280/1000 | Loss: 0.00000854
Iteration 281/1000 | Loss: 0.00000854
Iteration 282/1000 | Loss: 0.00000854
Iteration 283/1000 | Loss: 0.00000854
Iteration 284/1000 | Loss: 0.00000854
Iteration 285/1000 | Loss: 0.00000854
Iteration 286/1000 | Loss: 0.00000854
Iteration 287/1000 | Loss: 0.00000854
Iteration 288/1000 | Loss: 0.00000854
Iteration 289/1000 | Loss: 0.00000854
Iteration 290/1000 | Loss: 0.00000854
Iteration 291/1000 | Loss: 0.00000853
Iteration 292/1000 | Loss: 0.00000853
Iteration 293/1000 | Loss: 0.00000853
Iteration 294/1000 | Loss: 0.00000853
Iteration 295/1000 | Loss: 0.00000853
Iteration 296/1000 | Loss: 0.00000853
Iteration 297/1000 | Loss: 0.00000853
Iteration 298/1000 | Loss: 0.00000853
Iteration 299/1000 | Loss: 0.00000853
Iteration 300/1000 | Loss: 0.00000853
Iteration 301/1000 | Loss: 0.00000853
Iteration 302/1000 | Loss: 0.00000852
Iteration 303/1000 | Loss: 0.00000852
Iteration 304/1000 | Loss: 0.00000852
Iteration 305/1000 | Loss: 0.00000852
Iteration 306/1000 | Loss: 0.00000852
Iteration 307/1000 | Loss: 0.00000852
Iteration 308/1000 | Loss: 0.00000852
Iteration 309/1000 | Loss: 0.00000852
Iteration 310/1000 | Loss: 0.00000852
Iteration 311/1000 | Loss: 0.00000852
Iteration 312/1000 | Loss: 0.00000852
Iteration 313/1000 | Loss: 0.00000852
Iteration 314/1000 | Loss: 0.00000852
Iteration 315/1000 | Loss: 0.00000852
Iteration 316/1000 | Loss: 0.00000852
Iteration 317/1000 | Loss: 0.00000852
Iteration 318/1000 | Loss: 0.00000852
Iteration 319/1000 | Loss: 0.00000852
Iteration 320/1000 | Loss: 0.00000852
Iteration 321/1000 | Loss: 0.00000851
Iteration 322/1000 | Loss: 0.00000851
Iteration 323/1000 | Loss: 0.00000851
Iteration 324/1000 | Loss: 0.00000851
Iteration 325/1000 | Loss: 0.00000851
Iteration 326/1000 | Loss: 0.00000851
Iteration 327/1000 | Loss: 0.00000851
Iteration 328/1000 | Loss: 0.00000851
Iteration 329/1000 | Loss: 0.00000851
Iteration 330/1000 | Loss: 0.00000851
Iteration 331/1000 | Loss: 0.00000851
Iteration 332/1000 | Loss: 0.00000851
Iteration 333/1000 | Loss: 0.00000851
Iteration 334/1000 | Loss: 0.00000850
Iteration 335/1000 | Loss: 0.00000850
Iteration 336/1000 | Loss: 0.00000850
Iteration 337/1000 | Loss: 0.00000850
Iteration 338/1000 | Loss: 0.00000850
Iteration 339/1000 | Loss: 0.00000850
Iteration 340/1000 | Loss: 0.00000850
Iteration 341/1000 | Loss: 0.00000850
Iteration 342/1000 | Loss: 0.00000850
Iteration 343/1000 | Loss: 0.00000850
Iteration 344/1000 | Loss: 0.00000850
Iteration 345/1000 | Loss: 0.00000850
Iteration 346/1000 | Loss: 0.00000850
Iteration 347/1000 | Loss: 0.00000850
Iteration 348/1000 | Loss: 0.00000850
Iteration 349/1000 | Loss: 0.00000850
Iteration 350/1000 | Loss: 0.00000850
Iteration 351/1000 | Loss: 0.00000850
Iteration 352/1000 | Loss: 0.00000850
Iteration 353/1000 | Loss: 0.00000850
Iteration 354/1000 | Loss: 0.00000850
Iteration 355/1000 | Loss: 0.00000850
Iteration 356/1000 | Loss: 0.00000850
Iteration 357/1000 | Loss: 0.00000850
Iteration 358/1000 | Loss: 0.00000850
Iteration 359/1000 | Loss: 0.00000850
Iteration 360/1000 | Loss: 0.00000850
Iteration 361/1000 | Loss: 0.00000850
Iteration 362/1000 | Loss: 0.00000850
Iteration 363/1000 | Loss: 0.00000850
Iteration 364/1000 | Loss: 0.00000850
Iteration 365/1000 | Loss: 0.00000850
Iteration 366/1000 | Loss: 0.00000850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 366. Stopping optimization.
Last 5 losses: [8.499479918100405e-06, 8.499479918100405e-06, 8.499479918100405e-06, 8.499479918100405e-06, 8.499479918100405e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.499479918100405e-06

Optimization complete. Final v2v error: 2.4890494346618652 mm

Highest mean error: 3.3937463760375977 mm for frame 64

Lowest mean error: 2.327911853790283 mm for frame 148

Saving results

Total time: 1063.0368382930756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1021
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00345338
Iteration 2/25 | Loss: 0.00123431
Iteration 3/25 | Loss: 0.00112567
Iteration 4/25 | Loss: 0.00111265
Iteration 5/25 | Loss: 0.00110812
Iteration 6/25 | Loss: 0.00110695
Iteration 7/25 | Loss: 0.00110651
Iteration 8/25 | Loss: 0.00110632
Iteration 9/25 | Loss: 0.00110632
Iteration 10/25 | Loss: 0.00110632
Iteration 11/25 | Loss: 0.00110632
Iteration 12/25 | Loss: 0.00110632
Iteration 13/25 | Loss: 0.00110632
Iteration 14/25 | Loss: 0.00110632
Iteration 15/25 | Loss: 0.00110632
Iteration 16/25 | Loss: 0.00110632
Iteration 17/25 | Loss: 0.00110632
Iteration 18/25 | Loss: 0.00110632
Iteration 19/25 | Loss: 0.00110632
Iteration 20/25 | Loss: 0.00110632
Iteration 21/25 | Loss: 0.00110632
Iteration 22/25 | Loss: 0.00110632
Iteration 23/25 | Loss: 0.00110632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011063235579058528, 0.0011063235579058528, 0.0011063235579058528, 0.0011063235579058528, 0.0011063235579058528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011063235579058528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42878926
Iteration 2/25 | Loss: 0.00100070
Iteration 3/25 | Loss: 0.00100070
Iteration 4/25 | Loss: 0.00100069
Iteration 5/25 | Loss: 0.00100069
Iteration 6/25 | Loss: 0.00100069
Iteration 7/25 | Loss: 0.00100069
Iteration 8/25 | Loss: 0.00100069
Iteration 9/25 | Loss: 0.00100069
Iteration 10/25 | Loss: 0.00100069
Iteration 11/25 | Loss: 0.00100069
Iteration 12/25 | Loss: 0.00100069
Iteration 13/25 | Loss: 0.00100069
Iteration 14/25 | Loss: 0.00100069
Iteration 15/25 | Loss: 0.00100069
Iteration 16/25 | Loss: 0.00100069
Iteration 17/25 | Loss: 0.00100069
Iteration 18/25 | Loss: 0.00100069
Iteration 19/25 | Loss: 0.00100069
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010006931843236089, 0.0010006931843236089, 0.0010006931843236089, 0.0010006931843236089, 0.0010006931843236089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010006931843236089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100069
Iteration 2/1000 | Loss: 0.00003635
Iteration 3/1000 | Loss: 0.00002320
Iteration 4/1000 | Loss: 0.00001684
Iteration 5/1000 | Loss: 0.00001532
Iteration 6/1000 | Loss: 0.00001431
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001306
Iteration 9/1000 | Loss: 0.00001259
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001204
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001185
Iteration 14/1000 | Loss: 0.00001180
Iteration 15/1000 | Loss: 0.00001165
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001155
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001153
Iteration 22/1000 | Loss: 0.00001152
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001148
Iteration 28/1000 | Loss: 0.00001147
Iteration 29/1000 | Loss: 0.00001147
Iteration 30/1000 | Loss: 0.00001146
Iteration 31/1000 | Loss: 0.00001145
Iteration 32/1000 | Loss: 0.00001145
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001143
Iteration 37/1000 | Loss: 0.00001143
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001142
Iteration 40/1000 | Loss: 0.00001139
Iteration 41/1000 | Loss: 0.00001139
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001136
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001135
Iteration 51/1000 | Loss: 0.00001135
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001134
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001133
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001133
Iteration 59/1000 | Loss: 0.00001133
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001131
Iteration 68/1000 | Loss: 0.00001131
Iteration 69/1000 | Loss: 0.00001131
Iteration 70/1000 | Loss: 0.00001130
Iteration 71/1000 | Loss: 0.00001130
Iteration 72/1000 | Loss: 0.00001130
Iteration 73/1000 | Loss: 0.00001130
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001129
Iteration 76/1000 | Loss: 0.00001129
Iteration 77/1000 | Loss: 0.00001129
Iteration 78/1000 | Loss: 0.00001129
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001127
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001125
Iteration 101/1000 | Loss: 0.00001125
Iteration 102/1000 | Loss: 0.00001125
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001125
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001124
Iteration 108/1000 | Loss: 0.00001124
Iteration 109/1000 | Loss: 0.00001124
Iteration 110/1000 | Loss: 0.00001124
Iteration 111/1000 | Loss: 0.00001124
Iteration 112/1000 | Loss: 0.00001123
Iteration 113/1000 | Loss: 0.00001123
Iteration 114/1000 | Loss: 0.00001123
Iteration 115/1000 | Loss: 0.00001123
Iteration 116/1000 | Loss: 0.00001123
Iteration 117/1000 | Loss: 0.00001123
Iteration 118/1000 | Loss: 0.00001122
Iteration 119/1000 | Loss: 0.00001122
Iteration 120/1000 | Loss: 0.00001122
Iteration 121/1000 | Loss: 0.00001122
Iteration 122/1000 | Loss: 0.00001122
Iteration 123/1000 | Loss: 0.00001122
Iteration 124/1000 | Loss: 0.00001122
Iteration 125/1000 | Loss: 0.00001121
Iteration 126/1000 | Loss: 0.00001121
Iteration 127/1000 | Loss: 0.00001121
Iteration 128/1000 | Loss: 0.00001121
Iteration 129/1000 | Loss: 0.00001121
Iteration 130/1000 | Loss: 0.00001121
Iteration 131/1000 | Loss: 0.00001121
Iteration 132/1000 | Loss: 0.00001121
Iteration 133/1000 | Loss: 0.00001121
Iteration 134/1000 | Loss: 0.00001121
Iteration 135/1000 | Loss: 0.00001121
Iteration 136/1000 | Loss: 0.00001121
Iteration 137/1000 | Loss: 0.00001121
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Iteration 142/1000 | Loss: 0.00001120
Iteration 143/1000 | Loss: 0.00001120
Iteration 144/1000 | Loss: 0.00001120
Iteration 145/1000 | Loss: 0.00001120
Iteration 146/1000 | Loss: 0.00001120
Iteration 147/1000 | Loss: 0.00001119
Iteration 148/1000 | Loss: 0.00001119
Iteration 149/1000 | Loss: 0.00001119
Iteration 150/1000 | Loss: 0.00001119
Iteration 151/1000 | Loss: 0.00001119
Iteration 152/1000 | Loss: 0.00001119
Iteration 153/1000 | Loss: 0.00001119
Iteration 154/1000 | Loss: 0.00001119
Iteration 155/1000 | Loss: 0.00001119
Iteration 156/1000 | Loss: 0.00001119
Iteration 157/1000 | Loss: 0.00001119
Iteration 158/1000 | Loss: 0.00001119
Iteration 159/1000 | Loss: 0.00001119
Iteration 160/1000 | Loss: 0.00001119
Iteration 161/1000 | Loss: 0.00001119
Iteration 162/1000 | Loss: 0.00001119
Iteration 163/1000 | Loss: 0.00001118
Iteration 164/1000 | Loss: 0.00001118
Iteration 165/1000 | Loss: 0.00001118
Iteration 166/1000 | Loss: 0.00001118
Iteration 167/1000 | Loss: 0.00001118
Iteration 168/1000 | Loss: 0.00001118
Iteration 169/1000 | Loss: 0.00001118
Iteration 170/1000 | Loss: 0.00001118
Iteration 171/1000 | Loss: 0.00001118
Iteration 172/1000 | Loss: 0.00001118
Iteration 173/1000 | Loss: 0.00001118
Iteration 174/1000 | Loss: 0.00001118
Iteration 175/1000 | Loss: 0.00001118
Iteration 176/1000 | Loss: 0.00001118
Iteration 177/1000 | Loss: 0.00001118
Iteration 178/1000 | Loss: 0.00001117
Iteration 179/1000 | Loss: 0.00001117
Iteration 180/1000 | Loss: 0.00001117
Iteration 181/1000 | Loss: 0.00001117
Iteration 182/1000 | Loss: 0.00001117
Iteration 183/1000 | Loss: 0.00001117
Iteration 184/1000 | Loss: 0.00001117
Iteration 185/1000 | Loss: 0.00001117
Iteration 186/1000 | Loss: 0.00001117
Iteration 187/1000 | Loss: 0.00001117
Iteration 188/1000 | Loss: 0.00001117
Iteration 189/1000 | Loss: 0.00001117
Iteration 190/1000 | Loss: 0.00001117
Iteration 191/1000 | Loss: 0.00001116
Iteration 192/1000 | Loss: 0.00001116
Iteration 193/1000 | Loss: 0.00001116
Iteration 194/1000 | Loss: 0.00001116
Iteration 195/1000 | Loss: 0.00001116
Iteration 196/1000 | Loss: 0.00001116
Iteration 197/1000 | Loss: 0.00001116
Iteration 198/1000 | Loss: 0.00001116
Iteration 199/1000 | Loss: 0.00001116
Iteration 200/1000 | Loss: 0.00001116
Iteration 201/1000 | Loss: 0.00001116
Iteration 202/1000 | Loss: 0.00001116
Iteration 203/1000 | Loss: 0.00001116
Iteration 204/1000 | Loss: 0.00001116
Iteration 205/1000 | Loss: 0.00001116
Iteration 206/1000 | Loss: 0.00001116
Iteration 207/1000 | Loss: 0.00001115
Iteration 208/1000 | Loss: 0.00001115
Iteration 209/1000 | Loss: 0.00001115
Iteration 210/1000 | Loss: 0.00001115
Iteration 211/1000 | Loss: 0.00001115
Iteration 212/1000 | Loss: 0.00001115
Iteration 213/1000 | Loss: 0.00001115
Iteration 214/1000 | Loss: 0.00001115
Iteration 215/1000 | Loss: 0.00001115
Iteration 216/1000 | Loss: 0.00001115
Iteration 217/1000 | Loss: 0.00001115
Iteration 218/1000 | Loss: 0.00001114
Iteration 219/1000 | Loss: 0.00001114
Iteration 220/1000 | Loss: 0.00001114
Iteration 221/1000 | Loss: 0.00001114
Iteration 222/1000 | Loss: 0.00001114
Iteration 223/1000 | Loss: 0.00001114
Iteration 224/1000 | Loss: 0.00001114
Iteration 225/1000 | Loss: 0.00001114
Iteration 226/1000 | Loss: 0.00001114
Iteration 227/1000 | Loss: 0.00001114
Iteration 228/1000 | Loss: 0.00001114
Iteration 229/1000 | Loss: 0.00001114
Iteration 230/1000 | Loss: 0.00001114
Iteration 231/1000 | Loss: 0.00001114
Iteration 232/1000 | Loss: 0.00001114
Iteration 233/1000 | Loss: 0.00001114
Iteration 234/1000 | Loss: 0.00001114
Iteration 235/1000 | Loss: 0.00001114
Iteration 236/1000 | Loss: 0.00001114
Iteration 237/1000 | Loss: 0.00001114
Iteration 238/1000 | Loss: 0.00001114
Iteration 239/1000 | Loss: 0.00001114
Iteration 240/1000 | Loss: 0.00001114
Iteration 241/1000 | Loss: 0.00001114
Iteration 242/1000 | Loss: 0.00001114
Iteration 243/1000 | Loss: 0.00001114
Iteration 244/1000 | Loss: 0.00001114
Iteration 245/1000 | Loss: 0.00001114
Iteration 246/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.1137773071823176e-05, 1.1137773071823176e-05, 1.1137773071823176e-05, 1.1137773071823176e-05, 1.1137773071823176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1137773071823176e-05

Optimization complete. Final v2v error: 2.8672573566436768 mm

Highest mean error: 3.061284065246582 mm for frame 104

Lowest mean error: 2.713942289352417 mm for frame 120

Saving results

Total time: 963.1327121257782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1027
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080603
Iteration 2/25 | Loss: 0.00231633
Iteration 3/25 | Loss: 0.00167700
Iteration 4/25 | Loss: 0.00170118
Iteration 5/25 | Loss: 0.00169587
Iteration 6/25 | Loss: 0.00174334
Iteration 7/25 | Loss: 0.00170317
Iteration 8/25 | Loss: 0.00160426
Iteration 9/25 | Loss: 0.00176554
Iteration 10/25 | Loss: 0.00170548
Iteration 11/25 | Loss: 0.00168997
Iteration 12/25 | Loss: 0.00161082
Iteration 13/25 | Loss: 0.00155090
Iteration 14/25 | Loss: 0.00150403
Iteration 15/25 | Loss: 0.00153420
Iteration 16/25 | Loss: 0.00147786
Iteration 17/25 | Loss: 0.00145127
Iteration 18/25 | Loss: 0.00141899
Iteration 19/25 | Loss: 0.00137712
Iteration 20/25 | Loss: 0.00136577
Iteration 21/25 | Loss: 0.00135894
Iteration 22/25 | Loss: 0.00132369
Iteration 23/25 | Loss: 0.00130398
Iteration 24/25 | Loss: 0.00128303
Iteration 25/25 | Loss: 0.00128323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23179877
Iteration 2/25 | Loss: 0.00217809
Iteration 3/25 | Loss: 0.00180471
Iteration 4/25 | Loss: 0.00180471
Iteration 5/25 | Loss: 0.00180471
Iteration 6/25 | Loss: 0.00180471
Iteration 7/25 | Loss: 0.00180471
Iteration 8/25 | Loss: 0.00180471
Iteration 9/25 | Loss: 0.00180471
Iteration 10/25 | Loss: 0.00180471
Iteration 11/25 | Loss: 0.00180471
Iteration 12/25 | Loss: 0.00180471
Iteration 13/25 | Loss: 0.00180471
Iteration 14/25 | Loss: 0.00180471
Iteration 15/25 | Loss: 0.00180471
Iteration 16/25 | Loss: 0.00180471
Iteration 17/25 | Loss: 0.00180471
Iteration 18/25 | Loss: 0.00180471
Iteration 19/25 | Loss: 0.00180471
Iteration 20/25 | Loss: 0.00180471
Iteration 21/25 | Loss: 0.00180471
Iteration 22/25 | Loss: 0.00180471
Iteration 23/25 | Loss: 0.00180471
Iteration 24/25 | Loss: 0.00180471
Iteration 25/25 | Loss: 0.00180471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180471
Iteration 2/1000 | Loss: 0.00076743
Iteration 3/1000 | Loss: 0.00009157
Iteration 4/1000 | Loss: 0.00066473
Iteration 5/1000 | Loss: 0.00028417
Iteration 6/1000 | Loss: 0.00023319
Iteration 7/1000 | Loss: 0.00031671
Iteration 8/1000 | Loss: 0.00072642
Iteration 9/1000 | Loss: 0.00045486
Iteration 10/1000 | Loss: 0.00008895
Iteration 11/1000 | Loss: 0.00021608
Iteration 12/1000 | Loss: 0.00018242
Iteration 13/1000 | Loss: 0.00020584
Iteration 14/1000 | Loss: 0.00033957
Iteration 15/1000 | Loss: 0.00018494
Iteration 16/1000 | Loss: 0.00009365
Iteration 17/1000 | Loss: 0.00018914
Iteration 18/1000 | Loss: 0.00018354
Iteration 19/1000 | Loss: 0.00021542
Iteration 20/1000 | Loss: 0.00066913
Iteration 21/1000 | Loss: 0.00047349
Iteration 22/1000 | Loss: 0.00047954
Iteration 23/1000 | Loss: 0.00108895
Iteration 24/1000 | Loss: 0.00105887
Iteration 25/1000 | Loss: 0.00046675
Iteration 26/1000 | Loss: 0.00047815
Iteration 27/1000 | Loss: 0.00061781
Iteration 28/1000 | Loss: 0.00081160
Iteration 29/1000 | Loss: 0.00060972
Iteration 30/1000 | Loss: 0.00037129
Iteration 31/1000 | Loss: 0.00038844
Iteration 32/1000 | Loss: 0.00036899
Iteration 33/1000 | Loss: 0.00033934
Iteration 34/1000 | Loss: 0.00033893
Iteration 35/1000 | Loss: 0.00095168
Iteration 36/1000 | Loss: 0.00080256
Iteration 37/1000 | Loss: 0.00018394
Iteration 38/1000 | Loss: 0.00012324
Iteration 39/1000 | Loss: 0.00022669
Iteration 40/1000 | Loss: 0.00023059
Iteration 41/1000 | Loss: 0.00131703
Iteration 42/1000 | Loss: 0.00009527
Iteration 43/1000 | Loss: 0.00006993
Iteration 44/1000 | Loss: 0.00006077
Iteration 45/1000 | Loss: 0.00006635
Iteration 46/1000 | Loss: 0.00009238
Iteration 47/1000 | Loss: 0.00006757
Iteration 48/1000 | Loss: 0.00005791
Iteration 49/1000 | Loss: 0.00023765
Iteration 50/1000 | Loss: 0.00019633
Iteration 51/1000 | Loss: 0.00016028
Iteration 52/1000 | Loss: 0.00021246
Iteration 53/1000 | Loss: 0.00077757
Iteration 54/1000 | Loss: 0.00073773
Iteration 55/1000 | Loss: 0.00115500
Iteration 56/1000 | Loss: 0.00068616
Iteration 57/1000 | Loss: 0.00043467
Iteration 58/1000 | Loss: 0.00043233
Iteration 59/1000 | Loss: 0.00004820
Iteration 60/1000 | Loss: 0.00006410
Iteration 61/1000 | Loss: 0.00004490
Iteration 62/1000 | Loss: 0.00003638
Iteration 63/1000 | Loss: 0.00006321
Iteration 64/1000 | Loss: 0.00003879
Iteration 65/1000 | Loss: 0.00041193
Iteration 66/1000 | Loss: 0.00050956
Iteration 67/1000 | Loss: 0.00006205
Iteration 68/1000 | Loss: 0.00004440
Iteration 69/1000 | Loss: 0.00003547
Iteration 70/1000 | Loss: 0.00039384
Iteration 71/1000 | Loss: 0.00032178
Iteration 72/1000 | Loss: 0.00048582
Iteration 73/1000 | Loss: 0.00043318
Iteration 74/1000 | Loss: 0.00004970
Iteration 75/1000 | Loss: 0.00020621
Iteration 76/1000 | Loss: 0.00005661
Iteration 77/1000 | Loss: 0.00003565
Iteration 78/1000 | Loss: 0.00005294
Iteration 79/1000 | Loss: 0.00003589
Iteration 80/1000 | Loss: 0.00004436
Iteration 81/1000 | Loss: 0.00036889
Iteration 82/1000 | Loss: 0.00026549
Iteration 83/1000 | Loss: 0.00005584
Iteration 84/1000 | Loss: 0.00026287
Iteration 85/1000 | Loss: 0.00034711
Iteration 86/1000 | Loss: 0.00005646
Iteration 87/1000 | Loss: 0.00020749
Iteration 88/1000 | Loss: 0.00008279
Iteration 89/1000 | Loss: 0.00008003
Iteration 90/1000 | Loss: 0.00006503
Iteration 91/1000 | Loss: 0.00006876
Iteration 92/1000 | Loss: 0.00002809
Iteration 93/1000 | Loss: 0.00028559
Iteration 94/1000 | Loss: 0.00023126
Iteration 95/1000 | Loss: 0.00042393
Iteration 96/1000 | Loss: 0.00018705
Iteration 97/1000 | Loss: 0.00028810
Iteration 98/1000 | Loss: 0.00028189
Iteration 99/1000 | Loss: 0.00034679
Iteration 100/1000 | Loss: 0.00009077
Iteration 101/1000 | Loss: 0.00008366
Iteration 102/1000 | Loss: 0.00005214
Iteration 103/1000 | Loss: 0.00005599
Iteration 104/1000 | Loss: 0.00005353
Iteration 105/1000 | Loss: 0.00006213
Iteration 106/1000 | Loss: 0.00005375
Iteration 107/1000 | Loss: 0.00031452
Iteration 108/1000 | Loss: 0.00026016
Iteration 109/1000 | Loss: 0.00021779
Iteration 110/1000 | Loss: 0.00021784
Iteration 111/1000 | Loss: 0.00021850
Iteration 112/1000 | Loss: 0.00020666
Iteration 113/1000 | Loss: 0.00020641
Iteration 114/1000 | Loss: 0.00007139
Iteration 115/1000 | Loss: 0.00011377
Iteration 116/1000 | Loss: 0.00006798
Iteration 117/1000 | Loss: 0.00004980
Iteration 118/1000 | Loss: 0.00004231
Iteration 119/1000 | Loss: 0.00031936
Iteration 120/1000 | Loss: 0.00003589
Iteration 121/1000 | Loss: 0.00003077
Iteration 122/1000 | Loss: 0.00002757
Iteration 123/1000 | Loss: 0.00004092
Iteration 124/1000 | Loss: 0.00022768
Iteration 125/1000 | Loss: 0.00015788
Iteration 126/1000 | Loss: 0.00035479
Iteration 127/1000 | Loss: 0.00035587
Iteration 128/1000 | Loss: 0.00011555
Iteration 129/1000 | Loss: 0.00003321
Iteration 130/1000 | Loss: 0.00012219
Iteration 131/1000 | Loss: 0.00002243
Iteration 132/1000 | Loss: 0.00001985
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001740
Iteration 136/1000 | Loss: 0.00001719
Iteration 137/1000 | Loss: 0.00001693
Iteration 138/1000 | Loss: 0.00001674
Iteration 139/1000 | Loss: 0.00001657
Iteration 140/1000 | Loss: 0.00001654
Iteration 141/1000 | Loss: 0.00001654
Iteration 142/1000 | Loss: 0.00001637
Iteration 143/1000 | Loss: 0.00001636
Iteration 144/1000 | Loss: 0.00001635
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001634
Iteration 148/1000 | Loss: 0.00001630
Iteration 149/1000 | Loss: 0.00001627
Iteration 150/1000 | Loss: 0.00001627
Iteration 151/1000 | Loss: 0.00001627
Iteration 152/1000 | Loss: 0.00001626
Iteration 153/1000 | Loss: 0.00001626
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001625
Iteration 157/1000 | Loss: 0.00001624
Iteration 158/1000 | Loss: 0.00001624
Iteration 159/1000 | Loss: 0.00001624
Iteration 160/1000 | Loss: 0.00001624
Iteration 161/1000 | Loss: 0.00001624
Iteration 162/1000 | Loss: 0.00001624
Iteration 163/1000 | Loss: 0.00001624
Iteration 164/1000 | Loss: 0.00001624
Iteration 165/1000 | Loss: 0.00001624
Iteration 166/1000 | Loss: 0.00001624
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001623
Iteration 169/1000 | Loss: 0.00001622
Iteration 170/1000 | Loss: 0.00001620
Iteration 171/1000 | Loss: 0.00001620
Iteration 172/1000 | Loss: 0.00001619
Iteration 173/1000 | Loss: 0.00001619
Iteration 174/1000 | Loss: 0.00001619
Iteration 175/1000 | Loss: 0.00001619
Iteration 176/1000 | Loss: 0.00001618
Iteration 177/1000 | Loss: 0.00001617
Iteration 178/1000 | Loss: 0.00001617
Iteration 179/1000 | Loss: 0.00001617
Iteration 180/1000 | Loss: 0.00001616
Iteration 181/1000 | Loss: 0.00001616
Iteration 182/1000 | Loss: 0.00001616
Iteration 183/1000 | Loss: 0.00001615
Iteration 184/1000 | Loss: 0.00001615
Iteration 185/1000 | Loss: 0.00001615
Iteration 186/1000 | Loss: 0.00001615
Iteration 187/1000 | Loss: 0.00001615
Iteration 188/1000 | Loss: 0.00001615
Iteration 189/1000 | Loss: 0.00001615
Iteration 190/1000 | Loss: 0.00001614
Iteration 191/1000 | Loss: 0.00001614
Iteration 192/1000 | Loss: 0.00001614
Iteration 193/1000 | Loss: 0.00001614
Iteration 194/1000 | Loss: 0.00001614
Iteration 195/1000 | Loss: 0.00001613
Iteration 196/1000 | Loss: 0.00001613
Iteration 197/1000 | Loss: 0.00001613
Iteration 198/1000 | Loss: 0.00001612
Iteration 199/1000 | Loss: 0.00001612
Iteration 200/1000 | Loss: 0.00001612
Iteration 201/1000 | Loss: 0.00001612
Iteration 202/1000 | Loss: 0.00001612
Iteration 203/1000 | Loss: 0.00001612
Iteration 204/1000 | Loss: 0.00001611
Iteration 205/1000 | Loss: 0.00001611
Iteration 206/1000 | Loss: 0.00001611
Iteration 207/1000 | Loss: 0.00001611
Iteration 208/1000 | Loss: 0.00001610
Iteration 209/1000 | Loss: 0.00001609
Iteration 210/1000 | Loss: 0.00001609
Iteration 211/1000 | Loss: 0.00001609
Iteration 212/1000 | Loss: 0.00001609
Iteration 213/1000 | Loss: 0.00001608
Iteration 214/1000 | Loss: 0.00001608
Iteration 215/1000 | Loss: 0.00001608
Iteration 216/1000 | Loss: 0.00001608
Iteration 217/1000 | Loss: 0.00001608
Iteration 218/1000 | Loss: 0.00001608
Iteration 219/1000 | Loss: 0.00001608
Iteration 220/1000 | Loss: 0.00001607
Iteration 221/1000 | Loss: 0.00001607
Iteration 222/1000 | Loss: 0.00001607
Iteration 223/1000 | Loss: 0.00001607
Iteration 224/1000 | Loss: 0.00001606
Iteration 225/1000 | Loss: 0.00001606
Iteration 226/1000 | Loss: 0.00001606
Iteration 227/1000 | Loss: 0.00001606
Iteration 228/1000 | Loss: 0.00001606
Iteration 229/1000 | Loss: 0.00001606
Iteration 230/1000 | Loss: 0.00001606
Iteration 231/1000 | Loss: 0.00001606
Iteration 232/1000 | Loss: 0.00001606
Iteration 233/1000 | Loss: 0.00001606
Iteration 234/1000 | Loss: 0.00001605
Iteration 235/1000 | Loss: 0.00001605
Iteration 236/1000 | Loss: 0.00001605
Iteration 237/1000 | Loss: 0.00001605
Iteration 238/1000 | Loss: 0.00001605
Iteration 239/1000 | Loss: 0.00001605
Iteration 240/1000 | Loss: 0.00001604
Iteration 241/1000 | Loss: 0.00001604
Iteration 242/1000 | Loss: 0.00001604
Iteration 243/1000 | Loss: 0.00001604
Iteration 244/1000 | Loss: 0.00001603
Iteration 245/1000 | Loss: 0.00001603
Iteration 246/1000 | Loss: 0.00001603
Iteration 247/1000 | Loss: 0.00001603
Iteration 248/1000 | Loss: 0.00001603
Iteration 249/1000 | Loss: 0.00001603
Iteration 250/1000 | Loss: 0.00001603
Iteration 251/1000 | Loss: 0.00001603
Iteration 252/1000 | Loss: 0.00001603
Iteration 253/1000 | Loss: 0.00001603
Iteration 254/1000 | Loss: 0.00001602
Iteration 255/1000 | Loss: 0.00001602
Iteration 256/1000 | Loss: 0.00001602
Iteration 257/1000 | Loss: 0.00001602
Iteration 258/1000 | Loss: 0.00001602
Iteration 259/1000 | Loss: 0.00001602
Iteration 260/1000 | Loss: 0.00001602
Iteration 261/1000 | Loss: 0.00001602
Iteration 262/1000 | Loss: 0.00001602
Iteration 263/1000 | Loss: 0.00001602
Iteration 264/1000 | Loss: 0.00001602
Iteration 265/1000 | Loss: 0.00001602
Iteration 266/1000 | Loss: 0.00001601
Iteration 267/1000 | Loss: 0.00001601
Iteration 268/1000 | Loss: 0.00001601
Iteration 269/1000 | Loss: 0.00001601
Iteration 270/1000 | Loss: 0.00001601
Iteration 271/1000 | Loss: 0.00001601
Iteration 272/1000 | Loss: 0.00001601
Iteration 273/1000 | Loss: 0.00001601
Iteration 274/1000 | Loss: 0.00001601
Iteration 275/1000 | Loss: 0.00001601
Iteration 276/1000 | Loss: 0.00001601
Iteration 277/1000 | Loss: 0.00001600
Iteration 278/1000 | Loss: 0.00001600
Iteration 279/1000 | Loss: 0.00001600
Iteration 280/1000 | Loss: 0.00001600
Iteration 281/1000 | Loss: 0.00001600
Iteration 282/1000 | Loss: 0.00001600
Iteration 283/1000 | Loss: 0.00001600
Iteration 284/1000 | Loss: 0.00001600
Iteration 285/1000 | Loss: 0.00001600
Iteration 286/1000 | Loss: 0.00001600
Iteration 287/1000 | Loss: 0.00001600
Iteration 288/1000 | Loss: 0.00001600
Iteration 289/1000 | Loss: 0.00001600
Iteration 290/1000 | Loss: 0.00001600
Iteration 291/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.6002797565306537e-05, 1.6002797565306537e-05, 1.6002797565306537e-05, 1.6002797565306537e-05, 1.6002797565306537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6002797565306537e-05

Optimization complete. Final v2v error: 3.405992031097412 mm

Highest mean error: 3.84446382522583 mm for frame 10

Lowest mean error: 3.283190965652466 mm for frame 6

Saving results

Total time: 3510.930324077606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1004
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001187
Iteration 2/25 | Loss: 0.00186229
Iteration 3/25 | Loss: 0.00167978
Iteration 4/25 | Loss: 0.00154502
Iteration 5/25 | Loss: 0.00148553
Iteration 6/25 | Loss: 0.00143757
Iteration 7/25 | Loss: 0.00138568
Iteration 8/25 | Loss: 0.00135973
Iteration 9/25 | Loss: 0.00128616
Iteration 10/25 | Loss: 0.00122951
Iteration 11/25 | Loss: 0.00122274
Iteration 12/25 | Loss: 0.00121818
Iteration 13/25 | Loss: 0.00122311
Iteration 14/25 | Loss: 0.00122031
Iteration 15/25 | Loss: 0.00121001
Iteration 16/25 | Loss: 0.00120832
Iteration 17/25 | Loss: 0.00120971
Iteration 18/25 | Loss: 0.00120182
Iteration 19/25 | Loss: 0.00119819
Iteration 20/25 | Loss: 0.00118888
Iteration 21/25 | Loss: 0.00118035
Iteration 22/25 | Loss: 0.00117710
Iteration 23/25 | Loss: 0.00118337
Iteration 24/25 | Loss: 0.00117972
Iteration 25/25 | Loss: 0.00117877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48580492
Iteration 2/25 | Loss: 0.00099687
Iteration 3/25 | Loss: 0.00099687
Iteration 4/25 | Loss: 0.00099687
Iteration 5/25 | Loss: 0.00099687
Iteration 6/25 | Loss: 0.00099687
Iteration 7/25 | Loss: 0.00099687
Iteration 8/25 | Loss: 0.00099687
Iteration 9/25 | Loss: 0.00099687
Iteration 10/25 | Loss: 0.00099687
Iteration 11/25 | Loss: 0.00099687
Iteration 12/25 | Loss: 0.00099687
Iteration 13/25 | Loss: 0.00099687
Iteration 14/25 | Loss: 0.00099687
Iteration 15/25 | Loss: 0.00099687
Iteration 16/25 | Loss: 0.00099687
Iteration 17/25 | Loss: 0.00099687
Iteration 18/25 | Loss: 0.00099687
Iteration 19/25 | Loss: 0.00099687
Iteration 20/25 | Loss: 0.00099687
Iteration 21/25 | Loss: 0.00099687
Iteration 22/25 | Loss: 0.00099687
Iteration 23/25 | Loss: 0.00099687
Iteration 24/25 | Loss: 0.00099687
Iteration 25/25 | Loss: 0.00099687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099687
Iteration 2/1000 | Loss: 0.00018816
Iteration 3/1000 | Loss: 0.00008361
Iteration 4/1000 | Loss: 0.00008935
Iteration 5/1000 | Loss: 0.00008679
Iteration 6/1000 | Loss: 0.00004709
Iteration 7/1000 | Loss: 0.00013938
Iteration 8/1000 | Loss: 0.00018273
Iteration 9/1000 | Loss: 0.00016318
Iteration 10/1000 | Loss: 0.00018369
Iteration 11/1000 | Loss: 0.00003943
Iteration 12/1000 | Loss: 0.00007261
Iteration 13/1000 | Loss: 0.00003501
Iteration 14/1000 | Loss: 0.00005600
Iteration 15/1000 | Loss: 0.00021862
Iteration 16/1000 | Loss: 0.00011932
Iteration 17/1000 | Loss: 0.00012477
Iteration 18/1000 | Loss: 0.00021097
Iteration 19/1000 | Loss: 0.00005500
Iteration 20/1000 | Loss: 0.00013362
Iteration 21/1000 | Loss: 0.00011562
Iteration 22/1000 | Loss: 0.00023418
Iteration 23/1000 | Loss: 0.00011894
Iteration 24/1000 | Loss: 0.00007941
Iteration 25/1000 | Loss: 0.00023177
Iteration 26/1000 | Loss: 0.00014945
Iteration 27/1000 | Loss: 0.00023512
Iteration 28/1000 | Loss: 0.00012596
Iteration 29/1000 | Loss: 0.00075874
Iteration 30/1000 | Loss: 0.00059593
Iteration 31/1000 | Loss: 0.00020104
Iteration 32/1000 | Loss: 0.00083800
Iteration 33/1000 | Loss: 0.00031011
Iteration 34/1000 | Loss: 0.00093034
Iteration 35/1000 | Loss: 0.00024349
Iteration 36/1000 | Loss: 0.00066755
Iteration 37/1000 | Loss: 0.00083880
Iteration 38/1000 | Loss: 0.00002984
Iteration 39/1000 | Loss: 0.00006425
Iteration 40/1000 | Loss: 0.00004108
Iteration 41/1000 | Loss: 0.00003091
Iteration 42/1000 | Loss: 0.00004011
Iteration 43/1000 | Loss: 0.00003684
Iteration 44/1000 | Loss: 0.00004643
Iteration 45/1000 | Loss: 0.00003678
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00003439
Iteration 48/1000 | Loss: 0.00003445
Iteration 49/1000 | Loss: 0.00003389
Iteration 50/1000 | Loss: 0.00003803
Iteration 51/1000 | Loss: 0.00003452
Iteration 52/1000 | Loss: 0.00003218
Iteration 53/1000 | Loss: 0.00003659
Iteration 54/1000 | Loss: 0.00003739
Iteration 55/1000 | Loss: 0.00003165
Iteration 56/1000 | Loss: 0.00004546
Iteration 57/1000 | Loss: 0.00003415
Iteration 58/1000 | Loss: 0.00003935
Iteration 59/1000 | Loss: 0.00002778
Iteration 60/1000 | Loss: 0.00003751
Iteration 61/1000 | Loss: 0.00003741
Iteration 62/1000 | Loss: 0.00002813
Iteration 63/1000 | Loss: 0.00004349
Iteration 64/1000 | Loss: 0.00005724
Iteration 65/1000 | Loss: 0.00003651
Iteration 66/1000 | Loss: 0.00005713
Iteration 67/1000 | Loss: 0.00004038
Iteration 68/1000 | Loss: 0.00003687
Iteration 69/1000 | Loss: 0.00004436
Iteration 70/1000 | Loss: 0.00003597
Iteration 71/1000 | Loss: 0.00003649
Iteration 72/1000 | Loss: 0.00003795
Iteration 73/1000 | Loss: 0.00004028
Iteration 74/1000 | Loss: 0.00004036
Iteration 75/1000 | Loss: 0.00004373
Iteration 76/1000 | Loss: 0.00054521
Iteration 77/1000 | Loss: 0.00010336
Iteration 78/1000 | Loss: 0.00005550
Iteration 79/1000 | Loss: 0.00002523
Iteration 80/1000 | Loss: 0.00001957
Iteration 81/1000 | Loss: 0.00002192
Iteration 82/1000 | Loss: 0.00003189
Iteration 83/1000 | Loss: 0.00001735
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00002171
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001431
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001400
Iteration 92/1000 | Loss: 0.00001376
Iteration 93/1000 | Loss: 0.00001363
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001347
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001344
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001340
Iteration 110/1000 | Loss: 0.00001340
Iteration 111/1000 | Loss: 0.00001339
Iteration 112/1000 | Loss: 0.00001339
Iteration 113/1000 | Loss: 0.00001339
Iteration 114/1000 | Loss: 0.00001339
Iteration 115/1000 | Loss: 0.00001339
Iteration 116/1000 | Loss: 0.00001339
Iteration 117/1000 | Loss: 0.00001339
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001338
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001337
Iteration 123/1000 | Loss: 0.00001336
Iteration 124/1000 | Loss: 0.00001336
Iteration 125/1000 | Loss: 0.00001336
Iteration 126/1000 | Loss: 0.00001336
Iteration 127/1000 | Loss: 0.00001336
Iteration 128/1000 | Loss: 0.00001336
Iteration 129/1000 | Loss: 0.00001335
Iteration 130/1000 | Loss: 0.00001335
Iteration 131/1000 | Loss: 0.00001335
Iteration 132/1000 | Loss: 0.00001335
Iteration 133/1000 | Loss: 0.00001335
Iteration 134/1000 | Loss: 0.00001335
Iteration 135/1000 | Loss: 0.00001335
Iteration 136/1000 | Loss: 0.00001335
Iteration 137/1000 | Loss: 0.00001335
Iteration 138/1000 | Loss: 0.00001335
Iteration 139/1000 | Loss: 0.00001334
Iteration 140/1000 | Loss: 0.00001334
Iteration 141/1000 | Loss: 0.00001334
Iteration 142/1000 | Loss: 0.00001334
Iteration 143/1000 | Loss: 0.00001333
Iteration 144/1000 | Loss: 0.00001333
Iteration 145/1000 | Loss: 0.00001333
Iteration 146/1000 | Loss: 0.00001332
Iteration 147/1000 | Loss: 0.00001332
Iteration 148/1000 | Loss: 0.00001332
Iteration 149/1000 | Loss: 0.00001332
Iteration 150/1000 | Loss: 0.00001332
Iteration 151/1000 | Loss: 0.00001332
Iteration 152/1000 | Loss: 0.00001332
Iteration 153/1000 | Loss: 0.00001332
Iteration 154/1000 | Loss: 0.00001332
Iteration 155/1000 | Loss: 0.00001332
Iteration 156/1000 | Loss: 0.00001331
Iteration 157/1000 | Loss: 0.00001331
Iteration 158/1000 | Loss: 0.00001331
Iteration 159/1000 | Loss: 0.00001331
Iteration 160/1000 | Loss: 0.00001331
Iteration 161/1000 | Loss: 0.00001331
Iteration 162/1000 | Loss: 0.00001331
Iteration 163/1000 | Loss: 0.00001331
Iteration 164/1000 | Loss: 0.00001330
Iteration 165/1000 | Loss: 0.00001330
Iteration 166/1000 | Loss: 0.00001330
Iteration 167/1000 | Loss: 0.00001330
Iteration 168/1000 | Loss: 0.00001330
Iteration 169/1000 | Loss: 0.00001330
Iteration 170/1000 | Loss: 0.00001330
Iteration 171/1000 | Loss: 0.00001330
Iteration 172/1000 | Loss: 0.00001330
Iteration 173/1000 | Loss: 0.00001329
Iteration 174/1000 | Loss: 0.00001329
Iteration 175/1000 | Loss: 0.00001329
Iteration 176/1000 | Loss: 0.00001329
Iteration 177/1000 | Loss: 0.00001329
Iteration 178/1000 | Loss: 0.00001329
Iteration 179/1000 | Loss: 0.00001329
Iteration 180/1000 | Loss: 0.00001329
Iteration 181/1000 | Loss: 0.00001329
Iteration 182/1000 | Loss: 0.00001329
Iteration 183/1000 | Loss: 0.00001329
Iteration 184/1000 | Loss: 0.00001328
Iteration 185/1000 | Loss: 0.00001328
Iteration 186/1000 | Loss: 0.00001328
Iteration 187/1000 | Loss: 0.00001328
Iteration 188/1000 | Loss: 0.00001328
Iteration 189/1000 | Loss: 0.00001328
Iteration 190/1000 | Loss: 0.00001328
Iteration 191/1000 | Loss: 0.00001328
Iteration 192/1000 | Loss: 0.00001328
Iteration 193/1000 | Loss: 0.00001328
Iteration 194/1000 | Loss: 0.00001328
Iteration 195/1000 | Loss: 0.00001327
Iteration 196/1000 | Loss: 0.00001327
Iteration 197/1000 | Loss: 0.00001327
Iteration 198/1000 | Loss: 0.00001327
Iteration 199/1000 | Loss: 0.00001327
Iteration 200/1000 | Loss: 0.00001327
Iteration 201/1000 | Loss: 0.00001327
Iteration 202/1000 | Loss: 0.00001327
Iteration 203/1000 | Loss: 0.00001327
Iteration 204/1000 | Loss: 0.00001327
Iteration 205/1000 | Loss: 0.00001327
Iteration 206/1000 | Loss: 0.00001327
Iteration 207/1000 | Loss: 0.00001327
Iteration 208/1000 | Loss: 0.00001326
Iteration 209/1000 | Loss: 0.00001326
Iteration 210/1000 | Loss: 0.00001326
Iteration 211/1000 | Loss: 0.00001326
Iteration 212/1000 | Loss: 0.00001326
Iteration 213/1000 | Loss: 0.00001326
Iteration 214/1000 | Loss: 0.00001326
Iteration 215/1000 | Loss: 0.00001326
Iteration 216/1000 | Loss: 0.00001326
Iteration 217/1000 | Loss: 0.00001326
Iteration 218/1000 | Loss: 0.00001326
Iteration 219/1000 | Loss: 0.00001326
Iteration 220/1000 | Loss: 0.00001326
Iteration 221/1000 | Loss: 0.00001326
Iteration 222/1000 | Loss: 0.00001326
Iteration 223/1000 | Loss: 0.00001326
Iteration 224/1000 | Loss: 0.00001326
Iteration 225/1000 | Loss: 0.00001325
Iteration 226/1000 | Loss: 0.00001325
Iteration 227/1000 | Loss: 0.00001325
Iteration 228/1000 | Loss: 0.00001325
Iteration 229/1000 | Loss: 0.00001325
Iteration 230/1000 | Loss: 0.00001325
Iteration 231/1000 | Loss: 0.00001325
Iteration 232/1000 | Loss: 0.00001325
Iteration 233/1000 | Loss: 0.00001325
Iteration 234/1000 | Loss: 0.00001325
Iteration 235/1000 | Loss: 0.00001325
Iteration 236/1000 | Loss: 0.00001325
Iteration 237/1000 | Loss: 0.00001325
Iteration 238/1000 | Loss: 0.00001325
Iteration 239/1000 | Loss: 0.00001324
Iteration 240/1000 | Loss: 0.00001324
Iteration 241/1000 | Loss: 0.00001324
Iteration 242/1000 | Loss: 0.00001324
Iteration 243/1000 | Loss: 0.00001324
Iteration 244/1000 | Loss: 0.00001324
Iteration 245/1000 | Loss: 0.00001324
Iteration 246/1000 | Loss: 0.00001324
Iteration 247/1000 | Loss: 0.00001324
Iteration 248/1000 | Loss: 0.00001324
Iteration 249/1000 | Loss: 0.00001324
Iteration 250/1000 | Loss: 0.00001324
Iteration 251/1000 | Loss: 0.00001324
Iteration 252/1000 | Loss: 0.00001324
Iteration 253/1000 | Loss: 0.00001323
Iteration 254/1000 | Loss: 0.00001323
Iteration 255/1000 | Loss: 0.00001323
Iteration 256/1000 | Loss: 0.00001323
Iteration 257/1000 | Loss: 0.00001323
Iteration 258/1000 | Loss: 0.00001323
Iteration 259/1000 | Loss: 0.00001323
Iteration 260/1000 | Loss: 0.00001323
Iteration 261/1000 | Loss: 0.00001323
Iteration 262/1000 | Loss: 0.00001323
Iteration 263/1000 | Loss: 0.00001323
Iteration 264/1000 | Loss: 0.00001323
Iteration 265/1000 | Loss: 0.00001323
Iteration 266/1000 | Loss: 0.00001323
Iteration 267/1000 | Loss: 0.00001323
Iteration 268/1000 | Loss: 0.00001323
Iteration 269/1000 | Loss: 0.00001323
Iteration 270/1000 | Loss: 0.00001323
Iteration 271/1000 | Loss: 0.00001323
Iteration 272/1000 | Loss: 0.00001323
Iteration 273/1000 | Loss: 0.00001323
Iteration 274/1000 | Loss: 0.00001323
Iteration 275/1000 | Loss: 0.00001323
Iteration 276/1000 | Loss: 0.00001323
Iteration 277/1000 | Loss: 0.00001322
Iteration 278/1000 | Loss: 0.00001322
Iteration 279/1000 | Loss: 0.00001322
Iteration 280/1000 | Loss: 0.00001322
Iteration 281/1000 | Loss: 0.00001322
Iteration 282/1000 | Loss: 0.00001322
Iteration 283/1000 | Loss: 0.00001322
Iteration 284/1000 | Loss: 0.00001322
Iteration 285/1000 | Loss: 0.00001322
Iteration 286/1000 | Loss: 0.00001322
Iteration 287/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [1.3224674148659687e-05, 1.3224674148659687e-05, 1.3224674148659687e-05, 1.3224674148659687e-05, 1.3224674148659687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3224674148659687e-05

Optimization complete. Final v2v error: 3.0773017406463623 mm

Highest mean error: 4.385564804077148 mm for frame 74

Lowest mean error: 2.6657066345214844 mm for frame 35

Saving results

Total time: 3919.9074442386627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1086
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446049
Iteration 2/25 | Loss: 0.00128635
Iteration 3/25 | Loss: 0.00118508
Iteration 4/25 | Loss: 0.00116835
Iteration 5/25 | Loss: 0.00116288
Iteration 6/25 | Loss: 0.00116213
Iteration 7/25 | Loss: 0.00116194
Iteration 8/25 | Loss: 0.00116194
Iteration 9/25 | Loss: 0.00116194
Iteration 10/25 | Loss: 0.00116194
Iteration 11/25 | Loss: 0.00116195
Iteration 12/25 | Loss: 0.00116194
Iteration 13/25 | Loss: 0.00116194
Iteration 14/25 | Loss: 0.00116194
Iteration 15/25 | Loss: 0.00116194
Iteration 16/25 | Loss: 0.00116194
Iteration 17/25 | Loss: 0.00116194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001161944936029613, 0.001161944936029613, 0.001161944936029613, 0.001161944936029613, 0.001161944936029613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001161944936029613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35624146
Iteration 2/25 | Loss: 0.00082637
Iteration 3/25 | Loss: 0.00082637
Iteration 4/25 | Loss: 0.00082637
Iteration 5/25 | Loss: 0.00082637
Iteration 6/25 | Loss: 0.00082637
Iteration 7/25 | Loss: 0.00082637
Iteration 8/25 | Loss: 0.00082637
Iteration 9/25 | Loss: 0.00082637
Iteration 10/25 | Loss: 0.00082637
Iteration 11/25 | Loss: 0.00082637
Iteration 12/25 | Loss: 0.00082637
Iteration 13/25 | Loss: 0.00082637
Iteration 14/25 | Loss: 0.00082637
Iteration 15/25 | Loss: 0.00082637
Iteration 16/25 | Loss: 0.00082637
Iteration 17/25 | Loss: 0.00082637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008263681083917618, 0.0008263681083917618, 0.0008263681083917618, 0.0008263681083917618, 0.0008263681083917618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008263681083917618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082637
Iteration 2/1000 | Loss: 0.00002932
Iteration 3/1000 | Loss: 0.00002266
Iteration 4/1000 | Loss: 0.00002129
Iteration 5/1000 | Loss: 0.00002049
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001936
Iteration 8/1000 | Loss: 0.00001897
Iteration 9/1000 | Loss: 0.00001870
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001825
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001793
Iteration 14/1000 | Loss: 0.00001790
Iteration 15/1000 | Loss: 0.00001785
Iteration 16/1000 | Loss: 0.00001784
Iteration 17/1000 | Loss: 0.00001778
Iteration 18/1000 | Loss: 0.00001773
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001768
Iteration 21/1000 | Loss: 0.00001767
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001764
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001763
Iteration 27/1000 | Loss: 0.00001761
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001756
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001755
Iteration 32/1000 | Loss: 0.00001755
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001754
Iteration 35/1000 | Loss: 0.00001753
Iteration 36/1000 | Loss: 0.00001753
Iteration 37/1000 | Loss: 0.00001752
Iteration 38/1000 | Loss: 0.00001752
Iteration 39/1000 | Loss: 0.00001752
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001751
Iteration 42/1000 | Loss: 0.00001751
Iteration 43/1000 | Loss: 0.00001751
Iteration 44/1000 | Loss: 0.00001751
Iteration 45/1000 | Loss: 0.00001751
Iteration 46/1000 | Loss: 0.00001751
Iteration 47/1000 | Loss: 0.00001750
Iteration 48/1000 | Loss: 0.00001750
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001750
Iteration 51/1000 | Loss: 0.00001750
Iteration 52/1000 | Loss: 0.00001750
Iteration 53/1000 | Loss: 0.00001750
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001749
Iteration 58/1000 | Loss: 0.00001749
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001749
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001748
Iteration 68/1000 | Loss: 0.00001748
Iteration 69/1000 | Loss: 0.00001747
Iteration 70/1000 | Loss: 0.00001747
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001747
Iteration 75/1000 | Loss: 0.00001747
Iteration 76/1000 | Loss: 0.00001747
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001746
Iteration 79/1000 | Loss: 0.00001746
Iteration 80/1000 | Loss: 0.00001746
Iteration 81/1000 | Loss: 0.00001746
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00001746
Iteration 84/1000 | Loss: 0.00001745
Iteration 85/1000 | Loss: 0.00001745
Iteration 86/1000 | Loss: 0.00001745
Iteration 87/1000 | Loss: 0.00001745
Iteration 88/1000 | Loss: 0.00001745
Iteration 89/1000 | Loss: 0.00001745
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001744
Iteration 93/1000 | Loss: 0.00001744
Iteration 94/1000 | Loss: 0.00001743
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001742
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001741
Iteration 114/1000 | Loss: 0.00001741
Iteration 115/1000 | Loss: 0.00001741
Iteration 116/1000 | Loss: 0.00001741
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001741
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001741
Iteration 123/1000 | Loss: 0.00001741
Iteration 124/1000 | Loss: 0.00001741
Iteration 125/1000 | Loss: 0.00001741
Iteration 126/1000 | Loss: 0.00001741
Iteration 127/1000 | Loss: 0.00001741
Iteration 128/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.7409452993888408e-05, 1.7409452993888408e-05, 1.7409452993888408e-05, 1.7409452993888408e-05, 1.7409452993888408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7409452993888408e-05

Optimization complete. Final v2v error: 3.5462591648101807 mm

Highest mean error: 4.294902801513672 mm for frame 30

Lowest mean error: 3.384051561355591 mm for frame 2

Saving results

Total time: 799.8293895721436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1010
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431926
Iteration 2/25 | Loss: 0.00132108
Iteration 3/25 | Loss: 0.00117933
Iteration 4/25 | Loss: 0.00117131
Iteration 5/25 | Loss: 0.00117034
Iteration 6/25 | Loss: 0.00117034
Iteration 7/25 | Loss: 0.00117034
Iteration 8/25 | Loss: 0.00117034
Iteration 9/25 | Loss: 0.00117034
Iteration 10/25 | Loss: 0.00117029
Iteration 11/25 | Loss: 0.00117029
Iteration 12/25 | Loss: 0.00117029
Iteration 13/25 | Loss: 0.00117029
Iteration 14/25 | Loss: 0.00117029
Iteration 15/25 | Loss: 0.00117029
Iteration 16/25 | Loss: 0.00117029
Iteration 17/25 | Loss: 0.00117029
Iteration 18/25 | Loss: 0.00117029
Iteration 19/25 | Loss: 0.00117029
Iteration 20/25 | Loss: 0.00117029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011702929623425007, 0.0011702929623425007, 0.0011702929623425007, 0.0011702929623425007, 0.0011702929623425007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011702929623425007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.31716633
Iteration 2/25 | Loss: 0.00057067
Iteration 3/25 | Loss: 0.00057065
Iteration 4/25 | Loss: 0.00057064
Iteration 5/25 | Loss: 0.00057064
Iteration 6/25 | Loss: 0.00057064
Iteration 7/25 | Loss: 0.00057064
Iteration 8/25 | Loss: 0.00057064
Iteration 9/25 | Loss: 0.00057064
Iteration 10/25 | Loss: 0.00057064
Iteration 11/25 | Loss: 0.00057064
Iteration 12/25 | Loss: 0.00057064
Iteration 13/25 | Loss: 0.00057064
Iteration 14/25 | Loss: 0.00057064
Iteration 15/25 | Loss: 0.00057064
Iteration 16/25 | Loss: 0.00057064
Iteration 17/25 | Loss: 0.00057064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005706428200937808, 0.0005706428200937808, 0.0005706428200937808, 0.0005706428200937808, 0.0005706428200937808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005706428200937808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057064
Iteration 2/1000 | Loss: 0.00002392
Iteration 3/1000 | Loss: 0.00001974
Iteration 4/1000 | Loss: 0.00001865
Iteration 5/1000 | Loss: 0.00001769
Iteration 6/1000 | Loss: 0.00001686
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001603
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001564
Iteration 11/1000 | Loss: 0.00001556
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001526
Iteration 14/1000 | Loss: 0.00001520
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001513
Iteration 18/1000 | Loss: 0.00001512
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001494
Iteration 31/1000 | Loss: 0.00001494
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001491
Iteration 36/1000 | Loss: 0.00001490
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001486
Iteration 42/1000 | Loss: 0.00001486
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001485
Iteration 45/1000 | Loss: 0.00001484
Iteration 46/1000 | Loss: 0.00001484
Iteration 47/1000 | Loss: 0.00001483
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001481
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001480
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001478
Iteration 68/1000 | Loss: 0.00001477
Iteration 69/1000 | Loss: 0.00001477
Iteration 70/1000 | Loss: 0.00001477
Iteration 71/1000 | Loss: 0.00001477
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001476
Iteration 77/1000 | Loss: 0.00001476
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001475
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001475
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001474
Iteration 92/1000 | Loss: 0.00001474
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001473
Iteration 95/1000 | Loss: 0.00001473
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001473
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001473
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001472
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001471
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001471
Iteration 116/1000 | Loss: 0.00001471
Iteration 117/1000 | Loss: 0.00001471
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.4714089957124088e-05, 1.4714089957124088e-05, 1.4714089957124088e-05, 1.4714089957124088e-05, 1.4714089957124088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4714089957124088e-05

Optimization complete. Final v2v error: 3.2614502906799316 mm

Highest mean error: 3.669200897216797 mm for frame 101

Lowest mean error: 2.967203378677368 mm for frame 131

Saving results

Total time: 865.943323135376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1007
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997950
Iteration 2/25 | Loss: 0.00997948
Iteration 3/25 | Loss: 0.00203414
Iteration 4/25 | Loss: 0.00145247
Iteration 5/25 | Loss: 0.00138784
Iteration 6/25 | Loss: 0.00138396
Iteration 7/25 | Loss: 0.00138821
Iteration 8/25 | Loss: 0.00133803
Iteration 9/25 | Loss: 0.00130066
Iteration 10/25 | Loss: 0.00127881
Iteration 11/25 | Loss: 0.00126311
Iteration 12/25 | Loss: 0.00123678
Iteration 13/25 | Loss: 0.00122810
Iteration 14/25 | Loss: 0.00122724
Iteration 15/25 | Loss: 0.00122279
Iteration 16/25 | Loss: 0.00121888
Iteration 17/25 | Loss: 0.00122060
Iteration 18/25 | Loss: 0.00122365
Iteration 19/25 | Loss: 0.00121919
Iteration 20/25 | Loss: 0.00121192
Iteration 21/25 | Loss: 0.00120744
Iteration 22/25 | Loss: 0.00119997
Iteration 23/25 | Loss: 0.00119918
Iteration 24/25 | Loss: 0.00119876
Iteration 25/25 | Loss: 0.00119521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42180753
Iteration 2/25 | Loss: 0.00202431
Iteration 3/25 | Loss: 0.00185443
Iteration 4/25 | Loss: 0.00185443
Iteration 5/25 | Loss: 0.00185443
Iteration 6/25 | Loss: 0.00185443
Iteration 7/25 | Loss: 0.00185443
Iteration 8/25 | Loss: 0.00185443
Iteration 9/25 | Loss: 0.00185443
Iteration 10/25 | Loss: 0.00185443
Iteration 11/25 | Loss: 0.00185443
Iteration 12/25 | Loss: 0.00185443
Iteration 13/25 | Loss: 0.00185443
Iteration 14/25 | Loss: 0.00185443
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00185442587826401, 0.00185442587826401, 0.00185442587826401, 0.00185442587826401, 0.00185442587826401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00185442587826401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185443
Iteration 2/1000 | Loss: 0.00051691
Iteration 3/1000 | Loss: 0.00100628
Iteration 4/1000 | Loss: 0.00068151
Iteration 5/1000 | Loss: 0.00192794
Iteration 6/1000 | Loss: 0.00056054
Iteration 7/1000 | Loss: 0.00017817
Iteration 8/1000 | Loss: 0.00015101
Iteration 9/1000 | Loss: 0.00044024
Iteration 10/1000 | Loss: 0.00013863
Iteration 11/1000 | Loss: 0.00031369
Iteration 12/1000 | Loss: 0.00016697
Iteration 13/1000 | Loss: 0.00018653
Iteration 14/1000 | Loss: 0.00040326
Iteration 15/1000 | Loss: 0.00010634
Iteration 16/1000 | Loss: 0.00017711
Iteration 17/1000 | Loss: 0.00018321
Iteration 18/1000 | Loss: 0.00025054
Iteration 19/1000 | Loss: 0.00008349
Iteration 20/1000 | Loss: 0.00035713
Iteration 21/1000 | Loss: 0.00044858
Iteration 22/1000 | Loss: 0.00058937
Iteration 23/1000 | Loss: 0.00028035
Iteration 24/1000 | Loss: 0.00010036
Iteration 25/1000 | Loss: 0.00060023
Iteration 26/1000 | Loss: 0.00003559
Iteration 27/1000 | Loss: 0.00003645
Iteration 28/1000 | Loss: 0.00002529
Iteration 29/1000 | Loss: 0.00013551
Iteration 30/1000 | Loss: 0.00008838
Iteration 31/1000 | Loss: 0.00022514
Iteration 32/1000 | Loss: 0.00006827
Iteration 33/1000 | Loss: 0.00004412
Iteration 34/1000 | Loss: 0.00009255
Iteration 35/1000 | Loss: 0.00014740
Iteration 36/1000 | Loss: 0.00014369
Iteration 37/1000 | Loss: 0.00023439
Iteration 38/1000 | Loss: 0.00014661
Iteration 39/1000 | Loss: 0.00018310
Iteration 40/1000 | Loss: 0.00014054
Iteration 41/1000 | Loss: 0.00015134
Iteration 42/1000 | Loss: 0.00015183
Iteration 43/1000 | Loss: 0.00005161
Iteration 44/1000 | Loss: 0.00015457
Iteration 45/1000 | Loss: 0.00004188
Iteration 46/1000 | Loss: 0.00004124
Iteration 47/1000 | Loss: 0.00008753
Iteration 48/1000 | Loss: 0.00018746
Iteration 49/1000 | Loss: 0.00004106
Iteration 50/1000 | Loss: 0.00020436
Iteration 51/1000 | Loss: 0.00010172
Iteration 52/1000 | Loss: 0.00005023
Iteration 53/1000 | Loss: 0.00010287
Iteration 54/1000 | Loss: 0.00015984
Iteration 55/1000 | Loss: 0.00048309
Iteration 56/1000 | Loss: 0.00040501
Iteration 57/1000 | Loss: 0.00007648
Iteration 58/1000 | Loss: 0.00003609
Iteration 59/1000 | Loss: 0.00004058
Iteration 60/1000 | Loss: 0.00002443
Iteration 61/1000 | Loss: 0.00002574
Iteration 62/1000 | Loss: 0.00002135
Iteration 63/1000 | Loss: 0.00001872
Iteration 64/1000 | Loss: 0.00002720
Iteration 65/1000 | Loss: 0.00002803
Iteration 66/1000 | Loss: 0.00002418
Iteration 67/1000 | Loss: 0.00002496
Iteration 68/1000 | Loss: 0.00003026
Iteration 69/1000 | Loss: 0.00002638
Iteration 70/1000 | Loss: 0.00016727
Iteration 71/1000 | Loss: 0.00002046
Iteration 72/1000 | Loss: 0.00003698
Iteration 73/1000 | Loss: 0.00002681
Iteration 74/1000 | Loss: 0.00003683
Iteration 75/1000 | Loss: 0.00003138
Iteration 76/1000 | Loss: 0.00014078
Iteration 77/1000 | Loss: 0.00005538
Iteration 78/1000 | Loss: 0.00002485
Iteration 79/1000 | Loss: 0.00002759
Iteration 80/1000 | Loss: 0.00002796
Iteration 81/1000 | Loss: 0.00003801
Iteration 82/1000 | Loss: 0.00002629
Iteration 83/1000 | Loss: 0.00003337
Iteration 84/1000 | Loss: 0.00002584
Iteration 85/1000 | Loss: 0.00003162
Iteration 86/1000 | Loss: 0.00004338
Iteration 87/1000 | Loss: 0.00002786
Iteration 88/1000 | Loss: 0.00002634
Iteration 89/1000 | Loss: 0.00002677
Iteration 90/1000 | Loss: 0.00002630
Iteration 91/1000 | Loss: 0.00002640
Iteration 92/1000 | Loss: 0.00002734
Iteration 93/1000 | Loss: 0.00002590
Iteration 94/1000 | Loss: 0.00002123
Iteration 95/1000 | Loss: 0.00002541
Iteration 96/1000 | Loss: 0.00002551
Iteration 97/1000 | Loss: 0.00002613
Iteration 98/1000 | Loss: 0.00002516
Iteration 99/1000 | Loss: 0.00002613
Iteration 100/1000 | Loss: 0.00002638
Iteration 101/1000 | Loss: 0.00002599
Iteration 102/1000 | Loss: 0.00002599
Iteration 103/1000 | Loss: 0.00002540
Iteration 104/1000 | Loss: 0.00002590
Iteration 105/1000 | Loss: 0.00002465
Iteration 106/1000 | Loss: 0.00002194
Iteration 107/1000 | Loss: 0.00003380
Iteration 108/1000 | Loss: 0.00001951
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001795
Iteration 113/1000 | Loss: 0.00001412
Iteration 114/1000 | Loss: 0.00001409
Iteration 115/1000 | Loss: 0.00001406
Iteration 116/1000 | Loss: 0.00001401
Iteration 117/1000 | Loss: 0.00001401
Iteration 118/1000 | Loss: 0.00001392
Iteration 119/1000 | Loss: 0.00001390
Iteration 120/1000 | Loss: 0.00001378
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00002239
Iteration 123/1000 | Loss: 0.00001363
Iteration 124/1000 | Loss: 0.00001362
Iteration 125/1000 | Loss: 0.00001362
Iteration 126/1000 | Loss: 0.00001361
Iteration 127/1000 | Loss: 0.00001361
Iteration 128/1000 | Loss: 0.00001361
Iteration 129/1000 | Loss: 0.00001361
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001360
Iteration 132/1000 | Loss: 0.00001360
Iteration 133/1000 | Loss: 0.00001359
Iteration 134/1000 | Loss: 0.00001359
Iteration 135/1000 | Loss: 0.00001683
Iteration 136/1000 | Loss: 0.00001358
Iteration 137/1000 | Loss: 0.00001358
Iteration 138/1000 | Loss: 0.00001358
Iteration 139/1000 | Loss: 0.00001358
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Iteration 150/1000 | Loss: 0.00001357
Iteration 151/1000 | Loss: 0.00001357
Iteration 152/1000 | Loss: 0.00001357
Iteration 153/1000 | Loss: 0.00001357
Iteration 154/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.3572051102528349e-05, 1.3572051102528349e-05, 1.3572051102528349e-05, 1.3572051102528349e-05, 1.3572051102528349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3572051102528349e-05

Optimization complete. Final v2v error: 3.0854673385620117 mm

Highest mean error: 6.401845455169678 mm for frame 130

Lowest mean error: 2.484206438064575 mm for frame 202

Saving results

Total time: 7958.221325874329
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1089
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863813
Iteration 2/25 | Loss: 0.00177469
Iteration 3/25 | Loss: 0.00144153
Iteration 4/25 | Loss: 0.00136856
Iteration 5/25 | Loss: 0.00140180
Iteration 6/25 | Loss: 0.00129656
Iteration 7/25 | Loss: 0.00126829
Iteration 8/25 | Loss: 0.00126467
Iteration 9/25 | Loss: 0.00125196
Iteration 10/25 | Loss: 0.00123518
Iteration 11/25 | Loss: 0.00124639
Iteration 12/25 | Loss: 0.00124679
Iteration 13/25 | Loss: 0.00122943
Iteration 14/25 | Loss: 0.00121938
Iteration 15/25 | Loss: 0.00121535
Iteration 16/25 | Loss: 0.00121405
Iteration 17/25 | Loss: 0.00121389
Iteration 18/25 | Loss: 0.00121381
Iteration 19/25 | Loss: 0.00121381
Iteration 20/25 | Loss: 0.00121381
Iteration 21/25 | Loss: 0.00121381
Iteration 22/25 | Loss: 0.00121381
Iteration 23/25 | Loss: 0.00121381
Iteration 24/25 | Loss: 0.00121380
Iteration 25/25 | Loss: 0.00121380

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.53081989
Iteration 2/25 | Loss: 0.00100328
Iteration 3/25 | Loss: 0.00093919
Iteration 4/25 | Loss: 0.00093919
Iteration 5/25 | Loss: 0.00093919
Iteration 6/25 | Loss: 0.00093919
Iteration 7/25 | Loss: 0.00093919
Iteration 8/25 | Loss: 0.00093919
Iteration 9/25 | Loss: 0.00093919
Iteration 10/25 | Loss: 0.00093919
Iteration 11/25 | Loss: 0.00093919
Iteration 12/25 | Loss: 0.00093919
Iteration 13/25 | Loss: 0.00093919
Iteration 14/25 | Loss: 0.00093919
Iteration 15/25 | Loss: 0.00093919
Iteration 16/25 | Loss: 0.00093919
Iteration 17/25 | Loss: 0.00093919
Iteration 18/25 | Loss: 0.00093919
Iteration 19/25 | Loss: 0.00093919
Iteration 20/25 | Loss: 0.00093919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009391912026330829, 0.0009391912026330829, 0.0009391912026330829, 0.0009391912026330829, 0.0009391912026330829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009391912026330829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093919
Iteration 2/1000 | Loss: 0.00006298
Iteration 3/1000 | Loss: 0.00017504
Iteration 4/1000 | Loss: 0.00003188
Iteration 5/1000 | Loss: 0.00005104
Iteration 6/1000 | Loss: 0.00002502
Iteration 7/1000 | Loss: 0.00003971
Iteration 8/1000 | Loss: 0.00003604
Iteration 9/1000 | Loss: 0.00003688
Iteration 10/1000 | Loss: 0.00003247
Iteration 11/1000 | Loss: 0.00002110
Iteration 12/1000 | Loss: 0.00002814
Iteration 13/1000 | Loss: 0.00003114
Iteration 14/1000 | Loss: 0.00003067
Iteration 15/1000 | Loss: 0.00004193
Iteration 16/1000 | Loss: 0.00003206
Iteration 17/1000 | Loss: 0.00003911
Iteration 18/1000 | Loss: 0.00002461
Iteration 19/1000 | Loss: 0.00002956
Iteration 20/1000 | Loss: 0.00002429
Iteration 21/1000 | Loss: 0.00003255
Iteration 22/1000 | Loss: 0.00003188
Iteration 23/1000 | Loss: 0.00003659
Iteration 24/1000 | Loss: 0.00002939
Iteration 25/1000 | Loss: 0.00003214
Iteration 26/1000 | Loss: 0.00001938
Iteration 27/1000 | Loss: 0.00003612
Iteration 28/1000 | Loss: 0.00003171
Iteration 29/1000 | Loss: 0.00003103
Iteration 30/1000 | Loss: 0.00003184
Iteration 31/1000 | Loss: 0.00002949
Iteration 32/1000 | Loss: 0.00004666
Iteration 33/1000 | Loss: 0.00003102
Iteration 34/1000 | Loss: 0.00003244
Iteration 35/1000 | Loss: 0.00003402
Iteration 36/1000 | Loss: 0.00003253
Iteration 37/1000 | Loss: 0.00003538
Iteration 38/1000 | Loss: 0.00003210
Iteration 39/1000 | Loss: 0.00003284
Iteration 40/1000 | Loss: 0.00003196
Iteration 41/1000 | Loss: 0.00003158
Iteration 42/1000 | Loss: 0.00003162
Iteration 43/1000 | Loss: 0.00003059
Iteration 44/1000 | Loss: 0.00003388
Iteration 45/1000 | Loss: 0.00003369
Iteration 46/1000 | Loss: 0.00003253
Iteration 47/1000 | Loss: 0.00003551
Iteration 48/1000 | Loss: 0.00003281
Iteration 49/1000 | Loss: 0.00003472
Iteration 50/1000 | Loss: 0.00003174
Iteration 51/1000 | Loss: 0.00003418
Iteration 52/1000 | Loss: 0.00002321
Iteration 53/1000 | Loss: 0.00003022
Iteration 54/1000 | Loss: 0.00002121
Iteration 55/1000 | Loss: 0.00003398
Iteration 56/1000 | Loss: 0.00003225
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00003092
Iteration 59/1000 | Loss: 0.00003780
Iteration 60/1000 | Loss: 0.00002867
Iteration 61/1000 | Loss: 0.00002250
Iteration 62/1000 | Loss: 0.00003532
Iteration 63/1000 | Loss: 0.00004564
Iteration 64/1000 | Loss: 0.00003408
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002838
Iteration 67/1000 | Loss: 0.00002514
Iteration 68/1000 | Loss: 0.00003947
Iteration 69/1000 | Loss: 0.00002809
Iteration 70/1000 | Loss: 0.00003127
Iteration 71/1000 | Loss: 0.00002504
Iteration 72/1000 | Loss: 0.00002662
Iteration 73/1000 | Loss: 0.00003806
Iteration 74/1000 | Loss: 0.00004238
Iteration 75/1000 | Loss: 0.00002958
Iteration 76/1000 | Loss: 0.00003190
Iteration 77/1000 | Loss: 0.00002873
Iteration 78/1000 | Loss: 0.00003302
Iteration 79/1000 | Loss: 0.00003153
Iteration 80/1000 | Loss: 0.00003181
Iteration 81/1000 | Loss: 0.00003196
Iteration 82/1000 | Loss: 0.00003834
Iteration 83/1000 | Loss: 0.00002473
Iteration 84/1000 | Loss: 0.00001982
Iteration 85/1000 | Loss: 0.00001865
Iteration 86/1000 | Loss: 0.00001796
Iteration 87/1000 | Loss: 0.00001775
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001731
Iteration 91/1000 | Loss: 0.00001725
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001710
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001708
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001703
Iteration 111/1000 | Loss: 0.00001702
Iteration 112/1000 | Loss: 0.00001702
Iteration 113/1000 | Loss: 0.00001702
Iteration 114/1000 | Loss: 0.00001702
Iteration 115/1000 | Loss: 0.00001702
Iteration 116/1000 | Loss: 0.00001702
Iteration 117/1000 | Loss: 0.00001701
Iteration 118/1000 | Loss: 0.00001701
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001700
Iteration 126/1000 | Loss: 0.00001700
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001699
Iteration 130/1000 | Loss: 0.00001699
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001698
Iteration 140/1000 | Loss: 0.00001698
Iteration 141/1000 | Loss: 0.00001698
Iteration 142/1000 | Loss: 0.00001698
Iteration 143/1000 | Loss: 0.00001697
Iteration 144/1000 | Loss: 0.00001697
Iteration 145/1000 | Loss: 0.00001697
Iteration 146/1000 | Loss: 0.00001697
Iteration 147/1000 | Loss: 0.00001697
Iteration 148/1000 | Loss: 0.00001697
Iteration 149/1000 | Loss: 0.00001697
Iteration 150/1000 | Loss: 0.00001696
Iteration 151/1000 | Loss: 0.00001696
Iteration 152/1000 | Loss: 0.00001696
Iteration 153/1000 | Loss: 0.00001695
Iteration 154/1000 | Loss: 0.00001695
Iteration 155/1000 | Loss: 0.00001694
Iteration 156/1000 | Loss: 0.00001694
Iteration 157/1000 | Loss: 0.00001693
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001689
Iteration 160/1000 | Loss: 0.00001688
Iteration 161/1000 | Loss: 0.00001686
Iteration 162/1000 | Loss: 0.00001686
Iteration 163/1000 | Loss: 0.00001686
Iteration 164/1000 | Loss: 0.00001686
Iteration 165/1000 | Loss: 0.00001685
Iteration 166/1000 | Loss: 0.00001685
Iteration 167/1000 | Loss: 0.00001685
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Iteration 173/1000 | Loss: 0.00001683
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001678
Iteration 177/1000 | Loss: 0.00001674
Iteration 178/1000 | Loss: 0.00001673
Iteration 179/1000 | Loss: 0.00001673
Iteration 180/1000 | Loss: 0.00001673
Iteration 181/1000 | Loss: 0.00001672
Iteration 182/1000 | Loss: 0.00001672
Iteration 183/1000 | Loss: 0.00001672
Iteration 184/1000 | Loss: 0.00001671
Iteration 185/1000 | Loss: 0.00001671
Iteration 186/1000 | Loss: 0.00001671
Iteration 187/1000 | Loss: 0.00001671
Iteration 188/1000 | Loss: 0.00001671
Iteration 189/1000 | Loss: 0.00001670
Iteration 190/1000 | Loss: 0.00001670
Iteration 191/1000 | Loss: 0.00001670
Iteration 192/1000 | Loss: 0.00001670
Iteration 193/1000 | Loss: 0.00001669
Iteration 194/1000 | Loss: 0.00001669
Iteration 195/1000 | Loss: 0.00001669
Iteration 196/1000 | Loss: 0.00001669
Iteration 197/1000 | Loss: 0.00001669
Iteration 198/1000 | Loss: 0.00001669
Iteration 199/1000 | Loss: 0.00001669
Iteration 200/1000 | Loss: 0.00001668
Iteration 201/1000 | Loss: 0.00001668
Iteration 202/1000 | Loss: 0.00001668
Iteration 203/1000 | Loss: 0.00001667
Iteration 204/1000 | Loss: 0.00001667
Iteration 205/1000 | Loss: 0.00001667
Iteration 206/1000 | Loss: 0.00001666
Iteration 207/1000 | Loss: 0.00001666
Iteration 208/1000 | Loss: 0.00001666
Iteration 209/1000 | Loss: 0.00001665
Iteration 210/1000 | Loss: 0.00001665
Iteration 211/1000 | Loss: 0.00001665
Iteration 212/1000 | Loss: 0.00001665
Iteration 213/1000 | Loss: 0.00001664
Iteration 214/1000 | Loss: 0.00001664
Iteration 215/1000 | Loss: 0.00001664
Iteration 216/1000 | Loss: 0.00001663
Iteration 217/1000 | Loss: 0.00001663
Iteration 218/1000 | Loss: 0.00001663
Iteration 219/1000 | Loss: 0.00001663
Iteration 220/1000 | Loss: 0.00001662
Iteration 221/1000 | Loss: 0.00001662
Iteration 222/1000 | Loss: 0.00001662
Iteration 223/1000 | Loss: 0.00001662
Iteration 224/1000 | Loss: 0.00001662
Iteration 225/1000 | Loss: 0.00001662
Iteration 226/1000 | Loss: 0.00001661
Iteration 227/1000 | Loss: 0.00001661
Iteration 228/1000 | Loss: 0.00001661
Iteration 229/1000 | Loss: 0.00001661
Iteration 230/1000 | Loss: 0.00001660
Iteration 231/1000 | Loss: 0.00001660
Iteration 232/1000 | Loss: 0.00001660
Iteration 233/1000 | Loss: 0.00001660
Iteration 234/1000 | Loss: 0.00001659
Iteration 235/1000 | Loss: 0.00001659
Iteration 236/1000 | Loss: 0.00001659
Iteration 237/1000 | Loss: 0.00001659
Iteration 238/1000 | Loss: 0.00001659
Iteration 239/1000 | Loss: 0.00001659
Iteration 240/1000 | Loss: 0.00001659
Iteration 241/1000 | Loss: 0.00001659
Iteration 242/1000 | Loss: 0.00001659
Iteration 243/1000 | Loss: 0.00001659
Iteration 244/1000 | Loss: 0.00001659
Iteration 245/1000 | Loss: 0.00001659
Iteration 246/1000 | Loss: 0.00001658
Iteration 247/1000 | Loss: 0.00001657
Iteration 248/1000 | Loss: 0.00001657
Iteration 249/1000 | Loss: 0.00001657
Iteration 250/1000 | Loss: 0.00001657
Iteration 251/1000 | Loss: 0.00001657
Iteration 252/1000 | Loss: 0.00001656
Iteration 253/1000 | Loss: 0.00001656
Iteration 254/1000 | Loss: 0.00001656
Iteration 255/1000 | Loss: 0.00001656
Iteration 256/1000 | Loss: 0.00001656
Iteration 257/1000 | Loss: 0.00001655
Iteration 258/1000 | Loss: 0.00001655
Iteration 259/1000 | Loss: 0.00001655
Iteration 260/1000 | Loss: 0.00001655
Iteration 261/1000 | Loss: 0.00001655
Iteration 262/1000 | Loss: 0.00001655
Iteration 263/1000 | Loss: 0.00001655
Iteration 264/1000 | Loss: 0.00001655
Iteration 265/1000 | Loss: 0.00001655
Iteration 266/1000 | Loss: 0.00001655
Iteration 267/1000 | Loss: 0.00001655
Iteration 268/1000 | Loss: 0.00001654
Iteration 269/1000 | Loss: 0.00001654
Iteration 270/1000 | Loss: 0.00001654
Iteration 271/1000 | Loss: 0.00001654
Iteration 272/1000 | Loss: 0.00001654
Iteration 273/1000 | Loss: 0.00001654
Iteration 274/1000 | Loss: 0.00001654
Iteration 275/1000 | Loss: 0.00001653
Iteration 276/1000 | Loss: 0.00001653
Iteration 277/1000 | Loss: 0.00001653
Iteration 278/1000 | Loss: 0.00001653
Iteration 279/1000 | Loss: 0.00001653
Iteration 280/1000 | Loss: 0.00001652
Iteration 281/1000 | Loss: 0.00001652
Iteration 282/1000 | Loss: 0.00001652
Iteration 283/1000 | Loss: 0.00001652
Iteration 284/1000 | Loss: 0.00001652
Iteration 285/1000 | Loss: 0.00001652
Iteration 286/1000 | Loss: 0.00001652
Iteration 287/1000 | Loss: 0.00001651
Iteration 288/1000 | Loss: 0.00001651
Iteration 289/1000 | Loss: 0.00001651
Iteration 290/1000 | Loss: 0.00001651
Iteration 291/1000 | Loss: 0.00001650
Iteration 292/1000 | Loss: 0.00001650
Iteration 293/1000 | Loss: 0.00001650
Iteration 294/1000 | Loss: 0.00001650
Iteration 295/1000 | Loss: 0.00001649
Iteration 296/1000 | Loss: 0.00001649
Iteration 297/1000 | Loss: 0.00001649
Iteration 298/1000 | Loss: 0.00001649
Iteration 299/1000 | Loss: 0.00001649
Iteration 300/1000 | Loss: 0.00001649
Iteration 301/1000 | Loss: 0.00001649
Iteration 302/1000 | Loss: 0.00001649
Iteration 303/1000 | Loss: 0.00001649
Iteration 304/1000 | Loss: 0.00001649
Iteration 305/1000 | Loss: 0.00001649
Iteration 306/1000 | Loss: 0.00001649
Iteration 307/1000 | Loss: 0.00001649
Iteration 308/1000 | Loss: 0.00001648
Iteration 309/1000 | Loss: 0.00001648
Iteration 310/1000 | Loss: 0.00001648
Iteration 311/1000 | Loss: 0.00001648
Iteration 312/1000 | Loss: 0.00001648
Iteration 313/1000 | Loss: 0.00001648
Iteration 314/1000 | Loss: 0.00001648
Iteration 315/1000 | Loss: 0.00001648
Iteration 316/1000 | Loss: 0.00001648
Iteration 317/1000 | Loss: 0.00001648
Iteration 318/1000 | Loss: 0.00001648
Iteration 319/1000 | Loss: 0.00001648
Iteration 320/1000 | Loss: 0.00001648
Iteration 321/1000 | Loss: 0.00001648
Iteration 322/1000 | Loss: 0.00001647
Iteration 323/1000 | Loss: 0.00001647
Iteration 324/1000 | Loss: 0.00001647
Iteration 325/1000 | Loss: 0.00001647
Iteration 326/1000 | Loss: 0.00001647
Iteration 327/1000 | Loss: 0.00001647
Iteration 328/1000 | Loss: 0.00001647
Iteration 329/1000 | Loss: 0.00001647
Iteration 330/1000 | Loss: 0.00001647
Iteration 331/1000 | Loss: 0.00001647
Iteration 332/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [1.6472900824737735e-05, 1.6472900824737735e-05, 1.6472900824737735e-05, 1.6472900824737735e-05, 1.6472900824737735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6472900824737735e-05

Optimization complete. Final v2v error: 3.336939573287964 mm

Highest mean error: 5.346240520477295 mm for frame 95

Lowest mean error: 2.9116084575653076 mm for frame 52

Saving results

Total time: 4407.681886434555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1036
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478994
Iteration 2/25 | Loss: 0.00133691
Iteration 3/25 | Loss: 0.00118745
Iteration 4/25 | Loss: 0.00115958
Iteration 5/25 | Loss: 0.00115054
Iteration 6/25 | Loss: 0.00114915
Iteration 7/25 | Loss: 0.00114915
Iteration 8/25 | Loss: 0.00114915
Iteration 9/25 | Loss: 0.00114915
Iteration 10/25 | Loss: 0.00114915
Iteration 11/25 | Loss: 0.00114915
Iteration 12/25 | Loss: 0.00114915
Iteration 13/25 | Loss: 0.00114915
Iteration 14/25 | Loss: 0.00114915
Iteration 15/25 | Loss: 0.00114915
Iteration 16/25 | Loss: 0.00114915
Iteration 17/25 | Loss: 0.00114915
Iteration 18/25 | Loss: 0.00114915
Iteration 19/25 | Loss: 0.00114915
Iteration 20/25 | Loss: 0.00114915
Iteration 21/25 | Loss: 0.00114915
Iteration 22/25 | Loss: 0.00114915
Iteration 23/25 | Loss: 0.00114915
Iteration 24/25 | Loss: 0.00114915
Iteration 25/25 | Loss: 0.00114915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31831205
Iteration 2/25 | Loss: 0.00091232
Iteration 3/25 | Loss: 0.00091232
Iteration 4/25 | Loss: 0.00091231
Iteration 5/25 | Loss: 0.00091231
Iteration 6/25 | Loss: 0.00091231
Iteration 7/25 | Loss: 0.00091231
Iteration 8/25 | Loss: 0.00091231
Iteration 9/25 | Loss: 0.00091231
Iteration 10/25 | Loss: 0.00091231
Iteration 11/25 | Loss: 0.00091231
Iteration 12/25 | Loss: 0.00091231
Iteration 13/25 | Loss: 0.00091231
Iteration 14/25 | Loss: 0.00091231
Iteration 15/25 | Loss: 0.00091231
Iteration 16/25 | Loss: 0.00091231
Iteration 17/25 | Loss: 0.00091231
Iteration 18/25 | Loss: 0.00091231
Iteration 19/25 | Loss: 0.00091231
Iteration 20/25 | Loss: 0.00091231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009123128256760538, 0.0009123128256760538, 0.0009123128256760538, 0.0009123128256760538, 0.0009123128256760538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009123128256760538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091231
Iteration 2/1000 | Loss: 0.00005357
Iteration 3/1000 | Loss: 0.00003533
Iteration 4/1000 | Loss: 0.00003033
Iteration 5/1000 | Loss: 0.00002887
Iteration 6/1000 | Loss: 0.00002793
Iteration 7/1000 | Loss: 0.00002738
Iteration 8/1000 | Loss: 0.00002701
Iteration 9/1000 | Loss: 0.00002677
Iteration 10/1000 | Loss: 0.00002665
Iteration 11/1000 | Loss: 0.00002649
Iteration 12/1000 | Loss: 0.00002644
Iteration 13/1000 | Loss: 0.00002644
Iteration 14/1000 | Loss: 0.00002643
Iteration 15/1000 | Loss: 0.00002643
Iteration 16/1000 | Loss: 0.00002635
Iteration 17/1000 | Loss: 0.00002635
Iteration 18/1000 | Loss: 0.00002634
Iteration 19/1000 | Loss: 0.00002634
Iteration 20/1000 | Loss: 0.00002624
Iteration 21/1000 | Loss: 0.00002624
Iteration 22/1000 | Loss: 0.00002623
Iteration 23/1000 | Loss: 0.00002623
Iteration 24/1000 | Loss: 0.00002622
Iteration 25/1000 | Loss: 0.00002622
Iteration 26/1000 | Loss: 0.00002616
Iteration 27/1000 | Loss: 0.00002616
Iteration 28/1000 | Loss: 0.00002616
Iteration 29/1000 | Loss: 0.00002616
Iteration 30/1000 | Loss: 0.00002616
Iteration 31/1000 | Loss: 0.00002614
Iteration 32/1000 | Loss: 0.00002607
Iteration 33/1000 | Loss: 0.00002602
Iteration 34/1000 | Loss: 0.00002601
Iteration 35/1000 | Loss: 0.00002601
Iteration 36/1000 | Loss: 0.00002600
Iteration 37/1000 | Loss: 0.00002600
Iteration 38/1000 | Loss: 0.00002599
Iteration 39/1000 | Loss: 0.00002599
Iteration 40/1000 | Loss: 0.00002590
Iteration 41/1000 | Loss: 0.00002589
Iteration 42/1000 | Loss: 0.00002588
Iteration 43/1000 | Loss: 0.00002588
Iteration 44/1000 | Loss: 0.00002587
Iteration 45/1000 | Loss: 0.00002587
Iteration 46/1000 | Loss: 0.00002587
Iteration 47/1000 | Loss: 0.00002587
Iteration 48/1000 | Loss: 0.00002586
Iteration 49/1000 | Loss: 0.00002586
Iteration 50/1000 | Loss: 0.00002585
Iteration 51/1000 | Loss: 0.00002585
Iteration 52/1000 | Loss: 0.00002584
Iteration 53/1000 | Loss: 0.00002584
Iteration 54/1000 | Loss: 0.00002584
Iteration 55/1000 | Loss: 0.00002583
Iteration 56/1000 | Loss: 0.00002583
Iteration 57/1000 | Loss: 0.00002583
Iteration 58/1000 | Loss: 0.00002583
Iteration 59/1000 | Loss: 0.00002583
Iteration 60/1000 | Loss: 0.00002583
Iteration 61/1000 | Loss: 0.00002583
Iteration 62/1000 | Loss: 0.00002583
Iteration 63/1000 | Loss: 0.00002583
Iteration 64/1000 | Loss: 0.00002582
Iteration 65/1000 | Loss: 0.00002582
Iteration 66/1000 | Loss: 0.00002582
Iteration 67/1000 | Loss: 0.00002582
Iteration 68/1000 | Loss: 0.00002581
Iteration 69/1000 | Loss: 0.00002581
Iteration 70/1000 | Loss: 0.00002581
Iteration 71/1000 | Loss: 0.00002581
Iteration 72/1000 | Loss: 0.00002580
Iteration 73/1000 | Loss: 0.00002580
Iteration 74/1000 | Loss: 0.00002580
Iteration 75/1000 | Loss: 0.00002580
Iteration 76/1000 | Loss: 0.00002580
Iteration 77/1000 | Loss: 0.00002580
Iteration 78/1000 | Loss: 0.00002580
Iteration 79/1000 | Loss: 0.00002580
Iteration 80/1000 | Loss: 0.00002580
Iteration 81/1000 | Loss: 0.00002580
Iteration 82/1000 | Loss: 0.00002580
Iteration 83/1000 | Loss: 0.00002580
Iteration 84/1000 | Loss: 0.00002580
Iteration 85/1000 | Loss: 0.00002580
Iteration 86/1000 | Loss: 0.00002580
Iteration 87/1000 | Loss: 0.00002580
Iteration 88/1000 | Loss: 0.00002580
Iteration 89/1000 | Loss: 0.00002580
Iteration 90/1000 | Loss: 0.00002580
Iteration 91/1000 | Loss: 0.00002580
Iteration 92/1000 | Loss: 0.00002580
Iteration 93/1000 | Loss: 0.00002580
Iteration 94/1000 | Loss: 0.00002580
Iteration 95/1000 | Loss: 0.00002580
Iteration 96/1000 | Loss: 0.00002580
Iteration 97/1000 | Loss: 0.00002580
Iteration 98/1000 | Loss: 0.00002580
Iteration 99/1000 | Loss: 0.00002580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.5797153284656815e-05, 2.5797153284656815e-05, 2.5797153284656815e-05, 2.5797153284656815e-05, 2.5797153284656815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5797153284656815e-05

Optimization complete. Final v2v error: 4.037248134613037 mm

Highest mean error: 4.383090496063232 mm for frame 38

Lowest mean error: 3.7314374446868896 mm for frame 169

Saving results

Total time: 951.8070373535156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1054
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00277411
Iteration 2/25 | Loss: 0.00133188
Iteration 3/25 | Loss: 0.00117394
Iteration 4/25 | Loss: 0.00114607
Iteration 5/25 | Loss: 0.00114110
Iteration 6/25 | Loss: 0.00113998
Iteration 7/25 | Loss: 0.00113953
Iteration 8/25 | Loss: 0.00113953
Iteration 9/25 | Loss: 0.00113953
Iteration 10/25 | Loss: 0.00113953
Iteration 11/25 | Loss: 0.00113953
Iteration 12/25 | Loss: 0.00113953
Iteration 13/25 | Loss: 0.00113953
Iteration 14/25 | Loss: 0.00113953
Iteration 15/25 | Loss: 0.00113953
Iteration 16/25 | Loss: 0.00113953
Iteration 17/25 | Loss: 0.00113953
Iteration 18/25 | Loss: 0.00113953
Iteration 19/25 | Loss: 0.00113953
Iteration 20/25 | Loss: 0.00113953
Iteration 21/25 | Loss: 0.00113953
Iteration 22/25 | Loss: 0.00113953
Iteration 23/25 | Loss: 0.00113953
Iteration 24/25 | Loss: 0.00113953
Iteration 25/25 | Loss: 0.00113953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34922791
Iteration 2/25 | Loss: 0.00102541
Iteration 3/25 | Loss: 0.00102541
Iteration 4/25 | Loss: 0.00102541
Iteration 5/25 | Loss: 0.00102541
Iteration 6/25 | Loss: 0.00102541
Iteration 7/25 | Loss: 0.00102541
Iteration 8/25 | Loss: 0.00102541
Iteration 9/25 | Loss: 0.00102541
Iteration 10/25 | Loss: 0.00102541
Iteration 11/25 | Loss: 0.00102541
Iteration 12/25 | Loss: 0.00102541
Iteration 13/25 | Loss: 0.00102541
Iteration 14/25 | Loss: 0.00102541
Iteration 15/25 | Loss: 0.00102541
Iteration 16/25 | Loss: 0.00102541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010254085063934326, 0.0010254085063934326, 0.0010254085063934326, 0.0010254085063934326, 0.0010254085063934326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010254085063934326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102541
Iteration 2/1000 | Loss: 0.00004742
Iteration 3/1000 | Loss: 0.00003150
Iteration 4/1000 | Loss: 0.00002124
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001866
Iteration 7/1000 | Loss: 0.00001795
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001710
Iteration 10/1000 | Loss: 0.00001665
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001590
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001568
Iteration 17/1000 | Loss: 0.00001567
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001563
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001559
Iteration 22/1000 | Loss: 0.00001555
Iteration 23/1000 | Loss: 0.00001551
Iteration 24/1000 | Loss: 0.00001551
Iteration 25/1000 | Loss: 0.00001551
Iteration 26/1000 | Loss: 0.00001550
Iteration 27/1000 | Loss: 0.00001550
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001547
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001545
Iteration 46/1000 | Loss: 0.00001545
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001544
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001543
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001542
Iteration 58/1000 | Loss: 0.00001542
Iteration 59/1000 | Loss: 0.00001542
Iteration 60/1000 | Loss: 0.00001542
Iteration 61/1000 | Loss: 0.00001542
Iteration 62/1000 | Loss: 0.00001542
Iteration 63/1000 | Loss: 0.00001542
Iteration 64/1000 | Loss: 0.00001541
Iteration 65/1000 | Loss: 0.00001541
Iteration 66/1000 | Loss: 0.00001541
Iteration 67/1000 | Loss: 0.00001540
Iteration 68/1000 | Loss: 0.00001540
Iteration 69/1000 | Loss: 0.00001540
Iteration 70/1000 | Loss: 0.00001539
Iteration 71/1000 | Loss: 0.00001539
Iteration 72/1000 | Loss: 0.00001539
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001539
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001538
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001537
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00001536
Iteration 92/1000 | Loss: 0.00001535
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001535
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001534
Iteration 98/1000 | Loss: 0.00001534
Iteration 99/1000 | Loss: 0.00001533
Iteration 100/1000 | Loss: 0.00001533
Iteration 101/1000 | Loss: 0.00001533
Iteration 102/1000 | Loss: 0.00001533
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001532
Iteration 105/1000 | Loss: 0.00001532
Iteration 106/1000 | Loss: 0.00001532
Iteration 107/1000 | Loss: 0.00001532
Iteration 108/1000 | Loss: 0.00001532
Iteration 109/1000 | Loss: 0.00001532
Iteration 110/1000 | Loss: 0.00001531
Iteration 111/1000 | Loss: 0.00001531
Iteration 112/1000 | Loss: 0.00001531
Iteration 113/1000 | Loss: 0.00001531
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001531
Iteration 116/1000 | Loss: 0.00001531
Iteration 117/1000 | Loss: 0.00001531
Iteration 118/1000 | Loss: 0.00001531
Iteration 119/1000 | Loss: 0.00001531
Iteration 120/1000 | Loss: 0.00001531
Iteration 121/1000 | Loss: 0.00001531
Iteration 122/1000 | Loss: 0.00001530
Iteration 123/1000 | Loss: 0.00001530
Iteration 124/1000 | Loss: 0.00001530
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00001530
Iteration 127/1000 | Loss: 0.00001530
Iteration 128/1000 | Loss: 0.00001530
Iteration 129/1000 | Loss: 0.00001530
Iteration 130/1000 | Loss: 0.00001530
Iteration 131/1000 | Loss: 0.00001530
Iteration 132/1000 | Loss: 0.00001530
Iteration 133/1000 | Loss: 0.00001529
Iteration 134/1000 | Loss: 0.00001529
Iteration 135/1000 | Loss: 0.00001529
Iteration 136/1000 | Loss: 0.00001529
Iteration 137/1000 | Loss: 0.00001529
Iteration 138/1000 | Loss: 0.00001529
Iteration 139/1000 | Loss: 0.00001529
Iteration 140/1000 | Loss: 0.00001529
Iteration 141/1000 | Loss: 0.00001529
Iteration 142/1000 | Loss: 0.00001529
Iteration 143/1000 | Loss: 0.00001529
Iteration 144/1000 | Loss: 0.00001529
Iteration 145/1000 | Loss: 0.00001529
Iteration 146/1000 | Loss: 0.00001529
Iteration 147/1000 | Loss: 0.00001529
Iteration 148/1000 | Loss: 0.00001529
Iteration 149/1000 | Loss: 0.00001529
Iteration 150/1000 | Loss: 0.00001529
Iteration 151/1000 | Loss: 0.00001529
Iteration 152/1000 | Loss: 0.00001529
Iteration 153/1000 | Loss: 0.00001529
Iteration 154/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.5286474081221968e-05, 1.5286474081221968e-05, 1.5286474081221968e-05, 1.5286474081221968e-05, 1.5286474081221968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5286474081221968e-05

Optimization complete. Final v2v error: 3.2708098888397217 mm

Highest mean error: 3.7333273887634277 mm for frame 87

Lowest mean error: 2.985029935836792 mm for frame 0

Saving results

Total time: 752.0631244182587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1015
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481603
Iteration 2/25 | Loss: 0.00124996
Iteration 3/25 | Loss: 0.00115796
Iteration 4/25 | Loss: 0.00115063
Iteration 5/25 | Loss: 0.00115063
Iteration 6/25 | Loss: 0.00115063
Iteration 7/25 | Loss: 0.00115063
Iteration 8/25 | Loss: 0.00115063
Iteration 9/25 | Loss: 0.00115063
Iteration 10/25 | Loss: 0.00115063
Iteration 11/25 | Loss: 0.00115063
Iteration 12/25 | Loss: 0.00115063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001150625990703702, 0.001150625990703702, 0.001150625990703702, 0.001150625990703702, 0.001150625990703702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001150625990703702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41589546
Iteration 2/25 | Loss: 0.00076122
Iteration 3/25 | Loss: 0.00076121
Iteration 4/25 | Loss: 0.00076121
Iteration 5/25 | Loss: 0.00076121
Iteration 6/25 | Loss: 0.00076121
Iteration 7/25 | Loss: 0.00076121
Iteration 8/25 | Loss: 0.00076121
Iteration 9/25 | Loss: 0.00076121
Iteration 10/25 | Loss: 0.00076121
Iteration 11/25 | Loss: 0.00076121
Iteration 12/25 | Loss: 0.00076121
Iteration 13/25 | Loss: 0.00076121
Iteration 14/25 | Loss: 0.00076121
Iteration 15/25 | Loss: 0.00076121
Iteration 16/25 | Loss: 0.00076121
Iteration 17/25 | Loss: 0.00076121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007612122572027147, 0.0007612122572027147, 0.0007612122572027147, 0.0007612122572027147, 0.0007612122572027147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007612122572027147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076121
Iteration 2/1000 | Loss: 0.00002104
Iteration 3/1000 | Loss: 0.00001703
Iteration 4/1000 | Loss: 0.00001576
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001458
Iteration 7/1000 | Loss: 0.00001421
Iteration 8/1000 | Loss: 0.00001386
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001351
Iteration 12/1000 | Loss: 0.00001335
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001315
Iteration 15/1000 | Loss: 0.00001300
Iteration 16/1000 | Loss: 0.00001290
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00001284
Iteration 21/1000 | Loss: 0.00001284
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001283
Iteration 25/1000 | Loss: 0.00001283
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001282
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001281
Iteration 31/1000 | Loss: 0.00001281
Iteration 32/1000 | Loss: 0.00001280
Iteration 33/1000 | Loss: 0.00001280
Iteration 34/1000 | Loss: 0.00001280
Iteration 35/1000 | Loss: 0.00001279
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001278
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001275
Iteration 40/1000 | Loss: 0.00001274
Iteration 41/1000 | Loss: 0.00001274
Iteration 42/1000 | Loss: 0.00001274
Iteration 43/1000 | Loss: 0.00001273
Iteration 44/1000 | Loss: 0.00001273
Iteration 45/1000 | Loss: 0.00001269
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001261
Iteration 57/1000 | Loss: 0.00001260
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001251
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001250
Iteration 96/1000 | Loss: 0.00001250
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001249
Iteration 101/1000 | Loss: 0.00001249
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001248
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001247
Iteration 120/1000 | Loss: 0.00001247
Iteration 121/1000 | Loss: 0.00001247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.2473856259020977e-05, 1.2473856259020977e-05, 1.2473856259020977e-05, 1.2473856259020977e-05, 1.2473856259020977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2473856259020977e-05

Optimization complete. Final v2v error: 3.025050640106201 mm

Highest mean error: 3.2699460983276367 mm for frame 219

Lowest mean error: 2.8451755046844482 mm for frame 115

Saving results

Total time: 1148.3043365478516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1053
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445454
Iteration 2/25 | Loss: 0.00124397
Iteration 3/25 | Loss: 0.00118390
Iteration 4/25 | Loss: 0.00117471
Iteration 5/25 | Loss: 0.00117072
Iteration 6/25 | Loss: 0.00117009
Iteration 7/25 | Loss: 0.00117009
Iteration 8/25 | Loss: 0.00117009
Iteration 9/25 | Loss: 0.00117009
Iteration 10/25 | Loss: 0.00117009
Iteration 11/25 | Loss: 0.00117009
Iteration 12/25 | Loss: 0.00117009
Iteration 13/25 | Loss: 0.00117009
Iteration 14/25 | Loss: 0.00117009
Iteration 15/25 | Loss: 0.00117009
Iteration 16/25 | Loss: 0.00117009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011700874892994761, 0.0011700874892994761, 0.0011700874892994761, 0.0011700874892994761, 0.0011700874892994761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011700874892994761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47072947
Iteration 2/25 | Loss: 0.00085225
Iteration 3/25 | Loss: 0.00085224
Iteration 4/25 | Loss: 0.00085224
Iteration 5/25 | Loss: 0.00085224
Iteration 6/25 | Loss: 0.00085224
Iteration 7/25 | Loss: 0.00085224
Iteration 8/25 | Loss: 0.00085224
Iteration 9/25 | Loss: 0.00085224
Iteration 10/25 | Loss: 0.00085224
Iteration 11/25 | Loss: 0.00085224
Iteration 12/25 | Loss: 0.00085224
Iteration 13/25 | Loss: 0.00085224
Iteration 14/25 | Loss: 0.00085224
Iteration 15/25 | Loss: 0.00085224
Iteration 16/25 | Loss: 0.00085224
Iteration 17/25 | Loss: 0.00085224
Iteration 18/25 | Loss: 0.00085224
Iteration 19/25 | Loss: 0.00085224
Iteration 20/25 | Loss: 0.00085224
Iteration 21/25 | Loss: 0.00085224
Iteration 22/25 | Loss: 0.00085224
Iteration 23/25 | Loss: 0.00085224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008522406569682062, 0.0008522406569682062, 0.0008522406569682062, 0.0008522406569682062, 0.0008522406569682062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008522406569682062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085224
Iteration 2/1000 | Loss: 0.00002555
Iteration 3/1000 | Loss: 0.00001723
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001410
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001350
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001332
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001332
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001331
Iteration 23/1000 | Loss: 0.00001327
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001311
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001305
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001301
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001300
Iteration 41/1000 | Loss: 0.00001300
Iteration 42/1000 | Loss: 0.00001300
Iteration 43/1000 | Loss: 0.00001299
Iteration 44/1000 | Loss: 0.00001299
Iteration 45/1000 | Loss: 0.00001298
Iteration 46/1000 | Loss: 0.00001298
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001297
Iteration 50/1000 | Loss: 0.00001297
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001297
Iteration 53/1000 | Loss: 0.00001297
Iteration 54/1000 | Loss: 0.00001296
Iteration 55/1000 | Loss: 0.00001296
Iteration 56/1000 | Loss: 0.00001295
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001294
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001293
Iteration 61/1000 | Loss: 0.00001293
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001284
Iteration 75/1000 | Loss: 0.00001284
Iteration 76/1000 | Loss: 0.00001284
Iteration 77/1000 | Loss: 0.00001284
Iteration 78/1000 | Loss: 0.00001283
Iteration 79/1000 | Loss: 0.00001283
Iteration 80/1000 | Loss: 0.00001283
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001280
Iteration 84/1000 | Loss: 0.00001279
Iteration 85/1000 | Loss: 0.00001279
Iteration 86/1000 | Loss: 0.00001279
Iteration 87/1000 | Loss: 0.00001278
Iteration 88/1000 | Loss: 0.00001277
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001275
Iteration 92/1000 | Loss: 0.00001275
Iteration 93/1000 | Loss: 0.00001274
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001273
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001273
Iteration 102/1000 | Loss: 0.00001272
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001271
Iteration 105/1000 | Loss: 0.00001271
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001271
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001271
Iteration 110/1000 | Loss: 0.00001271
Iteration 111/1000 | Loss: 0.00001271
Iteration 112/1000 | Loss: 0.00001271
Iteration 113/1000 | Loss: 0.00001271
Iteration 114/1000 | Loss: 0.00001271
Iteration 115/1000 | Loss: 0.00001271
Iteration 116/1000 | Loss: 0.00001271
Iteration 117/1000 | Loss: 0.00001270
Iteration 118/1000 | Loss: 0.00001270
Iteration 119/1000 | Loss: 0.00001270
Iteration 120/1000 | Loss: 0.00001270
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001269
Iteration 124/1000 | Loss: 0.00001269
Iteration 125/1000 | Loss: 0.00001269
Iteration 126/1000 | Loss: 0.00001269
Iteration 127/1000 | Loss: 0.00001269
Iteration 128/1000 | Loss: 0.00001269
Iteration 129/1000 | Loss: 0.00001269
Iteration 130/1000 | Loss: 0.00001269
Iteration 131/1000 | Loss: 0.00001269
Iteration 132/1000 | Loss: 0.00001269
Iteration 133/1000 | Loss: 0.00001269
Iteration 134/1000 | Loss: 0.00001269
Iteration 135/1000 | Loss: 0.00001269
Iteration 136/1000 | Loss: 0.00001269
Iteration 137/1000 | Loss: 0.00001268
Iteration 138/1000 | Loss: 0.00001268
Iteration 139/1000 | Loss: 0.00001268
Iteration 140/1000 | Loss: 0.00001268
Iteration 141/1000 | Loss: 0.00001268
Iteration 142/1000 | Loss: 0.00001268
Iteration 143/1000 | Loss: 0.00001268
Iteration 144/1000 | Loss: 0.00001268
Iteration 145/1000 | Loss: 0.00001268
Iteration 146/1000 | Loss: 0.00001268
Iteration 147/1000 | Loss: 0.00001268
Iteration 148/1000 | Loss: 0.00001267
Iteration 149/1000 | Loss: 0.00001267
Iteration 150/1000 | Loss: 0.00001267
Iteration 151/1000 | Loss: 0.00001266
Iteration 152/1000 | Loss: 0.00001266
Iteration 153/1000 | Loss: 0.00001266
Iteration 154/1000 | Loss: 0.00001266
Iteration 155/1000 | Loss: 0.00001266
Iteration 156/1000 | Loss: 0.00001266
Iteration 157/1000 | Loss: 0.00001266
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001266
Iteration 161/1000 | Loss: 0.00001266
Iteration 162/1000 | Loss: 0.00001266
Iteration 163/1000 | Loss: 0.00001266
Iteration 164/1000 | Loss: 0.00001266
Iteration 165/1000 | Loss: 0.00001266
Iteration 166/1000 | Loss: 0.00001266
Iteration 167/1000 | Loss: 0.00001266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.2658782907237764e-05, 1.2658782907237764e-05, 1.2658782907237764e-05, 1.2658782907237764e-05, 1.2658782907237764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2658782907237764e-05

Optimization complete. Final v2v error: 3.0191328525543213 mm

Highest mean error: 3.3215036392211914 mm for frame 174

Lowest mean error: 2.716360569000244 mm for frame 98

Saving results

Total time: 1143.826826095581
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1037
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00708655
Iteration 2/25 | Loss: 0.00141444
Iteration 3/25 | Loss: 0.00127965
Iteration 4/25 | Loss: 0.00126410
Iteration 5/25 | Loss: 0.00126116
Iteration 6/25 | Loss: 0.00126116
Iteration 7/25 | Loss: 0.00126116
Iteration 8/25 | Loss: 0.00126116
Iteration 9/25 | Loss: 0.00126116
Iteration 10/25 | Loss: 0.00126116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012611569836735725, 0.0012611569836735725, 0.0012611569836735725, 0.0012611569836735725, 0.0012611569836735725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012611569836735725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.00796795
Iteration 2/25 | Loss: 0.00103298
Iteration 3/25 | Loss: 0.00103296
Iteration 4/25 | Loss: 0.00103296
Iteration 5/25 | Loss: 0.00103296
Iteration 6/25 | Loss: 0.00103296
Iteration 7/25 | Loss: 0.00103296
Iteration 8/25 | Loss: 0.00103296
Iteration 9/25 | Loss: 0.00103296
Iteration 10/25 | Loss: 0.00103296
Iteration 11/25 | Loss: 0.00103296
Iteration 12/25 | Loss: 0.00103296
Iteration 13/25 | Loss: 0.00103296
Iteration 14/25 | Loss: 0.00103296
Iteration 15/25 | Loss: 0.00103296
Iteration 16/25 | Loss: 0.00103296
Iteration 17/25 | Loss: 0.00103296
Iteration 18/25 | Loss: 0.00103296
Iteration 19/25 | Loss: 0.00103296
Iteration 20/25 | Loss: 0.00103296
Iteration 21/25 | Loss: 0.00103296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010329559445381165, 0.0010329559445381165, 0.0010329559445381165, 0.0010329559445381165, 0.0010329559445381165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010329559445381165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103296
Iteration 2/1000 | Loss: 0.00004012
Iteration 3/1000 | Loss: 0.00002897
Iteration 4/1000 | Loss: 0.00002650
Iteration 5/1000 | Loss: 0.00002487
Iteration 6/1000 | Loss: 0.00002386
Iteration 7/1000 | Loss: 0.00002331
Iteration 8/1000 | Loss: 0.00002294
Iteration 9/1000 | Loss: 0.00002267
Iteration 10/1000 | Loss: 0.00002246
Iteration 11/1000 | Loss: 0.00002240
Iteration 12/1000 | Loss: 0.00002239
Iteration 13/1000 | Loss: 0.00002235
Iteration 14/1000 | Loss: 0.00002229
Iteration 15/1000 | Loss: 0.00002227
Iteration 16/1000 | Loss: 0.00002225
Iteration 17/1000 | Loss: 0.00002224
Iteration 18/1000 | Loss: 0.00002219
Iteration 19/1000 | Loss: 0.00002206
Iteration 20/1000 | Loss: 0.00002204
Iteration 21/1000 | Loss: 0.00002203
Iteration 22/1000 | Loss: 0.00002203
Iteration 23/1000 | Loss: 0.00002202
Iteration 24/1000 | Loss: 0.00002200
Iteration 25/1000 | Loss: 0.00002199
Iteration 26/1000 | Loss: 0.00002199
Iteration 27/1000 | Loss: 0.00002198
Iteration 28/1000 | Loss: 0.00002198
Iteration 29/1000 | Loss: 0.00002197
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002197
Iteration 32/1000 | Loss: 0.00002196
Iteration 33/1000 | Loss: 0.00002196
Iteration 34/1000 | Loss: 0.00002196
Iteration 35/1000 | Loss: 0.00002195
Iteration 36/1000 | Loss: 0.00002195
Iteration 37/1000 | Loss: 0.00002195
Iteration 38/1000 | Loss: 0.00002194
Iteration 39/1000 | Loss: 0.00002194
Iteration 40/1000 | Loss: 0.00002194
Iteration 41/1000 | Loss: 0.00002193
Iteration 42/1000 | Loss: 0.00002193
Iteration 43/1000 | Loss: 0.00002192
Iteration 44/1000 | Loss: 0.00002192
Iteration 45/1000 | Loss: 0.00002191
Iteration 46/1000 | Loss: 0.00002191
Iteration 47/1000 | Loss: 0.00002191
Iteration 48/1000 | Loss: 0.00002191
Iteration 49/1000 | Loss: 0.00002190
Iteration 50/1000 | Loss: 0.00002190
Iteration 51/1000 | Loss: 0.00002190
Iteration 52/1000 | Loss: 0.00002189
Iteration 53/1000 | Loss: 0.00002189
Iteration 54/1000 | Loss: 0.00002189
Iteration 55/1000 | Loss: 0.00002189
Iteration 56/1000 | Loss: 0.00002188
Iteration 57/1000 | Loss: 0.00002188
Iteration 58/1000 | Loss: 0.00002188
Iteration 59/1000 | Loss: 0.00002188
Iteration 60/1000 | Loss: 0.00002187
Iteration 61/1000 | Loss: 0.00002187
Iteration 62/1000 | Loss: 0.00002187
Iteration 63/1000 | Loss: 0.00002186
Iteration 64/1000 | Loss: 0.00002186
Iteration 65/1000 | Loss: 0.00002186
Iteration 66/1000 | Loss: 0.00002186
Iteration 67/1000 | Loss: 0.00002186
Iteration 68/1000 | Loss: 0.00002185
Iteration 69/1000 | Loss: 0.00002185
Iteration 70/1000 | Loss: 0.00002185
Iteration 71/1000 | Loss: 0.00002185
Iteration 72/1000 | Loss: 0.00002185
Iteration 73/1000 | Loss: 0.00002184
Iteration 74/1000 | Loss: 0.00002184
Iteration 75/1000 | Loss: 0.00002184
Iteration 76/1000 | Loss: 0.00002183
Iteration 77/1000 | Loss: 0.00002183
Iteration 78/1000 | Loss: 0.00002180
Iteration 79/1000 | Loss: 0.00002180
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002176
Iteration 83/1000 | Loss: 0.00002173
Iteration 84/1000 | Loss: 0.00002173
Iteration 85/1000 | Loss: 0.00002173
Iteration 86/1000 | Loss: 0.00002172
Iteration 87/1000 | Loss: 0.00002172
Iteration 88/1000 | Loss: 0.00002172
Iteration 89/1000 | Loss: 0.00002171
Iteration 90/1000 | Loss: 0.00002171
Iteration 91/1000 | Loss: 0.00002171
Iteration 92/1000 | Loss: 0.00002168
Iteration 93/1000 | Loss: 0.00002167
Iteration 94/1000 | Loss: 0.00002167
Iteration 95/1000 | Loss: 0.00002166
Iteration 96/1000 | Loss: 0.00002164
Iteration 97/1000 | Loss: 0.00002164
Iteration 98/1000 | Loss: 0.00002163
Iteration 99/1000 | Loss: 0.00002162
Iteration 100/1000 | Loss: 0.00002161
Iteration 101/1000 | Loss: 0.00002161
Iteration 102/1000 | Loss: 0.00002159
Iteration 103/1000 | Loss: 0.00002158
Iteration 104/1000 | Loss: 0.00002158
Iteration 105/1000 | Loss: 0.00002158
Iteration 106/1000 | Loss: 0.00002158
Iteration 107/1000 | Loss: 0.00002157
Iteration 108/1000 | Loss: 0.00002157
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002157
Iteration 111/1000 | Loss: 0.00002157
Iteration 112/1000 | Loss: 0.00002157
Iteration 113/1000 | Loss: 0.00002156
Iteration 114/1000 | Loss: 0.00002156
Iteration 115/1000 | Loss: 0.00002156
Iteration 116/1000 | Loss: 0.00002156
Iteration 117/1000 | Loss: 0.00002156
Iteration 118/1000 | Loss: 0.00002155
Iteration 119/1000 | Loss: 0.00002155
Iteration 120/1000 | Loss: 0.00002155
Iteration 121/1000 | Loss: 0.00002155
Iteration 122/1000 | Loss: 0.00002155
Iteration 123/1000 | Loss: 0.00002155
Iteration 124/1000 | Loss: 0.00002155
Iteration 125/1000 | Loss: 0.00002155
Iteration 126/1000 | Loss: 0.00002155
Iteration 127/1000 | Loss: 0.00002155
Iteration 128/1000 | Loss: 0.00002154
Iteration 129/1000 | Loss: 0.00002154
Iteration 130/1000 | Loss: 0.00002153
Iteration 131/1000 | Loss: 0.00002153
Iteration 132/1000 | Loss: 0.00002153
Iteration 133/1000 | Loss: 0.00002153
Iteration 134/1000 | Loss: 0.00002152
Iteration 135/1000 | Loss: 0.00002152
Iteration 136/1000 | Loss: 0.00002152
Iteration 137/1000 | Loss: 0.00002152
Iteration 138/1000 | Loss: 0.00002152
Iteration 139/1000 | Loss: 0.00002152
Iteration 140/1000 | Loss: 0.00002152
Iteration 141/1000 | Loss: 0.00002151
Iteration 142/1000 | Loss: 0.00002151
Iteration 143/1000 | Loss: 0.00002151
Iteration 144/1000 | Loss: 0.00002151
Iteration 145/1000 | Loss: 0.00002151
Iteration 146/1000 | Loss: 0.00002151
Iteration 147/1000 | Loss: 0.00002151
Iteration 148/1000 | Loss: 0.00002150
Iteration 149/1000 | Loss: 0.00002150
Iteration 150/1000 | Loss: 0.00002150
Iteration 151/1000 | Loss: 0.00002150
Iteration 152/1000 | Loss: 0.00002149
Iteration 153/1000 | Loss: 0.00002149
Iteration 154/1000 | Loss: 0.00002149
Iteration 155/1000 | Loss: 0.00002149
Iteration 156/1000 | Loss: 0.00002148
Iteration 157/1000 | Loss: 0.00002148
Iteration 158/1000 | Loss: 0.00002148
Iteration 159/1000 | Loss: 0.00002148
Iteration 160/1000 | Loss: 0.00002148
Iteration 161/1000 | Loss: 0.00002148
Iteration 162/1000 | Loss: 0.00002148
Iteration 163/1000 | Loss: 0.00002148
Iteration 164/1000 | Loss: 0.00002148
Iteration 165/1000 | Loss: 0.00002148
Iteration 166/1000 | Loss: 0.00002148
Iteration 167/1000 | Loss: 0.00002147
Iteration 168/1000 | Loss: 0.00002147
Iteration 169/1000 | Loss: 0.00002147
Iteration 170/1000 | Loss: 0.00002147
Iteration 171/1000 | Loss: 0.00002147
Iteration 172/1000 | Loss: 0.00002146
Iteration 173/1000 | Loss: 0.00002146
Iteration 174/1000 | Loss: 0.00002146
Iteration 175/1000 | Loss: 0.00002146
Iteration 176/1000 | Loss: 0.00002146
Iteration 177/1000 | Loss: 0.00002146
Iteration 178/1000 | Loss: 0.00002146
Iteration 179/1000 | Loss: 0.00002146
Iteration 180/1000 | Loss: 0.00002145
Iteration 181/1000 | Loss: 0.00002145
Iteration 182/1000 | Loss: 0.00002145
Iteration 183/1000 | Loss: 0.00002145
Iteration 184/1000 | Loss: 0.00002145
Iteration 185/1000 | Loss: 0.00002145
Iteration 186/1000 | Loss: 0.00002145
Iteration 187/1000 | Loss: 0.00002145
Iteration 188/1000 | Loss: 0.00002145
Iteration 189/1000 | Loss: 0.00002145
Iteration 190/1000 | Loss: 0.00002145
Iteration 191/1000 | Loss: 0.00002145
Iteration 192/1000 | Loss: 0.00002145
Iteration 193/1000 | Loss: 0.00002145
Iteration 194/1000 | Loss: 0.00002145
Iteration 195/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.1450146959978156e-05, 2.1450146959978156e-05, 2.1450146959978156e-05, 2.1450146959978156e-05, 2.1450146959978156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1450146959978156e-05

Optimization complete. Final v2v error: 3.8517165184020996 mm

Highest mean error: 4.217266082763672 mm for frame 63

Lowest mean error: 3.387226104736328 mm for frame 239

Saving results

Total time: 1324.6482229232788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1051
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842540
Iteration 2/25 | Loss: 0.00122989
Iteration 3/25 | Loss: 0.00114464
Iteration 4/25 | Loss: 0.00113493
Iteration 5/25 | Loss: 0.00113238
Iteration 6/25 | Loss: 0.00113230
Iteration 7/25 | Loss: 0.00113230
Iteration 8/25 | Loss: 0.00113230
Iteration 9/25 | Loss: 0.00113230
Iteration 10/25 | Loss: 0.00113230
Iteration 11/25 | Loss: 0.00113230
Iteration 12/25 | Loss: 0.00113230
Iteration 13/25 | Loss: 0.00113230
Iteration 14/25 | Loss: 0.00113230
Iteration 15/25 | Loss: 0.00113230
Iteration 16/25 | Loss: 0.00113230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001132303150370717, 0.001132303150370717, 0.001132303150370717, 0.001132303150370717, 0.001132303150370717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001132303150370717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.47973537
Iteration 2/25 | Loss: 0.00083010
Iteration 3/25 | Loss: 0.00083009
Iteration 4/25 | Loss: 0.00083009
Iteration 5/25 | Loss: 0.00083009
Iteration 6/25 | Loss: 0.00083009
Iteration 7/25 | Loss: 0.00083009
Iteration 8/25 | Loss: 0.00083009
Iteration 9/25 | Loss: 0.00083009
Iteration 10/25 | Loss: 0.00083009
Iteration 11/25 | Loss: 0.00083009
Iteration 12/25 | Loss: 0.00083009
Iteration 13/25 | Loss: 0.00083009
Iteration 14/25 | Loss: 0.00083009
Iteration 15/25 | Loss: 0.00083009
Iteration 16/25 | Loss: 0.00083009
Iteration 17/25 | Loss: 0.00083009
Iteration 18/25 | Loss: 0.00083009
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008300862973555923, 0.0008300862973555923, 0.0008300862973555923, 0.0008300862973555923, 0.0008300862973555923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008300862973555923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083009
Iteration 2/1000 | Loss: 0.00002327
Iteration 3/1000 | Loss: 0.00001767
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001584
Iteration 6/1000 | Loss: 0.00001518
Iteration 7/1000 | Loss: 0.00001464
Iteration 8/1000 | Loss: 0.00001437
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001398
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001374
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001357
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001357
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001353
Iteration 28/1000 | Loss: 0.00001353
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001352
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001344
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001334
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001332
Iteration 65/1000 | Loss: 0.00001332
Iteration 66/1000 | Loss: 0.00001332
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001331
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001330
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001328
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001325
Iteration 110/1000 | Loss: 0.00001325
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001323
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001320
Iteration 127/1000 | Loss: 0.00001320
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001318
Iteration 135/1000 | Loss: 0.00001318
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001315
Iteration 144/1000 | Loss: 0.00001315
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001312
Iteration 159/1000 | Loss: 0.00001312
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001312
Iteration 163/1000 | Loss: 0.00001312
Iteration 164/1000 | Loss: 0.00001312
Iteration 165/1000 | Loss: 0.00001312
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001311
Iteration 169/1000 | Loss: 0.00001311
Iteration 170/1000 | Loss: 0.00001311
Iteration 171/1000 | Loss: 0.00001311
Iteration 172/1000 | Loss: 0.00001311
Iteration 173/1000 | Loss: 0.00001310
Iteration 174/1000 | Loss: 0.00001310
Iteration 175/1000 | Loss: 0.00001310
Iteration 176/1000 | Loss: 0.00001310
Iteration 177/1000 | Loss: 0.00001310
Iteration 178/1000 | Loss: 0.00001310
Iteration 179/1000 | Loss: 0.00001310
Iteration 180/1000 | Loss: 0.00001310
Iteration 181/1000 | Loss: 0.00001310
Iteration 182/1000 | Loss: 0.00001310
Iteration 183/1000 | Loss: 0.00001310
Iteration 184/1000 | Loss: 0.00001310
Iteration 185/1000 | Loss: 0.00001310
Iteration 186/1000 | Loss: 0.00001310
Iteration 187/1000 | Loss: 0.00001310
Iteration 188/1000 | Loss: 0.00001310
Iteration 189/1000 | Loss: 0.00001310
Iteration 190/1000 | Loss: 0.00001310
Iteration 191/1000 | Loss: 0.00001310
Iteration 192/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.3095333997625858e-05, 1.3095333997625858e-05, 1.3095333997625858e-05, 1.3095333997625858e-05, 1.3095333997625858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3095333997625858e-05

Optimization complete. Final v2v error: 3.0912609100341797 mm

Highest mean error: 3.45288348197937 mm for frame 111

Lowest mean error: 2.7092695236206055 mm for frame 167

Saving results

Total time: 931.7779610157013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1030
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568426
Iteration 2/25 | Loss: 0.00178284
Iteration 3/25 | Loss: 0.00114350
Iteration 4/25 | Loss: 0.00105825
Iteration 5/25 | Loss: 0.00103526
Iteration 6/25 | Loss: 0.00102967
Iteration 7/25 | Loss: 0.00102796
Iteration 8/25 | Loss: 0.00102738
Iteration 9/25 | Loss: 0.00102738
Iteration 10/25 | Loss: 0.00102738
Iteration 11/25 | Loss: 0.00102738
Iteration 12/25 | Loss: 0.00102738
Iteration 13/25 | Loss: 0.00102738
Iteration 14/25 | Loss: 0.00102738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010273762745782733, 0.0010273762745782733, 0.0010273762745782733, 0.0010273762745782733, 0.0010273762745782733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010273762745782733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08754647
Iteration 2/25 | Loss: 0.00100590
Iteration 3/25 | Loss: 0.00100589
Iteration 4/25 | Loss: 0.00100589
Iteration 5/25 | Loss: 0.00100589
Iteration 6/25 | Loss: 0.00100589
Iteration 7/25 | Loss: 0.00100589
Iteration 8/25 | Loss: 0.00100589
Iteration 9/25 | Loss: 0.00100589
Iteration 10/25 | Loss: 0.00100589
Iteration 11/25 | Loss: 0.00100589
Iteration 12/25 | Loss: 0.00100589
Iteration 13/25 | Loss: 0.00100589
Iteration 14/25 | Loss: 0.00100589
Iteration 15/25 | Loss: 0.00100589
Iteration 16/25 | Loss: 0.00100589
Iteration 17/25 | Loss: 0.00100589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010058850748464465, 0.0010058850748464465, 0.0010058850748464465, 0.0010058850748464465, 0.0010058850748464465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010058850748464465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100589
Iteration 2/1000 | Loss: 0.00010141
Iteration 3/1000 | Loss: 0.00006919
Iteration 4/1000 | Loss: 0.00006206
Iteration 5/1000 | Loss: 0.00005799
Iteration 6/1000 | Loss: 0.00005593
Iteration 7/1000 | Loss: 0.00005452
Iteration 8/1000 | Loss: 0.00005342
Iteration 9/1000 | Loss: 0.00029380
Iteration 10/1000 | Loss: 0.00030152
Iteration 11/1000 | Loss: 0.00023200
Iteration 12/1000 | Loss: 0.00038461
Iteration 13/1000 | Loss: 0.00006917
Iteration 14/1000 | Loss: 0.00025987
Iteration 15/1000 | Loss: 0.00006122
Iteration 16/1000 | Loss: 0.00029492
Iteration 17/1000 | Loss: 0.00007340
Iteration 18/1000 | Loss: 0.00005786
Iteration 19/1000 | Loss: 0.00005481
Iteration 20/1000 | Loss: 0.00005297
Iteration 21/1000 | Loss: 0.00005822
Iteration 22/1000 | Loss: 0.00028367
Iteration 23/1000 | Loss: 0.00006370
Iteration 24/1000 | Loss: 0.00005467
Iteration 25/1000 | Loss: 0.00005252
Iteration 26/1000 | Loss: 0.00005071
Iteration 27/1000 | Loss: 0.00004843
Iteration 28/1000 | Loss: 0.00004715
Iteration 29/1000 | Loss: 0.00004661
Iteration 30/1000 | Loss: 0.00027741
Iteration 31/1000 | Loss: 0.00028629
Iteration 32/1000 | Loss: 0.00052614
Iteration 33/1000 | Loss: 0.00041743
Iteration 34/1000 | Loss: 0.00049501
Iteration 35/1000 | Loss: 0.00031411
Iteration 36/1000 | Loss: 0.00025797
Iteration 37/1000 | Loss: 0.00021792
Iteration 38/1000 | Loss: 0.00005791
Iteration 39/1000 | Loss: 0.00028437
Iteration 40/1000 | Loss: 0.00030597
Iteration 41/1000 | Loss: 0.00031131
Iteration 42/1000 | Loss: 0.00008846
Iteration 43/1000 | Loss: 0.00005806
Iteration 44/1000 | Loss: 0.00005292
Iteration 45/1000 | Loss: 0.00029423
Iteration 46/1000 | Loss: 0.00007787
Iteration 47/1000 | Loss: 0.00005284
Iteration 48/1000 | Loss: 0.00004932
Iteration 49/1000 | Loss: 0.00007203
Iteration 50/1000 | Loss: 0.00029876
Iteration 51/1000 | Loss: 0.00024979
Iteration 52/1000 | Loss: 0.00005772
Iteration 53/1000 | Loss: 0.00004779
Iteration 54/1000 | Loss: 0.00004355
Iteration 55/1000 | Loss: 0.00005182
Iteration 56/1000 | Loss: 0.00004180
Iteration 57/1000 | Loss: 0.00004041
Iteration 58/1000 | Loss: 0.00003928
Iteration 59/1000 | Loss: 0.00051104
Iteration 60/1000 | Loss: 0.00007521
Iteration 61/1000 | Loss: 0.00004905
Iteration 62/1000 | Loss: 0.00004248
Iteration 63/1000 | Loss: 0.00003900
Iteration 64/1000 | Loss: 0.00003736
Iteration 65/1000 | Loss: 0.00003677
Iteration 66/1000 | Loss: 0.00003638
Iteration 67/1000 | Loss: 0.00003593
Iteration 68/1000 | Loss: 0.00003567
Iteration 69/1000 | Loss: 0.00003543
Iteration 70/1000 | Loss: 0.00003537
Iteration 71/1000 | Loss: 0.00003532
Iteration 72/1000 | Loss: 0.00003532
Iteration 73/1000 | Loss: 0.00003532
Iteration 74/1000 | Loss: 0.00003531
Iteration 75/1000 | Loss: 0.00003531
Iteration 76/1000 | Loss: 0.00003531
Iteration 77/1000 | Loss: 0.00003531
Iteration 78/1000 | Loss: 0.00003530
Iteration 79/1000 | Loss: 0.00003530
Iteration 80/1000 | Loss: 0.00003529
Iteration 81/1000 | Loss: 0.00003528
Iteration 82/1000 | Loss: 0.00003526
Iteration 83/1000 | Loss: 0.00003525
Iteration 84/1000 | Loss: 0.00003525
Iteration 85/1000 | Loss: 0.00003525
Iteration 86/1000 | Loss: 0.00003524
Iteration 87/1000 | Loss: 0.00003524
Iteration 88/1000 | Loss: 0.00003523
Iteration 89/1000 | Loss: 0.00003523
Iteration 90/1000 | Loss: 0.00003523
Iteration 91/1000 | Loss: 0.00003522
Iteration 92/1000 | Loss: 0.00003522
Iteration 93/1000 | Loss: 0.00003522
Iteration 94/1000 | Loss: 0.00003522
Iteration 95/1000 | Loss: 0.00003522
Iteration 96/1000 | Loss: 0.00003522
Iteration 97/1000 | Loss: 0.00003521
Iteration 98/1000 | Loss: 0.00003521
Iteration 99/1000 | Loss: 0.00003518
Iteration 100/1000 | Loss: 0.00003518
Iteration 101/1000 | Loss: 0.00003515
Iteration 102/1000 | Loss: 0.00003515
Iteration 103/1000 | Loss: 0.00003512
Iteration 104/1000 | Loss: 0.00003511
Iteration 105/1000 | Loss: 0.00003511
Iteration 106/1000 | Loss: 0.00003511
Iteration 107/1000 | Loss: 0.00003510
Iteration 108/1000 | Loss: 0.00003510
Iteration 109/1000 | Loss: 0.00003510
Iteration 110/1000 | Loss: 0.00003509
Iteration 111/1000 | Loss: 0.00003508
Iteration 112/1000 | Loss: 0.00003508
Iteration 113/1000 | Loss: 0.00003507
Iteration 114/1000 | Loss: 0.00003507
Iteration 115/1000 | Loss: 0.00003506
Iteration 116/1000 | Loss: 0.00003506
Iteration 117/1000 | Loss: 0.00003506
Iteration 118/1000 | Loss: 0.00003505
Iteration 119/1000 | Loss: 0.00003505
Iteration 120/1000 | Loss: 0.00003505
Iteration 121/1000 | Loss: 0.00003504
Iteration 122/1000 | Loss: 0.00003503
Iteration 123/1000 | Loss: 0.00003503
Iteration 124/1000 | Loss: 0.00003502
Iteration 125/1000 | Loss: 0.00003501
Iteration 126/1000 | Loss: 0.00003501
Iteration 127/1000 | Loss: 0.00003501
Iteration 128/1000 | Loss: 0.00003501
Iteration 129/1000 | Loss: 0.00003501
Iteration 130/1000 | Loss: 0.00003500
Iteration 131/1000 | Loss: 0.00003500
Iteration 132/1000 | Loss: 0.00003500
Iteration 133/1000 | Loss: 0.00003500
Iteration 134/1000 | Loss: 0.00003500
Iteration 135/1000 | Loss: 0.00003500
Iteration 136/1000 | Loss: 0.00003500
Iteration 137/1000 | Loss: 0.00003500
Iteration 138/1000 | Loss: 0.00003500
Iteration 139/1000 | Loss: 0.00003500
Iteration 140/1000 | Loss: 0.00003500
Iteration 141/1000 | Loss: 0.00003499
Iteration 142/1000 | Loss: 0.00003498
Iteration 143/1000 | Loss: 0.00003498
Iteration 144/1000 | Loss: 0.00003498
Iteration 145/1000 | Loss: 0.00003498
Iteration 146/1000 | Loss: 0.00003497
Iteration 147/1000 | Loss: 0.00003497
Iteration 148/1000 | Loss: 0.00003497
Iteration 149/1000 | Loss: 0.00003496
Iteration 150/1000 | Loss: 0.00003496
Iteration 151/1000 | Loss: 0.00003496
Iteration 152/1000 | Loss: 0.00003495
Iteration 153/1000 | Loss: 0.00003495
Iteration 154/1000 | Loss: 0.00003495
Iteration 155/1000 | Loss: 0.00003494
Iteration 156/1000 | Loss: 0.00003494
Iteration 157/1000 | Loss: 0.00003494
Iteration 158/1000 | Loss: 0.00003494
Iteration 159/1000 | Loss: 0.00003494
Iteration 160/1000 | Loss: 0.00003494
Iteration 161/1000 | Loss: 0.00003493
Iteration 162/1000 | Loss: 0.00003493
Iteration 163/1000 | Loss: 0.00003493
Iteration 164/1000 | Loss: 0.00003493
Iteration 165/1000 | Loss: 0.00003493
Iteration 166/1000 | Loss: 0.00003493
Iteration 167/1000 | Loss: 0.00003493
Iteration 168/1000 | Loss: 0.00003493
Iteration 169/1000 | Loss: 0.00003493
Iteration 170/1000 | Loss: 0.00003493
Iteration 171/1000 | Loss: 0.00003493
Iteration 172/1000 | Loss: 0.00003493
Iteration 173/1000 | Loss: 0.00003492
Iteration 174/1000 | Loss: 0.00003492
Iteration 175/1000 | Loss: 0.00003492
Iteration 176/1000 | Loss: 0.00003492
Iteration 177/1000 | Loss: 0.00003492
Iteration 178/1000 | Loss: 0.00003492
Iteration 179/1000 | Loss: 0.00003492
Iteration 180/1000 | Loss: 0.00003492
Iteration 181/1000 | Loss: 0.00003492
Iteration 182/1000 | Loss: 0.00003492
Iteration 183/1000 | Loss: 0.00003492
Iteration 184/1000 | Loss: 0.00003492
Iteration 185/1000 | Loss: 0.00003492
Iteration 186/1000 | Loss: 0.00003492
Iteration 187/1000 | Loss: 0.00003492
Iteration 188/1000 | Loss: 0.00003492
Iteration 189/1000 | Loss: 0.00003491
Iteration 190/1000 | Loss: 0.00003491
Iteration 191/1000 | Loss: 0.00003491
Iteration 192/1000 | Loss: 0.00003491
Iteration 193/1000 | Loss: 0.00003491
Iteration 194/1000 | Loss: 0.00003491
Iteration 195/1000 | Loss: 0.00003491
Iteration 196/1000 | Loss: 0.00003491
Iteration 197/1000 | Loss: 0.00003491
Iteration 198/1000 | Loss: 0.00003491
Iteration 199/1000 | Loss: 0.00003491
Iteration 200/1000 | Loss: 0.00003491
Iteration 201/1000 | Loss: 0.00003491
Iteration 202/1000 | Loss: 0.00003491
Iteration 203/1000 | Loss: 0.00003491
Iteration 204/1000 | Loss: 0.00003491
Iteration 205/1000 | Loss: 0.00003491
Iteration 206/1000 | Loss: 0.00003491
Iteration 207/1000 | Loss: 0.00003491
Iteration 208/1000 | Loss: 0.00003491
Iteration 209/1000 | Loss: 0.00003490
Iteration 210/1000 | Loss: 0.00003490
Iteration 211/1000 | Loss: 0.00003490
Iteration 212/1000 | Loss: 0.00003490
Iteration 213/1000 | Loss: 0.00003490
Iteration 214/1000 | Loss: 0.00003490
Iteration 215/1000 | Loss: 0.00003490
Iteration 216/1000 | Loss: 0.00003490
Iteration 217/1000 | Loss: 0.00003490
Iteration 218/1000 | Loss: 0.00003490
Iteration 219/1000 | Loss: 0.00003490
Iteration 220/1000 | Loss: 0.00003490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [3.490115705062635e-05, 3.490115705062635e-05, 3.490115705062635e-05, 3.490115705062635e-05, 3.490115705062635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.490115705062635e-05

Optimization complete. Final v2v error: 4.59665060043335 mm

Highest mean error: 5.782192230224609 mm for frame 0

Lowest mean error: 3.4713237285614014 mm for frame 105

Saving results

Total time: 3636.5579795837402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1078
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785356
Iteration 2/25 | Loss: 0.00108110
Iteration 3/25 | Loss: 0.00083178
Iteration 4/25 | Loss: 0.00079374
Iteration 5/25 | Loss: 0.00078358
Iteration 6/25 | Loss: 0.00077974
Iteration 7/25 | Loss: 0.00077869
Iteration 8/25 | Loss: 0.00077869
Iteration 9/25 | Loss: 0.00077869
Iteration 10/25 | Loss: 0.00077869
Iteration 11/25 | Loss: 0.00077869
Iteration 12/25 | Loss: 0.00077869
Iteration 13/25 | Loss: 0.00077869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007786860805936158, 0.0007786860805936158, 0.0007786860805936158, 0.0007786860805936158, 0.0007786860805936158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007786860805936158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.49170446
Iteration 2/25 | Loss: 0.00054453
Iteration 3/25 | Loss: 0.00054453
Iteration 4/25 | Loss: 0.00054453
Iteration 5/25 | Loss: 0.00054453
Iteration 6/25 | Loss: 0.00054453
Iteration 7/25 | Loss: 0.00054453
Iteration 8/25 | Loss: 0.00054453
Iteration 9/25 | Loss: 0.00054453
Iteration 10/25 | Loss: 0.00054453
Iteration 11/25 | Loss: 0.00054453
Iteration 12/25 | Loss: 0.00054453
Iteration 13/25 | Loss: 0.00054453
Iteration 14/25 | Loss: 0.00054453
Iteration 15/25 | Loss: 0.00054453
Iteration 16/25 | Loss: 0.00054453
Iteration 17/25 | Loss: 0.00054453
Iteration 18/25 | Loss: 0.00054453
Iteration 19/25 | Loss: 0.00054453
Iteration 20/25 | Loss: 0.00054453
Iteration 21/25 | Loss: 0.00054453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005445305723696947, 0.0005445305723696947, 0.0005445305723696947, 0.0005445305723696947, 0.0005445305723696947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005445305723696947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054453
Iteration 2/1000 | Loss: 0.00003670
Iteration 3/1000 | Loss: 0.00002100
Iteration 4/1000 | Loss: 0.00001844
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001673
Iteration 7/1000 | Loss: 0.00001633
Iteration 8/1000 | Loss: 0.00001602
Iteration 9/1000 | Loss: 0.00001590
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001565
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001546
Iteration 18/1000 | Loss: 0.00001545
Iteration 19/1000 | Loss: 0.00001538
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001537
Iteration 22/1000 | Loss: 0.00001532
Iteration 23/1000 | Loss: 0.00001530
Iteration 24/1000 | Loss: 0.00001530
Iteration 25/1000 | Loss: 0.00001529
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001527
Iteration 28/1000 | Loss: 0.00001527
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001525
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001525
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001524
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001524
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001523
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001523
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001523
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.522950788057642e-05, 1.522950788057642e-05, 1.522950788057642e-05, 1.522950788057642e-05, 1.522950788057642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.522950788057642e-05

Optimization complete. Final v2v error: 3.2914862632751465 mm

Highest mean error: 4.318068981170654 mm for frame 73

Lowest mean error: 2.889000177383423 mm for frame 36

Saving results

Total time: 1181.8045499324799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1013
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022057
Iteration 2/25 | Loss: 0.00175428
Iteration 3/25 | Loss: 0.00109491
Iteration 4/25 | Loss: 0.00110225
Iteration 5/25 | Loss: 0.00102358
Iteration 6/25 | Loss: 0.00098213
Iteration 7/25 | Loss: 0.00090365
Iteration 8/25 | Loss: 0.00092532
Iteration 9/25 | Loss: 0.00089782
Iteration 10/25 | Loss: 0.00089028
Iteration 11/25 | Loss: 0.00089376
Iteration 12/25 | Loss: 0.00087644
Iteration 13/25 | Loss: 0.00083169
Iteration 14/25 | Loss: 0.00082708
Iteration 15/25 | Loss: 0.00082623
Iteration 16/25 | Loss: 0.00083256
Iteration 17/25 | Loss: 0.00082568
Iteration 18/25 | Loss: 0.00082280
Iteration 19/25 | Loss: 0.00082237
Iteration 20/25 | Loss: 0.00082232
Iteration 21/25 | Loss: 0.00082232
Iteration 22/25 | Loss: 0.00082232
Iteration 23/25 | Loss: 0.00082231
Iteration 24/25 | Loss: 0.00082231
Iteration 25/25 | Loss: 0.00082231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06430173
Iteration 2/25 | Loss: 0.00059308
Iteration 3/25 | Loss: 0.00059307
Iteration 4/25 | Loss: 0.00059307
Iteration 5/25 | Loss: 0.00059307
Iteration 6/25 | Loss: 0.00059307
Iteration 7/25 | Loss: 0.00059307
Iteration 8/25 | Loss: 0.00059307
Iteration 9/25 | Loss: 0.00056261
Iteration 10/25 | Loss: 0.00056260
Iteration 11/25 | Loss: 0.00056260
Iteration 12/25 | Loss: 0.00056260
Iteration 13/25 | Loss: 0.00056260
Iteration 14/25 | Loss: 0.00056259
Iteration 15/25 | Loss: 0.00056259
Iteration 16/25 | Loss: 0.00056259
Iteration 17/25 | Loss: 0.00056259
Iteration 18/25 | Loss: 0.00056259
Iteration 19/25 | Loss: 0.00056259
Iteration 20/25 | Loss: 0.00056259
Iteration 21/25 | Loss: 0.00056259
Iteration 22/25 | Loss: 0.00056259
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005625936901196837, 0.0005625936901196837, 0.0005625936901196837, 0.0005625936901196837, 0.0005625936901196837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005625936901196837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056259
Iteration 2/1000 | Loss: 0.00006505
Iteration 3/1000 | Loss: 0.00002497
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00002052
Iteration 6/1000 | Loss: 0.00001972
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001835
Iteration 11/1000 | Loss: 0.00054216
Iteration 12/1000 | Loss: 0.00002406
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00001845
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001605
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001563
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001556
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001543
Iteration 27/1000 | Loss: 0.00001542
Iteration 28/1000 | Loss: 0.00001542
Iteration 29/1000 | Loss: 0.00001542
Iteration 30/1000 | Loss: 0.00001542
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001542
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001542
Iteration 37/1000 | Loss: 0.00001542
Iteration 38/1000 | Loss: 0.00001542
Iteration 39/1000 | Loss: 0.00001541
Iteration 40/1000 | Loss: 0.00001541
Iteration 41/1000 | Loss: 0.00001541
Iteration 42/1000 | Loss: 0.00001541
Iteration 43/1000 | Loss: 0.00001541
Iteration 44/1000 | Loss: 0.00001540
Iteration 45/1000 | Loss: 0.00001539
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001538
Iteration 48/1000 | Loss: 0.00001538
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001538
Iteration 52/1000 | Loss: 0.00001538
Iteration 53/1000 | Loss: 0.00001538
Iteration 54/1000 | Loss: 0.00001537
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001534
Iteration 57/1000 | Loss: 0.00001534
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001533
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001533
Iteration 65/1000 | Loss: 0.00001533
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001533
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001533
Iteration 71/1000 | Loss: 0.00001533
Iteration 72/1000 | Loss: 0.00001533
Iteration 73/1000 | Loss: 0.00001533
Iteration 74/1000 | Loss: 0.00001532
Iteration 75/1000 | Loss: 0.00001532
Iteration 76/1000 | Loss: 0.00001532
Iteration 77/1000 | Loss: 0.00001532
Iteration 78/1000 | Loss: 0.00001532
Iteration 79/1000 | Loss: 0.00001532
Iteration 80/1000 | Loss: 0.00001531
Iteration 81/1000 | Loss: 0.00001531
Iteration 82/1000 | Loss: 0.00001531
Iteration 83/1000 | Loss: 0.00001531
Iteration 84/1000 | Loss: 0.00001531
Iteration 85/1000 | Loss: 0.00001531
Iteration 86/1000 | Loss: 0.00001530
Iteration 87/1000 | Loss: 0.00001530
Iteration 88/1000 | Loss: 0.00001530
Iteration 89/1000 | Loss: 0.00001530
Iteration 90/1000 | Loss: 0.00001530
Iteration 91/1000 | Loss: 0.00001530
Iteration 92/1000 | Loss: 0.00001529
Iteration 93/1000 | Loss: 0.00001529
Iteration 94/1000 | Loss: 0.00001528
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001525
Iteration 104/1000 | Loss: 0.00001525
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Iteration 110/1000 | Loss: 0.00001525
Iteration 111/1000 | Loss: 0.00001525
Iteration 112/1000 | Loss: 0.00001524
Iteration 113/1000 | Loss: 0.00001524
Iteration 114/1000 | Loss: 0.00001524
Iteration 115/1000 | Loss: 0.00001524
Iteration 116/1000 | Loss: 0.00001524
Iteration 117/1000 | Loss: 0.00001524
Iteration 118/1000 | Loss: 0.00001524
Iteration 119/1000 | Loss: 0.00001523
Iteration 120/1000 | Loss: 0.00001523
Iteration 121/1000 | Loss: 0.00001523
Iteration 122/1000 | Loss: 0.00001523
Iteration 123/1000 | Loss: 0.00001523
Iteration 124/1000 | Loss: 0.00001523
Iteration 125/1000 | Loss: 0.00001523
Iteration 126/1000 | Loss: 0.00001522
Iteration 127/1000 | Loss: 0.00001522
Iteration 128/1000 | Loss: 0.00001522
Iteration 129/1000 | Loss: 0.00001522
Iteration 130/1000 | Loss: 0.00001522
Iteration 131/1000 | Loss: 0.00001522
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001521
Iteration 134/1000 | Loss: 0.00001520
Iteration 135/1000 | Loss: 0.00001520
Iteration 136/1000 | Loss: 0.00001520
Iteration 137/1000 | Loss: 0.00001519
Iteration 138/1000 | Loss: 0.00001519
Iteration 139/1000 | Loss: 0.00001519
Iteration 140/1000 | Loss: 0.00001519
Iteration 141/1000 | Loss: 0.00001519
Iteration 142/1000 | Loss: 0.00001519
Iteration 143/1000 | Loss: 0.00001518
Iteration 144/1000 | Loss: 0.00001518
Iteration 145/1000 | Loss: 0.00001518
Iteration 146/1000 | Loss: 0.00001517
Iteration 147/1000 | Loss: 0.00001517
Iteration 148/1000 | Loss: 0.00001517
Iteration 149/1000 | Loss: 0.00001517
Iteration 150/1000 | Loss: 0.00001517
Iteration 151/1000 | Loss: 0.00001516
Iteration 152/1000 | Loss: 0.00001516
Iteration 153/1000 | Loss: 0.00001516
Iteration 154/1000 | Loss: 0.00001516
Iteration 155/1000 | Loss: 0.00001516
Iteration 156/1000 | Loss: 0.00001516
Iteration 157/1000 | Loss: 0.00001516
Iteration 158/1000 | Loss: 0.00001516
Iteration 159/1000 | Loss: 0.00001516
Iteration 160/1000 | Loss: 0.00001516
Iteration 161/1000 | Loss: 0.00001515
Iteration 162/1000 | Loss: 0.00001515
Iteration 163/1000 | Loss: 0.00001515
Iteration 164/1000 | Loss: 0.00001515
Iteration 165/1000 | Loss: 0.00001515
Iteration 166/1000 | Loss: 0.00001515
Iteration 167/1000 | Loss: 0.00001514
Iteration 168/1000 | Loss: 0.00001514
Iteration 169/1000 | Loss: 0.00001514
Iteration 170/1000 | Loss: 0.00001514
Iteration 171/1000 | Loss: 0.00001514
Iteration 172/1000 | Loss: 0.00001514
Iteration 173/1000 | Loss: 0.00001514
Iteration 174/1000 | Loss: 0.00001514
Iteration 175/1000 | Loss: 0.00001513
Iteration 176/1000 | Loss: 0.00001513
Iteration 177/1000 | Loss: 0.00001513
Iteration 178/1000 | Loss: 0.00001513
Iteration 179/1000 | Loss: 0.00001513
Iteration 180/1000 | Loss: 0.00001513
Iteration 181/1000 | Loss: 0.00001513
Iteration 182/1000 | Loss: 0.00001513
Iteration 183/1000 | Loss: 0.00001513
Iteration 184/1000 | Loss: 0.00001513
Iteration 185/1000 | Loss: 0.00001513
Iteration 186/1000 | Loss: 0.00001513
Iteration 187/1000 | Loss: 0.00001512
Iteration 188/1000 | Loss: 0.00001512
Iteration 189/1000 | Loss: 0.00001512
Iteration 190/1000 | Loss: 0.00001512
Iteration 191/1000 | Loss: 0.00001512
Iteration 192/1000 | Loss: 0.00001512
Iteration 193/1000 | Loss: 0.00001512
Iteration 194/1000 | Loss: 0.00001512
Iteration 195/1000 | Loss: 0.00001512
Iteration 196/1000 | Loss: 0.00001512
Iteration 197/1000 | Loss: 0.00001512
Iteration 198/1000 | Loss: 0.00001512
Iteration 199/1000 | Loss: 0.00001512
Iteration 200/1000 | Loss: 0.00001512
Iteration 201/1000 | Loss: 0.00001512
Iteration 202/1000 | Loss: 0.00001511
Iteration 203/1000 | Loss: 0.00001511
Iteration 204/1000 | Loss: 0.00001511
Iteration 205/1000 | Loss: 0.00001511
Iteration 206/1000 | Loss: 0.00001511
Iteration 207/1000 | Loss: 0.00001511
Iteration 208/1000 | Loss: 0.00001511
Iteration 209/1000 | Loss: 0.00001511
Iteration 210/1000 | Loss: 0.00001511
Iteration 211/1000 | Loss: 0.00001511
Iteration 212/1000 | Loss: 0.00001511
Iteration 213/1000 | Loss: 0.00001511
Iteration 214/1000 | Loss: 0.00001511
Iteration 215/1000 | Loss: 0.00001510
Iteration 216/1000 | Loss: 0.00001510
Iteration 217/1000 | Loss: 0.00001510
Iteration 218/1000 | Loss: 0.00001510
Iteration 219/1000 | Loss: 0.00001510
Iteration 220/1000 | Loss: 0.00001510
Iteration 221/1000 | Loss: 0.00001510
Iteration 222/1000 | Loss: 0.00001510
Iteration 223/1000 | Loss: 0.00001510
Iteration 224/1000 | Loss: 0.00001510
Iteration 225/1000 | Loss: 0.00001510
Iteration 226/1000 | Loss: 0.00001510
Iteration 227/1000 | Loss: 0.00001510
Iteration 228/1000 | Loss: 0.00001510
Iteration 229/1000 | Loss: 0.00001510
Iteration 230/1000 | Loss: 0.00001510
Iteration 231/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.5100456039363053e-05, 1.5100456039363053e-05, 1.5100456039363053e-05, 1.5100456039363053e-05, 1.5100456039363053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5100456039363053e-05

Optimization complete. Final v2v error: 3.2946395874023438 mm

Highest mean error: 3.7893168926239014 mm for frame 73

Lowest mean error: 2.8609447479248047 mm for frame 120

Saving results

Total time: 1323.6652102470398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1020
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502958
Iteration 2/25 | Loss: 0.00130622
Iteration 3/25 | Loss: 0.00095041
Iteration 4/25 | Loss: 0.00090126
Iteration 5/25 | Loss: 0.00089306
Iteration 6/25 | Loss: 0.00089049
Iteration 7/25 | Loss: 0.00089027
Iteration 8/25 | Loss: 0.00089027
Iteration 9/25 | Loss: 0.00089027
Iteration 10/25 | Loss: 0.00089027
Iteration 11/25 | Loss: 0.00089027
Iteration 12/25 | Loss: 0.00089027
Iteration 13/25 | Loss: 0.00089027
Iteration 14/25 | Loss: 0.00089027
Iteration 15/25 | Loss: 0.00089027
Iteration 16/25 | Loss: 0.00089027
Iteration 17/25 | Loss: 0.00089027
Iteration 18/25 | Loss: 0.00089027
Iteration 19/25 | Loss: 0.00089027
Iteration 20/25 | Loss: 0.00089027
Iteration 21/25 | Loss: 0.00089027
Iteration 22/25 | Loss: 0.00089027
Iteration 23/25 | Loss: 0.00089027
Iteration 24/25 | Loss: 0.00089027
Iteration 25/25 | Loss: 0.00089027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50403059
Iteration 2/25 | Loss: 0.00051968
Iteration 3/25 | Loss: 0.00051967
Iteration 4/25 | Loss: 0.00051967
Iteration 5/25 | Loss: 0.00051967
Iteration 6/25 | Loss: 0.00051967
Iteration 7/25 | Loss: 0.00051967
Iteration 8/25 | Loss: 0.00051967
Iteration 9/25 | Loss: 0.00051967
Iteration 10/25 | Loss: 0.00051967
Iteration 11/25 | Loss: 0.00051967
Iteration 12/25 | Loss: 0.00051967
Iteration 13/25 | Loss: 0.00051967
Iteration 14/25 | Loss: 0.00051967
Iteration 15/25 | Loss: 0.00051967
Iteration 16/25 | Loss: 0.00051967
Iteration 17/25 | Loss: 0.00051967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005196720012463629, 0.0005196720012463629, 0.0005196720012463629, 0.0005196720012463629, 0.0005196720012463629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005196720012463629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051967
Iteration 2/1000 | Loss: 0.00004441
Iteration 3/1000 | Loss: 0.00003156
Iteration 4/1000 | Loss: 0.00002941
Iteration 5/1000 | Loss: 0.00002830
Iteration 6/1000 | Loss: 0.00002760
Iteration 7/1000 | Loss: 0.00002692
Iteration 8/1000 | Loss: 0.00002641
Iteration 9/1000 | Loss: 0.00002609
Iteration 10/1000 | Loss: 0.00002588
Iteration 11/1000 | Loss: 0.00002585
Iteration 12/1000 | Loss: 0.00002575
Iteration 13/1000 | Loss: 0.00002560
Iteration 14/1000 | Loss: 0.00002555
Iteration 15/1000 | Loss: 0.00002552
Iteration 16/1000 | Loss: 0.00002552
Iteration 17/1000 | Loss: 0.00002552
Iteration 18/1000 | Loss: 0.00002550
Iteration 19/1000 | Loss: 0.00002547
Iteration 20/1000 | Loss: 0.00002547
Iteration 21/1000 | Loss: 0.00002547
Iteration 22/1000 | Loss: 0.00002547
Iteration 23/1000 | Loss: 0.00002547
Iteration 24/1000 | Loss: 0.00002547
Iteration 25/1000 | Loss: 0.00002547
Iteration 26/1000 | Loss: 0.00002546
Iteration 27/1000 | Loss: 0.00002545
Iteration 28/1000 | Loss: 0.00002543
Iteration 29/1000 | Loss: 0.00002542
Iteration 30/1000 | Loss: 0.00002542
Iteration 31/1000 | Loss: 0.00002537
Iteration 32/1000 | Loss: 0.00002536
Iteration 33/1000 | Loss: 0.00002536
Iteration 34/1000 | Loss: 0.00002535
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002534
Iteration 39/1000 | Loss: 0.00002534
Iteration 40/1000 | Loss: 0.00002534
Iteration 41/1000 | Loss: 0.00002534
Iteration 42/1000 | Loss: 0.00002534
Iteration 43/1000 | Loss: 0.00002534
Iteration 44/1000 | Loss: 0.00002534
Iteration 45/1000 | Loss: 0.00002534
Iteration 46/1000 | Loss: 0.00002533
Iteration 47/1000 | Loss: 0.00002533
Iteration 48/1000 | Loss: 0.00002532
Iteration 49/1000 | Loss: 0.00002532
Iteration 50/1000 | Loss: 0.00002532
Iteration 51/1000 | Loss: 0.00002532
Iteration 52/1000 | Loss: 0.00002532
Iteration 53/1000 | Loss: 0.00002532
Iteration 54/1000 | Loss: 0.00002532
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002531
Iteration 57/1000 | Loss: 0.00002531
Iteration 58/1000 | Loss: 0.00002531
Iteration 59/1000 | Loss: 0.00002531
Iteration 60/1000 | Loss: 0.00002531
Iteration 61/1000 | Loss: 0.00002531
Iteration 62/1000 | Loss: 0.00002531
Iteration 63/1000 | Loss: 0.00002531
Iteration 64/1000 | Loss: 0.00002531
Iteration 65/1000 | Loss: 0.00002531
Iteration 66/1000 | Loss: 0.00002531
Iteration 67/1000 | Loss: 0.00002530
Iteration 68/1000 | Loss: 0.00002530
Iteration 69/1000 | Loss: 0.00002530
Iteration 70/1000 | Loss: 0.00002529
Iteration 71/1000 | Loss: 0.00002529
Iteration 72/1000 | Loss: 0.00002529
Iteration 73/1000 | Loss: 0.00002529
Iteration 74/1000 | Loss: 0.00002529
Iteration 75/1000 | Loss: 0.00002529
Iteration 76/1000 | Loss: 0.00002529
Iteration 77/1000 | Loss: 0.00002529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.5292991267633624e-05, 2.5292991267633624e-05, 2.5292991267633624e-05, 2.5292991267633624e-05, 2.5292991267633624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5292991267633624e-05

Optimization complete. Final v2v error: 4.260586738586426 mm

Highest mean error: 4.781219482421875 mm for frame 195

Lowest mean error: 3.896984338760376 mm for frame 80

Saving results

Total time: 1243.520882844925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1009
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817500
Iteration 2/25 | Loss: 0.00146764
Iteration 3/25 | Loss: 0.00099373
Iteration 4/25 | Loss: 0.00090632
Iteration 5/25 | Loss: 0.00088556
Iteration 6/25 | Loss: 0.00088043
Iteration 7/25 | Loss: 0.00087798
Iteration 8/25 | Loss: 0.00085909
Iteration 9/25 | Loss: 0.00085242
Iteration 10/25 | Loss: 0.00084023
Iteration 11/25 | Loss: 0.00083488
Iteration 12/25 | Loss: 0.00083770
Iteration 13/25 | Loss: 0.00083827
Iteration 14/25 | Loss: 0.00083721
Iteration 15/25 | Loss: 0.00083204
Iteration 16/25 | Loss: 0.00083081
Iteration 17/25 | Loss: 0.00082988
Iteration 18/25 | Loss: 0.00082723
Iteration 19/25 | Loss: 0.00082685
Iteration 20/25 | Loss: 0.00082678
Iteration 21/25 | Loss: 0.00082678
Iteration 22/25 | Loss: 0.00082677
Iteration 23/25 | Loss: 0.00082677
Iteration 24/25 | Loss: 0.00082677
Iteration 25/25 | Loss: 0.00082677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62385464
Iteration 2/25 | Loss: 0.00068751
Iteration 3/25 | Loss: 0.00065437
Iteration 4/25 | Loss: 0.00065437
Iteration 5/25 | Loss: 0.00065437
Iteration 6/25 | Loss: 0.00065437
Iteration 7/25 | Loss: 0.00065437
Iteration 8/25 | Loss: 0.00065436
Iteration 9/25 | Loss: 0.00065436
Iteration 10/25 | Loss: 0.00065436
Iteration 11/25 | Loss: 0.00065436
Iteration 12/25 | Loss: 0.00065436
Iteration 13/25 | Loss: 0.00065436
Iteration 14/25 | Loss: 0.00065436
Iteration 15/25 | Loss: 0.00065436
Iteration 16/25 | Loss: 0.00065436
Iteration 17/25 | Loss: 0.00065436
Iteration 18/25 | Loss: 0.00065436
Iteration 19/25 | Loss: 0.00065436
Iteration 20/25 | Loss: 0.00065436
Iteration 21/25 | Loss: 0.00065436
Iteration 22/25 | Loss: 0.00065436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006543634808622301, 0.0006543634808622301, 0.0006543634808622301, 0.0006543634808622301, 0.0006543634808622301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006543634808622301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065436
Iteration 2/1000 | Loss: 0.00004077
Iteration 3/1000 | Loss: 0.00002957
Iteration 4/1000 | Loss: 0.00002643
Iteration 5/1000 | Loss: 0.00002464
Iteration 6/1000 | Loss: 0.00002391
Iteration 7/1000 | Loss: 0.00002348
Iteration 8/1000 | Loss: 0.00002304
Iteration 9/1000 | Loss: 0.00002253
Iteration 10/1000 | Loss: 0.00002224
Iteration 11/1000 | Loss: 0.00002199
Iteration 12/1000 | Loss: 0.00002185
Iteration 13/1000 | Loss: 0.00002181
Iteration 14/1000 | Loss: 0.00002179
Iteration 15/1000 | Loss: 0.00002167
Iteration 16/1000 | Loss: 0.00002163
Iteration 17/1000 | Loss: 0.00002162
Iteration 18/1000 | Loss: 0.00015486
Iteration 19/1000 | Loss: 0.00006118
Iteration 20/1000 | Loss: 0.00002399
Iteration 21/1000 | Loss: 0.00002172
Iteration 22/1000 | Loss: 0.00002063
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001847
Iteration 26/1000 | Loss: 0.00001823
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001803
Iteration 29/1000 | Loss: 0.00001796
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001786
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001781
Iteration 63/1000 | Loss: 0.00001781
Iteration 64/1000 | Loss: 0.00001781
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001779
Iteration 77/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.779300691850949e-05, 1.779300691850949e-05, 1.779300691850949e-05, 1.779300691850949e-05, 1.779300691850949e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.779300691850949e-05

Optimization complete. Final v2v error: 3.5899906158447266 mm

Highest mean error: 4.543594837188721 mm for frame 103

Lowest mean error: 3.0888235569000244 mm for frame 93

Saving results

Total time: 2835.4437987804413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1037
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847692
Iteration 2/25 | Loss: 0.00168707
Iteration 3/25 | Loss: 0.00102138
Iteration 4/25 | Loss: 0.00094063
Iteration 5/25 | Loss: 0.00093434
Iteration 6/25 | Loss: 0.00093395
Iteration 7/25 | Loss: 0.00093395
Iteration 8/25 | Loss: 0.00093395
Iteration 9/25 | Loss: 0.00093395
Iteration 10/25 | Loss: 0.00093395
Iteration 11/25 | Loss: 0.00093395
Iteration 12/25 | Loss: 0.00093395
Iteration 13/25 | Loss: 0.00093395
Iteration 14/25 | Loss: 0.00093395
Iteration 15/25 | Loss: 0.00093395
Iteration 16/25 | Loss: 0.00093395
Iteration 17/25 | Loss: 0.00093395
Iteration 18/25 | Loss: 0.00093395
Iteration 19/25 | Loss: 0.00093395
Iteration 20/25 | Loss: 0.00093395
Iteration 21/25 | Loss: 0.00093395
Iteration 22/25 | Loss: 0.00093395
Iteration 23/25 | Loss: 0.00093395
Iteration 24/25 | Loss: 0.00093395
Iteration 25/25 | Loss: 0.00093395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44947779
Iteration 2/25 | Loss: 0.00068237
Iteration 3/25 | Loss: 0.00068234
Iteration 4/25 | Loss: 0.00068234
Iteration 5/25 | Loss: 0.00068234
Iteration 6/25 | Loss: 0.00068234
Iteration 7/25 | Loss: 0.00068234
Iteration 8/25 | Loss: 0.00068234
Iteration 9/25 | Loss: 0.00068234
Iteration 10/25 | Loss: 0.00068234
Iteration 11/25 | Loss: 0.00068234
Iteration 12/25 | Loss: 0.00068234
Iteration 13/25 | Loss: 0.00068234
Iteration 14/25 | Loss: 0.00068234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006823376752436161, 0.0006823376752436161, 0.0006823376752436161, 0.0006823376752436161, 0.0006823376752436161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006823376752436161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068234
Iteration 2/1000 | Loss: 0.00004706
Iteration 3/1000 | Loss: 0.00002778
Iteration 4/1000 | Loss: 0.00002551
Iteration 5/1000 | Loss: 0.00002420
Iteration 6/1000 | Loss: 0.00002335
Iteration 7/1000 | Loss: 0.00002299
Iteration 8/1000 | Loss: 0.00002265
Iteration 9/1000 | Loss: 0.00002262
Iteration 10/1000 | Loss: 0.00002257
Iteration 11/1000 | Loss: 0.00002255
Iteration 12/1000 | Loss: 0.00002250
Iteration 13/1000 | Loss: 0.00002250
Iteration 14/1000 | Loss: 0.00002250
Iteration 15/1000 | Loss: 0.00002250
Iteration 16/1000 | Loss: 0.00002250
Iteration 17/1000 | Loss: 0.00002250
Iteration 18/1000 | Loss: 0.00002250
Iteration 19/1000 | Loss: 0.00002250
Iteration 20/1000 | Loss: 0.00002245
Iteration 21/1000 | Loss: 0.00002244
Iteration 22/1000 | Loss: 0.00002243
Iteration 23/1000 | Loss: 0.00002234
Iteration 24/1000 | Loss: 0.00002233
Iteration 25/1000 | Loss: 0.00002232
Iteration 26/1000 | Loss: 0.00002231
Iteration 27/1000 | Loss: 0.00002230
Iteration 28/1000 | Loss: 0.00002229
Iteration 29/1000 | Loss: 0.00002228
Iteration 30/1000 | Loss: 0.00002227
Iteration 31/1000 | Loss: 0.00002226
Iteration 32/1000 | Loss: 0.00002226
Iteration 33/1000 | Loss: 0.00002226
Iteration 34/1000 | Loss: 0.00002226
Iteration 35/1000 | Loss: 0.00002226
Iteration 36/1000 | Loss: 0.00002226
Iteration 37/1000 | Loss: 0.00002225
Iteration 38/1000 | Loss: 0.00002225
Iteration 39/1000 | Loss: 0.00002225
Iteration 40/1000 | Loss: 0.00002225
Iteration 41/1000 | Loss: 0.00002225
Iteration 42/1000 | Loss: 0.00002225
Iteration 43/1000 | Loss: 0.00002224
Iteration 44/1000 | Loss: 0.00002223
Iteration 45/1000 | Loss: 0.00002223
Iteration 46/1000 | Loss: 0.00002223
Iteration 47/1000 | Loss: 0.00002223
Iteration 48/1000 | Loss: 0.00002223
Iteration 49/1000 | Loss: 0.00002223
Iteration 50/1000 | Loss: 0.00002223
Iteration 51/1000 | Loss: 0.00002222
Iteration 52/1000 | Loss: 0.00002222
Iteration 53/1000 | Loss: 0.00002222
Iteration 54/1000 | Loss: 0.00002222
Iteration 55/1000 | Loss: 0.00002222
Iteration 56/1000 | Loss: 0.00002222
Iteration 57/1000 | Loss: 0.00002222
Iteration 58/1000 | Loss: 0.00002222
Iteration 59/1000 | Loss: 0.00002222
Iteration 60/1000 | Loss: 0.00002222
Iteration 61/1000 | Loss: 0.00002222
Iteration 62/1000 | Loss: 0.00002222
Iteration 63/1000 | Loss: 0.00002222
Iteration 64/1000 | Loss: 0.00002222
Iteration 65/1000 | Loss: 0.00002221
Iteration 66/1000 | Loss: 0.00002221
Iteration 67/1000 | Loss: 0.00002221
Iteration 68/1000 | Loss: 0.00002221
Iteration 69/1000 | Loss: 0.00002220
Iteration 70/1000 | Loss: 0.00002219
Iteration 71/1000 | Loss: 0.00002219
Iteration 72/1000 | Loss: 0.00002219
Iteration 73/1000 | Loss: 0.00002219
Iteration 74/1000 | Loss: 0.00002219
Iteration 75/1000 | Loss: 0.00002219
Iteration 76/1000 | Loss: 0.00002219
Iteration 77/1000 | Loss: 0.00002218
Iteration 78/1000 | Loss: 0.00002218
Iteration 79/1000 | Loss: 0.00002217
Iteration 80/1000 | Loss: 0.00002217
Iteration 81/1000 | Loss: 0.00002217
Iteration 82/1000 | Loss: 0.00002217
Iteration 83/1000 | Loss: 0.00002217
Iteration 84/1000 | Loss: 0.00002216
Iteration 85/1000 | Loss: 0.00002216
Iteration 86/1000 | Loss: 0.00002216
Iteration 87/1000 | Loss: 0.00002216
Iteration 88/1000 | Loss: 0.00002216
Iteration 89/1000 | Loss: 0.00002215
Iteration 90/1000 | Loss: 0.00002215
Iteration 91/1000 | Loss: 0.00002215
Iteration 92/1000 | Loss: 0.00002215
Iteration 93/1000 | Loss: 0.00002215
Iteration 94/1000 | Loss: 0.00002215
Iteration 95/1000 | Loss: 0.00002214
Iteration 96/1000 | Loss: 0.00002214
Iteration 97/1000 | Loss: 0.00002214
Iteration 98/1000 | Loss: 0.00002214
Iteration 99/1000 | Loss: 0.00002214
Iteration 100/1000 | Loss: 0.00002214
Iteration 101/1000 | Loss: 0.00002214
Iteration 102/1000 | Loss: 0.00002214
Iteration 103/1000 | Loss: 0.00002213
Iteration 104/1000 | Loss: 0.00002213
Iteration 105/1000 | Loss: 0.00002213
Iteration 106/1000 | Loss: 0.00002213
Iteration 107/1000 | Loss: 0.00002213
Iteration 108/1000 | Loss: 0.00002213
Iteration 109/1000 | Loss: 0.00002213
Iteration 110/1000 | Loss: 0.00002213
Iteration 111/1000 | Loss: 0.00002213
Iteration 112/1000 | Loss: 0.00002213
Iteration 113/1000 | Loss: 0.00002213
Iteration 114/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.2134099708637223e-05, 2.2134099708637223e-05, 2.2134099708637223e-05, 2.2134099708637223e-05, 2.2134099708637223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2134099708637223e-05

Optimization complete. Final v2v error: 3.9244606494903564 mm

Highest mean error: 4.142707347869873 mm for frame 77

Lowest mean error: 3.7602193355560303 mm for frame 108

Saving results

Total time: 1095.5029792785645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1075
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837914
Iteration 2/25 | Loss: 0.00092331
Iteration 3/25 | Loss: 0.00075140
Iteration 4/25 | Loss: 0.00072754
Iteration 5/25 | Loss: 0.00072194
Iteration 6/25 | Loss: 0.00071987
Iteration 7/25 | Loss: 0.00071938
Iteration 8/25 | Loss: 0.00071938
Iteration 9/25 | Loss: 0.00071938
Iteration 10/25 | Loss: 0.00071938
Iteration 11/25 | Loss: 0.00071938
Iteration 12/25 | Loss: 0.00071938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007193763740360737, 0.0007193763740360737, 0.0007193763740360737, 0.0007193763740360737, 0.0007193763740360737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007193763740360737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49245191
Iteration 2/25 | Loss: 0.00043603
Iteration 3/25 | Loss: 0.00043603
Iteration 4/25 | Loss: 0.00043603
Iteration 5/25 | Loss: 0.00043603
Iteration 6/25 | Loss: 0.00043603
Iteration 7/25 | Loss: 0.00043603
Iteration 8/25 | Loss: 0.00043603
Iteration 9/25 | Loss: 0.00043603
Iteration 10/25 | Loss: 0.00043603
Iteration 11/25 | Loss: 0.00043603
Iteration 12/25 | Loss: 0.00043603
Iteration 13/25 | Loss: 0.00043603
Iteration 14/25 | Loss: 0.00043603
Iteration 15/25 | Loss: 0.00043603
Iteration 16/25 | Loss: 0.00043603
Iteration 17/25 | Loss: 0.00043603
Iteration 18/25 | Loss: 0.00043603
Iteration 19/25 | Loss: 0.00043603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004360266320873052, 0.0004360266320873052, 0.0004360266320873052, 0.0004360266320873052, 0.0004360266320873052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004360266320873052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043603
Iteration 2/1000 | Loss: 0.00002230
Iteration 3/1000 | Loss: 0.00001406
Iteration 4/1000 | Loss: 0.00001272
Iteration 5/1000 | Loss: 0.00001189
Iteration 6/1000 | Loss: 0.00001135
Iteration 7/1000 | Loss: 0.00001106
Iteration 8/1000 | Loss: 0.00001085
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001079
Iteration 11/1000 | Loss: 0.00001077
Iteration 12/1000 | Loss: 0.00001075
Iteration 13/1000 | Loss: 0.00001073
Iteration 14/1000 | Loss: 0.00001072
Iteration 15/1000 | Loss: 0.00001069
Iteration 16/1000 | Loss: 0.00001065
Iteration 17/1000 | Loss: 0.00001063
Iteration 18/1000 | Loss: 0.00001058
Iteration 19/1000 | Loss: 0.00001054
Iteration 20/1000 | Loss: 0.00001053
Iteration 21/1000 | Loss: 0.00001048
Iteration 22/1000 | Loss: 0.00001048
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001046
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001042
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001041
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001040
Iteration 36/1000 | Loss: 0.00001040
Iteration 37/1000 | Loss: 0.00001039
Iteration 38/1000 | Loss: 0.00001039
Iteration 39/1000 | Loss: 0.00001038
Iteration 40/1000 | Loss: 0.00001038
Iteration 41/1000 | Loss: 0.00001037
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001037
Iteration 44/1000 | Loss: 0.00001036
Iteration 45/1000 | Loss: 0.00001036
Iteration 46/1000 | Loss: 0.00001035
Iteration 47/1000 | Loss: 0.00001034
Iteration 48/1000 | Loss: 0.00001033
Iteration 49/1000 | Loss: 0.00001032
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001032
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001030
Iteration 54/1000 | Loss: 0.00001030
Iteration 55/1000 | Loss: 0.00001030
Iteration 56/1000 | Loss: 0.00001029
Iteration 57/1000 | Loss: 0.00001029
Iteration 58/1000 | Loss: 0.00001029
Iteration 59/1000 | Loss: 0.00001029
Iteration 60/1000 | Loss: 0.00001028
Iteration 61/1000 | Loss: 0.00001028
Iteration 62/1000 | Loss: 0.00001028
Iteration 63/1000 | Loss: 0.00001028
Iteration 64/1000 | Loss: 0.00001028
Iteration 65/1000 | Loss: 0.00001028
Iteration 66/1000 | Loss: 0.00001028
Iteration 67/1000 | Loss: 0.00001027
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001027
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001027
Iteration 73/1000 | Loss: 0.00001027
Iteration 74/1000 | Loss: 0.00001026
Iteration 75/1000 | Loss: 0.00001026
Iteration 76/1000 | Loss: 0.00001026
Iteration 77/1000 | Loss: 0.00001026
Iteration 78/1000 | Loss: 0.00001026
Iteration 79/1000 | Loss: 0.00001026
Iteration 80/1000 | Loss: 0.00001026
Iteration 81/1000 | Loss: 0.00001026
Iteration 82/1000 | Loss: 0.00001026
Iteration 83/1000 | Loss: 0.00001026
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001025
Iteration 89/1000 | Loss: 0.00001025
Iteration 90/1000 | Loss: 0.00001025
Iteration 91/1000 | Loss: 0.00001025
Iteration 92/1000 | Loss: 0.00001025
Iteration 93/1000 | Loss: 0.00001025
Iteration 94/1000 | Loss: 0.00001025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.025242090690881e-05, 1.025242090690881e-05, 1.025242090690881e-05, 1.025242090690881e-05, 1.025242090690881e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.025242090690881e-05

Optimization complete. Final v2v error: 2.685246706008911 mm

Highest mean error: 3.618997812271118 mm for frame 68

Lowest mean error: 2.348991870880127 mm for frame 141

Saving results

Total time: 886.9075090885162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1051
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038634
Iteration 2/25 | Loss: 0.00239507
Iteration 3/25 | Loss: 0.00138665
Iteration 4/25 | Loss: 0.00120223
Iteration 5/25 | Loss: 0.00120612
Iteration 6/25 | Loss: 0.00118793
Iteration 7/25 | Loss: 0.00119185
Iteration 8/25 | Loss: 0.00115059
Iteration 9/25 | Loss: 0.00111164
Iteration 10/25 | Loss: 0.00105519
Iteration 11/25 | Loss: 0.00102389
Iteration 12/25 | Loss: 0.00098386
Iteration 13/25 | Loss: 0.00096949
Iteration 14/25 | Loss: 0.00093787
Iteration 15/25 | Loss: 0.00093198
Iteration 16/25 | Loss: 0.00093857
Iteration 17/25 | Loss: 0.00092282
Iteration 18/25 | Loss: 0.00091389
Iteration 19/25 | Loss: 0.00090234
Iteration 20/25 | Loss: 0.00090221
Iteration 21/25 | Loss: 0.00089894
Iteration 22/25 | Loss: 0.00089194
Iteration 23/25 | Loss: 0.00088817
Iteration 24/25 | Loss: 0.00088616
Iteration 25/25 | Loss: 0.00088482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38995159
Iteration 2/25 | Loss: 0.00131678
Iteration 3/25 | Loss: 0.00131678
Iteration 4/25 | Loss: 0.00131678
Iteration 5/25 | Loss: 0.00131678
Iteration 6/25 | Loss: 0.00131678
Iteration 7/25 | Loss: 0.00131678
Iteration 8/25 | Loss: 0.00131678
Iteration 9/25 | Loss: 0.00131678
Iteration 10/25 | Loss: 0.00131678
Iteration 11/25 | Loss: 0.00131678
Iteration 12/25 | Loss: 0.00131678
Iteration 13/25 | Loss: 0.00131678
Iteration 14/25 | Loss: 0.00131678
Iteration 15/25 | Loss: 0.00131678
Iteration 16/25 | Loss: 0.00131678
Iteration 17/25 | Loss: 0.00131678
Iteration 18/25 | Loss: 0.00131678
Iteration 19/25 | Loss: 0.00131678
Iteration 20/25 | Loss: 0.00131678
Iteration 21/25 | Loss: 0.00131678
Iteration 22/25 | Loss: 0.00131678
Iteration 23/25 | Loss: 0.00131678
Iteration 24/25 | Loss: 0.00131678
Iteration 25/25 | Loss: 0.00131678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131678
Iteration 2/1000 | Loss: 0.00100305
Iteration 3/1000 | Loss: 0.00073255
Iteration 4/1000 | Loss: 0.00056934
Iteration 5/1000 | Loss: 0.00057327
Iteration 6/1000 | Loss: 0.00073169
Iteration 7/1000 | Loss: 0.00044121
Iteration 8/1000 | Loss: 0.00029735
Iteration 9/1000 | Loss: 0.00024064
Iteration 10/1000 | Loss: 0.00055038
Iteration 11/1000 | Loss: 0.00061765
Iteration 12/1000 | Loss: 0.00016766
Iteration 13/1000 | Loss: 0.00032122
Iteration 14/1000 | Loss: 0.00089281
Iteration 15/1000 | Loss: 0.00070285
Iteration 16/1000 | Loss: 0.00052132
Iteration 17/1000 | Loss: 0.00043787
Iteration 18/1000 | Loss: 0.00098825
Iteration 19/1000 | Loss: 0.00045371
Iteration 20/1000 | Loss: 0.00058716
Iteration 21/1000 | Loss: 0.00063965
Iteration 22/1000 | Loss: 0.00051840
Iteration 23/1000 | Loss: 0.00060569
Iteration 24/1000 | Loss: 0.00073598
Iteration 25/1000 | Loss: 0.00095286
Iteration 26/1000 | Loss: 0.00072072
Iteration 27/1000 | Loss: 0.00091594
Iteration 28/1000 | Loss: 0.00036784
Iteration 29/1000 | Loss: 0.00036466
Iteration 30/1000 | Loss: 0.00031598
Iteration 31/1000 | Loss: 0.00030213
Iteration 32/1000 | Loss: 0.00055665
Iteration 33/1000 | Loss: 0.00024536
Iteration 34/1000 | Loss: 0.00025243
Iteration 35/1000 | Loss: 0.00029199
Iteration 36/1000 | Loss: 0.00028766
Iteration 37/1000 | Loss: 0.00030313
Iteration 38/1000 | Loss: 0.00026700
Iteration 39/1000 | Loss: 0.00027235
Iteration 40/1000 | Loss: 0.00028498
Iteration 41/1000 | Loss: 0.00029197
Iteration 42/1000 | Loss: 0.00027934
Iteration 43/1000 | Loss: 0.00027759
Iteration 44/1000 | Loss: 0.00015393
Iteration 45/1000 | Loss: 0.00044531
Iteration 46/1000 | Loss: 0.00060283
Iteration 47/1000 | Loss: 0.00027704
Iteration 48/1000 | Loss: 0.00025547
Iteration 49/1000 | Loss: 0.00012530
Iteration 50/1000 | Loss: 0.00054778
Iteration 51/1000 | Loss: 0.00098219
Iteration 52/1000 | Loss: 0.00032632
Iteration 53/1000 | Loss: 0.00026607
Iteration 54/1000 | Loss: 0.00045393
Iteration 55/1000 | Loss: 0.00042969
Iteration 56/1000 | Loss: 0.00019420
Iteration 57/1000 | Loss: 0.00013122
Iteration 58/1000 | Loss: 0.00041894
Iteration 59/1000 | Loss: 0.00046630
Iteration 60/1000 | Loss: 0.00030983
Iteration 61/1000 | Loss: 0.00044638
Iteration 62/1000 | Loss: 0.00023547
Iteration 63/1000 | Loss: 0.00022518
Iteration 64/1000 | Loss: 0.00042321
Iteration 65/1000 | Loss: 0.00090342
Iteration 66/1000 | Loss: 0.00071553
Iteration 67/1000 | Loss: 0.00070006
Iteration 68/1000 | Loss: 0.00027897
Iteration 69/1000 | Loss: 0.00070221
Iteration 70/1000 | Loss: 0.00034209
Iteration 71/1000 | Loss: 0.00011421
Iteration 72/1000 | Loss: 0.00010358
Iteration 73/1000 | Loss: 0.00032890
Iteration 74/1000 | Loss: 0.00047863
Iteration 75/1000 | Loss: 0.00020747
Iteration 76/1000 | Loss: 0.00028834
Iteration 77/1000 | Loss: 0.00021980
Iteration 78/1000 | Loss: 0.00025546
Iteration 79/1000 | Loss: 0.00013172
Iteration 80/1000 | Loss: 0.00018581
Iteration 81/1000 | Loss: 0.00116772
Iteration 82/1000 | Loss: 0.00046298
Iteration 83/1000 | Loss: 0.00047322
Iteration 84/1000 | Loss: 0.00049298
Iteration 85/1000 | Loss: 0.00047914
Iteration 86/1000 | Loss: 0.00042213
Iteration 87/1000 | Loss: 0.00043615
Iteration 88/1000 | Loss: 0.00056447
Iteration 89/1000 | Loss: 0.00027279
Iteration 90/1000 | Loss: 0.00033979
Iteration 91/1000 | Loss: 0.00068007
Iteration 92/1000 | Loss: 0.00023846
Iteration 93/1000 | Loss: 0.00041432
Iteration 94/1000 | Loss: 0.00030294
Iteration 95/1000 | Loss: 0.00031476
Iteration 96/1000 | Loss: 0.00017358
Iteration 97/1000 | Loss: 0.00020466
Iteration 98/1000 | Loss: 0.00013140
Iteration 99/1000 | Loss: 0.00095594
Iteration 100/1000 | Loss: 0.00090175
Iteration 101/1000 | Loss: 0.00050846
Iteration 102/1000 | Loss: 0.00079115
Iteration 103/1000 | Loss: 0.00160975
Iteration 104/1000 | Loss: 0.00074020
Iteration 105/1000 | Loss: 0.00120940
Iteration 106/1000 | Loss: 0.00053719
Iteration 107/1000 | Loss: 0.00066522
Iteration 108/1000 | Loss: 0.00038099
Iteration 109/1000 | Loss: 0.00084944
Iteration 110/1000 | Loss: 0.00047420
Iteration 111/1000 | Loss: 0.00051982
Iteration 112/1000 | Loss: 0.00053594
Iteration 113/1000 | Loss: 0.00028769
Iteration 114/1000 | Loss: 0.00033238
Iteration 115/1000 | Loss: 0.00018892
Iteration 116/1000 | Loss: 0.00013914
Iteration 117/1000 | Loss: 0.00009662
Iteration 118/1000 | Loss: 0.00014083
Iteration 119/1000 | Loss: 0.00046113
Iteration 120/1000 | Loss: 0.00037503
Iteration 121/1000 | Loss: 0.00039254
Iteration 122/1000 | Loss: 0.00034462
Iteration 123/1000 | Loss: 0.00051497
Iteration 124/1000 | Loss: 0.00076198
Iteration 125/1000 | Loss: 0.00050410
Iteration 126/1000 | Loss: 0.00088186
Iteration 127/1000 | Loss: 0.00133186
Iteration 128/1000 | Loss: 0.00088175
Iteration 129/1000 | Loss: 0.00026927
Iteration 130/1000 | Loss: 0.00018891
Iteration 131/1000 | Loss: 0.00019897
Iteration 132/1000 | Loss: 0.00008608
Iteration 133/1000 | Loss: 0.00007181
Iteration 134/1000 | Loss: 0.00008254
Iteration 135/1000 | Loss: 0.00063568
Iteration 136/1000 | Loss: 0.00028678
Iteration 137/1000 | Loss: 0.00006097
Iteration 138/1000 | Loss: 0.00007362
Iteration 139/1000 | Loss: 0.00006340
Iteration 140/1000 | Loss: 0.00005453
Iteration 141/1000 | Loss: 0.00007656
Iteration 142/1000 | Loss: 0.00005942
Iteration 143/1000 | Loss: 0.00005627
Iteration 144/1000 | Loss: 0.00007387
Iteration 145/1000 | Loss: 0.00006527
Iteration 146/1000 | Loss: 0.00007068
Iteration 147/1000 | Loss: 0.00006705
Iteration 148/1000 | Loss: 0.00007999
Iteration 149/1000 | Loss: 0.00007238
Iteration 150/1000 | Loss: 0.00006434
Iteration 151/1000 | Loss: 0.00006708
Iteration 152/1000 | Loss: 0.00007229
Iteration 153/1000 | Loss: 0.00007626
Iteration 154/1000 | Loss: 0.00007019
Iteration 155/1000 | Loss: 0.00005521
Iteration 156/1000 | Loss: 0.00007740
Iteration 157/1000 | Loss: 0.00006974
Iteration 158/1000 | Loss: 0.00006895
Iteration 159/1000 | Loss: 0.00007241
Iteration 160/1000 | Loss: 0.00008309
Iteration 161/1000 | Loss: 0.00007324
Iteration 162/1000 | Loss: 0.00007233
Iteration 163/1000 | Loss: 0.00006866
Iteration 164/1000 | Loss: 0.00006773
Iteration 165/1000 | Loss: 0.00006646
Iteration 166/1000 | Loss: 0.00008214
Iteration 167/1000 | Loss: 0.00006724
Iteration 168/1000 | Loss: 0.00007417
Iteration 169/1000 | Loss: 0.00006705
Iteration 170/1000 | Loss: 0.00006203
Iteration 171/1000 | Loss: 0.00007112
Iteration 172/1000 | Loss: 0.00006374
Iteration 173/1000 | Loss: 0.00007988
Iteration 174/1000 | Loss: 0.00007716
Iteration 175/1000 | Loss: 0.00007680
Iteration 176/1000 | Loss: 0.00007533
Iteration 177/1000 | Loss: 0.00007623
Iteration 178/1000 | Loss: 0.00007423
Iteration 179/1000 | Loss: 0.00007492
Iteration 180/1000 | Loss: 0.00007321
Iteration 181/1000 | Loss: 0.00007752
Iteration 182/1000 | Loss: 0.00062665
Iteration 183/1000 | Loss: 0.00035504
Iteration 184/1000 | Loss: 0.00052581
Iteration 185/1000 | Loss: 0.00008912
Iteration 186/1000 | Loss: 0.00007353
Iteration 187/1000 | Loss: 0.00008682
Iteration 188/1000 | Loss: 0.00006916
Iteration 189/1000 | Loss: 0.00005090
Iteration 190/1000 | Loss: 0.00005852
Iteration 191/1000 | Loss: 0.00007361
Iteration 192/1000 | Loss: 0.00007495
Iteration 193/1000 | Loss: 0.00007381
Iteration 194/1000 | Loss: 0.00007443
Iteration 195/1000 | Loss: 0.00007396
Iteration 196/1000 | Loss: 0.00007467
Iteration 197/1000 | Loss: 0.00007176
Iteration 198/1000 | Loss: 0.00007709
Iteration 199/1000 | Loss: 0.00007664
Iteration 200/1000 | Loss: 0.00006927
Iteration 201/1000 | Loss: 0.00007231
Iteration 202/1000 | Loss: 0.00006385
Iteration 203/1000 | Loss: 0.00005909
Iteration 204/1000 | Loss: 0.00005701
Iteration 205/1000 | Loss: 0.00006982
Iteration 206/1000 | Loss: 0.00008450
Iteration 207/1000 | Loss: 0.00006992
Iteration 208/1000 | Loss: 0.00007878
Iteration 209/1000 | Loss: 0.00006829
Iteration 210/1000 | Loss: 0.00007271
Iteration 211/1000 | Loss: 0.00007198
Iteration 212/1000 | Loss: 0.00006714
Iteration 213/1000 | Loss: 0.00006087
Iteration 214/1000 | Loss: 0.00007206
Iteration 215/1000 | Loss: 0.00006821
Iteration 216/1000 | Loss: 0.00007397
Iteration 217/1000 | Loss: 0.00007483
Iteration 218/1000 | Loss: 0.00008375
Iteration 219/1000 | Loss: 0.00007351
Iteration 220/1000 | Loss: 0.00007540
Iteration 221/1000 | Loss: 0.00008977
Iteration 222/1000 | Loss: 0.00004087
Iteration 223/1000 | Loss: 0.00003020
Iteration 224/1000 | Loss: 0.00002669
Iteration 225/1000 | Loss: 0.00005055
Iteration 226/1000 | Loss: 0.00002652
Iteration 227/1000 | Loss: 0.00004515
Iteration 228/1000 | Loss: 0.00002565
Iteration 229/1000 | Loss: 0.00002175
Iteration 230/1000 | Loss: 0.00002003
Iteration 231/1000 | Loss: 0.00001916
Iteration 232/1000 | Loss: 0.00001874
Iteration 233/1000 | Loss: 0.00001840
Iteration 234/1000 | Loss: 0.00001814
Iteration 235/1000 | Loss: 0.00001790
Iteration 236/1000 | Loss: 0.00001768
Iteration 237/1000 | Loss: 0.00001759
Iteration 238/1000 | Loss: 0.00001757
Iteration 239/1000 | Loss: 0.00001757
Iteration 240/1000 | Loss: 0.00001757
Iteration 241/1000 | Loss: 0.00001757
Iteration 242/1000 | Loss: 0.00001757
Iteration 243/1000 | Loss: 0.00001757
Iteration 244/1000 | Loss: 0.00001757
Iteration 245/1000 | Loss: 0.00001757
Iteration 246/1000 | Loss: 0.00001757
Iteration 247/1000 | Loss: 0.00001757
Iteration 248/1000 | Loss: 0.00001757
Iteration 249/1000 | Loss: 0.00001756
Iteration 250/1000 | Loss: 0.00001756
Iteration 251/1000 | Loss: 0.00001756
Iteration 252/1000 | Loss: 0.00001756
Iteration 253/1000 | Loss: 0.00001756
Iteration 254/1000 | Loss: 0.00001756
Iteration 255/1000 | Loss: 0.00001756
Iteration 256/1000 | Loss: 0.00001754
Iteration 257/1000 | Loss: 0.00001753
Iteration 258/1000 | Loss: 0.00001753
Iteration 259/1000 | Loss: 0.00001752
Iteration 260/1000 | Loss: 0.00001752
Iteration 261/1000 | Loss: 0.00001752
Iteration 262/1000 | Loss: 0.00001747
Iteration 263/1000 | Loss: 0.00001745
Iteration 264/1000 | Loss: 0.00001744
Iteration 265/1000 | Loss: 0.00001744
Iteration 266/1000 | Loss: 0.00001743
Iteration 267/1000 | Loss: 0.00001742
Iteration 268/1000 | Loss: 0.00001742
Iteration 269/1000 | Loss: 0.00001742
Iteration 270/1000 | Loss: 0.00001741
Iteration 271/1000 | Loss: 0.00001741
Iteration 272/1000 | Loss: 0.00001740
Iteration 273/1000 | Loss: 0.00001740
Iteration 274/1000 | Loss: 0.00001739
Iteration 275/1000 | Loss: 0.00001738
Iteration 276/1000 | Loss: 0.00001736
Iteration 277/1000 | Loss: 0.00001735
Iteration 278/1000 | Loss: 0.00001735
Iteration 279/1000 | Loss: 0.00001734
Iteration 280/1000 | Loss: 0.00001734
Iteration 281/1000 | Loss: 0.00001733
Iteration 282/1000 | Loss: 0.00001733
Iteration 283/1000 | Loss: 0.00001733
Iteration 284/1000 | Loss: 0.00001732
Iteration 285/1000 | Loss: 0.00001732
Iteration 286/1000 | Loss: 0.00001732
Iteration 287/1000 | Loss: 0.00001731
Iteration 288/1000 | Loss: 0.00001731
Iteration 289/1000 | Loss: 0.00001731
Iteration 290/1000 | Loss: 0.00001730
Iteration 291/1000 | Loss: 0.00001730
Iteration 292/1000 | Loss: 0.00001729
Iteration 293/1000 | Loss: 0.00001729
Iteration 294/1000 | Loss: 0.00001729
Iteration 295/1000 | Loss: 0.00001729
Iteration 296/1000 | Loss: 0.00001728
Iteration 297/1000 | Loss: 0.00001728
Iteration 298/1000 | Loss: 0.00001728
Iteration 299/1000 | Loss: 0.00001727
Iteration 300/1000 | Loss: 0.00001727
Iteration 301/1000 | Loss: 0.00001727
Iteration 302/1000 | Loss: 0.00001727
Iteration 303/1000 | Loss: 0.00001726
Iteration 304/1000 | Loss: 0.00001726
Iteration 305/1000 | Loss: 0.00001726
Iteration 306/1000 | Loss: 0.00001726
Iteration 307/1000 | Loss: 0.00001726
Iteration 308/1000 | Loss: 0.00001725
Iteration 309/1000 | Loss: 0.00001725
Iteration 310/1000 | Loss: 0.00001725
Iteration 311/1000 | Loss: 0.00001725
Iteration 312/1000 | Loss: 0.00001725
Iteration 313/1000 | Loss: 0.00001725
Iteration 314/1000 | Loss: 0.00001725
Iteration 315/1000 | Loss: 0.00001725
Iteration 316/1000 | Loss: 0.00001725
Iteration 317/1000 | Loss: 0.00001724
Iteration 318/1000 | Loss: 0.00001724
Iteration 319/1000 | Loss: 0.00001724
Iteration 320/1000 | Loss: 0.00001724
Iteration 321/1000 | Loss: 0.00001724
Iteration 322/1000 | Loss: 0.00001724
Iteration 323/1000 | Loss: 0.00001724
Iteration 324/1000 | Loss: 0.00001724
Iteration 325/1000 | Loss: 0.00001724
Iteration 326/1000 | Loss: 0.00001724
Iteration 327/1000 | Loss: 0.00001723
Iteration 328/1000 | Loss: 0.00001723
Iteration 329/1000 | Loss: 0.00001723
Iteration 330/1000 | Loss: 0.00001723
Iteration 331/1000 | Loss: 0.00001723
Iteration 332/1000 | Loss: 0.00001723
Iteration 333/1000 | Loss: 0.00001723
Iteration 334/1000 | Loss: 0.00001723
Iteration 335/1000 | Loss: 0.00001723
Iteration 336/1000 | Loss: 0.00001723
Iteration 337/1000 | Loss: 0.00001723
Iteration 338/1000 | Loss: 0.00001723
Iteration 339/1000 | Loss: 0.00001723
Iteration 340/1000 | Loss: 0.00001723
Iteration 341/1000 | Loss: 0.00001723
Iteration 342/1000 | Loss: 0.00001723
Iteration 343/1000 | Loss: 0.00001723
Iteration 344/1000 | Loss: 0.00001723
Iteration 345/1000 | Loss: 0.00001723
Iteration 346/1000 | Loss: 0.00001722
Iteration 347/1000 | Loss: 0.00001722
Iteration 348/1000 | Loss: 0.00001722
Iteration 349/1000 | Loss: 0.00001722
Iteration 350/1000 | Loss: 0.00001722
Iteration 351/1000 | Loss: 0.00001722
Iteration 352/1000 | Loss: 0.00001722
Iteration 353/1000 | Loss: 0.00001722
Iteration 354/1000 | Loss: 0.00001722
Iteration 355/1000 | Loss: 0.00001722
Iteration 356/1000 | Loss: 0.00001722
Iteration 357/1000 | Loss: 0.00001722
Iteration 358/1000 | Loss: 0.00001722
Iteration 359/1000 | Loss: 0.00001722
Iteration 360/1000 | Loss: 0.00001722
Iteration 361/1000 | Loss: 0.00001722
Iteration 362/1000 | Loss: 0.00001722
Iteration 363/1000 | Loss: 0.00001722
Iteration 364/1000 | Loss: 0.00001721
Iteration 365/1000 | Loss: 0.00001721
Iteration 366/1000 | Loss: 0.00001721
Iteration 367/1000 | Loss: 0.00001721
Iteration 368/1000 | Loss: 0.00001721
Iteration 369/1000 | Loss: 0.00001721
Iteration 370/1000 | Loss: 0.00001721
Iteration 371/1000 | Loss: 0.00001721
Iteration 372/1000 | Loss: 0.00001721
Iteration 373/1000 | Loss: 0.00001721
Iteration 374/1000 | Loss: 0.00001721
Iteration 375/1000 | Loss: 0.00001721
Iteration 376/1000 | Loss: 0.00001721
Iteration 377/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 377. Stopping optimization.
Last 5 losses: [1.7210848454851657e-05, 1.7210848454851657e-05, 1.7210848454851657e-05, 1.7210848454851657e-05, 1.7210848454851657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7210848454851657e-05

Optimization complete. Final v2v error: 3.39389967918396 mm

Highest mean error: 6.639810562133789 mm for frame 119

Lowest mean error: 3.1424691677093506 mm for frame 200

Saving results

Total time: 11955.630542039871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1055
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004467
Iteration 2/25 | Loss: 0.00247482
Iteration 3/25 | Loss: 0.00185299
Iteration 4/25 | Loss: 0.00171300
Iteration 5/25 | Loss: 0.00164647
Iteration 6/25 | Loss: 0.00152644
Iteration 7/25 | Loss: 0.00146953
Iteration 8/25 | Loss: 0.00144932
Iteration 9/25 | Loss: 0.00143192
Iteration 10/25 | Loss: 0.00141064
Iteration 11/25 | Loss: 0.00139748
Iteration 12/25 | Loss: 0.00140103
Iteration 13/25 | Loss: 0.00139543
Iteration 14/25 | Loss: 0.00138893
Iteration 15/25 | Loss: 0.00138554
Iteration 16/25 | Loss: 0.00138426
Iteration 17/25 | Loss: 0.00138362
Iteration 18/25 | Loss: 0.00138301
Iteration 19/25 | Loss: 0.00138256
Iteration 20/25 | Loss: 0.00138217
Iteration 21/25 | Loss: 0.00138193
Iteration 22/25 | Loss: 0.00138181
Iteration 23/25 | Loss: 0.00138177
Iteration 24/25 | Loss: 0.00138177
Iteration 25/25 | Loss: 0.00138177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49239647
Iteration 2/25 | Loss: 0.00382312
Iteration 3/25 | Loss: 0.00382311
Iteration 4/25 | Loss: 0.00382311
Iteration 5/25 | Loss: 0.00382311
Iteration 6/25 | Loss: 0.00382311
Iteration 7/25 | Loss: 0.00382311
Iteration 8/25 | Loss: 0.00382311
Iteration 9/25 | Loss: 0.00382311
Iteration 10/25 | Loss: 0.00382311
Iteration 11/25 | Loss: 0.00382311
Iteration 12/25 | Loss: 0.00382311
Iteration 13/25 | Loss: 0.00382311
Iteration 14/25 | Loss: 0.00382311
Iteration 15/25 | Loss: 0.00382311
Iteration 16/25 | Loss: 0.00382311
Iteration 17/25 | Loss: 0.00382311
Iteration 18/25 | Loss: 0.00382311
Iteration 19/25 | Loss: 0.00382311
Iteration 20/25 | Loss: 0.00382311
Iteration 21/25 | Loss: 0.00382311
Iteration 22/25 | Loss: 0.00382311
Iteration 23/25 | Loss: 0.00382311
Iteration 24/25 | Loss: 0.00382311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0038231112994253635, 0.0038231112994253635, 0.0038231112994253635, 0.0038231112994253635, 0.0038231112994253635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038231112994253635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00382311
Iteration 2/1000 | Loss: 0.00965124
Iteration 3/1000 | Loss: 0.00417745
Iteration 4/1000 | Loss: 0.00232663
Iteration 5/1000 | Loss: 0.00077174
Iteration 6/1000 | Loss: 0.00045057
Iteration 7/1000 | Loss: 0.00042010
Iteration 8/1000 | Loss: 0.00029476
Iteration 9/1000 | Loss: 0.00016337
Iteration 10/1000 | Loss: 0.00010566
Iteration 11/1000 | Loss: 0.00040504
Iteration 12/1000 | Loss: 0.00012443
Iteration 13/1000 | Loss: 0.00006304
Iteration 14/1000 | Loss: 0.00088705
Iteration 15/1000 | Loss: 0.00133484
Iteration 16/1000 | Loss: 0.00088771
Iteration 17/1000 | Loss: 0.00058356
Iteration 18/1000 | Loss: 0.00037371
Iteration 19/1000 | Loss: 0.00152911
Iteration 20/1000 | Loss: 0.00053697
Iteration 21/1000 | Loss: 0.00058036
Iteration 22/1000 | Loss: 0.00018290
Iteration 23/1000 | Loss: 0.00029083
Iteration 24/1000 | Loss: 0.00034809
Iteration 25/1000 | Loss: 0.00033516
Iteration 26/1000 | Loss: 0.00016165
Iteration 27/1000 | Loss: 0.00015491
Iteration 28/1000 | Loss: 0.00034382
Iteration 29/1000 | Loss: 0.00020381
Iteration 30/1000 | Loss: 0.00017122
Iteration 31/1000 | Loss: 0.00013553
Iteration 32/1000 | Loss: 0.00020976
Iteration 33/1000 | Loss: 0.00012145
Iteration 34/1000 | Loss: 0.00028311
Iteration 35/1000 | Loss: 0.00008358
Iteration 36/1000 | Loss: 0.00005477
Iteration 37/1000 | Loss: 0.00023741
Iteration 38/1000 | Loss: 0.00039747
Iteration 39/1000 | Loss: 0.00018994
Iteration 40/1000 | Loss: 0.00018012
Iteration 41/1000 | Loss: 0.00033025
Iteration 42/1000 | Loss: 0.00009078
Iteration 43/1000 | Loss: 0.00005147
Iteration 44/1000 | Loss: 0.00004270
Iteration 45/1000 | Loss: 0.00016097
Iteration 46/1000 | Loss: 0.00003679
Iteration 47/1000 | Loss: 0.00003134
Iteration 48/1000 | Loss: 0.00002907
Iteration 49/1000 | Loss: 0.00024280
Iteration 50/1000 | Loss: 0.00017225
Iteration 51/1000 | Loss: 0.00010981
Iteration 52/1000 | Loss: 0.00003375
Iteration 53/1000 | Loss: 0.00002789
Iteration 54/1000 | Loss: 0.00002502
Iteration 55/1000 | Loss: 0.00003005
Iteration 56/1000 | Loss: 0.00002678
Iteration 57/1000 | Loss: 0.00014995
Iteration 58/1000 | Loss: 0.00008393
Iteration 59/1000 | Loss: 0.00003504
Iteration 60/1000 | Loss: 0.00002751
Iteration 61/1000 | Loss: 0.00002979
Iteration 62/1000 | Loss: 0.00002482
Iteration 63/1000 | Loss: 0.00002362
Iteration 64/1000 | Loss: 0.00002294
Iteration 65/1000 | Loss: 0.00011979
Iteration 66/1000 | Loss: 0.00004281
Iteration 67/1000 | Loss: 0.00002587
Iteration 68/1000 | Loss: 0.00002474
Iteration 69/1000 | Loss: 0.00002360
Iteration 70/1000 | Loss: 0.00002261
Iteration 71/1000 | Loss: 0.00002171
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002099
Iteration 74/1000 | Loss: 0.00002097
Iteration 75/1000 | Loss: 0.00002097
Iteration 76/1000 | Loss: 0.00002096
Iteration 77/1000 | Loss: 0.00002095
Iteration 78/1000 | Loss: 0.00002094
Iteration 79/1000 | Loss: 0.00002094
Iteration 80/1000 | Loss: 0.00002093
Iteration 81/1000 | Loss: 0.00002093
Iteration 82/1000 | Loss: 0.00002093
Iteration 83/1000 | Loss: 0.00002093
Iteration 84/1000 | Loss: 0.00002092
Iteration 85/1000 | Loss: 0.00002092
Iteration 86/1000 | Loss: 0.00002092
Iteration 87/1000 | Loss: 0.00002092
Iteration 88/1000 | Loss: 0.00002092
Iteration 89/1000 | Loss: 0.00002092
Iteration 90/1000 | Loss: 0.00002092
Iteration 91/1000 | Loss: 0.00002092
Iteration 92/1000 | Loss: 0.00002092
Iteration 93/1000 | Loss: 0.00002092
Iteration 94/1000 | Loss: 0.00002092
Iteration 95/1000 | Loss: 0.00002092
Iteration 96/1000 | Loss: 0.00002092
Iteration 97/1000 | Loss: 0.00002092
Iteration 98/1000 | Loss: 0.00002091
Iteration 99/1000 | Loss: 0.00002091
Iteration 100/1000 | Loss: 0.00002091
Iteration 101/1000 | Loss: 0.00002091
Iteration 102/1000 | Loss: 0.00002091
Iteration 103/1000 | Loss: 0.00002091
Iteration 104/1000 | Loss: 0.00002091
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002090
Iteration 107/1000 | Loss: 0.00002090
Iteration 108/1000 | Loss: 0.00002090
Iteration 109/1000 | Loss: 0.00002090
Iteration 110/1000 | Loss: 0.00002090
Iteration 111/1000 | Loss: 0.00002090
Iteration 112/1000 | Loss: 0.00002090
Iteration 113/1000 | Loss: 0.00002090
Iteration 114/1000 | Loss: 0.00002090
Iteration 115/1000 | Loss: 0.00002090
Iteration 116/1000 | Loss: 0.00002090
Iteration 117/1000 | Loss: 0.00002089
Iteration 118/1000 | Loss: 0.00002089
Iteration 119/1000 | Loss: 0.00002089
Iteration 120/1000 | Loss: 0.00002089
Iteration 121/1000 | Loss: 0.00002089
Iteration 122/1000 | Loss: 0.00002089
Iteration 123/1000 | Loss: 0.00002089
Iteration 124/1000 | Loss: 0.00002089
Iteration 125/1000 | Loss: 0.00002089
Iteration 126/1000 | Loss: 0.00002088
Iteration 127/1000 | Loss: 0.00002088
Iteration 128/1000 | Loss: 0.00002088
Iteration 129/1000 | Loss: 0.00002088
Iteration 130/1000 | Loss: 0.00002088
Iteration 131/1000 | Loss: 0.00002088
Iteration 132/1000 | Loss: 0.00002088
Iteration 133/1000 | Loss: 0.00002088
Iteration 134/1000 | Loss: 0.00002088
Iteration 135/1000 | Loss: 0.00002088
Iteration 136/1000 | Loss: 0.00002088
Iteration 137/1000 | Loss: 0.00002088
Iteration 138/1000 | Loss: 0.00002088
Iteration 139/1000 | Loss: 0.00002088
Iteration 140/1000 | Loss: 0.00002087
Iteration 141/1000 | Loss: 0.00002087
Iteration 142/1000 | Loss: 0.00002087
Iteration 143/1000 | Loss: 0.00002087
Iteration 144/1000 | Loss: 0.00002087
Iteration 145/1000 | Loss: 0.00002087
Iteration 146/1000 | Loss: 0.00002087
Iteration 147/1000 | Loss: 0.00002087
Iteration 148/1000 | Loss: 0.00002087
Iteration 149/1000 | Loss: 0.00002087
Iteration 150/1000 | Loss: 0.00002087
Iteration 151/1000 | Loss: 0.00002087
Iteration 152/1000 | Loss: 0.00002087
Iteration 153/1000 | Loss: 0.00002087
Iteration 154/1000 | Loss: 0.00002087
Iteration 155/1000 | Loss: 0.00002087
Iteration 156/1000 | Loss: 0.00002087
Iteration 157/1000 | Loss: 0.00002087
Iteration 158/1000 | Loss: 0.00002087
Iteration 159/1000 | Loss: 0.00002087
Iteration 160/1000 | Loss: 0.00002087
Iteration 161/1000 | Loss: 0.00002086
Iteration 162/1000 | Loss: 0.00002086
Iteration 163/1000 | Loss: 0.00002086
Iteration 164/1000 | Loss: 0.00002086
Iteration 165/1000 | Loss: 0.00002086
Iteration 166/1000 | Loss: 0.00002086
Iteration 167/1000 | Loss: 0.00002086
Iteration 168/1000 | Loss: 0.00002086
Iteration 169/1000 | Loss: 0.00002086
Iteration 170/1000 | Loss: 0.00002086
Iteration 171/1000 | Loss: 0.00002086
Iteration 172/1000 | Loss: 0.00002086
Iteration 173/1000 | Loss: 0.00002086
Iteration 174/1000 | Loss: 0.00002086
Iteration 175/1000 | Loss: 0.00002086
Iteration 176/1000 | Loss: 0.00002086
Iteration 177/1000 | Loss: 0.00002086
Iteration 178/1000 | Loss: 0.00002086
Iteration 179/1000 | Loss: 0.00002086
Iteration 180/1000 | Loss: 0.00002086
Iteration 181/1000 | Loss: 0.00002086
Iteration 182/1000 | Loss: 0.00002086
Iteration 183/1000 | Loss: 0.00002086
Iteration 184/1000 | Loss: 0.00002085
Iteration 185/1000 | Loss: 0.00002085
Iteration 186/1000 | Loss: 0.00002085
Iteration 187/1000 | Loss: 0.00002085
Iteration 188/1000 | Loss: 0.00002085
Iteration 189/1000 | Loss: 0.00002085
Iteration 190/1000 | Loss: 0.00002085
Iteration 191/1000 | Loss: 0.00002085
Iteration 192/1000 | Loss: 0.00002085
Iteration 193/1000 | Loss: 0.00002085
Iteration 194/1000 | Loss: 0.00002085
Iteration 195/1000 | Loss: 0.00002085
Iteration 196/1000 | Loss: 0.00002085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.0854780814261176e-05, 2.0854780814261176e-05, 2.0854780814261176e-05, 2.0854780814261176e-05, 2.0854780814261176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0854780814261176e-05

Optimization complete. Final v2v error: 3.5649733543395996 mm

Highest mean error: 12.441823959350586 mm for frame 101

Lowest mean error: 3.292692184448242 mm for frame 59

Saving results

Total time: 5016.053245544434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1043
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837842
Iteration 2/25 | Loss: 0.00120551
Iteration 3/25 | Loss: 0.00083265
Iteration 4/25 | Loss: 0.00076502
Iteration 5/25 | Loss: 0.00075168
Iteration 6/25 | Loss: 0.00074819
Iteration 7/25 | Loss: 0.00074666
Iteration 8/25 | Loss: 0.00074603
Iteration 9/25 | Loss: 0.00074579
Iteration 10/25 | Loss: 0.00074567
Iteration 11/25 | Loss: 0.00074560
Iteration 12/25 | Loss: 0.00074560
Iteration 13/25 | Loss: 0.00074560
Iteration 14/25 | Loss: 0.00074560
Iteration 15/25 | Loss: 0.00074560
Iteration 16/25 | Loss: 0.00074560
Iteration 17/25 | Loss: 0.00074560
Iteration 18/25 | Loss: 0.00074560
Iteration 19/25 | Loss: 0.00074560
Iteration 20/25 | Loss: 0.00074560
Iteration 21/25 | Loss: 0.00074560
Iteration 22/25 | Loss: 0.00074560
Iteration 23/25 | Loss: 0.00074560
Iteration 24/25 | Loss: 0.00074560
Iteration 25/25 | Loss: 0.00074560

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50453782
Iteration 2/25 | Loss: 0.00058954
Iteration 3/25 | Loss: 0.00058954
Iteration 4/25 | Loss: 0.00058954
Iteration 5/25 | Loss: 0.00058953
Iteration 6/25 | Loss: 0.00058953
Iteration 7/25 | Loss: 0.00058953
Iteration 8/25 | Loss: 0.00058953
Iteration 9/25 | Loss: 0.00058953
Iteration 10/25 | Loss: 0.00058953
Iteration 11/25 | Loss: 0.00058953
Iteration 12/25 | Loss: 0.00058953
Iteration 13/25 | Loss: 0.00058953
Iteration 14/25 | Loss: 0.00058953
Iteration 15/25 | Loss: 0.00058953
Iteration 16/25 | Loss: 0.00058953
Iteration 17/25 | Loss: 0.00058953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005895333015359938, 0.0005895333015359938, 0.0005895333015359938, 0.0005895333015359938, 0.0005895333015359938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005895333015359938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058953
Iteration 2/1000 | Loss: 0.00003073
Iteration 3/1000 | Loss: 0.00001911
Iteration 4/1000 | Loss: 0.00006429
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001622
Iteration 7/1000 | Loss: 0.00001438
Iteration 8/1000 | Loss: 0.00001387
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001250
Iteration 12/1000 | Loss: 0.00001203
Iteration 13/1000 | Loss: 0.00001161
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001122
Iteration 23/1000 | Loss: 0.00001121
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001120
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001097
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001097
Iteration 44/1000 | Loss: 0.00001096
Iteration 45/1000 | Loss: 0.00001096
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001094
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001093
Iteration 60/1000 | Loss: 0.00001092
Iteration 61/1000 | Loss: 0.00001092
Iteration 62/1000 | Loss: 0.00001092
Iteration 63/1000 | Loss: 0.00001092
Iteration 64/1000 | Loss: 0.00001092
Iteration 65/1000 | Loss: 0.00001092
Iteration 66/1000 | Loss: 0.00001092
Iteration 67/1000 | Loss: 0.00001092
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001092
Iteration 72/1000 | Loss: 0.00001092
Iteration 73/1000 | Loss: 0.00001092
Iteration 74/1000 | Loss: 0.00001092
Iteration 75/1000 | Loss: 0.00001092
Iteration 76/1000 | Loss: 0.00001092
Iteration 77/1000 | Loss: 0.00001091
Iteration 78/1000 | Loss: 0.00001091
Iteration 79/1000 | Loss: 0.00001091
Iteration 80/1000 | Loss: 0.00001091
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001091
Iteration 83/1000 | Loss: 0.00001091
Iteration 84/1000 | Loss: 0.00001091
Iteration 85/1000 | Loss: 0.00001091
Iteration 86/1000 | Loss: 0.00001091
Iteration 87/1000 | Loss: 0.00001091
Iteration 88/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.0912415746133775e-05, 1.0912415746133775e-05, 1.0912415746133775e-05, 1.0912415746133775e-05, 1.0912415746133775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0912415746133775e-05

Optimization complete. Final v2v error: 2.789175033569336 mm

Highest mean error: 3.5423526763916016 mm for frame 63

Lowest mean error: 2.589911937713623 mm for frame 1

Saving results

Total time: 1228.563622713089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1085
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094111
Iteration 2/25 | Loss: 0.00203785
Iteration 3/25 | Loss: 0.00119816
Iteration 4/25 | Loss: 0.00102487
Iteration 5/25 | Loss: 0.00099531
Iteration 6/25 | Loss: 0.00096588
Iteration 7/25 | Loss: 0.00091937
Iteration 8/25 | Loss: 0.00087741
Iteration 9/25 | Loss: 0.00086000
Iteration 10/25 | Loss: 0.00084735
Iteration 11/25 | Loss: 0.00084077
Iteration 12/25 | Loss: 0.00083985
Iteration 13/25 | Loss: 0.00082899
Iteration 14/25 | Loss: 0.00081851
Iteration 15/25 | Loss: 0.00081803
Iteration 16/25 | Loss: 0.00081055
Iteration 17/25 | Loss: 0.00081137
Iteration 18/25 | Loss: 0.00081131
Iteration 19/25 | Loss: 0.00081115
Iteration 20/25 | Loss: 0.00080876
Iteration 21/25 | Loss: 0.00080849
Iteration 22/25 | Loss: 0.00080844
Iteration 23/25 | Loss: 0.00080440
Iteration 24/25 | Loss: 0.00079879
Iteration 25/25 | Loss: 0.00079866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67531419
Iteration 2/25 | Loss: 0.00085120
Iteration 3/25 | Loss: 0.00084514
Iteration 4/25 | Loss: 0.00084514
Iteration 5/25 | Loss: 0.00084514
Iteration 6/25 | Loss: 0.00084514
Iteration 7/25 | Loss: 0.00084514
Iteration 8/25 | Loss: 0.00084514
Iteration 9/25 | Loss: 0.00084514
Iteration 10/25 | Loss: 0.00084514
Iteration 11/25 | Loss: 0.00084514
Iteration 12/25 | Loss: 0.00084514
Iteration 13/25 | Loss: 0.00084514
Iteration 14/25 | Loss: 0.00084514
Iteration 15/25 | Loss: 0.00084514
Iteration 16/25 | Loss: 0.00084514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008451379835605621, 0.0008451379835605621, 0.0008451379835605621, 0.0008451379835605621, 0.0008451379835605621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008451379835605621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084514
Iteration 2/1000 | Loss: 0.00013905
Iteration 3/1000 | Loss: 0.00004953
Iteration 4/1000 | Loss: 0.00009645
Iteration 5/1000 | Loss: 0.00003059
Iteration 6/1000 | Loss: 0.00007056
Iteration 7/1000 | Loss: 0.00002577
Iteration 8/1000 | Loss: 0.00004111
Iteration 9/1000 | Loss: 0.00002538
Iteration 10/1000 | Loss: 0.00010780
Iteration 11/1000 | Loss: 0.00029456
Iteration 12/1000 | Loss: 0.00020981
Iteration 13/1000 | Loss: 0.00014114
Iteration 14/1000 | Loss: 0.00005885
Iteration 15/1000 | Loss: 0.00011636
Iteration 16/1000 | Loss: 0.00005364
Iteration 17/1000 | Loss: 0.00020268
Iteration 18/1000 | Loss: 0.00006637
Iteration 19/1000 | Loss: 0.00011416
Iteration 20/1000 | Loss: 0.00037628
Iteration 21/1000 | Loss: 0.00003511
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002022
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001823
Iteration 26/1000 | Loss: 0.00001759
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00009433
Iteration 29/1000 | Loss: 0.00010301
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00012428
Iteration 32/1000 | Loss: 0.00013145
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00010593
Iteration 35/1000 | Loss: 0.00002121
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001705
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001550
Iteration 40/1000 | Loss: 0.00001509
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001480
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001477
Iteration 52/1000 | Loss: 0.00001477
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001473
Iteration 58/1000 | Loss: 0.00001472
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001471
Iteration 61/1000 | Loss: 0.00001470
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001468
Iteration 65/1000 | Loss: 0.00001468
Iteration 66/1000 | Loss: 0.00001467
Iteration 67/1000 | Loss: 0.00001467
Iteration 68/1000 | Loss: 0.00001467
Iteration 69/1000 | Loss: 0.00001466
Iteration 70/1000 | Loss: 0.00001466
Iteration 71/1000 | Loss: 0.00001466
Iteration 72/1000 | Loss: 0.00001465
Iteration 73/1000 | Loss: 0.00001465
Iteration 74/1000 | Loss: 0.00001465
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001464
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001462
Iteration 79/1000 | Loss: 0.00001461
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001456
Iteration 86/1000 | Loss: 0.00001456
Iteration 87/1000 | Loss: 0.00001456
Iteration 88/1000 | Loss: 0.00001456
Iteration 89/1000 | Loss: 0.00001455
Iteration 90/1000 | Loss: 0.00001455
Iteration 91/1000 | Loss: 0.00001455
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001452
Iteration 105/1000 | Loss: 0.00001450
Iteration 106/1000 | Loss: 0.00001450
Iteration 107/1000 | Loss: 0.00001450
Iteration 108/1000 | Loss: 0.00001450
Iteration 109/1000 | Loss: 0.00001450
Iteration 110/1000 | Loss: 0.00001450
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001449
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001448
Iteration 116/1000 | Loss: 0.00001448
Iteration 117/1000 | Loss: 0.00001448
Iteration 118/1000 | Loss: 0.00001448
Iteration 119/1000 | Loss: 0.00001447
Iteration 120/1000 | Loss: 0.00001447
Iteration 121/1000 | Loss: 0.00001447
Iteration 122/1000 | Loss: 0.00001446
Iteration 123/1000 | Loss: 0.00001446
Iteration 124/1000 | Loss: 0.00001446
Iteration 125/1000 | Loss: 0.00001446
Iteration 126/1000 | Loss: 0.00001446
Iteration 127/1000 | Loss: 0.00001446
Iteration 128/1000 | Loss: 0.00001446
Iteration 129/1000 | Loss: 0.00001445
Iteration 130/1000 | Loss: 0.00001445
Iteration 131/1000 | Loss: 0.00001445
Iteration 132/1000 | Loss: 0.00001445
Iteration 133/1000 | Loss: 0.00001445
Iteration 134/1000 | Loss: 0.00001445
Iteration 135/1000 | Loss: 0.00001445
Iteration 136/1000 | Loss: 0.00001444
Iteration 137/1000 | Loss: 0.00001444
Iteration 138/1000 | Loss: 0.00001444
Iteration 139/1000 | Loss: 0.00001444
Iteration 140/1000 | Loss: 0.00001444
Iteration 141/1000 | Loss: 0.00001444
Iteration 142/1000 | Loss: 0.00001443
Iteration 143/1000 | Loss: 0.00001443
Iteration 144/1000 | Loss: 0.00001443
Iteration 145/1000 | Loss: 0.00001443
Iteration 146/1000 | Loss: 0.00001443
Iteration 147/1000 | Loss: 0.00001443
Iteration 148/1000 | Loss: 0.00001443
Iteration 149/1000 | Loss: 0.00001443
Iteration 150/1000 | Loss: 0.00001443
Iteration 151/1000 | Loss: 0.00001443
Iteration 152/1000 | Loss: 0.00001443
Iteration 153/1000 | Loss: 0.00001443
Iteration 154/1000 | Loss: 0.00001443
Iteration 155/1000 | Loss: 0.00001443
Iteration 156/1000 | Loss: 0.00001443
Iteration 157/1000 | Loss: 0.00001443
Iteration 158/1000 | Loss: 0.00001443
Iteration 159/1000 | Loss: 0.00001442
Iteration 160/1000 | Loss: 0.00001442
Iteration 161/1000 | Loss: 0.00001442
Iteration 162/1000 | Loss: 0.00001442
Iteration 163/1000 | Loss: 0.00001442
Iteration 164/1000 | Loss: 0.00001442
Iteration 165/1000 | Loss: 0.00001442
Iteration 166/1000 | Loss: 0.00001442
Iteration 167/1000 | Loss: 0.00001442
Iteration 168/1000 | Loss: 0.00001442
Iteration 169/1000 | Loss: 0.00001442
Iteration 170/1000 | Loss: 0.00001442
Iteration 171/1000 | Loss: 0.00001442
Iteration 172/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4421467312786262e-05, 1.4421467312786262e-05, 1.4421467312786262e-05, 1.4421467312786262e-05, 1.4421467312786262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4421467312786262e-05

Optimization complete. Final v2v error: 3.0927865505218506 mm

Highest mean error: 5.819309711456299 mm for frame 197

Lowest mean error: 2.594982147216797 mm for frame 162

Saving results

Total time: 4127.110332250595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1000
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061910
Iteration 2/25 | Loss: 0.00244642
Iteration 3/25 | Loss: 0.00168915
Iteration 4/25 | Loss: 0.00152825
Iteration 5/25 | Loss: 0.00142441
Iteration 6/25 | Loss: 0.00125923
Iteration 7/25 | Loss: 0.00119609
Iteration 8/25 | Loss: 0.00116961
Iteration 9/25 | Loss: 0.00112726
Iteration 10/25 | Loss: 0.00110112
Iteration 11/25 | Loss: 0.00109397
Iteration 12/25 | Loss: 0.00108569
Iteration 13/25 | Loss: 0.00108485
Iteration 14/25 | Loss: 0.00106538
Iteration 15/25 | Loss: 0.00106399
Iteration 16/25 | Loss: 0.00105551
Iteration 17/25 | Loss: 0.00105598
Iteration 18/25 | Loss: 0.00105126
Iteration 19/25 | Loss: 0.00104343
Iteration 20/25 | Loss: 0.00104894
Iteration 21/25 | Loss: 0.00104243
Iteration 22/25 | Loss: 0.00103725
Iteration 23/25 | Loss: 0.00103561
Iteration 24/25 | Loss: 0.00103510
Iteration 25/25 | Loss: 0.00104258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94016719
Iteration 2/25 | Loss: 0.00253068
Iteration 3/25 | Loss: 0.00253068
Iteration 4/25 | Loss: 0.00253068
Iteration 5/25 | Loss: 0.00253068
Iteration 6/25 | Loss: 0.00253068
Iteration 7/25 | Loss: 0.00253068
Iteration 8/25 | Loss: 0.00253068
Iteration 9/25 | Loss: 0.00253068
Iteration 10/25 | Loss: 0.00253068
Iteration 11/25 | Loss: 0.00253068
Iteration 12/25 | Loss: 0.00253068
Iteration 13/25 | Loss: 0.00253068
Iteration 14/25 | Loss: 0.00253068
Iteration 15/25 | Loss: 0.00253068
Iteration 16/25 | Loss: 0.00253068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025306798052042723, 0.0025306798052042723, 0.0025306798052042723, 0.0025306798052042723, 0.0025306798052042723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025306798052042723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253068
Iteration 2/1000 | Loss: 0.00171417
Iteration 3/1000 | Loss: 0.00189860
Iteration 4/1000 | Loss: 0.00114181
Iteration 5/1000 | Loss: 0.00026943
Iteration 6/1000 | Loss: 0.00020995
Iteration 7/1000 | Loss: 0.00032067
Iteration 8/1000 | Loss: 0.00073423
Iteration 9/1000 | Loss: 0.00063534
Iteration 10/1000 | Loss: 0.00014978
Iteration 11/1000 | Loss: 0.00013634
Iteration 12/1000 | Loss: 0.00013340
Iteration 13/1000 | Loss: 0.00012349
Iteration 14/1000 | Loss: 0.00054320
Iteration 15/1000 | Loss: 0.00020806
Iteration 16/1000 | Loss: 0.00058960
Iteration 17/1000 | Loss: 0.00017084
Iteration 18/1000 | Loss: 0.00050129
Iteration 19/1000 | Loss: 0.00029743
Iteration 20/1000 | Loss: 0.00016410
Iteration 21/1000 | Loss: 0.00052376
Iteration 22/1000 | Loss: 0.00018986
Iteration 23/1000 | Loss: 0.00011603
Iteration 24/1000 | Loss: 0.00046849
Iteration 25/1000 | Loss: 0.00088222
Iteration 26/1000 | Loss: 0.00100989
Iteration 27/1000 | Loss: 0.00090092
Iteration 28/1000 | Loss: 0.00016639
Iteration 29/1000 | Loss: 0.00011511
Iteration 30/1000 | Loss: 0.00061976
Iteration 31/1000 | Loss: 0.00018694
Iteration 32/1000 | Loss: 0.00050451
Iteration 33/1000 | Loss: 0.00056736
Iteration 34/1000 | Loss: 0.00045893
Iteration 35/1000 | Loss: 0.00011420
Iteration 36/1000 | Loss: 0.00041438
Iteration 37/1000 | Loss: 0.00119079
Iteration 38/1000 | Loss: 0.00088819
Iteration 39/1000 | Loss: 0.00019346
Iteration 40/1000 | Loss: 0.00035049
Iteration 41/1000 | Loss: 0.00049839
Iteration 42/1000 | Loss: 0.00035976
Iteration 43/1000 | Loss: 0.00013514
Iteration 44/1000 | Loss: 0.00048524
Iteration 45/1000 | Loss: 0.00031981
Iteration 46/1000 | Loss: 0.00029966
Iteration 47/1000 | Loss: 0.00024148
Iteration 48/1000 | Loss: 0.00011689
Iteration 49/1000 | Loss: 0.00011114
Iteration 50/1000 | Loss: 0.00030560
Iteration 51/1000 | Loss: 0.00015730
Iteration 52/1000 | Loss: 0.00012139
Iteration 53/1000 | Loss: 0.00010675
Iteration 54/1000 | Loss: 0.00010305
Iteration 55/1000 | Loss: 0.00028187
Iteration 56/1000 | Loss: 0.00012447
Iteration 57/1000 | Loss: 0.00010714
Iteration 58/1000 | Loss: 0.00049787
Iteration 59/1000 | Loss: 0.00012212
Iteration 60/1000 | Loss: 0.00010743
Iteration 61/1000 | Loss: 0.00009742
Iteration 62/1000 | Loss: 0.00009369
Iteration 63/1000 | Loss: 0.00008915
Iteration 64/1000 | Loss: 0.00008694
Iteration 65/1000 | Loss: 0.00008571
Iteration 66/1000 | Loss: 0.00008461
Iteration 67/1000 | Loss: 0.00008362
Iteration 68/1000 | Loss: 0.00008295
Iteration 69/1000 | Loss: 0.00008212
Iteration 70/1000 | Loss: 0.00008152
Iteration 71/1000 | Loss: 0.00008083
Iteration 72/1000 | Loss: 0.00008036
Iteration 73/1000 | Loss: 0.00199437
Iteration 74/1000 | Loss: 0.00634585
Iteration 75/1000 | Loss: 0.00800719
Iteration 76/1000 | Loss: 0.00467317
Iteration 77/1000 | Loss: 0.00050444
Iteration 78/1000 | Loss: 0.00018996
Iteration 79/1000 | Loss: 0.00071653
Iteration 80/1000 | Loss: 0.00018462
Iteration 81/1000 | Loss: 0.00011638
Iteration 82/1000 | Loss: 0.00045003
Iteration 83/1000 | Loss: 0.00011547
Iteration 84/1000 | Loss: 0.00017512
Iteration 85/1000 | Loss: 0.00176372
Iteration 86/1000 | Loss: 0.00041643
Iteration 87/1000 | Loss: 0.00039816
Iteration 88/1000 | Loss: 0.00042656
Iteration 89/1000 | Loss: 0.00092604
Iteration 90/1000 | Loss: 0.00009563
Iteration 91/1000 | Loss: 0.00008021
Iteration 92/1000 | Loss: 0.00062279
Iteration 93/1000 | Loss: 0.00128245
Iteration 94/1000 | Loss: 0.00048469
Iteration 95/1000 | Loss: 0.00046947
Iteration 96/1000 | Loss: 0.00059278
Iteration 97/1000 | Loss: 0.00007926
Iteration 98/1000 | Loss: 0.00006528
Iteration 99/1000 | Loss: 0.00105069
Iteration 100/1000 | Loss: 0.00016862
Iteration 101/1000 | Loss: 0.00019855
Iteration 102/1000 | Loss: 0.00005866
Iteration 103/1000 | Loss: 0.00005315
Iteration 104/1000 | Loss: 0.00005096
Iteration 105/1000 | Loss: 0.00004995
Iteration 106/1000 | Loss: 0.00159210
Iteration 107/1000 | Loss: 0.00157951
Iteration 108/1000 | Loss: 0.00202293
Iteration 109/1000 | Loss: 0.00009192
Iteration 110/1000 | Loss: 0.00056095
Iteration 111/1000 | Loss: 0.00051585
Iteration 112/1000 | Loss: 0.00038841
Iteration 113/1000 | Loss: 0.00042471
Iteration 114/1000 | Loss: 0.00006609
Iteration 115/1000 | Loss: 0.00005901
Iteration 116/1000 | Loss: 0.00005348
Iteration 117/1000 | Loss: 0.00005077
Iteration 118/1000 | Loss: 0.00044446
Iteration 119/1000 | Loss: 0.00033316
Iteration 120/1000 | Loss: 0.00034707
Iteration 121/1000 | Loss: 0.00005329
Iteration 122/1000 | Loss: 0.00053416
Iteration 123/1000 | Loss: 0.00008121
Iteration 124/1000 | Loss: 0.00006379
Iteration 125/1000 | Loss: 0.00005271
Iteration 126/1000 | Loss: 0.00004857
Iteration 127/1000 | Loss: 0.00004579
Iteration 128/1000 | Loss: 0.00004462
Iteration 129/1000 | Loss: 0.00046308
Iteration 130/1000 | Loss: 0.00006167
Iteration 131/1000 | Loss: 0.00005397
Iteration 132/1000 | Loss: 0.00005063
Iteration 133/1000 | Loss: 0.00004877
Iteration 134/1000 | Loss: 0.00004751
Iteration 135/1000 | Loss: 0.00004615
Iteration 136/1000 | Loss: 0.00004527
Iteration 137/1000 | Loss: 0.00034125
Iteration 138/1000 | Loss: 0.00007394
Iteration 139/1000 | Loss: 0.00032491
Iteration 140/1000 | Loss: 0.00020984
Iteration 141/1000 | Loss: 0.00004665
Iteration 142/1000 | Loss: 0.00004325
Iteration 143/1000 | Loss: 0.00004188
Iteration 144/1000 | Loss: 0.00004084
Iteration 145/1000 | Loss: 0.00003981
Iteration 146/1000 | Loss: 0.00003934
Iteration 147/1000 | Loss: 0.00003917
Iteration 148/1000 | Loss: 0.00003904
Iteration 149/1000 | Loss: 0.00003902
Iteration 150/1000 | Loss: 0.00003890
Iteration 151/1000 | Loss: 0.00003885
Iteration 152/1000 | Loss: 0.00003880
Iteration 153/1000 | Loss: 0.00003870
Iteration 154/1000 | Loss: 0.00003867
Iteration 155/1000 | Loss: 0.00003863
Iteration 156/1000 | Loss: 0.00003859
Iteration 157/1000 | Loss: 0.00003858
Iteration 158/1000 | Loss: 0.00003858
Iteration 159/1000 | Loss: 0.00003858
Iteration 160/1000 | Loss: 0.00003857
Iteration 161/1000 | Loss: 0.00003856
Iteration 162/1000 | Loss: 0.00003856
Iteration 163/1000 | Loss: 0.00003856
Iteration 164/1000 | Loss: 0.00003856
Iteration 165/1000 | Loss: 0.00003856
Iteration 166/1000 | Loss: 0.00003856
Iteration 167/1000 | Loss: 0.00003855
Iteration 168/1000 | Loss: 0.00003855
Iteration 169/1000 | Loss: 0.00003855
Iteration 170/1000 | Loss: 0.00003854
Iteration 171/1000 | Loss: 0.00003854
Iteration 172/1000 | Loss: 0.00003853
Iteration 173/1000 | Loss: 0.00003853
Iteration 174/1000 | Loss: 0.00003852
Iteration 175/1000 | Loss: 0.00003852
Iteration 176/1000 | Loss: 0.00003852
Iteration 177/1000 | Loss: 0.00003851
Iteration 178/1000 | Loss: 0.00003851
Iteration 179/1000 | Loss: 0.00003851
Iteration 180/1000 | Loss: 0.00003850
Iteration 181/1000 | Loss: 0.00003850
Iteration 182/1000 | Loss: 0.00003850
Iteration 183/1000 | Loss: 0.00003850
Iteration 184/1000 | Loss: 0.00003849
Iteration 185/1000 | Loss: 0.00003849
Iteration 186/1000 | Loss: 0.00003849
Iteration 187/1000 | Loss: 0.00003848
Iteration 188/1000 | Loss: 0.00003848
Iteration 189/1000 | Loss: 0.00003848
Iteration 190/1000 | Loss: 0.00003848
Iteration 191/1000 | Loss: 0.00003848
Iteration 192/1000 | Loss: 0.00003848
Iteration 193/1000 | Loss: 0.00003847
Iteration 194/1000 | Loss: 0.00003847
Iteration 195/1000 | Loss: 0.00003847
Iteration 196/1000 | Loss: 0.00003847
Iteration 197/1000 | Loss: 0.00003846
Iteration 198/1000 | Loss: 0.00003846
Iteration 199/1000 | Loss: 0.00003845
Iteration 200/1000 | Loss: 0.00003845
Iteration 201/1000 | Loss: 0.00003845
Iteration 202/1000 | Loss: 0.00003845
Iteration 203/1000 | Loss: 0.00003845
Iteration 204/1000 | Loss: 0.00003845
Iteration 205/1000 | Loss: 0.00003844
Iteration 206/1000 | Loss: 0.00003844
Iteration 207/1000 | Loss: 0.00003844
Iteration 208/1000 | Loss: 0.00003844
Iteration 209/1000 | Loss: 0.00003843
Iteration 210/1000 | Loss: 0.00003843
Iteration 211/1000 | Loss: 0.00003843
Iteration 212/1000 | Loss: 0.00003843
Iteration 213/1000 | Loss: 0.00003843
Iteration 214/1000 | Loss: 0.00003842
Iteration 215/1000 | Loss: 0.00003842
Iteration 216/1000 | Loss: 0.00003842
Iteration 217/1000 | Loss: 0.00003842
Iteration 218/1000 | Loss: 0.00003842
Iteration 219/1000 | Loss: 0.00003842
Iteration 220/1000 | Loss: 0.00003842
Iteration 221/1000 | Loss: 0.00003841
Iteration 222/1000 | Loss: 0.00003841
Iteration 223/1000 | Loss: 0.00003841
Iteration 224/1000 | Loss: 0.00003841
Iteration 225/1000 | Loss: 0.00003841
Iteration 226/1000 | Loss: 0.00003841
Iteration 227/1000 | Loss: 0.00003841
Iteration 228/1000 | Loss: 0.00003841
Iteration 229/1000 | Loss: 0.00003841
Iteration 230/1000 | Loss: 0.00003840
Iteration 231/1000 | Loss: 0.00003840
Iteration 232/1000 | Loss: 0.00003840
Iteration 233/1000 | Loss: 0.00003840
Iteration 234/1000 | Loss: 0.00003840
Iteration 235/1000 | Loss: 0.00003840
Iteration 236/1000 | Loss: 0.00003839
Iteration 237/1000 | Loss: 0.00003839
Iteration 238/1000 | Loss: 0.00003839
Iteration 239/1000 | Loss: 0.00003839
Iteration 240/1000 | Loss: 0.00003838
Iteration 241/1000 | Loss: 0.00003838
Iteration 242/1000 | Loss: 0.00003838
Iteration 243/1000 | Loss: 0.00003837
Iteration 244/1000 | Loss: 0.00003837
Iteration 245/1000 | Loss: 0.00003837
Iteration 246/1000 | Loss: 0.00003837
Iteration 247/1000 | Loss: 0.00003837
Iteration 248/1000 | Loss: 0.00003837
Iteration 249/1000 | Loss: 0.00003837
Iteration 250/1000 | Loss: 0.00003837
Iteration 251/1000 | Loss: 0.00003837
Iteration 252/1000 | Loss: 0.00003837
Iteration 253/1000 | Loss: 0.00003836
Iteration 254/1000 | Loss: 0.00003836
Iteration 255/1000 | Loss: 0.00003836
Iteration 256/1000 | Loss: 0.00003836
Iteration 257/1000 | Loss: 0.00003836
Iteration 258/1000 | Loss: 0.00003836
Iteration 259/1000 | Loss: 0.00003835
Iteration 260/1000 | Loss: 0.00003835
Iteration 261/1000 | Loss: 0.00003835
Iteration 262/1000 | Loss: 0.00003835
Iteration 263/1000 | Loss: 0.00003835
Iteration 264/1000 | Loss: 0.00003834
Iteration 265/1000 | Loss: 0.00003834
Iteration 266/1000 | Loss: 0.00003834
Iteration 267/1000 | Loss: 0.00003834
Iteration 268/1000 | Loss: 0.00003834
Iteration 269/1000 | Loss: 0.00003834
Iteration 270/1000 | Loss: 0.00003834
Iteration 271/1000 | Loss: 0.00003834
Iteration 272/1000 | Loss: 0.00003834
Iteration 273/1000 | Loss: 0.00003834
Iteration 274/1000 | Loss: 0.00003833
Iteration 275/1000 | Loss: 0.00003833
Iteration 276/1000 | Loss: 0.00003833
Iteration 277/1000 | Loss: 0.00003833
Iteration 278/1000 | Loss: 0.00003833
Iteration 279/1000 | Loss: 0.00003833
Iteration 280/1000 | Loss: 0.00003833
Iteration 281/1000 | Loss: 0.00003833
Iteration 282/1000 | Loss: 0.00003833
Iteration 283/1000 | Loss: 0.00003833
Iteration 284/1000 | Loss: 0.00003833
Iteration 285/1000 | Loss: 0.00003833
Iteration 286/1000 | Loss: 0.00003832
Iteration 287/1000 | Loss: 0.00003832
Iteration 288/1000 | Loss: 0.00003832
Iteration 289/1000 | Loss: 0.00003832
Iteration 290/1000 | Loss: 0.00003832
Iteration 291/1000 | Loss: 0.00003832
Iteration 292/1000 | Loss: 0.00003832
Iteration 293/1000 | Loss: 0.00003832
Iteration 294/1000 | Loss: 0.00003832
Iteration 295/1000 | Loss: 0.00003832
Iteration 296/1000 | Loss: 0.00003831
Iteration 297/1000 | Loss: 0.00003831
Iteration 298/1000 | Loss: 0.00003831
Iteration 299/1000 | Loss: 0.00003831
Iteration 300/1000 | Loss: 0.00003831
Iteration 301/1000 | Loss: 0.00003831
Iteration 302/1000 | Loss: 0.00003831
Iteration 303/1000 | Loss: 0.00003831
Iteration 304/1000 | Loss: 0.00003831
Iteration 305/1000 | Loss: 0.00003831
Iteration 306/1000 | Loss: 0.00003831
Iteration 307/1000 | Loss: 0.00003831
Iteration 308/1000 | Loss: 0.00003831
Iteration 309/1000 | Loss: 0.00003830
Iteration 310/1000 | Loss: 0.00003830
Iteration 311/1000 | Loss: 0.00003830
Iteration 312/1000 | Loss: 0.00003830
Iteration 313/1000 | Loss: 0.00003830
Iteration 314/1000 | Loss: 0.00003830
Iteration 315/1000 | Loss: 0.00003830
Iteration 316/1000 | Loss: 0.00003830
Iteration 317/1000 | Loss: 0.00003830
Iteration 318/1000 | Loss: 0.00003830
Iteration 319/1000 | Loss: 0.00003830
Iteration 320/1000 | Loss: 0.00003830
Iteration 321/1000 | Loss: 0.00003830
Iteration 322/1000 | Loss: 0.00003829
Iteration 323/1000 | Loss: 0.00003829
Iteration 324/1000 | Loss: 0.00003829
Iteration 325/1000 | Loss: 0.00003829
Iteration 326/1000 | Loss: 0.00003829
Iteration 327/1000 | Loss: 0.00003829
Iteration 328/1000 | Loss: 0.00003829
Iteration 329/1000 | Loss: 0.00003829
Iteration 330/1000 | Loss: 0.00003829
Iteration 331/1000 | Loss: 0.00003829
Iteration 332/1000 | Loss: 0.00003829
Iteration 333/1000 | Loss: 0.00003829
Iteration 334/1000 | Loss: 0.00003829
Iteration 335/1000 | Loss: 0.00003829
Iteration 336/1000 | Loss: 0.00003829
Iteration 337/1000 | Loss: 0.00003829
Iteration 338/1000 | Loss: 0.00003829
Iteration 339/1000 | Loss: 0.00003829
Iteration 340/1000 | Loss: 0.00003828
Iteration 341/1000 | Loss: 0.00003828
Iteration 342/1000 | Loss: 0.00003828
Iteration 343/1000 | Loss: 0.00003828
Iteration 344/1000 | Loss: 0.00003828
Iteration 345/1000 | Loss: 0.00003828
Iteration 346/1000 | Loss: 0.00003828
Iteration 347/1000 | Loss: 0.00003828
Iteration 348/1000 | Loss: 0.00003828
Iteration 349/1000 | Loss: 0.00003828
Iteration 350/1000 | Loss: 0.00003828
Iteration 351/1000 | Loss: 0.00003828
Iteration 352/1000 | Loss: 0.00003828
Iteration 353/1000 | Loss: 0.00003828
Iteration 354/1000 | Loss: 0.00003828
Iteration 355/1000 | Loss: 0.00003828
Iteration 356/1000 | Loss: 0.00003828
Iteration 357/1000 | Loss: 0.00003828
Iteration 358/1000 | Loss: 0.00003828
Iteration 359/1000 | Loss: 0.00003828
Iteration 360/1000 | Loss: 0.00003828
Iteration 361/1000 | Loss: 0.00003828
Iteration 362/1000 | Loss: 0.00003828
Iteration 363/1000 | Loss: 0.00003828
Iteration 364/1000 | Loss: 0.00003828
Iteration 365/1000 | Loss: 0.00003828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 365. Stopping optimization.
Last 5 losses: [3.8278922147583216e-05, 3.8278922147583216e-05, 3.8278922147583216e-05, 3.8278922147583216e-05, 3.8278922147583216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8278922147583216e-05

Optimization complete. Final v2v error: 3.9911880493164062 mm

Highest mean error: 12.157824516296387 mm for frame 51

Lowest mean error: 3.0569703578948975 mm for frame 2

Saving results

Total time: 4017.385421514511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1038
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859366
Iteration 2/25 | Loss: 0.00111601
Iteration 3/25 | Loss: 0.00091133
Iteration 4/25 | Loss: 0.00087973
Iteration 5/25 | Loss: 0.00086600
Iteration 6/25 | Loss: 0.00086244
Iteration 7/25 | Loss: 0.00086127
Iteration 8/25 | Loss: 0.00086121
Iteration 9/25 | Loss: 0.00086121
Iteration 10/25 | Loss: 0.00086121
Iteration 11/25 | Loss: 0.00086121
Iteration 12/25 | Loss: 0.00086121
Iteration 13/25 | Loss: 0.00086121
Iteration 14/25 | Loss: 0.00086121
Iteration 15/25 | Loss: 0.00086121
Iteration 16/25 | Loss: 0.00086121
Iteration 17/25 | Loss: 0.00086121
Iteration 18/25 | Loss: 0.00086121
Iteration 19/25 | Loss: 0.00086121
Iteration 20/25 | Loss: 0.00086121
Iteration 21/25 | Loss: 0.00086121
Iteration 22/25 | Loss: 0.00086121
Iteration 23/25 | Loss: 0.00086121
Iteration 24/25 | Loss: 0.00086121
Iteration 25/25 | Loss: 0.00086121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28239262
Iteration 2/25 | Loss: 0.00050340
Iteration 3/25 | Loss: 0.00050339
Iteration 4/25 | Loss: 0.00050339
Iteration 5/25 | Loss: 0.00050339
Iteration 6/25 | Loss: 0.00050339
Iteration 7/25 | Loss: 0.00050339
Iteration 8/25 | Loss: 0.00050339
Iteration 9/25 | Loss: 0.00050339
Iteration 10/25 | Loss: 0.00050339
Iteration 11/25 | Loss: 0.00050339
Iteration 12/25 | Loss: 0.00050339
Iteration 13/25 | Loss: 0.00050339
Iteration 14/25 | Loss: 0.00050339
Iteration 15/25 | Loss: 0.00050339
Iteration 16/25 | Loss: 0.00050339
Iteration 17/25 | Loss: 0.00050339
Iteration 18/25 | Loss: 0.00050339
Iteration 19/25 | Loss: 0.00050339
Iteration 20/25 | Loss: 0.00050339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005033861380070448, 0.0005033861380070448, 0.0005033861380070448, 0.0005033861380070448, 0.0005033861380070448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005033861380070448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050339
Iteration 2/1000 | Loss: 0.00003716
Iteration 3/1000 | Loss: 0.00002740
Iteration 4/1000 | Loss: 0.00002539
Iteration 5/1000 | Loss: 0.00002411
Iteration 6/1000 | Loss: 0.00002342
Iteration 7/1000 | Loss: 0.00002292
Iteration 8/1000 | Loss: 0.00002288
Iteration 9/1000 | Loss: 0.00002254
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002229
Iteration 12/1000 | Loss: 0.00002215
Iteration 13/1000 | Loss: 0.00002200
Iteration 14/1000 | Loss: 0.00002197
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002190
Iteration 18/1000 | Loss: 0.00002190
Iteration 19/1000 | Loss: 0.00002188
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002187
Iteration 22/1000 | Loss: 0.00002182
Iteration 23/1000 | Loss: 0.00002180
Iteration 24/1000 | Loss: 0.00002180
Iteration 25/1000 | Loss: 0.00002179
Iteration 26/1000 | Loss: 0.00002179
Iteration 27/1000 | Loss: 0.00002178
Iteration 28/1000 | Loss: 0.00002178
Iteration 29/1000 | Loss: 0.00002175
Iteration 30/1000 | Loss: 0.00002175
Iteration 31/1000 | Loss: 0.00002175
Iteration 32/1000 | Loss: 0.00002175
Iteration 33/1000 | Loss: 0.00002175
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002174
Iteration 36/1000 | Loss: 0.00002174
Iteration 37/1000 | Loss: 0.00002174
Iteration 38/1000 | Loss: 0.00002174
Iteration 39/1000 | Loss: 0.00002174
Iteration 40/1000 | Loss: 0.00002174
Iteration 41/1000 | Loss: 0.00002174
Iteration 42/1000 | Loss: 0.00002174
Iteration 43/1000 | Loss: 0.00002174
Iteration 44/1000 | Loss: 0.00002173
Iteration 45/1000 | Loss: 0.00002172
Iteration 46/1000 | Loss: 0.00002172
Iteration 47/1000 | Loss: 0.00002172
Iteration 48/1000 | Loss: 0.00002172
Iteration 49/1000 | Loss: 0.00002172
Iteration 50/1000 | Loss: 0.00002172
Iteration 51/1000 | Loss: 0.00002172
Iteration 52/1000 | Loss: 0.00002171
Iteration 53/1000 | Loss: 0.00002171
Iteration 54/1000 | Loss: 0.00002171
Iteration 55/1000 | Loss: 0.00002171
Iteration 56/1000 | Loss: 0.00002171
Iteration 57/1000 | Loss: 0.00002170
Iteration 58/1000 | Loss: 0.00002170
Iteration 59/1000 | Loss: 0.00002170
Iteration 60/1000 | Loss: 0.00002170
Iteration 61/1000 | Loss: 0.00002170
Iteration 62/1000 | Loss: 0.00002170
Iteration 63/1000 | Loss: 0.00002170
Iteration 64/1000 | Loss: 0.00002170
Iteration 65/1000 | Loss: 0.00002170
Iteration 66/1000 | Loss: 0.00002170
Iteration 67/1000 | Loss: 0.00002169
Iteration 68/1000 | Loss: 0.00002169
Iteration 69/1000 | Loss: 0.00002169
Iteration 70/1000 | Loss: 0.00002169
Iteration 71/1000 | Loss: 0.00002169
Iteration 72/1000 | Loss: 0.00002169
Iteration 73/1000 | Loss: 0.00002169
Iteration 74/1000 | Loss: 0.00002168
Iteration 75/1000 | Loss: 0.00002168
Iteration 76/1000 | Loss: 0.00002168
Iteration 77/1000 | Loss: 0.00002168
Iteration 78/1000 | Loss: 0.00002168
Iteration 79/1000 | Loss: 0.00002168
Iteration 80/1000 | Loss: 0.00002168
Iteration 81/1000 | Loss: 0.00002168
Iteration 82/1000 | Loss: 0.00002168
Iteration 83/1000 | Loss: 0.00002168
Iteration 84/1000 | Loss: 0.00002167
Iteration 85/1000 | Loss: 0.00002167
Iteration 86/1000 | Loss: 0.00002167
Iteration 87/1000 | Loss: 0.00002167
Iteration 88/1000 | Loss: 0.00002167
Iteration 89/1000 | Loss: 0.00002167
Iteration 90/1000 | Loss: 0.00002167
Iteration 91/1000 | Loss: 0.00002167
Iteration 92/1000 | Loss: 0.00002167
Iteration 93/1000 | Loss: 0.00002167
Iteration 94/1000 | Loss: 0.00002166
Iteration 95/1000 | Loss: 0.00002166
Iteration 96/1000 | Loss: 0.00002166
Iteration 97/1000 | Loss: 0.00002166
Iteration 98/1000 | Loss: 0.00002166
Iteration 99/1000 | Loss: 0.00002166
Iteration 100/1000 | Loss: 0.00002166
Iteration 101/1000 | Loss: 0.00002166
Iteration 102/1000 | Loss: 0.00002166
Iteration 103/1000 | Loss: 0.00002166
Iteration 104/1000 | Loss: 0.00002166
Iteration 105/1000 | Loss: 0.00002166
Iteration 106/1000 | Loss: 0.00002166
Iteration 107/1000 | Loss: 0.00002166
Iteration 108/1000 | Loss: 0.00002165
Iteration 109/1000 | Loss: 0.00002165
Iteration 110/1000 | Loss: 0.00002165
Iteration 111/1000 | Loss: 0.00002165
Iteration 112/1000 | Loss: 0.00002165
Iteration 113/1000 | Loss: 0.00002165
Iteration 114/1000 | Loss: 0.00002165
Iteration 115/1000 | Loss: 0.00002165
Iteration 116/1000 | Loss: 0.00002165
Iteration 117/1000 | Loss: 0.00002165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.164943543903064e-05, 2.164943543903064e-05, 2.164943543903064e-05, 2.164943543903064e-05, 2.164943543903064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.164943543903064e-05

Optimization complete. Final v2v error: 3.8551619052886963 mm

Highest mean error: 4.836403846740723 mm for frame 127

Lowest mean error: 3.151317834854126 mm for frame 51

Saving results

Total time: 1197.8514590263367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1025
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952777
Iteration 2/25 | Loss: 0.00201878
Iteration 3/25 | Loss: 0.00104311
Iteration 4/25 | Loss: 0.00100751
Iteration 5/25 | Loss: 0.00099374
Iteration 6/25 | Loss: 0.00099168
Iteration 7/25 | Loss: 0.00099168
Iteration 8/25 | Loss: 0.00099168
Iteration 9/25 | Loss: 0.00099168
Iteration 10/25 | Loss: 0.00099168
Iteration 11/25 | Loss: 0.00099168
Iteration 12/25 | Loss: 0.00099168
Iteration 13/25 | Loss: 0.00099168
Iteration 14/25 | Loss: 0.00099168
Iteration 15/25 | Loss: 0.00099168
Iteration 16/25 | Loss: 0.00099168
Iteration 17/25 | Loss: 0.00099168
Iteration 18/25 | Loss: 0.00099168
Iteration 19/25 | Loss: 0.00099168
Iteration 20/25 | Loss: 0.00099168
Iteration 21/25 | Loss: 0.00099168
Iteration 22/25 | Loss: 0.00099168
Iteration 23/25 | Loss: 0.00099168
Iteration 24/25 | Loss: 0.00099168
Iteration 25/25 | Loss: 0.00099168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54883760
Iteration 2/25 | Loss: 0.00029931
Iteration 3/25 | Loss: 0.00029931
Iteration 4/25 | Loss: 0.00029930
Iteration 5/25 | Loss: 0.00029930
Iteration 6/25 | Loss: 0.00029930
Iteration 7/25 | Loss: 0.00029930
Iteration 8/25 | Loss: 0.00029930
Iteration 9/25 | Loss: 0.00029930
Iteration 10/25 | Loss: 0.00029930
Iteration 11/25 | Loss: 0.00029930
Iteration 12/25 | Loss: 0.00029930
Iteration 13/25 | Loss: 0.00029930
Iteration 14/25 | Loss: 0.00029930
Iteration 15/25 | Loss: 0.00029930
Iteration 16/25 | Loss: 0.00029930
Iteration 17/25 | Loss: 0.00029930
Iteration 18/25 | Loss: 0.00029930
Iteration 19/25 | Loss: 0.00029930
Iteration 20/25 | Loss: 0.00029930
Iteration 21/25 | Loss: 0.00029930
Iteration 22/25 | Loss: 0.00029930
Iteration 23/25 | Loss: 0.00029930
Iteration 24/25 | Loss: 0.00029930
Iteration 25/25 | Loss: 0.00029930

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029930
Iteration 2/1000 | Loss: 0.00005881
Iteration 3/1000 | Loss: 0.00004440
Iteration 4/1000 | Loss: 0.00003976
Iteration 5/1000 | Loss: 0.00003789
Iteration 6/1000 | Loss: 0.00003698
Iteration 7/1000 | Loss: 0.00003574
Iteration 8/1000 | Loss: 0.00003500
Iteration 9/1000 | Loss: 0.00003448
Iteration 10/1000 | Loss: 0.00003396
Iteration 11/1000 | Loss: 0.00003363
Iteration 12/1000 | Loss: 0.00003335
Iteration 13/1000 | Loss: 0.00003309
Iteration 14/1000 | Loss: 0.00003296
Iteration 15/1000 | Loss: 0.00003276
Iteration 16/1000 | Loss: 0.00003255
Iteration 17/1000 | Loss: 0.00003236
Iteration 18/1000 | Loss: 0.00003231
Iteration 19/1000 | Loss: 0.00003217
Iteration 20/1000 | Loss: 0.00003204
Iteration 21/1000 | Loss: 0.00003197
Iteration 22/1000 | Loss: 0.00003197
Iteration 23/1000 | Loss: 0.00003194
Iteration 24/1000 | Loss: 0.00003191
Iteration 25/1000 | Loss: 0.00003189
Iteration 26/1000 | Loss: 0.00003188
Iteration 27/1000 | Loss: 0.00003188
Iteration 28/1000 | Loss: 0.00003187
Iteration 29/1000 | Loss: 0.00003187
Iteration 30/1000 | Loss: 0.00003187
Iteration 31/1000 | Loss: 0.00003187
Iteration 32/1000 | Loss: 0.00003187
Iteration 33/1000 | Loss: 0.00003187
Iteration 34/1000 | Loss: 0.00003186
Iteration 35/1000 | Loss: 0.00003186
Iteration 36/1000 | Loss: 0.00003186
Iteration 37/1000 | Loss: 0.00003186
Iteration 38/1000 | Loss: 0.00003186
Iteration 39/1000 | Loss: 0.00003186
Iteration 40/1000 | Loss: 0.00003186
Iteration 41/1000 | Loss: 0.00003186
Iteration 42/1000 | Loss: 0.00003186
Iteration 43/1000 | Loss: 0.00003185
Iteration 44/1000 | Loss: 0.00003185
Iteration 45/1000 | Loss: 0.00003185
Iteration 46/1000 | Loss: 0.00003185
Iteration 47/1000 | Loss: 0.00003185
Iteration 48/1000 | Loss: 0.00003185
Iteration 49/1000 | Loss: 0.00003184
Iteration 50/1000 | Loss: 0.00003184
Iteration 51/1000 | Loss: 0.00003184
Iteration 52/1000 | Loss: 0.00003184
Iteration 53/1000 | Loss: 0.00003183
Iteration 54/1000 | Loss: 0.00003183
Iteration 55/1000 | Loss: 0.00003182
Iteration 56/1000 | Loss: 0.00003181
Iteration 57/1000 | Loss: 0.00003181
Iteration 58/1000 | Loss: 0.00003181
Iteration 59/1000 | Loss: 0.00003180
Iteration 60/1000 | Loss: 0.00003180
Iteration 61/1000 | Loss: 0.00003180
Iteration 62/1000 | Loss: 0.00003179
Iteration 63/1000 | Loss: 0.00003179
Iteration 64/1000 | Loss: 0.00003179
Iteration 65/1000 | Loss: 0.00003179
Iteration 66/1000 | Loss: 0.00003178
Iteration 67/1000 | Loss: 0.00003177
Iteration 68/1000 | Loss: 0.00003177
Iteration 69/1000 | Loss: 0.00003177
Iteration 70/1000 | Loss: 0.00003177
Iteration 71/1000 | Loss: 0.00003176
Iteration 72/1000 | Loss: 0.00003176
Iteration 73/1000 | Loss: 0.00003176
Iteration 74/1000 | Loss: 0.00003176
Iteration 75/1000 | Loss: 0.00003176
Iteration 76/1000 | Loss: 0.00003176
Iteration 77/1000 | Loss: 0.00003176
Iteration 78/1000 | Loss: 0.00003176
Iteration 79/1000 | Loss: 0.00003176
Iteration 80/1000 | Loss: 0.00003176
Iteration 81/1000 | Loss: 0.00003176
Iteration 82/1000 | Loss: 0.00003176
Iteration 83/1000 | Loss: 0.00003175
Iteration 84/1000 | Loss: 0.00003175
Iteration 85/1000 | Loss: 0.00003175
Iteration 86/1000 | Loss: 0.00003174
Iteration 87/1000 | Loss: 0.00003174
Iteration 88/1000 | Loss: 0.00003174
Iteration 89/1000 | Loss: 0.00003174
Iteration 90/1000 | Loss: 0.00003174
Iteration 91/1000 | Loss: 0.00003174
Iteration 92/1000 | Loss: 0.00003174
Iteration 93/1000 | Loss: 0.00003174
Iteration 94/1000 | Loss: 0.00003174
Iteration 95/1000 | Loss: 0.00003174
Iteration 96/1000 | Loss: 0.00003174
Iteration 97/1000 | Loss: 0.00003174
Iteration 98/1000 | Loss: 0.00003174
Iteration 99/1000 | Loss: 0.00003174
Iteration 100/1000 | Loss: 0.00003174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [3.1738327379571274e-05, 3.1738327379571274e-05, 3.1738327379571274e-05, 3.1738327379571274e-05, 3.1738327379571274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1738327379571274e-05

Optimization complete. Final v2v error: 4.7373270988464355 mm

Highest mean error: 5.150081634521484 mm for frame 69

Lowest mean error: 4.320576190948486 mm for frame 149

Saving results

Total time: 1358.2930297851562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1008
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518698
Iteration 2/25 | Loss: 0.00086973
Iteration 3/25 | Loss: 0.00075683
Iteration 4/25 | Loss: 0.00074106
Iteration 5/25 | Loss: 0.00073567
Iteration 6/25 | Loss: 0.00073410
Iteration 7/25 | Loss: 0.00073393
Iteration 8/25 | Loss: 0.00073393
Iteration 9/25 | Loss: 0.00073393
Iteration 10/25 | Loss: 0.00073393
Iteration 11/25 | Loss: 0.00073393
Iteration 12/25 | Loss: 0.00073393
Iteration 13/25 | Loss: 0.00073393
Iteration 14/25 | Loss: 0.00073393
Iteration 15/25 | Loss: 0.00073393
Iteration 16/25 | Loss: 0.00073393
Iteration 17/25 | Loss: 0.00073393
Iteration 18/25 | Loss: 0.00073393
Iteration 19/25 | Loss: 0.00073393
Iteration 20/25 | Loss: 0.00073393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000733932014554739, 0.000733932014554739, 0.000733932014554739, 0.000733932014554739, 0.000733932014554739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000733932014554739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.52543497
Iteration 2/25 | Loss: 0.00041811
Iteration 3/25 | Loss: 0.00041811
Iteration 4/25 | Loss: 0.00041811
Iteration 5/25 | Loss: 0.00041810
Iteration 6/25 | Loss: 0.00041810
Iteration 7/25 | Loss: 0.00041810
Iteration 8/25 | Loss: 0.00041810
Iteration 9/25 | Loss: 0.00041810
Iteration 10/25 | Loss: 0.00041810
Iteration 11/25 | Loss: 0.00041810
Iteration 12/25 | Loss: 0.00041810
Iteration 13/25 | Loss: 0.00041810
Iteration 14/25 | Loss: 0.00041810
Iteration 15/25 | Loss: 0.00041810
Iteration 16/25 | Loss: 0.00041810
Iteration 17/25 | Loss: 0.00041810
Iteration 18/25 | Loss: 0.00041810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004181032127235085, 0.0004181032127235085, 0.0004181032127235085, 0.0004181032127235085, 0.0004181032127235085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004181032127235085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041810
Iteration 2/1000 | Loss: 0.00002639
Iteration 3/1000 | Loss: 0.00001672
Iteration 4/1000 | Loss: 0.00001538
Iteration 5/1000 | Loss: 0.00001453
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001392
Iteration 8/1000 | Loss: 0.00001378
Iteration 9/1000 | Loss: 0.00001375
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001342
Iteration 14/1000 | Loss: 0.00001335
Iteration 15/1000 | Loss: 0.00001335
Iteration 16/1000 | Loss: 0.00001329
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001320
Iteration 22/1000 | Loss: 0.00001320
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001319
Iteration 25/1000 | Loss: 0.00001319
Iteration 26/1000 | Loss: 0.00001319
Iteration 27/1000 | Loss: 0.00001319
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001316
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001316
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001310
Iteration 42/1000 | Loss: 0.00001310
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001306
Iteration 45/1000 | Loss: 0.00001306
Iteration 46/1000 | Loss: 0.00001305
Iteration 47/1000 | Loss: 0.00001305
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001303
Iteration 50/1000 | Loss: 0.00001303
Iteration 51/1000 | Loss: 0.00001302
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001300
Iteration 57/1000 | Loss: 0.00001300
Iteration 58/1000 | Loss: 0.00001300
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001300
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001299
Iteration 64/1000 | Loss: 0.00001299
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001295
Iteration 70/1000 | Loss: 0.00001295
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00001295
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001294
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001294
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001294
Iteration 85/1000 | Loss: 0.00001294
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001293
Iteration 88/1000 | Loss: 0.00001293
Iteration 89/1000 | Loss: 0.00001293
Iteration 90/1000 | Loss: 0.00001293
Iteration 91/1000 | Loss: 0.00001293
Iteration 92/1000 | Loss: 0.00001293
Iteration 93/1000 | Loss: 0.00001293
Iteration 94/1000 | Loss: 0.00001292
Iteration 95/1000 | Loss: 0.00001292
Iteration 96/1000 | Loss: 0.00001292
Iteration 97/1000 | Loss: 0.00001292
Iteration 98/1000 | Loss: 0.00001292
Iteration 99/1000 | Loss: 0.00001292
Iteration 100/1000 | Loss: 0.00001292
Iteration 101/1000 | Loss: 0.00001292
Iteration 102/1000 | Loss: 0.00001292
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001291
Iteration 105/1000 | Loss: 0.00001291
Iteration 106/1000 | Loss: 0.00001291
Iteration 107/1000 | Loss: 0.00001291
Iteration 108/1000 | Loss: 0.00001291
Iteration 109/1000 | Loss: 0.00001291
Iteration 110/1000 | Loss: 0.00001291
Iteration 111/1000 | Loss: 0.00001291
Iteration 112/1000 | Loss: 0.00001291
Iteration 113/1000 | Loss: 0.00001291
Iteration 114/1000 | Loss: 0.00001291
Iteration 115/1000 | Loss: 0.00001291
Iteration 116/1000 | Loss: 0.00001291
Iteration 117/1000 | Loss: 0.00001291
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001290
Iteration 128/1000 | Loss: 0.00001290
Iteration 129/1000 | Loss: 0.00001290
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001289
Iteration 134/1000 | Loss: 0.00001289
Iteration 135/1000 | Loss: 0.00001289
Iteration 136/1000 | Loss: 0.00001289
Iteration 137/1000 | Loss: 0.00001289
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001289
Iteration 140/1000 | Loss: 0.00001289
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001288
Iteration 145/1000 | Loss: 0.00001288
Iteration 146/1000 | Loss: 0.00001288
Iteration 147/1000 | Loss: 0.00001288
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001286
Iteration 158/1000 | Loss: 0.00001286
Iteration 159/1000 | Loss: 0.00001286
Iteration 160/1000 | Loss: 0.00001286
Iteration 161/1000 | Loss: 0.00001286
Iteration 162/1000 | Loss: 0.00001286
Iteration 163/1000 | Loss: 0.00001286
Iteration 164/1000 | Loss: 0.00001286
Iteration 165/1000 | Loss: 0.00001286
Iteration 166/1000 | Loss: 0.00001286
Iteration 167/1000 | Loss: 0.00001286
Iteration 168/1000 | Loss: 0.00001286
Iteration 169/1000 | Loss: 0.00001286
Iteration 170/1000 | Loss: 0.00001286
Iteration 171/1000 | Loss: 0.00001286
Iteration 172/1000 | Loss: 0.00001286
Iteration 173/1000 | Loss: 0.00001286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.285780035686912e-05, 1.285780035686912e-05, 1.285780035686912e-05, 1.285780035686912e-05, 1.285780035686912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.285780035686912e-05

Optimization complete. Final v2v error: 3.1067678928375244 mm

Highest mean error: 3.3303186893463135 mm for frame 181

Lowest mean error: 2.886662721633911 mm for frame 260

Saving results

Total time: 1436.7081112861633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1062
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660444
Iteration 2/25 | Loss: 0.00124550
Iteration 3/25 | Loss: 0.00091836
Iteration 4/25 | Loss: 0.00084988
Iteration 5/25 | Loss: 0.00082871
Iteration 6/25 | Loss: 0.00082898
Iteration 7/25 | Loss: 0.00082917
Iteration 8/25 | Loss: 0.00082611
Iteration 9/25 | Loss: 0.00082308
Iteration 10/25 | Loss: 0.00081292
Iteration 11/25 | Loss: 0.00080655
Iteration 12/25 | Loss: 0.00080405
Iteration 13/25 | Loss: 0.00080227
Iteration 14/25 | Loss: 0.00080192
Iteration 15/25 | Loss: 0.00080185
Iteration 16/25 | Loss: 0.00080185
Iteration 17/25 | Loss: 0.00080185
Iteration 18/25 | Loss: 0.00080185
Iteration 19/25 | Loss: 0.00080185
Iteration 20/25 | Loss: 0.00080185
Iteration 21/25 | Loss: 0.00080184
Iteration 22/25 | Loss: 0.00080184
Iteration 23/25 | Loss: 0.00080184
Iteration 24/25 | Loss: 0.00080184
Iteration 25/25 | Loss: 0.00080184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16290140
Iteration 2/25 | Loss: 0.00058458
Iteration 3/25 | Loss: 0.00058428
Iteration 4/25 | Loss: 0.00058428
Iteration 5/25 | Loss: 0.00058427
Iteration 6/25 | Loss: 0.00058427
Iteration 7/25 | Loss: 0.00058427
Iteration 8/25 | Loss: 0.00058427
Iteration 9/25 | Loss: 0.00058427
Iteration 10/25 | Loss: 0.00058427
Iteration 11/25 | Loss: 0.00058427
Iteration 12/25 | Loss: 0.00058427
Iteration 13/25 | Loss: 0.00058427
Iteration 14/25 | Loss: 0.00058427
Iteration 15/25 | Loss: 0.00058427
Iteration 16/25 | Loss: 0.00058427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005842717946507037, 0.0005842717946507037, 0.0005842717946507037, 0.0005842717946507037, 0.0005842717946507037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005842717946507037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058427
Iteration 2/1000 | Loss: 0.00004925
Iteration 3/1000 | Loss: 0.00003448
Iteration 4/1000 | Loss: 0.00002992
Iteration 5/1000 | Loss: 0.00002797
Iteration 6/1000 | Loss: 0.00002683
Iteration 7/1000 | Loss: 0.00002603
Iteration 8/1000 | Loss: 0.00002546
Iteration 9/1000 | Loss: 0.00002498
Iteration 10/1000 | Loss: 0.00002469
Iteration 11/1000 | Loss: 0.00002442
Iteration 12/1000 | Loss: 0.00002440
Iteration 13/1000 | Loss: 0.00002420
Iteration 14/1000 | Loss: 0.00002403
Iteration 15/1000 | Loss: 0.00002391
Iteration 16/1000 | Loss: 0.00002375
Iteration 17/1000 | Loss: 0.00002362
Iteration 18/1000 | Loss: 0.00002361
Iteration 19/1000 | Loss: 0.00002357
Iteration 20/1000 | Loss: 0.00002357
Iteration 21/1000 | Loss: 0.00002357
Iteration 22/1000 | Loss: 0.00002355
Iteration 23/1000 | Loss: 0.00002355
Iteration 24/1000 | Loss: 0.00002354
Iteration 25/1000 | Loss: 0.00002354
Iteration 26/1000 | Loss: 0.00002353
Iteration 27/1000 | Loss: 0.00002350
Iteration 28/1000 | Loss: 0.00002350
Iteration 29/1000 | Loss: 0.00002348
Iteration 30/1000 | Loss: 0.00002348
Iteration 31/1000 | Loss: 0.00002347
Iteration 32/1000 | Loss: 0.00002347
Iteration 33/1000 | Loss: 0.00002344
Iteration 34/1000 | Loss: 0.00002343
Iteration 35/1000 | Loss: 0.00002342
Iteration 36/1000 | Loss: 0.00002342
Iteration 37/1000 | Loss: 0.00002341
Iteration 38/1000 | Loss: 0.00002341
Iteration 39/1000 | Loss: 0.00002334
Iteration 40/1000 | Loss: 0.00002330
Iteration 41/1000 | Loss: 0.00002330
Iteration 42/1000 | Loss: 0.00002329
Iteration 43/1000 | Loss: 0.00002329
Iteration 44/1000 | Loss: 0.00002328
Iteration 45/1000 | Loss: 0.00002327
Iteration 46/1000 | Loss: 0.00002321
Iteration 47/1000 | Loss: 0.00002321
Iteration 48/1000 | Loss: 0.00002320
Iteration 49/1000 | Loss: 0.00002319
Iteration 50/1000 | Loss: 0.00002319
Iteration 51/1000 | Loss: 0.00002318
Iteration 52/1000 | Loss: 0.00002317
Iteration 53/1000 | Loss: 0.00002317
Iteration 54/1000 | Loss: 0.00002317
Iteration 55/1000 | Loss: 0.00002317
Iteration 56/1000 | Loss: 0.00002316
Iteration 57/1000 | Loss: 0.00002316
Iteration 58/1000 | Loss: 0.00002316
Iteration 59/1000 | Loss: 0.00002316
Iteration 60/1000 | Loss: 0.00002316
Iteration 61/1000 | Loss: 0.00002316
Iteration 62/1000 | Loss: 0.00002315
Iteration 63/1000 | Loss: 0.00002315
Iteration 64/1000 | Loss: 0.00002315
Iteration 65/1000 | Loss: 0.00002315
Iteration 66/1000 | Loss: 0.00002315
Iteration 67/1000 | Loss: 0.00002315
Iteration 68/1000 | Loss: 0.00002315
Iteration 69/1000 | Loss: 0.00002315
Iteration 70/1000 | Loss: 0.00002315
Iteration 71/1000 | Loss: 0.00002315
Iteration 72/1000 | Loss: 0.00002315
Iteration 73/1000 | Loss: 0.00002315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.3151518689701334e-05, 2.3151518689701334e-05, 2.3151518689701334e-05, 2.3151518689701334e-05, 2.3151518689701334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3151518689701334e-05

Optimization complete. Final v2v error: 3.894101619720459 mm

Highest mean error: 6.210455417633057 mm for frame 118

Lowest mean error: 2.998272657394409 mm for frame 162

Saving results

Total time: 1967.1123278141022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1032
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591252
Iteration 2/25 | Loss: 0.00126826
Iteration 3/25 | Loss: 0.00095203
Iteration 4/25 | Loss: 0.00089881
Iteration 5/25 | Loss: 0.00088186
Iteration 6/25 | Loss: 0.00087914
Iteration 7/25 | Loss: 0.00087890
Iteration 8/25 | Loss: 0.00087890
Iteration 9/25 | Loss: 0.00087890
Iteration 10/25 | Loss: 0.00087890
Iteration 11/25 | Loss: 0.00087890
Iteration 12/25 | Loss: 0.00087890
Iteration 13/25 | Loss: 0.00087890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008788955747149885, 0.0008788955747149885, 0.0008788955747149885, 0.0008788955747149885, 0.0008788955747149885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008788955747149885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06061769
Iteration 2/25 | Loss: 0.00050092
Iteration 3/25 | Loss: 0.00050088
Iteration 4/25 | Loss: 0.00050088
Iteration 5/25 | Loss: 0.00050088
Iteration 6/25 | Loss: 0.00050088
Iteration 7/25 | Loss: 0.00050088
Iteration 8/25 | Loss: 0.00050088
Iteration 9/25 | Loss: 0.00050088
Iteration 10/25 | Loss: 0.00050088
Iteration 11/25 | Loss: 0.00050088
Iteration 12/25 | Loss: 0.00050088
Iteration 13/25 | Loss: 0.00050088
Iteration 14/25 | Loss: 0.00050088
Iteration 15/25 | Loss: 0.00050088
Iteration 16/25 | Loss: 0.00050088
Iteration 17/25 | Loss: 0.00050088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005008761072531343, 0.0005008761072531343, 0.0005008761072531343, 0.0005008761072531343, 0.0005008761072531343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005008761072531343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050088
Iteration 2/1000 | Loss: 0.00003498
Iteration 3/1000 | Loss: 0.00002620
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002194
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00002072
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00002015
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001983
Iteration 12/1000 | Loss: 0.00001981
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001978
Iteration 15/1000 | Loss: 0.00001974
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001972
Iteration 18/1000 | Loss: 0.00001972
Iteration 19/1000 | Loss: 0.00001972
Iteration 20/1000 | Loss: 0.00001971
Iteration 21/1000 | Loss: 0.00001970
Iteration 22/1000 | Loss: 0.00001970
Iteration 23/1000 | Loss: 0.00001969
Iteration 24/1000 | Loss: 0.00001967
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001964
Iteration 27/1000 | Loss: 0.00001961
Iteration 28/1000 | Loss: 0.00001961
Iteration 29/1000 | Loss: 0.00001960
Iteration 30/1000 | Loss: 0.00001960
Iteration 31/1000 | Loss: 0.00001960
Iteration 32/1000 | Loss: 0.00001959
Iteration 33/1000 | Loss: 0.00001958
Iteration 34/1000 | Loss: 0.00001958
Iteration 35/1000 | Loss: 0.00001958
Iteration 36/1000 | Loss: 0.00001957
Iteration 37/1000 | Loss: 0.00001957
Iteration 38/1000 | Loss: 0.00001957
Iteration 39/1000 | Loss: 0.00001957
Iteration 40/1000 | Loss: 0.00001956
Iteration 41/1000 | Loss: 0.00001956
Iteration 42/1000 | Loss: 0.00001955
Iteration 43/1000 | Loss: 0.00001955
Iteration 44/1000 | Loss: 0.00001955
Iteration 45/1000 | Loss: 0.00001955
Iteration 46/1000 | Loss: 0.00001955
Iteration 47/1000 | Loss: 0.00001955
Iteration 48/1000 | Loss: 0.00001955
Iteration 49/1000 | Loss: 0.00001955
Iteration 50/1000 | Loss: 0.00001955
Iteration 51/1000 | Loss: 0.00001955
Iteration 52/1000 | Loss: 0.00001955
Iteration 53/1000 | Loss: 0.00001955
Iteration 54/1000 | Loss: 0.00001955
Iteration 55/1000 | Loss: 0.00001955
Iteration 56/1000 | Loss: 0.00001955
Iteration 57/1000 | Loss: 0.00001955
Iteration 58/1000 | Loss: 0.00001955
Iteration 59/1000 | Loss: 0.00001955
Iteration 60/1000 | Loss: 0.00001955
Iteration 61/1000 | Loss: 0.00001955
Iteration 62/1000 | Loss: 0.00001955
Iteration 63/1000 | Loss: 0.00001955
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001955
Iteration 66/1000 | Loss: 0.00001955
Iteration 67/1000 | Loss: 0.00001955
Iteration 68/1000 | Loss: 0.00001955
Iteration 69/1000 | Loss: 0.00001955
Iteration 70/1000 | Loss: 0.00001955
Iteration 71/1000 | Loss: 0.00001955
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001955
Iteration 76/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.9545183022273704e-05, 1.9545183022273704e-05, 1.9545183022273704e-05, 1.9545183022273704e-05, 1.9545183022273704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9545183022273704e-05

Optimization complete. Final v2v error: 3.6959736347198486 mm

Highest mean error: 4.034914970397949 mm for frame 114

Lowest mean error: 3.4020721912384033 mm for frame 41

Saving results

Total time: 590.6806147098541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1040
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893967
Iteration 2/25 | Loss: 0.00099855
Iteration 3/25 | Loss: 0.00081019
Iteration 4/25 | Loss: 0.00077170
Iteration 5/25 | Loss: 0.00076262
Iteration 6/25 | Loss: 0.00076078
Iteration 7/25 | Loss: 0.00076022
Iteration 8/25 | Loss: 0.00076022
Iteration 9/25 | Loss: 0.00076022
Iteration 10/25 | Loss: 0.00076022
Iteration 11/25 | Loss: 0.00076022
Iteration 12/25 | Loss: 0.00076022
Iteration 13/25 | Loss: 0.00076022
Iteration 14/25 | Loss: 0.00076022
Iteration 15/25 | Loss: 0.00076022
Iteration 16/25 | Loss: 0.00076022
Iteration 17/25 | Loss: 0.00076022
Iteration 18/25 | Loss: 0.00076022
Iteration 19/25 | Loss: 0.00076022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007602244731970131, 0.0007602244731970131, 0.0007602244731970131, 0.0007602244731970131, 0.0007602244731970131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007602244731970131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66533065
Iteration 2/25 | Loss: 0.00043447
Iteration 3/25 | Loss: 0.00043445
Iteration 4/25 | Loss: 0.00043445
Iteration 5/25 | Loss: 0.00043444
Iteration 6/25 | Loss: 0.00043444
Iteration 7/25 | Loss: 0.00043444
Iteration 8/25 | Loss: 0.00043444
Iteration 9/25 | Loss: 0.00043444
Iteration 10/25 | Loss: 0.00043444
Iteration 11/25 | Loss: 0.00043444
Iteration 12/25 | Loss: 0.00043444
Iteration 13/25 | Loss: 0.00043444
Iteration 14/25 | Loss: 0.00043444
Iteration 15/25 | Loss: 0.00043444
Iteration 16/25 | Loss: 0.00043444
Iteration 17/25 | Loss: 0.00043444
Iteration 18/25 | Loss: 0.00043444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00043444280163384974, 0.00043444280163384974, 0.00043444280163384974, 0.00043444280163384974, 0.00043444280163384974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043444280163384974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043444
Iteration 2/1000 | Loss: 0.00003214
Iteration 3/1000 | Loss: 0.00001984
Iteration 4/1000 | Loss: 0.00001828
Iteration 5/1000 | Loss: 0.00001754
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001635
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001591
Iteration 12/1000 | Loss: 0.00001590
Iteration 13/1000 | Loss: 0.00001589
Iteration 14/1000 | Loss: 0.00001584
Iteration 15/1000 | Loss: 0.00001580
Iteration 16/1000 | Loss: 0.00001579
Iteration 17/1000 | Loss: 0.00001579
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001575
Iteration 20/1000 | Loss: 0.00001575
Iteration 21/1000 | Loss: 0.00001572
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001564
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001558
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001557
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001556
Iteration 33/1000 | Loss: 0.00001552
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001551
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00001550
Iteration 38/1000 | Loss: 0.00001550
Iteration 39/1000 | Loss: 0.00001547
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001544
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001539
Iteration 45/1000 | Loss: 0.00001538
Iteration 46/1000 | Loss: 0.00001538
Iteration 47/1000 | Loss: 0.00001537
Iteration 48/1000 | Loss: 0.00001537
Iteration 49/1000 | Loss: 0.00001537
Iteration 50/1000 | Loss: 0.00001537
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001537
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001536
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001536
Iteration 61/1000 | Loss: 0.00001536
Iteration 62/1000 | Loss: 0.00001535
Iteration 63/1000 | Loss: 0.00001535
Iteration 64/1000 | Loss: 0.00001535
Iteration 65/1000 | Loss: 0.00001535
Iteration 66/1000 | Loss: 0.00001534
Iteration 67/1000 | Loss: 0.00001534
Iteration 68/1000 | Loss: 0.00001534
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001533
Iteration 71/1000 | Loss: 0.00001533
Iteration 72/1000 | Loss: 0.00001533
Iteration 73/1000 | Loss: 0.00001533
Iteration 74/1000 | Loss: 0.00001533
Iteration 75/1000 | Loss: 0.00001533
Iteration 76/1000 | Loss: 0.00001533
Iteration 77/1000 | Loss: 0.00001533
Iteration 78/1000 | Loss: 0.00001533
Iteration 79/1000 | Loss: 0.00001533
Iteration 80/1000 | Loss: 0.00001533
Iteration 81/1000 | Loss: 0.00001533
Iteration 82/1000 | Loss: 0.00001533
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001532
Iteration 85/1000 | Loss: 0.00001532
Iteration 86/1000 | Loss: 0.00001532
Iteration 87/1000 | Loss: 0.00001532
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001531
Iteration 97/1000 | Loss: 0.00001531
Iteration 98/1000 | Loss: 0.00001531
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001530
Iteration 104/1000 | Loss: 0.00001530
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001529
Iteration 108/1000 | Loss: 0.00001529
Iteration 109/1000 | Loss: 0.00001529
Iteration 110/1000 | Loss: 0.00001529
Iteration 111/1000 | Loss: 0.00001529
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001528
Iteration 114/1000 | Loss: 0.00001528
Iteration 115/1000 | Loss: 0.00001528
Iteration 116/1000 | Loss: 0.00001528
Iteration 117/1000 | Loss: 0.00001528
Iteration 118/1000 | Loss: 0.00001528
Iteration 119/1000 | Loss: 0.00001528
Iteration 120/1000 | Loss: 0.00001528
Iteration 121/1000 | Loss: 0.00001528
Iteration 122/1000 | Loss: 0.00001528
Iteration 123/1000 | Loss: 0.00001528
Iteration 124/1000 | Loss: 0.00001528
Iteration 125/1000 | Loss: 0.00001528
Iteration 126/1000 | Loss: 0.00001527
Iteration 127/1000 | Loss: 0.00001527
Iteration 128/1000 | Loss: 0.00001527
Iteration 129/1000 | Loss: 0.00001526
Iteration 130/1000 | Loss: 0.00001526
Iteration 131/1000 | Loss: 0.00001526
Iteration 132/1000 | Loss: 0.00001526
Iteration 133/1000 | Loss: 0.00001526
Iteration 134/1000 | Loss: 0.00001526
Iteration 135/1000 | Loss: 0.00001526
Iteration 136/1000 | Loss: 0.00001526
Iteration 137/1000 | Loss: 0.00001526
Iteration 138/1000 | Loss: 0.00001525
Iteration 139/1000 | Loss: 0.00001525
Iteration 140/1000 | Loss: 0.00001525
Iteration 141/1000 | Loss: 0.00001525
Iteration 142/1000 | Loss: 0.00001525
Iteration 143/1000 | Loss: 0.00001525
Iteration 144/1000 | Loss: 0.00001525
Iteration 145/1000 | Loss: 0.00001525
Iteration 146/1000 | Loss: 0.00001525
Iteration 147/1000 | Loss: 0.00001525
Iteration 148/1000 | Loss: 0.00001525
Iteration 149/1000 | Loss: 0.00001525
Iteration 150/1000 | Loss: 0.00001525
Iteration 151/1000 | Loss: 0.00001525
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.5249674106598832e-05, 1.5249674106598832e-05, 1.5249674106598832e-05, 1.5249674106598832e-05, 1.5249674106598832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5249674106598832e-05

Optimization complete. Final v2v error: 3.319668769836426 mm

Highest mean error: 3.670604705810547 mm for frame 32

Lowest mean error: 3.0620744228363037 mm for frame 12

Saving results

Total time: 669.019611120224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1006
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386666
Iteration 2/25 | Loss: 0.00092725
Iteration 3/25 | Loss: 0.00077901
Iteration 4/25 | Loss: 0.00075412
Iteration 5/25 | Loss: 0.00074945
Iteration 6/25 | Loss: 0.00074803
Iteration 7/25 | Loss: 0.00074754
Iteration 8/25 | Loss: 0.00074751
Iteration 9/25 | Loss: 0.00074751
Iteration 10/25 | Loss: 0.00074751
Iteration 11/25 | Loss: 0.00074751
Iteration 12/25 | Loss: 0.00074751
Iteration 13/25 | Loss: 0.00074751
Iteration 14/25 | Loss: 0.00074751
Iteration 15/25 | Loss: 0.00074751
Iteration 16/25 | Loss: 0.00074751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000747513200622052, 0.000747513200622052, 0.000747513200622052, 0.000747513200622052, 0.000747513200622052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000747513200622052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.93192720
Iteration 2/25 | Loss: 0.00059249
Iteration 3/25 | Loss: 0.00059244
Iteration 4/25 | Loss: 0.00059244
Iteration 5/25 | Loss: 0.00059244
Iteration 6/25 | Loss: 0.00059244
Iteration 7/25 | Loss: 0.00059244
Iteration 8/25 | Loss: 0.00059243
Iteration 9/25 | Loss: 0.00059243
Iteration 10/25 | Loss: 0.00059243
Iteration 11/25 | Loss: 0.00059243
Iteration 12/25 | Loss: 0.00059243
Iteration 13/25 | Loss: 0.00059243
Iteration 14/25 | Loss: 0.00059243
Iteration 15/25 | Loss: 0.00059243
Iteration 16/25 | Loss: 0.00059243
Iteration 17/25 | Loss: 0.00059243
Iteration 18/25 | Loss: 0.00059243
Iteration 19/25 | Loss: 0.00059243
Iteration 20/25 | Loss: 0.00059243
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005924342549405992, 0.0005924342549405992, 0.0005924342549405992, 0.0005924342549405992, 0.0005924342549405992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005924342549405992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059243
Iteration 2/1000 | Loss: 0.00003365
Iteration 3/1000 | Loss: 0.00002140
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001596
Iteration 6/1000 | Loss: 0.00001520
Iteration 7/1000 | Loss: 0.00001460
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001388
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001360
Iteration 13/1000 | Loss: 0.00001358
Iteration 14/1000 | Loss: 0.00001354
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001349
Iteration 17/1000 | Loss: 0.00001349
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001346
Iteration 22/1000 | Loss: 0.00001344
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001326
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00001325
Iteration 32/1000 | Loss: 0.00001325
Iteration 33/1000 | Loss: 0.00001325
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001318
Iteration 39/1000 | Loss: 0.00001316
Iteration 40/1000 | Loss: 0.00001316
Iteration 41/1000 | Loss: 0.00001315
Iteration 42/1000 | Loss: 0.00001315
Iteration 43/1000 | Loss: 0.00001315
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001309
Iteration 48/1000 | Loss: 0.00001309
Iteration 49/1000 | Loss: 0.00001308
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001301
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001300
Iteration 57/1000 | Loss: 0.00001300
Iteration 58/1000 | Loss: 0.00001300
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001300
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001300
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001299
Iteration 66/1000 | Loss: 0.00001299
Iteration 67/1000 | Loss: 0.00001299
Iteration 68/1000 | Loss: 0.00001299
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001297
Iteration 73/1000 | Loss: 0.00001297
Iteration 74/1000 | Loss: 0.00001297
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001297
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001296
Iteration 82/1000 | Loss: 0.00001296
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001294
Iteration 93/1000 | Loss: 0.00001294
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001293
Iteration 115/1000 | Loss: 0.00001293
Iteration 116/1000 | Loss: 0.00001293
Iteration 117/1000 | Loss: 0.00001293
Iteration 118/1000 | Loss: 0.00001293
Iteration 119/1000 | Loss: 0.00001292
Iteration 120/1000 | Loss: 0.00001292
Iteration 121/1000 | Loss: 0.00001292
Iteration 122/1000 | Loss: 0.00001292
Iteration 123/1000 | Loss: 0.00001292
Iteration 124/1000 | Loss: 0.00001292
Iteration 125/1000 | Loss: 0.00001292
Iteration 126/1000 | Loss: 0.00001292
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001292
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001291
Iteration 138/1000 | Loss: 0.00001291
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001290
Iteration 142/1000 | Loss: 0.00001290
Iteration 143/1000 | Loss: 0.00001290
Iteration 144/1000 | Loss: 0.00001290
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001290
Iteration 147/1000 | Loss: 0.00001290
Iteration 148/1000 | Loss: 0.00001290
Iteration 149/1000 | Loss: 0.00001289
Iteration 150/1000 | Loss: 0.00001289
Iteration 151/1000 | Loss: 0.00001289
Iteration 152/1000 | Loss: 0.00001289
Iteration 153/1000 | Loss: 0.00001289
Iteration 154/1000 | Loss: 0.00001289
Iteration 155/1000 | Loss: 0.00001289
Iteration 156/1000 | Loss: 0.00001289
Iteration 157/1000 | Loss: 0.00001289
Iteration 158/1000 | Loss: 0.00001288
Iteration 159/1000 | Loss: 0.00001288
Iteration 160/1000 | Loss: 0.00001288
Iteration 161/1000 | Loss: 0.00001288
Iteration 162/1000 | Loss: 0.00001288
Iteration 163/1000 | Loss: 0.00001288
Iteration 164/1000 | Loss: 0.00001288
Iteration 165/1000 | Loss: 0.00001288
Iteration 166/1000 | Loss: 0.00001288
Iteration 167/1000 | Loss: 0.00001288
Iteration 168/1000 | Loss: 0.00001288
Iteration 169/1000 | Loss: 0.00001287
Iteration 170/1000 | Loss: 0.00001287
Iteration 171/1000 | Loss: 0.00001287
Iteration 172/1000 | Loss: 0.00001287
Iteration 173/1000 | Loss: 0.00001287
Iteration 174/1000 | Loss: 0.00001287
Iteration 175/1000 | Loss: 0.00001287
Iteration 176/1000 | Loss: 0.00001287
Iteration 177/1000 | Loss: 0.00001287
Iteration 178/1000 | Loss: 0.00001286
Iteration 179/1000 | Loss: 0.00001286
Iteration 180/1000 | Loss: 0.00001286
Iteration 181/1000 | Loss: 0.00001286
Iteration 182/1000 | Loss: 0.00001286
Iteration 183/1000 | Loss: 0.00001286
Iteration 184/1000 | Loss: 0.00001286
Iteration 185/1000 | Loss: 0.00001286
Iteration 186/1000 | Loss: 0.00001286
Iteration 187/1000 | Loss: 0.00001285
Iteration 188/1000 | Loss: 0.00001285
Iteration 189/1000 | Loss: 0.00001285
Iteration 190/1000 | Loss: 0.00001285
Iteration 191/1000 | Loss: 0.00001285
Iteration 192/1000 | Loss: 0.00001285
Iteration 193/1000 | Loss: 0.00001285
Iteration 194/1000 | Loss: 0.00001285
Iteration 195/1000 | Loss: 0.00001285
Iteration 196/1000 | Loss: 0.00001285
Iteration 197/1000 | Loss: 0.00001285
Iteration 198/1000 | Loss: 0.00001285
Iteration 199/1000 | Loss: 0.00001285
Iteration 200/1000 | Loss: 0.00001285
Iteration 201/1000 | Loss: 0.00001285
Iteration 202/1000 | Loss: 0.00001284
Iteration 203/1000 | Loss: 0.00001284
Iteration 204/1000 | Loss: 0.00001284
Iteration 205/1000 | Loss: 0.00001284
Iteration 206/1000 | Loss: 0.00001284
Iteration 207/1000 | Loss: 0.00001284
Iteration 208/1000 | Loss: 0.00001284
Iteration 209/1000 | Loss: 0.00001284
Iteration 210/1000 | Loss: 0.00001284
Iteration 211/1000 | Loss: 0.00001284
Iteration 212/1000 | Loss: 0.00001284
Iteration 213/1000 | Loss: 0.00001284
Iteration 214/1000 | Loss: 0.00001284
Iteration 215/1000 | Loss: 0.00001284
Iteration 216/1000 | Loss: 0.00001284
Iteration 217/1000 | Loss: 0.00001284
Iteration 218/1000 | Loss: 0.00001284
Iteration 219/1000 | Loss: 0.00001284
Iteration 220/1000 | Loss: 0.00001284
Iteration 221/1000 | Loss: 0.00001284
Iteration 222/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.283881465496961e-05, 1.283881465496961e-05, 1.283881465496961e-05, 1.283881465496961e-05, 1.283881465496961e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.283881465496961e-05

Optimization complete. Final v2v error: 3.0471127033233643 mm

Highest mean error: 3.6054210662841797 mm for frame 8

Lowest mean error: 2.5214226245880127 mm for frame 102

Saving results

Total time: 888.4281797409058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1077
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072367
Iteration 2/25 | Loss: 0.01072367
Iteration 3/25 | Loss: 0.01072367
Iteration 4/25 | Loss: 0.01072367
Iteration 5/25 | Loss: 0.01072367
Iteration 6/25 | Loss: 0.01072366
Iteration 7/25 | Loss: 0.01072366
Iteration 8/25 | Loss: 0.01072366
Iteration 9/25 | Loss: 0.01072366
Iteration 10/25 | Loss: 0.01072366
Iteration 11/25 | Loss: 0.01072366
Iteration 12/25 | Loss: 0.01072365
Iteration 13/25 | Loss: 0.01072365
Iteration 14/25 | Loss: 0.01072365
Iteration 15/25 | Loss: 0.01072365
Iteration 16/25 | Loss: 0.01072365
Iteration 17/25 | Loss: 0.01072364
Iteration 18/25 | Loss: 0.01072364
Iteration 19/25 | Loss: 0.01072364
Iteration 20/25 | Loss: 0.01072364
Iteration 21/25 | Loss: 0.01072364
Iteration 22/25 | Loss: 0.01072364
Iteration 23/25 | Loss: 0.01072363
Iteration 24/25 | Loss: 0.01072363
Iteration 25/25 | Loss: 0.01072363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22712111
Iteration 2/25 | Loss: 0.11311808
Iteration 3/25 | Loss: 0.11115321
Iteration 4/25 | Loss: 0.11022957
Iteration 5/25 | Loss: 0.11022808
Iteration 6/25 | Loss: 0.11022839
Iteration 7/25 | Loss: 0.11022820
Iteration 8/25 | Loss: 0.11022311
Iteration 9/25 | Loss: 0.11022311
Iteration 10/25 | Loss: 0.11022309
Iteration 11/25 | Loss: 0.11022309
Iteration 12/25 | Loss: 0.11022309
Iteration 13/25 | Loss: 0.11022309
Iteration 14/25 | Loss: 0.11022309
Iteration 15/25 | Loss: 0.11022309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.11022309213876724, 0.11022309213876724, 0.11022309213876724, 0.11022309213876724, 0.11022309213876724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11022309213876724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11022309
Iteration 2/1000 | Loss: 0.00186293
Iteration 3/1000 | Loss: 0.00200565
Iteration 4/1000 | Loss: 0.00043454
Iteration 5/1000 | Loss: 0.00036606
Iteration 6/1000 | Loss: 0.00014392
Iteration 7/1000 | Loss: 0.00016643
Iteration 8/1000 | Loss: 0.00008396
Iteration 9/1000 | Loss: 0.00024104
Iteration 10/1000 | Loss: 0.00010632
Iteration 11/1000 | Loss: 0.00008976
Iteration 12/1000 | Loss: 0.00015494
Iteration 13/1000 | Loss: 0.00016818
Iteration 14/1000 | Loss: 0.00004393
Iteration 15/1000 | Loss: 0.00020578
Iteration 16/1000 | Loss: 0.00045764
Iteration 17/1000 | Loss: 0.00020022
Iteration 18/1000 | Loss: 0.00004838
Iteration 19/1000 | Loss: 0.00011601
Iteration 20/1000 | Loss: 0.00003829
Iteration 21/1000 | Loss: 0.00004097
Iteration 22/1000 | Loss: 0.00016115
Iteration 23/1000 | Loss: 0.00007172
Iteration 24/1000 | Loss: 0.00005479
Iteration 25/1000 | Loss: 0.00004506
Iteration 26/1000 | Loss: 0.00003545
Iteration 27/1000 | Loss: 0.00003601
Iteration 28/1000 | Loss: 0.00003369
Iteration 29/1000 | Loss: 0.00049061
Iteration 30/1000 | Loss: 0.00005361
Iteration 31/1000 | Loss: 0.00008626
Iteration 32/1000 | Loss: 0.00007734
Iteration 33/1000 | Loss: 0.00003055
Iteration 34/1000 | Loss: 0.00003143
Iteration 35/1000 | Loss: 0.00011455
Iteration 36/1000 | Loss: 0.00002881
Iteration 37/1000 | Loss: 0.00003376
Iteration 38/1000 | Loss: 0.00002937
Iteration 39/1000 | Loss: 0.00019746
Iteration 40/1000 | Loss: 0.00022912
Iteration 41/1000 | Loss: 0.00006614
Iteration 42/1000 | Loss: 0.00002678
Iteration 43/1000 | Loss: 0.00002666
Iteration 44/1000 | Loss: 0.00002627
Iteration 45/1000 | Loss: 0.00009966
Iteration 46/1000 | Loss: 0.00026956
Iteration 47/1000 | Loss: 0.00003091
Iteration 48/1000 | Loss: 0.00008456
Iteration 49/1000 | Loss: 0.00045433
Iteration 50/1000 | Loss: 0.00003025
Iteration 51/1000 | Loss: 0.00002610
Iteration 52/1000 | Loss: 0.00013826
Iteration 53/1000 | Loss: 0.00002936
Iteration 54/1000 | Loss: 0.00002564
Iteration 55/1000 | Loss: 0.00010397
Iteration 56/1000 | Loss: 0.00003065
Iteration 57/1000 | Loss: 0.00003258
Iteration 58/1000 | Loss: 0.00008016
Iteration 59/1000 | Loss: 0.00003964
Iteration 60/1000 | Loss: 0.00002639
Iteration 61/1000 | Loss: 0.00003322
Iteration 62/1000 | Loss: 0.00002536
Iteration 63/1000 | Loss: 0.00002621
Iteration 64/1000 | Loss: 0.00002532
Iteration 65/1000 | Loss: 0.00002532
Iteration 66/1000 | Loss: 0.00002531
Iteration 67/1000 | Loss: 0.00002530
Iteration 68/1000 | Loss: 0.00002530
Iteration 69/1000 | Loss: 0.00002530
Iteration 70/1000 | Loss: 0.00002532
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002538
Iteration 73/1000 | Loss: 0.00002529
Iteration 74/1000 | Loss: 0.00002527
Iteration 75/1000 | Loss: 0.00002527
Iteration 76/1000 | Loss: 0.00004236
Iteration 77/1000 | Loss: 0.00002624
Iteration 78/1000 | Loss: 0.00002521
Iteration 79/1000 | Loss: 0.00002519
Iteration 80/1000 | Loss: 0.00002519
Iteration 81/1000 | Loss: 0.00002519
Iteration 82/1000 | Loss: 0.00002519
Iteration 83/1000 | Loss: 0.00002518
Iteration 84/1000 | Loss: 0.00002518
Iteration 85/1000 | Loss: 0.00002518
Iteration 86/1000 | Loss: 0.00002518
Iteration 87/1000 | Loss: 0.00002518
Iteration 88/1000 | Loss: 0.00002518
Iteration 89/1000 | Loss: 0.00002518
Iteration 90/1000 | Loss: 0.00002518
Iteration 91/1000 | Loss: 0.00002518
Iteration 92/1000 | Loss: 0.00002518
Iteration 93/1000 | Loss: 0.00002518
Iteration 94/1000 | Loss: 0.00002518
Iteration 95/1000 | Loss: 0.00002517
Iteration 96/1000 | Loss: 0.00002517
Iteration 97/1000 | Loss: 0.00002517
Iteration 98/1000 | Loss: 0.00002517
Iteration 99/1000 | Loss: 0.00002517
Iteration 100/1000 | Loss: 0.00002517
Iteration 101/1000 | Loss: 0.00002517
Iteration 102/1000 | Loss: 0.00002517
Iteration 103/1000 | Loss: 0.00002517
Iteration 104/1000 | Loss: 0.00002517
Iteration 105/1000 | Loss: 0.00002517
Iteration 106/1000 | Loss: 0.00002516
Iteration 107/1000 | Loss: 0.00002516
Iteration 108/1000 | Loss: 0.00002516
Iteration 109/1000 | Loss: 0.00002515
Iteration 110/1000 | Loss: 0.00002515
Iteration 111/1000 | Loss: 0.00002515
Iteration 112/1000 | Loss: 0.00002609
Iteration 113/1000 | Loss: 0.00002512
Iteration 114/1000 | Loss: 0.00013522
Iteration 115/1000 | Loss: 0.00010747
Iteration 116/1000 | Loss: 0.00002513
Iteration 117/1000 | Loss: 0.00002504
Iteration 118/1000 | Loss: 0.00002504
Iteration 119/1000 | Loss: 0.00002504
Iteration 120/1000 | Loss: 0.00002504
Iteration 121/1000 | Loss: 0.00002504
Iteration 122/1000 | Loss: 0.00002504
Iteration 123/1000 | Loss: 0.00002504
Iteration 124/1000 | Loss: 0.00002504
Iteration 125/1000 | Loss: 0.00002503
Iteration 126/1000 | Loss: 0.00002503
Iteration 127/1000 | Loss: 0.00002503
Iteration 128/1000 | Loss: 0.00002503
Iteration 129/1000 | Loss: 0.00002503
Iteration 130/1000 | Loss: 0.00002503
Iteration 131/1000 | Loss: 0.00002503
Iteration 132/1000 | Loss: 0.00002503
Iteration 133/1000 | Loss: 0.00002502
Iteration 134/1000 | Loss: 0.00004120
Iteration 135/1000 | Loss: 0.00002818
Iteration 136/1000 | Loss: 0.00002699
Iteration 137/1000 | Loss: 0.00002498
Iteration 138/1000 | Loss: 0.00002498
Iteration 139/1000 | Loss: 0.00002498
Iteration 140/1000 | Loss: 0.00002498
Iteration 141/1000 | Loss: 0.00002498
Iteration 142/1000 | Loss: 0.00002498
Iteration 143/1000 | Loss: 0.00002498
Iteration 144/1000 | Loss: 0.00002498
Iteration 145/1000 | Loss: 0.00002498
Iteration 146/1000 | Loss: 0.00002498
Iteration 147/1000 | Loss: 0.00002498
Iteration 148/1000 | Loss: 0.00002498
Iteration 149/1000 | Loss: 0.00002498
Iteration 150/1000 | Loss: 0.00002497
Iteration 151/1000 | Loss: 0.00002497
Iteration 152/1000 | Loss: 0.00002497
Iteration 153/1000 | Loss: 0.00002497
Iteration 154/1000 | Loss: 0.00002497
Iteration 155/1000 | Loss: 0.00002497
Iteration 156/1000 | Loss: 0.00002496
Iteration 157/1000 | Loss: 0.00002496
Iteration 158/1000 | Loss: 0.00002496
Iteration 159/1000 | Loss: 0.00002496
Iteration 160/1000 | Loss: 0.00002496
Iteration 161/1000 | Loss: 0.00002496
Iteration 162/1000 | Loss: 0.00002496
Iteration 163/1000 | Loss: 0.00002495
Iteration 164/1000 | Loss: 0.00002495
Iteration 165/1000 | Loss: 0.00002495
Iteration 166/1000 | Loss: 0.00002495
Iteration 167/1000 | Loss: 0.00002495
Iteration 168/1000 | Loss: 0.00002495
Iteration 169/1000 | Loss: 0.00002495
Iteration 170/1000 | Loss: 0.00002510
Iteration 171/1000 | Loss: 0.00002495
Iteration 172/1000 | Loss: 0.00002495
Iteration 173/1000 | Loss: 0.00002494
Iteration 174/1000 | Loss: 0.00002494
Iteration 175/1000 | Loss: 0.00002494
Iteration 176/1000 | Loss: 0.00002494
Iteration 177/1000 | Loss: 0.00002494
Iteration 178/1000 | Loss: 0.00002494
Iteration 179/1000 | Loss: 0.00002494
Iteration 180/1000 | Loss: 0.00002494
Iteration 181/1000 | Loss: 0.00002494
Iteration 182/1000 | Loss: 0.00002494
Iteration 183/1000 | Loss: 0.00002931
Iteration 184/1000 | Loss: 0.00002494
Iteration 185/1000 | Loss: 0.00002493
Iteration 186/1000 | Loss: 0.00002493
Iteration 187/1000 | Loss: 0.00002492
Iteration 188/1000 | Loss: 0.00002491
Iteration 189/1000 | Loss: 0.00002491
Iteration 190/1000 | Loss: 0.00002491
Iteration 191/1000 | Loss: 0.00002491
Iteration 192/1000 | Loss: 0.00002491
Iteration 193/1000 | Loss: 0.00002491
Iteration 194/1000 | Loss: 0.00002491
Iteration 195/1000 | Loss: 0.00002491
Iteration 196/1000 | Loss: 0.00002491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.490792394382879e-05, 2.490792394382879e-05, 2.490792394382879e-05, 2.490792394382879e-05, 2.490792394382879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.490792394382879e-05

Optimization complete. Final v2v error: 4.155055522918701 mm

Highest mean error: 5.818776607513428 mm for frame 192

Lowest mean error: 3.288109302520752 mm for frame 180

Saving results

Total time: 3139.2741384506226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1073/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1073. Skipping.
