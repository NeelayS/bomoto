Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=126, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7056-7111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433184
Iteration 2/25 | Loss: 0.00153258
Iteration 3/25 | Loss: 0.00143921
Iteration 4/25 | Loss: 0.00142720
Iteration 5/25 | Loss: 0.00142262
Iteration 6/25 | Loss: 0.00142113
Iteration 7/25 | Loss: 0.00142113
Iteration 8/25 | Loss: 0.00142113
Iteration 9/25 | Loss: 0.00142113
Iteration 10/25 | Loss: 0.00142113
Iteration 11/25 | Loss: 0.00142113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014211342204362154, 0.0014211342204362154, 0.0014211342204362154, 0.0014211342204362154, 0.0014211342204362154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014211342204362154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56406879
Iteration 2/25 | Loss: 0.00092324
Iteration 3/25 | Loss: 0.00092324
Iteration 4/25 | Loss: 0.00092324
Iteration 5/25 | Loss: 0.00092324
Iteration 6/25 | Loss: 0.00092324
Iteration 7/25 | Loss: 0.00092324
Iteration 8/25 | Loss: 0.00092324
Iteration 9/25 | Loss: 0.00092324
Iteration 10/25 | Loss: 0.00092323
Iteration 11/25 | Loss: 0.00092323
Iteration 12/25 | Loss: 0.00092323
Iteration 13/25 | Loss: 0.00092323
Iteration 14/25 | Loss: 0.00092323
Iteration 15/25 | Loss: 0.00092323
Iteration 16/25 | Loss: 0.00092323
Iteration 17/25 | Loss: 0.00092323
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009232346783392131, 0.0009232346783392131, 0.0009232346783392131, 0.0009232346783392131, 0.0009232346783392131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009232346783392131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092323
Iteration 2/1000 | Loss: 0.00005988
Iteration 3/1000 | Loss: 0.00003137
Iteration 4/1000 | Loss: 0.00002512
Iteration 5/1000 | Loss: 0.00002248
Iteration 6/1000 | Loss: 0.00002102
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001878
Iteration 10/1000 | Loss: 0.00001845
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00001809
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001797
Iteration 15/1000 | Loss: 0.00001796
Iteration 16/1000 | Loss: 0.00001782
Iteration 17/1000 | Loss: 0.00001781
Iteration 18/1000 | Loss: 0.00001779
Iteration 19/1000 | Loss: 0.00001776
Iteration 20/1000 | Loss: 0.00001773
Iteration 21/1000 | Loss: 0.00001773
Iteration 22/1000 | Loss: 0.00001773
Iteration 23/1000 | Loss: 0.00001771
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001768
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001768
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001767
Iteration 31/1000 | Loss: 0.00001767
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001767
Iteration 34/1000 | Loss: 0.00001766
Iteration 35/1000 | Loss: 0.00001766
Iteration 36/1000 | Loss: 0.00001766
Iteration 37/1000 | Loss: 0.00001766
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001765
Iteration 40/1000 | Loss: 0.00001765
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001764
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001762
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001761
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001759
Iteration 75/1000 | Loss: 0.00001759
Iteration 76/1000 | Loss: 0.00001759
Iteration 77/1000 | Loss: 0.00001759
Iteration 78/1000 | Loss: 0.00001759
Iteration 79/1000 | Loss: 0.00001759
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001756
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001756
Iteration 95/1000 | Loss: 0.00001756
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001755
Iteration 99/1000 | Loss: 0.00001755
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001755
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001752
Iteration 114/1000 | Loss: 0.00001752
Iteration 115/1000 | Loss: 0.00001752
Iteration 116/1000 | Loss: 0.00001752
Iteration 117/1000 | Loss: 0.00001751
Iteration 118/1000 | Loss: 0.00001751
Iteration 119/1000 | Loss: 0.00001751
Iteration 120/1000 | Loss: 0.00001751
Iteration 121/1000 | Loss: 0.00001751
Iteration 122/1000 | Loss: 0.00001751
Iteration 123/1000 | Loss: 0.00001751
Iteration 124/1000 | Loss: 0.00001751
Iteration 125/1000 | Loss: 0.00001751
Iteration 126/1000 | Loss: 0.00001751
Iteration 127/1000 | Loss: 0.00001751
Iteration 128/1000 | Loss: 0.00001751
Iteration 129/1000 | Loss: 0.00001751
Iteration 130/1000 | Loss: 0.00001751
Iteration 131/1000 | Loss: 0.00001751
Iteration 132/1000 | Loss: 0.00001751
Iteration 133/1000 | Loss: 0.00001751
Iteration 134/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.7510050383862108e-05, 1.7510050383862108e-05, 1.7510050383862108e-05, 1.7510050383862108e-05, 1.7510050383862108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7510050383862108e-05

Optimization complete. Final v2v error: 3.608860492706299 mm

Highest mean error: 4.049317359924316 mm for frame 109

Lowest mean error: 3.248148202896118 mm for frame 1

Saving results

Total time: 37.48958420753479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413389
Iteration 2/25 | Loss: 0.00146967
Iteration 3/25 | Loss: 0.00139994
Iteration 4/25 | Loss: 0.00138875
Iteration 5/25 | Loss: 0.00138439
Iteration 6/25 | Loss: 0.00138341
Iteration 7/25 | Loss: 0.00138333
Iteration 8/25 | Loss: 0.00138333
Iteration 9/25 | Loss: 0.00138333
Iteration 10/25 | Loss: 0.00138333
Iteration 11/25 | Loss: 0.00138333
Iteration 12/25 | Loss: 0.00138333
Iteration 13/25 | Loss: 0.00138333
Iteration 14/25 | Loss: 0.00138333
Iteration 15/25 | Loss: 0.00138333
Iteration 16/25 | Loss: 0.00138333
Iteration 17/25 | Loss: 0.00138333
Iteration 18/25 | Loss: 0.00138333
Iteration 19/25 | Loss: 0.00138333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001383332535624504, 0.001383332535624504, 0.001383332535624504, 0.001383332535624504, 0.001383332535624504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001383332535624504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39163256
Iteration 2/25 | Loss: 0.00102313
Iteration 3/25 | Loss: 0.00102313
Iteration 4/25 | Loss: 0.00102313
Iteration 5/25 | Loss: 0.00102313
Iteration 6/25 | Loss: 0.00102313
Iteration 7/25 | Loss: 0.00102313
Iteration 8/25 | Loss: 0.00102313
Iteration 9/25 | Loss: 0.00102313
Iteration 10/25 | Loss: 0.00102313
Iteration 11/25 | Loss: 0.00102313
Iteration 12/25 | Loss: 0.00102313
Iteration 13/25 | Loss: 0.00102313
Iteration 14/25 | Loss: 0.00102313
Iteration 15/25 | Loss: 0.00102313
Iteration 16/25 | Loss: 0.00102313
Iteration 17/25 | Loss: 0.00102313
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010231256019324064, 0.0010231256019324064, 0.0010231256019324064, 0.0010231256019324064, 0.0010231256019324064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010231256019324064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102313
Iteration 2/1000 | Loss: 0.00003689
Iteration 3/1000 | Loss: 0.00002206
Iteration 4/1000 | Loss: 0.00001976
Iteration 5/1000 | Loss: 0.00001854
Iteration 6/1000 | Loss: 0.00001799
Iteration 7/1000 | Loss: 0.00001740
Iteration 8/1000 | Loss: 0.00001711
Iteration 9/1000 | Loss: 0.00001681
Iteration 10/1000 | Loss: 0.00001670
Iteration 11/1000 | Loss: 0.00001667
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001655
Iteration 14/1000 | Loss: 0.00001655
Iteration 15/1000 | Loss: 0.00001643
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001635
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001614
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001610
Iteration 24/1000 | Loss: 0.00001608
Iteration 25/1000 | Loss: 0.00001608
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001608
Iteration 28/1000 | Loss: 0.00001608
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001608
Iteration 31/1000 | Loss: 0.00001607
Iteration 32/1000 | Loss: 0.00001606
Iteration 33/1000 | Loss: 0.00001605
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001604
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001603
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00001603
Iteration 42/1000 | Loss: 0.00001603
Iteration 43/1000 | Loss: 0.00001602
Iteration 44/1000 | Loss: 0.00001602
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001601
Iteration 54/1000 | Loss: 0.00001601
Iteration 55/1000 | Loss: 0.00001601
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001600
Iteration 61/1000 | Loss: 0.00001600
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001600
Iteration 64/1000 | Loss: 0.00001600
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001600
Iteration 69/1000 | Loss: 0.00001600
Iteration 70/1000 | Loss: 0.00001600
Iteration 71/1000 | Loss: 0.00001600
Iteration 72/1000 | Loss: 0.00001600
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001600
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001600
Iteration 79/1000 | Loss: 0.00001600
Iteration 80/1000 | Loss: 0.00001600
Iteration 81/1000 | Loss: 0.00001600
Iteration 82/1000 | Loss: 0.00001600
Iteration 83/1000 | Loss: 0.00001600
Iteration 84/1000 | Loss: 0.00001600
Iteration 85/1000 | Loss: 0.00001600
Iteration 86/1000 | Loss: 0.00001600
Iteration 87/1000 | Loss: 0.00001600
Iteration 88/1000 | Loss: 0.00001600
Iteration 89/1000 | Loss: 0.00001600
Iteration 90/1000 | Loss: 0.00001600
Iteration 91/1000 | Loss: 0.00001600
Iteration 92/1000 | Loss: 0.00001600
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001600
Iteration 95/1000 | Loss: 0.00001600
Iteration 96/1000 | Loss: 0.00001600
Iteration 97/1000 | Loss: 0.00001600
Iteration 98/1000 | Loss: 0.00001600
Iteration 99/1000 | Loss: 0.00001600
Iteration 100/1000 | Loss: 0.00001600
Iteration 101/1000 | Loss: 0.00001600
Iteration 102/1000 | Loss: 0.00001600
Iteration 103/1000 | Loss: 0.00001600
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.600044379301835e-05, 1.600044379301835e-05, 1.600044379301835e-05, 1.600044379301835e-05, 1.600044379301835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.600044379301835e-05

Optimization complete. Final v2v error: 3.4804024696350098 mm

Highest mean error: 3.8258066177368164 mm for frame 0

Lowest mean error: 3.307143211364746 mm for frame 71

Saving results

Total time: 31.94581961631775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01124562
Iteration 2/25 | Loss: 0.00357875
Iteration 3/25 | Loss: 0.00278974
Iteration 4/25 | Loss: 0.00160493
Iteration 5/25 | Loss: 0.00154693
Iteration 6/25 | Loss: 0.00153203
Iteration 7/25 | Loss: 0.00152206
Iteration 8/25 | Loss: 0.00152517
Iteration 9/25 | Loss: 0.00153522
Iteration 10/25 | Loss: 0.00152690
Iteration 11/25 | Loss: 0.00150267
Iteration 12/25 | Loss: 0.00150232
Iteration 13/25 | Loss: 0.00149447
Iteration 14/25 | Loss: 0.00148950
Iteration 15/25 | Loss: 0.00148536
Iteration 16/25 | Loss: 0.00148202
Iteration 17/25 | Loss: 0.00148022
Iteration 18/25 | Loss: 0.00148171
Iteration 19/25 | Loss: 0.00147591
Iteration 20/25 | Loss: 0.00147248
Iteration 21/25 | Loss: 0.00147119
Iteration 22/25 | Loss: 0.00147427
Iteration 23/25 | Loss: 0.00147144
Iteration 24/25 | Loss: 0.00146777
Iteration 25/25 | Loss: 0.00146802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43702412
Iteration 2/25 | Loss: 0.00107662
Iteration 3/25 | Loss: 0.00107662
Iteration 4/25 | Loss: 0.00107662
Iteration 5/25 | Loss: 0.00107662
Iteration 6/25 | Loss: 0.00107662
Iteration 7/25 | Loss: 0.00107662
Iteration 8/25 | Loss: 0.00107662
Iteration 9/25 | Loss: 0.00107662
Iteration 10/25 | Loss: 0.00107662
Iteration 11/25 | Loss: 0.00107662
Iteration 12/25 | Loss: 0.00107662
Iteration 13/25 | Loss: 0.00107662
Iteration 14/25 | Loss: 0.00107662
Iteration 15/25 | Loss: 0.00107662
Iteration 16/25 | Loss: 0.00107662
Iteration 17/25 | Loss: 0.00107662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010766156483441591, 0.0010766156483441591, 0.0010766156483441591, 0.0010766156483441591, 0.0010766156483441591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010766156483441591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107662
Iteration 2/1000 | Loss: 0.00040955
Iteration 3/1000 | Loss: 0.00038390
Iteration 4/1000 | Loss: 0.00025329
Iteration 5/1000 | Loss: 0.00034143
Iteration 6/1000 | Loss: 0.00036489
Iteration 7/1000 | Loss: 0.00032523
Iteration 8/1000 | Loss: 0.00004839
Iteration 9/1000 | Loss: 0.00004879
Iteration 10/1000 | Loss: 0.00004785
Iteration 11/1000 | Loss: 0.00004664
Iteration 12/1000 | Loss: 0.00003328
Iteration 13/1000 | Loss: 0.00021729
Iteration 14/1000 | Loss: 0.00016149
Iteration 15/1000 | Loss: 0.00018240
Iteration 16/1000 | Loss: 0.00023230
Iteration 17/1000 | Loss: 0.00021107
Iteration 18/1000 | Loss: 0.00015654
Iteration 19/1000 | Loss: 0.00013200
Iteration 20/1000 | Loss: 0.00012693
Iteration 21/1000 | Loss: 0.00005353
Iteration 22/1000 | Loss: 0.00004009
Iteration 23/1000 | Loss: 0.00004421
Iteration 24/1000 | Loss: 0.00003863
Iteration 25/1000 | Loss: 0.00003536
Iteration 26/1000 | Loss: 0.00004170
Iteration 27/1000 | Loss: 0.00004416
Iteration 28/1000 | Loss: 0.00003483
Iteration 29/1000 | Loss: 0.00004381
Iteration 30/1000 | Loss: 0.00004113
Iteration 31/1000 | Loss: 0.00004198
Iteration 32/1000 | Loss: 0.00003448
Iteration 33/1000 | Loss: 0.00003726
Iteration 34/1000 | Loss: 0.00004211
Iteration 35/1000 | Loss: 0.00004585
Iteration 36/1000 | Loss: 0.00004221
Iteration 37/1000 | Loss: 0.00004441
Iteration 38/1000 | Loss: 0.00004210
Iteration 39/1000 | Loss: 0.00004473
Iteration 40/1000 | Loss: 0.00003550
Iteration 41/1000 | Loss: 0.00003733
Iteration 42/1000 | Loss: 0.00003831
Iteration 43/1000 | Loss: 0.00004481
Iteration 44/1000 | Loss: 0.00004186
Iteration 45/1000 | Loss: 0.00004368
Iteration 46/1000 | Loss: 0.00004154
Iteration 47/1000 | Loss: 0.00004334
Iteration 48/1000 | Loss: 0.00003912
Iteration 49/1000 | Loss: 0.00004295
Iteration 50/1000 | Loss: 0.00002589
Iteration 51/1000 | Loss: 0.00003031
Iteration 52/1000 | Loss: 0.00002433
Iteration 53/1000 | Loss: 0.00003586
Iteration 54/1000 | Loss: 0.00004158
Iteration 55/1000 | Loss: 0.00004896
Iteration 56/1000 | Loss: 0.00003448
Iteration 57/1000 | Loss: 0.00003472
Iteration 58/1000 | Loss: 0.00004244
Iteration 59/1000 | Loss: 0.00003113
Iteration 60/1000 | Loss: 0.00003242
Iteration 61/1000 | Loss: 0.00004934
Iteration 62/1000 | Loss: 0.00004123
Iteration 63/1000 | Loss: 0.00003026
Iteration 64/1000 | Loss: 0.00005229
Iteration 65/1000 | Loss: 0.00004540
Iteration 66/1000 | Loss: 0.00003785
Iteration 67/1000 | Loss: 0.00003531
Iteration 68/1000 | Loss: 0.00003632
Iteration 69/1000 | Loss: 0.00003486
Iteration 70/1000 | Loss: 0.00002939
Iteration 71/1000 | Loss: 0.00003334
Iteration 72/1000 | Loss: 0.00002885
Iteration 73/1000 | Loss: 0.00003226
Iteration 74/1000 | Loss: 0.00002843
Iteration 75/1000 | Loss: 0.00003206
Iteration 76/1000 | Loss: 0.00002882
Iteration 77/1000 | Loss: 0.00003808
Iteration 78/1000 | Loss: 0.00003242
Iteration 79/1000 | Loss: 0.00003936
Iteration 80/1000 | Loss: 0.00003192
Iteration 81/1000 | Loss: 0.00003633
Iteration 82/1000 | Loss: 0.00002879
Iteration 83/1000 | Loss: 0.00006176
Iteration 84/1000 | Loss: 0.00003959
Iteration 85/1000 | Loss: 0.00003284
Iteration 86/1000 | Loss: 0.00005947
Iteration 87/1000 | Loss: 0.00004322
Iteration 88/1000 | Loss: 0.00005260
Iteration 89/1000 | Loss: 0.00003664
Iteration 90/1000 | Loss: 0.00003767
Iteration 91/1000 | Loss: 0.00002671
Iteration 92/1000 | Loss: 0.00003459
Iteration 93/1000 | Loss: 0.00003118
Iteration 94/1000 | Loss: 0.00003829
Iteration 95/1000 | Loss: 0.00003684
Iteration 96/1000 | Loss: 0.00003068
Iteration 97/1000 | Loss: 0.00002880
Iteration 98/1000 | Loss: 0.00002908
Iteration 99/1000 | Loss: 0.00003232
Iteration 100/1000 | Loss: 0.00003422
Iteration 101/1000 | Loss: 0.00003230
Iteration 102/1000 | Loss: 0.00003500
Iteration 103/1000 | Loss: 0.00003362
Iteration 104/1000 | Loss: 0.00003330
Iteration 105/1000 | Loss: 0.00003156
Iteration 106/1000 | Loss: 0.00003679
Iteration 107/1000 | Loss: 0.00003103
Iteration 108/1000 | Loss: 0.00003712
Iteration 109/1000 | Loss: 0.00002916
Iteration 110/1000 | Loss: 0.00003268
Iteration 111/1000 | Loss: 0.00003485
Iteration 112/1000 | Loss: 0.00003669
Iteration 113/1000 | Loss: 0.00003404
Iteration 114/1000 | Loss: 0.00003599
Iteration 115/1000 | Loss: 0.00003332
Iteration 116/1000 | Loss: 0.00003314
Iteration 117/1000 | Loss: 0.00003443
Iteration 118/1000 | Loss: 0.00003482
Iteration 119/1000 | Loss: 0.00003420
Iteration 120/1000 | Loss: 0.00003059
Iteration 121/1000 | Loss: 0.00002994
Iteration 122/1000 | Loss: 0.00002908
Iteration 123/1000 | Loss: 0.00002882
Iteration 124/1000 | Loss: 0.00003248
Iteration 125/1000 | Loss: 0.00002962
Iteration 126/1000 | Loss: 0.00003159
Iteration 127/1000 | Loss: 0.00002946
Iteration 128/1000 | Loss: 0.00003137
Iteration 129/1000 | Loss: 0.00003114
Iteration 130/1000 | Loss: 0.00003997
Iteration 131/1000 | Loss: 0.00003187
Iteration 132/1000 | Loss: 0.00003617
Iteration 133/1000 | Loss: 0.00003045
Iteration 134/1000 | Loss: 0.00003550
Iteration 135/1000 | Loss: 0.00003329
Iteration 136/1000 | Loss: 0.00003544
Iteration 137/1000 | Loss: 0.00002897
Iteration 138/1000 | Loss: 0.00002699
Iteration 139/1000 | Loss: 0.00003165
Iteration 140/1000 | Loss: 0.00002752
Iteration 141/1000 | Loss: 0.00002909
Iteration 142/1000 | Loss: 0.00002591
Iteration 143/1000 | Loss: 0.00002594
Iteration 144/1000 | Loss: 0.00002397
Iteration 145/1000 | Loss: 0.00002484
Iteration 146/1000 | Loss: 0.00002786
Iteration 147/1000 | Loss: 0.00002899
Iteration 148/1000 | Loss: 0.00002407
Iteration 149/1000 | Loss: 0.00002468
Iteration 150/1000 | Loss: 0.00002779
Iteration 151/1000 | Loss: 0.00002779
Iteration 152/1000 | Loss: 0.00002778
Iteration 153/1000 | Loss: 0.00003849
Iteration 154/1000 | Loss: 0.00002576
Iteration 155/1000 | Loss: 0.00002413
Iteration 156/1000 | Loss: 0.00002326
Iteration 157/1000 | Loss: 0.00002252
Iteration 158/1000 | Loss: 0.00002215
Iteration 159/1000 | Loss: 0.00002188
Iteration 160/1000 | Loss: 0.00002178
Iteration 161/1000 | Loss: 0.00002176
Iteration 162/1000 | Loss: 0.00002161
Iteration 163/1000 | Loss: 0.00002155
Iteration 164/1000 | Loss: 0.00002152
Iteration 165/1000 | Loss: 0.00002152
Iteration 166/1000 | Loss: 0.00002150
Iteration 167/1000 | Loss: 0.00002148
Iteration 168/1000 | Loss: 0.00002146
Iteration 169/1000 | Loss: 0.00002145
Iteration 170/1000 | Loss: 0.00002142
Iteration 171/1000 | Loss: 0.00002141
Iteration 172/1000 | Loss: 0.00002137
Iteration 173/1000 | Loss: 0.00002137
Iteration 174/1000 | Loss: 0.00002133
Iteration 175/1000 | Loss: 0.00002132
Iteration 176/1000 | Loss: 0.00002132
Iteration 177/1000 | Loss: 0.00002132
Iteration 178/1000 | Loss: 0.00002131
Iteration 179/1000 | Loss: 0.00002131
Iteration 180/1000 | Loss: 0.00002131
Iteration 181/1000 | Loss: 0.00002131
Iteration 182/1000 | Loss: 0.00002130
Iteration 183/1000 | Loss: 0.00002130
Iteration 184/1000 | Loss: 0.00002130
Iteration 185/1000 | Loss: 0.00002129
Iteration 186/1000 | Loss: 0.00002129
Iteration 187/1000 | Loss: 0.00002129
Iteration 188/1000 | Loss: 0.00002129
Iteration 189/1000 | Loss: 0.00002129
Iteration 190/1000 | Loss: 0.00002129
Iteration 191/1000 | Loss: 0.00002129
Iteration 192/1000 | Loss: 0.00002129
Iteration 193/1000 | Loss: 0.00002129
Iteration 194/1000 | Loss: 0.00002128
Iteration 195/1000 | Loss: 0.00002128
Iteration 196/1000 | Loss: 0.00002128
Iteration 197/1000 | Loss: 0.00002126
Iteration 198/1000 | Loss: 0.00002126
Iteration 199/1000 | Loss: 0.00002126
Iteration 200/1000 | Loss: 0.00002126
Iteration 201/1000 | Loss: 0.00002126
Iteration 202/1000 | Loss: 0.00002126
Iteration 203/1000 | Loss: 0.00002126
Iteration 204/1000 | Loss: 0.00002125
Iteration 205/1000 | Loss: 0.00002125
Iteration 206/1000 | Loss: 0.00002125
Iteration 207/1000 | Loss: 0.00002125
Iteration 208/1000 | Loss: 0.00002125
Iteration 209/1000 | Loss: 0.00002125
Iteration 210/1000 | Loss: 0.00002125
Iteration 211/1000 | Loss: 0.00002124
Iteration 212/1000 | Loss: 0.00002124
Iteration 213/1000 | Loss: 0.00002124
Iteration 214/1000 | Loss: 0.00002123
Iteration 215/1000 | Loss: 0.00002123
Iteration 216/1000 | Loss: 0.00002123
Iteration 217/1000 | Loss: 0.00002123
Iteration 218/1000 | Loss: 0.00002123
Iteration 219/1000 | Loss: 0.00002123
Iteration 220/1000 | Loss: 0.00002123
Iteration 221/1000 | Loss: 0.00002122
Iteration 222/1000 | Loss: 0.00002122
Iteration 223/1000 | Loss: 0.00002121
Iteration 224/1000 | Loss: 0.00002121
Iteration 225/1000 | Loss: 0.00002121
Iteration 226/1000 | Loss: 0.00002121
Iteration 227/1000 | Loss: 0.00002120
Iteration 228/1000 | Loss: 0.00002120
Iteration 229/1000 | Loss: 0.00002119
Iteration 230/1000 | Loss: 0.00002119
Iteration 231/1000 | Loss: 0.00002119
Iteration 232/1000 | Loss: 0.00002119
Iteration 233/1000 | Loss: 0.00002119
Iteration 234/1000 | Loss: 0.00002119
Iteration 235/1000 | Loss: 0.00002119
Iteration 236/1000 | Loss: 0.00002118
Iteration 237/1000 | Loss: 0.00002118
Iteration 238/1000 | Loss: 0.00002118
Iteration 239/1000 | Loss: 0.00002118
Iteration 240/1000 | Loss: 0.00002118
Iteration 241/1000 | Loss: 0.00002117
Iteration 242/1000 | Loss: 0.00002117
Iteration 243/1000 | Loss: 0.00002117
Iteration 244/1000 | Loss: 0.00002117
Iteration 245/1000 | Loss: 0.00002117
Iteration 246/1000 | Loss: 0.00002117
Iteration 247/1000 | Loss: 0.00002116
Iteration 248/1000 | Loss: 0.00002116
Iteration 249/1000 | Loss: 0.00002116
Iteration 250/1000 | Loss: 0.00002116
Iteration 251/1000 | Loss: 0.00002116
Iteration 252/1000 | Loss: 0.00002116
Iteration 253/1000 | Loss: 0.00002115
Iteration 254/1000 | Loss: 0.00002115
Iteration 255/1000 | Loss: 0.00002115
Iteration 256/1000 | Loss: 0.00002115
Iteration 257/1000 | Loss: 0.00002115
Iteration 258/1000 | Loss: 0.00002115
Iteration 259/1000 | Loss: 0.00002115
Iteration 260/1000 | Loss: 0.00002115
Iteration 261/1000 | Loss: 0.00002115
Iteration 262/1000 | Loss: 0.00002115
Iteration 263/1000 | Loss: 0.00002115
Iteration 264/1000 | Loss: 0.00002115
Iteration 265/1000 | Loss: 0.00002115
Iteration 266/1000 | Loss: 0.00002115
Iteration 267/1000 | Loss: 0.00002115
Iteration 268/1000 | Loss: 0.00002115
Iteration 269/1000 | Loss: 0.00002115
Iteration 270/1000 | Loss: 0.00002115
Iteration 271/1000 | Loss: 0.00002115
Iteration 272/1000 | Loss: 0.00002115
Iteration 273/1000 | Loss: 0.00002115
Iteration 274/1000 | Loss: 0.00002115
Iteration 275/1000 | Loss: 0.00002115
Iteration 276/1000 | Loss: 0.00002115
Iteration 277/1000 | Loss: 0.00002115
Iteration 278/1000 | Loss: 0.00002115
Iteration 279/1000 | Loss: 0.00002115
Iteration 280/1000 | Loss: 0.00002115
Iteration 281/1000 | Loss: 0.00002115
Iteration 282/1000 | Loss: 0.00002115
Iteration 283/1000 | Loss: 0.00002115
Iteration 284/1000 | Loss: 0.00002115
Iteration 285/1000 | Loss: 0.00002115
Iteration 286/1000 | Loss: 0.00002115
Iteration 287/1000 | Loss: 0.00002115
Iteration 288/1000 | Loss: 0.00002115
Iteration 289/1000 | Loss: 0.00002115
Iteration 290/1000 | Loss: 0.00002115
Iteration 291/1000 | Loss: 0.00002115
Iteration 292/1000 | Loss: 0.00002115
Iteration 293/1000 | Loss: 0.00002115
Iteration 294/1000 | Loss: 0.00002115
Iteration 295/1000 | Loss: 0.00002115
Iteration 296/1000 | Loss: 0.00002115
Iteration 297/1000 | Loss: 0.00002115
Iteration 298/1000 | Loss: 0.00002115
Iteration 299/1000 | Loss: 0.00002115
Iteration 300/1000 | Loss: 0.00002115
Iteration 301/1000 | Loss: 0.00002115
Iteration 302/1000 | Loss: 0.00002115
Iteration 303/1000 | Loss: 0.00002115
Iteration 304/1000 | Loss: 0.00002115
Iteration 305/1000 | Loss: 0.00002115
Iteration 306/1000 | Loss: 0.00002115
Iteration 307/1000 | Loss: 0.00002115
Iteration 308/1000 | Loss: 0.00002115
Iteration 309/1000 | Loss: 0.00002115
Iteration 310/1000 | Loss: 0.00002115
Iteration 311/1000 | Loss: 0.00002115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [2.1150221073185094e-05, 2.1150221073185094e-05, 2.1150221073185094e-05, 2.1150221073185094e-05, 2.1150221073185094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1150221073185094e-05

Optimization complete. Final v2v error: 3.931764602661133 mm

Highest mean error: 4.624448776245117 mm for frame 79

Lowest mean error: 3.540226697921753 mm for frame 83

Saving results

Total time: 300.21335673332214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451458
Iteration 2/25 | Loss: 0.00148347
Iteration 3/25 | Loss: 0.00140209
Iteration 4/25 | Loss: 0.00139193
Iteration 5/25 | Loss: 0.00138753
Iteration 6/25 | Loss: 0.00138681
Iteration 7/25 | Loss: 0.00138681
Iteration 8/25 | Loss: 0.00138681
Iteration 9/25 | Loss: 0.00138681
Iteration 10/25 | Loss: 0.00138681
Iteration 11/25 | Loss: 0.00138681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001386808231472969, 0.001386808231472969, 0.001386808231472969, 0.001386808231472969, 0.001386808231472969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001386808231472969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.42538786
Iteration 2/25 | Loss: 0.00090162
Iteration 3/25 | Loss: 0.00090161
Iteration 4/25 | Loss: 0.00090161
Iteration 5/25 | Loss: 0.00090161
Iteration 6/25 | Loss: 0.00090161
Iteration 7/25 | Loss: 0.00090161
Iteration 8/25 | Loss: 0.00090161
Iteration 9/25 | Loss: 0.00090161
Iteration 10/25 | Loss: 0.00090161
Iteration 11/25 | Loss: 0.00090161
Iteration 12/25 | Loss: 0.00090161
Iteration 13/25 | Loss: 0.00090161
Iteration 14/25 | Loss: 0.00090161
Iteration 15/25 | Loss: 0.00090161
Iteration 16/25 | Loss: 0.00090161
Iteration 17/25 | Loss: 0.00090161
Iteration 18/25 | Loss: 0.00090161
Iteration 19/25 | Loss: 0.00090161
Iteration 20/25 | Loss: 0.00090161
Iteration 21/25 | Loss: 0.00090161
Iteration 22/25 | Loss: 0.00090161
Iteration 23/25 | Loss: 0.00090161
Iteration 24/25 | Loss: 0.00090161
Iteration 25/25 | Loss: 0.00090161
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009016073890961707, 0.0009016073890961707, 0.0009016073890961707, 0.0009016073890961707, 0.0009016073890961707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009016073890961707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090161
Iteration 2/1000 | Loss: 0.00004560
Iteration 3/1000 | Loss: 0.00002743
Iteration 4/1000 | Loss: 0.00002445
Iteration 5/1000 | Loss: 0.00002274
Iteration 6/1000 | Loss: 0.00002177
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002023
Iteration 10/1000 | Loss: 0.00002007
Iteration 11/1000 | Loss: 0.00002003
Iteration 12/1000 | Loss: 0.00002000
Iteration 13/1000 | Loss: 0.00001999
Iteration 14/1000 | Loss: 0.00001999
Iteration 15/1000 | Loss: 0.00001993
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001991
Iteration 18/1000 | Loss: 0.00001990
Iteration 19/1000 | Loss: 0.00001990
Iteration 20/1000 | Loss: 0.00001989
Iteration 21/1000 | Loss: 0.00001988
Iteration 22/1000 | Loss: 0.00001982
Iteration 23/1000 | Loss: 0.00001977
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001974
Iteration 26/1000 | Loss: 0.00001972
Iteration 27/1000 | Loss: 0.00001971
Iteration 28/1000 | Loss: 0.00001971
Iteration 29/1000 | Loss: 0.00001970
Iteration 30/1000 | Loss: 0.00001969
Iteration 31/1000 | Loss: 0.00001968
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001967
Iteration 34/1000 | Loss: 0.00001967
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001967
Iteration 40/1000 | Loss: 0.00001966
Iteration 41/1000 | Loss: 0.00001966
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001963
Iteration 45/1000 | Loss: 0.00001963
Iteration 46/1000 | Loss: 0.00001963
Iteration 47/1000 | Loss: 0.00001963
Iteration 48/1000 | Loss: 0.00001963
Iteration 49/1000 | Loss: 0.00001962
Iteration 50/1000 | Loss: 0.00001962
Iteration 51/1000 | Loss: 0.00001962
Iteration 52/1000 | Loss: 0.00001962
Iteration 53/1000 | Loss: 0.00001962
Iteration 54/1000 | Loss: 0.00001962
Iteration 55/1000 | Loss: 0.00001961
Iteration 56/1000 | Loss: 0.00001961
Iteration 57/1000 | Loss: 0.00001961
Iteration 58/1000 | Loss: 0.00001961
Iteration 59/1000 | Loss: 0.00001960
Iteration 60/1000 | Loss: 0.00001960
Iteration 61/1000 | Loss: 0.00001960
Iteration 62/1000 | Loss: 0.00001960
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001960
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001959
Iteration 69/1000 | Loss: 0.00001959
Iteration 70/1000 | Loss: 0.00001959
Iteration 71/1000 | Loss: 0.00001959
Iteration 72/1000 | Loss: 0.00001959
Iteration 73/1000 | Loss: 0.00001958
Iteration 74/1000 | Loss: 0.00001958
Iteration 75/1000 | Loss: 0.00001958
Iteration 76/1000 | Loss: 0.00001957
Iteration 77/1000 | Loss: 0.00001957
Iteration 78/1000 | Loss: 0.00001957
Iteration 79/1000 | Loss: 0.00001957
Iteration 80/1000 | Loss: 0.00001957
Iteration 81/1000 | Loss: 0.00001957
Iteration 82/1000 | Loss: 0.00001956
Iteration 83/1000 | Loss: 0.00001956
Iteration 84/1000 | Loss: 0.00001956
Iteration 85/1000 | Loss: 0.00001955
Iteration 86/1000 | Loss: 0.00001955
Iteration 87/1000 | Loss: 0.00001955
Iteration 88/1000 | Loss: 0.00001954
Iteration 89/1000 | Loss: 0.00001954
Iteration 90/1000 | Loss: 0.00001954
Iteration 91/1000 | Loss: 0.00001954
Iteration 92/1000 | Loss: 0.00001954
Iteration 93/1000 | Loss: 0.00001954
Iteration 94/1000 | Loss: 0.00001954
Iteration 95/1000 | Loss: 0.00001954
Iteration 96/1000 | Loss: 0.00001954
Iteration 97/1000 | Loss: 0.00001954
Iteration 98/1000 | Loss: 0.00001954
Iteration 99/1000 | Loss: 0.00001954
Iteration 100/1000 | Loss: 0.00001954
Iteration 101/1000 | Loss: 0.00001954
Iteration 102/1000 | Loss: 0.00001954
Iteration 103/1000 | Loss: 0.00001954
Iteration 104/1000 | Loss: 0.00001954
Iteration 105/1000 | Loss: 0.00001954
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001954
Iteration 108/1000 | Loss: 0.00001954
Iteration 109/1000 | Loss: 0.00001954
Iteration 110/1000 | Loss: 0.00001954
Iteration 111/1000 | Loss: 0.00001954
Iteration 112/1000 | Loss: 0.00001954
Iteration 113/1000 | Loss: 0.00001954
Iteration 114/1000 | Loss: 0.00001954
Iteration 115/1000 | Loss: 0.00001954
Iteration 116/1000 | Loss: 0.00001954
Iteration 117/1000 | Loss: 0.00001954
Iteration 118/1000 | Loss: 0.00001954
Iteration 119/1000 | Loss: 0.00001954
Iteration 120/1000 | Loss: 0.00001954
Iteration 121/1000 | Loss: 0.00001954
Iteration 122/1000 | Loss: 0.00001954
Iteration 123/1000 | Loss: 0.00001954
Iteration 124/1000 | Loss: 0.00001954
Iteration 125/1000 | Loss: 0.00001954
Iteration 126/1000 | Loss: 0.00001954
Iteration 127/1000 | Loss: 0.00001954
Iteration 128/1000 | Loss: 0.00001954
Iteration 129/1000 | Loss: 0.00001954
Iteration 130/1000 | Loss: 0.00001954
Iteration 131/1000 | Loss: 0.00001954
Iteration 132/1000 | Loss: 0.00001954
Iteration 133/1000 | Loss: 0.00001954
Iteration 134/1000 | Loss: 0.00001954
Iteration 135/1000 | Loss: 0.00001954
Iteration 136/1000 | Loss: 0.00001954
Iteration 137/1000 | Loss: 0.00001954
Iteration 138/1000 | Loss: 0.00001954
Iteration 139/1000 | Loss: 0.00001954
Iteration 140/1000 | Loss: 0.00001954
Iteration 141/1000 | Loss: 0.00001954
Iteration 142/1000 | Loss: 0.00001954
Iteration 143/1000 | Loss: 0.00001954
Iteration 144/1000 | Loss: 0.00001954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.953701394086238e-05, 1.953701394086238e-05, 1.953701394086238e-05, 1.953701394086238e-05, 1.953701394086238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.953701394086238e-05

Optimization complete. Final v2v error: 3.853389263153076 mm

Highest mean error: 4.476687431335449 mm for frame 103

Lowest mean error: 3.493659734725952 mm for frame 1

Saving results

Total time: 33.88280916213989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586110
Iteration 2/25 | Loss: 0.00148814
Iteration 3/25 | Loss: 0.00140619
Iteration 4/25 | Loss: 0.00139583
Iteration 5/25 | Loss: 0.00139160
Iteration 6/25 | Loss: 0.00139036
Iteration 7/25 | Loss: 0.00139036
Iteration 8/25 | Loss: 0.00139036
Iteration 9/25 | Loss: 0.00139036
Iteration 10/25 | Loss: 0.00139036
Iteration 11/25 | Loss: 0.00139036
Iteration 12/25 | Loss: 0.00139036
Iteration 13/25 | Loss: 0.00139036
Iteration 14/25 | Loss: 0.00139036
Iteration 15/25 | Loss: 0.00139036
Iteration 16/25 | Loss: 0.00139036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013903609942644835, 0.0013903609942644835, 0.0013903609942644835, 0.0013903609942644835, 0.0013903609942644835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013903609942644835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.52091932
Iteration 2/25 | Loss: 0.00092234
Iteration 3/25 | Loss: 0.00092232
Iteration 4/25 | Loss: 0.00092232
Iteration 5/25 | Loss: 0.00092232
Iteration 6/25 | Loss: 0.00092232
Iteration 7/25 | Loss: 0.00092232
Iteration 8/25 | Loss: 0.00092232
Iteration 9/25 | Loss: 0.00092232
Iteration 10/25 | Loss: 0.00092232
Iteration 11/25 | Loss: 0.00092232
Iteration 12/25 | Loss: 0.00092232
Iteration 13/25 | Loss: 0.00092232
Iteration 14/25 | Loss: 0.00092232
Iteration 15/25 | Loss: 0.00092232
Iteration 16/25 | Loss: 0.00092232
Iteration 17/25 | Loss: 0.00092232
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009223220404237509, 0.0009223220404237509, 0.0009223220404237509, 0.0009223220404237509, 0.0009223220404237509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009223220404237509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092232
Iteration 2/1000 | Loss: 0.00004189
Iteration 3/1000 | Loss: 0.00002756
Iteration 4/1000 | Loss: 0.00002254
Iteration 5/1000 | Loss: 0.00002086
Iteration 6/1000 | Loss: 0.00002003
Iteration 7/1000 | Loss: 0.00001952
Iteration 8/1000 | Loss: 0.00001921
Iteration 9/1000 | Loss: 0.00001898
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001888
Iteration 12/1000 | Loss: 0.00001887
Iteration 13/1000 | Loss: 0.00001881
Iteration 14/1000 | Loss: 0.00001878
Iteration 15/1000 | Loss: 0.00001877
Iteration 16/1000 | Loss: 0.00001876
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001869
Iteration 19/1000 | Loss: 0.00001866
Iteration 20/1000 | Loss: 0.00001865
Iteration 21/1000 | Loss: 0.00001865
Iteration 22/1000 | Loss: 0.00001865
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001864
Iteration 25/1000 | Loss: 0.00001863
Iteration 26/1000 | Loss: 0.00001862
Iteration 27/1000 | Loss: 0.00001861
Iteration 28/1000 | Loss: 0.00001861
Iteration 29/1000 | Loss: 0.00001861
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001861
Iteration 33/1000 | Loss: 0.00001861
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001860
Iteration 37/1000 | Loss: 0.00001860
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001858
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001857
Iteration 43/1000 | Loss: 0.00001857
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001854
Iteration 54/1000 | Loss: 0.00001854
Iteration 55/1000 | Loss: 0.00001854
Iteration 56/1000 | Loss: 0.00001854
Iteration 57/1000 | Loss: 0.00001854
Iteration 58/1000 | Loss: 0.00001854
Iteration 59/1000 | Loss: 0.00001854
Iteration 60/1000 | Loss: 0.00001854
Iteration 61/1000 | Loss: 0.00001854
Iteration 62/1000 | Loss: 0.00001854
Iteration 63/1000 | Loss: 0.00001854
Iteration 64/1000 | Loss: 0.00001854
Iteration 65/1000 | Loss: 0.00001854
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001854
Iteration 73/1000 | Loss: 0.00001854
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001854
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001854
Iteration 83/1000 | Loss: 0.00001854
Iteration 84/1000 | Loss: 0.00001854
Iteration 85/1000 | Loss: 0.00001854
Iteration 86/1000 | Loss: 0.00001854
Iteration 87/1000 | Loss: 0.00001854
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001854
Iteration 90/1000 | Loss: 0.00001854
Iteration 91/1000 | Loss: 0.00001854
Iteration 92/1000 | Loss: 0.00001854
Iteration 93/1000 | Loss: 0.00001854
Iteration 94/1000 | Loss: 0.00001854
Iteration 95/1000 | Loss: 0.00001854
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.853868343459908e-05, 1.853868343459908e-05, 1.853868343459908e-05, 1.853868343459908e-05, 1.853868343459908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.853868343459908e-05

Optimization complete. Final v2v error: 3.620746374130249 mm

Highest mean error: 4.344099044799805 mm for frame 67

Lowest mean error: 3.3334128856658936 mm for frame 45

Saving results

Total time: 29.883025407791138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906340
Iteration 2/25 | Loss: 0.00172316
Iteration 3/25 | Loss: 0.00144835
Iteration 4/25 | Loss: 0.00142393
Iteration 5/25 | Loss: 0.00141776
Iteration 6/25 | Loss: 0.00141673
Iteration 7/25 | Loss: 0.00141673
Iteration 8/25 | Loss: 0.00141673
Iteration 9/25 | Loss: 0.00141673
Iteration 10/25 | Loss: 0.00141673
Iteration 11/25 | Loss: 0.00141673
Iteration 12/25 | Loss: 0.00141673
Iteration 13/25 | Loss: 0.00141673
Iteration 14/25 | Loss: 0.00141673
Iteration 15/25 | Loss: 0.00141673
Iteration 16/25 | Loss: 0.00141673
Iteration 17/25 | Loss: 0.00141673
Iteration 18/25 | Loss: 0.00141673
Iteration 19/25 | Loss: 0.00141673
Iteration 20/25 | Loss: 0.00141673
Iteration 21/25 | Loss: 0.00141673
Iteration 22/25 | Loss: 0.00141673
Iteration 23/25 | Loss: 0.00141673
Iteration 24/25 | Loss: 0.00141673
Iteration 25/25 | Loss: 0.00141673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23751187
Iteration 2/25 | Loss: 0.00091660
Iteration 3/25 | Loss: 0.00091659
Iteration 4/25 | Loss: 0.00091659
Iteration 5/25 | Loss: 0.00091659
Iteration 6/25 | Loss: 0.00091659
Iteration 7/25 | Loss: 0.00091659
Iteration 8/25 | Loss: 0.00091659
Iteration 9/25 | Loss: 0.00091659
Iteration 10/25 | Loss: 0.00091659
Iteration 11/25 | Loss: 0.00091659
Iteration 12/25 | Loss: 0.00091659
Iteration 13/25 | Loss: 0.00091659
Iteration 14/25 | Loss: 0.00091659
Iteration 15/25 | Loss: 0.00091659
Iteration 16/25 | Loss: 0.00091659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009165884112007916, 0.0009165884112007916, 0.0009165884112007916, 0.0009165884112007916, 0.0009165884112007916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009165884112007916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091659
Iteration 2/1000 | Loss: 0.00005453
Iteration 3/1000 | Loss: 0.00003004
Iteration 4/1000 | Loss: 0.00002565
Iteration 5/1000 | Loss: 0.00002324
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002139
Iteration 8/1000 | Loss: 0.00002079
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00001989
Iteration 11/1000 | Loss: 0.00001973
Iteration 12/1000 | Loss: 0.00001945
Iteration 13/1000 | Loss: 0.00001919
Iteration 14/1000 | Loss: 0.00001903
Iteration 15/1000 | Loss: 0.00001897
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001879
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001876
Iteration 21/1000 | Loss: 0.00001876
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001876
Iteration 25/1000 | Loss: 0.00001876
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001871
Iteration 28/1000 | Loss: 0.00001870
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001864
Iteration 33/1000 | Loss: 0.00001864
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001864
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001864
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001861
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001854
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001853
Iteration 52/1000 | Loss: 0.00001852
Iteration 53/1000 | Loss: 0.00001851
Iteration 54/1000 | Loss: 0.00001851
Iteration 55/1000 | Loss: 0.00001850
Iteration 56/1000 | Loss: 0.00001850
Iteration 57/1000 | Loss: 0.00001850
Iteration 58/1000 | Loss: 0.00001849
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001848
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001848
Iteration 64/1000 | Loss: 0.00001848
Iteration 65/1000 | Loss: 0.00001847
Iteration 66/1000 | Loss: 0.00001847
Iteration 67/1000 | Loss: 0.00001847
Iteration 68/1000 | Loss: 0.00001847
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001845
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001845
Iteration 78/1000 | Loss: 0.00001845
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001844
Iteration 82/1000 | Loss: 0.00001844
Iteration 83/1000 | Loss: 0.00001844
Iteration 84/1000 | Loss: 0.00001843
Iteration 85/1000 | Loss: 0.00001843
Iteration 86/1000 | Loss: 0.00001843
Iteration 87/1000 | Loss: 0.00001843
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001843
Iteration 90/1000 | Loss: 0.00001843
Iteration 91/1000 | Loss: 0.00001843
Iteration 92/1000 | Loss: 0.00001843
Iteration 93/1000 | Loss: 0.00001843
Iteration 94/1000 | Loss: 0.00001843
Iteration 95/1000 | Loss: 0.00001843
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001842
Iteration 98/1000 | Loss: 0.00001842
Iteration 99/1000 | Loss: 0.00001842
Iteration 100/1000 | Loss: 0.00001842
Iteration 101/1000 | Loss: 0.00001842
Iteration 102/1000 | Loss: 0.00001842
Iteration 103/1000 | Loss: 0.00001842
Iteration 104/1000 | Loss: 0.00001842
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001842
Iteration 107/1000 | Loss: 0.00001842
Iteration 108/1000 | Loss: 0.00001842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.8416050806990825e-05, 1.8416050806990825e-05, 1.8416050806990825e-05, 1.8416050806990825e-05, 1.8416050806990825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8416050806990825e-05

Optimization complete. Final v2v error: 3.686864137649536 mm

Highest mean error: 4.332383632659912 mm for frame 90

Lowest mean error: 3.316568613052368 mm for frame 60

Saving results

Total time: 43.60600996017456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514411
Iteration 2/25 | Loss: 0.00148931
Iteration 3/25 | Loss: 0.00140021
Iteration 4/25 | Loss: 0.00139249
Iteration 5/25 | Loss: 0.00139093
Iteration 6/25 | Loss: 0.00139063
Iteration 7/25 | Loss: 0.00139063
Iteration 8/25 | Loss: 0.00139063
Iteration 9/25 | Loss: 0.00139063
Iteration 10/25 | Loss: 0.00139063
Iteration 11/25 | Loss: 0.00139063
Iteration 12/25 | Loss: 0.00139063
Iteration 13/25 | Loss: 0.00139063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013906267704442143, 0.0013906267704442143, 0.0013906267704442143, 0.0013906267704442143, 0.0013906267704442143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013906267704442143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.02098083
Iteration 2/25 | Loss: 0.00088253
Iteration 3/25 | Loss: 0.00088252
Iteration 4/25 | Loss: 0.00088252
Iteration 5/25 | Loss: 0.00088252
Iteration 6/25 | Loss: 0.00088252
Iteration 7/25 | Loss: 0.00088252
Iteration 8/25 | Loss: 0.00088252
Iteration 9/25 | Loss: 0.00088251
Iteration 10/25 | Loss: 0.00088251
Iteration 11/25 | Loss: 0.00088251
Iteration 12/25 | Loss: 0.00088251
Iteration 13/25 | Loss: 0.00088251
Iteration 14/25 | Loss: 0.00088251
Iteration 15/25 | Loss: 0.00088251
Iteration 16/25 | Loss: 0.00088251
Iteration 17/25 | Loss: 0.00088251
Iteration 18/25 | Loss: 0.00088251
Iteration 19/25 | Loss: 0.00088251
Iteration 20/25 | Loss: 0.00088251
Iteration 21/25 | Loss: 0.00088251
Iteration 22/25 | Loss: 0.00088251
Iteration 23/25 | Loss: 0.00088251
Iteration 24/25 | Loss: 0.00088251
Iteration 25/25 | Loss: 0.00088251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088251
Iteration 2/1000 | Loss: 0.00004162
Iteration 3/1000 | Loss: 0.00002748
Iteration 4/1000 | Loss: 0.00002199
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001931
Iteration 7/1000 | Loss: 0.00001873
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001803
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001781
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001776
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001774
Iteration 19/1000 | Loss: 0.00001774
Iteration 20/1000 | Loss: 0.00001772
Iteration 21/1000 | Loss: 0.00001770
Iteration 22/1000 | Loss: 0.00001764
Iteration 23/1000 | Loss: 0.00001764
Iteration 24/1000 | Loss: 0.00001763
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001761
Iteration 28/1000 | Loss: 0.00001761
Iteration 29/1000 | Loss: 0.00001761
Iteration 30/1000 | Loss: 0.00001761
Iteration 31/1000 | Loss: 0.00001761
Iteration 32/1000 | Loss: 0.00001761
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001761
Iteration 35/1000 | Loss: 0.00001761
Iteration 36/1000 | Loss: 0.00001761
Iteration 37/1000 | Loss: 0.00001760
Iteration 38/1000 | Loss: 0.00001760
Iteration 39/1000 | Loss: 0.00001760
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001758
Iteration 42/1000 | Loss: 0.00001758
Iteration 43/1000 | Loss: 0.00001758
Iteration 44/1000 | Loss: 0.00001758
Iteration 45/1000 | Loss: 0.00001758
Iteration 46/1000 | Loss: 0.00001758
Iteration 47/1000 | Loss: 0.00001758
Iteration 48/1000 | Loss: 0.00001758
Iteration 49/1000 | Loss: 0.00001757
Iteration 50/1000 | Loss: 0.00001757
Iteration 51/1000 | Loss: 0.00001757
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001755
Iteration 57/1000 | Loss: 0.00001755
Iteration 58/1000 | Loss: 0.00001755
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Iteration 63/1000 | Loss: 0.00001754
Iteration 64/1000 | Loss: 0.00001753
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001753
Iteration 69/1000 | Loss: 0.00001753
Iteration 70/1000 | Loss: 0.00001752
Iteration 71/1000 | Loss: 0.00001752
Iteration 72/1000 | Loss: 0.00001752
Iteration 73/1000 | Loss: 0.00001752
Iteration 74/1000 | Loss: 0.00001752
Iteration 75/1000 | Loss: 0.00001752
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001750
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001750
Iteration 90/1000 | Loss: 0.00001750
Iteration 91/1000 | Loss: 0.00001750
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001749
Iteration 94/1000 | Loss: 0.00001749
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001749
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001748
Iteration 102/1000 | Loss: 0.00001748
Iteration 103/1000 | Loss: 0.00001748
Iteration 104/1000 | Loss: 0.00001748
Iteration 105/1000 | Loss: 0.00001748
Iteration 106/1000 | Loss: 0.00001748
Iteration 107/1000 | Loss: 0.00001748
Iteration 108/1000 | Loss: 0.00001748
Iteration 109/1000 | Loss: 0.00001748
Iteration 110/1000 | Loss: 0.00001748
Iteration 111/1000 | Loss: 0.00001748
Iteration 112/1000 | Loss: 0.00001748
Iteration 113/1000 | Loss: 0.00001748
Iteration 114/1000 | Loss: 0.00001748
Iteration 115/1000 | Loss: 0.00001748
Iteration 116/1000 | Loss: 0.00001747
Iteration 117/1000 | Loss: 0.00001747
Iteration 118/1000 | Loss: 0.00001747
Iteration 119/1000 | Loss: 0.00001747
Iteration 120/1000 | Loss: 0.00001747
Iteration 121/1000 | Loss: 0.00001747
Iteration 122/1000 | Loss: 0.00001747
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001746
Iteration 125/1000 | Loss: 0.00001746
Iteration 126/1000 | Loss: 0.00001746
Iteration 127/1000 | Loss: 0.00001746
Iteration 128/1000 | Loss: 0.00001746
Iteration 129/1000 | Loss: 0.00001746
Iteration 130/1000 | Loss: 0.00001746
Iteration 131/1000 | Loss: 0.00001746
Iteration 132/1000 | Loss: 0.00001746
Iteration 133/1000 | Loss: 0.00001746
Iteration 134/1000 | Loss: 0.00001746
Iteration 135/1000 | Loss: 0.00001746
Iteration 136/1000 | Loss: 0.00001746
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001746
Iteration 142/1000 | Loss: 0.00001746
Iteration 143/1000 | Loss: 0.00001746
Iteration 144/1000 | Loss: 0.00001746
Iteration 145/1000 | Loss: 0.00001746
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.7456319255870767e-05, 1.7456319255870767e-05, 1.7456319255870767e-05, 1.7456319255870767e-05, 1.7456319255870767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7456319255870767e-05

Optimization complete. Final v2v error: 3.6126532554626465 mm

Highest mean error: 4.244203567504883 mm for frame 44

Lowest mean error: 3.3326234817504883 mm for frame 6

Saving results

Total time: 33.257792472839355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875057
Iteration 2/25 | Loss: 0.00168768
Iteration 3/25 | Loss: 0.00143320
Iteration 4/25 | Loss: 0.00141202
Iteration 5/25 | Loss: 0.00140917
Iteration 6/25 | Loss: 0.00140900
Iteration 7/25 | Loss: 0.00140900
Iteration 8/25 | Loss: 0.00140900
Iteration 9/25 | Loss: 0.00140900
Iteration 10/25 | Loss: 0.00140900
Iteration 11/25 | Loss: 0.00140900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014090024633333087, 0.0014090024633333087, 0.0014090024633333087, 0.0014090024633333087, 0.0014090024633333087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014090024633333087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30881608
Iteration 2/25 | Loss: 0.00083149
Iteration 3/25 | Loss: 0.00083149
Iteration 4/25 | Loss: 0.00083149
Iteration 5/25 | Loss: 0.00083149
Iteration 6/25 | Loss: 0.00083149
Iteration 7/25 | Loss: 0.00083149
Iteration 8/25 | Loss: 0.00083149
Iteration 9/25 | Loss: 0.00083149
Iteration 10/25 | Loss: 0.00083149
Iteration 11/25 | Loss: 0.00083149
Iteration 12/25 | Loss: 0.00083149
Iteration 13/25 | Loss: 0.00083149
Iteration 14/25 | Loss: 0.00083149
Iteration 15/25 | Loss: 0.00083149
Iteration 16/25 | Loss: 0.00083149
Iteration 17/25 | Loss: 0.00083149
Iteration 18/25 | Loss: 0.00083149
Iteration 19/25 | Loss: 0.00083149
Iteration 20/25 | Loss: 0.00083149
Iteration 21/25 | Loss: 0.00083149
Iteration 22/25 | Loss: 0.00083149
Iteration 23/25 | Loss: 0.00083149
Iteration 24/25 | Loss: 0.00083149
Iteration 25/25 | Loss: 0.00083149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083149
Iteration 2/1000 | Loss: 0.00003428
Iteration 3/1000 | Loss: 0.00002423
Iteration 4/1000 | Loss: 0.00002159
Iteration 5/1000 | Loss: 0.00002017
Iteration 6/1000 | Loss: 0.00001943
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001830
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001773
Iteration 11/1000 | Loss: 0.00001756
Iteration 12/1000 | Loss: 0.00001747
Iteration 13/1000 | Loss: 0.00001746
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001729
Iteration 17/1000 | Loss: 0.00001728
Iteration 18/1000 | Loss: 0.00001728
Iteration 19/1000 | Loss: 0.00001727
Iteration 20/1000 | Loss: 0.00001726
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001721
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001720
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001718
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001717
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001715
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001714
Iteration 41/1000 | Loss: 0.00001714
Iteration 42/1000 | Loss: 0.00001713
Iteration 43/1000 | Loss: 0.00001713
Iteration 44/1000 | Loss: 0.00001713
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001712
Iteration 49/1000 | Loss: 0.00001712
Iteration 50/1000 | Loss: 0.00001712
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001711
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001711
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001711
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001710
Iteration 65/1000 | Loss: 0.00001710
Iteration 66/1000 | Loss: 0.00001710
Iteration 67/1000 | Loss: 0.00001710
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001709
Iteration 73/1000 | Loss: 0.00001709
Iteration 74/1000 | Loss: 0.00001709
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001706
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001705
Iteration 109/1000 | Loss: 0.00001705
Iteration 110/1000 | Loss: 0.00001705
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001704
Iteration 126/1000 | Loss: 0.00001704
Iteration 127/1000 | Loss: 0.00001704
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001704
Iteration 133/1000 | Loss: 0.00001704
Iteration 134/1000 | Loss: 0.00001704
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00001704
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001703
Iteration 140/1000 | Loss: 0.00001703
Iteration 141/1000 | Loss: 0.00001703
Iteration 142/1000 | Loss: 0.00001703
Iteration 143/1000 | Loss: 0.00001703
Iteration 144/1000 | Loss: 0.00001703
Iteration 145/1000 | Loss: 0.00001703
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001703
Iteration 148/1000 | Loss: 0.00001703
Iteration 149/1000 | Loss: 0.00001703
Iteration 150/1000 | Loss: 0.00001702
Iteration 151/1000 | Loss: 0.00001702
Iteration 152/1000 | Loss: 0.00001702
Iteration 153/1000 | Loss: 0.00001702
Iteration 154/1000 | Loss: 0.00001702
Iteration 155/1000 | Loss: 0.00001702
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001701
Iteration 160/1000 | Loss: 0.00001701
Iteration 161/1000 | Loss: 0.00001701
Iteration 162/1000 | Loss: 0.00001701
Iteration 163/1000 | Loss: 0.00001701
Iteration 164/1000 | Loss: 0.00001701
Iteration 165/1000 | Loss: 0.00001701
Iteration 166/1000 | Loss: 0.00001701
Iteration 167/1000 | Loss: 0.00001701
Iteration 168/1000 | Loss: 0.00001701
Iteration 169/1000 | Loss: 0.00001701
Iteration 170/1000 | Loss: 0.00001701
Iteration 171/1000 | Loss: 0.00001701
Iteration 172/1000 | Loss: 0.00001701
Iteration 173/1000 | Loss: 0.00001701
Iteration 174/1000 | Loss: 0.00001701
Iteration 175/1000 | Loss: 0.00001701
Iteration 176/1000 | Loss: 0.00001701
Iteration 177/1000 | Loss: 0.00001701
Iteration 178/1000 | Loss: 0.00001701
Iteration 179/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.700673055893276e-05, 1.700673055893276e-05, 1.700673055893276e-05, 1.700673055893276e-05, 1.700673055893276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.700673055893276e-05

Optimization complete. Final v2v error: 3.4998390674591064 mm

Highest mean error: 3.9061386585235596 mm for frame 119

Lowest mean error: 3.150158643722534 mm for frame 56

Saving results

Total time: 37.419129848480225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992894
Iteration 2/25 | Loss: 0.00167310
Iteration 3/25 | Loss: 0.00150525
Iteration 4/25 | Loss: 0.00148256
Iteration 5/25 | Loss: 0.00147271
Iteration 6/25 | Loss: 0.00146962
Iteration 7/25 | Loss: 0.00146832
Iteration 8/25 | Loss: 0.00146832
Iteration 9/25 | Loss: 0.00146832
Iteration 10/25 | Loss: 0.00146832
Iteration 11/25 | Loss: 0.00146832
Iteration 12/25 | Loss: 0.00146832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014683170011267066, 0.0014683170011267066, 0.0014683170011267066, 0.0014683170011267066, 0.0014683170011267066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014683170011267066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34616446
Iteration 2/25 | Loss: 0.00097730
Iteration 3/25 | Loss: 0.00097702
Iteration 4/25 | Loss: 0.00097702
Iteration 5/25 | Loss: 0.00097701
Iteration 6/25 | Loss: 0.00097701
Iteration 7/25 | Loss: 0.00097701
Iteration 8/25 | Loss: 0.00097701
Iteration 9/25 | Loss: 0.00097701
Iteration 10/25 | Loss: 0.00097701
Iteration 11/25 | Loss: 0.00097701
Iteration 12/25 | Loss: 0.00097701
Iteration 13/25 | Loss: 0.00097701
Iteration 14/25 | Loss: 0.00097701
Iteration 15/25 | Loss: 0.00097701
Iteration 16/25 | Loss: 0.00097701
Iteration 17/25 | Loss: 0.00097701
Iteration 18/25 | Loss: 0.00097701
Iteration 19/25 | Loss: 0.00097701
Iteration 20/25 | Loss: 0.00097701
Iteration 21/25 | Loss: 0.00097701
Iteration 22/25 | Loss: 0.00097701
Iteration 23/25 | Loss: 0.00097701
Iteration 24/25 | Loss: 0.00097701
Iteration 25/25 | Loss: 0.00097701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097701
Iteration 2/1000 | Loss: 0.00008222
Iteration 3/1000 | Loss: 0.00005947
Iteration 4/1000 | Loss: 0.00004866
Iteration 5/1000 | Loss: 0.00004631
Iteration 6/1000 | Loss: 0.00004440
Iteration 7/1000 | Loss: 0.00004247
Iteration 8/1000 | Loss: 0.00004091
Iteration 9/1000 | Loss: 0.00003942
Iteration 10/1000 | Loss: 0.00003812
Iteration 11/1000 | Loss: 0.00003739
Iteration 12/1000 | Loss: 0.00003685
Iteration 13/1000 | Loss: 0.00003643
Iteration 14/1000 | Loss: 0.00003613
Iteration 15/1000 | Loss: 0.00003596
Iteration 16/1000 | Loss: 0.00003583
Iteration 17/1000 | Loss: 0.00003574
Iteration 18/1000 | Loss: 0.00003559
Iteration 19/1000 | Loss: 0.00003558
Iteration 20/1000 | Loss: 0.00003557
Iteration 21/1000 | Loss: 0.00003556
Iteration 22/1000 | Loss: 0.00003555
Iteration 23/1000 | Loss: 0.00003554
Iteration 24/1000 | Loss: 0.00003550
Iteration 25/1000 | Loss: 0.00003548
Iteration 26/1000 | Loss: 0.00003547
Iteration 27/1000 | Loss: 0.00003547
Iteration 28/1000 | Loss: 0.00003547
Iteration 29/1000 | Loss: 0.00003546
Iteration 30/1000 | Loss: 0.00003546
Iteration 31/1000 | Loss: 0.00003545
Iteration 32/1000 | Loss: 0.00003545
Iteration 33/1000 | Loss: 0.00003544
Iteration 34/1000 | Loss: 0.00003543
Iteration 35/1000 | Loss: 0.00003542
Iteration 36/1000 | Loss: 0.00003541
Iteration 37/1000 | Loss: 0.00003541
Iteration 38/1000 | Loss: 0.00003540
Iteration 39/1000 | Loss: 0.00003540
Iteration 40/1000 | Loss: 0.00003540
Iteration 41/1000 | Loss: 0.00003539
Iteration 42/1000 | Loss: 0.00003539
Iteration 43/1000 | Loss: 0.00003538
Iteration 44/1000 | Loss: 0.00003537
Iteration 45/1000 | Loss: 0.00003537
Iteration 46/1000 | Loss: 0.00003536
Iteration 47/1000 | Loss: 0.00003536
Iteration 48/1000 | Loss: 0.00003536
Iteration 49/1000 | Loss: 0.00003535
Iteration 50/1000 | Loss: 0.00003535
Iteration 51/1000 | Loss: 0.00003534
Iteration 52/1000 | Loss: 0.00003534
Iteration 53/1000 | Loss: 0.00003534
Iteration 54/1000 | Loss: 0.00003532
Iteration 55/1000 | Loss: 0.00003532
Iteration 56/1000 | Loss: 0.00003527
Iteration 57/1000 | Loss: 0.00003526
Iteration 58/1000 | Loss: 0.00003526
Iteration 59/1000 | Loss: 0.00003526
Iteration 60/1000 | Loss: 0.00003525
Iteration 61/1000 | Loss: 0.00003525
Iteration 62/1000 | Loss: 0.00003525
Iteration 63/1000 | Loss: 0.00003524
Iteration 64/1000 | Loss: 0.00003523
Iteration 65/1000 | Loss: 0.00003522
Iteration 66/1000 | Loss: 0.00003521
Iteration 67/1000 | Loss: 0.00003521
Iteration 68/1000 | Loss: 0.00003521
Iteration 69/1000 | Loss: 0.00003521
Iteration 70/1000 | Loss: 0.00003521
Iteration 71/1000 | Loss: 0.00003521
Iteration 72/1000 | Loss: 0.00003521
Iteration 73/1000 | Loss: 0.00003521
Iteration 74/1000 | Loss: 0.00003521
Iteration 75/1000 | Loss: 0.00003520
Iteration 76/1000 | Loss: 0.00003520
Iteration 77/1000 | Loss: 0.00003520
Iteration 78/1000 | Loss: 0.00003520
Iteration 79/1000 | Loss: 0.00003520
Iteration 80/1000 | Loss: 0.00003518
Iteration 81/1000 | Loss: 0.00003518
Iteration 82/1000 | Loss: 0.00003516
Iteration 83/1000 | Loss: 0.00003515
Iteration 84/1000 | Loss: 0.00003515
Iteration 85/1000 | Loss: 0.00003515
Iteration 86/1000 | Loss: 0.00003514
Iteration 87/1000 | Loss: 0.00003511
Iteration 88/1000 | Loss: 0.00003511
Iteration 89/1000 | Loss: 0.00003510
Iteration 90/1000 | Loss: 0.00003510
Iteration 91/1000 | Loss: 0.00003509
Iteration 92/1000 | Loss: 0.00003509
Iteration 93/1000 | Loss: 0.00003509
Iteration 94/1000 | Loss: 0.00003508
Iteration 95/1000 | Loss: 0.00003507
Iteration 96/1000 | Loss: 0.00003507
Iteration 97/1000 | Loss: 0.00003507
Iteration 98/1000 | Loss: 0.00003507
Iteration 99/1000 | Loss: 0.00003507
Iteration 100/1000 | Loss: 0.00003507
Iteration 101/1000 | Loss: 0.00003507
Iteration 102/1000 | Loss: 0.00003507
Iteration 103/1000 | Loss: 0.00003506
Iteration 104/1000 | Loss: 0.00003506
Iteration 105/1000 | Loss: 0.00003505
Iteration 106/1000 | Loss: 0.00003505
Iteration 107/1000 | Loss: 0.00003505
Iteration 108/1000 | Loss: 0.00003505
Iteration 109/1000 | Loss: 0.00003505
Iteration 110/1000 | Loss: 0.00003505
Iteration 111/1000 | Loss: 0.00003504
Iteration 112/1000 | Loss: 0.00003504
Iteration 113/1000 | Loss: 0.00003504
Iteration 114/1000 | Loss: 0.00003503
Iteration 115/1000 | Loss: 0.00003503
Iteration 116/1000 | Loss: 0.00003503
Iteration 117/1000 | Loss: 0.00003503
Iteration 118/1000 | Loss: 0.00003503
Iteration 119/1000 | Loss: 0.00003503
Iteration 120/1000 | Loss: 0.00003503
Iteration 121/1000 | Loss: 0.00003503
Iteration 122/1000 | Loss: 0.00003503
Iteration 123/1000 | Loss: 0.00003502
Iteration 124/1000 | Loss: 0.00003501
Iteration 125/1000 | Loss: 0.00003501
Iteration 126/1000 | Loss: 0.00003501
Iteration 127/1000 | Loss: 0.00003501
Iteration 128/1000 | Loss: 0.00003501
Iteration 129/1000 | Loss: 0.00003501
Iteration 130/1000 | Loss: 0.00003501
Iteration 131/1000 | Loss: 0.00003500
Iteration 132/1000 | Loss: 0.00003500
Iteration 133/1000 | Loss: 0.00003500
Iteration 134/1000 | Loss: 0.00003500
Iteration 135/1000 | Loss: 0.00003499
Iteration 136/1000 | Loss: 0.00003499
Iteration 137/1000 | Loss: 0.00003499
Iteration 138/1000 | Loss: 0.00003499
Iteration 139/1000 | Loss: 0.00003498
Iteration 140/1000 | Loss: 0.00003498
Iteration 141/1000 | Loss: 0.00003498
Iteration 142/1000 | Loss: 0.00003498
Iteration 143/1000 | Loss: 0.00003498
Iteration 144/1000 | Loss: 0.00003498
Iteration 145/1000 | Loss: 0.00003498
Iteration 146/1000 | Loss: 0.00003498
Iteration 147/1000 | Loss: 0.00003497
Iteration 148/1000 | Loss: 0.00003497
Iteration 149/1000 | Loss: 0.00003497
Iteration 150/1000 | Loss: 0.00003497
Iteration 151/1000 | Loss: 0.00003497
Iteration 152/1000 | Loss: 0.00003497
Iteration 153/1000 | Loss: 0.00003497
Iteration 154/1000 | Loss: 0.00003497
Iteration 155/1000 | Loss: 0.00003497
Iteration 156/1000 | Loss: 0.00003497
Iteration 157/1000 | Loss: 0.00003496
Iteration 158/1000 | Loss: 0.00003496
Iteration 159/1000 | Loss: 0.00003496
Iteration 160/1000 | Loss: 0.00003496
Iteration 161/1000 | Loss: 0.00003496
Iteration 162/1000 | Loss: 0.00003496
Iteration 163/1000 | Loss: 0.00003496
Iteration 164/1000 | Loss: 0.00003496
Iteration 165/1000 | Loss: 0.00003496
Iteration 166/1000 | Loss: 0.00003496
Iteration 167/1000 | Loss: 0.00003496
Iteration 168/1000 | Loss: 0.00003496
Iteration 169/1000 | Loss: 0.00003496
Iteration 170/1000 | Loss: 0.00003495
Iteration 171/1000 | Loss: 0.00003495
Iteration 172/1000 | Loss: 0.00003495
Iteration 173/1000 | Loss: 0.00003495
Iteration 174/1000 | Loss: 0.00003495
Iteration 175/1000 | Loss: 0.00003495
Iteration 176/1000 | Loss: 0.00003495
Iteration 177/1000 | Loss: 0.00003495
Iteration 178/1000 | Loss: 0.00003495
Iteration 179/1000 | Loss: 0.00003495
Iteration 180/1000 | Loss: 0.00003495
Iteration 181/1000 | Loss: 0.00003495
Iteration 182/1000 | Loss: 0.00003495
Iteration 183/1000 | Loss: 0.00003495
Iteration 184/1000 | Loss: 0.00003495
Iteration 185/1000 | Loss: 0.00003495
Iteration 186/1000 | Loss: 0.00003495
Iteration 187/1000 | Loss: 0.00003495
Iteration 188/1000 | Loss: 0.00003495
Iteration 189/1000 | Loss: 0.00003495
Iteration 190/1000 | Loss: 0.00003495
Iteration 191/1000 | Loss: 0.00003495
Iteration 192/1000 | Loss: 0.00003494
Iteration 193/1000 | Loss: 0.00003494
Iteration 194/1000 | Loss: 0.00003494
Iteration 195/1000 | Loss: 0.00003494
Iteration 196/1000 | Loss: 0.00003494
Iteration 197/1000 | Loss: 0.00003494
Iteration 198/1000 | Loss: 0.00003494
Iteration 199/1000 | Loss: 0.00003494
Iteration 200/1000 | Loss: 0.00003494
Iteration 201/1000 | Loss: 0.00003494
Iteration 202/1000 | Loss: 0.00003494
Iteration 203/1000 | Loss: 0.00003494
Iteration 204/1000 | Loss: 0.00003493
Iteration 205/1000 | Loss: 0.00003493
Iteration 206/1000 | Loss: 0.00003493
Iteration 207/1000 | Loss: 0.00003493
Iteration 208/1000 | Loss: 0.00003493
Iteration 209/1000 | Loss: 0.00003493
Iteration 210/1000 | Loss: 0.00003493
Iteration 211/1000 | Loss: 0.00003493
Iteration 212/1000 | Loss: 0.00003493
Iteration 213/1000 | Loss: 0.00003493
Iteration 214/1000 | Loss: 0.00003493
Iteration 215/1000 | Loss: 0.00003493
Iteration 216/1000 | Loss: 0.00003493
Iteration 217/1000 | Loss: 0.00003493
Iteration 218/1000 | Loss: 0.00003493
Iteration 219/1000 | Loss: 0.00003493
Iteration 220/1000 | Loss: 0.00003493
Iteration 221/1000 | Loss: 0.00003493
Iteration 222/1000 | Loss: 0.00003493
Iteration 223/1000 | Loss: 0.00003493
Iteration 224/1000 | Loss: 0.00003493
Iteration 225/1000 | Loss: 0.00003492
Iteration 226/1000 | Loss: 0.00003492
Iteration 227/1000 | Loss: 0.00003492
Iteration 228/1000 | Loss: 0.00003492
Iteration 229/1000 | Loss: 0.00003492
Iteration 230/1000 | Loss: 0.00003492
Iteration 231/1000 | Loss: 0.00003492
Iteration 232/1000 | Loss: 0.00003492
Iteration 233/1000 | Loss: 0.00003492
Iteration 234/1000 | Loss: 0.00003492
Iteration 235/1000 | Loss: 0.00003492
Iteration 236/1000 | Loss: 0.00003492
Iteration 237/1000 | Loss: 0.00003492
Iteration 238/1000 | Loss: 0.00003492
Iteration 239/1000 | Loss: 0.00003492
Iteration 240/1000 | Loss: 0.00003492
Iteration 241/1000 | Loss: 0.00003492
Iteration 242/1000 | Loss: 0.00003492
Iteration 243/1000 | Loss: 0.00003492
Iteration 244/1000 | Loss: 0.00003492
Iteration 245/1000 | Loss: 0.00003492
Iteration 246/1000 | Loss: 0.00003492
Iteration 247/1000 | Loss: 0.00003492
Iteration 248/1000 | Loss: 0.00003492
Iteration 249/1000 | Loss: 0.00003492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [3.491729512461461e-05, 3.491729512461461e-05, 3.491729512461461e-05, 3.491729512461461e-05, 3.491729512461461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.491729512461461e-05

Optimization complete. Final v2v error: 4.707877159118652 mm

Highest mean error: 6.590395927429199 mm for frame 86

Lowest mean error: 3.670964241027832 mm for frame 147

Saving results

Total time: 53.61808967590332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925231
Iteration 2/25 | Loss: 0.00163259
Iteration 3/25 | Loss: 0.00143882
Iteration 4/25 | Loss: 0.00138215
Iteration 5/25 | Loss: 0.00137690
Iteration 6/25 | Loss: 0.00137062
Iteration 7/25 | Loss: 0.00136913
Iteration 8/25 | Loss: 0.00136871
Iteration 9/25 | Loss: 0.00136860
Iteration 10/25 | Loss: 0.00136859
Iteration 11/25 | Loss: 0.00136858
Iteration 12/25 | Loss: 0.00136857
Iteration 13/25 | Loss: 0.00136856
Iteration 14/25 | Loss: 0.00136856
Iteration 15/25 | Loss: 0.00136856
Iteration 16/25 | Loss: 0.00136856
Iteration 17/25 | Loss: 0.00136856
Iteration 18/25 | Loss: 0.00136856
Iteration 19/25 | Loss: 0.00136856
Iteration 20/25 | Loss: 0.00136856
Iteration 21/25 | Loss: 0.00136856
Iteration 22/25 | Loss: 0.00136856
Iteration 23/25 | Loss: 0.00136856
Iteration 24/25 | Loss: 0.00136856
Iteration 25/25 | Loss: 0.00136856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68840575
Iteration 2/25 | Loss: 0.00098024
Iteration 3/25 | Loss: 0.00095483
Iteration 4/25 | Loss: 0.00095483
Iteration 5/25 | Loss: 0.00095483
Iteration 6/25 | Loss: 0.00095483
Iteration 7/25 | Loss: 0.00095483
Iteration 8/25 | Loss: 0.00095483
Iteration 9/25 | Loss: 0.00095483
Iteration 10/25 | Loss: 0.00095483
Iteration 11/25 | Loss: 0.00095483
Iteration 12/25 | Loss: 0.00095483
Iteration 13/25 | Loss: 0.00095483
Iteration 14/25 | Loss: 0.00095483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009548266534693539, 0.0009548266534693539, 0.0009548266534693539, 0.0009548266534693539, 0.0009548266534693539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009548266534693539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095483
Iteration 2/1000 | Loss: 0.00003142
Iteration 3/1000 | Loss: 0.00002221
Iteration 4/1000 | Loss: 0.00002836
Iteration 5/1000 | Loss: 0.00002011
Iteration 6/1000 | Loss: 0.00001878
Iteration 7/1000 | Loss: 0.00001864
Iteration 8/1000 | Loss: 0.00001823
Iteration 9/1000 | Loss: 0.00003042
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001771
Iteration 12/1000 | Loss: 0.00001937
Iteration 13/1000 | Loss: 0.00001773
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001771
Iteration 16/1000 | Loss: 0.00001769
Iteration 17/1000 | Loss: 0.00001768
Iteration 18/1000 | Loss: 0.00001767
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001763
Iteration 21/1000 | Loss: 0.00001763
Iteration 22/1000 | Loss: 0.00001762
Iteration 23/1000 | Loss: 0.00001762
Iteration 24/1000 | Loss: 0.00001759
Iteration 25/1000 | Loss: 0.00001759
Iteration 26/1000 | Loss: 0.00001758
Iteration 27/1000 | Loss: 0.00001758
Iteration 28/1000 | Loss: 0.00001757
Iteration 29/1000 | Loss: 0.00001757
Iteration 30/1000 | Loss: 0.00001757
Iteration 31/1000 | Loss: 0.00001756
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001742
Iteration 35/1000 | Loss: 0.00001741
Iteration 36/1000 | Loss: 0.00001735
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001730
Iteration 39/1000 | Loss: 0.00001730
Iteration 40/1000 | Loss: 0.00001730
Iteration 41/1000 | Loss: 0.00001730
Iteration 42/1000 | Loss: 0.00001730
Iteration 43/1000 | Loss: 0.00001730
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001729
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001725
Iteration 58/1000 | Loss: 0.00001725
Iteration 59/1000 | Loss: 0.00001725
Iteration 60/1000 | Loss: 0.00001725
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001722
Iteration 64/1000 | Loss: 0.00001722
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001722
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001722
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001721
Iteration 73/1000 | Loss: 0.00001721
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001719
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001719
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001718
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001718
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001716
Iteration 92/1000 | Loss: 0.00001716
Iteration 93/1000 | Loss: 0.00001716
Iteration 94/1000 | Loss: 0.00001715
Iteration 95/1000 | Loss: 0.00001715
Iteration 96/1000 | Loss: 0.00001715
Iteration 97/1000 | Loss: 0.00001715
Iteration 98/1000 | Loss: 0.00001715
Iteration 99/1000 | Loss: 0.00001715
Iteration 100/1000 | Loss: 0.00001715
Iteration 101/1000 | Loss: 0.00001715
Iteration 102/1000 | Loss: 0.00001715
Iteration 103/1000 | Loss: 0.00001715
Iteration 104/1000 | Loss: 0.00001715
Iteration 105/1000 | Loss: 0.00001715
Iteration 106/1000 | Loss: 0.00001715
Iteration 107/1000 | Loss: 0.00001715
Iteration 108/1000 | Loss: 0.00001715
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001714
Iteration 111/1000 | Loss: 0.00001714
Iteration 112/1000 | Loss: 0.00001714
Iteration 113/1000 | Loss: 0.00001714
Iteration 114/1000 | Loss: 0.00001714
Iteration 115/1000 | Loss: 0.00001714
Iteration 116/1000 | Loss: 0.00001714
Iteration 117/1000 | Loss: 0.00001714
Iteration 118/1000 | Loss: 0.00001714
Iteration 119/1000 | Loss: 0.00001713
Iteration 120/1000 | Loss: 0.00001713
Iteration 121/1000 | Loss: 0.00001713
Iteration 122/1000 | Loss: 0.00001713
Iteration 123/1000 | Loss: 0.00001713
Iteration 124/1000 | Loss: 0.00001713
Iteration 125/1000 | Loss: 0.00001713
Iteration 126/1000 | Loss: 0.00001713
Iteration 127/1000 | Loss: 0.00001713
Iteration 128/1000 | Loss: 0.00001713
Iteration 129/1000 | Loss: 0.00001713
Iteration 130/1000 | Loss: 0.00001713
Iteration 131/1000 | Loss: 0.00001712
Iteration 132/1000 | Loss: 0.00001712
Iteration 133/1000 | Loss: 0.00001712
Iteration 134/1000 | Loss: 0.00001712
Iteration 135/1000 | Loss: 0.00001712
Iteration 136/1000 | Loss: 0.00001712
Iteration 137/1000 | Loss: 0.00001711
Iteration 138/1000 | Loss: 0.00001711
Iteration 139/1000 | Loss: 0.00001711
Iteration 140/1000 | Loss: 0.00001711
Iteration 141/1000 | Loss: 0.00001711
Iteration 142/1000 | Loss: 0.00001711
Iteration 143/1000 | Loss: 0.00001710
Iteration 144/1000 | Loss: 0.00001710
Iteration 145/1000 | Loss: 0.00001710
Iteration 146/1000 | Loss: 0.00001710
Iteration 147/1000 | Loss: 0.00001710
Iteration 148/1000 | Loss: 0.00001710
Iteration 149/1000 | Loss: 0.00001710
Iteration 150/1000 | Loss: 0.00001710
Iteration 151/1000 | Loss: 0.00001710
Iteration 152/1000 | Loss: 0.00001710
Iteration 153/1000 | Loss: 0.00001710
Iteration 154/1000 | Loss: 0.00001710
Iteration 155/1000 | Loss: 0.00001710
Iteration 156/1000 | Loss: 0.00001710
Iteration 157/1000 | Loss: 0.00001710
Iteration 158/1000 | Loss: 0.00001709
Iteration 159/1000 | Loss: 0.00001709
Iteration 160/1000 | Loss: 0.00001709
Iteration 161/1000 | Loss: 0.00001709
Iteration 162/1000 | Loss: 0.00001709
Iteration 163/1000 | Loss: 0.00001709
Iteration 164/1000 | Loss: 0.00001709
Iteration 165/1000 | Loss: 0.00001709
Iteration 166/1000 | Loss: 0.00001709
Iteration 167/1000 | Loss: 0.00001709
Iteration 168/1000 | Loss: 0.00001709
Iteration 169/1000 | Loss: 0.00001709
Iteration 170/1000 | Loss: 0.00001709
Iteration 171/1000 | Loss: 0.00001709
Iteration 172/1000 | Loss: 0.00001709
Iteration 173/1000 | Loss: 0.00001709
Iteration 174/1000 | Loss: 0.00001709
Iteration 175/1000 | Loss: 0.00001709
Iteration 176/1000 | Loss: 0.00001709
Iteration 177/1000 | Loss: 0.00001709
Iteration 178/1000 | Loss: 0.00001709
Iteration 179/1000 | Loss: 0.00001709
Iteration 180/1000 | Loss: 0.00001709
Iteration 181/1000 | Loss: 0.00001709
Iteration 182/1000 | Loss: 0.00001709
Iteration 183/1000 | Loss: 0.00001709
Iteration 184/1000 | Loss: 0.00001709
Iteration 185/1000 | Loss: 0.00001709
Iteration 186/1000 | Loss: 0.00001709
Iteration 187/1000 | Loss: 0.00001709
Iteration 188/1000 | Loss: 0.00001709
Iteration 189/1000 | Loss: 0.00001709
Iteration 190/1000 | Loss: 0.00001709
Iteration 191/1000 | Loss: 0.00001709
Iteration 192/1000 | Loss: 0.00001709
Iteration 193/1000 | Loss: 0.00001709
Iteration 194/1000 | Loss: 0.00001709
Iteration 195/1000 | Loss: 0.00001709
Iteration 196/1000 | Loss: 0.00001709
Iteration 197/1000 | Loss: 0.00001709
Iteration 198/1000 | Loss: 0.00001709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.7090998881030828e-05, 1.7090998881030828e-05, 1.7090998881030828e-05, 1.7090998881030828e-05, 1.7090998881030828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7090998881030828e-05

Optimization complete. Final v2v error: 3.5333917140960693 mm

Highest mean error: 3.851041555404663 mm for frame 211

Lowest mean error: 3.2816262245178223 mm for frame 90

Saving results

Total time: 51.335575103759766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926415
Iteration 2/25 | Loss: 0.00157371
Iteration 3/25 | Loss: 0.00139824
Iteration 4/25 | Loss: 0.00138287
Iteration 5/25 | Loss: 0.00137935
Iteration 6/25 | Loss: 0.00137872
Iteration 7/25 | Loss: 0.00137872
Iteration 8/25 | Loss: 0.00137872
Iteration 9/25 | Loss: 0.00137872
Iteration 10/25 | Loss: 0.00137872
Iteration 11/25 | Loss: 0.00137872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013787231873720884, 0.0013787231873720884, 0.0013787231873720884, 0.0013787231873720884, 0.0013787231873720884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013787231873720884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.78619051
Iteration 2/25 | Loss: 0.00093185
Iteration 3/25 | Loss: 0.00093184
Iteration 4/25 | Loss: 0.00093184
Iteration 5/25 | Loss: 0.00093184
Iteration 6/25 | Loss: 0.00093184
Iteration 7/25 | Loss: 0.00093184
Iteration 8/25 | Loss: 0.00093184
Iteration 9/25 | Loss: 0.00093184
Iteration 10/25 | Loss: 0.00093184
Iteration 11/25 | Loss: 0.00093184
Iteration 12/25 | Loss: 0.00093184
Iteration 13/25 | Loss: 0.00093184
Iteration 14/25 | Loss: 0.00093184
Iteration 15/25 | Loss: 0.00093184
Iteration 16/25 | Loss: 0.00093184
Iteration 17/25 | Loss: 0.00093184
Iteration 18/25 | Loss: 0.00093184
Iteration 19/25 | Loss: 0.00093184
Iteration 20/25 | Loss: 0.00093184
Iteration 21/25 | Loss: 0.00093184
Iteration 22/25 | Loss: 0.00093184
Iteration 23/25 | Loss: 0.00093184
Iteration 24/25 | Loss: 0.00093184
Iteration 25/25 | Loss: 0.00093184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093184
Iteration 2/1000 | Loss: 0.00003663
Iteration 3/1000 | Loss: 0.00002269
Iteration 4/1000 | Loss: 0.00002081
Iteration 5/1000 | Loss: 0.00001955
Iteration 6/1000 | Loss: 0.00001908
Iteration 7/1000 | Loss: 0.00001883
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001844
Iteration 11/1000 | Loss: 0.00001843
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001840
Iteration 15/1000 | Loss: 0.00001839
Iteration 16/1000 | Loss: 0.00001835
Iteration 17/1000 | Loss: 0.00001835
Iteration 18/1000 | Loss: 0.00001834
Iteration 19/1000 | Loss: 0.00001833
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001830
Iteration 23/1000 | Loss: 0.00001829
Iteration 24/1000 | Loss: 0.00001829
Iteration 25/1000 | Loss: 0.00001829
Iteration 26/1000 | Loss: 0.00001826
Iteration 27/1000 | Loss: 0.00001825
Iteration 28/1000 | Loss: 0.00001825
Iteration 29/1000 | Loss: 0.00001824
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001821
Iteration 32/1000 | Loss: 0.00001820
Iteration 33/1000 | Loss: 0.00001820
Iteration 34/1000 | Loss: 0.00001818
Iteration 35/1000 | Loss: 0.00001818
Iteration 36/1000 | Loss: 0.00001818
Iteration 37/1000 | Loss: 0.00001818
Iteration 38/1000 | Loss: 0.00001817
Iteration 39/1000 | Loss: 0.00001817
Iteration 40/1000 | Loss: 0.00001816
Iteration 41/1000 | Loss: 0.00001816
Iteration 42/1000 | Loss: 0.00001815
Iteration 43/1000 | Loss: 0.00001815
Iteration 44/1000 | Loss: 0.00001815
Iteration 45/1000 | Loss: 0.00001815
Iteration 46/1000 | Loss: 0.00001815
Iteration 47/1000 | Loss: 0.00001815
Iteration 48/1000 | Loss: 0.00001814
Iteration 49/1000 | Loss: 0.00001814
Iteration 50/1000 | Loss: 0.00001814
Iteration 51/1000 | Loss: 0.00001814
Iteration 52/1000 | Loss: 0.00001814
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001813
Iteration 55/1000 | Loss: 0.00001813
Iteration 56/1000 | Loss: 0.00001813
Iteration 57/1000 | Loss: 0.00001813
Iteration 58/1000 | Loss: 0.00001813
Iteration 59/1000 | Loss: 0.00001813
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001812
Iteration 62/1000 | Loss: 0.00001812
Iteration 63/1000 | Loss: 0.00001812
Iteration 64/1000 | Loss: 0.00001812
Iteration 65/1000 | Loss: 0.00001812
Iteration 66/1000 | Loss: 0.00001812
Iteration 67/1000 | Loss: 0.00001812
Iteration 68/1000 | Loss: 0.00001812
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001812
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001811
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001810
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001809
Iteration 79/1000 | Loss: 0.00001809
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001807
Iteration 87/1000 | Loss: 0.00001807
Iteration 88/1000 | Loss: 0.00001807
Iteration 89/1000 | Loss: 0.00001807
Iteration 90/1000 | Loss: 0.00001807
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001806
Iteration 94/1000 | Loss: 0.00001806
Iteration 95/1000 | Loss: 0.00001806
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001805
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001805
Iteration 102/1000 | Loss: 0.00001805
Iteration 103/1000 | Loss: 0.00001804
Iteration 104/1000 | Loss: 0.00001804
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00001804
Iteration 107/1000 | Loss: 0.00001803
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001803
Iteration 111/1000 | Loss: 0.00001803
Iteration 112/1000 | Loss: 0.00001803
Iteration 113/1000 | Loss: 0.00001803
Iteration 114/1000 | Loss: 0.00001802
Iteration 115/1000 | Loss: 0.00001802
Iteration 116/1000 | Loss: 0.00001802
Iteration 117/1000 | Loss: 0.00001802
Iteration 118/1000 | Loss: 0.00001802
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001802
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001800
Iteration 130/1000 | Loss: 0.00001800
Iteration 131/1000 | Loss: 0.00001800
Iteration 132/1000 | Loss: 0.00001800
Iteration 133/1000 | Loss: 0.00001800
Iteration 134/1000 | Loss: 0.00001800
Iteration 135/1000 | Loss: 0.00001800
Iteration 136/1000 | Loss: 0.00001800
Iteration 137/1000 | Loss: 0.00001800
Iteration 138/1000 | Loss: 0.00001800
Iteration 139/1000 | Loss: 0.00001800
Iteration 140/1000 | Loss: 0.00001800
Iteration 141/1000 | Loss: 0.00001800
Iteration 142/1000 | Loss: 0.00001800
Iteration 143/1000 | Loss: 0.00001800
Iteration 144/1000 | Loss: 0.00001800
Iteration 145/1000 | Loss: 0.00001799
Iteration 146/1000 | Loss: 0.00001799
Iteration 147/1000 | Loss: 0.00001799
Iteration 148/1000 | Loss: 0.00001799
Iteration 149/1000 | Loss: 0.00001799
Iteration 150/1000 | Loss: 0.00001799
Iteration 151/1000 | Loss: 0.00001798
Iteration 152/1000 | Loss: 0.00001798
Iteration 153/1000 | Loss: 0.00001798
Iteration 154/1000 | Loss: 0.00001798
Iteration 155/1000 | Loss: 0.00001798
Iteration 156/1000 | Loss: 0.00001798
Iteration 157/1000 | Loss: 0.00001798
Iteration 158/1000 | Loss: 0.00001798
Iteration 159/1000 | Loss: 0.00001798
Iteration 160/1000 | Loss: 0.00001798
Iteration 161/1000 | Loss: 0.00001798
Iteration 162/1000 | Loss: 0.00001798
Iteration 163/1000 | Loss: 0.00001798
Iteration 164/1000 | Loss: 0.00001798
Iteration 165/1000 | Loss: 0.00001798
Iteration 166/1000 | Loss: 0.00001798
Iteration 167/1000 | Loss: 0.00001798
Iteration 168/1000 | Loss: 0.00001798
Iteration 169/1000 | Loss: 0.00001798
Iteration 170/1000 | Loss: 0.00001798
Iteration 171/1000 | Loss: 0.00001798
Iteration 172/1000 | Loss: 0.00001798
Iteration 173/1000 | Loss: 0.00001798
Iteration 174/1000 | Loss: 0.00001798
Iteration 175/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.7976830349653028e-05, 1.7976830349653028e-05, 1.7976830349653028e-05, 1.7976830349653028e-05, 1.7976830349653028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7976830349653028e-05

Optimization complete. Final v2v error: 3.60178804397583 mm

Highest mean error: 4.114991188049316 mm for frame 170

Lowest mean error: 3.179508924484253 mm for frame 0

Saving results

Total time: 39.66695547103882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414339
Iteration 2/25 | Loss: 0.00144669
Iteration 3/25 | Loss: 0.00136105
Iteration 4/25 | Loss: 0.00135439
Iteration 5/25 | Loss: 0.00135253
Iteration 6/25 | Loss: 0.00135205
Iteration 7/25 | Loss: 0.00135205
Iteration 8/25 | Loss: 0.00135205
Iteration 9/25 | Loss: 0.00135205
Iteration 10/25 | Loss: 0.00135205
Iteration 11/25 | Loss: 0.00135205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001352054299786687, 0.001352054299786687, 0.001352054299786687, 0.001352054299786687, 0.001352054299786687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001352054299786687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40318012
Iteration 2/25 | Loss: 0.00090240
Iteration 3/25 | Loss: 0.00090240
Iteration 4/25 | Loss: 0.00090240
Iteration 5/25 | Loss: 0.00090240
Iteration 6/25 | Loss: 0.00090240
Iteration 7/25 | Loss: 0.00090240
Iteration 8/25 | Loss: 0.00090240
Iteration 9/25 | Loss: 0.00090240
Iteration 10/25 | Loss: 0.00090240
Iteration 11/25 | Loss: 0.00090240
Iteration 12/25 | Loss: 0.00090240
Iteration 13/25 | Loss: 0.00090240
Iteration 14/25 | Loss: 0.00090240
Iteration 15/25 | Loss: 0.00090240
Iteration 16/25 | Loss: 0.00090240
Iteration 17/25 | Loss: 0.00090240
Iteration 18/25 | Loss: 0.00090240
Iteration 19/25 | Loss: 0.00090240
Iteration 20/25 | Loss: 0.00090240
Iteration 21/25 | Loss: 0.00090240
Iteration 22/25 | Loss: 0.00090240
Iteration 23/25 | Loss: 0.00090240
Iteration 24/25 | Loss: 0.00090240
Iteration 25/25 | Loss: 0.00090240

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090240
Iteration 2/1000 | Loss: 0.00003094
Iteration 3/1000 | Loss: 0.00001954
Iteration 4/1000 | Loss: 0.00001822
Iteration 5/1000 | Loss: 0.00001713
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001626
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001578
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001576
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001562
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001559
Iteration 18/1000 | Loss: 0.00001558
Iteration 19/1000 | Loss: 0.00001558
Iteration 20/1000 | Loss: 0.00001558
Iteration 21/1000 | Loss: 0.00001557
Iteration 22/1000 | Loss: 0.00001556
Iteration 23/1000 | Loss: 0.00001556
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001550
Iteration 26/1000 | Loss: 0.00001550
Iteration 27/1000 | Loss: 0.00001550
Iteration 28/1000 | Loss: 0.00001550
Iteration 29/1000 | Loss: 0.00001545
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001544
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001544
Iteration 35/1000 | Loss: 0.00001543
Iteration 36/1000 | Loss: 0.00001543
Iteration 37/1000 | Loss: 0.00001543
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001539
Iteration 44/1000 | Loss: 0.00001539
Iteration 45/1000 | Loss: 0.00001539
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001536
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001536
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001536
Iteration 61/1000 | Loss: 0.00001536
Iteration 62/1000 | Loss: 0.00001535
Iteration 63/1000 | Loss: 0.00001535
Iteration 64/1000 | Loss: 0.00001535
Iteration 65/1000 | Loss: 0.00001535
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001535
Iteration 69/1000 | Loss: 0.00001535
Iteration 70/1000 | Loss: 0.00001535
Iteration 71/1000 | Loss: 0.00001535
Iteration 72/1000 | Loss: 0.00001535
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001533
Iteration 77/1000 | Loss: 0.00001533
Iteration 78/1000 | Loss: 0.00001533
Iteration 79/1000 | Loss: 0.00001533
Iteration 80/1000 | Loss: 0.00001533
Iteration 81/1000 | Loss: 0.00001533
Iteration 82/1000 | Loss: 0.00001533
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001533
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001531
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001530
Iteration 109/1000 | Loss: 0.00001530
Iteration 110/1000 | Loss: 0.00001530
Iteration 111/1000 | Loss: 0.00001530
Iteration 112/1000 | Loss: 0.00001530
Iteration 113/1000 | Loss: 0.00001530
Iteration 114/1000 | Loss: 0.00001530
Iteration 115/1000 | Loss: 0.00001530
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001530
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001529
Iteration 123/1000 | Loss: 0.00001529
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00001529
Iteration 126/1000 | Loss: 0.00001529
Iteration 127/1000 | Loss: 0.00001529
Iteration 128/1000 | Loss: 0.00001529
Iteration 129/1000 | Loss: 0.00001529
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001528
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001527
Iteration 144/1000 | Loss: 0.00001527
Iteration 145/1000 | Loss: 0.00001527
Iteration 146/1000 | Loss: 0.00001527
Iteration 147/1000 | Loss: 0.00001527
Iteration 148/1000 | Loss: 0.00001527
Iteration 149/1000 | Loss: 0.00001527
Iteration 150/1000 | Loss: 0.00001526
Iteration 151/1000 | Loss: 0.00001526
Iteration 152/1000 | Loss: 0.00001526
Iteration 153/1000 | Loss: 0.00001526
Iteration 154/1000 | Loss: 0.00001526
Iteration 155/1000 | Loss: 0.00001526
Iteration 156/1000 | Loss: 0.00001526
Iteration 157/1000 | Loss: 0.00001526
Iteration 158/1000 | Loss: 0.00001526
Iteration 159/1000 | Loss: 0.00001526
Iteration 160/1000 | Loss: 0.00001526
Iteration 161/1000 | Loss: 0.00001526
Iteration 162/1000 | Loss: 0.00001526
Iteration 163/1000 | Loss: 0.00001526
Iteration 164/1000 | Loss: 0.00001526
Iteration 165/1000 | Loss: 0.00001526
Iteration 166/1000 | Loss: 0.00001525
Iteration 167/1000 | Loss: 0.00001525
Iteration 168/1000 | Loss: 0.00001525
Iteration 169/1000 | Loss: 0.00001525
Iteration 170/1000 | Loss: 0.00001525
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00001525
Iteration 173/1000 | Loss: 0.00001525
Iteration 174/1000 | Loss: 0.00001525
Iteration 175/1000 | Loss: 0.00001525
Iteration 176/1000 | Loss: 0.00001525
Iteration 177/1000 | Loss: 0.00001525
Iteration 178/1000 | Loss: 0.00001525
Iteration 179/1000 | Loss: 0.00001525
Iteration 180/1000 | Loss: 0.00001525
Iteration 181/1000 | Loss: 0.00001525
Iteration 182/1000 | Loss: 0.00001525
Iteration 183/1000 | Loss: 0.00001525
Iteration 184/1000 | Loss: 0.00001524
Iteration 185/1000 | Loss: 0.00001524
Iteration 186/1000 | Loss: 0.00001524
Iteration 187/1000 | Loss: 0.00001524
Iteration 188/1000 | Loss: 0.00001524
Iteration 189/1000 | Loss: 0.00001524
Iteration 190/1000 | Loss: 0.00001524
Iteration 191/1000 | Loss: 0.00001524
Iteration 192/1000 | Loss: 0.00001524
Iteration 193/1000 | Loss: 0.00001524
Iteration 194/1000 | Loss: 0.00001524
Iteration 195/1000 | Loss: 0.00001524
Iteration 196/1000 | Loss: 0.00001524
Iteration 197/1000 | Loss: 0.00001524
Iteration 198/1000 | Loss: 0.00001524
Iteration 199/1000 | Loss: 0.00001524
Iteration 200/1000 | Loss: 0.00001524
Iteration 201/1000 | Loss: 0.00001524
Iteration 202/1000 | Loss: 0.00001524
Iteration 203/1000 | Loss: 0.00001524
Iteration 204/1000 | Loss: 0.00001524
Iteration 205/1000 | Loss: 0.00001524
Iteration 206/1000 | Loss: 0.00001524
Iteration 207/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.5243520465446636e-05, 1.5243520465446636e-05, 1.5243520465446636e-05, 1.5243520465446636e-05, 1.5243520465446636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5243520465446636e-05

Optimization complete. Final v2v error: 3.293592691421509 mm

Highest mean error: 3.8320350646972656 mm for frame 97

Lowest mean error: 3.139317750930786 mm for frame 84

Saving results

Total time: 37.978076219558716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908523
Iteration 2/25 | Loss: 0.00152774
Iteration 3/25 | Loss: 0.00141473
Iteration 4/25 | Loss: 0.00140027
Iteration 5/25 | Loss: 0.00139615
Iteration 6/25 | Loss: 0.00139488
Iteration 7/25 | Loss: 0.00139485
Iteration 8/25 | Loss: 0.00139485
Iteration 9/25 | Loss: 0.00139485
Iteration 10/25 | Loss: 0.00139485
Iteration 11/25 | Loss: 0.00139485
Iteration 12/25 | Loss: 0.00139485
Iteration 13/25 | Loss: 0.00139485
Iteration 14/25 | Loss: 0.00139485
Iteration 15/25 | Loss: 0.00139485
Iteration 16/25 | Loss: 0.00139485
Iteration 17/25 | Loss: 0.00139485
Iteration 18/25 | Loss: 0.00139485
Iteration 19/25 | Loss: 0.00139485
Iteration 20/25 | Loss: 0.00139485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013948543928563595, 0.0013948543928563595, 0.0013948543928563595, 0.0013948543928563595, 0.0013948543928563595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013948543928563595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32821524
Iteration 2/25 | Loss: 0.00096500
Iteration 3/25 | Loss: 0.00096498
Iteration 4/25 | Loss: 0.00096498
Iteration 5/25 | Loss: 0.00096497
Iteration 6/25 | Loss: 0.00096497
Iteration 7/25 | Loss: 0.00096497
Iteration 8/25 | Loss: 0.00096497
Iteration 9/25 | Loss: 0.00096497
Iteration 10/25 | Loss: 0.00096497
Iteration 11/25 | Loss: 0.00096497
Iteration 12/25 | Loss: 0.00096497
Iteration 13/25 | Loss: 0.00096497
Iteration 14/25 | Loss: 0.00096497
Iteration 15/25 | Loss: 0.00096497
Iteration 16/25 | Loss: 0.00096497
Iteration 17/25 | Loss: 0.00096497
Iteration 18/25 | Loss: 0.00096497
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009649731800891459, 0.0009649731800891459, 0.0009649731800891459, 0.0009649731800891459, 0.0009649731800891459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009649731800891459

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096497
Iteration 2/1000 | Loss: 0.00004704
Iteration 3/1000 | Loss: 0.00003053
Iteration 4/1000 | Loss: 0.00002358
Iteration 5/1000 | Loss: 0.00002201
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00001940
Iteration 8/1000 | Loss: 0.00001877
Iteration 9/1000 | Loss: 0.00001817
Iteration 10/1000 | Loss: 0.00001772
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001691
Iteration 15/1000 | Loss: 0.00001679
Iteration 16/1000 | Loss: 0.00001677
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001673
Iteration 19/1000 | Loss: 0.00001673
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001668
Iteration 24/1000 | Loss: 0.00001668
Iteration 25/1000 | Loss: 0.00001667
Iteration 26/1000 | Loss: 0.00001667
Iteration 27/1000 | Loss: 0.00001667
Iteration 28/1000 | Loss: 0.00001667
Iteration 29/1000 | Loss: 0.00001666
Iteration 30/1000 | Loss: 0.00001666
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001663
Iteration 34/1000 | Loss: 0.00001662
Iteration 35/1000 | Loss: 0.00001662
Iteration 36/1000 | Loss: 0.00001662
Iteration 37/1000 | Loss: 0.00001661
Iteration 38/1000 | Loss: 0.00001661
Iteration 39/1000 | Loss: 0.00001661
Iteration 40/1000 | Loss: 0.00001661
Iteration 41/1000 | Loss: 0.00001660
Iteration 42/1000 | Loss: 0.00001660
Iteration 43/1000 | Loss: 0.00001660
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001659
Iteration 47/1000 | Loss: 0.00001659
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001659
Iteration 51/1000 | Loss: 0.00001659
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001658
Iteration 55/1000 | Loss: 0.00001658
Iteration 56/1000 | Loss: 0.00001658
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001657
Iteration 59/1000 | Loss: 0.00001657
Iteration 60/1000 | Loss: 0.00001657
Iteration 61/1000 | Loss: 0.00001656
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001655
Iteration 66/1000 | Loss: 0.00001655
Iteration 67/1000 | Loss: 0.00001655
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001653
Iteration 79/1000 | Loss: 0.00001653
Iteration 80/1000 | Loss: 0.00001653
Iteration 81/1000 | Loss: 0.00001653
Iteration 82/1000 | Loss: 0.00001653
Iteration 83/1000 | Loss: 0.00001653
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001653
Iteration 86/1000 | Loss: 0.00001653
Iteration 87/1000 | Loss: 0.00001652
Iteration 88/1000 | Loss: 0.00001652
Iteration 89/1000 | Loss: 0.00001652
Iteration 90/1000 | Loss: 0.00001652
Iteration 91/1000 | Loss: 0.00001652
Iteration 92/1000 | Loss: 0.00001652
Iteration 93/1000 | Loss: 0.00001652
Iteration 94/1000 | Loss: 0.00001652
Iteration 95/1000 | Loss: 0.00001652
Iteration 96/1000 | Loss: 0.00001652
Iteration 97/1000 | Loss: 0.00001651
Iteration 98/1000 | Loss: 0.00001651
Iteration 99/1000 | Loss: 0.00001651
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001651
Iteration 102/1000 | Loss: 0.00001651
Iteration 103/1000 | Loss: 0.00001651
Iteration 104/1000 | Loss: 0.00001651
Iteration 105/1000 | Loss: 0.00001651
Iteration 106/1000 | Loss: 0.00001651
Iteration 107/1000 | Loss: 0.00001650
Iteration 108/1000 | Loss: 0.00001650
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001650
Iteration 111/1000 | Loss: 0.00001650
Iteration 112/1000 | Loss: 0.00001650
Iteration 113/1000 | Loss: 0.00001650
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.65001747518545e-05, 1.65001747518545e-05, 1.65001747518545e-05, 1.65001747518545e-05, 1.65001747518545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.65001747518545e-05

Optimization complete. Final v2v error: 3.494868040084839 mm

Highest mean error: 3.7158169746398926 mm for frame 90

Lowest mean error: 3.2232425212860107 mm for frame 138

Saving results

Total time: 38.015125036239624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005572
Iteration 2/25 | Loss: 0.00201310
Iteration 3/25 | Loss: 0.00155008
Iteration 4/25 | Loss: 0.00151059
Iteration 5/25 | Loss: 0.00149195
Iteration 6/25 | Loss: 0.00148874
Iteration 7/25 | Loss: 0.00148850
Iteration 8/25 | Loss: 0.00148850
Iteration 9/25 | Loss: 0.00148850
Iteration 10/25 | Loss: 0.00148850
Iteration 11/25 | Loss: 0.00148850
Iteration 12/25 | Loss: 0.00148850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014884993433952332, 0.0014884993433952332, 0.0014884993433952332, 0.0014884993433952332, 0.0014884993433952332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014884993433952332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11691213
Iteration 2/25 | Loss: 0.00098240
Iteration 3/25 | Loss: 0.00098239
Iteration 4/25 | Loss: 0.00098239
Iteration 5/25 | Loss: 0.00098239
Iteration 6/25 | Loss: 0.00098239
Iteration 7/25 | Loss: 0.00098239
Iteration 8/25 | Loss: 0.00098239
Iteration 9/25 | Loss: 0.00098239
Iteration 10/25 | Loss: 0.00098239
Iteration 11/25 | Loss: 0.00098239
Iteration 12/25 | Loss: 0.00098239
Iteration 13/25 | Loss: 0.00098239
Iteration 14/25 | Loss: 0.00098239
Iteration 15/25 | Loss: 0.00098239
Iteration 16/25 | Loss: 0.00098239
Iteration 17/25 | Loss: 0.00098239
Iteration 18/25 | Loss: 0.00098239
Iteration 19/25 | Loss: 0.00098239
Iteration 20/25 | Loss: 0.00098239
Iteration 21/25 | Loss: 0.00098239
Iteration 22/25 | Loss: 0.00098239
Iteration 23/25 | Loss: 0.00098239
Iteration 24/25 | Loss: 0.00098239
Iteration 25/25 | Loss: 0.00098239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098239
Iteration 2/1000 | Loss: 0.00008823
Iteration 3/1000 | Loss: 0.00006652
Iteration 4/1000 | Loss: 0.00005563
Iteration 5/1000 | Loss: 0.00005192
Iteration 6/1000 | Loss: 0.00004954
Iteration 7/1000 | Loss: 0.00004846
Iteration 8/1000 | Loss: 0.00004712
Iteration 9/1000 | Loss: 0.00004611
Iteration 10/1000 | Loss: 0.00004549
Iteration 11/1000 | Loss: 0.00004496
Iteration 12/1000 | Loss: 0.00004446
Iteration 13/1000 | Loss: 0.00004398
Iteration 14/1000 | Loss: 0.00004364
Iteration 15/1000 | Loss: 0.00004329
Iteration 16/1000 | Loss: 0.00004298
Iteration 17/1000 | Loss: 0.00004269
Iteration 18/1000 | Loss: 0.00004246
Iteration 19/1000 | Loss: 0.00004225
Iteration 20/1000 | Loss: 0.00004206
Iteration 21/1000 | Loss: 0.00004196
Iteration 22/1000 | Loss: 0.00004196
Iteration 23/1000 | Loss: 0.00004191
Iteration 24/1000 | Loss: 0.00004191
Iteration 25/1000 | Loss: 0.00004188
Iteration 26/1000 | Loss: 0.00004187
Iteration 27/1000 | Loss: 0.00004187
Iteration 28/1000 | Loss: 0.00004187
Iteration 29/1000 | Loss: 0.00004187
Iteration 30/1000 | Loss: 0.00004185
Iteration 31/1000 | Loss: 0.00004185
Iteration 32/1000 | Loss: 0.00004184
Iteration 33/1000 | Loss: 0.00004184
Iteration 34/1000 | Loss: 0.00004184
Iteration 35/1000 | Loss: 0.00004183
Iteration 36/1000 | Loss: 0.00004183
Iteration 37/1000 | Loss: 0.00004182
Iteration 38/1000 | Loss: 0.00004182
Iteration 39/1000 | Loss: 0.00004181
Iteration 40/1000 | Loss: 0.00004180
Iteration 41/1000 | Loss: 0.00004178
Iteration 42/1000 | Loss: 0.00004178
Iteration 43/1000 | Loss: 0.00004178
Iteration 44/1000 | Loss: 0.00004178
Iteration 45/1000 | Loss: 0.00004178
Iteration 46/1000 | Loss: 0.00004177
Iteration 47/1000 | Loss: 0.00004177
Iteration 48/1000 | Loss: 0.00004176
Iteration 49/1000 | Loss: 0.00004174
Iteration 50/1000 | Loss: 0.00004174
Iteration 51/1000 | Loss: 0.00004174
Iteration 52/1000 | Loss: 0.00004174
Iteration 53/1000 | Loss: 0.00004174
Iteration 54/1000 | Loss: 0.00004174
Iteration 55/1000 | Loss: 0.00004173
Iteration 56/1000 | Loss: 0.00004173
Iteration 57/1000 | Loss: 0.00004173
Iteration 58/1000 | Loss: 0.00004173
Iteration 59/1000 | Loss: 0.00004173
Iteration 60/1000 | Loss: 0.00004173
Iteration 61/1000 | Loss: 0.00004173
Iteration 62/1000 | Loss: 0.00004173
Iteration 63/1000 | Loss: 0.00004172
Iteration 64/1000 | Loss: 0.00004172
Iteration 65/1000 | Loss: 0.00004172
Iteration 66/1000 | Loss: 0.00004171
Iteration 67/1000 | Loss: 0.00004170
Iteration 68/1000 | Loss: 0.00004170
Iteration 69/1000 | Loss: 0.00004170
Iteration 70/1000 | Loss: 0.00004169
Iteration 71/1000 | Loss: 0.00004169
Iteration 72/1000 | Loss: 0.00004169
Iteration 73/1000 | Loss: 0.00004169
Iteration 74/1000 | Loss: 0.00004169
Iteration 75/1000 | Loss: 0.00004169
Iteration 76/1000 | Loss: 0.00004168
Iteration 77/1000 | Loss: 0.00004168
Iteration 78/1000 | Loss: 0.00004168
Iteration 79/1000 | Loss: 0.00004168
Iteration 80/1000 | Loss: 0.00004167
Iteration 81/1000 | Loss: 0.00004167
Iteration 82/1000 | Loss: 0.00004167
Iteration 83/1000 | Loss: 0.00004167
Iteration 84/1000 | Loss: 0.00004167
Iteration 85/1000 | Loss: 0.00004167
Iteration 86/1000 | Loss: 0.00004166
Iteration 87/1000 | Loss: 0.00004166
Iteration 88/1000 | Loss: 0.00004166
Iteration 89/1000 | Loss: 0.00004166
Iteration 90/1000 | Loss: 0.00004166
Iteration 91/1000 | Loss: 0.00004166
Iteration 92/1000 | Loss: 0.00004166
Iteration 93/1000 | Loss: 0.00004166
Iteration 94/1000 | Loss: 0.00004166
Iteration 95/1000 | Loss: 0.00004166
Iteration 96/1000 | Loss: 0.00004166
Iteration 97/1000 | Loss: 0.00004166
Iteration 98/1000 | Loss: 0.00004165
Iteration 99/1000 | Loss: 0.00004165
Iteration 100/1000 | Loss: 0.00004165
Iteration 101/1000 | Loss: 0.00004164
Iteration 102/1000 | Loss: 0.00004164
Iteration 103/1000 | Loss: 0.00004164
Iteration 104/1000 | Loss: 0.00004164
Iteration 105/1000 | Loss: 0.00004164
Iteration 106/1000 | Loss: 0.00004163
Iteration 107/1000 | Loss: 0.00004163
Iteration 108/1000 | Loss: 0.00004163
Iteration 109/1000 | Loss: 0.00004163
Iteration 110/1000 | Loss: 0.00004163
Iteration 111/1000 | Loss: 0.00004163
Iteration 112/1000 | Loss: 0.00004163
Iteration 113/1000 | Loss: 0.00004163
Iteration 114/1000 | Loss: 0.00004162
Iteration 115/1000 | Loss: 0.00004162
Iteration 116/1000 | Loss: 0.00004162
Iteration 117/1000 | Loss: 0.00004162
Iteration 118/1000 | Loss: 0.00004162
Iteration 119/1000 | Loss: 0.00004162
Iteration 120/1000 | Loss: 0.00004162
Iteration 121/1000 | Loss: 0.00004162
Iteration 122/1000 | Loss: 0.00004162
Iteration 123/1000 | Loss: 0.00004161
Iteration 124/1000 | Loss: 0.00004161
Iteration 125/1000 | Loss: 0.00004161
Iteration 126/1000 | Loss: 0.00004161
Iteration 127/1000 | Loss: 0.00004161
Iteration 128/1000 | Loss: 0.00004160
Iteration 129/1000 | Loss: 0.00004160
Iteration 130/1000 | Loss: 0.00004160
Iteration 131/1000 | Loss: 0.00004160
Iteration 132/1000 | Loss: 0.00004160
Iteration 133/1000 | Loss: 0.00004160
Iteration 134/1000 | Loss: 0.00004160
Iteration 135/1000 | Loss: 0.00004160
Iteration 136/1000 | Loss: 0.00004160
Iteration 137/1000 | Loss: 0.00004159
Iteration 138/1000 | Loss: 0.00004159
Iteration 139/1000 | Loss: 0.00004159
Iteration 140/1000 | Loss: 0.00004159
Iteration 141/1000 | Loss: 0.00004159
Iteration 142/1000 | Loss: 0.00004159
Iteration 143/1000 | Loss: 0.00004159
Iteration 144/1000 | Loss: 0.00004159
Iteration 145/1000 | Loss: 0.00004159
Iteration 146/1000 | Loss: 0.00004159
Iteration 147/1000 | Loss: 0.00004159
Iteration 148/1000 | Loss: 0.00004159
Iteration 149/1000 | Loss: 0.00004159
Iteration 150/1000 | Loss: 0.00004158
Iteration 151/1000 | Loss: 0.00004158
Iteration 152/1000 | Loss: 0.00004158
Iteration 153/1000 | Loss: 0.00004158
Iteration 154/1000 | Loss: 0.00004158
Iteration 155/1000 | Loss: 0.00004158
Iteration 156/1000 | Loss: 0.00004158
Iteration 157/1000 | Loss: 0.00004158
Iteration 158/1000 | Loss: 0.00004158
Iteration 159/1000 | Loss: 0.00004158
Iteration 160/1000 | Loss: 0.00004158
Iteration 161/1000 | Loss: 0.00004158
Iteration 162/1000 | Loss: 0.00004158
Iteration 163/1000 | Loss: 0.00004158
Iteration 164/1000 | Loss: 0.00004158
Iteration 165/1000 | Loss: 0.00004157
Iteration 166/1000 | Loss: 0.00004157
Iteration 167/1000 | Loss: 0.00004157
Iteration 168/1000 | Loss: 0.00004157
Iteration 169/1000 | Loss: 0.00004157
Iteration 170/1000 | Loss: 0.00004157
Iteration 171/1000 | Loss: 0.00004157
Iteration 172/1000 | Loss: 0.00004157
Iteration 173/1000 | Loss: 0.00004157
Iteration 174/1000 | Loss: 0.00004157
Iteration 175/1000 | Loss: 0.00004157
Iteration 176/1000 | Loss: 0.00004157
Iteration 177/1000 | Loss: 0.00004157
Iteration 178/1000 | Loss: 0.00004157
Iteration 179/1000 | Loss: 0.00004157
Iteration 180/1000 | Loss: 0.00004157
Iteration 181/1000 | Loss: 0.00004157
Iteration 182/1000 | Loss: 0.00004157
Iteration 183/1000 | Loss: 0.00004157
Iteration 184/1000 | Loss: 0.00004157
Iteration 185/1000 | Loss: 0.00004157
Iteration 186/1000 | Loss: 0.00004157
Iteration 187/1000 | Loss: 0.00004157
Iteration 188/1000 | Loss: 0.00004157
Iteration 189/1000 | Loss: 0.00004157
Iteration 190/1000 | Loss: 0.00004157
Iteration 191/1000 | Loss: 0.00004157
Iteration 192/1000 | Loss: 0.00004157
Iteration 193/1000 | Loss: 0.00004157
Iteration 194/1000 | Loss: 0.00004157
Iteration 195/1000 | Loss: 0.00004157
Iteration 196/1000 | Loss: 0.00004157
Iteration 197/1000 | Loss: 0.00004157
Iteration 198/1000 | Loss: 0.00004157
Iteration 199/1000 | Loss: 0.00004157
Iteration 200/1000 | Loss: 0.00004157
Iteration 201/1000 | Loss: 0.00004157
Iteration 202/1000 | Loss: 0.00004157
Iteration 203/1000 | Loss: 0.00004157
Iteration 204/1000 | Loss: 0.00004157
Iteration 205/1000 | Loss: 0.00004157
Iteration 206/1000 | Loss: 0.00004157
Iteration 207/1000 | Loss: 0.00004157
Iteration 208/1000 | Loss: 0.00004157
Iteration 209/1000 | Loss: 0.00004157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [4.156809882260859e-05, 4.156809882260859e-05, 4.156809882260859e-05, 4.156809882260859e-05, 4.156809882260859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.156809882260859e-05

Optimization complete. Final v2v error: 5.1160078048706055 mm

Highest mean error: 6.91855525970459 mm for frame 66

Lowest mean error: 3.70923113822937 mm for frame 3

Saving results

Total time: 51.90069103240967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443310
Iteration 2/25 | Loss: 0.00163894
Iteration 3/25 | Loss: 0.00143108
Iteration 4/25 | Loss: 0.00141074
Iteration 5/25 | Loss: 0.00140752
Iteration 6/25 | Loss: 0.00140702
Iteration 7/25 | Loss: 0.00140702
Iteration 8/25 | Loss: 0.00140702
Iteration 9/25 | Loss: 0.00140702
Iteration 10/25 | Loss: 0.00140702
Iteration 11/25 | Loss: 0.00140702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001407018513418734, 0.001407018513418734, 0.001407018513418734, 0.001407018513418734, 0.001407018513418734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001407018513418734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29642522
Iteration 2/25 | Loss: 0.00098907
Iteration 3/25 | Loss: 0.00098906
Iteration 4/25 | Loss: 0.00098906
Iteration 5/25 | Loss: 0.00098906
Iteration 6/25 | Loss: 0.00098906
Iteration 7/25 | Loss: 0.00098906
Iteration 8/25 | Loss: 0.00098906
Iteration 9/25 | Loss: 0.00098906
Iteration 10/25 | Loss: 0.00098906
Iteration 11/25 | Loss: 0.00098906
Iteration 12/25 | Loss: 0.00098906
Iteration 13/25 | Loss: 0.00098906
Iteration 14/25 | Loss: 0.00098906
Iteration 15/25 | Loss: 0.00098906
Iteration 16/25 | Loss: 0.00098906
Iteration 17/25 | Loss: 0.00098906
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009890592191368341, 0.0009890592191368341, 0.0009890592191368341, 0.0009890592191368341, 0.0009890592191368341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009890592191368341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098906
Iteration 2/1000 | Loss: 0.00004479
Iteration 3/1000 | Loss: 0.00002721
Iteration 4/1000 | Loss: 0.00002076
Iteration 5/1000 | Loss: 0.00001881
Iteration 6/1000 | Loss: 0.00001779
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001686
Iteration 9/1000 | Loss: 0.00001652
Iteration 10/1000 | Loss: 0.00001647
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001598
Iteration 19/1000 | Loss: 0.00001597
Iteration 20/1000 | Loss: 0.00001597
Iteration 21/1000 | Loss: 0.00001596
Iteration 22/1000 | Loss: 0.00001596
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001594
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001593
Iteration 27/1000 | Loss: 0.00001593
Iteration 28/1000 | Loss: 0.00001593
Iteration 29/1000 | Loss: 0.00001592
Iteration 30/1000 | Loss: 0.00001592
Iteration 31/1000 | Loss: 0.00001592
Iteration 32/1000 | Loss: 0.00001592
Iteration 33/1000 | Loss: 0.00001590
Iteration 34/1000 | Loss: 0.00001589
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001588
Iteration 37/1000 | Loss: 0.00001588
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001588
Iteration 40/1000 | Loss: 0.00001588
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001587
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001585
Iteration 46/1000 | Loss: 0.00001584
Iteration 47/1000 | Loss: 0.00001584
Iteration 48/1000 | Loss: 0.00001583
Iteration 49/1000 | Loss: 0.00001583
Iteration 50/1000 | Loss: 0.00001582
Iteration 51/1000 | Loss: 0.00001582
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001581
Iteration 54/1000 | Loss: 0.00001581
Iteration 55/1000 | Loss: 0.00001581
Iteration 56/1000 | Loss: 0.00001581
Iteration 57/1000 | Loss: 0.00001581
Iteration 58/1000 | Loss: 0.00001581
Iteration 59/1000 | Loss: 0.00001581
Iteration 60/1000 | Loss: 0.00001580
Iteration 61/1000 | Loss: 0.00001579
Iteration 62/1000 | Loss: 0.00001579
Iteration 63/1000 | Loss: 0.00001579
Iteration 64/1000 | Loss: 0.00001579
Iteration 65/1000 | Loss: 0.00001578
Iteration 66/1000 | Loss: 0.00001578
Iteration 67/1000 | Loss: 0.00001577
Iteration 68/1000 | Loss: 0.00001577
Iteration 69/1000 | Loss: 0.00001576
Iteration 70/1000 | Loss: 0.00001576
Iteration 71/1000 | Loss: 0.00001576
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001574
Iteration 79/1000 | Loss: 0.00001574
Iteration 80/1000 | Loss: 0.00001574
Iteration 81/1000 | Loss: 0.00001573
Iteration 82/1000 | Loss: 0.00001573
Iteration 83/1000 | Loss: 0.00001573
Iteration 84/1000 | Loss: 0.00001573
Iteration 85/1000 | Loss: 0.00001572
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001572
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001572
Iteration 90/1000 | Loss: 0.00001572
Iteration 91/1000 | Loss: 0.00001572
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001572
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001571
Iteration 99/1000 | Loss: 0.00001571
Iteration 100/1000 | Loss: 0.00001570
Iteration 101/1000 | Loss: 0.00001570
Iteration 102/1000 | Loss: 0.00001570
Iteration 103/1000 | Loss: 0.00001570
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001569
Iteration 110/1000 | Loss: 0.00001569
Iteration 111/1000 | Loss: 0.00001569
Iteration 112/1000 | Loss: 0.00001569
Iteration 113/1000 | Loss: 0.00001568
Iteration 114/1000 | Loss: 0.00001568
Iteration 115/1000 | Loss: 0.00001568
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001568
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001568
Iteration 124/1000 | Loss: 0.00001568
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001567
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001567
Iteration 131/1000 | Loss: 0.00001567
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001567
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001567
Iteration 137/1000 | Loss: 0.00001567
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001567
Iteration 141/1000 | Loss: 0.00001567
Iteration 142/1000 | Loss: 0.00001566
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001566
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001566
Iteration 149/1000 | Loss: 0.00001566
Iteration 150/1000 | Loss: 0.00001566
Iteration 151/1000 | Loss: 0.00001566
Iteration 152/1000 | Loss: 0.00001566
Iteration 153/1000 | Loss: 0.00001566
Iteration 154/1000 | Loss: 0.00001566
Iteration 155/1000 | Loss: 0.00001566
Iteration 156/1000 | Loss: 0.00001566
Iteration 157/1000 | Loss: 0.00001566
Iteration 158/1000 | Loss: 0.00001566
Iteration 159/1000 | Loss: 0.00001566
Iteration 160/1000 | Loss: 0.00001566
Iteration 161/1000 | Loss: 0.00001566
Iteration 162/1000 | Loss: 0.00001566
Iteration 163/1000 | Loss: 0.00001566
Iteration 164/1000 | Loss: 0.00001566
Iteration 165/1000 | Loss: 0.00001566
Iteration 166/1000 | Loss: 0.00001566
Iteration 167/1000 | Loss: 0.00001566
Iteration 168/1000 | Loss: 0.00001566
Iteration 169/1000 | Loss: 0.00001566
Iteration 170/1000 | Loss: 0.00001566
Iteration 171/1000 | Loss: 0.00001566
Iteration 172/1000 | Loss: 0.00001566
Iteration 173/1000 | Loss: 0.00001566
Iteration 174/1000 | Loss: 0.00001566
Iteration 175/1000 | Loss: 0.00001566
Iteration 176/1000 | Loss: 0.00001566
Iteration 177/1000 | Loss: 0.00001566
Iteration 178/1000 | Loss: 0.00001566
Iteration 179/1000 | Loss: 0.00001566
Iteration 180/1000 | Loss: 0.00001566
Iteration 181/1000 | Loss: 0.00001566
Iteration 182/1000 | Loss: 0.00001566
Iteration 183/1000 | Loss: 0.00001566
Iteration 184/1000 | Loss: 0.00001566
Iteration 185/1000 | Loss: 0.00001566
Iteration 186/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.5661764336982742e-05, 1.5661764336982742e-05, 1.5661764336982742e-05, 1.5661764336982742e-05, 1.5661764336982742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5661764336982742e-05

Optimization complete. Final v2v error: 3.4525909423828125 mm

Highest mean error: 3.620821952819824 mm for frame 117

Lowest mean error: 3.254917621612549 mm for frame 50

Saving results

Total time: 34.95636796951294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988747
Iteration 2/25 | Loss: 0.00286759
Iteration 3/25 | Loss: 0.00209446
Iteration 4/25 | Loss: 0.00177618
Iteration 5/25 | Loss: 0.00152114
Iteration 6/25 | Loss: 0.00149000
Iteration 7/25 | Loss: 0.00141746
Iteration 8/25 | Loss: 0.00132753
Iteration 9/25 | Loss: 0.00127382
Iteration 10/25 | Loss: 0.00126401
Iteration 11/25 | Loss: 0.00122619
Iteration 12/25 | Loss: 0.00122847
Iteration 13/25 | Loss: 0.00121721
Iteration 14/25 | Loss: 0.00121429
Iteration 15/25 | Loss: 0.00121322
Iteration 16/25 | Loss: 0.00120022
Iteration 17/25 | Loss: 0.00118566
Iteration 18/25 | Loss: 0.00117740
Iteration 19/25 | Loss: 0.00117453
Iteration 20/25 | Loss: 0.00117925
Iteration 21/25 | Loss: 0.00117370
Iteration 22/25 | Loss: 0.00116738
Iteration 23/25 | Loss: 0.00116577
Iteration 24/25 | Loss: 0.00116515
Iteration 25/25 | Loss: 0.00116488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28665471
Iteration 2/25 | Loss: 0.00165672
Iteration 3/25 | Loss: 0.00123237
Iteration 4/25 | Loss: 0.00123236
Iteration 5/25 | Loss: 0.00123236
Iteration 6/25 | Loss: 0.00123236
Iteration 7/25 | Loss: 0.00123236
Iteration 8/25 | Loss: 0.00123236
Iteration 9/25 | Loss: 0.00123236
Iteration 10/25 | Loss: 0.00123236
Iteration 11/25 | Loss: 0.00123236
Iteration 12/25 | Loss: 0.00123236
Iteration 13/25 | Loss: 0.00123236
Iteration 14/25 | Loss: 0.00123236
Iteration 15/25 | Loss: 0.00123236
Iteration 16/25 | Loss: 0.00123236
Iteration 17/25 | Loss: 0.00123236
Iteration 18/25 | Loss: 0.00123236
Iteration 19/25 | Loss: 0.00123236
Iteration 20/25 | Loss: 0.00123236
Iteration 21/25 | Loss: 0.00123236
Iteration 22/25 | Loss: 0.00123236
Iteration 23/25 | Loss: 0.00123236
Iteration 24/25 | Loss: 0.00123236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012323606060817838, 0.0012323606060817838, 0.0012323606060817838, 0.0012323606060817838, 0.0012323606060817838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012323606060817838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123236
Iteration 2/1000 | Loss: 0.00027180
Iteration 3/1000 | Loss: 0.00012029
Iteration 4/1000 | Loss: 0.00010229
Iteration 5/1000 | Loss: 0.00009440
Iteration 6/1000 | Loss: 0.00008893
Iteration 7/1000 | Loss: 0.00008596
Iteration 8/1000 | Loss: 0.00008440
Iteration 9/1000 | Loss: 0.00008223
Iteration 10/1000 | Loss: 0.00008056
Iteration 11/1000 | Loss: 0.00020066
Iteration 12/1000 | Loss: 0.00269161
Iteration 13/1000 | Loss: 0.00204016
Iteration 14/1000 | Loss: 0.00097018
Iteration 15/1000 | Loss: 0.00053419
Iteration 16/1000 | Loss: 0.00100767
Iteration 17/1000 | Loss: 0.00203144
Iteration 18/1000 | Loss: 0.00139257
Iteration 19/1000 | Loss: 0.00078858
Iteration 20/1000 | Loss: 0.00091821
Iteration 21/1000 | Loss: 0.00157634
Iteration 22/1000 | Loss: 0.00186579
Iteration 23/1000 | Loss: 0.00120638
Iteration 24/1000 | Loss: 0.00056417
Iteration 25/1000 | Loss: 0.00036624
Iteration 26/1000 | Loss: 0.00054348
Iteration 27/1000 | Loss: 0.00086847
Iteration 28/1000 | Loss: 0.00034044
Iteration 29/1000 | Loss: 0.00015658
Iteration 30/1000 | Loss: 0.00036870
Iteration 31/1000 | Loss: 0.00029154
Iteration 32/1000 | Loss: 0.00018041
Iteration 33/1000 | Loss: 0.00026141
Iteration 34/1000 | Loss: 0.00067278
Iteration 35/1000 | Loss: 0.00027595
Iteration 36/1000 | Loss: 0.00065774
Iteration 37/1000 | Loss: 0.00046443
Iteration 38/1000 | Loss: 0.00015762
Iteration 39/1000 | Loss: 0.00019025
Iteration 40/1000 | Loss: 0.00022957
Iteration 41/1000 | Loss: 0.00023761
Iteration 42/1000 | Loss: 0.00023708
Iteration 43/1000 | Loss: 0.00023629
Iteration 44/1000 | Loss: 0.00011991
Iteration 45/1000 | Loss: 0.00040701
Iteration 46/1000 | Loss: 0.00050045
Iteration 47/1000 | Loss: 0.00063722
Iteration 48/1000 | Loss: 0.00015109
Iteration 49/1000 | Loss: 0.00027934
Iteration 50/1000 | Loss: 0.00042267
Iteration 51/1000 | Loss: 0.00009811
Iteration 52/1000 | Loss: 0.00008835
Iteration 53/1000 | Loss: 0.00007824
Iteration 54/1000 | Loss: 0.00007445
Iteration 55/1000 | Loss: 0.00007210
Iteration 56/1000 | Loss: 0.00019002
Iteration 57/1000 | Loss: 0.00007486
Iteration 58/1000 | Loss: 0.00020294
Iteration 59/1000 | Loss: 0.00084883
Iteration 60/1000 | Loss: 0.00028995
Iteration 61/1000 | Loss: 0.00008939
Iteration 62/1000 | Loss: 0.00007256
Iteration 63/1000 | Loss: 0.00006606
Iteration 64/1000 | Loss: 0.00006275
Iteration 65/1000 | Loss: 0.00060596
Iteration 66/1000 | Loss: 0.00010396
Iteration 67/1000 | Loss: 0.00027240
Iteration 68/1000 | Loss: 0.00005896
Iteration 69/1000 | Loss: 0.00018459
Iteration 70/1000 | Loss: 0.00034013
Iteration 71/1000 | Loss: 0.00039562
Iteration 72/1000 | Loss: 0.00006515
Iteration 73/1000 | Loss: 0.00005695
Iteration 74/1000 | Loss: 0.00005446
Iteration 75/1000 | Loss: 0.00025291
Iteration 76/1000 | Loss: 0.00005230
Iteration 77/1000 | Loss: 0.00005076
Iteration 78/1000 | Loss: 0.00005007
Iteration 79/1000 | Loss: 0.00004961
Iteration 80/1000 | Loss: 0.00004917
Iteration 81/1000 | Loss: 0.00004893
Iteration 82/1000 | Loss: 0.00004875
Iteration 83/1000 | Loss: 0.00004861
Iteration 84/1000 | Loss: 0.00004861
Iteration 85/1000 | Loss: 0.00004856
Iteration 86/1000 | Loss: 0.00004846
Iteration 87/1000 | Loss: 0.00004839
Iteration 88/1000 | Loss: 0.00004839
Iteration 89/1000 | Loss: 0.00004836
Iteration 90/1000 | Loss: 0.00004835
Iteration 91/1000 | Loss: 0.00004832
Iteration 92/1000 | Loss: 0.00004832
Iteration 93/1000 | Loss: 0.00004832
Iteration 94/1000 | Loss: 0.00004831
Iteration 95/1000 | Loss: 0.00004831
Iteration 96/1000 | Loss: 0.00004831
Iteration 97/1000 | Loss: 0.00004830
Iteration 98/1000 | Loss: 0.00004830
Iteration 99/1000 | Loss: 0.00004830
Iteration 100/1000 | Loss: 0.00004830
Iteration 101/1000 | Loss: 0.00004830
Iteration 102/1000 | Loss: 0.00004829
Iteration 103/1000 | Loss: 0.00004829
Iteration 104/1000 | Loss: 0.00004829
Iteration 105/1000 | Loss: 0.00004829
Iteration 106/1000 | Loss: 0.00004870
Iteration 107/1000 | Loss: 0.00004870
Iteration 108/1000 | Loss: 0.00004869
Iteration 109/1000 | Loss: 0.00004869
Iteration 110/1000 | Loss: 0.00004868
Iteration 111/1000 | Loss: 0.00004868
Iteration 112/1000 | Loss: 0.00004837
Iteration 113/1000 | Loss: 0.00004825
Iteration 114/1000 | Loss: 0.00004825
Iteration 115/1000 | Loss: 0.00004824
Iteration 116/1000 | Loss: 0.00004824
Iteration 117/1000 | Loss: 0.00004824
Iteration 118/1000 | Loss: 0.00004824
Iteration 119/1000 | Loss: 0.00004824
Iteration 120/1000 | Loss: 0.00004824
Iteration 121/1000 | Loss: 0.00004824
Iteration 122/1000 | Loss: 0.00004824
Iteration 123/1000 | Loss: 0.00004824
Iteration 124/1000 | Loss: 0.00004824
Iteration 125/1000 | Loss: 0.00004823
Iteration 126/1000 | Loss: 0.00004823
Iteration 127/1000 | Loss: 0.00004823
Iteration 128/1000 | Loss: 0.00004823
Iteration 129/1000 | Loss: 0.00004823
Iteration 130/1000 | Loss: 0.00004822
Iteration 131/1000 | Loss: 0.00004822
Iteration 132/1000 | Loss: 0.00004822
Iteration 133/1000 | Loss: 0.00004822
Iteration 134/1000 | Loss: 0.00004822
Iteration 135/1000 | Loss: 0.00004822
Iteration 136/1000 | Loss: 0.00004827
Iteration 137/1000 | Loss: 0.00004826
Iteration 138/1000 | Loss: 0.00004826
Iteration 139/1000 | Loss: 0.00004826
Iteration 140/1000 | Loss: 0.00004826
Iteration 141/1000 | Loss: 0.00004826
Iteration 142/1000 | Loss: 0.00004826
Iteration 143/1000 | Loss: 0.00004826
Iteration 144/1000 | Loss: 0.00004826
Iteration 145/1000 | Loss: 0.00004826
Iteration 146/1000 | Loss: 0.00004826
Iteration 147/1000 | Loss: 0.00004826
Iteration 148/1000 | Loss: 0.00004826
Iteration 149/1000 | Loss: 0.00004826
Iteration 150/1000 | Loss: 0.00004826
Iteration 151/1000 | Loss: 0.00004826
Iteration 152/1000 | Loss: 0.00004826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [4.826315853279084e-05, 4.826315853279084e-05, 4.826315853279084e-05, 4.826315853279084e-05, 4.826315853279084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.826315853279084e-05

Optimization complete. Final v2v error: 4.5353922843933105 mm

Highest mean error: 23.176393508911133 mm for frame 13

Lowest mean error: 3.069627046585083 mm for frame 122

Saving results

Total time: 194.4996621608734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961899
Iteration 2/25 | Loss: 0.00168726
Iteration 3/25 | Loss: 0.00153337
Iteration 4/25 | Loss: 0.00149660
Iteration 5/25 | Loss: 0.00148034
Iteration 6/25 | Loss: 0.00147753
Iteration 7/25 | Loss: 0.00147700
Iteration 8/25 | Loss: 0.00147700
Iteration 9/25 | Loss: 0.00147700
Iteration 10/25 | Loss: 0.00147700
Iteration 11/25 | Loss: 0.00147700
Iteration 12/25 | Loss: 0.00147700
Iteration 13/25 | Loss: 0.00147700
Iteration 14/25 | Loss: 0.00147700
Iteration 15/25 | Loss: 0.00147700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001476997509598732, 0.001476997509598732, 0.001476997509598732, 0.001476997509598732, 0.001476997509598732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001476997509598732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35622883
Iteration 2/25 | Loss: 0.00116110
Iteration 3/25 | Loss: 0.00116107
Iteration 4/25 | Loss: 0.00116107
Iteration 5/25 | Loss: 0.00116107
Iteration 6/25 | Loss: 0.00116107
Iteration 7/25 | Loss: 0.00116107
Iteration 8/25 | Loss: 0.00116107
Iteration 9/25 | Loss: 0.00116107
Iteration 10/25 | Loss: 0.00116107
Iteration 11/25 | Loss: 0.00116107
Iteration 12/25 | Loss: 0.00116107
Iteration 13/25 | Loss: 0.00116107
Iteration 14/25 | Loss: 0.00116107
Iteration 15/25 | Loss: 0.00116107
Iteration 16/25 | Loss: 0.00116107
Iteration 17/25 | Loss: 0.00116107
Iteration 18/25 | Loss: 0.00116107
Iteration 19/25 | Loss: 0.00116107
Iteration 20/25 | Loss: 0.00116107
Iteration 21/25 | Loss: 0.00116107
Iteration 22/25 | Loss: 0.00116107
Iteration 23/25 | Loss: 0.00116107
Iteration 24/25 | Loss: 0.00116107
Iteration 25/25 | Loss: 0.00116107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116107
Iteration 2/1000 | Loss: 0.00008349
Iteration 3/1000 | Loss: 0.00005406
Iteration 4/1000 | Loss: 0.00004112
Iteration 5/1000 | Loss: 0.00003749
Iteration 6/1000 | Loss: 0.00003499
Iteration 7/1000 | Loss: 0.00003381
Iteration 8/1000 | Loss: 0.00003268
Iteration 9/1000 | Loss: 0.00003174
Iteration 10/1000 | Loss: 0.00003103
Iteration 11/1000 | Loss: 0.00003059
Iteration 12/1000 | Loss: 0.00003035
Iteration 13/1000 | Loss: 0.00003009
Iteration 14/1000 | Loss: 0.00003000
Iteration 15/1000 | Loss: 0.00002993
Iteration 16/1000 | Loss: 0.00002993
Iteration 17/1000 | Loss: 0.00002993
Iteration 18/1000 | Loss: 0.00002992
Iteration 19/1000 | Loss: 0.00002992
Iteration 20/1000 | Loss: 0.00002991
Iteration 21/1000 | Loss: 0.00002987
Iteration 22/1000 | Loss: 0.00002986
Iteration 23/1000 | Loss: 0.00002985
Iteration 24/1000 | Loss: 0.00002984
Iteration 25/1000 | Loss: 0.00002984
Iteration 26/1000 | Loss: 0.00002984
Iteration 27/1000 | Loss: 0.00002979
Iteration 28/1000 | Loss: 0.00002979
Iteration 29/1000 | Loss: 0.00002974
Iteration 30/1000 | Loss: 0.00002974
Iteration 31/1000 | Loss: 0.00002974
Iteration 32/1000 | Loss: 0.00002973
Iteration 33/1000 | Loss: 0.00002972
Iteration 34/1000 | Loss: 0.00002970
Iteration 35/1000 | Loss: 0.00002969
Iteration 36/1000 | Loss: 0.00002968
Iteration 37/1000 | Loss: 0.00002968
Iteration 38/1000 | Loss: 0.00002968
Iteration 39/1000 | Loss: 0.00002968
Iteration 40/1000 | Loss: 0.00002968
Iteration 41/1000 | Loss: 0.00002968
Iteration 42/1000 | Loss: 0.00002968
Iteration 43/1000 | Loss: 0.00002968
Iteration 44/1000 | Loss: 0.00002968
Iteration 45/1000 | Loss: 0.00002967
Iteration 46/1000 | Loss: 0.00002967
Iteration 47/1000 | Loss: 0.00002966
Iteration 48/1000 | Loss: 0.00002965
Iteration 49/1000 | Loss: 0.00002965
Iteration 50/1000 | Loss: 0.00002964
Iteration 51/1000 | Loss: 0.00002964
Iteration 52/1000 | Loss: 0.00002964
Iteration 53/1000 | Loss: 0.00002963
Iteration 54/1000 | Loss: 0.00002963
Iteration 55/1000 | Loss: 0.00002962
Iteration 56/1000 | Loss: 0.00002960
Iteration 57/1000 | Loss: 0.00002960
Iteration 58/1000 | Loss: 0.00002959
Iteration 59/1000 | Loss: 0.00002959
Iteration 60/1000 | Loss: 0.00002958
Iteration 61/1000 | Loss: 0.00002957
Iteration 62/1000 | Loss: 0.00002957
Iteration 63/1000 | Loss: 0.00002957
Iteration 64/1000 | Loss: 0.00002957
Iteration 65/1000 | Loss: 0.00002956
Iteration 66/1000 | Loss: 0.00002956
Iteration 67/1000 | Loss: 0.00002956
Iteration 68/1000 | Loss: 0.00002955
Iteration 69/1000 | Loss: 0.00002955
Iteration 70/1000 | Loss: 0.00002954
Iteration 71/1000 | Loss: 0.00002954
Iteration 72/1000 | Loss: 0.00002954
Iteration 73/1000 | Loss: 0.00002953
Iteration 74/1000 | Loss: 0.00002953
Iteration 75/1000 | Loss: 0.00002953
Iteration 76/1000 | Loss: 0.00002953
Iteration 77/1000 | Loss: 0.00002953
Iteration 78/1000 | Loss: 0.00002953
Iteration 79/1000 | Loss: 0.00002953
Iteration 80/1000 | Loss: 0.00002952
Iteration 81/1000 | Loss: 0.00002952
Iteration 82/1000 | Loss: 0.00002952
Iteration 83/1000 | Loss: 0.00002951
Iteration 84/1000 | Loss: 0.00002951
Iteration 85/1000 | Loss: 0.00002951
Iteration 86/1000 | Loss: 0.00002951
Iteration 87/1000 | Loss: 0.00002950
Iteration 88/1000 | Loss: 0.00002950
Iteration 89/1000 | Loss: 0.00002950
Iteration 90/1000 | Loss: 0.00002950
Iteration 91/1000 | Loss: 0.00002950
Iteration 92/1000 | Loss: 0.00002949
Iteration 93/1000 | Loss: 0.00002949
Iteration 94/1000 | Loss: 0.00002949
Iteration 95/1000 | Loss: 0.00002949
Iteration 96/1000 | Loss: 0.00002949
Iteration 97/1000 | Loss: 0.00002949
Iteration 98/1000 | Loss: 0.00002949
Iteration 99/1000 | Loss: 0.00002949
Iteration 100/1000 | Loss: 0.00002948
Iteration 101/1000 | Loss: 0.00002948
Iteration 102/1000 | Loss: 0.00002948
Iteration 103/1000 | Loss: 0.00002948
Iteration 104/1000 | Loss: 0.00002948
Iteration 105/1000 | Loss: 0.00002948
Iteration 106/1000 | Loss: 0.00002947
Iteration 107/1000 | Loss: 0.00002947
Iteration 108/1000 | Loss: 0.00002947
Iteration 109/1000 | Loss: 0.00002947
Iteration 110/1000 | Loss: 0.00002947
Iteration 111/1000 | Loss: 0.00002946
Iteration 112/1000 | Loss: 0.00002946
Iteration 113/1000 | Loss: 0.00002946
Iteration 114/1000 | Loss: 0.00002946
Iteration 115/1000 | Loss: 0.00002946
Iteration 116/1000 | Loss: 0.00002946
Iteration 117/1000 | Loss: 0.00002945
Iteration 118/1000 | Loss: 0.00002945
Iteration 119/1000 | Loss: 0.00002945
Iteration 120/1000 | Loss: 0.00002944
Iteration 121/1000 | Loss: 0.00002944
Iteration 122/1000 | Loss: 0.00002944
Iteration 123/1000 | Loss: 0.00002944
Iteration 124/1000 | Loss: 0.00002944
Iteration 125/1000 | Loss: 0.00002944
Iteration 126/1000 | Loss: 0.00002944
Iteration 127/1000 | Loss: 0.00002944
Iteration 128/1000 | Loss: 0.00002943
Iteration 129/1000 | Loss: 0.00002943
Iteration 130/1000 | Loss: 0.00002943
Iteration 131/1000 | Loss: 0.00002943
Iteration 132/1000 | Loss: 0.00002943
Iteration 133/1000 | Loss: 0.00002943
Iteration 134/1000 | Loss: 0.00002942
Iteration 135/1000 | Loss: 0.00002942
Iteration 136/1000 | Loss: 0.00002942
Iteration 137/1000 | Loss: 0.00002942
Iteration 138/1000 | Loss: 0.00002942
Iteration 139/1000 | Loss: 0.00002942
Iteration 140/1000 | Loss: 0.00002942
Iteration 141/1000 | Loss: 0.00002941
Iteration 142/1000 | Loss: 0.00002941
Iteration 143/1000 | Loss: 0.00002941
Iteration 144/1000 | Loss: 0.00002941
Iteration 145/1000 | Loss: 0.00002941
Iteration 146/1000 | Loss: 0.00002941
Iteration 147/1000 | Loss: 0.00002941
Iteration 148/1000 | Loss: 0.00002941
Iteration 149/1000 | Loss: 0.00002941
Iteration 150/1000 | Loss: 0.00002941
Iteration 151/1000 | Loss: 0.00002941
Iteration 152/1000 | Loss: 0.00002941
Iteration 153/1000 | Loss: 0.00002941
Iteration 154/1000 | Loss: 0.00002940
Iteration 155/1000 | Loss: 0.00002940
Iteration 156/1000 | Loss: 0.00002940
Iteration 157/1000 | Loss: 0.00002940
Iteration 158/1000 | Loss: 0.00002940
Iteration 159/1000 | Loss: 0.00002939
Iteration 160/1000 | Loss: 0.00002939
Iteration 161/1000 | Loss: 0.00002939
Iteration 162/1000 | Loss: 0.00002939
Iteration 163/1000 | Loss: 0.00002939
Iteration 164/1000 | Loss: 0.00002939
Iteration 165/1000 | Loss: 0.00002939
Iteration 166/1000 | Loss: 0.00002939
Iteration 167/1000 | Loss: 0.00002939
Iteration 168/1000 | Loss: 0.00002939
Iteration 169/1000 | Loss: 0.00002939
Iteration 170/1000 | Loss: 0.00002939
Iteration 171/1000 | Loss: 0.00002939
Iteration 172/1000 | Loss: 0.00002939
Iteration 173/1000 | Loss: 0.00002939
Iteration 174/1000 | Loss: 0.00002939
Iteration 175/1000 | Loss: 0.00002939
Iteration 176/1000 | Loss: 0.00002939
Iteration 177/1000 | Loss: 0.00002939
Iteration 178/1000 | Loss: 0.00002939
Iteration 179/1000 | Loss: 0.00002939
Iteration 180/1000 | Loss: 0.00002939
Iteration 181/1000 | Loss: 0.00002939
Iteration 182/1000 | Loss: 0.00002939
Iteration 183/1000 | Loss: 0.00002939
Iteration 184/1000 | Loss: 0.00002939
Iteration 185/1000 | Loss: 0.00002939
Iteration 186/1000 | Loss: 0.00002939
Iteration 187/1000 | Loss: 0.00002939
Iteration 188/1000 | Loss: 0.00002939
Iteration 189/1000 | Loss: 0.00002939
Iteration 190/1000 | Loss: 0.00002939
Iteration 191/1000 | Loss: 0.00002939
Iteration 192/1000 | Loss: 0.00002939
Iteration 193/1000 | Loss: 0.00002939
Iteration 194/1000 | Loss: 0.00002939
Iteration 195/1000 | Loss: 0.00002939
Iteration 196/1000 | Loss: 0.00002939
Iteration 197/1000 | Loss: 0.00002939
Iteration 198/1000 | Loss: 0.00002939
Iteration 199/1000 | Loss: 0.00002939
Iteration 200/1000 | Loss: 0.00002939
Iteration 201/1000 | Loss: 0.00002939
Iteration 202/1000 | Loss: 0.00002939
Iteration 203/1000 | Loss: 0.00002939
Iteration 204/1000 | Loss: 0.00002939
Iteration 205/1000 | Loss: 0.00002939
Iteration 206/1000 | Loss: 0.00002939
Iteration 207/1000 | Loss: 0.00002939
Iteration 208/1000 | Loss: 0.00002939
Iteration 209/1000 | Loss: 0.00002939
Iteration 210/1000 | Loss: 0.00002939
Iteration 211/1000 | Loss: 0.00002939
Iteration 212/1000 | Loss: 0.00002939
Iteration 213/1000 | Loss: 0.00002939
Iteration 214/1000 | Loss: 0.00002939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.939264777523931e-05, 2.939264777523931e-05, 2.939264777523931e-05, 2.939264777523931e-05, 2.939264777523931e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.939264777523931e-05

Optimization complete. Final v2v error: 4.543145656585693 mm

Highest mean error: 6.273189067840576 mm for frame 69

Lowest mean error: 3.8655574321746826 mm for frame 114

Saving results

Total time: 43.552485942840576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426590
Iteration 2/25 | Loss: 0.00153411
Iteration 3/25 | Loss: 0.00143173
Iteration 4/25 | Loss: 0.00142119
Iteration 5/25 | Loss: 0.00141572
Iteration 6/25 | Loss: 0.00141433
Iteration 7/25 | Loss: 0.00141433
Iteration 8/25 | Loss: 0.00141433
Iteration 9/25 | Loss: 0.00141433
Iteration 10/25 | Loss: 0.00141433
Iteration 11/25 | Loss: 0.00141433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014143261360004544, 0.0014143261360004544, 0.0014143261360004544, 0.0014143261360004544, 0.0014143261360004544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014143261360004544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62760532
Iteration 2/25 | Loss: 0.00095581
Iteration 3/25 | Loss: 0.00095577
Iteration 4/25 | Loss: 0.00095577
Iteration 5/25 | Loss: 0.00095577
Iteration 6/25 | Loss: 0.00095577
Iteration 7/25 | Loss: 0.00095577
Iteration 8/25 | Loss: 0.00095577
Iteration 9/25 | Loss: 0.00095577
Iteration 10/25 | Loss: 0.00095577
Iteration 11/25 | Loss: 0.00095577
Iteration 12/25 | Loss: 0.00095577
Iteration 13/25 | Loss: 0.00095577
Iteration 14/25 | Loss: 0.00095577
Iteration 15/25 | Loss: 0.00095577
Iteration 16/25 | Loss: 0.00095577
Iteration 17/25 | Loss: 0.00095577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009557706653140485, 0.0009557706653140485, 0.0009557706653140485, 0.0009557706653140485, 0.0009557706653140485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009557706653140485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095577
Iteration 2/1000 | Loss: 0.00004838
Iteration 3/1000 | Loss: 0.00003240
Iteration 4/1000 | Loss: 0.00002637
Iteration 5/1000 | Loss: 0.00002367
Iteration 6/1000 | Loss: 0.00002207
Iteration 7/1000 | Loss: 0.00002081
Iteration 8/1000 | Loss: 0.00002016
Iteration 9/1000 | Loss: 0.00001965
Iteration 10/1000 | Loss: 0.00001931
Iteration 11/1000 | Loss: 0.00001897
Iteration 12/1000 | Loss: 0.00001867
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001844
Iteration 15/1000 | Loss: 0.00001836
Iteration 16/1000 | Loss: 0.00001831
Iteration 17/1000 | Loss: 0.00001830
Iteration 18/1000 | Loss: 0.00001827
Iteration 19/1000 | Loss: 0.00001826
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001825
Iteration 22/1000 | Loss: 0.00001816
Iteration 23/1000 | Loss: 0.00001816
Iteration 24/1000 | Loss: 0.00001816
Iteration 25/1000 | Loss: 0.00001816
Iteration 26/1000 | Loss: 0.00001816
Iteration 27/1000 | Loss: 0.00001816
Iteration 28/1000 | Loss: 0.00001815
Iteration 29/1000 | Loss: 0.00001815
Iteration 30/1000 | Loss: 0.00001815
Iteration 31/1000 | Loss: 0.00001815
Iteration 32/1000 | Loss: 0.00001815
Iteration 33/1000 | Loss: 0.00001813
Iteration 34/1000 | Loss: 0.00001813
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001812
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001812
Iteration 41/1000 | Loss: 0.00001812
Iteration 42/1000 | Loss: 0.00001812
Iteration 43/1000 | Loss: 0.00001812
Iteration 44/1000 | Loss: 0.00001811
Iteration 45/1000 | Loss: 0.00001811
Iteration 46/1000 | Loss: 0.00001811
Iteration 47/1000 | Loss: 0.00001811
Iteration 48/1000 | Loss: 0.00001810
Iteration 49/1000 | Loss: 0.00001810
Iteration 50/1000 | Loss: 0.00001810
Iteration 51/1000 | Loss: 0.00001810
Iteration 52/1000 | Loss: 0.00001810
Iteration 53/1000 | Loss: 0.00001810
Iteration 54/1000 | Loss: 0.00001810
Iteration 55/1000 | Loss: 0.00001810
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001809
Iteration 59/1000 | Loss: 0.00001809
Iteration 60/1000 | Loss: 0.00001809
Iteration 61/1000 | Loss: 0.00001809
Iteration 62/1000 | Loss: 0.00001809
Iteration 63/1000 | Loss: 0.00001809
Iteration 64/1000 | Loss: 0.00001809
Iteration 65/1000 | Loss: 0.00001809
Iteration 66/1000 | Loss: 0.00001809
Iteration 67/1000 | Loss: 0.00001809
Iteration 68/1000 | Loss: 0.00001809
Iteration 69/1000 | Loss: 0.00001808
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001808
Iteration 72/1000 | Loss: 0.00001808
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00001807
Iteration 76/1000 | Loss: 0.00001806
Iteration 77/1000 | Loss: 0.00001806
Iteration 78/1000 | Loss: 0.00001806
Iteration 79/1000 | Loss: 0.00001806
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Iteration 86/1000 | Loss: 0.00001805
Iteration 87/1000 | Loss: 0.00001804
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001804
Iteration 92/1000 | Loss: 0.00001804
Iteration 93/1000 | Loss: 0.00001804
Iteration 94/1000 | Loss: 0.00001804
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001804
Iteration 97/1000 | Loss: 0.00001804
Iteration 98/1000 | Loss: 0.00001804
Iteration 99/1000 | Loss: 0.00001803
Iteration 100/1000 | Loss: 0.00001803
Iteration 101/1000 | Loss: 0.00001803
Iteration 102/1000 | Loss: 0.00001803
Iteration 103/1000 | Loss: 0.00001803
Iteration 104/1000 | Loss: 0.00001803
Iteration 105/1000 | Loss: 0.00001803
Iteration 106/1000 | Loss: 0.00001803
Iteration 107/1000 | Loss: 0.00001803
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001803
Iteration 111/1000 | Loss: 0.00001803
Iteration 112/1000 | Loss: 0.00001803
Iteration 113/1000 | Loss: 0.00001803
Iteration 114/1000 | Loss: 0.00001802
Iteration 115/1000 | Loss: 0.00001802
Iteration 116/1000 | Loss: 0.00001802
Iteration 117/1000 | Loss: 0.00001802
Iteration 118/1000 | Loss: 0.00001802
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001801
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001801
Iteration 132/1000 | Loss: 0.00001801
Iteration 133/1000 | Loss: 0.00001800
Iteration 134/1000 | Loss: 0.00001800
Iteration 135/1000 | Loss: 0.00001800
Iteration 136/1000 | Loss: 0.00001800
Iteration 137/1000 | Loss: 0.00001800
Iteration 138/1000 | Loss: 0.00001800
Iteration 139/1000 | Loss: 0.00001800
Iteration 140/1000 | Loss: 0.00001800
Iteration 141/1000 | Loss: 0.00001800
Iteration 142/1000 | Loss: 0.00001800
Iteration 143/1000 | Loss: 0.00001800
Iteration 144/1000 | Loss: 0.00001800
Iteration 145/1000 | Loss: 0.00001800
Iteration 146/1000 | Loss: 0.00001799
Iteration 147/1000 | Loss: 0.00001799
Iteration 148/1000 | Loss: 0.00001799
Iteration 149/1000 | Loss: 0.00001799
Iteration 150/1000 | Loss: 0.00001799
Iteration 151/1000 | Loss: 0.00001799
Iteration 152/1000 | Loss: 0.00001799
Iteration 153/1000 | Loss: 0.00001799
Iteration 154/1000 | Loss: 0.00001799
Iteration 155/1000 | Loss: 0.00001799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.799248275347054e-05, 1.799248275347054e-05, 1.799248275347054e-05, 1.799248275347054e-05, 1.799248275347054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.799248275347054e-05

Optimization complete. Final v2v error: 3.6720781326293945 mm

Highest mean error: 4.306951999664307 mm for frame 36

Lowest mean error: 3.361551284790039 mm for frame 251

Saving results

Total time: 44.937681674957275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891624
Iteration 2/25 | Loss: 0.00156723
Iteration 3/25 | Loss: 0.00140744
Iteration 4/25 | Loss: 0.00141280
Iteration 5/25 | Loss: 0.00138898
Iteration 6/25 | Loss: 0.00138756
Iteration 7/25 | Loss: 0.00138715
Iteration 8/25 | Loss: 0.00138699
Iteration 9/25 | Loss: 0.00138695
Iteration 10/25 | Loss: 0.00138695
Iteration 11/25 | Loss: 0.00138695
Iteration 12/25 | Loss: 0.00138695
Iteration 13/25 | Loss: 0.00138694
Iteration 14/25 | Loss: 0.00138694
Iteration 15/25 | Loss: 0.00138694
Iteration 16/25 | Loss: 0.00138694
Iteration 17/25 | Loss: 0.00138694
Iteration 18/25 | Loss: 0.00138694
Iteration 19/25 | Loss: 0.00138694
Iteration 20/25 | Loss: 0.00138694
Iteration 21/25 | Loss: 0.00138694
Iteration 22/25 | Loss: 0.00138694
Iteration 23/25 | Loss: 0.00138693
Iteration 24/25 | Loss: 0.00138693
Iteration 25/25 | Loss: 0.00138693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40864849
Iteration 2/25 | Loss: 0.00090339
Iteration 3/25 | Loss: 0.00090339
Iteration 4/25 | Loss: 0.00090339
Iteration 5/25 | Loss: 0.00090338
Iteration 6/25 | Loss: 0.00090338
Iteration 7/25 | Loss: 0.00090338
Iteration 8/25 | Loss: 0.00090338
Iteration 9/25 | Loss: 0.00090338
Iteration 10/25 | Loss: 0.00090338
Iteration 11/25 | Loss: 0.00090338
Iteration 12/25 | Loss: 0.00090338
Iteration 13/25 | Loss: 0.00090338
Iteration 14/25 | Loss: 0.00090338
Iteration 15/25 | Loss: 0.00090338
Iteration 16/25 | Loss: 0.00090338
Iteration 17/25 | Loss: 0.00090338
Iteration 18/25 | Loss: 0.00090338
Iteration 19/25 | Loss: 0.00090338
Iteration 20/25 | Loss: 0.00090338
Iteration 21/25 | Loss: 0.00090338
Iteration 22/25 | Loss: 0.00090338
Iteration 23/25 | Loss: 0.00090338
Iteration 24/25 | Loss: 0.00090338
Iteration 25/25 | Loss: 0.00090338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090338
Iteration 2/1000 | Loss: 0.00003572
Iteration 3/1000 | Loss: 0.00002477
Iteration 4/1000 | Loss: 0.00002231
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00002010
Iteration 7/1000 | Loss: 0.00001952
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001868
Iteration 10/1000 | Loss: 0.00001837
Iteration 11/1000 | Loss: 0.00001810
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001785
Iteration 14/1000 | Loss: 0.00001782
Iteration 15/1000 | Loss: 0.00001781
Iteration 16/1000 | Loss: 0.00001781
Iteration 17/1000 | Loss: 0.00001779
Iteration 18/1000 | Loss: 0.00001777
Iteration 19/1000 | Loss: 0.00001776
Iteration 20/1000 | Loss: 0.00001776
Iteration 21/1000 | Loss: 0.00001774
Iteration 22/1000 | Loss: 0.00001774
Iteration 23/1000 | Loss: 0.00001771
Iteration 24/1000 | Loss: 0.00001771
Iteration 25/1000 | Loss: 0.00001770
Iteration 26/1000 | Loss: 0.00001770
Iteration 27/1000 | Loss: 0.00001770
Iteration 28/1000 | Loss: 0.00001769
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001769
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001768
Iteration 33/1000 | Loss: 0.00001768
Iteration 34/1000 | Loss: 0.00001767
Iteration 35/1000 | Loss: 0.00001767
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001764
Iteration 49/1000 | Loss: 0.00001764
Iteration 50/1000 | Loss: 0.00001764
Iteration 51/1000 | Loss: 0.00001764
Iteration 52/1000 | Loss: 0.00001764
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00001763
Iteration 56/1000 | Loss: 0.00001763
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001762
Iteration 59/1000 | Loss: 0.00001762
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00001760
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001759
Iteration 72/1000 | Loss: 0.00001758
Iteration 73/1000 | Loss: 0.00001758
Iteration 74/1000 | Loss: 0.00001758
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001757
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001756
Iteration 95/1000 | Loss: 0.00001756
Iteration 96/1000 | Loss: 0.00001756
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001756
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001756
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001756
Iteration 104/1000 | Loss: 0.00001756
Iteration 105/1000 | Loss: 0.00001755
Iteration 106/1000 | Loss: 0.00001755
Iteration 107/1000 | Loss: 0.00001755
Iteration 108/1000 | Loss: 0.00001755
Iteration 109/1000 | Loss: 0.00001755
Iteration 110/1000 | Loss: 0.00001755
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001755
Iteration 113/1000 | Loss: 0.00001754
Iteration 114/1000 | Loss: 0.00001754
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Iteration 118/1000 | Loss: 0.00001754
Iteration 119/1000 | Loss: 0.00001754
Iteration 120/1000 | Loss: 0.00001754
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001753
Iteration 123/1000 | Loss: 0.00001753
Iteration 124/1000 | Loss: 0.00001753
Iteration 125/1000 | Loss: 0.00001753
Iteration 126/1000 | Loss: 0.00001753
Iteration 127/1000 | Loss: 0.00001752
Iteration 128/1000 | Loss: 0.00001752
Iteration 129/1000 | Loss: 0.00001752
Iteration 130/1000 | Loss: 0.00001752
Iteration 131/1000 | Loss: 0.00001752
Iteration 132/1000 | Loss: 0.00001752
Iteration 133/1000 | Loss: 0.00001752
Iteration 134/1000 | Loss: 0.00001752
Iteration 135/1000 | Loss: 0.00001752
Iteration 136/1000 | Loss: 0.00001752
Iteration 137/1000 | Loss: 0.00001751
Iteration 138/1000 | Loss: 0.00001751
Iteration 139/1000 | Loss: 0.00001751
Iteration 140/1000 | Loss: 0.00001751
Iteration 141/1000 | Loss: 0.00001751
Iteration 142/1000 | Loss: 0.00001751
Iteration 143/1000 | Loss: 0.00001751
Iteration 144/1000 | Loss: 0.00001751
Iteration 145/1000 | Loss: 0.00001751
Iteration 146/1000 | Loss: 0.00001751
Iteration 147/1000 | Loss: 0.00001751
Iteration 148/1000 | Loss: 0.00001751
Iteration 149/1000 | Loss: 0.00001751
Iteration 150/1000 | Loss: 0.00001751
Iteration 151/1000 | Loss: 0.00001751
Iteration 152/1000 | Loss: 0.00001751
Iteration 153/1000 | Loss: 0.00001751
Iteration 154/1000 | Loss: 0.00001751
Iteration 155/1000 | Loss: 0.00001751
Iteration 156/1000 | Loss: 0.00001751
Iteration 157/1000 | Loss: 0.00001751
Iteration 158/1000 | Loss: 0.00001751
Iteration 159/1000 | Loss: 0.00001751
Iteration 160/1000 | Loss: 0.00001751
Iteration 161/1000 | Loss: 0.00001751
Iteration 162/1000 | Loss: 0.00001751
Iteration 163/1000 | Loss: 0.00001751
Iteration 164/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.7505442883702926e-05, 1.7505442883702926e-05, 1.7505442883702926e-05, 1.7505442883702926e-05, 1.7505442883702926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7505442883702926e-05

Optimization complete. Final v2v error: 3.552626132965088 mm

Highest mean error: 4.149918556213379 mm for frame 68

Lowest mean error: 3.2474560737609863 mm for frame 40

Saving results

Total time: 41.80593752861023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393498
Iteration 2/25 | Loss: 0.00129724
Iteration 3/25 | Loss: 0.00121713
Iteration 4/25 | Loss: 0.00120891
Iteration 5/25 | Loss: 0.00120648
Iteration 6/25 | Loss: 0.00120556
Iteration 7/25 | Loss: 0.00120556
Iteration 8/25 | Loss: 0.00120556
Iteration 9/25 | Loss: 0.00120556
Iteration 10/25 | Loss: 0.00120556
Iteration 11/25 | Loss: 0.00120556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001205560751259327, 0.001205560751259327, 0.001205560751259327, 0.001205560751259327, 0.001205560751259327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001205560751259327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26760781
Iteration 2/25 | Loss: 0.00167053
Iteration 3/25 | Loss: 0.00167053
Iteration 4/25 | Loss: 0.00167053
Iteration 5/25 | Loss: 0.00167053
Iteration 6/25 | Loss: 0.00167053
Iteration 7/25 | Loss: 0.00167053
Iteration 8/25 | Loss: 0.00167053
Iteration 9/25 | Loss: 0.00167053
Iteration 10/25 | Loss: 0.00167053
Iteration 11/25 | Loss: 0.00167052
Iteration 12/25 | Loss: 0.00167053
Iteration 13/25 | Loss: 0.00167053
Iteration 14/25 | Loss: 0.00167053
Iteration 15/25 | Loss: 0.00167052
Iteration 16/25 | Loss: 0.00167052
Iteration 17/25 | Loss: 0.00167052
Iteration 18/25 | Loss: 0.00167052
Iteration 19/25 | Loss: 0.00167052
Iteration 20/25 | Loss: 0.00167052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016705248272046447, 0.0016705248272046447, 0.0016705248272046447, 0.0016705248272046447, 0.0016705248272046447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016705248272046447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167052
Iteration 2/1000 | Loss: 0.00003005
Iteration 3/1000 | Loss: 0.00002265
Iteration 4/1000 | Loss: 0.00002077
Iteration 5/1000 | Loss: 0.00002002
Iteration 6/1000 | Loss: 0.00001927
Iteration 7/1000 | Loss: 0.00001876
Iteration 8/1000 | Loss: 0.00001842
Iteration 9/1000 | Loss: 0.00001829
Iteration 10/1000 | Loss: 0.00001812
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001798
Iteration 14/1000 | Loss: 0.00001797
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001793
Iteration 17/1000 | Loss: 0.00001792
Iteration 18/1000 | Loss: 0.00001792
Iteration 19/1000 | Loss: 0.00001791
Iteration 20/1000 | Loss: 0.00001791
Iteration 21/1000 | Loss: 0.00001791
Iteration 22/1000 | Loss: 0.00001790
Iteration 23/1000 | Loss: 0.00001786
Iteration 24/1000 | Loss: 0.00001786
Iteration 25/1000 | Loss: 0.00001786
Iteration 26/1000 | Loss: 0.00001785
Iteration 27/1000 | Loss: 0.00001784
Iteration 28/1000 | Loss: 0.00001784
Iteration 29/1000 | Loss: 0.00001780
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00001780
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001780
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001779
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001775
Iteration 48/1000 | Loss: 0.00001775
Iteration 49/1000 | Loss: 0.00001775
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001775
Iteration 56/1000 | Loss: 0.00001772
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001770
Iteration 66/1000 | Loss: 0.00001770
Iteration 67/1000 | Loss: 0.00001770
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001770
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001769
Iteration 72/1000 | Loss: 0.00001769
Iteration 73/1000 | Loss: 0.00001768
Iteration 74/1000 | Loss: 0.00001768
Iteration 75/1000 | Loss: 0.00001767
Iteration 76/1000 | Loss: 0.00001767
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001766
Iteration 79/1000 | Loss: 0.00001766
Iteration 80/1000 | Loss: 0.00001766
Iteration 81/1000 | Loss: 0.00001766
Iteration 82/1000 | Loss: 0.00001765
Iteration 83/1000 | Loss: 0.00001765
Iteration 84/1000 | Loss: 0.00001764
Iteration 85/1000 | Loss: 0.00001763
Iteration 86/1000 | Loss: 0.00001762
Iteration 87/1000 | Loss: 0.00001762
Iteration 88/1000 | Loss: 0.00001761
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001761
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001761
Iteration 93/1000 | Loss: 0.00001760
Iteration 94/1000 | Loss: 0.00001760
Iteration 95/1000 | Loss: 0.00001759
Iteration 96/1000 | Loss: 0.00001759
Iteration 97/1000 | Loss: 0.00001759
Iteration 98/1000 | Loss: 0.00001758
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001756
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001755
Iteration 104/1000 | Loss: 0.00001755
Iteration 105/1000 | Loss: 0.00001755
Iteration 106/1000 | Loss: 0.00001755
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001754
Iteration 109/1000 | Loss: 0.00001754
Iteration 110/1000 | Loss: 0.00001754
Iteration 111/1000 | Loss: 0.00001754
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00001753
Iteration 116/1000 | Loss: 0.00001753
Iteration 117/1000 | Loss: 0.00001753
Iteration 118/1000 | Loss: 0.00001753
Iteration 119/1000 | Loss: 0.00001752
Iteration 120/1000 | Loss: 0.00001752
Iteration 121/1000 | Loss: 0.00001752
Iteration 122/1000 | Loss: 0.00001752
Iteration 123/1000 | Loss: 0.00001752
Iteration 124/1000 | Loss: 0.00001752
Iteration 125/1000 | Loss: 0.00001752
Iteration 126/1000 | Loss: 0.00001752
Iteration 127/1000 | Loss: 0.00001752
Iteration 128/1000 | Loss: 0.00001751
Iteration 129/1000 | Loss: 0.00001751
Iteration 130/1000 | Loss: 0.00001751
Iteration 131/1000 | Loss: 0.00001751
Iteration 132/1000 | Loss: 0.00001751
Iteration 133/1000 | Loss: 0.00001751
Iteration 134/1000 | Loss: 0.00001751
Iteration 135/1000 | Loss: 0.00001751
Iteration 136/1000 | Loss: 0.00001751
Iteration 137/1000 | Loss: 0.00001751
Iteration 138/1000 | Loss: 0.00001751
Iteration 139/1000 | Loss: 0.00001751
Iteration 140/1000 | Loss: 0.00001751
Iteration 141/1000 | Loss: 0.00001751
Iteration 142/1000 | Loss: 0.00001751
Iteration 143/1000 | Loss: 0.00001751
Iteration 144/1000 | Loss: 0.00001751
Iteration 145/1000 | Loss: 0.00001751
Iteration 146/1000 | Loss: 0.00001751
Iteration 147/1000 | Loss: 0.00001751
Iteration 148/1000 | Loss: 0.00001751
Iteration 149/1000 | Loss: 0.00001751
Iteration 150/1000 | Loss: 0.00001751
Iteration 151/1000 | Loss: 0.00001751
Iteration 152/1000 | Loss: 0.00001751
Iteration 153/1000 | Loss: 0.00001751
Iteration 154/1000 | Loss: 0.00001751
Iteration 155/1000 | Loss: 0.00001751
Iteration 156/1000 | Loss: 0.00001751
Iteration 157/1000 | Loss: 0.00001751
Iteration 158/1000 | Loss: 0.00001751
Iteration 159/1000 | Loss: 0.00001751
Iteration 160/1000 | Loss: 0.00001751
Iteration 161/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.7510406905785203e-05, 1.7510406905785203e-05, 1.7510406905785203e-05, 1.7510406905785203e-05, 1.7510406905785203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7510406905785203e-05

Optimization complete. Final v2v error: 3.5243120193481445 mm

Highest mean error: 3.878830671310425 mm for frame 175

Lowest mean error: 3.2565715312957764 mm for frame 98

Saving results

Total time: 36.02917838096619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00577660
Iteration 2/25 | Loss: 0.00141430
Iteration 3/25 | Loss: 0.00132075
Iteration 4/25 | Loss: 0.00131267
Iteration 5/25 | Loss: 0.00131027
Iteration 6/25 | Loss: 0.00131027
Iteration 7/25 | Loss: 0.00131027
Iteration 8/25 | Loss: 0.00131027
Iteration 9/25 | Loss: 0.00131027
Iteration 10/25 | Loss: 0.00131027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013102733064442873, 0.0013102733064442873, 0.0013102733064442873, 0.0013102733064442873, 0.0013102733064442873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013102733064442873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26693308
Iteration 2/25 | Loss: 0.00141523
Iteration 3/25 | Loss: 0.00141522
Iteration 4/25 | Loss: 0.00141522
Iteration 5/25 | Loss: 0.00141522
Iteration 6/25 | Loss: 0.00141522
Iteration 7/25 | Loss: 0.00141522
Iteration 8/25 | Loss: 0.00141522
Iteration 9/25 | Loss: 0.00141522
Iteration 10/25 | Loss: 0.00141522
Iteration 11/25 | Loss: 0.00141522
Iteration 12/25 | Loss: 0.00141522
Iteration 13/25 | Loss: 0.00141522
Iteration 14/25 | Loss: 0.00141522
Iteration 15/25 | Loss: 0.00141522
Iteration 16/25 | Loss: 0.00141522
Iteration 17/25 | Loss: 0.00141522
Iteration 18/25 | Loss: 0.00141522
Iteration 19/25 | Loss: 0.00141522
Iteration 20/25 | Loss: 0.00141522
Iteration 21/25 | Loss: 0.00141522
Iteration 22/25 | Loss: 0.00141522
Iteration 23/25 | Loss: 0.00141522
Iteration 24/25 | Loss: 0.00141522
Iteration 25/25 | Loss: 0.00141522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141522
Iteration 2/1000 | Loss: 0.00005888
Iteration 3/1000 | Loss: 0.00004781
Iteration 4/1000 | Loss: 0.00004262
Iteration 5/1000 | Loss: 0.00004119
Iteration 6/1000 | Loss: 0.00004017
Iteration 7/1000 | Loss: 0.00003966
Iteration 8/1000 | Loss: 0.00003918
Iteration 9/1000 | Loss: 0.00003887
Iteration 10/1000 | Loss: 0.00003869
Iteration 11/1000 | Loss: 0.00003855
Iteration 12/1000 | Loss: 0.00003852
Iteration 13/1000 | Loss: 0.00003841
Iteration 14/1000 | Loss: 0.00003828
Iteration 15/1000 | Loss: 0.00003826
Iteration 16/1000 | Loss: 0.00003812
Iteration 17/1000 | Loss: 0.00003802
Iteration 18/1000 | Loss: 0.00003796
Iteration 19/1000 | Loss: 0.00003796
Iteration 20/1000 | Loss: 0.00003792
Iteration 21/1000 | Loss: 0.00003792
Iteration 22/1000 | Loss: 0.00003785
Iteration 23/1000 | Loss: 0.00003783
Iteration 24/1000 | Loss: 0.00003779
Iteration 25/1000 | Loss: 0.00003774
Iteration 26/1000 | Loss: 0.00003770
Iteration 27/1000 | Loss: 0.00003769
Iteration 28/1000 | Loss: 0.00003759
Iteration 29/1000 | Loss: 0.00003757
Iteration 30/1000 | Loss: 0.00003750
Iteration 31/1000 | Loss: 0.00003749
Iteration 32/1000 | Loss: 0.00003748
Iteration 33/1000 | Loss: 0.00003746
Iteration 34/1000 | Loss: 0.00003746
Iteration 35/1000 | Loss: 0.00003746
Iteration 36/1000 | Loss: 0.00003746
Iteration 37/1000 | Loss: 0.00003746
Iteration 38/1000 | Loss: 0.00003746
Iteration 39/1000 | Loss: 0.00003746
Iteration 40/1000 | Loss: 0.00003746
Iteration 41/1000 | Loss: 0.00003745
Iteration 42/1000 | Loss: 0.00003745
Iteration 43/1000 | Loss: 0.00003745
Iteration 44/1000 | Loss: 0.00003745
Iteration 45/1000 | Loss: 0.00003745
Iteration 46/1000 | Loss: 0.00003745
Iteration 47/1000 | Loss: 0.00003745
Iteration 48/1000 | Loss: 0.00003745
Iteration 49/1000 | Loss: 0.00003743
Iteration 50/1000 | Loss: 0.00003743
Iteration 51/1000 | Loss: 0.00003743
Iteration 52/1000 | Loss: 0.00003743
Iteration 53/1000 | Loss: 0.00003743
Iteration 54/1000 | Loss: 0.00003743
Iteration 55/1000 | Loss: 0.00003743
Iteration 56/1000 | Loss: 0.00003743
Iteration 57/1000 | Loss: 0.00003743
Iteration 58/1000 | Loss: 0.00003743
Iteration 59/1000 | Loss: 0.00003743
Iteration 60/1000 | Loss: 0.00003742
Iteration 61/1000 | Loss: 0.00003742
Iteration 62/1000 | Loss: 0.00003742
Iteration 63/1000 | Loss: 0.00003742
Iteration 64/1000 | Loss: 0.00003742
Iteration 65/1000 | Loss: 0.00003742
Iteration 66/1000 | Loss: 0.00003742
Iteration 67/1000 | Loss: 0.00003741
Iteration 68/1000 | Loss: 0.00003741
Iteration 69/1000 | Loss: 0.00003741
Iteration 70/1000 | Loss: 0.00003741
Iteration 71/1000 | Loss: 0.00003741
Iteration 72/1000 | Loss: 0.00003741
Iteration 73/1000 | Loss: 0.00003741
Iteration 74/1000 | Loss: 0.00003741
Iteration 75/1000 | Loss: 0.00003741
Iteration 76/1000 | Loss: 0.00003741
Iteration 77/1000 | Loss: 0.00003740
Iteration 78/1000 | Loss: 0.00003740
Iteration 79/1000 | Loss: 0.00003740
Iteration 80/1000 | Loss: 0.00003740
Iteration 81/1000 | Loss: 0.00003740
Iteration 82/1000 | Loss: 0.00003740
Iteration 83/1000 | Loss: 0.00003740
Iteration 84/1000 | Loss: 0.00003739
Iteration 85/1000 | Loss: 0.00003739
Iteration 86/1000 | Loss: 0.00003739
Iteration 87/1000 | Loss: 0.00003738
Iteration 88/1000 | Loss: 0.00003738
Iteration 89/1000 | Loss: 0.00003737
Iteration 90/1000 | Loss: 0.00003737
Iteration 91/1000 | Loss: 0.00003737
Iteration 92/1000 | Loss: 0.00003737
Iteration 93/1000 | Loss: 0.00003737
Iteration 94/1000 | Loss: 0.00003736
Iteration 95/1000 | Loss: 0.00003736
Iteration 96/1000 | Loss: 0.00003735
Iteration 97/1000 | Loss: 0.00003735
Iteration 98/1000 | Loss: 0.00003735
Iteration 99/1000 | Loss: 0.00003735
Iteration 100/1000 | Loss: 0.00003735
Iteration 101/1000 | Loss: 0.00003735
Iteration 102/1000 | Loss: 0.00003735
Iteration 103/1000 | Loss: 0.00003735
Iteration 104/1000 | Loss: 0.00003735
Iteration 105/1000 | Loss: 0.00003734
Iteration 106/1000 | Loss: 0.00003734
Iteration 107/1000 | Loss: 0.00003734
Iteration 108/1000 | Loss: 0.00003734
Iteration 109/1000 | Loss: 0.00003733
Iteration 110/1000 | Loss: 0.00003733
Iteration 111/1000 | Loss: 0.00003732
Iteration 112/1000 | Loss: 0.00003732
Iteration 113/1000 | Loss: 0.00003732
Iteration 114/1000 | Loss: 0.00003732
Iteration 115/1000 | Loss: 0.00003732
Iteration 116/1000 | Loss: 0.00003732
Iteration 117/1000 | Loss: 0.00003732
Iteration 118/1000 | Loss: 0.00003732
Iteration 119/1000 | Loss: 0.00003732
Iteration 120/1000 | Loss: 0.00003732
Iteration 121/1000 | Loss: 0.00003732
Iteration 122/1000 | Loss: 0.00003731
Iteration 123/1000 | Loss: 0.00003731
Iteration 124/1000 | Loss: 0.00003731
Iteration 125/1000 | Loss: 0.00003731
Iteration 126/1000 | Loss: 0.00003731
Iteration 127/1000 | Loss: 0.00003730
Iteration 128/1000 | Loss: 0.00003730
Iteration 129/1000 | Loss: 0.00003730
Iteration 130/1000 | Loss: 0.00003730
Iteration 131/1000 | Loss: 0.00003729
Iteration 132/1000 | Loss: 0.00003729
Iteration 133/1000 | Loss: 0.00003729
Iteration 134/1000 | Loss: 0.00003729
Iteration 135/1000 | Loss: 0.00003729
Iteration 136/1000 | Loss: 0.00003729
Iteration 137/1000 | Loss: 0.00003729
Iteration 138/1000 | Loss: 0.00003729
Iteration 139/1000 | Loss: 0.00003728
Iteration 140/1000 | Loss: 0.00003728
Iteration 141/1000 | Loss: 0.00003728
Iteration 142/1000 | Loss: 0.00003727
Iteration 143/1000 | Loss: 0.00003727
Iteration 144/1000 | Loss: 0.00003727
Iteration 145/1000 | Loss: 0.00003727
Iteration 146/1000 | Loss: 0.00003727
Iteration 147/1000 | Loss: 0.00003726
Iteration 148/1000 | Loss: 0.00003726
Iteration 149/1000 | Loss: 0.00003725
Iteration 150/1000 | Loss: 0.00003725
Iteration 151/1000 | Loss: 0.00003725
Iteration 152/1000 | Loss: 0.00003725
Iteration 153/1000 | Loss: 0.00003725
Iteration 154/1000 | Loss: 0.00003725
Iteration 155/1000 | Loss: 0.00003725
Iteration 156/1000 | Loss: 0.00003725
Iteration 157/1000 | Loss: 0.00003725
Iteration 158/1000 | Loss: 0.00003725
Iteration 159/1000 | Loss: 0.00003725
Iteration 160/1000 | Loss: 0.00003725
Iteration 161/1000 | Loss: 0.00003724
Iteration 162/1000 | Loss: 0.00003724
Iteration 163/1000 | Loss: 0.00003724
Iteration 164/1000 | Loss: 0.00003724
Iteration 165/1000 | Loss: 0.00003724
Iteration 166/1000 | Loss: 0.00003724
Iteration 167/1000 | Loss: 0.00003724
Iteration 168/1000 | Loss: 0.00003723
Iteration 169/1000 | Loss: 0.00003723
Iteration 170/1000 | Loss: 0.00003723
Iteration 171/1000 | Loss: 0.00003723
Iteration 172/1000 | Loss: 0.00003723
Iteration 173/1000 | Loss: 0.00003723
Iteration 174/1000 | Loss: 0.00003722
Iteration 175/1000 | Loss: 0.00003722
Iteration 176/1000 | Loss: 0.00003722
Iteration 177/1000 | Loss: 0.00003722
Iteration 178/1000 | Loss: 0.00003722
Iteration 179/1000 | Loss: 0.00003722
Iteration 180/1000 | Loss: 0.00003722
Iteration 181/1000 | Loss: 0.00003722
Iteration 182/1000 | Loss: 0.00003722
Iteration 183/1000 | Loss: 0.00003722
Iteration 184/1000 | Loss: 0.00003722
Iteration 185/1000 | Loss: 0.00003721
Iteration 186/1000 | Loss: 0.00003721
Iteration 187/1000 | Loss: 0.00003721
Iteration 188/1000 | Loss: 0.00003721
Iteration 189/1000 | Loss: 0.00003721
Iteration 190/1000 | Loss: 0.00003721
Iteration 191/1000 | Loss: 0.00003721
Iteration 192/1000 | Loss: 0.00003721
Iteration 193/1000 | Loss: 0.00003721
Iteration 194/1000 | Loss: 0.00003721
Iteration 195/1000 | Loss: 0.00003721
Iteration 196/1000 | Loss: 0.00003721
Iteration 197/1000 | Loss: 0.00003721
Iteration 198/1000 | Loss: 0.00003721
Iteration 199/1000 | Loss: 0.00003721
Iteration 200/1000 | Loss: 0.00003721
Iteration 201/1000 | Loss: 0.00003721
Iteration 202/1000 | Loss: 0.00003721
Iteration 203/1000 | Loss: 0.00003721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [3.721052053151652e-05, 3.721052053151652e-05, 3.721052053151652e-05, 3.721052053151652e-05, 3.721052053151652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.721052053151652e-05

Optimization complete. Final v2v error: 4.826968669891357 mm

Highest mean error: 5.391488552093506 mm for frame 24

Lowest mean error: 4.191892623901367 mm for frame 80

Saving results

Total time: 54.56905269622803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863824
Iteration 2/25 | Loss: 0.00153658
Iteration 3/25 | Loss: 0.00131999
Iteration 4/25 | Loss: 0.00129224
Iteration 5/25 | Loss: 0.00129728
Iteration 6/25 | Loss: 0.00128560
Iteration 7/25 | Loss: 0.00127656
Iteration 8/25 | Loss: 0.00127196
Iteration 9/25 | Loss: 0.00126742
Iteration 10/25 | Loss: 0.00126638
Iteration 11/25 | Loss: 0.00126612
Iteration 12/25 | Loss: 0.00126610
Iteration 13/25 | Loss: 0.00126610
Iteration 14/25 | Loss: 0.00126610
Iteration 15/25 | Loss: 0.00126610
Iteration 16/25 | Loss: 0.00126610
Iteration 17/25 | Loss: 0.00126610
Iteration 18/25 | Loss: 0.00126610
Iteration 19/25 | Loss: 0.00126610
Iteration 20/25 | Loss: 0.00126610
Iteration 21/25 | Loss: 0.00126610
Iteration 22/25 | Loss: 0.00126610
Iteration 23/25 | Loss: 0.00126610
Iteration 24/25 | Loss: 0.00126610
Iteration 25/25 | Loss: 0.00126610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22836876
Iteration 2/25 | Loss: 0.00170582
Iteration 3/25 | Loss: 0.00170582
Iteration 4/25 | Loss: 0.00170582
Iteration 5/25 | Loss: 0.00170582
Iteration 6/25 | Loss: 0.00170582
Iteration 7/25 | Loss: 0.00170582
Iteration 8/25 | Loss: 0.00170582
Iteration 9/25 | Loss: 0.00170582
Iteration 10/25 | Loss: 0.00170582
Iteration 11/25 | Loss: 0.00170582
Iteration 12/25 | Loss: 0.00170582
Iteration 13/25 | Loss: 0.00170582
Iteration 14/25 | Loss: 0.00170582
Iteration 15/25 | Loss: 0.00170582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0017058165976777673, 0.0017058165976777673, 0.0017058165976777673, 0.0017058165976777673, 0.0017058165976777673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017058165976777673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170582
Iteration 2/1000 | Loss: 0.00005707
Iteration 3/1000 | Loss: 0.00003519
Iteration 4/1000 | Loss: 0.00002577
Iteration 5/1000 | Loss: 0.00002308
Iteration 6/1000 | Loss: 0.00002181
Iteration 7/1000 | Loss: 0.00002091
Iteration 8/1000 | Loss: 0.00002048
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001961
Iteration 12/1000 | Loss: 0.00001961
Iteration 13/1000 | Loss: 0.00001942
Iteration 14/1000 | Loss: 0.00001941
Iteration 15/1000 | Loss: 0.00001939
Iteration 16/1000 | Loss: 0.00001935
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001931
Iteration 19/1000 | Loss: 0.00001927
Iteration 20/1000 | Loss: 0.00001923
Iteration 21/1000 | Loss: 0.00001922
Iteration 22/1000 | Loss: 0.00001921
Iteration 23/1000 | Loss: 0.00001921
Iteration 24/1000 | Loss: 0.00001920
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00001915
Iteration 29/1000 | Loss: 0.00001915
Iteration 30/1000 | Loss: 0.00001915
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00001914
Iteration 33/1000 | Loss: 0.00001913
Iteration 34/1000 | Loss: 0.00001913
Iteration 35/1000 | Loss: 0.00001912
Iteration 36/1000 | Loss: 0.00001912
Iteration 37/1000 | Loss: 0.00001912
Iteration 38/1000 | Loss: 0.00001911
Iteration 39/1000 | Loss: 0.00001911
Iteration 40/1000 | Loss: 0.00001910
Iteration 41/1000 | Loss: 0.00001910
Iteration 42/1000 | Loss: 0.00001909
Iteration 43/1000 | Loss: 0.00001909
Iteration 44/1000 | Loss: 0.00001909
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001908
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001905
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001905
Iteration 60/1000 | Loss: 0.00001905
Iteration 61/1000 | Loss: 0.00001905
Iteration 62/1000 | Loss: 0.00001904
Iteration 63/1000 | Loss: 0.00001904
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001904
Iteration 68/1000 | Loss: 0.00001903
Iteration 69/1000 | Loss: 0.00001903
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001903
Iteration 73/1000 | Loss: 0.00001903
Iteration 74/1000 | Loss: 0.00001903
Iteration 75/1000 | Loss: 0.00001903
Iteration 76/1000 | Loss: 0.00001903
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001903
Iteration 79/1000 | Loss: 0.00001903
Iteration 80/1000 | Loss: 0.00001903
Iteration 81/1000 | Loss: 0.00001903
Iteration 82/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.9033072021557018e-05, 1.9033072021557018e-05, 1.9033072021557018e-05, 1.9033072021557018e-05, 1.9033072021557018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9033072021557018e-05

Optimization complete. Final v2v error: 3.64365291595459 mm

Highest mean error: 4.237668514251709 mm for frame 19

Lowest mean error: 3.141284704208374 mm for frame 236

Saving results

Total time: 48.08243751525879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423764
Iteration 2/25 | Loss: 0.00132704
Iteration 3/25 | Loss: 0.00124296
Iteration 4/25 | Loss: 0.00123327
Iteration 5/25 | Loss: 0.00123001
Iteration 6/25 | Loss: 0.00122914
Iteration 7/25 | Loss: 0.00122914
Iteration 8/25 | Loss: 0.00122914
Iteration 9/25 | Loss: 0.00122914
Iteration 10/25 | Loss: 0.00122914
Iteration 11/25 | Loss: 0.00122914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012291416060179472, 0.0012291416060179472, 0.0012291416060179472, 0.0012291416060179472, 0.0012291416060179472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012291416060179472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75972009
Iteration 2/25 | Loss: 0.00168008
Iteration 3/25 | Loss: 0.00168008
Iteration 4/25 | Loss: 0.00168008
Iteration 5/25 | Loss: 0.00168008
Iteration 6/25 | Loss: 0.00168008
Iteration 7/25 | Loss: 0.00168008
Iteration 8/25 | Loss: 0.00168008
Iteration 9/25 | Loss: 0.00168008
Iteration 10/25 | Loss: 0.00168008
Iteration 11/25 | Loss: 0.00168008
Iteration 12/25 | Loss: 0.00168008
Iteration 13/25 | Loss: 0.00168008
Iteration 14/25 | Loss: 0.00168008
Iteration 15/25 | Loss: 0.00168008
Iteration 16/25 | Loss: 0.00168008
Iteration 17/25 | Loss: 0.00168008
Iteration 18/25 | Loss: 0.00168008
Iteration 19/25 | Loss: 0.00168008
Iteration 20/25 | Loss: 0.00168008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00168007577303797, 0.00168007577303797, 0.00168007577303797, 0.00168007577303797, 0.00168007577303797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00168007577303797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168008
Iteration 2/1000 | Loss: 0.00003042
Iteration 3/1000 | Loss: 0.00002171
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001882
Iteration 6/1000 | Loss: 0.00001825
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001764
Iteration 10/1000 | Loss: 0.00001763
Iteration 11/1000 | Loss: 0.00001762
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001757
Iteration 15/1000 | Loss: 0.00001757
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001751
Iteration 20/1000 | Loss: 0.00001747
Iteration 21/1000 | Loss: 0.00001747
Iteration 22/1000 | Loss: 0.00001747
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001746
Iteration 25/1000 | Loss: 0.00001745
Iteration 26/1000 | Loss: 0.00001745
Iteration 27/1000 | Loss: 0.00001745
Iteration 28/1000 | Loss: 0.00001744
Iteration 29/1000 | Loss: 0.00001744
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001742
Iteration 33/1000 | Loss: 0.00001742
Iteration 34/1000 | Loss: 0.00001742
Iteration 35/1000 | Loss: 0.00001741
Iteration 36/1000 | Loss: 0.00001741
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001740
Iteration 39/1000 | Loss: 0.00001740
Iteration 40/1000 | Loss: 0.00001739
Iteration 41/1000 | Loss: 0.00001739
Iteration 42/1000 | Loss: 0.00001739
Iteration 43/1000 | Loss: 0.00001738
Iteration 44/1000 | Loss: 0.00001738
Iteration 45/1000 | Loss: 0.00001738
Iteration 46/1000 | Loss: 0.00001738
Iteration 47/1000 | Loss: 0.00001737
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00001736
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001735
Iteration 57/1000 | Loss: 0.00001735
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001735
Iteration 60/1000 | Loss: 0.00001735
Iteration 61/1000 | Loss: 0.00001735
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001733
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001732
Iteration 82/1000 | Loss: 0.00001732
Iteration 83/1000 | Loss: 0.00001732
Iteration 84/1000 | Loss: 0.00001731
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001730
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001730
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001728
Iteration 106/1000 | Loss: 0.00001728
Iteration 107/1000 | Loss: 0.00001728
Iteration 108/1000 | Loss: 0.00001727
Iteration 109/1000 | Loss: 0.00001727
Iteration 110/1000 | Loss: 0.00001727
Iteration 111/1000 | Loss: 0.00001727
Iteration 112/1000 | Loss: 0.00001727
Iteration 113/1000 | Loss: 0.00001727
Iteration 114/1000 | Loss: 0.00001727
Iteration 115/1000 | Loss: 0.00001726
Iteration 116/1000 | Loss: 0.00001726
Iteration 117/1000 | Loss: 0.00001726
Iteration 118/1000 | Loss: 0.00001726
Iteration 119/1000 | Loss: 0.00001726
Iteration 120/1000 | Loss: 0.00001726
Iteration 121/1000 | Loss: 0.00001725
Iteration 122/1000 | Loss: 0.00001725
Iteration 123/1000 | Loss: 0.00001725
Iteration 124/1000 | Loss: 0.00001725
Iteration 125/1000 | Loss: 0.00001725
Iteration 126/1000 | Loss: 0.00001725
Iteration 127/1000 | Loss: 0.00001725
Iteration 128/1000 | Loss: 0.00001725
Iteration 129/1000 | Loss: 0.00001725
Iteration 130/1000 | Loss: 0.00001725
Iteration 131/1000 | Loss: 0.00001725
Iteration 132/1000 | Loss: 0.00001724
Iteration 133/1000 | Loss: 0.00001724
Iteration 134/1000 | Loss: 0.00001724
Iteration 135/1000 | Loss: 0.00001724
Iteration 136/1000 | Loss: 0.00001724
Iteration 137/1000 | Loss: 0.00001724
Iteration 138/1000 | Loss: 0.00001724
Iteration 139/1000 | Loss: 0.00001724
Iteration 140/1000 | Loss: 0.00001724
Iteration 141/1000 | Loss: 0.00001724
Iteration 142/1000 | Loss: 0.00001724
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Iteration 145/1000 | Loss: 0.00001723
Iteration 146/1000 | Loss: 0.00001723
Iteration 147/1000 | Loss: 0.00001723
Iteration 148/1000 | Loss: 0.00001723
Iteration 149/1000 | Loss: 0.00001723
Iteration 150/1000 | Loss: 0.00001723
Iteration 151/1000 | Loss: 0.00001723
Iteration 152/1000 | Loss: 0.00001723
Iteration 153/1000 | Loss: 0.00001723
Iteration 154/1000 | Loss: 0.00001722
Iteration 155/1000 | Loss: 0.00001722
Iteration 156/1000 | Loss: 0.00001722
Iteration 157/1000 | Loss: 0.00001722
Iteration 158/1000 | Loss: 0.00001722
Iteration 159/1000 | Loss: 0.00001722
Iteration 160/1000 | Loss: 0.00001722
Iteration 161/1000 | Loss: 0.00001722
Iteration 162/1000 | Loss: 0.00001722
Iteration 163/1000 | Loss: 0.00001722
Iteration 164/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.7222588212462142e-05, 1.7222588212462142e-05, 1.7222588212462142e-05, 1.7222588212462142e-05, 1.7222588212462142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7222588212462142e-05

Optimization complete. Final v2v error: 3.511516571044922 mm

Highest mean error: 3.8903191089630127 mm for frame 89

Lowest mean error: 3.234666347503662 mm for frame 10

Saving results

Total time: 34.10645127296448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090062
Iteration 2/25 | Loss: 0.00270879
Iteration 3/25 | Loss: 0.00325681
Iteration 4/25 | Loss: 0.00156030
Iteration 5/25 | Loss: 0.00145836
Iteration 6/25 | Loss: 0.00136749
Iteration 7/25 | Loss: 0.00130596
Iteration 8/25 | Loss: 0.00128269
Iteration 9/25 | Loss: 0.00126411
Iteration 10/25 | Loss: 0.00126453
Iteration 11/25 | Loss: 0.00125638
Iteration 12/25 | Loss: 0.00124927
Iteration 13/25 | Loss: 0.00124204
Iteration 14/25 | Loss: 0.00124241
Iteration 15/25 | Loss: 0.00124540
Iteration 16/25 | Loss: 0.00124868
Iteration 17/25 | Loss: 0.00124357
Iteration 18/25 | Loss: 0.00123410
Iteration 19/25 | Loss: 0.00123457
Iteration 20/25 | Loss: 0.00123319
Iteration 21/25 | Loss: 0.00123276
Iteration 22/25 | Loss: 0.00122655
Iteration 23/25 | Loss: 0.00122535
Iteration 24/25 | Loss: 0.00123235
Iteration 25/25 | Loss: 0.00123182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32940745
Iteration 2/25 | Loss: 0.00156269
Iteration 3/25 | Loss: 0.00156269
Iteration 4/25 | Loss: 0.00156269
Iteration 5/25 | Loss: 0.00156269
Iteration 6/25 | Loss: 0.00156268
Iteration 7/25 | Loss: 0.00156268
Iteration 8/25 | Loss: 0.00156268
Iteration 9/25 | Loss: 0.00156268
Iteration 10/25 | Loss: 0.00156268
Iteration 11/25 | Loss: 0.00156268
Iteration 12/25 | Loss: 0.00156268
Iteration 13/25 | Loss: 0.00156268
Iteration 14/25 | Loss: 0.00156268
Iteration 15/25 | Loss: 0.00156268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015626837266609073, 0.0015626837266609073, 0.0015626837266609073, 0.0015626837266609073, 0.0015626837266609073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015626837266609073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156268
Iteration 2/1000 | Loss: 0.00066302
Iteration 3/1000 | Loss: 0.00049804
Iteration 4/1000 | Loss: 0.00048745
Iteration 5/1000 | Loss: 0.00015297
Iteration 6/1000 | Loss: 0.00019863
Iteration 7/1000 | Loss: 0.00018695
Iteration 8/1000 | Loss: 0.00034520
Iteration 9/1000 | Loss: 0.00022575
Iteration 10/1000 | Loss: 0.00030556
Iteration 11/1000 | Loss: 0.00024353
Iteration 12/1000 | Loss: 0.00032566
Iteration 13/1000 | Loss: 0.00031965
Iteration 14/1000 | Loss: 0.00035965
Iteration 15/1000 | Loss: 0.00034483
Iteration 16/1000 | Loss: 0.00022229
Iteration 17/1000 | Loss: 0.00052344
Iteration 18/1000 | Loss: 0.00081359
Iteration 19/1000 | Loss: 0.00039280
Iteration 20/1000 | Loss: 0.00020391
Iteration 21/1000 | Loss: 0.00076663
Iteration 22/1000 | Loss: 0.00091740
Iteration 23/1000 | Loss: 0.00052611
Iteration 24/1000 | Loss: 0.00106893
Iteration 25/1000 | Loss: 0.00015300
Iteration 26/1000 | Loss: 0.00029232
Iteration 27/1000 | Loss: 0.00020575
Iteration 28/1000 | Loss: 0.00131578
Iteration 29/1000 | Loss: 0.00080857
Iteration 30/1000 | Loss: 0.00009294
Iteration 31/1000 | Loss: 0.00010006
Iteration 32/1000 | Loss: 0.00004802
Iteration 33/1000 | Loss: 0.00050821
Iteration 34/1000 | Loss: 0.00022283
Iteration 35/1000 | Loss: 0.00017152
Iteration 36/1000 | Loss: 0.00011542
Iteration 37/1000 | Loss: 0.00011774
Iteration 38/1000 | Loss: 0.00023208
Iteration 39/1000 | Loss: 0.00015634
Iteration 40/1000 | Loss: 0.00022225
Iteration 41/1000 | Loss: 0.00023719
Iteration 42/1000 | Loss: 0.00020654
Iteration 43/1000 | Loss: 0.00005882
Iteration 44/1000 | Loss: 0.00029833
Iteration 45/1000 | Loss: 0.00016933
Iteration 46/1000 | Loss: 0.00023586
Iteration 47/1000 | Loss: 0.00027787
Iteration 48/1000 | Loss: 0.00014456
Iteration 49/1000 | Loss: 0.00033595
Iteration 50/1000 | Loss: 0.00018859
Iteration 51/1000 | Loss: 0.00005212
Iteration 52/1000 | Loss: 0.00054148
Iteration 53/1000 | Loss: 0.00016856
Iteration 54/1000 | Loss: 0.00019283
Iteration 55/1000 | Loss: 0.00008909
Iteration 56/1000 | Loss: 0.00002933
Iteration 57/1000 | Loss: 0.00002685
Iteration 58/1000 | Loss: 0.00002502
Iteration 59/1000 | Loss: 0.00006522
Iteration 60/1000 | Loss: 0.00014998
Iteration 61/1000 | Loss: 0.00010368
Iteration 62/1000 | Loss: 0.00002666
Iteration 63/1000 | Loss: 0.00020599
Iteration 64/1000 | Loss: 0.00006281
Iteration 65/1000 | Loss: 0.00012785
Iteration 66/1000 | Loss: 0.00019617
Iteration 67/1000 | Loss: 0.00005775
Iteration 68/1000 | Loss: 0.00014761
Iteration 69/1000 | Loss: 0.00009585
Iteration 70/1000 | Loss: 0.00004050
Iteration 71/1000 | Loss: 0.00007991
Iteration 72/1000 | Loss: 0.00021608
Iteration 73/1000 | Loss: 0.00007488
Iteration 74/1000 | Loss: 0.00002914
Iteration 75/1000 | Loss: 0.00004946
Iteration 76/1000 | Loss: 0.00021943
Iteration 77/1000 | Loss: 0.00004345
Iteration 78/1000 | Loss: 0.00004326
Iteration 79/1000 | Loss: 0.00002739
Iteration 80/1000 | Loss: 0.00002651
Iteration 81/1000 | Loss: 0.00024459
Iteration 82/1000 | Loss: 0.00007314
Iteration 83/1000 | Loss: 0.00011984
Iteration 84/1000 | Loss: 0.00024516
Iteration 85/1000 | Loss: 0.00020018
Iteration 86/1000 | Loss: 0.00016636
Iteration 87/1000 | Loss: 0.00018051
Iteration 88/1000 | Loss: 0.00016153
Iteration 89/1000 | Loss: 0.00002347
Iteration 90/1000 | Loss: 0.00002245
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002113
Iteration 93/1000 | Loss: 0.00002080
Iteration 94/1000 | Loss: 0.00002042
Iteration 95/1000 | Loss: 0.00053987
Iteration 96/1000 | Loss: 0.00030913
Iteration 97/1000 | Loss: 0.00013723
Iteration 98/1000 | Loss: 0.00020348
Iteration 99/1000 | Loss: 0.00023939
Iteration 100/1000 | Loss: 0.00033060
Iteration 101/1000 | Loss: 0.00004183
Iteration 102/1000 | Loss: 0.00003402
Iteration 103/1000 | Loss: 0.00006401
Iteration 104/1000 | Loss: 0.00002831
Iteration 105/1000 | Loss: 0.00002615
Iteration 106/1000 | Loss: 0.00002465
Iteration 107/1000 | Loss: 0.00024576
Iteration 108/1000 | Loss: 0.00033616
Iteration 109/1000 | Loss: 0.00015946
Iteration 110/1000 | Loss: 0.00028906
Iteration 111/1000 | Loss: 0.00002540
Iteration 112/1000 | Loss: 0.00025328
Iteration 113/1000 | Loss: 0.00012295
Iteration 114/1000 | Loss: 0.00006779
Iteration 115/1000 | Loss: 0.00011181
Iteration 116/1000 | Loss: 0.00006548
Iteration 117/1000 | Loss: 0.00003407
Iteration 118/1000 | Loss: 0.00012646
Iteration 119/1000 | Loss: 0.00005726
Iteration 120/1000 | Loss: 0.00002767
Iteration 121/1000 | Loss: 0.00014362
Iteration 122/1000 | Loss: 0.00003495
Iteration 123/1000 | Loss: 0.00013968
Iteration 124/1000 | Loss: 0.00003764
Iteration 125/1000 | Loss: 0.00012983
Iteration 126/1000 | Loss: 0.00030184
Iteration 127/1000 | Loss: 0.00015128
Iteration 128/1000 | Loss: 0.00014796
Iteration 129/1000 | Loss: 0.00018325
Iteration 130/1000 | Loss: 0.00020328
Iteration 131/1000 | Loss: 0.00005415
Iteration 132/1000 | Loss: 0.00003057
Iteration 133/1000 | Loss: 0.00004424
Iteration 134/1000 | Loss: 0.00019103
Iteration 135/1000 | Loss: 0.00005311
Iteration 136/1000 | Loss: 0.00009096
Iteration 137/1000 | Loss: 0.00027026
Iteration 138/1000 | Loss: 0.00003758
Iteration 139/1000 | Loss: 0.00002701
Iteration 140/1000 | Loss: 0.00002214
Iteration 141/1000 | Loss: 0.00001978
Iteration 142/1000 | Loss: 0.00001871
Iteration 143/1000 | Loss: 0.00002429
Iteration 144/1000 | Loss: 0.00004633
Iteration 145/1000 | Loss: 0.00002440
Iteration 146/1000 | Loss: 0.00003002
Iteration 147/1000 | Loss: 0.00024134
Iteration 148/1000 | Loss: 0.00008746
Iteration 149/1000 | Loss: 0.00021221
Iteration 150/1000 | Loss: 0.00011894
Iteration 151/1000 | Loss: 0.00002113
Iteration 152/1000 | Loss: 0.00002409
Iteration 153/1000 | Loss: 0.00002011
Iteration 154/1000 | Loss: 0.00002039
Iteration 155/1000 | Loss: 0.00004052
Iteration 156/1000 | Loss: 0.00004186
Iteration 157/1000 | Loss: 0.00003897
Iteration 158/1000 | Loss: 0.00004204
Iteration 159/1000 | Loss: 0.00002341
Iteration 160/1000 | Loss: 0.00004238
Iteration 161/1000 | Loss: 0.00004313
Iteration 162/1000 | Loss: 0.00003721
Iteration 163/1000 | Loss: 0.00004235
Iteration 164/1000 | Loss: 0.00002947
Iteration 165/1000 | Loss: 0.00003485
Iteration 166/1000 | Loss: 0.00004907
Iteration 167/1000 | Loss: 0.00003775
Iteration 168/1000 | Loss: 0.00004540
Iteration 169/1000 | Loss: 0.00003557
Iteration 170/1000 | Loss: 0.00003806
Iteration 171/1000 | Loss: 0.00003789
Iteration 172/1000 | Loss: 0.00003665
Iteration 173/1000 | Loss: 0.00002929
Iteration 174/1000 | Loss: 0.00003817
Iteration 175/1000 | Loss: 0.00004374
Iteration 176/1000 | Loss: 0.00003893
Iteration 177/1000 | Loss: 0.00002439
Iteration 178/1000 | Loss: 0.00006475
Iteration 179/1000 | Loss: 0.00004364
Iteration 180/1000 | Loss: 0.00002155
Iteration 181/1000 | Loss: 0.00004422
Iteration 182/1000 | Loss: 0.00005499
Iteration 183/1000 | Loss: 0.00004197
Iteration 184/1000 | Loss: 0.00005259
Iteration 185/1000 | Loss: 0.00003897
Iteration 186/1000 | Loss: 0.00002669
Iteration 187/1000 | Loss: 0.00003648
Iteration 188/1000 | Loss: 0.00005623
Iteration 189/1000 | Loss: 0.00002633
Iteration 190/1000 | Loss: 0.00002295
Iteration 191/1000 | Loss: 0.00002059
Iteration 192/1000 | Loss: 0.00001939
Iteration 193/1000 | Loss: 0.00001850
Iteration 194/1000 | Loss: 0.00001774
Iteration 195/1000 | Loss: 0.00001724
Iteration 196/1000 | Loss: 0.00001702
Iteration 197/1000 | Loss: 0.00001694
Iteration 198/1000 | Loss: 0.00001683
Iteration 199/1000 | Loss: 0.00001675
Iteration 200/1000 | Loss: 0.00001675
Iteration 201/1000 | Loss: 0.00001675
Iteration 202/1000 | Loss: 0.00001675
Iteration 203/1000 | Loss: 0.00001674
Iteration 204/1000 | Loss: 0.00001674
Iteration 205/1000 | Loss: 0.00001674
Iteration 206/1000 | Loss: 0.00001674
Iteration 207/1000 | Loss: 0.00001674
Iteration 208/1000 | Loss: 0.00001674
Iteration 209/1000 | Loss: 0.00001674
Iteration 210/1000 | Loss: 0.00001674
Iteration 211/1000 | Loss: 0.00001674
Iteration 212/1000 | Loss: 0.00001674
Iteration 213/1000 | Loss: 0.00001673
Iteration 214/1000 | Loss: 0.00001673
Iteration 215/1000 | Loss: 0.00001673
Iteration 216/1000 | Loss: 0.00001672
Iteration 217/1000 | Loss: 0.00001672
Iteration 218/1000 | Loss: 0.00001672
Iteration 219/1000 | Loss: 0.00001672
Iteration 220/1000 | Loss: 0.00001672
Iteration 221/1000 | Loss: 0.00001671
Iteration 222/1000 | Loss: 0.00001671
Iteration 223/1000 | Loss: 0.00001670
Iteration 224/1000 | Loss: 0.00001669
Iteration 225/1000 | Loss: 0.00001669
Iteration 226/1000 | Loss: 0.00001668
Iteration 227/1000 | Loss: 0.00001667
Iteration 228/1000 | Loss: 0.00001667
Iteration 229/1000 | Loss: 0.00001667
Iteration 230/1000 | Loss: 0.00001667
Iteration 231/1000 | Loss: 0.00001667
Iteration 232/1000 | Loss: 0.00001666
Iteration 233/1000 | Loss: 0.00001666
Iteration 234/1000 | Loss: 0.00001666
Iteration 235/1000 | Loss: 0.00001666
Iteration 236/1000 | Loss: 0.00001666
Iteration 237/1000 | Loss: 0.00001666
Iteration 238/1000 | Loss: 0.00001666
Iteration 239/1000 | Loss: 0.00001666
Iteration 240/1000 | Loss: 0.00001665
Iteration 241/1000 | Loss: 0.00001665
Iteration 242/1000 | Loss: 0.00001665
Iteration 243/1000 | Loss: 0.00001665
Iteration 244/1000 | Loss: 0.00001665
Iteration 245/1000 | Loss: 0.00001664
Iteration 246/1000 | Loss: 0.00001664
Iteration 247/1000 | Loss: 0.00001664
Iteration 248/1000 | Loss: 0.00001664
Iteration 249/1000 | Loss: 0.00001664
Iteration 250/1000 | Loss: 0.00001664
Iteration 251/1000 | Loss: 0.00001664
Iteration 252/1000 | Loss: 0.00001663
Iteration 253/1000 | Loss: 0.00001663
Iteration 254/1000 | Loss: 0.00001663
Iteration 255/1000 | Loss: 0.00001662
Iteration 256/1000 | Loss: 0.00001662
Iteration 257/1000 | Loss: 0.00001662
Iteration 258/1000 | Loss: 0.00001661
Iteration 259/1000 | Loss: 0.00001661
Iteration 260/1000 | Loss: 0.00001661
Iteration 261/1000 | Loss: 0.00001661
Iteration 262/1000 | Loss: 0.00001661
Iteration 263/1000 | Loss: 0.00001661
Iteration 264/1000 | Loss: 0.00001661
Iteration 265/1000 | Loss: 0.00001660
Iteration 266/1000 | Loss: 0.00001660
Iteration 267/1000 | Loss: 0.00001660
Iteration 268/1000 | Loss: 0.00001660
Iteration 269/1000 | Loss: 0.00001660
Iteration 270/1000 | Loss: 0.00001660
Iteration 271/1000 | Loss: 0.00001660
Iteration 272/1000 | Loss: 0.00001659
Iteration 273/1000 | Loss: 0.00001659
Iteration 274/1000 | Loss: 0.00001659
Iteration 275/1000 | Loss: 0.00001659
Iteration 276/1000 | Loss: 0.00001659
Iteration 277/1000 | Loss: 0.00001659
Iteration 278/1000 | Loss: 0.00001659
Iteration 279/1000 | Loss: 0.00001659
Iteration 280/1000 | Loss: 0.00001659
Iteration 281/1000 | Loss: 0.00001659
Iteration 282/1000 | Loss: 0.00001659
Iteration 283/1000 | Loss: 0.00001659
Iteration 284/1000 | Loss: 0.00001659
Iteration 285/1000 | Loss: 0.00001659
Iteration 286/1000 | Loss: 0.00001659
Iteration 287/1000 | Loss: 0.00001659
Iteration 288/1000 | Loss: 0.00001659
Iteration 289/1000 | Loss: 0.00001659
Iteration 290/1000 | Loss: 0.00001659
Iteration 291/1000 | Loss: 0.00001659
Iteration 292/1000 | Loss: 0.00001659
Iteration 293/1000 | Loss: 0.00001659
Iteration 294/1000 | Loss: 0.00001659
Iteration 295/1000 | Loss: 0.00001659
Iteration 296/1000 | Loss: 0.00001659
Iteration 297/1000 | Loss: 0.00001659
Iteration 298/1000 | Loss: 0.00001659
Iteration 299/1000 | Loss: 0.00001659
Iteration 300/1000 | Loss: 0.00001659
Iteration 301/1000 | Loss: 0.00001659
Iteration 302/1000 | Loss: 0.00001659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [1.6587759091635235e-05, 1.6587759091635235e-05, 1.6587759091635235e-05, 1.6587759091635235e-05, 1.6587759091635235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6587759091635235e-05

Optimization complete. Final v2v error: 3.362701416015625 mm

Highest mean error: 5.245822906494141 mm for frame 102

Lowest mean error: 3.01639461517334 mm for frame 126

Saving results

Total time: 329.43786573410034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487293
Iteration 2/25 | Loss: 0.00130875
Iteration 3/25 | Loss: 0.00121941
Iteration 4/25 | Loss: 0.00120494
Iteration 5/25 | Loss: 0.00119944
Iteration 6/25 | Loss: 0.00119801
Iteration 7/25 | Loss: 0.00119789
Iteration 8/25 | Loss: 0.00119789
Iteration 9/25 | Loss: 0.00119789
Iteration 10/25 | Loss: 0.00119789
Iteration 11/25 | Loss: 0.00119789
Iteration 12/25 | Loss: 0.00119789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011978879338130355, 0.0011978879338130355, 0.0011978879338130355, 0.0011978879338130355, 0.0011978879338130355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011978879338130355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43098307
Iteration 2/25 | Loss: 0.00149570
Iteration 3/25 | Loss: 0.00149570
Iteration 4/25 | Loss: 0.00149570
Iteration 5/25 | Loss: 0.00149570
Iteration 6/25 | Loss: 0.00149569
Iteration 7/25 | Loss: 0.00149569
Iteration 8/25 | Loss: 0.00149569
Iteration 9/25 | Loss: 0.00149569
Iteration 10/25 | Loss: 0.00149569
Iteration 11/25 | Loss: 0.00149569
Iteration 12/25 | Loss: 0.00149569
Iteration 13/25 | Loss: 0.00149569
Iteration 14/25 | Loss: 0.00149569
Iteration 15/25 | Loss: 0.00149569
Iteration 16/25 | Loss: 0.00149569
Iteration 17/25 | Loss: 0.00149569
Iteration 18/25 | Loss: 0.00149569
Iteration 19/25 | Loss: 0.00149569
Iteration 20/25 | Loss: 0.00149569
Iteration 21/25 | Loss: 0.00149569
Iteration 22/25 | Loss: 0.00149569
Iteration 23/25 | Loss: 0.00149569
Iteration 24/25 | Loss: 0.00149569
Iteration 25/25 | Loss: 0.00149569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149569
Iteration 2/1000 | Loss: 0.00004949
Iteration 3/1000 | Loss: 0.00002854
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00002023
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001801
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001752
Iteration 11/1000 | Loss: 0.00001752
Iteration 12/1000 | Loss: 0.00001747
Iteration 13/1000 | Loss: 0.00001741
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001725
Iteration 18/1000 | Loss: 0.00001725
Iteration 19/1000 | Loss: 0.00001721
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001720
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001719
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001718
Iteration 27/1000 | Loss: 0.00001716
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001713
Iteration 30/1000 | Loss: 0.00001712
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001706
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001705
Iteration 41/1000 | Loss: 0.00001704
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001701
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001700
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001699
Iteration 57/1000 | Loss: 0.00001698
Iteration 58/1000 | Loss: 0.00001698
Iteration 59/1000 | Loss: 0.00001698
Iteration 60/1000 | Loss: 0.00001698
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001698
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001697
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001695
Iteration 68/1000 | Loss: 0.00001694
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00001694
Iteration 71/1000 | Loss: 0.00001694
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001693
Iteration 74/1000 | Loss: 0.00001693
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001692
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001692
Iteration 79/1000 | Loss: 0.00001692
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001690
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001689
Iteration 87/1000 | Loss: 0.00001689
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001687
Iteration 93/1000 | Loss: 0.00001687
Iteration 94/1000 | Loss: 0.00001687
Iteration 95/1000 | Loss: 0.00001687
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001687
Iteration 98/1000 | Loss: 0.00001687
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001686
Iteration 101/1000 | Loss: 0.00001686
Iteration 102/1000 | Loss: 0.00001686
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00001686
Iteration 109/1000 | Loss: 0.00001685
Iteration 110/1000 | Loss: 0.00001685
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001685
Iteration 113/1000 | Loss: 0.00001685
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001685
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00001685
Iteration 119/1000 | Loss: 0.00001685
Iteration 120/1000 | Loss: 0.00001685
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001685
Iteration 124/1000 | Loss: 0.00001685
Iteration 125/1000 | Loss: 0.00001685
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001684
Iteration 128/1000 | Loss: 0.00001684
Iteration 129/1000 | Loss: 0.00001684
Iteration 130/1000 | Loss: 0.00001684
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001683
Iteration 135/1000 | Loss: 0.00001683
Iteration 136/1000 | Loss: 0.00001683
Iteration 137/1000 | Loss: 0.00001683
Iteration 138/1000 | Loss: 0.00001683
Iteration 139/1000 | Loss: 0.00001683
Iteration 140/1000 | Loss: 0.00001683
Iteration 141/1000 | Loss: 0.00001683
Iteration 142/1000 | Loss: 0.00001683
Iteration 143/1000 | Loss: 0.00001683
Iteration 144/1000 | Loss: 0.00001683
Iteration 145/1000 | Loss: 0.00001683
Iteration 146/1000 | Loss: 0.00001683
Iteration 147/1000 | Loss: 0.00001683
Iteration 148/1000 | Loss: 0.00001683
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00001683
Iteration 151/1000 | Loss: 0.00001683
Iteration 152/1000 | Loss: 0.00001683
Iteration 153/1000 | Loss: 0.00001683
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.6834230336826295e-05, 1.6834230336826295e-05, 1.6834230336826295e-05, 1.6834230336826295e-05, 1.6834230336826295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6834230336826295e-05

Optimization complete. Final v2v error: 3.4979982376098633 mm

Highest mean error: 4.009279727935791 mm for frame 55

Lowest mean error: 3.117743730545044 mm for frame 0

Saving results

Total time: 38.00427842140198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920953
Iteration 2/25 | Loss: 0.00168343
Iteration 3/25 | Loss: 0.00142155
Iteration 4/25 | Loss: 0.00136785
Iteration 5/25 | Loss: 0.00137075
Iteration 6/25 | Loss: 0.00132108
Iteration 7/25 | Loss: 0.00130211
Iteration 8/25 | Loss: 0.00128039
Iteration 9/25 | Loss: 0.00126542
Iteration 10/25 | Loss: 0.00125534
Iteration 11/25 | Loss: 0.00124558
Iteration 12/25 | Loss: 0.00123857
Iteration 13/25 | Loss: 0.00124063
Iteration 14/25 | Loss: 0.00124309
Iteration 15/25 | Loss: 0.00123917
Iteration 16/25 | Loss: 0.00123527
Iteration 17/25 | Loss: 0.00123095
Iteration 18/25 | Loss: 0.00123005
Iteration 19/25 | Loss: 0.00122991
Iteration 20/25 | Loss: 0.00122988
Iteration 21/25 | Loss: 0.00122988
Iteration 22/25 | Loss: 0.00122987
Iteration 23/25 | Loss: 0.00122987
Iteration 24/25 | Loss: 0.00122987
Iteration 25/25 | Loss: 0.00122987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.42150927
Iteration 2/25 | Loss: 0.00140251
Iteration 3/25 | Loss: 0.00136112
Iteration 4/25 | Loss: 0.00136103
Iteration 5/25 | Loss: 0.00132912
Iteration 6/25 | Loss: 0.00132912
Iteration 7/25 | Loss: 0.00132912
Iteration 8/25 | Loss: 0.00132912
Iteration 9/25 | Loss: 0.00132912
Iteration 10/25 | Loss: 0.00132912
Iteration 11/25 | Loss: 0.00132912
Iteration 12/25 | Loss: 0.00132912
Iteration 13/25 | Loss: 0.00132912
Iteration 14/25 | Loss: 0.00132912
Iteration 15/25 | Loss: 0.00132912
Iteration 16/25 | Loss: 0.00132912
Iteration 17/25 | Loss: 0.00132912
Iteration 18/25 | Loss: 0.00132912
Iteration 19/25 | Loss: 0.00132912
Iteration 20/25 | Loss: 0.00132912
Iteration 21/25 | Loss: 0.00132912
Iteration 22/25 | Loss: 0.00132912
Iteration 23/25 | Loss: 0.00132912
Iteration 24/25 | Loss: 0.00132912
Iteration 25/25 | Loss: 0.00132912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013291192008182406, 0.0013291192008182406, 0.0013291192008182406, 0.0013291192008182406, 0.0013291192008182406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013291192008182406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132912
Iteration 2/1000 | Loss: 0.00010285
Iteration 3/1000 | Loss: 0.00011601
Iteration 4/1000 | Loss: 0.00048171
Iteration 5/1000 | Loss: 0.00083676
Iteration 6/1000 | Loss: 0.00094684
Iteration 7/1000 | Loss: 0.00105715
Iteration 8/1000 | Loss: 0.00015061
Iteration 9/1000 | Loss: 0.00003212
Iteration 10/1000 | Loss: 0.00006949
Iteration 11/1000 | Loss: 0.00002315
Iteration 12/1000 | Loss: 0.00002161
Iteration 13/1000 | Loss: 0.00002082
Iteration 14/1000 | Loss: 0.00002026
Iteration 15/1000 | Loss: 0.00001999
Iteration 16/1000 | Loss: 0.00001970
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001940
Iteration 19/1000 | Loss: 0.00001939
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001926
Iteration 24/1000 | Loss: 0.00001926
Iteration 25/1000 | Loss: 0.00001925
Iteration 26/1000 | Loss: 0.00001925
Iteration 27/1000 | Loss: 0.00001925
Iteration 28/1000 | Loss: 0.00001924
Iteration 29/1000 | Loss: 0.00001921
Iteration 30/1000 | Loss: 0.00001921
Iteration 31/1000 | Loss: 0.00001921
Iteration 32/1000 | Loss: 0.00001921
Iteration 33/1000 | Loss: 0.00001921
Iteration 34/1000 | Loss: 0.00001921
Iteration 35/1000 | Loss: 0.00001920
Iteration 36/1000 | Loss: 0.00001920
Iteration 37/1000 | Loss: 0.00001918
Iteration 38/1000 | Loss: 0.00001918
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001915
Iteration 41/1000 | Loss: 0.00001915
Iteration 42/1000 | Loss: 0.00001915
Iteration 43/1000 | Loss: 0.00001915
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001914
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001914
Iteration 50/1000 | Loss: 0.00001912
Iteration 51/1000 | Loss: 0.00001912
Iteration 52/1000 | Loss: 0.00001911
Iteration 53/1000 | Loss: 0.00001911
Iteration 54/1000 | Loss: 0.00001911
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001910
Iteration 57/1000 | Loss: 0.00001910
Iteration 58/1000 | Loss: 0.00001910
Iteration 59/1000 | Loss: 0.00001909
Iteration 60/1000 | Loss: 0.00001909
Iteration 61/1000 | Loss: 0.00001909
Iteration 62/1000 | Loss: 0.00001909
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001909
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001908
Iteration 71/1000 | Loss: 0.00001908
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001907
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001907
Iteration 78/1000 | Loss: 0.00001907
Iteration 79/1000 | Loss: 0.00001907
Iteration 80/1000 | Loss: 0.00001907
Iteration 81/1000 | Loss: 0.00001907
Iteration 82/1000 | Loss: 0.00001907
Iteration 83/1000 | Loss: 0.00001907
Iteration 84/1000 | Loss: 0.00001907
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001907
Iteration 91/1000 | Loss: 0.00001907
Iteration 92/1000 | Loss: 0.00001907
Iteration 93/1000 | Loss: 0.00001907
Iteration 94/1000 | Loss: 0.00001907
Iteration 95/1000 | Loss: 0.00001907
Iteration 96/1000 | Loss: 0.00001907
Iteration 97/1000 | Loss: 0.00001907
Iteration 98/1000 | Loss: 0.00001907
Iteration 99/1000 | Loss: 0.00001907
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001907
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001906
Iteration 106/1000 | Loss: 0.00001906
Iteration 107/1000 | Loss: 0.00001906
Iteration 108/1000 | Loss: 0.00001906
Iteration 109/1000 | Loss: 0.00001906
Iteration 110/1000 | Loss: 0.00001906
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001906
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001906
Iteration 116/1000 | Loss: 0.00001906
Iteration 117/1000 | Loss: 0.00001906
Iteration 118/1000 | Loss: 0.00001906
Iteration 119/1000 | Loss: 0.00001906
Iteration 120/1000 | Loss: 0.00001906
Iteration 121/1000 | Loss: 0.00001906
Iteration 122/1000 | Loss: 0.00001906
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Iteration 126/1000 | Loss: 0.00001906
Iteration 127/1000 | Loss: 0.00001906
Iteration 128/1000 | Loss: 0.00001906
Iteration 129/1000 | Loss: 0.00001906
Iteration 130/1000 | Loss: 0.00001906
Iteration 131/1000 | Loss: 0.00001906
Iteration 132/1000 | Loss: 0.00001906
Iteration 133/1000 | Loss: 0.00001906
Iteration 134/1000 | Loss: 0.00001906
Iteration 135/1000 | Loss: 0.00001906
Iteration 136/1000 | Loss: 0.00001906
Iteration 137/1000 | Loss: 0.00001906
Iteration 138/1000 | Loss: 0.00001906
Iteration 139/1000 | Loss: 0.00001906
Iteration 140/1000 | Loss: 0.00001906
Iteration 141/1000 | Loss: 0.00001906
Iteration 142/1000 | Loss: 0.00001906
Iteration 143/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.9064982552663423e-05, 1.9064982552663423e-05, 1.9064982552663423e-05, 1.9064982552663423e-05, 1.9064982552663423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9064982552663423e-05

Optimization complete. Final v2v error: 3.6151282787323 mm

Highest mean error: 4.1664276123046875 mm for frame 170

Lowest mean error: 3.1833114624023438 mm for frame 16

Saving results

Total time: 75.02628660202026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741430
Iteration 2/25 | Loss: 0.00151586
Iteration 3/25 | Loss: 0.00131353
Iteration 4/25 | Loss: 0.00129340
Iteration 5/25 | Loss: 0.00129238
Iteration 6/25 | Loss: 0.00129238
Iteration 7/25 | Loss: 0.00129238
Iteration 8/25 | Loss: 0.00129238
Iteration 9/25 | Loss: 0.00129238
Iteration 10/25 | Loss: 0.00129238
Iteration 11/25 | Loss: 0.00129238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001292377128265798, 0.001292377128265798, 0.001292377128265798, 0.001292377128265798, 0.001292377128265798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001292377128265798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22158802
Iteration 2/25 | Loss: 0.00109556
Iteration 3/25 | Loss: 0.00109555
Iteration 4/25 | Loss: 0.00109555
Iteration 5/25 | Loss: 0.00109555
Iteration 6/25 | Loss: 0.00109555
Iteration 7/25 | Loss: 0.00109555
Iteration 8/25 | Loss: 0.00109555
Iteration 9/25 | Loss: 0.00109555
Iteration 10/25 | Loss: 0.00109555
Iteration 11/25 | Loss: 0.00109555
Iteration 12/25 | Loss: 0.00109555
Iteration 13/25 | Loss: 0.00109555
Iteration 14/25 | Loss: 0.00109555
Iteration 15/25 | Loss: 0.00109555
Iteration 16/25 | Loss: 0.00109555
Iteration 17/25 | Loss: 0.00109555
Iteration 18/25 | Loss: 0.00109555
Iteration 19/25 | Loss: 0.00109555
Iteration 20/25 | Loss: 0.00109555
Iteration 21/25 | Loss: 0.00109555
Iteration 22/25 | Loss: 0.00109555
Iteration 23/25 | Loss: 0.00109555
Iteration 24/25 | Loss: 0.00109555
Iteration 25/25 | Loss: 0.00109555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109555
Iteration 2/1000 | Loss: 0.00003042
Iteration 3/1000 | Loss: 0.00002394
Iteration 4/1000 | Loss: 0.00002126
Iteration 5/1000 | Loss: 0.00002009
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001906
Iteration 8/1000 | Loss: 0.00001879
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001852
Iteration 11/1000 | Loss: 0.00001833
Iteration 12/1000 | Loss: 0.00001824
Iteration 13/1000 | Loss: 0.00001817
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001791
Iteration 18/1000 | Loss: 0.00001790
Iteration 19/1000 | Loss: 0.00001788
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001787
Iteration 23/1000 | Loss: 0.00001782
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001781
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001778
Iteration 35/1000 | Loss: 0.00001778
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001777
Iteration 39/1000 | Loss: 0.00001777
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001775
Iteration 44/1000 | Loss: 0.00001775
Iteration 45/1000 | Loss: 0.00001774
Iteration 46/1000 | Loss: 0.00001774
Iteration 47/1000 | Loss: 0.00001774
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001773
Iteration 50/1000 | Loss: 0.00001773
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001771
Iteration 55/1000 | Loss: 0.00001771
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001770
Iteration 58/1000 | Loss: 0.00001770
Iteration 59/1000 | Loss: 0.00001770
Iteration 60/1000 | Loss: 0.00001770
Iteration 61/1000 | Loss: 0.00001769
Iteration 62/1000 | Loss: 0.00001769
Iteration 63/1000 | Loss: 0.00001769
Iteration 64/1000 | Loss: 0.00001769
Iteration 65/1000 | Loss: 0.00001768
Iteration 66/1000 | Loss: 0.00001768
Iteration 67/1000 | Loss: 0.00001768
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001768
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001768
Iteration 73/1000 | Loss: 0.00001767
Iteration 74/1000 | Loss: 0.00001767
Iteration 75/1000 | Loss: 0.00001767
Iteration 76/1000 | Loss: 0.00001767
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001767
Iteration 79/1000 | Loss: 0.00001767
Iteration 80/1000 | Loss: 0.00001767
Iteration 81/1000 | Loss: 0.00001767
Iteration 82/1000 | Loss: 0.00001767
Iteration 83/1000 | Loss: 0.00001767
Iteration 84/1000 | Loss: 0.00001767
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001767
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001767
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001767
Iteration 112/1000 | Loss: 0.00001767
Iteration 113/1000 | Loss: 0.00001767
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.7667945940047503e-05, 1.7667945940047503e-05, 1.7667945940047503e-05, 1.7667945940047503e-05, 1.7667945940047503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7667945940047503e-05

Optimization complete. Final v2v error: 3.6260478496551514 mm

Highest mean error: 3.924401044845581 mm for frame 89

Lowest mean error: 3.2373015880584717 mm for frame 200

Saving results

Total time: 35.17371702194214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785105
Iteration 2/25 | Loss: 0.00177848
Iteration 3/25 | Loss: 0.00136488
Iteration 4/25 | Loss: 0.00130303
Iteration 5/25 | Loss: 0.00128995
Iteration 6/25 | Loss: 0.00127749
Iteration 7/25 | Loss: 0.00127422
Iteration 8/25 | Loss: 0.00127561
Iteration 9/25 | Loss: 0.00127407
Iteration 10/25 | Loss: 0.00127274
Iteration 11/25 | Loss: 0.00127214
Iteration 12/25 | Loss: 0.00126836
Iteration 13/25 | Loss: 0.00126787
Iteration 14/25 | Loss: 0.00126773
Iteration 15/25 | Loss: 0.00127095
Iteration 16/25 | Loss: 0.00126935
Iteration 17/25 | Loss: 0.00126816
Iteration 18/25 | Loss: 0.00126563
Iteration 19/25 | Loss: 0.00126493
Iteration 20/25 | Loss: 0.00126472
Iteration 21/25 | Loss: 0.00126464
Iteration 22/25 | Loss: 0.00126463
Iteration 23/25 | Loss: 0.00126463
Iteration 24/25 | Loss: 0.00126463
Iteration 25/25 | Loss: 0.00126463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.76555824
Iteration 2/25 | Loss: 0.00173324
Iteration 3/25 | Loss: 0.00173311
Iteration 4/25 | Loss: 0.00173311
Iteration 5/25 | Loss: 0.00173311
Iteration 6/25 | Loss: 0.00173311
Iteration 7/25 | Loss: 0.00173311
Iteration 8/25 | Loss: 0.00173311
Iteration 9/25 | Loss: 0.00173311
Iteration 10/25 | Loss: 0.00173311
Iteration 11/25 | Loss: 0.00173311
Iteration 12/25 | Loss: 0.00173311
Iteration 13/25 | Loss: 0.00173311
Iteration 14/25 | Loss: 0.00173311
Iteration 15/25 | Loss: 0.00173311
Iteration 16/25 | Loss: 0.00173311
Iteration 17/25 | Loss: 0.00173311
Iteration 18/25 | Loss: 0.00173311
Iteration 19/25 | Loss: 0.00173311
Iteration 20/25 | Loss: 0.00173311
Iteration 21/25 | Loss: 0.00173311
Iteration 22/25 | Loss: 0.00173311
Iteration 23/25 | Loss: 0.00173311
Iteration 24/25 | Loss: 0.00173311
Iteration 25/25 | Loss: 0.00173311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173311
Iteration 2/1000 | Loss: 0.00006009
Iteration 3/1000 | Loss: 0.00003331
Iteration 4/1000 | Loss: 0.00002701
Iteration 5/1000 | Loss: 0.00002425
Iteration 6/1000 | Loss: 0.00002299
Iteration 7/1000 | Loss: 0.00002237
Iteration 8/1000 | Loss: 0.00002194
Iteration 9/1000 | Loss: 0.00002164
Iteration 10/1000 | Loss: 0.00002142
Iteration 11/1000 | Loss: 0.00002123
Iteration 12/1000 | Loss: 0.00002100
Iteration 13/1000 | Loss: 0.00002090
Iteration 14/1000 | Loss: 0.00002087
Iteration 15/1000 | Loss: 0.00002083
Iteration 16/1000 | Loss: 0.00002076
Iteration 17/1000 | Loss: 0.00002071
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002070
Iteration 20/1000 | Loss: 0.00002070
Iteration 21/1000 | Loss: 0.00002069
Iteration 22/1000 | Loss: 0.00002068
Iteration 23/1000 | Loss: 0.00002068
Iteration 24/1000 | Loss: 0.00002067
Iteration 25/1000 | Loss: 0.00002066
Iteration 26/1000 | Loss: 0.00002066
Iteration 27/1000 | Loss: 0.00002064
Iteration 28/1000 | Loss: 0.00002064
Iteration 29/1000 | Loss: 0.00002062
Iteration 30/1000 | Loss: 0.00002061
Iteration 31/1000 | Loss: 0.00002060
Iteration 32/1000 | Loss: 0.00002060
Iteration 33/1000 | Loss: 0.00002059
Iteration 34/1000 | Loss: 0.00002059
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002056
Iteration 41/1000 | Loss: 0.00002056
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00002055
Iteration 44/1000 | Loss: 0.00002055
Iteration 45/1000 | Loss: 0.00002055
Iteration 46/1000 | Loss: 0.00002055
Iteration 47/1000 | Loss: 0.00002054
Iteration 48/1000 | Loss: 0.00002054
Iteration 49/1000 | Loss: 0.00002053
Iteration 50/1000 | Loss: 0.00002053
Iteration 51/1000 | Loss: 0.00002053
Iteration 52/1000 | Loss: 0.00002053
Iteration 53/1000 | Loss: 0.00002053
Iteration 54/1000 | Loss: 0.00002053
Iteration 55/1000 | Loss: 0.00002053
Iteration 56/1000 | Loss: 0.00002053
Iteration 57/1000 | Loss: 0.00002052
Iteration 58/1000 | Loss: 0.00002051
Iteration 59/1000 | Loss: 0.00002051
Iteration 60/1000 | Loss: 0.00002050
Iteration 61/1000 | Loss: 0.00002050
Iteration 62/1000 | Loss: 0.00002050
Iteration 63/1000 | Loss: 0.00002050
Iteration 64/1000 | Loss: 0.00002049
Iteration 65/1000 | Loss: 0.00002049
Iteration 66/1000 | Loss: 0.00002049
Iteration 67/1000 | Loss: 0.00002049
Iteration 68/1000 | Loss: 0.00002048
Iteration 69/1000 | Loss: 0.00002048
Iteration 70/1000 | Loss: 0.00002048
Iteration 71/1000 | Loss: 0.00002048
Iteration 72/1000 | Loss: 0.00002048
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002048
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00002047
Iteration 78/1000 | Loss: 0.00002047
Iteration 79/1000 | Loss: 0.00002047
Iteration 80/1000 | Loss: 0.00002047
Iteration 81/1000 | Loss: 0.00002047
Iteration 82/1000 | Loss: 0.00002046
Iteration 83/1000 | Loss: 0.00002046
Iteration 84/1000 | Loss: 0.00002046
Iteration 85/1000 | Loss: 0.00002046
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002046
Iteration 90/1000 | Loss: 0.00002046
Iteration 91/1000 | Loss: 0.00002046
Iteration 92/1000 | Loss: 0.00002046
Iteration 93/1000 | Loss: 0.00002045
Iteration 94/1000 | Loss: 0.00002045
Iteration 95/1000 | Loss: 0.00002045
Iteration 96/1000 | Loss: 0.00002045
Iteration 97/1000 | Loss: 0.00002045
Iteration 98/1000 | Loss: 0.00002045
Iteration 99/1000 | Loss: 0.00002045
Iteration 100/1000 | Loss: 0.00002045
Iteration 101/1000 | Loss: 0.00002045
Iteration 102/1000 | Loss: 0.00002045
Iteration 103/1000 | Loss: 0.00002045
Iteration 104/1000 | Loss: 0.00002045
Iteration 105/1000 | Loss: 0.00002045
Iteration 106/1000 | Loss: 0.00002044
Iteration 107/1000 | Loss: 0.00002044
Iteration 108/1000 | Loss: 0.00002044
Iteration 109/1000 | Loss: 0.00002044
Iteration 110/1000 | Loss: 0.00002044
Iteration 111/1000 | Loss: 0.00002044
Iteration 112/1000 | Loss: 0.00002044
Iteration 113/1000 | Loss: 0.00002044
Iteration 114/1000 | Loss: 0.00002044
Iteration 115/1000 | Loss: 0.00002043
Iteration 116/1000 | Loss: 0.00002043
Iteration 117/1000 | Loss: 0.00002043
Iteration 118/1000 | Loss: 0.00002043
Iteration 119/1000 | Loss: 0.00002043
Iteration 120/1000 | Loss: 0.00002043
Iteration 121/1000 | Loss: 0.00002042
Iteration 122/1000 | Loss: 0.00002042
Iteration 123/1000 | Loss: 0.00002042
Iteration 124/1000 | Loss: 0.00002042
Iteration 125/1000 | Loss: 0.00002042
Iteration 126/1000 | Loss: 0.00002042
Iteration 127/1000 | Loss: 0.00002042
Iteration 128/1000 | Loss: 0.00002042
Iteration 129/1000 | Loss: 0.00002041
Iteration 130/1000 | Loss: 0.00002041
Iteration 131/1000 | Loss: 0.00002041
Iteration 132/1000 | Loss: 0.00002041
Iteration 133/1000 | Loss: 0.00002041
Iteration 134/1000 | Loss: 0.00002040
Iteration 135/1000 | Loss: 0.00002040
Iteration 136/1000 | Loss: 0.00002040
Iteration 137/1000 | Loss: 0.00002040
Iteration 138/1000 | Loss: 0.00002040
Iteration 139/1000 | Loss: 0.00002040
Iteration 140/1000 | Loss: 0.00002039
Iteration 141/1000 | Loss: 0.00002039
Iteration 142/1000 | Loss: 0.00002039
Iteration 143/1000 | Loss: 0.00002039
Iteration 144/1000 | Loss: 0.00002039
Iteration 145/1000 | Loss: 0.00002039
Iteration 146/1000 | Loss: 0.00002039
Iteration 147/1000 | Loss: 0.00002039
Iteration 148/1000 | Loss: 0.00002039
Iteration 149/1000 | Loss: 0.00002039
Iteration 150/1000 | Loss: 0.00002039
Iteration 151/1000 | Loss: 0.00002039
Iteration 152/1000 | Loss: 0.00002039
Iteration 153/1000 | Loss: 0.00002039
Iteration 154/1000 | Loss: 0.00002039
Iteration 155/1000 | Loss: 0.00002039
Iteration 156/1000 | Loss: 0.00002039
Iteration 157/1000 | Loss: 0.00002039
Iteration 158/1000 | Loss: 0.00002039
Iteration 159/1000 | Loss: 0.00002039
Iteration 160/1000 | Loss: 0.00002039
Iteration 161/1000 | Loss: 0.00002039
Iteration 162/1000 | Loss: 0.00002039
Iteration 163/1000 | Loss: 0.00002039
Iteration 164/1000 | Loss: 0.00002039
Iteration 165/1000 | Loss: 0.00002039
Iteration 166/1000 | Loss: 0.00002039
Iteration 167/1000 | Loss: 0.00002039
Iteration 168/1000 | Loss: 0.00002039
Iteration 169/1000 | Loss: 0.00002039
Iteration 170/1000 | Loss: 0.00002039
Iteration 171/1000 | Loss: 0.00002039
Iteration 172/1000 | Loss: 0.00002039
Iteration 173/1000 | Loss: 0.00002039
Iteration 174/1000 | Loss: 0.00002039
Iteration 175/1000 | Loss: 0.00002039
Iteration 176/1000 | Loss: 0.00002039
Iteration 177/1000 | Loss: 0.00002039
Iteration 178/1000 | Loss: 0.00002039
Iteration 179/1000 | Loss: 0.00002039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.0385010429890826e-05, 2.0385010429890826e-05, 2.0385010429890826e-05, 2.0385010429890826e-05, 2.0385010429890826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0385010429890826e-05

Optimization complete. Final v2v error: 3.8052451610565186 mm

Highest mean error: 5.044281959533691 mm for frame 173

Lowest mean error: 3.237541675567627 mm for frame 116

Saving results

Total time: 77.12300062179565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872584
Iteration 2/25 | Loss: 0.00136531
Iteration 3/25 | Loss: 0.00121634
Iteration 4/25 | Loss: 0.00120276
Iteration 5/25 | Loss: 0.00119304
Iteration 6/25 | Loss: 0.00119476
Iteration 7/25 | Loss: 0.00119086
Iteration 8/25 | Loss: 0.00119057
Iteration 9/25 | Loss: 0.00119036
Iteration 10/25 | Loss: 0.00119091
Iteration 11/25 | Loss: 0.00118961
Iteration 12/25 | Loss: 0.00118910
Iteration 13/25 | Loss: 0.00118894
Iteration 14/25 | Loss: 0.00118894
Iteration 15/25 | Loss: 0.00118893
Iteration 16/25 | Loss: 0.00118893
Iteration 17/25 | Loss: 0.00118893
Iteration 18/25 | Loss: 0.00118893
Iteration 19/25 | Loss: 0.00118893
Iteration 20/25 | Loss: 0.00118893
Iteration 21/25 | Loss: 0.00118893
Iteration 22/25 | Loss: 0.00118893
Iteration 23/25 | Loss: 0.00118893
Iteration 24/25 | Loss: 0.00118893
Iteration 25/25 | Loss: 0.00118892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.51357651
Iteration 2/25 | Loss: 0.00129437
Iteration 3/25 | Loss: 0.00129437
Iteration 4/25 | Loss: 0.00129437
Iteration 5/25 | Loss: 0.00129437
Iteration 6/25 | Loss: 0.00129437
Iteration 7/25 | Loss: 0.00129437
Iteration 8/25 | Loss: 0.00129437
Iteration 9/25 | Loss: 0.00129437
Iteration 10/25 | Loss: 0.00129437
Iteration 11/25 | Loss: 0.00129437
Iteration 12/25 | Loss: 0.00129437
Iteration 13/25 | Loss: 0.00129437
Iteration 14/25 | Loss: 0.00129437
Iteration 15/25 | Loss: 0.00129437
Iteration 16/25 | Loss: 0.00129437
Iteration 17/25 | Loss: 0.00129437
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012943684123456478, 0.0012943684123456478, 0.0012943684123456478, 0.0012943684123456478, 0.0012943684123456478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012943684123456478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129437
Iteration 2/1000 | Loss: 0.00003083
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00001995
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001834
Iteration 7/1000 | Loss: 0.00001786
Iteration 8/1000 | Loss: 0.00001752
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001724
Iteration 11/1000 | Loss: 0.00001721
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001716
Iteration 14/1000 | Loss: 0.00001712
Iteration 15/1000 | Loss: 0.00001711
Iteration 16/1000 | Loss: 0.00001711
Iteration 17/1000 | Loss: 0.00001710
Iteration 18/1000 | Loss: 0.00001708
Iteration 19/1000 | Loss: 0.00001701
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001694
Iteration 24/1000 | Loss: 0.00001693
Iteration 25/1000 | Loss: 0.00001692
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001688
Iteration 29/1000 | Loss: 0.00001688
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001688
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001685
Iteration 36/1000 | Loss: 0.00001684
Iteration 37/1000 | Loss: 0.00001684
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001684
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00001683
Iteration 44/1000 | Loss: 0.00001683
Iteration 45/1000 | Loss: 0.00001683
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001682
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001679
Iteration 56/1000 | Loss: 0.00001679
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001678
Iteration 59/1000 | Loss: 0.00001678
Iteration 60/1000 | Loss: 0.00001678
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001677
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001676
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001675
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001673
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001672
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001672
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001672
Iteration 93/1000 | Loss: 0.00001672
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001672
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.6721145584597252e-05, 1.6721145584597252e-05, 1.6721145584597252e-05, 1.6721145584597252e-05, 1.6721145584597252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6721145584597252e-05

Optimization complete. Final v2v error: 3.426903486251831 mm

Highest mean error: 3.7628567218780518 mm for frame 101

Lowest mean error: 3.1607725620269775 mm for frame 6

Saving results

Total time: 51.55484485626221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00636361
Iteration 2/25 | Loss: 0.00132816
Iteration 3/25 | Loss: 0.00123408
Iteration 4/25 | Loss: 0.00121983
Iteration 5/25 | Loss: 0.00121325
Iteration 6/25 | Loss: 0.00121104
Iteration 7/25 | Loss: 0.00121088
Iteration 8/25 | Loss: 0.00121088
Iteration 9/25 | Loss: 0.00121088
Iteration 10/25 | Loss: 0.00121088
Iteration 11/25 | Loss: 0.00121088
Iteration 12/25 | Loss: 0.00121088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012108816299587488, 0.0012108816299587488, 0.0012108816299587488, 0.0012108816299587488, 0.0012108816299587488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012108816299587488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39677691
Iteration 2/25 | Loss: 0.00167206
Iteration 3/25 | Loss: 0.00167205
Iteration 4/25 | Loss: 0.00167205
Iteration 5/25 | Loss: 0.00167205
Iteration 6/25 | Loss: 0.00167205
Iteration 7/25 | Loss: 0.00167205
Iteration 8/25 | Loss: 0.00167205
Iteration 9/25 | Loss: 0.00167205
Iteration 10/25 | Loss: 0.00167205
Iteration 11/25 | Loss: 0.00167205
Iteration 12/25 | Loss: 0.00167205
Iteration 13/25 | Loss: 0.00167205
Iteration 14/25 | Loss: 0.00167205
Iteration 15/25 | Loss: 0.00167205
Iteration 16/25 | Loss: 0.00167205
Iteration 17/25 | Loss: 0.00167205
Iteration 18/25 | Loss: 0.00167205
Iteration 19/25 | Loss: 0.00167205
Iteration 20/25 | Loss: 0.00167205
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016720511484891176, 0.0016720511484891176, 0.0016720511484891176, 0.0016720511484891176, 0.0016720511484891176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016720511484891176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167205
Iteration 2/1000 | Loss: 0.00003583
Iteration 3/1000 | Loss: 0.00002518
Iteration 4/1000 | Loss: 0.00002264
Iteration 5/1000 | Loss: 0.00002144
Iteration 6/1000 | Loss: 0.00002066
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001981
Iteration 9/1000 | Loss: 0.00001952
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001937
Iteration 12/1000 | Loss: 0.00001931
Iteration 13/1000 | Loss: 0.00001921
Iteration 14/1000 | Loss: 0.00001919
Iteration 15/1000 | Loss: 0.00001915
Iteration 16/1000 | Loss: 0.00001913
Iteration 17/1000 | Loss: 0.00001913
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001909
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001909
Iteration 23/1000 | Loss: 0.00001908
Iteration 24/1000 | Loss: 0.00001908
Iteration 25/1000 | Loss: 0.00001907
Iteration 26/1000 | Loss: 0.00001907
Iteration 27/1000 | Loss: 0.00001907
Iteration 28/1000 | Loss: 0.00001906
Iteration 29/1000 | Loss: 0.00001906
Iteration 30/1000 | Loss: 0.00001906
Iteration 31/1000 | Loss: 0.00001906
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001906
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001906
Iteration 36/1000 | Loss: 0.00001906
Iteration 37/1000 | Loss: 0.00001905
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001905
Iteration 42/1000 | Loss: 0.00001905
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001904
Iteration 46/1000 | Loss: 0.00001904
Iteration 47/1000 | Loss: 0.00001904
Iteration 48/1000 | Loss: 0.00001904
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001903
Iteration 51/1000 | Loss: 0.00001903
Iteration 52/1000 | Loss: 0.00001903
Iteration 53/1000 | Loss: 0.00001902
Iteration 54/1000 | Loss: 0.00001902
Iteration 55/1000 | Loss: 0.00001902
Iteration 56/1000 | Loss: 0.00001902
Iteration 57/1000 | Loss: 0.00001902
Iteration 58/1000 | Loss: 0.00001901
Iteration 59/1000 | Loss: 0.00001901
Iteration 60/1000 | Loss: 0.00001901
Iteration 61/1000 | Loss: 0.00001901
Iteration 62/1000 | Loss: 0.00001900
Iteration 63/1000 | Loss: 0.00001900
Iteration 64/1000 | Loss: 0.00001899
Iteration 65/1000 | Loss: 0.00001899
Iteration 66/1000 | Loss: 0.00001899
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001898
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00001898
Iteration 72/1000 | Loss: 0.00001897
Iteration 73/1000 | Loss: 0.00001897
Iteration 74/1000 | Loss: 0.00001897
Iteration 75/1000 | Loss: 0.00001897
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001896
Iteration 78/1000 | Loss: 0.00001895
Iteration 79/1000 | Loss: 0.00001895
Iteration 80/1000 | Loss: 0.00001895
Iteration 81/1000 | Loss: 0.00001895
Iteration 82/1000 | Loss: 0.00001895
Iteration 83/1000 | Loss: 0.00001895
Iteration 84/1000 | Loss: 0.00001895
Iteration 85/1000 | Loss: 0.00001895
Iteration 86/1000 | Loss: 0.00001895
Iteration 87/1000 | Loss: 0.00001894
Iteration 88/1000 | Loss: 0.00001894
Iteration 89/1000 | Loss: 0.00001894
Iteration 90/1000 | Loss: 0.00001893
Iteration 91/1000 | Loss: 0.00001893
Iteration 92/1000 | Loss: 0.00001893
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001892
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001891
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001890
Iteration 104/1000 | Loss: 0.00001890
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00001890
Iteration 107/1000 | Loss: 0.00001890
Iteration 108/1000 | Loss: 0.00001890
Iteration 109/1000 | Loss: 0.00001890
Iteration 110/1000 | Loss: 0.00001890
Iteration 111/1000 | Loss: 0.00001890
Iteration 112/1000 | Loss: 0.00001890
Iteration 113/1000 | Loss: 0.00001890
Iteration 114/1000 | Loss: 0.00001890
Iteration 115/1000 | Loss: 0.00001890
Iteration 116/1000 | Loss: 0.00001890
Iteration 117/1000 | Loss: 0.00001889
Iteration 118/1000 | Loss: 0.00001889
Iteration 119/1000 | Loss: 0.00001889
Iteration 120/1000 | Loss: 0.00001889
Iteration 121/1000 | Loss: 0.00001889
Iteration 122/1000 | Loss: 0.00001889
Iteration 123/1000 | Loss: 0.00001889
Iteration 124/1000 | Loss: 0.00001889
Iteration 125/1000 | Loss: 0.00001889
Iteration 126/1000 | Loss: 0.00001889
Iteration 127/1000 | Loss: 0.00001889
Iteration 128/1000 | Loss: 0.00001889
Iteration 129/1000 | Loss: 0.00001889
Iteration 130/1000 | Loss: 0.00001889
Iteration 131/1000 | Loss: 0.00001889
Iteration 132/1000 | Loss: 0.00001889
Iteration 133/1000 | Loss: 0.00001889
Iteration 134/1000 | Loss: 0.00001889
Iteration 135/1000 | Loss: 0.00001889
Iteration 136/1000 | Loss: 0.00001889
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.889071245386731e-05, 1.889071245386731e-05, 1.889071245386731e-05, 1.889071245386731e-05, 1.889071245386731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.889071245386731e-05

Optimization complete. Final v2v error: 3.6562106609344482 mm

Highest mean error: 4.1509575843811035 mm for frame 90

Lowest mean error: 3.4268958568573 mm for frame 0

Saving results

Total time: 34.416133642196655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093103
Iteration 2/25 | Loss: 0.00203767
Iteration 3/25 | Loss: 0.00176942
Iteration 4/25 | Loss: 0.00156085
Iteration 5/25 | Loss: 0.00149553
Iteration 6/25 | Loss: 0.00152944
Iteration 7/25 | Loss: 0.00143755
Iteration 8/25 | Loss: 0.00138986
Iteration 9/25 | Loss: 0.00136155
Iteration 10/25 | Loss: 0.00134778
Iteration 11/25 | Loss: 0.00134056
Iteration 12/25 | Loss: 0.00133435
Iteration 13/25 | Loss: 0.00133386
Iteration 14/25 | Loss: 0.00133263
Iteration 15/25 | Loss: 0.00132785
Iteration 16/25 | Loss: 0.00132118
Iteration 17/25 | Loss: 0.00131873
Iteration 18/25 | Loss: 0.00131493
Iteration 19/25 | Loss: 0.00131185
Iteration 20/25 | Loss: 0.00131068
Iteration 21/25 | Loss: 0.00130984
Iteration 22/25 | Loss: 0.00131261
Iteration 23/25 | Loss: 0.00130950
Iteration 24/25 | Loss: 0.00130973
Iteration 25/25 | Loss: 0.00130276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26728082
Iteration 2/25 | Loss: 0.00176321
Iteration 3/25 | Loss: 0.00176321
Iteration 4/25 | Loss: 0.00176321
Iteration 5/25 | Loss: 0.00176321
Iteration 6/25 | Loss: 0.00176321
Iteration 7/25 | Loss: 0.00176320
Iteration 8/25 | Loss: 0.00176320
Iteration 9/25 | Loss: 0.00176320
Iteration 10/25 | Loss: 0.00176320
Iteration 11/25 | Loss: 0.00176320
Iteration 12/25 | Loss: 0.00176320
Iteration 13/25 | Loss: 0.00176320
Iteration 14/25 | Loss: 0.00176320
Iteration 15/25 | Loss: 0.00176320
Iteration 16/25 | Loss: 0.00176320
Iteration 17/25 | Loss: 0.00176320
Iteration 18/25 | Loss: 0.00176320
Iteration 19/25 | Loss: 0.00176320
Iteration 20/25 | Loss: 0.00176320
Iteration 21/25 | Loss: 0.00176320
Iteration 22/25 | Loss: 0.00176320
Iteration 23/25 | Loss: 0.00176320
Iteration 24/25 | Loss: 0.00176320
Iteration 25/25 | Loss: 0.00176320

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176320
Iteration 2/1000 | Loss: 0.00013462
Iteration 3/1000 | Loss: 0.00022473
Iteration 4/1000 | Loss: 0.00007979
Iteration 5/1000 | Loss: 0.00053462
Iteration 6/1000 | Loss: 0.00438592
Iteration 7/1000 | Loss: 0.00227383
Iteration 8/1000 | Loss: 0.00149981
Iteration 9/1000 | Loss: 0.00195417
Iteration 10/1000 | Loss: 0.00137000
Iteration 11/1000 | Loss: 0.00261151
Iteration 12/1000 | Loss: 0.00365493
Iteration 13/1000 | Loss: 0.00291978
Iteration 14/1000 | Loss: 0.00162576
Iteration 15/1000 | Loss: 0.00304647
Iteration 16/1000 | Loss: 0.00166265
Iteration 17/1000 | Loss: 0.00196712
Iteration 18/1000 | Loss: 0.00109267
Iteration 19/1000 | Loss: 0.00169005
Iteration 20/1000 | Loss: 0.00200895
Iteration 21/1000 | Loss: 0.00179110
Iteration 22/1000 | Loss: 0.00011518
Iteration 23/1000 | Loss: 0.00066279
Iteration 24/1000 | Loss: 0.00093554
Iteration 25/1000 | Loss: 0.00025137
Iteration 26/1000 | Loss: 0.00044827
Iteration 27/1000 | Loss: 0.00022277
Iteration 28/1000 | Loss: 0.00042444
Iteration 29/1000 | Loss: 0.00057411
Iteration 30/1000 | Loss: 0.00058505
Iteration 31/1000 | Loss: 0.00031904
Iteration 32/1000 | Loss: 0.00062996
Iteration 33/1000 | Loss: 0.00053404
Iteration 34/1000 | Loss: 0.00057367
Iteration 35/1000 | Loss: 0.00005008
Iteration 36/1000 | Loss: 0.00004246
Iteration 37/1000 | Loss: 0.00014167
Iteration 38/1000 | Loss: 0.00003525
Iteration 39/1000 | Loss: 0.00002969
Iteration 40/1000 | Loss: 0.00002663
Iteration 41/1000 | Loss: 0.00002414
Iteration 42/1000 | Loss: 0.00021083
Iteration 43/1000 | Loss: 0.00013378
Iteration 44/1000 | Loss: 0.00023512
Iteration 45/1000 | Loss: 0.00011901
Iteration 46/1000 | Loss: 0.00014272
Iteration 47/1000 | Loss: 0.00003442
Iteration 48/1000 | Loss: 0.00002376
Iteration 49/1000 | Loss: 0.00002108
Iteration 50/1000 | Loss: 0.00001980
Iteration 51/1000 | Loss: 0.00001912
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001799
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001727
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001720
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001703
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001701
Iteration 68/1000 | Loss: 0.00001701
Iteration 69/1000 | Loss: 0.00001701
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001697
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001696
Iteration 80/1000 | Loss: 0.00001696
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001696
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001694
Iteration 87/1000 | Loss: 0.00001694
Iteration 88/1000 | Loss: 0.00001694
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Iteration 91/1000 | Loss: 0.00001693
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001692
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001689
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001688
Iteration 109/1000 | Loss: 0.00001688
Iteration 110/1000 | Loss: 0.00001688
Iteration 111/1000 | Loss: 0.00001688
Iteration 112/1000 | Loss: 0.00001688
Iteration 113/1000 | Loss: 0.00001688
Iteration 114/1000 | Loss: 0.00001688
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001687
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001686
Iteration 123/1000 | Loss: 0.00001686
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001685
Iteration 127/1000 | Loss: 0.00001685
Iteration 128/1000 | Loss: 0.00001685
Iteration 129/1000 | Loss: 0.00001684
Iteration 130/1000 | Loss: 0.00001684
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001683
Iteration 134/1000 | Loss: 0.00001683
Iteration 135/1000 | Loss: 0.00001683
Iteration 136/1000 | Loss: 0.00001682
Iteration 137/1000 | Loss: 0.00001682
Iteration 138/1000 | Loss: 0.00001681
Iteration 139/1000 | Loss: 0.00001681
Iteration 140/1000 | Loss: 0.00001680
Iteration 141/1000 | Loss: 0.00001680
Iteration 142/1000 | Loss: 0.00001680
Iteration 143/1000 | Loss: 0.00001680
Iteration 144/1000 | Loss: 0.00001679
Iteration 145/1000 | Loss: 0.00001679
Iteration 146/1000 | Loss: 0.00001679
Iteration 147/1000 | Loss: 0.00001679
Iteration 148/1000 | Loss: 0.00001679
Iteration 149/1000 | Loss: 0.00001679
Iteration 150/1000 | Loss: 0.00001679
Iteration 151/1000 | Loss: 0.00001679
Iteration 152/1000 | Loss: 0.00001678
Iteration 153/1000 | Loss: 0.00001678
Iteration 154/1000 | Loss: 0.00001678
Iteration 155/1000 | Loss: 0.00001678
Iteration 156/1000 | Loss: 0.00001678
Iteration 157/1000 | Loss: 0.00001678
Iteration 158/1000 | Loss: 0.00001678
Iteration 159/1000 | Loss: 0.00001678
Iteration 160/1000 | Loss: 0.00001678
Iteration 161/1000 | Loss: 0.00001677
Iteration 162/1000 | Loss: 0.00001677
Iteration 163/1000 | Loss: 0.00001677
Iteration 164/1000 | Loss: 0.00001677
Iteration 165/1000 | Loss: 0.00001677
Iteration 166/1000 | Loss: 0.00001676
Iteration 167/1000 | Loss: 0.00001676
Iteration 168/1000 | Loss: 0.00001676
Iteration 169/1000 | Loss: 0.00001676
Iteration 170/1000 | Loss: 0.00001675
Iteration 171/1000 | Loss: 0.00001675
Iteration 172/1000 | Loss: 0.00001675
Iteration 173/1000 | Loss: 0.00001675
Iteration 174/1000 | Loss: 0.00001675
Iteration 175/1000 | Loss: 0.00001675
Iteration 176/1000 | Loss: 0.00001675
Iteration 177/1000 | Loss: 0.00001675
Iteration 178/1000 | Loss: 0.00001675
Iteration 179/1000 | Loss: 0.00001675
Iteration 180/1000 | Loss: 0.00001675
Iteration 181/1000 | Loss: 0.00001674
Iteration 182/1000 | Loss: 0.00001674
Iteration 183/1000 | Loss: 0.00001674
Iteration 184/1000 | Loss: 0.00001674
Iteration 185/1000 | Loss: 0.00001674
Iteration 186/1000 | Loss: 0.00001674
Iteration 187/1000 | Loss: 0.00001674
Iteration 188/1000 | Loss: 0.00001674
Iteration 189/1000 | Loss: 0.00001674
Iteration 190/1000 | Loss: 0.00001674
Iteration 191/1000 | Loss: 0.00001673
Iteration 192/1000 | Loss: 0.00001673
Iteration 193/1000 | Loss: 0.00001673
Iteration 194/1000 | Loss: 0.00001673
Iteration 195/1000 | Loss: 0.00001673
Iteration 196/1000 | Loss: 0.00001673
Iteration 197/1000 | Loss: 0.00001673
Iteration 198/1000 | Loss: 0.00001673
Iteration 199/1000 | Loss: 0.00001672
Iteration 200/1000 | Loss: 0.00001672
Iteration 201/1000 | Loss: 0.00001672
Iteration 202/1000 | Loss: 0.00001672
Iteration 203/1000 | Loss: 0.00001672
Iteration 204/1000 | Loss: 0.00001672
Iteration 205/1000 | Loss: 0.00001672
Iteration 206/1000 | Loss: 0.00001672
Iteration 207/1000 | Loss: 0.00001672
Iteration 208/1000 | Loss: 0.00001672
Iteration 209/1000 | Loss: 0.00001672
Iteration 210/1000 | Loss: 0.00001671
Iteration 211/1000 | Loss: 0.00001671
Iteration 212/1000 | Loss: 0.00001671
Iteration 213/1000 | Loss: 0.00001671
Iteration 214/1000 | Loss: 0.00001671
Iteration 215/1000 | Loss: 0.00001671
Iteration 216/1000 | Loss: 0.00001671
Iteration 217/1000 | Loss: 0.00001671
Iteration 218/1000 | Loss: 0.00001671
Iteration 219/1000 | Loss: 0.00001671
Iteration 220/1000 | Loss: 0.00001671
Iteration 221/1000 | Loss: 0.00001671
Iteration 222/1000 | Loss: 0.00001671
Iteration 223/1000 | Loss: 0.00001670
Iteration 224/1000 | Loss: 0.00001670
Iteration 225/1000 | Loss: 0.00001670
Iteration 226/1000 | Loss: 0.00001670
Iteration 227/1000 | Loss: 0.00001670
Iteration 228/1000 | Loss: 0.00001670
Iteration 229/1000 | Loss: 0.00001670
Iteration 230/1000 | Loss: 0.00001670
Iteration 231/1000 | Loss: 0.00001670
Iteration 232/1000 | Loss: 0.00001670
Iteration 233/1000 | Loss: 0.00001670
Iteration 234/1000 | Loss: 0.00001670
Iteration 235/1000 | Loss: 0.00001669
Iteration 236/1000 | Loss: 0.00001669
Iteration 237/1000 | Loss: 0.00001669
Iteration 238/1000 | Loss: 0.00001669
Iteration 239/1000 | Loss: 0.00001669
Iteration 240/1000 | Loss: 0.00001669
Iteration 241/1000 | Loss: 0.00001669
Iteration 242/1000 | Loss: 0.00001669
Iteration 243/1000 | Loss: 0.00001669
Iteration 244/1000 | Loss: 0.00001669
Iteration 245/1000 | Loss: 0.00001669
Iteration 246/1000 | Loss: 0.00001669
Iteration 247/1000 | Loss: 0.00001669
Iteration 248/1000 | Loss: 0.00001669
Iteration 249/1000 | Loss: 0.00001669
Iteration 250/1000 | Loss: 0.00001669
Iteration 251/1000 | Loss: 0.00001669
Iteration 252/1000 | Loss: 0.00001669
Iteration 253/1000 | Loss: 0.00001668
Iteration 254/1000 | Loss: 0.00001668
Iteration 255/1000 | Loss: 0.00001668
Iteration 256/1000 | Loss: 0.00001668
Iteration 257/1000 | Loss: 0.00001668
Iteration 258/1000 | Loss: 0.00001668
Iteration 259/1000 | Loss: 0.00001668
Iteration 260/1000 | Loss: 0.00001668
Iteration 261/1000 | Loss: 0.00001668
Iteration 262/1000 | Loss: 0.00001668
Iteration 263/1000 | Loss: 0.00001668
Iteration 264/1000 | Loss: 0.00001668
Iteration 265/1000 | Loss: 0.00001668
Iteration 266/1000 | Loss: 0.00001668
Iteration 267/1000 | Loss: 0.00001668
Iteration 268/1000 | Loss: 0.00001668
Iteration 269/1000 | Loss: 0.00001668
Iteration 270/1000 | Loss: 0.00001668
Iteration 271/1000 | Loss: 0.00001668
Iteration 272/1000 | Loss: 0.00001668
Iteration 273/1000 | Loss: 0.00001668
Iteration 274/1000 | Loss: 0.00001667
Iteration 275/1000 | Loss: 0.00001667
Iteration 276/1000 | Loss: 0.00001667
Iteration 277/1000 | Loss: 0.00001667
Iteration 278/1000 | Loss: 0.00001667
Iteration 279/1000 | Loss: 0.00001667
Iteration 280/1000 | Loss: 0.00001667
Iteration 281/1000 | Loss: 0.00001667
Iteration 282/1000 | Loss: 0.00001667
Iteration 283/1000 | Loss: 0.00001667
Iteration 284/1000 | Loss: 0.00001667
Iteration 285/1000 | Loss: 0.00001667
Iteration 286/1000 | Loss: 0.00001667
Iteration 287/1000 | Loss: 0.00001667
Iteration 288/1000 | Loss: 0.00001667
Iteration 289/1000 | Loss: 0.00001667
Iteration 290/1000 | Loss: 0.00001667
Iteration 291/1000 | Loss: 0.00001667
Iteration 292/1000 | Loss: 0.00001667
Iteration 293/1000 | Loss: 0.00001667
Iteration 294/1000 | Loss: 0.00001667
Iteration 295/1000 | Loss: 0.00001667
Iteration 296/1000 | Loss: 0.00001667
Iteration 297/1000 | Loss: 0.00001667
Iteration 298/1000 | Loss: 0.00001667
Iteration 299/1000 | Loss: 0.00001667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [1.6668125681462698e-05, 1.6668125681462698e-05, 1.6668125681462698e-05, 1.6668125681462698e-05, 1.6668125681462698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6668125681462698e-05

Optimization complete. Final v2v error: 3.468848705291748 mm

Highest mean error: 4.468507766723633 mm for frame 88

Lowest mean error: 3.0456793308258057 mm for frame 1

Saving results

Total time: 144.1014382839203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550118
Iteration 2/25 | Loss: 0.00148277
Iteration 3/25 | Loss: 0.00130954
Iteration 4/25 | Loss: 0.00127979
Iteration 5/25 | Loss: 0.00125510
Iteration 6/25 | Loss: 0.00125561
Iteration 7/25 | Loss: 0.00125726
Iteration 8/25 | Loss: 0.00124788
Iteration 9/25 | Loss: 0.00123930
Iteration 10/25 | Loss: 0.00123313
Iteration 11/25 | Loss: 0.00122895
Iteration 12/25 | Loss: 0.00122777
Iteration 13/25 | Loss: 0.00122685
Iteration 14/25 | Loss: 0.00122668
Iteration 15/25 | Loss: 0.00122659
Iteration 16/25 | Loss: 0.00122654
Iteration 17/25 | Loss: 0.00122653
Iteration 18/25 | Loss: 0.00122652
Iteration 19/25 | Loss: 0.00122652
Iteration 20/25 | Loss: 0.00122652
Iteration 21/25 | Loss: 0.00122652
Iteration 22/25 | Loss: 0.00122652
Iteration 23/25 | Loss: 0.00122652
Iteration 24/25 | Loss: 0.00122652
Iteration 25/25 | Loss: 0.00122652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29600191
Iteration 2/25 | Loss: 0.00160443
Iteration 3/25 | Loss: 0.00160443
Iteration 4/25 | Loss: 0.00160443
Iteration 5/25 | Loss: 0.00160443
Iteration 6/25 | Loss: 0.00160443
Iteration 7/25 | Loss: 0.00160443
Iteration 8/25 | Loss: 0.00160443
Iteration 9/25 | Loss: 0.00160442
Iteration 10/25 | Loss: 0.00160442
Iteration 11/25 | Loss: 0.00160442
Iteration 12/25 | Loss: 0.00160442
Iteration 13/25 | Loss: 0.00160442
Iteration 14/25 | Loss: 0.00160442
Iteration 15/25 | Loss: 0.00160442
Iteration 16/25 | Loss: 0.00160442
Iteration 17/25 | Loss: 0.00160442
Iteration 18/25 | Loss: 0.00160442
Iteration 19/25 | Loss: 0.00160442
Iteration 20/25 | Loss: 0.00160442
Iteration 21/25 | Loss: 0.00160442
Iteration 22/25 | Loss: 0.00160442
Iteration 23/25 | Loss: 0.00160442
Iteration 24/25 | Loss: 0.00160442
Iteration 25/25 | Loss: 0.00160442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160442
Iteration 2/1000 | Loss: 0.00004412
Iteration 3/1000 | Loss: 0.00002522
Iteration 4/1000 | Loss: 0.00002186
Iteration 5/1000 | Loss: 0.00002002
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001824
Iteration 10/1000 | Loss: 0.00001801
Iteration 11/1000 | Loss: 0.00001796
Iteration 12/1000 | Loss: 0.00001792
Iteration 13/1000 | Loss: 0.00001791
Iteration 14/1000 | Loss: 0.00001789
Iteration 15/1000 | Loss: 0.00001789
Iteration 16/1000 | Loss: 0.00001788
Iteration 17/1000 | Loss: 0.00001788
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001785
Iteration 21/1000 | Loss: 0.00001785
Iteration 22/1000 | Loss: 0.00001780
Iteration 23/1000 | Loss: 0.00001780
Iteration 24/1000 | Loss: 0.00001777
Iteration 25/1000 | Loss: 0.00001776
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001776
Iteration 29/1000 | Loss: 0.00001776
Iteration 30/1000 | Loss: 0.00001775
Iteration 31/1000 | Loss: 0.00001775
Iteration 32/1000 | Loss: 0.00001774
Iteration 33/1000 | Loss: 0.00001773
Iteration 34/1000 | Loss: 0.00001773
Iteration 35/1000 | Loss: 0.00001772
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001771
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001769
Iteration 41/1000 | Loss: 0.00001769
Iteration 42/1000 | Loss: 0.00001768
Iteration 43/1000 | Loss: 0.00001768
Iteration 44/1000 | Loss: 0.00001767
Iteration 45/1000 | Loss: 0.00001767
Iteration 46/1000 | Loss: 0.00001767
Iteration 47/1000 | Loss: 0.00001766
Iteration 48/1000 | Loss: 0.00001766
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001765
Iteration 51/1000 | Loss: 0.00001765
Iteration 52/1000 | Loss: 0.00001764
Iteration 53/1000 | Loss: 0.00001764
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00001763
Iteration 56/1000 | Loss: 0.00001763
Iteration 57/1000 | Loss: 0.00001763
Iteration 58/1000 | Loss: 0.00001763
Iteration 59/1000 | Loss: 0.00001762
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001760
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00001760
Iteration 74/1000 | Loss: 0.00001760
Iteration 75/1000 | Loss: 0.00001760
Iteration 76/1000 | Loss: 0.00001760
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001759
Iteration 85/1000 | Loss: 0.00001759
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001759
Iteration 88/1000 | Loss: 0.00001759
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Iteration 92/1000 | Loss: 0.00001758
Iteration 93/1000 | Loss: 0.00001758
Iteration 94/1000 | Loss: 0.00001758
Iteration 95/1000 | Loss: 0.00001758
Iteration 96/1000 | Loss: 0.00001758
Iteration 97/1000 | Loss: 0.00001758
Iteration 98/1000 | Loss: 0.00001758
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001757
Iteration 102/1000 | Loss: 0.00001757
Iteration 103/1000 | Loss: 0.00001757
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001757
Iteration 107/1000 | Loss: 0.00001757
Iteration 108/1000 | Loss: 0.00001757
Iteration 109/1000 | Loss: 0.00001757
Iteration 110/1000 | Loss: 0.00001757
Iteration 111/1000 | Loss: 0.00001757
Iteration 112/1000 | Loss: 0.00001757
Iteration 113/1000 | Loss: 0.00001757
Iteration 114/1000 | Loss: 0.00001757
Iteration 115/1000 | Loss: 0.00001757
Iteration 116/1000 | Loss: 0.00001757
Iteration 117/1000 | Loss: 0.00001757
Iteration 118/1000 | Loss: 0.00001757
Iteration 119/1000 | Loss: 0.00001757
Iteration 120/1000 | Loss: 0.00001757
Iteration 121/1000 | Loss: 0.00001757
Iteration 122/1000 | Loss: 0.00001757
Iteration 123/1000 | Loss: 0.00001757
Iteration 124/1000 | Loss: 0.00001757
Iteration 125/1000 | Loss: 0.00001757
Iteration 126/1000 | Loss: 0.00001757
Iteration 127/1000 | Loss: 0.00001757
Iteration 128/1000 | Loss: 0.00001757
Iteration 129/1000 | Loss: 0.00001757
Iteration 130/1000 | Loss: 0.00001757
Iteration 131/1000 | Loss: 0.00001757
Iteration 132/1000 | Loss: 0.00001757
Iteration 133/1000 | Loss: 0.00001757
Iteration 134/1000 | Loss: 0.00001757
Iteration 135/1000 | Loss: 0.00001757
Iteration 136/1000 | Loss: 0.00001757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.757389873091597e-05, 1.757389873091597e-05, 1.757389873091597e-05, 1.757389873091597e-05, 1.757389873091597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.757389873091597e-05

Optimization complete. Final v2v error: 3.544569969177246 mm

Highest mean error: 4.116215705871582 mm for frame 39

Lowest mean error: 3.0324909687042236 mm for frame 163

Saving results

Total time: 50.516624212265015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00298204
Iteration 2/25 | Loss: 0.00149976
Iteration 3/25 | Loss: 0.00127262
Iteration 4/25 | Loss: 0.00124366
Iteration 5/25 | Loss: 0.00123814
Iteration 6/25 | Loss: 0.00123624
Iteration 7/25 | Loss: 0.00123595
Iteration 8/25 | Loss: 0.00123595
Iteration 9/25 | Loss: 0.00123595
Iteration 10/25 | Loss: 0.00123595
Iteration 11/25 | Loss: 0.00123595
Iteration 12/25 | Loss: 0.00123595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012359502725303173, 0.0012359502725303173, 0.0012359502725303173, 0.0012359502725303173, 0.0012359502725303173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012359502725303173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21716976
Iteration 2/25 | Loss: 0.00229458
Iteration 3/25 | Loss: 0.00229458
Iteration 4/25 | Loss: 0.00229458
Iteration 5/25 | Loss: 0.00229458
Iteration 6/25 | Loss: 0.00229458
Iteration 7/25 | Loss: 0.00229458
Iteration 8/25 | Loss: 0.00229458
Iteration 9/25 | Loss: 0.00229458
Iteration 10/25 | Loss: 0.00229458
Iteration 11/25 | Loss: 0.00229458
Iteration 12/25 | Loss: 0.00229458
Iteration 13/25 | Loss: 0.00229458
Iteration 14/25 | Loss: 0.00229458
Iteration 15/25 | Loss: 0.00229458
Iteration 16/25 | Loss: 0.00229458
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022945760283619165, 0.0022945760283619165, 0.0022945760283619165, 0.0022945760283619165, 0.0022945760283619165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022945760283619165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229458
Iteration 2/1000 | Loss: 0.00004734
Iteration 3/1000 | Loss: 0.00002478
Iteration 4/1000 | Loss: 0.00002034
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001827
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001756
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00001731
Iteration 12/1000 | Loss: 0.00001713
Iteration 13/1000 | Loss: 0.00001700
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001696
Iteration 16/1000 | Loss: 0.00001696
Iteration 17/1000 | Loss: 0.00001693
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001693
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001689
Iteration 26/1000 | Loss: 0.00001689
Iteration 27/1000 | Loss: 0.00001689
Iteration 28/1000 | Loss: 0.00001689
Iteration 29/1000 | Loss: 0.00001688
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001687
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001681
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001681
Iteration 44/1000 | Loss: 0.00001681
Iteration 45/1000 | Loss: 0.00001681
Iteration 46/1000 | Loss: 0.00001681
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001680
Iteration 49/1000 | Loss: 0.00001680
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001679
Iteration 52/1000 | Loss: 0.00001679
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00001678
Iteration 56/1000 | Loss: 0.00001678
Iteration 57/1000 | Loss: 0.00001678
Iteration 58/1000 | Loss: 0.00001678
Iteration 59/1000 | Loss: 0.00001677
Iteration 60/1000 | Loss: 0.00001677
Iteration 61/1000 | Loss: 0.00001677
Iteration 62/1000 | Loss: 0.00001677
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001676
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001674
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001672
Iteration 93/1000 | Loss: 0.00001672
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001671
Iteration 100/1000 | Loss: 0.00001671
Iteration 101/1000 | Loss: 0.00001671
Iteration 102/1000 | Loss: 0.00001671
Iteration 103/1000 | Loss: 0.00001671
Iteration 104/1000 | Loss: 0.00001671
Iteration 105/1000 | Loss: 0.00001671
Iteration 106/1000 | Loss: 0.00001671
Iteration 107/1000 | Loss: 0.00001671
Iteration 108/1000 | Loss: 0.00001671
Iteration 109/1000 | Loss: 0.00001671
Iteration 110/1000 | Loss: 0.00001671
Iteration 111/1000 | Loss: 0.00001671
Iteration 112/1000 | Loss: 0.00001670
Iteration 113/1000 | Loss: 0.00001670
Iteration 114/1000 | Loss: 0.00001670
Iteration 115/1000 | Loss: 0.00001670
Iteration 116/1000 | Loss: 0.00001670
Iteration 117/1000 | Loss: 0.00001670
Iteration 118/1000 | Loss: 0.00001670
Iteration 119/1000 | Loss: 0.00001670
Iteration 120/1000 | Loss: 0.00001670
Iteration 121/1000 | Loss: 0.00001670
Iteration 122/1000 | Loss: 0.00001670
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001669
Iteration 129/1000 | Loss: 0.00001669
Iteration 130/1000 | Loss: 0.00001669
Iteration 131/1000 | Loss: 0.00001669
Iteration 132/1000 | Loss: 0.00001669
Iteration 133/1000 | Loss: 0.00001669
Iteration 134/1000 | Loss: 0.00001669
Iteration 135/1000 | Loss: 0.00001668
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001668
Iteration 139/1000 | Loss: 0.00001668
Iteration 140/1000 | Loss: 0.00001668
Iteration 141/1000 | Loss: 0.00001668
Iteration 142/1000 | Loss: 0.00001668
Iteration 143/1000 | Loss: 0.00001668
Iteration 144/1000 | Loss: 0.00001668
Iteration 145/1000 | Loss: 0.00001668
Iteration 146/1000 | Loss: 0.00001668
Iteration 147/1000 | Loss: 0.00001667
Iteration 148/1000 | Loss: 0.00001667
Iteration 149/1000 | Loss: 0.00001667
Iteration 150/1000 | Loss: 0.00001667
Iteration 151/1000 | Loss: 0.00001667
Iteration 152/1000 | Loss: 0.00001667
Iteration 153/1000 | Loss: 0.00001667
Iteration 154/1000 | Loss: 0.00001667
Iteration 155/1000 | Loss: 0.00001667
Iteration 156/1000 | Loss: 0.00001667
Iteration 157/1000 | Loss: 0.00001667
Iteration 158/1000 | Loss: 0.00001666
Iteration 159/1000 | Loss: 0.00001666
Iteration 160/1000 | Loss: 0.00001666
Iteration 161/1000 | Loss: 0.00001666
Iteration 162/1000 | Loss: 0.00001666
Iteration 163/1000 | Loss: 0.00001666
Iteration 164/1000 | Loss: 0.00001666
Iteration 165/1000 | Loss: 0.00001666
Iteration 166/1000 | Loss: 0.00001666
Iteration 167/1000 | Loss: 0.00001666
Iteration 168/1000 | Loss: 0.00001665
Iteration 169/1000 | Loss: 0.00001665
Iteration 170/1000 | Loss: 0.00001665
Iteration 171/1000 | Loss: 0.00001665
Iteration 172/1000 | Loss: 0.00001665
Iteration 173/1000 | Loss: 0.00001665
Iteration 174/1000 | Loss: 0.00001665
Iteration 175/1000 | Loss: 0.00001665
Iteration 176/1000 | Loss: 0.00001665
Iteration 177/1000 | Loss: 0.00001664
Iteration 178/1000 | Loss: 0.00001664
Iteration 179/1000 | Loss: 0.00001664
Iteration 180/1000 | Loss: 0.00001664
Iteration 181/1000 | Loss: 0.00001664
Iteration 182/1000 | Loss: 0.00001664
Iteration 183/1000 | Loss: 0.00001664
Iteration 184/1000 | Loss: 0.00001664
Iteration 185/1000 | Loss: 0.00001664
Iteration 186/1000 | Loss: 0.00001664
Iteration 187/1000 | Loss: 0.00001664
Iteration 188/1000 | Loss: 0.00001664
Iteration 189/1000 | Loss: 0.00001664
Iteration 190/1000 | Loss: 0.00001664
Iteration 191/1000 | Loss: 0.00001664
Iteration 192/1000 | Loss: 0.00001664
Iteration 193/1000 | Loss: 0.00001664
Iteration 194/1000 | Loss: 0.00001664
Iteration 195/1000 | Loss: 0.00001664
Iteration 196/1000 | Loss: 0.00001664
Iteration 197/1000 | Loss: 0.00001664
Iteration 198/1000 | Loss: 0.00001664
Iteration 199/1000 | Loss: 0.00001664
Iteration 200/1000 | Loss: 0.00001664
Iteration 201/1000 | Loss: 0.00001664
Iteration 202/1000 | Loss: 0.00001664
Iteration 203/1000 | Loss: 0.00001664
Iteration 204/1000 | Loss: 0.00001664
Iteration 205/1000 | Loss: 0.00001664
Iteration 206/1000 | Loss: 0.00001664
Iteration 207/1000 | Loss: 0.00001664
Iteration 208/1000 | Loss: 0.00001664
Iteration 209/1000 | Loss: 0.00001664
Iteration 210/1000 | Loss: 0.00001664
Iteration 211/1000 | Loss: 0.00001664
Iteration 212/1000 | Loss: 0.00001664
Iteration 213/1000 | Loss: 0.00001664
Iteration 214/1000 | Loss: 0.00001664
Iteration 215/1000 | Loss: 0.00001664
Iteration 216/1000 | Loss: 0.00001664
Iteration 217/1000 | Loss: 0.00001664
Iteration 218/1000 | Loss: 0.00001664
Iteration 219/1000 | Loss: 0.00001664
Iteration 220/1000 | Loss: 0.00001664
Iteration 221/1000 | Loss: 0.00001664
Iteration 222/1000 | Loss: 0.00001664
Iteration 223/1000 | Loss: 0.00001664
Iteration 224/1000 | Loss: 0.00001664
Iteration 225/1000 | Loss: 0.00001664
Iteration 226/1000 | Loss: 0.00001664
Iteration 227/1000 | Loss: 0.00001664
Iteration 228/1000 | Loss: 0.00001664
Iteration 229/1000 | Loss: 0.00001664
Iteration 230/1000 | Loss: 0.00001664
Iteration 231/1000 | Loss: 0.00001664
Iteration 232/1000 | Loss: 0.00001664
Iteration 233/1000 | Loss: 0.00001664
Iteration 234/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.6642623450024985e-05, 1.6642623450024985e-05, 1.6642623450024985e-05, 1.6642623450024985e-05, 1.6642623450024985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6642623450024985e-05

Optimization complete. Final v2v error: 3.4248626232147217 mm

Highest mean error: 3.835709810256958 mm for frame 131

Lowest mean error: 3.115823984146118 mm for frame 189

Saving results

Total time: 41.17084503173828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368035
Iteration 2/25 | Loss: 0.00121576
Iteration 3/25 | Loss: 0.00115364
Iteration 4/25 | Loss: 0.00114765
Iteration 5/25 | Loss: 0.00114581
Iteration 6/25 | Loss: 0.00114531
Iteration 7/25 | Loss: 0.00114531
Iteration 8/25 | Loss: 0.00114531
Iteration 9/25 | Loss: 0.00114531
Iteration 10/25 | Loss: 0.00114531
Iteration 11/25 | Loss: 0.00114531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011453117476776242, 0.0011453117476776242, 0.0011453117476776242, 0.0011453117476776242, 0.0011453117476776242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011453117476776242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26799703
Iteration 2/25 | Loss: 0.00150795
Iteration 3/25 | Loss: 0.00150795
Iteration 4/25 | Loss: 0.00150795
Iteration 5/25 | Loss: 0.00150795
Iteration 6/25 | Loss: 0.00150795
Iteration 7/25 | Loss: 0.00150795
Iteration 8/25 | Loss: 0.00150795
Iteration 9/25 | Loss: 0.00150795
Iteration 10/25 | Loss: 0.00150795
Iteration 11/25 | Loss: 0.00150795
Iteration 12/25 | Loss: 0.00150795
Iteration 13/25 | Loss: 0.00150795
Iteration 14/25 | Loss: 0.00150795
Iteration 15/25 | Loss: 0.00150795
Iteration 16/25 | Loss: 0.00150795
Iteration 17/25 | Loss: 0.00150795
Iteration 18/25 | Loss: 0.00150795
Iteration 19/25 | Loss: 0.00150795
Iteration 20/25 | Loss: 0.00150795
Iteration 21/25 | Loss: 0.00150795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015079487347975373, 0.0015079487347975373, 0.0015079487347975373, 0.0015079487347975373, 0.0015079487347975373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015079487347975373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150795
Iteration 2/1000 | Loss: 0.00002965
Iteration 3/1000 | Loss: 0.00001791
Iteration 4/1000 | Loss: 0.00001518
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001375
Iteration 7/1000 | Loss: 0.00001339
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001320
Iteration 10/1000 | Loss: 0.00001319
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001314
Iteration 15/1000 | Loss: 0.00001314
Iteration 16/1000 | Loss: 0.00001314
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001311
Iteration 20/1000 | Loss: 0.00001311
Iteration 21/1000 | Loss: 0.00001310
Iteration 22/1000 | Loss: 0.00001310
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001310
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001309
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001307
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001306
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001304
Iteration 47/1000 | Loss: 0.00001303
Iteration 48/1000 | Loss: 0.00001303
Iteration 49/1000 | Loss: 0.00001303
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001302
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001301
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001298
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001298
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001298
Iteration 72/1000 | Loss: 0.00001298
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001298
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001298
Iteration 82/1000 | Loss: 0.00001298
Iteration 83/1000 | Loss: 0.00001298
Iteration 84/1000 | Loss: 0.00001298
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.2979326129425317e-05, 1.2979326129425317e-05, 1.2979326129425317e-05, 1.2979326129425317e-05, 1.2979326129425317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2979326129425317e-05

Optimization complete. Final v2v error: 3.033262252807617 mm

Highest mean error: 3.1399998664855957 mm for frame 106

Lowest mean error: 2.881225109100342 mm for frame 6

Saving results

Total time: 25.286577939987183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852332
Iteration 2/25 | Loss: 0.00133140
Iteration 3/25 | Loss: 0.00123880
Iteration 4/25 | Loss: 0.00123153
Iteration 5/25 | Loss: 0.00122963
Iteration 6/25 | Loss: 0.00122963
Iteration 7/25 | Loss: 0.00122963
Iteration 8/25 | Loss: 0.00122963
Iteration 9/25 | Loss: 0.00122963
Iteration 10/25 | Loss: 0.00122963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012296345084905624, 0.0012296345084905624, 0.0012296345084905624, 0.0012296345084905624, 0.0012296345084905624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012296345084905624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23695922
Iteration 2/25 | Loss: 0.00190976
Iteration 3/25 | Loss: 0.00190976
Iteration 4/25 | Loss: 0.00190976
Iteration 5/25 | Loss: 0.00190976
Iteration 6/25 | Loss: 0.00190976
Iteration 7/25 | Loss: 0.00190976
Iteration 8/25 | Loss: 0.00190976
Iteration 9/25 | Loss: 0.00190976
Iteration 10/25 | Loss: 0.00190976
Iteration 11/25 | Loss: 0.00190976
Iteration 12/25 | Loss: 0.00190976
Iteration 13/25 | Loss: 0.00190976
Iteration 14/25 | Loss: 0.00190976
Iteration 15/25 | Loss: 0.00190976
Iteration 16/25 | Loss: 0.00190976
Iteration 17/25 | Loss: 0.00190976
Iteration 18/25 | Loss: 0.00190976
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00190975540317595, 0.00190975540317595, 0.00190975540317595, 0.00190975540317595, 0.00190975540317595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00190975540317595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190976
Iteration 2/1000 | Loss: 0.00005584
Iteration 3/1000 | Loss: 0.00002951
Iteration 4/1000 | Loss: 0.00002334
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001883
Iteration 7/1000 | Loss: 0.00001816
Iteration 8/1000 | Loss: 0.00001762
Iteration 9/1000 | Loss: 0.00001733
Iteration 10/1000 | Loss: 0.00001713
Iteration 11/1000 | Loss: 0.00001694
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001650
Iteration 14/1000 | Loss: 0.00001643
Iteration 15/1000 | Loss: 0.00001638
Iteration 16/1000 | Loss: 0.00001633
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001628
Iteration 19/1000 | Loss: 0.00001627
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001625
Iteration 22/1000 | Loss: 0.00001624
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001622
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001621
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001619
Iteration 34/1000 | Loss: 0.00001619
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001618
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001616
Iteration 40/1000 | Loss: 0.00001610
Iteration 41/1000 | Loss: 0.00001608
Iteration 42/1000 | Loss: 0.00001608
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001606
Iteration 46/1000 | Loss: 0.00001605
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001602
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001601
Iteration 55/1000 | Loss: 0.00001601
Iteration 56/1000 | Loss: 0.00001600
Iteration 57/1000 | Loss: 0.00001600
Iteration 58/1000 | Loss: 0.00001600
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001599
Iteration 61/1000 | Loss: 0.00001597
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001596
Iteration 67/1000 | Loss: 0.00001595
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001594
Iteration 70/1000 | Loss: 0.00001594
Iteration 71/1000 | Loss: 0.00001593
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001592
Iteration 74/1000 | Loss: 0.00001591
Iteration 75/1000 | Loss: 0.00001591
Iteration 76/1000 | Loss: 0.00001590
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001587
Iteration 80/1000 | Loss: 0.00001586
Iteration 81/1000 | Loss: 0.00001586
Iteration 82/1000 | Loss: 0.00001586
Iteration 83/1000 | Loss: 0.00001585
Iteration 84/1000 | Loss: 0.00001585
Iteration 85/1000 | Loss: 0.00001585
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001585
Iteration 88/1000 | Loss: 0.00001585
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001584
Iteration 91/1000 | Loss: 0.00001584
Iteration 92/1000 | Loss: 0.00001584
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001583
Iteration 95/1000 | Loss: 0.00001583
Iteration 96/1000 | Loss: 0.00001583
Iteration 97/1000 | Loss: 0.00001583
Iteration 98/1000 | Loss: 0.00001582
Iteration 99/1000 | Loss: 0.00001582
Iteration 100/1000 | Loss: 0.00001582
Iteration 101/1000 | Loss: 0.00001582
Iteration 102/1000 | Loss: 0.00001582
Iteration 103/1000 | Loss: 0.00001581
Iteration 104/1000 | Loss: 0.00001581
Iteration 105/1000 | Loss: 0.00001581
Iteration 106/1000 | Loss: 0.00001581
Iteration 107/1000 | Loss: 0.00001580
Iteration 108/1000 | Loss: 0.00001580
Iteration 109/1000 | Loss: 0.00001580
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001579
Iteration 114/1000 | Loss: 0.00001579
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001578
Iteration 118/1000 | Loss: 0.00001577
Iteration 119/1000 | Loss: 0.00001577
Iteration 120/1000 | Loss: 0.00001577
Iteration 121/1000 | Loss: 0.00001577
Iteration 122/1000 | Loss: 0.00001577
Iteration 123/1000 | Loss: 0.00001577
Iteration 124/1000 | Loss: 0.00001576
Iteration 125/1000 | Loss: 0.00001576
Iteration 126/1000 | Loss: 0.00001576
Iteration 127/1000 | Loss: 0.00001576
Iteration 128/1000 | Loss: 0.00001576
Iteration 129/1000 | Loss: 0.00001576
Iteration 130/1000 | Loss: 0.00001576
Iteration 131/1000 | Loss: 0.00001576
Iteration 132/1000 | Loss: 0.00001576
Iteration 133/1000 | Loss: 0.00001575
Iteration 134/1000 | Loss: 0.00001575
Iteration 135/1000 | Loss: 0.00001575
Iteration 136/1000 | Loss: 0.00001575
Iteration 137/1000 | Loss: 0.00001575
Iteration 138/1000 | Loss: 0.00001575
Iteration 139/1000 | Loss: 0.00001575
Iteration 140/1000 | Loss: 0.00001575
Iteration 141/1000 | Loss: 0.00001575
Iteration 142/1000 | Loss: 0.00001574
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001574
Iteration 146/1000 | Loss: 0.00001574
Iteration 147/1000 | Loss: 0.00001574
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001574
Iteration 150/1000 | Loss: 0.00001574
Iteration 151/1000 | Loss: 0.00001574
Iteration 152/1000 | Loss: 0.00001574
Iteration 153/1000 | Loss: 0.00001574
Iteration 154/1000 | Loss: 0.00001574
Iteration 155/1000 | Loss: 0.00001574
Iteration 156/1000 | Loss: 0.00001573
Iteration 157/1000 | Loss: 0.00001573
Iteration 158/1000 | Loss: 0.00001573
Iteration 159/1000 | Loss: 0.00001573
Iteration 160/1000 | Loss: 0.00001573
Iteration 161/1000 | Loss: 0.00001573
Iteration 162/1000 | Loss: 0.00001573
Iteration 163/1000 | Loss: 0.00001573
Iteration 164/1000 | Loss: 0.00001573
Iteration 165/1000 | Loss: 0.00001572
Iteration 166/1000 | Loss: 0.00001572
Iteration 167/1000 | Loss: 0.00001572
Iteration 168/1000 | Loss: 0.00001572
Iteration 169/1000 | Loss: 0.00001572
Iteration 170/1000 | Loss: 0.00001572
Iteration 171/1000 | Loss: 0.00001572
Iteration 172/1000 | Loss: 0.00001572
Iteration 173/1000 | Loss: 0.00001572
Iteration 174/1000 | Loss: 0.00001571
Iteration 175/1000 | Loss: 0.00001571
Iteration 176/1000 | Loss: 0.00001571
Iteration 177/1000 | Loss: 0.00001571
Iteration 178/1000 | Loss: 0.00001571
Iteration 179/1000 | Loss: 0.00001571
Iteration 180/1000 | Loss: 0.00001571
Iteration 181/1000 | Loss: 0.00001571
Iteration 182/1000 | Loss: 0.00001571
Iteration 183/1000 | Loss: 0.00001571
Iteration 184/1000 | Loss: 0.00001571
Iteration 185/1000 | Loss: 0.00001571
Iteration 186/1000 | Loss: 0.00001571
Iteration 187/1000 | Loss: 0.00001571
Iteration 188/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.571300163050182e-05, 1.571300163050182e-05, 1.571300163050182e-05, 1.571300163050182e-05, 1.571300163050182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.571300163050182e-05

Optimization complete. Final v2v error: 3.3948490619659424 mm

Highest mean error: 3.6297757625579834 mm for frame 152

Lowest mean error: 3.088012933731079 mm for frame 1

Saving results

Total time: 47.76764965057373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970772
Iteration 2/25 | Loss: 0.00166849
Iteration 3/25 | Loss: 0.00131899
Iteration 4/25 | Loss: 0.00128957
Iteration 5/25 | Loss: 0.00127829
Iteration 6/25 | Loss: 0.00127540
Iteration 7/25 | Loss: 0.00127510
Iteration 8/25 | Loss: 0.00127510
Iteration 9/25 | Loss: 0.00127510
Iteration 10/25 | Loss: 0.00127510
Iteration 11/25 | Loss: 0.00127510
Iteration 12/25 | Loss: 0.00127510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012751002795994282, 0.0012751002795994282, 0.0012751002795994282, 0.0012751002795994282, 0.0012751002795994282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012751002795994282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09874725
Iteration 2/25 | Loss: 0.00127600
Iteration 3/25 | Loss: 0.00127600
Iteration 4/25 | Loss: 0.00127600
Iteration 5/25 | Loss: 0.00127600
Iteration 6/25 | Loss: 0.00127600
Iteration 7/25 | Loss: 0.00127600
Iteration 8/25 | Loss: 0.00127600
Iteration 9/25 | Loss: 0.00127600
Iteration 10/25 | Loss: 0.00127600
Iteration 11/25 | Loss: 0.00127600
Iteration 12/25 | Loss: 0.00127600
Iteration 13/25 | Loss: 0.00127600
Iteration 14/25 | Loss: 0.00127600
Iteration 15/25 | Loss: 0.00127600
Iteration 16/25 | Loss: 0.00127600
Iteration 17/25 | Loss: 0.00127600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012760000536218286, 0.0012760000536218286, 0.0012760000536218286, 0.0012760000536218286, 0.0012760000536218286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012760000536218286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127600
Iteration 2/1000 | Loss: 0.00005953
Iteration 3/1000 | Loss: 0.00004732
Iteration 4/1000 | Loss: 0.00004389
Iteration 5/1000 | Loss: 0.00004145
Iteration 6/1000 | Loss: 0.00003980
Iteration 7/1000 | Loss: 0.00003894
Iteration 8/1000 | Loss: 0.00003822
Iteration 9/1000 | Loss: 0.00003759
Iteration 10/1000 | Loss: 0.00003718
Iteration 11/1000 | Loss: 0.00003669
Iteration 12/1000 | Loss: 0.00003635
Iteration 13/1000 | Loss: 0.00003601
Iteration 14/1000 | Loss: 0.00003574
Iteration 15/1000 | Loss: 0.00003550
Iteration 16/1000 | Loss: 0.00003527
Iteration 17/1000 | Loss: 0.00003510
Iteration 18/1000 | Loss: 0.00003494
Iteration 19/1000 | Loss: 0.00003480
Iteration 20/1000 | Loss: 0.00003479
Iteration 21/1000 | Loss: 0.00003474
Iteration 22/1000 | Loss: 0.00003473
Iteration 23/1000 | Loss: 0.00003472
Iteration 24/1000 | Loss: 0.00003471
Iteration 25/1000 | Loss: 0.00003470
Iteration 26/1000 | Loss: 0.00003469
Iteration 27/1000 | Loss: 0.00003469
Iteration 28/1000 | Loss: 0.00003465
Iteration 29/1000 | Loss: 0.00003464
Iteration 30/1000 | Loss: 0.00003459
Iteration 31/1000 | Loss: 0.00003458
Iteration 32/1000 | Loss: 0.00003456
Iteration 33/1000 | Loss: 0.00003456
Iteration 34/1000 | Loss: 0.00003455
Iteration 35/1000 | Loss: 0.00003454
Iteration 36/1000 | Loss: 0.00003454
Iteration 37/1000 | Loss: 0.00003453
Iteration 38/1000 | Loss: 0.00003453
Iteration 39/1000 | Loss: 0.00003453
Iteration 40/1000 | Loss: 0.00003452
Iteration 41/1000 | Loss: 0.00003452
Iteration 42/1000 | Loss: 0.00003452
Iteration 43/1000 | Loss: 0.00003451
Iteration 44/1000 | Loss: 0.00003451
Iteration 45/1000 | Loss: 0.00003451
Iteration 46/1000 | Loss: 0.00003451
Iteration 47/1000 | Loss: 0.00003451
Iteration 48/1000 | Loss: 0.00003451
Iteration 49/1000 | Loss: 0.00003451
Iteration 50/1000 | Loss: 0.00003451
Iteration 51/1000 | Loss: 0.00003451
Iteration 52/1000 | Loss: 0.00003451
Iteration 53/1000 | Loss: 0.00003451
Iteration 54/1000 | Loss: 0.00003451
Iteration 55/1000 | Loss: 0.00003451
Iteration 56/1000 | Loss: 0.00003450
Iteration 57/1000 | Loss: 0.00003450
Iteration 58/1000 | Loss: 0.00003450
Iteration 59/1000 | Loss: 0.00003450
Iteration 60/1000 | Loss: 0.00003450
Iteration 61/1000 | Loss: 0.00003450
Iteration 62/1000 | Loss: 0.00003450
Iteration 63/1000 | Loss: 0.00003450
Iteration 64/1000 | Loss: 0.00003449
Iteration 65/1000 | Loss: 0.00003449
Iteration 66/1000 | Loss: 0.00003449
Iteration 67/1000 | Loss: 0.00003449
Iteration 68/1000 | Loss: 0.00003449
Iteration 69/1000 | Loss: 0.00003448
Iteration 70/1000 | Loss: 0.00003448
Iteration 71/1000 | Loss: 0.00003448
Iteration 72/1000 | Loss: 0.00003448
Iteration 73/1000 | Loss: 0.00003448
Iteration 74/1000 | Loss: 0.00003448
Iteration 75/1000 | Loss: 0.00003448
Iteration 76/1000 | Loss: 0.00003448
Iteration 77/1000 | Loss: 0.00003448
Iteration 78/1000 | Loss: 0.00003448
Iteration 79/1000 | Loss: 0.00003447
Iteration 80/1000 | Loss: 0.00003447
Iteration 81/1000 | Loss: 0.00003447
Iteration 82/1000 | Loss: 0.00003447
Iteration 83/1000 | Loss: 0.00003447
Iteration 84/1000 | Loss: 0.00003446
Iteration 85/1000 | Loss: 0.00003446
Iteration 86/1000 | Loss: 0.00003446
Iteration 87/1000 | Loss: 0.00003446
Iteration 88/1000 | Loss: 0.00003446
Iteration 89/1000 | Loss: 0.00003446
Iteration 90/1000 | Loss: 0.00003446
Iteration 91/1000 | Loss: 0.00003446
Iteration 92/1000 | Loss: 0.00003446
Iteration 93/1000 | Loss: 0.00003446
Iteration 94/1000 | Loss: 0.00003446
Iteration 95/1000 | Loss: 0.00003446
Iteration 96/1000 | Loss: 0.00003446
Iteration 97/1000 | Loss: 0.00003446
Iteration 98/1000 | Loss: 0.00003446
Iteration 99/1000 | Loss: 0.00003446
Iteration 100/1000 | Loss: 0.00003446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [3.445837501203641e-05, 3.445837501203641e-05, 3.445837501203641e-05, 3.445837501203641e-05, 3.445837501203641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.445837501203641e-05

Optimization complete. Final v2v error: 4.611875534057617 mm

Highest mean error: 6.3282294273376465 mm for frame 65

Lowest mean error: 3.3101212978363037 mm for frame 133

Saving results

Total time: 42.54997706413269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01124233
Iteration 2/25 | Loss: 0.00224535
Iteration 3/25 | Loss: 0.00157794
Iteration 4/25 | Loss: 0.00147784
Iteration 5/25 | Loss: 0.00145406
Iteration 6/25 | Loss: 0.00144960
Iteration 7/25 | Loss: 0.00142604
Iteration 8/25 | Loss: 0.00142309
Iteration 9/25 | Loss: 0.00142204
Iteration 10/25 | Loss: 0.00142174
Iteration 11/25 | Loss: 0.00142100
Iteration 12/25 | Loss: 0.00141915
Iteration 13/25 | Loss: 0.00141801
Iteration 14/25 | Loss: 0.00141769
Iteration 15/25 | Loss: 0.00141765
Iteration 16/25 | Loss: 0.00141765
Iteration 17/25 | Loss: 0.00141765
Iteration 18/25 | Loss: 0.00141765
Iteration 19/25 | Loss: 0.00141765
Iteration 20/25 | Loss: 0.00141765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014176475815474987, 0.0014176475815474987, 0.0014176475815474987, 0.0014176475815474987, 0.0014176475815474987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014176475815474987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.48909283
Iteration 2/25 | Loss: 0.00157286
Iteration 3/25 | Loss: 0.00157286
Iteration 4/25 | Loss: 0.00157286
Iteration 5/25 | Loss: 0.00157286
Iteration 6/25 | Loss: 0.00157286
Iteration 7/25 | Loss: 0.00157286
Iteration 8/25 | Loss: 0.00157286
Iteration 9/25 | Loss: 0.00157286
Iteration 10/25 | Loss: 0.00157286
Iteration 11/25 | Loss: 0.00157286
Iteration 12/25 | Loss: 0.00157286
Iteration 13/25 | Loss: 0.00157286
Iteration 14/25 | Loss: 0.00157286
Iteration 15/25 | Loss: 0.00157286
Iteration 16/25 | Loss: 0.00157286
Iteration 17/25 | Loss: 0.00157286
Iteration 18/25 | Loss: 0.00157286
Iteration 19/25 | Loss: 0.00157286
Iteration 20/25 | Loss: 0.00157286
Iteration 21/25 | Loss: 0.00157286
Iteration 22/25 | Loss: 0.00157286
Iteration 23/25 | Loss: 0.00157286
Iteration 24/25 | Loss: 0.00157286
Iteration 25/25 | Loss: 0.00157286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157286
Iteration 2/1000 | Loss: 0.00015544
Iteration 3/1000 | Loss: 0.00016343
Iteration 4/1000 | Loss: 0.00009915
Iteration 5/1000 | Loss: 0.00013668
Iteration 6/1000 | Loss: 0.00011581
Iteration 7/1000 | Loss: 0.00013560
Iteration 8/1000 | Loss: 0.00008683
Iteration 9/1000 | Loss: 0.00006436
Iteration 10/1000 | Loss: 0.00005814
Iteration 11/1000 | Loss: 0.00005600
Iteration 12/1000 | Loss: 0.00005455
Iteration 13/1000 | Loss: 0.00005328
Iteration 14/1000 | Loss: 0.00005203
Iteration 15/1000 | Loss: 0.00005135
Iteration 16/1000 | Loss: 0.00005094
Iteration 17/1000 | Loss: 0.00005049
Iteration 18/1000 | Loss: 0.00005005
Iteration 19/1000 | Loss: 0.00004960
Iteration 20/1000 | Loss: 0.00004922
Iteration 21/1000 | Loss: 0.00004895
Iteration 22/1000 | Loss: 0.00004872
Iteration 23/1000 | Loss: 0.00004850
Iteration 24/1000 | Loss: 0.00004832
Iteration 25/1000 | Loss: 0.00004824
Iteration 26/1000 | Loss: 0.00004811
Iteration 27/1000 | Loss: 0.00004810
Iteration 28/1000 | Loss: 0.00004803
Iteration 29/1000 | Loss: 0.00004793
Iteration 30/1000 | Loss: 0.00004789
Iteration 31/1000 | Loss: 0.00004785
Iteration 32/1000 | Loss: 0.00004784
Iteration 33/1000 | Loss: 0.00004784
Iteration 34/1000 | Loss: 0.00004784
Iteration 35/1000 | Loss: 0.00004777
Iteration 36/1000 | Loss: 0.00004771
Iteration 37/1000 | Loss: 0.00004766
Iteration 38/1000 | Loss: 0.00004761
Iteration 39/1000 | Loss: 0.00004761
Iteration 40/1000 | Loss: 0.00004757
Iteration 41/1000 | Loss: 0.00004757
Iteration 42/1000 | Loss: 0.00004754
Iteration 43/1000 | Loss: 0.00004754
Iteration 44/1000 | Loss: 0.00004754
Iteration 45/1000 | Loss: 0.00004754
Iteration 46/1000 | Loss: 0.00004754
Iteration 47/1000 | Loss: 0.00004754
Iteration 48/1000 | Loss: 0.00004754
Iteration 49/1000 | Loss: 0.00004754
Iteration 50/1000 | Loss: 0.00004754
Iteration 51/1000 | Loss: 0.00004754
Iteration 52/1000 | Loss: 0.00004754
Iteration 53/1000 | Loss: 0.00004753
Iteration 54/1000 | Loss: 0.00004753
Iteration 55/1000 | Loss: 0.00004753
Iteration 56/1000 | Loss: 0.00004753
Iteration 57/1000 | Loss: 0.00004753
Iteration 58/1000 | Loss: 0.00004751
Iteration 59/1000 | Loss: 0.00004751
Iteration 60/1000 | Loss: 0.00004751
Iteration 61/1000 | Loss: 0.00004751
Iteration 62/1000 | Loss: 0.00004751
Iteration 63/1000 | Loss: 0.00004751
Iteration 64/1000 | Loss: 0.00004751
Iteration 65/1000 | Loss: 0.00004750
Iteration 66/1000 | Loss: 0.00004750
Iteration 67/1000 | Loss: 0.00004750
Iteration 68/1000 | Loss: 0.00004750
Iteration 69/1000 | Loss: 0.00004750
Iteration 70/1000 | Loss: 0.00004750
Iteration 71/1000 | Loss: 0.00004750
Iteration 72/1000 | Loss: 0.00004750
Iteration 73/1000 | Loss: 0.00004750
Iteration 74/1000 | Loss: 0.00004750
Iteration 75/1000 | Loss: 0.00004750
Iteration 76/1000 | Loss: 0.00004750
Iteration 77/1000 | Loss: 0.00004750
Iteration 78/1000 | Loss: 0.00004749
Iteration 79/1000 | Loss: 0.00004749
Iteration 80/1000 | Loss: 0.00004748
Iteration 81/1000 | Loss: 0.00004748
Iteration 82/1000 | Loss: 0.00004748
Iteration 83/1000 | Loss: 0.00004748
Iteration 84/1000 | Loss: 0.00004748
Iteration 85/1000 | Loss: 0.00004747
Iteration 86/1000 | Loss: 0.00004747
Iteration 87/1000 | Loss: 0.00004747
Iteration 88/1000 | Loss: 0.00004746
Iteration 89/1000 | Loss: 0.00004746
Iteration 90/1000 | Loss: 0.00004746
Iteration 91/1000 | Loss: 0.00004746
Iteration 92/1000 | Loss: 0.00004746
Iteration 93/1000 | Loss: 0.00004746
Iteration 94/1000 | Loss: 0.00004746
Iteration 95/1000 | Loss: 0.00004746
Iteration 96/1000 | Loss: 0.00004745
Iteration 97/1000 | Loss: 0.00004745
Iteration 98/1000 | Loss: 0.00004745
Iteration 99/1000 | Loss: 0.00004745
Iteration 100/1000 | Loss: 0.00004744
Iteration 101/1000 | Loss: 0.00004744
Iteration 102/1000 | Loss: 0.00004743
Iteration 103/1000 | Loss: 0.00004743
Iteration 104/1000 | Loss: 0.00004743
Iteration 105/1000 | Loss: 0.00004743
Iteration 106/1000 | Loss: 0.00004743
Iteration 107/1000 | Loss: 0.00004742
Iteration 108/1000 | Loss: 0.00004741
Iteration 109/1000 | Loss: 0.00004741
Iteration 110/1000 | Loss: 0.00004741
Iteration 111/1000 | Loss: 0.00004741
Iteration 112/1000 | Loss: 0.00004741
Iteration 113/1000 | Loss: 0.00004741
Iteration 114/1000 | Loss: 0.00004741
Iteration 115/1000 | Loss: 0.00004741
Iteration 116/1000 | Loss: 0.00004741
Iteration 117/1000 | Loss: 0.00004740
Iteration 118/1000 | Loss: 0.00004740
Iteration 119/1000 | Loss: 0.00004740
Iteration 120/1000 | Loss: 0.00004740
Iteration 121/1000 | Loss: 0.00004740
Iteration 122/1000 | Loss: 0.00004740
Iteration 123/1000 | Loss: 0.00004739
Iteration 124/1000 | Loss: 0.00004739
Iteration 125/1000 | Loss: 0.00004739
Iteration 126/1000 | Loss: 0.00004739
Iteration 127/1000 | Loss: 0.00004739
Iteration 128/1000 | Loss: 0.00004739
Iteration 129/1000 | Loss: 0.00004739
Iteration 130/1000 | Loss: 0.00004739
Iteration 131/1000 | Loss: 0.00004739
Iteration 132/1000 | Loss: 0.00004739
Iteration 133/1000 | Loss: 0.00004738
Iteration 134/1000 | Loss: 0.00004738
Iteration 135/1000 | Loss: 0.00004738
Iteration 136/1000 | Loss: 0.00004738
Iteration 137/1000 | Loss: 0.00004738
Iteration 138/1000 | Loss: 0.00004738
Iteration 139/1000 | Loss: 0.00004738
Iteration 140/1000 | Loss: 0.00004737
Iteration 141/1000 | Loss: 0.00004737
Iteration 142/1000 | Loss: 0.00004737
Iteration 143/1000 | Loss: 0.00004737
Iteration 144/1000 | Loss: 0.00004737
Iteration 145/1000 | Loss: 0.00004736
Iteration 146/1000 | Loss: 0.00004736
Iteration 147/1000 | Loss: 0.00004736
Iteration 148/1000 | Loss: 0.00004736
Iteration 149/1000 | Loss: 0.00004736
Iteration 150/1000 | Loss: 0.00004736
Iteration 151/1000 | Loss: 0.00004735
Iteration 152/1000 | Loss: 0.00004735
Iteration 153/1000 | Loss: 0.00004735
Iteration 154/1000 | Loss: 0.00004735
Iteration 155/1000 | Loss: 0.00004735
Iteration 156/1000 | Loss: 0.00004735
Iteration 157/1000 | Loss: 0.00004735
Iteration 158/1000 | Loss: 0.00004735
Iteration 159/1000 | Loss: 0.00004735
Iteration 160/1000 | Loss: 0.00004735
Iteration 161/1000 | Loss: 0.00004735
Iteration 162/1000 | Loss: 0.00004735
Iteration 163/1000 | Loss: 0.00004735
Iteration 164/1000 | Loss: 0.00004735
Iteration 165/1000 | Loss: 0.00004735
Iteration 166/1000 | Loss: 0.00004734
Iteration 167/1000 | Loss: 0.00004734
Iteration 168/1000 | Loss: 0.00004734
Iteration 169/1000 | Loss: 0.00004734
Iteration 170/1000 | Loss: 0.00004734
Iteration 171/1000 | Loss: 0.00004734
Iteration 172/1000 | Loss: 0.00004734
Iteration 173/1000 | Loss: 0.00004733
Iteration 174/1000 | Loss: 0.00004733
Iteration 175/1000 | Loss: 0.00004733
Iteration 176/1000 | Loss: 0.00004733
Iteration 177/1000 | Loss: 0.00004733
Iteration 178/1000 | Loss: 0.00004733
Iteration 179/1000 | Loss: 0.00004733
Iteration 180/1000 | Loss: 0.00004733
Iteration 181/1000 | Loss: 0.00004733
Iteration 182/1000 | Loss: 0.00004733
Iteration 183/1000 | Loss: 0.00004732
Iteration 184/1000 | Loss: 0.00004732
Iteration 185/1000 | Loss: 0.00004732
Iteration 186/1000 | Loss: 0.00004732
Iteration 187/1000 | Loss: 0.00004732
Iteration 188/1000 | Loss: 0.00004732
Iteration 189/1000 | Loss: 0.00004732
Iteration 190/1000 | Loss: 0.00004732
Iteration 191/1000 | Loss: 0.00004732
Iteration 192/1000 | Loss: 0.00004732
Iteration 193/1000 | Loss: 0.00004731
Iteration 194/1000 | Loss: 0.00004731
Iteration 195/1000 | Loss: 0.00004731
Iteration 196/1000 | Loss: 0.00004731
Iteration 197/1000 | Loss: 0.00004731
Iteration 198/1000 | Loss: 0.00004731
Iteration 199/1000 | Loss: 0.00004731
Iteration 200/1000 | Loss: 0.00004731
Iteration 201/1000 | Loss: 0.00004731
Iteration 202/1000 | Loss: 0.00004731
Iteration 203/1000 | Loss: 0.00004731
Iteration 204/1000 | Loss: 0.00004731
Iteration 205/1000 | Loss: 0.00004730
Iteration 206/1000 | Loss: 0.00004730
Iteration 207/1000 | Loss: 0.00004730
Iteration 208/1000 | Loss: 0.00004730
Iteration 209/1000 | Loss: 0.00004730
Iteration 210/1000 | Loss: 0.00004730
Iteration 211/1000 | Loss: 0.00004730
Iteration 212/1000 | Loss: 0.00004730
Iteration 213/1000 | Loss: 0.00004730
Iteration 214/1000 | Loss: 0.00004730
Iteration 215/1000 | Loss: 0.00004730
Iteration 216/1000 | Loss: 0.00004730
Iteration 217/1000 | Loss: 0.00004730
Iteration 218/1000 | Loss: 0.00004730
Iteration 219/1000 | Loss: 0.00004730
Iteration 220/1000 | Loss: 0.00004730
Iteration 221/1000 | Loss: 0.00004730
Iteration 222/1000 | Loss: 0.00004729
Iteration 223/1000 | Loss: 0.00004729
Iteration 224/1000 | Loss: 0.00004729
Iteration 225/1000 | Loss: 0.00004729
Iteration 226/1000 | Loss: 0.00004729
Iteration 227/1000 | Loss: 0.00004729
Iteration 228/1000 | Loss: 0.00004729
Iteration 229/1000 | Loss: 0.00004729
Iteration 230/1000 | Loss: 0.00004729
Iteration 231/1000 | Loss: 0.00004729
Iteration 232/1000 | Loss: 0.00004729
Iteration 233/1000 | Loss: 0.00004729
Iteration 234/1000 | Loss: 0.00004729
Iteration 235/1000 | Loss: 0.00004729
Iteration 236/1000 | Loss: 0.00004729
Iteration 237/1000 | Loss: 0.00004729
Iteration 238/1000 | Loss: 0.00004729
Iteration 239/1000 | Loss: 0.00004729
Iteration 240/1000 | Loss: 0.00004729
Iteration 241/1000 | Loss: 0.00004729
Iteration 242/1000 | Loss: 0.00004729
Iteration 243/1000 | Loss: 0.00004729
Iteration 244/1000 | Loss: 0.00004729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [4.728946805698797e-05, 4.728946805698797e-05, 4.728946805698797e-05, 4.728946805698797e-05, 4.728946805698797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.728946805698797e-05

Optimization complete. Final v2v error: 5.380085468292236 mm

Highest mean error: 7.031518936157227 mm for frame 125

Lowest mean error: 3.643153667449951 mm for frame 5

Saving results

Total time: 92.94072556495667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946651
Iteration 2/25 | Loss: 0.00186442
Iteration 3/25 | Loss: 0.00139512
Iteration 4/25 | Loss: 0.00136332
Iteration 5/25 | Loss: 0.00135908
Iteration 6/25 | Loss: 0.00135836
Iteration 7/25 | Loss: 0.00135836
Iteration 8/25 | Loss: 0.00135836
Iteration 9/25 | Loss: 0.00135836
Iteration 10/25 | Loss: 0.00135836
Iteration 11/25 | Loss: 0.00135836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001358356443233788, 0.001358356443233788, 0.001358356443233788, 0.001358356443233788, 0.001358356443233788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001358356443233788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52463591
Iteration 2/25 | Loss: 0.00090342
Iteration 3/25 | Loss: 0.00090342
Iteration 4/25 | Loss: 0.00090341
Iteration 5/25 | Loss: 0.00090341
Iteration 6/25 | Loss: 0.00090341
Iteration 7/25 | Loss: 0.00090341
Iteration 8/25 | Loss: 0.00090341
Iteration 9/25 | Loss: 0.00090341
Iteration 10/25 | Loss: 0.00090341
Iteration 11/25 | Loss: 0.00090341
Iteration 12/25 | Loss: 0.00090341
Iteration 13/25 | Loss: 0.00090341
Iteration 14/25 | Loss: 0.00090341
Iteration 15/25 | Loss: 0.00090341
Iteration 16/25 | Loss: 0.00090341
Iteration 17/25 | Loss: 0.00090341
Iteration 18/25 | Loss: 0.00090341
Iteration 19/25 | Loss: 0.00090341
Iteration 20/25 | Loss: 0.00090341
Iteration 21/25 | Loss: 0.00090341
Iteration 22/25 | Loss: 0.00090341
Iteration 23/25 | Loss: 0.00090341
Iteration 24/25 | Loss: 0.00090341
Iteration 25/25 | Loss: 0.00090341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009034142713062465, 0.0009034142713062465, 0.0009034142713062465, 0.0009034142713062465, 0.0009034142713062465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009034142713062465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090341
Iteration 2/1000 | Loss: 0.00008768
Iteration 3/1000 | Loss: 0.00005581
Iteration 4/1000 | Loss: 0.00004531
Iteration 5/1000 | Loss: 0.00004184
Iteration 6/1000 | Loss: 0.00004030
Iteration 7/1000 | Loss: 0.00003939
Iteration 8/1000 | Loss: 0.00003893
Iteration 9/1000 | Loss: 0.00003864
Iteration 10/1000 | Loss: 0.00003844
Iteration 11/1000 | Loss: 0.00003815
Iteration 12/1000 | Loss: 0.00003797
Iteration 13/1000 | Loss: 0.00003779
Iteration 14/1000 | Loss: 0.00003771
Iteration 15/1000 | Loss: 0.00003770
Iteration 16/1000 | Loss: 0.00003770
Iteration 17/1000 | Loss: 0.00003769
Iteration 18/1000 | Loss: 0.00003765
Iteration 19/1000 | Loss: 0.00003758
Iteration 20/1000 | Loss: 0.00003754
Iteration 21/1000 | Loss: 0.00003754
Iteration 22/1000 | Loss: 0.00003754
Iteration 23/1000 | Loss: 0.00003754
Iteration 24/1000 | Loss: 0.00003754
Iteration 25/1000 | Loss: 0.00003753
Iteration 26/1000 | Loss: 0.00003753
Iteration 27/1000 | Loss: 0.00003753
Iteration 28/1000 | Loss: 0.00003746
Iteration 29/1000 | Loss: 0.00003742
Iteration 30/1000 | Loss: 0.00003741
Iteration 31/1000 | Loss: 0.00003738
Iteration 32/1000 | Loss: 0.00003738
Iteration 33/1000 | Loss: 0.00003738
Iteration 34/1000 | Loss: 0.00003738
Iteration 35/1000 | Loss: 0.00003738
Iteration 36/1000 | Loss: 0.00003738
Iteration 37/1000 | Loss: 0.00003738
Iteration 38/1000 | Loss: 0.00003738
Iteration 39/1000 | Loss: 0.00003738
Iteration 40/1000 | Loss: 0.00003737
Iteration 41/1000 | Loss: 0.00003737
Iteration 42/1000 | Loss: 0.00003737
Iteration 43/1000 | Loss: 0.00003737
Iteration 44/1000 | Loss: 0.00003736
Iteration 45/1000 | Loss: 0.00003736
Iteration 46/1000 | Loss: 0.00003734
Iteration 47/1000 | Loss: 0.00003734
Iteration 48/1000 | Loss: 0.00003734
Iteration 49/1000 | Loss: 0.00003734
Iteration 50/1000 | Loss: 0.00003734
Iteration 51/1000 | Loss: 0.00003734
Iteration 52/1000 | Loss: 0.00003734
Iteration 53/1000 | Loss: 0.00003733
Iteration 54/1000 | Loss: 0.00003731
Iteration 55/1000 | Loss: 0.00003731
Iteration 56/1000 | Loss: 0.00003730
Iteration 57/1000 | Loss: 0.00003730
Iteration 58/1000 | Loss: 0.00003730
Iteration 59/1000 | Loss: 0.00003730
Iteration 60/1000 | Loss: 0.00003730
Iteration 61/1000 | Loss: 0.00003729
Iteration 62/1000 | Loss: 0.00003729
Iteration 63/1000 | Loss: 0.00003729
Iteration 64/1000 | Loss: 0.00003729
Iteration 65/1000 | Loss: 0.00003729
Iteration 66/1000 | Loss: 0.00003729
Iteration 67/1000 | Loss: 0.00003729
Iteration 68/1000 | Loss: 0.00003729
Iteration 69/1000 | Loss: 0.00003729
Iteration 70/1000 | Loss: 0.00003729
Iteration 71/1000 | Loss: 0.00003729
Iteration 72/1000 | Loss: 0.00003729
Iteration 73/1000 | Loss: 0.00003728
Iteration 74/1000 | Loss: 0.00003728
Iteration 75/1000 | Loss: 0.00003728
Iteration 76/1000 | Loss: 0.00003728
Iteration 77/1000 | Loss: 0.00003728
Iteration 78/1000 | Loss: 0.00003727
Iteration 79/1000 | Loss: 0.00003727
Iteration 80/1000 | Loss: 0.00003726
Iteration 81/1000 | Loss: 0.00003726
Iteration 82/1000 | Loss: 0.00003726
Iteration 83/1000 | Loss: 0.00003725
Iteration 84/1000 | Loss: 0.00003724
Iteration 85/1000 | Loss: 0.00003724
Iteration 86/1000 | Loss: 0.00003724
Iteration 87/1000 | Loss: 0.00003724
Iteration 88/1000 | Loss: 0.00003724
Iteration 89/1000 | Loss: 0.00003723
Iteration 90/1000 | Loss: 0.00003723
Iteration 91/1000 | Loss: 0.00003723
Iteration 92/1000 | Loss: 0.00003723
Iteration 93/1000 | Loss: 0.00003723
Iteration 94/1000 | Loss: 0.00003723
Iteration 95/1000 | Loss: 0.00003723
Iteration 96/1000 | Loss: 0.00003723
Iteration 97/1000 | Loss: 0.00003723
Iteration 98/1000 | Loss: 0.00003722
Iteration 99/1000 | Loss: 0.00003721
Iteration 100/1000 | Loss: 0.00003721
Iteration 101/1000 | Loss: 0.00003721
Iteration 102/1000 | Loss: 0.00003721
Iteration 103/1000 | Loss: 0.00003720
Iteration 104/1000 | Loss: 0.00003720
Iteration 105/1000 | Loss: 0.00003720
Iteration 106/1000 | Loss: 0.00003720
Iteration 107/1000 | Loss: 0.00003719
Iteration 108/1000 | Loss: 0.00003719
Iteration 109/1000 | Loss: 0.00003719
Iteration 110/1000 | Loss: 0.00003718
Iteration 111/1000 | Loss: 0.00003718
Iteration 112/1000 | Loss: 0.00003718
Iteration 113/1000 | Loss: 0.00003718
Iteration 114/1000 | Loss: 0.00003718
Iteration 115/1000 | Loss: 0.00003717
Iteration 116/1000 | Loss: 0.00003717
Iteration 117/1000 | Loss: 0.00003717
Iteration 118/1000 | Loss: 0.00003717
Iteration 119/1000 | Loss: 0.00003717
Iteration 120/1000 | Loss: 0.00003717
Iteration 121/1000 | Loss: 0.00003716
Iteration 122/1000 | Loss: 0.00003716
Iteration 123/1000 | Loss: 0.00003715
Iteration 124/1000 | Loss: 0.00003715
Iteration 125/1000 | Loss: 0.00003715
Iteration 126/1000 | Loss: 0.00003715
Iteration 127/1000 | Loss: 0.00003715
Iteration 128/1000 | Loss: 0.00003715
Iteration 129/1000 | Loss: 0.00003715
Iteration 130/1000 | Loss: 0.00003714
Iteration 131/1000 | Loss: 0.00003714
Iteration 132/1000 | Loss: 0.00003714
Iteration 133/1000 | Loss: 0.00003714
Iteration 134/1000 | Loss: 0.00003714
Iteration 135/1000 | Loss: 0.00003714
Iteration 136/1000 | Loss: 0.00003714
Iteration 137/1000 | Loss: 0.00003714
Iteration 138/1000 | Loss: 0.00003714
Iteration 139/1000 | Loss: 0.00003713
Iteration 140/1000 | Loss: 0.00003713
Iteration 141/1000 | Loss: 0.00003713
Iteration 142/1000 | Loss: 0.00003713
Iteration 143/1000 | Loss: 0.00003713
Iteration 144/1000 | Loss: 0.00003713
Iteration 145/1000 | Loss: 0.00003713
Iteration 146/1000 | Loss: 0.00003713
Iteration 147/1000 | Loss: 0.00003713
Iteration 148/1000 | Loss: 0.00003713
Iteration 149/1000 | Loss: 0.00003712
Iteration 150/1000 | Loss: 0.00003712
Iteration 151/1000 | Loss: 0.00003712
Iteration 152/1000 | Loss: 0.00003712
Iteration 153/1000 | Loss: 0.00003711
Iteration 154/1000 | Loss: 0.00003711
Iteration 155/1000 | Loss: 0.00003711
Iteration 156/1000 | Loss: 0.00003711
Iteration 157/1000 | Loss: 0.00003711
Iteration 158/1000 | Loss: 0.00003711
Iteration 159/1000 | Loss: 0.00003711
Iteration 160/1000 | Loss: 0.00003711
Iteration 161/1000 | Loss: 0.00003711
Iteration 162/1000 | Loss: 0.00003711
Iteration 163/1000 | Loss: 0.00003711
Iteration 164/1000 | Loss: 0.00003711
Iteration 165/1000 | Loss: 0.00003710
Iteration 166/1000 | Loss: 0.00003710
Iteration 167/1000 | Loss: 0.00003710
Iteration 168/1000 | Loss: 0.00003710
Iteration 169/1000 | Loss: 0.00003710
Iteration 170/1000 | Loss: 0.00003710
Iteration 171/1000 | Loss: 0.00003710
Iteration 172/1000 | Loss: 0.00003709
Iteration 173/1000 | Loss: 0.00003709
Iteration 174/1000 | Loss: 0.00003709
Iteration 175/1000 | Loss: 0.00003709
Iteration 176/1000 | Loss: 0.00003709
Iteration 177/1000 | Loss: 0.00003709
Iteration 178/1000 | Loss: 0.00003709
Iteration 179/1000 | Loss: 0.00003709
Iteration 180/1000 | Loss: 0.00003709
Iteration 181/1000 | Loss: 0.00003708
Iteration 182/1000 | Loss: 0.00003708
Iteration 183/1000 | Loss: 0.00003708
Iteration 184/1000 | Loss: 0.00003708
Iteration 185/1000 | Loss: 0.00003708
Iteration 186/1000 | Loss: 0.00003708
Iteration 187/1000 | Loss: 0.00003708
Iteration 188/1000 | Loss: 0.00003708
Iteration 189/1000 | Loss: 0.00003708
Iteration 190/1000 | Loss: 0.00003708
Iteration 191/1000 | Loss: 0.00003708
Iteration 192/1000 | Loss: 0.00003708
Iteration 193/1000 | Loss: 0.00003708
Iteration 194/1000 | Loss: 0.00003708
Iteration 195/1000 | Loss: 0.00003708
Iteration 196/1000 | Loss: 0.00003708
Iteration 197/1000 | Loss: 0.00003708
Iteration 198/1000 | Loss: 0.00003708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [3.707678115461022e-05, 3.707678115461022e-05, 3.707678115461022e-05, 3.707678115461022e-05, 3.707678115461022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.707678115461022e-05

Optimization complete. Final v2v error: 4.958489894866943 mm

Highest mean error: 5.67213249206543 mm for frame 16

Lowest mean error: 4.6588134765625 mm for frame 65

Saving results

Total time: 45.21949243545532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839268
Iteration 2/25 | Loss: 0.00127235
Iteration 3/25 | Loss: 0.00120744
Iteration 4/25 | Loss: 0.00119967
Iteration 5/25 | Loss: 0.00119722
Iteration 6/25 | Loss: 0.00119663
Iteration 7/25 | Loss: 0.00119663
Iteration 8/25 | Loss: 0.00119663
Iteration 9/25 | Loss: 0.00119663
Iteration 10/25 | Loss: 0.00119663
Iteration 11/25 | Loss: 0.00119663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011966262245550752, 0.0011966262245550752, 0.0011966262245550752, 0.0011966262245550752, 0.0011966262245550752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011966262245550752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28517711
Iteration 2/25 | Loss: 0.00160341
Iteration 3/25 | Loss: 0.00160341
Iteration 4/25 | Loss: 0.00160341
Iteration 5/25 | Loss: 0.00160341
Iteration 6/25 | Loss: 0.00160341
Iteration 7/25 | Loss: 0.00160341
Iteration 8/25 | Loss: 0.00160340
Iteration 9/25 | Loss: 0.00160340
Iteration 10/25 | Loss: 0.00160340
Iteration 11/25 | Loss: 0.00160340
Iteration 12/25 | Loss: 0.00160340
Iteration 13/25 | Loss: 0.00160340
Iteration 14/25 | Loss: 0.00160340
Iteration 15/25 | Loss: 0.00160340
Iteration 16/25 | Loss: 0.00160340
Iteration 17/25 | Loss: 0.00160340
Iteration 18/25 | Loss: 0.00160340
Iteration 19/25 | Loss: 0.00160340
Iteration 20/25 | Loss: 0.00160340
Iteration 21/25 | Loss: 0.00160340
Iteration 22/25 | Loss: 0.00160340
Iteration 23/25 | Loss: 0.00160340
Iteration 24/25 | Loss: 0.00160340
Iteration 25/25 | Loss: 0.00160340

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160340
Iteration 2/1000 | Loss: 0.00003607
Iteration 3/1000 | Loss: 0.00002418
Iteration 4/1000 | Loss: 0.00002018
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001853
Iteration 7/1000 | Loss: 0.00001787
Iteration 8/1000 | Loss: 0.00001754
Iteration 9/1000 | Loss: 0.00001734
Iteration 10/1000 | Loss: 0.00001731
Iteration 11/1000 | Loss: 0.00001731
Iteration 12/1000 | Loss: 0.00001730
Iteration 13/1000 | Loss: 0.00001729
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001727
Iteration 16/1000 | Loss: 0.00001726
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001725
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001724
Iteration 21/1000 | Loss: 0.00001723
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001721
Iteration 27/1000 | Loss: 0.00001721
Iteration 28/1000 | Loss: 0.00001720
Iteration 29/1000 | Loss: 0.00001720
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001718
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001714
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001713
Iteration 48/1000 | Loss: 0.00001712
Iteration 49/1000 | Loss: 0.00001712
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001708
Iteration 52/1000 | Loss: 0.00001708
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001707
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001703
Iteration 65/1000 | Loss: 0.00001703
Iteration 66/1000 | Loss: 0.00001703
Iteration 67/1000 | Loss: 0.00001703
Iteration 68/1000 | Loss: 0.00001703
Iteration 69/1000 | Loss: 0.00001703
Iteration 70/1000 | Loss: 0.00001702
Iteration 71/1000 | Loss: 0.00001702
Iteration 72/1000 | Loss: 0.00001702
Iteration 73/1000 | Loss: 0.00001702
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001701
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001701
Iteration 105/1000 | Loss: 0.00001701
Iteration 106/1000 | Loss: 0.00001701
Iteration 107/1000 | Loss: 0.00001701
Iteration 108/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.7011027011903934e-05, 1.7011027011903934e-05, 1.7011027011903934e-05, 1.7011027011903934e-05, 1.7011027011903934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7011027011903934e-05

Optimization complete. Final v2v error: 3.455286979675293 mm

Highest mean error: 3.599670648574829 mm for frame 83

Lowest mean error: 3.26831316947937 mm for frame 8

Saving results

Total time: 28.710699319839478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895198
Iteration 2/25 | Loss: 0.00221375
Iteration 3/25 | Loss: 0.00182539
Iteration 4/25 | Loss: 0.00150670
Iteration 5/25 | Loss: 0.00149226
Iteration 6/25 | Loss: 0.00147813
Iteration 7/25 | Loss: 0.00144086
Iteration 8/25 | Loss: 0.00141373
Iteration 9/25 | Loss: 0.00141461
Iteration 10/25 | Loss: 0.00140586
Iteration 11/25 | Loss: 0.00140033
Iteration 12/25 | Loss: 0.00139483
Iteration 13/25 | Loss: 0.00138812
Iteration 14/25 | Loss: 0.00138684
Iteration 15/25 | Loss: 0.00138444
Iteration 16/25 | Loss: 0.00138276
Iteration 17/25 | Loss: 0.00138310
Iteration 18/25 | Loss: 0.00138707
Iteration 19/25 | Loss: 0.00138724
Iteration 20/25 | Loss: 0.00139009
Iteration 21/25 | Loss: 0.00139166
Iteration 22/25 | Loss: 0.00138947
Iteration 23/25 | Loss: 0.00138808
Iteration 24/25 | Loss: 0.00138673
Iteration 25/25 | Loss: 0.00138394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.65186596
Iteration 2/25 | Loss: 0.00365129
Iteration 3/25 | Loss: 0.00365129
Iteration 4/25 | Loss: 0.00365128
Iteration 5/25 | Loss: 0.00365128
Iteration 6/25 | Loss: 0.00365128
Iteration 7/25 | Loss: 0.00365128
Iteration 8/25 | Loss: 0.00365128
Iteration 9/25 | Loss: 0.00365128
Iteration 10/25 | Loss: 0.00365128
Iteration 11/25 | Loss: 0.00365128
Iteration 12/25 | Loss: 0.00365128
Iteration 13/25 | Loss: 0.00365128
Iteration 14/25 | Loss: 0.00365128
Iteration 15/25 | Loss: 0.00365128
Iteration 16/25 | Loss: 0.00365128
Iteration 17/25 | Loss: 0.00365128
Iteration 18/25 | Loss: 0.00365128
Iteration 19/25 | Loss: 0.00365128
Iteration 20/25 | Loss: 0.00365128
Iteration 21/25 | Loss: 0.00365128
Iteration 22/25 | Loss: 0.00365128
Iteration 23/25 | Loss: 0.00365128
Iteration 24/25 | Loss: 0.00365128
Iteration 25/25 | Loss: 0.00365128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00365128
Iteration 2/1000 | Loss: 0.00041103
Iteration 3/1000 | Loss: 0.00050877
Iteration 4/1000 | Loss: 0.00345019
Iteration 5/1000 | Loss: 0.00024557
Iteration 6/1000 | Loss: 0.00253393
Iteration 7/1000 | Loss: 0.00204156
Iteration 8/1000 | Loss: 0.00052262
Iteration 9/1000 | Loss: 0.00027106
Iteration 10/1000 | Loss: 0.00008269
Iteration 11/1000 | Loss: 0.00025717
Iteration 12/1000 | Loss: 0.00006578
Iteration 13/1000 | Loss: 0.00005504
Iteration 14/1000 | Loss: 0.00089749
Iteration 15/1000 | Loss: 0.00006605
Iteration 16/1000 | Loss: 0.00005039
Iteration 17/1000 | Loss: 0.00004393
Iteration 18/1000 | Loss: 0.00004137
Iteration 19/1000 | Loss: 0.00003945
Iteration 20/1000 | Loss: 0.00003791
Iteration 21/1000 | Loss: 0.00003672
Iteration 22/1000 | Loss: 0.00003583
Iteration 23/1000 | Loss: 0.00003508
Iteration 24/1000 | Loss: 0.00003440
Iteration 25/1000 | Loss: 0.00003387
Iteration 26/1000 | Loss: 0.00003340
Iteration 27/1000 | Loss: 0.00003312
Iteration 28/1000 | Loss: 0.00003284
Iteration 29/1000 | Loss: 0.00003265
Iteration 30/1000 | Loss: 0.00003265
Iteration 31/1000 | Loss: 0.00003261
Iteration 32/1000 | Loss: 0.00003246
Iteration 33/1000 | Loss: 0.00003241
Iteration 34/1000 | Loss: 0.00003233
Iteration 35/1000 | Loss: 0.00003232
Iteration 36/1000 | Loss: 0.00003230
Iteration 37/1000 | Loss: 0.00003230
Iteration 38/1000 | Loss: 0.00003226
Iteration 39/1000 | Loss: 0.00003223
Iteration 40/1000 | Loss: 0.00003222
Iteration 41/1000 | Loss: 0.00003222
Iteration 42/1000 | Loss: 0.00003221
Iteration 43/1000 | Loss: 0.00003220
Iteration 44/1000 | Loss: 0.00003219
Iteration 45/1000 | Loss: 0.00003216
Iteration 46/1000 | Loss: 0.00003213
Iteration 47/1000 | Loss: 0.00003212
Iteration 48/1000 | Loss: 0.00003212
Iteration 49/1000 | Loss: 0.00003212
Iteration 50/1000 | Loss: 0.00003211
Iteration 51/1000 | Loss: 0.00003211
Iteration 52/1000 | Loss: 0.00003210
Iteration 53/1000 | Loss: 0.00003210
Iteration 54/1000 | Loss: 0.00003209
Iteration 55/1000 | Loss: 0.00003209
Iteration 56/1000 | Loss: 0.00003208
Iteration 57/1000 | Loss: 0.00003204
Iteration 58/1000 | Loss: 0.00003202
Iteration 59/1000 | Loss: 0.00003201
Iteration 60/1000 | Loss: 0.00003200
Iteration 61/1000 | Loss: 0.00003200
Iteration 62/1000 | Loss: 0.00003197
Iteration 63/1000 | Loss: 0.00003197
Iteration 64/1000 | Loss: 0.00003196
Iteration 65/1000 | Loss: 0.00003196
Iteration 66/1000 | Loss: 0.00003195
Iteration 67/1000 | Loss: 0.00003195
Iteration 68/1000 | Loss: 0.00003194
Iteration 69/1000 | Loss: 0.00003194
Iteration 70/1000 | Loss: 0.00003194
Iteration 71/1000 | Loss: 0.00003193
Iteration 72/1000 | Loss: 0.00003193
Iteration 73/1000 | Loss: 0.00003193
Iteration 74/1000 | Loss: 0.00003192
Iteration 75/1000 | Loss: 0.00003191
Iteration 76/1000 | Loss: 0.00003191
Iteration 77/1000 | Loss: 0.00003190
Iteration 78/1000 | Loss: 0.00003188
Iteration 79/1000 | Loss: 0.00003188
Iteration 80/1000 | Loss: 0.00003188
Iteration 81/1000 | Loss: 0.00003188
Iteration 82/1000 | Loss: 0.00003187
Iteration 83/1000 | Loss: 0.00003186
Iteration 84/1000 | Loss: 0.00003185
Iteration 85/1000 | Loss: 0.00003185
Iteration 86/1000 | Loss: 0.00003185
Iteration 87/1000 | Loss: 0.00003184
Iteration 88/1000 | Loss: 0.00003184
Iteration 89/1000 | Loss: 0.00003184
Iteration 90/1000 | Loss: 0.00003184
Iteration 91/1000 | Loss: 0.00003184
Iteration 92/1000 | Loss: 0.00003184
Iteration 93/1000 | Loss: 0.00003184
Iteration 94/1000 | Loss: 0.00003184
Iteration 95/1000 | Loss: 0.00003183
Iteration 96/1000 | Loss: 0.00003183
Iteration 97/1000 | Loss: 0.00003183
Iteration 98/1000 | Loss: 0.00003182
Iteration 99/1000 | Loss: 0.00003182
Iteration 100/1000 | Loss: 0.00003182
Iteration 101/1000 | Loss: 0.00003181
Iteration 102/1000 | Loss: 0.00003181
Iteration 103/1000 | Loss: 0.00003181
Iteration 104/1000 | Loss: 0.00003180
Iteration 105/1000 | Loss: 0.00003180
Iteration 106/1000 | Loss: 0.00003180
Iteration 107/1000 | Loss: 0.00003180
Iteration 108/1000 | Loss: 0.00003179
Iteration 109/1000 | Loss: 0.00003179
Iteration 110/1000 | Loss: 0.00003179
Iteration 111/1000 | Loss: 0.00003179
Iteration 112/1000 | Loss: 0.00003178
Iteration 113/1000 | Loss: 0.00003178
Iteration 114/1000 | Loss: 0.00003177
Iteration 115/1000 | Loss: 0.00003177
Iteration 116/1000 | Loss: 0.00003176
Iteration 117/1000 | Loss: 0.00003176
Iteration 118/1000 | Loss: 0.00003176
Iteration 119/1000 | Loss: 0.00003176
Iteration 120/1000 | Loss: 0.00003175
Iteration 121/1000 | Loss: 0.00003175
Iteration 122/1000 | Loss: 0.00003174
Iteration 123/1000 | Loss: 0.00003174
Iteration 124/1000 | Loss: 0.00003174
Iteration 125/1000 | Loss: 0.00003174
Iteration 126/1000 | Loss: 0.00003174
Iteration 127/1000 | Loss: 0.00003174
Iteration 128/1000 | Loss: 0.00003173
Iteration 129/1000 | Loss: 0.00003173
Iteration 130/1000 | Loss: 0.00003173
Iteration 131/1000 | Loss: 0.00003173
Iteration 132/1000 | Loss: 0.00003173
Iteration 133/1000 | Loss: 0.00003173
Iteration 134/1000 | Loss: 0.00003172
Iteration 135/1000 | Loss: 0.00003172
Iteration 136/1000 | Loss: 0.00003172
Iteration 137/1000 | Loss: 0.00003172
Iteration 138/1000 | Loss: 0.00003172
Iteration 139/1000 | Loss: 0.00003171
Iteration 140/1000 | Loss: 0.00003171
Iteration 141/1000 | Loss: 0.00003171
Iteration 142/1000 | Loss: 0.00003171
Iteration 143/1000 | Loss: 0.00003170
Iteration 144/1000 | Loss: 0.00003170
Iteration 145/1000 | Loss: 0.00003170
Iteration 146/1000 | Loss: 0.00003170
Iteration 147/1000 | Loss: 0.00003170
Iteration 148/1000 | Loss: 0.00003170
Iteration 149/1000 | Loss: 0.00003170
Iteration 150/1000 | Loss: 0.00003170
Iteration 151/1000 | Loss: 0.00003170
Iteration 152/1000 | Loss: 0.00003170
Iteration 153/1000 | Loss: 0.00003170
Iteration 154/1000 | Loss: 0.00003170
Iteration 155/1000 | Loss: 0.00003170
Iteration 156/1000 | Loss: 0.00003169
Iteration 157/1000 | Loss: 0.00003169
Iteration 158/1000 | Loss: 0.00003169
Iteration 159/1000 | Loss: 0.00003169
Iteration 160/1000 | Loss: 0.00003169
Iteration 161/1000 | Loss: 0.00003169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [3.169166302541271e-05, 3.169166302541271e-05, 3.169166302541271e-05, 3.169166302541271e-05, 3.169166302541271e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.169166302541271e-05

Optimization complete. Final v2v error: 3.7937111854553223 mm

Highest mean error: 12.902708053588867 mm for frame 78

Lowest mean error: 2.931105375289917 mm for frame 30

Saving results

Total time: 106.55345416069031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053262
Iteration 2/25 | Loss: 0.00173110
Iteration 3/25 | Loss: 0.00129399
Iteration 4/25 | Loss: 0.00127273
Iteration 5/25 | Loss: 0.00126787
Iteration 6/25 | Loss: 0.00126649
Iteration 7/25 | Loss: 0.00126649
Iteration 8/25 | Loss: 0.00126649
Iteration 9/25 | Loss: 0.00126649
Iteration 10/25 | Loss: 0.00126649
Iteration 11/25 | Loss: 0.00126649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012664874084293842, 0.0012664874084293842, 0.0012664874084293842, 0.0012664874084293842, 0.0012664874084293842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012664874084293842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53872555
Iteration 2/25 | Loss: 0.00112426
Iteration 3/25 | Loss: 0.00112426
Iteration 4/25 | Loss: 0.00112425
Iteration 5/25 | Loss: 0.00112425
Iteration 6/25 | Loss: 0.00112425
Iteration 7/25 | Loss: 0.00112425
Iteration 8/25 | Loss: 0.00112425
Iteration 9/25 | Loss: 0.00112425
Iteration 10/25 | Loss: 0.00112425
Iteration 11/25 | Loss: 0.00112425
Iteration 12/25 | Loss: 0.00112425
Iteration 13/25 | Loss: 0.00112425
Iteration 14/25 | Loss: 0.00112425
Iteration 15/25 | Loss: 0.00112425
Iteration 16/25 | Loss: 0.00112425
Iteration 17/25 | Loss: 0.00112425
Iteration 18/25 | Loss: 0.00112425
Iteration 19/25 | Loss: 0.00112425
Iteration 20/25 | Loss: 0.00112425
Iteration 21/25 | Loss: 0.00112425
Iteration 22/25 | Loss: 0.00112425
Iteration 23/25 | Loss: 0.00112425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011242523323744535, 0.0011242523323744535, 0.0011242523323744535, 0.0011242523323744535, 0.0011242523323744535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011242523323744535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112425
Iteration 2/1000 | Loss: 0.00005619
Iteration 3/1000 | Loss: 0.00004004
Iteration 4/1000 | Loss: 0.00003532
Iteration 5/1000 | Loss: 0.00003264
Iteration 6/1000 | Loss: 0.00003179
Iteration 7/1000 | Loss: 0.00003114
Iteration 8/1000 | Loss: 0.00003053
Iteration 9/1000 | Loss: 0.00003018
Iteration 10/1000 | Loss: 0.00002994
Iteration 11/1000 | Loss: 0.00002968
Iteration 12/1000 | Loss: 0.00002961
Iteration 13/1000 | Loss: 0.00002943
Iteration 14/1000 | Loss: 0.00002929
Iteration 15/1000 | Loss: 0.00002915
Iteration 16/1000 | Loss: 0.00002909
Iteration 17/1000 | Loss: 0.00002902
Iteration 18/1000 | Loss: 0.00002889
Iteration 19/1000 | Loss: 0.00002884
Iteration 20/1000 | Loss: 0.00002883
Iteration 21/1000 | Loss: 0.00002881
Iteration 22/1000 | Loss: 0.00002879
Iteration 23/1000 | Loss: 0.00002871
Iteration 24/1000 | Loss: 0.00002864
Iteration 25/1000 | Loss: 0.00002863
Iteration 26/1000 | Loss: 0.00002863
Iteration 27/1000 | Loss: 0.00002862
Iteration 28/1000 | Loss: 0.00002862
Iteration 29/1000 | Loss: 0.00002862
Iteration 30/1000 | Loss: 0.00002861
Iteration 31/1000 | Loss: 0.00002861
Iteration 32/1000 | Loss: 0.00002861
Iteration 33/1000 | Loss: 0.00002861
Iteration 34/1000 | Loss: 0.00002861
Iteration 35/1000 | Loss: 0.00002861
Iteration 36/1000 | Loss: 0.00002860
Iteration 37/1000 | Loss: 0.00002860
Iteration 38/1000 | Loss: 0.00002860
Iteration 39/1000 | Loss: 0.00002860
Iteration 40/1000 | Loss: 0.00002859
Iteration 41/1000 | Loss: 0.00002859
Iteration 42/1000 | Loss: 0.00002859
Iteration 43/1000 | Loss: 0.00002858
Iteration 44/1000 | Loss: 0.00002858
Iteration 45/1000 | Loss: 0.00002858
Iteration 46/1000 | Loss: 0.00002858
Iteration 47/1000 | Loss: 0.00002858
Iteration 48/1000 | Loss: 0.00002858
Iteration 49/1000 | Loss: 0.00002858
Iteration 50/1000 | Loss: 0.00002858
Iteration 51/1000 | Loss: 0.00002858
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00002857
Iteration 54/1000 | Loss: 0.00002857
Iteration 55/1000 | Loss: 0.00002856
Iteration 56/1000 | Loss: 0.00002856
Iteration 57/1000 | Loss: 0.00002856
Iteration 58/1000 | Loss: 0.00002856
Iteration 59/1000 | Loss: 0.00002856
Iteration 60/1000 | Loss: 0.00002855
Iteration 61/1000 | Loss: 0.00002855
Iteration 62/1000 | Loss: 0.00002855
Iteration 63/1000 | Loss: 0.00002855
Iteration 64/1000 | Loss: 0.00002855
Iteration 65/1000 | Loss: 0.00002855
Iteration 66/1000 | Loss: 0.00002855
Iteration 67/1000 | Loss: 0.00002854
Iteration 68/1000 | Loss: 0.00002854
Iteration 69/1000 | Loss: 0.00002854
Iteration 70/1000 | Loss: 0.00002854
Iteration 71/1000 | Loss: 0.00002854
Iteration 72/1000 | Loss: 0.00002854
Iteration 73/1000 | Loss: 0.00002854
Iteration 74/1000 | Loss: 0.00002854
Iteration 75/1000 | Loss: 0.00002853
Iteration 76/1000 | Loss: 0.00002853
Iteration 77/1000 | Loss: 0.00002853
Iteration 78/1000 | Loss: 0.00002853
Iteration 79/1000 | Loss: 0.00002852
Iteration 80/1000 | Loss: 0.00002852
Iteration 81/1000 | Loss: 0.00002852
Iteration 82/1000 | Loss: 0.00002852
Iteration 83/1000 | Loss: 0.00002852
Iteration 84/1000 | Loss: 0.00002852
Iteration 85/1000 | Loss: 0.00002852
Iteration 86/1000 | Loss: 0.00002852
Iteration 87/1000 | Loss: 0.00002851
Iteration 88/1000 | Loss: 0.00002851
Iteration 89/1000 | Loss: 0.00002851
Iteration 90/1000 | Loss: 0.00002851
Iteration 91/1000 | Loss: 0.00002851
Iteration 92/1000 | Loss: 0.00002851
Iteration 93/1000 | Loss: 0.00002851
Iteration 94/1000 | Loss: 0.00002851
Iteration 95/1000 | Loss: 0.00002851
Iteration 96/1000 | Loss: 0.00002851
Iteration 97/1000 | Loss: 0.00002851
Iteration 98/1000 | Loss: 0.00002851
Iteration 99/1000 | Loss: 0.00002850
Iteration 100/1000 | Loss: 0.00002850
Iteration 101/1000 | Loss: 0.00002850
Iteration 102/1000 | Loss: 0.00002850
Iteration 103/1000 | Loss: 0.00002850
Iteration 104/1000 | Loss: 0.00002850
Iteration 105/1000 | Loss: 0.00002849
Iteration 106/1000 | Loss: 0.00002849
Iteration 107/1000 | Loss: 0.00002849
Iteration 108/1000 | Loss: 0.00002849
Iteration 109/1000 | Loss: 0.00002849
Iteration 110/1000 | Loss: 0.00002849
Iteration 111/1000 | Loss: 0.00002849
Iteration 112/1000 | Loss: 0.00002849
Iteration 113/1000 | Loss: 0.00002849
Iteration 114/1000 | Loss: 0.00002848
Iteration 115/1000 | Loss: 0.00002848
Iteration 116/1000 | Loss: 0.00002848
Iteration 117/1000 | Loss: 0.00002848
Iteration 118/1000 | Loss: 0.00002847
Iteration 119/1000 | Loss: 0.00002847
Iteration 120/1000 | Loss: 0.00002847
Iteration 121/1000 | Loss: 0.00002847
Iteration 122/1000 | Loss: 0.00002846
Iteration 123/1000 | Loss: 0.00002846
Iteration 124/1000 | Loss: 0.00002846
Iteration 125/1000 | Loss: 0.00002846
Iteration 126/1000 | Loss: 0.00002846
Iteration 127/1000 | Loss: 0.00002846
Iteration 128/1000 | Loss: 0.00002846
Iteration 129/1000 | Loss: 0.00002846
Iteration 130/1000 | Loss: 0.00002846
Iteration 131/1000 | Loss: 0.00002845
Iteration 132/1000 | Loss: 0.00002845
Iteration 133/1000 | Loss: 0.00002845
Iteration 134/1000 | Loss: 0.00002845
Iteration 135/1000 | Loss: 0.00002845
Iteration 136/1000 | Loss: 0.00002845
Iteration 137/1000 | Loss: 0.00002844
Iteration 138/1000 | Loss: 0.00002844
Iteration 139/1000 | Loss: 0.00002844
Iteration 140/1000 | Loss: 0.00002844
Iteration 141/1000 | Loss: 0.00002844
Iteration 142/1000 | Loss: 0.00002844
Iteration 143/1000 | Loss: 0.00002844
Iteration 144/1000 | Loss: 0.00002844
Iteration 145/1000 | Loss: 0.00002844
Iteration 146/1000 | Loss: 0.00002843
Iteration 147/1000 | Loss: 0.00002843
Iteration 148/1000 | Loss: 0.00002843
Iteration 149/1000 | Loss: 0.00002843
Iteration 150/1000 | Loss: 0.00002843
Iteration 151/1000 | Loss: 0.00002843
Iteration 152/1000 | Loss: 0.00002843
Iteration 153/1000 | Loss: 0.00002843
Iteration 154/1000 | Loss: 0.00002843
Iteration 155/1000 | Loss: 0.00002843
Iteration 156/1000 | Loss: 0.00002843
Iteration 157/1000 | Loss: 0.00002843
Iteration 158/1000 | Loss: 0.00002843
Iteration 159/1000 | Loss: 0.00002843
Iteration 160/1000 | Loss: 0.00002843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.8429541998775676e-05, 2.8429541998775676e-05, 2.8429541998775676e-05, 2.8429541998775676e-05, 2.8429541998775676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8429541998775676e-05

Optimization complete. Final v2v error: 4.232104778289795 mm

Highest mean error: 5.392153739929199 mm for frame 76

Lowest mean error: 3.4206173419952393 mm for frame 201

Saving results

Total time: 49.699166774749756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487425
Iteration 2/25 | Loss: 0.00150160
Iteration 3/25 | Loss: 0.00132587
Iteration 4/25 | Loss: 0.00129713
Iteration 5/25 | Loss: 0.00129370
Iteration 6/25 | Loss: 0.00129264
Iteration 7/25 | Loss: 0.00129264
Iteration 8/25 | Loss: 0.00129264
Iteration 9/25 | Loss: 0.00129264
Iteration 10/25 | Loss: 0.00129264
Iteration 11/25 | Loss: 0.00129264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012926366180181503, 0.0012926366180181503, 0.0012926366180181503, 0.0012926366180181503, 0.0012926366180181503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012926366180181503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27554142
Iteration 2/25 | Loss: 0.00145896
Iteration 3/25 | Loss: 0.00145896
Iteration 4/25 | Loss: 0.00145895
Iteration 5/25 | Loss: 0.00145895
Iteration 6/25 | Loss: 0.00145895
Iteration 7/25 | Loss: 0.00145895
Iteration 8/25 | Loss: 0.00145895
Iteration 9/25 | Loss: 0.00145895
Iteration 10/25 | Loss: 0.00145895
Iteration 11/25 | Loss: 0.00145895
Iteration 12/25 | Loss: 0.00145895
Iteration 13/25 | Loss: 0.00145895
Iteration 14/25 | Loss: 0.00145895
Iteration 15/25 | Loss: 0.00145895
Iteration 16/25 | Loss: 0.00145895
Iteration 17/25 | Loss: 0.00145895
Iteration 18/25 | Loss: 0.00145895
Iteration 19/25 | Loss: 0.00145895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014589524362236261, 0.0014589524362236261, 0.0014589524362236261, 0.0014589524362236261, 0.0014589524362236261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014589524362236261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145895
Iteration 2/1000 | Loss: 0.00004768
Iteration 3/1000 | Loss: 0.00003233
Iteration 4/1000 | Loss: 0.00002609
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002174
Iteration 8/1000 | Loss: 0.00002108
Iteration 9/1000 | Loss: 0.00002073
Iteration 10/1000 | Loss: 0.00002048
Iteration 11/1000 | Loss: 0.00002028
Iteration 12/1000 | Loss: 0.00002022
Iteration 13/1000 | Loss: 0.00002020
Iteration 14/1000 | Loss: 0.00002018
Iteration 15/1000 | Loss: 0.00002017
Iteration 16/1000 | Loss: 0.00002010
Iteration 17/1000 | Loss: 0.00002009
Iteration 18/1000 | Loss: 0.00002004
Iteration 19/1000 | Loss: 0.00001996
Iteration 20/1000 | Loss: 0.00001996
Iteration 21/1000 | Loss: 0.00001995
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001990
Iteration 29/1000 | Loss: 0.00001989
Iteration 30/1000 | Loss: 0.00001988
Iteration 31/1000 | Loss: 0.00001988
Iteration 32/1000 | Loss: 0.00001988
Iteration 33/1000 | Loss: 0.00001987
Iteration 34/1000 | Loss: 0.00001987
Iteration 35/1000 | Loss: 0.00001986
Iteration 36/1000 | Loss: 0.00001985
Iteration 37/1000 | Loss: 0.00001985
Iteration 38/1000 | Loss: 0.00001984
Iteration 39/1000 | Loss: 0.00001984
Iteration 40/1000 | Loss: 0.00001984
Iteration 41/1000 | Loss: 0.00001984
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001981
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001980
Iteration 47/1000 | Loss: 0.00001980
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00001980
Iteration 50/1000 | Loss: 0.00001979
Iteration 51/1000 | Loss: 0.00001979
Iteration 52/1000 | Loss: 0.00001978
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001976
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001975
Iteration 57/1000 | Loss: 0.00001975
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001973
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001972
Iteration 62/1000 | Loss: 0.00001972
Iteration 63/1000 | Loss: 0.00001972
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001972
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00001970
Iteration 70/1000 | Loss: 0.00001970
Iteration 71/1000 | Loss: 0.00001970
Iteration 72/1000 | Loss: 0.00001969
Iteration 73/1000 | Loss: 0.00001969
Iteration 74/1000 | Loss: 0.00001968
Iteration 75/1000 | Loss: 0.00001967
Iteration 76/1000 | Loss: 0.00001967
Iteration 77/1000 | Loss: 0.00001967
Iteration 78/1000 | Loss: 0.00001966
Iteration 79/1000 | Loss: 0.00001966
Iteration 80/1000 | Loss: 0.00001966
Iteration 81/1000 | Loss: 0.00001965
Iteration 82/1000 | Loss: 0.00001965
Iteration 83/1000 | Loss: 0.00001965
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001964
Iteration 86/1000 | Loss: 0.00001964
Iteration 87/1000 | Loss: 0.00001964
Iteration 88/1000 | Loss: 0.00001964
Iteration 89/1000 | Loss: 0.00001964
Iteration 90/1000 | Loss: 0.00001964
Iteration 91/1000 | Loss: 0.00001964
Iteration 92/1000 | Loss: 0.00001963
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00001963
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001963
Iteration 98/1000 | Loss: 0.00001963
Iteration 99/1000 | Loss: 0.00001963
Iteration 100/1000 | Loss: 0.00001962
Iteration 101/1000 | Loss: 0.00001962
Iteration 102/1000 | Loss: 0.00001962
Iteration 103/1000 | Loss: 0.00001962
Iteration 104/1000 | Loss: 0.00001962
Iteration 105/1000 | Loss: 0.00001962
Iteration 106/1000 | Loss: 0.00001962
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001961
Iteration 110/1000 | Loss: 0.00001960
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001960
Iteration 114/1000 | Loss: 0.00001960
Iteration 115/1000 | Loss: 0.00001960
Iteration 116/1000 | Loss: 0.00001960
Iteration 117/1000 | Loss: 0.00001960
Iteration 118/1000 | Loss: 0.00001960
Iteration 119/1000 | Loss: 0.00001960
Iteration 120/1000 | Loss: 0.00001960
Iteration 121/1000 | Loss: 0.00001960
Iteration 122/1000 | Loss: 0.00001960
Iteration 123/1000 | Loss: 0.00001960
Iteration 124/1000 | Loss: 0.00001959
Iteration 125/1000 | Loss: 0.00001959
Iteration 126/1000 | Loss: 0.00001959
Iteration 127/1000 | Loss: 0.00001959
Iteration 128/1000 | Loss: 0.00001959
Iteration 129/1000 | Loss: 0.00001959
Iteration 130/1000 | Loss: 0.00001959
Iteration 131/1000 | Loss: 0.00001959
Iteration 132/1000 | Loss: 0.00001959
Iteration 133/1000 | Loss: 0.00001958
Iteration 134/1000 | Loss: 0.00001958
Iteration 135/1000 | Loss: 0.00001958
Iteration 136/1000 | Loss: 0.00001958
Iteration 137/1000 | Loss: 0.00001958
Iteration 138/1000 | Loss: 0.00001958
Iteration 139/1000 | Loss: 0.00001958
Iteration 140/1000 | Loss: 0.00001958
Iteration 141/1000 | Loss: 0.00001958
Iteration 142/1000 | Loss: 0.00001958
Iteration 143/1000 | Loss: 0.00001958
Iteration 144/1000 | Loss: 0.00001958
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001958
Iteration 147/1000 | Loss: 0.00001958
Iteration 148/1000 | Loss: 0.00001958
Iteration 149/1000 | Loss: 0.00001958
Iteration 150/1000 | Loss: 0.00001958
Iteration 151/1000 | Loss: 0.00001958
Iteration 152/1000 | Loss: 0.00001958
Iteration 153/1000 | Loss: 0.00001958
Iteration 154/1000 | Loss: 0.00001958
Iteration 155/1000 | Loss: 0.00001958
Iteration 156/1000 | Loss: 0.00001958
Iteration 157/1000 | Loss: 0.00001958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.9576320482883602e-05, 1.9576320482883602e-05, 1.9576320482883602e-05, 1.9576320482883602e-05, 1.9576320482883602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9576320482883602e-05

Optimization complete. Final v2v error: 3.7777202129364014 mm

Highest mean error: 4.387478828430176 mm for frame 70

Lowest mean error: 3.3983771800994873 mm for frame 50

Saving results

Total time: 38.844491958618164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101404
Iteration 2/25 | Loss: 0.00155775
Iteration 3/25 | Loss: 0.00132122
Iteration 4/25 | Loss: 0.00129071
Iteration 5/25 | Loss: 0.00128509
Iteration 6/25 | Loss: 0.00128384
Iteration 7/25 | Loss: 0.00128384
Iteration 8/25 | Loss: 0.00128384
Iteration 9/25 | Loss: 0.00128384
Iteration 10/25 | Loss: 0.00128384
Iteration 11/25 | Loss: 0.00128384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012838440015912056, 0.0012838440015912056, 0.0012838440015912056, 0.0012838440015912056, 0.0012838440015912056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012838440015912056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44650865
Iteration 2/25 | Loss: 0.00164708
Iteration 3/25 | Loss: 0.00164708
Iteration 4/25 | Loss: 0.00164708
Iteration 5/25 | Loss: 0.00164708
Iteration 6/25 | Loss: 0.00164708
Iteration 7/25 | Loss: 0.00164708
Iteration 8/25 | Loss: 0.00164707
Iteration 9/25 | Loss: 0.00164707
Iteration 10/25 | Loss: 0.00164707
Iteration 11/25 | Loss: 0.00164707
Iteration 12/25 | Loss: 0.00164707
Iteration 13/25 | Loss: 0.00164707
Iteration 14/25 | Loss: 0.00164707
Iteration 15/25 | Loss: 0.00164707
Iteration 16/25 | Loss: 0.00164707
Iteration 17/25 | Loss: 0.00164707
Iteration 18/25 | Loss: 0.00164707
Iteration 19/25 | Loss: 0.00164707
Iteration 20/25 | Loss: 0.00164707
Iteration 21/25 | Loss: 0.00164707
Iteration 22/25 | Loss: 0.00164707
Iteration 23/25 | Loss: 0.00164707
Iteration 24/25 | Loss: 0.00164707
Iteration 25/25 | Loss: 0.00164707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164707
Iteration 2/1000 | Loss: 0.00005342
Iteration 3/1000 | Loss: 0.00003169
Iteration 4/1000 | Loss: 0.00002542
Iteration 5/1000 | Loss: 0.00002282
Iteration 6/1000 | Loss: 0.00002176
Iteration 7/1000 | Loss: 0.00002123
Iteration 8/1000 | Loss: 0.00002076
Iteration 9/1000 | Loss: 0.00002049
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002017
Iteration 12/1000 | Loss: 0.00002001
Iteration 13/1000 | Loss: 0.00002001
Iteration 14/1000 | Loss: 0.00002000
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001998
Iteration 17/1000 | Loss: 0.00001997
Iteration 18/1000 | Loss: 0.00001996
Iteration 19/1000 | Loss: 0.00001995
Iteration 20/1000 | Loss: 0.00001991
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001988
Iteration 23/1000 | Loss: 0.00001987
Iteration 24/1000 | Loss: 0.00001987
Iteration 25/1000 | Loss: 0.00001986
Iteration 26/1000 | Loss: 0.00001985
Iteration 27/1000 | Loss: 0.00001985
Iteration 28/1000 | Loss: 0.00001985
Iteration 29/1000 | Loss: 0.00001985
Iteration 30/1000 | Loss: 0.00001985
Iteration 31/1000 | Loss: 0.00001984
Iteration 32/1000 | Loss: 0.00001984
Iteration 33/1000 | Loss: 0.00001983
Iteration 34/1000 | Loss: 0.00001983
Iteration 35/1000 | Loss: 0.00001982
Iteration 36/1000 | Loss: 0.00001981
Iteration 37/1000 | Loss: 0.00001981
Iteration 38/1000 | Loss: 0.00001981
Iteration 39/1000 | Loss: 0.00001981
Iteration 40/1000 | Loss: 0.00001981
Iteration 41/1000 | Loss: 0.00001981
Iteration 42/1000 | Loss: 0.00001981
Iteration 43/1000 | Loss: 0.00001981
Iteration 44/1000 | Loss: 0.00001981
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001980
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00001980
Iteration 50/1000 | Loss: 0.00001979
Iteration 51/1000 | Loss: 0.00001979
Iteration 52/1000 | Loss: 0.00001979
Iteration 53/1000 | Loss: 0.00001979
Iteration 54/1000 | Loss: 0.00001978
Iteration 55/1000 | Loss: 0.00001978
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001977
Iteration 58/1000 | Loss: 0.00001977
Iteration 59/1000 | Loss: 0.00001977
Iteration 60/1000 | Loss: 0.00001977
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001976
Iteration 63/1000 | Loss: 0.00001976
Iteration 64/1000 | Loss: 0.00001976
Iteration 65/1000 | Loss: 0.00001976
Iteration 66/1000 | Loss: 0.00001976
Iteration 67/1000 | Loss: 0.00001976
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001975
Iteration 73/1000 | Loss: 0.00001975
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00001975
Iteration 78/1000 | Loss: 0.00001975
Iteration 79/1000 | Loss: 0.00001975
Iteration 80/1000 | Loss: 0.00001975
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001974
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001974
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001974
Iteration 89/1000 | Loss: 0.00001974
Iteration 90/1000 | Loss: 0.00001974
Iteration 91/1000 | Loss: 0.00001974
Iteration 92/1000 | Loss: 0.00001973
Iteration 93/1000 | Loss: 0.00001973
Iteration 94/1000 | Loss: 0.00001973
Iteration 95/1000 | Loss: 0.00001973
Iteration 96/1000 | Loss: 0.00001973
Iteration 97/1000 | Loss: 0.00001973
Iteration 98/1000 | Loss: 0.00001973
Iteration 99/1000 | Loss: 0.00001973
Iteration 100/1000 | Loss: 0.00001973
Iteration 101/1000 | Loss: 0.00001973
Iteration 102/1000 | Loss: 0.00001973
Iteration 103/1000 | Loss: 0.00001972
Iteration 104/1000 | Loss: 0.00001972
Iteration 105/1000 | Loss: 0.00001972
Iteration 106/1000 | Loss: 0.00001972
Iteration 107/1000 | Loss: 0.00001972
Iteration 108/1000 | Loss: 0.00001972
Iteration 109/1000 | Loss: 0.00001972
Iteration 110/1000 | Loss: 0.00001972
Iteration 111/1000 | Loss: 0.00001972
Iteration 112/1000 | Loss: 0.00001972
Iteration 113/1000 | Loss: 0.00001972
Iteration 114/1000 | Loss: 0.00001972
Iteration 115/1000 | Loss: 0.00001972
Iteration 116/1000 | Loss: 0.00001971
Iteration 117/1000 | Loss: 0.00001971
Iteration 118/1000 | Loss: 0.00001971
Iteration 119/1000 | Loss: 0.00001971
Iteration 120/1000 | Loss: 0.00001971
Iteration 121/1000 | Loss: 0.00001971
Iteration 122/1000 | Loss: 0.00001971
Iteration 123/1000 | Loss: 0.00001971
Iteration 124/1000 | Loss: 0.00001971
Iteration 125/1000 | Loss: 0.00001971
Iteration 126/1000 | Loss: 0.00001971
Iteration 127/1000 | Loss: 0.00001971
Iteration 128/1000 | Loss: 0.00001971
Iteration 129/1000 | Loss: 0.00001971
Iteration 130/1000 | Loss: 0.00001971
Iteration 131/1000 | Loss: 0.00001971
Iteration 132/1000 | Loss: 0.00001970
Iteration 133/1000 | Loss: 0.00001970
Iteration 134/1000 | Loss: 0.00001970
Iteration 135/1000 | Loss: 0.00001970
Iteration 136/1000 | Loss: 0.00001970
Iteration 137/1000 | Loss: 0.00001970
Iteration 138/1000 | Loss: 0.00001970
Iteration 139/1000 | Loss: 0.00001970
Iteration 140/1000 | Loss: 0.00001970
Iteration 141/1000 | Loss: 0.00001970
Iteration 142/1000 | Loss: 0.00001970
Iteration 143/1000 | Loss: 0.00001970
Iteration 144/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.97014378500171e-05, 1.97014378500171e-05, 1.97014378500171e-05, 1.97014378500171e-05, 1.97014378500171e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.97014378500171e-05

Optimization complete. Final v2v error: 3.739734172821045 mm

Highest mean error: 4.010448932647705 mm for frame 63

Lowest mean error: 3.3068175315856934 mm for frame 52

Saving results

Total time: 36.04219388961792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_1087/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_1087/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819717
Iteration 2/25 | Loss: 0.00167086
Iteration 3/25 | Loss: 0.00141290
Iteration 4/25 | Loss: 0.00137766
Iteration 5/25 | Loss: 0.00136128
Iteration 6/25 | Loss: 0.00137136
Iteration 7/25 | Loss: 0.00136366
Iteration 8/25 | Loss: 0.00134914
Iteration 9/25 | Loss: 0.00134310
Iteration 10/25 | Loss: 0.00133448
Iteration 11/25 | Loss: 0.00133436
Iteration 12/25 | Loss: 0.00133709
Iteration 13/25 | Loss: 0.00133225
Iteration 14/25 | Loss: 0.00133781
Iteration 15/25 | Loss: 0.00132629
Iteration 16/25 | Loss: 0.00132243
Iteration 17/25 | Loss: 0.00132012
Iteration 18/25 | Loss: 0.00132328
Iteration 19/25 | Loss: 0.00132256
Iteration 20/25 | Loss: 0.00131630
Iteration 21/25 | Loss: 0.00130971
Iteration 22/25 | Loss: 0.00131478
Iteration 23/25 | Loss: 0.00130853
Iteration 24/25 | Loss: 0.00130472
Iteration 25/25 | Loss: 0.00130405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.79451036
Iteration 2/25 | Loss: 0.00177214
Iteration 3/25 | Loss: 0.00177198
Iteration 4/25 | Loss: 0.00177198
Iteration 5/25 | Loss: 0.00177198
Iteration 6/25 | Loss: 0.00177198
Iteration 7/25 | Loss: 0.00177198
Iteration 8/25 | Loss: 0.00177198
Iteration 9/25 | Loss: 0.00177198
Iteration 10/25 | Loss: 0.00177198
Iteration 11/25 | Loss: 0.00177198
Iteration 12/25 | Loss: 0.00177198
Iteration 13/25 | Loss: 0.00177198
Iteration 14/25 | Loss: 0.00177198
Iteration 15/25 | Loss: 0.00177198
Iteration 16/25 | Loss: 0.00177198
Iteration 17/25 | Loss: 0.00177198
Iteration 18/25 | Loss: 0.00177198
Iteration 19/25 | Loss: 0.00177198
Iteration 20/25 | Loss: 0.00177198
Iteration 21/25 | Loss: 0.00177198
Iteration 22/25 | Loss: 0.00177198
Iteration 23/25 | Loss: 0.00177198
Iteration 24/25 | Loss: 0.00177198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017719767056405544, 0.0017719767056405544, 0.0017719767056405544, 0.0017719767056405544, 0.0017719767056405544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017719767056405544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177198
Iteration 2/1000 | Loss: 0.00014509
Iteration 3/1000 | Loss: 0.01532019
Iteration 4/1000 | Loss: 0.00986096
Iteration 5/1000 | Loss: 0.00872313
Iteration 6/1000 | Loss: 0.00227563
Iteration 7/1000 | Loss: 0.00512952
Iteration 8/1000 | Loss: 0.00012838
Iteration 9/1000 | Loss: 0.00007021
Iteration 10/1000 | Loss: 0.00005130
Iteration 11/1000 | Loss: 0.00004322
Iteration 12/1000 | Loss: 0.00003920
Iteration 13/1000 | Loss: 0.00003600
Iteration 14/1000 | Loss: 0.00003457
Iteration 15/1000 | Loss: 0.00003334
Iteration 16/1000 | Loss: 0.00003225
Iteration 17/1000 | Loss: 0.00003196
Iteration 18/1000 | Loss: 0.00003129
Iteration 19/1000 | Loss: 0.00003087
Iteration 20/1000 | Loss: 0.00003053
Iteration 21/1000 | Loss: 0.00003025
Iteration 22/1000 | Loss: 0.00003020
Iteration 23/1000 | Loss: 0.00002998
Iteration 24/1000 | Loss: 0.00119022
Iteration 25/1000 | Loss: 0.00036106
Iteration 26/1000 | Loss: 0.00003091
Iteration 27/1000 | Loss: 0.00002974
Iteration 28/1000 | Loss: 0.00002968
Iteration 29/1000 | Loss: 0.00002968
Iteration 30/1000 | Loss: 0.00002968
Iteration 31/1000 | Loss: 0.00002967
Iteration 32/1000 | Loss: 0.00002967
Iteration 33/1000 | Loss: 0.00002967
Iteration 34/1000 | Loss: 0.00002966
Iteration 35/1000 | Loss: 0.00002966
Iteration 36/1000 | Loss: 0.00002959
Iteration 37/1000 | Loss: 0.00002955
Iteration 38/1000 | Loss: 0.00002953
Iteration 39/1000 | Loss: 0.00002952
Iteration 40/1000 | Loss: 0.00002951
Iteration 41/1000 | Loss: 0.00002951
Iteration 42/1000 | Loss: 0.00002950
Iteration 43/1000 | Loss: 0.00002950
Iteration 44/1000 | Loss: 0.00002950
Iteration 45/1000 | Loss: 0.00002950
Iteration 46/1000 | Loss: 0.00002950
Iteration 47/1000 | Loss: 0.00002950
Iteration 48/1000 | Loss: 0.00002950
Iteration 49/1000 | Loss: 0.00002949
Iteration 50/1000 | Loss: 0.00002948
Iteration 51/1000 | Loss: 0.00002943
Iteration 52/1000 | Loss: 0.00002943
Iteration 53/1000 | Loss: 0.00002941
Iteration 54/1000 | Loss: 0.00002941
Iteration 55/1000 | Loss: 0.00002940
Iteration 56/1000 | Loss: 0.00002940
Iteration 57/1000 | Loss: 0.00002939
Iteration 58/1000 | Loss: 0.00002939
Iteration 59/1000 | Loss: 0.00002939
Iteration 60/1000 | Loss: 0.00002938
Iteration 61/1000 | Loss: 0.00002938
Iteration 62/1000 | Loss: 0.00002938
Iteration 63/1000 | Loss: 0.00002938
Iteration 64/1000 | Loss: 0.00002938
Iteration 65/1000 | Loss: 0.00002938
Iteration 66/1000 | Loss: 0.00002937
Iteration 67/1000 | Loss: 0.00002937
Iteration 68/1000 | Loss: 0.00002937
Iteration 69/1000 | Loss: 0.00002937
Iteration 70/1000 | Loss: 0.00002937
Iteration 71/1000 | Loss: 0.00002937
Iteration 72/1000 | Loss: 0.00002937
Iteration 73/1000 | Loss: 0.00002937
Iteration 74/1000 | Loss: 0.00002937
Iteration 75/1000 | Loss: 0.00002937
Iteration 76/1000 | Loss: 0.00002937
Iteration 77/1000 | Loss: 0.00002937
Iteration 78/1000 | Loss: 0.00002937
Iteration 79/1000 | Loss: 0.00002937
Iteration 80/1000 | Loss: 0.00002937
Iteration 81/1000 | Loss: 0.00002937
Iteration 82/1000 | Loss: 0.00002937
Iteration 83/1000 | Loss: 0.00002937
Iteration 84/1000 | Loss: 0.00002937
Iteration 85/1000 | Loss: 0.00002937
Iteration 86/1000 | Loss: 0.00002937
Iteration 87/1000 | Loss: 0.00002937
Iteration 88/1000 | Loss: 0.00002937
Iteration 89/1000 | Loss: 0.00002937
Iteration 90/1000 | Loss: 0.00002937
Iteration 91/1000 | Loss: 0.00002937
Iteration 92/1000 | Loss: 0.00002937
Iteration 93/1000 | Loss: 0.00002937
Iteration 94/1000 | Loss: 0.00002937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.9370536140049808e-05, 2.9370536140049808e-05, 2.9370536140049808e-05, 2.9370536140049808e-05, 2.9370536140049808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9370536140049808e-05

Optimization complete. Final v2v error: 4.125151634216309 mm

Highest mean error: 13.614374160766602 mm for frame 50

Lowest mean error: 3.0247035026550293 mm for frame 8

Saving results

Total time: 90.81062984466553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053576
Iteration 2/25 | Loss: 0.01053576
Iteration 3/25 | Loss: 0.01053576
Iteration 4/25 | Loss: 0.01053576
Iteration 5/25 | Loss: 0.01053576
Iteration 6/25 | Loss: 0.01053576
Iteration 7/25 | Loss: 0.01053576
Iteration 8/25 | Loss: 0.01053576
Iteration 9/25 | Loss: 0.01053575
Iteration 10/25 | Loss: 0.01053575
Iteration 11/25 | Loss: 0.01053575
Iteration 12/25 | Loss: 0.01053575
Iteration 13/25 | Loss: 0.01053575
Iteration 14/25 | Loss: 0.01053575
Iteration 15/25 | Loss: 0.01053575
Iteration 16/25 | Loss: 0.01053575
Iteration 17/25 | Loss: 0.01053574
Iteration 18/25 | Loss: 0.01053574
Iteration 19/25 | Loss: 0.01053574
Iteration 20/25 | Loss: 0.01053574
Iteration 21/25 | Loss: 0.01053574
Iteration 22/25 | Loss: 0.01053574
Iteration 23/25 | Loss: 0.01053574
Iteration 24/25 | Loss: 0.01053574
Iteration 25/25 | Loss: 0.01053574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93325138
Iteration 2/25 | Loss: 0.07349121
Iteration 3/25 | Loss: 0.07349016
Iteration 4/25 | Loss: 0.07349016
Iteration 5/25 | Loss: 0.07349014
Iteration 6/25 | Loss: 0.07349014
Iteration 7/25 | Loss: 0.07349014
Iteration 8/25 | Loss: 0.07349014
Iteration 9/25 | Loss: 0.07349014
Iteration 10/25 | Loss: 0.07349014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.07349013537168503, 0.07349013537168503, 0.07349013537168503, 0.07349013537168503, 0.07349013537168503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07349013537168503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07349013
Iteration 2/1000 | Loss: 0.00171266
Iteration 3/1000 | Loss: 0.00144279
Iteration 4/1000 | Loss: 0.00080082
Iteration 5/1000 | Loss: 0.00055690
Iteration 6/1000 | Loss: 0.00032584
Iteration 7/1000 | Loss: 0.00018139
Iteration 8/1000 | Loss: 0.00013311
Iteration 9/1000 | Loss: 0.00006247
Iteration 10/1000 | Loss: 0.00005407
Iteration 11/1000 | Loss: 0.00007451
Iteration 12/1000 | Loss: 0.00004640
Iteration 13/1000 | Loss: 0.00004410
Iteration 14/1000 | Loss: 0.00005879
Iteration 15/1000 | Loss: 0.00007874
Iteration 16/1000 | Loss: 0.00053875
Iteration 17/1000 | Loss: 0.00006212
Iteration 18/1000 | Loss: 0.00003153
Iteration 19/1000 | Loss: 0.00004510
Iteration 20/1000 | Loss: 0.00019222
Iteration 21/1000 | Loss: 0.00007429
Iteration 22/1000 | Loss: 0.00002692
Iteration 23/1000 | Loss: 0.00002763
Iteration 24/1000 | Loss: 0.00017546
Iteration 25/1000 | Loss: 0.00002509
Iteration 26/1000 | Loss: 0.00004682
Iteration 27/1000 | Loss: 0.00012777
Iteration 28/1000 | Loss: 0.00004312
Iteration 29/1000 | Loss: 0.00002948
Iteration 30/1000 | Loss: 0.00005112
Iteration 31/1000 | Loss: 0.00005383
Iteration 32/1000 | Loss: 0.00015842
Iteration 33/1000 | Loss: 0.00002716
Iteration 34/1000 | Loss: 0.00015789
Iteration 35/1000 | Loss: 0.00002024
Iteration 36/1000 | Loss: 0.00002555
Iteration 37/1000 | Loss: 0.00002529
Iteration 38/1000 | Loss: 0.00003837
Iteration 39/1000 | Loss: 0.00002658
Iteration 40/1000 | Loss: 0.00009225
Iteration 41/1000 | Loss: 0.00006126
Iteration 42/1000 | Loss: 0.00002815
Iteration 43/1000 | Loss: 0.00007765
Iteration 44/1000 | Loss: 0.00001854
Iteration 45/1000 | Loss: 0.00003790
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00003996
Iteration 48/1000 | Loss: 0.00003668
Iteration 49/1000 | Loss: 0.00001660
Iteration 50/1000 | Loss: 0.00002118
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001653
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00002668
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001587
Iteration 57/1000 | Loss: 0.00001587
Iteration 58/1000 | Loss: 0.00001587
Iteration 59/1000 | Loss: 0.00001587
Iteration 60/1000 | Loss: 0.00001586
Iteration 61/1000 | Loss: 0.00001586
Iteration 62/1000 | Loss: 0.00001586
Iteration 63/1000 | Loss: 0.00001586
Iteration 64/1000 | Loss: 0.00001586
Iteration 65/1000 | Loss: 0.00001585
Iteration 66/1000 | Loss: 0.00001630
Iteration 67/1000 | Loss: 0.00001577
Iteration 68/1000 | Loss: 0.00001577
Iteration 69/1000 | Loss: 0.00001577
Iteration 70/1000 | Loss: 0.00001577
Iteration 71/1000 | Loss: 0.00001577
Iteration 72/1000 | Loss: 0.00001577
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001577
Iteration 75/1000 | Loss: 0.00001577
Iteration 76/1000 | Loss: 0.00001578
Iteration 77/1000 | Loss: 0.00001569
Iteration 78/1000 | Loss: 0.00003497
Iteration 79/1000 | Loss: 0.00011996
Iteration 80/1000 | Loss: 0.00006366
Iteration 81/1000 | Loss: 0.00004976
Iteration 82/1000 | Loss: 0.00002055
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001541
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001540
Iteration 89/1000 | Loss: 0.00001540
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001539
Iteration 92/1000 | Loss: 0.00001539
Iteration 93/1000 | Loss: 0.00001539
Iteration 94/1000 | Loss: 0.00002691
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00002519
Iteration 97/1000 | Loss: 0.00002710
Iteration 98/1000 | Loss: 0.00002098
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00002517
Iteration 101/1000 | Loss: 0.00002027
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00002105
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001421
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001416
Iteration 108/1000 | Loss: 0.00001416
Iteration 109/1000 | Loss: 0.00001414
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00003253
Iteration 112/1000 | Loss: 0.00004005
Iteration 113/1000 | Loss: 0.00001990
Iteration 114/1000 | Loss: 0.00001814
Iteration 115/1000 | Loss: 0.00001405
Iteration 116/1000 | Loss: 0.00001404
Iteration 117/1000 | Loss: 0.00001404
Iteration 118/1000 | Loss: 0.00001404
Iteration 119/1000 | Loss: 0.00001404
Iteration 120/1000 | Loss: 0.00001404
Iteration 121/1000 | Loss: 0.00001403
Iteration 122/1000 | Loss: 0.00001403
Iteration 123/1000 | Loss: 0.00001403
Iteration 124/1000 | Loss: 0.00001403
Iteration 125/1000 | Loss: 0.00001403
Iteration 126/1000 | Loss: 0.00001403
Iteration 127/1000 | Loss: 0.00001403
Iteration 128/1000 | Loss: 0.00001403
Iteration 129/1000 | Loss: 0.00001403
Iteration 130/1000 | Loss: 0.00001403
Iteration 131/1000 | Loss: 0.00001403
Iteration 132/1000 | Loss: 0.00001403
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001402
Iteration 141/1000 | Loss: 0.00001402
Iteration 142/1000 | Loss: 0.00001402
Iteration 143/1000 | Loss: 0.00001402
Iteration 144/1000 | Loss: 0.00001402
Iteration 145/1000 | Loss: 0.00001402
Iteration 146/1000 | Loss: 0.00001402
Iteration 147/1000 | Loss: 0.00001402
Iteration 148/1000 | Loss: 0.00001402
Iteration 149/1000 | Loss: 0.00001402
Iteration 150/1000 | Loss: 0.00001402
Iteration 151/1000 | Loss: 0.00001402
Iteration 152/1000 | Loss: 0.00001402
Iteration 153/1000 | Loss: 0.00001402
Iteration 154/1000 | Loss: 0.00001402
Iteration 155/1000 | Loss: 0.00001402
Iteration 156/1000 | Loss: 0.00001402
Iteration 157/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.4024625670572277e-05, 1.4024625670572277e-05, 1.4024625670572277e-05, 1.4024625670572277e-05, 1.4024625670572277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4024625670572277e-05

Optimization complete. Final v2v error: 3.1040360927581787 mm

Highest mean error: 12.772988319396973 mm for frame 223

Lowest mean error: 2.386866331100464 mm for frame 69

Saving results

Total time: 134.04676151275635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002027
Iteration 2/25 | Loss: 0.00245146
Iteration 3/25 | Loss: 0.00158643
Iteration 4/25 | Loss: 0.00144387
Iteration 5/25 | Loss: 0.00140866
Iteration 6/25 | Loss: 0.00144738
Iteration 7/25 | Loss: 0.00121486
Iteration 8/25 | Loss: 0.00108678
Iteration 9/25 | Loss: 0.00104433
Iteration 10/25 | Loss: 0.00102446
Iteration 11/25 | Loss: 0.00101686
Iteration 12/25 | Loss: 0.00101629
Iteration 13/25 | Loss: 0.00101722
Iteration 14/25 | Loss: 0.00101356
Iteration 15/25 | Loss: 0.00101032
Iteration 16/25 | Loss: 0.00100608
Iteration 17/25 | Loss: 0.00100946
Iteration 18/25 | Loss: 0.00100753
Iteration 19/25 | Loss: 0.00100577
Iteration 20/25 | Loss: 0.00100568
Iteration 21/25 | Loss: 0.00100568
Iteration 22/25 | Loss: 0.00100568
Iteration 23/25 | Loss: 0.00100568
Iteration 24/25 | Loss: 0.00100568
Iteration 25/25 | Loss: 0.00100568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32998025
Iteration 2/25 | Loss: 0.00089340
Iteration 3/25 | Loss: 0.00089340
Iteration 4/25 | Loss: 0.00089340
Iteration 5/25 | Loss: 0.00089340
Iteration 6/25 | Loss: 0.00089340
Iteration 7/25 | Loss: 0.00089340
Iteration 8/25 | Loss: 0.00089340
Iteration 9/25 | Loss: 0.00089340
Iteration 10/25 | Loss: 0.00089340
Iteration 11/25 | Loss: 0.00089340
Iteration 12/25 | Loss: 0.00089340
Iteration 13/25 | Loss: 0.00089340
Iteration 14/25 | Loss: 0.00089340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008933991775847971, 0.0008933991775847971, 0.0008933991775847971, 0.0008933991775847971, 0.0008933991775847971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008933991775847971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089340
Iteration 2/1000 | Loss: 0.00006151
Iteration 3/1000 | Loss: 0.00027443
Iteration 4/1000 | Loss: 0.00024030
Iteration 5/1000 | Loss: 0.00039819
Iteration 6/1000 | Loss: 0.00005795
Iteration 7/1000 | Loss: 0.00008662
Iteration 8/1000 | Loss: 0.00008807
Iteration 9/1000 | Loss: 0.00029747
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00003696
Iteration 12/1000 | Loss: 0.00054452
Iteration 13/1000 | Loss: 0.00251860
Iteration 14/1000 | Loss: 0.00003015
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00003593
Iteration 18/1000 | Loss: 0.00013245
Iteration 19/1000 | Loss: 0.00002286
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00012737
Iteration 22/1000 | Loss: 0.00002782
Iteration 23/1000 | Loss: 0.00007189
Iteration 24/1000 | Loss: 0.00002799
Iteration 25/1000 | Loss: 0.00002096
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00004097
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00003877
Iteration 30/1000 | Loss: 0.00001670
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001190
Iteration 46/1000 | Loss: 0.00001535
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001187
Iteration 59/1000 | Loss: 0.00001187
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001187
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.1865531632793136e-05, 1.1865531632793136e-05, 1.1865531632793136e-05, 1.1865531632793136e-05, 1.1865531632793136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1865531632793136e-05

Optimization complete. Final v2v error: 2.9237349033355713 mm

Highest mean error: 3.4798851013183594 mm for frame 39

Lowest mean error: 2.509779453277588 mm for frame 1

Saving results

Total time: 81.11693811416626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497283
Iteration 2/25 | Loss: 0.00128782
Iteration 3/25 | Loss: 0.00107124
Iteration 4/25 | Loss: 0.00104718
Iteration 5/25 | Loss: 0.00104461
Iteration 6/25 | Loss: 0.00104369
Iteration 7/25 | Loss: 0.00104369
Iteration 8/25 | Loss: 0.00104369
Iteration 9/25 | Loss: 0.00104369
Iteration 10/25 | Loss: 0.00104369
Iteration 11/25 | Loss: 0.00104369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010436910670250654, 0.0010436910670250654, 0.0010436910670250654, 0.0010436910670250654, 0.0010436910670250654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010436910670250654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35420144
Iteration 2/25 | Loss: 0.00087067
Iteration 3/25 | Loss: 0.00087067
Iteration 4/25 | Loss: 0.00087067
Iteration 5/25 | Loss: 0.00087067
Iteration 6/25 | Loss: 0.00087066
Iteration 7/25 | Loss: 0.00087066
Iteration 8/25 | Loss: 0.00087066
Iteration 9/25 | Loss: 0.00087066
Iteration 10/25 | Loss: 0.00087066
Iteration 11/25 | Loss: 0.00087066
Iteration 12/25 | Loss: 0.00087066
Iteration 13/25 | Loss: 0.00087066
Iteration 14/25 | Loss: 0.00087066
Iteration 15/25 | Loss: 0.00087066
Iteration 16/25 | Loss: 0.00087066
Iteration 17/25 | Loss: 0.00087066
Iteration 18/25 | Loss: 0.00087066
Iteration 19/25 | Loss: 0.00087066
Iteration 20/25 | Loss: 0.00087066
Iteration 21/25 | Loss: 0.00087066
Iteration 22/25 | Loss: 0.00087066
Iteration 23/25 | Loss: 0.00087066
Iteration 24/25 | Loss: 0.00087066
Iteration 25/25 | Loss: 0.00087066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087066
Iteration 2/1000 | Loss: 0.00004083
Iteration 3/1000 | Loss: 0.00002673
Iteration 4/1000 | Loss: 0.00002297
Iteration 5/1000 | Loss: 0.00002044
Iteration 6/1000 | Loss: 0.00001959
Iteration 7/1000 | Loss: 0.00001898
Iteration 8/1000 | Loss: 0.00001845
Iteration 9/1000 | Loss: 0.00001792
Iteration 10/1000 | Loss: 0.00001764
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001746
Iteration 13/1000 | Loss: 0.00001746
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001740
Iteration 16/1000 | Loss: 0.00001740
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001739
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001728
Iteration 23/1000 | Loss: 0.00001728
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001720
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001712
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001707
Iteration 32/1000 | Loss: 0.00001704
Iteration 33/1000 | Loss: 0.00001704
Iteration 34/1000 | Loss: 0.00001703
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001701
Iteration 38/1000 | Loss: 0.00001700
Iteration 39/1000 | Loss: 0.00001700
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00001697
Iteration 43/1000 | Loss: 0.00001695
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001694
Iteration 46/1000 | Loss: 0.00001694
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001689
Iteration 52/1000 | Loss: 0.00001687
Iteration 53/1000 | Loss: 0.00001686
Iteration 54/1000 | Loss: 0.00001685
Iteration 55/1000 | Loss: 0.00001685
Iteration 56/1000 | Loss: 0.00001684
Iteration 57/1000 | Loss: 0.00001684
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001683
Iteration 61/1000 | Loss: 0.00001682
Iteration 62/1000 | Loss: 0.00001682
Iteration 63/1000 | Loss: 0.00001681
Iteration 64/1000 | Loss: 0.00001680
Iteration 65/1000 | Loss: 0.00001680
Iteration 66/1000 | Loss: 0.00001679
Iteration 67/1000 | Loss: 0.00001679
Iteration 68/1000 | Loss: 0.00001679
Iteration 69/1000 | Loss: 0.00001679
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001673
Iteration 75/1000 | Loss: 0.00001673
Iteration 76/1000 | Loss: 0.00001671
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001671
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001670
Iteration 85/1000 | Loss: 0.00001670
Iteration 86/1000 | Loss: 0.00001670
Iteration 87/1000 | Loss: 0.00001669
Iteration 88/1000 | Loss: 0.00001668
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001666
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001666
Iteration 94/1000 | Loss: 0.00001666
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001664
Iteration 100/1000 | Loss: 0.00001664
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001664
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001663
Iteration 108/1000 | Loss: 0.00001663
Iteration 109/1000 | Loss: 0.00001663
Iteration 110/1000 | Loss: 0.00001663
Iteration 111/1000 | Loss: 0.00001663
Iteration 112/1000 | Loss: 0.00001663
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001663
Iteration 115/1000 | Loss: 0.00001663
Iteration 116/1000 | Loss: 0.00001663
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001663
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001663
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001663
Iteration 128/1000 | Loss: 0.00001663
Iteration 129/1000 | Loss: 0.00001663
Iteration 130/1000 | Loss: 0.00001663
Iteration 131/1000 | Loss: 0.00001663
Iteration 132/1000 | Loss: 0.00001663
Iteration 133/1000 | Loss: 0.00001663
Iteration 134/1000 | Loss: 0.00001663
Iteration 135/1000 | Loss: 0.00001663
Iteration 136/1000 | Loss: 0.00001663
Iteration 137/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.6632044207653962e-05, 1.6632044207653962e-05, 1.6632044207653962e-05, 1.6632044207653962e-05, 1.6632044207653962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6632044207653962e-05

Optimization complete. Final v2v error: 3.231229782104492 mm

Highest mean error: 4.773859024047852 mm for frame 69

Lowest mean error: 2.5128235816955566 mm for frame 149

Saving results

Total time: 38.27979779243469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00340795
Iteration 2/25 | Loss: 0.00107464
Iteration 3/25 | Loss: 0.00101234
Iteration 4/25 | Loss: 0.00100225
Iteration 5/25 | Loss: 0.00099819
Iteration 6/25 | Loss: 0.00099670
Iteration 7/25 | Loss: 0.00099670
Iteration 8/25 | Loss: 0.00099670
Iteration 9/25 | Loss: 0.00099670
Iteration 10/25 | Loss: 0.00099670
Iteration 11/25 | Loss: 0.00099670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000996700138784945, 0.000996700138784945, 0.000996700138784945, 0.000996700138784945, 0.000996700138784945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000996700138784945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34540367
Iteration 2/25 | Loss: 0.00127809
Iteration 3/25 | Loss: 0.00127809
Iteration 4/25 | Loss: 0.00127809
Iteration 5/25 | Loss: 0.00127809
Iteration 6/25 | Loss: 0.00127809
Iteration 7/25 | Loss: 0.00127809
Iteration 8/25 | Loss: 0.00127809
Iteration 9/25 | Loss: 0.00127809
Iteration 10/25 | Loss: 0.00127809
Iteration 11/25 | Loss: 0.00127809
Iteration 12/25 | Loss: 0.00127809
Iteration 13/25 | Loss: 0.00127809
Iteration 14/25 | Loss: 0.00127809
Iteration 15/25 | Loss: 0.00127809
Iteration 16/25 | Loss: 0.00127809
Iteration 17/25 | Loss: 0.00127809
Iteration 18/25 | Loss: 0.00127809
Iteration 19/25 | Loss: 0.00127809
Iteration 20/25 | Loss: 0.00127809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001278086332604289, 0.001278086332604289, 0.001278086332604289, 0.001278086332604289, 0.001278086332604289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001278086332604289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127809
Iteration 2/1000 | Loss: 0.00002855
Iteration 3/1000 | Loss: 0.00001844
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001454
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001311
Iteration 9/1000 | Loss: 0.00001295
Iteration 10/1000 | Loss: 0.00001294
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001274
Iteration 23/1000 | Loss: 0.00001271
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001258
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001254
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001252
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001251
Iteration 48/1000 | Loss: 0.00001251
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001250
Iteration 55/1000 | Loss: 0.00001250
Iteration 56/1000 | Loss: 0.00001250
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001249
Iteration 59/1000 | Loss: 0.00001249
Iteration 60/1000 | Loss: 0.00001249
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001239
Iteration 72/1000 | Loss: 0.00001239
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001228
Iteration 129/1000 | Loss: 0.00001228
Iteration 130/1000 | Loss: 0.00001228
Iteration 131/1000 | Loss: 0.00001228
Iteration 132/1000 | Loss: 0.00001228
Iteration 133/1000 | Loss: 0.00001228
Iteration 134/1000 | Loss: 0.00001228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2281926501600537e-05, 1.2281926501600537e-05, 1.2281926501600537e-05, 1.2281926501600537e-05, 1.2281926501600537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2281926501600537e-05

Optimization complete. Final v2v error: 3.04251766204834 mm

Highest mean error: 3.5748679637908936 mm for frame 57

Lowest mean error: 2.544848680496216 mm for frame 136

Saving results

Total time: 33.664774656295776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487039
Iteration 2/25 | Loss: 0.00113560
Iteration 3/25 | Loss: 0.00105120
Iteration 4/25 | Loss: 0.00103787
Iteration 5/25 | Loss: 0.00103328
Iteration 6/25 | Loss: 0.00103242
Iteration 7/25 | Loss: 0.00103242
Iteration 8/25 | Loss: 0.00103242
Iteration 9/25 | Loss: 0.00103242
Iteration 10/25 | Loss: 0.00103242
Iteration 11/25 | Loss: 0.00103242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001032416825182736, 0.001032416825182736, 0.001032416825182736, 0.001032416825182736, 0.001032416825182736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001032416825182736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52053559
Iteration 2/25 | Loss: 0.00111430
Iteration 3/25 | Loss: 0.00111429
Iteration 4/25 | Loss: 0.00111428
Iteration 5/25 | Loss: 0.00111428
Iteration 6/25 | Loss: 0.00111428
Iteration 7/25 | Loss: 0.00111428
Iteration 8/25 | Loss: 0.00111428
Iteration 9/25 | Loss: 0.00111428
Iteration 10/25 | Loss: 0.00111428
Iteration 11/25 | Loss: 0.00111428
Iteration 12/25 | Loss: 0.00111428
Iteration 13/25 | Loss: 0.00111428
Iteration 14/25 | Loss: 0.00111428
Iteration 15/25 | Loss: 0.00111428
Iteration 16/25 | Loss: 0.00111428
Iteration 17/25 | Loss: 0.00111428
Iteration 18/25 | Loss: 0.00111428
Iteration 19/25 | Loss: 0.00111428
Iteration 20/25 | Loss: 0.00111428
Iteration 21/25 | Loss: 0.00111428
Iteration 22/25 | Loss: 0.00111428
Iteration 23/25 | Loss: 0.00111428
Iteration 24/25 | Loss: 0.00111428
Iteration 25/25 | Loss: 0.00111428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111428
Iteration 2/1000 | Loss: 0.00003703
Iteration 3/1000 | Loss: 0.00002185
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001458
Iteration 7/1000 | Loss: 0.00001398
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001336
Iteration 10/1000 | Loss: 0.00001319
Iteration 11/1000 | Loss: 0.00001316
Iteration 12/1000 | Loss: 0.00001311
Iteration 13/1000 | Loss: 0.00001299
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001293
Iteration 19/1000 | Loss: 0.00001291
Iteration 20/1000 | Loss: 0.00001290
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001287
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001287
Iteration 28/1000 | Loss: 0.00001286
Iteration 29/1000 | Loss: 0.00001286
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001286
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001285
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001284
Iteration 39/1000 | Loss: 0.00001283
Iteration 40/1000 | Loss: 0.00001282
Iteration 41/1000 | Loss: 0.00001282
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001281
Iteration 46/1000 | Loss: 0.00001281
Iteration 47/1000 | Loss: 0.00001281
Iteration 48/1000 | Loss: 0.00001281
Iteration 49/1000 | Loss: 0.00001281
Iteration 50/1000 | Loss: 0.00001280
Iteration 51/1000 | Loss: 0.00001279
Iteration 52/1000 | Loss: 0.00001279
Iteration 53/1000 | Loss: 0.00001278
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001278
Iteration 56/1000 | Loss: 0.00001278
Iteration 57/1000 | Loss: 0.00001278
Iteration 58/1000 | Loss: 0.00001278
Iteration 59/1000 | Loss: 0.00001278
Iteration 60/1000 | Loss: 0.00001278
Iteration 61/1000 | Loss: 0.00001278
Iteration 62/1000 | Loss: 0.00001278
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001278
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001277
Iteration 67/1000 | Loss: 0.00001277
Iteration 68/1000 | Loss: 0.00001277
Iteration 69/1000 | Loss: 0.00001277
Iteration 70/1000 | Loss: 0.00001277
Iteration 71/1000 | Loss: 0.00001277
Iteration 72/1000 | Loss: 0.00001276
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001275
Iteration 75/1000 | Loss: 0.00001275
Iteration 76/1000 | Loss: 0.00001275
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001272
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001272
Iteration 87/1000 | Loss: 0.00001271
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001271
Iteration 95/1000 | Loss: 0.00001270
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001270
Iteration 98/1000 | Loss: 0.00001270
Iteration 99/1000 | Loss: 0.00001270
Iteration 100/1000 | Loss: 0.00001270
Iteration 101/1000 | Loss: 0.00001269
Iteration 102/1000 | Loss: 0.00001269
Iteration 103/1000 | Loss: 0.00001269
Iteration 104/1000 | Loss: 0.00001268
Iteration 105/1000 | Loss: 0.00001268
Iteration 106/1000 | Loss: 0.00001268
Iteration 107/1000 | Loss: 0.00001268
Iteration 108/1000 | Loss: 0.00001267
Iteration 109/1000 | Loss: 0.00001267
Iteration 110/1000 | Loss: 0.00001267
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001266
Iteration 113/1000 | Loss: 0.00001266
Iteration 114/1000 | Loss: 0.00001265
Iteration 115/1000 | Loss: 0.00001265
Iteration 116/1000 | Loss: 0.00001265
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001264
Iteration 120/1000 | Loss: 0.00001264
Iteration 121/1000 | Loss: 0.00001264
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001262
Iteration 126/1000 | Loss: 0.00001262
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001261
Iteration 138/1000 | Loss: 0.00001261
Iteration 139/1000 | Loss: 0.00001261
Iteration 140/1000 | Loss: 0.00001261
Iteration 141/1000 | Loss: 0.00001260
Iteration 142/1000 | Loss: 0.00001260
Iteration 143/1000 | Loss: 0.00001260
Iteration 144/1000 | Loss: 0.00001260
Iteration 145/1000 | Loss: 0.00001260
Iteration 146/1000 | Loss: 0.00001260
Iteration 147/1000 | Loss: 0.00001260
Iteration 148/1000 | Loss: 0.00001260
Iteration 149/1000 | Loss: 0.00001260
Iteration 150/1000 | Loss: 0.00001260
Iteration 151/1000 | Loss: 0.00001260
Iteration 152/1000 | Loss: 0.00001260
Iteration 153/1000 | Loss: 0.00001260
Iteration 154/1000 | Loss: 0.00001260
Iteration 155/1000 | Loss: 0.00001260
Iteration 156/1000 | Loss: 0.00001260
Iteration 157/1000 | Loss: 0.00001260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.259937107533915e-05, 1.259937107533915e-05, 1.259937107533915e-05, 1.259937107533915e-05, 1.259937107533915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.259937107533915e-05

Optimization complete. Final v2v error: 3.058825969696045 mm

Highest mean error: 3.735145330429077 mm for frame 227

Lowest mean error: 2.65956974029541 mm for frame 184

Saving results

Total time: 41.34440088272095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822960
Iteration 2/25 | Loss: 0.00122722
Iteration 3/25 | Loss: 0.00104085
Iteration 4/25 | Loss: 0.00101915
Iteration 5/25 | Loss: 0.00101393
Iteration 6/25 | Loss: 0.00101257
Iteration 7/25 | Loss: 0.00101257
Iteration 8/25 | Loss: 0.00101257
Iteration 9/25 | Loss: 0.00101257
Iteration 10/25 | Loss: 0.00101257
Iteration 11/25 | Loss: 0.00101257
Iteration 12/25 | Loss: 0.00101257
Iteration 13/25 | Loss: 0.00101257
Iteration 14/25 | Loss: 0.00101257
Iteration 15/25 | Loss: 0.00101257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010125706903636456, 0.0010125706903636456, 0.0010125706903636456, 0.0010125706903636456, 0.0010125706903636456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010125706903636456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35541940
Iteration 2/25 | Loss: 0.00127760
Iteration 3/25 | Loss: 0.00127760
Iteration 4/25 | Loss: 0.00127760
Iteration 5/25 | Loss: 0.00127759
Iteration 6/25 | Loss: 0.00127759
Iteration 7/25 | Loss: 0.00127759
Iteration 8/25 | Loss: 0.00127759
Iteration 9/25 | Loss: 0.00127759
Iteration 10/25 | Loss: 0.00127759
Iteration 11/25 | Loss: 0.00127759
Iteration 12/25 | Loss: 0.00127759
Iteration 13/25 | Loss: 0.00127759
Iteration 14/25 | Loss: 0.00127759
Iteration 15/25 | Loss: 0.00127759
Iteration 16/25 | Loss: 0.00127759
Iteration 17/25 | Loss: 0.00127759
Iteration 18/25 | Loss: 0.00127759
Iteration 19/25 | Loss: 0.00127759
Iteration 20/25 | Loss: 0.00127759
Iteration 21/25 | Loss: 0.00127759
Iteration 22/25 | Loss: 0.00127759
Iteration 23/25 | Loss: 0.00127759
Iteration 24/25 | Loss: 0.00127759
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012775929644703865, 0.0012775929644703865, 0.0012775929644703865, 0.0012775929644703865, 0.0012775929644703865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012775929644703865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127759
Iteration 2/1000 | Loss: 0.00004341
Iteration 3/1000 | Loss: 0.00002246
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001413
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001308
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001257
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001215
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001203
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001190
Iteration 26/1000 | Loss: 0.00001189
Iteration 27/1000 | Loss: 0.00001189
Iteration 28/1000 | Loss: 0.00001188
Iteration 29/1000 | Loss: 0.00001187
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001186
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001183
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001179
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001174
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001163
Iteration 75/1000 | Loss: 0.00001163
Iteration 76/1000 | Loss: 0.00001163
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001161
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001155
Iteration 95/1000 | Loss: 0.00001155
Iteration 96/1000 | Loss: 0.00001155
Iteration 97/1000 | Loss: 0.00001154
Iteration 98/1000 | Loss: 0.00001154
Iteration 99/1000 | Loss: 0.00001154
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001152
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001150
Iteration 106/1000 | Loss: 0.00001150
Iteration 107/1000 | Loss: 0.00001150
Iteration 108/1000 | Loss: 0.00001148
Iteration 109/1000 | Loss: 0.00001148
Iteration 110/1000 | Loss: 0.00001148
Iteration 111/1000 | Loss: 0.00001147
Iteration 112/1000 | Loss: 0.00001147
Iteration 113/1000 | Loss: 0.00001147
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001146
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001144
Iteration 125/1000 | Loss: 0.00001144
Iteration 126/1000 | Loss: 0.00001144
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001142
Iteration 133/1000 | Loss: 0.00001141
Iteration 134/1000 | Loss: 0.00001141
Iteration 135/1000 | Loss: 0.00001141
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001138
Iteration 140/1000 | Loss: 0.00001138
Iteration 141/1000 | Loss: 0.00001138
Iteration 142/1000 | Loss: 0.00001138
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001136
Iteration 146/1000 | Loss: 0.00001136
Iteration 147/1000 | Loss: 0.00001135
Iteration 148/1000 | Loss: 0.00001135
Iteration 149/1000 | Loss: 0.00001135
Iteration 150/1000 | Loss: 0.00001135
Iteration 151/1000 | Loss: 0.00001134
Iteration 152/1000 | Loss: 0.00001134
Iteration 153/1000 | Loss: 0.00001134
Iteration 154/1000 | Loss: 0.00001134
Iteration 155/1000 | Loss: 0.00001134
Iteration 156/1000 | Loss: 0.00001133
Iteration 157/1000 | Loss: 0.00001133
Iteration 158/1000 | Loss: 0.00001132
Iteration 159/1000 | Loss: 0.00001132
Iteration 160/1000 | Loss: 0.00001132
Iteration 161/1000 | Loss: 0.00001132
Iteration 162/1000 | Loss: 0.00001131
Iteration 163/1000 | Loss: 0.00001131
Iteration 164/1000 | Loss: 0.00001131
Iteration 165/1000 | Loss: 0.00001131
Iteration 166/1000 | Loss: 0.00001131
Iteration 167/1000 | Loss: 0.00001131
Iteration 168/1000 | Loss: 0.00001130
Iteration 169/1000 | Loss: 0.00001130
Iteration 170/1000 | Loss: 0.00001130
Iteration 171/1000 | Loss: 0.00001130
Iteration 172/1000 | Loss: 0.00001130
Iteration 173/1000 | Loss: 0.00001130
Iteration 174/1000 | Loss: 0.00001130
Iteration 175/1000 | Loss: 0.00001130
Iteration 176/1000 | Loss: 0.00001130
Iteration 177/1000 | Loss: 0.00001130
Iteration 178/1000 | Loss: 0.00001129
Iteration 179/1000 | Loss: 0.00001129
Iteration 180/1000 | Loss: 0.00001129
Iteration 181/1000 | Loss: 0.00001129
Iteration 182/1000 | Loss: 0.00001129
Iteration 183/1000 | Loss: 0.00001129
Iteration 184/1000 | Loss: 0.00001129
Iteration 185/1000 | Loss: 0.00001129
Iteration 186/1000 | Loss: 0.00001129
Iteration 187/1000 | Loss: 0.00001128
Iteration 188/1000 | Loss: 0.00001128
Iteration 189/1000 | Loss: 0.00001128
Iteration 190/1000 | Loss: 0.00001128
Iteration 191/1000 | Loss: 0.00001128
Iteration 192/1000 | Loss: 0.00001128
Iteration 193/1000 | Loss: 0.00001128
Iteration 194/1000 | Loss: 0.00001128
Iteration 195/1000 | Loss: 0.00001128
Iteration 196/1000 | Loss: 0.00001127
Iteration 197/1000 | Loss: 0.00001127
Iteration 198/1000 | Loss: 0.00001127
Iteration 199/1000 | Loss: 0.00001127
Iteration 200/1000 | Loss: 0.00001127
Iteration 201/1000 | Loss: 0.00001126
Iteration 202/1000 | Loss: 0.00001126
Iteration 203/1000 | Loss: 0.00001126
Iteration 204/1000 | Loss: 0.00001126
Iteration 205/1000 | Loss: 0.00001126
Iteration 206/1000 | Loss: 0.00001126
Iteration 207/1000 | Loss: 0.00001126
Iteration 208/1000 | Loss: 0.00001126
Iteration 209/1000 | Loss: 0.00001126
Iteration 210/1000 | Loss: 0.00001126
Iteration 211/1000 | Loss: 0.00001126
Iteration 212/1000 | Loss: 0.00001125
Iteration 213/1000 | Loss: 0.00001125
Iteration 214/1000 | Loss: 0.00001125
Iteration 215/1000 | Loss: 0.00001125
Iteration 216/1000 | Loss: 0.00001125
Iteration 217/1000 | Loss: 0.00001125
Iteration 218/1000 | Loss: 0.00001125
Iteration 219/1000 | Loss: 0.00001125
Iteration 220/1000 | Loss: 0.00001124
Iteration 221/1000 | Loss: 0.00001124
Iteration 222/1000 | Loss: 0.00001124
Iteration 223/1000 | Loss: 0.00001124
Iteration 224/1000 | Loss: 0.00001124
Iteration 225/1000 | Loss: 0.00001124
Iteration 226/1000 | Loss: 0.00001124
Iteration 227/1000 | Loss: 0.00001124
Iteration 228/1000 | Loss: 0.00001124
Iteration 229/1000 | Loss: 0.00001124
Iteration 230/1000 | Loss: 0.00001124
Iteration 231/1000 | Loss: 0.00001124
Iteration 232/1000 | Loss: 0.00001124
Iteration 233/1000 | Loss: 0.00001124
Iteration 234/1000 | Loss: 0.00001124
Iteration 235/1000 | Loss: 0.00001124
Iteration 236/1000 | Loss: 0.00001124
Iteration 237/1000 | Loss: 0.00001124
Iteration 238/1000 | Loss: 0.00001124
Iteration 239/1000 | Loss: 0.00001124
Iteration 240/1000 | Loss: 0.00001124
Iteration 241/1000 | Loss: 0.00001124
Iteration 242/1000 | Loss: 0.00001124
Iteration 243/1000 | Loss: 0.00001124
Iteration 244/1000 | Loss: 0.00001124
Iteration 245/1000 | Loss: 0.00001124
Iteration 246/1000 | Loss: 0.00001124
Iteration 247/1000 | Loss: 0.00001124
Iteration 248/1000 | Loss: 0.00001124
Iteration 249/1000 | Loss: 0.00001124
Iteration 250/1000 | Loss: 0.00001124
Iteration 251/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.1241084393986966e-05, 1.1241084393986966e-05, 1.1241084393986966e-05, 1.1241084393986966e-05, 1.1241084393986966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1241084393986966e-05

Optimization complete. Final v2v error: 2.8523449897766113 mm

Highest mean error: 3.3194897174835205 mm for frame 80

Lowest mean error: 2.356055498123169 mm for frame 184

Saving results

Total time: 51.05279898643494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483084
Iteration 2/25 | Loss: 0.00132659
Iteration 3/25 | Loss: 0.00105127
Iteration 4/25 | Loss: 0.00102106
Iteration 5/25 | Loss: 0.00101760
Iteration 6/25 | Loss: 0.00101677
Iteration 7/25 | Loss: 0.00101677
Iteration 8/25 | Loss: 0.00101677
Iteration 9/25 | Loss: 0.00101677
Iteration 10/25 | Loss: 0.00101677
Iteration 11/25 | Loss: 0.00101677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010167703730985522, 0.0010167703730985522, 0.0010167703730985522, 0.0010167703730985522, 0.0010167703730985522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010167703730985522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35227394
Iteration 2/25 | Loss: 0.00089600
Iteration 3/25 | Loss: 0.00089600
Iteration 4/25 | Loss: 0.00089600
Iteration 5/25 | Loss: 0.00089600
Iteration 6/25 | Loss: 0.00089600
Iteration 7/25 | Loss: 0.00089600
Iteration 8/25 | Loss: 0.00089600
Iteration 9/25 | Loss: 0.00089600
Iteration 10/25 | Loss: 0.00089600
Iteration 11/25 | Loss: 0.00089600
Iteration 12/25 | Loss: 0.00089600
Iteration 13/25 | Loss: 0.00089600
Iteration 14/25 | Loss: 0.00089600
Iteration 15/25 | Loss: 0.00089600
Iteration 16/25 | Loss: 0.00089600
Iteration 17/25 | Loss: 0.00089600
Iteration 18/25 | Loss: 0.00089600
Iteration 19/25 | Loss: 0.00089600
Iteration 20/25 | Loss: 0.00089600
Iteration 21/25 | Loss: 0.00089600
Iteration 22/25 | Loss: 0.00089600
Iteration 23/25 | Loss: 0.00089600
Iteration 24/25 | Loss: 0.00089600
Iteration 25/25 | Loss: 0.00089600

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089600
Iteration 2/1000 | Loss: 0.00002755
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001670
Iteration 5/1000 | Loss: 0.00001556
Iteration 6/1000 | Loss: 0.00001509
Iteration 7/1000 | Loss: 0.00001470
Iteration 8/1000 | Loss: 0.00001431
Iteration 9/1000 | Loss: 0.00001407
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001390
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001381
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001374
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001367
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001365
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001365
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001364
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001363
Iteration 52/1000 | Loss: 0.00001363
Iteration 53/1000 | Loss: 0.00001363
Iteration 54/1000 | Loss: 0.00001363
Iteration 55/1000 | Loss: 0.00001363
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001362
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001362
Iteration 60/1000 | Loss: 0.00001362
Iteration 61/1000 | Loss: 0.00001362
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001361
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001358
Iteration 80/1000 | Loss: 0.00001358
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001356
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001354
Iteration 108/1000 | Loss: 0.00001354
Iteration 109/1000 | Loss: 0.00001354
Iteration 110/1000 | Loss: 0.00001354
Iteration 111/1000 | Loss: 0.00001353
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001353
Iteration 114/1000 | Loss: 0.00001353
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001351
Iteration 120/1000 | Loss: 0.00001351
Iteration 121/1000 | Loss: 0.00001351
Iteration 122/1000 | Loss: 0.00001351
Iteration 123/1000 | Loss: 0.00001350
Iteration 124/1000 | Loss: 0.00001350
Iteration 125/1000 | Loss: 0.00001350
Iteration 126/1000 | Loss: 0.00001350
Iteration 127/1000 | Loss: 0.00001350
Iteration 128/1000 | Loss: 0.00001350
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001350
Iteration 131/1000 | Loss: 0.00001349
Iteration 132/1000 | Loss: 0.00001349
Iteration 133/1000 | Loss: 0.00001349
Iteration 134/1000 | Loss: 0.00001349
Iteration 135/1000 | Loss: 0.00001349
Iteration 136/1000 | Loss: 0.00001349
Iteration 137/1000 | Loss: 0.00001349
Iteration 138/1000 | Loss: 0.00001349
Iteration 139/1000 | Loss: 0.00001348
Iteration 140/1000 | Loss: 0.00001348
Iteration 141/1000 | Loss: 0.00001348
Iteration 142/1000 | Loss: 0.00001348
Iteration 143/1000 | Loss: 0.00001347
Iteration 144/1000 | Loss: 0.00001347
Iteration 145/1000 | Loss: 0.00001347
Iteration 146/1000 | Loss: 0.00001346
Iteration 147/1000 | Loss: 0.00001346
Iteration 148/1000 | Loss: 0.00001346
Iteration 149/1000 | Loss: 0.00001346
Iteration 150/1000 | Loss: 0.00001346
Iteration 151/1000 | Loss: 0.00001346
Iteration 152/1000 | Loss: 0.00001345
Iteration 153/1000 | Loss: 0.00001345
Iteration 154/1000 | Loss: 0.00001345
Iteration 155/1000 | Loss: 0.00001345
Iteration 156/1000 | Loss: 0.00001345
Iteration 157/1000 | Loss: 0.00001345
Iteration 158/1000 | Loss: 0.00001345
Iteration 159/1000 | Loss: 0.00001344
Iteration 160/1000 | Loss: 0.00001344
Iteration 161/1000 | Loss: 0.00001344
Iteration 162/1000 | Loss: 0.00001344
Iteration 163/1000 | Loss: 0.00001344
Iteration 164/1000 | Loss: 0.00001344
Iteration 165/1000 | Loss: 0.00001344
Iteration 166/1000 | Loss: 0.00001344
Iteration 167/1000 | Loss: 0.00001343
Iteration 168/1000 | Loss: 0.00001343
Iteration 169/1000 | Loss: 0.00001343
Iteration 170/1000 | Loss: 0.00001343
Iteration 171/1000 | Loss: 0.00001343
Iteration 172/1000 | Loss: 0.00001343
Iteration 173/1000 | Loss: 0.00001343
Iteration 174/1000 | Loss: 0.00001343
Iteration 175/1000 | Loss: 0.00001343
Iteration 176/1000 | Loss: 0.00001343
Iteration 177/1000 | Loss: 0.00001343
Iteration 178/1000 | Loss: 0.00001342
Iteration 179/1000 | Loss: 0.00001342
Iteration 180/1000 | Loss: 0.00001342
Iteration 181/1000 | Loss: 0.00001342
Iteration 182/1000 | Loss: 0.00001342
Iteration 183/1000 | Loss: 0.00001342
Iteration 184/1000 | Loss: 0.00001342
Iteration 185/1000 | Loss: 0.00001342
Iteration 186/1000 | Loss: 0.00001341
Iteration 187/1000 | Loss: 0.00001341
Iteration 188/1000 | Loss: 0.00001341
Iteration 189/1000 | Loss: 0.00001341
Iteration 190/1000 | Loss: 0.00001341
Iteration 191/1000 | Loss: 0.00001341
Iteration 192/1000 | Loss: 0.00001341
Iteration 193/1000 | Loss: 0.00001340
Iteration 194/1000 | Loss: 0.00001340
Iteration 195/1000 | Loss: 0.00001340
Iteration 196/1000 | Loss: 0.00001340
Iteration 197/1000 | Loss: 0.00001340
Iteration 198/1000 | Loss: 0.00001340
Iteration 199/1000 | Loss: 0.00001340
Iteration 200/1000 | Loss: 0.00001340
Iteration 201/1000 | Loss: 0.00001340
Iteration 202/1000 | Loss: 0.00001340
Iteration 203/1000 | Loss: 0.00001340
Iteration 204/1000 | Loss: 0.00001340
Iteration 205/1000 | Loss: 0.00001339
Iteration 206/1000 | Loss: 0.00001339
Iteration 207/1000 | Loss: 0.00001339
Iteration 208/1000 | Loss: 0.00001339
Iteration 209/1000 | Loss: 0.00001339
Iteration 210/1000 | Loss: 0.00001339
Iteration 211/1000 | Loss: 0.00001339
Iteration 212/1000 | Loss: 0.00001339
Iteration 213/1000 | Loss: 0.00001339
Iteration 214/1000 | Loss: 0.00001339
Iteration 215/1000 | Loss: 0.00001339
Iteration 216/1000 | Loss: 0.00001339
Iteration 217/1000 | Loss: 0.00001339
Iteration 218/1000 | Loss: 0.00001338
Iteration 219/1000 | Loss: 0.00001338
Iteration 220/1000 | Loss: 0.00001338
Iteration 221/1000 | Loss: 0.00001338
Iteration 222/1000 | Loss: 0.00001338
Iteration 223/1000 | Loss: 0.00001338
Iteration 224/1000 | Loss: 0.00001338
Iteration 225/1000 | Loss: 0.00001338
Iteration 226/1000 | Loss: 0.00001338
Iteration 227/1000 | Loss: 0.00001338
Iteration 228/1000 | Loss: 0.00001338
Iteration 229/1000 | Loss: 0.00001338
Iteration 230/1000 | Loss: 0.00001337
Iteration 231/1000 | Loss: 0.00001337
Iteration 232/1000 | Loss: 0.00001337
Iteration 233/1000 | Loss: 0.00001337
Iteration 234/1000 | Loss: 0.00001337
Iteration 235/1000 | Loss: 0.00001337
Iteration 236/1000 | Loss: 0.00001337
Iteration 237/1000 | Loss: 0.00001337
Iteration 238/1000 | Loss: 0.00001337
Iteration 239/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.3374998161452822e-05, 1.3374998161452822e-05, 1.3374998161452822e-05, 1.3374998161452822e-05, 1.3374998161452822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3374998161452822e-05

Optimization complete. Final v2v error: 2.9054980278015137 mm

Highest mean error: 4.606525421142578 mm for frame 82

Lowest mean error: 2.2378101348876953 mm for frame 48

Saving results

Total time: 41.51965689659119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436513
Iteration 2/25 | Loss: 0.00113615
Iteration 3/25 | Loss: 0.00103619
Iteration 4/25 | Loss: 0.00101710
Iteration 5/25 | Loss: 0.00101013
Iteration 6/25 | Loss: 0.00100809
Iteration 7/25 | Loss: 0.00100777
Iteration 8/25 | Loss: 0.00100777
Iteration 9/25 | Loss: 0.00100777
Iteration 10/25 | Loss: 0.00100777
Iteration 11/25 | Loss: 0.00100777
Iteration 12/25 | Loss: 0.00100777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010077726328745484, 0.0010077726328745484, 0.0010077726328745484, 0.0010077726328745484, 0.0010077726328745484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010077726328745484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.83318758
Iteration 2/25 | Loss: 0.00117825
Iteration 3/25 | Loss: 0.00117823
Iteration 4/25 | Loss: 0.00117823
Iteration 5/25 | Loss: 0.00117823
Iteration 6/25 | Loss: 0.00117823
Iteration 7/25 | Loss: 0.00117823
Iteration 8/25 | Loss: 0.00117823
Iteration 9/25 | Loss: 0.00117823
Iteration 10/25 | Loss: 0.00117823
Iteration 11/25 | Loss: 0.00117823
Iteration 12/25 | Loss: 0.00117823
Iteration 13/25 | Loss: 0.00117823
Iteration 14/25 | Loss: 0.00117823
Iteration 15/25 | Loss: 0.00117823
Iteration 16/25 | Loss: 0.00117823
Iteration 17/25 | Loss: 0.00117823
Iteration 18/25 | Loss: 0.00117823
Iteration 19/25 | Loss: 0.00117823
Iteration 20/25 | Loss: 0.00117823
Iteration 21/25 | Loss: 0.00117823
Iteration 22/25 | Loss: 0.00117823
Iteration 23/25 | Loss: 0.00117823
Iteration 24/25 | Loss: 0.00117823
Iteration 25/25 | Loss: 0.00117823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117823
Iteration 2/1000 | Loss: 0.00003318
Iteration 3/1000 | Loss: 0.00002106
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001405
Iteration 7/1000 | Loss: 0.00001369
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001313
Iteration 10/1000 | Loss: 0.00001304
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001270
Iteration 14/1000 | Loss: 0.00001269
Iteration 15/1000 | Loss: 0.00001269
Iteration 16/1000 | Loss: 0.00001263
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001262
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001262
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001260
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001258
Iteration 32/1000 | Loss: 0.00001258
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001245
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001245
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001242
Iteration 81/1000 | Loss: 0.00001242
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001240
Iteration 96/1000 | Loss: 0.00001240
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001238
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001237
Iteration 120/1000 | Loss: 0.00001237
Iteration 121/1000 | Loss: 0.00001237
Iteration 122/1000 | Loss: 0.00001237
Iteration 123/1000 | Loss: 0.00001237
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001237
Iteration 127/1000 | Loss: 0.00001237
Iteration 128/1000 | Loss: 0.00001237
Iteration 129/1000 | Loss: 0.00001237
Iteration 130/1000 | Loss: 0.00001237
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001236
Iteration 133/1000 | Loss: 0.00001236
Iteration 134/1000 | Loss: 0.00001236
Iteration 135/1000 | Loss: 0.00001236
Iteration 136/1000 | Loss: 0.00001236
Iteration 137/1000 | Loss: 0.00001236
Iteration 138/1000 | Loss: 0.00001236
Iteration 139/1000 | Loss: 0.00001236
Iteration 140/1000 | Loss: 0.00001236
Iteration 141/1000 | Loss: 0.00001236
Iteration 142/1000 | Loss: 0.00001236
Iteration 143/1000 | Loss: 0.00001236
Iteration 144/1000 | Loss: 0.00001236
Iteration 145/1000 | Loss: 0.00001236
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001235
Iteration 148/1000 | Loss: 0.00001235
Iteration 149/1000 | Loss: 0.00001235
Iteration 150/1000 | Loss: 0.00001235
Iteration 151/1000 | Loss: 0.00001235
Iteration 152/1000 | Loss: 0.00001235
Iteration 153/1000 | Loss: 0.00001235
Iteration 154/1000 | Loss: 0.00001235
Iteration 155/1000 | Loss: 0.00001235
Iteration 156/1000 | Loss: 0.00001235
Iteration 157/1000 | Loss: 0.00001235
Iteration 158/1000 | Loss: 0.00001235
Iteration 159/1000 | Loss: 0.00001234
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001234
Iteration 162/1000 | Loss: 0.00001234
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001234
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001233
Iteration 168/1000 | Loss: 0.00001233
Iteration 169/1000 | Loss: 0.00001233
Iteration 170/1000 | Loss: 0.00001233
Iteration 171/1000 | Loss: 0.00001233
Iteration 172/1000 | Loss: 0.00001233
Iteration 173/1000 | Loss: 0.00001233
Iteration 174/1000 | Loss: 0.00001233
Iteration 175/1000 | Loss: 0.00001233
Iteration 176/1000 | Loss: 0.00001233
Iteration 177/1000 | Loss: 0.00001233
Iteration 178/1000 | Loss: 0.00001233
Iteration 179/1000 | Loss: 0.00001233
Iteration 180/1000 | Loss: 0.00001233
Iteration 181/1000 | Loss: 0.00001233
Iteration 182/1000 | Loss: 0.00001233
Iteration 183/1000 | Loss: 0.00001232
Iteration 184/1000 | Loss: 0.00001232
Iteration 185/1000 | Loss: 0.00001232
Iteration 186/1000 | Loss: 0.00001232
Iteration 187/1000 | Loss: 0.00001232
Iteration 188/1000 | Loss: 0.00001232
Iteration 189/1000 | Loss: 0.00001232
Iteration 190/1000 | Loss: 0.00001232
Iteration 191/1000 | Loss: 0.00001232
Iteration 192/1000 | Loss: 0.00001232
Iteration 193/1000 | Loss: 0.00001232
Iteration 194/1000 | Loss: 0.00001232
Iteration 195/1000 | Loss: 0.00001232
Iteration 196/1000 | Loss: 0.00001232
Iteration 197/1000 | Loss: 0.00001232
Iteration 198/1000 | Loss: 0.00001232
Iteration 199/1000 | Loss: 0.00001232
Iteration 200/1000 | Loss: 0.00001232
Iteration 201/1000 | Loss: 0.00001232
Iteration 202/1000 | Loss: 0.00001232
Iteration 203/1000 | Loss: 0.00001232
Iteration 204/1000 | Loss: 0.00001232
Iteration 205/1000 | Loss: 0.00001232
Iteration 206/1000 | Loss: 0.00001232
Iteration 207/1000 | Loss: 0.00001232
Iteration 208/1000 | Loss: 0.00001232
Iteration 209/1000 | Loss: 0.00001232
Iteration 210/1000 | Loss: 0.00001232
Iteration 211/1000 | Loss: 0.00001232
Iteration 212/1000 | Loss: 0.00001232
Iteration 213/1000 | Loss: 0.00001232
Iteration 214/1000 | Loss: 0.00001232
Iteration 215/1000 | Loss: 0.00001232
Iteration 216/1000 | Loss: 0.00001232
Iteration 217/1000 | Loss: 0.00001232
Iteration 218/1000 | Loss: 0.00001232
Iteration 219/1000 | Loss: 0.00001232
Iteration 220/1000 | Loss: 0.00001232
Iteration 221/1000 | Loss: 0.00001232
Iteration 222/1000 | Loss: 0.00001232
Iteration 223/1000 | Loss: 0.00001232
Iteration 224/1000 | Loss: 0.00001232
Iteration 225/1000 | Loss: 0.00001232
Iteration 226/1000 | Loss: 0.00001232
Iteration 227/1000 | Loss: 0.00001232
Iteration 228/1000 | Loss: 0.00001232
Iteration 229/1000 | Loss: 0.00001232
Iteration 230/1000 | Loss: 0.00001232
Iteration 231/1000 | Loss: 0.00001232
Iteration 232/1000 | Loss: 0.00001232
Iteration 233/1000 | Loss: 0.00001232
Iteration 234/1000 | Loss: 0.00001232
Iteration 235/1000 | Loss: 0.00001232
Iteration 236/1000 | Loss: 0.00001232
Iteration 237/1000 | Loss: 0.00001232
Iteration 238/1000 | Loss: 0.00001232
Iteration 239/1000 | Loss: 0.00001232
Iteration 240/1000 | Loss: 0.00001232
Iteration 241/1000 | Loss: 0.00001232
Iteration 242/1000 | Loss: 0.00001232
Iteration 243/1000 | Loss: 0.00001232
Iteration 244/1000 | Loss: 0.00001232
Iteration 245/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.2322667316766456e-05, 1.2322667316766456e-05, 1.2322667316766456e-05, 1.2322667316766456e-05, 1.2322667316766456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2322667316766456e-05

Optimization complete. Final v2v error: 2.9823288917541504 mm

Highest mean error: 3.602734088897705 mm for frame 19

Lowest mean error: 2.370060920715332 mm for frame 7

Saving results

Total time: 39.80188703536987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049856
Iteration 2/25 | Loss: 0.00123396
Iteration 3/25 | Loss: 0.00110581
Iteration 4/25 | Loss: 0.00109168
Iteration 5/25 | Loss: 0.00108826
Iteration 6/25 | Loss: 0.00108711
Iteration 7/25 | Loss: 0.00108711
Iteration 8/25 | Loss: 0.00108711
Iteration 9/25 | Loss: 0.00108711
Iteration 10/25 | Loss: 0.00108711
Iteration 11/25 | Loss: 0.00108711
Iteration 12/25 | Loss: 0.00108711
Iteration 13/25 | Loss: 0.00108711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010871115373447537, 0.0010871115373447537, 0.0010871115373447537, 0.0010871115373447537, 0.0010871115373447537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010871115373447537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.18062973
Iteration 2/25 | Loss: 0.00109453
Iteration 3/25 | Loss: 0.00109453
Iteration 4/25 | Loss: 0.00109453
Iteration 5/25 | Loss: 0.00109453
Iteration 6/25 | Loss: 0.00109453
Iteration 7/25 | Loss: 0.00109453
Iteration 8/25 | Loss: 0.00109453
Iteration 9/25 | Loss: 0.00109453
Iteration 10/25 | Loss: 0.00109453
Iteration 11/25 | Loss: 0.00109453
Iteration 12/25 | Loss: 0.00109453
Iteration 13/25 | Loss: 0.00109453
Iteration 14/25 | Loss: 0.00109453
Iteration 15/25 | Loss: 0.00109453
Iteration 16/25 | Loss: 0.00109453
Iteration 17/25 | Loss: 0.00109453
Iteration 18/25 | Loss: 0.00109453
Iteration 19/25 | Loss: 0.00109453
Iteration 20/25 | Loss: 0.00109453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001094527542591095, 0.001094527542591095, 0.001094527542591095, 0.001094527542591095, 0.001094527542591095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001094527542591095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109453
Iteration 2/1000 | Loss: 0.00003098
Iteration 3/1000 | Loss: 0.00001956
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001533
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001447
Iteration 16/1000 | Loss: 0.00001447
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001441
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001441
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001437
Iteration 24/1000 | Loss: 0.00001434
Iteration 25/1000 | Loss: 0.00001433
Iteration 26/1000 | Loss: 0.00001433
Iteration 27/1000 | Loss: 0.00001431
Iteration 28/1000 | Loss: 0.00001431
Iteration 29/1000 | Loss: 0.00001431
Iteration 30/1000 | Loss: 0.00001430
Iteration 31/1000 | Loss: 0.00001430
Iteration 32/1000 | Loss: 0.00001430
Iteration 33/1000 | Loss: 0.00001429
Iteration 34/1000 | Loss: 0.00001429
Iteration 35/1000 | Loss: 0.00001429
Iteration 36/1000 | Loss: 0.00001428
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001427
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001426
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001424
Iteration 51/1000 | Loss: 0.00001424
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001423
Iteration 58/1000 | Loss: 0.00001423
Iteration 59/1000 | Loss: 0.00001423
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001423
Iteration 63/1000 | Loss: 0.00001422
Iteration 64/1000 | Loss: 0.00001422
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001421
Iteration 67/1000 | Loss: 0.00001421
Iteration 68/1000 | Loss: 0.00001420
Iteration 69/1000 | Loss: 0.00001420
Iteration 70/1000 | Loss: 0.00001420
Iteration 71/1000 | Loss: 0.00001420
Iteration 72/1000 | Loss: 0.00001420
Iteration 73/1000 | Loss: 0.00001420
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001419
Iteration 79/1000 | Loss: 0.00001419
Iteration 80/1000 | Loss: 0.00001418
Iteration 81/1000 | Loss: 0.00001418
Iteration 82/1000 | Loss: 0.00001418
Iteration 83/1000 | Loss: 0.00001418
Iteration 84/1000 | Loss: 0.00001418
Iteration 85/1000 | Loss: 0.00001418
Iteration 86/1000 | Loss: 0.00001418
Iteration 87/1000 | Loss: 0.00001417
Iteration 88/1000 | Loss: 0.00001417
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001417
Iteration 91/1000 | Loss: 0.00001417
Iteration 92/1000 | Loss: 0.00001417
Iteration 93/1000 | Loss: 0.00001417
Iteration 94/1000 | Loss: 0.00001417
Iteration 95/1000 | Loss: 0.00001417
Iteration 96/1000 | Loss: 0.00001417
Iteration 97/1000 | Loss: 0.00001417
Iteration 98/1000 | Loss: 0.00001416
Iteration 99/1000 | Loss: 0.00001416
Iteration 100/1000 | Loss: 0.00001416
Iteration 101/1000 | Loss: 0.00001415
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001415
Iteration 109/1000 | Loss: 0.00001415
Iteration 110/1000 | Loss: 0.00001415
Iteration 111/1000 | Loss: 0.00001415
Iteration 112/1000 | Loss: 0.00001415
Iteration 113/1000 | Loss: 0.00001415
Iteration 114/1000 | Loss: 0.00001415
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.4145516615826637e-05, 1.4145516615826637e-05, 1.4145516615826637e-05, 1.4145516615826637e-05, 1.4145516615826637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4145516615826637e-05

Optimization complete. Final v2v error: 3.2586491107940674 mm

Highest mean error: 3.5774245262145996 mm for frame 99

Lowest mean error: 2.783097267150879 mm for frame 125

Saving results

Total time: 31.615405082702637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812420
Iteration 2/25 | Loss: 0.00118322
Iteration 3/25 | Loss: 0.00106834
Iteration 4/25 | Loss: 0.00105171
Iteration 5/25 | Loss: 0.00104722
Iteration 6/25 | Loss: 0.00104582
Iteration 7/25 | Loss: 0.00104582
Iteration 8/25 | Loss: 0.00104582
Iteration 9/25 | Loss: 0.00104582
Iteration 10/25 | Loss: 0.00104582
Iteration 11/25 | Loss: 0.00104582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010458234464749694, 0.0010458234464749694, 0.0010458234464749694, 0.0010458234464749694, 0.0010458234464749694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010458234464749694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43191051
Iteration 2/25 | Loss: 0.00107849
Iteration 3/25 | Loss: 0.00107848
Iteration 4/25 | Loss: 0.00107848
Iteration 5/25 | Loss: 0.00107848
Iteration 6/25 | Loss: 0.00107848
Iteration 7/25 | Loss: 0.00107848
Iteration 8/25 | Loss: 0.00107848
Iteration 9/25 | Loss: 0.00107848
Iteration 10/25 | Loss: 0.00107848
Iteration 11/25 | Loss: 0.00107848
Iteration 12/25 | Loss: 0.00107848
Iteration 13/25 | Loss: 0.00107848
Iteration 14/25 | Loss: 0.00107848
Iteration 15/25 | Loss: 0.00107848
Iteration 16/25 | Loss: 0.00107848
Iteration 17/25 | Loss: 0.00107848
Iteration 18/25 | Loss: 0.00107848
Iteration 19/25 | Loss: 0.00107848
Iteration 20/25 | Loss: 0.00107848
Iteration 21/25 | Loss: 0.00107848
Iteration 22/25 | Loss: 0.00107848
Iteration 23/25 | Loss: 0.00107848
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010784812038764358, 0.0010784812038764358, 0.0010784812038764358, 0.0010784812038764358, 0.0010784812038764358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010784812038764358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107848
Iteration 2/1000 | Loss: 0.00004274
Iteration 3/1000 | Loss: 0.00002592
Iteration 4/1000 | Loss: 0.00002039
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001681
Iteration 7/1000 | Loss: 0.00001619
Iteration 8/1000 | Loss: 0.00001581
Iteration 9/1000 | Loss: 0.00001554
Iteration 10/1000 | Loss: 0.00001527
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001482
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00001467
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001460
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001456
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001450
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001450
Iteration 35/1000 | Loss: 0.00001449
Iteration 36/1000 | Loss: 0.00001449
Iteration 37/1000 | Loss: 0.00001448
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001446
Iteration 43/1000 | Loss: 0.00001446
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001445
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001445
Iteration 51/1000 | Loss: 0.00001445
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001444
Iteration 54/1000 | Loss: 0.00001444
Iteration 55/1000 | Loss: 0.00001444
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001443
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001441
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001440
Iteration 80/1000 | Loss: 0.00001440
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001439
Iteration 91/1000 | Loss: 0.00001439
Iteration 92/1000 | Loss: 0.00001439
Iteration 93/1000 | Loss: 0.00001439
Iteration 94/1000 | Loss: 0.00001439
Iteration 95/1000 | Loss: 0.00001439
Iteration 96/1000 | Loss: 0.00001439
Iteration 97/1000 | Loss: 0.00001439
Iteration 98/1000 | Loss: 0.00001439
Iteration 99/1000 | Loss: 0.00001439
Iteration 100/1000 | Loss: 0.00001439
Iteration 101/1000 | Loss: 0.00001439
Iteration 102/1000 | Loss: 0.00001438
Iteration 103/1000 | Loss: 0.00001438
Iteration 104/1000 | Loss: 0.00001438
Iteration 105/1000 | Loss: 0.00001438
Iteration 106/1000 | Loss: 0.00001438
Iteration 107/1000 | Loss: 0.00001437
Iteration 108/1000 | Loss: 0.00001437
Iteration 109/1000 | Loss: 0.00001437
Iteration 110/1000 | Loss: 0.00001437
Iteration 111/1000 | Loss: 0.00001437
Iteration 112/1000 | Loss: 0.00001437
Iteration 113/1000 | Loss: 0.00001437
Iteration 114/1000 | Loss: 0.00001437
Iteration 115/1000 | Loss: 0.00001437
Iteration 116/1000 | Loss: 0.00001437
Iteration 117/1000 | Loss: 0.00001437
Iteration 118/1000 | Loss: 0.00001437
Iteration 119/1000 | Loss: 0.00001437
Iteration 120/1000 | Loss: 0.00001436
Iteration 121/1000 | Loss: 0.00001436
Iteration 122/1000 | Loss: 0.00001436
Iteration 123/1000 | Loss: 0.00001436
Iteration 124/1000 | Loss: 0.00001436
Iteration 125/1000 | Loss: 0.00001436
Iteration 126/1000 | Loss: 0.00001435
Iteration 127/1000 | Loss: 0.00001435
Iteration 128/1000 | Loss: 0.00001435
Iteration 129/1000 | Loss: 0.00001435
Iteration 130/1000 | Loss: 0.00001435
Iteration 131/1000 | Loss: 0.00001435
Iteration 132/1000 | Loss: 0.00001435
Iteration 133/1000 | Loss: 0.00001435
Iteration 134/1000 | Loss: 0.00001435
Iteration 135/1000 | Loss: 0.00001435
Iteration 136/1000 | Loss: 0.00001435
Iteration 137/1000 | Loss: 0.00001435
Iteration 138/1000 | Loss: 0.00001435
Iteration 139/1000 | Loss: 0.00001435
Iteration 140/1000 | Loss: 0.00001434
Iteration 141/1000 | Loss: 0.00001434
Iteration 142/1000 | Loss: 0.00001434
Iteration 143/1000 | Loss: 0.00001434
Iteration 144/1000 | Loss: 0.00001434
Iteration 145/1000 | Loss: 0.00001434
Iteration 146/1000 | Loss: 0.00001434
Iteration 147/1000 | Loss: 0.00001434
Iteration 148/1000 | Loss: 0.00001434
Iteration 149/1000 | Loss: 0.00001434
Iteration 150/1000 | Loss: 0.00001434
Iteration 151/1000 | Loss: 0.00001434
Iteration 152/1000 | Loss: 0.00001434
Iteration 153/1000 | Loss: 0.00001434
Iteration 154/1000 | Loss: 0.00001434
Iteration 155/1000 | Loss: 0.00001434
Iteration 156/1000 | Loss: 0.00001434
Iteration 157/1000 | Loss: 0.00001433
Iteration 158/1000 | Loss: 0.00001433
Iteration 159/1000 | Loss: 0.00001433
Iteration 160/1000 | Loss: 0.00001433
Iteration 161/1000 | Loss: 0.00001433
Iteration 162/1000 | Loss: 0.00001433
Iteration 163/1000 | Loss: 0.00001433
Iteration 164/1000 | Loss: 0.00001433
Iteration 165/1000 | Loss: 0.00001433
Iteration 166/1000 | Loss: 0.00001433
Iteration 167/1000 | Loss: 0.00001433
Iteration 168/1000 | Loss: 0.00001433
Iteration 169/1000 | Loss: 0.00001433
Iteration 170/1000 | Loss: 0.00001433
Iteration 171/1000 | Loss: 0.00001433
Iteration 172/1000 | Loss: 0.00001433
Iteration 173/1000 | Loss: 0.00001433
Iteration 174/1000 | Loss: 0.00001433
Iteration 175/1000 | Loss: 0.00001433
Iteration 176/1000 | Loss: 0.00001433
Iteration 177/1000 | Loss: 0.00001433
Iteration 178/1000 | Loss: 0.00001432
Iteration 179/1000 | Loss: 0.00001432
Iteration 180/1000 | Loss: 0.00001432
Iteration 181/1000 | Loss: 0.00001432
Iteration 182/1000 | Loss: 0.00001432
Iteration 183/1000 | Loss: 0.00001432
Iteration 184/1000 | Loss: 0.00001432
Iteration 185/1000 | Loss: 0.00001432
Iteration 186/1000 | Loss: 0.00001432
Iteration 187/1000 | Loss: 0.00001432
Iteration 188/1000 | Loss: 0.00001432
Iteration 189/1000 | Loss: 0.00001432
Iteration 190/1000 | Loss: 0.00001432
Iteration 191/1000 | Loss: 0.00001432
Iteration 192/1000 | Loss: 0.00001432
Iteration 193/1000 | Loss: 0.00001432
Iteration 194/1000 | Loss: 0.00001432
Iteration 195/1000 | Loss: 0.00001432
Iteration 196/1000 | Loss: 0.00001431
Iteration 197/1000 | Loss: 0.00001431
Iteration 198/1000 | Loss: 0.00001431
Iteration 199/1000 | Loss: 0.00001431
Iteration 200/1000 | Loss: 0.00001431
Iteration 201/1000 | Loss: 0.00001431
Iteration 202/1000 | Loss: 0.00001431
Iteration 203/1000 | Loss: 0.00001431
Iteration 204/1000 | Loss: 0.00001431
Iteration 205/1000 | Loss: 0.00001431
Iteration 206/1000 | Loss: 0.00001431
Iteration 207/1000 | Loss: 0.00001431
Iteration 208/1000 | Loss: 0.00001431
Iteration 209/1000 | Loss: 0.00001431
Iteration 210/1000 | Loss: 0.00001431
Iteration 211/1000 | Loss: 0.00001431
Iteration 212/1000 | Loss: 0.00001430
Iteration 213/1000 | Loss: 0.00001430
Iteration 214/1000 | Loss: 0.00001430
Iteration 215/1000 | Loss: 0.00001430
Iteration 216/1000 | Loss: 0.00001430
Iteration 217/1000 | Loss: 0.00001430
Iteration 218/1000 | Loss: 0.00001430
Iteration 219/1000 | Loss: 0.00001430
Iteration 220/1000 | Loss: 0.00001430
Iteration 221/1000 | Loss: 0.00001430
Iteration 222/1000 | Loss: 0.00001430
Iteration 223/1000 | Loss: 0.00001430
Iteration 224/1000 | Loss: 0.00001430
Iteration 225/1000 | Loss: 0.00001430
Iteration 226/1000 | Loss: 0.00001430
Iteration 227/1000 | Loss: 0.00001430
Iteration 228/1000 | Loss: 0.00001430
Iteration 229/1000 | Loss: 0.00001430
Iteration 230/1000 | Loss: 0.00001430
Iteration 231/1000 | Loss: 0.00001430
Iteration 232/1000 | Loss: 0.00001429
Iteration 233/1000 | Loss: 0.00001429
Iteration 234/1000 | Loss: 0.00001429
Iteration 235/1000 | Loss: 0.00001429
Iteration 236/1000 | Loss: 0.00001429
Iteration 237/1000 | Loss: 0.00001429
Iteration 238/1000 | Loss: 0.00001429
Iteration 239/1000 | Loss: 0.00001428
Iteration 240/1000 | Loss: 0.00001428
Iteration 241/1000 | Loss: 0.00001428
Iteration 242/1000 | Loss: 0.00001428
Iteration 243/1000 | Loss: 0.00001428
Iteration 244/1000 | Loss: 0.00001428
Iteration 245/1000 | Loss: 0.00001428
Iteration 246/1000 | Loss: 0.00001428
Iteration 247/1000 | Loss: 0.00001428
Iteration 248/1000 | Loss: 0.00001428
Iteration 249/1000 | Loss: 0.00001428
Iteration 250/1000 | Loss: 0.00001428
Iteration 251/1000 | Loss: 0.00001428
Iteration 252/1000 | Loss: 0.00001428
Iteration 253/1000 | Loss: 0.00001428
Iteration 254/1000 | Loss: 0.00001428
Iteration 255/1000 | Loss: 0.00001428
Iteration 256/1000 | Loss: 0.00001428
Iteration 257/1000 | Loss: 0.00001428
Iteration 258/1000 | Loss: 0.00001428
Iteration 259/1000 | Loss: 0.00001428
Iteration 260/1000 | Loss: 0.00001428
Iteration 261/1000 | Loss: 0.00001428
Iteration 262/1000 | Loss: 0.00001428
Iteration 263/1000 | Loss: 0.00001428
Iteration 264/1000 | Loss: 0.00001428
Iteration 265/1000 | Loss: 0.00001428
Iteration 266/1000 | Loss: 0.00001428
Iteration 267/1000 | Loss: 0.00001428
Iteration 268/1000 | Loss: 0.00001428
Iteration 269/1000 | Loss: 0.00001428
Iteration 270/1000 | Loss: 0.00001428
Iteration 271/1000 | Loss: 0.00001428
Iteration 272/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.4277589798439294e-05, 1.4277589798439294e-05, 1.4277589798439294e-05, 1.4277589798439294e-05, 1.4277589798439294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4277589798439294e-05

Optimization complete. Final v2v error: 3.1799087524414062 mm

Highest mean error: 3.9365272521972656 mm for frame 38

Lowest mean error: 2.333493232727051 mm for frame 0

Saving results

Total time: 44.81797790527344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071589
Iteration 2/25 | Loss: 0.00623440
Iteration 3/25 | Loss: 0.00357924
Iteration 4/25 | Loss: 0.00378292
Iteration 5/25 | Loss: 0.00221535
Iteration 6/25 | Loss: 0.00199736
Iteration 7/25 | Loss: 0.00229726
Iteration 8/25 | Loss: 0.00211829
Iteration 9/25 | Loss: 0.00175022
Iteration 10/25 | Loss: 0.00145002
Iteration 11/25 | Loss: 0.00139505
Iteration 12/25 | Loss: 0.00132548
Iteration 13/25 | Loss: 0.00131143
Iteration 14/25 | Loss: 0.00130726
Iteration 15/25 | Loss: 0.00128263
Iteration 16/25 | Loss: 0.00127547
Iteration 17/25 | Loss: 0.00128424
Iteration 18/25 | Loss: 0.00127311
Iteration 19/25 | Loss: 0.00126412
Iteration 20/25 | Loss: 0.00126010
Iteration 21/25 | Loss: 0.00125399
Iteration 22/25 | Loss: 0.00125567
Iteration 23/25 | Loss: 0.00125333
Iteration 24/25 | Loss: 0.00125014
Iteration 25/25 | Loss: 0.00125260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43756026
Iteration 2/25 | Loss: 0.00059706
Iteration 3/25 | Loss: 0.00054016
Iteration 4/25 | Loss: 0.00054016
Iteration 5/25 | Loss: 0.00054016
Iteration 6/25 | Loss: 0.00054016
Iteration 7/25 | Loss: 0.00054016
Iteration 8/25 | Loss: 0.00054016
Iteration 9/25 | Loss: 0.00054016
Iteration 10/25 | Loss: 0.00054016
Iteration 11/25 | Loss: 0.00054016
Iteration 12/25 | Loss: 0.00054016
Iteration 13/25 | Loss: 0.00054016
Iteration 14/25 | Loss: 0.00054016
Iteration 15/25 | Loss: 0.00054016
Iteration 16/25 | Loss: 0.00054016
Iteration 17/25 | Loss: 0.00054016
Iteration 18/25 | Loss: 0.00054016
Iteration 19/25 | Loss: 0.00054016
Iteration 20/25 | Loss: 0.00054016
Iteration 21/25 | Loss: 0.00054016
Iteration 22/25 | Loss: 0.00054016
Iteration 23/25 | Loss: 0.00054016
Iteration 24/25 | Loss: 0.00054016
Iteration 25/25 | Loss: 0.00054016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054016
Iteration 2/1000 | Loss: 0.00050670
Iteration 3/1000 | Loss: 0.00004969
Iteration 4/1000 | Loss: 0.00036765
Iteration 5/1000 | Loss: 0.00076620
Iteration 6/1000 | Loss: 0.00025856
Iteration 7/1000 | Loss: 0.00027046
Iteration 8/1000 | Loss: 0.00023423
Iteration 9/1000 | Loss: 0.00021361
Iteration 10/1000 | Loss: 0.00019993
Iteration 11/1000 | Loss: 0.00020399
Iteration 12/1000 | Loss: 0.00123042
Iteration 13/1000 | Loss: 0.00024065
Iteration 14/1000 | Loss: 0.00017599
Iteration 15/1000 | Loss: 0.00006143
Iteration 16/1000 | Loss: 0.00008705
Iteration 17/1000 | Loss: 0.00005852
Iteration 18/1000 | Loss: 0.00004486
Iteration 19/1000 | Loss: 0.00006765
Iteration 20/1000 | Loss: 0.00040793
Iteration 21/1000 | Loss: 0.00026806
Iteration 22/1000 | Loss: 0.00017385
Iteration 23/1000 | Loss: 0.00005045
Iteration 24/1000 | Loss: 0.00024295
Iteration 25/1000 | Loss: 0.00033980
Iteration 26/1000 | Loss: 0.00006924
Iteration 27/1000 | Loss: 0.00004938
Iteration 28/1000 | Loss: 0.00004954
Iteration 29/1000 | Loss: 0.00005690
Iteration 30/1000 | Loss: 0.00004899
Iteration 31/1000 | Loss: 0.00004715
Iteration 32/1000 | Loss: 0.00004218
Iteration 33/1000 | Loss: 0.00004380
Iteration 34/1000 | Loss: 0.00003796
Iteration 35/1000 | Loss: 0.00003597
Iteration 36/1000 | Loss: 0.00003467
Iteration 37/1000 | Loss: 0.00003395
Iteration 38/1000 | Loss: 0.00003376
Iteration 39/1000 | Loss: 0.00003325
Iteration 40/1000 | Loss: 0.00003302
Iteration 41/1000 | Loss: 0.00003283
Iteration 42/1000 | Loss: 0.00003281
Iteration 43/1000 | Loss: 0.00003487
Iteration 44/1000 | Loss: 0.00003256
Iteration 45/1000 | Loss: 0.00003565
Iteration 46/1000 | Loss: 0.00003378
Iteration 47/1000 | Loss: 0.00003215
Iteration 48/1000 | Loss: 0.00003214
Iteration 49/1000 | Loss: 0.00003213
Iteration 50/1000 | Loss: 0.00003213
Iteration 51/1000 | Loss: 0.00003213
Iteration 52/1000 | Loss: 0.00003212
Iteration 53/1000 | Loss: 0.00003212
Iteration 54/1000 | Loss: 0.00003747
Iteration 55/1000 | Loss: 0.00003196
Iteration 56/1000 | Loss: 0.00003195
Iteration 57/1000 | Loss: 0.00003195
Iteration 58/1000 | Loss: 0.00003195
Iteration 59/1000 | Loss: 0.00003195
Iteration 60/1000 | Loss: 0.00003195
Iteration 61/1000 | Loss: 0.00003195
Iteration 62/1000 | Loss: 0.00003195
Iteration 63/1000 | Loss: 0.00003194
Iteration 64/1000 | Loss: 0.00003194
Iteration 65/1000 | Loss: 0.00003194
Iteration 66/1000 | Loss: 0.00003185
Iteration 67/1000 | Loss: 0.00003183
Iteration 68/1000 | Loss: 0.00003183
Iteration 69/1000 | Loss: 0.00003183
Iteration 70/1000 | Loss: 0.00003182
Iteration 71/1000 | Loss: 0.00003174
Iteration 72/1000 | Loss: 0.00003998
Iteration 73/1000 | Loss: 0.00007461
Iteration 74/1000 | Loss: 0.00003182
Iteration 75/1000 | Loss: 0.00004069
Iteration 76/1000 | Loss: 0.00003167
Iteration 77/1000 | Loss: 0.00003159
Iteration 78/1000 | Loss: 0.00003157
Iteration 79/1000 | Loss: 0.00003155
Iteration 80/1000 | Loss: 0.00003154
Iteration 81/1000 | Loss: 0.00003152
Iteration 82/1000 | Loss: 0.00003152
Iteration 83/1000 | Loss: 0.00003152
Iteration 84/1000 | Loss: 0.00003152
Iteration 85/1000 | Loss: 0.00003152
Iteration 86/1000 | Loss: 0.00003152
Iteration 87/1000 | Loss: 0.00003151
Iteration 88/1000 | Loss: 0.00003151
Iteration 89/1000 | Loss: 0.00003151
Iteration 90/1000 | Loss: 0.00003151
Iteration 91/1000 | Loss: 0.00003151
Iteration 92/1000 | Loss: 0.00003150
Iteration 93/1000 | Loss: 0.00003150
Iteration 94/1000 | Loss: 0.00003150
Iteration 95/1000 | Loss: 0.00003149
Iteration 96/1000 | Loss: 0.00003149
Iteration 97/1000 | Loss: 0.00003149
Iteration 98/1000 | Loss: 0.00003149
Iteration 99/1000 | Loss: 0.00003149
Iteration 100/1000 | Loss: 0.00003149
Iteration 101/1000 | Loss: 0.00003149
Iteration 102/1000 | Loss: 0.00003149
Iteration 103/1000 | Loss: 0.00003149
Iteration 104/1000 | Loss: 0.00003148
Iteration 105/1000 | Loss: 0.00003147
Iteration 106/1000 | Loss: 0.00003146
Iteration 107/1000 | Loss: 0.00003146
Iteration 108/1000 | Loss: 0.00003145
Iteration 109/1000 | Loss: 0.00003145
Iteration 110/1000 | Loss: 0.00003144
Iteration 111/1000 | Loss: 0.00003437
Iteration 112/1000 | Loss: 0.00003143
Iteration 113/1000 | Loss: 0.00003143
Iteration 114/1000 | Loss: 0.00003143
Iteration 115/1000 | Loss: 0.00003143
Iteration 116/1000 | Loss: 0.00003143
Iteration 117/1000 | Loss: 0.00003143
Iteration 118/1000 | Loss: 0.00003143
Iteration 119/1000 | Loss: 0.00003142
Iteration 120/1000 | Loss: 0.00003142
Iteration 121/1000 | Loss: 0.00003142
Iteration 122/1000 | Loss: 0.00003142
Iteration 123/1000 | Loss: 0.00003142
Iteration 124/1000 | Loss: 0.00003142
Iteration 125/1000 | Loss: 0.00003142
Iteration 126/1000 | Loss: 0.00003142
Iteration 127/1000 | Loss: 0.00003142
Iteration 128/1000 | Loss: 0.00003142
Iteration 129/1000 | Loss: 0.00003142
Iteration 130/1000 | Loss: 0.00003142
Iteration 131/1000 | Loss: 0.00003142
Iteration 132/1000 | Loss: 0.00003142
Iteration 133/1000 | Loss: 0.00003142
Iteration 134/1000 | Loss: 0.00003142
Iteration 135/1000 | Loss: 0.00003142
Iteration 136/1000 | Loss: 0.00003142
Iteration 137/1000 | Loss: 0.00003142
Iteration 138/1000 | Loss: 0.00003142
Iteration 139/1000 | Loss: 0.00003142
Iteration 140/1000 | Loss: 0.00003142
Iteration 141/1000 | Loss: 0.00003142
Iteration 142/1000 | Loss: 0.00003142
Iteration 143/1000 | Loss: 0.00003142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [3.141965135000646e-05, 3.141965135000646e-05, 3.141965135000646e-05, 3.141965135000646e-05, 3.141965135000646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.141965135000646e-05

Optimization complete. Final v2v error: 4.519683361053467 mm

Highest mean error: 5.220882892608643 mm for frame 63

Lowest mean error: 3.971376419067383 mm for frame 5

Saving results

Total time: 130.541161775589
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040028
Iteration 2/25 | Loss: 0.01040028
Iteration 3/25 | Loss: 0.01040028
Iteration 4/25 | Loss: 0.01040027
Iteration 5/25 | Loss: 0.01040027
Iteration 6/25 | Loss: 0.01040027
Iteration 7/25 | Loss: 0.01040027
Iteration 8/25 | Loss: 0.01040027
Iteration 9/25 | Loss: 0.01040026
Iteration 10/25 | Loss: 0.01040026
Iteration 11/25 | Loss: 0.01040026
Iteration 12/25 | Loss: 0.01040026
Iteration 13/25 | Loss: 0.01040026
Iteration 14/25 | Loss: 0.01040026
Iteration 15/25 | Loss: 0.01040026
Iteration 16/25 | Loss: 0.01040026
Iteration 17/25 | Loss: 0.01040026
Iteration 18/25 | Loss: 0.01040025
Iteration 19/25 | Loss: 0.01040025
Iteration 20/25 | Loss: 0.01040025
Iteration 21/25 | Loss: 0.01040025
Iteration 22/25 | Loss: 0.01040025
Iteration 23/25 | Loss: 0.01040025
Iteration 24/25 | Loss: 0.01040024
Iteration 25/25 | Loss: 0.01040024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52916563
Iteration 2/25 | Loss: 0.12358272
Iteration 3/25 | Loss: 0.12009436
Iteration 4/25 | Loss: 0.12406903
Iteration 5/25 | Loss: 0.12037253
Iteration 6/25 | Loss: 0.11891036
Iteration 7/25 | Loss: 0.11862414
Iteration 8/25 | Loss: 0.11870763
Iteration 9/25 | Loss: 0.11870502
Iteration 10/25 | Loss: 0.11856168
Iteration 11/25 | Loss: 0.11856168
Iteration 12/25 | Loss: 0.11856167
Iteration 13/25 | Loss: 0.11856167
Iteration 14/25 | Loss: 0.11856167
Iteration 15/25 | Loss: 0.11856167
Iteration 16/25 | Loss: 0.11856167
Iteration 17/25 | Loss: 0.11856167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.11856167018413544, 0.11856167018413544, 0.11856167018413544, 0.11856167018413544, 0.11856167018413544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11856167018413544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11856167
Iteration 2/1000 | Loss: 0.00326632
Iteration 3/1000 | Loss: 0.00087203
Iteration 4/1000 | Loss: 0.00088011
Iteration 5/1000 | Loss: 0.00199854
Iteration 6/1000 | Loss: 0.00048436
Iteration 7/1000 | Loss: 0.00118340
Iteration 8/1000 | Loss: 0.00009613
Iteration 9/1000 | Loss: 0.00006449
Iteration 10/1000 | Loss: 0.00010368
Iteration 11/1000 | Loss: 0.00003277
Iteration 12/1000 | Loss: 0.00002888
Iteration 13/1000 | Loss: 0.00002562
Iteration 14/1000 | Loss: 0.00012011
Iteration 15/1000 | Loss: 0.00002313
Iteration 16/1000 | Loss: 0.00006963
Iteration 17/1000 | Loss: 0.00002128
Iteration 18/1000 | Loss: 0.00013553
Iteration 19/1000 | Loss: 0.00001988
Iteration 20/1000 | Loss: 0.00001918
Iteration 21/1000 | Loss: 0.00001856
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001741
Iteration 24/1000 | Loss: 0.00001696
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001622
Iteration 27/1000 | Loss: 0.00001594
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001511
Iteration 31/1000 | Loss: 0.00001489
Iteration 32/1000 | Loss: 0.00001480
Iteration 33/1000 | Loss: 0.00001476
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001469
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001465
Iteration 43/1000 | Loss: 0.00001463
Iteration 44/1000 | Loss: 0.00001462
Iteration 45/1000 | Loss: 0.00001462
Iteration 46/1000 | Loss: 0.00001461
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001457
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001456
Iteration 53/1000 | Loss: 0.00001452
Iteration 54/1000 | Loss: 0.00001452
Iteration 55/1000 | Loss: 0.00001451
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001450
Iteration 60/1000 | Loss: 0.00001450
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001449
Iteration 65/1000 | Loss: 0.00001449
Iteration 66/1000 | Loss: 0.00001449
Iteration 67/1000 | Loss: 0.00001449
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001446
Iteration 75/1000 | Loss: 0.00001446
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001442
Iteration 107/1000 | Loss: 0.00001442
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001440
Iteration 119/1000 | Loss: 0.00001440
Iteration 120/1000 | Loss: 0.00001440
Iteration 121/1000 | Loss: 0.00001440
Iteration 122/1000 | Loss: 0.00001440
Iteration 123/1000 | Loss: 0.00001440
Iteration 124/1000 | Loss: 0.00001440
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001440
Iteration 127/1000 | Loss: 0.00001439
Iteration 128/1000 | Loss: 0.00001439
Iteration 129/1000 | Loss: 0.00001439
Iteration 130/1000 | Loss: 0.00001439
Iteration 131/1000 | Loss: 0.00001439
Iteration 132/1000 | Loss: 0.00001439
Iteration 133/1000 | Loss: 0.00001439
Iteration 134/1000 | Loss: 0.00001439
Iteration 135/1000 | Loss: 0.00001439
Iteration 136/1000 | Loss: 0.00001439
Iteration 137/1000 | Loss: 0.00001438
Iteration 138/1000 | Loss: 0.00001438
Iteration 139/1000 | Loss: 0.00001438
Iteration 140/1000 | Loss: 0.00001438
Iteration 141/1000 | Loss: 0.00001438
Iteration 142/1000 | Loss: 0.00001438
Iteration 143/1000 | Loss: 0.00001438
Iteration 144/1000 | Loss: 0.00001438
Iteration 145/1000 | Loss: 0.00001438
Iteration 146/1000 | Loss: 0.00001438
Iteration 147/1000 | Loss: 0.00001438
Iteration 148/1000 | Loss: 0.00001438
Iteration 149/1000 | Loss: 0.00001438
Iteration 150/1000 | Loss: 0.00001438
Iteration 151/1000 | Loss: 0.00001438
Iteration 152/1000 | Loss: 0.00001437
Iteration 153/1000 | Loss: 0.00001437
Iteration 154/1000 | Loss: 0.00001437
Iteration 155/1000 | Loss: 0.00001437
Iteration 156/1000 | Loss: 0.00001437
Iteration 157/1000 | Loss: 0.00001437
Iteration 158/1000 | Loss: 0.00001437
Iteration 159/1000 | Loss: 0.00001437
Iteration 160/1000 | Loss: 0.00001437
Iteration 161/1000 | Loss: 0.00001437
Iteration 162/1000 | Loss: 0.00001437
Iteration 163/1000 | Loss: 0.00001437
Iteration 164/1000 | Loss: 0.00001437
Iteration 165/1000 | Loss: 0.00001437
Iteration 166/1000 | Loss: 0.00001437
Iteration 167/1000 | Loss: 0.00001437
Iteration 168/1000 | Loss: 0.00001437
Iteration 169/1000 | Loss: 0.00001437
Iteration 170/1000 | Loss: 0.00001437
Iteration 171/1000 | Loss: 0.00001437
Iteration 172/1000 | Loss: 0.00001436
Iteration 173/1000 | Loss: 0.00001436
Iteration 174/1000 | Loss: 0.00001436
Iteration 175/1000 | Loss: 0.00001436
Iteration 176/1000 | Loss: 0.00001436
Iteration 177/1000 | Loss: 0.00001436
Iteration 178/1000 | Loss: 0.00001436
Iteration 179/1000 | Loss: 0.00001436
Iteration 180/1000 | Loss: 0.00001436
Iteration 181/1000 | Loss: 0.00001436
Iteration 182/1000 | Loss: 0.00001436
Iteration 183/1000 | Loss: 0.00001436
Iteration 184/1000 | Loss: 0.00001436
Iteration 185/1000 | Loss: 0.00001436
Iteration 186/1000 | Loss: 0.00001435
Iteration 187/1000 | Loss: 0.00001435
Iteration 188/1000 | Loss: 0.00001435
Iteration 189/1000 | Loss: 0.00001435
Iteration 190/1000 | Loss: 0.00001435
Iteration 191/1000 | Loss: 0.00001435
Iteration 192/1000 | Loss: 0.00001435
Iteration 193/1000 | Loss: 0.00001435
Iteration 194/1000 | Loss: 0.00001435
Iteration 195/1000 | Loss: 0.00001435
Iteration 196/1000 | Loss: 0.00001435
Iteration 197/1000 | Loss: 0.00001435
Iteration 198/1000 | Loss: 0.00001435
Iteration 199/1000 | Loss: 0.00001435
Iteration 200/1000 | Loss: 0.00001435
Iteration 201/1000 | Loss: 0.00001435
Iteration 202/1000 | Loss: 0.00001435
Iteration 203/1000 | Loss: 0.00001435
Iteration 204/1000 | Loss: 0.00001435
Iteration 205/1000 | Loss: 0.00001435
Iteration 206/1000 | Loss: 0.00001435
Iteration 207/1000 | Loss: 0.00001435
Iteration 208/1000 | Loss: 0.00001435
Iteration 209/1000 | Loss: 0.00001435
Iteration 210/1000 | Loss: 0.00001435
Iteration 211/1000 | Loss: 0.00001435
Iteration 212/1000 | Loss: 0.00001435
Iteration 213/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.4349015145853627e-05, 1.4349015145853627e-05, 1.4349015145853627e-05, 1.4349015145853627e-05, 1.4349015145853627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4349015145853627e-05

Optimization complete. Final v2v error: 3.2568295001983643 mm

Highest mean error: 3.6019527912139893 mm for frame 239

Lowest mean error: 2.8640074729919434 mm for frame 56

Saving results

Total time: 82.49272179603577
