Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=34, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1904-1959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785427
Iteration 2/25 | Loss: 0.00120533
Iteration 3/25 | Loss: 0.00073977
Iteration 4/25 | Loss: 0.00067410
Iteration 5/25 | Loss: 0.00066184
Iteration 6/25 | Loss: 0.00065962
Iteration 7/25 | Loss: 0.00065960
Iteration 8/25 | Loss: 0.00065960
Iteration 9/25 | Loss: 0.00065960
Iteration 10/25 | Loss: 0.00065960
Iteration 11/25 | Loss: 0.00065960
Iteration 12/25 | Loss: 0.00065960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006596019375137985, 0.0006596019375137985, 0.0006596019375137985, 0.0006596019375137985, 0.0006596019375137985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006596019375137985

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44591224
Iteration 2/25 | Loss: 0.00029072
Iteration 3/25 | Loss: 0.00029071
Iteration 4/25 | Loss: 0.00029071
Iteration 5/25 | Loss: 0.00029071
Iteration 6/25 | Loss: 0.00029071
Iteration 7/25 | Loss: 0.00029071
Iteration 8/25 | Loss: 0.00029071
Iteration 9/25 | Loss: 0.00029071
Iteration 10/25 | Loss: 0.00029071
Iteration 11/25 | Loss: 0.00029071
Iteration 12/25 | Loss: 0.00029071
Iteration 13/25 | Loss: 0.00029071
Iteration 14/25 | Loss: 0.00029071
Iteration 15/25 | Loss: 0.00029071
Iteration 16/25 | Loss: 0.00029071
Iteration 17/25 | Loss: 0.00029071
Iteration 18/25 | Loss: 0.00029071
Iteration 19/25 | Loss: 0.00029071
Iteration 20/25 | Loss: 0.00029071
Iteration 21/25 | Loss: 0.00029071
Iteration 22/25 | Loss: 0.00029071
Iteration 23/25 | Loss: 0.00029071
Iteration 24/25 | Loss: 0.00029071
Iteration 25/25 | Loss: 0.00029071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029071
Iteration 2/1000 | Loss: 0.00002141
Iteration 3/1000 | Loss: 0.00001654
Iteration 4/1000 | Loss: 0.00001536
Iteration 5/1000 | Loss: 0.00001481
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001368
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001345
Iteration 14/1000 | Loss: 0.00001343
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001341
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001340
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001339
Iteration 25/1000 | Loss: 0.00001339
Iteration 26/1000 | Loss: 0.00001338
Iteration 27/1000 | Loss: 0.00001338
Iteration 28/1000 | Loss: 0.00001337
Iteration 29/1000 | Loss: 0.00001337
Iteration 30/1000 | Loss: 0.00001337
Iteration 31/1000 | Loss: 0.00001337
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001336
Iteration 35/1000 | Loss: 0.00001336
Iteration 36/1000 | Loss: 0.00001336
Iteration 37/1000 | Loss: 0.00001335
Iteration 38/1000 | Loss: 0.00001335
Iteration 39/1000 | Loss: 0.00001335
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001330
Iteration 42/1000 | Loss: 0.00001329
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001327
Iteration 45/1000 | Loss: 0.00001327
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001326
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001326
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001325
Iteration 52/1000 | Loss: 0.00001325
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001323
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001322
Iteration 60/1000 | Loss: 0.00001322
Iteration 61/1000 | Loss: 0.00001322
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001320
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001320
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001320
Iteration 81/1000 | Loss: 0.00001320
Iteration 82/1000 | Loss: 0.00001320
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001320
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.3197827684052754e-05, 1.3197827684052754e-05, 1.3197827684052754e-05, 1.3197827684052754e-05, 1.3197827684052754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3197827684052754e-05

Optimization complete. Final v2v error: 3.1088385581970215 mm

Highest mean error: 3.342085599899292 mm for frame 0

Lowest mean error: 2.9648423194885254 mm for frame 180

Saving results

Total time: 50.90544629096985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450794
Iteration 2/25 | Loss: 0.00080946
Iteration 3/25 | Loss: 0.00067833
Iteration 4/25 | Loss: 0.00066448
Iteration 5/25 | Loss: 0.00065932
Iteration 6/25 | Loss: 0.00065729
Iteration 7/25 | Loss: 0.00065687
Iteration 8/25 | Loss: 0.00065687
Iteration 9/25 | Loss: 0.00065687
Iteration 10/25 | Loss: 0.00065687
Iteration 11/25 | Loss: 0.00065687
Iteration 12/25 | Loss: 0.00065687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006568719982169569, 0.0006568719982169569, 0.0006568719982169569, 0.0006568719982169569, 0.0006568719982169569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006568719982169569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47747922
Iteration 2/25 | Loss: 0.00032544
Iteration 3/25 | Loss: 0.00032544
Iteration 4/25 | Loss: 0.00032544
Iteration 5/25 | Loss: 0.00032544
Iteration 6/25 | Loss: 0.00032544
Iteration 7/25 | Loss: 0.00032544
Iteration 8/25 | Loss: 0.00032544
Iteration 9/25 | Loss: 0.00032544
Iteration 10/25 | Loss: 0.00032544
Iteration 11/25 | Loss: 0.00032544
Iteration 12/25 | Loss: 0.00032544
Iteration 13/25 | Loss: 0.00032544
Iteration 14/25 | Loss: 0.00032544
Iteration 15/25 | Loss: 0.00032544
Iteration 16/25 | Loss: 0.00032544
Iteration 17/25 | Loss: 0.00032544
Iteration 18/25 | Loss: 0.00032544
Iteration 19/25 | Loss: 0.00032544
Iteration 20/25 | Loss: 0.00032544
Iteration 21/25 | Loss: 0.00032544
Iteration 22/25 | Loss: 0.00032544
Iteration 23/25 | Loss: 0.00032544
Iteration 24/25 | Loss: 0.00032544
Iteration 25/25 | Loss: 0.00032544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032544
Iteration 2/1000 | Loss: 0.00002902
Iteration 3/1000 | Loss: 0.00001866
Iteration 4/1000 | Loss: 0.00001692
Iteration 5/1000 | Loss: 0.00001595
Iteration 6/1000 | Loss: 0.00001543
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00001475
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001453
Iteration 14/1000 | Loss: 0.00001448
Iteration 15/1000 | Loss: 0.00001444
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001443
Iteration 19/1000 | Loss: 0.00001443
Iteration 20/1000 | Loss: 0.00001442
Iteration 21/1000 | Loss: 0.00001442
Iteration 22/1000 | Loss: 0.00001441
Iteration 23/1000 | Loss: 0.00001440
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001439
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001438
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001437
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001435
Iteration 47/1000 | Loss: 0.00001435
Iteration 48/1000 | Loss: 0.00001435
Iteration 49/1000 | Loss: 0.00001434
Iteration 50/1000 | Loss: 0.00001434
Iteration 51/1000 | Loss: 0.00001434
Iteration 52/1000 | Loss: 0.00001434
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001434
Iteration 55/1000 | Loss: 0.00001434
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001434
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001433
Iteration 60/1000 | Loss: 0.00001433
Iteration 61/1000 | Loss: 0.00001433
Iteration 62/1000 | Loss: 0.00001433
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001433
Iteration 65/1000 | Loss: 0.00001433
Iteration 66/1000 | Loss: 0.00001433
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001430
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001428
Iteration 93/1000 | Loss: 0.00001428
Iteration 94/1000 | Loss: 0.00001428
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001427
Iteration 100/1000 | Loss: 0.00001427
Iteration 101/1000 | Loss: 0.00001427
Iteration 102/1000 | Loss: 0.00001427
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001426
Iteration 110/1000 | Loss: 0.00001426
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001424
Iteration 119/1000 | Loss: 0.00001424
Iteration 120/1000 | Loss: 0.00001424
Iteration 121/1000 | Loss: 0.00001424
Iteration 122/1000 | Loss: 0.00001424
Iteration 123/1000 | Loss: 0.00001424
Iteration 124/1000 | Loss: 0.00001424
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001423
Iteration 131/1000 | Loss: 0.00001423
Iteration 132/1000 | Loss: 0.00001423
Iteration 133/1000 | Loss: 0.00001423
Iteration 134/1000 | Loss: 0.00001423
Iteration 135/1000 | Loss: 0.00001423
Iteration 136/1000 | Loss: 0.00001423
Iteration 137/1000 | Loss: 0.00001423
Iteration 138/1000 | Loss: 0.00001423
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001422
Iteration 141/1000 | Loss: 0.00001422
Iteration 142/1000 | Loss: 0.00001422
Iteration 143/1000 | Loss: 0.00001422
Iteration 144/1000 | Loss: 0.00001422
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001421
Iteration 147/1000 | Loss: 0.00001421
Iteration 148/1000 | Loss: 0.00001421
Iteration 149/1000 | Loss: 0.00001421
Iteration 150/1000 | Loss: 0.00001421
Iteration 151/1000 | Loss: 0.00001421
Iteration 152/1000 | Loss: 0.00001421
Iteration 153/1000 | Loss: 0.00001421
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001420
Iteration 158/1000 | Loss: 0.00001420
Iteration 159/1000 | Loss: 0.00001420
Iteration 160/1000 | Loss: 0.00001420
Iteration 161/1000 | Loss: 0.00001420
Iteration 162/1000 | Loss: 0.00001420
Iteration 163/1000 | Loss: 0.00001420
Iteration 164/1000 | Loss: 0.00001419
Iteration 165/1000 | Loss: 0.00001419
Iteration 166/1000 | Loss: 0.00001419
Iteration 167/1000 | Loss: 0.00001419
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001419
Iteration 170/1000 | Loss: 0.00001419
Iteration 171/1000 | Loss: 0.00001419
Iteration 172/1000 | Loss: 0.00001419
Iteration 173/1000 | Loss: 0.00001419
Iteration 174/1000 | Loss: 0.00001419
Iteration 175/1000 | Loss: 0.00001419
Iteration 176/1000 | Loss: 0.00001419
Iteration 177/1000 | Loss: 0.00001419
Iteration 178/1000 | Loss: 0.00001419
Iteration 179/1000 | Loss: 0.00001418
Iteration 180/1000 | Loss: 0.00001418
Iteration 181/1000 | Loss: 0.00001418
Iteration 182/1000 | Loss: 0.00001418
Iteration 183/1000 | Loss: 0.00001418
Iteration 184/1000 | Loss: 0.00001418
Iteration 185/1000 | Loss: 0.00001418
Iteration 186/1000 | Loss: 0.00001418
Iteration 187/1000 | Loss: 0.00001418
Iteration 188/1000 | Loss: 0.00001418
Iteration 189/1000 | Loss: 0.00001418
Iteration 190/1000 | Loss: 0.00001418
Iteration 191/1000 | Loss: 0.00001418
Iteration 192/1000 | Loss: 0.00001418
Iteration 193/1000 | Loss: 0.00001418
Iteration 194/1000 | Loss: 0.00001418
Iteration 195/1000 | Loss: 0.00001418
Iteration 196/1000 | Loss: 0.00001418
Iteration 197/1000 | Loss: 0.00001418
Iteration 198/1000 | Loss: 0.00001418
Iteration 199/1000 | Loss: 0.00001418
Iteration 200/1000 | Loss: 0.00001418
Iteration 201/1000 | Loss: 0.00001417
Iteration 202/1000 | Loss: 0.00001417
Iteration 203/1000 | Loss: 0.00001417
Iteration 204/1000 | Loss: 0.00001417
Iteration 205/1000 | Loss: 0.00001417
Iteration 206/1000 | Loss: 0.00001417
Iteration 207/1000 | Loss: 0.00001417
Iteration 208/1000 | Loss: 0.00001417
Iteration 209/1000 | Loss: 0.00001417
Iteration 210/1000 | Loss: 0.00001417
Iteration 211/1000 | Loss: 0.00001417
Iteration 212/1000 | Loss: 0.00001417
Iteration 213/1000 | Loss: 0.00001417
Iteration 214/1000 | Loss: 0.00001417
Iteration 215/1000 | Loss: 0.00001417
Iteration 216/1000 | Loss: 0.00001417
Iteration 217/1000 | Loss: 0.00001417
Iteration 218/1000 | Loss: 0.00001417
Iteration 219/1000 | Loss: 0.00001417
Iteration 220/1000 | Loss: 0.00001417
Iteration 221/1000 | Loss: 0.00001417
Iteration 222/1000 | Loss: 0.00001417
Iteration 223/1000 | Loss: 0.00001417
Iteration 224/1000 | Loss: 0.00001417
Iteration 225/1000 | Loss: 0.00001417
Iteration 226/1000 | Loss: 0.00001417
Iteration 227/1000 | Loss: 0.00001417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.4166542314342223e-05, 1.4166542314342223e-05, 1.4166542314342223e-05, 1.4166542314342223e-05, 1.4166542314342223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4166542314342223e-05

Optimization complete. Final v2v error: 3.162163734436035 mm

Highest mean error: 3.540606737136841 mm for frame 6

Lowest mean error: 2.5498359203338623 mm for frame 147

Saving results

Total time: 41.3163948059082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803491
Iteration 2/25 | Loss: 0.00115966
Iteration 3/25 | Loss: 0.00074707
Iteration 4/25 | Loss: 0.00068394
Iteration 5/25 | Loss: 0.00067065
Iteration 6/25 | Loss: 0.00066762
Iteration 7/25 | Loss: 0.00066748
Iteration 8/25 | Loss: 0.00066748
Iteration 9/25 | Loss: 0.00066748
Iteration 10/25 | Loss: 0.00066748
Iteration 11/25 | Loss: 0.00066748
Iteration 12/25 | Loss: 0.00066748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006674822070635855, 0.0006674822070635855, 0.0006674822070635855, 0.0006674822070635855, 0.0006674822070635855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006674822070635855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46155834
Iteration 2/25 | Loss: 0.00027983
Iteration 3/25 | Loss: 0.00027982
Iteration 4/25 | Loss: 0.00027982
Iteration 5/25 | Loss: 0.00027982
Iteration 6/25 | Loss: 0.00027982
Iteration 7/25 | Loss: 0.00027982
Iteration 8/25 | Loss: 0.00027982
Iteration 9/25 | Loss: 0.00027982
Iteration 10/25 | Loss: 0.00027982
Iteration 11/25 | Loss: 0.00027982
Iteration 12/25 | Loss: 0.00027982
Iteration 13/25 | Loss: 0.00027982
Iteration 14/25 | Loss: 0.00027982
Iteration 15/25 | Loss: 0.00027982
Iteration 16/25 | Loss: 0.00027982
Iteration 17/25 | Loss: 0.00027982
Iteration 18/25 | Loss: 0.00027982
Iteration 19/25 | Loss: 0.00027982
Iteration 20/25 | Loss: 0.00027982
Iteration 21/25 | Loss: 0.00027982
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00027981988387182355, 0.00027981988387182355, 0.00027981988387182355, 0.00027981988387182355, 0.00027981988387182355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027981988387182355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027982
Iteration 2/1000 | Loss: 0.00003049
Iteration 3/1000 | Loss: 0.00002378
Iteration 4/1000 | Loss: 0.00002189
Iteration 5/1000 | Loss: 0.00002071
Iteration 6/1000 | Loss: 0.00002015
Iteration 7/1000 | Loss: 0.00001937
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001874
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001840
Iteration 15/1000 | Loss: 0.00001832
Iteration 16/1000 | Loss: 0.00001831
Iteration 17/1000 | Loss: 0.00001824
Iteration 18/1000 | Loss: 0.00001822
Iteration 19/1000 | Loss: 0.00001822
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001822
Iteration 22/1000 | Loss: 0.00001822
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001821
Iteration 27/1000 | Loss: 0.00001821
Iteration 28/1000 | Loss: 0.00001821
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001821
Iteration 32/1000 | Loss: 0.00001820
Iteration 33/1000 | Loss: 0.00001820
Iteration 34/1000 | Loss: 0.00001820
Iteration 35/1000 | Loss: 0.00001820
Iteration 36/1000 | Loss: 0.00001820
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001819
Iteration 44/1000 | Loss: 0.00001819
Iteration 45/1000 | Loss: 0.00001819
Iteration 46/1000 | Loss: 0.00001819
Iteration 47/1000 | Loss: 0.00001819
Iteration 48/1000 | Loss: 0.00001818
Iteration 49/1000 | Loss: 0.00001818
Iteration 50/1000 | Loss: 0.00001818
Iteration 51/1000 | Loss: 0.00001818
Iteration 52/1000 | Loss: 0.00001817
Iteration 53/1000 | Loss: 0.00001817
Iteration 54/1000 | Loss: 0.00001817
Iteration 55/1000 | Loss: 0.00001817
Iteration 56/1000 | Loss: 0.00001817
Iteration 57/1000 | Loss: 0.00001817
Iteration 58/1000 | Loss: 0.00001816
Iteration 59/1000 | Loss: 0.00001816
Iteration 60/1000 | Loss: 0.00001816
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001815
Iteration 63/1000 | Loss: 0.00001815
Iteration 64/1000 | Loss: 0.00001815
Iteration 65/1000 | Loss: 0.00001815
Iteration 66/1000 | Loss: 0.00001815
Iteration 67/1000 | Loss: 0.00001814
Iteration 68/1000 | Loss: 0.00001814
Iteration 69/1000 | Loss: 0.00001814
Iteration 70/1000 | Loss: 0.00001814
Iteration 71/1000 | Loss: 0.00001814
Iteration 72/1000 | Loss: 0.00001814
Iteration 73/1000 | Loss: 0.00001814
Iteration 74/1000 | Loss: 0.00001814
Iteration 75/1000 | Loss: 0.00001813
Iteration 76/1000 | Loss: 0.00001813
Iteration 77/1000 | Loss: 0.00001813
Iteration 78/1000 | Loss: 0.00001813
Iteration 79/1000 | Loss: 0.00001813
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001813
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001813
Iteration 84/1000 | Loss: 0.00001813
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001813
Iteration 87/1000 | Loss: 0.00001813
Iteration 88/1000 | Loss: 0.00001813
Iteration 89/1000 | Loss: 0.00001813
Iteration 90/1000 | Loss: 0.00001813
Iteration 91/1000 | Loss: 0.00001813
Iteration 92/1000 | Loss: 0.00001813
Iteration 93/1000 | Loss: 0.00001813
Iteration 94/1000 | Loss: 0.00001813
Iteration 95/1000 | Loss: 0.00001813
Iteration 96/1000 | Loss: 0.00001813
Iteration 97/1000 | Loss: 0.00001813
Iteration 98/1000 | Loss: 0.00001813
Iteration 99/1000 | Loss: 0.00001813
Iteration 100/1000 | Loss: 0.00001813
Iteration 101/1000 | Loss: 0.00001813
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001813
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001813
Iteration 108/1000 | Loss: 0.00001813
Iteration 109/1000 | Loss: 0.00001813
Iteration 110/1000 | Loss: 0.00001813
Iteration 111/1000 | Loss: 0.00001813
Iteration 112/1000 | Loss: 0.00001813
Iteration 113/1000 | Loss: 0.00001813
Iteration 114/1000 | Loss: 0.00001813
Iteration 115/1000 | Loss: 0.00001813
Iteration 116/1000 | Loss: 0.00001813
Iteration 117/1000 | Loss: 0.00001813
Iteration 118/1000 | Loss: 0.00001813
Iteration 119/1000 | Loss: 0.00001813
Iteration 120/1000 | Loss: 0.00001813
Iteration 121/1000 | Loss: 0.00001813
Iteration 122/1000 | Loss: 0.00001813
Iteration 123/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.8130542230210267e-05, 1.8130542230210267e-05, 1.8130542230210267e-05, 1.8130542230210267e-05, 1.8130542230210267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8130542230210267e-05

Optimization complete. Final v2v error: 3.579266309738159 mm

Highest mean error: 3.9300215244293213 mm for frame 93

Lowest mean error: 3.3921759128570557 mm for frame 62

Saving results

Total time: 122.80403327941895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149161
Iteration 2/25 | Loss: 0.00190302
Iteration 3/25 | Loss: 0.00092308
Iteration 4/25 | Loss: 0.00080633
Iteration 5/25 | Loss: 0.00080098
Iteration 6/25 | Loss: 0.00080838
Iteration 7/25 | Loss: 0.00078691
Iteration 8/25 | Loss: 0.00078954
Iteration 9/25 | Loss: 0.00078901
Iteration 10/25 | Loss: 0.00078255
Iteration 11/25 | Loss: 0.00078569
Iteration 12/25 | Loss: 0.00078147
Iteration 13/25 | Loss: 0.00078139
Iteration 14/25 | Loss: 0.00077770
Iteration 15/25 | Loss: 0.00077795
Iteration 16/25 | Loss: 0.00077322
Iteration 17/25 | Loss: 0.00077495
Iteration 18/25 | Loss: 0.00077762
Iteration 19/25 | Loss: 0.00077391
Iteration 20/25 | Loss: 0.00077349
Iteration 21/25 | Loss: 0.00079853
Iteration 22/25 | Loss: 0.00077838
Iteration 23/25 | Loss: 0.00077495
Iteration 24/25 | Loss: 0.00077579
Iteration 25/25 | Loss: 0.00077141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47045326
Iteration 2/25 | Loss: 0.00079828
Iteration 3/25 | Loss: 0.00079828
Iteration 4/25 | Loss: 0.00079828
Iteration 5/25 | Loss: 0.00079828
Iteration 6/25 | Loss: 0.00079828
Iteration 7/25 | Loss: 0.00079828
Iteration 8/25 | Loss: 0.00079827
Iteration 9/25 | Loss: 0.00079827
Iteration 10/25 | Loss: 0.00079827
Iteration 11/25 | Loss: 0.00079827
Iteration 12/25 | Loss: 0.00079827
Iteration 13/25 | Loss: 0.00079827
Iteration 14/25 | Loss: 0.00079827
Iteration 15/25 | Loss: 0.00079827
Iteration 16/25 | Loss: 0.00079827
Iteration 17/25 | Loss: 0.00079827
Iteration 18/25 | Loss: 0.00079827
Iteration 19/25 | Loss: 0.00079827
Iteration 20/25 | Loss: 0.00079827
Iteration 21/25 | Loss: 0.00079827
Iteration 22/25 | Loss: 0.00079827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007982743554748595, 0.0007982743554748595, 0.0007982743554748595, 0.0007982743554748595, 0.0007982743554748595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007982743554748595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079827
Iteration 2/1000 | Loss: 0.00004407
Iteration 3/1000 | Loss: 0.00002924
Iteration 4/1000 | Loss: 0.00002790
Iteration 5/1000 | Loss: 0.00002467
Iteration 6/1000 | Loss: 0.00002765
Iteration 7/1000 | Loss: 0.00002288
Iteration 8/1000 | Loss: 0.00002190
Iteration 9/1000 | Loss: 0.00002139
Iteration 10/1000 | Loss: 0.00002068
Iteration 11/1000 | Loss: 0.00002010
Iteration 12/1000 | Loss: 0.00001979
Iteration 13/1000 | Loss: 0.00001948
Iteration 14/1000 | Loss: 0.00001921
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001895
Iteration 19/1000 | Loss: 0.00001895
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001893
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001881
Iteration 24/1000 | Loss: 0.00001879
Iteration 25/1000 | Loss: 0.00001877
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001859
Iteration 30/1000 | Loss: 0.00001859
Iteration 31/1000 | Loss: 0.00001858
Iteration 32/1000 | Loss: 0.00001856
Iteration 33/1000 | Loss: 0.00001855
Iteration 34/1000 | Loss: 0.00001855
Iteration 35/1000 | Loss: 0.00001855
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001854
Iteration 40/1000 | Loss: 0.00001854
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001852
Iteration 46/1000 | Loss: 0.00001852
Iteration 47/1000 | Loss: 0.00001852
Iteration 48/1000 | Loss: 0.00001852
Iteration 49/1000 | Loss: 0.00001852
Iteration 50/1000 | Loss: 0.00001852
Iteration 51/1000 | Loss: 0.00001852
Iteration 52/1000 | Loss: 0.00001851
Iteration 53/1000 | Loss: 0.00001851
Iteration 54/1000 | Loss: 0.00001851
Iteration 55/1000 | Loss: 0.00001851
Iteration 56/1000 | Loss: 0.00001850
Iteration 57/1000 | Loss: 0.00001850
Iteration 58/1000 | Loss: 0.00001850
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00001849
Iteration 61/1000 | Loss: 0.00001849
Iteration 62/1000 | Loss: 0.00001849
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001849
Iteration 67/1000 | Loss: 0.00001849
Iteration 68/1000 | Loss: 0.00001848
Iteration 69/1000 | Loss: 0.00001848
Iteration 70/1000 | Loss: 0.00001848
Iteration 71/1000 | Loss: 0.00001848
Iteration 72/1000 | Loss: 0.00001848
Iteration 73/1000 | Loss: 0.00001848
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001847
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001847
Iteration 85/1000 | Loss: 0.00001847
Iteration 86/1000 | Loss: 0.00001847
Iteration 87/1000 | Loss: 0.00001847
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001847
Iteration 91/1000 | Loss: 0.00001847
Iteration 92/1000 | Loss: 0.00001847
Iteration 93/1000 | Loss: 0.00001847
Iteration 94/1000 | Loss: 0.00001847
Iteration 95/1000 | Loss: 0.00001847
Iteration 96/1000 | Loss: 0.00001847
Iteration 97/1000 | Loss: 0.00001847
Iteration 98/1000 | Loss: 0.00001847
Iteration 99/1000 | Loss: 0.00001847
Iteration 100/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.8468714188202284e-05, 1.8468714188202284e-05, 1.8468714188202284e-05, 1.8468714188202284e-05, 1.8468714188202284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8468714188202284e-05

Optimization complete. Final v2v error: 3.4430294036865234 mm

Highest mean error: 4.480906963348389 mm for frame 87

Lowest mean error: 2.8615612983703613 mm for frame 153

Saving results

Total time: 120.27220153808594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822038
Iteration 2/25 | Loss: 0.00078152
Iteration 3/25 | Loss: 0.00062994
Iteration 4/25 | Loss: 0.00060300
Iteration 5/25 | Loss: 0.00059872
Iteration 6/25 | Loss: 0.00059790
Iteration 7/25 | Loss: 0.00059790
Iteration 8/25 | Loss: 0.00059790
Iteration 9/25 | Loss: 0.00059790
Iteration 10/25 | Loss: 0.00059790
Iteration 11/25 | Loss: 0.00059790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005978966946713626, 0.0005978966946713626, 0.0005978966946713626, 0.0005978966946713626, 0.0005978966946713626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005978966946713626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46128690
Iteration 2/25 | Loss: 0.00027786
Iteration 3/25 | Loss: 0.00027786
Iteration 4/25 | Loss: 0.00027786
Iteration 5/25 | Loss: 0.00027786
Iteration 6/25 | Loss: 0.00027786
Iteration 7/25 | Loss: 0.00027786
Iteration 8/25 | Loss: 0.00027786
Iteration 9/25 | Loss: 0.00027786
Iteration 10/25 | Loss: 0.00027786
Iteration 11/25 | Loss: 0.00027786
Iteration 12/25 | Loss: 0.00027786
Iteration 13/25 | Loss: 0.00027786
Iteration 14/25 | Loss: 0.00027786
Iteration 15/25 | Loss: 0.00027786
Iteration 16/25 | Loss: 0.00027786
Iteration 17/25 | Loss: 0.00027786
Iteration 18/25 | Loss: 0.00027786
Iteration 19/25 | Loss: 0.00027786
Iteration 20/25 | Loss: 0.00027786
Iteration 21/25 | Loss: 0.00027786
Iteration 22/25 | Loss: 0.00027786
Iteration 23/25 | Loss: 0.00027786
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002778574707917869, 0.0002778574707917869, 0.0002778574707917869, 0.0002778574707917869, 0.0002778574707917869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002778574707917869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027786
Iteration 2/1000 | Loss: 0.00002201
Iteration 3/1000 | Loss: 0.00001302
Iteration 4/1000 | Loss: 0.00001221
Iteration 5/1000 | Loss: 0.00001146
Iteration 6/1000 | Loss: 0.00001114
Iteration 7/1000 | Loss: 0.00001086
Iteration 8/1000 | Loss: 0.00001073
Iteration 9/1000 | Loss: 0.00001072
Iteration 10/1000 | Loss: 0.00001069
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001068
Iteration 13/1000 | Loss: 0.00001068
Iteration 14/1000 | Loss: 0.00001068
Iteration 15/1000 | Loss: 0.00001063
Iteration 16/1000 | Loss: 0.00001062
Iteration 17/1000 | Loss: 0.00001060
Iteration 18/1000 | Loss: 0.00001060
Iteration 19/1000 | Loss: 0.00001060
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001060
Iteration 22/1000 | Loss: 0.00001060
Iteration 23/1000 | Loss: 0.00001059
Iteration 24/1000 | Loss: 0.00001058
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001058
Iteration 27/1000 | Loss: 0.00001058
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001057
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001056
Iteration 32/1000 | Loss: 0.00001056
Iteration 33/1000 | Loss: 0.00001051
Iteration 34/1000 | Loss: 0.00001050
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001048
Iteration 37/1000 | Loss: 0.00001048
Iteration 38/1000 | Loss: 0.00001044
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001040
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001039
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001039
Iteration 50/1000 | Loss: 0.00001039
Iteration 51/1000 | Loss: 0.00001039
Iteration 52/1000 | Loss: 0.00001038
Iteration 53/1000 | Loss: 0.00001038
Iteration 54/1000 | Loss: 0.00001038
Iteration 55/1000 | Loss: 0.00001037
Iteration 56/1000 | Loss: 0.00001035
Iteration 57/1000 | Loss: 0.00001034
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001032
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001031
Iteration 66/1000 | Loss: 0.00001030
Iteration 67/1000 | Loss: 0.00001030
Iteration 68/1000 | Loss: 0.00001030
Iteration 69/1000 | Loss: 0.00001030
Iteration 70/1000 | Loss: 0.00001030
Iteration 71/1000 | Loss: 0.00001030
Iteration 72/1000 | Loss: 0.00001030
Iteration 73/1000 | Loss: 0.00001030
Iteration 74/1000 | Loss: 0.00001029
Iteration 75/1000 | Loss: 0.00001029
Iteration 76/1000 | Loss: 0.00001029
Iteration 77/1000 | Loss: 0.00001029
Iteration 78/1000 | Loss: 0.00001029
Iteration 79/1000 | Loss: 0.00001029
Iteration 80/1000 | Loss: 0.00001029
Iteration 81/1000 | Loss: 0.00001029
Iteration 82/1000 | Loss: 0.00001029
Iteration 83/1000 | Loss: 0.00001029
Iteration 84/1000 | Loss: 0.00001029
Iteration 85/1000 | Loss: 0.00001029
Iteration 86/1000 | Loss: 0.00001028
Iteration 87/1000 | Loss: 0.00001028
Iteration 88/1000 | Loss: 0.00001028
Iteration 89/1000 | Loss: 0.00001028
Iteration 90/1000 | Loss: 0.00001028
Iteration 91/1000 | Loss: 0.00001028
Iteration 92/1000 | Loss: 0.00001028
Iteration 93/1000 | Loss: 0.00001028
Iteration 94/1000 | Loss: 0.00001028
Iteration 95/1000 | Loss: 0.00001028
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001028
Iteration 98/1000 | Loss: 0.00001027
Iteration 99/1000 | Loss: 0.00001027
Iteration 100/1000 | Loss: 0.00001027
Iteration 101/1000 | Loss: 0.00001027
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001027
Iteration 107/1000 | Loss: 0.00001027
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001027
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001027
Iteration 115/1000 | Loss: 0.00001027
Iteration 116/1000 | Loss: 0.00001027
Iteration 117/1000 | Loss: 0.00001027
Iteration 118/1000 | Loss: 0.00001027
Iteration 119/1000 | Loss: 0.00001027
Iteration 120/1000 | Loss: 0.00001027
Iteration 121/1000 | Loss: 0.00001027
Iteration 122/1000 | Loss: 0.00001027
Iteration 123/1000 | Loss: 0.00001027
Iteration 124/1000 | Loss: 0.00001027
Iteration 125/1000 | Loss: 0.00001027
Iteration 126/1000 | Loss: 0.00001027
Iteration 127/1000 | Loss: 0.00001027
Iteration 128/1000 | Loss: 0.00001027
Iteration 129/1000 | Loss: 0.00001027
Iteration 130/1000 | Loss: 0.00001027
Iteration 131/1000 | Loss: 0.00001027
Iteration 132/1000 | Loss: 0.00001027
Iteration 133/1000 | Loss: 0.00001027
Iteration 134/1000 | Loss: 0.00001027
Iteration 135/1000 | Loss: 0.00001027
Iteration 136/1000 | Loss: 0.00001027
Iteration 137/1000 | Loss: 0.00001027
Iteration 138/1000 | Loss: 0.00001027
Iteration 139/1000 | Loss: 0.00001027
Iteration 140/1000 | Loss: 0.00001027
Iteration 141/1000 | Loss: 0.00001027
Iteration 142/1000 | Loss: 0.00001027
Iteration 143/1000 | Loss: 0.00001027
Iteration 144/1000 | Loss: 0.00001027
Iteration 145/1000 | Loss: 0.00001027
Iteration 146/1000 | Loss: 0.00001027
Iteration 147/1000 | Loss: 0.00001027
Iteration 148/1000 | Loss: 0.00001027
Iteration 149/1000 | Loss: 0.00001027
Iteration 150/1000 | Loss: 0.00001027
Iteration 151/1000 | Loss: 0.00001027
Iteration 152/1000 | Loss: 0.00001027
Iteration 153/1000 | Loss: 0.00001027
Iteration 154/1000 | Loss: 0.00001027
Iteration 155/1000 | Loss: 0.00001027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.0265588571201079e-05, 1.0265588571201079e-05, 1.0265588571201079e-05, 1.0265588571201079e-05, 1.0265588571201079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0265588571201079e-05

Optimization complete. Final v2v error: 2.6939592361450195 mm

Highest mean error: 2.857208728790283 mm for frame 84

Lowest mean error: 2.556445598602295 mm for frame 166

Saving results

Total time: 34.44853711128235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971640
Iteration 2/25 | Loss: 0.00181126
Iteration 3/25 | Loss: 0.00126937
Iteration 4/25 | Loss: 0.00112664
Iteration 5/25 | Loss: 0.00118935
Iteration 6/25 | Loss: 0.00107464
Iteration 7/25 | Loss: 0.00106239
Iteration 8/25 | Loss: 0.00101516
Iteration 9/25 | Loss: 0.00101598
Iteration 10/25 | Loss: 0.00098614
Iteration 11/25 | Loss: 0.00097132
Iteration 12/25 | Loss: 0.00096565
Iteration 13/25 | Loss: 0.00095760
Iteration 14/25 | Loss: 0.00096441
Iteration 15/25 | Loss: 0.00096340
Iteration 16/25 | Loss: 0.00096012
Iteration 17/25 | Loss: 0.00095667
Iteration 18/25 | Loss: 0.00094967
Iteration 19/25 | Loss: 0.00096012
Iteration 20/25 | Loss: 0.00094651
Iteration 21/25 | Loss: 0.00094427
Iteration 22/25 | Loss: 0.00094217
Iteration 23/25 | Loss: 0.00093322
Iteration 24/25 | Loss: 0.00093676
Iteration 25/25 | Loss: 0.00095441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.77073359
Iteration 2/25 | Loss: 0.00161601
Iteration 3/25 | Loss: 0.00161555
Iteration 4/25 | Loss: 0.00161555
Iteration 5/25 | Loss: 0.00161555
Iteration 6/25 | Loss: 0.00161555
Iteration 7/25 | Loss: 0.00161555
Iteration 8/25 | Loss: 0.00161555
Iteration 9/25 | Loss: 0.00161555
Iteration 10/25 | Loss: 0.00161555
Iteration 11/25 | Loss: 0.00161555
Iteration 12/25 | Loss: 0.00161555
Iteration 13/25 | Loss: 0.00161555
Iteration 14/25 | Loss: 0.00161555
Iteration 15/25 | Loss: 0.00161555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016155494377017021, 0.0016155494377017021, 0.0016155494377017021, 0.0016155494377017021, 0.0016155494377017021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016155494377017021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161555
Iteration 2/1000 | Loss: 0.00822613
Iteration 3/1000 | Loss: 0.00062360
Iteration 4/1000 | Loss: 0.00054908
Iteration 5/1000 | Loss: 0.00050477
Iteration 6/1000 | Loss: 0.00151643
Iteration 7/1000 | Loss: 0.00926656
Iteration 8/1000 | Loss: 0.00256960
Iteration 9/1000 | Loss: 0.00173238
Iteration 10/1000 | Loss: 0.00045501
Iteration 11/1000 | Loss: 0.00026480
Iteration 12/1000 | Loss: 0.00031141
Iteration 13/1000 | Loss: 0.00057550
Iteration 14/1000 | Loss: 0.00023681
Iteration 15/1000 | Loss: 0.00042817
Iteration 16/1000 | Loss: 0.00133786
Iteration 17/1000 | Loss: 0.00282015
Iteration 18/1000 | Loss: 0.00203543
Iteration 19/1000 | Loss: 0.00229769
Iteration 20/1000 | Loss: 0.00159781
Iteration 21/1000 | Loss: 0.00260237
Iteration 22/1000 | Loss: 0.00146035
Iteration 23/1000 | Loss: 0.00129238
Iteration 24/1000 | Loss: 0.00155341
Iteration 25/1000 | Loss: 0.00166386
Iteration 26/1000 | Loss: 0.00225666
Iteration 27/1000 | Loss: 0.00277064
Iteration 28/1000 | Loss: 0.00361012
Iteration 29/1000 | Loss: 0.00321428
Iteration 30/1000 | Loss: 0.00124949
Iteration 31/1000 | Loss: 0.00298358
Iteration 32/1000 | Loss: 0.00211246
Iteration 33/1000 | Loss: 0.00231319
Iteration 34/1000 | Loss: 0.00199477
Iteration 35/1000 | Loss: 0.00218844
Iteration 36/1000 | Loss: 0.00087194
Iteration 37/1000 | Loss: 0.00051211
Iteration 38/1000 | Loss: 0.00150281
Iteration 39/1000 | Loss: 0.00111157
Iteration 40/1000 | Loss: 0.00023559
Iteration 41/1000 | Loss: 0.00031031
Iteration 42/1000 | Loss: 0.00090378
Iteration 43/1000 | Loss: 0.00031960
Iteration 44/1000 | Loss: 0.00026358
Iteration 45/1000 | Loss: 0.00029187
Iteration 46/1000 | Loss: 0.00020015
Iteration 47/1000 | Loss: 0.00025646
Iteration 48/1000 | Loss: 0.00025923
Iteration 49/1000 | Loss: 0.00027005
Iteration 50/1000 | Loss: 0.00016217
Iteration 51/1000 | Loss: 0.00020796
Iteration 52/1000 | Loss: 0.00053339
Iteration 53/1000 | Loss: 0.00170617
Iteration 54/1000 | Loss: 0.00165790
Iteration 55/1000 | Loss: 0.00033394
Iteration 56/1000 | Loss: 0.00032752
Iteration 57/1000 | Loss: 0.00031845
Iteration 58/1000 | Loss: 0.00024052
Iteration 59/1000 | Loss: 0.00148681
Iteration 60/1000 | Loss: 0.00055377
Iteration 61/1000 | Loss: 0.00025720
Iteration 62/1000 | Loss: 0.00028569
Iteration 63/1000 | Loss: 0.00028091
Iteration 64/1000 | Loss: 0.00036047
Iteration 65/1000 | Loss: 0.00031949
Iteration 66/1000 | Loss: 0.00030468
Iteration 67/1000 | Loss: 0.00029946
Iteration 68/1000 | Loss: 0.00064881
Iteration 69/1000 | Loss: 0.00048240
Iteration 70/1000 | Loss: 0.00144948
Iteration 71/1000 | Loss: 0.00157697
Iteration 72/1000 | Loss: 0.00230898
Iteration 73/1000 | Loss: 0.00171789
Iteration 74/1000 | Loss: 0.00181473
Iteration 75/1000 | Loss: 0.00059711
Iteration 76/1000 | Loss: 0.00249466
Iteration 77/1000 | Loss: 0.00149729
Iteration 78/1000 | Loss: 0.00229627
Iteration 79/1000 | Loss: 0.00026309
Iteration 80/1000 | Loss: 0.00300990
Iteration 81/1000 | Loss: 0.00022532
Iteration 82/1000 | Loss: 0.00158234
Iteration 83/1000 | Loss: 0.00019870
Iteration 84/1000 | Loss: 0.00138453
Iteration 85/1000 | Loss: 0.00127089
Iteration 86/1000 | Loss: 0.00098892
Iteration 87/1000 | Loss: 0.00048286
Iteration 88/1000 | Loss: 0.00009541
Iteration 89/1000 | Loss: 0.00006371
Iteration 90/1000 | Loss: 0.00142866
Iteration 91/1000 | Loss: 0.00144973
Iteration 92/1000 | Loss: 0.00049216
Iteration 93/1000 | Loss: 0.00168790
Iteration 94/1000 | Loss: 0.00025211
Iteration 95/1000 | Loss: 0.00178001
Iteration 96/1000 | Loss: 0.00121770
Iteration 97/1000 | Loss: 0.00155202
Iteration 98/1000 | Loss: 0.00108986
Iteration 99/1000 | Loss: 0.00158814
Iteration 100/1000 | Loss: 0.00111929
Iteration 101/1000 | Loss: 0.00075834
Iteration 102/1000 | Loss: 0.00096754
Iteration 103/1000 | Loss: 0.00289900
Iteration 104/1000 | Loss: 0.00007918
Iteration 105/1000 | Loss: 0.00005649
Iteration 106/1000 | Loss: 0.00004922
Iteration 107/1000 | Loss: 0.00004668
Iteration 108/1000 | Loss: 0.00004454
Iteration 109/1000 | Loss: 0.00070904
Iteration 110/1000 | Loss: 0.00005188
Iteration 111/1000 | Loss: 0.00004407
Iteration 112/1000 | Loss: 0.00004194
Iteration 113/1000 | Loss: 0.00004075
Iteration 114/1000 | Loss: 0.00003986
Iteration 115/1000 | Loss: 0.00033112
Iteration 116/1000 | Loss: 0.00006517
Iteration 117/1000 | Loss: 0.00005615
Iteration 118/1000 | Loss: 0.00006316
Iteration 119/1000 | Loss: 0.00005396
Iteration 120/1000 | Loss: 0.00005573
Iteration 121/1000 | Loss: 0.00005632
Iteration 122/1000 | Loss: 0.00005072
Iteration 123/1000 | Loss: 0.00004994
Iteration 124/1000 | Loss: 0.00006451
Iteration 125/1000 | Loss: 0.00005426
Iteration 126/1000 | Loss: 0.00005587
Iteration 127/1000 | Loss: 0.00005045
Iteration 128/1000 | Loss: 0.00006419
Iteration 129/1000 | Loss: 0.00003971
Iteration 130/1000 | Loss: 0.00003737
Iteration 131/1000 | Loss: 0.00003627
Iteration 132/1000 | Loss: 0.00003573
Iteration 133/1000 | Loss: 0.00003538
Iteration 134/1000 | Loss: 0.00003530
Iteration 135/1000 | Loss: 0.00003525
Iteration 136/1000 | Loss: 0.00003524
Iteration 137/1000 | Loss: 0.00003522
Iteration 138/1000 | Loss: 0.00003507
Iteration 139/1000 | Loss: 0.00003503
Iteration 140/1000 | Loss: 0.00003501
Iteration 141/1000 | Loss: 0.00003501
Iteration 142/1000 | Loss: 0.00003493
Iteration 143/1000 | Loss: 0.00003492
Iteration 144/1000 | Loss: 0.00003490
Iteration 145/1000 | Loss: 0.00003490
Iteration 146/1000 | Loss: 0.00003490
Iteration 147/1000 | Loss: 0.00003487
Iteration 148/1000 | Loss: 0.00003485
Iteration 149/1000 | Loss: 0.00003483
Iteration 150/1000 | Loss: 0.00003483
Iteration 151/1000 | Loss: 0.00003481
Iteration 152/1000 | Loss: 0.00003480
Iteration 153/1000 | Loss: 0.00003480
Iteration 154/1000 | Loss: 0.00003479
Iteration 155/1000 | Loss: 0.00003479
Iteration 156/1000 | Loss: 0.00003479
Iteration 157/1000 | Loss: 0.00003478
Iteration 158/1000 | Loss: 0.00003477
Iteration 159/1000 | Loss: 0.00003477
Iteration 160/1000 | Loss: 0.00003477
Iteration 161/1000 | Loss: 0.00003476
Iteration 162/1000 | Loss: 0.00003475
Iteration 163/1000 | Loss: 0.00003475
Iteration 164/1000 | Loss: 0.00003475
Iteration 165/1000 | Loss: 0.00003475
Iteration 166/1000 | Loss: 0.00003475
Iteration 167/1000 | Loss: 0.00003475
Iteration 168/1000 | Loss: 0.00003474
Iteration 169/1000 | Loss: 0.00003474
Iteration 170/1000 | Loss: 0.00003474
Iteration 171/1000 | Loss: 0.00003474
Iteration 172/1000 | Loss: 0.00003473
Iteration 173/1000 | Loss: 0.00003473
Iteration 174/1000 | Loss: 0.00003473
Iteration 175/1000 | Loss: 0.00003473
Iteration 176/1000 | Loss: 0.00003473
Iteration 177/1000 | Loss: 0.00003473
Iteration 178/1000 | Loss: 0.00003473
Iteration 179/1000 | Loss: 0.00003473
Iteration 180/1000 | Loss: 0.00003473
Iteration 181/1000 | Loss: 0.00003472
Iteration 182/1000 | Loss: 0.00003472
Iteration 183/1000 | Loss: 0.00003472
Iteration 184/1000 | Loss: 0.00003472
Iteration 185/1000 | Loss: 0.00003472
Iteration 186/1000 | Loss: 0.00003472
Iteration 187/1000 | Loss: 0.00003472
Iteration 188/1000 | Loss: 0.00003472
Iteration 189/1000 | Loss: 0.00003472
Iteration 190/1000 | Loss: 0.00003472
Iteration 191/1000 | Loss: 0.00003471
Iteration 192/1000 | Loss: 0.00003471
Iteration 193/1000 | Loss: 0.00003471
Iteration 194/1000 | Loss: 0.00003471
Iteration 195/1000 | Loss: 0.00003470
Iteration 196/1000 | Loss: 0.00003470
Iteration 197/1000 | Loss: 0.00003468
Iteration 198/1000 | Loss: 0.00003468
Iteration 199/1000 | Loss: 0.00003468
Iteration 200/1000 | Loss: 0.00003468
Iteration 201/1000 | Loss: 0.00003468
Iteration 202/1000 | Loss: 0.00003468
Iteration 203/1000 | Loss: 0.00003468
Iteration 204/1000 | Loss: 0.00003468
Iteration 205/1000 | Loss: 0.00003468
Iteration 206/1000 | Loss: 0.00003467
Iteration 207/1000 | Loss: 0.00003467
Iteration 208/1000 | Loss: 0.00003467
Iteration 209/1000 | Loss: 0.00003467
Iteration 210/1000 | Loss: 0.00003466
Iteration 211/1000 | Loss: 0.00003466
Iteration 212/1000 | Loss: 0.00003466
Iteration 213/1000 | Loss: 0.00003466
Iteration 214/1000 | Loss: 0.00003465
Iteration 215/1000 | Loss: 0.00003465
Iteration 216/1000 | Loss: 0.00003465
Iteration 217/1000 | Loss: 0.00003465
Iteration 218/1000 | Loss: 0.00003464
Iteration 219/1000 | Loss: 0.00003464
Iteration 220/1000 | Loss: 0.00003464
Iteration 221/1000 | Loss: 0.00003464
Iteration 222/1000 | Loss: 0.00003464
Iteration 223/1000 | Loss: 0.00003463
Iteration 224/1000 | Loss: 0.00003462
Iteration 225/1000 | Loss: 0.00003462
Iteration 226/1000 | Loss: 0.00003461
Iteration 227/1000 | Loss: 0.00003461
Iteration 228/1000 | Loss: 0.00003461
Iteration 229/1000 | Loss: 0.00003461
Iteration 230/1000 | Loss: 0.00003461
Iteration 231/1000 | Loss: 0.00003461
Iteration 232/1000 | Loss: 0.00003460
Iteration 233/1000 | Loss: 0.00003460
Iteration 234/1000 | Loss: 0.00003460
Iteration 235/1000 | Loss: 0.00003460
Iteration 236/1000 | Loss: 0.00003460
Iteration 237/1000 | Loss: 0.00003459
Iteration 238/1000 | Loss: 0.00003459
Iteration 239/1000 | Loss: 0.00003459
Iteration 240/1000 | Loss: 0.00003459
Iteration 241/1000 | Loss: 0.00003458
Iteration 242/1000 | Loss: 0.00003458
Iteration 243/1000 | Loss: 0.00003458
Iteration 244/1000 | Loss: 0.00003458
Iteration 245/1000 | Loss: 0.00003458
Iteration 246/1000 | Loss: 0.00003458
Iteration 247/1000 | Loss: 0.00003457
Iteration 248/1000 | Loss: 0.00003457
Iteration 249/1000 | Loss: 0.00003457
Iteration 250/1000 | Loss: 0.00003457
Iteration 251/1000 | Loss: 0.00003457
Iteration 252/1000 | Loss: 0.00003457
Iteration 253/1000 | Loss: 0.00003457
Iteration 254/1000 | Loss: 0.00003457
Iteration 255/1000 | Loss: 0.00003457
Iteration 256/1000 | Loss: 0.00003456
Iteration 257/1000 | Loss: 0.00003456
Iteration 258/1000 | Loss: 0.00003456
Iteration 259/1000 | Loss: 0.00003456
Iteration 260/1000 | Loss: 0.00003456
Iteration 261/1000 | Loss: 0.00003456
Iteration 262/1000 | Loss: 0.00003456
Iteration 263/1000 | Loss: 0.00003455
Iteration 264/1000 | Loss: 0.00003455
Iteration 265/1000 | Loss: 0.00003455
Iteration 266/1000 | Loss: 0.00003455
Iteration 267/1000 | Loss: 0.00003455
Iteration 268/1000 | Loss: 0.00003455
Iteration 269/1000 | Loss: 0.00003455
Iteration 270/1000 | Loss: 0.00003455
Iteration 271/1000 | Loss: 0.00003455
Iteration 272/1000 | Loss: 0.00003455
Iteration 273/1000 | Loss: 0.00003455
Iteration 274/1000 | Loss: 0.00003455
Iteration 275/1000 | Loss: 0.00003454
Iteration 276/1000 | Loss: 0.00003454
Iteration 277/1000 | Loss: 0.00003454
Iteration 278/1000 | Loss: 0.00003454
Iteration 279/1000 | Loss: 0.00003454
Iteration 280/1000 | Loss: 0.00003453
Iteration 281/1000 | Loss: 0.00003453
Iteration 282/1000 | Loss: 0.00003453
Iteration 283/1000 | Loss: 0.00003453
Iteration 284/1000 | Loss: 0.00003453
Iteration 285/1000 | Loss: 0.00003453
Iteration 286/1000 | Loss: 0.00003453
Iteration 287/1000 | Loss: 0.00003453
Iteration 288/1000 | Loss: 0.00003452
Iteration 289/1000 | Loss: 0.00003452
Iteration 290/1000 | Loss: 0.00003452
Iteration 291/1000 | Loss: 0.00003452
Iteration 292/1000 | Loss: 0.00003452
Iteration 293/1000 | Loss: 0.00003451
Iteration 294/1000 | Loss: 0.00003451
Iteration 295/1000 | Loss: 0.00003451
Iteration 296/1000 | Loss: 0.00003451
Iteration 297/1000 | Loss: 0.00003451
Iteration 298/1000 | Loss: 0.00003450
Iteration 299/1000 | Loss: 0.00003450
Iteration 300/1000 | Loss: 0.00003450
Iteration 301/1000 | Loss: 0.00003450
Iteration 302/1000 | Loss: 0.00003450
Iteration 303/1000 | Loss: 0.00003450
Iteration 304/1000 | Loss: 0.00003450
Iteration 305/1000 | Loss: 0.00003450
Iteration 306/1000 | Loss: 0.00003450
Iteration 307/1000 | Loss: 0.00003449
Iteration 308/1000 | Loss: 0.00003449
Iteration 309/1000 | Loss: 0.00003449
Iteration 310/1000 | Loss: 0.00003449
Iteration 311/1000 | Loss: 0.00003449
Iteration 312/1000 | Loss: 0.00003449
Iteration 313/1000 | Loss: 0.00003449
Iteration 314/1000 | Loss: 0.00003448
Iteration 315/1000 | Loss: 0.00003448
Iteration 316/1000 | Loss: 0.00003448
Iteration 317/1000 | Loss: 0.00003448
Iteration 318/1000 | Loss: 0.00003448
Iteration 319/1000 | Loss: 0.00003448
Iteration 320/1000 | Loss: 0.00003448
Iteration 321/1000 | Loss: 0.00003448
Iteration 322/1000 | Loss: 0.00003448
Iteration 323/1000 | Loss: 0.00003447
Iteration 324/1000 | Loss: 0.00003447
Iteration 325/1000 | Loss: 0.00003447
Iteration 326/1000 | Loss: 0.00003447
Iteration 327/1000 | Loss: 0.00003447
Iteration 328/1000 | Loss: 0.00003447
Iteration 329/1000 | Loss: 0.00003447
Iteration 330/1000 | Loss: 0.00003447
Iteration 331/1000 | Loss: 0.00003447
Iteration 332/1000 | Loss: 0.00003447
Iteration 333/1000 | Loss: 0.00003447
Iteration 334/1000 | Loss: 0.00003447
Iteration 335/1000 | Loss: 0.00003447
Iteration 336/1000 | Loss: 0.00003447
Iteration 337/1000 | Loss: 0.00003447
Iteration 338/1000 | Loss: 0.00003447
Iteration 339/1000 | Loss: 0.00003447
Iteration 340/1000 | Loss: 0.00003447
Iteration 341/1000 | Loss: 0.00003447
Iteration 342/1000 | Loss: 0.00003447
Iteration 343/1000 | Loss: 0.00003447
Iteration 344/1000 | Loss: 0.00003447
Iteration 345/1000 | Loss: 0.00003447
Iteration 346/1000 | Loss: 0.00003447
Iteration 347/1000 | Loss: 0.00003447
Iteration 348/1000 | Loss: 0.00003447
Iteration 349/1000 | Loss: 0.00003447
Iteration 350/1000 | Loss: 0.00003447
Iteration 351/1000 | Loss: 0.00003447
Iteration 352/1000 | Loss: 0.00003447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 352. Stopping optimization.
Last 5 losses: [3.4466189390514046e-05, 3.4466189390514046e-05, 3.4466189390514046e-05, 3.4466189390514046e-05, 3.4466189390514046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4466189390514046e-05

Optimization complete. Final v2v error: 4.666738986968994 mm

Highest mean error: 13.071608543395996 mm for frame 22

Lowest mean error: 3.4962024688720703 mm for frame 150

Saving results

Total time: 326.12766790390015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419396
Iteration 2/25 | Loss: 0.00101662
Iteration 3/25 | Loss: 0.00076702
Iteration 4/25 | Loss: 0.00072500
Iteration 5/25 | Loss: 0.00071647
Iteration 6/25 | Loss: 0.00071452
Iteration 7/25 | Loss: 0.00071429
Iteration 8/25 | Loss: 0.00071429
Iteration 9/25 | Loss: 0.00071429
Iteration 10/25 | Loss: 0.00071429
Iteration 11/25 | Loss: 0.00071429
Iteration 12/25 | Loss: 0.00071429
Iteration 13/25 | Loss: 0.00071429
Iteration 14/25 | Loss: 0.00071429
Iteration 15/25 | Loss: 0.00071429
Iteration 16/25 | Loss: 0.00071429
Iteration 17/25 | Loss: 0.00071429
Iteration 18/25 | Loss: 0.00071429
Iteration 19/25 | Loss: 0.00071429
Iteration 20/25 | Loss: 0.00071429
Iteration 21/25 | Loss: 0.00071429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007142919348552823, 0.0007142919348552823, 0.0007142919348552823, 0.0007142919348552823, 0.0007142919348552823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007142919348552823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41250467
Iteration 2/25 | Loss: 0.00034670
Iteration 3/25 | Loss: 0.00034667
Iteration 4/25 | Loss: 0.00034667
Iteration 5/25 | Loss: 0.00034667
Iteration 6/25 | Loss: 0.00034667
Iteration 7/25 | Loss: 0.00034667
Iteration 8/25 | Loss: 0.00034667
Iteration 9/25 | Loss: 0.00034667
Iteration 10/25 | Loss: 0.00034667
Iteration 11/25 | Loss: 0.00034667
Iteration 12/25 | Loss: 0.00034667
Iteration 13/25 | Loss: 0.00034667
Iteration 14/25 | Loss: 0.00034667
Iteration 15/25 | Loss: 0.00034667
Iteration 16/25 | Loss: 0.00034667
Iteration 17/25 | Loss: 0.00034667
Iteration 18/25 | Loss: 0.00034667
Iteration 19/25 | Loss: 0.00034667
Iteration 20/25 | Loss: 0.00034667
Iteration 21/25 | Loss: 0.00034667
Iteration 22/25 | Loss: 0.00034667
Iteration 23/25 | Loss: 0.00034667
Iteration 24/25 | Loss: 0.00034667
Iteration 25/25 | Loss: 0.00034667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034667
Iteration 2/1000 | Loss: 0.00004163
Iteration 3/1000 | Loss: 0.00002876
Iteration 4/1000 | Loss: 0.00002753
Iteration 5/1000 | Loss: 0.00002583
Iteration 6/1000 | Loss: 0.00002498
Iteration 7/1000 | Loss: 0.00002432
Iteration 8/1000 | Loss: 0.00002396
Iteration 9/1000 | Loss: 0.00002368
Iteration 10/1000 | Loss: 0.00002358
Iteration 11/1000 | Loss: 0.00002357
Iteration 12/1000 | Loss: 0.00002345
Iteration 13/1000 | Loss: 0.00002342
Iteration 14/1000 | Loss: 0.00002342
Iteration 15/1000 | Loss: 0.00002341
Iteration 16/1000 | Loss: 0.00002341
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002329
Iteration 19/1000 | Loss: 0.00002329
Iteration 20/1000 | Loss: 0.00002329
Iteration 21/1000 | Loss: 0.00002328
Iteration 22/1000 | Loss: 0.00002328
Iteration 23/1000 | Loss: 0.00002326
Iteration 24/1000 | Loss: 0.00002326
Iteration 25/1000 | Loss: 0.00002326
Iteration 26/1000 | Loss: 0.00002326
Iteration 27/1000 | Loss: 0.00002326
Iteration 28/1000 | Loss: 0.00002326
Iteration 29/1000 | Loss: 0.00002326
Iteration 30/1000 | Loss: 0.00002326
Iteration 31/1000 | Loss: 0.00002326
Iteration 32/1000 | Loss: 0.00002326
Iteration 33/1000 | Loss: 0.00002326
Iteration 34/1000 | Loss: 0.00002326
Iteration 35/1000 | Loss: 0.00002326
Iteration 36/1000 | Loss: 0.00002326
Iteration 37/1000 | Loss: 0.00002326
Iteration 38/1000 | Loss: 0.00002325
Iteration 39/1000 | Loss: 0.00002325
Iteration 40/1000 | Loss: 0.00002325
Iteration 41/1000 | Loss: 0.00002325
Iteration 42/1000 | Loss: 0.00002325
Iteration 43/1000 | Loss: 0.00002325
Iteration 44/1000 | Loss: 0.00002325
Iteration 45/1000 | Loss: 0.00002325
Iteration 46/1000 | Loss: 0.00002325
Iteration 47/1000 | Loss: 0.00002325
Iteration 48/1000 | Loss: 0.00002325
Iteration 49/1000 | Loss: 0.00002325
Iteration 50/1000 | Loss: 0.00002325
Iteration 51/1000 | Loss: 0.00002325
Iteration 52/1000 | Loss: 0.00002325
Iteration 53/1000 | Loss: 0.00002325
Iteration 54/1000 | Loss: 0.00002325
Iteration 55/1000 | Loss: 0.00002325
Iteration 56/1000 | Loss: 0.00002325
Iteration 57/1000 | Loss: 0.00002325
Iteration 58/1000 | Loss: 0.00002325
Iteration 59/1000 | Loss: 0.00002325
Iteration 60/1000 | Loss: 0.00002325
Iteration 61/1000 | Loss: 0.00002325
Iteration 62/1000 | Loss: 0.00002325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [2.3252650862559676e-05, 2.3252650862559676e-05, 2.3252650862559676e-05, 2.3252650862559676e-05, 2.3252650862559676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3252650862559676e-05

Optimization complete. Final v2v error: 4.0336012840271 mm

Highest mean error: 4.731400966644287 mm for frame 143

Lowest mean error: 3.6831250190734863 mm for frame 152

Saving results

Total time: 158.71389651298523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959878
Iteration 2/25 | Loss: 0.00094859
Iteration 3/25 | Loss: 0.00070940
Iteration 4/25 | Loss: 0.00068126
Iteration 5/25 | Loss: 0.00066955
Iteration 6/25 | Loss: 0.00066099
Iteration 7/25 | Loss: 0.00066490
Iteration 8/25 | Loss: 0.00065968
Iteration 9/25 | Loss: 0.00066649
Iteration 10/25 | Loss: 0.00065970
Iteration 11/25 | Loss: 0.00065951
Iteration 12/25 | Loss: 0.00065951
Iteration 13/25 | Loss: 0.00065951
Iteration 14/25 | Loss: 0.00065951
Iteration 15/25 | Loss: 0.00065951
Iteration 16/25 | Loss: 0.00065950
Iteration 17/25 | Loss: 0.00065950
Iteration 18/25 | Loss: 0.00065950
Iteration 19/25 | Loss: 0.00065950
Iteration 20/25 | Loss: 0.00065950
Iteration 21/25 | Loss: 0.00065950
Iteration 22/25 | Loss: 0.00065950
Iteration 23/25 | Loss: 0.00065950
Iteration 24/25 | Loss: 0.00065950
Iteration 25/25 | Loss: 0.00065950

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54894471
Iteration 2/25 | Loss: 0.00025617
Iteration 3/25 | Loss: 0.00025614
Iteration 4/25 | Loss: 0.00025614
Iteration 5/25 | Loss: 0.00025614
Iteration 6/25 | Loss: 0.00025614
Iteration 7/25 | Loss: 0.00025614
Iteration 8/25 | Loss: 0.00025614
Iteration 9/25 | Loss: 0.00025614
Iteration 10/25 | Loss: 0.00025614
Iteration 11/25 | Loss: 0.00025614
Iteration 12/25 | Loss: 0.00025614
Iteration 13/25 | Loss: 0.00025614
Iteration 14/25 | Loss: 0.00025614
Iteration 15/25 | Loss: 0.00025614
Iteration 16/25 | Loss: 0.00025614
Iteration 17/25 | Loss: 0.00025614
Iteration 18/25 | Loss: 0.00025614
Iteration 19/25 | Loss: 0.00025614
Iteration 20/25 | Loss: 0.00025614
Iteration 21/25 | Loss: 0.00025614
Iteration 22/25 | Loss: 0.00025614
Iteration 23/25 | Loss: 0.00025614
Iteration 24/25 | Loss: 0.00025614
Iteration 25/25 | Loss: 0.00025614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025614
Iteration 2/1000 | Loss: 0.00004314
Iteration 3/1000 | Loss: 0.00002671
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002259
Iteration 6/1000 | Loss: 0.00002175
Iteration 7/1000 | Loss: 0.00002106
Iteration 8/1000 | Loss: 0.00002061
Iteration 9/1000 | Loss: 0.00002023
Iteration 10/1000 | Loss: 0.00002000
Iteration 11/1000 | Loss: 0.00001980
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00001964
Iteration 14/1000 | Loss: 0.00001959
Iteration 15/1000 | Loss: 0.00001955
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00001952
Iteration 18/1000 | Loss: 0.00001950
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001947
Iteration 22/1000 | Loss: 0.00001945
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001942
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001941
Iteration 31/1000 | Loss: 0.00001941
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001941
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001940
Iteration 40/1000 | Loss: 0.00001940
Iteration 41/1000 | Loss: 0.00001940
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001937
Iteration 45/1000 | Loss: 0.00001936
Iteration 46/1000 | Loss: 0.00001936
Iteration 47/1000 | Loss: 0.00001935
Iteration 48/1000 | Loss: 0.00001935
Iteration 49/1000 | Loss: 0.00001934
Iteration 50/1000 | Loss: 0.00001932
Iteration 51/1000 | Loss: 0.00001931
Iteration 52/1000 | Loss: 0.00001931
Iteration 53/1000 | Loss: 0.00001931
Iteration 54/1000 | Loss: 0.00001930
Iteration 55/1000 | Loss: 0.00001930
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001929
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001928
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001926
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001921
Iteration 69/1000 | Loss: 0.00001920
Iteration 70/1000 | Loss: 0.00001920
Iteration 71/1000 | Loss: 0.00001920
Iteration 72/1000 | Loss: 0.00001919
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001918
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001916
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001915
Iteration 87/1000 | Loss: 0.00001915
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001914
Iteration 92/1000 | Loss: 0.00001914
Iteration 93/1000 | Loss: 0.00001914
Iteration 94/1000 | Loss: 0.00001914
Iteration 95/1000 | Loss: 0.00001914
Iteration 96/1000 | Loss: 0.00001914
Iteration 97/1000 | Loss: 0.00001914
Iteration 98/1000 | Loss: 0.00001914
Iteration 99/1000 | Loss: 0.00001913
Iteration 100/1000 | Loss: 0.00001913
Iteration 101/1000 | Loss: 0.00001913
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00001913
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001913
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001912
Iteration 114/1000 | Loss: 0.00001912
Iteration 115/1000 | Loss: 0.00001912
Iteration 116/1000 | Loss: 0.00001912
Iteration 117/1000 | Loss: 0.00001912
Iteration 118/1000 | Loss: 0.00001912
Iteration 119/1000 | Loss: 0.00001912
Iteration 120/1000 | Loss: 0.00001911
Iteration 121/1000 | Loss: 0.00001911
Iteration 122/1000 | Loss: 0.00001911
Iteration 123/1000 | Loss: 0.00001911
Iteration 124/1000 | Loss: 0.00001911
Iteration 125/1000 | Loss: 0.00001911
Iteration 126/1000 | Loss: 0.00001911
Iteration 127/1000 | Loss: 0.00001911
Iteration 128/1000 | Loss: 0.00001911
Iteration 129/1000 | Loss: 0.00001911
Iteration 130/1000 | Loss: 0.00001911
Iteration 131/1000 | Loss: 0.00001911
Iteration 132/1000 | Loss: 0.00001910
Iteration 133/1000 | Loss: 0.00001910
Iteration 134/1000 | Loss: 0.00001910
Iteration 135/1000 | Loss: 0.00001910
Iteration 136/1000 | Loss: 0.00001910
Iteration 137/1000 | Loss: 0.00001910
Iteration 138/1000 | Loss: 0.00001910
Iteration 139/1000 | Loss: 0.00001910
Iteration 140/1000 | Loss: 0.00001910
Iteration 141/1000 | Loss: 0.00001909
Iteration 142/1000 | Loss: 0.00001909
Iteration 143/1000 | Loss: 0.00001909
Iteration 144/1000 | Loss: 0.00001909
Iteration 145/1000 | Loss: 0.00001909
Iteration 146/1000 | Loss: 0.00001909
Iteration 147/1000 | Loss: 0.00001909
Iteration 148/1000 | Loss: 0.00001909
Iteration 149/1000 | Loss: 0.00001909
Iteration 150/1000 | Loss: 0.00001909
Iteration 151/1000 | Loss: 0.00001909
Iteration 152/1000 | Loss: 0.00001909
Iteration 153/1000 | Loss: 0.00001909
Iteration 154/1000 | Loss: 0.00001909
Iteration 155/1000 | Loss: 0.00001909
Iteration 156/1000 | Loss: 0.00001909
Iteration 157/1000 | Loss: 0.00001909
Iteration 158/1000 | Loss: 0.00001909
Iteration 159/1000 | Loss: 0.00001909
Iteration 160/1000 | Loss: 0.00001909
Iteration 161/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.9093644368695095e-05, 1.9093644368695095e-05, 1.9093644368695095e-05, 1.9093644368695095e-05, 1.9093644368695095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9093644368695095e-05

Optimization complete. Final v2v error: 3.421984910964966 mm

Highest mean error: 5.801310062408447 mm for frame 179

Lowest mean error: 2.8194823265075684 mm for frame 111

Saving results

Total time: 70.1548399925232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356830
Iteration 2/25 | Loss: 0.00071772
Iteration 3/25 | Loss: 0.00059510
Iteration 4/25 | Loss: 0.00057843
Iteration 5/25 | Loss: 0.00057334
Iteration 6/25 | Loss: 0.00057202
Iteration 7/25 | Loss: 0.00057177
Iteration 8/25 | Loss: 0.00057177
Iteration 9/25 | Loss: 0.00057177
Iteration 10/25 | Loss: 0.00057177
Iteration 11/25 | Loss: 0.00057177
Iteration 12/25 | Loss: 0.00057177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005717735621146858, 0.0005717735621146858, 0.0005717735621146858, 0.0005717735621146858, 0.0005717735621146858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005717735621146858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46914423
Iteration 2/25 | Loss: 0.00027838
Iteration 3/25 | Loss: 0.00027838
Iteration 4/25 | Loss: 0.00027838
Iteration 5/25 | Loss: 0.00027838
Iteration 6/25 | Loss: 0.00027837
Iteration 7/25 | Loss: 0.00027837
Iteration 8/25 | Loss: 0.00027837
Iteration 9/25 | Loss: 0.00027837
Iteration 10/25 | Loss: 0.00027837
Iteration 11/25 | Loss: 0.00027837
Iteration 12/25 | Loss: 0.00027837
Iteration 13/25 | Loss: 0.00027837
Iteration 14/25 | Loss: 0.00027837
Iteration 15/25 | Loss: 0.00027837
Iteration 16/25 | Loss: 0.00027837
Iteration 17/25 | Loss: 0.00027837
Iteration 18/25 | Loss: 0.00027837
Iteration 19/25 | Loss: 0.00027837
Iteration 20/25 | Loss: 0.00027837
Iteration 21/25 | Loss: 0.00027837
Iteration 22/25 | Loss: 0.00027837
Iteration 23/25 | Loss: 0.00027837
Iteration 24/25 | Loss: 0.00027837
Iteration 25/25 | Loss: 0.00027837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027837
Iteration 2/1000 | Loss: 0.00001876
Iteration 3/1000 | Loss: 0.00001121
Iteration 4/1000 | Loss: 0.00001043
Iteration 5/1000 | Loss: 0.00000986
Iteration 6/1000 | Loss: 0.00000974
Iteration 7/1000 | Loss: 0.00000950
Iteration 8/1000 | Loss: 0.00000943
Iteration 9/1000 | Loss: 0.00000942
Iteration 10/1000 | Loss: 0.00000942
Iteration 11/1000 | Loss: 0.00000942
Iteration 12/1000 | Loss: 0.00000942
Iteration 13/1000 | Loss: 0.00000942
Iteration 14/1000 | Loss: 0.00000942
Iteration 15/1000 | Loss: 0.00000942
Iteration 16/1000 | Loss: 0.00000942
Iteration 17/1000 | Loss: 0.00000941
Iteration 18/1000 | Loss: 0.00000933
Iteration 19/1000 | Loss: 0.00000933
Iteration 20/1000 | Loss: 0.00000931
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000930
Iteration 24/1000 | Loss: 0.00000929
Iteration 25/1000 | Loss: 0.00000927
Iteration 26/1000 | Loss: 0.00000926
Iteration 27/1000 | Loss: 0.00000926
Iteration 28/1000 | Loss: 0.00000925
Iteration 29/1000 | Loss: 0.00000925
Iteration 30/1000 | Loss: 0.00000925
Iteration 31/1000 | Loss: 0.00000924
Iteration 32/1000 | Loss: 0.00000923
Iteration 33/1000 | Loss: 0.00000921
Iteration 34/1000 | Loss: 0.00000920
Iteration 35/1000 | Loss: 0.00000920
Iteration 36/1000 | Loss: 0.00000920
Iteration 37/1000 | Loss: 0.00000919
Iteration 38/1000 | Loss: 0.00000919
Iteration 39/1000 | Loss: 0.00000919
Iteration 40/1000 | Loss: 0.00000919
Iteration 41/1000 | Loss: 0.00000919
Iteration 42/1000 | Loss: 0.00000918
Iteration 43/1000 | Loss: 0.00000918
Iteration 44/1000 | Loss: 0.00000917
Iteration 45/1000 | Loss: 0.00000916
Iteration 46/1000 | Loss: 0.00000916
Iteration 47/1000 | Loss: 0.00000916
Iteration 48/1000 | Loss: 0.00000915
Iteration 49/1000 | Loss: 0.00000915
Iteration 50/1000 | Loss: 0.00000915
Iteration 51/1000 | Loss: 0.00000914
Iteration 52/1000 | Loss: 0.00000914
Iteration 53/1000 | Loss: 0.00000913
Iteration 54/1000 | Loss: 0.00000913
Iteration 55/1000 | Loss: 0.00000913
Iteration 56/1000 | Loss: 0.00000913
Iteration 57/1000 | Loss: 0.00000913
Iteration 58/1000 | Loss: 0.00000913
Iteration 59/1000 | Loss: 0.00000913
Iteration 60/1000 | Loss: 0.00000913
Iteration 61/1000 | Loss: 0.00000912
Iteration 62/1000 | Loss: 0.00000912
Iteration 63/1000 | Loss: 0.00000912
Iteration 64/1000 | Loss: 0.00000912
Iteration 65/1000 | Loss: 0.00000911
Iteration 66/1000 | Loss: 0.00000911
Iteration 67/1000 | Loss: 0.00000911
Iteration 68/1000 | Loss: 0.00000911
Iteration 69/1000 | Loss: 0.00000911
Iteration 70/1000 | Loss: 0.00000910
Iteration 71/1000 | Loss: 0.00000908
Iteration 72/1000 | Loss: 0.00000907
Iteration 73/1000 | Loss: 0.00000907
Iteration 74/1000 | Loss: 0.00000906
Iteration 75/1000 | Loss: 0.00000905
Iteration 76/1000 | Loss: 0.00000905
Iteration 77/1000 | Loss: 0.00000904
Iteration 78/1000 | Loss: 0.00000904
Iteration 79/1000 | Loss: 0.00000903
Iteration 80/1000 | Loss: 0.00000903
Iteration 81/1000 | Loss: 0.00000903
Iteration 82/1000 | Loss: 0.00000903
Iteration 83/1000 | Loss: 0.00000903
Iteration 84/1000 | Loss: 0.00000902
Iteration 85/1000 | Loss: 0.00000902
Iteration 86/1000 | Loss: 0.00000902
Iteration 87/1000 | Loss: 0.00000902
Iteration 88/1000 | Loss: 0.00000902
Iteration 89/1000 | Loss: 0.00000902
Iteration 90/1000 | Loss: 0.00000902
Iteration 91/1000 | Loss: 0.00000902
Iteration 92/1000 | Loss: 0.00000902
Iteration 93/1000 | Loss: 0.00000901
Iteration 94/1000 | Loss: 0.00000901
Iteration 95/1000 | Loss: 0.00000901
Iteration 96/1000 | Loss: 0.00000901
Iteration 97/1000 | Loss: 0.00000901
Iteration 98/1000 | Loss: 0.00000901
Iteration 99/1000 | Loss: 0.00000901
Iteration 100/1000 | Loss: 0.00000901
Iteration 101/1000 | Loss: 0.00000901
Iteration 102/1000 | Loss: 0.00000901
Iteration 103/1000 | Loss: 0.00000901
Iteration 104/1000 | Loss: 0.00000901
Iteration 105/1000 | Loss: 0.00000901
Iteration 106/1000 | Loss: 0.00000901
Iteration 107/1000 | Loss: 0.00000901
Iteration 108/1000 | Loss: 0.00000901
Iteration 109/1000 | Loss: 0.00000901
Iteration 110/1000 | Loss: 0.00000901
Iteration 111/1000 | Loss: 0.00000901
Iteration 112/1000 | Loss: 0.00000901
Iteration 113/1000 | Loss: 0.00000901
Iteration 114/1000 | Loss: 0.00000901
Iteration 115/1000 | Loss: 0.00000901
Iteration 116/1000 | Loss: 0.00000901
Iteration 117/1000 | Loss: 0.00000901
Iteration 118/1000 | Loss: 0.00000901
Iteration 119/1000 | Loss: 0.00000901
Iteration 120/1000 | Loss: 0.00000901
Iteration 121/1000 | Loss: 0.00000901
Iteration 122/1000 | Loss: 0.00000901
Iteration 123/1000 | Loss: 0.00000901
Iteration 124/1000 | Loss: 0.00000901
Iteration 125/1000 | Loss: 0.00000901
Iteration 126/1000 | Loss: 0.00000901
Iteration 127/1000 | Loss: 0.00000901
Iteration 128/1000 | Loss: 0.00000901
Iteration 129/1000 | Loss: 0.00000901
Iteration 130/1000 | Loss: 0.00000901
Iteration 131/1000 | Loss: 0.00000901
Iteration 132/1000 | Loss: 0.00000901
Iteration 133/1000 | Loss: 0.00000901
Iteration 134/1000 | Loss: 0.00000901
Iteration 135/1000 | Loss: 0.00000901
Iteration 136/1000 | Loss: 0.00000901
Iteration 137/1000 | Loss: 0.00000901
Iteration 138/1000 | Loss: 0.00000901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [9.00541363080265e-06, 9.00541363080265e-06, 9.00541363080265e-06, 9.00541363080265e-06, 9.00541363080265e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.00541363080265e-06

Optimization complete. Final v2v error: 2.560420513153076 mm

Highest mean error: 3.00480318069458 mm for frame 96

Lowest mean error: 2.4499120712280273 mm for frame 119

Saving results

Total time: 33.98657202720642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368633
Iteration 2/25 | Loss: 0.00071450
Iteration 3/25 | Loss: 0.00059653
Iteration 4/25 | Loss: 0.00058098
Iteration 5/25 | Loss: 0.00057570
Iteration 6/25 | Loss: 0.00057430
Iteration 7/25 | Loss: 0.00057388
Iteration 8/25 | Loss: 0.00057382
Iteration 9/25 | Loss: 0.00057382
Iteration 10/25 | Loss: 0.00057382
Iteration 11/25 | Loss: 0.00057382
Iteration 12/25 | Loss: 0.00057382
Iteration 13/25 | Loss: 0.00057382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000573823694139719, 0.000573823694139719, 0.000573823694139719, 0.000573823694139719, 0.000573823694139719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000573823694139719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71463752
Iteration 2/25 | Loss: 0.00027608
Iteration 3/25 | Loss: 0.00027607
Iteration 4/25 | Loss: 0.00027607
Iteration 5/25 | Loss: 0.00027607
Iteration 6/25 | Loss: 0.00027607
Iteration 7/25 | Loss: 0.00027607
Iteration 8/25 | Loss: 0.00027607
Iteration 9/25 | Loss: 0.00027607
Iteration 10/25 | Loss: 0.00027607
Iteration 11/25 | Loss: 0.00027607
Iteration 12/25 | Loss: 0.00027607
Iteration 13/25 | Loss: 0.00027607
Iteration 14/25 | Loss: 0.00027607
Iteration 15/25 | Loss: 0.00027607
Iteration 16/25 | Loss: 0.00027607
Iteration 17/25 | Loss: 0.00027607
Iteration 18/25 | Loss: 0.00027607
Iteration 19/25 | Loss: 0.00027607
Iteration 20/25 | Loss: 0.00027607
Iteration 21/25 | Loss: 0.00027607
Iteration 22/25 | Loss: 0.00027607
Iteration 23/25 | Loss: 0.00027607
Iteration 24/25 | Loss: 0.00027607
Iteration 25/25 | Loss: 0.00027607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027607
Iteration 2/1000 | Loss: 0.00002312
Iteration 3/1000 | Loss: 0.00001465
Iteration 4/1000 | Loss: 0.00001075
Iteration 5/1000 | Loss: 0.00001014
Iteration 6/1000 | Loss: 0.00000972
Iteration 7/1000 | Loss: 0.00000968
Iteration 8/1000 | Loss: 0.00000943
Iteration 9/1000 | Loss: 0.00000931
Iteration 10/1000 | Loss: 0.00000931
Iteration 11/1000 | Loss: 0.00000929
Iteration 12/1000 | Loss: 0.00000928
Iteration 13/1000 | Loss: 0.00000927
Iteration 14/1000 | Loss: 0.00000927
Iteration 15/1000 | Loss: 0.00000922
Iteration 16/1000 | Loss: 0.00000918
Iteration 17/1000 | Loss: 0.00000917
Iteration 18/1000 | Loss: 0.00000917
Iteration 19/1000 | Loss: 0.00000917
Iteration 20/1000 | Loss: 0.00000917
Iteration 21/1000 | Loss: 0.00000917
Iteration 22/1000 | Loss: 0.00000916
Iteration 23/1000 | Loss: 0.00000916
Iteration 24/1000 | Loss: 0.00000916
Iteration 25/1000 | Loss: 0.00000915
Iteration 26/1000 | Loss: 0.00000915
Iteration 27/1000 | Loss: 0.00000914
Iteration 28/1000 | Loss: 0.00000914
Iteration 29/1000 | Loss: 0.00000914
Iteration 30/1000 | Loss: 0.00000913
Iteration 31/1000 | Loss: 0.00000913
Iteration 32/1000 | Loss: 0.00000913
Iteration 33/1000 | Loss: 0.00000912
Iteration 34/1000 | Loss: 0.00000912
Iteration 35/1000 | Loss: 0.00000912
Iteration 36/1000 | Loss: 0.00000911
Iteration 37/1000 | Loss: 0.00000910
Iteration 38/1000 | Loss: 0.00000909
Iteration 39/1000 | Loss: 0.00000909
Iteration 40/1000 | Loss: 0.00000909
Iteration 41/1000 | Loss: 0.00000909
Iteration 42/1000 | Loss: 0.00000907
Iteration 43/1000 | Loss: 0.00000906
Iteration 44/1000 | Loss: 0.00000906
Iteration 45/1000 | Loss: 0.00000905
Iteration 46/1000 | Loss: 0.00000905
Iteration 47/1000 | Loss: 0.00000905
Iteration 48/1000 | Loss: 0.00000904
Iteration 49/1000 | Loss: 0.00000903
Iteration 50/1000 | Loss: 0.00000903
Iteration 51/1000 | Loss: 0.00000903
Iteration 52/1000 | Loss: 0.00000902
Iteration 53/1000 | Loss: 0.00000902
Iteration 54/1000 | Loss: 0.00000902
Iteration 55/1000 | Loss: 0.00000902
Iteration 56/1000 | Loss: 0.00000902
Iteration 57/1000 | Loss: 0.00000902
Iteration 58/1000 | Loss: 0.00000902
Iteration 59/1000 | Loss: 0.00000902
Iteration 60/1000 | Loss: 0.00000901
Iteration 61/1000 | Loss: 0.00000901
Iteration 62/1000 | Loss: 0.00000901
Iteration 63/1000 | Loss: 0.00000900
Iteration 64/1000 | Loss: 0.00000900
Iteration 65/1000 | Loss: 0.00000900
Iteration 66/1000 | Loss: 0.00000899
Iteration 67/1000 | Loss: 0.00000899
Iteration 68/1000 | Loss: 0.00000898
Iteration 69/1000 | Loss: 0.00000898
Iteration 70/1000 | Loss: 0.00000898
Iteration 71/1000 | Loss: 0.00000898
Iteration 72/1000 | Loss: 0.00000897
Iteration 73/1000 | Loss: 0.00000897
Iteration 74/1000 | Loss: 0.00000896
Iteration 75/1000 | Loss: 0.00000895
Iteration 76/1000 | Loss: 0.00000895
Iteration 77/1000 | Loss: 0.00000894
Iteration 78/1000 | Loss: 0.00000894
Iteration 79/1000 | Loss: 0.00000893
Iteration 80/1000 | Loss: 0.00000893
Iteration 81/1000 | Loss: 0.00000893
Iteration 82/1000 | Loss: 0.00000893
Iteration 83/1000 | Loss: 0.00000893
Iteration 84/1000 | Loss: 0.00000893
Iteration 85/1000 | Loss: 0.00000893
Iteration 86/1000 | Loss: 0.00000893
Iteration 87/1000 | Loss: 0.00000892
Iteration 88/1000 | Loss: 0.00000891
Iteration 89/1000 | Loss: 0.00000890
Iteration 90/1000 | Loss: 0.00000889
Iteration 91/1000 | Loss: 0.00000889
Iteration 92/1000 | Loss: 0.00000889
Iteration 93/1000 | Loss: 0.00000889
Iteration 94/1000 | Loss: 0.00000888
Iteration 95/1000 | Loss: 0.00000888
Iteration 96/1000 | Loss: 0.00000888
Iteration 97/1000 | Loss: 0.00000888
Iteration 98/1000 | Loss: 0.00000887
Iteration 99/1000 | Loss: 0.00000887
Iteration 100/1000 | Loss: 0.00000887
Iteration 101/1000 | Loss: 0.00000887
Iteration 102/1000 | Loss: 0.00000887
Iteration 103/1000 | Loss: 0.00000886
Iteration 104/1000 | Loss: 0.00000886
Iteration 105/1000 | Loss: 0.00000886
Iteration 106/1000 | Loss: 0.00000886
Iteration 107/1000 | Loss: 0.00000886
Iteration 108/1000 | Loss: 0.00000886
Iteration 109/1000 | Loss: 0.00000885
Iteration 110/1000 | Loss: 0.00000885
Iteration 111/1000 | Loss: 0.00000885
Iteration 112/1000 | Loss: 0.00000885
Iteration 113/1000 | Loss: 0.00000885
Iteration 114/1000 | Loss: 0.00000885
Iteration 115/1000 | Loss: 0.00000884
Iteration 116/1000 | Loss: 0.00000884
Iteration 117/1000 | Loss: 0.00000884
Iteration 118/1000 | Loss: 0.00000883
Iteration 119/1000 | Loss: 0.00000883
Iteration 120/1000 | Loss: 0.00000883
Iteration 121/1000 | Loss: 0.00000883
Iteration 122/1000 | Loss: 0.00000883
Iteration 123/1000 | Loss: 0.00000883
Iteration 124/1000 | Loss: 0.00000883
Iteration 125/1000 | Loss: 0.00000883
Iteration 126/1000 | Loss: 0.00000883
Iteration 127/1000 | Loss: 0.00000883
Iteration 128/1000 | Loss: 0.00000883
Iteration 129/1000 | Loss: 0.00000883
Iteration 130/1000 | Loss: 0.00000883
Iteration 131/1000 | Loss: 0.00000883
Iteration 132/1000 | Loss: 0.00000883
Iteration 133/1000 | Loss: 0.00000882
Iteration 134/1000 | Loss: 0.00000882
Iteration 135/1000 | Loss: 0.00000882
Iteration 136/1000 | Loss: 0.00000882
Iteration 137/1000 | Loss: 0.00000882
Iteration 138/1000 | Loss: 0.00000882
Iteration 139/1000 | Loss: 0.00000882
Iteration 140/1000 | Loss: 0.00000882
Iteration 141/1000 | Loss: 0.00000882
Iteration 142/1000 | Loss: 0.00000882
Iteration 143/1000 | Loss: 0.00000882
Iteration 144/1000 | Loss: 0.00000882
Iteration 145/1000 | Loss: 0.00000882
Iteration 146/1000 | Loss: 0.00000881
Iteration 147/1000 | Loss: 0.00000881
Iteration 148/1000 | Loss: 0.00000881
Iteration 149/1000 | Loss: 0.00000881
Iteration 150/1000 | Loss: 0.00000881
Iteration 151/1000 | Loss: 0.00000881
Iteration 152/1000 | Loss: 0.00000881
Iteration 153/1000 | Loss: 0.00000881
Iteration 154/1000 | Loss: 0.00000881
Iteration 155/1000 | Loss: 0.00000881
Iteration 156/1000 | Loss: 0.00000881
Iteration 157/1000 | Loss: 0.00000881
Iteration 158/1000 | Loss: 0.00000880
Iteration 159/1000 | Loss: 0.00000880
Iteration 160/1000 | Loss: 0.00000880
Iteration 161/1000 | Loss: 0.00000880
Iteration 162/1000 | Loss: 0.00000880
Iteration 163/1000 | Loss: 0.00000880
Iteration 164/1000 | Loss: 0.00000880
Iteration 165/1000 | Loss: 0.00000880
Iteration 166/1000 | Loss: 0.00000879
Iteration 167/1000 | Loss: 0.00000879
Iteration 168/1000 | Loss: 0.00000879
Iteration 169/1000 | Loss: 0.00000879
Iteration 170/1000 | Loss: 0.00000879
Iteration 171/1000 | Loss: 0.00000879
Iteration 172/1000 | Loss: 0.00000879
Iteration 173/1000 | Loss: 0.00000879
Iteration 174/1000 | Loss: 0.00000879
Iteration 175/1000 | Loss: 0.00000879
Iteration 176/1000 | Loss: 0.00000879
Iteration 177/1000 | Loss: 0.00000879
Iteration 178/1000 | Loss: 0.00000878
Iteration 179/1000 | Loss: 0.00000878
Iteration 180/1000 | Loss: 0.00000878
Iteration 181/1000 | Loss: 0.00000878
Iteration 182/1000 | Loss: 0.00000878
Iteration 183/1000 | Loss: 0.00000878
Iteration 184/1000 | Loss: 0.00000878
Iteration 185/1000 | Loss: 0.00000878
Iteration 186/1000 | Loss: 0.00000878
Iteration 187/1000 | Loss: 0.00000878
Iteration 188/1000 | Loss: 0.00000878
Iteration 189/1000 | Loss: 0.00000878
Iteration 190/1000 | Loss: 0.00000878
Iteration 191/1000 | Loss: 0.00000878
Iteration 192/1000 | Loss: 0.00000878
Iteration 193/1000 | Loss: 0.00000877
Iteration 194/1000 | Loss: 0.00000877
Iteration 195/1000 | Loss: 0.00000877
Iteration 196/1000 | Loss: 0.00000877
Iteration 197/1000 | Loss: 0.00000877
Iteration 198/1000 | Loss: 0.00000877
Iteration 199/1000 | Loss: 0.00000877
Iteration 200/1000 | Loss: 0.00000877
Iteration 201/1000 | Loss: 0.00000877
Iteration 202/1000 | Loss: 0.00000876
Iteration 203/1000 | Loss: 0.00000876
Iteration 204/1000 | Loss: 0.00000876
Iteration 205/1000 | Loss: 0.00000876
Iteration 206/1000 | Loss: 0.00000876
Iteration 207/1000 | Loss: 0.00000876
Iteration 208/1000 | Loss: 0.00000876
Iteration 209/1000 | Loss: 0.00000876
Iteration 210/1000 | Loss: 0.00000876
Iteration 211/1000 | Loss: 0.00000876
Iteration 212/1000 | Loss: 0.00000876
Iteration 213/1000 | Loss: 0.00000876
Iteration 214/1000 | Loss: 0.00000875
Iteration 215/1000 | Loss: 0.00000875
Iteration 216/1000 | Loss: 0.00000875
Iteration 217/1000 | Loss: 0.00000875
Iteration 218/1000 | Loss: 0.00000875
Iteration 219/1000 | Loss: 0.00000875
Iteration 220/1000 | Loss: 0.00000875
Iteration 221/1000 | Loss: 0.00000875
Iteration 222/1000 | Loss: 0.00000875
Iteration 223/1000 | Loss: 0.00000875
Iteration 224/1000 | Loss: 0.00000875
Iteration 225/1000 | Loss: 0.00000875
Iteration 226/1000 | Loss: 0.00000874
Iteration 227/1000 | Loss: 0.00000874
Iteration 228/1000 | Loss: 0.00000874
Iteration 229/1000 | Loss: 0.00000874
Iteration 230/1000 | Loss: 0.00000874
Iteration 231/1000 | Loss: 0.00000874
Iteration 232/1000 | Loss: 0.00000874
Iteration 233/1000 | Loss: 0.00000874
Iteration 234/1000 | Loss: 0.00000874
Iteration 235/1000 | Loss: 0.00000874
Iteration 236/1000 | Loss: 0.00000874
Iteration 237/1000 | Loss: 0.00000874
Iteration 238/1000 | Loss: 0.00000874
Iteration 239/1000 | Loss: 0.00000874
Iteration 240/1000 | Loss: 0.00000874
Iteration 241/1000 | Loss: 0.00000874
Iteration 242/1000 | Loss: 0.00000874
Iteration 243/1000 | Loss: 0.00000874
Iteration 244/1000 | Loss: 0.00000874
Iteration 245/1000 | Loss: 0.00000873
Iteration 246/1000 | Loss: 0.00000873
Iteration 247/1000 | Loss: 0.00000873
Iteration 248/1000 | Loss: 0.00000873
Iteration 249/1000 | Loss: 0.00000873
Iteration 250/1000 | Loss: 0.00000873
Iteration 251/1000 | Loss: 0.00000873
Iteration 252/1000 | Loss: 0.00000873
Iteration 253/1000 | Loss: 0.00000873
Iteration 254/1000 | Loss: 0.00000873
Iteration 255/1000 | Loss: 0.00000873
Iteration 256/1000 | Loss: 0.00000873
Iteration 257/1000 | Loss: 0.00000873
Iteration 258/1000 | Loss: 0.00000873
Iteration 259/1000 | Loss: 0.00000873
Iteration 260/1000 | Loss: 0.00000873
Iteration 261/1000 | Loss: 0.00000873
Iteration 262/1000 | Loss: 0.00000873
Iteration 263/1000 | Loss: 0.00000873
Iteration 264/1000 | Loss: 0.00000873
Iteration 265/1000 | Loss: 0.00000873
Iteration 266/1000 | Loss: 0.00000873
Iteration 267/1000 | Loss: 0.00000873
Iteration 268/1000 | Loss: 0.00000873
Iteration 269/1000 | Loss: 0.00000873
Iteration 270/1000 | Loss: 0.00000873
Iteration 271/1000 | Loss: 0.00000873
Iteration 272/1000 | Loss: 0.00000873
Iteration 273/1000 | Loss: 0.00000873
Iteration 274/1000 | Loss: 0.00000873
Iteration 275/1000 | Loss: 0.00000873
Iteration 276/1000 | Loss: 0.00000873
Iteration 277/1000 | Loss: 0.00000873
Iteration 278/1000 | Loss: 0.00000873
Iteration 279/1000 | Loss: 0.00000873
Iteration 280/1000 | Loss: 0.00000873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [8.726048690732569e-06, 8.726048690732569e-06, 8.726048690732569e-06, 8.726048690732569e-06, 8.726048690732569e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.726048690732569e-06

Optimization complete. Final v2v error: 2.5245604515075684 mm

Highest mean error: 2.9288270473480225 mm for frame 67

Lowest mean error: 2.437171220779419 mm for frame 102

Saving results

Total time: 70.09691190719604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925277
Iteration 2/25 | Loss: 0.00096006
Iteration 3/25 | Loss: 0.00076268
Iteration 4/25 | Loss: 0.00072031
Iteration 5/25 | Loss: 0.00070118
Iteration 6/25 | Loss: 0.00069640
Iteration 7/25 | Loss: 0.00069498
Iteration 8/25 | Loss: 0.00069462
Iteration 9/25 | Loss: 0.00069462
Iteration 10/25 | Loss: 0.00069462
Iteration 11/25 | Loss: 0.00069462
Iteration 12/25 | Loss: 0.00069462
Iteration 13/25 | Loss: 0.00069462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006946173380129039, 0.0006946173380129039, 0.0006946173380129039, 0.0006946173380129039, 0.0006946173380129039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006946173380129039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45271480
Iteration 2/25 | Loss: 0.00026439
Iteration 3/25 | Loss: 0.00026437
Iteration 4/25 | Loss: 0.00026437
Iteration 5/25 | Loss: 0.00026437
Iteration 6/25 | Loss: 0.00026437
Iteration 7/25 | Loss: 0.00026437
Iteration 8/25 | Loss: 0.00026437
Iteration 9/25 | Loss: 0.00026437
Iteration 10/25 | Loss: 0.00026437
Iteration 11/25 | Loss: 0.00026437
Iteration 12/25 | Loss: 0.00026437
Iteration 13/25 | Loss: 0.00026437
Iteration 14/25 | Loss: 0.00026437
Iteration 15/25 | Loss: 0.00026437
Iteration 16/25 | Loss: 0.00026437
Iteration 17/25 | Loss: 0.00026437
Iteration 18/25 | Loss: 0.00026437
Iteration 19/25 | Loss: 0.00026437
Iteration 20/25 | Loss: 0.00026437
Iteration 21/25 | Loss: 0.00026437
Iteration 22/25 | Loss: 0.00026437
Iteration 23/25 | Loss: 0.00026437
Iteration 24/25 | Loss: 0.00026437
Iteration 25/25 | Loss: 0.00026437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026437
Iteration 2/1000 | Loss: 0.00004571
Iteration 3/1000 | Loss: 0.00003288
Iteration 4/1000 | Loss: 0.00002952
Iteration 5/1000 | Loss: 0.00002817
Iteration 6/1000 | Loss: 0.00002675
Iteration 7/1000 | Loss: 0.00002573
Iteration 8/1000 | Loss: 0.00002511
Iteration 9/1000 | Loss: 0.00002470
Iteration 10/1000 | Loss: 0.00002441
Iteration 11/1000 | Loss: 0.00002436
Iteration 12/1000 | Loss: 0.00002420
Iteration 13/1000 | Loss: 0.00002415
Iteration 14/1000 | Loss: 0.00002412
Iteration 15/1000 | Loss: 0.00002411
Iteration 16/1000 | Loss: 0.00002409
Iteration 17/1000 | Loss: 0.00002407
Iteration 18/1000 | Loss: 0.00002406
Iteration 19/1000 | Loss: 0.00002402
Iteration 20/1000 | Loss: 0.00002399
Iteration 21/1000 | Loss: 0.00002396
Iteration 22/1000 | Loss: 0.00002394
Iteration 23/1000 | Loss: 0.00002393
Iteration 24/1000 | Loss: 0.00002393
Iteration 25/1000 | Loss: 0.00002393
Iteration 26/1000 | Loss: 0.00002392
Iteration 27/1000 | Loss: 0.00002392
Iteration 28/1000 | Loss: 0.00002392
Iteration 29/1000 | Loss: 0.00002391
Iteration 30/1000 | Loss: 0.00002390
Iteration 31/1000 | Loss: 0.00002390
Iteration 32/1000 | Loss: 0.00002390
Iteration 33/1000 | Loss: 0.00002390
Iteration 34/1000 | Loss: 0.00002389
Iteration 35/1000 | Loss: 0.00002389
Iteration 36/1000 | Loss: 0.00002389
Iteration 37/1000 | Loss: 0.00002389
Iteration 38/1000 | Loss: 0.00002388
Iteration 39/1000 | Loss: 0.00002388
Iteration 40/1000 | Loss: 0.00002388
Iteration 41/1000 | Loss: 0.00002387
Iteration 42/1000 | Loss: 0.00002387
Iteration 43/1000 | Loss: 0.00002387
Iteration 44/1000 | Loss: 0.00002386
Iteration 45/1000 | Loss: 0.00002386
Iteration 46/1000 | Loss: 0.00002386
Iteration 47/1000 | Loss: 0.00002385
Iteration 48/1000 | Loss: 0.00002385
Iteration 49/1000 | Loss: 0.00002385
Iteration 50/1000 | Loss: 0.00002384
Iteration 51/1000 | Loss: 0.00002384
Iteration 52/1000 | Loss: 0.00002384
Iteration 53/1000 | Loss: 0.00002384
Iteration 54/1000 | Loss: 0.00002383
Iteration 55/1000 | Loss: 0.00002383
Iteration 56/1000 | Loss: 0.00002383
Iteration 57/1000 | Loss: 0.00002383
Iteration 58/1000 | Loss: 0.00002382
Iteration 59/1000 | Loss: 0.00002382
Iteration 60/1000 | Loss: 0.00002382
Iteration 61/1000 | Loss: 0.00002382
Iteration 62/1000 | Loss: 0.00002382
Iteration 63/1000 | Loss: 0.00002381
Iteration 64/1000 | Loss: 0.00002381
Iteration 65/1000 | Loss: 0.00002381
Iteration 66/1000 | Loss: 0.00002381
Iteration 67/1000 | Loss: 0.00002380
Iteration 68/1000 | Loss: 0.00002380
Iteration 69/1000 | Loss: 0.00002380
Iteration 70/1000 | Loss: 0.00002379
Iteration 71/1000 | Loss: 0.00002379
Iteration 72/1000 | Loss: 0.00002379
Iteration 73/1000 | Loss: 0.00002379
Iteration 74/1000 | Loss: 0.00002378
Iteration 75/1000 | Loss: 0.00002378
Iteration 76/1000 | Loss: 0.00002378
Iteration 77/1000 | Loss: 0.00002377
Iteration 78/1000 | Loss: 0.00002377
Iteration 79/1000 | Loss: 0.00002376
Iteration 80/1000 | Loss: 0.00002376
Iteration 81/1000 | Loss: 0.00002375
Iteration 82/1000 | Loss: 0.00002375
Iteration 83/1000 | Loss: 0.00002375
Iteration 84/1000 | Loss: 0.00002375
Iteration 85/1000 | Loss: 0.00002374
Iteration 86/1000 | Loss: 0.00002374
Iteration 87/1000 | Loss: 0.00002374
Iteration 88/1000 | Loss: 0.00002373
Iteration 89/1000 | Loss: 0.00002373
Iteration 90/1000 | Loss: 0.00002373
Iteration 91/1000 | Loss: 0.00002373
Iteration 92/1000 | Loss: 0.00002372
Iteration 93/1000 | Loss: 0.00002372
Iteration 94/1000 | Loss: 0.00002372
Iteration 95/1000 | Loss: 0.00002372
Iteration 96/1000 | Loss: 0.00002372
Iteration 97/1000 | Loss: 0.00002372
Iteration 98/1000 | Loss: 0.00002372
Iteration 99/1000 | Loss: 0.00002372
Iteration 100/1000 | Loss: 0.00002372
Iteration 101/1000 | Loss: 0.00002372
Iteration 102/1000 | Loss: 0.00002372
Iteration 103/1000 | Loss: 0.00002372
Iteration 104/1000 | Loss: 0.00002371
Iteration 105/1000 | Loss: 0.00002371
Iteration 106/1000 | Loss: 0.00002371
Iteration 107/1000 | Loss: 0.00002371
Iteration 108/1000 | Loss: 0.00002370
Iteration 109/1000 | Loss: 0.00002370
Iteration 110/1000 | Loss: 0.00002370
Iteration 111/1000 | Loss: 0.00002370
Iteration 112/1000 | Loss: 0.00002370
Iteration 113/1000 | Loss: 0.00002370
Iteration 114/1000 | Loss: 0.00002369
Iteration 115/1000 | Loss: 0.00002369
Iteration 116/1000 | Loss: 0.00002369
Iteration 117/1000 | Loss: 0.00002369
Iteration 118/1000 | Loss: 0.00002369
Iteration 119/1000 | Loss: 0.00002369
Iteration 120/1000 | Loss: 0.00002369
Iteration 121/1000 | Loss: 0.00002369
Iteration 122/1000 | Loss: 0.00002368
Iteration 123/1000 | Loss: 0.00002368
Iteration 124/1000 | Loss: 0.00002368
Iteration 125/1000 | Loss: 0.00002368
Iteration 126/1000 | Loss: 0.00002368
Iteration 127/1000 | Loss: 0.00002368
Iteration 128/1000 | Loss: 0.00002367
Iteration 129/1000 | Loss: 0.00002367
Iteration 130/1000 | Loss: 0.00002367
Iteration 131/1000 | Loss: 0.00002367
Iteration 132/1000 | Loss: 0.00002367
Iteration 133/1000 | Loss: 0.00002367
Iteration 134/1000 | Loss: 0.00002367
Iteration 135/1000 | Loss: 0.00002367
Iteration 136/1000 | Loss: 0.00002367
Iteration 137/1000 | Loss: 0.00002366
Iteration 138/1000 | Loss: 0.00002366
Iteration 139/1000 | Loss: 0.00002366
Iteration 140/1000 | Loss: 0.00002366
Iteration 141/1000 | Loss: 0.00002366
Iteration 142/1000 | Loss: 0.00002366
Iteration 143/1000 | Loss: 0.00002366
Iteration 144/1000 | Loss: 0.00002366
Iteration 145/1000 | Loss: 0.00002366
Iteration 146/1000 | Loss: 0.00002366
Iteration 147/1000 | Loss: 0.00002366
Iteration 148/1000 | Loss: 0.00002366
Iteration 149/1000 | Loss: 0.00002366
Iteration 150/1000 | Loss: 0.00002366
Iteration 151/1000 | Loss: 0.00002366
Iteration 152/1000 | Loss: 0.00002366
Iteration 153/1000 | Loss: 0.00002366
Iteration 154/1000 | Loss: 0.00002366
Iteration 155/1000 | Loss: 0.00002366
Iteration 156/1000 | Loss: 0.00002366
Iteration 157/1000 | Loss: 0.00002366
Iteration 158/1000 | Loss: 0.00002366
Iteration 159/1000 | Loss: 0.00002366
Iteration 160/1000 | Loss: 0.00002366
Iteration 161/1000 | Loss: 0.00002366
Iteration 162/1000 | Loss: 0.00002366
Iteration 163/1000 | Loss: 0.00002366
Iteration 164/1000 | Loss: 0.00002366
Iteration 165/1000 | Loss: 0.00002366
Iteration 166/1000 | Loss: 0.00002366
Iteration 167/1000 | Loss: 0.00002366
Iteration 168/1000 | Loss: 0.00002366
Iteration 169/1000 | Loss: 0.00002366
Iteration 170/1000 | Loss: 0.00002366
Iteration 171/1000 | Loss: 0.00002366
Iteration 172/1000 | Loss: 0.00002366
Iteration 173/1000 | Loss: 0.00002366
Iteration 174/1000 | Loss: 0.00002366
Iteration 175/1000 | Loss: 0.00002366
Iteration 176/1000 | Loss: 0.00002366
Iteration 177/1000 | Loss: 0.00002366
Iteration 178/1000 | Loss: 0.00002366
Iteration 179/1000 | Loss: 0.00002366
Iteration 180/1000 | Loss: 0.00002366
Iteration 181/1000 | Loss: 0.00002366
Iteration 182/1000 | Loss: 0.00002366
Iteration 183/1000 | Loss: 0.00002366
Iteration 184/1000 | Loss: 0.00002366
Iteration 185/1000 | Loss: 0.00002366
Iteration 186/1000 | Loss: 0.00002366
Iteration 187/1000 | Loss: 0.00002366
Iteration 188/1000 | Loss: 0.00002366
Iteration 189/1000 | Loss: 0.00002366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.3659744329052046e-05, 2.3659744329052046e-05, 2.3659744329052046e-05, 2.3659744329052046e-05, 2.3659744329052046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3659744329052046e-05

Optimization complete. Final v2v error: 4.0783305168151855 mm

Highest mean error: 5.781793117523193 mm for frame 69

Lowest mean error: 3.6150739192962646 mm for frame 93

Saving results

Total time: 69.99179911613464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436949
Iteration 2/25 | Loss: 0.00075108
Iteration 3/25 | Loss: 0.00064620
Iteration 4/25 | Loss: 0.00062594
Iteration 5/25 | Loss: 0.00062263
Iteration 6/25 | Loss: 0.00062122
Iteration 7/25 | Loss: 0.00062121
Iteration 8/25 | Loss: 0.00062121
Iteration 9/25 | Loss: 0.00062121
Iteration 10/25 | Loss: 0.00062121
Iteration 11/25 | Loss: 0.00062121
Iteration 12/25 | Loss: 0.00062121
Iteration 13/25 | Loss: 0.00062121
Iteration 14/25 | Loss: 0.00062121
Iteration 15/25 | Loss: 0.00062121
Iteration 16/25 | Loss: 0.00062121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006212090374901891, 0.0006212090374901891, 0.0006212090374901891, 0.0006212090374901891, 0.0006212090374901891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006212090374901891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46204066
Iteration 2/25 | Loss: 0.00029396
Iteration 3/25 | Loss: 0.00029396
Iteration 4/25 | Loss: 0.00029396
Iteration 5/25 | Loss: 0.00029396
Iteration 6/25 | Loss: 0.00029396
Iteration 7/25 | Loss: 0.00029396
Iteration 8/25 | Loss: 0.00029396
Iteration 9/25 | Loss: 0.00029396
Iteration 10/25 | Loss: 0.00029396
Iteration 11/25 | Loss: 0.00029396
Iteration 12/25 | Loss: 0.00029396
Iteration 13/25 | Loss: 0.00029396
Iteration 14/25 | Loss: 0.00029396
Iteration 15/25 | Loss: 0.00029396
Iteration 16/25 | Loss: 0.00029396
Iteration 17/25 | Loss: 0.00029396
Iteration 18/25 | Loss: 0.00029396
Iteration 19/25 | Loss: 0.00029396
Iteration 20/25 | Loss: 0.00029396
Iteration 21/25 | Loss: 0.00029396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002939609403256327, 0.0002939609403256327, 0.0002939609403256327, 0.0002939609403256327, 0.0002939609403256327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002939609403256327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029396
Iteration 2/1000 | Loss: 0.00002119
Iteration 3/1000 | Loss: 0.00001549
Iteration 4/1000 | Loss: 0.00001458
Iteration 5/1000 | Loss: 0.00001398
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001388
Iteration 8/1000 | Loss: 0.00001357
Iteration 9/1000 | Loss: 0.00001337
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001323
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001310
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001308
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001303
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001290
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001288
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001286
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001285
Iteration 60/1000 | Loss: 0.00001285
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001283
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001282
Iteration 73/1000 | Loss: 0.00001282
Iteration 74/1000 | Loss: 0.00001282
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001280
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001280
Iteration 88/1000 | Loss: 0.00001280
Iteration 89/1000 | Loss: 0.00001280
Iteration 90/1000 | Loss: 0.00001280
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001279
Iteration 93/1000 | Loss: 0.00001279
Iteration 94/1000 | Loss: 0.00001278
Iteration 95/1000 | Loss: 0.00001278
Iteration 96/1000 | Loss: 0.00001278
Iteration 97/1000 | Loss: 0.00001277
Iteration 98/1000 | Loss: 0.00001277
Iteration 99/1000 | Loss: 0.00001277
Iteration 100/1000 | Loss: 0.00001276
Iteration 101/1000 | Loss: 0.00001276
Iteration 102/1000 | Loss: 0.00001276
Iteration 103/1000 | Loss: 0.00001276
Iteration 104/1000 | Loss: 0.00001276
Iteration 105/1000 | Loss: 0.00001275
Iteration 106/1000 | Loss: 0.00001275
Iteration 107/1000 | Loss: 0.00001275
Iteration 108/1000 | Loss: 0.00001275
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001274
Iteration 114/1000 | Loss: 0.00001274
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001273
Iteration 117/1000 | Loss: 0.00001273
Iteration 118/1000 | Loss: 0.00001273
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001272
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001272
Iteration 130/1000 | Loss: 0.00001272
Iteration 131/1000 | Loss: 0.00001272
Iteration 132/1000 | Loss: 0.00001272
Iteration 133/1000 | Loss: 0.00001272
Iteration 134/1000 | Loss: 0.00001272
Iteration 135/1000 | Loss: 0.00001272
Iteration 136/1000 | Loss: 0.00001272
Iteration 137/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.2717218851321377e-05, 1.2717218851321377e-05, 1.2717218851321377e-05, 1.2717218851321377e-05, 1.2717218851321377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2717218851321377e-05

Optimization complete. Final v2v error: 3.0132651329040527 mm

Highest mean error: 3.3839023113250732 mm for frame 8

Lowest mean error: 2.799386978149414 mm for frame 189

Saving results

Total time: 90.58338618278503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075208
Iteration 2/25 | Loss: 0.00210121
Iteration 3/25 | Loss: 0.00121526
Iteration 4/25 | Loss: 0.00104072
Iteration 5/25 | Loss: 0.00107363
Iteration 6/25 | Loss: 0.00111221
Iteration 7/25 | Loss: 0.00105556
Iteration 8/25 | Loss: 0.00097502
Iteration 9/25 | Loss: 0.00094278
Iteration 10/25 | Loss: 0.00095369
Iteration 11/25 | Loss: 0.00086974
Iteration 12/25 | Loss: 0.00084154
Iteration 13/25 | Loss: 0.00082144
Iteration 14/25 | Loss: 0.00080414
Iteration 15/25 | Loss: 0.00080938
Iteration 16/25 | Loss: 0.00080265
Iteration 17/25 | Loss: 0.00080443
Iteration 18/25 | Loss: 0.00080072
Iteration 19/25 | Loss: 0.00080224
Iteration 20/25 | Loss: 0.00079549
Iteration 21/25 | Loss: 0.00079007
Iteration 22/25 | Loss: 0.00078841
Iteration 23/25 | Loss: 0.00078837
Iteration 24/25 | Loss: 0.00078415
Iteration 25/25 | Loss: 0.00077611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71466112
Iteration 2/25 | Loss: 0.00138256
Iteration 3/25 | Loss: 0.00138256
Iteration 4/25 | Loss: 0.00138255
Iteration 5/25 | Loss: 0.00138255
Iteration 6/25 | Loss: 0.00138255
Iteration 7/25 | Loss: 0.00138255
Iteration 8/25 | Loss: 0.00138255
Iteration 9/25 | Loss: 0.00138255
Iteration 10/25 | Loss: 0.00138255
Iteration 11/25 | Loss: 0.00138255
Iteration 12/25 | Loss: 0.00138255
Iteration 13/25 | Loss: 0.00138255
Iteration 14/25 | Loss: 0.00138255
Iteration 15/25 | Loss: 0.00138255
Iteration 16/25 | Loss: 0.00138255
Iteration 17/25 | Loss: 0.00138255
Iteration 18/25 | Loss: 0.00138255
Iteration 19/25 | Loss: 0.00138255
Iteration 20/25 | Loss: 0.00138255
Iteration 21/25 | Loss: 0.00138255
Iteration 22/25 | Loss: 0.00138255
Iteration 23/25 | Loss: 0.00138255
Iteration 24/25 | Loss: 0.00138255
Iteration 25/25 | Loss: 0.00138255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138255
Iteration 2/1000 | Loss: 0.00367914
Iteration 3/1000 | Loss: 0.00175237
Iteration 4/1000 | Loss: 0.00616552
Iteration 5/1000 | Loss: 0.00535954
Iteration 6/1000 | Loss: 0.00380770
Iteration 7/1000 | Loss: 0.00050632
Iteration 8/1000 | Loss: 0.00038971
Iteration 9/1000 | Loss: 0.00051706
Iteration 10/1000 | Loss: 0.00327870
Iteration 11/1000 | Loss: 0.00460906
Iteration 12/1000 | Loss: 0.00383669
Iteration 13/1000 | Loss: 0.00465788
Iteration 14/1000 | Loss: 0.00289978
Iteration 15/1000 | Loss: 0.00076949
Iteration 16/1000 | Loss: 0.00235783
Iteration 17/1000 | Loss: 0.00112437
Iteration 18/1000 | Loss: 0.00289680
Iteration 19/1000 | Loss: 0.00440990
Iteration 20/1000 | Loss: 0.00081091
Iteration 21/1000 | Loss: 0.00182223
Iteration 22/1000 | Loss: 0.00061386
Iteration 23/1000 | Loss: 0.00044287
Iteration 24/1000 | Loss: 0.00036792
Iteration 25/1000 | Loss: 0.00032117
Iteration 26/1000 | Loss: 0.00194867
Iteration 27/1000 | Loss: 0.00105454
Iteration 28/1000 | Loss: 0.00060446
Iteration 29/1000 | Loss: 0.00059913
Iteration 30/1000 | Loss: 0.00054003
Iteration 31/1000 | Loss: 0.00051659
Iteration 32/1000 | Loss: 0.00046147
Iteration 33/1000 | Loss: 0.00161200
Iteration 34/1000 | Loss: 0.00475599
Iteration 35/1000 | Loss: 0.00209107
Iteration 36/1000 | Loss: 0.00379661
Iteration 37/1000 | Loss: 0.00180819
Iteration 38/1000 | Loss: 0.00173881
Iteration 39/1000 | Loss: 0.00368963
Iteration 40/1000 | Loss: 0.00220980
Iteration 41/1000 | Loss: 0.00239200
Iteration 42/1000 | Loss: 0.00249761
Iteration 43/1000 | Loss: 0.00368552
Iteration 44/1000 | Loss: 0.00358038
Iteration 45/1000 | Loss: 0.00370351
Iteration 46/1000 | Loss: 0.00531860
Iteration 47/1000 | Loss: 0.00171025
Iteration 48/1000 | Loss: 0.00238833
Iteration 49/1000 | Loss: 0.00209691
Iteration 50/1000 | Loss: 0.00185379
Iteration 51/1000 | Loss: 0.00132793
Iteration 52/1000 | Loss: 0.00221625
Iteration 53/1000 | Loss: 0.00200426
Iteration 54/1000 | Loss: 0.00296466
Iteration 55/1000 | Loss: 0.00358978
Iteration 56/1000 | Loss: 0.00059377
Iteration 57/1000 | Loss: 0.00085585
Iteration 58/1000 | Loss: 0.00052965
Iteration 59/1000 | Loss: 0.00110850
Iteration 60/1000 | Loss: 0.00054610
Iteration 61/1000 | Loss: 0.00049990
Iteration 62/1000 | Loss: 0.00061741
Iteration 63/1000 | Loss: 0.00032822
Iteration 64/1000 | Loss: 0.00036876
Iteration 65/1000 | Loss: 0.00030410
Iteration 66/1000 | Loss: 0.00034574
Iteration 67/1000 | Loss: 0.00029174
Iteration 68/1000 | Loss: 0.00193367
Iteration 69/1000 | Loss: 0.00134495
Iteration 70/1000 | Loss: 0.00089454
Iteration 71/1000 | Loss: 0.00037927
Iteration 72/1000 | Loss: 0.00036003
Iteration 73/1000 | Loss: 0.00019215
Iteration 74/1000 | Loss: 0.00035088
Iteration 75/1000 | Loss: 0.00055116
Iteration 76/1000 | Loss: 0.00054117
Iteration 77/1000 | Loss: 0.00052018
Iteration 78/1000 | Loss: 0.00022359
Iteration 79/1000 | Loss: 0.00028150
Iteration 80/1000 | Loss: 0.00019025
Iteration 81/1000 | Loss: 0.00022673
Iteration 82/1000 | Loss: 0.00023622
Iteration 83/1000 | Loss: 0.00037969
Iteration 84/1000 | Loss: 0.00028039
Iteration 85/1000 | Loss: 0.00020828
Iteration 86/1000 | Loss: 0.00008994
Iteration 87/1000 | Loss: 0.00020171
Iteration 88/1000 | Loss: 0.00020361
Iteration 89/1000 | Loss: 0.00015875
Iteration 90/1000 | Loss: 0.00040194
Iteration 91/1000 | Loss: 0.00013913
Iteration 92/1000 | Loss: 0.00018597
Iteration 93/1000 | Loss: 0.00028182
Iteration 94/1000 | Loss: 0.00026630
Iteration 95/1000 | Loss: 0.00003940
Iteration 96/1000 | Loss: 0.00005396
Iteration 97/1000 | Loss: 0.00005477
Iteration 98/1000 | Loss: 0.00006377
Iteration 99/1000 | Loss: 0.00003909
Iteration 100/1000 | Loss: 0.00004849
Iteration 101/1000 | Loss: 0.00030352
Iteration 102/1000 | Loss: 0.00010385
Iteration 103/1000 | Loss: 0.00021765
Iteration 104/1000 | Loss: 0.00018741
Iteration 105/1000 | Loss: 0.00006069
Iteration 106/1000 | Loss: 0.00005131
Iteration 107/1000 | Loss: 0.00006066
Iteration 108/1000 | Loss: 0.00018242
Iteration 109/1000 | Loss: 0.00007185
Iteration 110/1000 | Loss: 0.00017913
Iteration 111/1000 | Loss: 0.00034558
Iteration 112/1000 | Loss: 0.00016789
Iteration 113/1000 | Loss: 0.00027497
Iteration 114/1000 | Loss: 0.00024512
Iteration 115/1000 | Loss: 0.00022384
Iteration 116/1000 | Loss: 0.00026783
Iteration 117/1000 | Loss: 0.00018173
Iteration 118/1000 | Loss: 0.00023961
Iteration 119/1000 | Loss: 0.00019029
Iteration 120/1000 | Loss: 0.00006151
Iteration 121/1000 | Loss: 0.00017773
Iteration 122/1000 | Loss: 0.00017998
Iteration 123/1000 | Loss: 0.00015020
Iteration 124/1000 | Loss: 0.00007935
Iteration 125/1000 | Loss: 0.00019566
Iteration 126/1000 | Loss: 0.00015274
Iteration 127/1000 | Loss: 0.00014443
Iteration 128/1000 | Loss: 0.00021222
Iteration 129/1000 | Loss: 0.00013856
Iteration 130/1000 | Loss: 0.00017564
Iteration 131/1000 | Loss: 0.00012003
Iteration 132/1000 | Loss: 0.00017061
Iteration 133/1000 | Loss: 0.00012173
Iteration 134/1000 | Loss: 0.00016277
Iteration 135/1000 | Loss: 0.00012946
Iteration 136/1000 | Loss: 0.00014785
Iteration 137/1000 | Loss: 0.00015107
Iteration 138/1000 | Loss: 0.00017244
Iteration 139/1000 | Loss: 0.00014832
Iteration 140/1000 | Loss: 0.00018603
Iteration 141/1000 | Loss: 0.00015840
Iteration 142/1000 | Loss: 0.00019365
Iteration 143/1000 | Loss: 0.00013740
Iteration 144/1000 | Loss: 0.00019821
Iteration 145/1000 | Loss: 0.00010586
Iteration 146/1000 | Loss: 0.00012672
Iteration 147/1000 | Loss: 0.00007047
Iteration 148/1000 | Loss: 0.00013809
Iteration 149/1000 | Loss: 0.00015971
Iteration 150/1000 | Loss: 0.00018937
Iteration 151/1000 | Loss: 0.00015468
Iteration 152/1000 | Loss: 0.00023643
Iteration 153/1000 | Loss: 0.00017681
Iteration 154/1000 | Loss: 0.00005001
Iteration 155/1000 | Loss: 0.00004148
Iteration 156/1000 | Loss: 0.00027765
Iteration 157/1000 | Loss: 0.00005595
Iteration 158/1000 | Loss: 0.00007773
Iteration 159/1000 | Loss: 0.00003745
Iteration 160/1000 | Loss: 0.00004464
Iteration 161/1000 | Loss: 0.00002932
Iteration 162/1000 | Loss: 0.00003062
Iteration 163/1000 | Loss: 0.00004557
Iteration 164/1000 | Loss: 0.00004398
Iteration 165/1000 | Loss: 0.00003526
Iteration 166/1000 | Loss: 0.00003899
Iteration 167/1000 | Loss: 0.00003619
Iteration 168/1000 | Loss: 0.00003755
Iteration 169/1000 | Loss: 0.00003819
Iteration 170/1000 | Loss: 0.00003565
Iteration 171/1000 | Loss: 0.00004034
Iteration 172/1000 | Loss: 0.00003737
Iteration 173/1000 | Loss: 0.00003895
Iteration 174/1000 | Loss: 0.00003607
Iteration 175/1000 | Loss: 0.00003802
Iteration 176/1000 | Loss: 0.00004097
Iteration 177/1000 | Loss: 0.00002645
Iteration 178/1000 | Loss: 0.00005487
Iteration 179/1000 | Loss: 0.00002337
Iteration 180/1000 | Loss: 0.00001953
Iteration 181/1000 | Loss: 0.00001647
Iteration 182/1000 | Loss: 0.00001603
Iteration 183/1000 | Loss: 0.00001601
Iteration 184/1000 | Loss: 0.00001579
Iteration 185/1000 | Loss: 0.00001549
Iteration 186/1000 | Loss: 0.00001519
Iteration 187/1000 | Loss: 0.00001491
Iteration 188/1000 | Loss: 0.00001476
Iteration 189/1000 | Loss: 0.00001468
Iteration 190/1000 | Loss: 0.00001468
Iteration 191/1000 | Loss: 0.00001465
Iteration 192/1000 | Loss: 0.00001465
Iteration 193/1000 | Loss: 0.00001463
Iteration 194/1000 | Loss: 0.00001462
Iteration 195/1000 | Loss: 0.00001460
Iteration 196/1000 | Loss: 0.00001457
Iteration 197/1000 | Loss: 0.00001455
Iteration 198/1000 | Loss: 0.00001454
Iteration 199/1000 | Loss: 0.00001453
Iteration 200/1000 | Loss: 0.00001452
Iteration 201/1000 | Loss: 0.00001452
Iteration 202/1000 | Loss: 0.00001450
Iteration 203/1000 | Loss: 0.00001450
Iteration 204/1000 | Loss: 0.00001450
Iteration 205/1000 | Loss: 0.00001449
Iteration 206/1000 | Loss: 0.00001449
Iteration 207/1000 | Loss: 0.00001449
Iteration 208/1000 | Loss: 0.00001448
Iteration 209/1000 | Loss: 0.00001448
Iteration 210/1000 | Loss: 0.00001448
Iteration 211/1000 | Loss: 0.00001448
Iteration 212/1000 | Loss: 0.00001448
Iteration 213/1000 | Loss: 0.00001448
Iteration 214/1000 | Loss: 0.00001447
Iteration 215/1000 | Loss: 0.00001445
Iteration 216/1000 | Loss: 0.00001445
Iteration 217/1000 | Loss: 0.00001445
Iteration 218/1000 | Loss: 0.00001445
Iteration 219/1000 | Loss: 0.00001444
Iteration 220/1000 | Loss: 0.00001444
Iteration 221/1000 | Loss: 0.00001443
Iteration 222/1000 | Loss: 0.00001443
Iteration 223/1000 | Loss: 0.00001442
Iteration 224/1000 | Loss: 0.00001442
Iteration 225/1000 | Loss: 0.00001442
Iteration 226/1000 | Loss: 0.00001442
Iteration 227/1000 | Loss: 0.00001441
Iteration 228/1000 | Loss: 0.00001441
Iteration 229/1000 | Loss: 0.00001441
Iteration 230/1000 | Loss: 0.00001441
Iteration 231/1000 | Loss: 0.00001441
Iteration 232/1000 | Loss: 0.00001441
Iteration 233/1000 | Loss: 0.00001441
Iteration 234/1000 | Loss: 0.00001441
Iteration 235/1000 | Loss: 0.00001441
Iteration 236/1000 | Loss: 0.00001441
Iteration 237/1000 | Loss: 0.00001441
Iteration 238/1000 | Loss: 0.00001441
Iteration 239/1000 | Loss: 0.00001441
Iteration 240/1000 | Loss: 0.00001440
Iteration 241/1000 | Loss: 0.00001440
Iteration 242/1000 | Loss: 0.00001440
Iteration 243/1000 | Loss: 0.00001440
Iteration 244/1000 | Loss: 0.00001440
Iteration 245/1000 | Loss: 0.00001440
Iteration 246/1000 | Loss: 0.00001440
Iteration 247/1000 | Loss: 0.00001440
Iteration 248/1000 | Loss: 0.00001440
Iteration 249/1000 | Loss: 0.00001440
Iteration 250/1000 | Loss: 0.00001440
Iteration 251/1000 | Loss: 0.00001440
Iteration 252/1000 | Loss: 0.00001440
Iteration 253/1000 | Loss: 0.00001439
Iteration 254/1000 | Loss: 0.00001439
Iteration 255/1000 | Loss: 0.00001439
Iteration 256/1000 | Loss: 0.00001439
Iteration 257/1000 | Loss: 0.00001439
Iteration 258/1000 | Loss: 0.00001439
Iteration 259/1000 | Loss: 0.00001439
Iteration 260/1000 | Loss: 0.00001439
Iteration 261/1000 | Loss: 0.00001439
Iteration 262/1000 | Loss: 0.00001439
Iteration 263/1000 | Loss: 0.00001439
Iteration 264/1000 | Loss: 0.00001439
Iteration 265/1000 | Loss: 0.00001439
Iteration 266/1000 | Loss: 0.00001439
Iteration 267/1000 | Loss: 0.00001439
Iteration 268/1000 | Loss: 0.00001439
Iteration 269/1000 | Loss: 0.00001439
Iteration 270/1000 | Loss: 0.00001439
Iteration 271/1000 | Loss: 0.00001439
Iteration 272/1000 | Loss: 0.00001439
Iteration 273/1000 | Loss: 0.00001439
Iteration 274/1000 | Loss: 0.00001439
Iteration 275/1000 | Loss: 0.00001439
Iteration 276/1000 | Loss: 0.00001439
Iteration 277/1000 | Loss: 0.00001439
Iteration 278/1000 | Loss: 0.00001439
Iteration 279/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.439174866391113e-05, 1.439174866391113e-05, 1.439174866391113e-05, 1.439174866391113e-05, 1.439174866391113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.439174866391113e-05

Optimization complete. Final v2v error: 3.1411566734313965 mm

Highest mean error: 5.30202054977417 mm for frame 94

Lowest mean error: 2.756110429763794 mm for frame 19

Saving results

Total time: 405.0456268787384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814529
Iteration 2/25 | Loss: 0.00086142
Iteration 3/25 | Loss: 0.00065576
Iteration 4/25 | Loss: 0.00062159
Iteration 5/25 | Loss: 0.00061481
Iteration 6/25 | Loss: 0.00061228
Iteration 7/25 | Loss: 0.00061212
Iteration 8/25 | Loss: 0.00061212
Iteration 9/25 | Loss: 0.00061212
Iteration 10/25 | Loss: 0.00061212
Iteration 11/25 | Loss: 0.00061212
Iteration 12/25 | Loss: 0.00061212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006121151964180171, 0.0006121151964180171, 0.0006121151964180171, 0.0006121151964180171, 0.0006121151964180171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006121151964180171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46762633
Iteration 2/25 | Loss: 0.00033213
Iteration 3/25 | Loss: 0.00033213
Iteration 4/25 | Loss: 0.00033213
Iteration 5/25 | Loss: 0.00033213
Iteration 6/25 | Loss: 0.00033213
Iteration 7/25 | Loss: 0.00033212
Iteration 8/25 | Loss: 0.00033212
Iteration 9/25 | Loss: 0.00033212
Iteration 10/25 | Loss: 0.00033212
Iteration 11/25 | Loss: 0.00033212
Iteration 12/25 | Loss: 0.00033212
Iteration 13/25 | Loss: 0.00033212
Iteration 14/25 | Loss: 0.00033212
Iteration 15/25 | Loss: 0.00033212
Iteration 16/25 | Loss: 0.00033212
Iteration 17/25 | Loss: 0.00033212
Iteration 18/25 | Loss: 0.00033212
Iteration 19/25 | Loss: 0.00033212
Iteration 20/25 | Loss: 0.00033212
Iteration 21/25 | Loss: 0.00033212
Iteration 22/25 | Loss: 0.00033212
Iteration 23/25 | Loss: 0.00033212
Iteration 24/25 | Loss: 0.00033212
Iteration 25/25 | Loss: 0.00033212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033212
Iteration 2/1000 | Loss: 0.00002191
Iteration 3/1000 | Loss: 0.00001402
Iteration 4/1000 | Loss: 0.00001299
Iteration 5/1000 | Loss: 0.00001224
Iteration 6/1000 | Loss: 0.00001175
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001125
Iteration 9/1000 | Loss: 0.00001124
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00001120
Iteration 12/1000 | Loss: 0.00001114
Iteration 13/1000 | Loss: 0.00001109
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001109
Iteration 16/1000 | Loss: 0.00001099
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001096
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001090
Iteration 21/1000 | Loss: 0.00001088
Iteration 22/1000 | Loss: 0.00001086
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001083
Iteration 27/1000 | Loss: 0.00001080
Iteration 28/1000 | Loss: 0.00001080
Iteration 29/1000 | Loss: 0.00001080
Iteration 30/1000 | Loss: 0.00001079
Iteration 31/1000 | Loss: 0.00001079
Iteration 32/1000 | Loss: 0.00001079
Iteration 33/1000 | Loss: 0.00001079
Iteration 34/1000 | Loss: 0.00001078
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001077
Iteration 37/1000 | Loss: 0.00001076
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001076
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001074
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001072
Iteration 48/1000 | Loss: 0.00001069
Iteration 49/1000 | Loss: 0.00001068
Iteration 50/1000 | Loss: 0.00001068
Iteration 51/1000 | Loss: 0.00001067
Iteration 52/1000 | Loss: 0.00001067
Iteration 53/1000 | Loss: 0.00001066
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001066
Iteration 56/1000 | Loss: 0.00001065
Iteration 57/1000 | Loss: 0.00001065
Iteration 58/1000 | Loss: 0.00001065
Iteration 59/1000 | Loss: 0.00001064
Iteration 60/1000 | Loss: 0.00001064
Iteration 61/1000 | Loss: 0.00001064
Iteration 62/1000 | Loss: 0.00001064
Iteration 63/1000 | Loss: 0.00001064
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001063
Iteration 66/1000 | Loss: 0.00001063
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001062
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001062
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001061
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001061
Iteration 86/1000 | Loss: 0.00001061
Iteration 87/1000 | Loss: 0.00001060
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001060
Iteration 90/1000 | Loss: 0.00001060
Iteration 91/1000 | Loss: 0.00001060
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Iteration 100/1000 | Loss: 0.00001059
Iteration 101/1000 | Loss: 0.00001059
Iteration 102/1000 | Loss: 0.00001059
Iteration 103/1000 | Loss: 0.00001059
Iteration 104/1000 | Loss: 0.00001059
Iteration 105/1000 | Loss: 0.00001059
Iteration 106/1000 | Loss: 0.00001059
Iteration 107/1000 | Loss: 0.00001059
Iteration 108/1000 | Loss: 0.00001059
Iteration 109/1000 | Loss: 0.00001059
Iteration 110/1000 | Loss: 0.00001058
Iteration 111/1000 | Loss: 0.00001058
Iteration 112/1000 | Loss: 0.00001058
Iteration 113/1000 | Loss: 0.00001058
Iteration 114/1000 | Loss: 0.00001058
Iteration 115/1000 | Loss: 0.00001057
Iteration 116/1000 | Loss: 0.00001057
Iteration 117/1000 | Loss: 0.00001057
Iteration 118/1000 | Loss: 0.00001057
Iteration 119/1000 | Loss: 0.00001057
Iteration 120/1000 | Loss: 0.00001057
Iteration 121/1000 | Loss: 0.00001057
Iteration 122/1000 | Loss: 0.00001057
Iteration 123/1000 | Loss: 0.00001057
Iteration 124/1000 | Loss: 0.00001057
Iteration 125/1000 | Loss: 0.00001057
Iteration 126/1000 | Loss: 0.00001057
Iteration 127/1000 | Loss: 0.00001057
Iteration 128/1000 | Loss: 0.00001057
Iteration 129/1000 | Loss: 0.00001057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.056568362400867e-05, 1.056568362400867e-05, 1.056568362400867e-05, 1.056568362400867e-05, 1.056568362400867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.056568362400867e-05

Optimization complete. Final v2v error: 2.7386367321014404 mm

Highest mean error: 3.0834035873413086 mm for frame 101

Lowest mean error: 2.566110849380493 mm for frame 214

Saving results

Total time: 69.96507906913757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826054
Iteration 2/25 | Loss: 0.00110847
Iteration 3/25 | Loss: 0.00071466
Iteration 4/25 | Loss: 0.00066486
Iteration 5/25 | Loss: 0.00065106
Iteration 6/25 | Loss: 0.00064801
Iteration 7/25 | Loss: 0.00064768
Iteration 8/25 | Loss: 0.00064768
Iteration 9/25 | Loss: 0.00064768
Iteration 10/25 | Loss: 0.00064768
Iteration 11/25 | Loss: 0.00064768
Iteration 12/25 | Loss: 0.00064768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006476793787442148, 0.0006476793787442148, 0.0006476793787442148, 0.0006476793787442148, 0.0006476793787442148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006476793787442148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19429731
Iteration 2/25 | Loss: 0.00025031
Iteration 3/25 | Loss: 0.00025031
Iteration 4/25 | Loss: 0.00025031
Iteration 5/25 | Loss: 0.00025031
Iteration 6/25 | Loss: 0.00025031
Iteration 7/25 | Loss: 0.00025031
Iteration 8/25 | Loss: 0.00025031
Iteration 9/25 | Loss: 0.00025031
Iteration 10/25 | Loss: 0.00025031
Iteration 11/25 | Loss: 0.00025031
Iteration 12/25 | Loss: 0.00025031
Iteration 13/25 | Loss: 0.00025031
Iteration 14/25 | Loss: 0.00025031
Iteration 15/25 | Loss: 0.00025031
Iteration 16/25 | Loss: 0.00025031
Iteration 17/25 | Loss: 0.00025031
Iteration 18/25 | Loss: 0.00025031
Iteration 19/25 | Loss: 0.00025031
Iteration 20/25 | Loss: 0.00025031
Iteration 21/25 | Loss: 0.00025031
Iteration 22/25 | Loss: 0.00025031
Iteration 23/25 | Loss: 0.00025031
Iteration 24/25 | Loss: 0.00025031
Iteration 25/25 | Loss: 0.00025031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025031
Iteration 2/1000 | Loss: 0.00003847
Iteration 3/1000 | Loss: 0.00002782
Iteration 4/1000 | Loss: 0.00002400
Iteration 5/1000 | Loss: 0.00002272
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002096
Iteration 8/1000 | Loss: 0.00002038
Iteration 9/1000 | Loss: 0.00001991
Iteration 10/1000 | Loss: 0.00001964
Iteration 11/1000 | Loss: 0.00001939
Iteration 12/1000 | Loss: 0.00001924
Iteration 13/1000 | Loss: 0.00001913
Iteration 14/1000 | Loss: 0.00001905
Iteration 15/1000 | Loss: 0.00001897
Iteration 16/1000 | Loss: 0.00001886
Iteration 17/1000 | Loss: 0.00001885
Iteration 18/1000 | Loss: 0.00001884
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001883
Iteration 25/1000 | Loss: 0.00001883
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001882
Iteration 32/1000 | Loss: 0.00001882
Iteration 33/1000 | Loss: 0.00001882
Iteration 34/1000 | Loss: 0.00001882
Iteration 35/1000 | Loss: 0.00001881
Iteration 36/1000 | Loss: 0.00001879
Iteration 37/1000 | Loss: 0.00001879
Iteration 38/1000 | Loss: 0.00001878
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001878
Iteration 41/1000 | Loss: 0.00001877
Iteration 42/1000 | Loss: 0.00001877
Iteration 43/1000 | Loss: 0.00001877
Iteration 44/1000 | Loss: 0.00001877
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001876
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001874
Iteration 52/1000 | Loss: 0.00001874
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00001873
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001873
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001872
Iteration 59/1000 | Loss: 0.00001872
Iteration 60/1000 | Loss: 0.00001871
Iteration 61/1000 | Loss: 0.00001871
Iteration 62/1000 | Loss: 0.00001871
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001870
Iteration 68/1000 | Loss: 0.00001869
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001868
Iteration 72/1000 | Loss: 0.00001868
Iteration 73/1000 | Loss: 0.00001868
Iteration 74/1000 | Loss: 0.00001868
Iteration 75/1000 | Loss: 0.00001868
Iteration 76/1000 | Loss: 0.00001868
Iteration 77/1000 | Loss: 0.00001868
Iteration 78/1000 | Loss: 0.00001867
Iteration 79/1000 | Loss: 0.00001867
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001866
Iteration 82/1000 | Loss: 0.00001866
Iteration 83/1000 | Loss: 0.00001866
Iteration 84/1000 | Loss: 0.00001866
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001865
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001864
Iteration 91/1000 | Loss: 0.00001864
Iteration 92/1000 | Loss: 0.00001864
Iteration 93/1000 | Loss: 0.00001864
Iteration 94/1000 | Loss: 0.00001863
Iteration 95/1000 | Loss: 0.00001863
Iteration 96/1000 | Loss: 0.00001863
Iteration 97/1000 | Loss: 0.00001863
Iteration 98/1000 | Loss: 0.00001863
Iteration 99/1000 | Loss: 0.00001862
Iteration 100/1000 | Loss: 0.00001862
Iteration 101/1000 | Loss: 0.00001862
Iteration 102/1000 | Loss: 0.00001862
Iteration 103/1000 | Loss: 0.00001862
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001860
Iteration 108/1000 | Loss: 0.00001860
Iteration 109/1000 | Loss: 0.00001860
Iteration 110/1000 | Loss: 0.00001860
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001859
Iteration 113/1000 | Loss: 0.00001859
Iteration 114/1000 | Loss: 0.00001859
Iteration 115/1000 | Loss: 0.00001859
Iteration 116/1000 | Loss: 0.00001859
Iteration 117/1000 | Loss: 0.00001859
Iteration 118/1000 | Loss: 0.00001859
Iteration 119/1000 | Loss: 0.00001859
Iteration 120/1000 | Loss: 0.00001859
Iteration 121/1000 | Loss: 0.00001859
Iteration 122/1000 | Loss: 0.00001859
Iteration 123/1000 | Loss: 0.00001859
Iteration 124/1000 | Loss: 0.00001858
Iteration 125/1000 | Loss: 0.00001858
Iteration 126/1000 | Loss: 0.00001858
Iteration 127/1000 | Loss: 0.00001858
Iteration 128/1000 | Loss: 0.00001858
Iteration 129/1000 | Loss: 0.00001858
Iteration 130/1000 | Loss: 0.00001858
Iteration 131/1000 | Loss: 0.00001858
Iteration 132/1000 | Loss: 0.00001858
Iteration 133/1000 | Loss: 0.00001858
Iteration 134/1000 | Loss: 0.00001858
Iteration 135/1000 | Loss: 0.00001857
Iteration 136/1000 | Loss: 0.00001857
Iteration 137/1000 | Loss: 0.00001857
Iteration 138/1000 | Loss: 0.00001857
Iteration 139/1000 | Loss: 0.00001857
Iteration 140/1000 | Loss: 0.00001857
Iteration 141/1000 | Loss: 0.00001857
Iteration 142/1000 | Loss: 0.00001857
Iteration 143/1000 | Loss: 0.00001857
Iteration 144/1000 | Loss: 0.00001857
Iteration 145/1000 | Loss: 0.00001857
Iteration 146/1000 | Loss: 0.00001857
Iteration 147/1000 | Loss: 0.00001857
Iteration 148/1000 | Loss: 0.00001857
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001857
Iteration 151/1000 | Loss: 0.00001857
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.856727794802282e-05, 1.856727794802282e-05, 1.856727794802282e-05, 1.856727794802282e-05, 1.856727794802282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.856727794802282e-05

Optimization complete. Final v2v error: 3.6058876514434814 mm

Highest mean error: 4.907385349273682 mm for frame 82

Lowest mean error: 2.8054394721984863 mm for frame 196

Saving results

Total time: 59.9341561794281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826952
Iteration 2/25 | Loss: 0.00081312
Iteration 3/25 | Loss: 0.00064164
Iteration 4/25 | Loss: 0.00061532
Iteration 5/25 | Loss: 0.00060841
Iteration 6/25 | Loss: 0.00060584
Iteration 7/25 | Loss: 0.00060504
Iteration 8/25 | Loss: 0.00060504
Iteration 9/25 | Loss: 0.00060504
Iteration 10/25 | Loss: 0.00060504
Iteration 11/25 | Loss: 0.00060504
Iteration 12/25 | Loss: 0.00060504
Iteration 13/25 | Loss: 0.00060504
Iteration 14/25 | Loss: 0.00060504
Iteration 15/25 | Loss: 0.00060504
Iteration 16/25 | Loss: 0.00060504
Iteration 17/25 | Loss: 0.00060504
Iteration 18/25 | Loss: 0.00060504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006050445372238755, 0.0006050445372238755, 0.0006050445372238755, 0.0006050445372238755, 0.0006050445372238755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006050445372238755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46396065
Iteration 2/25 | Loss: 0.00026158
Iteration 3/25 | Loss: 0.00026158
Iteration 4/25 | Loss: 0.00026158
Iteration 5/25 | Loss: 0.00026158
Iteration 6/25 | Loss: 0.00026158
Iteration 7/25 | Loss: 0.00026158
Iteration 8/25 | Loss: 0.00026157
Iteration 9/25 | Loss: 0.00026157
Iteration 10/25 | Loss: 0.00026157
Iteration 11/25 | Loss: 0.00026157
Iteration 12/25 | Loss: 0.00026157
Iteration 13/25 | Loss: 0.00026157
Iteration 14/25 | Loss: 0.00026157
Iteration 15/25 | Loss: 0.00026157
Iteration 16/25 | Loss: 0.00026157
Iteration 17/25 | Loss: 0.00026157
Iteration 18/25 | Loss: 0.00026157
Iteration 19/25 | Loss: 0.00026157
Iteration 20/25 | Loss: 0.00026157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002615740813780576, 0.0002615740813780576, 0.0002615740813780576, 0.0002615740813780576, 0.0002615740813780576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002615740813780576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026157
Iteration 2/1000 | Loss: 0.00002417
Iteration 3/1000 | Loss: 0.00001473
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00001291
Iteration 6/1000 | Loss: 0.00001241
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001200
Iteration 9/1000 | Loss: 0.00001199
Iteration 10/1000 | Loss: 0.00001199
Iteration 11/1000 | Loss: 0.00001199
Iteration 12/1000 | Loss: 0.00001197
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001191
Iteration 16/1000 | Loss: 0.00001190
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001184
Iteration 19/1000 | Loss: 0.00001182
Iteration 20/1000 | Loss: 0.00001176
Iteration 21/1000 | Loss: 0.00001175
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001170
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001170
Iteration 27/1000 | Loss: 0.00001170
Iteration 28/1000 | Loss: 0.00001170
Iteration 29/1000 | Loss: 0.00001169
Iteration 30/1000 | Loss: 0.00001169
Iteration 31/1000 | Loss: 0.00001169
Iteration 32/1000 | Loss: 0.00001169
Iteration 33/1000 | Loss: 0.00001169
Iteration 34/1000 | Loss: 0.00001169
Iteration 35/1000 | Loss: 0.00001169
Iteration 36/1000 | Loss: 0.00001169
Iteration 37/1000 | Loss: 0.00001169
Iteration 38/1000 | Loss: 0.00001169
Iteration 39/1000 | Loss: 0.00001168
Iteration 40/1000 | Loss: 0.00001168
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001165
Iteration 43/1000 | Loss: 0.00001165
Iteration 44/1000 | Loss: 0.00001165
Iteration 45/1000 | Loss: 0.00001164
Iteration 46/1000 | Loss: 0.00001164
Iteration 47/1000 | Loss: 0.00001164
Iteration 48/1000 | Loss: 0.00001164
Iteration 49/1000 | Loss: 0.00001164
Iteration 50/1000 | Loss: 0.00001163
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001159
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001158
Iteration 58/1000 | Loss: 0.00001158
Iteration 59/1000 | Loss: 0.00001158
Iteration 60/1000 | Loss: 0.00001157
Iteration 61/1000 | Loss: 0.00001157
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001155
Iteration 64/1000 | Loss: 0.00001155
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001154
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001153
Iteration 72/1000 | Loss: 0.00001153
Iteration 73/1000 | Loss: 0.00001153
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001151
Iteration 84/1000 | Loss: 0.00001150
Iteration 85/1000 | Loss: 0.00001150
Iteration 86/1000 | Loss: 0.00001150
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001148
Iteration 91/1000 | Loss: 0.00001148
Iteration 92/1000 | Loss: 0.00001148
Iteration 93/1000 | Loss: 0.00001148
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001146
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001145
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001144
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001142
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001142
Iteration 133/1000 | Loss: 0.00001142
Iteration 134/1000 | Loss: 0.00001142
Iteration 135/1000 | Loss: 0.00001142
Iteration 136/1000 | Loss: 0.00001142
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.1422177522035781e-05, 1.1422177522035781e-05, 1.1422177522035781e-05, 1.1422177522035781e-05, 1.1422177522035781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1422177522035781e-05

Optimization complete. Final v2v error: 2.785698652267456 mm

Highest mean error: 4.123274326324463 mm for frame 79

Lowest mean error: 2.388291120529175 mm for frame 217

Saving results

Total time: 54.07722496986389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406777
Iteration 2/25 | Loss: 0.00083874
Iteration 3/25 | Loss: 0.00065966
Iteration 4/25 | Loss: 0.00063484
Iteration 5/25 | Loss: 0.00062844
Iteration 6/25 | Loss: 0.00062775
Iteration 7/25 | Loss: 0.00062775
Iteration 8/25 | Loss: 0.00062775
Iteration 9/25 | Loss: 0.00062775
Iteration 10/25 | Loss: 0.00062775
Iteration 11/25 | Loss: 0.00062775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006277472712099552, 0.0006277472712099552, 0.0006277472712099552, 0.0006277472712099552, 0.0006277472712099552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006277472712099552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49930310
Iteration 2/25 | Loss: 0.00031208
Iteration 3/25 | Loss: 0.00031208
Iteration 4/25 | Loss: 0.00031208
Iteration 5/25 | Loss: 0.00031208
Iteration 6/25 | Loss: 0.00031208
Iteration 7/25 | Loss: 0.00031208
Iteration 8/25 | Loss: 0.00031208
Iteration 9/25 | Loss: 0.00031208
Iteration 10/25 | Loss: 0.00031208
Iteration 11/25 | Loss: 0.00031208
Iteration 12/25 | Loss: 0.00031208
Iteration 13/25 | Loss: 0.00031208
Iteration 14/25 | Loss: 0.00031208
Iteration 15/25 | Loss: 0.00031208
Iteration 16/25 | Loss: 0.00031208
Iteration 17/25 | Loss: 0.00031208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031207629945129156, 0.00031207629945129156, 0.00031207629945129156, 0.00031207629945129156, 0.00031207629945129156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031207629945129156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031208
Iteration 2/1000 | Loss: 0.00002297
Iteration 3/1000 | Loss: 0.00001571
Iteration 4/1000 | Loss: 0.00001484
Iteration 5/1000 | Loss: 0.00001420
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001346
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001330
Iteration 17/1000 | Loss: 0.00001329
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001328
Iteration 21/1000 | Loss: 0.00001328
Iteration 22/1000 | Loss: 0.00001328
Iteration 23/1000 | Loss: 0.00001327
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001323
Iteration 30/1000 | Loss: 0.00001322
Iteration 31/1000 | Loss: 0.00001322
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001321
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001318
Iteration 36/1000 | Loss: 0.00001318
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001316
Iteration 39/1000 | Loss: 0.00001316
Iteration 40/1000 | Loss: 0.00001316
Iteration 41/1000 | Loss: 0.00001316
Iteration 42/1000 | Loss: 0.00001316
Iteration 43/1000 | Loss: 0.00001316
Iteration 44/1000 | Loss: 0.00001316
Iteration 45/1000 | Loss: 0.00001315
Iteration 46/1000 | Loss: 0.00001315
Iteration 47/1000 | Loss: 0.00001315
Iteration 48/1000 | Loss: 0.00001315
Iteration 49/1000 | Loss: 0.00001314
Iteration 50/1000 | Loss: 0.00001314
Iteration 51/1000 | Loss: 0.00001314
Iteration 52/1000 | Loss: 0.00001314
Iteration 53/1000 | Loss: 0.00001313
Iteration 54/1000 | Loss: 0.00001313
Iteration 55/1000 | Loss: 0.00001313
Iteration 56/1000 | Loss: 0.00001313
Iteration 57/1000 | Loss: 0.00001312
Iteration 58/1000 | Loss: 0.00001312
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00001312
Iteration 61/1000 | Loss: 0.00001312
Iteration 62/1000 | Loss: 0.00001311
Iteration 63/1000 | Loss: 0.00001311
Iteration 64/1000 | Loss: 0.00001311
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001310
Iteration 68/1000 | Loss: 0.00001310
Iteration 69/1000 | Loss: 0.00001310
Iteration 70/1000 | Loss: 0.00001310
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001309
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001306
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001306
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001304
Iteration 94/1000 | Loss: 0.00001304
Iteration 95/1000 | Loss: 0.00001304
Iteration 96/1000 | Loss: 0.00001304
Iteration 97/1000 | Loss: 0.00001304
Iteration 98/1000 | Loss: 0.00001304
Iteration 99/1000 | Loss: 0.00001304
Iteration 100/1000 | Loss: 0.00001303
Iteration 101/1000 | Loss: 0.00001303
Iteration 102/1000 | Loss: 0.00001303
Iteration 103/1000 | Loss: 0.00001302
Iteration 104/1000 | Loss: 0.00001302
Iteration 105/1000 | Loss: 0.00001302
Iteration 106/1000 | Loss: 0.00001302
Iteration 107/1000 | Loss: 0.00001302
Iteration 108/1000 | Loss: 0.00001302
Iteration 109/1000 | Loss: 0.00001302
Iteration 110/1000 | Loss: 0.00001301
Iteration 111/1000 | Loss: 0.00001301
Iteration 112/1000 | Loss: 0.00001301
Iteration 113/1000 | Loss: 0.00001301
Iteration 114/1000 | Loss: 0.00001301
Iteration 115/1000 | Loss: 0.00001301
Iteration 116/1000 | Loss: 0.00001301
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001300
Iteration 121/1000 | Loss: 0.00001300
Iteration 122/1000 | Loss: 0.00001300
Iteration 123/1000 | Loss: 0.00001300
Iteration 124/1000 | Loss: 0.00001300
Iteration 125/1000 | Loss: 0.00001300
Iteration 126/1000 | Loss: 0.00001299
Iteration 127/1000 | Loss: 0.00001299
Iteration 128/1000 | Loss: 0.00001299
Iteration 129/1000 | Loss: 0.00001299
Iteration 130/1000 | Loss: 0.00001299
Iteration 131/1000 | Loss: 0.00001299
Iteration 132/1000 | Loss: 0.00001299
Iteration 133/1000 | Loss: 0.00001299
Iteration 134/1000 | Loss: 0.00001299
Iteration 135/1000 | Loss: 0.00001299
Iteration 136/1000 | Loss: 0.00001299
Iteration 137/1000 | Loss: 0.00001299
Iteration 138/1000 | Loss: 0.00001299
Iteration 139/1000 | Loss: 0.00001299
Iteration 140/1000 | Loss: 0.00001299
Iteration 141/1000 | Loss: 0.00001298
Iteration 142/1000 | Loss: 0.00001298
Iteration 143/1000 | Loss: 0.00001298
Iteration 144/1000 | Loss: 0.00001298
Iteration 145/1000 | Loss: 0.00001298
Iteration 146/1000 | Loss: 0.00001298
Iteration 147/1000 | Loss: 0.00001298
Iteration 148/1000 | Loss: 0.00001298
Iteration 149/1000 | Loss: 0.00001298
Iteration 150/1000 | Loss: 0.00001298
Iteration 151/1000 | Loss: 0.00001298
Iteration 152/1000 | Loss: 0.00001298
Iteration 153/1000 | Loss: 0.00001298
Iteration 154/1000 | Loss: 0.00001298
Iteration 155/1000 | Loss: 0.00001298
Iteration 156/1000 | Loss: 0.00001298
Iteration 157/1000 | Loss: 0.00001298
Iteration 158/1000 | Loss: 0.00001298
Iteration 159/1000 | Loss: 0.00001298
Iteration 160/1000 | Loss: 0.00001298
Iteration 161/1000 | Loss: 0.00001298
Iteration 162/1000 | Loss: 0.00001298
Iteration 163/1000 | Loss: 0.00001298
Iteration 164/1000 | Loss: 0.00001298
Iteration 165/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2980116480321158e-05, 1.2980116480321158e-05, 1.2980116480321158e-05, 1.2980116480321158e-05, 1.2980116480321158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2980116480321158e-05

Optimization complete. Final v2v error: 3.0664312839508057 mm

Highest mean error: 3.1977460384368896 mm for frame 142

Lowest mean error: 2.953040599822998 mm for frame 186

Saving results

Total time: 34.80862545967102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785249
Iteration 2/25 | Loss: 0.00141637
Iteration 3/25 | Loss: 0.00083528
Iteration 4/25 | Loss: 0.00099620
Iteration 5/25 | Loss: 0.00071153
Iteration 6/25 | Loss: 0.00069762
Iteration 7/25 | Loss: 0.00069508
Iteration 8/25 | Loss: 0.00069439
Iteration 9/25 | Loss: 0.00069425
Iteration 10/25 | Loss: 0.00069417
Iteration 11/25 | Loss: 0.00069416
Iteration 12/25 | Loss: 0.00069416
Iteration 13/25 | Loss: 0.00069416
Iteration 14/25 | Loss: 0.00069416
Iteration 15/25 | Loss: 0.00069416
Iteration 16/25 | Loss: 0.00069416
Iteration 17/25 | Loss: 0.00069416
Iteration 18/25 | Loss: 0.00069416
Iteration 19/25 | Loss: 0.00069416
Iteration 20/25 | Loss: 0.00069416
Iteration 21/25 | Loss: 0.00069416
Iteration 22/25 | Loss: 0.00069416
Iteration 23/25 | Loss: 0.00069416
Iteration 24/25 | Loss: 0.00069416
Iteration 25/25 | Loss: 0.00069415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.88046741
Iteration 2/25 | Loss: 0.00033626
Iteration 3/25 | Loss: 0.00033626
Iteration 4/25 | Loss: 0.00033626
Iteration 5/25 | Loss: 0.00033626
Iteration 6/25 | Loss: 0.00033626
Iteration 7/25 | Loss: 0.00033626
Iteration 8/25 | Loss: 0.00033626
Iteration 9/25 | Loss: 0.00033626
Iteration 10/25 | Loss: 0.00033626
Iteration 11/25 | Loss: 0.00033626
Iteration 12/25 | Loss: 0.00033626
Iteration 13/25 | Loss: 0.00033626
Iteration 14/25 | Loss: 0.00033626
Iteration 15/25 | Loss: 0.00033626
Iteration 16/25 | Loss: 0.00033626
Iteration 17/25 | Loss: 0.00033626
Iteration 18/25 | Loss: 0.00033626
Iteration 19/25 | Loss: 0.00033626
Iteration 20/25 | Loss: 0.00033626
Iteration 21/25 | Loss: 0.00033626
Iteration 22/25 | Loss: 0.00033626
Iteration 23/25 | Loss: 0.00033626
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00033625602372922003, 0.00033625602372922003, 0.00033625602372922003, 0.00033625602372922003, 0.00033625602372922003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033625602372922003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033626
Iteration 2/1000 | Loss: 0.00003946
Iteration 3/1000 | Loss: 0.00193133
Iteration 4/1000 | Loss: 0.00050811
Iteration 5/1000 | Loss: 0.00002806
Iteration 6/1000 | Loss: 0.00002427
Iteration 7/1000 | Loss: 0.00002276
Iteration 8/1000 | Loss: 0.00209840
Iteration 9/1000 | Loss: 0.00169781
Iteration 10/1000 | Loss: 0.00049776
Iteration 11/1000 | Loss: 0.00017792
Iteration 12/1000 | Loss: 0.00008158
Iteration 13/1000 | Loss: 0.00027074
Iteration 14/1000 | Loss: 0.00043532
Iteration 15/1000 | Loss: 0.00177856
Iteration 16/1000 | Loss: 0.00088573
Iteration 17/1000 | Loss: 0.00005252
Iteration 18/1000 | Loss: 0.00196677
Iteration 19/1000 | Loss: 0.00124130
Iteration 20/1000 | Loss: 0.00055652
Iteration 21/1000 | Loss: 0.00153266
Iteration 22/1000 | Loss: 0.00059829
Iteration 23/1000 | Loss: 0.00075167
Iteration 24/1000 | Loss: 0.00011125
Iteration 25/1000 | Loss: 0.00013919
Iteration 26/1000 | Loss: 0.00003507
Iteration 27/1000 | Loss: 0.00002823
Iteration 28/1000 | Loss: 0.00002420
Iteration 29/1000 | Loss: 0.00002255
Iteration 30/1000 | Loss: 0.00002180
Iteration 31/1000 | Loss: 0.00002145
Iteration 32/1000 | Loss: 0.00002114
Iteration 33/1000 | Loss: 0.00002381
Iteration 34/1000 | Loss: 0.00002293
Iteration 35/1000 | Loss: 0.00002424
Iteration 36/1000 | Loss: 0.00002472
Iteration 37/1000 | Loss: 0.00002528
Iteration 38/1000 | Loss: 0.00002474
Iteration 39/1000 | Loss: 0.00002491
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002632
Iteration 42/1000 | Loss: 0.00002452
Iteration 43/1000 | Loss: 0.00002735
Iteration 44/1000 | Loss: 0.00002475
Iteration 45/1000 | Loss: 0.00002682
Iteration 46/1000 | Loss: 0.00002324
Iteration 47/1000 | Loss: 0.00002584
Iteration 48/1000 | Loss: 0.00002425
Iteration 49/1000 | Loss: 0.00002268
Iteration 50/1000 | Loss: 0.00002221
Iteration 51/1000 | Loss: 0.00002269
Iteration 52/1000 | Loss: 0.00002055
Iteration 53/1000 | Loss: 0.00001982
Iteration 54/1000 | Loss: 0.00002026
Iteration 55/1000 | Loss: 0.00002362
Iteration 56/1000 | Loss: 0.00002666
Iteration 57/1000 | Loss: 0.00002272
Iteration 58/1000 | Loss: 0.00002114
Iteration 59/1000 | Loss: 0.00002142
Iteration 60/1000 | Loss: 0.00002336
Iteration 61/1000 | Loss: 0.00002832
Iteration 62/1000 | Loss: 0.00002313
Iteration 63/1000 | Loss: 0.00002901
Iteration 64/1000 | Loss: 0.00002282
Iteration 65/1000 | Loss: 0.00002913
Iteration 66/1000 | Loss: 0.00002263
Iteration 67/1000 | Loss: 0.00002754
Iteration 68/1000 | Loss: 0.00002329
Iteration 69/1000 | Loss: 0.00002218
Iteration 70/1000 | Loss: 0.00002745
Iteration 71/1000 | Loss: 0.00002361
Iteration 72/1000 | Loss: 0.00002271
Iteration 73/1000 | Loss: 0.00002757
Iteration 74/1000 | Loss: 0.00002261
Iteration 75/1000 | Loss: 0.00002831
Iteration 76/1000 | Loss: 0.00002280
Iteration 77/1000 | Loss: 0.00002760
Iteration 78/1000 | Loss: 0.00002338
Iteration 79/1000 | Loss: 0.00002240
Iteration 80/1000 | Loss: 0.00002875
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002076
Iteration 83/1000 | Loss: 0.00002023
Iteration 84/1000 | Loss: 0.00002009
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001950
Iteration 92/1000 | Loss: 0.00001948
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001935
Iteration 95/1000 | Loss: 0.00001934
Iteration 96/1000 | Loss: 0.00001932
Iteration 97/1000 | Loss: 0.00001932
Iteration 98/1000 | Loss: 0.00001932
Iteration 99/1000 | Loss: 0.00001932
Iteration 100/1000 | Loss: 0.00001932
Iteration 101/1000 | Loss: 0.00001932
Iteration 102/1000 | Loss: 0.00001932
Iteration 103/1000 | Loss: 0.00001932
Iteration 104/1000 | Loss: 0.00001931
Iteration 105/1000 | Loss: 0.00001931
Iteration 106/1000 | Loss: 0.00001931
Iteration 107/1000 | Loss: 0.00001931
Iteration 108/1000 | Loss: 0.00001931
Iteration 109/1000 | Loss: 0.00001931
Iteration 110/1000 | Loss: 0.00001930
Iteration 111/1000 | Loss: 0.00001929
Iteration 112/1000 | Loss: 0.00001929
Iteration 113/1000 | Loss: 0.00001929
Iteration 114/1000 | Loss: 0.00001929
Iteration 115/1000 | Loss: 0.00001928
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001928
Iteration 118/1000 | Loss: 0.00001928
Iteration 119/1000 | Loss: 0.00001928
Iteration 120/1000 | Loss: 0.00001927
Iteration 121/1000 | Loss: 0.00001927
Iteration 122/1000 | Loss: 0.00001927
Iteration 123/1000 | Loss: 0.00001927
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00001927
Iteration 126/1000 | Loss: 0.00001926
Iteration 127/1000 | Loss: 0.00001926
Iteration 128/1000 | Loss: 0.00001926
Iteration 129/1000 | Loss: 0.00001925
Iteration 130/1000 | Loss: 0.00001925
Iteration 131/1000 | Loss: 0.00001925
Iteration 132/1000 | Loss: 0.00001924
Iteration 133/1000 | Loss: 0.00001924
Iteration 134/1000 | Loss: 0.00001924
Iteration 135/1000 | Loss: 0.00001924
Iteration 136/1000 | Loss: 0.00001924
Iteration 137/1000 | Loss: 0.00001923
Iteration 138/1000 | Loss: 0.00001923
Iteration 139/1000 | Loss: 0.00001922
Iteration 140/1000 | Loss: 0.00001922
Iteration 141/1000 | Loss: 0.00001922
Iteration 142/1000 | Loss: 0.00001921
Iteration 143/1000 | Loss: 0.00001921
Iteration 144/1000 | Loss: 0.00001921
Iteration 145/1000 | Loss: 0.00001921
Iteration 146/1000 | Loss: 0.00001921
Iteration 147/1000 | Loss: 0.00001920
Iteration 148/1000 | Loss: 0.00001920
Iteration 149/1000 | Loss: 0.00001920
Iteration 150/1000 | Loss: 0.00001920
Iteration 151/1000 | Loss: 0.00001919
Iteration 152/1000 | Loss: 0.00001919
Iteration 153/1000 | Loss: 0.00001919
Iteration 154/1000 | Loss: 0.00001918
Iteration 155/1000 | Loss: 0.00001918
Iteration 156/1000 | Loss: 0.00001918
Iteration 157/1000 | Loss: 0.00001917
Iteration 158/1000 | Loss: 0.00001917
Iteration 159/1000 | Loss: 0.00001917
Iteration 160/1000 | Loss: 0.00001917
Iteration 161/1000 | Loss: 0.00001917
Iteration 162/1000 | Loss: 0.00001917
Iteration 163/1000 | Loss: 0.00001917
Iteration 164/1000 | Loss: 0.00001917
Iteration 165/1000 | Loss: 0.00001917
Iteration 166/1000 | Loss: 0.00001917
Iteration 167/1000 | Loss: 0.00001916
Iteration 168/1000 | Loss: 0.00001916
Iteration 169/1000 | Loss: 0.00001916
Iteration 170/1000 | Loss: 0.00001916
Iteration 171/1000 | Loss: 0.00001916
Iteration 172/1000 | Loss: 0.00001915
Iteration 173/1000 | Loss: 0.00001915
Iteration 174/1000 | Loss: 0.00001915
Iteration 175/1000 | Loss: 0.00001915
Iteration 176/1000 | Loss: 0.00001915
Iteration 177/1000 | Loss: 0.00001915
Iteration 178/1000 | Loss: 0.00001915
Iteration 179/1000 | Loss: 0.00001915
Iteration 180/1000 | Loss: 0.00001915
Iteration 181/1000 | Loss: 0.00001915
Iteration 182/1000 | Loss: 0.00001915
Iteration 183/1000 | Loss: 0.00001915
Iteration 184/1000 | Loss: 0.00001915
Iteration 185/1000 | Loss: 0.00001915
Iteration 186/1000 | Loss: 0.00001915
Iteration 187/1000 | Loss: 0.00001915
Iteration 188/1000 | Loss: 0.00001915
Iteration 189/1000 | Loss: 0.00001914
Iteration 190/1000 | Loss: 0.00001914
Iteration 191/1000 | Loss: 0.00001914
Iteration 192/1000 | Loss: 0.00001914
Iteration 193/1000 | Loss: 0.00001914
Iteration 194/1000 | Loss: 0.00001914
Iteration 195/1000 | Loss: 0.00001914
Iteration 196/1000 | Loss: 0.00001914
Iteration 197/1000 | Loss: 0.00001914
Iteration 198/1000 | Loss: 0.00001914
Iteration 199/1000 | Loss: 0.00001914
Iteration 200/1000 | Loss: 0.00001914
Iteration 201/1000 | Loss: 0.00001914
Iteration 202/1000 | Loss: 0.00001914
Iteration 203/1000 | Loss: 0.00001914
Iteration 204/1000 | Loss: 0.00001914
Iteration 205/1000 | Loss: 0.00001914
Iteration 206/1000 | Loss: 0.00001914
Iteration 207/1000 | Loss: 0.00001914
Iteration 208/1000 | Loss: 0.00001914
Iteration 209/1000 | Loss: 0.00001914
Iteration 210/1000 | Loss: 0.00001914
Iteration 211/1000 | Loss: 0.00001914
Iteration 212/1000 | Loss: 0.00001914
Iteration 213/1000 | Loss: 0.00001914
Iteration 214/1000 | Loss: 0.00001914
Iteration 215/1000 | Loss: 0.00001914
Iteration 216/1000 | Loss: 0.00001914
Iteration 217/1000 | Loss: 0.00001914
Iteration 218/1000 | Loss: 0.00001914
Iteration 219/1000 | Loss: 0.00001914
Iteration 220/1000 | Loss: 0.00001914
Iteration 221/1000 | Loss: 0.00001914
Iteration 222/1000 | Loss: 0.00001914
Iteration 223/1000 | Loss: 0.00001914
Iteration 224/1000 | Loss: 0.00001914
Iteration 225/1000 | Loss: 0.00001914
Iteration 226/1000 | Loss: 0.00001914
Iteration 227/1000 | Loss: 0.00001914
Iteration 228/1000 | Loss: 0.00001914
Iteration 229/1000 | Loss: 0.00001914
Iteration 230/1000 | Loss: 0.00001914
Iteration 231/1000 | Loss: 0.00001914
Iteration 232/1000 | Loss: 0.00001914
Iteration 233/1000 | Loss: 0.00001914
Iteration 234/1000 | Loss: 0.00001914
Iteration 235/1000 | Loss: 0.00001914
Iteration 236/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.9135935872327536e-05, 1.9135935872327536e-05, 1.9135935872327536e-05, 1.9135935872327536e-05, 1.9135935872327536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9135935872327536e-05

Optimization complete. Final v2v error: 3.5791046619415283 mm

Highest mean error: 4.72984504699707 mm for frame 27

Lowest mean error: 2.790140390396118 mm for frame 47

Saving results

Total time: 199.2922945022583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428025
Iteration 2/25 | Loss: 0.00085407
Iteration 3/25 | Loss: 0.00072034
Iteration 4/25 | Loss: 0.00069336
Iteration 5/25 | Loss: 0.00068784
Iteration 6/25 | Loss: 0.00068594
Iteration 7/25 | Loss: 0.00068510
Iteration 8/25 | Loss: 0.00068507
Iteration 9/25 | Loss: 0.00068507
Iteration 10/25 | Loss: 0.00068507
Iteration 11/25 | Loss: 0.00068507
Iteration 12/25 | Loss: 0.00068507
Iteration 13/25 | Loss: 0.00068507
Iteration 14/25 | Loss: 0.00068507
Iteration 15/25 | Loss: 0.00068507
Iteration 16/25 | Loss: 0.00068507
Iteration 17/25 | Loss: 0.00068507
Iteration 18/25 | Loss: 0.00068507
Iteration 19/25 | Loss: 0.00068507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006850679055787623, 0.0006850679055787623, 0.0006850679055787623, 0.0006850679055787623, 0.0006850679055787623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006850679055787623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85595703
Iteration 2/25 | Loss: 0.00031064
Iteration 3/25 | Loss: 0.00031064
Iteration 4/25 | Loss: 0.00031064
Iteration 5/25 | Loss: 0.00031064
Iteration 6/25 | Loss: 0.00031064
Iteration 7/25 | Loss: 0.00031064
Iteration 8/25 | Loss: 0.00031064
Iteration 9/25 | Loss: 0.00031064
Iteration 10/25 | Loss: 0.00031064
Iteration 11/25 | Loss: 0.00031064
Iteration 12/25 | Loss: 0.00031064
Iteration 13/25 | Loss: 0.00031064
Iteration 14/25 | Loss: 0.00031064
Iteration 15/25 | Loss: 0.00031064
Iteration 16/25 | Loss: 0.00031064
Iteration 17/25 | Loss: 0.00031064
Iteration 18/25 | Loss: 0.00031064
Iteration 19/25 | Loss: 0.00031064
Iteration 20/25 | Loss: 0.00031064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003106353397015482, 0.0003106353397015482, 0.0003106353397015482, 0.0003106353397015482, 0.0003106353397015482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003106353397015482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031064
Iteration 2/1000 | Loss: 0.00003157
Iteration 3/1000 | Loss: 0.00002196
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001953
Iteration 7/1000 | Loss: 0.00001912
Iteration 8/1000 | Loss: 0.00001882
Iteration 9/1000 | Loss: 0.00001866
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001852
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001838
Iteration 15/1000 | Loss: 0.00001837
Iteration 16/1000 | Loss: 0.00001837
Iteration 17/1000 | Loss: 0.00001836
Iteration 18/1000 | Loss: 0.00001836
Iteration 19/1000 | Loss: 0.00001831
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001827
Iteration 22/1000 | Loss: 0.00001827
Iteration 23/1000 | Loss: 0.00001826
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001825
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001824
Iteration 29/1000 | Loss: 0.00001824
Iteration 30/1000 | Loss: 0.00001823
Iteration 31/1000 | Loss: 0.00001823
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001821
Iteration 37/1000 | Loss: 0.00001821
Iteration 38/1000 | Loss: 0.00001821
Iteration 39/1000 | Loss: 0.00001821
Iteration 40/1000 | Loss: 0.00001821
Iteration 41/1000 | Loss: 0.00001821
Iteration 42/1000 | Loss: 0.00001821
Iteration 43/1000 | Loss: 0.00001821
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00001820
Iteration 46/1000 | Loss: 0.00001820
Iteration 47/1000 | Loss: 0.00001820
Iteration 48/1000 | Loss: 0.00001819
Iteration 49/1000 | Loss: 0.00001819
Iteration 50/1000 | Loss: 0.00001819
Iteration 51/1000 | Loss: 0.00001819
Iteration 52/1000 | Loss: 0.00001819
Iteration 53/1000 | Loss: 0.00001818
Iteration 54/1000 | Loss: 0.00001818
Iteration 55/1000 | Loss: 0.00001818
Iteration 56/1000 | Loss: 0.00001818
Iteration 57/1000 | Loss: 0.00001818
Iteration 58/1000 | Loss: 0.00001818
Iteration 59/1000 | Loss: 0.00001817
Iteration 60/1000 | Loss: 0.00001814
Iteration 61/1000 | Loss: 0.00001814
Iteration 62/1000 | Loss: 0.00001814
Iteration 63/1000 | Loss: 0.00001814
Iteration 64/1000 | Loss: 0.00001814
Iteration 65/1000 | Loss: 0.00001813
Iteration 66/1000 | Loss: 0.00001813
Iteration 67/1000 | Loss: 0.00001813
Iteration 68/1000 | Loss: 0.00001812
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001805
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Iteration 86/1000 | Loss: 0.00001805
Iteration 87/1000 | Loss: 0.00001805
Iteration 88/1000 | Loss: 0.00001805
Iteration 89/1000 | Loss: 0.00001805
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001805
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001804
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001804
Iteration 97/1000 | Loss: 0.00001803
Iteration 98/1000 | Loss: 0.00001803
Iteration 99/1000 | Loss: 0.00001803
Iteration 100/1000 | Loss: 0.00001802
Iteration 101/1000 | Loss: 0.00001802
Iteration 102/1000 | Loss: 0.00001802
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001802
Iteration 105/1000 | Loss: 0.00001802
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001802
Iteration 109/1000 | Loss: 0.00001802
Iteration 110/1000 | Loss: 0.00001801
Iteration 111/1000 | Loss: 0.00001801
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001800
Iteration 118/1000 | Loss: 0.00001800
Iteration 119/1000 | Loss: 0.00001800
Iteration 120/1000 | Loss: 0.00001799
Iteration 121/1000 | Loss: 0.00001799
Iteration 122/1000 | Loss: 0.00001799
Iteration 123/1000 | Loss: 0.00001799
Iteration 124/1000 | Loss: 0.00001799
Iteration 125/1000 | Loss: 0.00001799
Iteration 126/1000 | Loss: 0.00001799
Iteration 127/1000 | Loss: 0.00001799
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001799
Iteration 130/1000 | Loss: 0.00001799
Iteration 131/1000 | Loss: 0.00001798
Iteration 132/1000 | Loss: 0.00001798
Iteration 133/1000 | Loss: 0.00001798
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Iteration 142/1000 | Loss: 0.00001798
Iteration 143/1000 | Loss: 0.00001798
Iteration 144/1000 | Loss: 0.00001798
Iteration 145/1000 | Loss: 0.00001797
Iteration 146/1000 | Loss: 0.00001797
Iteration 147/1000 | Loss: 0.00001797
Iteration 148/1000 | Loss: 0.00001797
Iteration 149/1000 | Loss: 0.00001797
Iteration 150/1000 | Loss: 0.00001797
Iteration 151/1000 | Loss: 0.00001797
Iteration 152/1000 | Loss: 0.00001797
Iteration 153/1000 | Loss: 0.00001797
Iteration 154/1000 | Loss: 0.00001797
Iteration 155/1000 | Loss: 0.00001797
Iteration 156/1000 | Loss: 0.00001797
Iteration 157/1000 | Loss: 0.00001797
Iteration 158/1000 | Loss: 0.00001797
Iteration 159/1000 | Loss: 0.00001796
Iteration 160/1000 | Loss: 0.00001796
Iteration 161/1000 | Loss: 0.00001796
Iteration 162/1000 | Loss: 0.00001796
Iteration 163/1000 | Loss: 0.00001796
Iteration 164/1000 | Loss: 0.00001796
Iteration 165/1000 | Loss: 0.00001796
Iteration 166/1000 | Loss: 0.00001796
Iteration 167/1000 | Loss: 0.00001796
Iteration 168/1000 | Loss: 0.00001796
Iteration 169/1000 | Loss: 0.00001796
Iteration 170/1000 | Loss: 0.00001796
Iteration 171/1000 | Loss: 0.00001795
Iteration 172/1000 | Loss: 0.00001795
Iteration 173/1000 | Loss: 0.00001795
Iteration 174/1000 | Loss: 0.00001795
Iteration 175/1000 | Loss: 0.00001795
Iteration 176/1000 | Loss: 0.00001795
Iteration 177/1000 | Loss: 0.00001795
Iteration 178/1000 | Loss: 0.00001795
Iteration 179/1000 | Loss: 0.00001795
Iteration 180/1000 | Loss: 0.00001795
Iteration 181/1000 | Loss: 0.00001795
Iteration 182/1000 | Loss: 0.00001795
Iteration 183/1000 | Loss: 0.00001795
Iteration 184/1000 | Loss: 0.00001795
Iteration 185/1000 | Loss: 0.00001795
Iteration 186/1000 | Loss: 0.00001795
Iteration 187/1000 | Loss: 0.00001795
Iteration 188/1000 | Loss: 0.00001794
Iteration 189/1000 | Loss: 0.00001794
Iteration 190/1000 | Loss: 0.00001794
Iteration 191/1000 | Loss: 0.00001794
Iteration 192/1000 | Loss: 0.00001794
Iteration 193/1000 | Loss: 0.00001794
Iteration 194/1000 | Loss: 0.00001794
Iteration 195/1000 | Loss: 0.00001794
Iteration 196/1000 | Loss: 0.00001794
Iteration 197/1000 | Loss: 0.00001794
Iteration 198/1000 | Loss: 0.00001794
Iteration 199/1000 | Loss: 0.00001794
Iteration 200/1000 | Loss: 0.00001794
Iteration 201/1000 | Loss: 0.00001794
Iteration 202/1000 | Loss: 0.00001794
Iteration 203/1000 | Loss: 0.00001794
Iteration 204/1000 | Loss: 0.00001794
Iteration 205/1000 | Loss: 0.00001794
Iteration 206/1000 | Loss: 0.00001793
Iteration 207/1000 | Loss: 0.00001793
Iteration 208/1000 | Loss: 0.00001793
Iteration 209/1000 | Loss: 0.00001793
Iteration 210/1000 | Loss: 0.00001793
Iteration 211/1000 | Loss: 0.00001793
Iteration 212/1000 | Loss: 0.00001793
Iteration 213/1000 | Loss: 0.00001793
Iteration 214/1000 | Loss: 0.00001793
Iteration 215/1000 | Loss: 0.00001793
Iteration 216/1000 | Loss: 0.00001793
Iteration 217/1000 | Loss: 0.00001793
Iteration 218/1000 | Loss: 0.00001793
Iteration 219/1000 | Loss: 0.00001793
Iteration 220/1000 | Loss: 0.00001793
Iteration 221/1000 | Loss: 0.00001793
Iteration 222/1000 | Loss: 0.00001793
Iteration 223/1000 | Loss: 0.00001793
Iteration 224/1000 | Loss: 0.00001793
Iteration 225/1000 | Loss: 0.00001793
Iteration 226/1000 | Loss: 0.00001793
Iteration 227/1000 | Loss: 0.00001793
Iteration 228/1000 | Loss: 0.00001793
Iteration 229/1000 | Loss: 0.00001793
Iteration 230/1000 | Loss: 0.00001792
Iteration 231/1000 | Loss: 0.00001792
Iteration 232/1000 | Loss: 0.00001792
Iteration 233/1000 | Loss: 0.00001792
Iteration 234/1000 | Loss: 0.00001792
Iteration 235/1000 | Loss: 0.00001792
Iteration 236/1000 | Loss: 0.00001792
Iteration 237/1000 | Loss: 0.00001792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.7924838175531477e-05, 1.7924838175531477e-05, 1.7924838175531477e-05, 1.7924838175531477e-05, 1.7924838175531477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7924838175531477e-05

Optimization complete. Final v2v error: 3.5955474376678467 mm

Highest mean error: 3.8940556049346924 mm for frame 158

Lowest mean error: 3.386589288711548 mm for frame 187

Saving results

Total time: 75.05692672729492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00728674
Iteration 2/25 | Loss: 0.00151833
Iteration 3/25 | Loss: 0.00081584
Iteration 4/25 | Loss: 0.00082538
Iteration 5/25 | Loss: 0.00067550
Iteration 6/25 | Loss: 0.00065935
Iteration 7/25 | Loss: 0.00066816
Iteration 8/25 | Loss: 0.00065483
Iteration 9/25 | Loss: 0.00063800
Iteration 10/25 | Loss: 0.00063536
Iteration 11/25 | Loss: 0.00063448
Iteration 12/25 | Loss: 0.00062140
Iteration 13/25 | Loss: 0.00062326
Iteration 14/25 | Loss: 0.00061849
Iteration 15/25 | Loss: 0.00061901
Iteration 16/25 | Loss: 0.00061802
Iteration 17/25 | Loss: 0.00061801
Iteration 18/25 | Loss: 0.00061801
Iteration 19/25 | Loss: 0.00061801
Iteration 20/25 | Loss: 0.00061801
Iteration 21/25 | Loss: 0.00061800
Iteration 22/25 | Loss: 0.00061799
Iteration 23/25 | Loss: 0.00061799
Iteration 24/25 | Loss: 0.00061799
Iteration 25/25 | Loss: 0.00061799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.83379436
Iteration 2/25 | Loss: 0.00027683
Iteration 3/25 | Loss: 0.00027683
Iteration 4/25 | Loss: 0.00027683
Iteration 5/25 | Loss: 0.00027683
Iteration 6/25 | Loss: 0.00027683
Iteration 7/25 | Loss: 0.00027683
Iteration 8/25 | Loss: 0.00027683
Iteration 9/25 | Loss: 0.00027683
Iteration 10/25 | Loss: 0.00027683
Iteration 11/25 | Loss: 0.00027683
Iteration 12/25 | Loss: 0.00027683
Iteration 13/25 | Loss: 0.00027683
Iteration 14/25 | Loss: 0.00027683
Iteration 15/25 | Loss: 0.00027683
Iteration 16/25 | Loss: 0.00027683
Iteration 17/25 | Loss: 0.00027683
Iteration 18/25 | Loss: 0.00027683
Iteration 19/25 | Loss: 0.00027683
Iteration 20/25 | Loss: 0.00027683
Iteration 21/25 | Loss: 0.00027683
Iteration 22/25 | Loss: 0.00027683
Iteration 23/25 | Loss: 0.00027683
Iteration 24/25 | Loss: 0.00027683
Iteration 25/25 | Loss: 0.00027683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027683
Iteration 2/1000 | Loss: 0.00002567
Iteration 3/1000 | Loss: 0.00003489
Iteration 4/1000 | Loss: 0.00001802
Iteration 5/1000 | Loss: 0.00005554
Iteration 6/1000 | Loss: 0.00001624
Iteration 7/1000 | Loss: 0.00001584
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00003645
Iteration 11/1000 | Loss: 0.00001499
Iteration 12/1000 | Loss: 0.00001498
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00004681
Iteration 15/1000 | Loss: 0.00001545
Iteration 16/1000 | Loss: 0.00003386
Iteration 17/1000 | Loss: 0.00001577
Iteration 18/1000 | Loss: 0.00004111
Iteration 19/1000 | Loss: 0.00001461
Iteration 20/1000 | Loss: 0.00002664
Iteration 21/1000 | Loss: 0.00001451
Iteration 22/1000 | Loss: 0.00001450
Iteration 23/1000 | Loss: 0.00001450
Iteration 24/1000 | Loss: 0.00001449
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001449
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001446
Iteration 37/1000 | Loss: 0.00002498
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001447
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001446
Iteration 42/1000 | Loss: 0.00001446
Iteration 43/1000 | Loss: 0.00001445
Iteration 44/1000 | Loss: 0.00001445
Iteration 45/1000 | Loss: 0.00001445
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001444
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001444
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00002675
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001434
Iteration 90/1000 | Loss: 0.00001434
Iteration 91/1000 | Loss: 0.00001433
Iteration 92/1000 | Loss: 0.00001433
Iteration 93/1000 | Loss: 0.00001433
Iteration 94/1000 | Loss: 0.00001433
Iteration 95/1000 | Loss: 0.00001433
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001429
Iteration 101/1000 | Loss: 0.00001429
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001428
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001427
Iteration 111/1000 | Loss: 0.00001427
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001426
Iteration 116/1000 | Loss: 0.00001426
Iteration 117/1000 | Loss: 0.00001426
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00006661
Iteration 121/1000 | Loss: 0.00003658
Iteration 122/1000 | Loss: 0.00001447
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001420
Iteration 125/1000 | Loss: 0.00001420
Iteration 126/1000 | Loss: 0.00001420
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001419
Iteration 131/1000 | Loss: 0.00001419
Iteration 132/1000 | Loss: 0.00001418
Iteration 133/1000 | Loss: 0.00001418
Iteration 134/1000 | Loss: 0.00003565
Iteration 135/1000 | Loss: 0.00003565
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00006872
Iteration 138/1000 | Loss: 0.00001684
Iteration 139/1000 | Loss: 0.00001939
Iteration 140/1000 | Loss: 0.00001425
Iteration 141/1000 | Loss: 0.00001422
Iteration 142/1000 | Loss: 0.00001422
Iteration 143/1000 | Loss: 0.00001422
Iteration 144/1000 | Loss: 0.00001418
Iteration 145/1000 | Loss: 0.00001417
Iteration 146/1000 | Loss: 0.00001417
Iteration 147/1000 | Loss: 0.00001417
Iteration 148/1000 | Loss: 0.00001417
Iteration 149/1000 | Loss: 0.00001417
Iteration 150/1000 | Loss: 0.00001417
Iteration 151/1000 | Loss: 0.00001417
Iteration 152/1000 | Loss: 0.00001417
Iteration 153/1000 | Loss: 0.00001417
Iteration 154/1000 | Loss: 0.00001416
Iteration 155/1000 | Loss: 0.00001416
Iteration 156/1000 | Loss: 0.00001416
Iteration 157/1000 | Loss: 0.00001416
Iteration 158/1000 | Loss: 0.00001416
Iteration 159/1000 | Loss: 0.00001416
Iteration 160/1000 | Loss: 0.00001416
Iteration 161/1000 | Loss: 0.00001416
Iteration 162/1000 | Loss: 0.00001416
Iteration 163/1000 | Loss: 0.00001416
Iteration 164/1000 | Loss: 0.00001416
Iteration 165/1000 | Loss: 0.00001416
Iteration 166/1000 | Loss: 0.00001416
Iteration 167/1000 | Loss: 0.00001416
Iteration 168/1000 | Loss: 0.00001416
Iteration 169/1000 | Loss: 0.00001416
Iteration 170/1000 | Loss: 0.00001416
Iteration 171/1000 | Loss: 0.00001416
Iteration 172/1000 | Loss: 0.00001415
Iteration 173/1000 | Loss: 0.00001415
Iteration 174/1000 | Loss: 0.00001415
Iteration 175/1000 | Loss: 0.00001415
Iteration 176/1000 | Loss: 0.00001415
Iteration 177/1000 | Loss: 0.00001415
Iteration 178/1000 | Loss: 0.00001415
Iteration 179/1000 | Loss: 0.00001415
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Iteration 183/1000 | Loss: 0.00001415
Iteration 184/1000 | Loss: 0.00001415
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001415
Iteration 187/1000 | Loss: 0.00001415
Iteration 188/1000 | Loss: 0.00001415
Iteration 189/1000 | Loss: 0.00001415
Iteration 190/1000 | Loss: 0.00001415
Iteration 191/1000 | Loss: 0.00001415
Iteration 192/1000 | Loss: 0.00001415
Iteration 193/1000 | Loss: 0.00001415
Iteration 194/1000 | Loss: 0.00001415
Iteration 195/1000 | Loss: 0.00001415
Iteration 196/1000 | Loss: 0.00001415
Iteration 197/1000 | Loss: 0.00001415
Iteration 198/1000 | Loss: 0.00001415
Iteration 199/1000 | Loss: 0.00001415
Iteration 200/1000 | Loss: 0.00001415
Iteration 201/1000 | Loss: 0.00001415
Iteration 202/1000 | Loss: 0.00001415
Iteration 203/1000 | Loss: 0.00001415
Iteration 204/1000 | Loss: 0.00001415
Iteration 205/1000 | Loss: 0.00001415
Iteration 206/1000 | Loss: 0.00001415
Iteration 207/1000 | Loss: 0.00001415
Iteration 208/1000 | Loss: 0.00001415
Iteration 209/1000 | Loss: 0.00001415
Iteration 210/1000 | Loss: 0.00001415
Iteration 211/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.4148983609629795e-05, 1.4148983609629795e-05, 1.4148983609629795e-05, 1.4148983609629795e-05, 1.4148983609629795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4148983609629795e-05

Optimization complete. Final v2v error: 3.170738697052002 mm

Highest mean error: 3.8219029903411865 mm for frame 87

Lowest mean error: 2.9113144874572754 mm for frame 108

Saving results

Total time: 130.57241034507751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904711
Iteration 2/25 | Loss: 0.00128482
Iteration 3/25 | Loss: 0.00089736
Iteration 4/25 | Loss: 0.00082493
Iteration 5/25 | Loss: 0.00081278
Iteration 6/25 | Loss: 0.00081017
Iteration 7/25 | Loss: 0.00080925
Iteration 8/25 | Loss: 0.00081628
Iteration 9/25 | Loss: 0.00080314
Iteration 10/25 | Loss: 0.00079545
Iteration 11/25 | Loss: 0.00078921
Iteration 12/25 | Loss: 0.00078721
Iteration 13/25 | Loss: 0.00078651
Iteration 14/25 | Loss: 0.00078624
Iteration 15/25 | Loss: 0.00078623
Iteration 16/25 | Loss: 0.00078623
Iteration 17/25 | Loss: 0.00078623
Iteration 18/25 | Loss: 0.00078623
Iteration 19/25 | Loss: 0.00078623
Iteration 20/25 | Loss: 0.00078623
Iteration 21/25 | Loss: 0.00078623
Iteration 22/25 | Loss: 0.00078623
Iteration 23/25 | Loss: 0.00078623
Iteration 24/25 | Loss: 0.00078623
Iteration 25/25 | Loss: 0.00078622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99670279
Iteration 2/25 | Loss: 0.00030045
Iteration 3/25 | Loss: 0.00030045
Iteration 4/25 | Loss: 0.00030044
Iteration 5/25 | Loss: 0.00030044
Iteration 6/25 | Loss: 0.00030044
Iteration 7/25 | Loss: 0.00030044
Iteration 8/25 | Loss: 0.00030044
Iteration 9/25 | Loss: 0.00030044
Iteration 10/25 | Loss: 0.00030044
Iteration 11/25 | Loss: 0.00030044
Iteration 12/25 | Loss: 0.00030044
Iteration 13/25 | Loss: 0.00030044
Iteration 14/25 | Loss: 0.00030044
Iteration 15/25 | Loss: 0.00030044
Iteration 16/25 | Loss: 0.00030044
Iteration 17/25 | Loss: 0.00030044
Iteration 18/25 | Loss: 0.00030044
Iteration 19/25 | Loss: 0.00030044
Iteration 20/25 | Loss: 0.00030044
Iteration 21/25 | Loss: 0.00030044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00030044245067983866, 0.00030044245067983866, 0.00030044245067983866, 0.00030044245067983866, 0.00030044245067983866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030044245067983866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030044
Iteration 2/1000 | Loss: 0.00004334
Iteration 3/1000 | Loss: 0.00003591
Iteration 4/1000 | Loss: 0.00003375
Iteration 5/1000 | Loss: 0.00003257
Iteration 6/1000 | Loss: 0.00003138
Iteration 7/1000 | Loss: 0.00003046
Iteration 8/1000 | Loss: 0.00002997
Iteration 9/1000 | Loss: 0.00002968
Iteration 10/1000 | Loss: 0.00002952
Iteration 11/1000 | Loss: 0.00002951
Iteration 12/1000 | Loss: 0.00002945
Iteration 13/1000 | Loss: 0.00002945
Iteration 14/1000 | Loss: 0.00002935
Iteration 15/1000 | Loss: 0.00002923
Iteration 16/1000 | Loss: 0.00002914
Iteration 17/1000 | Loss: 0.00002914
Iteration 18/1000 | Loss: 0.00002914
Iteration 19/1000 | Loss: 0.00002914
Iteration 20/1000 | Loss: 0.00002914
Iteration 21/1000 | Loss: 0.00002914
Iteration 22/1000 | Loss: 0.00002913
Iteration 23/1000 | Loss: 0.00002913
Iteration 24/1000 | Loss: 0.00002913
Iteration 25/1000 | Loss: 0.00002913
Iteration 26/1000 | Loss: 0.00002912
Iteration 27/1000 | Loss: 0.00002912
Iteration 28/1000 | Loss: 0.00002912
Iteration 29/1000 | Loss: 0.00002903
Iteration 30/1000 | Loss: 0.00002894
Iteration 31/1000 | Loss: 0.00046306
Iteration 32/1000 | Loss: 0.00038524
Iteration 33/1000 | Loss: 0.00010086
Iteration 34/1000 | Loss: 0.00008134
Iteration 35/1000 | Loss: 0.00003352
Iteration 36/1000 | Loss: 0.00003082
Iteration 37/1000 | Loss: 0.00002999
Iteration 38/1000 | Loss: 0.00002944
Iteration 39/1000 | Loss: 0.00002879
Iteration 40/1000 | Loss: 0.00002839
Iteration 41/1000 | Loss: 0.00002814
Iteration 42/1000 | Loss: 0.00002801
Iteration 43/1000 | Loss: 0.00002800
Iteration 44/1000 | Loss: 0.00002800
Iteration 45/1000 | Loss: 0.00002800
Iteration 46/1000 | Loss: 0.00002800
Iteration 47/1000 | Loss: 0.00002800
Iteration 48/1000 | Loss: 0.00002800
Iteration 49/1000 | Loss: 0.00002800
Iteration 50/1000 | Loss: 0.00002800
Iteration 51/1000 | Loss: 0.00002800
Iteration 52/1000 | Loss: 0.00002799
Iteration 53/1000 | Loss: 0.00002799
Iteration 54/1000 | Loss: 0.00002799
Iteration 55/1000 | Loss: 0.00002792
Iteration 56/1000 | Loss: 0.00002790
Iteration 57/1000 | Loss: 0.00002787
Iteration 58/1000 | Loss: 0.00002780
Iteration 59/1000 | Loss: 0.00002780
Iteration 60/1000 | Loss: 0.00002780
Iteration 61/1000 | Loss: 0.00002780
Iteration 62/1000 | Loss: 0.00002780
Iteration 63/1000 | Loss: 0.00002780
Iteration 64/1000 | Loss: 0.00002780
Iteration 65/1000 | Loss: 0.00002780
Iteration 66/1000 | Loss: 0.00002780
Iteration 67/1000 | Loss: 0.00002779
Iteration 68/1000 | Loss: 0.00002779
Iteration 69/1000 | Loss: 0.00002777
Iteration 70/1000 | Loss: 0.00002777
Iteration 71/1000 | Loss: 0.00002776
Iteration 72/1000 | Loss: 0.00002776
Iteration 73/1000 | Loss: 0.00002776
Iteration 74/1000 | Loss: 0.00002776
Iteration 75/1000 | Loss: 0.00002776
Iteration 76/1000 | Loss: 0.00002776
Iteration 77/1000 | Loss: 0.00002776
Iteration 78/1000 | Loss: 0.00002776
Iteration 79/1000 | Loss: 0.00002775
Iteration 80/1000 | Loss: 0.00002775
Iteration 81/1000 | Loss: 0.00002775
Iteration 82/1000 | Loss: 0.00002774
Iteration 83/1000 | Loss: 0.00002774
Iteration 84/1000 | Loss: 0.00002774
Iteration 85/1000 | Loss: 0.00002774
Iteration 86/1000 | Loss: 0.00002774
Iteration 87/1000 | Loss: 0.00002774
Iteration 88/1000 | Loss: 0.00002774
Iteration 89/1000 | Loss: 0.00002774
Iteration 90/1000 | Loss: 0.00002774
Iteration 91/1000 | Loss: 0.00002774
Iteration 92/1000 | Loss: 0.00002774
Iteration 93/1000 | Loss: 0.00002773
Iteration 94/1000 | Loss: 0.00002773
Iteration 95/1000 | Loss: 0.00002773
Iteration 96/1000 | Loss: 0.00002773
Iteration 97/1000 | Loss: 0.00002773
Iteration 98/1000 | Loss: 0.00002772
Iteration 99/1000 | Loss: 0.00002772
Iteration 100/1000 | Loss: 0.00002772
Iteration 101/1000 | Loss: 0.00002772
Iteration 102/1000 | Loss: 0.00002772
Iteration 103/1000 | Loss: 0.00002772
Iteration 104/1000 | Loss: 0.00002772
Iteration 105/1000 | Loss: 0.00002771
Iteration 106/1000 | Loss: 0.00002771
Iteration 107/1000 | Loss: 0.00002771
Iteration 108/1000 | Loss: 0.00002771
Iteration 109/1000 | Loss: 0.00002771
Iteration 110/1000 | Loss: 0.00002771
Iteration 111/1000 | Loss: 0.00002771
Iteration 112/1000 | Loss: 0.00002770
Iteration 113/1000 | Loss: 0.00002770
Iteration 114/1000 | Loss: 0.00002770
Iteration 115/1000 | Loss: 0.00002770
Iteration 116/1000 | Loss: 0.00002770
Iteration 117/1000 | Loss: 0.00002770
Iteration 118/1000 | Loss: 0.00002770
Iteration 119/1000 | Loss: 0.00002770
Iteration 120/1000 | Loss: 0.00002770
Iteration 121/1000 | Loss: 0.00002770
Iteration 122/1000 | Loss: 0.00002770
Iteration 123/1000 | Loss: 0.00002770
Iteration 124/1000 | Loss: 0.00002770
Iteration 125/1000 | Loss: 0.00002770
Iteration 126/1000 | Loss: 0.00002770
Iteration 127/1000 | Loss: 0.00002770
Iteration 128/1000 | Loss: 0.00002770
Iteration 129/1000 | Loss: 0.00002770
Iteration 130/1000 | Loss: 0.00002770
Iteration 131/1000 | Loss: 0.00002770
Iteration 132/1000 | Loss: 0.00002770
Iteration 133/1000 | Loss: 0.00002770
Iteration 134/1000 | Loss: 0.00002770
Iteration 135/1000 | Loss: 0.00002770
Iteration 136/1000 | Loss: 0.00002770
Iteration 137/1000 | Loss: 0.00002770
Iteration 138/1000 | Loss: 0.00002770
Iteration 139/1000 | Loss: 0.00002770
Iteration 140/1000 | Loss: 0.00002770
Iteration 141/1000 | Loss: 0.00002770
Iteration 142/1000 | Loss: 0.00002770
Iteration 143/1000 | Loss: 0.00002770
Iteration 144/1000 | Loss: 0.00002770
Iteration 145/1000 | Loss: 0.00002770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.76999289781088e-05, 2.76999289781088e-05, 2.76999289781088e-05, 2.76999289781088e-05, 2.76999289781088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.76999289781088e-05

Optimization complete. Final v2v error: 4.404344081878662 mm

Highest mean error: 5.609389781951904 mm for frame 137

Lowest mean error: 4.25977897644043 mm for frame 117

Saving results

Total time: 145.530042886734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076473
Iteration 2/25 | Loss: 0.00272079
Iteration 3/25 | Loss: 0.00151839
Iteration 4/25 | Loss: 0.00147566
Iteration 5/25 | Loss: 0.00100962
Iteration 6/25 | Loss: 0.00091071
Iteration 7/25 | Loss: 0.00101433
Iteration 8/25 | Loss: 0.00105624
Iteration 9/25 | Loss: 0.00087776
Iteration 10/25 | Loss: 0.00081473
Iteration 11/25 | Loss: 0.00079288
Iteration 12/25 | Loss: 0.00077979
Iteration 13/25 | Loss: 0.00076655
Iteration 14/25 | Loss: 0.00077399
Iteration 15/25 | Loss: 0.00077903
Iteration 16/25 | Loss: 0.00075029
Iteration 17/25 | Loss: 0.00074052
Iteration 18/25 | Loss: 0.00073182
Iteration 19/25 | Loss: 0.00072308
Iteration 20/25 | Loss: 0.00072525
Iteration 21/25 | Loss: 0.00072303
Iteration 22/25 | Loss: 0.00072292
Iteration 23/25 | Loss: 0.00071779
Iteration 24/25 | Loss: 0.00071816
Iteration 25/25 | Loss: 0.00071108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53878284
Iteration 2/25 | Loss: 0.00147169
Iteration 3/25 | Loss: 0.00113548
Iteration 4/25 | Loss: 0.00113547
Iteration 5/25 | Loss: 0.00113547
Iteration 6/25 | Loss: 0.00113547
Iteration 7/25 | Loss: 0.00113546
Iteration 8/25 | Loss: 0.00113546
Iteration 9/25 | Loss: 0.00113546
Iteration 10/25 | Loss: 0.00113546
Iteration 11/25 | Loss: 0.00113546
Iteration 12/25 | Loss: 0.00113546
Iteration 13/25 | Loss: 0.00113546
Iteration 14/25 | Loss: 0.00113546
Iteration 15/25 | Loss: 0.00113546
Iteration 16/25 | Loss: 0.00113546
Iteration 17/25 | Loss: 0.00113546
Iteration 18/25 | Loss: 0.00113546
Iteration 19/25 | Loss: 0.00113546
Iteration 20/25 | Loss: 0.00113546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011354634771123528, 0.0011354634771123528, 0.0011354634771123528, 0.0011354634771123528, 0.0011354634771123528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011354634771123528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113546
Iteration 2/1000 | Loss: 0.00085814
Iteration 3/1000 | Loss: 0.00056549
Iteration 4/1000 | Loss: 0.00017263
Iteration 5/1000 | Loss: 0.00016138
Iteration 6/1000 | Loss: 0.00071544
Iteration 7/1000 | Loss: 0.00104754
Iteration 8/1000 | Loss: 0.00064092
Iteration 9/1000 | Loss: 0.00061475
Iteration 10/1000 | Loss: 0.00049511
Iteration 11/1000 | Loss: 0.00039351
Iteration 12/1000 | Loss: 0.00025467
Iteration 13/1000 | Loss: 0.00023825
Iteration 14/1000 | Loss: 0.00022987
Iteration 15/1000 | Loss: 0.00023002
Iteration 16/1000 | Loss: 0.00013171
Iteration 17/1000 | Loss: 0.00005162
Iteration 18/1000 | Loss: 0.00022748
Iteration 19/1000 | Loss: 0.00045855
Iteration 20/1000 | Loss: 0.00167103
Iteration 21/1000 | Loss: 0.00070209
Iteration 22/1000 | Loss: 0.00053605
Iteration 23/1000 | Loss: 0.00047043
Iteration 24/1000 | Loss: 0.00090709
Iteration 25/1000 | Loss: 0.00067516
Iteration 26/1000 | Loss: 0.00144149
Iteration 27/1000 | Loss: 0.00078368
Iteration 28/1000 | Loss: 0.00068354
Iteration 29/1000 | Loss: 0.00027150
Iteration 30/1000 | Loss: 0.00016274
Iteration 31/1000 | Loss: 0.00025925
Iteration 32/1000 | Loss: 0.00027376
Iteration 33/1000 | Loss: 0.00029810
Iteration 34/1000 | Loss: 0.00029347
Iteration 35/1000 | Loss: 0.00005833
Iteration 36/1000 | Loss: 0.00031261
Iteration 37/1000 | Loss: 0.00020022
Iteration 38/1000 | Loss: 0.00038510
Iteration 39/1000 | Loss: 0.00025300
Iteration 40/1000 | Loss: 0.00038934
Iteration 41/1000 | Loss: 0.00044642
Iteration 42/1000 | Loss: 0.00050299
Iteration 43/1000 | Loss: 0.00046237
Iteration 44/1000 | Loss: 0.00051116
Iteration 45/1000 | Loss: 0.00025795
Iteration 46/1000 | Loss: 0.00038471
Iteration 47/1000 | Loss: 0.00034690
Iteration 48/1000 | Loss: 0.00034275
Iteration 49/1000 | Loss: 0.00034371
Iteration 50/1000 | Loss: 0.00030689
Iteration 51/1000 | Loss: 0.00033016
Iteration 52/1000 | Loss: 0.00025001
Iteration 53/1000 | Loss: 0.00032516
Iteration 54/1000 | Loss: 0.00032295
Iteration 55/1000 | Loss: 0.00021969
Iteration 56/1000 | Loss: 0.00034056
Iteration 57/1000 | Loss: 0.00030983
Iteration 58/1000 | Loss: 0.00033064
Iteration 59/1000 | Loss: 0.00025077
Iteration 60/1000 | Loss: 0.00031392
Iteration 61/1000 | Loss: 0.00026186
Iteration 62/1000 | Loss: 0.00026412
Iteration 63/1000 | Loss: 0.00028298
Iteration 64/1000 | Loss: 0.00021660
Iteration 65/1000 | Loss: 0.00037493
Iteration 66/1000 | Loss: 0.00048776
Iteration 67/1000 | Loss: 0.00043627
Iteration 68/1000 | Loss: 0.00030704
Iteration 69/1000 | Loss: 0.00030247
Iteration 70/1000 | Loss: 0.00024888
Iteration 71/1000 | Loss: 0.00045068
Iteration 72/1000 | Loss: 0.00057177
Iteration 73/1000 | Loss: 0.00024955
Iteration 74/1000 | Loss: 0.00032108
Iteration 75/1000 | Loss: 0.00040825
Iteration 76/1000 | Loss: 0.00043461
Iteration 77/1000 | Loss: 0.00050355
Iteration 78/1000 | Loss: 0.00051285
Iteration 79/1000 | Loss: 0.00031814
Iteration 80/1000 | Loss: 0.00124177
Iteration 81/1000 | Loss: 0.00050573
Iteration 82/1000 | Loss: 0.00047571
Iteration 83/1000 | Loss: 0.00050260
Iteration 84/1000 | Loss: 0.00052194
Iteration 85/1000 | Loss: 0.00032476
Iteration 86/1000 | Loss: 0.00054299
Iteration 87/1000 | Loss: 0.00058634
Iteration 88/1000 | Loss: 0.00082274
Iteration 89/1000 | Loss: 0.00021674
Iteration 90/1000 | Loss: 0.00019123
Iteration 91/1000 | Loss: 0.00015889
Iteration 92/1000 | Loss: 0.00040774
Iteration 93/1000 | Loss: 0.00016444
Iteration 94/1000 | Loss: 0.00031796
Iteration 95/1000 | Loss: 0.00013441
Iteration 96/1000 | Loss: 0.00020746
Iteration 97/1000 | Loss: 0.00005314
Iteration 98/1000 | Loss: 0.00013416
Iteration 99/1000 | Loss: 0.00012346
Iteration 100/1000 | Loss: 0.00008046
Iteration 101/1000 | Loss: 0.00002965
Iteration 102/1000 | Loss: 0.00005639
Iteration 103/1000 | Loss: 0.00020523
Iteration 104/1000 | Loss: 0.00006042
Iteration 105/1000 | Loss: 0.00007159
Iteration 106/1000 | Loss: 0.00011519
Iteration 107/1000 | Loss: 0.00013087
Iteration 108/1000 | Loss: 0.00014107
Iteration 109/1000 | Loss: 0.00017500
Iteration 110/1000 | Loss: 0.00020250
Iteration 111/1000 | Loss: 0.00006035
Iteration 112/1000 | Loss: 0.00014003
Iteration 113/1000 | Loss: 0.00015888
Iteration 114/1000 | Loss: 0.00025098
Iteration 115/1000 | Loss: 0.00018008
Iteration 116/1000 | Loss: 0.00019882
Iteration 117/1000 | Loss: 0.00032878
Iteration 118/1000 | Loss: 0.00055485
Iteration 119/1000 | Loss: 0.00041653
Iteration 120/1000 | Loss: 0.00004568
Iteration 121/1000 | Loss: 0.00030062
Iteration 122/1000 | Loss: 0.00019299
Iteration 123/1000 | Loss: 0.00017953
Iteration 124/1000 | Loss: 0.00016331
Iteration 125/1000 | Loss: 0.00020479
Iteration 126/1000 | Loss: 0.00016301
Iteration 127/1000 | Loss: 0.00013273
Iteration 128/1000 | Loss: 0.00013308
Iteration 129/1000 | Loss: 0.00007199
Iteration 130/1000 | Loss: 0.00012020
Iteration 131/1000 | Loss: 0.00008034
Iteration 132/1000 | Loss: 0.00006798
Iteration 133/1000 | Loss: 0.00034292
Iteration 134/1000 | Loss: 0.00024849
Iteration 135/1000 | Loss: 0.00015503
Iteration 136/1000 | Loss: 0.00014346
Iteration 137/1000 | Loss: 0.00037753
Iteration 138/1000 | Loss: 0.00018859
Iteration 139/1000 | Loss: 0.00012088
Iteration 140/1000 | Loss: 0.00025196
Iteration 141/1000 | Loss: 0.00050423
Iteration 142/1000 | Loss: 0.00014583
Iteration 143/1000 | Loss: 0.00018223
Iteration 144/1000 | Loss: 0.00020718
Iteration 145/1000 | Loss: 0.00015721
Iteration 146/1000 | Loss: 0.00044912
Iteration 147/1000 | Loss: 0.00050655
Iteration 148/1000 | Loss: 0.00019520
Iteration 149/1000 | Loss: 0.00021761
Iteration 150/1000 | Loss: 0.00008121
Iteration 151/1000 | Loss: 0.00022260
Iteration 152/1000 | Loss: 0.00063437
Iteration 153/1000 | Loss: 0.00038092
Iteration 154/1000 | Loss: 0.00042678
Iteration 155/1000 | Loss: 0.00035235
Iteration 156/1000 | Loss: 0.00039701
Iteration 157/1000 | Loss: 0.00022045
Iteration 158/1000 | Loss: 0.00019891
Iteration 159/1000 | Loss: 0.00017423
Iteration 160/1000 | Loss: 0.00016597
Iteration 161/1000 | Loss: 0.00040797
Iteration 162/1000 | Loss: 0.00010836
Iteration 163/1000 | Loss: 0.00018296
Iteration 164/1000 | Loss: 0.00021504
Iteration 165/1000 | Loss: 0.00025682
Iteration 166/1000 | Loss: 0.00021067
Iteration 167/1000 | Loss: 0.00051185
Iteration 168/1000 | Loss: 0.00010382
Iteration 169/1000 | Loss: 0.00060376
Iteration 170/1000 | Loss: 0.00081621
Iteration 171/1000 | Loss: 0.00023442
Iteration 172/1000 | Loss: 0.00025502
Iteration 173/1000 | Loss: 0.00055804
Iteration 174/1000 | Loss: 0.00041271
Iteration 175/1000 | Loss: 0.00018519
Iteration 176/1000 | Loss: 0.00042698
Iteration 177/1000 | Loss: 0.00021200
Iteration 178/1000 | Loss: 0.00045193
Iteration 179/1000 | Loss: 0.00061611
Iteration 180/1000 | Loss: 0.00017111
Iteration 181/1000 | Loss: 0.00015772
Iteration 182/1000 | Loss: 0.00059493
Iteration 183/1000 | Loss: 0.00019623
Iteration 184/1000 | Loss: 0.00016074
Iteration 185/1000 | Loss: 0.00035576
Iteration 186/1000 | Loss: 0.00083366
Iteration 187/1000 | Loss: 0.00074129
Iteration 188/1000 | Loss: 0.00021760
Iteration 189/1000 | Loss: 0.00030561
Iteration 190/1000 | Loss: 0.00041011
Iteration 191/1000 | Loss: 0.00027607
Iteration 192/1000 | Loss: 0.00058402
Iteration 193/1000 | Loss: 0.00014633
Iteration 194/1000 | Loss: 0.00045289
Iteration 195/1000 | Loss: 0.00024881
Iteration 196/1000 | Loss: 0.00023047
Iteration 197/1000 | Loss: 0.00049804
Iteration 198/1000 | Loss: 0.00086714
Iteration 199/1000 | Loss: 0.00060973
Iteration 200/1000 | Loss: 0.00004484
Iteration 201/1000 | Loss: 0.00009148
Iteration 202/1000 | Loss: 0.00014538
Iteration 203/1000 | Loss: 0.00045696
Iteration 204/1000 | Loss: 0.00056722
Iteration 205/1000 | Loss: 0.00055965
Iteration 206/1000 | Loss: 0.00050976
Iteration 207/1000 | Loss: 0.00010707
Iteration 208/1000 | Loss: 0.00010926
Iteration 209/1000 | Loss: 0.00062041
Iteration 210/1000 | Loss: 0.00031027
Iteration 211/1000 | Loss: 0.00040667
Iteration 212/1000 | Loss: 0.00003015
Iteration 213/1000 | Loss: 0.00002364
Iteration 214/1000 | Loss: 0.00002212
Iteration 215/1000 | Loss: 0.00011017
Iteration 216/1000 | Loss: 0.00010956
Iteration 217/1000 | Loss: 0.00002080
Iteration 218/1000 | Loss: 0.00030962
Iteration 219/1000 | Loss: 0.00035404
Iteration 220/1000 | Loss: 0.00021250
Iteration 221/1000 | Loss: 0.00049722
Iteration 222/1000 | Loss: 0.00031026
Iteration 223/1000 | Loss: 0.00034869
Iteration 224/1000 | Loss: 0.00037192
Iteration 225/1000 | Loss: 0.00006165
Iteration 226/1000 | Loss: 0.00005436
Iteration 227/1000 | Loss: 0.00024671
Iteration 228/1000 | Loss: 0.00007959
Iteration 229/1000 | Loss: 0.00010974
Iteration 230/1000 | Loss: 0.00016142
Iteration 231/1000 | Loss: 0.00006524
Iteration 232/1000 | Loss: 0.00014782
Iteration 233/1000 | Loss: 0.00046634
Iteration 234/1000 | Loss: 0.00023673
Iteration 235/1000 | Loss: 0.00018107
Iteration 236/1000 | Loss: 0.00056958
Iteration 237/1000 | Loss: 0.00034841
Iteration 238/1000 | Loss: 0.00044713
Iteration 239/1000 | Loss: 0.00037974
Iteration 240/1000 | Loss: 0.00033243
Iteration 241/1000 | Loss: 0.00032857
Iteration 242/1000 | Loss: 0.00141325
Iteration 243/1000 | Loss: 0.00005118
Iteration 244/1000 | Loss: 0.00003109
Iteration 245/1000 | Loss: 0.00090762
Iteration 246/1000 | Loss: 0.00031936
Iteration 247/1000 | Loss: 0.00048807
Iteration 248/1000 | Loss: 0.00049018
Iteration 249/1000 | Loss: 0.00087737
Iteration 250/1000 | Loss: 0.00036332
Iteration 251/1000 | Loss: 0.00084095
Iteration 252/1000 | Loss: 0.00031727
Iteration 253/1000 | Loss: 0.00032197
Iteration 254/1000 | Loss: 0.00021247
Iteration 255/1000 | Loss: 0.00027019
Iteration 256/1000 | Loss: 0.00003284
Iteration 257/1000 | Loss: 0.00002488
Iteration 258/1000 | Loss: 0.00040299
Iteration 259/1000 | Loss: 0.00023493
Iteration 260/1000 | Loss: 0.00026368
Iteration 261/1000 | Loss: 0.00003531
Iteration 262/1000 | Loss: 0.00009602
Iteration 263/1000 | Loss: 0.00001970
Iteration 264/1000 | Loss: 0.00002134
Iteration 265/1000 | Loss: 0.00002080
Iteration 266/1000 | Loss: 0.00001778
Iteration 267/1000 | Loss: 0.00001739
Iteration 268/1000 | Loss: 0.00001712
Iteration 269/1000 | Loss: 0.00001689
Iteration 270/1000 | Loss: 0.00001670
Iteration 271/1000 | Loss: 0.00001652
Iteration 272/1000 | Loss: 0.00001640
Iteration 273/1000 | Loss: 0.00001640
Iteration 274/1000 | Loss: 0.00001636
Iteration 275/1000 | Loss: 0.00001636
Iteration 276/1000 | Loss: 0.00001636
Iteration 277/1000 | Loss: 0.00001636
Iteration 278/1000 | Loss: 0.00001635
Iteration 279/1000 | Loss: 0.00001635
Iteration 280/1000 | Loss: 0.00001635
Iteration 281/1000 | Loss: 0.00001635
Iteration 282/1000 | Loss: 0.00001635
Iteration 283/1000 | Loss: 0.00001633
Iteration 284/1000 | Loss: 0.00001633
Iteration 285/1000 | Loss: 0.00001632
Iteration 286/1000 | Loss: 0.00001632
Iteration 287/1000 | Loss: 0.00001631
Iteration 288/1000 | Loss: 0.00001630
Iteration 289/1000 | Loss: 0.00001630
Iteration 290/1000 | Loss: 0.00001630
Iteration 291/1000 | Loss: 0.00001630
Iteration 292/1000 | Loss: 0.00001629
Iteration 293/1000 | Loss: 0.00001629
Iteration 294/1000 | Loss: 0.00001629
Iteration 295/1000 | Loss: 0.00001628
Iteration 296/1000 | Loss: 0.00001627
Iteration 297/1000 | Loss: 0.00001627
Iteration 298/1000 | Loss: 0.00001627
Iteration 299/1000 | Loss: 0.00001627
Iteration 300/1000 | Loss: 0.00001627
Iteration 301/1000 | Loss: 0.00001627
Iteration 302/1000 | Loss: 0.00001627
Iteration 303/1000 | Loss: 0.00001627
Iteration 304/1000 | Loss: 0.00001627
Iteration 305/1000 | Loss: 0.00001627
Iteration 306/1000 | Loss: 0.00001626
Iteration 307/1000 | Loss: 0.00001626
Iteration 308/1000 | Loss: 0.00001626
Iteration 309/1000 | Loss: 0.00001626
Iteration 310/1000 | Loss: 0.00001626
Iteration 311/1000 | Loss: 0.00001625
Iteration 312/1000 | Loss: 0.00001624
Iteration 313/1000 | Loss: 0.00001624
Iteration 314/1000 | Loss: 0.00001623
Iteration 315/1000 | Loss: 0.00001623
Iteration 316/1000 | Loss: 0.00001623
Iteration 317/1000 | Loss: 0.00001623
Iteration 318/1000 | Loss: 0.00001622
Iteration 319/1000 | Loss: 0.00001622
Iteration 320/1000 | Loss: 0.00001622
Iteration 321/1000 | Loss: 0.00001622
Iteration 322/1000 | Loss: 0.00001622
Iteration 323/1000 | Loss: 0.00001622
Iteration 324/1000 | Loss: 0.00001622
Iteration 325/1000 | Loss: 0.00001621
Iteration 326/1000 | Loss: 0.00001621
Iteration 327/1000 | Loss: 0.00001621
Iteration 328/1000 | Loss: 0.00001621
Iteration 329/1000 | Loss: 0.00001621
Iteration 330/1000 | Loss: 0.00001621
Iteration 331/1000 | Loss: 0.00001621
Iteration 332/1000 | Loss: 0.00001621
Iteration 333/1000 | Loss: 0.00001621
Iteration 334/1000 | Loss: 0.00001621
Iteration 335/1000 | Loss: 0.00001621
Iteration 336/1000 | Loss: 0.00001621
Iteration 337/1000 | Loss: 0.00001620
Iteration 338/1000 | Loss: 0.00001620
Iteration 339/1000 | Loss: 0.00001620
Iteration 340/1000 | Loss: 0.00001620
Iteration 341/1000 | Loss: 0.00001620
Iteration 342/1000 | Loss: 0.00001620
Iteration 343/1000 | Loss: 0.00001620
Iteration 344/1000 | Loss: 0.00001620
Iteration 345/1000 | Loss: 0.00001620
Iteration 346/1000 | Loss: 0.00001619
Iteration 347/1000 | Loss: 0.00001619
Iteration 348/1000 | Loss: 0.00001619
Iteration 349/1000 | Loss: 0.00001619
Iteration 350/1000 | Loss: 0.00001619
Iteration 351/1000 | Loss: 0.00001619
Iteration 352/1000 | Loss: 0.00001619
Iteration 353/1000 | Loss: 0.00001619
Iteration 354/1000 | Loss: 0.00001619
Iteration 355/1000 | Loss: 0.00001619
Iteration 356/1000 | Loss: 0.00001619
Iteration 357/1000 | Loss: 0.00001619
Iteration 358/1000 | Loss: 0.00001619
Iteration 359/1000 | Loss: 0.00001619
Iteration 360/1000 | Loss: 0.00001619
Iteration 361/1000 | Loss: 0.00001619
Iteration 362/1000 | Loss: 0.00001619
Iteration 363/1000 | Loss: 0.00001619
Iteration 364/1000 | Loss: 0.00001619
Iteration 365/1000 | Loss: 0.00001619
Iteration 366/1000 | Loss: 0.00001619
Iteration 367/1000 | Loss: 0.00001619
Iteration 368/1000 | Loss: 0.00001619
Iteration 369/1000 | Loss: 0.00001619
Iteration 370/1000 | Loss: 0.00001619
Iteration 371/1000 | Loss: 0.00001619
Iteration 372/1000 | Loss: 0.00001619
Iteration 373/1000 | Loss: 0.00001619
Iteration 374/1000 | Loss: 0.00001619
Iteration 375/1000 | Loss: 0.00001619
Iteration 376/1000 | Loss: 0.00001619
Iteration 377/1000 | Loss: 0.00001619
Iteration 378/1000 | Loss: 0.00001619
Iteration 379/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 379. Stopping optimization.
Last 5 losses: [1.6186366337933578e-05, 1.6186366337933578e-05, 1.6186366337933578e-05, 1.6186366337933578e-05, 1.6186366337933578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6186366337933578e-05

Optimization complete. Final v2v error: 3.275207996368408 mm

Highest mean error: 6.30220365524292 mm for frame 122

Lowest mean error: 2.9403328895568848 mm for frame 13

Saving results

Total time: 476.25201964378357
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023449
Iteration 2/25 | Loss: 0.00266203
Iteration 3/25 | Loss: 0.00177629
Iteration 4/25 | Loss: 0.00168895
Iteration 5/25 | Loss: 0.00167359
Iteration 6/25 | Loss: 0.00160967
Iteration 7/25 | Loss: 0.00159865
Iteration 8/25 | Loss: 0.00159271
Iteration 9/25 | Loss: 0.00159097
Iteration 10/25 | Loss: 0.00159097
Iteration 11/25 | Loss: 0.00159097
Iteration 12/25 | Loss: 0.00159097
Iteration 13/25 | Loss: 0.00159097
Iteration 14/25 | Loss: 0.00159097
Iteration 15/25 | Loss: 0.00159097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015909741632640362, 0.0015909741632640362, 0.0015909741632640362, 0.0015909741632640362, 0.0015909741632640362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015909741632640362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47847903
Iteration 2/25 | Loss: 0.00766741
Iteration 3/25 | Loss: 0.00641442
Iteration 4/25 | Loss: 0.00641442
Iteration 5/25 | Loss: 0.00641442
Iteration 6/25 | Loss: 0.00641442
Iteration 7/25 | Loss: 0.00641442
Iteration 8/25 | Loss: 0.00641441
Iteration 9/25 | Loss: 0.00641441
Iteration 10/25 | Loss: 0.00641441
Iteration 11/25 | Loss: 0.00641441
Iteration 12/25 | Loss: 0.00641441
Iteration 13/25 | Loss: 0.00641441
Iteration 14/25 | Loss: 0.00641441
Iteration 15/25 | Loss: 0.00641441
Iteration 16/25 | Loss: 0.00641441
Iteration 17/25 | Loss: 0.00641441
Iteration 18/25 | Loss: 0.00641441
Iteration 19/25 | Loss: 0.00641441
Iteration 20/25 | Loss: 0.00641441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.006414414383471012, 0.006414414383471012, 0.006414414383471012, 0.006414414383471012, 0.006414414383471012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006414414383471012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00641441
Iteration 2/1000 | Loss: 0.00371666
Iteration 3/1000 | Loss: 0.00087580
Iteration 4/1000 | Loss: 0.00087076
Iteration 5/1000 | Loss: 0.00083180
Iteration 6/1000 | Loss: 0.00053567
Iteration 7/1000 | Loss: 0.00677718
Iteration 8/1000 | Loss: 0.01603946
Iteration 9/1000 | Loss: 0.00258074
Iteration 10/1000 | Loss: 0.00068700
Iteration 11/1000 | Loss: 0.00068970
Iteration 12/1000 | Loss: 0.00012634
Iteration 13/1000 | Loss: 0.00008733
Iteration 14/1000 | Loss: 0.00006405
Iteration 15/1000 | Loss: 0.00004986
Iteration 16/1000 | Loss: 0.00003947
Iteration 17/1000 | Loss: 0.00003370
Iteration 18/1000 | Loss: 0.00002941
Iteration 19/1000 | Loss: 0.00002627
Iteration 20/1000 | Loss: 0.00002417
Iteration 21/1000 | Loss: 0.00002227
Iteration 22/1000 | Loss: 0.00002094
Iteration 23/1000 | Loss: 0.00001998
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001913
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001897
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001879
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001875
Iteration 33/1000 | Loss: 0.00001875
Iteration 34/1000 | Loss: 0.00001875
Iteration 35/1000 | Loss: 0.00001875
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001875
Iteration 38/1000 | Loss: 0.00001875
Iteration 39/1000 | Loss: 0.00001875
Iteration 40/1000 | Loss: 0.00001875
Iteration 41/1000 | Loss: 0.00001875
Iteration 42/1000 | Loss: 0.00001875
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.8751217794488184e-05, 1.8751217794488184e-05, 1.8751217794488184e-05, 1.8751217794488184e-05, 1.8751217794488184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8751217794488184e-05

Optimization complete. Final v2v error: 3.6956984996795654 mm

Highest mean error: 3.7752625942230225 mm for frame 60

Lowest mean error: 3.636486291885376 mm for frame 29

Saving results

Total time: 70.548983335495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419701
Iteration 2/25 | Loss: 0.00091115
Iteration 3/25 | Loss: 0.00069081
Iteration 4/25 | Loss: 0.00065094
Iteration 5/25 | Loss: 0.00064262
Iteration 6/25 | Loss: 0.00064095
Iteration 7/25 | Loss: 0.00064056
Iteration 8/25 | Loss: 0.00064056
Iteration 9/25 | Loss: 0.00064056
Iteration 10/25 | Loss: 0.00064056
Iteration 11/25 | Loss: 0.00064056
Iteration 12/25 | Loss: 0.00064056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006405625026673079, 0.0006405625026673079, 0.0006405625026673079, 0.0006405625026673079, 0.0006405625026673079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006405625026673079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52032471
Iteration 2/25 | Loss: 0.00029527
Iteration 3/25 | Loss: 0.00029527
Iteration 4/25 | Loss: 0.00029527
Iteration 5/25 | Loss: 0.00029527
Iteration 6/25 | Loss: 0.00029527
Iteration 7/25 | Loss: 0.00029527
Iteration 8/25 | Loss: 0.00029527
Iteration 9/25 | Loss: 0.00029527
Iteration 10/25 | Loss: 0.00029527
Iteration 11/25 | Loss: 0.00029527
Iteration 12/25 | Loss: 0.00029527
Iteration 13/25 | Loss: 0.00029527
Iteration 14/25 | Loss: 0.00029527
Iteration 15/25 | Loss: 0.00029527
Iteration 16/25 | Loss: 0.00029527
Iteration 17/25 | Loss: 0.00029527
Iteration 18/25 | Loss: 0.00029527
Iteration 19/25 | Loss: 0.00029527
Iteration 20/25 | Loss: 0.00029527
Iteration 21/25 | Loss: 0.00029527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00029526822618208826, 0.00029526822618208826, 0.00029526822618208826, 0.00029526822618208826, 0.00029526822618208826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029526822618208826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029527
Iteration 2/1000 | Loss: 0.00003076
Iteration 3/1000 | Loss: 0.00001993
Iteration 4/1000 | Loss: 0.00001874
Iteration 5/1000 | Loss: 0.00001806
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001698
Iteration 8/1000 | Loss: 0.00001661
Iteration 9/1000 | Loss: 0.00001637
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001610
Iteration 12/1000 | Loss: 0.00001607
Iteration 13/1000 | Loss: 0.00001606
Iteration 14/1000 | Loss: 0.00001605
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001597
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001595
Iteration 20/1000 | Loss: 0.00001595
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001594
Iteration 23/1000 | Loss: 0.00001594
Iteration 24/1000 | Loss: 0.00001594
Iteration 25/1000 | Loss: 0.00001593
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001590
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001589
Iteration 31/1000 | Loss: 0.00001589
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001588
Iteration 34/1000 | Loss: 0.00001588
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001587
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001586
Iteration 45/1000 | Loss: 0.00001586
Iteration 46/1000 | Loss: 0.00001586
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001586
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001584
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001583
Iteration 70/1000 | Loss: 0.00001583
Iteration 71/1000 | Loss: 0.00001583
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00001583
Iteration 74/1000 | Loss: 0.00001583
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001582
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001582
Iteration 84/1000 | Loss: 0.00001582
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001582
Iteration 87/1000 | Loss: 0.00001582
Iteration 88/1000 | Loss: 0.00001582
Iteration 89/1000 | Loss: 0.00001582
Iteration 90/1000 | Loss: 0.00001582
Iteration 91/1000 | Loss: 0.00001582
Iteration 92/1000 | Loss: 0.00001582
Iteration 93/1000 | Loss: 0.00001582
Iteration 94/1000 | Loss: 0.00001582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.5824480215087533e-05, 1.5824480215087533e-05, 1.5824480215087533e-05, 1.5824480215087533e-05, 1.5824480215087533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5824480215087533e-05

Optimization complete. Final v2v error: 3.3404500484466553 mm

Highest mean error: 3.9450912475585938 mm for frame 80

Lowest mean error: 2.835961103439331 mm for frame 165

Saving results

Total time: 36.299750089645386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622964
Iteration 2/25 | Loss: 0.00112682
Iteration 3/25 | Loss: 0.00081434
Iteration 4/25 | Loss: 0.00074966
Iteration 5/25 | Loss: 0.00073927
Iteration 6/25 | Loss: 0.00072641
Iteration 7/25 | Loss: 0.00072349
Iteration 8/25 | Loss: 0.00072251
Iteration 9/25 | Loss: 0.00071745
Iteration 10/25 | Loss: 0.00071578
Iteration 11/25 | Loss: 0.00071503
Iteration 12/25 | Loss: 0.00071469
Iteration 13/25 | Loss: 0.00071455
Iteration 14/25 | Loss: 0.00071447
Iteration 15/25 | Loss: 0.00071446
Iteration 16/25 | Loss: 0.00071446
Iteration 17/25 | Loss: 0.00071446
Iteration 18/25 | Loss: 0.00071445
Iteration 19/25 | Loss: 0.00071445
Iteration 20/25 | Loss: 0.00071445
Iteration 21/25 | Loss: 0.00071445
Iteration 22/25 | Loss: 0.00071445
Iteration 23/25 | Loss: 0.00071445
Iteration 24/25 | Loss: 0.00071445
Iteration 25/25 | Loss: 0.00071445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48365855
Iteration 2/25 | Loss: 0.00035427
Iteration 3/25 | Loss: 0.00035423
Iteration 4/25 | Loss: 0.00035423
Iteration 5/25 | Loss: 0.00035423
Iteration 6/25 | Loss: 0.00035423
Iteration 7/25 | Loss: 0.00035423
Iteration 8/25 | Loss: 0.00035423
Iteration 9/25 | Loss: 0.00035423
Iteration 10/25 | Loss: 0.00035423
Iteration 11/25 | Loss: 0.00035423
Iteration 12/25 | Loss: 0.00035423
Iteration 13/25 | Loss: 0.00035423
Iteration 14/25 | Loss: 0.00035423
Iteration 15/25 | Loss: 0.00035423
Iteration 16/25 | Loss: 0.00035423
Iteration 17/25 | Loss: 0.00035423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003542251361068338, 0.0003542251361068338, 0.0003542251361068338, 0.0003542251361068338, 0.0003542251361068338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003542251361068338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035423
Iteration 2/1000 | Loss: 0.00004429
Iteration 3/1000 | Loss: 0.00002900
Iteration 4/1000 | Loss: 0.00002566
Iteration 5/1000 | Loss: 0.00024804
Iteration 6/1000 | Loss: 0.00019176
Iteration 7/1000 | Loss: 0.00024342
Iteration 8/1000 | Loss: 0.00015054
Iteration 9/1000 | Loss: 0.00002342
Iteration 10/1000 | Loss: 0.00002277
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002185
Iteration 13/1000 | Loss: 0.00002152
Iteration 14/1000 | Loss: 0.00002123
Iteration 15/1000 | Loss: 0.00055143
Iteration 16/1000 | Loss: 0.00002469
Iteration 17/1000 | Loss: 0.00002186
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00002040
Iteration 20/1000 | Loss: 0.00001980
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001940
Iteration 23/1000 | Loss: 0.00001937
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001935
Iteration 26/1000 | Loss: 0.00001934
Iteration 27/1000 | Loss: 0.00001934
Iteration 28/1000 | Loss: 0.00001934
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001933
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001930
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001929
Iteration 46/1000 | Loss: 0.00001928
Iteration 47/1000 | Loss: 0.00001928
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001920
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001917
Iteration 55/1000 | Loss: 0.00001916
Iteration 56/1000 | Loss: 0.00001916
Iteration 57/1000 | Loss: 0.00001916
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001916
Iteration 63/1000 | Loss: 0.00001916
Iteration 64/1000 | Loss: 0.00001916
Iteration 65/1000 | Loss: 0.00001916
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001914
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001913
Iteration 71/1000 | Loss: 0.00001913
Iteration 72/1000 | Loss: 0.00001913
Iteration 73/1000 | Loss: 0.00001912
Iteration 74/1000 | Loss: 0.00001912
Iteration 75/1000 | Loss: 0.00001912
Iteration 76/1000 | Loss: 0.00001912
Iteration 77/1000 | Loss: 0.00001912
Iteration 78/1000 | Loss: 0.00001911
Iteration 79/1000 | Loss: 0.00001911
Iteration 80/1000 | Loss: 0.00001911
Iteration 81/1000 | Loss: 0.00001911
Iteration 82/1000 | Loss: 0.00001911
Iteration 83/1000 | Loss: 0.00001910
Iteration 84/1000 | Loss: 0.00001910
Iteration 85/1000 | Loss: 0.00001910
Iteration 86/1000 | Loss: 0.00001910
Iteration 87/1000 | Loss: 0.00001910
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001910
Iteration 90/1000 | Loss: 0.00001910
Iteration 91/1000 | Loss: 0.00001910
Iteration 92/1000 | Loss: 0.00001910
Iteration 93/1000 | Loss: 0.00001910
Iteration 94/1000 | Loss: 0.00001909
Iteration 95/1000 | Loss: 0.00001909
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001907
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001904
Iteration 108/1000 | Loss: 0.00001904
Iteration 109/1000 | Loss: 0.00001904
Iteration 110/1000 | Loss: 0.00001903
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001902
Iteration 113/1000 | Loss: 0.00001901
Iteration 114/1000 | Loss: 0.00001901
Iteration 115/1000 | Loss: 0.00001900
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00001900
Iteration 118/1000 | Loss: 0.00001899
Iteration 119/1000 | Loss: 0.00001898
Iteration 120/1000 | Loss: 0.00001898
Iteration 121/1000 | Loss: 0.00001898
Iteration 122/1000 | Loss: 0.00001897
Iteration 123/1000 | Loss: 0.00001897
Iteration 124/1000 | Loss: 0.00001897
Iteration 125/1000 | Loss: 0.00001897
Iteration 126/1000 | Loss: 0.00001897
Iteration 127/1000 | Loss: 0.00001896
Iteration 128/1000 | Loss: 0.00001896
Iteration 129/1000 | Loss: 0.00001896
Iteration 130/1000 | Loss: 0.00001896
Iteration 131/1000 | Loss: 0.00001896
Iteration 132/1000 | Loss: 0.00001896
Iteration 133/1000 | Loss: 0.00001896
Iteration 134/1000 | Loss: 0.00001896
Iteration 135/1000 | Loss: 0.00001896
Iteration 136/1000 | Loss: 0.00001896
Iteration 137/1000 | Loss: 0.00001896
Iteration 138/1000 | Loss: 0.00001896
Iteration 139/1000 | Loss: 0.00001895
Iteration 140/1000 | Loss: 0.00001895
Iteration 141/1000 | Loss: 0.00001895
Iteration 142/1000 | Loss: 0.00001894
Iteration 143/1000 | Loss: 0.00001894
Iteration 144/1000 | Loss: 0.00001894
Iteration 145/1000 | Loss: 0.00001893
Iteration 146/1000 | Loss: 0.00001893
Iteration 147/1000 | Loss: 0.00001893
Iteration 148/1000 | Loss: 0.00001893
Iteration 149/1000 | Loss: 0.00001892
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001891
Iteration 152/1000 | Loss: 0.00001891
Iteration 153/1000 | Loss: 0.00001891
Iteration 154/1000 | Loss: 0.00001890
Iteration 155/1000 | Loss: 0.00001890
Iteration 156/1000 | Loss: 0.00001890
Iteration 157/1000 | Loss: 0.00001889
Iteration 158/1000 | Loss: 0.00001889
Iteration 159/1000 | Loss: 0.00001889
Iteration 160/1000 | Loss: 0.00001889
Iteration 161/1000 | Loss: 0.00001889
Iteration 162/1000 | Loss: 0.00001888
Iteration 163/1000 | Loss: 0.00001888
Iteration 164/1000 | Loss: 0.00001888
Iteration 165/1000 | Loss: 0.00001888
Iteration 166/1000 | Loss: 0.00001888
Iteration 167/1000 | Loss: 0.00001888
Iteration 168/1000 | Loss: 0.00001887
Iteration 169/1000 | Loss: 0.00001887
Iteration 170/1000 | Loss: 0.00001887
Iteration 171/1000 | Loss: 0.00001887
Iteration 172/1000 | Loss: 0.00001886
Iteration 173/1000 | Loss: 0.00001886
Iteration 174/1000 | Loss: 0.00001886
Iteration 175/1000 | Loss: 0.00001886
Iteration 176/1000 | Loss: 0.00001885
Iteration 177/1000 | Loss: 0.00001884
Iteration 178/1000 | Loss: 0.00001884
Iteration 179/1000 | Loss: 0.00001884
Iteration 180/1000 | Loss: 0.00001883
Iteration 181/1000 | Loss: 0.00001883
Iteration 182/1000 | Loss: 0.00001883
Iteration 183/1000 | Loss: 0.00001883
Iteration 184/1000 | Loss: 0.00001882
Iteration 185/1000 | Loss: 0.00001882
Iteration 186/1000 | Loss: 0.00001882
Iteration 187/1000 | Loss: 0.00001882
Iteration 188/1000 | Loss: 0.00001882
Iteration 189/1000 | Loss: 0.00001882
Iteration 190/1000 | Loss: 0.00001882
Iteration 191/1000 | Loss: 0.00001882
Iteration 192/1000 | Loss: 0.00001882
Iteration 193/1000 | Loss: 0.00001881
Iteration 194/1000 | Loss: 0.00001881
Iteration 195/1000 | Loss: 0.00001881
Iteration 196/1000 | Loss: 0.00001881
Iteration 197/1000 | Loss: 0.00001881
Iteration 198/1000 | Loss: 0.00001880
Iteration 199/1000 | Loss: 0.00001880
Iteration 200/1000 | Loss: 0.00001880
Iteration 201/1000 | Loss: 0.00001880
Iteration 202/1000 | Loss: 0.00001880
Iteration 203/1000 | Loss: 0.00001880
Iteration 204/1000 | Loss: 0.00001880
Iteration 205/1000 | Loss: 0.00001880
Iteration 206/1000 | Loss: 0.00001880
Iteration 207/1000 | Loss: 0.00001879
Iteration 208/1000 | Loss: 0.00001879
Iteration 209/1000 | Loss: 0.00001879
Iteration 210/1000 | Loss: 0.00001879
Iteration 211/1000 | Loss: 0.00001879
Iteration 212/1000 | Loss: 0.00001879
Iteration 213/1000 | Loss: 0.00001879
Iteration 214/1000 | Loss: 0.00001879
Iteration 215/1000 | Loss: 0.00001879
Iteration 216/1000 | Loss: 0.00001879
Iteration 217/1000 | Loss: 0.00001879
Iteration 218/1000 | Loss: 0.00001879
Iteration 219/1000 | Loss: 0.00001879
Iteration 220/1000 | Loss: 0.00001879
Iteration 221/1000 | Loss: 0.00001879
Iteration 222/1000 | Loss: 0.00001879
Iteration 223/1000 | Loss: 0.00001879
Iteration 224/1000 | Loss: 0.00001879
Iteration 225/1000 | Loss: 0.00001878
Iteration 226/1000 | Loss: 0.00001878
Iteration 227/1000 | Loss: 0.00001878
Iteration 228/1000 | Loss: 0.00001878
Iteration 229/1000 | Loss: 0.00001878
Iteration 230/1000 | Loss: 0.00001878
Iteration 231/1000 | Loss: 0.00001878
Iteration 232/1000 | Loss: 0.00001878
Iteration 233/1000 | Loss: 0.00001878
Iteration 234/1000 | Loss: 0.00001878
Iteration 235/1000 | Loss: 0.00001878
Iteration 236/1000 | Loss: 0.00001878
Iteration 237/1000 | Loss: 0.00001878
Iteration 238/1000 | Loss: 0.00001878
Iteration 239/1000 | Loss: 0.00001878
Iteration 240/1000 | Loss: 0.00001878
Iteration 241/1000 | Loss: 0.00001878
Iteration 242/1000 | Loss: 0.00001878
Iteration 243/1000 | Loss: 0.00001878
Iteration 244/1000 | Loss: 0.00001878
Iteration 245/1000 | Loss: 0.00001878
Iteration 246/1000 | Loss: 0.00001878
Iteration 247/1000 | Loss: 0.00001878
Iteration 248/1000 | Loss: 0.00001878
Iteration 249/1000 | Loss: 0.00001878
Iteration 250/1000 | Loss: 0.00001878
Iteration 251/1000 | Loss: 0.00001878
Iteration 252/1000 | Loss: 0.00001878
Iteration 253/1000 | Loss: 0.00001878
Iteration 254/1000 | Loss: 0.00001878
Iteration 255/1000 | Loss: 0.00001878
Iteration 256/1000 | Loss: 0.00001878
Iteration 257/1000 | Loss: 0.00001878
Iteration 258/1000 | Loss: 0.00001878
Iteration 259/1000 | Loss: 0.00001878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.877748036349658e-05, 1.877748036349658e-05, 1.877748036349658e-05, 1.877748036349658e-05, 1.877748036349658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.877748036349658e-05

Optimization complete. Final v2v error: 3.629873752593994 mm

Highest mean error: 5.860342502593994 mm for frame 166

Lowest mean error: 2.7810592651367188 mm for frame 237

Saving results

Total time: 88.28606033325195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135173
Iteration 2/25 | Loss: 0.00315199
Iteration 3/25 | Loss: 0.00194146
Iteration 4/25 | Loss: 0.00177330
Iteration 5/25 | Loss: 0.00160752
Iteration 6/25 | Loss: 0.00130460
Iteration 7/25 | Loss: 0.00118769
Iteration 8/25 | Loss: 0.00114847
Iteration 9/25 | Loss: 0.00114241
Iteration 10/25 | Loss: 0.00114168
Iteration 11/25 | Loss: 0.00113872
Iteration 12/25 | Loss: 0.00113807
Iteration 13/25 | Loss: 0.00113780
Iteration 14/25 | Loss: 0.00113748
Iteration 15/25 | Loss: 0.00113720
Iteration 16/25 | Loss: 0.00113689
Iteration 17/25 | Loss: 0.00113667
Iteration 18/25 | Loss: 0.00113659
Iteration 19/25 | Loss: 0.00113659
Iteration 20/25 | Loss: 0.00113659
Iteration 21/25 | Loss: 0.00113659
Iteration 22/25 | Loss: 0.00113659
Iteration 23/25 | Loss: 0.00113659
Iteration 24/25 | Loss: 0.00113659
Iteration 25/25 | Loss: 0.00113659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011365897953510284, 0.0011365897953510284, 0.0011365897953510284, 0.0011365897953510284, 0.0011365897953510284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011365897953510284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27733123
Iteration 2/25 | Loss: 0.00212456
Iteration 3/25 | Loss: 0.00212454
Iteration 4/25 | Loss: 0.00212454
Iteration 5/25 | Loss: 0.00212454
Iteration 6/25 | Loss: 0.00212454
Iteration 7/25 | Loss: 0.00212454
Iteration 8/25 | Loss: 0.00212454
Iteration 9/25 | Loss: 0.00212454
Iteration 10/25 | Loss: 0.00212454
Iteration 11/25 | Loss: 0.00212454
Iteration 12/25 | Loss: 0.00212454
Iteration 13/25 | Loss: 0.00212454
Iteration 14/25 | Loss: 0.00212454
Iteration 15/25 | Loss: 0.00212454
Iteration 16/25 | Loss: 0.00212454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021245393436402082, 0.0021245393436402082, 0.0021245393436402082, 0.0021245393436402082, 0.0021245393436402082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021245393436402082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212454
Iteration 2/1000 | Loss: 0.00040231
Iteration 3/1000 | Loss: 0.00031527
Iteration 4/1000 | Loss: 0.00081263
Iteration 5/1000 | Loss: 0.00042203
Iteration 6/1000 | Loss: 0.00073286
Iteration 7/1000 | Loss: 0.00104143
Iteration 8/1000 | Loss: 0.00045000
Iteration 9/1000 | Loss: 0.00027455
Iteration 10/1000 | Loss: 0.00021383
Iteration 11/1000 | Loss: 0.00019384
Iteration 12/1000 | Loss: 0.00018300
Iteration 13/1000 | Loss: 0.00017525
Iteration 14/1000 | Loss: 0.00016545
Iteration 15/1000 | Loss: 0.00015973
Iteration 16/1000 | Loss: 0.00095006
Iteration 17/1000 | Loss: 0.00976044
Iteration 18/1000 | Loss: 0.00334316
Iteration 19/1000 | Loss: 0.00070176
Iteration 20/1000 | Loss: 0.00119958
Iteration 21/1000 | Loss: 0.00086098
Iteration 22/1000 | Loss: 0.00071970
Iteration 23/1000 | Loss: 0.00160493
Iteration 24/1000 | Loss: 0.00036743
Iteration 25/1000 | Loss: 0.00031646
Iteration 26/1000 | Loss: 0.00048916
Iteration 27/1000 | Loss: 0.00013088
Iteration 28/1000 | Loss: 0.00031187
Iteration 29/1000 | Loss: 0.00008212
Iteration 30/1000 | Loss: 0.00006576
Iteration 31/1000 | Loss: 0.00005764
Iteration 32/1000 | Loss: 0.00005800
Iteration 33/1000 | Loss: 0.00039192
Iteration 34/1000 | Loss: 0.00005094
Iteration 35/1000 | Loss: 0.00004662
Iteration 36/1000 | Loss: 0.00004442
Iteration 37/1000 | Loss: 0.00004225
Iteration 38/1000 | Loss: 0.00034636
Iteration 39/1000 | Loss: 0.00081730
Iteration 40/1000 | Loss: 0.00089397
Iteration 41/1000 | Loss: 0.00081660
Iteration 42/1000 | Loss: 0.00019680
Iteration 43/1000 | Loss: 0.00008740
Iteration 44/1000 | Loss: 0.00007536
Iteration 45/1000 | Loss: 0.00004986
Iteration 46/1000 | Loss: 0.00004130
Iteration 47/1000 | Loss: 0.00003652
Iteration 48/1000 | Loss: 0.00071811
Iteration 49/1000 | Loss: 0.00022657
Iteration 50/1000 | Loss: 0.00053411
Iteration 51/1000 | Loss: 0.00004700
Iteration 52/1000 | Loss: 0.00003492
Iteration 53/1000 | Loss: 0.00003144
Iteration 54/1000 | Loss: 0.00003044
Iteration 55/1000 | Loss: 0.00002965
Iteration 56/1000 | Loss: 0.00002922
Iteration 57/1000 | Loss: 0.00027184
Iteration 58/1000 | Loss: 0.00101707
Iteration 59/1000 | Loss: 0.00009655
Iteration 60/1000 | Loss: 0.00003305
Iteration 61/1000 | Loss: 0.00002992
Iteration 62/1000 | Loss: 0.00002899
Iteration 63/1000 | Loss: 0.00002837
Iteration 64/1000 | Loss: 0.00002806
Iteration 65/1000 | Loss: 0.00002805
Iteration 66/1000 | Loss: 0.00002797
Iteration 67/1000 | Loss: 0.00002783
Iteration 68/1000 | Loss: 0.00002779
Iteration 69/1000 | Loss: 0.00002779
Iteration 70/1000 | Loss: 0.00002779
Iteration 71/1000 | Loss: 0.00002779
Iteration 72/1000 | Loss: 0.00002778
Iteration 73/1000 | Loss: 0.00002777
Iteration 74/1000 | Loss: 0.00002775
Iteration 75/1000 | Loss: 0.00002774
Iteration 76/1000 | Loss: 0.00002774
Iteration 77/1000 | Loss: 0.00002774
Iteration 78/1000 | Loss: 0.00002773
Iteration 79/1000 | Loss: 0.00002773
Iteration 80/1000 | Loss: 0.00002773
Iteration 81/1000 | Loss: 0.00002773
Iteration 82/1000 | Loss: 0.00002773
Iteration 83/1000 | Loss: 0.00002772
Iteration 84/1000 | Loss: 0.00002772
Iteration 85/1000 | Loss: 0.00002772
Iteration 86/1000 | Loss: 0.00002771
Iteration 87/1000 | Loss: 0.00002771
Iteration 88/1000 | Loss: 0.00002771
Iteration 89/1000 | Loss: 0.00002771
Iteration 90/1000 | Loss: 0.00002770
Iteration 91/1000 | Loss: 0.00002770
Iteration 92/1000 | Loss: 0.00002770
Iteration 93/1000 | Loss: 0.00002769
Iteration 94/1000 | Loss: 0.00002769
Iteration 95/1000 | Loss: 0.00002769
Iteration 96/1000 | Loss: 0.00002768
Iteration 97/1000 | Loss: 0.00002768
Iteration 98/1000 | Loss: 0.00002768
Iteration 99/1000 | Loss: 0.00002767
Iteration 100/1000 | Loss: 0.00002767
Iteration 101/1000 | Loss: 0.00002767
Iteration 102/1000 | Loss: 0.00002767
Iteration 103/1000 | Loss: 0.00002767
Iteration 104/1000 | Loss: 0.00002767
Iteration 105/1000 | Loss: 0.00002767
Iteration 106/1000 | Loss: 0.00002767
Iteration 107/1000 | Loss: 0.00002767
Iteration 108/1000 | Loss: 0.00002767
Iteration 109/1000 | Loss: 0.00002766
Iteration 110/1000 | Loss: 0.00002766
Iteration 111/1000 | Loss: 0.00002766
Iteration 112/1000 | Loss: 0.00002766
Iteration 113/1000 | Loss: 0.00002765
Iteration 114/1000 | Loss: 0.00002765
Iteration 115/1000 | Loss: 0.00002765
Iteration 116/1000 | Loss: 0.00002765
Iteration 117/1000 | Loss: 0.00002765
Iteration 118/1000 | Loss: 0.00002764
Iteration 119/1000 | Loss: 0.00002764
Iteration 120/1000 | Loss: 0.00002764
Iteration 121/1000 | Loss: 0.00002764
Iteration 122/1000 | Loss: 0.00002764
Iteration 123/1000 | Loss: 0.00002764
Iteration 124/1000 | Loss: 0.00002764
Iteration 125/1000 | Loss: 0.00002764
Iteration 126/1000 | Loss: 0.00002764
Iteration 127/1000 | Loss: 0.00002764
Iteration 128/1000 | Loss: 0.00002764
Iteration 129/1000 | Loss: 0.00002763
Iteration 130/1000 | Loss: 0.00002763
Iteration 131/1000 | Loss: 0.00002763
Iteration 132/1000 | Loss: 0.00002763
Iteration 133/1000 | Loss: 0.00002763
Iteration 134/1000 | Loss: 0.00002763
Iteration 135/1000 | Loss: 0.00002763
Iteration 136/1000 | Loss: 0.00002763
Iteration 137/1000 | Loss: 0.00002763
Iteration 138/1000 | Loss: 0.00002763
Iteration 139/1000 | Loss: 0.00002763
Iteration 140/1000 | Loss: 0.00002763
Iteration 141/1000 | Loss: 0.00002763
Iteration 142/1000 | Loss: 0.00002763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.763004886219278e-05, 2.763004886219278e-05, 2.763004886219278e-05, 2.763004886219278e-05, 2.763004886219278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.763004886219278e-05

Optimization complete. Final v2v error: 4.336148738861084 mm

Highest mean error: 5.488157749176025 mm for frame 94

Lowest mean error: 4.060862064361572 mm for frame 138

Saving results

Total time: 170.16968846321106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071262
Iteration 2/25 | Loss: 0.00161668
Iteration 3/25 | Loss: 0.00109597
Iteration 4/25 | Loss: 0.00084927
Iteration 5/25 | Loss: 0.00077735
Iteration 6/25 | Loss: 0.00076487
Iteration 7/25 | Loss: 0.00076497
Iteration 8/25 | Loss: 0.00076749
Iteration 9/25 | Loss: 0.00077395
Iteration 10/25 | Loss: 0.00076761
Iteration 11/25 | Loss: 0.00075230
Iteration 12/25 | Loss: 0.00073866
Iteration 13/25 | Loss: 0.00073426
Iteration 14/25 | Loss: 0.00073303
Iteration 15/25 | Loss: 0.00073279
Iteration 16/25 | Loss: 0.00073275
Iteration 17/25 | Loss: 0.00073275
Iteration 18/25 | Loss: 0.00073274
Iteration 19/25 | Loss: 0.00073274
Iteration 20/25 | Loss: 0.00073274
Iteration 21/25 | Loss: 0.00073274
Iteration 22/25 | Loss: 0.00073274
Iteration 23/25 | Loss: 0.00073274
Iteration 24/25 | Loss: 0.00073274
Iteration 25/25 | Loss: 0.00073274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68030119
Iteration 2/25 | Loss: 0.00053712
Iteration 3/25 | Loss: 0.00053712
Iteration 4/25 | Loss: 0.00053712
Iteration 5/25 | Loss: 0.00053712
Iteration 6/25 | Loss: 0.00053712
Iteration 7/25 | Loss: 0.00053712
Iteration 8/25 | Loss: 0.00053712
Iteration 9/25 | Loss: 0.00053712
Iteration 10/25 | Loss: 0.00053712
Iteration 11/25 | Loss: 0.00053712
Iteration 12/25 | Loss: 0.00053712
Iteration 13/25 | Loss: 0.00053712
Iteration 14/25 | Loss: 0.00053712
Iteration 15/25 | Loss: 0.00053712
Iteration 16/25 | Loss: 0.00053712
Iteration 17/25 | Loss: 0.00053712
Iteration 18/25 | Loss: 0.00053712
Iteration 19/25 | Loss: 0.00053712
Iteration 20/25 | Loss: 0.00053712
Iteration 21/25 | Loss: 0.00053712
Iteration 22/25 | Loss: 0.00053712
Iteration 23/25 | Loss: 0.00053712
Iteration 24/25 | Loss: 0.00053712
Iteration 25/25 | Loss: 0.00053712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053712
Iteration 2/1000 | Loss: 0.00005123
Iteration 3/1000 | Loss: 0.00003761
Iteration 4/1000 | Loss: 0.00003139
Iteration 5/1000 | Loss: 0.00002779
Iteration 6/1000 | Loss: 0.00025227
Iteration 7/1000 | Loss: 0.00010401
Iteration 8/1000 | Loss: 0.00009616
Iteration 9/1000 | Loss: 0.00020448
Iteration 10/1000 | Loss: 0.00012512
Iteration 11/1000 | Loss: 0.00016091
Iteration 12/1000 | Loss: 0.00003210
Iteration 13/1000 | Loss: 0.00002712
Iteration 14/1000 | Loss: 0.00002473
Iteration 15/1000 | Loss: 0.00002341
Iteration 16/1000 | Loss: 0.00002291
Iteration 17/1000 | Loss: 0.00002237
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002140
Iteration 20/1000 | Loss: 0.00002096
Iteration 21/1000 | Loss: 0.00002066
Iteration 22/1000 | Loss: 0.00002046
Iteration 23/1000 | Loss: 0.00002026
Iteration 24/1000 | Loss: 0.00002024
Iteration 25/1000 | Loss: 0.00002011
Iteration 26/1000 | Loss: 0.00002009
Iteration 27/1000 | Loss: 0.00002007
Iteration 28/1000 | Loss: 0.00002007
Iteration 29/1000 | Loss: 0.00002006
Iteration 30/1000 | Loss: 0.00002005
Iteration 31/1000 | Loss: 0.00002003
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002003
Iteration 34/1000 | Loss: 0.00002003
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00002003
Iteration 39/1000 | Loss: 0.00002003
Iteration 40/1000 | Loss: 0.00002003
Iteration 41/1000 | Loss: 0.00002003
Iteration 42/1000 | Loss: 0.00002002
Iteration 43/1000 | Loss: 0.00002002
Iteration 44/1000 | Loss: 0.00002001
Iteration 45/1000 | Loss: 0.00002001
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00002000
Iteration 48/1000 | Loss: 0.00002000
Iteration 49/1000 | Loss: 0.00002000
Iteration 50/1000 | Loss: 0.00001999
Iteration 51/1000 | Loss: 0.00001999
Iteration 52/1000 | Loss: 0.00001999
Iteration 53/1000 | Loss: 0.00001999
Iteration 54/1000 | Loss: 0.00001999
Iteration 55/1000 | Loss: 0.00001999
Iteration 56/1000 | Loss: 0.00001999
Iteration 57/1000 | Loss: 0.00001999
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001999
Iteration 60/1000 | Loss: 0.00001999
Iteration 61/1000 | Loss: 0.00001999
Iteration 62/1000 | Loss: 0.00001999
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001997
Iteration 66/1000 | Loss: 0.00001997
Iteration 67/1000 | Loss: 0.00001996
Iteration 68/1000 | Loss: 0.00001996
Iteration 69/1000 | Loss: 0.00001995
Iteration 70/1000 | Loss: 0.00001995
Iteration 71/1000 | Loss: 0.00001995
Iteration 72/1000 | Loss: 0.00001995
Iteration 73/1000 | Loss: 0.00001995
Iteration 74/1000 | Loss: 0.00001994
Iteration 75/1000 | Loss: 0.00001994
Iteration 76/1000 | Loss: 0.00001994
Iteration 77/1000 | Loss: 0.00001994
Iteration 78/1000 | Loss: 0.00001994
Iteration 79/1000 | Loss: 0.00001993
Iteration 80/1000 | Loss: 0.00001993
Iteration 81/1000 | Loss: 0.00001993
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00001993
Iteration 84/1000 | Loss: 0.00001993
Iteration 85/1000 | Loss: 0.00001992
Iteration 86/1000 | Loss: 0.00001992
Iteration 87/1000 | Loss: 0.00001992
Iteration 88/1000 | Loss: 0.00001992
Iteration 89/1000 | Loss: 0.00001992
Iteration 90/1000 | Loss: 0.00001992
Iteration 91/1000 | Loss: 0.00001992
Iteration 92/1000 | Loss: 0.00001991
Iteration 93/1000 | Loss: 0.00001991
Iteration 94/1000 | Loss: 0.00001991
Iteration 95/1000 | Loss: 0.00001991
Iteration 96/1000 | Loss: 0.00001991
Iteration 97/1000 | Loss: 0.00001991
Iteration 98/1000 | Loss: 0.00001991
Iteration 99/1000 | Loss: 0.00001991
Iteration 100/1000 | Loss: 0.00001991
Iteration 101/1000 | Loss: 0.00001991
Iteration 102/1000 | Loss: 0.00001991
Iteration 103/1000 | Loss: 0.00001991
Iteration 104/1000 | Loss: 0.00001991
Iteration 105/1000 | Loss: 0.00001990
Iteration 106/1000 | Loss: 0.00001990
Iteration 107/1000 | Loss: 0.00001990
Iteration 108/1000 | Loss: 0.00001990
Iteration 109/1000 | Loss: 0.00001990
Iteration 110/1000 | Loss: 0.00001990
Iteration 111/1000 | Loss: 0.00001990
Iteration 112/1000 | Loss: 0.00001990
Iteration 113/1000 | Loss: 0.00001990
Iteration 114/1000 | Loss: 0.00001990
Iteration 115/1000 | Loss: 0.00001990
Iteration 116/1000 | Loss: 0.00001990
Iteration 117/1000 | Loss: 0.00001989
Iteration 118/1000 | Loss: 0.00001989
Iteration 119/1000 | Loss: 0.00001989
Iteration 120/1000 | Loss: 0.00001989
Iteration 121/1000 | Loss: 0.00001989
Iteration 122/1000 | Loss: 0.00001989
Iteration 123/1000 | Loss: 0.00001989
Iteration 124/1000 | Loss: 0.00001989
Iteration 125/1000 | Loss: 0.00001989
Iteration 126/1000 | Loss: 0.00001988
Iteration 127/1000 | Loss: 0.00001988
Iteration 128/1000 | Loss: 0.00001988
Iteration 129/1000 | Loss: 0.00001988
Iteration 130/1000 | Loss: 0.00001988
Iteration 131/1000 | Loss: 0.00001988
Iteration 132/1000 | Loss: 0.00001988
Iteration 133/1000 | Loss: 0.00001988
Iteration 134/1000 | Loss: 0.00001988
Iteration 135/1000 | Loss: 0.00001987
Iteration 136/1000 | Loss: 0.00001987
Iteration 137/1000 | Loss: 0.00001987
Iteration 138/1000 | Loss: 0.00001987
Iteration 139/1000 | Loss: 0.00001987
Iteration 140/1000 | Loss: 0.00001987
Iteration 141/1000 | Loss: 0.00001987
Iteration 142/1000 | Loss: 0.00001987
Iteration 143/1000 | Loss: 0.00001986
Iteration 144/1000 | Loss: 0.00001986
Iteration 145/1000 | Loss: 0.00001986
Iteration 146/1000 | Loss: 0.00001986
Iteration 147/1000 | Loss: 0.00001986
Iteration 148/1000 | Loss: 0.00001986
Iteration 149/1000 | Loss: 0.00001986
Iteration 150/1000 | Loss: 0.00001986
Iteration 151/1000 | Loss: 0.00001986
Iteration 152/1000 | Loss: 0.00001986
Iteration 153/1000 | Loss: 0.00001986
Iteration 154/1000 | Loss: 0.00001986
Iteration 155/1000 | Loss: 0.00001986
Iteration 156/1000 | Loss: 0.00001986
Iteration 157/1000 | Loss: 0.00001986
Iteration 158/1000 | Loss: 0.00001985
Iteration 159/1000 | Loss: 0.00001985
Iteration 160/1000 | Loss: 0.00001985
Iteration 161/1000 | Loss: 0.00001985
Iteration 162/1000 | Loss: 0.00001985
Iteration 163/1000 | Loss: 0.00001985
Iteration 164/1000 | Loss: 0.00001985
Iteration 165/1000 | Loss: 0.00001985
Iteration 166/1000 | Loss: 0.00001985
Iteration 167/1000 | Loss: 0.00001985
Iteration 168/1000 | Loss: 0.00001985
Iteration 169/1000 | Loss: 0.00001985
Iteration 170/1000 | Loss: 0.00001985
Iteration 171/1000 | Loss: 0.00001985
Iteration 172/1000 | Loss: 0.00001985
Iteration 173/1000 | Loss: 0.00001985
Iteration 174/1000 | Loss: 0.00001985
Iteration 175/1000 | Loss: 0.00001984
Iteration 176/1000 | Loss: 0.00001984
Iteration 177/1000 | Loss: 0.00001984
Iteration 178/1000 | Loss: 0.00001984
Iteration 179/1000 | Loss: 0.00001984
Iteration 180/1000 | Loss: 0.00001984
Iteration 181/1000 | Loss: 0.00001984
Iteration 182/1000 | Loss: 0.00001984
Iteration 183/1000 | Loss: 0.00001983
Iteration 184/1000 | Loss: 0.00001983
Iteration 185/1000 | Loss: 0.00001983
Iteration 186/1000 | Loss: 0.00001983
Iteration 187/1000 | Loss: 0.00001983
Iteration 188/1000 | Loss: 0.00001983
Iteration 189/1000 | Loss: 0.00001983
Iteration 190/1000 | Loss: 0.00001983
Iteration 191/1000 | Loss: 0.00001983
Iteration 192/1000 | Loss: 0.00001983
Iteration 193/1000 | Loss: 0.00001983
Iteration 194/1000 | Loss: 0.00001983
Iteration 195/1000 | Loss: 0.00001983
Iteration 196/1000 | Loss: 0.00001983
Iteration 197/1000 | Loss: 0.00001983
Iteration 198/1000 | Loss: 0.00001983
Iteration 199/1000 | Loss: 0.00001983
Iteration 200/1000 | Loss: 0.00001983
Iteration 201/1000 | Loss: 0.00001982
Iteration 202/1000 | Loss: 0.00001982
Iteration 203/1000 | Loss: 0.00001982
Iteration 204/1000 | Loss: 0.00001982
Iteration 205/1000 | Loss: 0.00001982
Iteration 206/1000 | Loss: 0.00001982
Iteration 207/1000 | Loss: 0.00001982
Iteration 208/1000 | Loss: 0.00001982
Iteration 209/1000 | Loss: 0.00001982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.9824086848529987e-05, 1.9824086848529987e-05, 1.9824086848529987e-05, 1.9824086848529987e-05, 1.9824086848529987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9824086848529987e-05

Optimization complete. Final v2v error: 3.780177593231201 mm

Highest mean error: 4.789022922515869 mm for frame 152

Lowest mean error: 3.1832897663116455 mm for frame 169

Saving results

Total time: 81.77931213378906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853122
Iteration 2/25 | Loss: 0.00077043
Iteration 3/25 | Loss: 0.00064824
Iteration 4/25 | Loss: 0.00062209
Iteration 5/25 | Loss: 0.00061242
Iteration 6/25 | Loss: 0.00061103
Iteration 7/25 | Loss: 0.00061081
Iteration 8/25 | Loss: 0.00061081
Iteration 9/25 | Loss: 0.00061081
Iteration 10/25 | Loss: 0.00061081
Iteration 11/25 | Loss: 0.00061081
Iteration 12/25 | Loss: 0.00061081
Iteration 13/25 | Loss: 0.00061081
Iteration 14/25 | Loss: 0.00061081
Iteration 15/25 | Loss: 0.00061081
Iteration 16/25 | Loss: 0.00061081
Iteration 17/25 | Loss: 0.00061081
Iteration 18/25 | Loss: 0.00061081
Iteration 19/25 | Loss: 0.00061081
Iteration 20/25 | Loss: 0.00061081
Iteration 21/25 | Loss: 0.00061081
Iteration 22/25 | Loss: 0.00061081
Iteration 23/25 | Loss: 0.00061081
Iteration 24/25 | Loss: 0.00061081
Iteration 25/25 | Loss: 0.00061081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.32206059
Iteration 2/25 | Loss: 0.00029079
Iteration 3/25 | Loss: 0.00029078
Iteration 4/25 | Loss: 0.00029078
Iteration 5/25 | Loss: 0.00029077
Iteration 6/25 | Loss: 0.00029077
Iteration 7/25 | Loss: 0.00029077
Iteration 8/25 | Loss: 0.00029077
Iteration 9/25 | Loss: 0.00029077
Iteration 10/25 | Loss: 0.00029077
Iteration 11/25 | Loss: 0.00029077
Iteration 12/25 | Loss: 0.00029077
Iteration 13/25 | Loss: 0.00029077
Iteration 14/25 | Loss: 0.00029077
Iteration 15/25 | Loss: 0.00029077
Iteration 16/25 | Loss: 0.00029077
Iteration 17/25 | Loss: 0.00029077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002907733141910285, 0.0002907733141910285, 0.0002907733141910285, 0.0002907733141910285, 0.0002907733141910285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002907733141910285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029077
Iteration 2/1000 | Loss: 0.00003028
Iteration 3/1000 | Loss: 0.00002090
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001695
Iteration 6/1000 | Loss: 0.00001635
Iteration 7/1000 | Loss: 0.00001593
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001541
Iteration 10/1000 | Loss: 0.00001527
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001519
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001513
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001510
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001495
Iteration 23/1000 | Loss: 0.00001494
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001485
Iteration 28/1000 | Loss: 0.00001484
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001483
Iteration 31/1000 | Loss: 0.00001483
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001482
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001482
Iteration 36/1000 | Loss: 0.00001481
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001481
Iteration 39/1000 | Loss: 0.00001480
Iteration 40/1000 | Loss: 0.00001480
Iteration 41/1000 | Loss: 0.00001479
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001478
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001473
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001471
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001468
Iteration 52/1000 | Loss: 0.00001468
Iteration 53/1000 | Loss: 0.00001468
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001466
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001465
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001465
Iteration 70/1000 | Loss: 0.00001465
Iteration 71/1000 | Loss: 0.00001465
Iteration 72/1000 | Loss: 0.00001465
Iteration 73/1000 | Loss: 0.00001465
Iteration 74/1000 | Loss: 0.00001464
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001464
Iteration 77/1000 | Loss: 0.00001464
Iteration 78/1000 | Loss: 0.00001464
Iteration 79/1000 | Loss: 0.00001464
Iteration 80/1000 | Loss: 0.00001464
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001462
Iteration 86/1000 | Loss: 0.00001462
Iteration 87/1000 | Loss: 0.00001462
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001461
Iteration 94/1000 | Loss: 0.00001461
Iteration 95/1000 | Loss: 0.00001461
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001461
Iteration 99/1000 | Loss: 0.00001460
Iteration 100/1000 | Loss: 0.00001460
Iteration 101/1000 | Loss: 0.00001460
Iteration 102/1000 | Loss: 0.00001460
Iteration 103/1000 | Loss: 0.00001460
Iteration 104/1000 | Loss: 0.00001460
Iteration 105/1000 | Loss: 0.00001460
Iteration 106/1000 | Loss: 0.00001460
Iteration 107/1000 | Loss: 0.00001459
Iteration 108/1000 | Loss: 0.00001459
Iteration 109/1000 | Loss: 0.00001459
Iteration 110/1000 | Loss: 0.00001459
Iteration 111/1000 | Loss: 0.00001459
Iteration 112/1000 | Loss: 0.00001459
Iteration 113/1000 | Loss: 0.00001459
Iteration 114/1000 | Loss: 0.00001459
Iteration 115/1000 | Loss: 0.00001459
Iteration 116/1000 | Loss: 0.00001459
Iteration 117/1000 | Loss: 0.00001459
Iteration 118/1000 | Loss: 0.00001459
Iteration 119/1000 | Loss: 0.00001458
Iteration 120/1000 | Loss: 0.00001458
Iteration 121/1000 | Loss: 0.00001458
Iteration 122/1000 | Loss: 0.00001458
Iteration 123/1000 | Loss: 0.00001458
Iteration 124/1000 | Loss: 0.00001458
Iteration 125/1000 | Loss: 0.00001458
Iteration 126/1000 | Loss: 0.00001458
Iteration 127/1000 | Loss: 0.00001458
Iteration 128/1000 | Loss: 0.00001457
Iteration 129/1000 | Loss: 0.00001457
Iteration 130/1000 | Loss: 0.00001457
Iteration 131/1000 | Loss: 0.00001457
Iteration 132/1000 | Loss: 0.00001457
Iteration 133/1000 | Loss: 0.00001457
Iteration 134/1000 | Loss: 0.00001457
Iteration 135/1000 | Loss: 0.00001457
Iteration 136/1000 | Loss: 0.00001457
Iteration 137/1000 | Loss: 0.00001457
Iteration 138/1000 | Loss: 0.00001457
Iteration 139/1000 | Loss: 0.00001457
Iteration 140/1000 | Loss: 0.00001457
Iteration 141/1000 | Loss: 0.00001457
Iteration 142/1000 | Loss: 0.00001457
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001456
Iteration 146/1000 | Loss: 0.00001456
Iteration 147/1000 | Loss: 0.00001456
Iteration 148/1000 | Loss: 0.00001456
Iteration 149/1000 | Loss: 0.00001456
Iteration 150/1000 | Loss: 0.00001456
Iteration 151/1000 | Loss: 0.00001456
Iteration 152/1000 | Loss: 0.00001456
Iteration 153/1000 | Loss: 0.00001456
Iteration 154/1000 | Loss: 0.00001456
Iteration 155/1000 | Loss: 0.00001456
Iteration 156/1000 | Loss: 0.00001456
Iteration 157/1000 | Loss: 0.00001456
Iteration 158/1000 | Loss: 0.00001456
Iteration 159/1000 | Loss: 0.00001456
Iteration 160/1000 | Loss: 0.00001456
Iteration 161/1000 | Loss: 0.00001456
Iteration 162/1000 | Loss: 0.00001456
Iteration 163/1000 | Loss: 0.00001456
Iteration 164/1000 | Loss: 0.00001456
Iteration 165/1000 | Loss: 0.00001456
Iteration 166/1000 | Loss: 0.00001456
Iteration 167/1000 | Loss: 0.00001456
Iteration 168/1000 | Loss: 0.00001456
Iteration 169/1000 | Loss: 0.00001456
Iteration 170/1000 | Loss: 0.00001456
Iteration 171/1000 | Loss: 0.00001456
Iteration 172/1000 | Loss: 0.00001456
Iteration 173/1000 | Loss: 0.00001456
Iteration 174/1000 | Loss: 0.00001456
Iteration 175/1000 | Loss: 0.00001456
Iteration 176/1000 | Loss: 0.00001456
Iteration 177/1000 | Loss: 0.00001456
Iteration 178/1000 | Loss: 0.00001456
Iteration 179/1000 | Loss: 0.00001456
Iteration 180/1000 | Loss: 0.00001456
Iteration 181/1000 | Loss: 0.00001456
Iteration 182/1000 | Loss: 0.00001456
Iteration 183/1000 | Loss: 0.00001456
Iteration 184/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4556434507539961e-05, 1.4556434507539961e-05, 1.4556434507539961e-05, 1.4556434507539961e-05, 1.4556434507539961e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4556434507539961e-05

Optimization complete. Final v2v error: 3.2580716609954834 mm

Highest mean error: 3.5586936473846436 mm for frame 76

Lowest mean error: 2.9282140731811523 mm for frame 138

Saving results

Total time: 52.765769481658936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944544
Iteration 2/25 | Loss: 0.00119646
Iteration 3/25 | Loss: 0.00089792
Iteration 4/25 | Loss: 0.00085382
Iteration 5/25 | Loss: 0.00086209
Iteration 6/25 | Loss: 0.00083733
Iteration 7/25 | Loss: 0.00081414
Iteration 8/25 | Loss: 0.00080696
Iteration 9/25 | Loss: 0.00081389
Iteration 10/25 | Loss: 0.00081069
Iteration 11/25 | Loss: 0.00080370
Iteration 12/25 | Loss: 0.00080221
Iteration 13/25 | Loss: 0.00080635
Iteration 14/25 | Loss: 0.00080620
Iteration 15/25 | Loss: 0.00080461
Iteration 16/25 | Loss: 0.00080215
Iteration 17/25 | Loss: 0.00080061
Iteration 18/25 | Loss: 0.00080036
Iteration 19/25 | Loss: 0.00080032
Iteration 20/25 | Loss: 0.00080032
Iteration 21/25 | Loss: 0.00080032
Iteration 22/25 | Loss: 0.00080032
Iteration 23/25 | Loss: 0.00080032
Iteration 24/25 | Loss: 0.00080032
Iteration 25/25 | Loss: 0.00080032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42082000
Iteration 2/25 | Loss: 0.00036166
Iteration 3/25 | Loss: 0.00036160
Iteration 4/25 | Loss: 0.00036160
Iteration 5/25 | Loss: 0.00036160
Iteration 6/25 | Loss: 0.00036160
Iteration 7/25 | Loss: 0.00036160
Iteration 8/25 | Loss: 0.00036160
Iteration 9/25 | Loss: 0.00036160
Iteration 10/25 | Loss: 0.00036160
Iteration 11/25 | Loss: 0.00036160
Iteration 12/25 | Loss: 0.00036160
Iteration 13/25 | Loss: 0.00036160
Iteration 14/25 | Loss: 0.00036160
Iteration 15/25 | Loss: 0.00036160
Iteration 16/25 | Loss: 0.00036160
Iteration 17/25 | Loss: 0.00036160
Iteration 18/25 | Loss: 0.00036160
Iteration 19/25 | Loss: 0.00036160
Iteration 20/25 | Loss: 0.00036160
Iteration 21/25 | Loss: 0.00036160
Iteration 22/25 | Loss: 0.00036160
Iteration 23/25 | Loss: 0.00036160
Iteration 24/25 | Loss: 0.00036160
Iteration 25/25 | Loss: 0.00036160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036160
Iteration 2/1000 | Loss: 0.00004530
Iteration 3/1000 | Loss: 0.00003066
Iteration 4/1000 | Loss: 0.00002795
Iteration 5/1000 | Loss: 0.00002689
Iteration 6/1000 | Loss: 0.00002607
Iteration 7/1000 | Loss: 0.00002549
Iteration 8/1000 | Loss: 0.00002502
Iteration 9/1000 | Loss: 0.00002458
Iteration 10/1000 | Loss: 0.00002444
Iteration 11/1000 | Loss: 0.00002422
Iteration 12/1000 | Loss: 0.00002406
Iteration 13/1000 | Loss: 0.00002405
Iteration 14/1000 | Loss: 0.00002402
Iteration 15/1000 | Loss: 0.00002400
Iteration 16/1000 | Loss: 0.00002395
Iteration 17/1000 | Loss: 0.00002394
Iteration 18/1000 | Loss: 0.00002389
Iteration 19/1000 | Loss: 0.00002383
Iteration 20/1000 | Loss: 0.00002382
Iteration 21/1000 | Loss: 0.00002381
Iteration 22/1000 | Loss: 0.00002380
Iteration 23/1000 | Loss: 0.00002379
Iteration 24/1000 | Loss: 0.00002378
Iteration 25/1000 | Loss: 0.00002377
Iteration 26/1000 | Loss: 0.00002377
Iteration 27/1000 | Loss: 0.00002376
Iteration 28/1000 | Loss: 0.00002375
Iteration 29/1000 | Loss: 0.00002375
Iteration 30/1000 | Loss: 0.00002374
Iteration 31/1000 | Loss: 0.00002370
Iteration 32/1000 | Loss: 0.00002370
Iteration 33/1000 | Loss: 0.00002369
Iteration 34/1000 | Loss: 0.00002369
Iteration 35/1000 | Loss: 0.00002368
Iteration 36/1000 | Loss: 0.00002367
Iteration 37/1000 | Loss: 0.00002367
Iteration 38/1000 | Loss: 0.00002367
Iteration 39/1000 | Loss: 0.00002366
Iteration 40/1000 | Loss: 0.00002366
Iteration 41/1000 | Loss: 0.00002366
Iteration 42/1000 | Loss: 0.00002365
Iteration 43/1000 | Loss: 0.00002365
Iteration 44/1000 | Loss: 0.00002365
Iteration 45/1000 | Loss: 0.00002364
Iteration 46/1000 | Loss: 0.00002363
Iteration 47/1000 | Loss: 0.00002363
Iteration 48/1000 | Loss: 0.00002363
Iteration 49/1000 | Loss: 0.00002363
Iteration 50/1000 | Loss: 0.00002363
Iteration 51/1000 | Loss: 0.00002363
Iteration 52/1000 | Loss: 0.00002362
Iteration 53/1000 | Loss: 0.00002362
Iteration 54/1000 | Loss: 0.00002362
Iteration 55/1000 | Loss: 0.00002362
Iteration 56/1000 | Loss: 0.00002362
Iteration 57/1000 | Loss: 0.00002362
Iteration 58/1000 | Loss: 0.00002362
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00002361
Iteration 61/1000 | Loss: 0.00002361
Iteration 62/1000 | Loss: 0.00002361
Iteration 63/1000 | Loss: 0.00002360
Iteration 64/1000 | Loss: 0.00002360
Iteration 65/1000 | Loss: 0.00002360
Iteration 66/1000 | Loss: 0.00002360
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00002359
Iteration 69/1000 | Loss: 0.00002359
Iteration 70/1000 | Loss: 0.00002359
Iteration 71/1000 | Loss: 0.00002359
Iteration 72/1000 | Loss: 0.00002359
Iteration 73/1000 | Loss: 0.00002359
Iteration 74/1000 | Loss: 0.00002359
Iteration 75/1000 | Loss: 0.00002359
Iteration 76/1000 | Loss: 0.00002359
Iteration 77/1000 | Loss: 0.00002358
Iteration 78/1000 | Loss: 0.00002358
Iteration 79/1000 | Loss: 0.00002358
Iteration 80/1000 | Loss: 0.00002358
Iteration 81/1000 | Loss: 0.00002357
Iteration 82/1000 | Loss: 0.00002357
Iteration 83/1000 | Loss: 0.00002357
Iteration 84/1000 | Loss: 0.00002356
Iteration 85/1000 | Loss: 0.00002356
Iteration 86/1000 | Loss: 0.00002356
Iteration 87/1000 | Loss: 0.00002356
Iteration 88/1000 | Loss: 0.00002355
Iteration 89/1000 | Loss: 0.00002355
Iteration 90/1000 | Loss: 0.00002355
Iteration 91/1000 | Loss: 0.00002355
Iteration 92/1000 | Loss: 0.00002355
Iteration 93/1000 | Loss: 0.00002354
Iteration 94/1000 | Loss: 0.00002354
Iteration 95/1000 | Loss: 0.00002354
Iteration 96/1000 | Loss: 0.00002354
Iteration 97/1000 | Loss: 0.00002353
Iteration 98/1000 | Loss: 0.00002353
Iteration 99/1000 | Loss: 0.00002353
Iteration 100/1000 | Loss: 0.00002353
Iteration 101/1000 | Loss: 0.00002353
Iteration 102/1000 | Loss: 0.00002353
Iteration 103/1000 | Loss: 0.00002353
Iteration 104/1000 | Loss: 0.00002353
Iteration 105/1000 | Loss: 0.00002353
Iteration 106/1000 | Loss: 0.00002353
Iteration 107/1000 | Loss: 0.00002353
Iteration 108/1000 | Loss: 0.00002353
Iteration 109/1000 | Loss: 0.00002352
Iteration 110/1000 | Loss: 0.00002352
Iteration 111/1000 | Loss: 0.00002352
Iteration 112/1000 | Loss: 0.00002352
Iteration 113/1000 | Loss: 0.00002352
Iteration 114/1000 | Loss: 0.00002352
Iteration 115/1000 | Loss: 0.00002352
Iteration 116/1000 | Loss: 0.00002352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.3522190531366505e-05, 2.3522190531366505e-05, 2.3522190531366505e-05, 2.3522190531366505e-05, 2.3522190531366505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3522190531366505e-05

Optimization complete. Final v2v error: 3.9795284271240234 mm

Highest mean error: 4.839544773101807 mm for frame 88

Lowest mean error: 3.5612590312957764 mm for frame 233

Saving results

Total time: 74.7151734828949
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466205
Iteration 2/25 | Loss: 0.00083335
Iteration 3/25 | Loss: 0.00064835
Iteration 4/25 | Loss: 0.00062376
Iteration 5/25 | Loss: 0.00061654
Iteration 6/25 | Loss: 0.00061482
Iteration 7/25 | Loss: 0.00061462
Iteration 8/25 | Loss: 0.00061462
Iteration 9/25 | Loss: 0.00061462
Iteration 10/25 | Loss: 0.00061462
Iteration 11/25 | Loss: 0.00061462
Iteration 12/25 | Loss: 0.00061462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006146152154542506, 0.0006146152154542506, 0.0006146152154542506, 0.0006146152154542506, 0.0006146152154542506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006146152154542506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.20559120
Iteration 2/25 | Loss: 0.00027592
Iteration 3/25 | Loss: 0.00027590
Iteration 4/25 | Loss: 0.00027590
Iteration 5/25 | Loss: 0.00027590
Iteration 6/25 | Loss: 0.00027590
Iteration 7/25 | Loss: 0.00027590
Iteration 8/25 | Loss: 0.00027590
Iteration 9/25 | Loss: 0.00027590
Iteration 10/25 | Loss: 0.00027590
Iteration 11/25 | Loss: 0.00027590
Iteration 12/25 | Loss: 0.00027590
Iteration 13/25 | Loss: 0.00027590
Iteration 14/25 | Loss: 0.00027590
Iteration 15/25 | Loss: 0.00027590
Iteration 16/25 | Loss: 0.00027590
Iteration 17/25 | Loss: 0.00027590
Iteration 18/25 | Loss: 0.00027590
Iteration 19/25 | Loss: 0.00027590
Iteration 20/25 | Loss: 0.00027590
Iteration 21/25 | Loss: 0.00027590
Iteration 22/25 | Loss: 0.00027590
Iteration 23/25 | Loss: 0.00027590
Iteration 24/25 | Loss: 0.00027590
Iteration 25/25 | Loss: 0.00027590

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027590
Iteration 2/1000 | Loss: 0.00002437
Iteration 3/1000 | Loss: 0.00001604
Iteration 4/1000 | Loss: 0.00001485
Iteration 5/1000 | Loss: 0.00001398
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001298
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001262
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001258
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001252
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001249
Iteration 22/1000 | Loss: 0.00001248
Iteration 23/1000 | Loss: 0.00001248
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001242
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001239
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001237
Iteration 88/1000 | Loss: 0.00001237
Iteration 89/1000 | Loss: 0.00001237
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2314231753407512e-05, 1.2314231753407512e-05, 1.2314231753407512e-05, 1.2314231753407512e-05, 1.2314231753407512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2314231753407512e-05

Optimization complete. Final v2v error: 2.989997386932373 mm

Highest mean error: 3.307577610015869 mm for frame 109

Lowest mean error: 2.7848494052886963 mm for frame 154

Saving results

Total time: 64.71577262878418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375631
Iteration 2/25 | Loss: 0.00082089
Iteration 3/25 | Loss: 0.00063276
Iteration 4/25 | Loss: 0.00060375
Iteration 5/25 | Loss: 0.00059749
Iteration 6/25 | Loss: 0.00059607
Iteration 7/25 | Loss: 0.00059585
Iteration 8/25 | Loss: 0.00059585
Iteration 9/25 | Loss: 0.00059585
Iteration 10/25 | Loss: 0.00059585
Iteration 11/25 | Loss: 0.00059585
Iteration 12/25 | Loss: 0.00059585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005958530819043517, 0.0005958530819043517, 0.0005958530819043517, 0.0005958530819043517, 0.0005958530819043517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005958530819043517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45806134
Iteration 2/25 | Loss: 0.00024818
Iteration 3/25 | Loss: 0.00024818
Iteration 4/25 | Loss: 0.00024818
Iteration 5/25 | Loss: 0.00024818
Iteration 6/25 | Loss: 0.00024818
Iteration 7/25 | Loss: 0.00024818
Iteration 8/25 | Loss: 0.00024818
Iteration 9/25 | Loss: 0.00024818
Iteration 10/25 | Loss: 0.00024818
Iteration 11/25 | Loss: 0.00024818
Iteration 12/25 | Loss: 0.00024818
Iteration 13/25 | Loss: 0.00024818
Iteration 14/25 | Loss: 0.00024818
Iteration 15/25 | Loss: 0.00024818
Iteration 16/25 | Loss: 0.00024818
Iteration 17/25 | Loss: 0.00024818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000248175609158352, 0.000248175609158352, 0.000248175609158352, 0.000248175609158352, 0.000248175609158352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000248175609158352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024818
Iteration 2/1000 | Loss: 0.00002658
Iteration 3/1000 | Loss: 0.00001741
Iteration 4/1000 | Loss: 0.00001512
Iteration 5/1000 | Loss: 0.00001416
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001296
Iteration 8/1000 | Loss: 0.00001262
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001213
Iteration 19/1000 | Loss: 0.00001213
Iteration 20/1000 | Loss: 0.00001212
Iteration 21/1000 | Loss: 0.00001212
Iteration 22/1000 | Loss: 0.00001212
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001211
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00001206
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001198
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001197
Iteration 43/1000 | Loss: 0.00001197
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001196
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001193
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001191
Iteration 56/1000 | Loss: 0.00001191
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001188
Iteration 65/1000 | Loss: 0.00001188
Iteration 66/1000 | Loss: 0.00001188
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001179
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001177
Iteration 107/1000 | Loss: 0.00001177
Iteration 108/1000 | Loss: 0.00001177
Iteration 109/1000 | Loss: 0.00001176
Iteration 110/1000 | Loss: 0.00001176
Iteration 111/1000 | Loss: 0.00001176
Iteration 112/1000 | Loss: 0.00001176
Iteration 113/1000 | Loss: 0.00001176
Iteration 114/1000 | Loss: 0.00001176
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001175
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001174
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001173
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001168
Iteration 154/1000 | Loss: 0.00001168
Iteration 155/1000 | Loss: 0.00001168
Iteration 156/1000 | Loss: 0.00001168
Iteration 157/1000 | Loss: 0.00001168
Iteration 158/1000 | Loss: 0.00001168
Iteration 159/1000 | Loss: 0.00001168
Iteration 160/1000 | Loss: 0.00001167
Iteration 161/1000 | Loss: 0.00001167
Iteration 162/1000 | Loss: 0.00001167
Iteration 163/1000 | Loss: 0.00001167
Iteration 164/1000 | Loss: 0.00001167
Iteration 165/1000 | Loss: 0.00001167
Iteration 166/1000 | Loss: 0.00001167
Iteration 167/1000 | Loss: 0.00001167
Iteration 168/1000 | Loss: 0.00001166
Iteration 169/1000 | Loss: 0.00001166
Iteration 170/1000 | Loss: 0.00001166
Iteration 171/1000 | Loss: 0.00001166
Iteration 172/1000 | Loss: 0.00001166
Iteration 173/1000 | Loss: 0.00001166
Iteration 174/1000 | Loss: 0.00001166
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001166
Iteration 178/1000 | Loss: 0.00001166
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001165
Iteration 184/1000 | Loss: 0.00001165
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001165
Iteration 187/1000 | Loss: 0.00001165
Iteration 188/1000 | Loss: 0.00001165
Iteration 189/1000 | Loss: 0.00001165
Iteration 190/1000 | Loss: 0.00001165
Iteration 191/1000 | Loss: 0.00001165
Iteration 192/1000 | Loss: 0.00001165
Iteration 193/1000 | Loss: 0.00001165
Iteration 194/1000 | Loss: 0.00001165
Iteration 195/1000 | Loss: 0.00001165
Iteration 196/1000 | Loss: 0.00001165
Iteration 197/1000 | Loss: 0.00001165
Iteration 198/1000 | Loss: 0.00001165
Iteration 199/1000 | Loss: 0.00001165
Iteration 200/1000 | Loss: 0.00001165
Iteration 201/1000 | Loss: 0.00001165
Iteration 202/1000 | Loss: 0.00001164
Iteration 203/1000 | Loss: 0.00001164
Iteration 204/1000 | Loss: 0.00001164
Iteration 205/1000 | Loss: 0.00001164
Iteration 206/1000 | Loss: 0.00001164
Iteration 207/1000 | Loss: 0.00001164
Iteration 208/1000 | Loss: 0.00001164
Iteration 209/1000 | Loss: 0.00001164
Iteration 210/1000 | Loss: 0.00001164
Iteration 211/1000 | Loss: 0.00001164
Iteration 212/1000 | Loss: 0.00001164
Iteration 213/1000 | Loss: 0.00001164
Iteration 214/1000 | Loss: 0.00001164
Iteration 215/1000 | Loss: 0.00001164
Iteration 216/1000 | Loss: 0.00001164
Iteration 217/1000 | Loss: 0.00001164
Iteration 218/1000 | Loss: 0.00001164
Iteration 219/1000 | Loss: 0.00001164
Iteration 220/1000 | Loss: 0.00001164
Iteration 221/1000 | Loss: 0.00001164
Iteration 222/1000 | Loss: 0.00001164
Iteration 223/1000 | Loss: 0.00001164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.1644605365290772e-05, 1.1644605365290772e-05, 1.1644605365290772e-05, 1.1644605365290772e-05, 1.1644605365290772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1644605365290772e-05

Optimization complete. Final v2v error: 2.9265546798706055 mm

Highest mean error: 3.010564088821411 mm for frame 76

Lowest mean error: 2.828230381011963 mm for frame 8

Saving results

Total time: 55.696572065353394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874084
Iteration 2/25 | Loss: 0.00091072
Iteration 3/25 | Loss: 0.00076284
Iteration 4/25 | Loss: 0.00071508
Iteration 5/25 | Loss: 0.00070522
Iteration 6/25 | Loss: 0.00070264
Iteration 7/25 | Loss: 0.00070170
Iteration 8/25 | Loss: 0.00070170
Iteration 9/25 | Loss: 0.00070170
Iteration 10/25 | Loss: 0.00070170
Iteration 11/25 | Loss: 0.00070170
Iteration 12/25 | Loss: 0.00070170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000701697135809809, 0.000701697135809809, 0.000701697135809809, 0.000701697135809809, 0.000701697135809809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000701697135809809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43275595
Iteration 2/25 | Loss: 0.00032640
Iteration 3/25 | Loss: 0.00032640
Iteration 4/25 | Loss: 0.00032640
Iteration 5/25 | Loss: 0.00032640
Iteration 6/25 | Loss: 0.00032640
Iteration 7/25 | Loss: 0.00032640
Iteration 8/25 | Loss: 0.00032640
Iteration 9/25 | Loss: 0.00032640
Iteration 10/25 | Loss: 0.00032640
Iteration 11/25 | Loss: 0.00032640
Iteration 12/25 | Loss: 0.00032640
Iteration 13/25 | Loss: 0.00032640
Iteration 14/25 | Loss: 0.00032640
Iteration 15/25 | Loss: 0.00032640
Iteration 16/25 | Loss: 0.00032640
Iteration 17/25 | Loss: 0.00032640
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00032639590790495276, 0.00032639590790495276, 0.00032639590790495276, 0.00032639590790495276, 0.00032639590790495276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032639590790495276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032640
Iteration 2/1000 | Loss: 0.00004758
Iteration 3/1000 | Loss: 0.00003502
Iteration 4/1000 | Loss: 0.00003162
Iteration 5/1000 | Loss: 0.00002994
Iteration 6/1000 | Loss: 0.00002868
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002715
Iteration 9/1000 | Loss: 0.00002662
Iteration 10/1000 | Loss: 0.00002653
Iteration 11/1000 | Loss: 0.00002615
Iteration 12/1000 | Loss: 0.00002594
Iteration 13/1000 | Loss: 0.00002593
Iteration 14/1000 | Loss: 0.00002579
Iteration 15/1000 | Loss: 0.00002564
Iteration 16/1000 | Loss: 0.00002560
Iteration 17/1000 | Loss: 0.00002559
Iteration 18/1000 | Loss: 0.00002558
Iteration 19/1000 | Loss: 0.00002558
Iteration 20/1000 | Loss: 0.00002557
Iteration 21/1000 | Loss: 0.00002556
Iteration 22/1000 | Loss: 0.00002554
Iteration 23/1000 | Loss: 0.00002554
Iteration 24/1000 | Loss: 0.00002554
Iteration 25/1000 | Loss: 0.00002553
Iteration 26/1000 | Loss: 0.00002552
Iteration 27/1000 | Loss: 0.00002550
Iteration 28/1000 | Loss: 0.00002549
Iteration 29/1000 | Loss: 0.00002548
Iteration 30/1000 | Loss: 0.00002548
Iteration 31/1000 | Loss: 0.00002548
Iteration 32/1000 | Loss: 0.00002547
Iteration 33/1000 | Loss: 0.00002546
Iteration 34/1000 | Loss: 0.00002546
Iteration 35/1000 | Loss: 0.00002545
Iteration 36/1000 | Loss: 0.00002544
Iteration 37/1000 | Loss: 0.00002544
Iteration 38/1000 | Loss: 0.00002543
Iteration 39/1000 | Loss: 0.00002543
Iteration 40/1000 | Loss: 0.00002541
Iteration 41/1000 | Loss: 0.00002541
Iteration 42/1000 | Loss: 0.00002539
Iteration 43/1000 | Loss: 0.00002539
Iteration 44/1000 | Loss: 0.00002539
Iteration 45/1000 | Loss: 0.00002539
Iteration 46/1000 | Loss: 0.00002539
Iteration 47/1000 | Loss: 0.00002539
Iteration 48/1000 | Loss: 0.00002538
Iteration 49/1000 | Loss: 0.00002538
Iteration 50/1000 | Loss: 0.00002538
Iteration 51/1000 | Loss: 0.00002538
Iteration 52/1000 | Loss: 0.00002538
Iteration 53/1000 | Loss: 0.00002538
Iteration 54/1000 | Loss: 0.00002538
Iteration 55/1000 | Loss: 0.00002538
Iteration 56/1000 | Loss: 0.00002538
Iteration 57/1000 | Loss: 0.00002538
Iteration 58/1000 | Loss: 0.00002537
Iteration 59/1000 | Loss: 0.00002537
Iteration 60/1000 | Loss: 0.00002536
Iteration 61/1000 | Loss: 0.00002536
Iteration 62/1000 | Loss: 0.00002535
Iteration 63/1000 | Loss: 0.00002535
Iteration 64/1000 | Loss: 0.00002535
Iteration 65/1000 | Loss: 0.00002535
Iteration 66/1000 | Loss: 0.00002535
Iteration 67/1000 | Loss: 0.00002535
Iteration 68/1000 | Loss: 0.00002535
Iteration 69/1000 | Loss: 0.00002535
Iteration 70/1000 | Loss: 0.00002535
Iteration 71/1000 | Loss: 0.00002535
Iteration 72/1000 | Loss: 0.00002535
Iteration 73/1000 | Loss: 0.00002534
Iteration 74/1000 | Loss: 0.00002534
Iteration 75/1000 | Loss: 0.00002534
Iteration 76/1000 | Loss: 0.00002533
Iteration 77/1000 | Loss: 0.00002533
Iteration 78/1000 | Loss: 0.00002533
Iteration 79/1000 | Loss: 0.00002533
Iteration 80/1000 | Loss: 0.00002533
Iteration 81/1000 | Loss: 0.00002533
Iteration 82/1000 | Loss: 0.00002533
Iteration 83/1000 | Loss: 0.00002533
Iteration 84/1000 | Loss: 0.00002533
Iteration 85/1000 | Loss: 0.00002532
Iteration 86/1000 | Loss: 0.00002532
Iteration 87/1000 | Loss: 0.00002532
Iteration 88/1000 | Loss: 0.00002532
Iteration 89/1000 | Loss: 0.00002532
Iteration 90/1000 | Loss: 0.00002532
Iteration 91/1000 | Loss: 0.00002532
Iteration 92/1000 | Loss: 0.00002532
Iteration 93/1000 | Loss: 0.00002532
Iteration 94/1000 | Loss: 0.00002532
Iteration 95/1000 | Loss: 0.00002532
Iteration 96/1000 | Loss: 0.00002532
Iteration 97/1000 | Loss: 0.00002532
Iteration 98/1000 | Loss: 0.00002532
Iteration 99/1000 | Loss: 0.00002532
Iteration 100/1000 | Loss: 0.00002532
Iteration 101/1000 | Loss: 0.00002532
Iteration 102/1000 | Loss: 0.00002531
Iteration 103/1000 | Loss: 0.00002531
Iteration 104/1000 | Loss: 0.00002531
Iteration 105/1000 | Loss: 0.00002531
Iteration 106/1000 | Loss: 0.00002531
Iteration 107/1000 | Loss: 0.00002531
Iteration 108/1000 | Loss: 0.00002531
Iteration 109/1000 | Loss: 0.00002531
Iteration 110/1000 | Loss: 0.00002530
Iteration 111/1000 | Loss: 0.00002530
Iteration 112/1000 | Loss: 0.00002530
Iteration 113/1000 | Loss: 0.00002530
Iteration 114/1000 | Loss: 0.00002530
Iteration 115/1000 | Loss: 0.00002530
Iteration 116/1000 | Loss: 0.00002530
Iteration 117/1000 | Loss: 0.00002530
Iteration 118/1000 | Loss: 0.00002530
Iteration 119/1000 | Loss: 0.00002530
Iteration 120/1000 | Loss: 0.00002530
Iteration 121/1000 | Loss: 0.00002529
Iteration 122/1000 | Loss: 0.00002529
Iteration 123/1000 | Loss: 0.00002529
Iteration 124/1000 | Loss: 0.00002529
Iteration 125/1000 | Loss: 0.00002529
Iteration 126/1000 | Loss: 0.00002529
Iteration 127/1000 | Loss: 0.00002529
Iteration 128/1000 | Loss: 0.00002529
Iteration 129/1000 | Loss: 0.00002528
Iteration 130/1000 | Loss: 0.00002528
Iteration 131/1000 | Loss: 0.00002528
Iteration 132/1000 | Loss: 0.00002528
Iteration 133/1000 | Loss: 0.00002528
Iteration 134/1000 | Loss: 0.00002528
Iteration 135/1000 | Loss: 0.00002528
Iteration 136/1000 | Loss: 0.00002528
Iteration 137/1000 | Loss: 0.00002528
Iteration 138/1000 | Loss: 0.00002528
Iteration 139/1000 | Loss: 0.00002528
Iteration 140/1000 | Loss: 0.00002528
Iteration 141/1000 | Loss: 0.00002528
Iteration 142/1000 | Loss: 0.00002527
Iteration 143/1000 | Loss: 0.00002527
Iteration 144/1000 | Loss: 0.00002527
Iteration 145/1000 | Loss: 0.00002527
Iteration 146/1000 | Loss: 0.00002527
Iteration 147/1000 | Loss: 0.00002527
Iteration 148/1000 | Loss: 0.00002527
Iteration 149/1000 | Loss: 0.00002527
Iteration 150/1000 | Loss: 0.00002527
Iteration 151/1000 | Loss: 0.00002527
Iteration 152/1000 | Loss: 0.00002527
Iteration 153/1000 | Loss: 0.00002527
Iteration 154/1000 | Loss: 0.00002527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.527298784116283e-05, 2.527298784116283e-05, 2.527298784116283e-05, 2.527298784116283e-05, 2.527298784116283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.527298784116283e-05

Optimization complete. Final v2v error: 4.145322322845459 mm

Highest mean error: 4.716073036193848 mm for frame 124

Lowest mean error: 3.7504587173461914 mm for frame 209

Saving results

Total time: 74.42426657676697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884536
Iteration 2/25 | Loss: 0.00081392
Iteration 3/25 | Loss: 0.00065326
Iteration 4/25 | Loss: 0.00062246
Iteration 5/25 | Loss: 0.00061725
Iteration 6/25 | Loss: 0.00061661
Iteration 7/25 | Loss: 0.00061661
Iteration 8/25 | Loss: 0.00061661
Iteration 9/25 | Loss: 0.00061661
Iteration 10/25 | Loss: 0.00061661
Iteration 11/25 | Loss: 0.00061661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006166056264191866, 0.0006166056264191866, 0.0006166056264191866, 0.0006166056264191866, 0.0006166056264191866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006166056264191866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.98313046
Iteration 2/25 | Loss: 0.00022138
Iteration 3/25 | Loss: 0.00022138
Iteration 4/25 | Loss: 0.00022138
Iteration 5/25 | Loss: 0.00022138
Iteration 6/25 | Loss: 0.00022138
Iteration 7/25 | Loss: 0.00022138
Iteration 8/25 | Loss: 0.00022138
Iteration 9/25 | Loss: 0.00022138
Iteration 10/25 | Loss: 0.00022138
Iteration 11/25 | Loss: 0.00022138
Iteration 12/25 | Loss: 0.00022138
Iteration 13/25 | Loss: 0.00022138
Iteration 14/25 | Loss: 0.00022138
Iteration 15/25 | Loss: 0.00022138
Iteration 16/25 | Loss: 0.00022138
Iteration 17/25 | Loss: 0.00022138
Iteration 18/25 | Loss: 0.00022138
Iteration 19/25 | Loss: 0.00022138
Iteration 20/25 | Loss: 0.00022138
Iteration 21/25 | Loss: 0.00022138
Iteration 22/25 | Loss: 0.00022138
Iteration 23/25 | Loss: 0.00022138
Iteration 24/25 | Loss: 0.00022138
Iteration 25/25 | Loss: 0.00022138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022138
Iteration 2/1000 | Loss: 0.00002163
Iteration 3/1000 | Loss: 0.00001667
Iteration 4/1000 | Loss: 0.00001549
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001346
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001330
Iteration 11/1000 | Loss: 0.00001328
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001322
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001319
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001315
Iteration 34/1000 | Loss: 0.00001314
Iteration 35/1000 | Loss: 0.00001314
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001313
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001313
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001310
Iteration 57/1000 | Loss: 0.00001310
Iteration 58/1000 | Loss: 0.00001310
Iteration 59/1000 | Loss: 0.00001310
Iteration 60/1000 | Loss: 0.00001310
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001309
Iteration 66/1000 | Loss: 0.00001309
Iteration 67/1000 | Loss: 0.00001309
Iteration 68/1000 | Loss: 0.00001309
Iteration 69/1000 | Loss: 0.00001309
Iteration 70/1000 | Loss: 0.00001309
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001308
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001307
Iteration 86/1000 | Loss: 0.00001307
Iteration 87/1000 | Loss: 0.00001307
Iteration 88/1000 | Loss: 0.00001307
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001306
Iteration 92/1000 | Loss: 0.00001306
Iteration 93/1000 | Loss: 0.00001306
Iteration 94/1000 | Loss: 0.00001306
Iteration 95/1000 | Loss: 0.00001306
Iteration 96/1000 | Loss: 0.00001306
Iteration 97/1000 | Loss: 0.00001306
Iteration 98/1000 | Loss: 0.00001306
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001305
Iteration 101/1000 | Loss: 0.00001305
Iteration 102/1000 | Loss: 0.00001305
Iteration 103/1000 | Loss: 0.00001305
Iteration 104/1000 | Loss: 0.00001305
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001304
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001304
Iteration 114/1000 | Loss: 0.00001304
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001303
Iteration 121/1000 | Loss: 0.00001303
Iteration 122/1000 | Loss: 0.00001303
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001302
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001301
Iteration 130/1000 | Loss: 0.00001301
Iteration 131/1000 | Loss: 0.00001301
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001300
Iteration 136/1000 | Loss: 0.00001300
Iteration 137/1000 | Loss: 0.00001299
Iteration 138/1000 | Loss: 0.00001299
Iteration 139/1000 | Loss: 0.00001299
Iteration 140/1000 | Loss: 0.00001299
Iteration 141/1000 | Loss: 0.00001299
Iteration 142/1000 | Loss: 0.00001299
Iteration 143/1000 | Loss: 0.00001299
Iteration 144/1000 | Loss: 0.00001299
Iteration 145/1000 | Loss: 0.00001298
Iteration 146/1000 | Loss: 0.00001298
Iteration 147/1000 | Loss: 0.00001298
Iteration 148/1000 | Loss: 0.00001298
Iteration 149/1000 | Loss: 0.00001298
Iteration 150/1000 | Loss: 0.00001298
Iteration 151/1000 | Loss: 0.00001298
Iteration 152/1000 | Loss: 0.00001298
Iteration 153/1000 | Loss: 0.00001298
Iteration 154/1000 | Loss: 0.00001298
Iteration 155/1000 | Loss: 0.00001298
Iteration 156/1000 | Loss: 0.00001298
Iteration 157/1000 | Loss: 0.00001298
Iteration 158/1000 | Loss: 0.00001297
Iteration 159/1000 | Loss: 0.00001297
Iteration 160/1000 | Loss: 0.00001297
Iteration 161/1000 | Loss: 0.00001297
Iteration 162/1000 | Loss: 0.00001297
Iteration 163/1000 | Loss: 0.00001297
Iteration 164/1000 | Loss: 0.00001297
Iteration 165/1000 | Loss: 0.00001297
Iteration 166/1000 | Loss: 0.00001296
Iteration 167/1000 | Loss: 0.00001296
Iteration 168/1000 | Loss: 0.00001296
Iteration 169/1000 | Loss: 0.00001296
Iteration 170/1000 | Loss: 0.00001296
Iteration 171/1000 | Loss: 0.00001296
Iteration 172/1000 | Loss: 0.00001296
Iteration 173/1000 | Loss: 0.00001296
Iteration 174/1000 | Loss: 0.00001296
Iteration 175/1000 | Loss: 0.00001296
Iteration 176/1000 | Loss: 0.00001296
Iteration 177/1000 | Loss: 0.00001296
Iteration 178/1000 | Loss: 0.00001296
Iteration 179/1000 | Loss: 0.00001295
Iteration 180/1000 | Loss: 0.00001295
Iteration 181/1000 | Loss: 0.00001295
Iteration 182/1000 | Loss: 0.00001295
Iteration 183/1000 | Loss: 0.00001295
Iteration 184/1000 | Loss: 0.00001295
Iteration 185/1000 | Loss: 0.00001295
Iteration 186/1000 | Loss: 0.00001295
Iteration 187/1000 | Loss: 0.00001295
Iteration 188/1000 | Loss: 0.00001295
Iteration 189/1000 | Loss: 0.00001295
Iteration 190/1000 | Loss: 0.00001295
Iteration 191/1000 | Loss: 0.00001295
Iteration 192/1000 | Loss: 0.00001295
Iteration 193/1000 | Loss: 0.00001295
Iteration 194/1000 | Loss: 0.00001295
Iteration 195/1000 | Loss: 0.00001295
Iteration 196/1000 | Loss: 0.00001295
Iteration 197/1000 | Loss: 0.00001295
Iteration 198/1000 | Loss: 0.00001295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.2946271453984082e-05, 1.2946271453984082e-05, 1.2946271453984082e-05, 1.2946271453984082e-05, 1.2946271453984082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2946271453984082e-05

Optimization complete. Final v2v error: 3.0557518005371094 mm

Highest mean error: 3.2483866214752197 mm for frame 239

Lowest mean error: 2.9432687759399414 mm for frame 182

Saving results

Total time: 40.15869617462158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605337
Iteration 2/25 | Loss: 0.00084047
Iteration 3/25 | Loss: 0.00068020
Iteration 4/25 | Loss: 0.00065499
Iteration 5/25 | Loss: 0.00064520
Iteration 6/25 | Loss: 0.00064270
Iteration 7/25 | Loss: 0.00064203
Iteration 8/25 | Loss: 0.00064202
Iteration 9/25 | Loss: 0.00064202
Iteration 10/25 | Loss: 0.00064202
Iteration 11/25 | Loss: 0.00064202
Iteration 12/25 | Loss: 0.00064202
Iteration 13/25 | Loss: 0.00064202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006420228164643049, 0.0006420228164643049, 0.0006420228164643049, 0.0006420228164643049, 0.0006420228164643049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006420228164643049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.55644083
Iteration 2/25 | Loss: 0.00036850
Iteration 3/25 | Loss: 0.00036847
Iteration 4/25 | Loss: 0.00036847
Iteration 5/25 | Loss: 0.00036847
Iteration 6/25 | Loss: 0.00036847
Iteration 7/25 | Loss: 0.00036847
Iteration 8/25 | Loss: 0.00036847
Iteration 9/25 | Loss: 0.00036847
Iteration 10/25 | Loss: 0.00036847
Iteration 11/25 | Loss: 0.00036847
Iteration 12/25 | Loss: 0.00036847
Iteration 13/25 | Loss: 0.00036847
Iteration 14/25 | Loss: 0.00036847
Iteration 15/25 | Loss: 0.00036847
Iteration 16/25 | Loss: 0.00036847
Iteration 17/25 | Loss: 0.00036847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00036846951115876436, 0.00036846951115876436, 0.00036846951115876436, 0.00036846951115876436, 0.00036846951115876436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036846951115876436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036847
Iteration 2/1000 | Loss: 0.00003281
Iteration 3/1000 | Loss: 0.00001969
Iteration 4/1000 | Loss: 0.00001724
Iteration 5/1000 | Loss: 0.00001622
Iteration 6/1000 | Loss: 0.00001572
Iteration 7/1000 | Loss: 0.00001539
Iteration 8/1000 | Loss: 0.00001511
Iteration 9/1000 | Loss: 0.00001493
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001476
Iteration 12/1000 | Loss: 0.00001476
Iteration 13/1000 | Loss: 0.00001474
Iteration 14/1000 | Loss: 0.00001473
Iteration 15/1000 | Loss: 0.00001469
Iteration 16/1000 | Loss: 0.00001466
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001462
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001461
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001459
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001457
Iteration 34/1000 | Loss: 0.00001457
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001456
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001455
Iteration 46/1000 | Loss: 0.00001455
Iteration 47/1000 | Loss: 0.00001455
Iteration 48/1000 | Loss: 0.00001455
Iteration 49/1000 | Loss: 0.00001455
Iteration 50/1000 | Loss: 0.00001454
Iteration 51/1000 | Loss: 0.00001454
Iteration 52/1000 | Loss: 0.00001454
Iteration 53/1000 | Loss: 0.00001454
Iteration 54/1000 | Loss: 0.00001454
Iteration 55/1000 | Loss: 0.00001454
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001454
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001453
Iteration 65/1000 | Loss: 0.00001452
Iteration 66/1000 | Loss: 0.00001452
Iteration 67/1000 | Loss: 0.00001452
Iteration 68/1000 | Loss: 0.00001452
Iteration 69/1000 | Loss: 0.00001452
Iteration 70/1000 | Loss: 0.00001452
Iteration 71/1000 | Loss: 0.00001452
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001451
Iteration 78/1000 | Loss: 0.00001451
Iteration 79/1000 | Loss: 0.00001451
Iteration 80/1000 | Loss: 0.00001451
Iteration 81/1000 | Loss: 0.00001451
Iteration 82/1000 | Loss: 0.00001451
Iteration 83/1000 | Loss: 0.00001450
Iteration 84/1000 | Loss: 0.00001450
Iteration 85/1000 | Loss: 0.00001450
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001450
Iteration 88/1000 | Loss: 0.00001450
Iteration 89/1000 | Loss: 0.00001449
Iteration 90/1000 | Loss: 0.00001449
Iteration 91/1000 | Loss: 0.00001449
Iteration 92/1000 | Loss: 0.00001449
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00001449
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001448
Iteration 97/1000 | Loss: 0.00001448
Iteration 98/1000 | Loss: 0.00001448
Iteration 99/1000 | Loss: 0.00001448
Iteration 100/1000 | Loss: 0.00001448
Iteration 101/1000 | Loss: 0.00001448
Iteration 102/1000 | Loss: 0.00001448
Iteration 103/1000 | Loss: 0.00001448
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001448
Iteration 106/1000 | Loss: 0.00001448
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001447
Iteration 115/1000 | Loss: 0.00001447
Iteration 116/1000 | Loss: 0.00001447
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001446
Iteration 119/1000 | Loss: 0.00001446
Iteration 120/1000 | Loss: 0.00001446
Iteration 121/1000 | Loss: 0.00001446
Iteration 122/1000 | Loss: 0.00001446
Iteration 123/1000 | Loss: 0.00001446
Iteration 124/1000 | Loss: 0.00001446
Iteration 125/1000 | Loss: 0.00001446
Iteration 126/1000 | Loss: 0.00001446
Iteration 127/1000 | Loss: 0.00001446
Iteration 128/1000 | Loss: 0.00001446
Iteration 129/1000 | Loss: 0.00001446
Iteration 130/1000 | Loss: 0.00001445
Iteration 131/1000 | Loss: 0.00001445
Iteration 132/1000 | Loss: 0.00001445
Iteration 133/1000 | Loss: 0.00001445
Iteration 134/1000 | Loss: 0.00001445
Iteration 135/1000 | Loss: 0.00001445
Iteration 136/1000 | Loss: 0.00001445
Iteration 137/1000 | Loss: 0.00001445
Iteration 138/1000 | Loss: 0.00001445
Iteration 139/1000 | Loss: 0.00001445
Iteration 140/1000 | Loss: 0.00001445
Iteration 141/1000 | Loss: 0.00001444
Iteration 142/1000 | Loss: 0.00001444
Iteration 143/1000 | Loss: 0.00001444
Iteration 144/1000 | Loss: 0.00001444
Iteration 145/1000 | Loss: 0.00001444
Iteration 146/1000 | Loss: 0.00001444
Iteration 147/1000 | Loss: 0.00001444
Iteration 148/1000 | Loss: 0.00001444
Iteration 149/1000 | Loss: 0.00001444
Iteration 150/1000 | Loss: 0.00001444
Iteration 151/1000 | Loss: 0.00001444
Iteration 152/1000 | Loss: 0.00001444
Iteration 153/1000 | Loss: 0.00001444
Iteration 154/1000 | Loss: 0.00001444
Iteration 155/1000 | Loss: 0.00001443
Iteration 156/1000 | Loss: 0.00001443
Iteration 157/1000 | Loss: 0.00001443
Iteration 158/1000 | Loss: 0.00001443
Iteration 159/1000 | Loss: 0.00001443
Iteration 160/1000 | Loss: 0.00001443
Iteration 161/1000 | Loss: 0.00001443
Iteration 162/1000 | Loss: 0.00001443
Iteration 163/1000 | Loss: 0.00001443
Iteration 164/1000 | Loss: 0.00001443
Iteration 165/1000 | Loss: 0.00001442
Iteration 166/1000 | Loss: 0.00001442
Iteration 167/1000 | Loss: 0.00001442
Iteration 168/1000 | Loss: 0.00001442
Iteration 169/1000 | Loss: 0.00001442
Iteration 170/1000 | Loss: 0.00001442
Iteration 171/1000 | Loss: 0.00001442
Iteration 172/1000 | Loss: 0.00001442
Iteration 173/1000 | Loss: 0.00001442
Iteration 174/1000 | Loss: 0.00001442
Iteration 175/1000 | Loss: 0.00001442
Iteration 176/1000 | Loss: 0.00001442
Iteration 177/1000 | Loss: 0.00001442
Iteration 178/1000 | Loss: 0.00001442
Iteration 179/1000 | Loss: 0.00001442
Iteration 180/1000 | Loss: 0.00001442
Iteration 181/1000 | Loss: 0.00001442
Iteration 182/1000 | Loss: 0.00001442
Iteration 183/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.4423348147829529e-05, 1.4423348147829529e-05, 1.4423348147829529e-05, 1.4423348147829529e-05, 1.4423348147829529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4423348147829529e-05

Optimization complete. Final v2v error: 3.218294143676758 mm

Highest mean error: 4.542187690734863 mm for frame 152

Lowest mean error: 2.8882856369018555 mm for frame 30

Saving results

Total time: 38.975815534591675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066627
Iteration 2/25 | Loss: 0.01066627
Iteration 3/25 | Loss: 0.00191707
Iteration 4/25 | Loss: 0.00113074
Iteration 5/25 | Loss: 0.00099428
Iteration 6/25 | Loss: 0.00103486
Iteration 7/25 | Loss: 0.00098266
Iteration 8/25 | Loss: 0.00082460
Iteration 9/25 | Loss: 0.00072522
Iteration 10/25 | Loss: 0.00069108
Iteration 11/25 | Loss: 0.00065819
Iteration 12/25 | Loss: 0.00061380
Iteration 13/25 | Loss: 0.00061005
Iteration 14/25 | Loss: 0.00061158
Iteration 15/25 | Loss: 0.00060275
Iteration 16/25 | Loss: 0.00060110
Iteration 17/25 | Loss: 0.00060698
Iteration 18/25 | Loss: 0.00060241
Iteration 19/25 | Loss: 0.00060101
Iteration 20/25 | Loss: 0.00060101
Iteration 21/25 | Loss: 0.00060101
Iteration 22/25 | Loss: 0.00060101
Iteration 23/25 | Loss: 0.00060101
Iteration 24/25 | Loss: 0.00060101
Iteration 25/25 | Loss: 0.00060101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42310059
Iteration 2/25 | Loss: 0.00032425
Iteration 3/25 | Loss: 0.00032425
Iteration 4/25 | Loss: 0.00026168
Iteration 5/25 | Loss: 0.00026168
Iteration 6/25 | Loss: 0.00026168
Iteration 7/25 | Loss: 0.00026168
Iteration 8/25 | Loss: 0.00026168
Iteration 9/25 | Loss: 0.00026168
Iteration 10/25 | Loss: 0.00026168
Iteration 11/25 | Loss: 0.00026168
Iteration 12/25 | Loss: 0.00026168
Iteration 13/25 | Loss: 0.00028142
Iteration 14/25 | Loss: 0.00026168
Iteration 15/25 | Loss: 0.00026168
Iteration 16/25 | Loss: 0.00026168
Iteration 17/25 | Loss: 0.00026168
Iteration 18/25 | Loss: 0.00026168
Iteration 19/25 | Loss: 0.00026168
Iteration 20/25 | Loss: 0.00026168
Iteration 21/25 | Loss: 0.00026168
Iteration 22/25 | Loss: 0.00026168
Iteration 23/25 | Loss: 0.00026168
Iteration 24/25 | Loss: 0.00026168
Iteration 25/25 | Loss: 0.00026168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026168
Iteration 2/1000 | Loss: 0.00008902
Iteration 3/1000 | Loss: 0.00042137
Iteration 4/1000 | Loss: 0.00001881
Iteration 5/1000 | Loss: 0.00005541
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00003084
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00008089
Iteration 10/1000 | Loss: 0.00004477
Iteration 11/1000 | Loss: 0.00032252
Iteration 12/1000 | Loss: 0.00005952
Iteration 13/1000 | Loss: 0.00002024
Iteration 14/1000 | Loss: 0.00004936
Iteration 15/1000 | Loss: 0.00004713
Iteration 16/1000 | Loss: 0.00003617
Iteration 17/1000 | Loss: 0.00001560
Iteration 18/1000 | Loss: 0.00009487
Iteration 19/1000 | Loss: 0.00001875
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00001710
Iteration 22/1000 | Loss: 0.00007109
Iteration 23/1000 | Loss: 0.00003167
Iteration 24/1000 | Loss: 0.00001677
Iteration 25/1000 | Loss: 0.00006000
Iteration 26/1000 | Loss: 0.00002175
Iteration 27/1000 | Loss: 0.00002326
Iteration 28/1000 | Loss: 0.00001708
Iteration 29/1000 | Loss: 0.00004054
Iteration 30/1000 | Loss: 0.00039191
Iteration 31/1000 | Loss: 0.00003097
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00003451
Iteration 35/1000 | Loss: 0.00001534
Iteration 36/1000 | Loss: 0.00001528
Iteration 37/1000 | Loss: 0.00001527
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001527
Iteration 40/1000 | Loss: 0.00001527
Iteration 41/1000 | Loss: 0.00001527
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001526
Iteration 45/1000 | Loss: 0.00003515
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001523
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001522
Iteration 56/1000 | Loss: 0.00001522
Iteration 57/1000 | Loss: 0.00001521
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001520
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001520
Iteration 64/1000 | Loss: 0.00001520
Iteration 65/1000 | Loss: 0.00001520
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001520
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.5197288121271413e-05, 1.5197288121271413e-05, 1.5197288121271413e-05, 1.5197288121271413e-05, 1.5197288121271413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5197288121271413e-05

Optimization complete. Final v2v error: 3.2545602321624756 mm

Highest mean error: 3.3719770908355713 mm for frame 9

Lowest mean error: 3.000744581222534 mm for frame 119

Saving results

Total time: 136.19469213485718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886394
Iteration 2/25 | Loss: 0.00076492
Iteration 3/25 | Loss: 0.00064861
Iteration 4/25 | Loss: 0.00062554
Iteration 5/25 | Loss: 0.00062185
Iteration 6/25 | Loss: 0.00062101
Iteration 7/25 | Loss: 0.00062096
Iteration 8/25 | Loss: 0.00062096
Iteration 9/25 | Loss: 0.00062096
Iteration 10/25 | Loss: 0.00062096
Iteration 11/25 | Loss: 0.00062096
Iteration 12/25 | Loss: 0.00062096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00062095548491925, 0.00062095548491925, 0.00062095548491925, 0.00062095548491925, 0.00062095548491925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00062095548491925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52613139
Iteration 2/25 | Loss: 0.00022342
Iteration 3/25 | Loss: 0.00022342
Iteration 4/25 | Loss: 0.00022342
Iteration 5/25 | Loss: 0.00022342
Iteration 6/25 | Loss: 0.00022342
Iteration 7/25 | Loss: 0.00022342
Iteration 8/25 | Loss: 0.00022342
Iteration 9/25 | Loss: 0.00022342
Iteration 10/25 | Loss: 0.00022342
Iteration 11/25 | Loss: 0.00022342
Iteration 12/25 | Loss: 0.00022342
Iteration 13/25 | Loss: 0.00022342
Iteration 14/25 | Loss: 0.00022342
Iteration 15/25 | Loss: 0.00022342
Iteration 16/25 | Loss: 0.00022342
Iteration 17/25 | Loss: 0.00022342
Iteration 18/25 | Loss: 0.00022342
Iteration 19/25 | Loss: 0.00022342
Iteration 20/25 | Loss: 0.00022342
Iteration 21/25 | Loss: 0.00022342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00022341861040331423, 0.00022341861040331423, 0.00022341861040331423, 0.00022341861040331423, 0.00022341861040331423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022341861040331423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022342
Iteration 2/1000 | Loss: 0.00002645
Iteration 3/1000 | Loss: 0.00001910
Iteration 4/1000 | Loss: 0.00001795
Iteration 5/1000 | Loss: 0.00001681
Iteration 6/1000 | Loss: 0.00001633
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001555
Iteration 10/1000 | Loss: 0.00001548
Iteration 11/1000 | Loss: 0.00001547
Iteration 12/1000 | Loss: 0.00001547
Iteration 13/1000 | Loss: 0.00001544
Iteration 14/1000 | Loss: 0.00001542
Iteration 15/1000 | Loss: 0.00001542
Iteration 16/1000 | Loss: 0.00001540
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001534
Iteration 21/1000 | Loss: 0.00001533
Iteration 22/1000 | Loss: 0.00001532
Iteration 23/1000 | Loss: 0.00001532
Iteration 24/1000 | Loss: 0.00001531
Iteration 25/1000 | Loss: 0.00001530
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001528
Iteration 28/1000 | Loss: 0.00001527
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001527
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001525
Iteration 42/1000 | Loss: 0.00001525
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001522
Iteration 56/1000 | Loss: 0.00001522
Iteration 57/1000 | Loss: 0.00001521
Iteration 58/1000 | Loss: 0.00001521
Iteration 59/1000 | Loss: 0.00001521
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001521
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001520
Iteration 65/1000 | Loss: 0.00001520
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001520
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001519
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001519
Iteration 80/1000 | Loss: 0.00001519
Iteration 81/1000 | Loss: 0.00001519
Iteration 82/1000 | Loss: 0.00001519
Iteration 83/1000 | Loss: 0.00001519
Iteration 84/1000 | Loss: 0.00001519
Iteration 85/1000 | Loss: 0.00001519
Iteration 86/1000 | Loss: 0.00001519
Iteration 87/1000 | Loss: 0.00001519
Iteration 88/1000 | Loss: 0.00001519
Iteration 89/1000 | Loss: 0.00001519
Iteration 90/1000 | Loss: 0.00001519
Iteration 91/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.5193772014754359e-05, 1.5193772014754359e-05, 1.5193772014754359e-05, 1.5193772014754359e-05, 1.5193772014754359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5193772014754359e-05

Optimization complete. Final v2v error: 3.2907705307006836 mm

Highest mean error: 3.677945375442505 mm for frame 83

Lowest mean error: 3.158390998840332 mm for frame 74

Saving results

Total time: 29.63560700416565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828877
Iteration 2/25 | Loss: 0.00082523
Iteration 3/25 | Loss: 0.00068633
Iteration 4/25 | Loss: 0.00065356
Iteration 5/25 | Loss: 0.00064928
Iteration 6/25 | Loss: 0.00064893
Iteration 7/25 | Loss: 0.00064893
Iteration 8/25 | Loss: 0.00064893
Iteration 9/25 | Loss: 0.00064893
Iteration 10/25 | Loss: 0.00064893
Iteration 11/25 | Loss: 0.00064893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006489299121312797, 0.0006489299121312797, 0.0006489299121312797, 0.0006489299121312797, 0.0006489299121312797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006489299121312797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49441361
Iteration 2/25 | Loss: 0.00023125
Iteration 3/25 | Loss: 0.00023124
Iteration 4/25 | Loss: 0.00023124
Iteration 5/25 | Loss: 0.00023124
Iteration 6/25 | Loss: 0.00023124
Iteration 7/25 | Loss: 0.00023124
Iteration 8/25 | Loss: 0.00023124
Iteration 9/25 | Loss: 0.00023124
Iteration 10/25 | Loss: 0.00023124
Iteration 11/25 | Loss: 0.00023124
Iteration 12/25 | Loss: 0.00023124
Iteration 13/25 | Loss: 0.00023124
Iteration 14/25 | Loss: 0.00023124
Iteration 15/25 | Loss: 0.00023124
Iteration 16/25 | Loss: 0.00023124
Iteration 17/25 | Loss: 0.00023124
Iteration 18/25 | Loss: 0.00023124
Iteration 19/25 | Loss: 0.00023124
Iteration 20/25 | Loss: 0.00023124
Iteration 21/25 | Loss: 0.00023124
Iteration 22/25 | Loss: 0.00023124
Iteration 23/25 | Loss: 0.00023124
Iteration 24/25 | Loss: 0.00023124
Iteration 25/25 | Loss: 0.00023124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023124
Iteration 2/1000 | Loss: 0.00003731
Iteration 3/1000 | Loss: 0.00002524
Iteration 4/1000 | Loss: 0.00002320
Iteration 5/1000 | Loss: 0.00002210
Iteration 6/1000 | Loss: 0.00002081
Iteration 7/1000 | Loss: 0.00002011
Iteration 8/1000 | Loss: 0.00001952
Iteration 9/1000 | Loss: 0.00001917
Iteration 10/1000 | Loss: 0.00001892
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001874
Iteration 14/1000 | Loss: 0.00001864
Iteration 15/1000 | Loss: 0.00001863
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001857
Iteration 20/1000 | Loss: 0.00001857
Iteration 21/1000 | Loss: 0.00001857
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001857
Iteration 24/1000 | Loss: 0.00001857
Iteration 25/1000 | Loss: 0.00001856
Iteration 26/1000 | Loss: 0.00001856
Iteration 27/1000 | Loss: 0.00001856
Iteration 28/1000 | Loss: 0.00001856
Iteration 29/1000 | Loss: 0.00001856
Iteration 30/1000 | Loss: 0.00001856
Iteration 31/1000 | Loss: 0.00001854
Iteration 32/1000 | Loss: 0.00001853
Iteration 33/1000 | Loss: 0.00001848
Iteration 34/1000 | Loss: 0.00001847
Iteration 35/1000 | Loss: 0.00001847
Iteration 36/1000 | Loss: 0.00001846
Iteration 37/1000 | Loss: 0.00001846
Iteration 38/1000 | Loss: 0.00001846
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001845
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001845
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001844
Iteration 47/1000 | Loss: 0.00001844
Iteration 48/1000 | Loss: 0.00001844
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001843
Iteration 51/1000 | Loss: 0.00001843
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001843
Iteration 57/1000 | Loss: 0.00001843
Iteration 58/1000 | Loss: 0.00001843
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001843
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001842
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001841
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001841
Iteration 95/1000 | Loss: 0.00001841
Iteration 96/1000 | Loss: 0.00001841
Iteration 97/1000 | Loss: 0.00001841
Iteration 98/1000 | Loss: 0.00001841
Iteration 99/1000 | Loss: 0.00001841
Iteration 100/1000 | Loss: 0.00001841
Iteration 101/1000 | Loss: 0.00001841
Iteration 102/1000 | Loss: 0.00001841
Iteration 103/1000 | Loss: 0.00001841
Iteration 104/1000 | Loss: 0.00001841
Iteration 105/1000 | Loss: 0.00001841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.8409773474559188e-05, 1.8409773474559188e-05, 1.8409773474559188e-05, 1.8409773474559188e-05, 1.8409773474559188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8409773474559188e-05

Optimization complete. Final v2v error: 3.6039235591888428 mm

Highest mean error: 3.954284906387329 mm for frame 74

Lowest mean error: 3.448941946029663 mm for frame 57

Saving results

Total time: 36.2518584728241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002487
Iteration 2/25 | Loss: 0.00374488
Iteration 3/25 | Loss: 0.00213752
Iteration 4/25 | Loss: 0.00191176
Iteration 5/25 | Loss: 0.00180945
Iteration 6/25 | Loss: 0.00171247
Iteration 7/25 | Loss: 0.00160617
Iteration 8/25 | Loss: 0.00164047
Iteration 9/25 | Loss: 0.00143137
Iteration 10/25 | Loss: 0.00139844
Iteration 11/25 | Loss: 0.00137604
Iteration 12/25 | Loss: 0.00136746
Iteration 13/25 | Loss: 0.00139213
Iteration 14/25 | Loss: 0.00147076
Iteration 15/25 | Loss: 0.00132558
Iteration 16/25 | Loss: 0.00124331
Iteration 17/25 | Loss: 0.00122325
Iteration 18/25 | Loss: 0.00121577
Iteration 19/25 | Loss: 0.00121575
Iteration 20/25 | Loss: 0.00121072
Iteration 21/25 | Loss: 0.00120941
Iteration 22/25 | Loss: 0.00120901
Iteration 23/25 | Loss: 0.00120826
Iteration 24/25 | Loss: 0.00120765
Iteration 25/25 | Loss: 0.00121057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43115425
Iteration 2/25 | Loss: 0.00266208
Iteration 3/25 | Loss: 0.00266208
Iteration 4/25 | Loss: 0.00266208
Iteration 5/25 | Loss: 0.00266208
Iteration 6/25 | Loss: 0.00266208
Iteration 7/25 | Loss: 0.00266208
Iteration 8/25 | Loss: 0.00266207
Iteration 9/25 | Loss: 0.00266207
Iteration 10/25 | Loss: 0.00266207
Iteration 11/25 | Loss: 0.00266207
Iteration 12/25 | Loss: 0.00266208
Iteration 13/25 | Loss: 0.00266208
Iteration 14/25 | Loss: 0.00266208
Iteration 15/25 | Loss: 0.00266207
Iteration 16/25 | Loss: 0.00266208
Iteration 17/25 | Loss: 0.00266208
Iteration 18/25 | Loss: 0.00266208
Iteration 19/25 | Loss: 0.00266208
Iteration 20/25 | Loss: 0.00266208
Iteration 21/25 | Loss: 0.00266208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0026620752178132534, 0.0026620752178132534, 0.0026620752178132534, 0.0026620752178132534, 0.0026620752178132534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026620752178132534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266208
Iteration 2/1000 | Loss: 0.00055744
Iteration 3/1000 | Loss: 0.00106594
Iteration 4/1000 | Loss: 0.00049149
Iteration 5/1000 | Loss: 0.00243451
Iteration 6/1000 | Loss: 0.00469070
Iteration 7/1000 | Loss: 0.00176125
Iteration 8/1000 | Loss: 0.00292535
Iteration 9/1000 | Loss: 0.00081909
Iteration 10/1000 | Loss: 0.00116502
Iteration 11/1000 | Loss: 0.00033760
Iteration 12/1000 | Loss: 0.00109443
Iteration 13/1000 | Loss: 0.00085319
Iteration 14/1000 | Loss: 0.00207797
Iteration 15/1000 | Loss: 0.00025772
Iteration 16/1000 | Loss: 0.00040903
Iteration 17/1000 | Loss: 0.00033040
Iteration 18/1000 | Loss: 0.00022329
Iteration 19/1000 | Loss: 0.00025313
Iteration 20/1000 | Loss: 0.00100214
Iteration 21/1000 | Loss: 0.00010403
Iteration 22/1000 | Loss: 0.00024361
Iteration 23/1000 | Loss: 0.00011610
Iteration 24/1000 | Loss: 0.00015174
Iteration 25/1000 | Loss: 0.00005682
Iteration 26/1000 | Loss: 0.00005021
Iteration 27/1000 | Loss: 0.00006098
Iteration 28/1000 | Loss: 0.00003582
Iteration 29/1000 | Loss: 0.00006348
Iteration 30/1000 | Loss: 0.00003017
Iteration 31/1000 | Loss: 0.00003862
Iteration 32/1000 | Loss: 0.00005913
Iteration 33/1000 | Loss: 0.00003516
Iteration 34/1000 | Loss: 0.00002600
Iteration 35/1000 | Loss: 0.00003671
Iteration 36/1000 | Loss: 0.00002461
Iteration 37/1000 | Loss: 0.00002706
Iteration 38/1000 | Loss: 0.00003261
Iteration 39/1000 | Loss: 0.00002390
Iteration 40/1000 | Loss: 0.00002526
Iteration 41/1000 | Loss: 0.00002379
Iteration 42/1000 | Loss: 0.00002363
Iteration 43/1000 | Loss: 0.00002354
Iteration 44/1000 | Loss: 0.00002352
Iteration 45/1000 | Loss: 0.00002350
Iteration 46/1000 | Loss: 0.00002350
Iteration 47/1000 | Loss: 0.00002349
Iteration 48/1000 | Loss: 0.00002349
Iteration 49/1000 | Loss: 0.00002347
Iteration 50/1000 | Loss: 0.00002347
Iteration 51/1000 | Loss: 0.00002347
Iteration 52/1000 | Loss: 0.00002347
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002346
Iteration 56/1000 | Loss: 0.00002346
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002345
Iteration 59/1000 | Loss: 0.00002345
Iteration 60/1000 | Loss: 0.00002345
Iteration 61/1000 | Loss: 0.00002345
Iteration 62/1000 | Loss: 0.00002345
Iteration 63/1000 | Loss: 0.00002345
Iteration 64/1000 | Loss: 0.00002345
Iteration 65/1000 | Loss: 0.00002344
Iteration 66/1000 | Loss: 0.00002344
Iteration 67/1000 | Loss: 0.00002344
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002344
Iteration 70/1000 | Loss: 0.00002344
Iteration 71/1000 | Loss: 0.00002344
Iteration 72/1000 | Loss: 0.00002344
Iteration 73/1000 | Loss: 0.00002344
Iteration 74/1000 | Loss: 0.00002344
Iteration 75/1000 | Loss: 0.00002343
Iteration 76/1000 | Loss: 0.00002343
Iteration 77/1000 | Loss: 0.00002343
Iteration 78/1000 | Loss: 0.00002342
Iteration 79/1000 | Loss: 0.00002342
Iteration 80/1000 | Loss: 0.00002342
Iteration 81/1000 | Loss: 0.00002342
Iteration 82/1000 | Loss: 0.00002342
Iteration 83/1000 | Loss: 0.00002342
Iteration 84/1000 | Loss: 0.00002342
Iteration 85/1000 | Loss: 0.00002342
Iteration 86/1000 | Loss: 0.00002342
Iteration 87/1000 | Loss: 0.00002341
Iteration 88/1000 | Loss: 0.00002341
Iteration 89/1000 | Loss: 0.00002341
Iteration 90/1000 | Loss: 0.00002341
Iteration 91/1000 | Loss: 0.00002341
Iteration 92/1000 | Loss: 0.00002341
Iteration 93/1000 | Loss: 0.00002341
Iteration 94/1000 | Loss: 0.00002341
Iteration 95/1000 | Loss: 0.00002341
Iteration 96/1000 | Loss: 0.00002341
Iteration 97/1000 | Loss: 0.00002340
Iteration 98/1000 | Loss: 0.00002340
Iteration 99/1000 | Loss: 0.00002340
Iteration 100/1000 | Loss: 0.00002340
Iteration 101/1000 | Loss: 0.00002340
Iteration 102/1000 | Loss: 0.00002340
Iteration 103/1000 | Loss: 0.00002340
Iteration 104/1000 | Loss: 0.00002340
Iteration 105/1000 | Loss: 0.00002340
Iteration 106/1000 | Loss: 0.00002340
Iteration 107/1000 | Loss: 0.00002340
Iteration 108/1000 | Loss: 0.00002340
Iteration 109/1000 | Loss: 0.00002340
Iteration 110/1000 | Loss: 0.00002340
Iteration 111/1000 | Loss: 0.00002340
Iteration 112/1000 | Loss: 0.00002340
Iteration 113/1000 | Loss: 0.00002340
Iteration 114/1000 | Loss: 0.00002339
Iteration 115/1000 | Loss: 0.00002339
Iteration 116/1000 | Loss: 0.00002339
Iteration 117/1000 | Loss: 0.00002339
Iteration 118/1000 | Loss: 0.00002339
Iteration 119/1000 | Loss: 0.00002339
Iteration 120/1000 | Loss: 0.00002339
Iteration 121/1000 | Loss: 0.00002339
Iteration 122/1000 | Loss: 0.00002339
Iteration 123/1000 | Loss: 0.00002339
Iteration 124/1000 | Loss: 0.00002339
Iteration 125/1000 | Loss: 0.00002339
Iteration 126/1000 | Loss: 0.00002339
Iteration 127/1000 | Loss: 0.00002339
Iteration 128/1000 | Loss: 0.00002339
Iteration 129/1000 | Loss: 0.00002339
Iteration 130/1000 | Loss: 0.00002339
Iteration 131/1000 | Loss: 0.00002339
Iteration 132/1000 | Loss: 0.00002339
Iteration 133/1000 | Loss: 0.00002339
Iteration 134/1000 | Loss: 0.00002339
Iteration 135/1000 | Loss: 0.00002339
Iteration 136/1000 | Loss: 0.00002339
Iteration 137/1000 | Loss: 0.00002339
Iteration 138/1000 | Loss: 0.00002339
Iteration 139/1000 | Loss: 0.00002339
Iteration 140/1000 | Loss: 0.00002339
Iteration 141/1000 | Loss: 0.00002339
Iteration 142/1000 | Loss: 0.00002339
Iteration 143/1000 | Loss: 0.00002339
Iteration 144/1000 | Loss: 0.00002339
Iteration 145/1000 | Loss: 0.00002339
Iteration 146/1000 | Loss: 0.00002339
Iteration 147/1000 | Loss: 0.00002339
Iteration 148/1000 | Loss: 0.00002339
Iteration 149/1000 | Loss: 0.00002339
Iteration 150/1000 | Loss: 0.00002339
Iteration 151/1000 | Loss: 0.00002339
Iteration 152/1000 | Loss: 0.00002339
Iteration 153/1000 | Loss: 0.00002339
Iteration 154/1000 | Loss: 0.00002339
Iteration 155/1000 | Loss: 0.00002339
Iteration 156/1000 | Loss: 0.00002339
Iteration 157/1000 | Loss: 0.00002339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.3392922230414115e-05, 2.3392922230414115e-05, 2.3392922230414115e-05, 2.3392922230414115e-05, 2.3392922230414115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3392922230414115e-05

Optimization complete. Final v2v error: 4.103172302246094 mm

Highest mean error: 4.527702808380127 mm for frame 118

Lowest mean error: 3.5990607738494873 mm for frame 165

Saving results

Total time: 149.1588408946991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098534
Iteration 2/25 | Loss: 0.01098534
Iteration 3/25 | Loss: 0.01098533
Iteration 4/25 | Loss: 0.00185757
Iteration 5/25 | Loss: 0.00116791
Iteration 6/25 | Loss: 0.00102329
Iteration 7/25 | Loss: 0.00092974
Iteration 8/25 | Loss: 0.00080361
Iteration 9/25 | Loss: 0.00075364
Iteration 10/25 | Loss: 0.00071274
Iteration 11/25 | Loss: 0.00069297
Iteration 12/25 | Loss: 0.00068197
Iteration 13/25 | Loss: 0.00067878
Iteration 14/25 | Loss: 0.00067746
Iteration 15/25 | Loss: 0.00068661
Iteration 16/25 | Loss: 0.00068424
Iteration 17/25 | Loss: 0.00068564
Iteration 18/25 | Loss: 0.00068271
Iteration 19/25 | Loss: 0.00068364
Iteration 20/25 | Loss: 0.00068288
Iteration 21/25 | Loss: 0.00068806
Iteration 22/25 | Loss: 0.00068218
Iteration 23/25 | Loss: 0.00068228
Iteration 24/25 | Loss: 0.00068215
Iteration 25/25 | Loss: 0.00068514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53733850
Iteration 2/25 | Loss: 0.00037283
Iteration 3/25 | Loss: 0.00037283
Iteration 4/25 | Loss: 0.00037283
Iteration 5/25 | Loss: 0.00037283
Iteration 6/25 | Loss: 0.00037283
Iteration 7/25 | Loss: 0.00037283
Iteration 8/25 | Loss: 0.00037283
Iteration 9/25 | Loss: 0.00037283
Iteration 10/25 | Loss: 0.00037283
Iteration 11/25 | Loss: 0.00037283
Iteration 12/25 | Loss: 0.00037283
Iteration 13/25 | Loss: 0.00037283
Iteration 14/25 | Loss: 0.00037283
Iteration 15/25 | Loss: 0.00037283
Iteration 16/25 | Loss: 0.00037283
Iteration 17/25 | Loss: 0.00037283
Iteration 18/25 | Loss: 0.00037283
Iteration 19/25 | Loss: 0.00037283
Iteration 20/25 | Loss: 0.00037283
Iteration 21/25 | Loss: 0.00037283
Iteration 22/25 | Loss: 0.00037283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00037282766425050795, 0.00037282766425050795, 0.00037282766425050795, 0.00037282766425050795, 0.00037282766425050795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037282766425050795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037283
Iteration 2/1000 | Loss: 0.00005913
Iteration 3/1000 | Loss: 0.00015172
Iteration 4/1000 | Loss: 0.00004264
Iteration 5/1000 | Loss: 0.00006269
Iteration 6/1000 | Loss: 0.00004981
Iteration 7/1000 | Loss: 0.00005136
Iteration 8/1000 | Loss: 0.00004106
Iteration 9/1000 | Loss: 0.00007116
Iteration 10/1000 | Loss: 0.00004368
Iteration 11/1000 | Loss: 0.00007219
Iteration 12/1000 | Loss: 0.00003982
Iteration 13/1000 | Loss: 0.00005792
Iteration 14/1000 | Loss: 0.00005012
Iteration 15/1000 | Loss: 0.00007027
Iteration 16/1000 | Loss: 0.00003202
Iteration 17/1000 | Loss: 0.00002192
Iteration 18/1000 | Loss: 0.00002031
Iteration 19/1000 | Loss: 0.00001931
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001869
Iteration 22/1000 | Loss: 0.00061976
Iteration 23/1000 | Loss: 0.00003654
Iteration 24/1000 | Loss: 0.00002374
Iteration 25/1000 | Loss: 0.00001962
Iteration 26/1000 | Loss: 0.00001770
Iteration 27/1000 | Loss: 0.00001638
Iteration 28/1000 | Loss: 0.00001577
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001531
Iteration 31/1000 | Loss: 0.00001530
Iteration 32/1000 | Loss: 0.00001522
Iteration 33/1000 | Loss: 0.00001521
Iteration 34/1000 | Loss: 0.00001518
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001487
Iteration 52/1000 | Loss: 0.00001487
Iteration 53/1000 | Loss: 0.00001485
Iteration 54/1000 | Loss: 0.00001485
Iteration 55/1000 | Loss: 0.00001485
Iteration 56/1000 | Loss: 0.00001485
Iteration 57/1000 | Loss: 0.00001485
Iteration 58/1000 | Loss: 0.00001485
Iteration 59/1000 | Loss: 0.00001485
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001484
Iteration 62/1000 | Loss: 0.00001484
Iteration 63/1000 | Loss: 0.00001484
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001482
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001481
Iteration 74/1000 | Loss: 0.00001481
Iteration 75/1000 | Loss: 0.00001481
Iteration 76/1000 | Loss: 0.00001481
Iteration 77/1000 | Loss: 0.00001481
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001480
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001477
Iteration 100/1000 | Loss: 0.00001477
Iteration 101/1000 | Loss: 0.00001477
Iteration 102/1000 | Loss: 0.00001477
Iteration 103/1000 | Loss: 0.00001477
Iteration 104/1000 | Loss: 0.00001477
Iteration 105/1000 | Loss: 0.00001476
Iteration 106/1000 | Loss: 0.00001476
Iteration 107/1000 | Loss: 0.00001476
Iteration 108/1000 | Loss: 0.00001476
Iteration 109/1000 | Loss: 0.00001476
Iteration 110/1000 | Loss: 0.00001476
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001476
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001476
Iteration 117/1000 | Loss: 0.00001476
Iteration 118/1000 | Loss: 0.00001475
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001474
Iteration 128/1000 | Loss: 0.00001474
Iteration 129/1000 | Loss: 0.00001474
Iteration 130/1000 | Loss: 0.00001474
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001473
Iteration 146/1000 | Loss: 0.00001473
Iteration 147/1000 | Loss: 0.00001473
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001473
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Iteration 156/1000 | Loss: 0.00001473
Iteration 157/1000 | Loss: 0.00001473
Iteration 158/1000 | Loss: 0.00001473
Iteration 159/1000 | Loss: 0.00001473
Iteration 160/1000 | Loss: 0.00001473
Iteration 161/1000 | Loss: 0.00001473
Iteration 162/1000 | Loss: 0.00001473
Iteration 163/1000 | Loss: 0.00001473
Iteration 164/1000 | Loss: 0.00001473
Iteration 165/1000 | Loss: 0.00001473
Iteration 166/1000 | Loss: 0.00001473
Iteration 167/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.4727260349900462e-05, 1.4727260349900462e-05, 1.4727260349900462e-05, 1.4727260349900462e-05, 1.4727260349900462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4727260349900462e-05

Optimization complete. Final v2v error: 3.286888599395752 mm

Highest mean error: 3.7178666591644287 mm for frame 107

Lowest mean error: 2.978801965713501 mm for frame 11

Saving results

Total time: 104.90091109275818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810460
Iteration 2/25 | Loss: 0.00145547
Iteration 3/25 | Loss: 0.00095097
Iteration 4/25 | Loss: 0.00083567
Iteration 5/25 | Loss: 0.00081386
Iteration 6/25 | Loss: 0.00080642
Iteration 7/25 | Loss: 0.00080419
Iteration 8/25 | Loss: 0.00080862
Iteration 9/25 | Loss: 0.00079578
Iteration 10/25 | Loss: 0.00079165
Iteration 11/25 | Loss: 0.00078865
Iteration 12/25 | Loss: 0.00078500
Iteration 13/25 | Loss: 0.00078409
Iteration 14/25 | Loss: 0.00078336
Iteration 15/25 | Loss: 0.00078238
Iteration 16/25 | Loss: 0.00078120
Iteration 17/25 | Loss: 0.00077751
Iteration 18/25 | Loss: 0.00077639
Iteration 19/25 | Loss: 0.00077585
Iteration 20/25 | Loss: 0.00077553
Iteration 21/25 | Loss: 0.00077542
Iteration 22/25 | Loss: 0.00077538
Iteration 23/25 | Loss: 0.00077536
Iteration 24/25 | Loss: 0.00077536
Iteration 25/25 | Loss: 0.00077536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89975691
Iteration 2/25 | Loss: 0.00045536
Iteration 3/25 | Loss: 0.00045532
Iteration 4/25 | Loss: 0.00045532
Iteration 5/25 | Loss: 0.00045532
Iteration 6/25 | Loss: 0.00045532
Iteration 7/25 | Loss: 0.00045532
Iteration 8/25 | Loss: 0.00045532
Iteration 9/25 | Loss: 0.00045532
Iteration 10/25 | Loss: 0.00045532
Iteration 11/25 | Loss: 0.00045532
Iteration 12/25 | Loss: 0.00045532
Iteration 13/25 | Loss: 0.00045532
Iteration 14/25 | Loss: 0.00045532
Iteration 15/25 | Loss: 0.00045532
Iteration 16/25 | Loss: 0.00045532
Iteration 17/25 | Loss: 0.00045532
Iteration 18/25 | Loss: 0.00045532
Iteration 19/25 | Loss: 0.00045532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004553204926196486, 0.0004553204926196486, 0.0004553204926196486, 0.0004553204926196486, 0.0004553204926196486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004553204926196486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045532
Iteration 2/1000 | Loss: 0.00004970
Iteration 3/1000 | Loss: 0.00003963
Iteration 4/1000 | Loss: 0.00003625
Iteration 5/1000 | Loss: 0.00003414
Iteration 6/1000 | Loss: 0.00003289
Iteration 7/1000 | Loss: 0.00033450
Iteration 8/1000 | Loss: 0.00046518
Iteration 9/1000 | Loss: 0.00003893
Iteration 10/1000 | Loss: 0.00003179
Iteration 11/1000 | Loss: 0.00002889
Iteration 12/1000 | Loss: 0.00002771
Iteration 13/1000 | Loss: 0.00002672
Iteration 14/1000 | Loss: 0.00002619
Iteration 15/1000 | Loss: 0.00002597
Iteration 16/1000 | Loss: 0.00002592
Iteration 17/1000 | Loss: 0.00002570
Iteration 18/1000 | Loss: 0.00002550
Iteration 19/1000 | Loss: 0.00002538
Iteration 20/1000 | Loss: 0.00002530
Iteration 21/1000 | Loss: 0.00002522
Iteration 22/1000 | Loss: 0.00002522
Iteration 23/1000 | Loss: 0.00002522
Iteration 24/1000 | Loss: 0.00002520
Iteration 25/1000 | Loss: 0.00002517
Iteration 26/1000 | Loss: 0.00002516
Iteration 27/1000 | Loss: 0.00002515
Iteration 28/1000 | Loss: 0.00002508
Iteration 29/1000 | Loss: 0.00002508
Iteration 30/1000 | Loss: 0.00002505
Iteration 31/1000 | Loss: 0.00002505
Iteration 32/1000 | Loss: 0.00002505
Iteration 33/1000 | Loss: 0.00002505
Iteration 34/1000 | Loss: 0.00002505
Iteration 35/1000 | Loss: 0.00002505
Iteration 36/1000 | Loss: 0.00002505
Iteration 37/1000 | Loss: 0.00002505
Iteration 38/1000 | Loss: 0.00002505
Iteration 39/1000 | Loss: 0.00002505
Iteration 40/1000 | Loss: 0.00002505
Iteration 41/1000 | Loss: 0.00002504
Iteration 42/1000 | Loss: 0.00002504
Iteration 43/1000 | Loss: 0.00002503
Iteration 44/1000 | Loss: 0.00002503
Iteration 45/1000 | Loss: 0.00002503
Iteration 46/1000 | Loss: 0.00002503
Iteration 47/1000 | Loss: 0.00002503
Iteration 48/1000 | Loss: 0.00002503
Iteration 49/1000 | Loss: 0.00002503
Iteration 50/1000 | Loss: 0.00002503
Iteration 51/1000 | Loss: 0.00002503
Iteration 52/1000 | Loss: 0.00002502
Iteration 53/1000 | Loss: 0.00002502
Iteration 54/1000 | Loss: 0.00002502
Iteration 55/1000 | Loss: 0.00002502
Iteration 56/1000 | Loss: 0.00002502
Iteration 57/1000 | Loss: 0.00002502
Iteration 58/1000 | Loss: 0.00002501
Iteration 59/1000 | Loss: 0.00002501
Iteration 60/1000 | Loss: 0.00002501
Iteration 61/1000 | Loss: 0.00002501
Iteration 62/1000 | Loss: 0.00002501
Iteration 63/1000 | Loss: 0.00002501
Iteration 64/1000 | Loss: 0.00002501
Iteration 65/1000 | Loss: 0.00002501
Iteration 66/1000 | Loss: 0.00002501
Iteration 67/1000 | Loss: 0.00002501
Iteration 68/1000 | Loss: 0.00002500
Iteration 69/1000 | Loss: 0.00002500
Iteration 70/1000 | Loss: 0.00002500
Iteration 71/1000 | Loss: 0.00002500
Iteration 72/1000 | Loss: 0.00002500
Iteration 73/1000 | Loss: 0.00002499
Iteration 74/1000 | Loss: 0.00002499
Iteration 75/1000 | Loss: 0.00002499
Iteration 76/1000 | Loss: 0.00002499
Iteration 77/1000 | Loss: 0.00002499
Iteration 78/1000 | Loss: 0.00002498
Iteration 79/1000 | Loss: 0.00002498
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00002498
Iteration 82/1000 | Loss: 0.00002498
Iteration 83/1000 | Loss: 0.00002498
Iteration 84/1000 | Loss: 0.00002497
Iteration 85/1000 | Loss: 0.00002497
Iteration 86/1000 | Loss: 0.00002497
Iteration 87/1000 | Loss: 0.00002497
Iteration 88/1000 | Loss: 0.00002497
Iteration 89/1000 | Loss: 0.00002497
Iteration 90/1000 | Loss: 0.00002497
Iteration 91/1000 | Loss: 0.00002497
Iteration 92/1000 | Loss: 0.00002497
Iteration 93/1000 | Loss: 0.00002497
Iteration 94/1000 | Loss: 0.00002497
Iteration 95/1000 | Loss: 0.00002497
Iteration 96/1000 | Loss: 0.00002496
Iteration 97/1000 | Loss: 0.00002496
Iteration 98/1000 | Loss: 0.00002496
Iteration 99/1000 | Loss: 0.00002496
Iteration 100/1000 | Loss: 0.00002496
Iteration 101/1000 | Loss: 0.00002495
Iteration 102/1000 | Loss: 0.00002495
Iteration 103/1000 | Loss: 0.00002495
Iteration 104/1000 | Loss: 0.00002495
Iteration 105/1000 | Loss: 0.00002495
Iteration 106/1000 | Loss: 0.00002495
Iteration 107/1000 | Loss: 0.00002495
Iteration 108/1000 | Loss: 0.00002495
Iteration 109/1000 | Loss: 0.00002495
Iteration 110/1000 | Loss: 0.00002495
Iteration 111/1000 | Loss: 0.00002495
Iteration 112/1000 | Loss: 0.00002495
Iteration 113/1000 | Loss: 0.00002495
Iteration 114/1000 | Loss: 0.00002495
Iteration 115/1000 | Loss: 0.00002495
Iteration 116/1000 | Loss: 0.00002495
Iteration 117/1000 | Loss: 0.00002495
Iteration 118/1000 | Loss: 0.00002495
Iteration 119/1000 | Loss: 0.00002495
Iteration 120/1000 | Loss: 0.00002495
Iteration 121/1000 | Loss: 0.00002495
Iteration 122/1000 | Loss: 0.00002495
Iteration 123/1000 | Loss: 0.00002495
Iteration 124/1000 | Loss: 0.00002495
Iteration 125/1000 | Loss: 0.00002495
Iteration 126/1000 | Loss: 0.00002495
Iteration 127/1000 | Loss: 0.00002495
Iteration 128/1000 | Loss: 0.00002495
Iteration 129/1000 | Loss: 0.00002495
Iteration 130/1000 | Loss: 0.00002495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.494724321877584e-05, 2.494724321877584e-05, 2.494724321877584e-05, 2.494724321877584e-05, 2.494724321877584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.494724321877584e-05

Optimization complete. Final v2v error: 4.094296455383301 mm

Highest mean error: 4.683774948120117 mm for frame 68

Lowest mean error: 3.568204879760742 mm for frame 139

Saving results

Total time: 110.20172381401062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816275
Iteration 2/25 | Loss: 0.00121464
Iteration 3/25 | Loss: 0.00077451
Iteration 4/25 | Loss: 0.00071539
Iteration 5/25 | Loss: 0.00069953
Iteration 6/25 | Loss: 0.00069540
Iteration 7/25 | Loss: 0.00069428
Iteration 8/25 | Loss: 0.00069428
Iteration 9/25 | Loss: 0.00069428
Iteration 10/25 | Loss: 0.00069428
Iteration 11/25 | Loss: 0.00069428
Iteration 12/25 | Loss: 0.00069428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006942760082893074, 0.0006942760082893074, 0.0006942760082893074, 0.0006942760082893074, 0.0006942760082893074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006942760082893074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49604774
Iteration 2/25 | Loss: 0.00026681
Iteration 3/25 | Loss: 0.00026680
Iteration 4/25 | Loss: 0.00026680
Iteration 5/25 | Loss: 0.00026680
Iteration 6/25 | Loss: 0.00026680
Iteration 7/25 | Loss: 0.00026680
Iteration 8/25 | Loss: 0.00026680
Iteration 9/25 | Loss: 0.00026680
Iteration 10/25 | Loss: 0.00026680
Iteration 11/25 | Loss: 0.00026680
Iteration 12/25 | Loss: 0.00026680
Iteration 13/25 | Loss: 0.00026680
Iteration 14/25 | Loss: 0.00026680
Iteration 15/25 | Loss: 0.00026680
Iteration 16/25 | Loss: 0.00026680
Iteration 17/25 | Loss: 0.00026680
Iteration 18/25 | Loss: 0.00026680
Iteration 19/25 | Loss: 0.00026680
Iteration 20/25 | Loss: 0.00026680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002668017987161875, 0.0002668017987161875, 0.0002668017987161875, 0.0002668017987161875, 0.0002668017987161875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002668017987161875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026680
Iteration 2/1000 | Loss: 0.00003168
Iteration 3/1000 | Loss: 0.00002422
Iteration 4/1000 | Loss: 0.00002246
Iteration 5/1000 | Loss: 0.00002119
Iteration 6/1000 | Loss: 0.00002039
Iteration 7/1000 | Loss: 0.00001989
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001894
Iteration 10/1000 | Loss: 0.00001868
Iteration 11/1000 | Loss: 0.00001866
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001865
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001859
Iteration 16/1000 | Loss: 0.00001853
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001852
Iteration 19/1000 | Loss: 0.00001848
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001848
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001847
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001847
Iteration 29/1000 | Loss: 0.00001844
Iteration 30/1000 | Loss: 0.00001844
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001843
Iteration 33/1000 | Loss: 0.00001836
Iteration 34/1000 | Loss: 0.00001836
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001834
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001833
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001833
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001833
Iteration 48/1000 | Loss: 0.00001833
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001832
Iteration 51/1000 | Loss: 0.00001832
Iteration 52/1000 | Loss: 0.00001831
Iteration 53/1000 | Loss: 0.00001831
Iteration 54/1000 | Loss: 0.00001831
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001829
Iteration 61/1000 | Loss: 0.00001829
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001828
Iteration 64/1000 | Loss: 0.00001828
Iteration 65/1000 | Loss: 0.00001828
Iteration 66/1000 | Loss: 0.00001828
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001828
Iteration 69/1000 | Loss: 0.00001828
Iteration 70/1000 | Loss: 0.00001827
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001827
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001826
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001826
Iteration 80/1000 | Loss: 0.00001825
Iteration 81/1000 | Loss: 0.00001825
Iteration 82/1000 | Loss: 0.00001825
Iteration 83/1000 | Loss: 0.00001825
Iteration 84/1000 | Loss: 0.00001825
Iteration 85/1000 | Loss: 0.00001825
Iteration 86/1000 | Loss: 0.00001825
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001825
Iteration 89/1000 | Loss: 0.00001825
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001824
Iteration 92/1000 | Loss: 0.00001824
Iteration 93/1000 | Loss: 0.00001824
Iteration 94/1000 | Loss: 0.00001824
Iteration 95/1000 | Loss: 0.00001824
Iteration 96/1000 | Loss: 0.00001823
Iteration 97/1000 | Loss: 0.00001823
Iteration 98/1000 | Loss: 0.00001823
Iteration 99/1000 | Loss: 0.00001823
Iteration 100/1000 | Loss: 0.00001823
Iteration 101/1000 | Loss: 0.00001823
Iteration 102/1000 | Loss: 0.00001823
Iteration 103/1000 | Loss: 0.00001823
Iteration 104/1000 | Loss: 0.00001823
Iteration 105/1000 | Loss: 0.00001823
Iteration 106/1000 | Loss: 0.00001823
Iteration 107/1000 | Loss: 0.00001823
Iteration 108/1000 | Loss: 0.00001823
Iteration 109/1000 | Loss: 0.00001823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.8227787222713232e-05, 1.8227787222713232e-05, 1.8227787222713232e-05, 1.8227787222713232e-05, 1.8227787222713232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8227787222713232e-05

Optimization complete. Final v2v error: 3.5365195274353027 mm

Highest mean error: 4.060940265655518 mm for frame 166

Lowest mean error: 3.195361614227295 mm for frame 149

Saving results

Total time: 99.37563562393188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818159
Iteration 2/25 | Loss: 0.00101484
Iteration 3/25 | Loss: 0.00074157
Iteration 4/25 | Loss: 0.00069406
Iteration 5/25 | Loss: 0.00068172
Iteration 6/25 | Loss: 0.00067410
Iteration 7/25 | Loss: 0.00067236
Iteration 8/25 | Loss: 0.00067143
Iteration 9/25 | Loss: 0.00067295
Iteration 10/25 | Loss: 0.00067167
Iteration 11/25 | Loss: 0.00067062
Iteration 12/25 | Loss: 0.00066783
Iteration 13/25 | Loss: 0.00066617
Iteration 14/25 | Loss: 0.00066574
Iteration 15/25 | Loss: 0.00066565
Iteration 16/25 | Loss: 0.00066564
Iteration 17/25 | Loss: 0.00066564
Iteration 18/25 | Loss: 0.00066564
Iteration 19/25 | Loss: 0.00066564
Iteration 20/25 | Loss: 0.00066564
Iteration 21/25 | Loss: 0.00066564
Iteration 22/25 | Loss: 0.00066564
Iteration 23/25 | Loss: 0.00066564
Iteration 24/25 | Loss: 0.00066564
Iteration 25/25 | Loss: 0.00066564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68486476
Iteration 2/25 | Loss: 0.00034232
Iteration 3/25 | Loss: 0.00034232
Iteration 4/25 | Loss: 0.00034232
Iteration 5/25 | Loss: 0.00034232
Iteration 6/25 | Loss: 0.00034232
Iteration 7/25 | Loss: 0.00034232
Iteration 8/25 | Loss: 0.00034232
Iteration 9/25 | Loss: 0.00034232
Iteration 10/25 | Loss: 0.00034232
Iteration 11/25 | Loss: 0.00034232
Iteration 12/25 | Loss: 0.00034232
Iteration 13/25 | Loss: 0.00034232
Iteration 14/25 | Loss: 0.00034232
Iteration 15/25 | Loss: 0.00034232
Iteration 16/25 | Loss: 0.00034232
Iteration 17/25 | Loss: 0.00034232
Iteration 18/25 | Loss: 0.00034232
Iteration 19/25 | Loss: 0.00034232
Iteration 20/25 | Loss: 0.00034232
Iteration 21/25 | Loss: 0.00034232
Iteration 22/25 | Loss: 0.00034232
Iteration 23/25 | Loss: 0.00034232
Iteration 24/25 | Loss: 0.00034232
Iteration 25/25 | Loss: 0.00034232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034232
Iteration 2/1000 | Loss: 0.00002507
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001671
Iteration 5/1000 | Loss: 0.00001596
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001517
Iteration 8/1000 | Loss: 0.00001517
Iteration 9/1000 | Loss: 0.00001495
Iteration 10/1000 | Loss: 0.00001491
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001485
Iteration 13/1000 | Loss: 0.00001482
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00001475
Iteration 16/1000 | Loss: 0.00001473
Iteration 17/1000 | Loss: 0.00001472
Iteration 18/1000 | Loss: 0.00001466
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001463
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001463
Iteration 29/1000 | Loss: 0.00001463
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001463
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001460
Iteration 36/1000 | Loss: 0.00001460
Iteration 37/1000 | Loss: 0.00001460
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001457
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001456
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001455
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001454
Iteration 55/1000 | Loss: 0.00001453
Iteration 56/1000 | Loss: 0.00001453
Iteration 57/1000 | Loss: 0.00001453
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001453
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001452
Iteration 65/1000 | Loss: 0.00001452
Iteration 66/1000 | Loss: 0.00001452
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001450
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001450
Iteration 75/1000 | Loss: 0.00001450
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001448
Iteration 82/1000 | Loss: 0.00001448
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001442
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00001441
Iteration 117/1000 | Loss: 0.00001441
Iteration 118/1000 | Loss: 0.00001441
Iteration 119/1000 | Loss: 0.00001441
Iteration 120/1000 | Loss: 0.00001441
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001441
Iteration 123/1000 | Loss: 0.00001441
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Iteration 131/1000 | Loss: 0.00001441
Iteration 132/1000 | Loss: 0.00001441
Iteration 133/1000 | Loss: 0.00001441
Iteration 134/1000 | Loss: 0.00001441
Iteration 135/1000 | Loss: 0.00001441
Iteration 136/1000 | Loss: 0.00001441
Iteration 137/1000 | Loss: 0.00001441
Iteration 138/1000 | Loss: 0.00001441
Iteration 139/1000 | Loss: 0.00001441
Iteration 140/1000 | Loss: 0.00001441
Iteration 141/1000 | Loss: 0.00001441
Iteration 142/1000 | Loss: 0.00001441
Iteration 143/1000 | Loss: 0.00001441
Iteration 144/1000 | Loss: 0.00001441
Iteration 145/1000 | Loss: 0.00001441
Iteration 146/1000 | Loss: 0.00001441
Iteration 147/1000 | Loss: 0.00001441
Iteration 148/1000 | Loss: 0.00001441
Iteration 149/1000 | Loss: 0.00001441
Iteration 150/1000 | Loss: 0.00001441
Iteration 151/1000 | Loss: 0.00001441
Iteration 152/1000 | Loss: 0.00001441
Iteration 153/1000 | Loss: 0.00001441
Iteration 154/1000 | Loss: 0.00001441
Iteration 155/1000 | Loss: 0.00001441
Iteration 156/1000 | Loss: 0.00001441
Iteration 157/1000 | Loss: 0.00001441
Iteration 158/1000 | Loss: 0.00001441
Iteration 159/1000 | Loss: 0.00001441
Iteration 160/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.4407172784558497e-05, 1.4407172784558497e-05, 1.4407172784558497e-05, 1.4407172784558497e-05, 1.4407172784558497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4407172784558497e-05

Optimization complete. Final v2v error: 3.237640380859375 mm

Highest mean error: 3.9640390872955322 mm for frame 172

Lowest mean error: 2.922569751739502 mm for frame 14

Saving results

Total time: 75.82481455802917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369499
Iteration 2/25 | Loss: 0.00079722
Iteration 3/25 | Loss: 0.00064637
Iteration 4/25 | Loss: 0.00062667
Iteration 5/25 | Loss: 0.00061759
Iteration 6/25 | Loss: 0.00061540
Iteration 7/25 | Loss: 0.00061508
Iteration 8/25 | Loss: 0.00061508
Iteration 9/25 | Loss: 0.00061508
Iteration 10/25 | Loss: 0.00061508
Iteration 11/25 | Loss: 0.00061508
Iteration 12/25 | Loss: 0.00061508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006150770932435989, 0.0006150770932435989, 0.0006150770932435989, 0.0006150770932435989, 0.0006150770932435989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006150770932435989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61092651
Iteration 2/25 | Loss: 0.00031313
Iteration 3/25 | Loss: 0.00031313
Iteration 4/25 | Loss: 0.00031313
Iteration 5/25 | Loss: 0.00031313
Iteration 6/25 | Loss: 0.00031313
Iteration 7/25 | Loss: 0.00031313
Iteration 8/25 | Loss: 0.00031313
Iteration 9/25 | Loss: 0.00031313
Iteration 10/25 | Loss: 0.00031313
Iteration 11/25 | Loss: 0.00031312
Iteration 12/25 | Loss: 0.00031312
Iteration 13/25 | Loss: 0.00031313
Iteration 14/25 | Loss: 0.00031312
Iteration 15/25 | Loss: 0.00031312
Iteration 16/25 | Loss: 0.00031312
Iteration 17/25 | Loss: 0.00031312
Iteration 18/25 | Loss: 0.00031312
Iteration 19/25 | Loss: 0.00031312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00031312499777413905, 0.00031312499777413905, 0.00031312499777413905, 0.00031312499777413905, 0.00031312499777413905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031312499777413905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031312
Iteration 2/1000 | Loss: 0.00002163
Iteration 3/1000 | Loss: 0.00001442
Iteration 4/1000 | Loss: 0.00001333
Iteration 5/1000 | Loss: 0.00001254
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001175
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001149
Iteration 12/1000 | Loss: 0.00001147
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001122
Iteration 15/1000 | Loss: 0.00001118
Iteration 16/1000 | Loss: 0.00001118
Iteration 17/1000 | Loss: 0.00001116
Iteration 18/1000 | Loss: 0.00001115
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001112
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001110
Iteration 24/1000 | Loss: 0.00001109
Iteration 25/1000 | Loss: 0.00001109
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001105
Iteration 31/1000 | Loss: 0.00001105
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001104
Iteration 34/1000 | Loss: 0.00001104
Iteration 35/1000 | Loss: 0.00001103
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001103
Iteration 38/1000 | Loss: 0.00001102
Iteration 39/1000 | Loss: 0.00001102
Iteration 40/1000 | Loss: 0.00001102
Iteration 41/1000 | Loss: 0.00001102
Iteration 42/1000 | Loss: 0.00001102
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001101
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001100
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001098
Iteration 57/1000 | Loss: 0.00001098
Iteration 58/1000 | Loss: 0.00001098
Iteration 59/1000 | Loss: 0.00001098
Iteration 60/1000 | Loss: 0.00001098
Iteration 61/1000 | Loss: 0.00001097
Iteration 62/1000 | Loss: 0.00001097
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001096
Iteration 66/1000 | Loss: 0.00001095
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001095
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001094
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.0944248060695827e-05, 1.0944248060695827e-05, 1.0944248060695827e-05, 1.0944248060695827e-05, 1.0944248060695827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0944248060695827e-05

Optimization complete. Final v2v error: 2.8229482173919678 mm

Highest mean error: 3.1230084896087646 mm for frame 131

Lowest mean error: 2.629070520401001 mm for frame 21

Saving results

Total time: 63.37929034233093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383996
Iteration 2/25 | Loss: 0.00073328
Iteration 3/25 | Loss: 0.00063809
Iteration 4/25 | Loss: 0.00062171
Iteration 5/25 | Loss: 0.00061427
Iteration 6/25 | Loss: 0.00061304
Iteration 7/25 | Loss: 0.00061304
Iteration 8/25 | Loss: 0.00061304
Iteration 9/25 | Loss: 0.00061304
Iteration 10/25 | Loss: 0.00061304
Iteration 11/25 | Loss: 0.00061304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006130352267064154, 0.0006130352267064154, 0.0006130352267064154, 0.0006130352267064154, 0.0006130352267064154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006130352267064154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45886695
Iteration 2/25 | Loss: 0.00027676
Iteration 3/25 | Loss: 0.00027676
Iteration 4/25 | Loss: 0.00027676
Iteration 5/25 | Loss: 0.00027676
Iteration 6/25 | Loss: 0.00027676
Iteration 7/25 | Loss: 0.00027676
Iteration 8/25 | Loss: 0.00027676
Iteration 9/25 | Loss: 0.00027676
Iteration 10/25 | Loss: 0.00027676
Iteration 11/25 | Loss: 0.00027676
Iteration 12/25 | Loss: 0.00027676
Iteration 13/25 | Loss: 0.00027676
Iteration 14/25 | Loss: 0.00027676
Iteration 15/25 | Loss: 0.00027676
Iteration 16/25 | Loss: 0.00027676
Iteration 17/25 | Loss: 0.00027676
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002767560654319823, 0.0002767560654319823, 0.0002767560654319823, 0.0002767560654319823, 0.0002767560654319823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002767560654319823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027676
Iteration 2/1000 | Loss: 0.00002339
Iteration 3/1000 | Loss: 0.00001604
Iteration 4/1000 | Loss: 0.00001449
Iteration 5/1000 | Loss: 0.00001372
Iteration 6/1000 | Loss: 0.00001317
Iteration 7/1000 | Loss: 0.00001290
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001272
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001272
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001272
Iteration 25/1000 | Loss: 0.00001272
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001272
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 29. Stopping optimization.
Last 5 losses: [1.2722820429189596e-05, 1.2722820429189596e-05, 1.2722820429189596e-05, 1.2722820429189596e-05, 1.2722820429189596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2722820429189596e-05

Optimization complete. Final v2v error: 3.0478053092956543 mm

Highest mean error: 3.1906728744506836 mm for frame 41

Lowest mean error: 2.8851375579833984 mm for frame 228

Saving results

Total time: 29.364734888076782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874111
Iteration 2/25 | Loss: 0.00135177
Iteration 3/25 | Loss: 0.00086273
Iteration 4/25 | Loss: 0.00079573
Iteration 5/25 | Loss: 0.00078385
Iteration 6/25 | Loss: 0.00078137
Iteration 7/25 | Loss: 0.00078011
Iteration 8/25 | Loss: 0.00077826
Iteration 9/25 | Loss: 0.00077788
Iteration 10/25 | Loss: 0.00077773
Iteration 11/25 | Loss: 0.00077768
Iteration 12/25 | Loss: 0.00077768
Iteration 13/25 | Loss: 0.00077768
Iteration 14/25 | Loss: 0.00077768
Iteration 15/25 | Loss: 0.00077768
Iteration 16/25 | Loss: 0.00077768
Iteration 17/25 | Loss: 0.00077768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007776752463541925, 0.0007776752463541925, 0.0007776752463541925, 0.0007776752463541925, 0.0007776752463541925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007776752463541925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37834907
Iteration 2/25 | Loss: 0.00039032
Iteration 3/25 | Loss: 0.00039032
Iteration 4/25 | Loss: 0.00039032
Iteration 5/25 | Loss: 0.00039032
Iteration 6/25 | Loss: 0.00039032
Iteration 7/25 | Loss: 0.00039032
Iteration 8/25 | Loss: 0.00039032
Iteration 9/25 | Loss: 0.00039032
Iteration 10/25 | Loss: 0.00039032
Iteration 11/25 | Loss: 0.00039032
Iteration 12/25 | Loss: 0.00039032
Iteration 13/25 | Loss: 0.00039032
Iteration 14/25 | Loss: 0.00039032
Iteration 15/25 | Loss: 0.00039032
Iteration 16/25 | Loss: 0.00039032
Iteration 17/25 | Loss: 0.00039032
Iteration 18/25 | Loss: 0.00039032
Iteration 19/25 | Loss: 0.00039032
Iteration 20/25 | Loss: 0.00039032
Iteration 21/25 | Loss: 0.00039032
Iteration 22/25 | Loss: 0.00039032
Iteration 23/25 | Loss: 0.00039032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003903168544638902, 0.0003903168544638902, 0.0003903168544638902, 0.0003903168544638902, 0.0003903168544638902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003903168544638902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039032
Iteration 2/1000 | Loss: 0.00003950
Iteration 3/1000 | Loss: 0.00003085
Iteration 4/1000 | Loss: 0.00002853
Iteration 5/1000 | Loss: 0.00002755
Iteration 6/1000 | Loss: 0.00002644
Iteration 7/1000 | Loss: 0.00002573
Iteration 8/1000 | Loss: 0.00002534
Iteration 9/1000 | Loss: 0.00002504
Iteration 10/1000 | Loss: 0.00002489
Iteration 11/1000 | Loss: 0.00002485
Iteration 12/1000 | Loss: 0.00002485
Iteration 13/1000 | Loss: 0.00002477
Iteration 14/1000 | Loss: 0.00002476
Iteration 15/1000 | Loss: 0.00002476
Iteration 16/1000 | Loss: 0.00002473
Iteration 17/1000 | Loss: 0.00002471
Iteration 18/1000 | Loss: 0.00002469
Iteration 19/1000 | Loss: 0.00002469
Iteration 20/1000 | Loss: 0.00002469
Iteration 21/1000 | Loss: 0.00002469
Iteration 22/1000 | Loss: 0.00002469
Iteration 23/1000 | Loss: 0.00002468
Iteration 24/1000 | Loss: 0.00002468
Iteration 25/1000 | Loss: 0.00002468
Iteration 26/1000 | Loss: 0.00002468
Iteration 27/1000 | Loss: 0.00002468
Iteration 28/1000 | Loss: 0.00002466
Iteration 29/1000 | Loss: 0.00002466
Iteration 30/1000 | Loss: 0.00002466
Iteration 31/1000 | Loss: 0.00002466
Iteration 32/1000 | Loss: 0.00002466
Iteration 33/1000 | Loss: 0.00002465
Iteration 34/1000 | Loss: 0.00002465
Iteration 35/1000 | Loss: 0.00002465
Iteration 36/1000 | Loss: 0.00002465
Iteration 37/1000 | Loss: 0.00002465
Iteration 38/1000 | Loss: 0.00002465
Iteration 39/1000 | Loss: 0.00002465
Iteration 40/1000 | Loss: 0.00002465
Iteration 41/1000 | Loss: 0.00002465
Iteration 42/1000 | Loss: 0.00002464
Iteration 43/1000 | Loss: 0.00002464
Iteration 44/1000 | Loss: 0.00002464
Iteration 45/1000 | Loss: 0.00002464
Iteration 46/1000 | Loss: 0.00002464
Iteration 47/1000 | Loss: 0.00002463
Iteration 48/1000 | Loss: 0.00002463
Iteration 49/1000 | Loss: 0.00002463
Iteration 50/1000 | Loss: 0.00002463
Iteration 51/1000 | Loss: 0.00002462
Iteration 52/1000 | Loss: 0.00002462
Iteration 53/1000 | Loss: 0.00002462
Iteration 54/1000 | Loss: 0.00002462
Iteration 55/1000 | Loss: 0.00002462
Iteration 56/1000 | Loss: 0.00002462
Iteration 57/1000 | Loss: 0.00002461
Iteration 58/1000 | Loss: 0.00002461
Iteration 59/1000 | Loss: 0.00002461
Iteration 60/1000 | Loss: 0.00002461
Iteration 61/1000 | Loss: 0.00002461
Iteration 62/1000 | Loss: 0.00002461
Iteration 63/1000 | Loss: 0.00002460
Iteration 64/1000 | Loss: 0.00002460
Iteration 65/1000 | Loss: 0.00002460
Iteration 66/1000 | Loss: 0.00002460
Iteration 67/1000 | Loss: 0.00002460
Iteration 68/1000 | Loss: 0.00002460
Iteration 69/1000 | Loss: 0.00002459
Iteration 70/1000 | Loss: 0.00002459
Iteration 71/1000 | Loss: 0.00002459
Iteration 72/1000 | Loss: 0.00002459
Iteration 73/1000 | Loss: 0.00002459
Iteration 74/1000 | Loss: 0.00002458
Iteration 75/1000 | Loss: 0.00002458
Iteration 76/1000 | Loss: 0.00002458
Iteration 77/1000 | Loss: 0.00002458
Iteration 78/1000 | Loss: 0.00002458
Iteration 79/1000 | Loss: 0.00002458
Iteration 80/1000 | Loss: 0.00002458
Iteration 81/1000 | Loss: 0.00002458
Iteration 82/1000 | Loss: 0.00002458
Iteration 83/1000 | Loss: 0.00002457
Iteration 84/1000 | Loss: 0.00002457
Iteration 85/1000 | Loss: 0.00002457
Iteration 86/1000 | Loss: 0.00002457
Iteration 87/1000 | Loss: 0.00002457
Iteration 88/1000 | Loss: 0.00002456
Iteration 89/1000 | Loss: 0.00002456
Iteration 90/1000 | Loss: 0.00002456
Iteration 91/1000 | Loss: 0.00002456
Iteration 92/1000 | Loss: 0.00002456
Iteration 93/1000 | Loss: 0.00002456
Iteration 94/1000 | Loss: 0.00002456
Iteration 95/1000 | Loss: 0.00002456
Iteration 96/1000 | Loss: 0.00002456
Iteration 97/1000 | Loss: 0.00002456
Iteration 98/1000 | Loss: 0.00002456
Iteration 99/1000 | Loss: 0.00002455
Iteration 100/1000 | Loss: 0.00002455
Iteration 101/1000 | Loss: 0.00002455
Iteration 102/1000 | Loss: 0.00002455
Iteration 103/1000 | Loss: 0.00002455
Iteration 104/1000 | Loss: 0.00002455
Iteration 105/1000 | Loss: 0.00002455
Iteration 106/1000 | Loss: 0.00002455
Iteration 107/1000 | Loss: 0.00002455
Iteration 108/1000 | Loss: 0.00002455
Iteration 109/1000 | Loss: 0.00002455
Iteration 110/1000 | Loss: 0.00002455
Iteration 111/1000 | Loss: 0.00002455
Iteration 112/1000 | Loss: 0.00002455
Iteration 113/1000 | Loss: 0.00002455
Iteration 114/1000 | Loss: 0.00002455
Iteration 115/1000 | Loss: 0.00002455
Iteration 116/1000 | Loss: 0.00002455
Iteration 117/1000 | Loss: 0.00002455
Iteration 118/1000 | Loss: 0.00002455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.4546199711039662e-05, 2.4546199711039662e-05, 2.4546199711039662e-05, 2.4546199711039662e-05, 2.4546199711039662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4546199711039662e-05

Optimization complete. Final v2v error: 4.13732385635376 mm

Highest mean error: 4.846864223480225 mm for frame 134

Lowest mean error: 3.5247762203216553 mm for frame 82

Saving results

Total time: 75.14057540893555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787247
Iteration 2/25 | Loss: 0.00094579
Iteration 3/25 | Loss: 0.00079423
Iteration 4/25 | Loss: 0.00075881
Iteration 5/25 | Loss: 0.00074798
Iteration 6/25 | Loss: 0.00074586
Iteration 7/25 | Loss: 0.00074518
Iteration 8/25 | Loss: 0.00074518
Iteration 9/25 | Loss: 0.00074518
Iteration 10/25 | Loss: 0.00074518
Iteration 11/25 | Loss: 0.00074518
Iteration 12/25 | Loss: 0.00074518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007451800629496574, 0.0007451800629496574, 0.0007451800629496574, 0.0007451800629496574, 0.0007451800629496574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007451800629496574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47059810
Iteration 2/25 | Loss: 0.00039020
Iteration 3/25 | Loss: 0.00039020
Iteration 4/25 | Loss: 0.00039020
Iteration 5/25 | Loss: 0.00039020
Iteration 6/25 | Loss: 0.00039020
Iteration 7/25 | Loss: 0.00039020
Iteration 8/25 | Loss: 0.00039020
Iteration 9/25 | Loss: 0.00039019
Iteration 10/25 | Loss: 0.00039019
Iteration 11/25 | Loss: 0.00039019
Iteration 12/25 | Loss: 0.00039019
Iteration 13/25 | Loss: 0.00039019
Iteration 14/25 | Loss: 0.00039019
Iteration 15/25 | Loss: 0.00039019
Iteration 16/25 | Loss: 0.00039019
Iteration 17/25 | Loss: 0.00039019
Iteration 18/25 | Loss: 0.00039019
Iteration 19/25 | Loss: 0.00039019
Iteration 20/25 | Loss: 0.00039019
Iteration 21/25 | Loss: 0.00039019
Iteration 22/25 | Loss: 0.00039019
Iteration 23/25 | Loss: 0.00039019
Iteration 24/25 | Loss: 0.00039019
Iteration 25/25 | Loss: 0.00039019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039019
Iteration 2/1000 | Loss: 0.00005643
Iteration 3/1000 | Loss: 0.00004000
Iteration 4/1000 | Loss: 0.00003430
Iteration 5/1000 | Loss: 0.00003239
Iteration 6/1000 | Loss: 0.00003110
Iteration 7/1000 | Loss: 0.00003021
Iteration 8/1000 | Loss: 0.00002951
Iteration 9/1000 | Loss: 0.00002906
Iteration 10/1000 | Loss: 0.00002874
Iteration 11/1000 | Loss: 0.00002852
Iteration 12/1000 | Loss: 0.00002837
Iteration 13/1000 | Loss: 0.00002829
Iteration 14/1000 | Loss: 0.00002821
Iteration 15/1000 | Loss: 0.00002813
Iteration 16/1000 | Loss: 0.00002807
Iteration 17/1000 | Loss: 0.00002806
Iteration 18/1000 | Loss: 0.00002806
Iteration 19/1000 | Loss: 0.00002806
Iteration 20/1000 | Loss: 0.00002805
Iteration 21/1000 | Loss: 0.00002805
Iteration 22/1000 | Loss: 0.00002803
Iteration 23/1000 | Loss: 0.00002802
Iteration 24/1000 | Loss: 0.00002801
Iteration 25/1000 | Loss: 0.00002801
Iteration 26/1000 | Loss: 0.00002801
Iteration 27/1000 | Loss: 0.00002801
Iteration 28/1000 | Loss: 0.00002800
Iteration 29/1000 | Loss: 0.00002800
Iteration 30/1000 | Loss: 0.00002799
Iteration 31/1000 | Loss: 0.00002799
Iteration 32/1000 | Loss: 0.00002798
Iteration 33/1000 | Loss: 0.00002798
Iteration 34/1000 | Loss: 0.00002797
Iteration 35/1000 | Loss: 0.00002797
Iteration 36/1000 | Loss: 0.00002797
Iteration 37/1000 | Loss: 0.00002797
Iteration 38/1000 | Loss: 0.00002797
Iteration 39/1000 | Loss: 0.00002796
Iteration 40/1000 | Loss: 0.00002796
Iteration 41/1000 | Loss: 0.00002795
Iteration 42/1000 | Loss: 0.00002795
Iteration 43/1000 | Loss: 0.00002795
Iteration 44/1000 | Loss: 0.00002794
Iteration 45/1000 | Loss: 0.00002794
Iteration 46/1000 | Loss: 0.00002794
Iteration 47/1000 | Loss: 0.00002793
Iteration 48/1000 | Loss: 0.00002793
Iteration 49/1000 | Loss: 0.00002792
Iteration 50/1000 | Loss: 0.00002792
Iteration 51/1000 | Loss: 0.00002792
Iteration 52/1000 | Loss: 0.00002792
Iteration 53/1000 | Loss: 0.00002791
Iteration 54/1000 | Loss: 0.00002791
Iteration 55/1000 | Loss: 0.00002791
Iteration 56/1000 | Loss: 0.00002791
Iteration 57/1000 | Loss: 0.00002791
Iteration 58/1000 | Loss: 0.00002790
Iteration 59/1000 | Loss: 0.00002790
Iteration 60/1000 | Loss: 0.00002790
Iteration 61/1000 | Loss: 0.00002790
Iteration 62/1000 | Loss: 0.00002789
Iteration 63/1000 | Loss: 0.00002789
Iteration 64/1000 | Loss: 0.00002789
Iteration 65/1000 | Loss: 0.00002789
Iteration 66/1000 | Loss: 0.00002789
Iteration 67/1000 | Loss: 0.00002789
Iteration 68/1000 | Loss: 0.00002789
Iteration 69/1000 | Loss: 0.00002789
Iteration 70/1000 | Loss: 0.00002788
Iteration 71/1000 | Loss: 0.00002788
Iteration 72/1000 | Loss: 0.00002788
Iteration 73/1000 | Loss: 0.00002788
Iteration 74/1000 | Loss: 0.00002788
Iteration 75/1000 | Loss: 0.00002788
Iteration 76/1000 | Loss: 0.00002788
Iteration 77/1000 | Loss: 0.00002788
Iteration 78/1000 | Loss: 0.00002788
Iteration 79/1000 | Loss: 0.00002787
Iteration 80/1000 | Loss: 0.00002787
Iteration 81/1000 | Loss: 0.00002787
Iteration 82/1000 | Loss: 0.00002787
Iteration 83/1000 | Loss: 0.00002787
Iteration 84/1000 | Loss: 0.00002787
Iteration 85/1000 | Loss: 0.00002787
Iteration 86/1000 | Loss: 0.00002787
Iteration 87/1000 | Loss: 0.00002787
Iteration 88/1000 | Loss: 0.00002787
Iteration 89/1000 | Loss: 0.00002787
Iteration 90/1000 | Loss: 0.00002787
Iteration 91/1000 | Loss: 0.00002787
Iteration 92/1000 | Loss: 0.00002787
Iteration 93/1000 | Loss: 0.00002787
Iteration 94/1000 | Loss: 0.00002787
Iteration 95/1000 | Loss: 0.00002787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.787367338896729e-05, 2.787367338896729e-05, 2.787367338896729e-05, 2.787367338896729e-05, 2.787367338896729e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.787367338896729e-05

Optimization complete. Final v2v error: 4.329079627990723 mm

Highest mean error: 5.051175117492676 mm for frame 40

Lowest mean error: 3.805586814880371 mm for frame 170

Saving results

Total time: 39.79365825653076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930029
Iteration 2/25 | Loss: 0.00324797
Iteration 3/25 | Loss: 0.00199556
Iteration 4/25 | Loss: 0.00151891
Iteration 5/25 | Loss: 0.00139183
Iteration 6/25 | Loss: 0.00120706
Iteration 7/25 | Loss: 0.00119196
Iteration 8/25 | Loss: 0.00111393
Iteration 9/25 | Loss: 0.00106949
Iteration 10/25 | Loss: 0.00100495
Iteration 11/25 | Loss: 0.00096378
Iteration 12/25 | Loss: 0.00094807
Iteration 13/25 | Loss: 0.00094402
Iteration 14/25 | Loss: 0.00093494
Iteration 15/25 | Loss: 0.00093075
Iteration 16/25 | Loss: 0.00092899
Iteration 17/25 | Loss: 0.00093248
Iteration 18/25 | Loss: 0.00093057
Iteration 19/25 | Loss: 0.00092918
Iteration 20/25 | Loss: 0.00092309
Iteration 21/25 | Loss: 0.00093614
Iteration 22/25 | Loss: 0.00091698
Iteration 23/25 | Loss: 0.00091593
Iteration 24/25 | Loss: 0.00091542
Iteration 25/25 | Loss: 0.00091520

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.65839815
Iteration 2/25 | Loss: 0.00345233
Iteration 3/25 | Loss: 0.00273445
Iteration 4/25 | Loss: 0.00273445
Iteration 5/25 | Loss: 0.00273445
Iteration 6/25 | Loss: 0.00273445
Iteration 7/25 | Loss: 0.00273445
Iteration 8/25 | Loss: 0.00273445
Iteration 9/25 | Loss: 0.00273445
Iteration 10/25 | Loss: 0.00273445
Iteration 11/25 | Loss: 0.00273445
Iteration 12/25 | Loss: 0.00273445
Iteration 13/25 | Loss: 0.00273445
Iteration 14/25 | Loss: 0.00273445
Iteration 15/25 | Loss: 0.00273445
Iteration 16/25 | Loss: 0.00273445
Iteration 17/25 | Loss: 0.00273445
Iteration 18/25 | Loss: 0.00273445
Iteration 19/25 | Loss: 0.00273445
Iteration 20/25 | Loss: 0.00273445
Iteration 21/25 | Loss: 0.00273445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0027344487607479095, 0.0027344487607479095, 0.0027344487607479095, 0.0027344487607479095, 0.0027344487607479095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027344487607479095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273445
Iteration 2/1000 | Loss: 0.00136293
Iteration 3/1000 | Loss: 0.00124995
Iteration 4/1000 | Loss: 0.00544178
Iteration 5/1000 | Loss: 0.00421949
Iteration 6/1000 | Loss: 0.00347107
Iteration 7/1000 | Loss: 0.00393262
Iteration 8/1000 | Loss: 0.00090784
Iteration 9/1000 | Loss: 0.00067425
Iteration 10/1000 | Loss: 0.00219146
Iteration 11/1000 | Loss: 0.00074793
Iteration 12/1000 | Loss: 0.00049102
Iteration 13/1000 | Loss: 0.00251028
Iteration 14/1000 | Loss: 0.00085047
Iteration 15/1000 | Loss: 0.00160617
Iteration 16/1000 | Loss: 0.00064129
Iteration 17/1000 | Loss: 0.00085213
Iteration 18/1000 | Loss: 0.00074365
Iteration 19/1000 | Loss: 0.00126680
Iteration 20/1000 | Loss: 0.00238256
Iteration 21/1000 | Loss: 0.00241988
Iteration 22/1000 | Loss: 0.00245081
Iteration 23/1000 | Loss: 0.00106581
Iteration 24/1000 | Loss: 0.00074196
Iteration 25/1000 | Loss: 0.00310842
Iteration 26/1000 | Loss: 0.00124278
Iteration 27/1000 | Loss: 0.00117618
Iteration 28/1000 | Loss: 0.00375939
Iteration 29/1000 | Loss: 0.00111012
Iteration 30/1000 | Loss: 0.00129236
Iteration 31/1000 | Loss: 0.00112704
Iteration 32/1000 | Loss: 0.00152629
Iteration 33/1000 | Loss: 0.00162524
Iteration 34/1000 | Loss: 0.00146166
Iteration 35/1000 | Loss: 0.00138640
Iteration 36/1000 | Loss: 0.00128572
Iteration 37/1000 | Loss: 0.00119961
Iteration 38/1000 | Loss: 0.00109832
Iteration 39/1000 | Loss: 0.00332113
Iteration 40/1000 | Loss: 0.00396344
Iteration 41/1000 | Loss: 0.00454537
Iteration 42/1000 | Loss: 0.00279242
Iteration 43/1000 | Loss: 0.00244286
Iteration 44/1000 | Loss: 0.00056640
Iteration 45/1000 | Loss: 0.00138437
Iteration 46/1000 | Loss: 0.00094945
Iteration 47/1000 | Loss: 0.00201551
Iteration 48/1000 | Loss: 0.00267416
Iteration 49/1000 | Loss: 0.00100580
Iteration 50/1000 | Loss: 0.00049974
Iteration 51/1000 | Loss: 0.00134429
Iteration 52/1000 | Loss: 0.00096790
Iteration 53/1000 | Loss: 0.00077931
Iteration 54/1000 | Loss: 0.00143702
Iteration 55/1000 | Loss: 0.00121505
Iteration 56/1000 | Loss: 0.00065355
Iteration 57/1000 | Loss: 0.00151951
Iteration 58/1000 | Loss: 0.00072614
Iteration 59/1000 | Loss: 0.00018380
Iteration 60/1000 | Loss: 0.00015851
Iteration 61/1000 | Loss: 0.00143323
Iteration 62/1000 | Loss: 0.00139345
Iteration 63/1000 | Loss: 0.00058033
Iteration 64/1000 | Loss: 0.00093620
Iteration 65/1000 | Loss: 0.00028212
Iteration 66/1000 | Loss: 0.00090999
Iteration 67/1000 | Loss: 0.00050413
Iteration 68/1000 | Loss: 0.00160628
Iteration 69/1000 | Loss: 0.00038673
Iteration 70/1000 | Loss: 0.00035211
Iteration 71/1000 | Loss: 0.00030021
Iteration 72/1000 | Loss: 0.00136415
Iteration 73/1000 | Loss: 0.00065565
Iteration 74/1000 | Loss: 0.00060272
Iteration 75/1000 | Loss: 0.00077409
Iteration 76/1000 | Loss: 0.00081346
Iteration 77/1000 | Loss: 0.00071285
Iteration 78/1000 | Loss: 0.00018885
Iteration 79/1000 | Loss: 0.00004806
Iteration 80/1000 | Loss: 0.00004256
Iteration 81/1000 | Loss: 0.00004001
Iteration 82/1000 | Loss: 0.00045362
Iteration 83/1000 | Loss: 0.00025824
Iteration 84/1000 | Loss: 0.00043404
Iteration 85/1000 | Loss: 0.00012439
Iteration 86/1000 | Loss: 0.00083599
Iteration 87/1000 | Loss: 0.00082319
Iteration 88/1000 | Loss: 0.00021481
Iteration 89/1000 | Loss: 0.00075564
Iteration 90/1000 | Loss: 0.00119900
Iteration 91/1000 | Loss: 0.00129112
Iteration 92/1000 | Loss: 0.00086656
Iteration 93/1000 | Loss: 0.00038723
Iteration 94/1000 | Loss: 0.00011463
Iteration 95/1000 | Loss: 0.00056682
Iteration 96/1000 | Loss: 0.00049690
Iteration 97/1000 | Loss: 0.00027366
Iteration 98/1000 | Loss: 0.00005320
Iteration 99/1000 | Loss: 0.00004377
Iteration 100/1000 | Loss: 0.00048394
Iteration 101/1000 | Loss: 0.00048034
Iteration 102/1000 | Loss: 0.00047761
Iteration 103/1000 | Loss: 0.00047004
Iteration 104/1000 | Loss: 0.00047033
Iteration 105/1000 | Loss: 0.00033262
Iteration 106/1000 | Loss: 0.00017678
Iteration 107/1000 | Loss: 0.00058801
Iteration 108/1000 | Loss: 0.00025221
Iteration 109/1000 | Loss: 0.00016729
Iteration 110/1000 | Loss: 0.00011192
Iteration 111/1000 | Loss: 0.00016516
Iteration 112/1000 | Loss: 0.00017917
Iteration 113/1000 | Loss: 0.00025600
Iteration 114/1000 | Loss: 0.00016553
Iteration 115/1000 | Loss: 0.00019411
Iteration 116/1000 | Loss: 0.00041977
Iteration 117/1000 | Loss: 0.00068148
Iteration 118/1000 | Loss: 0.00025317
Iteration 119/1000 | Loss: 0.00004249
Iteration 120/1000 | Loss: 0.00039537
Iteration 121/1000 | Loss: 0.00036630
Iteration 122/1000 | Loss: 0.00003972
Iteration 123/1000 | Loss: 0.00039428
Iteration 124/1000 | Loss: 0.00003750
Iteration 125/1000 | Loss: 0.00025608
Iteration 126/1000 | Loss: 0.00012034
Iteration 127/1000 | Loss: 0.00047848
Iteration 128/1000 | Loss: 0.00049733
Iteration 129/1000 | Loss: 0.00080204
Iteration 130/1000 | Loss: 0.00016023
Iteration 131/1000 | Loss: 0.00015921
Iteration 132/1000 | Loss: 0.00021425
Iteration 133/1000 | Loss: 0.00065524
Iteration 134/1000 | Loss: 0.00056124
Iteration 135/1000 | Loss: 0.00049865
Iteration 136/1000 | Loss: 0.00020422
Iteration 137/1000 | Loss: 0.00030852
Iteration 138/1000 | Loss: 0.00021779
Iteration 139/1000 | Loss: 0.00045978
Iteration 140/1000 | Loss: 0.00020665
Iteration 141/1000 | Loss: 0.00034089
Iteration 142/1000 | Loss: 0.00068297
Iteration 143/1000 | Loss: 0.00023999
Iteration 144/1000 | Loss: 0.00004458
Iteration 145/1000 | Loss: 0.00003750
Iteration 146/1000 | Loss: 0.00003259
Iteration 147/1000 | Loss: 0.00003068
Iteration 148/1000 | Loss: 0.00028521
Iteration 149/1000 | Loss: 0.00003611
Iteration 150/1000 | Loss: 0.00097281
Iteration 151/1000 | Loss: 0.00090783
Iteration 152/1000 | Loss: 0.00004121
Iteration 153/1000 | Loss: 0.00019135
Iteration 154/1000 | Loss: 0.00125261
Iteration 155/1000 | Loss: 0.00057959
Iteration 156/1000 | Loss: 0.00008869
Iteration 157/1000 | Loss: 0.00003957
Iteration 158/1000 | Loss: 0.00076333
Iteration 159/1000 | Loss: 0.00017544
Iteration 160/1000 | Loss: 0.00076501
Iteration 161/1000 | Loss: 0.00032687
Iteration 162/1000 | Loss: 0.00004647
Iteration 163/1000 | Loss: 0.00003489
Iteration 164/1000 | Loss: 0.00003224
Iteration 165/1000 | Loss: 0.00044594
Iteration 166/1000 | Loss: 0.00005709
Iteration 167/1000 | Loss: 0.00003353
Iteration 168/1000 | Loss: 0.00018182
Iteration 169/1000 | Loss: 0.00017246
Iteration 170/1000 | Loss: 0.00066340
Iteration 171/1000 | Loss: 0.00025342
Iteration 172/1000 | Loss: 0.00082981
Iteration 173/1000 | Loss: 0.00026921
Iteration 174/1000 | Loss: 0.00053427
Iteration 175/1000 | Loss: 0.00111703
Iteration 176/1000 | Loss: 0.00007105
Iteration 177/1000 | Loss: 0.00003533
Iteration 178/1000 | Loss: 0.00003162
Iteration 179/1000 | Loss: 0.00044160
Iteration 180/1000 | Loss: 0.00003564
Iteration 181/1000 | Loss: 0.00023499
Iteration 182/1000 | Loss: 0.00003607
Iteration 183/1000 | Loss: 0.00003064
Iteration 184/1000 | Loss: 0.00002962
Iteration 185/1000 | Loss: 0.00020519
Iteration 186/1000 | Loss: 0.00006260
Iteration 187/1000 | Loss: 0.00002948
Iteration 188/1000 | Loss: 0.00002893
Iteration 189/1000 | Loss: 0.00027211
Iteration 190/1000 | Loss: 0.00024155
Iteration 191/1000 | Loss: 0.00023718
Iteration 192/1000 | Loss: 0.00002988
Iteration 193/1000 | Loss: 0.00011437
Iteration 194/1000 | Loss: 0.00005121
Iteration 195/1000 | Loss: 0.00024667
Iteration 196/1000 | Loss: 0.00042201
Iteration 197/1000 | Loss: 0.00007935
Iteration 198/1000 | Loss: 0.00003069
Iteration 199/1000 | Loss: 0.00026715
Iteration 200/1000 | Loss: 0.00037781
Iteration 201/1000 | Loss: 0.00015472
Iteration 202/1000 | Loss: 0.00028913
Iteration 203/1000 | Loss: 0.00029452
Iteration 204/1000 | Loss: 0.00024570
Iteration 205/1000 | Loss: 0.00033945
Iteration 206/1000 | Loss: 0.00063173
Iteration 207/1000 | Loss: 0.00047286
Iteration 208/1000 | Loss: 0.00036649
Iteration 209/1000 | Loss: 0.00050192
Iteration 210/1000 | Loss: 0.00142272
Iteration 211/1000 | Loss: 0.00091039
Iteration 212/1000 | Loss: 0.00101794
Iteration 213/1000 | Loss: 0.00042460
Iteration 214/1000 | Loss: 0.00031602
Iteration 215/1000 | Loss: 0.00024249
Iteration 216/1000 | Loss: 0.00045994
Iteration 217/1000 | Loss: 0.00046383
Iteration 218/1000 | Loss: 0.00068315
Iteration 219/1000 | Loss: 0.00063712
Iteration 220/1000 | Loss: 0.00034851
Iteration 221/1000 | Loss: 0.00009705
Iteration 222/1000 | Loss: 0.00090544
Iteration 223/1000 | Loss: 0.00044005
Iteration 224/1000 | Loss: 0.00038284
Iteration 225/1000 | Loss: 0.00013692
Iteration 226/1000 | Loss: 0.00029234
Iteration 227/1000 | Loss: 0.00028971
Iteration 228/1000 | Loss: 0.00034333
Iteration 229/1000 | Loss: 0.00052980
Iteration 230/1000 | Loss: 0.00062965
Iteration 231/1000 | Loss: 0.00052345
Iteration 232/1000 | Loss: 0.00016982
Iteration 233/1000 | Loss: 0.00042372
Iteration 234/1000 | Loss: 0.00044141
Iteration 235/1000 | Loss: 0.00026554
Iteration 236/1000 | Loss: 0.00046774
Iteration 237/1000 | Loss: 0.00019148
Iteration 238/1000 | Loss: 0.00037594
Iteration 239/1000 | Loss: 0.00004667
Iteration 240/1000 | Loss: 0.00003915
Iteration 241/1000 | Loss: 0.00136251
Iteration 242/1000 | Loss: 0.00022852
Iteration 243/1000 | Loss: 0.00045097
Iteration 244/1000 | Loss: 0.00082151
Iteration 245/1000 | Loss: 0.00003274
Iteration 246/1000 | Loss: 0.00002863
Iteration 247/1000 | Loss: 0.00002636
Iteration 248/1000 | Loss: 0.00041611
Iteration 249/1000 | Loss: 0.00004640
Iteration 250/1000 | Loss: 0.00029221
Iteration 251/1000 | Loss: 0.00005622
Iteration 252/1000 | Loss: 0.00013050
Iteration 253/1000 | Loss: 0.00056753
Iteration 254/1000 | Loss: 0.00004865
Iteration 255/1000 | Loss: 0.00003460
Iteration 256/1000 | Loss: 0.00002691
Iteration 257/1000 | Loss: 0.00039361
Iteration 258/1000 | Loss: 0.00002461
Iteration 259/1000 | Loss: 0.00002333
Iteration 260/1000 | Loss: 0.00002292
Iteration 261/1000 | Loss: 0.00002271
Iteration 262/1000 | Loss: 0.00002270
Iteration 263/1000 | Loss: 0.00002264
Iteration 264/1000 | Loss: 0.00002256
Iteration 265/1000 | Loss: 0.00002256
Iteration 266/1000 | Loss: 0.00002251
Iteration 267/1000 | Loss: 0.00002243
Iteration 268/1000 | Loss: 0.00002233
Iteration 269/1000 | Loss: 0.00002232
Iteration 270/1000 | Loss: 0.00002232
Iteration 271/1000 | Loss: 0.00002231
Iteration 272/1000 | Loss: 0.00002231
Iteration 273/1000 | Loss: 0.00002231
Iteration 274/1000 | Loss: 0.00002231
Iteration 275/1000 | Loss: 0.00002231
Iteration 276/1000 | Loss: 0.00002231
Iteration 277/1000 | Loss: 0.00002231
Iteration 278/1000 | Loss: 0.00002230
Iteration 279/1000 | Loss: 0.00002230
Iteration 280/1000 | Loss: 0.00002229
Iteration 281/1000 | Loss: 0.00002229
Iteration 282/1000 | Loss: 0.00002228
Iteration 283/1000 | Loss: 0.00002227
Iteration 284/1000 | Loss: 0.00002226
Iteration 285/1000 | Loss: 0.00002226
Iteration 286/1000 | Loss: 0.00002226
Iteration 287/1000 | Loss: 0.00002226
Iteration 288/1000 | Loss: 0.00002226
Iteration 289/1000 | Loss: 0.00002225
Iteration 290/1000 | Loss: 0.00002225
Iteration 291/1000 | Loss: 0.00002225
Iteration 292/1000 | Loss: 0.00002225
Iteration 293/1000 | Loss: 0.00002225
Iteration 294/1000 | Loss: 0.00002224
Iteration 295/1000 | Loss: 0.00002224
Iteration 296/1000 | Loss: 0.00002224
Iteration 297/1000 | Loss: 0.00002223
Iteration 298/1000 | Loss: 0.00002223
Iteration 299/1000 | Loss: 0.00002223
Iteration 300/1000 | Loss: 0.00002223
Iteration 301/1000 | Loss: 0.00002222
Iteration 302/1000 | Loss: 0.00002222
Iteration 303/1000 | Loss: 0.00002222
Iteration 304/1000 | Loss: 0.00002222
Iteration 305/1000 | Loss: 0.00002222
Iteration 306/1000 | Loss: 0.00002221
Iteration 307/1000 | Loss: 0.00002221
Iteration 308/1000 | Loss: 0.00002221
Iteration 309/1000 | Loss: 0.00002221
Iteration 310/1000 | Loss: 0.00002221
Iteration 311/1000 | Loss: 0.00002220
Iteration 312/1000 | Loss: 0.00002220
Iteration 313/1000 | Loss: 0.00002220
Iteration 314/1000 | Loss: 0.00002220
Iteration 315/1000 | Loss: 0.00002220
Iteration 316/1000 | Loss: 0.00002220
Iteration 317/1000 | Loss: 0.00002220
Iteration 318/1000 | Loss: 0.00002220
Iteration 319/1000 | Loss: 0.00002220
Iteration 320/1000 | Loss: 0.00002220
Iteration 321/1000 | Loss: 0.00002219
Iteration 322/1000 | Loss: 0.00002219
Iteration 323/1000 | Loss: 0.00002219
Iteration 324/1000 | Loss: 0.00002219
Iteration 325/1000 | Loss: 0.00002219
Iteration 326/1000 | Loss: 0.00002219
Iteration 327/1000 | Loss: 0.00002219
Iteration 328/1000 | Loss: 0.00002219
Iteration 329/1000 | Loss: 0.00002219
Iteration 330/1000 | Loss: 0.00002219
Iteration 331/1000 | Loss: 0.00002218
Iteration 332/1000 | Loss: 0.00002218
Iteration 333/1000 | Loss: 0.00002218
Iteration 334/1000 | Loss: 0.00002218
Iteration 335/1000 | Loss: 0.00002218
Iteration 336/1000 | Loss: 0.00002217
Iteration 337/1000 | Loss: 0.00002217
Iteration 338/1000 | Loss: 0.00002217
Iteration 339/1000 | Loss: 0.00002216
Iteration 340/1000 | Loss: 0.00002216
Iteration 341/1000 | Loss: 0.00002216
Iteration 342/1000 | Loss: 0.00002216
Iteration 343/1000 | Loss: 0.00002216
Iteration 344/1000 | Loss: 0.00002216
Iteration 345/1000 | Loss: 0.00002216
Iteration 346/1000 | Loss: 0.00002215
Iteration 347/1000 | Loss: 0.00002215
Iteration 348/1000 | Loss: 0.00002215
Iteration 349/1000 | Loss: 0.00002215
Iteration 350/1000 | Loss: 0.00002214
Iteration 351/1000 | Loss: 0.00002214
Iteration 352/1000 | Loss: 0.00002214
Iteration 353/1000 | Loss: 0.00002214
Iteration 354/1000 | Loss: 0.00002213
Iteration 355/1000 | Loss: 0.00002213
Iteration 356/1000 | Loss: 0.00002213
Iteration 357/1000 | Loss: 0.00002213
Iteration 358/1000 | Loss: 0.00002213
Iteration 359/1000 | Loss: 0.00002212
Iteration 360/1000 | Loss: 0.00002212
Iteration 361/1000 | Loss: 0.00002212
Iteration 362/1000 | Loss: 0.00002211
Iteration 363/1000 | Loss: 0.00002211
Iteration 364/1000 | Loss: 0.00002211
Iteration 365/1000 | Loss: 0.00002211
Iteration 366/1000 | Loss: 0.00002211
Iteration 367/1000 | Loss: 0.00002211
Iteration 368/1000 | Loss: 0.00002211
Iteration 369/1000 | Loss: 0.00002211
Iteration 370/1000 | Loss: 0.00002211
Iteration 371/1000 | Loss: 0.00002211
Iteration 372/1000 | Loss: 0.00002211
Iteration 373/1000 | Loss: 0.00002210
Iteration 374/1000 | Loss: 0.00002210
Iteration 375/1000 | Loss: 0.00002210
Iteration 376/1000 | Loss: 0.00002210
Iteration 377/1000 | Loss: 0.00002210
Iteration 378/1000 | Loss: 0.00002210
Iteration 379/1000 | Loss: 0.00002210
Iteration 380/1000 | Loss: 0.00002210
Iteration 381/1000 | Loss: 0.00002210
Iteration 382/1000 | Loss: 0.00002210
Iteration 383/1000 | Loss: 0.00002210
Iteration 384/1000 | Loss: 0.00002210
Iteration 385/1000 | Loss: 0.00002210
Iteration 386/1000 | Loss: 0.00002210
Iteration 387/1000 | Loss: 0.00002210
Iteration 388/1000 | Loss: 0.00002210
Iteration 389/1000 | Loss: 0.00002210
Iteration 390/1000 | Loss: 0.00002210
Iteration 391/1000 | Loss: 0.00002210
Iteration 392/1000 | Loss: 0.00002210
Iteration 393/1000 | Loss: 0.00002210
Iteration 394/1000 | Loss: 0.00002210
Iteration 395/1000 | Loss: 0.00002210
Iteration 396/1000 | Loss: 0.00002210
Iteration 397/1000 | Loss: 0.00002210
Iteration 398/1000 | Loss: 0.00002210
Iteration 399/1000 | Loss: 0.00002210
Iteration 400/1000 | Loss: 0.00002210
Iteration 401/1000 | Loss: 0.00002210
Iteration 402/1000 | Loss: 0.00002210
Iteration 403/1000 | Loss: 0.00002210
Iteration 404/1000 | Loss: 0.00002210
Iteration 405/1000 | Loss: 0.00002210
Iteration 406/1000 | Loss: 0.00002210
Iteration 407/1000 | Loss: 0.00002210
Iteration 408/1000 | Loss: 0.00002210
Iteration 409/1000 | Loss: 0.00002210
Iteration 410/1000 | Loss: 0.00002210
Iteration 411/1000 | Loss: 0.00002210
Iteration 412/1000 | Loss: 0.00002210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 412. Stopping optimization.
Last 5 losses: [2.209672129538376e-05, 2.209672129538376e-05, 2.209672129538376e-05, 2.209672129538376e-05, 2.209672129538376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.209672129538376e-05

Optimization complete. Final v2v error: 3.7307183742523193 mm

Highest mean error: 14.097107887268066 mm for frame 82

Lowest mean error: 3.110426187515259 mm for frame 193

Saving results

Total time: 466.53830003738403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055230
Iteration 2/25 | Loss: 0.00311927
Iteration 3/25 | Loss: 0.00176297
Iteration 4/25 | Loss: 0.00154435
Iteration 5/25 | Loss: 0.00128198
Iteration 6/25 | Loss: 0.00141279
Iteration 7/25 | Loss: 0.00120888
Iteration 8/25 | Loss: 0.00106649
Iteration 9/25 | Loss: 0.00102582
Iteration 10/25 | Loss: 0.00102126
Iteration 11/25 | Loss: 0.00098726
Iteration 12/25 | Loss: 0.00095127
Iteration 13/25 | Loss: 0.00094415
Iteration 14/25 | Loss: 0.00091588
Iteration 15/25 | Loss: 0.00089705
Iteration 16/25 | Loss: 0.00089576
Iteration 17/25 | Loss: 0.00087555
Iteration 18/25 | Loss: 0.00087464
Iteration 19/25 | Loss: 0.00087657
Iteration 20/25 | Loss: 0.00087082
Iteration 21/25 | Loss: 0.00087010
Iteration 22/25 | Loss: 0.00087567
Iteration 23/25 | Loss: 0.00086847
Iteration 24/25 | Loss: 0.00086517
Iteration 25/25 | Loss: 0.00086499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42533314
Iteration 2/25 | Loss: 0.00423071
Iteration 3/25 | Loss: 0.00163072
Iteration 4/25 | Loss: 0.00163062
Iteration 5/25 | Loss: 0.00163062
Iteration 6/25 | Loss: 0.00163062
Iteration 7/25 | Loss: 0.00163061
Iteration 8/25 | Loss: 0.00163061
Iteration 9/25 | Loss: 0.00163061
Iteration 10/25 | Loss: 0.00163061
Iteration 11/25 | Loss: 0.00163061
Iteration 12/25 | Loss: 0.00163061
Iteration 13/25 | Loss: 0.00163061
Iteration 14/25 | Loss: 0.00163061
Iteration 15/25 | Loss: 0.00163061
Iteration 16/25 | Loss: 0.00163061
Iteration 17/25 | Loss: 0.00163061
Iteration 18/25 | Loss: 0.00163061
Iteration 19/25 | Loss: 0.00163061
Iteration 20/25 | Loss: 0.00163061
Iteration 21/25 | Loss: 0.00163061
Iteration 22/25 | Loss: 0.00163061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016306131146848202, 0.0016306131146848202, 0.0016306131146848202, 0.0016306131146848202, 0.0016306131146848202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016306131146848202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163061
Iteration 2/1000 | Loss: 0.00058495
Iteration 3/1000 | Loss: 0.00047497
Iteration 4/1000 | Loss: 0.00055517
Iteration 5/1000 | Loss: 0.00047845
Iteration 6/1000 | Loss: 0.00081918
Iteration 7/1000 | Loss: 0.00049995
Iteration 8/1000 | Loss: 0.00063106
Iteration 9/1000 | Loss: 0.00043453
Iteration 10/1000 | Loss: 0.00042376
Iteration 11/1000 | Loss: 0.00072748
Iteration 12/1000 | Loss: 0.00283502
Iteration 13/1000 | Loss: 0.00142454
Iteration 14/1000 | Loss: 0.00259108
Iteration 15/1000 | Loss: 0.00122955
Iteration 16/1000 | Loss: 0.00035188
Iteration 17/1000 | Loss: 0.00289992
Iteration 18/1000 | Loss: 0.00138364
Iteration 19/1000 | Loss: 0.00234286
Iteration 20/1000 | Loss: 0.00278448
Iteration 21/1000 | Loss: 0.00042449
Iteration 22/1000 | Loss: 0.00039302
Iteration 23/1000 | Loss: 0.00269280
Iteration 24/1000 | Loss: 0.00045734
Iteration 25/1000 | Loss: 0.00202527
Iteration 26/1000 | Loss: 0.00119082
Iteration 27/1000 | Loss: 0.00038744
Iteration 28/1000 | Loss: 0.00032498
Iteration 29/1000 | Loss: 0.00017950
Iteration 30/1000 | Loss: 0.00095194
Iteration 31/1000 | Loss: 0.00281471
Iteration 32/1000 | Loss: 0.00045174
Iteration 33/1000 | Loss: 0.00010431
Iteration 34/1000 | Loss: 0.00054118
Iteration 35/1000 | Loss: 0.00272249
Iteration 36/1000 | Loss: 0.00039675
Iteration 37/1000 | Loss: 0.00008189
Iteration 38/1000 | Loss: 0.00028388
Iteration 39/1000 | Loss: 0.00118244
Iteration 40/1000 | Loss: 0.00054022
Iteration 41/1000 | Loss: 0.00061891
Iteration 42/1000 | Loss: 0.00026180
Iteration 43/1000 | Loss: 0.00019685
Iteration 44/1000 | Loss: 0.00062242
Iteration 45/1000 | Loss: 0.00209202
Iteration 46/1000 | Loss: 0.00013509
Iteration 47/1000 | Loss: 0.00006982
Iteration 48/1000 | Loss: 0.00052094
Iteration 49/1000 | Loss: 0.00032153
Iteration 50/1000 | Loss: 0.00010985
Iteration 51/1000 | Loss: 0.00012157
Iteration 52/1000 | Loss: 0.00039169
Iteration 53/1000 | Loss: 0.00021573
Iteration 54/1000 | Loss: 0.00017918
Iteration 55/1000 | Loss: 0.00038796
Iteration 56/1000 | Loss: 0.00020518
Iteration 57/1000 | Loss: 0.00091914
Iteration 58/1000 | Loss: 0.00028454
Iteration 59/1000 | Loss: 0.00092517
Iteration 60/1000 | Loss: 0.00010213
Iteration 61/1000 | Loss: 0.00030287
Iteration 62/1000 | Loss: 0.00004926
Iteration 63/1000 | Loss: 0.00046586
Iteration 64/1000 | Loss: 0.00004039
Iteration 65/1000 | Loss: 0.00003784
Iteration 66/1000 | Loss: 0.00025645
Iteration 67/1000 | Loss: 0.00004251
Iteration 68/1000 | Loss: 0.00003392
Iteration 69/1000 | Loss: 0.00002981
Iteration 70/1000 | Loss: 0.00002740
Iteration 71/1000 | Loss: 0.00083186
Iteration 72/1000 | Loss: 0.00049596
Iteration 73/1000 | Loss: 0.00062719
Iteration 74/1000 | Loss: 0.00005118
Iteration 75/1000 | Loss: 0.00006964
Iteration 76/1000 | Loss: 0.00002317
Iteration 77/1000 | Loss: 0.00004752
Iteration 78/1000 | Loss: 0.00003062
Iteration 79/1000 | Loss: 0.00002139
Iteration 80/1000 | Loss: 0.00002590
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002010
Iteration 83/1000 | Loss: 0.00001987
Iteration 84/1000 | Loss: 0.00016589
Iteration 85/1000 | Loss: 0.00011938
Iteration 86/1000 | Loss: 0.00017037
Iteration 87/1000 | Loss: 0.00010203
Iteration 88/1000 | Loss: 0.00002883
Iteration 89/1000 | Loss: 0.00002249
Iteration 90/1000 | Loss: 0.00008546
Iteration 91/1000 | Loss: 0.00002506
Iteration 92/1000 | Loss: 0.00002104
Iteration 93/1000 | Loss: 0.00002102
Iteration 94/1000 | Loss: 0.00016968
Iteration 95/1000 | Loss: 0.00002736
Iteration 96/1000 | Loss: 0.00041108
Iteration 97/1000 | Loss: 0.00040932
Iteration 98/1000 | Loss: 0.00040794
Iteration 99/1000 | Loss: 0.00046158
Iteration 100/1000 | Loss: 0.00012515
Iteration 101/1000 | Loss: 0.00036912
Iteration 102/1000 | Loss: 0.00015645
Iteration 103/1000 | Loss: 0.00004285
Iteration 104/1000 | Loss: 0.00010439
Iteration 105/1000 | Loss: 0.00002587
Iteration 106/1000 | Loss: 0.00002310
Iteration 107/1000 | Loss: 0.00002529
Iteration 108/1000 | Loss: 0.00005750
Iteration 109/1000 | Loss: 0.00002068
Iteration 110/1000 | Loss: 0.00002262
Iteration 111/1000 | Loss: 0.00002733
Iteration 112/1000 | Loss: 0.00002690
Iteration 113/1000 | Loss: 0.00002616
Iteration 114/1000 | Loss: 0.00001965
Iteration 115/1000 | Loss: 0.00001965
Iteration 116/1000 | Loss: 0.00002078
Iteration 117/1000 | Loss: 0.00001879
Iteration 118/1000 | Loss: 0.00001879
Iteration 119/1000 | Loss: 0.00001875
Iteration 120/1000 | Loss: 0.00001874
Iteration 121/1000 | Loss: 0.00001874
Iteration 122/1000 | Loss: 0.00001873
Iteration 123/1000 | Loss: 0.00001873
Iteration 124/1000 | Loss: 0.00001872
Iteration 125/1000 | Loss: 0.00001872
Iteration 126/1000 | Loss: 0.00001871
Iteration 127/1000 | Loss: 0.00001871
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Iteration 130/1000 | Loss: 0.00001869
Iteration 131/1000 | Loss: 0.00001869
Iteration 132/1000 | Loss: 0.00001868
Iteration 133/1000 | Loss: 0.00001868
Iteration 134/1000 | Loss: 0.00001867
Iteration 135/1000 | Loss: 0.00001867
Iteration 136/1000 | Loss: 0.00001866
Iteration 137/1000 | Loss: 0.00001865
Iteration 138/1000 | Loss: 0.00001865
Iteration 139/1000 | Loss: 0.00001864
Iteration 140/1000 | Loss: 0.00001864
Iteration 141/1000 | Loss: 0.00007536
Iteration 142/1000 | Loss: 0.00002153
Iteration 143/1000 | Loss: 0.00001936
Iteration 144/1000 | Loss: 0.00001856
Iteration 145/1000 | Loss: 0.00001855
Iteration 146/1000 | Loss: 0.00001855
Iteration 147/1000 | Loss: 0.00001855
Iteration 148/1000 | Loss: 0.00001855
Iteration 149/1000 | Loss: 0.00001855
Iteration 150/1000 | Loss: 0.00001854
Iteration 151/1000 | Loss: 0.00001854
Iteration 152/1000 | Loss: 0.00001854
Iteration 153/1000 | Loss: 0.00001854
Iteration 154/1000 | Loss: 0.00001853
Iteration 155/1000 | Loss: 0.00001853
Iteration 156/1000 | Loss: 0.00001853
Iteration 157/1000 | Loss: 0.00001853
Iteration 158/1000 | Loss: 0.00001853
Iteration 159/1000 | Loss: 0.00001853
Iteration 160/1000 | Loss: 0.00001852
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001850
Iteration 165/1000 | Loss: 0.00001850
Iteration 166/1000 | Loss: 0.00001850
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001849
Iteration 169/1000 | Loss: 0.00001849
Iteration 170/1000 | Loss: 0.00001848
Iteration 171/1000 | Loss: 0.00001909
Iteration 172/1000 | Loss: 0.00001909
Iteration 173/1000 | Loss: 0.00010451
Iteration 174/1000 | Loss: 0.00001901
Iteration 175/1000 | Loss: 0.00006546
Iteration 176/1000 | Loss: 0.00002090
Iteration 177/1000 | Loss: 0.00001842
Iteration 178/1000 | Loss: 0.00001842
Iteration 179/1000 | Loss: 0.00001842
Iteration 180/1000 | Loss: 0.00001842
Iteration 181/1000 | Loss: 0.00001842
Iteration 182/1000 | Loss: 0.00001842
Iteration 183/1000 | Loss: 0.00001841
Iteration 184/1000 | Loss: 0.00001841
Iteration 185/1000 | Loss: 0.00001841
Iteration 186/1000 | Loss: 0.00001841
Iteration 187/1000 | Loss: 0.00001841
Iteration 188/1000 | Loss: 0.00001841
Iteration 189/1000 | Loss: 0.00001841
Iteration 190/1000 | Loss: 0.00002040
Iteration 191/1000 | Loss: 0.00002242
Iteration 192/1000 | Loss: 0.00001841
Iteration 193/1000 | Loss: 0.00001841
Iteration 194/1000 | Loss: 0.00001841
Iteration 195/1000 | Loss: 0.00001840
Iteration 196/1000 | Loss: 0.00001840
Iteration 197/1000 | Loss: 0.00001840
Iteration 198/1000 | Loss: 0.00001840
Iteration 199/1000 | Loss: 0.00001840
Iteration 200/1000 | Loss: 0.00001840
Iteration 201/1000 | Loss: 0.00001840
Iteration 202/1000 | Loss: 0.00001840
Iteration 203/1000 | Loss: 0.00001840
Iteration 204/1000 | Loss: 0.00001840
Iteration 205/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.8402486603008583e-05, 1.8402486603008583e-05, 1.8402486603008583e-05, 1.8402486603008583e-05, 1.8402486603008583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8402486603008583e-05

Optimization complete. Final v2v error: 3.485276699066162 mm

Highest mean error: 11.962520599365234 mm for frame 222

Lowest mean error: 2.7921252250671387 mm for frame 63

Saving results

Total time: 271.84243273735046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452097
Iteration 2/25 | Loss: 0.00075025
Iteration 3/25 | Loss: 0.00062621
Iteration 4/25 | Loss: 0.00060660
Iteration 5/25 | Loss: 0.00060173
Iteration 6/25 | Loss: 0.00059991
Iteration 7/25 | Loss: 0.00059933
Iteration 8/25 | Loss: 0.00059930
Iteration 9/25 | Loss: 0.00059930
Iteration 10/25 | Loss: 0.00059930
Iteration 11/25 | Loss: 0.00059930
Iteration 12/25 | Loss: 0.00059930
Iteration 13/25 | Loss: 0.00059930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005992965889163315, 0.0005992965889163315, 0.0005992965889163315, 0.0005992965889163315, 0.0005992965889163315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005992965889163315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46133769
Iteration 2/25 | Loss: 0.00021120
Iteration 3/25 | Loss: 0.00021119
Iteration 4/25 | Loss: 0.00021119
Iteration 5/25 | Loss: 0.00021119
Iteration 6/25 | Loss: 0.00021119
Iteration 7/25 | Loss: 0.00021119
Iteration 8/25 | Loss: 0.00021119
Iteration 9/25 | Loss: 0.00021119
Iteration 10/25 | Loss: 0.00021119
Iteration 11/25 | Loss: 0.00021119
Iteration 12/25 | Loss: 0.00021118
Iteration 13/25 | Loss: 0.00021118
Iteration 14/25 | Loss: 0.00021118
Iteration 15/25 | Loss: 0.00021118
Iteration 16/25 | Loss: 0.00021118
Iteration 17/25 | Loss: 0.00021118
Iteration 18/25 | Loss: 0.00021118
Iteration 19/25 | Loss: 0.00021118
Iteration 20/25 | Loss: 0.00021118
Iteration 21/25 | Loss: 0.00021118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000211184800718911, 0.000211184800718911, 0.000211184800718911, 0.000211184800718911, 0.000211184800718911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000211184800718911

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021118
Iteration 2/1000 | Loss: 0.00002736
Iteration 3/1000 | Loss: 0.00001641
Iteration 4/1000 | Loss: 0.00001343
Iteration 5/1000 | Loss: 0.00001262
Iteration 6/1000 | Loss: 0.00001221
Iteration 7/1000 | Loss: 0.00001191
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001139
Iteration 18/1000 | Loss: 0.00001138
Iteration 19/1000 | Loss: 0.00001138
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001136
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001136
Iteration 28/1000 | Loss: 0.00001136
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001135
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001135
Iteration 33/1000 | Loss: 0.00001135
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001134
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001133
Iteration 39/1000 | Loss: 0.00001132
Iteration 40/1000 | Loss: 0.00001132
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001127
Iteration 52/1000 | Loss: 0.00001127
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001125
Iteration 56/1000 | Loss: 0.00001125
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001124
Iteration 61/1000 | Loss: 0.00001124
Iteration 62/1000 | Loss: 0.00001124
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001123
Iteration 65/1000 | Loss: 0.00001123
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001120
Iteration 76/1000 | Loss: 0.00001120
Iteration 77/1000 | Loss: 0.00001120
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001117
Iteration 85/1000 | Loss: 0.00001117
Iteration 86/1000 | Loss: 0.00001117
Iteration 87/1000 | Loss: 0.00001117
Iteration 88/1000 | Loss: 0.00001116
Iteration 89/1000 | Loss: 0.00001116
Iteration 90/1000 | Loss: 0.00001116
Iteration 91/1000 | Loss: 0.00001115
Iteration 92/1000 | Loss: 0.00001115
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001114
Iteration 97/1000 | Loss: 0.00001114
Iteration 98/1000 | Loss: 0.00001114
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001113
Iteration 101/1000 | Loss: 0.00001113
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001113
Iteration 105/1000 | Loss: 0.00001113
Iteration 106/1000 | Loss: 0.00001113
Iteration 107/1000 | Loss: 0.00001113
Iteration 108/1000 | Loss: 0.00001113
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001112
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001111
Iteration 114/1000 | Loss: 0.00001111
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001108
Iteration 130/1000 | Loss: 0.00001108
Iteration 131/1000 | Loss: 0.00001108
Iteration 132/1000 | Loss: 0.00001108
Iteration 133/1000 | Loss: 0.00001108
Iteration 134/1000 | Loss: 0.00001108
Iteration 135/1000 | Loss: 0.00001108
Iteration 136/1000 | Loss: 0.00001108
Iteration 137/1000 | Loss: 0.00001108
Iteration 138/1000 | Loss: 0.00001108
Iteration 139/1000 | Loss: 0.00001108
Iteration 140/1000 | Loss: 0.00001108
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001106
Iteration 157/1000 | Loss: 0.00001106
Iteration 158/1000 | Loss: 0.00001106
Iteration 159/1000 | Loss: 0.00001106
Iteration 160/1000 | Loss: 0.00001106
Iteration 161/1000 | Loss: 0.00001106
Iteration 162/1000 | Loss: 0.00001106
Iteration 163/1000 | Loss: 0.00001106
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001105
Iteration 166/1000 | Loss: 0.00001105
Iteration 167/1000 | Loss: 0.00001105
Iteration 168/1000 | Loss: 0.00001105
Iteration 169/1000 | Loss: 0.00001105
Iteration 170/1000 | Loss: 0.00001105
Iteration 171/1000 | Loss: 0.00001105
Iteration 172/1000 | Loss: 0.00001105
Iteration 173/1000 | Loss: 0.00001105
Iteration 174/1000 | Loss: 0.00001105
Iteration 175/1000 | Loss: 0.00001105
Iteration 176/1000 | Loss: 0.00001105
Iteration 177/1000 | Loss: 0.00001105
Iteration 178/1000 | Loss: 0.00001105
Iteration 179/1000 | Loss: 0.00001105
Iteration 180/1000 | Loss: 0.00001104
Iteration 181/1000 | Loss: 0.00001104
Iteration 182/1000 | Loss: 0.00001104
Iteration 183/1000 | Loss: 0.00001104
Iteration 184/1000 | Loss: 0.00001104
Iteration 185/1000 | Loss: 0.00001104
Iteration 186/1000 | Loss: 0.00001104
Iteration 187/1000 | Loss: 0.00001104
Iteration 188/1000 | Loss: 0.00001103
Iteration 189/1000 | Loss: 0.00001103
Iteration 190/1000 | Loss: 0.00001103
Iteration 191/1000 | Loss: 0.00001103
Iteration 192/1000 | Loss: 0.00001103
Iteration 193/1000 | Loss: 0.00001103
Iteration 194/1000 | Loss: 0.00001103
Iteration 195/1000 | Loss: 0.00001103
Iteration 196/1000 | Loss: 0.00001103
Iteration 197/1000 | Loss: 0.00001103
Iteration 198/1000 | Loss: 0.00001103
Iteration 199/1000 | Loss: 0.00001103
Iteration 200/1000 | Loss: 0.00001103
Iteration 201/1000 | Loss: 0.00001103
Iteration 202/1000 | Loss: 0.00001103
Iteration 203/1000 | Loss: 0.00001103
Iteration 204/1000 | Loss: 0.00001103
Iteration 205/1000 | Loss: 0.00001103
Iteration 206/1000 | Loss: 0.00001103
Iteration 207/1000 | Loss: 0.00001103
Iteration 208/1000 | Loss: 0.00001103
Iteration 209/1000 | Loss: 0.00001103
Iteration 210/1000 | Loss: 0.00001103
Iteration 211/1000 | Loss: 0.00001103
Iteration 212/1000 | Loss: 0.00001103
Iteration 213/1000 | Loss: 0.00001103
Iteration 214/1000 | Loss: 0.00001103
Iteration 215/1000 | Loss: 0.00001103
Iteration 216/1000 | Loss: 0.00001103
Iteration 217/1000 | Loss: 0.00001103
Iteration 218/1000 | Loss: 0.00001103
Iteration 219/1000 | Loss: 0.00001103
Iteration 220/1000 | Loss: 0.00001103
Iteration 221/1000 | Loss: 0.00001103
Iteration 222/1000 | Loss: 0.00001103
Iteration 223/1000 | Loss: 0.00001103
Iteration 224/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.1028350854758173e-05, 1.1028350854758173e-05, 1.1028350854758173e-05, 1.1028350854758173e-05, 1.1028350854758173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1028350854758173e-05

Optimization complete. Final v2v error: 2.709597587585449 mm

Highest mean error: 3.330193042755127 mm for frame 62

Lowest mean error: 2.4455695152282715 mm for frame 20

Saving results

Total time: 64.03425002098083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424694
Iteration 2/25 | Loss: 0.00090153
Iteration 3/25 | Loss: 0.00075339
Iteration 4/25 | Loss: 0.00071640
Iteration 5/25 | Loss: 0.00070939
Iteration 6/25 | Loss: 0.00070810
Iteration 7/25 | Loss: 0.00070766
Iteration 8/25 | Loss: 0.00070764
Iteration 9/25 | Loss: 0.00070764
Iteration 10/25 | Loss: 0.00070764
Iteration 11/25 | Loss: 0.00070764
Iteration 12/25 | Loss: 0.00070764
Iteration 13/25 | Loss: 0.00070764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007076364709064364, 0.0007076364709064364, 0.0007076364709064364, 0.0007076364709064364, 0.0007076364709064364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007076364709064364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99369299
Iteration 2/25 | Loss: 0.00029496
Iteration 3/25 | Loss: 0.00029496
Iteration 4/25 | Loss: 0.00029495
Iteration 5/25 | Loss: 0.00029495
Iteration 6/25 | Loss: 0.00029495
Iteration 7/25 | Loss: 0.00029495
Iteration 8/25 | Loss: 0.00029495
Iteration 9/25 | Loss: 0.00029495
Iteration 10/25 | Loss: 0.00029495
Iteration 11/25 | Loss: 0.00029495
Iteration 12/25 | Loss: 0.00029495
Iteration 13/25 | Loss: 0.00029495
Iteration 14/25 | Loss: 0.00029495
Iteration 15/25 | Loss: 0.00029495
Iteration 16/25 | Loss: 0.00029495
Iteration 17/25 | Loss: 0.00029495
Iteration 18/25 | Loss: 0.00029495
Iteration 19/25 | Loss: 0.00029495
Iteration 20/25 | Loss: 0.00029495
Iteration 21/25 | Loss: 0.00029495
Iteration 22/25 | Loss: 0.00029495
Iteration 23/25 | Loss: 0.00029495
Iteration 24/25 | Loss: 0.00029495
Iteration 25/25 | Loss: 0.00029495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029495
Iteration 2/1000 | Loss: 0.00005131
Iteration 3/1000 | Loss: 0.00003534
Iteration 4/1000 | Loss: 0.00003271
Iteration 5/1000 | Loss: 0.00003095
Iteration 6/1000 | Loss: 0.00002970
Iteration 7/1000 | Loss: 0.00002887
Iteration 8/1000 | Loss: 0.00002821
Iteration 9/1000 | Loss: 0.00002789
Iteration 10/1000 | Loss: 0.00002764
Iteration 11/1000 | Loss: 0.00002762
Iteration 12/1000 | Loss: 0.00002741
Iteration 13/1000 | Loss: 0.00002740
Iteration 14/1000 | Loss: 0.00002732
Iteration 15/1000 | Loss: 0.00002726
Iteration 16/1000 | Loss: 0.00002718
Iteration 17/1000 | Loss: 0.00002717
Iteration 18/1000 | Loss: 0.00002717
Iteration 19/1000 | Loss: 0.00002716
Iteration 20/1000 | Loss: 0.00002716
Iteration 21/1000 | Loss: 0.00002715
Iteration 22/1000 | Loss: 0.00002714
Iteration 23/1000 | Loss: 0.00002713
Iteration 24/1000 | Loss: 0.00002713
Iteration 25/1000 | Loss: 0.00002712
Iteration 26/1000 | Loss: 0.00002712
Iteration 27/1000 | Loss: 0.00002712
Iteration 28/1000 | Loss: 0.00002710
Iteration 29/1000 | Loss: 0.00002707
Iteration 30/1000 | Loss: 0.00002707
Iteration 31/1000 | Loss: 0.00002706
Iteration 32/1000 | Loss: 0.00002705
Iteration 33/1000 | Loss: 0.00002703
Iteration 34/1000 | Loss: 0.00002702
Iteration 35/1000 | Loss: 0.00002702
Iteration 36/1000 | Loss: 0.00002702
Iteration 37/1000 | Loss: 0.00002701
Iteration 38/1000 | Loss: 0.00002700
Iteration 39/1000 | Loss: 0.00002700
Iteration 40/1000 | Loss: 0.00002700
Iteration 41/1000 | Loss: 0.00002697
Iteration 42/1000 | Loss: 0.00002692
Iteration 43/1000 | Loss: 0.00002692
Iteration 44/1000 | Loss: 0.00002692
Iteration 45/1000 | Loss: 0.00002692
Iteration 46/1000 | Loss: 0.00002692
Iteration 47/1000 | Loss: 0.00002692
Iteration 48/1000 | Loss: 0.00002692
Iteration 49/1000 | Loss: 0.00002692
Iteration 50/1000 | Loss: 0.00002692
Iteration 51/1000 | Loss: 0.00002692
Iteration 52/1000 | Loss: 0.00002692
Iteration 53/1000 | Loss: 0.00002692
Iteration 54/1000 | Loss: 0.00002692
Iteration 55/1000 | Loss: 0.00002692
Iteration 56/1000 | Loss: 0.00002692
Iteration 57/1000 | Loss: 0.00002692
Iteration 58/1000 | Loss: 0.00002692
Iteration 59/1000 | Loss: 0.00002692
Iteration 60/1000 | Loss: 0.00002692
Iteration 61/1000 | Loss: 0.00002692
Iteration 62/1000 | Loss: 0.00002692
Iteration 63/1000 | Loss: 0.00002692
Iteration 64/1000 | Loss: 0.00002692
Iteration 65/1000 | Loss: 0.00002692
Iteration 66/1000 | Loss: 0.00002692
Iteration 67/1000 | Loss: 0.00002692
Iteration 68/1000 | Loss: 0.00002692
Iteration 69/1000 | Loss: 0.00002692
Iteration 70/1000 | Loss: 0.00002692
Iteration 71/1000 | Loss: 0.00002692
Iteration 72/1000 | Loss: 0.00002692
Iteration 73/1000 | Loss: 0.00002692
Iteration 74/1000 | Loss: 0.00002692
Iteration 75/1000 | Loss: 0.00002692
Iteration 76/1000 | Loss: 0.00002692
Iteration 77/1000 | Loss: 0.00002692
Iteration 78/1000 | Loss: 0.00002692
Iteration 79/1000 | Loss: 0.00002692
Iteration 80/1000 | Loss: 0.00002692
Iteration 81/1000 | Loss: 0.00002692
Iteration 82/1000 | Loss: 0.00002692
Iteration 83/1000 | Loss: 0.00002692
Iteration 84/1000 | Loss: 0.00002692
Iteration 85/1000 | Loss: 0.00002692
Iteration 86/1000 | Loss: 0.00002692
Iteration 87/1000 | Loss: 0.00002692
Iteration 88/1000 | Loss: 0.00002692
Iteration 89/1000 | Loss: 0.00002692
Iteration 90/1000 | Loss: 0.00002692
Iteration 91/1000 | Loss: 0.00002692
Iteration 92/1000 | Loss: 0.00002692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.6915682610706426e-05, 2.6915682610706426e-05, 2.6915682610706426e-05, 2.6915682610706426e-05, 2.6915682610706426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6915682610706426e-05

Optimization complete. Final v2v error: 4.319991111755371 mm

Highest mean error: 4.680818557739258 mm for frame 104

Lowest mean error: 3.783834218978882 mm for frame 42

Saving results

Total time: 63.23941445350647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057643
Iteration 2/25 | Loss: 0.00204156
Iteration 3/25 | Loss: 0.00162286
Iteration 4/25 | Loss: 0.00142838
Iteration 5/25 | Loss: 0.00131646
Iteration 6/25 | Loss: 0.00101199
Iteration 7/25 | Loss: 0.00085252
Iteration 8/25 | Loss: 0.00082058
Iteration 9/25 | Loss: 0.00081539
Iteration 10/25 | Loss: 0.00081408
Iteration 11/25 | Loss: 0.00081388
Iteration 12/25 | Loss: 0.00081379
Iteration 13/25 | Loss: 0.00081355
Iteration 14/25 | Loss: 0.00081525
Iteration 15/25 | Loss: 0.00081345
Iteration 16/25 | Loss: 0.00081231
Iteration 17/25 | Loss: 0.00081198
Iteration 18/25 | Loss: 0.00081176
Iteration 19/25 | Loss: 0.00081160
Iteration 20/25 | Loss: 0.00081160
Iteration 21/25 | Loss: 0.00081160
Iteration 22/25 | Loss: 0.00081160
Iteration 23/25 | Loss: 0.00081159
Iteration 24/25 | Loss: 0.00081159
Iteration 25/25 | Loss: 0.00081159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44767773
Iteration 2/25 | Loss: 0.00024561
Iteration 3/25 | Loss: 0.00024560
Iteration 4/25 | Loss: 0.00024560
Iteration 5/25 | Loss: 0.00024560
Iteration 6/25 | Loss: 0.00024560
Iteration 7/25 | Loss: 0.00024560
Iteration 8/25 | Loss: 0.00024560
Iteration 9/25 | Loss: 0.00024560
Iteration 10/25 | Loss: 0.00024560
Iteration 11/25 | Loss: 0.00024560
Iteration 12/25 | Loss: 0.00024560
Iteration 13/25 | Loss: 0.00024560
Iteration 14/25 | Loss: 0.00024560
Iteration 15/25 | Loss: 0.00024560
Iteration 16/25 | Loss: 0.00024560
Iteration 17/25 | Loss: 0.00024560
Iteration 18/25 | Loss: 0.00024560
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002456007932778448, 0.0002456007932778448, 0.0002456007932778448, 0.0002456007932778448, 0.0002456007932778448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002456007932778448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024560
Iteration 2/1000 | Loss: 0.00003345
Iteration 3/1000 | Loss: 0.00002752
Iteration 4/1000 | Loss: 0.00002614
Iteration 5/1000 | Loss: 0.00002513
Iteration 6/1000 | Loss: 0.00002475
Iteration 7/1000 | Loss: 0.00002435
Iteration 8/1000 | Loss: 0.00002412
Iteration 9/1000 | Loss: 0.00002401
Iteration 10/1000 | Loss: 0.00002400
Iteration 11/1000 | Loss: 0.00002400
Iteration 12/1000 | Loss: 0.00002397
Iteration 13/1000 | Loss: 0.00002396
Iteration 14/1000 | Loss: 0.00002396
Iteration 15/1000 | Loss: 0.00002395
Iteration 16/1000 | Loss: 0.00002395
Iteration 17/1000 | Loss: 0.00002395
Iteration 18/1000 | Loss: 0.00002394
Iteration 19/1000 | Loss: 0.00002394
Iteration 20/1000 | Loss: 0.00002394
Iteration 21/1000 | Loss: 0.00002393
Iteration 22/1000 | Loss: 0.00002393
Iteration 23/1000 | Loss: 0.00002392
Iteration 24/1000 | Loss: 0.00002392
Iteration 25/1000 | Loss: 0.00002392
Iteration 26/1000 | Loss: 0.00002392
Iteration 27/1000 | Loss: 0.00002391
Iteration 28/1000 | Loss: 0.00002391
Iteration 29/1000 | Loss: 0.00002390
Iteration 30/1000 | Loss: 0.00002390
Iteration 31/1000 | Loss: 0.00002390
Iteration 32/1000 | Loss: 0.00002390
Iteration 33/1000 | Loss: 0.00002389
Iteration 34/1000 | Loss: 0.00002389
Iteration 35/1000 | Loss: 0.00002389
Iteration 36/1000 | Loss: 0.00002388
Iteration 37/1000 | Loss: 0.00002388
Iteration 38/1000 | Loss: 0.00002387
Iteration 39/1000 | Loss: 0.00002387
Iteration 40/1000 | Loss: 0.00002387
Iteration 41/1000 | Loss: 0.00002387
Iteration 42/1000 | Loss: 0.00002387
Iteration 43/1000 | Loss: 0.00002387
Iteration 44/1000 | Loss: 0.00002387
Iteration 45/1000 | Loss: 0.00002387
Iteration 46/1000 | Loss: 0.00002387
Iteration 47/1000 | Loss: 0.00002387
Iteration 48/1000 | Loss: 0.00002387
Iteration 49/1000 | Loss: 0.00002386
Iteration 50/1000 | Loss: 0.00002386
Iteration 51/1000 | Loss: 0.00002386
Iteration 52/1000 | Loss: 0.00002386
Iteration 53/1000 | Loss: 0.00002386
Iteration 54/1000 | Loss: 0.00002385
Iteration 55/1000 | Loss: 0.00002385
Iteration 56/1000 | Loss: 0.00002385
Iteration 57/1000 | Loss: 0.00002385
Iteration 58/1000 | Loss: 0.00002385
Iteration 59/1000 | Loss: 0.00002385
Iteration 60/1000 | Loss: 0.00002385
Iteration 61/1000 | Loss: 0.00002385
Iteration 62/1000 | Loss: 0.00002384
Iteration 63/1000 | Loss: 0.00002384
Iteration 64/1000 | Loss: 0.00002384
Iteration 65/1000 | Loss: 0.00002384
Iteration 66/1000 | Loss: 0.00002384
Iteration 67/1000 | Loss: 0.00002384
Iteration 68/1000 | Loss: 0.00002384
Iteration 69/1000 | Loss: 0.00002384
Iteration 70/1000 | Loss: 0.00002384
Iteration 71/1000 | Loss: 0.00002384
Iteration 72/1000 | Loss: 0.00002384
Iteration 73/1000 | Loss: 0.00002384
Iteration 74/1000 | Loss: 0.00002384
Iteration 75/1000 | Loss: 0.00002384
Iteration 76/1000 | Loss: 0.00002384
Iteration 77/1000 | Loss: 0.00002384
Iteration 78/1000 | Loss: 0.00002384
Iteration 79/1000 | Loss: 0.00002384
Iteration 80/1000 | Loss: 0.00002384
Iteration 81/1000 | Loss: 0.00002384
Iteration 82/1000 | Loss: 0.00002384
Iteration 83/1000 | Loss: 0.00002384
Iteration 84/1000 | Loss: 0.00002384
Iteration 85/1000 | Loss: 0.00002384
Iteration 86/1000 | Loss: 0.00002384
Iteration 87/1000 | Loss: 0.00002384
Iteration 88/1000 | Loss: 0.00002384
Iteration 89/1000 | Loss: 0.00002384
Iteration 90/1000 | Loss: 0.00002384
Iteration 91/1000 | Loss: 0.00002384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.3842236259952188e-05, 2.3842236259952188e-05, 2.3842236259952188e-05, 2.3842236259952188e-05, 2.3842236259952188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3842236259952188e-05

Optimization complete. Final v2v error: 4.106362342834473 mm

Highest mean error: 4.495868682861328 mm for frame 25

Lowest mean error: 3.940321445465088 mm for frame 5

Saving results

Total time: 69.45276761054993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753719
Iteration 2/25 | Loss: 0.00101143
Iteration 3/25 | Loss: 0.00077811
Iteration 4/25 | Loss: 0.00073344
Iteration 5/25 | Loss: 0.00071818
Iteration 6/25 | Loss: 0.00071370
Iteration 7/25 | Loss: 0.00071241
Iteration 8/25 | Loss: 0.00071234
Iteration 9/25 | Loss: 0.00071234
Iteration 10/25 | Loss: 0.00071234
Iteration 11/25 | Loss: 0.00071234
Iteration 12/25 | Loss: 0.00071234
Iteration 13/25 | Loss: 0.00071234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007123394752852619, 0.0007123394752852619, 0.0007123394752852619, 0.0007123394752852619, 0.0007123394752852619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007123394752852619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.05775166
Iteration 2/25 | Loss: 0.00033018
Iteration 3/25 | Loss: 0.00033018
Iteration 4/25 | Loss: 0.00033018
Iteration 5/25 | Loss: 0.00033018
Iteration 6/25 | Loss: 0.00033018
Iteration 7/25 | Loss: 0.00033018
Iteration 8/25 | Loss: 0.00033018
Iteration 9/25 | Loss: 0.00033018
Iteration 10/25 | Loss: 0.00033018
Iteration 11/25 | Loss: 0.00033018
Iteration 12/25 | Loss: 0.00033018
Iteration 13/25 | Loss: 0.00033018
Iteration 14/25 | Loss: 0.00033018
Iteration 15/25 | Loss: 0.00033018
Iteration 16/25 | Loss: 0.00033018
Iteration 17/25 | Loss: 0.00033018
Iteration 18/25 | Loss: 0.00033018
Iteration 19/25 | Loss: 0.00033018
Iteration 20/25 | Loss: 0.00033018
Iteration 21/25 | Loss: 0.00033018
Iteration 22/25 | Loss: 0.00033018
Iteration 23/25 | Loss: 0.00033018
Iteration 24/25 | Loss: 0.00033018
Iteration 25/25 | Loss: 0.00033018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033018
Iteration 2/1000 | Loss: 0.00004383
Iteration 3/1000 | Loss: 0.00003337
Iteration 4/1000 | Loss: 0.00003081
Iteration 5/1000 | Loss: 0.00002932
Iteration 6/1000 | Loss: 0.00002796
Iteration 7/1000 | Loss: 0.00002710
Iteration 8/1000 | Loss: 0.00002645
Iteration 9/1000 | Loss: 0.00002593
Iteration 10/1000 | Loss: 0.00002554
Iteration 11/1000 | Loss: 0.00002528
Iteration 12/1000 | Loss: 0.00002508
Iteration 13/1000 | Loss: 0.00002490
Iteration 14/1000 | Loss: 0.00002484
Iteration 15/1000 | Loss: 0.00002484
Iteration 16/1000 | Loss: 0.00002479
Iteration 17/1000 | Loss: 0.00002476
Iteration 18/1000 | Loss: 0.00002476
Iteration 19/1000 | Loss: 0.00002475
Iteration 20/1000 | Loss: 0.00002475
Iteration 21/1000 | Loss: 0.00002474
Iteration 22/1000 | Loss: 0.00002471
Iteration 23/1000 | Loss: 0.00002470
Iteration 24/1000 | Loss: 0.00002470
Iteration 25/1000 | Loss: 0.00002469
Iteration 26/1000 | Loss: 0.00002468
Iteration 27/1000 | Loss: 0.00002468
Iteration 28/1000 | Loss: 0.00002468
Iteration 29/1000 | Loss: 0.00002467
Iteration 30/1000 | Loss: 0.00002467
Iteration 31/1000 | Loss: 0.00002466
Iteration 32/1000 | Loss: 0.00002465
Iteration 33/1000 | Loss: 0.00002465
Iteration 34/1000 | Loss: 0.00002464
Iteration 35/1000 | Loss: 0.00002464
Iteration 36/1000 | Loss: 0.00002463
Iteration 37/1000 | Loss: 0.00002463
Iteration 38/1000 | Loss: 0.00002463
Iteration 39/1000 | Loss: 0.00002462
Iteration 40/1000 | Loss: 0.00002462
Iteration 41/1000 | Loss: 0.00002462
Iteration 42/1000 | Loss: 0.00002461
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002461
Iteration 45/1000 | Loss: 0.00002461
Iteration 46/1000 | Loss: 0.00002461
Iteration 47/1000 | Loss: 0.00002460
Iteration 48/1000 | Loss: 0.00002460
Iteration 49/1000 | Loss: 0.00002460
Iteration 50/1000 | Loss: 0.00002460
Iteration 51/1000 | Loss: 0.00002460
Iteration 52/1000 | Loss: 0.00002460
Iteration 53/1000 | Loss: 0.00002460
Iteration 54/1000 | Loss: 0.00002460
Iteration 55/1000 | Loss: 0.00002460
Iteration 56/1000 | Loss: 0.00002460
Iteration 57/1000 | Loss: 0.00002460
Iteration 58/1000 | Loss: 0.00002460
Iteration 59/1000 | Loss: 0.00002460
Iteration 60/1000 | Loss: 0.00002460
Iteration 61/1000 | Loss: 0.00002460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [2.4601369659649208e-05, 2.4601369659649208e-05, 2.4601369659649208e-05, 2.4601369659649208e-05, 2.4601369659649208e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4601369659649208e-05

Optimization complete. Final v2v error: 4.166184425354004 mm

Highest mean error: 5.286133766174316 mm for frame 188

Lowest mean error: 3.370670795440674 mm for frame 224

Saving results

Total time: 40.175182580947876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740483
Iteration 2/25 | Loss: 0.00103118
Iteration 3/25 | Loss: 0.00070082
Iteration 4/25 | Loss: 0.00066011
Iteration 5/25 | Loss: 0.00064619
Iteration 6/25 | Loss: 0.00063636
Iteration 7/25 | Loss: 0.00063058
Iteration 8/25 | Loss: 0.00063023
Iteration 9/25 | Loss: 0.00063010
Iteration 10/25 | Loss: 0.00063008
Iteration 11/25 | Loss: 0.00063008
Iteration 12/25 | Loss: 0.00063008
Iteration 13/25 | Loss: 0.00063008
Iteration 14/25 | Loss: 0.00063008
Iteration 15/25 | Loss: 0.00063007
Iteration 16/25 | Loss: 0.00063007
Iteration 17/25 | Loss: 0.00063007
Iteration 18/25 | Loss: 0.00063007
Iteration 19/25 | Loss: 0.00063007
Iteration 20/25 | Loss: 0.00063007
Iteration 21/25 | Loss: 0.00063007
Iteration 22/25 | Loss: 0.00063007
Iteration 23/25 | Loss: 0.00063007
Iteration 24/25 | Loss: 0.00063007
Iteration 25/25 | Loss: 0.00063006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02298379
Iteration 2/25 | Loss: 0.00028523
Iteration 3/25 | Loss: 0.00028522
Iteration 4/25 | Loss: 0.00028522
Iteration 5/25 | Loss: 0.00028522
Iteration 6/25 | Loss: 0.00028522
Iteration 7/25 | Loss: 0.00028522
Iteration 8/25 | Loss: 0.00028522
Iteration 9/25 | Loss: 0.00028522
Iteration 10/25 | Loss: 0.00028522
Iteration 11/25 | Loss: 0.00028522
Iteration 12/25 | Loss: 0.00028522
Iteration 13/25 | Loss: 0.00028522
Iteration 14/25 | Loss: 0.00028522
Iteration 15/25 | Loss: 0.00028522
Iteration 16/25 | Loss: 0.00028522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002852223115041852, 0.0002852223115041852, 0.0002852223115041852, 0.0002852223115041852, 0.0002852223115041852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002852223115041852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028522
Iteration 2/1000 | Loss: 0.00002666
Iteration 3/1000 | Loss: 0.00001891
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00008201
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001573
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001521
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001506
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00010536
Iteration 14/1000 | Loss: 0.00002431
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00004602
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001469
Iteration 21/1000 | Loss: 0.00001468
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001468
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001467
Iteration 28/1000 | Loss: 0.00001467
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001465
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00003240
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001459
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001454
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001447
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001443
Iteration 69/1000 | Loss: 0.00001443
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001443
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001442
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001441
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001436
Iteration 81/1000 | Loss: 0.00001436
Iteration 82/1000 | Loss: 0.00001436
Iteration 83/1000 | Loss: 0.00001436
Iteration 84/1000 | Loss: 0.00001436
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001435
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001433
Iteration 95/1000 | Loss: 0.00001433
Iteration 96/1000 | Loss: 0.00001433
Iteration 97/1000 | Loss: 0.00001433
Iteration 98/1000 | Loss: 0.00001433
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001431
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001429
Iteration 113/1000 | Loss: 0.00001429
Iteration 114/1000 | Loss: 0.00001429
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001429
Iteration 117/1000 | Loss: 0.00001429
Iteration 118/1000 | Loss: 0.00001429
Iteration 119/1000 | Loss: 0.00001429
Iteration 120/1000 | Loss: 0.00001429
Iteration 121/1000 | Loss: 0.00001429
Iteration 122/1000 | Loss: 0.00001429
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001427
Iteration 128/1000 | Loss: 0.00001427
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001426
Iteration 131/1000 | Loss: 0.00001426
Iteration 132/1000 | Loss: 0.00001426
Iteration 133/1000 | Loss: 0.00001426
Iteration 134/1000 | Loss: 0.00001426
Iteration 135/1000 | Loss: 0.00001426
Iteration 136/1000 | Loss: 0.00001426
Iteration 137/1000 | Loss: 0.00008909
Iteration 138/1000 | Loss: 0.00001438
Iteration 139/1000 | Loss: 0.00001432
Iteration 140/1000 | Loss: 0.00001425
Iteration 141/1000 | Loss: 0.00001425
Iteration 142/1000 | Loss: 0.00001425
Iteration 143/1000 | Loss: 0.00001425
Iteration 144/1000 | Loss: 0.00001424
Iteration 145/1000 | Loss: 0.00001424
Iteration 146/1000 | Loss: 0.00001424
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001423
Iteration 150/1000 | Loss: 0.00001423
Iteration 151/1000 | Loss: 0.00001423
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001422
Iteration 154/1000 | Loss: 0.00001422
Iteration 155/1000 | Loss: 0.00001422
Iteration 156/1000 | Loss: 0.00001422
Iteration 157/1000 | Loss: 0.00001422
Iteration 158/1000 | Loss: 0.00001422
Iteration 159/1000 | Loss: 0.00001422
Iteration 160/1000 | Loss: 0.00001422
Iteration 161/1000 | Loss: 0.00001422
Iteration 162/1000 | Loss: 0.00001422
Iteration 163/1000 | Loss: 0.00001422
Iteration 164/1000 | Loss: 0.00001422
Iteration 165/1000 | Loss: 0.00001422
Iteration 166/1000 | Loss: 0.00001422
Iteration 167/1000 | Loss: 0.00001422
Iteration 168/1000 | Loss: 0.00001422
Iteration 169/1000 | Loss: 0.00001422
Iteration 170/1000 | Loss: 0.00001422
Iteration 171/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.4216477211448364e-05, 1.4216477211448364e-05, 1.4216477211448364e-05, 1.4216477211448364e-05, 1.4216477211448364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4216477211448364e-05

Optimization complete. Final v2v error: 3.215785264968872 mm

Highest mean error: 3.6805715560913086 mm for frame 149

Lowest mean error: 3.060389518737793 mm for frame 122

Saving results

Total time: 98.01413559913635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862647
Iteration 2/25 | Loss: 0.00168426
Iteration 3/25 | Loss: 0.00130764
Iteration 4/25 | Loss: 0.00118363
Iteration 5/25 | Loss: 0.00191809
Iteration 6/25 | Loss: 0.00115704
Iteration 7/25 | Loss: 0.00095786
Iteration 8/25 | Loss: 0.00095031
Iteration 9/25 | Loss: 0.00094911
Iteration 10/25 | Loss: 0.00094891
Iteration 11/25 | Loss: 0.00094891
Iteration 12/25 | Loss: 0.00094891
Iteration 13/25 | Loss: 0.00094891
Iteration 14/25 | Loss: 0.00094891
Iteration 15/25 | Loss: 0.00094891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009489133371971548, 0.0009489133371971548, 0.0009489133371971548, 0.0009489133371971548, 0.0009489133371971548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009489133371971548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17605722
Iteration 2/25 | Loss: 0.00051806
Iteration 3/25 | Loss: 0.00051806
Iteration 4/25 | Loss: 0.00051806
Iteration 5/25 | Loss: 0.00051806
Iteration 6/25 | Loss: 0.00051806
Iteration 7/25 | Loss: 0.00051806
Iteration 8/25 | Loss: 0.00051806
Iteration 9/25 | Loss: 0.00051806
Iteration 10/25 | Loss: 0.00051806
Iteration 11/25 | Loss: 0.00051806
Iteration 12/25 | Loss: 0.00051806
Iteration 13/25 | Loss: 0.00051806
Iteration 14/25 | Loss: 0.00051806
Iteration 15/25 | Loss: 0.00051806
Iteration 16/25 | Loss: 0.00051806
Iteration 17/25 | Loss: 0.00051806
Iteration 18/25 | Loss: 0.00051806
Iteration 19/25 | Loss: 0.00051806
Iteration 20/25 | Loss: 0.00051806
Iteration 21/25 | Loss: 0.00051806
Iteration 22/25 | Loss: 0.00051806
Iteration 23/25 | Loss: 0.00051806
Iteration 24/25 | Loss: 0.00051806
Iteration 25/25 | Loss: 0.00051806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051806
Iteration 2/1000 | Loss: 0.00005772
Iteration 3/1000 | Loss: 0.00004556
Iteration 4/1000 | Loss: 0.00004294
Iteration 5/1000 | Loss: 0.00004208
Iteration 6/1000 | Loss: 0.00004134
Iteration 7/1000 | Loss: 0.00004088
Iteration 8/1000 | Loss: 0.00004054
Iteration 9/1000 | Loss: 0.00004037
Iteration 10/1000 | Loss: 0.00004018
Iteration 11/1000 | Loss: 0.00004017
Iteration 12/1000 | Loss: 0.00004017
Iteration 13/1000 | Loss: 0.00004011
Iteration 14/1000 | Loss: 0.00004006
Iteration 15/1000 | Loss: 0.00003994
Iteration 16/1000 | Loss: 0.00003991
Iteration 17/1000 | Loss: 0.00003990
Iteration 18/1000 | Loss: 0.00003987
Iteration 19/1000 | Loss: 0.00003987
Iteration 20/1000 | Loss: 0.00003985
Iteration 21/1000 | Loss: 0.00003981
Iteration 22/1000 | Loss: 0.00003978
Iteration 23/1000 | Loss: 0.00003978
Iteration 24/1000 | Loss: 0.00003978
Iteration 25/1000 | Loss: 0.00003978
Iteration 26/1000 | Loss: 0.00003978
Iteration 27/1000 | Loss: 0.00003978
Iteration 28/1000 | Loss: 0.00003978
Iteration 29/1000 | Loss: 0.00003978
Iteration 30/1000 | Loss: 0.00003977
Iteration 31/1000 | Loss: 0.00003976
Iteration 32/1000 | Loss: 0.00003976
Iteration 33/1000 | Loss: 0.00003976
Iteration 34/1000 | Loss: 0.00003975
Iteration 35/1000 | Loss: 0.00003975
Iteration 36/1000 | Loss: 0.00003974
Iteration 37/1000 | Loss: 0.00003974
Iteration 38/1000 | Loss: 0.00003973
Iteration 39/1000 | Loss: 0.00003973
Iteration 40/1000 | Loss: 0.00003973
Iteration 41/1000 | Loss: 0.00003973
Iteration 42/1000 | Loss: 0.00003973
Iteration 43/1000 | Loss: 0.00003973
Iteration 44/1000 | Loss: 0.00003972
Iteration 45/1000 | Loss: 0.00003972
Iteration 46/1000 | Loss: 0.00003972
Iteration 47/1000 | Loss: 0.00003972
Iteration 48/1000 | Loss: 0.00003972
Iteration 49/1000 | Loss: 0.00003972
Iteration 50/1000 | Loss: 0.00003972
Iteration 51/1000 | Loss: 0.00003971
Iteration 52/1000 | Loss: 0.00003971
Iteration 53/1000 | Loss: 0.00003971
Iteration 54/1000 | Loss: 0.00003971
Iteration 55/1000 | Loss: 0.00003971
Iteration 56/1000 | Loss: 0.00003971
Iteration 57/1000 | Loss: 0.00003971
Iteration 58/1000 | Loss: 0.00003971
Iteration 59/1000 | Loss: 0.00003970
Iteration 60/1000 | Loss: 0.00003970
Iteration 61/1000 | Loss: 0.00003970
Iteration 62/1000 | Loss: 0.00003970
Iteration 63/1000 | Loss: 0.00003970
Iteration 64/1000 | Loss: 0.00003970
Iteration 65/1000 | Loss: 0.00003970
Iteration 66/1000 | Loss: 0.00003970
Iteration 67/1000 | Loss: 0.00003970
Iteration 68/1000 | Loss: 0.00003970
Iteration 69/1000 | Loss: 0.00003970
Iteration 70/1000 | Loss: 0.00003970
Iteration 71/1000 | Loss: 0.00003969
Iteration 72/1000 | Loss: 0.00003969
Iteration 73/1000 | Loss: 0.00003969
Iteration 74/1000 | Loss: 0.00003969
Iteration 75/1000 | Loss: 0.00003969
Iteration 76/1000 | Loss: 0.00003969
Iteration 77/1000 | Loss: 0.00003969
Iteration 78/1000 | Loss: 0.00003968
Iteration 79/1000 | Loss: 0.00003968
Iteration 80/1000 | Loss: 0.00003968
Iteration 81/1000 | Loss: 0.00003968
Iteration 82/1000 | Loss: 0.00003968
Iteration 83/1000 | Loss: 0.00003968
Iteration 84/1000 | Loss: 0.00003967
Iteration 85/1000 | Loss: 0.00003967
Iteration 86/1000 | Loss: 0.00003967
Iteration 87/1000 | Loss: 0.00003967
Iteration 88/1000 | Loss: 0.00003967
Iteration 89/1000 | Loss: 0.00003967
Iteration 90/1000 | Loss: 0.00003967
Iteration 91/1000 | Loss: 0.00003967
Iteration 92/1000 | Loss: 0.00003967
Iteration 93/1000 | Loss: 0.00003967
Iteration 94/1000 | Loss: 0.00003967
Iteration 95/1000 | Loss: 0.00003967
Iteration 96/1000 | Loss: 0.00003967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [3.967040538555011e-05, 3.967040538555011e-05, 3.967040538555011e-05, 3.967040538555011e-05, 3.967040538555011e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.967040538555011e-05

Optimization complete. Final v2v error: 5.107240200042725 mm

Highest mean error: 5.224122524261475 mm for frame 26

Lowest mean error: 5.060272693634033 mm for frame 54

Saving results

Total time: 64.40706443786621
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911294
Iteration 2/25 | Loss: 0.00080084
Iteration 3/25 | Loss: 0.00067767
Iteration 4/25 | Loss: 0.00066252
Iteration 5/25 | Loss: 0.00065831
Iteration 6/25 | Loss: 0.00065701
Iteration 7/25 | Loss: 0.00065699
Iteration 8/25 | Loss: 0.00065699
Iteration 9/25 | Loss: 0.00065699
Iteration 10/25 | Loss: 0.00065699
Iteration 11/25 | Loss: 0.00065699
Iteration 12/25 | Loss: 0.00065699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000656993652228266, 0.000656993652228266, 0.000656993652228266, 0.000656993652228266, 0.000656993652228266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000656993652228266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38595092
Iteration 2/25 | Loss: 0.00055743
Iteration 3/25 | Loss: 0.00055743
Iteration 4/25 | Loss: 0.00055743
Iteration 5/25 | Loss: 0.00055743
Iteration 6/25 | Loss: 0.00055743
Iteration 7/25 | Loss: 0.00055743
Iteration 8/25 | Loss: 0.00055743
Iteration 9/25 | Loss: 0.00055742
Iteration 10/25 | Loss: 0.00055742
Iteration 11/25 | Loss: 0.00055742
Iteration 12/25 | Loss: 0.00055742
Iteration 13/25 | Loss: 0.00055742
Iteration 14/25 | Loss: 0.00055742
Iteration 15/25 | Loss: 0.00055742
Iteration 16/25 | Loss: 0.00055742
Iteration 17/25 | Loss: 0.00055742
Iteration 18/25 | Loss: 0.00055742
Iteration 19/25 | Loss: 0.00055742
Iteration 20/25 | Loss: 0.00055742
Iteration 21/25 | Loss: 0.00055742
Iteration 22/25 | Loss: 0.00055742
Iteration 23/25 | Loss: 0.00055742
Iteration 24/25 | Loss: 0.00055742
Iteration 25/25 | Loss: 0.00055742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055742
Iteration 2/1000 | Loss: 0.00002602
Iteration 3/1000 | Loss: 0.00001446
Iteration 4/1000 | Loss: 0.00001324
Iteration 5/1000 | Loss: 0.00001306
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001263
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001238
Iteration 15/1000 | Loss: 0.00001235
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001232
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001228
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001225
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001224
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001223
Iteration 48/1000 | Loss: 0.00001223
Iteration 49/1000 | Loss: 0.00001223
Iteration 50/1000 | Loss: 0.00001223
Iteration 51/1000 | Loss: 0.00001222
Iteration 52/1000 | Loss: 0.00001222
Iteration 53/1000 | Loss: 0.00001222
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001212
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001204
Iteration 77/1000 | Loss: 0.00001200
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001200
Iteration 83/1000 | Loss: 0.00001200
Iteration 84/1000 | Loss: 0.00001200
Iteration 85/1000 | Loss: 0.00001200
Iteration 86/1000 | Loss: 0.00001200
Iteration 87/1000 | Loss: 0.00001199
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001197
Iteration 92/1000 | Loss: 0.00001197
Iteration 93/1000 | Loss: 0.00001196
Iteration 94/1000 | Loss: 0.00001196
Iteration 95/1000 | Loss: 0.00001195
Iteration 96/1000 | Loss: 0.00001195
Iteration 97/1000 | Loss: 0.00001195
Iteration 98/1000 | Loss: 0.00001195
Iteration 99/1000 | Loss: 0.00001195
Iteration 100/1000 | Loss: 0.00001195
Iteration 101/1000 | Loss: 0.00001195
Iteration 102/1000 | Loss: 0.00001195
Iteration 103/1000 | Loss: 0.00001194
Iteration 104/1000 | Loss: 0.00001194
Iteration 105/1000 | Loss: 0.00001194
Iteration 106/1000 | Loss: 0.00001194
Iteration 107/1000 | Loss: 0.00001194
Iteration 108/1000 | Loss: 0.00001193
Iteration 109/1000 | Loss: 0.00001193
Iteration 110/1000 | Loss: 0.00001193
Iteration 111/1000 | Loss: 0.00001193
Iteration 112/1000 | Loss: 0.00001193
Iteration 113/1000 | Loss: 0.00001193
Iteration 114/1000 | Loss: 0.00001193
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001191
Iteration 130/1000 | Loss: 0.00001191
Iteration 131/1000 | Loss: 0.00001191
Iteration 132/1000 | Loss: 0.00001191
Iteration 133/1000 | Loss: 0.00001191
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1913190974155441e-05, 1.1913190974155441e-05, 1.1913190974155441e-05, 1.1913190974155441e-05, 1.1913190974155441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1913190974155441e-05

Optimization complete. Final v2v error: 2.933922290802002 mm

Highest mean error: 3.1592276096343994 mm for frame 19

Lowest mean error: 2.8098950386047363 mm for frame 115

Saving results

Total time: 34.05006289482117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043320
Iteration 2/25 | Loss: 0.01043320
Iteration 3/25 | Loss: 0.01043319
Iteration 4/25 | Loss: 0.00266800
Iteration 5/25 | Loss: 0.00180392
Iteration 6/25 | Loss: 0.00144672
Iteration 7/25 | Loss: 0.00173284
Iteration 8/25 | Loss: 0.00134474
Iteration 9/25 | Loss: 0.00097727
Iteration 10/25 | Loss: 0.00083713
Iteration 11/25 | Loss: 0.00073095
Iteration 12/25 | Loss: 0.00069795
Iteration 13/25 | Loss: 0.00068724
Iteration 14/25 | Loss: 0.00068246
Iteration 15/25 | Loss: 0.00068488
Iteration 16/25 | Loss: 0.00068368
Iteration 17/25 | Loss: 0.00068551
Iteration 18/25 | Loss: 0.00068158
Iteration 19/25 | Loss: 0.00068102
Iteration 20/25 | Loss: 0.00068355
Iteration 21/25 | Loss: 0.00068068
Iteration 22/25 | Loss: 0.00067538
Iteration 23/25 | Loss: 0.00067445
Iteration 24/25 | Loss: 0.00067425
Iteration 25/25 | Loss: 0.00067419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40173423
Iteration 2/25 | Loss: 0.00020120
Iteration 3/25 | Loss: 0.00020118
Iteration 4/25 | Loss: 0.00020118
Iteration 5/25 | Loss: 0.00020118
Iteration 6/25 | Loss: 0.00020118
Iteration 7/25 | Loss: 0.00020118
Iteration 8/25 | Loss: 0.00020118
Iteration 9/25 | Loss: 0.00020117
Iteration 10/25 | Loss: 0.00020117
Iteration 11/25 | Loss: 0.00020117
Iteration 12/25 | Loss: 0.00020117
Iteration 13/25 | Loss: 0.00020117
Iteration 14/25 | Loss: 0.00020117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00020117480016779155, 0.00020117480016779155, 0.00020117480016779155, 0.00020117480016779155, 0.00020117480016779155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020117480016779155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020117
Iteration 2/1000 | Loss: 0.00002486
Iteration 3/1000 | Loss: 0.00001977
Iteration 4/1000 | Loss: 0.00001846
Iteration 5/1000 | Loss: 0.00001734
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001646
Iteration 8/1000 | Loss: 0.00001640
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001598
Iteration 11/1000 | Loss: 0.00001597
Iteration 12/1000 | Loss: 0.00001589
Iteration 13/1000 | Loss: 0.00001584
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001577
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001577
Iteration 18/1000 | Loss: 0.00001577
Iteration 19/1000 | Loss: 0.00001577
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001576
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001574
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001574
Iteration 39/1000 | Loss: 0.00001574
Iteration 40/1000 | Loss: 0.00001574
Iteration 41/1000 | Loss: 0.00001574
Iteration 42/1000 | Loss: 0.00001574
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001574
Iteration 46/1000 | Loss: 0.00001574
Iteration 47/1000 | Loss: 0.00001574
Iteration 48/1000 | Loss: 0.00001574
Iteration 49/1000 | Loss: 0.00001574
Iteration 50/1000 | Loss: 0.00001574
Iteration 51/1000 | Loss: 0.00001574
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001574
Iteration 57/1000 | Loss: 0.00001574
Iteration 58/1000 | Loss: 0.00001574
Iteration 59/1000 | Loss: 0.00001574
Iteration 60/1000 | Loss: 0.00001574
Iteration 61/1000 | Loss: 0.00001574
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001574
Iteration 79/1000 | Loss: 0.00001574
Iteration 80/1000 | Loss: 0.00001574
Iteration 81/1000 | Loss: 0.00001574
Iteration 82/1000 | Loss: 0.00001574
Iteration 83/1000 | Loss: 0.00001574
Iteration 84/1000 | Loss: 0.00001574
Iteration 85/1000 | Loss: 0.00001574
Iteration 86/1000 | Loss: 0.00001574
Iteration 87/1000 | Loss: 0.00001574
Iteration 88/1000 | Loss: 0.00001574
Iteration 89/1000 | Loss: 0.00001574
Iteration 90/1000 | Loss: 0.00001574
Iteration 91/1000 | Loss: 0.00001574
Iteration 92/1000 | Loss: 0.00001574
Iteration 93/1000 | Loss: 0.00001574
Iteration 94/1000 | Loss: 0.00001574
Iteration 95/1000 | Loss: 0.00001574
Iteration 96/1000 | Loss: 0.00001574
Iteration 97/1000 | Loss: 0.00001574
Iteration 98/1000 | Loss: 0.00001574
Iteration 99/1000 | Loss: 0.00001574
Iteration 100/1000 | Loss: 0.00001574
Iteration 101/1000 | Loss: 0.00001574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.5737117792014033e-05, 1.5737117792014033e-05, 1.5737117792014033e-05, 1.5737117792014033e-05, 1.5737117792014033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5737117792014033e-05

Optimization complete. Final v2v error: 3.385629177093506 mm

Highest mean error: 3.5510365962982178 mm for frame 225

Lowest mean error: 3.233837842941284 mm for frame 80

Saving results

Total time: 85.14999151229858
