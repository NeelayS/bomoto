Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=133, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7448-7503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813213
Iteration 2/25 | Loss: 0.00218457
Iteration 3/25 | Loss: 0.00127307
Iteration 4/25 | Loss: 0.00103782
Iteration 5/25 | Loss: 0.00098151
Iteration 6/25 | Loss: 0.00096430
Iteration 7/25 | Loss: 0.00092986
Iteration 8/25 | Loss: 0.00090597
Iteration 9/25 | Loss: 0.00091634
Iteration 10/25 | Loss: 0.00089795
Iteration 11/25 | Loss: 0.00089380
Iteration 12/25 | Loss: 0.00088492
Iteration 13/25 | Loss: 0.00088181
Iteration 14/25 | Loss: 0.00087992
Iteration 15/25 | Loss: 0.00087966
Iteration 16/25 | Loss: 0.00087950
Iteration 17/25 | Loss: 0.00087944
Iteration 18/25 | Loss: 0.00087943
Iteration 19/25 | Loss: 0.00087943
Iteration 20/25 | Loss: 0.00087943
Iteration 21/25 | Loss: 0.00087943
Iteration 22/25 | Loss: 0.00087943
Iteration 23/25 | Loss: 0.00087943
Iteration 24/25 | Loss: 0.00087943
Iteration 25/25 | Loss: 0.00087943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94727969
Iteration 2/25 | Loss: 0.00091073
Iteration 3/25 | Loss: 0.00089869
Iteration 4/25 | Loss: 0.00089869
Iteration 5/25 | Loss: 0.00089869
Iteration 6/25 | Loss: 0.00089869
Iteration 7/25 | Loss: 0.00089869
Iteration 8/25 | Loss: 0.00089869
Iteration 9/25 | Loss: 0.00089869
Iteration 10/25 | Loss: 0.00089869
Iteration 11/25 | Loss: 0.00089869
Iteration 12/25 | Loss: 0.00089869
Iteration 13/25 | Loss: 0.00089869
Iteration 14/25 | Loss: 0.00089869
Iteration 15/25 | Loss: 0.00089869
Iteration 16/25 | Loss: 0.00089869
Iteration 17/25 | Loss: 0.00089869
Iteration 18/25 | Loss: 0.00089869
Iteration 19/25 | Loss: 0.00089869
Iteration 20/25 | Loss: 0.00089869
Iteration 21/25 | Loss: 0.00089869
Iteration 22/25 | Loss: 0.00089869
Iteration 23/25 | Loss: 0.00089869
Iteration 24/25 | Loss: 0.00089869
Iteration 25/25 | Loss: 0.00089869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089869
Iteration 2/1000 | Loss: 0.00005005
Iteration 3/1000 | Loss: 0.00002909
Iteration 4/1000 | Loss: 0.00002095
Iteration 5/1000 | Loss: 0.00001925
Iteration 6/1000 | Loss: 0.00003000
Iteration 7/1000 | Loss: 0.00002998
Iteration 8/1000 | Loss: 0.00002120
Iteration 9/1000 | Loss: 0.00001783
Iteration 10/1000 | Loss: 0.00001752
Iteration 11/1000 | Loss: 0.00002696
Iteration 12/1000 | Loss: 0.00001730
Iteration 13/1000 | Loss: 0.00002359
Iteration 14/1000 | Loss: 0.00002176
Iteration 15/1000 | Loss: 0.00001711
Iteration 16/1000 | Loss: 0.00002622
Iteration 17/1000 | Loss: 0.00001981
Iteration 18/1000 | Loss: 0.00002620
Iteration 19/1000 | Loss: 0.00001982
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001706
Iteration 22/1000 | Loss: 0.00001705
Iteration 23/1000 | Loss: 0.00001705
Iteration 24/1000 | Loss: 0.00001705
Iteration 25/1000 | Loss: 0.00001705
Iteration 26/1000 | Loss: 0.00001705
Iteration 27/1000 | Loss: 0.00001705
Iteration 28/1000 | Loss: 0.00001705
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001704
Iteration 32/1000 | Loss: 0.00001703
Iteration 33/1000 | Loss: 0.00001703
Iteration 34/1000 | Loss: 0.00001703
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001700
Iteration 38/1000 | Loss: 0.00001700
Iteration 39/1000 | Loss: 0.00001699
Iteration 40/1000 | Loss: 0.00001698
Iteration 41/1000 | Loss: 0.00001698
Iteration 42/1000 | Loss: 0.00001698
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001697
Iteration 46/1000 | Loss: 0.00001696
Iteration 47/1000 | Loss: 0.00001696
Iteration 48/1000 | Loss: 0.00001696
Iteration 49/1000 | Loss: 0.00002818
Iteration 50/1000 | Loss: 0.00004019
Iteration 51/1000 | Loss: 0.00003166
Iteration 52/1000 | Loss: 0.00002042
Iteration 53/1000 | Loss: 0.00001689
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001686
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001985
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001689
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001688
Iteration 76/1000 | Loss: 0.00001688
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001688
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001686
Iteration 81/1000 | Loss: 0.00001686
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001686
Iteration 86/1000 | Loss: 0.00001686
Iteration 87/1000 | Loss: 0.00001686
Iteration 88/1000 | Loss: 0.00001686
Iteration 89/1000 | Loss: 0.00001685
Iteration 90/1000 | Loss: 0.00001685
Iteration 91/1000 | Loss: 0.00001685
Iteration 92/1000 | Loss: 0.00001685
Iteration 93/1000 | Loss: 0.00001685
Iteration 94/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.685488678049296e-05, 1.685488678049296e-05, 1.685488678049296e-05, 1.685488678049296e-05, 1.685488678049296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.685488678049296e-05

Optimization complete. Final v2v error: 3.4756674766540527 mm

Highest mean error: 9.892669677734375 mm for frame 109

Lowest mean error: 2.9827957153320312 mm for frame 0

Saving results

Total time: 69.64359998703003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454297
Iteration 2/25 | Loss: 0.00107411
Iteration 3/25 | Loss: 0.00099141
Iteration 4/25 | Loss: 0.00095707
Iteration 5/25 | Loss: 0.00095003
Iteration 6/25 | Loss: 0.00094874
Iteration 7/25 | Loss: 0.00094872
Iteration 8/25 | Loss: 0.00094872
Iteration 9/25 | Loss: 0.00094872
Iteration 10/25 | Loss: 0.00094872
Iteration 11/25 | Loss: 0.00094872
Iteration 12/25 | Loss: 0.00094872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009487189818173647, 0.0009487189818173647, 0.0009487189818173647, 0.0009487189818173647, 0.0009487189818173647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009487189818173647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32725441
Iteration 2/25 | Loss: 0.00092397
Iteration 3/25 | Loss: 0.00092397
Iteration 4/25 | Loss: 0.00092397
Iteration 5/25 | Loss: 0.00092397
Iteration 6/25 | Loss: 0.00092397
Iteration 7/25 | Loss: 0.00092397
Iteration 8/25 | Loss: 0.00092397
Iteration 9/25 | Loss: 0.00092397
Iteration 10/25 | Loss: 0.00092397
Iteration 11/25 | Loss: 0.00092397
Iteration 12/25 | Loss: 0.00092397
Iteration 13/25 | Loss: 0.00092397
Iteration 14/25 | Loss: 0.00092397
Iteration 15/25 | Loss: 0.00092397
Iteration 16/25 | Loss: 0.00092397
Iteration 17/25 | Loss: 0.00092397
Iteration 18/25 | Loss: 0.00092397
Iteration 19/25 | Loss: 0.00092397
Iteration 20/25 | Loss: 0.00092397
Iteration 21/25 | Loss: 0.00092397
Iteration 22/25 | Loss: 0.00092397
Iteration 23/25 | Loss: 0.00092397
Iteration 24/25 | Loss: 0.00092397
Iteration 25/25 | Loss: 0.00092397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092397
Iteration 2/1000 | Loss: 0.00004898
Iteration 3/1000 | Loss: 0.00002895
Iteration 4/1000 | Loss: 0.00002413
Iteration 5/1000 | Loss: 0.00002230
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002068
Iteration 8/1000 | Loss: 0.00002002
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001968
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001946
Iteration 13/1000 | Loss: 0.00001935
Iteration 14/1000 | Loss: 0.00001933
Iteration 15/1000 | Loss: 0.00001932
Iteration 16/1000 | Loss: 0.00001931
Iteration 17/1000 | Loss: 0.00001929
Iteration 18/1000 | Loss: 0.00001929
Iteration 19/1000 | Loss: 0.00001929
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001927
Iteration 24/1000 | Loss: 0.00001927
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001926
Iteration 28/1000 | Loss: 0.00001926
Iteration 29/1000 | Loss: 0.00001925
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00001925
Iteration 32/1000 | Loss: 0.00001925
Iteration 33/1000 | Loss: 0.00001925
Iteration 34/1000 | Loss: 0.00001925
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001924
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001922
Iteration 50/1000 | Loss: 0.00001921
Iteration 51/1000 | Loss: 0.00001921
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00001921
Iteration 56/1000 | Loss: 0.00001921
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001919
Iteration 69/1000 | Loss: 0.00001919
Iteration 70/1000 | Loss: 0.00001919
Iteration 71/1000 | Loss: 0.00001919
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001916
Iteration 76/1000 | Loss: 0.00001916
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001915
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001914
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001912
Iteration 102/1000 | Loss: 0.00001912
Iteration 103/1000 | Loss: 0.00001911
Iteration 104/1000 | Loss: 0.00001911
Iteration 105/1000 | Loss: 0.00001911
Iteration 106/1000 | Loss: 0.00001911
Iteration 107/1000 | Loss: 0.00001911
Iteration 108/1000 | Loss: 0.00001911
Iteration 109/1000 | Loss: 0.00001911
Iteration 110/1000 | Loss: 0.00001911
Iteration 111/1000 | Loss: 0.00001911
Iteration 112/1000 | Loss: 0.00001911
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00001911
Iteration 115/1000 | Loss: 0.00001911
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001910
Iteration 120/1000 | Loss: 0.00001910
Iteration 121/1000 | Loss: 0.00001910
Iteration 122/1000 | Loss: 0.00001910
Iteration 123/1000 | Loss: 0.00001910
Iteration 124/1000 | Loss: 0.00001910
Iteration 125/1000 | Loss: 0.00001910
Iteration 126/1000 | Loss: 0.00001910
Iteration 127/1000 | Loss: 0.00001909
Iteration 128/1000 | Loss: 0.00001909
Iteration 129/1000 | Loss: 0.00001909
Iteration 130/1000 | Loss: 0.00001909
Iteration 131/1000 | Loss: 0.00001909
Iteration 132/1000 | Loss: 0.00001909
Iteration 133/1000 | Loss: 0.00001909
Iteration 134/1000 | Loss: 0.00001909
Iteration 135/1000 | Loss: 0.00001909
Iteration 136/1000 | Loss: 0.00001909
Iteration 137/1000 | Loss: 0.00001909
Iteration 138/1000 | Loss: 0.00001909
Iteration 139/1000 | Loss: 0.00001909
Iteration 140/1000 | Loss: 0.00001908
Iteration 141/1000 | Loss: 0.00001908
Iteration 142/1000 | Loss: 0.00001908
Iteration 143/1000 | Loss: 0.00001908
Iteration 144/1000 | Loss: 0.00001908
Iteration 145/1000 | Loss: 0.00001908
Iteration 146/1000 | Loss: 0.00001908
Iteration 147/1000 | Loss: 0.00001907
Iteration 148/1000 | Loss: 0.00001907
Iteration 149/1000 | Loss: 0.00001907
Iteration 150/1000 | Loss: 0.00001907
Iteration 151/1000 | Loss: 0.00001907
Iteration 152/1000 | Loss: 0.00001907
Iteration 153/1000 | Loss: 0.00001907
Iteration 154/1000 | Loss: 0.00001907
Iteration 155/1000 | Loss: 0.00001907
Iteration 156/1000 | Loss: 0.00001907
Iteration 157/1000 | Loss: 0.00001907
Iteration 158/1000 | Loss: 0.00001907
Iteration 159/1000 | Loss: 0.00001907
Iteration 160/1000 | Loss: 0.00001907
Iteration 161/1000 | Loss: 0.00001907
Iteration 162/1000 | Loss: 0.00001907
Iteration 163/1000 | Loss: 0.00001907
Iteration 164/1000 | Loss: 0.00001907
Iteration 165/1000 | Loss: 0.00001907
Iteration 166/1000 | Loss: 0.00001907
Iteration 167/1000 | Loss: 0.00001907
Iteration 168/1000 | Loss: 0.00001907
Iteration 169/1000 | Loss: 0.00001907
Iteration 170/1000 | Loss: 0.00001907
Iteration 171/1000 | Loss: 0.00001907
Iteration 172/1000 | Loss: 0.00001907
Iteration 173/1000 | Loss: 0.00001907
Iteration 174/1000 | Loss: 0.00001907
Iteration 175/1000 | Loss: 0.00001907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.9073100702371448e-05, 1.9073100702371448e-05, 1.9073100702371448e-05, 1.9073100702371448e-05, 1.9073100702371448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9073100702371448e-05

Optimization complete. Final v2v error: 3.7336928844451904 mm

Highest mean error: 4.07281494140625 mm for frame 79

Lowest mean error: 3.433619737625122 mm for frame 155

Saving results

Total time: 35.99251961708069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01146129
Iteration 2/25 | Loss: 0.01146129
Iteration 3/25 | Loss: 0.01146129
Iteration 4/25 | Loss: 0.01146129
Iteration 5/25 | Loss: 0.01146129
Iteration 6/25 | Loss: 0.01146129
Iteration 7/25 | Loss: 0.01146128
Iteration 8/25 | Loss: 0.01146128
Iteration 9/25 | Loss: 0.01146128
Iteration 10/25 | Loss: 0.01146128
Iteration 11/25 | Loss: 0.01146128
Iteration 12/25 | Loss: 0.01146128
Iteration 13/25 | Loss: 0.01146128
Iteration 14/25 | Loss: 0.01146128
Iteration 15/25 | Loss: 0.01146127
Iteration 16/25 | Loss: 0.01146127
Iteration 17/25 | Loss: 0.01146127
Iteration 18/25 | Loss: 0.01146127
Iteration 19/25 | Loss: 0.01146127
Iteration 20/25 | Loss: 0.01146127
Iteration 21/25 | Loss: 0.01146127
Iteration 22/25 | Loss: 0.01146127
Iteration 23/25 | Loss: 0.01146126
Iteration 24/25 | Loss: 0.01146126
Iteration 25/25 | Loss: 0.01146126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90628326
Iteration 2/25 | Loss: 0.14924738
Iteration 3/25 | Loss: 0.14923526
Iteration 4/25 | Loss: 0.14923523
Iteration 5/25 | Loss: 0.14923520
Iteration 6/25 | Loss: 0.14923520
Iteration 7/25 | Loss: 0.14923520
Iteration 8/25 | Loss: 0.14923520
Iteration 9/25 | Loss: 0.14923520
Iteration 10/25 | Loss: 0.14923519
Iteration 11/25 | Loss: 0.14923519
Iteration 12/25 | Loss: 0.14923519
Iteration 13/25 | Loss: 0.14923519
Iteration 14/25 | Loss: 0.14923519
Iteration 15/25 | Loss: 0.14923519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.14923518896102905, 0.14923518896102905, 0.14923518896102905, 0.14923518896102905, 0.14923518896102905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14923518896102905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14923519
Iteration 2/1000 | Loss: 0.00477713
Iteration 3/1000 | Loss: 0.00083667
Iteration 4/1000 | Loss: 0.00144751
Iteration 5/1000 | Loss: 0.00016358
Iteration 6/1000 | Loss: 0.00145794
Iteration 7/1000 | Loss: 0.00012357
Iteration 8/1000 | Loss: 0.00007975
Iteration 9/1000 | Loss: 0.00014768
Iteration 10/1000 | Loss: 0.00009503
Iteration 11/1000 | Loss: 0.00009653
Iteration 12/1000 | Loss: 0.00016403
Iteration 13/1000 | Loss: 0.00005744
Iteration 14/1000 | Loss: 0.00003924
Iteration 15/1000 | Loss: 0.00011283
Iteration 16/1000 | Loss: 0.00003473
Iteration 17/1000 | Loss: 0.00020452
Iteration 18/1000 | Loss: 0.00005785
Iteration 19/1000 | Loss: 0.00003562
Iteration 20/1000 | Loss: 0.00003220
Iteration 21/1000 | Loss: 0.00003342
Iteration 22/1000 | Loss: 0.00011728
Iteration 23/1000 | Loss: 0.00003868
Iteration 24/1000 | Loss: 0.00002768
Iteration 25/1000 | Loss: 0.00007761
Iteration 26/1000 | Loss: 0.00002656
Iteration 27/1000 | Loss: 0.00005668
Iteration 28/1000 | Loss: 0.00002542
Iteration 29/1000 | Loss: 0.00010311
Iteration 30/1000 | Loss: 0.00009267
Iteration 31/1000 | Loss: 0.00002411
Iteration 32/1000 | Loss: 0.00013664
Iteration 33/1000 | Loss: 0.00002375
Iteration 34/1000 | Loss: 0.00002336
Iteration 35/1000 | Loss: 0.00002335
Iteration 36/1000 | Loss: 0.00008604
Iteration 37/1000 | Loss: 0.00003816
Iteration 38/1000 | Loss: 0.00005308
Iteration 39/1000 | Loss: 0.00003983
Iteration 40/1000 | Loss: 0.00002663
Iteration 41/1000 | Loss: 0.00002297
Iteration 42/1000 | Loss: 0.00002289
Iteration 43/1000 | Loss: 0.00002287
Iteration 44/1000 | Loss: 0.00002287
Iteration 45/1000 | Loss: 0.00002286
Iteration 46/1000 | Loss: 0.00003321
Iteration 47/1000 | Loss: 0.00003321
Iteration 48/1000 | Loss: 0.00051993
Iteration 49/1000 | Loss: 0.00007310
Iteration 50/1000 | Loss: 0.00002904
Iteration 51/1000 | Loss: 0.00002433
Iteration 52/1000 | Loss: 0.00014466
Iteration 53/1000 | Loss: 0.00008050
Iteration 54/1000 | Loss: 0.00002300
Iteration 55/1000 | Loss: 0.00002278
Iteration 56/1000 | Loss: 0.00002277
Iteration 57/1000 | Loss: 0.00002276
Iteration 58/1000 | Loss: 0.00002271
Iteration 59/1000 | Loss: 0.00002271
Iteration 60/1000 | Loss: 0.00002271
Iteration 61/1000 | Loss: 0.00002270
Iteration 62/1000 | Loss: 0.00002270
Iteration 63/1000 | Loss: 0.00004979
Iteration 64/1000 | Loss: 0.00002275
Iteration 65/1000 | Loss: 0.00002271
Iteration 66/1000 | Loss: 0.00002271
Iteration 67/1000 | Loss: 0.00002269
Iteration 68/1000 | Loss: 0.00002269
Iteration 69/1000 | Loss: 0.00002268
Iteration 70/1000 | Loss: 0.00002267
Iteration 71/1000 | Loss: 0.00002267
Iteration 72/1000 | Loss: 0.00002267
Iteration 73/1000 | Loss: 0.00002266
Iteration 74/1000 | Loss: 0.00002266
Iteration 75/1000 | Loss: 0.00002266
Iteration 76/1000 | Loss: 0.00002266
Iteration 77/1000 | Loss: 0.00002266
Iteration 78/1000 | Loss: 0.00002265
Iteration 79/1000 | Loss: 0.00002265
Iteration 80/1000 | Loss: 0.00002265
Iteration 81/1000 | Loss: 0.00002264
Iteration 82/1000 | Loss: 0.00002264
Iteration 83/1000 | Loss: 0.00002264
Iteration 84/1000 | Loss: 0.00002264
Iteration 85/1000 | Loss: 0.00002264
Iteration 86/1000 | Loss: 0.00002264
Iteration 87/1000 | Loss: 0.00002264
Iteration 88/1000 | Loss: 0.00002264
Iteration 89/1000 | Loss: 0.00002264
Iteration 90/1000 | Loss: 0.00002264
Iteration 91/1000 | Loss: 0.00002264
Iteration 92/1000 | Loss: 0.00002264
Iteration 93/1000 | Loss: 0.00002264
Iteration 94/1000 | Loss: 0.00002264
Iteration 95/1000 | Loss: 0.00002264
Iteration 96/1000 | Loss: 0.00002264
Iteration 97/1000 | Loss: 0.00002264
Iteration 98/1000 | Loss: 0.00002264
Iteration 99/1000 | Loss: 0.00002264
Iteration 100/1000 | Loss: 0.00002264
Iteration 101/1000 | Loss: 0.00002264
Iteration 102/1000 | Loss: 0.00002264
Iteration 103/1000 | Loss: 0.00002264
Iteration 104/1000 | Loss: 0.00002264
Iteration 105/1000 | Loss: 0.00002264
Iteration 106/1000 | Loss: 0.00002264
Iteration 107/1000 | Loss: 0.00002264
Iteration 108/1000 | Loss: 0.00002264
Iteration 109/1000 | Loss: 0.00002264
Iteration 110/1000 | Loss: 0.00002264
Iteration 111/1000 | Loss: 0.00002264
Iteration 112/1000 | Loss: 0.00002264
Iteration 113/1000 | Loss: 0.00002264
Iteration 114/1000 | Loss: 0.00002264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.263515671074856e-05, 2.263515671074856e-05, 2.263515671074856e-05, 2.263515671074856e-05, 2.263515671074856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.263515671074856e-05

Optimization complete. Final v2v error: 3.993682622909546 mm

Highest mean error: 9.35892391204834 mm for frame 164

Lowest mean error: 3.7374401092529297 mm for frame 91

Saving results

Total time: 91.73057675361633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014982
Iteration 2/25 | Loss: 0.00153456
Iteration 3/25 | Loss: 0.00107356
Iteration 4/25 | Loss: 0.00099145
Iteration 5/25 | Loss: 0.00097076
Iteration 6/25 | Loss: 0.00096514
Iteration 7/25 | Loss: 0.00096309
Iteration 8/25 | Loss: 0.00096239
Iteration 9/25 | Loss: 0.00096227
Iteration 10/25 | Loss: 0.00096227
Iteration 11/25 | Loss: 0.00096227
Iteration 12/25 | Loss: 0.00096227
Iteration 13/25 | Loss: 0.00096227
Iteration 14/25 | Loss: 0.00096227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009622742072679102, 0.0009622742072679102, 0.0009622742072679102, 0.0009622742072679102, 0.0009622742072679102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009622742072679102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.24607468
Iteration 2/25 | Loss: 0.00146452
Iteration 3/25 | Loss: 0.00146449
Iteration 4/25 | Loss: 0.00146449
Iteration 5/25 | Loss: 0.00146449
Iteration 6/25 | Loss: 0.00146449
Iteration 7/25 | Loss: 0.00146449
Iteration 8/25 | Loss: 0.00146449
Iteration 9/25 | Loss: 0.00146449
Iteration 10/25 | Loss: 0.00146449
Iteration 11/25 | Loss: 0.00146449
Iteration 12/25 | Loss: 0.00146449
Iteration 13/25 | Loss: 0.00146449
Iteration 14/25 | Loss: 0.00146449
Iteration 15/25 | Loss: 0.00146449
Iteration 16/25 | Loss: 0.00146449
Iteration 17/25 | Loss: 0.00146449
Iteration 18/25 | Loss: 0.00146449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001464489265345037, 0.001464489265345037, 0.001464489265345037, 0.001464489265345037, 0.001464489265345037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001464489265345037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146449
Iteration 2/1000 | Loss: 0.00006880
Iteration 3/1000 | Loss: 0.00004797
Iteration 4/1000 | Loss: 0.00004129
Iteration 5/1000 | Loss: 0.00003918
Iteration 6/1000 | Loss: 0.00003804
Iteration 7/1000 | Loss: 0.00003685
Iteration 8/1000 | Loss: 0.00003626
Iteration 9/1000 | Loss: 0.00003557
Iteration 10/1000 | Loss: 0.00003508
Iteration 11/1000 | Loss: 0.00003477
Iteration 12/1000 | Loss: 0.00003453
Iteration 13/1000 | Loss: 0.00003428
Iteration 14/1000 | Loss: 0.00003408
Iteration 15/1000 | Loss: 0.00003402
Iteration 16/1000 | Loss: 0.00003385
Iteration 17/1000 | Loss: 0.00003371
Iteration 18/1000 | Loss: 0.00003370
Iteration 19/1000 | Loss: 0.00003354
Iteration 20/1000 | Loss: 0.00003350
Iteration 21/1000 | Loss: 0.00003346
Iteration 22/1000 | Loss: 0.00003341
Iteration 23/1000 | Loss: 0.00003330
Iteration 24/1000 | Loss: 0.00003324
Iteration 25/1000 | Loss: 0.00003314
Iteration 26/1000 | Loss: 0.00003313
Iteration 27/1000 | Loss: 0.00003310
Iteration 28/1000 | Loss: 0.00003305
Iteration 29/1000 | Loss: 0.00003304
Iteration 30/1000 | Loss: 0.00003304
Iteration 31/1000 | Loss: 0.00003302
Iteration 32/1000 | Loss: 0.00003301
Iteration 33/1000 | Loss: 0.00003299
Iteration 34/1000 | Loss: 0.00003299
Iteration 35/1000 | Loss: 0.00003297
Iteration 36/1000 | Loss: 0.00003296
Iteration 37/1000 | Loss: 0.00003296
Iteration 38/1000 | Loss: 0.00003295
Iteration 39/1000 | Loss: 0.00003295
Iteration 40/1000 | Loss: 0.00003295
Iteration 41/1000 | Loss: 0.00003295
Iteration 42/1000 | Loss: 0.00003295
Iteration 43/1000 | Loss: 0.00003295
Iteration 44/1000 | Loss: 0.00003295
Iteration 45/1000 | Loss: 0.00003295
Iteration 46/1000 | Loss: 0.00003295
Iteration 47/1000 | Loss: 0.00003295
Iteration 48/1000 | Loss: 0.00003294
Iteration 49/1000 | Loss: 0.00003294
Iteration 50/1000 | Loss: 0.00003293
Iteration 51/1000 | Loss: 0.00003292
Iteration 52/1000 | Loss: 0.00003290
Iteration 53/1000 | Loss: 0.00003290
Iteration 54/1000 | Loss: 0.00003290
Iteration 55/1000 | Loss: 0.00003289
Iteration 56/1000 | Loss: 0.00003289
Iteration 57/1000 | Loss: 0.00003287
Iteration 58/1000 | Loss: 0.00003287
Iteration 59/1000 | Loss: 0.00003287
Iteration 60/1000 | Loss: 0.00003287
Iteration 61/1000 | Loss: 0.00003287
Iteration 62/1000 | Loss: 0.00003287
Iteration 63/1000 | Loss: 0.00003287
Iteration 64/1000 | Loss: 0.00003287
Iteration 65/1000 | Loss: 0.00003287
Iteration 66/1000 | Loss: 0.00003287
Iteration 67/1000 | Loss: 0.00003287
Iteration 68/1000 | Loss: 0.00003287
Iteration 69/1000 | Loss: 0.00003286
Iteration 70/1000 | Loss: 0.00003286
Iteration 71/1000 | Loss: 0.00003286
Iteration 72/1000 | Loss: 0.00003286
Iteration 73/1000 | Loss: 0.00003286
Iteration 74/1000 | Loss: 0.00003286
Iteration 75/1000 | Loss: 0.00003286
Iteration 76/1000 | Loss: 0.00003286
Iteration 77/1000 | Loss: 0.00003285
Iteration 78/1000 | Loss: 0.00003284
Iteration 79/1000 | Loss: 0.00003284
Iteration 80/1000 | Loss: 0.00003283
Iteration 81/1000 | Loss: 0.00003283
Iteration 82/1000 | Loss: 0.00003283
Iteration 83/1000 | Loss: 0.00003283
Iteration 84/1000 | Loss: 0.00003283
Iteration 85/1000 | Loss: 0.00003283
Iteration 86/1000 | Loss: 0.00003283
Iteration 87/1000 | Loss: 0.00003283
Iteration 88/1000 | Loss: 0.00003283
Iteration 89/1000 | Loss: 0.00003283
Iteration 90/1000 | Loss: 0.00003283
Iteration 91/1000 | Loss: 0.00003283
Iteration 92/1000 | Loss: 0.00003282
Iteration 93/1000 | Loss: 0.00003282
Iteration 94/1000 | Loss: 0.00003281
Iteration 95/1000 | Loss: 0.00003281
Iteration 96/1000 | Loss: 0.00003281
Iteration 97/1000 | Loss: 0.00003281
Iteration 98/1000 | Loss: 0.00003281
Iteration 99/1000 | Loss: 0.00003280
Iteration 100/1000 | Loss: 0.00003280
Iteration 101/1000 | Loss: 0.00003279
Iteration 102/1000 | Loss: 0.00003279
Iteration 103/1000 | Loss: 0.00003279
Iteration 104/1000 | Loss: 0.00003279
Iteration 105/1000 | Loss: 0.00003279
Iteration 106/1000 | Loss: 0.00003279
Iteration 107/1000 | Loss: 0.00003279
Iteration 108/1000 | Loss: 0.00003279
Iteration 109/1000 | Loss: 0.00003279
Iteration 110/1000 | Loss: 0.00003279
Iteration 111/1000 | Loss: 0.00003278
Iteration 112/1000 | Loss: 0.00003278
Iteration 113/1000 | Loss: 0.00003277
Iteration 114/1000 | Loss: 0.00003277
Iteration 115/1000 | Loss: 0.00003277
Iteration 116/1000 | Loss: 0.00003277
Iteration 117/1000 | Loss: 0.00003276
Iteration 118/1000 | Loss: 0.00003276
Iteration 119/1000 | Loss: 0.00003276
Iteration 120/1000 | Loss: 0.00003276
Iteration 121/1000 | Loss: 0.00003276
Iteration 122/1000 | Loss: 0.00003276
Iteration 123/1000 | Loss: 0.00003276
Iteration 124/1000 | Loss: 0.00003276
Iteration 125/1000 | Loss: 0.00003276
Iteration 126/1000 | Loss: 0.00003276
Iteration 127/1000 | Loss: 0.00003276
Iteration 128/1000 | Loss: 0.00003275
Iteration 129/1000 | Loss: 0.00003275
Iteration 130/1000 | Loss: 0.00003275
Iteration 131/1000 | Loss: 0.00003275
Iteration 132/1000 | Loss: 0.00003275
Iteration 133/1000 | Loss: 0.00003275
Iteration 134/1000 | Loss: 0.00003275
Iteration 135/1000 | Loss: 0.00003274
Iteration 136/1000 | Loss: 0.00003274
Iteration 137/1000 | Loss: 0.00003274
Iteration 138/1000 | Loss: 0.00003273
Iteration 139/1000 | Loss: 0.00003273
Iteration 140/1000 | Loss: 0.00003273
Iteration 141/1000 | Loss: 0.00003273
Iteration 142/1000 | Loss: 0.00003273
Iteration 143/1000 | Loss: 0.00003273
Iteration 144/1000 | Loss: 0.00003273
Iteration 145/1000 | Loss: 0.00003273
Iteration 146/1000 | Loss: 0.00003273
Iteration 147/1000 | Loss: 0.00003273
Iteration 148/1000 | Loss: 0.00003272
Iteration 149/1000 | Loss: 0.00003272
Iteration 150/1000 | Loss: 0.00003272
Iteration 151/1000 | Loss: 0.00003272
Iteration 152/1000 | Loss: 0.00003271
Iteration 153/1000 | Loss: 0.00003271
Iteration 154/1000 | Loss: 0.00003271
Iteration 155/1000 | Loss: 0.00003271
Iteration 156/1000 | Loss: 0.00003270
Iteration 157/1000 | Loss: 0.00003270
Iteration 158/1000 | Loss: 0.00003270
Iteration 159/1000 | Loss: 0.00003269
Iteration 160/1000 | Loss: 0.00003269
Iteration 161/1000 | Loss: 0.00003269
Iteration 162/1000 | Loss: 0.00003269
Iteration 163/1000 | Loss: 0.00003269
Iteration 164/1000 | Loss: 0.00003269
Iteration 165/1000 | Loss: 0.00003269
Iteration 166/1000 | Loss: 0.00003269
Iteration 167/1000 | Loss: 0.00003269
Iteration 168/1000 | Loss: 0.00003269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [3.269353328505531e-05, 3.269353328505531e-05, 3.269353328505531e-05, 3.269353328505531e-05, 3.269353328505531e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.269353328505531e-05

Optimization complete. Final v2v error: 4.515557765960693 mm

Highest mean error: 6.26292610168457 mm for frame 123

Lowest mean error: 3.443203926086426 mm for frame 233

Saving results

Total time: 64.39055752754211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935002
Iteration 2/25 | Loss: 0.00140864
Iteration 3/25 | Loss: 0.00102021
Iteration 4/25 | Loss: 0.00096087
Iteration 5/25 | Loss: 0.00094087
Iteration 6/25 | Loss: 0.00093203
Iteration 7/25 | Loss: 0.00094360
Iteration 8/25 | Loss: 0.00092621
Iteration 9/25 | Loss: 0.00092794
Iteration 10/25 | Loss: 0.00092657
Iteration 11/25 | Loss: 0.00092308
Iteration 12/25 | Loss: 0.00092180
Iteration 13/25 | Loss: 0.00092102
Iteration 14/25 | Loss: 0.00092084
Iteration 15/25 | Loss: 0.00092004
Iteration 16/25 | Loss: 0.00091923
Iteration 17/25 | Loss: 0.00091886
Iteration 18/25 | Loss: 0.00091872
Iteration 19/25 | Loss: 0.00091857
Iteration 20/25 | Loss: 0.00092286
Iteration 21/25 | Loss: 0.00092170
Iteration 22/25 | Loss: 0.00091750
Iteration 23/25 | Loss: 0.00091551
Iteration 24/25 | Loss: 0.00091355
Iteration 25/25 | Loss: 0.00091321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56431127
Iteration 2/25 | Loss: 0.00122584
Iteration 3/25 | Loss: 0.00122583
Iteration 4/25 | Loss: 0.00122583
Iteration 5/25 | Loss: 0.00122583
Iteration 6/25 | Loss: 0.00122583
Iteration 7/25 | Loss: 0.00122583
Iteration 8/25 | Loss: 0.00122583
Iteration 9/25 | Loss: 0.00122583
Iteration 10/25 | Loss: 0.00122583
Iteration 11/25 | Loss: 0.00122583
Iteration 12/25 | Loss: 0.00122583
Iteration 13/25 | Loss: 0.00122583
Iteration 14/25 | Loss: 0.00122583
Iteration 15/25 | Loss: 0.00122583
Iteration 16/25 | Loss: 0.00122583
Iteration 17/25 | Loss: 0.00122583
Iteration 18/25 | Loss: 0.00122583
Iteration 19/25 | Loss: 0.00122583
Iteration 20/25 | Loss: 0.00122583
Iteration 21/25 | Loss: 0.00122583
Iteration 22/25 | Loss: 0.00122583
Iteration 23/25 | Loss: 0.00122583
Iteration 24/25 | Loss: 0.00122583
Iteration 25/25 | Loss: 0.00122583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122583
Iteration 2/1000 | Loss: 0.00004473
Iteration 3/1000 | Loss: 0.00003477
Iteration 4/1000 | Loss: 0.00003147
Iteration 5/1000 | Loss: 0.00002969
Iteration 6/1000 | Loss: 0.00002862
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002744
Iteration 9/1000 | Loss: 0.00002722
Iteration 10/1000 | Loss: 0.00002696
Iteration 11/1000 | Loss: 0.00002680
Iteration 12/1000 | Loss: 0.00002664
Iteration 13/1000 | Loss: 0.00002649
Iteration 14/1000 | Loss: 0.00002647
Iteration 15/1000 | Loss: 0.00002643
Iteration 16/1000 | Loss: 0.00002642
Iteration 17/1000 | Loss: 0.00002642
Iteration 18/1000 | Loss: 0.00002641
Iteration 19/1000 | Loss: 0.00002641
Iteration 20/1000 | Loss: 0.00002639
Iteration 21/1000 | Loss: 0.00002639
Iteration 22/1000 | Loss: 0.00002638
Iteration 23/1000 | Loss: 0.00002638
Iteration 24/1000 | Loss: 0.00002638
Iteration 25/1000 | Loss: 0.00002638
Iteration 26/1000 | Loss: 0.00002638
Iteration 27/1000 | Loss: 0.00002638
Iteration 28/1000 | Loss: 0.00002638
Iteration 29/1000 | Loss: 0.00002638
Iteration 30/1000 | Loss: 0.00002638
Iteration 31/1000 | Loss: 0.00002637
Iteration 32/1000 | Loss: 0.00002637
Iteration 33/1000 | Loss: 0.00002635
Iteration 34/1000 | Loss: 0.00002634
Iteration 35/1000 | Loss: 0.00002634
Iteration 36/1000 | Loss: 0.00002633
Iteration 37/1000 | Loss: 0.00002633
Iteration 38/1000 | Loss: 0.00002633
Iteration 39/1000 | Loss: 0.00002630
Iteration 40/1000 | Loss: 0.00002630
Iteration 41/1000 | Loss: 0.00002630
Iteration 42/1000 | Loss: 0.00002630
Iteration 43/1000 | Loss: 0.00002630
Iteration 44/1000 | Loss: 0.00002630
Iteration 45/1000 | Loss: 0.00002630
Iteration 46/1000 | Loss: 0.00002630
Iteration 47/1000 | Loss: 0.00002629
Iteration 48/1000 | Loss: 0.00002629
Iteration 49/1000 | Loss: 0.00002627
Iteration 50/1000 | Loss: 0.00002627
Iteration 51/1000 | Loss: 0.00002627
Iteration 52/1000 | Loss: 0.00002627
Iteration 53/1000 | Loss: 0.00002627
Iteration 54/1000 | Loss: 0.00002627
Iteration 55/1000 | Loss: 0.00002626
Iteration 56/1000 | Loss: 0.00002626
Iteration 57/1000 | Loss: 0.00002626
Iteration 58/1000 | Loss: 0.00002626
Iteration 59/1000 | Loss: 0.00002626
Iteration 60/1000 | Loss: 0.00002626
Iteration 61/1000 | Loss: 0.00002625
Iteration 62/1000 | Loss: 0.00002625
Iteration 63/1000 | Loss: 0.00002625
Iteration 64/1000 | Loss: 0.00002624
Iteration 65/1000 | Loss: 0.00002624
Iteration 66/1000 | Loss: 0.00002624
Iteration 67/1000 | Loss: 0.00002623
Iteration 68/1000 | Loss: 0.00002623
Iteration 69/1000 | Loss: 0.00002623
Iteration 70/1000 | Loss: 0.00002623
Iteration 71/1000 | Loss: 0.00002622
Iteration 72/1000 | Loss: 0.00002622
Iteration 73/1000 | Loss: 0.00002622
Iteration 74/1000 | Loss: 0.00002621
Iteration 75/1000 | Loss: 0.00002621
Iteration 76/1000 | Loss: 0.00002621
Iteration 77/1000 | Loss: 0.00002621
Iteration 78/1000 | Loss: 0.00002621
Iteration 79/1000 | Loss: 0.00002620
Iteration 80/1000 | Loss: 0.00002620
Iteration 81/1000 | Loss: 0.00002620
Iteration 82/1000 | Loss: 0.00002620
Iteration 83/1000 | Loss: 0.00002620
Iteration 84/1000 | Loss: 0.00002620
Iteration 85/1000 | Loss: 0.00002620
Iteration 86/1000 | Loss: 0.00002619
Iteration 87/1000 | Loss: 0.00002619
Iteration 88/1000 | Loss: 0.00002619
Iteration 89/1000 | Loss: 0.00002619
Iteration 90/1000 | Loss: 0.00002619
Iteration 91/1000 | Loss: 0.00002618
Iteration 92/1000 | Loss: 0.00002618
Iteration 93/1000 | Loss: 0.00002618
Iteration 94/1000 | Loss: 0.00002618
Iteration 95/1000 | Loss: 0.00002618
Iteration 96/1000 | Loss: 0.00002618
Iteration 97/1000 | Loss: 0.00002618
Iteration 98/1000 | Loss: 0.00002618
Iteration 99/1000 | Loss: 0.00002618
Iteration 100/1000 | Loss: 0.00002618
Iteration 101/1000 | Loss: 0.00002618
Iteration 102/1000 | Loss: 0.00002618
Iteration 103/1000 | Loss: 0.00002618
Iteration 104/1000 | Loss: 0.00002618
Iteration 105/1000 | Loss: 0.00002617
Iteration 106/1000 | Loss: 0.00002617
Iteration 107/1000 | Loss: 0.00002617
Iteration 108/1000 | Loss: 0.00002617
Iteration 109/1000 | Loss: 0.00002617
Iteration 110/1000 | Loss: 0.00002617
Iteration 111/1000 | Loss: 0.00002617
Iteration 112/1000 | Loss: 0.00002616
Iteration 113/1000 | Loss: 0.00002616
Iteration 114/1000 | Loss: 0.00002616
Iteration 115/1000 | Loss: 0.00002616
Iteration 116/1000 | Loss: 0.00002616
Iteration 117/1000 | Loss: 0.00002616
Iteration 118/1000 | Loss: 0.00002616
Iteration 119/1000 | Loss: 0.00002616
Iteration 120/1000 | Loss: 0.00002616
Iteration 121/1000 | Loss: 0.00002616
Iteration 122/1000 | Loss: 0.00002615
Iteration 123/1000 | Loss: 0.00002615
Iteration 124/1000 | Loss: 0.00002615
Iteration 125/1000 | Loss: 0.00002615
Iteration 126/1000 | Loss: 0.00002615
Iteration 127/1000 | Loss: 0.00002615
Iteration 128/1000 | Loss: 0.00002615
Iteration 129/1000 | Loss: 0.00002615
Iteration 130/1000 | Loss: 0.00002615
Iteration 131/1000 | Loss: 0.00002614
Iteration 132/1000 | Loss: 0.00002614
Iteration 133/1000 | Loss: 0.00002614
Iteration 134/1000 | Loss: 0.00002614
Iteration 135/1000 | Loss: 0.00002614
Iteration 136/1000 | Loss: 0.00002614
Iteration 137/1000 | Loss: 0.00002614
Iteration 138/1000 | Loss: 0.00002614
Iteration 139/1000 | Loss: 0.00002614
Iteration 140/1000 | Loss: 0.00002614
Iteration 141/1000 | Loss: 0.00002613
Iteration 142/1000 | Loss: 0.00002613
Iteration 143/1000 | Loss: 0.00002613
Iteration 144/1000 | Loss: 0.00002613
Iteration 145/1000 | Loss: 0.00002613
Iteration 146/1000 | Loss: 0.00002613
Iteration 147/1000 | Loss: 0.00002613
Iteration 148/1000 | Loss: 0.00002613
Iteration 149/1000 | Loss: 0.00002613
Iteration 150/1000 | Loss: 0.00002613
Iteration 151/1000 | Loss: 0.00002613
Iteration 152/1000 | Loss: 0.00002613
Iteration 153/1000 | Loss: 0.00002613
Iteration 154/1000 | Loss: 0.00002613
Iteration 155/1000 | Loss: 0.00002613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.6132216589758173e-05, 2.6132216589758173e-05, 2.6132216589758173e-05, 2.6132216589758173e-05, 2.6132216589758173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6132216589758173e-05

Optimization complete. Final v2v error: 4.208499431610107 mm

Highest mean error: 5.4830145835876465 mm for frame 223

Lowest mean error: 3.8210365772247314 mm for frame 231

Saving results

Total time: 86.44959902763367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020672
Iteration 2/25 | Loss: 0.00291449
Iteration 3/25 | Loss: 0.00175548
Iteration 4/25 | Loss: 0.00149634
Iteration 5/25 | Loss: 0.00133777
Iteration 6/25 | Loss: 0.00118420
Iteration 7/25 | Loss: 0.00105185
Iteration 8/25 | Loss: 0.00101074
Iteration 9/25 | Loss: 0.00098311
Iteration 10/25 | Loss: 0.00097122
Iteration 11/25 | Loss: 0.00095743
Iteration 12/25 | Loss: 0.00094625
Iteration 13/25 | Loss: 0.00093600
Iteration 14/25 | Loss: 0.00095033
Iteration 15/25 | Loss: 0.00091332
Iteration 16/25 | Loss: 0.00090096
Iteration 17/25 | Loss: 0.00088691
Iteration 18/25 | Loss: 0.00088421
Iteration 19/25 | Loss: 0.00088266
Iteration 20/25 | Loss: 0.00088318
Iteration 21/25 | Loss: 0.00088238
Iteration 22/25 | Loss: 0.00088233
Iteration 23/25 | Loss: 0.00088233
Iteration 24/25 | Loss: 0.00088233
Iteration 25/25 | Loss: 0.00088233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60378993
Iteration 2/25 | Loss: 0.00160517
Iteration 3/25 | Loss: 0.00151029
Iteration 4/25 | Loss: 0.00151029
Iteration 5/25 | Loss: 0.00151029
Iteration 6/25 | Loss: 0.00151029
Iteration 7/25 | Loss: 0.00151029
Iteration 8/25 | Loss: 0.00151029
Iteration 9/25 | Loss: 0.00151029
Iteration 10/25 | Loss: 0.00151029
Iteration 11/25 | Loss: 0.00151029
Iteration 12/25 | Loss: 0.00151029
Iteration 13/25 | Loss: 0.00151029
Iteration 14/25 | Loss: 0.00151029
Iteration 15/25 | Loss: 0.00151029
Iteration 16/25 | Loss: 0.00151029
Iteration 17/25 | Loss: 0.00151029
Iteration 18/25 | Loss: 0.00151029
Iteration 19/25 | Loss: 0.00151029
Iteration 20/25 | Loss: 0.00151029
Iteration 21/25 | Loss: 0.00151029
Iteration 22/25 | Loss: 0.00151029
Iteration 23/25 | Loss: 0.00151029
Iteration 24/25 | Loss: 0.00151029
Iteration 25/25 | Loss: 0.00151029
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001510285190306604, 0.001510285190306604, 0.001510285190306604, 0.001510285190306604, 0.001510285190306604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001510285190306604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151029
Iteration 2/1000 | Loss: 0.00015860
Iteration 3/1000 | Loss: 0.00014662
Iteration 4/1000 | Loss: 0.00008209
Iteration 5/1000 | Loss: 0.00005863
Iteration 6/1000 | Loss: 0.00010922
Iteration 7/1000 | Loss: 0.00006086
Iteration 8/1000 | Loss: 0.00004505
Iteration 9/1000 | Loss: 0.00003238
Iteration 10/1000 | Loss: 0.00011633
Iteration 11/1000 | Loss: 0.00085830
Iteration 12/1000 | Loss: 0.00003008
Iteration 13/1000 | Loss: 0.00004038
Iteration 14/1000 | Loss: 0.00004735
Iteration 15/1000 | Loss: 0.00002753
Iteration 16/1000 | Loss: 0.00002693
Iteration 17/1000 | Loss: 0.00005587
Iteration 18/1000 | Loss: 0.00004109
Iteration 19/1000 | Loss: 0.00002856
Iteration 20/1000 | Loss: 0.00004025
Iteration 21/1000 | Loss: 0.00002618
Iteration 22/1000 | Loss: 0.00002617
Iteration 23/1000 | Loss: 0.00002604
Iteration 24/1000 | Loss: 0.00003517
Iteration 25/1000 | Loss: 0.00002594
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002592
Iteration 28/1000 | Loss: 0.00002592
Iteration 29/1000 | Loss: 0.00002591
Iteration 30/1000 | Loss: 0.00002591
Iteration 31/1000 | Loss: 0.00002591
Iteration 32/1000 | Loss: 0.00002590
Iteration 33/1000 | Loss: 0.00002588
Iteration 34/1000 | Loss: 0.00002587
Iteration 35/1000 | Loss: 0.00002587
Iteration 36/1000 | Loss: 0.00002585
Iteration 37/1000 | Loss: 0.00002585
Iteration 38/1000 | Loss: 0.00002584
Iteration 39/1000 | Loss: 0.00002584
Iteration 40/1000 | Loss: 0.00002583
Iteration 41/1000 | Loss: 0.00002583
Iteration 42/1000 | Loss: 0.00002583
Iteration 43/1000 | Loss: 0.00002583
Iteration 44/1000 | Loss: 0.00002583
Iteration 45/1000 | Loss: 0.00002583
Iteration 46/1000 | Loss: 0.00002583
Iteration 47/1000 | Loss: 0.00002582
Iteration 48/1000 | Loss: 0.00002582
Iteration 49/1000 | Loss: 0.00002582
Iteration 50/1000 | Loss: 0.00002582
Iteration 51/1000 | Loss: 0.00002582
Iteration 52/1000 | Loss: 0.00002582
Iteration 53/1000 | Loss: 0.00002582
Iteration 54/1000 | Loss: 0.00002582
Iteration 55/1000 | Loss: 0.00002582
Iteration 56/1000 | Loss: 0.00002582
Iteration 57/1000 | Loss: 0.00002582
Iteration 58/1000 | Loss: 0.00002582
Iteration 59/1000 | Loss: 0.00002582
Iteration 60/1000 | Loss: 0.00002582
Iteration 61/1000 | Loss: 0.00002582
Iteration 62/1000 | Loss: 0.00002582
Iteration 63/1000 | Loss: 0.00002582
Iteration 64/1000 | Loss: 0.00002582
Iteration 65/1000 | Loss: 0.00002582
Iteration 66/1000 | Loss: 0.00002582
Iteration 67/1000 | Loss: 0.00002582
Iteration 68/1000 | Loss: 0.00002582
Iteration 69/1000 | Loss: 0.00002582
Iteration 70/1000 | Loss: 0.00002582
Iteration 71/1000 | Loss: 0.00002582
Iteration 72/1000 | Loss: 0.00002582
Iteration 73/1000 | Loss: 0.00002582
Iteration 74/1000 | Loss: 0.00002582
Iteration 75/1000 | Loss: 0.00002582
Iteration 76/1000 | Loss: 0.00002582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [2.5819590518949553e-05, 2.5819590518949553e-05, 2.5819590518949553e-05, 2.5819590518949553e-05, 2.5819590518949553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5819590518949553e-05

Optimization complete. Final v2v error: 4.174790859222412 mm

Highest mean error: 4.8912739753723145 mm for frame 171

Lowest mean error: 3.579617500305176 mm for frame 80

Saving results

Total time: 86.60042595863342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395352
Iteration 2/25 | Loss: 0.00085252
Iteration 3/25 | Loss: 0.00076255
Iteration 4/25 | Loss: 0.00074974
Iteration 5/25 | Loss: 0.00074594
Iteration 6/25 | Loss: 0.00074547
Iteration 7/25 | Loss: 0.00074547
Iteration 8/25 | Loss: 0.00074547
Iteration 9/25 | Loss: 0.00074547
Iteration 10/25 | Loss: 0.00074547
Iteration 11/25 | Loss: 0.00074547
Iteration 12/25 | Loss: 0.00074547
Iteration 13/25 | Loss: 0.00074547
Iteration 14/25 | Loss: 0.00074547
Iteration 15/25 | Loss: 0.00074547
Iteration 16/25 | Loss: 0.00074547
Iteration 17/25 | Loss: 0.00074547
Iteration 18/25 | Loss: 0.00074547
Iteration 19/25 | Loss: 0.00074547
Iteration 20/25 | Loss: 0.00074547
Iteration 21/25 | Loss: 0.00074547
Iteration 22/25 | Loss: 0.00074547
Iteration 23/25 | Loss: 0.00074547
Iteration 24/25 | Loss: 0.00074547
Iteration 25/25 | Loss: 0.00074547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58499932
Iteration 2/25 | Loss: 0.00116940
Iteration 3/25 | Loss: 0.00116940
Iteration 4/25 | Loss: 0.00116940
Iteration 5/25 | Loss: 0.00116940
Iteration 6/25 | Loss: 0.00116940
Iteration 7/25 | Loss: 0.00116940
Iteration 8/25 | Loss: 0.00116940
Iteration 9/25 | Loss: 0.00116940
Iteration 10/25 | Loss: 0.00116940
Iteration 11/25 | Loss: 0.00116940
Iteration 12/25 | Loss: 0.00116940
Iteration 13/25 | Loss: 0.00116940
Iteration 14/25 | Loss: 0.00116940
Iteration 15/25 | Loss: 0.00116940
Iteration 16/25 | Loss: 0.00116940
Iteration 17/25 | Loss: 0.00116940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001169398776255548, 0.001169398776255548, 0.001169398776255548, 0.001169398776255548, 0.001169398776255548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001169398776255548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116940
Iteration 2/1000 | Loss: 0.00002242
Iteration 3/1000 | Loss: 0.00001581
Iteration 4/1000 | Loss: 0.00001474
Iteration 5/1000 | Loss: 0.00001386
Iteration 6/1000 | Loss: 0.00001341
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001315
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001285
Iteration 11/1000 | Loss: 0.00001275
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001267
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001260
Iteration 28/1000 | Loss: 0.00001259
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001253
Iteration 32/1000 | Loss: 0.00001252
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001250
Iteration 35/1000 | Loss: 0.00001250
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001250
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001249
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001239
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001229
Iteration 81/1000 | Loss: 0.00001229
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001228
Iteration 85/1000 | Loss: 0.00001228
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001226
Iteration 100/1000 | Loss: 0.00001226
Iteration 101/1000 | Loss: 0.00001226
Iteration 102/1000 | Loss: 0.00001226
Iteration 103/1000 | Loss: 0.00001226
Iteration 104/1000 | Loss: 0.00001226
Iteration 105/1000 | Loss: 0.00001226
Iteration 106/1000 | Loss: 0.00001226
Iteration 107/1000 | Loss: 0.00001226
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001225
Iteration 117/1000 | Loss: 0.00001225
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001224
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001224
Iteration 136/1000 | Loss: 0.00001224
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.2242844604770653e-05, 1.2242844604770653e-05, 1.2242844604770653e-05, 1.2242844604770653e-05, 1.2242844604770653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2242844604770653e-05

Optimization complete. Final v2v error: 2.973550796508789 mm

Highest mean error: 3.09962797164917 mm for frame 131

Lowest mean error: 2.8564679622650146 mm for frame 224

Saving results

Total time: 37.70679521560669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385974
Iteration 2/25 | Loss: 0.00083256
Iteration 3/25 | Loss: 0.00075798
Iteration 4/25 | Loss: 0.00074301
Iteration 5/25 | Loss: 0.00074083
Iteration 6/25 | Loss: 0.00074064
Iteration 7/25 | Loss: 0.00074064
Iteration 8/25 | Loss: 0.00074064
Iteration 9/25 | Loss: 0.00074064
Iteration 10/25 | Loss: 0.00074064
Iteration 11/25 | Loss: 0.00074064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000740644580218941, 0.000740644580218941, 0.000740644580218941, 0.000740644580218941, 0.000740644580218941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000740644580218941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58740759
Iteration 2/25 | Loss: 0.00115283
Iteration 3/25 | Loss: 0.00115283
Iteration 4/25 | Loss: 0.00115283
Iteration 5/25 | Loss: 0.00115283
Iteration 6/25 | Loss: 0.00115283
Iteration 7/25 | Loss: 0.00115283
Iteration 8/25 | Loss: 0.00115283
Iteration 9/25 | Loss: 0.00115283
Iteration 10/25 | Loss: 0.00115283
Iteration 11/25 | Loss: 0.00115283
Iteration 12/25 | Loss: 0.00115283
Iteration 13/25 | Loss: 0.00115283
Iteration 14/25 | Loss: 0.00115283
Iteration 15/25 | Loss: 0.00115283
Iteration 16/25 | Loss: 0.00115283
Iteration 17/25 | Loss: 0.00115283
Iteration 18/25 | Loss: 0.00115283
Iteration 19/25 | Loss: 0.00115283
Iteration 20/25 | Loss: 0.00115283
Iteration 21/25 | Loss: 0.00115283
Iteration 22/25 | Loss: 0.00115283
Iteration 23/25 | Loss: 0.00115283
Iteration 24/25 | Loss: 0.00115283
Iteration 25/25 | Loss: 0.00115283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115283
Iteration 2/1000 | Loss: 0.00002136
Iteration 3/1000 | Loss: 0.00001601
Iteration 4/1000 | Loss: 0.00001464
Iteration 5/1000 | Loss: 0.00001411
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001346
Iteration 8/1000 | Loss: 0.00001325
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001307
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001300
Iteration 15/1000 | Loss: 0.00001300
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00001300
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001295
Iteration 23/1000 | Loss: 0.00001294
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001281
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001281
Iteration 31/1000 | Loss: 0.00001281
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001281
Iteration 34/1000 | Loss: 0.00001281
Iteration 35/1000 | Loss: 0.00001280
Iteration 36/1000 | Loss: 0.00001280
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001280
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001280
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001277
Iteration 44/1000 | Loss: 0.00001277
Iteration 45/1000 | Loss: 0.00001276
Iteration 46/1000 | Loss: 0.00001276
Iteration 47/1000 | Loss: 0.00001276
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001275
Iteration 50/1000 | Loss: 0.00001275
Iteration 51/1000 | Loss: 0.00001274
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001273
Iteration 54/1000 | Loss: 0.00001271
Iteration 55/1000 | Loss: 0.00001270
Iteration 56/1000 | Loss: 0.00001270
Iteration 57/1000 | Loss: 0.00001269
Iteration 58/1000 | Loss: 0.00001269
Iteration 59/1000 | Loss: 0.00001268
Iteration 60/1000 | Loss: 0.00001268
Iteration 61/1000 | Loss: 0.00001268
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001267
Iteration 64/1000 | Loss: 0.00001266
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001264
Iteration 68/1000 | Loss: 0.00001264
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001258
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001255
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001254
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001254
Iteration 89/1000 | Loss: 0.00001254
Iteration 90/1000 | Loss: 0.00001254
Iteration 91/1000 | Loss: 0.00001254
Iteration 92/1000 | Loss: 0.00001254
Iteration 93/1000 | Loss: 0.00001254
Iteration 94/1000 | Loss: 0.00001254
Iteration 95/1000 | Loss: 0.00001254
Iteration 96/1000 | Loss: 0.00001254
Iteration 97/1000 | Loss: 0.00001254
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001253
Iteration 100/1000 | Loss: 0.00001253
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001253
Iteration 105/1000 | Loss: 0.00001253
Iteration 106/1000 | Loss: 0.00001253
Iteration 107/1000 | Loss: 0.00001253
Iteration 108/1000 | Loss: 0.00001253
Iteration 109/1000 | Loss: 0.00001253
Iteration 110/1000 | Loss: 0.00001253
Iteration 111/1000 | Loss: 0.00001253
Iteration 112/1000 | Loss: 0.00001253
Iteration 113/1000 | Loss: 0.00001253
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.2534967027022503e-05, 1.2534967027022503e-05, 1.2534967027022503e-05, 1.2534967027022503e-05, 1.2534967027022503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2534967027022503e-05

Optimization complete. Final v2v error: 3.0026402473449707 mm

Highest mean error: 3.085224151611328 mm for frame 179

Lowest mean error: 2.925523281097412 mm for frame 63

Saving results

Total time: 34.20346927642822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862810
Iteration 2/25 | Loss: 0.00138182
Iteration 3/25 | Loss: 0.00091093
Iteration 4/25 | Loss: 0.00086620
Iteration 5/25 | Loss: 0.00085805
Iteration 6/25 | Loss: 0.00085547
Iteration 7/25 | Loss: 0.00085496
Iteration 8/25 | Loss: 0.00085496
Iteration 9/25 | Loss: 0.00085496
Iteration 10/25 | Loss: 0.00085496
Iteration 11/25 | Loss: 0.00085496
Iteration 12/25 | Loss: 0.00085496
Iteration 13/25 | Loss: 0.00085496
Iteration 14/25 | Loss: 0.00085496
Iteration 15/25 | Loss: 0.00085496
Iteration 16/25 | Loss: 0.00085496
Iteration 17/25 | Loss: 0.00085496
Iteration 18/25 | Loss: 0.00085496
Iteration 19/25 | Loss: 0.00085496
Iteration 20/25 | Loss: 0.00085496
Iteration 21/25 | Loss: 0.00085496
Iteration 22/25 | Loss: 0.00085496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00085496308747679, 0.00085496308747679, 0.00085496308747679, 0.00085496308747679, 0.00085496308747679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00085496308747679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02627206
Iteration 2/25 | Loss: 0.00135172
Iteration 3/25 | Loss: 0.00135168
Iteration 4/25 | Loss: 0.00135168
Iteration 5/25 | Loss: 0.00135168
Iteration 6/25 | Loss: 0.00135168
Iteration 7/25 | Loss: 0.00135168
Iteration 8/25 | Loss: 0.00135168
Iteration 9/25 | Loss: 0.00135168
Iteration 10/25 | Loss: 0.00135168
Iteration 11/25 | Loss: 0.00135168
Iteration 12/25 | Loss: 0.00135168
Iteration 13/25 | Loss: 0.00135168
Iteration 14/25 | Loss: 0.00135168
Iteration 15/25 | Loss: 0.00135168
Iteration 16/25 | Loss: 0.00135168
Iteration 17/25 | Loss: 0.00135168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001351678860373795, 0.001351678860373795, 0.001351678860373795, 0.001351678860373795, 0.001351678860373795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001351678860373795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135168
Iteration 2/1000 | Loss: 0.00003454
Iteration 3/1000 | Loss: 0.00002028
Iteration 4/1000 | Loss: 0.00001799
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001641
Iteration 7/1000 | Loss: 0.00001599
Iteration 8/1000 | Loss: 0.00001571
Iteration 9/1000 | Loss: 0.00001542
Iteration 10/1000 | Loss: 0.00001525
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001521
Iteration 13/1000 | Loss: 0.00001511
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001490
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001466
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001466
Iteration 30/1000 | Loss: 0.00001466
Iteration 31/1000 | Loss: 0.00001466
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001465
Iteration 34/1000 | Loss: 0.00001465
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001463
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001463
Iteration 43/1000 | Loss: 0.00001463
Iteration 44/1000 | Loss: 0.00001462
Iteration 45/1000 | Loss: 0.00001462
Iteration 46/1000 | Loss: 0.00001461
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001461
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001461
Iteration 51/1000 | Loss: 0.00001461
Iteration 52/1000 | Loss: 0.00001460
Iteration 53/1000 | Loss: 0.00001460
Iteration 54/1000 | Loss: 0.00001460
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001459
Iteration 57/1000 | Loss: 0.00001459
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001458
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001455
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001454
Iteration 84/1000 | Loss: 0.00001454
Iteration 85/1000 | Loss: 0.00001454
Iteration 86/1000 | Loss: 0.00001454
Iteration 87/1000 | Loss: 0.00001454
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001454
Iteration 103/1000 | Loss: 0.00001454
Iteration 104/1000 | Loss: 0.00001454
Iteration 105/1000 | Loss: 0.00001454
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001454
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001454
Iteration 111/1000 | Loss: 0.00001454
Iteration 112/1000 | Loss: 0.00001454
Iteration 113/1000 | Loss: 0.00001454
Iteration 114/1000 | Loss: 0.00001454
Iteration 115/1000 | Loss: 0.00001454
Iteration 116/1000 | Loss: 0.00001454
Iteration 117/1000 | Loss: 0.00001454
Iteration 118/1000 | Loss: 0.00001454
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001454
Iteration 128/1000 | Loss: 0.00001454
Iteration 129/1000 | Loss: 0.00001454
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001454
Iteration 138/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.454235734854592e-05, 1.454235734854592e-05, 1.454235734854592e-05, 1.454235734854592e-05, 1.454235734854592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.454235734854592e-05

Optimization complete. Final v2v error: 3.218759536743164 mm

Highest mean error: 3.537099838256836 mm for frame 43

Lowest mean error: 2.7983148097991943 mm for frame 5

Saving results

Total time: 40.578025817871094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446662
Iteration 2/25 | Loss: 0.00098835
Iteration 3/25 | Loss: 0.00081173
Iteration 4/25 | Loss: 0.00078136
Iteration 5/25 | Loss: 0.00077191
Iteration 6/25 | Loss: 0.00076940
Iteration 7/25 | Loss: 0.00076860
Iteration 8/25 | Loss: 0.00076858
Iteration 9/25 | Loss: 0.00076858
Iteration 10/25 | Loss: 0.00076858
Iteration 11/25 | Loss: 0.00076858
Iteration 12/25 | Loss: 0.00076858
Iteration 13/25 | Loss: 0.00076858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007685828604735434, 0.0007685828604735434, 0.0007685828604735434, 0.0007685828604735434, 0.0007685828604735434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007685828604735434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61733663
Iteration 2/25 | Loss: 0.00130320
Iteration 3/25 | Loss: 0.00130319
Iteration 4/25 | Loss: 0.00130319
Iteration 5/25 | Loss: 0.00130319
Iteration 6/25 | Loss: 0.00130319
Iteration 7/25 | Loss: 0.00130319
Iteration 8/25 | Loss: 0.00130319
Iteration 9/25 | Loss: 0.00130319
Iteration 10/25 | Loss: 0.00130319
Iteration 11/25 | Loss: 0.00130319
Iteration 12/25 | Loss: 0.00130319
Iteration 13/25 | Loss: 0.00130319
Iteration 14/25 | Loss: 0.00130319
Iteration 15/25 | Loss: 0.00130319
Iteration 16/25 | Loss: 0.00130319
Iteration 17/25 | Loss: 0.00130319
Iteration 18/25 | Loss: 0.00130319
Iteration 19/25 | Loss: 0.00130319
Iteration 20/25 | Loss: 0.00130319
Iteration 21/25 | Loss: 0.00130319
Iteration 22/25 | Loss: 0.00130319
Iteration 23/25 | Loss: 0.00130319
Iteration 24/25 | Loss: 0.00130319
Iteration 25/25 | Loss: 0.00130319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130319
Iteration 2/1000 | Loss: 0.00003284
Iteration 3/1000 | Loss: 0.00002029
Iteration 4/1000 | Loss: 0.00001736
Iteration 5/1000 | Loss: 0.00001619
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001486
Iteration 8/1000 | Loss: 0.00001453
Iteration 9/1000 | Loss: 0.00001431
Iteration 10/1000 | Loss: 0.00001412
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001385
Iteration 20/1000 | Loss: 0.00001384
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001377
Iteration 25/1000 | Loss: 0.00001376
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001371
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001370
Iteration 42/1000 | Loss: 0.00001370
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001369
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001368
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001367
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001366
Iteration 59/1000 | Loss: 0.00001366
Iteration 60/1000 | Loss: 0.00001366
Iteration 61/1000 | Loss: 0.00001366
Iteration 62/1000 | Loss: 0.00001366
Iteration 63/1000 | Loss: 0.00001366
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001365
Iteration 67/1000 | Loss: 0.00001365
Iteration 68/1000 | Loss: 0.00001365
Iteration 69/1000 | Loss: 0.00001365
Iteration 70/1000 | Loss: 0.00001365
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001364
Iteration 73/1000 | Loss: 0.00001364
Iteration 74/1000 | Loss: 0.00001364
Iteration 75/1000 | Loss: 0.00001363
Iteration 76/1000 | Loss: 0.00001363
Iteration 77/1000 | Loss: 0.00001363
Iteration 78/1000 | Loss: 0.00001363
Iteration 79/1000 | Loss: 0.00001363
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001363
Iteration 83/1000 | Loss: 0.00001362
Iteration 84/1000 | Loss: 0.00001362
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001361
Iteration 90/1000 | Loss: 0.00001361
Iteration 91/1000 | Loss: 0.00001361
Iteration 92/1000 | Loss: 0.00001361
Iteration 93/1000 | Loss: 0.00001361
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001360
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001360
Iteration 101/1000 | Loss: 0.00001360
Iteration 102/1000 | Loss: 0.00001360
Iteration 103/1000 | Loss: 0.00001360
Iteration 104/1000 | Loss: 0.00001360
Iteration 105/1000 | Loss: 0.00001360
Iteration 106/1000 | Loss: 0.00001360
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001359
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001359
Iteration 114/1000 | Loss: 0.00001359
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001358
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001358
Iteration 131/1000 | Loss: 0.00001358
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001358
Iteration 136/1000 | Loss: 0.00001358
Iteration 137/1000 | Loss: 0.00001358
Iteration 138/1000 | Loss: 0.00001358
Iteration 139/1000 | Loss: 0.00001358
Iteration 140/1000 | Loss: 0.00001358
Iteration 141/1000 | Loss: 0.00001358
Iteration 142/1000 | Loss: 0.00001358
Iteration 143/1000 | Loss: 0.00001358
Iteration 144/1000 | Loss: 0.00001358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.3576691344496794e-05, 1.3576691344496794e-05, 1.3576691344496794e-05, 1.3576691344496794e-05, 1.3576691344496794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3576691344496794e-05

Optimization complete. Final v2v error: 3.0739734172821045 mm

Highest mean error: 4.023435592651367 mm for frame 160

Lowest mean error: 2.7613179683685303 mm for frame 168

Saving results

Total time: 38.80992412567139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500029
Iteration 2/25 | Loss: 0.00108171
Iteration 3/25 | Loss: 0.00081770
Iteration 4/25 | Loss: 0.00078779
Iteration 5/25 | Loss: 0.00077749
Iteration 6/25 | Loss: 0.00077498
Iteration 7/25 | Loss: 0.00077418
Iteration 8/25 | Loss: 0.00077411
Iteration 9/25 | Loss: 0.00077411
Iteration 10/25 | Loss: 0.00077411
Iteration 11/25 | Loss: 0.00077411
Iteration 12/25 | Loss: 0.00077411
Iteration 13/25 | Loss: 0.00077411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007741061272099614, 0.0007741061272099614, 0.0007741061272099614, 0.0007741061272099614, 0.0007741061272099614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007741061272099614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71086562
Iteration 2/25 | Loss: 0.00107269
Iteration 3/25 | Loss: 0.00107267
Iteration 4/25 | Loss: 0.00107267
Iteration 5/25 | Loss: 0.00107267
Iteration 6/25 | Loss: 0.00107267
Iteration 7/25 | Loss: 0.00107267
Iteration 8/25 | Loss: 0.00107267
Iteration 9/25 | Loss: 0.00107267
Iteration 10/25 | Loss: 0.00107266
Iteration 11/25 | Loss: 0.00107266
Iteration 12/25 | Loss: 0.00107266
Iteration 13/25 | Loss: 0.00107266
Iteration 14/25 | Loss: 0.00107266
Iteration 15/25 | Loss: 0.00107266
Iteration 16/25 | Loss: 0.00107266
Iteration 17/25 | Loss: 0.00107266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010726646287366748, 0.0010726646287366748, 0.0010726646287366748, 0.0010726646287366748, 0.0010726646287366748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010726646287366748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107266
Iteration 2/1000 | Loss: 0.00003001
Iteration 3/1000 | Loss: 0.00002026
Iteration 4/1000 | Loss: 0.00001820
Iteration 5/1000 | Loss: 0.00001723
Iteration 6/1000 | Loss: 0.00001657
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001520
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001507
Iteration 16/1000 | Loss: 0.00001507
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001504
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001503
Iteration 25/1000 | Loss: 0.00001503
Iteration 26/1000 | Loss: 0.00001503
Iteration 27/1000 | Loss: 0.00001502
Iteration 28/1000 | Loss: 0.00001502
Iteration 29/1000 | Loss: 0.00001502
Iteration 30/1000 | Loss: 0.00001501
Iteration 31/1000 | Loss: 0.00001501
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001501
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001500
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001499
Iteration 42/1000 | Loss: 0.00001498
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001498
Iteration 45/1000 | Loss: 0.00001498
Iteration 46/1000 | Loss: 0.00001498
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001498
Iteration 49/1000 | Loss: 0.00001498
Iteration 50/1000 | Loss: 0.00001498
Iteration 51/1000 | Loss: 0.00001498
Iteration 52/1000 | Loss: 0.00001497
Iteration 53/1000 | Loss: 0.00001497
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001497
Iteration 60/1000 | Loss: 0.00001497
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001496
Iteration 64/1000 | Loss: 0.00001496
Iteration 65/1000 | Loss: 0.00001496
Iteration 66/1000 | Loss: 0.00001496
Iteration 67/1000 | Loss: 0.00001496
Iteration 68/1000 | Loss: 0.00001496
Iteration 69/1000 | Loss: 0.00001496
Iteration 70/1000 | Loss: 0.00001496
Iteration 71/1000 | Loss: 0.00001495
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001495
Iteration 74/1000 | Loss: 0.00001495
Iteration 75/1000 | Loss: 0.00001495
Iteration 76/1000 | Loss: 0.00001495
Iteration 77/1000 | Loss: 0.00001495
Iteration 78/1000 | Loss: 0.00001495
Iteration 79/1000 | Loss: 0.00001495
Iteration 80/1000 | Loss: 0.00001494
Iteration 81/1000 | Loss: 0.00001494
Iteration 82/1000 | Loss: 0.00001494
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001494
Iteration 85/1000 | Loss: 0.00001494
Iteration 86/1000 | Loss: 0.00001494
Iteration 87/1000 | Loss: 0.00001494
Iteration 88/1000 | Loss: 0.00001494
Iteration 89/1000 | Loss: 0.00001494
Iteration 90/1000 | Loss: 0.00001494
Iteration 91/1000 | Loss: 0.00001494
Iteration 92/1000 | Loss: 0.00001493
Iteration 93/1000 | Loss: 0.00001493
Iteration 94/1000 | Loss: 0.00001493
Iteration 95/1000 | Loss: 0.00001493
Iteration 96/1000 | Loss: 0.00001493
Iteration 97/1000 | Loss: 0.00001493
Iteration 98/1000 | Loss: 0.00001493
Iteration 99/1000 | Loss: 0.00001493
Iteration 100/1000 | Loss: 0.00001493
Iteration 101/1000 | Loss: 0.00001493
Iteration 102/1000 | Loss: 0.00001493
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001493
Iteration 106/1000 | Loss: 0.00001492
Iteration 107/1000 | Loss: 0.00001492
Iteration 108/1000 | Loss: 0.00001492
Iteration 109/1000 | Loss: 0.00001492
Iteration 110/1000 | Loss: 0.00001492
Iteration 111/1000 | Loss: 0.00001492
Iteration 112/1000 | Loss: 0.00001492
Iteration 113/1000 | Loss: 0.00001492
Iteration 114/1000 | Loss: 0.00001492
Iteration 115/1000 | Loss: 0.00001492
Iteration 116/1000 | Loss: 0.00001491
Iteration 117/1000 | Loss: 0.00001491
Iteration 118/1000 | Loss: 0.00001491
Iteration 119/1000 | Loss: 0.00001491
Iteration 120/1000 | Loss: 0.00001491
Iteration 121/1000 | Loss: 0.00001491
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001490
Iteration 125/1000 | Loss: 0.00001490
Iteration 126/1000 | Loss: 0.00001490
Iteration 127/1000 | Loss: 0.00001490
Iteration 128/1000 | Loss: 0.00001490
Iteration 129/1000 | Loss: 0.00001490
Iteration 130/1000 | Loss: 0.00001489
Iteration 131/1000 | Loss: 0.00001489
Iteration 132/1000 | Loss: 0.00001489
Iteration 133/1000 | Loss: 0.00001489
Iteration 134/1000 | Loss: 0.00001489
Iteration 135/1000 | Loss: 0.00001489
Iteration 136/1000 | Loss: 0.00001488
Iteration 137/1000 | Loss: 0.00001488
Iteration 138/1000 | Loss: 0.00001488
Iteration 139/1000 | Loss: 0.00001488
Iteration 140/1000 | Loss: 0.00001488
Iteration 141/1000 | Loss: 0.00001488
Iteration 142/1000 | Loss: 0.00001488
Iteration 143/1000 | Loss: 0.00001488
Iteration 144/1000 | Loss: 0.00001488
Iteration 145/1000 | Loss: 0.00001488
Iteration 146/1000 | Loss: 0.00001488
Iteration 147/1000 | Loss: 0.00001488
Iteration 148/1000 | Loss: 0.00001488
Iteration 149/1000 | Loss: 0.00001488
Iteration 150/1000 | Loss: 0.00001488
Iteration 151/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.4875519809720572e-05, 1.4875519809720572e-05, 1.4875519809720572e-05, 1.4875519809720572e-05, 1.4875519809720572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4875519809720572e-05

Optimization complete. Final v2v error: 3.2048044204711914 mm

Highest mean error: 4.343907356262207 mm for frame 110

Lowest mean error: 2.6677253246307373 mm for frame 17

Saving results

Total time: 39.60316038131714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548644
Iteration 2/25 | Loss: 0.00124512
Iteration 3/25 | Loss: 0.00094917
Iteration 4/25 | Loss: 0.00090423
Iteration 5/25 | Loss: 0.00089480
Iteration 6/25 | Loss: 0.00089240
Iteration 7/25 | Loss: 0.00089210
Iteration 8/25 | Loss: 0.00089210
Iteration 9/25 | Loss: 0.00089210
Iteration 10/25 | Loss: 0.00089210
Iteration 11/25 | Loss: 0.00089210
Iteration 12/25 | Loss: 0.00089210
Iteration 13/25 | Loss: 0.00089210
Iteration 14/25 | Loss: 0.00089210
Iteration 15/25 | Loss: 0.00089210
Iteration 16/25 | Loss: 0.00089210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008920988766476512, 0.0008920988766476512, 0.0008920988766476512, 0.0008920988766476512, 0.0008920988766476512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008920988766476512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69652748
Iteration 2/25 | Loss: 0.00117962
Iteration 3/25 | Loss: 0.00117961
Iteration 4/25 | Loss: 0.00117961
Iteration 5/25 | Loss: 0.00117961
Iteration 6/25 | Loss: 0.00117961
Iteration 7/25 | Loss: 0.00117961
Iteration 8/25 | Loss: 0.00117961
Iteration 9/25 | Loss: 0.00117961
Iteration 10/25 | Loss: 0.00117961
Iteration 11/25 | Loss: 0.00117961
Iteration 12/25 | Loss: 0.00117961
Iteration 13/25 | Loss: 0.00117961
Iteration 14/25 | Loss: 0.00117961
Iteration 15/25 | Loss: 0.00117961
Iteration 16/25 | Loss: 0.00117961
Iteration 17/25 | Loss: 0.00117961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011796088656410575, 0.0011796088656410575, 0.0011796088656410575, 0.0011796088656410575, 0.0011796088656410575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011796088656410575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117961
Iteration 2/1000 | Loss: 0.00004852
Iteration 3/1000 | Loss: 0.00003829
Iteration 4/1000 | Loss: 0.00003466
Iteration 5/1000 | Loss: 0.00003300
Iteration 6/1000 | Loss: 0.00003172
Iteration 7/1000 | Loss: 0.00003083
Iteration 8/1000 | Loss: 0.00003022
Iteration 9/1000 | Loss: 0.00002994
Iteration 10/1000 | Loss: 0.00002972
Iteration 11/1000 | Loss: 0.00002953
Iteration 12/1000 | Loss: 0.00002936
Iteration 13/1000 | Loss: 0.00002930
Iteration 14/1000 | Loss: 0.00002929
Iteration 15/1000 | Loss: 0.00002926
Iteration 16/1000 | Loss: 0.00002926
Iteration 17/1000 | Loss: 0.00002925
Iteration 18/1000 | Loss: 0.00002925
Iteration 19/1000 | Loss: 0.00002924
Iteration 20/1000 | Loss: 0.00002923
Iteration 21/1000 | Loss: 0.00002923
Iteration 22/1000 | Loss: 0.00002922
Iteration 23/1000 | Loss: 0.00002922
Iteration 24/1000 | Loss: 0.00002919
Iteration 25/1000 | Loss: 0.00002917
Iteration 26/1000 | Loss: 0.00002917
Iteration 27/1000 | Loss: 0.00002917
Iteration 28/1000 | Loss: 0.00002917
Iteration 29/1000 | Loss: 0.00002917
Iteration 30/1000 | Loss: 0.00002917
Iteration 31/1000 | Loss: 0.00002917
Iteration 32/1000 | Loss: 0.00002917
Iteration 33/1000 | Loss: 0.00002916
Iteration 34/1000 | Loss: 0.00002916
Iteration 35/1000 | Loss: 0.00002916
Iteration 36/1000 | Loss: 0.00002915
Iteration 37/1000 | Loss: 0.00002915
Iteration 38/1000 | Loss: 0.00002915
Iteration 39/1000 | Loss: 0.00002914
Iteration 40/1000 | Loss: 0.00002914
Iteration 41/1000 | Loss: 0.00002914
Iteration 42/1000 | Loss: 0.00002913
Iteration 43/1000 | Loss: 0.00002913
Iteration 44/1000 | Loss: 0.00002913
Iteration 45/1000 | Loss: 0.00002913
Iteration 46/1000 | Loss: 0.00002913
Iteration 47/1000 | Loss: 0.00002912
Iteration 48/1000 | Loss: 0.00002912
Iteration 49/1000 | Loss: 0.00002912
Iteration 50/1000 | Loss: 0.00002912
Iteration 51/1000 | Loss: 0.00002912
Iteration 52/1000 | Loss: 0.00002911
Iteration 53/1000 | Loss: 0.00002911
Iteration 54/1000 | Loss: 0.00002911
Iteration 55/1000 | Loss: 0.00002910
Iteration 56/1000 | Loss: 0.00002910
Iteration 57/1000 | Loss: 0.00002910
Iteration 58/1000 | Loss: 0.00002910
Iteration 59/1000 | Loss: 0.00002909
Iteration 60/1000 | Loss: 0.00002909
Iteration 61/1000 | Loss: 0.00002909
Iteration 62/1000 | Loss: 0.00002908
Iteration 63/1000 | Loss: 0.00002908
Iteration 64/1000 | Loss: 0.00002908
Iteration 65/1000 | Loss: 0.00002908
Iteration 66/1000 | Loss: 0.00002908
Iteration 67/1000 | Loss: 0.00002908
Iteration 68/1000 | Loss: 0.00002908
Iteration 69/1000 | Loss: 0.00002908
Iteration 70/1000 | Loss: 0.00002907
Iteration 71/1000 | Loss: 0.00002907
Iteration 72/1000 | Loss: 0.00002907
Iteration 73/1000 | Loss: 0.00002907
Iteration 74/1000 | Loss: 0.00002907
Iteration 75/1000 | Loss: 0.00002906
Iteration 76/1000 | Loss: 0.00002906
Iteration 77/1000 | Loss: 0.00002906
Iteration 78/1000 | Loss: 0.00002906
Iteration 79/1000 | Loss: 0.00002906
Iteration 80/1000 | Loss: 0.00002906
Iteration 81/1000 | Loss: 0.00002906
Iteration 82/1000 | Loss: 0.00002906
Iteration 83/1000 | Loss: 0.00002906
Iteration 84/1000 | Loss: 0.00002906
Iteration 85/1000 | Loss: 0.00002906
Iteration 86/1000 | Loss: 0.00002905
Iteration 87/1000 | Loss: 0.00002905
Iteration 88/1000 | Loss: 0.00002905
Iteration 89/1000 | Loss: 0.00002905
Iteration 90/1000 | Loss: 0.00002905
Iteration 91/1000 | Loss: 0.00002905
Iteration 92/1000 | Loss: 0.00002904
Iteration 93/1000 | Loss: 0.00002904
Iteration 94/1000 | Loss: 0.00002904
Iteration 95/1000 | Loss: 0.00002904
Iteration 96/1000 | Loss: 0.00002904
Iteration 97/1000 | Loss: 0.00002904
Iteration 98/1000 | Loss: 0.00002903
Iteration 99/1000 | Loss: 0.00002903
Iteration 100/1000 | Loss: 0.00002903
Iteration 101/1000 | Loss: 0.00002903
Iteration 102/1000 | Loss: 0.00002903
Iteration 103/1000 | Loss: 0.00002903
Iteration 104/1000 | Loss: 0.00002903
Iteration 105/1000 | Loss: 0.00002903
Iteration 106/1000 | Loss: 0.00002903
Iteration 107/1000 | Loss: 0.00002902
Iteration 108/1000 | Loss: 0.00002902
Iteration 109/1000 | Loss: 0.00002902
Iteration 110/1000 | Loss: 0.00002902
Iteration 111/1000 | Loss: 0.00002902
Iteration 112/1000 | Loss: 0.00002902
Iteration 113/1000 | Loss: 0.00002902
Iteration 114/1000 | Loss: 0.00002902
Iteration 115/1000 | Loss: 0.00002902
Iteration 116/1000 | Loss: 0.00002902
Iteration 117/1000 | Loss: 0.00002901
Iteration 118/1000 | Loss: 0.00002901
Iteration 119/1000 | Loss: 0.00002901
Iteration 120/1000 | Loss: 0.00002901
Iteration 121/1000 | Loss: 0.00002901
Iteration 122/1000 | Loss: 0.00002901
Iteration 123/1000 | Loss: 0.00002901
Iteration 124/1000 | Loss: 0.00002901
Iteration 125/1000 | Loss: 0.00002901
Iteration 126/1000 | Loss: 0.00002900
Iteration 127/1000 | Loss: 0.00002900
Iteration 128/1000 | Loss: 0.00002900
Iteration 129/1000 | Loss: 0.00002900
Iteration 130/1000 | Loss: 0.00002900
Iteration 131/1000 | Loss: 0.00002899
Iteration 132/1000 | Loss: 0.00002899
Iteration 133/1000 | Loss: 0.00002899
Iteration 134/1000 | Loss: 0.00002899
Iteration 135/1000 | Loss: 0.00002899
Iteration 136/1000 | Loss: 0.00002899
Iteration 137/1000 | Loss: 0.00002899
Iteration 138/1000 | Loss: 0.00002899
Iteration 139/1000 | Loss: 0.00002899
Iteration 140/1000 | Loss: 0.00002899
Iteration 141/1000 | Loss: 0.00002899
Iteration 142/1000 | Loss: 0.00002899
Iteration 143/1000 | Loss: 0.00002899
Iteration 144/1000 | Loss: 0.00002899
Iteration 145/1000 | Loss: 0.00002898
Iteration 146/1000 | Loss: 0.00002898
Iteration 147/1000 | Loss: 0.00002898
Iteration 148/1000 | Loss: 0.00002898
Iteration 149/1000 | Loss: 0.00002898
Iteration 150/1000 | Loss: 0.00002898
Iteration 151/1000 | Loss: 0.00002898
Iteration 152/1000 | Loss: 0.00002898
Iteration 153/1000 | Loss: 0.00002898
Iteration 154/1000 | Loss: 0.00002898
Iteration 155/1000 | Loss: 0.00002898
Iteration 156/1000 | Loss: 0.00002898
Iteration 157/1000 | Loss: 0.00002898
Iteration 158/1000 | Loss: 0.00002898
Iteration 159/1000 | Loss: 0.00002898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.8981878131162375e-05, 2.8981878131162375e-05, 2.8981878131162375e-05, 2.8981878131162375e-05, 2.8981878131162375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8981878131162375e-05

Optimization complete. Final v2v error: 4.485557556152344 mm

Highest mean error: 4.8355393409729 mm for frame 105

Lowest mean error: 4.058554649353027 mm for frame 72

Saving results

Total time: 38.14535617828369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013932
Iteration 2/25 | Loss: 0.00136943
Iteration 3/25 | Loss: 0.00116439
Iteration 4/25 | Loss: 0.00102174
Iteration 5/25 | Loss: 0.00089484
Iteration 6/25 | Loss: 0.00088051
Iteration 7/25 | Loss: 0.00086186
Iteration 8/25 | Loss: 0.00083856
Iteration 9/25 | Loss: 0.00082665
Iteration 10/25 | Loss: 0.00082018
Iteration 11/25 | Loss: 0.00081822
Iteration 12/25 | Loss: 0.00081730
Iteration 13/25 | Loss: 0.00081688
Iteration 14/25 | Loss: 0.00081819
Iteration 15/25 | Loss: 0.00081763
Iteration 16/25 | Loss: 0.00081657
Iteration 17/25 | Loss: 0.00081571
Iteration 18/25 | Loss: 0.00081544
Iteration 19/25 | Loss: 0.00081529
Iteration 20/25 | Loss: 0.00081529
Iteration 21/25 | Loss: 0.00081529
Iteration 22/25 | Loss: 0.00081529
Iteration 23/25 | Loss: 0.00081529
Iteration 24/25 | Loss: 0.00081529
Iteration 25/25 | Loss: 0.00081529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44141555
Iteration 2/25 | Loss: 0.00128332
Iteration 3/25 | Loss: 0.00128331
Iteration 4/25 | Loss: 0.00128331
Iteration 5/25 | Loss: 0.00128331
Iteration 6/25 | Loss: 0.00128331
Iteration 7/25 | Loss: 0.00128331
Iteration 8/25 | Loss: 0.00128331
Iteration 9/25 | Loss: 0.00128331
Iteration 10/25 | Loss: 0.00128331
Iteration 11/25 | Loss: 0.00128331
Iteration 12/25 | Loss: 0.00128331
Iteration 13/25 | Loss: 0.00128331
Iteration 14/25 | Loss: 0.00128331
Iteration 15/25 | Loss: 0.00128331
Iteration 16/25 | Loss: 0.00128331
Iteration 17/25 | Loss: 0.00128331
Iteration 18/25 | Loss: 0.00128331
Iteration 19/25 | Loss: 0.00128331
Iteration 20/25 | Loss: 0.00128331
Iteration 21/25 | Loss: 0.00128331
Iteration 22/25 | Loss: 0.00128331
Iteration 23/25 | Loss: 0.00128331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001283307676203549, 0.001283307676203549, 0.001283307676203549, 0.001283307676203549, 0.001283307676203549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001283307676203549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128331
Iteration 2/1000 | Loss: 0.00004007
Iteration 3/1000 | Loss: 0.00002767
Iteration 4/1000 | Loss: 0.00002546
Iteration 5/1000 | Loss: 0.00002329
Iteration 6/1000 | Loss: 0.00045905
Iteration 7/1000 | Loss: 0.00002369
Iteration 8/1000 | Loss: 0.00002138
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00001984
Iteration 11/1000 | Loss: 0.00001946
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001908
Iteration 15/1000 | Loss: 0.00001907
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00001907
Iteration 18/1000 | Loss: 0.00001906
Iteration 19/1000 | Loss: 0.00001905
Iteration 20/1000 | Loss: 0.00001905
Iteration 21/1000 | Loss: 0.00001904
Iteration 22/1000 | Loss: 0.00001901
Iteration 23/1000 | Loss: 0.00001901
Iteration 24/1000 | Loss: 0.00001890
Iteration 25/1000 | Loss: 0.00001878
Iteration 26/1000 | Loss: 0.00001875
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001874
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001871
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001870
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001863
Iteration 55/1000 | Loss: 0.00001862
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001858
Iteration 71/1000 | Loss: 0.00001858
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001857
Iteration 76/1000 | Loss: 0.00001857
Iteration 77/1000 | Loss: 0.00001857
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001855
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001854
Iteration 90/1000 | Loss: 0.00001854
Iteration 91/1000 | Loss: 0.00001854
Iteration 92/1000 | Loss: 0.00001854
Iteration 93/1000 | Loss: 0.00001854
Iteration 94/1000 | Loss: 0.00001854
Iteration 95/1000 | Loss: 0.00001853
Iteration 96/1000 | Loss: 0.00001853
Iteration 97/1000 | Loss: 0.00001853
Iteration 98/1000 | Loss: 0.00001853
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001853
Iteration 101/1000 | Loss: 0.00001853
Iteration 102/1000 | Loss: 0.00001853
Iteration 103/1000 | Loss: 0.00001853
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001852
Iteration 107/1000 | Loss: 0.00001852
Iteration 108/1000 | Loss: 0.00001852
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001851
Iteration 111/1000 | Loss: 0.00001851
Iteration 112/1000 | Loss: 0.00001851
Iteration 113/1000 | Loss: 0.00001850
Iteration 114/1000 | Loss: 0.00001850
Iteration 115/1000 | Loss: 0.00001850
Iteration 116/1000 | Loss: 0.00001850
Iteration 117/1000 | Loss: 0.00001850
Iteration 118/1000 | Loss: 0.00001850
Iteration 119/1000 | Loss: 0.00001850
Iteration 120/1000 | Loss: 0.00001850
Iteration 121/1000 | Loss: 0.00001850
Iteration 122/1000 | Loss: 0.00001850
Iteration 123/1000 | Loss: 0.00001849
Iteration 124/1000 | Loss: 0.00001849
Iteration 125/1000 | Loss: 0.00001849
Iteration 126/1000 | Loss: 0.00001849
Iteration 127/1000 | Loss: 0.00001849
Iteration 128/1000 | Loss: 0.00001849
Iteration 129/1000 | Loss: 0.00001849
Iteration 130/1000 | Loss: 0.00001849
Iteration 131/1000 | Loss: 0.00001849
Iteration 132/1000 | Loss: 0.00001849
Iteration 133/1000 | Loss: 0.00001849
Iteration 134/1000 | Loss: 0.00001849
Iteration 135/1000 | Loss: 0.00001849
Iteration 136/1000 | Loss: 0.00001849
Iteration 137/1000 | Loss: 0.00001849
Iteration 138/1000 | Loss: 0.00001849
Iteration 139/1000 | Loss: 0.00001849
Iteration 140/1000 | Loss: 0.00001849
Iteration 141/1000 | Loss: 0.00001849
Iteration 142/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.849074033088982e-05, 1.849074033088982e-05, 1.849074033088982e-05, 1.849074033088982e-05, 1.849074033088982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.849074033088982e-05

Optimization complete. Final v2v error: 3.603156089782715 mm

Highest mean error: 4.222879886627197 mm for frame 116

Lowest mean error: 3.0448408126831055 mm for frame 62

Saving results

Total time: 74.57340168952942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015645
Iteration 2/25 | Loss: 0.00169631
Iteration 3/25 | Loss: 0.00107509
Iteration 4/25 | Loss: 0.00101968
Iteration 5/25 | Loss: 0.00100707
Iteration 6/25 | Loss: 0.00100496
Iteration 7/25 | Loss: 0.00100496
Iteration 8/25 | Loss: 0.00100496
Iteration 9/25 | Loss: 0.00100496
Iteration 10/25 | Loss: 0.00100496
Iteration 11/25 | Loss: 0.00100496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010049615520983934, 0.0010049615520983934, 0.0010049615520983934, 0.0010049615520983934, 0.0010049615520983934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010049615520983934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87530845
Iteration 2/25 | Loss: 0.00078953
Iteration 3/25 | Loss: 0.00078952
Iteration 4/25 | Loss: 0.00078952
Iteration 5/25 | Loss: 0.00078952
Iteration 6/25 | Loss: 0.00078952
Iteration 7/25 | Loss: 0.00078952
Iteration 8/25 | Loss: 0.00078952
Iteration 9/25 | Loss: 0.00078952
Iteration 10/25 | Loss: 0.00078952
Iteration 11/25 | Loss: 0.00078952
Iteration 12/25 | Loss: 0.00078952
Iteration 13/25 | Loss: 0.00078952
Iteration 14/25 | Loss: 0.00078952
Iteration 15/25 | Loss: 0.00078952
Iteration 16/25 | Loss: 0.00078952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007895183516666293, 0.0007895183516666293, 0.0007895183516666293, 0.0007895183516666293, 0.0007895183516666293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007895183516666293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078952
Iteration 2/1000 | Loss: 0.00008103
Iteration 3/1000 | Loss: 0.00004986
Iteration 4/1000 | Loss: 0.00004256
Iteration 5/1000 | Loss: 0.00004049
Iteration 6/1000 | Loss: 0.00003960
Iteration 7/1000 | Loss: 0.00003888
Iteration 8/1000 | Loss: 0.00003810
Iteration 9/1000 | Loss: 0.00003757
Iteration 10/1000 | Loss: 0.00003717
Iteration 11/1000 | Loss: 0.00003689
Iteration 12/1000 | Loss: 0.00003663
Iteration 13/1000 | Loss: 0.00003635
Iteration 14/1000 | Loss: 0.00003618
Iteration 15/1000 | Loss: 0.00003603
Iteration 16/1000 | Loss: 0.00003589
Iteration 17/1000 | Loss: 0.00003578
Iteration 18/1000 | Loss: 0.00003574
Iteration 19/1000 | Loss: 0.00003568
Iteration 20/1000 | Loss: 0.00003568
Iteration 21/1000 | Loss: 0.00003567
Iteration 22/1000 | Loss: 0.00003567
Iteration 23/1000 | Loss: 0.00003566
Iteration 24/1000 | Loss: 0.00003565
Iteration 25/1000 | Loss: 0.00003565
Iteration 26/1000 | Loss: 0.00003561
Iteration 27/1000 | Loss: 0.00003560
Iteration 28/1000 | Loss: 0.00003558
Iteration 29/1000 | Loss: 0.00003558
Iteration 30/1000 | Loss: 0.00003555
Iteration 31/1000 | Loss: 0.00003555
Iteration 32/1000 | Loss: 0.00003550
Iteration 33/1000 | Loss: 0.00003550
Iteration 34/1000 | Loss: 0.00003549
Iteration 35/1000 | Loss: 0.00003549
Iteration 36/1000 | Loss: 0.00003548
Iteration 37/1000 | Loss: 0.00003544
Iteration 38/1000 | Loss: 0.00003544
Iteration 39/1000 | Loss: 0.00003544
Iteration 40/1000 | Loss: 0.00003539
Iteration 41/1000 | Loss: 0.00003539
Iteration 42/1000 | Loss: 0.00003537
Iteration 43/1000 | Loss: 0.00003537
Iteration 44/1000 | Loss: 0.00003536
Iteration 45/1000 | Loss: 0.00003536
Iteration 46/1000 | Loss: 0.00003536
Iteration 47/1000 | Loss: 0.00003536
Iteration 48/1000 | Loss: 0.00003535
Iteration 49/1000 | Loss: 0.00003535
Iteration 50/1000 | Loss: 0.00003535
Iteration 51/1000 | Loss: 0.00003535
Iteration 52/1000 | Loss: 0.00003535
Iteration 53/1000 | Loss: 0.00003535
Iteration 54/1000 | Loss: 0.00003535
Iteration 55/1000 | Loss: 0.00003535
Iteration 56/1000 | Loss: 0.00003535
Iteration 57/1000 | Loss: 0.00003535
Iteration 58/1000 | Loss: 0.00003534
Iteration 59/1000 | Loss: 0.00003534
Iteration 60/1000 | Loss: 0.00003533
Iteration 61/1000 | Loss: 0.00003533
Iteration 62/1000 | Loss: 0.00003533
Iteration 63/1000 | Loss: 0.00003532
Iteration 64/1000 | Loss: 0.00003532
Iteration 65/1000 | Loss: 0.00003532
Iteration 66/1000 | Loss: 0.00003532
Iteration 67/1000 | Loss: 0.00003531
Iteration 68/1000 | Loss: 0.00003531
Iteration 69/1000 | Loss: 0.00003531
Iteration 70/1000 | Loss: 0.00003531
Iteration 71/1000 | Loss: 0.00003530
Iteration 72/1000 | Loss: 0.00003530
Iteration 73/1000 | Loss: 0.00003530
Iteration 74/1000 | Loss: 0.00003530
Iteration 75/1000 | Loss: 0.00003530
Iteration 76/1000 | Loss: 0.00003530
Iteration 77/1000 | Loss: 0.00003530
Iteration 78/1000 | Loss: 0.00003530
Iteration 79/1000 | Loss: 0.00003530
Iteration 80/1000 | Loss: 0.00003530
Iteration 81/1000 | Loss: 0.00003530
Iteration 82/1000 | Loss: 0.00003530
Iteration 83/1000 | Loss: 0.00003530
Iteration 84/1000 | Loss: 0.00003530
Iteration 85/1000 | Loss: 0.00003529
Iteration 86/1000 | Loss: 0.00003529
Iteration 87/1000 | Loss: 0.00003529
Iteration 88/1000 | Loss: 0.00003529
Iteration 89/1000 | Loss: 0.00003528
Iteration 90/1000 | Loss: 0.00003528
Iteration 91/1000 | Loss: 0.00003528
Iteration 92/1000 | Loss: 0.00003527
Iteration 93/1000 | Loss: 0.00003527
Iteration 94/1000 | Loss: 0.00003527
Iteration 95/1000 | Loss: 0.00003527
Iteration 96/1000 | Loss: 0.00003527
Iteration 97/1000 | Loss: 0.00003527
Iteration 98/1000 | Loss: 0.00003526
Iteration 99/1000 | Loss: 0.00003526
Iteration 100/1000 | Loss: 0.00003526
Iteration 101/1000 | Loss: 0.00003526
Iteration 102/1000 | Loss: 0.00003526
Iteration 103/1000 | Loss: 0.00003526
Iteration 104/1000 | Loss: 0.00003526
Iteration 105/1000 | Loss: 0.00003526
Iteration 106/1000 | Loss: 0.00003525
Iteration 107/1000 | Loss: 0.00003525
Iteration 108/1000 | Loss: 0.00003525
Iteration 109/1000 | Loss: 0.00003525
Iteration 110/1000 | Loss: 0.00003525
Iteration 111/1000 | Loss: 0.00003525
Iteration 112/1000 | Loss: 0.00003525
Iteration 113/1000 | Loss: 0.00003524
Iteration 114/1000 | Loss: 0.00003524
Iteration 115/1000 | Loss: 0.00003524
Iteration 116/1000 | Loss: 0.00003524
Iteration 117/1000 | Loss: 0.00003524
Iteration 118/1000 | Loss: 0.00003523
Iteration 119/1000 | Loss: 0.00003523
Iteration 120/1000 | Loss: 0.00003523
Iteration 121/1000 | Loss: 0.00003523
Iteration 122/1000 | Loss: 0.00003523
Iteration 123/1000 | Loss: 0.00003522
Iteration 124/1000 | Loss: 0.00003522
Iteration 125/1000 | Loss: 0.00003522
Iteration 126/1000 | Loss: 0.00003522
Iteration 127/1000 | Loss: 0.00003522
Iteration 128/1000 | Loss: 0.00003521
Iteration 129/1000 | Loss: 0.00003521
Iteration 130/1000 | Loss: 0.00003521
Iteration 131/1000 | Loss: 0.00003521
Iteration 132/1000 | Loss: 0.00003521
Iteration 133/1000 | Loss: 0.00003521
Iteration 134/1000 | Loss: 0.00003521
Iteration 135/1000 | Loss: 0.00003521
Iteration 136/1000 | Loss: 0.00003520
Iteration 137/1000 | Loss: 0.00003520
Iteration 138/1000 | Loss: 0.00003520
Iteration 139/1000 | Loss: 0.00003520
Iteration 140/1000 | Loss: 0.00003520
Iteration 141/1000 | Loss: 0.00003520
Iteration 142/1000 | Loss: 0.00003520
Iteration 143/1000 | Loss: 0.00003520
Iteration 144/1000 | Loss: 0.00003520
Iteration 145/1000 | Loss: 0.00003520
Iteration 146/1000 | Loss: 0.00003519
Iteration 147/1000 | Loss: 0.00003519
Iteration 148/1000 | Loss: 0.00003519
Iteration 149/1000 | Loss: 0.00003519
Iteration 150/1000 | Loss: 0.00003519
Iteration 151/1000 | Loss: 0.00003519
Iteration 152/1000 | Loss: 0.00003519
Iteration 153/1000 | Loss: 0.00003518
Iteration 154/1000 | Loss: 0.00003518
Iteration 155/1000 | Loss: 0.00003518
Iteration 156/1000 | Loss: 0.00003518
Iteration 157/1000 | Loss: 0.00003518
Iteration 158/1000 | Loss: 0.00003518
Iteration 159/1000 | Loss: 0.00003517
Iteration 160/1000 | Loss: 0.00003517
Iteration 161/1000 | Loss: 0.00003517
Iteration 162/1000 | Loss: 0.00003517
Iteration 163/1000 | Loss: 0.00003517
Iteration 164/1000 | Loss: 0.00003517
Iteration 165/1000 | Loss: 0.00003517
Iteration 166/1000 | Loss: 0.00003517
Iteration 167/1000 | Loss: 0.00003517
Iteration 168/1000 | Loss: 0.00003517
Iteration 169/1000 | Loss: 0.00003517
Iteration 170/1000 | Loss: 0.00003517
Iteration 171/1000 | Loss: 0.00003517
Iteration 172/1000 | Loss: 0.00003516
Iteration 173/1000 | Loss: 0.00003516
Iteration 174/1000 | Loss: 0.00003516
Iteration 175/1000 | Loss: 0.00003516
Iteration 176/1000 | Loss: 0.00003516
Iteration 177/1000 | Loss: 0.00003516
Iteration 178/1000 | Loss: 0.00003516
Iteration 179/1000 | Loss: 0.00003516
Iteration 180/1000 | Loss: 0.00003516
Iteration 181/1000 | Loss: 0.00003516
Iteration 182/1000 | Loss: 0.00003516
Iteration 183/1000 | Loss: 0.00003516
Iteration 184/1000 | Loss: 0.00003516
Iteration 185/1000 | Loss: 0.00003516
Iteration 186/1000 | Loss: 0.00003516
Iteration 187/1000 | Loss: 0.00003516
Iteration 188/1000 | Loss: 0.00003516
Iteration 189/1000 | Loss: 0.00003516
Iteration 190/1000 | Loss: 0.00003516
Iteration 191/1000 | Loss: 0.00003516
Iteration 192/1000 | Loss: 0.00003516
Iteration 193/1000 | Loss: 0.00003516
Iteration 194/1000 | Loss: 0.00003516
Iteration 195/1000 | Loss: 0.00003516
Iteration 196/1000 | Loss: 0.00003516
Iteration 197/1000 | Loss: 0.00003516
Iteration 198/1000 | Loss: 0.00003516
Iteration 199/1000 | Loss: 0.00003516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [3.515501521178521e-05, 3.515501521178521e-05, 3.515501521178521e-05, 3.515501521178521e-05, 3.515501521178521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.515501521178521e-05

Optimization complete. Final v2v error: 4.770698547363281 mm

Highest mean error: 5.803661823272705 mm for frame 105

Lowest mean error: 3.888791799545288 mm for frame 32

Saving results

Total time: 52.60004186630249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772201
Iteration 2/25 | Loss: 0.00163942
Iteration 3/25 | Loss: 0.00102958
Iteration 4/25 | Loss: 0.00092434
Iteration 5/25 | Loss: 0.00089693
Iteration 6/25 | Loss: 0.00088540
Iteration 7/25 | Loss: 0.00086920
Iteration 8/25 | Loss: 0.00086268
Iteration 9/25 | Loss: 0.00086172
Iteration 10/25 | Loss: 0.00086147
Iteration 11/25 | Loss: 0.00086138
Iteration 12/25 | Loss: 0.00086137
Iteration 13/25 | Loss: 0.00086137
Iteration 14/25 | Loss: 0.00086137
Iteration 15/25 | Loss: 0.00086137
Iteration 16/25 | Loss: 0.00086137
Iteration 17/25 | Loss: 0.00086137
Iteration 18/25 | Loss: 0.00086137
Iteration 19/25 | Loss: 0.00086137
Iteration 20/25 | Loss: 0.00086137
Iteration 21/25 | Loss: 0.00086137
Iteration 22/25 | Loss: 0.00086136
Iteration 23/25 | Loss: 0.00086136
Iteration 24/25 | Loss: 0.00086136
Iteration 25/25 | Loss: 0.00086136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27612662
Iteration 2/25 | Loss: 0.00127522
Iteration 3/25 | Loss: 0.00127518
Iteration 4/25 | Loss: 0.00127518
Iteration 5/25 | Loss: 0.00127518
Iteration 6/25 | Loss: 0.00127518
Iteration 7/25 | Loss: 0.00127518
Iteration 8/25 | Loss: 0.00127518
Iteration 9/25 | Loss: 0.00127518
Iteration 10/25 | Loss: 0.00127518
Iteration 11/25 | Loss: 0.00127518
Iteration 12/25 | Loss: 0.00127518
Iteration 13/25 | Loss: 0.00127518
Iteration 14/25 | Loss: 0.00127518
Iteration 15/25 | Loss: 0.00127518
Iteration 16/25 | Loss: 0.00127518
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012751775793731213, 0.0012751775793731213, 0.0012751775793731213, 0.0012751775793731213, 0.0012751775793731213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012751775793731213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127518
Iteration 2/1000 | Loss: 0.00003389
Iteration 3/1000 | Loss: 0.00002382
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002016
Iteration 6/1000 | Loss: 0.00001948
Iteration 7/1000 | Loss: 0.00001903
Iteration 8/1000 | Loss: 0.00001879
Iteration 9/1000 | Loss: 0.00001875
Iteration 10/1000 | Loss: 0.00001874
Iteration 11/1000 | Loss: 0.00001867
Iteration 12/1000 | Loss: 0.00001867
Iteration 13/1000 | Loss: 0.00001866
Iteration 14/1000 | Loss: 0.00001866
Iteration 15/1000 | Loss: 0.00001865
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001860
Iteration 18/1000 | Loss: 0.00001859
Iteration 19/1000 | Loss: 0.00001857
Iteration 20/1000 | Loss: 0.00001856
Iteration 21/1000 | Loss: 0.00001856
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001846
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001841
Iteration 28/1000 | Loss: 0.00001839
Iteration 29/1000 | Loss: 0.00001838
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001834
Iteration 32/1000 | Loss: 0.00001833
Iteration 33/1000 | Loss: 0.00001833
Iteration 34/1000 | Loss: 0.00001832
Iteration 35/1000 | Loss: 0.00001831
Iteration 36/1000 | Loss: 0.00001829
Iteration 37/1000 | Loss: 0.00001828
Iteration 38/1000 | Loss: 0.00001828
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001828
Iteration 41/1000 | Loss: 0.00001828
Iteration 42/1000 | Loss: 0.00001827
Iteration 43/1000 | Loss: 0.00001827
Iteration 44/1000 | Loss: 0.00001827
Iteration 45/1000 | Loss: 0.00001827
Iteration 46/1000 | Loss: 0.00001827
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001826
Iteration 49/1000 | Loss: 0.00001826
Iteration 50/1000 | Loss: 0.00001825
Iteration 51/1000 | Loss: 0.00001825
Iteration 52/1000 | Loss: 0.00001825
Iteration 53/1000 | Loss: 0.00001825
Iteration 54/1000 | Loss: 0.00001825
Iteration 55/1000 | Loss: 0.00001824
Iteration 56/1000 | Loss: 0.00001824
Iteration 57/1000 | Loss: 0.00001823
Iteration 58/1000 | Loss: 0.00001822
Iteration 59/1000 | Loss: 0.00001822
Iteration 60/1000 | Loss: 0.00001821
Iteration 61/1000 | Loss: 0.00001821
Iteration 62/1000 | Loss: 0.00001821
Iteration 63/1000 | Loss: 0.00001821
Iteration 64/1000 | Loss: 0.00001821
Iteration 65/1000 | Loss: 0.00001821
Iteration 66/1000 | Loss: 0.00001821
Iteration 67/1000 | Loss: 0.00001821
Iteration 68/1000 | Loss: 0.00001821
Iteration 69/1000 | Loss: 0.00001821
Iteration 70/1000 | Loss: 0.00001821
Iteration 71/1000 | Loss: 0.00001820
Iteration 72/1000 | Loss: 0.00001820
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001818
Iteration 75/1000 | Loss: 0.00001818
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001816
Iteration 80/1000 | Loss: 0.00001815
Iteration 81/1000 | Loss: 0.00001815
Iteration 82/1000 | Loss: 0.00001815
Iteration 83/1000 | Loss: 0.00001815
Iteration 84/1000 | Loss: 0.00001814
Iteration 85/1000 | Loss: 0.00001814
Iteration 86/1000 | Loss: 0.00001814
Iteration 87/1000 | Loss: 0.00001814
Iteration 88/1000 | Loss: 0.00001814
Iteration 89/1000 | Loss: 0.00001814
Iteration 90/1000 | Loss: 0.00001814
Iteration 91/1000 | Loss: 0.00001814
Iteration 92/1000 | Loss: 0.00001814
Iteration 93/1000 | Loss: 0.00001813
Iteration 94/1000 | Loss: 0.00001813
Iteration 95/1000 | Loss: 0.00001813
Iteration 96/1000 | Loss: 0.00001813
Iteration 97/1000 | Loss: 0.00001813
Iteration 98/1000 | Loss: 0.00001812
Iteration 99/1000 | Loss: 0.00001812
Iteration 100/1000 | Loss: 0.00001812
Iteration 101/1000 | Loss: 0.00001812
Iteration 102/1000 | Loss: 0.00001812
Iteration 103/1000 | Loss: 0.00001812
Iteration 104/1000 | Loss: 0.00001812
Iteration 105/1000 | Loss: 0.00001812
Iteration 106/1000 | Loss: 0.00001811
Iteration 107/1000 | Loss: 0.00001811
Iteration 108/1000 | Loss: 0.00001811
Iteration 109/1000 | Loss: 0.00001811
Iteration 110/1000 | Loss: 0.00001810
Iteration 111/1000 | Loss: 0.00001810
Iteration 112/1000 | Loss: 0.00001810
Iteration 113/1000 | Loss: 0.00001810
Iteration 114/1000 | Loss: 0.00001810
Iteration 115/1000 | Loss: 0.00001810
Iteration 116/1000 | Loss: 0.00001810
Iteration 117/1000 | Loss: 0.00001809
Iteration 118/1000 | Loss: 0.00001809
Iteration 119/1000 | Loss: 0.00001809
Iteration 120/1000 | Loss: 0.00001809
Iteration 121/1000 | Loss: 0.00001808
Iteration 122/1000 | Loss: 0.00001808
Iteration 123/1000 | Loss: 0.00001808
Iteration 124/1000 | Loss: 0.00001808
Iteration 125/1000 | Loss: 0.00001808
Iteration 126/1000 | Loss: 0.00001808
Iteration 127/1000 | Loss: 0.00001808
Iteration 128/1000 | Loss: 0.00001807
Iteration 129/1000 | Loss: 0.00001807
Iteration 130/1000 | Loss: 0.00001807
Iteration 131/1000 | Loss: 0.00001807
Iteration 132/1000 | Loss: 0.00001807
Iteration 133/1000 | Loss: 0.00001807
Iteration 134/1000 | Loss: 0.00001807
Iteration 135/1000 | Loss: 0.00001807
Iteration 136/1000 | Loss: 0.00001807
Iteration 137/1000 | Loss: 0.00001807
Iteration 138/1000 | Loss: 0.00001807
Iteration 139/1000 | Loss: 0.00001807
Iteration 140/1000 | Loss: 0.00001807
Iteration 141/1000 | Loss: 0.00001806
Iteration 142/1000 | Loss: 0.00001806
Iteration 143/1000 | Loss: 0.00001806
Iteration 144/1000 | Loss: 0.00001806
Iteration 145/1000 | Loss: 0.00001806
Iteration 146/1000 | Loss: 0.00001806
Iteration 147/1000 | Loss: 0.00001806
Iteration 148/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.806142427085433e-05, 1.806142427085433e-05, 1.806142427085433e-05, 1.806142427085433e-05, 1.806142427085433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.806142427085433e-05

Optimization complete. Final v2v error: 3.5907387733459473 mm

Highest mean error: 4.039007663726807 mm for frame 112

Lowest mean error: 3.348702907562256 mm for frame 28

Saving results

Total time: 50.880980491638184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086935
Iteration 2/25 | Loss: 0.00166081
Iteration 3/25 | Loss: 0.00106628
Iteration 4/25 | Loss: 0.00096813
Iteration 5/25 | Loss: 0.00093432
Iteration 6/25 | Loss: 0.00092480
Iteration 7/25 | Loss: 0.00092220
Iteration 8/25 | Loss: 0.00092110
Iteration 9/25 | Loss: 0.00092035
Iteration 10/25 | Loss: 0.00092008
Iteration 11/25 | Loss: 0.00091998
Iteration 12/25 | Loss: 0.00091993
Iteration 13/25 | Loss: 0.00091993
Iteration 14/25 | Loss: 0.00091993
Iteration 15/25 | Loss: 0.00091993
Iteration 16/25 | Loss: 0.00091993
Iteration 17/25 | Loss: 0.00091992
Iteration 18/25 | Loss: 0.00091992
Iteration 19/25 | Loss: 0.00091992
Iteration 20/25 | Loss: 0.00091992
Iteration 21/25 | Loss: 0.00091992
Iteration 22/25 | Loss: 0.00091992
Iteration 23/25 | Loss: 0.00091992
Iteration 24/25 | Loss: 0.00091992
Iteration 25/25 | Loss: 0.00091991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97857636
Iteration 2/25 | Loss: 0.00142653
Iteration 3/25 | Loss: 0.00142653
Iteration 4/25 | Loss: 0.00142653
Iteration 5/25 | Loss: 0.00142653
Iteration 6/25 | Loss: 0.00142653
Iteration 7/25 | Loss: 0.00142653
Iteration 8/25 | Loss: 0.00142653
Iteration 9/25 | Loss: 0.00142653
Iteration 10/25 | Loss: 0.00142653
Iteration 11/25 | Loss: 0.00142653
Iteration 12/25 | Loss: 0.00142653
Iteration 13/25 | Loss: 0.00142653
Iteration 14/25 | Loss: 0.00142653
Iteration 15/25 | Loss: 0.00142653
Iteration 16/25 | Loss: 0.00142653
Iteration 17/25 | Loss: 0.00142653
Iteration 18/25 | Loss: 0.00142653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014265301870182157, 0.0014265301870182157, 0.0014265301870182157, 0.0014265301870182157, 0.0014265301870182157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014265301870182157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142653
Iteration 2/1000 | Loss: 0.00006351
Iteration 3/1000 | Loss: 0.00004525
Iteration 4/1000 | Loss: 0.00003769
Iteration 5/1000 | Loss: 0.00003523
Iteration 6/1000 | Loss: 0.00044319
Iteration 7/1000 | Loss: 0.00004274
Iteration 8/1000 | Loss: 0.00003558
Iteration 9/1000 | Loss: 0.00003349
Iteration 10/1000 | Loss: 0.00003218
Iteration 11/1000 | Loss: 0.00003114
Iteration 12/1000 | Loss: 0.00003022
Iteration 13/1000 | Loss: 0.00002925
Iteration 14/1000 | Loss: 0.00002843
Iteration 15/1000 | Loss: 0.00002772
Iteration 16/1000 | Loss: 0.00002741
Iteration 17/1000 | Loss: 0.00002724
Iteration 18/1000 | Loss: 0.00002707
Iteration 19/1000 | Loss: 0.00002696
Iteration 20/1000 | Loss: 0.00002693
Iteration 21/1000 | Loss: 0.00002692
Iteration 22/1000 | Loss: 0.00002670
Iteration 23/1000 | Loss: 0.00002668
Iteration 24/1000 | Loss: 0.00002656
Iteration 25/1000 | Loss: 0.00002654
Iteration 26/1000 | Loss: 0.00002645
Iteration 27/1000 | Loss: 0.00002644
Iteration 28/1000 | Loss: 0.00002644
Iteration 29/1000 | Loss: 0.00002641
Iteration 30/1000 | Loss: 0.00002640
Iteration 31/1000 | Loss: 0.00002640
Iteration 32/1000 | Loss: 0.00002639
Iteration 33/1000 | Loss: 0.00002639
Iteration 34/1000 | Loss: 0.00002638
Iteration 35/1000 | Loss: 0.00002638
Iteration 36/1000 | Loss: 0.00002638
Iteration 37/1000 | Loss: 0.00002637
Iteration 38/1000 | Loss: 0.00002637
Iteration 39/1000 | Loss: 0.00002636
Iteration 40/1000 | Loss: 0.00002636
Iteration 41/1000 | Loss: 0.00002636
Iteration 42/1000 | Loss: 0.00002636
Iteration 43/1000 | Loss: 0.00002636
Iteration 44/1000 | Loss: 0.00002636
Iteration 45/1000 | Loss: 0.00002635
Iteration 46/1000 | Loss: 0.00002635
Iteration 47/1000 | Loss: 0.00002634
Iteration 48/1000 | Loss: 0.00002634
Iteration 49/1000 | Loss: 0.00002633
Iteration 50/1000 | Loss: 0.00002633
Iteration 51/1000 | Loss: 0.00002633
Iteration 52/1000 | Loss: 0.00002633
Iteration 53/1000 | Loss: 0.00002633
Iteration 54/1000 | Loss: 0.00002632
Iteration 55/1000 | Loss: 0.00002632
Iteration 56/1000 | Loss: 0.00002632
Iteration 57/1000 | Loss: 0.00002631
Iteration 58/1000 | Loss: 0.00002631
Iteration 59/1000 | Loss: 0.00002629
Iteration 60/1000 | Loss: 0.00002628
Iteration 61/1000 | Loss: 0.00002628
Iteration 62/1000 | Loss: 0.00002627
Iteration 63/1000 | Loss: 0.00002627
Iteration 64/1000 | Loss: 0.00002626
Iteration 65/1000 | Loss: 0.00002625
Iteration 66/1000 | Loss: 0.00002625
Iteration 67/1000 | Loss: 0.00002625
Iteration 68/1000 | Loss: 0.00002625
Iteration 69/1000 | Loss: 0.00002625
Iteration 70/1000 | Loss: 0.00002623
Iteration 71/1000 | Loss: 0.00002623
Iteration 72/1000 | Loss: 0.00002623
Iteration 73/1000 | Loss: 0.00002623
Iteration 74/1000 | Loss: 0.00002623
Iteration 75/1000 | Loss: 0.00002623
Iteration 76/1000 | Loss: 0.00002623
Iteration 77/1000 | Loss: 0.00002622
Iteration 78/1000 | Loss: 0.00002621
Iteration 79/1000 | Loss: 0.00002621
Iteration 80/1000 | Loss: 0.00002621
Iteration 81/1000 | Loss: 0.00002621
Iteration 82/1000 | Loss: 0.00002620
Iteration 83/1000 | Loss: 0.00002619
Iteration 84/1000 | Loss: 0.00002618
Iteration 85/1000 | Loss: 0.00002617
Iteration 86/1000 | Loss: 0.00002616
Iteration 87/1000 | Loss: 0.00002615
Iteration 88/1000 | Loss: 0.00002615
Iteration 89/1000 | Loss: 0.00002614
Iteration 90/1000 | Loss: 0.00002612
Iteration 91/1000 | Loss: 0.00002612
Iteration 92/1000 | Loss: 0.00002612
Iteration 93/1000 | Loss: 0.00002612
Iteration 94/1000 | Loss: 0.00002612
Iteration 95/1000 | Loss: 0.00002612
Iteration 96/1000 | Loss: 0.00002612
Iteration 97/1000 | Loss: 0.00002612
Iteration 98/1000 | Loss: 0.00002611
Iteration 99/1000 | Loss: 0.00002611
Iteration 100/1000 | Loss: 0.00002609
Iteration 101/1000 | Loss: 0.00002609
Iteration 102/1000 | Loss: 0.00002609
Iteration 103/1000 | Loss: 0.00002608
Iteration 104/1000 | Loss: 0.00002608
Iteration 105/1000 | Loss: 0.00002608
Iteration 106/1000 | Loss: 0.00002607
Iteration 107/1000 | Loss: 0.00002605
Iteration 108/1000 | Loss: 0.00002605
Iteration 109/1000 | Loss: 0.00002605
Iteration 110/1000 | Loss: 0.00002605
Iteration 111/1000 | Loss: 0.00002605
Iteration 112/1000 | Loss: 0.00002605
Iteration 113/1000 | Loss: 0.00002605
Iteration 114/1000 | Loss: 0.00002605
Iteration 115/1000 | Loss: 0.00002605
Iteration 116/1000 | Loss: 0.00002605
Iteration 117/1000 | Loss: 0.00002605
Iteration 118/1000 | Loss: 0.00002605
Iteration 119/1000 | Loss: 0.00002605
Iteration 120/1000 | Loss: 0.00002604
Iteration 121/1000 | Loss: 0.00002604
Iteration 122/1000 | Loss: 0.00002604
Iteration 123/1000 | Loss: 0.00002604
Iteration 124/1000 | Loss: 0.00002604
Iteration 125/1000 | Loss: 0.00002603
Iteration 126/1000 | Loss: 0.00002603
Iteration 127/1000 | Loss: 0.00002603
Iteration 128/1000 | Loss: 0.00002602
Iteration 129/1000 | Loss: 0.00002602
Iteration 130/1000 | Loss: 0.00002602
Iteration 131/1000 | Loss: 0.00002602
Iteration 132/1000 | Loss: 0.00002602
Iteration 133/1000 | Loss: 0.00002602
Iteration 134/1000 | Loss: 0.00002602
Iteration 135/1000 | Loss: 0.00002601
Iteration 136/1000 | Loss: 0.00002601
Iteration 137/1000 | Loss: 0.00002601
Iteration 138/1000 | Loss: 0.00002601
Iteration 139/1000 | Loss: 0.00002600
Iteration 140/1000 | Loss: 0.00002600
Iteration 141/1000 | Loss: 0.00002600
Iteration 142/1000 | Loss: 0.00002600
Iteration 143/1000 | Loss: 0.00002600
Iteration 144/1000 | Loss: 0.00002599
Iteration 145/1000 | Loss: 0.00002599
Iteration 146/1000 | Loss: 0.00002599
Iteration 147/1000 | Loss: 0.00002599
Iteration 148/1000 | Loss: 0.00002599
Iteration 149/1000 | Loss: 0.00002598
Iteration 150/1000 | Loss: 0.00002598
Iteration 151/1000 | Loss: 0.00002598
Iteration 152/1000 | Loss: 0.00002598
Iteration 153/1000 | Loss: 0.00002598
Iteration 154/1000 | Loss: 0.00002598
Iteration 155/1000 | Loss: 0.00002598
Iteration 156/1000 | Loss: 0.00002598
Iteration 157/1000 | Loss: 0.00002598
Iteration 158/1000 | Loss: 0.00002597
Iteration 159/1000 | Loss: 0.00002597
Iteration 160/1000 | Loss: 0.00002597
Iteration 161/1000 | Loss: 0.00002597
Iteration 162/1000 | Loss: 0.00002597
Iteration 163/1000 | Loss: 0.00002597
Iteration 164/1000 | Loss: 0.00002597
Iteration 165/1000 | Loss: 0.00002596
Iteration 166/1000 | Loss: 0.00002596
Iteration 167/1000 | Loss: 0.00002596
Iteration 168/1000 | Loss: 0.00002596
Iteration 169/1000 | Loss: 0.00002596
Iteration 170/1000 | Loss: 0.00002596
Iteration 171/1000 | Loss: 0.00002596
Iteration 172/1000 | Loss: 0.00002596
Iteration 173/1000 | Loss: 0.00002596
Iteration 174/1000 | Loss: 0.00002596
Iteration 175/1000 | Loss: 0.00002596
Iteration 176/1000 | Loss: 0.00002595
Iteration 177/1000 | Loss: 0.00002595
Iteration 178/1000 | Loss: 0.00002595
Iteration 179/1000 | Loss: 0.00002595
Iteration 180/1000 | Loss: 0.00002595
Iteration 181/1000 | Loss: 0.00002595
Iteration 182/1000 | Loss: 0.00002594
Iteration 183/1000 | Loss: 0.00002594
Iteration 184/1000 | Loss: 0.00002594
Iteration 185/1000 | Loss: 0.00002594
Iteration 186/1000 | Loss: 0.00002594
Iteration 187/1000 | Loss: 0.00002593
Iteration 188/1000 | Loss: 0.00002593
Iteration 189/1000 | Loss: 0.00002593
Iteration 190/1000 | Loss: 0.00002593
Iteration 191/1000 | Loss: 0.00002593
Iteration 192/1000 | Loss: 0.00002593
Iteration 193/1000 | Loss: 0.00002592
Iteration 194/1000 | Loss: 0.00002592
Iteration 195/1000 | Loss: 0.00002592
Iteration 196/1000 | Loss: 0.00002592
Iteration 197/1000 | Loss: 0.00002592
Iteration 198/1000 | Loss: 0.00002592
Iteration 199/1000 | Loss: 0.00002592
Iteration 200/1000 | Loss: 0.00002592
Iteration 201/1000 | Loss: 0.00002591
Iteration 202/1000 | Loss: 0.00002591
Iteration 203/1000 | Loss: 0.00002591
Iteration 204/1000 | Loss: 0.00002591
Iteration 205/1000 | Loss: 0.00002591
Iteration 206/1000 | Loss: 0.00002591
Iteration 207/1000 | Loss: 0.00002591
Iteration 208/1000 | Loss: 0.00002591
Iteration 209/1000 | Loss: 0.00002591
Iteration 210/1000 | Loss: 0.00002591
Iteration 211/1000 | Loss: 0.00002590
Iteration 212/1000 | Loss: 0.00002590
Iteration 213/1000 | Loss: 0.00002590
Iteration 214/1000 | Loss: 0.00002590
Iteration 215/1000 | Loss: 0.00002590
Iteration 216/1000 | Loss: 0.00002590
Iteration 217/1000 | Loss: 0.00002590
Iteration 218/1000 | Loss: 0.00002590
Iteration 219/1000 | Loss: 0.00002590
Iteration 220/1000 | Loss: 0.00002590
Iteration 221/1000 | Loss: 0.00002590
Iteration 222/1000 | Loss: 0.00002590
Iteration 223/1000 | Loss: 0.00002590
Iteration 224/1000 | Loss: 0.00002590
Iteration 225/1000 | Loss: 0.00002589
Iteration 226/1000 | Loss: 0.00002589
Iteration 227/1000 | Loss: 0.00002589
Iteration 228/1000 | Loss: 0.00002589
Iteration 229/1000 | Loss: 0.00002589
Iteration 230/1000 | Loss: 0.00002589
Iteration 231/1000 | Loss: 0.00002589
Iteration 232/1000 | Loss: 0.00002589
Iteration 233/1000 | Loss: 0.00002589
Iteration 234/1000 | Loss: 0.00002589
Iteration 235/1000 | Loss: 0.00002589
Iteration 236/1000 | Loss: 0.00002589
Iteration 237/1000 | Loss: 0.00002589
Iteration 238/1000 | Loss: 0.00002589
Iteration 239/1000 | Loss: 0.00002589
Iteration 240/1000 | Loss: 0.00002589
Iteration 241/1000 | Loss: 0.00002589
Iteration 242/1000 | Loss: 0.00002589
Iteration 243/1000 | Loss: 0.00002588
Iteration 244/1000 | Loss: 0.00002588
Iteration 245/1000 | Loss: 0.00002588
Iteration 246/1000 | Loss: 0.00002588
Iteration 247/1000 | Loss: 0.00002588
Iteration 248/1000 | Loss: 0.00002588
Iteration 249/1000 | Loss: 0.00002588
Iteration 250/1000 | Loss: 0.00002588
Iteration 251/1000 | Loss: 0.00002588
Iteration 252/1000 | Loss: 0.00002588
Iteration 253/1000 | Loss: 0.00002588
Iteration 254/1000 | Loss: 0.00002587
Iteration 255/1000 | Loss: 0.00002587
Iteration 256/1000 | Loss: 0.00002587
Iteration 257/1000 | Loss: 0.00002587
Iteration 258/1000 | Loss: 0.00002587
Iteration 259/1000 | Loss: 0.00002587
Iteration 260/1000 | Loss: 0.00002587
Iteration 261/1000 | Loss: 0.00002587
Iteration 262/1000 | Loss: 0.00002586
Iteration 263/1000 | Loss: 0.00002586
Iteration 264/1000 | Loss: 0.00002586
Iteration 265/1000 | Loss: 0.00002586
Iteration 266/1000 | Loss: 0.00002586
Iteration 267/1000 | Loss: 0.00002586
Iteration 268/1000 | Loss: 0.00002586
Iteration 269/1000 | Loss: 0.00002586
Iteration 270/1000 | Loss: 0.00002586
Iteration 271/1000 | Loss: 0.00002586
Iteration 272/1000 | Loss: 0.00002586
Iteration 273/1000 | Loss: 0.00002586
Iteration 274/1000 | Loss: 0.00002586
Iteration 275/1000 | Loss: 0.00002586
Iteration 276/1000 | Loss: 0.00002586
Iteration 277/1000 | Loss: 0.00002586
Iteration 278/1000 | Loss: 0.00002586
Iteration 279/1000 | Loss: 0.00002585
Iteration 280/1000 | Loss: 0.00002585
Iteration 281/1000 | Loss: 0.00002585
Iteration 282/1000 | Loss: 0.00002585
Iteration 283/1000 | Loss: 0.00002585
Iteration 284/1000 | Loss: 0.00002585
Iteration 285/1000 | Loss: 0.00002585
Iteration 286/1000 | Loss: 0.00002585
Iteration 287/1000 | Loss: 0.00002585
Iteration 288/1000 | Loss: 0.00002585
Iteration 289/1000 | Loss: 0.00002585
Iteration 290/1000 | Loss: 0.00002585
Iteration 291/1000 | Loss: 0.00002585
Iteration 292/1000 | Loss: 0.00002585
Iteration 293/1000 | Loss: 0.00002584
Iteration 294/1000 | Loss: 0.00002584
Iteration 295/1000 | Loss: 0.00002584
Iteration 296/1000 | Loss: 0.00002584
Iteration 297/1000 | Loss: 0.00002584
Iteration 298/1000 | Loss: 0.00002584
Iteration 299/1000 | Loss: 0.00002584
Iteration 300/1000 | Loss: 0.00002584
Iteration 301/1000 | Loss: 0.00002584
Iteration 302/1000 | Loss: 0.00002584
Iteration 303/1000 | Loss: 0.00002584
Iteration 304/1000 | Loss: 0.00002584
Iteration 305/1000 | Loss: 0.00002583
Iteration 306/1000 | Loss: 0.00002583
Iteration 307/1000 | Loss: 0.00002583
Iteration 308/1000 | Loss: 0.00002583
Iteration 309/1000 | Loss: 0.00002583
Iteration 310/1000 | Loss: 0.00002583
Iteration 311/1000 | Loss: 0.00002583
Iteration 312/1000 | Loss: 0.00002583
Iteration 313/1000 | Loss: 0.00002583
Iteration 314/1000 | Loss: 0.00002582
Iteration 315/1000 | Loss: 0.00002582
Iteration 316/1000 | Loss: 0.00002582
Iteration 317/1000 | Loss: 0.00002582
Iteration 318/1000 | Loss: 0.00002582
Iteration 319/1000 | Loss: 0.00002582
Iteration 320/1000 | Loss: 0.00002582
Iteration 321/1000 | Loss: 0.00002582
Iteration 322/1000 | Loss: 0.00002581
Iteration 323/1000 | Loss: 0.00002581
Iteration 324/1000 | Loss: 0.00002581
Iteration 325/1000 | Loss: 0.00002581
Iteration 326/1000 | Loss: 0.00002581
Iteration 327/1000 | Loss: 0.00002581
Iteration 328/1000 | Loss: 0.00002581
Iteration 329/1000 | Loss: 0.00002581
Iteration 330/1000 | Loss: 0.00002581
Iteration 331/1000 | Loss: 0.00002581
Iteration 332/1000 | Loss: 0.00002581
Iteration 333/1000 | Loss: 0.00002581
Iteration 334/1000 | Loss: 0.00002581
Iteration 335/1000 | Loss: 0.00002581
Iteration 336/1000 | Loss: 0.00002581
Iteration 337/1000 | Loss: 0.00002580
Iteration 338/1000 | Loss: 0.00002580
Iteration 339/1000 | Loss: 0.00002580
Iteration 340/1000 | Loss: 0.00002580
Iteration 341/1000 | Loss: 0.00002580
Iteration 342/1000 | Loss: 0.00002580
Iteration 343/1000 | Loss: 0.00002580
Iteration 344/1000 | Loss: 0.00002580
Iteration 345/1000 | Loss: 0.00002580
Iteration 346/1000 | Loss: 0.00002580
Iteration 347/1000 | Loss: 0.00002580
Iteration 348/1000 | Loss: 0.00002580
Iteration 349/1000 | Loss: 0.00002580
Iteration 350/1000 | Loss: 0.00002580
Iteration 351/1000 | Loss: 0.00002580
Iteration 352/1000 | Loss: 0.00002580
Iteration 353/1000 | Loss: 0.00002580
Iteration 354/1000 | Loss: 0.00002580
Iteration 355/1000 | Loss: 0.00002580
Iteration 356/1000 | Loss: 0.00002580
Iteration 357/1000 | Loss: 0.00002580
Iteration 358/1000 | Loss: 0.00002580
Iteration 359/1000 | Loss: 0.00002580
Iteration 360/1000 | Loss: 0.00002580
Iteration 361/1000 | Loss: 0.00002580
Iteration 362/1000 | Loss: 0.00002580
Iteration 363/1000 | Loss: 0.00002580
Iteration 364/1000 | Loss: 0.00002580
Iteration 365/1000 | Loss: 0.00002580
Iteration 366/1000 | Loss: 0.00002580
Iteration 367/1000 | Loss: 0.00002580
Iteration 368/1000 | Loss: 0.00002580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 368. Stopping optimization.
Last 5 losses: [2.579704778327141e-05, 2.579704778327141e-05, 2.579704778327141e-05, 2.579704778327141e-05, 2.579704778327141e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.579704778327141e-05

Optimization complete. Final v2v error: 4.155595779418945 mm

Highest mean error: 5.294284820556641 mm for frame 78

Lowest mean error: 3.3690707683563232 mm for frame 59

Saving results

Total time: 77.93833589553833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468136
Iteration 2/25 | Loss: 0.00100270
Iteration 3/25 | Loss: 0.00083309
Iteration 4/25 | Loss: 0.00081233
Iteration 5/25 | Loss: 0.00080432
Iteration 6/25 | Loss: 0.00080256
Iteration 7/25 | Loss: 0.00080218
Iteration 8/25 | Loss: 0.00080218
Iteration 9/25 | Loss: 0.00080218
Iteration 10/25 | Loss: 0.00080218
Iteration 11/25 | Loss: 0.00080218
Iteration 12/25 | Loss: 0.00080218
Iteration 13/25 | Loss: 0.00080218
Iteration 14/25 | Loss: 0.00080218
Iteration 15/25 | Loss: 0.00080218
Iteration 16/25 | Loss: 0.00080218
Iteration 17/25 | Loss: 0.00080218
Iteration 18/25 | Loss: 0.00080218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008021807298064232, 0.0008021807298064232, 0.0008021807298064232, 0.0008021807298064232, 0.0008021807298064232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008021807298064232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15542197
Iteration 2/25 | Loss: 0.00137946
Iteration 3/25 | Loss: 0.00137945
Iteration 4/25 | Loss: 0.00137945
Iteration 5/25 | Loss: 0.00137945
Iteration 6/25 | Loss: 0.00137945
Iteration 7/25 | Loss: 0.00137945
Iteration 8/25 | Loss: 0.00137945
Iteration 9/25 | Loss: 0.00137945
Iteration 10/25 | Loss: 0.00137945
Iteration 11/25 | Loss: 0.00137945
Iteration 12/25 | Loss: 0.00137945
Iteration 13/25 | Loss: 0.00137945
Iteration 14/25 | Loss: 0.00137945
Iteration 15/25 | Loss: 0.00137945
Iteration 16/25 | Loss: 0.00137945
Iteration 17/25 | Loss: 0.00137945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013794455444440246, 0.0013794455444440246, 0.0013794455444440246, 0.0013794455444440246, 0.0013794455444440246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013794455444440246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137945
Iteration 2/1000 | Loss: 0.00002950
Iteration 3/1000 | Loss: 0.00001910
Iteration 4/1000 | Loss: 0.00001735
Iteration 5/1000 | Loss: 0.00001629
Iteration 6/1000 | Loss: 0.00001540
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00001474
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001452
Iteration 11/1000 | Loss: 0.00001439
Iteration 12/1000 | Loss: 0.00001426
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001415
Iteration 15/1000 | Loss: 0.00001408
Iteration 16/1000 | Loss: 0.00001405
Iteration 17/1000 | Loss: 0.00001404
Iteration 18/1000 | Loss: 0.00001404
Iteration 19/1000 | Loss: 0.00001403
Iteration 20/1000 | Loss: 0.00001399
Iteration 21/1000 | Loss: 0.00001398
Iteration 22/1000 | Loss: 0.00001397
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001396
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001392
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001392
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001389
Iteration 54/1000 | Loss: 0.00001389
Iteration 55/1000 | Loss: 0.00001389
Iteration 56/1000 | Loss: 0.00001389
Iteration 57/1000 | Loss: 0.00001389
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001388
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001386
Iteration 85/1000 | Loss: 0.00001386
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001385
Iteration 96/1000 | Loss: 0.00001385
Iteration 97/1000 | Loss: 0.00001384
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001383
Iteration 104/1000 | Loss: 0.00001383
Iteration 105/1000 | Loss: 0.00001383
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001383
Iteration 112/1000 | Loss: 0.00001382
Iteration 113/1000 | Loss: 0.00001382
Iteration 114/1000 | Loss: 0.00001382
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001382
Iteration 117/1000 | Loss: 0.00001382
Iteration 118/1000 | Loss: 0.00001382
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001379
Iteration 135/1000 | Loss: 0.00001379
Iteration 136/1000 | Loss: 0.00001379
Iteration 137/1000 | Loss: 0.00001379
Iteration 138/1000 | Loss: 0.00001379
Iteration 139/1000 | Loss: 0.00001379
Iteration 140/1000 | Loss: 0.00001379
Iteration 141/1000 | Loss: 0.00001379
Iteration 142/1000 | Loss: 0.00001379
Iteration 143/1000 | Loss: 0.00001379
Iteration 144/1000 | Loss: 0.00001379
Iteration 145/1000 | Loss: 0.00001379
Iteration 146/1000 | Loss: 0.00001379
Iteration 147/1000 | Loss: 0.00001379
Iteration 148/1000 | Loss: 0.00001379
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001379
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001379
Iteration 153/1000 | Loss: 0.00001379
Iteration 154/1000 | Loss: 0.00001379
Iteration 155/1000 | Loss: 0.00001379
Iteration 156/1000 | Loss: 0.00001379
Iteration 157/1000 | Loss: 0.00001379
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3788553587801289e-05, 1.3788553587801289e-05, 1.3788553587801289e-05, 1.3788553587801289e-05, 1.3788553587801289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3788553587801289e-05

Optimization complete. Final v2v error: 3.1474380493164062 mm

Highest mean error: 3.817194700241089 mm for frame 171

Lowest mean error: 2.830087184906006 mm for frame 238

Saving results

Total time: 43.11419987678528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046989
Iteration 2/25 | Loss: 0.00170313
Iteration 3/25 | Loss: 0.00114712
Iteration 4/25 | Loss: 0.00106047
Iteration 5/25 | Loss: 0.00099755
Iteration 6/25 | Loss: 0.00102878
Iteration 7/25 | Loss: 0.00098981
Iteration 8/25 | Loss: 0.00095682
Iteration 9/25 | Loss: 0.00093340
Iteration 10/25 | Loss: 0.00091316
Iteration 11/25 | Loss: 0.00090147
Iteration 12/25 | Loss: 0.00091508
Iteration 13/25 | Loss: 0.00089803
Iteration 14/25 | Loss: 0.00088855
Iteration 15/25 | Loss: 0.00087643
Iteration 16/25 | Loss: 0.00087378
Iteration 17/25 | Loss: 0.00087211
Iteration 18/25 | Loss: 0.00087143
Iteration 19/25 | Loss: 0.00087433
Iteration 20/25 | Loss: 0.00087568
Iteration 21/25 | Loss: 0.00087317
Iteration 22/25 | Loss: 0.00086897
Iteration 23/25 | Loss: 0.00086743
Iteration 24/25 | Loss: 0.00086652
Iteration 25/25 | Loss: 0.00086389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05531120
Iteration 2/25 | Loss: 0.00099607
Iteration 3/25 | Loss: 0.00099605
Iteration 4/25 | Loss: 0.00099605
Iteration 5/25 | Loss: 0.00099605
Iteration 6/25 | Loss: 0.00099605
Iteration 7/25 | Loss: 0.00099605
Iteration 8/25 | Loss: 0.00099605
Iteration 9/25 | Loss: 0.00099605
Iteration 10/25 | Loss: 0.00099605
Iteration 11/25 | Loss: 0.00099605
Iteration 12/25 | Loss: 0.00099605
Iteration 13/25 | Loss: 0.00099605
Iteration 14/25 | Loss: 0.00099605
Iteration 15/25 | Loss: 0.00099605
Iteration 16/25 | Loss: 0.00099605
Iteration 17/25 | Loss: 0.00099605
Iteration 18/25 | Loss: 0.00099605
Iteration 19/25 | Loss: 0.00099605
Iteration 20/25 | Loss: 0.00099605
Iteration 21/25 | Loss: 0.00099605
Iteration 22/25 | Loss: 0.00099605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000996046932414174, 0.000996046932414174, 0.000996046932414174, 0.000996046932414174, 0.000996046932414174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000996046932414174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099605
Iteration 2/1000 | Loss: 0.00003466
Iteration 3/1000 | Loss: 0.00002778
Iteration 4/1000 | Loss: 0.00002555
Iteration 5/1000 | Loss: 0.00024033
Iteration 6/1000 | Loss: 0.00060917
Iteration 7/1000 | Loss: 0.00002467
Iteration 8/1000 | Loss: 0.00002348
Iteration 9/1000 | Loss: 0.00027125
Iteration 10/1000 | Loss: 0.00002375
Iteration 11/1000 | Loss: 0.00002289
Iteration 12/1000 | Loss: 0.00002270
Iteration 13/1000 | Loss: 0.00002254
Iteration 14/1000 | Loss: 0.00002253
Iteration 15/1000 | Loss: 0.00002252
Iteration 16/1000 | Loss: 0.00002241
Iteration 17/1000 | Loss: 0.00002236
Iteration 18/1000 | Loss: 0.00002235
Iteration 19/1000 | Loss: 0.00002231
Iteration 20/1000 | Loss: 0.00002231
Iteration 21/1000 | Loss: 0.00002230
Iteration 22/1000 | Loss: 0.00002229
Iteration 23/1000 | Loss: 0.00002224
Iteration 24/1000 | Loss: 0.00002223
Iteration 25/1000 | Loss: 0.00002223
Iteration 26/1000 | Loss: 0.00002222
Iteration 27/1000 | Loss: 0.00002222
Iteration 28/1000 | Loss: 0.00002222
Iteration 29/1000 | Loss: 0.00002221
Iteration 30/1000 | Loss: 0.00002221
Iteration 31/1000 | Loss: 0.00002221
Iteration 32/1000 | Loss: 0.00002220
Iteration 33/1000 | Loss: 0.00002220
Iteration 34/1000 | Loss: 0.00002220
Iteration 35/1000 | Loss: 0.00002220
Iteration 36/1000 | Loss: 0.00002220
Iteration 37/1000 | Loss: 0.00002219
Iteration 38/1000 | Loss: 0.00002219
Iteration 39/1000 | Loss: 0.00002219
Iteration 40/1000 | Loss: 0.00002219
Iteration 41/1000 | Loss: 0.00002219
Iteration 42/1000 | Loss: 0.00002219
Iteration 43/1000 | Loss: 0.00002218
Iteration 44/1000 | Loss: 0.00002218
Iteration 45/1000 | Loss: 0.00002218
Iteration 46/1000 | Loss: 0.00002217
Iteration 47/1000 | Loss: 0.00002217
Iteration 48/1000 | Loss: 0.00002217
Iteration 49/1000 | Loss: 0.00002216
Iteration 50/1000 | Loss: 0.00002216
Iteration 51/1000 | Loss: 0.00002216
Iteration 52/1000 | Loss: 0.00002215
Iteration 53/1000 | Loss: 0.00002214
Iteration 54/1000 | Loss: 0.00002214
Iteration 55/1000 | Loss: 0.00002214
Iteration 56/1000 | Loss: 0.00002214
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002212
Iteration 59/1000 | Loss: 0.00002211
Iteration 60/1000 | Loss: 0.00002211
Iteration 61/1000 | Loss: 0.00002211
Iteration 62/1000 | Loss: 0.00002211
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002211
Iteration 65/1000 | Loss: 0.00002211
Iteration 66/1000 | Loss: 0.00002211
Iteration 67/1000 | Loss: 0.00002211
Iteration 68/1000 | Loss: 0.00002211
Iteration 69/1000 | Loss: 0.00002211
Iteration 70/1000 | Loss: 0.00002211
Iteration 71/1000 | Loss: 0.00002211
Iteration 72/1000 | Loss: 0.00002210
Iteration 73/1000 | Loss: 0.00002210
Iteration 74/1000 | Loss: 0.00002210
Iteration 75/1000 | Loss: 0.00002210
Iteration 76/1000 | Loss: 0.00002210
Iteration 77/1000 | Loss: 0.00002210
Iteration 78/1000 | Loss: 0.00002210
Iteration 79/1000 | Loss: 0.00002210
Iteration 80/1000 | Loss: 0.00002210
Iteration 81/1000 | Loss: 0.00002209
Iteration 82/1000 | Loss: 0.00002209
Iteration 83/1000 | Loss: 0.00002209
Iteration 84/1000 | Loss: 0.00002209
Iteration 85/1000 | Loss: 0.00002209
Iteration 86/1000 | Loss: 0.00002209
Iteration 87/1000 | Loss: 0.00002209
Iteration 88/1000 | Loss: 0.00002208
Iteration 89/1000 | Loss: 0.00002208
Iteration 90/1000 | Loss: 0.00002208
Iteration 91/1000 | Loss: 0.00002208
Iteration 92/1000 | Loss: 0.00002208
Iteration 93/1000 | Loss: 0.00002208
Iteration 94/1000 | Loss: 0.00002207
Iteration 95/1000 | Loss: 0.00002207
Iteration 96/1000 | Loss: 0.00002207
Iteration 97/1000 | Loss: 0.00002207
Iteration 98/1000 | Loss: 0.00002207
Iteration 99/1000 | Loss: 0.00002206
Iteration 100/1000 | Loss: 0.00002206
Iteration 101/1000 | Loss: 0.00002206
Iteration 102/1000 | Loss: 0.00002206
Iteration 103/1000 | Loss: 0.00002206
Iteration 104/1000 | Loss: 0.00002206
Iteration 105/1000 | Loss: 0.00002205
Iteration 106/1000 | Loss: 0.00002205
Iteration 107/1000 | Loss: 0.00002205
Iteration 108/1000 | Loss: 0.00002205
Iteration 109/1000 | Loss: 0.00002205
Iteration 110/1000 | Loss: 0.00002205
Iteration 111/1000 | Loss: 0.00002205
Iteration 112/1000 | Loss: 0.00002205
Iteration 113/1000 | Loss: 0.00002205
Iteration 114/1000 | Loss: 0.00002204
Iteration 115/1000 | Loss: 0.00002204
Iteration 116/1000 | Loss: 0.00002204
Iteration 117/1000 | Loss: 0.00002204
Iteration 118/1000 | Loss: 0.00002204
Iteration 119/1000 | Loss: 0.00002204
Iteration 120/1000 | Loss: 0.00002204
Iteration 121/1000 | Loss: 0.00002203
Iteration 122/1000 | Loss: 0.00002203
Iteration 123/1000 | Loss: 0.00002203
Iteration 124/1000 | Loss: 0.00002203
Iteration 125/1000 | Loss: 0.00002203
Iteration 126/1000 | Loss: 0.00002203
Iteration 127/1000 | Loss: 0.00002202
Iteration 128/1000 | Loss: 0.00002202
Iteration 129/1000 | Loss: 0.00002202
Iteration 130/1000 | Loss: 0.00002202
Iteration 131/1000 | Loss: 0.00002202
Iteration 132/1000 | Loss: 0.00002202
Iteration 133/1000 | Loss: 0.00002202
Iteration 134/1000 | Loss: 0.00002202
Iteration 135/1000 | Loss: 0.00002202
Iteration 136/1000 | Loss: 0.00002202
Iteration 137/1000 | Loss: 0.00002202
Iteration 138/1000 | Loss: 0.00002202
Iteration 139/1000 | Loss: 0.00002202
Iteration 140/1000 | Loss: 0.00002202
Iteration 141/1000 | Loss: 0.00002201
Iteration 142/1000 | Loss: 0.00002201
Iteration 143/1000 | Loss: 0.00002201
Iteration 144/1000 | Loss: 0.00002201
Iteration 145/1000 | Loss: 0.00002201
Iteration 146/1000 | Loss: 0.00002201
Iteration 147/1000 | Loss: 0.00002201
Iteration 148/1000 | Loss: 0.00002201
Iteration 149/1000 | Loss: 0.00002201
Iteration 150/1000 | Loss: 0.00002201
Iteration 151/1000 | Loss: 0.00002201
Iteration 152/1000 | Loss: 0.00002201
Iteration 153/1000 | Loss: 0.00002201
Iteration 154/1000 | Loss: 0.00002201
Iteration 155/1000 | Loss: 0.00002201
Iteration 156/1000 | Loss: 0.00002201
Iteration 157/1000 | Loss: 0.00002201
Iteration 158/1000 | Loss: 0.00002201
Iteration 159/1000 | Loss: 0.00002201
Iteration 160/1000 | Loss: 0.00002201
Iteration 161/1000 | Loss: 0.00002201
Iteration 162/1000 | Loss: 0.00002201
Iteration 163/1000 | Loss: 0.00002201
Iteration 164/1000 | Loss: 0.00002201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.2009171516401693e-05, 2.2009171516401693e-05, 2.2009171516401693e-05, 2.2009171516401693e-05, 2.2009171516401693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2009171516401693e-05

Optimization complete. Final v2v error: 3.97830867767334 mm

Highest mean error: 4.763936996459961 mm for frame 94

Lowest mean error: 3.708327531814575 mm for frame 16

Saving results

Total time: 88.4598388671875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773008
Iteration 2/25 | Loss: 0.00129992
Iteration 3/25 | Loss: 0.00093847
Iteration 4/25 | Loss: 0.00087219
Iteration 5/25 | Loss: 0.00085339
Iteration 6/25 | Loss: 0.00085056
Iteration 7/25 | Loss: 0.00085055
Iteration 8/25 | Loss: 0.00085055
Iteration 9/25 | Loss: 0.00085055
Iteration 10/25 | Loss: 0.00085055
Iteration 11/25 | Loss: 0.00085055
Iteration 12/25 | Loss: 0.00085055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008505477453581989, 0.0008505477453581989, 0.0008505477453581989, 0.0008505477453581989, 0.0008505477453581989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008505477453581989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58158255
Iteration 2/25 | Loss: 0.00120372
Iteration 3/25 | Loss: 0.00120371
Iteration 4/25 | Loss: 0.00120371
Iteration 5/25 | Loss: 0.00120371
Iteration 6/25 | Loss: 0.00120371
Iteration 7/25 | Loss: 0.00120371
Iteration 8/25 | Loss: 0.00120371
Iteration 9/25 | Loss: 0.00120371
Iteration 10/25 | Loss: 0.00120371
Iteration 11/25 | Loss: 0.00120371
Iteration 12/25 | Loss: 0.00120371
Iteration 13/25 | Loss: 0.00120371
Iteration 14/25 | Loss: 0.00120371
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001203708816319704, 0.001203708816319704, 0.001203708816319704, 0.001203708816319704, 0.001203708816319704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001203708816319704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120371
Iteration 2/1000 | Loss: 0.00003657
Iteration 3/1000 | Loss: 0.00002826
Iteration 4/1000 | Loss: 0.00002551
Iteration 5/1000 | Loss: 0.00002382
Iteration 6/1000 | Loss: 0.00002274
Iteration 7/1000 | Loss: 0.00002193
Iteration 8/1000 | Loss: 0.00002145
Iteration 9/1000 | Loss: 0.00002119
Iteration 10/1000 | Loss: 0.00002090
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00002072
Iteration 13/1000 | Loss: 0.00002072
Iteration 14/1000 | Loss: 0.00002071
Iteration 15/1000 | Loss: 0.00002070
Iteration 16/1000 | Loss: 0.00002066
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00002057
Iteration 19/1000 | Loss: 0.00002053
Iteration 20/1000 | Loss: 0.00002052
Iteration 21/1000 | Loss: 0.00002049
Iteration 22/1000 | Loss: 0.00002049
Iteration 23/1000 | Loss: 0.00002049
Iteration 24/1000 | Loss: 0.00002048
Iteration 25/1000 | Loss: 0.00002048
Iteration 26/1000 | Loss: 0.00002048
Iteration 27/1000 | Loss: 0.00002048
Iteration 28/1000 | Loss: 0.00002047
Iteration 29/1000 | Loss: 0.00002047
Iteration 30/1000 | Loss: 0.00002047
Iteration 31/1000 | Loss: 0.00002047
Iteration 32/1000 | Loss: 0.00002047
Iteration 33/1000 | Loss: 0.00002047
Iteration 34/1000 | Loss: 0.00002047
Iteration 35/1000 | Loss: 0.00002047
Iteration 36/1000 | Loss: 0.00002047
Iteration 37/1000 | Loss: 0.00002046
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002046
Iteration 40/1000 | Loss: 0.00002046
Iteration 41/1000 | Loss: 0.00002045
Iteration 42/1000 | Loss: 0.00002045
Iteration 43/1000 | Loss: 0.00002045
Iteration 44/1000 | Loss: 0.00002045
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002043
Iteration 48/1000 | Loss: 0.00002043
Iteration 49/1000 | Loss: 0.00002042
Iteration 50/1000 | Loss: 0.00002042
Iteration 51/1000 | Loss: 0.00002042
Iteration 52/1000 | Loss: 0.00002041
Iteration 53/1000 | Loss: 0.00002041
Iteration 54/1000 | Loss: 0.00002040
Iteration 55/1000 | Loss: 0.00002040
Iteration 56/1000 | Loss: 0.00002040
Iteration 57/1000 | Loss: 0.00002040
Iteration 58/1000 | Loss: 0.00002040
Iteration 59/1000 | Loss: 0.00002040
Iteration 60/1000 | Loss: 0.00002040
Iteration 61/1000 | Loss: 0.00002039
Iteration 62/1000 | Loss: 0.00002039
Iteration 63/1000 | Loss: 0.00002039
Iteration 64/1000 | Loss: 0.00002039
Iteration 65/1000 | Loss: 0.00002038
Iteration 66/1000 | Loss: 0.00002038
Iteration 67/1000 | Loss: 0.00002038
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002038
Iteration 70/1000 | Loss: 0.00002037
Iteration 71/1000 | Loss: 0.00002037
Iteration 72/1000 | Loss: 0.00002037
Iteration 73/1000 | Loss: 0.00002037
Iteration 74/1000 | Loss: 0.00002037
Iteration 75/1000 | Loss: 0.00002037
Iteration 76/1000 | Loss: 0.00002037
Iteration 77/1000 | Loss: 0.00002037
Iteration 78/1000 | Loss: 0.00002037
Iteration 79/1000 | Loss: 0.00002037
Iteration 80/1000 | Loss: 0.00002036
Iteration 81/1000 | Loss: 0.00002036
Iteration 82/1000 | Loss: 0.00002036
Iteration 83/1000 | Loss: 0.00002036
Iteration 84/1000 | Loss: 0.00002036
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002035
Iteration 87/1000 | Loss: 0.00002035
Iteration 88/1000 | Loss: 0.00002035
Iteration 89/1000 | Loss: 0.00002035
Iteration 90/1000 | Loss: 0.00002035
Iteration 91/1000 | Loss: 0.00002035
Iteration 92/1000 | Loss: 0.00002035
Iteration 93/1000 | Loss: 0.00002034
Iteration 94/1000 | Loss: 0.00002034
Iteration 95/1000 | Loss: 0.00002034
Iteration 96/1000 | Loss: 0.00002034
Iteration 97/1000 | Loss: 0.00002034
Iteration 98/1000 | Loss: 0.00002034
Iteration 99/1000 | Loss: 0.00002033
Iteration 100/1000 | Loss: 0.00002033
Iteration 101/1000 | Loss: 0.00002033
Iteration 102/1000 | Loss: 0.00002033
Iteration 103/1000 | Loss: 0.00002033
Iteration 104/1000 | Loss: 0.00002033
Iteration 105/1000 | Loss: 0.00002033
Iteration 106/1000 | Loss: 0.00002033
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002033
Iteration 111/1000 | Loss: 0.00002033
Iteration 112/1000 | Loss: 0.00002033
Iteration 113/1000 | Loss: 0.00002033
Iteration 114/1000 | Loss: 0.00002033
Iteration 115/1000 | Loss: 0.00002033
Iteration 116/1000 | Loss: 0.00002033
Iteration 117/1000 | Loss: 0.00002033
Iteration 118/1000 | Loss: 0.00002033
Iteration 119/1000 | Loss: 0.00002033
Iteration 120/1000 | Loss: 0.00002033
Iteration 121/1000 | Loss: 0.00002033
Iteration 122/1000 | Loss: 0.00002033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.032511474681087e-05, 2.032511474681087e-05, 2.032511474681087e-05, 2.032511474681087e-05, 2.032511474681087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.032511474681087e-05

Optimization complete. Final v2v error: 3.7912473678588867 mm

Highest mean error: 4.106928825378418 mm for frame 70

Lowest mean error: 3.513354778289795 mm for frame 184

Saving results

Total time: 38.271851539611816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453516
Iteration 2/25 | Loss: 0.00121983
Iteration 3/25 | Loss: 0.00087196
Iteration 4/25 | Loss: 0.00077891
Iteration 5/25 | Loss: 0.00076148
Iteration 6/25 | Loss: 0.00075908
Iteration 7/25 | Loss: 0.00075878
Iteration 8/25 | Loss: 0.00075878
Iteration 9/25 | Loss: 0.00075878
Iteration 10/25 | Loss: 0.00075878
Iteration 11/25 | Loss: 0.00075878
Iteration 12/25 | Loss: 0.00075878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007587779546156526, 0.0007587779546156526, 0.0007587779546156526, 0.0007587779546156526, 0.0007587779546156526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007587779546156526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70241141
Iteration 2/25 | Loss: 0.00123751
Iteration 3/25 | Loss: 0.00123751
Iteration 4/25 | Loss: 0.00123751
Iteration 5/25 | Loss: 0.00123751
Iteration 6/25 | Loss: 0.00123751
Iteration 7/25 | Loss: 0.00123751
Iteration 8/25 | Loss: 0.00123751
Iteration 9/25 | Loss: 0.00123751
Iteration 10/25 | Loss: 0.00123751
Iteration 11/25 | Loss: 0.00123751
Iteration 12/25 | Loss: 0.00123751
Iteration 13/25 | Loss: 0.00123751
Iteration 14/25 | Loss: 0.00123751
Iteration 15/25 | Loss: 0.00123751
Iteration 16/25 | Loss: 0.00123751
Iteration 17/25 | Loss: 0.00123751
Iteration 18/25 | Loss: 0.00123751
Iteration 19/25 | Loss: 0.00123751
Iteration 20/25 | Loss: 0.00123751
Iteration 21/25 | Loss: 0.00123751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012375066289678216, 0.0012375066289678216, 0.0012375066289678216, 0.0012375066289678216, 0.0012375066289678216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012375066289678216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123751
Iteration 2/1000 | Loss: 0.00002548
Iteration 3/1000 | Loss: 0.00001803
Iteration 4/1000 | Loss: 0.00001670
Iteration 5/1000 | Loss: 0.00001584
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001464
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001380
Iteration 12/1000 | Loss: 0.00001365
Iteration 13/1000 | Loss: 0.00001363
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001361
Iteration 16/1000 | Loss: 0.00001360
Iteration 17/1000 | Loss: 0.00001359
Iteration 18/1000 | Loss: 0.00001359
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001351
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001329
Iteration 112/1000 | Loss: 0.00001329
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001326
Iteration 124/1000 | Loss: 0.00001326
Iteration 125/1000 | Loss: 0.00001326
Iteration 126/1000 | Loss: 0.00001326
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001325
Iteration 129/1000 | Loss: 0.00001325
Iteration 130/1000 | Loss: 0.00001325
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001323
Iteration 136/1000 | Loss: 0.00001323
Iteration 137/1000 | Loss: 0.00001323
Iteration 138/1000 | Loss: 0.00001322
Iteration 139/1000 | Loss: 0.00001322
Iteration 140/1000 | Loss: 0.00001322
Iteration 141/1000 | Loss: 0.00001322
Iteration 142/1000 | Loss: 0.00001322
Iteration 143/1000 | Loss: 0.00001322
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001319
Iteration 153/1000 | Loss: 0.00001319
Iteration 154/1000 | Loss: 0.00001319
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001319
Iteration 159/1000 | Loss: 0.00001318
Iteration 160/1000 | Loss: 0.00001318
Iteration 161/1000 | Loss: 0.00001318
Iteration 162/1000 | Loss: 0.00001318
Iteration 163/1000 | Loss: 0.00001318
Iteration 164/1000 | Loss: 0.00001318
Iteration 165/1000 | Loss: 0.00001318
Iteration 166/1000 | Loss: 0.00001318
Iteration 167/1000 | Loss: 0.00001318
Iteration 168/1000 | Loss: 0.00001318
Iteration 169/1000 | Loss: 0.00001318
Iteration 170/1000 | Loss: 0.00001318
Iteration 171/1000 | Loss: 0.00001318
Iteration 172/1000 | Loss: 0.00001317
Iteration 173/1000 | Loss: 0.00001317
Iteration 174/1000 | Loss: 0.00001317
Iteration 175/1000 | Loss: 0.00001317
Iteration 176/1000 | Loss: 0.00001317
Iteration 177/1000 | Loss: 0.00001317
Iteration 178/1000 | Loss: 0.00001317
Iteration 179/1000 | Loss: 0.00001317
Iteration 180/1000 | Loss: 0.00001317
Iteration 181/1000 | Loss: 0.00001317
Iteration 182/1000 | Loss: 0.00001317
Iteration 183/1000 | Loss: 0.00001317
Iteration 184/1000 | Loss: 0.00001317
Iteration 185/1000 | Loss: 0.00001317
Iteration 186/1000 | Loss: 0.00001317
Iteration 187/1000 | Loss: 0.00001317
Iteration 188/1000 | Loss: 0.00001317
Iteration 189/1000 | Loss: 0.00001317
Iteration 190/1000 | Loss: 0.00001317
Iteration 191/1000 | Loss: 0.00001317
Iteration 192/1000 | Loss: 0.00001317
Iteration 193/1000 | Loss: 0.00001317
Iteration 194/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.3169175872462802e-05, 1.3169175872462802e-05, 1.3169175872462802e-05, 1.3169175872462802e-05, 1.3169175872462802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3169175872462802e-05

Optimization complete. Final v2v error: 3.0405750274658203 mm

Highest mean error: 4.055347442626953 mm for frame 104

Lowest mean error: 2.81923770904541 mm for frame 179

Saving results

Total time: 45.29905986785889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00627226
Iteration 2/25 | Loss: 0.00143857
Iteration 3/25 | Loss: 0.00115507
Iteration 4/25 | Loss: 0.00104485
Iteration 5/25 | Loss: 0.00106543
Iteration 6/25 | Loss: 0.00106965
Iteration 7/25 | Loss: 0.00099581
Iteration 8/25 | Loss: 0.00093512
Iteration 9/25 | Loss: 0.00091710
Iteration 10/25 | Loss: 0.00091389
Iteration 11/25 | Loss: 0.00091313
Iteration 12/25 | Loss: 0.00091285
Iteration 13/25 | Loss: 0.00091268
Iteration 14/25 | Loss: 0.00091242
Iteration 15/25 | Loss: 0.00091148
Iteration 16/25 | Loss: 0.00090921
Iteration 17/25 | Loss: 0.00090728
Iteration 18/25 | Loss: 0.00090658
Iteration 19/25 | Loss: 0.00090642
Iteration 20/25 | Loss: 0.00090641
Iteration 21/25 | Loss: 0.00090641
Iteration 22/25 | Loss: 0.00090641
Iteration 23/25 | Loss: 0.00090641
Iteration 24/25 | Loss: 0.00090641
Iteration 25/25 | Loss: 0.00090641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45550275
Iteration 2/25 | Loss: 0.00142354
Iteration 3/25 | Loss: 0.00142343
Iteration 4/25 | Loss: 0.00142343
Iteration 5/25 | Loss: 0.00142343
Iteration 6/25 | Loss: 0.00142343
Iteration 7/25 | Loss: 0.00142343
Iteration 8/25 | Loss: 0.00142343
Iteration 9/25 | Loss: 0.00142343
Iteration 10/25 | Loss: 0.00142343
Iteration 11/25 | Loss: 0.00142343
Iteration 12/25 | Loss: 0.00142343
Iteration 13/25 | Loss: 0.00142343
Iteration 14/25 | Loss: 0.00142343
Iteration 15/25 | Loss: 0.00142343
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014234315603971481, 0.0014234315603971481, 0.0014234315603971481, 0.0014234315603971481, 0.0014234315603971481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014234315603971481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142343
Iteration 2/1000 | Loss: 0.00014410
Iteration 3/1000 | Loss: 0.00007664
Iteration 4/1000 | Loss: 0.00005515
Iteration 5/1000 | Loss: 0.00003895
Iteration 6/1000 | Loss: 0.00003204
Iteration 7/1000 | Loss: 0.00003016
Iteration 8/1000 | Loss: 0.00002915
Iteration 9/1000 | Loss: 0.00002811
Iteration 10/1000 | Loss: 0.00002749
Iteration 11/1000 | Loss: 0.00002693
Iteration 12/1000 | Loss: 0.00002655
Iteration 13/1000 | Loss: 0.00002633
Iteration 14/1000 | Loss: 0.00002604
Iteration 15/1000 | Loss: 0.00002602
Iteration 16/1000 | Loss: 0.00002599
Iteration 17/1000 | Loss: 0.00002581
Iteration 18/1000 | Loss: 0.00002563
Iteration 19/1000 | Loss: 0.00002551
Iteration 20/1000 | Loss: 0.00002548
Iteration 21/1000 | Loss: 0.00002547
Iteration 22/1000 | Loss: 0.00002546
Iteration 23/1000 | Loss: 0.00002545
Iteration 24/1000 | Loss: 0.00002544
Iteration 25/1000 | Loss: 0.00002543
Iteration 26/1000 | Loss: 0.00002543
Iteration 27/1000 | Loss: 0.00002542
Iteration 28/1000 | Loss: 0.00002542
Iteration 29/1000 | Loss: 0.00002541
Iteration 30/1000 | Loss: 0.00002541
Iteration 31/1000 | Loss: 0.00002540
Iteration 32/1000 | Loss: 0.00002540
Iteration 33/1000 | Loss: 0.00002539
Iteration 34/1000 | Loss: 0.00002538
Iteration 35/1000 | Loss: 0.00002536
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002534
Iteration 39/1000 | Loss: 0.00002533
Iteration 40/1000 | Loss: 0.00002533
Iteration 41/1000 | Loss: 0.00002532
Iteration 42/1000 | Loss: 0.00002531
Iteration 43/1000 | Loss: 0.00002530
Iteration 44/1000 | Loss: 0.00002524
Iteration 45/1000 | Loss: 0.00002522
Iteration 46/1000 | Loss: 0.00002521
Iteration 47/1000 | Loss: 0.00002520
Iteration 48/1000 | Loss: 0.00002520
Iteration 49/1000 | Loss: 0.00002519
Iteration 50/1000 | Loss: 0.00002518
Iteration 51/1000 | Loss: 0.00002517
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002515
Iteration 54/1000 | Loss: 0.00002514
Iteration 55/1000 | Loss: 0.00002513
Iteration 56/1000 | Loss: 0.00002513
Iteration 57/1000 | Loss: 0.00002512
Iteration 58/1000 | Loss: 0.00002512
Iteration 59/1000 | Loss: 0.00002511
Iteration 60/1000 | Loss: 0.00002511
Iteration 61/1000 | Loss: 0.00002511
Iteration 62/1000 | Loss: 0.00002511
Iteration 63/1000 | Loss: 0.00002511
Iteration 64/1000 | Loss: 0.00002511
Iteration 65/1000 | Loss: 0.00002511
Iteration 66/1000 | Loss: 0.00002510
Iteration 67/1000 | Loss: 0.00002510
Iteration 68/1000 | Loss: 0.00002510
Iteration 69/1000 | Loss: 0.00002509
Iteration 70/1000 | Loss: 0.00002509
Iteration 71/1000 | Loss: 0.00002508
Iteration 72/1000 | Loss: 0.00002508
Iteration 73/1000 | Loss: 0.00002508
Iteration 74/1000 | Loss: 0.00002508
Iteration 75/1000 | Loss: 0.00002507
Iteration 76/1000 | Loss: 0.00002507
Iteration 77/1000 | Loss: 0.00002507
Iteration 78/1000 | Loss: 0.00002507
Iteration 79/1000 | Loss: 0.00002506
Iteration 80/1000 | Loss: 0.00002506
Iteration 81/1000 | Loss: 0.00002506
Iteration 82/1000 | Loss: 0.00002505
Iteration 83/1000 | Loss: 0.00002505
Iteration 84/1000 | Loss: 0.00002505
Iteration 85/1000 | Loss: 0.00002505
Iteration 86/1000 | Loss: 0.00002505
Iteration 87/1000 | Loss: 0.00002505
Iteration 88/1000 | Loss: 0.00002505
Iteration 89/1000 | Loss: 0.00002504
Iteration 90/1000 | Loss: 0.00002504
Iteration 91/1000 | Loss: 0.00002504
Iteration 92/1000 | Loss: 0.00002504
Iteration 93/1000 | Loss: 0.00002504
Iteration 94/1000 | Loss: 0.00002504
Iteration 95/1000 | Loss: 0.00002504
Iteration 96/1000 | Loss: 0.00002504
Iteration 97/1000 | Loss: 0.00002504
Iteration 98/1000 | Loss: 0.00002504
Iteration 99/1000 | Loss: 0.00002504
Iteration 100/1000 | Loss: 0.00002504
Iteration 101/1000 | Loss: 0.00002504
Iteration 102/1000 | Loss: 0.00002503
Iteration 103/1000 | Loss: 0.00002503
Iteration 104/1000 | Loss: 0.00002503
Iteration 105/1000 | Loss: 0.00002503
Iteration 106/1000 | Loss: 0.00002503
Iteration 107/1000 | Loss: 0.00002503
Iteration 108/1000 | Loss: 0.00002503
Iteration 109/1000 | Loss: 0.00002503
Iteration 110/1000 | Loss: 0.00002503
Iteration 111/1000 | Loss: 0.00002503
Iteration 112/1000 | Loss: 0.00002503
Iteration 113/1000 | Loss: 0.00002503
Iteration 114/1000 | Loss: 0.00002503
Iteration 115/1000 | Loss: 0.00002503
Iteration 116/1000 | Loss: 0.00002503
Iteration 117/1000 | Loss: 0.00002503
Iteration 118/1000 | Loss: 0.00002503
Iteration 119/1000 | Loss: 0.00002502
Iteration 120/1000 | Loss: 0.00002502
Iteration 121/1000 | Loss: 0.00002502
Iteration 122/1000 | Loss: 0.00002502
Iteration 123/1000 | Loss: 0.00002502
Iteration 124/1000 | Loss: 0.00002502
Iteration 125/1000 | Loss: 0.00002502
Iteration 126/1000 | Loss: 0.00002502
Iteration 127/1000 | Loss: 0.00002501
Iteration 128/1000 | Loss: 0.00002501
Iteration 129/1000 | Loss: 0.00002501
Iteration 130/1000 | Loss: 0.00002501
Iteration 131/1000 | Loss: 0.00002500
Iteration 132/1000 | Loss: 0.00002500
Iteration 133/1000 | Loss: 0.00002500
Iteration 134/1000 | Loss: 0.00002500
Iteration 135/1000 | Loss: 0.00002500
Iteration 136/1000 | Loss: 0.00002500
Iteration 137/1000 | Loss: 0.00002500
Iteration 138/1000 | Loss: 0.00002500
Iteration 139/1000 | Loss: 0.00002500
Iteration 140/1000 | Loss: 0.00002500
Iteration 141/1000 | Loss: 0.00002500
Iteration 142/1000 | Loss: 0.00002500
Iteration 143/1000 | Loss: 0.00002500
Iteration 144/1000 | Loss: 0.00002500
Iteration 145/1000 | Loss: 0.00002500
Iteration 146/1000 | Loss: 0.00002500
Iteration 147/1000 | Loss: 0.00002500
Iteration 148/1000 | Loss: 0.00002500
Iteration 149/1000 | Loss: 0.00002500
Iteration 150/1000 | Loss: 0.00002500
Iteration 151/1000 | Loss: 0.00002500
Iteration 152/1000 | Loss: 0.00002500
Iteration 153/1000 | Loss: 0.00002500
Iteration 154/1000 | Loss: 0.00002500
Iteration 155/1000 | Loss: 0.00002500
Iteration 156/1000 | Loss: 0.00002500
Iteration 157/1000 | Loss: 0.00002500
Iteration 158/1000 | Loss: 0.00002500
Iteration 159/1000 | Loss: 0.00002500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.4995102648972534e-05, 2.4995102648972534e-05, 2.4995102648972534e-05, 2.4995102648972534e-05, 2.4995102648972534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4995102648972534e-05

Optimization complete. Final v2v error: 3.913889169692993 mm

Highest mean error: 5.932057857513428 mm for frame 139

Lowest mean error: 3.0345799922943115 mm for frame 77

Saving results

Total time: 71.33918905258179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831821
Iteration 2/25 | Loss: 0.00112118
Iteration 3/25 | Loss: 0.00081096
Iteration 4/25 | Loss: 0.00078303
Iteration 5/25 | Loss: 0.00077336
Iteration 6/25 | Loss: 0.00077124
Iteration 7/25 | Loss: 0.00077076
Iteration 8/25 | Loss: 0.00077076
Iteration 9/25 | Loss: 0.00077076
Iteration 10/25 | Loss: 0.00077076
Iteration 11/25 | Loss: 0.00077076
Iteration 12/25 | Loss: 0.00077076
Iteration 13/25 | Loss: 0.00077076
Iteration 14/25 | Loss: 0.00077076
Iteration 15/25 | Loss: 0.00077076
Iteration 16/25 | Loss: 0.00077076
Iteration 17/25 | Loss: 0.00077076
Iteration 18/25 | Loss: 0.00077076
Iteration 19/25 | Loss: 0.00077076
Iteration 20/25 | Loss: 0.00077076
Iteration 21/25 | Loss: 0.00077076
Iteration 22/25 | Loss: 0.00077076
Iteration 23/25 | Loss: 0.00077076
Iteration 24/25 | Loss: 0.00077076
Iteration 25/25 | Loss: 0.00077076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35763407
Iteration 2/25 | Loss: 0.00112068
Iteration 3/25 | Loss: 0.00112067
Iteration 4/25 | Loss: 0.00112067
Iteration 5/25 | Loss: 0.00112067
Iteration 6/25 | Loss: 0.00112067
Iteration 7/25 | Loss: 0.00112067
Iteration 8/25 | Loss: 0.00112067
Iteration 9/25 | Loss: 0.00112067
Iteration 10/25 | Loss: 0.00112067
Iteration 11/25 | Loss: 0.00112067
Iteration 12/25 | Loss: 0.00112067
Iteration 13/25 | Loss: 0.00112067
Iteration 14/25 | Loss: 0.00112067
Iteration 15/25 | Loss: 0.00112067
Iteration 16/25 | Loss: 0.00112067
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011206695344299078, 0.0011206695344299078, 0.0011206695344299078, 0.0011206695344299078, 0.0011206695344299078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011206695344299078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112067
Iteration 2/1000 | Loss: 0.00003838
Iteration 3/1000 | Loss: 0.00002766
Iteration 4/1000 | Loss: 0.00002385
Iteration 5/1000 | Loss: 0.00002228
Iteration 6/1000 | Loss: 0.00002128
Iteration 7/1000 | Loss: 0.00002058
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00001963
Iteration 10/1000 | Loss: 0.00001939
Iteration 11/1000 | Loss: 0.00001928
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001870
Iteration 17/1000 | Loss: 0.00001864
Iteration 18/1000 | Loss: 0.00001861
Iteration 19/1000 | Loss: 0.00001861
Iteration 20/1000 | Loss: 0.00001860
Iteration 21/1000 | Loss: 0.00001860
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001859
Iteration 24/1000 | Loss: 0.00001857
Iteration 25/1000 | Loss: 0.00001854
Iteration 26/1000 | Loss: 0.00001854
Iteration 27/1000 | Loss: 0.00001853
Iteration 28/1000 | Loss: 0.00001853
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001848
Iteration 33/1000 | Loss: 0.00001847
Iteration 34/1000 | Loss: 0.00001847
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001846
Iteration 37/1000 | Loss: 0.00001846
Iteration 38/1000 | Loss: 0.00001846
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001846
Iteration 41/1000 | Loss: 0.00001846
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001845
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001844
Iteration 47/1000 | Loss: 0.00001843
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001843
Iteration 50/1000 | Loss: 0.00001842
Iteration 51/1000 | Loss: 0.00001841
Iteration 52/1000 | Loss: 0.00001841
Iteration 53/1000 | Loss: 0.00001840
Iteration 54/1000 | Loss: 0.00001840
Iteration 55/1000 | Loss: 0.00001840
Iteration 56/1000 | Loss: 0.00001839
Iteration 57/1000 | Loss: 0.00001838
Iteration 58/1000 | Loss: 0.00001837
Iteration 59/1000 | Loss: 0.00001837
Iteration 60/1000 | Loss: 0.00001837
Iteration 61/1000 | Loss: 0.00001836
Iteration 62/1000 | Loss: 0.00001835
Iteration 63/1000 | Loss: 0.00001834
Iteration 64/1000 | Loss: 0.00001834
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001833
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001829
Iteration 73/1000 | Loss: 0.00001829
Iteration 74/1000 | Loss: 0.00001828
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001827
Iteration 77/1000 | Loss: 0.00001827
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001826
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001825
Iteration 82/1000 | Loss: 0.00001825
Iteration 83/1000 | Loss: 0.00001825
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001824
Iteration 86/1000 | Loss: 0.00001824
Iteration 87/1000 | Loss: 0.00001824
Iteration 88/1000 | Loss: 0.00001824
Iteration 89/1000 | Loss: 0.00001824
Iteration 90/1000 | Loss: 0.00001824
Iteration 91/1000 | Loss: 0.00001823
Iteration 92/1000 | Loss: 0.00001823
Iteration 93/1000 | Loss: 0.00001823
Iteration 94/1000 | Loss: 0.00001823
Iteration 95/1000 | Loss: 0.00001822
Iteration 96/1000 | Loss: 0.00001822
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001821
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001821
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00001821
Iteration 105/1000 | Loss: 0.00001821
Iteration 106/1000 | Loss: 0.00001821
Iteration 107/1000 | Loss: 0.00001821
Iteration 108/1000 | Loss: 0.00001821
Iteration 109/1000 | Loss: 0.00001821
Iteration 110/1000 | Loss: 0.00001821
Iteration 111/1000 | Loss: 0.00001820
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001820
Iteration 115/1000 | Loss: 0.00001820
Iteration 116/1000 | Loss: 0.00001820
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001819
Iteration 119/1000 | Loss: 0.00001819
Iteration 120/1000 | Loss: 0.00001819
Iteration 121/1000 | Loss: 0.00001819
Iteration 122/1000 | Loss: 0.00001819
Iteration 123/1000 | Loss: 0.00001818
Iteration 124/1000 | Loss: 0.00001818
Iteration 125/1000 | Loss: 0.00001818
Iteration 126/1000 | Loss: 0.00001818
Iteration 127/1000 | Loss: 0.00001818
Iteration 128/1000 | Loss: 0.00001818
Iteration 129/1000 | Loss: 0.00001818
Iteration 130/1000 | Loss: 0.00001818
Iteration 131/1000 | Loss: 0.00001818
Iteration 132/1000 | Loss: 0.00001818
Iteration 133/1000 | Loss: 0.00001818
Iteration 134/1000 | Loss: 0.00001818
Iteration 135/1000 | Loss: 0.00001818
Iteration 136/1000 | Loss: 0.00001817
Iteration 137/1000 | Loss: 0.00001817
Iteration 138/1000 | Loss: 0.00001817
Iteration 139/1000 | Loss: 0.00001817
Iteration 140/1000 | Loss: 0.00001817
Iteration 141/1000 | Loss: 0.00001817
Iteration 142/1000 | Loss: 0.00001817
Iteration 143/1000 | Loss: 0.00001817
Iteration 144/1000 | Loss: 0.00001817
Iteration 145/1000 | Loss: 0.00001817
Iteration 146/1000 | Loss: 0.00001816
Iteration 147/1000 | Loss: 0.00001816
Iteration 148/1000 | Loss: 0.00001816
Iteration 149/1000 | Loss: 0.00001816
Iteration 150/1000 | Loss: 0.00001816
Iteration 151/1000 | Loss: 0.00001816
Iteration 152/1000 | Loss: 0.00001815
Iteration 153/1000 | Loss: 0.00001815
Iteration 154/1000 | Loss: 0.00001815
Iteration 155/1000 | Loss: 0.00001815
Iteration 156/1000 | Loss: 0.00001815
Iteration 157/1000 | Loss: 0.00001815
Iteration 158/1000 | Loss: 0.00001815
Iteration 159/1000 | Loss: 0.00001815
Iteration 160/1000 | Loss: 0.00001815
Iteration 161/1000 | Loss: 0.00001814
Iteration 162/1000 | Loss: 0.00001814
Iteration 163/1000 | Loss: 0.00001814
Iteration 164/1000 | Loss: 0.00001814
Iteration 165/1000 | Loss: 0.00001814
Iteration 166/1000 | Loss: 0.00001813
Iteration 167/1000 | Loss: 0.00001813
Iteration 168/1000 | Loss: 0.00001813
Iteration 169/1000 | Loss: 0.00001813
Iteration 170/1000 | Loss: 0.00001813
Iteration 171/1000 | Loss: 0.00001813
Iteration 172/1000 | Loss: 0.00001812
Iteration 173/1000 | Loss: 0.00001812
Iteration 174/1000 | Loss: 0.00001812
Iteration 175/1000 | Loss: 0.00001812
Iteration 176/1000 | Loss: 0.00001812
Iteration 177/1000 | Loss: 0.00001812
Iteration 178/1000 | Loss: 0.00001812
Iteration 179/1000 | Loss: 0.00001811
Iteration 180/1000 | Loss: 0.00001811
Iteration 181/1000 | Loss: 0.00001811
Iteration 182/1000 | Loss: 0.00001811
Iteration 183/1000 | Loss: 0.00001811
Iteration 184/1000 | Loss: 0.00001811
Iteration 185/1000 | Loss: 0.00001811
Iteration 186/1000 | Loss: 0.00001811
Iteration 187/1000 | Loss: 0.00001811
Iteration 188/1000 | Loss: 0.00001810
Iteration 189/1000 | Loss: 0.00001810
Iteration 190/1000 | Loss: 0.00001810
Iteration 191/1000 | Loss: 0.00001810
Iteration 192/1000 | Loss: 0.00001810
Iteration 193/1000 | Loss: 0.00001810
Iteration 194/1000 | Loss: 0.00001810
Iteration 195/1000 | Loss: 0.00001810
Iteration 196/1000 | Loss: 0.00001810
Iteration 197/1000 | Loss: 0.00001810
Iteration 198/1000 | Loss: 0.00001810
Iteration 199/1000 | Loss: 0.00001810
Iteration 200/1000 | Loss: 0.00001810
Iteration 201/1000 | Loss: 0.00001809
Iteration 202/1000 | Loss: 0.00001809
Iteration 203/1000 | Loss: 0.00001809
Iteration 204/1000 | Loss: 0.00001809
Iteration 205/1000 | Loss: 0.00001809
Iteration 206/1000 | Loss: 0.00001809
Iteration 207/1000 | Loss: 0.00001809
Iteration 208/1000 | Loss: 0.00001809
Iteration 209/1000 | Loss: 0.00001809
Iteration 210/1000 | Loss: 0.00001809
Iteration 211/1000 | Loss: 0.00001809
Iteration 212/1000 | Loss: 0.00001809
Iteration 213/1000 | Loss: 0.00001809
Iteration 214/1000 | Loss: 0.00001809
Iteration 215/1000 | Loss: 0.00001809
Iteration 216/1000 | Loss: 0.00001809
Iteration 217/1000 | Loss: 0.00001809
Iteration 218/1000 | Loss: 0.00001809
Iteration 219/1000 | Loss: 0.00001808
Iteration 220/1000 | Loss: 0.00001808
Iteration 221/1000 | Loss: 0.00001808
Iteration 222/1000 | Loss: 0.00001808
Iteration 223/1000 | Loss: 0.00001808
Iteration 224/1000 | Loss: 0.00001808
Iteration 225/1000 | Loss: 0.00001808
Iteration 226/1000 | Loss: 0.00001808
Iteration 227/1000 | Loss: 0.00001808
Iteration 228/1000 | Loss: 0.00001808
Iteration 229/1000 | Loss: 0.00001808
Iteration 230/1000 | Loss: 0.00001808
Iteration 231/1000 | Loss: 0.00001808
Iteration 232/1000 | Loss: 0.00001808
Iteration 233/1000 | Loss: 0.00001808
Iteration 234/1000 | Loss: 0.00001808
Iteration 235/1000 | Loss: 0.00001808
Iteration 236/1000 | Loss: 0.00001808
Iteration 237/1000 | Loss: 0.00001808
Iteration 238/1000 | Loss: 0.00001808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.8084931070916355e-05, 1.8084931070916355e-05, 1.8084931070916355e-05, 1.8084931070916355e-05, 1.8084931070916355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8084931070916355e-05

Optimization complete. Final v2v error: 3.5277926921844482 mm

Highest mean error: 5.055723667144775 mm for frame 64

Lowest mean error: 2.869065761566162 mm for frame 95

Saving results

Total time: 48.662814140319824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388239
Iteration 2/25 | Loss: 0.00085187
Iteration 3/25 | Loss: 0.00073974
Iteration 4/25 | Loss: 0.00072343
Iteration 5/25 | Loss: 0.00071998
Iteration 6/25 | Loss: 0.00071870
Iteration 7/25 | Loss: 0.00071848
Iteration 8/25 | Loss: 0.00071848
Iteration 9/25 | Loss: 0.00071848
Iteration 10/25 | Loss: 0.00071848
Iteration 11/25 | Loss: 0.00071848
Iteration 12/25 | Loss: 0.00071848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007184813730418682, 0.0007184813730418682, 0.0007184813730418682, 0.0007184813730418682, 0.0007184813730418682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007184813730418682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60033131
Iteration 2/25 | Loss: 0.00118709
Iteration 3/25 | Loss: 0.00118709
Iteration 4/25 | Loss: 0.00118709
Iteration 5/25 | Loss: 0.00118709
Iteration 6/25 | Loss: 0.00118709
Iteration 7/25 | Loss: 0.00118709
Iteration 8/25 | Loss: 0.00118709
Iteration 9/25 | Loss: 0.00118709
Iteration 10/25 | Loss: 0.00118709
Iteration 11/25 | Loss: 0.00118709
Iteration 12/25 | Loss: 0.00118709
Iteration 13/25 | Loss: 0.00118709
Iteration 14/25 | Loss: 0.00118709
Iteration 15/25 | Loss: 0.00118709
Iteration 16/25 | Loss: 0.00118709
Iteration 17/25 | Loss: 0.00118709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011870916932821274, 0.0011870916932821274, 0.0011870916932821274, 0.0011870916932821274, 0.0011870916932821274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011870916932821274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118709
Iteration 2/1000 | Loss: 0.00002491
Iteration 3/1000 | Loss: 0.00001639
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001310
Iteration 8/1000 | Loss: 0.00001274
Iteration 9/1000 | Loss: 0.00001249
Iteration 10/1000 | Loss: 0.00001246
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001221
Iteration 13/1000 | Loss: 0.00001220
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001215
Iteration 16/1000 | Loss: 0.00001214
Iteration 17/1000 | Loss: 0.00001214
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001202
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001199
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001198
Iteration 33/1000 | Loss: 0.00001198
Iteration 34/1000 | Loss: 0.00001198
Iteration 35/1000 | Loss: 0.00001198
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001197
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001187
Iteration 91/1000 | Loss: 0.00001187
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001185
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001184
Iteration 110/1000 | Loss: 0.00001184
Iteration 111/1000 | Loss: 0.00001184
Iteration 112/1000 | Loss: 0.00001184
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001184
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001183
Iteration 121/1000 | Loss: 0.00001183
Iteration 122/1000 | Loss: 0.00001183
Iteration 123/1000 | Loss: 0.00001183
Iteration 124/1000 | Loss: 0.00001183
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001183
Iteration 128/1000 | Loss: 0.00001183
Iteration 129/1000 | Loss: 0.00001183
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001182
Iteration 141/1000 | Loss: 0.00001182
Iteration 142/1000 | Loss: 0.00001182
Iteration 143/1000 | Loss: 0.00001182
Iteration 144/1000 | Loss: 0.00001182
Iteration 145/1000 | Loss: 0.00001182
Iteration 146/1000 | Loss: 0.00001182
Iteration 147/1000 | Loss: 0.00001182
Iteration 148/1000 | Loss: 0.00001182
Iteration 149/1000 | Loss: 0.00001182
Iteration 150/1000 | Loss: 0.00001182
Iteration 151/1000 | Loss: 0.00001182
Iteration 152/1000 | Loss: 0.00001182
Iteration 153/1000 | Loss: 0.00001182
Iteration 154/1000 | Loss: 0.00001182
Iteration 155/1000 | Loss: 0.00001182
Iteration 156/1000 | Loss: 0.00001182
Iteration 157/1000 | Loss: 0.00001182
Iteration 158/1000 | Loss: 0.00001182
Iteration 159/1000 | Loss: 0.00001182
Iteration 160/1000 | Loss: 0.00001182
Iteration 161/1000 | Loss: 0.00001182
Iteration 162/1000 | Loss: 0.00001182
Iteration 163/1000 | Loss: 0.00001182
Iteration 164/1000 | Loss: 0.00001182
Iteration 165/1000 | Loss: 0.00001182
Iteration 166/1000 | Loss: 0.00001182
Iteration 167/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.1817966878879815e-05, 1.1817966878879815e-05, 1.1817966878879815e-05, 1.1817966878879815e-05, 1.1817966878879815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1817966878879815e-05

Optimization complete. Final v2v error: 2.9444901943206787 mm

Highest mean error: 3.1320862770080566 mm for frame 105

Lowest mean error: 2.8013007640838623 mm for frame 15

Saving results

Total time: 34.92309498786926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037696
Iteration 2/25 | Loss: 0.00399986
Iteration 3/25 | Loss: 0.00229762
Iteration 4/25 | Loss: 0.00192840
Iteration 5/25 | Loss: 0.00173488
Iteration 6/25 | Loss: 0.00156604
Iteration 7/25 | Loss: 0.00144643
Iteration 8/25 | Loss: 0.00141925
Iteration 9/25 | Loss: 0.00143907
Iteration 10/25 | Loss: 0.00134581
Iteration 11/25 | Loss: 0.00133208
Iteration 12/25 | Loss: 0.00129122
Iteration 13/25 | Loss: 0.00128466
Iteration 14/25 | Loss: 0.00128369
Iteration 15/25 | Loss: 0.00128284
Iteration 16/25 | Loss: 0.00127434
Iteration 17/25 | Loss: 0.00127060
Iteration 18/25 | Loss: 0.00126734
Iteration 19/25 | Loss: 0.00127330
Iteration 20/25 | Loss: 0.00123078
Iteration 21/25 | Loss: 0.00121339
Iteration 22/25 | Loss: 0.00121467
Iteration 23/25 | Loss: 0.00120698
Iteration 24/25 | Loss: 0.00119309
Iteration 25/25 | Loss: 0.00117683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72469628
Iteration 2/25 | Loss: 0.01758183
Iteration 3/25 | Loss: 0.03028865
Iteration 4/25 | Loss: 0.00672671
Iteration 5/25 | Loss: 0.00449480
Iteration 6/25 | Loss: 0.00448088
Iteration 7/25 | Loss: 0.00448088
Iteration 8/25 | Loss: 0.00448087
Iteration 9/25 | Loss: 0.00448087
Iteration 10/25 | Loss: 0.00448087
Iteration 11/25 | Loss: 0.00448087
Iteration 12/25 | Loss: 0.00448087
Iteration 13/25 | Loss: 0.00448087
Iteration 14/25 | Loss: 0.00448087
Iteration 15/25 | Loss: 0.00448087
Iteration 16/25 | Loss: 0.00448087
Iteration 17/25 | Loss: 0.00448087
Iteration 18/25 | Loss: 0.00448087
Iteration 19/25 | Loss: 0.00448087
Iteration 20/25 | Loss: 0.00448087
Iteration 21/25 | Loss: 0.00448087
Iteration 22/25 | Loss: 0.00448087
Iteration 23/25 | Loss: 0.00448087
Iteration 24/25 | Loss: 0.00448087
Iteration 25/25 | Loss: 0.00448087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00448087
Iteration 2/1000 | Loss: 0.00584169
Iteration 3/1000 | Loss: 0.00755447
Iteration 4/1000 | Loss: 0.00095630
Iteration 5/1000 | Loss: 0.00254196
Iteration 6/1000 | Loss: 0.00501066
Iteration 7/1000 | Loss: 0.00090044
Iteration 8/1000 | Loss: 0.00148040
Iteration 9/1000 | Loss: 0.00096025
Iteration 10/1000 | Loss: 0.00110572
Iteration 11/1000 | Loss: 0.00172377
Iteration 12/1000 | Loss: 0.00212450
Iteration 13/1000 | Loss: 0.00061675
Iteration 14/1000 | Loss: 0.00064566
Iteration 15/1000 | Loss: 0.00157191
Iteration 16/1000 | Loss: 0.00081257
Iteration 17/1000 | Loss: 0.00049184
Iteration 18/1000 | Loss: 0.00144421
Iteration 19/1000 | Loss: 0.00099368
Iteration 20/1000 | Loss: 0.00122671
Iteration 21/1000 | Loss: 0.00115433
Iteration 22/1000 | Loss: 0.00119493
Iteration 23/1000 | Loss: 0.00167424
Iteration 24/1000 | Loss: 0.00258631
Iteration 25/1000 | Loss: 0.00174122
Iteration 26/1000 | Loss: 0.00141032
Iteration 27/1000 | Loss: 0.00103529
Iteration 28/1000 | Loss: 0.00086742
Iteration 29/1000 | Loss: 0.00080759
Iteration 30/1000 | Loss: 0.00117478
Iteration 31/1000 | Loss: 0.00073609
Iteration 32/1000 | Loss: 0.00067813
Iteration 33/1000 | Loss: 0.00073123
Iteration 34/1000 | Loss: 0.00062045
Iteration 35/1000 | Loss: 0.00108031
Iteration 36/1000 | Loss: 0.00058034
Iteration 37/1000 | Loss: 0.00115431
Iteration 38/1000 | Loss: 0.00082155
Iteration 39/1000 | Loss: 0.00023913
Iteration 40/1000 | Loss: 0.00052921
Iteration 41/1000 | Loss: 0.00041321
Iteration 42/1000 | Loss: 0.00042579
Iteration 43/1000 | Loss: 0.00045731
Iteration 44/1000 | Loss: 0.00081313
Iteration 45/1000 | Loss: 0.00068745
Iteration 46/1000 | Loss: 0.00046752
Iteration 47/1000 | Loss: 0.00036339
Iteration 48/1000 | Loss: 0.00048085
Iteration 49/1000 | Loss: 0.00032628
Iteration 50/1000 | Loss: 0.00065387
Iteration 51/1000 | Loss: 0.00058403
Iteration 52/1000 | Loss: 0.00060856
Iteration 53/1000 | Loss: 0.00058721
Iteration 54/1000 | Loss: 0.00045384
Iteration 55/1000 | Loss: 0.00177244
Iteration 56/1000 | Loss: 0.00432881
Iteration 57/1000 | Loss: 0.00750645
Iteration 58/1000 | Loss: 0.00293549
Iteration 59/1000 | Loss: 0.00124318
Iteration 60/1000 | Loss: 0.00087366
Iteration 61/1000 | Loss: 0.00067516
Iteration 62/1000 | Loss: 0.00129366
Iteration 63/1000 | Loss: 0.00147496
Iteration 64/1000 | Loss: 0.00031235
Iteration 65/1000 | Loss: 0.00042490
Iteration 66/1000 | Loss: 0.00025777
Iteration 67/1000 | Loss: 0.00040310
Iteration 68/1000 | Loss: 0.00028953
Iteration 69/1000 | Loss: 0.00018068
Iteration 70/1000 | Loss: 0.00028290
Iteration 71/1000 | Loss: 0.00082064
Iteration 72/1000 | Loss: 0.00166664
Iteration 73/1000 | Loss: 0.00086646
Iteration 74/1000 | Loss: 0.00032466
Iteration 75/1000 | Loss: 0.00049070
Iteration 76/1000 | Loss: 0.00023686
Iteration 77/1000 | Loss: 0.00054586
Iteration 78/1000 | Loss: 0.00321272
Iteration 79/1000 | Loss: 0.00175384
Iteration 80/1000 | Loss: 0.00123066
Iteration 81/1000 | Loss: 0.00148422
Iteration 82/1000 | Loss: 0.00142349
Iteration 83/1000 | Loss: 0.00120539
Iteration 84/1000 | Loss: 0.00023719
Iteration 85/1000 | Loss: 0.00024555
Iteration 86/1000 | Loss: 0.00034448
Iteration 87/1000 | Loss: 0.00067645
Iteration 88/1000 | Loss: 0.00019781
Iteration 89/1000 | Loss: 0.00030040
Iteration 90/1000 | Loss: 0.00028468
Iteration 91/1000 | Loss: 0.00043843
Iteration 92/1000 | Loss: 0.00013488
Iteration 93/1000 | Loss: 0.00035367
Iteration 94/1000 | Loss: 0.00027068
Iteration 95/1000 | Loss: 0.00032212
Iteration 96/1000 | Loss: 0.00028712
Iteration 97/1000 | Loss: 0.00079198
Iteration 98/1000 | Loss: 0.00074621
Iteration 99/1000 | Loss: 0.00019969
Iteration 100/1000 | Loss: 0.00017752
Iteration 101/1000 | Loss: 0.00014958
Iteration 102/1000 | Loss: 0.00033707
Iteration 103/1000 | Loss: 0.00010210
Iteration 104/1000 | Loss: 0.00022419
Iteration 105/1000 | Loss: 0.00025327
Iteration 106/1000 | Loss: 0.00017847
Iteration 107/1000 | Loss: 0.00043238
Iteration 108/1000 | Loss: 0.00015677
Iteration 109/1000 | Loss: 0.00014685
Iteration 110/1000 | Loss: 0.00017488
Iteration 111/1000 | Loss: 0.00126297
Iteration 112/1000 | Loss: 0.00089324
Iteration 113/1000 | Loss: 0.00023973
Iteration 114/1000 | Loss: 0.00149024
Iteration 115/1000 | Loss: 0.00084439
Iteration 116/1000 | Loss: 0.00041633
Iteration 117/1000 | Loss: 0.00148022
Iteration 118/1000 | Loss: 0.00049554
Iteration 119/1000 | Loss: 0.00093995
Iteration 120/1000 | Loss: 0.00023148
Iteration 121/1000 | Loss: 0.00047636
Iteration 122/1000 | Loss: 0.00039884
Iteration 123/1000 | Loss: 0.00096340
Iteration 124/1000 | Loss: 0.00052726
Iteration 125/1000 | Loss: 0.00046736
Iteration 126/1000 | Loss: 0.00114084
Iteration 127/1000 | Loss: 0.00034785
Iteration 128/1000 | Loss: 0.00085230
Iteration 129/1000 | Loss: 0.00043656
Iteration 130/1000 | Loss: 0.00063129
Iteration 131/1000 | Loss: 0.00236100
Iteration 132/1000 | Loss: 0.00038300
Iteration 133/1000 | Loss: 0.00063367
Iteration 134/1000 | Loss: 0.00027943
Iteration 135/1000 | Loss: 0.00006947
Iteration 136/1000 | Loss: 0.00036955
Iteration 137/1000 | Loss: 0.00008157
Iteration 138/1000 | Loss: 0.00006790
Iteration 139/1000 | Loss: 0.00006668
Iteration 140/1000 | Loss: 0.00007174
Iteration 141/1000 | Loss: 0.00013236
Iteration 142/1000 | Loss: 0.00006601
Iteration 143/1000 | Loss: 0.00005602
Iteration 144/1000 | Loss: 0.00005328
Iteration 145/1000 | Loss: 0.00006383
Iteration 146/1000 | Loss: 0.00006161
Iteration 147/1000 | Loss: 0.00006337
Iteration 148/1000 | Loss: 0.00006294
Iteration 149/1000 | Loss: 0.00005832
Iteration 150/1000 | Loss: 0.00027437
Iteration 151/1000 | Loss: 0.00018212
Iteration 152/1000 | Loss: 0.00027243
Iteration 153/1000 | Loss: 0.00047716
Iteration 154/1000 | Loss: 0.00007772
Iteration 155/1000 | Loss: 0.00026179
Iteration 156/1000 | Loss: 0.00015493
Iteration 157/1000 | Loss: 0.00014859
Iteration 158/1000 | Loss: 0.00005995
Iteration 159/1000 | Loss: 0.00005876
Iteration 160/1000 | Loss: 0.00006393
Iteration 161/1000 | Loss: 0.00009058
Iteration 162/1000 | Loss: 0.00006081
Iteration 163/1000 | Loss: 0.00005981
Iteration 164/1000 | Loss: 0.00006101
Iteration 165/1000 | Loss: 0.00059101
Iteration 166/1000 | Loss: 0.00045530
Iteration 167/1000 | Loss: 0.00007694
Iteration 168/1000 | Loss: 0.00005938
Iteration 169/1000 | Loss: 0.00005335
Iteration 170/1000 | Loss: 0.00004877
Iteration 171/1000 | Loss: 0.00075622
Iteration 172/1000 | Loss: 0.00005639
Iteration 173/1000 | Loss: 0.00004768
Iteration 174/1000 | Loss: 0.00004437
Iteration 175/1000 | Loss: 0.00032302
Iteration 176/1000 | Loss: 0.00004923
Iteration 177/1000 | Loss: 0.00004141
Iteration 178/1000 | Loss: 0.00005380
Iteration 179/1000 | Loss: 0.00044535
Iteration 180/1000 | Loss: 0.00005887
Iteration 181/1000 | Loss: 0.00005796
Iteration 182/1000 | Loss: 0.00006034
Iteration 183/1000 | Loss: 0.00051175
Iteration 184/1000 | Loss: 0.00004490
Iteration 185/1000 | Loss: 0.00003816
Iteration 186/1000 | Loss: 0.00003774
Iteration 187/1000 | Loss: 0.00003746
Iteration 188/1000 | Loss: 0.00004655
Iteration 189/1000 | Loss: 0.00003713
Iteration 190/1000 | Loss: 0.00003863
Iteration 191/1000 | Loss: 0.00003701
Iteration 192/1000 | Loss: 0.00003862
Iteration 193/1000 | Loss: 0.00059213
Iteration 194/1000 | Loss: 0.00005609
Iteration 195/1000 | Loss: 0.00006717
Iteration 196/1000 | Loss: 0.00005532
Iteration 197/1000 | Loss: 0.00006482
Iteration 198/1000 | Loss: 0.00005234
Iteration 199/1000 | Loss: 0.00008069
Iteration 200/1000 | Loss: 0.00005401
Iteration 201/1000 | Loss: 0.00003663
Iteration 202/1000 | Loss: 0.00003660
Iteration 203/1000 | Loss: 0.00003660
Iteration 204/1000 | Loss: 0.00003659
Iteration 205/1000 | Loss: 0.00003659
Iteration 206/1000 | Loss: 0.00003659
Iteration 207/1000 | Loss: 0.00003656
Iteration 208/1000 | Loss: 0.00003654
Iteration 209/1000 | Loss: 0.00003653
Iteration 210/1000 | Loss: 0.00003648
Iteration 211/1000 | Loss: 0.00003647
Iteration 212/1000 | Loss: 0.00003646
Iteration 213/1000 | Loss: 0.00003646
Iteration 214/1000 | Loss: 0.00003646
Iteration 215/1000 | Loss: 0.00003646
Iteration 216/1000 | Loss: 0.00003646
Iteration 217/1000 | Loss: 0.00003646
Iteration 218/1000 | Loss: 0.00003646
Iteration 219/1000 | Loss: 0.00003646
Iteration 220/1000 | Loss: 0.00003646
Iteration 221/1000 | Loss: 0.00003646
Iteration 222/1000 | Loss: 0.00003645
Iteration 223/1000 | Loss: 0.00003645
Iteration 224/1000 | Loss: 0.00003645
Iteration 225/1000 | Loss: 0.00003645
Iteration 226/1000 | Loss: 0.00003644
Iteration 227/1000 | Loss: 0.00003856
Iteration 228/1000 | Loss: 0.00003856
Iteration 229/1000 | Loss: 0.00003648
Iteration 230/1000 | Loss: 0.00003632
Iteration 231/1000 | Loss: 0.00003632
Iteration 232/1000 | Loss: 0.00003631
Iteration 233/1000 | Loss: 0.00003631
Iteration 234/1000 | Loss: 0.00003631
Iteration 235/1000 | Loss: 0.00003630
Iteration 236/1000 | Loss: 0.00003630
Iteration 237/1000 | Loss: 0.00003630
Iteration 238/1000 | Loss: 0.00003630
Iteration 239/1000 | Loss: 0.00003630
Iteration 240/1000 | Loss: 0.00003793
Iteration 241/1000 | Loss: 0.00003629
Iteration 242/1000 | Loss: 0.00003628
Iteration 243/1000 | Loss: 0.00003628
Iteration 244/1000 | Loss: 0.00003627
Iteration 245/1000 | Loss: 0.00003627
Iteration 246/1000 | Loss: 0.00003627
Iteration 247/1000 | Loss: 0.00003627
Iteration 248/1000 | Loss: 0.00003627
Iteration 249/1000 | Loss: 0.00003627
Iteration 250/1000 | Loss: 0.00003627
Iteration 251/1000 | Loss: 0.00003626
Iteration 252/1000 | Loss: 0.00003626
Iteration 253/1000 | Loss: 0.00003626
Iteration 254/1000 | Loss: 0.00003625
Iteration 255/1000 | Loss: 0.00003625
Iteration 256/1000 | Loss: 0.00003625
Iteration 257/1000 | Loss: 0.00003625
Iteration 258/1000 | Loss: 0.00003625
Iteration 259/1000 | Loss: 0.00003625
Iteration 260/1000 | Loss: 0.00003625
Iteration 261/1000 | Loss: 0.00003625
Iteration 262/1000 | Loss: 0.00003625
Iteration 263/1000 | Loss: 0.00003625
Iteration 264/1000 | Loss: 0.00003625
Iteration 265/1000 | Loss: 0.00003625
Iteration 266/1000 | Loss: 0.00003625
Iteration 267/1000 | Loss: 0.00003625
Iteration 268/1000 | Loss: 0.00003624
Iteration 269/1000 | Loss: 0.00003624
Iteration 270/1000 | Loss: 0.00003624
Iteration 271/1000 | Loss: 0.00003624
Iteration 272/1000 | Loss: 0.00003624
Iteration 273/1000 | Loss: 0.00003624
Iteration 274/1000 | Loss: 0.00003624
Iteration 275/1000 | Loss: 0.00003623
Iteration 276/1000 | Loss: 0.00003623
Iteration 277/1000 | Loss: 0.00003623
Iteration 278/1000 | Loss: 0.00003623
Iteration 279/1000 | Loss: 0.00003623
Iteration 280/1000 | Loss: 0.00003623
Iteration 281/1000 | Loss: 0.00003623
Iteration 282/1000 | Loss: 0.00003623
Iteration 283/1000 | Loss: 0.00003622
Iteration 284/1000 | Loss: 0.00003622
Iteration 285/1000 | Loss: 0.00003622
Iteration 286/1000 | Loss: 0.00003622
Iteration 287/1000 | Loss: 0.00003622
Iteration 288/1000 | Loss: 0.00003621
Iteration 289/1000 | Loss: 0.00003621
Iteration 290/1000 | Loss: 0.00003621
Iteration 291/1000 | Loss: 0.00003620
Iteration 292/1000 | Loss: 0.00003619
Iteration 293/1000 | Loss: 0.00003619
Iteration 294/1000 | Loss: 0.00003619
Iteration 295/1000 | Loss: 0.00003619
Iteration 296/1000 | Loss: 0.00003619
Iteration 297/1000 | Loss: 0.00003618
Iteration 298/1000 | Loss: 0.00003618
Iteration 299/1000 | Loss: 0.00003617
Iteration 300/1000 | Loss: 0.00003617
Iteration 301/1000 | Loss: 0.00003616
Iteration 302/1000 | Loss: 0.00003616
Iteration 303/1000 | Loss: 0.00003616
Iteration 304/1000 | Loss: 0.00003616
Iteration 305/1000 | Loss: 0.00003616
Iteration 306/1000 | Loss: 0.00003616
Iteration 307/1000 | Loss: 0.00003616
Iteration 308/1000 | Loss: 0.00003616
Iteration 309/1000 | Loss: 0.00003616
Iteration 310/1000 | Loss: 0.00003615
Iteration 311/1000 | Loss: 0.00003615
Iteration 312/1000 | Loss: 0.00003615
Iteration 313/1000 | Loss: 0.00003615
Iteration 314/1000 | Loss: 0.00003615
Iteration 315/1000 | Loss: 0.00003615
Iteration 316/1000 | Loss: 0.00003615
Iteration 317/1000 | Loss: 0.00003615
Iteration 318/1000 | Loss: 0.00003615
Iteration 319/1000 | Loss: 0.00003615
Iteration 320/1000 | Loss: 0.00003615
Iteration 321/1000 | Loss: 0.00003615
Iteration 322/1000 | Loss: 0.00003615
Iteration 323/1000 | Loss: 0.00003615
Iteration 324/1000 | Loss: 0.00003615
Iteration 325/1000 | Loss: 0.00003615
Iteration 326/1000 | Loss: 0.00003615
Iteration 327/1000 | Loss: 0.00003615
Iteration 328/1000 | Loss: 0.00003615
Iteration 329/1000 | Loss: 0.00003615
Iteration 330/1000 | Loss: 0.00003614
Iteration 331/1000 | Loss: 0.00003614
Iteration 332/1000 | Loss: 0.00003614
Iteration 333/1000 | Loss: 0.00003614
Iteration 334/1000 | Loss: 0.00003614
Iteration 335/1000 | Loss: 0.00003614
Iteration 336/1000 | Loss: 0.00003614
Iteration 337/1000 | Loss: 0.00003614
Iteration 338/1000 | Loss: 0.00003614
Iteration 339/1000 | Loss: 0.00003614
Iteration 340/1000 | Loss: 0.00003614
Iteration 341/1000 | Loss: 0.00003613
Iteration 342/1000 | Loss: 0.00003613
Iteration 343/1000 | Loss: 0.00003613
Iteration 344/1000 | Loss: 0.00003613
Iteration 345/1000 | Loss: 0.00003613
Iteration 346/1000 | Loss: 0.00003613
Iteration 347/1000 | Loss: 0.00003613
Iteration 348/1000 | Loss: 0.00003613
Iteration 349/1000 | Loss: 0.00003613
Iteration 350/1000 | Loss: 0.00003613
Iteration 351/1000 | Loss: 0.00003613
Iteration 352/1000 | Loss: 0.00003613
Iteration 353/1000 | Loss: 0.00003613
Iteration 354/1000 | Loss: 0.00003613
Iteration 355/1000 | Loss: 0.00003613
Iteration 356/1000 | Loss: 0.00003613
Iteration 357/1000 | Loss: 0.00003613
Iteration 358/1000 | Loss: 0.00003613
Iteration 359/1000 | Loss: 0.00003612
Iteration 360/1000 | Loss: 0.00003612
Iteration 361/1000 | Loss: 0.00003612
Iteration 362/1000 | Loss: 0.00003612
Iteration 363/1000 | Loss: 0.00003612
Iteration 364/1000 | Loss: 0.00003612
Iteration 365/1000 | Loss: 0.00003612
Iteration 366/1000 | Loss: 0.00003612
Iteration 367/1000 | Loss: 0.00003612
Iteration 368/1000 | Loss: 0.00003612
Iteration 369/1000 | Loss: 0.00003612
Iteration 370/1000 | Loss: 0.00003611
Iteration 371/1000 | Loss: 0.00003611
Iteration 372/1000 | Loss: 0.00003611
Iteration 373/1000 | Loss: 0.00003611
Iteration 374/1000 | Loss: 0.00003611
Iteration 375/1000 | Loss: 0.00003611
Iteration 376/1000 | Loss: 0.00003611
Iteration 377/1000 | Loss: 0.00003611
Iteration 378/1000 | Loss: 0.00003611
Iteration 379/1000 | Loss: 0.00003611
Iteration 380/1000 | Loss: 0.00003611
Iteration 381/1000 | Loss: 0.00003611
Iteration 382/1000 | Loss: 0.00003611
Iteration 383/1000 | Loss: 0.00003611
Iteration 384/1000 | Loss: 0.00003611
Iteration 385/1000 | Loss: 0.00003611
Iteration 386/1000 | Loss: 0.00003611
Iteration 387/1000 | Loss: 0.00003611
Iteration 388/1000 | Loss: 0.00003611
Iteration 389/1000 | Loss: 0.00003611
Iteration 390/1000 | Loss: 0.00003611
Iteration 391/1000 | Loss: 0.00003611
Iteration 392/1000 | Loss: 0.00003611
Iteration 393/1000 | Loss: 0.00003611
Iteration 394/1000 | Loss: 0.00003611
Iteration 395/1000 | Loss: 0.00003611
Iteration 396/1000 | Loss: 0.00003611
Iteration 397/1000 | Loss: 0.00003611
Iteration 398/1000 | Loss: 0.00003611
Iteration 399/1000 | Loss: 0.00003611
Iteration 400/1000 | Loss: 0.00003611
Iteration 401/1000 | Loss: 0.00003611
Iteration 402/1000 | Loss: 0.00003611
Iteration 403/1000 | Loss: 0.00003611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 403. Stopping optimization.
Last 5 losses: [3.6105197068536654e-05, 3.6105197068536654e-05, 3.6105197068536654e-05, 3.6105197068536654e-05, 3.6105197068536654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6105197068536654e-05

Optimization complete. Final v2v error: 4.813403129577637 mm

Highest mean error: 6.63812255859375 mm for frame 125

Lowest mean error: 3.4421961307525635 mm for frame 109

Saving results

Total time: 395.01643109321594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558719
Iteration 2/25 | Loss: 0.00110748
Iteration 3/25 | Loss: 0.00091691
Iteration 4/25 | Loss: 0.00088629
Iteration 5/25 | Loss: 0.00087580
Iteration 6/25 | Loss: 0.00087341
Iteration 7/25 | Loss: 0.00087287
Iteration 8/25 | Loss: 0.00087287
Iteration 9/25 | Loss: 0.00087287
Iteration 10/25 | Loss: 0.00087287
Iteration 11/25 | Loss: 0.00087287
Iteration 12/25 | Loss: 0.00087287
Iteration 13/25 | Loss: 0.00087287
Iteration 14/25 | Loss: 0.00087287
Iteration 15/25 | Loss: 0.00087287
Iteration 16/25 | Loss: 0.00087287
Iteration 17/25 | Loss: 0.00087287
Iteration 18/25 | Loss: 0.00087287
Iteration 19/25 | Loss: 0.00087287
Iteration 20/25 | Loss: 0.00087287
Iteration 21/25 | Loss: 0.00087287
Iteration 22/25 | Loss: 0.00087287
Iteration 23/25 | Loss: 0.00087287
Iteration 24/25 | Loss: 0.00087287
Iteration 25/25 | Loss: 0.00087287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.06062603
Iteration 2/25 | Loss: 0.00116640
Iteration 3/25 | Loss: 0.00116640
Iteration 4/25 | Loss: 0.00116640
Iteration 5/25 | Loss: 0.00116640
Iteration 6/25 | Loss: 0.00116640
Iteration 7/25 | Loss: 0.00116640
Iteration 8/25 | Loss: 0.00116640
Iteration 9/25 | Loss: 0.00116640
Iteration 10/25 | Loss: 0.00116640
Iteration 11/25 | Loss: 0.00116640
Iteration 12/25 | Loss: 0.00116640
Iteration 13/25 | Loss: 0.00116640
Iteration 14/25 | Loss: 0.00116640
Iteration 15/25 | Loss: 0.00116640
Iteration 16/25 | Loss: 0.00116640
Iteration 17/25 | Loss: 0.00116640
Iteration 18/25 | Loss: 0.00116640
Iteration 19/25 | Loss: 0.00116640
Iteration 20/25 | Loss: 0.00116640
Iteration 21/25 | Loss: 0.00116640
Iteration 22/25 | Loss: 0.00116640
Iteration 23/25 | Loss: 0.00116640
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011663957266137004, 0.0011663957266137004, 0.0011663957266137004, 0.0011663957266137004, 0.0011663957266137004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011663957266137004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116640
Iteration 2/1000 | Loss: 0.00003803
Iteration 3/1000 | Loss: 0.00002493
Iteration 4/1000 | Loss: 0.00002269
Iteration 5/1000 | Loss: 0.00002193
Iteration 6/1000 | Loss: 0.00002113
Iteration 7/1000 | Loss: 0.00002066
Iteration 8/1000 | Loss: 0.00002034
Iteration 9/1000 | Loss: 0.00002008
Iteration 10/1000 | Loss: 0.00001996
Iteration 11/1000 | Loss: 0.00001990
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001977
Iteration 14/1000 | Loss: 0.00001974
Iteration 15/1000 | Loss: 0.00001972
Iteration 16/1000 | Loss: 0.00001972
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001970
Iteration 19/1000 | Loss: 0.00001970
Iteration 20/1000 | Loss: 0.00001969
Iteration 21/1000 | Loss: 0.00001966
Iteration 22/1000 | Loss: 0.00001966
Iteration 23/1000 | Loss: 0.00001964
Iteration 24/1000 | Loss: 0.00001962
Iteration 25/1000 | Loss: 0.00001962
Iteration 26/1000 | Loss: 0.00001962
Iteration 27/1000 | Loss: 0.00001962
Iteration 28/1000 | Loss: 0.00001962
Iteration 29/1000 | Loss: 0.00001962
Iteration 30/1000 | Loss: 0.00001961
Iteration 31/1000 | Loss: 0.00001961
Iteration 32/1000 | Loss: 0.00001960
Iteration 33/1000 | Loss: 0.00001959
Iteration 34/1000 | Loss: 0.00001959
Iteration 35/1000 | Loss: 0.00001959
Iteration 36/1000 | Loss: 0.00001958
Iteration 37/1000 | Loss: 0.00001958
Iteration 38/1000 | Loss: 0.00001958
Iteration 39/1000 | Loss: 0.00001958
Iteration 40/1000 | Loss: 0.00001957
Iteration 41/1000 | Loss: 0.00001957
Iteration 42/1000 | Loss: 0.00001957
Iteration 43/1000 | Loss: 0.00001956
Iteration 44/1000 | Loss: 0.00001956
Iteration 45/1000 | Loss: 0.00001956
Iteration 46/1000 | Loss: 0.00001956
Iteration 47/1000 | Loss: 0.00001955
Iteration 48/1000 | Loss: 0.00001954
Iteration 49/1000 | Loss: 0.00001954
Iteration 50/1000 | Loss: 0.00001954
Iteration 51/1000 | Loss: 0.00001954
Iteration 52/1000 | Loss: 0.00001954
Iteration 53/1000 | Loss: 0.00001953
Iteration 54/1000 | Loss: 0.00001953
Iteration 55/1000 | Loss: 0.00001953
Iteration 56/1000 | Loss: 0.00001952
Iteration 57/1000 | Loss: 0.00001952
Iteration 58/1000 | Loss: 0.00001952
Iteration 59/1000 | Loss: 0.00001952
Iteration 60/1000 | Loss: 0.00001952
Iteration 61/1000 | Loss: 0.00001952
Iteration 62/1000 | Loss: 0.00001952
Iteration 63/1000 | Loss: 0.00001952
Iteration 64/1000 | Loss: 0.00001952
Iteration 65/1000 | Loss: 0.00001951
Iteration 66/1000 | Loss: 0.00001951
Iteration 67/1000 | Loss: 0.00001951
Iteration 68/1000 | Loss: 0.00001951
Iteration 69/1000 | Loss: 0.00001951
Iteration 70/1000 | Loss: 0.00001951
Iteration 71/1000 | Loss: 0.00001951
Iteration 72/1000 | Loss: 0.00001951
Iteration 73/1000 | Loss: 0.00001951
Iteration 74/1000 | Loss: 0.00001951
Iteration 75/1000 | Loss: 0.00001951
Iteration 76/1000 | Loss: 0.00001951
Iteration 77/1000 | Loss: 0.00001951
Iteration 78/1000 | Loss: 0.00001951
Iteration 79/1000 | Loss: 0.00001951
Iteration 80/1000 | Loss: 0.00001951
Iteration 81/1000 | Loss: 0.00001951
Iteration 82/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.9507986507960595e-05, 1.9507986507960595e-05, 1.9507986507960595e-05, 1.9507986507960595e-05, 1.9507986507960595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9507986507960595e-05

Optimization complete. Final v2v error: 3.6980810165405273 mm

Highest mean error: 4.049135208129883 mm for frame 164

Lowest mean error: 3.3486928939819336 mm for frame 220

Saving results

Total time: 36.177013874053955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741281
Iteration 2/25 | Loss: 0.00103701
Iteration 3/25 | Loss: 0.00079681
Iteration 4/25 | Loss: 0.00077424
Iteration 5/25 | Loss: 0.00074922
Iteration 6/25 | Loss: 0.00073993
Iteration 7/25 | Loss: 0.00074095
Iteration 8/25 | Loss: 0.00074212
Iteration 9/25 | Loss: 0.00073769
Iteration 10/25 | Loss: 0.00074413
Iteration 11/25 | Loss: 0.00073808
Iteration 12/25 | Loss: 0.00073656
Iteration 13/25 | Loss: 0.00073635
Iteration 14/25 | Loss: 0.00074012
Iteration 15/25 | Loss: 0.00073916
Iteration 16/25 | Loss: 0.00073618
Iteration 17/25 | Loss: 0.00073618
Iteration 18/25 | Loss: 0.00073618
Iteration 19/25 | Loss: 0.00073618
Iteration 20/25 | Loss: 0.00073617
Iteration 21/25 | Loss: 0.00073617
Iteration 22/25 | Loss: 0.00073617
Iteration 23/25 | Loss: 0.00073617
Iteration 24/25 | Loss: 0.00073617
Iteration 25/25 | Loss: 0.00073617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.03626227
Iteration 2/25 | Loss: 0.00122602
Iteration 3/25 | Loss: 0.00122602
Iteration 4/25 | Loss: 0.00122602
Iteration 5/25 | Loss: 0.00122602
Iteration 6/25 | Loss: 0.00122602
Iteration 7/25 | Loss: 0.00122602
Iteration 8/25 | Loss: 0.00122602
Iteration 9/25 | Loss: 0.00122602
Iteration 10/25 | Loss: 0.00122602
Iteration 11/25 | Loss: 0.00122602
Iteration 12/25 | Loss: 0.00122602
Iteration 13/25 | Loss: 0.00122602
Iteration 14/25 | Loss: 0.00122602
Iteration 15/25 | Loss: 0.00122602
Iteration 16/25 | Loss: 0.00122602
Iteration 17/25 | Loss: 0.00122602
Iteration 18/25 | Loss: 0.00122602
Iteration 19/25 | Loss: 0.00122602
Iteration 20/25 | Loss: 0.00122602
Iteration 21/25 | Loss: 0.00122602
Iteration 22/25 | Loss: 0.00122602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012260200455784798, 0.0012260200455784798, 0.0012260200455784798, 0.0012260200455784798, 0.0012260200455784798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012260200455784798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122602
Iteration 2/1000 | Loss: 0.00002256
Iteration 3/1000 | Loss: 0.00005953
Iteration 4/1000 | Loss: 0.00001410
Iteration 5/1000 | Loss: 0.00001352
Iteration 6/1000 | Loss: 0.00001311
Iteration 7/1000 | Loss: 0.00001308
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001281
Iteration 10/1000 | Loss: 0.00001275
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001248
Iteration 13/1000 | Loss: 0.00001244
Iteration 14/1000 | Loss: 0.00001243
Iteration 15/1000 | Loss: 0.00001233
Iteration 16/1000 | Loss: 0.00001231
Iteration 17/1000 | Loss: 0.00001229
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001228
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001227
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001220
Iteration 27/1000 | Loss: 0.00001219
Iteration 28/1000 | Loss: 0.00001219
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001213
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001211
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001208
Iteration 52/1000 | Loss: 0.00001208
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001206
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001205
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001203
Iteration 62/1000 | Loss: 0.00001202
Iteration 63/1000 | Loss: 0.00001202
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001200
Iteration 72/1000 | Loss: 0.00001199
Iteration 73/1000 | Loss: 0.00001199
Iteration 74/1000 | Loss: 0.00001199
Iteration 75/1000 | Loss: 0.00001198
Iteration 76/1000 | Loss: 0.00001198
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001197
Iteration 80/1000 | Loss: 0.00001197
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001196
Iteration 83/1000 | Loss: 0.00001196
Iteration 84/1000 | Loss: 0.00001196
Iteration 85/1000 | Loss: 0.00001196
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001195
Iteration 88/1000 | Loss: 0.00001195
Iteration 89/1000 | Loss: 0.00001195
Iteration 90/1000 | Loss: 0.00001195
Iteration 91/1000 | Loss: 0.00001195
Iteration 92/1000 | Loss: 0.00001195
Iteration 93/1000 | Loss: 0.00001195
Iteration 94/1000 | Loss: 0.00001195
Iteration 95/1000 | Loss: 0.00001195
Iteration 96/1000 | Loss: 0.00001195
Iteration 97/1000 | Loss: 0.00001195
Iteration 98/1000 | Loss: 0.00001195
Iteration 99/1000 | Loss: 0.00001195
Iteration 100/1000 | Loss: 0.00001194
Iteration 101/1000 | Loss: 0.00001194
Iteration 102/1000 | Loss: 0.00001194
Iteration 103/1000 | Loss: 0.00001194
Iteration 104/1000 | Loss: 0.00001194
Iteration 105/1000 | Loss: 0.00001194
Iteration 106/1000 | Loss: 0.00001194
Iteration 107/1000 | Loss: 0.00001193
Iteration 108/1000 | Loss: 0.00001193
Iteration 109/1000 | Loss: 0.00001193
Iteration 110/1000 | Loss: 0.00001193
Iteration 111/1000 | Loss: 0.00001193
Iteration 112/1000 | Loss: 0.00001193
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001192
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.1921713849005755e-05, 1.1921713849005755e-05, 1.1921713849005755e-05, 1.1921713849005755e-05, 1.1921713849005755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1921713849005755e-05

Optimization complete. Final v2v error: 2.9583096504211426 mm

Highest mean error: 3.3145596981048584 mm for frame 191

Lowest mean error: 2.7820587158203125 mm for frame 30

Saving results

Total time: 59.745330572128296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442616
Iteration 2/25 | Loss: 0.00090511
Iteration 3/25 | Loss: 0.00081283
Iteration 4/25 | Loss: 0.00079415
Iteration 5/25 | Loss: 0.00078784
Iteration 6/25 | Loss: 0.00078633
Iteration 7/25 | Loss: 0.00078599
Iteration 8/25 | Loss: 0.00078599
Iteration 9/25 | Loss: 0.00078599
Iteration 10/25 | Loss: 0.00078599
Iteration 11/25 | Loss: 0.00078599
Iteration 12/25 | Loss: 0.00078599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007859884644858539, 0.0007859884644858539, 0.0007859884644858539, 0.0007859884644858539, 0.0007859884644858539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007859884644858539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58873141
Iteration 2/25 | Loss: 0.00125036
Iteration 3/25 | Loss: 0.00125036
Iteration 4/25 | Loss: 0.00125036
Iteration 5/25 | Loss: 0.00125036
Iteration 6/25 | Loss: 0.00125036
Iteration 7/25 | Loss: 0.00125036
Iteration 8/25 | Loss: 0.00125036
Iteration 9/25 | Loss: 0.00125036
Iteration 10/25 | Loss: 0.00125036
Iteration 11/25 | Loss: 0.00125036
Iteration 12/25 | Loss: 0.00125036
Iteration 13/25 | Loss: 0.00125036
Iteration 14/25 | Loss: 0.00125036
Iteration 15/25 | Loss: 0.00125036
Iteration 16/25 | Loss: 0.00125036
Iteration 17/25 | Loss: 0.00125036
Iteration 18/25 | Loss: 0.00125036
Iteration 19/25 | Loss: 0.00125036
Iteration 20/25 | Loss: 0.00125036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012503579491749406, 0.0012503579491749406, 0.0012503579491749406, 0.0012503579491749406, 0.0012503579491749406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012503579491749406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125036
Iteration 2/1000 | Loss: 0.00002684
Iteration 3/1000 | Loss: 0.00002133
Iteration 4/1000 | Loss: 0.00002073
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001861
Iteration 8/1000 | Loss: 0.00001838
Iteration 9/1000 | Loss: 0.00001834
Iteration 10/1000 | Loss: 0.00001834
Iteration 11/1000 | Loss: 0.00001819
Iteration 12/1000 | Loss: 0.00001814
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001814
Iteration 15/1000 | Loss: 0.00001814
Iteration 16/1000 | Loss: 0.00001814
Iteration 17/1000 | Loss: 0.00001814
Iteration 18/1000 | Loss: 0.00001814
Iteration 19/1000 | Loss: 0.00001814
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001811
Iteration 22/1000 | Loss: 0.00001810
Iteration 23/1000 | Loss: 0.00001810
Iteration 24/1000 | Loss: 0.00001809
Iteration 25/1000 | Loss: 0.00001809
Iteration 26/1000 | Loss: 0.00001809
Iteration 27/1000 | Loss: 0.00001809
Iteration 28/1000 | Loss: 0.00001808
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001807
Iteration 32/1000 | Loss: 0.00001807
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001807
Iteration 36/1000 | Loss: 0.00001807
Iteration 37/1000 | Loss: 0.00001807
Iteration 38/1000 | Loss: 0.00001807
Iteration 39/1000 | Loss: 0.00001806
Iteration 40/1000 | Loss: 0.00001806
Iteration 41/1000 | Loss: 0.00001806
Iteration 42/1000 | Loss: 0.00001805
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00001803
Iteration 45/1000 | Loss: 0.00001803
Iteration 46/1000 | Loss: 0.00001803
Iteration 47/1000 | Loss: 0.00001803
Iteration 48/1000 | Loss: 0.00001803
Iteration 49/1000 | Loss: 0.00001802
Iteration 50/1000 | Loss: 0.00001802
Iteration 51/1000 | Loss: 0.00001802
Iteration 52/1000 | Loss: 0.00001802
Iteration 53/1000 | Loss: 0.00001802
Iteration 54/1000 | Loss: 0.00001802
Iteration 55/1000 | Loss: 0.00001801
Iteration 56/1000 | Loss: 0.00001801
Iteration 57/1000 | Loss: 0.00001801
Iteration 58/1000 | Loss: 0.00001801
Iteration 59/1000 | Loss: 0.00001801
Iteration 60/1000 | Loss: 0.00001800
Iteration 61/1000 | Loss: 0.00001800
Iteration 62/1000 | Loss: 0.00001800
Iteration 63/1000 | Loss: 0.00001800
Iteration 64/1000 | Loss: 0.00001800
Iteration 65/1000 | Loss: 0.00001800
Iteration 66/1000 | Loss: 0.00001800
Iteration 67/1000 | Loss: 0.00001800
Iteration 68/1000 | Loss: 0.00001800
Iteration 69/1000 | Loss: 0.00001800
Iteration 70/1000 | Loss: 0.00001800
Iteration 71/1000 | Loss: 0.00001800
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001799
Iteration 76/1000 | Loss: 0.00001799
Iteration 77/1000 | Loss: 0.00001799
Iteration 78/1000 | Loss: 0.00001799
Iteration 79/1000 | Loss: 0.00001799
Iteration 80/1000 | Loss: 0.00001799
Iteration 81/1000 | Loss: 0.00001799
Iteration 82/1000 | Loss: 0.00001799
Iteration 83/1000 | Loss: 0.00001799
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.799371966626495e-05, 1.799371966626495e-05, 1.799371966626495e-05, 1.799371966626495e-05, 1.799371966626495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.799371966626495e-05

Optimization complete. Final v2v error: 3.5789480209350586 mm

Highest mean error: 3.7634613513946533 mm for frame 140

Lowest mean error: 3.4062812328338623 mm for frame 46

Saving results

Total time: 28.589171886444092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447253
Iteration 2/25 | Loss: 0.00107307
Iteration 3/25 | Loss: 0.00082374
Iteration 4/25 | Loss: 0.00080458
Iteration 5/25 | Loss: 0.00079901
Iteration 6/25 | Loss: 0.00079719
Iteration 7/25 | Loss: 0.00079665
Iteration 8/25 | Loss: 0.00079665
Iteration 9/25 | Loss: 0.00079665
Iteration 10/25 | Loss: 0.00079665
Iteration 11/25 | Loss: 0.00079665
Iteration 12/25 | Loss: 0.00079665
Iteration 13/25 | Loss: 0.00079665
Iteration 14/25 | Loss: 0.00079665
Iteration 15/25 | Loss: 0.00079665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007966536795720458, 0.0007966536795720458, 0.0007966536795720458, 0.0007966536795720458, 0.0007966536795720458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007966536795720458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.99624014
Iteration 2/25 | Loss: 0.00122022
Iteration 3/25 | Loss: 0.00122022
Iteration 4/25 | Loss: 0.00122022
Iteration 5/25 | Loss: 0.00122022
Iteration 6/25 | Loss: 0.00122022
Iteration 7/25 | Loss: 0.00122022
Iteration 8/25 | Loss: 0.00122022
Iteration 9/25 | Loss: 0.00122022
Iteration 10/25 | Loss: 0.00122022
Iteration 11/25 | Loss: 0.00122022
Iteration 12/25 | Loss: 0.00122022
Iteration 13/25 | Loss: 0.00122022
Iteration 14/25 | Loss: 0.00122022
Iteration 15/25 | Loss: 0.00122022
Iteration 16/25 | Loss: 0.00122022
Iteration 17/25 | Loss: 0.00122022
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012202175566926599, 0.0012202175566926599, 0.0012202175566926599, 0.0012202175566926599, 0.0012202175566926599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012202175566926599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122022
Iteration 2/1000 | Loss: 0.00002375
Iteration 3/1000 | Loss: 0.00001810
Iteration 4/1000 | Loss: 0.00001681
Iteration 5/1000 | Loss: 0.00001599
Iteration 6/1000 | Loss: 0.00001559
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001525
Iteration 9/1000 | Loss: 0.00001507
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001478
Iteration 18/1000 | Loss: 0.00001478
Iteration 19/1000 | Loss: 0.00001478
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001472
Iteration 29/1000 | Loss: 0.00001472
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001471
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001469
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001469
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001469
Iteration 59/1000 | Loss: 0.00001469
Iteration 60/1000 | Loss: 0.00001469
Iteration 61/1000 | Loss: 0.00001469
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001469
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001469
Iteration 67/1000 | Loss: 0.00001469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.468525078962557e-05, 1.468525078962557e-05, 1.468525078962557e-05, 1.468525078962557e-05, 1.468525078962557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.468525078962557e-05

Optimization complete. Final v2v error: 3.2118566036224365 mm

Highest mean error: 3.6580970287323 mm for frame 107

Lowest mean error: 2.963362455368042 mm for frame 18

Saving results

Total time: 28.100345611572266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810276
Iteration 2/25 | Loss: 0.00105116
Iteration 3/25 | Loss: 0.00079298
Iteration 4/25 | Loss: 0.00076377
Iteration 5/25 | Loss: 0.00075731
Iteration 6/25 | Loss: 0.00075500
Iteration 7/25 | Loss: 0.00075420
Iteration 8/25 | Loss: 0.00075420
Iteration 9/25 | Loss: 0.00075420
Iteration 10/25 | Loss: 0.00075420
Iteration 11/25 | Loss: 0.00075420
Iteration 12/25 | Loss: 0.00075420
Iteration 13/25 | Loss: 0.00075420
Iteration 14/25 | Loss: 0.00075420
Iteration 15/25 | Loss: 0.00075420
Iteration 16/25 | Loss: 0.00075420
Iteration 17/25 | Loss: 0.00075420
Iteration 18/25 | Loss: 0.00075420
Iteration 19/25 | Loss: 0.00075420
Iteration 20/25 | Loss: 0.00075420
Iteration 21/25 | Loss: 0.00075420
Iteration 22/25 | Loss: 0.00075420
Iteration 23/25 | Loss: 0.00075420
Iteration 24/25 | Loss: 0.00075420
Iteration 25/25 | Loss: 0.00075420

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60167575
Iteration 2/25 | Loss: 0.00126883
Iteration 3/25 | Loss: 0.00126883
Iteration 4/25 | Loss: 0.00126882
Iteration 5/25 | Loss: 0.00126882
Iteration 6/25 | Loss: 0.00126882
Iteration 7/25 | Loss: 0.00126882
Iteration 8/25 | Loss: 0.00126882
Iteration 9/25 | Loss: 0.00126882
Iteration 10/25 | Loss: 0.00126882
Iteration 11/25 | Loss: 0.00126882
Iteration 12/25 | Loss: 0.00126882
Iteration 13/25 | Loss: 0.00126882
Iteration 14/25 | Loss: 0.00126882
Iteration 15/25 | Loss: 0.00126882
Iteration 16/25 | Loss: 0.00126882
Iteration 17/25 | Loss: 0.00126882
Iteration 18/25 | Loss: 0.00126882
Iteration 19/25 | Loss: 0.00126882
Iteration 20/25 | Loss: 0.00126882
Iteration 21/25 | Loss: 0.00126882
Iteration 22/25 | Loss: 0.00126882
Iteration 23/25 | Loss: 0.00126882
Iteration 24/25 | Loss: 0.00126882
Iteration 25/25 | Loss: 0.00126882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126882
Iteration 2/1000 | Loss: 0.00002619
Iteration 3/1000 | Loss: 0.00001599
Iteration 4/1000 | Loss: 0.00001423
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001229
Iteration 8/1000 | Loss: 0.00001199
Iteration 9/1000 | Loss: 0.00001199
Iteration 10/1000 | Loss: 0.00001193
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001162
Iteration 14/1000 | Loss: 0.00001161
Iteration 15/1000 | Loss: 0.00001159
Iteration 16/1000 | Loss: 0.00001157
Iteration 17/1000 | Loss: 0.00001156
Iteration 18/1000 | Loss: 0.00001155
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001143
Iteration 25/1000 | Loss: 0.00001142
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001141
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001140
Iteration 30/1000 | Loss: 0.00001139
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001127
Iteration 41/1000 | Loss: 0.00001127
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001125
Iteration 44/1000 | Loss: 0.00001125
Iteration 45/1000 | Loss: 0.00001125
Iteration 46/1000 | Loss: 0.00001124
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001124
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001122
Iteration 58/1000 | Loss: 0.00001122
Iteration 59/1000 | Loss: 0.00001122
Iteration 60/1000 | Loss: 0.00001122
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001122
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001121
Iteration 74/1000 | Loss: 0.00001121
Iteration 75/1000 | Loss: 0.00001121
Iteration 76/1000 | Loss: 0.00001121
Iteration 77/1000 | Loss: 0.00001120
Iteration 78/1000 | Loss: 0.00001120
Iteration 79/1000 | Loss: 0.00001120
Iteration 80/1000 | Loss: 0.00001120
Iteration 81/1000 | Loss: 0.00001120
Iteration 82/1000 | Loss: 0.00001120
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001120
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001120
Iteration 89/1000 | Loss: 0.00001120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.1197947969776578e-05, 1.1197947969776578e-05, 1.1197947969776578e-05, 1.1197947969776578e-05, 1.1197947969776578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1197947969776578e-05

Optimization complete. Final v2v error: 2.82892107963562 mm

Highest mean error: 3.1586873531341553 mm for frame 85

Lowest mean error: 2.689279556274414 mm for frame 163

Saving results

Total time: 33.95850133895874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895803
Iteration 2/25 | Loss: 0.00266056
Iteration 3/25 | Loss: 0.00163918
Iteration 4/25 | Loss: 0.00170956
Iteration 5/25 | Loss: 0.00105567
Iteration 6/25 | Loss: 0.00093667
Iteration 7/25 | Loss: 0.00091542
Iteration 8/25 | Loss: 0.00090644
Iteration 9/25 | Loss: 0.00090470
Iteration 10/25 | Loss: 0.00090402
Iteration 11/25 | Loss: 0.00090380
Iteration 12/25 | Loss: 0.00090374
Iteration 13/25 | Loss: 0.00090374
Iteration 14/25 | Loss: 0.00090374
Iteration 15/25 | Loss: 0.00090374
Iteration 16/25 | Loss: 0.00090374
Iteration 17/25 | Loss: 0.00090374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009037443087436259, 0.0009037443087436259, 0.0009037443087436259, 0.0009037443087436259, 0.0009037443087436259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009037443087436259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54897761
Iteration 2/25 | Loss: 0.00137142
Iteration 3/25 | Loss: 0.00137134
Iteration 4/25 | Loss: 0.00137134
Iteration 5/25 | Loss: 0.00137134
Iteration 6/25 | Loss: 0.00137134
Iteration 7/25 | Loss: 0.00137133
Iteration 8/25 | Loss: 0.00137133
Iteration 9/25 | Loss: 0.00137133
Iteration 10/25 | Loss: 0.00137133
Iteration 11/25 | Loss: 0.00137133
Iteration 12/25 | Loss: 0.00137133
Iteration 13/25 | Loss: 0.00137133
Iteration 14/25 | Loss: 0.00137133
Iteration 15/25 | Loss: 0.00137133
Iteration 16/25 | Loss: 0.00137133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013713344233110547, 0.0013713344233110547, 0.0013713344233110547, 0.0013713344233110547, 0.0013713344233110547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013713344233110547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137133
Iteration 2/1000 | Loss: 0.00003249
Iteration 3/1000 | Loss: 0.00002493
Iteration 4/1000 | Loss: 0.00002327
Iteration 5/1000 | Loss: 0.00002234
Iteration 6/1000 | Loss: 0.00002186
Iteration 7/1000 | Loss: 0.00002151
Iteration 8/1000 | Loss: 0.00002127
Iteration 9/1000 | Loss: 0.00002109
Iteration 10/1000 | Loss: 0.00002092
Iteration 11/1000 | Loss: 0.00002078
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002074
Iteration 14/1000 | Loss: 0.00002074
Iteration 15/1000 | Loss: 0.00002073
Iteration 16/1000 | Loss: 0.00002073
Iteration 17/1000 | Loss: 0.00002073
Iteration 18/1000 | Loss: 0.00002073
Iteration 19/1000 | Loss: 0.00002073
Iteration 20/1000 | Loss: 0.00002073
Iteration 21/1000 | Loss: 0.00002071
Iteration 22/1000 | Loss: 0.00002068
Iteration 23/1000 | Loss: 0.00002065
Iteration 24/1000 | Loss: 0.00002065
Iteration 25/1000 | Loss: 0.00002064
Iteration 26/1000 | Loss: 0.00002064
Iteration 27/1000 | Loss: 0.00002064
Iteration 28/1000 | Loss: 0.00002064
Iteration 29/1000 | Loss: 0.00002062
Iteration 30/1000 | Loss: 0.00002062
Iteration 31/1000 | Loss: 0.00002061
Iteration 32/1000 | Loss: 0.00002061
Iteration 33/1000 | Loss: 0.00002061
Iteration 34/1000 | Loss: 0.00002060
Iteration 35/1000 | Loss: 0.00002059
Iteration 36/1000 | Loss: 0.00002059
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002058
Iteration 39/1000 | Loss: 0.00002058
Iteration 40/1000 | Loss: 0.00002058
Iteration 41/1000 | Loss: 0.00002058
Iteration 42/1000 | Loss: 0.00002058
Iteration 43/1000 | Loss: 0.00002058
Iteration 44/1000 | Loss: 0.00002058
Iteration 45/1000 | Loss: 0.00002058
Iteration 46/1000 | Loss: 0.00002058
Iteration 47/1000 | Loss: 0.00002058
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002057
Iteration 50/1000 | Loss: 0.00002057
Iteration 51/1000 | Loss: 0.00002056
Iteration 52/1000 | Loss: 0.00002056
Iteration 53/1000 | Loss: 0.00002056
Iteration 54/1000 | Loss: 0.00002056
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002056
Iteration 60/1000 | Loss: 0.00002056
Iteration 61/1000 | Loss: 0.00002056
Iteration 62/1000 | Loss: 0.00002056
Iteration 63/1000 | Loss: 0.00002056
Iteration 64/1000 | Loss: 0.00002056
Iteration 65/1000 | Loss: 0.00002056
Iteration 66/1000 | Loss: 0.00002056
Iteration 67/1000 | Loss: 0.00002056
Iteration 68/1000 | Loss: 0.00002056
Iteration 69/1000 | Loss: 0.00002056
Iteration 70/1000 | Loss: 0.00002056
Iteration 71/1000 | Loss: 0.00002056
Iteration 72/1000 | Loss: 0.00002056
Iteration 73/1000 | Loss: 0.00002056
Iteration 74/1000 | Loss: 0.00002056
Iteration 75/1000 | Loss: 0.00002056
Iteration 76/1000 | Loss: 0.00002056
Iteration 77/1000 | Loss: 0.00002056
Iteration 78/1000 | Loss: 0.00002056
Iteration 79/1000 | Loss: 0.00002056
Iteration 80/1000 | Loss: 0.00002056
Iteration 81/1000 | Loss: 0.00002056
Iteration 82/1000 | Loss: 0.00002056
Iteration 83/1000 | Loss: 0.00002056
Iteration 84/1000 | Loss: 0.00002056
Iteration 85/1000 | Loss: 0.00002056
Iteration 86/1000 | Loss: 0.00002056
Iteration 87/1000 | Loss: 0.00002056
Iteration 88/1000 | Loss: 0.00002056
Iteration 89/1000 | Loss: 0.00002056
Iteration 90/1000 | Loss: 0.00002056
Iteration 91/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.0555800801957957e-05, 2.0555800801957957e-05, 2.0555800801957957e-05, 2.0555800801957957e-05, 2.0555800801957957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0555800801957957e-05

Optimization complete. Final v2v error: 3.8260934352874756 mm

Highest mean error: 4.115885257720947 mm for frame 190

Lowest mean error: 3.574871063232422 mm for frame 4

Saving results

Total time: 46.35028576850891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848760
Iteration 2/25 | Loss: 0.00129839
Iteration 3/25 | Loss: 0.00099414
Iteration 4/25 | Loss: 0.00090428
Iteration 5/25 | Loss: 0.00088189
Iteration 6/25 | Loss: 0.00087706
Iteration 7/25 | Loss: 0.00087616
Iteration 8/25 | Loss: 0.00087616
Iteration 9/25 | Loss: 0.00087616
Iteration 10/25 | Loss: 0.00087616
Iteration 11/25 | Loss: 0.00087616
Iteration 12/25 | Loss: 0.00087616
Iteration 13/25 | Loss: 0.00087616
Iteration 14/25 | Loss: 0.00087616
Iteration 15/25 | Loss: 0.00087616
Iteration 16/25 | Loss: 0.00087616
Iteration 17/25 | Loss: 0.00087616
Iteration 18/25 | Loss: 0.00087616
Iteration 19/25 | Loss: 0.00087616
Iteration 20/25 | Loss: 0.00087616
Iteration 21/25 | Loss: 0.00087616
Iteration 22/25 | Loss: 0.00087616
Iteration 23/25 | Loss: 0.00087616
Iteration 24/25 | Loss: 0.00087616
Iteration 25/25 | Loss: 0.00087616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59409857
Iteration 2/25 | Loss: 0.00132175
Iteration 3/25 | Loss: 0.00132174
Iteration 4/25 | Loss: 0.00132174
Iteration 5/25 | Loss: 0.00132174
Iteration 6/25 | Loss: 0.00132174
Iteration 7/25 | Loss: 0.00132174
Iteration 8/25 | Loss: 0.00132174
Iteration 9/25 | Loss: 0.00132174
Iteration 10/25 | Loss: 0.00132174
Iteration 11/25 | Loss: 0.00132174
Iteration 12/25 | Loss: 0.00132174
Iteration 13/25 | Loss: 0.00132174
Iteration 14/25 | Loss: 0.00132174
Iteration 15/25 | Loss: 0.00132174
Iteration 16/25 | Loss: 0.00132174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001321742427535355, 0.001321742427535355, 0.001321742427535355, 0.001321742427535355, 0.001321742427535355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001321742427535355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132174
Iteration 2/1000 | Loss: 0.00004200
Iteration 3/1000 | Loss: 0.00002724
Iteration 4/1000 | Loss: 0.00002508
Iteration 5/1000 | Loss: 0.00002379
Iteration 6/1000 | Loss: 0.00002303
Iteration 7/1000 | Loss: 0.00002249
Iteration 8/1000 | Loss: 0.00002212
Iteration 9/1000 | Loss: 0.00002175
Iteration 10/1000 | Loss: 0.00002153
Iteration 11/1000 | Loss: 0.00002138
Iteration 12/1000 | Loss: 0.00002133
Iteration 13/1000 | Loss: 0.00002129
Iteration 14/1000 | Loss: 0.00002129
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00002128
Iteration 17/1000 | Loss: 0.00002127
Iteration 18/1000 | Loss: 0.00002126
Iteration 19/1000 | Loss: 0.00002126
Iteration 20/1000 | Loss: 0.00002125
Iteration 21/1000 | Loss: 0.00002125
Iteration 22/1000 | Loss: 0.00002124
Iteration 23/1000 | Loss: 0.00002123
Iteration 24/1000 | Loss: 0.00002123
Iteration 25/1000 | Loss: 0.00002121
Iteration 26/1000 | Loss: 0.00002121
Iteration 27/1000 | Loss: 0.00002118
Iteration 28/1000 | Loss: 0.00002118
Iteration 29/1000 | Loss: 0.00002116
Iteration 30/1000 | Loss: 0.00002115
Iteration 31/1000 | Loss: 0.00002115
Iteration 32/1000 | Loss: 0.00002115
Iteration 33/1000 | Loss: 0.00002115
Iteration 34/1000 | Loss: 0.00002115
Iteration 35/1000 | Loss: 0.00002115
Iteration 36/1000 | Loss: 0.00002115
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002115
Iteration 39/1000 | Loss: 0.00002115
Iteration 40/1000 | Loss: 0.00002114
Iteration 41/1000 | Loss: 0.00002114
Iteration 42/1000 | Loss: 0.00002114
Iteration 43/1000 | Loss: 0.00002114
Iteration 44/1000 | Loss: 0.00002114
Iteration 45/1000 | Loss: 0.00002114
Iteration 46/1000 | Loss: 0.00002114
Iteration 47/1000 | Loss: 0.00002114
Iteration 48/1000 | Loss: 0.00002113
Iteration 49/1000 | Loss: 0.00002113
Iteration 50/1000 | Loss: 0.00002113
Iteration 51/1000 | Loss: 0.00002113
Iteration 52/1000 | Loss: 0.00002113
Iteration 53/1000 | Loss: 0.00002113
Iteration 54/1000 | Loss: 0.00002113
Iteration 55/1000 | Loss: 0.00002113
Iteration 56/1000 | Loss: 0.00002112
Iteration 57/1000 | Loss: 0.00002112
Iteration 58/1000 | Loss: 0.00002112
Iteration 59/1000 | Loss: 0.00002112
Iteration 60/1000 | Loss: 0.00002112
Iteration 61/1000 | Loss: 0.00002112
Iteration 62/1000 | Loss: 0.00002112
Iteration 63/1000 | Loss: 0.00002112
Iteration 64/1000 | Loss: 0.00002111
Iteration 65/1000 | Loss: 0.00002111
Iteration 66/1000 | Loss: 0.00002111
Iteration 67/1000 | Loss: 0.00002111
Iteration 68/1000 | Loss: 0.00002111
Iteration 69/1000 | Loss: 0.00002111
Iteration 70/1000 | Loss: 0.00002111
Iteration 71/1000 | Loss: 0.00002111
Iteration 72/1000 | Loss: 0.00002111
Iteration 73/1000 | Loss: 0.00002110
Iteration 74/1000 | Loss: 0.00002110
Iteration 75/1000 | Loss: 0.00002110
Iteration 76/1000 | Loss: 0.00002110
Iteration 77/1000 | Loss: 0.00002110
Iteration 78/1000 | Loss: 0.00002110
Iteration 79/1000 | Loss: 0.00002110
Iteration 80/1000 | Loss: 0.00002109
Iteration 81/1000 | Loss: 0.00002109
Iteration 82/1000 | Loss: 0.00002109
Iteration 83/1000 | Loss: 0.00002109
Iteration 84/1000 | Loss: 0.00002109
Iteration 85/1000 | Loss: 0.00002109
Iteration 86/1000 | Loss: 0.00002109
Iteration 87/1000 | Loss: 0.00002109
Iteration 88/1000 | Loss: 0.00002109
Iteration 89/1000 | Loss: 0.00002109
Iteration 90/1000 | Loss: 0.00002109
Iteration 91/1000 | Loss: 0.00002109
Iteration 92/1000 | Loss: 0.00002109
Iteration 93/1000 | Loss: 0.00002109
Iteration 94/1000 | Loss: 0.00002109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.1088610083097592e-05, 2.1088610083097592e-05, 2.1088610083097592e-05, 2.1088610083097592e-05, 2.1088610083097592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1088610083097592e-05

Optimization complete. Final v2v error: 3.7815167903900146 mm

Highest mean error: 4.146029472351074 mm for frame 210

Lowest mean error: 3.5647764205932617 mm for frame 237

Saving results

Total time: 37.06117630004883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868692
Iteration 2/25 | Loss: 0.00110839
Iteration 3/25 | Loss: 0.00088850
Iteration 4/25 | Loss: 0.00084322
Iteration 5/25 | Loss: 0.00082945
Iteration 6/25 | Loss: 0.00082665
Iteration 7/25 | Loss: 0.00082610
Iteration 8/25 | Loss: 0.00082610
Iteration 9/25 | Loss: 0.00082610
Iteration 10/25 | Loss: 0.00082610
Iteration 11/25 | Loss: 0.00082610
Iteration 12/25 | Loss: 0.00082610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008261006441898644, 0.0008261006441898644, 0.0008261006441898644, 0.0008261006441898644, 0.0008261006441898644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008261006441898644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56128204
Iteration 2/25 | Loss: 0.00122313
Iteration 3/25 | Loss: 0.00122306
Iteration 4/25 | Loss: 0.00122306
Iteration 5/25 | Loss: 0.00122306
Iteration 6/25 | Loss: 0.00122306
Iteration 7/25 | Loss: 0.00122306
Iteration 8/25 | Loss: 0.00122306
Iteration 9/25 | Loss: 0.00122306
Iteration 10/25 | Loss: 0.00122306
Iteration 11/25 | Loss: 0.00122306
Iteration 12/25 | Loss: 0.00122306
Iteration 13/25 | Loss: 0.00122306
Iteration 14/25 | Loss: 0.00122306
Iteration 15/25 | Loss: 0.00122306
Iteration 16/25 | Loss: 0.00122306
Iteration 17/25 | Loss: 0.00122306
Iteration 18/25 | Loss: 0.00122306
Iteration 19/25 | Loss: 0.00122306
Iteration 20/25 | Loss: 0.00122306
Iteration 21/25 | Loss: 0.00122306
Iteration 22/25 | Loss: 0.00122306
Iteration 23/25 | Loss: 0.00122306
Iteration 24/25 | Loss: 0.00122306
Iteration 25/25 | Loss: 0.00122306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122306
Iteration 2/1000 | Loss: 0.00004046
Iteration 3/1000 | Loss: 0.00002669
Iteration 4/1000 | Loss: 0.00002288
Iteration 5/1000 | Loss: 0.00002150
Iteration 6/1000 | Loss: 0.00001993
Iteration 7/1000 | Loss: 0.00001917
Iteration 8/1000 | Loss: 0.00001863
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001802
Iteration 11/1000 | Loss: 0.00001778
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001729
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001726
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001723
Iteration 29/1000 | Loss: 0.00001722
Iteration 30/1000 | Loss: 0.00001722
Iteration 31/1000 | Loss: 0.00001721
Iteration 32/1000 | Loss: 0.00001720
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001719
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001719
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001717
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001716
Iteration 48/1000 | Loss: 0.00001716
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001713
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001713
Iteration 61/1000 | Loss: 0.00001713
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001708
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001708
Iteration 100/1000 | Loss: 0.00001708
Iteration 101/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.7078244127333164e-05, 1.7078244127333164e-05, 1.7078244127333164e-05, 1.7078244127333164e-05, 1.7078244127333164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7078244127333164e-05

Optimization complete. Final v2v error: 3.4047930240631104 mm

Highest mean error: 4.081078052520752 mm for frame 219

Lowest mean error: 3.007753610610962 mm for frame 159

Saving results

Total time: 41.985257148742676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01176339
Iteration 2/25 | Loss: 0.00168321
Iteration 3/25 | Loss: 0.00103945
Iteration 4/25 | Loss: 0.00096704
Iteration 5/25 | Loss: 0.00095043
Iteration 6/25 | Loss: 0.00094630
Iteration 7/25 | Loss: 0.00094572
Iteration 8/25 | Loss: 0.00094572
Iteration 9/25 | Loss: 0.00094572
Iteration 10/25 | Loss: 0.00094572
Iteration 11/25 | Loss: 0.00094572
Iteration 12/25 | Loss: 0.00094572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009457152336835861, 0.0009457152336835861, 0.0009457152336835861, 0.0009457152336835861, 0.0009457152336835861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009457152336835861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12639380
Iteration 2/25 | Loss: 0.00076348
Iteration 3/25 | Loss: 0.00076345
Iteration 4/25 | Loss: 0.00076345
Iteration 5/25 | Loss: 0.00076345
Iteration 6/25 | Loss: 0.00076345
Iteration 7/25 | Loss: 0.00076345
Iteration 8/25 | Loss: 0.00076345
Iteration 9/25 | Loss: 0.00076345
Iteration 10/25 | Loss: 0.00076345
Iteration 11/25 | Loss: 0.00076345
Iteration 12/25 | Loss: 0.00076345
Iteration 13/25 | Loss: 0.00076345
Iteration 14/25 | Loss: 0.00076345
Iteration 15/25 | Loss: 0.00076345
Iteration 16/25 | Loss: 0.00076345
Iteration 17/25 | Loss: 0.00076345
Iteration 18/25 | Loss: 0.00076345
Iteration 19/25 | Loss: 0.00076345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000763451389502734, 0.000763451389502734, 0.000763451389502734, 0.000763451389502734, 0.000763451389502734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000763451389502734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076345
Iteration 2/1000 | Loss: 0.00005962
Iteration 3/1000 | Loss: 0.00004010
Iteration 4/1000 | Loss: 0.00003577
Iteration 5/1000 | Loss: 0.00003414
Iteration 6/1000 | Loss: 0.00003289
Iteration 7/1000 | Loss: 0.00003223
Iteration 8/1000 | Loss: 0.00003171
Iteration 9/1000 | Loss: 0.00003135
Iteration 10/1000 | Loss: 0.00003109
Iteration 11/1000 | Loss: 0.00003091
Iteration 12/1000 | Loss: 0.00003074
Iteration 13/1000 | Loss: 0.00003056
Iteration 14/1000 | Loss: 0.00003040
Iteration 15/1000 | Loss: 0.00003035
Iteration 16/1000 | Loss: 0.00003031
Iteration 17/1000 | Loss: 0.00003028
Iteration 18/1000 | Loss: 0.00003023
Iteration 19/1000 | Loss: 0.00003020
Iteration 20/1000 | Loss: 0.00003020
Iteration 21/1000 | Loss: 0.00003018
Iteration 22/1000 | Loss: 0.00003018
Iteration 23/1000 | Loss: 0.00003018
Iteration 24/1000 | Loss: 0.00003018
Iteration 25/1000 | Loss: 0.00003016
Iteration 26/1000 | Loss: 0.00003014
Iteration 27/1000 | Loss: 0.00003014
Iteration 28/1000 | Loss: 0.00003014
Iteration 29/1000 | Loss: 0.00003013
Iteration 30/1000 | Loss: 0.00003011
Iteration 31/1000 | Loss: 0.00003010
Iteration 32/1000 | Loss: 0.00003010
Iteration 33/1000 | Loss: 0.00003010
Iteration 34/1000 | Loss: 0.00003010
Iteration 35/1000 | Loss: 0.00003010
Iteration 36/1000 | Loss: 0.00003010
Iteration 37/1000 | Loss: 0.00003007
Iteration 38/1000 | Loss: 0.00003007
Iteration 39/1000 | Loss: 0.00003006
Iteration 40/1000 | Loss: 0.00003004
Iteration 41/1000 | Loss: 0.00003004
Iteration 42/1000 | Loss: 0.00003004
Iteration 43/1000 | Loss: 0.00003003
Iteration 44/1000 | Loss: 0.00003003
Iteration 45/1000 | Loss: 0.00003002
Iteration 46/1000 | Loss: 0.00003002
Iteration 47/1000 | Loss: 0.00003002
Iteration 48/1000 | Loss: 0.00003002
Iteration 49/1000 | Loss: 0.00003002
Iteration 50/1000 | Loss: 0.00003001
Iteration 51/1000 | Loss: 0.00003001
Iteration 52/1000 | Loss: 0.00003001
Iteration 53/1000 | Loss: 0.00003001
Iteration 54/1000 | Loss: 0.00003001
Iteration 55/1000 | Loss: 0.00003001
Iteration 56/1000 | Loss: 0.00003001
Iteration 57/1000 | Loss: 0.00003001
Iteration 58/1000 | Loss: 0.00003001
Iteration 59/1000 | Loss: 0.00003001
Iteration 60/1000 | Loss: 0.00002999
Iteration 61/1000 | Loss: 0.00002999
Iteration 62/1000 | Loss: 0.00002998
Iteration 63/1000 | Loss: 0.00002997
Iteration 64/1000 | Loss: 0.00002997
Iteration 65/1000 | Loss: 0.00002997
Iteration 66/1000 | Loss: 0.00002997
Iteration 67/1000 | Loss: 0.00002997
Iteration 68/1000 | Loss: 0.00002997
Iteration 69/1000 | Loss: 0.00002997
Iteration 70/1000 | Loss: 0.00002996
Iteration 71/1000 | Loss: 0.00002995
Iteration 72/1000 | Loss: 0.00002995
Iteration 73/1000 | Loss: 0.00002995
Iteration 74/1000 | Loss: 0.00002995
Iteration 75/1000 | Loss: 0.00002995
Iteration 76/1000 | Loss: 0.00002995
Iteration 77/1000 | Loss: 0.00002995
Iteration 78/1000 | Loss: 0.00002995
Iteration 79/1000 | Loss: 0.00002995
Iteration 80/1000 | Loss: 0.00002995
Iteration 81/1000 | Loss: 0.00002995
Iteration 82/1000 | Loss: 0.00002995
Iteration 83/1000 | Loss: 0.00002995
Iteration 84/1000 | Loss: 0.00002995
Iteration 85/1000 | Loss: 0.00002995
Iteration 86/1000 | Loss: 0.00002994
Iteration 87/1000 | Loss: 0.00002994
Iteration 88/1000 | Loss: 0.00002994
Iteration 89/1000 | Loss: 0.00002994
Iteration 90/1000 | Loss: 0.00002993
Iteration 91/1000 | Loss: 0.00002993
Iteration 92/1000 | Loss: 0.00002993
Iteration 93/1000 | Loss: 0.00002992
Iteration 94/1000 | Loss: 0.00002992
Iteration 95/1000 | Loss: 0.00002992
Iteration 96/1000 | Loss: 0.00002992
Iteration 97/1000 | Loss: 0.00002992
Iteration 98/1000 | Loss: 0.00002991
Iteration 99/1000 | Loss: 0.00002991
Iteration 100/1000 | Loss: 0.00002991
Iteration 101/1000 | Loss: 0.00002991
Iteration 102/1000 | Loss: 0.00002991
Iteration 103/1000 | Loss: 0.00002991
Iteration 104/1000 | Loss: 0.00002991
Iteration 105/1000 | Loss: 0.00002990
Iteration 106/1000 | Loss: 0.00002990
Iteration 107/1000 | Loss: 0.00002990
Iteration 108/1000 | Loss: 0.00002990
Iteration 109/1000 | Loss: 0.00002989
Iteration 110/1000 | Loss: 0.00002989
Iteration 111/1000 | Loss: 0.00002989
Iteration 112/1000 | Loss: 0.00002988
Iteration 113/1000 | Loss: 0.00002988
Iteration 114/1000 | Loss: 0.00002987
Iteration 115/1000 | Loss: 0.00002987
Iteration 116/1000 | Loss: 0.00002987
Iteration 117/1000 | Loss: 0.00002987
Iteration 118/1000 | Loss: 0.00002987
Iteration 119/1000 | Loss: 0.00002987
Iteration 120/1000 | Loss: 0.00002986
Iteration 121/1000 | Loss: 0.00002986
Iteration 122/1000 | Loss: 0.00002986
Iteration 123/1000 | Loss: 0.00002986
Iteration 124/1000 | Loss: 0.00002986
Iteration 125/1000 | Loss: 0.00002986
Iteration 126/1000 | Loss: 0.00002986
Iteration 127/1000 | Loss: 0.00002986
Iteration 128/1000 | Loss: 0.00002986
Iteration 129/1000 | Loss: 0.00002986
Iteration 130/1000 | Loss: 0.00002986
Iteration 131/1000 | Loss: 0.00002986
Iteration 132/1000 | Loss: 0.00002986
Iteration 133/1000 | Loss: 0.00002986
Iteration 134/1000 | Loss: 0.00002986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.9858516427339055e-05, 2.9858516427339055e-05, 2.9858516427339055e-05, 2.9858516427339055e-05, 2.9858516427339055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9858516427339055e-05

Optimization complete. Final v2v error: 4.400130271911621 mm

Highest mean error: 5.700382232666016 mm for frame 52

Lowest mean error: 3.7856128215789795 mm for frame 27

Saving results

Total time: 49.796738386154175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062173
Iteration 2/25 | Loss: 0.00229842
Iteration 3/25 | Loss: 0.00146201
Iteration 4/25 | Loss: 0.00116291
Iteration 5/25 | Loss: 0.00105748
Iteration 6/25 | Loss: 0.00101321
Iteration 7/25 | Loss: 0.00102320
Iteration 8/25 | Loss: 0.00104151
Iteration 9/25 | Loss: 0.00094369
Iteration 10/25 | Loss: 0.00090002
Iteration 11/25 | Loss: 0.00088700
Iteration 12/25 | Loss: 0.00088850
Iteration 13/25 | Loss: 0.00086531
Iteration 14/25 | Loss: 0.00085564
Iteration 15/25 | Loss: 0.00085092
Iteration 16/25 | Loss: 0.00084591
Iteration 17/25 | Loss: 0.00084236
Iteration 18/25 | Loss: 0.00083963
Iteration 19/25 | Loss: 0.00083702
Iteration 20/25 | Loss: 0.00083640
Iteration 21/25 | Loss: 0.00083476
Iteration 22/25 | Loss: 0.00083519
Iteration 23/25 | Loss: 0.00083426
Iteration 24/25 | Loss: 0.00083177
Iteration 25/25 | Loss: 0.00083184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59432447
Iteration 2/25 | Loss: 0.00171152
Iteration 3/25 | Loss: 0.00122447
Iteration 4/25 | Loss: 0.00122447
Iteration 5/25 | Loss: 0.00122447
Iteration 6/25 | Loss: 0.00122447
Iteration 7/25 | Loss: 0.00122447
Iteration 8/25 | Loss: 0.00122447
Iteration 9/25 | Loss: 0.00122447
Iteration 10/25 | Loss: 0.00122447
Iteration 11/25 | Loss: 0.00122447
Iteration 12/25 | Loss: 0.00122447
Iteration 13/25 | Loss: 0.00122447
Iteration 14/25 | Loss: 0.00122447
Iteration 15/25 | Loss: 0.00122447
Iteration 16/25 | Loss: 0.00122447
Iteration 17/25 | Loss: 0.00122447
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012244698591530323, 0.0012244698591530323, 0.0012244698591530323, 0.0012244698591530323, 0.0012244698591530323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012244698591530323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122447
Iteration 2/1000 | Loss: 0.00005476
Iteration 3/1000 | Loss: 0.00054330
Iteration 4/1000 | Loss: 0.00265724
Iteration 5/1000 | Loss: 0.00330602
Iteration 6/1000 | Loss: 0.00028519
Iteration 7/1000 | Loss: 0.00017080
Iteration 8/1000 | Loss: 0.00118951
Iteration 9/1000 | Loss: 0.00099099
Iteration 10/1000 | Loss: 0.00121531
Iteration 11/1000 | Loss: 0.00227268
Iteration 12/1000 | Loss: 0.00014470
Iteration 13/1000 | Loss: 0.00155664
Iteration 14/1000 | Loss: 0.00095779
Iteration 15/1000 | Loss: 0.00011115
Iteration 16/1000 | Loss: 0.00006312
Iteration 17/1000 | Loss: 0.00010436
Iteration 18/1000 | Loss: 0.00005633
Iteration 19/1000 | Loss: 0.00009956
Iteration 20/1000 | Loss: 0.00007079
Iteration 21/1000 | Loss: 0.00003512
Iteration 22/1000 | Loss: 0.00084401
Iteration 23/1000 | Loss: 0.00057992
Iteration 24/1000 | Loss: 0.00046544
Iteration 25/1000 | Loss: 0.00109214
Iteration 26/1000 | Loss: 0.00058954
Iteration 27/1000 | Loss: 0.00004467
Iteration 28/1000 | Loss: 0.00110940
Iteration 29/1000 | Loss: 0.00019281
Iteration 30/1000 | Loss: 0.00021727
Iteration 31/1000 | Loss: 0.00033444
Iteration 32/1000 | Loss: 0.00006277
Iteration 33/1000 | Loss: 0.00006436
Iteration 34/1000 | Loss: 0.00050544
Iteration 35/1000 | Loss: 0.00006220
Iteration 36/1000 | Loss: 0.00035437
Iteration 37/1000 | Loss: 0.00002828
Iteration 38/1000 | Loss: 0.00005284
Iteration 39/1000 | Loss: 0.00002393
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00006039
Iteration 42/1000 | Loss: 0.00006951
Iteration 43/1000 | Loss: 0.00004642
Iteration 44/1000 | Loss: 0.00002235
Iteration 45/1000 | Loss: 0.00002199
Iteration 46/1000 | Loss: 0.00002199
Iteration 47/1000 | Loss: 0.00004596
Iteration 48/1000 | Loss: 0.00006339
Iteration 49/1000 | Loss: 0.00006750
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002179
Iteration 52/1000 | Loss: 0.00002178
Iteration 53/1000 | Loss: 0.00002178
Iteration 54/1000 | Loss: 0.00002172
Iteration 55/1000 | Loss: 0.00002548
Iteration 56/1000 | Loss: 0.00002547
Iteration 57/1000 | Loss: 0.00002541
Iteration 58/1000 | Loss: 0.00016199
Iteration 59/1000 | Loss: 0.00016615
Iteration 60/1000 | Loss: 0.00007915
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002508
Iteration 63/1000 | Loss: 0.00002939
Iteration 64/1000 | Loss: 0.00002304
Iteration 65/1000 | Loss: 0.00002147
Iteration 66/1000 | Loss: 0.00002147
Iteration 67/1000 | Loss: 0.00006018
Iteration 68/1000 | Loss: 0.00002149
Iteration 69/1000 | Loss: 0.00002140
Iteration 70/1000 | Loss: 0.00002139
Iteration 71/1000 | Loss: 0.00002139
Iteration 72/1000 | Loss: 0.00002138
Iteration 73/1000 | Loss: 0.00002137
Iteration 74/1000 | Loss: 0.00002135
Iteration 75/1000 | Loss: 0.00002135
Iteration 76/1000 | Loss: 0.00002135
Iteration 77/1000 | Loss: 0.00002135
Iteration 78/1000 | Loss: 0.00002135
Iteration 79/1000 | Loss: 0.00002135
Iteration 80/1000 | Loss: 0.00002135
Iteration 81/1000 | Loss: 0.00002135
Iteration 82/1000 | Loss: 0.00002134
Iteration 83/1000 | Loss: 0.00002134
Iteration 84/1000 | Loss: 0.00002134
Iteration 85/1000 | Loss: 0.00002134
Iteration 86/1000 | Loss: 0.00002134
Iteration 87/1000 | Loss: 0.00002134
Iteration 88/1000 | Loss: 0.00002134
Iteration 89/1000 | Loss: 0.00002134
Iteration 90/1000 | Loss: 0.00002134
Iteration 91/1000 | Loss: 0.00002134
Iteration 92/1000 | Loss: 0.00002134
Iteration 93/1000 | Loss: 0.00002134
Iteration 94/1000 | Loss: 0.00002133
Iteration 95/1000 | Loss: 0.00002133
Iteration 96/1000 | Loss: 0.00002133
Iteration 97/1000 | Loss: 0.00002133
Iteration 98/1000 | Loss: 0.00002133
Iteration 99/1000 | Loss: 0.00002133
Iteration 100/1000 | Loss: 0.00002133
Iteration 101/1000 | Loss: 0.00002133
Iteration 102/1000 | Loss: 0.00002132
Iteration 103/1000 | Loss: 0.00002132
Iteration 104/1000 | Loss: 0.00002132
Iteration 105/1000 | Loss: 0.00002132
Iteration 106/1000 | Loss: 0.00002132
Iteration 107/1000 | Loss: 0.00002132
Iteration 108/1000 | Loss: 0.00002132
Iteration 109/1000 | Loss: 0.00002132
Iteration 110/1000 | Loss: 0.00002132
Iteration 111/1000 | Loss: 0.00002132
Iteration 112/1000 | Loss: 0.00002132
Iteration 113/1000 | Loss: 0.00002132
Iteration 114/1000 | Loss: 0.00002132
Iteration 115/1000 | Loss: 0.00002132
Iteration 116/1000 | Loss: 0.00002132
Iteration 117/1000 | Loss: 0.00002132
Iteration 118/1000 | Loss: 0.00002131
Iteration 119/1000 | Loss: 0.00002131
Iteration 120/1000 | Loss: 0.00002131
Iteration 121/1000 | Loss: 0.00002131
Iteration 122/1000 | Loss: 0.00002131
Iteration 123/1000 | Loss: 0.00002131
Iteration 124/1000 | Loss: 0.00002131
Iteration 125/1000 | Loss: 0.00002131
Iteration 126/1000 | Loss: 0.00002131
Iteration 127/1000 | Loss: 0.00002131
Iteration 128/1000 | Loss: 0.00002130
Iteration 129/1000 | Loss: 0.00002130
Iteration 130/1000 | Loss: 0.00002130
Iteration 131/1000 | Loss: 0.00002130
Iteration 132/1000 | Loss: 0.00002130
Iteration 133/1000 | Loss: 0.00002130
Iteration 134/1000 | Loss: 0.00002130
Iteration 135/1000 | Loss: 0.00002130
Iteration 136/1000 | Loss: 0.00002130
Iteration 137/1000 | Loss: 0.00002130
Iteration 138/1000 | Loss: 0.00002130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.1304867914295755e-05, 2.1304867914295755e-05, 2.1304867914295755e-05, 2.1304867914295755e-05, 2.1304867914295755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1304867914295755e-05

Optimization complete. Final v2v error: 3.9149835109710693 mm

Highest mean error: 5.099535942077637 mm for frame 2

Lowest mean error: 3.6109707355499268 mm for frame 169

Saving results

Total time: 153.66484713554382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784064
Iteration 2/25 | Loss: 0.00102172
Iteration 3/25 | Loss: 0.00085622
Iteration 4/25 | Loss: 0.00080496
Iteration 5/25 | Loss: 0.00079041
Iteration 6/25 | Loss: 0.00078812
Iteration 7/25 | Loss: 0.00078782
Iteration 8/25 | Loss: 0.00078748
Iteration 9/25 | Loss: 0.00078748
Iteration 10/25 | Loss: 0.00078748
Iteration 11/25 | Loss: 0.00078748
Iteration 12/25 | Loss: 0.00078748
Iteration 13/25 | Loss: 0.00078748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007874774164520204, 0.0007874774164520204, 0.0007874774164520204, 0.0007874774164520204, 0.0007874774164520204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007874774164520204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54618371
Iteration 2/25 | Loss: 0.00127068
Iteration 3/25 | Loss: 0.00127064
Iteration 4/25 | Loss: 0.00127064
Iteration 5/25 | Loss: 0.00127064
Iteration 6/25 | Loss: 0.00127064
Iteration 7/25 | Loss: 0.00127064
Iteration 8/25 | Loss: 0.00127064
Iteration 9/25 | Loss: 0.00127064
Iteration 10/25 | Loss: 0.00127064
Iteration 11/25 | Loss: 0.00127064
Iteration 12/25 | Loss: 0.00127064
Iteration 13/25 | Loss: 0.00127064
Iteration 14/25 | Loss: 0.00127064
Iteration 15/25 | Loss: 0.00127064
Iteration 16/25 | Loss: 0.00127064
Iteration 17/25 | Loss: 0.00127064
Iteration 18/25 | Loss: 0.00127064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012706398265436292, 0.0012706398265436292, 0.0012706398265436292, 0.0012706398265436292, 0.0012706398265436292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012706398265436292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127064
Iteration 2/1000 | Loss: 0.00003609
Iteration 3/1000 | Loss: 0.00002536
Iteration 4/1000 | Loss: 0.00002266
Iteration 5/1000 | Loss: 0.00002143
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00002000
Iteration 8/1000 | Loss: 0.00001960
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001940
Iteration 11/1000 | Loss: 0.00001924
Iteration 12/1000 | Loss: 0.00001918
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001897
Iteration 16/1000 | Loss: 0.00001897
Iteration 17/1000 | Loss: 0.00001894
Iteration 18/1000 | Loss: 0.00001882
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001872
Iteration 21/1000 | Loss: 0.00001871
Iteration 22/1000 | Loss: 0.00001870
Iteration 23/1000 | Loss: 0.00001870
Iteration 24/1000 | Loss: 0.00001869
Iteration 25/1000 | Loss: 0.00001869
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001860
Iteration 29/1000 | Loss: 0.00001858
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001856
Iteration 33/1000 | Loss: 0.00001856
Iteration 34/1000 | Loss: 0.00001856
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001856
Iteration 38/1000 | Loss: 0.00001856
Iteration 39/1000 | Loss: 0.00001856
Iteration 40/1000 | Loss: 0.00001856
Iteration 41/1000 | Loss: 0.00001856
Iteration 42/1000 | Loss: 0.00001856
Iteration 43/1000 | Loss: 0.00001855
Iteration 44/1000 | Loss: 0.00001855
Iteration 45/1000 | Loss: 0.00001855
Iteration 46/1000 | Loss: 0.00001854
Iteration 47/1000 | Loss: 0.00001853
Iteration 48/1000 | Loss: 0.00001853
Iteration 49/1000 | Loss: 0.00001853
Iteration 50/1000 | Loss: 0.00001852
Iteration 51/1000 | Loss: 0.00001852
Iteration 52/1000 | Loss: 0.00001852
Iteration 53/1000 | Loss: 0.00001852
Iteration 54/1000 | Loss: 0.00001852
Iteration 55/1000 | Loss: 0.00001851
Iteration 56/1000 | Loss: 0.00001851
Iteration 57/1000 | Loss: 0.00001850
Iteration 58/1000 | Loss: 0.00001849
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001848
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001848
Iteration 64/1000 | Loss: 0.00001847
Iteration 65/1000 | Loss: 0.00001847
Iteration 66/1000 | Loss: 0.00001847
Iteration 67/1000 | Loss: 0.00001847
Iteration 68/1000 | Loss: 0.00001847
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001845
Iteration 73/1000 | Loss: 0.00001845
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001844
Iteration 76/1000 | Loss: 0.00001844
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001844
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001844
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001840
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001838
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001837
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001836
Iteration 113/1000 | Loss: 0.00001836
Iteration 114/1000 | Loss: 0.00001836
Iteration 115/1000 | Loss: 0.00001836
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001836
Iteration 119/1000 | Loss: 0.00001836
Iteration 120/1000 | Loss: 0.00001836
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001835
Iteration 130/1000 | Loss: 0.00001835
Iteration 131/1000 | Loss: 0.00001835
Iteration 132/1000 | Loss: 0.00001835
Iteration 133/1000 | Loss: 0.00001835
Iteration 134/1000 | Loss: 0.00001835
Iteration 135/1000 | Loss: 0.00001834
Iteration 136/1000 | Loss: 0.00001834
Iteration 137/1000 | Loss: 0.00001834
Iteration 138/1000 | Loss: 0.00001834
Iteration 139/1000 | Loss: 0.00001834
Iteration 140/1000 | Loss: 0.00001834
Iteration 141/1000 | Loss: 0.00001834
Iteration 142/1000 | Loss: 0.00001834
Iteration 143/1000 | Loss: 0.00001833
Iteration 144/1000 | Loss: 0.00001833
Iteration 145/1000 | Loss: 0.00001833
Iteration 146/1000 | Loss: 0.00001833
Iteration 147/1000 | Loss: 0.00001833
Iteration 148/1000 | Loss: 0.00001833
Iteration 149/1000 | Loss: 0.00001833
Iteration 150/1000 | Loss: 0.00001833
Iteration 151/1000 | Loss: 0.00001833
Iteration 152/1000 | Loss: 0.00001833
Iteration 153/1000 | Loss: 0.00001833
Iteration 154/1000 | Loss: 0.00001833
Iteration 155/1000 | Loss: 0.00001833
Iteration 156/1000 | Loss: 0.00001833
Iteration 157/1000 | Loss: 0.00001833
Iteration 158/1000 | Loss: 0.00001833
Iteration 159/1000 | Loss: 0.00001833
Iteration 160/1000 | Loss: 0.00001833
Iteration 161/1000 | Loss: 0.00001833
Iteration 162/1000 | Loss: 0.00001833
Iteration 163/1000 | Loss: 0.00001833
Iteration 164/1000 | Loss: 0.00001833
Iteration 165/1000 | Loss: 0.00001833
Iteration 166/1000 | Loss: 0.00001833
Iteration 167/1000 | Loss: 0.00001833
Iteration 168/1000 | Loss: 0.00001833
Iteration 169/1000 | Loss: 0.00001833
Iteration 170/1000 | Loss: 0.00001833
Iteration 171/1000 | Loss: 0.00001833
Iteration 172/1000 | Loss: 0.00001833
Iteration 173/1000 | Loss: 0.00001833
Iteration 174/1000 | Loss: 0.00001833
Iteration 175/1000 | Loss: 0.00001833
Iteration 176/1000 | Loss: 0.00001833
Iteration 177/1000 | Loss: 0.00001833
Iteration 178/1000 | Loss: 0.00001833
Iteration 179/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.8328490114072338e-05, 1.8328490114072338e-05, 1.8328490114072338e-05, 1.8328490114072338e-05, 1.8328490114072338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8328490114072338e-05

Optimization complete. Final v2v error: 3.640578269958496 mm

Highest mean error: 4.10247278213501 mm for frame 210

Lowest mean error: 3.400695562362671 mm for frame 61

Saving results

Total time: 46.14355134963989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985504
Iteration 2/25 | Loss: 0.00171179
Iteration 3/25 | Loss: 0.00131377
Iteration 4/25 | Loss: 0.00122877
Iteration 5/25 | Loss: 0.00119544
Iteration 6/25 | Loss: 0.00114688
Iteration 7/25 | Loss: 0.00112697
Iteration 8/25 | Loss: 0.00111965
Iteration 9/25 | Loss: 0.00110305
Iteration 10/25 | Loss: 0.00110077
Iteration 11/25 | Loss: 0.00110421
Iteration 12/25 | Loss: 0.00110232
Iteration 13/25 | Loss: 0.00110132
Iteration 14/25 | Loss: 0.00109882
Iteration 15/25 | Loss: 0.00109596
Iteration 16/25 | Loss: 0.00109549
Iteration 17/25 | Loss: 0.00109529
Iteration 18/25 | Loss: 0.00109522
Iteration 19/25 | Loss: 0.00109518
Iteration 20/25 | Loss: 0.00109518
Iteration 21/25 | Loss: 0.00109517
Iteration 22/25 | Loss: 0.00109517
Iteration 23/25 | Loss: 0.00109517
Iteration 24/25 | Loss: 0.00109517
Iteration 25/25 | Loss: 0.00109517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07190561
Iteration 2/25 | Loss: 0.00269198
Iteration 3/25 | Loss: 0.00244367
Iteration 4/25 | Loss: 0.00244367
Iteration 5/25 | Loss: 0.00244367
Iteration 6/25 | Loss: 0.00244367
Iteration 7/25 | Loss: 0.00244367
Iteration 8/25 | Loss: 0.00244367
Iteration 9/25 | Loss: 0.00244367
Iteration 10/25 | Loss: 0.00244367
Iteration 11/25 | Loss: 0.00244367
Iteration 12/25 | Loss: 0.00244367
Iteration 13/25 | Loss: 0.00244367
Iteration 14/25 | Loss: 0.00244367
Iteration 15/25 | Loss: 0.00244367
Iteration 16/25 | Loss: 0.00244367
Iteration 17/25 | Loss: 0.00244367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002443668432533741, 0.002443668432533741, 0.002443668432533741, 0.002443668432533741, 0.002443668432533741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002443668432533741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244367
Iteration 2/1000 | Loss: 0.00064798
Iteration 3/1000 | Loss: 0.00275683
Iteration 4/1000 | Loss: 0.00147805
Iteration 5/1000 | Loss: 0.00084661
Iteration 6/1000 | Loss: 0.00020579
Iteration 7/1000 | Loss: 0.00020460
Iteration 8/1000 | Loss: 0.00105539
Iteration 9/1000 | Loss: 0.00054723
Iteration 10/1000 | Loss: 0.00009560
Iteration 11/1000 | Loss: 0.00026213
Iteration 12/1000 | Loss: 0.00023471
Iteration 13/1000 | Loss: 0.00007907
Iteration 14/1000 | Loss: 0.00007319
Iteration 15/1000 | Loss: 0.00006936
Iteration 16/1000 | Loss: 0.00081710
Iteration 17/1000 | Loss: 0.00007256
Iteration 18/1000 | Loss: 0.00006330
Iteration 19/1000 | Loss: 0.00252546
Iteration 20/1000 | Loss: 0.00035105
Iteration 21/1000 | Loss: 0.00010354
Iteration 22/1000 | Loss: 0.00117242
Iteration 23/1000 | Loss: 0.00007345
Iteration 24/1000 | Loss: 0.00088874
Iteration 25/1000 | Loss: 0.00099576
Iteration 26/1000 | Loss: 0.00072491
Iteration 27/1000 | Loss: 0.00008199
Iteration 28/1000 | Loss: 0.00005175
Iteration 29/1000 | Loss: 0.00051727
Iteration 30/1000 | Loss: 0.00005794
Iteration 31/1000 | Loss: 0.00060934
Iteration 32/1000 | Loss: 0.00063456
Iteration 33/1000 | Loss: 0.00015728
Iteration 34/1000 | Loss: 0.00004289
Iteration 35/1000 | Loss: 0.00058815
Iteration 36/1000 | Loss: 0.00017304
Iteration 37/1000 | Loss: 0.00065216
Iteration 38/1000 | Loss: 0.00017376
Iteration 39/1000 | Loss: 0.00017711
Iteration 40/1000 | Loss: 0.00013111
Iteration 41/1000 | Loss: 0.00103529
Iteration 42/1000 | Loss: 0.00014378
Iteration 43/1000 | Loss: 0.00025569
Iteration 44/1000 | Loss: 0.00013727
Iteration 45/1000 | Loss: 0.00004370
Iteration 46/1000 | Loss: 0.00004010
Iteration 47/1000 | Loss: 0.00005487
Iteration 48/1000 | Loss: 0.00003795
Iteration 49/1000 | Loss: 0.00003500
Iteration 50/1000 | Loss: 0.00003391
Iteration 51/1000 | Loss: 0.00003350
Iteration 52/1000 | Loss: 0.00003323
Iteration 53/1000 | Loss: 0.00003301
Iteration 54/1000 | Loss: 0.00003270
Iteration 55/1000 | Loss: 0.00003251
Iteration 56/1000 | Loss: 0.00003246
Iteration 57/1000 | Loss: 0.00003238
Iteration 58/1000 | Loss: 0.00003236
Iteration 59/1000 | Loss: 0.00003235
Iteration 60/1000 | Loss: 0.00003235
Iteration 61/1000 | Loss: 0.00003234
Iteration 62/1000 | Loss: 0.00003234
Iteration 63/1000 | Loss: 0.00003234
Iteration 64/1000 | Loss: 0.00003233
Iteration 65/1000 | Loss: 0.00003233
Iteration 66/1000 | Loss: 0.00003233
Iteration 67/1000 | Loss: 0.00003232
Iteration 68/1000 | Loss: 0.00003230
Iteration 69/1000 | Loss: 0.00003230
Iteration 70/1000 | Loss: 0.00003229
Iteration 71/1000 | Loss: 0.00003229
Iteration 72/1000 | Loss: 0.00003228
Iteration 73/1000 | Loss: 0.00003228
Iteration 74/1000 | Loss: 0.00003227
Iteration 75/1000 | Loss: 0.00003225
Iteration 76/1000 | Loss: 0.00003225
Iteration 77/1000 | Loss: 0.00003224
Iteration 78/1000 | Loss: 0.00003221
Iteration 79/1000 | Loss: 0.00003220
Iteration 80/1000 | Loss: 0.00003215
Iteration 81/1000 | Loss: 0.00003215
Iteration 82/1000 | Loss: 0.00003213
Iteration 83/1000 | Loss: 0.00003213
Iteration 84/1000 | Loss: 0.00003210
Iteration 85/1000 | Loss: 0.00003210
Iteration 86/1000 | Loss: 0.00003208
Iteration 87/1000 | Loss: 0.00003208
Iteration 88/1000 | Loss: 0.00003207
Iteration 89/1000 | Loss: 0.00003207
Iteration 90/1000 | Loss: 0.00003207
Iteration 91/1000 | Loss: 0.00003206
Iteration 92/1000 | Loss: 0.00003206
Iteration 93/1000 | Loss: 0.00003206
Iteration 94/1000 | Loss: 0.00003206
Iteration 95/1000 | Loss: 0.00003205
Iteration 96/1000 | Loss: 0.00003205
Iteration 97/1000 | Loss: 0.00003204
Iteration 98/1000 | Loss: 0.00003204
Iteration 99/1000 | Loss: 0.00003203
Iteration 100/1000 | Loss: 0.00003203
Iteration 101/1000 | Loss: 0.00003203
Iteration 102/1000 | Loss: 0.00003202
Iteration 103/1000 | Loss: 0.00003202
Iteration 104/1000 | Loss: 0.00003202
Iteration 105/1000 | Loss: 0.00003202
Iteration 106/1000 | Loss: 0.00003202
Iteration 107/1000 | Loss: 0.00003201
Iteration 108/1000 | Loss: 0.00003201
Iteration 109/1000 | Loss: 0.00003201
Iteration 110/1000 | Loss: 0.00003201
Iteration 111/1000 | Loss: 0.00003201
Iteration 112/1000 | Loss: 0.00003201
Iteration 113/1000 | Loss: 0.00003200
Iteration 114/1000 | Loss: 0.00003200
Iteration 115/1000 | Loss: 0.00003200
Iteration 116/1000 | Loss: 0.00003200
Iteration 117/1000 | Loss: 0.00003200
Iteration 118/1000 | Loss: 0.00003199
Iteration 119/1000 | Loss: 0.00003199
Iteration 120/1000 | Loss: 0.00003199
Iteration 121/1000 | Loss: 0.00003198
Iteration 122/1000 | Loss: 0.00003198
Iteration 123/1000 | Loss: 0.00003197
Iteration 124/1000 | Loss: 0.00003197
Iteration 125/1000 | Loss: 0.00003197
Iteration 126/1000 | Loss: 0.00003197
Iteration 127/1000 | Loss: 0.00003196
Iteration 128/1000 | Loss: 0.00003196
Iteration 129/1000 | Loss: 0.00003196
Iteration 130/1000 | Loss: 0.00003196
Iteration 131/1000 | Loss: 0.00003196
Iteration 132/1000 | Loss: 0.00003196
Iteration 133/1000 | Loss: 0.00003195
Iteration 134/1000 | Loss: 0.00003195
Iteration 135/1000 | Loss: 0.00003195
Iteration 136/1000 | Loss: 0.00003195
Iteration 137/1000 | Loss: 0.00003195
Iteration 138/1000 | Loss: 0.00003195
Iteration 139/1000 | Loss: 0.00003195
Iteration 140/1000 | Loss: 0.00003194
Iteration 141/1000 | Loss: 0.00003194
Iteration 142/1000 | Loss: 0.00003194
Iteration 143/1000 | Loss: 0.00003194
Iteration 144/1000 | Loss: 0.00003194
Iteration 145/1000 | Loss: 0.00003194
Iteration 146/1000 | Loss: 0.00003194
Iteration 147/1000 | Loss: 0.00003194
Iteration 148/1000 | Loss: 0.00003194
Iteration 149/1000 | Loss: 0.00003194
Iteration 150/1000 | Loss: 0.00003194
Iteration 151/1000 | Loss: 0.00003194
Iteration 152/1000 | Loss: 0.00003194
Iteration 153/1000 | Loss: 0.00003194
Iteration 154/1000 | Loss: 0.00003194
Iteration 155/1000 | Loss: 0.00003194
Iteration 156/1000 | Loss: 0.00003194
Iteration 157/1000 | Loss: 0.00003194
Iteration 158/1000 | Loss: 0.00003194
Iteration 159/1000 | Loss: 0.00003194
Iteration 160/1000 | Loss: 0.00003194
Iteration 161/1000 | Loss: 0.00003194
Iteration 162/1000 | Loss: 0.00003194
Iteration 163/1000 | Loss: 0.00003194
Iteration 164/1000 | Loss: 0.00003194
Iteration 165/1000 | Loss: 0.00003194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [3.194027885911055e-05, 3.194027885911055e-05, 3.194027885911055e-05, 3.194027885911055e-05, 3.194027885911055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.194027885911055e-05

Optimization complete. Final v2v error: 4.53500509262085 mm

Highest mean error: 7.92502498626709 mm for frame 17

Lowest mean error: 3.273050546646118 mm for frame 207

Saving results

Total time: 140.0617105960846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888463
Iteration 2/25 | Loss: 0.00089851
Iteration 3/25 | Loss: 0.00077959
Iteration 4/25 | Loss: 0.00074726
Iteration 5/25 | Loss: 0.00073652
Iteration 6/25 | Loss: 0.00073437
Iteration 7/25 | Loss: 0.00073375
Iteration 8/25 | Loss: 0.00073375
Iteration 9/25 | Loss: 0.00073375
Iteration 10/25 | Loss: 0.00073375
Iteration 11/25 | Loss: 0.00073375
Iteration 12/25 | Loss: 0.00073375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007337526767514646, 0.0007337526767514646, 0.0007337526767514646, 0.0007337526767514646, 0.0007337526767514646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007337526767514646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13850188
Iteration 2/25 | Loss: 0.00109633
Iteration 3/25 | Loss: 0.00109633
Iteration 4/25 | Loss: 0.00109633
Iteration 5/25 | Loss: 0.00109633
Iteration 6/25 | Loss: 0.00109633
Iteration 7/25 | Loss: 0.00109633
Iteration 8/25 | Loss: 0.00109633
Iteration 9/25 | Loss: 0.00109633
Iteration 10/25 | Loss: 0.00109632
Iteration 11/25 | Loss: 0.00109632
Iteration 12/25 | Loss: 0.00109632
Iteration 13/25 | Loss: 0.00109632
Iteration 14/25 | Loss: 0.00109632
Iteration 15/25 | Loss: 0.00109632
Iteration 16/25 | Loss: 0.00109632
Iteration 17/25 | Loss: 0.00109632
Iteration 18/25 | Loss: 0.00109632
Iteration 19/25 | Loss: 0.00109632
Iteration 20/25 | Loss: 0.00109632
Iteration 21/25 | Loss: 0.00109632
Iteration 22/25 | Loss: 0.00109632
Iteration 23/25 | Loss: 0.00109632
Iteration 24/25 | Loss: 0.00109632
Iteration 25/25 | Loss: 0.00109632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109632
Iteration 2/1000 | Loss: 0.00002958
Iteration 3/1000 | Loss: 0.00002039
Iteration 4/1000 | Loss: 0.00001769
Iteration 5/1000 | Loss: 0.00001691
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001482
Iteration 11/1000 | Loss: 0.00001471
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001470
Iteration 14/1000 | Loss: 0.00001469
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001467
Iteration 18/1000 | Loss: 0.00001467
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001462
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001459
Iteration 30/1000 | Loss: 0.00001459
Iteration 31/1000 | Loss: 0.00001459
Iteration 32/1000 | Loss: 0.00001458
Iteration 33/1000 | Loss: 0.00001454
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001453
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001448
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001447
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001446
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001445
Iteration 68/1000 | Loss: 0.00001445
Iteration 69/1000 | Loss: 0.00001445
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001445
Iteration 74/1000 | Loss: 0.00001445
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001444
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001443
Iteration 84/1000 | Loss: 0.00001443
Iteration 85/1000 | Loss: 0.00001443
Iteration 86/1000 | Loss: 0.00001443
Iteration 87/1000 | Loss: 0.00001442
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001441
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Iteration 96/1000 | Loss: 0.00001441
Iteration 97/1000 | Loss: 0.00001441
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001439
Iteration 102/1000 | Loss: 0.00001439
Iteration 103/1000 | Loss: 0.00001439
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001438
Iteration 106/1000 | Loss: 0.00001438
Iteration 107/1000 | Loss: 0.00001437
Iteration 108/1000 | Loss: 0.00001437
Iteration 109/1000 | Loss: 0.00001437
Iteration 110/1000 | Loss: 0.00001436
Iteration 111/1000 | Loss: 0.00001436
Iteration 112/1000 | Loss: 0.00001436
Iteration 113/1000 | Loss: 0.00001435
Iteration 114/1000 | Loss: 0.00001435
Iteration 115/1000 | Loss: 0.00001435
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001433
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001430
Iteration 129/1000 | Loss: 0.00001430
Iteration 130/1000 | Loss: 0.00001430
Iteration 131/1000 | Loss: 0.00001430
Iteration 132/1000 | Loss: 0.00001430
Iteration 133/1000 | Loss: 0.00001430
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001429
Iteration 141/1000 | Loss: 0.00001429
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001428
Iteration 146/1000 | Loss: 0.00001428
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001427
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001427
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001427
Iteration 162/1000 | Loss: 0.00001427
Iteration 163/1000 | Loss: 0.00001427
Iteration 164/1000 | Loss: 0.00001427
Iteration 165/1000 | Loss: 0.00001427
Iteration 166/1000 | Loss: 0.00001427
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4267079677665606e-05, 1.4267079677665606e-05, 1.4267079677665606e-05, 1.4267079677665606e-05, 1.4267079677665606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4267079677665606e-05

Optimization complete. Final v2v error: 3.216325283050537 mm

Highest mean error: 3.9639341831207275 mm for frame 60

Lowest mean error: 2.9787793159484863 mm for frame 99

Saving results

Total time: 37.44120001792908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401327
Iteration 2/25 | Loss: 0.00084848
Iteration 3/25 | Loss: 0.00075942
Iteration 4/25 | Loss: 0.00074217
Iteration 5/25 | Loss: 0.00073728
Iteration 6/25 | Loss: 0.00073597
Iteration 7/25 | Loss: 0.00073597
Iteration 8/25 | Loss: 0.00073597
Iteration 9/25 | Loss: 0.00073595
Iteration 10/25 | Loss: 0.00073595
Iteration 11/25 | Loss: 0.00073595
Iteration 12/25 | Loss: 0.00073595
Iteration 13/25 | Loss: 0.00073595
Iteration 14/25 | Loss: 0.00073595
Iteration 15/25 | Loss: 0.00073595
Iteration 16/25 | Loss: 0.00073595
Iteration 17/25 | Loss: 0.00073595
Iteration 18/25 | Loss: 0.00073595
Iteration 19/25 | Loss: 0.00073595
Iteration 20/25 | Loss: 0.00073595
Iteration 21/25 | Loss: 0.00073595
Iteration 22/25 | Loss: 0.00073595
Iteration 23/25 | Loss: 0.00073595
Iteration 24/25 | Loss: 0.00073595
Iteration 25/25 | Loss: 0.00073595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17963886
Iteration 2/25 | Loss: 0.00117974
Iteration 3/25 | Loss: 0.00117974
Iteration 4/25 | Loss: 0.00117974
Iteration 5/25 | Loss: 0.00117974
Iteration 6/25 | Loss: 0.00117974
Iteration 7/25 | Loss: 0.00117974
Iteration 8/25 | Loss: 0.00117974
Iteration 9/25 | Loss: 0.00117974
Iteration 10/25 | Loss: 0.00117974
Iteration 11/25 | Loss: 0.00117974
Iteration 12/25 | Loss: 0.00117974
Iteration 13/25 | Loss: 0.00117974
Iteration 14/25 | Loss: 0.00117974
Iteration 15/25 | Loss: 0.00117974
Iteration 16/25 | Loss: 0.00117974
Iteration 17/25 | Loss: 0.00117974
Iteration 18/25 | Loss: 0.00117974
Iteration 19/25 | Loss: 0.00117974
Iteration 20/25 | Loss: 0.00117974
Iteration 21/25 | Loss: 0.00117974
Iteration 22/25 | Loss: 0.00117974
Iteration 23/25 | Loss: 0.00117974
Iteration 24/25 | Loss: 0.00117974
Iteration 25/25 | Loss: 0.00117974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117974
Iteration 2/1000 | Loss: 0.00002134
Iteration 3/1000 | Loss: 0.00001737
Iteration 4/1000 | Loss: 0.00001634
Iteration 5/1000 | Loss: 0.00001535
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001465
Iteration 8/1000 | Loss: 0.00001441
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001406
Iteration 12/1000 | Loss: 0.00001403
Iteration 13/1000 | Loss: 0.00001399
Iteration 14/1000 | Loss: 0.00001399
Iteration 15/1000 | Loss: 0.00001398
Iteration 16/1000 | Loss: 0.00001389
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001388
Iteration 20/1000 | Loss: 0.00001388
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001386
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001386
Iteration 25/1000 | Loss: 0.00001385
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001385
Iteration 28/1000 | Loss: 0.00001385
Iteration 29/1000 | Loss: 0.00001385
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001384
Iteration 32/1000 | Loss: 0.00001384
Iteration 33/1000 | Loss: 0.00001384
Iteration 34/1000 | Loss: 0.00001384
Iteration 35/1000 | Loss: 0.00001383
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001383
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001381
Iteration 45/1000 | Loss: 0.00001380
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001379
Iteration 48/1000 | Loss: 0.00001379
Iteration 49/1000 | Loss: 0.00001379
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001379
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001378
Iteration 55/1000 | Loss: 0.00001378
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001374
Iteration 62/1000 | Loss: 0.00001374
Iteration 63/1000 | Loss: 0.00001374
Iteration 64/1000 | Loss: 0.00001374
Iteration 65/1000 | Loss: 0.00001373
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001370
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001368
Iteration 73/1000 | Loss: 0.00001367
Iteration 74/1000 | Loss: 0.00001367
Iteration 75/1000 | Loss: 0.00001367
Iteration 76/1000 | Loss: 0.00001367
Iteration 77/1000 | Loss: 0.00001367
Iteration 78/1000 | Loss: 0.00001367
Iteration 79/1000 | Loss: 0.00001367
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001367
Iteration 82/1000 | Loss: 0.00001366
Iteration 83/1000 | Loss: 0.00001366
Iteration 84/1000 | Loss: 0.00001366
Iteration 85/1000 | Loss: 0.00001366
Iteration 86/1000 | Loss: 0.00001366
Iteration 87/1000 | Loss: 0.00001366
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001365
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001365
Iteration 100/1000 | Loss: 0.00001365
Iteration 101/1000 | Loss: 0.00001365
Iteration 102/1000 | Loss: 0.00001365
Iteration 103/1000 | Loss: 0.00001365
Iteration 104/1000 | Loss: 0.00001365
Iteration 105/1000 | Loss: 0.00001365
Iteration 106/1000 | Loss: 0.00001365
Iteration 107/1000 | Loss: 0.00001365
Iteration 108/1000 | Loss: 0.00001365
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001365
Iteration 112/1000 | Loss: 0.00001365
Iteration 113/1000 | Loss: 0.00001365
Iteration 114/1000 | Loss: 0.00001365
Iteration 115/1000 | Loss: 0.00001365
Iteration 116/1000 | Loss: 0.00001365
Iteration 117/1000 | Loss: 0.00001365
Iteration 118/1000 | Loss: 0.00001365
Iteration 119/1000 | Loss: 0.00001365
Iteration 120/1000 | Loss: 0.00001365
Iteration 121/1000 | Loss: 0.00001365
Iteration 122/1000 | Loss: 0.00001365
Iteration 123/1000 | Loss: 0.00001365
Iteration 124/1000 | Loss: 0.00001365
Iteration 125/1000 | Loss: 0.00001365
Iteration 126/1000 | Loss: 0.00001365
Iteration 127/1000 | Loss: 0.00001365
Iteration 128/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.3646179468196351e-05, 1.3646179468196351e-05, 1.3646179468196351e-05, 1.3646179468196351e-05, 1.3646179468196351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3646179468196351e-05

Optimization complete. Final v2v error: 3.14398193359375 mm

Highest mean error: 3.399428367614746 mm for frame 139

Lowest mean error: 3.0413169860839844 mm for frame 127

Saving results

Total time: 38.09043097496033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403514
Iteration 2/25 | Loss: 0.00096885
Iteration 3/25 | Loss: 0.00078896
Iteration 4/25 | Loss: 0.00075931
Iteration 5/25 | Loss: 0.00075112
Iteration 6/25 | Loss: 0.00074822
Iteration 7/25 | Loss: 0.00074733
Iteration 8/25 | Loss: 0.00074729
Iteration 9/25 | Loss: 0.00074729
Iteration 10/25 | Loss: 0.00074729
Iteration 11/25 | Loss: 0.00074729
Iteration 12/25 | Loss: 0.00074729
Iteration 13/25 | Loss: 0.00074729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007472944562323391, 0.0007472944562323391, 0.0007472944562323391, 0.0007472944562323391, 0.0007472944562323391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007472944562323391

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57702971
Iteration 2/25 | Loss: 0.00114649
Iteration 3/25 | Loss: 0.00114649
Iteration 4/25 | Loss: 0.00114649
Iteration 5/25 | Loss: 0.00114649
Iteration 6/25 | Loss: 0.00114649
Iteration 7/25 | Loss: 0.00114648
Iteration 8/25 | Loss: 0.00114648
Iteration 9/25 | Loss: 0.00114648
Iteration 10/25 | Loss: 0.00114648
Iteration 11/25 | Loss: 0.00114648
Iteration 12/25 | Loss: 0.00114648
Iteration 13/25 | Loss: 0.00114648
Iteration 14/25 | Loss: 0.00114648
Iteration 15/25 | Loss: 0.00114648
Iteration 16/25 | Loss: 0.00114648
Iteration 17/25 | Loss: 0.00114648
Iteration 18/25 | Loss: 0.00114648
Iteration 19/25 | Loss: 0.00114648
Iteration 20/25 | Loss: 0.00114648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011464833514764905, 0.0011464833514764905, 0.0011464833514764905, 0.0011464833514764905, 0.0011464833514764905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011464833514764905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114648
Iteration 2/1000 | Loss: 0.00002490
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001740
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001557
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001531
Iteration 10/1000 | Loss: 0.00001512
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001507
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001504
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001502
Iteration 20/1000 | Loss: 0.00001494
Iteration 21/1000 | Loss: 0.00001494
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001490
Iteration 27/1000 | Loss: 0.00001490
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001488
Iteration 30/1000 | Loss: 0.00001488
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001488
Iteration 34/1000 | Loss: 0.00001488
Iteration 35/1000 | Loss: 0.00001488
Iteration 36/1000 | Loss: 0.00001488
Iteration 37/1000 | Loss: 0.00001488
Iteration 38/1000 | Loss: 0.00001487
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001485
Iteration 54/1000 | Loss: 0.00001485
Iteration 55/1000 | Loss: 0.00001485
Iteration 56/1000 | Loss: 0.00001485
Iteration 57/1000 | Loss: 0.00001485
Iteration 58/1000 | Loss: 0.00001484
Iteration 59/1000 | Loss: 0.00001484
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001484
Iteration 62/1000 | Loss: 0.00001484
Iteration 63/1000 | Loss: 0.00001484
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001483
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001478
Iteration 80/1000 | Loss: 0.00001478
Iteration 81/1000 | Loss: 0.00001478
Iteration 82/1000 | Loss: 0.00001478
Iteration 83/1000 | Loss: 0.00001477
Iteration 84/1000 | Loss: 0.00001477
Iteration 85/1000 | Loss: 0.00001477
Iteration 86/1000 | Loss: 0.00001477
Iteration 87/1000 | Loss: 0.00001477
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001475
Iteration 94/1000 | Loss: 0.00001475
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001473
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001473
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001472
Iteration 112/1000 | Loss: 0.00001472
Iteration 113/1000 | Loss: 0.00001472
Iteration 114/1000 | Loss: 0.00001472
Iteration 115/1000 | Loss: 0.00001472
Iteration 116/1000 | Loss: 0.00001472
Iteration 117/1000 | Loss: 0.00001472
Iteration 118/1000 | Loss: 0.00001472
Iteration 119/1000 | Loss: 0.00001472
Iteration 120/1000 | Loss: 0.00001472
Iteration 121/1000 | Loss: 0.00001472
Iteration 122/1000 | Loss: 0.00001472
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001472
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001472
Iteration 133/1000 | Loss: 0.00001472
Iteration 134/1000 | Loss: 0.00001472
Iteration 135/1000 | Loss: 0.00001472
Iteration 136/1000 | Loss: 0.00001472
Iteration 137/1000 | Loss: 0.00001472
Iteration 138/1000 | Loss: 0.00001472
Iteration 139/1000 | Loss: 0.00001472
Iteration 140/1000 | Loss: 0.00001472
Iteration 141/1000 | Loss: 0.00001472
Iteration 142/1000 | Loss: 0.00001472
Iteration 143/1000 | Loss: 0.00001472
Iteration 144/1000 | Loss: 0.00001472
Iteration 145/1000 | Loss: 0.00001472
Iteration 146/1000 | Loss: 0.00001472
Iteration 147/1000 | Loss: 0.00001472
Iteration 148/1000 | Loss: 0.00001472
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001472
Iteration 151/1000 | Loss: 0.00001472
Iteration 152/1000 | Loss: 0.00001472
Iteration 153/1000 | Loss: 0.00001472
Iteration 154/1000 | Loss: 0.00001472
Iteration 155/1000 | Loss: 0.00001472
Iteration 156/1000 | Loss: 0.00001472
Iteration 157/1000 | Loss: 0.00001472
Iteration 158/1000 | Loss: 0.00001472
Iteration 159/1000 | Loss: 0.00001472
Iteration 160/1000 | Loss: 0.00001472
Iteration 161/1000 | Loss: 0.00001472
Iteration 162/1000 | Loss: 0.00001472
Iteration 163/1000 | Loss: 0.00001472
Iteration 164/1000 | Loss: 0.00001472
Iteration 165/1000 | Loss: 0.00001472
Iteration 166/1000 | Loss: 0.00001472
Iteration 167/1000 | Loss: 0.00001472
Iteration 168/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.4717372323502786e-05, 1.4717372323502786e-05, 1.4717372323502786e-05, 1.4717372323502786e-05, 1.4717372323502786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4717372323502786e-05

Optimization complete. Final v2v error: 3.231224775314331 mm

Highest mean error: 3.528196334838867 mm for frame 111

Lowest mean error: 2.950105905532837 mm for frame 3

Saving results

Total time: 36.578917264938354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429793
Iteration 2/25 | Loss: 0.00097546
Iteration 3/25 | Loss: 0.00079998
Iteration 4/25 | Loss: 0.00077508
Iteration 5/25 | Loss: 0.00076952
Iteration 6/25 | Loss: 0.00076834
Iteration 7/25 | Loss: 0.00076792
Iteration 8/25 | Loss: 0.00076792
Iteration 9/25 | Loss: 0.00076792
Iteration 10/25 | Loss: 0.00076792
Iteration 11/25 | Loss: 0.00076792
Iteration 12/25 | Loss: 0.00076792
Iteration 13/25 | Loss: 0.00076792
Iteration 14/25 | Loss: 0.00076792
Iteration 15/25 | Loss: 0.00076792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007679218542762101, 0.0007679218542762101, 0.0007679218542762101, 0.0007679218542762101, 0.0007679218542762101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007679218542762101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.81870317
Iteration 2/25 | Loss: 0.00123028
Iteration 3/25 | Loss: 0.00123027
Iteration 4/25 | Loss: 0.00123027
Iteration 5/25 | Loss: 0.00123027
Iteration 6/25 | Loss: 0.00123027
Iteration 7/25 | Loss: 0.00123027
Iteration 8/25 | Loss: 0.00123027
Iteration 9/25 | Loss: 0.00123027
Iteration 10/25 | Loss: 0.00123027
Iteration 11/25 | Loss: 0.00123027
Iteration 12/25 | Loss: 0.00123027
Iteration 13/25 | Loss: 0.00123027
Iteration 14/25 | Loss: 0.00123027
Iteration 15/25 | Loss: 0.00123027
Iteration 16/25 | Loss: 0.00123027
Iteration 17/25 | Loss: 0.00123027
Iteration 18/25 | Loss: 0.00123027
Iteration 19/25 | Loss: 0.00123027
Iteration 20/25 | Loss: 0.00123027
Iteration 21/25 | Loss: 0.00123027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012302710674703121, 0.0012302710674703121, 0.0012302710674703121, 0.0012302710674703121, 0.0012302710674703121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012302710674703121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123027
Iteration 2/1000 | Loss: 0.00002567
Iteration 3/1000 | Loss: 0.00001846
Iteration 4/1000 | Loss: 0.00001714
Iteration 5/1000 | Loss: 0.00001637
Iteration 6/1000 | Loss: 0.00001580
Iteration 7/1000 | Loss: 0.00001547
Iteration 8/1000 | Loss: 0.00001524
Iteration 9/1000 | Loss: 0.00001505
Iteration 10/1000 | Loss: 0.00001502
Iteration 11/1000 | Loss: 0.00001497
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001482
Iteration 17/1000 | Loss: 0.00001481
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001475
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001470
Iteration 33/1000 | Loss: 0.00001470
Iteration 34/1000 | Loss: 0.00001470
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001467
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001465
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001462
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001462
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001462
Iteration 74/1000 | Loss: 0.00001462
Iteration 75/1000 | Loss: 0.00001462
Iteration 76/1000 | Loss: 0.00001462
Iteration 77/1000 | Loss: 0.00001462
Iteration 78/1000 | Loss: 0.00001462
Iteration 79/1000 | Loss: 0.00001462
Iteration 80/1000 | Loss: 0.00001462
Iteration 81/1000 | Loss: 0.00001462
Iteration 82/1000 | Loss: 0.00001462
Iteration 83/1000 | Loss: 0.00001462
Iteration 84/1000 | Loss: 0.00001462
Iteration 85/1000 | Loss: 0.00001462
Iteration 86/1000 | Loss: 0.00001462
Iteration 87/1000 | Loss: 0.00001462
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001462
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001462
Iteration 96/1000 | Loss: 0.00001462
Iteration 97/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.4617218766943552e-05, 1.4617218766943552e-05, 1.4617218766943552e-05, 1.4617218766943552e-05, 1.4617218766943552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4617218766943552e-05

Optimization complete. Final v2v error: 3.2708826065063477 mm

Highest mean error: 3.771134614944458 mm for frame 70

Lowest mean error: 2.9708242416381836 mm for frame 33

Saving results

Total time: 30.731539249420166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041645
Iteration 2/25 | Loss: 0.00225181
Iteration 3/25 | Loss: 0.00146093
Iteration 4/25 | Loss: 0.00121158
Iteration 5/25 | Loss: 0.00113207
Iteration 6/25 | Loss: 0.00126038
Iteration 7/25 | Loss: 0.00118421
Iteration 8/25 | Loss: 0.00104805
Iteration 9/25 | Loss: 0.00094540
Iteration 10/25 | Loss: 0.00090199
Iteration 11/25 | Loss: 0.00086697
Iteration 12/25 | Loss: 0.00085782
Iteration 13/25 | Loss: 0.00084048
Iteration 14/25 | Loss: 0.00083716
Iteration 15/25 | Loss: 0.00082997
Iteration 16/25 | Loss: 0.00084051
Iteration 17/25 | Loss: 0.00083072
Iteration 18/25 | Loss: 0.00081824
Iteration 19/25 | Loss: 0.00081914
Iteration 20/25 | Loss: 0.00082256
Iteration 21/25 | Loss: 0.00081931
Iteration 22/25 | Loss: 0.00082097
Iteration 23/25 | Loss: 0.00082287
Iteration 24/25 | Loss: 0.00081801
Iteration 25/25 | Loss: 0.00081911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64397657
Iteration 2/25 | Loss: 0.00158419
Iteration 3/25 | Loss: 0.00158419
Iteration 4/25 | Loss: 0.00158419
Iteration 5/25 | Loss: 0.00158419
Iteration 6/25 | Loss: 0.00158419
Iteration 7/25 | Loss: 0.00158419
Iteration 8/25 | Loss: 0.00158419
Iteration 9/25 | Loss: 0.00158419
Iteration 10/25 | Loss: 0.00158419
Iteration 11/25 | Loss: 0.00158419
Iteration 12/25 | Loss: 0.00158418
Iteration 13/25 | Loss: 0.00158418
Iteration 14/25 | Loss: 0.00158418
Iteration 15/25 | Loss: 0.00158418
Iteration 16/25 | Loss: 0.00158418
Iteration 17/25 | Loss: 0.00158418
Iteration 18/25 | Loss: 0.00158418
Iteration 19/25 | Loss: 0.00158418
Iteration 20/25 | Loss: 0.00158418
Iteration 21/25 | Loss: 0.00158418
Iteration 22/25 | Loss: 0.00158418
Iteration 23/25 | Loss: 0.00158418
Iteration 24/25 | Loss: 0.00158418
Iteration 25/25 | Loss: 0.00158418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158418
Iteration 2/1000 | Loss: 0.00005666
Iteration 3/1000 | Loss: 0.00013252
Iteration 4/1000 | Loss: 0.00047965
Iteration 5/1000 | Loss: 0.00027021
Iteration 6/1000 | Loss: 0.00015837
Iteration 7/1000 | Loss: 0.00027788
Iteration 8/1000 | Loss: 0.00072721
Iteration 9/1000 | Loss: 0.00031647
Iteration 10/1000 | Loss: 0.00026388
Iteration 11/1000 | Loss: 0.00026338
Iteration 12/1000 | Loss: 0.00020454
Iteration 13/1000 | Loss: 0.00003415
Iteration 14/1000 | Loss: 0.00026089
Iteration 15/1000 | Loss: 0.00034101
Iteration 16/1000 | Loss: 0.00015487
Iteration 17/1000 | Loss: 0.00002974
Iteration 18/1000 | Loss: 0.00005857
Iteration 19/1000 | Loss: 0.00020073
Iteration 20/1000 | Loss: 0.00007085
Iteration 21/1000 | Loss: 0.00094678
Iteration 22/1000 | Loss: 0.00005924
Iteration 23/1000 | Loss: 0.00016607
Iteration 24/1000 | Loss: 0.00004717
Iteration 25/1000 | Loss: 0.00002820
Iteration 26/1000 | Loss: 0.00002488
Iteration 27/1000 | Loss: 0.00003457
Iteration 28/1000 | Loss: 0.00001837
Iteration 29/1000 | Loss: 0.00002700
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001667
Iteration 32/1000 | Loss: 0.00003379
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00002060
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001557
Iteration 39/1000 | Loss: 0.00004251
Iteration 40/1000 | Loss: 0.00001568
Iteration 41/1000 | Loss: 0.00001538
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001535
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001507
Iteration 50/1000 | Loss: 0.00001507
Iteration 51/1000 | Loss: 0.00001505
Iteration 52/1000 | Loss: 0.00001504
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00006593
Iteration 55/1000 | Loss: 0.00006593
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001549
Iteration 58/1000 | Loss: 0.00001491
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001486
Iteration 63/1000 | Loss: 0.00001486
Iteration 64/1000 | Loss: 0.00001486
Iteration 65/1000 | Loss: 0.00001486
Iteration 66/1000 | Loss: 0.00001486
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001486
Iteration 71/1000 | Loss: 0.00001486
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001485
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001484
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001484
Iteration 84/1000 | Loss: 0.00001484
Iteration 85/1000 | Loss: 0.00001484
Iteration 86/1000 | Loss: 0.00001484
Iteration 87/1000 | Loss: 0.00001484
Iteration 88/1000 | Loss: 0.00001484
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001484
Iteration 94/1000 | Loss: 0.00001484
Iteration 95/1000 | Loss: 0.00001484
Iteration 96/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.4839769391983282e-05, 1.4839769391983282e-05, 1.4839769391983282e-05, 1.4839769391983282e-05, 1.4839769391983282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4839769391983282e-05

Optimization complete. Final v2v error: 3.203268527984619 mm

Highest mean error: 3.925791025161743 mm for frame 35

Lowest mean error: 2.6362476348876953 mm for frame 148

Saving results

Total time: 114.96806311607361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988860
Iteration 2/25 | Loss: 0.00140137
Iteration 3/25 | Loss: 0.00095047
Iteration 4/25 | Loss: 0.00086447
Iteration 5/25 | Loss: 0.00084231
Iteration 6/25 | Loss: 0.00086530
Iteration 7/25 | Loss: 0.00083499
Iteration 8/25 | Loss: 0.00082117
Iteration 9/25 | Loss: 0.00081145
Iteration 10/25 | Loss: 0.00081323
Iteration 11/25 | Loss: 0.00081475
Iteration 12/25 | Loss: 0.00081523
Iteration 13/25 | Loss: 0.00080122
Iteration 14/25 | Loss: 0.00079242
Iteration 15/25 | Loss: 0.00078901
Iteration 16/25 | Loss: 0.00078852
Iteration 17/25 | Loss: 0.00078833
Iteration 18/25 | Loss: 0.00078833
Iteration 19/25 | Loss: 0.00078833
Iteration 20/25 | Loss: 0.00078833
Iteration 21/25 | Loss: 0.00078832
Iteration 22/25 | Loss: 0.00078832
Iteration 23/25 | Loss: 0.00078832
Iteration 24/25 | Loss: 0.00078832
Iteration 25/25 | Loss: 0.00078832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.11952066
Iteration 2/25 | Loss: 0.00124516
Iteration 3/25 | Loss: 0.00124513
Iteration 4/25 | Loss: 0.00124513
Iteration 5/25 | Loss: 0.00124513
Iteration 6/25 | Loss: 0.00124513
Iteration 7/25 | Loss: 0.00124513
Iteration 8/25 | Loss: 0.00124513
Iteration 9/25 | Loss: 0.00124513
Iteration 10/25 | Loss: 0.00124513
Iteration 11/25 | Loss: 0.00124513
Iteration 12/25 | Loss: 0.00124513
Iteration 13/25 | Loss: 0.00124513
Iteration 14/25 | Loss: 0.00124513
Iteration 15/25 | Loss: 0.00124513
Iteration 16/25 | Loss: 0.00124513
Iteration 17/25 | Loss: 0.00124513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012451279908418655, 0.0012451279908418655, 0.0012451279908418655, 0.0012451279908418655, 0.0012451279908418655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012451279908418655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124513
Iteration 2/1000 | Loss: 0.00003008
Iteration 3/1000 | Loss: 0.00001946
Iteration 4/1000 | Loss: 0.00001783
Iteration 5/1000 | Loss: 0.00001684
Iteration 6/1000 | Loss: 0.00001622
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001555
Iteration 9/1000 | Loss: 0.00001548
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001531
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001516
Iteration 14/1000 | Loss: 0.00001509
Iteration 15/1000 | Loss: 0.00001500
Iteration 16/1000 | Loss: 0.00001496
Iteration 17/1000 | Loss: 0.00001495
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001494
Iteration 20/1000 | Loss: 0.00001493
Iteration 21/1000 | Loss: 0.00001492
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001490
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001489
Iteration 27/1000 | Loss: 0.00001489
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001488
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001486
Iteration 33/1000 | Loss: 0.00001486
Iteration 34/1000 | Loss: 0.00001486
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001485
Iteration 44/1000 | Loss: 0.00001485
Iteration 45/1000 | Loss: 0.00001485
Iteration 46/1000 | Loss: 0.00001485
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001483
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001483
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001482
Iteration 57/1000 | Loss: 0.00001482
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001480
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001476
Iteration 74/1000 | Loss: 0.00001476
Iteration 75/1000 | Loss: 0.00001476
Iteration 76/1000 | Loss: 0.00001475
Iteration 77/1000 | Loss: 0.00001475
Iteration 78/1000 | Loss: 0.00001475
Iteration 79/1000 | Loss: 0.00001475
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001474
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001473
Iteration 89/1000 | Loss: 0.00001473
Iteration 90/1000 | Loss: 0.00001473
Iteration 91/1000 | Loss: 0.00001473
Iteration 92/1000 | Loss: 0.00001473
Iteration 93/1000 | Loss: 0.00001473
Iteration 94/1000 | Loss: 0.00001473
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001472
Iteration 97/1000 | Loss: 0.00001472
Iteration 98/1000 | Loss: 0.00001472
Iteration 99/1000 | Loss: 0.00001472
Iteration 100/1000 | Loss: 0.00001472
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00001471
Iteration 103/1000 | Loss: 0.00001471
Iteration 104/1000 | Loss: 0.00001471
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001469
Iteration 110/1000 | Loss: 0.00001469
Iteration 111/1000 | Loss: 0.00001469
Iteration 112/1000 | Loss: 0.00001469
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001468
Iteration 119/1000 | Loss: 0.00001468
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001466
Iteration 126/1000 | Loss: 0.00001466
Iteration 127/1000 | Loss: 0.00001466
Iteration 128/1000 | Loss: 0.00001466
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001465
Iteration 138/1000 | Loss: 0.00001465
Iteration 139/1000 | Loss: 0.00001465
Iteration 140/1000 | Loss: 0.00001465
Iteration 141/1000 | Loss: 0.00001465
Iteration 142/1000 | Loss: 0.00001465
Iteration 143/1000 | Loss: 0.00001465
Iteration 144/1000 | Loss: 0.00001465
Iteration 145/1000 | Loss: 0.00001465
Iteration 146/1000 | Loss: 0.00001465
Iteration 147/1000 | Loss: 0.00001465
Iteration 148/1000 | Loss: 0.00001465
Iteration 149/1000 | Loss: 0.00001465
Iteration 150/1000 | Loss: 0.00001465
Iteration 151/1000 | Loss: 0.00001465
Iteration 152/1000 | Loss: 0.00001465
Iteration 153/1000 | Loss: 0.00001465
Iteration 154/1000 | Loss: 0.00001464
Iteration 155/1000 | Loss: 0.00001464
Iteration 156/1000 | Loss: 0.00001464
Iteration 157/1000 | Loss: 0.00001464
Iteration 158/1000 | Loss: 0.00001464
Iteration 159/1000 | Loss: 0.00001464
Iteration 160/1000 | Loss: 0.00001464
Iteration 161/1000 | Loss: 0.00001464
Iteration 162/1000 | Loss: 0.00001464
Iteration 163/1000 | Loss: 0.00001464
Iteration 164/1000 | Loss: 0.00001464
Iteration 165/1000 | Loss: 0.00001464
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001463
Iteration 169/1000 | Loss: 0.00001463
Iteration 170/1000 | Loss: 0.00001463
Iteration 171/1000 | Loss: 0.00001463
Iteration 172/1000 | Loss: 0.00001463
Iteration 173/1000 | Loss: 0.00001463
Iteration 174/1000 | Loss: 0.00001463
Iteration 175/1000 | Loss: 0.00001463
Iteration 176/1000 | Loss: 0.00001463
Iteration 177/1000 | Loss: 0.00001462
Iteration 178/1000 | Loss: 0.00001462
Iteration 179/1000 | Loss: 0.00001462
Iteration 180/1000 | Loss: 0.00001462
Iteration 181/1000 | Loss: 0.00001462
Iteration 182/1000 | Loss: 0.00001462
Iteration 183/1000 | Loss: 0.00001462
Iteration 184/1000 | Loss: 0.00001462
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001461
Iteration 187/1000 | Loss: 0.00001461
Iteration 188/1000 | Loss: 0.00001461
Iteration 189/1000 | Loss: 0.00001461
Iteration 190/1000 | Loss: 0.00001461
Iteration 191/1000 | Loss: 0.00001461
Iteration 192/1000 | Loss: 0.00001461
Iteration 193/1000 | Loss: 0.00001461
Iteration 194/1000 | Loss: 0.00001461
Iteration 195/1000 | Loss: 0.00001461
Iteration 196/1000 | Loss: 0.00001461
Iteration 197/1000 | Loss: 0.00001460
Iteration 198/1000 | Loss: 0.00001460
Iteration 199/1000 | Loss: 0.00001460
Iteration 200/1000 | Loss: 0.00001460
Iteration 201/1000 | Loss: 0.00001460
Iteration 202/1000 | Loss: 0.00001460
Iteration 203/1000 | Loss: 0.00001460
Iteration 204/1000 | Loss: 0.00001460
Iteration 205/1000 | Loss: 0.00001460
Iteration 206/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.4604564057663083e-05, 1.4604564057663083e-05, 1.4604564057663083e-05, 1.4604564057663083e-05, 1.4604564057663083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4604564057663083e-05

Optimization complete. Final v2v error: 3.242473602294922 mm

Highest mean error: 3.9757676124572754 mm for frame 83

Lowest mean error: 2.8059184551239014 mm for frame 29

Saving results

Total time: 58.142902135849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076086
Iteration 2/25 | Loss: 0.00153961
Iteration 3/25 | Loss: 0.00114649
Iteration 4/25 | Loss: 0.00098923
Iteration 5/25 | Loss: 0.00088950
Iteration 6/25 | Loss: 0.00080798
Iteration 7/25 | Loss: 0.00078734
Iteration 8/25 | Loss: 0.00077782
Iteration 9/25 | Loss: 0.00077888
Iteration 10/25 | Loss: 0.00077470
Iteration 11/25 | Loss: 0.00076577
Iteration 12/25 | Loss: 0.00075841
Iteration 13/25 | Loss: 0.00075179
Iteration 14/25 | Loss: 0.00074916
Iteration 15/25 | Loss: 0.00074947
Iteration 16/25 | Loss: 0.00074910
Iteration 17/25 | Loss: 0.00075012
Iteration 18/25 | Loss: 0.00074793
Iteration 19/25 | Loss: 0.00074793
Iteration 20/25 | Loss: 0.00074793
Iteration 21/25 | Loss: 0.00074793
Iteration 22/25 | Loss: 0.00074793
Iteration 23/25 | Loss: 0.00074793
Iteration 24/25 | Loss: 0.00074793
Iteration 25/25 | Loss: 0.00074793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62316954
Iteration 2/25 | Loss: 0.00123561
Iteration 3/25 | Loss: 0.00123561
Iteration 4/25 | Loss: 0.00123561
Iteration 5/25 | Loss: 0.00122197
Iteration 6/25 | Loss: 0.00122197
Iteration 7/25 | Loss: 0.00122197
Iteration 8/25 | Loss: 0.00122196
Iteration 9/25 | Loss: 0.00122196
Iteration 10/25 | Loss: 0.00122196
Iteration 11/25 | Loss: 0.00122196
Iteration 12/25 | Loss: 0.00122196
Iteration 13/25 | Loss: 0.00122196
Iteration 14/25 | Loss: 0.00122196
Iteration 15/25 | Loss: 0.00122196
Iteration 16/25 | Loss: 0.00122196
Iteration 17/25 | Loss: 0.00122196
Iteration 18/25 | Loss: 0.00122196
Iteration 19/25 | Loss: 0.00122196
Iteration 20/25 | Loss: 0.00122196
Iteration 21/25 | Loss: 0.00122196
Iteration 22/25 | Loss: 0.00122196
Iteration 23/25 | Loss: 0.00122196
Iteration 24/25 | Loss: 0.00122196
Iteration 25/25 | Loss: 0.00122196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122196
Iteration 2/1000 | Loss: 0.00003932
Iteration 3/1000 | Loss: 0.00009215
Iteration 4/1000 | Loss: 0.00001805
Iteration 5/1000 | Loss: 0.00001541
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001395
Iteration 9/1000 | Loss: 0.00006364
Iteration 10/1000 | Loss: 0.00001567
Iteration 11/1000 | Loss: 0.00003147
Iteration 12/1000 | Loss: 0.00009633
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001358
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001337
Iteration 17/1000 | Loss: 0.00001336
Iteration 18/1000 | Loss: 0.00001335
Iteration 19/1000 | Loss: 0.00001334
Iteration 20/1000 | Loss: 0.00004663
Iteration 21/1000 | Loss: 0.00002902
Iteration 22/1000 | Loss: 0.00001343
Iteration 23/1000 | Loss: 0.00003736
Iteration 24/1000 | Loss: 0.00003116
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00004674
Iteration 28/1000 | Loss: 0.00001468
Iteration 29/1000 | Loss: 0.00001722
Iteration 30/1000 | Loss: 0.00001325
Iteration 31/1000 | Loss: 0.00001325
Iteration 32/1000 | Loss: 0.00001325
Iteration 33/1000 | Loss: 0.00001325
Iteration 34/1000 | Loss: 0.00001325
Iteration 35/1000 | Loss: 0.00001324
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001324
Iteration 42/1000 | Loss: 0.00001324
Iteration 43/1000 | Loss: 0.00001324
Iteration 44/1000 | Loss: 0.00001323
Iteration 45/1000 | Loss: 0.00001323
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001322
Iteration 49/1000 | Loss: 0.00001322
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00003061
Iteration 54/1000 | Loss: 0.00001504
Iteration 55/1000 | Loss: 0.00001321
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001318
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001317
Iteration 70/1000 | Loss: 0.00001317
Iteration 71/1000 | Loss: 0.00001880
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001314
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001314
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001313
Iteration 87/1000 | Loss: 0.00001313
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00001313
Iteration 90/1000 | Loss: 0.00001313
Iteration 91/1000 | Loss: 0.00001313
Iteration 92/1000 | Loss: 0.00001313
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001312
Iteration 107/1000 | Loss: 0.00001312
Iteration 108/1000 | Loss: 0.00001312
Iteration 109/1000 | Loss: 0.00001312
Iteration 110/1000 | Loss: 0.00001312
Iteration 111/1000 | Loss: 0.00001312
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001312
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001312
Iteration 119/1000 | Loss: 0.00001312
Iteration 120/1000 | Loss: 0.00001312
Iteration 121/1000 | Loss: 0.00001312
Iteration 122/1000 | Loss: 0.00001312
Iteration 123/1000 | Loss: 0.00001312
Iteration 124/1000 | Loss: 0.00001312
Iteration 125/1000 | Loss: 0.00001312
Iteration 126/1000 | Loss: 0.00001312
Iteration 127/1000 | Loss: 0.00001312
Iteration 128/1000 | Loss: 0.00001312
Iteration 129/1000 | Loss: 0.00001312
Iteration 130/1000 | Loss: 0.00001312
Iteration 131/1000 | Loss: 0.00001312
Iteration 132/1000 | Loss: 0.00001312
Iteration 133/1000 | Loss: 0.00001312
Iteration 134/1000 | Loss: 0.00001312
Iteration 135/1000 | Loss: 0.00001312
Iteration 136/1000 | Loss: 0.00001312
Iteration 137/1000 | Loss: 0.00001312
Iteration 138/1000 | Loss: 0.00001312
Iteration 139/1000 | Loss: 0.00001312
Iteration 140/1000 | Loss: 0.00001312
Iteration 141/1000 | Loss: 0.00001312
Iteration 142/1000 | Loss: 0.00001312
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001312
Iteration 146/1000 | Loss: 0.00001312
Iteration 147/1000 | Loss: 0.00001312
Iteration 148/1000 | Loss: 0.00001312
Iteration 149/1000 | Loss: 0.00001312
Iteration 150/1000 | Loss: 0.00001312
Iteration 151/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3120094990881626e-05, 1.3120094990881626e-05, 1.3120094990881626e-05, 1.3120094990881626e-05, 1.3120094990881626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3120094990881626e-05

Optimization complete. Final v2v error: 3.0692837238311768 mm

Highest mean error: 3.771595001220703 mm for frame 67

Lowest mean error: 2.7699685096740723 mm for frame 137

Saving results

Total time: 77.44068098068237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00624953
Iteration 2/25 | Loss: 0.00091580
Iteration 3/25 | Loss: 0.00078565
Iteration 4/25 | Loss: 0.00076546
Iteration 5/25 | Loss: 0.00075974
Iteration 6/25 | Loss: 0.00075812
Iteration 7/25 | Loss: 0.00075793
Iteration 8/25 | Loss: 0.00075793
Iteration 9/25 | Loss: 0.00075793
Iteration 10/25 | Loss: 0.00075793
Iteration 11/25 | Loss: 0.00075793
Iteration 12/25 | Loss: 0.00075793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007579306256957352, 0.0007579306256957352, 0.0007579306256957352, 0.0007579306256957352, 0.0007579306256957352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007579306256957352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.94403362
Iteration 2/25 | Loss: 0.00131422
Iteration 3/25 | Loss: 0.00131418
Iteration 4/25 | Loss: 0.00131418
Iteration 5/25 | Loss: 0.00131418
Iteration 6/25 | Loss: 0.00131418
Iteration 7/25 | Loss: 0.00131418
Iteration 8/25 | Loss: 0.00131418
Iteration 9/25 | Loss: 0.00131418
Iteration 10/25 | Loss: 0.00131418
Iteration 11/25 | Loss: 0.00131418
Iteration 12/25 | Loss: 0.00131418
Iteration 13/25 | Loss: 0.00131418
Iteration 14/25 | Loss: 0.00131418
Iteration 15/25 | Loss: 0.00131418
Iteration 16/25 | Loss: 0.00131418
Iteration 17/25 | Loss: 0.00131418
Iteration 18/25 | Loss: 0.00131418
Iteration 19/25 | Loss: 0.00131418
Iteration 20/25 | Loss: 0.00131418
Iteration 21/25 | Loss: 0.00131418
Iteration 22/25 | Loss: 0.00131418
Iteration 23/25 | Loss: 0.00131418
Iteration 24/25 | Loss: 0.00131418
Iteration 25/25 | Loss: 0.00131418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131418
Iteration 2/1000 | Loss: 0.00002152
Iteration 3/1000 | Loss: 0.00001601
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001397
Iteration 6/1000 | Loss: 0.00001358
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001318
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001291
Iteration 11/1000 | Loss: 0.00001278
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001273
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001273
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001270
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001269
Iteration 29/1000 | Loss: 0.00001269
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001269
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001268
Iteration 35/1000 | Loss: 0.00001268
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001268
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001268
Iteration 41/1000 | Loss: 0.00001268
Iteration 42/1000 | Loss: 0.00001267
Iteration 43/1000 | Loss: 0.00001267
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001265
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001264
Iteration 59/1000 | Loss: 0.00001264
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001263
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001258
Iteration 76/1000 | Loss: 0.00001258
Iteration 77/1000 | Loss: 0.00001258
Iteration 78/1000 | Loss: 0.00001258
Iteration 79/1000 | Loss: 0.00001258
Iteration 80/1000 | Loss: 0.00001257
Iteration 81/1000 | Loss: 0.00001257
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001256
Iteration 84/1000 | Loss: 0.00001256
Iteration 85/1000 | Loss: 0.00001256
Iteration 86/1000 | Loss: 0.00001256
Iteration 87/1000 | Loss: 0.00001256
Iteration 88/1000 | Loss: 0.00001255
Iteration 89/1000 | Loss: 0.00001255
Iteration 90/1000 | Loss: 0.00001255
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001255
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001255
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001254
Iteration 104/1000 | Loss: 0.00001254
Iteration 105/1000 | Loss: 0.00001254
Iteration 106/1000 | Loss: 0.00001254
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001253
Iteration 109/1000 | Loss: 0.00001253
Iteration 110/1000 | Loss: 0.00001253
Iteration 111/1000 | Loss: 0.00001253
Iteration 112/1000 | Loss: 0.00001253
Iteration 113/1000 | Loss: 0.00001253
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001252
Iteration 127/1000 | Loss: 0.00001252
Iteration 128/1000 | Loss: 0.00001252
Iteration 129/1000 | Loss: 0.00001252
Iteration 130/1000 | Loss: 0.00001252
Iteration 131/1000 | Loss: 0.00001252
Iteration 132/1000 | Loss: 0.00001252
Iteration 133/1000 | Loss: 0.00001252
Iteration 134/1000 | Loss: 0.00001252
Iteration 135/1000 | Loss: 0.00001252
Iteration 136/1000 | Loss: 0.00001252
Iteration 137/1000 | Loss: 0.00001252
Iteration 138/1000 | Loss: 0.00001252
Iteration 139/1000 | Loss: 0.00001252
Iteration 140/1000 | Loss: 0.00001252
Iteration 141/1000 | Loss: 0.00001252
Iteration 142/1000 | Loss: 0.00001252
Iteration 143/1000 | Loss: 0.00001252
Iteration 144/1000 | Loss: 0.00001252
Iteration 145/1000 | Loss: 0.00001251
Iteration 146/1000 | Loss: 0.00001251
Iteration 147/1000 | Loss: 0.00001251
Iteration 148/1000 | Loss: 0.00001251
Iteration 149/1000 | Loss: 0.00001251
Iteration 150/1000 | Loss: 0.00001251
Iteration 151/1000 | Loss: 0.00001251
Iteration 152/1000 | Loss: 0.00001251
Iteration 153/1000 | Loss: 0.00001251
Iteration 154/1000 | Loss: 0.00001251
Iteration 155/1000 | Loss: 0.00001251
Iteration 156/1000 | Loss: 0.00001251
Iteration 157/1000 | Loss: 0.00001251
Iteration 158/1000 | Loss: 0.00001251
Iteration 159/1000 | Loss: 0.00001251
Iteration 160/1000 | Loss: 0.00001251
Iteration 161/1000 | Loss: 0.00001251
Iteration 162/1000 | Loss: 0.00001251
Iteration 163/1000 | Loss: 0.00001251
Iteration 164/1000 | Loss: 0.00001251
Iteration 165/1000 | Loss: 0.00001251
Iteration 166/1000 | Loss: 0.00001251
Iteration 167/1000 | Loss: 0.00001251
Iteration 168/1000 | Loss: 0.00001251
Iteration 169/1000 | Loss: 0.00001251
Iteration 170/1000 | Loss: 0.00001251
Iteration 171/1000 | Loss: 0.00001251
Iteration 172/1000 | Loss: 0.00001251
Iteration 173/1000 | Loss: 0.00001251
Iteration 174/1000 | Loss: 0.00001251
Iteration 175/1000 | Loss: 0.00001251
Iteration 176/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.251230150955962e-05, 1.251230150955962e-05, 1.251230150955962e-05, 1.251230150955962e-05, 1.251230150955962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.251230150955962e-05

Optimization complete. Final v2v error: 3.012205123901367 mm

Highest mean error: 3.320204496383667 mm for frame 118

Lowest mean error: 2.796001672744751 mm for frame 22

Saving results

Total time: 38.95044660568237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756736
Iteration 2/25 | Loss: 0.00121215
Iteration 3/25 | Loss: 0.00093631
Iteration 4/25 | Loss: 0.00086185
Iteration 5/25 | Loss: 0.00084164
Iteration 6/25 | Loss: 0.00083517
Iteration 7/25 | Loss: 0.00083308
Iteration 8/25 | Loss: 0.00083259
Iteration 9/25 | Loss: 0.00083259
Iteration 10/25 | Loss: 0.00083259
Iteration 11/25 | Loss: 0.00083259
Iteration 12/25 | Loss: 0.00083259
Iteration 13/25 | Loss: 0.00083259
Iteration 14/25 | Loss: 0.00083259
Iteration 15/25 | Loss: 0.00083259
Iteration 16/25 | Loss: 0.00083259
Iteration 17/25 | Loss: 0.00083259
Iteration 18/25 | Loss: 0.00083259
Iteration 19/25 | Loss: 0.00083259
Iteration 20/25 | Loss: 0.00083259
Iteration 21/25 | Loss: 0.00083259
Iteration 22/25 | Loss: 0.00083259
Iteration 23/25 | Loss: 0.00083259
Iteration 24/25 | Loss: 0.00083259
Iteration 25/25 | Loss: 0.00083259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81770349
Iteration 2/25 | Loss: 0.00146164
Iteration 3/25 | Loss: 0.00146164
Iteration 4/25 | Loss: 0.00146164
Iteration 5/25 | Loss: 0.00146164
Iteration 6/25 | Loss: 0.00146164
Iteration 7/25 | Loss: 0.00146164
Iteration 8/25 | Loss: 0.00146164
Iteration 9/25 | Loss: 0.00146164
Iteration 10/25 | Loss: 0.00146164
Iteration 11/25 | Loss: 0.00146164
Iteration 12/25 | Loss: 0.00146164
Iteration 13/25 | Loss: 0.00146164
Iteration 14/25 | Loss: 0.00146164
Iteration 15/25 | Loss: 0.00146164
Iteration 16/25 | Loss: 0.00146164
Iteration 17/25 | Loss: 0.00146164
Iteration 18/25 | Loss: 0.00146164
Iteration 19/25 | Loss: 0.00146164
Iteration 20/25 | Loss: 0.00146164
Iteration 21/25 | Loss: 0.00146164
Iteration 22/25 | Loss: 0.00146164
Iteration 23/25 | Loss: 0.00146164
Iteration 24/25 | Loss: 0.00146164
Iteration 25/25 | Loss: 0.00146164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146164
Iteration 2/1000 | Loss: 0.00005562
Iteration 3/1000 | Loss: 0.00003458
Iteration 4/1000 | Loss: 0.00002900
Iteration 5/1000 | Loss: 0.00002631
Iteration 6/1000 | Loss: 0.00002469
Iteration 7/1000 | Loss: 0.00002362
Iteration 8/1000 | Loss: 0.00002294
Iteration 9/1000 | Loss: 0.00002225
Iteration 10/1000 | Loss: 0.00002187
Iteration 11/1000 | Loss: 0.00002157
Iteration 12/1000 | Loss: 0.00002143
Iteration 13/1000 | Loss: 0.00002122
Iteration 14/1000 | Loss: 0.00002101
Iteration 15/1000 | Loss: 0.00002095
Iteration 16/1000 | Loss: 0.00002088
Iteration 17/1000 | Loss: 0.00002082
Iteration 18/1000 | Loss: 0.00002080
Iteration 19/1000 | Loss: 0.00002076
Iteration 20/1000 | Loss: 0.00002076
Iteration 21/1000 | Loss: 0.00002075
Iteration 22/1000 | Loss: 0.00002074
Iteration 23/1000 | Loss: 0.00002074
Iteration 24/1000 | Loss: 0.00002071
Iteration 25/1000 | Loss: 0.00002069
Iteration 26/1000 | Loss: 0.00002069
Iteration 27/1000 | Loss: 0.00002068
Iteration 28/1000 | Loss: 0.00002068
Iteration 29/1000 | Loss: 0.00002067
Iteration 30/1000 | Loss: 0.00002067
Iteration 31/1000 | Loss: 0.00002066
Iteration 32/1000 | Loss: 0.00002065
Iteration 33/1000 | Loss: 0.00002064
Iteration 34/1000 | Loss: 0.00002063
Iteration 35/1000 | Loss: 0.00002063
Iteration 36/1000 | Loss: 0.00002062
Iteration 37/1000 | Loss: 0.00002062
Iteration 38/1000 | Loss: 0.00002062
Iteration 39/1000 | Loss: 0.00002061
Iteration 40/1000 | Loss: 0.00002061
Iteration 41/1000 | Loss: 0.00002060
Iteration 42/1000 | Loss: 0.00002059
Iteration 43/1000 | Loss: 0.00002059
Iteration 44/1000 | Loss: 0.00002058
Iteration 45/1000 | Loss: 0.00002058
Iteration 46/1000 | Loss: 0.00002058
Iteration 47/1000 | Loss: 0.00002057
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002056
Iteration 50/1000 | Loss: 0.00002056
Iteration 51/1000 | Loss: 0.00002055
Iteration 52/1000 | Loss: 0.00002055
Iteration 53/1000 | Loss: 0.00002055
Iteration 54/1000 | Loss: 0.00002054
Iteration 55/1000 | Loss: 0.00002054
Iteration 56/1000 | Loss: 0.00002053
Iteration 57/1000 | Loss: 0.00002053
Iteration 58/1000 | Loss: 0.00002053
Iteration 59/1000 | Loss: 0.00002052
Iteration 60/1000 | Loss: 0.00002052
Iteration 61/1000 | Loss: 0.00002052
Iteration 62/1000 | Loss: 0.00002051
Iteration 63/1000 | Loss: 0.00002051
Iteration 64/1000 | Loss: 0.00002051
Iteration 65/1000 | Loss: 0.00002050
Iteration 66/1000 | Loss: 0.00002050
Iteration 67/1000 | Loss: 0.00002050
Iteration 68/1000 | Loss: 0.00002050
Iteration 69/1000 | Loss: 0.00002049
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002049
Iteration 72/1000 | Loss: 0.00002048
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002048
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00002047
Iteration 78/1000 | Loss: 0.00002047
Iteration 79/1000 | Loss: 0.00002047
Iteration 80/1000 | Loss: 0.00002046
Iteration 81/1000 | Loss: 0.00002046
Iteration 82/1000 | Loss: 0.00002046
Iteration 83/1000 | Loss: 0.00002045
Iteration 84/1000 | Loss: 0.00002045
Iteration 85/1000 | Loss: 0.00002045
Iteration 86/1000 | Loss: 0.00002044
Iteration 87/1000 | Loss: 0.00002044
Iteration 88/1000 | Loss: 0.00002044
Iteration 89/1000 | Loss: 0.00002044
Iteration 90/1000 | Loss: 0.00002044
Iteration 91/1000 | Loss: 0.00002043
Iteration 92/1000 | Loss: 0.00002043
Iteration 93/1000 | Loss: 0.00002043
Iteration 94/1000 | Loss: 0.00002043
Iteration 95/1000 | Loss: 0.00002043
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00002043
Iteration 99/1000 | Loss: 0.00002043
Iteration 100/1000 | Loss: 0.00002042
Iteration 101/1000 | Loss: 0.00002042
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002041
Iteration 104/1000 | Loss: 0.00002041
Iteration 105/1000 | Loss: 0.00002041
Iteration 106/1000 | Loss: 0.00002041
Iteration 107/1000 | Loss: 0.00002041
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002040
Iteration 113/1000 | Loss: 0.00002040
Iteration 114/1000 | Loss: 0.00002040
Iteration 115/1000 | Loss: 0.00002040
Iteration 116/1000 | Loss: 0.00002040
Iteration 117/1000 | Loss: 0.00002040
Iteration 118/1000 | Loss: 0.00002040
Iteration 119/1000 | Loss: 0.00002039
Iteration 120/1000 | Loss: 0.00002039
Iteration 121/1000 | Loss: 0.00002039
Iteration 122/1000 | Loss: 0.00002039
Iteration 123/1000 | Loss: 0.00002039
Iteration 124/1000 | Loss: 0.00002038
Iteration 125/1000 | Loss: 0.00002038
Iteration 126/1000 | Loss: 0.00002038
Iteration 127/1000 | Loss: 0.00002038
Iteration 128/1000 | Loss: 0.00002038
Iteration 129/1000 | Loss: 0.00002037
Iteration 130/1000 | Loss: 0.00002037
Iteration 131/1000 | Loss: 0.00002037
Iteration 132/1000 | Loss: 0.00002037
Iteration 133/1000 | Loss: 0.00002037
Iteration 134/1000 | Loss: 0.00002037
Iteration 135/1000 | Loss: 0.00002037
Iteration 136/1000 | Loss: 0.00002037
Iteration 137/1000 | Loss: 0.00002037
Iteration 138/1000 | Loss: 0.00002037
Iteration 139/1000 | Loss: 0.00002037
Iteration 140/1000 | Loss: 0.00002036
Iteration 141/1000 | Loss: 0.00002036
Iteration 142/1000 | Loss: 0.00002036
Iteration 143/1000 | Loss: 0.00002036
Iteration 144/1000 | Loss: 0.00002036
Iteration 145/1000 | Loss: 0.00002036
Iteration 146/1000 | Loss: 0.00002036
Iteration 147/1000 | Loss: 0.00002036
Iteration 148/1000 | Loss: 0.00002036
Iteration 149/1000 | Loss: 0.00002036
Iteration 150/1000 | Loss: 0.00002036
Iteration 151/1000 | Loss: 0.00002036
Iteration 152/1000 | Loss: 0.00002036
Iteration 153/1000 | Loss: 0.00002036
Iteration 154/1000 | Loss: 0.00002036
Iteration 155/1000 | Loss: 0.00002035
Iteration 156/1000 | Loss: 0.00002035
Iteration 157/1000 | Loss: 0.00002035
Iteration 158/1000 | Loss: 0.00002035
Iteration 159/1000 | Loss: 0.00002035
Iteration 160/1000 | Loss: 0.00002035
Iteration 161/1000 | Loss: 0.00002035
Iteration 162/1000 | Loss: 0.00002035
Iteration 163/1000 | Loss: 0.00002035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.035198667726945e-05, 2.035198667726945e-05, 2.035198667726945e-05, 2.035198667726945e-05, 2.035198667726945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.035198667726945e-05

Optimization complete. Final v2v error: 3.809985637664795 mm

Highest mean error: 5.33807373046875 mm for frame 64

Lowest mean error: 3.0520362854003906 mm for frame 101

Saving results

Total time: 51.689342975616455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869628
Iteration 2/25 | Loss: 0.00142083
Iteration 3/25 | Loss: 0.00093038
Iteration 4/25 | Loss: 0.00086977
Iteration 5/25 | Loss: 0.00085989
Iteration 6/25 | Loss: 0.00086360
Iteration 7/25 | Loss: 0.00086976
Iteration 8/25 | Loss: 0.00085633
Iteration 9/25 | Loss: 0.00085668
Iteration 10/25 | Loss: 0.00084311
Iteration 11/25 | Loss: 0.00083379
Iteration 12/25 | Loss: 0.00082531
Iteration 13/25 | Loss: 0.00082922
Iteration 14/25 | Loss: 0.00082490
Iteration 15/25 | Loss: 0.00082015
Iteration 16/25 | Loss: 0.00082210
Iteration 17/25 | Loss: 0.00082057
Iteration 18/25 | Loss: 0.00081757
Iteration 19/25 | Loss: 0.00081647
Iteration 20/25 | Loss: 0.00081535
Iteration 21/25 | Loss: 0.00081053
Iteration 22/25 | Loss: 0.00080989
Iteration 23/25 | Loss: 0.00080965
Iteration 24/25 | Loss: 0.00080962
Iteration 25/25 | Loss: 0.00080962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.32258415
Iteration 2/25 | Loss: 0.00128700
Iteration 3/25 | Loss: 0.00128692
Iteration 4/25 | Loss: 0.00128692
Iteration 5/25 | Loss: 0.00128691
Iteration 6/25 | Loss: 0.00128691
Iteration 7/25 | Loss: 0.00128691
Iteration 8/25 | Loss: 0.00128691
Iteration 9/25 | Loss: 0.00128691
Iteration 10/25 | Loss: 0.00128691
Iteration 11/25 | Loss: 0.00128691
Iteration 12/25 | Loss: 0.00128691
Iteration 13/25 | Loss: 0.00128691
Iteration 14/25 | Loss: 0.00128691
Iteration 15/25 | Loss: 0.00128691
Iteration 16/25 | Loss: 0.00128691
Iteration 17/25 | Loss: 0.00128691
Iteration 18/25 | Loss: 0.00128691
Iteration 19/25 | Loss: 0.00128691
Iteration 20/25 | Loss: 0.00128691
Iteration 21/25 | Loss: 0.00128691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012869127094745636, 0.0012869127094745636, 0.0012869127094745636, 0.0012869127094745636, 0.0012869127094745636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012869127094745636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128691
Iteration 2/1000 | Loss: 0.00002534
Iteration 3/1000 | Loss: 0.00002020
Iteration 4/1000 | Loss: 0.00001917
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001774
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00001724
Iteration 9/1000 | Loss: 0.00001716
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001711
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001693
Iteration 14/1000 | Loss: 0.00001690
Iteration 15/1000 | Loss: 0.00001687
Iteration 16/1000 | Loss: 0.00001687
Iteration 17/1000 | Loss: 0.00001686
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001683
Iteration 20/1000 | Loss: 0.00001683
Iteration 21/1000 | Loss: 0.00001681
Iteration 22/1000 | Loss: 0.00001681
Iteration 23/1000 | Loss: 0.00001680
Iteration 24/1000 | Loss: 0.00001680
Iteration 25/1000 | Loss: 0.00001680
Iteration 26/1000 | Loss: 0.00001679
Iteration 27/1000 | Loss: 0.00001679
Iteration 28/1000 | Loss: 0.00001679
Iteration 29/1000 | Loss: 0.00001679
Iteration 30/1000 | Loss: 0.00001679
Iteration 31/1000 | Loss: 0.00001678
Iteration 32/1000 | Loss: 0.00001678
Iteration 33/1000 | Loss: 0.00001678
Iteration 34/1000 | Loss: 0.00001678
Iteration 35/1000 | Loss: 0.00001677
Iteration 36/1000 | Loss: 0.00001677
Iteration 37/1000 | Loss: 0.00001677
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001677
Iteration 40/1000 | Loss: 0.00001677
Iteration 41/1000 | Loss: 0.00001677
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00001676
Iteration 44/1000 | Loss: 0.00001676
Iteration 45/1000 | Loss: 0.00001676
Iteration 46/1000 | Loss: 0.00001676
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001676
Iteration 51/1000 | Loss: 0.00001675
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001674
Iteration 64/1000 | Loss: 0.00001674
Iteration 65/1000 | Loss: 0.00001674
Iteration 66/1000 | Loss: 0.00001674
Iteration 67/1000 | Loss: 0.00001674
Iteration 68/1000 | Loss: 0.00001674
Iteration 69/1000 | Loss: 0.00001673
Iteration 70/1000 | Loss: 0.00001673
Iteration 71/1000 | Loss: 0.00001673
Iteration 72/1000 | Loss: 0.00001673
Iteration 73/1000 | Loss: 0.00001673
Iteration 74/1000 | Loss: 0.00001673
Iteration 75/1000 | Loss: 0.00001673
Iteration 76/1000 | Loss: 0.00001673
Iteration 77/1000 | Loss: 0.00001673
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001673
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001673
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001673
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.6734746168367565e-05, 1.6734746168367565e-05, 1.6734746168367565e-05, 1.6734746168367565e-05, 1.6734746168367565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6734746168367565e-05

Optimization complete. Final v2v error: 3.4598639011383057 mm

Highest mean error: 4.087594509124756 mm for frame 101

Lowest mean error: 2.9213168621063232 mm for frame 172

Saving results

Total time: 63.290608644485474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040115
Iteration 2/25 | Loss: 0.01040115
Iteration 3/25 | Loss: 0.01040115
Iteration 4/25 | Loss: 0.01040114
Iteration 5/25 | Loss: 0.01040114
Iteration 6/25 | Loss: 0.01040114
Iteration 7/25 | Loss: 0.01040114
Iteration 8/25 | Loss: 0.01040114
Iteration 9/25 | Loss: 0.01040113
Iteration 10/25 | Loss: 0.01040113
Iteration 11/25 | Loss: 0.01040113
Iteration 12/25 | Loss: 0.01040113
Iteration 13/25 | Loss: 0.01040113
Iteration 14/25 | Loss: 0.01040112
Iteration 15/25 | Loss: 0.01040112
Iteration 16/25 | Loss: 0.01040112
Iteration 17/25 | Loss: 0.01040111
Iteration 18/25 | Loss: 0.01040111
Iteration 19/25 | Loss: 0.01040111
Iteration 20/25 | Loss: 0.01040111
Iteration 21/25 | Loss: 0.01040111
Iteration 22/25 | Loss: 0.01040111
Iteration 23/25 | Loss: 0.01040110
Iteration 24/25 | Loss: 0.01040110
Iteration 25/25 | Loss: 0.01040110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73884761
Iteration 2/25 | Loss: 0.18065958
Iteration 3/25 | Loss: 0.18030062
Iteration 4/25 | Loss: 0.18190929
Iteration 5/25 | Loss: 0.17723969
Iteration 6/25 | Loss: 0.17719054
Iteration 7/25 | Loss: 0.17716101
Iteration 8/25 | Loss: 0.17716096
Iteration 9/25 | Loss: 0.17716092
Iteration 10/25 | Loss: 0.17716092
Iteration 11/25 | Loss: 0.17716090
Iteration 12/25 | Loss: 0.17716090
Iteration 13/25 | Loss: 0.17716090
Iteration 14/25 | Loss: 0.17716090
Iteration 15/25 | Loss: 0.17716090
Iteration 16/25 | Loss: 0.17716090
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.17716090381145477, 0.17716090381145477, 0.17716090381145477, 0.17716090381145477, 0.17716090381145477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17716090381145477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17716090
Iteration 2/1000 | Loss: 0.00643710
Iteration 3/1000 | Loss: 0.00246830
Iteration 4/1000 | Loss: 0.00083358
Iteration 5/1000 | Loss: 0.00142562
Iteration 6/1000 | Loss: 0.00114950
Iteration 7/1000 | Loss: 0.00173249
Iteration 8/1000 | Loss: 0.00216431
Iteration 9/1000 | Loss: 0.00015054
Iteration 10/1000 | Loss: 0.00196128
Iteration 11/1000 | Loss: 0.00036735
Iteration 12/1000 | Loss: 0.00014735
Iteration 13/1000 | Loss: 0.00072047
Iteration 14/1000 | Loss: 0.00377207
Iteration 15/1000 | Loss: 0.00224285
Iteration 16/1000 | Loss: 0.00381702
Iteration 17/1000 | Loss: 0.00288436
Iteration 18/1000 | Loss: 0.00214556
Iteration 19/1000 | Loss: 0.00601274
Iteration 20/1000 | Loss: 0.00018549
Iteration 21/1000 | Loss: 0.00045249
Iteration 22/1000 | Loss: 0.00045920
Iteration 23/1000 | Loss: 0.00028616
Iteration 24/1000 | Loss: 0.00024075
Iteration 25/1000 | Loss: 0.00019372
Iteration 26/1000 | Loss: 0.00176889
Iteration 27/1000 | Loss: 0.00004329
Iteration 28/1000 | Loss: 0.00053450
Iteration 29/1000 | Loss: 0.00013199
Iteration 30/1000 | Loss: 0.00051097
Iteration 31/1000 | Loss: 0.00009512
Iteration 32/1000 | Loss: 0.00020721
Iteration 33/1000 | Loss: 0.00002837
Iteration 34/1000 | Loss: 0.00002649
Iteration 35/1000 | Loss: 0.00002439
Iteration 36/1000 | Loss: 0.00009894
Iteration 37/1000 | Loss: 0.00002212
Iteration 38/1000 | Loss: 0.00016727
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002034
Iteration 41/1000 | Loss: 0.00007119
Iteration 42/1000 | Loss: 0.00004341
Iteration 43/1000 | Loss: 0.00010535
Iteration 44/1000 | Loss: 0.00001925
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00001873
Iteration 47/1000 | Loss: 0.00001866
Iteration 48/1000 | Loss: 0.00001854
Iteration 49/1000 | Loss: 0.00001852
Iteration 50/1000 | Loss: 0.00001852
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001844
Iteration 53/1000 | Loss: 0.00001844
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001842
Iteration 56/1000 | Loss: 0.00001840
Iteration 57/1000 | Loss: 0.00001839
Iteration 58/1000 | Loss: 0.00001839
Iteration 59/1000 | Loss: 0.00001838
Iteration 60/1000 | Loss: 0.00001838
Iteration 61/1000 | Loss: 0.00001838
Iteration 62/1000 | Loss: 0.00001838
Iteration 63/1000 | Loss: 0.00011274
Iteration 64/1000 | Loss: 0.00001866
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001832
Iteration 74/1000 | Loss: 0.00001832
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00001831
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001830
Iteration 83/1000 | Loss: 0.00001830
Iteration 84/1000 | Loss: 0.00001830
Iteration 85/1000 | Loss: 0.00001829
Iteration 86/1000 | Loss: 0.00001829
Iteration 87/1000 | Loss: 0.00001829
Iteration 88/1000 | Loss: 0.00001829
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001827
Iteration 110/1000 | Loss: 0.00001827
Iteration 111/1000 | Loss: 0.00001827
Iteration 112/1000 | Loss: 0.00001827
Iteration 113/1000 | Loss: 0.00001827
Iteration 114/1000 | Loss: 0.00001826
Iteration 115/1000 | Loss: 0.00001826
Iteration 116/1000 | Loss: 0.00001825
Iteration 117/1000 | Loss: 0.00001825
Iteration 118/1000 | Loss: 0.00001825
Iteration 119/1000 | Loss: 0.00001825
Iteration 120/1000 | Loss: 0.00001825
Iteration 121/1000 | Loss: 0.00001825
Iteration 122/1000 | Loss: 0.00001825
Iteration 123/1000 | Loss: 0.00001825
Iteration 124/1000 | Loss: 0.00001825
Iteration 125/1000 | Loss: 0.00001825
Iteration 126/1000 | Loss: 0.00001825
Iteration 127/1000 | Loss: 0.00001825
Iteration 128/1000 | Loss: 0.00001825
Iteration 129/1000 | Loss: 0.00001825
Iteration 130/1000 | Loss: 0.00001824
Iteration 131/1000 | Loss: 0.00001824
Iteration 132/1000 | Loss: 0.00001824
Iteration 133/1000 | Loss: 0.00001823
Iteration 134/1000 | Loss: 0.00001823
Iteration 135/1000 | Loss: 0.00001823
Iteration 136/1000 | Loss: 0.00001823
Iteration 137/1000 | Loss: 0.00001823
Iteration 138/1000 | Loss: 0.00001823
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001822
Iteration 141/1000 | Loss: 0.00001822
Iteration 142/1000 | Loss: 0.00001822
Iteration 143/1000 | Loss: 0.00001822
Iteration 144/1000 | Loss: 0.00001822
Iteration 145/1000 | Loss: 0.00001822
Iteration 146/1000 | Loss: 0.00001822
Iteration 147/1000 | Loss: 0.00001822
Iteration 148/1000 | Loss: 0.00001822
Iteration 149/1000 | Loss: 0.00001822
Iteration 150/1000 | Loss: 0.00001822
Iteration 151/1000 | Loss: 0.00001822
Iteration 152/1000 | Loss: 0.00001822
Iteration 153/1000 | Loss: 0.00001822
Iteration 154/1000 | Loss: 0.00001822
Iteration 155/1000 | Loss: 0.00001822
Iteration 156/1000 | Loss: 0.00001822
Iteration 157/1000 | Loss: 0.00001822
Iteration 158/1000 | Loss: 0.00001822
Iteration 159/1000 | Loss: 0.00001822
Iteration 160/1000 | Loss: 0.00001822
Iteration 161/1000 | Loss: 0.00001822
Iteration 162/1000 | Loss: 0.00001822
Iteration 163/1000 | Loss: 0.00001822
Iteration 164/1000 | Loss: 0.00001822
Iteration 165/1000 | Loss: 0.00001822
Iteration 166/1000 | Loss: 0.00001822
Iteration 167/1000 | Loss: 0.00001822
Iteration 168/1000 | Loss: 0.00001822
Iteration 169/1000 | Loss: 0.00001822
Iteration 170/1000 | Loss: 0.00001822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.821815203584265e-05, 1.821815203584265e-05, 1.821815203584265e-05, 1.821815203584265e-05, 1.821815203584265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.821815203584265e-05

Optimization complete. Final v2v error: 3.624581813812256 mm

Highest mean error: 4.05631685256958 mm for frame 26

Lowest mean error: 3.3394410610198975 mm for frame 211

Saving results

Total time: 103.01216006278992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018013
Iteration 2/25 | Loss: 0.00381103
Iteration 3/25 | Loss: 0.00235370
Iteration 4/25 | Loss: 0.00208966
Iteration 5/25 | Loss: 0.00187313
Iteration 6/25 | Loss: 0.00160095
Iteration 7/25 | Loss: 0.00150427
Iteration 8/25 | Loss: 0.00146613
Iteration 9/25 | Loss: 0.00151535
Iteration 10/25 | Loss: 0.00150282
Iteration 11/25 | Loss: 0.00143400
Iteration 12/25 | Loss: 0.00140739
Iteration 13/25 | Loss: 0.00140854
Iteration 14/25 | Loss: 0.00135621
Iteration 15/25 | Loss: 0.00134113
Iteration 16/25 | Loss: 0.00133705
Iteration 17/25 | Loss: 0.00132700
Iteration 18/25 | Loss: 0.00132436
Iteration 19/25 | Loss: 0.00131147
Iteration 20/25 | Loss: 0.00130942
Iteration 21/25 | Loss: 0.00130378
Iteration 22/25 | Loss: 0.00130468
Iteration 23/25 | Loss: 0.00130461
Iteration 24/25 | Loss: 0.00129289
Iteration 25/25 | Loss: 0.00128416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56085634
Iteration 2/25 | Loss: 0.00501234
Iteration 3/25 | Loss: 0.00478996
Iteration 4/25 | Loss: 0.00478996
Iteration 5/25 | Loss: 0.00478996
Iteration 6/25 | Loss: 0.00478996
Iteration 7/25 | Loss: 0.00478996
Iteration 8/25 | Loss: 0.00478996
Iteration 9/25 | Loss: 0.00478996
Iteration 10/25 | Loss: 0.00478996
Iteration 11/25 | Loss: 0.00478996
Iteration 12/25 | Loss: 0.00478996
Iteration 13/25 | Loss: 0.00478996
Iteration 14/25 | Loss: 0.00478996
Iteration 15/25 | Loss: 0.00478996
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004789956379681826, 0.004789956379681826, 0.004789956379681826, 0.004789956379681826, 0.004789956379681826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004789956379681826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00478996
Iteration 2/1000 | Loss: 0.00213685
Iteration 3/1000 | Loss: 0.00091403
Iteration 4/1000 | Loss: 0.00096000
Iteration 5/1000 | Loss: 0.00073873
Iteration 6/1000 | Loss: 0.00052579
Iteration 7/1000 | Loss: 0.00200054
Iteration 8/1000 | Loss: 0.00098352
Iteration 9/1000 | Loss: 0.00135132
Iteration 10/1000 | Loss: 0.00102869
Iteration 11/1000 | Loss: 0.00061158
Iteration 12/1000 | Loss: 0.00054136
Iteration 13/1000 | Loss: 0.00160117
Iteration 14/1000 | Loss: 0.00031578
Iteration 15/1000 | Loss: 0.00026097
Iteration 16/1000 | Loss: 0.00095715
Iteration 17/1000 | Loss: 0.00259210
Iteration 18/1000 | Loss: 0.01035181
Iteration 19/1000 | Loss: 0.00927257
Iteration 20/1000 | Loss: 0.00186122
Iteration 21/1000 | Loss: 0.00128061
Iteration 22/1000 | Loss: 0.00241890
Iteration 23/1000 | Loss: 0.00159578
Iteration 24/1000 | Loss: 0.00067273
Iteration 25/1000 | Loss: 0.00090981
Iteration 26/1000 | Loss: 0.00086204
Iteration 27/1000 | Loss: 0.00129630
Iteration 28/1000 | Loss: 0.00031542
Iteration 29/1000 | Loss: 0.00050710
Iteration 30/1000 | Loss: 0.00051136
Iteration 31/1000 | Loss: 0.00077220
Iteration 32/1000 | Loss: 0.00010715
Iteration 33/1000 | Loss: 0.00015163
Iteration 34/1000 | Loss: 0.00048935
Iteration 35/1000 | Loss: 0.00029269
Iteration 36/1000 | Loss: 0.00055007
Iteration 37/1000 | Loss: 0.00010289
Iteration 38/1000 | Loss: 0.00034216
Iteration 39/1000 | Loss: 0.00083764
Iteration 40/1000 | Loss: 0.00034878
Iteration 41/1000 | Loss: 0.00012321
Iteration 42/1000 | Loss: 0.00028188
Iteration 43/1000 | Loss: 0.00022507
Iteration 44/1000 | Loss: 0.00010418
Iteration 45/1000 | Loss: 0.00006318
Iteration 46/1000 | Loss: 0.00008936
Iteration 47/1000 | Loss: 0.00027546
Iteration 48/1000 | Loss: 0.00040314
Iteration 49/1000 | Loss: 0.00010696
Iteration 50/1000 | Loss: 0.00040552
Iteration 51/1000 | Loss: 0.00028181
Iteration 52/1000 | Loss: 0.00009187
Iteration 53/1000 | Loss: 0.00005691
Iteration 54/1000 | Loss: 0.00008216
Iteration 55/1000 | Loss: 0.00007783
Iteration 56/1000 | Loss: 0.00006780
Iteration 57/1000 | Loss: 0.00007711
Iteration 58/1000 | Loss: 0.00005651
Iteration 59/1000 | Loss: 0.00012333
Iteration 60/1000 | Loss: 0.00042177
Iteration 61/1000 | Loss: 0.00026654
Iteration 62/1000 | Loss: 0.00032165
Iteration 63/1000 | Loss: 0.00035722
Iteration 64/1000 | Loss: 0.00037029
Iteration 65/1000 | Loss: 0.00047115
Iteration 66/1000 | Loss: 0.00024397
Iteration 67/1000 | Loss: 0.00006867
Iteration 68/1000 | Loss: 0.00005870
Iteration 69/1000 | Loss: 0.00004351
Iteration 70/1000 | Loss: 0.00010391
Iteration 71/1000 | Loss: 0.00014825
Iteration 72/1000 | Loss: 0.00033405
Iteration 73/1000 | Loss: 0.00027006
Iteration 74/1000 | Loss: 0.00006017
Iteration 75/1000 | Loss: 0.00009785
Iteration 76/1000 | Loss: 0.00020265
Iteration 77/1000 | Loss: 0.00029365
Iteration 78/1000 | Loss: 0.00004743
Iteration 79/1000 | Loss: 0.00009216
Iteration 80/1000 | Loss: 0.00006638
Iteration 81/1000 | Loss: 0.00005777
Iteration 82/1000 | Loss: 0.00006313
Iteration 83/1000 | Loss: 0.00006009
Iteration 84/1000 | Loss: 0.00006326
Iteration 85/1000 | Loss: 0.00050624
Iteration 86/1000 | Loss: 0.00010132
Iteration 87/1000 | Loss: 0.00065436
Iteration 88/1000 | Loss: 0.00022310
Iteration 89/1000 | Loss: 0.00017190
Iteration 90/1000 | Loss: 0.00051281
Iteration 91/1000 | Loss: 0.00006646
Iteration 92/1000 | Loss: 0.00006524
Iteration 93/1000 | Loss: 0.00006404
Iteration 94/1000 | Loss: 0.00016239
Iteration 95/1000 | Loss: 0.00005524
Iteration 96/1000 | Loss: 0.00046029
Iteration 97/1000 | Loss: 0.00004100
Iteration 98/1000 | Loss: 0.00006487
Iteration 99/1000 | Loss: 0.00003823
Iteration 100/1000 | Loss: 0.00003703
Iteration 101/1000 | Loss: 0.00011882
Iteration 102/1000 | Loss: 0.00003583
Iteration 103/1000 | Loss: 0.00003533
Iteration 104/1000 | Loss: 0.00017116
Iteration 105/1000 | Loss: 0.00004884
Iteration 106/1000 | Loss: 0.00005676
Iteration 107/1000 | Loss: 0.00003409
Iteration 108/1000 | Loss: 0.00007366
Iteration 109/1000 | Loss: 0.00003333
Iteration 110/1000 | Loss: 0.00003283
Iteration 111/1000 | Loss: 0.00031048
Iteration 112/1000 | Loss: 0.00096713
Iteration 113/1000 | Loss: 0.00060871
Iteration 114/1000 | Loss: 0.00213914
Iteration 115/1000 | Loss: 0.00008085
Iteration 116/1000 | Loss: 0.00010818
Iteration 117/1000 | Loss: 0.00004467
Iteration 118/1000 | Loss: 0.00004088
Iteration 119/1000 | Loss: 0.00009222
Iteration 120/1000 | Loss: 0.00003744
Iteration 121/1000 | Loss: 0.00012715
Iteration 122/1000 | Loss: 0.00004644
Iteration 123/1000 | Loss: 0.00003561
Iteration 124/1000 | Loss: 0.00003738
Iteration 125/1000 | Loss: 0.00010034
Iteration 126/1000 | Loss: 0.00003713
Iteration 127/1000 | Loss: 0.00005425
Iteration 128/1000 | Loss: 0.00003688
Iteration 129/1000 | Loss: 0.00003537
Iteration 130/1000 | Loss: 0.00012212
Iteration 131/1000 | Loss: 0.00006578
Iteration 132/1000 | Loss: 0.00004330
Iteration 133/1000 | Loss: 0.00003282
Iteration 134/1000 | Loss: 0.00003251
Iteration 135/1000 | Loss: 0.00003222
Iteration 136/1000 | Loss: 0.00003205
Iteration 137/1000 | Loss: 0.00003194
Iteration 138/1000 | Loss: 0.00003185
Iteration 139/1000 | Loss: 0.00003165
Iteration 140/1000 | Loss: 0.00010916
Iteration 141/1000 | Loss: 0.00003156
Iteration 142/1000 | Loss: 0.00003137
Iteration 143/1000 | Loss: 0.00003135
Iteration 144/1000 | Loss: 0.00003131
Iteration 145/1000 | Loss: 0.00003128
Iteration 146/1000 | Loss: 0.00003127
Iteration 147/1000 | Loss: 0.00003127
Iteration 148/1000 | Loss: 0.00003126
Iteration 149/1000 | Loss: 0.00003126
Iteration 150/1000 | Loss: 0.00003123
Iteration 151/1000 | Loss: 0.00003122
Iteration 152/1000 | Loss: 0.00003121
Iteration 153/1000 | Loss: 0.00003121
Iteration 154/1000 | Loss: 0.00017006
Iteration 155/1000 | Loss: 0.00004838
Iteration 156/1000 | Loss: 0.00004305
Iteration 157/1000 | Loss: 0.00005420
Iteration 158/1000 | Loss: 0.00005250
Iteration 159/1000 | Loss: 0.00003127
Iteration 160/1000 | Loss: 0.00004552
Iteration 161/1000 | Loss: 0.00003117
Iteration 162/1000 | Loss: 0.00003115
Iteration 163/1000 | Loss: 0.00003114
Iteration 164/1000 | Loss: 0.00003114
Iteration 165/1000 | Loss: 0.00003114
Iteration 166/1000 | Loss: 0.00003114
Iteration 167/1000 | Loss: 0.00003113
Iteration 168/1000 | Loss: 0.00003113
Iteration 169/1000 | Loss: 0.00003112
Iteration 170/1000 | Loss: 0.00003111
Iteration 171/1000 | Loss: 0.00003111
Iteration 172/1000 | Loss: 0.00003110
Iteration 173/1000 | Loss: 0.00003110
Iteration 174/1000 | Loss: 0.00003109
Iteration 175/1000 | Loss: 0.00003108
Iteration 176/1000 | Loss: 0.00003107
Iteration 177/1000 | Loss: 0.00003106
Iteration 178/1000 | Loss: 0.00003106
Iteration 179/1000 | Loss: 0.00003106
Iteration 180/1000 | Loss: 0.00003104
Iteration 181/1000 | Loss: 0.00003102
Iteration 182/1000 | Loss: 0.00003102
Iteration 183/1000 | Loss: 0.00003101
Iteration 184/1000 | Loss: 0.00003101
Iteration 185/1000 | Loss: 0.00003101
Iteration 186/1000 | Loss: 0.00003100
Iteration 187/1000 | Loss: 0.00003099
Iteration 188/1000 | Loss: 0.00003097
Iteration 189/1000 | Loss: 0.00003095
Iteration 190/1000 | Loss: 0.00003095
Iteration 191/1000 | Loss: 0.00003094
Iteration 192/1000 | Loss: 0.00003094
Iteration 193/1000 | Loss: 0.00003093
Iteration 194/1000 | Loss: 0.00003093
Iteration 195/1000 | Loss: 0.00003093
Iteration 196/1000 | Loss: 0.00003093
Iteration 197/1000 | Loss: 0.00003093
Iteration 198/1000 | Loss: 0.00003093
Iteration 199/1000 | Loss: 0.00003093
Iteration 200/1000 | Loss: 0.00003093
Iteration 201/1000 | Loss: 0.00003092
Iteration 202/1000 | Loss: 0.00003092
Iteration 203/1000 | Loss: 0.00003092
Iteration 204/1000 | Loss: 0.00003092
Iteration 205/1000 | Loss: 0.00003092
Iteration 206/1000 | Loss: 0.00003092
Iteration 207/1000 | Loss: 0.00003092
Iteration 208/1000 | Loss: 0.00003092
Iteration 209/1000 | Loss: 0.00003092
Iteration 210/1000 | Loss: 0.00003092
Iteration 211/1000 | Loss: 0.00003092
Iteration 212/1000 | Loss: 0.00003092
Iteration 213/1000 | Loss: 0.00003092
Iteration 214/1000 | Loss: 0.00003092
Iteration 215/1000 | Loss: 0.00003091
Iteration 216/1000 | Loss: 0.00003091
Iteration 217/1000 | Loss: 0.00003091
Iteration 218/1000 | Loss: 0.00003091
Iteration 219/1000 | Loss: 0.00003091
Iteration 220/1000 | Loss: 0.00003091
Iteration 221/1000 | Loss: 0.00003091
Iteration 222/1000 | Loss: 0.00003091
Iteration 223/1000 | Loss: 0.00003090
Iteration 224/1000 | Loss: 0.00003090
Iteration 225/1000 | Loss: 0.00003090
Iteration 226/1000 | Loss: 0.00003090
Iteration 227/1000 | Loss: 0.00003090
Iteration 228/1000 | Loss: 0.00003089
Iteration 229/1000 | Loss: 0.00003089
Iteration 230/1000 | Loss: 0.00003089
Iteration 231/1000 | Loss: 0.00003089
Iteration 232/1000 | Loss: 0.00003089
Iteration 233/1000 | Loss: 0.00003089
Iteration 234/1000 | Loss: 0.00003089
Iteration 235/1000 | Loss: 0.00003088
Iteration 236/1000 | Loss: 0.00003088
Iteration 237/1000 | Loss: 0.00003088
Iteration 238/1000 | Loss: 0.00003088
Iteration 239/1000 | Loss: 0.00003088
Iteration 240/1000 | Loss: 0.00003088
Iteration 241/1000 | Loss: 0.00003088
Iteration 242/1000 | Loss: 0.00003088
Iteration 243/1000 | Loss: 0.00003088
Iteration 244/1000 | Loss: 0.00003087
Iteration 245/1000 | Loss: 0.00003087
Iteration 246/1000 | Loss: 0.00003087
Iteration 247/1000 | Loss: 0.00003087
Iteration 248/1000 | Loss: 0.00003087
Iteration 249/1000 | Loss: 0.00003087
Iteration 250/1000 | Loss: 0.00003087
Iteration 251/1000 | Loss: 0.00003087
Iteration 252/1000 | Loss: 0.00003086
Iteration 253/1000 | Loss: 0.00003085
Iteration 254/1000 | Loss: 0.00003085
Iteration 255/1000 | Loss: 0.00027634
Iteration 256/1000 | Loss: 0.00024899
Iteration 257/1000 | Loss: 0.00061691
Iteration 258/1000 | Loss: 0.00003720
Iteration 259/1000 | Loss: 0.00010474
Iteration 260/1000 | Loss: 0.00003274
Iteration 261/1000 | Loss: 0.00007166
Iteration 262/1000 | Loss: 0.00003163
Iteration 263/1000 | Loss: 0.00003130
Iteration 264/1000 | Loss: 0.00008028
Iteration 265/1000 | Loss: 0.00003100
Iteration 266/1000 | Loss: 0.00003069
Iteration 267/1000 | Loss: 0.00003069
Iteration 268/1000 | Loss: 0.00003068
Iteration 269/1000 | Loss: 0.00003067
Iteration 270/1000 | Loss: 0.00008837
Iteration 271/1000 | Loss: 0.00003049
Iteration 272/1000 | Loss: 0.00003038
Iteration 273/1000 | Loss: 0.00003037
Iteration 274/1000 | Loss: 0.00003022
Iteration 275/1000 | Loss: 0.00003010
Iteration 276/1000 | Loss: 0.00003007
Iteration 277/1000 | Loss: 0.00003000
Iteration 278/1000 | Loss: 0.00003000
Iteration 279/1000 | Loss: 0.00002999
Iteration 280/1000 | Loss: 0.00002999
Iteration 281/1000 | Loss: 0.00002999
Iteration 282/1000 | Loss: 0.00002999
Iteration 283/1000 | Loss: 0.00002999
Iteration 284/1000 | Loss: 0.00002999
Iteration 285/1000 | Loss: 0.00002998
Iteration 286/1000 | Loss: 0.00008318
Iteration 287/1000 | Loss: 0.00085931
Iteration 288/1000 | Loss: 0.00029923
Iteration 289/1000 | Loss: 0.00078015
Iteration 290/1000 | Loss: 0.00103760
Iteration 291/1000 | Loss: 0.00084757
Iteration 292/1000 | Loss: 0.00031478
Iteration 293/1000 | Loss: 0.00010417
Iteration 294/1000 | Loss: 0.00005787
Iteration 295/1000 | Loss: 0.00004227
Iteration 296/1000 | Loss: 0.00003556
Iteration 297/1000 | Loss: 0.00003102
Iteration 298/1000 | Loss: 0.00014079
Iteration 299/1000 | Loss: 0.00011909
Iteration 300/1000 | Loss: 0.00004102
Iteration 301/1000 | Loss: 0.00002945
Iteration 302/1000 | Loss: 0.00010885
Iteration 303/1000 | Loss: 0.00003410
Iteration 304/1000 | Loss: 0.00007476
Iteration 305/1000 | Loss: 0.00020748
Iteration 306/1000 | Loss: 0.00004515
Iteration 307/1000 | Loss: 0.00012153
Iteration 308/1000 | Loss: 0.00003716
Iteration 309/1000 | Loss: 0.00002944
Iteration 310/1000 | Loss: 0.00012784
Iteration 311/1000 | Loss: 0.00007505
Iteration 312/1000 | Loss: 0.00142064
Iteration 313/1000 | Loss: 0.00005047
Iteration 314/1000 | Loss: 0.00002698
Iteration 315/1000 | Loss: 0.00009713
Iteration 316/1000 | Loss: 0.00003075
Iteration 317/1000 | Loss: 0.00002626
Iteration 318/1000 | Loss: 0.00037339
Iteration 319/1000 | Loss: 0.00017559
Iteration 320/1000 | Loss: 0.00021706
Iteration 321/1000 | Loss: 0.00007436
Iteration 322/1000 | Loss: 0.00002766
Iteration 323/1000 | Loss: 0.00002600
Iteration 324/1000 | Loss: 0.00022115
Iteration 325/1000 | Loss: 0.00015475
Iteration 326/1000 | Loss: 0.00003862
Iteration 327/1000 | Loss: 0.00002301
Iteration 328/1000 | Loss: 0.00002257
Iteration 329/1000 | Loss: 0.00002232
Iteration 330/1000 | Loss: 0.00002229
Iteration 331/1000 | Loss: 0.00002228
Iteration 332/1000 | Loss: 0.00002220
Iteration 333/1000 | Loss: 0.00002219
Iteration 334/1000 | Loss: 0.00002219
Iteration 335/1000 | Loss: 0.00002219
Iteration 336/1000 | Loss: 0.00002218
Iteration 337/1000 | Loss: 0.00002216
Iteration 338/1000 | Loss: 0.00002216
Iteration 339/1000 | Loss: 0.00002215
Iteration 340/1000 | Loss: 0.00002215
Iteration 341/1000 | Loss: 0.00002214
Iteration 342/1000 | Loss: 0.00002214
Iteration 343/1000 | Loss: 0.00002214
Iteration 344/1000 | Loss: 0.00010657
Iteration 345/1000 | Loss: 0.00025237
Iteration 346/1000 | Loss: 0.00005781
Iteration 347/1000 | Loss: 0.00002214
Iteration 348/1000 | Loss: 0.00008507
Iteration 349/1000 | Loss: 0.00002208
Iteration 350/1000 | Loss: 0.00002206
Iteration 351/1000 | Loss: 0.00002206
Iteration 352/1000 | Loss: 0.00002205
Iteration 353/1000 | Loss: 0.00002204
Iteration 354/1000 | Loss: 0.00002202
Iteration 355/1000 | Loss: 0.00002201
Iteration 356/1000 | Loss: 0.00002200
Iteration 357/1000 | Loss: 0.00002200
Iteration 358/1000 | Loss: 0.00002199
Iteration 359/1000 | Loss: 0.00002199
Iteration 360/1000 | Loss: 0.00002198
Iteration 361/1000 | Loss: 0.00002198
Iteration 362/1000 | Loss: 0.00002198
Iteration 363/1000 | Loss: 0.00002197
Iteration 364/1000 | Loss: 0.00002197
Iteration 365/1000 | Loss: 0.00002197
Iteration 366/1000 | Loss: 0.00002197
Iteration 367/1000 | Loss: 0.00002197
Iteration 368/1000 | Loss: 0.00002197
Iteration 369/1000 | Loss: 0.00002197
Iteration 370/1000 | Loss: 0.00002197
Iteration 371/1000 | Loss: 0.00002197
Iteration 372/1000 | Loss: 0.00002197
Iteration 373/1000 | Loss: 0.00002196
Iteration 374/1000 | Loss: 0.00002196
Iteration 375/1000 | Loss: 0.00002196
Iteration 376/1000 | Loss: 0.00002196
Iteration 377/1000 | Loss: 0.00002196
Iteration 378/1000 | Loss: 0.00002196
Iteration 379/1000 | Loss: 0.00002196
Iteration 380/1000 | Loss: 0.00002196
Iteration 381/1000 | Loss: 0.00002196
Iteration 382/1000 | Loss: 0.00002195
Iteration 383/1000 | Loss: 0.00002195
Iteration 384/1000 | Loss: 0.00002195
Iteration 385/1000 | Loss: 0.00002195
Iteration 386/1000 | Loss: 0.00002195
Iteration 387/1000 | Loss: 0.00002195
Iteration 388/1000 | Loss: 0.00002194
Iteration 389/1000 | Loss: 0.00002194
Iteration 390/1000 | Loss: 0.00002194
Iteration 391/1000 | Loss: 0.00002194
Iteration 392/1000 | Loss: 0.00002194
Iteration 393/1000 | Loss: 0.00002194
Iteration 394/1000 | Loss: 0.00002194
Iteration 395/1000 | Loss: 0.00002194
Iteration 396/1000 | Loss: 0.00002194
Iteration 397/1000 | Loss: 0.00002194
Iteration 398/1000 | Loss: 0.00002194
Iteration 399/1000 | Loss: 0.00002194
Iteration 400/1000 | Loss: 0.00002194
Iteration 401/1000 | Loss: 0.00002194
Iteration 402/1000 | Loss: 0.00002194
Iteration 403/1000 | Loss: 0.00002194
Iteration 404/1000 | Loss: 0.00002194
Iteration 405/1000 | Loss: 0.00002194
Iteration 406/1000 | Loss: 0.00002194
Iteration 407/1000 | Loss: 0.00002194
Iteration 408/1000 | Loss: 0.00002194
Iteration 409/1000 | Loss: 0.00002194
Iteration 410/1000 | Loss: 0.00002194
Iteration 411/1000 | Loss: 0.00002194
Iteration 412/1000 | Loss: 0.00002194
Iteration 413/1000 | Loss: 0.00002194
Iteration 414/1000 | Loss: 0.00002194
Iteration 415/1000 | Loss: 0.00002194
Iteration 416/1000 | Loss: 0.00002194
Iteration 417/1000 | Loss: 0.00002194
Iteration 418/1000 | Loss: 0.00002194
Iteration 419/1000 | Loss: 0.00002194
Iteration 420/1000 | Loss: 0.00002194
Iteration 421/1000 | Loss: 0.00002194
Iteration 422/1000 | Loss: 0.00002194
Iteration 423/1000 | Loss: 0.00002194
Iteration 424/1000 | Loss: 0.00002194
Iteration 425/1000 | Loss: 0.00002194
Iteration 426/1000 | Loss: 0.00002194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 426. Stopping optimization.
Last 5 losses: [2.194291664636694e-05, 2.194291664636694e-05, 2.194291664636694e-05, 2.194291664636694e-05, 2.194291664636694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.194291664636694e-05

Optimization complete. Final v2v error: 3.7429769039154053 mm

Highest mean error: 11.7442045211792 mm for frame 44

Lowest mean error: 3.4850056171417236 mm for frame 135

Saving results

Total time: 363.1462199687958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391087
Iteration 2/25 | Loss: 0.00093510
Iteration 3/25 | Loss: 0.00079438
Iteration 4/25 | Loss: 0.00077715
Iteration 5/25 | Loss: 0.00077294
Iteration 6/25 | Loss: 0.00077158
Iteration 7/25 | Loss: 0.00077128
Iteration 8/25 | Loss: 0.00077128
Iteration 9/25 | Loss: 0.00077128
Iteration 10/25 | Loss: 0.00077128
Iteration 11/25 | Loss: 0.00077128
Iteration 12/25 | Loss: 0.00077128
Iteration 13/25 | Loss: 0.00077128
Iteration 14/25 | Loss: 0.00077128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007712843944318593, 0.0007712843944318593, 0.0007712843944318593, 0.0007712843944318593, 0.0007712843944318593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007712843944318593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59812701
Iteration 2/25 | Loss: 0.00128577
Iteration 3/25 | Loss: 0.00128577
Iteration 4/25 | Loss: 0.00128577
Iteration 5/25 | Loss: 0.00128577
Iteration 6/25 | Loss: 0.00128577
Iteration 7/25 | Loss: 0.00128577
Iteration 8/25 | Loss: 0.00128577
Iteration 9/25 | Loss: 0.00128577
Iteration 10/25 | Loss: 0.00128577
Iteration 11/25 | Loss: 0.00128577
Iteration 12/25 | Loss: 0.00128577
Iteration 13/25 | Loss: 0.00128577
Iteration 14/25 | Loss: 0.00128577
Iteration 15/25 | Loss: 0.00128577
Iteration 16/25 | Loss: 0.00128577
Iteration 17/25 | Loss: 0.00128577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012857663678005338, 0.0012857663678005338, 0.0012857663678005338, 0.0012857663678005338, 0.0012857663678005338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012857663678005338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128577
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001367
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001355
Iteration 11/1000 | Loss: 0.00001354
Iteration 12/1000 | Loss: 0.00001347
Iteration 13/1000 | Loss: 0.00001346
Iteration 14/1000 | Loss: 0.00001343
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001342
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001335
Iteration 19/1000 | Loss: 0.00001334
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001333
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001328
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001322
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001322
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001319
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001318
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001315
Iteration 51/1000 | Loss: 0.00001315
Iteration 52/1000 | Loss: 0.00001314
Iteration 53/1000 | Loss: 0.00001313
Iteration 54/1000 | Loss: 0.00001313
Iteration 55/1000 | Loss: 0.00001312
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001311
Iteration 58/1000 | Loss: 0.00001311
Iteration 59/1000 | Loss: 0.00001311
Iteration 60/1000 | Loss: 0.00001310
Iteration 61/1000 | Loss: 0.00001310
Iteration 62/1000 | Loss: 0.00001310
Iteration 63/1000 | Loss: 0.00001310
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001309
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001307
Iteration 69/1000 | Loss: 0.00001307
Iteration 70/1000 | Loss: 0.00001307
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001306
Iteration 73/1000 | Loss: 0.00001306
Iteration 74/1000 | Loss: 0.00001306
Iteration 75/1000 | Loss: 0.00001306
Iteration 76/1000 | Loss: 0.00001306
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001305
Iteration 79/1000 | Loss: 0.00001305
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001304
Iteration 83/1000 | Loss: 0.00001304
Iteration 84/1000 | Loss: 0.00001304
Iteration 85/1000 | Loss: 0.00001304
Iteration 86/1000 | Loss: 0.00001304
Iteration 87/1000 | Loss: 0.00001304
Iteration 88/1000 | Loss: 0.00001304
Iteration 89/1000 | Loss: 0.00001303
Iteration 90/1000 | Loss: 0.00001303
Iteration 91/1000 | Loss: 0.00001303
Iteration 92/1000 | Loss: 0.00001303
Iteration 93/1000 | Loss: 0.00001303
Iteration 94/1000 | Loss: 0.00001302
Iteration 95/1000 | Loss: 0.00001302
Iteration 96/1000 | Loss: 0.00001302
Iteration 97/1000 | Loss: 0.00001301
Iteration 98/1000 | Loss: 0.00001301
Iteration 99/1000 | Loss: 0.00001301
Iteration 100/1000 | Loss: 0.00001301
Iteration 101/1000 | Loss: 0.00001301
Iteration 102/1000 | Loss: 0.00001301
Iteration 103/1000 | Loss: 0.00001301
Iteration 104/1000 | Loss: 0.00001301
Iteration 105/1000 | Loss: 0.00001301
Iteration 106/1000 | Loss: 0.00001301
Iteration 107/1000 | Loss: 0.00001301
Iteration 108/1000 | Loss: 0.00001300
Iteration 109/1000 | Loss: 0.00001300
Iteration 110/1000 | Loss: 0.00001300
Iteration 111/1000 | Loss: 0.00001300
Iteration 112/1000 | Loss: 0.00001300
Iteration 113/1000 | Loss: 0.00001300
Iteration 114/1000 | Loss: 0.00001300
Iteration 115/1000 | Loss: 0.00001299
Iteration 116/1000 | Loss: 0.00001299
Iteration 117/1000 | Loss: 0.00001299
Iteration 118/1000 | Loss: 0.00001299
Iteration 119/1000 | Loss: 0.00001299
Iteration 120/1000 | Loss: 0.00001299
Iteration 121/1000 | Loss: 0.00001299
Iteration 122/1000 | Loss: 0.00001299
Iteration 123/1000 | Loss: 0.00001299
Iteration 124/1000 | Loss: 0.00001299
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001298
Iteration 129/1000 | Loss: 0.00001298
Iteration 130/1000 | Loss: 0.00001298
Iteration 131/1000 | Loss: 0.00001298
Iteration 132/1000 | Loss: 0.00001298
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001298
Iteration 138/1000 | Loss: 0.00001298
Iteration 139/1000 | Loss: 0.00001298
Iteration 140/1000 | Loss: 0.00001298
Iteration 141/1000 | Loss: 0.00001298
Iteration 142/1000 | Loss: 0.00001298
Iteration 143/1000 | Loss: 0.00001298
Iteration 144/1000 | Loss: 0.00001298
Iteration 145/1000 | Loss: 0.00001298
Iteration 146/1000 | Loss: 0.00001298
Iteration 147/1000 | Loss: 0.00001298
Iteration 148/1000 | Loss: 0.00001298
Iteration 149/1000 | Loss: 0.00001298
Iteration 150/1000 | Loss: 0.00001298
Iteration 151/1000 | Loss: 0.00001298
Iteration 152/1000 | Loss: 0.00001298
Iteration 153/1000 | Loss: 0.00001298
Iteration 154/1000 | Loss: 0.00001298
Iteration 155/1000 | Loss: 0.00001298
Iteration 156/1000 | Loss: 0.00001298
Iteration 157/1000 | Loss: 0.00001298
Iteration 158/1000 | Loss: 0.00001298
Iteration 159/1000 | Loss: 0.00001298
Iteration 160/1000 | Loss: 0.00001298
Iteration 161/1000 | Loss: 0.00001298
Iteration 162/1000 | Loss: 0.00001298
Iteration 163/1000 | Loss: 0.00001298
Iteration 164/1000 | Loss: 0.00001298
Iteration 165/1000 | Loss: 0.00001298
Iteration 166/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.2976351172255818e-05, 1.2976351172255818e-05, 1.2976351172255818e-05, 1.2976351172255818e-05, 1.2976351172255818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2976351172255818e-05

Optimization complete. Final v2v error: 2.9586915969848633 mm

Highest mean error: 3.5376737117767334 mm for frame 72

Lowest mean error: 2.525026559829712 mm for frame 140

Saving results

Total time: 34.40726447105408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863265
Iteration 2/25 | Loss: 0.00092691
Iteration 3/25 | Loss: 0.00075533
Iteration 4/25 | Loss: 0.00073317
Iteration 5/25 | Loss: 0.00072890
Iteration 6/25 | Loss: 0.00072742
Iteration 7/25 | Loss: 0.00072742
Iteration 8/25 | Loss: 0.00072742
Iteration 9/25 | Loss: 0.00072742
Iteration 10/25 | Loss: 0.00072742
Iteration 11/25 | Loss: 0.00072742
Iteration 12/25 | Loss: 0.00072742
Iteration 13/25 | Loss: 0.00072742
Iteration 14/25 | Loss: 0.00072742
Iteration 15/25 | Loss: 0.00072742
Iteration 16/25 | Loss: 0.00072742
Iteration 17/25 | Loss: 0.00072742
Iteration 18/25 | Loss: 0.00072742
Iteration 19/25 | Loss: 0.00072742
Iteration 20/25 | Loss: 0.00072742
Iteration 21/25 | Loss: 0.00072742
Iteration 22/25 | Loss: 0.00072742
Iteration 23/25 | Loss: 0.00072742
Iteration 24/25 | Loss: 0.00072742
Iteration 25/25 | Loss: 0.00072742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007274241070263088, 0.0007274241070263088, 0.0007274241070263088, 0.0007274241070263088, 0.0007274241070263088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007274241070263088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58807516
Iteration 2/25 | Loss: 0.00114120
Iteration 3/25 | Loss: 0.00114119
Iteration 4/25 | Loss: 0.00114119
Iteration 5/25 | Loss: 0.00114119
Iteration 6/25 | Loss: 0.00114119
Iteration 7/25 | Loss: 0.00114119
Iteration 8/25 | Loss: 0.00114119
Iteration 9/25 | Loss: 0.00114119
Iteration 10/25 | Loss: 0.00114119
Iteration 11/25 | Loss: 0.00114119
Iteration 12/25 | Loss: 0.00114119
Iteration 13/25 | Loss: 0.00114119
Iteration 14/25 | Loss: 0.00114119
Iteration 15/25 | Loss: 0.00114119
Iteration 16/25 | Loss: 0.00114119
Iteration 17/25 | Loss: 0.00114119
Iteration 18/25 | Loss: 0.00114119
Iteration 19/25 | Loss: 0.00114119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011411909945309162, 0.0011411909945309162, 0.0011411909945309162, 0.0011411909945309162, 0.0011411909945309162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011411909945309162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114119
Iteration 2/1000 | Loss: 0.00002216
Iteration 3/1000 | Loss: 0.00001464
Iteration 4/1000 | Loss: 0.00001319
Iteration 5/1000 | Loss: 0.00001204
Iteration 6/1000 | Loss: 0.00001162
Iteration 7/1000 | Loss: 0.00001136
Iteration 8/1000 | Loss: 0.00001123
Iteration 9/1000 | Loss: 0.00001108
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001100
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001094
Iteration 14/1000 | Loss: 0.00001091
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001090
Iteration 17/1000 | Loss: 0.00001090
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001086
Iteration 21/1000 | Loss: 0.00001085
Iteration 22/1000 | Loss: 0.00001085
Iteration 23/1000 | Loss: 0.00001084
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001081
Iteration 29/1000 | Loss: 0.00001081
Iteration 30/1000 | Loss: 0.00001081
Iteration 31/1000 | Loss: 0.00001080
Iteration 32/1000 | Loss: 0.00001080
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001080
Iteration 36/1000 | Loss: 0.00001080
Iteration 37/1000 | Loss: 0.00001080
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001077
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001076
Iteration 45/1000 | Loss: 0.00001076
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001072
Iteration 55/1000 | Loss: 0.00001072
Iteration 56/1000 | Loss: 0.00001072
Iteration 57/1000 | Loss: 0.00001072
Iteration 58/1000 | Loss: 0.00001071
Iteration 59/1000 | Loss: 0.00001071
Iteration 60/1000 | Loss: 0.00001071
Iteration 61/1000 | Loss: 0.00001071
Iteration 62/1000 | Loss: 0.00001071
Iteration 63/1000 | Loss: 0.00001070
Iteration 64/1000 | Loss: 0.00001070
Iteration 65/1000 | Loss: 0.00001070
Iteration 66/1000 | Loss: 0.00001070
Iteration 67/1000 | Loss: 0.00001070
Iteration 68/1000 | Loss: 0.00001070
Iteration 69/1000 | Loss: 0.00001070
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001069
Iteration 73/1000 | Loss: 0.00001069
Iteration 74/1000 | Loss: 0.00001069
Iteration 75/1000 | Loss: 0.00001069
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001069
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001068
Iteration 80/1000 | Loss: 0.00001068
Iteration 81/1000 | Loss: 0.00001068
Iteration 82/1000 | Loss: 0.00001068
Iteration 83/1000 | Loss: 0.00001067
Iteration 84/1000 | Loss: 0.00001067
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001066
Iteration 87/1000 | Loss: 0.00001066
Iteration 88/1000 | Loss: 0.00001065
Iteration 89/1000 | Loss: 0.00001065
Iteration 90/1000 | Loss: 0.00001065
Iteration 91/1000 | Loss: 0.00001063
Iteration 92/1000 | Loss: 0.00001063
Iteration 93/1000 | Loss: 0.00001063
Iteration 94/1000 | Loss: 0.00001062
Iteration 95/1000 | Loss: 0.00001062
Iteration 96/1000 | Loss: 0.00001062
Iteration 97/1000 | Loss: 0.00001062
Iteration 98/1000 | Loss: 0.00001062
Iteration 99/1000 | Loss: 0.00001062
Iteration 100/1000 | Loss: 0.00001062
Iteration 101/1000 | Loss: 0.00001062
Iteration 102/1000 | Loss: 0.00001062
Iteration 103/1000 | Loss: 0.00001062
Iteration 104/1000 | Loss: 0.00001062
Iteration 105/1000 | Loss: 0.00001062
Iteration 106/1000 | Loss: 0.00001062
Iteration 107/1000 | Loss: 0.00001062
Iteration 108/1000 | Loss: 0.00001061
Iteration 109/1000 | Loss: 0.00001061
Iteration 110/1000 | Loss: 0.00001061
Iteration 111/1000 | Loss: 0.00001061
Iteration 112/1000 | Loss: 0.00001061
Iteration 113/1000 | Loss: 0.00001060
Iteration 114/1000 | Loss: 0.00001060
Iteration 115/1000 | Loss: 0.00001060
Iteration 116/1000 | Loss: 0.00001060
Iteration 117/1000 | Loss: 0.00001060
Iteration 118/1000 | Loss: 0.00001060
Iteration 119/1000 | Loss: 0.00001059
Iteration 120/1000 | Loss: 0.00001059
Iteration 121/1000 | Loss: 0.00001059
Iteration 122/1000 | Loss: 0.00001059
Iteration 123/1000 | Loss: 0.00001059
Iteration 124/1000 | Loss: 0.00001059
Iteration 125/1000 | Loss: 0.00001058
Iteration 126/1000 | Loss: 0.00001058
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001058
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001057
Iteration 135/1000 | Loss: 0.00001057
Iteration 136/1000 | Loss: 0.00001057
Iteration 137/1000 | Loss: 0.00001057
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001056
Iteration 141/1000 | Loss: 0.00001056
Iteration 142/1000 | Loss: 0.00001056
Iteration 143/1000 | Loss: 0.00001056
Iteration 144/1000 | Loss: 0.00001056
Iteration 145/1000 | Loss: 0.00001056
Iteration 146/1000 | Loss: 0.00001056
Iteration 147/1000 | Loss: 0.00001055
Iteration 148/1000 | Loss: 0.00001055
Iteration 149/1000 | Loss: 0.00001055
Iteration 150/1000 | Loss: 0.00001055
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Iteration 153/1000 | Loss: 0.00001054
Iteration 154/1000 | Loss: 0.00001054
Iteration 155/1000 | Loss: 0.00001054
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001054
Iteration 159/1000 | Loss: 0.00001054
Iteration 160/1000 | Loss: 0.00001054
Iteration 161/1000 | Loss: 0.00001054
Iteration 162/1000 | Loss: 0.00001054
Iteration 163/1000 | Loss: 0.00001054
Iteration 164/1000 | Loss: 0.00001054
Iteration 165/1000 | Loss: 0.00001053
Iteration 166/1000 | Loss: 0.00001053
Iteration 167/1000 | Loss: 0.00001053
Iteration 168/1000 | Loss: 0.00001053
Iteration 169/1000 | Loss: 0.00001053
Iteration 170/1000 | Loss: 0.00001053
Iteration 171/1000 | Loss: 0.00001053
Iteration 172/1000 | Loss: 0.00001053
Iteration 173/1000 | Loss: 0.00001053
Iteration 174/1000 | Loss: 0.00001053
Iteration 175/1000 | Loss: 0.00001053
Iteration 176/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.0532407941354904e-05, 1.0532407941354904e-05, 1.0532407941354904e-05, 1.0532407941354904e-05, 1.0532407941354904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0532407941354904e-05

Optimization complete. Final v2v error: 2.754926919937134 mm

Highest mean error: 2.9865946769714355 mm for frame 101

Lowest mean error: 2.595733165740967 mm for frame 28

Saving results

Total time: 42.193742513656616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864549
Iteration 2/25 | Loss: 0.00096762
Iteration 3/25 | Loss: 0.00077573
Iteration 4/25 | Loss: 0.00075020
Iteration 5/25 | Loss: 0.00074408
Iteration 6/25 | Loss: 0.00074177
Iteration 7/25 | Loss: 0.00074125
Iteration 8/25 | Loss: 0.00074125
Iteration 9/25 | Loss: 0.00074125
Iteration 10/25 | Loss: 0.00074125
Iteration 11/25 | Loss: 0.00074125
Iteration 12/25 | Loss: 0.00074125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007412515114992857, 0.0007412515114992857, 0.0007412515114992857, 0.0007412515114992857, 0.0007412515114992857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007412515114992857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58066642
Iteration 2/25 | Loss: 0.00117010
Iteration 3/25 | Loss: 0.00117007
Iteration 4/25 | Loss: 0.00117007
Iteration 5/25 | Loss: 0.00117007
Iteration 6/25 | Loss: 0.00117007
Iteration 7/25 | Loss: 0.00117007
Iteration 8/25 | Loss: 0.00117007
Iteration 9/25 | Loss: 0.00117007
Iteration 10/25 | Loss: 0.00117007
Iteration 11/25 | Loss: 0.00117007
Iteration 12/25 | Loss: 0.00117007
Iteration 13/25 | Loss: 0.00117007
Iteration 14/25 | Loss: 0.00117007
Iteration 15/25 | Loss: 0.00117007
Iteration 16/25 | Loss: 0.00117007
Iteration 17/25 | Loss: 0.00117007
Iteration 18/25 | Loss: 0.00117007
Iteration 19/25 | Loss: 0.00117007
Iteration 20/25 | Loss: 0.00117007
Iteration 21/25 | Loss: 0.00117007
Iteration 22/25 | Loss: 0.00117007
Iteration 23/25 | Loss: 0.00117007
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011700683971866965, 0.0011700683971866965, 0.0011700683971866965, 0.0011700683971866965, 0.0011700683971866965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011700683971866965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117007
Iteration 2/1000 | Loss: 0.00002048
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001240
Iteration 6/1000 | Loss: 0.00001187
Iteration 7/1000 | Loss: 0.00001141
Iteration 8/1000 | Loss: 0.00001124
Iteration 9/1000 | Loss: 0.00001121
Iteration 10/1000 | Loss: 0.00001121
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001114
Iteration 13/1000 | Loss: 0.00001102
Iteration 14/1000 | Loss: 0.00001099
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001094
Iteration 17/1000 | Loss: 0.00001093
Iteration 18/1000 | Loss: 0.00001093
Iteration 19/1000 | Loss: 0.00001093
Iteration 20/1000 | Loss: 0.00001093
Iteration 21/1000 | Loss: 0.00001092
Iteration 22/1000 | Loss: 0.00001092
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001087
Iteration 27/1000 | Loss: 0.00001087
Iteration 28/1000 | Loss: 0.00001087
Iteration 29/1000 | Loss: 0.00001087
Iteration 30/1000 | Loss: 0.00001086
Iteration 31/1000 | Loss: 0.00001086
Iteration 32/1000 | Loss: 0.00001086
Iteration 33/1000 | Loss: 0.00001086
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001086
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001085
Iteration 39/1000 | Loss: 0.00001082
Iteration 40/1000 | Loss: 0.00001082
Iteration 41/1000 | Loss: 0.00001082
Iteration 42/1000 | Loss: 0.00001082
Iteration 43/1000 | Loss: 0.00001081
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001081
Iteration 49/1000 | Loss: 0.00001081
Iteration 50/1000 | Loss: 0.00001081
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001080
Iteration 53/1000 | Loss: 0.00001080
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001080
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001079
Iteration 61/1000 | Loss: 0.00001078
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001078
Iteration 64/1000 | Loss: 0.00001078
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001077
Iteration 68/1000 | Loss: 0.00001077
Iteration 69/1000 | Loss: 0.00001077
Iteration 70/1000 | Loss: 0.00001077
Iteration 71/1000 | Loss: 0.00001077
Iteration 72/1000 | Loss: 0.00001077
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001076
Iteration 75/1000 | Loss: 0.00001076
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001075
Iteration 80/1000 | Loss: 0.00001075
Iteration 81/1000 | Loss: 0.00001075
Iteration 82/1000 | Loss: 0.00001075
Iteration 83/1000 | Loss: 0.00001075
Iteration 84/1000 | Loss: 0.00001075
Iteration 85/1000 | Loss: 0.00001075
Iteration 86/1000 | Loss: 0.00001075
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001074
Iteration 89/1000 | Loss: 0.00001074
Iteration 90/1000 | Loss: 0.00001074
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001073
Iteration 94/1000 | Loss: 0.00001073
Iteration 95/1000 | Loss: 0.00001073
Iteration 96/1000 | Loss: 0.00001073
Iteration 97/1000 | Loss: 0.00001073
Iteration 98/1000 | Loss: 0.00001073
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001073
Iteration 101/1000 | Loss: 0.00001073
Iteration 102/1000 | Loss: 0.00001073
Iteration 103/1000 | Loss: 0.00001073
Iteration 104/1000 | Loss: 0.00001073
Iteration 105/1000 | Loss: 0.00001073
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001072
Iteration 108/1000 | Loss: 0.00001072
Iteration 109/1000 | Loss: 0.00001072
Iteration 110/1000 | Loss: 0.00001072
Iteration 111/1000 | Loss: 0.00001072
Iteration 112/1000 | Loss: 0.00001072
Iteration 113/1000 | Loss: 0.00001072
Iteration 114/1000 | Loss: 0.00001072
Iteration 115/1000 | Loss: 0.00001072
Iteration 116/1000 | Loss: 0.00001072
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001071
Iteration 130/1000 | Loss: 0.00001071
Iteration 131/1000 | Loss: 0.00001071
Iteration 132/1000 | Loss: 0.00001071
Iteration 133/1000 | Loss: 0.00001071
Iteration 134/1000 | Loss: 0.00001071
Iteration 135/1000 | Loss: 0.00001071
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001071
Iteration 138/1000 | Loss: 0.00001071
Iteration 139/1000 | Loss: 0.00001071
Iteration 140/1000 | Loss: 0.00001071
Iteration 141/1000 | Loss: 0.00001070
Iteration 142/1000 | Loss: 0.00001070
Iteration 143/1000 | Loss: 0.00001070
Iteration 144/1000 | Loss: 0.00001070
Iteration 145/1000 | Loss: 0.00001070
Iteration 146/1000 | Loss: 0.00001070
Iteration 147/1000 | Loss: 0.00001070
Iteration 148/1000 | Loss: 0.00001070
Iteration 149/1000 | Loss: 0.00001070
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001070
Iteration 154/1000 | Loss: 0.00001070
Iteration 155/1000 | Loss: 0.00001070
Iteration 156/1000 | Loss: 0.00001070
Iteration 157/1000 | Loss: 0.00001070
Iteration 158/1000 | Loss: 0.00001070
Iteration 159/1000 | Loss: 0.00001070
Iteration 160/1000 | Loss: 0.00001070
Iteration 161/1000 | Loss: 0.00001070
Iteration 162/1000 | Loss: 0.00001070
Iteration 163/1000 | Loss: 0.00001070
Iteration 164/1000 | Loss: 0.00001070
Iteration 165/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.0698030564526562e-05, 1.0698030564526562e-05, 1.0698030564526562e-05, 1.0698030564526562e-05, 1.0698030564526562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0698030564526562e-05

Optimization complete. Final v2v error: 2.778860092163086 mm

Highest mean error: 2.980214834213257 mm for frame 121

Lowest mean error: 2.6715011596679688 mm for frame 24

Saving results

Total time: 33.400890588760376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802455
Iteration 2/25 | Loss: 0.00195100
Iteration 3/25 | Loss: 0.00112856
Iteration 4/25 | Loss: 0.00092335
Iteration 5/25 | Loss: 0.00086602
Iteration 6/25 | Loss: 0.00085777
Iteration 7/25 | Loss: 0.00084750
Iteration 8/25 | Loss: 0.00085579
Iteration 9/25 | Loss: 0.00084862
Iteration 10/25 | Loss: 0.00084758
Iteration 11/25 | Loss: 0.00084364
Iteration 12/25 | Loss: 0.00084187
Iteration 13/25 | Loss: 0.00083877
Iteration 14/25 | Loss: 0.00083815
Iteration 15/25 | Loss: 0.00083880
Iteration 16/25 | Loss: 0.00083846
Iteration 17/25 | Loss: 0.00083907
Iteration 18/25 | Loss: 0.00083803
Iteration 19/25 | Loss: 0.00083933
Iteration 20/25 | Loss: 0.00083918
Iteration 21/25 | Loss: 0.00083505
Iteration 22/25 | Loss: 0.00083470
Iteration 23/25 | Loss: 0.00083459
Iteration 24/25 | Loss: 0.00083450
Iteration 25/25 | Loss: 0.00083450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.28673124
Iteration 2/25 | Loss: 0.00137030
Iteration 3/25 | Loss: 0.00137027
Iteration 4/25 | Loss: 0.00137027
Iteration 5/25 | Loss: 0.00137027
Iteration 6/25 | Loss: 0.00137027
Iteration 7/25 | Loss: 0.00137027
Iteration 8/25 | Loss: 0.00137027
Iteration 9/25 | Loss: 0.00137027
Iteration 10/25 | Loss: 0.00137027
Iteration 11/25 | Loss: 0.00137027
Iteration 12/25 | Loss: 0.00137027
Iteration 13/25 | Loss: 0.00137027
Iteration 14/25 | Loss: 0.00137027
Iteration 15/25 | Loss: 0.00137027
Iteration 16/25 | Loss: 0.00137027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013702715514227748, 0.0013702715514227748, 0.0013702715514227748, 0.0013702715514227748, 0.0013702715514227748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013702715514227748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137027
Iteration 2/1000 | Loss: 0.00019329
Iteration 3/1000 | Loss: 0.00002719
Iteration 4/1000 | Loss: 0.00002208
Iteration 5/1000 | Loss: 0.00001965
Iteration 6/1000 | Loss: 0.00001818
Iteration 7/1000 | Loss: 0.00001725
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00012887
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001629
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001424
Iteration 15/1000 | Loss: 0.00001400
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001399
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001393
Iteration 22/1000 | Loss: 0.00001390
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001388
Iteration 25/1000 | Loss: 0.00001387
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001386
Iteration 28/1000 | Loss: 0.00001386
Iteration 29/1000 | Loss: 0.00001385
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001384
Iteration 33/1000 | Loss: 0.00001383
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001365
Iteration 47/1000 | Loss: 0.00001365
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001362
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001353
Iteration 70/1000 | Loss: 0.00001353
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001352
Iteration 73/1000 | Loss: 0.00001352
Iteration 74/1000 | Loss: 0.00001351
Iteration 75/1000 | Loss: 0.00001351
Iteration 76/1000 | Loss: 0.00001351
Iteration 77/1000 | Loss: 0.00001351
Iteration 78/1000 | Loss: 0.00001351
Iteration 79/1000 | Loss: 0.00001351
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001351
Iteration 85/1000 | Loss: 0.00001351
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001351
Iteration 91/1000 | Loss: 0.00001351
Iteration 92/1000 | Loss: 0.00001351
Iteration 93/1000 | Loss: 0.00001351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.351085211354075e-05, 1.351085211354075e-05, 1.351085211354075e-05, 1.351085211354075e-05, 1.351085211354075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.351085211354075e-05

Optimization complete. Final v2v error: 3.048727035522461 mm

Highest mean error: 6.352560997009277 mm for frame 72

Lowest mean error: 2.785076141357422 mm for frame 200

Saving results

Total time: 82.5660490989685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815356
Iteration 2/25 | Loss: 0.00157175
Iteration 3/25 | Loss: 0.00107877
Iteration 4/25 | Loss: 0.00098274
Iteration 5/25 | Loss: 0.00095382
Iteration 6/25 | Loss: 0.00094518
Iteration 7/25 | Loss: 0.00094780
Iteration 8/25 | Loss: 0.00094619
Iteration 9/25 | Loss: 0.00094214
Iteration 10/25 | Loss: 0.00094208
Iteration 11/25 | Loss: 0.00093870
Iteration 12/25 | Loss: 0.00093117
Iteration 13/25 | Loss: 0.00092621
Iteration 14/25 | Loss: 0.00092026
Iteration 15/25 | Loss: 0.00091821
Iteration 16/25 | Loss: 0.00091718
Iteration 17/25 | Loss: 0.00091681
Iteration 18/25 | Loss: 0.00091663
Iteration 19/25 | Loss: 0.00091662
Iteration 20/25 | Loss: 0.00091662
Iteration 21/25 | Loss: 0.00091662
Iteration 22/25 | Loss: 0.00091662
Iteration 23/25 | Loss: 0.00091662
Iteration 24/25 | Loss: 0.00091661
Iteration 25/25 | Loss: 0.00091661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02701426
Iteration 2/25 | Loss: 0.00139650
Iteration 3/25 | Loss: 0.00139646
Iteration 4/25 | Loss: 0.00139645
Iteration 5/25 | Loss: 0.00139645
Iteration 6/25 | Loss: 0.00139645
Iteration 7/25 | Loss: 0.00139645
Iteration 8/25 | Loss: 0.00139645
Iteration 9/25 | Loss: 0.00139645
Iteration 10/25 | Loss: 0.00139645
Iteration 11/25 | Loss: 0.00139645
Iteration 12/25 | Loss: 0.00139645
Iteration 13/25 | Loss: 0.00139645
Iteration 14/25 | Loss: 0.00139645
Iteration 15/25 | Loss: 0.00139645
Iteration 16/25 | Loss: 0.00139645
Iteration 17/25 | Loss: 0.00139645
Iteration 18/25 | Loss: 0.00139645
Iteration 19/25 | Loss: 0.00139645
Iteration 20/25 | Loss: 0.00139645
Iteration 21/25 | Loss: 0.00139645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013964527752250433, 0.0013964527752250433, 0.0013964527752250433, 0.0013964527752250433, 0.0013964527752250433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013964527752250433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139645
Iteration 2/1000 | Loss: 0.00005940
Iteration 3/1000 | Loss: 0.00004454
Iteration 4/1000 | Loss: 0.00003849
Iteration 5/1000 | Loss: 0.00003608
Iteration 6/1000 | Loss: 0.00003457
Iteration 7/1000 | Loss: 0.00036995
Iteration 8/1000 | Loss: 0.00047291
Iteration 9/1000 | Loss: 0.00004532
Iteration 10/1000 | Loss: 0.00003590
Iteration 11/1000 | Loss: 0.00003201
Iteration 12/1000 | Loss: 0.00003010
Iteration 13/1000 | Loss: 0.00002892
Iteration 14/1000 | Loss: 0.00002833
Iteration 15/1000 | Loss: 0.00002780
Iteration 16/1000 | Loss: 0.00002757
Iteration 17/1000 | Loss: 0.00002733
Iteration 18/1000 | Loss: 0.00002711
Iteration 19/1000 | Loss: 0.00002697
Iteration 20/1000 | Loss: 0.00002689
Iteration 21/1000 | Loss: 0.00002685
Iteration 22/1000 | Loss: 0.00002684
Iteration 23/1000 | Loss: 0.00002683
Iteration 24/1000 | Loss: 0.00002681
Iteration 25/1000 | Loss: 0.00002668
Iteration 26/1000 | Loss: 0.00002667
Iteration 27/1000 | Loss: 0.00002664
Iteration 28/1000 | Loss: 0.00002663
Iteration 29/1000 | Loss: 0.00002663
Iteration 30/1000 | Loss: 0.00002662
Iteration 31/1000 | Loss: 0.00002661
Iteration 32/1000 | Loss: 0.00002661
Iteration 33/1000 | Loss: 0.00002661
Iteration 34/1000 | Loss: 0.00002661
Iteration 35/1000 | Loss: 0.00002660
Iteration 36/1000 | Loss: 0.00002660
Iteration 37/1000 | Loss: 0.00002659
Iteration 38/1000 | Loss: 0.00002659
Iteration 39/1000 | Loss: 0.00002658
Iteration 40/1000 | Loss: 0.00002658
Iteration 41/1000 | Loss: 0.00002658
Iteration 42/1000 | Loss: 0.00002658
Iteration 43/1000 | Loss: 0.00002657
Iteration 44/1000 | Loss: 0.00002657
Iteration 45/1000 | Loss: 0.00002657
Iteration 46/1000 | Loss: 0.00002657
Iteration 47/1000 | Loss: 0.00002656
Iteration 48/1000 | Loss: 0.00002656
Iteration 49/1000 | Loss: 0.00002656
Iteration 50/1000 | Loss: 0.00002656
Iteration 51/1000 | Loss: 0.00002656
Iteration 52/1000 | Loss: 0.00002656
Iteration 53/1000 | Loss: 0.00002655
Iteration 54/1000 | Loss: 0.00002655
Iteration 55/1000 | Loss: 0.00002655
Iteration 56/1000 | Loss: 0.00002655
Iteration 57/1000 | Loss: 0.00002655
Iteration 58/1000 | Loss: 0.00002655
Iteration 59/1000 | Loss: 0.00002655
Iteration 60/1000 | Loss: 0.00002655
Iteration 61/1000 | Loss: 0.00002655
Iteration 62/1000 | Loss: 0.00002654
Iteration 63/1000 | Loss: 0.00002654
Iteration 64/1000 | Loss: 0.00002654
Iteration 65/1000 | Loss: 0.00002653
Iteration 66/1000 | Loss: 0.00002653
Iteration 67/1000 | Loss: 0.00002652
Iteration 68/1000 | Loss: 0.00002652
Iteration 69/1000 | Loss: 0.00002652
Iteration 70/1000 | Loss: 0.00002652
Iteration 71/1000 | Loss: 0.00002652
Iteration 72/1000 | Loss: 0.00002652
Iteration 73/1000 | Loss: 0.00002652
Iteration 74/1000 | Loss: 0.00002652
Iteration 75/1000 | Loss: 0.00002652
Iteration 76/1000 | Loss: 0.00002652
Iteration 77/1000 | Loss: 0.00002652
Iteration 78/1000 | Loss: 0.00002652
Iteration 79/1000 | Loss: 0.00002651
Iteration 80/1000 | Loss: 0.00002651
Iteration 81/1000 | Loss: 0.00002651
Iteration 82/1000 | Loss: 0.00002650
Iteration 83/1000 | Loss: 0.00002650
Iteration 84/1000 | Loss: 0.00002650
Iteration 85/1000 | Loss: 0.00002650
Iteration 86/1000 | Loss: 0.00002650
Iteration 87/1000 | Loss: 0.00002650
Iteration 88/1000 | Loss: 0.00002650
Iteration 89/1000 | Loss: 0.00002650
Iteration 90/1000 | Loss: 0.00002650
Iteration 91/1000 | Loss: 0.00002650
Iteration 92/1000 | Loss: 0.00002650
Iteration 93/1000 | Loss: 0.00002650
Iteration 94/1000 | Loss: 0.00002650
Iteration 95/1000 | Loss: 0.00002650
Iteration 96/1000 | Loss: 0.00002650
Iteration 97/1000 | Loss: 0.00002650
Iteration 98/1000 | Loss: 0.00002650
Iteration 99/1000 | Loss: 0.00002649
Iteration 100/1000 | Loss: 0.00002649
Iteration 101/1000 | Loss: 0.00002649
Iteration 102/1000 | Loss: 0.00002649
Iteration 103/1000 | Loss: 0.00002649
Iteration 104/1000 | Loss: 0.00002649
Iteration 105/1000 | Loss: 0.00002648
Iteration 106/1000 | Loss: 0.00002648
Iteration 107/1000 | Loss: 0.00002648
Iteration 108/1000 | Loss: 0.00002648
Iteration 109/1000 | Loss: 0.00002648
Iteration 110/1000 | Loss: 0.00002648
Iteration 111/1000 | Loss: 0.00002647
Iteration 112/1000 | Loss: 0.00002647
Iteration 113/1000 | Loss: 0.00002647
Iteration 114/1000 | Loss: 0.00002647
Iteration 115/1000 | Loss: 0.00002647
Iteration 116/1000 | Loss: 0.00002647
Iteration 117/1000 | Loss: 0.00002647
Iteration 118/1000 | Loss: 0.00002647
Iteration 119/1000 | Loss: 0.00002646
Iteration 120/1000 | Loss: 0.00002646
Iteration 121/1000 | Loss: 0.00002646
Iteration 122/1000 | Loss: 0.00002646
Iteration 123/1000 | Loss: 0.00002646
Iteration 124/1000 | Loss: 0.00002646
Iteration 125/1000 | Loss: 0.00002646
Iteration 126/1000 | Loss: 0.00002646
Iteration 127/1000 | Loss: 0.00002646
Iteration 128/1000 | Loss: 0.00002646
Iteration 129/1000 | Loss: 0.00002645
Iteration 130/1000 | Loss: 0.00002645
Iteration 131/1000 | Loss: 0.00002645
Iteration 132/1000 | Loss: 0.00002645
Iteration 133/1000 | Loss: 0.00002645
Iteration 134/1000 | Loss: 0.00002645
Iteration 135/1000 | Loss: 0.00002645
Iteration 136/1000 | Loss: 0.00002645
Iteration 137/1000 | Loss: 0.00002645
Iteration 138/1000 | Loss: 0.00002645
Iteration 139/1000 | Loss: 0.00002645
Iteration 140/1000 | Loss: 0.00002645
Iteration 141/1000 | Loss: 0.00002644
Iteration 142/1000 | Loss: 0.00002644
Iteration 143/1000 | Loss: 0.00002644
Iteration 144/1000 | Loss: 0.00002644
Iteration 145/1000 | Loss: 0.00002644
Iteration 146/1000 | Loss: 0.00002644
Iteration 147/1000 | Loss: 0.00002644
Iteration 148/1000 | Loss: 0.00002644
Iteration 149/1000 | Loss: 0.00002644
Iteration 150/1000 | Loss: 0.00002644
Iteration 151/1000 | Loss: 0.00002644
Iteration 152/1000 | Loss: 0.00002644
Iteration 153/1000 | Loss: 0.00002644
Iteration 154/1000 | Loss: 0.00002644
Iteration 155/1000 | Loss: 0.00002644
Iteration 156/1000 | Loss: 0.00002644
Iteration 157/1000 | Loss: 0.00002644
Iteration 158/1000 | Loss: 0.00002644
Iteration 159/1000 | Loss: 0.00002644
Iteration 160/1000 | Loss: 0.00002644
Iteration 161/1000 | Loss: 0.00002644
Iteration 162/1000 | Loss: 0.00002643
Iteration 163/1000 | Loss: 0.00002643
Iteration 164/1000 | Loss: 0.00002643
Iteration 165/1000 | Loss: 0.00002643
Iteration 166/1000 | Loss: 0.00002643
Iteration 167/1000 | Loss: 0.00002643
Iteration 168/1000 | Loss: 0.00002643
Iteration 169/1000 | Loss: 0.00002643
Iteration 170/1000 | Loss: 0.00002643
Iteration 171/1000 | Loss: 0.00002643
Iteration 172/1000 | Loss: 0.00002643
Iteration 173/1000 | Loss: 0.00002643
Iteration 174/1000 | Loss: 0.00002643
Iteration 175/1000 | Loss: 0.00002643
Iteration 176/1000 | Loss: 0.00002643
Iteration 177/1000 | Loss: 0.00002643
Iteration 178/1000 | Loss: 0.00002643
Iteration 179/1000 | Loss: 0.00002643
Iteration 180/1000 | Loss: 0.00002643
Iteration 181/1000 | Loss: 0.00002643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.6427218472235836e-05, 2.6427218472235836e-05, 2.6427218472235836e-05, 2.6427218472235836e-05, 2.6427218472235836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6427218472235836e-05

Optimization complete. Final v2v error: 4.1547417640686035 mm

Highest mean error: 10.2442626953125 mm for frame 2

Lowest mean error: 3.6101763248443604 mm for frame 99

Saving results

Total time: 82.0547730922699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784997
Iteration 2/25 | Loss: 0.00108140
Iteration 3/25 | Loss: 0.00088677
Iteration 4/25 | Loss: 0.00081857
Iteration 5/25 | Loss: 0.00079951
Iteration 6/25 | Loss: 0.00079569
Iteration 7/25 | Loss: 0.00079407
Iteration 8/25 | Loss: 0.00079380
Iteration 9/25 | Loss: 0.00079380
Iteration 10/25 | Loss: 0.00079380
Iteration 11/25 | Loss: 0.00079380
Iteration 12/25 | Loss: 0.00079380
Iteration 13/25 | Loss: 0.00079380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007937955670058727, 0.0007937955670058727, 0.0007937955670058727, 0.0007937955670058727, 0.0007937955670058727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007937955670058727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61388266
Iteration 2/25 | Loss: 0.00148793
Iteration 3/25 | Loss: 0.00148793
Iteration 4/25 | Loss: 0.00148793
Iteration 5/25 | Loss: 0.00148793
Iteration 6/25 | Loss: 0.00148793
Iteration 7/25 | Loss: 0.00148793
Iteration 8/25 | Loss: 0.00148793
Iteration 9/25 | Loss: 0.00148793
Iteration 10/25 | Loss: 0.00148793
Iteration 11/25 | Loss: 0.00148793
Iteration 12/25 | Loss: 0.00148793
Iteration 13/25 | Loss: 0.00148793
Iteration 14/25 | Loss: 0.00148793
Iteration 15/25 | Loss: 0.00148793
Iteration 16/25 | Loss: 0.00148793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014879257651045918, 0.0014879257651045918, 0.0014879257651045918, 0.0014879257651045918, 0.0014879257651045918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014879257651045918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148793
Iteration 2/1000 | Loss: 0.00004643
Iteration 3/1000 | Loss: 0.00003211
Iteration 4/1000 | Loss: 0.00002701
Iteration 5/1000 | Loss: 0.00002445
Iteration 6/1000 | Loss: 0.00002311
Iteration 7/1000 | Loss: 0.00002224
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002116
Iteration 10/1000 | Loss: 0.00002088
Iteration 11/1000 | Loss: 0.00002058
Iteration 12/1000 | Loss: 0.00002046
Iteration 13/1000 | Loss: 0.00002041
Iteration 14/1000 | Loss: 0.00002028
Iteration 15/1000 | Loss: 0.00002021
Iteration 16/1000 | Loss: 0.00002020
Iteration 17/1000 | Loss: 0.00002015
Iteration 18/1000 | Loss: 0.00002013
Iteration 19/1000 | Loss: 0.00002008
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002005
Iteration 22/1000 | Loss: 0.00002005
Iteration 23/1000 | Loss: 0.00002004
Iteration 24/1000 | Loss: 0.00002003
Iteration 25/1000 | Loss: 0.00002002
Iteration 26/1000 | Loss: 0.00002001
Iteration 27/1000 | Loss: 0.00002001
Iteration 28/1000 | Loss: 0.00002000
Iteration 29/1000 | Loss: 0.00002000
Iteration 30/1000 | Loss: 0.00001997
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001994
Iteration 34/1000 | Loss: 0.00001994
Iteration 35/1000 | Loss: 0.00001994
Iteration 36/1000 | Loss: 0.00001993
Iteration 37/1000 | Loss: 0.00001993
Iteration 38/1000 | Loss: 0.00001993
Iteration 39/1000 | Loss: 0.00001992
Iteration 40/1000 | Loss: 0.00001991
Iteration 41/1000 | Loss: 0.00001991
Iteration 42/1000 | Loss: 0.00001990
Iteration 43/1000 | Loss: 0.00001990
Iteration 44/1000 | Loss: 0.00001989
Iteration 45/1000 | Loss: 0.00001989
Iteration 46/1000 | Loss: 0.00001989
Iteration 47/1000 | Loss: 0.00001989
Iteration 48/1000 | Loss: 0.00001989
Iteration 49/1000 | Loss: 0.00001989
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001988
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001986
Iteration 70/1000 | Loss: 0.00001986
Iteration 71/1000 | Loss: 0.00001986
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001986
Iteration 76/1000 | Loss: 0.00001986
Iteration 77/1000 | Loss: 0.00001986
Iteration 78/1000 | Loss: 0.00001985
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001985
Iteration 82/1000 | Loss: 0.00001985
Iteration 83/1000 | Loss: 0.00001985
Iteration 84/1000 | Loss: 0.00001985
Iteration 85/1000 | Loss: 0.00001985
Iteration 86/1000 | Loss: 0.00001985
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001985
Iteration 89/1000 | Loss: 0.00001985
Iteration 90/1000 | Loss: 0.00001984
Iteration 91/1000 | Loss: 0.00001984
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001984
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001984
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001983
Iteration 101/1000 | Loss: 0.00001983
Iteration 102/1000 | Loss: 0.00001983
Iteration 103/1000 | Loss: 0.00001983
Iteration 104/1000 | Loss: 0.00001983
Iteration 105/1000 | Loss: 0.00001982
Iteration 106/1000 | Loss: 0.00001982
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001982
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001982
Iteration 111/1000 | Loss: 0.00001982
Iteration 112/1000 | Loss: 0.00001981
Iteration 113/1000 | Loss: 0.00001981
Iteration 114/1000 | Loss: 0.00001981
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001980
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001979
Iteration 124/1000 | Loss: 0.00001979
Iteration 125/1000 | Loss: 0.00001979
Iteration 126/1000 | Loss: 0.00001979
Iteration 127/1000 | Loss: 0.00001979
Iteration 128/1000 | Loss: 0.00001979
Iteration 129/1000 | Loss: 0.00001979
Iteration 130/1000 | Loss: 0.00001979
Iteration 131/1000 | Loss: 0.00001979
Iteration 132/1000 | Loss: 0.00001979
Iteration 133/1000 | Loss: 0.00001979
Iteration 134/1000 | Loss: 0.00001979
Iteration 135/1000 | Loss: 0.00001979
Iteration 136/1000 | Loss: 0.00001979
Iteration 137/1000 | Loss: 0.00001979
Iteration 138/1000 | Loss: 0.00001979
Iteration 139/1000 | Loss: 0.00001979
Iteration 140/1000 | Loss: 0.00001979
Iteration 141/1000 | Loss: 0.00001979
Iteration 142/1000 | Loss: 0.00001979
Iteration 143/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.9788156350841746e-05, 1.9788156350841746e-05, 1.9788156350841746e-05, 1.9788156350841746e-05, 1.9788156350841746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9788156350841746e-05

Optimization complete. Final v2v error: 3.7088046073913574 mm

Highest mean error: 4.551916599273682 mm for frame 66

Lowest mean error: 3.0716826915740967 mm for frame 115

Saving results

Total time: 46.94838070869446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837505
Iteration 2/25 | Loss: 0.00122068
Iteration 3/25 | Loss: 0.00095791
Iteration 4/25 | Loss: 0.00092486
Iteration 5/25 | Loss: 0.00091782
Iteration 6/25 | Loss: 0.00091590
Iteration 7/25 | Loss: 0.00091583
Iteration 8/25 | Loss: 0.00091583
Iteration 9/25 | Loss: 0.00091583
Iteration 10/25 | Loss: 0.00091583
Iteration 11/25 | Loss: 0.00091583
Iteration 12/25 | Loss: 0.00091583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009158255415968597, 0.0009158255415968597, 0.0009158255415968597, 0.0009158255415968597, 0.0009158255415968597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009158255415968597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.22218037
Iteration 2/25 | Loss: 0.00105145
Iteration 3/25 | Loss: 0.00105145
Iteration 4/25 | Loss: 0.00105145
Iteration 5/25 | Loss: 0.00105145
Iteration 6/25 | Loss: 0.00105145
Iteration 7/25 | Loss: 0.00105145
Iteration 8/25 | Loss: 0.00105145
Iteration 9/25 | Loss: 0.00105145
Iteration 10/25 | Loss: 0.00105145
Iteration 11/25 | Loss: 0.00105145
Iteration 12/25 | Loss: 0.00105145
Iteration 13/25 | Loss: 0.00105145
Iteration 14/25 | Loss: 0.00105145
Iteration 15/25 | Loss: 0.00105145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010514466557651758, 0.0010514466557651758, 0.0010514466557651758, 0.0010514466557651758, 0.0010514466557651758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010514466557651758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105145
Iteration 2/1000 | Loss: 0.00004879
Iteration 3/1000 | Loss: 0.00002942
Iteration 4/1000 | Loss: 0.00002378
Iteration 5/1000 | Loss: 0.00002154
Iteration 6/1000 | Loss: 0.00002043
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001953
Iteration 9/1000 | Loss: 0.00001948
Iteration 10/1000 | Loss: 0.00001925
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00001907
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001885
Iteration 16/1000 | Loss: 0.00001884
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001877
Iteration 19/1000 | Loss: 0.00001876
Iteration 20/1000 | Loss: 0.00001873
Iteration 21/1000 | Loss: 0.00001864
Iteration 22/1000 | Loss: 0.00001859
Iteration 23/1000 | Loss: 0.00001859
Iteration 24/1000 | Loss: 0.00001857
Iteration 25/1000 | Loss: 0.00001852
Iteration 26/1000 | Loss: 0.00001849
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001849
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001849
Iteration 32/1000 | Loss: 0.00001848
Iteration 33/1000 | Loss: 0.00001848
Iteration 34/1000 | Loss: 0.00001848
Iteration 35/1000 | Loss: 0.00001848
Iteration 36/1000 | Loss: 0.00001848
Iteration 37/1000 | Loss: 0.00001848
Iteration 38/1000 | Loss: 0.00001848
Iteration 39/1000 | Loss: 0.00001847
Iteration 40/1000 | Loss: 0.00001847
Iteration 41/1000 | Loss: 0.00001847
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001846
Iteration 44/1000 | Loss: 0.00001846
Iteration 45/1000 | Loss: 0.00001846
Iteration 46/1000 | Loss: 0.00001846
Iteration 47/1000 | Loss: 0.00001846
Iteration 48/1000 | Loss: 0.00001846
Iteration 49/1000 | Loss: 0.00001845
Iteration 50/1000 | Loss: 0.00001845
Iteration 51/1000 | Loss: 0.00001845
Iteration 52/1000 | Loss: 0.00001844
Iteration 53/1000 | Loss: 0.00001844
Iteration 54/1000 | Loss: 0.00001844
Iteration 55/1000 | Loss: 0.00001844
Iteration 56/1000 | Loss: 0.00001843
Iteration 57/1000 | Loss: 0.00001843
Iteration 58/1000 | Loss: 0.00001843
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001840
Iteration 78/1000 | Loss: 0.00001840
Iteration 79/1000 | Loss: 0.00001840
Iteration 80/1000 | Loss: 0.00001839
Iteration 81/1000 | Loss: 0.00001839
Iteration 82/1000 | Loss: 0.00001839
Iteration 83/1000 | Loss: 0.00001839
Iteration 84/1000 | Loss: 0.00001838
Iteration 85/1000 | Loss: 0.00001838
Iteration 86/1000 | Loss: 0.00001838
Iteration 87/1000 | Loss: 0.00001838
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00001838
Iteration 90/1000 | Loss: 0.00001838
Iteration 91/1000 | Loss: 0.00001838
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001837
Iteration 97/1000 | Loss: 0.00001837
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001836
Iteration 110/1000 | Loss: 0.00001836
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001836
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001835
Iteration 115/1000 | Loss: 0.00001835
Iteration 116/1000 | Loss: 0.00001835
Iteration 117/1000 | Loss: 0.00001835
Iteration 118/1000 | Loss: 0.00001835
Iteration 119/1000 | Loss: 0.00001835
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001834
Iteration 124/1000 | Loss: 0.00001834
Iteration 125/1000 | Loss: 0.00001834
Iteration 126/1000 | Loss: 0.00001834
Iteration 127/1000 | Loss: 0.00001833
Iteration 128/1000 | Loss: 0.00001833
Iteration 129/1000 | Loss: 0.00001833
Iteration 130/1000 | Loss: 0.00001833
Iteration 131/1000 | Loss: 0.00001833
Iteration 132/1000 | Loss: 0.00001833
Iteration 133/1000 | Loss: 0.00001832
Iteration 134/1000 | Loss: 0.00001832
Iteration 135/1000 | Loss: 0.00001832
Iteration 136/1000 | Loss: 0.00001832
Iteration 137/1000 | Loss: 0.00001832
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001830
Iteration 143/1000 | Loss: 0.00001830
Iteration 144/1000 | Loss: 0.00001830
Iteration 145/1000 | Loss: 0.00001830
Iteration 146/1000 | Loss: 0.00001830
Iteration 147/1000 | Loss: 0.00001830
Iteration 148/1000 | Loss: 0.00001830
Iteration 149/1000 | Loss: 0.00001830
Iteration 150/1000 | Loss: 0.00001829
Iteration 151/1000 | Loss: 0.00001829
Iteration 152/1000 | Loss: 0.00001829
Iteration 153/1000 | Loss: 0.00001829
Iteration 154/1000 | Loss: 0.00001829
Iteration 155/1000 | Loss: 0.00001829
Iteration 156/1000 | Loss: 0.00001829
Iteration 157/1000 | Loss: 0.00001829
Iteration 158/1000 | Loss: 0.00001829
Iteration 159/1000 | Loss: 0.00001828
Iteration 160/1000 | Loss: 0.00001828
Iteration 161/1000 | Loss: 0.00001828
Iteration 162/1000 | Loss: 0.00001828
Iteration 163/1000 | Loss: 0.00001828
Iteration 164/1000 | Loss: 0.00001828
Iteration 165/1000 | Loss: 0.00001828
Iteration 166/1000 | Loss: 0.00001828
Iteration 167/1000 | Loss: 0.00001828
Iteration 168/1000 | Loss: 0.00001828
Iteration 169/1000 | Loss: 0.00001828
Iteration 170/1000 | Loss: 0.00001828
Iteration 171/1000 | Loss: 0.00001828
Iteration 172/1000 | Loss: 0.00001828
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001828
Iteration 176/1000 | Loss: 0.00001827
Iteration 177/1000 | Loss: 0.00001827
Iteration 178/1000 | Loss: 0.00001827
Iteration 179/1000 | Loss: 0.00001827
Iteration 180/1000 | Loss: 0.00001827
Iteration 181/1000 | Loss: 0.00001827
Iteration 182/1000 | Loss: 0.00001827
Iteration 183/1000 | Loss: 0.00001827
Iteration 184/1000 | Loss: 0.00001827
Iteration 185/1000 | Loss: 0.00001827
Iteration 186/1000 | Loss: 0.00001827
Iteration 187/1000 | Loss: 0.00001827
Iteration 188/1000 | Loss: 0.00001827
Iteration 189/1000 | Loss: 0.00001827
Iteration 190/1000 | Loss: 0.00001827
Iteration 191/1000 | Loss: 0.00001827
Iteration 192/1000 | Loss: 0.00001827
Iteration 193/1000 | Loss: 0.00001827
Iteration 194/1000 | Loss: 0.00001827
Iteration 195/1000 | Loss: 0.00001827
Iteration 196/1000 | Loss: 0.00001827
Iteration 197/1000 | Loss: 0.00001827
Iteration 198/1000 | Loss: 0.00001827
Iteration 199/1000 | Loss: 0.00001827
Iteration 200/1000 | Loss: 0.00001827
Iteration 201/1000 | Loss: 0.00001827
Iteration 202/1000 | Loss: 0.00001827
Iteration 203/1000 | Loss: 0.00001827
Iteration 204/1000 | Loss: 0.00001827
Iteration 205/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.826759398682043e-05, 1.826759398682043e-05, 1.826759398682043e-05, 1.826759398682043e-05, 1.826759398682043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.826759398682043e-05

Optimization complete. Final v2v error: 3.6177237033843994 mm

Highest mean error: 4.119544506072998 mm for frame 24

Lowest mean error: 3.2588493824005127 mm for frame 82

Saving results

Total time: 42.802570819854736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01231194
Iteration 2/25 | Loss: 0.01231194
Iteration 3/25 | Loss: 0.01231194
Iteration 4/25 | Loss: 0.00372500
Iteration 5/25 | Loss: 0.00220385
Iteration 6/25 | Loss: 0.00199368
Iteration 7/25 | Loss: 0.00171818
Iteration 8/25 | Loss: 0.00158127
Iteration 9/25 | Loss: 0.00141604
Iteration 10/25 | Loss: 0.00131953
Iteration 11/25 | Loss: 0.00127983
Iteration 12/25 | Loss: 0.00126412
Iteration 13/25 | Loss: 0.00125838
Iteration 14/25 | Loss: 0.00123838
Iteration 15/25 | Loss: 0.00123425
Iteration 16/25 | Loss: 0.00123157
Iteration 17/25 | Loss: 0.00122746
Iteration 18/25 | Loss: 0.00122475
Iteration 19/25 | Loss: 0.00122357
Iteration 20/25 | Loss: 0.00122130
Iteration 21/25 | Loss: 0.00122099
Iteration 22/25 | Loss: 0.00121577
Iteration 23/25 | Loss: 0.00121218
Iteration 24/25 | Loss: 0.00120245
Iteration 25/25 | Loss: 0.00120005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73152423
Iteration 2/25 | Loss: 0.00139960
Iteration 3/25 | Loss: 0.00139960
Iteration 4/25 | Loss: 0.00139960
Iteration 5/25 | Loss: 0.00139960
Iteration 6/25 | Loss: 0.00139960
Iteration 7/25 | Loss: 0.00139960
Iteration 8/25 | Loss: 0.00139960
Iteration 9/25 | Loss: 0.00139960
Iteration 10/25 | Loss: 0.00139960
Iteration 11/25 | Loss: 0.00139960
Iteration 12/25 | Loss: 0.00139960
Iteration 13/25 | Loss: 0.00139960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001399597735144198, 0.001399597735144198, 0.001399597735144198, 0.001399597735144198, 0.001399597735144198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001399597735144198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139960
Iteration 2/1000 | Loss: 0.00014010
Iteration 3/1000 | Loss: 0.00008799
Iteration 4/1000 | Loss: 0.00010402
Iteration 5/1000 | Loss: 0.00010598
Iteration 6/1000 | Loss: 0.00007761
Iteration 7/1000 | Loss: 0.00006283
Iteration 8/1000 | Loss: 0.00005738
Iteration 9/1000 | Loss: 0.00005443
Iteration 10/1000 | Loss: 0.00005250
Iteration 11/1000 | Loss: 0.00005637
Iteration 12/1000 | Loss: 0.00004987
Iteration 13/1000 | Loss: 0.00004841
Iteration 14/1000 | Loss: 0.00004709
Iteration 15/1000 | Loss: 0.00004623
Iteration 16/1000 | Loss: 0.00004556
Iteration 17/1000 | Loss: 0.00004521
Iteration 18/1000 | Loss: 0.00004497
Iteration 19/1000 | Loss: 0.00004474
Iteration 20/1000 | Loss: 0.00004458
Iteration 21/1000 | Loss: 0.00004453
Iteration 22/1000 | Loss: 0.00004439
Iteration 23/1000 | Loss: 0.00004434
Iteration 24/1000 | Loss: 0.00004434
Iteration 25/1000 | Loss: 0.00004433
Iteration 26/1000 | Loss: 0.00004433
Iteration 27/1000 | Loss: 0.00004432
Iteration 28/1000 | Loss: 0.00004432
Iteration 29/1000 | Loss: 0.00004429
Iteration 30/1000 | Loss: 0.00004428
Iteration 31/1000 | Loss: 0.00004428
Iteration 32/1000 | Loss: 0.00004428
Iteration 33/1000 | Loss: 0.00004425
Iteration 34/1000 | Loss: 0.00004425
Iteration 35/1000 | Loss: 0.00004425
Iteration 36/1000 | Loss: 0.00004425
Iteration 37/1000 | Loss: 0.00004425
Iteration 38/1000 | Loss: 0.00004425
Iteration 39/1000 | Loss: 0.00004425
Iteration 40/1000 | Loss: 0.00004425
Iteration 41/1000 | Loss: 0.00004424
Iteration 42/1000 | Loss: 0.00004424
Iteration 43/1000 | Loss: 0.00004424
Iteration 44/1000 | Loss: 0.00004424
Iteration 45/1000 | Loss: 0.00004424
Iteration 46/1000 | Loss: 0.00004424
Iteration 47/1000 | Loss: 0.00004424
Iteration 48/1000 | Loss: 0.00004424
Iteration 49/1000 | Loss: 0.00004424
Iteration 50/1000 | Loss: 0.00004423
Iteration 51/1000 | Loss: 0.00004422
Iteration 52/1000 | Loss: 0.00004422
Iteration 53/1000 | Loss: 0.00004422
Iteration 54/1000 | Loss: 0.00004421
Iteration 55/1000 | Loss: 0.00004421
Iteration 56/1000 | Loss: 0.00004421
Iteration 57/1000 | Loss: 0.00004419
Iteration 58/1000 | Loss: 0.00004418
Iteration 59/1000 | Loss: 0.00004418
Iteration 60/1000 | Loss: 0.00004418
Iteration 61/1000 | Loss: 0.00004417
Iteration 62/1000 | Loss: 0.00004417
Iteration 63/1000 | Loss: 0.00004416
Iteration 64/1000 | Loss: 0.00004416
Iteration 65/1000 | Loss: 0.00004416
Iteration 66/1000 | Loss: 0.00004415
Iteration 67/1000 | Loss: 0.00004415
Iteration 68/1000 | Loss: 0.00004415
Iteration 69/1000 | Loss: 0.00004415
Iteration 70/1000 | Loss: 0.00004415
Iteration 71/1000 | Loss: 0.00004415
Iteration 72/1000 | Loss: 0.00004414
Iteration 73/1000 | Loss: 0.00004414
Iteration 74/1000 | Loss: 0.00004414
Iteration 75/1000 | Loss: 0.00004414
Iteration 76/1000 | Loss: 0.00004414
Iteration 77/1000 | Loss: 0.00004414
Iteration 78/1000 | Loss: 0.00004414
Iteration 79/1000 | Loss: 0.00004413
Iteration 80/1000 | Loss: 0.00004413
Iteration 81/1000 | Loss: 0.00004413
Iteration 82/1000 | Loss: 0.00004413
Iteration 83/1000 | Loss: 0.00004412
Iteration 84/1000 | Loss: 0.00004412
Iteration 85/1000 | Loss: 0.00004412
Iteration 86/1000 | Loss: 0.00004412
Iteration 87/1000 | Loss: 0.00004412
Iteration 88/1000 | Loss: 0.00004412
Iteration 89/1000 | Loss: 0.00004412
Iteration 90/1000 | Loss: 0.00004411
Iteration 91/1000 | Loss: 0.00004411
Iteration 92/1000 | Loss: 0.00004411
Iteration 93/1000 | Loss: 0.00004410
Iteration 94/1000 | Loss: 0.00004410
Iteration 95/1000 | Loss: 0.00004410
Iteration 96/1000 | Loss: 0.00004410
Iteration 97/1000 | Loss: 0.00004409
Iteration 98/1000 | Loss: 0.00004409
Iteration 99/1000 | Loss: 0.00004409
Iteration 100/1000 | Loss: 0.00004409
Iteration 101/1000 | Loss: 0.00004409
Iteration 102/1000 | Loss: 0.00004409
Iteration 103/1000 | Loss: 0.00004409
Iteration 104/1000 | Loss: 0.00004409
Iteration 105/1000 | Loss: 0.00004409
Iteration 106/1000 | Loss: 0.00004409
Iteration 107/1000 | Loss: 0.00004409
Iteration 108/1000 | Loss: 0.00004409
Iteration 109/1000 | Loss: 0.00004409
Iteration 110/1000 | Loss: 0.00004409
Iteration 111/1000 | Loss: 0.00004409
Iteration 112/1000 | Loss: 0.00004409
Iteration 113/1000 | Loss: 0.00004409
Iteration 114/1000 | Loss: 0.00004409
Iteration 115/1000 | Loss: 0.00004409
Iteration 116/1000 | Loss: 0.00004409
Iteration 117/1000 | Loss: 0.00004409
Iteration 118/1000 | Loss: 0.00004409
Iteration 119/1000 | Loss: 0.00004409
Iteration 120/1000 | Loss: 0.00004409
Iteration 121/1000 | Loss: 0.00004409
Iteration 122/1000 | Loss: 0.00004409
Iteration 123/1000 | Loss: 0.00004409
Iteration 124/1000 | Loss: 0.00004409
Iteration 125/1000 | Loss: 0.00004409
Iteration 126/1000 | Loss: 0.00004409
Iteration 127/1000 | Loss: 0.00004409
Iteration 128/1000 | Loss: 0.00004409
Iteration 129/1000 | Loss: 0.00004409
Iteration 130/1000 | Loss: 0.00004409
Iteration 131/1000 | Loss: 0.00004409
Iteration 132/1000 | Loss: 0.00004409
Iteration 133/1000 | Loss: 0.00004409
Iteration 134/1000 | Loss: 0.00004409
Iteration 135/1000 | Loss: 0.00004409
Iteration 136/1000 | Loss: 0.00004409
Iteration 137/1000 | Loss: 0.00004409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [4.408524910104461e-05, 4.408524910104461e-05, 4.408524910104461e-05, 4.408524910104461e-05, 4.408524910104461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.408524910104461e-05

Optimization complete. Final v2v error: 5.379374980926514 mm

Highest mean error: 5.952518939971924 mm for frame 4

Lowest mean error: 4.856118679046631 mm for frame 19

Saving results

Total time: 79.63669204711914
