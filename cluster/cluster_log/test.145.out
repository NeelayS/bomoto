Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=145, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8120-8175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000729
Iteration 2/25 | Loss: 0.00350425
Iteration 3/25 | Loss: 0.00217223
Iteration 4/25 | Loss: 0.00187472
Iteration 5/25 | Loss: 0.00176857
Iteration 6/25 | Loss: 0.00184586
Iteration 7/25 | Loss: 0.00174373
Iteration 8/25 | Loss: 0.00155534
Iteration 9/25 | Loss: 0.00152439
Iteration 10/25 | Loss: 0.00147755
Iteration 11/25 | Loss: 0.00143604
Iteration 12/25 | Loss: 0.00142268
Iteration 13/25 | Loss: 0.00139534
Iteration 14/25 | Loss: 0.00139005
Iteration 15/25 | Loss: 0.00139034
Iteration 16/25 | Loss: 0.00137959
Iteration 17/25 | Loss: 0.00137488
Iteration 18/25 | Loss: 0.00136260
Iteration 19/25 | Loss: 0.00135773
Iteration 20/25 | Loss: 0.00135628
Iteration 21/25 | Loss: 0.00136014
Iteration 22/25 | Loss: 0.00135853
Iteration 23/25 | Loss: 0.00134684
Iteration 24/25 | Loss: 0.00134370
Iteration 25/25 | Loss: 0.00134278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37536156
Iteration 2/25 | Loss: 0.00631543
Iteration 3/25 | Loss: 0.00387058
Iteration 4/25 | Loss: 0.00387058
Iteration 5/25 | Loss: 0.00387058
Iteration 6/25 | Loss: 0.00387058
Iteration 7/25 | Loss: 0.00387058
Iteration 8/25 | Loss: 0.00387058
Iteration 9/25 | Loss: 0.00387058
Iteration 10/25 | Loss: 0.00387058
Iteration 11/25 | Loss: 0.00387058
Iteration 12/25 | Loss: 0.00387058
Iteration 13/25 | Loss: 0.00387058
Iteration 14/25 | Loss: 0.00387058
Iteration 15/25 | Loss: 0.00387058
Iteration 16/25 | Loss: 0.00387058
Iteration 17/25 | Loss: 0.00387058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0038705754559487104, 0.0038705754559487104, 0.0038705754559487104, 0.0038705754559487104, 0.0038705754559487104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038705754559487104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00387058
Iteration 2/1000 | Loss: 0.00381242
Iteration 3/1000 | Loss: 0.00047069
Iteration 4/1000 | Loss: 0.00042313
Iteration 5/1000 | Loss: 0.00050253
Iteration 6/1000 | Loss: 0.00044279
Iteration 7/1000 | Loss: 0.00038750
Iteration 8/1000 | Loss: 0.00026031
Iteration 9/1000 | Loss: 0.00023610
Iteration 10/1000 | Loss: 0.00021730
Iteration 11/1000 | Loss: 0.00103329
Iteration 12/1000 | Loss: 0.00079719
Iteration 13/1000 | Loss: 0.00144610
Iteration 14/1000 | Loss: 0.00071435
Iteration 15/1000 | Loss: 0.00056592
Iteration 16/1000 | Loss: 0.00034100
Iteration 17/1000 | Loss: 0.00027463
Iteration 18/1000 | Loss: 0.00018969
Iteration 19/1000 | Loss: 0.00073343
Iteration 20/1000 | Loss: 0.00048621
Iteration 21/1000 | Loss: 0.00070603
Iteration 22/1000 | Loss: 0.00101311
Iteration 23/1000 | Loss: 0.00150093
Iteration 24/1000 | Loss: 0.00320112
Iteration 25/1000 | Loss: 0.00192964
Iteration 26/1000 | Loss: 0.00018737
Iteration 27/1000 | Loss: 0.00017392
Iteration 28/1000 | Loss: 0.00016799
Iteration 29/1000 | Loss: 0.00016247
Iteration 30/1000 | Loss: 0.00015879
Iteration 31/1000 | Loss: 0.00057759
Iteration 32/1000 | Loss: 0.00015539
Iteration 33/1000 | Loss: 0.00036264
Iteration 34/1000 | Loss: 0.00015243
Iteration 35/1000 | Loss: 0.00015102
Iteration 36/1000 | Loss: 0.00069174
Iteration 37/1000 | Loss: 0.00103385
Iteration 38/1000 | Loss: 0.00051687
Iteration 39/1000 | Loss: 0.00048903
Iteration 40/1000 | Loss: 0.00020575
Iteration 41/1000 | Loss: 0.00028657
Iteration 42/1000 | Loss: 0.00020259
Iteration 43/1000 | Loss: 0.00018507
Iteration 44/1000 | Loss: 0.00014573
Iteration 45/1000 | Loss: 0.00014382
Iteration 46/1000 | Loss: 0.00051943
Iteration 47/1000 | Loss: 0.00038283
Iteration 48/1000 | Loss: 0.00046318
Iteration 49/1000 | Loss: 0.00027518
Iteration 50/1000 | Loss: 0.00023000
Iteration 51/1000 | Loss: 0.00014552
Iteration 52/1000 | Loss: 0.00014043
Iteration 53/1000 | Loss: 0.00019080
Iteration 54/1000 | Loss: 0.00053112
Iteration 55/1000 | Loss: 0.00029652
Iteration 56/1000 | Loss: 0.00023117
Iteration 57/1000 | Loss: 0.00013645
Iteration 58/1000 | Loss: 0.00019894
Iteration 59/1000 | Loss: 0.00019503
Iteration 60/1000 | Loss: 0.00013294
Iteration 61/1000 | Loss: 0.00013131
Iteration 62/1000 | Loss: 0.00104244
Iteration 63/1000 | Loss: 0.00052693
Iteration 64/1000 | Loss: 0.00054158
Iteration 65/1000 | Loss: 0.00059368
Iteration 66/1000 | Loss: 0.00051373
Iteration 67/1000 | Loss: 0.00065987
Iteration 68/1000 | Loss: 0.00054728
Iteration 69/1000 | Loss: 0.00036474
Iteration 70/1000 | Loss: 0.00014814
Iteration 71/1000 | Loss: 0.00062743
Iteration 72/1000 | Loss: 0.00026143
Iteration 73/1000 | Loss: 0.00022220
Iteration 74/1000 | Loss: 0.00055676
Iteration 75/1000 | Loss: 0.00012915
Iteration 76/1000 | Loss: 0.00040170
Iteration 77/1000 | Loss: 0.00015682
Iteration 78/1000 | Loss: 0.00012832
Iteration 79/1000 | Loss: 0.00052096
Iteration 80/1000 | Loss: 0.00019620
Iteration 81/1000 | Loss: 0.00023160
Iteration 82/1000 | Loss: 0.00013384
Iteration 83/1000 | Loss: 0.00012114
Iteration 84/1000 | Loss: 0.00047307
Iteration 85/1000 | Loss: 0.00018540
Iteration 86/1000 | Loss: 0.00012577
Iteration 87/1000 | Loss: 0.00011296
Iteration 88/1000 | Loss: 0.00011222
Iteration 89/1000 | Loss: 0.00011116
Iteration 90/1000 | Loss: 0.00011040
Iteration 91/1000 | Loss: 0.00030706
Iteration 92/1000 | Loss: 0.00029609
Iteration 93/1000 | Loss: 0.00038018
Iteration 94/1000 | Loss: 0.00011530
Iteration 95/1000 | Loss: 0.00011064
Iteration 96/1000 | Loss: 0.00032575
Iteration 97/1000 | Loss: 0.00033748
Iteration 98/1000 | Loss: 0.00034445
Iteration 99/1000 | Loss: 0.00032379
Iteration 100/1000 | Loss: 0.00029727
Iteration 101/1000 | Loss: 0.00012001
Iteration 102/1000 | Loss: 0.00011039
Iteration 103/1000 | Loss: 0.00010778
Iteration 104/1000 | Loss: 0.00010613
Iteration 105/1000 | Loss: 0.00022572
Iteration 106/1000 | Loss: 0.00018102
Iteration 107/1000 | Loss: 0.00015413
Iteration 108/1000 | Loss: 0.00010889
Iteration 109/1000 | Loss: 0.00010493
Iteration 110/1000 | Loss: 0.00010380
Iteration 111/1000 | Loss: 0.00010240
Iteration 112/1000 | Loss: 0.00010136
Iteration 113/1000 | Loss: 0.00010086
Iteration 114/1000 | Loss: 0.00010029
Iteration 115/1000 | Loss: 0.00009985
Iteration 116/1000 | Loss: 0.00021562
Iteration 117/1000 | Loss: 0.00010712
Iteration 118/1000 | Loss: 0.00016441
Iteration 119/1000 | Loss: 0.00010413
Iteration 120/1000 | Loss: 0.00009920
Iteration 121/1000 | Loss: 0.00057322
Iteration 122/1000 | Loss: 0.00054520
Iteration 123/1000 | Loss: 0.00015817
Iteration 124/1000 | Loss: 0.00022884
Iteration 125/1000 | Loss: 0.00015137
Iteration 126/1000 | Loss: 0.00011090
Iteration 127/1000 | Loss: 0.00009711
Iteration 128/1000 | Loss: 0.00009145
Iteration 129/1000 | Loss: 0.00008742
Iteration 130/1000 | Loss: 0.00019342
Iteration 131/1000 | Loss: 0.00010516
Iteration 132/1000 | Loss: 0.00008678
Iteration 133/1000 | Loss: 0.00008396
Iteration 134/1000 | Loss: 0.00008260
Iteration 135/1000 | Loss: 0.00008110
Iteration 136/1000 | Loss: 0.00007998
Iteration 137/1000 | Loss: 0.00007947
Iteration 138/1000 | Loss: 0.00007902
Iteration 139/1000 | Loss: 0.00007871
Iteration 140/1000 | Loss: 0.00007841
Iteration 141/1000 | Loss: 0.00007821
Iteration 142/1000 | Loss: 0.00007818
Iteration 143/1000 | Loss: 0.00007814
Iteration 144/1000 | Loss: 0.00007809
Iteration 145/1000 | Loss: 0.00007798
Iteration 146/1000 | Loss: 0.00007797
Iteration 147/1000 | Loss: 0.00007791
Iteration 148/1000 | Loss: 0.00007785
Iteration 149/1000 | Loss: 0.00007785
Iteration 150/1000 | Loss: 0.00007784
Iteration 151/1000 | Loss: 0.00007784
Iteration 152/1000 | Loss: 0.00007784
Iteration 153/1000 | Loss: 0.00007784
Iteration 154/1000 | Loss: 0.00007784
Iteration 155/1000 | Loss: 0.00007784
Iteration 156/1000 | Loss: 0.00007784
Iteration 157/1000 | Loss: 0.00007784
Iteration 158/1000 | Loss: 0.00007783
Iteration 159/1000 | Loss: 0.00007783
Iteration 160/1000 | Loss: 0.00007783
Iteration 161/1000 | Loss: 0.00007782
Iteration 162/1000 | Loss: 0.00007782
Iteration 163/1000 | Loss: 0.00007781
Iteration 164/1000 | Loss: 0.00007780
Iteration 165/1000 | Loss: 0.00007779
Iteration 166/1000 | Loss: 0.00007778
Iteration 167/1000 | Loss: 0.00007778
Iteration 168/1000 | Loss: 0.00007777
Iteration 169/1000 | Loss: 0.00007777
Iteration 170/1000 | Loss: 0.00007776
Iteration 171/1000 | Loss: 0.00007775
Iteration 172/1000 | Loss: 0.00007775
Iteration 173/1000 | Loss: 0.00007774
Iteration 174/1000 | Loss: 0.00007774
Iteration 175/1000 | Loss: 0.00007774
Iteration 176/1000 | Loss: 0.00007774
Iteration 177/1000 | Loss: 0.00007773
Iteration 178/1000 | Loss: 0.00007773
Iteration 179/1000 | Loss: 0.00007773
Iteration 180/1000 | Loss: 0.00007772
Iteration 181/1000 | Loss: 0.00007772
Iteration 182/1000 | Loss: 0.00007772
Iteration 183/1000 | Loss: 0.00007772
Iteration 184/1000 | Loss: 0.00007771
Iteration 185/1000 | Loss: 0.00007771
Iteration 186/1000 | Loss: 0.00007770
Iteration 187/1000 | Loss: 0.00007770
Iteration 188/1000 | Loss: 0.00007770
Iteration 189/1000 | Loss: 0.00007770
Iteration 190/1000 | Loss: 0.00007770
Iteration 191/1000 | Loss: 0.00007770
Iteration 192/1000 | Loss: 0.00007769
Iteration 193/1000 | Loss: 0.00007769
Iteration 194/1000 | Loss: 0.00007769
Iteration 195/1000 | Loss: 0.00007768
Iteration 196/1000 | Loss: 0.00007768
Iteration 197/1000 | Loss: 0.00007768
Iteration 198/1000 | Loss: 0.00007767
Iteration 199/1000 | Loss: 0.00007767
Iteration 200/1000 | Loss: 0.00007767
Iteration 201/1000 | Loss: 0.00007767
Iteration 202/1000 | Loss: 0.00007767
Iteration 203/1000 | Loss: 0.00007766
Iteration 204/1000 | Loss: 0.00007766
Iteration 205/1000 | Loss: 0.00007766
Iteration 206/1000 | Loss: 0.00007766
Iteration 207/1000 | Loss: 0.00007766
Iteration 208/1000 | Loss: 0.00007766
Iteration 209/1000 | Loss: 0.00007766
Iteration 210/1000 | Loss: 0.00007766
Iteration 211/1000 | Loss: 0.00007766
Iteration 212/1000 | Loss: 0.00007766
Iteration 213/1000 | Loss: 0.00007765
Iteration 214/1000 | Loss: 0.00007765
Iteration 215/1000 | Loss: 0.00007765
Iteration 216/1000 | Loss: 0.00007765
Iteration 217/1000 | Loss: 0.00017958
Iteration 218/1000 | Loss: 0.00008341
Iteration 219/1000 | Loss: 0.00008029
Iteration 220/1000 | Loss: 0.00007880
Iteration 221/1000 | Loss: 0.00007753
Iteration 222/1000 | Loss: 0.00021336
Iteration 223/1000 | Loss: 0.00014457
Iteration 224/1000 | Loss: 0.00007865
Iteration 225/1000 | Loss: 0.00007698
Iteration 226/1000 | Loss: 0.00007657
Iteration 227/1000 | Loss: 0.00022603
Iteration 228/1000 | Loss: 0.00007991
Iteration 229/1000 | Loss: 0.00018160
Iteration 230/1000 | Loss: 0.00008678
Iteration 231/1000 | Loss: 0.00022733
Iteration 232/1000 | Loss: 0.00011631
Iteration 233/1000 | Loss: 0.00009027
Iteration 234/1000 | Loss: 0.00066741
Iteration 235/1000 | Loss: 0.00031209
Iteration 236/1000 | Loss: 0.00019616
Iteration 237/1000 | Loss: 0.00041079
Iteration 238/1000 | Loss: 0.00033524
Iteration 239/1000 | Loss: 0.00011571
Iteration 240/1000 | Loss: 0.00008159
Iteration 241/1000 | Loss: 0.00007861
Iteration 242/1000 | Loss: 0.00007667
Iteration 243/1000 | Loss: 0.00017779
Iteration 244/1000 | Loss: 0.00044329
Iteration 245/1000 | Loss: 0.00062722
Iteration 246/1000 | Loss: 0.00009857
Iteration 247/1000 | Loss: 0.00065320
Iteration 248/1000 | Loss: 0.00022711
Iteration 249/1000 | Loss: 0.00033151
Iteration 250/1000 | Loss: 0.00058038
Iteration 251/1000 | Loss: 0.00048532
Iteration 252/1000 | Loss: 0.00057201
Iteration 253/1000 | Loss: 0.00046582
Iteration 254/1000 | Loss: 0.00049201
Iteration 255/1000 | Loss: 0.00007895
Iteration 256/1000 | Loss: 0.00007438
Iteration 257/1000 | Loss: 0.00015448
Iteration 258/1000 | Loss: 0.00019099
Iteration 259/1000 | Loss: 0.00007893
Iteration 260/1000 | Loss: 0.00006974
Iteration 261/1000 | Loss: 0.00009630
Iteration 262/1000 | Loss: 0.00017866
Iteration 263/1000 | Loss: 0.00012130
Iteration 264/1000 | Loss: 0.00026063
Iteration 265/1000 | Loss: 0.00022808
Iteration 266/1000 | Loss: 0.00007070
Iteration 267/1000 | Loss: 0.00006784
Iteration 268/1000 | Loss: 0.00011272
Iteration 269/1000 | Loss: 0.00006800
Iteration 270/1000 | Loss: 0.00018288
Iteration 271/1000 | Loss: 0.00010965
Iteration 272/1000 | Loss: 0.00017396
Iteration 273/1000 | Loss: 0.00012114
Iteration 274/1000 | Loss: 0.00027995
Iteration 275/1000 | Loss: 0.00006808
Iteration 276/1000 | Loss: 0.00006571
Iteration 277/1000 | Loss: 0.00006431
Iteration 278/1000 | Loss: 0.00011087
Iteration 279/1000 | Loss: 0.00006271
Iteration 280/1000 | Loss: 0.00006240
Iteration 281/1000 | Loss: 0.00006204
Iteration 282/1000 | Loss: 0.00006184
Iteration 283/1000 | Loss: 0.00006166
Iteration 284/1000 | Loss: 0.00006162
Iteration 285/1000 | Loss: 0.00026700
Iteration 286/1000 | Loss: 0.00018487
Iteration 287/1000 | Loss: 0.00078831
Iteration 288/1000 | Loss: 0.00010831
Iteration 289/1000 | Loss: 0.00019197
Iteration 290/1000 | Loss: 0.00006953
Iteration 291/1000 | Loss: 0.00012519
Iteration 292/1000 | Loss: 0.00009452
Iteration 293/1000 | Loss: 0.00006425
Iteration 294/1000 | Loss: 0.00008703
Iteration 295/1000 | Loss: 0.00006243
Iteration 296/1000 | Loss: 0.00006178
Iteration 297/1000 | Loss: 0.00007645
Iteration 298/1000 | Loss: 0.00006130
Iteration 299/1000 | Loss: 0.00006105
Iteration 300/1000 | Loss: 0.00006089
Iteration 301/1000 | Loss: 0.00006075
Iteration 302/1000 | Loss: 0.00006071
Iteration 303/1000 | Loss: 0.00006065
Iteration 304/1000 | Loss: 0.00016093
Iteration 305/1000 | Loss: 0.00006637
Iteration 306/1000 | Loss: 0.00006269
Iteration 307/1000 | Loss: 0.00006142
Iteration 308/1000 | Loss: 0.00019400
Iteration 309/1000 | Loss: 0.00016992
Iteration 310/1000 | Loss: 0.00006052
Iteration 311/1000 | Loss: 0.00014131
Iteration 312/1000 | Loss: 0.00006471
Iteration 313/1000 | Loss: 0.00006152
Iteration 314/1000 | Loss: 0.00005977
Iteration 315/1000 | Loss: 0.00005888
Iteration 316/1000 | Loss: 0.00008441
Iteration 317/1000 | Loss: 0.00022064
Iteration 318/1000 | Loss: 0.00006987
Iteration 319/1000 | Loss: 0.00006204
Iteration 320/1000 | Loss: 0.00005965
Iteration 321/1000 | Loss: 0.00007929
Iteration 322/1000 | Loss: 0.00005740
Iteration 323/1000 | Loss: 0.00005664
Iteration 324/1000 | Loss: 0.00005622
Iteration 325/1000 | Loss: 0.00005599
Iteration 326/1000 | Loss: 0.00005589
Iteration 327/1000 | Loss: 0.00005586
Iteration 328/1000 | Loss: 0.00005576
Iteration 329/1000 | Loss: 0.00009015
Iteration 330/1000 | Loss: 0.00005870
Iteration 331/1000 | Loss: 0.00005562
Iteration 332/1000 | Loss: 0.00005559
Iteration 333/1000 | Loss: 0.00005559
Iteration 334/1000 | Loss: 0.00005558
Iteration 335/1000 | Loss: 0.00005558
Iteration 336/1000 | Loss: 0.00006636
Iteration 337/1000 | Loss: 0.00005558
Iteration 338/1000 | Loss: 0.00005558
Iteration 339/1000 | Loss: 0.00005557
Iteration 340/1000 | Loss: 0.00005557
Iteration 341/1000 | Loss: 0.00005557
Iteration 342/1000 | Loss: 0.00005557
Iteration 343/1000 | Loss: 0.00005556
Iteration 344/1000 | Loss: 0.00005556
Iteration 345/1000 | Loss: 0.00005556
Iteration 346/1000 | Loss: 0.00005556
Iteration 347/1000 | Loss: 0.00005555
Iteration 348/1000 | Loss: 0.00005555
Iteration 349/1000 | Loss: 0.00005555
Iteration 350/1000 | Loss: 0.00005555
Iteration 351/1000 | Loss: 0.00005555
Iteration 352/1000 | Loss: 0.00005554
Iteration 353/1000 | Loss: 0.00005554
Iteration 354/1000 | Loss: 0.00005554
Iteration 355/1000 | Loss: 0.00005553
Iteration 356/1000 | Loss: 0.00005553
Iteration 357/1000 | Loss: 0.00005553
Iteration 358/1000 | Loss: 0.00005553
Iteration 359/1000 | Loss: 0.00005553
Iteration 360/1000 | Loss: 0.00005553
Iteration 361/1000 | Loss: 0.00005553
Iteration 362/1000 | Loss: 0.00005553
Iteration 363/1000 | Loss: 0.00005553
Iteration 364/1000 | Loss: 0.00005553
Iteration 365/1000 | Loss: 0.00005553
Iteration 366/1000 | Loss: 0.00005553
Iteration 367/1000 | Loss: 0.00005553
Iteration 368/1000 | Loss: 0.00005553
Iteration 369/1000 | Loss: 0.00005553
Iteration 370/1000 | Loss: 0.00005553
Iteration 371/1000 | Loss: 0.00005553
Iteration 372/1000 | Loss: 0.00005553
Iteration 373/1000 | Loss: 0.00005553
Iteration 374/1000 | Loss: 0.00005553
Iteration 375/1000 | Loss: 0.00005553
Iteration 376/1000 | Loss: 0.00005553
Iteration 377/1000 | Loss: 0.00005553
Iteration 378/1000 | Loss: 0.00005553
Iteration 379/1000 | Loss: 0.00005553
Iteration 380/1000 | Loss: 0.00005553
Iteration 381/1000 | Loss: 0.00005553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 381. Stopping optimization.
Last 5 losses: [5.553090886678547e-05, 5.553090886678547e-05, 5.553090886678547e-05, 5.553090886678547e-05, 5.553090886678547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.553090886678547e-05

Optimization complete. Final v2v error: 3.7505834102630615 mm

Highest mean error: 11.75831413269043 mm for frame 39

Lowest mean error: 2.5278046131134033 mm for frame 70

Saving results

Total time: 464.2244942188263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002190
Iteration 2/25 | Loss: 0.00239244
Iteration 3/25 | Loss: 0.00178464
Iteration 4/25 | Loss: 0.00164590
Iteration 5/25 | Loss: 0.00149550
Iteration 6/25 | Loss: 0.00127355
Iteration 7/25 | Loss: 0.00117965
Iteration 8/25 | Loss: 0.00114100
Iteration 9/25 | Loss: 0.00110952
Iteration 10/25 | Loss: 0.00109878
Iteration 11/25 | Loss: 0.00109715
Iteration 12/25 | Loss: 0.00109676
Iteration 13/25 | Loss: 0.00109661
Iteration 14/25 | Loss: 0.00109659
Iteration 15/25 | Loss: 0.00109658
Iteration 16/25 | Loss: 0.00109658
Iteration 17/25 | Loss: 0.00109658
Iteration 18/25 | Loss: 0.00109658
Iteration 19/25 | Loss: 0.00109658
Iteration 20/25 | Loss: 0.00109658
Iteration 21/25 | Loss: 0.00109657
Iteration 22/25 | Loss: 0.00109657
Iteration 23/25 | Loss: 0.00109657
Iteration 24/25 | Loss: 0.00109657
Iteration 25/25 | Loss: 0.00109657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27435410
Iteration 2/25 | Loss: 0.00042056
Iteration 3/25 | Loss: 0.00042055
Iteration 4/25 | Loss: 0.00042055
Iteration 5/25 | Loss: 0.00042055
Iteration 6/25 | Loss: 0.00042054
Iteration 7/25 | Loss: 0.00042054
Iteration 8/25 | Loss: 0.00042054
Iteration 9/25 | Loss: 0.00042054
Iteration 10/25 | Loss: 0.00042054
Iteration 11/25 | Loss: 0.00042054
Iteration 12/25 | Loss: 0.00042054
Iteration 13/25 | Loss: 0.00042054
Iteration 14/25 | Loss: 0.00042054
Iteration 15/25 | Loss: 0.00042054
Iteration 16/25 | Loss: 0.00042054
Iteration 17/25 | Loss: 0.00042054
Iteration 18/25 | Loss: 0.00042054
Iteration 19/25 | Loss: 0.00042054
Iteration 20/25 | Loss: 0.00042054
Iteration 21/25 | Loss: 0.00042054
Iteration 22/25 | Loss: 0.00042054
Iteration 23/25 | Loss: 0.00042054
Iteration 24/25 | Loss: 0.00042054
Iteration 25/25 | Loss: 0.00042054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004205431614536792, 0.0004205431614536792, 0.0004205431614536792, 0.0004205431614536792, 0.0004205431614536792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004205431614536792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042054
Iteration 2/1000 | Loss: 0.00005877
Iteration 3/1000 | Loss: 0.00002805
Iteration 4/1000 | Loss: 0.00002110
Iteration 5/1000 | Loss: 0.00001880
Iteration 6/1000 | Loss: 0.00001768
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001638
Iteration 9/1000 | Loss: 0.00001608
Iteration 10/1000 | Loss: 0.00001581
Iteration 11/1000 | Loss: 0.00001567
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001557
Iteration 14/1000 | Loss: 0.00001555
Iteration 15/1000 | Loss: 0.00001554
Iteration 16/1000 | Loss: 0.00001553
Iteration 17/1000 | Loss: 0.00001553
Iteration 18/1000 | Loss: 0.00001552
Iteration 19/1000 | Loss: 0.00001551
Iteration 20/1000 | Loss: 0.00001550
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001544
Iteration 23/1000 | Loss: 0.00001542
Iteration 24/1000 | Loss: 0.00001542
Iteration 25/1000 | Loss: 0.00001533
Iteration 26/1000 | Loss: 0.00001532
Iteration 27/1000 | Loss: 0.00001532
Iteration 28/1000 | Loss: 0.00001532
Iteration 29/1000 | Loss: 0.00001532
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001532
Iteration 32/1000 | Loss: 0.00001531
Iteration 33/1000 | Loss: 0.00001531
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001531
Iteration 40/1000 | Loss: 0.00001531
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001529
Iteration 48/1000 | Loss: 0.00001529
Iteration 49/1000 | Loss: 0.00001529
Iteration 50/1000 | Loss: 0.00001528
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001527
Iteration 53/1000 | Loss: 0.00001526
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001525
Iteration 56/1000 | Loss: 0.00001525
Iteration 57/1000 | Loss: 0.00001525
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001515
Iteration 62/1000 | Loss: 0.00001515
Iteration 63/1000 | Loss: 0.00001515
Iteration 64/1000 | Loss: 0.00001515
Iteration 65/1000 | Loss: 0.00001514
Iteration 66/1000 | Loss: 0.00001514
Iteration 67/1000 | Loss: 0.00001514
Iteration 68/1000 | Loss: 0.00001514
Iteration 69/1000 | Loss: 0.00001514
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001514
Iteration 73/1000 | Loss: 0.00001513
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00019170
Iteration 77/1000 | Loss: 0.00002280
Iteration 78/1000 | Loss: 0.00002039
Iteration 79/1000 | Loss: 0.00001826
Iteration 80/1000 | Loss: 0.00001725
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001645
Iteration 83/1000 | Loss: 0.00001619
Iteration 84/1000 | Loss: 0.00001592
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001553
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001551
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001550
Iteration 95/1000 | Loss: 0.00001550
Iteration 96/1000 | Loss: 0.00001550
Iteration 97/1000 | Loss: 0.00001550
Iteration 98/1000 | Loss: 0.00001550
Iteration 99/1000 | Loss: 0.00001550
Iteration 100/1000 | Loss: 0.00001550
Iteration 101/1000 | Loss: 0.00001549
Iteration 102/1000 | Loss: 0.00001549
Iteration 103/1000 | Loss: 0.00001549
Iteration 104/1000 | Loss: 0.00001549
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001548
Iteration 107/1000 | Loss: 0.00001548
Iteration 108/1000 | Loss: 0.00001548
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001547
Iteration 111/1000 | Loss: 0.00001547
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001546
Iteration 115/1000 | Loss: 0.00001546
Iteration 116/1000 | Loss: 0.00001546
Iteration 117/1000 | Loss: 0.00001546
Iteration 118/1000 | Loss: 0.00001546
Iteration 119/1000 | Loss: 0.00001546
Iteration 120/1000 | Loss: 0.00001546
Iteration 121/1000 | Loss: 0.00001546
Iteration 122/1000 | Loss: 0.00001546
Iteration 123/1000 | Loss: 0.00001546
Iteration 124/1000 | Loss: 0.00001545
Iteration 125/1000 | Loss: 0.00001545
Iteration 126/1000 | Loss: 0.00001545
Iteration 127/1000 | Loss: 0.00001545
Iteration 128/1000 | Loss: 0.00001545
Iteration 129/1000 | Loss: 0.00001545
Iteration 130/1000 | Loss: 0.00001545
Iteration 131/1000 | Loss: 0.00001545
Iteration 132/1000 | Loss: 0.00001545
Iteration 133/1000 | Loss: 0.00001545
Iteration 134/1000 | Loss: 0.00001545
Iteration 135/1000 | Loss: 0.00001544
Iteration 136/1000 | Loss: 0.00001544
Iteration 137/1000 | Loss: 0.00001544
Iteration 138/1000 | Loss: 0.00001544
Iteration 139/1000 | Loss: 0.00001544
Iteration 140/1000 | Loss: 0.00001544
Iteration 141/1000 | Loss: 0.00001544
Iteration 142/1000 | Loss: 0.00001544
Iteration 143/1000 | Loss: 0.00001544
Iteration 144/1000 | Loss: 0.00001544
Iteration 145/1000 | Loss: 0.00001544
Iteration 146/1000 | Loss: 0.00001544
Iteration 147/1000 | Loss: 0.00001544
Iteration 148/1000 | Loss: 0.00001544
Iteration 149/1000 | Loss: 0.00001543
Iteration 150/1000 | Loss: 0.00001543
Iteration 151/1000 | Loss: 0.00001543
Iteration 152/1000 | Loss: 0.00001543
Iteration 153/1000 | Loss: 0.00001543
Iteration 154/1000 | Loss: 0.00001543
Iteration 155/1000 | Loss: 0.00001543
Iteration 156/1000 | Loss: 0.00001543
Iteration 157/1000 | Loss: 0.00001543
Iteration 158/1000 | Loss: 0.00001543
Iteration 159/1000 | Loss: 0.00001543
Iteration 160/1000 | Loss: 0.00001543
Iteration 161/1000 | Loss: 0.00001543
Iteration 162/1000 | Loss: 0.00001543
Iteration 163/1000 | Loss: 0.00001543
Iteration 164/1000 | Loss: 0.00001543
Iteration 165/1000 | Loss: 0.00001543
Iteration 166/1000 | Loss: 0.00001543
Iteration 167/1000 | Loss: 0.00001543
Iteration 168/1000 | Loss: 0.00001543
Iteration 169/1000 | Loss: 0.00001543
Iteration 170/1000 | Loss: 0.00001543
Iteration 171/1000 | Loss: 0.00001543
Iteration 172/1000 | Loss: 0.00001543
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001543
Iteration 175/1000 | Loss: 0.00001543
Iteration 176/1000 | Loss: 0.00001543
Iteration 177/1000 | Loss: 0.00001543
Iteration 178/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.543206235510297e-05, 1.543206235510297e-05, 1.543206235510297e-05, 1.543206235510297e-05, 1.543206235510297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.543206235510297e-05

Optimization complete. Final v2v error: 3.225731134414673 mm

Highest mean error: 6.504429817199707 mm for frame 1

Lowest mean error: 2.821408271789551 mm for frame 108

Saving results

Total time: 64.10495567321777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400396
Iteration 2/25 | Loss: 0.00116914
Iteration 3/25 | Loss: 0.00103514
Iteration 4/25 | Loss: 0.00101536
Iteration 5/25 | Loss: 0.00100958
Iteration 6/25 | Loss: 0.00100762
Iteration 7/25 | Loss: 0.00100735
Iteration 8/25 | Loss: 0.00100735
Iteration 9/25 | Loss: 0.00100735
Iteration 10/25 | Loss: 0.00100735
Iteration 11/25 | Loss: 0.00100735
Iteration 12/25 | Loss: 0.00100735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001007353188470006, 0.001007353188470006, 0.001007353188470006, 0.001007353188470006, 0.001007353188470006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001007353188470006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34509122
Iteration 2/25 | Loss: 0.00087188
Iteration 3/25 | Loss: 0.00087186
Iteration 4/25 | Loss: 0.00087186
Iteration 5/25 | Loss: 0.00087186
Iteration 6/25 | Loss: 0.00087186
Iteration 7/25 | Loss: 0.00087186
Iteration 8/25 | Loss: 0.00087186
Iteration 9/25 | Loss: 0.00087186
Iteration 10/25 | Loss: 0.00087186
Iteration 11/25 | Loss: 0.00087186
Iteration 12/25 | Loss: 0.00087185
Iteration 13/25 | Loss: 0.00087185
Iteration 14/25 | Loss: 0.00087185
Iteration 15/25 | Loss: 0.00087185
Iteration 16/25 | Loss: 0.00087185
Iteration 17/25 | Loss: 0.00087185
Iteration 18/25 | Loss: 0.00087185
Iteration 19/25 | Loss: 0.00087185
Iteration 20/25 | Loss: 0.00087185
Iteration 21/25 | Loss: 0.00087185
Iteration 22/25 | Loss: 0.00087185
Iteration 23/25 | Loss: 0.00087185
Iteration 24/25 | Loss: 0.00087185
Iteration 25/25 | Loss: 0.00087185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087185
Iteration 2/1000 | Loss: 0.00005251
Iteration 3/1000 | Loss: 0.00003108
Iteration 4/1000 | Loss: 0.00002371
Iteration 5/1000 | Loss: 0.00001948
Iteration 6/1000 | Loss: 0.00001783
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001574
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001482
Iteration 11/1000 | Loss: 0.00001456
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001449
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001421
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001416
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001410
Iteration 21/1000 | Loss: 0.00001410
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00001409
Iteration 24/1000 | Loss: 0.00001409
Iteration 25/1000 | Loss: 0.00001408
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001404
Iteration 29/1000 | Loss: 0.00001404
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001403
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001402
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001400
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001399
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001398
Iteration 45/1000 | Loss: 0.00001397
Iteration 46/1000 | Loss: 0.00001396
Iteration 47/1000 | Loss: 0.00001396
Iteration 48/1000 | Loss: 0.00001396
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001395
Iteration 51/1000 | Loss: 0.00001395
Iteration 52/1000 | Loss: 0.00001391
Iteration 53/1000 | Loss: 0.00001391
Iteration 54/1000 | Loss: 0.00001390
Iteration 55/1000 | Loss: 0.00001389
Iteration 56/1000 | Loss: 0.00001389
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001388
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001383
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001382
Iteration 70/1000 | Loss: 0.00001382
Iteration 71/1000 | Loss: 0.00001382
Iteration 72/1000 | Loss: 0.00001382
Iteration 73/1000 | Loss: 0.00001382
Iteration 74/1000 | Loss: 0.00001382
Iteration 75/1000 | Loss: 0.00001382
Iteration 76/1000 | Loss: 0.00001382
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001381
Iteration 79/1000 | Loss: 0.00001381
Iteration 80/1000 | Loss: 0.00001381
Iteration 81/1000 | Loss: 0.00001380
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001379
Iteration 84/1000 | Loss: 0.00001379
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001378
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001376
Iteration 105/1000 | Loss: 0.00001376
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001376
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001375
Iteration 111/1000 | Loss: 0.00001375
Iteration 112/1000 | Loss: 0.00001375
Iteration 113/1000 | Loss: 0.00001375
Iteration 114/1000 | Loss: 0.00001375
Iteration 115/1000 | Loss: 0.00001375
Iteration 116/1000 | Loss: 0.00001375
Iteration 117/1000 | Loss: 0.00001375
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001374
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001374
Iteration 124/1000 | Loss: 0.00001374
Iteration 125/1000 | Loss: 0.00001374
Iteration 126/1000 | Loss: 0.00001374
Iteration 127/1000 | Loss: 0.00001374
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001372
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001371
Iteration 142/1000 | Loss: 0.00001371
Iteration 143/1000 | Loss: 0.00001371
Iteration 144/1000 | Loss: 0.00001371
Iteration 145/1000 | Loss: 0.00001371
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001370
Iteration 148/1000 | Loss: 0.00001370
Iteration 149/1000 | Loss: 0.00001370
Iteration 150/1000 | Loss: 0.00001370
Iteration 151/1000 | Loss: 0.00001370
Iteration 152/1000 | Loss: 0.00001369
Iteration 153/1000 | Loss: 0.00001369
Iteration 154/1000 | Loss: 0.00001369
Iteration 155/1000 | Loss: 0.00001369
Iteration 156/1000 | Loss: 0.00001369
Iteration 157/1000 | Loss: 0.00001369
Iteration 158/1000 | Loss: 0.00001368
Iteration 159/1000 | Loss: 0.00001368
Iteration 160/1000 | Loss: 0.00001368
Iteration 161/1000 | Loss: 0.00001368
Iteration 162/1000 | Loss: 0.00001367
Iteration 163/1000 | Loss: 0.00001367
Iteration 164/1000 | Loss: 0.00001367
Iteration 165/1000 | Loss: 0.00001366
Iteration 166/1000 | Loss: 0.00001366
Iteration 167/1000 | Loss: 0.00001366
Iteration 168/1000 | Loss: 0.00001366
Iteration 169/1000 | Loss: 0.00001366
Iteration 170/1000 | Loss: 0.00001366
Iteration 171/1000 | Loss: 0.00001366
Iteration 172/1000 | Loss: 0.00001366
Iteration 173/1000 | Loss: 0.00001366
Iteration 174/1000 | Loss: 0.00001366
Iteration 175/1000 | Loss: 0.00001366
Iteration 176/1000 | Loss: 0.00001366
Iteration 177/1000 | Loss: 0.00001366
Iteration 178/1000 | Loss: 0.00001366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3659870091942139e-05, 1.3659870091942139e-05, 1.3659870091942139e-05, 1.3659870091942139e-05, 1.3659870091942139e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3659870091942139e-05

Optimization complete. Final v2v error: 2.92380428314209 mm

Highest mean error: 5.0351362228393555 mm for frame 87

Lowest mean error: 2.2678277492523193 mm for frame 22

Saving results

Total time: 41.9271445274353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046266
Iteration 2/25 | Loss: 0.01046266
Iteration 3/25 | Loss: 0.00158310
Iteration 4/25 | Loss: 0.00151855
Iteration 5/25 | Loss: 0.00106787
Iteration 6/25 | Loss: 0.00103734
Iteration 7/25 | Loss: 0.00103915
Iteration 8/25 | Loss: 0.00101309
Iteration 9/25 | Loss: 0.00099402
Iteration 10/25 | Loss: 0.00097818
Iteration 11/25 | Loss: 0.00097331
Iteration 12/25 | Loss: 0.00097187
Iteration 13/25 | Loss: 0.00097143
Iteration 14/25 | Loss: 0.00097129
Iteration 15/25 | Loss: 0.00097127
Iteration 16/25 | Loss: 0.00097127
Iteration 17/25 | Loss: 0.00097127
Iteration 18/25 | Loss: 0.00097127
Iteration 19/25 | Loss: 0.00097127
Iteration 20/25 | Loss: 0.00097127
Iteration 21/25 | Loss: 0.00097127
Iteration 22/25 | Loss: 0.00097127
Iteration 23/25 | Loss: 0.00097126
Iteration 24/25 | Loss: 0.00097126
Iteration 25/25 | Loss: 0.00097126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58697462
Iteration 2/25 | Loss: 0.00077077
Iteration 3/25 | Loss: 0.00077077
Iteration 4/25 | Loss: 0.00077076
Iteration 5/25 | Loss: 0.00077076
Iteration 6/25 | Loss: 0.00077076
Iteration 7/25 | Loss: 0.00077076
Iteration 8/25 | Loss: 0.00077076
Iteration 9/25 | Loss: 0.00077076
Iteration 10/25 | Loss: 0.00077076
Iteration 11/25 | Loss: 0.00077076
Iteration 12/25 | Loss: 0.00077076
Iteration 13/25 | Loss: 0.00077076
Iteration 14/25 | Loss: 0.00077076
Iteration 15/25 | Loss: 0.00077076
Iteration 16/25 | Loss: 0.00077076
Iteration 17/25 | Loss: 0.00077076
Iteration 18/25 | Loss: 0.00077076
Iteration 19/25 | Loss: 0.00077076
Iteration 20/25 | Loss: 0.00077076
Iteration 21/25 | Loss: 0.00077076
Iteration 22/25 | Loss: 0.00077076
Iteration 23/25 | Loss: 0.00077076
Iteration 24/25 | Loss: 0.00077076
Iteration 25/25 | Loss: 0.00077076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077076
Iteration 2/1000 | Loss: 0.00001712
Iteration 3/1000 | Loss: 0.00001247
Iteration 4/1000 | Loss: 0.00013297
Iteration 5/1000 | Loss: 0.00001098
Iteration 6/1000 | Loss: 0.00001043
Iteration 7/1000 | Loss: 0.00010200
Iteration 8/1000 | Loss: 0.00014210
Iteration 9/1000 | Loss: 0.00067993
Iteration 10/1000 | Loss: 0.00007608
Iteration 11/1000 | Loss: 0.00004546
Iteration 12/1000 | Loss: 0.00002210
Iteration 13/1000 | Loss: 0.00000996
Iteration 14/1000 | Loss: 0.00004638
Iteration 15/1000 | Loss: 0.00001063
Iteration 16/1000 | Loss: 0.00000954
Iteration 17/1000 | Loss: 0.00000935
Iteration 18/1000 | Loss: 0.00000934
Iteration 19/1000 | Loss: 0.00000934
Iteration 20/1000 | Loss: 0.00000934
Iteration 21/1000 | Loss: 0.00005707
Iteration 22/1000 | Loss: 0.00000941
Iteration 23/1000 | Loss: 0.00000929
Iteration 24/1000 | Loss: 0.00000929
Iteration 25/1000 | Loss: 0.00003600
Iteration 26/1000 | Loss: 0.00001083
Iteration 27/1000 | Loss: 0.00001550
Iteration 28/1000 | Loss: 0.00000922
Iteration 29/1000 | Loss: 0.00000918
Iteration 30/1000 | Loss: 0.00000918
Iteration 31/1000 | Loss: 0.00000917
Iteration 32/1000 | Loss: 0.00000916
Iteration 33/1000 | Loss: 0.00000915
Iteration 34/1000 | Loss: 0.00000915
Iteration 35/1000 | Loss: 0.00000915
Iteration 36/1000 | Loss: 0.00005751
Iteration 37/1000 | Loss: 0.00000912
Iteration 38/1000 | Loss: 0.00000898
Iteration 39/1000 | Loss: 0.00009371
Iteration 40/1000 | Loss: 0.00009430
Iteration 41/1000 | Loss: 0.00004478
Iteration 42/1000 | Loss: 0.00000897
Iteration 43/1000 | Loss: 0.00000896
Iteration 44/1000 | Loss: 0.00003998
Iteration 45/1000 | Loss: 0.00000893
Iteration 46/1000 | Loss: 0.00000886
Iteration 47/1000 | Loss: 0.00000886
Iteration 48/1000 | Loss: 0.00000886
Iteration 49/1000 | Loss: 0.00000886
Iteration 50/1000 | Loss: 0.00000886
Iteration 51/1000 | Loss: 0.00000886
Iteration 52/1000 | Loss: 0.00000886
Iteration 53/1000 | Loss: 0.00000886
Iteration 54/1000 | Loss: 0.00000885
Iteration 55/1000 | Loss: 0.00000884
Iteration 56/1000 | Loss: 0.00000884
Iteration 57/1000 | Loss: 0.00000884
Iteration 58/1000 | Loss: 0.00000884
Iteration 59/1000 | Loss: 0.00000884
Iteration 60/1000 | Loss: 0.00000884
Iteration 61/1000 | Loss: 0.00000883
Iteration 62/1000 | Loss: 0.00000883
Iteration 63/1000 | Loss: 0.00000883
Iteration 64/1000 | Loss: 0.00000882
Iteration 65/1000 | Loss: 0.00000882
Iteration 66/1000 | Loss: 0.00000882
Iteration 67/1000 | Loss: 0.00000881
Iteration 68/1000 | Loss: 0.00000881
Iteration 69/1000 | Loss: 0.00000880
Iteration 70/1000 | Loss: 0.00000880
Iteration 71/1000 | Loss: 0.00000879
Iteration 72/1000 | Loss: 0.00000879
Iteration 73/1000 | Loss: 0.00000879
Iteration 74/1000 | Loss: 0.00000879
Iteration 75/1000 | Loss: 0.00000878
Iteration 76/1000 | Loss: 0.00009387
Iteration 77/1000 | Loss: 0.00026930
Iteration 78/1000 | Loss: 0.00003957
Iteration 79/1000 | Loss: 0.00005748
Iteration 80/1000 | Loss: 0.00011016
Iteration 81/1000 | Loss: 0.00006627
Iteration 82/1000 | Loss: 0.00011251
Iteration 83/1000 | Loss: 0.00009716
Iteration 84/1000 | Loss: 0.00002144
Iteration 85/1000 | Loss: 0.00003968
Iteration 86/1000 | Loss: 0.00004617
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000886
Iteration 91/1000 | Loss: 0.00006597
Iteration 92/1000 | Loss: 0.00001339
Iteration 93/1000 | Loss: 0.00002037
Iteration 94/1000 | Loss: 0.00000886
Iteration 95/1000 | Loss: 0.00008219
Iteration 96/1000 | Loss: 0.00010997
Iteration 97/1000 | Loss: 0.00000888
Iteration 98/1000 | Loss: 0.00006460
Iteration 99/1000 | Loss: 0.00005120
Iteration 100/1000 | Loss: 0.00001271
Iteration 101/1000 | Loss: 0.00000878
Iteration 102/1000 | Loss: 0.00000873
Iteration 103/1000 | Loss: 0.00000872
Iteration 104/1000 | Loss: 0.00000871
Iteration 105/1000 | Loss: 0.00000871
Iteration 106/1000 | Loss: 0.00000871
Iteration 107/1000 | Loss: 0.00000871
Iteration 108/1000 | Loss: 0.00000871
Iteration 109/1000 | Loss: 0.00000870
Iteration 110/1000 | Loss: 0.00003738
Iteration 111/1000 | Loss: 0.00015886
Iteration 112/1000 | Loss: 0.00003516
Iteration 113/1000 | Loss: 0.00000886
Iteration 114/1000 | Loss: 0.00004290
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001169
Iteration 117/1000 | Loss: 0.00006487
Iteration 118/1000 | Loss: 0.00049929
Iteration 119/1000 | Loss: 0.00018482
Iteration 120/1000 | Loss: 0.00001931
Iteration 121/1000 | Loss: 0.00001934
Iteration 122/1000 | Loss: 0.00017774
Iteration 123/1000 | Loss: 0.00002864
Iteration 124/1000 | Loss: 0.00007203
Iteration 125/1000 | Loss: 0.00000880
Iteration 126/1000 | Loss: 0.00000875
Iteration 127/1000 | Loss: 0.00000875
Iteration 128/1000 | Loss: 0.00000874
Iteration 129/1000 | Loss: 0.00000874
Iteration 130/1000 | Loss: 0.00000873
Iteration 131/1000 | Loss: 0.00000873
Iteration 132/1000 | Loss: 0.00000873
Iteration 133/1000 | Loss: 0.00000872
Iteration 134/1000 | Loss: 0.00000872
Iteration 135/1000 | Loss: 0.00000872
Iteration 136/1000 | Loss: 0.00000871
Iteration 137/1000 | Loss: 0.00000871
Iteration 138/1000 | Loss: 0.00000871
Iteration 139/1000 | Loss: 0.00000870
Iteration 140/1000 | Loss: 0.00000869
Iteration 141/1000 | Loss: 0.00000869
Iteration 142/1000 | Loss: 0.00000868
Iteration 143/1000 | Loss: 0.00000867
Iteration 144/1000 | Loss: 0.00000866
Iteration 145/1000 | Loss: 0.00000866
Iteration 146/1000 | Loss: 0.00000866
Iteration 147/1000 | Loss: 0.00000866
Iteration 148/1000 | Loss: 0.00000866
Iteration 149/1000 | Loss: 0.00000866
Iteration 150/1000 | Loss: 0.00000866
Iteration 151/1000 | Loss: 0.00000866
Iteration 152/1000 | Loss: 0.00000865
Iteration 153/1000 | Loss: 0.00000865
Iteration 154/1000 | Loss: 0.00000865
Iteration 155/1000 | Loss: 0.00000865
Iteration 156/1000 | Loss: 0.00000865
Iteration 157/1000 | Loss: 0.00000865
Iteration 158/1000 | Loss: 0.00000865
Iteration 159/1000 | Loss: 0.00000865
Iteration 160/1000 | Loss: 0.00000865
Iteration 161/1000 | Loss: 0.00000865
Iteration 162/1000 | Loss: 0.00000865
Iteration 163/1000 | Loss: 0.00000864
Iteration 164/1000 | Loss: 0.00000864
Iteration 165/1000 | Loss: 0.00000864
Iteration 166/1000 | Loss: 0.00000864
Iteration 167/1000 | Loss: 0.00000864
Iteration 168/1000 | Loss: 0.00000864
Iteration 169/1000 | Loss: 0.00000864
Iteration 170/1000 | Loss: 0.00000864
Iteration 171/1000 | Loss: 0.00000864
Iteration 172/1000 | Loss: 0.00000864
Iteration 173/1000 | Loss: 0.00000864
Iteration 174/1000 | Loss: 0.00000864
Iteration 175/1000 | Loss: 0.00000864
Iteration 176/1000 | Loss: 0.00000863
Iteration 177/1000 | Loss: 0.00000863
Iteration 178/1000 | Loss: 0.00000863
Iteration 179/1000 | Loss: 0.00000863
Iteration 180/1000 | Loss: 0.00000863
Iteration 181/1000 | Loss: 0.00000863
Iteration 182/1000 | Loss: 0.00000863
Iteration 183/1000 | Loss: 0.00000863
Iteration 184/1000 | Loss: 0.00000863
Iteration 185/1000 | Loss: 0.00000863
Iteration 186/1000 | Loss: 0.00000863
Iteration 187/1000 | Loss: 0.00000863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [8.631222044641618e-06, 8.631222044641618e-06, 8.631222044641618e-06, 8.631222044641618e-06, 8.631222044641618e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.631222044641618e-06

Optimization complete. Final v2v error: 2.529017448425293 mm

Highest mean error: 2.7354111671447754 mm for frame 25

Lowest mean error: 2.402400493621826 mm for frame 117

Saving results

Total time: 124.98708081245422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010688
Iteration 2/25 | Loss: 0.00250164
Iteration 3/25 | Loss: 0.00173931
Iteration 4/25 | Loss: 0.00157831
Iteration 5/25 | Loss: 0.00161547
Iteration 6/25 | Loss: 0.00165196
Iteration 7/25 | Loss: 0.00157916
Iteration 8/25 | Loss: 0.00144139
Iteration 9/25 | Loss: 0.00138505
Iteration 10/25 | Loss: 0.00134065
Iteration 11/25 | Loss: 0.00131672
Iteration 12/25 | Loss: 0.00129866
Iteration 13/25 | Loss: 0.00129018
Iteration 14/25 | Loss: 0.00129135
Iteration 15/25 | Loss: 0.00127835
Iteration 16/25 | Loss: 0.00128000
Iteration 17/25 | Loss: 0.00127140
Iteration 18/25 | Loss: 0.00126802
Iteration 19/25 | Loss: 0.00125888
Iteration 20/25 | Loss: 0.00125596
Iteration 21/25 | Loss: 0.00125699
Iteration 22/25 | Loss: 0.00124975
Iteration 23/25 | Loss: 0.00124457
Iteration 24/25 | Loss: 0.00124245
Iteration 25/25 | Loss: 0.00123769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36998820
Iteration 2/25 | Loss: 0.00232589
Iteration 3/25 | Loss: 0.00230275
Iteration 4/25 | Loss: 0.00230275
Iteration 5/25 | Loss: 0.00230275
Iteration 6/25 | Loss: 0.00230275
Iteration 7/25 | Loss: 0.00230274
Iteration 8/25 | Loss: 0.00230274
Iteration 9/25 | Loss: 0.00230274
Iteration 10/25 | Loss: 0.00230274
Iteration 11/25 | Loss: 0.00230274
Iteration 12/25 | Loss: 0.00230274
Iteration 13/25 | Loss: 0.00230274
Iteration 14/25 | Loss: 0.00230274
Iteration 15/25 | Loss: 0.00230274
Iteration 16/25 | Loss: 0.00230274
Iteration 17/25 | Loss: 0.00230274
Iteration 18/25 | Loss: 0.00230274
Iteration 19/25 | Loss: 0.00230274
Iteration 20/25 | Loss: 0.00230274
Iteration 21/25 | Loss: 0.00230274
Iteration 22/25 | Loss: 0.00230274
Iteration 23/25 | Loss: 0.00230274
Iteration 24/25 | Loss: 0.00230274
Iteration 25/25 | Loss: 0.00230274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230274
Iteration 2/1000 | Loss: 0.00249880
Iteration 3/1000 | Loss: 0.00089862
Iteration 4/1000 | Loss: 0.00043043
Iteration 5/1000 | Loss: 0.00034287
Iteration 6/1000 | Loss: 0.00050388
Iteration 7/1000 | Loss: 0.00063011
Iteration 8/1000 | Loss: 0.00017067
Iteration 9/1000 | Loss: 0.00033900
Iteration 10/1000 | Loss: 0.00082130
Iteration 11/1000 | Loss: 0.00032873
Iteration 12/1000 | Loss: 0.00039899
Iteration 13/1000 | Loss: 0.00062042
Iteration 14/1000 | Loss: 0.00032221
Iteration 15/1000 | Loss: 0.00015344
Iteration 16/1000 | Loss: 0.00017412
Iteration 17/1000 | Loss: 0.00014531
Iteration 18/1000 | Loss: 0.00063763
Iteration 19/1000 | Loss: 0.00015322
Iteration 20/1000 | Loss: 0.00020771
Iteration 21/1000 | Loss: 0.00010720
Iteration 22/1000 | Loss: 0.00022918
Iteration 23/1000 | Loss: 0.00021215
Iteration 24/1000 | Loss: 0.00010603
Iteration 25/1000 | Loss: 0.00029840
Iteration 26/1000 | Loss: 0.00122055
Iteration 27/1000 | Loss: 0.00181533
Iteration 28/1000 | Loss: 0.00054746
Iteration 29/1000 | Loss: 0.00032110
Iteration 30/1000 | Loss: 0.00027126
Iteration 31/1000 | Loss: 0.00045947
Iteration 32/1000 | Loss: 0.00027141
Iteration 33/1000 | Loss: 0.00027736
Iteration 34/1000 | Loss: 0.00007475
Iteration 35/1000 | Loss: 0.00040274
Iteration 36/1000 | Loss: 0.00016191
Iteration 37/1000 | Loss: 0.00007451
Iteration 38/1000 | Loss: 0.00059460
Iteration 39/1000 | Loss: 0.00021389
Iteration 40/1000 | Loss: 0.00020873
Iteration 41/1000 | Loss: 0.00005777
Iteration 42/1000 | Loss: 0.00006721
Iteration 43/1000 | Loss: 0.00004273
Iteration 44/1000 | Loss: 0.00018036
Iteration 45/1000 | Loss: 0.00004734
Iteration 46/1000 | Loss: 0.00016347
Iteration 47/1000 | Loss: 0.00015995
Iteration 48/1000 | Loss: 0.00016376
Iteration 49/1000 | Loss: 0.00004976
Iteration 50/1000 | Loss: 0.00018387
Iteration 51/1000 | Loss: 0.00021993
Iteration 52/1000 | Loss: 0.00030486
Iteration 53/1000 | Loss: 0.00013255
Iteration 54/1000 | Loss: 0.00019853
Iteration 55/1000 | Loss: 0.00011086
Iteration 56/1000 | Loss: 0.00016472
Iteration 57/1000 | Loss: 0.00008985
Iteration 58/1000 | Loss: 0.00021751
Iteration 59/1000 | Loss: 0.00005891
Iteration 60/1000 | Loss: 0.00050196
Iteration 61/1000 | Loss: 0.00027507
Iteration 62/1000 | Loss: 0.00027966
Iteration 63/1000 | Loss: 0.00022234
Iteration 64/1000 | Loss: 0.00007502
Iteration 65/1000 | Loss: 0.00021943
Iteration 66/1000 | Loss: 0.00042095
Iteration 67/1000 | Loss: 0.00038493
Iteration 68/1000 | Loss: 0.00036749
Iteration 69/1000 | Loss: 0.00037198
Iteration 70/1000 | Loss: 0.00012445
Iteration 71/1000 | Loss: 0.00040519
Iteration 72/1000 | Loss: 0.00038277
Iteration 73/1000 | Loss: 0.00029031
Iteration 74/1000 | Loss: 0.00004741
Iteration 75/1000 | Loss: 0.00004934
Iteration 76/1000 | Loss: 0.00005762
Iteration 77/1000 | Loss: 0.00017416
Iteration 78/1000 | Loss: 0.00018119
Iteration 79/1000 | Loss: 0.00014849
Iteration 80/1000 | Loss: 0.00014318
Iteration 81/1000 | Loss: 0.00016037
Iteration 82/1000 | Loss: 0.00005729
Iteration 83/1000 | Loss: 0.00005814
Iteration 84/1000 | Loss: 0.00006079
Iteration 85/1000 | Loss: 0.00003896
Iteration 86/1000 | Loss: 0.00007037
Iteration 87/1000 | Loss: 0.00007865
Iteration 88/1000 | Loss: 0.00011884
Iteration 89/1000 | Loss: 0.00011113
Iteration 90/1000 | Loss: 0.00006532
Iteration 91/1000 | Loss: 0.00011326
Iteration 92/1000 | Loss: 0.00029538
Iteration 93/1000 | Loss: 0.00041464
Iteration 94/1000 | Loss: 0.00012745
Iteration 95/1000 | Loss: 0.00006185
Iteration 96/1000 | Loss: 0.00042340
Iteration 97/1000 | Loss: 0.00013631
Iteration 98/1000 | Loss: 0.00005760
Iteration 99/1000 | Loss: 0.00005456
Iteration 100/1000 | Loss: 0.00005659
Iteration 101/1000 | Loss: 0.00005198
Iteration 102/1000 | Loss: 0.00005313
Iteration 103/1000 | Loss: 0.00019729
Iteration 104/1000 | Loss: 0.00011207
Iteration 105/1000 | Loss: 0.00034645
Iteration 106/1000 | Loss: 0.00053413
Iteration 107/1000 | Loss: 0.00020915
Iteration 108/1000 | Loss: 0.00013471
Iteration 109/1000 | Loss: 0.00020050
Iteration 110/1000 | Loss: 0.00010526
Iteration 111/1000 | Loss: 0.00003568
Iteration 112/1000 | Loss: 0.00003552
Iteration 113/1000 | Loss: 0.00003004
Iteration 114/1000 | Loss: 0.00002694
Iteration 115/1000 | Loss: 0.00003709
Iteration 116/1000 | Loss: 0.00002804
Iteration 117/1000 | Loss: 0.00002518
Iteration 118/1000 | Loss: 0.00002475
Iteration 119/1000 | Loss: 0.00027551
Iteration 120/1000 | Loss: 0.00018356
Iteration 121/1000 | Loss: 0.00003236
Iteration 122/1000 | Loss: 0.00002651
Iteration 123/1000 | Loss: 0.00002552
Iteration 124/1000 | Loss: 0.00002418
Iteration 125/1000 | Loss: 0.00002551
Iteration 126/1000 | Loss: 0.00002407
Iteration 127/1000 | Loss: 0.00002403
Iteration 128/1000 | Loss: 0.00002423
Iteration 129/1000 | Loss: 0.00026100
Iteration 130/1000 | Loss: 0.00006703
Iteration 131/1000 | Loss: 0.00010941
Iteration 132/1000 | Loss: 0.00014037
Iteration 133/1000 | Loss: 0.00010710
Iteration 134/1000 | Loss: 0.00006864
Iteration 135/1000 | Loss: 0.00002874
Iteration 136/1000 | Loss: 0.00004134
Iteration 137/1000 | Loss: 0.00004550
Iteration 138/1000 | Loss: 0.00002521
Iteration 139/1000 | Loss: 0.00002626
Iteration 140/1000 | Loss: 0.00002386
Iteration 141/1000 | Loss: 0.00002469
Iteration 142/1000 | Loss: 0.00002364
Iteration 143/1000 | Loss: 0.00002363
Iteration 144/1000 | Loss: 0.00002363
Iteration 145/1000 | Loss: 0.00002362
Iteration 146/1000 | Loss: 0.00002362
Iteration 147/1000 | Loss: 0.00002399
Iteration 148/1000 | Loss: 0.00002372
Iteration 149/1000 | Loss: 0.00002348
Iteration 150/1000 | Loss: 0.00002339
Iteration 151/1000 | Loss: 0.00002598
Iteration 152/1000 | Loss: 0.00002339
Iteration 153/1000 | Loss: 0.00002324
Iteration 154/1000 | Loss: 0.00002324
Iteration 155/1000 | Loss: 0.00002324
Iteration 156/1000 | Loss: 0.00002323
Iteration 157/1000 | Loss: 0.00002323
Iteration 158/1000 | Loss: 0.00002320
Iteration 159/1000 | Loss: 0.00002319
Iteration 160/1000 | Loss: 0.00002319
Iteration 161/1000 | Loss: 0.00002317
Iteration 162/1000 | Loss: 0.00002317
Iteration 163/1000 | Loss: 0.00002317
Iteration 164/1000 | Loss: 0.00002316
Iteration 165/1000 | Loss: 0.00002316
Iteration 166/1000 | Loss: 0.00002315
Iteration 167/1000 | Loss: 0.00002315
Iteration 168/1000 | Loss: 0.00002315
Iteration 169/1000 | Loss: 0.00002315
Iteration 170/1000 | Loss: 0.00002315
Iteration 171/1000 | Loss: 0.00002315
Iteration 172/1000 | Loss: 0.00002315
Iteration 173/1000 | Loss: 0.00002315
Iteration 174/1000 | Loss: 0.00002314
Iteration 175/1000 | Loss: 0.00002314
Iteration 176/1000 | Loss: 0.00002314
Iteration 177/1000 | Loss: 0.00002314
Iteration 178/1000 | Loss: 0.00002314
Iteration 179/1000 | Loss: 0.00002314
Iteration 180/1000 | Loss: 0.00002314
Iteration 181/1000 | Loss: 0.00002314
Iteration 182/1000 | Loss: 0.00002313
Iteration 183/1000 | Loss: 0.00002313
Iteration 184/1000 | Loss: 0.00002313
Iteration 185/1000 | Loss: 0.00002313
Iteration 186/1000 | Loss: 0.00002313
Iteration 187/1000 | Loss: 0.00002313
Iteration 188/1000 | Loss: 0.00002313
Iteration 189/1000 | Loss: 0.00002312
Iteration 190/1000 | Loss: 0.00002312
Iteration 191/1000 | Loss: 0.00002312
Iteration 192/1000 | Loss: 0.00002311
Iteration 193/1000 | Loss: 0.00002311
Iteration 194/1000 | Loss: 0.00002311
Iteration 195/1000 | Loss: 0.00002311
Iteration 196/1000 | Loss: 0.00002311
Iteration 197/1000 | Loss: 0.00002311
Iteration 198/1000 | Loss: 0.00002311
Iteration 199/1000 | Loss: 0.00002311
Iteration 200/1000 | Loss: 0.00002311
Iteration 201/1000 | Loss: 0.00002311
Iteration 202/1000 | Loss: 0.00002311
Iteration 203/1000 | Loss: 0.00002311
Iteration 204/1000 | Loss: 0.00002311
Iteration 205/1000 | Loss: 0.00002310
Iteration 206/1000 | Loss: 0.00002310
Iteration 207/1000 | Loss: 0.00002310
Iteration 208/1000 | Loss: 0.00002310
Iteration 209/1000 | Loss: 0.00002310
Iteration 210/1000 | Loss: 0.00002310
Iteration 211/1000 | Loss: 0.00002310
Iteration 212/1000 | Loss: 0.00002309
Iteration 213/1000 | Loss: 0.00002309
Iteration 214/1000 | Loss: 0.00002309
Iteration 215/1000 | Loss: 0.00002309
Iteration 216/1000 | Loss: 0.00002309
Iteration 217/1000 | Loss: 0.00002309
Iteration 218/1000 | Loss: 0.00002309
Iteration 219/1000 | Loss: 0.00002309
Iteration 220/1000 | Loss: 0.00002309
Iteration 221/1000 | Loss: 0.00002309
Iteration 222/1000 | Loss: 0.00002309
Iteration 223/1000 | Loss: 0.00002309
Iteration 224/1000 | Loss: 0.00002309
Iteration 225/1000 | Loss: 0.00002309
Iteration 226/1000 | Loss: 0.00002309
Iteration 227/1000 | Loss: 0.00002309
Iteration 228/1000 | Loss: 0.00002309
Iteration 229/1000 | Loss: 0.00002309
Iteration 230/1000 | Loss: 0.00002309
Iteration 231/1000 | Loss: 0.00002309
Iteration 232/1000 | Loss: 0.00002309
Iteration 233/1000 | Loss: 0.00002309
Iteration 234/1000 | Loss: 0.00002309
Iteration 235/1000 | Loss: 0.00002309
Iteration 236/1000 | Loss: 0.00002309
Iteration 237/1000 | Loss: 0.00002309
Iteration 238/1000 | Loss: 0.00002309
Iteration 239/1000 | Loss: 0.00002309
Iteration 240/1000 | Loss: 0.00002309
Iteration 241/1000 | Loss: 0.00002309
Iteration 242/1000 | Loss: 0.00002309
Iteration 243/1000 | Loss: 0.00002309
Iteration 244/1000 | Loss: 0.00002309
Iteration 245/1000 | Loss: 0.00002309
Iteration 246/1000 | Loss: 0.00002309
Iteration 247/1000 | Loss: 0.00002309
Iteration 248/1000 | Loss: 0.00002309
Iteration 249/1000 | Loss: 0.00002309
Iteration 250/1000 | Loss: 0.00002309
Iteration 251/1000 | Loss: 0.00002309
Iteration 252/1000 | Loss: 0.00002309
Iteration 253/1000 | Loss: 0.00002309
Iteration 254/1000 | Loss: 0.00002309
Iteration 255/1000 | Loss: 0.00002309
Iteration 256/1000 | Loss: 0.00002309
Iteration 257/1000 | Loss: 0.00002309
Iteration 258/1000 | Loss: 0.00002309
Iteration 259/1000 | Loss: 0.00002309
Iteration 260/1000 | Loss: 0.00002309
Iteration 261/1000 | Loss: 0.00002309
Iteration 262/1000 | Loss: 0.00002309
Iteration 263/1000 | Loss: 0.00002309
Iteration 264/1000 | Loss: 0.00002309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.3085465727490373e-05, 2.3085465727490373e-05, 2.3085465727490373e-05, 2.3085465727490373e-05, 2.3085465727490373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3085465727490373e-05

Optimization complete. Final v2v error: 3.1368227005004883 mm

Highest mean error: 11.041000366210938 mm for frame 17

Lowest mean error: 2.7156739234924316 mm for frame 220

Saving results

Total time: 284.93206000328064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036292
Iteration 2/25 | Loss: 0.00475360
Iteration 3/25 | Loss: 0.00308131
Iteration 4/25 | Loss: 0.00265843
Iteration 5/25 | Loss: 0.00235069
Iteration 6/25 | Loss: 0.00224295
Iteration 7/25 | Loss: 0.00213800
Iteration 8/25 | Loss: 0.00199162
Iteration 9/25 | Loss: 0.00190816
Iteration 10/25 | Loss: 0.00183058
Iteration 11/25 | Loss: 0.00178172
Iteration 12/25 | Loss: 0.00175218
Iteration 13/25 | Loss: 0.00172322
Iteration 14/25 | Loss: 0.00171356
Iteration 15/25 | Loss: 0.00170120
Iteration 16/25 | Loss: 0.00170579
Iteration 17/25 | Loss: 0.00171043
Iteration 18/25 | Loss: 0.00170662
Iteration 19/25 | Loss: 0.00169615
Iteration 20/25 | Loss: 0.00169433
Iteration 21/25 | Loss: 0.00168688
Iteration 22/25 | Loss: 0.00169206
Iteration 23/25 | Loss: 0.00169639
Iteration 24/25 | Loss: 0.00169364
Iteration 25/25 | Loss: 0.00168267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35654366
Iteration 2/25 | Loss: 0.00644040
Iteration 3/25 | Loss: 0.00439054
Iteration 4/25 | Loss: 0.00439210
Iteration 5/25 | Loss: 0.00448089
Iteration 6/25 | Loss: 0.00429788
Iteration 7/25 | Loss: 0.00429788
Iteration 8/25 | Loss: 0.00429788
Iteration 9/25 | Loss: 0.00429788
Iteration 10/25 | Loss: 0.00429788
Iteration 11/25 | Loss: 0.00429788
Iteration 12/25 | Loss: 0.00429788
Iteration 13/25 | Loss: 0.00429788
Iteration 14/25 | Loss: 0.00429788
Iteration 15/25 | Loss: 0.00429788
Iteration 16/25 | Loss: 0.00429788
Iteration 17/25 | Loss: 0.00429788
Iteration 18/25 | Loss: 0.00429788
Iteration 19/25 | Loss: 0.00429788
Iteration 20/25 | Loss: 0.00429788
Iteration 21/25 | Loss: 0.00429788
Iteration 22/25 | Loss: 0.00429788
Iteration 23/25 | Loss: 0.00429788
Iteration 24/25 | Loss: 0.00429788
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.004297877661883831, 0.004297877661883831, 0.004297877661883831, 0.004297877661883831, 0.004297877661883831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004297877661883831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00429788
Iteration 2/1000 | Loss: 0.00332244
Iteration 3/1000 | Loss: 0.00137356
Iteration 4/1000 | Loss: 0.00225009
Iteration 5/1000 | Loss: 0.00058185
Iteration 6/1000 | Loss: 0.00088347
Iteration 7/1000 | Loss: 0.00139427
Iteration 8/1000 | Loss: 0.00125286
Iteration 9/1000 | Loss: 0.00176992
Iteration 10/1000 | Loss: 0.00061258
Iteration 11/1000 | Loss: 0.00059204
Iteration 12/1000 | Loss: 0.00100898
Iteration 13/1000 | Loss: 0.00050111
Iteration 14/1000 | Loss: 0.00024579
Iteration 15/1000 | Loss: 0.00072156
Iteration 16/1000 | Loss: 0.00043854
Iteration 17/1000 | Loss: 0.00067958
Iteration 18/1000 | Loss: 0.00040138
Iteration 19/1000 | Loss: 0.00052087
Iteration 20/1000 | Loss: 0.00020529
Iteration 21/1000 | Loss: 0.00065503
Iteration 22/1000 | Loss: 0.00081178
Iteration 23/1000 | Loss: 0.00019474
Iteration 24/1000 | Loss: 0.00038379
Iteration 25/1000 | Loss: 0.00075654
Iteration 26/1000 | Loss: 0.00207837
Iteration 27/1000 | Loss: 0.00018960
Iteration 28/1000 | Loss: 0.00018713
Iteration 29/1000 | Loss: 0.00061495
Iteration 30/1000 | Loss: 0.00018386
Iteration 31/1000 | Loss: 0.00053653
Iteration 32/1000 | Loss: 0.00036223
Iteration 33/1000 | Loss: 0.00018020
Iteration 34/1000 | Loss: 0.00113457
Iteration 35/1000 | Loss: 0.00031124
Iteration 36/1000 | Loss: 0.00018461
Iteration 37/1000 | Loss: 0.00134265
Iteration 38/1000 | Loss: 0.00072976
Iteration 39/1000 | Loss: 0.00020012
Iteration 40/1000 | Loss: 0.00040699
Iteration 41/1000 | Loss: 0.00021761
Iteration 42/1000 | Loss: 0.00040346
Iteration 43/1000 | Loss: 0.00017328
Iteration 44/1000 | Loss: 0.00017000
Iteration 45/1000 | Loss: 0.00092979
Iteration 46/1000 | Loss: 0.00018757
Iteration 47/1000 | Loss: 0.00017831
Iteration 48/1000 | Loss: 0.00016707
Iteration 49/1000 | Loss: 0.00019547
Iteration 50/1000 | Loss: 0.00092521
Iteration 51/1000 | Loss: 0.00016805
Iteration 52/1000 | Loss: 0.00016340
Iteration 53/1000 | Loss: 0.00016240
Iteration 54/1000 | Loss: 0.00032618
Iteration 55/1000 | Loss: 0.00017505
Iteration 56/1000 | Loss: 0.00016189
Iteration 57/1000 | Loss: 0.00020502
Iteration 58/1000 | Loss: 0.00036056
Iteration 59/1000 | Loss: 0.00046712
Iteration 60/1000 | Loss: 0.00081588
Iteration 61/1000 | Loss: 0.00016240
Iteration 62/1000 | Loss: 0.00015943
Iteration 63/1000 | Loss: 0.00015887
Iteration 64/1000 | Loss: 0.00015853
Iteration 65/1000 | Loss: 0.00048399
Iteration 66/1000 | Loss: 0.00033633
Iteration 67/1000 | Loss: 0.00023355
Iteration 68/1000 | Loss: 0.00017010
Iteration 69/1000 | Loss: 0.00019921
Iteration 70/1000 | Loss: 0.00015571
Iteration 71/1000 | Loss: 0.00015488
Iteration 72/1000 | Loss: 0.00036402
Iteration 73/1000 | Loss: 0.00016052
Iteration 74/1000 | Loss: 0.00015785
Iteration 75/1000 | Loss: 0.00017656
Iteration 76/1000 | Loss: 0.00015924
Iteration 77/1000 | Loss: 0.00018030
Iteration 78/1000 | Loss: 0.00015630
Iteration 79/1000 | Loss: 0.00015782
Iteration 80/1000 | Loss: 0.00015319
Iteration 81/1000 | Loss: 0.00015303
Iteration 82/1000 | Loss: 0.00015288
Iteration 83/1000 | Loss: 0.00015281
Iteration 84/1000 | Loss: 0.00019509
Iteration 85/1000 | Loss: 0.00015311
Iteration 86/1000 | Loss: 0.00015272
Iteration 87/1000 | Loss: 0.00015269
Iteration 88/1000 | Loss: 0.00015268
Iteration 89/1000 | Loss: 0.00015265
Iteration 90/1000 | Loss: 0.00015265
Iteration 91/1000 | Loss: 0.00015264
Iteration 92/1000 | Loss: 0.00015264
Iteration 93/1000 | Loss: 0.00015263
Iteration 94/1000 | Loss: 0.00015261
Iteration 95/1000 | Loss: 0.00015261
Iteration 96/1000 | Loss: 0.00015261
Iteration 97/1000 | Loss: 0.00015260
Iteration 98/1000 | Loss: 0.00015260
Iteration 99/1000 | Loss: 0.00015260
Iteration 100/1000 | Loss: 0.00015257
Iteration 101/1000 | Loss: 0.00015257
Iteration 102/1000 | Loss: 0.00015255
Iteration 103/1000 | Loss: 0.00015255
Iteration 104/1000 | Loss: 0.00015255
Iteration 105/1000 | Loss: 0.00015254
Iteration 106/1000 | Loss: 0.00015250
Iteration 107/1000 | Loss: 0.00015250
Iteration 108/1000 | Loss: 0.00015249
Iteration 109/1000 | Loss: 0.00015246
Iteration 110/1000 | Loss: 0.00015242
Iteration 111/1000 | Loss: 0.00015242
Iteration 112/1000 | Loss: 0.00015241
Iteration 113/1000 | Loss: 0.00015241
Iteration 114/1000 | Loss: 0.00015241
Iteration 115/1000 | Loss: 0.00015241
Iteration 116/1000 | Loss: 0.00015241
Iteration 117/1000 | Loss: 0.00015241
Iteration 118/1000 | Loss: 0.00015241
Iteration 119/1000 | Loss: 0.00015240
Iteration 120/1000 | Loss: 0.00015240
Iteration 121/1000 | Loss: 0.00015240
Iteration 122/1000 | Loss: 0.00015239
Iteration 123/1000 | Loss: 0.00015239
Iteration 124/1000 | Loss: 0.00015239
Iteration 125/1000 | Loss: 0.00015239
Iteration 126/1000 | Loss: 0.00015238
Iteration 127/1000 | Loss: 0.00015238
Iteration 128/1000 | Loss: 0.00015238
Iteration 129/1000 | Loss: 0.00015238
Iteration 130/1000 | Loss: 0.00015238
Iteration 131/1000 | Loss: 0.00015238
Iteration 132/1000 | Loss: 0.00015238
Iteration 133/1000 | Loss: 0.00015238
Iteration 134/1000 | Loss: 0.00015238
Iteration 135/1000 | Loss: 0.00015238
Iteration 136/1000 | Loss: 0.00015238
Iteration 137/1000 | Loss: 0.00015237
Iteration 138/1000 | Loss: 0.00015237
Iteration 139/1000 | Loss: 0.00015236
Iteration 140/1000 | Loss: 0.00015236
Iteration 141/1000 | Loss: 0.00015233
Iteration 142/1000 | Loss: 0.00015233
Iteration 143/1000 | Loss: 0.00015233
Iteration 144/1000 | Loss: 0.00015233
Iteration 145/1000 | Loss: 0.00015233
Iteration 146/1000 | Loss: 0.00015233
Iteration 147/1000 | Loss: 0.00015233
Iteration 148/1000 | Loss: 0.00015233
Iteration 149/1000 | Loss: 0.00015233
Iteration 150/1000 | Loss: 0.00015232
Iteration 151/1000 | Loss: 0.00015232
Iteration 152/1000 | Loss: 0.00015232
Iteration 153/1000 | Loss: 0.00015231
Iteration 154/1000 | Loss: 0.00015231
Iteration 155/1000 | Loss: 0.00015231
Iteration 156/1000 | Loss: 0.00015230
Iteration 157/1000 | Loss: 0.00015230
Iteration 158/1000 | Loss: 0.00015230
Iteration 159/1000 | Loss: 0.00015230
Iteration 160/1000 | Loss: 0.00015230
Iteration 161/1000 | Loss: 0.00015230
Iteration 162/1000 | Loss: 0.00015230
Iteration 163/1000 | Loss: 0.00015230
Iteration 164/1000 | Loss: 0.00015229
Iteration 165/1000 | Loss: 0.00015229
Iteration 166/1000 | Loss: 0.00015229
Iteration 167/1000 | Loss: 0.00015229
Iteration 168/1000 | Loss: 0.00015229
Iteration 169/1000 | Loss: 0.00015229
Iteration 170/1000 | Loss: 0.00015229
Iteration 171/1000 | Loss: 0.00015229
Iteration 172/1000 | Loss: 0.00015229
Iteration 173/1000 | Loss: 0.00015228
Iteration 174/1000 | Loss: 0.00015228
Iteration 175/1000 | Loss: 0.00015228
Iteration 176/1000 | Loss: 0.00015227
Iteration 177/1000 | Loss: 0.00015227
Iteration 178/1000 | Loss: 0.00015227
Iteration 179/1000 | Loss: 0.00015227
Iteration 180/1000 | Loss: 0.00015227
Iteration 181/1000 | Loss: 0.00015227
Iteration 182/1000 | Loss: 0.00015227
Iteration 183/1000 | Loss: 0.00015227
Iteration 184/1000 | Loss: 0.00015227
Iteration 185/1000 | Loss: 0.00015227
Iteration 186/1000 | Loss: 0.00015226
Iteration 187/1000 | Loss: 0.00015226
Iteration 188/1000 | Loss: 0.00015226
Iteration 189/1000 | Loss: 0.00015226
Iteration 190/1000 | Loss: 0.00015226
Iteration 191/1000 | Loss: 0.00015226
Iteration 192/1000 | Loss: 0.00015226
Iteration 193/1000 | Loss: 0.00015226
Iteration 194/1000 | Loss: 0.00015226
Iteration 195/1000 | Loss: 0.00015226
Iteration 196/1000 | Loss: 0.00015225
Iteration 197/1000 | Loss: 0.00015225
Iteration 198/1000 | Loss: 0.00015225
Iteration 199/1000 | Loss: 0.00015225
Iteration 200/1000 | Loss: 0.00015225
Iteration 201/1000 | Loss: 0.00015225
Iteration 202/1000 | Loss: 0.00015224
Iteration 203/1000 | Loss: 0.00015224
Iteration 204/1000 | Loss: 0.00015224
Iteration 205/1000 | Loss: 0.00015224
Iteration 206/1000 | Loss: 0.00015224
Iteration 207/1000 | Loss: 0.00015224
Iteration 208/1000 | Loss: 0.00015224
Iteration 209/1000 | Loss: 0.00015224
Iteration 210/1000 | Loss: 0.00015224
Iteration 211/1000 | Loss: 0.00015223
Iteration 212/1000 | Loss: 0.00015223
Iteration 213/1000 | Loss: 0.00015223
Iteration 214/1000 | Loss: 0.00015222
Iteration 215/1000 | Loss: 0.00015222
Iteration 216/1000 | Loss: 0.00015222
Iteration 217/1000 | Loss: 0.00015222
Iteration 218/1000 | Loss: 0.00015222
Iteration 219/1000 | Loss: 0.00015222
Iteration 220/1000 | Loss: 0.00015222
Iteration 221/1000 | Loss: 0.00015222
Iteration 222/1000 | Loss: 0.00015222
Iteration 223/1000 | Loss: 0.00015222
Iteration 224/1000 | Loss: 0.00015221
Iteration 225/1000 | Loss: 0.00015221
Iteration 226/1000 | Loss: 0.00015221
Iteration 227/1000 | Loss: 0.00015221
Iteration 228/1000 | Loss: 0.00015221
Iteration 229/1000 | Loss: 0.00015221
Iteration 230/1000 | Loss: 0.00015221
Iteration 231/1000 | Loss: 0.00015221
Iteration 232/1000 | Loss: 0.00015221
Iteration 233/1000 | Loss: 0.00015220
Iteration 234/1000 | Loss: 0.00015220
Iteration 235/1000 | Loss: 0.00015220
Iteration 236/1000 | Loss: 0.00015220
Iteration 237/1000 | Loss: 0.00015220
Iteration 238/1000 | Loss: 0.00015220
Iteration 239/1000 | Loss: 0.00015220
Iteration 240/1000 | Loss: 0.00015220
Iteration 241/1000 | Loss: 0.00015220
Iteration 242/1000 | Loss: 0.00015220
Iteration 243/1000 | Loss: 0.00015220
Iteration 244/1000 | Loss: 0.00015219
Iteration 245/1000 | Loss: 0.00015219
Iteration 246/1000 | Loss: 0.00015219
Iteration 247/1000 | Loss: 0.00015218
Iteration 248/1000 | Loss: 0.00015218
Iteration 249/1000 | Loss: 0.00015218
Iteration 250/1000 | Loss: 0.00015217
Iteration 251/1000 | Loss: 0.00015217
Iteration 252/1000 | Loss: 0.00015217
Iteration 253/1000 | Loss: 0.00015217
Iteration 254/1000 | Loss: 0.00015217
Iteration 255/1000 | Loss: 0.00015217
Iteration 256/1000 | Loss: 0.00015217
Iteration 257/1000 | Loss: 0.00015217
Iteration 258/1000 | Loss: 0.00015216
Iteration 259/1000 | Loss: 0.00015216
Iteration 260/1000 | Loss: 0.00015216
Iteration 261/1000 | Loss: 0.00015216
Iteration 262/1000 | Loss: 0.00015216
Iteration 263/1000 | Loss: 0.00015216
Iteration 264/1000 | Loss: 0.00015216
Iteration 265/1000 | Loss: 0.00015216
Iteration 266/1000 | Loss: 0.00015216
Iteration 267/1000 | Loss: 0.00015215
Iteration 268/1000 | Loss: 0.00015215
Iteration 269/1000 | Loss: 0.00015215
Iteration 270/1000 | Loss: 0.00015215
Iteration 271/1000 | Loss: 0.00015214
Iteration 272/1000 | Loss: 0.00015214
Iteration 273/1000 | Loss: 0.00015214
Iteration 274/1000 | Loss: 0.00015214
Iteration 275/1000 | Loss: 0.00015214
Iteration 276/1000 | Loss: 0.00015214
Iteration 277/1000 | Loss: 0.00015214
Iteration 278/1000 | Loss: 0.00015214
Iteration 279/1000 | Loss: 0.00015214
Iteration 280/1000 | Loss: 0.00015214
Iteration 281/1000 | Loss: 0.00015214
Iteration 282/1000 | Loss: 0.00015214
Iteration 283/1000 | Loss: 0.00015213
Iteration 284/1000 | Loss: 0.00015213
Iteration 285/1000 | Loss: 0.00015213
Iteration 286/1000 | Loss: 0.00015213
Iteration 287/1000 | Loss: 0.00015213
Iteration 288/1000 | Loss: 0.00015213
Iteration 289/1000 | Loss: 0.00015213
Iteration 290/1000 | Loss: 0.00015213
Iteration 291/1000 | Loss: 0.00015213
Iteration 292/1000 | Loss: 0.00015213
Iteration 293/1000 | Loss: 0.00015213
Iteration 294/1000 | Loss: 0.00015213
Iteration 295/1000 | Loss: 0.00015213
Iteration 296/1000 | Loss: 0.00015213
Iteration 297/1000 | Loss: 0.00015213
Iteration 298/1000 | Loss: 0.00015212
Iteration 299/1000 | Loss: 0.00015212
Iteration 300/1000 | Loss: 0.00015212
Iteration 301/1000 | Loss: 0.00015212
Iteration 302/1000 | Loss: 0.00015212
Iteration 303/1000 | Loss: 0.00015212
Iteration 304/1000 | Loss: 0.00015212
Iteration 305/1000 | Loss: 0.00015212
Iteration 306/1000 | Loss: 0.00015212
Iteration 307/1000 | Loss: 0.00015212
Iteration 308/1000 | Loss: 0.00015212
Iteration 309/1000 | Loss: 0.00015212
Iteration 310/1000 | Loss: 0.00015212
Iteration 311/1000 | Loss: 0.00015212
Iteration 312/1000 | Loss: 0.00015212
Iteration 313/1000 | Loss: 0.00015212
Iteration 314/1000 | Loss: 0.00015212
Iteration 315/1000 | Loss: 0.00015212
Iteration 316/1000 | Loss: 0.00015212
Iteration 317/1000 | Loss: 0.00015212
Iteration 318/1000 | Loss: 0.00015212
Iteration 319/1000 | Loss: 0.00015212
Iteration 320/1000 | Loss: 0.00015212
Iteration 321/1000 | Loss: 0.00015212
Iteration 322/1000 | Loss: 0.00015212
Iteration 323/1000 | Loss: 0.00015212
Iteration 324/1000 | Loss: 0.00015212
Iteration 325/1000 | Loss: 0.00015212
Iteration 326/1000 | Loss: 0.00015212
Iteration 327/1000 | Loss: 0.00015212
Iteration 328/1000 | Loss: 0.00015212
Iteration 329/1000 | Loss: 0.00015212
Iteration 330/1000 | Loss: 0.00015212
Iteration 331/1000 | Loss: 0.00015212
Iteration 332/1000 | Loss: 0.00015212
Iteration 333/1000 | Loss: 0.00015212
Iteration 334/1000 | Loss: 0.00015212
Iteration 335/1000 | Loss: 0.00015212
Iteration 336/1000 | Loss: 0.00015212
Iteration 337/1000 | Loss: 0.00015212
Iteration 338/1000 | Loss: 0.00015212
Iteration 339/1000 | Loss: 0.00015212
Iteration 340/1000 | Loss: 0.00015212
Iteration 341/1000 | Loss: 0.00015212
Iteration 342/1000 | Loss: 0.00015212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [0.00015212185098789632, 0.00015212185098789632, 0.00015212185098789632, 0.00015212185098789632, 0.00015212185098789632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015212185098789632

Optimization complete. Final v2v error: 6.989039897918701 mm

Highest mean error: 10.841079711914062 mm for frame 56

Lowest mean error: 3.994929075241089 mm for frame 115

Saving results

Total time: 181.09643459320068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464907
Iteration 2/25 | Loss: 0.00110088
Iteration 3/25 | Loss: 0.00101846
Iteration 4/25 | Loss: 0.00100544
Iteration 5/25 | Loss: 0.00100088
Iteration 6/25 | Loss: 0.00099981
Iteration 7/25 | Loss: 0.00099981
Iteration 8/25 | Loss: 0.00099981
Iteration 9/25 | Loss: 0.00099981
Iteration 10/25 | Loss: 0.00099981
Iteration 11/25 | Loss: 0.00099981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009998076129704714, 0.0009998076129704714, 0.0009998076129704714, 0.0009998076129704714, 0.0009998076129704714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009998076129704714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53631186
Iteration 2/25 | Loss: 0.00075247
Iteration 3/25 | Loss: 0.00075247
Iteration 4/25 | Loss: 0.00075247
Iteration 5/25 | Loss: 0.00075247
Iteration 6/25 | Loss: 0.00075247
Iteration 7/25 | Loss: 0.00075247
Iteration 8/25 | Loss: 0.00075247
Iteration 9/25 | Loss: 0.00075247
Iteration 10/25 | Loss: 0.00075247
Iteration 11/25 | Loss: 0.00075247
Iteration 12/25 | Loss: 0.00075247
Iteration 13/25 | Loss: 0.00075247
Iteration 14/25 | Loss: 0.00075247
Iteration 15/25 | Loss: 0.00075247
Iteration 16/25 | Loss: 0.00075247
Iteration 17/25 | Loss: 0.00075247
Iteration 18/25 | Loss: 0.00075247
Iteration 19/25 | Loss: 0.00075247
Iteration 20/25 | Loss: 0.00075247
Iteration 21/25 | Loss: 0.00075247
Iteration 22/25 | Loss: 0.00075247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000752467371057719, 0.000752467371057719, 0.000752467371057719, 0.000752467371057719, 0.000752467371057719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000752467371057719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075247
Iteration 2/1000 | Loss: 0.00003074
Iteration 3/1000 | Loss: 0.00001612
Iteration 4/1000 | Loss: 0.00001232
Iteration 5/1000 | Loss: 0.00001140
Iteration 6/1000 | Loss: 0.00001109
Iteration 7/1000 | Loss: 0.00001082
Iteration 8/1000 | Loss: 0.00001059
Iteration 9/1000 | Loss: 0.00001054
Iteration 10/1000 | Loss: 0.00001049
Iteration 11/1000 | Loss: 0.00001047
Iteration 12/1000 | Loss: 0.00001047
Iteration 13/1000 | Loss: 0.00001040
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001032
Iteration 16/1000 | Loss: 0.00001032
Iteration 17/1000 | Loss: 0.00001026
Iteration 18/1000 | Loss: 0.00001026
Iteration 19/1000 | Loss: 0.00001026
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001023
Iteration 22/1000 | Loss: 0.00001022
Iteration 23/1000 | Loss: 0.00001021
Iteration 24/1000 | Loss: 0.00001021
Iteration 25/1000 | Loss: 0.00001021
Iteration 26/1000 | Loss: 0.00001021
Iteration 27/1000 | Loss: 0.00001020
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001020
Iteration 30/1000 | Loss: 0.00001019
Iteration 31/1000 | Loss: 0.00001017
Iteration 32/1000 | Loss: 0.00001016
Iteration 33/1000 | Loss: 0.00001015
Iteration 34/1000 | Loss: 0.00001015
Iteration 35/1000 | Loss: 0.00001015
Iteration 36/1000 | Loss: 0.00001015
Iteration 37/1000 | Loss: 0.00001015
Iteration 38/1000 | Loss: 0.00001014
Iteration 39/1000 | Loss: 0.00001014
Iteration 40/1000 | Loss: 0.00001014
Iteration 41/1000 | Loss: 0.00001013
Iteration 42/1000 | Loss: 0.00001013
Iteration 43/1000 | Loss: 0.00001013
Iteration 44/1000 | Loss: 0.00001012
Iteration 45/1000 | Loss: 0.00001012
Iteration 46/1000 | Loss: 0.00001011
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001010
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001009
Iteration 54/1000 | Loss: 0.00001009
Iteration 55/1000 | Loss: 0.00001009
Iteration 56/1000 | Loss: 0.00001009
Iteration 57/1000 | Loss: 0.00001009
Iteration 58/1000 | Loss: 0.00001009
Iteration 59/1000 | Loss: 0.00001009
Iteration 60/1000 | Loss: 0.00001009
Iteration 61/1000 | Loss: 0.00001008
Iteration 62/1000 | Loss: 0.00001008
Iteration 63/1000 | Loss: 0.00001007
Iteration 64/1000 | Loss: 0.00001006
Iteration 65/1000 | Loss: 0.00001006
Iteration 66/1000 | Loss: 0.00001006
Iteration 67/1000 | Loss: 0.00001004
Iteration 68/1000 | Loss: 0.00001004
Iteration 69/1000 | Loss: 0.00001004
Iteration 70/1000 | Loss: 0.00001003
Iteration 71/1000 | Loss: 0.00001003
Iteration 72/1000 | Loss: 0.00001002
Iteration 73/1000 | Loss: 0.00001002
Iteration 74/1000 | Loss: 0.00001001
Iteration 75/1000 | Loss: 0.00001001
Iteration 76/1000 | Loss: 0.00001001
Iteration 77/1000 | Loss: 0.00001000
Iteration 78/1000 | Loss: 0.00001000
Iteration 79/1000 | Loss: 0.00001000
Iteration 80/1000 | Loss: 0.00000999
Iteration 81/1000 | Loss: 0.00000999
Iteration 82/1000 | Loss: 0.00000999
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000998
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00000997
Iteration 87/1000 | Loss: 0.00000997
Iteration 88/1000 | Loss: 0.00000997
Iteration 89/1000 | Loss: 0.00000996
Iteration 90/1000 | Loss: 0.00000996
Iteration 91/1000 | Loss: 0.00000996
Iteration 92/1000 | Loss: 0.00000996
Iteration 93/1000 | Loss: 0.00000996
Iteration 94/1000 | Loss: 0.00000996
Iteration 95/1000 | Loss: 0.00000996
Iteration 96/1000 | Loss: 0.00000995
Iteration 97/1000 | Loss: 0.00000995
Iteration 98/1000 | Loss: 0.00000995
Iteration 99/1000 | Loss: 0.00000995
Iteration 100/1000 | Loss: 0.00000995
Iteration 101/1000 | Loss: 0.00000995
Iteration 102/1000 | Loss: 0.00000994
Iteration 103/1000 | Loss: 0.00000994
Iteration 104/1000 | Loss: 0.00000994
Iteration 105/1000 | Loss: 0.00000994
Iteration 106/1000 | Loss: 0.00000994
Iteration 107/1000 | Loss: 0.00000994
Iteration 108/1000 | Loss: 0.00000994
Iteration 109/1000 | Loss: 0.00000994
Iteration 110/1000 | Loss: 0.00000994
Iteration 111/1000 | Loss: 0.00000994
Iteration 112/1000 | Loss: 0.00000994
Iteration 113/1000 | Loss: 0.00000994
Iteration 114/1000 | Loss: 0.00000993
Iteration 115/1000 | Loss: 0.00000993
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000993
Iteration 122/1000 | Loss: 0.00000993
Iteration 123/1000 | Loss: 0.00000993
Iteration 124/1000 | Loss: 0.00000993
Iteration 125/1000 | Loss: 0.00000993
Iteration 126/1000 | Loss: 0.00000993
Iteration 127/1000 | Loss: 0.00000993
Iteration 128/1000 | Loss: 0.00000992
Iteration 129/1000 | Loss: 0.00000992
Iteration 130/1000 | Loss: 0.00000992
Iteration 131/1000 | Loss: 0.00000992
Iteration 132/1000 | Loss: 0.00000992
Iteration 133/1000 | Loss: 0.00000992
Iteration 134/1000 | Loss: 0.00000992
Iteration 135/1000 | Loss: 0.00000991
Iteration 136/1000 | Loss: 0.00000991
Iteration 137/1000 | Loss: 0.00000991
Iteration 138/1000 | Loss: 0.00000991
Iteration 139/1000 | Loss: 0.00000991
Iteration 140/1000 | Loss: 0.00000991
Iteration 141/1000 | Loss: 0.00000991
Iteration 142/1000 | Loss: 0.00000991
Iteration 143/1000 | Loss: 0.00000991
Iteration 144/1000 | Loss: 0.00000991
Iteration 145/1000 | Loss: 0.00000991
Iteration 146/1000 | Loss: 0.00000990
Iteration 147/1000 | Loss: 0.00000990
Iteration 148/1000 | Loss: 0.00000990
Iteration 149/1000 | Loss: 0.00000990
Iteration 150/1000 | Loss: 0.00000989
Iteration 151/1000 | Loss: 0.00000989
Iteration 152/1000 | Loss: 0.00000989
Iteration 153/1000 | Loss: 0.00000989
Iteration 154/1000 | Loss: 0.00000989
Iteration 155/1000 | Loss: 0.00000988
Iteration 156/1000 | Loss: 0.00000988
Iteration 157/1000 | Loss: 0.00000988
Iteration 158/1000 | Loss: 0.00000988
Iteration 159/1000 | Loss: 0.00000988
Iteration 160/1000 | Loss: 0.00000988
Iteration 161/1000 | Loss: 0.00000988
Iteration 162/1000 | Loss: 0.00000988
Iteration 163/1000 | Loss: 0.00000988
Iteration 164/1000 | Loss: 0.00000988
Iteration 165/1000 | Loss: 0.00000988
Iteration 166/1000 | Loss: 0.00000987
Iteration 167/1000 | Loss: 0.00000987
Iteration 168/1000 | Loss: 0.00000987
Iteration 169/1000 | Loss: 0.00000987
Iteration 170/1000 | Loss: 0.00000987
Iteration 171/1000 | Loss: 0.00000987
Iteration 172/1000 | Loss: 0.00000987
Iteration 173/1000 | Loss: 0.00000987
Iteration 174/1000 | Loss: 0.00000987
Iteration 175/1000 | Loss: 0.00000987
Iteration 176/1000 | Loss: 0.00000987
Iteration 177/1000 | Loss: 0.00000986
Iteration 178/1000 | Loss: 0.00000986
Iteration 179/1000 | Loss: 0.00000986
Iteration 180/1000 | Loss: 0.00000986
Iteration 181/1000 | Loss: 0.00000986
Iteration 182/1000 | Loss: 0.00000986
Iteration 183/1000 | Loss: 0.00000986
Iteration 184/1000 | Loss: 0.00000986
Iteration 185/1000 | Loss: 0.00000986
Iteration 186/1000 | Loss: 0.00000986
Iteration 187/1000 | Loss: 0.00000986
Iteration 188/1000 | Loss: 0.00000985
Iteration 189/1000 | Loss: 0.00000985
Iteration 190/1000 | Loss: 0.00000985
Iteration 191/1000 | Loss: 0.00000985
Iteration 192/1000 | Loss: 0.00000985
Iteration 193/1000 | Loss: 0.00000985
Iteration 194/1000 | Loss: 0.00000985
Iteration 195/1000 | Loss: 0.00000985
Iteration 196/1000 | Loss: 0.00000985
Iteration 197/1000 | Loss: 0.00000985
Iteration 198/1000 | Loss: 0.00000985
Iteration 199/1000 | Loss: 0.00000985
Iteration 200/1000 | Loss: 0.00000985
Iteration 201/1000 | Loss: 0.00000985
Iteration 202/1000 | Loss: 0.00000984
Iteration 203/1000 | Loss: 0.00000984
Iteration 204/1000 | Loss: 0.00000984
Iteration 205/1000 | Loss: 0.00000984
Iteration 206/1000 | Loss: 0.00000984
Iteration 207/1000 | Loss: 0.00000984
Iteration 208/1000 | Loss: 0.00000984
Iteration 209/1000 | Loss: 0.00000984
Iteration 210/1000 | Loss: 0.00000984
Iteration 211/1000 | Loss: 0.00000984
Iteration 212/1000 | Loss: 0.00000983
Iteration 213/1000 | Loss: 0.00000983
Iteration 214/1000 | Loss: 0.00000983
Iteration 215/1000 | Loss: 0.00000983
Iteration 216/1000 | Loss: 0.00000983
Iteration 217/1000 | Loss: 0.00000983
Iteration 218/1000 | Loss: 0.00000983
Iteration 219/1000 | Loss: 0.00000983
Iteration 220/1000 | Loss: 0.00000983
Iteration 221/1000 | Loss: 0.00000983
Iteration 222/1000 | Loss: 0.00000983
Iteration 223/1000 | Loss: 0.00000983
Iteration 224/1000 | Loss: 0.00000983
Iteration 225/1000 | Loss: 0.00000983
Iteration 226/1000 | Loss: 0.00000983
Iteration 227/1000 | Loss: 0.00000983
Iteration 228/1000 | Loss: 0.00000983
Iteration 229/1000 | Loss: 0.00000982
Iteration 230/1000 | Loss: 0.00000982
Iteration 231/1000 | Loss: 0.00000982
Iteration 232/1000 | Loss: 0.00000982
Iteration 233/1000 | Loss: 0.00000982
Iteration 234/1000 | Loss: 0.00000982
Iteration 235/1000 | Loss: 0.00000981
Iteration 236/1000 | Loss: 0.00000981
Iteration 237/1000 | Loss: 0.00000981
Iteration 238/1000 | Loss: 0.00000981
Iteration 239/1000 | Loss: 0.00000981
Iteration 240/1000 | Loss: 0.00000981
Iteration 241/1000 | Loss: 0.00000981
Iteration 242/1000 | Loss: 0.00000981
Iteration 243/1000 | Loss: 0.00000981
Iteration 244/1000 | Loss: 0.00000981
Iteration 245/1000 | Loss: 0.00000981
Iteration 246/1000 | Loss: 0.00000981
Iteration 247/1000 | Loss: 0.00000981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [9.810429219214711e-06, 9.810429219214711e-06, 9.810429219214711e-06, 9.810429219214711e-06, 9.810429219214711e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.810429219214711e-06

Optimization complete. Final v2v error: 2.62864089012146 mm

Highest mean error: 3.231323003768921 mm for frame 40

Lowest mean error: 2.3656222820281982 mm for frame 99

Saving results

Total time: 38.93242835998535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730435
Iteration 2/25 | Loss: 0.00154015
Iteration 3/25 | Loss: 0.00121165
Iteration 4/25 | Loss: 0.00115344
Iteration 5/25 | Loss: 0.00113798
Iteration 6/25 | Loss: 0.00113386
Iteration 7/25 | Loss: 0.00113275
Iteration 8/25 | Loss: 0.00113226
Iteration 9/25 | Loss: 0.00113426
Iteration 10/25 | Loss: 0.00113147
Iteration 11/25 | Loss: 0.00112939
Iteration 12/25 | Loss: 0.00112866
Iteration 13/25 | Loss: 0.00112822
Iteration 14/25 | Loss: 0.00112895
Iteration 15/25 | Loss: 0.00116133
Iteration 16/25 | Loss: 0.00112384
Iteration 17/25 | Loss: 0.00111510
Iteration 18/25 | Loss: 0.00111121
Iteration 19/25 | Loss: 0.00110960
Iteration 20/25 | Loss: 0.00111465
Iteration 21/25 | Loss: 0.00111495
Iteration 22/25 | Loss: 0.00110831
Iteration 23/25 | Loss: 0.00110479
Iteration 24/25 | Loss: 0.00110444
Iteration 25/25 | Loss: 0.00110435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.31847048
Iteration 2/25 | Loss: 0.00089739
Iteration 3/25 | Loss: 0.00089737
Iteration 4/25 | Loss: 0.00089737
Iteration 5/25 | Loss: 0.00089737
Iteration 6/25 | Loss: 0.00089737
Iteration 7/25 | Loss: 0.00089737
Iteration 8/25 | Loss: 0.00089737
Iteration 9/25 | Loss: 0.00089737
Iteration 10/25 | Loss: 0.00089737
Iteration 11/25 | Loss: 0.00089737
Iteration 12/25 | Loss: 0.00089737
Iteration 13/25 | Loss: 0.00089737
Iteration 14/25 | Loss: 0.00089737
Iteration 15/25 | Loss: 0.00089737
Iteration 16/25 | Loss: 0.00089737
Iteration 17/25 | Loss: 0.00089737
Iteration 18/25 | Loss: 0.00089737
Iteration 19/25 | Loss: 0.00089737
Iteration 20/25 | Loss: 0.00089737
Iteration 21/25 | Loss: 0.00089737
Iteration 22/25 | Loss: 0.00089737
Iteration 23/25 | Loss: 0.00089737
Iteration 24/25 | Loss: 0.00089737
Iteration 25/25 | Loss: 0.00089737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089737
Iteration 2/1000 | Loss: 0.00006887
Iteration 3/1000 | Loss: 0.00004023
Iteration 4/1000 | Loss: 0.00003000
Iteration 5/1000 | Loss: 0.00002723
Iteration 6/1000 | Loss: 0.00002593
Iteration 7/1000 | Loss: 0.00002499
Iteration 8/1000 | Loss: 0.00002432
Iteration 9/1000 | Loss: 0.00002373
Iteration 10/1000 | Loss: 0.00002333
Iteration 11/1000 | Loss: 0.00002304
Iteration 12/1000 | Loss: 0.00002292
Iteration 13/1000 | Loss: 0.00002271
Iteration 14/1000 | Loss: 0.00002251
Iteration 15/1000 | Loss: 0.00002241
Iteration 16/1000 | Loss: 0.00002235
Iteration 17/1000 | Loss: 0.00002227
Iteration 18/1000 | Loss: 0.00002216
Iteration 19/1000 | Loss: 0.00002216
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002210
Iteration 22/1000 | Loss: 0.00002206
Iteration 23/1000 | Loss: 0.00002201
Iteration 24/1000 | Loss: 0.00002195
Iteration 25/1000 | Loss: 0.00002194
Iteration 26/1000 | Loss: 0.00002193
Iteration 27/1000 | Loss: 0.00002193
Iteration 28/1000 | Loss: 0.00002193
Iteration 29/1000 | Loss: 0.00002192
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002191
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002190
Iteration 34/1000 | Loss: 0.00002189
Iteration 35/1000 | Loss: 0.00002189
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002186
Iteration 39/1000 | Loss: 0.00002185
Iteration 40/1000 | Loss: 0.00002185
Iteration 41/1000 | Loss: 0.00002184
Iteration 42/1000 | Loss: 0.00002184
Iteration 43/1000 | Loss: 0.00002184
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00002183
Iteration 46/1000 | Loss: 0.00002183
Iteration 47/1000 | Loss: 0.00002182
Iteration 48/1000 | Loss: 0.00002182
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002180
Iteration 53/1000 | Loss: 0.00002178
Iteration 54/1000 | Loss: 0.00002178
Iteration 55/1000 | Loss: 0.00002178
Iteration 56/1000 | Loss: 0.00002177
Iteration 57/1000 | Loss: 0.00002177
Iteration 58/1000 | Loss: 0.00002176
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002176
Iteration 61/1000 | Loss: 0.00002175
Iteration 62/1000 | Loss: 0.00002175
Iteration 63/1000 | Loss: 0.00002175
Iteration 64/1000 | Loss: 0.00002175
Iteration 65/1000 | Loss: 0.00002174
Iteration 66/1000 | Loss: 0.00002174
Iteration 67/1000 | Loss: 0.00002174
Iteration 68/1000 | Loss: 0.00002173
Iteration 69/1000 | Loss: 0.00002173
Iteration 70/1000 | Loss: 0.00002173
Iteration 71/1000 | Loss: 0.00002173
Iteration 72/1000 | Loss: 0.00002173
Iteration 73/1000 | Loss: 0.00002173
Iteration 74/1000 | Loss: 0.00002173
Iteration 75/1000 | Loss: 0.00002173
Iteration 76/1000 | Loss: 0.00002173
Iteration 77/1000 | Loss: 0.00002173
Iteration 78/1000 | Loss: 0.00002172
Iteration 79/1000 | Loss: 0.00002172
Iteration 80/1000 | Loss: 0.00002172
Iteration 81/1000 | Loss: 0.00002171
Iteration 82/1000 | Loss: 0.00002171
Iteration 83/1000 | Loss: 0.00002171
Iteration 84/1000 | Loss: 0.00002170
Iteration 85/1000 | Loss: 0.00002170
Iteration 86/1000 | Loss: 0.00002170
Iteration 87/1000 | Loss: 0.00002170
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002169
Iteration 90/1000 | Loss: 0.00002169
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002168
Iteration 93/1000 | Loss: 0.00002168
Iteration 94/1000 | Loss: 0.00002167
Iteration 95/1000 | Loss: 0.00002167
Iteration 96/1000 | Loss: 0.00002167
Iteration 97/1000 | Loss: 0.00002167
Iteration 98/1000 | Loss: 0.00002167
Iteration 99/1000 | Loss: 0.00002167
Iteration 100/1000 | Loss: 0.00002166
Iteration 101/1000 | Loss: 0.00002166
Iteration 102/1000 | Loss: 0.00002166
Iteration 103/1000 | Loss: 0.00002166
Iteration 104/1000 | Loss: 0.00002165
Iteration 105/1000 | Loss: 0.00002165
Iteration 106/1000 | Loss: 0.00002165
Iteration 107/1000 | Loss: 0.00002165
Iteration 108/1000 | Loss: 0.00002165
Iteration 109/1000 | Loss: 0.00002164
Iteration 110/1000 | Loss: 0.00002164
Iteration 111/1000 | Loss: 0.00002164
Iteration 112/1000 | Loss: 0.00002164
Iteration 113/1000 | Loss: 0.00002163
Iteration 114/1000 | Loss: 0.00002163
Iteration 115/1000 | Loss: 0.00002163
Iteration 116/1000 | Loss: 0.00002163
Iteration 117/1000 | Loss: 0.00002162
Iteration 118/1000 | Loss: 0.00002162
Iteration 119/1000 | Loss: 0.00002162
Iteration 120/1000 | Loss: 0.00002161
Iteration 121/1000 | Loss: 0.00002161
Iteration 122/1000 | Loss: 0.00002161
Iteration 123/1000 | Loss: 0.00002160
Iteration 124/1000 | Loss: 0.00002160
Iteration 125/1000 | Loss: 0.00002160
Iteration 126/1000 | Loss: 0.00002159
Iteration 127/1000 | Loss: 0.00002159
Iteration 128/1000 | Loss: 0.00002159
Iteration 129/1000 | Loss: 0.00002159
Iteration 130/1000 | Loss: 0.00002158
Iteration 131/1000 | Loss: 0.00002158
Iteration 132/1000 | Loss: 0.00002158
Iteration 133/1000 | Loss: 0.00002158
Iteration 134/1000 | Loss: 0.00002157
Iteration 135/1000 | Loss: 0.00002157
Iteration 136/1000 | Loss: 0.00002157
Iteration 137/1000 | Loss: 0.00002157
Iteration 138/1000 | Loss: 0.00002157
Iteration 139/1000 | Loss: 0.00002156
Iteration 140/1000 | Loss: 0.00002156
Iteration 141/1000 | Loss: 0.00002156
Iteration 142/1000 | Loss: 0.00002156
Iteration 143/1000 | Loss: 0.00002155
Iteration 144/1000 | Loss: 0.00002155
Iteration 145/1000 | Loss: 0.00002155
Iteration 146/1000 | Loss: 0.00002155
Iteration 147/1000 | Loss: 0.00002154
Iteration 148/1000 | Loss: 0.00002154
Iteration 149/1000 | Loss: 0.00002154
Iteration 150/1000 | Loss: 0.00002154
Iteration 151/1000 | Loss: 0.00002154
Iteration 152/1000 | Loss: 0.00002154
Iteration 153/1000 | Loss: 0.00002154
Iteration 154/1000 | Loss: 0.00002154
Iteration 155/1000 | Loss: 0.00002153
Iteration 156/1000 | Loss: 0.00002153
Iteration 157/1000 | Loss: 0.00002153
Iteration 158/1000 | Loss: 0.00002153
Iteration 159/1000 | Loss: 0.00002153
Iteration 160/1000 | Loss: 0.00002153
Iteration 161/1000 | Loss: 0.00002153
Iteration 162/1000 | Loss: 0.00002153
Iteration 163/1000 | Loss: 0.00002153
Iteration 164/1000 | Loss: 0.00002153
Iteration 165/1000 | Loss: 0.00002153
Iteration 166/1000 | Loss: 0.00002153
Iteration 167/1000 | Loss: 0.00002153
Iteration 168/1000 | Loss: 0.00002153
Iteration 169/1000 | Loss: 0.00002153
Iteration 170/1000 | Loss: 0.00002153
Iteration 171/1000 | Loss: 0.00002153
Iteration 172/1000 | Loss: 0.00002152
Iteration 173/1000 | Loss: 0.00002152
Iteration 174/1000 | Loss: 0.00002152
Iteration 175/1000 | Loss: 0.00002152
Iteration 176/1000 | Loss: 0.00002152
Iteration 177/1000 | Loss: 0.00002152
Iteration 178/1000 | Loss: 0.00002151
Iteration 179/1000 | Loss: 0.00002151
Iteration 180/1000 | Loss: 0.00002151
Iteration 181/1000 | Loss: 0.00002151
Iteration 182/1000 | Loss: 0.00002151
Iteration 183/1000 | Loss: 0.00002151
Iteration 184/1000 | Loss: 0.00002151
Iteration 185/1000 | Loss: 0.00002151
Iteration 186/1000 | Loss: 0.00002151
Iteration 187/1000 | Loss: 0.00002151
Iteration 188/1000 | Loss: 0.00002151
Iteration 189/1000 | Loss: 0.00002150
Iteration 190/1000 | Loss: 0.00002150
Iteration 191/1000 | Loss: 0.00002150
Iteration 192/1000 | Loss: 0.00002150
Iteration 193/1000 | Loss: 0.00002150
Iteration 194/1000 | Loss: 0.00002150
Iteration 195/1000 | Loss: 0.00002150
Iteration 196/1000 | Loss: 0.00002150
Iteration 197/1000 | Loss: 0.00002150
Iteration 198/1000 | Loss: 0.00002150
Iteration 199/1000 | Loss: 0.00002150
Iteration 200/1000 | Loss: 0.00002150
Iteration 201/1000 | Loss: 0.00002150
Iteration 202/1000 | Loss: 0.00002150
Iteration 203/1000 | Loss: 0.00002150
Iteration 204/1000 | Loss: 0.00002150
Iteration 205/1000 | Loss: 0.00002150
Iteration 206/1000 | Loss: 0.00002150
Iteration 207/1000 | Loss: 0.00002150
Iteration 208/1000 | Loss: 0.00002150
Iteration 209/1000 | Loss: 0.00002150
Iteration 210/1000 | Loss: 0.00002150
Iteration 211/1000 | Loss: 0.00002150
Iteration 212/1000 | Loss: 0.00002150
Iteration 213/1000 | Loss: 0.00002150
Iteration 214/1000 | Loss: 0.00002150
Iteration 215/1000 | Loss: 0.00002150
Iteration 216/1000 | Loss: 0.00002150
Iteration 217/1000 | Loss: 0.00002150
Iteration 218/1000 | Loss: 0.00002150
Iteration 219/1000 | Loss: 0.00002150
Iteration 220/1000 | Loss: 0.00002150
Iteration 221/1000 | Loss: 0.00002150
Iteration 222/1000 | Loss: 0.00002150
Iteration 223/1000 | Loss: 0.00002150
Iteration 224/1000 | Loss: 0.00002150
Iteration 225/1000 | Loss: 0.00002150
Iteration 226/1000 | Loss: 0.00002150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.1500696675502695e-05, 2.1500696675502695e-05, 2.1500696675502695e-05, 2.1500696675502695e-05, 2.1500696675502695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1500696675502695e-05

Optimization complete. Final v2v error: 3.758928060531616 mm

Highest mean error: 5.663912773132324 mm for frame 64

Lowest mean error: 2.758432388305664 mm for frame 107

Saving results

Total time: 81.02755212783813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762532
Iteration 2/25 | Loss: 0.00174894
Iteration 3/25 | Loss: 0.00112877
Iteration 4/25 | Loss: 0.00102606
Iteration 5/25 | Loss: 0.00101035
Iteration 6/25 | Loss: 0.00101714
Iteration 7/25 | Loss: 0.00101652
Iteration 8/25 | Loss: 0.00100305
Iteration 9/25 | Loss: 0.00099514
Iteration 10/25 | Loss: 0.00099173
Iteration 11/25 | Loss: 0.00099021
Iteration 12/25 | Loss: 0.00098965
Iteration 13/25 | Loss: 0.00098948
Iteration 14/25 | Loss: 0.00098945
Iteration 15/25 | Loss: 0.00098945
Iteration 16/25 | Loss: 0.00098944
Iteration 17/25 | Loss: 0.00098944
Iteration 18/25 | Loss: 0.00098944
Iteration 19/25 | Loss: 0.00098944
Iteration 20/25 | Loss: 0.00098944
Iteration 21/25 | Loss: 0.00098944
Iteration 22/25 | Loss: 0.00098943
Iteration 23/25 | Loss: 0.00098943
Iteration 24/25 | Loss: 0.00098943
Iteration 25/25 | Loss: 0.00098943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87018251
Iteration 2/25 | Loss: 0.00069851
Iteration 3/25 | Loss: 0.00069851
Iteration 4/25 | Loss: 0.00069851
Iteration 5/25 | Loss: 0.00069851
Iteration 6/25 | Loss: 0.00069851
Iteration 7/25 | Loss: 0.00069851
Iteration 8/25 | Loss: 0.00069851
Iteration 9/25 | Loss: 0.00069851
Iteration 10/25 | Loss: 0.00069851
Iteration 11/25 | Loss: 0.00069851
Iteration 12/25 | Loss: 0.00069851
Iteration 13/25 | Loss: 0.00069851
Iteration 14/25 | Loss: 0.00069851
Iteration 15/25 | Loss: 0.00069851
Iteration 16/25 | Loss: 0.00069851
Iteration 17/25 | Loss: 0.00069851
Iteration 18/25 | Loss: 0.00069851
Iteration 19/25 | Loss: 0.00069851
Iteration 20/25 | Loss: 0.00069851
Iteration 21/25 | Loss: 0.00069851
Iteration 22/25 | Loss: 0.00069851
Iteration 23/25 | Loss: 0.00069851
Iteration 24/25 | Loss: 0.00069851
Iteration 25/25 | Loss: 0.00069851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069851
Iteration 2/1000 | Loss: 0.00002022
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001368
Iteration 7/1000 | Loss: 0.00001331
Iteration 8/1000 | Loss: 0.00001296
Iteration 9/1000 | Loss: 0.00001267
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001229
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001228
Iteration 15/1000 | Loss: 0.00001225
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001198
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001193
Iteration 24/1000 | Loss: 0.00001192
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001190
Iteration 29/1000 | Loss: 0.00001189
Iteration 30/1000 | Loss: 0.00001189
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001187
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001186
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001186
Iteration 40/1000 | Loss: 0.00001186
Iteration 41/1000 | Loss: 0.00001186
Iteration 42/1000 | Loss: 0.00001186
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001185
Iteration 48/1000 | Loss: 0.00001185
Iteration 49/1000 | Loss: 0.00001185
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.1780351087509189e-05, 1.1780351087509189e-05, 1.1780351087509189e-05, 1.1780351087509189e-05, 1.1780351087509189e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1780351087509189e-05

Optimization complete. Final v2v error: 2.9466938972473145 mm

Highest mean error: 3.222435235977173 mm for frame 88

Lowest mean error: 2.729694366455078 mm for frame 36

Saving results

Total time: 50.05232071876526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429063
Iteration 2/25 | Loss: 0.00109727
Iteration 3/25 | Loss: 0.00100243
Iteration 4/25 | Loss: 0.00098754
Iteration 5/25 | Loss: 0.00098262
Iteration 6/25 | Loss: 0.00098140
Iteration 7/25 | Loss: 0.00098140
Iteration 8/25 | Loss: 0.00098140
Iteration 9/25 | Loss: 0.00098140
Iteration 10/25 | Loss: 0.00098140
Iteration 11/25 | Loss: 0.00098140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009814038639888167, 0.0009814038639888167, 0.0009814038639888167, 0.0009814038639888167, 0.0009814038639888167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009814038639888167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.61642933
Iteration 2/25 | Loss: 0.00076316
Iteration 3/25 | Loss: 0.00076315
Iteration 4/25 | Loss: 0.00076315
Iteration 5/25 | Loss: 0.00076315
Iteration 6/25 | Loss: 0.00076315
Iteration 7/25 | Loss: 0.00076315
Iteration 8/25 | Loss: 0.00076315
Iteration 9/25 | Loss: 0.00076315
Iteration 10/25 | Loss: 0.00076315
Iteration 11/25 | Loss: 0.00076315
Iteration 12/25 | Loss: 0.00076315
Iteration 13/25 | Loss: 0.00076315
Iteration 14/25 | Loss: 0.00076315
Iteration 15/25 | Loss: 0.00076315
Iteration 16/25 | Loss: 0.00076315
Iteration 17/25 | Loss: 0.00076315
Iteration 18/25 | Loss: 0.00076315
Iteration 19/25 | Loss: 0.00076315
Iteration 20/25 | Loss: 0.00076315
Iteration 21/25 | Loss: 0.00076315
Iteration 22/25 | Loss: 0.00076315
Iteration 23/25 | Loss: 0.00076315
Iteration 24/25 | Loss: 0.00076315
Iteration 25/25 | Loss: 0.00076315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007631476037204266, 0.0007631476037204266, 0.0007631476037204266, 0.0007631476037204266, 0.0007631476037204266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007631476037204266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076315
Iteration 2/1000 | Loss: 0.00001948
Iteration 3/1000 | Loss: 0.00001421
Iteration 4/1000 | Loss: 0.00001258
Iteration 5/1000 | Loss: 0.00001190
Iteration 6/1000 | Loss: 0.00001138
Iteration 7/1000 | Loss: 0.00001098
Iteration 8/1000 | Loss: 0.00001070
Iteration 9/1000 | Loss: 0.00001061
Iteration 10/1000 | Loss: 0.00001039
Iteration 11/1000 | Loss: 0.00001027
Iteration 12/1000 | Loss: 0.00001026
Iteration 13/1000 | Loss: 0.00001019
Iteration 14/1000 | Loss: 0.00001018
Iteration 15/1000 | Loss: 0.00001016
Iteration 16/1000 | Loss: 0.00001014
Iteration 17/1000 | Loss: 0.00001013
Iteration 18/1000 | Loss: 0.00001009
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00000999
Iteration 21/1000 | Loss: 0.00000998
Iteration 22/1000 | Loss: 0.00000997
Iteration 23/1000 | Loss: 0.00000996
Iteration 24/1000 | Loss: 0.00000996
Iteration 25/1000 | Loss: 0.00000995
Iteration 26/1000 | Loss: 0.00000995
Iteration 27/1000 | Loss: 0.00000992
Iteration 28/1000 | Loss: 0.00000991
Iteration 29/1000 | Loss: 0.00000990
Iteration 30/1000 | Loss: 0.00000989
Iteration 31/1000 | Loss: 0.00000989
Iteration 32/1000 | Loss: 0.00000989
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000987
Iteration 36/1000 | Loss: 0.00000986
Iteration 37/1000 | Loss: 0.00000985
Iteration 38/1000 | Loss: 0.00000985
Iteration 39/1000 | Loss: 0.00000985
Iteration 40/1000 | Loss: 0.00000985
Iteration 41/1000 | Loss: 0.00000985
Iteration 42/1000 | Loss: 0.00000985
Iteration 43/1000 | Loss: 0.00000984
Iteration 44/1000 | Loss: 0.00000984
Iteration 45/1000 | Loss: 0.00000983
Iteration 46/1000 | Loss: 0.00000983
Iteration 47/1000 | Loss: 0.00000982
Iteration 48/1000 | Loss: 0.00000981
Iteration 49/1000 | Loss: 0.00000981
Iteration 50/1000 | Loss: 0.00000979
Iteration 51/1000 | Loss: 0.00000979
Iteration 52/1000 | Loss: 0.00000978
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000974
Iteration 55/1000 | Loss: 0.00000974
Iteration 56/1000 | Loss: 0.00000973
Iteration 57/1000 | Loss: 0.00000973
Iteration 58/1000 | Loss: 0.00000973
Iteration 59/1000 | Loss: 0.00000972
Iteration 60/1000 | Loss: 0.00000972
Iteration 61/1000 | Loss: 0.00000971
Iteration 62/1000 | Loss: 0.00000971
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000969
Iteration 68/1000 | Loss: 0.00000969
Iteration 69/1000 | Loss: 0.00000969
Iteration 70/1000 | Loss: 0.00000968
Iteration 71/1000 | Loss: 0.00000968
Iteration 72/1000 | Loss: 0.00000968
Iteration 73/1000 | Loss: 0.00000968
Iteration 74/1000 | Loss: 0.00000968
Iteration 75/1000 | Loss: 0.00000968
Iteration 76/1000 | Loss: 0.00000967
Iteration 77/1000 | Loss: 0.00000967
Iteration 78/1000 | Loss: 0.00000967
Iteration 79/1000 | Loss: 0.00000967
Iteration 80/1000 | Loss: 0.00000967
Iteration 81/1000 | Loss: 0.00000967
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000967
Iteration 84/1000 | Loss: 0.00000966
Iteration 85/1000 | Loss: 0.00000965
Iteration 86/1000 | Loss: 0.00000965
Iteration 87/1000 | Loss: 0.00000965
Iteration 88/1000 | Loss: 0.00000965
Iteration 89/1000 | Loss: 0.00000964
Iteration 90/1000 | Loss: 0.00000964
Iteration 91/1000 | Loss: 0.00000964
Iteration 92/1000 | Loss: 0.00000964
Iteration 93/1000 | Loss: 0.00000964
Iteration 94/1000 | Loss: 0.00000963
Iteration 95/1000 | Loss: 0.00000963
Iteration 96/1000 | Loss: 0.00000963
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000963
Iteration 101/1000 | Loss: 0.00000963
Iteration 102/1000 | Loss: 0.00000962
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000962
Iteration 107/1000 | Loss: 0.00000962
Iteration 108/1000 | Loss: 0.00000962
Iteration 109/1000 | Loss: 0.00000962
Iteration 110/1000 | Loss: 0.00000962
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000961
Iteration 118/1000 | Loss: 0.00000961
Iteration 119/1000 | Loss: 0.00000961
Iteration 120/1000 | Loss: 0.00000961
Iteration 121/1000 | Loss: 0.00000961
Iteration 122/1000 | Loss: 0.00000961
Iteration 123/1000 | Loss: 0.00000961
Iteration 124/1000 | Loss: 0.00000961
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000961
Iteration 129/1000 | Loss: 0.00000961
Iteration 130/1000 | Loss: 0.00000961
Iteration 131/1000 | Loss: 0.00000961
Iteration 132/1000 | Loss: 0.00000961
Iteration 133/1000 | Loss: 0.00000961
Iteration 134/1000 | Loss: 0.00000961
Iteration 135/1000 | Loss: 0.00000961
Iteration 136/1000 | Loss: 0.00000961
Iteration 137/1000 | Loss: 0.00000961
Iteration 138/1000 | Loss: 0.00000961
Iteration 139/1000 | Loss: 0.00000961
Iteration 140/1000 | Loss: 0.00000961
Iteration 141/1000 | Loss: 0.00000961
Iteration 142/1000 | Loss: 0.00000961
Iteration 143/1000 | Loss: 0.00000961
Iteration 144/1000 | Loss: 0.00000961
Iteration 145/1000 | Loss: 0.00000961
Iteration 146/1000 | Loss: 0.00000961
Iteration 147/1000 | Loss: 0.00000961
Iteration 148/1000 | Loss: 0.00000961
Iteration 149/1000 | Loss: 0.00000961
Iteration 150/1000 | Loss: 0.00000961
Iteration 151/1000 | Loss: 0.00000961
Iteration 152/1000 | Loss: 0.00000961
Iteration 153/1000 | Loss: 0.00000961
Iteration 154/1000 | Loss: 0.00000961
Iteration 155/1000 | Loss: 0.00000961
Iteration 156/1000 | Loss: 0.00000961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [9.609713742975146e-06, 9.609713742975146e-06, 9.609713742975146e-06, 9.609713742975146e-06, 9.609713742975146e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.609713742975146e-06

Optimization complete. Final v2v error: 2.6628036499023438 mm

Highest mean error: 3.202721357345581 mm for frame 92

Lowest mean error: 2.3332178592681885 mm for frame 204

Saving results

Total time: 39.11454105377197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532653
Iteration 2/25 | Loss: 0.00106168
Iteration 3/25 | Loss: 0.00098601
Iteration 4/25 | Loss: 0.00097424
Iteration 5/25 | Loss: 0.00097009
Iteration 6/25 | Loss: 0.00096913
Iteration 7/25 | Loss: 0.00096913
Iteration 8/25 | Loss: 0.00096913
Iteration 9/25 | Loss: 0.00096913
Iteration 10/25 | Loss: 0.00096913
Iteration 11/25 | Loss: 0.00096913
Iteration 12/25 | Loss: 0.00096913
Iteration 13/25 | Loss: 0.00096913
Iteration 14/25 | Loss: 0.00096913
Iteration 15/25 | Loss: 0.00096913
Iteration 16/25 | Loss: 0.00096913
Iteration 17/25 | Loss: 0.00096913
Iteration 18/25 | Loss: 0.00096913
Iteration 19/25 | Loss: 0.00096913
Iteration 20/25 | Loss: 0.00096913
Iteration 21/25 | Loss: 0.00096913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009691321174614131, 0.0009691321174614131, 0.0009691321174614131, 0.0009691321174614131, 0.0009691321174614131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009691321174614131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.65607929
Iteration 2/25 | Loss: 0.00070712
Iteration 3/25 | Loss: 0.00070712
Iteration 4/25 | Loss: 0.00070712
Iteration 5/25 | Loss: 0.00070712
Iteration 6/25 | Loss: 0.00070712
Iteration 7/25 | Loss: 0.00070712
Iteration 8/25 | Loss: 0.00070712
Iteration 9/25 | Loss: 0.00070712
Iteration 10/25 | Loss: 0.00070712
Iteration 11/25 | Loss: 0.00070712
Iteration 12/25 | Loss: 0.00070712
Iteration 13/25 | Loss: 0.00070712
Iteration 14/25 | Loss: 0.00070712
Iteration 15/25 | Loss: 0.00070712
Iteration 16/25 | Loss: 0.00070712
Iteration 17/25 | Loss: 0.00070712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007071187137626112, 0.0007071187137626112, 0.0007071187137626112, 0.0007071187137626112, 0.0007071187137626112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007071187137626112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070712
Iteration 2/1000 | Loss: 0.00002125
Iteration 3/1000 | Loss: 0.00001480
Iteration 4/1000 | Loss: 0.00001272
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001123
Iteration 7/1000 | Loss: 0.00001085
Iteration 8/1000 | Loss: 0.00001043
Iteration 9/1000 | Loss: 0.00001040
Iteration 10/1000 | Loss: 0.00001039
Iteration 11/1000 | Loss: 0.00001012
Iteration 12/1000 | Loss: 0.00000994
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000981
Iteration 15/1000 | Loss: 0.00000981
Iteration 16/1000 | Loss: 0.00000981
Iteration 17/1000 | Loss: 0.00000979
Iteration 18/1000 | Loss: 0.00000974
Iteration 19/1000 | Loss: 0.00000973
Iteration 20/1000 | Loss: 0.00000967
Iteration 21/1000 | Loss: 0.00000964
Iteration 22/1000 | Loss: 0.00000962
Iteration 23/1000 | Loss: 0.00000959
Iteration 24/1000 | Loss: 0.00000958
Iteration 25/1000 | Loss: 0.00000958
Iteration 26/1000 | Loss: 0.00000958
Iteration 27/1000 | Loss: 0.00000957
Iteration 28/1000 | Loss: 0.00000956
Iteration 29/1000 | Loss: 0.00000956
Iteration 30/1000 | Loss: 0.00000956
Iteration 31/1000 | Loss: 0.00000955
Iteration 32/1000 | Loss: 0.00000953
Iteration 33/1000 | Loss: 0.00000952
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000951
Iteration 36/1000 | Loss: 0.00000951
Iteration 37/1000 | Loss: 0.00000950
Iteration 38/1000 | Loss: 0.00000950
Iteration 39/1000 | Loss: 0.00000949
Iteration 40/1000 | Loss: 0.00000949
Iteration 41/1000 | Loss: 0.00000949
Iteration 42/1000 | Loss: 0.00000949
Iteration 43/1000 | Loss: 0.00000949
Iteration 44/1000 | Loss: 0.00000949
Iteration 45/1000 | Loss: 0.00000949
Iteration 46/1000 | Loss: 0.00000949
Iteration 47/1000 | Loss: 0.00000949
Iteration 48/1000 | Loss: 0.00000949
Iteration 49/1000 | Loss: 0.00000949
Iteration 50/1000 | Loss: 0.00000949
Iteration 51/1000 | Loss: 0.00000949
Iteration 52/1000 | Loss: 0.00000949
Iteration 53/1000 | Loss: 0.00000949
Iteration 54/1000 | Loss: 0.00000948
Iteration 55/1000 | Loss: 0.00000948
Iteration 56/1000 | Loss: 0.00000948
Iteration 57/1000 | Loss: 0.00000948
Iteration 58/1000 | Loss: 0.00000948
Iteration 59/1000 | Loss: 0.00000948
Iteration 60/1000 | Loss: 0.00000948
Iteration 61/1000 | Loss: 0.00000948
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000948
Iteration 64/1000 | Loss: 0.00000948
Iteration 65/1000 | Loss: 0.00000948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [9.484078873356339e-06, 9.484078873356339e-06, 9.484078873356339e-06, 9.484078873356339e-06, 9.484078873356339e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.484078873356339e-06

Optimization complete. Final v2v error: 2.6799464225769043 mm

Highest mean error: 3.0561840534210205 mm for frame 150

Lowest mean error: 2.464449882507324 mm for frame 43

Saving results

Total time: 28.86129355430603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878723
Iteration 2/25 | Loss: 0.00155393
Iteration 3/25 | Loss: 0.00123554
Iteration 4/25 | Loss: 0.00121642
Iteration 5/25 | Loss: 0.00120961
Iteration 6/25 | Loss: 0.00120766
Iteration 7/25 | Loss: 0.00120744
Iteration 8/25 | Loss: 0.00120744
Iteration 9/25 | Loss: 0.00120744
Iteration 10/25 | Loss: 0.00120744
Iteration 11/25 | Loss: 0.00120744
Iteration 12/25 | Loss: 0.00120744
Iteration 13/25 | Loss: 0.00120744
Iteration 14/25 | Loss: 0.00120744
Iteration 15/25 | Loss: 0.00120744
Iteration 16/25 | Loss: 0.00120744
Iteration 17/25 | Loss: 0.00120744
Iteration 18/25 | Loss: 0.00120744
Iteration 19/25 | Loss: 0.00120744
Iteration 20/25 | Loss: 0.00120744
Iteration 21/25 | Loss: 0.00120744
Iteration 22/25 | Loss: 0.00120744
Iteration 23/25 | Loss: 0.00120744
Iteration 24/25 | Loss: 0.00120744
Iteration 25/25 | Loss: 0.00120744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87008107
Iteration 2/25 | Loss: 0.00100321
Iteration 3/25 | Loss: 0.00100321
Iteration 4/25 | Loss: 0.00100320
Iteration 5/25 | Loss: 0.00100320
Iteration 6/25 | Loss: 0.00100320
Iteration 7/25 | Loss: 0.00100320
Iteration 8/25 | Loss: 0.00100320
Iteration 9/25 | Loss: 0.00100320
Iteration 10/25 | Loss: 0.00100320
Iteration 11/25 | Loss: 0.00100320
Iteration 12/25 | Loss: 0.00100320
Iteration 13/25 | Loss: 0.00100320
Iteration 14/25 | Loss: 0.00100320
Iteration 15/25 | Loss: 0.00100320
Iteration 16/25 | Loss: 0.00100320
Iteration 17/25 | Loss: 0.00100320
Iteration 18/25 | Loss: 0.00100320
Iteration 19/25 | Loss: 0.00100320
Iteration 20/25 | Loss: 0.00100320
Iteration 21/25 | Loss: 0.00100320
Iteration 22/25 | Loss: 0.00100320
Iteration 23/25 | Loss: 0.00100320
Iteration 24/25 | Loss: 0.00100320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010032020509243011, 0.0010032020509243011, 0.0010032020509243011, 0.0010032020509243011, 0.0010032020509243011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010032020509243011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100320
Iteration 2/1000 | Loss: 0.00009049
Iteration 3/1000 | Loss: 0.00004922
Iteration 4/1000 | Loss: 0.00003506
Iteration 5/1000 | Loss: 0.00003174
Iteration 6/1000 | Loss: 0.00003017
Iteration 7/1000 | Loss: 0.00002905
Iteration 8/1000 | Loss: 0.00002822
Iteration 9/1000 | Loss: 0.00002753
Iteration 10/1000 | Loss: 0.00002704
Iteration 11/1000 | Loss: 0.00002662
Iteration 12/1000 | Loss: 0.00002626
Iteration 13/1000 | Loss: 0.00002595
Iteration 14/1000 | Loss: 0.00002571
Iteration 15/1000 | Loss: 0.00002552
Iteration 16/1000 | Loss: 0.00002536
Iteration 17/1000 | Loss: 0.00002532
Iteration 18/1000 | Loss: 0.00002518
Iteration 19/1000 | Loss: 0.00002507
Iteration 20/1000 | Loss: 0.00002502
Iteration 21/1000 | Loss: 0.00002501
Iteration 22/1000 | Loss: 0.00002501
Iteration 23/1000 | Loss: 0.00002495
Iteration 24/1000 | Loss: 0.00002490
Iteration 25/1000 | Loss: 0.00002490
Iteration 26/1000 | Loss: 0.00002489
Iteration 27/1000 | Loss: 0.00002485
Iteration 28/1000 | Loss: 0.00002482
Iteration 29/1000 | Loss: 0.00002481
Iteration 30/1000 | Loss: 0.00002481
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002479
Iteration 34/1000 | Loss: 0.00002479
Iteration 35/1000 | Loss: 0.00002479
Iteration 36/1000 | Loss: 0.00002479
Iteration 37/1000 | Loss: 0.00002479
Iteration 38/1000 | Loss: 0.00002479
Iteration 39/1000 | Loss: 0.00002479
Iteration 40/1000 | Loss: 0.00002479
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002478
Iteration 45/1000 | Loss: 0.00002478
Iteration 46/1000 | Loss: 0.00002478
Iteration 47/1000 | Loss: 0.00002478
Iteration 48/1000 | Loss: 0.00002478
Iteration 49/1000 | Loss: 0.00002477
Iteration 50/1000 | Loss: 0.00002476
Iteration 51/1000 | Loss: 0.00002476
Iteration 52/1000 | Loss: 0.00002476
Iteration 53/1000 | Loss: 0.00002476
Iteration 54/1000 | Loss: 0.00002476
Iteration 55/1000 | Loss: 0.00002475
Iteration 56/1000 | Loss: 0.00002475
Iteration 57/1000 | Loss: 0.00002475
Iteration 58/1000 | Loss: 0.00002475
Iteration 59/1000 | Loss: 0.00002475
Iteration 60/1000 | Loss: 0.00002475
Iteration 61/1000 | Loss: 0.00002475
Iteration 62/1000 | Loss: 0.00002475
Iteration 63/1000 | Loss: 0.00002475
Iteration 64/1000 | Loss: 0.00002474
Iteration 65/1000 | Loss: 0.00002474
Iteration 66/1000 | Loss: 0.00002474
Iteration 67/1000 | Loss: 0.00002474
Iteration 68/1000 | Loss: 0.00002474
Iteration 69/1000 | Loss: 0.00002474
Iteration 70/1000 | Loss: 0.00002474
Iteration 71/1000 | Loss: 0.00002474
Iteration 72/1000 | Loss: 0.00002474
Iteration 73/1000 | Loss: 0.00002473
Iteration 74/1000 | Loss: 0.00002473
Iteration 75/1000 | Loss: 0.00002473
Iteration 76/1000 | Loss: 0.00002473
Iteration 77/1000 | Loss: 0.00002473
Iteration 78/1000 | Loss: 0.00002473
Iteration 79/1000 | Loss: 0.00002472
Iteration 80/1000 | Loss: 0.00002472
Iteration 81/1000 | Loss: 0.00002472
Iteration 82/1000 | Loss: 0.00002472
Iteration 83/1000 | Loss: 0.00002471
Iteration 84/1000 | Loss: 0.00002471
Iteration 85/1000 | Loss: 0.00002471
Iteration 86/1000 | Loss: 0.00002471
Iteration 87/1000 | Loss: 0.00002471
Iteration 88/1000 | Loss: 0.00002471
Iteration 89/1000 | Loss: 0.00002470
Iteration 90/1000 | Loss: 0.00002470
Iteration 91/1000 | Loss: 0.00002470
Iteration 92/1000 | Loss: 0.00002470
Iteration 93/1000 | Loss: 0.00002470
Iteration 94/1000 | Loss: 0.00002469
Iteration 95/1000 | Loss: 0.00002469
Iteration 96/1000 | Loss: 0.00002468
Iteration 97/1000 | Loss: 0.00002468
Iteration 98/1000 | Loss: 0.00002467
Iteration 99/1000 | Loss: 0.00002467
Iteration 100/1000 | Loss: 0.00002466
Iteration 101/1000 | Loss: 0.00002466
Iteration 102/1000 | Loss: 0.00002465
Iteration 103/1000 | Loss: 0.00002465
Iteration 104/1000 | Loss: 0.00002465
Iteration 105/1000 | Loss: 0.00002465
Iteration 106/1000 | Loss: 0.00002464
Iteration 107/1000 | Loss: 0.00002464
Iteration 108/1000 | Loss: 0.00002464
Iteration 109/1000 | Loss: 0.00002463
Iteration 110/1000 | Loss: 0.00002463
Iteration 111/1000 | Loss: 0.00002463
Iteration 112/1000 | Loss: 0.00002462
Iteration 113/1000 | Loss: 0.00002462
Iteration 114/1000 | Loss: 0.00002462
Iteration 115/1000 | Loss: 0.00002461
Iteration 116/1000 | Loss: 0.00002461
Iteration 117/1000 | Loss: 0.00002461
Iteration 118/1000 | Loss: 0.00002460
Iteration 119/1000 | Loss: 0.00002460
Iteration 120/1000 | Loss: 0.00002460
Iteration 121/1000 | Loss: 0.00002460
Iteration 122/1000 | Loss: 0.00002460
Iteration 123/1000 | Loss: 0.00002459
Iteration 124/1000 | Loss: 0.00002459
Iteration 125/1000 | Loss: 0.00002459
Iteration 126/1000 | Loss: 0.00002459
Iteration 127/1000 | Loss: 0.00002459
Iteration 128/1000 | Loss: 0.00002458
Iteration 129/1000 | Loss: 0.00002458
Iteration 130/1000 | Loss: 0.00002458
Iteration 131/1000 | Loss: 0.00002458
Iteration 132/1000 | Loss: 0.00002458
Iteration 133/1000 | Loss: 0.00002458
Iteration 134/1000 | Loss: 0.00002458
Iteration 135/1000 | Loss: 0.00002458
Iteration 136/1000 | Loss: 0.00002458
Iteration 137/1000 | Loss: 0.00002458
Iteration 138/1000 | Loss: 0.00002458
Iteration 139/1000 | Loss: 0.00002458
Iteration 140/1000 | Loss: 0.00002457
Iteration 141/1000 | Loss: 0.00002457
Iteration 142/1000 | Loss: 0.00002457
Iteration 143/1000 | Loss: 0.00002457
Iteration 144/1000 | Loss: 0.00002457
Iteration 145/1000 | Loss: 0.00002457
Iteration 146/1000 | Loss: 0.00002457
Iteration 147/1000 | Loss: 0.00002457
Iteration 148/1000 | Loss: 0.00002457
Iteration 149/1000 | Loss: 0.00002457
Iteration 150/1000 | Loss: 0.00002457
Iteration 151/1000 | Loss: 0.00002457
Iteration 152/1000 | Loss: 0.00002457
Iteration 153/1000 | Loss: 0.00002457
Iteration 154/1000 | Loss: 0.00002457
Iteration 155/1000 | Loss: 0.00002457
Iteration 156/1000 | Loss: 0.00002456
Iteration 157/1000 | Loss: 0.00002456
Iteration 158/1000 | Loss: 0.00002456
Iteration 159/1000 | Loss: 0.00002456
Iteration 160/1000 | Loss: 0.00002456
Iteration 161/1000 | Loss: 0.00002456
Iteration 162/1000 | Loss: 0.00002456
Iteration 163/1000 | Loss: 0.00002456
Iteration 164/1000 | Loss: 0.00002456
Iteration 165/1000 | Loss: 0.00002456
Iteration 166/1000 | Loss: 0.00002456
Iteration 167/1000 | Loss: 0.00002455
Iteration 168/1000 | Loss: 0.00002455
Iteration 169/1000 | Loss: 0.00002455
Iteration 170/1000 | Loss: 0.00002455
Iteration 171/1000 | Loss: 0.00002455
Iteration 172/1000 | Loss: 0.00002455
Iteration 173/1000 | Loss: 0.00002455
Iteration 174/1000 | Loss: 0.00002455
Iteration 175/1000 | Loss: 0.00002455
Iteration 176/1000 | Loss: 0.00002454
Iteration 177/1000 | Loss: 0.00002454
Iteration 178/1000 | Loss: 0.00002454
Iteration 179/1000 | Loss: 0.00002454
Iteration 180/1000 | Loss: 0.00002454
Iteration 181/1000 | Loss: 0.00002454
Iteration 182/1000 | Loss: 0.00002454
Iteration 183/1000 | Loss: 0.00002454
Iteration 184/1000 | Loss: 0.00002454
Iteration 185/1000 | Loss: 0.00002454
Iteration 186/1000 | Loss: 0.00002454
Iteration 187/1000 | Loss: 0.00002454
Iteration 188/1000 | Loss: 0.00002454
Iteration 189/1000 | Loss: 0.00002453
Iteration 190/1000 | Loss: 0.00002453
Iteration 191/1000 | Loss: 0.00002453
Iteration 192/1000 | Loss: 0.00002453
Iteration 193/1000 | Loss: 0.00002453
Iteration 194/1000 | Loss: 0.00002453
Iteration 195/1000 | Loss: 0.00002453
Iteration 196/1000 | Loss: 0.00002453
Iteration 197/1000 | Loss: 0.00002453
Iteration 198/1000 | Loss: 0.00002453
Iteration 199/1000 | Loss: 0.00002453
Iteration 200/1000 | Loss: 0.00002453
Iteration 201/1000 | Loss: 0.00002452
Iteration 202/1000 | Loss: 0.00002452
Iteration 203/1000 | Loss: 0.00002452
Iteration 204/1000 | Loss: 0.00002452
Iteration 205/1000 | Loss: 0.00002452
Iteration 206/1000 | Loss: 0.00002452
Iteration 207/1000 | Loss: 0.00002452
Iteration 208/1000 | Loss: 0.00002452
Iteration 209/1000 | Loss: 0.00002452
Iteration 210/1000 | Loss: 0.00002452
Iteration 211/1000 | Loss: 0.00002452
Iteration 212/1000 | Loss: 0.00002452
Iteration 213/1000 | Loss: 0.00002452
Iteration 214/1000 | Loss: 0.00002452
Iteration 215/1000 | Loss: 0.00002452
Iteration 216/1000 | Loss: 0.00002452
Iteration 217/1000 | Loss: 0.00002452
Iteration 218/1000 | Loss: 0.00002451
Iteration 219/1000 | Loss: 0.00002451
Iteration 220/1000 | Loss: 0.00002451
Iteration 221/1000 | Loss: 0.00002451
Iteration 222/1000 | Loss: 0.00002451
Iteration 223/1000 | Loss: 0.00002451
Iteration 224/1000 | Loss: 0.00002451
Iteration 225/1000 | Loss: 0.00002451
Iteration 226/1000 | Loss: 0.00002451
Iteration 227/1000 | Loss: 0.00002451
Iteration 228/1000 | Loss: 0.00002451
Iteration 229/1000 | Loss: 0.00002451
Iteration 230/1000 | Loss: 0.00002451
Iteration 231/1000 | Loss: 0.00002451
Iteration 232/1000 | Loss: 0.00002451
Iteration 233/1000 | Loss: 0.00002451
Iteration 234/1000 | Loss: 0.00002451
Iteration 235/1000 | Loss: 0.00002451
Iteration 236/1000 | Loss: 0.00002451
Iteration 237/1000 | Loss: 0.00002451
Iteration 238/1000 | Loss: 0.00002451
Iteration 239/1000 | Loss: 0.00002451
Iteration 240/1000 | Loss: 0.00002451
Iteration 241/1000 | Loss: 0.00002451
Iteration 242/1000 | Loss: 0.00002451
Iteration 243/1000 | Loss: 0.00002451
Iteration 244/1000 | Loss: 0.00002451
Iteration 245/1000 | Loss: 0.00002451
Iteration 246/1000 | Loss: 0.00002451
Iteration 247/1000 | Loss: 0.00002451
Iteration 248/1000 | Loss: 0.00002451
Iteration 249/1000 | Loss: 0.00002451
Iteration 250/1000 | Loss: 0.00002451
Iteration 251/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [2.4509921786375344e-05, 2.4509921786375344e-05, 2.4509921786375344e-05, 2.4509921786375344e-05, 2.4509921786375344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4509921786375344e-05

Optimization complete. Final v2v error: 4.049289703369141 mm

Highest mean error: 4.966714859008789 mm for frame 26

Lowest mean error: 2.9864470958709717 mm for frame 128

Saving results

Total time: 52.35589599609375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760049
Iteration 2/25 | Loss: 0.00149136
Iteration 3/25 | Loss: 0.00126156
Iteration 4/25 | Loss: 0.00120206
Iteration 5/25 | Loss: 0.00118763
Iteration 6/25 | Loss: 0.00118389
Iteration 7/25 | Loss: 0.00118355
Iteration 8/25 | Loss: 0.00118355
Iteration 9/25 | Loss: 0.00118355
Iteration 10/25 | Loss: 0.00118355
Iteration 11/25 | Loss: 0.00118352
Iteration 12/25 | Loss: 0.00118352
Iteration 13/25 | Loss: 0.00118352
Iteration 14/25 | Loss: 0.00118352
Iteration 15/25 | Loss: 0.00118352
Iteration 16/25 | Loss: 0.00118352
Iteration 17/25 | Loss: 0.00118352
Iteration 18/25 | Loss: 0.00118352
Iteration 19/25 | Loss: 0.00118352
Iteration 20/25 | Loss: 0.00118352
Iteration 21/25 | Loss: 0.00118352
Iteration 22/25 | Loss: 0.00118352
Iteration 23/25 | Loss: 0.00118352
Iteration 24/25 | Loss: 0.00118352
Iteration 25/25 | Loss: 0.00118352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20988905
Iteration 2/25 | Loss: 0.00113473
Iteration 3/25 | Loss: 0.00113473
Iteration 4/25 | Loss: 0.00113473
Iteration 5/25 | Loss: 0.00113473
Iteration 6/25 | Loss: 0.00113473
Iteration 7/25 | Loss: 0.00113473
Iteration 8/25 | Loss: 0.00113473
Iteration 9/25 | Loss: 0.00113473
Iteration 10/25 | Loss: 0.00113473
Iteration 11/25 | Loss: 0.00113473
Iteration 12/25 | Loss: 0.00113473
Iteration 13/25 | Loss: 0.00113473
Iteration 14/25 | Loss: 0.00113473
Iteration 15/25 | Loss: 0.00113473
Iteration 16/25 | Loss: 0.00113473
Iteration 17/25 | Loss: 0.00113473
Iteration 18/25 | Loss: 0.00113473
Iteration 19/25 | Loss: 0.00113473
Iteration 20/25 | Loss: 0.00113473
Iteration 21/25 | Loss: 0.00113473
Iteration 22/25 | Loss: 0.00113473
Iteration 23/25 | Loss: 0.00113473
Iteration 24/25 | Loss: 0.00113473
Iteration 25/25 | Loss: 0.00113473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113473
Iteration 2/1000 | Loss: 0.00008139
Iteration 3/1000 | Loss: 0.00005096
Iteration 4/1000 | Loss: 0.00004092
Iteration 5/1000 | Loss: 0.00003792
Iteration 6/1000 | Loss: 0.00003603
Iteration 7/1000 | Loss: 0.00003472
Iteration 8/1000 | Loss: 0.00003383
Iteration 9/1000 | Loss: 0.00003313
Iteration 10/1000 | Loss: 0.00003260
Iteration 11/1000 | Loss: 0.00003210
Iteration 12/1000 | Loss: 0.00003170
Iteration 13/1000 | Loss: 0.00003141
Iteration 14/1000 | Loss: 0.00003117
Iteration 15/1000 | Loss: 0.00003112
Iteration 16/1000 | Loss: 0.00003104
Iteration 17/1000 | Loss: 0.00003095
Iteration 18/1000 | Loss: 0.00003085
Iteration 19/1000 | Loss: 0.00003071
Iteration 20/1000 | Loss: 0.00003071
Iteration 21/1000 | Loss: 0.00003061
Iteration 22/1000 | Loss: 0.00003061
Iteration 23/1000 | Loss: 0.00003056
Iteration 24/1000 | Loss: 0.00003054
Iteration 25/1000 | Loss: 0.00003053
Iteration 26/1000 | Loss: 0.00003053
Iteration 27/1000 | Loss: 0.00003051
Iteration 28/1000 | Loss: 0.00003050
Iteration 29/1000 | Loss: 0.00003050
Iteration 30/1000 | Loss: 0.00003049
Iteration 31/1000 | Loss: 0.00003047
Iteration 32/1000 | Loss: 0.00003044
Iteration 33/1000 | Loss: 0.00003044
Iteration 34/1000 | Loss: 0.00003042
Iteration 35/1000 | Loss: 0.00003042
Iteration 36/1000 | Loss: 0.00003042
Iteration 37/1000 | Loss: 0.00003041
Iteration 38/1000 | Loss: 0.00003041
Iteration 39/1000 | Loss: 0.00003040
Iteration 40/1000 | Loss: 0.00003039
Iteration 41/1000 | Loss: 0.00003038
Iteration 42/1000 | Loss: 0.00003038
Iteration 43/1000 | Loss: 0.00003037
Iteration 44/1000 | Loss: 0.00003037
Iteration 45/1000 | Loss: 0.00003036
Iteration 46/1000 | Loss: 0.00003036
Iteration 47/1000 | Loss: 0.00003034
Iteration 48/1000 | Loss: 0.00003033
Iteration 49/1000 | Loss: 0.00003033
Iteration 50/1000 | Loss: 0.00003032
Iteration 51/1000 | Loss: 0.00003032
Iteration 52/1000 | Loss: 0.00003032
Iteration 53/1000 | Loss: 0.00003031
Iteration 54/1000 | Loss: 0.00003030
Iteration 55/1000 | Loss: 0.00003030
Iteration 56/1000 | Loss: 0.00003029
Iteration 57/1000 | Loss: 0.00003029
Iteration 58/1000 | Loss: 0.00003029
Iteration 59/1000 | Loss: 0.00003028
Iteration 60/1000 | Loss: 0.00003028
Iteration 61/1000 | Loss: 0.00003028
Iteration 62/1000 | Loss: 0.00003027
Iteration 63/1000 | Loss: 0.00003027
Iteration 64/1000 | Loss: 0.00003027
Iteration 65/1000 | Loss: 0.00003026
Iteration 66/1000 | Loss: 0.00003026
Iteration 67/1000 | Loss: 0.00003026
Iteration 68/1000 | Loss: 0.00003026
Iteration 69/1000 | Loss: 0.00003026
Iteration 70/1000 | Loss: 0.00003026
Iteration 71/1000 | Loss: 0.00003025
Iteration 72/1000 | Loss: 0.00003025
Iteration 73/1000 | Loss: 0.00003025
Iteration 74/1000 | Loss: 0.00003025
Iteration 75/1000 | Loss: 0.00003025
Iteration 76/1000 | Loss: 0.00003024
Iteration 77/1000 | Loss: 0.00003024
Iteration 78/1000 | Loss: 0.00003024
Iteration 79/1000 | Loss: 0.00003023
Iteration 80/1000 | Loss: 0.00003023
Iteration 81/1000 | Loss: 0.00003023
Iteration 82/1000 | Loss: 0.00003023
Iteration 83/1000 | Loss: 0.00003022
Iteration 84/1000 | Loss: 0.00003022
Iteration 85/1000 | Loss: 0.00003022
Iteration 86/1000 | Loss: 0.00003021
Iteration 87/1000 | Loss: 0.00003021
Iteration 88/1000 | Loss: 0.00003021
Iteration 89/1000 | Loss: 0.00003021
Iteration 90/1000 | Loss: 0.00003021
Iteration 91/1000 | Loss: 0.00003021
Iteration 92/1000 | Loss: 0.00003021
Iteration 93/1000 | Loss: 0.00003021
Iteration 94/1000 | Loss: 0.00003021
Iteration 95/1000 | Loss: 0.00003020
Iteration 96/1000 | Loss: 0.00003020
Iteration 97/1000 | Loss: 0.00003020
Iteration 98/1000 | Loss: 0.00003020
Iteration 99/1000 | Loss: 0.00003020
Iteration 100/1000 | Loss: 0.00003020
Iteration 101/1000 | Loss: 0.00003020
Iteration 102/1000 | Loss: 0.00003020
Iteration 103/1000 | Loss: 0.00003019
Iteration 104/1000 | Loss: 0.00003019
Iteration 105/1000 | Loss: 0.00003019
Iteration 106/1000 | Loss: 0.00003019
Iteration 107/1000 | Loss: 0.00003019
Iteration 108/1000 | Loss: 0.00003019
Iteration 109/1000 | Loss: 0.00003019
Iteration 110/1000 | Loss: 0.00003018
Iteration 111/1000 | Loss: 0.00003018
Iteration 112/1000 | Loss: 0.00003018
Iteration 113/1000 | Loss: 0.00003018
Iteration 114/1000 | Loss: 0.00003018
Iteration 115/1000 | Loss: 0.00003018
Iteration 116/1000 | Loss: 0.00003017
Iteration 117/1000 | Loss: 0.00003017
Iteration 118/1000 | Loss: 0.00003017
Iteration 119/1000 | Loss: 0.00003017
Iteration 120/1000 | Loss: 0.00003017
Iteration 121/1000 | Loss: 0.00003016
Iteration 122/1000 | Loss: 0.00003016
Iteration 123/1000 | Loss: 0.00003016
Iteration 124/1000 | Loss: 0.00003016
Iteration 125/1000 | Loss: 0.00003016
Iteration 126/1000 | Loss: 0.00003016
Iteration 127/1000 | Loss: 0.00003016
Iteration 128/1000 | Loss: 0.00003016
Iteration 129/1000 | Loss: 0.00003016
Iteration 130/1000 | Loss: 0.00003015
Iteration 131/1000 | Loss: 0.00003015
Iteration 132/1000 | Loss: 0.00003015
Iteration 133/1000 | Loss: 0.00003015
Iteration 134/1000 | Loss: 0.00003015
Iteration 135/1000 | Loss: 0.00003014
Iteration 136/1000 | Loss: 0.00003014
Iteration 137/1000 | Loss: 0.00003014
Iteration 138/1000 | Loss: 0.00003014
Iteration 139/1000 | Loss: 0.00003014
Iteration 140/1000 | Loss: 0.00003014
Iteration 141/1000 | Loss: 0.00003014
Iteration 142/1000 | Loss: 0.00003014
Iteration 143/1000 | Loss: 0.00003014
Iteration 144/1000 | Loss: 0.00003014
Iteration 145/1000 | Loss: 0.00003013
Iteration 146/1000 | Loss: 0.00003013
Iteration 147/1000 | Loss: 0.00003013
Iteration 148/1000 | Loss: 0.00003013
Iteration 149/1000 | Loss: 0.00003013
Iteration 150/1000 | Loss: 0.00003013
Iteration 151/1000 | Loss: 0.00003013
Iteration 152/1000 | Loss: 0.00003013
Iteration 153/1000 | Loss: 0.00003013
Iteration 154/1000 | Loss: 0.00003013
Iteration 155/1000 | Loss: 0.00003013
Iteration 156/1000 | Loss: 0.00003012
Iteration 157/1000 | Loss: 0.00003012
Iteration 158/1000 | Loss: 0.00003012
Iteration 159/1000 | Loss: 0.00003012
Iteration 160/1000 | Loss: 0.00003012
Iteration 161/1000 | Loss: 0.00003012
Iteration 162/1000 | Loss: 0.00003012
Iteration 163/1000 | Loss: 0.00003012
Iteration 164/1000 | Loss: 0.00003012
Iteration 165/1000 | Loss: 0.00003012
Iteration 166/1000 | Loss: 0.00003012
Iteration 167/1000 | Loss: 0.00003012
Iteration 168/1000 | Loss: 0.00003012
Iteration 169/1000 | Loss: 0.00003011
Iteration 170/1000 | Loss: 0.00003011
Iteration 171/1000 | Loss: 0.00003011
Iteration 172/1000 | Loss: 0.00003011
Iteration 173/1000 | Loss: 0.00003011
Iteration 174/1000 | Loss: 0.00003011
Iteration 175/1000 | Loss: 0.00003011
Iteration 176/1000 | Loss: 0.00003011
Iteration 177/1000 | Loss: 0.00003011
Iteration 178/1000 | Loss: 0.00003011
Iteration 179/1000 | Loss: 0.00003011
Iteration 180/1000 | Loss: 0.00003011
Iteration 181/1000 | Loss: 0.00003011
Iteration 182/1000 | Loss: 0.00003011
Iteration 183/1000 | Loss: 0.00003011
Iteration 184/1000 | Loss: 0.00003011
Iteration 185/1000 | Loss: 0.00003011
Iteration 186/1000 | Loss: 0.00003011
Iteration 187/1000 | Loss: 0.00003011
Iteration 188/1000 | Loss: 0.00003011
Iteration 189/1000 | Loss: 0.00003011
Iteration 190/1000 | Loss: 0.00003011
Iteration 191/1000 | Loss: 0.00003011
Iteration 192/1000 | Loss: 0.00003011
Iteration 193/1000 | Loss: 0.00003011
Iteration 194/1000 | Loss: 0.00003011
Iteration 195/1000 | Loss: 0.00003011
Iteration 196/1000 | Loss: 0.00003011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [3.010536056535784e-05, 3.010536056535784e-05, 3.010536056535784e-05, 3.010536056535784e-05, 3.010536056535784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.010536056535784e-05

Optimization complete. Final v2v error: 4.451368808746338 mm

Highest mean error: 5.971644878387451 mm for frame 53

Lowest mean error: 3.344158172607422 mm for frame 193

Saving results

Total time: 55.27893328666687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052257
Iteration 2/25 | Loss: 0.01052257
Iteration 3/25 | Loss: 0.01052257
Iteration 4/25 | Loss: 0.01052257
Iteration 5/25 | Loss: 0.01052256
Iteration 6/25 | Loss: 0.01052256
Iteration 7/25 | Loss: 0.01052256
Iteration 8/25 | Loss: 0.00364933
Iteration 9/25 | Loss: 0.00191653
Iteration 10/25 | Loss: 0.00177550
Iteration 11/25 | Loss: 0.00171555
Iteration 12/25 | Loss: 0.00155989
Iteration 13/25 | Loss: 0.00151870
Iteration 14/25 | Loss: 0.00152691
Iteration 15/25 | Loss: 0.00145646
Iteration 16/25 | Loss: 0.00144594
Iteration 17/25 | Loss: 0.00138349
Iteration 18/25 | Loss: 0.00135556
Iteration 19/25 | Loss: 0.00133759
Iteration 20/25 | Loss: 0.00133017
Iteration 21/25 | Loss: 0.00133243
Iteration 22/25 | Loss: 0.00133692
Iteration 23/25 | Loss: 0.00131680
Iteration 24/25 | Loss: 0.00131285
Iteration 25/25 | Loss: 0.00131682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34890604
Iteration 2/25 | Loss: 0.00291930
Iteration 3/25 | Loss: 0.00291930
Iteration 4/25 | Loss: 0.00291930
Iteration 5/25 | Loss: 0.00291930
Iteration 6/25 | Loss: 0.00291930
Iteration 7/25 | Loss: 0.00291930
Iteration 8/25 | Loss: 0.00291930
Iteration 9/25 | Loss: 0.00291930
Iteration 10/25 | Loss: 0.00291930
Iteration 11/25 | Loss: 0.00291930
Iteration 12/25 | Loss: 0.00291930
Iteration 13/25 | Loss: 0.00291930
Iteration 14/25 | Loss: 0.00291930
Iteration 15/25 | Loss: 0.00291930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002919297432526946, 0.002919297432526946, 0.002919297432526946, 0.002919297432526946, 0.002919297432526946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002919297432526946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00291930
Iteration 2/1000 | Loss: 0.00242903
Iteration 3/1000 | Loss: 0.00282025
Iteration 4/1000 | Loss: 0.00096632
Iteration 5/1000 | Loss: 0.00232410
Iteration 6/1000 | Loss: 0.00187718
Iteration 7/1000 | Loss: 0.00166126
Iteration 8/1000 | Loss: 0.00171051
Iteration 9/1000 | Loss: 0.00293430
Iteration 10/1000 | Loss: 0.00181428
Iteration 11/1000 | Loss: 0.00218977
Iteration 12/1000 | Loss: 0.00196701
Iteration 13/1000 | Loss: 0.00353388
Iteration 14/1000 | Loss: 0.00340158
Iteration 15/1000 | Loss: 0.00500545
Iteration 16/1000 | Loss: 0.00052521
Iteration 17/1000 | Loss: 0.00063641
Iteration 18/1000 | Loss: 0.00121472
Iteration 19/1000 | Loss: 0.00041605
Iteration 20/1000 | Loss: 0.00036000
Iteration 21/1000 | Loss: 0.00065438
Iteration 22/1000 | Loss: 0.00031015
Iteration 23/1000 | Loss: 0.00041137
Iteration 24/1000 | Loss: 0.00015656
Iteration 25/1000 | Loss: 0.00075249
Iteration 26/1000 | Loss: 0.00200357
Iteration 27/1000 | Loss: 0.00308342
Iteration 28/1000 | Loss: 0.00323436
Iteration 29/1000 | Loss: 0.00542112
Iteration 30/1000 | Loss: 0.00606933
Iteration 31/1000 | Loss: 0.00541457
Iteration 32/1000 | Loss: 0.00933223
Iteration 33/1000 | Loss: 0.00357005
Iteration 34/1000 | Loss: 0.00366418
Iteration 35/1000 | Loss: 0.00163855
Iteration 36/1000 | Loss: 0.00050693
Iteration 37/1000 | Loss: 0.00075245
Iteration 38/1000 | Loss: 0.00029721
Iteration 39/1000 | Loss: 0.00060755
Iteration 40/1000 | Loss: 0.00026607
Iteration 41/1000 | Loss: 0.00071234
Iteration 42/1000 | Loss: 0.00081901
Iteration 43/1000 | Loss: 0.00035490
Iteration 44/1000 | Loss: 0.00035530
Iteration 45/1000 | Loss: 0.00008790
Iteration 46/1000 | Loss: 0.00010349
Iteration 47/1000 | Loss: 0.00009461
Iteration 48/1000 | Loss: 0.00018797
Iteration 49/1000 | Loss: 0.00014404
Iteration 50/1000 | Loss: 0.00018147
Iteration 51/1000 | Loss: 0.00010078
Iteration 52/1000 | Loss: 0.00049985
Iteration 53/1000 | Loss: 0.00016404
Iteration 54/1000 | Loss: 0.00020539
Iteration 55/1000 | Loss: 0.00036149
Iteration 56/1000 | Loss: 0.00064008
Iteration 57/1000 | Loss: 0.00031988
Iteration 58/1000 | Loss: 0.00054843
Iteration 59/1000 | Loss: 0.00069507
Iteration 60/1000 | Loss: 0.00030379
Iteration 61/1000 | Loss: 0.00027853
Iteration 62/1000 | Loss: 0.00023749
Iteration 63/1000 | Loss: 0.00036878
Iteration 64/1000 | Loss: 0.00029901
Iteration 65/1000 | Loss: 0.00031248
Iteration 66/1000 | Loss: 0.00008967
Iteration 67/1000 | Loss: 0.00033432
Iteration 68/1000 | Loss: 0.00030908
Iteration 69/1000 | Loss: 0.00019871
Iteration 70/1000 | Loss: 0.00006437
Iteration 71/1000 | Loss: 0.00011211
Iteration 72/1000 | Loss: 0.00007375
Iteration 73/1000 | Loss: 0.00036284
Iteration 74/1000 | Loss: 0.00090495
Iteration 75/1000 | Loss: 0.00067336
Iteration 76/1000 | Loss: 0.00063924
Iteration 77/1000 | Loss: 0.00009574
Iteration 78/1000 | Loss: 0.00031425
Iteration 79/1000 | Loss: 0.00018703
Iteration 80/1000 | Loss: 0.00083834
Iteration 81/1000 | Loss: 0.00032594
Iteration 82/1000 | Loss: 0.00009490
Iteration 83/1000 | Loss: 0.00008580
Iteration 84/1000 | Loss: 0.00006287
Iteration 85/1000 | Loss: 0.00015122
Iteration 86/1000 | Loss: 0.00028684
Iteration 87/1000 | Loss: 0.00006526
Iteration 88/1000 | Loss: 0.00022106
Iteration 89/1000 | Loss: 0.00009724
Iteration 90/1000 | Loss: 0.00006105
Iteration 91/1000 | Loss: 0.00006833
Iteration 92/1000 | Loss: 0.00007307
Iteration 93/1000 | Loss: 0.00010399
Iteration 94/1000 | Loss: 0.00017360
Iteration 95/1000 | Loss: 0.00007583
Iteration 96/1000 | Loss: 0.00006995
Iteration 97/1000 | Loss: 0.00009304
Iteration 98/1000 | Loss: 0.00007405
Iteration 99/1000 | Loss: 0.00007053
Iteration 100/1000 | Loss: 0.00009876
Iteration 101/1000 | Loss: 0.00004866
Iteration 102/1000 | Loss: 0.00006207
Iteration 103/1000 | Loss: 0.00006006
Iteration 104/1000 | Loss: 0.00006018
Iteration 105/1000 | Loss: 0.00006626
Iteration 106/1000 | Loss: 0.00006746
Iteration 107/1000 | Loss: 0.00006598
Iteration 108/1000 | Loss: 0.00009946
Iteration 109/1000 | Loss: 0.00014282
Iteration 110/1000 | Loss: 0.00008107
Iteration 111/1000 | Loss: 0.00006937
Iteration 112/1000 | Loss: 0.00005956
Iteration 113/1000 | Loss: 0.00006047
Iteration 114/1000 | Loss: 0.00007532
Iteration 115/1000 | Loss: 0.00005885
Iteration 116/1000 | Loss: 0.00006218
Iteration 117/1000 | Loss: 0.00006693
Iteration 118/1000 | Loss: 0.00012181
Iteration 119/1000 | Loss: 0.00007030
Iteration 120/1000 | Loss: 0.00008548
Iteration 121/1000 | Loss: 0.00012377
Iteration 122/1000 | Loss: 0.00006795
Iteration 123/1000 | Loss: 0.00009859
Iteration 124/1000 | Loss: 0.00007195
Iteration 125/1000 | Loss: 0.00007454
Iteration 126/1000 | Loss: 0.00007012
Iteration 127/1000 | Loss: 0.00006872
Iteration 128/1000 | Loss: 0.00004574
Iteration 129/1000 | Loss: 0.00004833
Iteration 130/1000 | Loss: 0.00008015
Iteration 131/1000 | Loss: 0.00007560
Iteration 132/1000 | Loss: 0.00009603
Iteration 133/1000 | Loss: 0.00020345
Iteration 134/1000 | Loss: 0.00005388
Iteration 135/1000 | Loss: 0.00010620
Iteration 136/1000 | Loss: 0.00006396
Iteration 137/1000 | Loss: 0.00006988
Iteration 138/1000 | Loss: 0.00011199
Iteration 139/1000 | Loss: 0.00015383
Iteration 140/1000 | Loss: 0.00007839
Iteration 141/1000 | Loss: 0.00007895
Iteration 142/1000 | Loss: 0.00007094
Iteration 143/1000 | Loss: 0.00010318
Iteration 144/1000 | Loss: 0.00008544
Iteration 145/1000 | Loss: 0.00012414
Iteration 146/1000 | Loss: 0.00020750
Iteration 147/1000 | Loss: 0.00006444
Iteration 148/1000 | Loss: 0.00083351
Iteration 149/1000 | Loss: 0.00023184
Iteration 150/1000 | Loss: 0.00007311
Iteration 151/1000 | Loss: 0.00006980
Iteration 152/1000 | Loss: 0.00004160
Iteration 153/1000 | Loss: 0.00005934
Iteration 154/1000 | Loss: 0.00004706
Iteration 155/1000 | Loss: 0.00008034
Iteration 156/1000 | Loss: 0.00007098
Iteration 157/1000 | Loss: 0.00006423
Iteration 158/1000 | Loss: 0.00007059
Iteration 159/1000 | Loss: 0.00011440
Iteration 160/1000 | Loss: 0.00007294
Iteration 161/1000 | Loss: 0.00026410
Iteration 162/1000 | Loss: 0.00009257
Iteration 163/1000 | Loss: 0.00013939
Iteration 164/1000 | Loss: 0.00008239
Iteration 165/1000 | Loss: 0.00006249
Iteration 166/1000 | Loss: 0.00008323
Iteration 167/1000 | Loss: 0.00008467
Iteration 168/1000 | Loss: 0.00008519
Iteration 169/1000 | Loss: 0.00007280
Iteration 170/1000 | Loss: 0.00009259
Iteration 171/1000 | Loss: 0.00010311
Iteration 172/1000 | Loss: 0.00009345
Iteration 173/1000 | Loss: 0.00009814
Iteration 174/1000 | Loss: 0.00009137
Iteration 175/1000 | Loss: 0.00008041
Iteration 176/1000 | Loss: 0.00007320
Iteration 177/1000 | Loss: 0.00014618
Iteration 178/1000 | Loss: 0.00006996
Iteration 179/1000 | Loss: 0.00011670
Iteration 180/1000 | Loss: 0.00007470
Iteration 181/1000 | Loss: 0.00012913
Iteration 182/1000 | Loss: 0.00008102
Iteration 183/1000 | Loss: 0.00007580
Iteration 184/1000 | Loss: 0.00017073
Iteration 185/1000 | Loss: 0.00009495
Iteration 186/1000 | Loss: 0.00005738
Iteration 187/1000 | Loss: 0.00009619
Iteration 188/1000 | Loss: 0.00007344
Iteration 189/1000 | Loss: 0.00007456
Iteration 190/1000 | Loss: 0.00006952
Iteration 191/1000 | Loss: 0.00007042
Iteration 192/1000 | Loss: 0.00007156
Iteration 193/1000 | Loss: 0.00005169
Iteration 194/1000 | Loss: 0.00007727
Iteration 195/1000 | Loss: 0.00005828
Iteration 196/1000 | Loss: 0.00006821
Iteration 197/1000 | Loss: 0.00006932
Iteration 198/1000 | Loss: 0.00006532
Iteration 199/1000 | Loss: 0.00007743
Iteration 200/1000 | Loss: 0.00006901
Iteration 201/1000 | Loss: 0.00008766
Iteration 202/1000 | Loss: 0.00017405
Iteration 203/1000 | Loss: 0.00006789
Iteration 204/1000 | Loss: 0.00054498
Iteration 205/1000 | Loss: 0.00024869
Iteration 206/1000 | Loss: 0.00010746
Iteration 207/1000 | Loss: 0.00006637
Iteration 208/1000 | Loss: 0.00005730
Iteration 209/1000 | Loss: 0.00007556
Iteration 210/1000 | Loss: 0.00007959
Iteration 211/1000 | Loss: 0.00010290
Iteration 212/1000 | Loss: 0.00007297
Iteration 213/1000 | Loss: 0.00007873
Iteration 214/1000 | Loss: 0.00007459
Iteration 215/1000 | Loss: 0.00007743
Iteration 216/1000 | Loss: 0.00007233
Iteration 217/1000 | Loss: 0.00007982
Iteration 218/1000 | Loss: 0.00055018
Iteration 219/1000 | Loss: 0.00024917
Iteration 220/1000 | Loss: 0.00007041
Iteration 221/1000 | Loss: 0.00007403
Iteration 222/1000 | Loss: 0.00006947
Iteration 223/1000 | Loss: 0.00006842
Iteration 224/1000 | Loss: 0.00006766
Iteration 225/1000 | Loss: 0.00006961
Iteration 226/1000 | Loss: 0.00007219
Iteration 227/1000 | Loss: 0.00006819
Iteration 228/1000 | Loss: 0.00006968
Iteration 229/1000 | Loss: 0.00006629
Iteration 230/1000 | Loss: 0.00006628
Iteration 231/1000 | Loss: 0.00054057
Iteration 232/1000 | Loss: 0.00017809
Iteration 233/1000 | Loss: 0.00008663
Iteration 234/1000 | Loss: 0.00009131
Iteration 235/1000 | Loss: 0.00019931
Iteration 236/1000 | Loss: 0.00003537
Iteration 237/1000 | Loss: 0.00009984
Iteration 238/1000 | Loss: 0.00004014
Iteration 239/1000 | Loss: 0.00010642
Iteration 240/1000 | Loss: 0.00002775
Iteration 241/1000 | Loss: 0.00054270
Iteration 242/1000 | Loss: 0.00019894
Iteration 243/1000 | Loss: 0.00002422
Iteration 244/1000 | Loss: 0.00002694
Iteration 245/1000 | Loss: 0.00002246
Iteration 246/1000 | Loss: 0.00002029
Iteration 247/1000 | Loss: 0.00001965
Iteration 248/1000 | Loss: 0.00044394
Iteration 249/1000 | Loss: 0.00014774
Iteration 250/1000 | Loss: 0.00002795
Iteration 251/1000 | Loss: 0.00001926
Iteration 252/1000 | Loss: 0.00003193
Iteration 253/1000 | Loss: 0.00001884
Iteration 254/1000 | Loss: 0.00002488
Iteration 255/1000 | Loss: 0.00001864
Iteration 256/1000 | Loss: 0.00040520
Iteration 257/1000 | Loss: 0.00031777
Iteration 258/1000 | Loss: 0.00041041
Iteration 259/1000 | Loss: 0.00024617
Iteration 260/1000 | Loss: 0.00059485
Iteration 261/1000 | Loss: 0.00022974
Iteration 262/1000 | Loss: 0.00033297
Iteration 263/1000 | Loss: 0.00001907
Iteration 264/1000 | Loss: 0.00002231
Iteration 265/1000 | Loss: 0.00001687
Iteration 266/1000 | Loss: 0.00001629
Iteration 267/1000 | Loss: 0.00001586
Iteration 268/1000 | Loss: 0.00001531
Iteration 269/1000 | Loss: 0.00011614
Iteration 270/1000 | Loss: 0.00002336
Iteration 271/1000 | Loss: 0.00001761
Iteration 272/1000 | Loss: 0.00006180
Iteration 273/1000 | Loss: 0.00002393
Iteration 274/1000 | Loss: 0.00004725
Iteration 275/1000 | Loss: 0.00001472
Iteration 276/1000 | Loss: 0.00001429
Iteration 277/1000 | Loss: 0.00001405
Iteration 278/1000 | Loss: 0.00001395
Iteration 279/1000 | Loss: 0.00001390
Iteration 280/1000 | Loss: 0.00001389
Iteration 281/1000 | Loss: 0.00001387
Iteration 282/1000 | Loss: 0.00001385
Iteration 283/1000 | Loss: 0.00001381
Iteration 284/1000 | Loss: 0.00001378
Iteration 285/1000 | Loss: 0.00001375
Iteration 286/1000 | Loss: 0.00001374
Iteration 287/1000 | Loss: 0.00001373
Iteration 288/1000 | Loss: 0.00001372
Iteration 289/1000 | Loss: 0.00001371
Iteration 290/1000 | Loss: 0.00001370
Iteration 291/1000 | Loss: 0.00001369
Iteration 292/1000 | Loss: 0.00001369
Iteration 293/1000 | Loss: 0.00001369
Iteration 294/1000 | Loss: 0.00001368
Iteration 295/1000 | Loss: 0.00001367
Iteration 296/1000 | Loss: 0.00001367
Iteration 297/1000 | Loss: 0.00001366
Iteration 298/1000 | Loss: 0.00001366
Iteration 299/1000 | Loss: 0.00001364
Iteration 300/1000 | Loss: 0.00001364
Iteration 301/1000 | Loss: 0.00001363
Iteration 302/1000 | Loss: 0.00001363
Iteration 303/1000 | Loss: 0.00001363
Iteration 304/1000 | Loss: 0.00001362
Iteration 305/1000 | Loss: 0.00001362
Iteration 306/1000 | Loss: 0.00001361
Iteration 307/1000 | Loss: 0.00001358
Iteration 308/1000 | Loss: 0.00001358
Iteration 309/1000 | Loss: 0.00001357
Iteration 310/1000 | Loss: 0.00001357
Iteration 311/1000 | Loss: 0.00001356
Iteration 312/1000 | Loss: 0.00001356
Iteration 313/1000 | Loss: 0.00001356
Iteration 314/1000 | Loss: 0.00001356
Iteration 315/1000 | Loss: 0.00001356
Iteration 316/1000 | Loss: 0.00001356
Iteration 317/1000 | Loss: 0.00001355
Iteration 318/1000 | Loss: 0.00001355
Iteration 319/1000 | Loss: 0.00001355
Iteration 320/1000 | Loss: 0.00001355
Iteration 321/1000 | Loss: 0.00001354
Iteration 322/1000 | Loss: 0.00001354
Iteration 323/1000 | Loss: 0.00001354
Iteration 324/1000 | Loss: 0.00001353
Iteration 325/1000 | Loss: 0.00001353
Iteration 326/1000 | Loss: 0.00001353
Iteration 327/1000 | Loss: 0.00001352
Iteration 328/1000 | Loss: 0.00001352
Iteration 329/1000 | Loss: 0.00001351
Iteration 330/1000 | Loss: 0.00001351
Iteration 331/1000 | Loss: 0.00001351
Iteration 332/1000 | Loss: 0.00001351
Iteration 333/1000 | Loss: 0.00001351
Iteration 334/1000 | Loss: 0.00001351
Iteration 335/1000 | Loss: 0.00001351
Iteration 336/1000 | Loss: 0.00001350
Iteration 337/1000 | Loss: 0.00001350
Iteration 338/1000 | Loss: 0.00001350
Iteration 339/1000 | Loss: 0.00001350
Iteration 340/1000 | Loss: 0.00001350
Iteration 341/1000 | Loss: 0.00001350
Iteration 342/1000 | Loss: 0.00001350
Iteration 343/1000 | Loss: 0.00001350
Iteration 344/1000 | Loss: 0.00001349
Iteration 345/1000 | Loss: 0.00001349
Iteration 346/1000 | Loss: 0.00001349
Iteration 347/1000 | Loss: 0.00001348
Iteration 348/1000 | Loss: 0.00001348
Iteration 349/1000 | Loss: 0.00001348
Iteration 350/1000 | Loss: 0.00001348
Iteration 351/1000 | Loss: 0.00001348
Iteration 352/1000 | Loss: 0.00001348
Iteration 353/1000 | Loss: 0.00001347
Iteration 354/1000 | Loss: 0.00001347
Iteration 355/1000 | Loss: 0.00001347
Iteration 356/1000 | Loss: 0.00001347
Iteration 357/1000 | Loss: 0.00001347
Iteration 358/1000 | Loss: 0.00001347
Iteration 359/1000 | Loss: 0.00001347
Iteration 360/1000 | Loss: 0.00001347
Iteration 361/1000 | Loss: 0.00001347
Iteration 362/1000 | Loss: 0.00001347
Iteration 363/1000 | Loss: 0.00001347
Iteration 364/1000 | Loss: 0.00001347
Iteration 365/1000 | Loss: 0.00001347
Iteration 366/1000 | Loss: 0.00001347
Iteration 367/1000 | Loss: 0.00001347
Iteration 368/1000 | Loss: 0.00001347
Iteration 369/1000 | Loss: 0.00001347
Iteration 370/1000 | Loss: 0.00001347
Iteration 371/1000 | Loss: 0.00001347
Iteration 372/1000 | Loss: 0.00001347
Iteration 373/1000 | Loss: 0.00001347
Iteration 374/1000 | Loss: 0.00001347
Iteration 375/1000 | Loss: 0.00001347
Iteration 376/1000 | Loss: 0.00001347
Iteration 377/1000 | Loss: 0.00001347
Iteration 378/1000 | Loss: 0.00001347
Iteration 379/1000 | Loss: 0.00001347
Iteration 380/1000 | Loss: 0.00001347
Iteration 381/1000 | Loss: 0.00001347
Iteration 382/1000 | Loss: 0.00001347
Iteration 383/1000 | Loss: 0.00001347
Iteration 384/1000 | Loss: 0.00001347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 384. Stopping optimization.
Last 5 losses: [1.3469568330037873e-05, 1.3469568330037873e-05, 1.3469568330037873e-05, 1.3469568330037873e-05, 1.3469568330037873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3469568330037873e-05

Optimization complete. Final v2v error: 2.9652764797210693 mm

Highest mean error: 5.510375022888184 mm for frame 60

Lowest mean error: 2.427729368209839 mm for frame 178

Saving results

Total time: 477.9650058746338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00690407
Iteration 2/25 | Loss: 0.00140937
Iteration 3/25 | Loss: 0.00110954
Iteration 4/25 | Loss: 0.00107823
Iteration 5/25 | Loss: 0.00109430
Iteration 6/25 | Loss: 0.00106739
Iteration 7/25 | Loss: 0.00105200
Iteration 8/25 | Loss: 0.00104929
Iteration 9/25 | Loss: 0.00104877
Iteration 10/25 | Loss: 0.00104864
Iteration 11/25 | Loss: 0.00104850
Iteration 12/25 | Loss: 0.00104816
Iteration 13/25 | Loss: 0.00105140
Iteration 14/25 | Loss: 0.00104977
Iteration 15/25 | Loss: 0.00104715
Iteration 16/25 | Loss: 0.00104646
Iteration 17/25 | Loss: 0.00104632
Iteration 18/25 | Loss: 0.00104632
Iteration 19/25 | Loss: 0.00104632
Iteration 20/25 | Loss: 0.00104631
Iteration 21/25 | Loss: 0.00104631
Iteration 22/25 | Loss: 0.00104631
Iteration 23/25 | Loss: 0.00104843
Iteration 24/25 | Loss: 0.00104702
Iteration 25/25 | Loss: 0.00104751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31121540
Iteration 2/25 | Loss: 0.00117171
Iteration 3/25 | Loss: 0.00117171
Iteration 4/25 | Loss: 0.00117171
Iteration 5/25 | Loss: 0.00117171
Iteration 6/25 | Loss: 0.00117171
Iteration 7/25 | Loss: 0.00117171
Iteration 8/25 | Loss: 0.00117171
Iteration 9/25 | Loss: 0.00117171
Iteration 10/25 | Loss: 0.00117171
Iteration 11/25 | Loss: 0.00117171
Iteration 12/25 | Loss: 0.00117171
Iteration 13/25 | Loss: 0.00117171
Iteration 14/25 | Loss: 0.00117171
Iteration 15/25 | Loss: 0.00117171
Iteration 16/25 | Loss: 0.00117171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011717054294422269, 0.0011717054294422269, 0.0011717054294422269, 0.0011717054294422269, 0.0011717054294422269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011717054294422269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117171
Iteration 2/1000 | Loss: 0.00010278
Iteration 3/1000 | Loss: 0.00009720
Iteration 4/1000 | Loss: 0.00005030
Iteration 5/1000 | Loss: 0.00003170
Iteration 6/1000 | Loss: 0.00002496
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001487
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00029323
Iteration 15/1000 | Loss: 0.00012921
Iteration 16/1000 | Loss: 0.00002286
Iteration 17/1000 | Loss: 0.00001856
Iteration 18/1000 | Loss: 0.00001673
Iteration 19/1000 | Loss: 0.00001551
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001423
Iteration 22/1000 | Loss: 0.00002342
Iteration 23/1000 | Loss: 0.00002305
Iteration 24/1000 | Loss: 0.00002285
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001195
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001167
Iteration 50/1000 | Loss: 0.00001162
Iteration 51/1000 | Loss: 0.00001162
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001159
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001157
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001156
Iteration 67/1000 | Loss: 0.00001156
Iteration 68/1000 | Loss: 0.00001156
Iteration 69/1000 | Loss: 0.00001156
Iteration 70/1000 | Loss: 0.00001156
Iteration 71/1000 | Loss: 0.00001156
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001156
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001155
Iteration 81/1000 | Loss: 0.00001155
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001152
Iteration 100/1000 | Loss: 0.00001152
Iteration 101/1000 | Loss: 0.00001152
Iteration 102/1000 | Loss: 0.00001152
Iteration 103/1000 | Loss: 0.00001152
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001152
Iteration 106/1000 | Loss: 0.00001152
Iteration 107/1000 | Loss: 0.00001152
Iteration 108/1000 | Loss: 0.00001152
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001151
Iteration 112/1000 | Loss: 0.00001151
Iteration 113/1000 | Loss: 0.00001151
Iteration 114/1000 | Loss: 0.00001151
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001151
Iteration 117/1000 | Loss: 0.00001151
Iteration 118/1000 | Loss: 0.00001151
Iteration 119/1000 | Loss: 0.00001150
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001150
Iteration 122/1000 | Loss: 0.00001150
Iteration 123/1000 | Loss: 0.00001150
Iteration 124/1000 | Loss: 0.00001150
Iteration 125/1000 | Loss: 0.00001149
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001149
Iteration 128/1000 | Loss: 0.00001148
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001148
Iteration 131/1000 | Loss: 0.00001148
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001147
Iteration 137/1000 | Loss: 0.00001147
Iteration 138/1000 | Loss: 0.00001147
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001147
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001147
Iteration 147/1000 | Loss: 0.00001147
Iteration 148/1000 | Loss: 0.00001146
Iteration 149/1000 | Loss: 0.00001146
Iteration 150/1000 | Loss: 0.00001146
Iteration 151/1000 | Loss: 0.00001146
Iteration 152/1000 | Loss: 0.00001146
Iteration 153/1000 | Loss: 0.00001146
Iteration 154/1000 | Loss: 0.00001146
Iteration 155/1000 | Loss: 0.00001146
Iteration 156/1000 | Loss: 0.00001146
Iteration 157/1000 | Loss: 0.00001146
Iteration 158/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.1460182577138767e-05, 1.1460182577138767e-05, 1.1460182577138767e-05, 1.1460182577138767e-05, 1.1460182577138767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1460182577138767e-05

Optimization complete. Final v2v error: 2.902280330657959 mm

Highest mean error: 3.7457568645477295 mm for frame 118

Lowest mean error: 2.608502149581909 mm for frame 12

Saving results

Total time: 104.7587366104126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068175
Iteration 2/25 | Loss: 0.00154763
Iteration 3/25 | Loss: 0.00124769
Iteration 4/25 | Loss: 0.00119052
Iteration 5/25 | Loss: 0.00116786
Iteration 6/25 | Loss: 0.00113339
Iteration 7/25 | Loss: 0.00112757
Iteration 8/25 | Loss: 0.00111666
Iteration 9/25 | Loss: 0.00111388
Iteration 10/25 | Loss: 0.00111306
Iteration 11/25 | Loss: 0.00111283
Iteration 12/25 | Loss: 0.00111280
Iteration 13/25 | Loss: 0.00111280
Iteration 14/25 | Loss: 0.00111280
Iteration 15/25 | Loss: 0.00111280
Iteration 16/25 | Loss: 0.00111280
Iteration 17/25 | Loss: 0.00111280
Iteration 18/25 | Loss: 0.00111280
Iteration 19/25 | Loss: 0.00111280
Iteration 20/25 | Loss: 0.00111280
Iteration 21/25 | Loss: 0.00111280
Iteration 22/25 | Loss: 0.00111279
Iteration 23/25 | Loss: 0.00111279
Iteration 24/25 | Loss: 0.00111279
Iteration 25/25 | Loss: 0.00111279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.07080460
Iteration 2/25 | Loss: 0.00100099
Iteration 3/25 | Loss: 0.00100098
Iteration 4/25 | Loss: 0.00100098
Iteration 5/25 | Loss: 0.00100098
Iteration 6/25 | Loss: 0.00100098
Iteration 7/25 | Loss: 0.00100098
Iteration 8/25 | Loss: 0.00100098
Iteration 9/25 | Loss: 0.00100098
Iteration 10/25 | Loss: 0.00100098
Iteration 11/25 | Loss: 0.00100098
Iteration 12/25 | Loss: 0.00100098
Iteration 13/25 | Loss: 0.00100098
Iteration 14/25 | Loss: 0.00100098
Iteration 15/25 | Loss: 0.00100098
Iteration 16/25 | Loss: 0.00100098
Iteration 17/25 | Loss: 0.00100098
Iteration 18/25 | Loss: 0.00100098
Iteration 19/25 | Loss: 0.00100098
Iteration 20/25 | Loss: 0.00100098
Iteration 21/25 | Loss: 0.00100098
Iteration 22/25 | Loss: 0.00100098
Iteration 23/25 | Loss: 0.00100098
Iteration 24/25 | Loss: 0.00100098
Iteration 25/25 | Loss: 0.00100098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100098
Iteration 2/1000 | Loss: 0.00010090
Iteration 3/1000 | Loss: 0.00006180
Iteration 4/1000 | Loss: 0.00004657
Iteration 5/1000 | Loss: 0.00004274
Iteration 6/1000 | Loss: 0.00004093
Iteration 7/1000 | Loss: 0.00003933
Iteration 8/1000 | Loss: 0.00003815
Iteration 9/1000 | Loss: 0.00003742
Iteration 10/1000 | Loss: 0.00003678
Iteration 11/1000 | Loss: 0.00003628
Iteration 12/1000 | Loss: 0.00003590
Iteration 13/1000 | Loss: 0.00003561
Iteration 14/1000 | Loss: 0.00003534
Iteration 15/1000 | Loss: 0.00003524
Iteration 16/1000 | Loss: 0.00003504
Iteration 17/1000 | Loss: 0.00003502
Iteration 18/1000 | Loss: 0.00003485
Iteration 19/1000 | Loss: 0.00003471
Iteration 20/1000 | Loss: 0.00003467
Iteration 21/1000 | Loss: 0.00003465
Iteration 22/1000 | Loss: 0.00003461
Iteration 23/1000 | Loss: 0.00003459
Iteration 24/1000 | Loss: 0.00003456
Iteration 25/1000 | Loss: 0.00003453
Iteration 26/1000 | Loss: 0.00003452
Iteration 27/1000 | Loss: 0.00003452
Iteration 28/1000 | Loss: 0.00003449
Iteration 29/1000 | Loss: 0.00003448
Iteration 30/1000 | Loss: 0.00003445
Iteration 31/1000 | Loss: 0.00003444
Iteration 32/1000 | Loss: 0.00003443
Iteration 33/1000 | Loss: 0.00003442
Iteration 34/1000 | Loss: 0.00003436
Iteration 35/1000 | Loss: 0.00003435
Iteration 36/1000 | Loss: 0.00003434
Iteration 37/1000 | Loss: 0.00003433
Iteration 38/1000 | Loss: 0.00003432
Iteration 39/1000 | Loss: 0.00003432
Iteration 40/1000 | Loss: 0.00003429
Iteration 41/1000 | Loss: 0.00003428
Iteration 42/1000 | Loss: 0.00003426
Iteration 43/1000 | Loss: 0.00003426
Iteration 44/1000 | Loss: 0.00003425
Iteration 45/1000 | Loss: 0.00003425
Iteration 46/1000 | Loss: 0.00003425
Iteration 47/1000 | Loss: 0.00003424
Iteration 48/1000 | Loss: 0.00003424
Iteration 49/1000 | Loss: 0.00003423
Iteration 50/1000 | Loss: 0.00003422
Iteration 51/1000 | Loss: 0.00003422
Iteration 52/1000 | Loss: 0.00003422
Iteration 53/1000 | Loss: 0.00003421
Iteration 54/1000 | Loss: 0.00003420
Iteration 55/1000 | Loss: 0.00003420
Iteration 56/1000 | Loss: 0.00003420
Iteration 57/1000 | Loss: 0.00003419
Iteration 58/1000 | Loss: 0.00003419
Iteration 59/1000 | Loss: 0.00003419
Iteration 60/1000 | Loss: 0.00003418
Iteration 61/1000 | Loss: 0.00003418
Iteration 62/1000 | Loss: 0.00003418
Iteration 63/1000 | Loss: 0.00003417
Iteration 64/1000 | Loss: 0.00010204
Iteration 65/1000 | Loss: 0.00003423
Iteration 66/1000 | Loss: 0.00003416
Iteration 67/1000 | Loss: 0.00003416
Iteration 68/1000 | Loss: 0.00003416
Iteration 69/1000 | Loss: 0.00003416
Iteration 70/1000 | Loss: 0.00003416
Iteration 71/1000 | Loss: 0.00003416
Iteration 72/1000 | Loss: 0.00003415
Iteration 73/1000 | Loss: 0.00003415
Iteration 74/1000 | Loss: 0.00003411
Iteration 75/1000 | Loss: 0.00003411
Iteration 76/1000 | Loss: 0.00003411
Iteration 77/1000 | Loss: 0.00003411
Iteration 78/1000 | Loss: 0.00003411
Iteration 79/1000 | Loss: 0.00003411
Iteration 80/1000 | Loss: 0.00003411
Iteration 81/1000 | Loss: 0.00003410
Iteration 82/1000 | Loss: 0.00003410
Iteration 83/1000 | Loss: 0.00003410
Iteration 84/1000 | Loss: 0.00003410
Iteration 85/1000 | Loss: 0.00003410
Iteration 86/1000 | Loss: 0.00003409
Iteration 87/1000 | Loss: 0.00003409
Iteration 88/1000 | Loss: 0.00003409
Iteration 89/1000 | Loss: 0.00003409
Iteration 90/1000 | Loss: 0.00003409
Iteration 91/1000 | Loss: 0.00003409
Iteration 92/1000 | Loss: 0.00003409
Iteration 93/1000 | Loss: 0.00003408
Iteration 94/1000 | Loss: 0.00003408
Iteration 95/1000 | Loss: 0.00003408
Iteration 96/1000 | Loss: 0.00003408
Iteration 97/1000 | Loss: 0.00003407
Iteration 98/1000 | Loss: 0.00003407
Iteration 99/1000 | Loss: 0.00003407
Iteration 100/1000 | Loss: 0.00003407
Iteration 101/1000 | Loss: 0.00003407
Iteration 102/1000 | Loss: 0.00003406
Iteration 103/1000 | Loss: 0.00003406
Iteration 104/1000 | Loss: 0.00003405
Iteration 105/1000 | Loss: 0.00003405
Iteration 106/1000 | Loss: 0.00003405
Iteration 107/1000 | Loss: 0.00003405
Iteration 108/1000 | Loss: 0.00003404
Iteration 109/1000 | Loss: 0.00003404
Iteration 110/1000 | Loss: 0.00003404
Iteration 111/1000 | Loss: 0.00003404
Iteration 112/1000 | Loss: 0.00003404
Iteration 113/1000 | Loss: 0.00003404
Iteration 114/1000 | Loss: 0.00003404
Iteration 115/1000 | Loss: 0.00003404
Iteration 116/1000 | Loss: 0.00003404
Iteration 117/1000 | Loss: 0.00003404
Iteration 118/1000 | Loss: 0.00003403
Iteration 119/1000 | Loss: 0.00003403
Iteration 120/1000 | Loss: 0.00003403
Iteration 121/1000 | Loss: 0.00003402
Iteration 122/1000 | Loss: 0.00003402
Iteration 123/1000 | Loss: 0.00003402
Iteration 124/1000 | Loss: 0.00003401
Iteration 125/1000 | Loss: 0.00003401
Iteration 126/1000 | Loss: 0.00003401
Iteration 127/1000 | Loss: 0.00003401
Iteration 128/1000 | Loss: 0.00003400
Iteration 129/1000 | Loss: 0.00003400
Iteration 130/1000 | Loss: 0.00003400
Iteration 131/1000 | Loss: 0.00003400
Iteration 132/1000 | Loss: 0.00003399
Iteration 133/1000 | Loss: 0.00003399
Iteration 134/1000 | Loss: 0.00003399
Iteration 135/1000 | Loss: 0.00003398
Iteration 136/1000 | Loss: 0.00003398
Iteration 137/1000 | Loss: 0.00003398
Iteration 138/1000 | Loss: 0.00003398
Iteration 139/1000 | Loss: 0.00003398
Iteration 140/1000 | Loss: 0.00003398
Iteration 141/1000 | Loss: 0.00003398
Iteration 142/1000 | Loss: 0.00003398
Iteration 143/1000 | Loss: 0.00003398
Iteration 144/1000 | Loss: 0.00003398
Iteration 145/1000 | Loss: 0.00003397
Iteration 146/1000 | Loss: 0.00003397
Iteration 147/1000 | Loss: 0.00003397
Iteration 148/1000 | Loss: 0.00003397
Iteration 149/1000 | Loss: 0.00003397
Iteration 150/1000 | Loss: 0.00003396
Iteration 151/1000 | Loss: 0.00003396
Iteration 152/1000 | Loss: 0.00003396
Iteration 153/1000 | Loss: 0.00003396
Iteration 154/1000 | Loss: 0.00003396
Iteration 155/1000 | Loss: 0.00003396
Iteration 156/1000 | Loss: 0.00003396
Iteration 157/1000 | Loss: 0.00003395
Iteration 158/1000 | Loss: 0.00003395
Iteration 159/1000 | Loss: 0.00003395
Iteration 160/1000 | Loss: 0.00003395
Iteration 161/1000 | Loss: 0.00003395
Iteration 162/1000 | Loss: 0.00003395
Iteration 163/1000 | Loss: 0.00003395
Iteration 164/1000 | Loss: 0.00003395
Iteration 165/1000 | Loss: 0.00003394
Iteration 166/1000 | Loss: 0.00003394
Iteration 167/1000 | Loss: 0.00003394
Iteration 168/1000 | Loss: 0.00003394
Iteration 169/1000 | Loss: 0.00003394
Iteration 170/1000 | Loss: 0.00003394
Iteration 171/1000 | Loss: 0.00003394
Iteration 172/1000 | Loss: 0.00003394
Iteration 173/1000 | Loss: 0.00003393
Iteration 174/1000 | Loss: 0.00003393
Iteration 175/1000 | Loss: 0.00003393
Iteration 176/1000 | Loss: 0.00003393
Iteration 177/1000 | Loss: 0.00003393
Iteration 178/1000 | Loss: 0.00003393
Iteration 179/1000 | Loss: 0.00003392
Iteration 180/1000 | Loss: 0.00003392
Iteration 181/1000 | Loss: 0.00003392
Iteration 182/1000 | Loss: 0.00003392
Iteration 183/1000 | Loss: 0.00003392
Iteration 184/1000 | Loss: 0.00003392
Iteration 185/1000 | Loss: 0.00003392
Iteration 186/1000 | Loss: 0.00003392
Iteration 187/1000 | Loss: 0.00003392
Iteration 188/1000 | Loss: 0.00003392
Iteration 189/1000 | Loss: 0.00003391
Iteration 190/1000 | Loss: 0.00003391
Iteration 191/1000 | Loss: 0.00003391
Iteration 192/1000 | Loss: 0.00003391
Iteration 193/1000 | Loss: 0.00003391
Iteration 194/1000 | Loss: 0.00003391
Iteration 195/1000 | Loss: 0.00003391
Iteration 196/1000 | Loss: 0.00003391
Iteration 197/1000 | Loss: 0.00003391
Iteration 198/1000 | Loss: 0.00003391
Iteration 199/1000 | Loss: 0.00003391
Iteration 200/1000 | Loss: 0.00003391
Iteration 201/1000 | Loss: 0.00003391
Iteration 202/1000 | Loss: 0.00003391
Iteration 203/1000 | Loss: 0.00003391
Iteration 204/1000 | Loss: 0.00003391
Iteration 205/1000 | Loss: 0.00003391
Iteration 206/1000 | Loss: 0.00003391
Iteration 207/1000 | Loss: 0.00003391
Iteration 208/1000 | Loss: 0.00003390
Iteration 209/1000 | Loss: 0.00003390
Iteration 210/1000 | Loss: 0.00003390
Iteration 211/1000 | Loss: 0.00003390
Iteration 212/1000 | Loss: 0.00003390
Iteration 213/1000 | Loss: 0.00003390
Iteration 214/1000 | Loss: 0.00003390
Iteration 215/1000 | Loss: 0.00003390
Iteration 216/1000 | Loss: 0.00003390
Iteration 217/1000 | Loss: 0.00003390
Iteration 218/1000 | Loss: 0.00003390
Iteration 219/1000 | Loss: 0.00003390
Iteration 220/1000 | Loss: 0.00003390
Iteration 221/1000 | Loss: 0.00003389
Iteration 222/1000 | Loss: 0.00003389
Iteration 223/1000 | Loss: 0.00003389
Iteration 224/1000 | Loss: 0.00003389
Iteration 225/1000 | Loss: 0.00003389
Iteration 226/1000 | Loss: 0.00003389
Iteration 227/1000 | Loss: 0.00003389
Iteration 228/1000 | Loss: 0.00003389
Iteration 229/1000 | Loss: 0.00003388
Iteration 230/1000 | Loss: 0.00003388
Iteration 231/1000 | Loss: 0.00003388
Iteration 232/1000 | Loss: 0.00003388
Iteration 233/1000 | Loss: 0.00003388
Iteration 234/1000 | Loss: 0.00003387
Iteration 235/1000 | Loss: 0.00003387
Iteration 236/1000 | Loss: 0.00003387
Iteration 237/1000 | Loss: 0.00003387
Iteration 238/1000 | Loss: 0.00003387
Iteration 239/1000 | Loss: 0.00003387
Iteration 240/1000 | Loss: 0.00003387
Iteration 241/1000 | Loss: 0.00003387
Iteration 242/1000 | Loss: 0.00003387
Iteration 243/1000 | Loss: 0.00003387
Iteration 244/1000 | Loss: 0.00003387
Iteration 245/1000 | Loss: 0.00003387
Iteration 246/1000 | Loss: 0.00003387
Iteration 247/1000 | Loss: 0.00003387
Iteration 248/1000 | Loss: 0.00003387
Iteration 249/1000 | Loss: 0.00003387
Iteration 250/1000 | Loss: 0.00003387
Iteration 251/1000 | Loss: 0.00003387
Iteration 252/1000 | Loss: 0.00003387
Iteration 253/1000 | Loss: 0.00003387
Iteration 254/1000 | Loss: 0.00003387
Iteration 255/1000 | Loss: 0.00003387
Iteration 256/1000 | Loss: 0.00003387
Iteration 257/1000 | Loss: 0.00003387
Iteration 258/1000 | Loss: 0.00003387
Iteration 259/1000 | Loss: 0.00003387
Iteration 260/1000 | Loss: 0.00003387
Iteration 261/1000 | Loss: 0.00003387
Iteration 262/1000 | Loss: 0.00003387
Iteration 263/1000 | Loss: 0.00003387
Iteration 264/1000 | Loss: 0.00003387
Iteration 265/1000 | Loss: 0.00003387
Iteration 266/1000 | Loss: 0.00003387
Iteration 267/1000 | Loss: 0.00003387
Iteration 268/1000 | Loss: 0.00003387
Iteration 269/1000 | Loss: 0.00003387
Iteration 270/1000 | Loss: 0.00003387
Iteration 271/1000 | Loss: 0.00003387
Iteration 272/1000 | Loss: 0.00003387
Iteration 273/1000 | Loss: 0.00003387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [3.38718164130114e-05, 3.38718164130114e-05, 3.38718164130114e-05, 3.38718164130114e-05, 3.38718164130114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.38718164130114e-05

Optimization complete. Final v2v error: 4.605197906494141 mm

Highest mean error: 7.2267351150512695 mm for frame 99

Lowest mean error: 3.028029441833496 mm for frame 141

Saving results

Total time: 64.62315368652344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00624344
Iteration 2/25 | Loss: 0.00136896
Iteration 3/25 | Loss: 0.00121925
Iteration 4/25 | Loss: 0.00119890
Iteration 5/25 | Loss: 0.00119279
Iteration 6/25 | Loss: 0.00119175
Iteration 7/25 | Loss: 0.00119175
Iteration 8/25 | Loss: 0.00119175
Iteration 9/25 | Loss: 0.00119175
Iteration 10/25 | Loss: 0.00119175
Iteration 11/25 | Loss: 0.00119175
Iteration 12/25 | Loss: 0.00119175
Iteration 13/25 | Loss: 0.00119175
Iteration 14/25 | Loss: 0.00119175
Iteration 15/25 | Loss: 0.00119175
Iteration 16/25 | Loss: 0.00119175
Iteration 17/25 | Loss: 0.00119175
Iteration 18/25 | Loss: 0.00119175
Iteration 19/25 | Loss: 0.00119175
Iteration 20/25 | Loss: 0.00119175
Iteration 21/25 | Loss: 0.00119175
Iteration 22/25 | Loss: 0.00119175
Iteration 23/25 | Loss: 0.00119175
Iteration 24/25 | Loss: 0.00119175
Iteration 25/25 | Loss: 0.00119175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.75554609
Iteration 2/25 | Loss: 0.00080187
Iteration 3/25 | Loss: 0.00080187
Iteration 4/25 | Loss: 0.00080187
Iteration 5/25 | Loss: 0.00080187
Iteration 6/25 | Loss: 0.00080187
Iteration 7/25 | Loss: 0.00080187
Iteration 8/25 | Loss: 0.00080187
Iteration 9/25 | Loss: 0.00080187
Iteration 10/25 | Loss: 0.00080186
Iteration 11/25 | Loss: 0.00080186
Iteration 12/25 | Loss: 0.00080186
Iteration 13/25 | Loss: 0.00080186
Iteration 14/25 | Loss: 0.00080186
Iteration 15/25 | Loss: 0.00080186
Iteration 16/25 | Loss: 0.00080186
Iteration 17/25 | Loss: 0.00080186
Iteration 18/25 | Loss: 0.00080186
Iteration 19/25 | Loss: 0.00080186
Iteration 20/25 | Loss: 0.00080186
Iteration 21/25 | Loss: 0.00080186
Iteration 22/25 | Loss: 0.00080186
Iteration 23/25 | Loss: 0.00080186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008018646040000021, 0.0008018646040000021, 0.0008018646040000021, 0.0008018646040000021, 0.0008018646040000021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008018646040000021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080186
Iteration 2/1000 | Loss: 0.00004890
Iteration 3/1000 | Loss: 0.00002780
Iteration 4/1000 | Loss: 0.00002365
Iteration 5/1000 | Loss: 0.00002258
Iteration 6/1000 | Loss: 0.00002168
Iteration 7/1000 | Loss: 0.00002118
Iteration 8/1000 | Loss: 0.00002089
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002029
Iteration 11/1000 | Loss: 0.00002006
Iteration 12/1000 | Loss: 0.00001993
Iteration 13/1000 | Loss: 0.00001989
Iteration 14/1000 | Loss: 0.00001981
Iteration 15/1000 | Loss: 0.00001979
Iteration 16/1000 | Loss: 0.00001978
Iteration 17/1000 | Loss: 0.00001972
Iteration 18/1000 | Loss: 0.00001970
Iteration 19/1000 | Loss: 0.00001967
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001962
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001957
Iteration 24/1000 | Loss: 0.00001957
Iteration 25/1000 | Loss: 0.00001955
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001950
Iteration 29/1000 | Loss: 0.00001949
Iteration 30/1000 | Loss: 0.00001949
Iteration 31/1000 | Loss: 0.00001948
Iteration 32/1000 | Loss: 0.00001948
Iteration 33/1000 | Loss: 0.00001948
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001947
Iteration 36/1000 | Loss: 0.00001947
Iteration 37/1000 | Loss: 0.00001947
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001946
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001946
Iteration 42/1000 | Loss: 0.00001946
Iteration 43/1000 | Loss: 0.00001946
Iteration 44/1000 | Loss: 0.00001946
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00001945
Iteration 48/1000 | Loss: 0.00001945
Iteration 49/1000 | Loss: 0.00001944
Iteration 50/1000 | Loss: 0.00001943
Iteration 51/1000 | Loss: 0.00001943
Iteration 52/1000 | Loss: 0.00001943
Iteration 53/1000 | Loss: 0.00001943
Iteration 54/1000 | Loss: 0.00001943
Iteration 55/1000 | Loss: 0.00001943
Iteration 56/1000 | Loss: 0.00001943
Iteration 57/1000 | Loss: 0.00001943
Iteration 58/1000 | Loss: 0.00001942
Iteration 59/1000 | Loss: 0.00001942
Iteration 60/1000 | Loss: 0.00001941
Iteration 61/1000 | Loss: 0.00001941
Iteration 62/1000 | Loss: 0.00001941
Iteration 63/1000 | Loss: 0.00001941
Iteration 64/1000 | Loss: 0.00001941
Iteration 65/1000 | Loss: 0.00001941
Iteration 66/1000 | Loss: 0.00001941
Iteration 67/1000 | Loss: 0.00001941
Iteration 68/1000 | Loss: 0.00001940
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001940
Iteration 71/1000 | Loss: 0.00001940
Iteration 72/1000 | Loss: 0.00001939
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001939
Iteration 75/1000 | Loss: 0.00001939
Iteration 76/1000 | Loss: 0.00001939
Iteration 77/1000 | Loss: 0.00001938
Iteration 78/1000 | Loss: 0.00001938
Iteration 79/1000 | Loss: 0.00001938
Iteration 80/1000 | Loss: 0.00001938
Iteration 81/1000 | Loss: 0.00001938
Iteration 82/1000 | Loss: 0.00001938
Iteration 83/1000 | Loss: 0.00001938
Iteration 84/1000 | Loss: 0.00001938
Iteration 85/1000 | Loss: 0.00001938
Iteration 86/1000 | Loss: 0.00001938
Iteration 87/1000 | Loss: 0.00001938
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001936
Iteration 95/1000 | Loss: 0.00001936
Iteration 96/1000 | Loss: 0.00001936
Iteration 97/1000 | Loss: 0.00001936
Iteration 98/1000 | Loss: 0.00001936
Iteration 99/1000 | Loss: 0.00001935
Iteration 100/1000 | Loss: 0.00001935
Iteration 101/1000 | Loss: 0.00001935
Iteration 102/1000 | Loss: 0.00001935
Iteration 103/1000 | Loss: 0.00001934
Iteration 104/1000 | Loss: 0.00001934
Iteration 105/1000 | Loss: 0.00001934
Iteration 106/1000 | Loss: 0.00001934
Iteration 107/1000 | Loss: 0.00001934
Iteration 108/1000 | Loss: 0.00001934
Iteration 109/1000 | Loss: 0.00001934
Iteration 110/1000 | Loss: 0.00001934
Iteration 111/1000 | Loss: 0.00001934
Iteration 112/1000 | Loss: 0.00001934
Iteration 113/1000 | Loss: 0.00001934
Iteration 114/1000 | Loss: 0.00001934
Iteration 115/1000 | Loss: 0.00001934
Iteration 116/1000 | Loss: 0.00001934
Iteration 117/1000 | Loss: 0.00001934
Iteration 118/1000 | Loss: 0.00001934
Iteration 119/1000 | Loss: 0.00001934
Iteration 120/1000 | Loss: 0.00001934
Iteration 121/1000 | Loss: 0.00001934
Iteration 122/1000 | Loss: 0.00001934
Iteration 123/1000 | Loss: 0.00001934
Iteration 124/1000 | Loss: 0.00001934
Iteration 125/1000 | Loss: 0.00001934
Iteration 126/1000 | Loss: 0.00001934
Iteration 127/1000 | Loss: 0.00001934
Iteration 128/1000 | Loss: 0.00001934
Iteration 129/1000 | Loss: 0.00001934
Iteration 130/1000 | Loss: 0.00001934
Iteration 131/1000 | Loss: 0.00001934
Iteration 132/1000 | Loss: 0.00001934
Iteration 133/1000 | Loss: 0.00001934
Iteration 134/1000 | Loss: 0.00001934
Iteration 135/1000 | Loss: 0.00001934
Iteration 136/1000 | Loss: 0.00001934
Iteration 137/1000 | Loss: 0.00001934
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001934
Iteration 148/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.933756902872119e-05, 1.933756902872119e-05, 1.933756902872119e-05, 1.933756902872119e-05, 1.933756902872119e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.933756902872119e-05

Optimization complete. Final v2v error: 3.6243338584899902 mm

Highest mean error: 3.907207489013672 mm for frame 136

Lowest mean error: 3.126105308532715 mm for frame 215

Saving results

Total time: 42.19323182106018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813227
Iteration 2/25 | Loss: 0.00164197
Iteration 3/25 | Loss: 0.00129744
Iteration 4/25 | Loss: 0.00124446
Iteration 5/25 | Loss: 0.00123253
Iteration 6/25 | Loss: 0.00122775
Iteration 7/25 | Loss: 0.00122535
Iteration 8/25 | Loss: 0.00122331
Iteration 9/25 | Loss: 0.00122577
Iteration 10/25 | Loss: 0.00121971
Iteration 11/25 | Loss: 0.00121814
Iteration 12/25 | Loss: 0.00121709
Iteration 13/25 | Loss: 0.00121621
Iteration 14/25 | Loss: 0.00121540
Iteration 15/25 | Loss: 0.00121493
Iteration 16/25 | Loss: 0.00121465
Iteration 17/25 | Loss: 0.00121453
Iteration 18/25 | Loss: 0.00121450
Iteration 19/25 | Loss: 0.00121450
Iteration 20/25 | Loss: 0.00121449
Iteration 21/25 | Loss: 0.00121449
Iteration 22/25 | Loss: 0.00121449
Iteration 23/25 | Loss: 0.00121449
Iteration 24/25 | Loss: 0.00121449
Iteration 25/25 | Loss: 0.00121448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45456266
Iteration 2/25 | Loss: 0.00228663
Iteration 3/25 | Loss: 0.00228658
Iteration 4/25 | Loss: 0.00228658
Iteration 5/25 | Loss: 0.00228658
Iteration 6/25 | Loss: 0.00228658
Iteration 7/25 | Loss: 0.00228658
Iteration 8/25 | Loss: 0.00228658
Iteration 9/25 | Loss: 0.00228658
Iteration 10/25 | Loss: 0.00228658
Iteration 11/25 | Loss: 0.00228658
Iteration 12/25 | Loss: 0.00228658
Iteration 13/25 | Loss: 0.00228658
Iteration 14/25 | Loss: 0.00228658
Iteration 15/25 | Loss: 0.00228658
Iteration 16/25 | Loss: 0.00228658
Iteration 17/25 | Loss: 0.00228658
Iteration 18/25 | Loss: 0.00228658
Iteration 19/25 | Loss: 0.00228658
Iteration 20/25 | Loss: 0.00228658
Iteration 21/25 | Loss: 0.00228658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022865773644298315, 0.0022865773644298315, 0.0022865773644298315, 0.0022865773644298315, 0.0022865773644298315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022865773644298315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228658
Iteration 2/1000 | Loss: 0.00178688
Iteration 3/1000 | Loss: 0.00433118
Iteration 4/1000 | Loss: 0.00187743
Iteration 5/1000 | Loss: 0.00117531
Iteration 6/1000 | Loss: 0.00026874
Iteration 7/1000 | Loss: 0.00038255
Iteration 8/1000 | Loss: 0.00073189
Iteration 9/1000 | Loss: 0.00013724
Iteration 10/1000 | Loss: 0.00011322
Iteration 11/1000 | Loss: 0.00009013
Iteration 12/1000 | Loss: 0.00007683
Iteration 13/1000 | Loss: 0.00006936
Iteration 14/1000 | Loss: 0.00006364
Iteration 15/1000 | Loss: 0.00005969
Iteration 16/1000 | Loss: 0.00005693
Iteration 17/1000 | Loss: 0.00013649
Iteration 18/1000 | Loss: 0.00009680
Iteration 19/1000 | Loss: 0.00010336
Iteration 20/1000 | Loss: 0.00005229
Iteration 21/1000 | Loss: 0.00004979
Iteration 22/1000 | Loss: 0.00004801
Iteration 23/1000 | Loss: 0.00004637
Iteration 24/1000 | Loss: 0.00004516
Iteration 25/1000 | Loss: 0.00008529
Iteration 26/1000 | Loss: 0.00004624
Iteration 27/1000 | Loss: 0.00004321
Iteration 28/1000 | Loss: 0.00033326
Iteration 29/1000 | Loss: 0.00029585
Iteration 30/1000 | Loss: 0.00031168
Iteration 31/1000 | Loss: 0.00038655
Iteration 32/1000 | Loss: 0.00029000
Iteration 33/1000 | Loss: 0.00004600
Iteration 34/1000 | Loss: 0.00036445
Iteration 35/1000 | Loss: 0.00021878
Iteration 36/1000 | Loss: 0.00004235
Iteration 37/1000 | Loss: 0.00004109
Iteration 38/1000 | Loss: 0.00004033
Iteration 39/1000 | Loss: 0.00030892
Iteration 40/1000 | Loss: 0.00025487
Iteration 41/1000 | Loss: 0.00068816
Iteration 42/1000 | Loss: 0.00040571
Iteration 43/1000 | Loss: 0.00035081
Iteration 44/1000 | Loss: 0.00061615
Iteration 45/1000 | Loss: 0.00057079
Iteration 46/1000 | Loss: 0.00033266
Iteration 47/1000 | Loss: 0.00004704
Iteration 48/1000 | Loss: 0.00004335
Iteration 49/1000 | Loss: 0.00066556
Iteration 50/1000 | Loss: 0.00161884
Iteration 51/1000 | Loss: 0.00116942
Iteration 52/1000 | Loss: 0.00214060
Iteration 53/1000 | Loss: 0.00132283
Iteration 54/1000 | Loss: 0.00123254
Iteration 55/1000 | Loss: 0.00170129
Iteration 56/1000 | Loss: 0.00196085
Iteration 57/1000 | Loss: 0.00122798
Iteration 58/1000 | Loss: 0.00161857
Iteration 59/1000 | Loss: 0.00126321
Iteration 60/1000 | Loss: 0.00068504
Iteration 61/1000 | Loss: 0.00033365
Iteration 62/1000 | Loss: 0.00021384
Iteration 63/1000 | Loss: 0.00004813
Iteration 64/1000 | Loss: 0.00003972
Iteration 65/1000 | Loss: 0.00003413
Iteration 66/1000 | Loss: 0.00003054
Iteration 67/1000 | Loss: 0.00002797
Iteration 68/1000 | Loss: 0.00002607
Iteration 69/1000 | Loss: 0.00002513
Iteration 70/1000 | Loss: 0.00002431
Iteration 71/1000 | Loss: 0.00002378
Iteration 72/1000 | Loss: 0.00061783
Iteration 73/1000 | Loss: 0.00059774
Iteration 74/1000 | Loss: 0.00018614
Iteration 75/1000 | Loss: 0.00034243
Iteration 76/1000 | Loss: 0.00030279
Iteration 77/1000 | Loss: 0.00004408
Iteration 78/1000 | Loss: 0.00002744
Iteration 79/1000 | Loss: 0.00024523
Iteration 80/1000 | Loss: 0.00029208
Iteration 81/1000 | Loss: 0.00056149
Iteration 82/1000 | Loss: 0.00039601
Iteration 83/1000 | Loss: 0.00029815
Iteration 84/1000 | Loss: 0.00023215
Iteration 85/1000 | Loss: 0.00004811
Iteration 86/1000 | Loss: 0.00003871
Iteration 87/1000 | Loss: 0.00002977
Iteration 88/1000 | Loss: 0.00002805
Iteration 89/1000 | Loss: 0.00029433
Iteration 90/1000 | Loss: 0.00003475
Iteration 91/1000 | Loss: 0.00002930
Iteration 92/1000 | Loss: 0.00002689
Iteration 93/1000 | Loss: 0.00002511
Iteration 94/1000 | Loss: 0.00002362
Iteration 95/1000 | Loss: 0.00002202
Iteration 96/1000 | Loss: 0.00008876
Iteration 97/1000 | Loss: 0.00002506
Iteration 98/1000 | Loss: 0.00002245
Iteration 99/1000 | Loss: 0.00002025
Iteration 100/1000 | Loss: 0.00001944
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001835
Iteration 105/1000 | Loss: 0.00059349
Iteration 106/1000 | Loss: 0.00021579
Iteration 107/1000 | Loss: 0.00015991
Iteration 108/1000 | Loss: 0.00037501
Iteration 109/1000 | Loss: 0.00027057
Iteration 110/1000 | Loss: 0.00016525
Iteration 111/1000 | Loss: 0.00005865
Iteration 112/1000 | Loss: 0.00022052
Iteration 113/1000 | Loss: 0.00002443
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001843
Iteration 116/1000 | Loss: 0.00001751
Iteration 117/1000 | Loss: 0.00001711
Iteration 118/1000 | Loss: 0.00001669
Iteration 119/1000 | Loss: 0.00001629
Iteration 120/1000 | Loss: 0.00001606
Iteration 121/1000 | Loss: 0.00001590
Iteration 122/1000 | Loss: 0.00001589
Iteration 123/1000 | Loss: 0.00001586
Iteration 124/1000 | Loss: 0.00001586
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001574
Iteration 129/1000 | Loss: 0.00001572
Iteration 130/1000 | Loss: 0.00001572
Iteration 131/1000 | Loss: 0.00001572
Iteration 132/1000 | Loss: 0.00001572
Iteration 133/1000 | Loss: 0.00001570
Iteration 134/1000 | Loss: 0.00001569
Iteration 135/1000 | Loss: 0.00001569
Iteration 136/1000 | Loss: 0.00001568
Iteration 137/1000 | Loss: 0.00001568
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001567
Iteration 141/1000 | Loss: 0.00001567
Iteration 142/1000 | Loss: 0.00001566
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001566
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001566
Iteration 149/1000 | Loss: 0.00001565
Iteration 150/1000 | Loss: 0.00001565
Iteration 151/1000 | Loss: 0.00001565
Iteration 152/1000 | Loss: 0.00001564
Iteration 153/1000 | Loss: 0.00001564
Iteration 154/1000 | Loss: 0.00001564
Iteration 155/1000 | Loss: 0.00001564
Iteration 156/1000 | Loss: 0.00001563
Iteration 157/1000 | Loss: 0.00001563
Iteration 158/1000 | Loss: 0.00001563
Iteration 159/1000 | Loss: 0.00001563
Iteration 160/1000 | Loss: 0.00001563
Iteration 161/1000 | Loss: 0.00001563
Iteration 162/1000 | Loss: 0.00001563
Iteration 163/1000 | Loss: 0.00001563
Iteration 164/1000 | Loss: 0.00001563
Iteration 165/1000 | Loss: 0.00001563
Iteration 166/1000 | Loss: 0.00001563
Iteration 167/1000 | Loss: 0.00001562
Iteration 168/1000 | Loss: 0.00001562
Iteration 169/1000 | Loss: 0.00001562
Iteration 170/1000 | Loss: 0.00001562
Iteration 171/1000 | Loss: 0.00001562
Iteration 172/1000 | Loss: 0.00001562
Iteration 173/1000 | Loss: 0.00001562
Iteration 174/1000 | Loss: 0.00001561
Iteration 175/1000 | Loss: 0.00001561
Iteration 176/1000 | Loss: 0.00001561
Iteration 177/1000 | Loss: 0.00001561
Iteration 178/1000 | Loss: 0.00001561
Iteration 179/1000 | Loss: 0.00001561
Iteration 180/1000 | Loss: 0.00001561
Iteration 181/1000 | Loss: 0.00001561
Iteration 182/1000 | Loss: 0.00001561
Iteration 183/1000 | Loss: 0.00001561
Iteration 184/1000 | Loss: 0.00001561
Iteration 185/1000 | Loss: 0.00001561
Iteration 186/1000 | Loss: 0.00001561
Iteration 187/1000 | Loss: 0.00001561
Iteration 188/1000 | Loss: 0.00001561
Iteration 189/1000 | Loss: 0.00001561
Iteration 190/1000 | Loss: 0.00001561
Iteration 191/1000 | Loss: 0.00001561
Iteration 192/1000 | Loss: 0.00001561
Iteration 193/1000 | Loss: 0.00001561
Iteration 194/1000 | Loss: 0.00001561
Iteration 195/1000 | Loss: 0.00001561
Iteration 196/1000 | Loss: 0.00001561
Iteration 197/1000 | Loss: 0.00001561
Iteration 198/1000 | Loss: 0.00001561
Iteration 199/1000 | Loss: 0.00001561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.5606730812578462e-05, 1.5606730812578462e-05, 1.5606730812578462e-05, 1.5606730812578462e-05, 1.5606730812578462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5606730812578462e-05

Optimization complete. Final v2v error: 3.1536483764648438 mm

Highest mean error: 11.486896514892578 mm for frame 41

Lowest mean error: 2.8263158798217773 mm for frame 1

Saving results

Total time: 233.0581865310669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777596
Iteration 2/25 | Loss: 0.00124426
Iteration 3/25 | Loss: 0.00104797
Iteration 4/25 | Loss: 0.00103143
Iteration 5/25 | Loss: 0.00102955
Iteration 6/25 | Loss: 0.00102955
Iteration 7/25 | Loss: 0.00102955
Iteration 8/25 | Loss: 0.00102955
Iteration 9/25 | Loss: 0.00102955
Iteration 10/25 | Loss: 0.00102955
Iteration 11/25 | Loss: 0.00102955
Iteration 12/25 | Loss: 0.00102955
Iteration 13/25 | Loss: 0.00102955
Iteration 14/25 | Loss: 0.00102955
Iteration 15/25 | Loss: 0.00102955
Iteration 16/25 | Loss: 0.00102955
Iteration 17/25 | Loss: 0.00102955
Iteration 18/25 | Loss: 0.00102955
Iteration 19/25 | Loss: 0.00102955
Iteration 20/25 | Loss: 0.00102955
Iteration 21/25 | Loss: 0.00102955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010295497486367822, 0.0010295497486367822, 0.0010295497486367822, 0.0010295497486367822, 0.0010295497486367822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010295497486367822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35352194
Iteration 2/25 | Loss: 0.00058722
Iteration 3/25 | Loss: 0.00058721
Iteration 4/25 | Loss: 0.00058721
Iteration 5/25 | Loss: 0.00058721
Iteration 6/25 | Loss: 0.00058721
Iteration 7/25 | Loss: 0.00058721
Iteration 8/25 | Loss: 0.00058721
Iteration 9/25 | Loss: 0.00058721
Iteration 10/25 | Loss: 0.00058721
Iteration 11/25 | Loss: 0.00058721
Iteration 12/25 | Loss: 0.00058721
Iteration 13/25 | Loss: 0.00058721
Iteration 14/25 | Loss: 0.00058721
Iteration 15/25 | Loss: 0.00058721
Iteration 16/25 | Loss: 0.00058721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005872075562365353, 0.0005872075562365353, 0.0005872075562365353, 0.0005872075562365353, 0.0005872075562365353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005872075562365353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058721
Iteration 2/1000 | Loss: 0.00002544
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001651
Iteration 5/1000 | Loss: 0.00001547
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001427
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001328
Iteration 11/1000 | Loss: 0.00001308
Iteration 12/1000 | Loss: 0.00001293
Iteration 13/1000 | Loss: 0.00001292
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001280
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001271
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001238
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001233
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001228
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001225
Iteration 54/1000 | Loss: 0.00001225
Iteration 55/1000 | Loss: 0.00001225
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001219
Iteration 87/1000 | Loss: 0.00001219
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001219
Iteration 90/1000 | Loss: 0.00001219
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001218
Iteration 93/1000 | Loss: 0.00001218
Iteration 94/1000 | Loss: 0.00001218
Iteration 95/1000 | Loss: 0.00001218
Iteration 96/1000 | Loss: 0.00001218
Iteration 97/1000 | Loss: 0.00001218
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001218
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001217
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001217
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001217
Iteration 109/1000 | Loss: 0.00001217
Iteration 110/1000 | Loss: 0.00001217
Iteration 111/1000 | Loss: 0.00001217
Iteration 112/1000 | Loss: 0.00001217
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001216
Iteration 122/1000 | Loss: 0.00001216
Iteration 123/1000 | Loss: 0.00001216
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001216
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001216
Iteration 132/1000 | Loss: 0.00001216
Iteration 133/1000 | Loss: 0.00001216
Iteration 134/1000 | Loss: 0.00001216
Iteration 135/1000 | Loss: 0.00001216
Iteration 136/1000 | Loss: 0.00001216
Iteration 137/1000 | Loss: 0.00001216
Iteration 138/1000 | Loss: 0.00001216
Iteration 139/1000 | Loss: 0.00001216
Iteration 140/1000 | Loss: 0.00001216
Iteration 141/1000 | Loss: 0.00001216
Iteration 142/1000 | Loss: 0.00001216
Iteration 143/1000 | Loss: 0.00001216
Iteration 144/1000 | Loss: 0.00001216
Iteration 145/1000 | Loss: 0.00001216
Iteration 146/1000 | Loss: 0.00001216
Iteration 147/1000 | Loss: 0.00001216
Iteration 148/1000 | Loss: 0.00001216
Iteration 149/1000 | Loss: 0.00001216
Iteration 150/1000 | Loss: 0.00001216
Iteration 151/1000 | Loss: 0.00001216
Iteration 152/1000 | Loss: 0.00001216
Iteration 153/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.216389864566736e-05, 1.216389864566736e-05, 1.216389864566736e-05, 1.216389864566736e-05, 1.216389864566736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.216389864566736e-05

Optimization complete. Final v2v error: 2.92881178855896 mm

Highest mean error: 3.4361414909362793 mm for frame 157

Lowest mean error: 2.343184471130371 mm for frame 5

Saving results

Total time: 42.89492678642273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00334867
Iteration 2/25 | Loss: 0.00135585
Iteration 3/25 | Loss: 0.00110310
Iteration 4/25 | Loss: 0.00101817
Iteration 5/25 | Loss: 0.00100244
Iteration 6/25 | Loss: 0.00099372
Iteration 7/25 | Loss: 0.00099224
Iteration 8/25 | Loss: 0.00099167
Iteration 9/25 | Loss: 0.00099149
Iteration 10/25 | Loss: 0.00099141
Iteration 11/25 | Loss: 0.00099140
Iteration 12/25 | Loss: 0.00099140
Iteration 13/25 | Loss: 0.00099140
Iteration 14/25 | Loss: 0.00099140
Iteration 15/25 | Loss: 0.00099140
Iteration 16/25 | Loss: 0.00099139
Iteration 17/25 | Loss: 0.00099139
Iteration 18/25 | Loss: 0.00099139
Iteration 19/25 | Loss: 0.00099139
Iteration 20/25 | Loss: 0.00099139
Iteration 21/25 | Loss: 0.00099139
Iteration 22/25 | Loss: 0.00099138
Iteration 23/25 | Loss: 0.00099138
Iteration 24/25 | Loss: 0.00099138
Iteration 25/25 | Loss: 0.00099138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37917960
Iteration 2/25 | Loss: 0.00086510
Iteration 3/25 | Loss: 0.00086510
Iteration 4/25 | Loss: 0.00086510
Iteration 5/25 | Loss: 0.00086510
Iteration 6/25 | Loss: 0.00086510
Iteration 7/25 | Loss: 0.00086510
Iteration 8/25 | Loss: 0.00086510
Iteration 9/25 | Loss: 0.00086510
Iteration 10/25 | Loss: 0.00086510
Iteration 11/25 | Loss: 0.00086510
Iteration 12/25 | Loss: 0.00086510
Iteration 13/25 | Loss: 0.00086510
Iteration 14/25 | Loss: 0.00086510
Iteration 15/25 | Loss: 0.00086510
Iteration 16/25 | Loss: 0.00086510
Iteration 17/25 | Loss: 0.00086510
Iteration 18/25 | Loss: 0.00086510
Iteration 19/25 | Loss: 0.00086510
Iteration 20/25 | Loss: 0.00086510
Iteration 21/25 | Loss: 0.00086510
Iteration 22/25 | Loss: 0.00086510
Iteration 23/25 | Loss: 0.00086510
Iteration 24/25 | Loss: 0.00086510
Iteration 25/25 | Loss: 0.00086510

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086510
Iteration 2/1000 | Loss: 0.00004937
Iteration 3/1000 | Loss: 0.00002469
Iteration 4/1000 | Loss: 0.00001779
Iteration 5/1000 | Loss: 0.00001609
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001352
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001297
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001264
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001263
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001261
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001251
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001247
Iteration 44/1000 | Loss: 0.00001246
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001243
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001243
Iteration 61/1000 | Loss: 0.00001243
Iteration 62/1000 | Loss: 0.00001243
Iteration 63/1000 | Loss: 0.00001242
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001242
Iteration 66/1000 | Loss: 0.00001242
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001241
Iteration 72/1000 | Loss: 0.00001241
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001241
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001240
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001238
Iteration 101/1000 | Loss: 0.00001238
Iteration 102/1000 | Loss: 0.00001238
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001237
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001237
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001236
Iteration 117/1000 | Loss: 0.00001236
Iteration 118/1000 | Loss: 0.00001236
Iteration 119/1000 | Loss: 0.00001236
Iteration 120/1000 | Loss: 0.00001236
Iteration 121/1000 | Loss: 0.00001236
Iteration 122/1000 | Loss: 0.00001236
Iteration 123/1000 | Loss: 0.00001236
Iteration 124/1000 | Loss: 0.00001236
Iteration 125/1000 | Loss: 0.00001236
Iteration 126/1000 | Loss: 0.00001236
Iteration 127/1000 | Loss: 0.00001236
Iteration 128/1000 | Loss: 0.00001236
Iteration 129/1000 | Loss: 0.00001236
Iteration 130/1000 | Loss: 0.00001236
Iteration 131/1000 | Loss: 0.00001236
Iteration 132/1000 | Loss: 0.00001236
Iteration 133/1000 | Loss: 0.00001236
Iteration 134/1000 | Loss: 0.00001236
Iteration 135/1000 | Loss: 0.00001236
Iteration 136/1000 | Loss: 0.00001236
Iteration 137/1000 | Loss: 0.00001236
Iteration 138/1000 | Loss: 0.00001236
Iteration 139/1000 | Loss: 0.00001236
Iteration 140/1000 | Loss: 0.00001236
Iteration 141/1000 | Loss: 0.00001236
Iteration 142/1000 | Loss: 0.00001236
Iteration 143/1000 | Loss: 0.00001236
Iteration 144/1000 | Loss: 0.00001236
Iteration 145/1000 | Loss: 0.00001236
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001236
Iteration 149/1000 | Loss: 0.00001236
Iteration 150/1000 | Loss: 0.00001236
Iteration 151/1000 | Loss: 0.00001236
Iteration 152/1000 | Loss: 0.00001236
Iteration 153/1000 | Loss: 0.00001236
Iteration 154/1000 | Loss: 0.00001236
Iteration 155/1000 | Loss: 0.00001236
Iteration 156/1000 | Loss: 0.00001236
Iteration 157/1000 | Loss: 0.00001236
Iteration 158/1000 | Loss: 0.00001236
Iteration 159/1000 | Loss: 0.00001236
Iteration 160/1000 | Loss: 0.00001236
Iteration 161/1000 | Loss: 0.00001236
Iteration 162/1000 | Loss: 0.00001236
Iteration 163/1000 | Loss: 0.00001236
Iteration 164/1000 | Loss: 0.00001236
Iteration 165/1000 | Loss: 0.00001236
Iteration 166/1000 | Loss: 0.00001236
Iteration 167/1000 | Loss: 0.00001236
Iteration 168/1000 | Loss: 0.00001236
Iteration 169/1000 | Loss: 0.00001236
Iteration 170/1000 | Loss: 0.00001236
Iteration 171/1000 | Loss: 0.00001236
Iteration 172/1000 | Loss: 0.00001236
Iteration 173/1000 | Loss: 0.00001236
Iteration 174/1000 | Loss: 0.00001236
Iteration 175/1000 | Loss: 0.00001236
Iteration 176/1000 | Loss: 0.00001236
Iteration 177/1000 | Loss: 0.00001236
Iteration 178/1000 | Loss: 0.00001236
Iteration 179/1000 | Loss: 0.00001236
Iteration 180/1000 | Loss: 0.00001236
Iteration 181/1000 | Loss: 0.00001236
Iteration 182/1000 | Loss: 0.00001236
Iteration 183/1000 | Loss: 0.00001236
Iteration 184/1000 | Loss: 0.00001236
Iteration 185/1000 | Loss: 0.00001236
Iteration 186/1000 | Loss: 0.00001236
Iteration 187/1000 | Loss: 0.00001236
Iteration 188/1000 | Loss: 0.00001236
Iteration 189/1000 | Loss: 0.00001236
Iteration 190/1000 | Loss: 0.00001236
Iteration 191/1000 | Loss: 0.00001236
Iteration 192/1000 | Loss: 0.00001236
Iteration 193/1000 | Loss: 0.00001236
Iteration 194/1000 | Loss: 0.00001236
Iteration 195/1000 | Loss: 0.00001236
Iteration 196/1000 | Loss: 0.00001236
Iteration 197/1000 | Loss: 0.00001236
Iteration 198/1000 | Loss: 0.00001236
Iteration 199/1000 | Loss: 0.00001236
Iteration 200/1000 | Loss: 0.00001236
Iteration 201/1000 | Loss: 0.00001236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.2357597370282747e-05, 1.2357597370282747e-05, 1.2357597370282747e-05, 1.2357597370282747e-05, 1.2357597370282747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2357597370282747e-05

Optimization complete. Final v2v error: 2.9673678874969482 mm

Highest mean error: 3.80641508102417 mm for frame 70

Lowest mean error: 2.4373879432678223 mm for frame 151

Saving results

Total time: 45.17837882041931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470916
Iteration 2/25 | Loss: 0.00105876
Iteration 3/25 | Loss: 0.00098658
Iteration 4/25 | Loss: 0.00097821
Iteration 5/25 | Loss: 0.00097601
Iteration 6/25 | Loss: 0.00097542
Iteration 7/25 | Loss: 0.00097542
Iteration 8/25 | Loss: 0.00097542
Iteration 9/25 | Loss: 0.00097542
Iteration 10/25 | Loss: 0.00097542
Iteration 11/25 | Loss: 0.00097542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009754158090800047, 0.0009754158090800047, 0.0009754158090800047, 0.0009754158090800047, 0.0009754158090800047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009754158090800047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12207031
Iteration 2/25 | Loss: 0.00069652
Iteration 3/25 | Loss: 0.00069652
Iteration 4/25 | Loss: 0.00069652
Iteration 5/25 | Loss: 0.00069652
Iteration 6/25 | Loss: 0.00069651
Iteration 7/25 | Loss: 0.00069651
Iteration 8/25 | Loss: 0.00069651
Iteration 9/25 | Loss: 0.00069651
Iteration 10/25 | Loss: 0.00069651
Iteration 11/25 | Loss: 0.00069651
Iteration 12/25 | Loss: 0.00069651
Iteration 13/25 | Loss: 0.00069651
Iteration 14/25 | Loss: 0.00069651
Iteration 15/25 | Loss: 0.00069651
Iteration 16/25 | Loss: 0.00069651
Iteration 17/25 | Loss: 0.00069651
Iteration 18/25 | Loss: 0.00069651
Iteration 19/25 | Loss: 0.00069651
Iteration 20/25 | Loss: 0.00069651
Iteration 21/25 | Loss: 0.00069651
Iteration 22/25 | Loss: 0.00069651
Iteration 23/25 | Loss: 0.00069651
Iteration 24/25 | Loss: 0.00069651
Iteration 25/25 | Loss: 0.00069651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069651
Iteration 2/1000 | Loss: 0.00001797
Iteration 3/1000 | Loss: 0.00001267
Iteration 4/1000 | Loss: 0.00001169
Iteration 5/1000 | Loss: 0.00001106
Iteration 6/1000 | Loss: 0.00001051
Iteration 7/1000 | Loss: 0.00001003
Iteration 8/1000 | Loss: 0.00000984
Iteration 9/1000 | Loss: 0.00000982
Iteration 10/1000 | Loss: 0.00000974
Iteration 11/1000 | Loss: 0.00000969
Iteration 12/1000 | Loss: 0.00000950
Iteration 13/1000 | Loss: 0.00000948
Iteration 14/1000 | Loss: 0.00000939
Iteration 15/1000 | Loss: 0.00000939
Iteration 16/1000 | Loss: 0.00000937
Iteration 17/1000 | Loss: 0.00000936
Iteration 18/1000 | Loss: 0.00000935
Iteration 19/1000 | Loss: 0.00000934
Iteration 20/1000 | Loss: 0.00000932
Iteration 21/1000 | Loss: 0.00000932
Iteration 22/1000 | Loss: 0.00000931
Iteration 23/1000 | Loss: 0.00000930
Iteration 24/1000 | Loss: 0.00000930
Iteration 25/1000 | Loss: 0.00000928
Iteration 26/1000 | Loss: 0.00000927
Iteration 27/1000 | Loss: 0.00000927
Iteration 28/1000 | Loss: 0.00000926
Iteration 29/1000 | Loss: 0.00000925
Iteration 30/1000 | Loss: 0.00000922
Iteration 31/1000 | Loss: 0.00000921
Iteration 32/1000 | Loss: 0.00000921
Iteration 33/1000 | Loss: 0.00000920
Iteration 34/1000 | Loss: 0.00000919
Iteration 35/1000 | Loss: 0.00000918
Iteration 36/1000 | Loss: 0.00000917
Iteration 37/1000 | Loss: 0.00000917
Iteration 38/1000 | Loss: 0.00000917
Iteration 39/1000 | Loss: 0.00000917
Iteration 40/1000 | Loss: 0.00000916
Iteration 41/1000 | Loss: 0.00000916
Iteration 42/1000 | Loss: 0.00000916
Iteration 43/1000 | Loss: 0.00000916
Iteration 44/1000 | Loss: 0.00000915
Iteration 45/1000 | Loss: 0.00000915
Iteration 46/1000 | Loss: 0.00000915
Iteration 47/1000 | Loss: 0.00000914
Iteration 48/1000 | Loss: 0.00000914
Iteration 49/1000 | Loss: 0.00000914
Iteration 50/1000 | Loss: 0.00000914
Iteration 51/1000 | Loss: 0.00000914
Iteration 52/1000 | Loss: 0.00000914
Iteration 53/1000 | Loss: 0.00000914
Iteration 54/1000 | Loss: 0.00000913
Iteration 55/1000 | Loss: 0.00000913
Iteration 56/1000 | Loss: 0.00000913
Iteration 57/1000 | Loss: 0.00000913
Iteration 58/1000 | Loss: 0.00000913
Iteration 59/1000 | Loss: 0.00000913
Iteration 60/1000 | Loss: 0.00000912
Iteration 61/1000 | Loss: 0.00000912
Iteration 62/1000 | Loss: 0.00000912
Iteration 63/1000 | Loss: 0.00000912
Iteration 64/1000 | Loss: 0.00000912
Iteration 65/1000 | Loss: 0.00000911
Iteration 66/1000 | Loss: 0.00000911
Iteration 67/1000 | Loss: 0.00000911
Iteration 68/1000 | Loss: 0.00000911
Iteration 69/1000 | Loss: 0.00000910
Iteration 70/1000 | Loss: 0.00000910
Iteration 71/1000 | Loss: 0.00000910
Iteration 72/1000 | Loss: 0.00000909
Iteration 73/1000 | Loss: 0.00000909
Iteration 74/1000 | Loss: 0.00000909
Iteration 75/1000 | Loss: 0.00000909
Iteration 76/1000 | Loss: 0.00000908
Iteration 77/1000 | Loss: 0.00000908
Iteration 78/1000 | Loss: 0.00000907
Iteration 79/1000 | Loss: 0.00000907
Iteration 80/1000 | Loss: 0.00000907
Iteration 81/1000 | Loss: 0.00000907
Iteration 82/1000 | Loss: 0.00000907
Iteration 83/1000 | Loss: 0.00000907
Iteration 84/1000 | Loss: 0.00000907
Iteration 85/1000 | Loss: 0.00000907
Iteration 86/1000 | Loss: 0.00000906
Iteration 87/1000 | Loss: 0.00000906
Iteration 88/1000 | Loss: 0.00000906
Iteration 89/1000 | Loss: 0.00000906
Iteration 90/1000 | Loss: 0.00000906
Iteration 91/1000 | Loss: 0.00000906
Iteration 92/1000 | Loss: 0.00000906
Iteration 93/1000 | Loss: 0.00000906
Iteration 94/1000 | Loss: 0.00000906
Iteration 95/1000 | Loss: 0.00000906
Iteration 96/1000 | Loss: 0.00000905
Iteration 97/1000 | Loss: 0.00000905
Iteration 98/1000 | Loss: 0.00000905
Iteration 99/1000 | Loss: 0.00000904
Iteration 100/1000 | Loss: 0.00000904
Iteration 101/1000 | Loss: 0.00000904
Iteration 102/1000 | Loss: 0.00000903
Iteration 103/1000 | Loss: 0.00000903
Iteration 104/1000 | Loss: 0.00000903
Iteration 105/1000 | Loss: 0.00000903
Iteration 106/1000 | Loss: 0.00000903
Iteration 107/1000 | Loss: 0.00000903
Iteration 108/1000 | Loss: 0.00000903
Iteration 109/1000 | Loss: 0.00000903
Iteration 110/1000 | Loss: 0.00000903
Iteration 111/1000 | Loss: 0.00000903
Iteration 112/1000 | Loss: 0.00000903
Iteration 113/1000 | Loss: 0.00000903
Iteration 114/1000 | Loss: 0.00000902
Iteration 115/1000 | Loss: 0.00000902
Iteration 116/1000 | Loss: 0.00000902
Iteration 117/1000 | Loss: 0.00000901
Iteration 118/1000 | Loss: 0.00000901
Iteration 119/1000 | Loss: 0.00000901
Iteration 120/1000 | Loss: 0.00000901
Iteration 121/1000 | Loss: 0.00000901
Iteration 122/1000 | Loss: 0.00000901
Iteration 123/1000 | Loss: 0.00000901
Iteration 124/1000 | Loss: 0.00000901
Iteration 125/1000 | Loss: 0.00000901
Iteration 126/1000 | Loss: 0.00000901
Iteration 127/1000 | Loss: 0.00000900
Iteration 128/1000 | Loss: 0.00000900
Iteration 129/1000 | Loss: 0.00000899
Iteration 130/1000 | Loss: 0.00000899
Iteration 131/1000 | Loss: 0.00000899
Iteration 132/1000 | Loss: 0.00000899
Iteration 133/1000 | Loss: 0.00000899
Iteration 134/1000 | Loss: 0.00000899
Iteration 135/1000 | Loss: 0.00000898
Iteration 136/1000 | Loss: 0.00000898
Iteration 137/1000 | Loss: 0.00000898
Iteration 138/1000 | Loss: 0.00000898
Iteration 139/1000 | Loss: 0.00000898
Iteration 140/1000 | Loss: 0.00000898
Iteration 141/1000 | Loss: 0.00000898
Iteration 142/1000 | Loss: 0.00000898
Iteration 143/1000 | Loss: 0.00000898
Iteration 144/1000 | Loss: 0.00000898
Iteration 145/1000 | Loss: 0.00000898
Iteration 146/1000 | Loss: 0.00000898
Iteration 147/1000 | Loss: 0.00000897
Iteration 148/1000 | Loss: 0.00000897
Iteration 149/1000 | Loss: 0.00000897
Iteration 150/1000 | Loss: 0.00000897
Iteration 151/1000 | Loss: 0.00000897
Iteration 152/1000 | Loss: 0.00000897
Iteration 153/1000 | Loss: 0.00000897
Iteration 154/1000 | Loss: 0.00000897
Iteration 155/1000 | Loss: 0.00000897
Iteration 156/1000 | Loss: 0.00000897
Iteration 157/1000 | Loss: 0.00000897
Iteration 158/1000 | Loss: 0.00000896
Iteration 159/1000 | Loss: 0.00000896
Iteration 160/1000 | Loss: 0.00000896
Iteration 161/1000 | Loss: 0.00000896
Iteration 162/1000 | Loss: 0.00000896
Iteration 163/1000 | Loss: 0.00000895
Iteration 164/1000 | Loss: 0.00000895
Iteration 165/1000 | Loss: 0.00000895
Iteration 166/1000 | Loss: 0.00000894
Iteration 167/1000 | Loss: 0.00000894
Iteration 168/1000 | Loss: 0.00000894
Iteration 169/1000 | Loss: 0.00000893
Iteration 170/1000 | Loss: 0.00000893
Iteration 171/1000 | Loss: 0.00000893
Iteration 172/1000 | Loss: 0.00000893
Iteration 173/1000 | Loss: 0.00000892
Iteration 174/1000 | Loss: 0.00000892
Iteration 175/1000 | Loss: 0.00000892
Iteration 176/1000 | Loss: 0.00000891
Iteration 177/1000 | Loss: 0.00000891
Iteration 178/1000 | Loss: 0.00000891
Iteration 179/1000 | Loss: 0.00000891
Iteration 180/1000 | Loss: 0.00000891
Iteration 181/1000 | Loss: 0.00000891
Iteration 182/1000 | Loss: 0.00000891
Iteration 183/1000 | Loss: 0.00000890
Iteration 184/1000 | Loss: 0.00000890
Iteration 185/1000 | Loss: 0.00000890
Iteration 186/1000 | Loss: 0.00000890
Iteration 187/1000 | Loss: 0.00000890
Iteration 188/1000 | Loss: 0.00000890
Iteration 189/1000 | Loss: 0.00000890
Iteration 190/1000 | Loss: 0.00000890
Iteration 191/1000 | Loss: 0.00000890
Iteration 192/1000 | Loss: 0.00000890
Iteration 193/1000 | Loss: 0.00000890
Iteration 194/1000 | Loss: 0.00000890
Iteration 195/1000 | Loss: 0.00000890
Iteration 196/1000 | Loss: 0.00000889
Iteration 197/1000 | Loss: 0.00000889
Iteration 198/1000 | Loss: 0.00000889
Iteration 199/1000 | Loss: 0.00000889
Iteration 200/1000 | Loss: 0.00000889
Iteration 201/1000 | Loss: 0.00000889
Iteration 202/1000 | Loss: 0.00000889
Iteration 203/1000 | Loss: 0.00000889
Iteration 204/1000 | Loss: 0.00000889
Iteration 205/1000 | Loss: 0.00000889
Iteration 206/1000 | Loss: 0.00000889
Iteration 207/1000 | Loss: 0.00000889
Iteration 208/1000 | Loss: 0.00000889
Iteration 209/1000 | Loss: 0.00000889
Iteration 210/1000 | Loss: 0.00000889
Iteration 211/1000 | Loss: 0.00000889
Iteration 212/1000 | Loss: 0.00000888
Iteration 213/1000 | Loss: 0.00000888
Iteration 214/1000 | Loss: 0.00000888
Iteration 215/1000 | Loss: 0.00000888
Iteration 216/1000 | Loss: 0.00000888
Iteration 217/1000 | Loss: 0.00000888
Iteration 218/1000 | Loss: 0.00000888
Iteration 219/1000 | Loss: 0.00000887
Iteration 220/1000 | Loss: 0.00000887
Iteration 221/1000 | Loss: 0.00000887
Iteration 222/1000 | Loss: 0.00000887
Iteration 223/1000 | Loss: 0.00000887
Iteration 224/1000 | Loss: 0.00000887
Iteration 225/1000 | Loss: 0.00000887
Iteration 226/1000 | Loss: 0.00000887
Iteration 227/1000 | Loss: 0.00000887
Iteration 228/1000 | Loss: 0.00000887
Iteration 229/1000 | Loss: 0.00000887
Iteration 230/1000 | Loss: 0.00000887
Iteration 231/1000 | Loss: 0.00000887
Iteration 232/1000 | Loss: 0.00000887
Iteration 233/1000 | Loss: 0.00000887
Iteration 234/1000 | Loss: 0.00000887
Iteration 235/1000 | Loss: 0.00000887
Iteration 236/1000 | Loss: 0.00000887
Iteration 237/1000 | Loss: 0.00000887
Iteration 238/1000 | Loss: 0.00000887
Iteration 239/1000 | Loss: 0.00000887
Iteration 240/1000 | Loss: 0.00000887
Iteration 241/1000 | Loss: 0.00000887
Iteration 242/1000 | Loss: 0.00000887
Iteration 243/1000 | Loss: 0.00000887
Iteration 244/1000 | Loss: 0.00000887
Iteration 245/1000 | Loss: 0.00000887
Iteration 246/1000 | Loss: 0.00000887
Iteration 247/1000 | Loss: 0.00000887
Iteration 248/1000 | Loss: 0.00000887
Iteration 249/1000 | Loss: 0.00000887
Iteration 250/1000 | Loss: 0.00000887
Iteration 251/1000 | Loss: 0.00000887
Iteration 252/1000 | Loss: 0.00000887
Iteration 253/1000 | Loss: 0.00000887
Iteration 254/1000 | Loss: 0.00000887
Iteration 255/1000 | Loss: 0.00000887
Iteration 256/1000 | Loss: 0.00000887
Iteration 257/1000 | Loss: 0.00000887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [8.866155440045986e-06, 8.866155440045986e-06, 8.866155440045986e-06, 8.866155440045986e-06, 8.866155440045986e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.866155440045986e-06

Optimization complete. Final v2v error: 2.5662317276000977 mm

Highest mean error: 2.9850919246673584 mm for frame 66

Lowest mean error: 2.333256483078003 mm for frame 126

Saving results

Total time: 39.684179067611694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383781
Iteration 2/25 | Loss: 0.00108141
Iteration 3/25 | Loss: 0.00098505
Iteration 4/25 | Loss: 0.00097755
Iteration 5/25 | Loss: 0.00097556
Iteration 6/25 | Loss: 0.00097556
Iteration 7/25 | Loss: 0.00097556
Iteration 8/25 | Loss: 0.00097556
Iteration 9/25 | Loss: 0.00097556
Iteration 10/25 | Loss: 0.00097556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009755606879480183, 0.0009755606879480183, 0.0009755606879480183, 0.0009755606879480183, 0.0009755606879480183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009755606879480183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36507773
Iteration 2/25 | Loss: 0.00057834
Iteration 3/25 | Loss: 0.00057834
Iteration 4/25 | Loss: 0.00057834
Iteration 5/25 | Loss: 0.00057834
Iteration 6/25 | Loss: 0.00057834
Iteration 7/25 | Loss: 0.00057834
Iteration 8/25 | Loss: 0.00057834
Iteration 9/25 | Loss: 0.00057834
Iteration 10/25 | Loss: 0.00057834
Iteration 11/25 | Loss: 0.00057833
Iteration 12/25 | Loss: 0.00057833
Iteration 13/25 | Loss: 0.00057833
Iteration 14/25 | Loss: 0.00057833
Iteration 15/25 | Loss: 0.00057833
Iteration 16/25 | Loss: 0.00057833
Iteration 17/25 | Loss: 0.00057833
Iteration 18/25 | Loss: 0.00057833
Iteration 19/25 | Loss: 0.00057833
Iteration 20/25 | Loss: 0.00057833
Iteration 21/25 | Loss: 0.00057833
Iteration 22/25 | Loss: 0.00057833
Iteration 23/25 | Loss: 0.00057833
Iteration 24/25 | Loss: 0.00057833
Iteration 25/25 | Loss: 0.00057833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057833
Iteration 2/1000 | Loss: 0.00002418
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001628
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001430
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001309
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001250
Iteration 13/1000 | Loss: 0.00001232
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001209
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001207
Iteration 21/1000 | Loss: 0.00001206
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001205
Iteration 24/1000 | Loss: 0.00001203
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001195
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001193
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001188
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001187
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001184
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001179
Iteration 64/1000 | Loss: 0.00001178
Iteration 65/1000 | Loss: 0.00001178
Iteration 66/1000 | Loss: 0.00001178
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001177
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001177
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001166
Iteration 108/1000 | Loss: 0.00001166
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001166
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001165
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001165
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001165
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001162
Iteration 134/1000 | Loss: 0.00001162
Iteration 135/1000 | Loss: 0.00001162
Iteration 136/1000 | Loss: 0.00001161
Iteration 137/1000 | Loss: 0.00001161
Iteration 138/1000 | Loss: 0.00001161
Iteration 139/1000 | Loss: 0.00001161
Iteration 140/1000 | Loss: 0.00001161
Iteration 141/1000 | Loss: 0.00001161
Iteration 142/1000 | Loss: 0.00001161
Iteration 143/1000 | Loss: 0.00001161
Iteration 144/1000 | Loss: 0.00001161
Iteration 145/1000 | Loss: 0.00001160
Iteration 146/1000 | Loss: 0.00001160
Iteration 147/1000 | Loss: 0.00001160
Iteration 148/1000 | Loss: 0.00001160
Iteration 149/1000 | Loss: 0.00001160
Iteration 150/1000 | Loss: 0.00001160
Iteration 151/1000 | Loss: 0.00001160
Iteration 152/1000 | Loss: 0.00001160
Iteration 153/1000 | Loss: 0.00001160
Iteration 154/1000 | Loss: 0.00001160
Iteration 155/1000 | Loss: 0.00001160
Iteration 156/1000 | Loss: 0.00001160
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001160
Iteration 165/1000 | Loss: 0.00001160
Iteration 166/1000 | Loss: 0.00001160
Iteration 167/1000 | Loss: 0.00001160
Iteration 168/1000 | Loss: 0.00001160
Iteration 169/1000 | Loss: 0.00001160
Iteration 170/1000 | Loss: 0.00001160
Iteration 171/1000 | Loss: 0.00001160
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001160
Iteration 177/1000 | Loss: 0.00001160
Iteration 178/1000 | Loss: 0.00001160
Iteration 179/1000 | Loss: 0.00001160
Iteration 180/1000 | Loss: 0.00001160
Iteration 181/1000 | Loss: 0.00001160
Iteration 182/1000 | Loss: 0.00001160
Iteration 183/1000 | Loss: 0.00001160
Iteration 184/1000 | Loss: 0.00001160
Iteration 185/1000 | Loss: 0.00001160
Iteration 186/1000 | Loss: 0.00001160
Iteration 187/1000 | Loss: 0.00001160
Iteration 188/1000 | Loss: 0.00001160
Iteration 189/1000 | Loss: 0.00001160
Iteration 190/1000 | Loss: 0.00001160
Iteration 191/1000 | Loss: 0.00001160
Iteration 192/1000 | Loss: 0.00001160
Iteration 193/1000 | Loss: 0.00001160
Iteration 194/1000 | Loss: 0.00001160
Iteration 195/1000 | Loss: 0.00001160
Iteration 196/1000 | Loss: 0.00001160
Iteration 197/1000 | Loss: 0.00001160
Iteration 198/1000 | Loss: 0.00001160
Iteration 199/1000 | Loss: 0.00001160
Iteration 200/1000 | Loss: 0.00001160
Iteration 201/1000 | Loss: 0.00001160
Iteration 202/1000 | Loss: 0.00001160
Iteration 203/1000 | Loss: 0.00001160
Iteration 204/1000 | Loss: 0.00001160
Iteration 205/1000 | Loss: 0.00001160
Iteration 206/1000 | Loss: 0.00001160
Iteration 207/1000 | Loss: 0.00001160
Iteration 208/1000 | Loss: 0.00001160
Iteration 209/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.1604845894908067e-05, 1.1604845894908067e-05, 1.1604845894908067e-05, 1.1604845894908067e-05, 1.1604845894908067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1604845894908067e-05

Optimization complete. Final v2v error: 2.8206777572631836 mm

Highest mean error: 3.318039655685425 mm for frame 100

Lowest mean error: 2.388387680053711 mm for frame 25

Saving results

Total time: 39.371734619140625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803815
Iteration 2/25 | Loss: 0.00114324
Iteration 3/25 | Loss: 0.00104153
Iteration 4/25 | Loss: 0.00102611
Iteration 5/25 | Loss: 0.00102237
Iteration 6/25 | Loss: 0.00102177
Iteration 7/25 | Loss: 0.00102177
Iteration 8/25 | Loss: 0.00102177
Iteration 9/25 | Loss: 0.00102177
Iteration 10/25 | Loss: 0.00102177
Iteration 11/25 | Loss: 0.00102177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010217714589089155, 0.0010217714589089155, 0.0010217714589089155, 0.0010217714589089155, 0.0010217714589089155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010217714589089155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42286766
Iteration 2/25 | Loss: 0.00068470
Iteration 3/25 | Loss: 0.00068470
Iteration 4/25 | Loss: 0.00068470
Iteration 5/25 | Loss: 0.00068470
Iteration 6/25 | Loss: 0.00068470
Iteration 7/25 | Loss: 0.00068470
Iteration 8/25 | Loss: 0.00068470
Iteration 9/25 | Loss: 0.00068470
Iteration 10/25 | Loss: 0.00068470
Iteration 11/25 | Loss: 0.00068470
Iteration 12/25 | Loss: 0.00068470
Iteration 13/25 | Loss: 0.00068470
Iteration 14/25 | Loss: 0.00068470
Iteration 15/25 | Loss: 0.00068470
Iteration 16/25 | Loss: 0.00068470
Iteration 17/25 | Loss: 0.00068470
Iteration 18/25 | Loss: 0.00068470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006846992182545364, 0.0006846992182545364, 0.0006846992182545364, 0.0006846992182545364, 0.0006846992182545364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006846992182545364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068470
Iteration 2/1000 | Loss: 0.00002904
Iteration 3/1000 | Loss: 0.00001776
Iteration 4/1000 | Loss: 0.00001483
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001341
Iteration 7/1000 | Loss: 0.00001299
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001222
Iteration 11/1000 | Loss: 0.00001221
Iteration 12/1000 | Loss: 0.00001212
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001201
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001190
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001174
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001173
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001170
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001169
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001168
Iteration 74/1000 | Loss: 0.00001168
Iteration 75/1000 | Loss: 0.00001168
Iteration 76/1000 | Loss: 0.00001168
Iteration 77/1000 | Loss: 0.00001168
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001167
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001167
Iteration 83/1000 | Loss: 0.00001167
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001167
Iteration 86/1000 | Loss: 0.00001167
Iteration 87/1000 | Loss: 0.00001167
Iteration 88/1000 | Loss: 0.00001167
Iteration 89/1000 | Loss: 0.00001166
Iteration 90/1000 | Loss: 0.00001166
Iteration 91/1000 | Loss: 0.00001166
Iteration 92/1000 | Loss: 0.00001166
Iteration 93/1000 | Loss: 0.00001165
Iteration 94/1000 | Loss: 0.00001165
Iteration 95/1000 | Loss: 0.00001165
Iteration 96/1000 | Loss: 0.00001165
Iteration 97/1000 | Loss: 0.00001165
Iteration 98/1000 | Loss: 0.00001164
Iteration 99/1000 | Loss: 0.00001164
Iteration 100/1000 | Loss: 0.00001164
Iteration 101/1000 | Loss: 0.00001164
Iteration 102/1000 | Loss: 0.00001164
Iteration 103/1000 | Loss: 0.00001164
Iteration 104/1000 | Loss: 0.00001164
Iteration 105/1000 | Loss: 0.00001164
Iteration 106/1000 | Loss: 0.00001163
Iteration 107/1000 | Loss: 0.00001163
Iteration 108/1000 | Loss: 0.00001163
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001162
Iteration 111/1000 | Loss: 0.00001162
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001161
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001161
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001161
Iteration 118/1000 | Loss: 0.00001161
Iteration 119/1000 | Loss: 0.00001161
Iteration 120/1000 | Loss: 0.00001161
Iteration 121/1000 | Loss: 0.00001161
Iteration 122/1000 | Loss: 0.00001161
Iteration 123/1000 | Loss: 0.00001161
Iteration 124/1000 | Loss: 0.00001161
Iteration 125/1000 | Loss: 0.00001161
Iteration 126/1000 | Loss: 0.00001160
Iteration 127/1000 | Loss: 0.00001160
Iteration 128/1000 | Loss: 0.00001160
Iteration 129/1000 | Loss: 0.00001160
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001159
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001158
Iteration 136/1000 | Loss: 0.00001158
Iteration 137/1000 | Loss: 0.00001158
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001157
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001155
Iteration 156/1000 | Loss: 0.00001155
Iteration 157/1000 | Loss: 0.00001155
Iteration 158/1000 | Loss: 0.00001155
Iteration 159/1000 | Loss: 0.00001155
Iteration 160/1000 | Loss: 0.00001155
Iteration 161/1000 | Loss: 0.00001155
Iteration 162/1000 | Loss: 0.00001155
Iteration 163/1000 | Loss: 0.00001155
Iteration 164/1000 | Loss: 0.00001155
Iteration 165/1000 | Loss: 0.00001155
Iteration 166/1000 | Loss: 0.00001155
Iteration 167/1000 | Loss: 0.00001155
Iteration 168/1000 | Loss: 0.00001155
Iteration 169/1000 | Loss: 0.00001155
Iteration 170/1000 | Loss: 0.00001155
Iteration 171/1000 | Loss: 0.00001155
Iteration 172/1000 | Loss: 0.00001155
Iteration 173/1000 | Loss: 0.00001155
Iteration 174/1000 | Loss: 0.00001155
Iteration 175/1000 | Loss: 0.00001155
Iteration 176/1000 | Loss: 0.00001155
Iteration 177/1000 | Loss: 0.00001155
Iteration 178/1000 | Loss: 0.00001155
Iteration 179/1000 | Loss: 0.00001155
Iteration 180/1000 | Loss: 0.00001155
Iteration 181/1000 | Loss: 0.00001155
Iteration 182/1000 | Loss: 0.00001155
Iteration 183/1000 | Loss: 0.00001155
Iteration 184/1000 | Loss: 0.00001155
Iteration 185/1000 | Loss: 0.00001155
Iteration 186/1000 | Loss: 0.00001155
Iteration 187/1000 | Loss: 0.00001155
Iteration 188/1000 | Loss: 0.00001155
Iteration 189/1000 | Loss: 0.00001155
Iteration 190/1000 | Loss: 0.00001155
Iteration 191/1000 | Loss: 0.00001155
Iteration 192/1000 | Loss: 0.00001155
Iteration 193/1000 | Loss: 0.00001155
Iteration 194/1000 | Loss: 0.00001155
Iteration 195/1000 | Loss: 0.00001155
Iteration 196/1000 | Loss: 0.00001155
Iteration 197/1000 | Loss: 0.00001155
Iteration 198/1000 | Loss: 0.00001155
Iteration 199/1000 | Loss: 0.00001155
Iteration 200/1000 | Loss: 0.00001155
Iteration 201/1000 | Loss: 0.00001155
Iteration 202/1000 | Loss: 0.00001155
Iteration 203/1000 | Loss: 0.00001155
Iteration 204/1000 | Loss: 0.00001155
Iteration 205/1000 | Loss: 0.00001155
Iteration 206/1000 | Loss: 0.00001155
Iteration 207/1000 | Loss: 0.00001155
Iteration 208/1000 | Loss: 0.00001155
Iteration 209/1000 | Loss: 0.00001155
Iteration 210/1000 | Loss: 0.00001155
Iteration 211/1000 | Loss: 0.00001155
Iteration 212/1000 | Loss: 0.00001155
Iteration 213/1000 | Loss: 0.00001155
Iteration 214/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.1546348105184734e-05, 1.1546348105184734e-05, 1.1546348105184734e-05, 1.1546348105184734e-05, 1.1546348105184734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1546348105184734e-05

Optimization complete. Final v2v error: 2.9169816970825195 mm

Highest mean error: 3.606525182723999 mm for frame 87

Lowest mean error: 2.739180564880371 mm for frame 43

Saving results

Total time: 36.7661771774292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392895
Iteration 2/25 | Loss: 0.00123078
Iteration 3/25 | Loss: 0.00101264
Iteration 4/25 | Loss: 0.00099437
Iteration 5/25 | Loss: 0.00099188
Iteration 6/25 | Loss: 0.00099133
Iteration 7/25 | Loss: 0.00099133
Iteration 8/25 | Loss: 0.00099133
Iteration 9/25 | Loss: 0.00099133
Iteration 10/25 | Loss: 0.00099133
Iteration 11/25 | Loss: 0.00099133
Iteration 12/25 | Loss: 0.00099133
Iteration 13/25 | Loss: 0.00099133
Iteration 14/25 | Loss: 0.00099133
Iteration 15/25 | Loss: 0.00099133
Iteration 16/25 | Loss: 0.00099133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009913253597915173, 0.0009913253597915173, 0.0009913253597915173, 0.0009913253597915173, 0.0009913253597915173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009913253597915173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37055051
Iteration 2/25 | Loss: 0.00050553
Iteration 3/25 | Loss: 0.00050552
Iteration 4/25 | Loss: 0.00050552
Iteration 5/25 | Loss: 0.00050552
Iteration 6/25 | Loss: 0.00050552
Iteration 7/25 | Loss: 0.00050552
Iteration 8/25 | Loss: 0.00050552
Iteration 9/25 | Loss: 0.00050552
Iteration 10/25 | Loss: 0.00050552
Iteration 11/25 | Loss: 0.00050552
Iteration 12/25 | Loss: 0.00050552
Iteration 13/25 | Loss: 0.00050552
Iteration 14/25 | Loss: 0.00050552
Iteration 15/25 | Loss: 0.00050552
Iteration 16/25 | Loss: 0.00050552
Iteration 17/25 | Loss: 0.00050552
Iteration 18/25 | Loss: 0.00050552
Iteration 19/25 | Loss: 0.00050552
Iteration 20/25 | Loss: 0.00050552
Iteration 21/25 | Loss: 0.00050552
Iteration 22/25 | Loss: 0.00050552
Iteration 23/25 | Loss: 0.00050552
Iteration 24/25 | Loss: 0.00050552
Iteration 25/25 | Loss: 0.00050552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050552
Iteration 2/1000 | Loss: 0.00002329
Iteration 3/1000 | Loss: 0.00001470
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001161
Iteration 7/1000 | Loss: 0.00001117
Iteration 8/1000 | Loss: 0.00001095
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001061
Iteration 11/1000 | Loss: 0.00001057
Iteration 12/1000 | Loss: 0.00001053
Iteration 13/1000 | Loss: 0.00001051
Iteration 14/1000 | Loss: 0.00001050
Iteration 15/1000 | Loss: 0.00001047
Iteration 16/1000 | Loss: 0.00001047
Iteration 17/1000 | Loss: 0.00001046
Iteration 18/1000 | Loss: 0.00001046
Iteration 19/1000 | Loss: 0.00001045
Iteration 20/1000 | Loss: 0.00001043
Iteration 21/1000 | Loss: 0.00001042
Iteration 22/1000 | Loss: 0.00001042
Iteration 23/1000 | Loss: 0.00001041
Iteration 24/1000 | Loss: 0.00001040
Iteration 25/1000 | Loss: 0.00001036
Iteration 26/1000 | Loss: 0.00001036
Iteration 27/1000 | Loss: 0.00001035
Iteration 28/1000 | Loss: 0.00001035
Iteration 29/1000 | Loss: 0.00001034
Iteration 30/1000 | Loss: 0.00001033
Iteration 31/1000 | Loss: 0.00001032
Iteration 32/1000 | Loss: 0.00001029
Iteration 33/1000 | Loss: 0.00001028
Iteration 34/1000 | Loss: 0.00001028
Iteration 35/1000 | Loss: 0.00001026
Iteration 36/1000 | Loss: 0.00001026
Iteration 37/1000 | Loss: 0.00001025
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001025
Iteration 40/1000 | Loss: 0.00001024
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001022
Iteration 44/1000 | Loss: 0.00001022
Iteration 45/1000 | Loss: 0.00001021
Iteration 46/1000 | Loss: 0.00001021
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001020
Iteration 49/1000 | Loss: 0.00001020
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001018
Iteration 53/1000 | Loss: 0.00001018
Iteration 54/1000 | Loss: 0.00001017
Iteration 55/1000 | Loss: 0.00001017
Iteration 56/1000 | Loss: 0.00001016
Iteration 57/1000 | Loss: 0.00001016
Iteration 58/1000 | Loss: 0.00001016
Iteration 59/1000 | Loss: 0.00001016
Iteration 60/1000 | Loss: 0.00001016
Iteration 61/1000 | Loss: 0.00001015
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001015
Iteration 66/1000 | Loss: 0.00001015
Iteration 67/1000 | Loss: 0.00001014
Iteration 68/1000 | Loss: 0.00001014
Iteration 69/1000 | Loss: 0.00001014
Iteration 70/1000 | Loss: 0.00001014
Iteration 71/1000 | Loss: 0.00001014
Iteration 72/1000 | Loss: 0.00001014
Iteration 73/1000 | Loss: 0.00001013
Iteration 74/1000 | Loss: 0.00001013
Iteration 75/1000 | Loss: 0.00001012
Iteration 76/1000 | Loss: 0.00001012
Iteration 77/1000 | Loss: 0.00001012
Iteration 78/1000 | Loss: 0.00001012
Iteration 79/1000 | Loss: 0.00001012
Iteration 80/1000 | Loss: 0.00001011
Iteration 81/1000 | Loss: 0.00001011
Iteration 82/1000 | Loss: 0.00001011
Iteration 83/1000 | Loss: 0.00001010
Iteration 84/1000 | Loss: 0.00001010
Iteration 85/1000 | Loss: 0.00001009
Iteration 86/1000 | Loss: 0.00001008
Iteration 87/1000 | Loss: 0.00001008
Iteration 88/1000 | Loss: 0.00001008
Iteration 89/1000 | Loss: 0.00001008
Iteration 90/1000 | Loss: 0.00001007
Iteration 91/1000 | Loss: 0.00001007
Iteration 92/1000 | Loss: 0.00001005
Iteration 93/1000 | Loss: 0.00001004
Iteration 94/1000 | Loss: 0.00001004
Iteration 95/1000 | Loss: 0.00001004
Iteration 96/1000 | Loss: 0.00001004
Iteration 97/1000 | Loss: 0.00001004
Iteration 98/1000 | Loss: 0.00001004
Iteration 99/1000 | Loss: 0.00001004
Iteration 100/1000 | Loss: 0.00001004
Iteration 101/1000 | Loss: 0.00001004
Iteration 102/1000 | Loss: 0.00001003
Iteration 103/1000 | Loss: 0.00001003
Iteration 104/1000 | Loss: 0.00001003
Iteration 105/1000 | Loss: 0.00001003
Iteration 106/1000 | Loss: 0.00001003
Iteration 107/1000 | Loss: 0.00001001
Iteration 108/1000 | Loss: 0.00001001
Iteration 109/1000 | Loss: 0.00001001
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00001000
Iteration 112/1000 | Loss: 0.00001000
Iteration 113/1000 | Loss: 0.00001000
Iteration 114/1000 | Loss: 0.00001000
Iteration 115/1000 | Loss: 0.00001000
Iteration 116/1000 | Loss: 0.00000999
Iteration 117/1000 | Loss: 0.00000999
Iteration 118/1000 | Loss: 0.00000998
Iteration 119/1000 | Loss: 0.00000998
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000997
Iteration 122/1000 | Loss: 0.00000997
Iteration 123/1000 | Loss: 0.00000997
Iteration 124/1000 | Loss: 0.00000997
Iteration 125/1000 | Loss: 0.00000997
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000996
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000994
Iteration 133/1000 | Loss: 0.00000994
Iteration 134/1000 | Loss: 0.00000993
Iteration 135/1000 | Loss: 0.00000993
Iteration 136/1000 | Loss: 0.00000993
Iteration 137/1000 | Loss: 0.00000993
Iteration 138/1000 | Loss: 0.00000992
Iteration 139/1000 | Loss: 0.00000992
Iteration 140/1000 | Loss: 0.00000992
Iteration 141/1000 | Loss: 0.00000992
Iteration 142/1000 | Loss: 0.00000992
Iteration 143/1000 | Loss: 0.00000992
Iteration 144/1000 | Loss: 0.00000992
Iteration 145/1000 | Loss: 0.00000992
Iteration 146/1000 | Loss: 0.00000991
Iteration 147/1000 | Loss: 0.00000991
Iteration 148/1000 | Loss: 0.00000991
Iteration 149/1000 | Loss: 0.00000991
Iteration 150/1000 | Loss: 0.00000990
Iteration 151/1000 | Loss: 0.00000990
Iteration 152/1000 | Loss: 0.00000990
Iteration 153/1000 | Loss: 0.00000990
Iteration 154/1000 | Loss: 0.00000990
Iteration 155/1000 | Loss: 0.00000990
Iteration 156/1000 | Loss: 0.00000990
Iteration 157/1000 | Loss: 0.00000990
Iteration 158/1000 | Loss: 0.00000990
Iteration 159/1000 | Loss: 0.00000990
Iteration 160/1000 | Loss: 0.00000990
Iteration 161/1000 | Loss: 0.00000990
Iteration 162/1000 | Loss: 0.00000990
Iteration 163/1000 | Loss: 0.00000990
Iteration 164/1000 | Loss: 0.00000990
Iteration 165/1000 | Loss: 0.00000990
Iteration 166/1000 | Loss: 0.00000990
Iteration 167/1000 | Loss: 0.00000990
Iteration 168/1000 | Loss: 0.00000989
Iteration 169/1000 | Loss: 0.00000989
Iteration 170/1000 | Loss: 0.00000989
Iteration 171/1000 | Loss: 0.00000989
Iteration 172/1000 | Loss: 0.00000989
Iteration 173/1000 | Loss: 0.00000989
Iteration 174/1000 | Loss: 0.00000989
Iteration 175/1000 | Loss: 0.00000989
Iteration 176/1000 | Loss: 0.00000989
Iteration 177/1000 | Loss: 0.00000989
Iteration 178/1000 | Loss: 0.00000989
Iteration 179/1000 | Loss: 0.00000989
Iteration 180/1000 | Loss: 0.00000988
Iteration 181/1000 | Loss: 0.00000988
Iteration 182/1000 | Loss: 0.00000988
Iteration 183/1000 | Loss: 0.00000988
Iteration 184/1000 | Loss: 0.00000988
Iteration 185/1000 | Loss: 0.00000988
Iteration 186/1000 | Loss: 0.00000988
Iteration 187/1000 | Loss: 0.00000988
Iteration 188/1000 | Loss: 0.00000988
Iteration 189/1000 | Loss: 0.00000988
Iteration 190/1000 | Loss: 0.00000988
Iteration 191/1000 | Loss: 0.00000988
Iteration 192/1000 | Loss: 0.00000988
Iteration 193/1000 | Loss: 0.00000988
Iteration 194/1000 | Loss: 0.00000988
Iteration 195/1000 | Loss: 0.00000988
Iteration 196/1000 | Loss: 0.00000988
Iteration 197/1000 | Loss: 0.00000988
Iteration 198/1000 | Loss: 0.00000988
Iteration 199/1000 | Loss: 0.00000988
Iteration 200/1000 | Loss: 0.00000988
Iteration 201/1000 | Loss: 0.00000988
Iteration 202/1000 | Loss: 0.00000988
Iteration 203/1000 | Loss: 0.00000988
Iteration 204/1000 | Loss: 0.00000988
Iteration 205/1000 | Loss: 0.00000988
Iteration 206/1000 | Loss: 0.00000988
Iteration 207/1000 | Loss: 0.00000988
Iteration 208/1000 | Loss: 0.00000988
Iteration 209/1000 | Loss: 0.00000988
Iteration 210/1000 | Loss: 0.00000988
Iteration 211/1000 | Loss: 0.00000988
Iteration 212/1000 | Loss: 0.00000988
Iteration 213/1000 | Loss: 0.00000988
Iteration 214/1000 | Loss: 0.00000988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [9.883289749268442e-06, 9.883289749268442e-06, 9.883289749268442e-06, 9.883289749268442e-06, 9.883289749268442e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.883289749268442e-06

Optimization complete. Final v2v error: 2.7130701541900635 mm

Highest mean error: 2.820892810821533 mm for frame 66

Lowest mean error: 2.6232051849365234 mm for frame 13

Saving results

Total time: 38.334081172943115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856858
Iteration 2/25 | Loss: 0.00132148
Iteration 3/25 | Loss: 0.00109491
Iteration 4/25 | Loss: 0.00106671
Iteration 5/25 | Loss: 0.00106031
Iteration 6/25 | Loss: 0.00106742
Iteration 7/25 | Loss: 0.00106606
Iteration 8/25 | Loss: 0.00106507
Iteration 9/25 | Loss: 0.00105199
Iteration 10/25 | Loss: 0.00104719
Iteration 11/25 | Loss: 0.00105540
Iteration 12/25 | Loss: 0.00104049
Iteration 13/25 | Loss: 0.00104477
Iteration 14/25 | Loss: 0.00102590
Iteration 15/25 | Loss: 0.00101902
Iteration 16/25 | Loss: 0.00101696
Iteration 17/25 | Loss: 0.00101670
Iteration 18/25 | Loss: 0.00101662
Iteration 19/25 | Loss: 0.00101662
Iteration 20/25 | Loss: 0.00101661
Iteration 21/25 | Loss: 0.00101661
Iteration 22/25 | Loss: 0.00101661
Iteration 23/25 | Loss: 0.00101661
Iteration 24/25 | Loss: 0.00101661
Iteration 25/25 | Loss: 0.00101661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19170475
Iteration 2/25 | Loss: 0.00065152
Iteration 3/25 | Loss: 0.00065148
Iteration 4/25 | Loss: 0.00065148
Iteration 5/25 | Loss: 0.00065148
Iteration 6/25 | Loss: 0.00065148
Iteration 7/25 | Loss: 0.00065148
Iteration 8/25 | Loss: 0.00065148
Iteration 9/25 | Loss: 0.00065147
Iteration 10/25 | Loss: 0.00065147
Iteration 11/25 | Loss: 0.00065147
Iteration 12/25 | Loss: 0.00065147
Iteration 13/25 | Loss: 0.00065147
Iteration 14/25 | Loss: 0.00065147
Iteration 15/25 | Loss: 0.00065147
Iteration 16/25 | Loss: 0.00065147
Iteration 17/25 | Loss: 0.00065147
Iteration 18/25 | Loss: 0.00065147
Iteration 19/25 | Loss: 0.00065147
Iteration 20/25 | Loss: 0.00065147
Iteration 21/25 | Loss: 0.00065147
Iteration 22/25 | Loss: 0.00065147
Iteration 23/25 | Loss: 0.00065147
Iteration 24/25 | Loss: 0.00065147
Iteration 25/25 | Loss: 0.00065147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065147
Iteration 2/1000 | Loss: 0.00002346
Iteration 3/1000 | Loss: 0.00001611
Iteration 4/1000 | Loss: 0.00001344
Iteration 5/1000 | Loss: 0.00001238
Iteration 6/1000 | Loss: 0.00001173
Iteration 7/1000 | Loss: 0.00001139
Iteration 8/1000 | Loss: 0.00001116
Iteration 9/1000 | Loss: 0.00001102
Iteration 10/1000 | Loss: 0.00001094
Iteration 11/1000 | Loss: 0.00001080
Iteration 12/1000 | Loss: 0.00001078
Iteration 13/1000 | Loss: 0.00001078
Iteration 14/1000 | Loss: 0.00001078
Iteration 15/1000 | Loss: 0.00001075
Iteration 16/1000 | Loss: 0.00001070
Iteration 17/1000 | Loss: 0.00001070
Iteration 18/1000 | Loss: 0.00001069
Iteration 19/1000 | Loss: 0.00001068
Iteration 20/1000 | Loss: 0.00001068
Iteration 21/1000 | Loss: 0.00001068
Iteration 22/1000 | Loss: 0.00001068
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001066
Iteration 25/1000 | Loss: 0.00001065
Iteration 26/1000 | Loss: 0.00001064
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001063
Iteration 29/1000 | Loss: 0.00001063
Iteration 30/1000 | Loss: 0.00001063
Iteration 31/1000 | Loss: 0.00001062
Iteration 32/1000 | Loss: 0.00001062
Iteration 33/1000 | Loss: 0.00001061
Iteration 34/1000 | Loss: 0.00001061
Iteration 35/1000 | Loss: 0.00001059
Iteration 36/1000 | Loss: 0.00001059
Iteration 37/1000 | Loss: 0.00001058
Iteration 38/1000 | Loss: 0.00001058
Iteration 39/1000 | Loss: 0.00001058
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001057
Iteration 43/1000 | Loss: 0.00001057
Iteration 44/1000 | Loss: 0.00001056
Iteration 45/1000 | Loss: 0.00001056
Iteration 46/1000 | Loss: 0.00001056
Iteration 47/1000 | Loss: 0.00001055
Iteration 48/1000 | Loss: 0.00001055
Iteration 49/1000 | Loss: 0.00001055
Iteration 50/1000 | Loss: 0.00001054
Iteration 51/1000 | Loss: 0.00001054
Iteration 52/1000 | Loss: 0.00001053
Iteration 53/1000 | Loss: 0.00001053
Iteration 54/1000 | Loss: 0.00001052
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001052
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001052
Iteration 59/1000 | Loss: 0.00001052
Iteration 60/1000 | Loss: 0.00001052
Iteration 61/1000 | Loss: 0.00001052
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001052
Iteration 64/1000 | Loss: 0.00001052
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001052
Iteration 67/1000 | Loss: 0.00001052
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001051
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001049
Iteration 75/1000 | Loss: 0.00001049
Iteration 76/1000 | Loss: 0.00001049
Iteration 77/1000 | Loss: 0.00001049
Iteration 78/1000 | Loss: 0.00001049
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001048
Iteration 83/1000 | Loss: 0.00001048
Iteration 84/1000 | Loss: 0.00001048
Iteration 85/1000 | Loss: 0.00001048
Iteration 86/1000 | Loss: 0.00001048
Iteration 87/1000 | Loss: 0.00001048
Iteration 88/1000 | Loss: 0.00001048
Iteration 89/1000 | Loss: 0.00001048
Iteration 90/1000 | Loss: 0.00001048
Iteration 91/1000 | Loss: 0.00001048
Iteration 92/1000 | Loss: 0.00001048
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001047
Iteration 95/1000 | Loss: 0.00001047
Iteration 96/1000 | Loss: 0.00001047
Iteration 97/1000 | Loss: 0.00001047
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001046
Iteration 102/1000 | Loss: 0.00001046
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001046
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001046
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001046
Iteration 112/1000 | Loss: 0.00001046
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001046
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001046
Iteration 117/1000 | Loss: 0.00001046
Iteration 118/1000 | Loss: 0.00001046
Iteration 119/1000 | Loss: 0.00001046
Iteration 120/1000 | Loss: 0.00001046
Iteration 121/1000 | Loss: 0.00001046
Iteration 122/1000 | Loss: 0.00001046
Iteration 123/1000 | Loss: 0.00001046
Iteration 124/1000 | Loss: 0.00001046
Iteration 125/1000 | Loss: 0.00001046
Iteration 126/1000 | Loss: 0.00001046
Iteration 127/1000 | Loss: 0.00001046
Iteration 128/1000 | Loss: 0.00001046
Iteration 129/1000 | Loss: 0.00001046
Iteration 130/1000 | Loss: 0.00001046
Iteration 131/1000 | Loss: 0.00001046
Iteration 132/1000 | Loss: 0.00001046
Iteration 133/1000 | Loss: 0.00001046
Iteration 134/1000 | Loss: 0.00001046
Iteration 135/1000 | Loss: 0.00001046
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001046
Iteration 139/1000 | Loss: 0.00001046
Iteration 140/1000 | Loss: 0.00001046
Iteration 141/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.04558675957378e-05, 1.04558675957378e-05, 1.04558675957378e-05, 1.04558675957378e-05, 1.04558675957378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.04558675957378e-05

Optimization complete. Final v2v error: 2.693894863128662 mm

Highest mean error: 5.056303024291992 mm for frame 162

Lowest mean error: 2.331305980682373 mm for frame 25

Saving results

Total time: 54.162468910217285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849718
Iteration 2/25 | Loss: 0.00105261
Iteration 3/25 | Loss: 0.00099276
Iteration 4/25 | Loss: 0.00098267
Iteration 5/25 | Loss: 0.00097981
Iteration 6/25 | Loss: 0.00097952
Iteration 7/25 | Loss: 0.00097952
Iteration 8/25 | Loss: 0.00097952
Iteration 9/25 | Loss: 0.00097952
Iteration 10/25 | Loss: 0.00097952
Iteration 11/25 | Loss: 0.00097952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009795159567147493, 0.0009795159567147493, 0.0009795159567147493, 0.0009795159567147493, 0.0009795159567147493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009795159567147493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38890278
Iteration 2/25 | Loss: 0.00069904
Iteration 3/25 | Loss: 0.00069904
Iteration 4/25 | Loss: 0.00069904
Iteration 5/25 | Loss: 0.00069904
Iteration 6/25 | Loss: 0.00069904
Iteration 7/25 | Loss: 0.00069904
Iteration 8/25 | Loss: 0.00069904
Iteration 9/25 | Loss: 0.00069904
Iteration 10/25 | Loss: 0.00069904
Iteration 11/25 | Loss: 0.00069904
Iteration 12/25 | Loss: 0.00069904
Iteration 13/25 | Loss: 0.00069904
Iteration 14/25 | Loss: 0.00069904
Iteration 15/25 | Loss: 0.00069904
Iteration 16/25 | Loss: 0.00069904
Iteration 17/25 | Loss: 0.00069904
Iteration 18/25 | Loss: 0.00069904
Iteration 19/25 | Loss: 0.00069904
Iteration 20/25 | Loss: 0.00069904
Iteration 21/25 | Loss: 0.00069904
Iteration 22/25 | Loss: 0.00069904
Iteration 23/25 | Loss: 0.00069904
Iteration 24/25 | Loss: 0.00069904
Iteration 25/25 | Loss: 0.00069904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069904
Iteration 2/1000 | Loss: 0.00001757
Iteration 3/1000 | Loss: 0.00001238
Iteration 4/1000 | Loss: 0.00001128
Iteration 5/1000 | Loss: 0.00001076
Iteration 6/1000 | Loss: 0.00001038
Iteration 7/1000 | Loss: 0.00001007
Iteration 8/1000 | Loss: 0.00000988
Iteration 9/1000 | Loss: 0.00000982
Iteration 10/1000 | Loss: 0.00000981
Iteration 11/1000 | Loss: 0.00000980
Iteration 12/1000 | Loss: 0.00000977
Iteration 13/1000 | Loss: 0.00000974
Iteration 14/1000 | Loss: 0.00000971
Iteration 15/1000 | Loss: 0.00000971
Iteration 16/1000 | Loss: 0.00000962
Iteration 17/1000 | Loss: 0.00000961
Iteration 18/1000 | Loss: 0.00000961
Iteration 19/1000 | Loss: 0.00000960
Iteration 20/1000 | Loss: 0.00000960
Iteration 21/1000 | Loss: 0.00000959
Iteration 22/1000 | Loss: 0.00000959
Iteration 23/1000 | Loss: 0.00000958
Iteration 24/1000 | Loss: 0.00000958
Iteration 25/1000 | Loss: 0.00000957
Iteration 26/1000 | Loss: 0.00000957
Iteration 27/1000 | Loss: 0.00000956
Iteration 28/1000 | Loss: 0.00000956
Iteration 29/1000 | Loss: 0.00000956
Iteration 30/1000 | Loss: 0.00000956
Iteration 31/1000 | Loss: 0.00000955
Iteration 32/1000 | Loss: 0.00000955
Iteration 33/1000 | Loss: 0.00000955
Iteration 34/1000 | Loss: 0.00000954
Iteration 35/1000 | Loss: 0.00000953
Iteration 36/1000 | Loss: 0.00000953
Iteration 37/1000 | Loss: 0.00000952
Iteration 38/1000 | Loss: 0.00000952
Iteration 39/1000 | Loss: 0.00000952
Iteration 40/1000 | Loss: 0.00000952
Iteration 41/1000 | Loss: 0.00000952
Iteration 42/1000 | Loss: 0.00000951
Iteration 43/1000 | Loss: 0.00000951
Iteration 44/1000 | Loss: 0.00000951
Iteration 45/1000 | Loss: 0.00000950
Iteration 46/1000 | Loss: 0.00000950
Iteration 47/1000 | Loss: 0.00000949
Iteration 48/1000 | Loss: 0.00000949
Iteration 49/1000 | Loss: 0.00000948
Iteration 50/1000 | Loss: 0.00000948
Iteration 51/1000 | Loss: 0.00000948
Iteration 52/1000 | Loss: 0.00000948
Iteration 53/1000 | Loss: 0.00000947
Iteration 54/1000 | Loss: 0.00000947
Iteration 55/1000 | Loss: 0.00000945
Iteration 56/1000 | Loss: 0.00000945
Iteration 57/1000 | Loss: 0.00000944
Iteration 58/1000 | Loss: 0.00000944
Iteration 59/1000 | Loss: 0.00000944
Iteration 60/1000 | Loss: 0.00000944
Iteration 61/1000 | Loss: 0.00000944
Iteration 62/1000 | Loss: 0.00000944
Iteration 63/1000 | Loss: 0.00000943
Iteration 64/1000 | Loss: 0.00000943
Iteration 65/1000 | Loss: 0.00000943
Iteration 66/1000 | Loss: 0.00000942
Iteration 67/1000 | Loss: 0.00000942
Iteration 68/1000 | Loss: 0.00000941
Iteration 69/1000 | Loss: 0.00000941
Iteration 70/1000 | Loss: 0.00000941
Iteration 71/1000 | Loss: 0.00000940
Iteration 72/1000 | Loss: 0.00000940
Iteration 73/1000 | Loss: 0.00000940
Iteration 74/1000 | Loss: 0.00000940
Iteration 75/1000 | Loss: 0.00000940
Iteration 76/1000 | Loss: 0.00000940
Iteration 77/1000 | Loss: 0.00000939
Iteration 78/1000 | Loss: 0.00000939
Iteration 79/1000 | Loss: 0.00000938
Iteration 80/1000 | Loss: 0.00000938
Iteration 81/1000 | Loss: 0.00000937
Iteration 82/1000 | Loss: 0.00000937
Iteration 83/1000 | Loss: 0.00000937
Iteration 84/1000 | Loss: 0.00000937
Iteration 85/1000 | Loss: 0.00000937
Iteration 86/1000 | Loss: 0.00000937
Iteration 87/1000 | Loss: 0.00000937
Iteration 88/1000 | Loss: 0.00000937
Iteration 89/1000 | Loss: 0.00000937
Iteration 90/1000 | Loss: 0.00000937
Iteration 91/1000 | Loss: 0.00000937
Iteration 92/1000 | Loss: 0.00000936
Iteration 93/1000 | Loss: 0.00000936
Iteration 94/1000 | Loss: 0.00000936
Iteration 95/1000 | Loss: 0.00000936
Iteration 96/1000 | Loss: 0.00000936
Iteration 97/1000 | Loss: 0.00000936
Iteration 98/1000 | Loss: 0.00000936
Iteration 99/1000 | Loss: 0.00000936
Iteration 100/1000 | Loss: 0.00000936
Iteration 101/1000 | Loss: 0.00000935
Iteration 102/1000 | Loss: 0.00000935
Iteration 103/1000 | Loss: 0.00000935
Iteration 104/1000 | Loss: 0.00000935
Iteration 105/1000 | Loss: 0.00000934
Iteration 106/1000 | Loss: 0.00000934
Iteration 107/1000 | Loss: 0.00000934
Iteration 108/1000 | Loss: 0.00000933
Iteration 109/1000 | Loss: 0.00000933
Iteration 110/1000 | Loss: 0.00000933
Iteration 111/1000 | Loss: 0.00000933
Iteration 112/1000 | Loss: 0.00000932
Iteration 113/1000 | Loss: 0.00000932
Iteration 114/1000 | Loss: 0.00000932
Iteration 115/1000 | Loss: 0.00000932
Iteration 116/1000 | Loss: 0.00000932
Iteration 117/1000 | Loss: 0.00000931
Iteration 118/1000 | Loss: 0.00000931
Iteration 119/1000 | Loss: 0.00000931
Iteration 120/1000 | Loss: 0.00000931
Iteration 121/1000 | Loss: 0.00000931
Iteration 122/1000 | Loss: 0.00000931
Iteration 123/1000 | Loss: 0.00000931
Iteration 124/1000 | Loss: 0.00000931
Iteration 125/1000 | Loss: 0.00000931
Iteration 126/1000 | Loss: 0.00000931
Iteration 127/1000 | Loss: 0.00000931
Iteration 128/1000 | Loss: 0.00000931
Iteration 129/1000 | Loss: 0.00000931
Iteration 130/1000 | Loss: 0.00000931
Iteration 131/1000 | Loss: 0.00000930
Iteration 132/1000 | Loss: 0.00000930
Iteration 133/1000 | Loss: 0.00000930
Iteration 134/1000 | Loss: 0.00000930
Iteration 135/1000 | Loss: 0.00000930
Iteration 136/1000 | Loss: 0.00000930
Iteration 137/1000 | Loss: 0.00000930
Iteration 138/1000 | Loss: 0.00000930
Iteration 139/1000 | Loss: 0.00000930
Iteration 140/1000 | Loss: 0.00000930
Iteration 141/1000 | Loss: 0.00000930
Iteration 142/1000 | Loss: 0.00000930
Iteration 143/1000 | Loss: 0.00000930
Iteration 144/1000 | Loss: 0.00000930
Iteration 145/1000 | Loss: 0.00000930
Iteration 146/1000 | Loss: 0.00000930
Iteration 147/1000 | Loss: 0.00000930
Iteration 148/1000 | Loss: 0.00000929
Iteration 149/1000 | Loss: 0.00000929
Iteration 150/1000 | Loss: 0.00000929
Iteration 151/1000 | Loss: 0.00000929
Iteration 152/1000 | Loss: 0.00000929
Iteration 153/1000 | Loss: 0.00000929
Iteration 154/1000 | Loss: 0.00000929
Iteration 155/1000 | Loss: 0.00000929
Iteration 156/1000 | Loss: 0.00000929
Iteration 157/1000 | Loss: 0.00000929
Iteration 158/1000 | Loss: 0.00000929
Iteration 159/1000 | Loss: 0.00000929
Iteration 160/1000 | Loss: 0.00000929
Iteration 161/1000 | Loss: 0.00000929
Iteration 162/1000 | Loss: 0.00000929
Iteration 163/1000 | Loss: 0.00000928
Iteration 164/1000 | Loss: 0.00000928
Iteration 165/1000 | Loss: 0.00000928
Iteration 166/1000 | Loss: 0.00000928
Iteration 167/1000 | Loss: 0.00000928
Iteration 168/1000 | Loss: 0.00000928
Iteration 169/1000 | Loss: 0.00000927
Iteration 170/1000 | Loss: 0.00000927
Iteration 171/1000 | Loss: 0.00000927
Iteration 172/1000 | Loss: 0.00000927
Iteration 173/1000 | Loss: 0.00000926
Iteration 174/1000 | Loss: 0.00000926
Iteration 175/1000 | Loss: 0.00000926
Iteration 176/1000 | Loss: 0.00000926
Iteration 177/1000 | Loss: 0.00000926
Iteration 178/1000 | Loss: 0.00000926
Iteration 179/1000 | Loss: 0.00000926
Iteration 180/1000 | Loss: 0.00000926
Iteration 181/1000 | Loss: 0.00000926
Iteration 182/1000 | Loss: 0.00000926
Iteration 183/1000 | Loss: 0.00000925
Iteration 184/1000 | Loss: 0.00000925
Iteration 185/1000 | Loss: 0.00000925
Iteration 186/1000 | Loss: 0.00000925
Iteration 187/1000 | Loss: 0.00000925
Iteration 188/1000 | Loss: 0.00000925
Iteration 189/1000 | Loss: 0.00000925
Iteration 190/1000 | Loss: 0.00000925
Iteration 191/1000 | Loss: 0.00000925
Iteration 192/1000 | Loss: 0.00000925
Iteration 193/1000 | Loss: 0.00000925
Iteration 194/1000 | Loss: 0.00000925
Iteration 195/1000 | Loss: 0.00000924
Iteration 196/1000 | Loss: 0.00000924
Iteration 197/1000 | Loss: 0.00000924
Iteration 198/1000 | Loss: 0.00000923
Iteration 199/1000 | Loss: 0.00000923
Iteration 200/1000 | Loss: 0.00000923
Iteration 201/1000 | Loss: 0.00000923
Iteration 202/1000 | Loss: 0.00000923
Iteration 203/1000 | Loss: 0.00000922
Iteration 204/1000 | Loss: 0.00000922
Iteration 205/1000 | Loss: 0.00000922
Iteration 206/1000 | Loss: 0.00000922
Iteration 207/1000 | Loss: 0.00000922
Iteration 208/1000 | Loss: 0.00000922
Iteration 209/1000 | Loss: 0.00000922
Iteration 210/1000 | Loss: 0.00000922
Iteration 211/1000 | Loss: 0.00000922
Iteration 212/1000 | Loss: 0.00000922
Iteration 213/1000 | Loss: 0.00000921
Iteration 214/1000 | Loss: 0.00000921
Iteration 215/1000 | Loss: 0.00000921
Iteration 216/1000 | Loss: 0.00000921
Iteration 217/1000 | Loss: 0.00000921
Iteration 218/1000 | Loss: 0.00000921
Iteration 219/1000 | Loss: 0.00000921
Iteration 220/1000 | Loss: 0.00000921
Iteration 221/1000 | Loss: 0.00000921
Iteration 222/1000 | Loss: 0.00000921
Iteration 223/1000 | Loss: 0.00000921
Iteration 224/1000 | Loss: 0.00000921
Iteration 225/1000 | Loss: 0.00000921
Iteration 226/1000 | Loss: 0.00000921
Iteration 227/1000 | Loss: 0.00000921
Iteration 228/1000 | Loss: 0.00000921
Iteration 229/1000 | Loss: 0.00000921
Iteration 230/1000 | Loss: 0.00000921
Iteration 231/1000 | Loss: 0.00000921
Iteration 232/1000 | Loss: 0.00000921
Iteration 233/1000 | Loss: 0.00000921
Iteration 234/1000 | Loss: 0.00000921
Iteration 235/1000 | Loss: 0.00000921
Iteration 236/1000 | Loss: 0.00000921
Iteration 237/1000 | Loss: 0.00000921
Iteration 238/1000 | Loss: 0.00000921
Iteration 239/1000 | Loss: 0.00000921
Iteration 240/1000 | Loss: 0.00000921
Iteration 241/1000 | Loss: 0.00000921
Iteration 242/1000 | Loss: 0.00000921
Iteration 243/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [9.213675184582826e-06, 9.213675184582826e-06, 9.213675184582826e-06, 9.213675184582826e-06, 9.213675184582826e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.213675184582826e-06

Optimization complete. Final v2v error: 2.5967016220092773 mm

Highest mean error: 2.7433102130889893 mm for frame 109

Lowest mean error: 2.5263726711273193 mm for frame 138

Saving results

Total time: 34.24842047691345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010927
Iteration 2/25 | Loss: 0.00206485
Iteration 3/25 | Loss: 0.00143569
Iteration 4/25 | Loss: 0.00135027
Iteration 5/25 | Loss: 0.00121169
Iteration 6/25 | Loss: 0.00119803
Iteration 7/25 | Loss: 0.00121025
Iteration 8/25 | Loss: 0.00118429
Iteration 9/25 | Loss: 0.00117211
Iteration 10/25 | Loss: 0.00114456
Iteration 11/25 | Loss: 0.00113037
Iteration 12/25 | Loss: 0.00112220
Iteration 13/25 | Loss: 0.00110676
Iteration 14/25 | Loss: 0.00109704
Iteration 15/25 | Loss: 0.00109766
Iteration 16/25 | Loss: 0.00109516
Iteration 17/25 | Loss: 0.00109309
Iteration 18/25 | Loss: 0.00109532
Iteration 19/25 | Loss: 0.00109308
Iteration 20/25 | Loss: 0.00109416
Iteration 21/25 | Loss: 0.00109253
Iteration 22/25 | Loss: 0.00109049
Iteration 23/25 | Loss: 0.00109049
Iteration 24/25 | Loss: 0.00109049
Iteration 25/25 | Loss: 0.00109048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37168634
Iteration 2/25 | Loss: 0.00089848
Iteration 3/25 | Loss: 0.00076031
Iteration 4/25 | Loss: 0.00076031
Iteration 5/25 | Loss: 0.00076031
Iteration 6/25 | Loss: 0.00076031
Iteration 7/25 | Loss: 0.00076031
Iteration 8/25 | Loss: 0.00076031
Iteration 9/25 | Loss: 0.00076031
Iteration 10/25 | Loss: 0.00076031
Iteration 11/25 | Loss: 0.00076031
Iteration 12/25 | Loss: 0.00076031
Iteration 13/25 | Loss: 0.00076031
Iteration 14/25 | Loss: 0.00076031
Iteration 15/25 | Loss: 0.00076031
Iteration 16/25 | Loss: 0.00076031
Iteration 17/25 | Loss: 0.00076031
Iteration 18/25 | Loss: 0.00076031
Iteration 19/25 | Loss: 0.00076031
Iteration 20/25 | Loss: 0.00076031
Iteration 21/25 | Loss: 0.00076031
Iteration 22/25 | Loss: 0.00076031
Iteration 23/25 | Loss: 0.00076031
Iteration 24/25 | Loss: 0.00076031
Iteration 25/25 | Loss: 0.00076031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076031
Iteration 2/1000 | Loss: 0.00014712
Iteration 3/1000 | Loss: 0.00002242
Iteration 4/1000 | Loss: 0.00005849
Iteration 5/1000 | Loss: 0.00025465
Iteration 6/1000 | Loss: 0.00005320
Iteration 7/1000 | Loss: 0.00001983
Iteration 8/1000 | Loss: 0.00006411
Iteration 9/1000 | Loss: 0.00004183
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001813
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001770
Iteration 15/1000 | Loss: 0.00001769
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00006353
Iteration 19/1000 | Loss: 0.00002680
Iteration 20/1000 | Loss: 0.00001744
Iteration 21/1000 | Loss: 0.00001744
Iteration 22/1000 | Loss: 0.00003709
Iteration 23/1000 | Loss: 0.00001738
Iteration 24/1000 | Loss: 0.00001737
Iteration 25/1000 | Loss: 0.00001737
Iteration 26/1000 | Loss: 0.00001737
Iteration 27/1000 | Loss: 0.00001736
Iteration 28/1000 | Loss: 0.00001736
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001736
Iteration 31/1000 | Loss: 0.00001736
Iteration 32/1000 | Loss: 0.00001736
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001736
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00006170
Iteration 42/1000 | Loss: 0.00006170
Iteration 43/1000 | Loss: 0.00008978
Iteration 44/1000 | Loss: 0.00001733
Iteration 45/1000 | Loss: 0.00001729
Iteration 46/1000 | Loss: 0.00001729
Iteration 47/1000 | Loss: 0.00001729
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001729
Iteration 51/1000 | Loss: 0.00001728
Iteration 52/1000 | Loss: 0.00001728
Iteration 53/1000 | Loss: 0.00001728
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001726
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001725
Iteration 67/1000 | Loss: 0.00001725
Iteration 68/1000 | Loss: 0.00001725
Iteration 69/1000 | Loss: 0.00001725
Iteration 70/1000 | Loss: 0.00001725
Iteration 71/1000 | Loss: 0.00001725
Iteration 72/1000 | Loss: 0.00001725
Iteration 73/1000 | Loss: 0.00001724
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001724
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001723
Iteration 78/1000 | Loss: 0.00001723
Iteration 79/1000 | Loss: 0.00001723
Iteration 80/1000 | Loss: 0.00001723
Iteration 81/1000 | Loss: 0.00001723
Iteration 82/1000 | Loss: 0.00001723
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001722
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001722
Iteration 92/1000 | Loss: 0.00001722
Iteration 93/1000 | Loss: 0.00001722
Iteration 94/1000 | Loss: 0.00001721
Iteration 95/1000 | Loss: 0.00001721
Iteration 96/1000 | Loss: 0.00001721
Iteration 97/1000 | Loss: 0.00001721
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001720
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001720
Iteration 104/1000 | Loss: 0.00001720
Iteration 105/1000 | Loss: 0.00001720
Iteration 106/1000 | Loss: 0.00001720
Iteration 107/1000 | Loss: 0.00001720
Iteration 108/1000 | Loss: 0.00001720
Iteration 109/1000 | Loss: 0.00001719
Iteration 110/1000 | Loss: 0.00001719
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001719
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001719
Iteration 119/1000 | Loss: 0.00001719
Iteration 120/1000 | Loss: 0.00001719
Iteration 121/1000 | Loss: 0.00001719
Iteration 122/1000 | Loss: 0.00001719
Iteration 123/1000 | Loss: 0.00001719
Iteration 124/1000 | Loss: 0.00001719
Iteration 125/1000 | Loss: 0.00001719
Iteration 126/1000 | Loss: 0.00001719
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001719
Iteration 129/1000 | Loss: 0.00001719
Iteration 130/1000 | Loss: 0.00001718
Iteration 131/1000 | Loss: 0.00001718
Iteration 132/1000 | Loss: 0.00001718
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001718
Iteration 135/1000 | Loss: 0.00001718
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001718
Iteration 139/1000 | Loss: 0.00001718
Iteration 140/1000 | Loss: 0.00001718
Iteration 141/1000 | Loss: 0.00001718
Iteration 142/1000 | Loss: 0.00001718
Iteration 143/1000 | Loss: 0.00001718
Iteration 144/1000 | Loss: 0.00001718
Iteration 145/1000 | Loss: 0.00001717
Iteration 146/1000 | Loss: 0.00001717
Iteration 147/1000 | Loss: 0.00001717
Iteration 148/1000 | Loss: 0.00001717
Iteration 149/1000 | Loss: 0.00001717
Iteration 150/1000 | Loss: 0.00001717
Iteration 151/1000 | Loss: 0.00001717
Iteration 152/1000 | Loss: 0.00001717
Iteration 153/1000 | Loss: 0.00001717
Iteration 154/1000 | Loss: 0.00001717
Iteration 155/1000 | Loss: 0.00001717
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Iteration 164/1000 | Loss: 0.00001716
Iteration 165/1000 | Loss: 0.00001716
Iteration 166/1000 | Loss: 0.00001716
Iteration 167/1000 | Loss: 0.00001716
Iteration 168/1000 | Loss: 0.00001716
Iteration 169/1000 | Loss: 0.00001716
Iteration 170/1000 | Loss: 0.00001716
Iteration 171/1000 | Loss: 0.00001716
Iteration 172/1000 | Loss: 0.00001716
Iteration 173/1000 | Loss: 0.00001716
Iteration 174/1000 | Loss: 0.00001716
Iteration 175/1000 | Loss: 0.00001716
Iteration 176/1000 | Loss: 0.00001715
Iteration 177/1000 | Loss: 0.00001715
Iteration 178/1000 | Loss: 0.00001715
Iteration 179/1000 | Loss: 0.00001715
Iteration 180/1000 | Loss: 0.00001715
Iteration 181/1000 | Loss: 0.00001715
Iteration 182/1000 | Loss: 0.00001715
Iteration 183/1000 | Loss: 0.00001715
Iteration 184/1000 | Loss: 0.00001715
Iteration 185/1000 | Loss: 0.00001715
Iteration 186/1000 | Loss: 0.00001715
Iteration 187/1000 | Loss: 0.00001714
Iteration 188/1000 | Loss: 0.00001714
Iteration 189/1000 | Loss: 0.00001714
Iteration 190/1000 | Loss: 0.00001714
Iteration 191/1000 | Loss: 0.00001714
Iteration 192/1000 | Loss: 0.00001714
Iteration 193/1000 | Loss: 0.00001714
Iteration 194/1000 | Loss: 0.00001714
Iteration 195/1000 | Loss: 0.00001714
Iteration 196/1000 | Loss: 0.00001714
Iteration 197/1000 | Loss: 0.00001714
Iteration 198/1000 | Loss: 0.00001714
Iteration 199/1000 | Loss: 0.00001714
Iteration 200/1000 | Loss: 0.00001714
Iteration 201/1000 | Loss: 0.00001713
Iteration 202/1000 | Loss: 0.00001713
Iteration 203/1000 | Loss: 0.00001713
Iteration 204/1000 | Loss: 0.00001713
Iteration 205/1000 | Loss: 0.00001713
Iteration 206/1000 | Loss: 0.00001713
Iteration 207/1000 | Loss: 0.00001713
Iteration 208/1000 | Loss: 0.00001713
Iteration 209/1000 | Loss: 0.00001713
Iteration 210/1000 | Loss: 0.00001713
Iteration 211/1000 | Loss: 0.00001713
Iteration 212/1000 | Loss: 0.00001713
Iteration 213/1000 | Loss: 0.00001713
Iteration 214/1000 | Loss: 0.00001713
Iteration 215/1000 | Loss: 0.00001713
Iteration 216/1000 | Loss: 0.00001713
Iteration 217/1000 | Loss: 0.00001713
Iteration 218/1000 | Loss: 0.00001712
Iteration 219/1000 | Loss: 0.00001712
Iteration 220/1000 | Loss: 0.00001712
Iteration 221/1000 | Loss: 0.00001712
Iteration 222/1000 | Loss: 0.00001712
Iteration 223/1000 | Loss: 0.00001712
Iteration 224/1000 | Loss: 0.00001712
Iteration 225/1000 | Loss: 0.00001712
Iteration 226/1000 | Loss: 0.00001712
Iteration 227/1000 | Loss: 0.00001712
Iteration 228/1000 | Loss: 0.00001712
Iteration 229/1000 | Loss: 0.00001712
Iteration 230/1000 | Loss: 0.00001712
Iteration 231/1000 | Loss: 0.00001712
Iteration 232/1000 | Loss: 0.00001712
Iteration 233/1000 | Loss: 0.00001712
Iteration 234/1000 | Loss: 0.00001712
Iteration 235/1000 | Loss: 0.00001712
Iteration 236/1000 | Loss: 0.00001712
Iteration 237/1000 | Loss: 0.00001712
Iteration 238/1000 | Loss: 0.00001712
Iteration 239/1000 | Loss: 0.00001712
Iteration 240/1000 | Loss: 0.00001712
Iteration 241/1000 | Loss: 0.00001712
Iteration 242/1000 | Loss: 0.00001712
Iteration 243/1000 | Loss: 0.00001712
Iteration 244/1000 | Loss: 0.00001712
Iteration 245/1000 | Loss: 0.00001712
Iteration 246/1000 | Loss: 0.00001712
Iteration 247/1000 | Loss: 0.00001712
Iteration 248/1000 | Loss: 0.00001712
Iteration 249/1000 | Loss: 0.00001712
Iteration 250/1000 | Loss: 0.00001712
Iteration 251/1000 | Loss: 0.00001712
Iteration 252/1000 | Loss: 0.00001712
Iteration 253/1000 | Loss: 0.00001712
Iteration 254/1000 | Loss: 0.00001712
Iteration 255/1000 | Loss: 0.00001712
Iteration 256/1000 | Loss: 0.00001712
Iteration 257/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.711548975436017e-05, 1.711548975436017e-05, 1.711548975436017e-05, 1.711548975436017e-05, 1.711548975436017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.711548975436017e-05

Optimization complete. Final v2v error: 3.4973533153533936 mm

Highest mean error: 5.127702713012695 mm for frame 183

Lowest mean error: 3.131298303604126 mm for frame 125

Saving results

Total time: 90.20017528533936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038570
Iteration 2/25 | Loss: 0.00163425
Iteration 3/25 | Loss: 0.00128333
Iteration 4/25 | Loss: 0.00125041
Iteration 5/25 | Loss: 0.00115443
Iteration 6/25 | Loss: 0.00114740
Iteration 7/25 | Loss: 0.00110241
Iteration 8/25 | Loss: 0.00107843
Iteration 9/25 | Loss: 0.00105607
Iteration 10/25 | Loss: 0.00104299
Iteration 11/25 | Loss: 0.00103163
Iteration 12/25 | Loss: 0.00102189
Iteration 13/25 | Loss: 0.00101690
Iteration 14/25 | Loss: 0.00101389
Iteration 15/25 | Loss: 0.00101239
Iteration 16/25 | Loss: 0.00101181
Iteration 17/25 | Loss: 0.00101154
Iteration 18/25 | Loss: 0.00101303
Iteration 19/25 | Loss: 0.00101034
Iteration 20/25 | Loss: 0.00100933
Iteration 21/25 | Loss: 0.00100881
Iteration 22/25 | Loss: 0.00100867
Iteration 23/25 | Loss: 0.00100865
Iteration 24/25 | Loss: 0.00100865
Iteration 25/25 | Loss: 0.00100865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40487289
Iteration 2/25 | Loss: 0.00111226
Iteration 3/25 | Loss: 0.00111226
Iteration 4/25 | Loss: 0.00111226
Iteration 5/25 | Loss: 0.00111226
Iteration 6/25 | Loss: 0.00111226
Iteration 7/25 | Loss: 0.00094091
Iteration 8/25 | Loss: 0.00094091
Iteration 9/25 | Loss: 0.00094091
Iteration 10/25 | Loss: 0.00094091
Iteration 11/25 | Loss: 0.00094091
Iteration 12/25 | Loss: 0.00094091
Iteration 13/25 | Loss: 0.00094091
Iteration 14/25 | Loss: 0.00094091
Iteration 15/25 | Loss: 0.00094091
Iteration 16/25 | Loss: 0.00094091
Iteration 17/25 | Loss: 0.00094091
Iteration 18/25 | Loss: 0.00094091
Iteration 19/25 | Loss: 0.00094091
Iteration 20/25 | Loss: 0.00094091
Iteration 21/25 | Loss: 0.00094091
Iteration 22/25 | Loss: 0.00094091
Iteration 23/25 | Loss: 0.00094091
Iteration 24/25 | Loss: 0.00094091
Iteration 25/25 | Loss: 0.00094091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094091
Iteration 2/1000 | Loss: 0.00004631
Iteration 3/1000 | Loss: 0.00003419
Iteration 4/1000 | Loss: 0.00017140
Iteration 5/1000 | Loss: 0.00201565
Iteration 6/1000 | Loss: 0.00003397
Iteration 7/1000 | Loss: 0.00002791
Iteration 8/1000 | Loss: 0.00002610
Iteration 9/1000 | Loss: 0.00002494
Iteration 10/1000 | Loss: 0.00002428
Iteration 11/1000 | Loss: 0.00002377
Iteration 12/1000 | Loss: 0.00002311
Iteration 13/1000 | Loss: 0.00070894
Iteration 14/1000 | Loss: 0.00271877
Iteration 15/1000 | Loss: 0.00007060
Iteration 16/1000 | Loss: 0.00029445
Iteration 17/1000 | Loss: 0.00002450
Iteration 18/1000 | Loss: 0.00011646
Iteration 19/1000 | Loss: 0.00041037
Iteration 20/1000 | Loss: 0.00128848
Iteration 21/1000 | Loss: 0.00086114
Iteration 22/1000 | Loss: 0.00135980
Iteration 23/1000 | Loss: 0.00106353
Iteration 24/1000 | Loss: 0.00040696
Iteration 25/1000 | Loss: 0.00004722
Iteration 26/1000 | Loss: 0.00002098
Iteration 27/1000 | Loss: 0.00027544
Iteration 28/1000 | Loss: 0.00002135
Iteration 29/1000 | Loss: 0.00001476
Iteration 30/1000 | Loss: 0.00001281
Iteration 31/1000 | Loss: 0.00001162
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001014
Iteration 34/1000 | Loss: 0.00000973
Iteration 35/1000 | Loss: 0.00000942
Iteration 36/1000 | Loss: 0.00000917
Iteration 37/1000 | Loss: 0.00018774
Iteration 38/1000 | Loss: 0.00011000
Iteration 39/1000 | Loss: 0.00022828
Iteration 40/1000 | Loss: 0.00005600
Iteration 41/1000 | Loss: 0.00001018
Iteration 42/1000 | Loss: 0.00011852
Iteration 43/1000 | Loss: 0.00007532
Iteration 44/1000 | Loss: 0.00006890
Iteration 45/1000 | Loss: 0.00001024
Iteration 46/1000 | Loss: 0.00000903
Iteration 47/1000 | Loss: 0.00000883
Iteration 48/1000 | Loss: 0.00000878
Iteration 49/1000 | Loss: 0.00000875
Iteration 50/1000 | Loss: 0.00000875
Iteration 51/1000 | Loss: 0.00000871
Iteration 52/1000 | Loss: 0.00000871
Iteration 53/1000 | Loss: 0.00000870
Iteration 54/1000 | Loss: 0.00000870
Iteration 55/1000 | Loss: 0.00000865
Iteration 56/1000 | Loss: 0.00000864
Iteration 57/1000 | Loss: 0.00000864
Iteration 58/1000 | Loss: 0.00000864
Iteration 59/1000 | Loss: 0.00000863
Iteration 60/1000 | Loss: 0.00000863
Iteration 61/1000 | Loss: 0.00000863
Iteration 62/1000 | Loss: 0.00000862
Iteration 63/1000 | Loss: 0.00000862
Iteration 64/1000 | Loss: 0.00000862
Iteration 65/1000 | Loss: 0.00000862
Iteration 66/1000 | Loss: 0.00000861
Iteration 67/1000 | Loss: 0.00000861
Iteration 68/1000 | Loss: 0.00000861
Iteration 69/1000 | Loss: 0.00000861
Iteration 70/1000 | Loss: 0.00000860
Iteration 71/1000 | Loss: 0.00000860
Iteration 72/1000 | Loss: 0.00000859
Iteration 73/1000 | Loss: 0.00000858
Iteration 74/1000 | Loss: 0.00000858
Iteration 75/1000 | Loss: 0.00000857
Iteration 76/1000 | Loss: 0.00000857
Iteration 77/1000 | Loss: 0.00000857
Iteration 78/1000 | Loss: 0.00000857
Iteration 79/1000 | Loss: 0.00000857
Iteration 80/1000 | Loss: 0.00000856
Iteration 81/1000 | Loss: 0.00000856
Iteration 82/1000 | Loss: 0.00000856
Iteration 83/1000 | Loss: 0.00000856
Iteration 84/1000 | Loss: 0.00000856
Iteration 85/1000 | Loss: 0.00000856
Iteration 86/1000 | Loss: 0.00000856
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000855
Iteration 89/1000 | Loss: 0.00000855
Iteration 90/1000 | Loss: 0.00000855
Iteration 91/1000 | Loss: 0.00000855
Iteration 92/1000 | Loss: 0.00000855
Iteration 93/1000 | Loss: 0.00000855
Iteration 94/1000 | Loss: 0.00000855
Iteration 95/1000 | Loss: 0.00000855
Iteration 96/1000 | Loss: 0.00000855
Iteration 97/1000 | Loss: 0.00000855
Iteration 98/1000 | Loss: 0.00000855
Iteration 99/1000 | Loss: 0.00000854
Iteration 100/1000 | Loss: 0.00000854
Iteration 101/1000 | Loss: 0.00000854
Iteration 102/1000 | Loss: 0.00000854
Iteration 103/1000 | Loss: 0.00000854
Iteration 104/1000 | Loss: 0.00000854
Iteration 105/1000 | Loss: 0.00000854
Iteration 106/1000 | Loss: 0.00000854
Iteration 107/1000 | Loss: 0.00000854
Iteration 108/1000 | Loss: 0.00000854
Iteration 109/1000 | Loss: 0.00000854
Iteration 110/1000 | Loss: 0.00000854
Iteration 111/1000 | Loss: 0.00000854
Iteration 112/1000 | Loss: 0.00000854
Iteration 113/1000 | Loss: 0.00000854
Iteration 114/1000 | Loss: 0.00000854
Iteration 115/1000 | Loss: 0.00000854
Iteration 116/1000 | Loss: 0.00000854
Iteration 117/1000 | Loss: 0.00000854
Iteration 118/1000 | Loss: 0.00000853
Iteration 119/1000 | Loss: 0.00000853
Iteration 120/1000 | Loss: 0.00000853
Iteration 121/1000 | Loss: 0.00000853
Iteration 122/1000 | Loss: 0.00000853
Iteration 123/1000 | Loss: 0.00000853
Iteration 124/1000 | Loss: 0.00000853
Iteration 125/1000 | Loss: 0.00000853
Iteration 126/1000 | Loss: 0.00000853
Iteration 127/1000 | Loss: 0.00000852
Iteration 128/1000 | Loss: 0.00000852
Iteration 129/1000 | Loss: 0.00000852
Iteration 130/1000 | Loss: 0.00000852
Iteration 131/1000 | Loss: 0.00000852
Iteration 132/1000 | Loss: 0.00000852
Iteration 133/1000 | Loss: 0.00000852
Iteration 134/1000 | Loss: 0.00000852
Iteration 135/1000 | Loss: 0.00000852
Iteration 136/1000 | Loss: 0.00000852
Iteration 137/1000 | Loss: 0.00000852
Iteration 138/1000 | Loss: 0.00000852
Iteration 139/1000 | Loss: 0.00000852
Iteration 140/1000 | Loss: 0.00000852
Iteration 141/1000 | Loss: 0.00000852
Iteration 142/1000 | Loss: 0.00000852
Iteration 143/1000 | Loss: 0.00000852
Iteration 144/1000 | Loss: 0.00000852
Iteration 145/1000 | Loss: 0.00000852
Iteration 146/1000 | Loss: 0.00000852
Iteration 147/1000 | Loss: 0.00000851
Iteration 148/1000 | Loss: 0.00000851
Iteration 149/1000 | Loss: 0.00000851
Iteration 150/1000 | Loss: 0.00000851
Iteration 151/1000 | Loss: 0.00000851
Iteration 152/1000 | Loss: 0.00000851
Iteration 153/1000 | Loss: 0.00000851
Iteration 154/1000 | Loss: 0.00000851
Iteration 155/1000 | Loss: 0.00000851
Iteration 156/1000 | Loss: 0.00000851
Iteration 157/1000 | Loss: 0.00000851
Iteration 158/1000 | Loss: 0.00000851
Iteration 159/1000 | Loss: 0.00000851
Iteration 160/1000 | Loss: 0.00000851
Iteration 161/1000 | Loss: 0.00000851
Iteration 162/1000 | Loss: 0.00000851
Iteration 163/1000 | Loss: 0.00000850
Iteration 164/1000 | Loss: 0.00000850
Iteration 165/1000 | Loss: 0.00000850
Iteration 166/1000 | Loss: 0.00000850
Iteration 167/1000 | Loss: 0.00000850
Iteration 168/1000 | Loss: 0.00000850
Iteration 169/1000 | Loss: 0.00000850
Iteration 170/1000 | Loss: 0.00000850
Iteration 171/1000 | Loss: 0.00000850
Iteration 172/1000 | Loss: 0.00000850
Iteration 173/1000 | Loss: 0.00000850
Iteration 174/1000 | Loss: 0.00000850
Iteration 175/1000 | Loss: 0.00000850
Iteration 176/1000 | Loss: 0.00000850
Iteration 177/1000 | Loss: 0.00000850
Iteration 178/1000 | Loss: 0.00000850
Iteration 179/1000 | Loss: 0.00000850
Iteration 180/1000 | Loss: 0.00000850
Iteration 181/1000 | Loss: 0.00000850
Iteration 182/1000 | Loss: 0.00000850
Iteration 183/1000 | Loss: 0.00000850
Iteration 184/1000 | Loss: 0.00000850
Iteration 185/1000 | Loss: 0.00000850
Iteration 186/1000 | Loss: 0.00000850
Iteration 187/1000 | Loss: 0.00000849
Iteration 188/1000 | Loss: 0.00000849
Iteration 189/1000 | Loss: 0.00000849
Iteration 190/1000 | Loss: 0.00000849
Iteration 191/1000 | Loss: 0.00000849
Iteration 192/1000 | Loss: 0.00000849
Iteration 193/1000 | Loss: 0.00000849
Iteration 194/1000 | Loss: 0.00000849
Iteration 195/1000 | Loss: 0.00000849
Iteration 196/1000 | Loss: 0.00000849
Iteration 197/1000 | Loss: 0.00000849
Iteration 198/1000 | Loss: 0.00000849
Iteration 199/1000 | Loss: 0.00000849
Iteration 200/1000 | Loss: 0.00000849
Iteration 201/1000 | Loss: 0.00000849
Iteration 202/1000 | Loss: 0.00000849
Iteration 203/1000 | Loss: 0.00000849
Iteration 204/1000 | Loss: 0.00000849
Iteration 205/1000 | Loss: 0.00000849
Iteration 206/1000 | Loss: 0.00000849
Iteration 207/1000 | Loss: 0.00000849
Iteration 208/1000 | Loss: 0.00000849
Iteration 209/1000 | Loss: 0.00000849
Iteration 210/1000 | Loss: 0.00000849
Iteration 211/1000 | Loss: 0.00000849
Iteration 212/1000 | Loss: 0.00000849
Iteration 213/1000 | Loss: 0.00000849
Iteration 214/1000 | Loss: 0.00000849
Iteration 215/1000 | Loss: 0.00000849
Iteration 216/1000 | Loss: 0.00000849
Iteration 217/1000 | Loss: 0.00000849
Iteration 218/1000 | Loss: 0.00000849
Iteration 219/1000 | Loss: 0.00000849
Iteration 220/1000 | Loss: 0.00000849
Iteration 221/1000 | Loss: 0.00000849
Iteration 222/1000 | Loss: 0.00000849
Iteration 223/1000 | Loss: 0.00000849
Iteration 224/1000 | Loss: 0.00000849
Iteration 225/1000 | Loss: 0.00000849
Iteration 226/1000 | Loss: 0.00000849
Iteration 227/1000 | Loss: 0.00000849
Iteration 228/1000 | Loss: 0.00000849
Iteration 229/1000 | Loss: 0.00000849
Iteration 230/1000 | Loss: 0.00000849
Iteration 231/1000 | Loss: 0.00000849
Iteration 232/1000 | Loss: 0.00000849
Iteration 233/1000 | Loss: 0.00000849
Iteration 234/1000 | Loss: 0.00000849
Iteration 235/1000 | Loss: 0.00000849
Iteration 236/1000 | Loss: 0.00000849
Iteration 237/1000 | Loss: 0.00000849
Iteration 238/1000 | Loss: 0.00000849
Iteration 239/1000 | Loss: 0.00000849
Iteration 240/1000 | Loss: 0.00000849
Iteration 241/1000 | Loss: 0.00000849
Iteration 242/1000 | Loss: 0.00000849
Iteration 243/1000 | Loss: 0.00000849
Iteration 244/1000 | Loss: 0.00000849
Iteration 245/1000 | Loss: 0.00000849
Iteration 246/1000 | Loss: 0.00000849
Iteration 247/1000 | Loss: 0.00000849
Iteration 248/1000 | Loss: 0.00000849
Iteration 249/1000 | Loss: 0.00000849
Iteration 250/1000 | Loss: 0.00000849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [8.492416782246437e-06, 8.492416782246437e-06, 8.492416782246437e-06, 8.492416782246437e-06, 8.492416782246437e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.492416782246437e-06

Optimization complete. Final v2v error: 2.463149309158325 mm

Highest mean error: 3.1535160541534424 mm for frame 96

Lowest mean error: 2.2066197395324707 mm for frame 42

Saving results

Total time: 114.82196497917175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455488
Iteration 2/25 | Loss: 0.00108614
Iteration 3/25 | Loss: 0.00099975
Iteration 4/25 | Loss: 0.00099418
Iteration 5/25 | Loss: 0.00099285
Iteration 6/25 | Loss: 0.00099248
Iteration 7/25 | Loss: 0.00099248
Iteration 8/25 | Loss: 0.00099248
Iteration 9/25 | Loss: 0.00099248
Iteration 10/25 | Loss: 0.00099248
Iteration 11/25 | Loss: 0.00099248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009924792684614658, 0.0009924792684614658, 0.0009924792684614658, 0.0009924792684614658, 0.0009924792684614658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009924792684614658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38492823
Iteration 2/25 | Loss: 0.00078524
Iteration 3/25 | Loss: 0.00078522
Iteration 4/25 | Loss: 0.00078522
Iteration 5/25 | Loss: 0.00078522
Iteration 6/25 | Loss: 0.00078522
Iteration 7/25 | Loss: 0.00078522
Iteration 8/25 | Loss: 0.00078522
Iteration 9/25 | Loss: 0.00078522
Iteration 10/25 | Loss: 0.00078522
Iteration 11/25 | Loss: 0.00078522
Iteration 12/25 | Loss: 0.00078522
Iteration 13/25 | Loss: 0.00078522
Iteration 14/25 | Loss: 0.00078522
Iteration 15/25 | Loss: 0.00078522
Iteration 16/25 | Loss: 0.00078522
Iteration 17/25 | Loss: 0.00078522
Iteration 18/25 | Loss: 0.00078522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007852193084545434, 0.0007852193084545434, 0.0007852193084545434, 0.0007852193084545434, 0.0007852193084545434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007852193084545434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078522
Iteration 2/1000 | Loss: 0.00002966
Iteration 3/1000 | Loss: 0.00001643
Iteration 4/1000 | Loss: 0.00001187
Iteration 5/1000 | Loss: 0.00001031
Iteration 6/1000 | Loss: 0.00000958
Iteration 7/1000 | Loss: 0.00000918
Iteration 8/1000 | Loss: 0.00000882
Iteration 9/1000 | Loss: 0.00000869
Iteration 10/1000 | Loss: 0.00000851
Iteration 11/1000 | Loss: 0.00000847
Iteration 12/1000 | Loss: 0.00000845
Iteration 13/1000 | Loss: 0.00000845
Iteration 14/1000 | Loss: 0.00000845
Iteration 15/1000 | Loss: 0.00000844
Iteration 16/1000 | Loss: 0.00000844
Iteration 17/1000 | Loss: 0.00000841
Iteration 18/1000 | Loss: 0.00000841
Iteration 19/1000 | Loss: 0.00000841
Iteration 20/1000 | Loss: 0.00000841
Iteration 21/1000 | Loss: 0.00000840
Iteration 22/1000 | Loss: 0.00000839
Iteration 23/1000 | Loss: 0.00000838
Iteration 24/1000 | Loss: 0.00000836
Iteration 25/1000 | Loss: 0.00000836
Iteration 26/1000 | Loss: 0.00000835
Iteration 27/1000 | Loss: 0.00000835
Iteration 28/1000 | Loss: 0.00000834
Iteration 29/1000 | Loss: 0.00000832
Iteration 30/1000 | Loss: 0.00000832
Iteration 31/1000 | Loss: 0.00000831
Iteration 32/1000 | Loss: 0.00000830
Iteration 33/1000 | Loss: 0.00000830
Iteration 34/1000 | Loss: 0.00000830
Iteration 35/1000 | Loss: 0.00000830
Iteration 36/1000 | Loss: 0.00000829
Iteration 37/1000 | Loss: 0.00000829
Iteration 38/1000 | Loss: 0.00000829
Iteration 39/1000 | Loss: 0.00000828
Iteration 40/1000 | Loss: 0.00000828
Iteration 41/1000 | Loss: 0.00000827
Iteration 42/1000 | Loss: 0.00000827
Iteration 43/1000 | Loss: 0.00000825
Iteration 44/1000 | Loss: 0.00000825
Iteration 45/1000 | Loss: 0.00000825
Iteration 46/1000 | Loss: 0.00000825
Iteration 47/1000 | Loss: 0.00000824
Iteration 48/1000 | Loss: 0.00000824
Iteration 49/1000 | Loss: 0.00000824
Iteration 50/1000 | Loss: 0.00000823
Iteration 51/1000 | Loss: 0.00000822
Iteration 52/1000 | Loss: 0.00000822
Iteration 53/1000 | Loss: 0.00000822
Iteration 54/1000 | Loss: 0.00000822
Iteration 55/1000 | Loss: 0.00000822
Iteration 56/1000 | Loss: 0.00000821
Iteration 57/1000 | Loss: 0.00000821
Iteration 58/1000 | Loss: 0.00000821
Iteration 59/1000 | Loss: 0.00000820
Iteration 60/1000 | Loss: 0.00000820
Iteration 61/1000 | Loss: 0.00000820
Iteration 62/1000 | Loss: 0.00000819
Iteration 63/1000 | Loss: 0.00000818
Iteration 64/1000 | Loss: 0.00000818
Iteration 65/1000 | Loss: 0.00000817
Iteration 66/1000 | Loss: 0.00000817
Iteration 67/1000 | Loss: 0.00000817
Iteration 68/1000 | Loss: 0.00000816
Iteration 69/1000 | Loss: 0.00000816
Iteration 70/1000 | Loss: 0.00000816
Iteration 71/1000 | Loss: 0.00000815
Iteration 72/1000 | Loss: 0.00000815
Iteration 73/1000 | Loss: 0.00000813
Iteration 74/1000 | Loss: 0.00000812
Iteration 75/1000 | Loss: 0.00000812
Iteration 76/1000 | Loss: 0.00000812
Iteration 77/1000 | Loss: 0.00000812
Iteration 78/1000 | Loss: 0.00000812
Iteration 79/1000 | Loss: 0.00000812
Iteration 80/1000 | Loss: 0.00000812
Iteration 81/1000 | Loss: 0.00000812
Iteration 82/1000 | Loss: 0.00000812
Iteration 83/1000 | Loss: 0.00000812
Iteration 84/1000 | Loss: 0.00000811
Iteration 85/1000 | Loss: 0.00000811
Iteration 86/1000 | Loss: 0.00000811
Iteration 87/1000 | Loss: 0.00000811
Iteration 88/1000 | Loss: 0.00000811
Iteration 89/1000 | Loss: 0.00000810
Iteration 90/1000 | Loss: 0.00000810
Iteration 91/1000 | Loss: 0.00000810
Iteration 92/1000 | Loss: 0.00000810
Iteration 93/1000 | Loss: 0.00000809
Iteration 94/1000 | Loss: 0.00000809
Iteration 95/1000 | Loss: 0.00000809
Iteration 96/1000 | Loss: 0.00000809
Iteration 97/1000 | Loss: 0.00000809
Iteration 98/1000 | Loss: 0.00000809
Iteration 99/1000 | Loss: 0.00000809
Iteration 100/1000 | Loss: 0.00000808
Iteration 101/1000 | Loss: 0.00000808
Iteration 102/1000 | Loss: 0.00000808
Iteration 103/1000 | Loss: 0.00000808
Iteration 104/1000 | Loss: 0.00000808
Iteration 105/1000 | Loss: 0.00000808
Iteration 106/1000 | Loss: 0.00000808
Iteration 107/1000 | Loss: 0.00000807
Iteration 108/1000 | Loss: 0.00000807
Iteration 109/1000 | Loss: 0.00000807
Iteration 110/1000 | Loss: 0.00000807
Iteration 111/1000 | Loss: 0.00000807
Iteration 112/1000 | Loss: 0.00000807
Iteration 113/1000 | Loss: 0.00000807
Iteration 114/1000 | Loss: 0.00000807
Iteration 115/1000 | Loss: 0.00000807
Iteration 116/1000 | Loss: 0.00000807
Iteration 117/1000 | Loss: 0.00000807
Iteration 118/1000 | Loss: 0.00000807
Iteration 119/1000 | Loss: 0.00000807
Iteration 120/1000 | Loss: 0.00000807
Iteration 121/1000 | Loss: 0.00000807
Iteration 122/1000 | Loss: 0.00000807
Iteration 123/1000 | Loss: 0.00000806
Iteration 124/1000 | Loss: 0.00000806
Iteration 125/1000 | Loss: 0.00000806
Iteration 126/1000 | Loss: 0.00000806
Iteration 127/1000 | Loss: 0.00000806
Iteration 128/1000 | Loss: 0.00000806
Iteration 129/1000 | Loss: 0.00000806
Iteration 130/1000 | Loss: 0.00000806
Iteration 131/1000 | Loss: 0.00000805
Iteration 132/1000 | Loss: 0.00000805
Iteration 133/1000 | Loss: 0.00000805
Iteration 134/1000 | Loss: 0.00000805
Iteration 135/1000 | Loss: 0.00000805
Iteration 136/1000 | Loss: 0.00000805
Iteration 137/1000 | Loss: 0.00000805
Iteration 138/1000 | Loss: 0.00000805
Iteration 139/1000 | Loss: 0.00000805
Iteration 140/1000 | Loss: 0.00000805
Iteration 141/1000 | Loss: 0.00000805
Iteration 142/1000 | Loss: 0.00000805
Iteration 143/1000 | Loss: 0.00000805
Iteration 144/1000 | Loss: 0.00000805
Iteration 145/1000 | Loss: 0.00000804
Iteration 146/1000 | Loss: 0.00000804
Iteration 147/1000 | Loss: 0.00000804
Iteration 148/1000 | Loss: 0.00000804
Iteration 149/1000 | Loss: 0.00000804
Iteration 150/1000 | Loss: 0.00000804
Iteration 151/1000 | Loss: 0.00000804
Iteration 152/1000 | Loss: 0.00000804
Iteration 153/1000 | Loss: 0.00000804
Iteration 154/1000 | Loss: 0.00000804
Iteration 155/1000 | Loss: 0.00000804
Iteration 156/1000 | Loss: 0.00000804
Iteration 157/1000 | Loss: 0.00000803
Iteration 158/1000 | Loss: 0.00000803
Iteration 159/1000 | Loss: 0.00000803
Iteration 160/1000 | Loss: 0.00000803
Iteration 161/1000 | Loss: 0.00000803
Iteration 162/1000 | Loss: 0.00000803
Iteration 163/1000 | Loss: 0.00000803
Iteration 164/1000 | Loss: 0.00000803
Iteration 165/1000 | Loss: 0.00000803
Iteration 166/1000 | Loss: 0.00000803
Iteration 167/1000 | Loss: 0.00000803
Iteration 168/1000 | Loss: 0.00000803
Iteration 169/1000 | Loss: 0.00000803
Iteration 170/1000 | Loss: 0.00000803
Iteration 171/1000 | Loss: 0.00000803
Iteration 172/1000 | Loss: 0.00000803
Iteration 173/1000 | Loss: 0.00000802
Iteration 174/1000 | Loss: 0.00000802
Iteration 175/1000 | Loss: 0.00000802
Iteration 176/1000 | Loss: 0.00000802
Iteration 177/1000 | Loss: 0.00000802
Iteration 178/1000 | Loss: 0.00000802
Iteration 179/1000 | Loss: 0.00000802
Iteration 180/1000 | Loss: 0.00000802
Iteration 181/1000 | Loss: 0.00000802
Iteration 182/1000 | Loss: 0.00000802
Iteration 183/1000 | Loss: 0.00000802
Iteration 184/1000 | Loss: 0.00000802
Iteration 185/1000 | Loss: 0.00000802
Iteration 186/1000 | Loss: 0.00000802
Iteration 187/1000 | Loss: 0.00000802
Iteration 188/1000 | Loss: 0.00000802
Iteration 189/1000 | Loss: 0.00000802
Iteration 190/1000 | Loss: 0.00000802
Iteration 191/1000 | Loss: 0.00000802
Iteration 192/1000 | Loss: 0.00000802
Iteration 193/1000 | Loss: 0.00000802
Iteration 194/1000 | Loss: 0.00000802
Iteration 195/1000 | Loss: 0.00000802
Iteration 196/1000 | Loss: 0.00000802
Iteration 197/1000 | Loss: 0.00000802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [8.021788744372316e-06, 8.021788744372316e-06, 8.021788744372316e-06, 8.021788744372316e-06, 8.021788744372316e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.021788744372316e-06

Optimization complete. Final v2v error: 2.3658533096313477 mm

Highest mean error: 2.7660603523254395 mm for frame 60

Lowest mean error: 2.1899325847625732 mm for frame 98

Saving results

Total time: 34.0799503326416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684185
Iteration 2/25 | Loss: 0.00149550
Iteration 3/25 | Loss: 0.00118702
Iteration 4/25 | Loss: 0.00115057
Iteration 5/25 | Loss: 0.00114389
Iteration 6/25 | Loss: 0.00112141
Iteration 7/25 | Loss: 0.00111828
Iteration 8/25 | Loss: 0.00110454
Iteration 9/25 | Loss: 0.00108534
Iteration 10/25 | Loss: 0.00107286
Iteration 11/25 | Loss: 0.00106512
Iteration 12/25 | Loss: 0.00106316
Iteration 13/25 | Loss: 0.00106133
Iteration 14/25 | Loss: 0.00106155
Iteration 15/25 | Loss: 0.00106177
Iteration 16/25 | Loss: 0.00106132
Iteration 17/25 | Loss: 0.00106140
Iteration 18/25 | Loss: 0.00106113
Iteration 19/25 | Loss: 0.00106115
Iteration 20/25 | Loss: 0.00106122
Iteration 21/25 | Loss: 0.00106122
Iteration 22/25 | Loss: 0.00106122
Iteration 23/25 | Loss: 0.00106109
Iteration 24/25 | Loss: 0.00106150
Iteration 25/25 | Loss: 0.00106123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28674936
Iteration 2/25 | Loss: 0.00079519
Iteration 3/25 | Loss: 0.00079516
Iteration 4/25 | Loss: 0.00079516
Iteration 5/25 | Loss: 0.00079516
Iteration 6/25 | Loss: 0.00079516
Iteration 7/25 | Loss: 0.00079516
Iteration 8/25 | Loss: 0.00079516
Iteration 9/25 | Loss: 0.00079516
Iteration 10/25 | Loss: 0.00079516
Iteration 11/25 | Loss: 0.00079516
Iteration 12/25 | Loss: 0.00079516
Iteration 13/25 | Loss: 0.00079516
Iteration 14/25 | Loss: 0.00079516
Iteration 15/25 | Loss: 0.00079516
Iteration 16/25 | Loss: 0.00079516
Iteration 17/25 | Loss: 0.00079516
Iteration 18/25 | Loss: 0.00079516
Iteration 19/25 | Loss: 0.00079516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007951559382490814, 0.0007951559382490814, 0.0007951559382490814, 0.0007951559382490814, 0.0007951559382490814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007951559382490814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079516
Iteration 2/1000 | Loss: 0.00004366
Iteration 3/1000 | Loss: 0.00004073
Iteration 4/1000 | Loss: 0.00003983
Iteration 5/1000 | Loss: 0.00002974
Iteration 6/1000 | Loss: 0.00002726
Iteration 7/1000 | Loss: 0.00003212
Iteration 8/1000 | Loss: 0.00002510
Iteration 9/1000 | Loss: 0.00002105
Iteration 10/1000 | Loss: 0.00002443
Iteration 11/1000 | Loss: 0.00002618
Iteration 12/1000 | Loss: 0.00002949
Iteration 13/1000 | Loss: 0.00003164
Iteration 14/1000 | Loss: 0.00002581
Iteration 15/1000 | Loss: 0.00002210
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00001988
Iteration 18/1000 | Loss: 0.00001900
Iteration 19/1000 | Loss: 0.00001826
Iteration 20/1000 | Loss: 0.00001789
Iteration 21/1000 | Loss: 0.00001756
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001716
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001714
Iteration 33/1000 | Loss: 0.00001714
Iteration 34/1000 | Loss: 0.00001714
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001713
Iteration 37/1000 | Loss: 0.00001713
Iteration 38/1000 | Loss: 0.00001713
Iteration 39/1000 | Loss: 0.00001713
Iteration 40/1000 | Loss: 0.00001712
Iteration 41/1000 | Loss: 0.00001712
Iteration 42/1000 | Loss: 0.00001712
Iteration 43/1000 | Loss: 0.00001712
Iteration 44/1000 | Loss: 0.00001712
Iteration 45/1000 | Loss: 0.00001712
Iteration 46/1000 | Loss: 0.00001711
Iteration 47/1000 | Loss: 0.00001711
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001710
Iteration 50/1000 | Loss: 0.00001710
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001707
Iteration 59/1000 | Loss: 0.00001707
Iteration 60/1000 | Loss: 0.00001706
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001706
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001705
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001704
Iteration 77/1000 | Loss: 0.00001704
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001702
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001700
Iteration 86/1000 | Loss: 0.00001700
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001700
Iteration 90/1000 | Loss: 0.00001699
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001698
Iteration 95/1000 | Loss: 0.00001698
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001698
Iteration 102/1000 | Loss: 0.00001698
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001697
Iteration 105/1000 | Loss: 0.00001697
Iteration 106/1000 | Loss: 0.00001697
Iteration 107/1000 | Loss: 0.00001697
Iteration 108/1000 | Loss: 0.00001697
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001696
Iteration 112/1000 | Loss: 0.00001696
Iteration 113/1000 | Loss: 0.00001696
Iteration 114/1000 | Loss: 0.00001696
Iteration 115/1000 | Loss: 0.00001696
Iteration 116/1000 | Loss: 0.00001696
Iteration 117/1000 | Loss: 0.00001696
Iteration 118/1000 | Loss: 0.00001696
Iteration 119/1000 | Loss: 0.00001695
Iteration 120/1000 | Loss: 0.00001695
Iteration 121/1000 | Loss: 0.00001695
Iteration 122/1000 | Loss: 0.00001695
Iteration 123/1000 | Loss: 0.00001695
Iteration 124/1000 | Loss: 0.00001695
Iteration 125/1000 | Loss: 0.00001695
Iteration 126/1000 | Loss: 0.00001694
Iteration 127/1000 | Loss: 0.00001694
Iteration 128/1000 | Loss: 0.00001694
Iteration 129/1000 | Loss: 0.00001694
Iteration 130/1000 | Loss: 0.00001694
Iteration 131/1000 | Loss: 0.00001694
Iteration 132/1000 | Loss: 0.00001694
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001693
Iteration 135/1000 | Loss: 0.00001693
Iteration 136/1000 | Loss: 0.00001693
Iteration 137/1000 | Loss: 0.00001692
Iteration 138/1000 | Loss: 0.00001692
Iteration 139/1000 | Loss: 0.00001692
Iteration 140/1000 | Loss: 0.00001691
Iteration 141/1000 | Loss: 0.00001691
Iteration 142/1000 | Loss: 0.00001691
Iteration 143/1000 | Loss: 0.00001691
Iteration 144/1000 | Loss: 0.00001691
Iteration 145/1000 | Loss: 0.00001691
Iteration 146/1000 | Loss: 0.00001691
Iteration 147/1000 | Loss: 0.00001691
Iteration 148/1000 | Loss: 0.00001691
Iteration 149/1000 | Loss: 0.00001690
Iteration 150/1000 | Loss: 0.00001690
Iteration 151/1000 | Loss: 0.00001690
Iteration 152/1000 | Loss: 0.00001690
Iteration 153/1000 | Loss: 0.00001689
Iteration 154/1000 | Loss: 0.00001689
Iteration 155/1000 | Loss: 0.00001689
Iteration 156/1000 | Loss: 0.00001689
Iteration 157/1000 | Loss: 0.00001689
Iteration 158/1000 | Loss: 0.00001689
Iteration 159/1000 | Loss: 0.00001689
Iteration 160/1000 | Loss: 0.00001688
Iteration 161/1000 | Loss: 0.00001688
Iteration 162/1000 | Loss: 0.00001688
Iteration 163/1000 | Loss: 0.00001688
Iteration 164/1000 | Loss: 0.00001687
Iteration 165/1000 | Loss: 0.00001687
Iteration 166/1000 | Loss: 0.00001687
Iteration 167/1000 | Loss: 0.00001687
Iteration 168/1000 | Loss: 0.00001687
Iteration 169/1000 | Loss: 0.00001687
Iteration 170/1000 | Loss: 0.00001687
Iteration 171/1000 | Loss: 0.00001687
Iteration 172/1000 | Loss: 0.00001687
Iteration 173/1000 | Loss: 0.00001687
Iteration 174/1000 | Loss: 0.00001687
Iteration 175/1000 | Loss: 0.00001687
Iteration 176/1000 | Loss: 0.00001687
Iteration 177/1000 | Loss: 0.00001687
Iteration 178/1000 | Loss: 0.00001686
Iteration 179/1000 | Loss: 0.00001686
Iteration 180/1000 | Loss: 0.00001686
Iteration 181/1000 | Loss: 0.00001686
Iteration 182/1000 | Loss: 0.00001686
Iteration 183/1000 | Loss: 0.00001686
Iteration 184/1000 | Loss: 0.00001686
Iteration 185/1000 | Loss: 0.00001686
Iteration 186/1000 | Loss: 0.00001686
Iteration 187/1000 | Loss: 0.00001686
Iteration 188/1000 | Loss: 0.00001686
Iteration 189/1000 | Loss: 0.00001686
Iteration 190/1000 | Loss: 0.00001686
Iteration 191/1000 | Loss: 0.00001686
Iteration 192/1000 | Loss: 0.00001686
Iteration 193/1000 | Loss: 0.00001686
Iteration 194/1000 | Loss: 0.00001686
Iteration 195/1000 | Loss: 0.00001686
Iteration 196/1000 | Loss: 0.00001686
Iteration 197/1000 | Loss: 0.00001686
Iteration 198/1000 | Loss: 0.00001686
Iteration 199/1000 | Loss: 0.00001686
Iteration 200/1000 | Loss: 0.00001686
Iteration 201/1000 | Loss: 0.00001686
Iteration 202/1000 | Loss: 0.00001686
Iteration 203/1000 | Loss: 0.00001686
Iteration 204/1000 | Loss: 0.00001686
Iteration 205/1000 | Loss: 0.00001686
Iteration 206/1000 | Loss: 0.00001686
Iteration 207/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.685594361333642e-05, 1.685594361333642e-05, 1.685594361333642e-05, 1.685594361333642e-05, 1.685594361333642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.685594361333642e-05

Optimization complete. Final v2v error: 3.294240713119507 mm

Highest mean error: 11.233099937438965 mm for frame 16

Lowest mean error: 2.5744128227233887 mm for frame 191

Saving results

Total time: 97.2525155544281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052606
Iteration 2/25 | Loss: 0.00219130
Iteration 3/25 | Loss: 0.00179652
Iteration 4/25 | Loss: 0.00149729
Iteration 5/25 | Loss: 0.00147432
Iteration 6/25 | Loss: 0.00138582
Iteration 7/25 | Loss: 0.00127547
Iteration 8/25 | Loss: 0.00120734
Iteration 9/25 | Loss: 0.00117640
Iteration 10/25 | Loss: 0.00114269
Iteration 11/25 | Loss: 0.00111863
Iteration 12/25 | Loss: 0.00110674
Iteration 13/25 | Loss: 0.00110175
Iteration 14/25 | Loss: 0.00111028
Iteration 15/25 | Loss: 0.00109316
Iteration 16/25 | Loss: 0.00108124
Iteration 17/25 | Loss: 0.00107650
Iteration 18/25 | Loss: 0.00108281
Iteration 19/25 | Loss: 0.00107186
Iteration 20/25 | Loss: 0.00107642
Iteration 21/25 | Loss: 0.00107188
Iteration 22/25 | Loss: 0.00106160
Iteration 23/25 | Loss: 0.00106737
Iteration 24/25 | Loss: 0.00107266
Iteration 25/25 | Loss: 0.00106274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23571599
Iteration 2/25 | Loss: 0.00159131
Iteration 3/25 | Loss: 0.00159131
Iteration 4/25 | Loss: 0.00159131
Iteration 5/25 | Loss: 0.00159131
Iteration 6/25 | Loss: 0.00159131
Iteration 7/25 | Loss: 0.00159131
Iteration 8/25 | Loss: 0.00159131
Iteration 9/25 | Loss: 0.00159131
Iteration 10/25 | Loss: 0.00159131
Iteration 11/25 | Loss: 0.00159131
Iteration 12/25 | Loss: 0.00159131
Iteration 13/25 | Loss: 0.00159131
Iteration 14/25 | Loss: 0.00159131
Iteration 15/25 | Loss: 0.00159131
Iteration 16/25 | Loss: 0.00159131
Iteration 17/25 | Loss: 0.00159131
Iteration 18/25 | Loss: 0.00159131
Iteration 19/25 | Loss: 0.00159131
Iteration 20/25 | Loss: 0.00159131
Iteration 21/25 | Loss: 0.00159131
Iteration 22/25 | Loss: 0.00159131
Iteration 23/25 | Loss: 0.00159131
Iteration 24/25 | Loss: 0.00159131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015913061797618866, 0.0015913061797618866, 0.0015913061797618866, 0.0015913061797618866, 0.0015913061797618866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015913061797618866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159131
Iteration 2/1000 | Loss: 0.00007715
Iteration 3/1000 | Loss: 0.00018525
Iteration 4/1000 | Loss: 0.00015533
Iteration 5/1000 | Loss: 0.00016830
Iteration 6/1000 | Loss: 0.00029918
Iteration 7/1000 | Loss: 0.00005913
Iteration 8/1000 | Loss: 0.00006227
Iteration 9/1000 | Loss: 0.00014299
Iteration 10/1000 | Loss: 0.00011486
Iteration 11/1000 | Loss: 0.00011533
Iteration 12/1000 | Loss: 0.00017479
Iteration 13/1000 | Loss: 0.00019337
Iteration 14/1000 | Loss: 0.00020069
Iteration 15/1000 | Loss: 0.00013471
Iteration 16/1000 | Loss: 0.00020358
Iteration 17/1000 | Loss: 0.00019092
Iteration 18/1000 | Loss: 0.00023930
Iteration 19/1000 | Loss: 0.00018222
Iteration 20/1000 | Loss: 0.00020360
Iteration 21/1000 | Loss: 0.00017587
Iteration 22/1000 | Loss: 0.00016285
Iteration 23/1000 | Loss: 0.00024535
Iteration 24/1000 | Loss: 0.00028583
Iteration 25/1000 | Loss: 0.00019290
Iteration 26/1000 | Loss: 0.00022127
Iteration 27/1000 | Loss: 0.00020519
Iteration 28/1000 | Loss: 0.00031404
Iteration 29/1000 | Loss: 0.00005175
Iteration 30/1000 | Loss: 0.00003512
Iteration 31/1000 | Loss: 0.00004552
Iteration 32/1000 | Loss: 0.00036503
Iteration 33/1000 | Loss: 0.00040405
Iteration 34/1000 | Loss: 0.00013288
Iteration 35/1000 | Loss: 0.00009741
Iteration 36/1000 | Loss: 0.00020425
Iteration 37/1000 | Loss: 0.00037717
Iteration 38/1000 | Loss: 0.00032601
Iteration 39/1000 | Loss: 0.00023419
Iteration 40/1000 | Loss: 0.00018509
Iteration 41/1000 | Loss: 0.00004232
Iteration 42/1000 | Loss: 0.00004046
Iteration 43/1000 | Loss: 0.00003453
Iteration 44/1000 | Loss: 0.00003104
Iteration 45/1000 | Loss: 0.00002965
Iteration 46/1000 | Loss: 0.00020417
Iteration 47/1000 | Loss: 0.00129267
Iteration 48/1000 | Loss: 0.00067202
Iteration 49/1000 | Loss: 0.00056785
Iteration 50/1000 | Loss: 0.00034862
Iteration 51/1000 | Loss: 0.00060603
Iteration 52/1000 | Loss: 0.00055073
Iteration 53/1000 | Loss: 0.00041519
Iteration 54/1000 | Loss: 0.00114574
Iteration 55/1000 | Loss: 0.00050510
Iteration 56/1000 | Loss: 0.00008871
Iteration 57/1000 | Loss: 0.00024134
Iteration 58/1000 | Loss: 0.00048982
Iteration 59/1000 | Loss: 0.00017700
Iteration 60/1000 | Loss: 0.00011526
Iteration 61/1000 | Loss: 0.00042549
Iteration 62/1000 | Loss: 0.00032458
Iteration 63/1000 | Loss: 0.00025768
Iteration 64/1000 | Loss: 0.00002926
Iteration 65/1000 | Loss: 0.00002670
Iteration 66/1000 | Loss: 0.00002300
Iteration 67/1000 | Loss: 0.00002111
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00001810
Iteration 70/1000 | Loss: 0.00035165
Iteration 71/1000 | Loss: 0.00004681
Iteration 72/1000 | Loss: 0.00002925
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00001629
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001421
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001265
Iteration 83/1000 | Loss: 0.00001265
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001239
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001231
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001223
Iteration 93/1000 | Loss: 0.00001222
Iteration 94/1000 | Loss: 0.00001220
Iteration 95/1000 | Loss: 0.00001220
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001219
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001217
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001212
Iteration 105/1000 | Loss: 0.00029582
Iteration 106/1000 | Loss: 0.00030744
Iteration 107/1000 | Loss: 0.00002526
Iteration 108/1000 | Loss: 0.00001936
Iteration 109/1000 | Loss: 0.00021981
Iteration 110/1000 | Loss: 0.00002049
Iteration 111/1000 | Loss: 0.00001682
Iteration 112/1000 | Loss: 0.00001462
Iteration 113/1000 | Loss: 0.00024751
Iteration 114/1000 | Loss: 0.00013351
Iteration 115/1000 | Loss: 0.00023777
Iteration 116/1000 | Loss: 0.00018616
Iteration 117/1000 | Loss: 0.00004646
Iteration 118/1000 | Loss: 0.00001390
Iteration 119/1000 | Loss: 0.00023772
Iteration 120/1000 | Loss: 0.00015816
Iteration 121/1000 | Loss: 0.00021689
Iteration 122/1000 | Loss: 0.00043948
Iteration 123/1000 | Loss: 0.00090778
Iteration 124/1000 | Loss: 0.00024079
Iteration 125/1000 | Loss: 0.00007103
Iteration 126/1000 | Loss: 0.00002152
Iteration 127/1000 | Loss: 0.00001694
Iteration 128/1000 | Loss: 0.00001476
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001255
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001180
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001155
Iteration 136/1000 | Loss: 0.00001154
Iteration 137/1000 | Loss: 0.00001150
Iteration 138/1000 | Loss: 0.00001149
Iteration 139/1000 | Loss: 0.00001148
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001134
Iteration 142/1000 | Loss: 0.00001133
Iteration 143/1000 | Loss: 0.00001128
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001126
Iteration 146/1000 | Loss: 0.00001125
Iteration 147/1000 | Loss: 0.00001125
Iteration 148/1000 | Loss: 0.00001125
Iteration 149/1000 | Loss: 0.00001124
Iteration 150/1000 | Loss: 0.00001124
Iteration 151/1000 | Loss: 0.00001124
Iteration 152/1000 | Loss: 0.00001122
Iteration 153/1000 | Loss: 0.00001122
Iteration 154/1000 | Loss: 0.00001122
Iteration 155/1000 | Loss: 0.00001122
Iteration 156/1000 | Loss: 0.00001122
Iteration 157/1000 | Loss: 0.00001122
Iteration 158/1000 | Loss: 0.00001122
Iteration 159/1000 | Loss: 0.00001122
Iteration 160/1000 | Loss: 0.00001122
Iteration 161/1000 | Loss: 0.00001122
Iteration 162/1000 | Loss: 0.00001122
Iteration 163/1000 | Loss: 0.00001121
Iteration 164/1000 | Loss: 0.00001121
Iteration 165/1000 | Loss: 0.00001121
Iteration 166/1000 | Loss: 0.00001121
Iteration 167/1000 | Loss: 0.00001120
Iteration 168/1000 | Loss: 0.00001120
Iteration 169/1000 | Loss: 0.00001120
Iteration 170/1000 | Loss: 0.00001120
Iteration 171/1000 | Loss: 0.00001120
Iteration 172/1000 | Loss: 0.00001120
Iteration 173/1000 | Loss: 0.00001120
Iteration 174/1000 | Loss: 0.00001120
Iteration 175/1000 | Loss: 0.00001119
Iteration 176/1000 | Loss: 0.00001119
Iteration 177/1000 | Loss: 0.00001119
Iteration 178/1000 | Loss: 0.00001119
Iteration 179/1000 | Loss: 0.00001118
Iteration 180/1000 | Loss: 0.00001118
Iteration 181/1000 | Loss: 0.00001118
Iteration 182/1000 | Loss: 0.00001118
Iteration 183/1000 | Loss: 0.00001118
Iteration 184/1000 | Loss: 0.00001118
Iteration 185/1000 | Loss: 0.00001118
Iteration 186/1000 | Loss: 0.00001118
Iteration 187/1000 | Loss: 0.00001118
Iteration 188/1000 | Loss: 0.00001118
Iteration 189/1000 | Loss: 0.00001117
Iteration 190/1000 | Loss: 0.00001117
Iteration 191/1000 | Loss: 0.00001117
Iteration 192/1000 | Loss: 0.00001117
Iteration 193/1000 | Loss: 0.00001117
Iteration 194/1000 | Loss: 0.00001117
Iteration 195/1000 | Loss: 0.00001117
Iteration 196/1000 | Loss: 0.00001117
Iteration 197/1000 | Loss: 0.00001117
Iteration 198/1000 | Loss: 0.00001117
Iteration 199/1000 | Loss: 0.00001117
Iteration 200/1000 | Loss: 0.00001117
Iteration 201/1000 | Loss: 0.00001116
Iteration 202/1000 | Loss: 0.00001116
Iteration 203/1000 | Loss: 0.00001116
Iteration 204/1000 | Loss: 0.00001116
Iteration 205/1000 | Loss: 0.00001116
Iteration 206/1000 | Loss: 0.00001116
Iteration 207/1000 | Loss: 0.00001116
Iteration 208/1000 | Loss: 0.00001116
Iteration 209/1000 | Loss: 0.00001116
Iteration 210/1000 | Loss: 0.00001116
Iteration 211/1000 | Loss: 0.00001116
Iteration 212/1000 | Loss: 0.00001115
Iteration 213/1000 | Loss: 0.00001115
Iteration 214/1000 | Loss: 0.00001115
Iteration 215/1000 | Loss: 0.00001115
Iteration 216/1000 | Loss: 0.00001115
Iteration 217/1000 | Loss: 0.00001115
Iteration 218/1000 | Loss: 0.00001115
Iteration 219/1000 | Loss: 0.00001115
Iteration 220/1000 | Loss: 0.00001115
Iteration 221/1000 | Loss: 0.00001115
Iteration 222/1000 | Loss: 0.00001115
Iteration 223/1000 | Loss: 0.00001115
Iteration 224/1000 | Loss: 0.00001115
Iteration 225/1000 | Loss: 0.00001115
Iteration 226/1000 | Loss: 0.00001115
Iteration 227/1000 | Loss: 0.00001115
Iteration 228/1000 | Loss: 0.00001115
Iteration 229/1000 | Loss: 0.00001115
Iteration 230/1000 | Loss: 0.00001115
Iteration 231/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.1151200851600152e-05, 1.1151200851600152e-05, 1.1151200851600152e-05, 1.1151200851600152e-05, 1.1151200851600152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1151200851600152e-05

Optimization complete. Final v2v error: 2.8253936767578125 mm

Highest mean error: 3.5343916416168213 mm for frame 53

Lowest mean error: 2.5624663829803467 mm for frame 80

Saving results

Total time: 208.26976943016052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902562
Iteration 2/25 | Loss: 0.00232895
Iteration 3/25 | Loss: 0.00173438
Iteration 4/25 | Loss: 0.00145347
Iteration 5/25 | Loss: 0.00132464
Iteration 6/25 | Loss: 0.00131108
Iteration 7/25 | Loss: 0.00131336
Iteration 8/25 | Loss: 0.00132247
Iteration 9/25 | Loss: 0.00131420
Iteration 10/25 | Loss: 0.00130566
Iteration 11/25 | Loss: 0.00129771
Iteration 12/25 | Loss: 0.00127765
Iteration 13/25 | Loss: 0.00126788
Iteration 14/25 | Loss: 0.00125431
Iteration 15/25 | Loss: 0.00125556
Iteration 16/25 | Loss: 0.00125419
Iteration 17/25 | Loss: 0.00125372
Iteration 18/25 | Loss: 0.00125219
Iteration 19/25 | Loss: 0.00125192
Iteration 20/25 | Loss: 0.00125196
Iteration 21/25 | Loss: 0.00125139
Iteration 22/25 | Loss: 0.00124911
Iteration 23/25 | Loss: 0.00125200
Iteration 24/25 | Loss: 0.00124862
Iteration 25/25 | Loss: 0.00124857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02145660
Iteration 2/25 | Loss: 0.00089502
Iteration 3/25 | Loss: 0.00089501
Iteration 4/25 | Loss: 0.00089501
Iteration 5/25 | Loss: 0.00089501
Iteration 6/25 | Loss: 0.00089501
Iteration 7/25 | Loss: 0.00089501
Iteration 8/25 | Loss: 0.00089501
Iteration 9/25 | Loss: 0.00089501
Iteration 10/25 | Loss: 0.00089501
Iteration 11/25 | Loss: 0.00089501
Iteration 12/25 | Loss: 0.00089501
Iteration 13/25 | Loss: 0.00089501
Iteration 14/25 | Loss: 0.00089501
Iteration 15/25 | Loss: 0.00089501
Iteration 16/25 | Loss: 0.00089501
Iteration 17/25 | Loss: 0.00089501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008950105402618647, 0.0008950105402618647, 0.0008950105402618647, 0.0008950105402618647, 0.0008950105402618647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008950105402618647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089501
Iteration 2/1000 | Loss: 0.00014514
Iteration 3/1000 | Loss: 0.00016550
Iteration 4/1000 | Loss: 0.00018743
Iteration 5/1000 | Loss: 0.00015853
Iteration 6/1000 | Loss: 0.00015108
Iteration 7/1000 | Loss: 0.00020185
Iteration 8/1000 | Loss: 0.00018445
Iteration 9/1000 | Loss: 0.00021427
Iteration 10/1000 | Loss: 0.00015956
Iteration 11/1000 | Loss: 0.00019776
Iteration 12/1000 | Loss: 0.00014876
Iteration 13/1000 | Loss: 0.00020814
Iteration 14/1000 | Loss: 0.00021789
Iteration 15/1000 | Loss: 0.00024303
Iteration 16/1000 | Loss: 0.00023568
Iteration 17/1000 | Loss: 0.00027053
Iteration 18/1000 | Loss: 0.00019974
Iteration 19/1000 | Loss: 0.00018903
Iteration 20/1000 | Loss: 0.00019150
Iteration 21/1000 | Loss: 0.00021900
Iteration 22/1000 | Loss: 0.00026868
Iteration 23/1000 | Loss: 0.00025782
Iteration 24/1000 | Loss: 0.00032627
Iteration 25/1000 | Loss: 0.00016116
Iteration 26/1000 | Loss: 0.00019208
Iteration 27/1000 | Loss: 0.00024350
Iteration 28/1000 | Loss: 0.00023902
Iteration 29/1000 | Loss: 0.00029762
Iteration 30/1000 | Loss: 0.00016920
Iteration 31/1000 | Loss: 0.00022231
Iteration 32/1000 | Loss: 0.00026537
Iteration 33/1000 | Loss: 0.00022981
Iteration 34/1000 | Loss: 0.00023136
Iteration 35/1000 | Loss: 0.00023686
Iteration 36/1000 | Loss: 0.00024515
Iteration 37/1000 | Loss: 0.00018043
Iteration 38/1000 | Loss: 0.00019640
Iteration 39/1000 | Loss: 0.00015251
Iteration 40/1000 | Loss: 0.00019330
Iteration 41/1000 | Loss: 0.00021778
Iteration 42/1000 | Loss: 0.00024873
Iteration 43/1000 | Loss: 0.00025568
Iteration 44/1000 | Loss: 0.00024322
Iteration 45/1000 | Loss: 0.00024540
Iteration 46/1000 | Loss: 0.00024523
Iteration 47/1000 | Loss: 0.00022576
Iteration 48/1000 | Loss: 0.00019657
Iteration 49/1000 | Loss: 0.00019644
Iteration 50/1000 | Loss: 0.00022009
Iteration 51/1000 | Loss: 0.00019378
Iteration 52/1000 | Loss: 0.00020409
Iteration 53/1000 | Loss: 0.00027780
Iteration 54/1000 | Loss: 0.00027788
Iteration 55/1000 | Loss: 0.00030819
Iteration 56/1000 | Loss: 0.00022956
Iteration 57/1000 | Loss: 0.00022036
Iteration 58/1000 | Loss: 0.00023571
Iteration 59/1000 | Loss: 0.00019845
Iteration 60/1000 | Loss: 0.00018557
Iteration 61/1000 | Loss: 0.00021066
Iteration 62/1000 | Loss: 0.00021300
Iteration 63/1000 | Loss: 0.00024365
Iteration 64/1000 | Loss: 0.00027310
Iteration 65/1000 | Loss: 0.00016577
Iteration 66/1000 | Loss: 0.00017516
Iteration 67/1000 | Loss: 0.00022219
Iteration 68/1000 | Loss: 0.00021147
Iteration 69/1000 | Loss: 0.00022198
Iteration 70/1000 | Loss: 0.00024077
Iteration 71/1000 | Loss: 0.00015987
Iteration 72/1000 | Loss: 0.00016059
Iteration 73/1000 | Loss: 0.00016663
Iteration 74/1000 | Loss: 0.00018401
Iteration 75/1000 | Loss: 0.00022206
Iteration 76/1000 | Loss: 0.00017528
Iteration 77/1000 | Loss: 0.00017630
Iteration 78/1000 | Loss: 0.00009988
Iteration 79/1000 | Loss: 0.00016895
Iteration 80/1000 | Loss: 0.00015955
Iteration 81/1000 | Loss: 0.00010134
Iteration 82/1000 | Loss: 0.00013697
Iteration 83/1000 | Loss: 0.00018698
Iteration 84/1000 | Loss: 0.00012749
Iteration 85/1000 | Loss: 0.00013900
Iteration 86/1000 | Loss: 0.00016389
Iteration 87/1000 | Loss: 0.00016777
Iteration 88/1000 | Loss: 0.00017001
Iteration 89/1000 | Loss: 0.00018449
Iteration 90/1000 | Loss: 0.00016384
Iteration 91/1000 | Loss: 0.00017511
Iteration 92/1000 | Loss: 0.00018453
Iteration 93/1000 | Loss: 0.00019165
Iteration 94/1000 | Loss: 0.00014473
Iteration 95/1000 | Loss: 0.00012666
Iteration 96/1000 | Loss: 0.00019984
Iteration 97/1000 | Loss: 0.00024681
Iteration 98/1000 | Loss: 0.00009049
Iteration 99/1000 | Loss: 0.00016582
Iteration 100/1000 | Loss: 0.00014964
Iteration 101/1000 | Loss: 0.00014817
Iteration 102/1000 | Loss: 0.00014756
Iteration 103/1000 | Loss: 0.00017246
Iteration 104/1000 | Loss: 0.00015608
Iteration 105/1000 | Loss: 0.00016997
Iteration 106/1000 | Loss: 0.00010206
Iteration 107/1000 | Loss: 0.00010205
Iteration 108/1000 | Loss: 0.00011707
Iteration 109/1000 | Loss: 0.00008931
Iteration 110/1000 | Loss: 0.00010031
Iteration 111/1000 | Loss: 0.00011504
Iteration 112/1000 | Loss: 0.00011742
Iteration 113/1000 | Loss: 0.00012718
Iteration 114/1000 | Loss: 0.00014466
Iteration 115/1000 | Loss: 0.00018737
Iteration 116/1000 | Loss: 0.00013778
Iteration 117/1000 | Loss: 0.00011724
Iteration 118/1000 | Loss: 0.00014574
Iteration 119/1000 | Loss: 0.00010184
Iteration 120/1000 | Loss: 0.00009673
Iteration 121/1000 | Loss: 0.00010307
Iteration 122/1000 | Loss: 0.00010459
Iteration 123/1000 | Loss: 0.00010820
Iteration 124/1000 | Loss: 0.00011273
Iteration 125/1000 | Loss: 0.00013518
Iteration 126/1000 | Loss: 0.00012038
Iteration 127/1000 | Loss: 0.00015716
Iteration 128/1000 | Loss: 0.00012191
Iteration 129/1000 | Loss: 0.00013960
Iteration 130/1000 | Loss: 0.00014564
Iteration 131/1000 | Loss: 0.00015735
Iteration 132/1000 | Loss: 0.00012247
Iteration 133/1000 | Loss: 0.00010920
Iteration 134/1000 | Loss: 0.00013158
Iteration 135/1000 | Loss: 0.00010681
Iteration 136/1000 | Loss: 0.00014027
Iteration 137/1000 | Loss: 0.00015086
Iteration 138/1000 | Loss: 0.00011694
Iteration 139/1000 | Loss: 0.00013078
Iteration 140/1000 | Loss: 0.00016479
Iteration 141/1000 | Loss: 0.00015149
Iteration 142/1000 | Loss: 0.00013083
Iteration 143/1000 | Loss: 0.00005943
Iteration 144/1000 | Loss: 0.00006763
Iteration 145/1000 | Loss: 0.00008504
Iteration 146/1000 | Loss: 0.00012183
Iteration 147/1000 | Loss: 0.00008302
Iteration 148/1000 | Loss: 0.00009798
Iteration 149/1000 | Loss: 0.00006696
Iteration 150/1000 | Loss: 0.00008589
Iteration 151/1000 | Loss: 0.00007736
Iteration 152/1000 | Loss: 0.00010468
Iteration 153/1000 | Loss: 0.00013046
Iteration 154/1000 | Loss: 0.00012923
Iteration 155/1000 | Loss: 0.00013628
Iteration 156/1000 | Loss: 0.00011368
Iteration 157/1000 | Loss: 0.00008625
Iteration 158/1000 | Loss: 0.00007428
Iteration 159/1000 | Loss: 0.00007449
Iteration 160/1000 | Loss: 0.00011292
Iteration 161/1000 | Loss: 0.00012700
Iteration 162/1000 | Loss: 0.00010873
Iteration 163/1000 | Loss: 0.00010056
Iteration 164/1000 | Loss: 0.00013184
Iteration 165/1000 | Loss: 0.00011945
Iteration 166/1000 | Loss: 0.00010281
Iteration 167/1000 | Loss: 0.00010818
Iteration 168/1000 | Loss: 0.00010737
Iteration 169/1000 | Loss: 0.00011070
Iteration 170/1000 | Loss: 0.00011993
Iteration 171/1000 | Loss: 0.00011266
Iteration 172/1000 | Loss: 0.00011449
Iteration 173/1000 | Loss: 0.00011981
Iteration 174/1000 | Loss: 0.00010668
Iteration 175/1000 | Loss: 0.00011209
Iteration 176/1000 | Loss: 0.00009442
Iteration 177/1000 | Loss: 0.00013280
Iteration 178/1000 | Loss: 0.00011244
Iteration 179/1000 | Loss: 0.00013018
Iteration 180/1000 | Loss: 0.00011497
Iteration 181/1000 | Loss: 0.00010984
Iteration 182/1000 | Loss: 0.00013652
Iteration 183/1000 | Loss: 0.00013700
Iteration 184/1000 | Loss: 0.00020466
Iteration 185/1000 | Loss: 0.00016142
Iteration 186/1000 | Loss: 0.00020470
Iteration 187/1000 | Loss: 0.00010859
Iteration 188/1000 | Loss: 0.00009486
Iteration 189/1000 | Loss: 0.00007912
Iteration 190/1000 | Loss: 0.00012601
Iteration 191/1000 | Loss: 0.00013370
Iteration 192/1000 | Loss: 0.00012381
Iteration 193/1000 | Loss: 0.00013432
Iteration 194/1000 | Loss: 0.00012632
Iteration 195/1000 | Loss: 0.00013357
Iteration 196/1000 | Loss: 0.00012428
Iteration 197/1000 | Loss: 0.00009393
Iteration 198/1000 | Loss: 0.00008683
Iteration 199/1000 | Loss: 0.00005714
Iteration 200/1000 | Loss: 0.00018617
Iteration 201/1000 | Loss: 0.00014503
Iteration 202/1000 | Loss: 0.00007010
Iteration 203/1000 | Loss: 0.00015404
Iteration 204/1000 | Loss: 0.00009434
Iteration 205/1000 | Loss: 0.00007621
Iteration 206/1000 | Loss: 0.00007969
Iteration 207/1000 | Loss: 0.00006620
Iteration 208/1000 | Loss: 0.00004045
Iteration 209/1000 | Loss: 0.00007948
Iteration 210/1000 | Loss: 0.00006760
Iteration 211/1000 | Loss: 0.00006619
Iteration 212/1000 | Loss: 0.00004322
Iteration 213/1000 | Loss: 0.00008166
Iteration 214/1000 | Loss: 0.00005446
Iteration 215/1000 | Loss: 0.00005411
Iteration 216/1000 | Loss: 0.00005252
Iteration 217/1000 | Loss: 0.00005157
Iteration 218/1000 | Loss: 0.00005789
Iteration 219/1000 | Loss: 0.00006853
Iteration 220/1000 | Loss: 0.00005468
Iteration 221/1000 | Loss: 0.00003927
Iteration 222/1000 | Loss: 0.00005217
Iteration 223/1000 | Loss: 0.00004710
Iteration 224/1000 | Loss: 0.00005037
Iteration 225/1000 | Loss: 0.00004458
Iteration 226/1000 | Loss: 0.00004205
Iteration 227/1000 | Loss: 0.00004766
Iteration 228/1000 | Loss: 0.00005614
Iteration 229/1000 | Loss: 0.00005295
Iteration 230/1000 | Loss: 0.00005880
Iteration 231/1000 | Loss: 0.00006549
Iteration 232/1000 | Loss: 0.00005157
Iteration 233/1000 | Loss: 0.00008721
Iteration 234/1000 | Loss: 0.00005019
Iteration 235/1000 | Loss: 0.00003617
Iteration 236/1000 | Loss: 0.00004884
Iteration 237/1000 | Loss: 0.00004183
Iteration 238/1000 | Loss: 0.00005392
Iteration 239/1000 | Loss: 0.00003917
Iteration 240/1000 | Loss: 0.00004076
Iteration 241/1000 | Loss: 0.00004196
Iteration 242/1000 | Loss: 0.00004204
Iteration 243/1000 | Loss: 0.00005917
Iteration 244/1000 | Loss: 0.00007198
Iteration 245/1000 | Loss: 0.00003175
Iteration 246/1000 | Loss: 0.00004000
Iteration 247/1000 | Loss: 0.00005011
Iteration 248/1000 | Loss: 0.00002852
Iteration 249/1000 | Loss: 0.00002530
Iteration 250/1000 | Loss: 0.00013782
Iteration 251/1000 | Loss: 0.00005749
Iteration 252/1000 | Loss: 0.00010174
Iteration 253/1000 | Loss: 0.00006530
Iteration 254/1000 | Loss: 0.00008950
Iteration 255/1000 | Loss: 0.00005225
Iteration 256/1000 | Loss: 0.00004894
Iteration 257/1000 | Loss: 0.00008651
Iteration 258/1000 | Loss: 0.00004241
Iteration 259/1000 | Loss: 0.00008017
Iteration 260/1000 | Loss: 0.00003082
Iteration 261/1000 | Loss: 0.00009074
Iteration 262/1000 | Loss: 0.00004563
Iteration 263/1000 | Loss: 0.00010350
Iteration 264/1000 | Loss: 0.00003527
Iteration 265/1000 | Loss: 0.00003435
Iteration 266/1000 | Loss: 0.00002556
Iteration 267/1000 | Loss: 0.00002319
Iteration 268/1000 | Loss: 0.00002232
Iteration 269/1000 | Loss: 0.00002188
Iteration 270/1000 | Loss: 0.00002158
Iteration 271/1000 | Loss: 0.00002122
Iteration 272/1000 | Loss: 0.00004204
Iteration 273/1000 | Loss: 0.00003380
Iteration 274/1000 | Loss: 0.00003657
Iteration 275/1000 | Loss: 0.00002714
Iteration 276/1000 | Loss: 0.00004189
Iteration 277/1000 | Loss: 0.00002814
Iteration 278/1000 | Loss: 0.00003143
Iteration 279/1000 | Loss: 0.00002565
Iteration 280/1000 | Loss: 0.00003799
Iteration 281/1000 | Loss: 0.00003458
Iteration 282/1000 | Loss: 0.00003444
Iteration 283/1000 | Loss: 0.00003341
Iteration 284/1000 | Loss: 0.00003401
Iteration 285/1000 | Loss: 0.00003335
Iteration 286/1000 | Loss: 0.00003351
Iteration 287/1000 | Loss: 0.00002657
Iteration 288/1000 | Loss: 0.00003027
Iteration 289/1000 | Loss: 0.00002628
Iteration 290/1000 | Loss: 0.00002847
Iteration 291/1000 | Loss: 0.00003294
Iteration 292/1000 | Loss: 0.00002856
Iteration 293/1000 | Loss: 0.00003528
Iteration 294/1000 | Loss: 0.00003964
Iteration 295/1000 | Loss: 0.00003350
Iteration 296/1000 | Loss: 0.00003824
Iteration 297/1000 | Loss: 0.00002565
Iteration 298/1000 | Loss: 0.00002270
Iteration 299/1000 | Loss: 0.00002096
Iteration 300/1000 | Loss: 0.00002060
Iteration 301/1000 | Loss: 0.00002039
Iteration 302/1000 | Loss: 0.00002029
Iteration 303/1000 | Loss: 0.00002028
Iteration 304/1000 | Loss: 0.00002028
Iteration 305/1000 | Loss: 0.00002028
Iteration 306/1000 | Loss: 0.00002027
Iteration 307/1000 | Loss: 0.00002024
Iteration 308/1000 | Loss: 0.00002023
Iteration 309/1000 | Loss: 0.00002023
Iteration 310/1000 | Loss: 0.00002022
Iteration 311/1000 | Loss: 0.00002021
Iteration 312/1000 | Loss: 0.00002021
Iteration 313/1000 | Loss: 0.00002019
Iteration 314/1000 | Loss: 0.00002019
Iteration 315/1000 | Loss: 0.00002019
Iteration 316/1000 | Loss: 0.00002018
Iteration 317/1000 | Loss: 0.00002018
Iteration 318/1000 | Loss: 0.00002017
Iteration 319/1000 | Loss: 0.00002017
Iteration 320/1000 | Loss: 0.00002016
Iteration 321/1000 | Loss: 0.00002016
Iteration 322/1000 | Loss: 0.00002016
Iteration 323/1000 | Loss: 0.00002016
Iteration 324/1000 | Loss: 0.00002015
Iteration 325/1000 | Loss: 0.00002015
Iteration 326/1000 | Loss: 0.00002015
Iteration 327/1000 | Loss: 0.00002015
Iteration 328/1000 | Loss: 0.00002015
Iteration 329/1000 | Loss: 0.00002015
Iteration 330/1000 | Loss: 0.00002014
Iteration 331/1000 | Loss: 0.00002014
Iteration 332/1000 | Loss: 0.00002014
Iteration 333/1000 | Loss: 0.00002014
Iteration 334/1000 | Loss: 0.00002014
Iteration 335/1000 | Loss: 0.00002014
Iteration 336/1000 | Loss: 0.00002013
Iteration 337/1000 | Loss: 0.00002013
Iteration 338/1000 | Loss: 0.00002013
Iteration 339/1000 | Loss: 0.00002013
Iteration 340/1000 | Loss: 0.00002012
Iteration 341/1000 | Loss: 0.00002012
Iteration 342/1000 | Loss: 0.00002012
Iteration 343/1000 | Loss: 0.00002012
Iteration 344/1000 | Loss: 0.00002012
Iteration 345/1000 | Loss: 0.00002012
Iteration 346/1000 | Loss: 0.00002012
Iteration 347/1000 | Loss: 0.00002012
Iteration 348/1000 | Loss: 0.00002012
Iteration 349/1000 | Loss: 0.00002011
Iteration 350/1000 | Loss: 0.00002011
Iteration 351/1000 | Loss: 0.00002011
Iteration 352/1000 | Loss: 0.00002011
Iteration 353/1000 | Loss: 0.00002010
Iteration 354/1000 | Loss: 0.00002010
Iteration 355/1000 | Loss: 0.00002010
Iteration 356/1000 | Loss: 0.00002010
Iteration 357/1000 | Loss: 0.00002010
Iteration 358/1000 | Loss: 0.00002010
Iteration 359/1000 | Loss: 0.00002010
Iteration 360/1000 | Loss: 0.00002010
Iteration 361/1000 | Loss: 0.00002010
Iteration 362/1000 | Loss: 0.00002010
Iteration 363/1000 | Loss: 0.00002010
Iteration 364/1000 | Loss: 0.00002009
Iteration 365/1000 | Loss: 0.00002009
Iteration 366/1000 | Loss: 0.00002009
Iteration 367/1000 | Loss: 0.00002008
Iteration 368/1000 | Loss: 0.00002008
Iteration 369/1000 | Loss: 0.00002008
Iteration 370/1000 | Loss: 0.00002008
Iteration 371/1000 | Loss: 0.00002008
Iteration 372/1000 | Loss: 0.00002008
Iteration 373/1000 | Loss: 0.00002008
Iteration 374/1000 | Loss: 0.00002008
Iteration 375/1000 | Loss: 0.00002008
Iteration 376/1000 | Loss: 0.00002008
Iteration 377/1000 | Loss: 0.00002008
Iteration 378/1000 | Loss: 0.00002008
Iteration 379/1000 | Loss: 0.00002008
Iteration 380/1000 | Loss: 0.00002008
Iteration 381/1000 | Loss: 0.00002008
Iteration 382/1000 | Loss: 0.00002008
Iteration 383/1000 | Loss: 0.00002008
Iteration 384/1000 | Loss: 0.00002008
Iteration 385/1000 | Loss: 0.00002008
Iteration 386/1000 | Loss: 0.00002008
Iteration 387/1000 | Loss: 0.00002008
Iteration 388/1000 | Loss: 0.00002008
Iteration 389/1000 | Loss: 0.00002008
Iteration 390/1000 | Loss: 0.00002008
Iteration 391/1000 | Loss: 0.00002008
Iteration 392/1000 | Loss: 0.00002008
Iteration 393/1000 | Loss: 0.00002008
Iteration 394/1000 | Loss: 0.00002008
Iteration 395/1000 | Loss: 0.00002008
Iteration 396/1000 | Loss: 0.00002008
Iteration 397/1000 | Loss: 0.00002008
Iteration 398/1000 | Loss: 0.00002008
Iteration 399/1000 | Loss: 0.00002008
Iteration 400/1000 | Loss: 0.00002008
Iteration 401/1000 | Loss: 0.00002008
Iteration 402/1000 | Loss: 0.00002008
Iteration 403/1000 | Loss: 0.00002008
Iteration 404/1000 | Loss: 0.00002008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 404. Stopping optimization.
Last 5 losses: [2.0076855435036123e-05, 2.0076855435036123e-05, 2.0076855435036123e-05, 2.0076855435036123e-05, 2.0076855435036123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0076855435036123e-05

Optimization complete. Final v2v error: 3.719411849975586 mm

Highest mean error: 9.921719551086426 mm for frame 239

Lowest mean error: 3.4353537559509277 mm for frame 0

Saving results

Total time: 535.4439780712128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420062
Iteration 2/25 | Loss: 0.00116153
Iteration 3/25 | Loss: 0.00106522
Iteration 4/25 | Loss: 0.00105034
Iteration 5/25 | Loss: 0.00104637
Iteration 6/25 | Loss: 0.00104521
Iteration 7/25 | Loss: 0.00104520
Iteration 8/25 | Loss: 0.00104520
Iteration 9/25 | Loss: 0.00104520
Iteration 10/25 | Loss: 0.00104520
Iteration 11/25 | Loss: 0.00104520
Iteration 12/25 | Loss: 0.00104520
Iteration 13/25 | Loss: 0.00104520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010451975977048278, 0.0010451975977048278, 0.0010451975977048278, 0.0010451975977048278, 0.0010451975977048278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010451975977048278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.64176416
Iteration 2/25 | Loss: 0.00117596
Iteration 3/25 | Loss: 0.00117595
Iteration 4/25 | Loss: 0.00117595
Iteration 5/25 | Loss: 0.00117595
Iteration 6/25 | Loss: 0.00117595
Iteration 7/25 | Loss: 0.00117595
Iteration 8/25 | Loss: 0.00117595
Iteration 9/25 | Loss: 0.00117595
Iteration 10/25 | Loss: 0.00117595
Iteration 11/25 | Loss: 0.00117595
Iteration 12/25 | Loss: 0.00117595
Iteration 13/25 | Loss: 0.00117595
Iteration 14/25 | Loss: 0.00117595
Iteration 15/25 | Loss: 0.00117595
Iteration 16/25 | Loss: 0.00117595
Iteration 17/25 | Loss: 0.00117595
Iteration 18/25 | Loss: 0.00117595
Iteration 19/25 | Loss: 0.00117595
Iteration 20/25 | Loss: 0.00117595
Iteration 21/25 | Loss: 0.00117595
Iteration 22/25 | Loss: 0.00117595
Iteration 23/25 | Loss: 0.00117595
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001175946556031704, 0.001175946556031704, 0.001175946556031704, 0.001175946556031704, 0.001175946556031704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175946556031704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117595
Iteration 2/1000 | Loss: 0.00002896
Iteration 3/1000 | Loss: 0.00002009
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00001606
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001493
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00001447
Iteration 12/1000 | Loss: 0.00001444
Iteration 13/1000 | Loss: 0.00001444
Iteration 14/1000 | Loss: 0.00001442
Iteration 15/1000 | Loss: 0.00001435
Iteration 16/1000 | Loss: 0.00001431
Iteration 17/1000 | Loss: 0.00001430
Iteration 18/1000 | Loss: 0.00001428
Iteration 19/1000 | Loss: 0.00001427
Iteration 20/1000 | Loss: 0.00001424
Iteration 21/1000 | Loss: 0.00001423
Iteration 22/1000 | Loss: 0.00001422
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001421
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001420
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001419
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001418
Iteration 33/1000 | Loss: 0.00001418
Iteration 34/1000 | Loss: 0.00001417
Iteration 35/1000 | Loss: 0.00001417
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001416
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001416
Iteration 41/1000 | Loss: 0.00001416
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001415
Iteration 46/1000 | Loss: 0.00001415
Iteration 47/1000 | Loss: 0.00001415
Iteration 48/1000 | Loss: 0.00001415
Iteration 49/1000 | Loss: 0.00001414
Iteration 50/1000 | Loss: 0.00001414
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001414
Iteration 54/1000 | Loss: 0.00001414
Iteration 55/1000 | Loss: 0.00001414
Iteration 56/1000 | Loss: 0.00001414
Iteration 57/1000 | Loss: 0.00001414
Iteration 58/1000 | Loss: 0.00001414
Iteration 59/1000 | Loss: 0.00001414
Iteration 60/1000 | Loss: 0.00001414
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001411
Iteration 84/1000 | Loss: 0.00001411
Iteration 85/1000 | Loss: 0.00001411
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001411
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001410
Iteration 95/1000 | Loss: 0.00001410
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001409
Iteration 98/1000 | Loss: 0.00001409
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001408
Iteration 102/1000 | Loss: 0.00001408
Iteration 103/1000 | Loss: 0.00001408
Iteration 104/1000 | Loss: 0.00001407
Iteration 105/1000 | Loss: 0.00001407
Iteration 106/1000 | Loss: 0.00001407
Iteration 107/1000 | Loss: 0.00001406
Iteration 108/1000 | Loss: 0.00001406
Iteration 109/1000 | Loss: 0.00001406
Iteration 110/1000 | Loss: 0.00001406
Iteration 111/1000 | Loss: 0.00001405
Iteration 112/1000 | Loss: 0.00001405
Iteration 113/1000 | Loss: 0.00001404
Iteration 114/1000 | Loss: 0.00001404
Iteration 115/1000 | Loss: 0.00001404
Iteration 116/1000 | Loss: 0.00001403
Iteration 117/1000 | Loss: 0.00001403
Iteration 118/1000 | Loss: 0.00001403
Iteration 119/1000 | Loss: 0.00001403
Iteration 120/1000 | Loss: 0.00001403
Iteration 121/1000 | Loss: 0.00001403
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001401
Iteration 127/1000 | Loss: 0.00001401
Iteration 128/1000 | Loss: 0.00001401
Iteration 129/1000 | Loss: 0.00001401
Iteration 130/1000 | Loss: 0.00001401
Iteration 131/1000 | Loss: 0.00001401
Iteration 132/1000 | Loss: 0.00001401
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001400
Iteration 137/1000 | Loss: 0.00001400
Iteration 138/1000 | Loss: 0.00001399
Iteration 139/1000 | Loss: 0.00001399
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001399
Iteration 146/1000 | Loss: 0.00001399
Iteration 147/1000 | Loss: 0.00001399
Iteration 148/1000 | Loss: 0.00001399
Iteration 149/1000 | Loss: 0.00001399
Iteration 150/1000 | Loss: 0.00001399
Iteration 151/1000 | Loss: 0.00001399
Iteration 152/1000 | Loss: 0.00001399
Iteration 153/1000 | Loss: 0.00001399
Iteration 154/1000 | Loss: 0.00001399
Iteration 155/1000 | Loss: 0.00001399
Iteration 156/1000 | Loss: 0.00001399
Iteration 157/1000 | Loss: 0.00001399
Iteration 158/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.3985509212943725e-05, 1.3985509212943725e-05, 1.3985509212943725e-05, 1.3985509212943725e-05, 1.3985509212943725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3985509212943725e-05

Optimization complete. Final v2v error: 3.1530256271362305 mm

Highest mean error: 3.5130698680877686 mm for frame 85

Lowest mean error: 2.812175989151001 mm for frame 2

Saving results

Total time: 34.057857036590576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394680
Iteration 2/25 | Loss: 0.00117239
Iteration 3/25 | Loss: 0.00107216
Iteration 4/25 | Loss: 0.00106309
Iteration 5/25 | Loss: 0.00106002
Iteration 6/25 | Loss: 0.00105979
Iteration 7/25 | Loss: 0.00105979
Iteration 8/25 | Loss: 0.00105979
Iteration 9/25 | Loss: 0.00105979
Iteration 10/25 | Loss: 0.00105979
Iteration 11/25 | Loss: 0.00105979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010597871150821447, 0.0010597871150821447, 0.0010597871150821447, 0.0010597871150821447, 0.0010597871150821447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010597871150821447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37978387
Iteration 2/25 | Loss: 0.00113670
Iteration 3/25 | Loss: 0.00113670
Iteration 4/25 | Loss: 0.00113670
Iteration 5/25 | Loss: 0.00113670
Iteration 6/25 | Loss: 0.00113670
Iteration 7/25 | Loss: 0.00113670
Iteration 8/25 | Loss: 0.00113670
Iteration 9/25 | Loss: 0.00113670
Iteration 10/25 | Loss: 0.00113670
Iteration 11/25 | Loss: 0.00113670
Iteration 12/25 | Loss: 0.00113670
Iteration 13/25 | Loss: 0.00113670
Iteration 14/25 | Loss: 0.00113670
Iteration 15/25 | Loss: 0.00113670
Iteration 16/25 | Loss: 0.00113670
Iteration 17/25 | Loss: 0.00113670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011366965482011437, 0.0011366965482011437, 0.0011366965482011437, 0.0011366965482011437, 0.0011366965482011437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011366965482011437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113670
Iteration 2/1000 | Loss: 0.00002629
Iteration 3/1000 | Loss: 0.00001770
Iteration 4/1000 | Loss: 0.00001528
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001287
Iteration 9/1000 | Loss: 0.00001257
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001221
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001218
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001217
Iteration 21/1000 | Loss: 0.00001217
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001208
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001201
Iteration 37/1000 | Loss: 0.00001201
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001200
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001188
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001188
Iteration 94/1000 | Loss: 0.00001188
Iteration 95/1000 | Loss: 0.00001188
Iteration 96/1000 | Loss: 0.00001188
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001185
Iteration 120/1000 | Loss: 0.00001185
Iteration 121/1000 | Loss: 0.00001185
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001185
Iteration 129/1000 | Loss: 0.00001185
Iteration 130/1000 | Loss: 0.00001185
Iteration 131/1000 | Loss: 0.00001185
Iteration 132/1000 | Loss: 0.00001184
Iteration 133/1000 | Loss: 0.00001184
Iteration 134/1000 | Loss: 0.00001184
Iteration 135/1000 | Loss: 0.00001184
Iteration 136/1000 | Loss: 0.00001184
Iteration 137/1000 | Loss: 0.00001184
Iteration 138/1000 | Loss: 0.00001184
Iteration 139/1000 | Loss: 0.00001184
Iteration 140/1000 | Loss: 0.00001184
Iteration 141/1000 | Loss: 0.00001184
Iteration 142/1000 | Loss: 0.00001184
Iteration 143/1000 | Loss: 0.00001184
Iteration 144/1000 | Loss: 0.00001184
Iteration 145/1000 | Loss: 0.00001184
Iteration 146/1000 | Loss: 0.00001184
Iteration 147/1000 | Loss: 0.00001184
Iteration 148/1000 | Loss: 0.00001184
Iteration 149/1000 | Loss: 0.00001184
Iteration 150/1000 | Loss: 0.00001184
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001183
Iteration 158/1000 | Loss: 0.00001183
Iteration 159/1000 | Loss: 0.00001183
Iteration 160/1000 | Loss: 0.00001183
Iteration 161/1000 | Loss: 0.00001183
Iteration 162/1000 | Loss: 0.00001183
Iteration 163/1000 | Loss: 0.00001183
Iteration 164/1000 | Loss: 0.00001183
Iteration 165/1000 | Loss: 0.00001183
Iteration 166/1000 | Loss: 0.00001183
Iteration 167/1000 | Loss: 0.00001183
Iteration 168/1000 | Loss: 0.00001183
Iteration 169/1000 | Loss: 0.00001183
Iteration 170/1000 | Loss: 0.00001183
Iteration 171/1000 | Loss: 0.00001183
Iteration 172/1000 | Loss: 0.00001183
Iteration 173/1000 | Loss: 0.00001183
Iteration 174/1000 | Loss: 0.00001183
Iteration 175/1000 | Loss: 0.00001183
Iteration 176/1000 | Loss: 0.00001183
Iteration 177/1000 | Loss: 0.00001183
Iteration 178/1000 | Loss: 0.00001183
Iteration 179/1000 | Loss: 0.00001183
Iteration 180/1000 | Loss: 0.00001183
Iteration 181/1000 | Loss: 0.00001183
Iteration 182/1000 | Loss: 0.00001183
Iteration 183/1000 | Loss: 0.00001183
Iteration 184/1000 | Loss: 0.00001183
Iteration 185/1000 | Loss: 0.00001183
Iteration 186/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.1826511581602972e-05, 1.1826511581602972e-05, 1.1826511581602972e-05, 1.1826511581602972e-05, 1.1826511581602972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1826511581602972e-05

Optimization complete. Final v2v error: 2.9627115726470947 mm

Highest mean error: 3.446986198425293 mm for frame 251

Lowest mean error: 2.6327126026153564 mm for frame 187

Saving results

Total time: 41.69733476638794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380562
Iteration 2/25 | Loss: 0.00109696
Iteration 3/25 | Loss: 0.00104351
Iteration 4/25 | Loss: 0.00103877
Iteration 5/25 | Loss: 0.00103718
Iteration 6/25 | Loss: 0.00103658
Iteration 7/25 | Loss: 0.00103658
Iteration 8/25 | Loss: 0.00103658
Iteration 9/25 | Loss: 0.00103658
Iteration 10/25 | Loss: 0.00103658
Iteration 11/25 | Loss: 0.00103658
Iteration 12/25 | Loss: 0.00103658
Iteration 13/25 | Loss: 0.00103658
Iteration 14/25 | Loss: 0.00103658
Iteration 15/25 | Loss: 0.00103658
Iteration 16/25 | Loss: 0.00103658
Iteration 17/25 | Loss: 0.00103658
Iteration 18/25 | Loss: 0.00103658
Iteration 19/25 | Loss: 0.00103658
Iteration 20/25 | Loss: 0.00103658
Iteration 21/25 | Loss: 0.00103658
Iteration 22/25 | Loss: 0.00103658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010365801863372326, 0.0010365801863372326, 0.0010365801863372326, 0.0010365801863372326, 0.0010365801863372326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010365801863372326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57953370
Iteration 2/25 | Loss: 0.00123216
Iteration 3/25 | Loss: 0.00123215
Iteration 4/25 | Loss: 0.00123215
Iteration 5/25 | Loss: 0.00123215
Iteration 6/25 | Loss: 0.00123215
Iteration 7/25 | Loss: 0.00123215
Iteration 8/25 | Loss: 0.00123215
Iteration 9/25 | Loss: 0.00123215
Iteration 10/25 | Loss: 0.00123215
Iteration 11/25 | Loss: 0.00123215
Iteration 12/25 | Loss: 0.00123215
Iteration 13/25 | Loss: 0.00123215
Iteration 14/25 | Loss: 0.00123215
Iteration 15/25 | Loss: 0.00123215
Iteration 16/25 | Loss: 0.00123215
Iteration 17/25 | Loss: 0.00123215
Iteration 18/25 | Loss: 0.00123215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012321503600105643, 0.0012321503600105643, 0.0012321503600105643, 0.0012321503600105643, 0.0012321503600105643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012321503600105643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123215
Iteration 2/1000 | Loss: 0.00003918
Iteration 3/1000 | Loss: 0.00001929
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001218
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001200
Iteration 13/1000 | Loss: 0.00001200
Iteration 14/1000 | Loss: 0.00001198
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001184
Iteration 17/1000 | Loss: 0.00001183
Iteration 18/1000 | Loss: 0.00001182
Iteration 19/1000 | Loss: 0.00001179
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00001179
Iteration 22/1000 | Loss: 0.00001178
Iteration 23/1000 | Loss: 0.00001178
Iteration 24/1000 | Loss: 0.00001178
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001177
Iteration 28/1000 | Loss: 0.00001176
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001175
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001174
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001173
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001170
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001167
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001164
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001158
Iteration 103/1000 | Loss: 0.00001158
Iteration 104/1000 | Loss: 0.00001158
Iteration 105/1000 | Loss: 0.00001158
Iteration 106/1000 | Loss: 0.00001157
Iteration 107/1000 | Loss: 0.00001157
Iteration 108/1000 | Loss: 0.00001157
Iteration 109/1000 | Loss: 0.00001156
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001154
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001151
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001150
Iteration 128/1000 | Loss: 0.00001150
Iteration 129/1000 | Loss: 0.00001150
Iteration 130/1000 | Loss: 0.00001150
Iteration 131/1000 | Loss: 0.00001150
Iteration 132/1000 | Loss: 0.00001150
Iteration 133/1000 | Loss: 0.00001150
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001150
Iteration 136/1000 | Loss: 0.00001150
Iteration 137/1000 | Loss: 0.00001149
Iteration 138/1000 | Loss: 0.00001149
Iteration 139/1000 | Loss: 0.00001149
Iteration 140/1000 | Loss: 0.00001149
Iteration 141/1000 | Loss: 0.00001149
Iteration 142/1000 | Loss: 0.00001149
Iteration 143/1000 | Loss: 0.00001149
Iteration 144/1000 | Loss: 0.00001149
Iteration 145/1000 | Loss: 0.00001149
Iteration 146/1000 | Loss: 0.00001149
Iteration 147/1000 | Loss: 0.00001149
Iteration 148/1000 | Loss: 0.00001149
Iteration 149/1000 | Loss: 0.00001149
Iteration 150/1000 | Loss: 0.00001149
Iteration 151/1000 | Loss: 0.00001149
Iteration 152/1000 | Loss: 0.00001149
Iteration 153/1000 | Loss: 0.00001149
Iteration 154/1000 | Loss: 0.00001149
Iteration 155/1000 | Loss: 0.00001149
Iteration 156/1000 | Loss: 0.00001149
Iteration 157/1000 | Loss: 0.00001149
Iteration 158/1000 | Loss: 0.00001149
Iteration 159/1000 | Loss: 0.00001149
Iteration 160/1000 | Loss: 0.00001149
Iteration 161/1000 | Loss: 0.00001149
Iteration 162/1000 | Loss: 0.00001149
Iteration 163/1000 | Loss: 0.00001149
Iteration 164/1000 | Loss: 0.00001149
Iteration 165/1000 | Loss: 0.00001149
Iteration 166/1000 | Loss: 0.00001149
Iteration 167/1000 | Loss: 0.00001149
Iteration 168/1000 | Loss: 0.00001149
Iteration 169/1000 | Loss: 0.00001149
Iteration 170/1000 | Loss: 0.00001149
Iteration 171/1000 | Loss: 0.00001149
Iteration 172/1000 | Loss: 0.00001149
Iteration 173/1000 | Loss: 0.00001149
Iteration 174/1000 | Loss: 0.00001149
Iteration 175/1000 | Loss: 0.00001149
Iteration 176/1000 | Loss: 0.00001149
Iteration 177/1000 | Loss: 0.00001149
Iteration 178/1000 | Loss: 0.00001149
Iteration 179/1000 | Loss: 0.00001149
Iteration 180/1000 | Loss: 0.00001149
Iteration 181/1000 | Loss: 0.00001149
Iteration 182/1000 | Loss: 0.00001149
Iteration 183/1000 | Loss: 0.00001149
Iteration 184/1000 | Loss: 0.00001149
Iteration 185/1000 | Loss: 0.00001149
Iteration 186/1000 | Loss: 0.00001149
Iteration 187/1000 | Loss: 0.00001149
Iteration 188/1000 | Loss: 0.00001149
Iteration 189/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.1493309102661442e-05, 1.1493309102661442e-05, 1.1493309102661442e-05, 1.1493309102661442e-05, 1.1493309102661442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1493309102661442e-05

Optimization complete. Final v2v error: 2.876128673553467 mm

Highest mean error: 3.22304368019104 mm for frame 66

Lowest mean error: 2.7069997787475586 mm for frame 129

Saving results

Total time: 35.79665517807007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128379
Iteration 2/25 | Loss: 0.00271001
Iteration 3/25 | Loss: 0.00164988
Iteration 4/25 | Loss: 0.00155209
Iteration 5/25 | Loss: 0.00156433
Iteration 6/25 | Loss: 0.00158187
Iteration 7/25 | Loss: 0.00151716
Iteration 8/25 | Loss: 0.00147133
Iteration 9/25 | Loss: 0.00142732
Iteration 10/25 | Loss: 0.00137458
Iteration 11/25 | Loss: 0.00133321
Iteration 12/25 | Loss: 0.00131026
Iteration 13/25 | Loss: 0.00131658
Iteration 14/25 | Loss: 0.00132202
Iteration 15/25 | Loss: 0.00130615
Iteration 16/25 | Loss: 0.00130202
Iteration 17/25 | Loss: 0.00129610
Iteration 18/25 | Loss: 0.00129621
Iteration 19/25 | Loss: 0.00128913
Iteration 20/25 | Loss: 0.00128617
Iteration 21/25 | Loss: 0.00128453
Iteration 22/25 | Loss: 0.00128458
Iteration 23/25 | Loss: 0.00128351
Iteration 24/25 | Loss: 0.00128003
Iteration 25/25 | Loss: 0.00128265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59449697
Iteration 2/25 | Loss: 0.00167536
Iteration 3/25 | Loss: 0.00167536
Iteration 4/25 | Loss: 0.00167535
Iteration 5/25 | Loss: 0.00167535
Iteration 6/25 | Loss: 0.00167535
Iteration 7/25 | Loss: 0.00167535
Iteration 8/25 | Loss: 0.00167535
Iteration 9/25 | Loss: 0.00167535
Iteration 10/25 | Loss: 0.00167535
Iteration 11/25 | Loss: 0.00167535
Iteration 12/25 | Loss: 0.00167535
Iteration 13/25 | Loss: 0.00167535
Iteration 14/25 | Loss: 0.00167535
Iteration 15/25 | Loss: 0.00167535
Iteration 16/25 | Loss: 0.00167535
Iteration 17/25 | Loss: 0.00167535
Iteration 18/25 | Loss: 0.00167535
Iteration 19/25 | Loss: 0.00167535
Iteration 20/25 | Loss: 0.00167535
Iteration 21/25 | Loss: 0.00167535
Iteration 22/25 | Loss: 0.00167535
Iteration 23/25 | Loss: 0.00167535
Iteration 24/25 | Loss: 0.00167535
Iteration 25/25 | Loss: 0.00167535
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016753512900322676, 0.0016753512900322676, 0.0016753512900322676, 0.0016753512900322676, 0.0016753512900322676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016753512900322676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167535
Iteration 2/1000 | Loss: 0.00028373
Iteration 3/1000 | Loss: 0.00014533
Iteration 4/1000 | Loss: 0.00037191
Iteration 5/1000 | Loss: 0.00036074
Iteration 6/1000 | Loss: 0.00020656
Iteration 7/1000 | Loss: 0.00023204
Iteration 8/1000 | Loss: 0.00011749
Iteration 9/1000 | Loss: 0.00040362
Iteration 10/1000 | Loss: 0.00083300
Iteration 11/1000 | Loss: 0.00027480
Iteration 12/1000 | Loss: 0.00016713
Iteration 13/1000 | Loss: 0.00009391
Iteration 14/1000 | Loss: 0.00015495
Iteration 15/1000 | Loss: 0.00033425
Iteration 16/1000 | Loss: 0.00029046
Iteration 17/1000 | Loss: 0.00022259
Iteration 18/1000 | Loss: 0.00022588
Iteration 19/1000 | Loss: 0.00010498
Iteration 20/1000 | Loss: 0.00010903
Iteration 21/1000 | Loss: 0.00021005
Iteration 22/1000 | Loss: 0.00050621
Iteration 23/1000 | Loss: 0.00046573
Iteration 24/1000 | Loss: 0.00039064
Iteration 25/1000 | Loss: 0.00020149
Iteration 26/1000 | Loss: 0.00029490
Iteration 27/1000 | Loss: 0.00030427
Iteration 28/1000 | Loss: 0.00028337
Iteration 29/1000 | Loss: 0.00039404
Iteration 30/1000 | Loss: 0.00032724
Iteration 31/1000 | Loss: 0.00038583
Iteration 32/1000 | Loss: 0.00039042
Iteration 33/1000 | Loss: 0.00018227
Iteration 34/1000 | Loss: 0.00024097
Iteration 35/1000 | Loss: 0.00077431
Iteration 36/1000 | Loss: 0.00021793
Iteration 37/1000 | Loss: 0.00018462
Iteration 38/1000 | Loss: 0.00015804
Iteration 39/1000 | Loss: 0.00009416
Iteration 40/1000 | Loss: 0.00037705
Iteration 41/1000 | Loss: 0.00014985
Iteration 42/1000 | Loss: 0.00039177
Iteration 43/1000 | Loss: 0.00010532
Iteration 44/1000 | Loss: 0.00026887
Iteration 45/1000 | Loss: 0.00029474
Iteration 46/1000 | Loss: 0.00022301
Iteration 47/1000 | Loss: 0.00017414
Iteration 48/1000 | Loss: 0.00023383
Iteration 49/1000 | Loss: 0.00022087
Iteration 50/1000 | Loss: 0.00008576
Iteration 51/1000 | Loss: 0.00007965
Iteration 52/1000 | Loss: 0.00042376
Iteration 53/1000 | Loss: 0.00015018
Iteration 54/1000 | Loss: 0.00031871
Iteration 55/1000 | Loss: 0.00043502
Iteration 56/1000 | Loss: 0.00026996
Iteration 57/1000 | Loss: 0.00039419
Iteration 58/1000 | Loss: 0.00019256
Iteration 59/1000 | Loss: 0.00023364
Iteration 60/1000 | Loss: 0.00066748
Iteration 61/1000 | Loss: 0.00030464
Iteration 62/1000 | Loss: 0.00037722
Iteration 63/1000 | Loss: 0.00045455
Iteration 64/1000 | Loss: 0.00059512
Iteration 65/1000 | Loss: 0.00121108
Iteration 66/1000 | Loss: 0.00072367
Iteration 67/1000 | Loss: 0.00056742
Iteration 68/1000 | Loss: 0.00009483
Iteration 69/1000 | Loss: 0.00047817
Iteration 70/1000 | Loss: 0.00131783
Iteration 71/1000 | Loss: 0.00081161
Iteration 72/1000 | Loss: 0.00195488
Iteration 73/1000 | Loss: 0.00120134
Iteration 74/1000 | Loss: 0.00156615
Iteration 75/1000 | Loss: 0.00020546
Iteration 76/1000 | Loss: 0.00006318
Iteration 77/1000 | Loss: 0.00179403
Iteration 78/1000 | Loss: 0.00031433
Iteration 79/1000 | Loss: 0.00054330
Iteration 80/1000 | Loss: 0.00006973
Iteration 81/1000 | Loss: 0.00005221
Iteration 82/1000 | Loss: 0.00004251
Iteration 83/1000 | Loss: 0.00003507
Iteration 84/1000 | Loss: 0.00003187
Iteration 85/1000 | Loss: 0.00002973
Iteration 86/1000 | Loss: 0.00002768
Iteration 87/1000 | Loss: 0.00002674
Iteration 88/1000 | Loss: 0.00002565
Iteration 89/1000 | Loss: 0.00002481
Iteration 90/1000 | Loss: 0.00002428
Iteration 91/1000 | Loss: 0.00002390
Iteration 92/1000 | Loss: 0.00002354
Iteration 93/1000 | Loss: 0.00002335
Iteration 94/1000 | Loss: 0.00002326
Iteration 95/1000 | Loss: 0.00002321
Iteration 96/1000 | Loss: 0.00002321
Iteration 97/1000 | Loss: 0.00002321
Iteration 98/1000 | Loss: 0.00002319
Iteration 99/1000 | Loss: 0.00002319
Iteration 100/1000 | Loss: 0.00002319
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002318
Iteration 103/1000 | Loss: 0.00002317
Iteration 104/1000 | Loss: 0.00002317
Iteration 105/1000 | Loss: 0.00002316
Iteration 106/1000 | Loss: 0.00002316
Iteration 107/1000 | Loss: 0.00002316
Iteration 108/1000 | Loss: 0.00002316
Iteration 109/1000 | Loss: 0.00002316
Iteration 110/1000 | Loss: 0.00002316
Iteration 111/1000 | Loss: 0.00002316
Iteration 112/1000 | Loss: 0.00002316
Iteration 113/1000 | Loss: 0.00002316
Iteration 114/1000 | Loss: 0.00002316
Iteration 115/1000 | Loss: 0.00002316
Iteration 116/1000 | Loss: 0.00002316
Iteration 117/1000 | Loss: 0.00002316
Iteration 118/1000 | Loss: 0.00002316
Iteration 119/1000 | Loss: 0.00002316
Iteration 120/1000 | Loss: 0.00002316
Iteration 121/1000 | Loss: 0.00002316
Iteration 122/1000 | Loss: 0.00002316
Iteration 123/1000 | Loss: 0.00002316
Iteration 124/1000 | Loss: 0.00002316
Iteration 125/1000 | Loss: 0.00002316
Iteration 126/1000 | Loss: 0.00002316
Iteration 127/1000 | Loss: 0.00002316
Iteration 128/1000 | Loss: 0.00002316
Iteration 129/1000 | Loss: 0.00002316
Iteration 130/1000 | Loss: 0.00002316
Iteration 131/1000 | Loss: 0.00002316
Iteration 132/1000 | Loss: 0.00002316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.3163045625551604e-05, 2.3163045625551604e-05, 2.3163045625551604e-05, 2.3163045625551604e-05, 2.3163045625551604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3163045625551604e-05

Optimization complete. Final v2v error: 3.9426493644714355 mm

Highest mean error: 4.366954803466797 mm for frame 139

Lowest mean error: 3.4265379905700684 mm for frame 26

Saving results

Total time: 180.84621143341064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949173
Iteration 2/25 | Loss: 0.00235283
Iteration 3/25 | Loss: 0.00172625
Iteration 4/25 | Loss: 0.00166621
Iteration 5/25 | Loss: 0.00157503
Iteration 6/25 | Loss: 0.00154909
Iteration 7/25 | Loss: 0.00136066
Iteration 8/25 | Loss: 0.00132963
Iteration 9/25 | Loss: 0.00132148
Iteration 10/25 | Loss: 0.00130641
Iteration 11/25 | Loss: 0.00130422
Iteration 12/25 | Loss: 0.00130091
Iteration 13/25 | Loss: 0.00130113
Iteration 14/25 | Loss: 0.00129602
Iteration 15/25 | Loss: 0.00129649
Iteration 16/25 | Loss: 0.00129629
Iteration 17/25 | Loss: 0.00129645
Iteration 18/25 | Loss: 0.00129568
Iteration 19/25 | Loss: 0.00129601
Iteration 20/25 | Loss: 0.00129559
Iteration 21/25 | Loss: 0.00129552
Iteration 22/25 | Loss: 0.00129525
Iteration 23/25 | Loss: 0.00129577
Iteration 24/25 | Loss: 0.00129549
Iteration 25/25 | Loss: 0.00129488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20282638
Iteration 2/25 | Loss: 0.00264125
Iteration 3/25 | Loss: 0.00259118
Iteration 4/25 | Loss: 0.00259118
Iteration 5/25 | Loss: 0.00259117
Iteration 6/25 | Loss: 0.00259117
Iteration 7/25 | Loss: 0.00259117
Iteration 8/25 | Loss: 0.00259117
Iteration 9/25 | Loss: 0.00259117
Iteration 10/25 | Loss: 0.00259117
Iteration 11/25 | Loss: 0.00259117
Iteration 12/25 | Loss: 0.00259117
Iteration 13/25 | Loss: 0.00259117
Iteration 14/25 | Loss: 0.00259117
Iteration 15/25 | Loss: 0.00259117
Iteration 16/25 | Loss: 0.00259117
Iteration 17/25 | Loss: 0.00259117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002591173630207777, 0.002591173630207777, 0.002591173630207777, 0.002591173630207777, 0.002591173630207777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002591173630207777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259117
Iteration 2/1000 | Loss: 0.00062043
Iteration 3/1000 | Loss: 0.00016709
Iteration 4/1000 | Loss: 0.00014569
Iteration 5/1000 | Loss: 0.00027002
Iteration 6/1000 | Loss: 0.00014547
Iteration 7/1000 | Loss: 0.00030670
Iteration 8/1000 | Loss: 0.00011657
Iteration 9/1000 | Loss: 0.00010437
Iteration 10/1000 | Loss: 0.00012858
Iteration 11/1000 | Loss: 0.00010610
Iteration 12/1000 | Loss: 0.00009152
Iteration 13/1000 | Loss: 0.00008962
Iteration 14/1000 | Loss: 0.00009458
Iteration 15/1000 | Loss: 0.00009282
Iteration 16/1000 | Loss: 0.00008217
Iteration 17/1000 | Loss: 0.00009024
Iteration 18/1000 | Loss: 0.00009358
Iteration 19/1000 | Loss: 0.00112579
Iteration 20/1000 | Loss: 0.00286789
Iteration 21/1000 | Loss: 0.00377517
Iteration 22/1000 | Loss: 0.00021006
Iteration 23/1000 | Loss: 0.00013206
Iteration 24/1000 | Loss: 0.00009495
Iteration 25/1000 | Loss: 0.00073836
Iteration 26/1000 | Loss: 0.00005839
Iteration 27/1000 | Loss: 0.00004210
Iteration 28/1000 | Loss: 0.00029742
Iteration 29/1000 | Loss: 0.00003581
Iteration 30/1000 | Loss: 0.00003559
Iteration 31/1000 | Loss: 0.00002870
Iteration 32/1000 | Loss: 0.00002456
Iteration 33/1000 | Loss: 0.00002293
Iteration 34/1000 | Loss: 0.00002128
Iteration 35/1000 | Loss: 0.00002227
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001857
Iteration 38/1000 | Loss: 0.00008402
Iteration 39/1000 | Loss: 0.00001991
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00015974
Iteration 43/1000 | Loss: 0.00002509
Iteration 44/1000 | Loss: 0.00002801
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001693
Iteration 47/1000 | Loss: 0.00001693
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001690
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001683
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001667
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001658
Iteration 63/1000 | Loss: 0.00001653
Iteration 64/1000 | Loss: 0.00001653
Iteration 65/1000 | Loss: 0.00001650
Iteration 66/1000 | Loss: 0.00022080
Iteration 67/1000 | Loss: 0.00019868
Iteration 68/1000 | Loss: 0.00014136
Iteration 69/1000 | Loss: 0.00018456
Iteration 70/1000 | Loss: 0.00013704
Iteration 71/1000 | Loss: 0.00021619
Iteration 72/1000 | Loss: 0.00013321
Iteration 73/1000 | Loss: 0.00021049
Iteration 74/1000 | Loss: 0.00002627
Iteration 75/1000 | Loss: 0.00002216
Iteration 76/1000 | Loss: 0.00011741
Iteration 77/1000 | Loss: 0.00010326
Iteration 78/1000 | Loss: 0.00002011
Iteration 79/1000 | Loss: 0.00001848
Iteration 80/1000 | Loss: 0.00012518
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001682
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001630
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001607
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001602
Iteration 91/1000 | Loss: 0.00001601
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001601
Iteration 94/1000 | Loss: 0.00001601
Iteration 95/1000 | Loss: 0.00001600
Iteration 96/1000 | Loss: 0.00001600
Iteration 97/1000 | Loss: 0.00001599
Iteration 98/1000 | Loss: 0.00001599
Iteration 99/1000 | Loss: 0.00001599
Iteration 100/1000 | Loss: 0.00001599
Iteration 101/1000 | Loss: 0.00001599
Iteration 102/1000 | Loss: 0.00001598
Iteration 103/1000 | Loss: 0.00001598
Iteration 104/1000 | Loss: 0.00001598
Iteration 105/1000 | Loss: 0.00001598
Iteration 106/1000 | Loss: 0.00001598
Iteration 107/1000 | Loss: 0.00001598
Iteration 108/1000 | Loss: 0.00001598
Iteration 109/1000 | Loss: 0.00001597
Iteration 110/1000 | Loss: 0.00001597
Iteration 111/1000 | Loss: 0.00001597
Iteration 112/1000 | Loss: 0.00001596
Iteration 113/1000 | Loss: 0.00001596
Iteration 114/1000 | Loss: 0.00001596
Iteration 115/1000 | Loss: 0.00001595
Iteration 116/1000 | Loss: 0.00001595
Iteration 117/1000 | Loss: 0.00001595
Iteration 118/1000 | Loss: 0.00001595
Iteration 119/1000 | Loss: 0.00001595
Iteration 120/1000 | Loss: 0.00001595
Iteration 121/1000 | Loss: 0.00001595
Iteration 122/1000 | Loss: 0.00001595
Iteration 123/1000 | Loss: 0.00001595
Iteration 124/1000 | Loss: 0.00001595
Iteration 125/1000 | Loss: 0.00001595
Iteration 126/1000 | Loss: 0.00001595
Iteration 127/1000 | Loss: 0.00001595
Iteration 128/1000 | Loss: 0.00001595
Iteration 129/1000 | Loss: 0.00001594
Iteration 130/1000 | Loss: 0.00001594
Iteration 131/1000 | Loss: 0.00001594
Iteration 132/1000 | Loss: 0.00001594
Iteration 133/1000 | Loss: 0.00001594
Iteration 134/1000 | Loss: 0.00001594
Iteration 135/1000 | Loss: 0.00001594
Iteration 136/1000 | Loss: 0.00001594
Iteration 137/1000 | Loss: 0.00001594
Iteration 138/1000 | Loss: 0.00001594
Iteration 139/1000 | Loss: 0.00001593
Iteration 140/1000 | Loss: 0.00001593
Iteration 141/1000 | Loss: 0.00001593
Iteration 142/1000 | Loss: 0.00001593
Iteration 143/1000 | Loss: 0.00001593
Iteration 144/1000 | Loss: 0.00001593
Iteration 145/1000 | Loss: 0.00001593
Iteration 146/1000 | Loss: 0.00001593
Iteration 147/1000 | Loss: 0.00001593
Iteration 148/1000 | Loss: 0.00001593
Iteration 149/1000 | Loss: 0.00001593
Iteration 150/1000 | Loss: 0.00001592
Iteration 151/1000 | Loss: 0.00001592
Iteration 152/1000 | Loss: 0.00001592
Iteration 153/1000 | Loss: 0.00001592
Iteration 154/1000 | Loss: 0.00001592
Iteration 155/1000 | Loss: 0.00001592
Iteration 156/1000 | Loss: 0.00001592
Iteration 157/1000 | Loss: 0.00001592
Iteration 158/1000 | Loss: 0.00001592
Iteration 159/1000 | Loss: 0.00001592
Iteration 160/1000 | Loss: 0.00001592
Iteration 161/1000 | Loss: 0.00001592
Iteration 162/1000 | Loss: 0.00001592
Iteration 163/1000 | Loss: 0.00001591
Iteration 164/1000 | Loss: 0.00001591
Iteration 165/1000 | Loss: 0.00001591
Iteration 166/1000 | Loss: 0.00001591
Iteration 167/1000 | Loss: 0.00001591
Iteration 168/1000 | Loss: 0.00001591
Iteration 169/1000 | Loss: 0.00001591
Iteration 170/1000 | Loss: 0.00001591
Iteration 171/1000 | Loss: 0.00001591
Iteration 172/1000 | Loss: 0.00001591
Iteration 173/1000 | Loss: 0.00001591
Iteration 174/1000 | Loss: 0.00001591
Iteration 175/1000 | Loss: 0.00001591
Iteration 176/1000 | Loss: 0.00001591
Iteration 177/1000 | Loss: 0.00001591
Iteration 178/1000 | Loss: 0.00001591
Iteration 179/1000 | Loss: 0.00001591
Iteration 180/1000 | Loss: 0.00001591
Iteration 181/1000 | Loss: 0.00001591
Iteration 182/1000 | Loss: 0.00001591
Iteration 183/1000 | Loss: 0.00001591
Iteration 184/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.590915235283319e-05, 1.590915235283319e-05, 1.590915235283319e-05, 1.590915235283319e-05, 1.590915235283319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.590915235283319e-05

Optimization complete. Final v2v error: 3.3170547485351562 mm

Highest mean error: 4.316292762756348 mm for frame 148

Lowest mean error: 2.916865110397339 mm for frame 28

Saving results

Total time: 169.59950351715088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895962
Iteration 2/25 | Loss: 0.00153052
Iteration 3/25 | Loss: 0.00123239
Iteration 4/25 | Loss: 0.00119663
Iteration 5/25 | Loss: 0.00118954
Iteration 6/25 | Loss: 0.00119113
Iteration 7/25 | Loss: 0.00118489
Iteration 8/25 | Loss: 0.00117586
Iteration 9/25 | Loss: 0.00117150
Iteration 10/25 | Loss: 0.00117056
Iteration 11/25 | Loss: 0.00117032
Iteration 12/25 | Loss: 0.00117030
Iteration 13/25 | Loss: 0.00117030
Iteration 14/25 | Loss: 0.00117029
Iteration 15/25 | Loss: 0.00117029
Iteration 16/25 | Loss: 0.00117029
Iteration 17/25 | Loss: 0.00117029
Iteration 18/25 | Loss: 0.00117029
Iteration 19/25 | Loss: 0.00117029
Iteration 20/25 | Loss: 0.00117029
Iteration 21/25 | Loss: 0.00117029
Iteration 22/25 | Loss: 0.00117029
Iteration 23/25 | Loss: 0.00117029
Iteration 24/25 | Loss: 0.00117029
Iteration 25/25 | Loss: 0.00117029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81642437
Iteration 2/25 | Loss: 0.00056423
Iteration 3/25 | Loss: 0.00056422
Iteration 4/25 | Loss: 0.00056422
Iteration 5/25 | Loss: 0.00056422
Iteration 6/25 | Loss: 0.00056422
Iteration 7/25 | Loss: 0.00056422
Iteration 8/25 | Loss: 0.00056422
Iteration 9/25 | Loss: 0.00056422
Iteration 10/25 | Loss: 0.00056422
Iteration 11/25 | Loss: 0.00056422
Iteration 12/25 | Loss: 0.00056422
Iteration 13/25 | Loss: 0.00056422
Iteration 14/25 | Loss: 0.00056422
Iteration 15/25 | Loss: 0.00056422
Iteration 16/25 | Loss: 0.00056422
Iteration 17/25 | Loss: 0.00056422
Iteration 18/25 | Loss: 0.00056422
Iteration 19/25 | Loss: 0.00056422
Iteration 20/25 | Loss: 0.00056422
Iteration 21/25 | Loss: 0.00056422
Iteration 22/25 | Loss: 0.00056422
Iteration 23/25 | Loss: 0.00056422
Iteration 24/25 | Loss: 0.00056422
Iteration 25/25 | Loss: 0.00056422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056422
Iteration 2/1000 | Loss: 0.00005305
Iteration 3/1000 | Loss: 0.00003978
Iteration 4/1000 | Loss: 0.00003587
Iteration 5/1000 | Loss: 0.00003359
Iteration 6/1000 | Loss: 0.00003246
Iteration 7/1000 | Loss: 0.00003144
Iteration 8/1000 | Loss: 0.00003083
Iteration 9/1000 | Loss: 0.00003041
Iteration 10/1000 | Loss: 0.00003015
Iteration 11/1000 | Loss: 0.00002999
Iteration 12/1000 | Loss: 0.00002993
Iteration 13/1000 | Loss: 0.00002993
Iteration 14/1000 | Loss: 0.00002967
Iteration 15/1000 | Loss: 0.00002965
Iteration 16/1000 | Loss: 0.00002964
Iteration 17/1000 | Loss: 0.00002964
Iteration 18/1000 | Loss: 0.00002960
Iteration 19/1000 | Loss: 0.00002960
Iteration 20/1000 | Loss: 0.00002959
Iteration 21/1000 | Loss: 0.00002959
Iteration 22/1000 | Loss: 0.00002958
Iteration 23/1000 | Loss: 0.00002954
Iteration 24/1000 | Loss: 0.00002954
Iteration 25/1000 | Loss: 0.00002954
Iteration 26/1000 | Loss: 0.00002953
Iteration 27/1000 | Loss: 0.00002953
Iteration 28/1000 | Loss: 0.00002953
Iteration 29/1000 | Loss: 0.00002953
Iteration 30/1000 | Loss: 0.00002953
Iteration 31/1000 | Loss: 0.00002953
Iteration 32/1000 | Loss: 0.00002951
Iteration 33/1000 | Loss: 0.00002950
Iteration 34/1000 | Loss: 0.00002950
Iteration 35/1000 | Loss: 0.00002950
Iteration 36/1000 | Loss: 0.00002950
Iteration 37/1000 | Loss: 0.00002949
Iteration 38/1000 | Loss: 0.00002949
Iteration 39/1000 | Loss: 0.00002949
Iteration 40/1000 | Loss: 0.00002949
Iteration 41/1000 | Loss: 0.00002949
Iteration 42/1000 | Loss: 0.00002948
Iteration 43/1000 | Loss: 0.00002948
Iteration 44/1000 | Loss: 0.00002948
Iteration 45/1000 | Loss: 0.00002948
Iteration 46/1000 | Loss: 0.00002948
Iteration 47/1000 | Loss: 0.00002948
Iteration 48/1000 | Loss: 0.00002948
Iteration 49/1000 | Loss: 0.00002948
Iteration 50/1000 | Loss: 0.00002948
Iteration 51/1000 | Loss: 0.00002948
Iteration 52/1000 | Loss: 0.00002948
Iteration 53/1000 | Loss: 0.00002948
Iteration 54/1000 | Loss: 0.00002948
Iteration 55/1000 | Loss: 0.00002948
Iteration 56/1000 | Loss: 0.00002948
Iteration 57/1000 | Loss: 0.00002948
Iteration 58/1000 | Loss: 0.00002948
Iteration 59/1000 | Loss: 0.00002948
Iteration 60/1000 | Loss: 0.00002947
Iteration 61/1000 | Loss: 0.00002947
Iteration 62/1000 | Loss: 0.00002947
Iteration 63/1000 | Loss: 0.00002947
Iteration 64/1000 | Loss: 0.00002947
Iteration 65/1000 | Loss: 0.00002947
Iteration 66/1000 | Loss: 0.00002947
Iteration 67/1000 | Loss: 0.00002947
Iteration 68/1000 | Loss: 0.00002947
Iteration 69/1000 | Loss: 0.00002947
Iteration 70/1000 | Loss: 0.00002947
Iteration 71/1000 | Loss: 0.00002947
Iteration 72/1000 | Loss: 0.00002946
Iteration 73/1000 | Loss: 0.00002946
Iteration 74/1000 | Loss: 0.00002946
Iteration 75/1000 | Loss: 0.00002946
Iteration 76/1000 | Loss: 0.00002946
Iteration 77/1000 | Loss: 0.00002946
Iteration 78/1000 | Loss: 0.00002946
Iteration 79/1000 | Loss: 0.00002945
Iteration 80/1000 | Loss: 0.00002945
Iteration 81/1000 | Loss: 0.00002945
Iteration 82/1000 | Loss: 0.00002945
Iteration 83/1000 | Loss: 0.00002945
Iteration 84/1000 | Loss: 0.00002945
Iteration 85/1000 | Loss: 0.00002945
Iteration 86/1000 | Loss: 0.00002944
Iteration 87/1000 | Loss: 0.00002944
Iteration 88/1000 | Loss: 0.00002944
Iteration 89/1000 | Loss: 0.00002944
Iteration 90/1000 | Loss: 0.00002944
Iteration 91/1000 | Loss: 0.00002944
Iteration 92/1000 | Loss: 0.00002944
Iteration 93/1000 | Loss: 0.00002944
Iteration 94/1000 | Loss: 0.00002944
Iteration 95/1000 | Loss: 0.00002944
Iteration 96/1000 | Loss: 0.00002944
Iteration 97/1000 | Loss: 0.00002944
Iteration 98/1000 | Loss: 0.00002944
Iteration 99/1000 | Loss: 0.00002944
Iteration 100/1000 | Loss: 0.00002944
Iteration 101/1000 | Loss: 0.00002944
Iteration 102/1000 | Loss: 0.00002943
Iteration 103/1000 | Loss: 0.00002943
Iteration 104/1000 | Loss: 0.00002943
Iteration 105/1000 | Loss: 0.00002943
Iteration 106/1000 | Loss: 0.00002943
Iteration 107/1000 | Loss: 0.00002943
Iteration 108/1000 | Loss: 0.00002943
Iteration 109/1000 | Loss: 0.00002943
Iteration 110/1000 | Loss: 0.00002943
Iteration 111/1000 | Loss: 0.00002943
Iteration 112/1000 | Loss: 0.00002943
Iteration 113/1000 | Loss: 0.00002943
Iteration 114/1000 | Loss: 0.00002943
Iteration 115/1000 | Loss: 0.00002943
Iteration 116/1000 | Loss: 0.00002943
Iteration 117/1000 | Loss: 0.00002943
Iteration 118/1000 | Loss: 0.00002943
Iteration 119/1000 | Loss: 0.00002943
Iteration 120/1000 | Loss: 0.00002943
Iteration 121/1000 | Loss: 0.00002943
Iteration 122/1000 | Loss: 0.00002943
Iteration 123/1000 | Loss: 0.00002943
Iteration 124/1000 | Loss: 0.00002943
Iteration 125/1000 | Loss: 0.00002943
Iteration 126/1000 | Loss: 0.00002943
Iteration 127/1000 | Loss: 0.00002943
Iteration 128/1000 | Loss: 0.00002943
Iteration 129/1000 | Loss: 0.00002943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.943108484032564e-05, 2.943108484032564e-05, 2.943108484032564e-05, 2.943108484032564e-05, 2.943108484032564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.943108484032564e-05

Optimization complete. Final v2v error: 4.598128795623779 mm

Highest mean error: 4.83153772354126 mm for frame 147

Lowest mean error: 4.481289386749268 mm for frame 36

Saving results

Total time: 43.356507539749146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00280275
Iteration 2/25 | Loss: 0.00125065
Iteration 3/25 | Loss: 0.00109812
Iteration 4/25 | Loss: 0.00108044
Iteration 5/25 | Loss: 0.00107461
Iteration 6/25 | Loss: 0.00107229
Iteration 7/25 | Loss: 0.00107127
Iteration 8/25 | Loss: 0.00107118
Iteration 9/25 | Loss: 0.00107118
Iteration 10/25 | Loss: 0.00107118
Iteration 11/25 | Loss: 0.00107118
Iteration 12/25 | Loss: 0.00107118
Iteration 13/25 | Loss: 0.00107118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010711821960285306, 0.0010711821960285306, 0.0010711821960285306, 0.0010711821960285306, 0.0010711821960285306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010711821960285306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19902933
Iteration 2/25 | Loss: 0.00191870
Iteration 3/25 | Loss: 0.00191870
Iteration 4/25 | Loss: 0.00191870
Iteration 5/25 | Loss: 0.00191870
Iteration 6/25 | Loss: 0.00191870
Iteration 7/25 | Loss: 0.00191870
Iteration 8/25 | Loss: 0.00191870
Iteration 9/25 | Loss: 0.00191870
Iteration 10/25 | Loss: 0.00191870
Iteration 11/25 | Loss: 0.00191870
Iteration 12/25 | Loss: 0.00191870
Iteration 13/25 | Loss: 0.00191870
Iteration 14/25 | Loss: 0.00191870
Iteration 15/25 | Loss: 0.00191870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019186981953680515, 0.0019186981953680515, 0.0019186981953680515, 0.0019186981953680515, 0.0019186981953680515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019186981953680515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191870
Iteration 2/1000 | Loss: 0.00004296
Iteration 3/1000 | Loss: 0.00002439
Iteration 4/1000 | Loss: 0.00001879
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001510
Iteration 7/1000 | Loss: 0.00001438
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001375
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001360
Iteration 12/1000 | Loss: 0.00001342
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001326
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001325
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001322
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001315
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001311
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001308
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001307
Iteration 41/1000 | Loss: 0.00001307
Iteration 42/1000 | Loss: 0.00001307
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001306
Iteration 45/1000 | Loss: 0.00001306
Iteration 46/1000 | Loss: 0.00001306
Iteration 47/1000 | Loss: 0.00001305
Iteration 48/1000 | Loss: 0.00001305
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001304
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001303
Iteration 57/1000 | Loss: 0.00001303
Iteration 58/1000 | Loss: 0.00001303
Iteration 59/1000 | Loss: 0.00001303
Iteration 60/1000 | Loss: 0.00001303
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001302
Iteration 66/1000 | Loss: 0.00001302
Iteration 67/1000 | Loss: 0.00001302
Iteration 68/1000 | Loss: 0.00001302
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001302
Iteration 71/1000 | Loss: 0.00001302
Iteration 72/1000 | Loss: 0.00001302
Iteration 73/1000 | Loss: 0.00001302
Iteration 74/1000 | Loss: 0.00001302
Iteration 75/1000 | Loss: 0.00001301
Iteration 76/1000 | Loss: 0.00001301
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001301
Iteration 82/1000 | Loss: 0.00001301
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001300
Iteration 85/1000 | Loss: 0.00001300
Iteration 86/1000 | Loss: 0.00001300
Iteration 87/1000 | Loss: 0.00001300
Iteration 88/1000 | Loss: 0.00001300
Iteration 89/1000 | Loss: 0.00001300
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.3002728337596636e-05, 1.3002728337596636e-05, 1.3002728337596636e-05, 1.3002728337596636e-05, 1.3002728337596636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3002728337596636e-05

Optimization complete. Final v2v error: 3.0977182388305664 mm

Highest mean error: 3.37022066116333 mm for frame 161

Lowest mean error: 2.6123247146606445 mm for frame 0

Saving results

Total time: 37.779099464416504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503987
Iteration 2/25 | Loss: 0.00126350
Iteration 3/25 | Loss: 0.00116601
Iteration 4/25 | Loss: 0.00114547
Iteration 5/25 | Loss: 0.00114166
Iteration 6/25 | Loss: 0.00114145
Iteration 7/25 | Loss: 0.00114145
Iteration 8/25 | Loss: 0.00114145
Iteration 9/25 | Loss: 0.00114145
Iteration 10/25 | Loss: 0.00114145
Iteration 11/25 | Loss: 0.00114145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011414482723921537, 0.0011414482723921537, 0.0011414482723921537, 0.0011414482723921537, 0.0011414482723921537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011414482723921537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.57203865
Iteration 2/25 | Loss: 0.00130837
Iteration 3/25 | Loss: 0.00130837
Iteration 4/25 | Loss: 0.00130837
Iteration 5/25 | Loss: 0.00130837
Iteration 6/25 | Loss: 0.00130837
Iteration 7/25 | Loss: 0.00130836
Iteration 8/25 | Loss: 0.00130836
Iteration 9/25 | Loss: 0.00130836
Iteration 10/25 | Loss: 0.00130836
Iteration 11/25 | Loss: 0.00130836
Iteration 12/25 | Loss: 0.00130836
Iteration 13/25 | Loss: 0.00130836
Iteration 14/25 | Loss: 0.00130836
Iteration 15/25 | Loss: 0.00130836
Iteration 16/25 | Loss: 0.00130836
Iteration 17/25 | Loss: 0.00130836
Iteration 18/25 | Loss: 0.00130836
Iteration 19/25 | Loss: 0.00130836
Iteration 20/25 | Loss: 0.00130836
Iteration 21/25 | Loss: 0.00130836
Iteration 22/25 | Loss: 0.00130836
Iteration 23/25 | Loss: 0.00130836
Iteration 24/25 | Loss: 0.00130836
Iteration 25/25 | Loss: 0.00130836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00130836374592036, 0.00130836374592036, 0.00130836374592036, 0.00130836374592036, 0.00130836374592036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00130836374592036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130836
Iteration 2/1000 | Loss: 0.00005047
Iteration 3/1000 | Loss: 0.00003336
Iteration 4/1000 | Loss: 0.00003020
Iteration 5/1000 | Loss: 0.00002839
Iteration 6/1000 | Loss: 0.00002718
Iteration 7/1000 | Loss: 0.00002628
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00002534
Iteration 10/1000 | Loss: 0.00002502
Iteration 11/1000 | Loss: 0.00002476
Iteration 12/1000 | Loss: 0.00002473
Iteration 13/1000 | Loss: 0.00002471
Iteration 14/1000 | Loss: 0.00002453
Iteration 15/1000 | Loss: 0.00002433
Iteration 16/1000 | Loss: 0.00002428
Iteration 17/1000 | Loss: 0.00002424
Iteration 18/1000 | Loss: 0.00002424
Iteration 19/1000 | Loss: 0.00002423
Iteration 20/1000 | Loss: 0.00002422
Iteration 21/1000 | Loss: 0.00002421
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002420
Iteration 24/1000 | Loss: 0.00002420
Iteration 25/1000 | Loss: 0.00002419
Iteration 26/1000 | Loss: 0.00002418
Iteration 27/1000 | Loss: 0.00002418
Iteration 28/1000 | Loss: 0.00002417
Iteration 29/1000 | Loss: 0.00002415
Iteration 30/1000 | Loss: 0.00002414
Iteration 31/1000 | Loss: 0.00002413
Iteration 32/1000 | Loss: 0.00002412
Iteration 33/1000 | Loss: 0.00002412
Iteration 34/1000 | Loss: 0.00002411
Iteration 35/1000 | Loss: 0.00002410
Iteration 36/1000 | Loss: 0.00002408
Iteration 37/1000 | Loss: 0.00002407
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002406
Iteration 40/1000 | Loss: 0.00002405
Iteration 41/1000 | Loss: 0.00002404
Iteration 42/1000 | Loss: 0.00002403
Iteration 43/1000 | Loss: 0.00002403
Iteration 44/1000 | Loss: 0.00002403
Iteration 45/1000 | Loss: 0.00002402
Iteration 46/1000 | Loss: 0.00002402
Iteration 47/1000 | Loss: 0.00002402
Iteration 48/1000 | Loss: 0.00002402
Iteration 49/1000 | Loss: 0.00002402
Iteration 50/1000 | Loss: 0.00002401
Iteration 51/1000 | Loss: 0.00002401
Iteration 52/1000 | Loss: 0.00002401
Iteration 53/1000 | Loss: 0.00002401
Iteration 54/1000 | Loss: 0.00002400
Iteration 55/1000 | Loss: 0.00002400
Iteration 56/1000 | Loss: 0.00002400
Iteration 57/1000 | Loss: 0.00002399
Iteration 58/1000 | Loss: 0.00002399
Iteration 59/1000 | Loss: 0.00002399
Iteration 60/1000 | Loss: 0.00002398
Iteration 61/1000 | Loss: 0.00002398
Iteration 62/1000 | Loss: 0.00002398
Iteration 63/1000 | Loss: 0.00002397
Iteration 64/1000 | Loss: 0.00002397
Iteration 65/1000 | Loss: 0.00002397
Iteration 66/1000 | Loss: 0.00002397
Iteration 67/1000 | Loss: 0.00002397
Iteration 68/1000 | Loss: 0.00002396
Iteration 69/1000 | Loss: 0.00002396
Iteration 70/1000 | Loss: 0.00002396
Iteration 71/1000 | Loss: 0.00002396
Iteration 72/1000 | Loss: 0.00002396
Iteration 73/1000 | Loss: 0.00002395
Iteration 74/1000 | Loss: 0.00002395
Iteration 75/1000 | Loss: 0.00002395
Iteration 76/1000 | Loss: 0.00002395
Iteration 77/1000 | Loss: 0.00002395
Iteration 78/1000 | Loss: 0.00002395
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002395
Iteration 81/1000 | Loss: 0.00002394
Iteration 82/1000 | Loss: 0.00002394
Iteration 83/1000 | Loss: 0.00002394
Iteration 84/1000 | Loss: 0.00002394
Iteration 85/1000 | Loss: 0.00002394
Iteration 86/1000 | Loss: 0.00002394
Iteration 87/1000 | Loss: 0.00002394
Iteration 88/1000 | Loss: 0.00002394
Iteration 89/1000 | Loss: 0.00002394
Iteration 90/1000 | Loss: 0.00002394
Iteration 91/1000 | Loss: 0.00002393
Iteration 92/1000 | Loss: 0.00002393
Iteration 93/1000 | Loss: 0.00002393
Iteration 94/1000 | Loss: 0.00002393
Iteration 95/1000 | Loss: 0.00002393
Iteration 96/1000 | Loss: 0.00002393
Iteration 97/1000 | Loss: 0.00002392
Iteration 98/1000 | Loss: 0.00002392
Iteration 99/1000 | Loss: 0.00002392
Iteration 100/1000 | Loss: 0.00002392
Iteration 101/1000 | Loss: 0.00002392
Iteration 102/1000 | Loss: 0.00002392
Iteration 103/1000 | Loss: 0.00002392
Iteration 104/1000 | Loss: 0.00002392
Iteration 105/1000 | Loss: 0.00002392
Iteration 106/1000 | Loss: 0.00002392
Iteration 107/1000 | Loss: 0.00002392
Iteration 108/1000 | Loss: 0.00002391
Iteration 109/1000 | Loss: 0.00002391
Iteration 110/1000 | Loss: 0.00002391
Iteration 111/1000 | Loss: 0.00002391
Iteration 112/1000 | Loss: 0.00002391
Iteration 113/1000 | Loss: 0.00002391
Iteration 114/1000 | Loss: 0.00002391
Iteration 115/1000 | Loss: 0.00002391
Iteration 116/1000 | Loss: 0.00002391
Iteration 117/1000 | Loss: 0.00002391
Iteration 118/1000 | Loss: 0.00002391
Iteration 119/1000 | Loss: 0.00002391
Iteration 120/1000 | Loss: 0.00002390
Iteration 121/1000 | Loss: 0.00002390
Iteration 122/1000 | Loss: 0.00002390
Iteration 123/1000 | Loss: 0.00002390
Iteration 124/1000 | Loss: 0.00002390
Iteration 125/1000 | Loss: 0.00002390
Iteration 126/1000 | Loss: 0.00002390
Iteration 127/1000 | Loss: 0.00002390
Iteration 128/1000 | Loss: 0.00002389
Iteration 129/1000 | Loss: 0.00002389
Iteration 130/1000 | Loss: 0.00002389
Iteration 131/1000 | Loss: 0.00002389
Iteration 132/1000 | Loss: 0.00002389
Iteration 133/1000 | Loss: 0.00002389
Iteration 134/1000 | Loss: 0.00002389
Iteration 135/1000 | Loss: 0.00002389
Iteration 136/1000 | Loss: 0.00002389
Iteration 137/1000 | Loss: 0.00002389
Iteration 138/1000 | Loss: 0.00002388
Iteration 139/1000 | Loss: 0.00002388
Iteration 140/1000 | Loss: 0.00002388
Iteration 141/1000 | Loss: 0.00002388
Iteration 142/1000 | Loss: 0.00002388
Iteration 143/1000 | Loss: 0.00002388
Iteration 144/1000 | Loss: 0.00002388
Iteration 145/1000 | Loss: 0.00002388
Iteration 146/1000 | Loss: 0.00002388
Iteration 147/1000 | Loss: 0.00002388
Iteration 148/1000 | Loss: 0.00002388
Iteration 149/1000 | Loss: 0.00002388
Iteration 150/1000 | Loss: 0.00002388
Iteration 151/1000 | Loss: 0.00002387
Iteration 152/1000 | Loss: 0.00002387
Iteration 153/1000 | Loss: 0.00002387
Iteration 154/1000 | Loss: 0.00002387
Iteration 155/1000 | Loss: 0.00002387
Iteration 156/1000 | Loss: 0.00002387
Iteration 157/1000 | Loss: 0.00002387
Iteration 158/1000 | Loss: 0.00002387
Iteration 159/1000 | Loss: 0.00002387
Iteration 160/1000 | Loss: 0.00002387
Iteration 161/1000 | Loss: 0.00002386
Iteration 162/1000 | Loss: 0.00002386
Iteration 163/1000 | Loss: 0.00002386
Iteration 164/1000 | Loss: 0.00002386
Iteration 165/1000 | Loss: 0.00002386
Iteration 166/1000 | Loss: 0.00002386
Iteration 167/1000 | Loss: 0.00002386
Iteration 168/1000 | Loss: 0.00002386
Iteration 169/1000 | Loss: 0.00002385
Iteration 170/1000 | Loss: 0.00002385
Iteration 171/1000 | Loss: 0.00002385
Iteration 172/1000 | Loss: 0.00002385
Iteration 173/1000 | Loss: 0.00002385
Iteration 174/1000 | Loss: 0.00002385
Iteration 175/1000 | Loss: 0.00002385
Iteration 176/1000 | Loss: 0.00002385
Iteration 177/1000 | Loss: 0.00002385
Iteration 178/1000 | Loss: 0.00002385
Iteration 179/1000 | Loss: 0.00002385
Iteration 180/1000 | Loss: 0.00002385
Iteration 181/1000 | Loss: 0.00002385
Iteration 182/1000 | Loss: 0.00002385
Iteration 183/1000 | Loss: 0.00002385
Iteration 184/1000 | Loss: 0.00002385
Iteration 185/1000 | Loss: 0.00002384
Iteration 186/1000 | Loss: 0.00002384
Iteration 187/1000 | Loss: 0.00002384
Iteration 188/1000 | Loss: 0.00002384
Iteration 189/1000 | Loss: 0.00002384
Iteration 190/1000 | Loss: 0.00002384
Iteration 191/1000 | Loss: 0.00002384
Iteration 192/1000 | Loss: 0.00002384
Iteration 193/1000 | Loss: 0.00002384
Iteration 194/1000 | Loss: 0.00002384
Iteration 195/1000 | Loss: 0.00002384
Iteration 196/1000 | Loss: 0.00002384
Iteration 197/1000 | Loss: 0.00002384
Iteration 198/1000 | Loss: 0.00002384
Iteration 199/1000 | Loss: 0.00002384
Iteration 200/1000 | Loss: 0.00002384
Iteration 201/1000 | Loss: 0.00002384
Iteration 202/1000 | Loss: 0.00002383
Iteration 203/1000 | Loss: 0.00002383
Iteration 204/1000 | Loss: 0.00002383
Iteration 205/1000 | Loss: 0.00002383
Iteration 206/1000 | Loss: 0.00002383
Iteration 207/1000 | Loss: 0.00002383
Iteration 208/1000 | Loss: 0.00002383
Iteration 209/1000 | Loss: 0.00002383
Iteration 210/1000 | Loss: 0.00002383
Iteration 211/1000 | Loss: 0.00002383
Iteration 212/1000 | Loss: 0.00002383
Iteration 213/1000 | Loss: 0.00002383
Iteration 214/1000 | Loss: 0.00002383
Iteration 215/1000 | Loss: 0.00002383
Iteration 216/1000 | Loss: 0.00002383
Iteration 217/1000 | Loss: 0.00002383
Iteration 218/1000 | Loss: 0.00002383
Iteration 219/1000 | Loss: 0.00002383
Iteration 220/1000 | Loss: 0.00002383
Iteration 221/1000 | Loss: 0.00002383
Iteration 222/1000 | Loss: 0.00002383
Iteration 223/1000 | Loss: 0.00002383
Iteration 224/1000 | Loss: 0.00002383
Iteration 225/1000 | Loss: 0.00002383
Iteration 226/1000 | Loss: 0.00002383
Iteration 227/1000 | Loss: 0.00002383
Iteration 228/1000 | Loss: 0.00002383
Iteration 229/1000 | Loss: 0.00002383
Iteration 230/1000 | Loss: 0.00002383
Iteration 231/1000 | Loss: 0.00002383
Iteration 232/1000 | Loss: 0.00002383
Iteration 233/1000 | Loss: 0.00002383
Iteration 234/1000 | Loss: 0.00002383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.3833623345126398e-05, 2.3833623345126398e-05, 2.3833623345126398e-05, 2.3833623345126398e-05, 2.3833623345126398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3833623345126398e-05

Optimization complete. Final v2v error: 4.21701717376709 mm

Highest mean error: 4.511056900024414 mm for frame 122

Lowest mean error: 3.8800017833709717 mm for frame 39

Saving results

Total time: 39.60256099700928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769252
Iteration 2/25 | Loss: 0.00145490
Iteration 3/25 | Loss: 0.00116879
Iteration 4/25 | Loss: 0.00113382
Iteration 5/25 | Loss: 0.00112867
Iteration 6/25 | Loss: 0.00112867
Iteration 7/25 | Loss: 0.00112867
Iteration 8/25 | Loss: 0.00112867
Iteration 9/25 | Loss: 0.00112867
Iteration 10/25 | Loss: 0.00112867
Iteration 11/25 | Loss: 0.00112867
Iteration 12/25 | Loss: 0.00112867
Iteration 13/25 | Loss: 0.00112867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011286715744063258, 0.0011286715744063258, 0.0011286715744063258, 0.0011286715744063258, 0.0011286715744063258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011286715744063258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21524179
Iteration 2/25 | Loss: 0.00097139
Iteration 3/25 | Loss: 0.00097138
Iteration 4/25 | Loss: 0.00097138
Iteration 5/25 | Loss: 0.00097138
Iteration 6/25 | Loss: 0.00097138
Iteration 7/25 | Loss: 0.00097138
Iteration 8/25 | Loss: 0.00097138
Iteration 9/25 | Loss: 0.00097138
Iteration 10/25 | Loss: 0.00097138
Iteration 11/25 | Loss: 0.00097138
Iteration 12/25 | Loss: 0.00097138
Iteration 13/25 | Loss: 0.00097138
Iteration 14/25 | Loss: 0.00097138
Iteration 15/25 | Loss: 0.00097138
Iteration 16/25 | Loss: 0.00097138
Iteration 17/25 | Loss: 0.00097138
Iteration 18/25 | Loss: 0.00097138
Iteration 19/25 | Loss: 0.00097138
Iteration 20/25 | Loss: 0.00097138
Iteration 21/25 | Loss: 0.00097138
Iteration 22/25 | Loss: 0.00097138
Iteration 23/25 | Loss: 0.00097138
Iteration 24/25 | Loss: 0.00097138
Iteration 25/25 | Loss: 0.00097138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097138
Iteration 2/1000 | Loss: 0.00004545
Iteration 3/1000 | Loss: 0.00002766
Iteration 4/1000 | Loss: 0.00002442
Iteration 5/1000 | Loss: 0.00002274
Iteration 6/1000 | Loss: 0.00002146
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00002017
Iteration 9/1000 | Loss: 0.00001958
Iteration 10/1000 | Loss: 0.00001934
Iteration 11/1000 | Loss: 0.00001909
Iteration 12/1000 | Loss: 0.00001891
Iteration 13/1000 | Loss: 0.00001880
Iteration 14/1000 | Loss: 0.00001879
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001876
Iteration 17/1000 | Loss: 0.00001874
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001854
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001850
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001849
Iteration 26/1000 | Loss: 0.00001848
Iteration 27/1000 | Loss: 0.00001846
Iteration 28/1000 | Loss: 0.00001846
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001841
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001841
Iteration 36/1000 | Loss: 0.00001841
Iteration 37/1000 | Loss: 0.00001839
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001835
Iteration 42/1000 | Loss: 0.00001835
Iteration 43/1000 | Loss: 0.00001834
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001833
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001833
Iteration 48/1000 | Loss: 0.00001833
Iteration 49/1000 | Loss: 0.00001833
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001833
Iteration 52/1000 | Loss: 0.00001833
Iteration 53/1000 | Loss: 0.00001832
Iteration 54/1000 | Loss: 0.00001831
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001830
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001829
Iteration 61/1000 | Loss: 0.00001829
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001828
Iteration 65/1000 | Loss: 0.00001825
Iteration 66/1000 | Loss: 0.00001825
Iteration 67/1000 | Loss: 0.00001824
Iteration 68/1000 | Loss: 0.00001823
Iteration 69/1000 | Loss: 0.00001823
Iteration 70/1000 | Loss: 0.00001823
Iteration 71/1000 | Loss: 0.00001822
Iteration 72/1000 | Loss: 0.00001822
Iteration 73/1000 | Loss: 0.00001822
Iteration 74/1000 | Loss: 0.00001822
Iteration 75/1000 | Loss: 0.00001822
Iteration 76/1000 | Loss: 0.00001821
Iteration 77/1000 | Loss: 0.00001821
Iteration 78/1000 | Loss: 0.00001821
Iteration 79/1000 | Loss: 0.00001820
Iteration 80/1000 | Loss: 0.00001820
Iteration 81/1000 | Loss: 0.00001820
Iteration 82/1000 | Loss: 0.00001819
Iteration 83/1000 | Loss: 0.00001819
Iteration 84/1000 | Loss: 0.00001818
Iteration 85/1000 | Loss: 0.00001817
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001817
Iteration 88/1000 | Loss: 0.00001816
Iteration 89/1000 | Loss: 0.00001816
Iteration 90/1000 | Loss: 0.00001816
Iteration 91/1000 | Loss: 0.00001816
Iteration 92/1000 | Loss: 0.00001816
Iteration 93/1000 | Loss: 0.00001815
Iteration 94/1000 | Loss: 0.00001815
Iteration 95/1000 | Loss: 0.00001815
Iteration 96/1000 | Loss: 0.00001815
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001814
Iteration 99/1000 | Loss: 0.00001814
Iteration 100/1000 | Loss: 0.00001814
Iteration 101/1000 | Loss: 0.00001814
Iteration 102/1000 | Loss: 0.00001814
Iteration 103/1000 | Loss: 0.00001814
Iteration 104/1000 | Loss: 0.00001814
Iteration 105/1000 | Loss: 0.00001814
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001813
Iteration 108/1000 | Loss: 0.00001813
Iteration 109/1000 | Loss: 0.00001813
Iteration 110/1000 | Loss: 0.00001812
Iteration 111/1000 | Loss: 0.00001812
Iteration 112/1000 | Loss: 0.00001812
Iteration 113/1000 | Loss: 0.00001811
Iteration 114/1000 | Loss: 0.00001811
Iteration 115/1000 | Loss: 0.00001811
Iteration 116/1000 | Loss: 0.00001811
Iteration 117/1000 | Loss: 0.00001811
Iteration 118/1000 | Loss: 0.00001810
Iteration 119/1000 | Loss: 0.00001810
Iteration 120/1000 | Loss: 0.00001809
Iteration 121/1000 | Loss: 0.00001809
Iteration 122/1000 | Loss: 0.00001809
Iteration 123/1000 | Loss: 0.00001809
Iteration 124/1000 | Loss: 0.00001808
Iteration 125/1000 | Loss: 0.00001808
Iteration 126/1000 | Loss: 0.00001808
Iteration 127/1000 | Loss: 0.00001808
Iteration 128/1000 | Loss: 0.00001808
Iteration 129/1000 | Loss: 0.00001808
Iteration 130/1000 | Loss: 0.00001808
Iteration 131/1000 | Loss: 0.00001808
Iteration 132/1000 | Loss: 0.00001808
Iteration 133/1000 | Loss: 0.00001808
Iteration 134/1000 | Loss: 0.00001807
Iteration 135/1000 | Loss: 0.00001807
Iteration 136/1000 | Loss: 0.00001807
Iteration 137/1000 | Loss: 0.00001807
Iteration 138/1000 | Loss: 0.00001807
Iteration 139/1000 | Loss: 0.00001807
Iteration 140/1000 | Loss: 0.00001807
Iteration 141/1000 | Loss: 0.00001807
Iteration 142/1000 | Loss: 0.00001807
Iteration 143/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.8071948943543248e-05, 1.8071948943543248e-05, 1.8071948943543248e-05, 1.8071948943543248e-05, 1.8071948943543248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8071948943543248e-05

Optimization complete. Final v2v error: 3.5932698249816895 mm

Highest mean error: 3.8456368446350098 mm for frame 230

Lowest mean error: 3.3573837280273438 mm for frame 30

Saving results

Total time: 45.31163835525513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393423
Iteration 2/25 | Loss: 0.00122094
Iteration 3/25 | Loss: 0.00113067
Iteration 4/25 | Loss: 0.00111778
Iteration 5/25 | Loss: 0.00111348
Iteration 6/25 | Loss: 0.00111348
Iteration 7/25 | Loss: 0.00111348
Iteration 8/25 | Loss: 0.00111348
Iteration 9/25 | Loss: 0.00111348
Iteration 10/25 | Loss: 0.00111348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001113483915105462, 0.001113483915105462, 0.001113483915105462, 0.001113483915105462, 0.001113483915105462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001113483915105462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37213111
Iteration 2/25 | Loss: 0.00114855
Iteration 3/25 | Loss: 0.00114853
Iteration 4/25 | Loss: 0.00114853
Iteration 5/25 | Loss: 0.00114853
Iteration 6/25 | Loss: 0.00114853
Iteration 7/25 | Loss: 0.00114853
Iteration 8/25 | Loss: 0.00114853
Iteration 9/25 | Loss: 0.00114853
Iteration 10/25 | Loss: 0.00114853
Iteration 11/25 | Loss: 0.00114853
Iteration 12/25 | Loss: 0.00114853
Iteration 13/25 | Loss: 0.00114853
Iteration 14/25 | Loss: 0.00114853
Iteration 15/25 | Loss: 0.00114853
Iteration 16/25 | Loss: 0.00114853
Iteration 17/25 | Loss: 0.00114853
Iteration 18/25 | Loss: 0.00114853
Iteration 19/25 | Loss: 0.00114853
Iteration 20/25 | Loss: 0.00114853
Iteration 21/25 | Loss: 0.00114853
Iteration 22/25 | Loss: 0.00114853
Iteration 23/25 | Loss: 0.00114853
Iteration 24/25 | Loss: 0.00114853
Iteration 25/25 | Loss: 0.00114853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114853
Iteration 2/1000 | Loss: 0.00003258
Iteration 3/1000 | Loss: 0.00002215
Iteration 4/1000 | Loss: 0.00001933
Iteration 5/1000 | Loss: 0.00001800
Iteration 6/1000 | Loss: 0.00001741
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001685
Iteration 9/1000 | Loss: 0.00001645
Iteration 10/1000 | Loss: 0.00001625
Iteration 11/1000 | Loss: 0.00001605
Iteration 12/1000 | Loss: 0.00001604
Iteration 13/1000 | Loss: 0.00001603
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001588
Iteration 17/1000 | Loss: 0.00001588
Iteration 18/1000 | Loss: 0.00001588
Iteration 19/1000 | Loss: 0.00001588
Iteration 20/1000 | Loss: 0.00001587
Iteration 21/1000 | Loss: 0.00001585
Iteration 22/1000 | Loss: 0.00001581
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001572
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001568
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001566
Iteration 43/1000 | Loss: 0.00001566
Iteration 44/1000 | Loss: 0.00001566
Iteration 45/1000 | Loss: 0.00001566
Iteration 46/1000 | Loss: 0.00001566
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001565
Iteration 49/1000 | Loss: 0.00001565
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001565
Iteration 53/1000 | Loss: 0.00001564
Iteration 54/1000 | Loss: 0.00001564
Iteration 55/1000 | Loss: 0.00001564
Iteration 56/1000 | Loss: 0.00001563
Iteration 57/1000 | Loss: 0.00001563
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001563
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001562
Iteration 63/1000 | Loss: 0.00001561
Iteration 64/1000 | Loss: 0.00001561
Iteration 65/1000 | Loss: 0.00001561
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001559
Iteration 69/1000 | Loss: 0.00001559
Iteration 70/1000 | Loss: 0.00001559
Iteration 71/1000 | Loss: 0.00001559
Iteration 72/1000 | Loss: 0.00001559
Iteration 73/1000 | Loss: 0.00001558
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001558
Iteration 77/1000 | Loss: 0.00001558
Iteration 78/1000 | Loss: 0.00001558
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001557
Iteration 83/1000 | Loss: 0.00001557
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001556
Iteration 86/1000 | Loss: 0.00001556
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001556
Iteration 90/1000 | Loss: 0.00001556
Iteration 91/1000 | Loss: 0.00001556
Iteration 92/1000 | Loss: 0.00001556
Iteration 93/1000 | Loss: 0.00001556
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001554
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001554
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001554
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001553
Iteration 114/1000 | Loss: 0.00001553
Iteration 115/1000 | Loss: 0.00001553
Iteration 116/1000 | Loss: 0.00001553
Iteration 117/1000 | Loss: 0.00001553
Iteration 118/1000 | Loss: 0.00001553
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001553
Iteration 121/1000 | Loss: 0.00001553
Iteration 122/1000 | Loss: 0.00001553
Iteration 123/1000 | Loss: 0.00001553
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001553
Iteration 127/1000 | Loss: 0.00001553
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001553
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001553
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001553
Iteration 140/1000 | Loss: 0.00001553
Iteration 141/1000 | Loss: 0.00001553
Iteration 142/1000 | Loss: 0.00001553
Iteration 143/1000 | Loss: 0.00001553
Iteration 144/1000 | Loss: 0.00001553
Iteration 145/1000 | Loss: 0.00001553
Iteration 146/1000 | Loss: 0.00001553
Iteration 147/1000 | Loss: 0.00001553
Iteration 148/1000 | Loss: 0.00001553
Iteration 149/1000 | Loss: 0.00001553
Iteration 150/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.5525982234976254e-05, 1.5525982234976254e-05, 1.5525982234976254e-05, 1.5525982234976254e-05, 1.5525982234976254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5525982234976254e-05

Optimization complete. Final v2v error: 3.419191360473633 mm

Highest mean error: 4.109997749328613 mm for frame 148

Lowest mean error: 2.9444644451141357 mm for frame 0

Saving results

Total time: 40.67835330963135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061253
Iteration 2/25 | Loss: 0.00241838
Iteration 3/25 | Loss: 0.00191938
Iteration 4/25 | Loss: 0.00203660
Iteration 5/25 | Loss: 0.00151148
Iteration 6/25 | Loss: 0.00131696
Iteration 7/25 | Loss: 0.00132067
Iteration 8/25 | Loss: 0.00126905
Iteration 9/25 | Loss: 0.00125788
Iteration 10/25 | Loss: 0.00119610
Iteration 11/25 | Loss: 0.00116013
Iteration 12/25 | Loss: 0.00115840
Iteration 13/25 | Loss: 0.00116465
Iteration 14/25 | Loss: 0.00120053
Iteration 15/25 | Loss: 0.00117396
Iteration 16/25 | Loss: 0.00110356
Iteration 17/25 | Loss: 0.00110000
Iteration 18/25 | Loss: 0.00109787
Iteration 19/25 | Loss: 0.00110040
Iteration 20/25 | Loss: 0.00109948
Iteration 21/25 | Loss: 0.00109978
Iteration 22/25 | Loss: 0.00110030
Iteration 23/25 | Loss: 0.00109984
Iteration 24/25 | Loss: 0.00109839
Iteration 25/25 | Loss: 0.00109410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25165725
Iteration 2/25 | Loss: 0.00168480
Iteration 3/25 | Loss: 0.00157930
Iteration 4/25 | Loss: 0.00157930
Iteration 5/25 | Loss: 0.00157930
Iteration 6/25 | Loss: 0.00157930
Iteration 7/25 | Loss: 0.00157930
Iteration 8/25 | Loss: 0.00157930
Iteration 9/25 | Loss: 0.00157930
Iteration 10/25 | Loss: 0.00157930
Iteration 11/25 | Loss: 0.00157930
Iteration 12/25 | Loss: 0.00157930
Iteration 13/25 | Loss: 0.00157930
Iteration 14/25 | Loss: 0.00157930
Iteration 15/25 | Loss: 0.00157930
Iteration 16/25 | Loss: 0.00157930
Iteration 17/25 | Loss: 0.00157930
Iteration 18/25 | Loss: 0.00157930
Iteration 19/25 | Loss: 0.00157930
Iteration 20/25 | Loss: 0.00157930
Iteration 21/25 | Loss: 0.00157930
Iteration 22/25 | Loss: 0.00157930
Iteration 23/25 | Loss: 0.00157929
Iteration 24/25 | Loss: 0.00157929
Iteration 25/25 | Loss: 0.00157929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157929
Iteration 2/1000 | Loss: 0.00018602
Iteration 3/1000 | Loss: 0.00203418
Iteration 4/1000 | Loss: 0.00007233
Iteration 5/1000 | Loss: 0.00133163
Iteration 6/1000 | Loss: 0.00135554
Iteration 7/1000 | Loss: 0.00177722
Iteration 8/1000 | Loss: 0.00260316
Iteration 9/1000 | Loss: 0.00224696
Iteration 10/1000 | Loss: 0.00175316
Iteration 11/1000 | Loss: 0.00187851
Iteration 12/1000 | Loss: 0.00114272
Iteration 13/1000 | Loss: 0.00006316
Iteration 14/1000 | Loss: 0.00014412
Iteration 15/1000 | Loss: 0.00110225
Iteration 16/1000 | Loss: 0.00017053
Iteration 17/1000 | Loss: 0.00028879
Iteration 18/1000 | Loss: 0.00254313
Iteration 19/1000 | Loss: 0.00458801
Iteration 20/1000 | Loss: 0.00185033
Iteration 21/1000 | Loss: 0.00206082
Iteration 22/1000 | Loss: 0.00050607
Iteration 23/1000 | Loss: 0.00135820
Iteration 24/1000 | Loss: 0.00225670
Iteration 25/1000 | Loss: 0.00169479
Iteration 26/1000 | Loss: 0.00150910
Iteration 27/1000 | Loss: 0.00036506
Iteration 28/1000 | Loss: 0.00287691
Iteration 29/1000 | Loss: 0.00127486
Iteration 30/1000 | Loss: 0.00034368
Iteration 31/1000 | Loss: 0.00027958
Iteration 32/1000 | Loss: 0.00172135
Iteration 33/1000 | Loss: 0.00132449
Iteration 34/1000 | Loss: 0.00098086
Iteration 35/1000 | Loss: 0.00088522
Iteration 36/1000 | Loss: 0.00017288
Iteration 37/1000 | Loss: 0.00007588
Iteration 38/1000 | Loss: 0.00005798
Iteration 39/1000 | Loss: 0.00045860
Iteration 40/1000 | Loss: 0.00108907
Iteration 41/1000 | Loss: 0.00059482
Iteration 42/1000 | Loss: 0.00046036
Iteration 43/1000 | Loss: 0.00045862
Iteration 44/1000 | Loss: 0.00053930
Iteration 45/1000 | Loss: 0.00027024
Iteration 46/1000 | Loss: 0.00027564
Iteration 47/1000 | Loss: 0.00046719
Iteration 48/1000 | Loss: 0.00022513
Iteration 49/1000 | Loss: 0.00024945
Iteration 50/1000 | Loss: 0.00033667
Iteration 51/1000 | Loss: 0.00056902
Iteration 52/1000 | Loss: 0.00076519
Iteration 53/1000 | Loss: 0.00030051
Iteration 54/1000 | Loss: 0.00063686
Iteration 55/1000 | Loss: 0.00037086
Iteration 56/1000 | Loss: 0.00007088
Iteration 57/1000 | Loss: 0.00004328
Iteration 58/1000 | Loss: 0.00019495
Iteration 59/1000 | Loss: 0.00003219
Iteration 60/1000 | Loss: 0.00024966
Iteration 61/1000 | Loss: 0.00002922
Iteration 62/1000 | Loss: 0.00002472
Iteration 63/1000 | Loss: 0.00002260
Iteration 64/1000 | Loss: 0.00002080
Iteration 65/1000 | Loss: 0.00002828
Iteration 66/1000 | Loss: 0.00041258
Iteration 67/1000 | Loss: 0.00002817
Iteration 68/1000 | Loss: 0.00002067
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001429
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001356
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001345
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001323
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001321
Iteration 94/1000 | Loss: 0.00001321
Iteration 95/1000 | Loss: 0.00001320
Iteration 96/1000 | Loss: 0.00001319
Iteration 97/1000 | Loss: 0.00001319
Iteration 98/1000 | Loss: 0.00001318
Iteration 99/1000 | Loss: 0.00001318
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001317
Iteration 102/1000 | Loss: 0.00001317
Iteration 103/1000 | Loss: 0.00001316
Iteration 104/1000 | Loss: 0.00001316
Iteration 105/1000 | Loss: 0.00001315
Iteration 106/1000 | Loss: 0.00001315
Iteration 107/1000 | Loss: 0.00001315
Iteration 108/1000 | Loss: 0.00001315
Iteration 109/1000 | Loss: 0.00001314
Iteration 110/1000 | Loss: 0.00001314
Iteration 111/1000 | Loss: 0.00001314
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001314
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001314
Iteration 117/1000 | Loss: 0.00001314
Iteration 118/1000 | Loss: 0.00001313
Iteration 119/1000 | Loss: 0.00001313
Iteration 120/1000 | Loss: 0.00001313
Iteration 121/1000 | Loss: 0.00001313
Iteration 122/1000 | Loss: 0.00001313
Iteration 123/1000 | Loss: 0.00001313
Iteration 124/1000 | Loss: 0.00001312
Iteration 125/1000 | Loss: 0.00001312
Iteration 126/1000 | Loss: 0.00001312
Iteration 127/1000 | Loss: 0.00001312
Iteration 128/1000 | Loss: 0.00001311
Iteration 129/1000 | Loss: 0.00001311
Iteration 130/1000 | Loss: 0.00001311
Iteration 131/1000 | Loss: 0.00001311
Iteration 132/1000 | Loss: 0.00001311
Iteration 133/1000 | Loss: 0.00001311
Iteration 134/1000 | Loss: 0.00001311
Iteration 135/1000 | Loss: 0.00001311
Iteration 136/1000 | Loss: 0.00001310
Iteration 137/1000 | Loss: 0.00001310
Iteration 138/1000 | Loss: 0.00001310
Iteration 139/1000 | Loss: 0.00001310
Iteration 140/1000 | Loss: 0.00001310
Iteration 141/1000 | Loss: 0.00001310
Iteration 142/1000 | Loss: 0.00001310
Iteration 143/1000 | Loss: 0.00001310
Iteration 144/1000 | Loss: 0.00001309
Iteration 145/1000 | Loss: 0.00001309
Iteration 146/1000 | Loss: 0.00001309
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001309
Iteration 149/1000 | Loss: 0.00001309
Iteration 150/1000 | Loss: 0.00001309
Iteration 151/1000 | Loss: 0.00001309
Iteration 152/1000 | Loss: 0.00001309
Iteration 153/1000 | Loss: 0.00001309
Iteration 154/1000 | Loss: 0.00001309
Iteration 155/1000 | Loss: 0.00001309
Iteration 156/1000 | Loss: 0.00001309
Iteration 157/1000 | Loss: 0.00001309
Iteration 158/1000 | Loss: 0.00001309
Iteration 159/1000 | Loss: 0.00001309
Iteration 160/1000 | Loss: 0.00001309
Iteration 161/1000 | Loss: 0.00001309
Iteration 162/1000 | Loss: 0.00001309
Iteration 163/1000 | Loss: 0.00001309
Iteration 164/1000 | Loss: 0.00001309
Iteration 165/1000 | Loss: 0.00001309
Iteration 166/1000 | Loss: 0.00001309
Iteration 167/1000 | Loss: 0.00001308
Iteration 168/1000 | Loss: 0.00001308
Iteration 169/1000 | Loss: 0.00001308
Iteration 170/1000 | Loss: 0.00001308
Iteration 171/1000 | Loss: 0.00001308
Iteration 172/1000 | Loss: 0.00001308
Iteration 173/1000 | Loss: 0.00001308
Iteration 174/1000 | Loss: 0.00001308
Iteration 175/1000 | Loss: 0.00001308
Iteration 176/1000 | Loss: 0.00001308
Iteration 177/1000 | Loss: 0.00001308
Iteration 178/1000 | Loss: 0.00001308
Iteration 179/1000 | Loss: 0.00001308
Iteration 180/1000 | Loss: 0.00001308
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001308
Iteration 186/1000 | Loss: 0.00001308
Iteration 187/1000 | Loss: 0.00001308
Iteration 188/1000 | Loss: 0.00001308
Iteration 189/1000 | Loss: 0.00001308
Iteration 190/1000 | Loss: 0.00001308
Iteration 191/1000 | Loss: 0.00001308
Iteration 192/1000 | Loss: 0.00001308
Iteration 193/1000 | Loss: 0.00001308
Iteration 194/1000 | Loss: 0.00001308
Iteration 195/1000 | Loss: 0.00001308
Iteration 196/1000 | Loss: 0.00001308
Iteration 197/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.3077963558316696e-05, 1.3077963558316696e-05, 1.3077963558316696e-05, 1.3077963558316696e-05, 1.3077963558316696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3077963558316696e-05

Optimization complete. Final v2v error: 3.0334575176239014 mm

Highest mean error: 3.6106042861938477 mm for frame 74

Lowest mean error: 2.692087173461914 mm for frame 113

Saving results

Total time: 158.62361097335815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092968
Iteration 2/25 | Loss: 0.00168216
Iteration 3/25 | Loss: 0.00125167
Iteration 4/25 | Loss: 0.00122188
Iteration 5/25 | Loss: 0.00121493
Iteration 6/25 | Loss: 0.00121299
Iteration 7/25 | Loss: 0.00121299
Iteration 8/25 | Loss: 0.00121299
Iteration 9/25 | Loss: 0.00121299
Iteration 10/25 | Loss: 0.00121299
Iteration 11/25 | Loss: 0.00121299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00121298769954592, 0.00121298769954592, 0.00121298769954592, 0.00121298769954592, 0.00121298769954592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00121298769954592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25371933
Iteration 2/25 | Loss: 0.00167552
Iteration 3/25 | Loss: 0.00167551
Iteration 4/25 | Loss: 0.00167551
Iteration 5/25 | Loss: 0.00167551
Iteration 6/25 | Loss: 0.00167551
Iteration 7/25 | Loss: 0.00167551
Iteration 8/25 | Loss: 0.00167551
Iteration 9/25 | Loss: 0.00167551
Iteration 10/25 | Loss: 0.00167551
Iteration 11/25 | Loss: 0.00167551
Iteration 12/25 | Loss: 0.00167551
Iteration 13/25 | Loss: 0.00167551
Iteration 14/25 | Loss: 0.00167551
Iteration 15/25 | Loss: 0.00167551
Iteration 16/25 | Loss: 0.00167551
Iteration 17/25 | Loss: 0.00167551
Iteration 18/25 | Loss: 0.00167551
Iteration 19/25 | Loss: 0.00167551
Iteration 20/25 | Loss: 0.00167551
Iteration 21/25 | Loss: 0.00167551
Iteration 22/25 | Loss: 0.00167551
Iteration 23/25 | Loss: 0.00167551
Iteration 24/25 | Loss: 0.00167551
Iteration 25/25 | Loss: 0.00167551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167551
Iteration 2/1000 | Loss: 0.00004861
Iteration 3/1000 | Loss: 0.00004090
Iteration 4/1000 | Loss: 0.00003753
Iteration 5/1000 | Loss: 0.00003535
Iteration 6/1000 | Loss: 0.00003448
Iteration 7/1000 | Loss: 0.00003380
Iteration 8/1000 | Loss: 0.00003337
Iteration 9/1000 | Loss: 0.00003297
Iteration 10/1000 | Loss: 0.00003270
Iteration 11/1000 | Loss: 0.00003252
Iteration 12/1000 | Loss: 0.00003251
Iteration 13/1000 | Loss: 0.00003249
Iteration 14/1000 | Loss: 0.00003243
Iteration 15/1000 | Loss: 0.00003237
Iteration 16/1000 | Loss: 0.00003233
Iteration 17/1000 | Loss: 0.00003229
Iteration 18/1000 | Loss: 0.00003226
Iteration 19/1000 | Loss: 0.00003226
Iteration 20/1000 | Loss: 0.00003225
Iteration 21/1000 | Loss: 0.00003224
Iteration 22/1000 | Loss: 0.00003224
Iteration 23/1000 | Loss: 0.00003224
Iteration 24/1000 | Loss: 0.00003224
Iteration 25/1000 | Loss: 0.00003224
Iteration 26/1000 | Loss: 0.00003223
Iteration 27/1000 | Loss: 0.00003223
Iteration 28/1000 | Loss: 0.00003223
Iteration 29/1000 | Loss: 0.00003222
Iteration 30/1000 | Loss: 0.00003221
Iteration 31/1000 | Loss: 0.00003221
Iteration 32/1000 | Loss: 0.00003221
Iteration 33/1000 | Loss: 0.00003221
Iteration 34/1000 | Loss: 0.00003221
Iteration 35/1000 | Loss: 0.00003220
Iteration 36/1000 | Loss: 0.00003220
Iteration 37/1000 | Loss: 0.00003220
Iteration 38/1000 | Loss: 0.00003220
Iteration 39/1000 | Loss: 0.00003220
Iteration 40/1000 | Loss: 0.00003220
Iteration 41/1000 | Loss: 0.00003220
Iteration 42/1000 | Loss: 0.00003220
Iteration 43/1000 | Loss: 0.00003219
Iteration 44/1000 | Loss: 0.00003219
Iteration 45/1000 | Loss: 0.00003219
Iteration 46/1000 | Loss: 0.00003219
Iteration 47/1000 | Loss: 0.00003219
Iteration 48/1000 | Loss: 0.00003219
Iteration 49/1000 | Loss: 0.00003218
Iteration 50/1000 | Loss: 0.00003218
Iteration 51/1000 | Loss: 0.00003217
Iteration 52/1000 | Loss: 0.00003217
Iteration 53/1000 | Loss: 0.00003217
Iteration 54/1000 | Loss: 0.00003217
Iteration 55/1000 | Loss: 0.00003217
Iteration 56/1000 | Loss: 0.00003216
Iteration 57/1000 | Loss: 0.00003216
Iteration 58/1000 | Loss: 0.00003216
Iteration 59/1000 | Loss: 0.00003216
Iteration 60/1000 | Loss: 0.00003216
Iteration 61/1000 | Loss: 0.00003216
Iteration 62/1000 | Loss: 0.00003216
Iteration 63/1000 | Loss: 0.00003216
Iteration 64/1000 | Loss: 0.00003215
Iteration 65/1000 | Loss: 0.00003215
Iteration 66/1000 | Loss: 0.00003215
Iteration 67/1000 | Loss: 0.00003215
Iteration 68/1000 | Loss: 0.00003214
Iteration 69/1000 | Loss: 0.00003214
Iteration 70/1000 | Loss: 0.00003214
Iteration 71/1000 | Loss: 0.00003214
Iteration 72/1000 | Loss: 0.00003214
Iteration 73/1000 | Loss: 0.00003213
Iteration 74/1000 | Loss: 0.00003213
Iteration 75/1000 | Loss: 0.00003213
Iteration 76/1000 | Loss: 0.00003213
Iteration 77/1000 | Loss: 0.00003213
Iteration 78/1000 | Loss: 0.00003213
Iteration 79/1000 | Loss: 0.00003212
Iteration 80/1000 | Loss: 0.00003212
Iteration 81/1000 | Loss: 0.00003212
Iteration 82/1000 | Loss: 0.00003212
Iteration 83/1000 | Loss: 0.00003212
Iteration 84/1000 | Loss: 0.00003212
Iteration 85/1000 | Loss: 0.00003211
Iteration 86/1000 | Loss: 0.00003211
Iteration 87/1000 | Loss: 0.00003211
Iteration 88/1000 | Loss: 0.00003211
Iteration 89/1000 | Loss: 0.00003211
Iteration 90/1000 | Loss: 0.00003211
Iteration 91/1000 | Loss: 0.00003211
Iteration 92/1000 | Loss: 0.00003211
Iteration 93/1000 | Loss: 0.00003211
Iteration 94/1000 | Loss: 0.00003210
Iteration 95/1000 | Loss: 0.00003210
Iteration 96/1000 | Loss: 0.00003210
Iteration 97/1000 | Loss: 0.00003210
Iteration 98/1000 | Loss: 0.00003210
Iteration 99/1000 | Loss: 0.00003209
Iteration 100/1000 | Loss: 0.00003209
Iteration 101/1000 | Loss: 0.00003209
Iteration 102/1000 | Loss: 0.00003209
Iteration 103/1000 | Loss: 0.00003209
Iteration 104/1000 | Loss: 0.00003208
Iteration 105/1000 | Loss: 0.00003208
Iteration 106/1000 | Loss: 0.00003208
Iteration 107/1000 | Loss: 0.00003208
Iteration 108/1000 | Loss: 0.00003208
Iteration 109/1000 | Loss: 0.00003208
Iteration 110/1000 | Loss: 0.00003207
Iteration 111/1000 | Loss: 0.00003207
Iteration 112/1000 | Loss: 0.00003207
Iteration 113/1000 | Loss: 0.00003207
Iteration 114/1000 | Loss: 0.00003207
Iteration 115/1000 | Loss: 0.00003207
Iteration 116/1000 | Loss: 0.00003207
Iteration 117/1000 | Loss: 0.00003207
Iteration 118/1000 | Loss: 0.00003207
Iteration 119/1000 | Loss: 0.00003207
Iteration 120/1000 | Loss: 0.00003207
Iteration 121/1000 | Loss: 0.00003207
Iteration 122/1000 | Loss: 0.00003207
Iteration 123/1000 | Loss: 0.00003207
Iteration 124/1000 | Loss: 0.00003207
Iteration 125/1000 | Loss: 0.00003206
Iteration 126/1000 | Loss: 0.00003206
Iteration 127/1000 | Loss: 0.00003206
Iteration 128/1000 | Loss: 0.00003206
Iteration 129/1000 | Loss: 0.00003206
Iteration 130/1000 | Loss: 0.00003206
Iteration 131/1000 | Loss: 0.00003206
Iteration 132/1000 | Loss: 0.00003206
Iteration 133/1000 | Loss: 0.00003206
Iteration 134/1000 | Loss: 0.00003205
Iteration 135/1000 | Loss: 0.00003205
Iteration 136/1000 | Loss: 0.00003205
Iteration 137/1000 | Loss: 0.00003205
Iteration 138/1000 | Loss: 0.00003205
Iteration 139/1000 | Loss: 0.00003205
Iteration 140/1000 | Loss: 0.00003205
Iteration 141/1000 | Loss: 0.00003205
Iteration 142/1000 | Loss: 0.00003205
Iteration 143/1000 | Loss: 0.00003205
Iteration 144/1000 | Loss: 0.00003204
Iteration 145/1000 | Loss: 0.00003204
Iteration 146/1000 | Loss: 0.00003204
Iteration 147/1000 | Loss: 0.00003204
Iteration 148/1000 | Loss: 0.00003204
Iteration 149/1000 | Loss: 0.00003204
Iteration 150/1000 | Loss: 0.00003204
Iteration 151/1000 | Loss: 0.00003204
Iteration 152/1000 | Loss: 0.00003204
Iteration 153/1000 | Loss: 0.00003203
Iteration 154/1000 | Loss: 0.00003203
Iteration 155/1000 | Loss: 0.00003203
Iteration 156/1000 | Loss: 0.00003203
Iteration 157/1000 | Loss: 0.00003203
Iteration 158/1000 | Loss: 0.00003203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [3.2034728064900264e-05, 3.2034728064900264e-05, 3.2034728064900264e-05, 3.2034728064900264e-05, 3.2034728064900264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2034728064900264e-05

Optimization complete. Final v2v error: 4.520287036895752 mm

Highest mean error: 5.070565223693848 mm for frame 106

Lowest mean error: 3.3914506435394287 mm for frame 1

Saving results

Total time: 42.31401205062866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406350
Iteration 2/25 | Loss: 0.00110567
Iteration 3/25 | Loss: 0.00103810
Iteration 4/25 | Loss: 0.00103024
Iteration 5/25 | Loss: 0.00102761
Iteration 6/25 | Loss: 0.00102711
Iteration 7/25 | Loss: 0.00102711
Iteration 8/25 | Loss: 0.00102711
Iteration 9/25 | Loss: 0.00102711
Iteration 10/25 | Loss: 0.00102711
Iteration 11/25 | Loss: 0.00102711
Iteration 12/25 | Loss: 0.00102711
Iteration 13/25 | Loss: 0.00102711
Iteration 14/25 | Loss: 0.00102711
Iteration 15/25 | Loss: 0.00102711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010271051432937384, 0.0010271051432937384, 0.0010271051432937384, 0.0010271051432937384, 0.0010271051432937384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010271051432937384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00600266
Iteration 2/25 | Loss: 0.00113779
Iteration 3/25 | Loss: 0.00113779
Iteration 4/25 | Loss: 0.00113779
Iteration 5/25 | Loss: 0.00113779
Iteration 6/25 | Loss: 0.00113779
Iteration 7/25 | Loss: 0.00113779
Iteration 8/25 | Loss: 0.00113779
Iteration 9/25 | Loss: 0.00113779
Iteration 10/25 | Loss: 0.00113779
Iteration 11/25 | Loss: 0.00113779
Iteration 12/25 | Loss: 0.00113779
Iteration 13/25 | Loss: 0.00113779
Iteration 14/25 | Loss: 0.00113779
Iteration 15/25 | Loss: 0.00113779
Iteration 16/25 | Loss: 0.00113779
Iteration 17/25 | Loss: 0.00113779
Iteration 18/25 | Loss: 0.00113779
Iteration 19/25 | Loss: 0.00113779
Iteration 20/25 | Loss: 0.00113779
Iteration 21/25 | Loss: 0.00113779
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011377857299521565, 0.0011377857299521565, 0.0011377857299521565, 0.0011377857299521565, 0.0011377857299521565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011377857299521565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113779
Iteration 2/1000 | Loss: 0.00004345
Iteration 3/1000 | Loss: 0.00002192
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001501
Iteration 6/1000 | Loss: 0.00001411
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001312
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001270
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001267
Iteration 15/1000 | Loss: 0.00001266
Iteration 16/1000 | Loss: 0.00001266
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001265
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001265
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001252
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001250
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001249
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001247
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001247
Iteration 70/1000 | Loss: 0.00001247
Iteration 71/1000 | Loss: 0.00001247
Iteration 72/1000 | Loss: 0.00001247
Iteration 73/1000 | Loss: 0.00001247
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001247
Iteration 78/1000 | Loss: 0.00001247
Iteration 79/1000 | Loss: 0.00001247
Iteration 80/1000 | Loss: 0.00001247
Iteration 81/1000 | Loss: 0.00001247
Iteration 82/1000 | Loss: 0.00001247
Iteration 83/1000 | Loss: 0.00001247
Iteration 84/1000 | Loss: 0.00001247
Iteration 85/1000 | Loss: 0.00001247
Iteration 86/1000 | Loss: 0.00001247
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.2471008631109726e-05, 1.2471008631109726e-05, 1.2471008631109726e-05, 1.2471008631109726e-05, 1.2471008631109726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2471008631109726e-05

Optimization complete. Final v2v error: 3.0186212062835693 mm

Highest mean error: 3.2971839904785156 mm for frame 54

Lowest mean error: 2.6315038204193115 mm for frame 1

Saving results

Total time: 27.695889234542847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793330
Iteration 2/25 | Loss: 0.00128534
Iteration 3/25 | Loss: 0.00112735
Iteration 4/25 | Loss: 0.00110893
Iteration 5/25 | Loss: 0.00110432
Iteration 6/25 | Loss: 0.00110342
Iteration 7/25 | Loss: 0.00110342
Iteration 8/25 | Loss: 0.00110342
Iteration 9/25 | Loss: 0.00110342
Iteration 10/25 | Loss: 0.00110342
Iteration 11/25 | Loss: 0.00110342
Iteration 12/25 | Loss: 0.00110342
Iteration 13/25 | Loss: 0.00110342
Iteration 14/25 | Loss: 0.00110342
Iteration 15/25 | Loss: 0.00110342
Iteration 16/25 | Loss: 0.00110342
Iteration 17/25 | Loss: 0.00110342
Iteration 18/25 | Loss: 0.00110342
Iteration 19/25 | Loss: 0.00110342
Iteration 20/25 | Loss: 0.00110342
Iteration 21/25 | Loss: 0.00110342
Iteration 22/25 | Loss: 0.00110342
Iteration 23/25 | Loss: 0.00110342
Iteration 24/25 | Loss: 0.00110342
Iteration 25/25 | Loss: 0.00110342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28917444
Iteration 2/25 | Loss: 0.00154691
Iteration 3/25 | Loss: 0.00154691
Iteration 4/25 | Loss: 0.00154691
Iteration 5/25 | Loss: 0.00154691
Iteration 6/25 | Loss: 0.00154691
Iteration 7/25 | Loss: 0.00154691
Iteration 8/25 | Loss: 0.00154691
Iteration 9/25 | Loss: 0.00154691
Iteration 10/25 | Loss: 0.00154691
Iteration 11/25 | Loss: 0.00154691
Iteration 12/25 | Loss: 0.00154691
Iteration 13/25 | Loss: 0.00154691
Iteration 14/25 | Loss: 0.00154691
Iteration 15/25 | Loss: 0.00154691
Iteration 16/25 | Loss: 0.00154691
Iteration 17/25 | Loss: 0.00154691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015469107311218977, 0.0015469107311218977, 0.0015469107311218977, 0.0015469107311218977, 0.0015469107311218977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015469107311218977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154691
Iteration 2/1000 | Loss: 0.00004948
Iteration 3/1000 | Loss: 0.00002775
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00001981
Iteration 6/1000 | Loss: 0.00001871
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00001735
Iteration 9/1000 | Loss: 0.00001701
Iteration 10/1000 | Loss: 0.00001673
Iteration 11/1000 | Loss: 0.00001650
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001623
Iteration 14/1000 | Loss: 0.00001621
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001594
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001593
Iteration 22/1000 | Loss: 0.00001592
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001588
Iteration 26/1000 | Loss: 0.00001587
Iteration 27/1000 | Loss: 0.00001586
Iteration 28/1000 | Loss: 0.00001586
Iteration 29/1000 | Loss: 0.00001585
Iteration 30/1000 | Loss: 0.00001585
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001583
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001582
Iteration 36/1000 | Loss: 0.00001582
Iteration 37/1000 | Loss: 0.00001581
Iteration 38/1000 | Loss: 0.00001580
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001577
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001575
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001575
Iteration 47/1000 | Loss: 0.00001575
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001574
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001573
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001571
Iteration 64/1000 | Loss: 0.00001571
Iteration 65/1000 | Loss: 0.00001571
Iteration 66/1000 | Loss: 0.00001571
Iteration 67/1000 | Loss: 0.00001570
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001569
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001568
Iteration 73/1000 | Loss: 0.00001568
Iteration 74/1000 | Loss: 0.00001567
Iteration 75/1000 | Loss: 0.00001567
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001565
Iteration 83/1000 | Loss: 0.00001565
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001561
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001561
Iteration 106/1000 | Loss: 0.00001561
Iteration 107/1000 | Loss: 0.00001561
Iteration 108/1000 | Loss: 0.00001561
Iteration 109/1000 | Loss: 0.00001561
Iteration 110/1000 | Loss: 0.00001561
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001560
Iteration 114/1000 | Loss: 0.00001560
Iteration 115/1000 | Loss: 0.00001560
Iteration 116/1000 | Loss: 0.00001560
Iteration 117/1000 | Loss: 0.00001560
Iteration 118/1000 | Loss: 0.00001560
Iteration 119/1000 | Loss: 0.00001560
Iteration 120/1000 | Loss: 0.00001560
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001559
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001558
Iteration 128/1000 | Loss: 0.00001558
Iteration 129/1000 | Loss: 0.00001558
Iteration 130/1000 | Loss: 0.00001558
Iteration 131/1000 | Loss: 0.00001558
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001557
Iteration 140/1000 | Loss: 0.00001557
Iteration 141/1000 | Loss: 0.00001557
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001557
Iteration 144/1000 | Loss: 0.00001556
Iteration 145/1000 | Loss: 0.00001556
Iteration 146/1000 | Loss: 0.00001556
Iteration 147/1000 | Loss: 0.00001556
Iteration 148/1000 | Loss: 0.00001556
Iteration 149/1000 | Loss: 0.00001556
Iteration 150/1000 | Loss: 0.00001556
Iteration 151/1000 | Loss: 0.00001556
Iteration 152/1000 | Loss: 0.00001556
Iteration 153/1000 | Loss: 0.00001556
Iteration 154/1000 | Loss: 0.00001556
Iteration 155/1000 | Loss: 0.00001556
Iteration 156/1000 | Loss: 0.00001556
Iteration 157/1000 | Loss: 0.00001556
Iteration 158/1000 | Loss: 0.00001556
Iteration 159/1000 | Loss: 0.00001556
Iteration 160/1000 | Loss: 0.00001556
Iteration 161/1000 | Loss: 0.00001556
Iteration 162/1000 | Loss: 0.00001556
Iteration 163/1000 | Loss: 0.00001556
Iteration 164/1000 | Loss: 0.00001556
Iteration 165/1000 | Loss: 0.00001556
Iteration 166/1000 | Loss: 0.00001556
Iteration 167/1000 | Loss: 0.00001556
Iteration 168/1000 | Loss: 0.00001555
Iteration 169/1000 | Loss: 0.00001555
Iteration 170/1000 | Loss: 0.00001555
Iteration 171/1000 | Loss: 0.00001555
Iteration 172/1000 | Loss: 0.00001555
Iteration 173/1000 | Loss: 0.00001555
Iteration 174/1000 | Loss: 0.00001555
Iteration 175/1000 | Loss: 0.00001555
Iteration 176/1000 | Loss: 0.00001555
Iteration 177/1000 | Loss: 0.00001555
Iteration 178/1000 | Loss: 0.00001555
Iteration 179/1000 | Loss: 0.00001555
Iteration 180/1000 | Loss: 0.00001555
Iteration 181/1000 | Loss: 0.00001555
Iteration 182/1000 | Loss: 0.00001555
Iteration 183/1000 | Loss: 0.00001555
Iteration 184/1000 | Loss: 0.00001555
Iteration 185/1000 | Loss: 0.00001555
Iteration 186/1000 | Loss: 0.00001555
Iteration 187/1000 | Loss: 0.00001555
Iteration 188/1000 | Loss: 0.00001555
Iteration 189/1000 | Loss: 0.00001554
Iteration 190/1000 | Loss: 0.00001554
Iteration 191/1000 | Loss: 0.00001554
Iteration 192/1000 | Loss: 0.00001554
Iteration 193/1000 | Loss: 0.00001554
Iteration 194/1000 | Loss: 0.00001554
Iteration 195/1000 | Loss: 0.00001554
Iteration 196/1000 | Loss: 0.00001554
Iteration 197/1000 | Loss: 0.00001554
Iteration 198/1000 | Loss: 0.00001554
Iteration 199/1000 | Loss: 0.00001554
Iteration 200/1000 | Loss: 0.00001554
Iteration 201/1000 | Loss: 0.00001554
Iteration 202/1000 | Loss: 0.00001554
Iteration 203/1000 | Loss: 0.00001554
Iteration 204/1000 | Loss: 0.00001554
Iteration 205/1000 | Loss: 0.00001554
Iteration 206/1000 | Loss: 0.00001554
Iteration 207/1000 | Loss: 0.00001554
Iteration 208/1000 | Loss: 0.00001554
Iteration 209/1000 | Loss: 0.00001554
Iteration 210/1000 | Loss: 0.00001554
Iteration 211/1000 | Loss: 0.00001554
Iteration 212/1000 | Loss: 0.00001554
Iteration 213/1000 | Loss: 0.00001553
Iteration 214/1000 | Loss: 0.00001553
Iteration 215/1000 | Loss: 0.00001553
Iteration 216/1000 | Loss: 0.00001553
Iteration 217/1000 | Loss: 0.00001553
Iteration 218/1000 | Loss: 0.00001553
Iteration 219/1000 | Loss: 0.00001553
Iteration 220/1000 | Loss: 0.00001553
Iteration 221/1000 | Loss: 0.00001553
Iteration 222/1000 | Loss: 0.00001553
Iteration 223/1000 | Loss: 0.00001553
Iteration 224/1000 | Loss: 0.00001553
Iteration 225/1000 | Loss: 0.00001553
Iteration 226/1000 | Loss: 0.00001553
Iteration 227/1000 | Loss: 0.00001553
Iteration 228/1000 | Loss: 0.00001553
Iteration 229/1000 | Loss: 0.00001553
Iteration 230/1000 | Loss: 0.00001553
Iteration 231/1000 | Loss: 0.00001553
Iteration 232/1000 | Loss: 0.00001553
Iteration 233/1000 | Loss: 0.00001553
Iteration 234/1000 | Loss: 0.00001553
Iteration 235/1000 | Loss: 0.00001553
Iteration 236/1000 | Loss: 0.00001553
Iteration 237/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.553156653244514e-05, 1.553156653244514e-05, 1.553156653244514e-05, 1.553156653244514e-05, 1.553156653244514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.553156653244514e-05

Optimization complete. Final v2v error: 3.3456625938415527 mm

Highest mean error: 3.712134838104248 mm for frame 206

Lowest mean error: 2.7400643825531006 mm for frame 170

Saving results

Total time: 48.505008935928345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558127
Iteration 2/25 | Loss: 0.00129210
Iteration 3/25 | Loss: 0.00111061
Iteration 4/25 | Loss: 0.00107706
Iteration 5/25 | Loss: 0.00106639
Iteration 6/25 | Loss: 0.00106992
Iteration 7/25 | Loss: 0.00106607
Iteration 8/25 | Loss: 0.00106210
Iteration 9/25 | Loss: 0.00106122
Iteration 10/25 | Loss: 0.00105906
Iteration 11/25 | Loss: 0.00105772
Iteration 12/25 | Loss: 0.00105725
Iteration 13/25 | Loss: 0.00105716
Iteration 14/25 | Loss: 0.00105710
Iteration 15/25 | Loss: 0.00105710
Iteration 16/25 | Loss: 0.00105709
Iteration 17/25 | Loss: 0.00105709
Iteration 18/25 | Loss: 0.00105709
Iteration 19/25 | Loss: 0.00105709
Iteration 20/25 | Loss: 0.00105709
Iteration 21/25 | Loss: 0.00105709
Iteration 22/25 | Loss: 0.00105709
Iteration 23/25 | Loss: 0.00105709
Iteration 24/25 | Loss: 0.00105709
Iteration 25/25 | Loss: 0.00105709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.12539291
Iteration 2/25 | Loss: 0.00122461
Iteration 3/25 | Loss: 0.00118261
Iteration 4/25 | Loss: 0.00118261
Iteration 5/25 | Loss: 0.00118261
Iteration 6/25 | Loss: 0.00118260
Iteration 7/25 | Loss: 0.00118260
Iteration 8/25 | Loss: 0.00118260
Iteration 9/25 | Loss: 0.00118260
Iteration 10/25 | Loss: 0.00118260
Iteration 11/25 | Loss: 0.00118260
Iteration 12/25 | Loss: 0.00118260
Iteration 13/25 | Loss: 0.00118260
Iteration 14/25 | Loss: 0.00118260
Iteration 15/25 | Loss: 0.00118260
Iteration 16/25 | Loss: 0.00118260
Iteration 17/25 | Loss: 0.00118260
Iteration 18/25 | Loss: 0.00118260
Iteration 19/25 | Loss: 0.00118260
Iteration 20/25 | Loss: 0.00118260
Iteration 21/25 | Loss: 0.00118260
Iteration 22/25 | Loss: 0.00118260
Iteration 23/25 | Loss: 0.00118260
Iteration 24/25 | Loss: 0.00118260
Iteration 25/25 | Loss: 0.00118260

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118260
Iteration 2/1000 | Loss: 0.00006903
Iteration 3/1000 | Loss: 0.00002842
Iteration 4/1000 | Loss: 0.00006985
Iteration 5/1000 | Loss: 0.00002314
Iteration 6/1000 | Loss: 0.00003340
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001948
Iteration 9/1000 | Loss: 0.00001941
Iteration 10/1000 | Loss: 0.00007183
Iteration 11/1000 | Loss: 0.00010892
Iteration 12/1000 | Loss: 0.00001905
Iteration 13/1000 | Loss: 0.00001875
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001869
Iteration 16/1000 | Loss: 0.00008821
Iteration 17/1000 | Loss: 0.00008913
Iteration 18/1000 | Loss: 0.00002235
Iteration 19/1000 | Loss: 0.00001972
Iteration 20/1000 | Loss: 0.00002122
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001828
Iteration 23/1000 | Loss: 0.00001820
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00001810
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00006393
Iteration 29/1000 | Loss: 0.00004823
Iteration 30/1000 | Loss: 0.00001918
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00002961
Iteration 33/1000 | Loss: 0.00002959
Iteration 34/1000 | Loss: 0.00016839
Iteration 35/1000 | Loss: 0.00001798
Iteration 36/1000 | Loss: 0.00001731
Iteration 37/1000 | Loss: 0.00003767
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001715
Iteration 44/1000 | Loss: 0.00001715
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001713
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001710
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001813
Iteration 74/1000 | Loss: 0.00001748
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001707
Iteration 78/1000 | Loss: 0.00001707
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001706
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001705
Iteration 87/1000 | Loss: 0.00001705
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001705
Iteration 95/1000 | Loss: 0.00001705
Iteration 96/1000 | Loss: 0.00001705
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.704785790934693e-05, 1.704785790934693e-05, 1.704785790934693e-05, 1.704785790934693e-05, 1.704785790934693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.704785790934693e-05

Optimization complete. Final v2v error: 3.155287981033325 mm

Highest mean error: 20.327478408813477 mm for frame 237

Lowest mean error: 2.793889284133911 mm for frame 126

Saving results

Total time: 89.69081115722656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499499
Iteration 2/25 | Loss: 0.00144730
Iteration 3/25 | Loss: 0.00114125
Iteration 4/25 | Loss: 0.00112039
Iteration 5/25 | Loss: 0.00111719
Iteration 6/25 | Loss: 0.00111671
Iteration 7/25 | Loss: 0.00111671
Iteration 8/25 | Loss: 0.00111671
Iteration 9/25 | Loss: 0.00111671
Iteration 10/25 | Loss: 0.00111671
Iteration 11/25 | Loss: 0.00111671
Iteration 12/25 | Loss: 0.00111671
Iteration 13/25 | Loss: 0.00111671
Iteration 14/25 | Loss: 0.00111671
Iteration 15/25 | Loss: 0.00111671
Iteration 16/25 | Loss: 0.00111671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001116714091040194, 0.001116714091040194, 0.001116714091040194, 0.001116714091040194, 0.001116714091040194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001116714091040194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26789272
Iteration 2/25 | Loss: 0.00099299
Iteration 3/25 | Loss: 0.00099299
Iteration 4/25 | Loss: 0.00099299
Iteration 5/25 | Loss: 0.00099299
Iteration 6/25 | Loss: 0.00099299
Iteration 7/25 | Loss: 0.00099299
Iteration 8/25 | Loss: 0.00099299
Iteration 9/25 | Loss: 0.00099299
Iteration 10/25 | Loss: 0.00099299
Iteration 11/25 | Loss: 0.00099298
Iteration 12/25 | Loss: 0.00099298
Iteration 13/25 | Loss: 0.00099298
Iteration 14/25 | Loss: 0.00099298
Iteration 15/25 | Loss: 0.00099298
Iteration 16/25 | Loss: 0.00099298
Iteration 17/25 | Loss: 0.00099298
Iteration 18/25 | Loss: 0.00099298
Iteration 19/25 | Loss: 0.00099298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009929846273735166, 0.0009929846273735166, 0.0009929846273735166, 0.0009929846273735166, 0.0009929846273735166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009929846273735166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099298
Iteration 2/1000 | Loss: 0.00003367
Iteration 3/1000 | Loss: 0.00002404
Iteration 4/1000 | Loss: 0.00002094
Iteration 5/1000 | Loss: 0.00001987
Iteration 6/1000 | Loss: 0.00001880
Iteration 7/1000 | Loss: 0.00001817
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001757
Iteration 10/1000 | Loss: 0.00001735
Iteration 11/1000 | Loss: 0.00001727
Iteration 12/1000 | Loss: 0.00001723
Iteration 13/1000 | Loss: 0.00001708
Iteration 14/1000 | Loss: 0.00001706
Iteration 15/1000 | Loss: 0.00001705
Iteration 16/1000 | Loss: 0.00001704
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001697
Iteration 19/1000 | Loss: 0.00001689
Iteration 20/1000 | Loss: 0.00001688
Iteration 21/1000 | Loss: 0.00001687
Iteration 22/1000 | Loss: 0.00001687
Iteration 23/1000 | Loss: 0.00001686
Iteration 24/1000 | Loss: 0.00001686
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001685
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001685
Iteration 31/1000 | Loss: 0.00001685
Iteration 32/1000 | Loss: 0.00001685
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001685
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001683
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001682
Iteration 40/1000 | Loss: 0.00001681
Iteration 41/1000 | Loss: 0.00001681
Iteration 42/1000 | Loss: 0.00001680
Iteration 43/1000 | Loss: 0.00001680
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00001679
Iteration 46/1000 | Loss: 0.00001678
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001677
Iteration 49/1000 | Loss: 0.00001677
Iteration 50/1000 | Loss: 0.00001677
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001674
Iteration 59/1000 | Loss: 0.00001671
Iteration 60/1000 | Loss: 0.00001671
Iteration 61/1000 | Loss: 0.00001669
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001669
Iteration 64/1000 | Loss: 0.00001669
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001668
Iteration 68/1000 | Loss: 0.00001668
Iteration 69/1000 | Loss: 0.00001668
Iteration 70/1000 | Loss: 0.00001668
Iteration 71/1000 | Loss: 0.00001668
Iteration 72/1000 | Loss: 0.00001666
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001665
Iteration 75/1000 | Loss: 0.00001665
Iteration 76/1000 | Loss: 0.00001665
Iteration 77/1000 | Loss: 0.00001664
Iteration 78/1000 | Loss: 0.00001664
Iteration 79/1000 | Loss: 0.00001664
Iteration 80/1000 | Loss: 0.00001664
Iteration 81/1000 | Loss: 0.00001664
Iteration 82/1000 | Loss: 0.00001664
Iteration 83/1000 | Loss: 0.00001664
Iteration 84/1000 | Loss: 0.00001664
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001664
Iteration 87/1000 | Loss: 0.00001664
Iteration 88/1000 | Loss: 0.00001663
Iteration 89/1000 | Loss: 0.00001662
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001661
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001659
Iteration 95/1000 | Loss: 0.00001659
Iteration 96/1000 | Loss: 0.00001659
Iteration 97/1000 | Loss: 0.00001659
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001658
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001656
Iteration 105/1000 | Loss: 0.00001656
Iteration 106/1000 | Loss: 0.00001655
Iteration 107/1000 | Loss: 0.00001655
Iteration 108/1000 | Loss: 0.00001655
Iteration 109/1000 | Loss: 0.00001654
Iteration 110/1000 | Loss: 0.00001654
Iteration 111/1000 | Loss: 0.00001654
Iteration 112/1000 | Loss: 0.00001654
Iteration 113/1000 | Loss: 0.00001653
Iteration 114/1000 | Loss: 0.00001653
Iteration 115/1000 | Loss: 0.00001653
Iteration 116/1000 | Loss: 0.00001653
Iteration 117/1000 | Loss: 0.00001653
Iteration 118/1000 | Loss: 0.00001652
Iteration 119/1000 | Loss: 0.00001652
Iteration 120/1000 | Loss: 0.00001652
Iteration 121/1000 | Loss: 0.00001652
Iteration 122/1000 | Loss: 0.00001652
Iteration 123/1000 | Loss: 0.00001652
Iteration 124/1000 | Loss: 0.00001652
Iteration 125/1000 | Loss: 0.00001652
Iteration 126/1000 | Loss: 0.00001652
Iteration 127/1000 | Loss: 0.00001652
Iteration 128/1000 | Loss: 0.00001652
Iteration 129/1000 | Loss: 0.00001652
Iteration 130/1000 | Loss: 0.00001652
Iteration 131/1000 | Loss: 0.00001651
Iteration 132/1000 | Loss: 0.00001651
Iteration 133/1000 | Loss: 0.00001651
Iteration 134/1000 | Loss: 0.00001651
Iteration 135/1000 | Loss: 0.00001651
Iteration 136/1000 | Loss: 0.00001651
Iteration 137/1000 | Loss: 0.00001651
Iteration 138/1000 | Loss: 0.00001650
Iteration 139/1000 | Loss: 0.00001650
Iteration 140/1000 | Loss: 0.00001650
Iteration 141/1000 | Loss: 0.00001650
Iteration 142/1000 | Loss: 0.00001650
Iteration 143/1000 | Loss: 0.00001650
Iteration 144/1000 | Loss: 0.00001650
Iteration 145/1000 | Loss: 0.00001649
Iteration 146/1000 | Loss: 0.00001649
Iteration 147/1000 | Loss: 0.00001649
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001648
Iteration 150/1000 | Loss: 0.00001648
Iteration 151/1000 | Loss: 0.00001648
Iteration 152/1000 | Loss: 0.00001648
Iteration 153/1000 | Loss: 0.00001648
Iteration 154/1000 | Loss: 0.00001648
Iteration 155/1000 | Loss: 0.00001648
Iteration 156/1000 | Loss: 0.00001648
Iteration 157/1000 | Loss: 0.00001648
Iteration 158/1000 | Loss: 0.00001648
Iteration 159/1000 | Loss: 0.00001647
Iteration 160/1000 | Loss: 0.00001647
Iteration 161/1000 | Loss: 0.00001647
Iteration 162/1000 | Loss: 0.00001647
Iteration 163/1000 | Loss: 0.00001646
Iteration 164/1000 | Loss: 0.00001646
Iteration 165/1000 | Loss: 0.00001646
Iteration 166/1000 | Loss: 0.00001646
Iteration 167/1000 | Loss: 0.00001646
Iteration 168/1000 | Loss: 0.00001646
Iteration 169/1000 | Loss: 0.00001646
Iteration 170/1000 | Loss: 0.00001646
Iteration 171/1000 | Loss: 0.00001646
Iteration 172/1000 | Loss: 0.00001645
Iteration 173/1000 | Loss: 0.00001645
Iteration 174/1000 | Loss: 0.00001645
Iteration 175/1000 | Loss: 0.00001645
Iteration 176/1000 | Loss: 0.00001645
Iteration 177/1000 | Loss: 0.00001645
Iteration 178/1000 | Loss: 0.00001644
Iteration 179/1000 | Loss: 0.00001644
Iteration 180/1000 | Loss: 0.00001643
Iteration 181/1000 | Loss: 0.00001643
Iteration 182/1000 | Loss: 0.00001643
Iteration 183/1000 | Loss: 0.00001643
Iteration 184/1000 | Loss: 0.00001643
Iteration 185/1000 | Loss: 0.00001643
Iteration 186/1000 | Loss: 0.00001642
Iteration 187/1000 | Loss: 0.00001642
Iteration 188/1000 | Loss: 0.00001642
Iteration 189/1000 | Loss: 0.00001642
Iteration 190/1000 | Loss: 0.00001642
Iteration 191/1000 | Loss: 0.00001641
Iteration 192/1000 | Loss: 0.00001641
Iteration 193/1000 | Loss: 0.00001641
Iteration 194/1000 | Loss: 0.00001641
Iteration 195/1000 | Loss: 0.00001641
Iteration 196/1000 | Loss: 0.00001641
Iteration 197/1000 | Loss: 0.00001641
Iteration 198/1000 | Loss: 0.00001641
Iteration 199/1000 | Loss: 0.00001640
Iteration 200/1000 | Loss: 0.00001640
Iteration 201/1000 | Loss: 0.00001640
Iteration 202/1000 | Loss: 0.00001640
Iteration 203/1000 | Loss: 0.00001640
Iteration 204/1000 | Loss: 0.00001640
Iteration 205/1000 | Loss: 0.00001639
Iteration 206/1000 | Loss: 0.00001639
Iteration 207/1000 | Loss: 0.00001639
Iteration 208/1000 | Loss: 0.00001639
Iteration 209/1000 | Loss: 0.00001639
Iteration 210/1000 | Loss: 0.00001639
Iteration 211/1000 | Loss: 0.00001639
Iteration 212/1000 | Loss: 0.00001639
Iteration 213/1000 | Loss: 0.00001639
Iteration 214/1000 | Loss: 0.00001639
Iteration 215/1000 | Loss: 0.00001639
Iteration 216/1000 | Loss: 0.00001638
Iteration 217/1000 | Loss: 0.00001638
Iteration 218/1000 | Loss: 0.00001638
Iteration 219/1000 | Loss: 0.00001638
Iteration 220/1000 | Loss: 0.00001638
Iteration 221/1000 | Loss: 0.00001638
Iteration 222/1000 | Loss: 0.00001638
Iteration 223/1000 | Loss: 0.00001638
Iteration 224/1000 | Loss: 0.00001638
Iteration 225/1000 | Loss: 0.00001638
Iteration 226/1000 | Loss: 0.00001638
Iteration 227/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.6380950910388492e-05, 1.6380950910388492e-05, 1.6380950910388492e-05, 1.6380950910388492e-05, 1.6380950910388492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6380950910388492e-05

Optimization complete. Final v2v error: 3.3975348472595215 mm

Highest mean error: 4.476517677307129 mm for frame 93

Lowest mean error: 2.9324491024017334 mm for frame 0

Saving results

Total time: 44.10304141044617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790445
Iteration 2/25 | Loss: 0.00182528
Iteration 3/25 | Loss: 0.00128567
Iteration 4/25 | Loss: 0.00123109
Iteration 5/25 | Loss: 0.00120943
Iteration 6/25 | Loss: 0.00119836
Iteration 7/25 | Loss: 0.00117396
Iteration 8/25 | Loss: 0.00115907
Iteration 9/25 | Loss: 0.00115641
Iteration 10/25 | Loss: 0.00115580
Iteration 11/25 | Loss: 0.00115543
Iteration 12/25 | Loss: 0.00115514
Iteration 13/25 | Loss: 0.00115501
Iteration 14/25 | Loss: 0.00115499
Iteration 15/25 | Loss: 0.00115499
Iteration 16/25 | Loss: 0.00115499
Iteration 17/25 | Loss: 0.00115499
Iteration 18/25 | Loss: 0.00115498
Iteration 19/25 | Loss: 0.00115498
Iteration 20/25 | Loss: 0.00115498
Iteration 21/25 | Loss: 0.00115498
Iteration 22/25 | Loss: 0.00115498
Iteration 23/25 | Loss: 0.00115498
Iteration 24/25 | Loss: 0.00115498
Iteration 25/25 | Loss: 0.00115498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.78245640
Iteration 2/25 | Loss: 0.00079688
Iteration 3/25 | Loss: 0.00079667
Iteration 4/25 | Loss: 0.00079667
Iteration 5/25 | Loss: 0.00079667
Iteration 6/25 | Loss: 0.00079667
Iteration 7/25 | Loss: 0.00079667
Iteration 8/25 | Loss: 0.00079667
Iteration 9/25 | Loss: 0.00079667
Iteration 10/25 | Loss: 0.00079667
Iteration 11/25 | Loss: 0.00079667
Iteration 12/25 | Loss: 0.00079667
Iteration 13/25 | Loss: 0.00079667
Iteration 14/25 | Loss: 0.00079667
Iteration 15/25 | Loss: 0.00079667
Iteration 16/25 | Loss: 0.00079667
Iteration 17/25 | Loss: 0.00079667
Iteration 18/25 | Loss: 0.00079667
Iteration 19/25 | Loss: 0.00079667
Iteration 20/25 | Loss: 0.00079667
Iteration 21/25 | Loss: 0.00079667
Iteration 22/25 | Loss: 0.00079667
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007966701523400843, 0.0007966701523400843, 0.0007966701523400843, 0.0007966701523400843, 0.0007966701523400843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007966701523400843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079667
Iteration 2/1000 | Loss: 0.00003692
Iteration 3/1000 | Loss: 0.00002815
Iteration 4/1000 | Loss: 0.00002454
Iteration 5/1000 | Loss: 0.00002276
Iteration 6/1000 | Loss: 0.00002190
Iteration 7/1000 | Loss: 0.00002138
Iteration 8/1000 | Loss: 0.00008515
Iteration 9/1000 | Loss: 0.00002165
Iteration 10/1000 | Loss: 0.00002026
Iteration 11/1000 | Loss: 0.00001963
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001889
Iteration 14/1000 | Loss: 0.00001886
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001870
Iteration 18/1000 | Loss: 0.00001869
Iteration 19/1000 | Loss: 0.00001866
Iteration 20/1000 | Loss: 0.00001865
Iteration 21/1000 | Loss: 0.00001865
Iteration 22/1000 | Loss: 0.00001864
Iteration 23/1000 | Loss: 0.00001863
Iteration 24/1000 | Loss: 0.00001862
Iteration 25/1000 | Loss: 0.00001861
Iteration 26/1000 | Loss: 0.00001861
Iteration 27/1000 | Loss: 0.00001860
Iteration 28/1000 | Loss: 0.00001855
Iteration 29/1000 | Loss: 0.00001851
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001850
Iteration 32/1000 | Loss: 0.00001849
Iteration 33/1000 | Loss: 0.00001849
Iteration 34/1000 | Loss: 0.00001849
Iteration 35/1000 | Loss: 0.00001849
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001848
Iteration 38/1000 | Loss: 0.00001848
Iteration 39/1000 | Loss: 0.00001847
Iteration 40/1000 | Loss: 0.00001846
Iteration 41/1000 | Loss: 0.00001846
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001844
Iteration 45/1000 | Loss: 0.00001842
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001840
Iteration 48/1000 | Loss: 0.00001839
Iteration 49/1000 | Loss: 0.00001839
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001836
Iteration 52/1000 | Loss: 0.00001836
Iteration 53/1000 | Loss: 0.00001836
Iteration 54/1000 | Loss: 0.00001836
Iteration 55/1000 | Loss: 0.00001836
Iteration 56/1000 | Loss: 0.00001836
Iteration 57/1000 | Loss: 0.00001835
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001835
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001835
Iteration 62/1000 | Loss: 0.00001835
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001835
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001833
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001827
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001827
Iteration 100/1000 | Loss: 0.00001827
Iteration 101/1000 | Loss: 0.00001827
Iteration 102/1000 | Loss: 0.00001827
Iteration 103/1000 | Loss: 0.00001827
Iteration 104/1000 | Loss: 0.00001827
Iteration 105/1000 | Loss: 0.00001827
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001826
Iteration 115/1000 | Loss: 0.00001826
Iteration 116/1000 | Loss: 0.00001826
Iteration 117/1000 | Loss: 0.00001826
Iteration 118/1000 | Loss: 0.00001826
Iteration 119/1000 | Loss: 0.00001826
Iteration 120/1000 | Loss: 0.00001825
Iteration 121/1000 | Loss: 0.00001825
Iteration 122/1000 | Loss: 0.00001825
Iteration 123/1000 | Loss: 0.00001825
Iteration 124/1000 | Loss: 0.00001825
Iteration 125/1000 | Loss: 0.00001825
Iteration 126/1000 | Loss: 0.00001825
Iteration 127/1000 | Loss: 0.00001825
Iteration 128/1000 | Loss: 0.00001825
Iteration 129/1000 | Loss: 0.00001825
Iteration 130/1000 | Loss: 0.00001825
Iteration 131/1000 | Loss: 0.00001825
Iteration 132/1000 | Loss: 0.00001825
Iteration 133/1000 | Loss: 0.00001824
Iteration 134/1000 | Loss: 0.00001824
Iteration 135/1000 | Loss: 0.00001824
Iteration 136/1000 | Loss: 0.00001824
Iteration 137/1000 | Loss: 0.00001824
Iteration 138/1000 | Loss: 0.00001824
Iteration 139/1000 | Loss: 0.00001824
Iteration 140/1000 | Loss: 0.00001824
Iteration 141/1000 | Loss: 0.00001824
Iteration 142/1000 | Loss: 0.00001824
Iteration 143/1000 | Loss: 0.00001824
Iteration 144/1000 | Loss: 0.00001824
Iteration 145/1000 | Loss: 0.00001824
Iteration 146/1000 | Loss: 0.00001823
Iteration 147/1000 | Loss: 0.00001823
Iteration 148/1000 | Loss: 0.00001823
Iteration 149/1000 | Loss: 0.00001823
Iteration 150/1000 | Loss: 0.00001823
Iteration 151/1000 | Loss: 0.00001823
Iteration 152/1000 | Loss: 0.00001823
Iteration 153/1000 | Loss: 0.00001823
Iteration 154/1000 | Loss: 0.00001822
Iteration 155/1000 | Loss: 0.00001822
Iteration 156/1000 | Loss: 0.00001822
Iteration 157/1000 | Loss: 0.00001822
Iteration 158/1000 | Loss: 0.00001822
Iteration 159/1000 | Loss: 0.00001822
Iteration 160/1000 | Loss: 0.00001822
Iteration 161/1000 | Loss: 0.00001822
Iteration 162/1000 | Loss: 0.00001822
Iteration 163/1000 | Loss: 0.00001821
Iteration 164/1000 | Loss: 0.00001821
Iteration 165/1000 | Loss: 0.00001821
Iteration 166/1000 | Loss: 0.00001821
Iteration 167/1000 | Loss: 0.00001821
Iteration 168/1000 | Loss: 0.00001821
Iteration 169/1000 | Loss: 0.00001821
Iteration 170/1000 | Loss: 0.00001821
Iteration 171/1000 | Loss: 0.00001821
Iteration 172/1000 | Loss: 0.00001820
Iteration 173/1000 | Loss: 0.00001820
Iteration 174/1000 | Loss: 0.00001820
Iteration 175/1000 | Loss: 0.00001820
Iteration 176/1000 | Loss: 0.00001820
Iteration 177/1000 | Loss: 0.00001819
Iteration 178/1000 | Loss: 0.00001819
Iteration 179/1000 | Loss: 0.00001819
Iteration 180/1000 | Loss: 0.00001819
Iteration 181/1000 | Loss: 0.00001819
Iteration 182/1000 | Loss: 0.00001819
Iteration 183/1000 | Loss: 0.00001819
Iteration 184/1000 | Loss: 0.00001819
Iteration 185/1000 | Loss: 0.00001819
Iteration 186/1000 | Loss: 0.00001819
Iteration 187/1000 | Loss: 0.00001819
Iteration 188/1000 | Loss: 0.00001819
Iteration 189/1000 | Loss: 0.00001819
Iteration 190/1000 | Loss: 0.00001819
Iteration 191/1000 | Loss: 0.00001819
Iteration 192/1000 | Loss: 0.00001819
Iteration 193/1000 | Loss: 0.00001819
Iteration 194/1000 | Loss: 0.00001819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.818510463635903e-05, 1.818510463635903e-05, 1.818510463635903e-05, 1.818510463635903e-05, 1.818510463635903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.818510463635903e-05

Optimization complete. Final v2v error: 3.6320300102233887 mm

Highest mean error: 4.361362934112549 mm for frame 223

Lowest mean error: 3.3143527507781982 mm for frame 99

Saving results

Total time: 63.48531746864319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873929
Iteration 2/25 | Loss: 0.00120045
Iteration 3/25 | Loss: 0.00108669
Iteration 4/25 | Loss: 0.00107600
Iteration 5/25 | Loss: 0.00107315
Iteration 6/25 | Loss: 0.00107281
Iteration 7/25 | Loss: 0.00107281
Iteration 8/25 | Loss: 0.00107281
Iteration 9/25 | Loss: 0.00107281
Iteration 10/25 | Loss: 0.00107281
Iteration 11/25 | Loss: 0.00107281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010728056076914072, 0.0010728056076914072, 0.0010728056076914072, 0.0010728056076914072, 0.0010728056076914072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010728056076914072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24101329
Iteration 2/25 | Loss: 0.00105700
Iteration 3/25 | Loss: 0.00105697
Iteration 4/25 | Loss: 0.00105697
Iteration 5/25 | Loss: 0.00105697
Iteration 6/25 | Loss: 0.00105697
Iteration 7/25 | Loss: 0.00105697
Iteration 8/25 | Loss: 0.00105697
Iteration 9/25 | Loss: 0.00105697
Iteration 10/25 | Loss: 0.00105697
Iteration 11/25 | Loss: 0.00105697
Iteration 12/25 | Loss: 0.00105697
Iteration 13/25 | Loss: 0.00105697
Iteration 14/25 | Loss: 0.00105697
Iteration 15/25 | Loss: 0.00105697
Iteration 16/25 | Loss: 0.00105697
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001056971144862473, 0.001056971144862473, 0.001056971144862473, 0.001056971144862473, 0.001056971144862473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001056971144862473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105697
Iteration 2/1000 | Loss: 0.00002947
Iteration 3/1000 | Loss: 0.00002010
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001298
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001275
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001249
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001238
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001233
Iteration 24/1000 | Loss: 0.00001232
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001229
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001227
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001212
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001211
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001203
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001201
Iteration 107/1000 | Loss: 0.00001201
Iteration 108/1000 | Loss: 0.00001201
Iteration 109/1000 | Loss: 0.00001201
Iteration 110/1000 | Loss: 0.00001201
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001196
Iteration 127/1000 | Loss: 0.00001196
Iteration 128/1000 | Loss: 0.00001196
Iteration 129/1000 | Loss: 0.00001196
Iteration 130/1000 | Loss: 0.00001196
Iteration 131/1000 | Loss: 0.00001196
Iteration 132/1000 | Loss: 0.00001196
Iteration 133/1000 | Loss: 0.00001196
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001195
Iteration 136/1000 | Loss: 0.00001195
Iteration 137/1000 | Loss: 0.00001195
Iteration 138/1000 | Loss: 0.00001195
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001195
Iteration 142/1000 | Loss: 0.00001195
Iteration 143/1000 | Loss: 0.00001195
Iteration 144/1000 | Loss: 0.00001195
Iteration 145/1000 | Loss: 0.00001195
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001195
Iteration 148/1000 | Loss: 0.00001195
Iteration 149/1000 | Loss: 0.00001195
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001194
Iteration 152/1000 | Loss: 0.00001194
Iteration 153/1000 | Loss: 0.00001194
Iteration 154/1000 | Loss: 0.00001194
Iteration 155/1000 | Loss: 0.00001194
Iteration 156/1000 | Loss: 0.00001194
Iteration 157/1000 | Loss: 0.00001194
Iteration 158/1000 | Loss: 0.00001194
Iteration 159/1000 | Loss: 0.00001194
Iteration 160/1000 | Loss: 0.00001194
Iteration 161/1000 | Loss: 0.00001194
Iteration 162/1000 | Loss: 0.00001194
Iteration 163/1000 | Loss: 0.00001194
Iteration 164/1000 | Loss: 0.00001193
Iteration 165/1000 | Loss: 0.00001193
Iteration 166/1000 | Loss: 0.00001193
Iteration 167/1000 | Loss: 0.00001193
Iteration 168/1000 | Loss: 0.00001193
Iteration 169/1000 | Loss: 0.00001193
Iteration 170/1000 | Loss: 0.00001193
Iteration 171/1000 | Loss: 0.00001193
Iteration 172/1000 | Loss: 0.00001193
Iteration 173/1000 | Loss: 0.00001193
Iteration 174/1000 | Loss: 0.00001193
Iteration 175/1000 | Loss: 0.00001193
Iteration 176/1000 | Loss: 0.00001193
Iteration 177/1000 | Loss: 0.00001193
Iteration 178/1000 | Loss: 0.00001193
Iteration 179/1000 | Loss: 0.00001193
Iteration 180/1000 | Loss: 0.00001193
Iteration 181/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.1931138942600228e-05, 1.1931138942600228e-05, 1.1931138942600228e-05, 1.1931138942600228e-05, 1.1931138942600228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1931138942600228e-05

Optimization complete. Final v2v error: 3.0039453506469727 mm

Highest mean error: 3.3023507595062256 mm for frame 129

Lowest mean error: 2.764200210571289 mm for frame 10

Saving results

Total time: 36.251397371292114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014723
Iteration 2/25 | Loss: 0.00154318
Iteration 3/25 | Loss: 0.00118064
Iteration 4/25 | Loss: 0.00113091
Iteration 5/25 | Loss: 0.00110766
Iteration 6/25 | Loss: 0.00111035
Iteration 7/25 | Loss: 0.00109989
Iteration 8/25 | Loss: 0.00108365
Iteration 9/25 | Loss: 0.00107199
Iteration 10/25 | Loss: 0.00106343
Iteration 11/25 | Loss: 0.00106621
Iteration 12/25 | Loss: 0.00105866
Iteration 13/25 | Loss: 0.00106068
Iteration 14/25 | Loss: 0.00105735
Iteration 15/25 | Loss: 0.00105724
Iteration 16/25 | Loss: 0.00105722
Iteration 17/25 | Loss: 0.00105721
Iteration 18/25 | Loss: 0.00105721
Iteration 19/25 | Loss: 0.00105721
Iteration 20/25 | Loss: 0.00105721
Iteration 21/25 | Loss: 0.00105721
Iteration 22/25 | Loss: 0.00105721
Iteration 23/25 | Loss: 0.00105721
Iteration 24/25 | Loss: 0.00105721
Iteration 25/25 | Loss: 0.00105721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73655736
Iteration 2/25 | Loss: 0.00110872
Iteration 3/25 | Loss: 0.00107313
Iteration 4/25 | Loss: 0.00107313
Iteration 5/25 | Loss: 0.00107313
Iteration 6/25 | Loss: 0.00107313
Iteration 7/25 | Loss: 0.00107313
Iteration 8/25 | Loss: 0.00107313
Iteration 9/25 | Loss: 0.00107313
Iteration 10/25 | Loss: 0.00107313
Iteration 11/25 | Loss: 0.00107313
Iteration 12/25 | Loss: 0.00107313
Iteration 13/25 | Loss: 0.00107313
Iteration 14/25 | Loss: 0.00107313
Iteration 15/25 | Loss: 0.00107313
Iteration 16/25 | Loss: 0.00107313
Iteration 17/25 | Loss: 0.00107313
Iteration 18/25 | Loss: 0.00107313
Iteration 19/25 | Loss: 0.00107313
Iteration 20/25 | Loss: 0.00107313
Iteration 21/25 | Loss: 0.00107313
Iteration 22/25 | Loss: 0.00107313
Iteration 23/25 | Loss: 0.00107313
Iteration 24/25 | Loss: 0.00107313
Iteration 25/25 | Loss: 0.00107313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107313
Iteration 2/1000 | Loss: 0.00009668
Iteration 3/1000 | Loss: 0.00004295
Iteration 4/1000 | Loss: 0.00002145
Iteration 5/1000 | Loss: 0.00004421
Iteration 6/1000 | Loss: 0.00008672
Iteration 7/1000 | Loss: 0.00001860
Iteration 8/1000 | Loss: 0.00002004
Iteration 9/1000 | Loss: 0.00002033
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00025719
Iteration 12/1000 | Loss: 0.00001956
Iteration 13/1000 | Loss: 0.00005372
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00003010
Iteration 16/1000 | Loss: 0.00001594
Iteration 17/1000 | Loss: 0.00001549
Iteration 18/1000 | Loss: 0.00005311
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001516
Iteration 22/1000 | Loss: 0.00001516
Iteration 23/1000 | Loss: 0.00001515
Iteration 24/1000 | Loss: 0.00001515
Iteration 25/1000 | Loss: 0.00001515
Iteration 26/1000 | Loss: 0.00001515
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001835
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00003574
Iteration 31/1000 | Loss: 0.00001509
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001503
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001502
Iteration 38/1000 | Loss: 0.00001502
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001556
Iteration 49/1000 | Loss: 0.00001510
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001495
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001495
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001498
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001492
Iteration 64/1000 | Loss: 0.00001492
Iteration 65/1000 | Loss: 0.00001491
Iteration 66/1000 | Loss: 0.00001491
Iteration 67/1000 | Loss: 0.00001490
Iteration 68/1000 | Loss: 0.00001490
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001489
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001488
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001487
Iteration 81/1000 | Loss: 0.00001487
Iteration 82/1000 | Loss: 0.00001487
Iteration 83/1000 | Loss: 0.00001487
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001486
Iteration 86/1000 | Loss: 0.00001486
Iteration 87/1000 | Loss: 0.00001486
Iteration 88/1000 | Loss: 0.00001605
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00001486
Iteration 92/1000 | Loss: 0.00001486
Iteration 93/1000 | Loss: 0.00001485
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001485
Iteration 99/1000 | Loss: 0.00001485
Iteration 100/1000 | Loss: 0.00001485
Iteration 101/1000 | Loss: 0.00001485
Iteration 102/1000 | Loss: 0.00001484
Iteration 103/1000 | Loss: 0.00001484
Iteration 104/1000 | Loss: 0.00001484
Iteration 105/1000 | Loss: 0.00001484
Iteration 106/1000 | Loss: 0.00001484
Iteration 107/1000 | Loss: 0.00001484
Iteration 108/1000 | Loss: 0.00001484
Iteration 109/1000 | Loss: 0.00001484
Iteration 110/1000 | Loss: 0.00001483
Iteration 111/1000 | Loss: 0.00001483
Iteration 112/1000 | Loss: 0.00001483
Iteration 113/1000 | Loss: 0.00001483
Iteration 114/1000 | Loss: 0.00001483
Iteration 115/1000 | Loss: 0.00001483
Iteration 116/1000 | Loss: 0.00001483
Iteration 117/1000 | Loss: 0.00001483
Iteration 118/1000 | Loss: 0.00001482
Iteration 119/1000 | Loss: 0.00001482
Iteration 120/1000 | Loss: 0.00001482
Iteration 121/1000 | Loss: 0.00001482
Iteration 122/1000 | Loss: 0.00001482
Iteration 123/1000 | Loss: 0.00001482
Iteration 124/1000 | Loss: 0.00001482
Iteration 125/1000 | Loss: 0.00001482
Iteration 126/1000 | Loss: 0.00001482
Iteration 127/1000 | Loss: 0.00001481
Iteration 128/1000 | Loss: 0.00001481
Iteration 129/1000 | Loss: 0.00001481
Iteration 130/1000 | Loss: 0.00001481
Iteration 131/1000 | Loss: 0.00001481
Iteration 132/1000 | Loss: 0.00001480
Iteration 133/1000 | Loss: 0.00001480
Iteration 134/1000 | Loss: 0.00001480
Iteration 135/1000 | Loss: 0.00001480
Iteration 136/1000 | Loss: 0.00001480
Iteration 137/1000 | Loss: 0.00001480
Iteration 138/1000 | Loss: 0.00001479
Iteration 139/1000 | Loss: 0.00001479
Iteration 140/1000 | Loss: 0.00001479
Iteration 141/1000 | Loss: 0.00001479
Iteration 142/1000 | Loss: 0.00001479
Iteration 143/1000 | Loss: 0.00001479
Iteration 144/1000 | Loss: 0.00001479
Iteration 145/1000 | Loss: 0.00001478
Iteration 146/1000 | Loss: 0.00001478
Iteration 147/1000 | Loss: 0.00001478
Iteration 148/1000 | Loss: 0.00001478
Iteration 149/1000 | Loss: 0.00001478
Iteration 150/1000 | Loss: 0.00001478
Iteration 151/1000 | Loss: 0.00001478
Iteration 152/1000 | Loss: 0.00001478
Iteration 153/1000 | Loss: 0.00001478
Iteration 154/1000 | Loss: 0.00001477
Iteration 155/1000 | Loss: 0.00001477
Iteration 156/1000 | Loss: 0.00001477
Iteration 157/1000 | Loss: 0.00001477
Iteration 158/1000 | Loss: 0.00001477
Iteration 159/1000 | Loss: 0.00001476
Iteration 160/1000 | Loss: 0.00001476
Iteration 161/1000 | Loss: 0.00001476
Iteration 162/1000 | Loss: 0.00001476
Iteration 163/1000 | Loss: 0.00001475
Iteration 164/1000 | Loss: 0.00001475
Iteration 165/1000 | Loss: 0.00001475
Iteration 166/1000 | Loss: 0.00001475
Iteration 167/1000 | Loss: 0.00001475
Iteration 168/1000 | Loss: 0.00001475
Iteration 169/1000 | Loss: 0.00001475
Iteration 170/1000 | Loss: 0.00001475
Iteration 171/1000 | Loss: 0.00001475
Iteration 172/1000 | Loss: 0.00001474
Iteration 173/1000 | Loss: 0.00001474
Iteration 174/1000 | Loss: 0.00001474
Iteration 175/1000 | Loss: 0.00001474
Iteration 176/1000 | Loss: 0.00001474
Iteration 177/1000 | Loss: 0.00001474
Iteration 178/1000 | Loss: 0.00001474
Iteration 179/1000 | Loss: 0.00001474
Iteration 180/1000 | Loss: 0.00001474
Iteration 181/1000 | Loss: 0.00001474
Iteration 182/1000 | Loss: 0.00001474
Iteration 183/1000 | Loss: 0.00001474
Iteration 184/1000 | Loss: 0.00001474
Iteration 185/1000 | Loss: 0.00001474
Iteration 186/1000 | Loss: 0.00001474
Iteration 187/1000 | Loss: 0.00001474
Iteration 188/1000 | Loss: 0.00001474
Iteration 189/1000 | Loss: 0.00001474
Iteration 190/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.4741188351763412e-05, 1.4741188351763412e-05, 1.4741188351763412e-05, 1.4741188351763412e-05, 1.4741188351763412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4741188351763412e-05

Optimization complete. Final v2v error: 3.2496025562286377 mm

Highest mean error: 3.7922539710998535 mm for frame 141

Lowest mean error: 2.64172101020813 mm for frame 219

Saving results

Total time: 83.38561725616455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065504
Iteration 2/25 | Loss: 0.00209255
Iteration 3/25 | Loss: 0.00131544
Iteration 4/25 | Loss: 0.00122581
Iteration 5/25 | Loss: 0.00119383
Iteration 6/25 | Loss: 0.00120644
Iteration 7/25 | Loss: 0.00120034
Iteration 8/25 | Loss: 0.00118886
Iteration 9/25 | Loss: 0.00118754
Iteration 10/25 | Loss: 0.00118639
Iteration 11/25 | Loss: 0.00118562
Iteration 12/25 | Loss: 0.00118524
Iteration 13/25 | Loss: 0.00120996
Iteration 14/25 | Loss: 0.00117994
Iteration 15/25 | Loss: 0.00117889
Iteration 16/25 | Loss: 0.00117880
Iteration 17/25 | Loss: 0.00117880
Iteration 18/25 | Loss: 0.00117880
Iteration 19/25 | Loss: 0.00117880
Iteration 20/25 | Loss: 0.00117879
Iteration 21/25 | Loss: 0.00117879
Iteration 22/25 | Loss: 0.00117878
Iteration 23/25 | Loss: 0.00117878
Iteration 24/25 | Loss: 0.00117878
Iteration 25/25 | Loss: 0.00117878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34378600
Iteration 2/25 | Loss: 0.00187005
Iteration 3/25 | Loss: 0.00187005
Iteration 4/25 | Loss: 0.00187005
Iteration 5/25 | Loss: 0.00187005
Iteration 6/25 | Loss: 0.00187005
Iteration 7/25 | Loss: 0.00187005
Iteration 8/25 | Loss: 0.00187005
Iteration 9/25 | Loss: 0.00187005
Iteration 10/25 | Loss: 0.00187005
Iteration 11/25 | Loss: 0.00187005
Iteration 12/25 | Loss: 0.00187005
Iteration 13/25 | Loss: 0.00187005
Iteration 14/25 | Loss: 0.00187005
Iteration 15/25 | Loss: 0.00187005
Iteration 16/25 | Loss: 0.00187005
Iteration 17/25 | Loss: 0.00187005
Iteration 18/25 | Loss: 0.00187005
Iteration 19/25 | Loss: 0.00187005
Iteration 20/25 | Loss: 0.00187005
Iteration 21/25 | Loss: 0.00187005
Iteration 22/25 | Loss: 0.00187005
Iteration 23/25 | Loss: 0.00187005
Iteration 24/25 | Loss: 0.00187005
Iteration 25/25 | Loss: 0.00187005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187005
Iteration 2/1000 | Loss: 0.00005902
Iteration 3/1000 | Loss: 0.00003330
Iteration 4/1000 | Loss: 0.00002678
Iteration 5/1000 | Loss: 0.00002464
Iteration 6/1000 | Loss: 0.00002332
Iteration 7/1000 | Loss: 0.00002238
Iteration 8/1000 | Loss: 0.00002172
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002088
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002062
Iteration 13/1000 | Loss: 0.00002047
Iteration 14/1000 | Loss: 0.00002046
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00002032
Iteration 17/1000 | Loss: 0.00002030
Iteration 18/1000 | Loss: 0.00002029
Iteration 19/1000 | Loss: 0.00002028
Iteration 20/1000 | Loss: 0.00002028
Iteration 21/1000 | Loss: 0.00002027
Iteration 22/1000 | Loss: 0.00002026
Iteration 23/1000 | Loss: 0.00002025
Iteration 24/1000 | Loss: 0.00002025
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002022
Iteration 27/1000 | Loss: 0.00002021
Iteration 28/1000 | Loss: 0.00002018
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002015
Iteration 31/1000 | Loss: 0.00002014
Iteration 32/1000 | Loss: 0.00002014
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002013
Iteration 35/1000 | Loss: 0.00002012
Iteration 36/1000 | Loss: 0.00002012
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00002010
Iteration 39/1000 | Loss: 0.00002010
Iteration 40/1000 | Loss: 0.00002009
Iteration 41/1000 | Loss: 0.00002008
Iteration 42/1000 | Loss: 0.00002008
Iteration 43/1000 | Loss: 0.00002008
Iteration 44/1000 | Loss: 0.00002008
Iteration 45/1000 | Loss: 0.00002007
Iteration 46/1000 | Loss: 0.00002007
Iteration 47/1000 | Loss: 0.00002006
Iteration 48/1000 | Loss: 0.00002006
Iteration 49/1000 | Loss: 0.00002003
Iteration 50/1000 | Loss: 0.00002003
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00002000
Iteration 53/1000 | Loss: 0.00002000
Iteration 54/1000 | Loss: 0.00002000
Iteration 55/1000 | Loss: 0.00001999
Iteration 56/1000 | Loss: 0.00001999
Iteration 57/1000 | Loss: 0.00001999
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001999
Iteration 60/1000 | Loss: 0.00001998
Iteration 61/1000 | Loss: 0.00001998
Iteration 62/1000 | Loss: 0.00001998
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001998
Iteration 66/1000 | Loss: 0.00001997
Iteration 67/1000 | Loss: 0.00001997
Iteration 68/1000 | Loss: 0.00001997
Iteration 69/1000 | Loss: 0.00001997
Iteration 70/1000 | Loss: 0.00001997
Iteration 71/1000 | Loss: 0.00001996
Iteration 72/1000 | Loss: 0.00001996
Iteration 73/1000 | Loss: 0.00001995
Iteration 74/1000 | Loss: 0.00001995
Iteration 75/1000 | Loss: 0.00001994
Iteration 76/1000 | Loss: 0.00001994
Iteration 77/1000 | Loss: 0.00001994
Iteration 78/1000 | Loss: 0.00001994
Iteration 79/1000 | Loss: 0.00001993
Iteration 80/1000 | Loss: 0.00001993
Iteration 81/1000 | Loss: 0.00001992
Iteration 82/1000 | Loss: 0.00001991
Iteration 83/1000 | Loss: 0.00001991
Iteration 84/1000 | Loss: 0.00001991
Iteration 85/1000 | Loss: 0.00001991
Iteration 86/1000 | Loss: 0.00001990
Iteration 87/1000 | Loss: 0.00001990
Iteration 88/1000 | Loss: 0.00001990
Iteration 89/1000 | Loss: 0.00001989
Iteration 90/1000 | Loss: 0.00001989
Iteration 91/1000 | Loss: 0.00001989
Iteration 92/1000 | Loss: 0.00001989
Iteration 93/1000 | Loss: 0.00001989
Iteration 94/1000 | Loss: 0.00001989
Iteration 95/1000 | Loss: 0.00001989
Iteration 96/1000 | Loss: 0.00001988
Iteration 97/1000 | Loss: 0.00001988
Iteration 98/1000 | Loss: 0.00001988
Iteration 99/1000 | Loss: 0.00001988
Iteration 100/1000 | Loss: 0.00001988
Iteration 101/1000 | Loss: 0.00001988
Iteration 102/1000 | Loss: 0.00001988
Iteration 103/1000 | Loss: 0.00001988
Iteration 104/1000 | Loss: 0.00001987
Iteration 105/1000 | Loss: 0.00001987
Iteration 106/1000 | Loss: 0.00001987
Iteration 107/1000 | Loss: 0.00001987
Iteration 108/1000 | Loss: 0.00001987
Iteration 109/1000 | Loss: 0.00001987
Iteration 110/1000 | Loss: 0.00001987
Iteration 111/1000 | Loss: 0.00001987
Iteration 112/1000 | Loss: 0.00001987
Iteration 113/1000 | Loss: 0.00001987
Iteration 114/1000 | Loss: 0.00001987
Iteration 115/1000 | Loss: 0.00001987
Iteration 116/1000 | Loss: 0.00001987
Iteration 117/1000 | Loss: 0.00001986
Iteration 118/1000 | Loss: 0.00001986
Iteration 119/1000 | Loss: 0.00001986
Iteration 120/1000 | Loss: 0.00001986
Iteration 121/1000 | Loss: 0.00001986
Iteration 122/1000 | Loss: 0.00001986
Iteration 123/1000 | Loss: 0.00001986
Iteration 124/1000 | Loss: 0.00001986
Iteration 125/1000 | Loss: 0.00001985
Iteration 126/1000 | Loss: 0.00001985
Iteration 127/1000 | Loss: 0.00001985
Iteration 128/1000 | Loss: 0.00001985
Iteration 129/1000 | Loss: 0.00001985
Iteration 130/1000 | Loss: 0.00001985
Iteration 131/1000 | Loss: 0.00001985
Iteration 132/1000 | Loss: 0.00001985
Iteration 133/1000 | Loss: 0.00001985
Iteration 134/1000 | Loss: 0.00001984
Iteration 135/1000 | Loss: 0.00001984
Iteration 136/1000 | Loss: 0.00001984
Iteration 137/1000 | Loss: 0.00001984
Iteration 138/1000 | Loss: 0.00001984
Iteration 139/1000 | Loss: 0.00001984
Iteration 140/1000 | Loss: 0.00001984
Iteration 141/1000 | Loss: 0.00001984
Iteration 142/1000 | Loss: 0.00001984
Iteration 143/1000 | Loss: 0.00001983
Iteration 144/1000 | Loss: 0.00001983
Iteration 145/1000 | Loss: 0.00001983
Iteration 146/1000 | Loss: 0.00001983
Iteration 147/1000 | Loss: 0.00001983
Iteration 148/1000 | Loss: 0.00001983
Iteration 149/1000 | Loss: 0.00001982
Iteration 150/1000 | Loss: 0.00001982
Iteration 151/1000 | Loss: 0.00001982
Iteration 152/1000 | Loss: 0.00001982
Iteration 153/1000 | Loss: 0.00001982
Iteration 154/1000 | Loss: 0.00001982
Iteration 155/1000 | Loss: 0.00001981
Iteration 156/1000 | Loss: 0.00001981
Iteration 157/1000 | Loss: 0.00001981
Iteration 158/1000 | Loss: 0.00001981
Iteration 159/1000 | Loss: 0.00001981
Iteration 160/1000 | Loss: 0.00001981
Iteration 161/1000 | Loss: 0.00001981
Iteration 162/1000 | Loss: 0.00001981
Iteration 163/1000 | Loss: 0.00001981
Iteration 164/1000 | Loss: 0.00001980
Iteration 165/1000 | Loss: 0.00001980
Iteration 166/1000 | Loss: 0.00001980
Iteration 167/1000 | Loss: 0.00001980
Iteration 168/1000 | Loss: 0.00001980
Iteration 169/1000 | Loss: 0.00001980
Iteration 170/1000 | Loss: 0.00001980
Iteration 171/1000 | Loss: 0.00001980
Iteration 172/1000 | Loss: 0.00001980
Iteration 173/1000 | Loss: 0.00001980
Iteration 174/1000 | Loss: 0.00001980
Iteration 175/1000 | Loss: 0.00001980
Iteration 176/1000 | Loss: 0.00001980
Iteration 177/1000 | Loss: 0.00001979
Iteration 178/1000 | Loss: 0.00001979
Iteration 179/1000 | Loss: 0.00001979
Iteration 180/1000 | Loss: 0.00001979
Iteration 181/1000 | Loss: 0.00001979
Iteration 182/1000 | Loss: 0.00001979
Iteration 183/1000 | Loss: 0.00001978
Iteration 184/1000 | Loss: 0.00001978
Iteration 185/1000 | Loss: 0.00001978
Iteration 186/1000 | Loss: 0.00001978
Iteration 187/1000 | Loss: 0.00001978
Iteration 188/1000 | Loss: 0.00001978
Iteration 189/1000 | Loss: 0.00001978
Iteration 190/1000 | Loss: 0.00001978
Iteration 191/1000 | Loss: 0.00001978
Iteration 192/1000 | Loss: 0.00001977
Iteration 193/1000 | Loss: 0.00001977
Iteration 194/1000 | Loss: 0.00001977
Iteration 195/1000 | Loss: 0.00001977
Iteration 196/1000 | Loss: 0.00001977
Iteration 197/1000 | Loss: 0.00001977
Iteration 198/1000 | Loss: 0.00001977
Iteration 199/1000 | Loss: 0.00001977
Iteration 200/1000 | Loss: 0.00001977
Iteration 201/1000 | Loss: 0.00001977
Iteration 202/1000 | Loss: 0.00001977
Iteration 203/1000 | Loss: 0.00001977
Iteration 204/1000 | Loss: 0.00001977
Iteration 205/1000 | Loss: 0.00001977
Iteration 206/1000 | Loss: 0.00001977
Iteration 207/1000 | Loss: 0.00001977
Iteration 208/1000 | Loss: 0.00001977
Iteration 209/1000 | Loss: 0.00001977
Iteration 210/1000 | Loss: 0.00001977
Iteration 211/1000 | Loss: 0.00001977
Iteration 212/1000 | Loss: 0.00001977
Iteration 213/1000 | Loss: 0.00001977
Iteration 214/1000 | Loss: 0.00001977
Iteration 215/1000 | Loss: 0.00001977
Iteration 216/1000 | Loss: 0.00001977
Iteration 217/1000 | Loss: 0.00001977
Iteration 218/1000 | Loss: 0.00001977
Iteration 219/1000 | Loss: 0.00001977
Iteration 220/1000 | Loss: 0.00001977
Iteration 221/1000 | Loss: 0.00001977
Iteration 222/1000 | Loss: 0.00001976
Iteration 223/1000 | Loss: 0.00001976
Iteration 224/1000 | Loss: 0.00001976
Iteration 225/1000 | Loss: 0.00001976
Iteration 226/1000 | Loss: 0.00001976
Iteration 227/1000 | Loss: 0.00001976
Iteration 228/1000 | Loss: 0.00001976
Iteration 229/1000 | Loss: 0.00001976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.9764867829508148e-05, 1.9764867829508148e-05, 1.9764867829508148e-05, 1.9764867829508148e-05, 1.9764867829508148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9764867829508148e-05

Optimization complete. Final v2v error: 3.7620675563812256 mm

Highest mean error: 3.988632917404175 mm for frame 239

Lowest mean error: 3.4331648349761963 mm for frame 111

Saving results

Total time: 71.90025162696838
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817457
Iteration 2/25 | Loss: 0.00180618
Iteration 3/25 | Loss: 0.00121808
Iteration 4/25 | Loss: 0.00114913
Iteration 5/25 | Loss: 0.00114266
Iteration 6/25 | Loss: 0.00114104
Iteration 7/25 | Loss: 0.00114089
Iteration 8/25 | Loss: 0.00114089
Iteration 9/25 | Loss: 0.00114089
Iteration 10/25 | Loss: 0.00114089
Iteration 11/25 | Loss: 0.00114089
Iteration 12/25 | Loss: 0.00114089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011408934369683266, 0.0011408934369683266, 0.0011408934369683266, 0.0011408934369683266, 0.0011408934369683266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011408934369683266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24536133
Iteration 2/25 | Loss: 0.00114058
Iteration 3/25 | Loss: 0.00114057
Iteration 4/25 | Loss: 0.00114057
Iteration 5/25 | Loss: 0.00114056
Iteration 6/25 | Loss: 0.00114056
Iteration 7/25 | Loss: 0.00114056
Iteration 8/25 | Loss: 0.00114056
Iteration 9/25 | Loss: 0.00114056
Iteration 10/25 | Loss: 0.00114056
Iteration 11/25 | Loss: 0.00114056
Iteration 12/25 | Loss: 0.00114056
Iteration 13/25 | Loss: 0.00114056
Iteration 14/25 | Loss: 0.00114056
Iteration 15/25 | Loss: 0.00114056
Iteration 16/25 | Loss: 0.00114056
Iteration 17/25 | Loss: 0.00114056
Iteration 18/25 | Loss: 0.00114056
Iteration 19/25 | Loss: 0.00114056
Iteration 20/25 | Loss: 0.00114056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011405625846236944, 0.0011405625846236944, 0.0011405625846236944, 0.0011405625846236944, 0.0011405625846236944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011405625846236944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114056
Iteration 2/1000 | Loss: 0.00003891
Iteration 3/1000 | Loss: 0.00002444
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001949
Iteration 6/1000 | Loss: 0.00001897
Iteration 7/1000 | Loss: 0.00001844
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001779
Iteration 12/1000 | Loss: 0.00001776
Iteration 13/1000 | Loss: 0.00001775
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001773
Iteration 16/1000 | Loss: 0.00001769
Iteration 17/1000 | Loss: 0.00001768
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001754
Iteration 20/1000 | Loss: 0.00001753
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001749
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001747
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001742
Iteration 30/1000 | Loss: 0.00001742
Iteration 31/1000 | Loss: 0.00001741
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001735
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001721
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001720
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001720
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001718
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001716
Iteration 68/1000 | Loss: 0.00001716
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001713
Iteration 76/1000 | Loss: 0.00001713
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001713
Iteration 82/1000 | Loss: 0.00001713
Iteration 83/1000 | Loss: 0.00001712
Iteration 84/1000 | Loss: 0.00001712
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001711
Iteration 87/1000 | Loss: 0.00001711
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001708
Iteration 100/1000 | Loss: 0.00001708
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001707
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001705
Iteration 110/1000 | Loss: 0.00001705
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001704
Iteration 121/1000 | Loss: 0.00001704
Iteration 122/1000 | Loss: 0.00001704
Iteration 123/1000 | Loss: 0.00001704
Iteration 124/1000 | Loss: 0.00001704
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001703
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001703
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001703
Iteration 135/1000 | Loss: 0.00001703
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001703
Iteration 140/1000 | Loss: 0.00001702
Iteration 141/1000 | Loss: 0.00001702
Iteration 142/1000 | Loss: 0.00001702
Iteration 143/1000 | Loss: 0.00001702
Iteration 144/1000 | Loss: 0.00001702
Iteration 145/1000 | Loss: 0.00001702
Iteration 146/1000 | Loss: 0.00001702
Iteration 147/1000 | Loss: 0.00001702
Iteration 148/1000 | Loss: 0.00001702
Iteration 149/1000 | Loss: 0.00001702
Iteration 150/1000 | Loss: 0.00001701
Iteration 151/1000 | Loss: 0.00001701
Iteration 152/1000 | Loss: 0.00001701
Iteration 153/1000 | Loss: 0.00001701
Iteration 154/1000 | Loss: 0.00001701
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001700
Iteration 160/1000 | Loss: 0.00001700
Iteration 161/1000 | Loss: 0.00001700
Iteration 162/1000 | Loss: 0.00001700
Iteration 163/1000 | Loss: 0.00001700
Iteration 164/1000 | Loss: 0.00001700
Iteration 165/1000 | Loss: 0.00001700
Iteration 166/1000 | Loss: 0.00001700
Iteration 167/1000 | Loss: 0.00001700
Iteration 168/1000 | Loss: 0.00001700
Iteration 169/1000 | Loss: 0.00001700
Iteration 170/1000 | Loss: 0.00001700
Iteration 171/1000 | Loss: 0.00001700
Iteration 172/1000 | Loss: 0.00001700
Iteration 173/1000 | Loss: 0.00001700
Iteration 174/1000 | Loss: 0.00001700
Iteration 175/1000 | Loss: 0.00001700
Iteration 176/1000 | Loss: 0.00001700
Iteration 177/1000 | Loss: 0.00001700
Iteration 178/1000 | Loss: 0.00001700
Iteration 179/1000 | Loss: 0.00001700
Iteration 180/1000 | Loss: 0.00001700
Iteration 181/1000 | Loss: 0.00001700
Iteration 182/1000 | Loss: 0.00001700
Iteration 183/1000 | Loss: 0.00001700
Iteration 184/1000 | Loss: 0.00001700
Iteration 185/1000 | Loss: 0.00001700
Iteration 186/1000 | Loss: 0.00001700
Iteration 187/1000 | Loss: 0.00001700
Iteration 188/1000 | Loss: 0.00001700
Iteration 189/1000 | Loss: 0.00001700
Iteration 190/1000 | Loss: 0.00001700
Iteration 191/1000 | Loss: 0.00001700
Iteration 192/1000 | Loss: 0.00001700
Iteration 193/1000 | Loss: 0.00001700
Iteration 194/1000 | Loss: 0.00001700
Iteration 195/1000 | Loss: 0.00001700
Iteration 196/1000 | Loss: 0.00001700
Iteration 197/1000 | Loss: 0.00001700
Iteration 198/1000 | Loss: 0.00001700
Iteration 199/1000 | Loss: 0.00001700
Iteration 200/1000 | Loss: 0.00001700
Iteration 201/1000 | Loss: 0.00001700
Iteration 202/1000 | Loss: 0.00001700
Iteration 203/1000 | Loss: 0.00001700
Iteration 204/1000 | Loss: 0.00001700
Iteration 205/1000 | Loss: 0.00001700
Iteration 206/1000 | Loss: 0.00001700
Iteration 207/1000 | Loss: 0.00001700
Iteration 208/1000 | Loss: 0.00001700
Iteration 209/1000 | Loss: 0.00001700
Iteration 210/1000 | Loss: 0.00001700
Iteration 211/1000 | Loss: 0.00001700
Iteration 212/1000 | Loss: 0.00001700
Iteration 213/1000 | Loss: 0.00001700
Iteration 214/1000 | Loss: 0.00001700
Iteration 215/1000 | Loss: 0.00001700
Iteration 216/1000 | Loss: 0.00001700
Iteration 217/1000 | Loss: 0.00001700
Iteration 218/1000 | Loss: 0.00001700
Iteration 219/1000 | Loss: 0.00001700
Iteration 220/1000 | Loss: 0.00001700
Iteration 221/1000 | Loss: 0.00001700
Iteration 222/1000 | Loss: 0.00001700
Iteration 223/1000 | Loss: 0.00001700
Iteration 224/1000 | Loss: 0.00001700
Iteration 225/1000 | Loss: 0.00001700
Iteration 226/1000 | Loss: 0.00001700
Iteration 227/1000 | Loss: 0.00001700
Iteration 228/1000 | Loss: 0.00001700
Iteration 229/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.6998228602460586e-05, 1.6998228602460586e-05, 1.6998228602460586e-05, 1.6998228602460586e-05, 1.6998228602460586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6998228602460586e-05

Optimization complete. Final v2v error: 3.5325498580932617 mm

Highest mean error: 3.7483692169189453 mm for frame 133

Lowest mean error: 3.274071455001831 mm for frame 5

Saving results

Total time: 40.135061502456665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396614
Iteration 2/25 | Loss: 0.00116011
Iteration 3/25 | Loss: 0.00108058
Iteration 4/25 | Loss: 0.00106944
Iteration 5/25 | Loss: 0.00106642
Iteration 6/25 | Loss: 0.00106568
Iteration 7/25 | Loss: 0.00106568
Iteration 8/25 | Loss: 0.00106568
Iteration 9/25 | Loss: 0.00106568
Iteration 10/25 | Loss: 0.00106568
Iteration 11/25 | Loss: 0.00106568
Iteration 12/25 | Loss: 0.00106568
Iteration 13/25 | Loss: 0.00106568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010656756348907948, 0.0010656756348907948, 0.0010656756348907948, 0.0010656756348907948, 0.0010656756348907948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010656756348907948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24704444
Iteration 2/25 | Loss: 0.00138867
Iteration 3/25 | Loss: 0.00138867
Iteration 4/25 | Loss: 0.00138867
Iteration 5/25 | Loss: 0.00138867
Iteration 6/25 | Loss: 0.00138867
Iteration 7/25 | Loss: 0.00138867
Iteration 8/25 | Loss: 0.00138867
Iteration 9/25 | Loss: 0.00138867
Iteration 10/25 | Loss: 0.00138867
Iteration 11/25 | Loss: 0.00138867
Iteration 12/25 | Loss: 0.00138867
Iteration 13/25 | Loss: 0.00138867
Iteration 14/25 | Loss: 0.00138867
Iteration 15/25 | Loss: 0.00138867
Iteration 16/25 | Loss: 0.00138867
Iteration 17/25 | Loss: 0.00138867
Iteration 18/25 | Loss: 0.00138867
Iteration 19/25 | Loss: 0.00138867
Iteration 20/25 | Loss: 0.00138867
Iteration 21/25 | Loss: 0.00138867
Iteration 22/25 | Loss: 0.00138867
Iteration 23/25 | Loss: 0.00138867
Iteration 24/25 | Loss: 0.00138867
Iteration 25/25 | Loss: 0.00138867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138867
Iteration 2/1000 | Loss: 0.00004919
Iteration 3/1000 | Loss: 0.00002197
Iteration 4/1000 | Loss: 0.00001768
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001414
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001379
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001354
Iteration 17/1000 | Loss: 0.00001351
Iteration 18/1000 | Loss: 0.00001350
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001344
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001338
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001337
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001334
Iteration 53/1000 | Loss: 0.00001334
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001333
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001329
Iteration 64/1000 | Loss: 0.00001329
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001324
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001319
Iteration 80/1000 | Loss: 0.00001318
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001317
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001315
Iteration 93/1000 | Loss: 0.00001315
Iteration 94/1000 | Loss: 0.00001315
Iteration 95/1000 | Loss: 0.00001315
Iteration 96/1000 | Loss: 0.00001315
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001314
Iteration 99/1000 | Loss: 0.00001314
Iteration 100/1000 | Loss: 0.00001314
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001314
Iteration 106/1000 | Loss: 0.00001314
Iteration 107/1000 | Loss: 0.00001314
Iteration 108/1000 | Loss: 0.00001313
Iteration 109/1000 | Loss: 0.00001313
Iteration 110/1000 | Loss: 0.00001313
Iteration 111/1000 | Loss: 0.00001313
Iteration 112/1000 | Loss: 0.00001313
Iteration 113/1000 | Loss: 0.00001313
Iteration 114/1000 | Loss: 0.00001313
Iteration 115/1000 | Loss: 0.00001313
Iteration 116/1000 | Loss: 0.00001313
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001312
Iteration 119/1000 | Loss: 0.00001312
Iteration 120/1000 | Loss: 0.00001312
Iteration 121/1000 | Loss: 0.00001312
Iteration 122/1000 | Loss: 0.00001311
Iteration 123/1000 | Loss: 0.00001311
Iteration 124/1000 | Loss: 0.00001311
Iteration 125/1000 | Loss: 0.00001311
Iteration 126/1000 | Loss: 0.00001311
Iteration 127/1000 | Loss: 0.00001311
Iteration 128/1000 | Loss: 0.00001311
Iteration 129/1000 | Loss: 0.00001311
Iteration 130/1000 | Loss: 0.00001311
Iteration 131/1000 | Loss: 0.00001311
Iteration 132/1000 | Loss: 0.00001311
Iteration 133/1000 | Loss: 0.00001311
Iteration 134/1000 | Loss: 0.00001311
Iteration 135/1000 | Loss: 0.00001311
Iteration 136/1000 | Loss: 0.00001311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.3107515769661404e-05, 1.3107515769661404e-05, 1.3107515769661404e-05, 1.3107515769661404e-05, 1.3107515769661404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3107515769661404e-05

Optimization complete. Final v2v error: 3.0039288997650146 mm

Highest mean error: 3.430551767349243 mm for frame 7

Lowest mean error: 2.5693893432617188 mm for frame 16

Saving results

Total time: 35.56265950202942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_us_0054/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_us_0054/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029618
Iteration 2/25 | Loss: 0.00209179
Iteration 3/25 | Loss: 0.00157963
Iteration 4/25 | Loss: 0.00160272
Iteration 5/25 | Loss: 0.00146058
Iteration 6/25 | Loss: 0.00125653
Iteration 7/25 | Loss: 0.00120895
Iteration 8/25 | Loss: 0.00116574
Iteration 9/25 | Loss: 0.00116584
Iteration 10/25 | Loss: 0.00113877
Iteration 11/25 | Loss: 0.00111339
Iteration 12/25 | Loss: 0.00109944
Iteration 13/25 | Loss: 0.00110633
Iteration 14/25 | Loss: 0.00109074
Iteration 15/25 | Loss: 0.00109790
Iteration 16/25 | Loss: 0.00108660
Iteration 17/25 | Loss: 0.00109336
Iteration 18/25 | Loss: 0.00107936
Iteration 19/25 | Loss: 0.00107941
Iteration 20/25 | Loss: 0.00108084
Iteration 21/25 | Loss: 0.00107284
Iteration 22/25 | Loss: 0.00106391
Iteration 23/25 | Loss: 0.00106288
Iteration 24/25 | Loss: 0.00106249
Iteration 25/25 | Loss: 0.00105880

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34675670
Iteration 2/25 | Loss: 0.00125825
Iteration 3/25 | Loss: 0.00124835
Iteration 4/25 | Loss: 0.00124835
Iteration 5/25 | Loss: 0.00124835
Iteration 6/25 | Loss: 0.00124835
Iteration 7/25 | Loss: 0.00124835
Iteration 8/25 | Loss: 0.00124835
Iteration 9/25 | Loss: 0.00124835
Iteration 10/25 | Loss: 0.00124835
Iteration 11/25 | Loss: 0.00124835
Iteration 12/25 | Loss: 0.00124835
Iteration 13/25 | Loss: 0.00124835
Iteration 14/25 | Loss: 0.00124835
Iteration 15/25 | Loss: 0.00124835
Iteration 16/25 | Loss: 0.00124835
Iteration 17/25 | Loss: 0.00124835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001248347107321024, 0.001248347107321024, 0.001248347107321024, 0.001248347107321024, 0.001248347107321024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001248347107321024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124835
Iteration 2/1000 | Loss: 0.00040030
Iteration 3/1000 | Loss: 0.00005407
Iteration 4/1000 | Loss: 0.00003876
Iteration 5/1000 | Loss: 0.00005155
Iteration 6/1000 | Loss: 0.00002868
Iteration 7/1000 | Loss: 0.00032474
Iteration 8/1000 | Loss: 0.00026992
Iteration 9/1000 | Loss: 0.00031332
Iteration 10/1000 | Loss: 0.00019606
Iteration 11/1000 | Loss: 0.00024508
Iteration 12/1000 | Loss: 0.00030905
Iteration 13/1000 | Loss: 0.00020574
Iteration 14/1000 | Loss: 0.00008694
Iteration 15/1000 | Loss: 0.00005925
Iteration 16/1000 | Loss: 0.00005358
Iteration 17/1000 | Loss: 0.00003125
Iteration 18/1000 | Loss: 0.00003972
Iteration 19/1000 | Loss: 0.00004007
Iteration 20/1000 | Loss: 0.00003306
Iteration 21/1000 | Loss: 0.00004191
Iteration 22/1000 | Loss: 0.00002642
Iteration 23/1000 | Loss: 0.00003513
Iteration 24/1000 | Loss: 0.00007110
Iteration 25/1000 | Loss: 0.00004023
Iteration 26/1000 | Loss: 0.00003793
Iteration 27/1000 | Loss: 0.00004162
Iteration 28/1000 | Loss: 0.00003710
Iteration 29/1000 | Loss: 0.00004635
Iteration 30/1000 | Loss: 0.00003622
Iteration 31/1000 | Loss: 0.00003444
Iteration 32/1000 | Loss: 0.00003879
Iteration 33/1000 | Loss: 0.00002534
Iteration 34/1000 | Loss: 0.00003724
Iteration 35/1000 | Loss: 0.00003667
Iteration 36/1000 | Loss: 0.00003598
Iteration 37/1000 | Loss: 0.00003695
Iteration 38/1000 | Loss: 0.00003875
Iteration 39/1000 | Loss: 0.00003591
Iteration 40/1000 | Loss: 0.00003514
Iteration 41/1000 | Loss: 0.00003644
Iteration 42/1000 | Loss: 0.00003395
Iteration 43/1000 | Loss: 0.00082054
Iteration 44/1000 | Loss: 0.00007669
Iteration 45/1000 | Loss: 0.00005638
Iteration 46/1000 | Loss: 0.00002519
Iteration 47/1000 | Loss: 0.00004306
Iteration 48/1000 | Loss: 0.00003010
Iteration 49/1000 | Loss: 0.00003271
Iteration 50/1000 | Loss: 0.00002396
Iteration 51/1000 | Loss: 0.00006399
Iteration 52/1000 | Loss: 0.00004273
Iteration 53/1000 | Loss: 0.00004145
Iteration 54/1000 | Loss: 0.00003536
Iteration 55/1000 | Loss: 0.00003190
Iteration 56/1000 | Loss: 0.00003455
Iteration 57/1000 | Loss: 0.00002781
Iteration 58/1000 | Loss: 0.00003358
Iteration 59/1000 | Loss: 0.00002802
Iteration 60/1000 | Loss: 0.00003545
Iteration 61/1000 | Loss: 0.00003928
Iteration 62/1000 | Loss: 0.00003446
Iteration 63/1000 | Loss: 0.00003294
Iteration 64/1000 | Loss: 0.00003466
Iteration 65/1000 | Loss: 0.00003495
Iteration 66/1000 | Loss: 0.00002597
Iteration 67/1000 | Loss: 0.00003394
Iteration 68/1000 | Loss: 0.00003340
Iteration 69/1000 | Loss: 0.00003354
Iteration 70/1000 | Loss: 0.00003340
Iteration 71/1000 | Loss: 0.00003445
Iteration 72/1000 | Loss: 0.00003328
Iteration 73/1000 | Loss: 0.00002513
Iteration 74/1000 | Loss: 0.00002008
Iteration 75/1000 | Loss: 0.00003101
Iteration 76/1000 | Loss: 0.00003747
Iteration 77/1000 | Loss: 0.00002010
Iteration 78/1000 | Loss: 0.00002993
Iteration 79/1000 | Loss: 0.00002921
Iteration 80/1000 | Loss: 0.00003611
Iteration 81/1000 | Loss: 0.00003238
Iteration 82/1000 | Loss: 0.00003684
Iteration 83/1000 | Loss: 0.00002524
Iteration 84/1000 | Loss: 0.00003678
Iteration 85/1000 | Loss: 0.00004314
Iteration 86/1000 | Loss: 0.00003713
Iteration 87/1000 | Loss: 0.00002234
Iteration 88/1000 | Loss: 0.00003274
Iteration 89/1000 | Loss: 0.00005438
Iteration 90/1000 | Loss: 0.00001976
Iteration 91/1000 | Loss: 0.00004168
Iteration 92/1000 | Loss: 0.00001452
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00002718
Iteration 95/1000 | Loss: 0.00006687
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001339
Iteration 98/1000 | Loss: 0.00001650
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001355
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001325
Iteration 109/1000 | Loss: 0.00001325
Iteration 110/1000 | Loss: 0.00001325
Iteration 111/1000 | Loss: 0.00001325
Iteration 112/1000 | Loss: 0.00001325
Iteration 113/1000 | Loss: 0.00001325
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001325
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001325
Iteration 127/1000 | Loss: 0.00001325
Iteration 128/1000 | Loss: 0.00001325
Iteration 129/1000 | Loss: 0.00001325
Iteration 130/1000 | Loss: 0.00001325
Iteration 131/1000 | Loss: 0.00001325
Iteration 132/1000 | Loss: 0.00001325
Iteration 133/1000 | Loss: 0.00001325
Iteration 134/1000 | Loss: 0.00001325
Iteration 135/1000 | Loss: 0.00001325
Iteration 136/1000 | Loss: 0.00001325
Iteration 137/1000 | Loss: 0.00001325
Iteration 138/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.324759068666026e-05, 1.324759068666026e-05, 1.324759068666026e-05, 1.324759068666026e-05, 1.324759068666026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.324759068666026e-05

Optimization complete. Final v2v error: 3.040672779083252 mm

Highest mean error: 5.172068119049072 mm for frame 89

Lowest mean error: 2.4985451698303223 mm for frame 51

Saving results

Total time: 178.9885869026184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772590
Iteration 2/25 | Loss: 0.00212463
Iteration 3/25 | Loss: 0.00199819
Iteration 4/25 | Loss: 0.00197314
Iteration 5/25 | Loss: 0.00194023
Iteration 6/25 | Loss: 0.00194420
Iteration 7/25 | Loss: 0.00194558
Iteration 8/25 | Loss: 0.00193243
Iteration 9/25 | Loss: 0.00193152
Iteration 10/25 | Loss: 0.00193129
Iteration 11/25 | Loss: 0.00193126
Iteration 12/25 | Loss: 0.00193126
Iteration 13/25 | Loss: 0.00193126
Iteration 14/25 | Loss: 0.00193126
Iteration 15/25 | Loss: 0.00193126
Iteration 16/25 | Loss: 0.00193126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019312631338834763, 0.0019312631338834763, 0.0019312631338834763, 0.0019312631338834763, 0.0019312631338834763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019312631338834763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27363372
Iteration 2/25 | Loss: 0.00310190
Iteration 3/25 | Loss: 0.00310190
Iteration 4/25 | Loss: 0.00310190
Iteration 5/25 | Loss: 0.00310190
Iteration 6/25 | Loss: 0.00310190
Iteration 7/25 | Loss: 0.00310190
Iteration 8/25 | Loss: 0.00310190
Iteration 9/25 | Loss: 0.00310190
Iteration 10/25 | Loss: 0.00310190
Iteration 11/25 | Loss: 0.00310190
Iteration 12/25 | Loss: 0.00310190
Iteration 13/25 | Loss: 0.00310190
Iteration 14/25 | Loss: 0.00310190
Iteration 15/25 | Loss: 0.00310190
Iteration 16/25 | Loss: 0.00310190
Iteration 17/25 | Loss: 0.00310190
Iteration 18/25 | Loss: 0.00310190
Iteration 19/25 | Loss: 0.00310190
Iteration 20/25 | Loss: 0.00310190
Iteration 21/25 | Loss: 0.00310190
Iteration 22/25 | Loss: 0.00310190
Iteration 23/25 | Loss: 0.00310190
Iteration 24/25 | Loss: 0.00310190
Iteration 25/25 | Loss: 0.00310190

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310190
Iteration 2/1000 | Loss: 0.00011465
Iteration 3/1000 | Loss: 0.00006619
Iteration 4/1000 | Loss: 0.00005131
Iteration 5/1000 | Loss: 0.00004531
Iteration 6/1000 | Loss: 0.00004223
Iteration 7/1000 | Loss: 0.00003993
Iteration 8/1000 | Loss: 0.00003871
Iteration 9/1000 | Loss: 0.00003794
Iteration 10/1000 | Loss: 0.00003728
Iteration 11/1000 | Loss: 0.00003665
Iteration 12/1000 | Loss: 0.00003606
Iteration 13/1000 | Loss: 0.00003569
Iteration 14/1000 | Loss: 0.00003549
Iteration 15/1000 | Loss: 0.00003545
Iteration 16/1000 | Loss: 0.00003527
Iteration 17/1000 | Loss: 0.00003523
Iteration 18/1000 | Loss: 0.00003517
Iteration 19/1000 | Loss: 0.00003501
Iteration 20/1000 | Loss: 0.00003495
Iteration 21/1000 | Loss: 0.00003489
Iteration 22/1000 | Loss: 0.00003480
Iteration 23/1000 | Loss: 0.00003475
Iteration 24/1000 | Loss: 0.00003474
Iteration 25/1000 | Loss: 0.00003474
Iteration 26/1000 | Loss: 0.00003473
Iteration 27/1000 | Loss: 0.00003472
Iteration 28/1000 | Loss: 0.00003472
Iteration 29/1000 | Loss: 0.00003471
Iteration 30/1000 | Loss: 0.00003470
Iteration 31/1000 | Loss: 0.00003469
Iteration 32/1000 | Loss: 0.00003468
Iteration 33/1000 | Loss: 0.00003464
Iteration 34/1000 | Loss: 0.00003464
Iteration 35/1000 | Loss: 0.00003463
Iteration 36/1000 | Loss: 0.00003461
Iteration 37/1000 | Loss: 0.00003461
Iteration 38/1000 | Loss: 0.00003460
Iteration 39/1000 | Loss: 0.00003460
Iteration 40/1000 | Loss: 0.00003460
Iteration 41/1000 | Loss: 0.00003459
Iteration 42/1000 | Loss: 0.00003459
Iteration 43/1000 | Loss: 0.00003458
Iteration 44/1000 | Loss: 0.00003458
Iteration 45/1000 | Loss: 0.00003457
Iteration 46/1000 | Loss: 0.00003456
Iteration 47/1000 | Loss: 0.00003455
Iteration 48/1000 | Loss: 0.00003455
Iteration 49/1000 | Loss: 0.00003455
Iteration 50/1000 | Loss: 0.00003454
Iteration 51/1000 | Loss: 0.00003454
Iteration 52/1000 | Loss: 0.00003454
Iteration 53/1000 | Loss: 0.00003453
Iteration 54/1000 | Loss: 0.00003453
Iteration 55/1000 | Loss: 0.00003453
Iteration 56/1000 | Loss: 0.00003452
Iteration 57/1000 | Loss: 0.00003452
Iteration 58/1000 | Loss: 0.00003452
Iteration 59/1000 | Loss: 0.00003452
Iteration 60/1000 | Loss: 0.00003451
Iteration 61/1000 | Loss: 0.00003451
Iteration 62/1000 | Loss: 0.00003451
Iteration 63/1000 | Loss: 0.00003450
Iteration 64/1000 | Loss: 0.00003449
Iteration 65/1000 | Loss: 0.00003449
Iteration 66/1000 | Loss: 0.00003449
Iteration 67/1000 | Loss: 0.00003449
Iteration 68/1000 | Loss: 0.00003448
Iteration 69/1000 | Loss: 0.00003448
Iteration 70/1000 | Loss: 0.00003448
Iteration 71/1000 | Loss: 0.00003448
Iteration 72/1000 | Loss: 0.00003447
Iteration 73/1000 | Loss: 0.00003447
Iteration 74/1000 | Loss: 0.00003447
Iteration 75/1000 | Loss: 0.00003446
Iteration 76/1000 | Loss: 0.00003446
Iteration 77/1000 | Loss: 0.00003446
Iteration 78/1000 | Loss: 0.00003445
Iteration 79/1000 | Loss: 0.00003445
Iteration 80/1000 | Loss: 0.00003445
Iteration 81/1000 | Loss: 0.00003445
Iteration 82/1000 | Loss: 0.00003444
Iteration 83/1000 | Loss: 0.00003444
Iteration 84/1000 | Loss: 0.00003444
Iteration 85/1000 | Loss: 0.00003444
Iteration 86/1000 | Loss: 0.00003444
Iteration 87/1000 | Loss: 0.00003444
Iteration 88/1000 | Loss: 0.00003444
Iteration 89/1000 | Loss: 0.00003444
Iteration 90/1000 | Loss: 0.00003444
Iteration 91/1000 | Loss: 0.00003443
Iteration 92/1000 | Loss: 0.00003443
Iteration 93/1000 | Loss: 0.00003443
Iteration 94/1000 | Loss: 0.00003442
Iteration 95/1000 | Loss: 0.00003442
Iteration 96/1000 | Loss: 0.00003442
Iteration 97/1000 | Loss: 0.00003441
Iteration 98/1000 | Loss: 0.00003441
Iteration 99/1000 | Loss: 0.00003441
Iteration 100/1000 | Loss: 0.00003440
Iteration 101/1000 | Loss: 0.00003440
Iteration 102/1000 | Loss: 0.00003440
Iteration 103/1000 | Loss: 0.00003440
Iteration 104/1000 | Loss: 0.00003440
Iteration 105/1000 | Loss: 0.00003440
Iteration 106/1000 | Loss: 0.00003440
Iteration 107/1000 | Loss: 0.00003440
Iteration 108/1000 | Loss: 0.00003440
Iteration 109/1000 | Loss: 0.00003439
Iteration 110/1000 | Loss: 0.00003439
Iteration 111/1000 | Loss: 0.00003439
Iteration 112/1000 | Loss: 0.00003439
Iteration 113/1000 | Loss: 0.00003439
Iteration 114/1000 | Loss: 0.00003439
Iteration 115/1000 | Loss: 0.00003438
Iteration 116/1000 | Loss: 0.00003438
Iteration 117/1000 | Loss: 0.00003438
Iteration 118/1000 | Loss: 0.00003438
Iteration 119/1000 | Loss: 0.00003438
Iteration 120/1000 | Loss: 0.00003437
Iteration 121/1000 | Loss: 0.00003437
Iteration 122/1000 | Loss: 0.00003437
Iteration 123/1000 | Loss: 0.00003437
Iteration 124/1000 | Loss: 0.00003436
Iteration 125/1000 | Loss: 0.00003436
Iteration 126/1000 | Loss: 0.00003436
Iteration 127/1000 | Loss: 0.00003436
Iteration 128/1000 | Loss: 0.00003435
Iteration 129/1000 | Loss: 0.00003435
Iteration 130/1000 | Loss: 0.00003435
Iteration 131/1000 | Loss: 0.00003435
Iteration 132/1000 | Loss: 0.00003434
Iteration 133/1000 | Loss: 0.00003434
Iteration 134/1000 | Loss: 0.00003434
Iteration 135/1000 | Loss: 0.00003433
Iteration 136/1000 | Loss: 0.00003433
Iteration 137/1000 | Loss: 0.00003433
Iteration 138/1000 | Loss: 0.00003433
Iteration 139/1000 | Loss: 0.00003433
Iteration 140/1000 | Loss: 0.00003433
Iteration 141/1000 | Loss: 0.00003433
Iteration 142/1000 | Loss: 0.00003433
Iteration 143/1000 | Loss: 0.00003432
Iteration 144/1000 | Loss: 0.00003432
Iteration 145/1000 | Loss: 0.00003432
Iteration 146/1000 | Loss: 0.00003432
Iteration 147/1000 | Loss: 0.00003432
Iteration 148/1000 | Loss: 0.00003432
Iteration 149/1000 | Loss: 0.00003432
Iteration 150/1000 | Loss: 0.00003431
Iteration 151/1000 | Loss: 0.00003431
Iteration 152/1000 | Loss: 0.00003431
Iteration 153/1000 | Loss: 0.00003431
Iteration 154/1000 | Loss: 0.00003430
Iteration 155/1000 | Loss: 0.00003430
Iteration 156/1000 | Loss: 0.00003430
Iteration 157/1000 | Loss: 0.00003430
Iteration 158/1000 | Loss: 0.00003430
Iteration 159/1000 | Loss: 0.00003430
Iteration 160/1000 | Loss: 0.00003429
Iteration 161/1000 | Loss: 0.00003429
Iteration 162/1000 | Loss: 0.00003429
Iteration 163/1000 | Loss: 0.00003429
Iteration 164/1000 | Loss: 0.00003429
Iteration 165/1000 | Loss: 0.00003429
Iteration 166/1000 | Loss: 0.00003429
Iteration 167/1000 | Loss: 0.00003429
Iteration 168/1000 | Loss: 0.00003429
Iteration 169/1000 | Loss: 0.00003429
Iteration 170/1000 | Loss: 0.00003429
Iteration 171/1000 | Loss: 0.00003429
Iteration 172/1000 | Loss: 0.00003429
Iteration 173/1000 | Loss: 0.00003429
Iteration 174/1000 | Loss: 0.00003429
Iteration 175/1000 | Loss: 0.00003428
Iteration 176/1000 | Loss: 0.00003428
Iteration 177/1000 | Loss: 0.00003428
Iteration 178/1000 | Loss: 0.00003428
Iteration 179/1000 | Loss: 0.00003428
Iteration 180/1000 | Loss: 0.00003428
Iteration 181/1000 | Loss: 0.00003428
Iteration 182/1000 | Loss: 0.00003428
Iteration 183/1000 | Loss: 0.00003428
Iteration 184/1000 | Loss: 0.00003428
Iteration 185/1000 | Loss: 0.00003428
Iteration 186/1000 | Loss: 0.00003427
Iteration 187/1000 | Loss: 0.00003427
Iteration 188/1000 | Loss: 0.00003427
Iteration 189/1000 | Loss: 0.00003427
Iteration 190/1000 | Loss: 0.00003427
Iteration 191/1000 | Loss: 0.00003427
Iteration 192/1000 | Loss: 0.00003427
Iteration 193/1000 | Loss: 0.00003427
Iteration 194/1000 | Loss: 0.00003427
Iteration 195/1000 | Loss: 0.00003427
Iteration 196/1000 | Loss: 0.00003427
Iteration 197/1000 | Loss: 0.00003427
Iteration 198/1000 | Loss: 0.00003427
Iteration 199/1000 | Loss: 0.00003427
Iteration 200/1000 | Loss: 0.00003427
Iteration 201/1000 | Loss: 0.00003427
Iteration 202/1000 | Loss: 0.00003426
Iteration 203/1000 | Loss: 0.00003426
Iteration 204/1000 | Loss: 0.00003426
Iteration 205/1000 | Loss: 0.00003426
Iteration 206/1000 | Loss: 0.00003426
Iteration 207/1000 | Loss: 0.00003426
Iteration 208/1000 | Loss: 0.00003426
Iteration 209/1000 | Loss: 0.00003426
Iteration 210/1000 | Loss: 0.00003426
Iteration 211/1000 | Loss: 0.00003426
Iteration 212/1000 | Loss: 0.00003426
Iteration 213/1000 | Loss: 0.00003426
Iteration 214/1000 | Loss: 0.00003426
Iteration 215/1000 | Loss: 0.00003426
Iteration 216/1000 | Loss: 0.00003426
Iteration 217/1000 | Loss: 0.00003426
Iteration 218/1000 | Loss: 0.00003425
Iteration 219/1000 | Loss: 0.00003425
Iteration 220/1000 | Loss: 0.00003425
Iteration 221/1000 | Loss: 0.00003425
Iteration 222/1000 | Loss: 0.00003425
Iteration 223/1000 | Loss: 0.00003425
Iteration 224/1000 | Loss: 0.00003425
Iteration 225/1000 | Loss: 0.00003425
Iteration 226/1000 | Loss: 0.00003425
Iteration 227/1000 | Loss: 0.00003425
Iteration 228/1000 | Loss: 0.00003425
Iteration 229/1000 | Loss: 0.00003425
Iteration 230/1000 | Loss: 0.00003425
Iteration 231/1000 | Loss: 0.00003425
Iteration 232/1000 | Loss: 0.00003425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [3.425160684855655e-05, 3.425160684855655e-05, 3.425160684855655e-05, 3.425160684855655e-05, 3.425160684855655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.425160684855655e-05

Optimization complete. Final v2v error: 4.894933700561523 mm

Highest mean error: 17.07047462463379 mm for frame 63

Lowest mean error: 4.193282604217529 mm for frame 3

Saving results

Total time: 59.779314279556274
