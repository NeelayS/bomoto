Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=14, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 784-839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870840
Iteration 2/25 | Loss: 0.00227637
Iteration 3/25 | Loss: 0.00180240
Iteration 4/25 | Loss: 0.00176695
Iteration 5/25 | Loss: 0.00156642
Iteration 6/25 | Loss: 0.00147728
Iteration 7/25 | Loss: 0.00144671
Iteration 8/25 | Loss: 0.00143136
Iteration 9/25 | Loss: 0.00144524
Iteration 10/25 | Loss: 0.00145800
Iteration 11/25 | Loss: 0.00141671
Iteration 12/25 | Loss: 0.00138802
Iteration 13/25 | Loss: 0.00138642
Iteration 14/25 | Loss: 0.00138999
Iteration 15/25 | Loss: 0.00137919
Iteration 16/25 | Loss: 0.00136597
Iteration 17/25 | Loss: 0.00137980
Iteration 18/25 | Loss: 0.00137251
Iteration 19/25 | Loss: 0.00134731
Iteration 20/25 | Loss: 0.00132073
Iteration 21/25 | Loss: 0.00131512
Iteration 22/25 | Loss: 0.00131123
Iteration 23/25 | Loss: 0.00131059
Iteration 24/25 | Loss: 0.00131911
Iteration 25/25 | Loss: 0.00131624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46975541
Iteration 2/25 | Loss: 0.00230822
Iteration 3/25 | Loss: 0.00230798
Iteration 4/25 | Loss: 0.00230798
Iteration 5/25 | Loss: 0.00230798
Iteration 6/25 | Loss: 0.00230798
Iteration 7/25 | Loss: 0.00230798
Iteration 8/25 | Loss: 0.00230798
Iteration 9/25 | Loss: 0.00230798
Iteration 10/25 | Loss: 0.00230798
Iteration 11/25 | Loss: 0.00230798
Iteration 12/25 | Loss: 0.00230798
Iteration 13/25 | Loss: 0.00230798
Iteration 14/25 | Loss: 0.00230798
Iteration 15/25 | Loss: 0.00230798
Iteration 16/25 | Loss: 0.00230798
Iteration 17/25 | Loss: 0.00230798
Iteration 18/25 | Loss: 0.00230798
Iteration 19/25 | Loss: 0.00230798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023079768288880587, 0.0023079768288880587, 0.0023079768288880587, 0.0023079768288880587, 0.0023079768288880587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023079768288880587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230798
Iteration 2/1000 | Loss: 0.00313781
Iteration 3/1000 | Loss: 0.00011848
Iteration 4/1000 | Loss: 0.00007977
Iteration 5/1000 | Loss: 0.00006940
Iteration 6/1000 | Loss: 0.00006581
Iteration 7/1000 | Loss: 0.00006027
Iteration 8/1000 | Loss: 0.00005801
Iteration 9/1000 | Loss: 0.00005293
Iteration 10/1000 | Loss: 0.00005397
Iteration 11/1000 | Loss: 0.00005121
Iteration 12/1000 | Loss: 0.00005151
Iteration 13/1000 | Loss: 0.00004980
Iteration 14/1000 | Loss: 0.00005355
Iteration 15/1000 | Loss: 0.00005113
Iteration 16/1000 | Loss: 0.00005303
Iteration 17/1000 | Loss: 0.00005117
Iteration 18/1000 | Loss: 0.00005098
Iteration 19/1000 | Loss: 0.00005059
Iteration 20/1000 | Loss: 0.00004743
Iteration 21/1000 | Loss: 0.00004989
Iteration 22/1000 | Loss: 0.00005017
Iteration 23/1000 | Loss: 0.00004843
Iteration 24/1000 | Loss: 0.00004794
Iteration 25/1000 | Loss: 0.00004896
Iteration 26/1000 | Loss: 0.00004894
Iteration 27/1000 | Loss: 0.00004903
Iteration 28/1000 | Loss: 0.00004919
Iteration 29/1000 | Loss: 0.00004910
Iteration 30/1000 | Loss: 0.00004925
Iteration 31/1000 | Loss: 0.00004724
Iteration 32/1000 | Loss: 0.00005053
Iteration 33/1000 | Loss: 0.00004922
Iteration 34/1000 | Loss: 0.00004957
Iteration 35/1000 | Loss: 0.00004822
Iteration 36/1000 | Loss: 0.00004581
Iteration 37/1000 | Loss: 0.00004733
Iteration 38/1000 | Loss: 0.00005145
Iteration 39/1000 | Loss: 0.00004905
Iteration 40/1000 | Loss: 0.00004904
Iteration 41/1000 | Loss: 0.00004568
Iteration 42/1000 | Loss: 0.00004734
Iteration 43/1000 | Loss: 0.00005242
Iteration 44/1000 | Loss: 0.00004928
Iteration 45/1000 | Loss: 0.00004922
Iteration 46/1000 | Loss: 0.00004910
Iteration 47/1000 | Loss: 0.00004919
Iteration 48/1000 | Loss: 0.00004920
Iteration 49/1000 | Loss: 0.00004906
Iteration 50/1000 | Loss: 0.00004919
Iteration 51/1000 | Loss: 0.00004874
Iteration 52/1000 | Loss: 0.00004570
Iteration 53/1000 | Loss: 0.00004906
Iteration 54/1000 | Loss: 0.00004889
Iteration 55/1000 | Loss: 0.00004914
Iteration 56/1000 | Loss: 0.00004918
Iteration 57/1000 | Loss: 0.00004582
Iteration 58/1000 | Loss: 0.00004928
Iteration 59/1000 | Loss: 0.00004904
Iteration 60/1000 | Loss: 0.00004606
Iteration 61/1000 | Loss: 0.00004800
Iteration 62/1000 | Loss: 0.00004868
Iteration 63/1000 | Loss: 0.00004896
Iteration 64/1000 | Loss: 0.00004880
Iteration 65/1000 | Loss: 0.00004775
Iteration 66/1000 | Loss: 0.00004815
Iteration 67/1000 | Loss: 0.00004934
Iteration 68/1000 | Loss: 0.00004908
Iteration 69/1000 | Loss: 0.00004962
Iteration 70/1000 | Loss: 0.00004860
Iteration 71/1000 | Loss: 0.00004883
Iteration 72/1000 | Loss: 0.00004901
Iteration 73/1000 | Loss: 0.00004906
Iteration 74/1000 | Loss: 0.00004850
Iteration 75/1000 | Loss: 0.00004833
Iteration 76/1000 | Loss: 0.00004927
Iteration 77/1000 | Loss: 0.00004929
Iteration 78/1000 | Loss: 0.00004920
Iteration 79/1000 | Loss: 0.00004908
Iteration 80/1000 | Loss: 0.00004907
Iteration 81/1000 | Loss: 0.00004687
Iteration 82/1000 | Loss: 0.00004710
Iteration 83/1000 | Loss: 0.00004830
Iteration 84/1000 | Loss: 0.00004935
Iteration 85/1000 | Loss: 0.00004872
Iteration 86/1000 | Loss: 0.00004930
Iteration 87/1000 | Loss: 0.00004914
Iteration 88/1000 | Loss: 0.00004869
Iteration 89/1000 | Loss: 0.00004899
Iteration 90/1000 | Loss: 0.00004712
Iteration 91/1000 | Loss: 0.00004770
Iteration 92/1000 | Loss: 0.00004920
Iteration 93/1000 | Loss: 0.00004779
Iteration 94/1000 | Loss: 0.00004931
Iteration 95/1000 | Loss: 0.00005015
Iteration 96/1000 | Loss: 0.00004881
Iteration 97/1000 | Loss: 0.00004933
Iteration 98/1000 | Loss: 0.00004795
Iteration 99/1000 | Loss: 0.00005071
Iteration 100/1000 | Loss: 0.00004886
Iteration 101/1000 | Loss: 0.00005016
Iteration 102/1000 | Loss: 0.00004820
Iteration 103/1000 | Loss: 0.00005003
Iteration 104/1000 | Loss: 0.00004919
Iteration 105/1000 | Loss: 0.00004756
Iteration 106/1000 | Loss: 0.00004832
Iteration 107/1000 | Loss: 0.00004880
Iteration 108/1000 | Loss: 0.00004946
Iteration 109/1000 | Loss: 0.00004946
Iteration 110/1000 | Loss: 0.00004838
Iteration 111/1000 | Loss: 0.00005004
Iteration 112/1000 | Loss: 0.00004896
Iteration 113/1000 | Loss: 0.00004884
Iteration 114/1000 | Loss: 0.00005002
Iteration 115/1000 | Loss: 0.00004848
Iteration 116/1000 | Loss: 0.00004889
Iteration 117/1000 | Loss: 0.00004927
Iteration 118/1000 | Loss: 0.00004891
Iteration 119/1000 | Loss: 0.00004898
Iteration 120/1000 | Loss: 0.00004933
Iteration 121/1000 | Loss: 0.00004856
Iteration 122/1000 | Loss: 0.00005007
Iteration 123/1000 | Loss: 0.00004896
Iteration 124/1000 | Loss: 0.00005011
Iteration 125/1000 | Loss: 0.00004769
Iteration 126/1000 | Loss: 0.00004850
Iteration 127/1000 | Loss: 0.00004845
Iteration 128/1000 | Loss: 0.00004601
Iteration 129/1000 | Loss: 0.00004816
Iteration 130/1000 | Loss: 0.00004896
Iteration 131/1000 | Loss: 0.00004644
Iteration 132/1000 | Loss: 0.00004639
Iteration 133/1000 | Loss: 0.00004899
Iteration 134/1000 | Loss: 0.00004906
Iteration 135/1000 | Loss: 0.00004906
Iteration 136/1000 | Loss: 0.00004683
Iteration 137/1000 | Loss: 0.00004900
Iteration 138/1000 | Loss: 0.00005029
Iteration 139/1000 | Loss: 0.00004666
Iteration 140/1000 | Loss: 0.00004896
Iteration 141/1000 | Loss: 0.00004903
Iteration 142/1000 | Loss: 0.00004933
Iteration 143/1000 | Loss: 0.00004903
Iteration 144/1000 | Loss: 0.00005008
Iteration 145/1000 | Loss: 0.00004696
Iteration 146/1000 | Loss: 0.00004864
Iteration 147/1000 | Loss: 0.00004917
Iteration 148/1000 | Loss: 0.00004935
Iteration 149/1000 | Loss: 0.00004912
Iteration 150/1000 | Loss: 0.00004664
Iteration 151/1000 | Loss: 0.00004776
Iteration 152/1000 | Loss: 0.00004920
Iteration 153/1000 | Loss: 0.00004904
Iteration 154/1000 | Loss: 0.00004919
Iteration 155/1000 | Loss: 0.00004901
Iteration 156/1000 | Loss: 0.00004783
Iteration 157/1000 | Loss: 0.00004768
Iteration 158/1000 | Loss: 0.00004680
Iteration 159/1000 | Loss: 0.00004639
Iteration 160/1000 | Loss: 0.00004898
Iteration 161/1000 | Loss: 0.00004901
Iteration 162/1000 | Loss: 0.00004830
Iteration 163/1000 | Loss: 0.00004850
Iteration 164/1000 | Loss: 0.00004889
Iteration 165/1000 | Loss: 0.00004928
Iteration 166/1000 | Loss: 0.00004927
Iteration 167/1000 | Loss: 0.00004927
Iteration 168/1000 | Loss: 0.00004912
Iteration 169/1000 | Loss: 0.00004912
Iteration 170/1000 | Loss: 0.00004894
Iteration 171/1000 | Loss: 0.00004938
Iteration 172/1000 | Loss: 0.00004849
Iteration 173/1000 | Loss: 0.00004894
Iteration 174/1000 | Loss: 0.00004870
Iteration 175/1000 | Loss: 0.00004954
Iteration 176/1000 | Loss: 0.00004906
Iteration 177/1000 | Loss: 0.00004658
Iteration 178/1000 | Loss: 0.00004759
Iteration 179/1000 | Loss: 0.00004838
Iteration 180/1000 | Loss: 0.00004773
Iteration 181/1000 | Loss: 0.00005227
Iteration 182/1000 | Loss: 0.00004705
Iteration 183/1000 | Loss: 0.00004535
Iteration 184/1000 | Loss: 0.00004489
Iteration 185/1000 | Loss: 0.00004482
Iteration 186/1000 | Loss: 0.00004479
Iteration 187/1000 | Loss: 0.00004479
Iteration 188/1000 | Loss: 0.00004479
Iteration 189/1000 | Loss: 0.00004479
Iteration 190/1000 | Loss: 0.00004478
Iteration 191/1000 | Loss: 0.00004478
Iteration 192/1000 | Loss: 0.00004478
Iteration 193/1000 | Loss: 0.00004478
Iteration 194/1000 | Loss: 0.00004478
Iteration 195/1000 | Loss: 0.00004478
Iteration 196/1000 | Loss: 0.00004478
Iteration 197/1000 | Loss: 0.00004478
Iteration 198/1000 | Loss: 0.00004478
Iteration 199/1000 | Loss: 0.00004478
Iteration 200/1000 | Loss: 0.00004477
Iteration 201/1000 | Loss: 0.00004477
Iteration 202/1000 | Loss: 0.00004477
Iteration 203/1000 | Loss: 0.00004477
Iteration 204/1000 | Loss: 0.00004476
Iteration 205/1000 | Loss: 0.00004476
Iteration 206/1000 | Loss: 0.00004476
Iteration 207/1000 | Loss: 0.00004476
Iteration 208/1000 | Loss: 0.00004475
Iteration 209/1000 | Loss: 0.00004475
Iteration 210/1000 | Loss: 0.00004475
Iteration 211/1000 | Loss: 0.00004474
Iteration 212/1000 | Loss: 0.00004474
Iteration 213/1000 | Loss: 0.00004473
Iteration 214/1000 | Loss: 0.00004473
Iteration 215/1000 | Loss: 0.00004473
Iteration 216/1000 | Loss: 0.00004473
Iteration 217/1000 | Loss: 0.00004473
Iteration 218/1000 | Loss: 0.00004473
Iteration 219/1000 | Loss: 0.00004473
Iteration 220/1000 | Loss: 0.00004473
Iteration 221/1000 | Loss: 0.00004473
Iteration 222/1000 | Loss: 0.00004472
Iteration 223/1000 | Loss: 0.00004472
Iteration 224/1000 | Loss: 0.00004472
Iteration 225/1000 | Loss: 0.00004472
Iteration 226/1000 | Loss: 0.00004472
Iteration 227/1000 | Loss: 0.00004472
Iteration 228/1000 | Loss: 0.00004472
Iteration 229/1000 | Loss: 0.00004472
Iteration 230/1000 | Loss: 0.00004472
Iteration 231/1000 | Loss: 0.00004472
Iteration 232/1000 | Loss: 0.00004471
Iteration 233/1000 | Loss: 0.00004469
Iteration 234/1000 | Loss: 0.00004467
Iteration 235/1000 | Loss: 0.00004467
Iteration 236/1000 | Loss: 0.00004467
Iteration 237/1000 | Loss: 0.00004466
Iteration 238/1000 | Loss: 0.00004466
Iteration 239/1000 | Loss: 0.00004465
Iteration 240/1000 | Loss: 0.00004465
Iteration 241/1000 | Loss: 0.00004465
Iteration 242/1000 | Loss: 0.00004464
Iteration 243/1000 | Loss: 0.00004464
Iteration 244/1000 | Loss: 0.00004464
Iteration 245/1000 | Loss: 0.00004463
Iteration 246/1000 | Loss: 0.00004463
Iteration 247/1000 | Loss: 0.00004463
Iteration 248/1000 | Loss: 0.00004463
Iteration 249/1000 | Loss: 0.00004463
Iteration 250/1000 | Loss: 0.00004463
Iteration 251/1000 | Loss: 0.00004462
Iteration 252/1000 | Loss: 0.00004462
Iteration 253/1000 | Loss: 0.00004462
Iteration 254/1000 | Loss: 0.00004462
Iteration 255/1000 | Loss: 0.00004462
Iteration 256/1000 | Loss: 0.00004462
Iteration 257/1000 | Loss: 0.00004462
Iteration 258/1000 | Loss: 0.00004461
Iteration 259/1000 | Loss: 0.00004461
Iteration 260/1000 | Loss: 0.00004461
Iteration 261/1000 | Loss: 0.00004460
Iteration 262/1000 | Loss: 0.00004460
Iteration 263/1000 | Loss: 0.00004460
Iteration 264/1000 | Loss: 0.00004460
Iteration 265/1000 | Loss: 0.00004459
Iteration 266/1000 | Loss: 0.00004459
Iteration 267/1000 | Loss: 0.00004459
Iteration 268/1000 | Loss: 0.00004459
Iteration 269/1000 | Loss: 0.00004459
Iteration 270/1000 | Loss: 0.00004459
Iteration 271/1000 | Loss: 0.00004459
Iteration 272/1000 | Loss: 0.00004459
Iteration 273/1000 | Loss: 0.00004458
Iteration 274/1000 | Loss: 0.00004458
Iteration 275/1000 | Loss: 0.00004458
Iteration 276/1000 | Loss: 0.00004458
Iteration 277/1000 | Loss: 0.00004458
Iteration 278/1000 | Loss: 0.00004458
Iteration 279/1000 | Loss: 0.00004458
Iteration 280/1000 | Loss: 0.00004458
Iteration 281/1000 | Loss: 0.00004458
Iteration 282/1000 | Loss: 0.00004457
Iteration 283/1000 | Loss: 0.00004457
Iteration 284/1000 | Loss: 0.00004457
Iteration 285/1000 | Loss: 0.00004457
Iteration 286/1000 | Loss: 0.00004457
Iteration 287/1000 | Loss: 0.00004457
Iteration 288/1000 | Loss: 0.00004457
Iteration 289/1000 | Loss: 0.00004457
Iteration 290/1000 | Loss: 0.00004457
Iteration 291/1000 | Loss: 0.00004457
Iteration 292/1000 | Loss: 0.00004457
Iteration 293/1000 | Loss: 0.00004456
Iteration 294/1000 | Loss: 0.00004456
Iteration 295/1000 | Loss: 0.00004456
Iteration 296/1000 | Loss: 0.00004456
Iteration 297/1000 | Loss: 0.00004456
Iteration 298/1000 | Loss: 0.00004456
Iteration 299/1000 | Loss: 0.00004456
Iteration 300/1000 | Loss: 0.00004456
Iteration 301/1000 | Loss: 0.00004456
Iteration 302/1000 | Loss: 0.00004455
Iteration 303/1000 | Loss: 0.00004455
Iteration 304/1000 | Loss: 0.00004455
Iteration 305/1000 | Loss: 0.00004455
Iteration 306/1000 | Loss: 0.00004455
Iteration 307/1000 | Loss: 0.00004455
Iteration 308/1000 | Loss: 0.00004454
Iteration 309/1000 | Loss: 0.00004454
Iteration 310/1000 | Loss: 0.00004454
Iteration 311/1000 | Loss: 0.00004454
Iteration 312/1000 | Loss: 0.00004453
Iteration 313/1000 | Loss: 0.00004453
Iteration 314/1000 | Loss: 0.00004453
Iteration 315/1000 | Loss: 0.00004452
Iteration 316/1000 | Loss: 0.00004452
Iteration 317/1000 | Loss: 0.00004452
Iteration 318/1000 | Loss: 0.00004452
Iteration 319/1000 | Loss: 0.00004452
Iteration 320/1000 | Loss: 0.00004452
Iteration 321/1000 | Loss: 0.00004452
Iteration 322/1000 | Loss: 0.00004451
Iteration 323/1000 | Loss: 0.00004451
Iteration 324/1000 | Loss: 0.00004451
Iteration 325/1000 | Loss: 0.00004451
Iteration 326/1000 | Loss: 0.00004451
Iteration 327/1000 | Loss: 0.00004451
Iteration 328/1000 | Loss: 0.00004451
Iteration 329/1000 | Loss: 0.00004451
Iteration 330/1000 | Loss: 0.00004451
Iteration 331/1000 | Loss: 0.00004451
Iteration 332/1000 | Loss: 0.00004451
Iteration 333/1000 | Loss: 0.00004450
Iteration 334/1000 | Loss: 0.00004450
Iteration 335/1000 | Loss: 0.00004450
Iteration 336/1000 | Loss: 0.00004450
Iteration 337/1000 | Loss: 0.00004450
Iteration 338/1000 | Loss: 0.00004450
Iteration 339/1000 | Loss: 0.00004450
Iteration 340/1000 | Loss: 0.00004450
Iteration 341/1000 | Loss: 0.00004450
Iteration 342/1000 | Loss: 0.00004450
Iteration 343/1000 | Loss: 0.00004450
Iteration 344/1000 | Loss: 0.00004449
Iteration 345/1000 | Loss: 0.00004449
Iteration 346/1000 | Loss: 0.00004449
Iteration 347/1000 | Loss: 0.00004449
Iteration 348/1000 | Loss: 0.00004449
Iteration 349/1000 | Loss: 0.00004449
Iteration 350/1000 | Loss: 0.00004449
Iteration 351/1000 | Loss: 0.00004449
Iteration 352/1000 | Loss: 0.00004449
Iteration 353/1000 | Loss: 0.00004449
Iteration 354/1000 | Loss: 0.00004449
Iteration 355/1000 | Loss: 0.00004449
Iteration 356/1000 | Loss: 0.00004449
Iteration 357/1000 | Loss: 0.00004449
Iteration 358/1000 | Loss: 0.00004449
Iteration 359/1000 | Loss: 0.00004449
Iteration 360/1000 | Loss: 0.00004449
Iteration 361/1000 | Loss: 0.00004449
Iteration 362/1000 | Loss: 0.00004449
Iteration 363/1000 | Loss: 0.00004449
Iteration 364/1000 | Loss: 0.00004449
Iteration 365/1000 | Loss: 0.00004449
Iteration 366/1000 | Loss: 0.00004449
Iteration 367/1000 | Loss: 0.00004449
Iteration 368/1000 | Loss: 0.00004449
Iteration 369/1000 | Loss: 0.00004449
Iteration 370/1000 | Loss: 0.00004449
Iteration 371/1000 | Loss: 0.00004449
Iteration 372/1000 | Loss: 0.00004449
Iteration 373/1000 | Loss: 0.00004449
Iteration 374/1000 | Loss: 0.00004449
Iteration 375/1000 | Loss: 0.00004449
Iteration 376/1000 | Loss: 0.00004449
Iteration 377/1000 | Loss: 0.00004449
Iteration 378/1000 | Loss: 0.00004449
Iteration 379/1000 | Loss: 0.00004449
Iteration 380/1000 | Loss: 0.00004449
Iteration 381/1000 | Loss: 0.00004449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 381. Stopping optimization.
Last 5 losses: [4.4494212488643825e-05, 4.4494212488643825e-05, 4.4494212488643825e-05, 4.4494212488643825e-05, 4.4494212488643825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4494212488643825e-05

Optimization complete. Final v2v error: 4.949902057647705 mm

Highest mean error: 11.321030616760254 mm for frame 45

Lowest mean error: 3.1145482063293457 mm for frame 106

Saving results

Total time: 350.17197012901306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914526
Iteration 2/25 | Loss: 0.00152697
Iteration 3/25 | Loss: 0.00123184
Iteration 4/25 | Loss: 0.00119225
Iteration 5/25 | Loss: 0.00120467
Iteration 6/25 | Loss: 0.00118431
Iteration 7/25 | Loss: 0.00118437
Iteration 8/25 | Loss: 0.00115132
Iteration 9/25 | Loss: 0.00113559
Iteration 10/25 | Loss: 0.00113101
Iteration 11/25 | Loss: 0.00113282
Iteration 12/25 | Loss: 0.00111890
Iteration 13/25 | Loss: 0.00112078
Iteration 14/25 | Loss: 0.00111641
Iteration 15/25 | Loss: 0.00111510
Iteration 16/25 | Loss: 0.00111870
Iteration 17/25 | Loss: 0.00111926
Iteration 18/25 | Loss: 0.00111792
Iteration 19/25 | Loss: 0.00111678
Iteration 20/25 | Loss: 0.00111370
Iteration 21/25 | Loss: 0.00111323
Iteration 22/25 | Loss: 0.00111235
Iteration 23/25 | Loss: 0.00111205
Iteration 24/25 | Loss: 0.00111193
Iteration 25/25 | Loss: 0.00111179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.29578733
Iteration 2/25 | Loss: 0.00158933
Iteration 3/25 | Loss: 0.00158924
Iteration 4/25 | Loss: 0.00158924
Iteration 5/25 | Loss: 0.00158924
Iteration 6/25 | Loss: 0.00158924
Iteration 7/25 | Loss: 0.00158924
Iteration 8/25 | Loss: 0.00158924
Iteration 9/25 | Loss: 0.00158924
Iteration 10/25 | Loss: 0.00158924
Iteration 11/25 | Loss: 0.00158924
Iteration 12/25 | Loss: 0.00158924
Iteration 13/25 | Loss: 0.00158924
Iteration 14/25 | Loss: 0.00158924
Iteration 15/25 | Loss: 0.00158924
Iteration 16/25 | Loss: 0.00158924
Iteration 17/25 | Loss: 0.00158924
Iteration 18/25 | Loss: 0.00158924
Iteration 19/25 | Loss: 0.00158924
Iteration 20/25 | Loss: 0.00158924
Iteration 21/25 | Loss: 0.00158924
Iteration 22/25 | Loss: 0.00158924
Iteration 23/25 | Loss: 0.00158924
Iteration 24/25 | Loss: 0.00158924
Iteration 25/25 | Loss: 0.00158924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158924
Iteration 2/1000 | Loss: 0.00022103
Iteration 3/1000 | Loss: 0.00044021
Iteration 4/1000 | Loss: 0.00005086
Iteration 5/1000 | Loss: 0.00003811
Iteration 6/1000 | Loss: 0.00003283
Iteration 7/1000 | Loss: 0.00024152
Iteration 8/1000 | Loss: 0.00005871
Iteration 9/1000 | Loss: 0.00004507
Iteration 10/1000 | Loss: 0.00003983
Iteration 11/1000 | Loss: 0.00005362
Iteration 12/1000 | Loss: 0.00004860
Iteration 13/1000 | Loss: 0.00003412
Iteration 14/1000 | Loss: 0.00010108
Iteration 15/1000 | Loss: 0.00003522
Iteration 16/1000 | Loss: 0.00003147
Iteration 17/1000 | Loss: 0.00002911
Iteration 18/1000 | Loss: 0.00002799
Iteration 19/1000 | Loss: 0.00002712
Iteration 20/1000 | Loss: 0.00002591
Iteration 21/1000 | Loss: 0.00002498
Iteration 22/1000 | Loss: 0.00004291
Iteration 23/1000 | Loss: 0.00002781
Iteration 24/1000 | Loss: 0.00003835
Iteration 25/1000 | Loss: 0.00004163
Iteration 26/1000 | Loss: 0.00002760
Iteration 27/1000 | Loss: 0.00004255
Iteration 28/1000 | Loss: 0.00006000
Iteration 29/1000 | Loss: 0.00003596
Iteration 30/1000 | Loss: 0.00003231
Iteration 31/1000 | Loss: 0.00002516
Iteration 32/1000 | Loss: 0.00005472
Iteration 33/1000 | Loss: 0.00003714
Iteration 34/1000 | Loss: 0.00003141
Iteration 35/1000 | Loss: 0.00003398
Iteration 36/1000 | Loss: 0.00003035
Iteration 37/1000 | Loss: 0.00002350
Iteration 38/1000 | Loss: 0.00002259
Iteration 39/1000 | Loss: 0.00002167
Iteration 40/1000 | Loss: 0.00002073
Iteration 41/1000 | Loss: 0.00002041
Iteration 42/1000 | Loss: 0.00002016
Iteration 43/1000 | Loss: 0.00001996
Iteration 44/1000 | Loss: 0.00001988
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001986
Iteration 47/1000 | Loss: 0.00001985
Iteration 48/1000 | Loss: 0.00001985
Iteration 49/1000 | Loss: 0.00001985
Iteration 50/1000 | Loss: 0.00001984
Iteration 51/1000 | Loss: 0.00001984
Iteration 52/1000 | Loss: 0.00001984
Iteration 53/1000 | Loss: 0.00001984
Iteration 54/1000 | Loss: 0.00001984
Iteration 55/1000 | Loss: 0.00001984
Iteration 56/1000 | Loss: 0.00001984
Iteration 57/1000 | Loss: 0.00001984
Iteration 58/1000 | Loss: 0.00001984
Iteration 59/1000 | Loss: 0.00001983
Iteration 60/1000 | Loss: 0.00001983
Iteration 61/1000 | Loss: 0.00001983
Iteration 62/1000 | Loss: 0.00001982
Iteration 63/1000 | Loss: 0.00001982
Iteration 64/1000 | Loss: 0.00001982
Iteration 65/1000 | Loss: 0.00001981
Iteration 66/1000 | Loss: 0.00001981
Iteration 67/1000 | Loss: 0.00001981
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001976
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001975
Iteration 73/1000 | Loss: 0.00001975
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00001975
Iteration 78/1000 | Loss: 0.00001975
Iteration 79/1000 | Loss: 0.00001975
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001973
Iteration 86/1000 | Loss: 0.00001973
Iteration 87/1000 | Loss: 0.00001972
Iteration 88/1000 | Loss: 0.00001972
Iteration 89/1000 | Loss: 0.00001972
Iteration 90/1000 | Loss: 0.00001971
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001969
Iteration 105/1000 | Loss: 0.00001969
Iteration 106/1000 | Loss: 0.00001969
Iteration 107/1000 | Loss: 0.00001969
Iteration 108/1000 | Loss: 0.00001968
Iteration 109/1000 | Loss: 0.00001968
Iteration 110/1000 | Loss: 0.00001968
Iteration 111/1000 | Loss: 0.00001968
Iteration 112/1000 | Loss: 0.00001967
Iteration 113/1000 | Loss: 0.00001967
Iteration 114/1000 | Loss: 0.00001967
Iteration 115/1000 | Loss: 0.00001966
Iteration 116/1000 | Loss: 0.00001966
Iteration 117/1000 | Loss: 0.00001966
Iteration 118/1000 | Loss: 0.00001966
Iteration 119/1000 | Loss: 0.00001966
Iteration 120/1000 | Loss: 0.00001966
Iteration 121/1000 | Loss: 0.00001965
Iteration 122/1000 | Loss: 0.00001965
Iteration 123/1000 | Loss: 0.00001965
Iteration 124/1000 | Loss: 0.00001963
Iteration 125/1000 | Loss: 0.00001963
Iteration 126/1000 | Loss: 0.00001963
Iteration 127/1000 | Loss: 0.00001962
Iteration 128/1000 | Loss: 0.00001962
Iteration 129/1000 | Loss: 0.00001962
Iteration 130/1000 | Loss: 0.00001962
Iteration 131/1000 | Loss: 0.00001961
Iteration 132/1000 | Loss: 0.00001961
Iteration 133/1000 | Loss: 0.00001959
Iteration 134/1000 | Loss: 0.00001959
Iteration 135/1000 | Loss: 0.00001959
Iteration 136/1000 | Loss: 0.00001959
Iteration 137/1000 | Loss: 0.00001959
Iteration 138/1000 | Loss: 0.00001959
Iteration 139/1000 | Loss: 0.00001959
Iteration 140/1000 | Loss: 0.00001959
Iteration 141/1000 | Loss: 0.00001959
Iteration 142/1000 | Loss: 0.00001959
Iteration 143/1000 | Loss: 0.00001959
Iteration 144/1000 | Loss: 0.00001959
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001957
Iteration 147/1000 | Loss: 0.00001957
Iteration 148/1000 | Loss: 0.00004085
Iteration 149/1000 | Loss: 0.00004623
Iteration 150/1000 | Loss: 0.00005298
Iteration 151/1000 | Loss: 0.00004732
Iteration 152/1000 | Loss: 0.00005074
Iteration 153/1000 | Loss: 0.00003906
Iteration 154/1000 | Loss: 0.00003679
Iteration 155/1000 | Loss: 0.00002969
Iteration 156/1000 | Loss: 0.00002908
Iteration 157/1000 | Loss: 0.00002406
Iteration 158/1000 | Loss: 0.00003441
Iteration 159/1000 | Loss: 0.00003216
Iteration 160/1000 | Loss: 0.00001975
Iteration 161/1000 | Loss: 0.00001959
Iteration 162/1000 | Loss: 0.00001956
Iteration 163/1000 | Loss: 0.00001955
Iteration 164/1000 | Loss: 0.00001955
Iteration 165/1000 | Loss: 0.00001954
Iteration 166/1000 | Loss: 0.00001954
Iteration 167/1000 | Loss: 0.00001954
Iteration 168/1000 | Loss: 0.00001953
Iteration 169/1000 | Loss: 0.00001953
Iteration 170/1000 | Loss: 0.00001952
Iteration 171/1000 | Loss: 0.00001952
Iteration 172/1000 | Loss: 0.00001952
Iteration 173/1000 | Loss: 0.00001952
Iteration 174/1000 | Loss: 0.00001952
Iteration 175/1000 | Loss: 0.00001952
Iteration 176/1000 | Loss: 0.00001952
Iteration 177/1000 | Loss: 0.00001952
Iteration 178/1000 | Loss: 0.00001952
Iteration 179/1000 | Loss: 0.00001952
Iteration 180/1000 | Loss: 0.00001952
Iteration 181/1000 | Loss: 0.00001952
Iteration 182/1000 | Loss: 0.00001951
Iteration 183/1000 | Loss: 0.00001951
Iteration 184/1000 | Loss: 0.00001951
Iteration 185/1000 | Loss: 0.00001951
Iteration 186/1000 | Loss: 0.00001951
Iteration 187/1000 | Loss: 0.00001951
Iteration 188/1000 | Loss: 0.00001951
Iteration 189/1000 | Loss: 0.00001950
Iteration 190/1000 | Loss: 0.00001950
Iteration 191/1000 | Loss: 0.00001950
Iteration 192/1000 | Loss: 0.00001950
Iteration 193/1000 | Loss: 0.00001950
Iteration 194/1000 | Loss: 0.00001950
Iteration 195/1000 | Loss: 0.00001950
Iteration 196/1000 | Loss: 0.00001950
Iteration 197/1000 | Loss: 0.00001950
Iteration 198/1000 | Loss: 0.00001950
Iteration 199/1000 | Loss: 0.00001950
Iteration 200/1000 | Loss: 0.00001950
Iteration 201/1000 | Loss: 0.00001950
Iteration 202/1000 | Loss: 0.00001950
Iteration 203/1000 | Loss: 0.00001950
Iteration 204/1000 | Loss: 0.00001950
Iteration 205/1000 | Loss: 0.00001950
Iteration 206/1000 | Loss: 0.00001950
Iteration 207/1000 | Loss: 0.00001950
Iteration 208/1000 | Loss: 0.00001950
Iteration 209/1000 | Loss: 0.00001950
Iteration 210/1000 | Loss: 0.00001950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.9497407265589572e-05, 1.9497407265589572e-05, 1.9497407265589572e-05, 1.9497407265589572e-05, 1.9497407265589572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9497407265589572e-05

Optimization complete. Final v2v error: 3.637233257293701 mm

Highest mean error: 6.538252353668213 mm for frame 95

Lowest mean error: 2.7546887397766113 mm for frame 0

Saving results

Total time: 145.35545682907104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848292
Iteration 2/25 | Loss: 0.00143819
Iteration 3/25 | Loss: 0.00108852
Iteration 4/25 | Loss: 0.00106877
Iteration 5/25 | Loss: 0.00106706
Iteration 6/25 | Loss: 0.00106706
Iteration 7/25 | Loss: 0.00106706
Iteration 8/25 | Loss: 0.00106706
Iteration 9/25 | Loss: 0.00106706
Iteration 10/25 | Loss: 0.00106706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010670630726963282, 0.0010670630726963282, 0.0010670630726963282, 0.0010670630726963282, 0.0010670630726963282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010670630726963282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25516641
Iteration 2/25 | Loss: 0.00094604
Iteration 3/25 | Loss: 0.00094604
Iteration 4/25 | Loss: 0.00094604
Iteration 5/25 | Loss: 0.00094604
Iteration 6/25 | Loss: 0.00094604
Iteration 7/25 | Loss: 0.00094604
Iteration 8/25 | Loss: 0.00094604
Iteration 9/25 | Loss: 0.00094604
Iteration 10/25 | Loss: 0.00094604
Iteration 11/25 | Loss: 0.00094604
Iteration 12/25 | Loss: 0.00094604
Iteration 13/25 | Loss: 0.00094604
Iteration 14/25 | Loss: 0.00094604
Iteration 15/25 | Loss: 0.00094604
Iteration 16/25 | Loss: 0.00094604
Iteration 17/25 | Loss: 0.00094604
Iteration 18/25 | Loss: 0.00094604
Iteration 19/25 | Loss: 0.00094604
Iteration 20/25 | Loss: 0.00094604
Iteration 21/25 | Loss: 0.00094604
Iteration 22/25 | Loss: 0.00094604
Iteration 23/25 | Loss: 0.00094604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009460350265726447, 0.0009460350265726447, 0.0009460350265726447, 0.0009460350265726447, 0.0009460350265726447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009460350265726447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094604
Iteration 2/1000 | Loss: 0.00003675
Iteration 3/1000 | Loss: 0.00001973
Iteration 4/1000 | Loss: 0.00001695
Iteration 5/1000 | Loss: 0.00001481
Iteration 6/1000 | Loss: 0.00001415
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001333
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001272
Iteration 11/1000 | Loss: 0.00001259
Iteration 12/1000 | Loss: 0.00001253
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001225
Iteration 21/1000 | Loss: 0.00001224
Iteration 22/1000 | Loss: 0.00001224
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001218
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001217
Iteration 45/1000 | Loss: 0.00001217
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001216
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001216
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001215
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001215
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001213
Iteration 62/1000 | Loss: 0.00001213
Iteration 63/1000 | Loss: 0.00001213
Iteration 64/1000 | Loss: 0.00001213
Iteration 65/1000 | Loss: 0.00001213
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001213
Iteration 68/1000 | Loss: 0.00001213
Iteration 69/1000 | Loss: 0.00001213
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001212
Iteration 75/1000 | Loss: 0.00001212
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001211
Iteration 84/1000 | Loss: 0.00001211
Iteration 85/1000 | Loss: 0.00001211
Iteration 86/1000 | Loss: 0.00001211
Iteration 87/1000 | Loss: 0.00001211
Iteration 88/1000 | Loss: 0.00001211
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001209
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001209
Iteration 113/1000 | Loss: 0.00001209
Iteration 114/1000 | Loss: 0.00001209
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001209
Iteration 117/1000 | Loss: 0.00001209
Iteration 118/1000 | Loss: 0.00001209
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001209
Iteration 122/1000 | Loss: 0.00001209
Iteration 123/1000 | Loss: 0.00001209
Iteration 124/1000 | Loss: 0.00001208
Iteration 125/1000 | Loss: 0.00001208
Iteration 126/1000 | Loss: 0.00001208
Iteration 127/1000 | Loss: 0.00001208
Iteration 128/1000 | Loss: 0.00001208
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001208
Iteration 132/1000 | Loss: 0.00001208
Iteration 133/1000 | Loss: 0.00001207
Iteration 134/1000 | Loss: 0.00001207
Iteration 135/1000 | Loss: 0.00001207
Iteration 136/1000 | Loss: 0.00001207
Iteration 137/1000 | Loss: 0.00001207
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001207
Iteration 144/1000 | Loss: 0.00001207
Iteration 145/1000 | Loss: 0.00001207
Iteration 146/1000 | Loss: 0.00001207
Iteration 147/1000 | Loss: 0.00001207
Iteration 148/1000 | Loss: 0.00001206
Iteration 149/1000 | Loss: 0.00001206
Iteration 150/1000 | Loss: 0.00001206
Iteration 151/1000 | Loss: 0.00001206
Iteration 152/1000 | Loss: 0.00001206
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001206
Iteration 156/1000 | Loss: 0.00001206
Iteration 157/1000 | Loss: 0.00001206
Iteration 158/1000 | Loss: 0.00001206
Iteration 159/1000 | Loss: 0.00001206
Iteration 160/1000 | Loss: 0.00001206
Iteration 161/1000 | Loss: 0.00001206
Iteration 162/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.2058916581736412e-05, 1.2058916581736412e-05, 1.2058916581736412e-05, 1.2058916581736412e-05, 1.2058916581736412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2058916581736412e-05

Optimization complete. Final v2v error: 2.9549152851104736 mm

Highest mean error: 3.4796347618103027 mm for frame 198

Lowest mean error: 2.384014368057251 mm for frame 65

Saving results

Total time: 40.957629680633545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498044
Iteration 2/25 | Loss: 0.00121067
Iteration 3/25 | Loss: 0.00107785
Iteration 4/25 | Loss: 0.00106550
Iteration 5/25 | Loss: 0.00106246
Iteration 6/25 | Loss: 0.00106161
Iteration 7/25 | Loss: 0.00106161
Iteration 8/25 | Loss: 0.00106161
Iteration 9/25 | Loss: 0.00106161
Iteration 10/25 | Loss: 0.00106161
Iteration 11/25 | Loss: 0.00106161
Iteration 12/25 | Loss: 0.00106161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010616104118525982, 0.0010616104118525982, 0.0010616104118525982, 0.0010616104118525982, 0.0010616104118525982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010616104118525982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35860431
Iteration 2/25 | Loss: 0.00131250
Iteration 3/25 | Loss: 0.00131250
Iteration 4/25 | Loss: 0.00131250
Iteration 5/25 | Loss: 0.00131250
Iteration 6/25 | Loss: 0.00131250
Iteration 7/25 | Loss: 0.00131249
Iteration 8/25 | Loss: 0.00131249
Iteration 9/25 | Loss: 0.00131249
Iteration 10/25 | Loss: 0.00131249
Iteration 11/25 | Loss: 0.00131249
Iteration 12/25 | Loss: 0.00131249
Iteration 13/25 | Loss: 0.00131249
Iteration 14/25 | Loss: 0.00131249
Iteration 15/25 | Loss: 0.00131249
Iteration 16/25 | Loss: 0.00131249
Iteration 17/25 | Loss: 0.00131249
Iteration 18/25 | Loss: 0.00131249
Iteration 19/25 | Loss: 0.00131249
Iteration 20/25 | Loss: 0.00131249
Iteration 21/25 | Loss: 0.00131249
Iteration 22/25 | Loss: 0.00131249
Iteration 23/25 | Loss: 0.00131249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001312493928708136, 0.001312493928708136, 0.001312493928708136, 0.001312493928708136, 0.001312493928708136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001312493928708136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131249
Iteration 2/1000 | Loss: 0.00006582
Iteration 3/1000 | Loss: 0.00003198
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001798
Iteration 8/1000 | Loss: 0.00001753
Iteration 9/1000 | Loss: 0.00001697
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001633
Iteration 12/1000 | Loss: 0.00001620
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001593
Iteration 15/1000 | Loss: 0.00001592
Iteration 16/1000 | Loss: 0.00001591
Iteration 17/1000 | Loss: 0.00001590
Iteration 18/1000 | Loss: 0.00001590
Iteration 19/1000 | Loss: 0.00001583
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001580
Iteration 22/1000 | Loss: 0.00001579
Iteration 23/1000 | Loss: 0.00001579
Iteration 24/1000 | Loss: 0.00001579
Iteration 25/1000 | Loss: 0.00001578
Iteration 26/1000 | Loss: 0.00001578
Iteration 27/1000 | Loss: 0.00001578
Iteration 28/1000 | Loss: 0.00001577
Iteration 29/1000 | Loss: 0.00001577
Iteration 30/1000 | Loss: 0.00001576
Iteration 31/1000 | Loss: 0.00001576
Iteration 32/1000 | Loss: 0.00001572
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001571
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001569
Iteration 40/1000 | Loss: 0.00001569
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001568
Iteration 44/1000 | Loss: 0.00001568
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001566
Iteration 49/1000 | Loss: 0.00001566
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001565
Iteration 53/1000 | Loss: 0.00001565
Iteration 54/1000 | Loss: 0.00001565
Iteration 55/1000 | Loss: 0.00001565
Iteration 56/1000 | Loss: 0.00001565
Iteration 57/1000 | Loss: 0.00001565
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001564
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001563
Iteration 64/1000 | Loss: 0.00001563
Iteration 65/1000 | Loss: 0.00001563
Iteration 66/1000 | Loss: 0.00001563
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001562
Iteration 73/1000 | Loss: 0.00001562
Iteration 74/1000 | Loss: 0.00001562
Iteration 75/1000 | Loss: 0.00001562
Iteration 76/1000 | Loss: 0.00001562
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001562
Iteration 81/1000 | Loss: 0.00001562
Iteration 82/1000 | Loss: 0.00001562
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001559
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001558
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001558
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001557
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001557
Iteration 127/1000 | Loss: 0.00001557
Iteration 128/1000 | Loss: 0.00001557
Iteration 129/1000 | Loss: 0.00001557
Iteration 130/1000 | Loss: 0.00001557
Iteration 131/1000 | Loss: 0.00001557
Iteration 132/1000 | Loss: 0.00001557
Iteration 133/1000 | Loss: 0.00001557
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001556
Iteration 136/1000 | Loss: 0.00001556
Iteration 137/1000 | Loss: 0.00001556
Iteration 138/1000 | Loss: 0.00001556
Iteration 139/1000 | Loss: 0.00001556
Iteration 140/1000 | Loss: 0.00001556
Iteration 141/1000 | Loss: 0.00001556
Iteration 142/1000 | Loss: 0.00001556
Iteration 143/1000 | Loss: 0.00001556
Iteration 144/1000 | Loss: 0.00001556
Iteration 145/1000 | Loss: 0.00001556
Iteration 146/1000 | Loss: 0.00001556
Iteration 147/1000 | Loss: 0.00001556
Iteration 148/1000 | Loss: 0.00001555
Iteration 149/1000 | Loss: 0.00001555
Iteration 150/1000 | Loss: 0.00001555
Iteration 151/1000 | Loss: 0.00001555
Iteration 152/1000 | Loss: 0.00001555
Iteration 153/1000 | Loss: 0.00001555
Iteration 154/1000 | Loss: 0.00001555
Iteration 155/1000 | Loss: 0.00001555
Iteration 156/1000 | Loss: 0.00001555
Iteration 157/1000 | Loss: 0.00001555
Iteration 158/1000 | Loss: 0.00001555
Iteration 159/1000 | Loss: 0.00001555
Iteration 160/1000 | Loss: 0.00001555
Iteration 161/1000 | Loss: 0.00001555
Iteration 162/1000 | Loss: 0.00001555
Iteration 163/1000 | Loss: 0.00001554
Iteration 164/1000 | Loss: 0.00001554
Iteration 165/1000 | Loss: 0.00001554
Iteration 166/1000 | Loss: 0.00001554
Iteration 167/1000 | Loss: 0.00001554
Iteration 168/1000 | Loss: 0.00001554
Iteration 169/1000 | Loss: 0.00001554
Iteration 170/1000 | Loss: 0.00001554
Iteration 171/1000 | Loss: 0.00001554
Iteration 172/1000 | Loss: 0.00001554
Iteration 173/1000 | Loss: 0.00001554
Iteration 174/1000 | Loss: 0.00001554
Iteration 175/1000 | Loss: 0.00001554
Iteration 176/1000 | Loss: 0.00001554
Iteration 177/1000 | Loss: 0.00001554
Iteration 178/1000 | Loss: 0.00001554
Iteration 179/1000 | Loss: 0.00001554
Iteration 180/1000 | Loss: 0.00001554
Iteration 181/1000 | Loss: 0.00001553
Iteration 182/1000 | Loss: 0.00001553
Iteration 183/1000 | Loss: 0.00001553
Iteration 184/1000 | Loss: 0.00001553
Iteration 185/1000 | Loss: 0.00001553
Iteration 186/1000 | Loss: 0.00001553
Iteration 187/1000 | Loss: 0.00001553
Iteration 188/1000 | Loss: 0.00001553
Iteration 189/1000 | Loss: 0.00001553
Iteration 190/1000 | Loss: 0.00001553
Iteration 191/1000 | Loss: 0.00001553
Iteration 192/1000 | Loss: 0.00001552
Iteration 193/1000 | Loss: 0.00001552
Iteration 194/1000 | Loss: 0.00001552
Iteration 195/1000 | Loss: 0.00001552
Iteration 196/1000 | Loss: 0.00001552
Iteration 197/1000 | Loss: 0.00001552
Iteration 198/1000 | Loss: 0.00001552
Iteration 199/1000 | Loss: 0.00001552
Iteration 200/1000 | Loss: 0.00001552
Iteration 201/1000 | Loss: 0.00001552
Iteration 202/1000 | Loss: 0.00001552
Iteration 203/1000 | Loss: 0.00001552
Iteration 204/1000 | Loss: 0.00001552
Iteration 205/1000 | Loss: 0.00001552
Iteration 206/1000 | Loss: 0.00001552
Iteration 207/1000 | Loss: 0.00001551
Iteration 208/1000 | Loss: 0.00001551
Iteration 209/1000 | Loss: 0.00001551
Iteration 210/1000 | Loss: 0.00001551
Iteration 211/1000 | Loss: 0.00001551
Iteration 212/1000 | Loss: 0.00001551
Iteration 213/1000 | Loss: 0.00001551
Iteration 214/1000 | Loss: 0.00001551
Iteration 215/1000 | Loss: 0.00001551
Iteration 216/1000 | Loss: 0.00001551
Iteration 217/1000 | Loss: 0.00001551
Iteration 218/1000 | Loss: 0.00001550
Iteration 219/1000 | Loss: 0.00001550
Iteration 220/1000 | Loss: 0.00001550
Iteration 221/1000 | Loss: 0.00001550
Iteration 222/1000 | Loss: 0.00001550
Iteration 223/1000 | Loss: 0.00001550
Iteration 224/1000 | Loss: 0.00001550
Iteration 225/1000 | Loss: 0.00001550
Iteration 226/1000 | Loss: 0.00001550
Iteration 227/1000 | Loss: 0.00001550
Iteration 228/1000 | Loss: 0.00001550
Iteration 229/1000 | Loss: 0.00001550
Iteration 230/1000 | Loss: 0.00001550
Iteration 231/1000 | Loss: 0.00001550
Iteration 232/1000 | Loss: 0.00001550
Iteration 233/1000 | Loss: 0.00001550
Iteration 234/1000 | Loss: 0.00001549
Iteration 235/1000 | Loss: 0.00001549
Iteration 236/1000 | Loss: 0.00001549
Iteration 237/1000 | Loss: 0.00001549
Iteration 238/1000 | Loss: 0.00001549
Iteration 239/1000 | Loss: 0.00001549
Iteration 240/1000 | Loss: 0.00001549
Iteration 241/1000 | Loss: 0.00001549
Iteration 242/1000 | Loss: 0.00001549
Iteration 243/1000 | Loss: 0.00001549
Iteration 244/1000 | Loss: 0.00001549
Iteration 245/1000 | Loss: 0.00001549
Iteration 246/1000 | Loss: 0.00001549
Iteration 247/1000 | Loss: 0.00001549
Iteration 248/1000 | Loss: 0.00001549
Iteration 249/1000 | Loss: 0.00001549
Iteration 250/1000 | Loss: 0.00001548
Iteration 251/1000 | Loss: 0.00001548
Iteration 252/1000 | Loss: 0.00001548
Iteration 253/1000 | Loss: 0.00001548
Iteration 254/1000 | Loss: 0.00001548
Iteration 255/1000 | Loss: 0.00001548
Iteration 256/1000 | Loss: 0.00001548
Iteration 257/1000 | Loss: 0.00001548
Iteration 258/1000 | Loss: 0.00001548
Iteration 259/1000 | Loss: 0.00001548
Iteration 260/1000 | Loss: 0.00001548
Iteration 261/1000 | Loss: 0.00001548
Iteration 262/1000 | Loss: 0.00001548
Iteration 263/1000 | Loss: 0.00001548
Iteration 264/1000 | Loss: 0.00001548
Iteration 265/1000 | Loss: 0.00001548
Iteration 266/1000 | Loss: 0.00001548
Iteration 267/1000 | Loss: 0.00001548
Iteration 268/1000 | Loss: 0.00001548
Iteration 269/1000 | Loss: 0.00001548
Iteration 270/1000 | Loss: 0.00001548
Iteration 271/1000 | Loss: 0.00001548
Iteration 272/1000 | Loss: 0.00001548
Iteration 273/1000 | Loss: 0.00001548
Iteration 274/1000 | Loss: 0.00001548
Iteration 275/1000 | Loss: 0.00001548
Iteration 276/1000 | Loss: 0.00001548
Iteration 277/1000 | Loss: 0.00001548
Iteration 278/1000 | Loss: 0.00001548
Iteration 279/1000 | Loss: 0.00001548
Iteration 280/1000 | Loss: 0.00001548
Iteration 281/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.5478977729799226e-05, 1.5478977729799226e-05, 1.5478977729799226e-05, 1.5478977729799226e-05, 1.5478977729799226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5478977729799226e-05

Optimization complete. Final v2v error: 3.0498173236846924 mm

Highest mean error: 4.882540225982666 mm for frame 60

Lowest mean error: 2.292823314666748 mm for frame 84

Saving results

Total time: 44.29443717002869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863049
Iteration 2/25 | Loss: 0.00160602
Iteration 3/25 | Loss: 0.00131982
Iteration 4/25 | Loss: 0.00130843
Iteration 5/25 | Loss: 0.00130562
Iteration 6/25 | Loss: 0.00130532
Iteration 7/25 | Loss: 0.00130532
Iteration 8/25 | Loss: 0.00130532
Iteration 9/25 | Loss: 0.00130532
Iteration 10/25 | Loss: 0.00130532
Iteration 11/25 | Loss: 0.00130532
Iteration 12/25 | Loss: 0.00130532
Iteration 13/25 | Loss: 0.00130532
Iteration 14/25 | Loss: 0.00130532
Iteration 15/25 | Loss: 0.00130532
Iteration 16/25 | Loss: 0.00130532
Iteration 17/25 | Loss: 0.00130532
Iteration 18/25 | Loss: 0.00130532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013053201837465167, 0.0013053201837465167, 0.0013053201837465167, 0.0013053201837465167, 0.0013053201837465167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013053201837465167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.48831135
Iteration 2/25 | Loss: 0.00101728
Iteration 3/25 | Loss: 0.00101727
Iteration 4/25 | Loss: 0.00101727
Iteration 5/25 | Loss: 0.00101727
Iteration 6/25 | Loss: 0.00101727
Iteration 7/25 | Loss: 0.00101727
Iteration 8/25 | Loss: 0.00101727
Iteration 9/25 | Loss: 0.00101727
Iteration 10/25 | Loss: 0.00101727
Iteration 11/25 | Loss: 0.00101727
Iteration 12/25 | Loss: 0.00101727
Iteration 13/25 | Loss: 0.00101727
Iteration 14/25 | Loss: 0.00101727
Iteration 15/25 | Loss: 0.00101727
Iteration 16/25 | Loss: 0.00101727
Iteration 17/25 | Loss: 0.00101727
Iteration 18/25 | Loss: 0.00101727
Iteration 19/25 | Loss: 0.00101727
Iteration 20/25 | Loss: 0.00101727
Iteration 21/25 | Loss: 0.00101727
Iteration 22/25 | Loss: 0.00101727
Iteration 23/25 | Loss: 0.00101727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010172683978453279, 0.0010172683978453279, 0.0010172683978453279, 0.0010172683978453279, 0.0010172683978453279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010172683978453279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101727
Iteration 2/1000 | Loss: 0.00013826
Iteration 3/1000 | Loss: 0.00009187
Iteration 4/1000 | Loss: 0.00007532
Iteration 5/1000 | Loss: 0.00006582
Iteration 6/1000 | Loss: 0.00006329
Iteration 7/1000 | Loss: 0.00006185
Iteration 8/1000 | Loss: 0.00006066
Iteration 9/1000 | Loss: 0.00005954
Iteration 10/1000 | Loss: 0.00005849
Iteration 11/1000 | Loss: 0.00005775
Iteration 12/1000 | Loss: 0.00005682
Iteration 13/1000 | Loss: 0.00005605
Iteration 14/1000 | Loss: 0.00005501
Iteration 15/1000 | Loss: 0.00005442
Iteration 16/1000 | Loss: 0.00005396
Iteration 17/1000 | Loss: 0.00005355
Iteration 18/1000 | Loss: 0.00005294
Iteration 19/1000 | Loss: 0.00005251
Iteration 20/1000 | Loss: 0.00005231
Iteration 21/1000 | Loss: 0.00005213
Iteration 22/1000 | Loss: 0.00005194
Iteration 23/1000 | Loss: 0.00005175
Iteration 24/1000 | Loss: 0.00005163
Iteration 25/1000 | Loss: 0.00005148
Iteration 26/1000 | Loss: 0.00005145
Iteration 27/1000 | Loss: 0.00005145
Iteration 28/1000 | Loss: 0.00005145
Iteration 29/1000 | Loss: 0.00005145
Iteration 30/1000 | Loss: 0.00005145
Iteration 31/1000 | Loss: 0.00005145
Iteration 32/1000 | Loss: 0.00005144
Iteration 33/1000 | Loss: 0.00005144
Iteration 34/1000 | Loss: 0.00005144
Iteration 35/1000 | Loss: 0.00005144
Iteration 36/1000 | Loss: 0.00005144
Iteration 37/1000 | Loss: 0.00005143
Iteration 38/1000 | Loss: 0.00005143
Iteration 39/1000 | Loss: 0.00005143
Iteration 40/1000 | Loss: 0.00005143
Iteration 41/1000 | Loss: 0.00005142
Iteration 42/1000 | Loss: 0.00005142
Iteration 43/1000 | Loss: 0.00005142
Iteration 44/1000 | Loss: 0.00005142
Iteration 45/1000 | Loss: 0.00005142
Iteration 46/1000 | Loss: 0.00005142
Iteration 47/1000 | Loss: 0.00005142
Iteration 48/1000 | Loss: 0.00005142
Iteration 49/1000 | Loss: 0.00005142
Iteration 50/1000 | Loss: 0.00005141
Iteration 51/1000 | Loss: 0.00005141
Iteration 52/1000 | Loss: 0.00005141
Iteration 53/1000 | Loss: 0.00005140
Iteration 54/1000 | Loss: 0.00005136
Iteration 55/1000 | Loss: 0.00005136
Iteration 56/1000 | Loss: 0.00005135
Iteration 57/1000 | Loss: 0.00005135
Iteration 58/1000 | Loss: 0.00005135
Iteration 59/1000 | Loss: 0.00005135
Iteration 60/1000 | Loss: 0.00005135
Iteration 61/1000 | Loss: 0.00005135
Iteration 62/1000 | Loss: 0.00005135
Iteration 63/1000 | Loss: 0.00005135
Iteration 64/1000 | Loss: 0.00005135
Iteration 65/1000 | Loss: 0.00005135
Iteration 66/1000 | Loss: 0.00005134
Iteration 67/1000 | Loss: 0.00005134
Iteration 68/1000 | Loss: 0.00005134
Iteration 69/1000 | Loss: 0.00005134
Iteration 70/1000 | Loss: 0.00005134
Iteration 71/1000 | Loss: 0.00005134
Iteration 72/1000 | Loss: 0.00005134
Iteration 73/1000 | Loss: 0.00005133
Iteration 74/1000 | Loss: 0.00005133
Iteration 75/1000 | Loss: 0.00005133
Iteration 76/1000 | Loss: 0.00005133
Iteration 77/1000 | Loss: 0.00005133
Iteration 78/1000 | Loss: 0.00005133
Iteration 79/1000 | Loss: 0.00005132
Iteration 80/1000 | Loss: 0.00005132
Iteration 81/1000 | Loss: 0.00005132
Iteration 82/1000 | Loss: 0.00005132
Iteration 83/1000 | Loss: 0.00005132
Iteration 84/1000 | Loss: 0.00005132
Iteration 85/1000 | Loss: 0.00005131
Iteration 86/1000 | Loss: 0.00005131
Iteration 87/1000 | Loss: 0.00005131
Iteration 88/1000 | Loss: 0.00005130
Iteration 89/1000 | Loss: 0.00005130
Iteration 90/1000 | Loss: 0.00005130
Iteration 91/1000 | Loss: 0.00005130
Iteration 92/1000 | Loss: 0.00005130
Iteration 93/1000 | Loss: 0.00005130
Iteration 94/1000 | Loss: 0.00005130
Iteration 95/1000 | Loss: 0.00005129
Iteration 96/1000 | Loss: 0.00005129
Iteration 97/1000 | Loss: 0.00005129
Iteration 98/1000 | Loss: 0.00005129
Iteration 99/1000 | Loss: 0.00005129
Iteration 100/1000 | Loss: 0.00005129
Iteration 101/1000 | Loss: 0.00005129
Iteration 102/1000 | Loss: 0.00005129
Iteration 103/1000 | Loss: 0.00005129
Iteration 104/1000 | Loss: 0.00005129
Iteration 105/1000 | Loss: 0.00005129
Iteration 106/1000 | Loss: 0.00005129
Iteration 107/1000 | Loss: 0.00005129
Iteration 108/1000 | Loss: 0.00005129
Iteration 109/1000 | Loss: 0.00005129
Iteration 110/1000 | Loss: 0.00005129
Iteration 111/1000 | Loss: 0.00005129
Iteration 112/1000 | Loss: 0.00005129
Iteration 113/1000 | Loss: 0.00005129
Iteration 114/1000 | Loss: 0.00005129
Iteration 115/1000 | Loss: 0.00005129
Iteration 116/1000 | Loss: 0.00005129
Iteration 117/1000 | Loss: 0.00005129
Iteration 118/1000 | Loss: 0.00005129
Iteration 119/1000 | Loss: 0.00005129
Iteration 120/1000 | Loss: 0.00005129
Iteration 121/1000 | Loss: 0.00005129
Iteration 122/1000 | Loss: 0.00005129
Iteration 123/1000 | Loss: 0.00005129
Iteration 124/1000 | Loss: 0.00005129
Iteration 125/1000 | Loss: 0.00005129
Iteration 126/1000 | Loss: 0.00005129
Iteration 127/1000 | Loss: 0.00005129
Iteration 128/1000 | Loss: 0.00005129
Iteration 129/1000 | Loss: 0.00005129
Iteration 130/1000 | Loss: 0.00005129
Iteration 131/1000 | Loss: 0.00005129
Iteration 132/1000 | Loss: 0.00005129
Iteration 133/1000 | Loss: 0.00005129
Iteration 134/1000 | Loss: 0.00005129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [5.1287566748214886e-05, 5.1287566748214886e-05, 5.1287566748214886e-05, 5.1287566748214886e-05, 5.1287566748214886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.1287566748214886e-05

Optimization complete. Final v2v error: 5.736072063446045 mm

Highest mean error: 7.205903053283691 mm for frame 143

Lowest mean error: 5.392215728759766 mm for frame 53

Saving results

Total time: 50.74170517921448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823578
Iteration 2/25 | Loss: 0.00110619
Iteration 3/25 | Loss: 0.00100827
Iteration 4/25 | Loss: 0.00100235
Iteration 5/25 | Loss: 0.00100151
Iteration 6/25 | Loss: 0.00100151
Iteration 7/25 | Loss: 0.00100151
Iteration 8/25 | Loss: 0.00100151
Iteration 9/25 | Loss: 0.00100151
Iteration 10/25 | Loss: 0.00100151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001001507043838501, 0.001001507043838501, 0.001001507043838501, 0.001001507043838501, 0.001001507043838501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001001507043838501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29530787
Iteration 2/25 | Loss: 0.00155586
Iteration 3/25 | Loss: 0.00155585
Iteration 4/25 | Loss: 0.00155585
Iteration 5/25 | Loss: 0.00155585
Iteration 6/25 | Loss: 0.00155585
Iteration 7/25 | Loss: 0.00155585
Iteration 8/25 | Loss: 0.00155585
Iteration 9/25 | Loss: 0.00155585
Iteration 10/25 | Loss: 0.00155585
Iteration 11/25 | Loss: 0.00155585
Iteration 12/25 | Loss: 0.00155585
Iteration 13/25 | Loss: 0.00155585
Iteration 14/25 | Loss: 0.00155585
Iteration 15/25 | Loss: 0.00155585
Iteration 16/25 | Loss: 0.00155585
Iteration 17/25 | Loss: 0.00155585
Iteration 18/25 | Loss: 0.00155585
Iteration 19/25 | Loss: 0.00155585
Iteration 20/25 | Loss: 0.00155585
Iteration 21/25 | Loss: 0.00155585
Iteration 22/25 | Loss: 0.00155585
Iteration 23/25 | Loss: 0.00155585
Iteration 24/25 | Loss: 0.00155585
Iteration 25/25 | Loss: 0.00155585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155585
Iteration 2/1000 | Loss: 0.00001806
Iteration 3/1000 | Loss: 0.00001268
Iteration 4/1000 | Loss: 0.00001145
Iteration 5/1000 | Loss: 0.00001062
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00000959
Iteration 8/1000 | Loss: 0.00000927
Iteration 9/1000 | Loss: 0.00000905
Iteration 10/1000 | Loss: 0.00000889
Iteration 11/1000 | Loss: 0.00000888
Iteration 12/1000 | Loss: 0.00000885
Iteration 13/1000 | Loss: 0.00000885
Iteration 14/1000 | Loss: 0.00000884
Iteration 15/1000 | Loss: 0.00000883
Iteration 16/1000 | Loss: 0.00000880
Iteration 17/1000 | Loss: 0.00000880
Iteration 18/1000 | Loss: 0.00000880
Iteration 19/1000 | Loss: 0.00000879
Iteration 20/1000 | Loss: 0.00000879
Iteration 21/1000 | Loss: 0.00000878
Iteration 22/1000 | Loss: 0.00000877
Iteration 23/1000 | Loss: 0.00000877
Iteration 24/1000 | Loss: 0.00000876
Iteration 25/1000 | Loss: 0.00000875
Iteration 26/1000 | Loss: 0.00000874
Iteration 27/1000 | Loss: 0.00000873
Iteration 28/1000 | Loss: 0.00000871
Iteration 29/1000 | Loss: 0.00000870
Iteration 30/1000 | Loss: 0.00000870
Iteration 31/1000 | Loss: 0.00000870
Iteration 32/1000 | Loss: 0.00000869
Iteration 33/1000 | Loss: 0.00000869
Iteration 34/1000 | Loss: 0.00000868
Iteration 35/1000 | Loss: 0.00000868
Iteration 36/1000 | Loss: 0.00000867
Iteration 37/1000 | Loss: 0.00000867
Iteration 38/1000 | Loss: 0.00000866
Iteration 39/1000 | Loss: 0.00000866
Iteration 40/1000 | Loss: 0.00000865
Iteration 41/1000 | Loss: 0.00000865
Iteration 42/1000 | Loss: 0.00000865
Iteration 43/1000 | Loss: 0.00000864
Iteration 44/1000 | Loss: 0.00000864
Iteration 45/1000 | Loss: 0.00000864
Iteration 46/1000 | Loss: 0.00000863
Iteration 47/1000 | Loss: 0.00000863
Iteration 48/1000 | Loss: 0.00000862
Iteration 49/1000 | Loss: 0.00000861
Iteration 50/1000 | Loss: 0.00000861
Iteration 51/1000 | Loss: 0.00000861
Iteration 52/1000 | Loss: 0.00000861
Iteration 53/1000 | Loss: 0.00000861
Iteration 54/1000 | Loss: 0.00000861
Iteration 55/1000 | Loss: 0.00000861
Iteration 56/1000 | Loss: 0.00000861
Iteration 57/1000 | Loss: 0.00000861
Iteration 58/1000 | Loss: 0.00000860
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000860
Iteration 61/1000 | Loss: 0.00000860
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000859
Iteration 64/1000 | Loss: 0.00000858
Iteration 65/1000 | Loss: 0.00000858
Iteration 66/1000 | Loss: 0.00000858
Iteration 67/1000 | Loss: 0.00000858
Iteration 68/1000 | Loss: 0.00000857
Iteration 69/1000 | Loss: 0.00000857
Iteration 70/1000 | Loss: 0.00000857
Iteration 71/1000 | Loss: 0.00000857
Iteration 72/1000 | Loss: 0.00000856
Iteration 73/1000 | Loss: 0.00000856
Iteration 74/1000 | Loss: 0.00000856
Iteration 75/1000 | Loss: 0.00000856
Iteration 76/1000 | Loss: 0.00000856
Iteration 77/1000 | Loss: 0.00000856
Iteration 78/1000 | Loss: 0.00000856
Iteration 79/1000 | Loss: 0.00000856
Iteration 80/1000 | Loss: 0.00000856
Iteration 81/1000 | Loss: 0.00000856
Iteration 82/1000 | Loss: 0.00000856
Iteration 83/1000 | Loss: 0.00000856
Iteration 84/1000 | Loss: 0.00000856
Iteration 85/1000 | Loss: 0.00000856
Iteration 86/1000 | Loss: 0.00000856
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000855
Iteration 89/1000 | Loss: 0.00000855
Iteration 90/1000 | Loss: 0.00000855
Iteration 91/1000 | Loss: 0.00000854
Iteration 92/1000 | Loss: 0.00000854
Iteration 93/1000 | Loss: 0.00000854
Iteration 94/1000 | Loss: 0.00000854
Iteration 95/1000 | Loss: 0.00000854
Iteration 96/1000 | Loss: 0.00000854
Iteration 97/1000 | Loss: 0.00000854
Iteration 98/1000 | Loss: 0.00000854
Iteration 99/1000 | Loss: 0.00000854
Iteration 100/1000 | Loss: 0.00000854
Iteration 101/1000 | Loss: 0.00000854
Iteration 102/1000 | Loss: 0.00000854
Iteration 103/1000 | Loss: 0.00000854
Iteration 104/1000 | Loss: 0.00000854
Iteration 105/1000 | Loss: 0.00000853
Iteration 106/1000 | Loss: 0.00000853
Iteration 107/1000 | Loss: 0.00000853
Iteration 108/1000 | Loss: 0.00000853
Iteration 109/1000 | Loss: 0.00000853
Iteration 110/1000 | Loss: 0.00000853
Iteration 111/1000 | Loss: 0.00000853
Iteration 112/1000 | Loss: 0.00000853
Iteration 113/1000 | Loss: 0.00000853
Iteration 114/1000 | Loss: 0.00000852
Iteration 115/1000 | Loss: 0.00000852
Iteration 116/1000 | Loss: 0.00000852
Iteration 117/1000 | Loss: 0.00000852
Iteration 118/1000 | Loss: 0.00000852
Iteration 119/1000 | Loss: 0.00000852
Iteration 120/1000 | Loss: 0.00000852
Iteration 121/1000 | Loss: 0.00000852
Iteration 122/1000 | Loss: 0.00000852
Iteration 123/1000 | Loss: 0.00000852
Iteration 124/1000 | Loss: 0.00000852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [8.520982191839721e-06, 8.520982191839721e-06, 8.520982191839721e-06, 8.520982191839721e-06, 8.520982191839721e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.520982191839721e-06

Optimization complete. Final v2v error: 2.472073793411255 mm

Highest mean error: 2.864757776260376 mm for frame 98

Lowest mean error: 2.0144777297973633 mm for frame 222

Saving results

Total time: 34.1133930683136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394628
Iteration 2/25 | Loss: 0.00112236
Iteration 3/25 | Loss: 0.00100982
Iteration 4/25 | Loss: 0.00099890
Iteration 5/25 | Loss: 0.00099715
Iteration 6/25 | Loss: 0.00099715
Iteration 7/25 | Loss: 0.00099715
Iteration 8/25 | Loss: 0.00099715
Iteration 9/25 | Loss: 0.00099715
Iteration 10/25 | Loss: 0.00099715
Iteration 11/25 | Loss: 0.00099715
Iteration 12/25 | Loss: 0.00099715
Iteration 13/25 | Loss: 0.00099715
Iteration 14/25 | Loss: 0.00099715
Iteration 15/25 | Loss: 0.00099715
Iteration 16/25 | Loss: 0.00099715
Iteration 17/25 | Loss: 0.00099715
Iteration 18/25 | Loss: 0.00099715
Iteration 19/25 | Loss: 0.00099715
Iteration 20/25 | Loss: 0.00099715
Iteration 21/25 | Loss: 0.00099715
Iteration 22/25 | Loss: 0.00099715
Iteration 23/25 | Loss: 0.00099715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009971547406166792, 0.0009971547406166792, 0.0009971547406166792, 0.0009971547406166792, 0.0009971547406166792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009971547406166792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47633278
Iteration 2/25 | Loss: 0.00123295
Iteration 3/25 | Loss: 0.00123295
Iteration 4/25 | Loss: 0.00123295
Iteration 5/25 | Loss: 0.00123295
Iteration 6/25 | Loss: 0.00123295
Iteration 7/25 | Loss: 0.00123295
Iteration 8/25 | Loss: 0.00123295
Iteration 9/25 | Loss: 0.00123295
Iteration 10/25 | Loss: 0.00123295
Iteration 11/25 | Loss: 0.00123295
Iteration 12/25 | Loss: 0.00123295
Iteration 13/25 | Loss: 0.00123295
Iteration 14/25 | Loss: 0.00123295
Iteration 15/25 | Loss: 0.00123295
Iteration 16/25 | Loss: 0.00123295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012329485034570098, 0.0012329485034570098, 0.0012329485034570098, 0.0012329485034570098, 0.0012329485034570098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012329485034570098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123295
Iteration 2/1000 | Loss: 0.00003233
Iteration 3/1000 | Loss: 0.00001450
Iteration 4/1000 | Loss: 0.00001141
Iteration 5/1000 | Loss: 0.00001050
Iteration 6/1000 | Loss: 0.00000977
Iteration 7/1000 | Loss: 0.00000953
Iteration 8/1000 | Loss: 0.00000938
Iteration 9/1000 | Loss: 0.00000911
Iteration 10/1000 | Loss: 0.00000911
Iteration 11/1000 | Loss: 0.00000889
Iteration 12/1000 | Loss: 0.00000889
Iteration 13/1000 | Loss: 0.00000885
Iteration 14/1000 | Loss: 0.00000877
Iteration 15/1000 | Loss: 0.00000875
Iteration 16/1000 | Loss: 0.00000875
Iteration 17/1000 | Loss: 0.00000875
Iteration 18/1000 | Loss: 0.00000875
Iteration 19/1000 | Loss: 0.00000874
Iteration 20/1000 | Loss: 0.00000874
Iteration 21/1000 | Loss: 0.00000874
Iteration 22/1000 | Loss: 0.00000874
Iteration 23/1000 | Loss: 0.00000874
Iteration 24/1000 | Loss: 0.00000874
Iteration 25/1000 | Loss: 0.00000874
Iteration 26/1000 | Loss: 0.00000874
Iteration 27/1000 | Loss: 0.00000873
Iteration 28/1000 | Loss: 0.00000868
Iteration 29/1000 | Loss: 0.00000868
Iteration 30/1000 | Loss: 0.00000867
Iteration 31/1000 | Loss: 0.00000867
Iteration 32/1000 | Loss: 0.00000866
Iteration 33/1000 | Loss: 0.00000866
Iteration 34/1000 | Loss: 0.00000866
Iteration 35/1000 | Loss: 0.00000865
Iteration 36/1000 | Loss: 0.00000865
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000864
Iteration 39/1000 | Loss: 0.00000864
Iteration 40/1000 | Loss: 0.00000863
Iteration 41/1000 | Loss: 0.00000863
Iteration 42/1000 | Loss: 0.00000862
Iteration 43/1000 | Loss: 0.00000861
Iteration 44/1000 | Loss: 0.00000860
Iteration 45/1000 | Loss: 0.00000860
Iteration 46/1000 | Loss: 0.00000860
Iteration 47/1000 | Loss: 0.00000859
Iteration 48/1000 | Loss: 0.00000859
Iteration 49/1000 | Loss: 0.00000858
Iteration 50/1000 | Loss: 0.00000858
Iteration 51/1000 | Loss: 0.00000858
Iteration 52/1000 | Loss: 0.00000858
Iteration 53/1000 | Loss: 0.00000857
Iteration 54/1000 | Loss: 0.00000857
Iteration 55/1000 | Loss: 0.00000857
Iteration 56/1000 | Loss: 0.00000856
Iteration 57/1000 | Loss: 0.00000856
Iteration 58/1000 | Loss: 0.00000855
Iteration 59/1000 | Loss: 0.00000855
Iteration 60/1000 | Loss: 0.00000855
Iteration 61/1000 | Loss: 0.00000855
Iteration 62/1000 | Loss: 0.00000855
Iteration 63/1000 | Loss: 0.00000855
Iteration 64/1000 | Loss: 0.00000854
Iteration 65/1000 | Loss: 0.00000854
Iteration 66/1000 | Loss: 0.00000854
Iteration 67/1000 | Loss: 0.00000854
Iteration 68/1000 | Loss: 0.00000854
Iteration 69/1000 | Loss: 0.00000851
Iteration 70/1000 | Loss: 0.00000850
Iteration 71/1000 | Loss: 0.00000850
Iteration 72/1000 | Loss: 0.00000850
Iteration 73/1000 | Loss: 0.00000850
Iteration 74/1000 | Loss: 0.00000850
Iteration 75/1000 | Loss: 0.00000849
Iteration 76/1000 | Loss: 0.00000849
Iteration 77/1000 | Loss: 0.00000849
Iteration 78/1000 | Loss: 0.00000849
Iteration 79/1000 | Loss: 0.00000848
Iteration 80/1000 | Loss: 0.00000848
Iteration 81/1000 | Loss: 0.00000848
Iteration 82/1000 | Loss: 0.00000848
Iteration 83/1000 | Loss: 0.00000848
Iteration 84/1000 | Loss: 0.00000848
Iteration 85/1000 | Loss: 0.00000848
Iteration 86/1000 | Loss: 0.00000848
Iteration 87/1000 | Loss: 0.00000848
Iteration 88/1000 | Loss: 0.00000848
Iteration 89/1000 | Loss: 0.00000847
Iteration 90/1000 | Loss: 0.00000847
Iteration 91/1000 | Loss: 0.00000847
Iteration 92/1000 | Loss: 0.00000846
Iteration 93/1000 | Loss: 0.00000846
Iteration 94/1000 | Loss: 0.00000846
Iteration 95/1000 | Loss: 0.00000846
Iteration 96/1000 | Loss: 0.00000845
Iteration 97/1000 | Loss: 0.00000845
Iteration 98/1000 | Loss: 0.00000845
Iteration 99/1000 | Loss: 0.00000844
Iteration 100/1000 | Loss: 0.00000844
Iteration 101/1000 | Loss: 0.00000844
Iteration 102/1000 | Loss: 0.00000844
Iteration 103/1000 | Loss: 0.00000844
Iteration 104/1000 | Loss: 0.00000844
Iteration 105/1000 | Loss: 0.00000844
Iteration 106/1000 | Loss: 0.00000844
Iteration 107/1000 | Loss: 0.00000844
Iteration 108/1000 | Loss: 0.00000844
Iteration 109/1000 | Loss: 0.00000844
Iteration 110/1000 | Loss: 0.00000844
Iteration 111/1000 | Loss: 0.00000844
Iteration 112/1000 | Loss: 0.00000844
Iteration 113/1000 | Loss: 0.00000844
Iteration 114/1000 | Loss: 0.00000844
Iteration 115/1000 | Loss: 0.00000844
Iteration 116/1000 | Loss: 0.00000844
Iteration 117/1000 | Loss: 0.00000844
Iteration 118/1000 | Loss: 0.00000844
Iteration 119/1000 | Loss: 0.00000844
Iteration 120/1000 | Loss: 0.00000844
Iteration 121/1000 | Loss: 0.00000844
Iteration 122/1000 | Loss: 0.00000844
Iteration 123/1000 | Loss: 0.00000844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [8.439443263341673e-06, 8.439443263341673e-06, 8.439443263341673e-06, 8.439443263341673e-06, 8.439443263341673e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.439443263341673e-06

Optimization complete. Final v2v error: 2.4640846252441406 mm

Highest mean error: 2.723428964614868 mm for frame 15

Lowest mean error: 2.235776662826538 mm for frame 124

Saving results

Total time: 35.302042961120605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398106
Iteration 2/25 | Loss: 0.00111912
Iteration 3/25 | Loss: 0.00098381
Iteration 4/25 | Loss: 0.00097282
Iteration 5/25 | Loss: 0.00097095
Iteration 6/25 | Loss: 0.00097055
Iteration 7/25 | Loss: 0.00097055
Iteration 8/25 | Loss: 0.00097055
Iteration 9/25 | Loss: 0.00097055
Iteration 10/25 | Loss: 0.00097055
Iteration 11/25 | Loss: 0.00097055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009705525590106845, 0.0009705525590106845, 0.0009705525590106845, 0.0009705525590106845, 0.0009705525590106845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009705525590106845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30073893
Iteration 2/25 | Loss: 0.00154615
Iteration 3/25 | Loss: 0.00154615
Iteration 4/25 | Loss: 0.00154615
Iteration 5/25 | Loss: 0.00154614
Iteration 6/25 | Loss: 0.00154614
Iteration 7/25 | Loss: 0.00154614
Iteration 8/25 | Loss: 0.00154614
Iteration 9/25 | Loss: 0.00154614
Iteration 10/25 | Loss: 0.00154614
Iteration 11/25 | Loss: 0.00154614
Iteration 12/25 | Loss: 0.00154614
Iteration 13/25 | Loss: 0.00154614
Iteration 14/25 | Loss: 0.00154614
Iteration 15/25 | Loss: 0.00154614
Iteration 16/25 | Loss: 0.00154614
Iteration 17/25 | Loss: 0.00154614
Iteration 18/25 | Loss: 0.00154614
Iteration 19/25 | Loss: 0.00154614
Iteration 20/25 | Loss: 0.00154614
Iteration 21/25 | Loss: 0.00154614
Iteration 22/25 | Loss: 0.00154614
Iteration 23/25 | Loss: 0.00154614
Iteration 24/25 | Loss: 0.00154614
Iteration 25/25 | Loss: 0.00154614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154614
Iteration 2/1000 | Loss: 0.00002629
Iteration 3/1000 | Loss: 0.00001279
Iteration 4/1000 | Loss: 0.00001060
Iteration 5/1000 | Loss: 0.00001002
Iteration 6/1000 | Loss: 0.00000958
Iteration 7/1000 | Loss: 0.00000923
Iteration 8/1000 | Loss: 0.00000904
Iteration 9/1000 | Loss: 0.00000894
Iteration 10/1000 | Loss: 0.00000894
Iteration 11/1000 | Loss: 0.00000894
Iteration 12/1000 | Loss: 0.00000894
Iteration 13/1000 | Loss: 0.00000894
Iteration 14/1000 | Loss: 0.00000894
Iteration 15/1000 | Loss: 0.00000890
Iteration 16/1000 | Loss: 0.00000886
Iteration 17/1000 | Loss: 0.00000885
Iteration 18/1000 | Loss: 0.00000885
Iteration 19/1000 | Loss: 0.00000884
Iteration 20/1000 | Loss: 0.00000880
Iteration 21/1000 | Loss: 0.00000877
Iteration 22/1000 | Loss: 0.00000877
Iteration 23/1000 | Loss: 0.00000874
Iteration 24/1000 | Loss: 0.00000874
Iteration 25/1000 | Loss: 0.00000873
Iteration 26/1000 | Loss: 0.00000873
Iteration 27/1000 | Loss: 0.00000872
Iteration 28/1000 | Loss: 0.00000872
Iteration 29/1000 | Loss: 0.00000872
Iteration 30/1000 | Loss: 0.00000872
Iteration 31/1000 | Loss: 0.00000872
Iteration 32/1000 | Loss: 0.00000871
Iteration 33/1000 | Loss: 0.00000871
Iteration 34/1000 | Loss: 0.00000871
Iteration 35/1000 | Loss: 0.00000871
Iteration 36/1000 | Loss: 0.00000871
Iteration 37/1000 | Loss: 0.00000871
Iteration 38/1000 | Loss: 0.00000871
Iteration 39/1000 | Loss: 0.00000871
Iteration 40/1000 | Loss: 0.00000870
Iteration 41/1000 | Loss: 0.00000870
Iteration 42/1000 | Loss: 0.00000870
Iteration 43/1000 | Loss: 0.00000870
Iteration 44/1000 | Loss: 0.00000869
Iteration 45/1000 | Loss: 0.00000869
Iteration 46/1000 | Loss: 0.00000869
Iteration 47/1000 | Loss: 0.00000868
Iteration 48/1000 | Loss: 0.00000868
Iteration 49/1000 | Loss: 0.00000867
Iteration 50/1000 | Loss: 0.00000866
Iteration 51/1000 | Loss: 0.00000866
Iteration 52/1000 | Loss: 0.00000866
Iteration 53/1000 | Loss: 0.00000866
Iteration 54/1000 | Loss: 0.00000866
Iteration 55/1000 | Loss: 0.00000865
Iteration 56/1000 | Loss: 0.00000865
Iteration 57/1000 | Loss: 0.00000865
Iteration 58/1000 | Loss: 0.00000865
Iteration 59/1000 | Loss: 0.00000864
Iteration 60/1000 | Loss: 0.00000864
Iteration 61/1000 | Loss: 0.00000864
Iteration 62/1000 | Loss: 0.00000863
Iteration 63/1000 | Loss: 0.00000863
Iteration 64/1000 | Loss: 0.00000863
Iteration 65/1000 | Loss: 0.00000863
Iteration 66/1000 | Loss: 0.00000863
Iteration 67/1000 | Loss: 0.00000863
Iteration 68/1000 | Loss: 0.00000863
Iteration 69/1000 | Loss: 0.00000863
Iteration 70/1000 | Loss: 0.00000862
Iteration 71/1000 | Loss: 0.00000862
Iteration 72/1000 | Loss: 0.00000862
Iteration 73/1000 | Loss: 0.00000861
Iteration 74/1000 | Loss: 0.00000861
Iteration 75/1000 | Loss: 0.00000860
Iteration 76/1000 | Loss: 0.00000860
Iteration 77/1000 | Loss: 0.00000860
Iteration 78/1000 | Loss: 0.00000859
Iteration 79/1000 | Loss: 0.00000859
Iteration 80/1000 | Loss: 0.00000859
Iteration 81/1000 | Loss: 0.00000859
Iteration 82/1000 | Loss: 0.00000858
Iteration 83/1000 | Loss: 0.00000858
Iteration 84/1000 | Loss: 0.00000858
Iteration 85/1000 | Loss: 0.00000858
Iteration 86/1000 | Loss: 0.00000858
Iteration 87/1000 | Loss: 0.00000858
Iteration 88/1000 | Loss: 0.00000858
Iteration 89/1000 | Loss: 0.00000858
Iteration 90/1000 | Loss: 0.00000858
Iteration 91/1000 | Loss: 0.00000858
Iteration 92/1000 | Loss: 0.00000857
Iteration 93/1000 | Loss: 0.00000857
Iteration 94/1000 | Loss: 0.00000857
Iteration 95/1000 | Loss: 0.00000857
Iteration 96/1000 | Loss: 0.00000857
Iteration 97/1000 | Loss: 0.00000857
Iteration 98/1000 | Loss: 0.00000857
Iteration 99/1000 | Loss: 0.00000857
Iteration 100/1000 | Loss: 0.00000856
Iteration 101/1000 | Loss: 0.00000856
Iteration 102/1000 | Loss: 0.00000856
Iteration 103/1000 | Loss: 0.00000856
Iteration 104/1000 | Loss: 0.00000856
Iteration 105/1000 | Loss: 0.00000856
Iteration 106/1000 | Loss: 0.00000856
Iteration 107/1000 | Loss: 0.00000856
Iteration 108/1000 | Loss: 0.00000855
Iteration 109/1000 | Loss: 0.00000855
Iteration 110/1000 | Loss: 0.00000855
Iteration 111/1000 | Loss: 0.00000855
Iteration 112/1000 | Loss: 0.00000855
Iteration 113/1000 | Loss: 0.00000855
Iteration 114/1000 | Loss: 0.00000855
Iteration 115/1000 | Loss: 0.00000855
Iteration 116/1000 | Loss: 0.00000855
Iteration 117/1000 | Loss: 0.00000855
Iteration 118/1000 | Loss: 0.00000855
Iteration 119/1000 | Loss: 0.00000855
Iteration 120/1000 | Loss: 0.00000855
Iteration 121/1000 | Loss: 0.00000855
Iteration 122/1000 | Loss: 0.00000855
Iteration 123/1000 | Loss: 0.00000855
Iteration 124/1000 | Loss: 0.00000855
Iteration 125/1000 | Loss: 0.00000855
Iteration 126/1000 | Loss: 0.00000855
Iteration 127/1000 | Loss: 0.00000855
Iteration 128/1000 | Loss: 0.00000855
Iteration 129/1000 | Loss: 0.00000855
Iteration 130/1000 | Loss: 0.00000855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [8.547282050130889e-06, 8.547282050130889e-06, 8.547282050130889e-06, 8.547282050130889e-06, 8.547282050130889e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.547282050130889e-06

Optimization complete. Final v2v error: 2.4505627155303955 mm

Highest mean error: 3.4929697513580322 mm for frame 54

Lowest mean error: 2.1486451625823975 mm for frame 2

Saving results

Total time: 30.66669535636902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118610
Iteration 2/25 | Loss: 0.01118610
Iteration 3/25 | Loss: 0.01118609
Iteration 4/25 | Loss: 0.01118609
Iteration 5/25 | Loss: 0.01118609
Iteration 6/25 | Loss: 0.01118609
Iteration 7/25 | Loss: 0.01118609
Iteration 8/25 | Loss: 0.01118609
Iteration 9/25 | Loss: 0.01118609
Iteration 10/25 | Loss: 0.01118609
Iteration 11/25 | Loss: 0.01118609
Iteration 12/25 | Loss: 0.01118608
Iteration 13/25 | Loss: 0.01118608
Iteration 14/25 | Loss: 0.01118608
Iteration 15/25 | Loss: 0.01118608
Iteration 16/25 | Loss: 0.01118608
Iteration 17/25 | Loss: 0.01118608
Iteration 18/25 | Loss: 0.01118608
Iteration 19/25 | Loss: 0.01118608
Iteration 20/25 | Loss: 0.01118608
Iteration 21/25 | Loss: 0.01118608
Iteration 22/25 | Loss: 0.01118608
Iteration 23/25 | Loss: 0.01118608
Iteration 24/25 | Loss: 0.01118608
Iteration 25/25 | Loss: 0.01118608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.22403431
Iteration 2/25 | Loss: 0.16230901
Iteration 3/25 | Loss: 0.16038311
Iteration 4/25 | Loss: 0.16014571
Iteration 5/25 | Loss: 0.16014373
Iteration 6/25 | Loss: 0.16014372
Iteration 7/25 | Loss: 0.16014370
Iteration 8/25 | Loss: 0.16014370
Iteration 9/25 | Loss: 0.16014370
Iteration 10/25 | Loss: 0.16014370
Iteration 11/25 | Loss: 0.16014370
Iteration 12/25 | Loss: 0.16014367
Iteration 13/25 | Loss: 0.16014367
Iteration 14/25 | Loss: 0.16014367
Iteration 15/25 | Loss: 0.16014367
Iteration 16/25 | Loss: 0.16014367
Iteration 17/25 | Loss: 0.16014367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.1601436734199524, 0.1601436734199524, 0.1601436734199524, 0.1601436734199524, 0.1601436734199524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1601436734199524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16014367
Iteration 2/1000 | Loss: 0.00508438
Iteration 3/1000 | Loss: 0.00072891
Iteration 4/1000 | Loss: 0.00057465
Iteration 5/1000 | Loss: 0.00073940
Iteration 6/1000 | Loss: 0.00026196
Iteration 7/1000 | Loss: 0.00026791
Iteration 8/1000 | Loss: 0.00002765
Iteration 9/1000 | Loss: 0.00014590
Iteration 10/1000 | Loss: 0.00002469
Iteration 11/1000 | Loss: 0.00022039
Iteration 12/1000 | Loss: 0.00002105
Iteration 13/1000 | Loss: 0.00006210
Iteration 14/1000 | Loss: 0.00011882
Iteration 15/1000 | Loss: 0.00001721
Iteration 16/1000 | Loss: 0.00001586
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00018717
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001476
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001157
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001149
Iteration 32/1000 | Loss: 0.00001149
Iteration 33/1000 | Loss: 0.00001149
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001139
Iteration 36/1000 | Loss: 0.00001139
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001138
Iteration 39/1000 | Loss: 0.00001138
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001134
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001130
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001129
Iteration 62/1000 | Loss: 0.00001129
Iteration 63/1000 | Loss: 0.00001128
Iteration 64/1000 | Loss: 0.00001128
Iteration 65/1000 | Loss: 0.00001127
Iteration 66/1000 | Loss: 0.00001127
Iteration 67/1000 | Loss: 0.00001127
Iteration 68/1000 | Loss: 0.00001127
Iteration 69/1000 | Loss: 0.00001126
Iteration 70/1000 | Loss: 0.00001126
Iteration 71/1000 | Loss: 0.00001126
Iteration 72/1000 | Loss: 0.00001126
Iteration 73/1000 | Loss: 0.00001125
Iteration 74/1000 | Loss: 0.00001125
Iteration 75/1000 | Loss: 0.00001125
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001135
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001121
Iteration 84/1000 | Loss: 0.00001121
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001120
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001120
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001122
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001120
Iteration 97/1000 | Loss: 0.00001120
Iteration 98/1000 | Loss: 0.00001120
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001117
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001115
Iteration 107/1000 | Loss: 0.00001115
Iteration 108/1000 | Loss: 0.00001114
Iteration 109/1000 | Loss: 0.00001114
Iteration 110/1000 | Loss: 0.00001114
Iteration 111/1000 | Loss: 0.00001114
Iteration 112/1000 | Loss: 0.00001113
Iteration 113/1000 | Loss: 0.00001113
Iteration 114/1000 | Loss: 0.00001113
Iteration 115/1000 | Loss: 0.00001113
Iteration 116/1000 | Loss: 0.00001113
Iteration 117/1000 | Loss: 0.00001113
Iteration 118/1000 | Loss: 0.00001112
Iteration 119/1000 | Loss: 0.00001112
Iteration 120/1000 | Loss: 0.00001112
Iteration 121/1000 | Loss: 0.00001112
Iteration 122/1000 | Loss: 0.00001112
Iteration 123/1000 | Loss: 0.00001112
Iteration 124/1000 | Loss: 0.00001112
Iteration 125/1000 | Loss: 0.00001112
Iteration 126/1000 | Loss: 0.00001112
Iteration 127/1000 | Loss: 0.00001111
Iteration 128/1000 | Loss: 0.00001111
Iteration 129/1000 | Loss: 0.00001111
Iteration 130/1000 | Loss: 0.00001111
Iteration 131/1000 | Loss: 0.00001111
Iteration 132/1000 | Loss: 0.00001111
Iteration 133/1000 | Loss: 0.00001111
Iteration 134/1000 | Loss: 0.00001110
Iteration 135/1000 | Loss: 0.00001110
Iteration 136/1000 | Loss: 0.00001110
Iteration 137/1000 | Loss: 0.00001110
Iteration 138/1000 | Loss: 0.00001110
Iteration 139/1000 | Loss: 0.00001110
Iteration 140/1000 | Loss: 0.00001110
Iteration 141/1000 | Loss: 0.00001110
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001110
Iteration 144/1000 | Loss: 0.00001110
Iteration 145/1000 | Loss: 0.00001110
Iteration 146/1000 | Loss: 0.00001110
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001109
Iteration 149/1000 | Loss: 0.00001109
Iteration 150/1000 | Loss: 0.00001109
Iteration 151/1000 | Loss: 0.00001109
Iteration 152/1000 | Loss: 0.00001109
Iteration 153/1000 | Loss: 0.00001108
Iteration 154/1000 | Loss: 0.00001108
Iteration 155/1000 | Loss: 0.00001125
Iteration 156/1000 | Loss: 0.00001124
Iteration 157/1000 | Loss: 0.00001124
Iteration 158/1000 | Loss: 0.00001123
Iteration 159/1000 | Loss: 0.00001122
Iteration 160/1000 | Loss: 0.00001121
Iteration 161/1000 | Loss: 0.00001112
Iteration 162/1000 | Loss: 0.00001105
Iteration 163/1000 | Loss: 0.00001104
Iteration 164/1000 | Loss: 0.00001104
Iteration 165/1000 | Loss: 0.00001103
Iteration 166/1000 | Loss: 0.00001103
Iteration 167/1000 | Loss: 0.00001102
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001102
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001101
Iteration 181/1000 | Loss: 0.00001101
Iteration 182/1000 | Loss: 0.00001101
Iteration 183/1000 | Loss: 0.00001101
Iteration 184/1000 | Loss: 0.00001101
Iteration 185/1000 | Loss: 0.00001101
Iteration 186/1000 | Loss: 0.00001101
Iteration 187/1000 | Loss: 0.00001101
Iteration 188/1000 | Loss: 0.00001101
Iteration 189/1000 | Loss: 0.00001101
Iteration 190/1000 | Loss: 0.00001101
Iteration 191/1000 | Loss: 0.00001101
Iteration 192/1000 | Loss: 0.00001101
Iteration 193/1000 | Loss: 0.00001101
Iteration 194/1000 | Loss: 0.00001101
Iteration 195/1000 | Loss: 0.00001101
Iteration 196/1000 | Loss: 0.00001101
Iteration 197/1000 | Loss: 0.00001101
Iteration 198/1000 | Loss: 0.00001101
Iteration 199/1000 | Loss: 0.00001101
Iteration 200/1000 | Loss: 0.00001101
Iteration 201/1000 | Loss: 0.00001101
Iteration 202/1000 | Loss: 0.00001101
Iteration 203/1000 | Loss: 0.00001101
Iteration 204/1000 | Loss: 0.00001101
Iteration 205/1000 | Loss: 0.00001101
Iteration 206/1000 | Loss: 0.00001101
Iteration 207/1000 | Loss: 0.00001101
Iteration 208/1000 | Loss: 0.00001101
Iteration 209/1000 | Loss: 0.00001101
Iteration 210/1000 | Loss: 0.00001101
Iteration 211/1000 | Loss: 0.00001101
Iteration 212/1000 | Loss: 0.00001101
Iteration 213/1000 | Loss: 0.00001101
Iteration 214/1000 | Loss: 0.00001101
Iteration 215/1000 | Loss: 0.00001101
Iteration 216/1000 | Loss: 0.00001101
Iteration 217/1000 | Loss: 0.00001101
Iteration 218/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.1013579751306679e-05, 1.1013579751306679e-05, 1.1013579751306679e-05, 1.1013579751306679e-05, 1.1013579751306679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1013579751306679e-05

Optimization complete. Final v2v error: 2.7381391525268555 mm

Highest mean error: 8.733185768127441 mm for frame 232

Lowest mean error: 2.2288191318511963 mm for frame 4

Saving results

Total time: 74.99988293647766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827132
Iteration 2/25 | Loss: 0.00111904
Iteration 3/25 | Loss: 0.00103359
Iteration 4/25 | Loss: 0.00101733
Iteration 5/25 | Loss: 0.00101152
Iteration 6/25 | Loss: 0.00101002
Iteration 7/25 | Loss: 0.00101002
Iteration 8/25 | Loss: 0.00101002
Iteration 9/25 | Loss: 0.00101002
Iteration 10/25 | Loss: 0.00101002
Iteration 11/25 | Loss: 0.00101002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010100222425535321, 0.0010100222425535321, 0.0010100222425535321, 0.0010100222425535321, 0.0010100222425535321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010100222425535321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34091938
Iteration 2/25 | Loss: 0.00155867
Iteration 3/25 | Loss: 0.00155866
Iteration 4/25 | Loss: 0.00155866
Iteration 5/25 | Loss: 0.00155866
Iteration 6/25 | Loss: 0.00155866
Iteration 7/25 | Loss: 0.00155866
Iteration 8/25 | Loss: 0.00155866
Iteration 9/25 | Loss: 0.00155866
Iteration 10/25 | Loss: 0.00155866
Iteration 11/25 | Loss: 0.00155866
Iteration 12/25 | Loss: 0.00155866
Iteration 13/25 | Loss: 0.00155866
Iteration 14/25 | Loss: 0.00155866
Iteration 15/25 | Loss: 0.00155866
Iteration 16/25 | Loss: 0.00155866
Iteration 17/25 | Loss: 0.00155866
Iteration 18/25 | Loss: 0.00155866
Iteration 19/25 | Loss: 0.00155866
Iteration 20/25 | Loss: 0.00155866
Iteration 21/25 | Loss: 0.00155866
Iteration 22/25 | Loss: 0.00155866
Iteration 23/25 | Loss: 0.00155866
Iteration 24/25 | Loss: 0.00155866
Iteration 25/25 | Loss: 0.00155866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155866
Iteration 2/1000 | Loss: 0.00003039
Iteration 3/1000 | Loss: 0.00001819
Iteration 4/1000 | Loss: 0.00001618
Iteration 5/1000 | Loss: 0.00001507
Iteration 6/1000 | Loss: 0.00001427
Iteration 7/1000 | Loss: 0.00001360
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001264
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001251
Iteration 14/1000 | Loss: 0.00001241
Iteration 15/1000 | Loss: 0.00001238
Iteration 16/1000 | Loss: 0.00001238
Iteration 17/1000 | Loss: 0.00001236
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001231
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001230
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001229
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001224
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001213
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001213
Iteration 53/1000 | Loss: 0.00001212
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001212
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001210
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001209
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001206
Iteration 88/1000 | Loss: 0.00001206
Iteration 89/1000 | Loss: 0.00001206
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001206
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001205
Iteration 111/1000 | Loss: 0.00001205
Iteration 112/1000 | Loss: 0.00001205
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001205
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001205
Iteration 133/1000 | Loss: 0.00001205
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001205
Iteration 137/1000 | Loss: 0.00001205
Iteration 138/1000 | Loss: 0.00001205
Iteration 139/1000 | Loss: 0.00001205
Iteration 140/1000 | Loss: 0.00001205
Iteration 141/1000 | Loss: 0.00001205
Iteration 142/1000 | Loss: 0.00001205
Iteration 143/1000 | Loss: 0.00001205
Iteration 144/1000 | Loss: 0.00001205
Iteration 145/1000 | Loss: 0.00001205
Iteration 146/1000 | Loss: 0.00001205
Iteration 147/1000 | Loss: 0.00001205
Iteration 148/1000 | Loss: 0.00001205
Iteration 149/1000 | Loss: 0.00001205
Iteration 150/1000 | Loss: 0.00001205
Iteration 151/1000 | Loss: 0.00001205
Iteration 152/1000 | Loss: 0.00001205
Iteration 153/1000 | Loss: 0.00001205
Iteration 154/1000 | Loss: 0.00001205
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2052259080519434e-05, 1.2052259080519434e-05, 1.2052259080519434e-05, 1.2052259080519434e-05, 1.2052259080519434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2052259080519434e-05

Optimization complete. Final v2v error: 2.9591987133026123 mm

Highest mean error: 3.463980197906494 mm for frame 217

Lowest mean error: 2.6310372352600098 mm for frame 1

Saving results

Total time: 38.676355838775635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856600
Iteration 2/25 | Loss: 0.00141079
Iteration 3/25 | Loss: 0.00111253
Iteration 4/25 | Loss: 0.00107768
Iteration 5/25 | Loss: 0.00107499
Iteration 6/25 | Loss: 0.00107499
Iteration 7/25 | Loss: 0.00107499
Iteration 8/25 | Loss: 0.00107499
Iteration 9/25 | Loss: 0.00107499
Iteration 10/25 | Loss: 0.00107499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001074990606866777, 0.001074990606866777, 0.001074990606866777, 0.001074990606866777, 0.001074990606866777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001074990606866777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90704757
Iteration 2/25 | Loss: 0.00059744
Iteration 3/25 | Loss: 0.00059744
Iteration 4/25 | Loss: 0.00059743
Iteration 5/25 | Loss: 0.00059743
Iteration 6/25 | Loss: 0.00059743
Iteration 7/25 | Loss: 0.00059743
Iteration 8/25 | Loss: 0.00059743
Iteration 9/25 | Loss: 0.00059743
Iteration 10/25 | Loss: 0.00059743
Iteration 11/25 | Loss: 0.00059743
Iteration 12/25 | Loss: 0.00059743
Iteration 13/25 | Loss: 0.00059743
Iteration 14/25 | Loss: 0.00059743
Iteration 15/25 | Loss: 0.00059743
Iteration 16/25 | Loss: 0.00059743
Iteration 17/25 | Loss: 0.00059743
Iteration 18/25 | Loss: 0.00059743
Iteration 19/25 | Loss: 0.00059743
Iteration 20/25 | Loss: 0.00059743
Iteration 21/25 | Loss: 0.00059743
Iteration 22/25 | Loss: 0.00059743
Iteration 23/25 | Loss: 0.00059743
Iteration 24/25 | Loss: 0.00059743
Iteration 25/25 | Loss: 0.00059743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059743
Iteration 2/1000 | Loss: 0.00004971
Iteration 3/1000 | Loss: 0.00002628
Iteration 4/1000 | Loss: 0.00002209
Iteration 5/1000 | Loss: 0.00002053
Iteration 6/1000 | Loss: 0.00001950
Iteration 7/1000 | Loss: 0.00001901
Iteration 8/1000 | Loss: 0.00001859
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001814
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001795
Iteration 14/1000 | Loss: 0.00001794
Iteration 15/1000 | Loss: 0.00001793
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001792
Iteration 18/1000 | Loss: 0.00001792
Iteration 19/1000 | Loss: 0.00001792
Iteration 20/1000 | Loss: 0.00001791
Iteration 21/1000 | Loss: 0.00001791
Iteration 22/1000 | Loss: 0.00001790
Iteration 23/1000 | Loss: 0.00001790
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001786
Iteration 26/1000 | Loss: 0.00001786
Iteration 27/1000 | Loss: 0.00001786
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001778
Iteration 34/1000 | Loss: 0.00001778
Iteration 35/1000 | Loss: 0.00001778
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001777
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001777
Iteration 42/1000 | Loss: 0.00001777
Iteration 43/1000 | Loss: 0.00001777
Iteration 44/1000 | Loss: 0.00001777
Iteration 45/1000 | Loss: 0.00001777
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001777
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001776
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001776
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001776
Iteration 56/1000 | Loss: 0.00001776
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001775
Iteration 66/1000 | Loss: 0.00001775
Iteration 67/1000 | Loss: 0.00001775
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001773
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001771
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001771
Iteration 95/1000 | Loss: 0.00001771
Iteration 96/1000 | Loss: 0.00001771
Iteration 97/1000 | Loss: 0.00001771
Iteration 98/1000 | Loss: 0.00001770
Iteration 99/1000 | Loss: 0.00001770
Iteration 100/1000 | Loss: 0.00001770
Iteration 101/1000 | Loss: 0.00001770
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001770
Iteration 104/1000 | Loss: 0.00001770
Iteration 105/1000 | Loss: 0.00001770
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001770
Iteration 111/1000 | Loss: 0.00001770
Iteration 112/1000 | Loss: 0.00001770
Iteration 113/1000 | Loss: 0.00001770
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001769
Iteration 116/1000 | Loss: 0.00001769
Iteration 117/1000 | Loss: 0.00001769
Iteration 118/1000 | Loss: 0.00001769
Iteration 119/1000 | Loss: 0.00001769
Iteration 120/1000 | Loss: 0.00001769
Iteration 121/1000 | Loss: 0.00001769
Iteration 122/1000 | Loss: 0.00001769
Iteration 123/1000 | Loss: 0.00001769
Iteration 124/1000 | Loss: 0.00001769
Iteration 125/1000 | Loss: 0.00001769
Iteration 126/1000 | Loss: 0.00001769
Iteration 127/1000 | Loss: 0.00001769
Iteration 128/1000 | Loss: 0.00001768
Iteration 129/1000 | Loss: 0.00001768
Iteration 130/1000 | Loss: 0.00001768
Iteration 131/1000 | Loss: 0.00001768
Iteration 132/1000 | Loss: 0.00001768
Iteration 133/1000 | Loss: 0.00001768
Iteration 134/1000 | Loss: 0.00001768
Iteration 135/1000 | Loss: 0.00001768
Iteration 136/1000 | Loss: 0.00001768
Iteration 137/1000 | Loss: 0.00001767
Iteration 138/1000 | Loss: 0.00001767
Iteration 139/1000 | Loss: 0.00001767
Iteration 140/1000 | Loss: 0.00001767
Iteration 141/1000 | Loss: 0.00001767
Iteration 142/1000 | Loss: 0.00001767
Iteration 143/1000 | Loss: 0.00001767
Iteration 144/1000 | Loss: 0.00001767
Iteration 145/1000 | Loss: 0.00001767
Iteration 146/1000 | Loss: 0.00001767
Iteration 147/1000 | Loss: 0.00001767
Iteration 148/1000 | Loss: 0.00001767
Iteration 149/1000 | Loss: 0.00001767
Iteration 150/1000 | Loss: 0.00001767
Iteration 151/1000 | Loss: 0.00001766
Iteration 152/1000 | Loss: 0.00001766
Iteration 153/1000 | Loss: 0.00001766
Iteration 154/1000 | Loss: 0.00001766
Iteration 155/1000 | Loss: 0.00001766
Iteration 156/1000 | Loss: 0.00001766
Iteration 157/1000 | Loss: 0.00001766
Iteration 158/1000 | Loss: 0.00001766
Iteration 159/1000 | Loss: 0.00001766
Iteration 160/1000 | Loss: 0.00001766
Iteration 161/1000 | Loss: 0.00001766
Iteration 162/1000 | Loss: 0.00001766
Iteration 163/1000 | Loss: 0.00001765
Iteration 164/1000 | Loss: 0.00001765
Iteration 165/1000 | Loss: 0.00001765
Iteration 166/1000 | Loss: 0.00001765
Iteration 167/1000 | Loss: 0.00001765
Iteration 168/1000 | Loss: 0.00001765
Iteration 169/1000 | Loss: 0.00001765
Iteration 170/1000 | Loss: 0.00001765
Iteration 171/1000 | Loss: 0.00001765
Iteration 172/1000 | Loss: 0.00001765
Iteration 173/1000 | Loss: 0.00001765
Iteration 174/1000 | Loss: 0.00001765
Iteration 175/1000 | Loss: 0.00001765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.7651973394094966e-05, 1.7651973394094966e-05, 1.7651973394094966e-05, 1.7651973394094966e-05, 1.7651973394094966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7651973394094966e-05

Optimization complete. Final v2v error: 3.580681800842285 mm

Highest mean error: 3.810650587081909 mm for frame 21

Lowest mean error: 3.3094563484191895 mm for frame 84

Saving results

Total time: 35.23829126358032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042480
Iteration 2/25 | Loss: 0.00188960
Iteration 3/25 | Loss: 0.00140812
Iteration 4/25 | Loss: 0.00124742
Iteration 5/25 | Loss: 0.00118362
Iteration 6/25 | Loss: 0.00115835
Iteration 7/25 | Loss: 0.00115078
Iteration 8/25 | Loss: 0.00114401
Iteration 9/25 | Loss: 0.00113287
Iteration 10/25 | Loss: 0.00112557
Iteration 11/25 | Loss: 0.00111769
Iteration 12/25 | Loss: 0.00112088
Iteration 13/25 | Loss: 0.00112171
Iteration 14/25 | Loss: 0.00111575
Iteration 15/25 | Loss: 0.00111168
Iteration 16/25 | Loss: 0.00111052
Iteration 17/25 | Loss: 0.00111027
Iteration 18/25 | Loss: 0.00110998
Iteration 19/25 | Loss: 0.00110981
Iteration 20/25 | Loss: 0.00110976
Iteration 21/25 | Loss: 0.00110976
Iteration 22/25 | Loss: 0.00110976
Iteration 23/25 | Loss: 0.00110976
Iteration 24/25 | Loss: 0.00110976
Iteration 25/25 | Loss: 0.00110975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27190888
Iteration 2/25 | Loss: 0.00187036
Iteration 3/25 | Loss: 0.00193987
Iteration 4/25 | Loss: 0.00193095
Iteration 5/25 | Loss: 0.00187933
Iteration 6/25 | Loss: 0.00181001
Iteration 7/25 | Loss: 0.00181001
Iteration 8/25 | Loss: 0.00181001
Iteration 9/25 | Loss: 0.00181001
Iteration 10/25 | Loss: 0.00181001
Iteration 11/25 | Loss: 0.00181001
Iteration 12/25 | Loss: 0.00181000
Iteration 13/25 | Loss: 0.00181000
Iteration 14/25 | Loss: 0.00181000
Iteration 15/25 | Loss: 0.00181000
Iteration 16/25 | Loss: 0.00181000
Iteration 17/25 | Loss: 0.00181000
Iteration 18/25 | Loss: 0.00181000
Iteration 19/25 | Loss: 0.00181000
Iteration 20/25 | Loss: 0.00181000
Iteration 21/25 | Loss: 0.00181000
Iteration 22/25 | Loss: 0.00181000
Iteration 23/25 | Loss: 0.00181000
Iteration 24/25 | Loss: 0.00181000
Iteration 25/25 | Loss: 0.00181000

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181000
Iteration 2/1000 | Loss: 0.00009970
Iteration 3/1000 | Loss: 0.00013565
Iteration 4/1000 | Loss: 0.00006949
Iteration 5/1000 | Loss: 0.00013622
Iteration 6/1000 | Loss: 0.00010590
Iteration 7/1000 | Loss: 0.00006788
Iteration 8/1000 | Loss: 0.00002417
Iteration 9/1000 | Loss: 0.00002124
Iteration 10/1000 | Loss: 0.00002062
Iteration 11/1000 | Loss: 0.00002000
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001906
Iteration 14/1000 | Loss: 0.00001879
Iteration 15/1000 | Loss: 0.00001864
Iteration 16/1000 | Loss: 0.00001861
Iteration 17/1000 | Loss: 0.00006867
Iteration 18/1000 | Loss: 0.00002705
Iteration 19/1000 | Loss: 0.00001850
Iteration 20/1000 | Loss: 0.00001850
Iteration 21/1000 | Loss: 0.00001849
Iteration 22/1000 | Loss: 0.00001849
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001849
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001845
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00004710
Iteration 30/1000 | Loss: 0.00002004
Iteration 31/1000 | Loss: 0.00001851
Iteration 32/1000 | Loss: 0.00001839
Iteration 33/1000 | Loss: 0.00001839
Iteration 34/1000 | Loss: 0.00001837
Iteration 35/1000 | Loss: 0.00001837
Iteration 36/1000 | Loss: 0.00001837
Iteration 37/1000 | Loss: 0.00001836
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001825
Iteration 40/1000 | Loss: 0.00001823
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001822
Iteration 44/1000 | Loss: 0.00001822
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00006251
Iteration 47/1000 | Loss: 0.00006250
Iteration 48/1000 | Loss: 0.00066167
Iteration 49/1000 | Loss: 0.00002321
Iteration 50/1000 | Loss: 0.00001883
Iteration 51/1000 | Loss: 0.00002768
Iteration 52/1000 | Loss: 0.00001821
Iteration 53/1000 | Loss: 0.00001818
Iteration 54/1000 | Loss: 0.00001818
Iteration 55/1000 | Loss: 0.00001818
Iteration 56/1000 | Loss: 0.00001818
Iteration 57/1000 | Loss: 0.00001818
Iteration 58/1000 | Loss: 0.00001818
Iteration 59/1000 | Loss: 0.00001817
Iteration 60/1000 | Loss: 0.00001817
Iteration 61/1000 | Loss: 0.00001809
Iteration 62/1000 | Loss: 0.00001803
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00017454
Iteration 65/1000 | Loss: 0.00002075
Iteration 66/1000 | Loss: 0.00001763
Iteration 67/1000 | Loss: 0.00003161
Iteration 68/1000 | Loss: 0.00003329
Iteration 69/1000 | Loss: 0.00001741
Iteration 70/1000 | Loss: 0.00001741
Iteration 71/1000 | Loss: 0.00001741
Iteration 72/1000 | Loss: 0.00001741
Iteration 73/1000 | Loss: 0.00001741
Iteration 74/1000 | Loss: 0.00001741
Iteration 75/1000 | Loss: 0.00001741
Iteration 76/1000 | Loss: 0.00001740
Iteration 77/1000 | Loss: 0.00002888
Iteration 78/1000 | Loss: 0.00003203
Iteration 79/1000 | Loss: 0.00002258
Iteration 80/1000 | Loss: 0.00002994
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00003072
Iteration 83/1000 | Loss: 0.00003559
Iteration 84/1000 | Loss: 0.00001737
Iteration 85/1000 | Loss: 0.00001737
Iteration 86/1000 | Loss: 0.00001736
Iteration 87/1000 | Loss: 0.00001736
Iteration 88/1000 | Loss: 0.00001736
Iteration 89/1000 | Loss: 0.00001736
Iteration 90/1000 | Loss: 0.00001736
Iteration 91/1000 | Loss: 0.00001736
Iteration 92/1000 | Loss: 0.00001736
Iteration 93/1000 | Loss: 0.00001736
Iteration 94/1000 | Loss: 0.00001736
Iteration 95/1000 | Loss: 0.00001736
Iteration 96/1000 | Loss: 0.00001735
Iteration 97/1000 | Loss: 0.00001734
Iteration 98/1000 | Loss: 0.00001734
Iteration 99/1000 | Loss: 0.00001734
Iteration 100/1000 | Loss: 0.00001734
Iteration 101/1000 | Loss: 0.00001734
Iteration 102/1000 | Loss: 0.00001734
Iteration 103/1000 | Loss: 0.00001734
Iteration 104/1000 | Loss: 0.00001734
Iteration 105/1000 | Loss: 0.00001733
Iteration 106/1000 | Loss: 0.00001733
Iteration 107/1000 | Loss: 0.00001733
Iteration 108/1000 | Loss: 0.00001733
Iteration 109/1000 | Loss: 0.00001733
Iteration 110/1000 | Loss: 0.00001733
Iteration 111/1000 | Loss: 0.00001733
Iteration 112/1000 | Loss: 0.00001732
Iteration 113/1000 | Loss: 0.00003497
Iteration 114/1000 | Loss: 0.00001884
Iteration 115/1000 | Loss: 0.00001734
Iteration 116/1000 | Loss: 0.00001733
Iteration 117/1000 | Loss: 0.00001733
Iteration 118/1000 | Loss: 0.00002576
Iteration 119/1000 | Loss: 0.00001737
Iteration 120/1000 | Loss: 0.00002225
Iteration 121/1000 | Loss: 0.00001890
Iteration 122/1000 | Loss: 0.00001736
Iteration 123/1000 | Loss: 0.00001736
Iteration 124/1000 | Loss: 0.00001736
Iteration 125/1000 | Loss: 0.00001736
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001736
Iteration 128/1000 | Loss: 0.00001736
Iteration 129/1000 | Loss: 0.00001736
Iteration 130/1000 | Loss: 0.00001735
Iteration 131/1000 | Loss: 0.00001733
Iteration 132/1000 | Loss: 0.00001733
Iteration 133/1000 | Loss: 0.00001732
Iteration 134/1000 | Loss: 0.00001732
Iteration 135/1000 | Loss: 0.00001732
Iteration 136/1000 | Loss: 0.00001732
Iteration 137/1000 | Loss: 0.00001731
Iteration 138/1000 | Loss: 0.00001731
Iteration 139/1000 | Loss: 0.00001731
Iteration 140/1000 | Loss: 0.00001730
Iteration 141/1000 | Loss: 0.00001730
Iteration 142/1000 | Loss: 0.00001730
Iteration 143/1000 | Loss: 0.00001730
Iteration 144/1000 | Loss: 0.00001730
Iteration 145/1000 | Loss: 0.00001730
Iteration 146/1000 | Loss: 0.00001729
Iteration 147/1000 | Loss: 0.00001729
Iteration 148/1000 | Loss: 0.00001729
Iteration 149/1000 | Loss: 0.00001729
Iteration 150/1000 | Loss: 0.00001729
Iteration 151/1000 | Loss: 0.00001729
Iteration 152/1000 | Loss: 0.00001729
Iteration 153/1000 | Loss: 0.00001729
Iteration 154/1000 | Loss: 0.00001729
Iteration 155/1000 | Loss: 0.00001729
Iteration 156/1000 | Loss: 0.00001729
Iteration 157/1000 | Loss: 0.00001729
Iteration 158/1000 | Loss: 0.00001728
Iteration 159/1000 | Loss: 0.00001728
Iteration 160/1000 | Loss: 0.00005632
Iteration 161/1000 | Loss: 0.00001729
Iteration 162/1000 | Loss: 0.00001727
Iteration 163/1000 | Loss: 0.00001727
Iteration 164/1000 | Loss: 0.00001727
Iteration 165/1000 | Loss: 0.00001726
Iteration 166/1000 | Loss: 0.00001726
Iteration 167/1000 | Loss: 0.00001726
Iteration 168/1000 | Loss: 0.00001726
Iteration 169/1000 | Loss: 0.00001726
Iteration 170/1000 | Loss: 0.00001726
Iteration 171/1000 | Loss: 0.00001726
Iteration 172/1000 | Loss: 0.00001726
Iteration 173/1000 | Loss: 0.00001726
Iteration 174/1000 | Loss: 0.00001726
Iteration 175/1000 | Loss: 0.00001726
Iteration 176/1000 | Loss: 0.00001726
Iteration 177/1000 | Loss: 0.00001726
Iteration 178/1000 | Loss: 0.00001726
Iteration 179/1000 | Loss: 0.00001726
Iteration 180/1000 | Loss: 0.00001726
Iteration 181/1000 | Loss: 0.00001726
Iteration 182/1000 | Loss: 0.00001726
Iteration 183/1000 | Loss: 0.00001726
Iteration 184/1000 | Loss: 0.00001726
Iteration 185/1000 | Loss: 0.00001726
Iteration 186/1000 | Loss: 0.00001726
Iteration 187/1000 | Loss: 0.00001726
Iteration 188/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.725963556964416e-05, 1.725963556964416e-05, 1.725963556964416e-05, 1.725963556964416e-05, 1.725963556964416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.725963556964416e-05

Optimization complete. Final v2v error: 3.4559590816497803 mm

Highest mean error: 10.021053314208984 mm for frame 201

Lowest mean error: 3.1783735752105713 mm for frame 139

Saving results

Total time: 132.05322456359863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051965
Iteration 2/25 | Loss: 0.00242117
Iteration 3/25 | Loss: 0.00190415
Iteration 4/25 | Loss: 0.00175169
Iteration 5/25 | Loss: 0.00154779
Iteration 6/25 | Loss: 0.00147824
Iteration 7/25 | Loss: 0.00143228
Iteration 8/25 | Loss: 0.00141645
Iteration 9/25 | Loss: 0.00142180
Iteration 10/25 | Loss: 0.00139709
Iteration 11/25 | Loss: 0.00138264
Iteration 12/25 | Loss: 0.00137398
Iteration 13/25 | Loss: 0.00137260
Iteration 14/25 | Loss: 0.00137708
Iteration 15/25 | Loss: 0.00137503
Iteration 16/25 | Loss: 0.00136886
Iteration 17/25 | Loss: 0.00136727
Iteration 18/25 | Loss: 0.00136745
Iteration 19/25 | Loss: 0.00136763
Iteration 20/25 | Loss: 0.00136901
Iteration 21/25 | Loss: 0.00136165
Iteration 22/25 | Loss: 0.00135856
Iteration 23/25 | Loss: 0.00135279
Iteration 24/25 | Loss: 0.00135172
Iteration 25/25 | Loss: 0.00135154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.72793889
Iteration 2/25 | Loss: 0.00334216
Iteration 3/25 | Loss: 0.00334216
Iteration 4/25 | Loss: 0.00334216
Iteration 5/25 | Loss: 0.00334216
Iteration 6/25 | Loss: 0.00334216
Iteration 7/25 | Loss: 0.00334216
Iteration 8/25 | Loss: 0.00334216
Iteration 9/25 | Loss: 0.00334216
Iteration 10/25 | Loss: 0.00334216
Iteration 11/25 | Loss: 0.00334216
Iteration 12/25 | Loss: 0.00334216
Iteration 13/25 | Loss: 0.00334216
Iteration 14/25 | Loss: 0.00334216
Iteration 15/25 | Loss: 0.00334216
Iteration 16/25 | Loss: 0.00334216
Iteration 17/25 | Loss: 0.00334216
Iteration 18/25 | Loss: 0.00334216
Iteration 19/25 | Loss: 0.00334216
Iteration 20/25 | Loss: 0.00334216
Iteration 21/25 | Loss: 0.00334216
Iteration 22/25 | Loss: 0.00334216
Iteration 23/25 | Loss: 0.00334216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0033421586267650127, 0.0033421586267650127, 0.0033421586267650127, 0.0033421586267650127, 0.0033421586267650127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033421586267650127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00334216
Iteration 2/1000 | Loss: 0.00035408
Iteration 3/1000 | Loss: 0.00022988
Iteration 4/1000 | Loss: 0.00043590
Iteration 5/1000 | Loss: 0.00020326
Iteration 6/1000 | Loss: 0.00017635
Iteration 7/1000 | Loss: 0.00016556
Iteration 8/1000 | Loss: 0.00115284
Iteration 9/1000 | Loss: 0.00056451
Iteration 10/1000 | Loss: 0.00066506
Iteration 11/1000 | Loss: 0.00017573
Iteration 12/1000 | Loss: 0.00034183
Iteration 13/1000 | Loss: 0.00020042
Iteration 14/1000 | Loss: 0.00024055
Iteration 15/1000 | Loss: 0.00013747
Iteration 16/1000 | Loss: 0.00013176
Iteration 17/1000 | Loss: 0.00012740
Iteration 18/1000 | Loss: 0.00012380
Iteration 19/1000 | Loss: 0.00012037
Iteration 20/1000 | Loss: 0.00011768
Iteration 21/1000 | Loss: 0.00011595
Iteration 22/1000 | Loss: 0.00011428
Iteration 23/1000 | Loss: 0.00011269
Iteration 24/1000 | Loss: 0.00011135
Iteration 25/1000 | Loss: 0.00011025
Iteration 26/1000 | Loss: 0.00010923
Iteration 27/1000 | Loss: 0.00018059
Iteration 28/1000 | Loss: 0.00018562
Iteration 29/1000 | Loss: 0.00013521
Iteration 30/1000 | Loss: 0.00011665
Iteration 31/1000 | Loss: 0.00011046
Iteration 32/1000 | Loss: 0.00010669
Iteration 33/1000 | Loss: 0.00010327
Iteration 34/1000 | Loss: 0.00010179
Iteration 35/1000 | Loss: 0.00010051
Iteration 36/1000 | Loss: 0.00009995
Iteration 37/1000 | Loss: 0.00009946
Iteration 38/1000 | Loss: 0.00009896
Iteration 39/1000 | Loss: 0.00009865
Iteration 40/1000 | Loss: 0.00009835
Iteration 41/1000 | Loss: 0.00009814
Iteration 42/1000 | Loss: 0.00009793
Iteration 43/1000 | Loss: 0.00009784
Iteration 44/1000 | Loss: 0.00009772
Iteration 45/1000 | Loss: 0.00009770
Iteration 46/1000 | Loss: 0.00047625
Iteration 47/1000 | Loss: 0.00028587
Iteration 48/1000 | Loss: 0.00217260
Iteration 49/1000 | Loss: 0.00435168
Iteration 50/1000 | Loss: 0.00047819
Iteration 51/1000 | Loss: 0.00023951
Iteration 52/1000 | Loss: 0.00017345
Iteration 53/1000 | Loss: 0.00012872
Iteration 54/1000 | Loss: 0.00008341
Iteration 55/1000 | Loss: 0.00007284
Iteration 56/1000 | Loss: 0.00004212
Iteration 57/1000 | Loss: 0.00003604
Iteration 58/1000 | Loss: 0.00003184
Iteration 59/1000 | Loss: 0.00002881
Iteration 60/1000 | Loss: 0.00002635
Iteration 61/1000 | Loss: 0.00002423
Iteration 62/1000 | Loss: 0.00002215
Iteration 63/1000 | Loss: 0.00002067
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001885
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001748
Iteration 69/1000 | Loss: 0.00001722
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001690
Iteration 76/1000 | Loss: 0.00001689
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001688
Iteration 79/1000 | Loss: 0.00001688
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001687
Iteration 83/1000 | Loss: 0.00001684
Iteration 84/1000 | Loss: 0.00001684
Iteration 85/1000 | Loss: 0.00001683
Iteration 86/1000 | Loss: 0.00001682
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00001682
Iteration 89/1000 | Loss: 0.00001682
Iteration 90/1000 | Loss: 0.00001682
Iteration 91/1000 | Loss: 0.00001682
Iteration 92/1000 | Loss: 0.00001682
Iteration 93/1000 | Loss: 0.00001682
Iteration 94/1000 | Loss: 0.00001682
Iteration 95/1000 | Loss: 0.00001682
Iteration 96/1000 | Loss: 0.00001682
Iteration 97/1000 | Loss: 0.00001681
Iteration 98/1000 | Loss: 0.00001681
Iteration 99/1000 | Loss: 0.00001681
Iteration 100/1000 | Loss: 0.00001680
Iteration 101/1000 | Loss: 0.00001680
Iteration 102/1000 | Loss: 0.00001680
Iteration 103/1000 | Loss: 0.00001680
Iteration 104/1000 | Loss: 0.00001680
Iteration 105/1000 | Loss: 0.00001680
Iteration 106/1000 | Loss: 0.00001680
Iteration 107/1000 | Loss: 0.00001680
Iteration 108/1000 | Loss: 0.00001680
Iteration 109/1000 | Loss: 0.00001680
Iteration 110/1000 | Loss: 0.00001680
Iteration 111/1000 | Loss: 0.00001680
Iteration 112/1000 | Loss: 0.00001679
Iteration 113/1000 | Loss: 0.00001679
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001679
Iteration 116/1000 | Loss: 0.00001679
Iteration 117/1000 | Loss: 0.00001679
Iteration 118/1000 | Loss: 0.00001679
Iteration 119/1000 | Loss: 0.00001679
Iteration 120/1000 | Loss: 0.00001679
Iteration 121/1000 | Loss: 0.00001679
Iteration 122/1000 | Loss: 0.00001679
Iteration 123/1000 | Loss: 0.00001679
Iteration 124/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.6792633687146008e-05, 1.6792633687146008e-05, 1.6792633687146008e-05, 1.6792633687146008e-05, 1.6792633687146008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6792633687146008e-05

Optimization complete. Final v2v error: 3.422393321990967 mm

Highest mean error: 9.333504676818848 mm for frame 87

Lowest mean error: 2.7868902683258057 mm for frame 4

Saving results

Total time: 144.15316081047058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986881
Iteration 2/25 | Loss: 0.00227086
Iteration 3/25 | Loss: 0.00174225
Iteration 4/25 | Loss: 0.00154149
Iteration 5/25 | Loss: 0.00148608
Iteration 6/25 | Loss: 0.00145316
Iteration 7/25 | Loss: 0.00143534
Iteration 8/25 | Loss: 0.00143600
Iteration 9/25 | Loss: 0.00128258
Iteration 10/25 | Loss: 0.00124772
Iteration 11/25 | Loss: 0.00123494
Iteration 12/25 | Loss: 0.00122738
Iteration 13/25 | Loss: 0.00122764
Iteration 14/25 | Loss: 0.00121853
Iteration 15/25 | Loss: 0.00120266
Iteration 16/25 | Loss: 0.00120207
Iteration 17/25 | Loss: 0.00119789
Iteration 18/25 | Loss: 0.00119921
Iteration 19/25 | Loss: 0.00120069
Iteration 20/25 | Loss: 0.00119854
Iteration 21/25 | Loss: 0.00119439
Iteration 22/25 | Loss: 0.00119648
Iteration 23/25 | Loss: 0.00119245
Iteration 24/25 | Loss: 0.00119142
Iteration 25/25 | Loss: 0.00119343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25269639
Iteration 2/25 | Loss: 0.00385438
Iteration 3/25 | Loss: 0.00304480
Iteration 4/25 | Loss: 0.00304480
Iteration 5/25 | Loss: 0.00304479
Iteration 6/25 | Loss: 0.00304479
Iteration 7/25 | Loss: 0.00304479
Iteration 8/25 | Loss: 0.00304479
Iteration 9/25 | Loss: 0.00304479
Iteration 10/25 | Loss: 0.00304479
Iteration 11/25 | Loss: 0.00304479
Iteration 12/25 | Loss: 0.00304479
Iteration 13/25 | Loss: 0.00304479
Iteration 14/25 | Loss: 0.00304479
Iteration 15/25 | Loss: 0.00304479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003044792916625738, 0.003044792916625738, 0.003044792916625738, 0.003044792916625738, 0.003044792916625738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003044792916625738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304479
Iteration 2/1000 | Loss: 0.00289413
Iteration 3/1000 | Loss: 0.00161188
Iteration 4/1000 | Loss: 0.00143393
Iteration 5/1000 | Loss: 0.00056721
Iteration 6/1000 | Loss: 0.00028311
Iteration 7/1000 | Loss: 0.00029897
Iteration 8/1000 | Loss: 0.00032350
Iteration 9/1000 | Loss: 0.00017511
Iteration 10/1000 | Loss: 0.00029141
Iteration 11/1000 | Loss: 0.00071033
Iteration 12/1000 | Loss: 0.00231499
Iteration 13/1000 | Loss: 0.00077747
Iteration 14/1000 | Loss: 0.00066570
Iteration 15/1000 | Loss: 0.00035640
Iteration 16/1000 | Loss: 0.00012750
Iteration 17/1000 | Loss: 0.00017667
Iteration 18/1000 | Loss: 0.00026017
Iteration 19/1000 | Loss: 0.00042392
Iteration 20/1000 | Loss: 0.00005380
Iteration 21/1000 | Loss: 0.00014591
Iteration 22/1000 | Loss: 0.00015985
Iteration 23/1000 | Loss: 0.00095969
Iteration 24/1000 | Loss: 0.00043557
Iteration 25/1000 | Loss: 0.00014342
Iteration 26/1000 | Loss: 0.00003557
Iteration 27/1000 | Loss: 0.00003877
Iteration 28/1000 | Loss: 0.00003845
Iteration 29/1000 | Loss: 0.00015136
Iteration 30/1000 | Loss: 0.00014306
Iteration 31/1000 | Loss: 0.00012804
Iteration 32/1000 | Loss: 0.00033230
Iteration 33/1000 | Loss: 0.00005465
Iteration 34/1000 | Loss: 0.00007074
Iteration 35/1000 | Loss: 0.00003331
Iteration 36/1000 | Loss: 0.00009045
Iteration 37/1000 | Loss: 0.00003002
Iteration 38/1000 | Loss: 0.00004871
Iteration 39/1000 | Loss: 0.00008195
Iteration 40/1000 | Loss: 0.00003645
Iteration 41/1000 | Loss: 0.00003779
Iteration 42/1000 | Loss: 0.00004479
Iteration 43/1000 | Loss: 0.00003975
Iteration 44/1000 | Loss: 0.00046648
Iteration 45/1000 | Loss: 0.00009039
Iteration 46/1000 | Loss: 0.00010441
Iteration 47/1000 | Loss: 0.00008846
Iteration 48/1000 | Loss: 0.00016949
Iteration 49/1000 | Loss: 0.00004651
Iteration 50/1000 | Loss: 0.00021193
Iteration 51/1000 | Loss: 0.00004756
Iteration 52/1000 | Loss: 0.00005136
Iteration 53/1000 | Loss: 0.00005786
Iteration 54/1000 | Loss: 0.00022465
Iteration 55/1000 | Loss: 0.00003145
Iteration 56/1000 | Loss: 0.00004296
Iteration 57/1000 | Loss: 0.00003773
Iteration 58/1000 | Loss: 0.00005772
Iteration 59/1000 | Loss: 0.00004361
Iteration 60/1000 | Loss: 0.00004345
Iteration 61/1000 | Loss: 0.00004678
Iteration 62/1000 | Loss: 0.00003761
Iteration 63/1000 | Loss: 0.00003658
Iteration 64/1000 | Loss: 0.00002357
Iteration 65/1000 | Loss: 0.00004197
Iteration 66/1000 | Loss: 0.00007526
Iteration 67/1000 | Loss: 0.00003016
Iteration 68/1000 | Loss: 0.00002453
Iteration 69/1000 | Loss: 0.00003758
Iteration 70/1000 | Loss: 0.00002894
Iteration 71/1000 | Loss: 0.00003529
Iteration 72/1000 | Loss: 0.00003843
Iteration 73/1000 | Loss: 0.00003521
Iteration 74/1000 | Loss: 0.00003021
Iteration 75/1000 | Loss: 0.00003124
Iteration 76/1000 | Loss: 0.00003165
Iteration 77/1000 | Loss: 0.00002989
Iteration 78/1000 | Loss: 0.00005987
Iteration 79/1000 | Loss: 0.00003181
Iteration 80/1000 | Loss: 0.00003195
Iteration 81/1000 | Loss: 0.00003200
Iteration 82/1000 | Loss: 0.00004810
Iteration 83/1000 | Loss: 0.00003336
Iteration 84/1000 | Loss: 0.00004280
Iteration 85/1000 | Loss: 0.00003768
Iteration 86/1000 | Loss: 0.00003706
Iteration 87/1000 | Loss: 0.00002176
Iteration 88/1000 | Loss: 0.00003873
Iteration 89/1000 | Loss: 0.00002052
Iteration 90/1000 | Loss: 0.00002049
Iteration 91/1000 | Loss: 0.00003379
Iteration 92/1000 | Loss: 0.00002033
Iteration 93/1000 | Loss: 0.00002016
Iteration 94/1000 | Loss: 0.00002983
Iteration 95/1000 | Loss: 0.00002007
Iteration 96/1000 | Loss: 0.00002006
Iteration 97/1000 | Loss: 0.00002006
Iteration 98/1000 | Loss: 0.00002005
Iteration 99/1000 | Loss: 0.00002004
Iteration 100/1000 | Loss: 0.00002002
Iteration 101/1000 | Loss: 0.00002002
Iteration 102/1000 | Loss: 0.00002002
Iteration 103/1000 | Loss: 0.00002001
Iteration 104/1000 | Loss: 0.00002001
Iteration 105/1000 | Loss: 0.00002001
Iteration 106/1000 | Loss: 0.00002001
Iteration 107/1000 | Loss: 0.00002001
Iteration 108/1000 | Loss: 0.00002001
Iteration 109/1000 | Loss: 0.00002001
Iteration 110/1000 | Loss: 0.00002001
Iteration 111/1000 | Loss: 0.00002001
Iteration 112/1000 | Loss: 0.00002001
Iteration 113/1000 | Loss: 0.00002001
Iteration 114/1000 | Loss: 0.00001999
Iteration 115/1000 | Loss: 0.00001999
Iteration 116/1000 | Loss: 0.00001999
Iteration 117/1000 | Loss: 0.00001998
Iteration 118/1000 | Loss: 0.00001998
Iteration 119/1000 | Loss: 0.00001998
Iteration 120/1000 | Loss: 0.00001998
Iteration 121/1000 | Loss: 0.00001998
Iteration 122/1000 | Loss: 0.00001998
Iteration 123/1000 | Loss: 0.00001998
Iteration 124/1000 | Loss: 0.00001998
Iteration 125/1000 | Loss: 0.00001998
Iteration 126/1000 | Loss: 0.00001998
Iteration 127/1000 | Loss: 0.00001997
Iteration 128/1000 | Loss: 0.00001997
Iteration 129/1000 | Loss: 0.00001997
Iteration 130/1000 | Loss: 0.00016419
Iteration 131/1000 | Loss: 0.00004554
Iteration 132/1000 | Loss: 0.00002917
Iteration 133/1000 | Loss: 0.00002226
Iteration 134/1000 | Loss: 0.00002141
Iteration 135/1000 | Loss: 0.00002101
Iteration 136/1000 | Loss: 0.00002066
Iteration 137/1000 | Loss: 0.00002039
Iteration 138/1000 | Loss: 0.00002016
Iteration 139/1000 | Loss: 0.00042978
Iteration 140/1000 | Loss: 0.00019258
Iteration 141/1000 | Loss: 0.00009888
Iteration 142/1000 | Loss: 0.00009863
Iteration 143/1000 | Loss: 0.00002138
Iteration 144/1000 | Loss: 0.00003150
Iteration 145/1000 | Loss: 0.00022168
Iteration 146/1000 | Loss: 0.00020374
Iteration 147/1000 | Loss: 0.00010015
Iteration 148/1000 | Loss: 0.00001979
Iteration 149/1000 | Loss: 0.00001905
Iteration 150/1000 | Loss: 0.00009131
Iteration 151/1000 | Loss: 0.00018328
Iteration 152/1000 | Loss: 0.00011134
Iteration 153/1000 | Loss: 0.00015996
Iteration 154/1000 | Loss: 0.00006287
Iteration 155/1000 | Loss: 0.00015480
Iteration 156/1000 | Loss: 0.00003604
Iteration 157/1000 | Loss: 0.00002306
Iteration 158/1000 | Loss: 0.00002106
Iteration 159/1000 | Loss: 0.00002036
Iteration 160/1000 | Loss: 0.00001970
Iteration 161/1000 | Loss: 0.00001936
Iteration 162/1000 | Loss: 0.00001904
Iteration 163/1000 | Loss: 0.00001874
Iteration 164/1000 | Loss: 0.00001806
Iteration 165/1000 | Loss: 0.00003530
Iteration 166/1000 | Loss: 0.00001741
Iteration 167/1000 | Loss: 0.00001725
Iteration 168/1000 | Loss: 0.00001720
Iteration 169/1000 | Loss: 0.00003305
Iteration 170/1000 | Loss: 0.00001716
Iteration 171/1000 | Loss: 0.00001708
Iteration 172/1000 | Loss: 0.00001708
Iteration 173/1000 | Loss: 0.00001708
Iteration 174/1000 | Loss: 0.00001708
Iteration 175/1000 | Loss: 0.00001707
Iteration 176/1000 | Loss: 0.00001707
Iteration 177/1000 | Loss: 0.00001707
Iteration 178/1000 | Loss: 0.00001705
Iteration 179/1000 | Loss: 0.00001704
Iteration 180/1000 | Loss: 0.00001704
Iteration 181/1000 | Loss: 0.00001704
Iteration 182/1000 | Loss: 0.00001703
Iteration 183/1000 | Loss: 0.00001703
Iteration 184/1000 | Loss: 0.00001703
Iteration 185/1000 | Loss: 0.00001703
Iteration 186/1000 | Loss: 0.00001700
Iteration 187/1000 | Loss: 0.00001700
Iteration 188/1000 | Loss: 0.00001699
Iteration 189/1000 | Loss: 0.00001699
Iteration 190/1000 | Loss: 0.00001699
Iteration 191/1000 | Loss: 0.00001698
Iteration 192/1000 | Loss: 0.00001698
Iteration 193/1000 | Loss: 0.00001698
Iteration 194/1000 | Loss: 0.00001698
Iteration 195/1000 | Loss: 0.00001697
Iteration 196/1000 | Loss: 0.00001697
Iteration 197/1000 | Loss: 0.00001697
Iteration 198/1000 | Loss: 0.00001697
Iteration 199/1000 | Loss: 0.00001697
Iteration 200/1000 | Loss: 0.00001697
Iteration 201/1000 | Loss: 0.00001697
Iteration 202/1000 | Loss: 0.00001697
Iteration 203/1000 | Loss: 0.00001697
Iteration 204/1000 | Loss: 0.00001697
Iteration 205/1000 | Loss: 0.00001697
Iteration 206/1000 | Loss: 0.00001697
Iteration 207/1000 | Loss: 0.00001697
Iteration 208/1000 | Loss: 0.00001697
Iteration 209/1000 | Loss: 0.00001697
Iteration 210/1000 | Loss: 0.00001696
Iteration 211/1000 | Loss: 0.00001696
Iteration 212/1000 | Loss: 0.00001696
Iteration 213/1000 | Loss: 0.00001696
Iteration 214/1000 | Loss: 0.00001696
Iteration 215/1000 | Loss: 0.00001696
Iteration 216/1000 | Loss: 0.00001696
Iteration 217/1000 | Loss: 0.00001696
Iteration 218/1000 | Loss: 0.00001696
Iteration 219/1000 | Loss: 0.00001696
Iteration 220/1000 | Loss: 0.00001696
Iteration 221/1000 | Loss: 0.00001696
Iteration 222/1000 | Loss: 0.00001696
Iteration 223/1000 | Loss: 0.00001696
Iteration 224/1000 | Loss: 0.00001696
Iteration 225/1000 | Loss: 0.00001696
Iteration 226/1000 | Loss: 0.00001696
Iteration 227/1000 | Loss: 0.00001696
Iteration 228/1000 | Loss: 0.00001696
Iteration 229/1000 | Loss: 0.00001696
Iteration 230/1000 | Loss: 0.00001696
Iteration 231/1000 | Loss: 0.00001695
Iteration 232/1000 | Loss: 0.00001695
Iteration 233/1000 | Loss: 0.00001695
Iteration 234/1000 | Loss: 0.00001695
Iteration 235/1000 | Loss: 0.00001695
Iteration 236/1000 | Loss: 0.00001695
Iteration 237/1000 | Loss: 0.00001695
Iteration 238/1000 | Loss: 0.00001695
Iteration 239/1000 | Loss: 0.00001695
Iteration 240/1000 | Loss: 0.00001695
Iteration 241/1000 | Loss: 0.00001695
Iteration 242/1000 | Loss: 0.00001695
Iteration 243/1000 | Loss: 0.00001695
Iteration 244/1000 | Loss: 0.00001695
Iteration 245/1000 | Loss: 0.00001695
Iteration 246/1000 | Loss: 0.00001695
Iteration 247/1000 | Loss: 0.00001695
Iteration 248/1000 | Loss: 0.00001695
Iteration 249/1000 | Loss: 0.00001695
Iteration 250/1000 | Loss: 0.00001695
Iteration 251/1000 | Loss: 0.00001695
Iteration 252/1000 | Loss: 0.00001695
Iteration 253/1000 | Loss: 0.00001695
Iteration 254/1000 | Loss: 0.00001695
Iteration 255/1000 | Loss: 0.00001695
Iteration 256/1000 | Loss: 0.00001695
Iteration 257/1000 | Loss: 0.00001695
Iteration 258/1000 | Loss: 0.00001695
Iteration 259/1000 | Loss: 0.00001695
Iteration 260/1000 | Loss: 0.00001695
Iteration 261/1000 | Loss: 0.00001695
Iteration 262/1000 | Loss: 0.00001695
Iteration 263/1000 | Loss: 0.00001695
Iteration 264/1000 | Loss: 0.00001695
Iteration 265/1000 | Loss: 0.00001695
Iteration 266/1000 | Loss: 0.00001695
Iteration 267/1000 | Loss: 0.00001695
Iteration 268/1000 | Loss: 0.00001695
Iteration 269/1000 | Loss: 0.00001695
Iteration 270/1000 | Loss: 0.00001695
Iteration 271/1000 | Loss: 0.00001695
Iteration 272/1000 | Loss: 0.00001695
Iteration 273/1000 | Loss: 0.00001695
Iteration 274/1000 | Loss: 0.00001695
Iteration 275/1000 | Loss: 0.00001695
Iteration 276/1000 | Loss: 0.00001695
Iteration 277/1000 | Loss: 0.00001695
Iteration 278/1000 | Loss: 0.00001695
Iteration 279/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.6948151824180968e-05, 1.6948151824180968e-05, 1.6948151824180968e-05, 1.6948151824180968e-05, 1.6948151824180968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6948151824180968e-05

Optimization complete. Final v2v error: 2.914120674133301 mm

Highest mean error: 20.603069305419922 mm for frame 6

Lowest mean error: 2.092801570892334 mm for frame 48

Saving results

Total time: 274.42659640312195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094383
Iteration 2/25 | Loss: 0.00144744
Iteration 3/25 | Loss: 0.00113818
Iteration 4/25 | Loss: 0.00106424
Iteration 5/25 | Loss: 0.00105600
Iteration 6/25 | Loss: 0.00106091
Iteration 7/25 | Loss: 0.00104974
Iteration 8/25 | Loss: 0.00104717
Iteration 9/25 | Loss: 0.00104685
Iteration 10/25 | Loss: 0.00104681
Iteration 11/25 | Loss: 0.00104681
Iteration 12/25 | Loss: 0.00104680
Iteration 13/25 | Loss: 0.00104680
Iteration 14/25 | Loss: 0.00104680
Iteration 15/25 | Loss: 0.00104680
Iteration 16/25 | Loss: 0.00104680
Iteration 17/25 | Loss: 0.00104680
Iteration 18/25 | Loss: 0.00104680
Iteration 19/25 | Loss: 0.00104680
Iteration 20/25 | Loss: 0.00104680
Iteration 21/25 | Loss: 0.00104680
Iteration 22/25 | Loss: 0.00104679
Iteration 23/25 | Loss: 0.00104679
Iteration 24/25 | Loss: 0.00104678
Iteration 25/25 | Loss: 0.00104678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28083527
Iteration 2/25 | Loss: 0.00175154
Iteration 3/25 | Loss: 0.00175154
Iteration 4/25 | Loss: 0.00175154
Iteration 5/25 | Loss: 0.00175154
Iteration 6/25 | Loss: 0.00175154
Iteration 7/25 | Loss: 0.00175154
Iteration 8/25 | Loss: 0.00175154
Iteration 9/25 | Loss: 0.00175154
Iteration 10/25 | Loss: 0.00175154
Iteration 11/25 | Loss: 0.00175154
Iteration 12/25 | Loss: 0.00175154
Iteration 13/25 | Loss: 0.00175154
Iteration 14/25 | Loss: 0.00175154
Iteration 15/25 | Loss: 0.00175154
Iteration 16/25 | Loss: 0.00175154
Iteration 17/25 | Loss: 0.00175154
Iteration 18/25 | Loss: 0.00175154
Iteration 19/25 | Loss: 0.00175154
Iteration 20/25 | Loss: 0.00175154
Iteration 21/25 | Loss: 0.00175154
Iteration 22/25 | Loss: 0.00175154
Iteration 23/25 | Loss: 0.00175154
Iteration 24/25 | Loss: 0.00175154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017515365034341812, 0.0017515365034341812, 0.0017515365034341812, 0.0017515365034341812, 0.0017515365034341812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017515365034341812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175154
Iteration 2/1000 | Loss: 0.00002839
Iteration 3/1000 | Loss: 0.00009204
Iteration 4/1000 | Loss: 0.00001515
Iteration 5/1000 | Loss: 0.00001416
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001337
Iteration 8/1000 | Loss: 0.00006679
Iteration 9/1000 | Loss: 0.00001286
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001250
Iteration 13/1000 | Loss: 0.00001241
Iteration 14/1000 | Loss: 0.00001240
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001229
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001221
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001216
Iteration 47/1000 | Loss: 0.00001216
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001215
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001215
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001215
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001213
Iteration 62/1000 | Loss: 0.00001213
Iteration 63/1000 | Loss: 0.00001213
Iteration 64/1000 | Loss: 0.00001213
Iteration 65/1000 | Loss: 0.00001213
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001213
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001211
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001208
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001207
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001204
Iteration 113/1000 | Loss: 0.00001204
Iteration 114/1000 | Loss: 0.00001204
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001201
Iteration 119/1000 | Loss: 0.00001201
Iteration 120/1000 | Loss: 0.00001201
Iteration 121/1000 | Loss: 0.00001200
Iteration 122/1000 | Loss: 0.00001200
Iteration 123/1000 | Loss: 0.00001199
Iteration 124/1000 | Loss: 0.00001199
Iteration 125/1000 | Loss: 0.00001199
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001197
Iteration 135/1000 | Loss: 0.00001197
Iteration 136/1000 | Loss: 0.00001197
Iteration 137/1000 | Loss: 0.00001197
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001196
Iteration 140/1000 | Loss: 0.00001196
Iteration 141/1000 | Loss: 0.00001196
Iteration 142/1000 | Loss: 0.00001196
Iteration 143/1000 | Loss: 0.00001196
Iteration 144/1000 | Loss: 0.00001196
Iteration 145/1000 | Loss: 0.00001196
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001195
Iteration 148/1000 | Loss: 0.00001195
Iteration 149/1000 | Loss: 0.00001195
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001194
Iteration 152/1000 | Loss: 0.00001194
Iteration 153/1000 | Loss: 0.00001194
Iteration 154/1000 | Loss: 0.00001194
Iteration 155/1000 | Loss: 0.00001194
Iteration 156/1000 | Loss: 0.00001194
Iteration 157/1000 | Loss: 0.00001194
Iteration 158/1000 | Loss: 0.00001194
Iteration 159/1000 | Loss: 0.00001194
Iteration 160/1000 | Loss: 0.00001193
Iteration 161/1000 | Loss: 0.00001193
Iteration 162/1000 | Loss: 0.00001193
Iteration 163/1000 | Loss: 0.00001193
Iteration 164/1000 | Loss: 0.00001193
Iteration 165/1000 | Loss: 0.00001193
Iteration 166/1000 | Loss: 0.00001193
Iteration 167/1000 | Loss: 0.00001192
Iteration 168/1000 | Loss: 0.00001192
Iteration 169/1000 | Loss: 0.00001192
Iteration 170/1000 | Loss: 0.00001192
Iteration 171/1000 | Loss: 0.00001192
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001192
Iteration 174/1000 | Loss: 0.00001192
Iteration 175/1000 | Loss: 0.00001192
Iteration 176/1000 | Loss: 0.00001192
Iteration 177/1000 | Loss: 0.00001192
Iteration 178/1000 | Loss: 0.00001192
Iteration 179/1000 | Loss: 0.00001192
Iteration 180/1000 | Loss: 0.00001192
Iteration 181/1000 | Loss: 0.00001192
Iteration 182/1000 | Loss: 0.00001192
Iteration 183/1000 | Loss: 0.00001192
Iteration 184/1000 | Loss: 0.00001192
Iteration 185/1000 | Loss: 0.00001192
Iteration 186/1000 | Loss: 0.00001192
Iteration 187/1000 | Loss: 0.00001192
Iteration 188/1000 | Loss: 0.00001192
Iteration 189/1000 | Loss: 0.00001192
Iteration 190/1000 | Loss: 0.00001192
Iteration 191/1000 | Loss: 0.00001192
Iteration 192/1000 | Loss: 0.00001192
Iteration 193/1000 | Loss: 0.00001192
Iteration 194/1000 | Loss: 0.00001192
Iteration 195/1000 | Loss: 0.00001192
Iteration 196/1000 | Loss: 0.00001192
Iteration 197/1000 | Loss: 0.00001192
Iteration 198/1000 | Loss: 0.00001192
Iteration 199/1000 | Loss: 0.00001192
Iteration 200/1000 | Loss: 0.00001192
Iteration 201/1000 | Loss: 0.00001192
Iteration 202/1000 | Loss: 0.00001192
Iteration 203/1000 | Loss: 0.00001192
Iteration 204/1000 | Loss: 0.00001192
Iteration 205/1000 | Loss: 0.00001192
Iteration 206/1000 | Loss: 0.00001192
Iteration 207/1000 | Loss: 0.00001192
Iteration 208/1000 | Loss: 0.00001192
Iteration 209/1000 | Loss: 0.00001192
Iteration 210/1000 | Loss: 0.00001192
Iteration 211/1000 | Loss: 0.00001192
Iteration 212/1000 | Loss: 0.00001192
Iteration 213/1000 | Loss: 0.00001192
Iteration 214/1000 | Loss: 0.00001192
Iteration 215/1000 | Loss: 0.00001192
Iteration 216/1000 | Loss: 0.00001192
Iteration 217/1000 | Loss: 0.00001192
Iteration 218/1000 | Loss: 0.00001192
Iteration 219/1000 | Loss: 0.00001192
Iteration 220/1000 | Loss: 0.00001192
Iteration 221/1000 | Loss: 0.00001192
Iteration 222/1000 | Loss: 0.00001192
Iteration 223/1000 | Loss: 0.00001192
Iteration 224/1000 | Loss: 0.00001192
Iteration 225/1000 | Loss: 0.00001192
Iteration 226/1000 | Loss: 0.00001192
Iteration 227/1000 | Loss: 0.00001192
Iteration 228/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.1918973541469313e-05, 1.1918973541469313e-05, 1.1918973541469313e-05, 1.1918973541469313e-05, 1.1918973541469313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1918973541469313e-05

Optimization complete. Final v2v error: 2.9290497303009033 mm

Highest mean error: 3.478642225265503 mm for frame 20

Lowest mean error: 2.6970267295837402 mm for frame 98

Saving results

Total time: 47.27204442024231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879643
Iteration 2/25 | Loss: 0.00122208
Iteration 3/25 | Loss: 0.00112069
Iteration 4/25 | Loss: 0.00110050
Iteration 5/25 | Loss: 0.00109156
Iteration 6/25 | Loss: 0.00108880
Iteration 7/25 | Loss: 0.00108880
Iteration 8/25 | Loss: 0.00108880
Iteration 9/25 | Loss: 0.00108880
Iteration 10/25 | Loss: 0.00108880
Iteration 11/25 | Loss: 0.00108880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010887967655435205, 0.0010887967655435205, 0.0010887967655435205, 0.0010887967655435205, 0.0010887967655435205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010887967655435205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27012920
Iteration 2/25 | Loss: 0.00186374
Iteration 3/25 | Loss: 0.00186374
Iteration 4/25 | Loss: 0.00186374
Iteration 5/25 | Loss: 0.00186374
Iteration 6/25 | Loss: 0.00186373
Iteration 7/25 | Loss: 0.00186374
Iteration 8/25 | Loss: 0.00186373
Iteration 9/25 | Loss: 0.00186373
Iteration 10/25 | Loss: 0.00186373
Iteration 11/25 | Loss: 0.00186373
Iteration 12/25 | Loss: 0.00186373
Iteration 13/25 | Loss: 0.00186373
Iteration 14/25 | Loss: 0.00186373
Iteration 15/25 | Loss: 0.00186373
Iteration 16/25 | Loss: 0.00186373
Iteration 17/25 | Loss: 0.00186373
Iteration 18/25 | Loss: 0.00186373
Iteration 19/25 | Loss: 0.00186373
Iteration 20/25 | Loss: 0.00186373
Iteration 21/25 | Loss: 0.00186373
Iteration 22/25 | Loss: 0.00186373
Iteration 23/25 | Loss: 0.00186373
Iteration 24/25 | Loss: 0.00186373
Iteration 25/25 | Loss: 0.00186373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186373
Iteration 2/1000 | Loss: 0.00003083
Iteration 3/1000 | Loss: 0.00002271
Iteration 4/1000 | Loss: 0.00002114
Iteration 5/1000 | Loss: 0.00002036
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001938
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001879
Iteration 10/1000 | Loss: 0.00001877
Iteration 11/1000 | Loss: 0.00001876
Iteration 12/1000 | Loss: 0.00001864
Iteration 13/1000 | Loss: 0.00001863
Iteration 14/1000 | Loss: 0.00001863
Iteration 15/1000 | Loss: 0.00001863
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001858
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001852
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001852
Iteration 29/1000 | Loss: 0.00001852
Iteration 30/1000 | Loss: 0.00001851
Iteration 31/1000 | Loss: 0.00001851
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001850
Iteration 34/1000 | Loss: 0.00001850
Iteration 35/1000 | Loss: 0.00001849
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001848
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001848
Iteration 43/1000 | Loss: 0.00001848
Iteration 44/1000 | Loss: 0.00001848
Iteration 45/1000 | Loss: 0.00001848
Iteration 46/1000 | Loss: 0.00001848
Iteration 47/1000 | Loss: 0.00001847
Iteration 48/1000 | Loss: 0.00001847
Iteration 49/1000 | Loss: 0.00001847
Iteration 50/1000 | Loss: 0.00001847
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001846
Iteration 53/1000 | Loss: 0.00001846
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001845
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001845
Iteration 62/1000 | Loss: 0.00001845
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001843
Iteration 67/1000 | Loss: 0.00001843
Iteration 68/1000 | Loss: 0.00001843
Iteration 69/1000 | Loss: 0.00001843
Iteration 70/1000 | Loss: 0.00001843
Iteration 71/1000 | Loss: 0.00001843
Iteration 72/1000 | Loss: 0.00001843
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001840
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001840
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001840
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001839
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001839
Iteration 105/1000 | Loss: 0.00001839
Iteration 106/1000 | Loss: 0.00001839
Iteration 107/1000 | Loss: 0.00001839
Iteration 108/1000 | Loss: 0.00001839
Iteration 109/1000 | Loss: 0.00001839
Iteration 110/1000 | Loss: 0.00001839
Iteration 111/1000 | Loss: 0.00001839
Iteration 112/1000 | Loss: 0.00001839
Iteration 113/1000 | Loss: 0.00001839
Iteration 114/1000 | Loss: 0.00001839
Iteration 115/1000 | Loss: 0.00001839
Iteration 116/1000 | Loss: 0.00001839
Iteration 117/1000 | Loss: 0.00001839
Iteration 118/1000 | Loss: 0.00001839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.838633033912629e-05, 1.838633033912629e-05, 1.838633033912629e-05, 1.838633033912629e-05, 1.838633033912629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.838633033912629e-05

Optimization complete. Final v2v error: 3.578932285308838 mm

Highest mean error: 3.857607364654541 mm for frame 113

Lowest mean error: 3.248368978500366 mm for frame 5

Saving results

Total time: 33.694830894470215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548615
Iteration 2/25 | Loss: 0.00072897
Iteration 3/25 | Loss: 0.00062115
Iteration 4/25 | Loss: 0.00060075
Iteration 5/25 | Loss: 0.00059606
Iteration 6/25 | Loss: 0.00059492
Iteration 7/25 | Loss: 0.00059454
Iteration 8/25 | Loss: 0.00059451
Iteration 9/25 | Loss: 0.00059451
Iteration 10/25 | Loss: 0.00059451
Iteration 11/25 | Loss: 0.00059451
Iteration 12/25 | Loss: 0.00059451
Iteration 13/25 | Loss: 0.00059451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005945117445662618, 0.0005945117445662618, 0.0005945117445662618, 0.0005945117445662618, 0.0005945117445662618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005945117445662618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.25730085
Iteration 2/25 | Loss: 0.00052851
Iteration 3/25 | Loss: 0.00052850
Iteration 4/25 | Loss: 0.00052850
Iteration 5/25 | Loss: 0.00052850
Iteration 6/25 | Loss: 0.00052850
Iteration 7/25 | Loss: 0.00052850
Iteration 8/25 | Loss: 0.00052850
Iteration 9/25 | Loss: 0.00052850
Iteration 10/25 | Loss: 0.00052850
Iteration 11/25 | Loss: 0.00052850
Iteration 12/25 | Loss: 0.00052850
Iteration 13/25 | Loss: 0.00052850
Iteration 14/25 | Loss: 0.00052850
Iteration 15/25 | Loss: 0.00052850
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005284983199089766, 0.0005284983199089766, 0.0005284983199089766, 0.0005284983199089766, 0.0005284983199089766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005284983199089766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052850
Iteration 2/1000 | Loss: 0.00002844
Iteration 3/1000 | Loss: 0.00002097
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001880
Iteration 6/1000 | Loss: 0.00001836
Iteration 7/1000 | Loss: 0.00001791
Iteration 8/1000 | Loss: 0.00001771
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001752
Iteration 11/1000 | Loss: 0.00001741
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001731
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001728
Iteration 17/1000 | Loss: 0.00001725
Iteration 18/1000 | Loss: 0.00001725
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001721
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001718
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001710
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001709
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001702
Iteration 59/1000 | Loss: 0.00001702
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001698
Iteration 67/1000 | Loss: 0.00001698
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001696
Iteration 72/1000 | Loss: 0.00001696
Iteration 73/1000 | Loss: 0.00001696
Iteration 74/1000 | Loss: 0.00001695
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001695
Iteration 77/1000 | Loss: 0.00001694
Iteration 78/1000 | Loss: 0.00001694
Iteration 79/1000 | Loss: 0.00001694
Iteration 80/1000 | Loss: 0.00001694
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001692
Iteration 86/1000 | Loss: 0.00001692
Iteration 87/1000 | Loss: 0.00001692
Iteration 88/1000 | Loss: 0.00001692
Iteration 89/1000 | Loss: 0.00001692
Iteration 90/1000 | Loss: 0.00001692
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001691
Iteration 97/1000 | Loss: 0.00001691
Iteration 98/1000 | Loss: 0.00001691
Iteration 99/1000 | Loss: 0.00001691
Iteration 100/1000 | Loss: 0.00001691
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00001689
Iteration 119/1000 | Loss: 0.00001689
Iteration 120/1000 | Loss: 0.00001689
Iteration 121/1000 | Loss: 0.00001689
Iteration 122/1000 | Loss: 0.00001688
Iteration 123/1000 | Loss: 0.00001688
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001688
Iteration 129/1000 | Loss: 0.00001688
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001687
Iteration 141/1000 | Loss: 0.00001687
Iteration 142/1000 | Loss: 0.00001687
Iteration 143/1000 | Loss: 0.00001687
Iteration 144/1000 | Loss: 0.00001687
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001686
Iteration 148/1000 | Loss: 0.00001686
Iteration 149/1000 | Loss: 0.00001686
Iteration 150/1000 | Loss: 0.00001686
Iteration 151/1000 | Loss: 0.00001686
Iteration 152/1000 | Loss: 0.00001686
Iteration 153/1000 | Loss: 0.00001685
Iteration 154/1000 | Loss: 0.00001685
Iteration 155/1000 | Loss: 0.00001685
Iteration 156/1000 | Loss: 0.00001685
Iteration 157/1000 | Loss: 0.00001685
Iteration 158/1000 | Loss: 0.00001685
Iteration 159/1000 | Loss: 0.00001685
Iteration 160/1000 | Loss: 0.00001685
Iteration 161/1000 | Loss: 0.00001685
Iteration 162/1000 | Loss: 0.00001685
Iteration 163/1000 | Loss: 0.00001685
Iteration 164/1000 | Loss: 0.00001685
Iteration 165/1000 | Loss: 0.00001685
Iteration 166/1000 | Loss: 0.00001685
Iteration 167/1000 | Loss: 0.00001685
Iteration 168/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.6848851373651996e-05, 1.6848851373651996e-05, 1.6848851373651996e-05, 1.6848851373651996e-05, 1.6848851373651996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6848851373651996e-05

Optimization complete. Final v2v error: 3.4762842655181885 mm

Highest mean error: 4.112711429595947 mm for frame 57

Lowest mean error: 3.018202781677246 mm for frame 1

Saving results

Total time: 37.19220685958862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730508
Iteration 2/25 | Loss: 0.00123453
Iteration 3/25 | Loss: 0.00089310
Iteration 4/25 | Loss: 0.00081028
Iteration 5/25 | Loss: 0.00078825
Iteration 6/25 | Loss: 0.00073874
Iteration 7/25 | Loss: 0.00072743
Iteration 8/25 | Loss: 0.00072682
Iteration 9/25 | Loss: 0.00072485
Iteration 10/25 | Loss: 0.00072450
Iteration 11/25 | Loss: 0.00072414
Iteration 12/25 | Loss: 0.00072375
Iteration 13/25 | Loss: 0.00072308
Iteration 14/25 | Loss: 0.00073261
Iteration 15/25 | Loss: 0.00071836
Iteration 16/25 | Loss: 0.00071215
Iteration 17/25 | Loss: 0.00071092
Iteration 18/25 | Loss: 0.00071069
Iteration 19/25 | Loss: 0.00071046
Iteration 20/25 | Loss: 0.00071026
Iteration 21/25 | Loss: 0.00071008
Iteration 22/25 | Loss: 0.00071579
Iteration 23/25 | Loss: 0.00071354
Iteration 24/25 | Loss: 0.00071349
Iteration 25/25 | Loss: 0.00070727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56792355
Iteration 2/25 | Loss: 0.00108576
Iteration 3/25 | Loss: 0.00108576
Iteration 4/25 | Loss: 0.00108576
Iteration 5/25 | Loss: 0.00108576
Iteration 6/25 | Loss: 0.00108576
Iteration 7/25 | Loss: 0.00108576
Iteration 8/25 | Loss: 0.00108576
Iteration 9/25 | Loss: 0.00108576
Iteration 10/25 | Loss: 0.00108576
Iteration 11/25 | Loss: 0.00108576
Iteration 12/25 | Loss: 0.00108576
Iteration 13/25 | Loss: 0.00108576
Iteration 14/25 | Loss: 0.00108576
Iteration 15/25 | Loss: 0.00108576
Iteration 16/25 | Loss: 0.00108576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010857557645067573, 0.0010857557645067573, 0.0010857557645067573, 0.0010857557645067573, 0.0010857557645067573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010857557645067573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108576
Iteration 2/1000 | Loss: 0.00060560
Iteration 3/1000 | Loss: 0.00049730
Iteration 4/1000 | Loss: 0.00059934
Iteration 5/1000 | Loss: 0.00035158
Iteration 6/1000 | Loss: 0.00069966
Iteration 7/1000 | Loss: 0.00004405
Iteration 8/1000 | Loss: 0.00003669
Iteration 9/1000 | Loss: 0.00003055
Iteration 10/1000 | Loss: 0.00002770
Iteration 11/1000 | Loss: 0.00002558
Iteration 12/1000 | Loss: 0.00002449
Iteration 13/1000 | Loss: 0.00002377
Iteration 14/1000 | Loss: 0.00002329
Iteration 15/1000 | Loss: 0.00002271
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002198
Iteration 19/1000 | Loss: 0.00002188
Iteration 20/1000 | Loss: 0.00002184
Iteration 21/1000 | Loss: 0.00002184
Iteration 22/1000 | Loss: 0.00002178
Iteration 23/1000 | Loss: 0.00002178
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002174
Iteration 26/1000 | Loss: 0.00002170
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002166
Iteration 29/1000 | Loss: 0.00002165
Iteration 30/1000 | Loss: 0.00002165
Iteration 31/1000 | Loss: 0.00002165
Iteration 32/1000 | Loss: 0.00002164
Iteration 33/1000 | Loss: 0.00002164
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002163
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002162
Iteration 38/1000 | Loss: 0.00002162
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002161
Iteration 41/1000 | Loss: 0.00002160
Iteration 42/1000 | Loss: 0.00002160
Iteration 43/1000 | Loss: 0.00002160
Iteration 44/1000 | Loss: 0.00002159
Iteration 45/1000 | Loss: 0.00002158
Iteration 46/1000 | Loss: 0.00002158
Iteration 47/1000 | Loss: 0.00002157
Iteration 48/1000 | Loss: 0.00002157
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002157
Iteration 51/1000 | Loss: 0.00002156
Iteration 52/1000 | Loss: 0.00002156
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002155
Iteration 55/1000 | Loss: 0.00002155
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002154
Iteration 58/1000 | Loss: 0.00002153
Iteration 59/1000 | Loss: 0.00002152
Iteration 60/1000 | Loss: 0.00002152
Iteration 61/1000 | Loss: 0.00002150
Iteration 62/1000 | Loss: 0.00002149
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002149
Iteration 65/1000 | Loss: 0.00002149
Iteration 66/1000 | Loss: 0.00002148
Iteration 67/1000 | Loss: 0.00002148
Iteration 68/1000 | Loss: 0.00002148
Iteration 69/1000 | Loss: 0.00002148
Iteration 70/1000 | Loss: 0.00002148
Iteration 71/1000 | Loss: 0.00002148
Iteration 72/1000 | Loss: 0.00002148
Iteration 73/1000 | Loss: 0.00002148
Iteration 74/1000 | Loss: 0.00002148
Iteration 75/1000 | Loss: 0.00002148
Iteration 76/1000 | Loss: 0.00002147
Iteration 77/1000 | Loss: 0.00002147
Iteration 78/1000 | Loss: 0.00002147
Iteration 79/1000 | Loss: 0.00002147
Iteration 80/1000 | Loss: 0.00002147
Iteration 81/1000 | Loss: 0.00002147
Iteration 82/1000 | Loss: 0.00002147
Iteration 83/1000 | Loss: 0.00002147
Iteration 84/1000 | Loss: 0.00002147
Iteration 85/1000 | Loss: 0.00002147
Iteration 86/1000 | Loss: 0.00002146
Iteration 87/1000 | Loss: 0.00002146
Iteration 88/1000 | Loss: 0.00002146
Iteration 89/1000 | Loss: 0.00002146
Iteration 90/1000 | Loss: 0.00002146
Iteration 91/1000 | Loss: 0.00002146
Iteration 92/1000 | Loss: 0.00002146
Iteration 93/1000 | Loss: 0.00002146
Iteration 94/1000 | Loss: 0.00002146
Iteration 95/1000 | Loss: 0.00002146
Iteration 96/1000 | Loss: 0.00002146
Iteration 97/1000 | Loss: 0.00002146
Iteration 98/1000 | Loss: 0.00002145
Iteration 99/1000 | Loss: 0.00002145
Iteration 100/1000 | Loss: 0.00002145
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002145
Iteration 107/1000 | Loss: 0.00002145
Iteration 108/1000 | Loss: 0.00002145
Iteration 109/1000 | Loss: 0.00002145
Iteration 110/1000 | Loss: 0.00002145
Iteration 111/1000 | Loss: 0.00002145
Iteration 112/1000 | Loss: 0.00002145
Iteration 113/1000 | Loss: 0.00002145
Iteration 114/1000 | Loss: 0.00002145
Iteration 115/1000 | Loss: 0.00002145
Iteration 116/1000 | Loss: 0.00002145
Iteration 117/1000 | Loss: 0.00002145
Iteration 118/1000 | Loss: 0.00002145
Iteration 119/1000 | Loss: 0.00002145
Iteration 120/1000 | Loss: 0.00002145
Iteration 121/1000 | Loss: 0.00002145
Iteration 122/1000 | Loss: 0.00002145
Iteration 123/1000 | Loss: 0.00002145
Iteration 124/1000 | Loss: 0.00002145
Iteration 125/1000 | Loss: 0.00002145
Iteration 126/1000 | Loss: 0.00002145
Iteration 127/1000 | Loss: 0.00002145
Iteration 128/1000 | Loss: 0.00002145
Iteration 129/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.1446048776851967e-05, 2.1446048776851967e-05, 2.1446048776851967e-05, 2.1446048776851967e-05, 2.1446048776851967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1446048776851967e-05

Optimization complete. Final v2v error: 3.8707985877990723 mm

Highest mean error: 4.481224536895752 mm for frame 134

Lowest mean error: 3.3907968997955322 mm for frame 213

Saving results

Total time: 88.82234382629395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126756
Iteration 2/25 | Loss: 0.00322871
Iteration 3/25 | Loss: 0.00232572
Iteration 4/25 | Loss: 0.00201347
Iteration 5/25 | Loss: 0.00154449
Iteration 6/25 | Loss: 0.00139962
Iteration 7/25 | Loss: 0.00114805
Iteration 8/25 | Loss: 0.00101876
Iteration 9/25 | Loss: 0.00095860
Iteration 10/25 | Loss: 0.00089130
Iteration 11/25 | Loss: 0.00082158
Iteration 12/25 | Loss: 0.00079664
Iteration 13/25 | Loss: 0.00079921
Iteration 14/25 | Loss: 0.00076782
Iteration 15/25 | Loss: 0.00074875
Iteration 16/25 | Loss: 0.00074132
Iteration 17/25 | Loss: 0.00074047
Iteration 18/25 | Loss: 0.00073752
Iteration 19/25 | Loss: 0.00073399
Iteration 20/25 | Loss: 0.00073013
Iteration 21/25 | Loss: 0.00072549
Iteration 22/25 | Loss: 0.00073196
Iteration 23/25 | Loss: 0.00073205
Iteration 24/25 | Loss: 0.00073799
Iteration 25/25 | Loss: 0.00073209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44564211
Iteration 2/25 | Loss: 0.00086342
Iteration 3/25 | Loss: 0.00086341
Iteration 4/25 | Loss: 0.00086341
Iteration 5/25 | Loss: 0.00086341
Iteration 6/25 | Loss: 0.00086341
Iteration 7/25 | Loss: 0.00086341
Iteration 8/25 | Loss: 0.00086341
Iteration 9/25 | Loss: 0.00086341
Iteration 10/25 | Loss: 0.00086341
Iteration 11/25 | Loss: 0.00086341
Iteration 12/25 | Loss: 0.00086341
Iteration 13/25 | Loss: 0.00086341
Iteration 14/25 | Loss: 0.00086341
Iteration 15/25 | Loss: 0.00086341
Iteration 16/25 | Loss: 0.00086341
Iteration 17/25 | Loss: 0.00086341
Iteration 18/25 | Loss: 0.00086341
Iteration 19/25 | Loss: 0.00086341
Iteration 20/25 | Loss: 0.00086341
Iteration 21/25 | Loss: 0.00086341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008634095429442823, 0.0008634095429442823, 0.0008634095429442823, 0.0008634095429442823, 0.0008634095429442823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008634095429442823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086341
Iteration 2/1000 | Loss: 0.00034191
Iteration 3/1000 | Loss: 0.00029572
Iteration 4/1000 | Loss: 0.00038083
Iteration 5/1000 | Loss: 0.00045340
Iteration 6/1000 | Loss: 0.00046315
Iteration 7/1000 | Loss: 0.00038441
Iteration 8/1000 | Loss: 0.00047287
Iteration 9/1000 | Loss: 0.00058365
Iteration 10/1000 | Loss: 0.00024549
Iteration 11/1000 | Loss: 0.00065304
Iteration 12/1000 | Loss: 0.00054154
Iteration 13/1000 | Loss: 0.00048269
Iteration 14/1000 | Loss: 0.00052543
Iteration 15/1000 | Loss: 0.00066546
Iteration 16/1000 | Loss: 0.00046421
Iteration 17/1000 | Loss: 0.00059911
Iteration 18/1000 | Loss: 0.00047346
Iteration 19/1000 | Loss: 0.00057582
Iteration 20/1000 | Loss: 0.00047116
Iteration 21/1000 | Loss: 0.00077193
Iteration 22/1000 | Loss: 0.00051457
Iteration 23/1000 | Loss: 0.00066418
Iteration 24/1000 | Loss: 0.00037290
Iteration 25/1000 | Loss: 0.00018676
Iteration 26/1000 | Loss: 0.00010819
Iteration 27/1000 | Loss: 0.00007202
Iteration 28/1000 | Loss: 0.00018455
Iteration 29/1000 | Loss: 0.00023041
Iteration 30/1000 | Loss: 0.00022163
Iteration 31/1000 | Loss: 0.00028693
Iteration 32/1000 | Loss: 0.00025671
Iteration 33/1000 | Loss: 0.00021051
Iteration 34/1000 | Loss: 0.00027103
Iteration 35/1000 | Loss: 0.00042463
Iteration 36/1000 | Loss: 0.00014437
Iteration 37/1000 | Loss: 0.00011427
Iteration 38/1000 | Loss: 0.00011964
Iteration 39/1000 | Loss: 0.00009414
Iteration 40/1000 | Loss: 0.00030486
Iteration 41/1000 | Loss: 0.00010977
Iteration 42/1000 | Loss: 0.00024247
Iteration 43/1000 | Loss: 0.00042486
Iteration 44/1000 | Loss: 0.00032779
Iteration 45/1000 | Loss: 0.00009574
Iteration 46/1000 | Loss: 0.00014559
Iteration 47/1000 | Loss: 0.00010651
Iteration 48/1000 | Loss: 0.00011048
Iteration 49/1000 | Loss: 0.00019911
Iteration 50/1000 | Loss: 0.00034382
Iteration 51/1000 | Loss: 0.00019671
Iteration 52/1000 | Loss: 0.00026700
Iteration 53/1000 | Loss: 0.00011401
Iteration 54/1000 | Loss: 0.00013114
Iteration 55/1000 | Loss: 0.00010457
Iteration 56/1000 | Loss: 0.00034212
Iteration 57/1000 | Loss: 0.00014180
Iteration 58/1000 | Loss: 0.00025725
Iteration 59/1000 | Loss: 0.00008472
Iteration 60/1000 | Loss: 0.00014609
Iteration 61/1000 | Loss: 0.00011162
Iteration 62/1000 | Loss: 0.00012932
Iteration 63/1000 | Loss: 0.00008504
Iteration 64/1000 | Loss: 0.00027361
Iteration 65/1000 | Loss: 0.00017246
Iteration 66/1000 | Loss: 0.00176002
Iteration 67/1000 | Loss: 0.00187494
Iteration 68/1000 | Loss: 0.00067345
Iteration 69/1000 | Loss: 0.00129499
Iteration 70/1000 | Loss: 0.00140799
Iteration 71/1000 | Loss: 0.00064583
Iteration 72/1000 | Loss: 0.00066920
Iteration 73/1000 | Loss: 0.00032928
Iteration 74/1000 | Loss: 0.00025691
Iteration 75/1000 | Loss: 0.00027140
Iteration 76/1000 | Loss: 0.00014907
Iteration 77/1000 | Loss: 0.00004175
Iteration 78/1000 | Loss: 0.00003780
Iteration 79/1000 | Loss: 0.00015975
Iteration 80/1000 | Loss: 0.00004907
Iteration 81/1000 | Loss: 0.00004081
Iteration 82/1000 | Loss: 0.00003620
Iteration 83/1000 | Loss: 0.00003338
Iteration 84/1000 | Loss: 0.00003051
Iteration 85/1000 | Loss: 0.00002865
Iteration 86/1000 | Loss: 0.00002769
Iteration 87/1000 | Loss: 0.00002709
Iteration 88/1000 | Loss: 0.00002650
Iteration 89/1000 | Loss: 0.00002604
Iteration 90/1000 | Loss: 0.00002556
Iteration 91/1000 | Loss: 0.00002520
Iteration 92/1000 | Loss: 0.00002500
Iteration 93/1000 | Loss: 0.00002489
Iteration 94/1000 | Loss: 0.00002485
Iteration 95/1000 | Loss: 0.00002484
Iteration 96/1000 | Loss: 0.00002483
Iteration 97/1000 | Loss: 0.00002482
Iteration 98/1000 | Loss: 0.00002482
Iteration 99/1000 | Loss: 0.00002482
Iteration 100/1000 | Loss: 0.00002481
Iteration 101/1000 | Loss: 0.00002481
Iteration 102/1000 | Loss: 0.00002481
Iteration 103/1000 | Loss: 0.00002480
Iteration 104/1000 | Loss: 0.00002480
Iteration 105/1000 | Loss: 0.00002477
Iteration 106/1000 | Loss: 0.00002477
Iteration 107/1000 | Loss: 0.00002476
Iteration 108/1000 | Loss: 0.00002475
Iteration 109/1000 | Loss: 0.00002475
Iteration 110/1000 | Loss: 0.00002475
Iteration 111/1000 | Loss: 0.00002474
Iteration 112/1000 | Loss: 0.00002473
Iteration 113/1000 | Loss: 0.00002471
Iteration 114/1000 | Loss: 0.00002471
Iteration 115/1000 | Loss: 0.00002470
Iteration 116/1000 | Loss: 0.00002470
Iteration 117/1000 | Loss: 0.00002469
Iteration 118/1000 | Loss: 0.00002469
Iteration 119/1000 | Loss: 0.00002469
Iteration 120/1000 | Loss: 0.00002469
Iteration 121/1000 | Loss: 0.00002468
Iteration 122/1000 | Loss: 0.00002467
Iteration 123/1000 | Loss: 0.00002467
Iteration 124/1000 | Loss: 0.00002467
Iteration 125/1000 | Loss: 0.00002466
Iteration 126/1000 | Loss: 0.00002466
Iteration 127/1000 | Loss: 0.00002466
Iteration 128/1000 | Loss: 0.00002465
Iteration 129/1000 | Loss: 0.00002465
Iteration 130/1000 | Loss: 0.00002464
Iteration 131/1000 | Loss: 0.00002463
Iteration 132/1000 | Loss: 0.00002462
Iteration 133/1000 | Loss: 0.00002462
Iteration 134/1000 | Loss: 0.00002462
Iteration 135/1000 | Loss: 0.00002461
Iteration 136/1000 | Loss: 0.00002461
Iteration 137/1000 | Loss: 0.00002460
Iteration 138/1000 | Loss: 0.00002460
Iteration 139/1000 | Loss: 0.00002460
Iteration 140/1000 | Loss: 0.00002460
Iteration 141/1000 | Loss: 0.00002460
Iteration 142/1000 | Loss: 0.00002460
Iteration 143/1000 | Loss: 0.00002460
Iteration 144/1000 | Loss: 0.00002459
Iteration 145/1000 | Loss: 0.00002459
Iteration 146/1000 | Loss: 0.00002459
Iteration 147/1000 | Loss: 0.00002459
Iteration 148/1000 | Loss: 0.00002459
Iteration 149/1000 | Loss: 0.00002459
Iteration 150/1000 | Loss: 0.00002458
Iteration 151/1000 | Loss: 0.00002458
Iteration 152/1000 | Loss: 0.00002458
Iteration 153/1000 | Loss: 0.00002458
Iteration 154/1000 | Loss: 0.00002458
Iteration 155/1000 | Loss: 0.00002458
Iteration 156/1000 | Loss: 0.00002457
Iteration 157/1000 | Loss: 0.00002457
Iteration 158/1000 | Loss: 0.00002457
Iteration 159/1000 | Loss: 0.00002457
Iteration 160/1000 | Loss: 0.00002456
Iteration 161/1000 | Loss: 0.00002456
Iteration 162/1000 | Loss: 0.00002456
Iteration 163/1000 | Loss: 0.00002455
Iteration 164/1000 | Loss: 0.00002455
Iteration 165/1000 | Loss: 0.00002455
Iteration 166/1000 | Loss: 0.00002454
Iteration 167/1000 | Loss: 0.00002454
Iteration 168/1000 | Loss: 0.00002454
Iteration 169/1000 | Loss: 0.00002453
Iteration 170/1000 | Loss: 0.00002453
Iteration 171/1000 | Loss: 0.00002453
Iteration 172/1000 | Loss: 0.00002452
Iteration 173/1000 | Loss: 0.00002452
Iteration 174/1000 | Loss: 0.00002452
Iteration 175/1000 | Loss: 0.00002451
Iteration 176/1000 | Loss: 0.00002451
Iteration 177/1000 | Loss: 0.00002451
Iteration 178/1000 | Loss: 0.00002451
Iteration 179/1000 | Loss: 0.00002451
Iteration 180/1000 | Loss: 0.00002451
Iteration 181/1000 | Loss: 0.00002451
Iteration 182/1000 | Loss: 0.00002451
Iteration 183/1000 | Loss: 0.00002451
Iteration 184/1000 | Loss: 0.00002450
Iteration 185/1000 | Loss: 0.00002450
Iteration 186/1000 | Loss: 0.00002450
Iteration 187/1000 | Loss: 0.00002450
Iteration 188/1000 | Loss: 0.00002450
Iteration 189/1000 | Loss: 0.00002450
Iteration 190/1000 | Loss: 0.00002450
Iteration 191/1000 | Loss: 0.00002450
Iteration 192/1000 | Loss: 0.00002450
Iteration 193/1000 | Loss: 0.00002450
Iteration 194/1000 | Loss: 0.00002450
Iteration 195/1000 | Loss: 0.00002450
Iteration 196/1000 | Loss: 0.00002450
Iteration 197/1000 | Loss: 0.00002450
Iteration 198/1000 | Loss: 0.00002450
Iteration 199/1000 | Loss: 0.00002450
Iteration 200/1000 | Loss: 0.00002450
Iteration 201/1000 | Loss: 0.00002450
Iteration 202/1000 | Loss: 0.00002450
Iteration 203/1000 | Loss: 0.00002450
Iteration 204/1000 | Loss: 0.00002450
Iteration 205/1000 | Loss: 0.00002450
Iteration 206/1000 | Loss: 0.00002450
Iteration 207/1000 | Loss: 0.00002450
Iteration 208/1000 | Loss: 0.00002450
Iteration 209/1000 | Loss: 0.00002450
Iteration 210/1000 | Loss: 0.00002450
Iteration 211/1000 | Loss: 0.00002450
Iteration 212/1000 | Loss: 0.00002450
Iteration 213/1000 | Loss: 0.00002450
Iteration 214/1000 | Loss: 0.00002450
Iteration 215/1000 | Loss: 0.00002450
Iteration 216/1000 | Loss: 0.00002450
Iteration 217/1000 | Loss: 0.00002450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.449523890390992e-05, 2.449523890390992e-05, 2.449523890390992e-05, 2.449523890390992e-05, 2.449523890390992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.449523890390992e-05

Optimization complete. Final v2v error: 4.157762050628662 mm

Highest mean error: 5.173132419586182 mm for frame 88

Lowest mean error: 3.80869197845459 mm for frame 158

Saving results

Total time: 185.86358451843262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063549
Iteration 2/25 | Loss: 0.01063549
Iteration 3/25 | Loss: 0.00262279
Iteration 4/25 | Loss: 0.00165323
Iteration 5/25 | Loss: 0.00145594
Iteration 6/25 | Loss: 0.00149930
Iteration 7/25 | Loss: 0.00137904
Iteration 8/25 | Loss: 0.00121767
Iteration 9/25 | Loss: 0.00104640
Iteration 10/25 | Loss: 0.00093432
Iteration 11/25 | Loss: 0.00087280
Iteration 12/25 | Loss: 0.00084067
Iteration 13/25 | Loss: 0.00083339
Iteration 14/25 | Loss: 0.00081755
Iteration 15/25 | Loss: 0.00080409
Iteration 16/25 | Loss: 0.00079599
Iteration 17/25 | Loss: 0.00079063
Iteration 18/25 | Loss: 0.00078148
Iteration 19/25 | Loss: 0.00077956
Iteration 20/25 | Loss: 0.00077964
Iteration 21/25 | Loss: 0.00077440
Iteration 22/25 | Loss: 0.00077128
Iteration 23/25 | Loss: 0.00076645
Iteration 24/25 | Loss: 0.00075704
Iteration 25/25 | Loss: 0.00075859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52015293
Iteration 2/25 | Loss: 0.00291252
Iteration 3/25 | Loss: 0.00282288
Iteration 4/25 | Loss: 0.00282288
Iteration 5/25 | Loss: 0.00282288
Iteration 6/25 | Loss: 0.00282288
Iteration 7/25 | Loss: 0.00282288
Iteration 8/25 | Loss: 0.00282288
Iteration 9/25 | Loss: 0.00282288
Iteration 10/25 | Loss: 0.00282288
Iteration 11/25 | Loss: 0.00282288
Iteration 12/25 | Loss: 0.00282288
Iteration 13/25 | Loss: 0.00282288
Iteration 14/25 | Loss: 0.00282288
Iteration 15/25 | Loss: 0.00282288
Iteration 16/25 | Loss: 0.00282288
Iteration 17/25 | Loss: 0.00282288
Iteration 18/25 | Loss: 0.00282288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002822877373546362, 0.002822877373546362, 0.002822877373546362, 0.002822877373546362, 0.002822877373546362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002822877373546362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00282288
Iteration 2/1000 | Loss: 0.00423913
Iteration 3/1000 | Loss: 0.00089659
Iteration 4/1000 | Loss: 0.00021147
Iteration 5/1000 | Loss: 0.00014595
Iteration 6/1000 | Loss: 0.00027075
Iteration 7/1000 | Loss: 0.00018104
Iteration 8/1000 | Loss: 0.00024538
Iteration 9/1000 | Loss: 0.00074465
Iteration 10/1000 | Loss: 0.00099211
Iteration 11/1000 | Loss: 0.00143697
Iteration 12/1000 | Loss: 0.00014889
Iteration 13/1000 | Loss: 0.00026749
Iteration 14/1000 | Loss: 0.00022400
Iteration 15/1000 | Loss: 0.00007899
Iteration 16/1000 | Loss: 0.00014100
Iteration 17/1000 | Loss: 0.00004980
Iteration 18/1000 | Loss: 0.00005477
Iteration 19/1000 | Loss: 0.00004573
Iteration 20/1000 | Loss: 0.00061865
Iteration 21/1000 | Loss: 0.00017112
Iteration 22/1000 | Loss: 0.00006833
Iteration 23/1000 | Loss: 0.00004315
Iteration 24/1000 | Loss: 0.00005676
Iteration 25/1000 | Loss: 0.00005708
Iteration 26/1000 | Loss: 0.00006287
Iteration 27/1000 | Loss: 0.00005276
Iteration 28/1000 | Loss: 0.00005951
Iteration 29/1000 | Loss: 0.00005797
Iteration 30/1000 | Loss: 0.00021319
Iteration 31/1000 | Loss: 0.00004065
Iteration 32/1000 | Loss: 0.00005983
Iteration 33/1000 | Loss: 0.00004114
Iteration 34/1000 | Loss: 0.00006451
Iteration 35/1000 | Loss: 0.00003500
Iteration 36/1000 | Loss: 0.00003936
Iteration 37/1000 | Loss: 0.00003069
Iteration 38/1000 | Loss: 0.00004999
Iteration 39/1000 | Loss: 0.00003132
Iteration 40/1000 | Loss: 0.00003089
Iteration 41/1000 | Loss: 0.00002657
Iteration 42/1000 | Loss: 0.00003363
Iteration 43/1000 | Loss: 0.00005242
Iteration 44/1000 | Loss: 0.00001677
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00002189
Iteration 47/1000 | Loss: 0.00001537
Iteration 48/1000 | Loss: 0.00001409
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001294
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001279
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001257
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001248
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001245
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001241
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001239
Iteration 85/1000 | Loss: 0.00001239
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001238
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001238
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001237
Iteration 96/1000 | Loss: 0.00001237
Iteration 97/1000 | Loss: 0.00001237
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001231
Iteration 114/1000 | Loss: 0.00001231
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001230
Iteration 118/1000 | Loss: 0.00001230
Iteration 119/1000 | Loss: 0.00001230
Iteration 120/1000 | Loss: 0.00001230
Iteration 121/1000 | Loss: 0.00001230
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001229
Iteration 127/1000 | Loss: 0.00001229
Iteration 128/1000 | Loss: 0.00001229
Iteration 129/1000 | Loss: 0.00001229
Iteration 130/1000 | Loss: 0.00001228
Iteration 131/1000 | Loss: 0.00001228
Iteration 132/1000 | Loss: 0.00001228
Iteration 133/1000 | Loss: 0.00001228
Iteration 134/1000 | Loss: 0.00001228
Iteration 135/1000 | Loss: 0.00001228
Iteration 136/1000 | Loss: 0.00001228
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001227
Iteration 140/1000 | Loss: 0.00001226
Iteration 141/1000 | Loss: 0.00001226
Iteration 142/1000 | Loss: 0.00001226
Iteration 143/1000 | Loss: 0.00001226
Iteration 144/1000 | Loss: 0.00001226
Iteration 145/1000 | Loss: 0.00001226
Iteration 146/1000 | Loss: 0.00001226
Iteration 147/1000 | Loss: 0.00001226
Iteration 148/1000 | Loss: 0.00001226
Iteration 149/1000 | Loss: 0.00001225
Iteration 150/1000 | Loss: 0.00001225
Iteration 151/1000 | Loss: 0.00001225
Iteration 152/1000 | Loss: 0.00001225
Iteration 153/1000 | Loss: 0.00001225
Iteration 154/1000 | Loss: 0.00001225
Iteration 155/1000 | Loss: 0.00001225
Iteration 156/1000 | Loss: 0.00001225
Iteration 157/1000 | Loss: 0.00001225
Iteration 158/1000 | Loss: 0.00001225
Iteration 159/1000 | Loss: 0.00001225
Iteration 160/1000 | Loss: 0.00001224
Iteration 161/1000 | Loss: 0.00001224
Iteration 162/1000 | Loss: 0.00001224
Iteration 163/1000 | Loss: 0.00001224
Iteration 164/1000 | Loss: 0.00001224
Iteration 165/1000 | Loss: 0.00001224
Iteration 166/1000 | Loss: 0.00001224
Iteration 167/1000 | Loss: 0.00001224
Iteration 168/1000 | Loss: 0.00001224
Iteration 169/1000 | Loss: 0.00001224
Iteration 170/1000 | Loss: 0.00001224
Iteration 171/1000 | Loss: 0.00001224
Iteration 172/1000 | Loss: 0.00001224
Iteration 173/1000 | Loss: 0.00001224
Iteration 174/1000 | Loss: 0.00001224
Iteration 175/1000 | Loss: 0.00001224
Iteration 176/1000 | Loss: 0.00001224
Iteration 177/1000 | Loss: 0.00001224
Iteration 178/1000 | Loss: 0.00001223
Iteration 179/1000 | Loss: 0.00001223
Iteration 180/1000 | Loss: 0.00001223
Iteration 181/1000 | Loss: 0.00001223
Iteration 182/1000 | Loss: 0.00001223
Iteration 183/1000 | Loss: 0.00001223
Iteration 184/1000 | Loss: 0.00001223
Iteration 185/1000 | Loss: 0.00001223
Iteration 186/1000 | Loss: 0.00001223
Iteration 187/1000 | Loss: 0.00001223
Iteration 188/1000 | Loss: 0.00001223
Iteration 189/1000 | Loss: 0.00001222
Iteration 190/1000 | Loss: 0.00001222
Iteration 191/1000 | Loss: 0.00001222
Iteration 192/1000 | Loss: 0.00001222
Iteration 193/1000 | Loss: 0.00001222
Iteration 194/1000 | Loss: 0.00001222
Iteration 195/1000 | Loss: 0.00001222
Iteration 196/1000 | Loss: 0.00001222
Iteration 197/1000 | Loss: 0.00001222
Iteration 198/1000 | Loss: 0.00001222
Iteration 199/1000 | Loss: 0.00001222
Iteration 200/1000 | Loss: 0.00001222
Iteration 201/1000 | Loss: 0.00001222
Iteration 202/1000 | Loss: 0.00001222
Iteration 203/1000 | Loss: 0.00001222
Iteration 204/1000 | Loss: 0.00001222
Iteration 205/1000 | Loss: 0.00001222
Iteration 206/1000 | Loss: 0.00001222
Iteration 207/1000 | Loss: 0.00001221
Iteration 208/1000 | Loss: 0.00001221
Iteration 209/1000 | Loss: 0.00001221
Iteration 210/1000 | Loss: 0.00001221
Iteration 211/1000 | Loss: 0.00001221
Iteration 212/1000 | Loss: 0.00001221
Iteration 213/1000 | Loss: 0.00001221
Iteration 214/1000 | Loss: 0.00001221
Iteration 215/1000 | Loss: 0.00001221
Iteration 216/1000 | Loss: 0.00001221
Iteration 217/1000 | Loss: 0.00001221
Iteration 218/1000 | Loss: 0.00001221
Iteration 219/1000 | Loss: 0.00001221
Iteration 220/1000 | Loss: 0.00001221
Iteration 221/1000 | Loss: 0.00001221
Iteration 222/1000 | Loss: 0.00001221
Iteration 223/1000 | Loss: 0.00001221
Iteration 224/1000 | Loss: 0.00001221
Iteration 225/1000 | Loss: 0.00001221
Iteration 226/1000 | Loss: 0.00001220
Iteration 227/1000 | Loss: 0.00001220
Iteration 228/1000 | Loss: 0.00001220
Iteration 229/1000 | Loss: 0.00001220
Iteration 230/1000 | Loss: 0.00001220
Iteration 231/1000 | Loss: 0.00001220
Iteration 232/1000 | Loss: 0.00001220
Iteration 233/1000 | Loss: 0.00001220
Iteration 234/1000 | Loss: 0.00001220
Iteration 235/1000 | Loss: 0.00001220
Iteration 236/1000 | Loss: 0.00001220
Iteration 237/1000 | Loss: 0.00001220
Iteration 238/1000 | Loss: 0.00001220
Iteration 239/1000 | Loss: 0.00001220
Iteration 240/1000 | Loss: 0.00001220
Iteration 241/1000 | Loss: 0.00001219
Iteration 242/1000 | Loss: 0.00001219
Iteration 243/1000 | Loss: 0.00001219
Iteration 244/1000 | Loss: 0.00001219
Iteration 245/1000 | Loss: 0.00001219
Iteration 246/1000 | Loss: 0.00001219
Iteration 247/1000 | Loss: 0.00001219
Iteration 248/1000 | Loss: 0.00001219
Iteration 249/1000 | Loss: 0.00001219
Iteration 250/1000 | Loss: 0.00001219
Iteration 251/1000 | Loss: 0.00001218
Iteration 252/1000 | Loss: 0.00001218
Iteration 253/1000 | Loss: 0.00001218
Iteration 254/1000 | Loss: 0.00001218
Iteration 255/1000 | Loss: 0.00001218
Iteration 256/1000 | Loss: 0.00001218
Iteration 257/1000 | Loss: 0.00001218
Iteration 258/1000 | Loss: 0.00001218
Iteration 259/1000 | Loss: 0.00001218
Iteration 260/1000 | Loss: 0.00001218
Iteration 261/1000 | Loss: 0.00001218
Iteration 262/1000 | Loss: 0.00001218
Iteration 263/1000 | Loss: 0.00001218
Iteration 264/1000 | Loss: 0.00001218
Iteration 265/1000 | Loss: 0.00001218
Iteration 266/1000 | Loss: 0.00001217
Iteration 267/1000 | Loss: 0.00001217
Iteration 268/1000 | Loss: 0.00001217
Iteration 269/1000 | Loss: 0.00001217
Iteration 270/1000 | Loss: 0.00001217
Iteration 271/1000 | Loss: 0.00001217
Iteration 272/1000 | Loss: 0.00001217
Iteration 273/1000 | Loss: 0.00001217
Iteration 274/1000 | Loss: 0.00001217
Iteration 275/1000 | Loss: 0.00001217
Iteration 276/1000 | Loss: 0.00001217
Iteration 277/1000 | Loss: 0.00001217
Iteration 278/1000 | Loss: 0.00001217
Iteration 279/1000 | Loss: 0.00001217
Iteration 280/1000 | Loss: 0.00001217
Iteration 281/1000 | Loss: 0.00001217
Iteration 282/1000 | Loss: 0.00001217
Iteration 283/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [1.2172003152954858e-05, 1.2172003152954858e-05, 1.2172003152954858e-05, 1.2172003152954858e-05, 1.2172003152954858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2172003152954858e-05

Optimization complete. Final v2v error: 2.9175212383270264 mm

Highest mean error: 5.40032958984375 mm for frame 186

Lowest mean error: 2.386528253555298 mm for frame 217

Saving results

Total time: 154.71167707443237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474477
Iteration 2/25 | Loss: 0.00090687
Iteration 3/25 | Loss: 0.00067580
Iteration 4/25 | Loss: 0.00065530
Iteration 5/25 | Loss: 0.00064865
Iteration 6/25 | Loss: 0.00064651
Iteration 7/25 | Loss: 0.00064613
Iteration 8/25 | Loss: 0.00064613
Iteration 9/25 | Loss: 0.00064613
Iteration 10/25 | Loss: 0.00064613
Iteration 11/25 | Loss: 0.00064613
Iteration 12/25 | Loss: 0.00064613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006461261655203998, 0.0006461261655203998, 0.0006461261655203998, 0.0006461261655203998, 0.0006461261655203998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006461261655203998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50880182
Iteration 2/25 | Loss: 0.00065077
Iteration 3/25 | Loss: 0.00065075
Iteration 4/25 | Loss: 0.00065075
Iteration 5/25 | Loss: 0.00065075
Iteration 6/25 | Loss: 0.00065075
Iteration 7/25 | Loss: 0.00065075
Iteration 8/25 | Loss: 0.00065075
Iteration 9/25 | Loss: 0.00065075
Iteration 10/25 | Loss: 0.00065075
Iteration 11/25 | Loss: 0.00065075
Iteration 12/25 | Loss: 0.00065075
Iteration 13/25 | Loss: 0.00065075
Iteration 14/25 | Loss: 0.00065075
Iteration 15/25 | Loss: 0.00065075
Iteration 16/25 | Loss: 0.00065075
Iteration 17/25 | Loss: 0.00065075
Iteration 18/25 | Loss: 0.00065075
Iteration 19/25 | Loss: 0.00065075
Iteration 20/25 | Loss: 0.00065075
Iteration 21/25 | Loss: 0.00065075
Iteration 22/25 | Loss: 0.00065075
Iteration 23/25 | Loss: 0.00065075
Iteration 24/25 | Loss: 0.00065075
Iteration 25/25 | Loss: 0.00065075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065075
Iteration 2/1000 | Loss: 0.00002943
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00002023
Iteration 5/1000 | Loss: 0.00001904
Iteration 6/1000 | Loss: 0.00001830
Iteration 7/1000 | Loss: 0.00001788
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001708
Iteration 11/1000 | Loss: 0.00001706
Iteration 12/1000 | Loss: 0.00001705
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001703
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001701
Iteration 18/1000 | Loss: 0.00001698
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001691
Iteration 22/1000 | Loss: 0.00001688
Iteration 23/1000 | Loss: 0.00001687
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001685
Iteration 26/1000 | Loss: 0.00001684
Iteration 27/1000 | Loss: 0.00001684
Iteration 28/1000 | Loss: 0.00001684
Iteration 29/1000 | Loss: 0.00001683
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001682
Iteration 32/1000 | Loss: 0.00001681
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001679
Iteration 36/1000 | Loss: 0.00001678
Iteration 37/1000 | Loss: 0.00001676
Iteration 38/1000 | Loss: 0.00001676
Iteration 39/1000 | Loss: 0.00001676
Iteration 40/1000 | Loss: 0.00001676
Iteration 41/1000 | Loss: 0.00001676
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00001675
Iteration 44/1000 | Loss: 0.00001675
Iteration 45/1000 | Loss: 0.00001675
Iteration 46/1000 | Loss: 0.00001675
Iteration 47/1000 | Loss: 0.00001675
Iteration 48/1000 | Loss: 0.00001675
Iteration 49/1000 | Loss: 0.00001675
Iteration 50/1000 | Loss: 0.00001675
Iteration 51/1000 | Loss: 0.00001674
Iteration 52/1000 | Loss: 0.00001674
Iteration 53/1000 | Loss: 0.00001674
Iteration 54/1000 | Loss: 0.00001674
Iteration 55/1000 | Loss: 0.00001674
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001673
Iteration 58/1000 | Loss: 0.00001673
Iteration 59/1000 | Loss: 0.00001673
Iteration 60/1000 | Loss: 0.00001673
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001672
Iteration 64/1000 | Loss: 0.00001672
Iteration 65/1000 | Loss: 0.00001672
Iteration 66/1000 | Loss: 0.00001672
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001671
Iteration 69/1000 | Loss: 0.00001671
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001670
Iteration 73/1000 | Loss: 0.00001670
Iteration 74/1000 | Loss: 0.00001670
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001669
Iteration 77/1000 | Loss: 0.00001669
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001669
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001669
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001669
Iteration 86/1000 | Loss: 0.00001669
Iteration 87/1000 | Loss: 0.00001668
Iteration 88/1000 | Loss: 0.00001668
Iteration 89/1000 | Loss: 0.00001668
Iteration 90/1000 | Loss: 0.00001668
Iteration 91/1000 | Loss: 0.00001668
Iteration 92/1000 | Loss: 0.00001667
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001667
Iteration 95/1000 | Loss: 0.00001667
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001666
Iteration 100/1000 | Loss: 0.00001666
Iteration 101/1000 | Loss: 0.00001666
Iteration 102/1000 | Loss: 0.00001666
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001665
Iteration 107/1000 | Loss: 0.00001665
Iteration 108/1000 | Loss: 0.00001665
Iteration 109/1000 | Loss: 0.00001665
Iteration 110/1000 | Loss: 0.00001665
Iteration 111/1000 | Loss: 0.00001665
Iteration 112/1000 | Loss: 0.00001665
Iteration 113/1000 | Loss: 0.00001665
Iteration 114/1000 | Loss: 0.00001665
Iteration 115/1000 | Loss: 0.00001664
Iteration 116/1000 | Loss: 0.00001664
Iteration 117/1000 | Loss: 0.00001664
Iteration 118/1000 | Loss: 0.00001664
Iteration 119/1000 | Loss: 0.00001664
Iteration 120/1000 | Loss: 0.00001664
Iteration 121/1000 | Loss: 0.00001664
Iteration 122/1000 | Loss: 0.00001664
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001664
Iteration 125/1000 | Loss: 0.00001664
Iteration 126/1000 | Loss: 0.00001664
Iteration 127/1000 | Loss: 0.00001664
Iteration 128/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.6644793504383415e-05, 1.6644793504383415e-05, 1.6644793504383415e-05, 1.6644793504383415e-05, 1.6644793504383415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6644793504383415e-05

Optimization complete. Final v2v error: 3.4979169368743896 mm

Highest mean error: 4.050295352935791 mm for frame 4

Lowest mean error: 2.9509308338165283 mm for frame 191

Saving results

Total time: 38.43206572532654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087541
Iteration 2/25 | Loss: 0.00152190
Iteration 3/25 | Loss: 0.00079183
Iteration 4/25 | Loss: 0.00078029
Iteration 5/25 | Loss: 0.00073528
Iteration 6/25 | Loss: 0.00072951
Iteration 7/25 | Loss: 0.00074680
Iteration 8/25 | Loss: 0.00070981
Iteration 9/25 | Loss: 0.00070776
Iteration 10/25 | Loss: 0.00064550
Iteration 11/25 | Loss: 0.00064255
Iteration 12/25 | Loss: 0.00063470
Iteration 13/25 | Loss: 0.00061781
Iteration 14/25 | Loss: 0.00063198
Iteration 15/25 | Loss: 0.00060655
Iteration 16/25 | Loss: 0.00060871
Iteration 17/25 | Loss: 0.00060941
Iteration 18/25 | Loss: 0.00060583
Iteration 19/25 | Loss: 0.00061435
Iteration 20/25 | Loss: 0.00060003
Iteration 21/25 | Loss: 0.00059926
Iteration 22/25 | Loss: 0.00059939
Iteration 23/25 | Loss: 0.00059875
Iteration 24/25 | Loss: 0.00059722
Iteration 25/25 | Loss: 0.00059522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47917616
Iteration 2/25 | Loss: 0.00066240
Iteration 3/25 | Loss: 0.00056989
Iteration 4/25 | Loss: 0.00056989
Iteration 5/25 | Loss: 0.00056989
Iteration 6/25 | Loss: 0.00056989
Iteration 7/25 | Loss: 0.00056989
Iteration 8/25 | Loss: 0.00056988
Iteration 9/25 | Loss: 0.00056988
Iteration 10/25 | Loss: 0.00056988
Iteration 11/25 | Loss: 0.00056988
Iteration 12/25 | Loss: 0.00056988
Iteration 13/25 | Loss: 0.00056988
Iteration 14/25 | Loss: 0.00056988
Iteration 15/25 | Loss: 0.00056988
Iteration 16/25 | Loss: 0.00056988
Iteration 17/25 | Loss: 0.00056988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005698843160644174, 0.0005698843160644174, 0.0005698843160644174, 0.0005698843160644174, 0.0005698843160644174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005698843160644174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056988
Iteration 2/1000 | Loss: 0.00004258
Iteration 3/1000 | Loss: 0.00020801
Iteration 4/1000 | Loss: 0.00013134
Iteration 5/1000 | Loss: 0.00007851
Iteration 6/1000 | Loss: 0.00013925
Iteration 7/1000 | Loss: 0.00012629
Iteration 8/1000 | Loss: 0.00001954
Iteration 9/1000 | Loss: 0.00007541
Iteration 10/1000 | Loss: 0.00015411
Iteration 11/1000 | Loss: 0.00003732
Iteration 12/1000 | Loss: 0.00001692
Iteration 13/1000 | Loss: 0.00001660
Iteration 14/1000 | Loss: 0.00001624
Iteration 15/1000 | Loss: 0.00006255
Iteration 16/1000 | Loss: 0.00001607
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001594
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001592
Iteration 22/1000 | Loss: 0.00002982
Iteration 23/1000 | Loss: 0.00001587
Iteration 24/1000 | Loss: 0.00001586
Iteration 25/1000 | Loss: 0.00001586
Iteration 26/1000 | Loss: 0.00001585
Iteration 27/1000 | Loss: 0.00001585
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00005085
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001582
Iteration 33/1000 | Loss: 0.00002958
Iteration 34/1000 | Loss: 0.00002823
Iteration 35/1000 | Loss: 0.00001579
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001577
Iteration 38/1000 | Loss: 0.00001577
Iteration 39/1000 | Loss: 0.00001577
Iteration 40/1000 | Loss: 0.00001577
Iteration 41/1000 | Loss: 0.00001577
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001576
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001574
Iteration 57/1000 | Loss: 0.00001574
Iteration 58/1000 | Loss: 0.00001574
Iteration 59/1000 | Loss: 0.00001574
Iteration 60/1000 | Loss: 0.00001573
Iteration 61/1000 | Loss: 0.00001573
Iteration 62/1000 | Loss: 0.00001573
Iteration 63/1000 | Loss: 0.00001573
Iteration 64/1000 | Loss: 0.00001573
Iteration 65/1000 | Loss: 0.00001573
Iteration 66/1000 | Loss: 0.00001573
Iteration 67/1000 | Loss: 0.00001573
Iteration 68/1000 | Loss: 0.00001573
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001572
Iteration 85/1000 | Loss: 0.00001572
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001572
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001571
Iteration 93/1000 | Loss: 0.00001571
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001571
Iteration 99/1000 | Loss: 0.00001571
Iteration 100/1000 | Loss: 0.00001571
Iteration 101/1000 | Loss: 0.00001571
Iteration 102/1000 | Loss: 0.00001571
Iteration 103/1000 | Loss: 0.00001571
Iteration 104/1000 | Loss: 0.00001571
Iteration 105/1000 | Loss: 0.00001571
Iteration 106/1000 | Loss: 0.00001571
Iteration 107/1000 | Loss: 0.00001571
Iteration 108/1000 | Loss: 0.00001571
Iteration 109/1000 | Loss: 0.00001571
Iteration 110/1000 | Loss: 0.00001571
Iteration 111/1000 | Loss: 0.00001571
Iteration 112/1000 | Loss: 0.00001571
Iteration 113/1000 | Loss: 0.00001571
Iteration 114/1000 | Loss: 0.00001571
Iteration 115/1000 | Loss: 0.00001571
Iteration 116/1000 | Loss: 0.00001571
Iteration 117/1000 | Loss: 0.00001571
Iteration 118/1000 | Loss: 0.00001571
Iteration 119/1000 | Loss: 0.00001571
Iteration 120/1000 | Loss: 0.00001571
Iteration 121/1000 | Loss: 0.00001571
Iteration 122/1000 | Loss: 0.00001571
Iteration 123/1000 | Loss: 0.00001571
Iteration 124/1000 | Loss: 0.00001571
Iteration 125/1000 | Loss: 0.00001571
Iteration 126/1000 | Loss: 0.00001571
Iteration 127/1000 | Loss: 0.00001571
Iteration 128/1000 | Loss: 0.00001571
Iteration 129/1000 | Loss: 0.00001571
Iteration 130/1000 | Loss: 0.00001571
Iteration 131/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.5710586012573913e-05, 1.5710586012573913e-05, 1.5710586012573913e-05, 1.5710586012573913e-05, 1.5710586012573913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5710586012573913e-05

Optimization complete. Final v2v error: 3.270458936691284 mm

Highest mean error: 3.5439205169677734 mm for frame 130

Lowest mean error: 2.989470958709717 mm for frame 160

Saving results

Total time: 81.04304242134094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425237
Iteration 2/25 | Loss: 0.00087063
Iteration 3/25 | Loss: 0.00067636
Iteration 4/25 | Loss: 0.00063421
Iteration 5/25 | Loss: 0.00062093
Iteration 6/25 | Loss: 0.00061844
Iteration 7/25 | Loss: 0.00061774
Iteration 8/25 | Loss: 0.00061762
Iteration 9/25 | Loss: 0.00061762
Iteration 10/25 | Loss: 0.00061762
Iteration 11/25 | Loss: 0.00061762
Iteration 12/25 | Loss: 0.00061762
Iteration 13/25 | Loss: 0.00061762
Iteration 14/25 | Loss: 0.00061762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000617616402450949, 0.000617616402450949, 0.000617616402450949, 0.000617616402450949, 0.000617616402450949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000617616402450949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64944816
Iteration 2/25 | Loss: 0.00055146
Iteration 3/25 | Loss: 0.00055146
Iteration 4/25 | Loss: 0.00055146
Iteration 5/25 | Loss: 0.00055146
Iteration 6/25 | Loss: 0.00055146
Iteration 7/25 | Loss: 0.00055146
Iteration 8/25 | Loss: 0.00055146
Iteration 9/25 | Loss: 0.00055146
Iteration 10/25 | Loss: 0.00055146
Iteration 11/25 | Loss: 0.00055146
Iteration 12/25 | Loss: 0.00055146
Iteration 13/25 | Loss: 0.00055146
Iteration 14/25 | Loss: 0.00055146
Iteration 15/25 | Loss: 0.00055146
Iteration 16/25 | Loss: 0.00055146
Iteration 17/25 | Loss: 0.00055146
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005514599615707994, 0.0005514599615707994, 0.0005514599615707994, 0.0005514599615707994, 0.0005514599615707994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005514599615707994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055146
Iteration 2/1000 | Loss: 0.00003256
Iteration 3/1000 | Loss: 0.00002470
Iteration 4/1000 | Loss: 0.00002194
Iteration 5/1000 | Loss: 0.00002084
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00001980
Iteration 8/1000 | Loss: 0.00001944
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001867
Iteration 13/1000 | Loss: 0.00001863
Iteration 14/1000 | Loss: 0.00001854
Iteration 15/1000 | Loss: 0.00001850
Iteration 16/1000 | Loss: 0.00001849
Iteration 17/1000 | Loss: 0.00001848
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001847
Iteration 20/1000 | Loss: 0.00001846
Iteration 21/1000 | Loss: 0.00001846
Iteration 22/1000 | Loss: 0.00001845
Iteration 23/1000 | Loss: 0.00001844
Iteration 24/1000 | Loss: 0.00001844
Iteration 25/1000 | Loss: 0.00001841
Iteration 26/1000 | Loss: 0.00001841
Iteration 27/1000 | Loss: 0.00001838
Iteration 28/1000 | Loss: 0.00001838
Iteration 29/1000 | Loss: 0.00001838
Iteration 30/1000 | Loss: 0.00001837
Iteration 31/1000 | Loss: 0.00001836
Iteration 32/1000 | Loss: 0.00001835
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001833
Iteration 36/1000 | Loss: 0.00001833
Iteration 37/1000 | Loss: 0.00001833
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00001832
Iteration 40/1000 | Loss: 0.00001831
Iteration 41/1000 | Loss: 0.00001831
Iteration 42/1000 | Loss: 0.00001831
Iteration 43/1000 | Loss: 0.00001830
Iteration 44/1000 | Loss: 0.00001830
Iteration 45/1000 | Loss: 0.00001828
Iteration 46/1000 | Loss: 0.00001827
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001827
Iteration 49/1000 | Loss: 0.00001827
Iteration 50/1000 | Loss: 0.00001826
Iteration 51/1000 | Loss: 0.00001826
Iteration 52/1000 | Loss: 0.00001825
Iteration 53/1000 | Loss: 0.00001825
Iteration 54/1000 | Loss: 0.00001825
Iteration 55/1000 | Loss: 0.00001824
Iteration 56/1000 | Loss: 0.00001824
Iteration 57/1000 | Loss: 0.00001824
Iteration 58/1000 | Loss: 0.00001823
Iteration 59/1000 | Loss: 0.00001823
Iteration 60/1000 | Loss: 0.00001823
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001822
Iteration 64/1000 | Loss: 0.00001822
Iteration 65/1000 | Loss: 0.00001821
Iteration 66/1000 | Loss: 0.00001821
Iteration 67/1000 | Loss: 0.00001821
Iteration 68/1000 | Loss: 0.00001821
Iteration 69/1000 | Loss: 0.00001821
Iteration 70/1000 | Loss: 0.00001820
Iteration 71/1000 | Loss: 0.00001820
Iteration 72/1000 | Loss: 0.00001820
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001820
Iteration 75/1000 | Loss: 0.00001819
Iteration 76/1000 | Loss: 0.00001819
Iteration 77/1000 | Loss: 0.00001819
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001818
Iteration 80/1000 | Loss: 0.00001818
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001816
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001815
Iteration 90/1000 | Loss: 0.00001814
Iteration 91/1000 | Loss: 0.00001814
Iteration 92/1000 | Loss: 0.00001814
Iteration 93/1000 | Loss: 0.00001814
Iteration 94/1000 | Loss: 0.00001814
Iteration 95/1000 | Loss: 0.00001813
Iteration 96/1000 | Loss: 0.00001813
Iteration 97/1000 | Loss: 0.00001813
Iteration 98/1000 | Loss: 0.00001813
Iteration 99/1000 | Loss: 0.00001813
Iteration 100/1000 | Loss: 0.00001812
Iteration 101/1000 | Loss: 0.00001812
Iteration 102/1000 | Loss: 0.00001812
Iteration 103/1000 | Loss: 0.00001812
Iteration 104/1000 | Loss: 0.00001812
Iteration 105/1000 | Loss: 0.00001812
Iteration 106/1000 | Loss: 0.00001812
Iteration 107/1000 | Loss: 0.00001812
Iteration 108/1000 | Loss: 0.00001812
Iteration 109/1000 | Loss: 0.00001811
Iteration 110/1000 | Loss: 0.00001811
Iteration 111/1000 | Loss: 0.00001811
Iteration 112/1000 | Loss: 0.00001811
Iteration 113/1000 | Loss: 0.00001811
Iteration 114/1000 | Loss: 0.00001811
Iteration 115/1000 | Loss: 0.00001811
Iteration 116/1000 | Loss: 0.00001811
Iteration 117/1000 | Loss: 0.00001811
Iteration 118/1000 | Loss: 0.00001811
Iteration 119/1000 | Loss: 0.00001811
Iteration 120/1000 | Loss: 0.00001811
Iteration 121/1000 | Loss: 0.00001811
Iteration 122/1000 | Loss: 0.00001811
Iteration 123/1000 | Loss: 0.00001810
Iteration 124/1000 | Loss: 0.00001810
Iteration 125/1000 | Loss: 0.00001810
Iteration 126/1000 | Loss: 0.00001810
Iteration 127/1000 | Loss: 0.00001810
Iteration 128/1000 | Loss: 0.00001810
Iteration 129/1000 | Loss: 0.00001810
Iteration 130/1000 | Loss: 0.00001810
Iteration 131/1000 | Loss: 0.00001810
Iteration 132/1000 | Loss: 0.00001810
Iteration 133/1000 | Loss: 0.00001810
Iteration 134/1000 | Loss: 0.00001810
Iteration 135/1000 | Loss: 0.00001809
Iteration 136/1000 | Loss: 0.00001809
Iteration 137/1000 | Loss: 0.00001809
Iteration 138/1000 | Loss: 0.00001809
Iteration 139/1000 | Loss: 0.00001809
Iteration 140/1000 | Loss: 0.00001809
Iteration 141/1000 | Loss: 0.00001809
Iteration 142/1000 | Loss: 0.00001809
Iteration 143/1000 | Loss: 0.00001809
Iteration 144/1000 | Loss: 0.00001809
Iteration 145/1000 | Loss: 0.00001809
Iteration 146/1000 | Loss: 0.00001809
Iteration 147/1000 | Loss: 0.00001809
Iteration 148/1000 | Loss: 0.00001808
Iteration 149/1000 | Loss: 0.00001808
Iteration 150/1000 | Loss: 0.00001808
Iteration 151/1000 | Loss: 0.00001808
Iteration 152/1000 | Loss: 0.00001808
Iteration 153/1000 | Loss: 0.00001808
Iteration 154/1000 | Loss: 0.00001808
Iteration 155/1000 | Loss: 0.00001808
Iteration 156/1000 | Loss: 0.00001808
Iteration 157/1000 | Loss: 0.00001808
Iteration 158/1000 | Loss: 0.00001808
Iteration 159/1000 | Loss: 0.00001808
Iteration 160/1000 | Loss: 0.00001808
Iteration 161/1000 | Loss: 0.00001808
Iteration 162/1000 | Loss: 0.00001808
Iteration 163/1000 | Loss: 0.00001807
Iteration 164/1000 | Loss: 0.00001807
Iteration 165/1000 | Loss: 0.00001807
Iteration 166/1000 | Loss: 0.00001807
Iteration 167/1000 | Loss: 0.00001807
Iteration 168/1000 | Loss: 0.00001807
Iteration 169/1000 | Loss: 0.00001807
Iteration 170/1000 | Loss: 0.00001807
Iteration 171/1000 | Loss: 0.00001807
Iteration 172/1000 | Loss: 0.00001807
Iteration 173/1000 | Loss: 0.00001807
Iteration 174/1000 | Loss: 0.00001807
Iteration 175/1000 | Loss: 0.00001807
Iteration 176/1000 | Loss: 0.00001807
Iteration 177/1000 | Loss: 0.00001807
Iteration 178/1000 | Loss: 0.00001807
Iteration 179/1000 | Loss: 0.00001807
Iteration 180/1000 | Loss: 0.00001807
Iteration 181/1000 | Loss: 0.00001807
Iteration 182/1000 | Loss: 0.00001807
Iteration 183/1000 | Loss: 0.00001807
Iteration 184/1000 | Loss: 0.00001807
Iteration 185/1000 | Loss: 0.00001807
Iteration 186/1000 | Loss: 0.00001807
Iteration 187/1000 | Loss: 0.00001807
Iteration 188/1000 | Loss: 0.00001807
Iteration 189/1000 | Loss: 0.00001807
Iteration 190/1000 | Loss: 0.00001807
Iteration 191/1000 | Loss: 0.00001807
Iteration 192/1000 | Loss: 0.00001807
Iteration 193/1000 | Loss: 0.00001807
Iteration 194/1000 | Loss: 0.00001807
Iteration 195/1000 | Loss: 0.00001807
Iteration 196/1000 | Loss: 0.00001807
Iteration 197/1000 | Loss: 0.00001807
Iteration 198/1000 | Loss: 0.00001807
Iteration 199/1000 | Loss: 0.00001807
Iteration 200/1000 | Loss: 0.00001807
Iteration 201/1000 | Loss: 0.00001807
Iteration 202/1000 | Loss: 0.00001807
Iteration 203/1000 | Loss: 0.00001807
Iteration 204/1000 | Loss: 0.00001807
Iteration 205/1000 | Loss: 0.00001807
Iteration 206/1000 | Loss: 0.00001807
Iteration 207/1000 | Loss: 0.00001807
Iteration 208/1000 | Loss: 0.00001807
Iteration 209/1000 | Loss: 0.00001807
Iteration 210/1000 | Loss: 0.00001807
Iteration 211/1000 | Loss: 0.00001807
Iteration 212/1000 | Loss: 0.00001807
Iteration 213/1000 | Loss: 0.00001807
Iteration 214/1000 | Loss: 0.00001807
Iteration 215/1000 | Loss: 0.00001807
Iteration 216/1000 | Loss: 0.00001807
Iteration 217/1000 | Loss: 0.00001807
Iteration 218/1000 | Loss: 0.00001807
Iteration 219/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.806548607419245e-05, 1.806548607419245e-05, 1.806548607419245e-05, 1.806548607419245e-05, 1.806548607419245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.806548607419245e-05

Optimization complete. Final v2v error: 3.598187208175659 mm

Highest mean error: 4.466787815093994 mm for frame 47

Lowest mean error: 3.144500255584717 mm for frame 22

Saving results

Total time: 41.44965982437134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677192
Iteration 2/25 | Loss: 0.00101570
Iteration 3/25 | Loss: 0.00081783
Iteration 4/25 | Loss: 0.00077697
Iteration 5/25 | Loss: 0.00076192
Iteration 6/25 | Loss: 0.00075967
Iteration 7/25 | Loss: 0.00075905
Iteration 8/25 | Loss: 0.00075902
Iteration 9/25 | Loss: 0.00075902
Iteration 10/25 | Loss: 0.00075902
Iteration 11/25 | Loss: 0.00075902
Iteration 12/25 | Loss: 0.00075902
Iteration 13/25 | Loss: 0.00075902
Iteration 14/25 | Loss: 0.00075902
Iteration 15/25 | Loss: 0.00075902
Iteration 16/25 | Loss: 0.00075902
Iteration 17/25 | Loss: 0.00075902
Iteration 18/25 | Loss: 0.00075902
Iteration 19/25 | Loss: 0.00075902
Iteration 20/25 | Loss: 0.00075902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007590170716866851, 0.0007590170716866851, 0.0007590170716866851, 0.0007590170716866851, 0.0007590170716866851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007590170716866851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20183635
Iteration 2/25 | Loss: 0.00058138
Iteration 3/25 | Loss: 0.00058134
Iteration 4/25 | Loss: 0.00058134
Iteration 5/25 | Loss: 0.00058134
Iteration 6/25 | Loss: 0.00058134
Iteration 7/25 | Loss: 0.00058134
Iteration 8/25 | Loss: 0.00058134
Iteration 9/25 | Loss: 0.00058134
Iteration 10/25 | Loss: 0.00058134
Iteration 11/25 | Loss: 0.00058134
Iteration 12/25 | Loss: 0.00058134
Iteration 13/25 | Loss: 0.00058134
Iteration 14/25 | Loss: 0.00058134
Iteration 15/25 | Loss: 0.00058134
Iteration 16/25 | Loss: 0.00058134
Iteration 17/25 | Loss: 0.00058134
Iteration 18/25 | Loss: 0.00058134
Iteration 19/25 | Loss: 0.00058134
Iteration 20/25 | Loss: 0.00058134
Iteration 21/25 | Loss: 0.00058134
Iteration 22/25 | Loss: 0.00058134
Iteration 23/25 | Loss: 0.00058134
Iteration 24/25 | Loss: 0.00058134
Iteration 25/25 | Loss: 0.00058134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058134
Iteration 2/1000 | Loss: 0.00004832
Iteration 3/1000 | Loss: 0.00003642
Iteration 4/1000 | Loss: 0.00003268
Iteration 5/1000 | Loss: 0.00003180
Iteration 6/1000 | Loss: 0.00003070
Iteration 7/1000 | Loss: 0.00003002
Iteration 8/1000 | Loss: 0.00002952
Iteration 9/1000 | Loss: 0.00002904
Iteration 10/1000 | Loss: 0.00002882
Iteration 11/1000 | Loss: 0.00002872
Iteration 12/1000 | Loss: 0.00002865
Iteration 13/1000 | Loss: 0.00002864
Iteration 14/1000 | Loss: 0.00002856
Iteration 15/1000 | Loss: 0.00002852
Iteration 16/1000 | Loss: 0.00002850
Iteration 17/1000 | Loss: 0.00002849
Iteration 18/1000 | Loss: 0.00002849
Iteration 19/1000 | Loss: 0.00002848
Iteration 20/1000 | Loss: 0.00002848
Iteration 21/1000 | Loss: 0.00002847
Iteration 22/1000 | Loss: 0.00002847
Iteration 23/1000 | Loss: 0.00002847
Iteration 24/1000 | Loss: 0.00002846
Iteration 25/1000 | Loss: 0.00002846
Iteration 26/1000 | Loss: 0.00002846
Iteration 27/1000 | Loss: 0.00002845
Iteration 28/1000 | Loss: 0.00002845
Iteration 29/1000 | Loss: 0.00002845
Iteration 30/1000 | Loss: 0.00002844
Iteration 31/1000 | Loss: 0.00002844
Iteration 32/1000 | Loss: 0.00002844
Iteration 33/1000 | Loss: 0.00002843
Iteration 34/1000 | Loss: 0.00002843
Iteration 35/1000 | Loss: 0.00002842
Iteration 36/1000 | Loss: 0.00002842
Iteration 37/1000 | Loss: 0.00002842
Iteration 38/1000 | Loss: 0.00002842
Iteration 39/1000 | Loss: 0.00002841
Iteration 40/1000 | Loss: 0.00002841
Iteration 41/1000 | Loss: 0.00002841
Iteration 42/1000 | Loss: 0.00002841
Iteration 43/1000 | Loss: 0.00002841
Iteration 44/1000 | Loss: 0.00002841
Iteration 45/1000 | Loss: 0.00002841
Iteration 46/1000 | Loss: 0.00002841
Iteration 47/1000 | Loss: 0.00002841
Iteration 48/1000 | Loss: 0.00002841
Iteration 49/1000 | Loss: 0.00002841
Iteration 50/1000 | Loss: 0.00002840
Iteration 51/1000 | Loss: 0.00002840
Iteration 52/1000 | Loss: 0.00002840
Iteration 53/1000 | Loss: 0.00002840
Iteration 54/1000 | Loss: 0.00002840
Iteration 55/1000 | Loss: 0.00002840
Iteration 56/1000 | Loss: 0.00002840
Iteration 57/1000 | Loss: 0.00002840
Iteration 58/1000 | Loss: 0.00002840
Iteration 59/1000 | Loss: 0.00002840
Iteration 60/1000 | Loss: 0.00002840
Iteration 61/1000 | Loss: 0.00002840
Iteration 62/1000 | Loss: 0.00002839
Iteration 63/1000 | Loss: 0.00002839
Iteration 64/1000 | Loss: 0.00002839
Iteration 65/1000 | Loss: 0.00002839
Iteration 66/1000 | Loss: 0.00002839
Iteration 67/1000 | Loss: 0.00002839
Iteration 68/1000 | Loss: 0.00002839
Iteration 69/1000 | Loss: 0.00002839
Iteration 70/1000 | Loss: 0.00002838
Iteration 71/1000 | Loss: 0.00002838
Iteration 72/1000 | Loss: 0.00002838
Iteration 73/1000 | Loss: 0.00002838
Iteration 74/1000 | Loss: 0.00002838
Iteration 75/1000 | Loss: 0.00002838
Iteration 76/1000 | Loss: 0.00002838
Iteration 77/1000 | Loss: 0.00002837
Iteration 78/1000 | Loss: 0.00002837
Iteration 79/1000 | Loss: 0.00002837
Iteration 80/1000 | Loss: 0.00002837
Iteration 81/1000 | Loss: 0.00002837
Iteration 82/1000 | Loss: 0.00002837
Iteration 83/1000 | Loss: 0.00002837
Iteration 84/1000 | Loss: 0.00002836
Iteration 85/1000 | Loss: 0.00002836
Iteration 86/1000 | Loss: 0.00002836
Iteration 87/1000 | Loss: 0.00002836
Iteration 88/1000 | Loss: 0.00002836
Iteration 89/1000 | Loss: 0.00002836
Iteration 90/1000 | Loss: 0.00002836
Iteration 91/1000 | Loss: 0.00002836
Iteration 92/1000 | Loss: 0.00002836
Iteration 93/1000 | Loss: 0.00002836
Iteration 94/1000 | Loss: 0.00002836
Iteration 95/1000 | Loss: 0.00002836
Iteration 96/1000 | Loss: 0.00002835
Iteration 97/1000 | Loss: 0.00002835
Iteration 98/1000 | Loss: 0.00002835
Iteration 99/1000 | Loss: 0.00002835
Iteration 100/1000 | Loss: 0.00002835
Iteration 101/1000 | Loss: 0.00002835
Iteration 102/1000 | Loss: 0.00002835
Iteration 103/1000 | Loss: 0.00002835
Iteration 104/1000 | Loss: 0.00002835
Iteration 105/1000 | Loss: 0.00002834
Iteration 106/1000 | Loss: 0.00002834
Iteration 107/1000 | Loss: 0.00002834
Iteration 108/1000 | Loss: 0.00002834
Iteration 109/1000 | Loss: 0.00002834
Iteration 110/1000 | Loss: 0.00002834
Iteration 111/1000 | Loss: 0.00002834
Iteration 112/1000 | Loss: 0.00002834
Iteration 113/1000 | Loss: 0.00002834
Iteration 114/1000 | Loss: 0.00002833
Iteration 115/1000 | Loss: 0.00002833
Iteration 116/1000 | Loss: 0.00002833
Iteration 117/1000 | Loss: 0.00002833
Iteration 118/1000 | Loss: 0.00002833
Iteration 119/1000 | Loss: 0.00002833
Iteration 120/1000 | Loss: 0.00002833
Iteration 121/1000 | Loss: 0.00002833
Iteration 122/1000 | Loss: 0.00002833
Iteration 123/1000 | Loss: 0.00002833
Iteration 124/1000 | Loss: 0.00002833
Iteration 125/1000 | Loss: 0.00002833
Iteration 126/1000 | Loss: 0.00002833
Iteration 127/1000 | Loss: 0.00002833
Iteration 128/1000 | Loss: 0.00002833
Iteration 129/1000 | Loss: 0.00002833
Iteration 130/1000 | Loss: 0.00002833
Iteration 131/1000 | Loss: 0.00002833
Iteration 132/1000 | Loss: 0.00002833
Iteration 133/1000 | Loss: 0.00002832
Iteration 134/1000 | Loss: 0.00002832
Iteration 135/1000 | Loss: 0.00002832
Iteration 136/1000 | Loss: 0.00002832
Iteration 137/1000 | Loss: 0.00002831
Iteration 138/1000 | Loss: 0.00002831
Iteration 139/1000 | Loss: 0.00002831
Iteration 140/1000 | Loss: 0.00002831
Iteration 141/1000 | Loss: 0.00002831
Iteration 142/1000 | Loss: 0.00002831
Iteration 143/1000 | Loss: 0.00002831
Iteration 144/1000 | Loss: 0.00002831
Iteration 145/1000 | Loss: 0.00002831
Iteration 146/1000 | Loss: 0.00002831
Iteration 147/1000 | Loss: 0.00002831
Iteration 148/1000 | Loss: 0.00002831
Iteration 149/1000 | Loss: 0.00002831
Iteration 150/1000 | Loss: 0.00002831
Iteration 151/1000 | Loss: 0.00002831
Iteration 152/1000 | Loss: 0.00002830
Iteration 153/1000 | Loss: 0.00002830
Iteration 154/1000 | Loss: 0.00002830
Iteration 155/1000 | Loss: 0.00002830
Iteration 156/1000 | Loss: 0.00002830
Iteration 157/1000 | Loss: 0.00002830
Iteration 158/1000 | Loss: 0.00002830
Iteration 159/1000 | Loss: 0.00002829
Iteration 160/1000 | Loss: 0.00002829
Iteration 161/1000 | Loss: 0.00002829
Iteration 162/1000 | Loss: 0.00002829
Iteration 163/1000 | Loss: 0.00002829
Iteration 164/1000 | Loss: 0.00002829
Iteration 165/1000 | Loss: 0.00002829
Iteration 166/1000 | Loss: 0.00002829
Iteration 167/1000 | Loss: 0.00002828
Iteration 168/1000 | Loss: 0.00002828
Iteration 169/1000 | Loss: 0.00002828
Iteration 170/1000 | Loss: 0.00002828
Iteration 171/1000 | Loss: 0.00002828
Iteration 172/1000 | Loss: 0.00002828
Iteration 173/1000 | Loss: 0.00002828
Iteration 174/1000 | Loss: 0.00002828
Iteration 175/1000 | Loss: 0.00002827
Iteration 176/1000 | Loss: 0.00002827
Iteration 177/1000 | Loss: 0.00002827
Iteration 178/1000 | Loss: 0.00002827
Iteration 179/1000 | Loss: 0.00002827
Iteration 180/1000 | Loss: 0.00002827
Iteration 181/1000 | Loss: 0.00002827
Iteration 182/1000 | Loss: 0.00002827
Iteration 183/1000 | Loss: 0.00002827
Iteration 184/1000 | Loss: 0.00002827
Iteration 185/1000 | Loss: 0.00002827
Iteration 186/1000 | Loss: 0.00002827
Iteration 187/1000 | Loss: 0.00002827
Iteration 188/1000 | Loss: 0.00002827
Iteration 189/1000 | Loss: 0.00002827
Iteration 190/1000 | Loss: 0.00002827
Iteration 191/1000 | Loss: 0.00002827
Iteration 192/1000 | Loss: 0.00002827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.8265754735912196e-05, 2.8265754735912196e-05, 2.8265754735912196e-05, 2.8265754735912196e-05, 2.8265754735912196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8265754735912196e-05

Optimization complete. Final v2v error: 4.536141395568848 mm

Highest mean error: 5.161018371582031 mm for frame 74

Lowest mean error: 3.570883274078369 mm for frame 8

Saving results

Total time: 44.81354260444641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408109
Iteration 2/25 | Loss: 0.00075551
Iteration 3/25 | Loss: 0.00060146
Iteration 4/25 | Loss: 0.00057504
Iteration 5/25 | Loss: 0.00056727
Iteration 6/25 | Loss: 0.00056511
Iteration 7/25 | Loss: 0.00056454
Iteration 8/25 | Loss: 0.00056454
Iteration 9/25 | Loss: 0.00056454
Iteration 10/25 | Loss: 0.00056454
Iteration 11/25 | Loss: 0.00056454
Iteration 12/25 | Loss: 0.00056454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005645377095788717, 0.0005645377095788717, 0.0005645377095788717, 0.0005645377095788717, 0.0005645377095788717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005645377095788717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63011956
Iteration 2/25 | Loss: 0.00056894
Iteration 3/25 | Loss: 0.00056894
Iteration 4/25 | Loss: 0.00056894
Iteration 5/25 | Loss: 0.00056894
Iteration 6/25 | Loss: 0.00056894
Iteration 7/25 | Loss: 0.00056894
Iteration 8/25 | Loss: 0.00056894
Iteration 9/25 | Loss: 0.00056894
Iteration 10/25 | Loss: 0.00056894
Iteration 11/25 | Loss: 0.00056894
Iteration 12/25 | Loss: 0.00056894
Iteration 13/25 | Loss: 0.00056894
Iteration 14/25 | Loss: 0.00056894
Iteration 15/25 | Loss: 0.00056894
Iteration 16/25 | Loss: 0.00056894
Iteration 17/25 | Loss: 0.00056894
Iteration 18/25 | Loss: 0.00056894
Iteration 19/25 | Loss: 0.00056894
Iteration 20/25 | Loss: 0.00056894
Iteration 21/25 | Loss: 0.00056894
Iteration 22/25 | Loss: 0.00056894
Iteration 23/25 | Loss: 0.00056894
Iteration 24/25 | Loss: 0.00056894
Iteration 25/25 | Loss: 0.00056894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056894
Iteration 2/1000 | Loss: 0.00002093
Iteration 3/1000 | Loss: 0.00001576
Iteration 4/1000 | Loss: 0.00001416
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001306
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00001271
Iteration 9/1000 | Loss: 0.00001259
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001242
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001241
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001226
Iteration 16/1000 | Loss: 0.00001225
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001220
Iteration 23/1000 | Loss: 0.00001220
Iteration 24/1000 | Loss: 0.00001220
Iteration 25/1000 | Loss: 0.00001219
Iteration 26/1000 | Loss: 0.00001219
Iteration 27/1000 | Loss: 0.00001219
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001216
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001214
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001213
Iteration 53/1000 | Loss: 0.00001213
Iteration 54/1000 | Loss: 0.00001213
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001212
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001210
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001209
Iteration 77/1000 | Loss: 0.00001209
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001208
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001206
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001199
Iteration 133/1000 | Loss: 0.00001199
Iteration 134/1000 | Loss: 0.00001199
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001198
Iteration 147/1000 | Loss: 0.00001198
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001198
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001197
Iteration 161/1000 | Loss: 0.00001197
Iteration 162/1000 | Loss: 0.00001197
Iteration 163/1000 | Loss: 0.00001197
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001197
Iteration 173/1000 | Loss: 0.00001197
Iteration 174/1000 | Loss: 0.00001197
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001197
Iteration 177/1000 | Loss: 0.00001197
Iteration 178/1000 | Loss: 0.00001197
Iteration 179/1000 | Loss: 0.00001197
Iteration 180/1000 | Loss: 0.00001197
Iteration 181/1000 | Loss: 0.00001196
Iteration 182/1000 | Loss: 0.00001196
Iteration 183/1000 | Loss: 0.00001196
Iteration 184/1000 | Loss: 0.00001196
Iteration 185/1000 | Loss: 0.00001196
Iteration 186/1000 | Loss: 0.00001196
Iteration 187/1000 | Loss: 0.00001196
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001195
Iteration 194/1000 | Loss: 0.00001195
Iteration 195/1000 | Loss: 0.00001195
Iteration 196/1000 | Loss: 0.00001195
Iteration 197/1000 | Loss: 0.00001195
Iteration 198/1000 | Loss: 0.00001195
Iteration 199/1000 | Loss: 0.00001195
Iteration 200/1000 | Loss: 0.00001195
Iteration 201/1000 | Loss: 0.00001195
Iteration 202/1000 | Loss: 0.00001195
Iteration 203/1000 | Loss: 0.00001195
Iteration 204/1000 | Loss: 0.00001194
Iteration 205/1000 | Loss: 0.00001194
Iteration 206/1000 | Loss: 0.00001194
Iteration 207/1000 | Loss: 0.00001194
Iteration 208/1000 | Loss: 0.00001194
Iteration 209/1000 | Loss: 0.00001194
Iteration 210/1000 | Loss: 0.00001194
Iteration 211/1000 | Loss: 0.00001194
Iteration 212/1000 | Loss: 0.00001194
Iteration 213/1000 | Loss: 0.00001194
Iteration 214/1000 | Loss: 0.00001194
Iteration 215/1000 | Loss: 0.00001194
Iteration 216/1000 | Loss: 0.00001194
Iteration 217/1000 | Loss: 0.00001194
Iteration 218/1000 | Loss: 0.00001194
Iteration 219/1000 | Loss: 0.00001194
Iteration 220/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.1938240277231671e-05, 1.1938240277231671e-05, 1.1938240277231671e-05, 1.1938240277231671e-05, 1.1938240277231671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1938240277231671e-05

Optimization complete. Final v2v error: 2.9132118225097656 mm

Highest mean error: 3.5276989936828613 mm for frame 190

Lowest mean error: 2.4431304931640625 mm for frame 146

Saving results

Total time: 44.35295391082764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808796
Iteration 2/25 | Loss: 0.00148036
Iteration 3/25 | Loss: 0.00096945
Iteration 4/25 | Loss: 0.00090227
Iteration 5/25 | Loss: 0.00087129
Iteration 6/25 | Loss: 0.00085974
Iteration 7/25 | Loss: 0.00085777
Iteration 8/25 | Loss: 0.00085693
Iteration 9/25 | Loss: 0.00085684
Iteration 10/25 | Loss: 0.00085684
Iteration 11/25 | Loss: 0.00085684
Iteration 12/25 | Loss: 0.00085684
Iteration 13/25 | Loss: 0.00085684
Iteration 14/25 | Loss: 0.00085684
Iteration 15/25 | Loss: 0.00085684
Iteration 16/25 | Loss: 0.00085684
Iteration 17/25 | Loss: 0.00085684
Iteration 18/25 | Loss: 0.00085684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008568392368033528, 0.0008568392368033528, 0.0008568392368033528, 0.0008568392368033528, 0.0008568392368033528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008568392368033528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.96222019
Iteration 2/25 | Loss: 0.00077044
Iteration 3/25 | Loss: 0.00077009
Iteration 4/25 | Loss: 0.00077009
Iteration 5/25 | Loss: 0.00077009
Iteration 6/25 | Loss: 0.00077009
Iteration 7/25 | Loss: 0.00077009
Iteration 8/25 | Loss: 0.00077009
Iteration 9/25 | Loss: 0.00077009
Iteration 10/25 | Loss: 0.00077009
Iteration 11/25 | Loss: 0.00077009
Iteration 12/25 | Loss: 0.00077009
Iteration 13/25 | Loss: 0.00077009
Iteration 14/25 | Loss: 0.00077009
Iteration 15/25 | Loss: 0.00077009
Iteration 16/25 | Loss: 0.00077009
Iteration 17/25 | Loss: 0.00077009
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007700888672843575, 0.0007700888672843575, 0.0007700888672843575, 0.0007700888672843575, 0.0007700888672843575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007700888672843575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077009
Iteration 2/1000 | Loss: 0.00006386
Iteration 3/1000 | Loss: 0.00004981
Iteration 4/1000 | Loss: 0.00004604
Iteration 5/1000 | Loss: 0.00004459
Iteration 6/1000 | Loss: 0.00004329
Iteration 7/1000 | Loss: 0.00004233
Iteration 8/1000 | Loss: 0.00004155
Iteration 9/1000 | Loss: 0.00004090
Iteration 10/1000 | Loss: 0.00004047
Iteration 11/1000 | Loss: 0.00004010
Iteration 12/1000 | Loss: 0.00003983
Iteration 13/1000 | Loss: 0.00003960
Iteration 14/1000 | Loss: 0.00003940
Iteration 15/1000 | Loss: 0.00003926
Iteration 16/1000 | Loss: 0.00003925
Iteration 17/1000 | Loss: 0.00003921
Iteration 18/1000 | Loss: 0.00003918
Iteration 19/1000 | Loss: 0.00003918
Iteration 20/1000 | Loss: 0.00003917
Iteration 21/1000 | Loss: 0.00003917
Iteration 22/1000 | Loss: 0.00003915
Iteration 23/1000 | Loss: 0.00003914
Iteration 24/1000 | Loss: 0.00003913
Iteration 25/1000 | Loss: 0.00003912
Iteration 26/1000 | Loss: 0.00003912
Iteration 27/1000 | Loss: 0.00003912
Iteration 28/1000 | Loss: 0.00003911
Iteration 29/1000 | Loss: 0.00003911
Iteration 30/1000 | Loss: 0.00003911
Iteration 31/1000 | Loss: 0.00003911
Iteration 32/1000 | Loss: 0.00003911
Iteration 33/1000 | Loss: 0.00003911
Iteration 34/1000 | Loss: 0.00003911
Iteration 35/1000 | Loss: 0.00003911
Iteration 36/1000 | Loss: 0.00003911
Iteration 37/1000 | Loss: 0.00003910
Iteration 38/1000 | Loss: 0.00003910
Iteration 39/1000 | Loss: 0.00003910
Iteration 40/1000 | Loss: 0.00003910
Iteration 41/1000 | Loss: 0.00003910
Iteration 42/1000 | Loss: 0.00003909
Iteration 43/1000 | Loss: 0.00003908
Iteration 44/1000 | Loss: 0.00003908
Iteration 45/1000 | Loss: 0.00003907
Iteration 46/1000 | Loss: 0.00003907
Iteration 47/1000 | Loss: 0.00003907
Iteration 48/1000 | Loss: 0.00003906
Iteration 49/1000 | Loss: 0.00003905
Iteration 50/1000 | Loss: 0.00003904
Iteration 51/1000 | Loss: 0.00003904
Iteration 52/1000 | Loss: 0.00003902
Iteration 53/1000 | Loss: 0.00003902
Iteration 54/1000 | Loss: 0.00003901
Iteration 55/1000 | Loss: 0.00003900
Iteration 56/1000 | Loss: 0.00003899
Iteration 57/1000 | Loss: 0.00003899
Iteration 58/1000 | Loss: 0.00003899
Iteration 59/1000 | Loss: 0.00003898
Iteration 60/1000 | Loss: 0.00003898
Iteration 61/1000 | Loss: 0.00003897
Iteration 62/1000 | Loss: 0.00003897
Iteration 63/1000 | Loss: 0.00003897
Iteration 64/1000 | Loss: 0.00003897
Iteration 65/1000 | Loss: 0.00003896
Iteration 66/1000 | Loss: 0.00003896
Iteration 67/1000 | Loss: 0.00003896
Iteration 68/1000 | Loss: 0.00003895
Iteration 69/1000 | Loss: 0.00003894
Iteration 70/1000 | Loss: 0.00003892
Iteration 71/1000 | Loss: 0.00003892
Iteration 72/1000 | Loss: 0.00003891
Iteration 73/1000 | Loss: 0.00003891
Iteration 74/1000 | Loss: 0.00003891
Iteration 75/1000 | Loss: 0.00003890
Iteration 76/1000 | Loss: 0.00003890
Iteration 77/1000 | Loss: 0.00003890
Iteration 78/1000 | Loss: 0.00003889
Iteration 79/1000 | Loss: 0.00003889
Iteration 80/1000 | Loss: 0.00003888
Iteration 81/1000 | Loss: 0.00003888
Iteration 82/1000 | Loss: 0.00003887
Iteration 83/1000 | Loss: 0.00003887
Iteration 84/1000 | Loss: 0.00003886
Iteration 85/1000 | Loss: 0.00003886
Iteration 86/1000 | Loss: 0.00003886
Iteration 87/1000 | Loss: 0.00003885
Iteration 88/1000 | Loss: 0.00003885
Iteration 89/1000 | Loss: 0.00003885
Iteration 90/1000 | Loss: 0.00003884
Iteration 91/1000 | Loss: 0.00003884
Iteration 92/1000 | Loss: 0.00003884
Iteration 93/1000 | Loss: 0.00003883
Iteration 94/1000 | Loss: 0.00003883
Iteration 95/1000 | Loss: 0.00003882
Iteration 96/1000 | Loss: 0.00003882
Iteration 97/1000 | Loss: 0.00003882
Iteration 98/1000 | Loss: 0.00003881
Iteration 99/1000 | Loss: 0.00003881
Iteration 100/1000 | Loss: 0.00003880
Iteration 101/1000 | Loss: 0.00003880
Iteration 102/1000 | Loss: 0.00003880
Iteration 103/1000 | Loss: 0.00003880
Iteration 104/1000 | Loss: 0.00003880
Iteration 105/1000 | Loss: 0.00003880
Iteration 106/1000 | Loss: 0.00003880
Iteration 107/1000 | Loss: 0.00003880
Iteration 108/1000 | Loss: 0.00003880
Iteration 109/1000 | Loss: 0.00003880
Iteration 110/1000 | Loss: 0.00003879
Iteration 111/1000 | Loss: 0.00003879
Iteration 112/1000 | Loss: 0.00003879
Iteration 113/1000 | Loss: 0.00003879
Iteration 114/1000 | Loss: 0.00003879
Iteration 115/1000 | Loss: 0.00003879
Iteration 116/1000 | Loss: 0.00003879
Iteration 117/1000 | Loss: 0.00003879
Iteration 118/1000 | Loss: 0.00003878
Iteration 119/1000 | Loss: 0.00003878
Iteration 120/1000 | Loss: 0.00003877
Iteration 121/1000 | Loss: 0.00003877
Iteration 122/1000 | Loss: 0.00003877
Iteration 123/1000 | Loss: 0.00003877
Iteration 124/1000 | Loss: 0.00003877
Iteration 125/1000 | Loss: 0.00003877
Iteration 126/1000 | Loss: 0.00003877
Iteration 127/1000 | Loss: 0.00003876
Iteration 128/1000 | Loss: 0.00003876
Iteration 129/1000 | Loss: 0.00003876
Iteration 130/1000 | Loss: 0.00003876
Iteration 131/1000 | Loss: 0.00003875
Iteration 132/1000 | Loss: 0.00003875
Iteration 133/1000 | Loss: 0.00003875
Iteration 134/1000 | Loss: 0.00003874
Iteration 135/1000 | Loss: 0.00003874
Iteration 136/1000 | Loss: 0.00003874
Iteration 137/1000 | Loss: 0.00003874
Iteration 138/1000 | Loss: 0.00003873
Iteration 139/1000 | Loss: 0.00003873
Iteration 140/1000 | Loss: 0.00003873
Iteration 141/1000 | Loss: 0.00003872
Iteration 142/1000 | Loss: 0.00003872
Iteration 143/1000 | Loss: 0.00003872
Iteration 144/1000 | Loss: 0.00003872
Iteration 145/1000 | Loss: 0.00003872
Iteration 146/1000 | Loss: 0.00003872
Iteration 147/1000 | Loss: 0.00003871
Iteration 148/1000 | Loss: 0.00003871
Iteration 149/1000 | Loss: 0.00003871
Iteration 150/1000 | Loss: 0.00003871
Iteration 151/1000 | Loss: 0.00003871
Iteration 152/1000 | Loss: 0.00003871
Iteration 153/1000 | Loss: 0.00003871
Iteration 154/1000 | Loss: 0.00003870
Iteration 155/1000 | Loss: 0.00003870
Iteration 156/1000 | Loss: 0.00003870
Iteration 157/1000 | Loss: 0.00003870
Iteration 158/1000 | Loss: 0.00003869
Iteration 159/1000 | Loss: 0.00003869
Iteration 160/1000 | Loss: 0.00003869
Iteration 161/1000 | Loss: 0.00003868
Iteration 162/1000 | Loss: 0.00003868
Iteration 163/1000 | Loss: 0.00003868
Iteration 164/1000 | Loss: 0.00003868
Iteration 165/1000 | Loss: 0.00003868
Iteration 166/1000 | Loss: 0.00003868
Iteration 167/1000 | Loss: 0.00003868
Iteration 168/1000 | Loss: 0.00003867
Iteration 169/1000 | Loss: 0.00003867
Iteration 170/1000 | Loss: 0.00003867
Iteration 171/1000 | Loss: 0.00003867
Iteration 172/1000 | Loss: 0.00003867
Iteration 173/1000 | Loss: 0.00003867
Iteration 174/1000 | Loss: 0.00003867
Iteration 175/1000 | Loss: 0.00003866
Iteration 176/1000 | Loss: 0.00003866
Iteration 177/1000 | Loss: 0.00003866
Iteration 178/1000 | Loss: 0.00003866
Iteration 179/1000 | Loss: 0.00003866
Iteration 180/1000 | Loss: 0.00003866
Iteration 181/1000 | Loss: 0.00003866
Iteration 182/1000 | Loss: 0.00003865
Iteration 183/1000 | Loss: 0.00003865
Iteration 184/1000 | Loss: 0.00003865
Iteration 185/1000 | Loss: 0.00003865
Iteration 186/1000 | Loss: 0.00003865
Iteration 187/1000 | Loss: 0.00003865
Iteration 188/1000 | Loss: 0.00003865
Iteration 189/1000 | Loss: 0.00003865
Iteration 190/1000 | Loss: 0.00003865
Iteration 191/1000 | Loss: 0.00003865
Iteration 192/1000 | Loss: 0.00003865
Iteration 193/1000 | Loss: 0.00003865
Iteration 194/1000 | Loss: 0.00003865
Iteration 195/1000 | Loss: 0.00003865
Iteration 196/1000 | Loss: 0.00003865
Iteration 197/1000 | Loss: 0.00003865
Iteration 198/1000 | Loss: 0.00003864
Iteration 199/1000 | Loss: 0.00003864
Iteration 200/1000 | Loss: 0.00003864
Iteration 201/1000 | Loss: 0.00003863
Iteration 202/1000 | Loss: 0.00003863
Iteration 203/1000 | Loss: 0.00003863
Iteration 204/1000 | Loss: 0.00003863
Iteration 205/1000 | Loss: 0.00003863
Iteration 206/1000 | Loss: 0.00003863
Iteration 207/1000 | Loss: 0.00003863
Iteration 208/1000 | Loss: 0.00003863
Iteration 209/1000 | Loss: 0.00003863
Iteration 210/1000 | Loss: 0.00003862
Iteration 211/1000 | Loss: 0.00003862
Iteration 212/1000 | Loss: 0.00003862
Iteration 213/1000 | Loss: 0.00003862
Iteration 214/1000 | Loss: 0.00003862
Iteration 215/1000 | Loss: 0.00003862
Iteration 216/1000 | Loss: 0.00003862
Iteration 217/1000 | Loss: 0.00003862
Iteration 218/1000 | Loss: 0.00003862
Iteration 219/1000 | Loss: 0.00003861
Iteration 220/1000 | Loss: 0.00003861
Iteration 221/1000 | Loss: 0.00003861
Iteration 222/1000 | Loss: 0.00003861
Iteration 223/1000 | Loss: 0.00003861
Iteration 224/1000 | Loss: 0.00003861
Iteration 225/1000 | Loss: 0.00003861
Iteration 226/1000 | Loss: 0.00003861
Iteration 227/1000 | Loss: 0.00003861
Iteration 228/1000 | Loss: 0.00003860
Iteration 229/1000 | Loss: 0.00003860
Iteration 230/1000 | Loss: 0.00003860
Iteration 231/1000 | Loss: 0.00003860
Iteration 232/1000 | Loss: 0.00003860
Iteration 233/1000 | Loss: 0.00003859
Iteration 234/1000 | Loss: 0.00003859
Iteration 235/1000 | Loss: 0.00003859
Iteration 236/1000 | Loss: 0.00003859
Iteration 237/1000 | Loss: 0.00003859
Iteration 238/1000 | Loss: 0.00003859
Iteration 239/1000 | Loss: 0.00003859
Iteration 240/1000 | Loss: 0.00003859
Iteration 241/1000 | Loss: 0.00003859
Iteration 242/1000 | Loss: 0.00003859
Iteration 243/1000 | Loss: 0.00003859
Iteration 244/1000 | Loss: 0.00003859
Iteration 245/1000 | Loss: 0.00003859
Iteration 246/1000 | Loss: 0.00003859
Iteration 247/1000 | Loss: 0.00003858
Iteration 248/1000 | Loss: 0.00003858
Iteration 249/1000 | Loss: 0.00003858
Iteration 250/1000 | Loss: 0.00003858
Iteration 251/1000 | Loss: 0.00003858
Iteration 252/1000 | Loss: 0.00003858
Iteration 253/1000 | Loss: 0.00003858
Iteration 254/1000 | Loss: 0.00003858
Iteration 255/1000 | Loss: 0.00003858
Iteration 256/1000 | Loss: 0.00003858
Iteration 257/1000 | Loss: 0.00003858
Iteration 258/1000 | Loss: 0.00003858
Iteration 259/1000 | Loss: 0.00003857
Iteration 260/1000 | Loss: 0.00003857
Iteration 261/1000 | Loss: 0.00003857
Iteration 262/1000 | Loss: 0.00003857
Iteration 263/1000 | Loss: 0.00003857
Iteration 264/1000 | Loss: 0.00003857
Iteration 265/1000 | Loss: 0.00003857
Iteration 266/1000 | Loss: 0.00003857
Iteration 267/1000 | Loss: 0.00003857
Iteration 268/1000 | Loss: 0.00003857
Iteration 269/1000 | Loss: 0.00003857
Iteration 270/1000 | Loss: 0.00003857
Iteration 271/1000 | Loss: 0.00003857
Iteration 272/1000 | Loss: 0.00003857
Iteration 273/1000 | Loss: 0.00003857
Iteration 274/1000 | Loss: 0.00003857
Iteration 275/1000 | Loss: 0.00003857
Iteration 276/1000 | Loss: 0.00003857
Iteration 277/1000 | Loss: 0.00003857
Iteration 278/1000 | Loss: 0.00003857
Iteration 279/1000 | Loss: 0.00003857
Iteration 280/1000 | Loss: 0.00003857
Iteration 281/1000 | Loss: 0.00003857
Iteration 282/1000 | Loss: 0.00003857
Iteration 283/1000 | Loss: 0.00003857
Iteration 284/1000 | Loss: 0.00003857
Iteration 285/1000 | Loss: 0.00003857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [3.857442789012566e-05, 3.857442789012566e-05, 3.857442789012566e-05, 3.857442789012566e-05, 3.857442789012566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.857442789012566e-05

Optimization complete. Final v2v error: 5.015928745269775 mm

Highest mean error: 6.098189353942871 mm for frame 41

Lowest mean error: 3.559915781021118 mm for frame 20

Saving results

Total time: 62.510422229766846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866191
Iteration 2/25 | Loss: 0.00090763
Iteration 3/25 | Loss: 0.00071008
Iteration 4/25 | Loss: 0.00067752
Iteration 5/25 | Loss: 0.00067135
Iteration 6/25 | Loss: 0.00066984
Iteration 7/25 | Loss: 0.00066942
Iteration 8/25 | Loss: 0.00066942
Iteration 9/25 | Loss: 0.00066942
Iteration 10/25 | Loss: 0.00066942
Iteration 11/25 | Loss: 0.00066942
Iteration 12/25 | Loss: 0.00066942
Iteration 13/25 | Loss: 0.00066942
Iteration 14/25 | Loss: 0.00066942
Iteration 15/25 | Loss: 0.00066942
Iteration 16/25 | Loss: 0.00066942
Iteration 17/25 | Loss: 0.00066942
Iteration 18/25 | Loss: 0.00066942
Iteration 19/25 | Loss: 0.00066942
Iteration 20/25 | Loss: 0.00066942
Iteration 21/25 | Loss: 0.00066942
Iteration 22/25 | Loss: 0.00066942
Iteration 23/25 | Loss: 0.00066942
Iteration 24/25 | Loss: 0.00066942
Iteration 25/25 | Loss: 0.00066942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85531926
Iteration 2/25 | Loss: 0.00056277
Iteration 3/25 | Loss: 0.00056273
Iteration 4/25 | Loss: 0.00056273
Iteration 5/25 | Loss: 0.00056273
Iteration 6/25 | Loss: 0.00056273
Iteration 7/25 | Loss: 0.00056273
Iteration 8/25 | Loss: 0.00056273
Iteration 9/25 | Loss: 0.00056273
Iteration 10/25 | Loss: 0.00056273
Iteration 11/25 | Loss: 0.00056273
Iteration 12/25 | Loss: 0.00056273
Iteration 13/25 | Loss: 0.00056273
Iteration 14/25 | Loss: 0.00056273
Iteration 15/25 | Loss: 0.00056273
Iteration 16/25 | Loss: 0.00056273
Iteration 17/25 | Loss: 0.00056273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000562731409445405, 0.000562731409445405, 0.000562731409445405, 0.000562731409445405, 0.000562731409445405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000562731409445405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056273
Iteration 2/1000 | Loss: 0.00004089
Iteration 3/1000 | Loss: 0.00002845
Iteration 4/1000 | Loss: 0.00002564
Iteration 5/1000 | Loss: 0.00002438
Iteration 6/1000 | Loss: 0.00002364
Iteration 7/1000 | Loss: 0.00002308
Iteration 8/1000 | Loss: 0.00002260
Iteration 9/1000 | Loss: 0.00002232
Iteration 10/1000 | Loss: 0.00002208
Iteration 11/1000 | Loss: 0.00002203
Iteration 12/1000 | Loss: 0.00002203
Iteration 13/1000 | Loss: 0.00002202
Iteration 14/1000 | Loss: 0.00002193
Iteration 15/1000 | Loss: 0.00002186
Iteration 16/1000 | Loss: 0.00002186
Iteration 17/1000 | Loss: 0.00002183
Iteration 18/1000 | Loss: 0.00002183
Iteration 19/1000 | Loss: 0.00002180
Iteration 20/1000 | Loss: 0.00002180
Iteration 21/1000 | Loss: 0.00002179
Iteration 22/1000 | Loss: 0.00002178
Iteration 23/1000 | Loss: 0.00002178
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002177
Iteration 26/1000 | Loss: 0.00002176
Iteration 27/1000 | Loss: 0.00002175
Iteration 28/1000 | Loss: 0.00002175
Iteration 29/1000 | Loss: 0.00002175
Iteration 30/1000 | Loss: 0.00002175
Iteration 31/1000 | Loss: 0.00002174
Iteration 32/1000 | Loss: 0.00002174
Iteration 33/1000 | Loss: 0.00002173
Iteration 34/1000 | Loss: 0.00002172
Iteration 35/1000 | Loss: 0.00002172
Iteration 36/1000 | Loss: 0.00002171
Iteration 37/1000 | Loss: 0.00002171
Iteration 38/1000 | Loss: 0.00002170
Iteration 39/1000 | Loss: 0.00002170
Iteration 40/1000 | Loss: 0.00002168
Iteration 41/1000 | Loss: 0.00002168
Iteration 42/1000 | Loss: 0.00002168
Iteration 43/1000 | Loss: 0.00002168
Iteration 44/1000 | Loss: 0.00002168
Iteration 45/1000 | Loss: 0.00002168
Iteration 46/1000 | Loss: 0.00002168
Iteration 47/1000 | Loss: 0.00002168
Iteration 48/1000 | Loss: 0.00002168
Iteration 49/1000 | Loss: 0.00002167
Iteration 50/1000 | Loss: 0.00002167
Iteration 51/1000 | Loss: 0.00002166
Iteration 52/1000 | Loss: 0.00002166
Iteration 53/1000 | Loss: 0.00002166
Iteration 54/1000 | Loss: 0.00002165
Iteration 55/1000 | Loss: 0.00002165
Iteration 56/1000 | Loss: 0.00002165
Iteration 57/1000 | Loss: 0.00002164
Iteration 58/1000 | Loss: 0.00002163
Iteration 59/1000 | Loss: 0.00002163
Iteration 60/1000 | Loss: 0.00002163
Iteration 61/1000 | Loss: 0.00002163
Iteration 62/1000 | Loss: 0.00002163
Iteration 63/1000 | Loss: 0.00002162
Iteration 64/1000 | Loss: 0.00002161
Iteration 65/1000 | Loss: 0.00002161
Iteration 66/1000 | Loss: 0.00002161
Iteration 67/1000 | Loss: 0.00002161
Iteration 68/1000 | Loss: 0.00002160
Iteration 69/1000 | Loss: 0.00002160
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002158
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002157
Iteration 78/1000 | Loss: 0.00002157
Iteration 79/1000 | Loss: 0.00002157
Iteration 80/1000 | Loss: 0.00002157
Iteration 81/1000 | Loss: 0.00002157
Iteration 82/1000 | Loss: 0.00002157
Iteration 83/1000 | Loss: 0.00002157
Iteration 84/1000 | Loss: 0.00002156
Iteration 85/1000 | Loss: 0.00002156
Iteration 86/1000 | Loss: 0.00002156
Iteration 87/1000 | Loss: 0.00002155
Iteration 88/1000 | Loss: 0.00002155
Iteration 89/1000 | Loss: 0.00002155
Iteration 90/1000 | Loss: 0.00002155
Iteration 91/1000 | Loss: 0.00002154
Iteration 92/1000 | Loss: 0.00002154
Iteration 93/1000 | Loss: 0.00002154
Iteration 94/1000 | Loss: 0.00002154
Iteration 95/1000 | Loss: 0.00002154
Iteration 96/1000 | Loss: 0.00002154
Iteration 97/1000 | Loss: 0.00002153
Iteration 98/1000 | Loss: 0.00002153
Iteration 99/1000 | Loss: 0.00002153
Iteration 100/1000 | Loss: 0.00002153
Iteration 101/1000 | Loss: 0.00002153
Iteration 102/1000 | Loss: 0.00002152
Iteration 103/1000 | Loss: 0.00002152
Iteration 104/1000 | Loss: 0.00002152
Iteration 105/1000 | Loss: 0.00002152
Iteration 106/1000 | Loss: 0.00002151
Iteration 107/1000 | Loss: 0.00002151
Iteration 108/1000 | Loss: 0.00002151
Iteration 109/1000 | Loss: 0.00002151
Iteration 110/1000 | Loss: 0.00002151
Iteration 111/1000 | Loss: 0.00002151
Iteration 112/1000 | Loss: 0.00002151
Iteration 113/1000 | Loss: 0.00002151
Iteration 114/1000 | Loss: 0.00002151
Iteration 115/1000 | Loss: 0.00002151
Iteration 116/1000 | Loss: 0.00002150
Iteration 117/1000 | Loss: 0.00002150
Iteration 118/1000 | Loss: 0.00002150
Iteration 119/1000 | Loss: 0.00002149
Iteration 120/1000 | Loss: 0.00002149
Iteration 121/1000 | Loss: 0.00002149
Iteration 122/1000 | Loss: 0.00002149
Iteration 123/1000 | Loss: 0.00002149
Iteration 124/1000 | Loss: 0.00002148
Iteration 125/1000 | Loss: 0.00002148
Iteration 126/1000 | Loss: 0.00002148
Iteration 127/1000 | Loss: 0.00002148
Iteration 128/1000 | Loss: 0.00002148
Iteration 129/1000 | Loss: 0.00002147
Iteration 130/1000 | Loss: 0.00002147
Iteration 131/1000 | Loss: 0.00002147
Iteration 132/1000 | Loss: 0.00002147
Iteration 133/1000 | Loss: 0.00002147
Iteration 134/1000 | Loss: 0.00002147
Iteration 135/1000 | Loss: 0.00002146
Iteration 136/1000 | Loss: 0.00002146
Iteration 137/1000 | Loss: 0.00002146
Iteration 138/1000 | Loss: 0.00002146
Iteration 139/1000 | Loss: 0.00002146
Iteration 140/1000 | Loss: 0.00002145
Iteration 141/1000 | Loss: 0.00002145
Iteration 142/1000 | Loss: 0.00002145
Iteration 143/1000 | Loss: 0.00002145
Iteration 144/1000 | Loss: 0.00002145
Iteration 145/1000 | Loss: 0.00002145
Iteration 146/1000 | Loss: 0.00002145
Iteration 147/1000 | Loss: 0.00002145
Iteration 148/1000 | Loss: 0.00002145
Iteration 149/1000 | Loss: 0.00002145
Iteration 150/1000 | Loss: 0.00002145
Iteration 151/1000 | Loss: 0.00002145
Iteration 152/1000 | Loss: 0.00002145
Iteration 153/1000 | Loss: 0.00002145
Iteration 154/1000 | Loss: 0.00002145
Iteration 155/1000 | Loss: 0.00002145
Iteration 156/1000 | Loss: 0.00002145
Iteration 157/1000 | Loss: 0.00002145
Iteration 158/1000 | Loss: 0.00002145
Iteration 159/1000 | Loss: 0.00002145
Iteration 160/1000 | Loss: 0.00002145
Iteration 161/1000 | Loss: 0.00002145
Iteration 162/1000 | Loss: 0.00002145
Iteration 163/1000 | Loss: 0.00002145
Iteration 164/1000 | Loss: 0.00002145
Iteration 165/1000 | Loss: 0.00002145
Iteration 166/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.144928294001147e-05, 2.144928294001147e-05, 2.144928294001147e-05, 2.144928294001147e-05, 2.144928294001147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.144928294001147e-05

Optimization complete. Final v2v error: 3.9406232833862305 mm

Highest mean error: 4.373651027679443 mm for frame 89

Lowest mean error: 3.522500991821289 mm for frame 2

Saving results

Total time: 37.263386726379395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00341032
Iteration 2/25 | Loss: 0.00118580
Iteration 3/25 | Loss: 0.00074228
Iteration 4/25 | Loss: 0.00061598
Iteration 5/25 | Loss: 0.00059348
Iteration 6/25 | Loss: 0.00058656
Iteration 7/25 | Loss: 0.00058498
Iteration 8/25 | Loss: 0.00058445
Iteration 9/25 | Loss: 0.00058433
Iteration 10/25 | Loss: 0.00058433
Iteration 11/25 | Loss: 0.00058433
Iteration 12/25 | Loss: 0.00058433
Iteration 13/25 | Loss: 0.00058433
Iteration 14/25 | Loss: 0.00058433
Iteration 15/25 | Loss: 0.00058433
Iteration 16/25 | Loss: 0.00058433
Iteration 17/25 | Loss: 0.00058433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005843309918418527, 0.0005843309918418527, 0.0005843309918418527, 0.0005843309918418527, 0.0005843309918418527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005843309918418527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58506000
Iteration 2/25 | Loss: 0.00069607
Iteration 3/25 | Loss: 0.00069607
Iteration 4/25 | Loss: 0.00069607
Iteration 5/25 | Loss: 0.00069607
Iteration 6/25 | Loss: 0.00069607
Iteration 7/25 | Loss: 0.00069607
Iteration 8/25 | Loss: 0.00069607
Iteration 9/25 | Loss: 0.00069607
Iteration 10/25 | Loss: 0.00069607
Iteration 11/25 | Loss: 0.00069607
Iteration 12/25 | Loss: 0.00069607
Iteration 13/25 | Loss: 0.00069607
Iteration 14/25 | Loss: 0.00069607
Iteration 15/25 | Loss: 0.00069607
Iteration 16/25 | Loss: 0.00069607
Iteration 17/25 | Loss: 0.00069607
Iteration 18/25 | Loss: 0.00069607
Iteration 19/25 | Loss: 0.00069607
Iteration 20/25 | Loss: 0.00069607
Iteration 21/25 | Loss: 0.00069607
Iteration 22/25 | Loss: 0.00069607
Iteration 23/25 | Loss: 0.00069607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006960689788684249, 0.0006960689788684249, 0.0006960689788684249, 0.0006960689788684249, 0.0006960689788684249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006960689788684249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069607
Iteration 2/1000 | Loss: 0.00002214
Iteration 3/1000 | Loss: 0.00001717
Iteration 4/1000 | Loss: 0.00001415
Iteration 5/1000 | Loss: 0.00001313
Iteration 6/1000 | Loss: 0.00001251
Iteration 7/1000 | Loss: 0.00001222
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001179
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001151
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001143
Iteration 17/1000 | Loss: 0.00001143
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001136
Iteration 22/1000 | Loss: 0.00001135
Iteration 23/1000 | Loss: 0.00001135
Iteration 24/1000 | Loss: 0.00001134
Iteration 25/1000 | Loss: 0.00001134
Iteration 26/1000 | Loss: 0.00001134
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001133
Iteration 29/1000 | Loss: 0.00001133
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001132
Iteration 32/1000 | Loss: 0.00001132
Iteration 33/1000 | Loss: 0.00001131
Iteration 34/1000 | Loss: 0.00001131
Iteration 35/1000 | Loss: 0.00001131
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001130
Iteration 40/1000 | Loss: 0.00001130
Iteration 41/1000 | Loss: 0.00001130
Iteration 42/1000 | Loss: 0.00001130
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001129
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001129
Iteration 50/1000 | Loss: 0.00001129
Iteration 51/1000 | Loss: 0.00001129
Iteration 52/1000 | Loss: 0.00001129
Iteration 53/1000 | Loss: 0.00001129
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001129
Iteration 57/1000 | Loss: 0.00001129
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001128
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001128
Iteration 63/1000 | Loss: 0.00001128
Iteration 64/1000 | Loss: 0.00001128
Iteration 65/1000 | Loss: 0.00001128
Iteration 66/1000 | Loss: 0.00001128
Iteration 67/1000 | Loss: 0.00001128
Iteration 68/1000 | Loss: 0.00001128
Iteration 69/1000 | Loss: 0.00001128
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001128
Iteration 95/1000 | Loss: 0.00001128
Iteration 96/1000 | Loss: 0.00001128
Iteration 97/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.12842953967629e-05, 1.12842953967629e-05, 1.12842953967629e-05, 1.12842953967629e-05, 1.12842953967629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.12842953967629e-05

Optimization complete. Final v2v error: 2.8785741329193115 mm

Highest mean error: 3.079106092453003 mm for frame 7

Lowest mean error: 2.6476941108703613 mm for frame 67

Saving results

Total time: 32.50538158416748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859463
Iteration 2/25 | Loss: 0.00081297
Iteration 3/25 | Loss: 0.00060957
Iteration 4/25 | Loss: 0.00057085
Iteration 5/25 | Loss: 0.00056299
Iteration 6/25 | Loss: 0.00056070
Iteration 7/25 | Loss: 0.00056007
Iteration 8/25 | Loss: 0.00056007
Iteration 9/25 | Loss: 0.00056007
Iteration 10/25 | Loss: 0.00056007
Iteration 11/25 | Loss: 0.00056007
Iteration 12/25 | Loss: 0.00056007
Iteration 13/25 | Loss: 0.00056007
Iteration 14/25 | Loss: 0.00056007
Iteration 15/25 | Loss: 0.00056007
Iteration 16/25 | Loss: 0.00056007
Iteration 17/25 | Loss: 0.00056007
Iteration 18/25 | Loss: 0.00056007
Iteration 19/25 | Loss: 0.00056007
Iteration 20/25 | Loss: 0.00056007
Iteration 21/25 | Loss: 0.00056007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005600721342489123, 0.0005600721342489123, 0.0005600721342489123, 0.0005600721342489123, 0.0005600721342489123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005600721342489123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52844870
Iteration 2/25 | Loss: 0.00053153
Iteration 3/25 | Loss: 0.00053153
Iteration 4/25 | Loss: 0.00053153
Iteration 5/25 | Loss: 0.00053153
Iteration 6/25 | Loss: 0.00053153
Iteration 7/25 | Loss: 0.00053153
Iteration 8/25 | Loss: 0.00053153
Iteration 9/25 | Loss: 0.00053153
Iteration 10/25 | Loss: 0.00053153
Iteration 11/25 | Loss: 0.00053153
Iteration 12/25 | Loss: 0.00053153
Iteration 13/25 | Loss: 0.00053153
Iteration 14/25 | Loss: 0.00053153
Iteration 15/25 | Loss: 0.00053153
Iteration 16/25 | Loss: 0.00053153
Iteration 17/25 | Loss: 0.00053153
Iteration 18/25 | Loss: 0.00053153
Iteration 19/25 | Loss: 0.00053153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005315280868671834, 0.0005315280868671834, 0.0005315280868671834, 0.0005315280868671834, 0.0005315280868671834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005315280868671834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053153
Iteration 2/1000 | Loss: 0.00002237
Iteration 3/1000 | Loss: 0.00001778
Iteration 4/1000 | Loss: 0.00001554
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001441
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001368
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00001360
Iteration 12/1000 | Loss: 0.00001359
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001358
Iteration 15/1000 | Loss: 0.00001357
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001355
Iteration 19/1000 | Loss: 0.00001355
Iteration 20/1000 | Loss: 0.00001355
Iteration 21/1000 | Loss: 0.00001354
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001351
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001349
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001342
Iteration 55/1000 | Loss: 0.00001342
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00001341
Iteration 59/1000 | Loss: 0.00001341
Iteration 60/1000 | Loss: 0.00001340
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001339
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001338
Iteration 66/1000 | Loss: 0.00001338
Iteration 67/1000 | Loss: 0.00001338
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001338
Iteration 71/1000 | Loss: 0.00001338
Iteration 72/1000 | Loss: 0.00001338
Iteration 73/1000 | Loss: 0.00001337
Iteration 74/1000 | Loss: 0.00001337
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001336
Iteration 77/1000 | Loss: 0.00001336
Iteration 78/1000 | Loss: 0.00001335
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001334
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001326
Iteration 125/1000 | Loss: 0.00001326
Iteration 126/1000 | Loss: 0.00001326
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001326
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001326
Iteration 133/1000 | Loss: 0.00001325
Iteration 134/1000 | Loss: 0.00001325
Iteration 135/1000 | Loss: 0.00001325
Iteration 136/1000 | Loss: 0.00001325
Iteration 137/1000 | Loss: 0.00001325
Iteration 138/1000 | Loss: 0.00001325
Iteration 139/1000 | Loss: 0.00001325
Iteration 140/1000 | Loss: 0.00001325
Iteration 141/1000 | Loss: 0.00001325
Iteration 142/1000 | Loss: 0.00001325
Iteration 143/1000 | Loss: 0.00001325
Iteration 144/1000 | Loss: 0.00001325
Iteration 145/1000 | Loss: 0.00001325
Iteration 146/1000 | Loss: 0.00001325
Iteration 147/1000 | Loss: 0.00001325
Iteration 148/1000 | Loss: 0.00001325
Iteration 149/1000 | Loss: 0.00001325
Iteration 150/1000 | Loss: 0.00001324
Iteration 151/1000 | Loss: 0.00001324
Iteration 152/1000 | Loss: 0.00001324
Iteration 153/1000 | Loss: 0.00001324
Iteration 154/1000 | Loss: 0.00001324
Iteration 155/1000 | Loss: 0.00001324
Iteration 156/1000 | Loss: 0.00001324
Iteration 157/1000 | Loss: 0.00001324
Iteration 158/1000 | Loss: 0.00001324
Iteration 159/1000 | Loss: 0.00001323
Iteration 160/1000 | Loss: 0.00001323
Iteration 161/1000 | Loss: 0.00001323
Iteration 162/1000 | Loss: 0.00001323
Iteration 163/1000 | Loss: 0.00001323
Iteration 164/1000 | Loss: 0.00001323
Iteration 165/1000 | Loss: 0.00001323
Iteration 166/1000 | Loss: 0.00001323
Iteration 167/1000 | Loss: 0.00001323
Iteration 168/1000 | Loss: 0.00001323
Iteration 169/1000 | Loss: 0.00001323
Iteration 170/1000 | Loss: 0.00001323
Iteration 171/1000 | Loss: 0.00001323
Iteration 172/1000 | Loss: 0.00001323
Iteration 173/1000 | Loss: 0.00001323
Iteration 174/1000 | Loss: 0.00001323
Iteration 175/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.3232203855295666e-05, 1.3232203855295666e-05, 1.3232203855295666e-05, 1.3232203855295666e-05, 1.3232203855295666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3232203855295666e-05

Optimization complete. Final v2v error: 3.100945472717285 mm

Highest mean error: 3.4272515773773193 mm for frame 45

Lowest mean error: 2.801901340484619 mm for frame 134

Saving results

Total time: 35.12208557128906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118302
Iteration 2/25 | Loss: 0.00440880
Iteration 3/25 | Loss: 0.00273818
Iteration 4/25 | Loss: 0.00186175
Iteration 5/25 | Loss: 0.00158576
Iteration 6/25 | Loss: 0.00147741
Iteration 7/25 | Loss: 0.00133644
Iteration 8/25 | Loss: 0.00119994
Iteration 9/25 | Loss: 0.00105561
Iteration 10/25 | Loss: 0.00103548
Iteration 11/25 | Loss: 0.00097027
Iteration 12/25 | Loss: 0.00090824
Iteration 13/25 | Loss: 0.00088223
Iteration 14/25 | Loss: 0.00085128
Iteration 15/25 | Loss: 0.00083306
Iteration 16/25 | Loss: 0.00083192
Iteration 17/25 | Loss: 0.00082065
Iteration 18/25 | Loss: 0.00080948
Iteration 19/25 | Loss: 0.00081021
Iteration 20/25 | Loss: 0.00080332
Iteration 21/25 | Loss: 0.00080487
Iteration 22/25 | Loss: 0.00080646
Iteration 23/25 | Loss: 0.00080648
Iteration 24/25 | Loss: 0.00081031
Iteration 25/25 | Loss: 0.00080043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21445346
Iteration 2/25 | Loss: 0.00150145
Iteration 3/25 | Loss: 0.00076983
Iteration 4/25 | Loss: 0.00076983
Iteration 5/25 | Loss: 0.00076983
Iteration 6/25 | Loss: 0.00076983
Iteration 7/25 | Loss: 0.00076983
Iteration 8/25 | Loss: 0.00076983
Iteration 9/25 | Loss: 0.00076983
Iteration 10/25 | Loss: 0.00076983
Iteration 11/25 | Loss: 0.00076983
Iteration 12/25 | Loss: 0.00076983
Iteration 13/25 | Loss: 0.00076983
Iteration 14/25 | Loss: 0.00076983
Iteration 15/25 | Loss: 0.00076983
Iteration 16/25 | Loss: 0.00076983
Iteration 17/25 | Loss: 0.00076983
Iteration 18/25 | Loss: 0.00076983
Iteration 19/25 | Loss: 0.00076983
Iteration 20/25 | Loss: 0.00076983
Iteration 21/25 | Loss: 0.00076983
Iteration 22/25 | Loss: 0.00076983
Iteration 23/25 | Loss: 0.00076983
Iteration 24/25 | Loss: 0.00076983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007698270492255688, 0.0007698270492255688, 0.0007698270492255688, 0.0007698270492255688, 0.0007698270492255688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007698270492255688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076983
Iteration 2/1000 | Loss: 0.00175398
Iteration 3/1000 | Loss: 0.00116690
Iteration 4/1000 | Loss: 0.00242535
Iteration 5/1000 | Loss: 0.00118041
Iteration 6/1000 | Loss: 0.00120504
Iteration 7/1000 | Loss: 0.00210246
Iteration 8/1000 | Loss: 0.00122144
Iteration 9/1000 | Loss: 0.00056233
Iteration 10/1000 | Loss: 0.00110537
Iteration 11/1000 | Loss: 0.00045236
Iteration 12/1000 | Loss: 0.00076499
Iteration 13/1000 | Loss: 0.00022147
Iteration 14/1000 | Loss: 0.00126445
Iteration 15/1000 | Loss: 0.00053354
Iteration 16/1000 | Loss: 0.00035775
Iteration 17/1000 | Loss: 0.00056718
Iteration 18/1000 | Loss: 0.00042294
Iteration 19/1000 | Loss: 0.00043810
Iteration 20/1000 | Loss: 0.00032891
Iteration 21/1000 | Loss: 0.00044344
Iteration 22/1000 | Loss: 0.00083706
Iteration 23/1000 | Loss: 0.00038406
Iteration 24/1000 | Loss: 0.00035941
Iteration 25/1000 | Loss: 0.00041324
Iteration 26/1000 | Loss: 0.00027245
Iteration 27/1000 | Loss: 0.00043259
Iteration 28/1000 | Loss: 0.00043229
Iteration 29/1000 | Loss: 0.00037146
Iteration 30/1000 | Loss: 0.00041073
Iteration 31/1000 | Loss: 0.00031165
Iteration 32/1000 | Loss: 0.00024794
Iteration 33/1000 | Loss: 0.00026935
Iteration 34/1000 | Loss: 0.00039342
Iteration 35/1000 | Loss: 0.00034772
Iteration 36/1000 | Loss: 0.00041105
Iteration 37/1000 | Loss: 0.00039315
Iteration 38/1000 | Loss: 0.00025690
Iteration 39/1000 | Loss: 0.00066186
Iteration 40/1000 | Loss: 0.00068901
Iteration 41/1000 | Loss: 0.00076277
Iteration 42/1000 | Loss: 0.00044971
Iteration 43/1000 | Loss: 0.00038210
Iteration 44/1000 | Loss: 0.00041014
Iteration 45/1000 | Loss: 0.00025088
Iteration 46/1000 | Loss: 0.00021681
Iteration 47/1000 | Loss: 0.00053104
Iteration 48/1000 | Loss: 0.00035878
Iteration 49/1000 | Loss: 0.00029241
Iteration 50/1000 | Loss: 0.00039915
Iteration 51/1000 | Loss: 0.00024180
Iteration 52/1000 | Loss: 0.00028188
Iteration 53/1000 | Loss: 0.00033989
Iteration 54/1000 | Loss: 0.00039383
Iteration 55/1000 | Loss: 0.00050249
Iteration 56/1000 | Loss: 0.00034162
Iteration 57/1000 | Loss: 0.00034076
Iteration 58/1000 | Loss: 0.00031169
Iteration 59/1000 | Loss: 0.00037724
Iteration 60/1000 | Loss: 0.00034896
Iteration 61/1000 | Loss: 0.00022133
Iteration 62/1000 | Loss: 0.00019809
Iteration 63/1000 | Loss: 0.00033645
Iteration 64/1000 | Loss: 0.00013029
Iteration 65/1000 | Loss: 0.00024346
Iteration 66/1000 | Loss: 0.00027378
Iteration 67/1000 | Loss: 0.00029233
Iteration 68/1000 | Loss: 0.00012108
Iteration 69/1000 | Loss: 0.00013831
Iteration 70/1000 | Loss: 0.00019087
Iteration 71/1000 | Loss: 0.00031473
Iteration 72/1000 | Loss: 0.00021225
Iteration 73/1000 | Loss: 0.00021992
Iteration 74/1000 | Loss: 0.00024403
Iteration 75/1000 | Loss: 0.00025264
Iteration 76/1000 | Loss: 0.00033572
Iteration 77/1000 | Loss: 0.00026560
Iteration 78/1000 | Loss: 0.00030052
Iteration 79/1000 | Loss: 0.00013770
Iteration 80/1000 | Loss: 0.00019798
Iteration 81/1000 | Loss: 0.00029030
Iteration 82/1000 | Loss: 0.00031813
Iteration 83/1000 | Loss: 0.00063489
Iteration 84/1000 | Loss: 0.00034379
Iteration 85/1000 | Loss: 0.00033108
Iteration 86/1000 | Loss: 0.00014416
Iteration 87/1000 | Loss: 0.00041503
Iteration 88/1000 | Loss: 0.00021259
Iteration 89/1000 | Loss: 0.00017580
Iteration 90/1000 | Loss: 0.00026777
Iteration 91/1000 | Loss: 0.00017625
Iteration 92/1000 | Loss: 0.00016461
Iteration 93/1000 | Loss: 0.00009427
Iteration 94/1000 | Loss: 0.00028017
Iteration 95/1000 | Loss: 0.00006534
Iteration 96/1000 | Loss: 0.00006192
Iteration 97/1000 | Loss: 0.00013410
Iteration 98/1000 | Loss: 0.00036710
Iteration 99/1000 | Loss: 0.00095553
Iteration 100/1000 | Loss: 0.00021504
Iteration 101/1000 | Loss: 0.00010813
Iteration 102/1000 | Loss: 0.00013515
Iteration 103/1000 | Loss: 0.00010336
Iteration 104/1000 | Loss: 0.00011054
Iteration 105/1000 | Loss: 0.00012630
Iteration 106/1000 | Loss: 0.00022653
Iteration 107/1000 | Loss: 0.00016824
Iteration 108/1000 | Loss: 0.00017394
Iteration 109/1000 | Loss: 0.00023612
Iteration 110/1000 | Loss: 0.00019089
Iteration 111/1000 | Loss: 0.00019387
Iteration 112/1000 | Loss: 0.00014681
Iteration 113/1000 | Loss: 0.00012500
Iteration 114/1000 | Loss: 0.00021854
Iteration 115/1000 | Loss: 0.00017876
Iteration 116/1000 | Loss: 0.00016405
Iteration 117/1000 | Loss: 0.00012828
Iteration 118/1000 | Loss: 0.00007725
Iteration 119/1000 | Loss: 0.00015167
Iteration 120/1000 | Loss: 0.00016301
Iteration 121/1000 | Loss: 0.00017186
Iteration 122/1000 | Loss: 0.00020791
Iteration 123/1000 | Loss: 0.00021994
Iteration 124/1000 | Loss: 0.00016412
Iteration 125/1000 | Loss: 0.00016956
Iteration 126/1000 | Loss: 0.00024951
Iteration 127/1000 | Loss: 0.00018128
Iteration 128/1000 | Loss: 0.00019752
Iteration 129/1000 | Loss: 0.00013528
Iteration 130/1000 | Loss: 0.00014156
Iteration 131/1000 | Loss: 0.00017834
Iteration 132/1000 | Loss: 0.00007472
Iteration 133/1000 | Loss: 0.00009708
Iteration 134/1000 | Loss: 0.00007704
Iteration 135/1000 | Loss: 0.00014865
Iteration 136/1000 | Loss: 0.00013077
Iteration 137/1000 | Loss: 0.00025001
Iteration 138/1000 | Loss: 0.00018652
Iteration 139/1000 | Loss: 0.00019750
Iteration 140/1000 | Loss: 0.00013981
Iteration 141/1000 | Loss: 0.00010217
Iteration 142/1000 | Loss: 0.00011925
Iteration 143/1000 | Loss: 0.00009939
Iteration 144/1000 | Loss: 0.00015646
Iteration 145/1000 | Loss: 0.00010712
Iteration 146/1000 | Loss: 0.00012854
Iteration 147/1000 | Loss: 0.00012376
Iteration 148/1000 | Loss: 0.00010474
Iteration 149/1000 | Loss: 0.00009248
Iteration 150/1000 | Loss: 0.00011609
Iteration 151/1000 | Loss: 0.00011059
Iteration 152/1000 | Loss: 0.00013773
Iteration 153/1000 | Loss: 0.00011165
Iteration 154/1000 | Loss: 0.00013780
Iteration 155/1000 | Loss: 0.00011260
Iteration 156/1000 | Loss: 0.00029446
Iteration 157/1000 | Loss: 0.00011102
Iteration 158/1000 | Loss: 0.00016019
Iteration 159/1000 | Loss: 0.00016409
Iteration 160/1000 | Loss: 0.00014570
Iteration 161/1000 | Loss: 0.00015482
Iteration 162/1000 | Loss: 0.00014859
Iteration 163/1000 | Loss: 0.00016618
Iteration 164/1000 | Loss: 0.00018888
Iteration 165/1000 | Loss: 0.00020339
Iteration 166/1000 | Loss: 0.00025889
Iteration 167/1000 | Loss: 0.00022749
Iteration 168/1000 | Loss: 0.00019065
Iteration 169/1000 | Loss: 0.00010775
Iteration 170/1000 | Loss: 0.00019245
Iteration 171/1000 | Loss: 0.00016152
Iteration 172/1000 | Loss: 0.00012771
Iteration 173/1000 | Loss: 0.00012582
Iteration 174/1000 | Loss: 0.00010090
Iteration 175/1000 | Loss: 0.00015420
Iteration 176/1000 | Loss: 0.00020974
Iteration 177/1000 | Loss: 0.00016082
Iteration 178/1000 | Loss: 0.00008793
Iteration 179/1000 | Loss: 0.00027601
Iteration 180/1000 | Loss: 0.00016986
Iteration 181/1000 | Loss: 0.00011212
Iteration 182/1000 | Loss: 0.00016255
Iteration 183/1000 | Loss: 0.00036299
Iteration 184/1000 | Loss: 0.00020226
Iteration 185/1000 | Loss: 0.00020530
Iteration 186/1000 | Loss: 0.00015832
Iteration 187/1000 | Loss: 0.00021836
Iteration 188/1000 | Loss: 0.00026697
Iteration 189/1000 | Loss: 0.00021263
Iteration 190/1000 | Loss: 0.00022588
Iteration 191/1000 | Loss: 0.00024685
Iteration 192/1000 | Loss: 0.00017373
Iteration 193/1000 | Loss: 0.00025165
Iteration 194/1000 | Loss: 0.00016409
Iteration 195/1000 | Loss: 0.00016218
Iteration 196/1000 | Loss: 0.00020422
Iteration 197/1000 | Loss: 0.00019927
Iteration 198/1000 | Loss: 0.00008706
Iteration 199/1000 | Loss: 0.00005664
Iteration 200/1000 | Loss: 0.00004517
Iteration 201/1000 | Loss: 0.00008098
Iteration 202/1000 | Loss: 0.00007456
Iteration 203/1000 | Loss: 0.00006783
Iteration 204/1000 | Loss: 0.00017212
Iteration 205/1000 | Loss: 0.00005794
Iteration 206/1000 | Loss: 0.00010507
Iteration 207/1000 | Loss: 0.00029075
Iteration 208/1000 | Loss: 0.00008035
Iteration 209/1000 | Loss: 0.00017368
Iteration 210/1000 | Loss: 0.00012410
Iteration 211/1000 | Loss: 0.00007450
Iteration 212/1000 | Loss: 0.00011188
Iteration 213/1000 | Loss: 0.00008927
Iteration 214/1000 | Loss: 0.00015526
Iteration 215/1000 | Loss: 0.00005092
Iteration 216/1000 | Loss: 0.00005519
Iteration 217/1000 | Loss: 0.00008469
Iteration 218/1000 | Loss: 0.00009471
Iteration 219/1000 | Loss: 0.00013688
Iteration 220/1000 | Loss: 0.00006118
Iteration 221/1000 | Loss: 0.00006673
Iteration 222/1000 | Loss: 0.00005813
Iteration 223/1000 | Loss: 0.00006741
Iteration 224/1000 | Loss: 0.00008021
Iteration 225/1000 | Loss: 0.00007476
Iteration 226/1000 | Loss: 0.00007195
Iteration 227/1000 | Loss: 0.00006426
Iteration 228/1000 | Loss: 0.00008511
Iteration 229/1000 | Loss: 0.00010078
Iteration 230/1000 | Loss: 0.00020602
Iteration 231/1000 | Loss: 0.00008808
Iteration 232/1000 | Loss: 0.00011998
Iteration 233/1000 | Loss: 0.00006839
Iteration 234/1000 | Loss: 0.00008120
Iteration 235/1000 | Loss: 0.00010610
Iteration 236/1000 | Loss: 0.00010949
Iteration 237/1000 | Loss: 0.00041340
Iteration 238/1000 | Loss: 0.00011736
Iteration 239/1000 | Loss: 0.00007270
Iteration 240/1000 | Loss: 0.00013420
Iteration 241/1000 | Loss: 0.00032254
Iteration 242/1000 | Loss: 0.00010673
Iteration 243/1000 | Loss: 0.00010704
Iteration 244/1000 | Loss: 0.00007040
Iteration 245/1000 | Loss: 0.00008020
Iteration 246/1000 | Loss: 0.00007386
Iteration 247/1000 | Loss: 0.00011009
Iteration 248/1000 | Loss: 0.00006552
Iteration 249/1000 | Loss: 0.00018133
Iteration 250/1000 | Loss: 0.00006946
Iteration 251/1000 | Loss: 0.00005969
Iteration 252/1000 | Loss: 0.00006144
Iteration 253/1000 | Loss: 0.00015286
Iteration 254/1000 | Loss: 0.00014229
Iteration 255/1000 | Loss: 0.00007468
Iteration 256/1000 | Loss: 0.00012874
Iteration 257/1000 | Loss: 0.00010129
Iteration 258/1000 | Loss: 0.00006977
Iteration 259/1000 | Loss: 0.00008499
Iteration 260/1000 | Loss: 0.00006170
Iteration 261/1000 | Loss: 0.00005553
Iteration 262/1000 | Loss: 0.00016125
Iteration 263/1000 | Loss: 0.00004876
Iteration 264/1000 | Loss: 0.00004959
Iteration 265/1000 | Loss: 0.00004667
Iteration 266/1000 | Loss: 0.00006765
Iteration 267/1000 | Loss: 0.00005429
Iteration 268/1000 | Loss: 0.00006541
Iteration 269/1000 | Loss: 0.00006059
Iteration 270/1000 | Loss: 0.00006458
Iteration 271/1000 | Loss: 0.00004656
Iteration 272/1000 | Loss: 0.00004016
Iteration 273/1000 | Loss: 0.00004625
Iteration 274/1000 | Loss: 0.00005258
Iteration 275/1000 | Loss: 0.00005141
Iteration 276/1000 | Loss: 0.00006108
Iteration 277/1000 | Loss: 0.00015705
Iteration 278/1000 | Loss: 0.00005157
Iteration 279/1000 | Loss: 0.00014886
Iteration 280/1000 | Loss: 0.00004982
Iteration 281/1000 | Loss: 0.00004476
Iteration 282/1000 | Loss: 0.00006212
Iteration 283/1000 | Loss: 0.00006649
Iteration 284/1000 | Loss: 0.00005809
Iteration 285/1000 | Loss: 0.00005169
Iteration 286/1000 | Loss: 0.00005164
Iteration 287/1000 | Loss: 0.00005097
Iteration 288/1000 | Loss: 0.00005039
Iteration 289/1000 | Loss: 0.00009398
Iteration 290/1000 | Loss: 0.00005460
Iteration 291/1000 | Loss: 0.00005005
Iteration 292/1000 | Loss: 0.00004909
Iteration 293/1000 | Loss: 0.00007121
Iteration 294/1000 | Loss: 0.00013287
Iteration 295/1000 | Loss: 0.00004866
Iteration 296/1000 | Loss: 0.00005064
Iteration 297/1000 | Loss: 0.00006843
Iteration 298/1000 | Loss: 0.00006714
Iteration 299/1000 | Loss: 0.00005724
Iteration 300/1000 | Loss: 0.00005057
Iteration 301/1000 | Loss: 0.00005309
Iteration 302/1000 | Loss: 0.00005818
Iteration 303/1000 | Loss: 0.00004221
Iteration 304/1000 | Loss: 0.00006344
Iteration 305/1000 | Loss: 0.00005199
Iteration 306/1000 | Loss: 0.00005191
Iteration 307/1000 | Loss: 0.00005602
Iteration 308/1000 | Loss: 0.00005528
Iteration 309/1000 | Loss: 0.00005282
Iteration 310/1000 | Loss: 0.00005036
Iteration 311/1000 | Loss: 0.00006208
Iteration 312/1000 | Loss: 0.00006342
Iteration 313/1000 | Loss: 0.00005701
Iteration 314/1000 | Loss: 0.00004728
Iteration 315/1000 | Loss: 0.00004929
Iteration 316/1000 | Loss: 0.00005562
Iteration 317/1000 | Loss: 0.00005297
Iteration 318/1000 | Loss: 0.00005353
Iteration 319/1000 | Loss: 0.00012115
Iteration 320/1000 | Loss: 0.00016252
Iteration 321/1000 | Loss: 0.00005502
Iteration 322/1000 | Loss: 0.00010057
Iteration 323/1000 | Loss: 0.00006014
Iteration 324/1000 | Loss: 0.00004528
Iteration 325/1000 | Loss: 0.00006683
Iteration 326/1000 | Loss: 0.00004139
Iteration 327/1000 | Loss: 0.00004790
Iteration 328/1000 | Loss: 0.00005906
Iteration 329/1000 | Loss: 0.00003670
Iteration 330/1000 | Loss: 0.00006019
Iteration 331/1000 | Loss: 0.00004526
Iteration 332/1000 | Loss: 0.00003594
Iteration 333/1000 | Loss: 0.00008409
Iteration 334/1000 | Loss: 0.00019028
Iteration 335/1000 | Loss: 0.00004680
Iteration 336/1000 | Loss: 0.00006901
Iteration 337/1000 | Loss: 0.00015875
Iteration 338/1000 | Loss: 0.00003646
Iteration 339/1000 | Loss: 0.00004493
Iteration 340/1000 | Loss: 0.00003542
Iteration 341/1000 | Loss: 0.00006534
Iteration 342/1000 | Loss: 0.00010361
Iteration 343/1000 | Loss: 0.00028171
Iteration 344/1000 | Loss: 0.00005010
Iteration 345/1000 | Loss: 0.00003821
Iteration 346/1000 | Loss: 0.00007661
Iteration 347/1000 | Loss: 0.00004898
Iteration 348/1000 | Loss: 0.00003467
Iteration 349/1000 | Loss: 0.00003958
Iteration 350/1000 | Loss: 0.00003380
Iteration 351/1000 | Loss: 0.00010843
Iteration 352/1000 | Loss: 0.00003695
Iteration 353/1000 | Loss: 0.00003634
Iteration 354/1000 | Loss: 0.00004164
Iteration 355/1000 | Loss: 0.00003315
Iteration 356/1000 | Loss: 0.00003315
Iteration 357/1000 | Loss: 0.00003315
Iteration 358/1000 | Loss: 0.00003315
Iteration 359/1000 | Loss: 0.00003315
Iteration 360/1000 | Loss: 0.00003315
Iteration 361/1000 | Loss: 0.00003315
Iteration 362/1000 | Loss: 0.00003314
Iteration 363/1000 | Loss: 0.00003314
Iteration 364/1000 | Loss: 0.00003314
Iteration 365/1000 | Loss: 0.00003314
Iteration 366/1000 | Loss: 0.00003314
Iteration 367/1000 | Loss: 0.00003314
Iteration 368/1000 | Loss: 0.00003314
Iteration 369/1000 | Loss: 0.00003314
Iteration 370/1000 | Loss: 0.00003305
Iteration 371/1000 | Loss: 0.00004055
Iteration 372/1000 | Loss: 0.00004492
Iteration 373/1000 | Loss: 0.00003300
Iteration 374/1000 | Loss: 0.00003298
Iteration 375/1000 | Loss: 0.00003298
Iteration 376/1000 | Loss: 0.00003298
Iteration 377/1000 | Loss: 0.00003298
Iteration 378/1000 | Loss: 0.00003298
Iteration 379/1000 | Loss: 0.00003298
Iteration 380/1000 | Loss: 0.00003298
Iteration 381/1000 | Loss: 0.00003297
Iteration 382/1000 | Loss: 0.00003297
Iteration 383/1000 | Loss: 0.00003297
Iteration 384/1000 | Loss: 0.00003297
Iteration 385/1000 | Loss: 0.00003297
Iteration 386/1000 | Loss: 0.00003297
Iteration 387/1000 | Loss: 0.00003297
Iteration 388/1000 | Loss: 0.00003296
Iteration 389/1000 | Loss: 0.00003296
Iteration 390/1000 | Loss: 0.00003296
Iteration 391/1000 | Loss: 0.00003296
Iteration 392/1000 | Loss: 0.00003296
Iteration 393/1000 | Loss: 0.00003296
Iteration 394/1000 | Loss: 0.00003296
Iteration 395/1000 | Loss: 0.00003295
Iteration 396/1000 | Loss: 0.00003295
Iteration 397/1000 | Loss: 0.00003295
Iteration 398/1000 | Loss: 0.00003295
Iteration 399/1000 | Loss: 0.00003295
Iteration 400/1000 | Loss: 0.00003294
Iteration 401/1000 | Loss: 0.00003294
Iteration 402/1000 | Loss: 0.00003294
Iteration 403/1000 | Loss: 0.00003294
Iteration 404/1000 | Loss: 0.00003294
Iteration 405/1000 | Loss: 0.00003294
Iteration 406/1000 | Loss: 0.00003293
Iteration 407/1000 | Loss: 0.00003293
Iteration 408/1000 | Loss: 0.00003292
Iteration 409/1000 | Loss: 0.00003292
Iteration 410/1000 | Loss: 0.00003292
Iteration 411/1000 | Loss: 0.00004106
Iteration 412/1000 | Loss: 0.00003540
Iteration 413/1000 | Loss: 0.00003291
Iteration 414/1000 | Loss: 0.00003291
Iteration 415/1000 | Loss: 0.00003291
Iteration 416/1000 | Loss: 0.00003291
Iteration 417/1000 | Loss: 0.00003291
Iteration 418/1000 | Loss: 0.00003291
Iteration 419/1000 | Loss: 0.00003291
Iteration 420/1000 | Loss: 0.00003291
Iteration 421/1000 | Loss: 0.00003290
Iteration 422/1000 | Loss: 0.00003290
Iteration 423/1000 | Loss: 0.00003290
Iteration 424/1000 | Loss: 0.00003290
Iteration 425/1000 | Loss: 0.00003290
Iteration 426/1000 | Loss: 0.00003290
Iteration 427/1000 | Loss: 0.00003290
Iteration 428/1000 | Loss: 0.00003290
Iteration 429/1000 | Loss: 0.00003289
Iteration 430/1000 | Loss: 0.00003289
Iteration 431/1000 | Loss: 0.00003289
Iteration 432/1000 | Loss: 0.00003289
Iteration 433/1000 | Loss: 0.00003289
Iteration 434/1000 | Loss: 0.00003289
Iteration 435/1000 | Loss: 0.00003289
Iteration 436/1000 | Loss: 0.00003289
Iteration 437/1000 | Loss: 0.00003289
Iteration 438/1000 | Loss: 0.00003289
Iteration 439/1000 | Loss: 0.00003289
Iteration 440/1000 | Loss: 0.00003289
Iteration 441/1000 | Loss: 0.00003289
Iteration 442/1000 | Loss: 0.00003289
Iteration 443/1000 | Loss: 0.00003289
Iteration 444/1000 | Loss: 0.00003289
Iteration 445/1000 | Loss: 0.00003289
Iteration 446/1000 | Loss: 0.00003289
Iteration 447/1000 | Loss: 0.00003289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 447. Stopping optimization.
Last 5 losses: [3.2890169677557424e-05, 3.2890169677557424e-05, 3.2890169677557424e-05, 3.2890169677557424e-05, 3.2890169677557424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2890169677557424e-05

Optimization complete. Final v2v error: 3.8527987003326416 mm

Highest mean error: 19.112409591674805 mm for frame 69

Lowest mean error: 2.624516010284424 mm for frame 236

Saving results

Total time: 625.0410783290863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903666
Iteration 2/25 | Loss: 0.00108932
Iteration 3/25 | Loss: 0.00081846
Iteration 4/25 | Loss: 0.00078074
Iteration 5/25 | Loss: 0.00077030
Iteration 6/25 | Loss: 0.00076895
Iteration 7/25 | Loss: 0.00076895
Iteration 8/25 | Loss: 0.00076895
Iteration 9/25 | Loss: 0.00076895
Iteration 10/25 | Loss: 0.00076895
Iteration 11/25 | Loss: 0.00076895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007689476478844881, 0.0007689476478844881, 0.0007689476478844881, 0.0007689476478844881, 0.0007689476478844881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007689476478844881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41663146
Iteration 2/25 | Loss: 0.00073640
Iteration 3/25 | Loss: 0.00073640
Iteration 4/25 | Loss: 0.00073639
Iteration 5/25 | Loss: 0.00073639
Iteration 6/25 | Loss: 0.00073639
Iteration 7/25 | Loss: 0.00073639
Iteration 8/25 | Loss: 0.00073639
Iteration 9/25 | Loss: 0.00073639
Iteration 10/25 | Loss: 0.00073639
Iteration 11/25 | Loss: 0.00073639
Iteration 12/25 | Loss: 0.00073639
Iteration 13/25 | Loss: 0.00073639
Iteration 14/25 | Loss: 0.00073639
Iteration 15/25 | Loss: 0.00073639
Iteration 16/25 | Loss: 0.00073639
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007363930344581604, 0.0007363930344581604, 0.0007363930344581604, 0.0007363930344581604, 0.0007363930344581604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007363930344581604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073639
Iteration 2/1000 | Loss: 0.00004608
Iteration 3/1000 | Loss: 0.00003476
Iteration 4/1000 | Loss: 0.00003166
Iteration 5/1000 | Loss: 0.00003016
Iteration 6/1000 | Loss: 0.00002918
Iteration 7/1000 | Loss: 0.00002863
Iteration 8/1000 | Loss: 0.00002817
Iteration 9/1000 | Loss: 0.00002779
Iteration 10/1000 | Loss: 0.00002768
Iteration 11/1000 | Loss: 0.00002746
Iteration 12/1000 | Loss: 0.00002739
Iteration 13/1000 | Loss: 0.00002739
Iteration 14/1000 | Loss: 0.00002736
Iteration 15/1000 | Loss: 0.00002723
Iteration 16/1000 | Loss: 0.00002722
Iteration 17/1000 | Loss: 0.00002720
Iteration 18/1000 | Loss: 0.00002716
Iteration 19/1000 | Loss: 0.00002715
Iteration 20/1000 | Loss: 0.00002714
Iteration 21/1000 | Loss: 0.00002711
Iteration 22/1000 | Loss: 0.00002707
Iteration 23/1000 | Loss: 0.00002706
Iteration 24/1000 | Loss: 0.00002706
Iteration 25/1000 | Loss: 0.00002704
Iteration 26/1000 | Loss: 0.00002703
Iteration 27/1000 | Loss: 0.00002703
Iteration 28/1000 | Loss: 0.00002703
Iteration 29/1000 | Loss: 0.00002702
Iteration 30/1000 | Loss: 0.00002702
Iteration 31/1000 | Loss: 0.00002701
Iteration 32/1000 | Loss: 0.00002701
Iteration 33/1000 | Loss: 0.00002699
Iteration 34/1000 | Loss: 0.00002699
Iteration 35/1000 | Loss: 0.00002699
Iteration 36/1000 | Loss: 0.00002698
Iteration 37/1000 | Loss: 0.00002697
Iteration 38/1000 | Loss: 0.00002697
Iteration 39/1000 | Loss: 0.00002696
Iteration 40/1000 | Loss: 0.00002696
Iteration 41/1000 | Loss: 0.00002696
Iteration 42/1000 | Loss: 0.00002695
Iteration 43/1000 | Loss: 0.00002695
Iteration 44/1000 | Loss: 0.00002694
Iteration 45/1000 | Loss: 0.00002693
Iteration 46/1000 | Loss: 0.00002693
Iteration 47/1000 | Loss: 0.00002693
Iteration 48/1000 | Loss: 0.00002692
Iteration 49/1000 | Loss: 0.00002692
Iteration 50/1000 | Loss: 0.00002692
Iteration 51/1000 | Loss: 0.00002692
Iteration 52/1000 | Loss: 0.00002691
Iteration 53/1000 | Loss: 0.00002691
Iteration 54/1000 | Loss: 0.00002690
Iteration 55/1000 | Loss: 0.00002690
Iteration 56/1000 | Loss: 0.00002690
Iteration 57/1000 | Loss: 0.00002690
Iteration 58/1000 | Loss: 0.00002690
Iteration 59/1000 | Loss: 0.00002689
Iteration 60/1000 | Loss: 0.00002688
Iteration 61/1000 | Loss: 0.00002688
Iteration 62/1000 | Loss: 0.00002688
Iteration 63/1000 | Loss: 0.00002688
Iteration 64/1000 | Loss: 0.00002688
Iteration 65/1000 | Loss: 0.00002688
Iteration 66/1000 | Loss: 0.00002687
Iteration 67/1000 | Loss: 0.00002687
Iteration 68/1000 | Loss: 0.00002687
Iteration 69/1000 | Loss: 0.00002687
Iteration 70/1000 | Loss: 0.00002686
Iteration 71/1000 | Loss: 0.00002686
Iteration 72/1000 | Loss: 0.00002686
Iteration 73/1000 | Loss: 0.00002686
Iteration 74/1000 | Loss: 0.00002685
Iteration 75/1000 | Loss: 0.00002685
Iteration 76/1000 | Loss: 0.00002685
Iteration 77/1000 | Loss: 0.00002684
Iteration 78/1000 | Loss: 0.00002683
Iteration 79/1000 | Loss: 0.00002683
Iteration 80/1000 | Loss: 0.00002683
Iteration 81/1000 | Loss: 0.00002683
Iteration 82/1000 | Loss: 0.00002683
Iteration 83/1000 | Loss: 0.00002683
Iteration 84/1000 | Loss: 0.00002683
Iteration 85/1000 | Loss: 0.00002683
Iteration 86/1000 | Loss: 0.00002682
Iteration 87/1000 | Loss: 0.00002682
Iteration 88/1000 | Loss: 0.00002682
Iteration 89/1000 | Loss: 0.00002682
Iteration 90/1000 | Loss: 0.00002682
Iteration 91/1000 | Loss: 0.00002682
Iteration 92/1000 | Loss: 0.00002682
Iteration 93/1000 | Loss: 0.00002682
Iteration 94/1000 | Loss: 0.00002682
Iteration 95/1000 | Loss: 0.00002681
Iteration 96/1000 | Loss: 0.00002681
Iteration 97/1000 | Loss: 0.00002681
Iteration 98/1000 | Loss: 0.00002680
Iteration 99/1000 | Loss: 0.00002680
Iteration 100/1000 | Loss: 0.00002680
Iteration 101/1000 | Loss: 0.00002680
Iteration 102/1000 | Loss: 0.00002679
Iteration 103/1000 | Loss: 0.00002678
Iteration 104/1000 | Loss: 0.00002678
Iteration 105/1000 | Loss: 0.00002678
Iteration 106/1000 | Loss: 0.00002678
Iteration 107/1000 | Loss: 0.00002678
Iteration 108/1000 | Loss: 0.00002677
Iteration 109/1000 | Loss: 0.00002677
Iteration 110/1000 | Loss: 0.00002677
Iteration 111/1000 | Loss: 0.00002677
Iteration 112/1000 | Loss: 0.00002677
Iteration 113/1000 | Loss: 0.00002677
Iteration 114/1000 | Loss: 0.00002677
Iteration 115/1000 | Loss: 0.00002677
Iteration 116/1000 | Loss: 0.00002676
Iteration 117/1000 | Loss: 0.00002676
Iteration 118/1000 | Loss: 0.00002676
Iteration 119/1000 | Loss: 0.00002676
Iteration 120/1000 | Loss: 0.00002676
Iteration 121/1000 | Loss: 0.00002676
Iteration 122/1000 | Loss: 0.00002675
Iteration 123/1000 | Loss: 0.00002675
Iteration 124/1000 | Loss: 0.00002675
Iteration 125/1000 | Loss: 0.00002675
Iteration 126/1000 | Loss: 0.00002675
Iteration 127/1000 | Loss: 0.00002675
Iteration 128/1000 | Loss: 0.00002675
Iteration 129/1000 | Loss: 0.00002675
Iteration 130/1000 | Loss: 0.00002674
Iteration 131/1000 | Loss: 0.00002674
Iteration 132/1000 | Loss: 0.00002674
Iteration 133/1000 | Loss: 0.00002674
Iteration 134/1000 | Loss: 0.00002674
Iteration 135/1000 | Loss: 0.00002674
Iteration 136/1000 | Loss: 0.00002674
Iteration 137/1000 | Loss: 0.00002674
Iteration 138/1000 | Loss: 0.00002674
Iteration 139/1000 | Loss: 0.00002674
Iteration 140/1000 | Loss: 0.00002674
Iteration 141/1000 | Loss: 0.00002674
Iteration 142/1000 | Loss: 0.00002674
Iteration 143/1000 | Loss: 0.00002674
Iteration 144/1000 | Loss: 0.00002674
Iteration 145/1000 | Loss: 0.00002674
Iteration 146/1000 | Loss: 0.00002674
Iteration 147/1000 | Loss: 0.00002674
Iteration 148/1000 | Loss: 0.00002674
Iteration 149/1000 | Loss: 0.00002674
Iteration 150/1000 | Loss: 0.00002674
Iteration 151/1000 | Loss: 0.00002674
Iteration 152/1000 | Loss: 0.00002674
Iteration 153/1000 | Loss: 0.00002674
Iteration 154/1000 | Loss: 0.00002674
Iteration 155/1000 | Loss: 0.00002674
Iteration 156/1000 | Loss: 0.00002674
Iteration 157/1000 | Loss: 0.00002674
Iteration 158/1000 | Loss: 0.00002674
Iteration 159/1000 | Loss: 0.00002674
Iteration 160/1000 | Loss: 0.00002674
Iteration 161/1000 | Loss: 0.00002674
Iteration 162/1000 | Loss: 0.00002674
Iteration 163/1000 | Loss: 0.00002674
Iteration 164/1000 | Loss: 0.00002674
Iteration 165/1000 | Loss: 0.00002674
Iteration 166/1000 | Loss: 0.00002674
Iteration 167/1000 | Loss: 0.00002674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.6742023692349903e-05, 2.6742023692349903e-05, 2.6742023692349903e-05, 2.6742023692349903e-05, 2.6742023692349903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6742023692349903e-05

Optimization complete. Final v2v error: 4.1938676834106445 mm

Highest mean error: 6.099671363830566 mm for frame 102

Lowest mean error: 3.2741692066192627 mm for frame 139

Saving results

Total time: 41.62878060340881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035284
Iteration 2/25 | Loss: 0.00260991
Iteration 3/25 | Loss: 0.00186486
Iteration 4/25 | Loss: 0.00164411
Iteration 5/25 | Loss: 0.00153617
Iteration 6/25 | Loss: 0.00141428
Iteration 7/25 | Loss: 0.00133503
Iteration 8/25 | Loss: 0.00130317
Iteration 9/25 | Loss: 0.00129725
Iteration 10/25 | Loss: 0.00123575
Iteration 11/25 | Loss: 0.00120815
Iteration 12/25 | Loss: 0.00121578
Iteration 13/25 | Loss: 0.00121175
Iteration 14/25 | Loss: 0.00119943
Iteration 15/25 | Loss: 0.00119447
Iteration 16/25 | Loss: 0.00119296
Iteration 17/25 | Loss: 0.00119246
Iteration 18/25 | Loss: 0.00119217
Iteration 19/25 | Loss: 0.00119195
Iteration 20/25 | Loss: 0.00119172
Iteration 21/25 | Loss: 0.00119145
Iteration 22/25 | Loss: 0.00119133
Iteration 23/25 | Loss: 0.00119133
Iteration 24/25 | Loss: 0.00119133
Iteration 25/25 | Loss: 0.00119133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50335479
Iteration 2/25 | Loss: 0.00375336
Iteration 3/25 | Loss: 0.00375336
Iteration 4/25 | Loss: 0.00375336
Iteration 5/25 | Loss: 0.00375336
Iteration 6/25 | Loss: 0.00375336
Iteration 7/25 | Loss: 0.00375336
Iteration 8/25 | Loss: 0.00375336
Iteration 9/25 | Loss: 0.00375336
Iteration 10/25 | Loss: 0.00375336
Iteration 11/25 | Loss: 0.00375336
Iteration 12/25 | Loss: 0.00375336
Iteration 13/25 | Loss: 0.00375336
Iteration 14/25 | Loss: 0.00375336
Iteration 15/25 | Loss: 0.00375336
Iteration 16/25 | Loss: 0.00375336
Iteration 17/25 | Loss: 0.00375336
Iteration 18/25 | Loss: 0.00375336
Iteration 19/25 | Loss: 0.00375336
Iteration 20/25 | Loss: 0.00375336
Iteration 21/25 | Loss: 0.00375336
Iteration 22/25 | Loss: 0.00375336
Iteration 23/25 | Loss: 0.00375336
Iteration 24/25 | Loss: 0.00375336
Iteration 25/25 | Loss: 0.00375336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00375336
Iteration 2/1000 | Loss: 0.00268147
Iteration 3/1000 | Loss: 0.00151866
Iteration 4/1000 | Loss: 0.00184168
Iteration 5/1000 | Loss: 0.00223594
Iteration 6/1000 | Loss: 0.00084015
Iteration 7/1000 | Loss: 0.00103599
Iteration 8/1000 | Loss: 0.00047246
Iteration 9/1000 | Loss: 0.00034422
Iteration 10/1000 | Loss: 0.00042606
Iteration 11/1000 | Loss: 0.00021643
Iteration 12/1000 | Loss: 0.00015005
Iteration 13/1000 | Loss: 0.00011640
Iteration 14/1000 | Loss: 0.00009742
Iteration 15/1000 | Loss: 0.00008288
Iteration 16/1000 | Loss: 0.00007484
Iteration 17/1000 | Loss: 0.00026950
Iteration 18/1000 | Loss: 0.00022496
Iteration 19/1000 | Loss: 0.00053137
Iteration 20/1000 | Loss: 0.00133410
Iteration 21/1000 | Loss: 0.00154646
Iteration 22/1000 | Loss: 0.00022691
Iteration 23/1000 | Loss: 0.00009616
Iteration 24/1000 | Loss: 0.00006638
Iteration 25/1000 | Loss: 0.00005028
Iteration 26/1000 | Loss: 0.00003879
Iteration 27/1000 | Loss: 0.00003163
Iteration 28/1000 | Loss: 0.00002842
Iteration 29/1000 | Loss: 0.00002561
Iteration 30/1000 | Loss: 0.00002419
Iteration 31/1000 | Loss: 0.00002356
Iteration 32/1000 | Loss: 0.00002303
Iteration 33/1000 | Loss: 0.00002252
Iteration 34/1000 | Loss: 0.00002220
Iteration 35/1000 | Loss: 0.00002200
Iteration 36/1000 | Loss: 0.00002189
Iteration 37/1000 | Loss: 0.00002186
Iteration 38/1000 | Loss: 0.00002174
Iteration 39/1000 | Loss: 0.00002172
Iteration 40/1000 | Loss: 0.00002172
Iteration 41/1000 | Loss: 0.00002171
Iteration 42/1000 | Loss: 0.00002171
Iteration 43/1000 | Loss: 0.00002171
Iteration 44/1000 | Loss: 0.00002170
Iteration 45/1000 | Loss: 0.00002170
Iteration 46/1000 | Loss: 0.00002170
Iteration 47/1000 | Loss: 0.00002170
Iteration 48/1000 | Loss: 0.00002170
Iteration 49/1000 | Loss: 0.00002170
Iteration 50/1000 | Loss: 0.00002170
Iteration 51/1000 | Loss: 0.00002170
Iteration 52/1000 | Loss: 0.00002170
Iteration 53/1000 | Loss: 0.00002170
Iteration 54/1000 | Loss: 0.00002170
Iteration 55/1000 | Loss: 0.00002169
Iteration 56/1000 | Loss: 0.00002166
Iteration 57/1000 | Loss: 0.00002165
Iteration 58/1000 | Loss: 0.00002165
Iteration 59/1000 | Loss: 0.00002165
Iteration 60/1000 | Loss: 0.00002164
Iteration 61/1000 | Loss: 0.00002162
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002161
Iteration 64/1000 | Loss: 0.00002161
Iteration 65/1000 | Loss: 0.00002161
Iteration 66/1000 | Loss: 0.00002161
Iteration 67/1000 | Loss: 0.00002161
Iteration 68/1000 | Loss: 0.00002161
Iteration 69/1000 | Loss: 0.00002160
Iteration 70/1000 | Loss: 0.00002160
Iteration 71/1000 | Loss: 0.00002160
Iteration 72/1000 | Loss: 0.00002160
Iteration 73/1000 | Loss: 0.00002159
Iteration 74/1000 | Loss: 0.00002159
Iteration 75/1000 | Loss: 0.00002159
Iteration 76/1000 | Loss: 0.00002159
Iteration 77/1000 | Loss: 0.00002159
Iteration 78/1000 | Loss: 0.00002158
Iteration 79/1000 | Loss: 0.00002158
Iteration 80/1000 | Loss: 0.00002158
Iteration 81/1000 | Loss: 0.00002158
Iteration 82/1000 | Loss: 0.00002158
Iteration 83/1000 | Loss: 0.00002158
Iteration 84/1000 | Loss: 0.00002158
Iteration 85/1000 | Loss: 0.00002158
Iteration 86/1000 | Loss: 0.00002157
Iteration 87/1000 | Loss: 0.00002157
Iteration 88/1000 | Loss: 0.00002157
Iteration 89/1000 | Loss: 0.00002157
Iteration 90/1000 | Loss: 0.00002157
Iteration 91/1000 | Loss: 0.00002157
Iteration 92/1000 | Loss: 0.00002157
Iteration 93/1000 | Loss: 0.00002156
Iteration 94/1000 | Loss: 0.00002156
Iteration 95/1000 | Loss: 0.00002156
Iteration 96/1000 | Loss: 0.00002156
Iteration 97/1000 | Loss: 0.00002156
Iteration 98/1000 | Loss: 0.00002156
Iteration 99/1000 | Loss: 0.00002156
Iteration 100/1000 | Loss: 0.00002156
Iteration 101/1000 | Loss: 0.00002156
Iteration 102/1000 | Loss: 0.00002156
Iteration 103/1000 | Loss: 0.00002156
Iteration 104/1000 | Loss: 0.00002156
Iteration 105/1000 | Loss: 0.00002156
Iteration 106/1000 | Loss: 0.00002156
Iteration 107/1000 | Loss: 0.00002156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.1561547328019515e-05, 2.1561547328019515e-05, 2.1561547328019515e-05, 2.1561547328019515e-05, 2.1561547328019515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1561547328019515e-05

Optimization complete. Final v2v error: 3.937246322631836 mm

Highest mean error: 4.297462463378906 mm for frame 24

Lowest mean error: 3.6315650939941406 mm for frame 105

Saving results

Total time: 106.96338868141174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109023
Iteration 2/25 | Loss: 0.01109023
Iteration 3/25 | Loss: 0.00326370
Iteration 4/25 | Loss: 0.00167496
Iteration 5/25 | Loss: 0.00140697
Iteration 6/25 | Loss: 0.00121884
Iteration 7/25 | Loss: 0.00112216
Iteration 8/25 | Loss: 0.00104994
Iteration 9/25 | Loss: 0.00094382
Iteration 10/25 | Loss: 0.00081703
Iteration 11/25 | Loss: 0.00077337
Iteration 12/25 | Loss: 0.00074294
Iteration 13/25 | Loss: 0.00071694
Iteration 14/25 | Loss: 0.00070953
Iteration 15/25 | Loss: 0.00070310
Iteration 16/25 | Loss: 0.00070031
Iteration 17/25 | Loss: 0.00069943
Iteration 18/25 | Loss: 0.00069915
Iteration 19/25 | Loss: 0.00069895
Iteration 20/25 | Loss: 0.00069889
Iteration 21/25 | Loss: 0.00069889
Iteration 22/25 | Loss: 0.00069889
Iteration 23/25 | Loss: 0.00069889
Iteration 24/25 | Loss: 0.00069889
Iteration 25/25 | Loss: 0.00069889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47286963
Iteration 2/25 | Loss: 0.00069610
Iteration 3/25 | Loss: 0.00069610
Iteration 4/25 | Loss: 0.00069610
Iteration 5/25 | Loss: 0.00069610
Iteration 6/25 | Loss: 0.00069610
Iteration 7/25 | Loss: 0.00069610
Iteration 8/25 | Loss: 0.00069610
Iteration 9/25 | Loss: 0.00069610
Iteration 10/25 | Loss: 0.00069610
Iteration 11/25 | Loss: 0.00069610
Iteration 12/25 | Loss: 0.00069610
Iteration 13/25 | Loss: 0.00069610
Iteration 14/25 | Loss: 0.00069610
Iteration 15/25 | Loss: 0.00069610
Iteration 16/25 | Loss: 0.00069610
Iteration 17/25 | Loss: 0.00069610
Iteration 18/25 | Loss: 0.00069610
Iteration 19/25 | Loss: 0.00069610
Iteration 20/25 | Loss: 0.00069610
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006960950559005141, 0.0006960950559005141, 0.0006960950559005141, 0.0006960950559005141, 0.0006960950559005141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006960950559005141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069610
Iteration 2/1000 | Loss: 0.00007731
Iteration 3/1000 | Loss: 0.00005728
Iteration 4/1000 | Loss: 0.00005135
Iteration 5/1000 | Loss: 0.00004811
Iteration 6/1000 | Loss: 0.00004618
Iteration 7/1000 | Loss: 0.00004484
Iteration 8/1000 | Loss: 0.00480997
Iteration 9/1000 | Loss: 0.00004253
Iteration 10/1000 | Loss: 0.00003395
Iteration 11/1000 | Loss: 0.00002922
Iteration 12/1000 | Loss: 0.00002674
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002421
Iteration 15/1000 | Loss: 0.00002345
Iteration 16/1000 | Loss: 0.00002295
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00002241
Iteration 19/1000 | Loss: 0.00002215
Iteration 20/1000 | Loss: 0.00002195
Iteration 21/1000 | Loss: 0.00002194
Iteration 22/1000 | Loss: 0.00002193
Iteration 23/1000 | Loss: 0.00002188
Iteration 24/1000 | Loss: 0.00002183
Iteration 25/1000 | Loss: 0.00002183
Iteration 26/1000 | Loss: 0.00002183
Iteration 27/1000 | Loss: 0.00002182
Iteration 28/1000 | Loss: 0.00002182
Iteration 29/1000 | Loss: 0.00002181
Iteration 30/1000 | Loss: 0.00002177
Iteration 31/1000 | Loss: 0.00002173
Iteration 32/1000 | Loss: 0.00002171
Iteration 33/1000 | Loss: 0.00002171
Iteration 34/1000 | Loss: 0.00002171
Iteration 35/1000 | Loss: 0.00002171
Iteration 36/1000 | Loss: 0.00002171
Iteration 37/1000 | Loss: 0.00002171
Iteration 38/1000 | Loss: 0.00002171
Iteration 39/1000 | Loss: 0.00002171
Iteration 40/1000 | Loss: 0.00002171
Iteration 41/1000 | Loss: 0.00002170
Iteration 42/1000 | Loss: 0.00002170
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00002169
Iteration 46/1000 | Loss: 0.00002169
Iteration 47/1000 | Loss: 0.00002169
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002169
Iteration 50/1000 | Loss: 0.00002169
Iteration 51/1000 | Loss: 0.00002168
Iteration 52/1000 | Loss: 0.00002168
Iteration 53/1000 | Loss: 0.00002168
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002168
Iteration 57/1000 | Loss: 0.00002167
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002167
Iteration 60/1000 | Loss: 0.00002167
Iteration 61/1000 | Loss: 0.00002167
Iteration 62/1000 | Loss: 0.00002166
Iteration 63/1000 | Loss: 0.00002166
Iteration 64/1000 | Loss: 0.00002166
Iteration 65/1000 | Loss: 0.00002166
Iteration 66/1000 | Loss: 0.00002166
Iteration 67/1000 | Loss: 0.00002166
Iteration 68/1000 | Loss: 0.00002166
Iteration 69/1000 | Loss: 0.00002166
Iteration 70/1000 | Loss: 0.00002166
Iteration 71/1000 | Loss: 0.00002165
Iteration 72/1000 | Loss: 0.00002165
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002164
Iteration 77/1000 | Loss: 0.00002164
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002163
Iteration 80/1000 | Loss: 0.00002163
Iteration 81/1000 | Loss: 0.00002163
Iteration 82/1000 | Loss: 0.00002163
Iteration 83/1000 | Loss: 0.00002162
Iteration 84/1000 | Loss: 0.00002162
Iteration 85/1000 | Loss: 0.00002162
Iteration 86/1000 | Loss: 0.00002162
Iteration 87/1000 | Loss: 0.00002162
Iteration 88/1000 | Loss: 0.00002162
Iteration 89/1000 | Loss: 0.00002162
Iteration 90/1000 | Loss: 0.00002162
Iteration 91/1000 | Loss: 0.00002162
Iteration 92/1000 | Loss: 0.00002162
Iteration 93/1000 | Loss: 0.00002162
Iteration 94/1000 | Loss: 0.00002162
Iteration 95/1000 | Loss: 0.00002162
Iteration 96/1000 | Loss: 0.00002162
Iteration 97/1000 | Loss: 0.00002161
Iteration 98/1000 | Loss: 0.00002161
Iteration 99/1000 | Loss: 0.00002161
Iteration 100/1000 | Loss: 0.00002161
Iteration 101/1000 | Loss: 0.00002161
Iteration 102/1000 | Loss: 0.00002161
Iteration 103/1000 | Loss: 0.00002161
Iteration 104/1000 | Loss: 0.00002161
Iteration 105/1000 | Loss: 0.00002161
Iteration 106/1000 | Loss: 0.00002161
Iteration 107/1000 | Loss: 0.00002161
Iteration 108/1000 | Loss: 0.00002161
Iteration 109/1000 | Loss: 0.00002161
Iteration 110/1000 | Loss: 0.00002161
Iteration 111/1000 | Loss: 0.00002160
Iteration 112/1000 | Loss: 0.00002160
Iteration 113/1000 | Loss: 0.00002160
Iteration 114/1000 | Loss: 0.00002160
Iteration 115/1000 | Loss: 0.00002160
Iteration 116/1000 | Loss: 0.00002160
Iteration 117/1000 | Loss: 0.00002160
Iteration 118/1000 | Loss: 0.00002160
Iteration 119/1000 | Loss: 0.00002160
Iteration 120/1000 | Loss: 0.00002159
Iteration 121/1000 | Loss: 0.00002159
Iteration 122/1000 | Loss: 0.00002159
Iteration 123/1000 | Loss: 0.00002159
Iteration 124/1000 | Loss: 0.00002159
Iteration 125/1000 | Loss: 0.00002159
Iteration 126/1000 | Loss: 0.00002159
Iteration 127/1000 | Loss: 0.00002159
Iteration 128/1000 | Loss: 0.00002159
Iteration 129/1000 | Loss: 0.00002159
Iteration 130/1000 | Loss: 0.00002159
Iteration 131/1000 | Loss: 0.00002159
Iteration 132/1000 | Loss: 0.00002159
Iteration 133/1000 | Loss: 0.00002159
Iteration 134/1000 | Loss: 0.00002159
Iteration 135/1000 | Loss: 0.00002159
Iteration 136/1000 | Loss: 0.00002159
Iteration 137/1000 | Loss: 0.00002159
Iteration 138/1000 | Loss: 0.00002159
Iteration 139/1000 | Loss: 0.00002159
Iteration 140/1000 | Loss: 0.00002159
Iteration 141/1000 | Loss: 0.00002159
Iteration 142/1000 | Loss: 0.00002159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.158587085432373e-05, 2.158587085432373e-05, 2.158587085432373e-05, 2.158587085432373e-05, 2.158587085432373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.158587085432373e-05

Optimization complete. Final v2v error: 3.8206064701080322 mm

Highest mean error: 9.0038480758667 mm for frame 79

Lowest mean error: 3.4716618061065674 mm for frame 42

Saving results

Total time: 68.67015552520752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068338
Iteration 2/25 | Loss: 0.00291857
Iteration 3/25 | Loss: 0.00176059
Iteration 4/25 | Loss: 0.00155651
Iteration 5/25 | Loss: 0.00150549
Iteration 6/25 | Loss: 0.00145061
Iteration 7/25 | Loss: 0.00146501
Iteration 8/25 | Loss: 0.00132236
Iteration 9/25 | Loss: 0.00131959
Iteration 10/25 | Loss: 0.00127933
Iteration 11/25 | Loss: 0.00124421
Iteration 12/25 | Loss: 0.00120734
Iteration 13/25 | Loss: 0.00119436
Iteration 14/25 | Loss: 0.00118966
Iteration 15/25 | Loss: 0.00117842
Iteration 16/25 | Loss: 0.00115628
Iteration 17/25 | Loss: 0.00114310
Iteration 18/25 | Loss: 0.00113274
Iteration 19/25 | Loss: 0.00112715
Iteration 20/25 | Loss: 0.00111888
Iteration 21/25 | Loss: 0.00111717
Iteration 22/25 | Loss: 0.00111216
Iteration 23/25 | Loss: 0.00110668
Iteration 24/25 | Loss: 0.00110769
Iteration 25/25 | Loss: 0.00110296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48310709
Iteration 2/25 | Loss: 0.00523805
Iteration 3/25 | Loss: 0.00511823
Iteration 4/25 | Loss: 0.00511807
Iteration 5/25 | Loss: 0.00511807
Iteration 6/25 | Loss: 0.00511807
Iteration 7/25 | Loss: 0.00511807
Iteration 8/25 | Loss: 0.00511807
Iteration 9/25 | Loss: 0.00511807
Iteration 10/25 | Loss: 0.00511807
Iteration 11/25 | Loss: 0.00511807
Iteration 12/25 | Loss: 0.00511807
Iteration 13/25 | Loss: 0.00511807
Iteration 14/25 | Loss: 0.00511807
Iteration 15/25 | Loss: 0.00511807
Iteration 16/25 | Loss: 0.00511807
Iteration 17/25 | Loss: 0.00511807
Iteration 18/25 | Loss: 0.00511807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005118065979331732, 0.005118065979331732, 0.005118065979331732, 0.005118065979331732, 0.005118065979331732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005118065979331732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00511807
Iteration 2/1000 | Loss: 0.00311821
Iteration 3/1000 | Loss: 0.00255819
Iteration 4/1000 | Loss: 0.00095768
Iteration 5/1000 | Loss: 0.00058160
Iteration 6/1000 | Loss: 0.00041107
Iteration 7/1000 | Loss: 0.00116519
Iteration 8/1000 | Loss: 0.00099558
Iteration 9/1000 | Loss: 0.00089537
Iteration 10/1000 | Loss: 0.00154054
Iteration 11/1000 | Loss: 0.00107169
Iteration 12/1000 | Loss: 0.00044821
Iteration 13/1000 | Loss: 0.00092848
Iteration 14/1000 | Loss: 0.00020005
Iteration 15/1000 | Loss: 0.00055682
Iteration 16/1000 | Loss: 0.00206726
Iteration 17/1000 | Loss: 0.00085931
Iteration 18/1000 | Loss: 0.00084522
Iteration 19/1000 | Loss: 0.00224307
Iteration 20/1000 | Loss: 0.00201971
Iteration 21/1000 | Loss: 0.00058650
Iteration 22/1000 | Loss: 0.00027925
Iteration 23/1000 | Loss: 0.00016926
Iteration 24/1000 | Loss: 0.00027182
Iteration 25/1000 | Loss: 0.00205053
Iteration 26/1000 | Loss: 0.00055611
Iteration 27/1000 | Loss: 0.00104788
Iteration 28/1000 | Loss: 0.00067500
Iteration 29/1000 | Loss: 0.00012989
Iteration 30/1000 | Loss: 0.00011840
Iteration 31/1000 | Loss: 0.00129579
Iteration 32/1000 | Loss: 0.00038396
Iteration 33/1000 | Loss: 0.00031798
Iteration 34/1000 | Loss: 0.00010578
Iteration 35/1000 | Loss: 0.00010483
Iteration 36/1000 | Loss: 0.00141822
Iteration 37/1000 | Loss: 0.00056687
Iteration 38/1000 | Loss: 0.00013788
Iteration 39/1000 | Loss: 0.00012040
Iteration 40/1000 | Loss: 0.00009748
Iteration 41/1000 | Loss: 0.00032205
Iteration 42/1000 | Loss: 0.00009983
Iteration 43/1000 | Loss: 0.00108283
Iteration 44/1000 | Loss: 0.00092418
Iteration 45/1000 | Loss: 0.00093824
Iteration 46/1000 | Loss: 0.00032428
Iteration 47/1000 | Loss: 0.00009169
Iteration 48/1000 | Loss: 0.00101044
Iteration 49/1000 | Loss: 0.00042665
Iteration 50/1000 | Loss: 0.00081942
Iteration 51/1000 | Loss: 0.00026510
Iteration 52/1000 | Loss: 0.00029424
Iteration 53/1000 | Loss: 0.00025058
Iteration 54/1000 | Loss: 0.00024926
Iteration 55/1000 | Loss: 0.00008018
Iteration 56/1000 | Loss: 0.00023433
Iteration 57/1000 | Loss: 0.00045538
Iteration 58/1000 | Loss: 0.00023969
Iteration 59/1000 | Loss: 0.00014135
Iteration 60/1000 | Loss: 0.00021598
Iteration 61/1000 | Loss: 0.00016516
Iteration 62/1000 | Loss: 0.00063277
Iteration 63/1000 | Loss: 0.00040217
Iteration 64/1000 | Loss: 0.00009820
Iteration 65/1000 | Loss: 0.00008056
Iteration 66/1000 | Loss: 0.00007618
Iteration 67/1000 | Loss: 0.00007237
Iteration 68/1000 | Loss: 0.00028319
Iteration 69/1000 | Loss: 0.00026308
Iteration 70/1000 | Loss: 0.00028175
Iteration 71/1000 | Loss: 0.00026928
Iteration 72/1000 | Loss: 0.00056532
Iteration 73/1000 | Loss: 0.00020817
Iteration 74/1000 | Loss: 0.00027880
Iteration 75/1000 | Loss: 0.00007362
Iteration 76/1000 | Loss: 0.00007176
Iteration 77/1000 | Loss: 0.00029012
Iteration 78/1000 | Loss: 0.00008029
Iteration 79/1000 | Loss: 0.00077816
Iteration 80/1000 | Loss: 0.00051514
Iteration 81/1000 | Loss: 0.00009012
Iteration 82/1000 | Loss: 0.00007970
Iteration 83/1000 | Loss: 0.00029906
Iteration 84/1000 | Loss: 0.00008057
Iteration 85/1000 | Loss: 0.00025232
Iteration 86/1000 | Loss: 0.00029978
Iteration 87/1000 | Loss: 0.00006848
Iteration 88/1000 | Loss: 0.00024213
Iteration 89/1000 | Loss: 0.00006999
Iteration 90/1000 | Loss: 0.00027299
Iteration 91/1000 | Loss: 0.00019929
Iteration 92/1000 | Loss: 0.00020043
Iteration 93/1000 | Loss: 0.00007331
Iteration 94/1000 | Loss: 0.00006827
Iteration 95/1000 | Loss: 0.00006520
Iteration 96/1000 | Loss: 0.00006365
Iteration 97/1000 | Loss: 0.00006272
Iteration 98/1000 | Loss: 0.00006211
Iteration 99/1000 | Loss: 0.00007073
Iteration 100/1000 | Loss: 0.00006561
Iteration 101/1000 | Loss: 0.00006383
Iteration 102/1000 | Loss: 0.00006263
Iteration 103/1000 | Loss: 0.00006195
Iteration 104/1000 | Loss: 0.00006190
Iteration 105/1000 | Loss: 0.00006157
Iteration 106/1000 | Loss: 0.00006120
Iteration 107/1000 | Loss: 0.00006076
Iteration 108/1000 | Loss: 0.00006043
Iteration 109/1000 | Loss: 0.00006016
Iteration 110/1000 | Loss: 0.00005985
Iteration 111/1000 | Loss: 0.00005969
Iteration 112/1000 | Loss: 0.00005969
Iteration 113/1000 | Loss: 0.00005953
Iteration 114/1000 | Loss: 0.00005948
Iteration 115/1000 | Loss: 0.00005945
Iteration 116/1000 | Loss: 0.00005945
Iteration 117/1000 | Loss: 0.00005944
Iteration 118/1000 | Loss: 0.00005944
Iteration 119/1000 | Loss: 0.00005943
Iteration 120/1000 | Loss: 0.00005943
Iteration 121/1000 | Loss: 0.00005942
Iteration 122/1000 | Loss: 0.00005942
Iteration 123/1000 | Loss: 0.00005942
Iteration 124/1000 | Loss: 0.00005941
Iteration 125/1000 | Loss: 0.00005940
Iteration 126/1000 | Loss: 0.00005940
Iteration 127/1000 | Loss: 0.00005929
Iteration 128/1000 | Loss: 0.00005923
Iteration 129/1000 | Loss: 0.00005916
Iteration 130/1000 | Loss: 0.00005910
Iteration 131/1000 | Loss: 0.00005909
Iteration 132/1000 | Loss: 0.00005909
Iteration 133/1000 | Loss: 0.00005908
Iteration 134/1000 | Loss: 0.00005908
Iteration 135/1000 | Loss: 0.00005907
Iteration 136/1000 | Loss: 0.00005907
Iteration 137/1000 | Loss: 0.00005906
Iteration 138/1000 | Loss: 0.00005906
Iteration 139/1000 | Loss: 0.00005906
Iteration 140/1000 | Loss: 0.00005905
Iteration 141/1000 | Loss: 0.00005905
Iteration 142/1000 | Loss: 0.00005905
Iteration 143/1000 | Loss: 0.00005905
Iteration 144/1000 | Loss: 0.00005904
Iteration 145/1000 | Loss: 0.00027662
Iteration 146/1000 | Loss: 0.00026380
Iteration 147/1000 | Loss: 0.00006492
Iteration 148/1000 | Loss: 0.00006126
Iteration 149/1000 | Loss: 0.00006042
Iteration 150/1000 | Loss: 0.00005993
Iteration 151/1000 | Loss: 0.00006682
Iteration 152/1000 | Loss: 0.00006101
Iteration 153/1000 | Loss: 0.00005954
Iteration 154/1000 | Loss: 0.00005907
Iteration 155/1000 | Loss: 0.00005882
Iteration 156/1000 | Loss: 0.00005869
Iteration 157/1000 | Loss: 0.00005865
Iteration 158/1000 | Loss: 0.00005865
Iteration 159/1000 | Loss: 0.00005864
Iteration 160/1000 | Loss: 0.00005861
Iteration 161/1000 | Loss: 0.00005860
Iteration 162/1000 | Loss: 0.00005860
Iteration 163/1000 | Loss: 0.00005859
Iteration 164/1000 | Loss: 0.00005853
Iteration 165/1000 | Loss: 0.00005852
Iteration 166/1000 | Loss: 0.00005849
Iteration 167/1000 | Loss: 0.00005849
Iteration 168/1000 | Loss: 0.00005848
Iteration 169/1000 | Loss: 0.00005848
Iteration 170/1000 | Loss: 0.00005847
Iteration 171/1000 | Loss: 0.00005847
Iteration 172/1000 | Loss: 0.00005846
Iteration 173/1000 | Loss: 0.00005846
Iteration 174/1000 | Loss: 0.00005845
Iteration 175/1000 | Loss: 0.00005844
Iteration 176/1000 | Loss: 0.00005843
Iteration 177/1000 | Loss: 0.00005843
Iteration 178/1000 | Loss: 0.00005842
Iteration 179/1000 | Loss: 0.00005841
Iteration 180/1000 | Loss: 0.00005841
Iteration 181/1000 | Loss: 0.00005840
Iteration 182/1000 | Loss: 0.00005840
Iteration 183/1000 | Loss: 0.00005840
Iteration 184/1000 | Loss: 0.00005839
Iteration 185/1000 | Loss: 0.00005839
Iteration 186/1000 | Loss: 0.00005839
Iteration 187/1000 | Loss: 0.00005838
Iteration 188/1000 | Loss: 0.00005838
Iteration 189/1000 | Loss: 0.00005838
Iteration 190/1000 | Loss: 0.00005838
Iteration 191/1000 | Loss: 0.00005838
Iteration 192/1000 | Loss: 0.00005838
Iteration 193/1000 | Loss: 0.00005838
Iteration 194/1000 | Loss: 0.00005838
Iteration 195/1000 | Loss: 0.00005837
Iteration 196/1000 | Loss: 0.00005837
Iteration 197/1000 | Loss: 0.00005837
Iteration 198/1000 | Loss: 0.00005837
Iteration 199/1000 | Loss: 0.00005837
Iteration 200/1000 | Loss: 0.00005837
Iteration 201/1000 | Loss: 0.00005837
Iteration 202/1000 | Loss: 0.00005837
Iteration 203/1000 | Loss: 0.00005837
Iteration 204/1000 | Loss: 0.00005837
Iteration 205/1000 | Loss: 0.00005837
Iteration 206/1000 | Loss: 0.00005837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [5.8370605984237045e-05, 5.8370605984237045e-05, 5.8370605984237045e-05, 5.8370605984237045e-05, 5.8370605984237045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.8370605984237045e-05

Optimization complete. Final v2v error: 4.870189666748047 mm

Highest mean error: 12.860299110412598 mm for frame 69

Lowest mean error: 3.7699832916259766 mm for frame 9

Saving results

Total time: 262.4155523777008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339644
Iteration 2/25 | Loss: 0.00088738
Iteration 3/25 | Loss: 0.00067688
Iteration 4/25 | Loss: 0.00062727
Iteration 5/25 | Loss: 0.00061366
Iteration 6/25 | Loss: 0.00060913
Iteration 7/25 | Loss: 0.00060820
Iteration 8/25 | Loss: 0.00060818
Iteration 9/25 | Loss: 0.00060818
Iteration 10/25 | Loss: 0.00060818
Iteration 11/25 | Loss: 0.00060818
Iteration 12/25 | Loss: 0.00060818
Iteration 13/25 | Loss: 0.00060818
Iteration 14/25 | Loss: 0.00060818
Iteration 15/25 | Loss: 0.00060818
Iteration 16/25 | Loss: 0.00060818
Iteration 17/25 | Loss: 0.00060818
Iteration 18/25 | Loss: 0.00060818
Iteration 19/25 | Loss: 0.00060818
Iteration 20/25 | Loss: 0.00060818
Iteration 21/25 | Loss: 0.00060818
Iteration 22/25 | Loss: 0.00060818
Iteration 23/25 | Loss: 0.00060818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006081804167479277, 0.0006081804167479277, 0.0006081804167479277, 0.0006081804167479277, 0.0006081804167479277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006081804167479277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52538848
Iteration 2/25 | Loss: 0.00063114
Iteration 3/25 | Loss: 0.00063114
Iteration 4/25 | Loss: 0.00063114
Iteration 5/25 | Loss: 0.00063114
Iteration 6/25 | Loss: 0.00063114
Iteration 7/25 | Loss: 0.00063114
Iteration 8/25 | Loss: 0.00063114
Iteration 9/25 | Loss: 0.00063114
Iteration 10/25 | Loss: 0.00063114
Iteration 11/25 | Loss: 0.00063114
Iteration 12/25 | Loss: 0.00063114
Iteration 13/25 | Loss: 0.00063114
Iteration 14/25 | Loss: 0.00063114
Iteration 15/25 | Loss: 0.00063114
Iteration 16/25 | Loss: 0.00063114
Iteration 17/25 | Loss: 0.00063114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006311354227364063, 0.0006311354227364063, 0.0006311354227364063, 0.0006311354227364063, 0.0006311354227364063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006311354227364063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063114
Iteration 2/1000 | Loss: 0.00002828
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00001961
Iteration 6/1000 | Loss: 0.00001893
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001794
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00001730
Iteration 14/1000 | Loss: 0.00001730
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001717
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001714
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001713
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001712
Iteration 28/1000 | Loss: 0.00001712
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001711
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001704
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001701
Iteration 50/1000 | Loss: 0.00001699
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001698
Iteration 53/1000 | Loss: 0.00001698
Iteration 54/1000 | Loss: 0.00001697
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001696
Iteration 57/1000 | Loss: 0.00001696
Iteration 58/1000 | Loss: 0.00001696
Iteration 59/1000 | Loss: 0.00001696
Iteration 60/1000 | Loss: 0.00001696
Iteration 61/1000 | Loss: 0.00001696
Iteration 62/1000 | Loss: 0.00001696
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001695
Iteration 68/1000 | Loss: 0.00001695
Iteration 69/1000 | Loss: 0.00001695
Iteration 70/1000 | Loss: 0.00001695
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00001695
Iteration 73/1000 | Loss: 0.00001694
Iteration 74/1000 | Loss: 0.00001694
Iteration 75/1000 | Loss: 0.00001694
Iteration 76/1000 | Loss: 0.00001694
Iteration 77/1000 | Loss: 0.00001694
Iteration 78/1000 | Loss: 0.00001694
Iteration 79/1000 | Loss: 0.00001694
Iteration 80/1000 | Loss: 0.00001694
Iteration 81/1000 | Loss: 0.00001693
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001693
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001692
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001692
Iteration 94/1000 | Loss: 0.00001692
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001691
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001690
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001689
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001688
Iteration 123/1000 | Loss: 0.00001688
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.686964424152393e-05, 1.686964424152393e-05, 1.686964424152393e-05, 1.686964424152393e-05, 1.686964424152393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.686964424152393e-05

Optimization complete. Final v2v error: 3.5130887031555176 mm

Highest mean error: 3.9294352531433105 mm for frame 92

Lowest mean error: 3.0693085193634033 mm for frame 60

Saving results

Total time: 42.0845947265625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387633
Iteration 2/25 | Loss: 0.00071283
Iteration 3/25 | Loss: 0.00059723
Iteration 4/25 | Loss: 0.00057785
Iteration 5/25 | Loss: 0.00057408
Iteration 6/25 | Loss: 0.00057298
Iteration 7/25 | Loss: 0.00057293
Iteration 8/25 | Loss: 0.00057293
Iteration 9/25 | Loss: 0.00057293
Iteration 10/25 | Loss: 0.00057293
Iteration 11/25 | Loss: 0.00057293
Iteration 12/25 | Loss: 0.00057293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005729281110689044, 0.0005729281110689044, 0.0005729281110689044, 0.0005729281110689044, 0.0005729281110689044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005729281110689044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13291454
Iteration 2/25 | Loss: 0.00054211
Iteration 3/25 | Loss: 0.00054210
Iteration 4/25 | Loss: 0.00054210
Iteration 5/25 | Loss: 0.00054210
Iteration 6/25 | Loss: 0.00054210
Iteration 7/25 | Loss: 0.00054210
Iteration 8/25 | Loss: 0.00054210
Iteration 9/25 | Loss: 0.00054210
Iteration 10/25 | Loss: 0.00054210
Iteration 11/25 | Loss: 0.00054210
Iteration 12/25 | Loss: 0.00054210
Iteration 13/25 | Loss: 0.00054210
Iteration 14/25 | Loss: 0.00054210
Iteration 15/25 | Loss: 0.00054210
Iteration 16/25 | Loss: 0.00054210
Iteration 17/25 | Loss: 0.00054210
Iteration 18/25 | Loss: 0.00054210
Iteration 19/25 | Loss: 0.00054210
Iteration 20/25 | Loss: 0.00054210
Iteration 21/25 | Loss: 0.00054210
Iteration 22/25 | Loss: 0.00054210
Iteration 23/25 | Loss: 0.00054210
Iteration 24/25 | Loss: 0.00054210
Iteration 25/25 | Loss: 0.00054210

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054210
Iteration 2/1000 | Loss: 0.00002042
Iteration 3/1000 | Loss: 0.00001688
Iteration 4/1000 | Loss: 0.00001608
Iteration 5/1000 | Loss: 0.00001553
Iteration 6/1000 | Loss: 0.00001505
Iteration 7/1000 | Loss: 0.00001478
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001455
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001450
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001446
Iteration 20/1000 | Loss: 0.00001446
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001446
Iteration 23/1000 | Loss: 0.00001446
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001446
Iteration 26/1000 | Loss: 0.00001446
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001441
Iteration 29/1000 | Loss: 0.00001441
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001441
Iteration 34/1000 | Loss: 0.00001441
Iteration 35/1000 | Loss: 0.00001441
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00001440
Iteration 38/1000 | Loss: 0.00001440
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001440
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001438
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001438
Iteration 55/1000 | Loss: 0.00001438
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001437
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001432
Iteration 67/1000 | Loss: 0.00001432
Iteration 68/1000 | Loss: 0.00001432
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001431
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001430
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001428
Iteration 79/1000 | Loss: 0.00001428
Iteration 80/1000 | Loss: 0.00001428
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001426
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001426
Iteration 86/1000 | Loss: 0.00001426
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001425
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001423
Iteration 100/1000 | Loss: 0.00001423
Iteration 101/1000 | Loss: 0.00001423
Iteration 102/1000 | Loss: 0.00001423
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001421
Iteration 110/1000 | Loss: 0.00001421
Iteration 111/1000 | Loss: 0.00001421
Iteration 112/1000 | Loss: 0.00001421
Iteration 113/1000 | Loss: 0.00001421
Iteration 114/1000 | Loss: 0.00001421
Iteration 115/1000 | Loss: 0.00001420
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001420
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001419
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001418
Iteration 131/1000 | Loss: 0.00001418
Iteration 132/1000 | Loss: 0.00001418
Iteration 133/1000 | Loss: 0.00001418
Iteration 134/1000 | Loss: 0.00001418
Iteration 135/1000 | Loss: 0.00001418
Iteration 136/1000 | Loss: 0.00001418
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001418
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001418
Iteration 141/1000 | Loss: 0.00001418
Iteration 142/1000 | Loss: 0.00001417
Iteration 143/1000 | Loss: 0.00001417
Iteration 144/1000 | Loss: 0.00001417
Iteration 145/1000 | Loss: 0.00001417
Iteration 146/1000 | Loss: 0.00001416
Iteration 147/1000 | Loss: 0.00001416
Iteration 148/1000 | Loss: 0.00001416
Iteration 149/1000 | Loss: 0.00001416
Iteration 150/1000 | Loss: 0.00001416
Iteration 151/1000 | Loss: 0.00001416
Iteration 152/1000 | Loss: 0.00001416
Iteration 153/1000 | Loss: 0.00001416
Iteration 154/1000 | Loss: 0.00001416
Iteration 155/1000 | Loss: 0.00001416
Iteration 156/1000 | Loss: 0.00001416
Iteration 157/1000 | Loss: 0.00001416
Iteration 158/1000 | Loss: 0.00001416
Iteration 159/1000 | Loss: 0.00001416
Iteration 160/1000 | Loss: 0.00001416
Iteration 161/1000 | Loss: 0.00001416
Iteration 162/1000 | Loss: 0.00001416
Iteration 163/1000 | Loss: 0.00001416
Iteration 164/1000 | Loss: 0.00001416
Iteration 165/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.415782480762573e-05, 1.415782480762573e-05, 1.415782480762573e-05, 1.415782480762573e-05, 1.415782480762573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.415782480762573e-05

Optimization complete. Final v2v error: 3.1789355278015137 mm

Highest mean error: 3.462489128112793 mm for frame 80

Lowest mean error: 2.9537858963012695 mm for frame 201

Saving results

Total time: 34.60057353973389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869575
Iteration 2/25 | Loss: 0.00111917
Iteration 3/25 | Loss: 0.00079416
Iteration 4/25 | Loss: 0.00075101
Iteration 5/25 | Loss: 0.00074492
Iteration 6/25 | Loss: 0.00074303
Iteration 7/25 | Loss: 0.00074282
Iteration 8/25 | Loss: 0.00074282
Iteration 9/25 | Loss: 0.00074282
Iteration 10/25 | Loss: 0.00074282
Iteration 11/25 | Loss: 0.00074282
Iteration 12/25 | Loss: 0.00074282
Iteration 13/25 | Loss: 0.00074282
Iteration 14/25 | Loss: 0.00074282
Iteration 15/25 | Loss: 0.00074282
Iteration 16/25 | Loss: 0.00074282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007428242824971676, 0.0007428242824971676, 0.0007428242824971676, 0.0007428242824971676, 0.0007428242824971676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007428242824971676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07687438
Iteration 2/25 | Loss: 0.00055773
Iteration 3/25 | Loss: 0.00055773
Iteration 4/25 | Loss: 0.00055773
Iteration 5/25 | Loss: 0.00055773
Iteration 6/25 | Loss: 0.00055773
Iteration 7/25 | Loss: 0.00055773
Iteration 8/25 | Loss: 0.00055773
Iteration 9/25 | Loss: 0.00055773
Iteration 10/25 | Loss: 0.00055773
Iteration 11/25 | Loss: 0.00055773
Iteration 12/25 | Loss: 0.00055773
Iteration 13/25 | Loss: 0.00055773
Iteration 14/25 | Loss: 0.00055773
Iteration 15/25 | Loss: 0.00055773
Iteration 16/25 | Loss: 0.00055773
Iteration 17/25 | Loss: 0.00055773
Iteration 18/25 | Loss: 0.00055773
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005577257834374905, 0.0005577257834374905, 0.0005577257834374905, 0.0005577257834374905, 0.0005577257834374905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005577257834374905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055773
Iteration 2/1000 | Loss: 0.00003527
Iteration 3/1000 | Loss: 0.00002977
Iteration 4/1000 | Loss: 0.00002814
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00002569
Iteration 7/1000 | Loss: 0.00002521
Iteration 8/1000 | Loss: 0.00002492
Iteration 9/1000 | Loss: 0.00002487
Iteration 10/1000 | Loss: 0.00002467
Iteration 11/1000 | Loss: 0.00002461
Iteration 12/1000 | Loss: 0.00002460
Iteration 13/1000 | Loss: 0.00002453
Iteration 14/1000 | Loss: 0.00002450
Iteration 15/1000 | Loss: 0.00002450
Iteration 16/1000 | Loss: 0.00002450
Iteration 17/1000 | Loss: 0.00002449
Iteration 18/1000 | Loss: 0.00002449
Iteration 19/1000 | Loss: 0.00002448
Iteration 20/1000 | Loss: 0.00002448
Iteration 21/1000 | Loss: 0.00002447
Iteration 22/1000 | Loss: 0.00002447
Iteration 23/1000 | Loss: 0.00002444
Iteration 24/1000 | Loss: 0.00002444
Iteration 25/1000 | Loss: 0.00002444
Iteration 26/1000 | Loss: 0.00002444
Iteration 27/1000 | Loss: 0.00002444
Iteration 28/1000 | Loss: 0.00002441
Iteration 29/1000 | Loss: 0.00002441
Iteration 30/1000 | Loss: 0.00002440
Iteration 31/1000 | Loss: 0.00002439
Iteration 32/1000 | Loss: 0.00002438
Iteration 33/1000 | Loss: 0.00002437
Iteration 34/1000 | Loss: 0.00002437
Iteration 35/1000 | Loss: 0.00002436
Iteration 36/1000 | Loss: 0.00002436
Iteration 37/1000 | Loss: 0.00002436
Iteration 38/1000 | Loss: 0.00002436
Iteration 39/1000 | Loss: 0.00002436
Iteration 40/1000 | Loss: 0.00002436
Iteration 41/1000 | Loss: 0.00002436
Iteration 42/1000 | Loss: 0.00002436
Iteration 43/1000 | Loss: 0.00002436
Iteration 44/1000 | Loss: 0.00002436
Iteration 45/1000 | Loss: 0.00002435
Iteration 46/1000 | Loss: 0.00002434
Iteration 47/1000 | Loss: 0.00002434
Iteration 48/1000 | Loss: 0.00002434
Iteration 49/1000 | Loss: 0.00002433
Iteration 50/1000 | Loss: 0.00002433
Iteration 51/1000 | Loss: 0.00002433
Iteration 52/1000 | Loss: 0.00002433
Iteration 53/1000 | Loss: 0.00002433
Iteration 54/1000 | Loss: 0.00002432
Iteration 55/1000 | Loss: 0.00002432
Iteration 56/1000 | Loss: 0.00002432
Iteration 57/1000 | Loss: 0.00002432
Iteration 58/1000 | Loss: 0.00002432
Iteration 59/1000 | Loss: 0.00002432
Iteration 60/1000 | Loss: 0.00002432
Iteration 61/1000 | Loss: 0.00002432
Iteration 62/1000 | Loss: 0.00002431
Iteration 63/1000 | Loss: 0.00002431
Iteration 64/1000 | Loss: 0.00002431
Iteration 65/1000 | Loss: 0.00002430
Iteration 66/1000 | Loss: 0.00002430
Iteration 67/1000 | Loss: 0.00002430
Iteration 68/1000 | Loss: 0.00002430
Iteration 69/1000 | Loss: 0.00002430
Iteration 70/1000 | Loss: 0.00002430
Iteration 71/1000 | Loss: 0.00002430
Iteration 72/1000 | Loss: 0.00002430
Iteration 73/1000 | Loss: 0.00002429
Iteration 74/1000 | Loss: 0.00002429
Iteration 75/1000 | Loss: 0.00002429
Iteration 76/1000 | Loss: 0.00002429
Iteration 77/1000 | Loss: 0.00002429
Iteration 78/1000 | Loss: 0.00002428
Iteration 79/1000 | Loss: 0.00002428
Iteration 80/1000 | Loss: 0.00002428
Iteration 81/1000 | Loss: 0.00002427
Iteration 82/1000 | Loss: 0.00002427
Iteration 83/1000 | Loss: 0.00002427
Iteration 84/1000 | Loss: 0.00002427
Iteration 85/1000 | Loss: 0.00002427
Iteration 86/1000 | Loss: 0.00002427
Iteration 87/1000 | Loss: 0.00002427
Iteration 88/1000 | Loss: 0.00002427
Iteration 89/1000 | Loss: 0.00002427
Iteration 90/1000 | Loss: 0.00002427
Iteration 91/1000 | Loss: 0.00002427
Iteration 92/1000 | Loss: 0.00002427
Iteration 93/1000 | Loss: 0.00002427
Iteration 94/1000 | Loss: 0.00002427
Iteration 95/1000 | Loss: 0.00002427
Iteration 96/1000 | Loss: 0.00002427
Iteration 97/1000 | Loss: 0.00002427
Iteration 98/1000 | Loss: 0.00002427
Iteration 99/1000 | Loss: 0.00002427
Iteration 100/1000 | Loss: 0.00002427
Iteration 101/1000 | Loss: 0.00002427
Iteration 102/1000 | Loss: 0.00002427
Iteration 103/1000 | Loss: 0.00002427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.426769060548395e-05, 2.426769060548395e-05, 2.426769060548395e-05, 2.426769060548395e-05, 2.426769060548395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.426769060548395e-05

Optimization complete. Final v2v error: 4.17823600769043 mm

Highest mean error: 4.468216896057129 mm for frame 119

Lowest mean error: 3.7415213584899902 mm for frame 49

Saving results

Total time: 30.555128574371338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982872
Iteration 2/25 | Loss: 0.00139059
Iteration 3/25 | Loss: 0.00087168
Iteration 4/25 | Loss: 0.00080945
Iteration 5/25 | Loss: 0.00079649
Iteration 6/25 | Loss: 0.00079398
Iteration 7/25 | Loss: 0.00079333
Iteration 8/25 | Loss: 0.00079333
Iteration 9/25 | Loss: 0.00079333
Iteration 10/25 | Loss: 0.00079333
Iteration 11/25 | Loss: 0.00079333
Iteration 12/25 | Loss: 0.00079333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007933289161883295, 0.0007933289161883295, 0.0007933289161883295, 0.0007933289161883295, 0.0007933289161883295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007933289161883295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95795202
Iteration 2/25 | Loss: 0.00044209
Iteration 3/25 | Loss: 0.00044207
Iteration 4/25 | Loss: 0.00044207
Iteration 5/25 | Loss: 0.00044207
Iteration 6/25 | Loss: 0.00044207
Iteration 7/25 | Loss: 0.00044207
Iteration 8/25 | Loss: 0.00044207
Iteration 9/25 | Loss: 0.00044207
Iteration 10/25 | Loss: 0.00044207
Iteration 11/25 | Loss: 0.00044207
Iteration 12/25 | Loss: 0.00044207
Iteration 13/25 | Loss: 0.00044207
Iteration 14/25 | Loss: 0.00044207
Iteration 15/25 | Loss: 0.00044207
Iteration 16/25 | Loss: 0.00044207
Iteration 17/25 | Loss: 0.00044207
Iteration 18/25 | Loss: 0.00044207
Iteration 19/25 | Loss: 0.00044207
Iteration 20/25 | Loss: 0.00044207
Iteration 21/25 | Loss: 0.00044207
Iteration 22/25 | Loss: 0.00044207
Iteration 23/25 | Loss: 0.00044207
Iteration 24/25 | Loss: 0.00044207
Iteration 25/25 | Loss: 0.00044207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044207
Iteration 2/1000 | Loss: 0.00005744
Iteration 3/1000 | Loss: 0.00004406
Iteration 4/1000 | Loss: 0.00003806
Iteration 5/1000 | Loss: 0.00003589
Iteration 6/1000 | Loss: 0.00003444
Iteration 7/1000 | Loss: 0.00003344
Iteration 8/1000 | Loss: 0.00003286
Iteration 9/1000 | Loss: 0.00003248
Iteration 10/1000 | Loss: 0.00003218
Iteration 11/1000 | Loss: 0.00003194
Iteration 12/1000 | Loss: 0.00003173
Iteration 13/1000 | Loss: 0.00003160
Iteration 14/1000 | Loss: 0.00003158
Iteration 15/1000 | Loss: 0.00003144
Iteration 16/1000 | Loss: 0.00003139
Iteration 17/1000 | Loss: 0.00003136
Iteration 18/1000 | Loss: 0.00003135
Iteration 19/1000 | Loss: 0.00003134
Iteration 20/1000 | Loss: 0.00003133
Iteration 21/1000 | Loss: 0.00003128
Iteration 22/1000 | Loss: 0.00003127
Iteration 23/1000 | Loss: 0.00003123
Iteration 24/1000 | Loss: 0.00003120
Iteration 25/1000 | Loss: 0.00003117
Iteration 26/1000 | Loss: 0.00003116
Iteration 27/1000 | Loss: 0.00003116
Iteration 28/1000 | Loss: 0.00003115
Iteration 29/1000 | Loss: 0.00003115
Iteration 30/1000 | Loss: 0.00003114
Iteration 31/1000 | Loss: 0.00003114
Iteration 32/1000 | Loss: 0.00003113
Iteration 33/1000 | Loss: 0.00003113
Iteration 34/1000 | Loss: 0.00003113
Iteration 35/1000 | Loss: 0.00003113
Iteration 36/1000 | Loss: 0.00003113
Iteration 37/1000 | Loss: 0.00003113
Iteration 38/1000 | Loss: 0.00003113
Iteration 39/1000 | Loss: 0.00003113
Iteration 40/1000 | Loss: 0.00003113
Iteration 41/1000 | Loss: 0.00003113
Iteration 42/1000 | Loss: 0.00003113
Iteration 43/1000 | Loss: 0.00003113
Iteration 44/1000 | Loss: 0.00003112
Iteration 45/1000 | Loss: 0.00003112
Iteration 46/1000 | Loss: 0.00003112
Iteration 47/1000 | Loss: 0.00003112
Iteration 48/1000 | Loss: 0.00003112
Iteration 49/1000 | Loss: 0.00003111
Iteration 50/1000 | Loss: 0.00003111
Iteration 51/1000 | Loss: 0.00003111
Iteration 52/1000 | Loss: 0.00003110
Iteration 53/1000 | Loss: 0.00003110
Iteration 54/1000 | Loss: 0.00003110
Iteration 55/1000 | Loss: 0.00003109
Iteration 56/1000 | Loss: 0.00003109
Iteration 57/1000 | Loss: 0.00003109
Iteration 58/1000 | Loss: 0.00003108
Iteration 59/1000 | Loss: 0.00003108
Iteration 60/1000 | Loss: 0.00003108
Iteration 61/1000 | Loss: 0.00003108
Iteration 62/1000 | Loss: 0.00003108
Iteration 63/1000 | Loss: 0.00003108
Iteration 64/1000 | Loss: 0.00003108
Iteration 65/1000 | Loss: 0.00003108
Iteration 66/1000 | Loss: 0.00003108
Iteration 67/1000 | Loss: 0.00003108
Iteration 68/1000 | Loss: 0.00003108
Iteration 69/1000 | Loss: 0.00003107
Iteration 70/1000 | Loss: 0.00003107
Iteration 71/1000 | Loss: 0.00003106
Iteration 72/1000 | Loss: 0.00003105
Iteration 73/1000 | Loss: 0.00003105
Iteration 74/1000 | Loss: 0.00003105
Iteration 75/1000 | Loss: 0.00003105
Iteration 76/1000 | Loss: 0.00003104
Iteration 77/1000 | Loss: 0.00003104
Iteration 78/1000 | Loss: 0.00003104
Iteration 79/1000 | Loss: 0.00003103
Iteration 80/1000 | Loss: 0.00003103
Iteration 81/1000 | Loss: 0.00003103
Iteration 82/1000 | Loss: 0.00003103
Iteration 83/1000 | Loss: 0.00003103
Iteration 84/1000 | Loss: 0.00003103
Iteration 85/1000 | Loss: 0.00003103
Iteration 86/1000 | Loss: 0.00003103
Iteration 87/1000 | Loss: 0.00003103
Iteration 88/1000 | Loss: 0.00003103
Iteration 89/1000 | Loss: 0.00003103
Iteration 90/1000 | Loss: 0.00003103
Iteration 91/1000 | Loss: 0.00003102
Iteration 92/1000 | Loss: 0.00003102
Iteration 93/1000 | Loss: 0.00003102
Iteration 94/1000 | Loss: 0.00003101
Iteration 95/1000 | Loss: 0.00003101
Iteration 96/1000 | Loss: 0.00003101
Iteration 97/1000 | Loss: 0.00003101
Iteration 98/1000 | Loss: 0.00003101
Iteration 99/1000 | Loss: 0.00003101
Iteration 100/1000 | Loss: 0.00003100
Iteration 101/1000 | Loss: 0.00003100
Iteration 102/1000 | Loss: 0.00003100
Iteration 103/1000 | Loss: 0.00003100
Iteration 104/1000 | Loss: 0.00003100
Iteration 105/1000 | Loss: 0.00003099
Iteration 106/1000 | Loss: 0.00003099
Iteration 107/1000 | Loss: 0.00003099
Iteration 108/1000 | Loss: 0.00003099
Iteration 109/1000 | Loss: 0.00003099
Iteration 110/1000 | Loss: 0.00003099
Iteration 111/1000 | Loss: 0.00003099
Iteration 112/1000 | Loss: 0.00003099
Iteration 113/1000 | Loss: 0.00003099
Iteration 114/1000 | Loss: 0.00003099
Iteration 115/1000 | Loss: 0.00003099
Iteration 116/1000 | Loss: 0.00003098
Iteration 117/1000 | Loss: 0.00003098
Iteration 118/1000 | Loss: 0.00003098
Iteration 119/1000 | Loss: 0.00003098
Iteration 120/1000 | Loss: 0.00003098
Iteration 121/1000 | Loss: 0.00003098
Iteration 122/1000 | Loss: 0.00003098
Iteration 123/1000 | Loss: 0.00003098
Iteration 124/1000 | Loss: 0.00003098
Iteration 125/1000 | Loss: 0.00003098
Iteration 126/1000 | Loss: 0.00003098
Iteration 127/1000 | Loss: 0.00003098
Iteration 128/1000 | Loss: 0.00003098
Iteration 129/1000 | Loss: 0.00003098
Iteration 130/1000 | Loss: 0.00003098
Iteration 131/1000 | Loss: 0.00003098
Iteration 132/1000 | Loss: 0.00003098
Iteration 133/1000 | Loss: 0.00003098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [3.09812712657731e-05, 3.09812712657731e-05, 3.09812712657731e-05, 3.09812712657731e-05, 3.09812712657731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.09812712657731e-05

Optimization complete. Final v2v error: 4.593550205230713 mm

Highest mean error: 5.212075710296631 mm for frame 132

Lowest mean error: 4.012083530426025 mm for frame 37

Saving results

Total time: 40.469616651535034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988978
Iteration 2/25 | Loss: 0.00112372
Iteration 3/25 | Loss: 0.00092084
Iteration 4/25 | Loss: 0.00086489
Iteration 5/25 | Loss: 0.00084699
Iteration 6/25 | Loss: 0.00084238
Iteration 7/25 | Loss: 0.00084020
Iteration 8/25 | Loss: 0.00084017
Iteration 9/25 | Loss: 0.00084017
Iteration 10/25 | Loss: 0.00084017
Iteration 11/25 | Loss: 0.00084017
Iteration 12/25 | Loss: 0.00084017
Iteration 13/25 | Loss: 0.00084017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008401727536693215, 0.0008401727536693215, 0.0008401727536693215, 0.0008401727536693215, 0.0008401727536693215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008401727536693215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.18483065
Iteration 2/25 | Loss: 0.00073451
Iteration 3/25 | Loss: 0.00073451
Iteration 4/25 | Loss: 0.00073451
Iteration 5/25 | Loss: 0.00073451
Iteration 6/25 | Loss: 0.00073451
Iteration 7/25 | Loss: 0.00073451
Iteration 8/25 | Loss: 0.00073451
Iteration 9/25 | Loss: 0.00073451
Iteration 10/25 | Loss: 0.00073451
Iteration 11/25 | Loss: 0.00073451
Iteration 12/25 | Loss: 0.00073451
Iteration 13/25 | Loss: 0.00073451
Iteration 14/25 | Loss: 0.00073451
Iteration 15/25 | Loss: 0.00073451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007345054764300585, 0.0007345054764300585, 0.0007345054764300585, 0.0007345054764300585, 0.0007345054764300585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007345054764300585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073451
Iteration 2/1000 | Loss: 0.00007071
Iteration 3/1000 | Loss: 0.00005135
Iteration 4/1000 | Loss: 0.00004320
Iteration 5/1000 | Loss: 0.00004149
Iteration 6/1000 | Loss: 0.00004033
Iteration 7/1000 | Loss: 0.00003895
Iteration 8/1000 | Loss: 0.00003787
Iteration 9/1000 | Loss: 0.00003716
Iteration 10/1000 | Loss: 0.00003682
Iteration 11/1000 | Loss: 0.00003649
Iteration 12/1000 | Loss: 0.00003617
Iteration 13/1000 | Loss: 0.00003590
Iteration 14/1000 | Loss: 0.00003571
Iteration 15/1000 | Loss: 0.00003568
Iteration 16/1000 | Loss: 0.00003558
Iteration 17/1000 | Loss: 0.00003546
Iteration 18/1000 | Loss: 0.00003545
Iteration 19/1000 | Loss: 0.00003545
Iteration 20/1000 | Loss: 0.00003544
Iteration 21/1000 | Loss: 0.00003543
Iteration 22/1000 | Loss: 0.00003543
Iteration 23/1000 | Loss: 0.00003542
Iteration 24/1000 | Loss: 0.00003542
Iteration 25/1000 | Loss: 0.00003542
Iteration 26/1000 | Loss: 0.00003542
Iteration 27/1000 | Loss: 0.00003541
Iteration 28/1000 | Loss: 0.00003541
Iteration 29/1000 | Loss: 0.00003541
Iteration 30/1000 | Loss: 0.00003539
Iteration 31/1000 | Loss: 0.00003539
Iteration 32/1000 | Loss: 0.00003539
Iteration 33/1000 | Loss: 0.00003539
Iteration 34/1000 | Loss: 0.00003539
Iteration 35/1000 | Loss: 0.00003539
Iteration 36/1000 | Loss: 0.00003539
Iteration 37/1000 | Loss: 0.00003539
Iteration 38/1000 | Loss: 0.00003538
Iteration 39/1000 | Loss: 0.00003538
Iteration 40/1000 | Loss: 0.00003538
Iteration 41/1000 | Loss: 0.00003538
Iteration 42/1000 | Loss: 0.00003538
Iteration 43/1000 | Loss: 0.00003538
Iteration 44/1000 | Loss: 0.00003538
Iteration 45/1000 | Loss: 0.00003538
Iteration 46/1000 | Loss: 0.00003538
Iteration 47/1000 | Loss: 0.00003537
Iteration 48/1000 | Loss: 0.00003537
Iteration 49/1000 | Loss: 0.00003537
Iteration 50/1000 | Loss: 0.00003536
Iteration 51/1000 | Loss: 0.00003536
Iteration 52/1000 | Loss: 0.00003535
Iteration 53/1000 | Loss: 0.00003535
Iteration 54/1000 | Loss: 0.00003534
Iteration 55/1000 | Loss: 0.00003534
Iteration 56/1000 | Loss: 0.00003534
Iteration 57/1000 | Loss: 0.00003534
Iteration 58/1000 | Loss: 0.00003534
Iteration 59/1000 | Loss: 0.00003534
Iteration 60/1000 | Loss: 0.00003534
Iteration 61/1000 | Loss: 0.00003534
Iteration 62/1000 | Loss: 0.00003534
Iteration 63/1000 | Loss: 0.00003534
Iteration 64/1000 | Loss: 0.00003533
Iteration 65/1000 | Loss: 0.00003533
Iteration 66/1000 | Loss: 0.00003533
Iteration 67/1000 | Loss: 0.00003533
Iteration 68/1000 | Loss: 0.00003533
Iteration 69/1000 | Loss: 0.00003533
Iteration 70/1000 | Loss: 0.00003533
Iteration 71/1000 | Loss: 0.00003532
Iteration 72/1000 | Loss: 0.00003532
Iteration 73/1000 | Loss: 0.00003531
Iteration 74/1000 | Loss: 0.00003531
Iteration 75/1000 | Loss: 0.00003531
Iteration 76/1000 | Loss: 0.00003531
Iteration 77/1000 | Loss: 0.00003531
Iteration 78/1000 | Loss: 0.00003530
Iteration 79/1000 | Loss: 0.00003530
Iteration 80/1000 | Loss: 0.00003530
Iteration 81/1000 | Loss: 0.00003530
Iteration 82/1000 | Loss: 0.00003530
Iteration 83/1000 | Loss: 0.00003530
Iteration 84/1000 | Loss: 0.00003530
Iteration 85/1000 | Loss: 0.00003530
Iteration 86/1000 | Loss: 0.00003530
Iteration 87/1000 | Loss: 0.00003530
Iteration 88/1000 | Loss: 0.00003529
Iteration 89/1000 | Loss: 0.00003529
Iteration 90/1000 | Loss: 0.00003529
Iteration 91/1000 | Loss: 0.00003529
Iteration 92/1000 | Loss: 0.00003529
Iteration 93/1000 | Loss: 0.00003529
Iteration 94/1000 | Loss: 0.00003529
Iteration 95/1000 | Loss: 0.00003529
Iteration 96/1000 | Loss: 0.00003528
Iteration 97/1000 | Loss: 0.00003528
Iteration 98/1000 | Loss: 0.00003528
Iteration 99/1000 | Loss: 0.00003528
Iteration 100/1000 | Loss: 0.00003528
Iteration 101/1000 | Loss: 0.00003528
Iteration 102/1000 | Loss: 0.00003528
Iteration 103/1000 | Loss: 0.00003528
Iteration 104/1000 | Loss: 0.00003528
Iteration 105/1000 | Loss: 0.00003528
Iteration 106/1000 | Loss: 0.00003528
Iteration 107/1000 | Loss: 0.00003528
Iteration 108/1000 | Loss: 0.00003528
Iteration 109/1000 | Loss: 0.00003528
Iteration 110/1000 | Loss: 0.00003528
Iteration 111/1000 | Loss: 0.00003528
Iteration 112/1000 | Loss: 0.00003528
Iteration 113/1000 | Loss: 0.00003528
Iteration 114/1000 | Loss: 0.00003528
Iteration 115/1000 | Loss: 0.00003528
Iteration 116/1000 | Loss: 0.00003528
Iteration 117/1000 | Loss: 0.00003528
Iteration 118/1000 | Loss: 0.00003528
Iteration 119/1000 | Loss: 0.00003528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [3.528012166498229e-05, 3.528012166498229e-05, 3.528012166498229e-05, 3.528012166498229e-05, 3.528012166498229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.528012166498229e-05

Optimization complete. Final v2v error: 5.036327362060547 mm

Highest mean error: 5.271437644958496 mm for frame 47

Lowest mean error: 4.6660895347595215 mm for frame 0

Saving results

Total time: 37.52208685874939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840808
Iteration 2/25 | Loss: 0.00082692
Iteration 3/25 | Loss: 0.00067565
Iteration 4/25 | Loss: 0.00064169
Iteration 5/25 | Loss: 0.00063168
Iteration 6/25 | Loss: 0.00063006
Iteration 7/25 | Loss: 0.00062963
Iteration 8/25 | Loss: 0.00062963
Iteration 9/25 | Loss: 0.00062963
Iteration 10/25 | Loss: 0.00062963
Iteration 11/25 | Loss: 0.00062963
Iteration 12/25 | Loss: 0.00062963
Iteration 13/25 | Loss: 0.00062963
Iteration 14/25 | Loss: 0.00062963
Iteration 15/25 | Loss: 0.00062963
Iteration 16/25 | Loss: 0.00062963
Iteration 17/25 | Loss: 0.00062963
Iteration 18/25 | Loss: 0.00062963
Iteration 19/25 | Loss: 0.00062963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006296281353570521, 0.0006296281353570521, 0.0006296281353570521, 0.0006296281353570521, 0.0006296281353570521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006296281353570521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57163310
Iteration 2/25 | Loss: 0.00058855
Iteration 3/25 | Loss: 0.00058855
Iteration 4/25 | Loss: 0.00058855
Iteration 5/25 | Loss: 0.00058855
Iteration 6/25 | Loss: 0.00058855
Iteration 7/25 | Loss: 0.00058855
Iteration 8/25 | Loss: 0.00058855
Iteration 9/25 | Loss: 0.00058855
Iteration 10/25 | Loss: 0.00058855
Iteration 11/25 | Loss: 0.00058855
Iteration 12/25 | Loss: 0.00058855
Iteration 13/25 | Loss: 0.00058855
Iteration 14/25 | Loss: 0.00058855
Iteration 15/25 | Loss: 0.00058855
Iteration 16/25 | Loss: 0.00058855
Iteration 17/25 | Loss: 0.00058855
Iteration 18/25 | Loss: 0.00058855
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005885502905584872, 0.0005885502905584872, 0.0005885502905584872, 0.0005885502905584872, 0.0005885502905584872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005885502905584872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058855
Iteration 2/1000 | Loss: 0.00002696
Iteration 3/1000 | Loss: 0.00002093
Iteration 4/1000 | Loss: 0.00001958
Iteration 5/1000 | Loss: 0.00001882
Iteration 6/1000 | Loss: 0.00001840
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001749
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001742
Iteration 12/1000 | Loss: 0.00001742
Iteration 13/1000 | Loss: 0.00001740
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001737
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001736
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001735
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001729
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001728
Iteration 33/1000 | Loss: 0.00001728
Iteration 34/1000 | Loss: 0.00001728
Iteration 35/1000 | Loss: 0.00001728
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001727
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001726
Iteration 47/1000 | Loss: 0.00001725
Iteration 48/1000 | Loss: 0.00001725
Iteration 49/1000 | Loss: 0.00001725
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001719
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001718
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001718
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001713
Iteration 88/1000 | Loss: 0.00001713
Iteration 89/1000 | Loss: 0.00001713
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001712
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001712
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00001711
Iteration 105/1000 | Loss: 0.00001711
Iteration 106/1000 | Loss: 0.00001711
Iteration 107/1000 | Loss: 0.00001710
Iteration 108/1000 | Loss: 0.00001710
Iteration 109/1000 | Loss: 0.00001710
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001710
Iteration 112/1000 | Loss: 0.00001710
Iteration 113/1000 | Loss: 0.00001710
Iteration 114/1000 | Loss: 0.00001710
Iteration 115/1000 | Loss: 0.00001709
Iteration 116/1000 | Loss: 0.00001709
Iteration 117/1000 | Loss: 0.00001709
Iteration 118/1000 | Loss: 0.00001709
Iteration 119/1000 | Loss: 0.00001709
Iteration 120/1000 | Loss: 0.00001709
Iteration 121/1000 | Loss: 0.00001709
Iteration 122/1000 | Loss: 0.00001709
Iteration 123/1000 | Loss: 0.00001709
Iteration 124/1000 | Loss: 0.00001709
Iteration 125/1000 | Loss: 0.00001709
Iteration 126/1000 | Loss: 0.00001709
Iteration 127/1000 | Loss: 0.00001709
Iteration 128/1000 | Loss: 0.00001709
Iteration 129/1000 | Loss: 0.00001709
Iteration 130/1000 | Loss: 0.00001709
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001708
Iteration 135/1000 | Loss: 0.00001708
Iteration 136/1000 | Loss: 0.00001708
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001708
Iteration 139/1000 | Loss: 0.00001708
Iteration 140/1000 | Loss: 0.00001708
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001708
Iteration 143/1000 | Loss: 0.00001707
Iteration 144/1000 | Loss: 0.00001707
Iteration 145/1000 | Loss: 0.00001707
Iteration 146/1000 | Loss: 0.00001707
Iteration 147/1000 | Loss: 0.00001707
Iteration 148/1000 | Loss: 0.00001707
Iteration 149/1000 | Loss: 0.00001707
Iteration 150/1000 | Loss: 0.00001707
Iteration 151/1000 | Loss: 0.00001707
Iteration 152/1000 | Loss: 0.00001707
Iteration 153/1000 | Loss: 0.00001707
Iteration 154/1000 | Loss: 0.00001707
Iteration 155/1000 | Loss: 0.00001707
Iteration 156/1000 | Loss: 0.00001707
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001706
Iteration 163/1000 | Loss: 0.00001706
Iteration 164/1000 | Loss: 0.00001706
Iteration 165/1000 | Loss: 0.00001706
Iteration 166/1000 | Loss: 0.00001706
Iteration 167/1000 | Loss: 0.00001706
Iteration 168/1000 | Loss: 0.00001706
Iteration 169/1000 | Loss: 0.00001706
Iteration 170/1000 | Loss: 0.00001706
Iteration 171/1000 | Loss: 0.00001706
Iteration 172/1000 | Loss: 0.00001706
Iteration 173/1000 | Loss: 0.00001706
Iteration 174/1000 | Loss: 0.00001705
Iteration 175/1000 | Loss: 0.00001705
Iteration 176/1000 | Loss: 0.00001705
Iteration 177/1000 | Loss: 0.00001705
Iteration 178/1000 | Loss: 0.00001705
Iteration 179/1000 | Loss: 0.00001705
Iteration 180/1000 | Loss: 0.00001705
Iteration 181/1000 | Loss: 0.00001705
Iteration 182/1000 | Loss: 0.00001705
Iteration 183/1000 | Loss: 0.00001705
Iteration 184/1000 | Loss: 0.00001705
Iteration 185/1000 | Loss: 0.00001705
Iteration 186/1000 | Loss: 0.00001705
Iteration 187/1000 | Loss: 0.00001705
Iteration 188/1000 | Loss: 0.00001705
Iteration 189/1000 | Loss: 0.00001705
Iteration 190/1000 | Loss: 0.00001705
Iteration 191/1000 | Loss: 0.00001704
Iteration 192/1000 | Loss: 0.00001704
Iteration 193/1000 | Loss: 0.00001704
Iteration 194/1000 | Loss: 0.00001704
Iteration 195/1000 | Loss: 0.00001704
Iteration 196/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.70446619449649e-05, 1.70446619449649e-05, 1.70446619449649e-05, 1.70446619449649e-05, 1.70446619449649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.70446619449649e-05

Optimization complete. Final v2v error: 3.5326359272003174 mm

Highest mean error: 3.974710702896118 mm for frame 91

Lowest mean error: 3.1967830657958984 mm for frame 1

Saving results

Total time: 35.51369380950928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0245/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0245/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825426
Iteration 2/25 | Loss: 0.00078725
Iteration 3/25 | Loss: 0.00059491
Iteration 4/25 | Loss: 0.00056661
Iteration 5/25 | Loss: 0.00056042
Iteration 6/25 | Loss: 0.00055880
Iteration 7/25 | Loss: 0.00055860
Iteration 8/25 | Loss: 0.00055860
Iteration 9/25 | Loss: 0.00055860
Iteration 10/25 | Loss: 0.00055860
Iteration 11/25 | Loss: 0.00055860
Iteration 12/25 | Loss: 0.00055860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000558599189389497, 0.000558599189389497, 0.000558599189389497, 0.000558599189389497, 0.000558599189389497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000558599189389497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52580166
Iteration 2/25 | Loss: 0.00053569
Iteration 3/25 | Loss: 0.00053569
Iteration 4/25 | Loss: 0.00053569
Iteration 5/25 | Loss: 0.00053569
Iteration 6/25 | Loss: 0.00053569
Iteration 7/25 | Loss: 0.00053569
Iteration 8/25 | Loss: 0.00053569
Iteration 9/25 | Loss: 0.00053569
Iteration 10/25 | Loss: 0.00053569
Iteration 11/25 | Loss: 0.00053569
Iteration 12/25 | Loss: 0.00053569
Iteration 13/25 | Loss: 0.00053569
Iteration 14/25 | Loss: 0.00053569
Iteration 15/25 | Loss: 0.00053569
Iteration 16/25 | Loss: 0.00053569
Iteration 17/25 | Loss: 0.00053569
Iteration 18/25 | Loss: 0.00053569
Iteration 19/25 | Loss: 0.00053569
Iteration 20/25 | Loss: 0.00053569
Iteration 21/25 | Loss: 0.00053569
Iteration 22/25 | Loss: 0.00053569
Iteration 23/25 | Loss: 0.00053569
Iteration 24/25 | Loss: 0.00053569
Iteration 25/25 | Loss: 0.00053569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053569
Iteration 2/1000 | Loss: 0.00001634
Iteration 3/1000 | Loss: 0.00001293
Iteration 4/1000 | Loss: 0.00001235
Iteration 5/1000 | Loss: 0.00001195
Iteration 6/1000 | Loss: 0.00001177
Iteration 7/1000 | Loss: 0.00001176
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001161
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001159
Iteration 14/1000 | Loss: 0.00001156
Iteration 15/1000 | Loss: 0.00001155
Iteration 16/1000 | Loss: 0.00001155
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001149
Iteration 23/1000 | Loss: 0.00001148
Iteration 24/1000 | Loss: 0.00001148
Iteration 25/1000 | Loss: 0.00001148
Iteration 26/1000 | Loss: 0.00001148
Iteration 27/1000 | Loss: 0.00001147
Iteration 28/1000 | Loss: 0.00001147
Iteration 29/1000 | Loss: 0.00001146
Iteration 30/1000 | Loss: 0.00001146
Iteration 31/1000 | Loss: 0.00001146
Iteration 32/1000 | Loss: 0.00001145
Iteration 33/1000 | Loss: 0.00001144
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001143
Iteration 43/1000 | Loss: 0.00001142
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001141
Iteration 47/1000 | Loss: 0.00001141
Iteration 48/1000 | Loss: 0.00001141
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001137
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001137
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001136
Iteration 68/1000 | Loss: 0.00001136
Iteration 69/1000 | Loss: 0.00001136
Iteration 70/1000 | Loss: 0.00001136
Iteration 71/1000 | Loss: 0.00001135
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001135
Iteration 74/1000 | Loss: 0.00001135
Iteration 75/1000 | Loss: 0.00001135
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001134
Iteration 78/1000 | Loss: 0.00001134
Iteration 79/1000 | Loss: 0.00001134
Iteration 80/1000 | Loss: 0.00001134
Iteration 81/1000 | Loss: 0.00001134
Iteration 82/1000 | Loss: 0.00001134
Iteration 83/1000 | Loss: 0.00001134
Iteration 84/1000 | Loss: 0.00001134
Iteration 85/1000 | Loss: 0.00001134
Iteration 86/1000 | Loss: 0.00001134
Iteration 87/1000 | Loss: 0.00001134
Iteration 88/1000 | Loss: 0.00001133
Iteration 89/1000 | Loss: 0.00001133
Iteration 90/1000 | Loss: 0.00001133
Iteration 91/1000 | Loss: 0.00001133
Iteration 92/1000 | Loss: 0.00001133
Iteration 93/1000 | Loss: 0.00001133
Iteration 94/1000 | Loss: 0.00001133
Iteration 95/1000 | Loss: 0.00001133
Iteration 96/1000 | Loss: 0.00001133
Iteration 97/1000 | Loss: 0.00001133
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001132
Iteration 101/1000 | Loss: 0.00001132
Iteration 102/1000 | Loss: 0.00001132
Iteration 103/1000 | Loss: 0.00001132
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001132
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001131
Iteration 111/1000 | Loss: 0.00001131
Iteration 112/1000 | Loss: 0.00001131
Iteration 113/1000 | Loss: 0.00001130
Iteration 114/1000 | Loss: 0.00001130
Iteration 115/1000 | Loss: 0.00001130
Iteration 116/1000 | Loss: 0.00001130
Iteration 117/1000 | Loss: 0.00001130
Iteration 118/1000 | Loss: 0.00001130
Iteration 119/1000 | Loss: 0.00001130
Iteration 120/1000 | Loss: 0.00001129
Iteration 121/1000 | Loss: 0.00001129
Iteration 122/1000 | Loss: 0.00001129
Iteration 123/1000 | Loss: 0.00001129
Iteration 124/1000 | Loss: 0.00001129
Iteration 125/1000 | Loss: 0.00001129
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001129
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001127
Iteration 136/1000 | Loss: 0.00001127
Iteration 137/1000 | Loss: 0.00001127
Iteration 138/1000 | Loss: 0.00001126
Iteration 139/1000 | Loss: 0.00001126
Iteration 140/1000 | Loss: 0.00001126
Iteration 141/1000 | Loss: 0.00001126
Iteration 142/1000 | Loss: 0.00001126
Iteration 143/1000 | Loss: 0.00001126
Iteration 144/1000 | Loss: 0.00001126
Iteration 145/1000 | Loss: 0.00001126
Iteration 146/1000 | Loss: 0.00001126
Iteration 147/1000 | Loss: 0.00001126
Iteration 148/1000 | Loss: 0.00001126
Iteration 149/1000 | Loss: 0.00001125
Iteration 150/1000 | Loss: 0.00001125
Iteration 151/1000 | Loss: 0.00001125
Iteration 152/1000 | Loss: 0.00001125
Iteration 153/1000 | Loss: 0.00001125
Iteration 154/1000 | Loss: 0.00001125
Iteration 155/1000 | Loss: 0.00001125
Iteration 156/1000 | Loss: 0.00001125
Iteration 157/1000 | Loss: 0.00001125
Iteration 158/1000 | Loss: 0.00001125
Iteration 159/1000 | Loss: 0.00001124
Iteration 160/1000 | Loss: 0.00001124
Iteration 161/1000 | Loss: 0.00001124
Iteration 162/1000 | Loss: 0.00001124
Iteration 163/1000 | Loss: 0.00001124
Iteration 164/1000 | Loss: 0.00001124
Iteration 165/1000 | Loss: 0.00001124
Iteration 166/1000 | Loss: 0.00001124
Iteration 167/1000 | Loss: 0.00001124
Iteration 168/1000 | Loss: 0.00001124
Iteration 169/1000 | Loss: 0.00001124
Iteration 170/1000 | Loss: 0.00001124
Iteration 171/1000 | Loss: 0.00001124
Iteration 172/1000 | Loss: 0.00001124
Iteration 173/1000 | Loss: 0.00001124
Iteration 174/1000 | Loss: 0.00001124
Iteration 175/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.1243700100749265e-05, 1.1243700100749265e-05, 1.1243700100749265e-05, 1.1243700100749265e-05, 1.1243700100749265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1243700100749265e-05

Optimization complete. Final v2v error: 2.8502252101898193 mm

Highest mean error: 3.092168092727661 mm for frame 57

Lowest mean error: 2.639303684234619 mm for frame 0

Saving results

Total time: 31.180443048477173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916561
Iteration 2/25 | Loss: 0.00098396
Iteration 3/25 | Loss: 0.00070524
Iteration 4/25 | Loss: 0.00067851
Iteration 5/25 | Loss: 0.00067671
Iteration 6/25 | Loss: 0.00066927
Iteration 7/25 | Loss: 0.00065318
Iteration 8/25 | Loss: 0.00066341
Iteration 9/25 | Loss: 0.00065177
Iteration 10/25 | Loss: 0.00065114
Iteration 11/25 | Loss: 0.00065113
Iteration 12/25 | Loss: 0.00065113
Iteration 13/25 | Loss: 0.00065113
Iteration 14/25 | Loss: 0.00065113
Iteration 15/25 | Loss: 0.00065113
Iteration 16/25 | Loss: 0.00065113
Iteration 17/25 | Loss: 0.00065113
Iteration 18/25 | Loss: 0.00065113
Iteration 19/25 | Loss: 0.00065113
Iteration 20/25 | Loss: 0.00065113
Iteration 21/25 | Loss: 0.00065113
Iteration 22/25 | Loss: 0.00065113
Iteration 23/25 | Loss: 0.00065112
Iteration 24/25 | Loss: 0.00065112
Iteration 25/25 | Loss: 0.00065112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41864872
Iteration 2/25 | Loss: 0.00028109
Iteration 3/25 | Loss: 0.00028108
Iteration 4/25 | Loss: 0.00028108
Iteration 5/25 | Loss: 0.00028108
Iteration 6/25 | Loss: 0.00028108
Iteration 7/25 | Loss: 0.00028108
Iteration 8/25 | Loss: 0.00028108
Iteration 9/25 | Loss: 0.00028108
Iteration 10/25 | Loss: 0.00028108
Iteration 11/25 | Loss: 0.00028108
Iteration 12/25 | Loss: 0.00028108
Iteration 13/25 | Loss: 0.00028108
Iteration 14/25 | Loss: 0.00028108
Iteration 15/25 | Loss: 0.00028108
Iteration 16/25 | Loss: 0.00028108
Iteration 17/25 | Loss: 0.00028108
Iteration 18/25 | Loss: 0.00028108
Iteration 19/25 | Loss: 0.00028108
Iteration 20/25 | Loss: 0.00028108
Iteration 21/25 | Loss: 0.00028108
Iteration 22/25 | Loss: 0.00028108
Iteration 23/25 | Loss: 0.00028108
Iteration 24/25 | Loss: 0.00028108
Iteration 25/25 | Loss: 0.00028108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028108
Iteration 2/1000 | Loss: 0.00002407
Iteration 3/1000 | Loss: 0.00001827
Iteration 4/1000 | Loss: 0.00001696
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00001572
Iteration 7/1000 | Loss: 0.00001566
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001502
Iteration 10/1000 | Loss: 0.00001484
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001457
Iteration 13/1000 | Loss: 0.00001456
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001448
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001444
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00001439
Iteration 24/1000 | Loss: 0.00001439
Iteration 25/1000 | Loss: 0.00001437
Iteration 26/1000 | Loss: 0.00001437
Iteration 27/1000 | Loss: 0.00001436
Iteration 28/1000 | Loss: 0.00001435
Iteration 29/1000 | Loss: 0.00001435
Iteration 30/1000 | Loss: 0.00001435
Iteration 31/1000 | Loss: 0.00001434
Iteration 32/1000 | Loss: 0.00001434
Iteration 33/1000 | Loss: 0.00001434
Iteration 34/1000 | Loss: 0.00001434
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001431
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001428
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001427
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001423
Iteration 74/1000 | Loss: 0.00001423
Iteration 75/1000 | Loss: 0.00001423
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001420
Iteration 84/1000 | Loss: 0.00001420
Iteration 85/1000 | Loss: 0.00001420
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001419
Iteration 88/1000 | Loss: 0.00001419
Iteration 89/1000 | Loss: 0.00001419
Iteration 90/1000 | Loss: 0.00001418
Iteration 91/1000 | Loss: 0.00001418
Iteration 92/1000 | Loss: 0.00001418
Iteration 93/1000 | Loss: 0.00001418
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00001417
Iteration 96/1000 | Loss: 0.00001417
Iteration 97/1000 | Loss: 0.00001417
Iteration 98/1000 | Loss: 0.00001417
Iteration 99/1000 | Loss: 0.00001416
Iteration 100/1000 | Loss: 0.00001416
Iteration 101/1000 | Loss: 0.00001416
Iteration 102/1000 | Loss: 0.00001416
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001414
Iteration 109/1000 | Loss: 0.00001414
Iteration 110/1000 | Loss: 0.00001414
Iteration 111/1000 | Loss: 0.00001414
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001413
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001412
Iteration 120/1000 | Loss: 0.00001412
Iteration 121/1000 | Loss: 0.00001412
Iteration 122/1000 | Loss: 0.00001412
Iteration 123/1000 | Loss: 0.00001412
Iteration 124/1000 | Loss: 0.00001412
Iteration 125/1000 | Loss: 0.00001411
Iteration 126/1000 | Loss: 0.00001411
Iteration 127/1000 | Loss: 0.00001411
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001411
Iteration 130/1000 | Loss: 0.00001411
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Iteration 138/1000 | Loss: 0.00001411
Iteration 139/1000 | Loss: 0.00001411
Iteration 140/1000 | Loss: 0.00001411
Iteration 141/1000 | Loss: 0.00001411
Iteration 142/1000 | Loss: 0.00001411
Iteration 143/1000 | Loss: 0.00001410
Iteration 144/1000 | Loss: 0.00001410
Iteration 145/1000 | Loss: 0.00001410
Iteration 146/1000 | Loss: 0.00001410
Iteration 147/1000 | Loss: 0.00001410
Iteration 148/1000 | Loss: 0.00001410
Iteration 149/1000 | Loss: 0.00001410
Iteration 150/1000 | Loss: 0.00001410
Iteration 151/1000 | Loss: 0.00001410
Iteration 152/1000 | Loss: 0.00001410
Iteration 153/1000 | Loss: 0.00001410
Iteration 154/1000 | Loss: 0.00001410
Iteration 155/1000 | Loss: 0.00001410
Iteration 156/1000 | Loss: 0.00001410
Iteration 157/1000 | Loss: 0.00001410
Iteration 158/1000 | Loss: 0.00001410
Iteration 159/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.4102139175520279e-05, 1.4102139175520279e-05, 1.4102139175520279e-05, 1.4102139175520279e-05, 1.4102139175520279e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4102139175520279e-05

Optimization complete. Final v2v error: 3.1612799167633057 mm

Highest mean error: 3.9295051097869873 mm for frame 113

Lowest mean error: 2.6765754222869873 mm for frame 8

Saving results

Total time: 50.965009927749634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828109
Iteration 2/25 | Loss: 0.00077478
Iteration 3/25 | Loss: 0.00061158
Iteration 4/25 | Loss: 0.00059112
Iteration 5/25 | Loss: 0.00058509
Iteration 6/25 | Loss: 0.00058416
Iteration 7/25 | Loss: 0.00058416
Iteration 8/25 | Loss: 0.00058416
Iteration 9/25 | Loss: 0.00058416
Iteration 10/25 | Loss: 0.00058416
Iteration 11/25 | Loss: 0.00058416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005841557867825031, 0.0005841557867825031, 0.0005841557867825031, 0.0005841557867825031, 0.0005841557867825031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005841557867825031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46796846
Iteration 2/25 | Loss: 0.00026748
Iteration 3/25 | Loss: 0.00026748
Iteration 4/25 | Loss: 0.00026748
Iteration 5/25 | Loss: 0.00026748
Iteration 6/25 | Loss: 0.00026747
Iteration 7/25 | Loss: 0.00026747
Iteration 8/25 | Loss: 0.00026747
Iteration 9/25 | Loss: 0.00026747
Iteration 10/25 | Loss: 0.00026747
Iteration 11/25 | Loss: 0.00026747
Iteration 12/25 | Loss: 0.00026747
Iteration 13/25 | Loss: 0.00026747
Iteration 14/25 | Loss: 0.00026747
Iteration 15/25 | Loss: 0.00026747
Iteration 16/25 | Loss: 0.00026747
Iteration 17/25 | Loss: 0.00026747
Iteration 18/25 | Loss: 0.00026747
Iteration 19/25 | Loss: 0.00026747
Iteration 20/25 | Loss: 0.00026747
Iteration 21/25 | Loss: 0.00026747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002674736606422812, 0.0002674736606422812, 0.0002674736606422812, 0.0002674736606422812, 0.0002674736606422812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002674736606422812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026747
Iteration 2/1000 | Loss: 0.00002153
Iteration 3/1000 | Loss: 0.00001481
Iteration 4/1000 | Loss: 0.00001175
Iteration 5/1000 | Loss: 0.00001116
Iteration 6/1000 | Loss: 0.00001057
Iteration 7/1000 | Loss: 0.00001030
Iteration 8/1000 | Loss: 0.00001007
Iteration 9/1000 | Loss: 0.00001000
Iteration 10/1000 | Loss: 0.00000997
Iteration 11/1000 | Loss: 0.00000997
Iteration 12/1000 | Loss: 0.00000997
Iteration 13/1000 | Loss: 0.00000996
Iteration 14/1000 | Loss: 0.00000995
Iteration 15/1000 | Loss: 0.00000995
Iteration 16/1000 | Loss: 0.00000994
Iteration 17/1000 | Loss: 0.00000991
Iteration 18/1000 | Loss: 0.00000990
Iteration 19/1000 | Loss: 0.00000990
Iteration 20/1000 | Loss: 0.00000989
Iteration 21/1000 | Loss: 0.00000989
Iteration 22/1000 | Loss: 0.00000988
Iteration 23/1000 | Loss: 0.00000988
Iteration 24/1000 | Loss: 0.00000988
Iteration 25/1000 | Loss: 0.00000987
Iteration 26/1000 | Loss: 0.00000987
Iteration 27/1000 | Loss: 0.00000986
Iteration 28/1000 | Loss: 0.00000986
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000985
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000984
Iteration 34/1000 | Loss: 0.00000984
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000981
Iteration 37/1000 | Loss: 0.00000980
Iteration 38/1000 | Loss: 0.00000979
Iteration 39/1000 | Loss: 0.00000979
Iteration 40/1000 | Loss: 0.00000978
Iteration 41/1000 | Loss: 0.00000978
Iteration 42/1000 | Loss: 0.00000977
Iteration 43/1000 | Loss: 0.00000977
Iteration 44/1000 | Loss: 0.00000977
Iteration 45/1000 | Loss: 0.00000977
Iteration 46/1000 | Loss: 0.00000977
Iteration 47/1000 | Loss: 0.00000976
Iteration 48/1000 | Loss: 0.00000976
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000974
Iteration 55/1000 | Loss: 0.00000974
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000974
Iteration 58/1000 | Loss: 0.00000974
Iteration 59/1000 | Loss: 0.00000973
Iteration 60/1000 | Loss: 0.00000973
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000973
Iteration 63/1000 | Loss: 0.00000972
Iteration 64/1000 | Loss: 0.00000972
Iteration 65/1000 | Loss: 0.00000971
Iteration 66/1000 | Loss: 0.00000970
Iteration 67/1000 | Loss: 0.00000970
Iteration 68/1000 | Loss: 0.00000970
Iteration 69/1000 | Loss: 0.00000970
Iteration 70/1000 | Loss: 0.00000970
Iteration 71/1000 | Loss: 0.00000970
Iteration 72/1000 | Loss: 0.00000970
Iteration 73/1000 | Loss: 0.00000970
Iteration 74/1000 | Loss: 0.00000970
Iteration 75/1000 | Loss: 0.00000970
Iteration 76/1000 | Loss: 0.00000970
Iteration 77/1000 | Loss: 0.00000969
Iteration 78/1000 | Loss: 0.00000969
Iteration 79/1000 | Loss: 0.00000969
Iteration 80/1000 | Loss: 0.00000969
Iteration 81/1000 | Loss: 0.00000968
Iteration 82/1000 | Loss: 0.00000968
Iteration 83/1000 | Loss: 0.00000968
Iteration 84/1000 | Loss: 0.00000967
Iteration 85/1000 | Loss: 0.00000967
Iteration 86/1000 | Loss: 0.00000967
Iteration 87/1000 | Loss: 0.00000967
Iteration 88/1000 | Loss: 0.00000967
Iteration 89/1000 | Loss: 0.00000967
Iteration 90/1000 | Loss: 0.00000967
Iteration 91/1000 | Loss: 0.00000966
Iteration 92/1000 | Loss: 0.00000966
Iteration 93/1000 | Loss: 0.00000965
Iteration 94/1000 | Loss: 0.00000965
Iteration 95/1000 | Loss: 0.00000964
Iteration 96/1000 | Loss: 0.00000964
Iteration 97/1000 | Loss: 0.00000964
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000963
Iteration 101/1000 | Loss: 0.00000963
Iteration 102/1000 | Loss: 0.00000962
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000961
Iteration 108/1000 | Loss: 0.00000961
Iteration 109/1000 | Loss: 0.00000961
Iteration 110/1000 | Loss: 0.00000961
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000961
Iteration 118/1000 | Loss: 0.00000961
Iteration 119/1000 | Loss: 0.00000961
Iteration 120/1000 | Loss: 0.00000961
Iteration 121/1000 | Loss: 0.00000961
Iteration 122/1000 | Loss: 0.00000961
Iteration 123/1000 | Loss: 0.00000960
Iteration 124/1000 | Loss: 0.00000960
Iteration 125/1000 | Loss: 0.00000960
Iteration 126/1000 | Loss: 0.00000960
Iteration 127/1000 | Loss: 0.00000960
Iteration 128/1000 | Loss: 0.00000960
Iteration 129/1000 | Loss: 0.00000960
Iteration 130/1000 | Loss: 0.00000960
Iteration 131/1000 | Loss: 0.00000960
Iteration 132/1000 | Loss: 0.00000960
Iteration 133/1000 | Loss: 0.00000960
Iteration 134/1000 | Loss: 0.00000960
Iteration 135/1000 | Loss: 0.00000960
Iteration 136/1000 | Loss: 0.00000960
Iteration 137/1000 | Loss: 0.00000960
Iteration 138/1000 | Loss: 0.00000960
Iteration 139/1000 | Loss: 0.00000960
Iteration 140/1000 | Loss: 0.00000960
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000959
Iteration 147/1000 | Loss: 0.00000959
Iteration 148/1000 | Loss: 0.00000959
Iteration 149/1000 | Loss: 0.00000959
Iteration 150/1000 | Loss: 0.00000959
Iteration 151/1000 | Loss: 0.00000959
Iteration 152/1000 | Loss: 0.00000958
Iteration 153/1000 | Loss: 0.00000958
Iteration 154/1000 | Loss: 0.00000958
Iteration 155/1000 | Loss: 0.00000958
Iteration 156/1000 | Loss: 0.00000958
Iteration 157/1000 | Loss: 0.00000958
Iteration 158/1000 | Loss: 0.00000958
Iteration 159/1000 | Loss: 0.00000958
Iteration 160/1000 | Loss: 0.00000958
Iteration 161/1000 | Loss: 0.00000957
Iteration 162/1000 | Loss: 0.00000957
Iteration 163/1000 | Loss: 0.00000957
Iteration 164/1000 | Loss: 0.00000957
Iteration 165/1000 | Loss: 0.00000957
Iteration 166/1000 | Loss: 0.00000957
Iteration 167/1000 | Loss: 0.00000957
Iteration 168/1000 | Loss: 0.00000957
Iteration 169/1000 | Loss: 0.00000957
Iteration 170/1000 | Loss: 0.00000957
Iteration 171/1000 | Loss: 0.00000957
Iteration 172/1000 | Loss: 0.00000957
Iteration 173/1000 | Loss: 0.00000957
Iteration 174/1000 | Loss: 0.00000956
Iteration 175/1000 | Loss: 0.00000956
Iteration 176/1000 | Loss: 0.00000956
Iteration 177/1000 | Loss: 0.00000956
Iteration 178/1000 | Loss: 0.00000956
Iteration 179/1000 | Loss: 0.00000956
Iteration 180/1000 | Loss: 0.00000956
Iteration 181/1000 | Loss: 0.00000956
Iteration 182/1000 | Loss: 0.00000956
Iteration 183/1000 | Loss: 0.00000956
Iteration 184/1000 | Loss: 0.00000956
Iteration 185/1000 | Loss: 0.00000956
Iteration 186/1000 | Loss: 0.00000956
Iteration 187/1000 | Loss: 0.00000956
Iteration 188/1000 | Loss: 0.00000956
Iteration 189/1000 | Loss: 0.00000956
Iteration 190/1000 | Loss: 0.00000955
Iteration 191/1000 | Loss: 0.00000955
Iteration 192/1000 | Loss: 0.00000955
Iteration 193/1000 | Loss: 0.00000955
Iteration 194/1000 | Loss: 0.00000955
Iteration 195/1000 | Loss: 0.00000955
Iteration 196/1000 | Loss: 0.00000955
Iteration 197/1000 | Loss: 0.00000955
Iteration 198/1000 | Loss: 0.00000954
Iteration 199/1000 | Loss: 0.00000954
Iteration 200/1000 | Loss: 0.00000954
Iteration 201/1000 | Loss: 0.00000954
Iteration 202/1000 | Loss: 0.00000954
Iteration 203/1000 | Loss: 0.00000954
Iteration 204/1000 | Loss: 0.00000954
Iteration 205/1000 | Loss: 0.00000954
Iteration 206/1000 | Loss: 0.00000954
Iteration 207/1000 | Loss: 0.00000953
Iteration 208/1000 | Loss: 0.00000953
Iteration 209/1000 | Loss: 0.00000953
Iteration 210/1000 | Loss: 0.00000953
Iteration 211/1000 | Loss: 0.00000952
Iteration 212/1000 | Loss: 0.00000952
Iteration 213/1000 | Loss: 0.00000952
Iteration 214/1000 | Loss: 0.00000952
Iteration 215/1000 | Loss: 0.00000952
Iteration 216/1000 | Loss: 0.00000952
Iteration 217/1000 | Loss: 0.00000952
Iteration 218/1000 | Loss: 0.00000952
Iteration 219/1000 | Loss: 0.00000952
Iteration 220/1000 | Loss: 0.00000952
Iteration 221/1000 | Loss: 0.00000952
Iteration 222/1000 | Loss: 0.00000952
Iteration 223/1000 | Loss: 0.00000952
Iteration 224/1000 | Loss: 0.00000952
Iteration 225/1000 | Loss: 0.00000952
Iteration 226/1000 | Loss: 0.00000951
Iteration 227/1000 | Loss: 0.00000951
Iteration 228/1000 | Loss: 0.00000951
Iteration 229/1000 | Loss: 0.00000951
Iteration 230/1000 | Loss: 0.00000951
Iteration 231/1000 | Loss: 0.00000951
Iteration 232/1000 | Loss: 0.00000951
Iteration 233/1000 | Loss: 0.00000951
Iteration 234/1000 | Loss: 0.00000951
Iteration 235/1000 | Loss: 0.00000951
Iteration 236/1000 | Loss: 0.00000951
Iteration 237/1000 | Loss: 0.00000951
Iteration 238/1000 | Loss: 0.00000951
Iteration 239/1000 | Loss: 0.00000951
Iteration 240/1000 | Loss: 0.00000951
Iteration 241/1000 | Loss: 0.00000951
Iteration 242/1000 | Loss: 0.00000951
Iteration 243/1000 | Loss: 0.00000951
Iteration 244/1000 | Loss: 0.00000951
Iteration 245/1000 | Loss: 0.00000951
Iteration 246/1000 | Loss: 0.00000951
Iteration 247/1000 | Loss: 0.00000951
Iteration 248/1000 | Loss: 0.00000951
Iteration 249/1000 | Loss: 0.00000951
Iteration 250/1000 | Loss: 0.00000951
Iteration 251/1000 | Loss: 0.00000951
Iteration 252/1000 | Loss: 0.00000951
Iteration 253/1000 | Loss: 0.00000951
Iteration 254/1000 | Loss: 0.00000951
Iteration 255/1000 | Loss: 0.00000951
Iteration 256/1000 | Loss: 0.00000951
Iteration 257/1000 | Loss: 0.00000951
Iteration 258/1000 | Loss: 0.00000951
Iteration 259/1000 | Loss: 0.00000951
Iteration 260/1000 | Loss: 0.00000951
Iteration 261/1000 | Loss: 0.00000951
Iteration 262/1000 | Loss: 0.00000951
Iteration 263/1000 | Loss: 0.00000951
Iteration 264/1000 | Loss: 0.00000951
Iteration 265/1000 | Loss: 0.00000951
Iteration 266/1000 | Loss: 0.00000951
Iteration 267/1000 | Loss: 0.00000951
Iteration 268/1000 | Loss: 0.00000951
Iteration 269/1000 | Loss: 0.00000951
Iteration 270/1000 | Loss: 0.00000951
Iteration 271/1000 | Loss: 0.00000951
Iteration 272/1000 | Loss: 0.00000951
Iteration 273/1000 | Loss: 0.00000951
Iteration 274/1000 | Loss: 0.00000951
Iteration 275/1000 | Loss: 0.00000951
Iteration 276/1000 | Loss: 0.00000951
Iteration 277/1000 | Loss: 0.00000951
Iteration 278/1000 | Loss: 0.00000951
Iteration 279/1000 | Loss: 0.00000951
Iteration 280/1000 | Loss: 0.00000951
Iteration 281/1000 | Loss: 0.00000951
Iteration 282/1000 | Loss: 0.00000951
Iteration 283/1000 | Loss: 0.00000951
Iteration 284/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [9.51135552895721e-06, 9.51135552895721e-06, 9.51135552895721e-06, 9.51135552895721e-06, 9.51135552895721e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.51135552895721e-06

Optimization complete. Final v2v error: 2.6080942153930664 mm

Highest mean error: 2.7391388416290283 mm for frame 66

Lowest mean error: 2.507220506668091 mm for frame 16

Saving results

Total time: 37.435344219207764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820903
Iteration 2/25 | Loss: 0.00313051
Iteration 3/25 | Loss: 0.00175064
Iteration 4/25 | Loss: 0.00145275
Iteration 5/25 | Loss: 0.00137216
Iteration 6/25 | Loss: 0.00137580
Iteration 7/25 | Loss: 0.00130148
Iteration 8/25 | Loss: 0.00115784
Iteration 9/25 | Loss: 0.00112652
Iteration 10/25 | Loss: 0.00103522
Iteration 11/25 | Loss: 0.00101464
Iteration 12/25 | Loss: 0.00098544
Iteration 13/25 | Loss: 0.00102758
Iteration 14/25 | Loss: 0.00095590
Iteration 15/25 | Loss: 0.00087153
Iteration 16/25 | Loss: 0.00089649
Iteration 17/25 | Loss: 0.00089991
Iteration 18/25 | Loss: 0.00083654
Iteration 19/25 | Loss: 0.00084665
Iteration 20/25 | Loss: 0.00083005
Iteration 21/25 | Loss: 0.00082781
Iteration 22/25 | Loss: 0.00081776
Iteration 23/25 | Loss: 0.00081616
Iteration 24/25 | Loss: 0.00081063
Iteration 25/25 | Loss: 0.00081568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89863205
Iteration 2/25 | Loss: 0.00167395
Iteration 3/25 | Loss: 0.00164337
Iteration 4/25 | Loss: 0.00155797
Iteration 5/25 | Loss: 0.00155797
Iteration 6/25 | Loss: 0.00155797
Iteration 7/25 | Loss: 0.00155797
Iteration 8/25 | Loss: 0.00155797
Iteration 9/25 | Loss: 0.00155797
Iteration 10/25 | Loss: 0.00155797
Iteration 11/25 | Loss: 0.00155797
Iteration 12/25 | Loss: 0.00155797
Iteration 13/25 | Loss: 0.00155797
Iteration 14/25 | Loss: 0.00155797
Iteration 15/25 | Loss: 0.00155797
Iteration 16/25 | Loss: 0.00155797
Iteration 17/25 | Loss: 0.00155797
Iteration 18/25 | Loss: 0.00155797
Iteration 19/25 | Loss: 0.00155797
Iteration 20/25 | Loss: 0.00155797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015579661121591926, 0.0015579661121591926, 0.0015579661121591926, 0.0015579661121591926, 0.0015579661121591926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015579661121591926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155797
Iteration 2/1000 | Loss: 0.00030927
Iteration 3/1000 | Loss: 0.00021289
Iteration 4/1000 | Loss: 0.00032893
Iteration 5/1000 | Loss: 0.00053200
Iteration 6/1000 | Loss: 0.00163773
Iteration 7/1000 | Loss: 0.00055636
Iteration 8/1000 | Loss: 0.00016757
Iteration 9/1000 | Loss: 0.00021667
Iteration 10/1000 | Loss: 0.00008339
Iteration 11/1000 | Loss: 0.00007924
Iteration 12/1000 | Loss: 0.00051181
Iteration 13/1000 | Loss: 0.01429737
Iteration 14/1000 | Loss: 0.00692908
Iteration 15/1000 | Loss: 0.00302426
Iteration 16/1000 | Loss: 0.00269275
Iteration 17/1000 | Loss: 0.00230220
Iteration 18/1000 | Loss: 0.00020804
Iteration 19/1000 | Loss: 0.00125664
Iteration 20/1000 | Loss: 0.00009766
Iteration 21/1000 | Loss: 0.00006134
Iteration 22/1000 | Loss: 0.00026449
Iteration 23/1000 | Loss: 0.00042876
Iteration 24/1000 | Loss: 0.00096200
Iteration 25/1000 | Loss: 0.00006965
Iteration 26/1000 | Loss: 0.00004129
Iteration 27/1000 | Loss: 0.00002975
Iteration 28/1000 | Loss: 0.00002723
Iteration 29/1000 | Loss: 0.00066031
Iteration 30/1000 | Loss: 0.00006915
Iteration 31/1000 | Loss: 0.00031465
Iteration 32/1000 | Loss: 0.00058590
Iteration 33/1000 | Loss: 0.00002889
Iteration 34/1000 | Loss: 0.00037557
Iteration 35/1000 | Loss: 0.00063926
Iteration 36/1000 | Loss: 0.00003602
Iteration 37/1000 | Loss: 0.00031287
Iteration 38/1000 | Loss: 0.00276796
Iteration 39/1000 | Loss: 0.00015230
Iteration 40/1000 | Loss: 0.00085716
Iteration 41/1000 | Loss: 0.00002327
Iteration 42/1000 | Loss: 0.00008661
Iteration 43/1000 | Loss: 0.00002191
Iteration 44/1000 | Loss: 0.00002117
Iteration 45/1000 | Loss: 0.00005736
Iteration 46/1000 | Loss: 0.00011904
Iteration 47/1000 | Loss: 0.00002064
Iteration 48/1000 | Loss: 0.00002013
Iteration 49/1000 | Loss: 0.00001984
Iteration 50/1000 | Loss: 0.00056269
Iteration 51/1000 | Loss: 0.00070385
Iteration 52/1000 | Loss: 0.00068841
Iteration 53/1000 | Loss: 0.00003607
Iteration 54/1000 | Loss: 0.00002309
Iteration 55/1000 | Loss: 0.00012903
Iteration 56/1000 | Loss: 0.00001863
Iteration 57/1000 | Loss: 0.00001824
Iteration 58/1000 | Loss: 0.00009295
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001810
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001791
Iteration 64/1000 | Loss: 0.00001791
Iteration 65/1000 | Loss: 0.00001790
Iteration 66/1000 | Loss: 0.00001789
Iteration 67/1000 | Loss: 0.00001788
Iteration 68/1000 | Loss: 0.00001788
Iteration 69/1000 | Loss: 0.00001788
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001786
Iteration 72/1000 | Loss: 0.00001784
Iteration 73/1000 | Loss: 0.00001783
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001782
Iteration 76/1000 | Loss: 0.00001781
Iteration 77/1000 | Loss: 0.00001781
Iteration 78/1000 | Loss: 0.00001780
Iteration 79/1000 | Loss: 0.00001779
Iteration 80/1000 | Loss: 0.00001779
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001777
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001772
Iteration 105/1000 | Loss: 0.00001772
Iteration 106/1000 | Loss: 0.00001772
Iteration 107/1000 | Loss: 0.00001772
Iteration 108/1000 | Loss: 0.00001772
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001771
Iteration 113/1000 | Loss: 0.00001771
Iteration 114/1000 | Loss: 0.00001771
Iteration 115/1000 | Loss: 0.00001771
Iteration 116/1000 | Loss: 0.00001771
Iteration 117/1000 | Loss: 0.00001771
Iteration 118/1000 | Loss: 0.00001771
Iteration 119/1000 | Loss: 0.00001771
Iteration 120/1000 | Loss: 0.00001771
Iteration 121/1000 | Loss: 0.00001771
Iteration 122/1000 | Loss: 0.00001771
Iteration 123/1000 | Loss: 0.00001771
Iteration 124/1000 | Loss: 0.00001771
Iteration 125/1000 | Loss: 0.00001771
Iteration 126/1000 | Loss: 0.00001771
Iteration 127/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.771137431205716e-05, 1.771137431205716e-05, 1.771137431205716e-05, 1.771137431205716e-05, 1.771137431205716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.771137431205716e-05

Optimization complete. Final v2v error: 3.309497356414795 mm

Highest mean error: 11.566391944885254 mm for frame 86

Lowest mean error: 2.934633731842041 mm for frame 125

Saving results

Total time: 131.7585732936859
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395106
Iteration 2/25 | Loss: 0.00081144
Iteration 3/25 | Loss: 0.00067463
Iteration 4/25 | Loss: 0.00065628
Iteration 5/25 | Loss: 0.00064528
Iteration 6/25 | Loss: 0.00064387
Iteration 7/25 | Loss: 0.00064387
Iteration 8/25 | Loss: 0.00064387
Iteration 9/25 | Loss: 0.00064387
Iteration 10/25 | Loss: 0.00064387
Iteration 11/25 | Loss: 0.00064387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006438748678192496, 0.0006438748678192496, 0.0006438748678192496, 0.0006438748678192496, 0.0006438748678192496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006438748678192496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72459650
Iteration 2/25 | Loss: 0.00032247
Iteration 3/25 | Loss: 0.00032247
Iteration 4/25 | Loss: 0.00032247
Iteration 5/25 | Loss: 0.00032247
Iteration 6/25 | Loss: 0.00032247
Iteration 7/25 | Loss: 0.00032247
Iteration 8/25 | Loss: 0.00032247
Iteration 9/25 | Loss: 0.00032247
Iteration 10/25 | Loss: 0.00032246
Iteration 11/25 | Loss: 0.00032246
Iteration 12/25 | Loss: 0.00032246
Iteration 13/25 | Loss: 0.00032246
Iteration 14/25 | Loss: 0.00032246
Iteration 15/25 | Loss: 0.00032246
Iteration 16/25 | Loss: 0.00032246
Iteration 17/25 | Loss: 0.00032246
Iteration 18/25 | Loss: 0.00032246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00032246464979834855, 0.00032246464979834855, 0.00032246464979834855, 0.00032246464979834855, 0.00032246464979834855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032246464979834855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032246
Iteration 2/1000 | Loss: 0.00002324
Iteration 3/1000 | Loss: 0.00001553
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001297
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001266
Iteration 10/1000 | Loss: 0.00001258
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001249
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001245
Iteration 16/1000 | Loss: 0.00001245
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001239
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001238
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00001234
Iteration 41/1000 | Loss: 0.00001234
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001232
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001230
Iteration 57/1000 | Loss: 0.00001230
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001228
Iteration 77/1000 | Loss: 0.00001228
Iteration 78/1000 | Loss: 0.00001228
Iteration 79/1000 | Loss: 0.00001228
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001228
Iteration 85/1000 | Loss: 0.00001228
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001228
Iteration 93/1000 | Loss: 0.00001228
Iteration 94/1000 | Loss: 0.00001228
Iteration 95/1000 | Loss: 0.00001228
Iteration 96/1000 | Loss: 0.00001228
Iteration 97/1000 | Loss: 0.00001228
Iteration 98/1000 | Loss: 0.00001228
Iteration 99/1000 | Loss: 0.00001228
Iteration 100/1000 | Loss: 0.00001228
Iteration 101/1000 | Loss: 0.00001228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.2276978850422893e-05, 1.2276978850422893e-05, 1.2276978850422893e-05, 1.2276978850422893e-05, 1.2276978850422893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2276978850422893e-05

Optimization complete. Final v2v error: 2.999762535095215 mm

Highest mean error: 3.298180103302002 mm for frame 204

Lowest mean error: 2.8071930408477783 mm for frame 164

Saving results

Total time: 32.00358486175537
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478801
Iteration 2/25 | Loss: 0.00100814
Iteration 3/25 | Loss: 0.00076858
Iteration 4/25 | Loss: 0.00072232
Iteration 5/25 | Loss: 0.00070322
Iteration 6/25 | Loss: 0.00069836
Iteration 7/25 | Loss: 0.00069733
Iteration 8/25 | Loss: 0.00069728
Iteration 9/25 | Loss: 0.00069728
Iteration 10/25 | Loss: 0.00069728
Iteration 11/25 | Loss: 0.00069728
Iteration 12/25 | Loss: 0.00069728
Iteration 13/25 | Loss: 0.00069728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000697276322171092, 0.000697276322171092, 0.000697276322171092, 0.000697276322171092, 0.000697276322171092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000697276322171092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46113443
Iteration 2/25 | Loss: 0.00028123
Iteration 3/25 | Loss: 0.00028120
Iteration 4/25 | Loss: 0.00028120
Iteration 5/25 | Loss: 0.00028120
Iteration 6/25 | Loss: 0.00028120
Iteration 7/25 | Loss: 0.00028120
Iteration 8/25 | Loss: 0.00028120
Iteration 9/25 | Loss: 0.00028120
Iteration 10/25 | Loss: 0.00028120
Iteration 11/25 | Loss: 0.00028120
Iteration 12/25 | Loss: 0.00028120
Iteration 13/25 | Loss: 0.00028120
Iteration 14/25 | Loss: 0.00028120
Iteration 15/25 | Loss: 0.00028120
Iteration 16/25 | Loss: 0.00028120
Iteration 17/25 | Loss: 0.00028120
Iteration 18/25 | Loss: 0.00028120
Iteration 19/25 | Loss: 0.00028120
Iteration 20/25 | Loss: 0.00028120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002811993472278118, 0.0002811993472278118, 0.0002811993472278118, 0.0002811993472278118, 0.0002811993472278118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002811993472278118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028120
Iteration 2/1000 | Loss: 0.00003457
Iteration 3/1000 | Loss: 0.00002665
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00002192
Iteration 7/1000 | Loss: 0.00002128
Iteration 8/1000 | Loss: 0.00002075
Iteration 9/1000 | Loss: 0.00002041
Iteration 10/1000 | Loss: 0.00002019
Iteration 11/1000 | Loss: 0.00002017
Iteration 12/1000 | Loss: 0.00002011
Iteration 13/1000 | Loss: 0.00002010
Iteration 14/1000 | Loss: 0.00002010
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002005
Iteration 17/1000 | Loss: 0.00002004
Iteration 18/1000 | Loss: 0.00002004
Iteration 19/1000 | Loss: 0.00002003
Iteration 20/1000 | Loss: 0.00002003
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00002002
Iteration 23/1000 | Loss: 0.00002002
Iteration 24/1000 | Loss: 0.00002002
Iteration 25/1000 | Loss: 0.00002001
Iteration 26/1000 | Loss: 0.00002001
Iteration 27/1000 | Loss: 0.00002000
Iteration 28/1000 | Loss: 0.00002000
Iteration 29/1000 | Loss: 0.00001999
Iteration 30/1000 | Loss: 0.00001999
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001998
Iteration 33/1000 | Loss: 0.00001997
Iteration 34/1000 | Loss: 0.00001995
Iteration 35/1000 | Loss: 0.00001995
Iteration 36/1000 | Loss: 0.00001995
Iteration 37/1000 | Loss: 0.00001993
Iteration 38/1000 | Loss: 0.00001992
Iteration 39/1000 | Loss: 0.00001990
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001990
Iteration 43/1000 | Loss: 0.00001990
Iteration 44/1000 | Loss: 0.00001990
Iteration 45/1000 | Loss: 0.00001990
Iteration 46/1000 | Loss: 0.00001990
Iteration 47/1000 | Loss: 0.00001990
Iteration 48/1000 | Loss: 0.00001989
Iteration 49/1000 | Loss: 0.00001989
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001989
Iteration 52/1000 | Loss: 0.00001989
Iteration 53/1000 | Loss: 0.00001989
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001986
Iteration 70/1000 | Loss: 0.00001986
Iteration 71/1000 | Loss: 0.00001986
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001986
Iteration 76/1000 | Loss: 0.00001986
Iteration 77/1000 | Loss: 0.00001986
Iteration 78/1000 | Loss: 0.00001986
Iteration 79/1000 | Loss: 0.00001986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.9859937310684472e-05, 1.9859937310684472e-05, 1.9859937310684472e-05, 1.9859937310684472e-05, 1.9859937310684472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9859937310684472e-05

Optimization complete. Final v2v error: 3.6302740573883057 mm

Highest mean error: 5.548248767852783 mm for frame 214

Lowest mean error: 2.856682777404785 mm for frame 187

Saving results

Total time: 35.68505072593689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589552
Iteration 2/25 | Loss: 0.00075625
Iteration 3/25 | Loss: 0.00063754
Iteration 4/25 | Loss: 0.00061514
Iteration 5/25 | Loss: 0.00060670
Iteration 6/25 | Loss: 0.00060514
Iteration 7/25 | Loss: 0.00060480
Iteration 8/25 | Loss: 0.00060480
Iteration 9/25 | Loss: 0.00060480
Iteration 10/25 | Loss: 0.00060480
Iteration 11/25 | Loss: 0.00060480
Iteration 12/25 | Loss: 0.00060480
Iteration 13/25 | Loss: 0.00060480
Iteration 14/25 | Loss: 0.00060480
Iteration 15/25 | Loss: 0.00060480
Iteration 16/25 | Loss: 0.00060480
Iteration 17/25 | Loss: 0.00060480
Iteration 18/25 | Loss: 0.00060480
Iteration 19/25 | Loss: 0.00060480
Iteration 20/25 | Loss: 0.00060480
Iteration 21/25 | Loss: 0.00060480
Iteration 22/25 | Loss: 0.00060480
Iteration 23/25 | Loss: 0.00060480
Iteration 24/25 | Loss: 0.00060480
Iteration 25/25 | Loss: 0.00060480

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.10147119
Iteration 2/25 | Loss: 0.00026777
Iteration 3/25 | Loss: 0.00026777
Iteration 4/25 | Loss: 0.00026777
Iteration 5/25 | Loss: 0.00026777
Iteration 6/25 | Loss: 0.00026777
Iteration 7/25 | Loss: 0.00026777
Iteration 8/25 | Loss: 0.00026777
Iteration 9/25 | Loss: 0.00026777
Iteration 10/25 | Loss: 0.00026777
Iteration 11/25 | Loss: 0.00026777
Iteration 12/25 | Loss: 0.00026777
Iteration 13/25 | Loss: 0.00026777
Iteration 14/25 | Loss: 0.00026777
Iteration 15/25 | Loss: 0.00026777
Iteration 16/25 | Loss: 0.00026777
Iteration 17/25 | Loss: 0.00026777
Iteration 18/25 | Loss: 0.00026777
Iteration 19/25 | Loss: 0.00026777
Iteration 20/25 | Loss: 0.00026777
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00026777086895890534, 0.00026777086895890534, 0.00026777086895890534, 0.00026777086895890534, 0.00026777086895890534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026777086895890534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026777
Iteration 2/1000 | Loss: 0.00002439
Iteration 3/1000 | Loss: 0.00001578
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001407
Iteration 6/1000 | Loss: 0.00001357
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001302
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001291
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001273
Iteration 18/1000 | Loss: 0.00001271
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001265
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001262
Iteration 30/1000 | Loss: 0.00001261
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001245
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001245
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001245
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00001244
Iteration 64/1000 | Loss: 0.00001244
Iteration 65/1000 | Loss: 0.00001244
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001243
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001241
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001237
Iteration 87/1000 | Loss: 0.00001237
Iteration 88/1000 | Loss: 0.00001237
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001233
Iteration 104/1000 | Loss: 0.00001233
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001233
Iteration 118/1000 | Loss: 0.00001233
Iteration 119/1000 | Loss: 0.00001233
Iteration 120/1000 | Loss: 0.00001233
Iteration 121/1000 | Loss: 0.00001233
Iteration 122/1000 | Loss: 0.00001233
Iteration 123/1000 | Loss: 0.00001233
Iteration 124/1000 | Loss: 0.00001233
Iteration 125/1000 | Loss: 0.00001233
Iteration 126/1000 | Loss: 0.00001233
Iteration 127/1000 | Loss: 0.00001233
Iteration 128/1000 | Loss: 0.00001233
Iteration 129/1000 | Loss: 0.00001233
Iteration 130/1000 | Loss: 0.00001233
Iteration 131/1000 | Loss: 0.00001233
Iteration 132/1000 | Loss: 0.00001233
Iteration 133/1000 | Loss: 0.00001233
Iteration 134/1000 | Loss: 0.00001233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2328342563705519e-05, 1.2328342563705519e-05, 1.2328342563705519e-05, 1.2328342563705519e-05, 1.2328342563705519e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2328342563705519e-05

Optimization complete. Final v2v error: 2.9984724521636963 mm

Highest mean error: 3.266688823699951 mm for frame 96

Lowest mean error: 2.839099168777466 mm for frame 29

Saving results

Total time: 32.98310422897339
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829263
Iteration 2/25 | Loss: 0.00091767
Iteration 3/25 | Loss: 0.00074765
Iteration 4/25 | Loss: 0.00069747
Iteration 5/25 | Loss: 0.00068968
Iteration 6/25 | Loss: 0.00068745
Iteration 7/25 | Loss: 0.00068647
Iteration 8/25 | Loss: 0.00068647
Iteration 9/25 | Loss: 0.00068645
Iteration 10/25 | Loss: 0.00068645
Iteration 11/25 | Loss: 0.00068645
Iteration 12/25 | Loss: 0.00068645
Iteration 13/25 | Loss: 0.00068645
Iteration 14/25 | Loss: 0.00068645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006864508031867445, 0.0006864508031867445, 0.0006864508031867445, 0.0006864508031867445, 0.0006864508031867445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006864508031867445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64816701
Iteration 2/25 | Loss: 0.00028303
Iteration 3/25 | Loss: 0.00028294
Iteration 4/25 | Loss: 0.00028294
Iteration 5/25 | Loss: 0.00028293
Iteration 6/25 | Loss: 0.00028293
Iteration 7/25 | Loss: 0.00028293
Iteration 8/25 | Loss: 0.00028293
Iteration 9/25 | Loss: 0.00028293
Iteration 10/25 | Loss: 0.00028293
Iteration 11/25 | Loss: 0.00028293
Iteration 12/25 | Loss: 0.00028293
Iteration 13/25 | Loss: 0.00028293
Iteration 14/25 | Loss: 0.00028293
Iteration 15/25 | Loss: 0.00028293
Iteration 16/25 | Loss: 0.00028293
Iteration 17/25 | Loss: 0.00028293
Iteration 18/25 | Loss: 0.00028293
Iteration 19/25 | Loss: 0.00028293
Iteration 20/25 | Loss: 0.00028293
Iteration 21/25 | Loss: 0.00028293
Iteration 22/25 | Loss: 0.00028293
Iteration 23/25 | Loss: 0.00028293
Iteration 24/25 | Loss: 0.00028293
Iteration 25/25 | Loss: 0.00028293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028293
Iteration 2/1000 | Loss: 0.00004867
Iteration 3/1000 | Loss: 0.00003413
Iteration 4/1000 | Loss: 0.00002692
Iteration 5/1000 | Loss: 0.00002427
Iteration 6/1000 | Loss: 0.00002247
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00001995
Iteration 10/1000 | Loss: 0.00001959
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001905
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001883
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001859
Iteration 19/1000 | Loss: 0.00001858
Iteration 20/1000 | Loss: 0.00001857
Iteration 21/1000 | Loss: 0.00001857
Iteration 22/1000 | Loss: 0.00001854
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001847
Iteration 25/1000 | Loss: 0.00001840
Iteration 26/1000 | Loss: 0.00001839
Iteration 27/1000 | Loss: 0.00001839
Iteration 28/1000 | Loss: 0.00001838
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001835
Iteration 32/1000 | Loss: 0.00001834
Iteration 33/1000 | Loss: 0.00001833
Iteration 34/1000 | Loss: 0.00001832
Iteration 35/1000 | Loss: 0.00001832
Iteration 36/1000 | Loss: 0.00001831
Iteration 37/1000 | Loss: 0.00001831
Iteration 38/1000 | Loss: 0.00001830
Iteration 39/1000 | Loss: 0.00001830
Iteration 40/1000 | Loss: 0.00001829
Iteration 41/1000 | Loss: 0.00001828
Iteration 42/1000 | Loss: 0.00001825
Iteration 43/1000 | Loss: 0.00001824
Iteration 44/1000 | Loss: 0.00001824
Iteration 45/1000 | Loss: 0.00001823
Iteration 46/1000 | Loss: 0.00001823
Iteration 47/1000 | Loss: 0.00001823
Iteration 48/1000 | Loss: 0.00001822
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001821
Iteration 52/1000 | Loss: 0.00001821
Iteration 53/1000 | Loss: 0.00001821
Iteration 54/1000 | Loss: 0.00001820
Iteration 55/1000 | Loss: 0.00001820
Iteration 56/1000 | Loss: 0.00001820
Iteration 57/1000 | Loss: 0.00001820
Iteration 58/1000 | Loss: 0.00001820
Iteration 59/1000 | Loss: 0.00001820
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001817
Iteration 70/1000 | Loss: 0.00001817
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001816
Iteration 73/1000 | Loss: 0.00001816
Iteration 74/1000 | Loss: 0.00001816
Iteration 75/1000 | Loss: 0.00001815
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001814
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001814
Iteration 81/1000 | Loss: 0.00001814
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001813
Iteration 84/1000 | Loss: 0.00001813
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001813
Iteration 87/1000 | Loss: 0.00001813
Iteration 88/1000 | Loss: 0.00001813
Iteration 89/1000 | Loss: 0.00001812
Iteration 90/1000 | Loss: 0.00001812
Iteration 91/1000 | Loss: 0.00001812
Iteration 92/1000 | Loss: 0.00001812
Iteration 93/1000 | Loss: 0.00001812
Iteration 94/1000 | Loss: 0.00001812
Iteration 95/1000 | Loss: 0.00001812
Iteration 96/1000 | Loss: 0.00001812
Iteration 97/1000 | Loss: 0.00001812
Iteration 98/1000 | Loss: 0.00001812
Iteration 99/1000 | Loss: 0.00001812
Iteration 100/1000 | Loss: 0.00001812
Iteration 101/1000 | Loss: 0.00001812
Iteration 102/1000 | Loss: 0.00001812
Iteration 103/1000 | Loss: 0.00001812
Iteration 104/1000 | Loss: 0.00001812
Iteration 105/1000 | Loss: 0.00001812
Iteration 106/1000 | Loss: 0.00001812
Iteration 107/1000 | Loss: 0.00001812
Iteration 108/1000 | Loss: 0.00001812
Iteration 109/1000 | Loss: 0.00001812
Iteration 110/1000 | Loss: 0.00001812
Iteration 111/1000 | Loss: 0.00001812
Iteration 112/1000 | Loss: 0.00001812
Iteration 113/1000 | Loss: 0.00001812
Iteration 114/1000 | Loss: 0.00001812
Iteration 115/1000 | Loss: 0.00001812
Iteration 116/1000 | Loss: 0.00001812
Iteration 117/1000 | Loss: 0.00001812
Iteration 118/1000 | Loss: 0.00001812
Iteration 119/1000 | Loss: 0.00001812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.8118869775207713e-05, 1.8118869775207713e-05, 1.8118869775207713e-05, 1.8118869775207713e-05, 1.8118869775207713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8118869775207713e-05

Optimization complete. Final v2v error: 3.5860519409179688 mm

Highest mean error: 3.9058618545532227 mm for frame 124

Lowest mean error: 3.1439061164855957 mm for frame 183

Saving results

Total time: 42.43612718582153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047821
Iteration 2/25 | Loss: 0.00138732
Iteration 3/25 | Loss: 0.00092276
Iteration 4/25 | Loss: 0.00080438
Iteration 5/25 | Loss: 0.00073720
Iteration 6/25 | Loss: 0.00072476
Iteration 7/25 | Loss: 0.00071389
Iteration 8/25 | Loss: 0.00072783
Iteration 9/25 | Loss: 0.00071387
Iteration 10/25 | Loss: 0.00068450
Iteration 11/25 | Loss: 0.00069984
Iteration 12/25 | Loss: 0.00067517
Iteration 13/25 | Loss: 0.00067118
Iteration 14/25 | Loss: 0.00067072
Iteration 15/25 | Loss: 0.00069198
Iteration 16/25 | Loss: 0.00066541
Iteration 17/25 | Loss: 0.00066048
Iteration 18/25 | Loss: 0.00066014
Iteration 19/25 | Loss: 0.00066012
Iteration 20/25 | Loss: 0.00066011
Iteration 21/25 | Loss: 0.00066011
Iteration 22/25 | Loss: 0.00066011
Iteration 23/25 | Loss: 0.00066011
Iteration 24/25 | Loss: 0.00066011
Iteration 25/25 | Loss: 0.00066011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70806241
Iteration 2/25 | Loss: 0.00038197
Iteration 3/25 | Loss: 0.00038197
Iteration 4/25 | Loss: 0.00038197
Iteration 5/25 | Loss: 0.00038197
Iteration 6/25 | Loss: 0.00038197
Iteration 7/25 | Loss: 0.00038197
Iteration 8/25 | Loss: 0.00038197
Iteration 9/25 | Loss: 0.00038197
Iteration 10/25 | Loss: 0.00038197
Iteration 11/25 | Loss: 0.00038197
Iteration 12/25 | Loss: 0.00038197
Iteration 13/25 | Loss: 0.00038197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00038197037065401673, 0.00038197037065401673, 0.00038197037065401673, 0.00038197037065401673, 0.00038197037065401673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038197037065401673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038197
Iteration 2/1000 | Loss: 0.00127568
Iteration 3/1000 | Loss: 0.00008899
Iteration 4/1000 | Loss: 0.00004332
Iteration 5/1000 | Loss: 0.00020344
Iteration 6/1000 | Loss: 0.00020476
Iteration 7/1000 | Loss: 0.00065724
Iteration 8/1000 | Loss: 0.00038338
Iteration 9/1000 | Loss: 0.00022618
Iteration 10/1000 | Loss: 0.00024505
Iteration 11/1000 | Loss: 0.00005316
Iteration 12/1000 | Loss: 0.00051128
Iteration 13/1000 | Loss: 0.00005500
Iteration 14/1000 | Loss: 0.00002789
Iteration 15/1000 | Loss: 0.00002480
Iteration 16/1000 | Loss: 0.00002293
Iteration 17/1000 | Loss: 0.00002205
Iteration 18/1000 | Loss: 0.00002104
Iteration 19/1000 | Loss: 0.00002039
Iteration 20/1000 | Loss: 0.00001975
Iteration 21/1000 | Loss: 0.00001921
Iteration 22/1000 | Loss: 0.00001879
Iteration 23/1000 | Loss: 0.00001839
Iteration 24/1000 | Loss: 0.00001809
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001737
Iteration 27/1000 | Loss: 0.00001680
Iteration 28/1000 | Loss: 0.00027971
Iteration 29/1000 | Loss: 0.00001905
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001573
Iteration 32/1000 | Loss: 0.00001543
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001502
Iteration 36/1000 | Loss: 0.00001499
Iteration 37/1000 | Loss: 0.00001498
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001497
Iteration 40/1000 | Loss: 0.00001497
Iteration 41/1000 | Loss: 0.00001496
Iteration 42/1000 | Loss: 0.00001493
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001489
Iteration 45/1000 | Loss: 0.00001488
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001484
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001476
Iteration 53/1000 | Loss: 0.00001474
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001473
Iteration 56/1000 | Loss: 0.00001473
Iteration 57/1000 | Loss: 0.00001472
Iteration 58/1000 | Loss: 0.00001472
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001471
Iteration 61/1000 | Loss: 0.00001471
Iteration 62/1000 | Loss: 0.00001471
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001470
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001469
Iteration 72/1000 | Loss: 0.00001469
Iteration 73/1000 | Loss: 0.00001468
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001468
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001468
Iteration 78/1000 | Loss: 0.00001468
Iteration 79/1000 | Loss: 0.00001468
Iteration 80/1000 | Loss: 0.00001467
Iteration 81/1000 | Loss: 0.00001467
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001466
Iteration 88/1000 | Loss: 0.00001466
Iteration 89/1000 | Loss: 0.00001466
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001465
Iteration 92/1000 | Loss: 0.00001465
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001465
Iteration 96/1000 | Loss: 0.00001465
Iteration 97/1000 | Loss: 0.00001465
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001464
Iteration 100/1000 | Loss: 0.00001464
Iteration 101/1000 | Loss: 0.00001464
Iteration 102/1000 | Loss: 0.00001464
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001464
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001464
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001463
Iteration 113/1000 | Loss: 0.00001463
Iteration 114/1000 | Loss: 0.00001463
Iteration 115/1000 | Loss: 0.00001463
Iteration 116/1000 | Loss: 0.00001463
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001462
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001462
Iteration 130/1000 | Loss: 0.00001462
Iteration 131/1000 | Loss: 0.00001462
Iteration 132/1000 | Loss: 0.00001462
Iteration 133/1000 | Loss: 0.00001462
Iteration 134/1000 | Loss: 0.00001462
Iteration 135/1000 | Loss: 0.00001462
Iteration 136/1000 | Loss: 0.00001462
Iteration 137/1000 | Loss: 0.00001462
Iteration 138/1000 | Loss: 0.00001461
Iteration 139/1000 | Loss: 0.00001461
Iteration 140/1000 | Loss: 0.00001461
Iteration 141/1000 | Loss: 0.00001461
Iteration 142/1000 | Loss: 0.00001461
Iteration 143/1000 | Loss: 0.00001461
Iteration 144/1000 | Loss: 0.00001461
Iteration 145/1000 | Loss: 0.00001461
Iteration 146/1000 | Loss: 0.00001461
Iteration 147/1000 | Loss: 0.00001461
Iteration 148/1000 | Loss: 0.00001461
Iteration 149/1000 | Loss: 0.00001461
Iteration 150/1000 | Loss: 0.00001461
Iteration 151/1000 | Loss: 0.00001460
Iteration 152/1000 | Loss: 0.00001460
Iteration 153/1000 | Loss: 0.00001460
Iteration 154/1000 | Loss: 0.00001460
Iteration 155/1000 | Loss: 0.00001460
Iteration 156/1000 | Loss: 0.00001460
Iteration 157/1000 | Loss: 0.00001460
Iteration 158/1000 | Loss: 0.00001460
Iteration 159/1000 | Loss: 0.00001460
Iteration 160/1000 | Loss: 0.00001460
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001459
Iteration 163/1000 | Loss: 0.00001459
Iteration 164/1000 | Loss: 0.00001459
Iteration 165/1000 | Loss: 0.00001459
Iteration 166/1000 | Loss: 0.00001459
Iteration 167/1000 | Loss: 0.00001459
Iteration 168/1000 | Loss: 0.00001459
Iteration 169/1000 | Loss: 0.00001459
Iteration 170/1000 | Loss: 0.00001459
Iteration 171/1000 | Loss: 0.00001459
Iteration 172/1000 | Loss: 0.00001458
Iteration 173/1000 | Loss: 0.00001458
Iteration 174/1000 | Loss: 0.00001458
Iteration 175/1000 | Loss: 0.00001458
Iteration 176/1000 | Loss: 0.00001458
Iteration 177/1000 | Loss: 0.00001458
Iteration 178/1000 | Loss: 0.00001458
Iteration 179/1000 | Loss: 0.00001458
Iteration 180/1000 | Loss: 0.00001458
Iteration 181/1000 | Loss: 0.00001458
Iteration 182/1000 | Loss: 0.00001458
Iteration 183/1000 | Loss: 0.00001458
Iteration 184/1000 | Loss: 0.00001458
Iteration 185/1000 | Loss: 0.00001458
Iteration 186/1000 | Loss: 0.00001457
Iteration 187/1000 | Loss: 0.00001457
Iteration 188/1000 | Loss: 0.00001457
Iteration 189/1000 | Loss: 0.00001457
Iteration 190/1000 | Loss: 0.00001457
Iteration 191/1000 | Loss: 0.00001457
Iteration 192/1000 | Loss: 0.00001457
Iteration 193/1000 | Loss: 0.00001457
Iteration 194/1000 | Loss: 0.00001457
Iteration 195/1000 | Loss: 0.00001457
Iteration 196/1000 | Loss: 0.00001457
Iteration 197/1000 | Loss: 0.00001457
Iteration 198/1000 | Loss: 0.00001457
Iteration 199/1000 | Loss: 0.00001457
Iteration 200/1000 | Loss: 0.00001457
Iteration 201/1000 | Loss: 0.00001457
Iteration 202/1000 | Loss: 0.00001457
Iteration 203/1000 | Loss: 0.00001457
Iteration 204/1000 | Loss: 0.00001457
Iteration 205/1000 | Loss: 0.00001457
Iteration 206/1000 | Loss: 0.00001457
Iteration 207/1000 | Loss: 0.00001457
Iteration 208/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.4573594853573013e-05, 1.4573594853573013e-05, 1.4573594853573013e-05, 1.4573594853573013e-05, 1.4573594853573013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4573594853573013e-05

Optimization complete. Final v2v error: 3.130261182785034 mm

Highest mean error: 5.515599250793457 mm for frame 124

Lowest mean error: 2.5845394134521484 mm for frame 71

Saving results

Total time: 93.704754114151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022714
Iteration 2/25 | Loss: 0.01022713
Iteration 3/25 | Loss: 0.00185394
Iteration 4/25 | Loss: 0.00082990
Iteration 5/25 | Loss: 0.00070600
Iteration 6/25 | Loss: 0.00069320
Iteration 7/25 | Loss: 0.00069099
Iteration 8/25 | Loss: 0.00069064
Iteration 9/25 | Loss: 0.00069064
Iteration 10/25 | Loss: 0.00069064
Iteration 11/25 | Loss: 0.00069064
Iteration 12/25 | Loss: 0.00069064
Iteration 13/25 | Loss: 0.00069064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006906426860950887, 0.0006906426860950887, 0.0006906426860950887, 0.0006906426860950887, 0.0006906426860950887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006906426860950887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43893099
Iteration 2/25 | Loss: 0.00039958
Iteration 3/25 | Loss: 0.00039958
Iteration 4/25 | Loss: 0.00039958
Iteration 5/25 | Loss: 0.00039958
Iteration 6/25 | Loss: 0.00039958
Iteration 7/25 | Loss: 0.00039958
Iteration 8/25 | Loss: 0.00039958
Iteration 9/25 | Loss: 0.00039958
Iteration 10/25 | Loss: 0.00039958
Iteration 11/25 | Loss: 0.00039958
Iteration 12/25 | Loss: 0.00039958
Iteration 13/25 | Loss: 0.00039958
Iteration 14/25 | Loss: 0.00039958
Iteration 15/25 | Loss: 0.00039958
Iteration 16/25 | Loss: 0.00039958
Iteration 17/25 | Loss: 0.00039958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00039957763510756195, 0.00039957763510756195, 0.00039957763510756195, 0.00039957763510756195, 0.00039957763510756195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039957763510756195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039958
Iteration 2/1000 | Loss: 0.00002481
Iteration 3/1000 | Loss: 0.00001649
Iteration 4/1000 | Loss: 0.00001484
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001318
Iteration 7/1000 | Loss: 0.00001283
Iteration 8/1000 | Loss: 0.00001257
Iteration 9/1000 | Loss: 0.00001236
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001223
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001215
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001213
Iteration 18/1000 | Loss: 0.00001212
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001212
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001211
Iteration 23/1000 | Loss: 0.00001211
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00001210
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001210
Iteration 35/1000 | Loss: 0.00001210
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001210
Iteration 44/1000 | Loss: 0.00001210
Iteration 45/1000 | Loss: 0.00001210
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001210
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001210
Iteration 53/1000 | Loss: 0.00001210
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001210
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.2100178537366446e-05, 1.2100178537366446e-05, 1.2100178537366446e-05, 1.2100178537366446e-05, 1.2100178537366446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2100178537366446e-05

Optimization complete. Final v2v error: 2.94769549369812 mm

Highest mean error: 3.8879897594451904 mm for frame 238

Lowest mean error: 2.6728098392486572 mm for frame 46

Saving results

Total time: 30.33244490623474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838282
Iteration 2/25 | Loss: 0.00080308
Iteration 3/25 | Loss: 0.00062517
Iteration 4/25 | Loss: 0.00059800
Iteration 5/25 | Loss: 0.00059190
Iteration 6/25 | Loss: 0.00059012
Iteration 7/25 | Loss: 0.00058970
Iteration 8/25 | Loss: 0.00058970
Iteration 9/25 | Loss: 0.00058970
Iteration 10/25 | Loss: 0.00058970
Iteration 11/25 | Loss: 0.00058970
Iteration 12/25 | Loss: 0.00058970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005897002993151546, 0.0005897002993151546, 0.0005897002993151546, 0.0005897002993151546, 0.0005897002993151546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005897002993151546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46067798
Iteration 2/25 | Loss: 0.00025921
Iteration 3/25 | Loss: 0.00025920
Iteration 4/25 | Loss: 0.00025920
Iteration 5/25 | Loss: 0.00025920
Iteration 6/25 | Loss: 0.00025920
Iteration 7/25 | Loss: 0.00025920
Iteration 8/25 | Loss: 0.00025920
Iteration 9/25 | Loss: 0.00025920
Iteration 10/25 | Loss: 0.00025920
Iteration 11/25 | Loss: 0.00025920
Iteration 12/25 | Loss: 0.00025920
Iteration 13/25 | Loss: 0.00025920
Iteration 14/25 | Loss: 0.00025920
Iteration 15/25 | Loss: 0.00025920
Iteration 16/25 | Loss: 0.00025920
Iteration 17/25 | Loss: 0.00025920
Iteration 18/25 | Loss: 0.00025920
Iteration 19/25 | Loss: 0.00025920
Iteration 20/25 | Loss: 0.00025920
Iteration 21/25 | Loss: 0.00025920
Iteration 22/25 | Loss: 0.00025920
Iteration 23/25 | Loss: 0.00025920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002592007804196328, 0.0002592007804196328, 0.0002592007804196328, 0.0002592007804196328, 0.0002592007804196328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002592007804196328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025920
Iteration 2/1000 | Loss: 0.00002203
Iteration 3/1000 | Loss: 0.00001535
Iteration 4/1000 | Loss: 0.00001310
Iteration 5/1000 | Loss: 0.00001241
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001166
Iteration 8/1000 | Loss: 0.00001134
Iteration 9/1000 | Loss: 0.00001134
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001127
Iteration 12/1000 | Loss: 0.00001127
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001121
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001113
Iteration 23/1000 | Loss: 0.00001113
Iteration 24/1000 | Loss: 0.00001113
Iteration 25/1000 | Loss: 0.00001112
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001107
Iteration 28/1000 | Loss: 0.00001107
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001105
Iteration 31/1000 | Loss: 0.00001105
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001102
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001101
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001098
Iteration 45/1000 | Loss: 0.00001098
Iteration 46/1000 | Loss: 0.00001097
Iteration 47/1000 | Loss: 0.00001096
Iteration 48/1000 | Loss: 0.00001089
Iteration 49/1000 | Loss: 0.00001089
Iteration 50/1000 | Loss: 0.00001089
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001082
Iteration 55/1000 | Loss: 0.00001082
Iteration 56/1000 | Loss: 0.00001082
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001078
Iteration 60/1000 | Loss: 0.00001078
Iteration 61/1000 | Loss: 0.00001078
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001078
Iteration 64/1000 | Loss: 0.00001078
Iteration 65/1000 | Loss: 0.00001078
Iteration 66/1000 | Loss: 0.00001078
Iteration 67/1000 | Loss: 0.00001077
Iteration 68/1000 | Loss: 0.00001077
Iteration 69/1000 | Loss: 0.00001077
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001076
Iteration 72/1000 | Loss: 0.00001076
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001075
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001074
Iteration 83/1000 | Loss: 0.00001074
Iteration 84/1000 | Loss: 0.00001074
Iteration 85/1000 | Loss: 0.00001073
Iteration 86/1000 | Loss: 0.00001073
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001072
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001070
Iteration 94/1000 | Loss: 0.00001070
Iteration 95/1000 | Loss: 0.00001070
Iteration 96/1000 | Loss: 0.00001070
Iteration 97/1000 | Loss: 0.00001070
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001069
Iteration 103/1000 | Loss: 0.00001069
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001068
Iteration 107/1000 | Loss: 0.00001067
Iteration 108/1000 | Loss: 0.00001067
Iteration 109/1000 | Loss: 0.00001067
Iteration 110/1000 | Loss: 0.00001066
Iteration 111/1000 | Loss: 0.00001066
Iteration 112/1000 | Loss: 0.00001065
Iteration 113/1000 | Loss: 0.00001065
Iteration 114/1000 | Loss: 0.00001065
Iteration 115/1000 | Loss: 0.00001065
Iteration 116/1000 | Loss: 0.00001065
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001064
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001064
Iteration 121/1000 | Loss: 0.00001064
Iteration 122/1000 | Loss: 0.00001064
Iteration 123/1000 | Loss: 0.00001064
Iteration 124/1000 | Loss: 0.00001064
Iteration 125/1000 | Loss: 0.00001064
Iteration 126/1000 | Loss: 0.00001064
Iteration 127/1000 | Loss: 0.00001064
Iteration 128/1000 | Loss: 0.00001064
Iteration 129/1000 | Loss: 0.00001064
Iteration 130/1000 | Loss: 0.00001064
Iteration 131/1000 | Loss: 0.00001064
Iteration 132/1000 | Loss: 0.00001064
Iteration 133/1000 | Loss: 0.00001064
Iteration 134/1000 | Loss: 0.00001064
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Iteration 139/1000 | Loss: 0.00001064
Iteration 140/1000 | Loss: 0.00001064
Iteration 141/1000 | Loss: 0.00001064
Iteration 142/1000 | Loss: 0.00001064
Iteration 143/1000 | Loss: 0.00001064
Iteration 144/1000 | Loss: 0.00001064
Iteration 145/1000 | Loss: 0.00001064
Iteration 146/1000 | Loss: 0.00001064
Iteration 147/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.06397228591959e-05, 1.06397228591959e-05, 1.06397228591959e-05, 1.06397228591959e-05, 1.06397228591959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.06397228591959e-05

Optimization complete. Final v2v error: 2.7414515018463135 mm

Highest mean error: 3.1308555603027344 mm for frame 47

Lowest mean error: 2.567631483078003 mm for frame 165

Saving results

Total time: 34.66421937942505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844175
Iteration 2/25 | Loss: 0.00099303
Iteration 3/25 | Loss: 0.00070510
Iteration 4/25 | Loss: 0.00067782
Iteration 5/25 | Loss: 0.00066750
Iteration 6/25 | Loss: 0.00066501
Iteration 7/25 | Loss: 0.00066459
Iteration 8/25 | Loss: 0.00066459
Iteration 9/25 | Loss: 0.00066459
Iteration 10/25 | Loss: 0.00066459
Iteration 11/25 | Loss: 0.00066459
Iteration 12/25 | Loss: 0.00066459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006645935936830938, 0.0006645935936830938, 0.0006645935936830938, 0.0006645935936830938, 0.0006645935936830938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006645935936830938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.45020914
Iteration 2/25 | Loss: 0.00030839
Iteration 3/25 | Loss: 0.00030833
Iteration 4/25 | Loss: 0.00030833
Iteration 5/25 | Loss: 0.00030833
Iteration 6/25 | Loss: 0.00030833
Iteration 7/25 | Loss: 0.00030833
Iteration 8/25 | Loss: 0.00030833
Iteration 9/25 | Loss: 0.00030833
Iteration 10/25 | Loss: 0.00030833
Iteration 11/25 | Loss: 0.00030833
Iteration 12/25 | Loss: 0.00030833
Iteration 13/25 | Loss: 0.00030833
Iteration 14/25 | Loss: 0.00030833
Iteration 15/25 | Loss: 0.00030833
Iteration 16/25 | Loss: 0.00030833
Iteration 17/25 | Loss: 0.00030833
Iteration 18/25 | Loss: 0.00030833
Iteration 19/25 | Loss: 0.00030833
Iteration 20/25 | Loss: 0.00030833
Iteration 21/25 | Loss: 0.00030833
Iteration 22/25 | Loss: 0.00030833
Iteration 23/25 | Loss: 0.00030833
Iteration 24/25 | Loss: 0.00030833
Iteration 25/25 | Loss: 0.00030833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030833
Iteration 2/1000 | Loss: 0.00003129
Iteration 3/1000 | Loss: 0.00002430
Iteration 4/1000 | Loss: 0.00002275
Iteration 5/1000 | Loss: 0.00002165
Iteration 6/1000 | Loss: 0.00002094
Iteration 7/1000 | Loss: 0.00002038
Iteration 8/1000 | Loss: 0.00002001
Iteration 9/1000 | Loss: 0.00001977
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001958
Iteration 12/1000 | Loss: 0.00001953
Iteration 13/1000 | Loss: 0.00001952
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001947
Iteration 16/1000 | Loss: 0.00001943
Iteration 17/1000 | Loss: 0.00001938
Iteration 18/1000 | Loss: 0.00001938
Iteration 19/1000 | Loss: 0.00001936
Iteration 20/1000 | Loss: 0.00001935
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001935
Iteration 24/1000 | Loss: 0.00001935
Iteration 25/1000 | Loss: 0.00001935
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001935
Iteration 28/1000 | Loss: 0.00001934
Iteration 29/1000 | Loss: 0.00001934
Iteration 30/1000 | Loss: 0.00001934
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001934
Iteration 33/1000 | Loss: 0.00001934
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001933
Iteration 36/1000 | Loss: 0.00001933
Iteration 37/1000 | Loss: 0.00001933
Iteration 38/1000 | Loss: 0.00001933
Iteration 39/1000 | Loss: 0.00001933
Iteration 40/1000 | Loss: 0.00001933
Iteration 41/1000 | Loss: 0.00001933
Iteration 42/1000 | Loss: 0.00001932
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001931
Iteration 46/1000 | Loss: 0.00001931
Iteration 47/1000 | Loss: 0.00001931
Iteration 48/1000 | Loss: 0.00001931
Iteration 49/1000 | Loss: 0.00001931
Iteration 50/1000 | Loss: 0.00001930
Iteration 51/1000 | Loss: 0.00001930
Iteration 52/1000 | Loss: 0.00001930
Iteration 53/1000 | Loss: 0.00001930
Iteration 54/1000 | Loss: 0.00001930
Iteration 55/1000 | Loss: 0.00001929
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001929
Iteration 58/1000 | Loss: 0.00001929
Iteration 59/1000 | Loss: 0.00001929
Iteration 60/1000 | Loss: 0.00001929
Iteration 61/1000 | Loss: 0.00001929
Iteration 62/1000 | Loss: 0.00001929
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001929
Iteration 65/1000 | Loss: 0.00001929
Iteration 66/1000 | Loss: 0.00001929
Iteration 67/1000 | Loss: 0.00001929
Iteration 68/1000 | Loss: 0.00001929
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001929
Iteration 71/1000 | Loss: 0.00001929
Iteration 72/1000 | Loss: 0.00001929
Iteration 73/1000 | Loss: 0.00001929
Iteration 74/1000 | Loss: 0.00001929
Iteration 75/1000 | Loss: 0.00001929
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.9289138435851783e-05, 1.9289138435851783e-05, 1.9289138435851783e-05, 1.9289138435851783e-05, 1.9289138435851783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9289138435851783e-05

Optimization complete. Final v2v error: 3.7005977630615234 mm

Highest mean error: 4.557014465332031 mm for frame 66

Lowest mean error: 3.205626964569092 mm for frame 173

Saving results

Total time: 34.42146277427673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564231
Iteration 2/25 | Loss: 0.00107203
Iteration 3/25 | Loss: 0.00071207
Iteration 4/25 | Loss: 0.00066675
Iteration 5/25 | Loss: 0.00065813
Iteration 6/25 | Loss: 0.00065546
Iteration 7/25 | Loss: 0.00065456
Iteration 8/25 | Loss: 0.00065434
Iteration 9/25 | Loss: 0.00065434
Iteration 10/25 | Loss: 0.00065434
Iteration 11/25 | Loss: 0.00065434
Iteration 12/25 | Loss: 0.00065434
Iteration 13/25 | Loss: 0.00065434
Iteration 14/25 | Loss: 0.00065434
Iteration 15/25 | Loss: 0.00065434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006543445633724332, 0.0006543445633724332, 0.0006543445633724332, 0.0006543445633724332, 0.0006543445633724332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006543445633724332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.51312041
Iteration 2/25 | Loss: 0.00032868
Iteration 3/25 | Loss: 0.00032857
Iteration 4/25 | Loss: 0.00032857
Iteration 5/25 | Loss: 0.00032857
Iteration 6/25 | Loss: 0.00032857
Iteration 7/25 | Loss: 0.00032857
Iteration 8/25 | Loss: 0.00032857
Iteration 9/25 | Loss: 0.00032857
Iteration 10/25 | Loss: 0.00032857
Iteration 11/25 | Loss: 0.00032857
Iteration 12/25 | Loss: 0.00032857
Iteration 13/25 | Loss: 0.00032857
Iteration 14/25 | Loss: 0.00032857
Iteration 15/25 | Loss: 0.00032857
Iteration 16/25 | Loss: 0.00032857
Iteration 17/25 | Loss: 0.00032857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00032857051701284945, 0.00032857051701284945, 0.00032857051701284945, 0.00032857051701284945, 0.00032857051701284945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032857051701284945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032857
Iteration 2/1000 | Loss: 0.00002829
Iteration 3/1000 | Loss: 0.00001910
Iteration 4/1000 | Loss: 0.00001733
Iteration 5/1000 | Loss: 0.00001643
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001551
Iteration 8/1000 | Loss: 0.00001521
Iteration 9/1000 | Loss: 0.00001495
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001472
Iteration 13/1000 | Loss: 0.00001466
Iteration 14/1000 | Loss: 0.00001461
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001459
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001458
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001456
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001455
Iteration 26/1000 | Loss: 0.00001454
Iteration 27/1000 | Loss: 0.00001454
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001452
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001451
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001448
Iteration 46/1000 | Loss: 0.00001448
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001447
Iteration 49/1000 | Loss: 0.00001447
Iteration 50/1000 | Loss: 0.00001447
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001446
Iteration 53/1000 | Loss: 0.00001446
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001442
Iteration 73/1000 | Loss: 0.00001442
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001442
Iteration 79/1000 | Loss: 0.00001442
Iteration 80/1000 | Loss: 0.00001442
Iteration 81/1000 | Loss: 0.00001442
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001442
Iteration 87/1000 | Loss: 0.00001442
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001442
Iteration 94/1000 | Loss: 0.00001442
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001442
Iteration 97/1000 | Loss: 0.00001442
Iteration 98/1000 | Loss: 0.00001442
Iteration 99/1000 | Loss: 0.00001442
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001442
Iteration 107/1000 | Loss: 0.00001442
Iteration 108/1000 | Loss: 0.00001442
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001442
Iteration 111/1000 | Loss: 0.00001442
Iteration 112/1000 | Loss: 0.00001442
Iteration 113/1000 | Loss: 0.00001442
Iteration 114/1000 | Loss: 0.00001442
Iteration 115/1000 | Loss: 0.00001442
Iteration 116/1000 | Loss: 0.00001442
Iteration 117/1000 | Loss: 0.00001442
Iteration 118/1000 | Loss: 0.00001442
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001442
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001442
Iteration 125/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.4419421859201975e-05, 1.4419421859201975e-05, 1.4419421859201975e-05, 1.4419421859201975e-05, 1.4419421859201975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4419421859201975e-05

Optimization complete. Final v2v error: 3.1618785858154297 mm

Highest mean error: 4.782215118408203 mm for frame 186

Lowest mean error: 2.5889196395874023 mm for frame 147

Saving results

Total time: 38.38368058204651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00166883
Iteration 2/25 | Loss: 0.00075470
Iteration 3/25 | Loss: 0.00065867
Iteration 4/25 | Loss: 0.00063519
Iteration 5/25 | Loss: 0.00062490
Iteration 6/25 | Loss: 0.00062298
Iteration 7/25 | Loss: 0.00062256
Iteration 8/25 | Loss: 0.00062256
Iteration 9/25 | Loss: 0.00062256
Iteration 10/25 | Loss: 0.00062256
Iteration 11/25 | Loss: 0.00062256
Iteration 12/25 | Loss: 0.00062256
Iteration 13/25 | Loss: 0.00062256
Iteration 14/25 | Loss: 0.00062256
Iteration 15/25 | Loss: 0.00062256
Iteration 16/25 | Loss: 0.00062256
Iteration 17/25 | Loss: 0.00062256
Iteration 18/25 | Loss: 0.00062256
Iteration 19/25 | Loss: 0.00062256
Iteration 20/25 | Loss: 0.00062256
Iteration 21/25 | Loss: 0.00062256
Iteration 22/25 | Loss: 0.00062256
Iteration 23/25 | Loss: 0.00062256
Iteration 24/25 | Loss: 0.00062256
Iteration 25/25 | Loss: 0.00062256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49164820
Iteration 2/25 | Loss: 0.00032370
Iteration 3/25 | Loss: 0.00032370
Iteration 4/25 | Loss: 0.00032370
Iteration 5/25 | Loss: 0.00032370
Iteration 6/25 | Loss: 0.00032370
Iteration 7/25 | Loss: 0.00032370
Iteration 8/25 | Loss: 0.00032370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0003237028431612998, 0.0003237028431612998, 0.0003237028431612998, 0.0003237028431612998, 0.0003237028431612998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003237028431612998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032370
Iteration 2/1000 | Loss: 0.00002995
Iteration 3/1000 | Loss: 0.00001635
Iteration 4/1000 | Loss: 0.00001434
Iteration 5/1000 | Loss: 0.00001368
Iteration 6/1000 | Loss: 0.00001295
Iteration 7/1000 | Loss: 0.00001274
Iteration 8/1000 | Loss: 0.00001243
Iteration 9/1000 | Loss: 0.00001238
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001208
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001201
Iteration 23/1000 | Loss: 0.00001201
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001185
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001184
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001183
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001182
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001181
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001179
Iteration 114/1000 | Loss: 0.00001179
Iteration 115/1000 | Loss: 0.00001179
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001179
Iteration 127/1000 | Loss: 0.00001179
Iteration 128/1000 | Loss: 0.00001179
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Iteration 133/1000 | Loss: 0.00001179
Iteration 134/1000 | Loss: 0.00001179
Iteration 135/1000 | Loss: 0.00001179
Iteration 136/1000 | Loss: 0.00001179
Iteration 137/1000 | Loss: 0.00001179
Iteration 138/1000 | Loss: 0.00001179
Iteration 139/1000 | Loss: 0.00001179
Iteration 140/1000 | Loss: 0.00001179
Iteration 141/1000 | Loss: 0.00001179
Iteration 142/1000 | Loss: 0.00001179
Iteration 143/1000 | Loss: 0.00001179
Iteration 144/1000 | Loss: 0.00001179
Iteration 145/1000 | Loss: 0.00001179
Iteration 146/1000 | Loss: 0.00001179
Iteration 147/1000 | Loss: 0.00001179
Iteration 148/1000 | Loss: 0.00001179
Iteration 149/1000 | Loss: 0.00001179
Iteration 150/1000 | Loss: 0.00001179
Iteration 151/1000 | Loss: 0.00001179
Iteration 152/1000 | Loss: 0.00001179
Iteration 153/1000 | Loss: 0.00001179
Iteration 154/1000 | Loss: 0.00001179
Iteration 155/1000 | Loss: 0.00001179
Iteration 156/1000 | Loss: 0.00001179
Iteration 157/1000 | Loss: 0.00001179
Iteration 158/1000 | Loss: 0.00001179
Iteration 159/1000 | Loss: 0.00001179
Iteration 160/1000 | Loss: 0.00001179
Iteration 161/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.1794087185990065e-05, 1.1794087185990065e-05, 1.1794087185990065e-05, 1.1794087185990065e-05, 1.1794087185990065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1794087185990065e-05

Optimization complete. Final v2v error: 2.9636290073394775 mm

Highest mean error: 3.2539010047912598 mm for frame 39

Lowest mean error: 2.791720390319824 mm for frame 186

Saving results

Total time: 37.58006167411804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403531
Iteration 2/25 | Loss: 0.00076189
Iteration 3/25 | Loss: 0.00066650
Iteration 4/25 | Loss: 0.00064068
Iteration 5/25 | Loss: 0.00063071
Iteration 6/25 | Loss: 0.00062912
Iteration 7/25 | Loss: 0.00062879
Iteration 8/25 | Loss: 0.00062879
Iteration 9/25 | Loss: 0.00062879
Iteration 10/25 | Loss: 0.00062879
Iteration 11/25 | Loss: 0.00062879
Iteration 12/25 | Loss: 0.00062879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006287927972152829, 0.0006287927972152829, 0.0006287927972152829, 0.0006287927972152829, 0.0006287927972152829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006287927972152829

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51883435
Iteration 2/25 | Loss: 0.00027185
Iteration 3/25 | Loss: 0.00027185
Iteration 4/25 | Loss: 0.00027185
Iteration 5/25 | Loss: 0.00027185
Iteration 6/25 | Loss: 0.00027185
Iteration 7/25 | Loss: 0.00027185
Iteration 8/25 | Loss: 0.00027185
Iteration 9/25 | Loss: 0.00027185
Iteration 10/25 | Loss: 0.00027185
Iteration 11/25 | Loss: 0.00027185
Iteration 12/25 | Loss: 0.00027185
Iteration 13/25 | Loss: 0.00027185
Iteration 14/25 | Loss: 0.00027185
Iteration 15/25 | Loss: 0.00027185
Iteration 16/25 | Loss: 0.00027185
Iteration 17/25 | Loss: 0.00027185
Iteration 18/25 | Loss: 0.00027185
Iteration 19/25 | Loss: 0.00027185
Iteration 20/25 | Loss: 0.00027185
Iteration 21/25 | Loss: 0.00027185
Iteration 22/25 | Loss: 0.00027185
Iteration 23/25 | Loss: 0.00027185
Iteration 24/25 | Loss: 0.00027185
Iteration 25/25 | Loss: 0.00027185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027185
Iteration 2/1000 | Loss: 0.00003009
Iteration 3/1000 | Loss: 0.00002111
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001683
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001573
Iteration 15/1000 | Loss: 0.00001572
Iteration 16/1000 | Loss: 0.00001567
Iteration 17/1000 | Loss: 0.00001567
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001566
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001566
Iteration 26/1000 | Loss: 0.00001566
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001566
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001562
Iteration 32/1000 | Loss: 0.00001561
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001561
Iteration 35/1000 | Loss: 0.00001560
Iteration 36/1000 | Loss: 0.00001560
Iteration 37/1000 | Loss: 0.00001559
Iteration 38/1000 | Loss: 0.00001559
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001557
Iteration 43/1000 | Loss: 0.00001556
Iteration 44/1000 | Loss: 0.00001556
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00001554
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001553
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001550
Iteration 52/1000 | Loss: 0.00001550
Iteration 53/1000 | Loss: 0.00001550
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001549
Iteration 57/1000 | Loss: 0.00001549
Iteration 58/1000 | Loss: 0.00001549
Iteration 59/1000 | Loss: 0.00001549
Iteration 60/1000 | Loss: 0.00001548
Iteration 61/1000 | Loss: 0.00001548
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001543
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001539
Iteration 94/1000 | Loss: 0.00001539
Iteration 95/1000 | Loss: 0.00001538
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001538
Iteration 98/1000 | Loss: 0.00001537
Iteration 99/1000 | Loss: 0.00001537
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001536
Iteration 102/1000 | Loss: 0.00001536
Iteration 103/1000 | Loss: 0.00001536
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001535
Iteration 106/1000 | Loss: 0.00001535
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001532
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001531
Iteration 113/1000 | Loss: 0.00001531
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001531
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001530
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001529
Iteration 123/1000 | Loss: 0.00001529
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00001529
Iteration 126/1000 | Loss: 0.00001529
Iteration 127/1000 | Loss: 0.00001528
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001528
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001527
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001527
Iteration 144/1000 | Loss: 0.00001527
Iteration 145/1000 | Loss: 0.00001527
Iteration 146/1000 | Loss: 0.00001527
Iteration 147/1000 | Loss: 0.00001527
Iteration 148/1000 | Loss: 0.00001527
Iteration 149/1000 | Loss: 0.00001527
Iteration 150/1000 | Loss: 0.00001527
Iteration 151/1000 | Loss: 0.00001527
Iteration 152/1000 | Loss: 0.00001526
Iteration 153/1000 | Loss: 0.00001526
Iteration 154/1000 | Loss: 0.00001526
Iteration 155/1000 | Loss: 0.00001526
Iteration 156/1000 | Loss: 0.00001526
Iteration 157/1000 | Loss: 0.00001526
Iteration 158/1000 | Loss: 0.00001526
Iteration 159/1000 | Loss: 0.00001526
Iteration 160/1000 | Loss: 0.00001526
Iteration 161/1000 | Loss: 0.00001526
Iteration 162/1000 | Loss: 0.00001526
Iteration 163/1000 | Loss: 0.00001526
Iteration 164/1000 | Loss: 0.00001526
Iteration 165/1000 | Loss: 0.00001526
Iteration 166/1000 | Loss: 0.00001526
Iteration 167/1000 | Loss: 0.00001525
Iteration 168/1000 | Loss: 0.00001525
Iteration 169/1000 | Loss: 0.00001525
Iteration 170/1000 | Loss: 0.00001525
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00001525
Iteration 173/1000 | Loss: 0.00001525
Iteration 174/1000 | Loss: 0.00001525
Iteration 175/1000 | Loss: 0.00001525
Iteration 176/1000 | Loss: 0.00001525
Iteration 177/1000 | Loss: 0.00001525
Iteration 178/1000 | Loss: 0.00001525
Iteration 179/1000 | Loss: 0.00001525
Iteration 180/1000 | Loss: 0.00001525
Iteration 181/1000 | Loss: 0.00001525
Iteration 182/1000 | Loss: 0.00001525
Iteration 183/1000 | Loss: 0.00001525
Iteration 184/1000 | Loss: 0.00001525
Iteration 185/1000 | Loss: 0.00001525
Iteration 186/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.525296647741925e-05, 1.525296647741925e-05, 1.525296647741925e-05, 1.525296647741925e-05, 1.525296647741925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.525296647741925e-05

Optimization complete. Final v2v error: 3.3077330589294434 mm

Highest mean error: 3.774132490158081 mm for frame 80

Lowest mean error: 3.0278186798095703 mm for frame 88

Saving results

Total time: 34.9486141204834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069317
Iteration 2/25 | Loss: 0.00259762
Iteration 3/25 | Loss: 0.00148912
Iteration 4/25 | Loss: 0.00120599
Iteration 5/25 | Loss: 0.00104975
Iteration 6/25 | Loss: 0.00102278
Iteration 7/25 | Loss: 0.00097294
Iteration 8/25 | Loss: 0.00087803
Iteration 9/25 | Loss: 0.00081741
Iteration 10/25 | Loss: 0.00078007
Iteration 11/25 | Loss: 0.00075900
Iteration 12/25 | Loss: 0.00076425
Iteration 13/25 | Loss: 0.00070131
Iteration 14/25 | Loss: 0.00070190
Iteration 15/25 | Loss: 0.00068473
Iteration 16/25 | Loss: 0.00068607
Iteration 17/25 | Loss: 0.00067543
Iteration 18/25 | Loss: 0.00067255
Iteration 19/25 | Loss: 0.00066683
Iteration 20/25 | Loss: 0.00066137
Iteration 21/25 | Loss: 0.00065917
Iteration 22/25 | Loss: 0.00065840
Iteration 23/25 | Loss: 0.00065813
Iteration 24/25 | Loss: 0.00065804
Iteration 25/25 | Loss: 0.00065795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03169990
Iteration 2/25 | Loss: 0.00070795
Iteration 3/25 | Loss: 0.00033402
Iteration 4/25 | Loss: 0.00033401
Iteration 5/25 | Loss: 0.00033401
Iteration 6/25 | Loss: 0.00033401
Iteration 7/25 | Loss: 0.00033401
Iteration 8/25 | Loss: 0.00033401
Iteration 9/25 | Loss: 0.00033401
Iteration 10/25 | Loss: 0.00033401
Iteration 11/25 | Loss: 0.00033401
Iteration 12/25 | Loss: 0.00033401
Iteration 13/25 | Loss: 0.00033401
Iteration 14/25 | Loss: 0.00033401
Iteration 15/25 | Loss: 0.00033401
Iteration 16/25 | Loss: 0.00033401
Iteration 17/25 | Loss: 0.00033401
Iteration 18/25 | Loss: 0.00033401
Iteration 19/25 | Loss: 0.00033401
Iteration 20/25 | Loss: 0.00033401
Iteration 21/25 | Loss: 0.00033401
Iteration 22/25 | Loss: 0.00033401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00033401232212781906, 0.00033401232212781906, 0.00033401232212781906, 0.00033401232212781906, 0.00033401232212781906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033401232212781906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033401
Iteration 2/1000 | Loss: 0.00008915
Iteration 3/1000 | Loss: 0.00004122
Iteration 4/1000 | Loss: 0.00008704
Iteration 5/1000 | Loss: 0.00018450
Iteration 6/1000 | Loss: 0.00083479
Iteration 7/1000 | Loss: 0.00004134
Iteration 8/1000 | Loss: 0.00006267
Iteration 9/1000 | Loss: 0.00002708
Iteration 10/1000 | Loss: 0.00002481
Iteration 11/1000 | Loss: 0.00002928
Iteration 12/1000 | Loss: 0.00005362
Iteration 13/1000 | Loss: 0.00002139
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00039891
Iteration 16/1000 | Loss: 0.00052664
Iteration 17/1000 | Loss: 0.00002105
Iteration 18/1000 | Loss: 0.00010183
Iteration 19/1000 | Loss: 0.00003552
Iteration 20/1000 | Loss: 0.00044821
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00003589
Iteration 24/1000 | Loss: 0.00003017
Iteration 25/1000 | Loss: 0.00001593
Iteration 26/1000 | Loss: 0.00002776
Iteration 27/1000 | Loss: 0.00001585
Iteration 28/1000 | Loss: 0.00002730
Iteration 29/1000 | Loss: 0.00001576
Iteration 30/1000 | Loss: 0.00001576
Iteration 31/1000 | Loss: 0.00001576
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001575
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00003196
Iteration 38/1000 | Loss: 0.00010737
Iteration 39/1000 | Loss: 0.00005234
Iteration 40/1000 | Loss: 0.00010010
Iteration 41/1000 | Loss: 0.00012192
Iteration 42/1000 | Loss: 0.00005293
Iteration 43/1000 | Loss: 0.00001580
Iteration 44/1000 | Loss: 0.00004953
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00005378
Iteration 47/1000 | Loss: 0.00010581
Iteration 48/1000 | Loss: 0.00065410
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00002721
Iteration 51/1000 | Loss: 0.00002802
Iteration 52/1000 | Loss: 0.00001554
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00003075
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001549
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001547
Iteration 60/1000 | Loss: 0.00008279
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00018621
Iteration 63/1000 | Loss: 0.00004437
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00004348
Iteration 66/1000 | Loss: 0.00002273
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00013713
Iteration 69/1000 | Loss: 0.00004624
Iteration 70/1000 | Loss: 0.00002950
Iteration 71/1000 | Loss: 0.00002758
Iteration 72/1000 | Loss: 0.00003365
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00002970
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001539
Iteration 80/1000 | Loss: 0.00001539
Iteration 81/1000 | Loss: 0.00001539
Iteration 82/1000 | Loss: 0.00001539
Iteration 83/1000 | Loss: 0.00001538
Iteration 84/1000 | Loss: 0.00001538
Iteration 85/1000 | Loss: 0.00001538
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001534
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001533
Iteration 98/1000 | Loss: 0.00001533
Iteration 99/1000 | Loss: 0.00001533
Iteration 100/1000 | Loss: 0.00001533
Iteration 101/1000 | Loss: 0.00001533
Iteration 102/1000 | Loss: 0.00001533
Iteration 103/1000 | Loss: 0.00001533
Iteration 104/1000 | Loss: 0.00001533
Iteration 105/1000 | Loss: 0.00001533
Iteration 106/1000 | Loss: 0.00001533
Iteration 107/1000 | Loss: 0.00001533
Iteration 108/1000 | Loss: 0.00001533
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001533
Iteration 112/1000 | Loss: 0.00001533
Iteration 113/1000 | Loss: 0.00001533
Iteration 114/1000 | Loss: 0.00001533
Iteration 115/1000 | Loss: 0.00001533
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001533
Iteration 118/1000 | Loss: 0.00001533
Iteration 119/1000 | Loss: 0.00001533
Iteration 120/1000 | Loss: 0.00001533
Iteration 121/1000 | Loss: 0.00001533
Iteration 122/1000 | Loss: 0.00001533
Iteration 123/1000 | Loss: 0.00001533
Iteration 124/1000 | Loss: 0.00001533
Iteration 125/1000 | Loss: 0.00001533
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001533
Iteration 128/1000 | Loss: 0.00001533
Iteration 129/1000 | Loss: 0.00001533
Iteration 130/1000 | Loss: 0.00001533
Iteration 131/1000 | Loss: 0.00001533
Iteration 132/1000 | Loss: 0.00001533
Iteration 133/1000 | Loss: 0.00001533
Iteration 134/1000 | Loss: 0.00001533
Iteration 135/1000 | Loss: 0.00001533
Iteration 136/1000 | Loss: 0.00001533
Iteration 137/1000 | Loss: 0.00001533
Iteration 138/1000 | Loss: 0.00001533
Iteration 139/1000 | Loss: 0.00001533
Iteration 140/1000 | Loss: 0.00001533
Iteration 141/1000 | Loss: 0.00001533
Iteration 142/1000 | Loss: 0.00001533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.533164868305903e-05, 1.533164868305903e-05, 1.533164868305903e-05, 1.533164868305903e-05, 1.533164868305903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.533164868305903e-05

Optimization complete. Final v2v error: 3.240602493286133 mm

Highest mean error: 9.046043395996094 mm for frame 133

Lowest mean error: 2.862769842147827 mm for frame 91

Saving results

Total time: 132.52871227264404
