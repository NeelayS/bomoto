Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=18, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1008-1063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800807
Iteration 2/25 | Loss: 0.00100482
Iteration 3/25 | Loss: 0.00080144
Iteration 4/25 | Loss: 0.00076027
Iteration 5/25 | Loss: 0.00075090
Iteration 6/25 | Loss: 0.00074956
Iteration 7/25 | Loss: 0.00074956
Iteration 8/25 | Loss: 0.00074956
Iteration 9/25 | Loss: 0.00074956
Iteration 10/25 | Loss: 0.00074956
Iteration 11/25 | Loss: 0.00074956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007495564641430974, 0.0007495564641430974, 0.0007495564641430974, 0.0007495564641430974, 0.0007495564641430974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007495564641430974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47612560
Iteration 2/25 | Loss: 0.00039306
Iteration 3/25 | Loss: 0.00039301
Iteration 4/25 | Loss: 0.00039301
Iteration 5/25 | Loss: 0.00039301
Iteration 6/25 | Loss: 0.00039301
Iteration 7/25 | Loss: 0.00039301
Iteration 8/25 | Loss: 0.00039301
Iteration 9/25 | Loss: 0.00039301
Iteration 10/25 | Loss: 0.00039301
Iteration 11/25 | Loss: 0.00039301
Iteration 12/25 | Loss: 0.00039301
Iteration 13/25 | Loss: 0.00039301
Iteration 14/25 | Loss: 0.00039301
Iteration 15/25 | Loss: 0.00039301
Iteration 16/25 | Loss: 0.00039301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00039301192737184465, 0.00039301192737184465, 0.00039301192737184465, 0.00039301192737184465, 0.00039301192737184465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039301192737184465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039301
Iteration 2/1000 | Loss: 0.00002848
Iteration 3/1000 | Loss: 0.00002318
Iteration 4/1000 | Loss: 0.00002155
Iteration 5/1000 | Loss: 0.00002015
Iteration 6/1000 | Loss: 0.00001926
Iteration 7/1000 | Loss: 0.00001863
Iteration 8/1000 | Loss: 0.00001830
Iteration 9/1000 | Loss: 0.00001805
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001781
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001768
Iteration 15/1000 | Loss: 0.00001764
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001760
Iteration 19/1000 | Loss: 0.00001753
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001750
Iteration 22/1000 | Loss: 0.00001750
Iteration 23/1000 | Loss: 0.00001750
Iteration 24/1000 | Loss: 0.00001750
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001748
Iteration 30/1000 | Loss: 0.00001748
Iteration 31/1000 | Loss: 0.00001748
Iteration 32/1000 | Loss: 0.00001747
Iteration 33/1000 | Loss: 0.00001747
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001747
Iteration 37/1000 | Loss: 0.00001747
Iteration 38/1000 | Loss: 0.00001747
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001746
Iteration 41/1000 | Loss: 0.00001746
Iteration 42/1000 | Loss: 0.00001746
Iteration 43/1000 | Loss: 0.00001745
Iteration 44/1000 | Loss: 0.00001745
Iteration 45/1000 | Loss: 0.00001745
Iteration 46/1000 | Loss: 0.00001745
Iteration 47/1000 | Loss: 0.00001745
Iteration 48/1000 | Loss: 0.00001744
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001744
Iteration 52/1000 | Loss: 0.00001744
Iteration 53/1000 | Loss: 0.00001744
Iteration 54/1000 | Loss: 0.00001744
Iteration 55/1000 | Loss: 0.00001744
Iteration 56/1000 | Loss: 0.00001743
Iteration 57/1000 | Loss: 0.00001743
Iteration 58/1000 | Loss: 0.00001743
Iteration 59/1000 | Loss: 0.00001743
Iteration 60/1000 | Loss: 0.00001743
Iteration 61/1000 | Loss: 0.00001743
Iteration 62/1000 | Loss: 0.00001743
Iteration 63/1000 | Loss: 0.00001743
Iteration 64/1000 | Loss: 0.00001743
Iteration 65/1000 | Loss: 0.00001743
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001742
Iteration 71/1000 | Loss: 0.00001742
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001742
Iteration 79/1000 | Loss: 0.00001742
Iteration 80/1000 | Loss: 0.00001742
Iteration 81/1000 | Loss: 0.00001742
Iteration 82/1000 | Loss: 0.00001742
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001741
Iteration 86/1000 | Loss: 0.00001741
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001741
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001741
Iteration 92/1000 | Loss: 0.00001741
Iteration 93/1000 | Loss: 0.00001741
Iteration 94/1000 | Loss: 0.00001741
Iteration 95/1000 | Loss: 0.00001741
Iteration 96/1000 | Loss: 0.00001741
Iteration 97/1000 | Loss: 0.00001741
Iteration 98/1000 | Loss: 0.00001741
Iteration 99/1000 | Loss: 0.00001741
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001741
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001740
Iteration 108/1000 | Loss: 0.00001740
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001738
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001737
Iteration 121/1000 | Loss: 0.00001737
Iteration 122/1000 | Loss: 0.00001737
Iteration 123/1000 | Loss: 0.00001737
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001736
Iteration 128/1000 | Loss: 0.00001736
Iteration 129/1000 | Loss: 0.00001736
Iteration 130/1000 | Loss: 0.00001736
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001736
Iteration 137/1000 | Loss: 0.00001736
Iteration 138/1000 | Loss: 0.00001736
Iteration 139/1000 | Loss: 0.00001736
Iteration 140/1000 | Loss: 0.00001736
Iteration 141/1000 | Loss: 0.00001736
Iteration 142/1000 | Loss: 0.00001736
Iteration 143/1000 | Loss: 0.00001736
Iteration 144/1000 | Loss: 0.00001736
Iteration 145/1000 | Loss: 0.00001736
Iteration 146/1000 | Loss: 0.00001736
Iteration 147/1000 | Loss: 0.00001736
Iteration 148/1000 | Loss: 0.00001736
Iteration 149/1000 | Loss: 0.00001735
Iteration 150/1000 | Loss: 0.00001735
Iteration 151/1000 | Loss: 0.00001735
Iteration 152/1000 | Loss: 0.00001735
Iteration 153/1000 | Loss: 0.00001735
Iteration 154/1000 | Loss: 0.00001735
Iteration 155/1000 | Loss: 0.00001735
Iteration 156/1000 | Loss: 0.00001735
Iteration 157/1000 | Loss: 0.00001735
Iteration 158/1000 | Loss: 0.00001735
Iteration 159/1000 | Loss: 0.00001735
Iteration 160/1000 | Loss: 0.00001735
Iteration 161/1000 | Loss: 0.00001735
Iteration 162/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.7350872440147214e-05, 1.7350872440147214e-05, 1.7350872440147214e-05, 1.7350872440147214e-05, 1.7350872440147214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7350872440147214e-05

Optimization complete. Final v2v error: 3.521604299545288 mm

Highest mean error: 3.837606430053711 mm for frame 221

Lowest mean error: 3.2376999855041504 mm for frame 60

Saving results

Total time: 42.115225076675415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962465
Iteration 2/25 | Loss: 0.00135941
Iteration 3/25 | Loss: 0.00102052
Iteration 4/25 | Loss: 0.00088383
Iteration 5/25 | Loss: 0.00085396
Iteration 6/25 | Loss: 0.00082399
Iteration 7/25 | Loss: 0.00081597
Iteration 8/25 | Loss: 0.00081891
Iteration 9/25 | Loss: 0.00082126
Iteration 10/25 | Loss: 0.00080631
Iteration 11/25 | Loss: 0.00080051
Iteration 12/25 | Loss: 0.00080100
Iteration 13/25 | Loss: 0.00079692
Iteration 14/25 | Loss: 0.00079159
Iteration 15/25 | Loss: 0.00078955
Iteration 16/25 | Loss: 0.00078916
Iteration 17/25 | Loss: 0.00078912
Iteration 18/25 | Loss: 0.00078912
Iteration 19/25 | Loss: 0.00078912
Iteration 20/25 | Loss: 0.00078911
Iteration 21/25 | Loss: 0.00078911
Iteration 22/25 | Loss: 0.00078911
Iteration 23/25 | Loss: 0.00078911
Iteration 24/25 | Loss: 0.00078911
Iteration 25/25 | Loss: 0.00078911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60581410
Iteration 2/25 | Loss: 0.00051916
Iteration 3/25 | Loss: 0.00049415
Iteration 4/25 | Loss: 0.00049415
Iteration 5/25 | Loss: 0.00049415
Iteration 6/25 | Loss: 0.00049415
Iteration 7/25 | Loss: 0.00049415
Iteration 8/25 | Loss: 0.00049415
Iteration 9/25 | Loss: 0.00049415
Iteration 10/25 | Loss: 0.00049415
Iteration 11/25 | Loss: 0.00049415
Iteration 12/25 | Loss: 0.00049415
Iteration 13/25 | Loss: 0.00049415
Iteration 14/25 | Loss: 0.00049415
Iteration 15/25 | Loss: 0.00049415
Iteration 16/25 | Loss: 0.00049415
Iteration 17/25 | Loss: 0.00049415
Iteration 18/25 | Loss: 0.00049415
Iteration 19/25 | Loss: 0.00049415
Iteration 20/25 | Loss: 0.00049415
Iteration 21/25 | Loss: 0.00049415
Iteration 22/25 | Loss: 0.00049415
Iteration 23/25 | Loss: 0.00049415
Iteration 24/25 | Loss: 0.00049415
Iteration 25/25 | Loss: 0.00049415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049415
Iteration 2/1000 | Loss: 0.00005950
Iteration 3/1000 | Loss: 0.00002865
Iteration 4/1000 | Loss: 0.00002355
Iteration 5/1000 | Loss: 0.00002224
Iteration 6/1000 | Loss: 0.00004454
Iteration 7/1000 | Loss: 0.00002080
Iteration 8/1000 | Loss: 0.00003966
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001980
Iteration 11/1000 | Loss: 0.00001956
Iteration 12/1000 | Loss: 0.00001942
Iteration 13/1000 | Loss: 0.00001924
Iteration 14/1000 | Loss: 0.00004226
Iteration 15/1000 | Loss: 0.00002126
Iteration 16/1000 | Loss: 0.00001980
Iteration 17/1000 | Loss: 0.00001912
Iteration 18/1000 | Loss: 0.00003195
Iteration 19/1000 | Loss: 0.00003195
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00002191
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001892
Iteration 25/1000 | Loss: 0.00001892
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00001891
Iteration 29/1000 | Loss: 0.00001891
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001891
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001891
Iteration 34/1000 | Loss: 0.00001891
Iteration 35/1000 | Loss: 0.00001891
Iteration 36/1000 | Loss: 0.00001891
Iteration 37/1000 | Loss: 0.00001890
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001890
Iteration 40/1000 | Loss: 0.00001889
Iteration 41/1000 | Loss: 0.00001889
Iteration 42/1000 | Loss: 0.00001887
Iteration 43/1000 | Loss: 0.00001885
Iteration 44/1000 | Loss: 0.00001885
Iteration 45/1000 | Loss: 0.00001885
Iteration 46/1000 | Loss: 0.00001885
Iteration 47/1000 | Loss: 0.00001884
Iteration 48/1000 | Loss: 0.00001884
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001882
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001879
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001879
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001877
Iteration 61/1000 | Loss: 0.00001877
Iteration 62/1000 | Loss: 0.00001876
Iteration 63/1000 | Loss: 0.00001876
Iteration 64/1000 | Loss: 0.00001876
Iteration 65/1000 | Loss: 0.00001875
Iteration 66/1000 | Loss: 0.00001875
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001874
Iteration 69/1000 | Loss: 0.00001874
Iteration 70/1000 | Loss: 0.00001874
Iteration 71/1000 | Loss: 0.00001873
Iteration 72/1000 | Loss: 0.00001873
Iteration 73/1000 | Loss: 0.00001873
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001872
Iteration 76/1000 | Loss: 0.00001872
Iteration 77/1000 | Loss: 0.00001872
Iteration 78/1000 | Loss: 0.00001872
Iteration 79/1000 | Loss: 0.00001872
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001872
Iteration 85/1000 | Loss: 0.00001872
Iteration 86/1000 | Loss: 0.00001872
Iteration 87/1000 | Loss: 0.00001871
Iteration 88/1000 | Loss: 0.00001871
Iteration 89/1000 | Loss: 0.00001871
Iteration 90/1000 | Loss: 0.00001871
Iteration 91/1000 | Loss: 0.00001871
Iteration 92/1000 | Loss: 0.00001871
Iteration 93/1000 | Loss: 0.00001871
Iteration 94/1000 | Loss: 0.00001871
Iteration 95/1000 | Loss: 0.00001871
Iteration 96/1000 | Loss: 0.00001870
Iteration 97/1000 | Loss: 0.00001870
Iteration 98/1000 | Loss: 0.00001870
Iteration 99/1000 | Loss: 0.00001870
Iteration 100/1000 | Loss: 0.00001870
Iteration 101/1000 | Loss: 0.00001870
Iteration 102/1000 | Loss: 0.00001869
Iteration 103/1000 | Loss: 0.00001869
Iteration 104/1000 | Loss: 0.00001869
Iteration 105/1000 | Loss: 0.00001869
Iteration 106/1000 | Loss: 0.00001869
Iteration 107/1000 | Loss: 0.00001869
Iteration 108/1000 | Loss: 0.00001869
Iteration 109/1000 | Loss: 0.00001869
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001867
Iteration 115/1000 | Loss: 0.00001867
Iteration 116/1000 | Loss: 0.00001867
Iteration 117/1000 | Loss: 0.00001866
Iteration 118/1000 | Loss: 0.00001866
Iteration 119/1000 | Loss: 0.00001866
Iteration 120/1000 | Loss: 0.00001866
Iteration 121/1000 | Loss: 0.00001866
Iteration 122/1000 | Loss: 0.00001865
Iteration 123/1000 | Loss: 0.00001865
Iteration 124/1000 | Loss: 0.00001865
Iteration 125/1000 | Loss: 0.00001865
Iteration 126/1000 | Loss: 0.00001865
Iteration 127/1000 | Loss: 0.00001865
Iteration 128/1000 | Loss: 0.00001865
Iteration 129/1000 | Loss: 0.00001865
Iteration 130/1000 | Loss: 0.00001865
Iteration 131/1000 | Loss: 0.00001865
Iteration 132/1000 | Loss: 0.00001865
Iteration 133/1000 | Loss: 0.00001865
Iteration 134/1000 | Loss: 0.00001864
Iteration 135/1000 | Loss: 0.00001864
Iteration 136/1000 | Loss: 0.00001864
Iteration 137/1000 | Loss: 0.00001864
Iteration 138/1000 | Loss: 0.00001864
Iteration 139/1000 | Loss: 0.00001864
Iteration 140/1000 | Loss: 0.00001864
Iteration 141/1000 | Loss: 0.00001864
Iteration 142/1000 | Loss: 0.00001864
Iteration 143/1000 | Loss: 0.00001863
Iteration 144/1000 | Loss: 0.00001863
Iteration 145/1000 | Loss: 0.00001863
Iteration 146/1000 | Loss: 0.00001863
Iteration 147/1000 | Loss: 0.00001863
Iteration 148/1000 | Loss: 0.00001863
Iteration 149/1000 | Loss: 0.00001863
Iteration 150/1000 | Loss: 0.00001863
Iteration 151/1000 | Loss: 0.00001862
Iteration 152/1000 | Loss: 0.00001862
Iteration 153/1000 | Loss: 0.00001862
Iteration 154/1000 | Loss: 0.00001862
Iteration 155/1000 | Loss: 0.00001862
Iteration 156/1000 | Loss: 0.00001861
Iteration 157/1000 | Loss: 0.00001861
Iteration 158/1000 | Loss: 0.00001861
Iteration 159/1000 | Loss: 0.00001861
Iteration 160/1000 | Loss: 0.00001861
Iteration 161/1000 | Loss: 0.00001861
Iteration 162/1000 | Loss: 0.00001861
Iteration 163/1000 | Loss: 0.00001861
Iteration 164/1000 | Loss: 0.00001861
Iteration 165/1000 | Loss: 0.00001861
Iteration 166/1000 | Loss: 0.00001861
Iteration 167/1000 | Loss: 0.00001861
Iteration 168/1000 | Loss: 0.00001861
Iteration 169/1000 | Loss: 0.00001861
Iteration 170/1000 | Loss: 0.00001861
Iteration 171/1000 | Loss: 0.00001860
Iteration 172/1000 | Loss: 0.00001860
Iteration 173/1000 | Loss: 0.00001860
Iteration 174/1000 | Loss: 0.00001860
Iteration 175/1000 | Loss: 0.00001860
Iteration 176/1000 | Loss: 0.00001860
Iteration 177/1000 | Loss: 0.00001860
Iteration 178/1000 | Loss: 0.00001860
Iteration 179/1000 | Loss: 0.00001860
Iteration 180/1000 | Loss: 0.00001860
Iteration 181/1000 | Loss: 0.00001860
Iteration 182/1000 | Loss: 0.00001860
Iteration 183/1000 | Loss: 0.00001860
Iteration 184/1000 | Loss: 0.00001860
Iteration 185/1000 | Loss: 0.00001860
Iteration 186/1000 | Loss: 0.00001860
Iteration 187/1000 | Loss: 0.00001860
Iteration 188/1000 | Loss: 0.00001860
Iteration 189/1000 | Loss: 0.00001860
Iteration 190/1000 | Loss: 0.00001860
Iteration 191/1000 | Loss: 0.00001860
Iteration 192/1000 | Loss: 0.00001860
Iteration 193/1000 | Loss: 0.00001860
Iteration 194/1000 | Loss: 0.00001860
Iteration 195/1000 | Loss: 0.00001860
Iteration 196/1000 | Loss: 0.00001860
Iteration 197/1000 | Loss: 0.00001860
Iteration 198/1000 | Loss: 0.00001860
Iteration 199/1000 | Loss: 0.00001860
Iteration 200/1000 | Loss: 0.00001860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.8598741007735953e-05, 1.8598741007735953e-05, 1.8598741007735953e-05, 1.8598741007735953e-05, 1.8598741007735953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8598741007735953e-05

Optimization complete. Final v2v error: 3.6435649394989014 mm

Highest mean error: 4.696319103240967 mm for frame 39

Lowest mean error: 3.0682930946350098 mm for frame 200

Saving results

Total time: 71.50127458572388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01216695
Iteration 2/25 | Loss: 0.00403798
Iteration 3/25 | Loss: 0.00227513
Iteration 4/25 | Loss: 0.00206570
Iteration 5/25 | Loss: 0.00183862
Iteration 6/25 | Loss: 0.00142751
Iteration 7/25 | Loss: 0.00131306
Iteration 8/25 | Loss: 0.00124406
Iteration 9/25 | Loss: 0.00122848
Iteration 10/25 | Loss: 0.00122716
Iteration 11/25 | Loss: 0.00120922
Iteration 12/25 | Loss: 0.00120524
Iteration 13/25 | Loss: 0.00120454
Iteration 14/25 | Loss: 0.00119840
Iteration 15/25 | Loss: 0.00118798
Iteration 16/25 | Loss: 0.00118863
Iteration 17/25 | Loss: 0.00118256
Iteration 18/25 | Loss: 0.00117596
Iteration 19/25 | Loss: 0.00117160
Iteration 20/25 | Loss: 0.00117169
Iteration 21/25 | Loss: 0.00116824
Iteration 22/25 | Loss: 0.00116595
Iteration 23/25 | Loss: 0.00116403
Iteration 24/25 | Loss: 0.00116145
Iteration 25/25 | Loss: 0.00116113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70768416
Iteration 2/25 | Loss: 0.00078928
Iteration 3/25 | Loss: 0.00078927
Iteration 4/25 | Loss: 0.00078927
Iteration 5/25 | Loss: 0.00078927
Iteration 6/25 | Loss: 0.00078927
Iteration 7/25 | Loss: 0.00078927
Iteration 8/25 | Loss: 0.00078927
Iteration 9/25 | Loss: 0.00078927
Iteration 10/25 | Loss: 0.00078927
Iteration 11/25 | Loss: 0.00078927
Iteration 12/25 | Loss: 0.00078927
Iteration 13/25 | Loss: 0.00078927
Iteration 14/25 | Loss: 0.00078927
Iteration 15/25 | Loss: 0.00078927
Iteration 16/25 | Loss: 0.00078927
Iteration 17/25 | Loss: 0.00078927
Iteration 18/25 | Loss: 0.00078927
Iteration 19/25 | Loss: 0.00078927
Iteration 20/25 | Loss: 0.00078927
Iteration 21/25 | Loss: 0.00078927
Iteration 22/25 | Loss: 0.00078927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007892689318396151, 0.0007892689318396151, 0.0007892689318396151, 0.0007892689318396151, 0.0007892689318396151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007892689318396151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078927
Iteration 2/1000 | Loss: 0.00012633
Iteration 3/1000 | Loss: 0.00007521
Iteration 4/1000 | Loss: 0.00005989
Iteration 5/1000 | Loss: 0.00005309
Iteration 6/1000 | Loss: 0.00005039
Iteration 7/1000 | Loss: 0.00004852
Iteration 8/1000 | Loss: 0.00004723
Iteration 9/1000 | Loss: 0.00004655
Iteration 10/1000 | Loss: 0.00004597
Iteration 11/1000 | Loss: 0.00004558
Iteration 12/1000 | Loss: 0.00004507
Iteration 13/1000 | Loss: 0.00004467
Iteration 14/1000 | Loss: 0.00004439
Iteration 15/1000 | Loss: 0.00004411
Iteration 16/1000 | Loss: 0.00004396
Iteration 17/1000 | Loss: 0.00004395
Iteration 18/1000 | Loss: 0.00004395
Iteration 19/1000 | Loss: 0.00004394
Iteration 20/1000 | Loss: 0.00004394
Iteration 21/1000 | Loss: 0.00004391
Iteration 22/1000 | Loss: 0.00004385
Iteration 23/1000 | Loss: 0.00004385
Iteration 24/1000 | Loss: 0.00004383
Iteration 25/1000 | Loss: 0.00004382
Iteration 26/1000 | Loss: 0.00004381
Iteration 27/1000 | Loss: 0.00004380
Iteration 28/1000 | Loss: 0.00004380
Iteration 29/1000 | Loss: 0.00004380
Iteration 30/1000 | Loss: 0.00004379
Iteration 31/1000 | Loss: 0.00004379
Iteration 32/1000 | Loss: 0.00004378
Iteration 33/1000 | Loss: 0.00004378
Iteration 34/1000 | Loss: 0.00004378
Iteration 35/1000 | Loss: 0.00004377
Iteration 36/1000 | Loss: 0.00004377
Iteration 37/1000 | Loss: 0.00004376
Iteration 38/1000 | Loss: 0.00004376
Iteration 39/1000 | Loss: 0.00004376
Iteration 40/1000 | Loss: 0.00004376
Iteration 41/1000 | Loss: 0.00004376
Iteration 42/1000 | Loss: 0.00004376
Iteration 43/1000 | Loss: 0.00004376
Iteration 44/1000 | Loss: 0.00004375
Iteration 45/1000 | Loss: 0.00004375
Iteration 46/1000 | Loss: 0.00004375
Iteration 47/1000 | Loss: 0.00004373
Iteration 48/1000 | Loss: 0.00004373
Iteration 49/1000 | Loss: 0.00004372
Iteration 50/1000 | Loss: 0.00004372
Iteration 51/1000 | Loss: 0.00004372
Iteration 52/1000 | Loss: 0.00004372
Iteration 53/1000 | Loss: 0.00004372
Iteration 54/1000 | Loss: 0.00004372
Iteration 55/1000 | Loss: 0.00004372
Iteration 56/1000 | Loss: 0.00004371
Iteration 57/1000 | Loss: 0.00004371
Iteration 58/1000 | Loss: 0.00004371
Iteration 59/1000 | Loss: 0.00004371
Iteration 60/1000 | Loss: 0.00004371
Iteration 61/1000 | Loss: 0.00004370
Iteration 62/1000 | Loss: 0.00004370
Iteration 63/1000 | Loss: 0.00004370
Iteration 64/1000 | Loss: 0.00004369
Iteration 65/1000 | Loss: 0.00004369
Iteration 66/1000 | Loss: 0.00004369
Iteration 67/1000 | Loss: 0.00004369
Iteration 68/1000 | Loss: 0.00004369
Iteration 69/1000 | Loss: 0.00004369
Iteration 70/1000 | Loss: 0.00004369
Iteration 71/1000 | Loss: 0.00004369
Iteration 72/1000 | Loss: 0.00004369
Iteration 73/1000 | Loss: 0.00004368
Iteration 74/1000 | Loss: 0.00004368
Iteration 75/1000 | Loss: 0.00004368
Iteration 76/1000 | Loss: 0.00004368
Iteration 77/1000 | Loss: 0.00004368
Iteration 78/1000 | Loss: 0.00004368
Iteration 79/1000 | Loss: 0.00004368
Iteration 80/1000 | Loss: 0.00004368
Iteration 81/1000 | Loss: 0.00004368
Iteration 82/1000 | Loss: 0.00004368
Iteration 83/1000 | Loss: 0.00004367
Iteration 84/1000 | Loss: 0.00004367
Iteration 85/1000 | Loss: 0.00004367
Iteration 86/1000 | Loss: 0.00004367
Iteration 87/1000 | Loss: 0.00004367
Iteration 88/1000 | Loss: 0.00004367
Iteration 89/1000 | Loss: 0.00004367
Iteration 90/1000 | Loss: 0.00004367
Iteration 91/1000 | Loss: 0.00004366
Iteration 92/1000 | Loss: 0.00004366
Iteration 93/1000 | Loss: 0.00004366
Iteration 94/1000 | Loss: 0.00004365
Iteration 95/1000 | Loss: 0.00004365
Iteration 96/1000 | Loss: 0.00004365
Iteration 97/1000 | Loss: 0.00004365
Iteration 98/1000 | Loss: 0.00004365
Iteration 99/1000 | Loss: 0.00004365
Iteration 100/1000 | Loss: 0.00004365
Iteration 101/1000 | Loss: 0.00004365
Iteration 102/1000 | Loss: 0.00004365
Iteration 103/1000 | Loss: 0.00004364
Iteration 104/1000 | Loss: 0.00004364
Iteration 105/1000 | Loss: 0.00004364
Iteration 106/1000 | Loss: 0.00004364
Iteration 107/1000 | Loss: 0.00004364
Iteration 108/1000 | Loss: 0.00004364
Iteration 109/1000 | Loss: 0.00004364
Iteration 110/1000 | Loss: 0.00004364
Iteration 111/1000 | Loss: 0.00004364
Iteration 112/1000 | Loss: 0.00004364
Iteration 113/1000 | Loss: 0.00004364
Iteration 114/1000 | Loss: 0.00004364
Iteration 115/1000 | Loss: 0.00004364
Iteration 116/1000 | Loss: 0.00004364
Iteration 117/1000 | Loss: 0.00004363
Iteration 118/1000 | Loss: 0.00004363
Iteration 119/1000 | Loss: 0.00004363
Iteration 120/1000 | Loss: 0.00004363
Iteration 121/1000 | Loss: 0.00004363
Iteration 122/1000 | Loss: 0.00004363
Iteration 123/1000 | Loss: 0.00004363
Iteration 124/1000 | Loss: 0.00004363
Iteration 125/1000 | Loss: 0.00004362
Iteration 126/1000 | Loss: 0.00004362
Iteration 127/1000 | Loss: 0.00004362
Iteration 128/1000 | Loss: 0.00004362
Iteration 129/1000 | Loss: 0.00004362
Iteration 130/1000 | Loss: 0.00004362
Iteration 131/1000 | Loss: 0.00004362
Iteration 132/1000 | Loss: 0.00004362
Iteration 133/1000 | Loss: 0.00004362
Iteration 134/1000 | Loss: 0.00004362
Iteration 135/1000 | Loss: 0.00004362
Iteration 136/1000 | Loss: 0.00004362
Iteration 137/1000 | Loss: 0.00004362
Iteration 138/1000 | Loss: 0.00004361
Iteration 139/1000 | Loss: 0.00004361
Iteration 140/1000 | Loss: 0.00004361
Iteration 141/1000 | Loss: 0.00004361
Iteration 142/1000 | Loss: 0.00004361
Iteration 143/1000 | Loss: 0.00004361
Iteration 144/1000 | Loss: 0.00004360
Iteration 145/1000 | Loss: 0.00004360
Iteration 146/1000 | Loss: 0.00004360
Iteration 147/1000 | Loss: 0.00004360
Iteration 148/1000 | Loss: 0.00004360
Iteration 149/1000 | Loss: 0.00004360
Iteration 150/1000 | Loss: 0.00004359
Iteration 151/1000 | Loss: 0.00004359
Iteration 152/1000 | Loss: 0.00004359
Iteration 153/1000 | Loss: 0.00004359
Iteration 154/1000 | Loss: 0.00004359
Iteration 155/1000 | Loss: 0.00004359
Iteration 156/1000 | Loss: 0.00004359
Iteration 157/1000 | Loss: 0.00004359
Iteration 158/1000 | Loss: 0.00004358
Iteration 159/1000 | Loss: 0.00004358
Iteration 160/1000 | Loss: 0.00004358
Iteration 161/1000 | Loss: 0.00004358
Iteration 162/1000 | Loss: 0.00004358
Iteration 163/1000 | Loss: 0.00004358
Iteration 164/1000 | Loss: 0.00004358
Iteration 165/1000 | Loss: 0.00004358
Iteration 166/1000 | Loss: 0.00004358
Iteration 167/1000 | Loss: 0.00004358
Iteration 168/1000 | Loss: 0.00004358
Iteration 169/1000 | Loss: 0.00004357
Iteration 170/1000 | Loss: 0.00004357
Iteration 171/1000 | Loss: 0.00004357
Iteration 172/1000 | Loss: 0.00004357
Iteration 173/1000 | Loss: 0.00004357
Iteration 174/1000 | Loss: 0.00004357
Iteration 175/1000 | Loss: 0.00004357
Iteration 176/1000 | Loss: 0.00004357
Iteration 177/1000 | Loss: 0.00004357
Iteration 178/1000 | Loss: 0.00004357
Iteration 179/1000 | Loss: 0.00004357
Iteration 180/1000 | Loss: 0.00004357
Iteration 181/1000 | Loss: 0.00004357
Iteration 182/1000 | Loss: 0.00004357
Iteration 183/1000 | Loss: 0.00004357
Iteration 184/1000 | Loss: 0.00004357
Iteration 185/1000 | Loss: 0.00004357
Iteration 186/1000 | Loss: 0.00004356
Iteration 187/1000 | Loss: 0.00004356
Iteration 188/1000 | Loss: 0.00004356
Iteration 189/1000 | Loss: 0.00004356
Iteration 190/1000 | Loss: 0.00004356
Iteration 191/1000 | Loss: 0.00004355
Iteration 192/1000 | Loss: 0.00004355
Iteration 193/1000 | Loss: 0.00004355
Iteration 194/1000 | Loss: 0.00004355
Iteration 195/1000 | Loss: 0.00004355
Iteration 196/1000 | Loss: 0.00004355
Iteration 197/1000 | Loss: 0.00004355
Iteration 198/1000 | Loss: 0.00004354
Iteration 199/1000 | Loss: 0.00004354
Iteration 200/1000 | Loss: 0.00004354
Iteration 201/1000 | Loss: 0.00004354
Iteration 202/1000 | Loss: 0.00004354
Iteration 203/1000 | Loss: 0.00004354
Iteration 204/1000 | Loss: 0.00004354
Iteration 205/1000 | Loss: 0.00004354
Iteration 206/1000 | Loss: 0.00004354
Iteration 207/1000 | Loss: 0.00004354
Iteration 208/1000 | Loss: 0.00004354
Iteration 209/1000 | Loss: 0.00004354
Iteration 210/1000 | Loss: 0.00004354
Iteration 211/1000 | Loss: 0.00004354
Iteration 212/1000 | Loss: 0.00004354
Iteration 213/1000 | Loss: 0.00004354
Iteration 214/1000 | Loss: 0.00004354
Iteration 215/1000 | Loss: 0.00004354
Iteration 216/1000 | Loss: 0.00004354
Iteration 217/1000 | Loss: 0.00004354
Iteration 218/1000 | Loss: 0.00004354
Iteration 219/1000 | Loss: 0.00004354
Iteration 220/1000 | Loss: 0.00004354
Iteration 221/1000 | Loss: 0.00004354
Iteration 222/1000 | Loss: 0.00004354
Iteration 223/1000 | Loss: 0.00004354
Iteration 224/1000 | Loss: 0.00004354
Iteration 225/1000 | Loss: 0.00004354
Iteration 226/1000 | Loss: 0.00004354
Iteration 227/1000 | Loss: 0.00004354
Iteration 228/1000 | Loss: 0.00004354
Iteration 229/1000 | Loss: 0.00004353
Iteration 230/1000 | Loss: 0.00004353
Iteration 231/1000 | Loss: 0.00004353
Iteration 232/1000 | Loss: 0.00004353
Iteration 233/1000 | Loss: 0.00004353
Iteration 234/1000 | Loss: 0.00004353
Iteration 235/1000 | Loss: 0.00004353
Iteration 236/1000 | Loss: 0.00004353
Iteration 237/1000 | Loss: 0.00004353
Iteration 238/1000 | Loss: 0.00004353
Iteration 239/1000 | Loss: 0.00004353
Iteration 240/1000 | Loss: 0.00004353
Iteration 241/1000 | Loss: 0.00004353
Iteration 242/1000 | Loss: 0.00004353
Iteration 243/1000 | Loss: 0.00004353
Iteration 244/1000 | Loss: 0.00004353
Iteration 245/1000 | Loss: 0.00004353
Iteration 246/1000 | Loss: 0.00004353
Iteration 247/1000 | Loss: 0.00004353
Iteration 248/1000 | Loss: 0.00004353
Iteration 249/1000 | Loss: 0.00004353
Iteration 250/1000 | Loss: 0.00004353
Iteration 251/1000 | Loss: 0.00004353
Iteration 252/1000 | Loss: 0.00004353
Iteration 253/1000 | Loss: 0.00004353
Iteration 254/1000 | Loss: 0.00004353
Iteration 255/1000 | Loss: 0.00004353
Iteration 256/1000 | Loss: 0.00004353
Iteration 257/1000 | Loss: 0.00004353
Iteration 258/1000 | Loss: 0.00004353
Iteration 259/1000 | Loss: 0.00004353
Iteration 260/1000 | Loss: 0.00004353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [4.3532982090255246e-05, 4.3532982090255246e-05, 4.3532982090255246e-05, 4.3532982090255246e-05, 4.3532982090255246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.3532982090255246e-05

Optimization complete. Final v2v error: 5.348862171173096 mm

Highest mean error: 5.957332611083984 mm for frame 8

Lowest mean error: 4.806694507598877 mm for frame 20

Saving results

Total time: 81.25764679908752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897699
Iteration 2/25 | Loss: 0.00238406
Iteration 3/25 | Loss: 0.00154090
Iteration 4/25 | Loss: 0.00129430
Iteration 5/25 | Loss: 0.00109394
Iteration 6/25 | Loss: 0.00103663
Iteration 7/25 | Loss: 0.00099909
Iteration 8/25 | Loss: 0.00097885
Iteration 9/25 | Loss: 0.00095975
Iteration 10/25 | Loss: 0.00089988
Iteration 11/25 | Loss: 0.00088301
Iteration 12/25 | Loss: 0.00086263
Iteration 13/25 | Loss: 0.00085476
Iteration 14/25 | Loss: 0.00084988
Iteration 15/25 | Loss: 0.00084157
Iteration 16/25 | Loss: 0.00083192
Iteration 17/25 | Loss: 0.00082622
Iteration 18/25 | Loss: 0.00082496
Iteration 19/25 | Loss: 0.00082458
Iteration 20/25 | Loss: 0.00082434
Iteration 21/25 | Loss: 0.00082415
Iteration 22/25 | Loss: 0.00082375
Iteration 23/25 | Loss: 0.00082339
Iteration 24/25 | Loss: 0.00082318
Iteration 25/25 | Loss: 0.00082672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59789515
Iteration 2/25 | Loss: 0.00113465
Iteration 3/25 | Loss: 0.00113465
Iteration 4/25 | Loss: 0.00113465
Iteration 5/25 | Loss: 0.00113465
Iteration 6/25 | Loss: 0.00113465
Iteration 7/25 | Loss: 0.00113465
Iteration 8/25 | Loss: 0.00113465
Iteration 9/25 | Loss: 0.00113465
Iteration 10/25 | Loss: 0.00113465
Iteration 11/25 | Loss: 0.00113465
Iteration 12/25 | Loss: 0.00113465
Iteration 13/25 | Loss: 0.00113465
Iteration 14/25 | Loss: 0.00113465
Iteration 15/25 | Loss: 0.00113465
Iteration 16/25 | Loss: 0.00113465
Iteration 17/25 | Loss: 0.00113465
Iteration 18/25 | Loss: 0.00113465
Iteration 19/25 | Loss: 0.00113465
Iteration 20/25 | Loss: 0.00113465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011346469400450587, 0.0011346469400450587, 0.0011346469400450587, 0.0011346469400450587, 0.0011346469400450587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011346469400450587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113465
Iteration 2/1000 | Loss: 0.00012109
Iteration 3/1000 | Loss: 0.00008769
Iteration 4/1000 | Loss: 0.00007514
Iteration 5/1000 | Loss: 0.00018126
Iteration 6/1000 | Loss: 0.00006872
Iteration 7/1000 | Loss: 0.00017575
Iteration 8/1000 | Loss: 0.00034669
Iteration 9/1000 | Loss: 0.00047720
Iteration 10/1000 | Loss: 0.00030723
Iteration 11/1000 | Loss: 0.00014060
Iteration 12/1000 | Loss: 0.00005548
Iteration 13/1000 | Loss: 0.00048678
Iteration 14/1000 | Loss: 0.00005989
Iteration 15/1000 | Loss: 0.00010894
Iteration 16/1000 | Loss: 0.00003963
Iteration 17/1000 | Loss: 0.00003471
Iteration 18/1000 | Loss: 0.00003102
Iteration 19/1000 | Loss: 0.00002880
Iteration 20/1000 | Loss: 0.00002718
Iteration 21/1000 | Loss: 0.00002627
Iteration 22/1000 | Loss: 0.00035533
Iteration 23/1000 | Loss: 0.00027302
Iteration 24/1000 | Loss: 0.00033570
Iteration 25/1000 | Loss: 0.00003055
Iteration 26/1000 | Loss: 0.00002539
Iteration 27/1000 | Loss: 0.00002396
Iteration 28/1000 | Loss: 0.00002313
Iteration 29/1000 | Loss: 0.00002271
Iteration 30/1000 | Loss: 0.00002246
Iteration 31/1000 | Loss: 0.00002231
Iteration 32/1000 | Loss: 0.00002227
Iteration 33/1000 | Loss: 0.00002211
Iteration 34/1000 | Loss: 0.00002211
Iteration 35/1000 | Loss: 0.00002209
Iteration 36/1000 | Loss: 0.00002191
Iteration 37/1000 | Loss: 0.00002171
Iteration 38/1000 | Loss: 0.00002155
Iteration 39/1000 | Loss: 0.00002143
Iteration 40/1000 | Loss: 0.00002135
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002099
Iteration 43/1000 | Loss: 0.00002089
Iteration 44/1000 | Loss: 0.00002086
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002077
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002075
Iteration 50/1000 | Loss: 0.00002075
Iteration 51/1000 | Loss: 0.00002074
Iteration 52/1000 | Loss: 0.00002074
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002071
Iteration 57/1000 | Loss: 0.00002071
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002071
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002070
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002068
Iteration 66/1000 | Loss: 0.00002068
Iteration 67/1000 | Loss: 0.00002067
Iteration 68/1000 | Loss: 0.00002067
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002066
Iteration 71/1000 | Loss: 0.00002066
Iteration 72/1000 | Loss: 0.00002066
Iteration 73/1000 | Loss: 0.00002065
Iteration 74/1000 | Loss: 0.00002065
Iteration 75/1000 | Loss: 0.00002064
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002057
Iteration 79/1000 | Loss: 0.00002057
Iteration 80/1000 | Loss: 0.00002056
Iteration 81/1000 | Loss: 0.00002056
Iteration 82/1000 | Loss: 0.00002056
Iteration 83/1000 | Loss: 0.00002055
Iteration 84/1000 | Loss: 0.00002055
Iteration 85/1000 | Loss: 0.00002055
Iteration 86/1000 | Loss: 0.00002052
Iteration 87/1000 | Loss: 0.00002052
Iteration 88/1000 | Loss: 0.00002052
Iteration 89/1000 | Loss: 0.00002052
Iteration 90/1000 | Loss: 0.00002051
Iteration 91/1000 | Loss: 0.00002051
Iteration 92/1000 | Loss: 0.00002051
Iteration 93/1000 | Loss: 0.00002050
Iteration 94/1000 | Loss: 0.00002050
Iteration 95/1000 | Loss: 0.00002050
Iteration 96/1000 | Loss: 0.00002050
Iteration 97/1000 | Loss: 0.00002050
Iteration 98/1000 | Loss: 0.00002049
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00002048
Iteration 101/1000 | Loss: 0.00002048
Iteration 102/1000 | Loss: 0.00002048
Iteration 103/1000 | Loss: 0.00002048
Iteration 104/1000 | Loss: 0.00002047
Iteration 105/1000 | Loss: 0.00002047
Iteration 106/1000 | Loss: 0.00002047
Iteration 107/1000 | Loss: 0.00002047
Iteration 108/1000 | Loss: 0.00002047
Iteration 109/1000 | Loss: 0.00002047
Iteration 110/1000 | Loss: 0.00002047
Iteration 111/1000 | Loss: 0.00002047
Iteration 112/1000 | Loss: 0.00002045
Iteration 113/1000 | Loss: 0.00002042
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002035
Iteration 116/1000 | Loss: 0.00002027
Iteration 117/1000 | Loss: 0.00002023
Iteration 118/1000 | Loss: 0.00002002
Iteration 119/1000 | Loss: 0.00001982
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001978
Iteration 122/1000 | Loss: 0.00001976
Iteration 123/1000 | Loss: 0.00001975
Iteration 124/1000 | Loss: 0.00001970
Iteration 125/1000 | Loss: 0.00001968
Iteration 126/1000 | Loss: 0.00001967
Iteration 127/1000 | Loss: 0.00001966
Iteration 128/1000 | Loss: 0.00001966
Iteration 129/1000 | Loss: 0.00001965
Iteration 130/1000 | Loss: 0.00001964
Iteration 131/1000 | Loss: 0.00001964
Iteration 132/1000 | Loss: 0.00001963
Iteration 133/1000 | Loss: 0.00001962
Iteration 134/1000 | Loss: 0.00001962
Iteration 135/1000 | Loss: 0.00001962
Iteration 136/1000 | Loss: 0.00001961
Iteration 137/1000 | Loss: 0.00001961
Iteration 138/1000 | Loss: 0.00001960
Iteration 139/1000 | Loss: 0.00001960
Iteration 140/1000 | Loss: 0.00001959
Iteration 141/1000 | Loss: 0.00001957
Iteration 142/1000 | Loss: 0.00001952
Iteration 143/1000 | Loss: 0.00001951
Iteration 144/1000 | Loss: 0.00001951
Iteration 145/1000 | Loss: 0.00001951
Iteration 146/1000 | Loss: 0.00001951
Iteration 147/1000 | Loss: 0.00001951
Iteration 148/1000 | Loss: 0.00001951
Iteration 149/1000 | Loss: 0.00001951
Iteration 150/1000 | Loss: 0.00001950
Iteration 151/1000 | Loss: 0.00001950
Iteration 152/1000 | Loss: 0.00001949
Iteration 153/1000 | Loss: 0.00001949
Iteration 154/1000 | Loss: 0.00001949
Iteration 155/1000 | Loss: 0.00001949
Iteration 156/1000 | Loss: 0.00001948
Iteration 157/1000 | Loss: 0.00001948
Iteration 158/1000 | Loss: 0.00001948
Iteration 159/1000 | Loss: 0.00001948
Iteration 160/1000 | Loss: 0.00001948
Iteration 161/1000 | Loss: 0.00001947
Iteration 162/1000 | Loss: 0.00001947
Iteration 163/1000 | Loss: 0.00001947
Iteration 164/1000 | Loss: 0.00001947
Iteration 165/1000 | Loss: 0.00001946
Iteration 166/1000 | Loss: 0.00001946
Iteration 167/1000 | Loss: 0.00001946
Iteration 168/1000 | Loss: 0.00001946
Iteration 169/1000 | Loss: 0.00001946
Iteration 170/1000 | Loss: 0.00001946
Iteration 171/1000 | Loss: 0.00001946
Iteration 172/1000 | Loss: 0.00001946
Iteration 173/1000 | Loss: 0.00001945
Iteration 174/1000 | Loss: 0.00001945
Iteration 175/1000 | Loss: 0.00001945
Iteration 176/1000 | Loss: 0.00001945
Iteration 177/1000 | Loss: 0.00001945
Iteration 178/1000 | Loss: 0.00001945
Iteration 179/1000 | Loss: 0.00001945
Iteration 180/1000 | Loss: 0.00001945
Iteration 181/1000 | Loss: 0.00001945
Iteration 182/1000 | Loss: 0.00001945
Iteration 183/1000 | Loss: 0.00001945
Iteration 184/1000 | Loss: 0.00001945
Iteration 185/1000 | Loss: 0.00001944
Iteration 186/1000 | Loss: 0.00001944
Iteration 187/1000 | Loss: 0.00001944
Iteration 188/1000 | Loss: 0.00001944
Iteration 189/1000 | Loss: 0.00001944
Iteration 190/1000 | Loss: 0.00001944
Iteration 191/1000 | Loss: 0.00001944
Iteration 192/1000 | Loss: 0.00001944
Iteration 193/1000 | Loss: 0.00001944
Iteration 194/1000 | Loss: 0.00001944
Iteration 195/1000 | Loss: 0.00001943
Iteration 196/1000 | Loss: 0.00001943
Iteration 197/1000 | Loss: 0.00001943
Iteration 198/1000 | Loss: 0.00001943
Iteration 199/1000 | Loss: 0.00001943
Iteration 200/1000 | Loss: 0.00001943
Iteration 201/1000 | Loss: 0.00001942
Iteration 202/1000 | Loss: 0.00001942
Iteration 203/1000 | Loss: 0.00001942
Iteration 204/1000 | Loss: 0.00001942
Iteration 205/1000 | Loss: 0.00001942
Iteration 206/1000 | Loss: 0.00001942
Iteration 207/1000 | Loss: 0.00001942
Iteration 208/1000 | Loss: 0.00001942
Iteration 209/1000 | Loss: 0.00001942
Iteration 210/1000 | Loss: 0.00001941
Iteration 211/1000 | Loss: 0.00001941
Iteration 212/1000 | Loss: 0.00001941
Iteration 213/1000 | Loss: 0.00001941
Iteration 214/1000 | Loss: 0.00001941
Iteration 215/1000 | Loss: 0.00001941
Iteration 216/1000 | Loss: 0.00001941
Iteration 217/1000 | Loss: 0.00016479
Iteration 218/1000 | Loss: 0.00002187
Iteration 219/1000 | Loss: 0.00002009
Iteration 220/1000 | Loss: 0.00001930
Iteration 221/1000 | Loss: 0.00001864
Iteration 222/1000 | Loss: 0.00001809
Iteration 223/1000 | Loss: 0.00001798
Iteration 224/1000 | Loss: 0.00001793
Iteration 225/1000 | Loss: 0.00001788
Iteration 226/1000 | Loss: 0.00001785
Iteration 227/1000 | Loss: 0.00001784
Iteration 228/1000 | Loss: 0.00001784
Iteration 229/1000 | Loss: 0.00001780
Iteration 230/1000 | Loss: 0.00001779
Iteration 231/1000 | Loss: 0.00001778
Iteration 232/1000 | Loss: 0.00001778
Iteration 233/1000 | Loss: 0.00001778
Iteration 234/1000 | Loss: 0.00001777
Iteration 235/1000 | Loss: 0.00001777
Iteration 236/1000 | Loss: 0.00001776
Iteration 237/1000 | Loss: 0.00001776
Iteration 238/1000 | Loss: 0.00001775
Iteration 239/1000 | Loss: 0.00001775
Iteration 240/1000 | Loss: 0.00001775
Iteration 241/1000 | Loss: 0.00001775
Iteration 242/1000 | Loss: 0.00001775
Iteration 243/1000 | Loss: 0.00001774
Iteration 244/1000 | Loss: 0.00001774
Iteration 245/1000 | Loss: 0.00001774
Iteration 246/1000 | Loss: 0.00001773
Iteration 247/1000 | Loss: 0.00001773
Iteration 248/1000 | Loss: 0.00001773
Iteration 249/1000 | Loss: 0.00001772
Iteration 250/1000 | Loss: 0.00001772
Iteration 251/1000 | Loss: 0.00001772
Iteration 252/1000 | Loss: 0.00001771
Iteration 253/1000 | Loss: 0.00001771
Iteration 254/1000 | Loss: 0.00001771
Iteration 255/1000 | Loss: 0.00001771
Iteration 256/1000 | Loss: 0.00001771
Iteration 257/1000 | Loss: 0.00001771
Iteration 258/1000 | Loss: 0.00001770
Iteration 259/1000 | Loss: 0.00001770
Iteration 260/1000 | Loss: 0.00001770
Iteration 261/1000 | Loss: 0.00001770
Iteration 262/1000 | Loss: 0.00001770
Iteration 263/1000 | Loss: 0.00001770
Iteration 264/1000 | Loss: 0.00001770
Iteration 265/1000 | Loss: 0.00001770
Iteration 266/1000 | Loss: 0.00001770
Iteration 267/1000 | Loss: 0.00001770
Iteration 268/1000 | Loss: 0.00001770
Iteration 269/1000 | Loss: 0.00001769
Iteration 270/1000 | Loss: 0.00001769
Iteration 271/1000 | Loss: 0.00001769
Iteration 272/1000 | Loss: 0.00001769
Iteration 273/1000 | Loss: 0.00001769
Iteration 274/1000 | Loss: 0.00001769
Iteration 275/1000 | Loss: 0.00001769
Iteration 276/1000 | Loss: 0.00001769
Iteration 277/1000 | Loss: 0.00001769
Iteration 278/1000 | Loss: 0.00001769
Iteration 279/1000 | Loss: 0.00001769
Iteration 280/1000 | Loss: 0.00001769
Iteration 281/1000 | Loss: 0.00001768
Iteration 282/1000 | Loss: 0.00001768
Iteration 283/1000 | Loss: 0.00001768
Iteration 284/1000 | Loss: 0.00001768
Iteration 285/1000 | Loss: 0.00001768
Iteration 286/1000 | Loss: 0.00001768
Iteration 287/1000 | Loss: 0.00001768
Iteration 288/1000 | Loss: 0.00001768
Iteration 289/1000 | Loss: 0.00001768
Iteration 290/1000 | Loss: 0.00001768
Iteration 291/1000 | Loss: 0.00001768
Iteration 292/1000 | Loss: 0.00001768
Iteration 293/1000 | Loss: 0.00001768
Iteration 294/1000 | Loss: 0.00001768
Iteration 295/1000 | Loss: 0.00001767
Iteration 296/1000 | Loss: 0.00001767
Iteration 297/1000 | Loss: 0.00001767
Iteration 298/1000 | Loss: 0.00001767
Iteration 299/1000 | Loss: 0.00001767
Iteration 300/1000 | Loss: 0.00001767
Iteration 301/1000 | Loss: 0.00001767
Iteration 302/1000 | Loss: 0.00001767
Iteration 303/1000 | Loss: 0.00001767
Iteration 304/1000 | Loss: 0.00001767
Iteration 305/1000 | Loss: 0.00001767
Iteration 306/1000 | Loss: 0.00001767
Iteration 307/1000 | Loss: 0.00001767
Iteration 308/1000 | Loss: 0.00001766
Iteration 309/1000 | Loss: 0.00001766
Iteration 310/1000 | Loss: 0.00001766
Iteration 311/1000 | Loss: 0.00001766
Iteration 312/1000 | Loss: 0.00001766
Iteration 313/1000 | Loss: 0.00001766
Iteration 314/1000 | Loss: 0.00001766
Iteration 315/1000 | Loss: 0.00001766
Iteration 316/1000 | Loss: 0.00001765
Iteration 317/1000 | Loss: 0.00001765
Iteration 318/1000 | Loss: 0.00001765
Iteration 319/1000 | Loss: 0.00001765
Iteration 320/1000 | Loss: 0.00001765
Iteration 321/1000 | Loss: 0.00001765
Iteration 322/1000 | Loss: 0.00001765
Iteration 323/1000 | Loss: 0.00001765
Iteration 324/1000 | Loss: 0.00001765
Iteration 325/1000 | Loss: 0.00001765
Iteration 326/1000 | Loss: 0.00001765
Iteration 327/1000 | Loss: 0.00001765
Iteration 328/1000 | Loss: 0.00001765
Iteration 329/1000 | Loss: 0.00001765
Iteration 330/1000 | Loss: 0.00001765
Iteration 331/1000 | Loss: 0.00001765
Iteration 332/1000 | Loss: 0.00001765
Iteration 333/1000 | Loss: 0.00001765
Iteration 334/1000 | Loss: 0.00001765
Iteration 335/1000 | Loss: 0.00001765
Iteration 336/1000 | Loss: 0.00001765
Iteration 337/1000 | Loss: 0.00001765
Iteration 338/1000 | Loss: 0.00001765
Iteration 339/1000 | Loss: 0.00001765
Iteration 340/1000 | Loss: 0.00001765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 340. Stopping optimization.
Last 5 losses: [1.764685475791339e-05, 1.764685475791339e-05, 1.764685475791339e-05, 1.764685475791339e-05, 1.764685475791339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.764685475791339e-05

Optimization complete. Final v2v error: 3.2276039123535156 mm

Highest mean error: 11.585963249206543 mm for frame 62

Lowest mean error: 2.8327841758728027 mm for frame 18

Saving results

Total time: 159.36301803588867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397167
Iteration 2/25 | Loss: 0.00094162
Iteration 3/25 | Loss: 0.00075772
Iteration 4/25 | Loss: 0.00073190
Iteration 5/25 | Loss: 0.00072408
Iteration 6/25 | Loss: 0.00072134
Iteration 7/25 | Loss: 0.00072056
Iteration 8/25 | Loss: 0.00072056
Iteration 9/25 | Loss: 0.00072056
Iteration 10/25 | Loss: 0.00072056
Iteration 11/25 | Loss: 0.00072056
Iteration 12/25 | Loss: 0.00072056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007205597357824445, 0.0007205597357824445, 0.0007205597357824445, 0.0007205597357824445, 0.0007205597357824445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007205597357824445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48994207
Iteration 2/25 | Loss: 0.00041769
Iteration 3/25 | Loss: 0.00041769
Iteration 4/25 | Loss: 0.00041769
Iteration 5/25 | Loss: 0.00041769
Iteration 6/25 | Loss: 0.00041769
Iteration 7/25 | Loss: 0.00041769
Iteration 8/25 | Loss: 0.00041769
Iteration 9/25 | Loss: 0.00041769
Iteration 10/25 | Loss: 0.00041769
Iteration 11/25 | Loss: 0.00041769
Iteration 12/25 | Loss: 0.00041769
Iteration 13/25 | Loss: 0.00041769
Iteration 14/25 | Loss: 0.00041769
Iteration 15/25 | Loss: 0.00041769
Iteration 16/25 | Loss: 0.00041769
Iteration 17/25 | Loss: 0.00041769
Iteration 18/25 | Loss: 0.00041769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004176876973360777, 0.0004176876973360777, 0.0004176876973360777, 0.0004176876973360777, 0.0004176876973360777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004176876973360777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041769
Iteration 2/1000 | Loss: 0.00002371
Iteration 3/1000 | Loss: 0.00001893
Iteration 4/1000 | Loss: 0.00001728
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001504
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001502
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001472
Iteration 15/1000 | Loss: 0.00001463
Iteration 16/1000 | Loss: 0.00001462
Iteration 17/1000 | Loss: 0.00001462
Iteration 18/1000 | Loss: 0.00001459
Iteration 19/1000 | Loss: 0.00001458
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001448
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001446
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001445
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001445
Iteration 29/1000 | Loss: 0.00001445
Iteration 30/1000 | Loss: 0.00001445
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001442
Iteration 40/1000 | Loss: 0.00001442
Iteration 41/1000 | Loss: 0.00001442
Iteration 42/1000 | Loss: 0.00001442
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001441
Iteration 46/1000 | Loss: 0.00001441
Iteration 47/1000 | Loss: 0.00001441
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001440
Iteration 51/1000 | Loss: 0.00001440
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001438
Iteration 55/1000 | Loss: 0.00001438
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001438
Iteration 58/1000 | Loss: 0.00001438
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001437
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001431
Iteration 67/1000 | Loss: 0.00001431
Iteration 68/1000 | Loss: 0.00001429
Iteration 69/1000 | Loss: 0.00001429
Iteration 70/1000 | Loss: 0.00001429
Iteration 71/1000 | Loss: 0.00001429
Iteration 72/1000 | Loss: 0.00001429
Iteration 73/1000 | Loss: 0.00001429
Iteration 74/1000 | Loss: 0.00001429
Iteration 75/1000 | Loss: 0.00001429
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001428
Iteration 78/1000 | Loss: 0.00001428
Iteration 79/1000 | Loss: 0.00001428
Iteration 80/1000 | Loss: 0.00001428
Iteration 81/1000 | Loss: 0.00001428
Iteration 82/1000 | Loss: 0.00001428
Iteration 83/1000 | Loss: 0.00001427
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001425
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001423
Iteration 105/1000 | Loss: 0.00001423
Iteration 106/1000 | Loss: 0.00001423
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001422
Iteration 112/1000 | Loss: 0.00001422
Iteration 113/1000 | Loss: 0.00001422
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001422
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001421
Iteration 122/1000 | Loss: 0.00001421
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001420
Iteration 138/1000 | Loss: 0.00001420
Iteration 139/1000 | Loss: 0.00001420
Iteration 140/1000 | Loss: 0.00001420
Iteration 141/1000 | Loss: 0.00001419
Iteration 142/1000 | Loss: 0.00001419
Iteration 143/1000 | Loss: 0.00001419
Iteration 144/1000 | Loss: 0.00001419
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.4194053619576152e-05, 1.4194053619576152e-05, 1.4194053619576152e-05, 1.4194053619576152e-05, 1.4194053619576152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4194053619576152e-05

Optimization complete. Final v2v error: 3.1773488521575928 mm

Highest mean error: 3.482771158218384 mm for frame 114

Lowest mean error: 2.9008607864379883 mm for frame 4

Saving results

Total time: 37.081157207489014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854301
Iteration 2/25 | Loss: 0.00132656
Iteration 3/25 | Loss: 0.00093247
Iteration 4/25 | Loss: 0.00085145
Iteration 5/25 | Loss: 0.00083536
Iteration 6/25 | Loss: 0.00083286
Iteration 7/25 | Loss: 0.00083253
Iteration 8/25 | Loss: 0.00083253
Iteration 9/25 | Loss: 0.00083253
Iteration 10/25 | Loss: 0.00083253
Iteration 11/25 | Loss: 0.00083253
Iteration 12/25 | Loss: 0.00083253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008325343951582909, 0.0008325343951582909, 0.0008325343951582909, 0.0008325343951582909, 0.0008325343951582909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008325343951582909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50467777
Iteration 2/25 | Loss: 0.00046928
Iteration 3/25 | Loss: 0.00046928
Iteration 4/25 | Loss: 0.00046928
Iteration 5/25 | Loss: 0.00046928
Iteration 6/25 | Loss: 0.00046927
Iteration 7/25 | Loss: 0.00046927
Iteration 8/25 | Loss: 0.00046927
Iteration 9/25 | Loss: 0.00046927
Iteration 10/25 | Loss: 0.00046927
Iteration 11/25 | Loss: 0.00046927
Iteration 12/25 | Loss: 0.00046927
Iteration 13/25 | Loss: 0.00046927
Iteration 14/25 | Loss: 0.00046927
Iteration 15/25 | Loss: 0.00046927
Iteration 16/25 | Loss: 0.00046927
Iteration 17/25 | Loss: 0.00046927
Iteration 18/25 | Loss: 0.00046927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00046927380026318133, 0.00046927380026318133, 0.00046927380026318133, 0.00046927380026318133, 0.00046927380026318133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046927380026318133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046927
Iteration 2/1000 | Loss: 0.00003354
Iteration 3/1000 | Loss: 0.00002827
Iteration 4/1000 | Loss: 0.00002606
Iteration 5/1000 | Loss: 0.00002464
Iteration 6/1000 | Loss: 0.00002408
Iteration 7/1000 | Loss: 0.00002366
Iteration 8/1000 | Loss: 0.00002331
Iteration 9/1000 | Loss: 0.00002294
Iteration 10/1000 | Loss: 0.00002263
Iteration 11/1000 | Loss: 0.00002238
Iteration 12/1000 | Loss: 0.00002227
Iteration 13/1000 | Loss: 0.00002226
Iteration 14/1000 | Loss: 0.00002218
Iteration 15/1000 | Loss: 0.00002217
Iteration 16/1000 | Loss: 0.00002217
Iteration 17/1000 | Loss: 0.00002215
Iteration 18/1000 | Loss: 0.00002215
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002211
Iteration 21/1000 | Loss: 0.00002211
Iteration 22/1000 | Loss: 0.00002211
Iteration 23/1000 | Loss: 0.00002211
Iteration 24/1000 | Loss: 0.00002210
Iteration 25/1000 | Loss: 0.00002210
Iteration 26/1000 | Loss: 0.00002210
Iteration 27/1000 | Loss: 0.00002210
Iteration 28/1000 | Loss: 0.00002210
Iteration 29/1000 | Loss: 0.00002210
Iteration 30/1000 | Loss: 0.00002210
Iteration 31/1000 | Loss: 0.00002210
Iteration 32/1000 | Loss: 0.00002210
Iteration 33/1000 | Loss: 0.00002210
Iteration 34/1000 | Loss: 0.00002209
Iteration 35/1000 | Loss: 0.00002209
Iteration 36/1000 | Loss: 0.00002207
Iteration 37/1000 | Loss: 0.00002204
Iteration 38/1000 | Loss: 0.00002204
Iteration 39/1000 | Loss: 0.00002201
Iteration 40/1000 | Loss: 0.00002198
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002197
Iteration 43/1000 | Loss: 0.00002197
Iteration 44/1000 | Loss: 0.00002197
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002197
Iteration 47/1000 | Loss: 0.00002197
Iteration 48/1000 | Loss: 0.00002197
Iteration 49/1000 | Loss: 0.00002196
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002196
Iteration 52/1000 | Loss: 0.00002196
Iteration 53/1000 | Loss: 0.00002195
Iteration 54/1000 | Loss: 0.00002195
Iteration 55/1000 | Loss: 0.00002195
Iteration 56/1000 | Loss: 0.00002191
Iteration 57/1000 | Loss: 0.00002191
Iteration 58/1000 | Loss: 0.00002191
Iteration 59/1000 | Loss: 0.00002191
Iteration 60/1000 | Loss: 0.00002191
Iteration 61/1000 | Loss: 0.00002190
Iteration 62/1000 | Loss: 0.00002189
Iteration 63/1000 | Loss: 0.00002188
Iteration 64/1000 | Loss: 0.00002188
Iteration 65/1000 | Loss: 0.00002188
Iteration 66/1000 | Loss: 0.00002187
Iteration 67/1000 | Loss: 0.00002187
Iteration 68/1000 | Loss: 0.00002187
Iteration 69/1000 | Loss: 0.00002187
Iteration 70/1000 | Loss: 0.00002187
Iteration 71/1000 | Loss: 0.00002187
Iteration 72/1000 | Loss: 0.00002187
Iteration 73/1000 | Loss: 0.00002187
Iteration 74/1000 | Loss: 0.00002187
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002186
Iteration 77/1000 | Loss: 0.00002186
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00002186
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002185
Iteration 83/1000 | Loss: 0.00002185
Iteration 84/1000 | Loss: 0.00002185
Iteration 85/1000 | Loss: 0.00002185
Iteration 86/1000 | Loss: 0.00002185
Iteration 87/1000 | Loss: 0.00002185
Iteration 88/1000 | Loss: 0.00002185
Iteration 89/1000 | Loss: 0.00002184
Iteration 90/1000 | Loss: 0.00002184
Iteration 91/1000 | Loss: 0.00002184
Iteration 92/1000 | Loss: 0.00002184
Iteration 93/1000 | Loss: 0.00002184
Iteration 94/1000 | Loss: 0.00002184
Iteration 95/1000 | Loss: 0.00002184
Iteration 96/1000 | Loss: 0.00002184
Iteration 97/1000 | Loss: 0.00002184
Iteration 98/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.183972901548259e-05, 2.183972901548259e-05, 2.183972901548259e-05, 2.183972901548259e-05, 2.183972901548259e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.183972901548259e-05

Optimization complete. Final v2v error: 3.9109537601470947 mm

Highest mean error: 4.077958583831787 mm for frame 2

Lowest mean error: 3.660027265548706 mm for frame 170

Saving results

Total time: 35.27396512031555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00337986
Iteration 2/25 | Loss: 0.00097592
Iteration 3/25 | Loss: 0.00079757
Iteration 4/25 | Loss: 0.00075206
Iteration 5/25 | Loss: 0.00074199
Iteration 6/25 | Loss: 0.00073936
Iteration 7/25 | Loss: 0.00073876
Iteration 8/25 | Loss: 0.00073876
Iteration 9/25 | Loss: 0.00073876
Iteration 10/25 | Loss: 0.00073876
Iteration 11/25 | Loss: 0.00073876
Iteration 12/25 | Loss: 0.00073876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007387627265416086, 0.0007387627265416086, 0.0007387627265416086, 0.0007387627265416086, 0.0007387627265416086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007387627265416086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50088704
Iteration 2/25 | Loss: 0.00046538
Iteration 3/25 | Loss: 0.00046538
Iteration 4/25 | Loss: 0.00046538
Iteration 5/25 | Loss: 0.00046538
Iteration 6/25 | Loss: 0.00046538
Iteration 7/25 | Loss: 0.00046537
Iteration 8/25 | Loss: 0.00046537
Iteration 9/25 | Loss: 0.00046537
Iteration 10/25 | Loss: 0.00046537
Iteration 11/25 | Loss: 0.00046537
Iteration 12/25 | Loss: 0.00046537
Iteration 13/25 | Loss: 0.00046537
Iteration 14/25 | Loss: 0.00046537
Iteration 15/25 | Loss: 0.00046537
Iteration 16/25 | Loss: 0.00046537
Iteration 17/25 | Loss: 0.00046537
Iteration 18/25 | Loss: 0.00046537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004653736250475049, 0.0004653736250475049, 0.0004653736250475049, 0.0004653736250475049, 0.0004653736250475049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004653736250475049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046537
Iteration 2/1000 | Loss: 0.00003033
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001686
Iteration 6/1000 | Loss: 0.00001607
Iteration 7/1000 | Loss: 0.00001561
Iteration 8/1000 | Loss: 0.00001526
Iteration 9/1000 | Loss: 0.00001502
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001489
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001487
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001478
Iteration 20/1000 | Loss: 0.00001475
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001458
Iteration 24/1000 | Loss: 0.00001456
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001433
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001426
Iteration 33/1000 | Loss: 0.00001426
Iteration 34/1000 | Loss: 0.00001425
Iteration 35/1000 | Loss: 0.00001424
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001422
Iteration 40/1000 | Loss: 0.00001422
Iteration 41/1000 | Loss: 0.00001422
Iteration 42/1000 | Loss: 0.00001421
Iteration 43/1000 | Loss: 0.00001421
Iteration 44/1000 | Loss: 0.00001421
Iteration 45/1000 | Loss: 0.00001421
Iteration 46/1000 | Loss: 0.00001420
Iteration 47/1000 | Loss: 0.00001420
Iteration 48/1000 | Loss: 0.00001420
Iteration 49/1000 | Loss: 0.00001419
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001419
Iteration 52/1000 | Loss: 0.00001418
Iteration 53/1000 | Loss: 0.00001418
Iteration 54/1000 | Loss: 0.00001418
Iteration 55/1000 | Loss: 0.00001418
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001417
Iteration 62/1000 | Loss: 0.00001417
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001417
Iteration 65/1000 | Loss: 0.00001416
Iteration 66/1000 | Loss: 0.00001416
Iteration 67/1000 | Loss: 0.00001416
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001415
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001413
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001407
Iteration 95/1000 | Loss: 0.00001407
Iteration 96/1000 | Loss: 0.00001407
Iteration 97/1000 | Loss: 0.00001407
Iteration 98/1000 | Loss: 0.00001407
Iteration 99/1000 | Loss: 0.00001407
Iteration 100/1000 | Loss: 0.00001406
Iteration 101/1000 | Loss: 0.00001406
Iteration 102/1000 | Loss: 0.00001406
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001405
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001404
Iteration 111/1000 | Loss: 0.00001404
Iteration 112/1000 | Loss: 0.00001404
Iteration 113/1000 | Loss: 0.00001404
Iteration 114/1000 | Loss: 0.00001404
Iteration 115/1000 | Loss: 0.00001404
Iteration 116/1000 | Loss: 0.00001404
Iteration 117/1000 | Loss: 0.00001404
Iteration 118/1000 | Loss: 0.00001403
Iteration 119/1000 | Loss: 0.00001403
Iteration 120/1000 | Loss: 0.00001403
Iteration 121/1000 | Loss: 0.00001403
Iteration 122/1000 | Loss: 0.00001403
Iteration 123/1000 | Loss: 0.00001403
Iteration 124/1000 | Loss: 0.00001403
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001402
Iteration 127/1000 | Loss: 0.00001402
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001402
Iteration 141/1000 | Loss: 0.00001402
Iteration 142/1000 | Loss: 0.00001402
Iteration 143/1000 | Loss: 0.00001401
Iteration 144/1000 | Loss: 0.00001401
Iteration 145/1000 | Loss: 0.00001401
Iteration 146/1000 | Loss: 0.00001401
Iteration 147/1000 | Loss: 0.00001401
Iteration 148/1000 | Loss: 0.00001401
Iteration 149/1000 | Loss: 0.00001401
Iteration 150/1000 | Loss: 0.00001401
Iteration 151/1000 | Loss: 0.00001401
Iteration 152/1000 | Loss: 0.00001401
Iteration 153/1000 | Loss: 0.00001401
Iteration 154/1000 | Loss: 0.00001401
Iteration 155/1000 | Loss: 0.00001401
Iteration 156/1000 | Loss: 0.00001401
Iteration 157/1000 | Loss: 0.00001401
Iteration 158/1000 | Loss: 0.00001401
Iteration 159/1000 | Loss: 0.00001401
Iteration 160/1000 | Loss: 0.00001401
Iteration 161/1000 | Loss: 0.00001401
Iteration 162/1000 | Loss: 0.00001401
Iteration 163/1000 | Loss: 0.00001400
Iteration 164/1000 | Loss: 0.00001400
Iteration 165/1000 | Loss: 0.00001400
Iteration 166/1000 | Loss: 0.00001400
Iteration 167/1000 | Loss: 0.00001400
Iteration 168/1000 | Loss: 0.00001400
Iteration 169/1000 | Loss: 0.00001400
Iteration 170/1000 | Loss: 0.00001400
Iteration 171/1000 | Loss: 0.00001400
Iteration 172/1000 | Loss: 0.00001400
Iteration 173/1000 | Loss: 0.00001400
Iteration 174/1000 | Loss: 0.00001400
Iteration 175/1000 | Loss: 0.00001400
Iteration 176/1000 | Loss: 0.00001400
Iteration 177/1000 | Loss: 0.00001400
Iteration 178/1000 | Loss: 0.00001400
Iteration 179/1000 | Loss: 0.00001400
Iteration 180/1000 | Loss: 0.00001400
Iteration 181/1000 | Loss: 0.00001400
Iteration 182/1000 | Loss: 0.00001400
Iteration 183/1000 | Loss: 0.00001400
Iteration 184/1000 | Loss: 0.00001400
Iteration 185/1000 | Loss: 0.00001400
Iteration 186/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.4000697774463333e-05, 1.4000697774463333e-05, 1.4000697774463333e-05, 1.4000697774463333e-05, 1.4000697774463333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4000697774463333e-05

Optimization complete. Final v2v error: 3.1719629764556885 mm

Highest mean error: 3.259330987930298 mm for frame 79

Lowest mean error: 3.0581071376800537 mm for frame 174

Saving results

Total time: 43.83520221710205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080370
Iteration 2/25 | Loss: 0.01080370
Iteration 3/25 | Loss: 0.01080370
Iteration 4/25 | Loss: 0.01080370
Iteration 5/25 | Loss: 0.01080370
Iteration 6/25 | Loss: 0.01080370
Iteration 7/25 | Loss: 0.01080370
Iteration 8/25 | Loss: 0.01080370
Iteration 9/25 | Loss: 0.01080370
Iteration 10/25 | Loss: 0.01080369
Iteration 11/25 | Loss: 0.01080369
Iteration 12/25 | Loss: 0.01080369
Iteration 13/25 | Loss: 0.01080369
Iteration 14/25 | Loss: 0.01080369
Iteration 15/25 | Loss: 0.01080369
Iteration 16/25 | Loss: 0.01080369
Iteration 17/25 | Loss: 0.01080369
Iteration 18/25 | Loss: 0.01080369
Iteration 19/25 | Loss: 0.01080369
Iteration 20/25 | Loss: 0.01080369
Iteration 21/25 | Loss: 0.01080368
Iteration 22/25 | Loss: 0.01080368
Iteration 23/25 | Loss: 0.01080368
Iteration 24/25 | Loss: 0.01080368
Iteration 25/25 | Loss: 0.01080368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70484638
Iteration 2/25 | Loss: 0.08439250
Iteration 3/25 | Loss: 0.08376514
Iteration 4/25 | Loss: 0.08379983
Iteration 5/25 | Loss: 0.08365915
Iteration 6/25 | Loss: 0.08365910
Iteration 7/25 | Loss: 0.08365909
Iteration 8/25 | Loss: 0.08365910
Iteration 9/25 | Loss: 0.08365909
Iteration 10/25 | Loss: 0.08365908
Iteration 11/25 | Loss: 0.08365908
Iteration 12/25 | Loss: 0.08365908
Iteration 13/25 | Loss: 0.08365908
Iteration 14/25 | Loss: 0.08365908
Iteration 15/25 | Loss: 0.08365908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0836590826511383, 0.0836590826511383, 0.0836590826511383, 0.0836590826511383, 0.0836590826511383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0836590826511383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08365908
Iteration 2/1000 | Loss: 0.00397465
Iteration 3/1000 | Loss: 0.00185665
Iteration 4/1000 | Loss: 0.00089785
Iteration 5/1000 | Loss: 0.00073582
Iteration 6/1000 | Loss: 0.00009436
Iteration 7/1000 | Loss: 0.00013794
Iteration 8/1000 | Loss: 0.00068005
Iteration 9/1000 | Loss: 0.00012105
Iteration 10/1000 | Loss: 0.00006684
Iteration 11/1000 | Loss: 0.00033757
Iteration 12/1000 | Loss: 0.00008422
Iteration 13/1000 | Loss: 0.00015362
Iteration 14/1000 | Loss: 0.00007964
Iteration 15/1000 | Loss: 0.00003685
Iteration 16/1000 | Loss: 0.00002482
Iteration 17/1000 | Loss: 0.00043354
Iteration 18/1000 | Loss: 0.00029410
Iteration 19/1000 | Loss: 0.00101572
Iteration 20/1000 | Loss: 0.00020975
Iteration 21/1000 | Loss: 0.00003507
Iteration 22/1000 | Loss: 0.00007427
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00007449
Iteration 25/1000 | Loss: 0.00001926
Iteration 26/1000 | Loss: 0.00028487
Iteration 27/1000 | Loss: 0.00009755
Iteration 28/1000 | Loss: 0.00012488
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001705
Iteration 31/1000 | Loss: 0.00014036
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001605
Iteration 34/1000 | Loss: 0.00031387
Iteration 35/1000 | Loss: 0.00012730
Iteration 36/1000 | Loss: 0.00006799
Iteration 37/1000 | Loss: 0.00002572
Iteration 38/1000 | Loss: 0.00001541
Iteration 39/1000 | Loss: 0.00007024
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00008784
Iteration 42/1000 | Loss: 0.00001471
Iteration 43/1000 | Loss: 0.00001445
Iteration 44/1000 | Loss: 0.00003162
Iteration 45/1000 | Loss: 0.00008299
Iteration 46/1000 | Loss: 0.00005482
Iteration 47/1000 | Loss: 0.00023682
Iteration 48/1000 | Loss: 0.00062793
Iteration 49/1000 | Loss: 0.00044835
Iteration 50/1000 | Loss: 0.00022147
Iteration 51/1000 | Loss: 0.00065002
Iteration 52/1000 | Loss: 0.00107332
Iteration 53/1000 | Loss: 0.00222848
Iteration 54/1000 | Loss: 0.00018320
Iteration 55/1000 | Loss: 0.00019136
Iteration 56/1000 | Loss: 0.00012155
Iteration 57/1000 | Loss: 0.00006091
Iteration 58/1000 | Loss: 0.00006056
Iteration 59/1000 | Loss: 0.00013723
Iteration 60/1000 | Loss: 0.00003784
Iteration 61/1000 | Loss: 0.00005478
Iteration 62/1000 | Loss: 0.00003418
Iteration 63/1000 | Loss: 0.00004650
Iteration 64/1000 | Loss: 0.00003217
Iteration 65/1000 | Loss: 0.00004566
Iteration 66/1000 | Loss: 0.00019867
Iteration 67/1000 | Loss: 0.00007076
Iteration 68/1000 | Loss: 0.00003274
Iteration 69/1000 | Loss: 0.00014936
Iteration 70/1000 | Loss: 0.00003229
Iteration 71/1000 | Loss: 0.00006469
Iteration 72/1000 | Loss: 0.00008334
Iteration 73/1000 | Loss: 0.00002372
Iteration 74/1000 | Loss: 0.00002369
Iteration 75/1000 | Loss: 0.00004113
Iteration 76/1000 | Loss: 0.00002960
Iteration 77/1000 | Loss: 0.00002699
Iteration 78/1000 | Loss: 0.00003038
Iteration 79/1000 | Loss: 0.00012533
Iteration 80/1000 | Loss: 0.00002697
Iteration 81/1000 | Loss: 0.00003013
Iteration 82/1000 | Loss: 0.00003175
Iteration 83/1000 | Loss: 0.00002932
Iteration 84/1000 | Loss: 0.00002539
Iteration 85/1000 | Loss: 0.00003959
Iteration 86/1000 | Loss: 0.00003343
Iteration 87/1000 | Loss: 0.00002690
Iteration 88/1000 | Loss: 0.00003058
Iteration 89/1000 | Loss: 0.00003399
Iteration 90/1000 | Loss: 0.00002897
Iteration 91/1000 | Loss: 0.00002982
Iteration 92/1000 | Loss: 0.00020171
Iteration 93/1000 | Loss: 0.00002152
Iteration 94/1000 | Loss: 0.00042212
Iteration 95/1000 | Loss: 0.00003693
Iteration 96/1000 | Loss: 0.00005630
Iteration 97/1000 | Loss: 0.00010795
Iteration 98/1000 | Loss: 0.00001544
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001500
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001445
Iteration 104/1000 | Loss: 0.00001430
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001414
Iteration 107/1000 | Loss: 0.00001412
Iteration 108/1000 | Loss: 0.00001409
Iteration 109/1000 | Loss: 0.00001408
Iteration 110/1000 | Loss: 0.00001401
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001401
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001398
Iteration 121/1000 | Loss: 0.00001398
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001396
Iteration 124/1000 | Loss: 0.00001396
Iteration 125/1000 | Loss: 0.00001396
Iteration 126/1000 | Loss: 0.00001396
Iteration 127/1000 | Loss: 0.00001395
Iteration 128/1000 | Loss: 0.00001395
Iteration 129/1000 | Loss: 0.00001395
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001403
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001392
Iteration 138/1000 | Loss: 0.00001392
Iteration 139/1000 | Loss: 0.00001392
Iteration 140/1000 | Loss: 0.00001392
Iteration 141/1000 | Loss: 0.00001392
Iteration 142/1000 | Loss: 0.00001391
Iteration 143/1000 | Loss: 0.00001391
Iteration 144/1000 | Loss: 0.00001391
Iteration 145/1000 | Loss: 0.00001391
Iteration 146/1000 | Loss: 0.00001391
Iteration 147/1000 | Loss: 0.00001391
Iteration 148/1000 | Loss: 0.00001391
Iteration 149/1000 | Loss: 0.00001391
Iteration 150/1000 | Loss: 0.00001391
Iteration 151/1000 | Loss: 0.00001391
Iteration 152/1000 | Loss: 0.00001391
Iteration 153/1000 | Loss: 0.00001391
Iteration 154/1000 | Loss: 0.00001391
Iteration 155/1000 | Loss: 0.00001391
Iteration 156/1000 | Loss: 0.00001391
Iteration 157/1000 | Loss: 0.00001391
Iteration 158/1000 | Loss: 0.00001391
Iteration 159/1000 | Loss: 0.00001391
Iteration 160/1000 | Loss: 0.00001391
Iteration 161/1000 | Loss: 0.00001390
Iteration 162/1000 | Loss: 0.00001390
Iteration 163/1000 | Loss: 0.00001390
Iteration 164/1000 | Loss: 0.00001390
Iteration 165/1000 | Loss: 0.00001390
Iteration 166/1000 | Loss: 0.00001390
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001389
Iteration 169/1000 | Loss: 0.00001389
Iteration 170/1000 | Loss: 0.00001389
Iteration 171/1000 | Loss: 0.00001389
Iteration 172/1000 | Loss: 0.00001389
Iteration 173/1000 | Loss: 0.00001389
Iteration 174/1000 | Loss: 0.00001389
Iteration 175/1000 | Loss: 0.00001389
Iteration 176/1000 | Loss: 0.00001388
Iteration 177/1000 | Loss: 0.00001388
Iteration 178/1000 | Loss: 0.00001388
Iteration 179/1000 | Loss: 0.00001388
Iteration 180/1000 | Loss: 0.00001388
Iteration 181/1000 | Loss: 0.00001388
Iteration 182/1000 | Loss: 0.00001388
Iteration 183/1000 | Loss: 0.00001388
Iteration 184/1000 | Loss: 0.00001388
Iteration 185/1000 | Loss: 0.00001388
Iteration 186/1000 | Loss: 0.00001388
Iteration 187/1000 | Loss: 0.00001388
Iteration 188/1000 | Loss: 0.00001388
Iteration 189/1000 | Loss: 0.00001388
Iteration 190/1000 | Loss: 0.00001388
Iteration 191/1000 | Loss: 0.00001388
Iteration 192/1000 | Loss: 0.00001388
Iteration 193/1000 | Loss: 0.00001388
Iteration 194/1000 | Loss: 0.00001388
Iteration 195/1000 | Loss: 0.00001388
Iteration 196/1000 | Loss: 0.00001388
Iteration 197/1000 | Loss: 0.00001388
Iteration 198/1000 | Loss: 0.00001388
Iteration 199/1000 | Loss: 0.00001388
Iteration 200/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.388265718560433e-05, 1.388265718560433e-05, 1.388265718560433e-05, 1.388265718560433e-05, 1.388265718560433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.388265718560433e-05

Optimization complete. Final v2v error: 3.0546417236328125 mm

Highest mean error: 9.916640281677246 mm for frame 177

Lowest mean error: 2.593069553375244 mm for frame 2

Saving results

Total time: 187.02533507347107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467684
Iteration 2/25 | Loss: 0.00100351
Iteration 3/25 | Loss: 0.00083988
Iteration 4/25 | Loss: 0.00078447
Iteration 5/25 | Loss: 0.00077482
Iteration 6/25 | Loss: 0.00077281
Iteration 7/25 | Loss: 0.00077247
Iteration 8/25 | Loss: 0.00077247
Iteration 9/25 | Loss: 0.00077247
Iteration 10/25 | Loss: 0.00077247
Iteration 11/25 | Loss: 0.00077247
Iteration 12/25 | Loss: 0.00077247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007724674651399255, 0.0007724674651399255, 0.0007724674651399255, 0.0007724674651399255, 0.0007724674651399255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007724674651399255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.44340897
Iteration 2/25 | Loss: 0.00048833
Iteration 3/25 | Loss: 0.00048832
Iteration 4/25 | Loss: 0.00048832
Iteration 5/25 | Loss: 0.00048832
Iteration 6/25 | Loss: 0.00048832
Iteration 7/25 | Loss: 0.00048832
Iteration 8/25 | Loss: 0.00048831
Iteration 9/25 | Loss: 0.00048831
Iteration 10/25 | Loss: 0.00048831
Iteration 11/25 | Loss: 0.00048831
Iteration 12/25 | Loss: 0.00048831
Iteration 13/25 | Loss: 0.00048831
Iteration 14/25 | Loss: 0.00048831
Iteration 15/25 | Loss: 0.00048831
Iteration 16/25 | Loss: 0.00048831
Iteration 17/25 | Loss: 0.00048831
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004883143701590598, 0.0004883143701590598, 0.0004883143701590598, 0.0004883143701590598, 0.0004883143701590598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004883143701590598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048831
Iteration 2/1000 | Loss: 0.00003638
Iteration 3/1000 | Loss: 0.00002441
Iteration 4/1000 | Loss: 0.00002147
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001925
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001728
Iteration 18/1000 | Loss: 0.00001727
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001722
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001709
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001708
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001707
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001703
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001701
Iteration 69/1000 | Loss: 0.00001701
Iteration 70/1000 | Loss: 0.00001701
Iteration 71/1000 | Loss: 0.00001701
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001700
Iteration 74/1000 | Loss: 0.00001700
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001696
Iteration 87/1000 | Loss: 0.00001696
Iteration 88/1000 | Loss: 0.00001696
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001693
Iteration 96/1000 | Loss: 0.00001693
Iteration 97/1000 | Loss: 0.00001693
Iteration 98/1000 | Loss: 0.00001693
Iteration 99/1000 | Loss: 0.00001692
Iteration 100/1000 | Loss: 0.00001692
Iteration 101/1000 | Loss: 0.00001691
Iteration 102/1000 | Loss: 0.00001691
Iteration 103/1000 | Loss: 0.00001691
Iteration 104/1000 | Loss: 0.00001691
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001689
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001688
Iteration 113/1000 | Loss: 0.00001688
Iteration 114/1000 | Loss: 0.00001688
Iteration 115/1000 | Loss: 0.00001688
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001687
Iteration 125/1000 | Loss: 0.00001687
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.6853226043167524e-05, 1.6853226043167524e-05, 1.6853226043167524e-05, 1.6853226043167524e-05, 1.6853226043167524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6853226043167524e-05

Optimization complete. Final v2v error: 3.43648099899292 mm

Highest mean error: 4.454986572265625 mm for frame 56

Lowest mean error: 2.8860034942626953 mm for frame 102

Saving results

Total time: 45.85643410682678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568576
Iteration 2/25 | Loss: 0.00178721
Iteration 3/25 | Loss: 0.00114418
Iteration 4/25 | Loss: 0.00105746
Iteration 5/25 | Loss: 0.00103423
Iteration 6/25 | Loss: 0.00102852
Iteration 7/25 | Loss: 0.00102678
Iteration 8/25 | Loss: 0.00102602
Iteration 9/25 | Loss: 0.00102578
Iteration 10/25 | Loss: 0.00102578
Iteration 11/25 | Loss: 0.00102578
Iteration 12/25 | Loss: 0.00102578
Iteration 13/25 | Loss: 0.00102578
Iteration 14/25 | Loss: 0.00102578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010257784742861986, 0.0010257784742861986, 0.0010257784742861986, 0.0010257784742861986, 0.0010257784742861986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010257784742861986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08917940
Iteration 2/25 | Loss: 0.00100554
Iteration 3/25 | Loss: 0.00100552
Iteration 4/25 | Loss: 0.00100552
Iteration 5/25 | Loss: 0.00100552
Iteration 6/25 | Loss: 0.00100552
Iteration 7/25 | Loss: 0.00100552
Iteration 8/25 | Loss: 0.00100552
Iteration 9/25 | Loss: 0.00100552
Iteration 10/25 | Loss: 0.00100552
Iteration 11/25 | Loss: 0.00100552
Iteration 12/25 | Loss: 0.00100552
Iteration 13/25 | Loss: 0.00100552
Iteration 14/25 | Loss: 0.00100552
Iteration 15/25 | Loss: 0.00100552
Iteration 16/25 | Loss: 0.00100552
Iteration 17/25 | Loss: 0.00100552
Iteration 18/25 | Loss: 0.00100552
Iteration 19/25 | Loss: 0.00100552
Iteration 20/25 | Loss: 0.00100552
Iteration 21/25 | Loss: 0.00100552
Iteration 22/25 | Loss: 0.00100552
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001005522790364921, 0.001005522790364921, 0.001005522790364921, 0.001005522790364921, 0.001005522790364921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001005522790364921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100552
Iteration 2/1000 | Loss: 0.00010783
Iteration 3/1000 | Loss: 0.00007144
Iteration 4/1000 | Loss: 0.00006234
Iteration 5/1000 | Loss: 0.00005827
Iteration 6/1000 | Loss: 0.00005619
Iteration 7/1000 | Loss: 0.00005453
Iteration 8/1000 | Loss: 0.00005349
Iteration 9/1000 | Loss: 0.00052169
Iteration 10/1000 | Loss: 0.00053809
Iteration 11/1000 | Loss: 0.00041814
Iteration 12/1000 | Loss: 0.00029078
Iteration 13/1000 | Loss: 0.00018118
Iteration 14/1000 | Loss: 0.00051207
Iteration 15/1000 | Loss: 0.00024371
Iteration 16/1000 | Loss: 0.00022566
Iteration 17/1000 | Loss: 0.00005701
Iteration 18/1000 | Loss: 0.00005307
Iteration 19/1000 | Loss: 0.00028190
Iteration 20/1000 | Loss: 0.00030903
Iteration 21/1000 | Loss: 0.00006573
Iteration 22/1000 | Loss: 0.00006566
Iteration 23/1000 | Loss: 0.00006039
Iteration 24/1000 | Loss: 0.00028782
Iteration 25/1000 | Loss: 0.00007428
Iteration 26/1000 | Loss: 0.00005641
Iteration 27/1000 | Loss: 0.00005307
Iteration 28/1000 | Loss: 0.00005160
Iteration 29/1000 | Loss: 0.00005019
Iteration 30/1000 | Loss: 0.00004879
Iteration 31/1000 | Loss: 0.00004758
Iteration 32/1000 | Loss: 0.00028357
Iteration 33/1000 | Loss: 0.00007847
Iteration 34/1000 | Loss: 0.00005266
Iteration 35/1000 | Loss: 0.00004969
Iteration 36/1000 | Loss: 0.00004813
Iteration 37/1000 | Loss: 0.00051323
Iteration 38/1000 | Loss: 0.00006914
Iteration 39/1000 | Loss: 0.00029885
Iteration 40/1000 | Loss: 0.00046954
Iteration 41/1000 | Loss: 0.00032377
Iteration 42/1000 | Loss: 0.00015094
Iteration 43/1000 | Loss: 0.00006334
Iteration 44/1000 | Loss: 0.00006194
Iteration 45/1000 | Loss: 0.00005344
Iteration 46/1000 | Loss: 0.00052441
Iteration 47/1000 | Loss: 0.00010923
Iteration 48/1000 | Loss: 0.00011486
Iteration 49/1000 | Loss: 0.00005544
Iteration 50/1000 | Loss: 0.00005233
Iteration 51/1000 | Loss: 0.00029142
Iteration 52/1000 | Loss: 0.00006849
Iteration 53/1000 | Loss: 0.00029135
Iteration 54/1000 | Loss: 0.00008626
Iteration 55/1000 | Loss: 0.00005410
Iteration 56/1000 | Loss: 0.00005531
Iteration 57/1000 | Loss: 0.00005246
Iteration 58/1000 | Loss: 0.00004811
Iteration 59/1000 | Loss: 0.00005931
Iteration 60/1000 | Loss: 0.00005141
Iteration 61/1000 | Loss: 0.00029089
Iteration 62/1000 | Loss: 0.00007616
Iteration 63/1000 | Loss: 0.00004924
Iteration 64/1000 | Loss: 0.00004386
Iteration 65/1000 | Loss: 0.00004211
Iteration 66/1000 | Loss: 0.00004014
Iteration 67/1000 | Loss: 0.00003876
Iteration 68/1000 | Loss: 0.00004749
Iteration 69/1000 | Loss: 0.00003822
Iteration 70/1000 | Loss: 0.00003723
Iteration 71/1000 | Loss: 0.00003633
Iteration 72/1000 | Loss: 0.00003603
Iteration 73/1000 | Loss: 0.00003577
Iteration 74/1000 | Loss: 0.00003558
Iteration 75/1000 | Loss: 0.00003534
Iteration 76/1000 | Loss: 0.00003524
Iteration 77/1000 | Loss: 0.00003521
Iteration 78/1000 | Loss: 0.00003520
Iteration 79/1000 | Loss: 0.00003519
Iteration 80/1000 | Loss: 0.00003518
Iteration 81/1000 | Loss: 0.00003518
Iteration 82/1000 | Loss: 0.00003518
Iteration 83/1000 | Loss: 0.00003517
Iteration 84/1000 | Loss: 0.00003517
Iteration 85/1000 | Loss: 0.00003516
Iteration 86/1000 | Loss: 0.00003516
Iteration 87/1000 | Loss: 0.00003516
Iteration 88/1000 | Loss: 0.00003516
Iteration 89/1000 | Loss: 0.00003515
Iteration 90/1000 | Loss: 0.00003515
Iteration 91/1000 | Loss: 0.00003515
Iteration 92/1000 | Loss: 0.00003515
Iteration 93/1000 | Loss: 0.00003514
Iteration 94/1000 | Loss: 0.00003514
Iteration 95/1000 | Loss: 0.00003512
Iteration 96/1000 | Loss: 0.00003509
Iteration 97/1000 | Loss: 0.00003508
Iteration 98/1000 | Loss: 0.00003508
Iteration 99/1000 | Loss: 0.00003508
Iteration 100/1000 | Loss: 0.00003507
Iteration 101/1000 | Loss: 0.00003506
Iteration 102/1000 | Loss: 0.00003506
Iteration 103/1000 | Loss: 0.00003505
Iteration 104/1000 | Loss: 0.00003505
Iteration 105/1000 | Loss: 0.00003504
Iteration 106/1000 | Loss: 0.00003504
Iteration 107/1000 | Loss: 0.00003504
Iteration 108/1000 | Loss: 0.00003504
Iteration 109/1000 | Loss: 0.00003503
Iteration 110/1000 | Loss: 0.00003503
Iteration 111/1000 | Loss: 0.00003503
Iteration 112/1000 | Loss: 0.00003502
Iteration 113/1000 | Loss: 0.00003502
Iteration 114/1000 | Loss: 0.00003502
Iteration 115/1000 | Loss: 0.00003501
Iteration 116/1000 | Loss: 0.00003501
Iteration 117/1000 | Loss: 0.00003501
Iteration 118/1000 | Loss: 0.00003501
Iteration 119/1000 | Loss: 0.00003500
Iteration 120/1000 | Loss: 0.00003500
Iteration 121/1000 | Loss: 0.00003499
Iteration 122/1000 | Loss: 0.00003499
Iteration 123/1000 | Loss: 0.00003499
Iteration 124/1000 | Loss: 0.00003499
Iteration 125/1000 | Loss: 0.00003498
Iteration 126/1000 | Loss: 0.00003498
Iteration 127/1000 | Loss: 0.00003498
Iteration 128/1000 | Loss: 0.00003498
Iteration 129/1000 | Loss: 0.00003498
Iteration 130/1000 | Loss: 0.00003497
Iteration 131/1000 | Loss: 0.00003497
Iteration 132/1000 | Loss: 0.00003497
Iteration 133/1000 | Loss: 0.00003497
Iteration 134/1000 | Loss: 0.00003497
Iteration 135/1000 | Loss: 0.00003496
Iteration 136/1000 | Loss: 0.00003496
Iteration 137/1000 | Loss: 0.00003496
Iteration 138/1000 | Loss: 0.00003496
Iteration 139/1000 | Loss: 0.00003496
Iteration 140/1000 | Loss: 0.00003496
Iteration 141/1000 | Loss: 0.00003496
Iteration 142/1000 | Loss: 0.00003496
Iteration 143/1000 | Loss: 0.00003496
Iteration 144/1000 | Loss: 0.00003495
Iteration 145/1000 | Loss: 0.00003495
Iteration 146/1000 | Loss: 0.00003495
Iteration 147/1000 | Loss: 0.00003495
Iteration 148/1000 | Loss: 0.00003495
Iteration 149/1000 | Loss: 0.00003494
Iteration 150/1000 | Loss: 0.00003494
Iteration 151/1000 | Loss: 0.00003494
Iteration 152/1000 | Loss: 0.00003494
Iteration 153/1000 | Loss: 0.00003494
Iteration 154/1000 | Loss: 0.00003493
Iteration 155/1000 | Loss: 0.00003493
Iteration 156/1000 | Loss: 0.00003493
Iteration 157/1000 | Loss: 0.00003492
Iteration 158/1000 | Loss: 0.00003492
Iteration 159/1000 | Loss: 0.00003492
Iteration 160/1000 | Loss: 0.00003492
Iteration 161/1000 | Loss: 0.00003491
Iteration 162/1000 | Loss: 0.00003491
Iteration 163/1000 | Loss: 0.00003491
Iteration 164/1000 | Loss: 0.00003491
Iteration 165/1000 | Loss: 0.00003491
Iteration 166/1000 | Loss: 0.00003491
Iteration 167/1000 | Loss: 0.00003491
Iteration 168/1000 | Loss: 0.00003491
Iteration 169/1000 | Loss: 0.00003491
Iteration 170/1000 | Loss: 0.00003491
Iteration 171/1000 | Loss: 0.00003491
Iteration 172/1000 | Loss: 0.00003490
Iteration 173/1000 | Loss: 0.00003490
Iteration 174/1000 | Loss: 0.00003490
Iteration 175/1000 | Loss: 0.00003490
Iteration 176/1000 | Loss: 0.00003490
Iteration 177/1000 | Loss: 0.00003490
Iteration 178/1000 | Loss: 0.00003490
Iteration 179/1000 | Loss: 0.00003490
Iteration 180/1000 | Loss: 0.00003490
Iteration 181/1000 | Loss: 0.00003490
Iteration 182/1000 | Loss: 0.00003490
Iteration 183/1000 | Loss: 0.00003490
Iteration 184/1000 | Loss: 0.00003490
Iteration 185/1000 | Loss: 0.00003490
Iteration 186/1000 | Loss: 0.00003490
Iteration 187/1000 | Loss: 0.00003490
Iteration 188/1000 | Loss: 0.00003490
Iteration 189/1000 | Loss: 0.00003490
Iteration 190/1000 | Loss: 0.00003490
Iteration 191/1000 | Loss: 0.00003490
Iteration 192/1000 | Loss: 0.00003490
Iteration 193/1000 | Loss: 0.00003490
Iteration 194/1000 | Loss: 0.00003490
Iteration 195/1000 | Loss: 0.00003490
Iteration 196/1000 | Loss: 0.00003490
Iteration 197/1000 | Loss: 0.00003490
Iteration 198/1000 | Loss: 0.00003490
Iteration 199/1000 | Loss: 0.00003490
Iteration 200/1000 | Loss: 0.00003490
Iteration 201/1000 | Loss: 0.00003490
Iteration 202/1000 | Loss: 0.00003490
Iteration 203/1000 | Loss: 0.00003490
Iteration 204/1000 | Loss: 0.00003490
Iteration 205/1000 | Loss: 0.00003490
Iteration 206/1000 | Loss: 0.00003490
Iteration 207/1000 | Loss: 0.00003490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [3.489869777695276e-05, 3.489869777695276e-05, 3.489869777695276e-05, 3.489869777695276e-05, 3.489869777695276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.489869777695276e-05

Optimization complete. Final v2v error: 4.593826770782471 mm

Highest mean error: 5.83131217956543 mm for frame 13

Lowest mean error: 3.4710142612457275 mm for frame 105

Saving results

Total time: 148.47786402702332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774210
Iteration 2/25 | Loss: 0.00121226
Iteration 3/25 | Loss: 0.00081748
Iteration 4/25 | Loss: 0.00077232
Iteration 5/25 | Loss: 0.00076412
Iteration 6/25 | Loss: 0.00076185
Iteration 7/25 | Loss: 0.00076133
Iteration 8/25 | Loss: 0.00076127
Iteration 9/25 | Loss: 0.00076127
Iteration 10/25 | Loss: 0.00076127
Iteration 11/25 | Loss: 0.00076127
Iteration 12/25 | Loss: 0.00076127
Iteration 13/25 | Loss: 0.00076127
Iteration 14/25 | Loss: 0.00076127
Iteration 15/25 | Loss: 0.00076127
Iteration 16/25 | Loss: 0.00076127
Iteration 17/25 | Loss: 0.00076127
Iteration 18/25 | Loss: 0.00076127
Iteration 19/25 | Loss: 0.00076127
Iteration 20/25 | Loss: 0.00076127
Iteration 21/25 | Loss: 0.00076127
Iteration 22/25 | Loss: 0.00076127
Iteration 23/25 | Loss: 0.00076127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007612720946781337, 0.0007612720946781337, 0.0007612720946781337, 0.0007612720946781337, 0.0007612720946781337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007612720946781337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50139332
Iteration 2/25 | Loss: 0.00056880
Iteration 3/25 | Loss: 0.00056878
Iteration 4/25 | Loss: 0.00056878
Iteration 5/25 | Loss: 0.00056878
Iteration 6/25 | Loss: 0.00056878
Iteration 7/25 | Loss: 0.00056878
Iteration 8/25 | Loss: 0.00056878
Iteration 9/25 | Loss: 0.00056878
Iteration 10/25 | Loss: 0.00056878
Iteration 11/25 | Loss: 0.00056878
Iteration 12/25 | Loss: 0.00056878
Iteration 13/25 | Loss: 0.00056878
Iteration 14/25 | Loss: 0.00056878
Iteration 15/25 | Loss: 0.00056878
Iteration 16/25 | Loss: 0.00056878
Iteration 17/25 | Loss: 0.00056878
Iteration 18/25 | Loss: 0.00056878
Iteration 19/25 | Loss: 0.00056878
Iteration 20/25 | Loss: 0.00056878
Iteration 21/25 | Loss: 0.00056878
Iteration 22/25 | Loss: 0.00056878
Iteration 23/25 | Loss: 0.00056878
Iteration 24/25 | Loss: 0.00056878
Iteration 25/25 | Loss: 0.00056878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056878
Iteration 2/1000 | Loss: 0.00002475
Iteration 3/1000 | Loss: 0.00001802
Iteration 4/1000 | Loss: 0.00001503
Iteration 5/1000 | Loss: 0.00001395
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001284
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001220
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001207
Iteration 17/1000 | Loss: 0.00001207
Iteration 18/1000 | Loss: 0.00001207
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001207
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001196
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001191
Iteration 42/1000 | Loss: 0.00001191
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001190
Iteration 46/1000 | Loss: 0.00001190
Iteration 47/1000 | Loss: 0.00001190
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001183
Iteration 75/1000 | Loss: 0.00001183
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001182
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.1824588000308722e-05, 1.1824588000308722e-05, 1.1824588000308722e-05, 1.1824588000308722e-05, 1.1824588000308722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1824588000308722e-05

Optimization complete. Final v2v error: 2.9667134284973145 mm

Highest mean error: 3.3281731605529785 mm for frame 109

Lowest mean error: 2.714705228805542 mm for frame 20

Saving results

Total time: 30.970842123031616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035416
Iteration 2/25 | Loss: 0.00312364
Iteration 3/25 | Loss: 0.00203525
Iteration 4/25 | Loss: 0.00172077
Iteration 5/25 | Loss: 0.00153871
Iteration 6/25 | Loss: 0.00148943
Iteration 7/25 | Loss: 0.00140653
Iteration 8/25 | Loss: 0.00141245
Iteration 9/25 | Loss: 0.00131948
Iteration 10/25 | Loss: 0.00123914
Iteration 11/25 | Loss: 0.00118571
Iteration 12/25 | Loss: 0.00115768
Iteration 13/25 | Loss: 0.00112493
Iteration 14/25 | Loss: 0.00113554
Iteration 15/25 | Loss: 0.00112130
Iteration 16/25 | Loss: 0.00109884
Iteration 17/25 | Loss: 0.00108989
Iteration 18/25 | Loss: 0.00106023
Iteration 19/25 | Loss: 0.00104022
Iteration 20/25 | Loss: 0.00103077
Iteration 21/25 | Loss: 0.00102738
Iteration 22/25 | Loss: 0.00102300
Iteration 23/25 | Loss: 0.00101740
Iteration 24/25 | Loss: 0.00100985
Iteration 25/25 | Loss: 0.00100416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51589549
Iteration 2/25 | Loss: 0.00334124
Iteration 3/25 | Loss: 0.00306876
Iteration 4/25 | Loss: 0.00300961
Iteration 5/25 | Loss: 0.00300961
Iteration 6/25 | Loss: 0.00300961
Iteration 7/25 | Loss: 0.00300961
Iteration 8/25 | Loss: 0.00300961
Iteration 9/25 | Loss: 0.00300960
Iteration 10/25 | Loss: 0.00300960
Iteration 11/25 | Loss: 0.00300960
Iteration 12/25 | Loss: 0.00300960
Iteration 13/25 | Loss: 0.00300960
Iteration 14/25 | Loss: 0.00300960
Iteration 15/25 | Loss: 0.00300960
Iteration 16/25 | Loss: 0.00300960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003009604290127754, 0.003009604290127754, 0.003009604290127754, 0.003009604290127754, 0.003009604290127754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003009604290127754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300960
Iteration 2/1000 | Loss: 0.00056821
Iteration 3/1000 | Loss: 0.00046828
Iteration 4/1000 | Loss: 0.00092479
Iteration 5/1000 | Loss: 0.00034087
Iteration 6/1000 | Loss: 0.00057513
Iteration 7/1000 | Loss: 0.00024040
Iteration 8/1000 | Loss: 0.00020408
Iteration 9/1000 | Loss: 0.00016532
Iteration 10/1000 | Loss: 0.00148054
Iteration 11/1000 | Loss: 0.00107168
Iteration 12/1000 | Loss: 0.00115700
Iteration 13/1000 | Loss: 0.00056426
Iteration 14/1000 | Loss: 0.00074218
Iteration 15/1000 | Loss: 0.00051639
Iteration 16/1000 | Loss: 0.00113580
Iteration 17/1000 | Loss: 0.00067839
Iteration 18/1000 | Loss: 0.00016227
Iteration 19/1000 | Loss: 0.00024811
Iteration 20/1000 | Loss: 0.00029003
Iteration 21/1000 | Loss: 0.00071185
Iteration 22/1000 | Loss: 0.00027373
Iteration 23/1000 | Loss: 0.00028041
Iteration 24/1000 | Loss: 0.00014052
Iteration 25/1000 | Loss: 0.00015207
Iteration 26/1000 | Loss: 0.00013144
Iteration 27/1000 | Loss: 0.00012939
Iteration 28/1000 | Loss: 0.00012456
Iteration 29/1000 | Loss: 0.00075916
Iteration 30/1000 | Loss: 0.00042643
Iteration 31/1000 | Loss: 0.00111834
Iteration 32/1000 | Loss: 0.00045600
Iteration 33/1000 | Loss: 0.00099917
Iteration 34/1000 | Loss: 0.00038166
Iteration 35/1000 | Loss: 0.00021818
Iteration 36/1000 | Loss: 0.00013518
Iteration 37/1000 | Loss: 0.00013111
Iteration 38/1000 | Loss: 0.00022531
Iteration 39/1000 | Loss: 0.00017203
Iteration 40/1000 | Loss: 0.00061540
Iteration 41/1000 | Loss: 0.00037903
Iteration 42/1000 | Loss: 0.00036398
Iteration 43/1000 | Loss: 0.00048379
Iteration 44/1000 | Loss: 0.00041981
Iteration 45/1000 | Loss: 0.00013696
Iteration 46/1000 | Loss: 0.00037552
Iteration 47/1000 | Loss: 0.00015864
Iteration 48/1000 | Loss: 0.00016975
Iteration 49/1000 | Loss: 0.00012221
Iteration 50/1000 | Loss: 0.00013962
Iteration 51/1000 | Loss: 0.00012049
Iteration 52/1000 | Loss: 0.00012156
Iteration 53/1000 | Loss: 0.00011575
Iteration 54/1000 | Loss: 0.00023686
Iteration 55/1000 | Loss: 0.00132745
Iteration 56/1000 | Loss: 0.00052936
Iteration 57/1000 | Loss: 0.00041456
Iteration 58/1000 | Loss: 0.00040003
Iteration 59/1000 | Loss: 0.00029873
Iteration 60/1000 | Loss: 0.00024178
Iteration 61/1000 | Loss: 0.00011810
Iteration 62/1000 | Loss: 0.00058950
Iteration 63/1000 | Loss: 0.00043464
Iteration 64/1000 | Loss: 0.00091451
Iteration 65/1000 | Loss: 0.00017856
Iteration 66/1000 | Loss: 0.00014041
Iteration 67/1000 | Loss: 0.00016619
Iteration 68/1000 | Loss: 0.00022546
Iteration 69/1000 | Loss: 0.00028938
Iteration 70/1000 | Loss: 0.00012598
Iteration 71/1000 | Loss: 0.00035011
Iteration 72/1000 | Loss: 0.00011574
Iteration 73/1000 | Loss: 0.00042641
Iteration 74/1000 | Loss: 0.00037988
Iteration 75/1000 | Loss: 0.00035811
Iteration 76/1000 | Loss: 0.00067517
Iteration 77/1000 | Loss: 0.00039690
Iteration 78/1000 | Loss: 0.00029789
Iteration 79/1000 | Loss: 0.00073081
Iteration 80/1000 | Loss: 0.00026091
Iteration 81/1000 | Loss: 0.00010193
Iteration 82/1000 | Loss: 0.00009124
Iteration 83/1000 | Loss: 0.00016361
Iteration 84/1000 | Loss: 0.00015620
Iteration 85/1000 | Loss: 0.00059955
Iteration 86/1000 | Loss: 0.00043236
Iteration 87/1000 | Loss: 0.00011477
Iteration 88/1000 | Loss: 0.00022501
Iteration 89/1000 | Loss: 0.00010820
Iteration 90/1000 | Loss: 0.00016522
Iteration 91/1000 | Loss: 0.00008823
Iteration 92/1000 | Loss: 0.00039366
Iteration 93/1000 | Loss: 0.00039179
Iteration 94/1000 | Loss: 0.00030154
Iteration 95/1000 | Loss: 0.00011597
Iteration 96/1000 | Loss: 0.00033375
Iteration 97/1000 | Loss: 0.00041801
Iteration 98/1000 | Loss: 0.00017317
Iteration 99/1000 | Loss: 0.00007027
Iteration 100/1000 | Loss: 0.00006867
Iteration 101/1000 | Loss: 0.00012795
Iteration 102/1000 | Loss: 0.00012541
Iteration 103/1000 | Loss: 0.00007685
Iteration 104/1000 | Loss: 0.00064560
Iteration 105/1000 | Loss: 0.00007298
Iteration 106/1000 | Loss: 0.00006654
Iteration 107/1000 | Loss: 0.00006297
Iteration 108/1000 | Loss: 0.00007002
Iteration 109/1000 | Loss: 0.00011066
Iteration 110/1000 | Loss: 0.00065177
Iteration 111/1000 | Loss: 0.00007290
Iteration 112/1000 | Loss: 0.00006057
Iteration 113/1000 | Loss: 0.00005949
Iteration 114/1000 | Loss: 0.00005499
Iteration 115/1000 | Loss: 0.00005413
Iteration 116/1000 | Loss: 0.00006866
Iteration 117/1000 | Loss: 0.00005179
Iteration 118/1000 | Loss: 0.00004957
Iteration 119/1000 | Loss: 0.00037272
Iteration 120/1000 | Loss: 0.00147396
Iteration 121/1000 | Loss: 0.00107944
Iteration 122/1000 | Loss: 0.00206080
Iteration 123/1000 | Loss: 0.00044861
Iteration 124/1000 | Loss: 0.00005987
Iteration 125/1000 | Loss: 0.00014566
Iteration 126/1000 | Loss: 0.00063922
Iteration 127/1000 | Loss: 0.00036955
Iteration 128/1000 | Loss: 0.00005218
Iteration 129/1000 | Loss: 0.00004190
Iteration 130/1000 | Loss: 0.00003819
Iteration 131/1000 | Loss: 0.00050263
Iteration 132/1000 | Loss: 0.00007046
Iteration 133/1000 | Loss: 0.00018490
Iteration 134/1000 | Loss: 0.00005368
Iteration 135/1000 | Loss: 0.00003978
Iteration 136/1000 | Loss: 0.00003677
Iteration 137/1000 | Loss: 0.00003573
Iteration 138/1000 | Loss: 0.00003458
Iteration 139/1000 | Loss: 0.00003313
Iteration 140/1000 | Loss: 0.00003234
Iteration 141/1000 | Loss: 0.00003172
Iteration 142/1000 | Loss: 0.00003240
Iteration 143/1000 | Loss: 0.00003142
Iteration 144/1000 | Loss: 0.00003266
Iteration 145/1000 | Loss: 0.00003163
Iteration 146/1000 | Loss: 0.00003105
Iteration 147/1000 | Loss: 0.00027706
Iteration 148/1000 | Loss: 0.00023472
Iteration 149/1000 | Loss: 0.00004431
Iteration 150/1000 | Loss: 0.00003153
Iteration 151/1000 | Loss: 0.00003026
Iteration 152/1000 | Loss: 0.00002839
Iteration 153/1000 | Loss: 0.00002765
Iteration 154/1000 | Loss: 0.00002701
Iteration 155/1000 | Loss: 0.00002697
Iteration 156/1000 | Loss: 0.00002677
Iteration 157/1000 | Loss: 0.00002670
Iteration 158/1000 | Loss: 0.00002670
Iteration 159/1000 | Loss: 0.00002656
Iteration 160/1000 | Loss: 0.00002653
Iteration 161/1000 | Loss: 0.00002652
Iteration 162/1000 | Loss: 0.00002652
Iteration 163/1000 | Loss: 0.00002651
Iteration 164/1000 | Loss: 0.00002647
Iteration 165/1000 | Loss: 0.00002642
Iteration 166/1000 | Loss: 0.00002642
Iteration 167/1000 | Loss: 0.00002640
Iteration 168/1000 | Loss: 0.00002639
Iteration 169/1000 | Loss: 0.00002639
Iteration 170/1000 | Loss: 0.00002638
Iteration 171/1000 | Loss: 0.00002638
Iteration 172/1000 | Loss: 0.00002637
Iteration 173/1000 | Loss: 0.00002637
Iteration 174/1000 | Loss: 0.00002636
Iteration 175/1000 | Loss: 0.00002635
Iteration 176/1000 | Loss: 0.00002635
Iteration 177/1000 | Loss: 0.00002635
Iteration 178/1000 | Loss: 0.00002635
Iteration 179/1000 | Loss: 0.00002635
Iteration 180/1000 | Loss: 0.00002635
Iteration 181/1000 | Loss: 0.00002635
Iteration 182/1000 | Loss: 0.00002635
Iteration 183/1000 | Loss: 0.00002635
Iteration 184/1000 | Loss: 0.00002635
Iteration 185/1000 | Loss: 0.00002635
Iteration 186/1000 | Loss: 0.00002634
Iteration 187/1000 | Loss: 0.00002634
Iteration 188/1000 | Loss: 0.00002634
Iteration 189/1000 | Loss: 0.00002634
Iteration 190/1000 | Loss: 0.00002632
Iteration 191/1000 | Loss: 0.00002631
Iteration 192/1000 | Loss: 0.00002631
Iteration 193/1000 | Loss: 0.00002631
Iteration 194/1000 | Loss: 0.00002631
Iteration 195/1000 | Loss: 0.00002631
Iteration 196/1000 | Loss: 0.00002631
Iteration 197/1000 | Loss: 0.00002631
Iteration 198/1000 | Loss: 0.00002631
Iteration 199/1000 | Loss: 0.00002631
Iteration 200/1000 | Loss: 0.00002631
Iteration 201/1000 | Loss: 0.00002631
Iteration 202/1000 | Loss: 0.00002630
Iteration 203/1000 | Loss: 0.00002630
Iteration 204/1000 | Loss: 0.00002630
Iteration 205/1000 | Loss: 0.00002630
Iteration 206/1000 | Loss: 0.00002629
Iteration 207/1000 | Loss: 0.00002629
Iteration 208/1000 | Loss: 0.00002629
Iteration 209/1000 | Loss: 0.00002629
Iteration 210/1000 | Loss: 0.00002629
Iteration 211/1000 | Loss: 0.00002629
Iteration 212/1000 | Loss: 0.00002629
Iteration 213/1000 | Loss: 0.00020562
Iteration 214/1000 | Loss: 0.00003169
Iteration 215/1000 | Loss: 0.00002986
Iteration 216/1000 | Loss: 0.00002838
Iteration 217/1000 | Loss: 0.00002790
Iteration 218/1000 | Loss: 0.00002764
Iteration 219/1000 | Loss: 0.00024241
Iteration 220/1000 | Loss: 0.00002959
Iteration 221/1000 | Loss: 0.00002756
Iteration 222/1000 | Loss: 0.00002685
Iteration 223/1000 | Loss: 0.00002568
Iteration 224/1000 | Loss: 0.00002519
Iteration 225/1000 | Loss: 0.00002483
Iteration 226/1000 | Loss: 0.00002481
Iteration 227/1000 | Loss: 0.00002479
Iteration 228/1000 | Loss: 0.00002478
Iteration 229/1000 | Loss: 0.00002478
Iteration 230/1000 | Loss: 0.00002477
Iteration 231/1000 | Loss: 0.00002468
Iteration 232/1000 | Loss: 0.00002460
Iteration 233/1000 | Loss: 0.00002460
Iteration 234/1000 | Loss: 0.00002460
Iteration 235/1000 | Loss: 0.00002459
Iteration 236/1000 | Loss: 0.00002458
Iteration 237/1000 | Loss: 0.00002454
Iteration 238/1000 | Loss: 0.00002453
Iteration 239/1000 | Loss: 0.00002451
Iteration 240/1000 | Loss: 0.00002450
Iteration 241/1000 | Loss: 0.00002450
Iteration 242/1000 | Loss: 0.00002450
Iteration 243/1000 | Loss: 0.00002449
Iteration 244/1000 | Loss: 0.00002449
Iteration 245/1000 | Loss: 0.00002449
Iteration 246/1000 | Loss: 0.00002448
Iteration 247/1000 | Loss: 0.00002448
Iteration 248/1000 | Loss: 0.00002448
Iteration 249/1000 | Loss: 0.00002447
Iteration 250/1000 | Loss: 0.00002447
Iteration 251/1000 | Loss: 0.00002447
Iteration 252/1000 | Loss: 0.00002447
Iteration 253/1000 | Loss: 0.00002447
Iteration 254/1000 | Loss: 0.00002447
Iteration 255/1000 | Loss: 0.00002447
Iteration 256/1000 | Loss: 0.00002447
Iteration 257/1000 | Loss: 0.00002446
Iteration 258/1000 | Loss: 0.00002446
Iteration 259/1000 | Loss: 0.00002446
Iteration 260/1000 | Loss: 0.00002445
Iteration 261/1000 | Loss: 0.00002445
Iteration 262/1000 | Loss: 0.00002445
Iteration 263/1000 | Loss: 0.00002445
Iteration 264/1000 | Loss: 0.00002445
Iteration 265/1000 | Loss: 0.00002445
Iteration 266/1000 | Loss: 0.00002444
Iteration 267/1000 | Loss: 0.00002444
Iteration 268/1000 | Loss: 0.00002444
Iteration 269/1000 | Loss: 0.00002444
Iteration 270/1000 | Loss: 0.00002444
Iteration 271/1000 | Loss: 0.00002444
Iteration 272/1000 | Loss: 0.00002444
Iteration 273/1000 | Loss: 0.00002443
Iteration 274/1000 | Loss: 0.00002443
Iteration 275/1000 | Loss: 0.00002443
Iteration 276/1000 | Loss: 0.00002442
Iteration 277/1000 | Loss: 0.00002442
Iteration 278/1000 | Loss: 0.00002442
Iteration 279/1000 | Loss: 0.00002442
Iteration 280/1000 | Loss: 0.00002441
Iteration 281/1000 | Loss: 0.00002441
Iteration 282/1000 | Loss: 0.00002440
Iteration 283/1000 | Loss: 0.00002440
Iteration 284/1000 | Loss: 0.00002440
Iteration 285/1000 | Loss: 0.00002440
Iteration 286/1000 | Loss: 0.00002440
Iteration 287/1000 | Loss: 0.00002440
Iteration 288/1000 | Loss: 0.00002439
Iteration 289/1000 | Loss: 0.00002439
Iteration 290/1000 | Loss: 0.00002439
Iteration 291/1000 | Loss: 0.00002439
Iteration 292/1000 | Loss: 0.00002439
Iteration 293/1000 | Loss: 0.00002439
Iteration 294/1000 | Loss: 0.00002439
Iteration 295/1000 | Loss: 0.00002439
Iteration 296/1000 | Loss: 0.00002439
Iteration 297/1000 | Loss: 0.00002439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [2.438989577058237e-05, 2.438989577058237e-05, 2.438989577058237e-05, 2.438989577058237e-05, 2.438989577058237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.438989577058237e-05

Optimization complete. Final v2v error: 3.2695212364196777 mm

Highest mean error: 11.356954574584961 mm for frame 203

Lowest mean error: 2.5805883407592773 mm for frame 144

Saving results

Total time: 333.68275260925293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00690902
Iteration 2/25 | Loss: 0.00180763
Iteration 3/25 | Loss: 0.00130668
Iteration 4/25 | Loss: 0.00094136
Iteration 5/25 | Loss: 0.00085105
Iteration 6/25 | Loss: 0.00083814
Iteration 7/25 | Loss: 0.00083499
Iteration 8/25 | Loss: 0.00083040
Iteration 9/25 | Loss: 0.00082780
Iteration 10/25 | Loss: 0.00082724
Iteration 11/25 | Loss: 0.00082639
Iteration 12/25 | Loss: 0.00082515
Iteration 13/25 | Loss: 0.00084137
Iteration 14/25 | Loss: 0.00081902
Iteration 15/25 | Loss: 0.00081706
Iteration 16/25 | Loss: 0.00081680
Iteration 17/25 | Loss: 0.00081680
Iteration 18/25 | Loss: 0.00081680
Iteration 19/25 | Loss: 0.00081680
Iteration 20/25 | Loss: 0.00081680
Iteration 21/25 | Loss: 0.00081680
Iteration 22/25 | Loss: 0.00081680
Iteration 23/25 | Loss: 0.00081680
Iteration 24/25 | Loss: 0.00081680
Iteration 25/25 | Loss: 0.00081680

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29530072
Iteration 2/25 | Loss: 0.00055783
Iteration 3/25 | Loss: 0.00055783
Iteration 4/25 | Loss: 0.00055299
Iteration 5/25 | Loss: 0.00055299
Iteration 6/25 | Loss: 0.00055299
Iteration 7/25 | Loss: 0.00055299
Iteration 8/25 | Loss: 0.00055299
Iteration 9/25 | Loss: 0.00055299
Iteration 10/25 | Loss: 0.00055299
Iteration 11/25 | Loss: 0.00055299
Iteration 12/25 | Loss: 0.00055299
Iteration 13/25 | Loss: 0.00055299
Iteration 14/25 | Loss: 0.00055299
Iteration 15/25 | Loss: 0.00055299
Iteration 16/25 | Loss: 0.00055299
Iteration 17/25 | Loss: 0.00055299
Iteration 18/25 | Loss: 0.00055299
Iteration 19/25 | Loss: 0.00055299
Iteration 20/25 | Loss: 0.00055299
Iteration 21/25 | Loss: 0.00055299
Iteration 22/25 | Loss: 0.00055299
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000552987854462117, 0.000552987854462117, 0.000552987854462117, 0.000552987854462117, 0.000552987854462117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000552987854462117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055299
Iteration 2/1000 | Loss: 0.00004684
Iteration 3/1000 | Loss: 0.00002703
Iteration 4/1000 | Loss: 0.00002899
Iteration 5/1000 | Loss: 0.00002558
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002143
Iteration 8/1000 | Loss: 0.00002097
Iteration 9/1000 | Loss: 0.00002053
Iteration 10/1000 | Loss: 0.00002021
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001974
Iteration 13/1000 | Loss: 0.00001953
Iteration 14/1000 | Loss: 0.00001935
Iteration 15/1000 | Loss: 0.00001930
Iteration 16/1000 | Loss: 0.00001926
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001923
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001922
Iteration 21/1000 | Loss: 0.00001921
Iteration 22/1000 | Loss: 0.00001921
Iteration 23/1000 | Loss: 0.00001919
Iteration 24/1000 | Loss: 0.00001918
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001917
Iteration 27/1000 | Loss: 0.00001917
Iteration 28/1000 | Loss: 0.00001917
Iteration 29/1000 | Loss: 0.00001917
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00001916
Iteration 32/1000 | Loss: 0.00001916
Iteration 33/1000 | Loss: 0.00001916
Iteration 34/1000 | Loss: 0.00001916
Iteration 35/1000 | Loss: 0.00001915
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001913
Iteration 38/1000 | Loss: 0.00001913
Iteration 39/1000 | Loss: 0.00001912
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001911
Iteration 45/1000 | Loss: 0.00001911
Iteration 46/1000 | Loss: 0.00001911
Iteration 47/1000 | Loss: 0.00001911
Iteration 48/1000 | Loss: 0.00001910
Iteration 49/1000 | Loss: 0.00001910
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001909
Iteration 52/1000 | Loss: 0.00001909
Iteration 53/1000 | Loss: 0.00001909
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001908
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001904
Iteration 68/1000 | Loss: 0.00001904
Iteration 69/1000 | Loss: 0.00001904
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001903
Iteration 73/1000 | Loss: 0.00001903
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001902
Iteration 76/1000 | Loss: 0.00001902
Iteration 77/1000 | Loss: 0.00001902
Iteration 78/1000 | Loss: 0.00001902
Iteration 79/1000 | Loss: 0.00001902
Iteration 80/1000 | Loss: 0.00001901
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001901
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001900
Iteration 86/1000 | Loss: 0.00001899
Iteration 87/1000 | Loss: 0.00001899
Iteration 88/1000 | Loss: 0.00001899
Iteration 89/1000 | Loss: 0.00001899
Iteration 90/1000 | Loss: 0.00001899
Iteration 91/1000 | Loss: 0.00001899
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001898
Iteration 94/1000 | Loss: 0.00001898
Iteration 95/1000 | Loss: 0.00001898
Iteration 96/1000 | Loss: 0.00001898
Iteration 97/1000 | Loss: 0.00001898
Iteration 98/1000 | Loss: 0.00001898
Iteration 99/1000 | Loss: 0.00001897
Iteration 100/1000 | Loss: 0.00001897
Iteration 101/1000 | Loss: 0.00001897
Iteration 102/1000 | Loss: 0.00001897
Iteration 103/1000 | Loss: 0.00001897
Iteration 104/1000 | Loss: 0.00001897
Iteration 105/1000 | Loss: 0.00001897
Iteration 106/1000 | Loss: 0.00001897
Iteration 107/1000 | Loss: 0.00001897
Iteration 108/1000 | Loss: 0.00001897
Iteration 109/1000 | Loss: 0.00001897
Iteration 110/1000 | Loss: 0.00001896
Iteration 111/1000 | Loss: 0.00001896
Iteration 112/1000 | Loss: 0.00001896
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001896
Iteration 116/1000 | Loss: 0.00001896
Iteration 117/1000 | Loss: 0.00001896
Iteration 118/1000 | Loss: 0.00001896
Iteration 119/1000 | Loss: 0.00001896
Iteration 120/1000 | Loss: 0.00001895
Iteration 121/1000 | Loss: 0.00001895
Iteration 122/1000 | Loss: 0.00001895
Iteration 123/1000 | Loss: 0.00001895
Iteration 124/1000 | Loss: 0.00001895
Iteration 125/1000 | Loss: 0.00001895
Iteration 126/1000 | Loss: 0.00001894
Iteration 127/1000 | Loss: 0.00001894
Iteration 128/1000 | Loss: 0.00001894
Iteration 129/1000 | Loss: 0.00001894
Iteration 130/1000 | Loss: 0.00001893
Iteration 131/1000 | Loss: 0.00001893
Iteration 132/1000 | Loss: 0.00001893
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001892
Iteration 136/1000 | Loss: 0.00001892
Iteration 137/1000 | Loss: 0.00001892
Iteration 138/1000 | Loss: 0.00001892
Iteration 139/1000 | Loss: 0.00001891
Iteration 140/1000 | Loss: 0.00001891
Iteration 141/1000 | Loss: 0.00001891
Iteration 142/1000 | Loss: 0.00001890
Iteration 143/1000 | Loss: 0.00001890
Iteration 144/1000 | Loss: 0.00001890
Iteration 145/1000 | Loss: 0.00001890
Iteration 146/1000 | Loss: 0.00001890
Iteration 147/1000 | Loss: 0.00001890
Iteration 148/1000 | Loss: 0.00001889
Iteration 149/1000 | Loss: 0.00001889
Iteration 150/1000 | Loss: 0.00001889
Iteration 151/1000 | Loss: 0.00001889
Iteration 152/1000 | Loss: 0.00001889
Iteration 153/1000 | Loss: 0.00001889
Iteration 154/1000 | Loss: 0.00001889
Iteration 155/1000 | Loss: 0.00001889
Iteration 156/1000 | Loss: 0.00001889
Iteration 157/1000 | Loss: 0.00001889
Iteration 158/1000 | Loss: 0.00001889
Iteration 159/1000 | Loss: 0.00001888
Iteration 160/1000 | Loss: 0.00001888
Iteration 161/1000 | Loss: 0.00001888
Iteration 162/1000 | Loss: 0.00001888
Iteration 163/1000 | Loss: 0.00001888
Iteration 164/1000 | Loss: 0.00001888
Iteration 165/1000 | Loss: 0.00001888
Iteration 166/1000 | Loss: 0.00001888
Iteration 167/1000 | Loss: 0.00001888
Iteration 168/1000 | Loss: 0.00001887
Iteration 169/1000 | Loss: 0.00001887
Iteration 170/1000 | Loss: 0.00001887
Iteration 171/1000 | Loss: 0.00001887
Iteration 172/1000 | Loss: 0.00001887
Iteration 173/1000 | Loss: 0.00001887
Iteration 174/1000 | Loss: 0.00001887
Iteration 175/1000 | Loss: 0.00001886
Iteration 176/1000 | Loss: 0.00001886
Iteration 177/1000 | Loss: 0.00001886
Iteration 178/1000 | Loss: 0.00001886
Iteration 179/1000 | Loss: 0.00001886
Iteration 180/1000 | Loss: 0.00001886
Iteration 181/1000 | Loss: 0.00001886
Iteration 182/1000 | Loss: 0.00001886
Iteration 183/1000 | Loss: 0.00001886
Iteration 184/1000 | Loss: 0.00001885
Iteration 185/1000 | Loss: 0.00001885
Iteration 186/1000 | Loss: 0.00001885
Iteration 187/1000 | Loss: 0.00001885
Iteration 188/1000 | Loss: 0.00001884
Iteration 189/1000 | Loss: 0.00001884
Iteration 190/1000 | Loss: 0.00001884
Iteration 191/1000 | Loss: 0.00001884
Iteration 192/1000 | Loss: 0.00001884
Iteration 193/1000 | Loss: 0.00001884
Iteration 194/1000 | Loss: 0.00001884
Iteration 195/1000 | Loss: 0.00001883
Iteration 196/1000 | Loss: 0.00001883
Iteration 197/1000 | Loss: 0.00001883
Iteration 198/1000 | Loss: 0.00001883
Iteration 199/1000 | Loss: 0.00001883
Iteration 200/1000 | Loss: 0.00001883
Iteration 201/1000 | Loss: 0.00001883
Iteration 202/1000 | Loss: 0.00001883
Iteration 203/1000 | Loss: 0.00001883
Iteration 204/1000 | Loss: 0.00001883
Iteration 205/1000 | Loss: 0.00001883
Iteration 206/1000 | Loss: 0.00001883
Iteration 207/1000 | Loss: 0.00001883
Iteration 208/1000 | Loss: 0.00001883
Iteration 209/1000 | Loss: 0.00001883
Iteration 210/1000 | Loss: 0.00001883
Iteration 211/1000 | Loss: 0.00001883
Iteration 212/1000 | Loss: 0.00001883
Iteration 213/1000 | Loss: 0.00001883
Iteration 214/1000 | Loss: 0.00001883
Iteration 215/1000 | Loss: 0.00001883
Iteration 216/1000 | Loss: 0.00001883
Iteration 217/1000 | Loss: 0.00001883
Iteration 218/1000 | Loss: 0.00001883
Iteration 219/1000 | Loss: 0.00001883
Iteration 220/1000 | Loss: 0.00001883
Iteration 221/1000 | Loss: 0.00001883
Iteration 222/1000 | Loss: 0.00001883
Iteration 223/1000 | Loss: 0.00001883
Iteration 224/1000 | Loss: 0.00001883
Iteration 225/1000 | Loss: 0.00001883
Iteration 226/1000 | Loss: 0.00001883
Iteration 227/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.8829983673640527e-05, 1.8829983673640527e-05, 1.8829983673640527e-05, 1.8829983673640527e-05, 1.8829983673640527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8829983673640527e-05

Optimization complete. Final v2v error: 3.5093398094177246 mm

Highest mean error: 5.734536170959473 mm for frame 12

Lowest mean error: 2.5991599559783936 mm for frame 178

Saving results

Total time: 71.47757148742676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415449
Iteration 2/25 | Loss: 0.00096361
Iteration 3/25 | Loss: 0.00079357
Iteration 4/25 | Loss: 0.00075895
Iteration 5/25 | Loss: 0.00075167
Iteration 6/25 | Loss: 0.00074999
Iteration 7/25 | Loss: 0.00074948
Iteration 8/25 | Loss: 0.00074937
Iteration 9/25 | Loss: 0.00074937
Iteration 10/25 | Loss: 0.00074937
Iteration 11/25 | Loss: 0.00074937
Iteration 12/25 | Loss: 0.00074937
Iteration 13/25 | Loss: 0.00074937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007493719458580017, 0.0007493719458580017, 0.0007493719458580017, 0.0007493719458580017, 0.0007493719458580017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007493719458580017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57853997
Iteration 2/25 | Loss: 0.00053775
Iteration 3/25 | Loss: 0.00053775
Iteration 4/25 | Loss: 0.00053775
Iteration 5/25 | Loss: 0.00053775
Iteration 6/25 | Loss: 0.00053775
Iteration 7/25 | Loss: 0.00053775
Iteration 8/25 | Loss: 0.00053775
Iteration 9/25 | Loss: 0.00053775
Iteration 10/25 | Loss: 0.00053775
Iteration 11/25 | Loss: 0.00053775
Iteration 12/25 | Loss: 0.00053775
Iteration 13/25 | Loss: 0.00053775
Iteration 14/25 | Loss: 0.00053775
Iteration 15/25 | Loss: 0.00053775
Iteration 16/25 | Loss: 0.00053775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005377517663873732, 0.0005377517663873732, 0.0005377517663873732, 0.0005377517663873732, 0.0005377517663873732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005377517663873732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053775
Iteration 2/1000 | Loss: 0.00002879
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001611
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001406
Iteration 9/1000 | Loss: 0.00001385
Iteration 10/1000 | Loss: 0.00001385
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001365
Iteration 13/1000 | Loss: 0.00001363
Iteration 14/1000 | Loss: 0.00001363
Iteration 15/1000 | Loss: 0.00001362
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001359
Iteration 18/1000 | Loss: 0.00001359
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001356
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001348
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001342
Iteration 29/1000 | Loss: 0.00001340
Iteration 30/1000 | Loss: 0.00001336
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001334
Iteration 39/1000 | Loss: 0.00001334
Iteration 40/1000 | Loss: 0.00001334
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001333
Iteration 44/1000 | Loss: 0.00001333
Iteration 45/1000 | Loss: 0.00001333
Iteration 46/1000 | Loss: 0.00001333
Iteration 47/1000 | Loss: 0.00001333
Iteration 48/1000 | Loss: 0.00001333
Iteration 49/1000 | Loss: 0.00001333
Iteration 50/1000 | Loss: 0.00001332
Iteration 51/1000 | Loss: 0.00001332
Iteration 52/1000 | Loss: 0.00001332
Iteration 53/1000 | Loss: 0.00001331
Iteration 54/1000 | Loss: 0.00001330
Iteration 55/1000 | Loss: 0.00001330
Iteration 56/1000 | Loss: 0.00001329
Iteration 57/1000 | Loss: 0.00001329
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001326
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001324
Iteration 76/1000 | Loss: 0.00001324
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001322
Iteration 80/1000 | Loss: 0.00001322
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001322
Iteration 83/1000 | Loss: 0.00001322
Iteration 84/1000 | Loss: 0.00001321
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001321
Iteration 87/1000 | Loss: 0.00001321
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001320
Iteration 90/1000 | Loss: 0.00001320
Iteration 91/1000 | Loss: 0.00001320
Iteration 92/1000 | Loss: 0.00001320
Iteration 93/1000 | Loss: 0.00001320
Iteration 94/1000 | Loss: 0.00001320
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00001319
Iteration 97/1000 | Loss: 0.00001319
Iteration 98/1000 | Loss: 0.00001319
Iteration 99/1000 | Loss: 0.00001319
Iteration 100/1000 | Loss: 0.00001319
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001318
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001318
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001317
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001316
Iteration 130/1000 | Loss: 0.00001316
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001316
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001315
Iteration 143/1000 | Loss: 0.00001315
Iteration 144/1000 | Loss: 0.00001315
Iteration 145/1000 | Loss: 0.00001315
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001312
Iteration 155/1000 | Loss: 0.00001312
Iteration 156/1000 | Loss: 0.00001312
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001311
Iteration 159/1000 | Loss: 0.00001311
Iteration 160/1000 | Loss: 0.00001311
Iteration 161/1000 | Loss: 0.00001311
Iteration 162/1000 | Loss: 0.00001311
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001310
Iteration 165/1000 | Loss: 0.00001310
Iteration 166/1000 | Loss: 0.00001310
Iteration 167/1000 | Loss: 0.00001310
Iteration 168/1000 | Loss: 0.00001310
Iteration 169/1000 | Loss: 0.00001310
Iteration 170/1000 | Loss: 0.00001309
Iteration 171/1000 | Loss: 0.00001309
Iteration 172/1000 | Loss: 0.00001309
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001309
Iteration 175/1000 | Loss: 0.00001309
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001308
Iteration 186/1000 | Loss: 0.00001308
Iteration 187/1000 | Loss: 0.00001308
Iteration 188/1000 | Loss: 0.00001308
Iteration 189/1000 | Loss: 0.00001308
Iteration 190/1000 | Loss: 0.00001308
Iteration 191/1000 | Loss: 0.00001308
Iteration 192/1000 | Loss: 0.00001308
Iteration 193/1000 | Loss: 0.00001308
Iteration 194/1000 | Loss: 0.00001308
Iteration 195/1000 | Loss: 0.00001308
Iteration 196/1000 | Loss: 0.00001308
Iteration 197/1000 | Loss: 0.00001307
Iteration 198/1000 | Loss: 0.00001307
Iteration 199/1000 | Loss: 0.00001307
Iteration 200/1000 | Loss: 0.00001307
Iteration 201/1000 | Loss: 0.00001307
Iteration 202/1000 | Loss: 0.00001307
Iteration 203/1000 | Loss: 0.00001307
Iteration 204/1000 | Loss: 0.00001307
Iteration 205/1000 | Loss: 0.00001307
Iteration 206/1000 | Loss: 0.00001307
Iteration 207/1000 | Loss: 0.00001307
Iteration 208/1000 | Loss: 0.00001307
Iteration 209/1000 | Loss: 0.00001307
Iteration 210/1000 | Loss: 0.00001307
Iteration 211/1000 | Loss: 0.00001307
Iteration 212/1000 | Loss: 0.00001307
Iteration 213/1000 | Loss: 0.00001307
Iteration 214/1000 | Loss: 0.00001307
Iteration 215/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.307004458794836e-05, 1.307004458794836e-05, 1.307004458794836e-05, 1.307004458794836e-05, 1.307004458794836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.307004458794836e-05

Optimization complete. Final v2v error: 3.0445942878723145 mm

Highest mean error: 4.178622722625732 mm for frame 32

Lowest mean error: 2.5729849338531494 mm for frame 17

Saving results

Total time: 40.29790711402893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877763
Iteration 2/25 | Loss: 0.00133071
Iteration 3/25 | Loss: 0.00091656
Iteration 4/25 | Loss: 0.00083850
Iteration 5/25 | Loss: 0.00083061
Iteration 6/25 | Loss: 0.00082970
Iteration 7/25 | Loss: 0.00082970
Iteration 8/25 | Loss: 0.00082970
Iteration 9/25 | Loss: 0.00082970
Iteration 10/25 | Loss: 0.00082970
Iteration 11/25 | Loss: 0.00082970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000829699682071805, 0.000829699682071805, 0.000829699682071805, 0.000829699682071805, 0.000829699682071805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000829699682071805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01777673
Iteration 2/25 | Loss: 0.00042724
Iteration 3/25 | Loss: 0.00042723
Iteration 4/25 | Loss: 0.00042723
Iteration 5/25 | Loss: 0.00042723
Iteration 6/25 | Loss: 0.00042723
Iteration 7/25 | Loss: 0.00042723
Iteration 8/25 | Loss: 0.00042723
Iteration 9/25 | Loss: 0.00042723
Iteration 10/25 | Loss: 0.00042723
Iteration 11/25 | Loss: 0.00042722
Iteration 12/25 | Loss: 0.00042722
Iteration 13/25 | Loss: 0.00042722
Iteration 14/25 | Loss: 0.00042722
Iteration 15/25 | Loss: 0.00042722
Iteration 16/25 | Loss: 0.00042722
Iteration 17/25 | Loss: 0.00042722
Iteration 18/25 | Loss: 0.00042722
Iteration 19/25 | Loss: 0.00042722
Iteration 20/25 | Loss: 0.00042722
Iteration 21/25 | Loss: 0.00042722
Iteration 22/25 | Loss: 0.00042722
Iteration 23/25 | Loss: 0.00042722
Iteration 24/25 | Loss: 0.00042722
Iteration 25/25 | Loss: 0.00042722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042722
Iteration 2/1000 | Loss: 0.00003607
Iteration 3/1000 | Loss: 0.00002851
Iteration 4/1000 | Loss: 0.00002622
Iteration 5/1000 | Loss: 0.00002481
Iteration 6/1000 | Loss: 0.00002388
Iteration 7/1000 | Loss: 0.00002313
Iteration 8/1000 | Loss: 0.00002268
Iteration 9/1000 | Loss: 0.00002236
Iteration 10/1000 | Loss: 0.00002228
Iteration 11/1000 | Loss: 0.00002211
Iteration 12/1000 | Loss: 0.00002199
Iteration 13/1000 | Loss: 0.00002198
Iteration 14/1000 | Loss: 0.00002197
Iteration 15/1000 | Loss: 0.00002197
Iteration 16/1000 | Loss: 0.00002196
Iteration 17/1000 | Loss: 0.00002196
Iteration 18/1000 | Loss: 0.00002196
Iteration 19/1000 | Loss: 0.00002195
Iteration 20/1000 | Loss: 0.00002194
Iteration 21/1000 | Loss: 0.00002194
Iteration 22/1000 | Loss: 0.00002193
Iteration 23/1000 | Loss: 0.00002193
Iteration 24/1000 | Loss: 0.00002193
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002191
Iteration 27/1000 | Loss: 0.00002191
Iteration 28/1000 | Loss: 0.00002190
Iteration 29/1000 | Loss: 0.00002190
Iteration 30/1000 | Loss: 0.00002190
Iteration 31/1000 | Loss: 0.00002190
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002190
Iteration 34/1000 | Loss: 0.00002190
Iteration 35/1000 | Loss: 0.00002190
Iteration 36/1000 | Loss: 0.00002190
Iteration 37/1000 | Loss: 0.00002190
Iteration 38/1000 | Loss: 0.00002190
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002189
Iteration 41/1000 | Loss: 0.00002189
Iteration 42/1000 | Loss: 0.00002189
Iteration 43/1000 | Loss: 0.00002189
Iteration 44/1000 | Loss: 0.00002188
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002186
Iteration 48/1000 | Loss: 0.00002184
Iteration 49/1000 | Loss: 0.00002184
Iteration 50/1000 | Loss: 0.00002183
Iteration 51/1000 | Loss: 0.00002183
Iteration 52/1000 | Loss: 0.00002183
Iteration 53/1000 | Loss: 0.00002183
Iteration 54/1000 | Loss: 0.00002183
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002183
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00002183
Iteration 59/1000 | Loss: 0.00002183
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00002181
Iteration 67/1000 | Loss: 0.00002181
Iteration 68/1000 | Loss: 0.00002181
Iteration 69/1000 | Loss: 0.00002181
Iteration 70/1000 | Loss: 0.00002181
Iteration 71/1000 | Loss: 0.00002181
Iteration 72/1000 | Loss: 0.00002181
Iteration 73/1000 | Loss: 0.00002180
Iteration 74/1000 | Loss: 0.00002180
Iteration 75/1000 | Loss: 0.00002180
Iteration 76/1000 | Loss: 0.00002180
Iteration 77/1000 | Loss: 0.00002180
Iteration 78/1000 | Loss: 0.00002180
Iteration 79/1000 | Loss: 0.00002180
Iteration 80/1000 | Loss: 0.00002180
Iteration 81/1000 | Loss: 0.00002180
Iteration 82/1000 | Loss: 0.00002180
Iteration 83/1000 | Loss: 0.00002180
Iteration 84/1000 | Loss: 0.00002180
Iteration 85/1000 | Loss: 0.00002180
Iteration 86/1000 | Loss: 0.00002180
Iteration 87/1000 | Loss: 0.00002179
Iteration 88/1000 | Loss: 0.00002179
Iteration 89/1000 | Loss: 0.00002179
Iteration 90/1000 | Loss: 0.00002179
Iteration 91/1000 | Loss: 0.00002179
Iteration 92/1000 | Loss: 0.00002179
Iteration 93/1000 | Loss: 0.00002179
Iteration 94/1000 | Loss: 0.00002179
Iteration 95/1000 | Loss: 0.00002178
Iteration 96/1000 | Loss: 0.00002178
Iteration 97/1000 | Loss: 0.00002178
Iteration 98/1000 | Loss: 0.00002178
Iteration 99/1000 | Loss: 0.00002178
Iteration 100/1000 | Loss: 0.00002178
Iteration 101/1000 | Loss: 0.00002178
Iteration 102/1000 | Loss: 0.00002178
Iteration 103/1000 | Loss: 0.00002178
Iteration 104/1000 | Loss: 0.00002178
Iteration 105/1000 | Loss: 0.00002178
Iteration 106/1000 | Loss: 0.00002178
Iteration 107/1000 | Loss: 0.00002178
Iteration 108/1000 | Loss: 0.00002178
Iteration 109/1000 | Loss: 0.00002178
Iteration 110/1000 | Loss: 0.00002178
Iteration 111/1000 | Loss: 0.00002178
Iteration 112/1000 | Loss: 0.00002178
Iteration 113/1000 | Loss: 0.00002178
Iteration 114/1000 | Loss: 0.00002178
Iteration 115/1000 | Loss: 0.00002178
Iteration 116/1000 | Loss: 0.00002178
Iteration 117/1000 | Loss: 0.00002178
Iteration 118/1000 | Loss: 0.00002178
Iteration 119/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.1777055735583417e-05, 2.1777055735583417e-05, 2.1777055735583417e-05, 2.1777055735583417e-05, 2.1777055735583417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1777055735583417e-05

Optimization complete. Final v2v error: 3.95592999458313 mm

Highest mean error: 4.59397554397583 mm for frame 0

Lowest mean error: 3.828641176223755 mm for frame 94

Saving results

Total time: 32.29705214500427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00606733
Iteration 2/25 | Loss: 0.00084180
Iteration 3/25 | Loss: 0.00073422
Iteration 4/25 | Loss: 0.00071633
Iteration 5/25 | Loss: 0.00071012
Iteration 6/25 | Loss: 0.00070838
Iteration 7/25 | Loss: 0.00070793
Iteration 8/25 | Loss: 0.00070793
Iteration 9/25 | Loss: 0.00070793
Iteration 10/25 | Loss: 0.00070793
Iteration 11/25 | Loss: 0.00070793
Iteration 12/25 | Loss: 0.00070793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007079349597916007, 0.0007079349597916007, 0.0007079349597916007, 0.0007079349597916007, 0.0007079349597916007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007079349597916007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04733896
Iteration 2/25 | Loss: 0.00050666
Iteration 3/25 | Loss: 0.00050666
Iteration 4/25 | Loss: 0.00050666
Iteration 5/25 | Loss: 0.00050666
Iteration 6/25 | Loss: 0.00050665
Iteration 7/25 | Loss: 0.00050665
Iteration 8/25 | Loss: 0.00050665
Iteration 9/25 | Loss: 0.00050665
Iteration 10/25 | Loss: 0.00050665
Iteration 11/25 | Loss: 0.00050665
Iteration 12/25 | Loss: 0.00050665
Iteration 13/25 | Loss: 0.00050665
Iteration 14/25 | Loss: 0.00050665
Iteration 15/25 | Loss: 0.00050665
Iteration 16/25 | Loss: 0.00050665
Iteration 17/25 | Loss: 0.00050665
Iteration 18/25 | Loss: 0.00050665
Iteration 19/25 | Loss: 0.00050665
Iteration 20/25 | Loss: 0.00050665
Iteration 21/25 | Loss: 0.00050665
Iteration 22/25 | Loss: 0.00050665
Iteration 23/25 | Loss: 0.00050665
Iteration 24/25 | Loss: 0.00050665
Iteration 25/25 | Loss: 0.00050665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050665
Iteration 2/1000 | Loss: 0.00002433
Iteration 3/1000 | Loss: 0.00001507
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001327
Iteration 6/1000 | Loss: 0.00001293
Iteration 7/1000 | Loss: 0.00001261
Iteration 8/1000 | Loss: 0.00001250
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001239
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001229
Iteration 15/1000 | Loss: 0.00001222
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001203
Iteration 21/1000 | Loss: 0.00001203
Iteration 22/1000 | Loss: 0.00001201
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001195
Iteration 26/1000 | Loss: 0.00001194
Iteration 27/1000 | Loss: 0.00001193
Iteration 28/1000 | Loss: 0.00001187
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001180
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001179
Iteration 43/1000 | Loss: 0.00001179
Iteration 44/1000 | Loss: 0.00001178
Iteration 45/1000 | Loss: 0.00001178
Iteration 46/1000 | Loss: 0.00001177
Iteration 47/1000 | Loss: 0.00001177
Iteration 48/1000 | Loss: 0.00001177
Iteration 49/1000 | Loss: 0.00001177
Iteration 50/1000 | Loss: 0.00001177
Iteration 51/1000 | Loss: 0.00001177
Iteration 52/1000 | Loss: 0.00001176
Iteration 53/1000 | Loss: 0.00001176
Iteration 54/1000 | Loss: 0.00001176
Iteration 55/1000 | Loss: 0.00001176
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001175
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001174
Iteration 63/1000 | Loss: 0.00001174
Iteration 64/1000 | Loss: 0.00001174
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001171
Iteration 68/1000 | Loss: 0.00001170
Iteration 69/1000 | Loss: 0.00001170
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001169
Iteration 72/1000 | Loss: 0.00001169
Iteration 73/1000 | Loss: 0.00001169
Iteration 74/1000 | Loss: 0.00001168
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001157
Iteration 96/1000 | Loss: 0.00001157
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001155
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001154
Iteration 122/1000 | Loss: 0.00001154
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001152
Iteration 135/1000 | Loss: 0.00001152
Iteration 136/1000 | Loss: 0.00001152
Iteration 137/1000 | Loss: 0.00001152
Iteration 138/1000 | Loss: 0.00001152
Iteration 139/1000 | Loss: 0.00001152
Iteration 140/1000 | Loss: 0.00001152
Iteration 141/1000 | Loss: 0.00001152
Iteration 142/1000 | Loss: 0.00001152
Iteration 143/1000 | Loss: 0.00001152
Iteration 144/1000 | Loss: 0.00001152
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001152
Iteration 148/1000 | Loss: 0.00001151
Iteration 149/1000 | Loss: 0.00001151
Iteration 150/1000 | Loss: 0.00001151
Iteration 151/1000 | Loss: 0.00001151
Iteration 152/1000 | Loss: 0.00001151
Iteration 153/1000 | Loss: 0.00001151
Iteration 154/1000 | Loss: 0.00001151
Iteration 155/1000 | Loss: 0.00001151
Iteration 156/1000 | Loss: 0.00001151
Iteration 157/1000 | Loss: 0.00001151
Iteration 158/1000 | Loss: 0.00001151
Iteration 159/1000 | Loss: 0.00001151
Iteration 160/1000 | Loss: 0.00001151
Iteration 161/1000 | Loss: 0.00001150
Iteration 162/1000 | Loss: 0.00001150
Iteration 163/1000 | Loss: 0.00001150
Iteration 164/1000 | Loss: 0.00001150
Iteration 165/1000 | Loss: 0.00001150
Iteration 166/1000 | Loss: 0.00001150
Iteration 167/1000 | Loss: 0.00001150
Iteration 168/1000 | Loss: 0.00001150
Iteration 169/1000 | Loss: 0.00001150
Iteration 170/1000 | Loss: 0.00001150
Iteration 171/1000 | Loss: 0.00001150
Iteration 172/1000 | Loss: 0.00001150
Iteration 173/1000 | Loss: 0.00001150
Iteration 174/1000 | Loss: 0.00001149
Iteration 175/1000 | Loss: 0.00001149
Iteration 176/1000 | Loss: 0.00001149
Iteration 177/1000 | Loss: 0.00001149
Iteration 178/1000 | Loss: 0.00001149
Iteration 179/1000 | Loss: 0.00001149
Iteration 180/1000 | Loss: 0.00001149
Iteration 181/1000 | Loss: 0.00001149
Iteration 182/1000 | Loss: 0.00001149
Iteration 183/1000 | Loss: 0.00001149
Iteration 184/1000 | Loss: 0.00001149
Iteration 185/1000 | Loss: 0.00001149
Iteration 186/1000 | Loss: 0.00001149
Iteration 187/1000 | Loss: 0.00001149
Iteration 188/1000 | Loss: 0.00001149
Iteration 189/1000 | Loss: 0.00001149
Iteration 190/1000 | Loss: 0.00001149
Iteration 191/1000 | Loss: 0.00001149
Iteration 192/1000 | Loss: 0.00001149
Iteration 193/1000 | Loss: 0.00001149
Iteration 194/1000 | Loss: 0.00001149
Iteration 195/1000 | Loss: 0.00001149
Iteration 196/1000 | Loss: 0.00001149
Iteration 197/1000 | Loss: 0.00001149
Iteration 198/1000 | Loss: 0.00001149
Iteration 199/1000 | Loss: 0.00001149
Iteration 200/1000 | Loss: 0.00001149
Iteration 201/1000 | Loss: 0.00001149
Iteration 202/1000 | Loss: 0.00001149
Iteration 203/1000 | Loss: 0.00001149
Iteration 204/1000 | Loss: 0.00001149
Iteration 205/1000 | Loss: 0.00001149
Iteration 206/1000 | Loss: 0.00001149
Iteration 207/1000 | Loss: 0.00001149
Iteration 208/1000 | Loss: 0.00001149
Iteration 209/1000 | Loss: 0.00001149
Iteration 210/1000 | Loss: 0.00001149
Iteration 211/1000 | Loss: 0.00001149
Iteration 212/1000 | Loss: 0.00001149
Iteration 213/1000 | Loss: 0.00001149
Iteration 214/1000 | Loss: 0.00001149
Iteration 215/1000 | Loss: 0.00001149
Iteration 216/1000 | Loss: 0.00001149
Iteration 217/1000 | Loss: 0.00001149
Iteration 218/1000 | Loss: 0.00001149
Iteration 219/1000 | Loss: 0.00001149
Iteration 220/1000 | Loss: 0.00001149
Iteration 221/1000 | Loss: 0.00001149
Iteration 222/1000 | Loss: 0.00001149
Iteration 223/1000 | Loss: 0.00001149
Iteration 224/1000 | Loss: 0.00001149
Iteration 225/1000 | Loss: 0.00001149
Iteration 226/1000 | Loss: 0.00001149
Iteration 227/1000 | Loss: 0.00001149
Iteration 228/1000 | Loss: 0.00001149
Iteration 229/1000 | Loss: 0.00001149
Iteration 230/1000 | Loss: 0.00001149
Iteration 231/1000 | Loss: 0.00001149
Iteration 232/1000 | Loss: 0.00001149
Iteration 233/1000 | Loss: 0.00001149
Iteration 234/1000 | Loss: 0.00001149
Iteration 235/1000 | Loss: 0.00001149
Iteration 236/1000 | Loss: 0.00001149
Iteration 237/1000 | Loss: 0.00001149
Iteration 238/1000 | Loss: 0.00001149
Iteration 239/1000 | Loss: 0.00001149
Iteration 240/1000 | Loss: 0.00001149
Iteration 241/1000 | Loss: 0.00001149
Iteration 242/1000 | Loss: 0.00001149
Iteration 243/1000 | Loss: 0.00001149
Iteration 244/1000 | Loss: 0.00001149
Iteration 245/1000 | Loss: 0.00001149
Iteration 246/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.1487576557556167e-05, 1.1487576557556167e-05, 1.1487576557556167e-05, 1.1487576557556167e-05, 1.1487576557556167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1487576557556167e-05

Optimization complete. Final v2v error: 2.913558006286621 mm

Highest mean error: 3.19364333152771 mm for frame 140

Lowest mean error: 2.727431535720825 mm for frame 165

Saving results

Total time: 39.90382742881775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014130
Iteration 2/25 | Loss: 0.00153919
Iteration 3/25 | Loss: 0.00099442
Iteration 4/25 | Loss: 0.00085443
Iteration 5/25 | Loss: 0.00084522
Iteration 6/25 | Loss: 0.00084436
Iteration 7/25 | Loss: 0.00084436
Iteration 8/25 | Loss: 0.00084436
Iteration 9/25 | Loss: 0.00084436
Iteration 10/25 | Loss: 0.00084436
Iteration 11/25 | Loss: 0.00084436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008443584083579481, 0.0008443584083579481, 0.0008443584083579481, 0.0008443584083579481, 0.0008443584083579481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008443584083579481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49242830
Iteration 2/25 | Loss: 0.00044262
Iteration 3/25 | Loss: 0.00044262
Iteration 4/25 | Loss: 0.00044262
Iteration 5/25 | Loss: 0.00044262
Iteration 6/25 | Loss: 0.00044262
Iteration 7/25 | Loss: 0.00044262
Iteration 8/25 | Loss: 0.00044262
Iteration 9/25 | Loss: 0.00044262
Iteration 10/25 | Loss: 0.00044262
Iteration 11/25 | Loss: 0.00044262
Iteration 12/25 | Loss: 0.00044262
Iteration 13/25 | Loss: 0.00044262
Iteration 14/25 | Loss: 0.00044262
Iteration 15/25 | Loss: 0.00044262
Iteration 16/25 | Loss: 0.00044262
Iteration 17/25 | Loss: 0.00044262
Iteration 18/25 | Loss: 0.00044262
Iteration 19/25 | Loss: 0.00044262
Iteration 20/25 | Loss: 0.00044262
Iteration 21/25 | Loss: 0.00044262
Iteration 22/25 | Loss: 0.00044262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004426177474670112, 0.0004426177474670112, 0.0004426177474670112, 0.0004426177474670112, 0.0004426177474670112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004426177474670112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044262
Iteration 2/1000 | Loss: 0.00003161
Iteration 3/1000 | Loss: 0.00002423
Iteration 4/1000 | Loss: 0.00002294
Iteration 5/1000 | Loss: 0.00002161
Iteration 6/1000 | Loss: 0.00002094
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00002018
Iteration 9/1000 | Loss: 0.00002001
Iteration 10/1000 | Loss: 0.00001997
Iteration 11/1000 | Loss: 0.00001991
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001978
Iteration 15/1000 | Loss: 0.00001977
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001977
Iteration 18/1000 | Loss: 0.00001977
Iteration 19/1000 | Loss: 0.00001976
Iteration 20/1000 | Loss: 0.00001976
Iteration 21/1000 | Loss: 0.00001974
Iteration 22/1000 | Loss: 0.00001973
Iteration 23/1000 | Loss: 0.00001973
Iteration 24/1000 | Loss: 0.00001973
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001973
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001970
Iteration 32/1000 | Loss: 0.00001969
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001967
Iteration 40/1000 | Loss: 0.00001967
Iteration 41/1000 | Loss: 0.00001966
Iteration 42/1000 | Loss: 0.00001966
Iteration 43/1000 | Loss: 0.00001966
Iteration 44/1000 | Loss: 0.00001966
Iteration 45/1000 | Loss: 0.00001966
Iteration 46/1000 | Loss: 0.00001966
Iteration 47/1000 | Loss: 0.00001965
Iteration 48/1000 | Loss: 0.00001965
Iteration 49/1000 | Loss: 0.00001964
Iteration 50/1000 | Loss: 0.00001964
Iteration 51/1000 | Loss: 0.00001964
Iteration 52/1000 | Loss: 0.00001964
Iteration 53/1000 | Loss: 0.00001964
Iteration 54/1000 | Loss: 0.00001964
Iteration 55/1000 | Loss: 0.00001964
Iteration 56/1000 | Loss: 0.00001964
Iteration 57/1000 | Loss: 0.00001964
Iteration 58/1000 | Loss: 0.00001964
Iteration 59/1000 | Loss: 0.00001964
Iteration 60/1000 | Loss: 0.00001964
Iteration 61/1000 | Loss: 0.00001964
Iteration 62/1000 | Loss: 0.00001964
Iteration 63/1000 | Loss: 0.00001964
Iteration 64/1000 | Loss: 0.00001964
Iteration 65/1000 | Loss: 0.00001964
Iteration 66/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.9638424419099465e-05, 1.9638424419099465e-05, 1.9638424419099465e-05, 1.9638424419099465e-05, 1.9638424419099465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9638424419099465e-05

Optimization complete. Final v2v error: 3.653432846069336 mm

Highest mean error: 3.76332426071167 mm for frame 153

Lowest mean error: 3.524092435836792 mm for frame 0

Saving results

Total time: 27.11404800415039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791490
Iteration 2/25 | Loss: 0.00129279
Iteration 3/25 | Loss: 0.00090091
Iteration 4/25 | Loss: 0.00078913
Iteration 5/25 | Loss: 0.00076402
Iteration 6/25 | Loss: 0.00075722
Iteration 7/25 | Loss: 0.00075560
Iteration 8/25 | Loss: 0.00075513
Iteration 9/25 | Loss: 0.00075513
Iteration 10/25 | Loss: 0.00075513
Iteration 11/25 | Loss: 0.00075513
Iteration 12/25 | Loss: 0.00075513
Iteration 13/25 | Loss: 0.00075513
Iteration 14/25 | Loss: 0.00075513
Iteration 15/25 | Loss: 0.00075513
Iteration 16/25 | Loss: 0.00075513
Iteration 17/25 | Loss: 0.00075513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000755129789467901, 0.000755129789467901, 0.000755129789467901, 0.000755129789467901, 0.000755129789467901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000755129789467901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57161951
Iteration 2/25 | Loss: 0.00057792
Iteration 3/25 | Loss: 0.00057792
Iteration 4/25 | Loss: 0.00057791
Iteration 5/25 | Loss: 0.00057791
Iteration 6/25 | Loss: 0.00057791
Iteration 7/25 | Loss: 0.00057791
Iteration 8/25 | Loss: 0.00057791
Iteration 9/25 | Loss: 0.00057791
Iteration 10/25 | Loss: 0.00057791
Iteration 11/25 | Loss: 0.00057791
Iteration 12/25 | Loss: 0.00057791
Iteration 13/25 | Loss: 0.00057791
Iteration 14/25 | Loss: 0.00057791
Iteration 15/25 | Loss: 0.00057791
Iteration 16/25 | Loss: 0.00057791
Iteration 17/25 | Loss: 0.00057791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005779126659035683, 0.0005779126659035683, 0.0005779126659035683, 0.0005779126659035683, 0.0005779126659035683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005779126659035683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057791
Iteration 2/1000 | Loss: 0.00002951
Iteration 3/1000 | Loss: 0.00001933
Iteration 4/1000 | Loss: 0.00001559
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001375
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001267
Iteration 10/1000 | Loss: 0.00001240
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001208
Iteration 18/1000 | Loss: 0.00001207
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001206
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001205
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001201
Iteration 37/1000 | Loss: 0.00001201
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001200
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001196
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001187
Iteration 122/1000 | Loss: 0.00001187
Iteration 123/1000 | Loss: 0.00001187
Iteration 124/1000 | Loss: 0.00001187
Iteration 125/1000 | Loss: 0.00001187
Iteration 126/1000 | Loss: 0.00001187
Iteration 127/1000 | Loss: 0.00001187
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001187
Iteration 130/1000 | Loss: 0.00001187
Iteration 131/1000 | Loss: 0.00001187
Iteration 132/1000 | Loss: 0.00001187
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001187
Iteration 138/1000 | Loss: 0.00001187
Iteration 139/1000 | Loss: 0.00001187
Iteration 140/1000 | Loss: 0.00001187
Iteration 141/1000 | Loss: 0.00001187
Iteration 142/1000 | Loss: 0.00001187
Iteration 143/1000 | Loss: 0.00001187
Iteration 144/1000 | Loss: 0.00001187
Iteration 145/1000 | Loss: 0.00001187
Iteration 146/1000 | Loss: 0.00001187
Iteration 147/1000 | Loss: 0.00001187
Iteration 148/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.1865742635563947e-05, 1.1865742635563947e-05, 1.1865742635563947e-05, 1.1865742635563947e-05, 1.1865742635563947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1865742635563947e-05

Optimization complete. Final v2v error: 2.909454107284546 mm

Highest mean error: 3.519198179244995 mm for frame 59

Lowest mean error: 2.54472017288208 mm for frame 21

Saving results

Total time: 39.63307213783264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854847
Iteration 2/25 | Loss: 0.00091203
Iteration 3/25 | Loss: 0.00075183
Iteration 4/25 | Loss: 0.00072836
Iteration 5/25 | Loss: 0.00072211
Iteration 6/25 | Loss: 0.00072010
Iteration 7/25 | Loss: 0.00071986
Iteration 8/25 | Loss: 0.00071986
Iteration 9/25 | Loss: 0.00071986
Iteration 10/25 | Loss: 0.00071986
Iteration 11/25 | Loss: 0.00071986
Iteration 12/25 | Loss: 0.00071986
Iteration 13/25 | Loss: 0.00071986
Iteration 14/25 | Loss: 0.00071986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007198589155450463, 0.0007198589155450463, 0.0007198589155450463, 0.0007198589155450463, 0.0007198589155450463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007198589155450463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49685729
Iteration 2/25 | Loss: 0.00044811
Iteration 3/25 | Loss: 0.00044809
Iteration 4/25 | Loss: 0.00044809
Iteration 5/25 | Loss: 0.00044809
Iteration 6/25 | Loss: 0.00044809
Iteration 7/25 | Loss: 0.00044808
Iteration 8/25 | Loss: 0.00044808
Iteration 9/25 | Loss: 0.00044808
Iteration 10/25 | Loss: 0.00044808
Iteration 11/25 | Loss: 0.00044808
Iteration 12/25 | Loss: 0.00044808
Iteration 13/25 | Loss: 0.00044808
Iteration 14/25 | Loss: 0.00044808
Iteration 15/25 | Loss: 0.00044808
Iteration 16/25 | Loss: 0.00044808
Iteration 17/25 | Loss: 0.00044808
Iteration 18/25 | Loss: 0.00044808
Iteration 19/25 | Loss: 0.00044808
Iteration 20/25 | Loss: 0.00044808
Iteration 21/25 | Loss: 0.00044808
Iteration 22/25 | Loss: 0.00044808
Iteration 23/25 | Loss: 0.00044808
Iteration 24/25 | Loss: 0.00044808
Iteration 25/25 | Loss: 0.00044808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044808
Iteration 2/1000 | Loss: 0.00002156
Iteration 3/1000 | Loss: 0.00001610
Iteration 4/1000 | Loss: 0.00001491
Iteration 5/1000 | Loss: 0.00001362
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001266
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001239
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001214
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001197
Iteration 43/1000 | Loss: 0.00001197
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001196
Iteration 47/1000 | Loss: 0.00001196
Iteration 48/1000 | Loss: 0.00001196
Iteration 49/1000 | Loss: 0.00001196
Iteration 50/1000 | Loss: 0.00001196
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001191
Iteration 76/1000 | Loss: 0.00001191
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001190
Iteration 81/1000 | Loss: 0.00001190
Iteration 82/1000 | Loss: 0.00001190
Iteration 83/1000 | Loss: 0.00001190
Iteration 84/1000 | Loss: 0.00001190
Iteration 85/1000 | Loss: 0.00001190
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001190
Iteration 88/1000 | Loss: 0.00001190
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.1900096978934016e-05, 1.1900096978934016e-05, 1.1900096978934016e-05, 1.1900096978934016e-05, 1.1900096978934016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1900096978934016e-05

Optimization complete. Final v2v error: 2.930891275405884 mm

Highest mean error: 3.0365898609161377 mm for frame 32

Lowest mean error: 2.8132336139678955 mm for frame 119

Saving results

Total time: 29.851333379745483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998930
Iteration 2/25 | Loss: 0.00358145
Iteration 3/25 | Loss: 0.00207543
Iteration 4/25 | Loss: 0.00179718
Iteration 5/25 | Loss: 0.00163005
Iteration 6/25 | Loss: 0.00162014
Iteration 7/25 | Loss: 0.00157360
Iteration 8/25 | Loss: 0.00146579
Iteration 9/25 | Loss: 0.00137374
Iteration 10/25 | Loss: 0.00129727
Iteration 11/25 | Loss: 0.00125892
Iteration 12/25 | Loss: 0.00120488
Iteration 13/25 | Loss: 0.00117893
Iteration 14/25 | Loss: 0.00116729
Iteration 15/25 | Loss: 0.00114129
Iteration 16/25 | Loss: 0.00113348
Iteration 17/25 | Loss: 0.00112422
Iteration 18/25 | Loss: 0.00112033
Iteration 19/25 | Loss: 0.00111554
Iteration 20/25 | Loss: 0.00110388
Iteration 21/25 | Loss: 0.00109315
Iteration 22/25 | Loss: 0.00109075
Iteration 23/25 | Loss: 0.00108602
Iteration 24/25 | Loss: 0.00108357
Iteration 25/25 | Loss: 0.00108198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51033676
Iteration 2/25 | Loss: 0.00337958
Iteration 3/25 | Loss: 0.00328547
Iteration 4/25 | Loss: 0.00328546
Iteration 5/25 | Loss: 0.00328546
Iteration 6/25 | Loss: 0.00328546
Iteration 7/25 | Loss: 0.00328546
Iteration 8/25 | Loss: 0.00328546
Iteration 9/25 | Loss: 0.00328546
Iteration 10/25 | Loss: 0.00328546
Iteration 11/25 | Loss: 0.00328546
Iteration 12/25 | Loss: 0.00328546
Iteration 13/25 | Loss: 0.00328546
Iteration 14/25 | Loss: 0.00328546
Iteration 15/25 | Loss: 0.00328546
Iteration 16/25 | Loss: 0.00328546
Iteration 17/25 | Loss: 0.00328546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0032854636665433645, 0.0032854636665433645, 0.0032854636665433645, 0.0032854636665433645, 0.0032854636665433645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032854636665433645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00328546
Iteration 2/1000 | Loss: 0.00161135
Iteration 3/1000 | Loss: 0.00251570
Iteration 4/1000 | Loss: 0.00098409
Iteration 5/1000 | Loss: 0.00428976
Iteration 6/1000 | Loss: 0.00075407
Iteration 7/1000 | Loss: 0.00099639
Iteration 8/1000 | Loss: 0.00045286
Iteration 9/1000 | Loss: 0.00091409
Iteration 10/1000 | Loss: 0.00033812
Iteration 11/1000 | Loss: 0.00031220
Iteration 12/1000 | Loss: 0.00139641
Iteration 13/1000 | Loss: 0.00037642
Iteration 14/1000 | Loss: 0.00102387
Iteration 15/1000 | Loss: 0.00123718
Iteration 16/1000 | Loss: 0.00096829
Iteration 17/1000 | Loss: 0.00029614
Iteration 18/1000 | Loss: 0.00022266
Iteration 19/1000 | Loss: 0.00032216
Iteration 20/1000 | Loss: 0.00085462
Iteration 21/1000 | Loss: 0.00047309
Iteration 22/1000 | Loss: 0.00027212
Iteration 23/1000 | Loss: 0.00150266
Iteration 24/1000 | Loss: 0.00036506
Iteration 25/1000 | Loss: 0.00084583
Iteration 26/1000 | Loss: 0.00090601
Iteration 27/1000 | Loss: 0.00015238
Iteration 28/1000 | Loss: 0.00028320
Iteration 29/1000 | Loss: 0.00057134
Iteration 30/1000 | Loss: 0.00092999
Iteration 31/1000 | Loss: 0.00097079
Iteration 32/1000 | Loss: 0.00261164
Iteration 33/1000 | Loss: 0.00290191
Iteration 34/1000 | Loss: 0.00384839
Iteration 35/1000 | Loss: 0.00370020
Iteration 36/1000 | Loss: 0.00129231
Iteration 37/1000 | Loss: 0.00157550
Iteration 38/1000 | Loss: 0.00168776
Iteration 39/1000 | Loss: 0.00238999
Iteration 40/1000 | Loss: 0.00207596
Iteration 41/1000 | Loss: 0.00130096
Iteration 42/1000 | Loss: 0.00039702
Iteration 43/1000 | Loss: 0.00021941
Iteration 44/1000 | Loss: 0.00024171
Iteration 45/1000 | Loss: 0.00033128
Iteration 46/1000 | Loss: 0.00056126
Iteration 47/1000 | Loss: 0.00055304
Iteration 48/1000 | Loss: 0.00048206
Iteration 49/1000 | Loss: 0.00041160
Iteration 50/1000 | Loss: 0.00010384
Iteration 51/1000 | Loss: 0.00017343
Iteration 52/1000 | Loss: 0.00007428
Iteration 53/1000 | Loss: 0.00005930
Iteration 54/1000 | Loss: 0.00031122
Iteration 55/1000 | Loss: 0.00028352
Iteration 56/1000 | Loss: 0.00010152
Iteration 57/1000 | Loss: 0.00020936
Iteration 58/1000 | Loss: 0.00007284
Iteration 59/1000 | Loss: 0.00026234
Iteration 60/1000 | Loss: 0.00005282
Iteration 61/1000 | Loss: 0.00006620
Iteration 62/1000 | Loss: 0.00058659
Iteration 63/1000 | Loss: 0.00054869
Iteration 64/1000 | Loss: 0.00057453
Iteration 65/1000 | Loss: 0.00013241
Iteration 66/1000 | Loss: 0.00036240
Iteration 67/1000 | Loss: 0.00013623
Iteration 68/1000 | Loss: 0.00014683
Iteration 69/1000 | Loss: 0.00023294
Iteration 70/1000 | Loss: 0.00047606
Iteration 71/1000 | Loss: 0.00007003
Iteration 72/1000 | Loss: 0.00007012
Iteration 73/1000 | Loss: 0.00007780
Iteration 74/1000 | Loss: 0.00045990
Iteration 75/1000 | Loss: 0.00043409
Iteration 76/1000 | Loss: 0.00009610
Iteration 77/1000 | Loss: 0.00013742
Iteration 78/1000 | Loss: 0.00007600
Iteration 79/1000 | Loss: 0.00006677
Iteration 80/1000 | Loss: 0.00010170
Iteration 81/1000 | Loss: 0.00006788
Iteration 82/1000 | Loss: 0.00005089
Iteration 83/1000 | Loss: 0.00009122
Iteration 84/1000 | Loss: 0.00008421
Iteration 85/1000 | Loss: 0.00004585
Iteration 86/1000 | Loss: 0.00005992
Iteration 87/1000 | Loss: 0.00006484
Iteration 88/1000 | Loss: 0.00007335
Iteration 89/1000 | Loss: 0.00007046
Iteration 90/1000 | Loss: 0.00007695
Iteration 91/1000 | Loss: 0.00006978
Iteration 92/1000 | Loss: 0.00006965
Iteration 93/1000 | Loss: 0.00009871
Iteration 94/1000 | Loss: 0.00007088
Iteration 95/1000 | Loss: 0.00005855
Iteration 96/1000 | Loss: 0.00007794
Iteration 97/1000 | Loss: 0.00015444
Iteration 98/1000 | Loss: 0.00008226
Iteration 99/1000 | Loss: 0.00006933
Iteration 100/1000 | Loss: 0.00007823
Iteration 101/1000 | Loss: 0.00007223
Iteration 102/1000 | Loss: 0.00007304
Iteration 103/1000 | Loss: 0.00006541
Iteration 104/1000 | Loss: 0.00006159
Iteration 105/1000 | Loss: 0.00007212
Iteration 106/1000 | Loss: 0.00006243
Iteration 107/1000 | Loss: 0.00006127
Iteration 108/1000 | Loss: 0.00006038
Iteration 109/1000 | Loss: 0.00006635
Iteration 110/1000 | Loss: 0.00008818
Iteration 111/1000 | Loss: 0.00005726
Iteration 112/1000 | Loss: 0.00005891
Iteration 113/1000 | Loss: 0.00006422
Iteration 114/1000 | Loss: 0.00007851
Iteration 115/1000 | Loss: 0.00006420
Iteration 116/1000 | Loss: 0.00006733
Iteration 117/1000 | Loss: 0.00006437
Iteration 118/1000 | Loss: 0.00006671
Iteration 119/1000 | Loss: 0.00005999
Iteration 120/1000 | Loss: 0.00007654
Iteration 121/1000 | Loss: 0.00006822
Iteration 122/1000 | Loss: 0.00007209
Iteration 123/1000 | Loss: 0.00007337
Iteration 124/1000 | Loss: 0.00006839
Iteration 125/1000 | Loss: 0.00006865
Iteration 126/1000 | Loss: 0.00007462
Iteration 127/1000 | Loss: 0.00007101
Iteration 128/1000 | Loss: 0.00008630
Iteration 129/1000 | Loss: 0.00007104
Iteration 130/1000 | Loss: 0.00006924
Iteration 131/1000 | Loss: 0.00007146
Iteration 132/1000 | Loss: 0.00006752
Iteration 133/1000 | Loss: 0.00006961
Iteration 134/1000 | Loss: 0.00007543
Iteration 135/1000 | Loss: 0.00006874
Iteration 136/1000 | Loss: 0.00027208
Iteration 137/1000 | Loss: 0.00037582
Iteration 138/1000 | Loss: 0.00006643
Iteration 139/1000 | Loss: 0.00008898
Iteration 140/1000 | Loss: 0.00006462
Iteration 141/1000 | Loss: 0.00006553
Iteration 142/1000 | Loss: 0.00006447
Iteration 143/1000 | Loss: 0.00004101
Iteration 144/1000 | Loss: 0.00004095
Iteration 145/1000 | Loss: 0.00003277
Iteration 146/1000 | Loss: 0.00002764
Iteration 147/1000 | Loss: 0.00004052
Iteration 148/1000 | Loss: 0.00003635
Iteration 149/1000 | Loss: 0.00003396
Iteration 150/1000 | Loss: 0.00003594
Iteration 151/1000 | Loss: 0.00003090
Iteration 152/1000 | Loss: 0.00006622
Iteration 153/1000 | Loss: 0.00004326
Iteration 154/1000 | Loss: 0.00004868
Iteration 155/1000 | Loss: 0.00002310
Iteration 156/1000 | Loss: 0.00003979
Iteration 157/1000 | Loss: 0.00001936
Iteration 158/1000 | Loss: 0.00001885
Iteration 159/1000 | Loss: 0.00001860
Iteration 160/1000 | Loss: 0.00002095
Iteration 161/1000 | Loss: 0.00001812
Iteration 162/1000 | Loss: 0.00002272
Iteration 163/1000 | Loss: 0.00002196
Iteration 164/1000 | Loss: 0.00002193
Iteration 165/1000 | Loss: 0.00002173
Iteration 166/1000 | Loss: 0.00008511
Iteration 167/1000 | Loss: 0.00002087
Iteration 168/1000 | Loss: 0.00001727
Iteration 169/1000 | Loss: 0.00002115
Iteration 170/1000 | Loss: 0.00002287
Iteration 171/1000 | Loss: 0.00002442
Iteration 172/1000 | Loss: 0.00001692
Iteration 173/1000 | Loss: 0.00001585
Iteration 174/1000 | Loss: 0.00001564
Iteration 175/1000 | Loss: 0.00002320
Iteration 176/1000 | Loss: 0.00001557
Iteration 177/1000 | Loss: 0.00001555
Iteration 178/1000 | Loss: 0.00001553
Iteration 179/1000 | Loss: 0.00001552
Iteration 180/1000 | Loss: 0.00001551
Iteration 181/1000 | Loss: 0.00001545
Iteration 182/1000 | Loss: 0.00001543
Iteration 183/1000 | Loss: 0.00001625
Iteration 184/1000 | Loss: 0.00001537
Iteration 185/1000 | Loss: 0.00001537
Iteration 186/1000 | Loss: 0.00001536
Iteration 187/1000 | Loss: 0.00001536
Iteration 188/1000 | Loss: 0.00001536
Iteration 189/1000 | Loss: 0.00001536
Iteration 190/1000 | Loss: 0.00001536
Iteration 191/1000 | Loss: 0.00001536
Iteration 192/1000 | Loss: 0.00001536
Iteration 193/1000 | Loss: 0.00001536
Iteration 194/1000 | Loss: 0.00001535
Iteration 195/1000 | Loss: 0.00001531
Iteration 196/1000 | Loss: 0.00001531
Iteration 197/1000 | Loss: 0.00001531
Iteration 198/1000 | Loss: 0.00001531
Iteration 199/1000 | Loss: 0.00001531
Iteration 200/1000 | Loss: 0.00001531
Iteration 201/1000 | Loss: 0.00001531
Iteration 202/1000 | Loss: 0.00001531
Iteration 203/1000 | Loss: 0.00001531
Iteration 204/1000 | Loss: 0.00001531
Iteration 205/1000 | Loss: 0.00001531
Iteration 206/1000 | Loss: 0.00001531
Iteration 207/1000 | Loss: 0.00001531
Iteration 208/1000 | Loss: 0.00001531
Iteration 209/1000 | Loss: 0.00001531
Iteration 210/1000 | Loss: 0.00001531
Iteration 211/1000 | Loss: 0.00001531
Iteration 212/1000 | Loss: 0.00001530
Iteration 213/1000 | Loss: 0.00001530
Iteration 214/1000 | Loss: 0.00001530
Iteration 215/1000 | Loss: 0.00001529
Iteration 216/1000 | Loss: 0.00001529
Iteration 217/1000 | Loss: 0.00001529
Iteration 218/1000 | Loss: 0.00001528
Iteration 219/1000 | Loss: 0.00001528
Iteration 220/1000 | Loss: 0.00001528
Iteration 221/1000 | Loss: 0.00001528
Iteration 222/1000 | Loss: 0.00001528
Iteration 223/1000 | Loss: 0.00002372
Iteration 224/1000 | Loss: 0.00001527
Iteration 225/1000 | Loss: 0.00001527
Iteration 226/1000 | Loss: 0.00001527
Iteration 227/1000 | Loss: 0.00001527
Iteration 228/1000 | Loss: 0.00001526
Iteration 229/1000 | Loss: 0.00001526
Iteration 230/1000 | Loss: 0.00001526
Iteration 231/1000 | Loss: 0.00001526
Iteration 232/1000 | Loss: 0.00001526
Iteration 233/1000 | Loss: 0.00001526
Iteration 234/1000 | Loss: 0.00001525
Iteration 235/1000 | Loss: 0.00001525
Iteration 236/1000 | Loss: 0.00001525
Iteration 237/1000 | Loss: 0.00001524
Iteration 238/1000 | Loss: 0.00001524
Iteration 239/1000 | Loss: 0.00001524
Iteration 240/1000 | Loss: 0.00001524
Iteration 241/1000 | Loss: 0.00001524
Iteration 242/1000 | Loss: 0.00001524
Iteration 243/1000 | Loss: 0.00001524
Iteration 244/1000 | Loss: 0.00001524
Iteration 245/1000 | Loss: 0.00001524
Iteration 246/1000 | Loss: 0.00001524
Iteration 247/1000 | Loss: 0.00001524
Iteration 248/1000 | Loss: 0.00001523
Iteration 249/1000 | Loss: 0.00001523
Iteration 250/1000 | Loss: 0.00001523
Iteration 251/1000 | Loss: 0.00001523
Iteration 252/1000 | Loss: 0.00001523
Iteration 253/1000 | Loss: 0.00001523
Iteration 254/1000 | Loss: 0.00001523
Iteration 255/1000 | Loss: 0.00001522
Iteration 256/1000 | Loss: 0.00001522
Iteration 257/1000 | Loss: 0.00001522
Iteration 258/1000 | Loss: 0.00001522
Iteration 259/1000 | Loss: 0.00001522
Iteration 260/1000 | Loss: 0.00001522
Iteration 261/1000 | Loss: 0.00001522
Iteration 262/1000 | Loss: 0.00001522
Iteration 263/1000 | Loss: 0.00001522
Iteration 264/1000 | Loss: 0.00001522
Iteration 265/1000 | Loss: 0.00001522
Iteration 266/1000 | Loss: 0.00001522
Iteration 267/1000 | Loss: 0.00001522
Iteration 268/1000 | Loss: 0.00001522
Iteration 269/1000 | Loss: 0.00001522
Iteration 270/1000 | Loss: 0.00001521
Iteration 271/1000 | Loss: 0.00001521
Iteration 272/1000 | Loss: 0.00001521
Iteration 273/1000 | Loss: 0.00001521
Iteration 274/1000 | Loss: 0.00001521
Iteration 275/1000 | Loss: 0.00001521
Iteration 276/1000 | Loss: 0.00001521
Iteration 277/1000 | Loss: 0.00001521
Iteration 278/1000 | Loss: 0.00001521
Iteration 279/1000 | Loss: 0.00001521
Iteration 280/1000 | Loss: 0.00001521
Iteration 281/1000 | Loss: 0.00001521
Iteration 282/1000 | Loss: 0.00001521
Iteration 283/1000 | Loss: 0.00001521
Iteration 284/1000 | Loss: 0.00001521
Iteration 285/1000 | Loss: 0.00001521
Iteration 286/1000 | Loss: 0.00001521
Iteration 287/1000 | Loss: 0.00001521
Iteration 288/1000 | Loss: 0.00001521
Iteration 289/1000 | Loss: 0.00001521
Iteration 290/1000 | Loss: 0.00001521
Iteration 291/1000 | Loss: 0.00001521
Iteration 292/1000 | Loss: 0.00001521
Iteration 293/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [1.521349713584641e-05, 1.521349713584641e-05, 1.521349713584641e-05, 1.521349713584641e-05, 1.521349713584641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.521349713584641e-05

Optimization complete. Final v2v error: 3.30509614944458 mm

Highest mean error: 4.461118698120117 mm for frame 94

Lowest mean error: 2.777444362640381 mm for frame 235

Saving results

Total time: 338.58947134017944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484455
Iteration 2/25 | Loss: 0.00089249
Iteration 3/25 | Loss: 0.00077780
Iteration 4/25 | Loss: 0.00075664
Iteration 5/25 | Loss: 0.00075070
Iteration 6/25 | Loss: 0.00074982
Iteration 7/25 | Loss: 0.00074982
Iteration 8/25 | Loss: 0.00074982
Iteration 9/25 | Loss: 0.00074982
Iteration 10/25 | Loss: 0.00074982
Iteration 11/25 | Loss: 0.00074982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007498246850445867, 0.0007498246850445867, 0.0007498246850445867, 0.0007498246850445867, 0.0007498246850445867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007498246850445867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.55073595
Iteration 2/25 | Loss: 0.00044036
Iteration 3/25 | Loss: 0.00044036
Iteration 4/25 | Loss: 0.00044036
Iteration 5/25 | Loss: 0.00044036
Iteration 6/25 | Loss: 0.00044036
Iteration 7/25 | Loss: 0.00044035
Iteration 8/25 | Loss: 0.00044035
Iteration 9/25 | Loss: 0.00044035
Iteration 10/25 | Loss: 0.00044035
Iteration 11/25 | Loss: 0.00044035
Iteration 12/25 | Loss: 0.00044035
Iteration 13/25 | Loss: 0.00044035
Iteration 14/25 | Loss: 0.00044035
Iteration 15/25 | Loss: 0.00044035
Iteration 16/25 | Loss: 0.00044035
Iteration 17/25 | Loss: 0.00044035
Iteration 18/25 | Loss: 0.00044035
Iteration 19/25 | Loss: 0.00044035
Iteration 20/25 | Loss: 0.00044035
Iteration 21/25 | Loss: 0.00044035
Iteration 22/25 | Loss: 0.00044035
Iteration 23/25 | Loss: 0.00044035
Iteration 24/25 | Loss: 0.00044035
Iteration 25/25 | Loss: 0.00044035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044035
Iteration 2/1000 | Loss: 0.00002352
Iteration 3/1000 | Loss: 0.00001804
Iteration 4/1000 | Loss: 0.00001678
Iteration 5/1000 | Loss: 0.00001572
Iteration 6/1000 | Loss: 0.00001535
Iteration 7/1000 | Loss: 0.00001494
Iteration 8/1000 | Loss: 0.00001475
Iteration 9/1000 | Loss: 0.00001456
Iteration 10/1000 | Loss: 0.00001455
Iteration 11/1000 | Loss: 0.00001449
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001440
Iteration 16/1000 | Loss: 0.00001439
Iteration 17/1000 | Loss: 0.00001434
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001425
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001419
Iteration 25/1000 | Loss: 0.00001419
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001417
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001404
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001391
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001388
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001387
Iteration 64/1000 | Loss: 0.00001386
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001386
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001385
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00001383
Iteration 90/1000 | Loss: 0.00001383
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001381
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001381
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001381
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Iteration 109/1000 | Loss: 0.00001381
Iteration 110/1000 | Loss: 0.00001381
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001380
Iteration 113/1000 | Loss: 0.00001380
Iteration 114/1000 | Loss: 0.00001380
Iteration 115/1000 | Loss: 0.00001380
Iteration 116/1000 | Loss: 0.00001380
Iteration 117/1000 | Loss: 0.00001379
Iteration 118/1000 | Loss: 0.00001379
Iteration 119/1000 | Loss: 0.00001379
Iteration 120/1000 | Loss: 0.00001379
Iteration 121/1000 | Loss: 0.00001379
Iteration 122/1000 | Loss: 0.00001379
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001379
Iteration 125/1000 | Loss: 0.00001379
Iteration 126/1000 | Loss: 0.00001379
Iteration 127/1000 | Loss: 0.00001379
Iteration 128/1000 | Loss: 0.00001379
Iteration 129/1000 | Loss: 0.00001379
Iteration 130/1000 | Loss: 0.00001379
Iteration 131/1000 | Loss: 0.00001379
Iteration 132/1000 | Loss: 0.00001379
Iteration 133/1000 | Loss: 0.00001378
Iteration 134/1000 | Loss: 0.00001378
Iteration 135/1000 | Loss: 0.00001378
Iteration 136/1000 | Loss: 0.00001378
Iteration 137/1000 | Loss: 0.00001378
Iteration 138/1000 | Loss: 0.00001378
Iteration 139/1000 | Loss: 0.00001378
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001378
Iteration 142/1000 | Loss: 0.00001378
Iteration 143/1000 | Loss: 0.00001378
Iteration 144/1000 | Loss: 0.00001378
Iteration 145/1000 | Loss: 0.00001378
Iteration 146/1000 | Loss: 0.00001378
Iteration 147/1000 | Loss: 0.00001378
Iteration 148/1000 | Loss: 0.00001378
Iteration 149/1000 | Loss: 0.00001378
Iteration 150/1000 | Loss: 0.00001378
Iteration 151/1000 | Loss: 0.00001378
Iteration 152/1000 | Loss: 0.00001378
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001378
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001378
Iteration 159/1000 | Loss: 0.00001378
Iteration 160/1000 | Loss: 0.00001378
Iteration 161/1000 | Loss: 0.00001378
Iteration 162/1000 | Loss: 0.00001378
Iteration 163/1000 | Loss: 0.00001378
Iteration 164/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3776641935692169e-05, 1.3776641935692169e-05, 1.3776641935692169e-05, 1.3776641935692169e-05, 1.3776641935692169e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3776641935692169e-05

Optimization complete. Final v2v error: 3.1706173419952393 mm

Highest mean error: 3.459459066390991 mm for frame 220

Lowest mean error: 2.9965970516204834 mm for frame 170

Saving results

Total time: 40.3328742980957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920552
Iteration 2/25 | Loss: 0.00261109
Iteration 3/25 | Loss: 0.00388641
Iteration 4/25 | Loss: 0.00112247
Iteration 5/25 | Loss: 0.00087536
Iteration 6/25 | Loss: 0.00087216
Iteration 7/25 | Loss: 0.00086422
Iteration 8/25 | Loss: 0.00090311
Iteration 9/25 | Loss: 0.00089642
Iteration 10/25 | Loss: 0.00084351
Iteration 11/25 | Loss: 0.00087978
Iteration 12/25 | Loss: 0.00088415
Iteration 13/25 | Loss: 0.00083366
Iteration 14/25 | Loss: 0.00082536
Iteration 15/25 | Loss: 0.00082329
Iteration 16/25 | Loss: 0.00082362
Iteration 17/25 | Loss: 0.00082339
Iteration 18/25 | Loss: 0.00082215
Iteration 19/25 | Loss: 0.00082167
Iteration 20/25 | Loss: 0.00082239
Iteration 21/25 | Loss: 0.00082230
Iteration 22/25 | Loss: 0.00082229
Iteration 23/25 | Loss: 0.00082168
Iteration 24/25 | Loss: 0.00083495
Iteration 25/25 | Loss: 0.00082631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50364327
Iteration 2/25 | Loss: 0.00050410
Iteration 3/25 | Loss: 0.00050408
Iteration 4/25 | Loss: 0.00050408
Iteration 5/25 | Loss: 0.00050408
Iteration 6/25 | Loss: 0.00050408
Iteration 7/25 | Loss: 0.00050408
Iteration 8/25 | Loss: 0.00050408
Iteration 9/25 | Loss: 0.00050408
Iteration 10/25 | Loss: 0.00050408
Iteration 11/25 | Loss: 0.00050408
Iteration 12/25 | Loss: 0.00050408
Iteration 13/25 | Loss: 0.00050408
Iteration 14/25 | Loss: 0.00050408
Iteration 15/25 | Loss: 0.00050408
Iteration 16/25 | Loss: 0.00050408
Iteration 17/25 | Loss: 0.00050408
Iteration 18/25 | Loss: 0.00050408
Iteration 19/25 | Loss: 0.00050408
Iteration 20/25 | Loss: 0.00050408
Iteration 21/25 | Loss: 0.00050408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005040777614340186, 0.0005040777614340186, 0.0005040777614340186, 0.0005040777614340186, 0.0005040777614340186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005040777614340186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050408
Iteration 2/1000 | Loss: 0.00006521
Iteration 3/1000 | Loss: 0.00004345
Iteration 4/1000 | Loss: 0.00003638
Iteration 5/1000 | Loss: 0.00003353
Iteration 6/1000 | Loss: 0.00003132
Iteration 7/1000 | Loss: 0.00003034
Iteration 8/1000 | Loss: 0.00002947
Iteration 9/1000 | Loss: 0.00002875
Iteration 10/1000 | Loss: 0.00002805
Iteration 11/1000 | Loss: 0.00002756
Iteration 12/1000 | Loss: 0.00002720
Iteration 13/1000 | Loss: 0.00002695
Iteration 14/1000 | Loss: 0.00002669
Iteration 15/1000 | Loss: 0.00002646
Iteration 16/1000 | Loss: 0.00002625
Iteration 17/1000 | Loss: 0.00002607
Iteration 18/1000 | Loss: 0.00002605
Iteration 19/1000 | Loss: 0.00002602
Iteration 20/1000 | Loss: 0.00002599
Iteration 21/1000 | Loss: 0.00002597
Iteration 22/1000 | Loss: 0.00002594
Iteration 23/1000 | Loss: 0.00002594
Iteration 24/1000 | Loss: 0.00002590
Iteration 25/1000 | Loss: 0.00002586
Iteration 26/1000 | Loss: 0.00002584
Iteration 27/1000 | Loss: 0.00002583
Iteration 28/1000 | Loss: 0.00002582
Iteration 29/1000 | Loss: 0.00002582
Iteration 30/1000 | Loss: 0.00002581
Iteration 31/1000 | Loss: 0.00002581
Iteration 32/1000 | Loss: 0.00002581
Iteration 33/1000 | Loss: 0.00002580
Iteration 34/1000 | Loss: 0.00002580
Iteration 35/1000 | Loss: 0.00002580
Iteration 36/1000 | Loss: 0.00002579
Iteration 37/1000 | Loss: 0.00002579
Iteration 38/1000 | Loss: 0.00002579
Iteration 39/1000 | Loss: 0.00031982
Iteration 40/1000 | Loss: 0.00003292
Iteration 41/1000 | Loss: 0.00003064
Iteration 42/1000 | Loss: 0.00002856
Iteration 43/1000 | Loss: 0.00002714
Iteration 44/1000 | Loss: 0.00002604
Iteration 45/1000 | Loss: 0.00002558
Iteration 46/1000 | Loss: 0.00002529
Iteration 47/1000 | Loss: 0.00002503
Iteration 48/1000 | Loss: 0.00002495
Iteration 49/1000 | Loss: 0.00002485
Iteration 50/1000 | Loss: 0.00002485
Iteration 51/1000 | Loss: 0.00002483
Iteration 52/1000 | Loss: 0.00002482
Iteration 53/1000 | Loss: 0.00002482
Iteration 54/1000 | Loss: 0.00002482
Iteration 55/1000 | Loss: 0.00002482
Iteration 56/1000 | Loss: 0.00002481
Iteration 57/1000 | Loss: 0.00002481
Iteration 58/1000 | Loss: 0.00002481
Iteration 59/1000 | Loss: 0.00002481
Iteration 60/1000 | Loss: 0.00002481
Iteration 61/1000 | Loss: 0.00002480
Iteration 62/1000 | Loss: 0.00002480
Iteration 63/1000 | Loss: 0.00002480
Iteration 64/1000 | Loss: 0.00002480
Iteration 65/1000 | Loss: 0.00002480
Iteration 66/1000 | Loss: 0.00002480
Iteration 67/1000 | Loss: 0.00002480
Iteration 68/1000 | Loss: 0.00002480
Iteration 69/1000 | Loss: 0.00002480
Iteration 70/1000 | Loss: 0.00002479
Iteration 71/1000 | Loss: 0.00002479
Iteration 72/1000 | Loss: 0.00002479
Iteration 73/1000 | Loss: 0.00002479
Iteration 74/1000 | Loss: 0.00002479
Iteration 75/1000 | Loss: 0.00002479
Iteration 76/1000 | Loss: 0.00002479
Iteration 77/1000 | Loss: 0.00002478
Iteration 78/1000 | Loss: 0.00002478
Iteration 79/1000 | Loss: 0.00002478
Iteration 80/1000 | Loss: 0.00002478
Iteration 81/1000 | Loss: 0.00002478
Iteration 82/1000 | Loss: 0.00002478
Iteration 83/1000 | Loss: 0.00002478
Iteration 84/1000 | Loss: 0.00002478
Iteration 85/1000 | Loss: 0.00002478
Iteration 86/1000 | Loss: 0.00002478
Iteration 87/1000 | Loss: 0.00002478
Iteration 88/1000 | Loss: 0.00002478
Iteration 89/1000 | Loss: 0.00002478
Iteration 90/1000 | Loss: 0.00002478
Iteration 91/1000 | Loss: 0.00002477
Iteration 92/1000 | Loss: 0.00002477
Iteration 93/1000 | Loss: 0.00002477
Iteration 94/1000 | Loss: 0.00002477
Iteration 95/1000 | Loss: 0.00002477
Iteration 96/1000 | Loss: 0.00002477
Iteration 97/1000 | Loss: 0.00002477
Iteration 98/1000 | Loss: 0.00002476
Iteration 99/1000 | Loss: 0.00002476
Iteration 100/1000 | Loss: 0.00002476
Iteration 101/1000 | Loss: 0.00002476
Iteration 102/1000 | Loss: 0.00002476
Iteration 103/1000 | Loss: 0.00002476
Iteration 104/1000 | Loss: 0.00002476
Iteration 105/1000 | Loss: 0.00002476
Iteration 106/1000 | Loss: 0.00002475
Iteration 107/1000 | Loss: 0.00002475
Iteration 108/1000 | Loss: 0.00002475
Iteration 109/1000 | Loss: 0.00002475
Iteration 110/1000 | Loss: 0.00002475
Iteration 111/1000 | Loss: 0.00002475
Iteration 112/1000 | Loss: 0.00002475
Iteration 113/1000 | Loss: 0.00002475
Iteration 114/1000 | Loss: 0.00002475
Iteration 115/1000 | Loss: 0.00002475
Iteration 116/1000 | Loss: 0.00002475
Iteration 117/1000 | Loss: 0.00002475
Iteration 118/1000 | Loss: 0.00002474
Iteration 119/1000 | Loss: 0.00002474
Iteration 120/1000 | Loss: 0.00002474
Iteration 121/1000 | Loss: 0.00002474
Iteration 122/1000 | Loss: 0.00002474
Iteration 123/1000 | Loss: 0.00002474
Iteration 124/1000 | Loss: 0.00002474
Iteration 125/1000 | Loss: 0.00002474
Iteration 126/1000 | Loss: 0.00002474
Iteration 127/1000 | Loss: 0.00002473
Iteration 128/1000 | Loss: 0.00002473
Iteration 129/1000 | Loss: 0.00002473
Iteration 130/1000 | Loss: 0.00002473
Iteration 131/1000 | Loss: 0.00002473
Iteration 132/1000 | Loss: 0.00002473
Iteration 133/1000 | Loss: 0.00002473
Iteration 134/1000 | Loss: 0.00002473
Iteration 135/1000 | Loss: 0.00002473
Iteration 136/1000 | Loss: 0.00002472
Iteration 137/1000 | Loss: 0.00002472
Iteration 138/1000 | Loss: 0.00002472
Iteration 139/1000 | Loss: 0.00002472
Iteration 140/1000 | Loss: 0.00002472
Iteration 141/1000 | Loss: 0.00002472
Iteration 142/1000 | Loss: 0.00002472
Iteration 143/1000 | Loss: 0.00002472
Iteration 144/1000 | Loss: 0.00002472
Iteration 145/1000 | Loss: 0.00002472
Iteration 146/1000 | Loss: 0.00002472
Iteration 147/1000 | Loss: 0.00002472
Iteration 148/1000 | Loss: 0.00002472
Iteration 149/1000 | Loss: 0.00002472
Iteration 150/1000 | Loss: 0.00002472
Iteration 151/1000 | Loss: 0.00002472
Iteration 152/1000 | Loss: 0.00002472
Iteration 153/1000 | Loss: 0.00002472
Iteration 154/1000 | Loss: 0.00002472
Iteration 155/1000 | Loss: 0.00002472
Iteration 156/1000 | Loss: 0.00002472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.4720055080251768e-05, 2.4720055080251768e-05, 2.4720055080251768e-05, 2.4720055080251768e-05, 2.4720055080251768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4720055080251768e-05

Optimization complete. Final v2v error: 4.16602897644043 mm

Highest mean error: 5.865474700927734 mm for frame 70

Lowest mean error: 3.6687402725219727 mm for frame 45

Saving results

Total time: 89.92873978614807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806525
Iteration 2/25 | Loss: 0.00100138
Iteration 3/25 | Loss: 0.00078505
Iteration 4/25 | Loss: 0.00075024
Iteration 5/25 | Loss: 0.00074521
Iteration 6/25 | Loss: 0.00074464
Iteration 7/25 | Loss: 0.00074464
Iteration 8/25 | Loss: 0.00074464
Iteration 9/25 | Loss: 0.00074464
Iteration 10/25 | Loss: 0.00074464
Iteration 11/25 | Loss: 0.00074464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007446362287737429, 0.0007446362287737429, 0.0007446362287737429, 0.0007446362287737429, 0.0007446362287737429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007446362287737429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50808680
Iteration 2/25 | Loss: 0.00049132
Iteration 3/25 | Loss: 0.00049132
Iteration 4/25 | Loss: 0.00049132
Iteration 5/25 | Loss: 0.00049132
Iteration 6/25 | Loss: 0.00049132
Iteration 7/25 | Loss: 0.00049132
Iteration 8/25 | Loss: 0.00049132
Iteration 9/25 | Loss: 0.00049132
Iteration 10/25 | Loss: 0.00049132
Iteration 11/25 | Loss: 0.00049132
Iteration 12/25 | Loss: 0.00049132
Iteration 13/25 | Loss: 0.00049132
Iteration 14/25 | Loss: 0.00049132
Iteration 15/25 | Loss: 0.00049132
Iteration 16/25 | Loss: 0.00049132
Iteration 17/25 | Loss: 0.00049132
Iteration 18/25 | Loss: 0.00049132
Iteration 19/25 | Loss: 0.00049132
Iteration 20/25 | Loss: 0.00049132
Iteration 21/25 | Loss: 0.00049132
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004913171287626028, 0.0004913171287626028, 0.0004913171287626028, 0.0004913171287626028, 0.0004913171287626028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004913171287626028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049132
Iteration 2/1000 | Loss: 0.00002352
Iteration 3/1000 | Loss: 0.00001508
Iteration 4/1000 | Loss: 0.00001375
Iteration 5/1000 | Loss: 0.00001291
Iteration 6/1000 | Loss: 0.00001255
Iteration 7/1000 | Loss: 0.00001227
Iteration 8/1000 | Loss: 0.00001209
Iteration 9/1000 | Loss: 0.00001194
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001177
Iteration 16/1000 | Loss: 0.00001177
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001172
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001171
Iteration 22/1000 | Loss: 0.00001171
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001166
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001164
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001157
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001155
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001154
Iteration 46/1000 | Loss: 0.00001154
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001153
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001153
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001153
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001152
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001151
Iteration 70/1000 | Loss: 0.00001151
Iteration 71/1000 | Loss: 0.00001151
Iteration 72/1000 | Loss: 0.00001151
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001151
Iteration 76/1000 | Loss: 0.00001151
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001151
Iteration 84/1000 | Loss: 0.00001151
Iteration 85/1000 | Loss: 0.00001151
Iteration 86/1000 | Loss: 0.00001151
Iteration 87/1000 | Loss: 0.00001151
Iteration 88/1000 | Loss: 0.00001151
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.1510322110552806e-05, 1.1510322110552806e-05, 1.1510322110552806e-05, 1.1510322110552806e-05, 1.1510322110552806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1510322110552806e-05

Optimization complete. Final v2v error: 2.828329086303711 mm

Highest mean error: 3.007938861846924 mm for frame 130

Lowest mean error: 2.5979385375976562 mm for frame 238

Saving results

Total time: 32.40036630630493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044646
Iteration 2/25 | Loss: 0.00203141
Iteration 3/25 | Loss: 0.00174190
Iteration 4/25 | Loss: 0.00167587
Iteration 5/25 | Loss: 0.00160123
Iteration 6/25 | Loss: 0.00111999
Iteration 7/25 | Loss: 0.00098257
Iteration 8/25 | Loss: 0.00093016
Iteration 9/25 | Loss: 0.00091608
Iteration 10/25 | Loss: 0.00091309
Iteration 11/25 | Loss: 0.00091257
Iteration 12/25 | Loss: 0.00091253
Iteration 13/25 | Loss: 0.00091253
Iteration 14/25 | Loss: 0.00091253
Iteration 15/25 | Loss: 0.00091253
Iteration 16/25 | Loss: 0.00091253
Iteration 17/25 | Loss: 0.00091253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009125324431806803, 0.0009125324431806803, 0.0009125324431806803, 0.0009125324431806803, 0.0009125324431806803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009125324431806803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46405292
Iteration 2/25 | Loss: 0.00048966
Iteration 3/25 | Loss: 0.00048966
Iteration 4/25 | Loss: 0.00048966
Iteration 5/25 | Loss: 0.00048966
Iteration 6/25 | Loss: 0.00048966
Iteration 7/25 | Loss: 0.00048966
Iteration 8/25 | Loss: 0.00048966
Iteration 9/25 | Loss: 0.00048966
Iteration 10/25 | Loss: 0.00048966
Iteration 11/25 | Loss: 0.00048966
Iteration 12/25 | Loss: 0.00048966
Iteration 13/25 | Loss: 0.00048966
Iteration 14/25 | Loss: 0.00048966
Iteration 15/25 | Loss: 0.00048966
Iteration 16/25 | Loss: 0.00048966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004896606551483274, 0.0004896606551483274, 0.0004896606551483274, 0.0004896606551483274, 0.0004896606551483274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004896606551483274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048966
Iteration 2/1000 | Loss: 0.00003634
Iteration 3/1000 | Loss: 0.00002825
Iteration 4/1000 | Loss: 0.00002645
Iteration 5/1000 | Loss: 0.00002522
Iteration 6/1000 | Loss: 0.00002450
Iteration 7/1000 | Loss: 0.00002406
Iteration 8/1000 | Loss: 0.00002383
Iteration 9/1000 | Loss: 0.00002368
Iteration 10/1000 | Loss: 0.00002368
Iteration 11/1000 | Loss: 0.00002359
Iteration 12/1000 | Loss: 0.00002358
Iteration 13/1000 | Loss: 0.00002357
Iteration 14/1000 | Loss: 0.00002356
Iteration 15/1000 | Loss: 0.00002356
Iteration 16/1000 | Loss: 0.00002356
Iteration 17/1000 | Loss: 0.00002356
Iteration 18/1000 | Loss: 0.00002356
Iteration 19/1000 | Loss: 0.00002356
Iteration 20/1000 | Loss: 0.00002355
Iteration 21/1000 | Loss: 0.00002355
Iteration 22/1000 | Loss: 0.00002355
Iteration 23/1000 | Loss: 0.00002355
Iteration 24/1000 | Loss: 0.00002354
Iteration 25/1000 | Loss: 0.00002354
Iteration 26/1000 | Loss: 0.00002353
Iteration 27/1000 | Loss: 0.00002353
Iteration 28/1000 | Loss: 0.00002353
Iteration 29/1000 | Loss: 0.00002353
Iteration 30/1000 | Loss: 0.00002353
Iteration 31/1000 | Loss: 0.00002353
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002353
Iteration 35/1000 | Loss: 0.00002353
Iteration 36/1000 | Loss: 0.00002353
Iteration 37/1000 | Loss: 0.00002349
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002349
Iteration 40/1000 | Loss: 0.00002349
Iteration 41/1000 | Loss: 0.00002349
Iteration 42/1000 | Loss: 0.00002349
Iteration 43/1000 | Loss: 0.00002348
Iteration 44/1000 | Loss: 0.00002348
Iteration 45/1000 | Loss: 0.00002348
Iteration 46/1000 | Loss: 0.00002348
Iteration 47/1000 | Loss: 0.00002348
Iteration 48/1000 | Loss: 0.00002348
Iteration 49/1000 | Loss: 0.00002348
Iteration 50/1000 | Loss: 0.00002347
Iteration 51/1000 | Loss: 0.00002347
Iteration 52/1000 | Loss: 0.00002347
Iteration 53/1000 | Loss: 0.00002346
Iteration 54/1000 | Loss: 0.00002346
Iteration 55/1000 | Loss: 0.00002346
Iteration 56/1000 | Loss: 0.00002346
Iteration 57/1000 | Loss: 0.00002345
Iteration 58/1000 | Loss: 0.00002345
Iteration 59/1000 | Loss: 0.00002344
Iteration 60/1000 | Loss: 0.00002344
Iteration 61/1000 | Loss: 0.00002344
Iteration 62/1000 | Loss: 0.00002344
Iteration 63/1000 | Loss: 0.00002344
Iteration 64/1000 | Loss: 0.00002344
Iteration 65/1000 | Loss: 0.00002344
Iteration 66/1000 | Loss: 0.00002344
Iteration 67/1000 | Loss: 0.00002344
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002344
Iteration 70/1000 | Loss: 0.00002344
Iteration 71/1000 | Loss: 0.00002344
Iteration 72/1000 | Loss: 0.00002344
Iteration 73/1000 | Loss: 0.00002344
Iteration 74/1000 | Loss: 0.00002344
Iteration 75/1000 | Loss: 0.00002344
Iteration 76/1000 | Loss: 0.00002344
Iteration 77/1000 | Loss: 0.00002344
Iteration 78/1000 | Loss: 0.00002344
Iteration 79/1000 | Loss: 0.00002344
Iteration 80/1000 | Loss: 0.00002344
Iteration 81/1000 | Loss: 0.00002344
Iteration 82/1000 | Loss: 0.00002344
Iteration 83/1000 | Loss: 0.00002344
Iteration 84/1000 | Loss: 0.00002344
Iteration 85/1000 | Loss: 0.00002344
Iteration 86/1000 | Loss: 0.00002344
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002344
Iteration 90/1000 | Loss: 0.00002344
Iteration 91/1000 | Loss: 0.00002344
Iteration 92/1000 | Loss: 0.00002344
Iteration 93/1000 | Loss: 0.00002344
Iteration 94/1000 | Loss: 0.00002344
Iteration 95/1000 | Loss: 0.00002344
Iteration 96/1000 | Loss: 0.00002344
Iteration 97/1000 | Loss: 0.00002344
Iteration 98/1000 | Loss: 0.00002344
Iteration 99/1000 | Loss: 0.00002344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.3439486540155485e-05, 2.3439486540155485e-05, 2.3439486540155485e-05, 2.3439486540155485e-05, 2.3439486540155485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3439486540155485e-05

Optimization complete. Final v2v error: 4.089678764343262 mm

Highest mean error: 4.405742168426514 mm for frame 183

Lowest mean error: 3.9162821769714355 mm for frame 44

Saving results

Total time: 42.04820203781128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809984
Iteration 2/25 | Loss: 0.00095639
Iteration 3/25 | Loss: 0.00075959
Iteration 4/25 | Loss: 0.00073356
Iteration 5/25 | Loss: 0.00072511
Iteration 6/25 | Loss: 0.00072323
Iteration 7/25 | Loss: 0.00072309
Iteration 8/25 | Loss: 0.00072309
Iteration 9/25 | Loss: 0.00072309
Iteration 10/25 | Loss: 0.00072309
Iteration 11/25 | Loss: 0.00072309
Iteration 12/25 | Loss: 0.00072309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007230862975120544, 0.0007230862975120544, 0.0007230862975120544, 0.0007230862975120544, 0.0007230862975120544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007230862975120544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50089407
Iteration 2/25 | Loss: 0.00047440
Iteration 3/25 | Loss: 0.00047440
Iteration 4/25 | Loss: 0.00047440
Iteration 5/25 | Loss: 0.00047440
Iteration 6/25 | Loss: 0.00047440
Iteration 7/25 | Loss: 0.00047440
Iteration 8/25 | Loss: 0.00047440
Iteration 9/25 | Loss: 0.00047440
Iteration 10/25 | Loss: 0.00047440
Iteration 11/25 | Loss: 0.00047440
Iteration 12/25 | Loss: 0.00047440
Iteration 13/25 | Loss: 0.00047440
Iteration 14/25 | Loss: 0.00047440
Iteration 15/25 | Loss: 0.00047440
Iteration 16/25 | Loss: 0.00047440
Iteration 17/25 | Loss: 0.00047440
Iteration 18/25 | Loss: 0.00047440
Iteration 19/25 | Loss: 0.00047440
Iteration 20/25 | Loss: 0.00047440
Iteration 21/25 | Loss: 0.00047440
Iteration 22/25 | Loss: 0.00047440
Iteration 23/25 | Loss: 0.00047440
Iteration 24/25 | Loss: 0.00047440
Iteration 25/25 | Loss: 0.00047440

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047440
Iteration 2/1000 | Loss: 0.00002141
Iteration 3/1000 | Loss: 0.00001334
Iteration 4/1000 | Loss: 0.00001232
Iteration 5/1000 | Loss: 0.00001154
Iteration 6/1000 | Loss: 0.00001122
Iteration 7/1000 | Loss: 0.00001097
Iteration 8/1000 | Loss: 0.00001094
Iteration 9/1000 | Loss: 0.00001078
Iteration 10/1000 | Loss: 0.00001073
Iteration 11/1000 | Loss: 0.00001071
Iteration 12/1000 | Loss: 0.00001071
Iteration 13/1000 | Loss: 0.00001066
Iteration 14/1000 | Loss: 0.00001063
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001059
Iteration 17/1000 | Loss: 0.00001058
Iteration 18/1000 | Loss: 0.00001056
Iteration 19/1000 | Loss: 0.00001056
Iteration 20/1000 | Loss: 0.00001055
Iteration 21/1000 | Loss: 0.00001055
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001054
Iteration 24/1000 | Loss: 0.00001053
Iteration 25/1000 | Loss: 0.00001053
Iteration 26/1000 | Loss: 0.00001052
Iteration 27/1000 | Loss: 0.00001051
Iteration 28/1000 | Loss: 0.00001051
Iteration 29/1000 | Loss: 0.00001050
Iteration 30/1000 | Loss: 0.00001050
Iteration 31/1000 | Loss: 0.00001049
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001048
Iteration 34/1000 | Loss: 0.00001047
Iteration 35/1000 | Loss: 0.00001047
Iteration 36/1000 | Loss: 0.00001047
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001046
Iteration 39/1000 | Loss: 0.00001046
Iteration 40/1000 | Loss: 0.00001046
Iteration 41/1000 | Loss: 0.00001045
Iteration 42/1000 | Loss: 0.00001044
Iteration 43/1000 | Loss: 0.00001044
Iteration 44/1000 | Loss: 0.00001044
Iteration 45/1000 | Loss: 0.00001043
Iteration 46/1000 | Loss: 0.00001043
Iteration 47/1000 | Loss: 0.00001042
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001038
Iteration 53/1000 | Loss: 0.00001038
Iteration 54/1000 | Loss: 0.00001037
Iteration 55/1000 | Loss: 0.00001037
Iteration 56/1000 | Loss: 0.00001037
Iteration 57/1000 | Loss: 0.00001037
Iteration 58/1000 | Loss: 0.00001036
Iteration 59/1000 | Loss: 0.00001036
Iteration 60/1000 | Loss: 0.00001036
Iteration 61/1000 | Loss: 0.00001036
Iteration 62/1000 | Loss: 0.00001036
Iteration 63/1000 | Loss: 0.00001035
Iteration 64/1000 | Loss: 0.00001035
Iteration 65/1000 | Loss: 0.00001035
Iteration 66/1000 | Loss: 0.00001035
Iteration 67/1000 | Loss: 0.00001035
Iteration 68/1000 | Loss: 0.00001035
Iteration 69/1000 | Loss: 0.00001035
Iteration 70/1000 | Loss: 0.00001035
Iteration 71/1000 | Loss: 0.00001034
Iteration 72/1000 | Loss: 0.00001034
Iteration 73/1000 | Loss: 0.00001034
Iteration 74/1000 | Loss: 0.00001034
Iteration 75/1000 | Loss: 0.00001034
Iteration 76/1000 | Loss: 0.00001034
Iteration 77/1000 | Loss: 0.00001034
Iteration 78/1000 | Loss: 0.00001033
Iteration 79/1000 | Loss: 0.00001033
Iteration 80/1000 | Loss: 0.00001033
Iteration 81/1000 | Loss: 0.00001033
Iteration 82/1000 | Loss: 0.00001033
Iteration 83/1000 | Loss: 0.00001033
Iteration 84/1000 | Loss: 0.00001032
Iteration 85/1000 | Loss: 0.00001032
Iteration 86/1000 | Loss: 0.00001032
Iteration 87/1000 | Loss: 0.00001032
Iteration 88/1000 | Loss: 0.00001032
Iteration 89/1000 | Loss: 0.00001032
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001031
Iteration 92/1000 | Loss: 0.00001031
Iteration 93/1000 | Loss: 0.00001031
Iteration 94/1000 | Loss: 0.00001031
Iteration 95/1000 | Loss: 0.00001031
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001031
Iteration 98/1000 | Loss: 0.00001031
Iteration 99/1000 | Loss: 0.00001031
Iteration 100/1000 | Loss: 0.00001031
Iteration 101/1000 | Loss: 0.00001031
Iteration 102/1000 | Loss: 0.00001031
Iteration 103/1000 | Loss: 0.00001031
Iteration 104/1000 | Loss: 0.00001031
Iteration 105/1000 | Loss: 0.00001031
Iteration 106/1000 | Loss: 0.00001031
Iteration 107/1000 | Loss: 0.00001031
Iteration 108/1000 | Loss: 0.00001031
Iteration 109/1000 | Loss: 0.00001031
Iteration 110/1000 | Loss: 0.00001031
Iteration 111/1000 | Loss: 0.00001031
Iteration 112/1000 | Loss: 0.00001031
Iteration 113/1000 | Loss: 0.00001031
Iteration 114/1000 | Loss: 0.00001031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.0313567145203706e-05, 1.0313567145203706e-05, 1.0313567145203706e-05, 1.0313567145203706e-05, 1.0313567145203706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0313567145203706e-05

Optimization complete. Final v2v error: 2.697908878326416 mm

Highest mean error: 2.9431965351104736 mm for frame 125

Lowest mean error: 2.5396478176116943 mm for frame 6

Saving results

Total time: 33.433673620224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601398
Iteration 2/25 | Loss: 0.00137927
Iteration 3/25 | Loss: 0.00090752
Iteration 4/25 | Loss: 0.00082561
Iteration 5/25 | Loss: 0.00081168
Iteration 6/25 | Loss: 0.00080781
Iteration 7/25 | Loss: 0.00080708
Iteration 8/25 | Loss: 0.00080704
Iteration 9/25 | Loss: 0.00080704
Iteration 10/25 | Loss: 0.00080704
Iteration 11/25 | Loss: 0.00080704
Iteration 12/25 | Loss: 0.00080704
Iteration 13/25 | Loss: 0.00080704
Iteration 14/25 | Loss: 0.00080704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008070403710007668, 0.0008070403710007668, 0.0008070403710007668, 0.0008070403710007668, 0.0008070403710007668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008070403710007668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51417434
Iteration 2/25 | Loss: 0.00048284
Iteration 3/25 | Loss: 0.00048282
Iteration 4/25 | Loss: 0.00048282
Iteration 5/25 | Loss: 0.00048282
Iteration 6/25 | Loss: 0.00048282
Iteration 7/25 | Loss: 0.00048282
Iteration 8/25 | Loss: 0.00048282
Iteration 9/25 | Loss: 0.00048282
Iteration 10/25 | Loss: 0.00048282
Iteration 11/25 | Loss: 0.00048282
Iteration 12/25 | Loss: 0.00048282
Iteration 13/25 | Loss: 0.00048282
Iteration 14/25 | Loss: 0.00048282
Iteration 15/25 | Loss: 0.00048282
Iteration 16/25 | Loss: 0.00048282
Iteration 17/25 | Loss: 0.00048282
Iteration 18/25 | Loss: 0.00048282
Iteration 19/25 | Loss: 0.00048282
Iteration 20/25 | Loss: 0.00048282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00048282090574502945, 0.00048282090574502945, 0.00048282090574502945, 0.00048282090574502945, 0.00048282090574502945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048282090574502945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048282
Iteration 2/1000 | Loss: 0.00003079
Iteration 3/1000 | Loss: 0.00002368
Iteration 4/1000 | Loss: 0.00002165
Iteration 5/1000 | Loss: 0.00002040
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001921
Iteration 8/1000 | Loss: 0.00001885
Iteration 9/1000 | Loss: 0.00001862
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001839
Iteration 12/1000 | Loss: 0.00001831
Iteration 13/1000 | Loss: 0.00001828
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001826
Iteration 16/1000 | Loss: 0.00001818
Iteration 17/1000 | Loss: 0.00001814
Iteration 18/1000 | Loss: 0.00001808
Iteration 19/1000 | Loss: 0.00001804
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001802
Iteration 22/1000 | Loss: 0.00001800
Iteration 23/1000 | Loss: 0.00001800
Iteration 24/1000 | Loss: 0.00001800
Iteration 25/1000 | Loss: 0.00001800
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001797
Iteration 30/1000 | Loss: 0.00001797
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001797
Iteration 34/1000 | Loss: 0.00001797
Iteration 35/1000 | Loss: 0.00001797
Iteration 36/1000 | Loss: 0.00001797
Iteration 37/1000 | Loss: 0.00001797
Iteration 38/1000 | Loss: 0.00001796
Iteration 39/1000 | Loss: 0.00001796
Iteration 40/1000 | Loss: 0.00001796
Iteration 41/1000 | Loss: 0.00001796
Iteration 42/1000 | Loss: 0.00001791
Iteration 43/1000 | Loss: 0.00001791
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001787
Iteration 52/1000 | Loss: 0.00001787
Iteration 53/1000 | Loss: 0.00001787
Iteration 54/1000 | Loss: 0.00001787
Iteration 55/1000 | Loss: 0.00001787
Iteration 56/1000 | Loss: 0.00001787
Iteration 57/1000 | Loss: 0.00001787
Iteration 58/1000 | Loss: 0.00001787
Iteration 59/1000 | Loss: 0.00001787
Iteration 60/1000 | Loss: 0.00001786
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001786
Iteration 64/1000 | Loss: 0.00001786
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001786
Iteration 68/1000 | Loss: 0.00001786
Iteration 69/1000 | Loss: 0.00001785
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001785
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001784
Iteration 80/1000 | Loss: 0.00001784
Iteration 81/1000 | Loss: 0.00001784
Iteration 82/1000 | Loss: 0.00001784
Iteration 83/1000 | Loss: 0.00001784
Iteration 84/1000 | Loss: 0.00001784
Iteration 85/1000 | Loss: 0.00001784
Iteration 86/1000 | Loss: 0.00001784
Iteration 87/1000 | Loss: 0.00001784
Iteration 88/1000 | Loss: 0.00001783
Iteration 89/1000 | Loss: 0.00001783
Iteration 90/1000 | Loss: 0.00001783
Iteration 91/1000 | Loss: 0.00001783
Iteration 92/1000 | Loss: 0.00001783
Iteration 93/1000 | Loss: 0.00001783
Iteration 94/1000 | Loss: 0.00001783
Iteration 95/1000 | Loss: 0.00001783
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00001783
Iteration 98/1000 | Loss: 0.00001783
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001783
Iteration 101/1000 | Loss: 0.00001783
Iteration 102/1000 | Loss: 0.00001783
Iteration 103/1000 | Loss: 0.00001783
Iteration 104/1000 | Loss: 0.00001782
Iteration 105/1000 | Loss: 0.00001782
Iteration 106/1000 | Loss: 0.00001782
Iteration 107/1000 | Loss: 0.00001782
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001782
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001782
Iteration 124/1000 | Loss: 0.00001782
Iteration 125/1000 | Loss: 0.00001782
Iteration 126/1000 | Loss: 0.00001782
Iteration 127/1000 | Loss: 0.00001782
Iteration 128/1000 | Loss: 0.00001782
Iteration 129/1000 | Loss: 0.00001782
Iteration 130/1000 | Loss: 0.00001782
Iteration 131/1000 | Loss: 0.00001782
Iteration 132/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.78160444193054e-05, 1.78160444193054e-05, 1.78160444193054e-05, 1.78160444193054e-05, 1.78160444193054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.78160444193054e-05

Optimization complete. Final v2v error: 3.590031147003174 mm

Highest mean error: 4.2681450843811035 mm for frame 56

Lowest mean error: 3.2314271926879883 mm for frame 131

Saving results

Total time: 35.21759748458862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039478
Iteration 2/25 | Loss: 0.00190931
Iteration 3/25 | Loss: 0.00126242
Iteration 4/25 | Loss: 0.00112565
Iteration 5/25 | Loss: 0.00117017
Iteration 6/25 | Loss: 0.00105827
Iteration 7/25 | Loss: 0.00101385
Iteration 8/25 | Loss: 0.00097979
Iteration 9/25 | Loss: 0.00095386
Iteration 10/25 | Loss: 0.00093074
Iteration 11/25 | Loss: 0.00092220
Iteration 12/25 | Loss: 0.00092002
Iteration 13/25 | Loss: 0.00091953
Iteration 14/25 | Loss: 0.00091935
Iteration 15/25 | Loss: 0.00091929
Iteration 16/25 | Loss: 0.00091929
Iteration 17/25 | Loss: 0.00091929
Iteration 18/25 | Loss: 0.00091928
Iteration 19/25 | Loss: 0.00091928
Iteration 20/25 | Loss: 0.00091928
Iteration 21/25 | Loss: 0.00091928
Iteration 22/25 | Loss: 0.00091928
Iteration 23/25 | Loss: 0.00091928
Iteration 24/25 | Loss: 0.00091928
Iteration 25/25 | Loss: 0.00091928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90293193
Iteration 2/25 | Loss: 0.00074577
Iteration 3/25 | Loss: 0.00074574
Iteration 4/25 | Loss: 0.00074574
Iteration 5/25 | Loss: 0.00074574
Iteration 6/25 | Loss: 0.00074574
Iteration 7/25 | Loss: 0.00074574
Iteration 8/25 | Loss: 0.00074574
Iteration 9/25 | Loss: 0.00068201
Iteration 10/25 | Loss: 0.00068200
Iteration 11/25 | Loss: 0.00068200
Iteration 12/25 | Loss: 0.00068200
Iteration 13/25 | Loss: 0.00068200
Iteration 14/25 | Loss: 0.00068200
Iteration 15/25 | Loss: 0.00068200
Iteration 16/25 | Loss: 0.00068200
Iteration 17/25 | Loss: 0.00068200
Iteration 18/25 | Loss: 0.00068200
Iteration 19/25 | Loss: 0.00068200
Iteration 20/25 | Loss: 0.00068200
Iteration 21/25 | Loss: 0.00068200
Iteration 22/25 | Loss: 0.00068200
Iteration 23/25 | Loss: 0.00068200
Iteration 24/25 | Loss: 0.00068200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006819954724051058, 0.0006819954724051058, 0.0006819954724051058, 0.0006819954724051058, 0.0006819954724051058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006819954724051058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068200
Iteration 2/1000 | Loss: 0.00005918
Iteration 3/1000 | Loss: 0.00004245
Iteration 4/1000 | Loss: 0.00003663
Iteration 5/1000 | Loss: 0.00003478
Iteration 6/1000 | Loss: 0.00003348
Iteration 7/1000 | Loss: 0.00065180
Iteration 8/1000 | Loss: 0.00068706
Iteration 9/1000 | Loss: 0.00012082
Iteration 10/1000 | Loss: 0.00063475
Iteration 11/1000 | Loss: 0.00045346
Iteration 12/1000 | Loss: 0.00058393
Iteration 13/1000 | Loss: 0.00004653
Iteration 14/1000 | Loss: 0.00043439
Iteration 15/1000 | Loss: 0.00019679
Iteration 16/1000 | Loss: 0.00012798
Iteration 17/1000 | Loss: 0.00025284
Iteration 18/1000 | Loss: 0.00020192
Iteration 19/1000 | Loss: 0.00078734
Iteration 20/1000 | Loss: 0.00030815
Iteration 21/1000 | Loss: 0.00061977
Iteration 22/1000 | Loss: 0.00021026
Iteration 23/1000 | Loss: 0.00050297
Iteration 24/1000 | Loss: 0.00059966
Iteration 25/1000 | Loss: 0.00003815
Iteration 26/1000 | Loss: 0.00032888
Iteration 27/1000 | Loss: 0.00003230
Iteration 28/1000 | Loss: 0.00003072
Iteration 29/1000 | Loss: 0.00002980
Iteration 30/1000 | Loss: 0.00002928
Iteration 31/1000 | Loss: 0.00002900
Iteration 32/1000 | Loss: 0.00002864
Iteration 33/1000 | Loss: 0.00021547
Iteration 34/1000 | Loss: 0.00014444
Iteration 35/1000 | Loss: 0.00035790
Iteration 36/1000 | Loss: 0.00002959
Iteration 37/1000 | Loss: 0.00002832
Iteration 38/1000 | Loss: 0.00002817
Iteration 39/1000 | Loss: 0.00018012
Iteration 40/1000 | Loss: 0.00156361
Iteration 41/1000 | Loss: 0.00017695
Iteration 42/1000 | Loss: 0.00002833
Iteration 43/1000 | Loss: 0.00017984
Iteration 44/1000 | Loss: 0.00052608
Iteration 45/1000 | Loss: 0.00033513
Iteration 46/1000 | Loss: 0.00073605
Iteration 47/1000 | Loss: 0.00022621
Iteration 48/1000 | Loss: 0.00015132
Iteration 49/1000 | Loss: 0.00012026
Iteration 50/1000 | Loss: 0.00013513
Iteration 51/1000 | Loss: 0.00037503
Iteration 52/1000 | Loss: 0.00029291
Iteration 53/1000 | Loss: 0.00098577
Iteration 54/1000 | Loss: 0.00020391
Iteration 55/1000 | Loss: 0.00030615
Iteration 56/1000 | Loss: 0.00003139
Iteration 57/1000 | Loss: 0.00007156
Iteration 58/1000 | Loss: 0.00002900
Iteration 59/1000 | Loss: 0.00002854
Iteration 60/1000 | Loss: 0.00002828
Iteration 61/1000 | Loss: 0.00002808
Iteration 62/1000 | Loss: 0.00002805
Iteration 63/1000 | Loss: 0.00002785
Iteration 64/1000 | Loss: 0.00002766
Iteration 65/1000 | Loss: 0.00002758
Iteration 66/1000 | Loss: 0.00002757
Iteration 67/1000 | Loss: 0.00002755
Iteration 68/1000 | Loss: 0.00002747
Iteration 69/1000 | Loss: 0.00002747
Iteration 70/1000 | Loss: 0.00002745
Iteration 71/1000 | Loss: 0.00002745
Iteration 72/1000 | Loss: 0.00002745
Iteration 73/1000 | Loss: 0.00002745
Iteration 74/1000 | Loss: 0.00002745
Iteration 75/1000 | Loss: 0.00002745
Iteration 76/1000 | Loss: 0.00002744
Iteration 77/1000 | Loss: 0.00002744
Iteration 78/1000 | Loss: 0.00002744
Iteration 79/1000 | Loss: 0.00002744
Iteration 80/1000 | Loss: 0.00002743
Iteration 81/1000 | Loss: 0.00002743
Iteration 82/1000 | Loss: 0.00002743
Iteration 83/1000 | Loss: 0.00002743
Iteration 84/1000 | Loss: 0.00002743
Iteration 85/1000 | Loss: 0.00002743
Iteration 86/1000 | Loss: 0.00002743
Iteration 87/1000 | Loss: 0.00002743
Iteration 88/1000 | Loss: 0.00002743
Iteration 89/1000 | Loss: 0.00002742
Iteration 90/1000 | Loss: 0.00002742
Iteration 91/1000 | Loss: 0.00002742
Iteration 92/1000 | Loss: 0.00002742
Iteration 93/1000 | Loss: 0.00002742
Iteration 94/1000 | Loss: 0.00002742
Iteration 95/1000 | Loss: 0.00002741
Iteration 96/1000 | Loss: 0.00002741
Iteration 97/1000 | Loss: 0.00002741
Iteration 98/1000 | Loss: 0.00002740
Iteration 99/1000 | Loss: 0.00002740
Iteration 100/1000 | Loss: 0.00002740
Iteration 101/1000 | Loss: 0.00002740
Iteration 102/1000 | Loss: 0.00002740
Iteration 103/1000 | Loss: 0.00002740
Iteration 104/1000 | Loss: 0.00002740
Iteration 105/1000 | Loss: 0.00002740
Iteration 106/1000 | Loss: 0.00002740
Iteration 107/1000 | Loss: 0.00002740
Iteration 108/1000 | Loss: 0.00002740
Iteration 109/1000 | Loss: 0.00002740
Iteration 110/1000 | Loss: 0.00002740
Iteration 111/1000 | Loss: 0.00002740
Iteration 112/1000 | Loss: 0.00002740
Iteration 113/1000 | Loss: 0.00002740
Iteration 114/1000 | Loss: 0.00002740
Iteration 115/1000 | Loss: 0.00002740
Iteration 116/1000 | Loss: 0.00002740
Iteration 117/1000 | Loss: 0.00002740
Iteration 118/1000 | Loss: 0.00002740
Iteration 119/1000 | Loss: 0.00002740
Iteration 120/1000 | Loss: 0.00002740
Iteration 121/1000 | Loss: 0.00002740
Iteration 122/1000 | Loss: 0.00002740
Iteration 123/1000 | Loss: 0.00002740
Iteration 124/1000 | Loss: 0.00002740
Iteration 125/1000 | Loss: 0.00002740
Iteration 126/1000 | Loss: 0.00002740
Iteration 127/1000 | Loss: 0.00002740
Iteration 128/1000 | Loss: 0.00002740
Iteration 129/1000 | Loss: 0.00002740
Iteration 130/1000 | Loss: 0.00002740
Iteration 131/1000 | Loss: 0.00002740
Iteration 132/1000 | Loss: 0.00002740
Iteration 133/1000 | Loss: 0.00002740
Iteration 134/1000 | Loss: 0.00002740
Iteration 135/1000 | Loss: 0.00002740
Iteration 136/1000 | Loss: 0.00002740
Iteration 137/1000 | Loss: 0.00002740
Iteration 138/1000 | Loss: 0.00002740
Iteration 139/1000 | Loss: 0.00002740
Iteration 140/1000 | Loss: 0.00002740
Iteration 141/1000 | Loss: 0.00002740
Iteration 142/1000 | Loss: 0.00002740
Iteration 143/1000 | Loss: 0.00002740
Iteration 144/1000 | Loss: 0.00002740
Iteration 145/1000 | Loss: 0.00002740
Iteration 146/1000 | Loss: 0.00002740
Iteration 147/1000 | Loss: 0.00002740
Iteration 148/1000 | Loss: 0.00002740
Iteration 149/1000 | Loss: 0.00002740
Iteration 150/1000 | Loss: 0.00002740
Iteration 151/1000 | Loss: 0.00002740
Iteration 152/1000 | Loss: 0.00002740
Iteration 153/1000 | Loss: 0.00002740
Iteration 154/1000 | Loss: 0.00002740
Iteration 155/1000 | Loss: 0.00002740
Iteration 156/1000 | Loss: 0.00002740
Iteration 157/1000 | Loss: 0.00002740
Iteration 158/1000 | Loss: 0.00002740
Iteration 159/1000 | Loss: 0.00002740
Iteration 160/1000 | Loss: 0.00002740
Iteration 161/1000 | Loss: 0.00002740
Iteration 162/1000 | Loss: 0.00002740
Iteration 163/1000 | Loss: 0.00002740
Iteration 164/1000 | Loss: 0.00002740
Iteration 165/1000 | Loss: 0.00002740
Iteration 166/1000 | Loss: 0.00002740
Iteration 167/1000 | Loss: 0.00002740
Iteration 168/1000 | Loss: 0.00002740
Iteration 169/1000 | Loss: 0.00002740
Iteration 170/1000 | Loss: 0.00002740
Iteration 171/1000 | Loss: 0.00002740
Iteration 172/1000 | Loss: 0.00002740
Iteration 173/1000 | Loss: 0.00002740
Iteration 174/1000 | Loss: 0.00002740
Iteration 175/1000 | Loss: 0.00002740
Iteration 176/1000 | Loss: 0.00002740
Iteration 177/1000 | Loss: 0.00002740
Iteration 178/1000 | Loss: 0.00002740
Iteration 179/1000 | Loss: 0.00002740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.7399215468904004e-05, 2.7399215468904004e-05, 2.7399215468904004e-05, 2.7399215468904004e-05, 2.7399215468904004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7399215468904004e-05

Optimization complete. Final v2v error: 4.007603168487549 mm

Highest mean error: 12.050862312316895 mm for frame 117

Lowest mean error: 3.295549154281616 mm for frame 8

Saving results

Total time: 116.93040132522583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375911
Iteration 2/25 | Loss: 0.00089048
Iteration 3/25 | Loss: 0.00075577
Iteration 4/25 | Loss: 0.00074038
Iteration 5/25 | Loss: 0.00073515
Iteration 6/25 | Loss: 0.00073365
Iteration 7/25 | Loss: 0.00073349
Iteration 8/25 | Loss: 0.00073347
Iteration 9/25 | Loss: 0.00073347
Iteration 10/25 | Loss: 0.00073347
Iteration 11/25 | Loss: 0.00073347
Iteration 12/25 | Loss: 0.00073347
Iteration 13/25 | Loss: 0.00073347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007334734546020627, 0.0007334734546020627, 0.0007334734546020627, 0.0007334734546020627, 0.0007334734546020627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007334734546020627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65062404
Iteration 2/25 | Loss: 0.00052020
Iteration 3/25 | Loss: 0.00052020
Iteration 4/25 | Loss: 0.00052019
Iteration 5/25 | Loss: 0.00052019
Iteration 6/25 | Loss: 0.00052019
Iteration 7/25 | Loss: 0.00052019
Iteration 8/25 | Loss: 0.00052019
Iteration 9/25 | Loss: 0.00052019
Iteration 10/25 | Loss: 0.00052019
Iteration 11/25 | Loss: 0.00052019
Iteration 12/25 | Loss: 0.00052019
Iteration 13/25 | Loss: 0.00052019
Iteration 14/25 | Loss: 0.00052019
Iteration 15/25 | Loss: 0.00052019
Iteration 16/25 | Loss: 0.00052019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005201925523579121, 0.0005201925523579121, 0.0005201925523579121, 0.0005201925523579121, 0.0005201925523579121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005201925523579121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052019
Iteration 2/1000 | Loss: 0.00002473
Iteration 3/1000 | Loss: 0.00001536
Iteration 4/1000 | Loss: 0.00001378
Iteration 5/1000 | Loss: 0.00001290
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001200
Iteration 8/1000 | Loss: 0.00001169
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001132
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001123
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001112
Iteration 16/1000 | Loss: 0.00001111
Iteration 17/1000 | Loss: 0.00001109
Iteration 18/1000 | Loss: 0.00001108
Iteration 19/1000 | Loss: 0.00001108
Iteration 20/1000 | Loss: 0.00001107
Iteration 21/1000 | Loss: 0.00001107
Iteration 22/1000 | Loss: 0.00001106
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001105
Iteration 25/1000 | Loss: 0.00001104
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001102
Iteration 28/1000 | Loss: 0.00001102
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001101
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001101
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001097
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001097
Iteration 44/1000 | Loss: 0.00001097
Iteration 45/1000 | Loss: 0.00001096
Iteration 46/1000 | Loss: 0.00001096
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001093
Iteration 60/1000 | Loss: 0.00001093
Iteration 61/1000 | Loss: 0.00001092
Iteration 62/1000 | Loss: 0.00001092
Iteration 63/1000 | Loss: 0.00001092
Iteration 64/1000 | Loss: 0.00001092
Iteration 65/1000 | Loss: 0.00001091
Iteration 66/1000 | Loss: 0.00001091
Iteration 67/1000 | Loss: 0.00001091
Iteration 68/1000 | Loss: 0.00001091
Iteration 69/1000 | Loss: 0.00001091
Iteration 70/1000 | Loss: 0.00001091
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001091
Iteration 73/1000 | Loss: 0.00001091
Iteration 74/1000 | Loss: 0.00001091
Iteration 75/1000 | Loss: 0.00001091
Iteration 76/1000 | Loss: 0.00001091
Iteration 77/1000 | Loss: 0.00001091
Iteration 78/1000 | Loss: 0.00001091
Iteration 79/1000 | Loss: 0.00001090
Iteration 80/1000 | Loss: 0.00001090
Iteration 81/1000 | Loss: 0.00001090
Iteration 82/1000 | Loss: 0.00001090
Iteration 83/1000 | Loss: 0.00001090
Iteration 84/1000 | Loss: 0.00001089
Iteration 85/1000 | Loss: 0.00001089
Iteration 86/1000 | Loss: 0.00001089
Iteration 87/1000 | Loss: 0.00001089
Iteration 88/1000 | Loss: 0.00001089
Iteration 89/1000 | Loss: 0.00001089
Iteration 90/1000 | Loss: 0.00001089
Iteration 91/1000 | Loss: 0.00001089
Iteration 92/1000 | Loss: 0.00001089
Iteration 93/1000 | Loss: 0.00001089
Iteration 94/1000 | Loss: 0.00001089
Iteration 95/1000 | Loss: 0.00001089
Iteration 96/1000 | Loss: 0.00001089
Iteration 97/1000 | Loss: 0.00001089
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.0890932571783196e-05, 1.0890932571783196e-05, 1.0890932571783196e-05, 1.0890932571783196e-05, 1.0890932571783196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0890932571783196e-05

Optimization complete. Final v2v error: 2.817093849182129 mm

Highest mean error: 3.1518399715423584 mm for frame 131

Lowest mean error: 2.6264748573303223 mm for frame 21

Saving results

Total time: 36.09623169898987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468280
Iteration 2/25 | Loss: 0.00091578
Iteration 3/25 | Loss: 0.00078703
Iteration 4/25 | Loss: 0.00076838
Iteration 5/25 | Loss: 0.00076287
Iteration 6/25 | Loss: 0.00076142
Iteration 7/25 | Loss: 0.00076107
Iteration 8/25 | Loss: 0.00076107
Iteration 9/25 | Loss: 0.00076107
Iteration 10/25 | Loss: 0.00076107
Iteration 11/25 | Loss: 0.00076107
Iteration 12/25 | Loss: 0.00076107
Iteration 13/25 | Loss: 0.00076107
Iteration 14/25 | Loss: 0.00076107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000761072151362896, 0.000761072151362896, 0.000761072151362896, 0.000761072151362896, 0.000761072151362896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000761072151362896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.26458549
Iteration 2/25 | Loss: 0.00051579
Iteration 3/25 | Loss: 0.00051579
Iteration 4/25 | Loss: 0.00051579
Iteration 5/25 | Loss: 0.00051579
Iteration 6/25 | Loss: 0.00051579
Iteration 7/25 | Loss: 0.00051579
Iteration 8/25 | Loss: 0.00051579
Iteration 9/25 | Loss: 0.00051579
Iteration 10/25 | Loss: 0.00051579
Iteration 11/25 | Loss: 0.00051579
Iteration 12/25 | Loss: 0.00051579
Iteration 13/25 | Loss: 0.00051579
Iteration 14/25 | Loss: 0.00051579
Iteration 15/25 | Loss: 0.00051579
Iteration 16/25 | Loss: 0.00051579
Iteration 17/25 | Loss: 0.00051579
Iteration 18/25 | Loss: 0.00051579
Iteration 19/25 | Loss: 0.00051579
Iteration 20/25 | Loss: 0.00051579
Iteration 21/25 | Loss: 0.00051579
Iteration 22/25 | Loss: 0.00051579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005157857667654753, 0.0005157857667654753, 0.0005157857667654753, 0.0005157857667654753, 0.0005157857667654753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005157857667654753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051579
Iteration 2/1000 | Loss: 0.00002986
Iteration 3/1000 | Loss: 0.00001912
Iteration 4/1000 | Loss: 0.00001708
Iteration 5/1000 | Loss: 0.00001609
Iteration 6/1000 | Loss: 0.00001546
Iteration 7/1000 | Loss: 0.00001507
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001443
Iteration 12/1000 | Loss: 0.00001442
Iteration 13/1000 | Loss: 0.00001441
Iteration 14/1000 | Loss: 0.00001440
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001439
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001424
Iteration 20/1000 | Loss: 0.00001421
Iteration 21/1000 | Loss: 0.00001420
Iteration 22/1000 | Loss: 0.00001420
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001416
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001414
Iteration 31/1000 | Loss: 0.00001413
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001411
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001411
Iteration 37/1000 | Loss: 0.00001411
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001411
Iteration 43/1000 | Loss: 0.00001410
Iteration 44/1000 | Loss: 0.00001410
Iteration 45/1000 | Loss: 0.00001410
Iteration 46/1000 | Loss: 0.00001410
Iteration 47/1000 | Loss: 0.00001410
Iteration 48/1000 | Loss: 0.00001409
Iteration 49/1000 | Loss: 0.00001408
Iteration 50/1000 | Loss: 0.00001408
Iteration 51/1000 | Loss: 0.00001408
Iteration 52/1000 | Loss: 0.00001408
Iteration 53/1000 | Loss: 0.00001408
Iteration 54/1000 | Loss: 0.00001407
Iteration 55/1000 | Loss: 0.00001407
Iteration 56/1000 | Loss: 0.00001407
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001407
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001407
Iteration 61/1000 | Loss: 0.00001407
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001407
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001407
Iteration 66/1000 | Loss: 0.00001406
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001406
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001405
Iteration 72/1000 | Loss: 0.00001405
Iteration 73/1000 | Loss: 0.00001405
Iteration 74/1000 | Loss: 0.00001405
Iteration 75/1000 | Loss: 0.00001405
Iteration 76/1000 | Loss: 0.00001405
Iteration 77/1000 | Loss: 0.00001405
Iteration 78/1000 | Loss: 0.00001404
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001404
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001403
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001403
Iteration 93/1000 | Loss: 0.00001403
Iteration 94/1000 | Loss: 0.00001403
Iteration 95/1000 | Loss: 0.00001402
Iteration 96/1000 | Loss: 0.00001402
Iteration 97/1000 | Loss: 0.00001402
Iteration 98/1000 | Loss: 0.00001402
Iteration 99/1000 | Loss: 0.00001402
Iteration 100/1000 | Loss: 0.00001402
Iteration 101/1000 | Loss: 0.00001402
Iteration 102/1000 | Loss: 0.00001402
Iteration 103/1000 | Loss: 0.00001402
Iteration 104/1000 | Loss: 0.00001402
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001402
Iteration 107/1000 | Loss: 0.00001402
Iteration 108/1000 | Loss: 0.00001402
Iteration 109/1000 | Loss: 0.00001401
Iteration 110/1000 | Loss: 0.00001401
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001401
Iteration 115/1000 | Loss: 0.00001401
Iteration 116/1000 | Loss: 0.00001401
Iteration 117/1000 | Loss: 0.00001401
Iteration 118/1000 | Loss: 0.00001401
Iteration 119/1000 | Loss: 0.00001401
Iteration 120/1000 | Loss: 0.00001401
Iteration 121/1000 | Loss: 0.00001401
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001400
Iteration 125/1000 | Loss: 0.00001400
Iteration 126/1000 | Loss: 0.00001400
Iteration 127/1000 | Loss: 0.00001400
Iteration 128/1000 | Loss: 0.00001400
Iteration 129/1000 | Loss: 0.00001400
Iteration 130/1000 | Loss: 0.00001400
Iteration 131/1000 | Loss: 0.00001400
Iteration 132/1000 | Loss: 0.00001400
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001400
Iteration 137/1000 | Loss: 0.00001399
Iteration 138/1000 | Loss: 0.00001399
Iteration 139/1000 | Loss: 0.00001399
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001398
Iteration 146/1000 | Loss: 0.00001398
Iteration 147/1000 | Loss: 0.00001398
Iteration 148/1000 | Loss: 0.00001398
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001398
Iteration 155/1000 | Loss: 0.00001398
Iteration 156/1000 | Loss: 0.00001398
Iteration 157/1000 | Loss: 0.00001398
Iteration 158/1000 | Loss: 0.00001398
Iteration 159/1000 | Loss: 0.00001398
Iteration 160/1000 | Loss: 0.00001398
Iteration 161/1000 | Loss: 0.00001398
Iteration 162/1000 | Loss: 0.00001398
Iteration 163/1000 | Loss: 0.00001398
Iteration 164/1000 | Loss: 0.00001398
Iteration 165/1000 | Loss: 0.00001398
Iteration 166/1000 | Loss: 0.00001398
Iteration 167/1000 | Loss: 0.00001398
Iteration 168/1000 | Loss: 0.00001398
Iteration 169/1000 | Loss: 0.00001398
Iteration 170/1000 | Loss: 0.00001398
Iteration 171/1000 | Loss: 0.00001398
Iteration 172/1000 | Loss: 0.00001398
Iteration 173/1000 | Loss: 0.00001398
Iteration 174/1000 | Loss: 0.00001398
Iteration 175/1000 | Loss: 0.00001398
Iteration 176/1000 | Loss: 0.00001398
Iteration 177/1000 | Loss: 0.00001398
Iteration 178/1000 | Loss: 0.00001398
Iteration 179/1000 | Loss: 0.00001398
Iteration 180/1000 | Loss: 0.00001398
Iteration 181/1000 | Loss: 0.00001398
Iteration 182/1000 | Loss: 0.00001398
Iteration 183/1000 | Loss: 0.00001398
Iteration 184/1000 | Loss: 0.00001398
Iteration 185/1000 | Loss: 0.00001398
Iteration 186/1000 | Loss: 0.00001398
Iteration 187/1000 | Loss: 0.00001398
Iteration 188/1000 | Loss: 0.00001398
Iteration 189/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.3979686627862975e-05, 1.3979686627862975e-05, 1.3979686627862975e-05, 1.3979686627862975e-05, 1.3979686627862975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3979686627862975e-05

Optimization complete. Final v2v error: 3.205707550048828 mm

Highest mean error: 3.5306289196014404 mm for frame 108

Lowest mean error: 2.915550947189331 mm for frame 136

Saving results

Total time: 34.8756320476532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698761
Iteration 2/25 | Loss: 0.00145657
Iteration 3/25 | Loss: 0.00104018
Iteration 4/25 | Loss: 0.00090376
Iteration 5/25 | Loss: 0.00086296
Iteration 6/25 | Loss: 0.00085767
Iteration 7/25 | Loss: 0.00085470
Iteration 8/25 | Loss: 0.00085315
Iteration 9/25 | Loss: 0.00085066
Iteration 10/25 | Loss: 0.00084839
Iteration 11/25 | Loss: 0.00084246
Iteration 12/25 | Loss: 0.00083541
Iteration 13/25 | Loss: 0.00083376
Iteration 14/25 | Loss: 0.00083290
Iteration 15/25 | Loss: 0.00083241
Iteration 16/25 | Loss: 0.00083226
Iteration 17/25 | Loss: 0.00083169
Iteration 18/25 | Loss: 0.00083133
Iteration 19/25 | Loss: 0.00083115
Iteration 20/25 | Loss: 0.00083100
Iteration 21/25 | Loss: 0.00083090
Iteration 22/25 | Loss: 0.00083087
Iteration 23/25 | Loss: 0.00083087
Iteration 24/25 | Loss: 0.00083087
Iteration 25/25 | Loss: 0.00083086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38452244
Iteration 2/25 | Loss: 0.00085072
Iteration 3/25 | Loss: 0.00064051
Iteration 4/25 | Loss: 0.00064050
Iteration 5/25 | Loss: 0.00064050
Iteration 6/25 | Loss: 0.00064050
Iteration 7/25 | Loss: 0.00064050
Iteration 8/25 | Loss: 0.00064050
Iteration 9/25 | Loss: 0.00064050
Iteration 10/25 | Loss: 0.00064050
Iteration 11/25 | Loss: 0.00064050
Iteration 12/25 | Loss: 0.00064050
Iteration 13/25 | Loss: 0.00064050
Iteration 14/25 | Loss: 0.00064050
Iteration 15/25 | Loss: 0.00064050
Iteration 16/25 | Loss: 0.00064050
Iteration 17/25 | Loss: 0.00064050
Iteration 18/25 | Loss: 0.00064050
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000640502548776567, 0.000640502548776567, 0.000640502548776567, 0.000640502548776567, 0.000640502548776567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000640502548776567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064050
Iteration 2/1000 | Loss: 0.00022244
Iteration 3/1000 | Loss: 0.00009489
Iteration 4/1000 | Loss: 0.00013513
Iteration 5/1000 | Loss: 0.00003653
Iteration 6/1000 | Loss: 0.00002929
Iteration 7/1000 | Loss: 0.00002378
Iteration 8/1000 | Loss: 0.00002241
Iteration 9/1000 | Loss: 0.00002144
Iteration 10/1000 | Loss: 0.00002050
Iteration 11/1000 | Loss: 0.00001992
Iteration 12/1000 | Loss: 0.00001945
Iteration 13/1000 | Loss: 0.00033636
Iteration 14/1000 | Loss: 0.00020542
Iteration 15/1000 | Loss: 0.00002766
Iteration 16/1000 | Loss: 0.00002039
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00001648
Iteration 21/1000 | Loss: 0.00001621
Iteration 22/1000 | Loss: 0.00001618
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00001617
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00001614
Iteration 28/1000 | Loss: 0.00001613
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001608
Iteration 31/1000 | Loss: 0.00001607
Iteration 32/1000 | Loss: 0.00001607
Iteration 33/1000 | Loss: 0.00001606
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001604
Iteration 36/1000 | Loss: 0.00001603
Iteration 37/1000 | Loss: 0.00001603
Iteration 38/1000 | Loss: 0.00001602
Iteration 39/1000 | Loss: 0.00001598
Iteration 40/1000 | Loss: 0.00001598
Iteration 41/1000 | Loss: 0.00001590
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001588
Iteration 44/1000 | Loss: 0.00001587
Iteration 45/1000 | Loss: 0.00001586
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001585
Iteration 48/1000 | Loss: 0.00001585
Iteration 49/1000 | Loss: 0.00001585
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001584
Iteration 56/1000 | Loss: 0.00001583
Iteration 57/1000 | Loss: 0.00001583
Iteration 58/1000 | Loss: 0.00001583
Iteration 59/1000 | Loss: 0.00001583
Iteration 60/1000 | Loss: 0.00001583
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001582
Iteration 63/1000 | Loss: 0.00001582
Iteration 64/1000 | Loss: 0.00001582
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001582
Iteration 67/1000 | Loss: 0.00001581
Iteration 68/1000 | Loss: 0.00001581
Iteration 69/1000 | Loss: 0.00001581
Iteration 70/1000 | Loss: 0.00001580
Iteration 71/1000 | Loss: 0.00001580
Iteration 72/1000 | Loss: 0.00001580
Iteration 73/1000 | Loss: 0.00001580
Iteration 74/1000 | Loss: 0.00001579
Iteration 75/1000 | Loss: 0.00001579
Iteration 76/1000 | Loss: 0.00001579
Iteration 77/1000 | Loss: 0.00001579
Iteration 78/1000 | Loss: 0.00001578
Iteration 79/1000 | Loss: 0.00001578
Iteration 80/1000 | Loss: 0.00001578
Iteration 81/1000 | Loss: 0.00001577
Iteration 82/1000 | Loss: 0.00001577
Iteration 83/1000 | Loss: 0.00001577
Iteration 84/1000 | Loss: 0.00001576
Iteration 85/1000 | Loss: 0.00001576
Iteration 86/1000 | Loss: 0.00001575
Iteration 87/1000 | Loss: 0.00001575
Iteration 88/1000 | Loss: 0.00001575
Iteration 89/1000 | Loss: 0.00001575
Iteration 90/1000 | Loss: 0.00001575
Iteration 91/1000 | Loss: 0.00001575
Iteration 92/1000 | Loss: 0.00001575
Iteration 93/1000 | Loss: 0.00001574
Iteration 94/1000 | Loss: 0.00001574
Iteration 95/1000 | Loss: 0.00001574
Iteration 96/1000 | Loss: 0.00001574
Iteration 97/1000 | Loss: 0.00001574
Iteration 98/1000 | Loss: 0.00001574
Iteration 99/1000 | Loss: 0.00001573
Iteration 100/1000 | Loss: 0.00001573
Iteration 101/1000 | Loss: 0.00001573
Iteration 102/1000 | Loss: 0.00001573
Iteration 103/1000 | Loss: 0.00001573
Iteration 104/1000 | Loss: 0.00001573
Iteration 105/1000 | Loss: 0.00001573
Iteration 106/1000 | Loss: 0.00001573
Iteration 107/1000 | Loss: 0.00001573
Iteration 108/1000 | Loss: 0.00001572
Iteration 109/1000 | Loss: 0.00001572
Iteration 110/1000 | Loss: 0.00001572
Iteration 111/1000 | Loss: 0.00001572
Iteration 112/1000 | Loss: 0.00001572
Iteration 113/1000 | Loss: 0.00001572
Iteration 114/1000 | Loss: 0.00001572
Iteration 115/1000 | Loss: 0.00001572
Iteration 116/1000 | Loss: 0.00001571
Iteration 117/1000 | Loss: 0.00001571
Iteration 118/1000 | Loss: 0.00001571
Iteration 119/1000 | Loss: 0.00001571
Iteration 120/1000 | Loss: 0.00001571
Iteration 121/1000 | Loss: 0.00001571
Iteration 122/1000 | Loss: 0.00001571
Iteration 123/1000 | Loss: 0.00001571
Iteration 124/1000 | Loss: 0.00001571
Iteration 125/1000 | Loss: 0.00001571
Iteration 126/1000 | Loss: 0.00001571
Iteration 127/1000 | Loss: 0.00001570
Iteration 128/1000 | Loss: 0.00001570
Iteration 129/1000 | Loss: 0.00001570
Iteration 130/1000 | Loss: 0.00001570
Iteration 131/1000 | Loss: 0.00001570
Iteration 132/1000 | Loss: 0.00001570
Iteration 133/1000 | Loss: 0.00001570
Iteration 134/1000 | Loss: 0.00001570
Iteration 135/1000 | Loss: 0.00001570
Iteration 136/1000 | Loss: 0.00001570
Iteration 137/1000 | Loss: 0.00001569
Iteration 138/1000 | Loss: 0.00001569
Iteration 139/1000 | Loss: 0.00001569
Iteration 140/1000 | Loss: 0.00001569
Iteration 141/1000 | Loss: 0.00001569
Iteration 142/1000 | Loss: 0.00001569
Iteration 143/1000 | Loss: 0.00001569
Iteration 144/1000 | Loss: 0.00001569
Iteration 145/1000 | Loss: 0.00001569
Iteration 146/1000 | Loss: 0.00001569
Iteration 147/1000 | Loss: 0.00001569
Iteration 148/1000 | Loss: 0.00001569
Iteration 149/1000 | Loss: 0.00001569
Iteration 150/1000 | Loss: 0.00001568
Iteration 151/1000 | Loss: 0.00001568
Iteration 152/1000 | Loss: 0.00001568
Iteration 153/1000 | Loss: 0.00001568
Iteration 154/1000 | Loss: 0.00001568
Iteration 155/1000 | Loss: 0.00001568
Iteration 156/1000 | Loss: 0.00001568
Iteration 157/1000 | Loss: 0.00001568
Iteration 158/1000 | Loss: 0.00001568
Iteration 159/1000 | Loss: 0.00001568
Iteration 160/1000 | Loss: 0.00001568
Iteration 161/1000 | Loss: 0.00001568
Iteration 162/1000 | Loss: 0.00001568
Iteration 163/1000 | Loss: 0.00001568
Iteration 164/1000 | Loss: 0.00001568
Iteration 165/1000 | Loss: 0.00001568
Iteration 166/1000 | Loss: 0.00001568
Iteration 167/1000 | Loss: 0.00001568
Iteration 168/1000 | Loss: 0.00001568
Iteration 169/1000 | Loss: 0.00001568
Iteration 170/1000 | Loss: 0.00001568
Iteration 171/1000 | Loss: 0.00001568
Iteration 172/1000 | Loss: 0.00001568
Iteration 173/1000 | Loss: 0.00001568
Iteration 174/1000 | Loss: 0.00001568
Iteration 175/1000 | Loss: 0.00001568
Iteration 176/1000 | Loss: 0.00001568
Iteration 177/1000 | Loss: 0.00001568
Iteration 178/1000 | Loss: 0.00001568
Iteration 179/1000 | Loss: 0.00001568
Iteration 180/1000 | Loss: 0.00001568
Iteration 181/1000 | Loss: 0.00001568
Iteration 182/1000 | Loss: 0.00001568
Iteration 183/1000 | Loss: 0.00001568
Iteration 184/1000 | Loss: 0.00001568
Iteration 185/1000 | Loss: 0.00001568
Iteration 186/1000 | Loss: 0.00001568
Iteration 187/1000 | Loss: 0.00001568
Iteration 188/1000 | Loss: 0.00001568
Iteration 189/1000 | Loss: 0.00001568
Iteration 190/1000 | Loss: 0.00001568
Iteration 191/1000 | Loss: 0.00001568
Iteration 192/1000 | Loss: 0.00001568
Iteration 193/1000 | Loss: 0.00001568
Iteration 194/1000 | Loss: 0.00001568
Iteration 195/1000 | Loss: 0.00001568
Iteration 196/1000 | Loss: 0.00001568
Iteration 197/1000 | Loss: 0.00001568
Iteration 198/1000 | Loss: 0.00001568
Iteration 199/1000 | Loss: 0.00001568
Iteration 200/1000 | Loss: 0.00001568
Iteration 201/1000 | Loss: 0.00001568
Iteration 202/1000 | Loss: 0.00001568
Iteration 203/1000 | Loss: 0.00001568
Iteration 204/1000 | Loss: 0.00001568
Iteration 205/1000 | Loss: 0.00001568
Iteration 206/1000 | Loss: 0.00001568
Iteration 207/1000 | Loss: 0.00001568
Iteration 208/1000 | Loss: 0.00001568
Iteration 209/1000 | Loss: 0.00001568
Iteration 210/1000 | Loss: 0.00001568
Iteration 211/1000 | Loss: 0.00001568
Iteration 212/1000 | Loss: 0.00001568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.5678961062803864e-05, 1.5678961062803864e-05, 1.5678961062803864e-05, 1.5678961062803864e-05, 1.5678961062803864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5678961062803864e-05

Optimization complete. Final v2v error: 3.367774724960327 mm

Highest mean error: 3.864304304122925 mm for frame 148

Lowest mean error: 2.8836312294006348 mm for frame 132

Saving results

Total time: 82.21572375297546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031079
Iteration 2/25 | Loss: 0.01031079
Iteration 3/25 | Loss: 0.01031078
Iteration 4/25 | Loss: 0.01031078
Iteration 5/25 | Loss: 0.01031078
Iteration 6/25 | Loss: 0.01031078
Iteration 7/25 | Loss: 0.00294401
Iteration 8/25 | Loss: 0.00225303
Iteration 9/25 | Loss: 0.00164233
Iteration 10/25 | Loss: 0.00158352
Iteration 11/25 | Loss: 0.00147373
Iteration 12/25 | Loss: 0.00139417
Iteration 13/25 | Loss: 0.00134104
Iteration 14/25 | Loss: 0.00126610
Iteration 15/25 | Loss: 0.00125124
Iteration 16/25 | Loss: 0.00122564
Iteration 17/25 | Loss: 0.00117740
Iteration 18/25 | Loss: 0.00114287
Iteration 19/25 | Loss: 0.00111858
Iteration 20/25 | Loss: 0.00108462
Iteration 21/25 | Loss: 0.00105965
Iteration 22/25 | Loss: 0.00105322
Iteration 23/25 | Loss: 0.00105111
Iteration 24/25 | Loss: 0.00102243
Iteration 25/25 | Loss: 0.00102222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79115653
Iteration 2/25 | Loss: 0.00378460
Iteration 3/25 | Loss: 0.00221930
Iteration 4/25 | Loss: 0.00221930
Iteration 5/25 | Loss: 0.00221930
Iteration 6/25 | Loss: 0.00221930
Iteration 7/25 | Loss: 0.00221930
Iteration 8/25 | Loss: 0.00221930
Iteration 9/25 | Loss: 0.00221930
Iteration 10/25 | Loss: 0.00221930
Iteration 11/25 | Loss: 0.00221930
Iteration 12/25 | Loss: 0.00221930
Iteration 13/25 | Loss: 0.00221930
Iteration 14/25 | Loss: 0.00221930
Iteration 15/25 | Loss: 0.00221930
Iteration 16/25 | Loss: 0.00221930
Iteration 17/25 | Loss: 0.00221930
Iteration 18/25 | Loss: 0.00221930
Iteration 19/25 | Loss: 0.00221930
Iteration 20/25 | Loss: 0.00221930
Iteration 21/25 | Loss: 0.00221930
Iteration 22/25 | Loss: 0.00221930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0022192958276718855, 0.0022192958276718855, 0.0022192958276718855, 0.0022192958276718855, 0.0022192958276718855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022192958276718855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221930
Iteration 2/1000 | Loss: 0.00149824
Iteration 3/1000 | Loss: 0.00528940
Iteration 4/1000 | Loss: 0.00055963
Iteration 5/1000 | Loss: 0.00097128
Iteration 6/1000 | Loss: 0.00020543
Iteration 7/1000 | Loss: 0.00112366
Iteration 8/1000 | Loss: 0.00038394
Iteration 9/1000 | Loss: 0.00045246
Iteration 10/1000 | Loss: 0.00087075
Iteration 11/1000 | Loss: 0.00037013
Iteration 12/1000 | Loss: 0.00019863
Iteration 13/1000 | Loss: 0.00015025
Iteration 14/1000 | Loss: 0.00061989
Iteration 15/1000 | Loss: 0.00045050
Iteration 16/1000 | Loss: 0.00026852
Iteration 17/1000 | Loss: 0.00032701
Iteration 18/1000 | Loss: 0.00039260
Iteration 19/1000 | Loss: 0.00022898
Iteration 20/1000 | Loss: 0.00066241
Iteration 21/1000 | Loss: 0.00025253
Iteration 22/1000 | Loss: 0.00036034
Iteration 23/1000 | Loss: 0.00071049
Iteration 24/1000 | Loss: 0.00029355
Iteration 25/1000 | Loss: 0.00032941
Iteration 26/1000 | Loss: 0.00043409
Iteration 27/1000 | Loss: 0.00026098
Iteration 28/1000 | Loss: 0.00070291
Iteration 29/1000 | Loss: 0.00024476
Iteration 30/1000 | Loss: 0.00028447
Iteration 31/1000 | Loss: 0.00024546
Iteration 32/1000 | Loss: 0.00066572
Iteration 33/1000 | Loss: 0.00025557
Iteration 34/1000 | Loss: 0.00029118
Iteration 35/1000 | Loss: 0.00024066
Iteration 36/1000 | Loss: 0.00017013
Iteration 37/1000 | Loss: 0.00025891
Iteration 38/1000 | Loss: 0.00023506
Iteration 39/1000 | Loss: 0.00022320
Iteration 40/1000 | Loss: 0.00019823
Iteration 41/1000 | Loss: 0.00022114
Iteration 42/1000 | Loss: 0.00022140
Iteration 43/1000 | Loss: 0.00022693
Iteration 44/1000 | Loss: 0.00019317
Iteration 45/1000 | Loss: 0.00016407
Iteration 46/1000 | Loss: 0.00019124
Iteration 47/1000 | Loss: 0.00020707
Iteration 48/1000 | Loss: 0.00021206
Iteration 49/1000 | Loss: 0.00020549
Iteration 50/1000 | Loss: 0.00021291
Iteration 51/1000 | Loss: 0.00027394
Iteration 52/1000 | Loss: 0.00023100
Iteration 53/1000 | Loss: 0.00020456
Iteration 54/1000 | Loss: 0.00027922
Iteration 55/1000 | Loss: 0.00020324
Iteration 56/1000 | Loss: 0.00029031
Iteration 57/1000 | Loss: 0.00020504
Iteration 58/1000 | Loss: 0.00018881
Iteration 59/1000 | Loss: 0.00020946
Iteration 60/1000 | Loss: 0.00022559
Iteration 61/1000 | Loss: 0.00022084
Iteration 62/1000 | Loss: 0.00023607
Iteration 63/1000 | Loss: 0.00018979
Iteration 64/1000 | Loss: 0.00025802
Iteration 65/1000 | Loss: 0.00019377
Iteration 66/1000 | Loss: 0.00022970
Iteration 67/1000 | Loss: 0.00020806
Iteration 68/1000 | Loss: 0.00020241
Iteration 69/1000 | Loss: 0.00040838
Iteration 70/1000 | Loss: 0.00031125
Iteration 71/1000 | Loss: 0.00019060
Iteration 72/1000 | Loss: 0.00034325
Iteration 73/1000 | Loss: 0.00039234
Iteration 74/1000 | Loss: 0.00041708
Iteration 75/1000 | Loss: 0.00028658
Iteration 76/1000 | Loss: 0.00049095
Iteration 77/1000 | Loss: 0.00053424
Iteration 78/1000 | Loss: 0.00025974
Iteration 79/1000 | Loss: 0.00065701
Iteration 80/1000 | Loss: 0.00065735
Iteration 81/1000 | Loss: 0.00019879
Iteration 82/1000 | Loss: 0.00075733
Iteration 83/1000 | Loss: 0.00041398
Iteration 84/1000 | Loss: 0.00083158
Iteration 85/1000 | Loss: 0.00034383
Iteration 86/1000 | Loss: 0.00024088
Iteration 87/1000 | Loss: 0.00025693
Iteration 88/1000 | Loss: 0.00017056
Iteration 89/1000 | Loss: 0.00024856
Iteration 90/1000 | Loss: 0.00026167
Iteration 91/1000 | Loss: 0.00044056
Iteration 92/1000 | Loss: 0.00017508
Iteration 93/1000 | Loss: 0.00012454
Iteration 94/1000 | Loss: 0.00068903
Iteration 95/1000 | Loss: 0.00082360
Iteration 96/1000 | Loss: 0.00046570
Iteration 97/1000 | Loss: 0.00032247
Iteration 98/1000 | Loss: 0.00022507
Iteration 99/1000 | Loss: 0.00020237
Iteration 100/1000 | Loss: 0.00010098
Iteration 101/1000 | Loss: 0.00009016
Iteration 102/1000 | Loss: 0.00014830
Iteration 103/1000 | Loss: 0.00022196
Iteration 104/1000 | Loss: 0.00012552
Iteration 105/1000 | Loss: 0.00019260
Iteration 106/1000 | Loss: 0.00009819
Iteration 107/1000 | Loss: 0.00009271
Iteration 108/1000 | Loss: 0.00008470
Iteration 109/1000 | Loss: 0.00011115
Iteration 110/1000 | Loss: 0.00041828
Iteration 111/1000 | Loss: 0.00009363
Iteration 112/1000 | Loss: 0.00009290
Iteration 113/1000 | Loss: 0.00014014
Iteration 114/1000 | Loss: 0.00009234
Iteration 115/1000 | Loss: 0.00010818
Iteration 116/1000 | Loss: 0.00021809
Iteration 117/1000 | Loss: 0.00031987
Iteration 118/1000 | Loss: 0.00023133
Iteration 119/1000 | Loss: 0.00009072
Iteration 120/1000 | Loss: 0.00010729
Iteration 121/1000 | Loss: 0.00010154
Iteration 122/1000 | Loss: 0.00010429
Iteration 123/1000 | Loss: 0.00010778
Iteration 124/1000 | Loss: 0.00008787
Iteration 125/1000 | Loss: 0.00024315
Iteration 126/1000 | Loss: 0.00150733
Iteration 127/1000 | Loss: 0.00051118
Iteration 128/1000 | Loss: 0.00048705
Iteration 129/1000 | Loss: 0.00119384
Iteration 130/1000 | Loss: 0.00082197
Iteration 131/1000 | Loss: 0.00065186
Iteration 132/1000 | Loss: 0.00056726
Iteration 133/1000 | Loss: 0.00050031
Iteration 134/1000 | Loss: 0.00083731
Iteration 135/1000 | Loss: 0.00050003
Iteration 136/1000 | Loss: 0.00065900
Iteration 137/1000 | Loss: 0.00041287
Iteration 138/1000 | Loss: 0.00044508
Iteration 139/1000 | Loss: 0.00018508
Iteration 140/1000 | Loss: 0.00029509
Iteration 141/1000 | Loss: 0.00033903
Iteration 142/1000 | Loss: 0.00035381
Iteration 143/1000 | Loss: 0.00011282
Iteration 144/1000 | Loss: 0.00034791
Iteration 145/1000 | Loss: 0.00012710
Iteration 146/1000 | Loss: 0.00008974
Iteration 147/1000 | Loss: 0.00008262
Iteration 148/1000 | Loss: 0.00007350
Iteration 149/1000 | Loss: 0.00016016
Iteration 150/1000 | Loss: 0.00007546
Iteration 151/1000 | Loss: 0.00103685
Iteration 152/1000 | Loss: 0.00084641
Iteration 153/1000 | Loss: 0.00065153
Iteration 154/1000 | Loss: 0.00112866
Iteration 155/1000 | Loss: 0.00050242
Iteration 156/1000 | Loss: 0.00028659
Iteration 157/1000 | Loss: 0.00028650
Iteration 158/1000 | Loss: 0.00044000
Iteration 159/1000 | Loss: 0.00031388
Iteration 160/1000 | Loss: 0.00026067
Iteration 161/1000 | Loss: 0.00019236
Iteration 162/1000 | Loss: 0.00018233
Iteration 163/1000 | Loss: 0.00037751
Iteration 164/1000 | Loss: 0.00041267
Iteration 165/1000 | Loss: 0.00035332
Iteration 166/1000 | Loss: 0.00026127
Iteration 167/1000 | Loss: 0.00036877
Iteration 168/1000 | Loss: 0.00022566
Iteration 169/1000 | Loss: 0.00048111
Iteration 170/1000 | Loss: 0.00030014
Iteration 171/1000 | Loss: 0.00035010
Iteration 172/1000 | Loss: 0.00026032
Iteration 173/1000 | Loss: 0.00026683
Iteration 174/1000 | Loss: 0.00008211
Iteration 175/1000 | Loss: 0.00007235
Iteration 176/1000 | Loss: 0.00006945
Iteration 177/1000 | Loss: 0.00006682
Iteration 178/1000 | Loss: 0.00006531
Iteration 179/1000 | Loss: 0.00006393
Iteration 180/1000 | Loss: 0.00006270
Iteration 181/1000 | Loss: 0.00075587
Iteration 182/1000 | Loss: 0.00040970
Iteration 183/1000 | Loss: 0.00006777
Iteration 184/1000 | Loss: 0.00006324
Iteration 185/1000 | Loss: 0.00125494
Iteration 186/1000 | Loss: 0.00206788
Iteration 187/1000 | Loss: 0.00083719
Iteration 188/1000 | Loss: 0.00123815
Iteration 189/1000 | Loss: 0.00097385
Iteration 190/1000 | Loss: 0.00098996
Iteration 191/1000 | Loss: 0.00065408
Iteration 192/1000 | Loss: 0.00064819
Iteration 193/1000 | Loss: 0.00077118
Iteration 194/1000 | Loss: 0.00013406
Iteration 195/1000 | Loss: 0.00029659
Iteration 196/1000 | Loss: 0.00020736
Iteration 197/1000 | Loss: 0.00006874
Iteration 198/1000 | Loss: 0.00025381
Iteration 199/1000 | Loss: 0.00013734
Iteration 200/1000 | Loss: 0.00026920
Iteration 201/1000 | Loss: 0.00013147
Iteration 202/1000 | Loss: 0.00012892
Iteration 203/1000 | Loss: 0.00017995
Iteration 204/1000 | Loss: 0.00011207
Iteration 205/1000 | Loss: 0.00020147
Iteration 206/1000 | Loss: 0.00011197
Iteration 207/1000 | Loss: 0.00006673
Iteration 208/1000 | Loss: 0.00005858
Iteration 209/1000 | Loss: 0.00005582
Iteration 210/1000 | Loss: 0.00005518
Iteration 211/1000 | Loss: 0.00058740
Iteration 212/1000 | Loss: 0.00013779
Iteration 213/1000 | Loss: 0.00036074
Iteration 214/1000 | Loss: 0.00014518
Iteration 215/1000 | Loss: 0.00032692
Iteration 216/1000 | Loss: 0.00005840
Iteration 217/1000 | Loss: 0.00005751
Iteration 218/1000 | Loss: 0.00005587
Iteration 219/1000 | Loss: 0.00005464
Iteration 220/1000 | Loss: 0.00005347
Iteration 221/1000 | Loss: 0.00005264
Iteration 222/1000 | Loss: 0.00005225
Iteration 223/1000 | Loss: 0.00005201
Iteration 224/1000 | Loss: 0.00005187
Iteration 225/1000 | Loss: 0.00005170
Iteration 226/1000 | Loss: 0.00005168
Iteration 227/1000 | Loss: 0.00005167
Iteration 228/1000 | Loss: 0.00005165
Iteration 229/1000 | Loss: 0.00005164
Iteration 230/1000 | Loss: 0.00005148
Iteration 231/1000 | Loss: 0.00005147
Iteration 232/1000 | Loss: 0.00005133
Iteration 233/1000 | Loss: 0.00005133
Iteration 234/1000 | Loss: 0.00005131
Iteration 235/1000 | Loss: 0.00005130
Iteration 236/1000 | Loss: 0.00005129
Iteration 237/1000 | Loss: 0.00005127
Iteration 238/1000 | Loss: 0.00005127
Iteration 239/1000 | Loss: 0.00005127
Iteration 240/1000 | Loss: 0.00005126
Iteration 241/1000 | Loss: 0.00005115
Iteration 242/1000 | Loss: 0.00005113
Iteration 243/1000 | Loss: 0.00005112
Iteration 244/1000 | Loss: 0.00005112
Iteration 245/1000 | Loss: 0.00005112
Iteration 246/1000 | Loss: 0.00005112
Iteration 247/1000 | Loss: 0.00005111
Iteration 248/1000 | Loss: 0.00005111
Iteration 249/1000 | Loss: 0.00005111
Iteration 250/1000 | Loss: 0.00005110
Iteration 251/1000 | Loss: 0.00005110
Iteration 252/1000 | Loss: 0.00005107
Iteration 253/1000 | Loss: 0.00005106
Iteration 254/1000 | Loss: 0.00005105
Iteration 255/1000 | Loss: 0.00005105
Iteration 256/1000 | Loss: 0.00005104
Iteration 257/1000 | Loss: 0.00005104
Iteration 258/1000 | Loss: 0.00005103
Iteration 259/1000 | Loss: 0.00005103
Iteration 260/1000 | Loss: 0.00005100
Iteration 261/1000 | Loss: 0.00005100
Iteration 262/1000 | Loss: 0.00005099
Iteration 263/1000 | Loss: 0.00005099
Iteration 264/1000 | Loss: 0.00005099
Iteration 265/1000 | Loss: 0.00005099
Iteration 266/1000 | Loss: 0.00005098
Iteration 267/1000 | Loss: 0.00005098
Iteration 268/1000 | Loss: 0.00005098
Iteration 269/1000 | Loss: 0.00005098
Iteration 270/1000 | Loss: 0.00005097
Iteration 271/1000 | Loss: 0.00005096
Iteration 272/1000 | Loss: 0.00005096
Iteration 273/1000 | Loss: 0.00005096
Iteration 274/1000 | Loss: 0.00005095
Iteration 275/1000 | Loss: 0.00005095
Iteration 276/1000 | Loss: 0.00005095
Iteration 277/1000 | Loss: 0.00005095
Iteration 278/1000 | Loss: 0.00005095
Iteration 279/1000 | Loss: 0.00005094
Iteration 280/1000 | Loss: 0.00005094
Iteration 281/1000 | Loss: 0.00005094
Iteration 282/1000 | Loss: 0.00005093
Iteration 283/1000 | Loss: 0.00005093
Iteration 284/1000 | Loss: 0.00005093
Iteration 285/1000 | Loss: 0.00005093
Iteration 286/1000 | Loss: 0.00005092
Iteration 287/1000 | Loss: 0.00005092
Iteration 288/1000 | Loss: 0.00005092
Iteration 289/1000 | Loss: 0.00005092
Iteration 290/1000 | Loss: 0.00005092
Iteration 291/1000 | Loss: 0.00005092
Iteration 292/1000 | Loss: 0.00005091
Iteration 293/1000 | Loss: 0.00005091
Iteration 294/1000 | Loss: 0.00005091
Iteration 295/1000 | Loss: 0.00005091
Iteration 296/1000 | Loss: 0.00005091
Iteration 297/1000 | Loss: 0.00005091
Iteration 298/1000 | Loss: 0.00005091
Iteration 299/1000 | Loss: 0.00005091
Iteration 300/1000 | Loss: 0.00005091
Iteration 301/1000 | Loss: 0.00005091
Iteration 302/1000 | Loss: 0.00005091
Iteration 303/1000 | Loss: 0.00005091
Iteration 304/1000 | Loss: 0.00005091
Iteration 305/1000 | Loss: 0.00005091
Iteration 306/1000 | Loss: 0.00005091
Iteration 307/1000 | Loss: 0.00005091
Iteration 308/1000 | Loss: 0.00005090
Iteration 309/1000 | Loss: 0.00005090
Iteration 310/1000 | Loss: 0.00005090
Iteration 311/1000 | Loss: 0.00005090
Iteration 312/1000 | Loss: 0.00005090
Iteration 313/1000 | Loss: 0.00005090
Iteration 314/1000 | Loss: 0.00005090
Iteration 315/1000 | Loss: 0.00005090
Iteration 316/1000 | Loss: 0.00005090
Iteration 317/1000 | Loss: 0.00005090
Iteration 318/1000 | Loss: 0.00005090
Iteration 319/1000 | Loss: 0.00005090
Iteration 320/1000 | Loss: 0.00005090
Iteration 321/1000 | Loss: 0.00005090
Iteration 322/1000 | Loss: 0.00005090
Iteration 323/1000 | Loss: 0.00005090
Iteration 324/1000 | Loss: 0.00005089
Iteration 325/1000 | Loss: 0.00005089
Iteration 326/1000 | Loss: 0.00005089
Iteration 327/1000 | Loss: 0.00005089
Iteration 328/1000 | Loss: 0.00005089
Iteration 329/1000 | Loss: 0.00005089
Iteration 330/1000 | Loss: 0.00005089
Iteration 331/1000 | Loss: 0.00005089
Iteration 332/1000 | Loss: 0.00005089
Iteration 333/1000 | Loss: 0.00005089
Iteration 334/1000 | Loss: 0.00005089
Iteration 335/1000 | Loss: 0.00005089
Iteration 336/1000 | Loss: 0.00005089
Iteration 337/1000 | Loss: 0.00005089
Iteration 338/1000 | Loss: 0.00005089
Iteration 339/1000 | Loss: 0.00005089
Iteration 340/1000 | Loss: 0.00005089
Iteration 341/1000 | Loss: 0.00005089
Iteration 342/1000 | Loss: 0.00005089
Iteration 343/1000 | Loss: 0.00005089
Iteration 344/1000 | Loss: 0.00005089
Iteration 345/1000 | Loss: 0.00005089
Iteration 346/1000 | Loss: 0.00005088
Iteration 347/1000 | Loss: 0.00005088
Iteration 348/1000 | Loss: 0.00005088
Iteration 349/1000 | Loss: 0.00005088
Iteration 350/1000 | Loss: 0.00005088
Iteration 351/1000 | Loss: 0.00005088
Iteration 352/1000 | Loss: 0.00005088
Iteration 353/1000 | Loss: 0.00005088
Iteration 354/1000 | Loss: 0.00005088
Iteration 355/1000 | Loss: 0.00005088
Iteration 356/1000 | Loss: 0.00005088
Iteration 357/1000 | Loss: 0.00005088
Iteration 358/1000 | Loss: 0.00005087
Iteration 359/1000 | Loss: 0.00005087
Iteration 360/1000 | Loss: 0.00005087
Iteration 361/1000 | Loss: 0.00005087
Iteration 362/1000 | Loss: 0.00005087
Iteration 363/1000 | Loss: 0.00005087
Iteration 364/1000 | Loss: 0.00005087
Iteration 365/1000 | Loss: 0.00005087
Iteration 366/1000 | Loss: 0.00005087
Iteration 367/1000 | Loss: 0.00005087
Iteration 368/1000 | Loss: 0.00005087
Iteration 369/1000 | Loss: 0.00005087
Iteration 370/1000 | Loss: 0.00005087
Iteration 371/1000 | Loss: 0.00005087
Iteration 372/1000 | Loss: 0.00005087
Iteration 373/1000 | Loss: 0.00005087
Iteration 374/1000 | Loss: 0.00005087
Iteration 375/1000 | Loss: 0.00005087
Iteration 376/1000 | Loss: 0.00005087
Iteration 377/1000 | Loss: 0.00005087
Iteration 378/1000 | Loss: 0.00005087
Iteration 379/1000 | Loss: 0.00005087
Iteration 380/1000 | Loss: 0.00005087
Iteration 381/1000 | Loss: 0.00005087
Iteration 382/1000 | Loss: 0.00005087
Iteration 383/1000 | Loss: 0.00005087
Iteration 384/1000 | Loss: 0.00005087
Iteration 385/1000 | Loss: 0.00005087
Iteration 386/1000 | Loss: 0.00005087
Iteration 387/1000 | Loss: 0.00005087
Iteration 388/1000 | Loss: 0.00005087
Iteration 389/1000 | Loss: 0.00005087
Iteration 390/1000 | Loss: 0.00005087
Iteration 391/1000 | Loss: 0.00005087
Iteration 392/1000 | Loss: 0.00005087
Iteration 393/1000 | Loss: 0.00005087
Iteration 394/1000 | Loss: 0.00005087
Iteration 395/1000 | Loss: 0.00005087
Iteration 396/1000 | Loss: 0.00005087
Iteration 397/1000 | Loss: 0.00005087
Iteration 398/1000 | Loss: 0.00005087
Iteration 399/1000 | Loss: 0.00005087
Iteration 400/1000 | Loss: 0.00005087
Iteration 401/1000 | Loss: 0.00005087
Iteration 402/1000 | Loss: 0.00005087
Iteration 403/1000 | Loss: 0.00005087
Iteration 404/1000 | Loss: 0.00005087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 404. Stopping optimization.
Last 5 losses: [5.0873470172518864e-05, 5.0873470172518864e-05, 5.0873470172518864e-05, 5.0873470172518864e-05, 5.0873470172518864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.0873470172518864e-05

Optimization complete. Final v2v error: 4.557566165924072 mm

Highest mean error: 13.37720775604248 mm for frame 18

Lowest mean error: 3.3750386238098145 mm for frame 159

Saving results

Total time: 414.2125606536865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014264
Iteration 2/25 | Loss: 0.00163566
Iteration 3/25 | Loss: 0.00104087
Iteration 4/25 | Loss: 0.00098560
Iteration 5/25 | Loss: 0.00097427
Iteration 6/25 | Loss: 0.00097124
Iteration 7/25 | Loss: 0.00097074
Iteration 8/25 | Loss: 0.00097074
Iteration 9/25 | Loss: 0.00097074
Iteration 10/25 | Loss: 0.00097074
Iteration 11/25 | Loss: 0.00097074
Iteration 12/25 | Loss: 0.00097074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009707403951324522, 0.0009707403951324522, 0.0009707403951324522, 0.0009707403951324522, 0.0009707403951324522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009707403951324522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88182700
Iteration 2/25 | Loss: 0.00039054
Iteration 3/25 | Loss: 0.00039054
Iteration 4/25 | Loss: 0.00039054
Iteration 5/25 | Loss: 0.00039054
Iteration 6/25 | Loss: 0.00039054
Iteration 7/25 | Loss: 0.00039054
Iteration 8/25 | Loss: 0.00039054
Iteration 9/25 | Loss: 0.00039054
Iteration 10/25 | Loss: 0.00039054
Iteration 11/25 | Loss: 0.00039054
Iteration 12/25 | Loss: 0.00039054
Iteration 13/25 | Loss: 0.00039054
Iteration 14/25 | Loss: 0.00039054
Iteration 15/25 | Loss: 0.00039054
Iteration 16/25 | Loss: 0.00039054
Iteration 17/25 | Loss: 0.00039054
Iteration 18/25 | Loss: 0.00039054
Iteration 19/25 | Loss: 0.00039054
Iteration 20/25 | Loss: 0.00039054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000390536617487669, 0.000390536617487669, 0.000390536617487669, 0.000390536617487669, 0.000390536617487669]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000390536617487669

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039054
Iteration 2/1000 | Loss: 0.00006794
Iteration 3/1000 | Loss: 0.00004373
Iteration 4/1000 | Loss: 0.00004015
Iteration 5/1000 | Loss: 0.00003859
Iteration 6/1000 | Loss: 0.00003759
Iteration 7/1000 | Loss: 0.00003691
Iteration 8/1000 | Loss: 0.00003629
Iteration 9/1000 | Loss: 0.00003595
Iteration 10/1000 | Loss: 0.00003562
Iteration 11/1000 | Loss: 0.00003537
Iteration 12/1000 | Loss: 0.00003512
Iteration 13/1000 | Loss: 0.00003488
Iteration 14/1000 | Loss: 0.00003471
Iteration 15/1000 | Loss: 0.00003467
Iteration 16/1000 | Loss: 0.00003451
Iteration 17/1000 | Loss: 0.00003436
Iteration 18/1000 | Loss: 0.00003425
Iteration 19/1000 | Loss: 0.00003424
Iteration 20/1000 | Loss: 0.00003421
Iteration 21/1000 | Loss: 0.00003420
Iteration 22/1000 | Loss: 0.00003419
Iteration 23/1000 | Loss: 0.00003419
Iteration 24/1000 | Loss: 0.00003418
Iteration 25/1000 | Loss: 0.00003415
Iteration 26/1000 | Loss: 0.00003415
Iteration 27/1000 | Loss: 0.00003415
Iteration 28/1000 | Loss: 0.00003414
Iteration 29/1000 | Loss: 0.00003413
Iteration 30/1000 | Loss: 0.00003411
Iteration 31/1000 | Loss: 0.00003411
Iteration 32/1000 | Loss: 0.00003411
Iteration 33/1000 | Loss: 0.00003411
Iteration 34/1000 | Loss: 0.00003411
Iteration 35/1000 | Loss: 0.00003411
Iteration 36/1000 | Loss: 0.00003411
Iteration 37/1000 | Loss: 0.00003411
Iteration 38/1000 | Loss: 0.00003411
Iteration 39/1000 | Loss: 0.00003410
Iteration 40/1000 | Loss: 0.00003410
Iteration 41/1000 | Loss: 0.00003410
Iteration 42/1000 | Loss: 0.00003410
Iteration 43/1000 | Loss: 0.00003410
Iteration 44/1000 | Loss: 0.00003410
Iteration 45/1000 | Loss: 0.00003410
Iteration 46/1000 | Loss: 0.00003409
Iteration 47/1000 | Loss: 0.00003409
Iteration 48/1000 | Loss: 0.00003408
Iteration 49/1000 | Loss: 0.00003407
Iteration 50/1000 | Loss: 0.00003407
Iteration 51/1000 | Loss: 0.00003406
Iteration 52/1000 | Loss: 0.00003406
Iteration 53/1000 | Loss: 0.00003404
Iteration 54/1000 | Loss: 0.00003404
Iteration 55/1000 | Loss: 0.00003404
Iteration 56/1000 | Loss: 0.00003404
Iteration 57/1000 | Loss: 0.00003404
Iteration 58/1000 | Loss: 0.00003403
Iteration 59/1000 | Loss: 0.00003400
Iteration 60/1000 | Loss: 0.00003400
Iteration 61/1000 | Loss: 0.00003400
Iteration 62/1000 | Loss: 0.00003400
Iteration 63/1000 | Loss: 0.00003399
Iteration 64/1000 | Loss: 0.00003399
Iteration 65/1000 | Loss: 0.00003399
Iteration 66/1000 | Loss: 0.00003399
Iteration 67/1000 | Loss: 0.00003399
Iteration 68/1000 | Loss: 0.00003398
Iteration 69/1000 | Loss: 0.00003398
Iteration 70/1000 | Loss: 0.00003397
Iteration 71/1000 | Loss: 0.00003397
Iteration 72/1000 | Loss: 0.00003397
Iteration 73/1000 | Loss: 0.00003396
Iteration 74/1000 | Loss: 0.00003396
Iteration 75/1000 | Loss: 0.00003395
Iteration 76/1000 | Loss: 0.00003395
Iteration 77/1000 | Loss: 0.00003394
Iteration 78/1000 | Loss: 0.00003394
Iteration 79/1000 | Loss: 0.00003394
Iteration 80/1000 | Loss: 0.00003394
Iteration 81/1000 | Loss: 0.00003393
Iteration 82/1000 | Loss: 0.00003393
Iteration 83/1000 | Loss: 0.00003392
Iteration 84/1000 | Loss: 0.00003392
Iteration 85/1000 | Loss: 0.00003392
Iteration 86/1000 | Loss: 0.00003391
Iteration 87/1000 | Loss: 0.00003391
Iteration 88/1000 | Loss: 0.00003391
Iteration 89/1000 | Loss: 0.00003391
Iteration 90/1000 | Loss: 0.00003391
Iteration 91/1000 | Loss: 0.00003391
Iteration 92/1000 | Loss: 0.00003391
Iteration 93/1000 | Loss: 0.00003391
Iteration 94/1000 | Loss: 0.00003391
Iteration 95/1000 | Loss: 0.00003390
Iteration 96/1000 | Loss: 0.00003390
Iteration 97/1000 | Loss: 0.00003390
Iteration 98/1000 | Loss: 0.00003389
Iteration 99/1000 | Loss: 0.00003388
Iteration 100/1000 | Loss: 0.00003388
Iteration 101/1000 | Loss: 0.00003388
Iteration 102/1000 | Loss: 0.00003388
Iteration 103/1000 | Loss: 0.00003388
Iteration 104/1000 | Loss: 0.00003387
Iteration 105/1000 | Loss: 0.00003387
Iteration 106/1000 | Loss: 0.00003387
Iteration 107/1000 | Loss: 0.00003387
Iteration 108/1000 | Loss: 0.00003387
Iteration 109/1000 | Loss: 0.00003387
Iteration 110/1000 | Loss: 0.00003387
Iteration 111/1000 | Loss: 0.00003387
Iteration 112/1000 | Loss: 0.00003387
Iteration 113/1000 | Loss: 0.00003387
Iteration 114/1000 | Loss: 0.00003386
Iteration 115/1000 | Loss: 0.00003386
Iteration 116/1000 | Loss: 0.00003386
Iteration 117/1000 | Loss: 0.00003385
Iteration 118/1000 | Loss: 0.00003385
Iteration 119/1000 | Loss: 0.00003385
Iteration 120/1000 | Loss: 0.00003385
Iteration 121/1000 | Loss: 0.00003385
Iteration 122/1000 | Loss: 0.00003385
Iteration 123/1000 | Loss: 0.00003384
Iteration 124/1000 | Loss: 0.00003384
Iteration 125/1000 | Loss: 0.00003384
Iteration 126/1000 | Loss: 0.00003384
Iteration 127/1000 | Loss: 0.00003384
Iteration 128/1000 | Loss: 0.00003383
Iteration 129/1000 | Loss: 0.00003383
Iteration 130/1000 | Loss: 0.00003383
Iteration 131/1000 | Loss: 0.00003383
Iteration 132/1000 | Loss: 0.00003383
Iteration 133/1000 | Loss: 0.00003383
Iteration 134/1000 | Loss: 0.00003383
Iteration 135/1000 | Loss: 0.00003382
Iteration 136/1000 | Loss: 0.00003382
Iteration 137/1000 | Loss: 0.00003382
Iteration 138/1000 | Loss: 0.00003382
Iteration 139/1000 | Loss: 0.00003382
Iteration 140/1000 | Loss: 0.00003381
Iteration 141/1000 | Loss: 0.00003381
Iteration 142/1000 | Loss: 0.00003381
Iteration 143/1000 | Loss: 0.00003381
Iteration 144/1000 | Loss: 0.00003381
Iteration 145/1000 | Loss: 0.00003381
Iteration 146/1000 | Loss: 0.00003381
Iteration 147/1000 | Loss: 0.00003381
Iteration 148/1000 | Loss: 0.00003381
Iteration 149/1000 | Loss: 0.00003381
Iteration 150/1000 | Loss: 0.00003381
Iteration 151/1000 | Loss: 0.00003381
Iteration 152/1000 | Loss: 0.00003380
Iteration 153/1000 | Loss: 0.00003380
Iteration 154/1000 | Loss: 0.00003380
Iteration 155/1000 | Loss: 0.00003380
Iteration 156/1000 | Loss: 0.00003380
Iteration 157/1000 | Loss: 0.00003380
Iteration 158/1000 | Loss: 0.00003380
Iteration 159/1000 | Loss: 0.00003380
Iteration 160/1000 | Loss: 0.00003379
Iteration 161/1000 | Loss: 0.00003379
Iteration 162/1000 | Loss: 0.00003379
Iteration 163/1000 | Loss: 0.00003379
Iteration 164/1000 | Loss: 0.00003379
Iteration 165/1000 | Loss: 0.00003379
Iteration 166/1000 | Loss: 0.00003379
Iteration 167/1000 | Loss: 0.00003379
Iteration 168/1000 | Loss: 0.00003379
Iteration 169/1000 | Loss: 0.00003379
Iteration 170/1000 | Loss: 0.00003379
Iteration 171/1000 | Loss: 0.00003378
Iteration 172/1000 | Loss: 0.00003378
Iteration 173/1000 | Loss: 0.00003378
Iteration 174/1000 | Loss: 0.00003378
Iteration 175/1000 | Loss: 0.00003378
Iteration 176/1000 | Loss: 0.00003378
Iteration 177/1000 | Loss: 0.00003378
Iteration 178/1000 | Loss: 0.00003378
Iteration 179/1000 | Loss: 0.00003378
Iteration 180/1000 | Loss: 0.00003378
Iteration 181/1000 | Loss: 0.00003377
Iteration 182/1000 | Loss: 0.00003377
Iteration 183/1000 | Loss: 0.00003377
Iteration 184/1000 | Loss: 0.00003377
Iteration 185/1000 | Loss: 0.00003377
Iteration 186/1000 | Loss: 0.00003377
Iteration 187/1000 | Loss: 0.00003377
Iteration 188/1000 | Loss: 0.00003377
Iteration 189/1000 | Loss: 0.00003377
Iteration 190/1000 | Loss: 0.00003377
Iteration 191/1000 | Loss: 0.00003377
Iteration 192/1000 | Loss: 0.00003377
Iteration 193/1000 | Loss: 0.00003377
Iteration 194/1000 | Loss: 0.00003377
Iteration 195/1000 | Loss: 0.00003377
Iteration 196/1000 | Loss: 0.00003376
Iteration 197/1000 | Loss: 0.00003376
Iteration 198/1000 | Loss: 0.00003376
Iteration 199/1000 | Loss: 0.00003376
Iteration 200/1000 | Loss: 0.00003376
Iteration 201/1000 | Loss: 0.00003376
Iteration 202/1000 | Loss: 0.00003376
Iteration 203/1000 | Loss: 0.00003376
Iteration 204/1000 | Loss: 0.00003376
Iteration 205/1000 | Loss: 0.00003376
Iteration 206/1000 | Loss: 0.00003375
Iteration 207/1000 | Loss: 0.00003375
Iteration 208/1000 | Loss: 0.00003375
Iteration 209/1000 | Loss: 0.00003375
Iteration 210/1000 | Loss: 0.00003375
Iteration 211/1000 | Loss: 0.00003375
Iteration 212/1000 | Loss: 0.00003375
Iteration 213/1000 | Loss: 0.00003375
Iteration 214/1000 | Loss: 0.00003375
Iteration 215/1000 | Loss: 0.00003375
Iteration 216/1000 | Loss: 0.00003375
Iteration 217/1000 | Loss: 0.00003375
Iteration 218/1000 | Loss: 0.00003375
Iteration 219/1000 | Loss: 0.00003375
Iteration 220/1000 | Loss: 0.00003375
Iteration 221/1000 | Loss: 0.00003375
Iteration 222/1000 | Loss: 0.00003374
Iteration 223/1000 | Loss: 0.00003374
Iteration 224/1000 | Loss: 0.00003374
Iteration 225/1000 | Loss: 0.00003374
Iteration 226/1000 | Loss: 0.00003374
Iteration 227/1000 | Loss: 0.00003374
Iteration 228/1000 | Loss: 0.00003374
Iteration 229/1000 | Loss: 0.00003374
Iteration 230/1000 | Loss: 0.00003374
Iteration 231/1000 | Loss: 0.00003374
Iteration 232/1000 | Loss: 0.00003374
Iteration 233/1000 | Loss: 0.00003374
Iteration 234/1000 | Loss: 0.00003374
Iteration 235/1000 | Loss: 0.00003374
Iteration 236/1000 | Loss: 0.00003374
Iteration 237/1000 | Loss: 0.00003374
Iteration 238/1000 | Loss: 0.00003374
Iteration 239/1000 | Loss: 0.00003374
Iteration 240/1000 | Loss: 0.00003374
Iteration 241/1000 | Loss: 0.00003374
Iteration 242/1000 | Loss: 0.00003374
Iteration 243/1000 | Loss: 0.00003374
Iteration 244/1000 | Loss: 0.00003373
Iteration 245/1000 | Loss: 0.00003373
Iteration 246/1000 | Loss: 0.00003373
Iteration 247/1000 | Loss: 0.00003373
Iteration 248/1000 | Loss: 0.00003373
Iteration 249/1000 | Loss: 0.00003373
Iteration 250/1000 | Loss: 0.00003373
Iteration 251/1000 | Loss: 0.00003373
Iteration 252/1000 | Loss: 0.00003373
Iteration 253/1000 | Loss: 0.00003373
Iteration 254/1000 | Loss: 0.00003373
Iteration 255/1000 | Loss: 0.00003373
Iteration 256/1000 | Loss: 0.00003373
Iteration 257/1000 | Loss: 0.00003373
Iteration 258/1000 | Loss: 0.00003373
Iteration 259/1000 | Loss: 0.00003372
Iteration 260/1000 | Loss: 0.00003372
Iteration 261/1000 | Loss: 0.00003372
Iteration 262/1000 | Loss: 0.00003372
Iteration 263/1000 | Loss: 0.00003372
Iteration 264/1000 | Loss: 0.00003372
Iteration 265/1000 | Loss: 0.00003372
Iteration 266/1000 | Loss: 0.00003372
Iteration 267/1000 | Loss: 0.00003372
Iteration 268/1000 | Loss: 0.00003372
Iteration 269/1000 | Loss: 0.00003372
Iteration 270/1000 | Loss: 0.00003372
Iteration 271/1000 | Loss: 0.00003372
Iteration 272/1000 | Loss: 0.00003372
Iteration 273/1000 | Loss: 0.00003372
Iteration 274/1000 | Loss: 0.00003372
Iteration 275/1000 | Loss: 0.00003371
Iteration 276/1000 | Loss: 0.00003371
Iteration 277/1000 | Loss: 0.00003371
Iteration 278/1000 | Loss: 0.00003371
Iteration 279/1000 | Loss: 0.00003371
Iteration 280/1000 | Loss: 0.00003371
Iteration 281/1000 | Loss: 0.00003371
Iteration 282/1000 | Loss: 0.00003371
Iteration 283/1000 | Loss: 0.00003371
Iteration 284/1000 | Loss: 0.00003371
Iteration 285/1000 | Loss: 0.00003371
Iteration 286/1000 | Loss: 0.00003371
Iteration 287/1000 | Loss: 0.00003371
Iteration 288/1000 | Loss: 0.00003371
Iteration 289/1000 | Loss: 0.00003371
Iteration 290/1000 | Loss: 0.00003371
Iteration 291/1000 | Loss: 0.00003371
Iteration 292/1000 | Loss: 0.00003371
Iteration 293/1000 | Loss: 0.00003371
Iteration 294/1000 | Loss: 0.00003371
Iteration 295/1000 | Loss: 0.00003371
Iteration 296/1000 | Loss: 0.00003371
Iteration 297/1000 | Loss: 0.00003371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [3.371308048372157e-05, 3.371308048372157e-05, 3.371308048372157e-05, 3.371308048372157e-05, 3.371308048372157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.371308048372157e-05

Optimization complete. Final v2v error: 4.701933860778809 mm

Highest mean error: 5.551812171936035 mm for frame 45

Lowest mean error: 3.760707378387451 mm for frame 29

Saving results

Total time: 54.364073753356934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104512
Iteration 2/25 | Loss: 0.00149542
Iteration 3/25 | Loss: 0.00097793
Iteration 4/25 | Loss: 0.00093391
Iteration 5/25 | Loss: 0.00092173
Iteration 6/25 | Loss: 0.00091840
Iteration 7/25 | Loss: 0.00091796
Iteration 8/25 | Loss: 0.00091796
Iteration 9/25 | Loss: 0.00091796
Iteration 10/25 | Loss: 0.00091796
Iteration 11/25 | Loss: 0.00091796
Iteration 12/25 | Loss: 0.00091796
Iteration 13/25 | Loss: 0.00091796
Iteration 14/25 | Loss: 0.00091796
Iteration 15/25 | Loss: 0.00091796
Iteration 16/25 | Loss: 0.00091796
Iteration 17/25 | Loss: 0.00091796
Iteration 18/25 | Loss: 0.00091796
Iteration 19/25 | Loss: 0.00091796
Iteration 20/25 | Loss: 0.00091796
Iteration 21/25 | Loss: 0.00091796
Iteration 22/25 | Loss: 0.00091796
Iteration 23/25 | Loss: 0.00091796
Iteration 24/25 | Loss: 0.00091796
Iteration 25/25 | Loss: 0.00091796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009179596672765911, 0.0009179596672765911, 0.0009179596672765911, 0.0009179596672765911, 0.0009179596672765911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009179596672765911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97202557
Iteration 2/25 | Loss: 0.00032990
Iteration 3/25 | Loss: 0.00032990
Iteration 4/25 | Loss: 0.00032990
Iteration 5/25 | Loss: 0.00032990
Iteration 6/25 | Loss: 0.00032990
Iteration 7/25 | Loss: 0.00032990
Iteration 8/25 | Loss: 0.00032990
Iteration 9/25 | Loss: 0.00032990
Iteration 10/25 | Loss: 0.00032990
Iteration 11/25 | Loss: 0.00032990
Iteration 12/25 | Loss: 0.00032990
Iteration 13/25 | Loss: 0.00032990
Iteration 14/25 | Loss: 0.00032990
Iteration 15/25 | Loss: 0.00032990
Iteration 16/25 | Loss: 0.00032990
Iteration 17/25 | Loss: 0.00032990
Iteration 18/25 | Loss: 0.00032990
Iteration 19/25 | Loss: 0.00032990
Iteration 20/25 | Loss: 0.00032990
Iteration 21/25 | Loss: 0.00032990
Iteration 22/25 | Loss: 0.00032990
Iteration 23/25 | Loss: 0.00032990
Iteration 24/25 | Loss: 0.00032990
Iteration 25/25 | Loss: 0.00032990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032990
Iteration 2/1000 | Loss: 0.00004288
Iteration 3/1000 | Loss: 0.00003407
Iteration 4/1000 | Loss: 0.00003215
Iteration 5/1000 | Loss: 0.00003082
Iteration 6/1000 | Loss: 0.00003022
Iteration 7/1000 | Loss: 0.00002967
Iteration 8/1000 | Loss: 0.00002935
Iteration 9/1000 | Loss: 0.00002908
Iteration 10/1000 | Loss: 0.00002882
Iteration 11/1000 | Loss: 0.00002857
Iteration 12/1000 | Loss: 0.00002839
Iteration 13/1000 | Loss: 0.00002827
Iteration 14/1000 | Loss: 0.00002826
Iteration 15/1000 | Loss: 0.00002823
Iteration 16/1000 | Loss: 0.00002823
Iteration 17/1000 | Loss: 0.00002819
Iteration 18/1000 | Loss: 0.00002819
Iteration 19/1000 | Loss: 0.00002815
Iteration 20/1000 | Loss: 0.00002811
Iteration 21/1000 | Loss: 0.00002811
Iteration 22/1000 | Loss: 0.00002810
Iteration 23/1000 | Loss: 0.00002809
Iteration 24/1000 | Loss: 0.00002805
Iteration 25/1000 | Loss: 0.00002804
Iteration 26/1000 | Loss: 0.00002804
Iteration 27/1000 | Loss: 0.00002803
Iteration 28/1000 | Loss: 0.00002802
Iteration 29/1000 | Loss: 0.00002802
Iteration 30/1000 | Loss: 0.00002801
Iteration 31/1000 | Loss: 0.00002801
Iteration 32/1000 | Loss: 0.00002800
Iteration 33/1000 | Loss: 0.00002799
Iteration 34/1000 | Loss: 0.00002799
Iteration 35/1000 | Loss: 0.00002798
Iteration 36/1000 | Loss: 0.00002798
Iteration 37/1000 | Loss: 0.00002798
Iteration 38/1000 | Loss: 0.00002798
Iteration 39/1000 | Loss: 0.00002798
Iteration 40/1000 | Loss: 0.00002797
Iteration 41/1000 | Loss: 0.00002797
Iteration 42/1000 | Loss: 0.00002797
Iteration 43/1000 | Loss: 0.00002796
Iteration 44/1000 | Loss: 0.00002796
Iteration 45/1000 | Loss: 0.00002796
Iteration 46/1000 | Loss: 0.00002796
Iteration 47/1000 | Loss: 0.00002796
Iteration 48/1000 | Loss: 0.00002796
Iteration 49/1000 | Loss: 0.00002796
Iteration 50/1000 | Loss: 0.00002796
Iteration 51/1000 | Loss: 0.00002796
Iteration 52/1000 | Loss: 0.00002796
Iteration 53/1000 | Loss: 0.00002795
Iteration 54/1000 | Loss: 0.00002795
Iteration 55/1000 | Loss: 0.00002795
Iteration 56/1000 | Loss: 0.00002795
Iteration 57/1000 | Loss: 0.00002795
Iteration 58/1000 | Loss: 0.00002795
Iteration 59/1000 | Loss: 0.00002795
Iteration 60/1000 | Loss: 0.00002794
Iteration 61/1000 | Loss: 0.00002794
Iteration 62/1000 | Loss: 0.00002793
Iteration 63/1000 | Loss: 0.00002793
Iteration 64/1000 | Loss: 0.00002792
Iteration 65/1000 | Loss: 0.00002792
Iteration 66/1000 | Loss: 0.00002792
Iteration 67/1000 | Loss: 0.00002792
Iteration 68/1000 | Loss: 0.00002792
Iteration 69/1000 | Loss: 0.00002792
Iteration 70/1000 | Loss: 0.00002792
Iteration 71/1000 | Loss: 0.00002792
Iteration 72/1000 | Loss: 0.00002791
Iteration 73/1000 | Loss: 0.00002791
Iteration 74/1000 | Loss: 0.00002791
Iteration 75/1000 | Loss: 0.00002791
Iteration 76/1000 | Loss: 0.00002791
Iteration 77/1000 | Loss: 0.00002791
Iteration 78/1000 | Loss: 0.00002791
Iteration 79/1000 | Loss: 0.00002791
Iteration 80/1000 | Loss: 0.00002790
Iteration 81/1000 | Loss: 0.00002790
Iteration 82/1000 | Loss: 0.00002789
Iteration 83/1000 | Loss: 0.00002789
Iteration 84/1000 | Loss: 0.00002789
Iteration 85/1000 | Loss: 0.00002789
Iteration 86/1000 | Loss: 0.00002789
Iteration 87/1000 | Loss: 0.00002789
Iteration 88/1000 | Loss: 0.00002789
Iteration 89/1000 | Loss: 0.00002789
Iteration 90/1000 | Loss: 0.00002789
Iteration 91/1000 | Loss: 0.00002788
Iteration 92/1000 | Loss: 0.00002788
Iteration 93/1000 | Loss: 0.00002788
Iteration 94/1000 | Loss: 0.00002788
Iteration 95/1000 | Loss: 0.00002787
Iteration 96/1000 | Loss: 0.00002787
Iteration 97/1000 | Loss: 0.00002787
Iteration 98/1000 | Loss: 0.00002787
Iteration 99/1000 | Loss: 0.00002786
Iteration 100/1000 | Loss: 0.00002786
Iteration 101/1000 | Loss: 0.00002786
Iteration 102/1000 | Loss: 0.00002786
Iteration 103/1000 | Loss: 0.00002786
Iteration 104/1000 | Loss: 0.00002786
Iteration 105/1000 | Loss: 0.00002785
Iteration 106/1000 | Loss: 0.00002785
Iteration 107/1000 | Loss: 0.00002785
Iteration 108/1000 | Loss: 0.00002785
Iteration 109/1000 | Loss: 0.00002785
Iteration 110/1000 | Loss: 0.00002785
Iteration 111/1000 | Loss: 0.00002785
Iteration 112/1000 | Loss: 0.00002785
Iteration 113/1000 | Loss: 0.00002785
Iteration 114/1000 | Loss: 0.00002785
Iteration 115/1000 | Loss: 0.00002784
Iteration 116/1000 | Loss: 0.00002784
Iteration 117/1000 | Loss: 0.00002784
Iteration 118/1000 | Loss: 0.00002784
Iteration 119/1000 | Loss: 0.00002784
Iteration 120/1000 | Loss: 0.00002784
Iteration 121/1000 | Loss: 0.00002783
Iteration 122/1000 | Loss: 0.00002783
Iteration 123/1000 | Loss: 0.00002783
Iteration 124/1000 | Loss: 0.00002783
Iteration 125/1000 | Loss: 0.00002783
Iteration 126/1000 | Loss: 0.00002783
Iteration 127/1000 | Loss: 0.00002783
Iteration 128/1000 | Loss: 0.00002783
Iteration 129/1000 | Loss: 0.00002783
Iteration 130/1000 | Loss: 0.00002782
Iteration 131/1000 | Loss: 0.00002782
Iteration 132/1000 | Loss: 0.00002782
Iteration 133/1000 | Loss: 0.00002782
Iteration 134/1000 | Loss: 0.00002782
Iteration 135/1000 | Loss: 0.00002782
Iteration 136/1000 | Loss: 0.00002782
Iteration 137/1000 | Loss: 0.00002782
Iteration 138/1000 | Loss: 0.00002782
Iteration 139/1000 | Loss: 0.00002782
Iteration 140/1000 | Loss: 0.00002782
Iteration 141/1000 | Loss: 0.00002782
Iteration 142/1000 | Loss: 0.00002782
Iteration 143/1000 | Loss: 0.00002782
Iteration 144/1000 | Loss: 0.00002781
Iteration 145/1000 | Loss: 0.00002781
Iteration 146/1000 | Loss: 0.00002781
Iteration 147/1000 | Loss: 0.00002781
Iteration 148/1000 | Loss: 0.00002781
Iteration 149/1000 | Loss: 0.00002781
Iteration 150/1000 | Loss: 0.00002780
Iteration 151/1000 | Loss: 0.00002780
Iteration 152/1000 | Loss: 0.00002780
Iteration 153/1000 | Loss: 0.00002780
Iteration 154/1000 | Loss: 0.00002779
Iteration 155/1000 | Loss: 0.00002779
Iteration 156/1000 | Loss: 0.00002779
Iteration 157/1000 | Loss: 0.00002779
Iteration 158/1000 | Loss: 0.00002779
Iteration 159/1000 | Loss: 0.00002778
Iteration 160/1000 | Loss: 0.00002778
Iteration 161/1000 | Loss: 0.00002778
Iteration 162/1000 | Loss: 0.00002778
Iteration 163/1000 | Loss: 0.00002777
Iteration 164/1000 | Loss: 0.00002777
Iteration 165/1000 | Loss: 0.00002777
Iteration 166/1000 | Loss: 0.00002777
Iteration 167/1000 | Loss: 0.00002777
Iteration 168/1000 | Loss: 0.00002777
Iteration 169/1000 | Loss: 0.00002776
Iteration 170/1000 | Loss: 0.00002776
Iteration 171/1000 | Loss: 0.00002776
Iteration 172/1000 | Loss: 0.00002776
Iteration 173/1000 | Loss: 0.00002776
Iteration 174/1000 | Loss: 0.00002776
Iteration 175/1000 | Loss: 0.00002776
Iteration 176/1000 | Loss: 0.00002776
Iteration 177/1000 | Loss: 0.00002775
Iteration 178/1000 | Loss: 0.00002775
Iteration 179/1000 | Loss: 0.00002775
Iteration 180/1000 | Loss: 0.00002775
Iteration 181/1000 | Loss: 0.00002775
Iteration 182/1000 | Loss: 0.00002775
Iteration 183/1000 | Loss: 0.00002775
Iteration 184/1000 | Loss: 0.00002775
Iteration 185/1000 | Loss: 0.00002775
Iteration 186/1000 | Loss: 0.00002775
Iteration 187/1000 | Loss: 0.00002775
Iteration 188/1000 | Loss: 0.00002775
Iteration 189/1000 | Loss: 0.00002775
Iteration 190/1000 | Loss: 0.00002775
Iteration 191/1000 | Loss: 0.00002775
Iteration 192/1000 | Loss: 0.00002775
Iteration 193/1000 | Loss: 0.00002775
Iteration 194/1000 | Loss: 0.00002775
Iteration 195/1000 | Loss: 0.00002775
Iteration 196/1000 | Loss: 0.00002775
Iteration 197/1000 | Loss: 0.00002775
Iteration 198/1000 | Loss: 0.00002774
Iteration 199/1000 | Loss: 0.00002774
Iteration 200/1000 | Loss: 0.00002774
Iteration 201/1000 | Loss: 0.00002774
Iteration 202/1000 | Loss: 0.00002774
Iteration 203/1000 | Loss: 0.00002774
Iteration 204/1000 | Loss: 0.00002774
Iteration 205/1000 | Loss: 0.00002774
Iteration 206/1000 | Loss: 0.00002774
Iteration 207/1000 | Loss: 0.00002774
Iteration 208/1000 | Loss: 0.00002774
Iteration 209/1000 | Loss: 0.00002774
Iteration 210/1000 | Loss: 0.00002774
Iteration 211/1000 | Loss: 0.00002774
Iteration 212/1000 | Loss: 0.00002774
Iteration 213/1000 | Loss: 0.00002774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.7740057703340426e-05, 2.7740057703340426e-05, 2.7740057703340426e-05, 2.7740057703340426e-05, 2.7740057703340426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7740057703340426e-05

Optimization complete. Final v2v error: 4.354872226715088 mm

Highest mean error: 5.262828826904297 mm for frame 143

Lowest mean error: 3.5365498065948486 mm for frame 48

Saving results

Total time: 45.113598346710205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021805
Iteration 2/25 | Loss: 0.00352724
Iteration 3/25 | Loss: 0.00215357
Iteration 4/25 | Loss: 0.00183198
Iteration 5/25 | Loss: 0.00164896
Iteration 6/25 | Loss: 0.00146869
Iteration 7/25 | Loss: 0.00143105
Iteration 8/25 | Loss: 0.00135037
Iteration 9/25 | Loss: 0.00131551
Iteration 10/25 | Loss: 0.00127455
Iteration 11/25 | Loss: 0.00128469
Iteration 12/25 | Loss: 0.00123942
Iteration 13/25 | Loss: 0.00124296
Iteration 14/25 | Loss: 0.00125751
Iteration 15/25 | Loss: 0.00124271
Iteration 16/25 | Loss: 0.00124895
Iteration 17/25 | Loss: 0.00123834
Iteration 18/25 | Loss: 0.00123509
Iteration 19/25 | Loss: 0.00121208
Iteration 20/25 | Loss: 0.00119417
Iteration 21/25 | Loss: 0.00118285
Iteration 22/25 | Loss: 0.00118126
Iteration 23/25 | Loss: 0.00117664
Iteration 24/25 | Loss: 0.00115235
Iteration 25/25 | Loss: 0.00113663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61486423
Iteration 2/25 | Loss: 0.01771699
Iteration 3/25 | Loss: 0.05710386
Iteration 4/25 | Loss: 0.07979456
Iteration 5/25 | Loss: 0.05228570
Iteration 6/25 | Loss: 0.03955236
Iteration 7/25 | Loss: 0.01361738
Iteration 8/25 | Loss: 0.00742804
Iteration 9/25 | Loss: 0.00680874
Iteration 10/25 | Loss: 0.00397139
Iteration 11/25 | Loss: 0.00317509
Iteration 12/25 | Loss: 0.00302425
Iteration 13/25 | Loss: 0.00302425
Iteration 14/25 | Loss: 0.00302425
Iteration 15/25 | Loss: 0.00302425
Iteration 16/25 | Loss: 0.00302425
Iteration 17/25 | Loss: 0.00302425
Iteration 18/25 | Loss: 0.00302425
Iteration 19/25 | Loss: 0.00302425
Iteration 20/25 | Loss: 0.00302425
Iteration 21/25 | Loss: 0.00302425
Iteration 22/25 | Loss: 0.00302425
Iteration 23/25 | Loss: 0.00302425
Iteration 24/25 | Loss: 0.00302425
Iteration 25/25 | Loss: 0.00302425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00302425
Iteration 2/1000 | Loss: 0.00455452
Iteration 3/1000 | Loss: 0.00486857
Iteration 4/1000 | Loss: 0.00105001
Iteration 5/1000 | Loss: 0.00071137
Iteration 6/1000 | Loss: 0.00087068
Iteration 7/1000 | Loss: 0.00183995
Iteration 8/1000 | Loss: 0.00062170
Iteration 9/1000 | Loss: 0.00068169
Iteration 10/1000 | Loss: 0.00063530
Iteration 11/1000 | Loss: 0.00044807
Iteration 12/1000 | Loss: 0.00081428
Iteration 13/1000 | Loss: 0.00082730
Iteration 14/1000 | Loss: 0.00052208
Iteration 15/1000 | Loss: 0.00044112
Iteration 16/1000 | Loss: 0.00084820
Iteration 17/1000 | Loss: 0.00070078
Iteration 18/1000 | Loss: 0.00114534
Iteration 19/1000 | Loss: 0.00066353
Iteration 20/1000 | Loss: 0.00051757
Iteration 21/1000 | Loss: 0.00045738
Iteration 22/1000 | Loss: 0.00051002
Iteration 23/1000 | Loss: 0.00048629
Iteration 24/1000 | Loss: 0.00049259
Iteration 25/1000 | Loss: 0.00073143
Iteration 26/1000 | Loss: 0.00082370
Iteration 27/1000 | Loss: 0.00100173
Iteration 28/1000 | Loss: 0.00065790
Iteration 29/1000 | Loss: 0.00044956
Iteration 30/1000 | Loss: 0.00039770
Iteration 31/1000 | Loss: 0.00065411
Iteration 32/1000 | Loss: 0.00063547
Iteration 33/1000 | Loss: 0.00038144
Iteration 34/1000 | Loss: 0.00037451
Iteration 35/1000 | Loss: 0.00047893
Iteration 36/1000 | Loss: 0.00039796
Iteration 37/1000 | Loss: 0.00086040
Iteration 38/1000 | Loss: 0.00055508
Iteration 39/1000 | Loss: 0.00048779
Iteration 40/1000 | Loss: 0.00046935
Iteration 41/1000 | Loss: 0.00086294
Iteration 42/1000 | Loss: 0.00046721
Iteration 43/1000 | Loss: 0.00019994
Iteration 44/1000 | Loss: 0.00039076
Iteration 45/1000 | Loss: 0.00034704
Iteration 46/1000 | Loss: 0.00035178
Iteration 47/1000 | Loss: 0.00067566
Iteration 48/1000 | Loss: 0.00025169
Iteration 49/1000 | Loss: 0.00031376
Iteration 50/1000 | Loss: 0.00041782
Iteration 51/1000 | Loss: 0.00032877
Iteration 52/1000 | Loss: 0.00052980
Iteration 53/1000 | Loss: 0.00032208
Iteration 54/1000 | Loss: 0.00235000
Iteration 55/1000 | Loss: 0.00308833
Iteration 56/1000 | Loss: 0.00289368
Iteration 57/1000 | Loss: 0.00038416
Iteration 58/1000 | Loss: 0.00139152
Iteration 59/1000 | Loss: 0.00047947
Iteration 60/1000 | Loss: 0.00266014
Iteration 61/1000 | Loss: 0.00096744
Iteration 62/1000 | Loss: 0.00091993
Iteration 63/1000 | Loss: 0.00148308
Iteration 64/1000 | Loss: 0.00048880
Iteration 65/1000 | Loss: 0.00090876
Iteration 66/1000 | Loss: 0.00085726
Iteration 67/1000 | Loss: 0.00021155
Iteration 68/1000 | Loss: 0.00025736
Iteration 69/1000 | Loss: 0.00035225
Iteration 70/1000 | Loss: 0.00125155
Iteration 71/1000 | Loss: 0.00050357
Iteration 72/1000 | Loss: 0.00079326
Iteration 73/1000 | Loss: 0.00061146
Iteration 74/1000 | Loss: 0.00085661
Iteration 75/1000 | Loss: 0.00108280
Iteration 76/1000 | Loss: 0.00187599
Iteration 77/1000 | Loss: 0.00067930
Iteration 78/1000 | Loss: 0.00048650
Iteration 79/1000 | Loss: 0.00058945
Iteration 80/1000 | Loss: 0.00015885
Iteration 81/1000 | Loss: 0.00046708
Iteration 82/1000 | Loss: 0.00046018
Iteration 83/1000 | Loss: 0.00026652
Iteration 84/1000 | Loss: 0.00021485
Iteration 85/1000 | Loss: 0.00025501
Iteration 86/1000 | Loss: 0.00034803
Iteration 87/1000 | Loss: 0.00027216
Iteration 88/1000 | Loss: 0.00028338
Iteration 89/1000 | Loss: 0.00051700
Iteration 90/1000 | Loss: 0.00036991
Iteration 91/1000 | Loss: 0.00026113
Iteration 92/1000 | Loss: 0.00035535
Iteration 93/1000 | Loss: 0.00024692
Iteration 94/1000 | Loss: 0.00141837
Iteration 95/1000 | Loss: 0.00067086
Iteration 96/1000 | Loss: 0.00100603
Iteration 97/1000 | Loss: 0.00024783
Iteration 98/1000 | Loss: 0.00029405
Iteration 99/1000 | Loss: 0.00022635
Iteration 100/1000 | Loss: 0.00042899
Iteration 101/1000 | Loss: 0.00025207
Iteration 102/1000 | Loss: 0.00024080
Iteration 103/1000 | Loss: 0.00055226
Iteration 104/1000 | Loss: 0.00022473
Iteration 105/1000 | Loss: 0.00045000
Iteration 106/1000 | Loss: 0.00035114
Iteration 107/1000 | Loss: 0.00047582
Iteration 108/1000 | Loss: 0.00059492
Iteration 109/1000 | Loss: 0.00034088
Iteration 110/1000 | Loss: 0.00022080
Iteration 111/1000 | Loss: 0.00033047
Iteration 112/1000 | Loss: 0.00025064
Iteration 113/1000 | Loss: 0.00029916
Iteration 114/1000 | Loss: 0.00058562
Iteration 115/1000 | Loss: 0.00035462
Iteration 116/1000 | Loss: 0.00010484
Iteration 117/1000 | Loss: 0.00022238
Iteration 118/1000 | Loss: 0.00070361
Iteration 119/1000 | Loss: 0.00187952
Iteration 120/1000 | Loss: 0.00043154
Iteration 121/1000 | Loss: 0.00011762
Iteration 122/1000 | Loss: 0.00047644
Iteration 123/1000 | Loss: 0.00012719
Iteration 124/1000 | Loss: 0.00021928
Iteration 125/1000 | Loss: 0.00054925
Iteration 126/1000 | Loss: 0.00042439
Iteration 127/1000 | Loss: 0.00010715
Iteration 128/1000 | Loss: 0.00016839
Iteration 129/1000 | Loss: 0.00019142
Iteration 130/1000 | Loss: 0.00029288
Iteration 131/1000 | Loss: 0.00079135
Iteration 132/1000 | Loss: 0.00023244
Iteration 133/1000 | Loss: 0.00027601
Iteration 134/1000 | Loss: 0.00110295
Iteration 135/1000 | Loss: 0.00026407
Iteration 136/1000 | Loss: 0.00018042
Iteration 137/1000 | Loss: 0.00027576
Iteration 138/1000 | Loss: 0.00016485
Iteration 139/1000 | Loss: 0.00016701
Iteration 140/1000 | Loss: 0.00016956
Iteration 141/1000 | Loss: 0.00090954
Iteration 142/1000 | Loss: 0.00018632
Iteration 143/1000 | Loss: 0.00022334
Iteration 144/1000 | Loss: 0.00038224
Iteration 145/1000 | Loss: 0.00047437
Iteration 146/1000 | Loss: 0.00031491
Iteration 147/1000 | Loss: 0.00009045
Iteration 148/1000 | Loss: 0.00026785
Iteration 149/1000 | Loss: 0.00008821
Iteration 150/1000 | Loss: 0.00008116
Iteration 151/1000 | Loss: 0.00012639
Iteration 152/1000 | Loss: 0.00007390
Iteration 153/1000 | Loss: 0.00007718
Iteration 154/1000 | Loss: 0.00027073
Iteration 155/1000 | Loss: 0.00010135
Iteration 156/1000 | Loss: 0.00008997
Iteration 157/1000 | Loss: 0.00008864
Iteration 158/1000 | Loss: 0.00022671
Iteration 159/1000 | Loss: 0.00007629
Iteration 160/1000 | Loss: 0.00028810
Iteration 161/1000 | Loss: 0.00008570
Iteration 162/1000 | Loss: 0.00021795
Iteration 163/1000 | Loss: 0.00010635
Iteration 164/1000 | Loss: 0.00008541
Iteration 165/1000 | Loss: 0.00010275
Iteration 166/1000 | Loss: 0.00009211
Iteration 167/1000 | Loss: 0.00012270
Iteration 168/1000 | Loss: 0.00017830
Iteration 169/1000 | Loss: 0.00017982
Iteration 170/1000 | Loss: 0.00008701
Iteration 171/1000 | Loss: 0.00007224
Iteration 172/1000 | Loss: 0.00007254
Iteration 173/1000 | Loss: 0.00006200
Iteration 174/1000 | Loss: 0.00007227
Iteration 175/1000 | Loss: 0.00013009
Iteration 176/1000 | Loss: 0.00012816
Iteration 177/1000 | Loss: 0.00011888
Iteration 178/1000 | Loss: 0.00011440
Iteration 179/1000 | Loss: 0.00008696
Iteration 180/1000 | Loss: 0.00007405
Iteration 181/1000 | Loss: 0.00007433
Iteration 182/1000 | Loss: 0.00095047
Iteration 183/1000 | Loss: 0.00007250
Iteration 184/1000 | Loss: 0.00007250
Iteration 185/1000 | Loss: 0.00006487
Iteration 186/1000 | Loss: 0.00007579
Iteration 187/1000 | Loss: 0.00012548
Iteration 188/1000 | Loss: 0.00007360
Iteration 189/1000 | Loss: 0.00052374
Iteration 190/1000 | Loss: 0.00037244
Iteration 191/1000 | Loss: 0.00027719
Iteration 192/1000 | Loss: 0.00033682
Iteration 193/1000 | Loss: 0.00007245
Iteration 194/1000 | Loss: 0.00007071
Iteration 195/1000 | Loss: 0.00033338
Iteration 196/1000 | Loss: 0.00041819
Iteration 197/1000 | Loss: 0.00011698
Iteration 198/1000 | Loss: 0.00021126
Iteration 199/1000 | Loss: 0.00007564
Iteration 200/1000 | Loss: 0.00007920
Iteration 201/1000 | Loss: 0.00006094
Iteration 202/1000 | Loss: 0.00005347
Iteration 203/1000 | Loss: 0.00017311
Iteration 204/1000 | Loss: 0.00007343
Iteration 205/1000 | Loss: 0.00010988
Iteration 206/1000 | Loss: 0.00006400
Iteration 207/1000 | Loss: 0.00004971
Iteration 208/1000 | Loss: 0.00007004
Iteration 209/1000 | Loss: 0.00005222
Iteration 210/1000 | Loss: 0.00005876
Iteration 211/1000 | Loss: 0.00007183
Iteration 212/1000 | Loss: 0.00005892
Iteration 213/1000 | Loss: 0.00007785
Iteration 214/1000 | Loss: 0.00007097
Iteration 215/1000 | Loss: 0.00007933
Iteration 216/1000 | Loss: 0.00006621
Iteration 217/1000 | Loss: 0.00007006
Iteration 218/1000 | Loss: 0.00006859
Iteration 219/1000 | Loss: 0.00006772
Iteration 220/1000 | Loss: 0.00006806
Iteration 221/1000 | Loss: 0.00006655
Iteration 222/1000 | Loss: 0.00006794
Iteration 223/1000 | Loss: 0.00011704
Iteration 224/1000 | Loss: 0.00005328
Iteration 225/1000 | Loss: 0.00004487
Iteration 226/1000 | Loss: 0.00004139
Iteration 227/1000 | Loss: 0.00004019
Iteration 228/1000 | Loss: 0.00003926
Iteration 229/1000 | Loss: 0.00003867
Iteration 230/1000 | Loss: 0.00021829
Iteration 231/1000 | Loss: 0.00003877
Iteration 232/1000 | Loss: 0.00003777
Iteration 233/1000 | Loss: 0.00003747
Iteration 234/1000 | Loss: 0.00003716
Iteration 235/1000 | Loss: 0.00003699
Iteration 236/1000 | Loss: 0.00003680
Iteration 237/1000 | Loss: 0.00003668
Iteration 238/1000 | Loss: 0.00003658
Iteration 239/1000 | Loss: 0.00006491
Iteration 240/1000 | Loss: 0.00003714
Iteration 241/1000 | Loss: 0.00003629
Iteration 242/1000 | Loss: 0.00003629
Iteration 243/1000 | Loss: 0.00003627
Iteration 244/1000 | Loss: 0.00003627
Iteration 245/1000 | Loss: 0.00003627
Iteration 246/1000 | Loss: 0.00003626
Iteration 247/1000 | Loss: 0.00003626
Iteration 248/1000 | Loss: 0.00003625
Iteration 249/1000 | Loss: 0.00003624
Iteration 250/1000 | Loss: 0.00003623
Iteration 251/1000 | Loss: 0.00003783
Iteration 252/1000 | Loss: 0.00004948
Iteration 253/1000 | Loss: 0.00003695
Iteration 254/1000 | Loss: 0.00003613
Iteration 255/1000 | Loss: 0.00003613
Iteration 256/1000 | Loss: 0.00003613
Iteration 257/1000 | Loss: 0.00003613
Iteration 258/1000 | Loss: 0.00003612
Iteration 259/1000 | Loss: 0.00003612
Iteration 260/1000 | Loss: 0.00003612
Iteration 261/1000 | Loss: 0.00003612
Iteration 262/1000 | Loss: 0.00003612
Iteration 263/1000 | Loss: 0.00003612
Iteration 264/1000 | Loss: 0.00003612
Iteration 265/1000 | Loss: 0.00003612
Iteration 266/1000 | Loss: 0.00003612
Iteration 267/1000 | Loss: 0.00003611
Iteration 268/1000 | Loss: 0.00003611
Iteration 269/1000 | Loss: 0.00003611
Iteration 270/1000 | Loss: 0.00003611
Iteration 271/1000 | Loss: 0.00003610
Iteration 272/1000 | Loss: 0.00003610
Iteration 273/1000 | Loss: 0.00003609
Iteration 274/1000 | Loss: 0.00003609
Iteration 275/1000 | Loss: 0.00003609
Iteration 276/1000 | Loss: 0.00003608
Iteration 277/1000 | Loss: 0.00003608
Iteration 278/1000 | Loss: 0.00003608
Iteration 279/1000 | Loss: 0.00003608
Iteration 280/1000 | Loss: 0.00003608
Iteration 281/1000 | Loss: 0.00003607
Iteration 282/1000 | Loss: 0.00003607
Iteration 283/1000 | Loss: 0.00003607
Iteration 284/1000 | Loss: 0.00003607
Iteration 285/1000 | Loss: 0.00003606
Iteration 286/1000 | Loss: 0.00003606
Iteration 287/1000 | Loss: 0.00003606
Iteration 288/1000 | Loss: 0.00003606
Iteration 289/1000 | Loss: 0.00003605
Iteration 290/1000 | Loss: 0.00003605
Iteration 291/1000 | Loss: 0.00003605
Iteration 292/1000 | Loss: 0.00003605
Iteration 293/1000 | Loss: 0.00003605
Iteration 294/1000 | Loss: 0.00003605
Iteration 295/1000 | Loss: 0.00003604
Iteration 296/1000 | Loss: 0.00003604
Iteration 297/1000 | Loss: 0.00003604
Iteration 298/1000 | Loss: 0.00003604
Iteration 299/1000 | Loss: 0.00003603
Iteration 300/1000 | Loss: 0.00003603
Iteration 301/1000 | Loss: 0.00003603
Iteration 302/1000 | Loss: 0.00003602
Iteration 303/1000 | Loss: 0.00003602
Iteration 304/1000 | Loss: 0.00003602
Iteration 305/1000 | Loss: 0.00003602
Iteration 306/1000 | Loss: 0.00003601
Iteration 307/1000 | Loss: 0.00003601
Iteration 308/1000 | Loss: 0.00003601
Iteration 309/1000 | Loss: 0.00003601
Iteration 310/1000 | Loss: 0.00003601
Iteration 311/1000 | Loss: 0.00003601
Iteration 312/1000 | Loss: 0.00003601
Iteration 313/1000 | Loss: 0.00003816
Iteration 314/1000 | Loss: 0.00003598
Iteration 315/1000 | Loss: 0.00003598
Iteration 316/1000 | Loss: 0.00003598
Iteration 317/1000 | Loss: 0.00003598
Iteration 318/1000 | Loss: 0.00003598
Iteration 319/1000 | Loss: 0.00003598
Iteration 320/1000 | Loss: 0.00003598
Iteration 321/1000 | Loss: 0.00003598
Iteration 322/1000 | Loss: 0.00003598
Iteration 323/1000 | Loss: 0.00003598
Iteration 324/1000 | Loss: 0.00003598
Iteration 325/1000 | Loss: 0.00003598
Iteration 326/1000 | Loss: 0.00003598
Iteration 327/1000 | Loss: 0.00003598
Iteration 328/1000 | Loss: 0.00003598
Iteration 329/1000 | Loss: 0.00003598
Iteration 330/1000 | Loss: 0.00003597
Iteration 331/1000 | Loss: 0.00003597
Iteration 332/1000 | Loss: 0.00003597
Iteration 333/1000 | Loss: 0.00003597
Iteration 334/1000 | Loss: 0.00003597
Iteration 335/1000 | Loss: 0.00003597
Iteration 336/1000 | Loss: 0.00003597
Iteration 337/1000 | Loss: 0.00003597
Iteration 338/1000 | Loss: 0.00003597
Iteration 339/1000 | Loss: 0.00003597
Iteration 340/1000 | Loss: 0.00003597
Iteration 341/1000 | Loss: 0.00003597
Iteration 342/1000 | Loss: 0.00003597
Iteration 343/1000 | Loss: 0.00003597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 343. Stopping optimization.
Last 5 losses: [3.5971788747701794e-05, 3.5971788747701794e-05, 3.5971788747701794e-05, 3.5971788747701794e-05, 3.5971788747701794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5971788747701794e-05

Optimization complete. Final v2v error: 4.829572677612305 mm

Highest mean error: 6.680543422698975 mm for frame 221

Lowest mean error: 3.4728033542633057 mm for frame 109

Saving results

Total time: 458.00976371765137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811918
Iteration 2/25 | Loss: 0.00117424
Iteration 3/25 | Loss: 0.00091970
Iteration 4/25 | Loss: 0.00086491
Iteration 5/25 | Loss: 0.00084612
Iteration 6/25 | Loss: 0.00084168
Iteration 7/25 | Loss: 0.00083992
Iteration 8/25 | Loss: 0.00083963
Iteration 9/25 | Loss: 0.00083963
Iteration 10/25 | Loss: 0.00083963
Iteration 11/25 | Loss: 0.00083963
Iteration 12/25 | Loss: 0.00083963
Iteration 13/25 | Loss: 0.00083963
Iteration 14/25 | Loss: 0.00083963
Iteration 15/25 | Loss: 0.00083963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008396250777877867, 0.0008396250777877867, 0.0008396250777877867, 0.0008396250777877867, 0.0008396250777877867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008396250777877867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53972685
Iteration 2/25 | Loss: 0.00082940
Iteration 3/25 | Loss: 0.00082940
Iteration 4/25 | Loss: 0.00082940
Iteration 5/25 | Loss: 0.00082939
Iteration 6/25 | Loss: 0.00082939
Iteration 7/25 | Loss: 0.00082939
Iteration 8/25 | Loss: 0.00082939
Iteration 9/25 | Loss: 0.00082939
Iteration 10/25 | Loss: 0.00082939
Iteration 11/25 | Loss: 0.00082939
Iteration 12/25 | Loss: 0.00082939
Iteration 13/25 | Loss: 0.00082939
Iteration 14/25 | Loss: 0.00082939
Iteration 15/25 | Loss: 0.00082939
Iteration 16/25 | Loss: 0.00082939
Iteration 17/25 | Loss: 0.00082939
Iteration 18/25 | Loss: 0.00082939
Iteration 19/25 | Loss: 0.00082939
Iteration 20/25 | Loss: 0.00082939
Iteration 21/25 | Loss: 0.00082939
Iteration 22/25 | Loss: 0.00082939
Iteration 23/25 | Loss: 0.00082939
Iteration 24/25 | Loss: 0.00082939
Iteration 25/25 | Loss: 0.00082939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082939
Iteration 2/1000 | Loss: 0.00005358
Iteration 3/1000 | Loss: 0.00004442
Iteration 4/1000 | Loss: 0.00003318
Iteration 5/1000 | Loss: 0.00003036
Iteration 6/1000 | Loss: 0.00002839
Iteration 7/1000 | Loss: 0.00002689
Iteration 8/1000 | Loss: 0.00002601
Iteration 9/1000 | Loss: 0.00002548
Iteration 10/1000 | Loss: 0.00002512
Iteration 11/1000 | Loss: 0.00002507
Iteration 12/1000 | Loss: 0.00002483
Iteration 13/1000 | Loss: 0.00002474
Iteration 14/1000 | Loss: 0.00002468
Iteration 15/1000 | Loss: 0.00002451
Iteration 16/1000 | Loss: 0.00002437
Iteration 17/1000 | Loss: 0.00002426
Iteration 18/1000 | Loss: 0.00002424
Iteration 19/1000 | Loss: 0.00002424
Iteration 20/1000 | Loss: 0.00002423
Iteration 21/1000 | Loss: 0.00002418
Iteration 22/1000 | Loss: 0.00002418
Iteration 23/1000 | Loss: 0.00002418
Iteration 24/1000 | Loss: 0.00002416
Iteration 25/1000 | Loss: 0.00002415
Iteration 26/1000 | Loss: 0.00002413
Iteration 27/1000 | Loss: 0.00002413
Iteration 28/1000 | Loss: 0.00002413
Iteration 29/1000 | Loss: 0.00002412
Iteration 30/1000 | Loss: 0.00002412
Iteration 31/1000 | Loss: 0.00002411
Iteration 32/1000 | Loss: 0.00002411
Iteration 33/1000 | Loss: 0.00002411
Iteration 34/1000 | Loss: 0.00002410
Iteration 35/1000 | Loss: 0.00002407
Iteration 36/1000 | Loss: 0.00002407
Iteration 37/1000 | Loss: 0.00002407
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002405
Iteration 40/1000 | Loss: 0.00002405
Iteration 41/1000 | Loss: 0.00002404
Iteration 42/1000 | Loss: 0.00002404
Iteration 43/1000 | Loss: 0.00002403
Iteration 44/1000 | Loss: 0.00002402
Iteration 45/1000 | Loss: 0.00002402
Iteration 46/1000 | Loss: 0.00002402
Iteration 47/1000 | Loss: 0.00002402
Iteration 48/1000 | Loss: 0.00002401
Iteration 49/1000 | Loss: 0.00002401
Iteration 50/1000 | Loss: 0.00002401
Iteration 51/1000 | Loss: 0.00002401
Iteration 52/1000 | Loss: 0.00002401
Iteration 53/1000 | Loss: 0.00002399
Iteration 54/1000 | Loss: 0.00002397
Iteration 55/1000 | Loss: 0.00002397
Iteration 56/1000 | Loss: 0.00002397
Iteration 57/1000 | Loss: 0.00002397
Iteration 58/1000 | Loss: 0.00002397
Iteration 59/1000 | Loss: 0.00002397
Iteration 60/1000 | Loss: 0.00002397
Iteration 61/1000 | Loss: 0.00002397
Iteration 62/1000 | Loss: 0.00002397
Iteration 63/1000 | Loss: 0.00002397
Iteration 64/1000 | Loss: 0.00002397
Iteration 65/1000 | Loss: 0.00002396
Iteration 66/1000 | Loss: 0.00002396
Iteration 67/1000 | Loss: 0.00002396
Iteration 68/1000 | Loss: 0.00002396
Iteration 69/1000 | Loss: 0.00002396
Iteration 70/1000 | Loss: 0.00002396
Iteration 71/1000 | Loss: 0.00002396
Iteration 72/1000 | Loss: 0.00002396
Iteration 73/1000 | Loss: 0.00002395
Iteration 74/1000 | Loss: 0.00002395
Iteration 75/1000 | Loss: 0.00002395
Iteration 76/1000 | Loss: 0.00002395
Iteration 77/1000 | Loss: 0.00002394
Iteration 78/1000 | Loss: 0.00002394
Iteration 79/1000 | Loss: 0.00002394
Iteration 80/1000 | Loss: 0.00002394
Iteration 81/1000 | Loss: 0.00002393
Iteration 82/1000 | Loss: 0.00002393
Iteration 83/1000 | Loss: 0.00002393
Iteration 84/1000 | Loss: 0.00002393
Iteration 85/1000 | Loss: 0.00002392
Iteration 86/1000 | Loss: 0.00002392
Iteration 87/1000 | Loss: 0.00002392
Iteration 88/1000 | Loss: 0.00002392
Iteration 89/1000 | Loss: 0.00002392
Iteration 90/1000 | Loss: 0.00002392
Iteration 91/1000 | Loss: 0.00002391
Iteration 92/1000 | Loss: 0.00002391
Iteration 93/1000 | Loss: 0.00002391
Iteration 94/1000 | Loss: 0.00002390
Iteration 95/1000 | Loss: 0.00002390
Iteration 96/1000 | Loss: 0.00002390
Iteration 97/1000 | Loss: 0.00002390
Iteration 98/1000 | Loss: 0.00002390
Iteration 99/1000 | Loss: 0.00002390
Iteration 100/1000 | Loss: 0.00002389
Iteration 101/1000 | Loss: 0.00002389
Iteration 102/1000 | Loss: 0.00002389
Iteration 103/1000 | Loss: 0.00002389
Iteration 104/1000 | Loss: 0.00002389
Iteration 105/1000 | Loss: 0.00002389
Iteration 106/1000 | Loss: 0.00002389
Iteration 107/1000 | Loss: 0.00002389
Iteration 108/1000 | Loss: 0.00002389
Iteration 109/1000 | Loss: 0.00002389
Iteration 110/1000 | Loss: 0.00002388
Iteration 111/1000 | Loss: 0.00002388
Iteration 112/1000 | Loss: 0.00002388
Iteration 113/1000 | Loss: 0.00002388
Iteration 114/1000 | Loss: 0.00002388
Iteration 115/1000 | Loss: 0.00002388
Iteration 116/1000 | Loss: 0.00002388
Iteration 117/1000 | Loss: 0.00002387
Iteration 118/1000 | Loss: 0.00002387
Iteration 119/1000 | Loss: 0.00002387
Iteration 120/1000 | Loss: 0.00002387
Iteration 121/1000 | Loss: 0.00002387
Iteration 122/1000 | Loss: 0.00002386
Iteration 123/1000 | Loss: 0.00002386
Iteration 124/1000 | Loss: 0.00002386
Iteration 125/1000 | Loss: 0.00002386
Iteration 126/1000 | Loss: 0.00002386
Iteration 127/1000 | Loss: 0.00002386
Iteration 128/1000 | Loss: 0.00002386
Iteration 129/1000 | Loss: 0.00002386
Iteration 130/1000 | Loss: 0.00002386
Iteration 131/1000 | Loss: 0.00002386
Iteration 132/1000 | Loss: 0.00002386
Iteration 133/1000 | Loss: 0.00002386
Iteration 134/1000 | Loss: 0.00002386
Iteration 135/1000 | Loss: 0.00002386
Iteration 136/1000 | Loss: 0.00002386
Iteration 137/1000 | Loss: 0.00002386
Iteration 138/1000 | Loss: 0.00002385
Iteration 139/1000 | Loss: 0.00002385
Iteration 140/1000 | Loss: 0.00002385
Iteration 141/1000 | Loss: 0.00002385
Iteration 142/1000 | Loss: 0.00002385
Iteration 143/1000 | Loss: 0.00002385
Iteration 144/1000 | Loss: 0.00002385
Iteration 145/1000 | Loss: 0.00002385
Iteration 146/1000 | Loss: 0.00002385
Iteration 147/1000 | Loss: 0.00002385
Iteration 148/1000 | Loss: 0.00002385
Iteration 149/1000 | Loss: 0.00002385
Iteration 150/1000 | Loss: 0.00002385
Iteration 151/1000 | Loss: 0.00002385
Iteration 152/1000 | Loss: 0.00002384
Iteration 153/1000 | Loss: 0.00002384
Iteration 154/1000 | Loss: 0.00002384
Iteration 155/1000 | Loss: 0.00002384
Iteration 156/1000 | Loss: 0.00002384
Iteration 157/1000 | Loss: 0.00002384
Iteration 158/1000 | Loss: 0.00002384
Iteration 159/1000 | Loss: 0.00002384
Iteration 160/1000 | Loss: 0.00002384
Iteration 161/1000 | Loss: 0.00002384
Iteration 162/1000 | Loss: 0.00002384
Iteration 163/1000 | Loss: 0.00002384
Iteration 164/1000 | Loss: 0.00002384
Iteration 165/1000 | Loss: 0.00002384
Iteration 166/1000 | Loss: 0.00002384
Iteration 167/1000 | Loss: 0.00002384
Iteration 168/1000 | Loss: 0.00002383
Iteration 169/1000 | Loss: 0.00002383
Iteration 170/1000 | Loss: 0.00002383
Iteration 171/1000 | Loss: 0.00002383
Iteration 172/1000 | Loss: 0.00002383
Iteration 173/1000 | Loss: 0.00002383
Iteration 174/1000 | Loss: 0.00002383
Iteration 175/1000 | Loss: 0.00002383
Iteration 176/1000 | Loss: 0.00002383
Iteration 177/1000 | Loss: 0.00002383
Iteration 178/1000 | Loss: 0.00002383
Iteration 179/1000 | Loss: 0.00002383
Iteration 180/1000 | Loss: 0.00002383
Iteration 181/1000 | Loss: 0.00002383
Iteration 182/1000 | Loss: 0.00002383
Iteration 183/1000 | Loss: 0.00002382
Iteration 184/1000 | Loss: 0.00002382
Iteration 185/1000 | Loss: 0.00002382
Iteration 186/1000 | Loss: 0.00002382
Iteration 187/1000 | Loss: 0.00002382
Iteration 188/1000 | Loss: 0.00002382
Iteration 189/1000 | Loss: 0.00002382
Iteration 190/1000 | Loss: 0.00002382
Iteration 191/1000 | Loss: 0.00002382
Iteration 192/1000 | Loss: 0.00002382
Iteration 193/1000 | Loss: 0.00002382
Iteration 194/1000 | Loss: 0.00002382
Iteration 195/1000 | Loss: 0.00002382
Iteration 196/1000 | Loss: 0.00002382
Iteration 197/1000 | Loss: 0.00002382
Iteration 198/1000 | Loss: 0.00002382
Iteration 199/1000 | Loss: 0.00002382
Iteration 200/1000 | Loss: 0.00002382
Iteration 201/1000 | Loss: 0.00002382
Iteration 202/1000 | Loss: 0.00002382
Iteration 203/1000 | Loss: 0.00002382
Iteration 204/1000 | Loss: 0.00002382
Iteration 205/1000 | Loss: 0.00002382
Iteration 206/1000 | Loss: 0.00002382
Iteration 207/1000 | Loss: 0.00002382
Iteration 208/1000 | Loss: 0.00002382
Iteration 209/1000 | Loss: 0.00002382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.3815959139028564e-05, 2.3815959139028564e-05, 2.3815959139028564e-05, 2.3815959139028564e-05, 2.3815959139028564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3815959139028564e-05

Optimization complete. Final v2v error: 3.943000078201294 mm

Highest mean error: 4.538015365600586 mm for frame 123

Lowest mean error: 3.2930798530578613 mm for frame 0

Saving results

Total time: 45.155421018600464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00543304
Iteration 2/25 | Loss: 0.00086685
Iteration 3/25 | Loss: 0.00076230
Iteration 4/25 | Loss: 0.00073972
Iteration 5/25 | Loss: 0.00073438
Iteration 6/25 | Loss: 0.00073313
Iteration 7/25 | Loss: 0.00073268
Iteration 8/25 | Loss: 0.00073267
Iteration 9/25 | Loss: 0.00073267
Iteration 10/25 | Loss: 0.00073266
Iteration 11/25 | Loss: 0.00073266
Iteration 12/25 | Loss: 0.00073266
Iteration 13/25 | Loss: 0.00073266
Iteration 14/25 | Loss: 0.00073266
Iteration 15/25 | Loss: 0.00073266
Iteration 16/25 | Loss: 0.00073266
Iteration 17/25 | Loss: 0.00073266
Iteration 18/25 | Loss: 0.00073266
Iteration 19/25 | Loss: 0.00073266
Iteration 20/25 | Loss: 0.00073266
Iteration 21/25 | Loss: 0.00073266
Iteration 22/25 | Loss: 0.00073266
Iteration 23/25 | Loss: 0.00073266
Iteration 24/25 | Loss: 0.00073266
Iteration 25/25 | Loss: 0.00073266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.24040174
Iteration 2/25 | Loss: 0.00044089
Iteration 3/25 | Loss: 0.00044088
Iteration 4/25 | Loss: 0.00044088
Iteration 5/25 | Loss: 0.00044087
Iteration 6/25 | Loss: 0.00044087
Iteration 7/25 | Loss: 0.00044087
Iteration 8/25 | Loss: 0.00044087
Iteration 9/25 | Loss: 0.00044087
Iteration 10/25 | Loss: 0.00044087
Iteration 11/25 | Loss: 0.00044087
Iteration 12/25 | Loss: 0.00044087
Iteration 13/25 | Loss: 0.00044087
Iteration 14/25 | Loss: 0.00044087
Iteration 15/25 | Loss: 0.00044087
Iteration 16/25 | Loss: 0.00044087
Iteration 17/25 | Loss: 0.00044087
Iteration 18/25 | Loss: 0.00044087
Iteration 19/25 | Loss: 0.00044087
Iteration 20/25 | Loss: 0.00044087
Iteration 21/25 | Loss: 0.00044087
Iteration 22/25 | Loss: 0.00044087
Iteration 23/25 | Loss: 0.00044087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004408727982081473, 0.0004408727982081473, 0.0004408727982081473, 0.0004408727982081473, 0.0004408727982081473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004408727982081473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044087
Iteration 2/1000 | Loss: 0.00002995
Iteration 3/1000 | Loss: 0.00001926
Iteration 4/1000 | Loss: 0.00001746
Iteration 5/1000 | Loss: 0.00001642
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001543
Iteration 8/1000 | Loss: 0.00001516
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001492
Iteration 11/1000 | Loss: 0.00001490
Iteration 12/1000 | Loss: 0.00001487
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001483
Iteration 15/1000 | Loss: 0.00001482
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001470
Iteration 18/1000 | Loss: 0.00001463
Iteration 19/1000 | Loss: 0.00001462
Iteration 20/1000 | Loss: 0.00001457
Iteration 21/1000 | Loss: 0.00001457
Iteration 22/1000 | Loss: 0.00001457
Iteration 23/1000 | Loss: 0.00001457
Iteration 24/1000 | Loss: 0.00001454
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001454
Iteration 27/1000 | Loss: 0.00001453
Iteration 28/1000 | Loss: 0.00001453
Iteration 29/1000 | Loss: 0.00001453
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001452
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001449
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001447
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001446
Iteration 57/1000 | Loss: 0.00001446
Iteration 58/1000 | Loss: 0.00001446
Iteration 59/1000 | Loss: 0.00001446
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001436
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001433
Iteration 85/1000 | Loss: 0.00001432
Iteration 86/1000 | Loss: 0.00001432
Iteration 87/1000 | Loss: 0.00001432
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001431
Iteration 94/1000 | Loss: 0.00001431
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001431
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001431
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001430
Iteration 104/1000 | Loss: 0.00001430
Iteration 105/1000 | Loss: 0.00001430
Iteration 106/1000 | Loss: 0.00001430
Iteration 107/1000 | Loss: 0.00001430
Iteration 108/1000 | Loss: 0.00001430
Iteration 109/1000 | Loss: 0.00001430
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001429
Iteration 112/1000 | Loss: 0.00001429
Iteration 113/1000 | Loss: 0.00001429
Iteration 114/1000 | Loss: 0.00001429
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001427
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001427
Iteration 135/1000 | Loss: 0.00001427
Iteration 136/1000 | Loss: 0.00001427
Iteration 137/1000 | Loss: 0.00001427
Iteration 138/1000 | Loss: 0.00001427
Iteration 139/1000 | Loss: 0.00001426
Iteration 140/1000 | Loss: 0.00001426
Iteration 141/1000 | Loss: 0.00001426
Iteration 142/1000 | Loss: 0.00001426
Iteration 143/1000 | Loss: 0.00001426
Iteration 144/1000 | Loss: 0.00001426
Iteration 145/1000 | Loss: 0.00001425
Iteration 146/1000 | Loss: 0.00001425
Iteration 147/1000 | Loss: 0.00001425
Iteration 148/1000 | Loss: 0.00001425
Iteration 149/1000 | Loss: 0.00001424
Iteration 150/1000 | Loss: 0.00001424
Iteration 151/1000 | Loss: 0.00001424
Iteration 152/1000 | Loss: 0.00001424
Iteration 153/1000 | Loss: 0.00001424
Iteration 154/1000 | Loss: 0.00001424
Iteration 155/1000 | Loss: 0.00001424
Iteration 156/1000 | Loss: 0.00001424
Iteration 157/1000 | Loss: 0.00001424
Iteration 158/1000 | Loss: 0.00001424
Iteration 159/1000 | Loss: 0.00001424
Iteration 160/1000 | Loss: 0.00001424
Iteration 161/1000 | Loss: 0.00001424
Iteration 162/1000 | Loss: 0.00001424
Iteration 163/1000 | Loss: 0.00001424
Iteration 164/1000 | Loss: 0.00001423
Iteration 165/1000 | Loss: 0.00001423
Iteration 166/1000 | Loss: 0.00001423
Iteration 167/1000 | Loss: 0.00001423
Iteration 168/1000 | Loss: 0.00001422
Iteration 169/1000 | Loss: 0.00001422
Iteration 170/1000 | Loss: 0.00001422
Iteration 171/1000 | Loss: 0.00001422
Iteration 172/1000 | Loss: 0.00001422
Iteration 173/1000 | Loss: 0.00001422
Iteration 174/1000 | Loss: 0.00001422
Iteration 175/1000 | Loss: 0.00001422
Iteration 176/1000 | Loss: 0.00001422
Iteration 177/1000 | Loss: 0.00001422
Iteration 178/1000 | Loss: 0.00001422
Iteration 179/1000 | Loss: 0.00001422
Iteration 180/1000 | Loss: 0.00001422
Iteration 181/1000 | Loss: 0.00001422
Iteration 182/1000 | Loss: 0.00001422
Iteration 183/1000 | Loss: 0.00001422
Iteration 184/1000 | Loss: 0.00001422
Iteration 185/1000 | Loss: 0.00001422
Iteration 186/1000 | Loss: 0.00001422
Iteration 187/1000 | Loss: 0.00001422
Iteration 188/1000 | Loss: 0.00001422
Iteration 189/1000 | Loss: 0.00001422
Iteration 190/1000 | Loss: 0.00001422
Iteration 191/1000 | Loss: 0.00001422
Iteration 192/1000 | Loss: 0.00001422
Iteration 193/1000 | Loss: 0.00001422
Iteration 194/1000 | Loss: 0.00001422
Iteration 195/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.4216753697837703e-05, 1.4216753697837703e-05, 1.4216753697837703e-05, 1.4216753697837703e-05, 1.4216753697837703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4216753697837703e-05

Optimization complete. Final v2v error: 3.2063748836517334 mm

Highest mean error: 3.699702262878418 mm for frame 58

Lowest mean error: 2.923178195953369 mm for frame 125

Saving results

Total time: 38.68252420425415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886346
Iteration 2/25 | Loss: 0.00131068
Iteration 3/25 | Loss: 0.00096009
Iteration 4/25 | Loss: 0.00087965
Iteration 5/25 | Loss: 0.00086630
Iteration 6/25 | Loss: 0.00086513
Iteration 7/25 | Loss: 0.00086513
Iteration 8/25 | Loss: 0.00086513
Iteration 9/25 | Loss: 0.00086513
Iteration 10/25 | Loss: 0.00086513
Iteration 11/25 | Loss: 0.00086513
Iteration 12/25 | Loss: 0.00086513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008651307434774935, 0.0008651307434774935, 0.0008651307434774935, 0.0008651307434774935, 0.0008651307434774935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008651307434774935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42526329
Iteration 2/25 | Loss: 0.00050034
Iteration 3/25 | Loss: 0.00050034
Iteration 4/25 | Loss: 0.00050034
Iteration 5/25 | Loss: 0.00050034
Iteration 6/25 | Loss: 0.00050034
Iteration 7/25 | Loss: 0.00050034
Iteration 8/25 | Loss: 0.00050034
Iteration 9/25 | Loss: 0.00050034
Iteration 10/25 | Loss: 0.00050034
Iteration 11/25 | Loss: 0.00050034
Iteration 12/25 | Loss: 0.00050034
Iteration 13/25 | Loss: 0.00050034
Iteration 14/25 | Loss: 0.00050034
Iteration 15/25 | Loss: 0.00050034
Iteration 16/25 | Loss: 0.00050034
Iteration 17/25 | Loss: 0.00050034
Iteration 18/25 | Loss: 0.00050034
Iteration 19/25 | Loss: 0.00050034
Iteration 20/25 | Loss: 0.00050034
Iteration 21/25 | Loss: 0.00050034
Iteration 22/25 | Loss: 0.00050034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005003350088372827, 0.0005003350088372827, 0.0005003350088372827, 0.0005003350088372827, 0.0005003350088372827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005003350088372827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050034
Iteration 2/1000 | Loss: 0.00004056
Iteration 3/1000 | Loss: 0.00003113
Iteration 4/1000 | Loss: 0.00002740
Iteration 5/1000 | Loss: 0.00002573
Iteration 6/1000 | Loss: 0.00002486
Iteration 7/1000 | Loss: 0.00002415
Iteration 8/1000 | Loss: 0.00002376
Iteration 9/1000 | Loss: 0.00002348
Iteration 10/1000 | Loss: 0.00002331
Iteration 11/1000 | Loss: 0.00002314
Iteration 12/1000 | Loss: 0.00002310
Iteration 13/1000 | Loss: 0.00002302
Iteration 14/1000 | Loss: 0.00002298
Iteration 15/1000 | Loss: 0.00002297
Iteration 16/1000 | Loss: 0.00002297
Iteration 17/1000 | Loss: 0.00002295
Iteration 18/1000 | Loss: 0.00002291
Iteration 19/1000 | Loss: 0.00002291
Iteration 20/1000 | Loss: 0.00002291
Iteration 21/1000 | Loss: 0.00002291
Iteration 22/1000 | Loss: 0.00002290
Iteration 23/1000 | Loss: 0.00002290
Iteration 24/1000 | Loss: 0.00002287
Iteration 25/1000 | Loss: 0.00002286
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002285
Iteration 28/1000 | Loss: 0.00002282
Iteration 29/1000 | Loss: 0.00002275
Iteration 30/1000 | Loss: 0.00002275
Iteration 31/1000 | Loss: 0.00002275
Iteration 32/1000 | Loss: 0.00002275
Iteration 33/1000 | Loss: 0.00002274
Iteration 34/1000 | Loss: 0.00002272
Iteration 35/1000 | Loss: 0.00002271
Iteration 36/1000 | Loss: 0.00002270
Iteration 37/1000 | Loss: 0.00002270
Iteration 38/1000 | Loss: 0.00002270
Iteration 39/1000 | Loss: 0.00002269
Iteration 40/1000 | Loss: 0.00002269
Iteration 41/1000 | Loss: 0.00002267
Iteration 42/1000 | Loss: 0.00002266
Iteration 43/1000 | Loss: 0.00002266
Iteration 44/1000 | Loss: 0.00002265
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002264
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002263
Iteration 50/1000 | Loss: 0.00002262
Iteration 51/1000 | Loss: 0.00002262
Iteration 52/1000 | Loss: 0.00002262
Iteration 53/1000 | Loss: 0.00002260
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002260
Iteration 57/1000 | Loss: 0.00002259
Iteration 58/1000 | Loss: 0.00002259
Iteration 59/1000 | Loss: 0.00002259
Iteration 60/1000 | Loss: 0.00002258
Iteration 61/1000 | Loss: 0.00002258
Iteration 62/1000 | Loss: 0.00002257
Iteration 63/1000 | Loss: 0.00002257
Iteration 64/1000 | Loss: 0.00002257
Iteration 65/1000 | Loss: 0.00002256
Iteration 66/1000 | Loss: 0.00002256
Iteration 67/1000 | Loss: 0.00002256
Iteration 68/1000 | Loss: 0.00002256
Iteration 69/1000 | Loss: 0.00002256
Iteration 70/1000 | Loss: 0.00002255
Iteration 71/1000 | Loss: 0.00002255
Iteration 72/1000 | Loss: 0.00002255
Iteration 73/1000 | Loss: 0.00002255
Iteration 74/1000 | Loss: 0.00002254
Iteration 75/1000 | Loss: 0.00002254
Iteration 76/1000 | Loss: 0.00002254
Iteration 77/1000 | Loss: 0.00002254
Iteration 78/1000 | Loss: 0.00002253
Iteration 79/1000 | Loss: 0.00002253
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002253
Iteration 82/1000 | Loss: 0.00002253
Iteration 83/1000 | Loss: 0.00002253
Iteration 84/1000 | Loss: 0.00002252
Iteration 85/1000 | Loss: 0.00002252
Iteration 86/1000 | Loss: 0.00002252
Iteration 87/1000 | Loss: 0.00002252
Iteration 88/1000 | Loss: 0.00002252
Iteration 89/1000 | Loss: 0.00002252
Iteration 90/1000 | Loss: 0.00002252
Iteration 91/1000 | Loss: 0.00002251
Iteration 92/1000 | Loss: 0.00002251
Iteration 93/1000 | Loss: 0.00002251
Iteration 94/1000 | Loss: 0.00002251
Iteration 95/1000 | Loss: 0.00002251
Iteration 96/1000 | Loss: 0.00002251
Iteration 97/1000 | Loss: 0.00002250
Iteration 98/1000 | Loss: 0.00002250
Iteration 99/1000 | Loss: 0.00002250
Iteration 100/1000 | Loss: 0.00002250
Iteration 101/1000 | Loss: 0.00002249
Iteration 102/1000 | Loss: 0.00002249
Iteration 103/1000 | Loss: 0.00002249
Iteration 104/1000 | Loss: 0.00002249
Iteration 105/1000 | Loss: 0.00002248
Iteration 106/1000 | Loss: 0.00002248
Iteration 107/1000 | Loss: 0.00002248
Iteration 108/1000 | Loss: 0.00002248
Iteration 109/1000 | Loss: 0.00002248
Iteration 110/1000 | Loss: 0.00002248
Iteration 111/1000 | Loss: 0.00002248
Iteration 112/1000 | Loss: 0.00002248
Iteration 113/1000 | Loss: 0.00002248
Iteration 114/1000 | Loss: 0.00002248
Iteration 115/1000 | Loss: 0.00002248
Iteration 116/1000 | Loss: 0.00002248
Iteration 117/1000 | Loss: 0.00002248
Iteration 118/1000 | Loss: 0.00002248
Iteration 119/1000 | Loss: 0.00002248
Iteration 120/1000 | Loss: 0.00002247
Iteration 121/1000 | Loss: 0.00002247
Iteration 122/1000 | Loss: 0.00002247
Iteration 123/1000 | Loss: 0.00002247
Iteration 124/1000 | Loss: 0.00002247
Iteration 125/1000 | Loss: 0.00002247
Iteration 126/1000 | Loss: 0.00002247
Iteration 127/1000 | Loss: 0.00002247
Iteration 128/1000 | Loss: 0.00002246
Iteration 129/1000 | Loss: 0.00002246
Iteration 130/1000 | Loss: 0.00002246
Iteration 131/1000 | Loss: 0.00002246
Iteration 132/1000 | Loss: 0.00002246
Iteration 133/1000 | Loss: 0.00002246
Iteration 134/1000 | Loss: 0.00002246
Iteration 135/1000 | Loss: 0.00002246
Iteration 136/1000 | Loss: 0.00002246
Iteration 137/1000 | Loss: 0.00002246
Iteration 138/1000 | Loss: 0.00002246
Iteration 139/1000 | Loss: 0.00002246
Iteration 140/1000 | Loss: 0.00002246
Iteration 141/1000 | Loss: 0.00002246
Iteration 142/1000 | Loss: 0.00002245
Iteration 143/1000 | Loss: 0.00002245
Iteration 144/1000 | Loss: 0.00002245
Iteration 145/1000 | Loss: 0.00002245
Iteration 146/1000 | Loss: 0.00002245
Iteration 147/1000 | Loss: 0.00002245
Iteration 148/1000 | Loss: 0.00002245
Iteration 149/1000 | Loss: 0.00002245
Iteration 150/1000 | Loss: 0.00002245
Iteration 151/1000 | Loss: 0.00002245
Iteration 152/1000 | Loss: 0.00002245
Iteration 153/1000 | Loss: 0.00002245
Iteration 154/1000 | Loss: 0.00002245
Iteration 155/1000 | Loss: 0.00002245
Iteration 156/1000 | Loss: 0.00002244
Iteration 157/1000 | Loss: 0.00002244
Iteration 158/1000 | Loss: 0.00002244
Iteration 159/1000 | Loss: 0.00002244
Iteration 160/1000 | Loss: 0.00002244
Iteration 161/1000 | Loss: 0.00002244
Iteration 162/1000 | Loss: 0.00002244
Iteration 163/1000 | Loss: 0.00002244
Iteration 164/1000 | Loss: 0.00002244
Iteration 165/1000 | Loss: 0.00002244
Iteration 166/1000 | Loss: 0.00002244
Iteration 167/1000 | Loss: 0.00002244
Iteration 168/1000 | Loss: 0.00002244
Iteration 169/1000 | Loss: 0.00002244
Iteration 170/1000 | Loss: 0.00002244
Iteration 171/1000 | Loss: 0.00002244
Iteration 172/1000 | Loss: 0.00002244
Iteration 173/1000 | Loss: 0.00002244
Iteration 174/1000 | Loss: 0.00002244
Iteration 175/1000 | Loss: 0.00002244
Iteration 176/1000 | Loss: 0.00002244
Iteration 177/1000 | Loss: 0.00002244
Iteration 178/1000 | Loss: 0.00002244
Iteration 179/1000 | Loss: 0.00002244
Iteration 180/1000 | Loss: 0.00002244
Iteration 181/1000 | Loss: 0.00002244
Iteration 182/1000 | Loss: 0.00002244
Iteration 183/1000 | Loss: 0.00002244
Iteration 184/1000 | Loss: 0.00002244
Iteration 185/1000 | Loss: 0.00002244
Iteration 186/1000 | Loss: 0.00002244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.2439775420934893e-05, 2.2439775420934893e-05, 2.2439775420934893e-05, 2.2439775420934893e-05, 2.2439775420934893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2439775420934893e-05

Optimization complete. Final v2v error: 3.928445339202881 mm

Highest mean error: 4.247117042541504 mm for frame 157

Lowest mean error: 3.733572483062744 mm for frame 88

Saving results

Total time: 39.479480504989624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846116
Iteration 2/25 | Loss: 0.00092532
Iteration 3/25 | Loss: 0.00074464
Iteration 4/25 | Loss: 0.00071421
Iteration 5/25 | Loss: 0.00070745
Iteration 6/25 | Loss: 0.00070556
Iteration 7/25 | Loss: 0.00070527
Iteration 8/25 | Loss: 0.00070527
Iteration 9/25 | Loss: 0.00070527
Iteration 10/25 | Loss: 0.00070527
Iteration 11/25 | Loss: 0.00070527
Iteration 12/25 | Loss: 0.00070527
Iteration 13/25 | Loss: 0.00070527
Iteration 14/25 | Loss: 0.00070527
Iteration 15/25 | Loss: 0.00070527
Iteration 16/25 | Loss: 0.00070527
Iteration 17/25 | Loss: 0.00070527
Iteration 18/25 | Loss: 0.00070527
Iteration 19/25 | Loss: 0.00070527
Iteration 20/25 | Loss: 0.00070527
Iteration 21/25 | Loss: 0.00070527
Iteration 22/25 | Loss: 0.00070527
Iteration 23/25 | Loss: 0.00070527
Iteration 24/25 | Loss: 0.00070527
Iteration 25/25 | Loss: 0.00070527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50180602
Iteration 2/25 | Loss: 0.00042399
Iteration 3/25 | Loss: 0.00042399
Iteration 4/25 | Loss: 0.00042399
Iteration 5/25 | Loss: 0.00042398
Iteration 6/25 | Loss: 0.00042398
Iteration 7/25 | Loss: 0.00042398
Iteration 8/25 | Loss: 0.00042398
Iteration 9/25 | Loss: 0.00042398
Iteration 10/25 | Loss: 0.00042398
Iteration 11/25 | Loss: 0.00042398
Iteration 12/25 | Loss: 0.00042398
Iteration 13/25 | Loss: 0.00042398
Iteration 14/25 | Loss: 0.00042398
Iteration 15/25 | Loss: 0.00042398
Iteration 16/25 | Loss: 0.00042398
Iteration 17/25 | Loss: 0.00042398
Iteration 18/25 | Loss: 0.00042398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004239827103447169, 0.0004239827103447169, 0.0004239827103447169, 0.0004239827103447169, 0.0004239827103447169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004239827103447169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042398
Iteration 2/1000 | Loss: 0.00002226
Iteration 3/1000 | Loss: 0.00001484
Iteration 4/1000 | Loss: 0.00001295
Iteration 5/1000 | Loss: 0.00001190
Iteration 6/1000 | Loss: 0.00001146
Iteration 7/1000 | Loss: 0.00001116
Iteration 8/1000 | Loss: 0.00001099
Iteration 9/1000 | Loss: 0.00001099
Iteration 10/1000 | Loss: 0.00001089
Iteration 11/1000 | Loss: 0.00001083
Iteration 12/1000 | Loss: 0.00001081
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001068
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001054
Iteration 17/1000 | Loss: 0.00001053
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001052
Iteration 20/1000 | Loss: 0.00001052
Iteration 21/1000 | Loss: 0.00001052
Iteration 22/1000 | Loss: 0.00001052
Iteration 23/1000 | Loss: 0.00001052
Iteration 24/1000 | Loss: 0.00001052
Iteration 25/1000 | Loss: 0.00001052
Iteration 26/1000 | Loss: 0.00001052
Iteration 27/1000 | Loss: 0.00001052
Iteration 28/1000 | Loss: 0.00001052
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001052
Iteration 32/1000 | Loss: 0.00001052
Iteration 33/1000 | Loss: 0.00001052
Iteration 34/1000 | Loss: 0.00001052
Iteration 35/1000 | Loss: 0.00001052
Iteration 36/1000 | Loss: 0.00001052
Iteration 37/1000 | Loss: 0.00001052
Iteration 38/1000 | Loss: 0.00001052
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001052
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001052
Iteration 46/1000 | Loss: 0.00001052
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001052
Iteration 49/1000 | Loss: 0.00001052
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001052
Iteration 53/1000 | Loss: 0.00001052
Iteration 54/1000 | Loss: 0.00001052
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001052
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001052
Iteration 59/1000 | Loss: 0.00001052
Iteration 60/1000 | Loss: 0.00001052
Iteration 61/1000 | Loss: 0.00001052
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001052
Iteration 64/1000 | Loss: 0.00001052
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.052033530868357e-05, 1.052033530868357e-05, 1.052033530868357e-05, 1.052033530868357e-05, 1.052033530868357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.052033530868357e-05

Optimization complete. Final v2v error: 2.7417259216308594 mm

Highest mean error: 3.067112684249878 mm for frame 86

Lowest mean error: 2.5963869094848633 mm for frame 205

Saving results

Total time: 30.640703439712524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811490
Iteration 2/25 | Loss: 0.00103053
Iteration 3/25 | Loss: 0.00079048
Iteration 4/25 | Loss: 0.00076098
Iteration 5/25 | Loss: 0.00075483
Iteration 6/25 | Loss: 0.00075231
Iteration 7/25 | Loss: 0.00075204
Iteration 8/25 | Loss: 0.00075204
Iteration 9/25 | Loss: 0.00075204
Iteration 10/25 | Loss: 0.00075202
Iteration 11/25 | Loss: 0.00075202
Iteration 12/25 | Loss: 0.00075202
Iteration 13/25 | Loss: 0.00075202
Iteration 14/25 | Loss: 0.00075202
Iteration 15/25 | Loss: 0.00075202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007520202198065817, 0.0007520202198065817, 0.0007520202198065817, 0.0007520202198065817, 0.0007520202198065817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007520202198065817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54302406
Iteration 2/25 | Loss: 0.00039835
Iteration 3/25 | Loss: 0.00039832
Iteration 4/25 | Loss: 0.00039832
Iteration 5/25 | Loss: 0.00039832
Iteration 6/25 | Loss: 0.00039832
Iteration 7/25 | Loss: 0.00039832
Iteration 8/25 | Loss: 0.00039832
Iteration 9/25 | Loss: 0.00039832
Iteration 10/25 | Loss: 0.00039832
Iteration 11/25 | Loss: 0.00039832
Iteration 12/25 | Loss: 0.00039832
Iteration 13/25 | Loss: 0.00039832
Iteration 14/25 | Loss: 0.00039832
Iteration 15/25 | Loss: 0.00039832
Iteration 16/25 | Loss: 0.00039832
Iteration 17/25 | Loss: 0.00039832
Iteration 18/25 | Loss: 0.00039832
Iteration 19/25 | Loss: 0.00039832
Iteration 20/25 | Loss: 0.00039832
Iteration 21/25 | Loss: 0.00039832
Iteration 22/25 | Loss: 0.00039832
Iteration 23/25 | Loss: 0.00039832
Iteration 24/25 | Loss: 0.00039832
Iteration 25/25 | Loss: 0.00039832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039832
Iteration 2/1000 | Loss: 0.00002516
Iteration 3/1000 | Loss: 0.00001817
Iteration 4/1000 | Loss: 0.00001610
Iteration 5/1000 | Loss: 0.00001500
Iteration 6/1000 | Loss: 0.00001441
Iteration 7/1000 | Loss: 0.00001392
Iteration 8/1000 | Loss: 0.00001362
Iteration 9/1000 | Loss: 0.00001338
Iteration 10/1000 | Loss: 0.00001331
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001309
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001299
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001294
Iteration 21/1000 | Loss: 0.00001294
Iteration 22/1000 | Loss: 0.00001294
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001288
Iteration 27/1000 | Loss: 0.00001287
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001286
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001285
Iteration 33/1000 | Loss: 0.00001285
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001285
Iteration 36/1000 | Loss: 0.00001285
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001284
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001284
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001283
Iteration 46/1000 | Loss: 0.00001282
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001281
Iteration 51/1000 | Loss: 0.00001281
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001280
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001278
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001277
Iteration 67/1000 | Loss: 0.00001277
Iteration 68/1000 | Loss: 0.00001276
Iteration 69/1000 | Loss: 0.00001276
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001275
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001275
Iteration 75/1000 | Loss: 0.00001275
Iteration 76/1000 | Loss: 0.00001275
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001272
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001270
Iteration 95/1000 | Loss: 0.00001270
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001270
Iteration 98/1000 | Loss: 0.00001270
Iteration 99/1000 | Loss: 0.00001270
Iteration 100/1000 | Loss: 0.00001269
Iteration 101/1000 | Loss: 0.00001269
Iteration 102/1000 | Loss: 0.00001269
Iteration 103/1000 | Loss: 0.00001268
Iteration 104/1000 | Loss: 0.00001268
Iteration 105/1000 | Loss: 0.00001268
Iteration 106/1000 | Loss: 0.00001268
Iteration 107/1000 | Loss: 0.00001267
Iteration 108/1000 | Loss: 0.00001267
Iteration 109/1000 | Loss: 0.00001267
Iteration 110/1000 | Loss: 0.00001267
Iteration 111/1000 | Loss: 0.00001267
Iteration 112/1000 | Loss: 0.00001267
Iteration 113/1000 | Loss: 0.00001267
Iteration 114/1000 | Loss: 0.00001267
Iteration 115/1000 | Loss: 0.00001267
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001266
Iteration 119/1000 | Loss: 0.00001266
Iteration 120/1000 | Loss: 0.00001266
Iteration 121/1000 | Loss: 0.00001266
Iteration 122/1000 | Loss: 0.00001266
Iteration 123/1000 | Loss: 0.00001266
Iteration 124/1000 | Loss: 0.00001266
Iteration 125/1000 | Loss: 0.00001266
Iteration 126/1000 | Loss: 0.00001266
Iteration 127/1000 | Loss: 0.00001266
Iteration 128/1000 | Loss: 0.00001266
Iteration 129/1000 | Loss: 0.00001266
Iteration 130/1000 | Loss: 0.00001266
Iteration 131/1000 | Loss: 0.00001266
Iteration 132/1000 | Loss: 0.00001266
Iteration 133/1000 | Loss: 0.00001265
Iteration 134/1000 | Loss: 0.00001265
Iteration 135/1000 | Loss: 0.00001265
Iteration 136/1000 | Loss: 0.00001265
Iteration 137/1000 | Loss: 0.00001265
Iteration 138/1000 | Loss: 0.00001265
Iteration 139/1000 | Loss: 0.00001265
Iteration 140/1000 | Loss: 0.00001265
Iteration 141/1000 | Loss: 0.00001265
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001265
Iteration 145/1000 | Loss: 0.00001265
Iteration 146/1000 | Loss: 0.00001264
Iteration 147/1000 | Loss: 0.00001264
Iteration 148/1000 | Loss: 0.00001264
Iteration 149/1000 | Loss: 0.00001264
Iteration 150/1000 | Loss: 0.00001264
Iteration 151/1000 | Loss: 0.00001264
Iteration 152/1000 | Loss: 0.00001264
Iteration 153/1000 | Loss: 0.00001264
Iteration 154/1000 | Loss: 0.00001264
Iteration 155/1000 | Loss: 0.00001264
Iteration 156/1000 | Loss: 0.00001264
Iteration 157/1000 | Loss: 0.00001264
Iteration 158/1000 | Loss: 0.00001264
Iteration 159/1000 | Loss: 0.00001264
Iteration 160/1000 | Loss: 0.00001264
Iteration 161/1000 | Loss: 0.00001264
Iteration 162/1000 | Loss: 0.00001264
Iteration 163/1000 | Loss: 0.00001264
Iteration 164/1000 | Loss: 0.00001264
Iteration 165/1000 | Loss: 0.00001263
Iteration 166/1000 | Loss: 0.00001263
Iteration 167/1000 | Loss: 0.00001263
Iteration 168/1000 | Loss: 0.00001263
Iteration 169/1000 | Loss: 0.00001263
Iteration 170/1000 | Loss: 0.00001263
Iteration 171/1000 | Loss: 0.00001263
Iteration 172/1000 | Loss: 0.00001263
Iteration 173/1000 | Loss: 0.00001263
Iteration 174/1000 | Loss: 0.00001263
Iteration 175/1000 | Loss: 0.00001263
Iteration 176/1000 | Loss: 0.00001263
Iteration 177/1000 | Loss: 0.00001263
Iteration 178/1000 | Loss: 0.00001262
Iteration 179/1000 | Loss: 0.00001262
Iteration 180/1000 | Loss: 0.00001262
Iteration 181/1000 | Loss: 0.00001262
Iteration 182/1000 | Loss: 0.00001262
Iteration 183/1000 | Loss: 0.00001262
Iteration 184/1000 | Loss: 0.00001262
Iteration 185/1000 | Loss: 0.00001262
Iteration 186/1000 | Loss: 0.00001262
Iteration 187/1000 | Loss: 0.00001262
Iteration 188/1000 | Loss: 0.00001262
Iteration 189/1000 | Loss: 0.00001262
Iteration 190/1000 | Loss: 0.00001262
Iteration 191/1000 | Loss: 0.00001262
Iteration 192/1000 | Loss: 0.00001262
Iteration 193/1000 | Loss: 0.00001262
Iteration 194/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.261791112483479e-05, 1.261791112483479e-05, 1.261791112483479e-05, 1.261791112483479e-05, 1.261791112483479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.261791112483479e-05

Optimization complete. Final v2v error: 3.0113346576690674 mm

Highest mean error: 3.5671231746673584 mm for frame 82

Lowest mean error: 2.5496084690093994 mm for frame 174

Saving results

Total time: 44.1632936000824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955959
Iteration 2/25 | Loss: 0.00114449
Iteration 3/25 | Loss: 0.00092144
Iteration 4/25 | Loss: 0.00088559
Iteration 5/25 | Loss: 0.00087392
Iteration 6/25 | Loss: 0.00087235
Iteration 7/25 | Loss: 0.00087177
Iteration 8/25 | Loss: 0.00087177
Iteration 9/25 | Loss: 0.00087177
Iteration 10/25 | Loss: 0.00087177
Iteration 11/25 | Loss: 0.00087177
Iteration 12/25 | Loss: 0.00087177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008717700839042664, 0.0008717700839042664, 0.0008717700839042664, 0.0008717700839042664, 0.0008717700839042664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008717700839042664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07242990
Iteration 2/25 | Loss: 0.00049266
Iteration 3/25 | Loss: 0.00049262
Iteration 4/25 | Loss: 0.00049262
Iteration 5/25 | Loss: 0.00049262
Iteration 6/25 | Loss: 0.00049261
Iteration 7/25 | Loss: 0.00049261
Iteration 8/25 | Loss: 0.00049261
Iteration 9/25 | Loss: 0.00049261
Iteration 10/25 | Loss: 0.00049261
Iteration 11/25 | Loss: 0.00049261
Iteration 12/25 | Loss: 0.00049261
Iteration 13/25 | Loss: 0.00049261
Iteration 14/25 | Loss: 0.00049261
Iteration 15/25 | Loss: 0.00049261
Iteration 16/25 | Loss: 0.00049261
Iteration 17/25 | Loss: 0.00049261
Iteration 18/25 | Loss: 0.00049261
Iteration 19/25 | Loss: 0.00049261
Iteration 20/25 | Loss: 0.00049261
Iteration 21/25 | Loss: 0.00049261
Iteration 22/25 | Loss: 0.00049261
Iteration 23/25 | Loss: 0.00049261
Iteration 24/25 | Loss: 0.00049261
Iteration 25/25 | Loss: 0.00049261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049261
Iteration 2/1000 | Loss: 0.00003795
Iteration 3/1000 | Loss: 0.00002813
Iteration 4/1000 | Loss: 0.00002576
Iteration 5/1000 | Loss: 0.00002488
Iteration 6/1000 | Loss: 0.00002420
Iteration 7/1000 | Loss: 0.00002380
Iteration 8/1000 | Loss: 0.00002339
Iteration 9/1000 | Loss: 0.00002309
Iteration 10/1000 | Loss: 0.00002289
Iteration 11/1000 | Loss: 0.00002281
Iteration 12/1000 | Loss: 0.00002279
Iteration 13/1000 | Loss: 0.00002272
Iteration 14/1000 | Loss: 0.00002268
Iteration 15/1000 | Loss: 0.00002267
Iteration 16/1000 | Loss: 0.00002262
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00002253
Iteration 19/1000 | Loss: 0.00002252
Iteration 20/1000 | Loss: 0.00002251
Iteration 21/1000 | Loss: 0.00002250
Iteration 22/1000 | Loss: 0.00002250
Iteration 23/1000 | Loss: 0.00002250
Iteration 24/1000 | Loss: 0.00002249
Iteration 25/1000 | Loss: 0.00002247
Iteration 26/1000 | Loss: 0.00002247
Iteration 27/1000 | Loss: 0.00002247
Iteration 28/1000 | Loss: 0.00002246
Iteration 29/1000 | Loss: 0.00002246
Iteration 30/1000 | Loss: 0.00002246
Iteration 31/1000 | Loss: 0.00002246
Iteration 32/1000 | Loss: 0.00002245
Iteration 33/1000 | Loss: 0.00002245
Iteration 34/1000 | Loss: 0.00002245
Iteration 35/1000 | Loss: 0.00002245
Iteration 36/1000 | Loss: 0.00002245
Iteration 37/1000 | Loss: 0.00002245
Iteration 38/1000 | Loss: 0.00002245
Iteration 39/1000 | Loss: 0.00002245
Iteration 40/1000 | Loss: 0.00002245
Iteration 41/1000 | Loss: 0.00002245
Iteration 42/1000 | Loss: 0.00002244
Iteration 43/1000 | Loss: 0.00002244
Iteration 44/1000 | Loss: 0.00002244
Iteration 45/1000 | Loss: 0.00002244
Iteration 46/1000 | Loss: 0.00002244
Iteration 47/1000 | Loss: 0.00002244
Iteration 48/1000 | Loss: 0.00002244
Iteration 49/1000 | Loss: 0.00002244
Iteration 50/1000 | Loss: 0.00002244
Iteration 51/1000 | Loss: 0.00002244
Iteration 52/1000 | Loss: 0.00002243
Iteration 53/1000 | Loss: 0.00002243
Iteration 54/1000 | Loss: 0.00002242
Iteration 55/1000 | Loss: 0.00002242
Iteration 56/1000 | Loss: 0.00002242
Iteration 57/1000 | Loss: 0.00002241
Iteration 58/1000 | Loss: 0.00002241
Iteration 59/1000 | Loss: 0.00002241
Iteration 60/1000 | Loss: 0.00002241
Iteration 61/1000 | Loss: 0.00002241
Iteration 62/1000 | Loss: 0.00002241
Iteration 63/1000 | Loss: 0.00002241
Iteration 64/1000 | Loss: 0.00002241
Iteration 65/1000 | Loss: 0.00002240
Iteration 66/1000 | Loss: 0.00002240
Iteration 67/1000 | Loss: 0.00002240
Iteration 68/1000 | Loss: 0.00002240
Iteration 69/1000 | Loss: 0.00002240
Iteration 70/1000 | Loss: 0.00002240
Iteration 71/1000 | Loss: 0.00002239
Iteration 72/1000 | Loss: 0.00002239
Iteration 73/1000 | Loss: 0.00002239
Iteration 74/1000 | Loss: 0.00002239
Iteration 75/1000 | Loss: 0.00002238
Iteration 76/1000 | Loss: 0.00002238
Iteration 77/1000 | Loss: 0.00002238
Iteration 78/1000 | Loss: 0.00002238
Iteration 79/1000 | Loss: 0.00002238
Iteration 80/1000 | Loss: 0.00002238
Iteration 81/1000 | Loss: 0.00002238
Iteration 82/1000 | Loss: 0.00002238
Iteration 83/1000 | Loss: 0.00002238
Iteration 84/1000 | Loss: 0.00002237
Iteration 85/1000 | Loss: 0.00002237
Iteration 86/1000 | Loss: 0.00002237
Iteration 87/1000 | Loss: 0.00002237
Iteration 88/1000 | Loss: 0.00002237
Iteration 89/1000 | Loss: 0.00002237
Iteration 90/1000 | Loss: 0.00002237
Iteration 91/1000 | Loss: 0.00002237
Iteration 92/1000 | Loss: 0.00002236
Iteration 93/1000 | Loss: 0.00002236
Iteration 94/1000 | Loss: 0.00002236
Iteration 95/1000 | Loss: 0.00002236
Iteration 96/1000 | Loss: 0.00002235
Iteration 97/1000 | Loss: 0.00002235
Iteration 98/1000 | Loss: 0.00002235
Iteration 99/1000 | Loss: 0.00002235
Iteration 100/1000 | Loss: 0.00002235
Iteration 101/1000 | Loss: 0.00002235
Iteration 102/1000 | Loss: 0.00002235
Iteration 103/1000 | Loss: 0.00002235
Iteration 104/1000 | Loss: 0.00002235
Iteration 105/1000 | Loss: 0.00002235
Iteration 106/1000 | Loss: 0.00002235
Iteration 107/1000 | Loss: 0.00002235
Iteration 108/1000 | Loss: 0.00002234
Iteration 109/1000 | Loss: 0.00002234
Iteration 110/1000 | Loss: 0.00002234
Iteration 111/1000 | Loss: 0.00002234
Iteration 112/1000 | Loss: 0.00002234
Iteration 113/1000 | Loss: 0.00002234
Iteration 114/1000 | Loss: 0.00002233
Iteration 115/1000 | Loss: 0.00002233
Iteration 116/1000 | Loss: 0.00002233
Iteration 117/1000 | Loss: 0.00002233
Iteration 118/1000 | Loss: 0.00002232
Iteration 119/1000 | Loss: 0.00002232
Iteration 120/1000 | Loss: 0.00002232
Iteration 121/1000 | Loss: 0.00002232
Iteration 122/1000 | Loss: 0.00002232
Iteration 123/1000 | Loss: 0.00002232
Iteration 124/1000 | Loss: 0.00002232
Iteration 125/1000 | Loss: 0.00002231
Iteration 126/1000 | Loss: 0.00002231
Iteration 127/1000 | Loss: 0.00002231
Iteration 128/1000 | Loss: 0.00002231
Iteration 129/1000 | Loss: 0.00002231
Iteration 130/1000 | Loss: 0.00002231
Iteration 131/1000 | Loss: 0.00002231
Iteration 132/1000 | Loss: 0.00002231
Iteration 133/1000 | Loss: 0.00002230
Iteration 134/1000 | Loss: 0.00002230
Iteration 135/1000 | Loss: 0.00002230
Iteration 136/1000 | Loss: 0.00002230
Iteration 137/1000 | Loss: 0.00002230
Iteration 138/1000 | Loss: 0.00002229
Iteration 139/1000 | Loss: 0.00002229
Iteration 140/1000 | Loss: 0.00002229
Iteration 141/1000 | Loss: 0.00002229
Iteration 142/1000 | Loss: 0.00002229
Iteration 143/1000 | Loss: 0.00002228
Iteration 144/1000 | Loss: 0.00002228
Iteration 145/1000 | Loss: 0.00002228
Iteration 146/1000 | Loss: 0.00002228
Iteration 147/1000 | Loss: 0.00002228
Iteration 148/1000 | Loss: 0.00002228
Iteration 149/1000 | Loss: 0.00002228
Iteration 150/1000 | Loss: 0.00002228
Iteration 151/1000 | Loss: 0.00002228
Iteration 152/1000 | Loss: 0.00002228
Iteration 153/1000 | Loss: 0.00002227
Iteration 154/1000 | Loss: 0.00002226
Iteration 155/1000 | Loss: 0.00002226
Iteration 156/1000 | Loss: 0.00002226
Iteration 157/1000 | Loss: 0.00002226
Iteration 158/1000 | Loss: 0.00002226
Iteration 159/1000 | Loss: 0.00002226
Iteration 160/1000 | Loss: 0.00002226
Iteration 161/1000 | Loss: 0.00002226
Iteration 162/1000 | Loss: 0.00002226
Iteration 163/1000 | Loss: 0.00002226
Iteration 164/1000 | Loss: 0.00002226
Iteration 165/1000 | Loss: 0.00002225
Iteration 166/1000 | Loss: 0.00002225
Iteration 167/1000 | Loss: 0.00002225
Iteration 168/1000 | Loss: 0.00002225
Iteration 169/1000 | Loss: 0.00002225
Iteration 170/1000 | Loss: 0.00002225
Iteration 171/1000 | Loss: 0.00002225
Iteration 172/1000 | Loss: 0.00002225
Iteration 173/1000 | Loss: 0.00002224
Iteration 174/1000 | Loss: 0.00002224
Iteration 175/1000 | Loss: 0.00002224
Iteration 176/1000 | Loss: 0.00002224
Iteration 177/1000 | Loss: 0.00002224
Iteration 178/1000 | Loss: 0.00002224
Iteration 179/1000 | Loss: 0.00002224
Iteration 180/1000 | Loss: 0.00002224
Iteration 181/1000 | Loss: 0.00002224
Iteration 182/1000 | Loss: 0.00002223
Iteration 183/1000 | Loss: 0.00002223
Iteration 184/1000 | Loss: 0.00002223
Iteration 185/1000 | Loss: 0.00002223
Iteration 186/1000 | Loss: 0.00002223
Iteration 187/1000 | Loss: 0.00002223
Iteration 188/1000 | Loss: 0.00002223
Iteration 189/1000 | Loss: 0.00002223
Iteration 190/1000 | Loss: 0.00002222
Iteration 191/1000 | Loss: 0.00002222
Iteration 192/1000 | Loss: 0.00002222
Iteration 193/1000 | Loss: 0.00002222
Iteration 194/1000 | Loss: 0.00002221
Iteration 195/1000 | Loss: 0.00002221
Iteration 196/1000 | Loss: 0.00002221
Iteration 197/1000 | Loss: 0.00002221
Iteration 198/1000 | Loss: 0.00002221
Iteration 199/1000 | Loss: 0.00002221
Iteration 200/1000 | Loss: 0.00002221
Iteration 201/1000 | Loss: 0.00002220
Iteration 202/1000 | Loss: 0.00002220
Iteration 203/1000 | Loss: 0.00002220
Iteration 204/1000 | Loss: 0.00002220
Iteration 205/1000 | Loss: 0.00002220
Iteration 206/1000 | Loss: 0.00002220
Iteration 207/1000 | Loss: 0.00002220
Iteration 208/1000 | Loss: 0.00002219
Iteration 209/1000 | Loss: 0.00002219
Iteration 210/1000 | Loss: 0.00002219
Iteration 211/1000 | Loss: 0.00002219
Iteration 212/1000 | Loss: 0.00002219
Iteration 213/1000 | Loss: 0.00002219
Iteration 214/1000 | Loss: 0.00002219
Iteration 215/1000 | Loss: 0.00002219
Iteration 216/1000 | Loss: 0.00002219
Iteration 217/1000 | Loss: 0.00002219
Iteration 218/1000 | Loss: 0.00002219
Iteration 219/1000 | Loss: 0.00002219
Iteration 220/1000 | Loss: 0.00002219
Iteration 221/1000 | Loss: 0.00002219
Iteration 222/1000 | Loss: 0.00002219
Iteration 223/1000 | Loss: 0.00002219
Iteration 224/1000 | Loss: 0.00002219
Iteration 225/1000 | Loss: 0.00002219
Iteration 226/1000 | Loss: 0.00002219
Iteration 227/1000 | Loss: 0.00002219
Iteration 228/1000 | Loss: 0.00002219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.219435373262968e-05, 2.219435373262968e-05, 2.219435373262968e-05, 2.219435373262968e-05, 2.219435373262968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.219435373262968e-05

Optimization complete. Final v2v error: 3.85841965675354 mm

Highest mean error: 4.503957271575928 mm for frame 82

Lowest mean error: 3.377898693084717 mm for frame 143

Saving results

Total time: 41.81858730316162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058457
Iteration 2/25 | Loss: 0.00367833
Iteration 3/25 | Loss: 0.00251282
Iteration 4/25 | Loss: 0.00220588
Iteration 5/25 | Loss: 0.00177938
Iteration 6/25 | Loss: 0.00157600
Iteration 7/25 | Loss: 0.00139630
Iteration 8/25 | Loss: 0.00136134
Iteration 9/25 | Loss: 0.00122041
Iteration 10/25 | Loss: 0.00118122
Iteration 11/25 | Loss: 0.00117485
Iteration 12/25 | Loss: 0.00113340
Iteration 13/25 | Loss: 0.00112860
Iteration 14/25 | Loss: 0.00111435
Iteration 15/25 | Loss: 0.00110209
Iteration 16/25 | Loss: 0.00109201
Iteration 17/25 | Loss: 0.00106999
Iteration 18/25 | Loss: 0.00105192
Iteration 19/25 | Loss: 0.00103566
Iteration 20/25 | Loss: 0.00102997
Iteration 21/25 | Loss: 0.00102464
Iteration 22/25 | Loss: 0.00102438
Iteration 23/25 | Loss: 0.00102365
Iteration 24/25 | Loss: 0.00102335
Iteration 25/25 | Loss: 0.00102345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50281179
Iteration 2/25 | Loss: 0.00214359
Iteration 3/25 | Loss: 0.00206332
Iteration 4/25 | Loss: 0.00206332
Iteration 5/25 | Loss: 0.00206332
Iteration 6/25 | Loss: 0.00206332
Iteration 7/25 | Loss: 0.00206332
Iteration 8/25 | Loss: 0.00206332
Iteration 9/25 | Loss: 0.00206332
Iteration 10/25 | Loss: 0.00206332
Iteration 11/25 | Loss: 0.00206332
Iteration 12/25 | Loss: 0.00206332
Iteration 13/25 | Loss: 0.00206332
Iteration 14/25 | Loss: 0.00206332
Iteration 15/25 | Loss: 0.00206332
Iteration 16/25 | Loss: 0.00206332
Iteration 17/25 | Loss: 0.00206332
Iteration 18/25 | Loss: 0.00206332
Iteration 19/25 | Loss: 0.00206332
Iteration 20/25 | Loss: 0.00206332
Iteration 21/25 | Loss: 0.00206332
Iteration 22/25 | Loss: 0.00206332
Iteration 23/25 | Loss: 0.00206332
Iteration 24/25 | Loss: 0.00206332
Iteration 25/25 | Loss: 0.00206332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206332
Iteration 2/1000 | Loss: 0.00478936
Iteration 3/1000 | Loss: 0.00059913
Iteration 4/1000 | Loss: 0.00048978
Iteration 5/1000 | Loss: 0.00042351
Iteration 6/1000 | Loss: 0.00027423
Iteration 7/1000 | Loss: 0.00012814
Iteration 8/1000 | Loss: 0.00026119
Iteration 9/1000 | Loss: 0.00057971
Iteration 10/1000 | Loss: 0.00017217
Iteration 11/1000 | Loss: 0.00009343
Iteration 12/1000 | Loss: 0.00006867
Iteration 13/1000 | Loss: 0.00052250
Iteration 14/1000 | Loss: 0.00042773
Iteration 15/1000 | Loss: 0.00036917
Iteration 16/1000 | Loss: 0.00024707
Iteration 17/1000 | Loss: 0.00030865
Iteration 18/1000 | Loss: 0.00062219
Iteration 19/1000 | Loss: 0.00031432
Iteration 20/1000 | Loss: 0.00036294
Iteration 21/1000 | Loss: 0.00007783
Iteration 22/1000 | Loss: 0.00017377
Iteration 23/1000 | Loss: 0.00005370
Iteration 24/1000 | Loss: 0.00004413
Iteration 25/1000 | Loss: 0.00002639
Iteration 26/1000 | Loss: 0.00002820
Iteration 27/1000 | Loss: 0.00004488
Iteration 28/1000 | Loss: 0.00002333
Iteration 29/1000 | Loss: 0.00010710
Iteration 30/1000 | Loss: 0.00002261
Iteration 31/1000 | Loss: 0.00002334
Iteration 32/1000 | Loss: 0.00002396
Iteration 33/1000 | Loss: 0.00023473
Iteration 34/1000 | Loss: 0.00002682
Iteration 35/1000 | Loss: 0.00002592
Iteration 36/1000 | Loss: 0.00002547
Iteration 37/1000 | Loss: 0.00002311
Iteration 38/1000 | Loss: 0.00002800
Iteration 39/1000 | Loss: 0.00002309
Iteration 40/1000 | Loss: 0.00002272
Iteration 41/1000 | Loss: 0.00002254
Iteration 42/1000 | Loss: 0.00002266
Iteration 43/1000 | Loss: 0.00002491
Iteration 44/1000 | Loss: 0.00002238
Iteration 45/1000 | Loss: 0.00002416
Iteration 46/1000 | Loss: 0.00002185
Iteration 47/1000 | Loss: 0.00002315
Iteration 48/1000 | Loss: 0.00002264
Iteration 49/1000 | Loss: 0.00007609
Iteration 50/1000 | Loss: 0.00002806
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002182
Iteration 53/1000 | Loss: 0.00002194
Iteration 54/1000 | Loss: 0.00002306
Iteration 55/1000 | Loss: 0.00002179
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002178
Iteration 58/1000 | Loss: 0.00004235
Iteration 59/1000 | Loss: 0.00002377
Iteration 60/1000 | Loss: 0.00002281
Iteration 61/1000 | Loss: 0.00002167
Iteration 62/1000 | Loss: 0.00002167
Iteration 63/1000 | Loss: 0.00002167
Iteration 64/1000 | Loss: 0.00002167
Iteration 65/1000 | Loss: 0.00002167
Iteration 66/1000 | Loss: 0.00003544
Iteration 67/1000 | Loss: 0.00002166
Iteration 68/1000 | Loss: 0.00002163
Iteration 69/1000 | Loss: 0.00002163
Iteration 70/1000 | Loss: 0.00002163
Iteration 71/1000 | Loss: 0.00002162
Iteration 72/1000 | Loss: 0.00002162
Iteration 73/1000 | Loss: 0.00002162
Iteration 74/1000 | Loss: 0.00002162
Iteration 75/1000 | Loss: 0.00002162
Iteration 76/1000 | Loss: 0.00002162
Iteration 77/1000 | Loss: 0.00002162
Iteration 78/1000 | Loss: 0.00002162
Iteration 79/1000 | Loss: 0.00002162
Iteration 80/1000 | Loss: 0.00002162
Iteration 81/1000 | Loss: 0.00002161
Iteration 82/1000 | Loss: 0.00002161
Iteration 83/1000 | Loss: 0.00002160
Iteration 84/1000 | Loss: 0.00002160
Iteration 85/1000 | Loss: 0.00002160
Iteration 86/1000 | Loss: 0.00002160
Iteration 87/1000 | Loss: 0.00002160
Iteration 88/1000 | Loss: 0.00002311
Iteration 89/1000 | Loss: 0.00002215
Iteration 90/1000 | Loss: 0.00002157
Iteration 91/1000 | Loss: 0.00002157
Iteration 92/1000 | Loss: 0.00002157
Iteration 93/1000 | Loss: 0.00002157
Iteration 94/1000 | Loss: 0.00002157
Iteration 95/1000 | Loss: 0.00002157
Iteration 96/1000 | Loss: 0.00002165
Iteration 97/1000 | Loss: 0.00002167
Iteration 98/1000 | Loss: 0.00002156
Iteration 99/1000 | Loss: 0.00002155
Iteration 100/1000 | Loss: 0.00002155
Iteration 101/1000 | Loss: 0.00002155
Iteration 102/1000 | Loss: 0.00002155
Iteration 103/1000 | Loss: 0.00002154
Iteration 104/1000 | Loss: 0.00002154
Iteration 105/1000 | Loss: 0.00002154
Iteration 106/1000 | Loss: 0.00002154
Iteration 107/1000 | Loss: 0.00002153
Iteration 108/1000 | Loss: 0.00002153
Iteration 109/1000 | Loss: 0.00002153
Iteration 110/1000 | Loss: 0.00002153
Iteration 111/1000 | Loss: 0.00002152
Iteration 112/1000 | Loss: 0.00002152
Iteration 113/1000 | Loss: 0.00002152
Iteration 114/1000 | Loss: 0.00002151
Iteration 115/1000 | Loss: 0.00002151
Iteration 116/1000 | Loss: 0.00011782
Iteration 117/1000 | Loss: 0.00002309
Iteration 118/1000 | Loss: 0.00002095
Iteration 119/1000 | Loss: 0.00002791
Iteration 120/1000 | Loss: 0.00010972
Iteration 121/1000 | Loss: 0.00007934
Iteration 122/1000 | Loss: 0.00002468
Iteration 123/1000 | Loss: 0.00002036
Iteration 124/1000 | Loss: 0.00002044
Iteration 125/1000 | Loss: 0.00002215
Iteration 126/1000 | Loss: 0.00002070
Iteration 127/1000 | Loss: 0.00001965
Iteration 128/1000 | Loss: 0.00001965
Iteration 129/1000 | Loss: 0.00001965
Iteration 130/1000 | Loss: 0.00001964
Iteration 131/1000 | Loss: 0.00001964
Iteration 132/1000 | Loss: 0.00001964
Iteration 133/1000 | Loss: 0.00001964
Iteration 134/1000 | Loss: 0.00001964
Iteration 135/1000 | Loss: 0.00001964
Iteration 136/1000 | Loss: 0.00001964
Iteration 137/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.9642699044197798e-05, 1.9642699044197798e-05, 1.9642699044197798e-05, 1.9642699044197798e-05, 1.9642699044197798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9642699044197798e-05

Optimization complete. Final v2v error: 3.699336528778076 mm

Highest mean error: 6.229457855224609 mm for frame 90

Lowest mean error: 3.318815231323242 mm for frame 220

Saving results

Total time: 159.03368973731995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880717
Iteration 2/25 | Loss: 0.00174014
Iteration 3/25 | Loss: 0.00097989
Iteration 4/25 | Loss: 0.00089011
Iteration 5/25 | Loss: 0.00083069
Iteration 6/25 | Loss: 0.00083588
Iteration 7/25 | Loss: 0.00080838
Iteration 8/25 | Loss: 0.00077391
Iteration 9/25 | Loss: 0.00075436
Iteration 10/25 | Loss: 0.00074572
Iteration 11/25 | Loss: 0.00074315
Iteration 12/25 | Loss: 0.00074136
Iteration 13/25 | Loss: 0.00074042
Iteration 14/25 | Loss: 0.00074005
Iteration 15/25 | Loss: 0.00073991
Iteration 16/25 | Loss: 0.00073989
Iteration 17/25 | Loss: 0.00073989
Iteration 18/25 | Loss: 0.00073989
Iteration 19/25 | Loss: 0.00073988
Iteration 20/25 | Loss: 0.00073988
Iteration 21/25 | Loss: 0.00073988
Iteration 22/25 | Loss: 0.00073988
Iteration 23/25 | Loss: 0.00073988
Iteration 24/25 | Loss: 0.00073988
Iteration 25/25 | Loss: 0.00073988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.02158451
Iteration 2/25 | Loss: 0.00053979
Iteration 3/25 | Loss: 0.00053975
Iteration 4/25 | Loss: 0.00053975
Iteration 5/25 | Loss: 0.00053975
Iteration 6/25 | Loss: 0.00053975
Iteration 7/25 | Loss: 0.00053975
Iteration 8/25 | Loss: 0.00053975
Iteration 9/25 | Loss: 0.00053975
Iteration 10/25 | Loss: 0.00053975
Iteration 11/25 | Loss: 0.00053975
Iteration 12/25 | Loss: 0.00053975
Iteration 13/25 | Loss: 0.00053975
Iteration 14/25 | Loss: 0.00053975
Iteration 15/25 | Loss: 0.00053975
Iteration 16/25 | Loss: 0.00053975
Iteration 17/25 | Loss: 0.00053975
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005397453787736595, 0.0005397453787736595, 0.0005397453787736595, 0.0005397453787736595, 0.0005397453787736595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005397453787736595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053975
Iteration 2/1000 | Loss: 0.00002624
Iteration 3/1000 | Loss: 0.00007235
Iteration 4/1000 | Loss: 0.00033960
Iteration 5/1000 | Loss: 0.00003974
Iteration 6/1000 | Loss: 0.00006228
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00002118
Iteration 10/1000 | Loss: 0.00001286
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001731
Iteration 13/1000 | Loss: 0.00001278
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00003795
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001462
Iteration 21/1000 | Loss: 0.00001242
Iteration 22/1000 | Loss: 0.00003595
Iteration 23/1000 | Loss: 0.00007797
Iteration 24/1000 | Loss: 0.00002871
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001232
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001228
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001225
Iteration 54/1000 | Loss: 0.00001225
Iteration 55/1000 | Loss: 0.00001225
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001218
Iteration 73/1000 | Loss: 0.00001218
Iteration 74/1000 | Loss: 0.00001218
Iteration 75/1000 | Loss: 0.00001217
Iteration 76/1000 | Loss: 0.00001217
Iteration 77/1000 | Loss: 0.00001217
Iteration 78/1000 | Loss: 0.00001217
Iteration 79/1000 | Loss: 0.00001217
Iteration 80/1000 | Loss: 0.00001216
Iteration 81/1000 | Loss: 0.00001216
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001215
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001213
Iteration 91/1000 | Loss: 0.00001213
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00004195
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001286
Iteration 96/1000 | Loss: 0.00001237
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001211
Iteration 104/1000 | Loss: 0.00001211
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001210
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001209
Iteration 114/1000 | Loss: 0.00001209
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001284
Iteration 118/1000 | Loss: 0.00001252
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001208
Iteration 122/1000 | Loss: 0.00001208
Iteration 123/1000 | Loss: 0.00001208
Iteration 124/1000 | Loss: 0.00001208
Iteration 125/1000 | Loss: 0.00001208
Iteration 126/1000 | Loss: 0.00001208
Iteration 127/1000 | Loss: 0.00001208
Iteration 128/1000 | Loss: 0.00001208
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001208
Iteration 132/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.2082732609997038e-05, 1.2082732609997038e-05, 1.2082732609997038e-05, 1.2082732609997038e-05, 1.2082732609997038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2082732609997038e-05

Optimization complete. Final v2v error: 2.948746681213379 mm

Highest mean error: 3.781432867050171 mm for frame 87

Lowest mean error: 2.5545499324798584 mm for frame 25

Saving results

Total time: 74.90091967582703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911890
Iteration 2/25 | Loss: 0.00104754
Iteration 3/25 | Loss: 0.00089690
Iteration 4/25 | Loss: 0.00087180
Iteration 5/25 | Loss: 0.00086561
Iteration 6/25 | Loss: 0.00086336
Iteration 7/25 | Loss: 0.00086257
Iteration 8/25 | Loss: 0.00086257
Iteration 9/25 | Loss: 0.00086257
Iteration 10/25 | Loss: 0.00086257
Iteration 11/25 | Loss: 0.00086257
Iteration 12/25 | Loss: 0.00086257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008625657064840198, 0.0008625657064840198, 0.0008625657064840198, 0.0008625657064840198, 0.0008625657064840198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008625657064840198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63119006
Iteration 2/25 | Loss: 0.00158635
Iteration 3/25 | Loss: 0.00158634
Iteration 4/25 | Loss: 0.00158634
Iteration 5/25 | Loss: 0.00158634
Iteration 6/25 | Loss: 0.00158634
Iteration 7/25 | Loss: 0.00158634
Iteration 8/25 | Loss: 0.00158634
Iteration 9/25 | Loss: 0.00158634
Iteration 10/25 | Loss: 0.00158634
Iteration 11/25 | Loss: 0.00158634
Iteration 12/25 | Loss: 0.00158634
Iteration 13/25 | Loss: 0.00158634
Iteration 14/25 | Loss: 0.00158634
Iteration 15/25 | Loss: 0.00158634
Iteration 16/25 | Loss: 0.00158634
Iteration 17/25 | Loss: 0.00158634
Iteration 18/25 | Loss: 0.00158634
Iteration 19/25 | Loss: 0.00158634
Iteration 20/25 | Loss: 0.00158634
Iteration 21/25 | Loss: 0.00158634
Iteration 22/25 | Loss: 0.00158634
Iteration 23/25 | Loss: 0.00158634
Iteration 24/25 | Loss: 0.00158634
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015863393200561404, 0.0015863393200561404, 0.0015863393200561404, 0.0015863393200561404, 0.0015863393200561404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015863393200561404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158634
Iteration 2/1000 | Loss: 0.00002715
Iteration 3/1000 | Loss: 0.00002016
Iteration 4/1000 | Loss: 0.00001839
Iteration 5/1000 | Loss: 0.00001740
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001632
Iteration 9/1000 | Loss: 0.00001622
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001614
Iteration 14/1000 | Loss: 0.00001613
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00001611
Iteration 18/1000 | Loss: 0.00001606
Iteration 19/1000 | Loss: 0.00001602
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001596
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001595
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001593
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001592
Iteration 29/1000 | Loss: 0.00001591
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001590
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00001589
Iteration 34/1000 | Loss: 0.00001589
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001586
Iteration 39/1000 | Loss: 0.00001586
Iteration 40/1000 | Loss: 0.00001586
Iteration 41/1000 | Loss: 0.00001585
Iteration 42/1000 | Loss: 0.00001584
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001584
Iteration 45/1000 | Loss: 0.00001583
Iteration 46/1000 | Loss: 0.00001583
Iteration 47/1000 | Loss: 0.00001583
Iteration 48/1000 | Loss: 0.00001581
Iteration 49/1000 | Loss: 0.00001581
Iteration 50/1000 | Loss: 0.00001580
Iteration 51/1000 | Loss: 0.00001580
Iteration 52/1000 | Loss: 0.00001580
Iteration 53/1000 | Loss: 0.00001579
Iteration 54/1000 | Loss: 0.00001579
Iteration 55/1000 | Loss: 0.00001579
Iteration 56/1000 | Loss: 0.00001578
Iteration 57/1000 | Loss: 0.00001578
Iteration 58/1000 | Loss: 0.00001577
Iteration 59/1000 | Loss: 0.00001577
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001577
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001576
Iteration 68/1000 | Loss: 0.00001576
Iteration 69/1000 | Loss: 0.00001576
Iteration 70/1000 | Loss: 0.00001576
Iteration 71/1000 | Loss: 0.00001576
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001575
Iteration 76/1000 | Loss: 0.00001575
Iteration 77/1000 | Loss: 0.00001575
Iteration 78/1000 | Loss: 0.00001575
Iteration 79/1000 | Loss: 0.00001575
Iteration 80/1000 | Loss: 0.00001575
Iteration 81/1000 | Loss: 0.00001575
Iteration 82/1000 | Loss: 0.00001575
Iteration 83/1000 | Loss: 0.00001574
Iteration 84/1000 | Loss: 0.00001574
Iteration 85/1000 | Loss: 0.00001574
Iteration 86/1000 | Loss: 0.00001574
Iteration 87/1000 | Loss: 0.00001573
Iteration 88/1000 | Loss: 0.00001573
Iteration 89/1000 | Loss: 0.00001573
Iteration 90/1000 | Loss: 0.00001573
Iteration 91/1000 | Loss: 0.00001573
Iteration 92/1000 | Loss: 0.00001573
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001571
Iteration 99/1000 | Loss: 0.00001571
Iteration 100/1000 | Loss: 0.00001571
Iteration 101/1000 | Loss: 0.00001570
Iteration 102/1000 | Loss: 0.00001570
Iteration 103/1000 | Loss: 0.00001570
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001570
Iteration 106/1000 | Loss: 0.00001570
Iteration 107/1000 | Loss: 0.00001570
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001569
Iteration 110/1000 | Loss: 0.00001569
Iteration 111/1000 | Loss: 0.00001569
Iteration 112/1000 | Loss: 0.00001569
Iteration 113/1000 | Loss: 0.00001568
Iteration 114/1000 | Loss: 0.00001568
Iteration 115/1000 | Loss: 0.00001568
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001568
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001568
Iteration 124/1000 | Loss: 0.00001568
Iteration 125/1000 | Loss: 0.00001568
Iteration 126/1000 | Loss: 0.00001568
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001567
Iteration 131/1000 | Loss: 0.00001567
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001567
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001567
Iteration 137/1000 | Loss: 0.00001567
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001567
Iteration 141/1000 | Loss: 0.00001567
Iteration 142/1000 | Loss: 0.00001567
Iteration 143/1000 | Loss: 0.00001567
Iteration 144/1000 | Loss: 0.00001567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.56659498316003e-05, 1.56659498316003e-05, 1.56659498316003e-05, 1.56659498316003e-05, 1.56659498316003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.56659498316003e-05

Optimization complete. Final v2v error: 3.440037250518799 mm

Highest mean error: 3.8406331539154053 mm for frame 37

Lowest mean error: 3.087279796600342 mm for frame 1

Saving results

Total time: 39.21036100387573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532763
Iteration 2/25 | Loss: 0.00141596
Iteration 3/25 | Loss: 0.00101950
Iteration 4/25 | Loss: 0.00095022
Iteration 5/25 | Loss: 0.00093770
Iteration 6/25 | Loss: 0.00093444
Iteration 7/25 | Loss: 0.00093351
Iteration 8/25 | Loss: 0.00093322
Iteration 9/25 | Loss: 0.00093322
Iteration 10/25 | Loss: 0.00093322
Iteration 11/25 | Loss: 0.00093322
Iteration 12/25 | Loss: 0.00093322
Iteration 13/25 | Loss: 0.00093322
Iteration 14/25 | Loss: 0.00093322
Iteration 15/25 | Loss: 0.00093322
Iteration 16/25 | Loss: 0.00093322
Iteration 17/25 | Loss: 0.00093322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009332243353128433, 0.0009332243353128433, 0.0009332243353128433, 0.0009332243353128433, 0.0009332243353128433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009332243353128433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69647837
Iteration 2/25 | Loss: 0.00174145
Iteration 3/25 | Loss: 0.00174145
Iteration 4/25 | Loss: 0.00174145
Iteration 5/25 | Loss: 0.00174145
Iteration 6/25 | Loss: 0.00174145
Iteration 7/25 | Loss: 0.00174145
Iteration 8/25 | Loss: 0.00174145
Iteration 9/25 | Loss: 0.00174145
Iteration 10/25 | Loss: 0.00174145
Iteration 11/25 | Loss: 0.00174144
Iteration 12/25 | Loss: 0.00174144
Iteration 13/25 | Loss: 0.00174144
Iteration 14/25 | Loss: 0.00174144
Iteration 15/25 | Loss: 0.00174144
Iteration 16/25 | Loss: 0.00174144
Iteration 17/25 | Loss: 0.00174145
Iteration 18/25 | Loss: 0.00174145
Iteration 19/25 | Loss: 0.00174144
Iteration 20/25 | Loss: 0.00174144
Iteration 21/25 | Loss: 0.00174144
Iteration 22/25 | Loss: 0.00174144
Iteration 23/25 | Loss: 0.00174144
Iteration 24/25 | Loss: 0.00174145
Iteration 25/25 | Loss: 0.00174145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174145
Iteration 2/1000 | Loss: 0.00004113
Iteration 3/1000 | Loss: 0.00002764
Iteration 4/1000 | Loss: 0.00002289
Iteration 5/1000 | Loss: 0.00002048
Iteration 6/1000 | Loss: 0.00001933
Iteration 7/1000 | Loss: 0.00001873
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001797
Iteration 10/1000 | Loss: 0.00001776
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001774
Iteration 13/1000 | Loss: 0.00001769
Iteration 14/1000 | Loss: 0.00001767
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001751
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001745
Iteration 20/1000 | Loss: 0.00001744
Iteration 21/1000 | Loss: 0.00001743
Iteration 22/1000 | Loss: 0.00001743
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001741
Iteration 25/1000 | Loss: 0.00001739
Iteration 26/1000 | Loss: 0.00001737
Iteration 27/1000 | Loss: 0.00001737
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001736
Iteration 32/1000 | Loss: 0.00001736
Iteration 33/1000 | Loss: 0.00001733
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001732
Iteration 36/1000 | Loss: 0.00001732
Iteration 37/1000 | Loss: 0.00001730
Iteration 38/1000 | Loss: 0.00001730
Iteration 39/1000 | Loss: 0.00001730
Iteration 40/1000 | Loss: 0.00001730
Iteration 41/1000 | Loss: 0.00001729
Iteration 42/1000 | Loss: 0.00001729
Iteration 43/1000 | Loss: 0.00001729
Iteration 44/1000 | Loss: 0.00001729
Iteration 45/1000 | Loss: 0.00001728
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001728
Iteration 48/1000 | Loss: 0.00001728
Iteration 49/1000 | Loss: 0.00001727
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001725
Iteration 56/1000 | Loss: 0.00001725
Iteration 57/1000 | Loss: 0.00001724
Iteration 58/1000 | Loss: 0.00001724
Iteration 59/1000 | Loss: 0.00001723
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001722
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001718
Iteration 81/1000 | Loss: 0.00001718
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00001718
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001717
Iteration 87/1000 | Loss: 0.00001717
Iteration 88/1000 | Loss: 0.00001717
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001717
Iteration 92/1000 | Loss: 0.00001717
Iteration 93/1000 | Loss: 0.00001717
Iteration 94/1000 | Loss: 0.00001717
Iteration 95/1000 | Loss: 0.00001717
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001717
Iteration 109/1000 | Loss: 0.00001717
Iteration 110/1000 | Loss: 0.00001717
Iteration 111/1000 | Loss: 0.00001717
Iteration 112/1000 | Loss: 0.00001717
Iteration 113/1000 | Loss: 0.00001717
Iteration 114/1000 | Loss: 0.00001717
Iteration 115/1000 | Loss: 0.00001717
Iteration 116/1000 | Loss: 0.00001717
Iteration 117/1000 | Loss: 0.00001717
Iteration 118/1000 | Loss: 0.00001717
Iteration 119/1000 | Loss: 0.00001717
Iteration 120/1000 | Loss: 0.00001717
Iteration 121/1000 | Loss: 0.00001717
Iteration 122/1000 | Loss: 0.00001717
Iteration 123/1000 | Loss: 0.00001717
Iteration 124/1000 | Loss: 0.00001717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.71668052644236e-05, 1.71668052644236e-05, 1.71668052644236e-05, 1.71668052644236e-05, 1.71668052644236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.71668052644236e-05

Optimization complete. Final v2v error: 3.481215476989746 mm

Highest mean error: 4.559200286865234 mm for frame 75

Lowest mean error: 3.064966917037964 mm for frame 1

Saving results

Total time: 35.354005336761475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873520
Iteration 2/25 | Loss: 0.00108789
Iteration 3/25 | Loss: 0.00089554
Iteration 4/25 | Loss: 0.00086684
Iteration 5/25 | Loss: 0.00086044
Iteration 6/25 | Loss: 0.00085871
Iteration 7/25 | Loss: 0.00085817
Iteration 8/25 | Loss: 0.00085817
Iteration 9/25 | Loss: 0.00085817
Iteration 10/25 | Loss: 0.00085817
Iteration 11/25 | Loss: 0.00085817
Iteration 12/25 | Loss: 0.00085817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008581719594076276, 0.0008581719594076276, 0.0008581719594076276, 0.0008581719594076276, 0.0008581719594076276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008581719594076276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64631915
Iteration 2/25 | Loss: 0.00177836
Iteration 3/25 | Loss: 0.00177836
Iteration 4/25 | Loss: 0.00177836
Iteration 5/25 | Loss: 0.00177836
Iteration 6/25 | Loss: 0.00177836
Iteration 7/25 | Loss: 0.00177836
Iteration 8/25 | Loss: 0.00177836
Iteration 9/25 | Loss: 0.00177836
Iteration 10/25 | Loss: 0.00177836
Iteration 11/25 | Loss: 0.00177836
Iteration 12/25 | Loss: 0.00177836
Iteration 13/25 | Loss: 0.00177836
Iteration 14/25 | Loss: 0.00177836
Iteration 15/25 | Loss: 0.00177836
Iteration 16/25 | Loss: 0.00177836
Iteration 17/25 | Loss: 0.00177836
Iteration 18/25 | Loss: 0.00177836
Iteration 19/25 | Loss: 0.00177836
Iteration 20/25 | Loss: 0.00177836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017783577786758542, 0.0017783577786758542, 0.0017783577786758542, 0.0017783577786758542, 0.0017783577786758542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017783577786758542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177836
Iteration 2/1000 | Loss: 0.00003917
Iteration 3/1000 | Loss: 0.00002761
Iteration 4/1000 | Loss: 0.00002196
Iteration 5/1000 | Loss: 0.00002017
Iteration 6/1000 | Loss: 0.00001920
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001838
Iteration 9/1000 | Loss: 0.00001805
Iteration 10/1000 | Loss: 0.00001778
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001759
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001738
Iteration 18/1000 | Loss: 0.00001735
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001728
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001727
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001724
Iteration 31/1000 | Loss: 0.00001723
Iteration 32/1000 | Loss: 0.00001722
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001722
Iteration 35/1000 | Loss: 0.00001722
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001721
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001720
Iteration 57/1000 | Loss: 0.00001720
Iteration 58/1000 | Loss: 0.00001720
Iteration 59/1000 | Loss: 0.00001720
Iteration 60/1000 | Loss: 0.00001720
Iteration 61/1000 | Loss: 0.00001719
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001719
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001718
Iteration 72/1000 | Loss: 0.00001718
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001717
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001717
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001716
Iteration 79/1000 | Loss: 0.00001716
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001714
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001713
Iteration 89/1000 | Loss: 0.00001713
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001712
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001712
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001711
Iteration 99/1000 | Loss: 0.00001711
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001710
Iteration 103/1000 | Loss: 0.00001710
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001709
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001707
Iteration 120/1000 | Loss: 0.00001707
Iteration 121/1000 | Loss: 0.00001707
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001706
Iteration 127/1000 | Loss: 0.00001706
Iteration 128/1000 | Loss: 0.00001706
Iteration 129/1000 | Loss: 0.00001706
Iteration 130/1000 | Loss: 0.00001706
Iteration 131/1000 | Loss: 0.00001706
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.7062540791812353e-05, 1.7062540791812353e-05, 1.7062540791812353e-05, 1.7062540791812353e-05, 1.7062540791812353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7062540791812353e-05

Optimization complete. Final v2v error: 3.5945167541503906 mm

Highest mean error: 3.920724630355835 mm for frame 91

Lowest mean error: 3.1986546516418457 mm for frame 18

Saving results

Total time: 36.81470823287964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805452
Iteration 2/25 | Loss: 0.00164320
Iteration 3/25 | Loss: 0.00136215
Iteration 4/25 | Loss: 0.00138193
Iteration 5/25 | Loss: 0.00126660
Iteration 6/25 | Loss: 0.00118595
Iteration 7/25 | Loss: 0.00113395
Iteration 8/25 | Loss: 0.00111690
Iteration 9/25 | Loss: 0.00111488
Iteration 10/25 | Loss: 0.00121290
Iteration 11/25 | Loss: 0.00109546
Iteration 12/25 | Loss: 0.00105275
Iteration 13/25 | Loss: 0.00104947
Iteration 14/25 | Loss: 0.00104922
Iteration 15/25 | Loss: 0.00104917
Iteration 16/25 | Loss: 0.00104917
Iteration 17/25 | Loss: 0.00104914
Iteration 18/25 | Loss: 0.00104914
Iteration 19/25 | Loss: 0.00104914
Iteration 20/25 | Loss: 0.00104914
Iteration 21/25 | Loss: 0.00104914
Iteration 22/25 | Loss: 0.00104914
Iteration 23/25 | Loss: 0.00104914
Iteration 24/25 | Loss: 0.00104914
Iteration 25/25 | Loss: 0.00104914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73386681
Iteration 2/25 | Loss: 0.00233906
Iteration 3/25 | Loss: 0.00233905
Iteration 4/25 | Loss: 0.00233905
Iteration 5/25 | Loss: 0.00233905
Iteration 6/25 | Loss: 0.00233905
Iteration 7/25 | Loss: 0.00233905
Iteration 8/25 | Loss: 0.00233905
Iteration 9/25 | Loss: 0.00233905
Iteration 10/25 | Loss: 0.00233905
Iteration 11/25 | Loss: 0.00233905
Iteration 12/25 | Loss: 0.00233905
Iteration 13/25 | Loss: 0.00233905
Iteration 14/25 | Loss: 0.00233905
Iteration 15/25 | Loss: 0.00233905
Iteration 16/25 | Loss: 0.00233905
Iteration 17/25 | Loss: 0.00233905
Iteration 18/25 | Loss: 0.00233905
Iteration 19/25 | Loss: 0.00233905
Iteration 20/25 | Loss: 0.00233905
Iteration 21/25 | Loss: 0.00233905
Iteration 22/25 | Loss: 0.00233905
Iteration 23/25 | Loss: 0.00233905
Iteration 24/25 | Loss: 0.00233905
Iteration 25/25 | Loss: 0.00233905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233905
Iteration 2/1000 | Loss: 0.00009808
Iteration 3/1000 | Loss: 0.00006183
Iteration 4/1000 | Loss: 0.00004738
Iteration 5/1000 | Loss: 0.00004237
Iteration 6/1000 | Loss: 0.00004051
Iteration 7/1000 | Loss: 0.00003880
Iteration 8/1000 | Loss: 0.00003804
Iteration 9/1000 | Loss: 0.00003721
Iteration 10/1000 | Loss: 0.00003655
Iteration 11/1000 | Loss: 0.00003612
Iteration 12/1000 | Loss: 0.00003579
Iteration 13/1000 | Loss: 0.00003557
Iteration 14/1000 | Loss: 0.00003542
Iteration 15/1000 | Loss: 0.00003530
Iteration 16/1000 | Loss: 0.00003529
Iteration 17/1000 | Loss: 0.00003527
Iteration 18/1000 | Loss: 0.00003526
Iteration 19/1000 | Loss: 0.00003523
Iteration 20/1000 | Loss: 0.00003514
Iteration 21/1000 | Loss: 0.00003508
Iteration 22/1000 | Loss: 0.00003508
Iteration 23/1000 | Loss: 0.00003506
Iteration 24/1000 | Loss: 0.00003506
Iteration 25/1000 | Loss: 0.00003505
Iteration 26/1000 | Loss: 0.00003505
Iteration 27/1000 | Loss: 0.00003505
Iteration 28/1000 | Loss: 0.00003505
Iteration 29/1000 | Loss: 0.00003504
Iteration 30/1000 | Loss: 0.00003504
Iteration 31/1000 | Loss: 0.00003503
Iteration 32/1000 | Loss: 0.00003503
Iteration 33/1000 | Loss: 0.00003503
Iteration 34/1000 | Loss: 0.00003502
Iteration 35/1000 | Loss: 0.00003502
Iteration 36/1000 | Loss: 0.00003502
Iteration 37/1000 | Loss: 0.00003502
Iteration 38/1000 | Loss: 0.00003502
Iteration 39/1000 | Loss: 0.00003502
Iteration 40/1000 | Loss: 0.00003502
Iteration 41/1000 | Loss: 0.00003501
Iteration 42/1000 | Loss: 0.00003501
Iteration 43/1000 | Loss: 0.00003501
Iteration 44/1000 | Loss: 0.00003501
Iteration 45/1000 | Loss: 0.00003501
Iteration 46/1000 | Loss: 0.00003500
Iteration 47/1000 | Loss: 0.00003500
Iteration 48/1000 | Loss: 0.00003500
Iteration 49/1000 | Loss: 0.00003500
Iteration 50/1000 | Loss: 0.00003500
Iteration 51/1000 | Loss: 0.00003500
Iteration 52/1000 | Loss: 0.00003499
Iteration 53/1000 | Loss: 0.00003499
Iteration 54/1000 | Loss: 0.00003499
Iteration 55/1000 | Loss: 0.00003499
Iteration 56/1000 | Loss: 0.00003499
Iteration 57/1000 | Loss: 0.00003498
Iteration 58/1000 | Loss: 0.00003498
Iteration 59/1000 | Loss: 0.00003498
Iteration 60/1000 | Loss: 0.00003498
Iteration 61/1000 | Loss: 0.00003498
Iteration 62/1000 | Loss: 0.00003498
Iteration 63/1000 | Loss: 0.00003497
Iteration 64/1000 | Loss: 0.00003497
Iteration 65/1000 | Loss: 0.00003496
Iteration 66/1000 | Loss: 0.00003496
Iteration 67/1000 | Loss: 0.00003496
Iteration 68/1000 | Loss: 0.00003496
Iteration 69/1000 | Loss: 0.00003495
Iteration 70/1000 | Loss: 0.00003495
Iteration 71/1000 | Loss: 0.00003495
Iteration 72/1000 | Loss: 0.00003495
Iteration 73/1000 | Loss: 0.00003495
Iteration 74/1000 | Loss: 0.00003494
Iteration 75/1000 | Loss: 0.00003494
Iteration 76/1000 | Loss: 0.00003494
Iteration 77/1000 | Loss: 0.00003494
Iteration 78/1000 | Loss: 0.00003493
Iteration 79/1000 | Loss: 0.00003493
Iteration 80/1000 | Loss: 0.00003493
Iteration 81/1000 | Loss: 0.00003493
Iteration 82/1000 | Loss: 0.00003492
Iteration 83/1000 | Loss: 0.00003492
Iteration 84/1000 | Loss: 0.00003492
Iteration 85/1000 | Loss: 0.00003492
Iteration 86/1000 | Loss: 0.00003491
Iteration 87/1000 | Loss: 0.00003491
Iteration 88/1000 | Loss: 0.00003491
Iteration 89/1000 | Loss: 0.00003491
Iteration 90/1000 | Loss: 0.00003490
Iteration 91/1000 | Loss: 0.00003490
Iteration 92/1000 | Loss: 0.00003490
Iteration 93/1000 | Loss: 0.00003490
Iteration 94/1000 | Loss: 0.00003490
Iteration 95/1000 | Loss: 0.00003490
Iteration 96/1000 | Loss: 0.00003490
Iteration 97/1000 | Loss: 0.00003490
Iteration 98/1000 | Loss: 0.00003490
Iteration 99/1000 | Loss: 0.00003490
Iteration 100/1000 | Loss: 0.00003489
Iteration 101/1000 | Loss: 0.00003489
Iteration 102/1000 | Loss: 0.00003489
Iteration 103/1000 | Loss: 0.00003489
Iteration 104/1000 | Loss: 0.00003489
Iteration 105/1000 | Loss: 0.00003488
Iteration 106/1000 | Loss: 0.00003488
Iteration 107/1000 | Loss: 0.00003488
Iteration 108/1000 | Loss: 0.00003488
Iteration 109/1000 | Loss: 0.00003488
Iteration 110/1000 | Loss: 0.00003488
Iteration 111/1000 | Loss: 0.00003487
Iteration 112/1000 | Loss: 0.00003487
Iteration 113/1000 | Loss: 0.00003487
Iteration 114/1000 | Loss: 0.00003487
Iteration 115/1000 | Loss: 0.00003487
Iteration 116/1000 | Loss: 0.00003487
Iteration 117/1000 | Loss: 0.00003487
Iteration 118/1000 | Loss: 0.00003487
Iteration 119/1000 | Loss: 0.00003486
Iteration 120/1000 | Loss: 0.00003486
Iteration 121/1000 | Loss: 0.00003486
Iteration 122/1000 | Loss: 0.00003486
Iteration 123/1000 | Loss: 0.00003486
Iteration 124/1000 | Loss: 0.00003486
Iteration 125/1000 | Loss: 0.00003486
Iteration 126/1000 | Loss: 0.00003486
Iteration 127/1000 | Loss: 0.00003486
Iteration 128/1000 | Loss: 0.00003486
Iteration 129/1000 | Loss: 0.00003486
Iteration 130/1000 | Loss: 0.00003486
Iteration 131/1000 | Loss: 0.00003486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.486185596557334e-05, 3.486185596557334e-05, 3.486185596557334e-05, 3.486185596557334e-05, 3.486185596557334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.486185596557334e-05

Optimization complete. Final v2v error: 4.966390132904053 mm

Highest mean error: 5.613671779632568 mm for frame 89

Lowest mean error: 4.293711185455322 mm for frame 0

Saving results

Total time: 56.051578760147095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581139
Iteration 2/25 | Loss: 0.00101270
Iteration 3/25 | Loss: 0.00092822
Iteration 4/25 | Loss: 0.00091124
Iteration 5/25 | Loss: 0.00090716
Iteration 6/25 | Loss: 0.00090566
Iteration 7/25 | Loss: 0.00090560
Iteration 8/25 | Loss: 0.00090560
Iteration 9/25 | Loss: 0.00090560
Iteration 10/25 | Loss: 0.00090560
Iteration 11/25 | Loss: 0.00090560
Iteration 12/25 | Loss: 0.00090560
Iteration 13/25 | Loss: 0.00090560
Iteration 14/25 | Loss: 0.00090560
Iteration 15/25 | Loss: 0.00090560
Iteration 16/25 | Loss: 0.00090560
Iteration 17/25 | Loss: 0.00090560
Iteration 18/25 | Loss: 0.00090560
Iteration 19/25 | Loss: 0.00090560
Iteration 20/25 | Loss: 0.00090560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009055971750058234, 0.0009055971750058234, 0.0009055971750058234, 0.0009055971750058234, 0.0009055971750058234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009055971750058234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.43479252
Iteration 2/25 | Loss: 0.00162614
Iteration 3/25 | Loss: 0.00162614
Iteration 4/25 | Loss: 0.00162614
Iteration 5/25 | Loss: 0.00162614
Iteration 6/25 | Loss: 0.00162614
Iteration 7/25 | Loss: 0.00162614
Iteration 8/25 | Loss: 0.00162614
Iteration 9/25 | Loss: 0.00162614
Iteration 10/25 | Loss: 0.00162614
Iteration 11/25 | Loss: 0.00162614
Iteration 12/25 | Loss: 0.00162614
Iteration 13/25 | Loss: 0.00162614
Iteration 14/25 | Loss: 0.00162614
Iteration 15/25 | Loss: 0.00162614
Iteration 16/25 | Loss: 0.00162614
Iteration 17/25 | Loss: 0.00162614
Iteration 18/25 | Loss: 0.00162614
Iteration 19/25 | Loss: 0.00162614
Iteration 20/25 | Loss: 0.00162614
Iteration 21/25 | Loss: 0.00162614
Iteration 22/25 | Loss: 0.00162614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016261361306533217, 0.0016261361306533217, 0.0016261361306533217, 0.0016261361306533217, 0.0016261361306533217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016261361306533217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162614
Iteration 2/1000 | Loss: 0.00004304
Iteration 3/1000 | Loss: 0.00002780
Iteration 4/1000 | Loss: 0.00002435
Iteration 5/1000 | Loss: 0.00002263
Iteration 6/1000 | Loss: 0.00002175
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002071
Iteration 9/1000 | Loss: 0.00002048
Iteration 10/1000 | Loss: 0.00002028
Iteration 11/1000 | Loss: 0.00002025
Iteration 12/1000 | Loss: 0.00002021
Iteration 13/1000 | Loss: 0.00002020
Iteration 14/1000 | Loss: 0.00002020
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00002018
Iteration 18/1000 | Loss: 0.00002018
Iteration 19/1000 | Loss: 0.00002018
Iteration 20/1000 | Loss: 0.00002017
Iteration 21/1000 | Loss: 0.00002017
Iteration 22/1000 | Loss: 0.00002017
Iteration 23/1000 | Loss: 0.00002017
Iteration 24/1000 | Loss: 0.00002017
Iteration 25/1000 | Loss: 0.00002017
Iteration 26/1000 | Loss: 0.00002017
Iteration 27/1000 | Loss: 0.00002016
Iteration 28/1000 | Loss: 0.00002016
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002015
Iteration 32/1000 | Loss: 0.00002015
Iteration 33/1000 | Loss: 0.00002015
Iteration 34/1000 | Loss: 0.00002015
Iteration 35/1000 | Loss: 0.00002014
Iteration 36/1000 | Loss: 0.00002014
Iteration 37/1000 | Loss: 0.00002014
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002013
Iteration 41/1000 | Loss: 0.00002013
Iteration 42/1000 | Loss: 0.00002013
Iteration 43/1000 | Loss: 0.00002013
Iteration 44/1000 | Loss: 0.00002013
Iteration 45/1000 | Loss: 0.00002012
Iteration 46/1000 | Loss: 0.00002011
Iteration 47/1000 | Loss: 0.00002010
Iteration 48/1000 | Loss: 0.00002010
Iteration 49/1000 | Loss: 0.00002010
Iteration 50/1000 | Loss: 0.00002009
Iteration 51/1000 | Loss: 0.00002009
Iteration 52/1000 | Loss: 0.00002009
Iteration 53/1000 | Loss: 0.00002009
Iteration 54/1000 | Loss: 0.00002008
Iteration 55/1000 | Loss: 0.00002008
Iteration 56/1000 | Loss: 0.00002008
Iteration 57/1000 | Loss: 0.00002007
Iteration 58/1000 | Loss: 0.00002007
Iteration 59/1000 | Loss: 0.00002007
Iteration 60/1000 | Loss: 0.00002006
Iteration 61/1000 | Loss: 0.00002006
Iteration 62/1000 | Loss: 0.00002006
Iteration 63/1000 | Loss: 0.00002006
Iteration 64/1000 | Loss: 0.00002005
Iteration 65/1000 | Loss: 0.00002004
Iteration 66/1000 | Loss: 0.00002004
Iteration 67/1000 | Loss: 0.00002004
Iteration 68/1000 | Loss: 0.00002004
Iteration 69/1000 | Loss: 0.00002004
Iteration 70/1000 | Loss: 0.00002003
Iteration 71/1000 | Loss: 0.00002003
Iteration 72/1000 | Loss: 0.00002003
Iteration 73/1000 | Loss: 0.00002003
Iteration 74/1000 | Loss: 0.00002003
Iteration 75/1000 | Loss: 0.00002002
Iteration 76/1000 | Loss: 0.00002002
Iteration 77/1000 | Loss: 0.00002002
Iteration 78/1000 | Loss: 0.00002002
Iteration 79/1000 | Loss: 0.00002001
Iteration 80/1000 | Loss: 0.00002001
Iteration 81/1000 | Loss: 0.00002001
Iteration 82/1000 | Loss: 0.00002000
Iteration 83/1000 | Loss: 0.00002000
Iteration 84/1000 | Loss: 0.00002000
Iteration 85/1000 | Loss: 0.00002000
Iteration 86/1000 | Loss: 0.00001999
Iteration 87/1000 | Loss: 0.00001999
Iteration 88/1000 | Loss: 0.00001998
Iteration 89/1000 | Loss: 0.00001998
Iteration 90/1000 | Loss: 0.00001998
Iteration 91/1000 | Loss: 0.00001997
Iteration 92/1000 | Loss: 0.00001997
Iteration 93/1000 | Loss: 0.00001996
Iteration 94/1000 | Loss: 0.00001996
Iteration 95/1000 | Loss: 0.00001996
Iteration 96/1000 | Loss: 0.00001995
Iteration 97/1000 | Loss: 0.00001995
Iteration 98/1000 | Loss: 0.00001995
Iteration 99/1000 | Loss: 0.00001995
Iteration 100/1000 | Loss: 0.00001995
Iteration 101/1000 | Loss: 0.00001995
Iteration 102/1000 | Loss: 0.00001995
Iteration 103/1000 | Loss: 0.00001994
Iteration 104/1000 | Loss: 0.00001994
Iteration 105/1000 | Loss: 0.00001994
Iteration 106/1000 | Loss: 0.00001994
Iteration 107/1000 | Loss: 0.00001994
Iteration 108/1000 | Loss: 0.00001993
Iteration 109/1000 | Loss: 0.00001993
Iteration 110/1000 | Loss: 0.00001993
Iteration 111/1000 | Loss: 0.00001993
Iteration 112/1000 | Loss: 0.00001993
Iteration 113/1000 | Loss: 0.00001993
Iteration 114/1000 | Loss: 0.00001993
Iteration 115/1000 | Loss: 0.00001992
Iteration 116/1000 | Loss: 0.00001992
Iteration 117/1000 | Loss: 0.00001992
Iteration 118/1000 | Loss: 0.00001992
Iteration 119/1000 | Loss: 0.00001992
Iteration 120/1000 | Loss: 0.00001992
Iteration 121/1000 | Loss: 0.00001992
Iteration 122/1000 | Loss: 0.00001992
Iteration 123/1000 | Loss: 0.00001991
Iteration 124/1000 | Loss: 0.00001991
Iteration 125/1000 | Loss: 0.00001991
Iteration 126/1000 | Loss: 0.00001991
Iteration 127/1000 | Loss: 0.00001991
Iteration 128/1000 | Loss: 0.00001991
Iteration 129/1000 | Loss: 0.00001991
Iteration 130/1000 | Loss: 0.00001991
Iteration 131/1000 | Loss: 0.00001991
Iteration 132/1000 | Loss: 0.00001991
Iteration 133/1000 | Loss: 0.00001991
Iteration 134/1000 | Loss: 0.00001991
Iteration 135/1000 | Loss: 0.00001991
Iteration 136/1000 | Loss: 0.00001991
Iteration 137/1000 | Loss: 0.00001991
Iteration 138/1000 | Loss: 0.00001991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.9910774426534772e-05, 1.9910774426534772e-05, 1.9910774426534772e-05, 1.9910774426534772e-05, 1.9910774426534772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9910774426534772e-05

Optimization complete. Final v2v error: 3.8066282272338867 mm

Highest mean error: 4.13265323638916 mm for frame 111

Lowest mean error: 3.544039011001587 mm for frame 0

Saving results

Total time: 35.3310387134552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069356
Iteration 2/25 | Loss: 0.01069355
Iteration 3/25 | Loss: 0.01069355
Iteration 4/25 | Loss: 0.01069355
Iteration 5/25 | Loss: 0.01069355
Iteration 6/25 | Loss: 0.01069355
Iteration 7/25 | Loss: 0.01069355
Iteration 8/25 | Loss: 0.01069355
Iteration 9/25 | Loss: 0.01069355
Iteration 10/25 | Loss: 0.01069355
Iteration 11/25 | Loss: 0.01069354
Iteration 12/25 | Loss: 0.01069354
Iteration 13/25 | Loss: 0.01069354
Iteration 14/25 | Loss: 0.01069354
Iteration 15/25 | Loss: 0.01069354
Iteration 16/25 | Loss: 0.01069354
Iteration 17/25 | Loss: 0.01069354
Iteration 18/25 | Loss: 0.01069354
Iteration 19/25 | Loss: 0.01069354
Iteration 20/25 | Loss: 0.01069353
Iteration 21/25 | Loss: 0.01069353
Iteration 22/25 | Loss: 0.01069353
Iteration 23/25 | Loss: 0.01069353
Iteration 24/25 | Loss: 0.01069353
Iteration 25/25 | Loss: 0.01069353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04206324
Iteration 2/25 | Loss: 0.13447352
Iteration 3/25 | Loss: 0.13437688
Iteration 4/25 | Loss: 0.13428445
Iteration 5/25 | Loss: 0.13428444
Iteration 6/25 | Loss: 0.13428444
Iteration 7/25 | Loss: 0.13428442
Iteration 8/25 | Loss: 0.13428442
Iteration 9/25 | Loss: 0.13428441
Iteration 10/25 | Loss: 0.13428441
Iteration 11/25 | Loss: 0.13428441
Iteration 12/25 | Loss: 0.13428441
Iteration 13/25 | Loss: 0.13428441
Iteration 14/25 | Loss: 0.13428441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.13428440690040588, 0.13428440690040588, 0.13428440690040588, 0.13428440690040588, 0.13428440690040588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.13428440690040588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13428441
Iteration 2/1000 | Loss: 0.01100555
Iteration 3/1000 | Loss: 0.00279018
Iteration 4/1000 | Loss: 0.00298434
Iteration 5/1000 | Loss: 0.00246493
Iteration 6/1000 | Loss: 0.00533530
Iteration 7/1000 | Loss: 0.00134624
Iteration 8/1000 | Loss: 0.00121227
Iteration 9/1000 | Loss: 0.00033817
Iteration 10/1000 | Loss: 0.00103995
Iteration 11/1000 | Loss: 0.00210459
Iteration 12/1000 | Loss: 0.00027208
Iteration 13/1000 | Loss: 0.00199896
Iteration 14/1000 | Loss: 0.00170210
Iteration 15/1000 | Loss: 0.00121279
Iteration 16/1000 | Loss: 0.00042831
Iteration 17/1000 | Loss: 0.00010715
Iteration 18/1000 | Loss: 0.00013022
Iteration 19/1000 | Loss: 0.00011634
Iteration 20/1000 | Loss: 0.00043386
Iteration 21/1000 | Loss: 0.00081225
Iteration 22/1000 | Loss: 0.00012688
Iteration 23/1000 | Loss: 0.00022803
Iteration 24/1000 | Loss: 0.00005121
Iteration 25/1000 | Loss: 0.00050543
Iteration 26/1000 | Loss: 0.00124726
Iteration 27/1000 | Loss: 0.00010309
Iteration 28/1000 | Loss: 0.00023825
Iteration 29/1000 | Loss: 0.00004179
Iteration 30/1000 | Loss: 0.00071201
Iteration 31/1000 | Loss: 0.00031767
Iteration 32/1000 | Loss: 0.00088814
Iteration 33/1000 | Loss: 0.00005725
Iteration 34/1000 | Loss: 0.00010925
Iteration 35/1000 | Loss: 0.00003458
Iteration 36/1000 | Loss: 0.00018160
Iteration 37/1000 | Loss: 0.00027278
Iteration 38/1000 | Loss: 0.00037633
Iteration 39/1000 | Loss: 0.00008616
Iteration 40/1000 | Loss: 0.00027814
Iteration 41/1000 | Loss: 0.00004525
Iteration 42/1000 | Loss: 0.00003186
Iteration 43/1000 | Loss: 0.00019316
Iteration 44/1000 | Loss: 0.00003105
Iteration 45/1000 | Loss: 0.00009865
Iteration 46/1000 | Loss: 0.00149293
Iteration 47/1000 | Loss: 0.00010011
Iteration 48/1000 | Loss: 0.00008154
Iteration 49/1000 | Loss: 0.00006900
Iteration 50/1000 | Loss: 0.00002947
Iteration 51/1000 | Loss: 0.00005498
Iteration 52/1000 | Loss: 0.00003897
Iteration 53/1000 | Loss: 0.00002863
Iteration 54/1000 | Loss: 0.00002850
Iteration 55/1000 | Loss: 0.00005658
Iteration 56/1000 | Loss: 0.00005319
Iteration 57/1000 | Loss: 0.00009870
Iteration 58/1000 | Loss: 0.00005887
Iteration 59/1000 | Loss: 0.00003078
Iteration 60/1000 | Loss: 0.00002820
Iteration 61/1000 | Loss: 0.00002809
Iteration 62/1000 | Loss: 0.00002891
Iteration 63/1000 | Loss: 0.00002790
Iteration 64/1000 | Loss: 0.00016560
Iteration 65/1000 | Loss: 0.00004431
Iteration 66/1000 | Loss: 0.00002797
Iteration 67/1000 | Loss: 0.00007568
Iteration 68/1000 | Loss: 0.00002799
Iteration 69/1000 | Loss: 0.00002784
Iteration 70/1000 | Loss: 0.00002783
Iteration 71/1000 | Loss: 0.00002782
Iteration 72/1000 | Loss: 0.00002775
Iteration 73/1000 | Loss: 0.00002775
Iteration 74/1000 | Loss: 0.00002774
Iteration 75/1000 | Loss: 0.00002773
Iteration 76/1000 | Loss: 0.00002772
Iteration 77/1000 | Loss: 0.00002772
Iteration 78/1000 | Loss: 0.00012214
Iteration 79/1000 | Loss: 0.00002783
Iteration 80/1000 | Loss: 0.00002767
Iteration 81/1000 | Loss: 0.00004843
Iteration 82/1000 | Loss: 0.00002765
Iteration 83/1000 | Loss: 0.00002763
Iteration 84/1000 | Loss: 0.00002762
Iteration 85/1000 | Loss: 0.00002762
Iteration 86/1000 | Loss: 0.00002762
Iteration 87/1000 | Loss: 0.00002762
Iteration 88/1000 | Loss: 0.00002760
Iteration 89/1000 | Loss: 0.00002760
Iteration 90/1000 | Loss: 0.00002759
Iteration 91/1000 | Loss: 0.00002759
Iteration 92/1000 | Loss: 0.00002759
Iteration 93/1000 | Loss: 0.00002759
Iteration 94/1000 | Loss: 0.00002758
Iteration 95/1000 | Loss: 0.00002758
Iteration 96/1000 | Loss: 0.00002758
Iteration 97/1000 | Loss: 0.00002758
Iteration 98/1000 | Loss: 0.00002758
Iteration 99/1000 | Loss: 0.00002758
Iteration 100/1000 | Loss: 0.00002757
Iteration 101/1000 | Loss: 0.00002757
Iteration 102/1000 | Loss: 0.00002757
Iteration 103/1000 | Loss: 0.00002757
Iteration 104/1000 | Loss: 0.00002757
Iteration 105/1000 | Loss: 0.00002757
Iteration 106/1000 | Loss: 0.00002756
Iteration 107/1000 | Loss: 0.00002756
Iteration 108/1000 | Loss: 0.00002756
Iteration 109/1000 | Loss: 0.00002756
Iteration 110/1000 | Loss: 0.00002756
Iteration 111/1000 | Loss: 0.00002756
Iteration 112/1000 | Loss: 0.00002756
Iteration 113/1000 | Loss: 0.00002755
Iteration 114/1000 | Loss: 0.00002755
Iteration 115/1000 | Loss: 0.00002755
Iteration 116/1000 | Loss: 0.00002755
Iteration 117/1000 | Loss: 0.00002755
Iteration 118/1000 | Loss: 0.00002755
Iteration 119/1000 | Loss: 0.00002755
Iteration 120/1000 | Loss: 0.00002755
Iteration 121/1000 | Loss: 0.00002755
Iteration 122/1000 | Loss: 0.00002755
Iteration 123/1000 | Loss: 0.00002755
Iteration 124/1000 | Loss: 0.00002755
Iteration 125/1000 | Loss: 0.00002754
Iteration 126/1000 | Loss: 0.00002754
Iteration 127/1000 | Loss: 0.00002754
Iteration 128/1000 | Loss: 0.00002754
Iteration 129/1000 | Loss: 0.00002754
Iteration 130/1000 | Loss: 0.00002754
Iteration 131/1000 | Loss: 0.00002753
Iteration 132/1000 | Loss: 0.00002753
Iteration 133/1000 | Loss: 0.00002753
Iteration 134/1000 | Loss: 0.00002753
Iteration 135/1000 | Loss: 0.00002752
Iteration 136/1000 | Loss: 0.00002752
Iteration 137/1000 | Loss: 0.00002752
Iteration 138/1000 | Loss: 0.00002752
Iteration 139/1000 | Loss: 0.00002751
Iteration 140/1000 | Loss: 0.00002751
Iteration 141/1000 | Loss: 0.00002751
Iteration 142/1000 | Loss: 0.00002751
Iteration 143/1000 | Loss: 0.00002751
Iteration 144/1000 | Loss: 0.00002751
Iteration 145/1000 | Loss: 0.00002751
Iteration 146/1000 | Loss: 0.00002751
Iteration 147/1000 | Loss: 0.00002750
Iteration 148/1000 | Loss: 0.00002750
Iteration 149/1000 | Loss: 0.00002750
Iteration 150/1000 | Loss: 0.00002750
Iteration 151/1000 | Loss: 0.00002750
Iteration 152/1000 | Loss: 0.00002750
Iteration 153/1000 | Loss: 0.00002750
Iteration 154/1000 | Loss: 0.00002750
Iteration 155/1000 | Loss: 0.00002750
Iteration 156/1000 | Loss: 0.00002750
Iteration 157/1000 | Loss: 0.00002750
Iteration 158/1000 | Loss: 0.00002750
Iteration 159/1000 | Loss: 0.00002750
Iteration 160/1000 | Loss: 0.00002749
Iteration 161/1000 | Loss: 0.00002749
Iteration 162/1000 | Loss: 0.00002749
Iteration 163/1000 | Loss: 0.00002749
Iteration 164/1000 | Loss: 0.00002749
Iteration 165/1000 | Loss: 0.00002749
Iteration 166/1000 | Loss: 0.00002749
Iteration 167/1000 | Loss: 0.00002749
Iteration 168/1000 | Loss: 0.00002749
Iteration 169/1000 | Loss: 0.00002749
Iteration 170/1000 | Loss: 0.00002749
Iteration 171/1000 | Loss: 0.00002749
Iteration 172/1000 | Loss: 0.00002749
Iteration 173/1000 | Loss: 0.00002749
Iteration 174/1000 | Loss: 0.00002749
Iteration 175/1000 | Loss: 0.00002748
Iteration 176/1000 | Loss: 0.00002748
Iteration 177/1000 | Loss: 0.00002748
Iteration 178/1000 | Loss: 0.00002748
Iteration 179/1000 | Loss: 0.00002748
Iteration 180/1000 | Loss: 0.00002747
Iteration 181/1000 | Loss: 0.00002747
Iteration 182/1000 | Loss: 0.00002747
Iteration 183/1000 | Loss: 0.00002747
Iteration 184/1000 | Loss: 0.00002747
Iteration 185/1000 | Loss: 0.00002747
Iteration 186/1000 | Loss: 0.00002747
Iteration 187/1000 | Loss: 0.00002747
Iteration 188/1000 | Loss: 0.00002747
Iteration 189/1000 | Loss: 0.00002747
Iteration 190/1000 | Loss: 0.00002747
Iteration 191/1000 | Loss: 0.00002747
Iteration 192/1000 | Loss: 0.00002747
Iteration 193/1000 | Loss: 0.00002747
Iteration 194/1000 | Loss: 0.00002746
Iteration 195/1000 | Loss: 0.00002746
Iteration 196/1000 | Loss: 0.00002746
Iteration 197/1000 | Loss: 0.00002746
Iteration 198/1000 | Loss: 0.00002746
Iteration 199/1000 | Loss: 0.00002746
Iteration 200/1000 | Loss: 0.00002746
Iteration 201/1000 | Loss: 0.00002746
Iteration 202/1000 | Loss: 0.00002746
Iteration 203/1000 | Loss: 0.00002746
Iteration 204/1000 | Loss: 0.00002746
Iteration 205/1000 | Loss: 0.00002746
Iteration 206/1000 | Loss: 0.00002745
Iteration 207/1000 | Loss: 0.00002745
Iteration 208/1000 | Loss: 0.00002745
Iteration 209/1000 | Loss: 0.00002745
Iteration 210/1000 | Loss: 0.00002745
Iteration 211/1000 | Loss: 0.00002745
Iteration 212/1000 | Loss: 0.00002745
Iteration 213/1000 | Loss: 0.00002745
Iteration 214/1000 | Loss: 0.00002745
Iteration 215/1000 | Loss: 0.00002745
Iteration 216/1000 | Loss: 0.00002745
Iteration 217/1000 | Loss: 0.00002745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.7452839276520535e-05, 2.7452839276520535e-05, 2.7452839276520535e-05, 2.7452839276520535e-05, 2.7452839276520535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7452839276520535e-05

Optimization complete. Final v2v error: 4.456421375274658 mm

Highest mean error: 4.983877182006836 mm for frame 226

Lowest mean error: 4.052835941314697 mm for frame 176

Saving results

Total time: 127.97966480255127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104489
Iteration 2/25 | Loss: 0.00274058
Iteration 3/25 | Loss: 0.00168625
Iteration 4/25 | Loss: 0.00148328
Iteration 5/25 | Loss: 0.00152976
Iteration 6/25 | Loss: 0.00148572
Iteration 7/25 | Loss: 0.00139092
Iteration 8/25 | Loss: 0.00135847
Iteration 9/25 | Loss: 0.00131324
Iteration 10/25 | Loss: 0.00128721
Iteration 11/25 | Loss: 0.00125428
Iteration 12/25 | Loss: 0.00122428
Iteration 13/25 | Loss: 0.00120833
Iteration 14/25 | Loss: 0.00119855
Iteration 15/25 | Loss: 0.00118825
Iteration 16/25 | Loss: 0.00117203
Iteration 17/25 | Loss: 0.00116239
Iteration 18/25 | Loss: 0.00115723
Iteration 19/25 | Loss: 0.00116050
Iteration 20/25 | Loss: 0.00115830
Iteration 21/25 | Loss: 0.00114929
Iteration 22/25 | Loss: 0.00114682
Iteration 23/25 | Loss: 0.00114141
Iteration 24/25 | Loss: 0.00113918
Iteration 25/25 | Loss: 0.00114429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52907050
Iteration 2/25 | Loss: 0.00333646
Iteration 3/25 | Loss: 0.00321732
Iteration 4/25 | Loss: 0.00321730
Iteration 5/25 | Loss: 0.00321730
Iteration 6/25 | Loss: 0.00321730
Iteration 7/25 | Loss: 0.00321730
Iteration 8/25 | Loss: 0.00321730
Iteration 9/25 | Loss: 0.00321730
Iteration 10/25 | Loss: 0.00321730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.003217303194105625, 0.003217303194105625, 0.003217303194105625, 0.003217303194105625, 0.003217303194105625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003217303194105625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00321730
Iteration 2/1000 | Loss: 0.00094629
Iteration 3/1000 | Loss: 0.00067659
Iteration 4/1000 | Loss: 0.00067800
Iteration 5/1000 | Loss: 0.00078836
Iteration 6/1000 | Loss: 0.00039120
Iteration 7/1000 | Loss: 0.00025603
Iteration 8/1000 | Loss: 0.00050312
Iteration 9/1000 | Loss: 0.00046966
Iteration 10/1000 | Loss: 0.00074773
Iteration 11/1000 | Loss: 0.00180431
Iteration 12/1000 | Loss: 0.00200467
Iteration 13/1000 | Loss: 0.00097479
Iteration 14/1000 | Loss: 0.00057832
Iteration 15/1000 | Loss: 0.00049955
Iteration 16/1000 | Loss: 0.00066673
Iteration 17/1000 | Loss: 0.00049853
Iteration 18/1000 | Loss: 0.00066532
Iteration 19/1000 | Loss: 0.00065522
Iteration 20/1000 | Loss: 0.00082356
Iteration 21/1000 | Loss: 0.00077674
Iteration 22/1000 | Loss: 0.00100718
Iteration 23/1000 | Loss: 0.00061715
Iteration 24/1000 | Loss: 0.00052291
Iteration 25/1000 | Loss: 0.00015468
Iteration 26/1000 | Loss: 0.00093677
Iteration 27/1000 | Loss: 0.00094074
Iteration 28/1000 | Loss: 0.00095788
Iteration 29/1000 | Loss: 0.00080041
Iteration 30/1000 | Loss: 0.00032623
Iteration 31/1000 | Loss: 0.00091149
Iteration 32/1000 | Loss: 0.00052092
Iteration 33/1000 | Loss: 0.00030289
Iteration 34/1000 | Loss: 0.00019517
Iteration 35/1000 | Loss: 0.00026143
Iteration 36/1000 | Loss: 0.00013305
Iteration 37/1000 | Loss: 0.00059867
Iteration 38/1000 | Loss: 0.00020815
Iteration 39/1000 | Loss: 0.00028893
Iteration 40/1000 | Loss: 0.00015970
Iteration 41/1000 | Loss: 0.00041558
Iteration 42/1000 | Loss: 0.00082297
Iteration 43/1000 | Loss: 0.00252462
Iteration 44/1000 | Loss: 0.00100152
Iteration 45/1000 | Loss: 0.00115539
Iteration 46/1000 | Loss: 0.00164091
Iteration 47/1000 | Loss: 0.00260184
Iteration 48/1000 | Loss: 0.00262101
Iteration 49/1000 | Loss: 0.00118068
Iteration 50/1000 | Loss: 0.00081981
Iteration 51/1000 | Loss: 0.00047447
Iteration 52/1000 | Loss: 0.00074747
Iteration 53/1000 | Loss: 0.00044992
Iteration 54/1000 | Loss: 0.00037229
Iteration 55/1000 | Loss: 0.00035087
Iteration 56/1000 | Loss: 0.00090629
Iteration 57/1000 | Loss: 0.00067045
Iteration 58/1000 | Loss: 0.00063201
Iteration 59/1000 | Loss: 0.00117802
Iteration 60/1000 | Loss: 0.00035739
Iteration 61/1000 | Loss: 0.00041467
Iteration 62/1000 | Loss: 0.00053917
Iteration 63/1000 | Loss: 0.00038123
Iteration 64/1000 | Loss: 0.00022253
Iteration 65/1000 | Loss: 0.00026646
Iteration 66/1000 | Loss: 0.00025157
Iteration 67/1000 | Loss: 0.00040825
Iteration 68/1000 | Loss: 0.00025938
Iteration 69/1000 | Loss: 0.00020237
Iteration 70/1000 | Loss: 0.00040528
Iteration 71/1000 | Loss: 0.00020575
Iteration 72/1000 | Loss: 0.00035053
Iteration 73/1000 | Loss: 0.00018247
Iteration 74/1000 | Loss: 0.00045467
Iteration 75/1000 | Loss: 0.00013555
Iteration 76/1000 | Loss: 0.00014170
Iteration 77/1000 | Loss: 0.00021858
Iteration 78/1000 | Loss: 0.00031848
Iteration 79/1000 | Loss: 0.00031973
Iteration 80/1000 | Loss: 0.00050621
Iteration 81/1000 | Loss: 0.00044519
Iteration 82/1000 | Loss: 0.00025519
Iteration 83/1000 | Loss: 0.00011386
Iteration 84/1000 | Loss: 0.00010780
Iteration 85/1000 | Loss: 0.00010418
Iteration 86/1000 | Loss: 0.00009195
Iteration 87/1000 | Loss: 0.00010856
Iteration 88/1000 | Loss: 0.00009516
Iteration 89/1000 | Loss: 0.00015555
Iteration 90/1000 | Loss: 0.00007659
Iteration 91/1000 | Loss: 0.00010001
Iteration 92/1000 | Loss: 0.00010458
Iteration 93/1000 | Loss: 0.00010015
Iteration 94/1000 | Loss: 0.00008637
Iteration 95/1000 | Loss: 0.00009007
Iteration 96/1000 | Loss: 0.00008557
Iteration 97/1000 | Loss: 0.00009455
Iteration 98/1000 | Loss: 0.00008399
Iteration 99/1000 | Loss: 0.00009553
Iteration 100/1000 | Loss: 0.00008365
Iteration 101/1000 | Loss: 0.00009095
Iteration 102/1000 | Loss: 0.00010885
Iteration 103/1000 | Loss: 0.00009733
Iteration 104/1000 | Loss: 0.00010238
Iteration 105/1000 | Loss: 0.00008913
Iteration 106/1000 | Loss: 0.00009138
Iteration 107/1000 | Loss: 0.00009872
Iteration 108/1000 | Loss: 0.00011184
Iteration 109/1000 | Loss: 0.00009634
Iteration 110/1000 | Loss: 0.00010946
Iteration 111/1000 | Loss: 0.00009626
Iteration 112/1000 | Loss: 0.00010236
Iteration 113/1000 | Loss: 0.00008294
Iteration 114/1000 | Loss: 0.00011156
Iteration 115/1000 | Loss: 0.00008717
Iteration 116/1000 | Loss: 0.00009659
Iteration 117/1000 | Loss: 0.00009342
Iteration 118/1000 | Loss: 0.00008960
Iteration 119/1000 | Loss: 0.00009188
Iteration 120/1000 | Loss: 0.00009999
Iteration 121/1000 | Loss: 0.00009595
Iteration 122/1000 | Loss: 0.00008906
Iteration 123/1000 | Loss: 0.00009566
Iteration 124/1000 | Loss: 0.00010731
Iteration 125/1000 | Loss: 0.00011048
Iteration 126/1000 | Loss: 0.00009667
Iteration 127/1000 | Loss: 0.00009654
Iteration 128/1000 | Loss: 0.00008731
Iteration 129/1000 | Loss: 0.00009044
Iteration 130/1000 | Loss: 0.00010487
Iteration 131/1000 | Loss: 0.00009482
Iteration 132/1000 | Loss: 0.00009282
Iteration 133/1000 | Loss: 0.00009976
Iteration 134/1000 | Loss: 0.00008969
Iteration 135/1000 | Loss: 0.00010564
Iteration 136/1000 | Loss: 0.00010238
Iteration 137/1000 | Loss: 0.00010111
Iteration 138/1000 | Loss: 0.00009962
Iteration 139/1000 | Loss: 0.00010991
Iteration 140/1000 | Loss: 0.00010717
Iteration 141/1000 | Loss: 0.00009335
Iteration 142/1000 | Loss: 0.00008843
Iteration 143/1000 | Loss: 0.00009364
Iteration 144/1000 | Loss: 0.00009883
Iteration 145/1000 | Loss: 0.00009809
Iteration 146/1000 | Loss: 0.00010028
Iteration 147/1000 | Loss: 0.00009438
Iteration 148/1000 | Loss: 0.00010013
Iteration 149/1000 | Loss: 0.00009484
Iteration 150/1000 | Loss: 0.00011028
Iteration 151/1000 | Loss: 0.00010024
Iteration 152/1000 | Loss: 0.00010402
Iteration 153/1000 | Loss: 0.00009300
Iteration 154/1000 | Loss: 0.00010713
Iteration 155/1000 | Loss: 0.00009258
Iteration 156/1000 | Loss: 0.00010074
Iteration 157/1000 | Loss: 0.00010153
Iteration 158/1000 | Loss: 0.00011636
Iteration 159/1000 | Loss: 0.00010243
Iteration 160/1000 | Loss: 0.00011119
Iteration 161/1000 | Loss: 0.00010175
Iteration 162/1000 | Loss: 0.00010609
Iteration 163/1000 | Loss: 0.00010005
Iteration 164/1000 | Loss: 0.00009879
Iteration 165/1000 | Loss: 0.00009809
Iteration 166/1000 | Loss: 0.00011169
Iteration 167/1000 | Loss: 0.00009416
Iteration 168/1000 | Loss: 0.00010589
Iteration 169/1000 | Loss: 0.00009498
Iteration 170/1000 | Loss: 0.00008318
Iteration 171/1000 | Loss: 0.00009907
Iteration 172/1000 | Loss: 0.00009434
Iteration 173/1000 | Loss: 0.00009475
Iteration 174/1000 | Loss: 0.00009904
Iteration 175/1000 | Loss: 0.00009539
Iteration 176/1000 | Loss: 0.00009984
Iteration 177/1000 | Loss: 0.00009537
Iteration 178/1000 | Loss: 0.00010265
Iteration 179/1000 | Loss: 0.00008673
Iteration 180/1000 | Loss: 0.00011167
Iteration 181/1000 | Loss: 0.00009774
Iteration 182/1000 | Loss: 0.00009679
Iteration 183/1000 | Loss: 0.00009329
Iteration 184/1000 | Loss: 0.00009968
Iteration 185/1000 | Loss: 0.00009431
Iteration 186/1000 | Loss: 0.00010226
Iteration 187/1000 | Loss: 0.00009603
Iteration 188/1000 | Loss: 0.00008906
Iteration 189/1000 | Loss: 0.00011235
Iteration 190/1000 | Loss: 0.00010456
Iteration 191/1000 | Loss: 0.00010702
Iteration 192/1000 | Loss: 0.00009328
Iteration 193/1000 | Loss: 0.00010621
Iteration 194/1000 | Loss: 0.00010479
Iteration 195/1000 | Loss: 0.00010039
Iteration 196/1000 | Loss: 0.00010130
Iteration 197/1000 | Loss: 0.00009607
Iteration 198/1000 | Loss: 0.00010526
Iteration 199/1000 | Loss: 0.00010036
Iteration 200/1000 | Loss: 0.00009037
Iteration 201/1000 | Loss: 0.00008148
Iteration 202/1000 | Loss: 0.00008407
Iteration 203/1000 | Loss: 0.00007994
Iteration 204/1000 | Loss: 0.00009344
Iteration 205/1000 | Loss: 0.00009180
Iteration 206/1000 | Loss: 0.00009429
Iteration 207/1000 | Loss: 0.00008579
Iteration 208/1000 | Loss: 0.00008895
Iteration 209/1000 | Loss: 0.00008867
Iteration 210/1000 | Loss: 0.00009446
Iteration 211/1000 | Loss: 0.00008490
Iteration 212/1000 | Loss: 0.00008166
Iteration 213/1000 | Loss: 0.00007956
Iteration 214/1000 | Loss: 0.00008576
Iteration 215/1000 | Loss: 0.00009109
Iteration 216/1000 | Loss: 0.00008592
Iteration 217/1000 | Loss: 0.00008929
Iteration 218/1000 | Loss: 0.00008610
Iteration 219/1000 | Loss: 0.00008852
Iteration 220/1000 | Loss: 0.00008893
Iteration 221/1000 | Loss: 0.00008831
Iteration 222/1000 | Loss: 0.00008157
Iteration 223/1000 | Loss: 0.00007597
Iteration 224/1000 | Loss: 0.00009254
Iteration 225/1000 | Loss: 0.00008693
Iteration 226/1000 | Loss: 0.00008729
Iteration 227/1000 | Loss: 0.00008721
Iteration 228/1000 | Loss: 0.00007594
Iteration 229/1000 | Loss: 0.00007517
Iteration 230/1000 | Loss: 0.00009091
Iteration 231/1000 | Loss: 0.00008826
Iteration 232/1000 | Loss: 0.00010231
Iteration 233/1000 | Loss: 0.00008737
Iteration 234/1000 | Loss: 0.00008847
Iteration 235/1000 | Loss: 0.00008014
Iteration 236/1000 | Loss: 0.00008744
Iteration 237/1000 | Loss: 0.00008198
Iteration 238/1000 | Loss: 0.00008579
Iteration 239/1000 | Loss: 0.00008482
Iteration 240/1000 | Loss: 0.00008257
Iteration 241/1000 | Loss: 0.00007908
Iteration 242/1000 | Loss: 0.00008662
Iteration 243/1000 | Loss: 0.00010205
Iteration 244/1000 | Loss: 0.00010072
Iteration 245/1000 | Loss: 0.00008057
Iteration 246/1000 | Loss: 0.00008528
Iteration 247/1000 | Loss: 0.00008243
Iteration 248/1000 | Loss: 0.00008388
Iteration 249/1000 | Loss: 0.00008505
Iteration 250/1000 | Loss: 0.00007924
Iteration 251/1000 | Loss: 0.00008080
Iteration 252/1000 | Loss: 0.00008355
Iteration 253/1000 | Loss: 0.00007920
Iteration 254/1000 | Loss: 0.00008624
Iteration 255/1000 | Loss: 0.00008829
Iteration 256/1000 | Loss: 0.00008959
Iteration 257/1000 | Loss: 0.00008623
Iteration 258/1000 | Loss: 0.00007674
Iteration 259/1000 | Loss: 0.00007209
Iteration 260/1000 | Loss: 0.00006985
Iteration 261/1000 | Loss: 0.00006945
Iteration 262/1000 | Loss: 0.00006931
Iteration 263/1000 | Loss: 0.00007102
Iteration 264/1000 | Loss: 0.00007075
Iteration 265/1000 | Loss: 0.00007144
Iteration 266/1000 | Loss: 0.00007073
Iteration 267/1000 | Loss: 0.00007099
Iteration 268/1000 | Loss: 0.00007110
Iteration 269/1000 | Loss: 0.00006831
Iteration 270/1000 | Loss: 0.00006876
Iteration 271/1000 | Loss: 0.00007394
Iteration 272/1000 | Loss: 0.00007043
Iteration 273/1000 | Loss: 0.00006768
Iteration 274/1000 | Loss: 0.00006856
Iteration 275/1000 | Loss: 0.00007274
Iteration 276/1000 | Loss: 0.00007056
Iteration 277/1000 | Loss: 0.00006596
Iteration 278/1000 | Loss: 0.00007117
Iteration 279/1000 | Loss: 0.00006779
Iteration 280/1000 | Loss: 0.00006715
Iteration 281/1000 | Loss: 0.00006779
Iteration 282/1000 | Loss: 0.00006557
Iteration 283/1000 | Loss: 0.00006718
Iteration 284/1000 | Loss: 0.00007106
Iteration 285/1000 | Loss: 0.00006802
Iteration 286/1000 | Loss: 0.00006514
Iteration 287/1000 | Loss: 0.00007091
Iteration 288/1000 | Loss: 0.00006921
Iteration 289/1000 | Loss: 0.00006839
Iteration 290/1000 | Loss: 0.00007178
Iteration 291/1000 | Loss: 0.00006805
Iteration 292/1000 | Loss: 0.00007078
Iteration 293/1000 | Loss: 0.00006840
Iteration 294/1000 | Loss: 0.00006728
Iteration 295/1000 | Loss: 0.00007364
Iteration 296/1000 | Loss: 0.00007008
Iteration 297/1000 | Loss: 0.00008197
Iteration 298/1000 | Loss: 0.00007373
Iteration 299/1000 | Loss: 0.00007284
Iteration 300/1000 | Loss: 0.00006934
Iteration 301/1000 | Loss: 0.00007336
Iteration 302/1000 | Loss: 0.00006908
Iteration 303/1000 | Loss: 0.00006620
Iteration 304/1000 | Loss: 0.00007479
Iteration 305/1000 | Loss: 0.00006813
Iteration 306/1000 | Loss: 0.00007357
Iteration 307/1000 | Loss: 0.00006789
Iteration 308/1000 | Loss: 0.00006735
Iteration 309/1000 | Loss: 0.00006727
Iteration 310/1000 | Loss: 0.00007262
Iteration 311/1000 | Loss: 0.00007078
Iteration 312/1000 | Loss: 0.00006744
Iteration 313/1000 | Loss: 0.00008181
Iteration 314/1000 | Loss: 0.00007253
Iteration 315/1000 | Loss: 0.00006980
Iteration 316/1000 | Loss: 0.00006687
Iteration 317/1000 | Loss: 0.00006538
Iteration 318/1000 | Loss: 0.00006947
Iteration 319/1000 | Loss: 0.00006638
Iteration 320/1000 | Loss: 0.00006576
Iteration 321/1000 | Loss: 0.00006549
Iteration 322/1000 | Loss: 0.00006538
Iteration 323/1000 | Loss: 0.00006822
Iteration 324/1000 | Loss: 0.00006813
Iteration 325/1000 | Loss: 0.00006848
Iteration 326/1000 | Loss: 0.00006834
Iteration 327/1000 | Loss: 0.00006863
Iteration 328/1000 | Loss: 0.00006512
Iteration 329/1000 | Loss: 0.00006271
Iteration 330/1000 | Loss: 0.00006213
Iteration 331/1000 | Loss: 0.00006183
Iteration 332/1000 | Loss: 0.00006163
Iteration 333/1000 | Loss: 0.00006158
Iteration 334/1000 | Loss: 0.00006154
Iteration 335/1000 | Loss: 0.00006154
Iteration 336/1000 | Loss: 0.00006154
Iteration 337/1000 | Loss: 0.00006154
Iteration 338/1000 | Loss: 0.00006153
Iteration 339/1000 | Loss: 0.00006153
Iteration 340/1000 | Loss: 0.00006153
Iteration 341/1000 | Loss: 0.00006153
Iteration 342/1000 | Loss: 0.00006153
Iteration 343/1000 | Loss: 0.00006152
Iteration 344/1000 | Loss: 0.00006151
Iteration 345/1000 | Loss: 0.00006151
Iteration 346/1000 | Loss: 0.00006150
Iteration 347/1000 | Loss: 0.00006150
Iteration 348/1000 | Loss: 0.00006150
Iteration 349/1000 | Loss: 0.00006150
Iteration 350/1000 | Loss: 0.00006150
Iteration 351/1000 | Loss: 0.00006150
Iteration 352/1000 | Loss: 0.00006150
Iteration 353/1000 | Loss: 0.00006150
Iteration 354/1000 | Loss: 0.00006150
Iteration 355/1000 | Loss: 0.00006149
Iteration 356/1000 | Loss: 0.00006149
Iteration 357/1000 | Loss: 0.00006149
Iteration 358/1000 | Loss: 0.00006149
Iteration 359/1000 | Loss: 0.00006149
Iteration 360/1000 | Loss: 0.00006148
Iteration 361/1000 | Loss: 0.00006148
Iteration 362/1000 | Loss: 0.00006148
Iteration 363/1000 | Loss: 0.00006148
Iteration 364/1000 | Loss: 0.00006148
Iteration 365/1000 | Loss: 0.00006148
Iteration 366/1000 | Loss: 0.00006148
Iteration 367/1000 | Loss: 0.00006148
Iteration 368/1000 | Loss: 0.00006148
Iteration 369/1000 | Loss: 0.00006148
Iteration 370/1000 | Loss: 0.00006148
Iteration 371/1000 | Loss: 0.00006148
Iteration 372/1000 | Loss: 0.00006148
Iteration 373/1000 | Loss: 0.00006148
Iteration 374/1000 | Loss: 0.00006148
Iteration 375/1000 | Loss: 0.00006148
Iteration 376/1000 | Loss: 0.00006148
Iteration 377/1000 | Loss: 0.00006148
Iteration 378/1000 | Loss: 0.00006148
Iteration 379/1000 | Loss: 0.00006148
Iteration 380/1000 | Loss: 0.00006148
Iteration 381/1000 | Loss: 0.00006148
Iteration 382/1000 | Loss: 0.00006148
Iteration 383/1000 | Loss: 0.00006148
Iteration 384/1000 | Loss: 0.00006148
Iteration 385/1000 | Loss: 0.00006148
Iteration 386/1000 | Loss: 0.00006148
Iteration 387/1000 | Loss: 0.00006148
Iteration 388/1000 | Loss: 0.00006148
Iteration 389/1000 | Loss: 0.00006148
Iteration 390/1000 | Loss: 0.00006148
Iteration 391/1000 | Loss: 0.00006148
Iteration 392/1000 | Loss: 0.00006148
Iteration 393/1000 | Loss: 0.00006148
Iteration 394/1000 | Loss: 0.00006148
Iteration 395/1000 | Loss: 0.00006148
Iteration 396/1000 | Loss: 0.00006148
Iteration 397/1000 | Loss: 0.00006148
Iteration 398/1000 | Loss: 0.00006148
Iteration 399/1000 | Loss: 0.00006148
Iteration 400/1000 | Loss: 0.00006148
Iteration 401/1000 | Loss: 0.00006148
Iteration 402/1000 | Loss: 0.00006148
Iteration 403/1000 | Loss: 0.00006148
Iteration 404/1000 | Loss: 0.00006148
Iteration 405/1000 | Loss: 0.00006148
Iteration 406/1000 | Loss: 0.00006148
Iteration 407/1000 | Loss: 0.00006148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 407. Stopping optimization.
Last 5 losses: [6.147831300040707e-05, 6.147831300040707e-05, 6.147831300040707e-05, 6.147831300040707e-05, 6.147831300040707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.147831300040707e-05

Optimization complete. Final v2v error: 4.512726783752441 mm

Highest mean error: 12.375589370727539 mm for frame 66

Lowest mean error: 3.6283488273620605 mm for frame 50

Saving results

Total time: 581.3671805858612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01219130
Iteration 2/25 | Loss: 0.00201761
Iteration 3/25 | Loss: 0.00134154
Iteration 4/25 | Loss: 0.00124842
Iteration 5/25 | Loss: 0.00126618
Iteration 6/25 | Loss: 0.00119821
Iteration 7/25 | Loss: 0.00120137
Iteration 8/25 | Loss: 0.00117846
Iteration 9/25 | Loss: 0.00116392
Iteration 10/25 | Loss: 0.00114330
Iteration 11/25 | Loss: 0.00113183
Iteration 12/25 | Loss: 0.00112507
Iteration 13/25 | Loss: 0.00111891
Iteration 14/25 | Loss: 0.00111563
Iteration 15/25 | Loss: 0.00111464
Iteration 16/25 | Loss: 0.00111273
Iteration 17/25 | Loss: 0.00111210
Iteration 18/25 | Loss: 0.00110946
Iteration 19/25 | Loss: 0.00110911
Iteration 20/25 | Loss: 0.00110598
Iteration 21/25 | Loss: 0.00110394
Iteration 22/25 | Loss: 0.00110428
Iteration 23/25 | Loss: 0.00110289
Iteration 24/25 | Loss: 0.00110299
Iteration 25/25 | Loss: 0.00110161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11710095
Iteration 2/25 | Loss: 0.00196856
Iteration 3/25 | Loss: 0.00196856
Iteration 4/25 | Loss: 0.00196856
Iteration 5/25 | Loss: 0.00196856
Iteration 6/25 | Loss: 0.00196856
Iteration 7/25 | Loss: 0.00196856
Iteration 8/25 | Loss: 0.00196856
Iteration 9/25 | Loss: 0.00196856
Iteration 10/25 | Loss: 0.00196856
Iteration 11/25 | Loss: 0.00196856
Iteration 12/25 | Loss: 0.00196856
Iteration 13/25 | Loss: 0.00196856
Iteration 14/25 | Loss: 0.00196856
Iteration 15/25 | Loss: 0.00196856
Iteration 16/25 | Loss: 0.00196856
Iteration 17/25 | Loss: 0.00196856
Iteration 18/25 | Loss: 0.00196856
Iteration 19/25 | Loss: 0.00196856
Iteration 20/25 | Loss: 0.00196856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001968563534319401, 0.001968563534319401, 0.001968563534319401, 0.001968563534319401, 0.001968563534319401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001968563534319401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196856
Iteration 2/1000 | Loss: 0.00025762
Iteration 3/1000 | Loss: 0.00020883
Iteration 4/1000 | Loss: 0.00013147
Iteration 5/1000 | Loss: 0.00019672
Iteration 6/1000 | Loss: 0.00011960
Iteration 7/1000 | Loss: 0.00010973
Iteration 8/1000 | Loss: 0.00016954
Iteration 9/1000 | Loss: 0.00008850
Iteration 10/1000 | Loss: 0.00037703
Iteration 11/1000 | Loss: 0.00019250
Iteration 12/1000 | Loss: 0.00056141
Iteration 13/1000 | Loss: 0.00146492
Iteration 14/1000 | Loss: 0.00044006
Iteration 15/1000 | Loss: 0.00031337
Iteration 16/1000 | Loss: 0.00008834
Iteration 17/1000 | Loss: 0.00006705
Iteration 18/1000 | Loss: 0.00005876
Iteration 19/1000 | Loss: 0.00005433
Iteration 20/1000 | Loss: 0.00005078
Iteration 21/1000 | Loss: 0.00054517
Iteration 22/1000 | Loss: 0.00005388
Iteration 23/1000 | Loss: 0.00004809
Iteration 24/1000 | Loss: 0.00026323
Iteration 25/1000 | Loss: 0.00004577
Iteration 26/1000 | Loss: 0.00031544
Iteration 27/1000 | Loss: 0.00004619
Iteration 28/1000 | Loss: 0.00004291
Iteration 29/1000 | Loss: 0.00004173
Iteration 30/1000 | Loss: 0.00004084
Iteration 31/1000 | Loss: 0.00004045
Iteration 32/1000 | Loss: 0.00004024
Iteration 33/1000 | Loss: 0.00004006
Iteration 34/1000 | Loss: 0.00003996
Iteration 35/1000 | Loss: 0.00003996
Iteration 36/1000 | Loss: 0.00003993
Iteration 37/1000 | Loss: 0.00003992
Iteration 38/1000 | Loss: 0.00003992
Iteration 39/1000 | Loss: 0.00003992
Iteration 40/1000 | Loss: 0.00003989
Iteration 41/1000 | Loss: 0.00003985
Iteration 42/1000 | Loss: 0.00003985
Iteration 43/1000 | Loss: 0.00003985
Iteration 44/1000 | Loss: 0.00003982
Iteration 45/1000 | Loss: 0.00003981
Iteration 46/1000 | Loss: 0.00003981
Iteration 47/1000 | Loss: 0.00003976
Iteration 48/1000 | Loss: 0.00003976
Iteration 49/1000 | Loss: 0.00003976
Iteration 50/1000 | Loss: 0.00003975
Iteration 51/1000 | Loss: 0.00003972
Iteration 52/1000 | Loss: 0.00003972
Iteration 53/1000 | Loss: 0.00003971
Iteration 54/1000 | Loss: 0.00003970
Iteration 55/1000 | Loss: 0.00003962
Iteration 56/1000 | Loss: 0.00003957
Iteration 57/1000 | Loss: 0.00003949
Iteration 58/1000 | Loss: 0.00003948
Iteration 59/1000 | Loss: 0.00003948
Iteration 60/1000 | Loss: 0.00003945
Iteration 61/1000 | Loss: 0.00003945
Iteration 62/1000 | Loss: 0.00003944
Iteration 63/1000 | Loss: 0.00003944
Iteration 64/1000 | Loss: 0.00003943
Iteration 65/1000 | Loss: 0.00003942
Iteration 66/1000 | Loss: 0.00003942
Iteration 67/1000 | Loss: 0.00003941
Iteration 68/1000 | Loss: 0.00003941
Iteration 69/1000 | Loss: 0.00003941
Iteration 70/1000 | Loss: 0.00003941
Iteration 71/1000 | Loss: 0.00003940
Iteration 72/1000 | Loss: 0.00003940
Iteration 73/1000 | Loss: 0.00003940
Iteration 74/1000 | Loss: 0.00003940
Iteration 75/1000 | Loss: 0.00003940
Iteration 76/1000 | Loss: 0.00003939
Iteration 77/1000 | Loss: 0.00003939
Iteration 78/1000 | Loss: 0.00003938
Iteration 79/1000 | Loss: 0.00003938
Iteration 80/1000 | Loss: 0.00003938
Iteration 81/1000 | Loss: 0.00003938
Iteration 82/1000 | Loss: 0.00003937
Iteration 83/1000 | Loss: 0.00003937
Iteration 84/1000 | Loss: 0.00003937
Iteration 85/1000 | Loss: 0.00003937
Iteration 86/1000 | Loss: 0.00003937
Iteration 87/1000 | Loss: 0.00003936
Iteration 88/1000 | Loss: 0.00003936
Iteration 89/1000 | Loss: 0.00003936
Iteration 90/1000 | Loss: 0.00003936
Iteration 91/1000 | Loss: 0.00003935
Iteration 92/1000 | Loss: 0.00003935
Iteration 93/1000 | Loss: 0.00003935
Iteration 94/1000 | Loss: 0.00003935
Iteration 95/1000 | Loss: 0.00003935
Iteration 96/1000 | Loss: 0.00003935
Iteration 97/1000 | Loss: 0.00003935
Iteration 98/1000 | Loss: 0.00003935
Iteration 99/1000 | Loss: 0.00003935
Iteration 100/1000 | Loss: 0.00003935
Iteration 101/1000 | Loss: 0.00003934
Iteration 102/1000 | Loss: 0.00003934
Iteration 103/1000 | Loss: 0.00003934
Iteration 104/1000 | Loss: 0.00003934
Iteration 105/1000 | Loss: 0.00003934
Iteration 106/1000 | Loss: 0.00003934
Iteration 107/1000 | Loss: 0.00003934
Iteration 108/1000 | Loss: 0.00003934
Iteration 109/1000 | Loss: 0.00003934
Iteration 110/1000 | Loss: 0.00003933
Iteration 111/1000 | Loss: 0.00003933
Iteration 112/1000 | Loss: 0.00003933
Iteration 113/1000 | Loss: 0.00003932
Iteration 114/1000 | Loss: 0.00003932
Iteration 115/1000 | Loss: 0.00003931
Iteration 116/1000 | Loss: 0.00003931
Iteration 117/1000 | Loss: 0.00003931
Iteration 118/1000 | Loss: 0.00003930
Iteration 119/1000 | Loss: 0.00003930
Iteration 120/1000 | Loss: 0.00003929
Iteration 121/1000 | Loss: 0.00003929
Iteration 122/1000 | Loss: 0.00003929
Iteration 123/1000 | Loss: 0.00003927
Iteration 124/1000 | Loss: 0.00003927
Iteration 125/1000 | Loss: 0.00003927
Iteration 126/1000 | Loss: 0.00003927
Iteration 127/1000 | Loss: 0.00003927
Iteration 128/1000 | Loss: 0.00003927
Iteration 129/1000 | Loss: 0.00003927
Iteration 130/1000 | Loss: 0.00003927
Iteration 131/1000 | Loss: 0.00003927
Iteration 132/1000 | Loss: 0.00003927
Iteration 133/1000 | Loss: 0.00003927
Iteration 134/1000 | Loss: 0.00003927
Iteration 135/1000 | Loss: 0.00003926
Iteration 136/1000 | Loss: 0.00003926
Iteration 137/1000 | Loss: 0.00003926
Iteration 138/1000 | Loss: 0.00003926
Iteration 139/1000 | Loss: 0.00003926
Iteration 140/1000 | Loss: 0.00003926
Iteration 141/1000 | Loss: 0.00003924
Iteration 142/1000 | Loss: 0.00003924
Iteration 143/1000 | Loss: 0.00003924
Iteration 144/1000 | Loss: 0.00003924
Iteration 145/1000 | Loss: 0.00003924
Iteration 146/1000 | Loss: 0.00003924
Iteration 147/1000 | Loss: 0.00003924
Iteration 148/1000 | Loss: 0.00003924
Iteration 149/1000 | Loss: 0.00003924
Iteration 150/1000 | Loss: 0.00003924
Iteration 151/1000 | Loss: 0.00003924
Iteration 152/1000 | Loss: 0.00003924
Iteration 153/1000 | Loss: 0.00003923
Iteration 154/1000 | Loss: 0.00003923
Iteration 155/1000 | Loss: 0.00003923
Iteration 156/1000 | Loss: 0.00003923
Iteration 157/1000 | Loss: 0.00003923
Iteration 158/1000 | Loss: 0.00003923
Iteration 159/1000 | Loss: 0.00003922
Iteration 160/1000 | Loss: 0.00003921
Iteration 161/1000 | Loss: 0.00003921
Iteration 162/1000 | Loss: 0.00003921
Iteration 163/1000 | Loss: 0.00003921
Iteration 164/1000 | Loss: 0.00003921
Iteration 165/1000 | Loss: 0.00003921
Iteration 166/1000 | Loss: 0.00003920
Iteration 167/1000 | Loss: 0.00003920
Iteration 168/1000 | Loss: 0.00003920
Iteration 169/1000 | Loss: 0.00003920
Iteration 170/1000 | Loss: 0.00003919
Iteration 171/1000 | Loss: 0.00003919
Iteration 172/1000 | Loss: 0.00003919
Iteration 173/1000 | Loss: 0.00003918
Iteration 174/1000 | Loss: 0.00003918
Iteration 175/1000 | Loss: 0.00003918
Iteration 176/1000 | Loss: 0.00003918
Iteration 177/1000 | Loss: 0.00003918
Iteration 178/1000 | Loss: 0.00003918
Iteration 179/1000 | Loss: 0.00003918
Iteration 180/1000 | Loss: 0.00003918
Iteration 181/1000 | Loss: 0.00003918
Iteration 182/1000 | Loss: 0.00003918
Iteration 183/1000 | Loss: 0.00003918
Iteration 184/1000 | Loss: 0.00003918
Iteration 185/1000 | Loss: 0.00003918
Iteration 186/1000 | Loss: 0.00003918
Iteration 187/1000 | Loss: 0.00003918
Iteration 188/1000 | Loss: 0.00003917
Iteration 189/1000 | Loss: 0.00003917
Iteration 190/1000 | Loss: 0.00003917
Iteration 191/1000 | Loss: 0.00003917
Iteration 192/1000 | Loss: 0.00003917
Iteration 193/1000 | Loss: 0.00003917
Iteration 194/1000 | Loss: 0.00003916
Iteration 195/1000 | Loss: 0.00003916
Iteration 196/1000 | Loss: 0.00003916
Iteration 197/1000 | Loss: 0.00003916
Iteration 198/1000 | Loss: 0.00003915
Iteration 199/1000 | Loss: 0.00003915
Iteration 200/1000 | Loss: 0.00003915
Iteration 201/1000 | Loss: 0.00003915
Iteration 202/1000 | Loss: 0.00003915
Iteration 203/1000 | Loss: 0.00003914
Iteration 204/1000 | Loss: 0.00003914
Iteration 205/1000 | Loss: 0.00003914
Iteration 206/1000 | Loss: 0.00003914
Iteration 207/1000 | Loss: 0.00003914
Iteration 208/1000 | Loss: 0.00003914
Iteration 209/1000 | Loss: 0.00003914
Iteration 210/1000 | Loss: 0.00003913
Iteration 211/1000 | Loss: 0.00003913
Iteration 212/1000 | Loss: 0.00003913
Iteration 213/1000 | Loss: 0.00003913
Iteration 214/1000 | Loss: 0.00003913
Iteration 215/1000 | Loss: 0.00003913
Iteration 216/1000 | Loss: 0.00003913
Iteration 217/1000 | Loss: 0.00003913
Iteration 218/1000 | Loss: 0.00003913
Iteration 219/1000 | Loss: 0.00003913
Iteration 220/1000 | Loss: 0.00003913
Iteration 221/1000 | Loss: 0.00003913
Iteration 222/1000 | Loss: 0.00003913
Iteration 223/1000 | Loss: 0.00003913
Iteration 224/1000 | Loss: 0.00003913
Iteration 225/1000 | Loss: 0.00003912
Iteration 226/1000 | Loss: 0.00003912
Iteration 227/1000 | Loss: 0.00003912
Iteration 228/1000 | Loss: 0.00003912
Iteration 229/1000 | Loss: 0.00003912
Iteration 230/1000 | Loss: 0.00003912
Iteration 231/1000 | Loss: 0.00003912
Iteration 232/1000 | Loss: 0.00003912
Iteration 233/1000 | Loss: 0.00003912
Iteration 234/1000 | Loss: 0.00003912
Iteration 235/1000 | Loss: 0.00003912
Iteration 236/1000 | Loss: 0.00003912
Iteration 237/1000 | Loss: 0.00003911
Iteration 238/1000 | Loss: 0.00003911
Iteration 239/1000 | Loss: 0.00003911
Iteration 240/1000 | Loss: 0.00003911
Iteration 241/1000 | Loss: 0.00003911
Iteration 242/1000 | Loss: 0.00003911
Iteration 243/1000 | Loss: 0.00003911
Iteration 244/1000 | Loss: 0.00003911
Iteration 245/1000 | Loss: 0.00003911
Iteration 246/1000 | Loss: 0.00003911
Iteration 247/1000 | Loss: 0.00003911
Iteration 248/1000 | Loss: 0.00003911
Iteration 249/1000 | Loss: 0.00003911
Iteration 250/1000 | Loss: 0.00003911
Iteration 251/1000 | Loss: 0.00003911
Iteration 252/1000 | Loss: 0.00003911
Iteration 253/1000 | Loss: 0.00003911
Iteration 254/1000 | Loss: 0.00003911
Iteration 255/1000 | Loss: 0.00003911
Iteration 256/1000 | Loss: 0.00003911
Iteration 257/1000 | Loss: 0.00003911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [3.911116436938755e-05, 3.911116436938755e-05, 3.911116436938755e-05, 3.911116436938755e-05, 3.911116436938755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.911116436938755e-05

Optimization complete. Final v2v error: 5.187743663787842 mm

Highest mean error: 9.699762344360352 mm for frame 212

Lowest mean error: 4.466327667236328 mm for frame 239

Saving results

Total time: 129.4518597126007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739102
Iteration 2/25 | Loss: 0.00115936
Iteration 3/25 | Loss: 0.00095739
Iteration 4/25 | Loss: 0.00089725
Iteration 5/25 | Loss: 0.00087919
Iteration 6/25 | Loss: 0.00087678
Iteration 7/25 | Loss: 0.00087615
Iteration 8/25 | Loss: 0.00087597
Iteration 9/25 | Loss: 0.00087595
Iteration 10/25 | Loss: 0.00087594
Iteration 11/25 | Loss: 0.00087594
Iteration 12/25 | Loss: 0.00087594
Iteration 13/25 | Loss: 0.00087594
Iteration 14/25 | Loss: 0.00087594
Iteration 15/25 | Loss: 0.00087594
Iteration 16/25 | Loss: 0.00087594
Iteration 17/25 | Loss: 0.00087594
Iteration 18/25 | Loss: 0.00087594
Iteration 19/25 | Loss: 0.00087594
Iteration 20/25 | Loss: 0.00087594
Iteration 21/25 | Loss: 0.00087593
Iteration 22/25 | Loss: 0.00087593
Iteration 23/25 | Loss: 0.00087593
Iteration 24/25 | Loss: 0.00087593
Iteration 25/25 | Loss: 0.00087593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55050206
Iteration 2/25 | Loss: 0.00160664
Iteration 3/25 | Loss: 0.00160664
Iteration 4/25 | Loss: 0.00160664
Iteration 5/25 | Loss: 0.00160664
Iteration 6/25 | Loss: 0.00160664
Iteration 7/25 | Loss: 0.00160664
Iteration 8/25 | Loss: 0.00160664
Iteration 9/25 | Loss: 0.00160664
Iteration 10/25 | Loss: 0.00160664
Iteration 11/25 | Loss: 0.00160664
Iteration 12/25 | Loss: 0.00160664
Iteration 13/25 | Loss: 0.00160664
Iteration 14/25 | Loss: 0.00160664
Iteration 15/25 | Loss: 0.00160664
Iteration 16/25 | Loss: 0.00160664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016066371463239193, 0.0016066371463239193, 0.0016066371463239193, 0.0016066371463239193, 0.0016066371463239193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016066371463239193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160664
Iteration 2/1000 | Loss: 0.00002553
Iteration 3/1000 | Loss: 0.00001896
Iteration 4/1000 | Loss: 0.00001706
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001596
Iteration 7/1000 | Loss: 0.00001586
Iteration 8/1000 | Loss: 0.00001568
Iteration 9/1000 | Loss: 0.00001553
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001550
Iteration 12/1000 | Loss: 0.00001549
Iteration 13/1000 | Loss: 0.00001549
Iteration 14/1000 | Loss: 0.00001548
Iteration 15/1000 | Loss: 0.00001547
Iteration 16/1000 | Loss: 0.00001545
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001539
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001539
Iteration 22/1000 | Loss: 0.00001539
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001539
Iteration 25/1000 | Loss: 0.00001539
Iteration 26/1000 | Loss: 0.00001538
Iteration 27/1000 | Loss: 0.00001538
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001538
Iteration 30/1000 | Loss: 0.00001538
Iteration 31/1000 | Loss: 0.00001538
Iteration 32/1000 | Loss: 0.00001538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 32. Stopping optimization.
Last 5 losses: [1.5381581761175767e-05, 1.5381581761175767e-05, 1.5381581761175767e-05, 1.5381581761175767e-05, 1.5381581761175767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5381581761175767e-05

Optimization complete. Final v2v error: 3.372735023498535 mm

Highest mean error: 3.6930360794067383 mm for frame 181

Lowest mean error: 3.1041433811187744 mm for frame 160

Saving results

Total time: 31.31323742866516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892458
Iteration 2/25 | Loss: 0.00119507
Iteration 3/25 | Loss: 0.00100429
Iteration 4/25 | Loss: 0.00099078
Iteration 5/25 | Loss: 0.00098546
Iteration 6/25 | Loss: 0.00098431
Iteration 7/25 | Loss: 0.00098431
Iteration 8/25 | Loss: 0.00098431
Iteration 9/25 | Loss: 0.00098431
Iteration 10/25 | Loss: 0.00098431
Iteration 11/25 | Loss: 0.00098431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00098431168589741, 0.00098431168589741, 0.00098431168589741, 0.00098431168589741, 0.00098431168589741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00098431168589741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.73343945
Iteration 2/25 | Loss: 0.00173060
Iteration 3/25 | Loss: 0.00173048
Iteration 4/25 | Loss: 0.00173048
Iteration 5/25 | Loss: 0.00173048
Iteration 6/25 | Loss: 0.00173048
Iteration 7/25 | Loss: 0.00173048
Iteration 8/25 | Loss: 0.00173048
Iteration 9/25 | Loss: 0.00173048
Iteration 10/25 | Loss: 0.00173048
Iteration 11/25 | Loss: 0.00173048
Iteration 12/25 | Loss: 0.00173048
Iteration 13/25 | Loss: 0.00173048
Iteration 14/25 | Loss: 0.00173048
Iteration 15/25 | Loss: 0.00173048
Iteration 16/25 | Loss: 0.00173048
Iteration 17/25 | Loss: 0.00173048
Iteration 18/25 | Loss: 0.00173048
Iteration 19/25 | Loss: 0.00173048
Iteration 20/25 | Loss: 0.00173048
Iteration 21/25 | Loss: 0.00173048
Iteration 22/25 | Loss: 0.00173048
Iteration 23/25 | Loss: 0.00173048
Iteration 24/25 | Loss: 0.00173048
Iteration 25/25 | Loss: 0.00173048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173048
Iteration 2/1000 | Loss: 0.00005307
Iteration 3/1000 | Loss: 0.00004072
Iteration 4/1000 | Loss: 0.00003507
Iteration 5/1000 | Loss: 0.00003274
Iteration 6/1000 | Loss: 0.00003161
Iteration 7/1000 | Loss: 0.00003089
Iteration 8/1000 | Loss: 0.00003001
Iteration 9/1000 | Loss: 0.00002949
Iteration 10/1000 | Loss: 0.00002919
Iteration 11/1000 | Loss: 0.00002897
Iteration 12/1000 | Loss: 0.00002892
Iteration 13/1000 | Loss: 0.00002888
Iteration 14/1000 | Loss: 0.00002882
Iteration 15/1000 | Loss: 0.00002881
Iteration 16/1000 | Loss: 0.00002881
Iteration 17/1000 | Loss: 0.00002880
Iteration 18/1000 | Loss: 0.00002878
Iteration 19/1000 | Loss: 0.00002877
Iteration 20/1000 | Loss: 0.00002877
Iteration 21/1000 | Loss: 0.00002877
Iteration 22/1000 | Loss: 0.00002875
Iteration 23/1000 | Loss: 0.00002875
Iteration 24/1000 | Loss: 0.00002873
Iteration 25/1000 | Loss: 0.00002873
Iteration 26/1000 | Loss: 0.00002873
Iteration 27/1000 | Loss: 0.00002873
Iteration 28/1000 | Loss: 0.00002873
Iteration 29/1000 | Loss: 0.00002873
Iteration 30/1000 | Loss: 0.00002873
Iteration 31/1000 | Loss: 0.00002873
Iteration 32/1000 | Loss: 0.00002873
Iteration 33/1000 | Loss: 0.00002872
Iteration 34/1000 | Loss: 0.00002871
Iteration 35/1000 | Loss: 0.00002870
Iteration 36/1000 | Loss: 0.00002870
Iteration 37/1000 | Loss: 0.00002870
Iteration 38/1000 | Loss: 0.00002870
Iteration 39/1000 | Loss: 0.00002869
Iteration 40/1000 | Loss: 0.00002869
Iteration 41/1000 | Loss: 0.00002869
Iteration 42/1000 | Loss: 0.00002869
Iteration 43/1000 | Loss: 0.00002868
Iteration 44/1000 | Loss: 0.00002867
Iteration 45/1000 | Loss: 0.00002867
Iteration 46/1000 | Loss: 0.00002867
Iteration 47/1000 | Loss: 0.00002867
Iteration 48/1000 | Loss: 0.00002866
Iteration 49/1000 | Loss: 0.00002866
Iteration 50/1000 | Loss: 0.00002865
Iteration 51/1000 | Loss: 0.00002864
Iteration 52/1000 | Loss: 0.00002864
Iteration 53/1000 | Loss: 0.00002864
Iteration 54/1000 | Loss: 0.00002863
Iteration 55/1000 | Loss: 0.00002863
Iteration 56/1000 | Loss: 0.00002863
Iteration 57/1000 | Loss: 0.00002863
Iteration 58/1000 | Loss: 0.00002862
Iteration 59/1000 | Loss: 0.00002862
Iteration 60/1000 | Loss: 0.00002862
Iteration 61/1000 | Loss: 0.00002862
Iteration 62/1000 | Loss: 0.00002862
Iteration 63/1000 | Loss: 0.00002860
Iteration 64/1000 | Loss: 0.00002859
Iteration 65/1000 | Loss: 0.00002859
Iteration 66/1000 | Loss: 0.00002859
Iteration 67/1000 | Loss: 0.00002858
Iteration 68/1000 | Loss: 0.00002858
Iteration 69/1000 | Loss: 0.00002857
Iteration 70/1000 | Loss: 0.00002857
Iteration 71/1000 | Loss: 0.00002856
Iteration 72/1000 | Loss: 0.00002856
Iteration 73/1000 | Loss: 0.00002856
Iteration 74/1000 | Loss: 0.00002855
Iteration 75/1000 | Loss: 0.00002855
Iteration 76/1000 | Loss: 0.00002855
Iteration 77/1000 | Loss: 0.00002855
Iteration 78/1000 | Loss: 0.00002855
Iteration 79/1000 | Loss: 0.00002855
Iteration 80/1000 | Loss: 0.00002855
Iteration 81/1000 | Loss: 0.00002855
Iteration 82/1000 | Loss: 0.00002855
Iteration 83/1000 | Loss: 0.00002855
Iteration 84/1000 | Loss: 0.00002854
Iteration 85/1000 | Loss: 0.00002854
Iteration 86/1000 | Loss: 0.00002854
Iteration 87/1000 | Loss: 0.00002854
Iteration 88/1000 | Loss: 0.00002854
Iteration 89/1000 | Loss: 0.00002854
Iteration 90/1000 | Loss: 0.00002853
Iteration 91/1000 | Loss: 0.00002853
Iteration 92/1000 | Loss: 0.00002853
Iteration 93/1000 | Loss: 0.00002852
Iteration 94/1000 | Loss: 0.00002852
Iteration 95/1000 | Loss: 0.00002852
Iteration 96/1000 | Loss: 0.00002852
Iteration 97/1000 | Loss: 0.00002852
Iteration 98/1000 | Loss: 0.00002852
Iteration 99/1000 | Loss: 0.00002851
Iteration 100/1000 | Loss: 0.00002851
Iteration 101/1000 | Loss: 0.00002851
Iteration 102/1000 | Loss: 0.00002851
Iteration 103/1000 | Loss: 0.00002851
Iteration 104/1000 | Loss: 0.00002851
Iteration 105/1000 | Loss: 0.00002851
Iteration 106/1000 | Loss: 0.00002851
Iteration 107/1000 | Loss: 0.00002851
Iteration 108/1000 | Loss: 0.00002850
Iteration 109/1000 | Loss: 0.00002850
Iteration 110/1000 | Loss: 0.00002850
Iteration 111/1000 | Loss: 0.00002849
Iteration 112/1000 | Loss: 0.00002849
Iteration 113/1000 | Loss: 0.00002849
Iteration 114/1000 | Loss: 0.00002849
Iteration 115/1000 | Loss: 0.00002849
Iteration 116/1000 | Loss: 0.00002849
Iteration 117/1000 | Loss: 0.00002848
Iteration 118/1000 | Loss: 0.00002848
Iteration 119/1000 | Loss: 0.00002848
Iteration 120/1000 | Loss: 0.00002847
Iteration 121/1000 | Loss: 0.00002847
Iteration 122/1000 | Loss: 0.00002847
Iteration 123/1000 | Loss: 0.00002846
Iteration 124/1000 | Loss: 0.00002846
Iteration 125/1000 | Loss: 0.00002846
Iteration 126/1000 | Loss: 0.00002846
Iteration 127/1000 | Loss: 0.00002846
Iteration 128/1000 | Loss: 0.00002845
Iteration 129/1000 | Loss: 0.00002845
Iteration 130/1000 | Loss: 0.00002845
Iteration 131/1000 | Loss: 0.00002845
Iteration 132/1000 | Loss: 0.00002845
Iteration 133/1000 | Loss: 0.00002844
Iteration 134/1000 | Loss: 0.00002844
Iteration 135/1000 | Loss: 0.00002844
Iteration 136/1000 | Loss: 0.00002844
Iteration 137/1000 | Loss: 0.00002844
Iteration 138/1000 | Loss: 0.00002844
Iteration 139/1000 | Loss: 0.00002844
Iteration 140/1000 | Loss: 0.00002843
Iteration 141/1000 | Loss: 0.00002843
Iteration 142/1000 | Loss: 0.00002843
Iteration 143/1000 | Loss: 0.00002843
Iteration 144/1000 | Loss: 0.00002843
Iteration 145/1000 | Loss: 0.00002843
Iteration 146/1000 | Loss: 0.00002843
Iteration 147/1000 | Loss: 0.00002842
Iteration 148/1000 | Loss: 0.00002842
Iteration 149/1000 | Loss: 0.00002842
Iteration 150/1000 | Loss: 0.00002842
Iteration 151/1000 | Loss: 0.00002842
Iteration 152/1000 | Loss: 0.00002842
Iteration 153/1000 | Loss: 0.00002841
Iteration 154/1000 | Loss: 0.00002841
Iteration 155/1000 | Loss: 0.00002841
Iteration 156/1000 | Loss: 0.00002841
Iteration 157/1000 | Loss: 0.00002841
Iteration 158/1000 | Loss: 0.00002841
Iteration 159/1000 | Loss: 0.00002841
Iteration 160/1000 | Loss: 0.00002841
Iteration 161/1000 | Loss: 0.00002841
Iteration 162/1000 | Loss: 0.00002841
Iteration 163/1000 | Loss: 0.00002841
Iteration 164/1000 | Loss: 0.00002841
Iteration 165/1000 | Loss: 0.00002840
Iteration 166/1000 | Loss: 0.00002840
Iteration 167/1000 | Loss: 0.00002840
Iteration 168/1000 | Loss: 0.00002840
Iteration 169/1000 | Loss: 0.00002840
Iteration 170/1000 | Loss: 0.00002840
Iteration 171/1000 | Loss: 0.00002840
Iteration 172/1000 | Loss: 0.00002840
Iteration 173/1000 | Loss: 0.00002840
Iteration 174/1000 | Loss: 0.00002840
Iteration 175/1000 | Loss: 0.00002840
Iteration 176/1000 | Loss: 0.00002840
Iteration 177/1000 | Loss: 0.00002839
Iteration 178/1000 | Loss: 0.00002839
Iteration 179/1000 | Loss: 0.00002839
Iteration 180/1000 | Loss: 0.00002839
Iteration 181/1000 | Loss: 0.00002839
Iteration 182/1000 | Loss: 0.00002839
Iteration 183/1000 | Loss: 0.00002839
Iteration 184/1000 | Loss: 0.00002839
Iteration 185/1000 | Loss: 0.00002839
Iteration 186/1000 | Loss: 0.00002839
Iteration 187/1000 | Loss: 0.00002839
Iteration 188/1000 | Loss: 0.00002839
Iteration 189/1000 | Loss: 0.00002838
Iteration 190/1000 | Loss: 0.00002838
Iteration 191/1000 | Loss: 0.00002838
Iteration 192/1000 | Loss: 0.00002838
Iteration 193/1000 | Loss: 0.00002838
Iteration 194/1000 | Loss: 0.00002838
Iteration 195/1000 | Loss: 0.00002838
Iteration 196/1000 | Loss: 0.00002838
Iteration 197/1000 | Loss: 0.00002838
Iteration 198/1000 | Loss: 0.00002838
Iteration 199/1000 | Loss: 0.00002838
Iteration 200/1000 | Loss: 0.00002838
Iteration 201/1000 | Loss: 0.00002838
Iteration 202/1000 | Loss: 0.00002838
Iteration 203/1000 | Loss: 0.00002838
Iteration 204/1000 | Loss: 0.00002838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.8376363843563013e-05, 2.8376363843563013e-05, 2.8376363843563013e-05, 2.8376363843563013e-05, 2.8376363843563013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8376363843563013e-05

Optimization complete. Final v2v error: 4.545388221740723 mm

Highest mean error: 5.393095016479492 mm for frame 108

Lowest mean error: 3.8143229484558105 mm for frame 15

Saving results

Total time: 46.140522956848145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388221
Iteration 2/25 | Loss: 0.00099644
Iteration 3/25 | Loss: 0.00090132
Iteration 4/25 | Loss: 0.00088432
Iteration 5/25 | Loss: 0.00087788
Iteration 6/25 | Loss: 0.00087619
Iteration 7/25 | Loss: 0.00087566
Iteration 8/25 | Loss: 0.00087566
Iteration 9/25 | Loss: 0.00087566
Iteration 10/25 | Loss: 0.00087566
Iteration 11/25 | Loss: 0.00087566
Iteration 12/25 | Loss: 0.00087566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008756606257520616, 0.0008756606257520616, 0.0008756606257520616, 0.0008756606257520616, 0.0008756606257520616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008756606257520616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.19159079
Iteration 2/25 | Loss: 0.00158826
Iteration 3/25 | Loss: 0.00158826
Iteration 4/25 | Loss: 0.00158826
Iteration 5/25 | Loss: 0.00158826
Iteration 6/25 | Loss: 0.00158826
Iteration 7/25 | Loss: 0.00158826
Iteration 8/25 | Loss: 0.00158826
Iteration 9/25 | Loss: 0.00158825
Iteration 10/25 | Loss: 0.00158825
Iteration 11/25 | Loss: 0.00158825
Iteration 12/25 | Loss: 0.00158825
Iteration 13/25 | Loss: 0.00158825
Iteration 14/25 | Loss: 0.00158825
Iteration 15/25 | Loss: 0.00158825
Iteration 16/25 | Loss: 0.00158825
Iteration 17/25 | Loss: 0.00158825
Iteration 18/25 | Loss: 0.00158825
Iteration 19/25 | Loss: 0.00158825
Iteration 20/25 | Loss: 0.00158825
Iteration 21/25 | Loss: 0.00158825
Iteration 22/25 | Loss: 0.00158825
Iteration 23/25 | Loss: 0.00158825
Iteration 24/25 | Loss: 0.00158825
Iteration 25/25 | Loss: 0.00158825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158825
Iteration 2/1000 | Loss: 0.00002991
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00002035
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00001882
Iteration 7/1000 | Loss: 0.00001842
Iteration 8/1000 | Loss: 0.00001822
Iteration 9/1000 | Loss: 0.00001813
Iteration 10/1000 | Loss: 0.00001802
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001787
Iteration 13/1000 | Loss: 0.00001787
Iteration 14/1000 | Loss: 0.00001786
Iteration 15/1000 | Loss: 0.00001785
Iteration 16/1000 | Loss: 0.00001785
Iteration 17/1000 | Loss: 0.00001784
Iteration 18/1000 | Loss: 0.00001784
Iteration 19/1000 | Loss: 0.00001783
Iteration 20/1000 | Loss: 0.00001783
Iteration 21/1000 | Loss: 0.00001783
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001781
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00001780
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001780
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001779
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001778
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001778
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001777
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001777
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001776
Iteration 61/1000 | Loss: 0.00001776
Iteration 62/1000 | Loss: 0.00001776
Iteration 63/1000 | Loss: 0.00001776
Iteration 64/1000 | Loss: 0.00001776
Iteration 65/1000 | Loss: 0.00001776
Iteration 66/1000 | Loss: 0.00001776
Iteration 67/1000 | Loss: 0.00001776
Iteration 68/1000 | Loss: 0.00001776
Iteration 69/1000 | Loss: 0.00001776
Iteration 70/1000 | Loss: 0.00001776
Iteration 71/1000 | Loss: 0.00001776
Iteration 72/1000 | Loss: 0.00001775
Iteration 73/1000 | Loss: 0.00001775
Iteration 74/1000 | Loss: 0.00001775
Iteration 75/1000 | Loss: 0.00001775
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001773
Iteration 85/1000 | Loss: 0.00001773
Iteration 86/1000 | Loss: 0.00001773
Iteration 87/1000 | Loss: 0.00001773
Iteration 88/1000 | Loss: 0.00001773
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001771
Iteration 113/1000 | Loss: 0.00001771
Iteration 114/1000 | Loss: 0.00001771
Iteration 115/1000 | Loss: 0.00001771
Iteration 116/1000 | Loss: 0.00001771
Iteration 117/1000 | Loss: 0.00001771
Iteration 118/1000 | Loss: 0.00001771
Iteration 119/1000 | Loss: 0.00001771
Iteration 120/1000 | Loss: 0.00001771
Iteration 121/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.7707223378238268e-05, 1.7707223378238268e-05, 1.7707223378238268e-05, 1.7707223378238268e-05, 1.7707223378238268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7707223378238268e-05

Optimization complete. Final v2v error: 3.549208641052246 mm

Highest mean error: 3.9710066318511963 mm for frame 171

Lowest mean error: 3.030181646347046 mm for frame 123

Saving results

Total time: 32.90161657333374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01168118
Iteration 2/25 | Loss: 0.00552208
Iteration 3/25 | Loss: 0.00349079
Iteration 4/25 | Loss: 0.00271600
Iteration 5/25 | Loss: 0.00236625
Iteration 6/25 | Loss: 0.00221462
Iteration 7/25 | Loss: 0.00205774
Iteration 8/25 | Loss: 0.00190942
Iteration 9/25 | Loss: 0.00175635
Iteration 10/25 | Loss: 0.00163929
Iteration 11/25 | Loss: 0.00155457
Iteration 12/25 | Loss: 0.00153681
Iteration 13/25 | Loss: 0.00151197
Iteration 14/25 | Loss: 0.00148324
Iteration 15/25 | Loss: 0.00141488
Iteration 16/25 | Loss: 0.00133499
Iteration 17/25 | Loss: 0.00131268
Iteration 18/25 | Loss: 0.00128597
Iteration 19/25 | Loss: 0.00126497
Iteration 20/25 | Loss: 0.00125771
Iteration 21/25 | Loss: 0.00124877
Iteration 22/25 | Loss: 0.00123809
Iteration 23/25 | Loss: 0.00124295
Iteration 24/25 | Loss: 0.00122010
Iteration 25/25 | Loss: 0.00121230

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66652799
Iteration 2/25 | Loss: 0.00304068
Iteration 3/25 | Loss: 0.00270568
Iteration 4/25 | Loss: 0.00270568
Iteration 5/25 | Loss: 0.00270568
Iteration 6/25 | Loss: 0.00270568
Iteration 7/25 | Loss: 0.00270568
Iteration 8/25 | Loss: 0.00270568
Iteration 9/25 | Loss: 0.00270568
Iteration 10/25 | Loss: 0.00270568
Iteration 11/25 | Loss: 0.00270568
Iteration 12/25 | Loss: 0.00270568
Iteration 13/25 | Loss: 0.00270568
Iteration 14/25 | Loss: 0.00270568
Iteration 15/25 | Loss: 0.00270568
Iteration 16/25 | Loss: 0.00270568
Iteration 17/25 | Loss: 0.00270568
Iteration 18/25 | Loss: 0.00270568
Iteration 19/25 | Loss: 0.00270568
Iteration 20/25 | Loss: 0.00270568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0027056755498051643, 0.0027056755498051643, 0.0027056755498051643, 0.0027056755498051643, 0.0027056755498051643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027056755498051643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270568
Iteration 2/1000 | Loss: 0.00050864
Iteration 3/1000 | Loss: 0.00012316
Iteration 4/1000 | Loss: 0.00042268
Iteration 5/1000 | Loss: 0.00026499
Iteration 6/1000 | Loss: 0.00021622
Iteration 7/1000 | Loss: 0.00007592
Iteration 8/1000 | Loss: 0.00066344
Iteration 9/1000 | Loss: 0.00316454
Iteration 10/1000 | Loss: 0.00016648
Iteration 11/1000 | Loss: 0.00007068
Iteration 12/1000 | Loss: 0.00005264
Iteration 13/1000 | Loss: 0.00004599
Iteration 14/1000 | Loss: 0.00004112
Iteration 15/1000 | Loss: 0.00070983
Iteration 16/1000 | Loss: 0.00003806
Iteration 17/1000 | Loss: 0.00003504
Iteration 18/1000 | Loss: 0.00003360
Iteration 19/1000 | Loss: 0.00082523
Iteration 20/1000 | Loss: 0.00008685
Iteration 21/1000 | Loss: 0.00013892
Iteration 22/1000 | Loss: 0.00003158
Iteration 23/1000 | Loss: 0.00003056
Iteration 24/1000 | Loss: 0.00002905
Iteration 25/1000 | Loss: 0.00053932
Iteration 26/1000 | Loss: 0.00015407
Iteration 27/1000 | Loss: 0.00003142
Iteration 28/1000 | Loss: 0.00002884
Iteration 29/1000 | Loss: 0.00053946
Iteration 30/1000 | Loss: 0.00004423
Iteration 31/1000 | Loss: 0.00003234
Iteration 32/1000 | Loss: 0.00002764
Iteration 33/1000 | Loss: 0.00002633
Iteration 34/1000 | Loss: 0.00002582
Iteration 35/1000 | Loss: 0.00002569
Iteration 36/1000 | Loss: 0.00002545
Iteration 37/1000 | Loss: 0.00002500
Iteration 38/1000 | Loss: 0.00002488
Iteration 39/1000 | Loss: 0.00002472
Iteration 40/1000 | Loss: 0.00002461
Iteration 41/1000 | Loss: 0.00002461
Iteration 42/1000 | Loss: 0.00002461
Iteration 43/1000 | Loss: 0.00002460
Iteration 44/1000 | Loss: 0.00002460
Iteration 45/1000 | Loss: 0.00002477
Iteration 46/1000 | Loss: 0.00002468
Iteration 47/1000 | Loss: 0.00002461
Iteration 48/1000 | Loss: 0.00002458
Iteration 49/1000 | Loss: 0.00002457
Iteration 50/1000 | Loss: 0.00002455
Iteration 51/1000 | Loss: 0.00002455
Iteration 52/1000 | Loss: 0.00002454
Iteration 53/1000 | Loss: 0.00002453
Iteration 54/1000 | Loss: 0.00002452
Iteration 55/1000 | Loss: 0.00002467
Iteration 56/1000 | Loss: 0.00002466
Iteration 57/1000 | Loss: 0.00002466
Iteration 58/1000 | Loss: 0.00002466
Iteration 59/1000 | Loss: 0.00002447
Iteration 60/1000 | Loss: 0.00002456
Iteration 61/1000 | Loss: 0.00002455
Iteration 62/1000 | Loss: 0.00002448
Iteration 63/1000 | Loss: 0.00002448
Iteration 64/1000 | Loss: 0.00002453
Iteration 65/1000 | Loss: 0.00002441
Iteration 66/1000 | Loss: 0.00002441
Iteration 67/1000 | Loss: 0.00002441
Iteration 68/1000 | Loss: 0.00002441
Iteration 69/1000 | Loss: 0.00002441
Iteration 70/1000 | Loss: 0.00002441
Iteration 71/1000 | Loss: 0.00002441
Iteration 72/1000 | Loss: 0.00002441
Iteration 73/1000 | Loss: 0.00002441
Iteration 74/1000 | Loss: 0.00002441
Iteration 75/1000 | Loss: 0.00002441
Iteration 76/1000 | Loss: 0.00002441
Iteration 77/1000 | Loss: 0.00002445
Iteration 78/1000 | Loss: 0.00002445
Iteration 79/1000 | Loss: 0.00002447
Iteration 80/1000 | Loss: 0.00002442
Iteration 81/1000 | Loss: 0.00002441
Iteration 82/1000 | Loss: 0.00002445
Iteration 83/1000 | Loss: 0.00002438
Iteration 84/1000 | Loss: 0.00002438
Iteration 85/1000 | Loss: 0.00002438
Iteration 86/1000 | Loss: 0.00002441
Iteration 87/1000 | Loss: 0.00002444
Iteration 88/1000 | Loss: 0.00002444
Iteration 89/1000 | Loss: 0.00002437
Iteration 90/1000 | Loss: 0.00002437
Iteration 91/1000 | Loss: 0.00002437
Iteration 92/1000 | Loss: 0.00002437
Iteration 93/1000 | Loss: 0.00002437
Iteration 94/1000 | Loss: 0.00002437
Iteration 95/1000 | Loss: 0.00002437
Iteration 96/1000 | Loss: 0.00002437
Iteration 97/1000 | Loss: 0.00002437
Iteration 98/1000 | Loss: 0.00002437
Iteration 99/1000 | Loss: 0.00002437
Iteration 100/1000 | Loss: 0.00002443
Iteration 101/1000 | Loss: 0.00002443
Iteration 102/1000 | Loss: 0.00002464
Iteration 103/1000 | Loss: 0.00002462
Iteration 104/1000 | Loss: 0.00002466
Iteration 105/1000 | Loss: 0.00002466
Iteration 106/1000 | Loss: 0.00002456
Iteration 107/1000 | Loss: 0.00002463
Iteration 108/1000 | Loss: 0.00002452
Iteration 109/1000 | Loss: 0.00002444
Iteration 110/1000 | Loss: 0.00002438
Iteration 111/1000 | Loss: 0.00002438
Iteration 112/1000 | Loss: 0.00002438
Iteration 113/1000 | Loss: 0.00002432
Iteration 114/1000 | Loss: 0.00002431
Iteration 115/1000 | Loss: 0.00002431
Iteration 116/1000 | Loss: 0.00002431
Iteration 117/1000 | Loss: 0.00002443
Iteration 118/1000 | Loss: 0.00002442
Iteration 119/1000 | Loss: 0.00002436
Iteration 120/1000 | Loss: 0.00002430
Iteration 121/1000 | Loss: 0.00002441
Iteration 122/1000 | Loss: 0.00002436
Iteration 123/1000 | Loss: 0.00002451
Iteration 124/1000 | Loss: 0.00002457
Iteration 125/1000 | Loss: 0.00002463
Iteration 126/1000 | Loss: 0.00002459
Iteration 127/1000 | Loss: 0.00002464
Iteration 128/1000 | Loss: 0.00002460
Iteration 129/1000 | Loss: 0.00002466
Iteration 130/1000 | Loss: 0.00002461
Iteration 131/1000 | Loss: 0.00002461
Iteration 132/1000 | Loss: 0.00002464
Iteration 133/1000 | Loss: 0.00002464
Iteration 134/1000 | Loss: 0.00002456
Iteration 135/1000 | Loss: 0.00002459
Iteration 136/1000 | Loss: 0.00002456
Iteration 137/1000 | Loss: 0.00002461
Iteration 138/1000 | Loss: 0.00002458
Iteration 139/1000 | Loss: 0.00002460
Iteration 140/1000 | Loss: 0.00002458
Iteration 141/1000 | Loss: 0.00002463
Iteration 142/1000 | Loss: 0.00002458
Iteration 143/1000 | Loss: 0.00002458
Iteration 144/1000 | Loss: 0.00002465
Iteration 145/1000 | Loss: 0.00002465
Iteration 146/1000 | Loss: 0.00002461
Iteration 147/1000 | Loss: 0.00002461
Iteration 148/1000 | Loss: 0.00002462
Iteration 149/1000 | Loss: 0.00002458
Iteration 150/1000 | Loss: 0.00002462
Iteration 151/1000 | Loss: 0.00002458
Iteration 152/1000 | Loss: 0.00002463
Iteration 153/1000 | Loss: 0.00002458
Iteration 154/1000 | Loss: 0.00002457
Iteration 155/1000 | Loss: 0.00002442
Iteration 156/1000 | Loss: 0.00002463
Iteration 157/1000 | Loss: 0.00002451
Iteration 158/1000 | Loss: 0.00002443
Iteration 159/1000 | Loss: 0.00002443
Iteration 160/1000 | Loss: 0.00002442
Iteration 161/1000 | Loss: 0.00002438
Iteration 162/1000 | Loss: 0.00002438
Iteration 163/1000 | Loss: 0.00002433
Iteration 164/1000 | Loss: 0.00002433
Iteration 165/1000 | Loss: 0.00002451
Iteration 166/1000 | Loss: 0.00002451
Iteration 167/1000 | Loss: 0.00002451
Iteration 168/1000 | Loss: 0.00002451
Iteration 169/1000 | Loss: 0.00002441
Iteration 170/1000 | Loss: 0.00002455
Iteration 171/1000 | Loss: 0.00002431
Iteration 172/1000 | Loss: 0.00002431
Iteration 173/1000 | Loss: 0.00002430
Iteration 174/1000 | Loss: 0.00002430
Iteration 175/1000 | Loss: 0.00002453
Iteration 176/1000 | Loss: 0.00002455
Iteration 177/1000 | Loss: 0.00002430
Iteration 178/1000 | Loss: 0.00002447
Iteration 179/1000 | Loss: 0.00002454
Iteration 180/1000 | Loss: 0.00002454
Iteration 181/1000 | Loss: 0.00002430
Iteration 182/1000 | Loss: 0.00002429
Iteration 183/1000 | Loss: 0.00002429
Iteration 184/1000 | Loss: 0.00002429
Iteration 185/1000 | Loss: 0.00002427
Iteration 186/1000 | Loss: 0.00002451
Iteration 187/1000 | Loss: 0.00002439
Iteration 188/1000 | Loss: 0.00002441
Iteration 189/1000 | Loss: 0.00002430
Iteration 190/1000 | Loss: 0.00002429
Iteration 191/1000 | Loss: 0.00002450
Iteration 192/1000 | Loss: 0.00002440
Iteration 193/1000 | Loss: 0.00002442
Iteration 194/1000 | Loss: 0.00002431
Iteration 195/1000 | Loss: 0.00002426
Iteration 196/1000 | Loss: 0.00002426
Iteration 197/1000 | Loss: 0.00002425
Iteration 198/1000 | Loss: 0.00002425
Iteration 199/1000 | Loss: 0.00002425
Iteration 200/1000 | Loss: 0.00002425
Iteration 201/1000 | Loss: 0.00002425
Iteration 202/1000 | Loss: 0.00002425
Iteration 203/1000 | Loss: 0.00002425
Iteration 204/1000 | Loss: 0.00002425
Iteration 205/1000 | Loss: 0.00002425
Iteration 206/1000 | Loss: 0.00002425
Iteration 207/1000 | Loss: 0.00002425
Iteration 208/1000 | Loss: 0.00002425
Iteration 209/1000 | Loss: 0.00002425
Iteration 210/1000 | Loss: 0.00002425
Iteration 211/1000 | Loss: 0.00002425
Iteration 212/1000 | Loss: 0.00002425
Iteration 213/1000 | Loss: 0.00002425
Iteration 214/1000 | Loss: 0.00002425
Iteration 215/1000 | Loss: 0.00002425
Iteration 216/1000 | Loss: 0.00002425
Iteration 217/1000 | Loss: 0.00002425
Iteration 218/1000 | Loss: 0.00002425
Iteration 219/1000 | Loss: 0.00002425
Iteration 220/1000 | Loss: 0.00002425
Iteration 221/1000 | Loss: 0.00002425
Iteration 222/1000 | Loss: 0.00002425
Iteration 223/1000 | Loss: 0.00002425
Iteration 224/1000 | Loss: 0.00002425
Iteration 225/1000 | Loss: 0.00002425
Iteration 226/1000 | Loss: 0.00002425
Iteration 227/1000 | Loss: 0.00002425
Iteration 228/1000 | Loss: 0.00002425
Iteration 229/1000 | Loss: 0.00002425
Iteration 230/1000 | Loss: 0.00002425
Iteration 231/1000 | Loss: 0.00002425
Iteration 232/1000 | Loss: 0.00002425
Iteration 233/1000 | Loss: 0.00002425
Iteration 234/1000 | Loss: 0.00002425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.4252531147794798e-05, 2.4252531147794798e-05, 2.4252531147794798e-05, 2.4252531147794798e-05, 2.4252531147794798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4252531147794798e-05

Optimization complete. Final v2v error: 4.176163196563721 mm

Highest mean error: 5.079294204711914 mm for frame 0

Lowest mean error: 3.9517922401428223 mm for frame 126

Saving results

Total time: 182.4870364665985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077201
Iteration 2/25 | Loss: 0.00476276
Iteration 3/25 | Loss: 0.00281067
Iteration 4/25 | Loss: 0.00220744
Iteration 5/25 | Loss: 0.00187852
Iteration 6/25 | Loss: 0.00170949
Iteration 7/25 | Loss: 0.00159234
Iteration 8/25 | Loss: 0.00155493
Iteration 9/25 | Loss: 0.00148671
Iteration 10/25 | Loss: 0.00138817
Iteration 11/25 | Loss: 0.00136686
Iteration 12/25 | Loss: 0.00134009
Iteration 13/25 | Loss: 0.00132609
Iteration 14/25 | Loss: 0.00131665
Iteration 15/25 | Loss: 0.00130753
Iteration 16/25 | Loss: 0.00128622
Iteration 17/25 | Loss: 0.00127784
Iteration 18/25 | Loss: 0.00127905
Iteration 19/25 | Loss: 0.00127192
Iteration 20/25 | Loss: 0.00125135
Iteration 21/25 | Loss: 0.00123741
Iteration 22/25 | Loss: 0.00124102
Iteration 23/25 | Loss: 0.00122869
Iteration 24/25 | Loss: 0.00123224
Iteration 25/25 | Loss: 0.00124047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65824163
Iteration 2/25 | Loss: 0.00512927
Iteration 3/25 | Loss: 0.00442838
Iteration 4/25 | Loss: 0.00442838
Iteration 5/25 | Loss: 0.00442837
Iteration 6/25 | Loss: 0.00442837
Iteration 7/25 | Loss: 0.00442837
Iteration 8/25 | Loss: 0.00442837
Iteration 9/25 | Loss: 0.00442837
Iteration 10/25 | Loss: 0.00442837
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.004428372252732515, 0.004428372252732515, 0.004428372252732515, 0.004428372252732515, 0.004428372252732515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004428372252732515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00442837
Iteration 2/1000 | Loss: 0.00176110
Iteration 3/1000 | Loss: 0.00196125
Iteration 4/1000 | Loss: 0.00102066
Iteration 5/1000 | Loss: 0.00051638
Iteration 6/1000 | Loss: 0.00294065
Iteration 7/1000 | Loss: 0.00054366
Iteration 8/1000 | Loss: 0.00086641
Iteration 9/1000 | Loss: 0.00368176
Iteration 10/1000 | Loss: 0.00130580
Iteration 11/1000 | Loss: 0.00155135
Iteration 12/1000 | Loss: 0.00275581
Iteration 13/1000 | Loss: 0.00082868
Iteration 14/1000 | Loss: 0.00196210
Iteration 15/1000 | Loss: 0.00065789
Iteration 16/1000 | Loss: 0.00111206
Iteration 17/1000 | Loss: 0.00332343
Iteration 18/1000 | Loss: 0.00172694
Iteration 19/1000 | Loss: 0.00090786
Iteration 20/1000 | Loss: 0.00053969
Iteration 21/1000 | Loss: 0.00222116
Iteration 22/1000 | Loss: 0.00068147
Iteration 23/1000 | Loss: 0.00130502
Iteration 24/1000 | Loss: 0.00235286
Iteration 25/1000 | Loss: 0.00164089
Iteration 26/1000 | Loss: 0.00353752
Iteration 27/1000 | Loss: 0.00191469
Iteration 28/1000 | Loss: 0.00082018
Iteration 29/1000 | Loss: 0.00066192
Iteration 30/1000 | Loss: 0.00067778
Iteration 31/1000 | Loss: 0.00069929
Iteration 32/1000 | Loss: 0.00092707
Iteration 33/1000 | Loss: 0.00029793
Iteration 34/1000 | Loss: 0.00067396
Iteration 35/1000 | Loss: 0.00041042
Iteration 36/1000 | Loss: 0.00095832
Iteration 37/1000 | Loss: 0.00193433
Iteration 38/1000 | Loss: 0.00019896
Iteration 39/1000 | Loss: 0.00022820
Iteration 40/1000 | Loss: 0.00072308
Iteration 41/1000 | Loss: 0.00361890
Iteration 42/1000 | Loss: 0.00317916
Iteration 43/1000 | Loss: 0.00121733
Iteration 44/1000 | Loss: 0.00169309
Iteration 45/1000 | Loss: 0.00053944
Iteration 46/1000 | Loss: 0.00021940
Iteration 47/1000 | Loss: 0.00049630
Iteration 48/1000 | Loss: 0.00012768
Iteration 49/1000 | Loss: 0.00137886
Iteration 50/1000 | Loss: 0.00095029
Iteration 51/1000 | Loss: 0.00166193
Iteration 52/1000 | Loss: 0.00037835
Iteration 53/1000 | Loss: 0.00024973
Iteration 54/1000 | Loss: 0.00047098
Iteration 55/1000 | Loss: 0.00009224
Iteration 56/1000 | Loss: 0.00013387
Iteration 57/1000 | Loss: 0.00008759
Iteration 58/1000 | Loss: 0.00007228
Iteration 59/1000 | Loss: 0.00040870
Iteration 60/1000 | Loss: 0.00015512
Iteration 61/1000 | Loss: 0.00013924
Iteration 62/1000 | Loss: 0.00009722
Iteration 63/1000 | Loss: 0.00065436
Iteration 64/1000 | Loss: 0.00022089
Iteration 65/1000 | Loss: 0.00012244
Iteration 66/1000 | Loss: 0.00007966
Iteration 67/1000 | Loss: 0.00015766
Iteration 68/1000 | Loss: 0.00015333
Iteration 69/1000 | Loss: 0.00013147
Iteration 70/1000 | Loss: 0.00007003
Iteration 71/1000 | Loss: 0.00005988
Iteration 72/1000 | Loss: 0.00024840
Iteration 73/1000 | Loss: 0.00023673
Iteration 74/1000 | Loss: 0.00024537
Iteration 75/1000 | Loss: 0.00005978
Iteration 76/1000 | Loss: 0.00009747
Iteration 77/1000 | Loss: 0.00006247
Iteration 78/1000 | Loss: 0.00007275
Iteration 79/1000 | Loss: 0.00028574
Iteration 80/1000 | Loss: 0.00052663
Iteration 81/1000 | Loss: 0.00009684
Iteration 82/1000 | Loss: 0.00017828
Iteration 83/1000 | Loss: 0.00006778
Iteration 84/1000 | Loss: 0.00018916
Iteration 85/1000 | Loss: 0.00005526
Iteration 86/1000 | Loss: 0.00005690
Iteration 87/1000 | Loss: 0.00020779
Iteration 88/1000 | Loss: 0.00013196
Iteration 89/1000 | Loss: 0.00005169
Iteration 90/1000 | Loss: 0.00012054
Iteration 91/1000 | Loss: 0.00006983
Iteration 92/1000 | Loss: 0.00006522
Iteration 93/1000 | Loss: 0.00026377
Iteration 94/1000 | Loss: 0.00006469
Iteration 95/1000 | Loss: 0.00009406
Iteration 96/1000 | Loss: 0.00005780
Iteration 97/1000 | Loss: 0.00007098
Iteration 98/1000 | Loss: 0.00014874
Iteration 99/1000 | Loss: 0.00005273
Iteration 100/1000 | Loss: 0.00006421
Iteration 101/1000 | Loss: 0.00004843
Iteration 102/1000 | Loss: 0.00009940
Iteration 103/1000 | Loss: 0.00005577
Iteration 104/1000 | Loss: 0.00005671
Iteration 105/1000 | Loss: 0.00005259
Iteration 106/1000 | Loss: 0.00005457
Iteration 107/1000 | Loss: 0.00038970
Iteration 108/1000 | Loss: 0.00008971
Iteration 109/1000 | Loss: 0.00016866
Iteration 110/1000 | Loss: 0.00006684
Iteration 111/1000 | Loss: 0.00006771
Iteration 112/1000 | Loss: 0.00005727
Iteration 113/1000 | Loss: 0.00006315
Iteration 114/1000 | Loss: 0.00010344
Iteration 115/1000 | Loss: 0.00009203
Iteration 116/1000 | Loss: 0.00006422
Iteration 117/1000 | Loss: 0.00012032
Iteration 118/1000 | Loss: 0.00010471
Iteration 119/1000 | Loss: 0.00011211
Iteration 120/1000 | Loss: 0.00006381
Iteration 121/1000 | Loss: 0.00006200
Iteration 122/1000 | Loss: 0.00007815
Iteration 123/1000 | Loss: 0.00005501
Iteration 124/1000 | Loss: 0.00005870
Iteration 125/1000 | Loss: 0.00006361
Iteration 126/1000 | Loss: 0.00005372
Iteration 127/1000 | Loss: 0.00007402
Iteration 128/1000 | Loss: 0.00005488
Iteration 129/1000 | Loss: 0.00005558
Iteration 130/1000 | Loss: 0.00005390
Iteration 131/1000 | Loss: 0.00028634
Iteration 132/1000 | Loss: 0.00010216
Iteration 133/1000 | Loss: 0.00009966
Iteration 134/1000 | Loss: 0.00008905
Iteration 135/1000 | Loss: 0.00006129
Iteration 136/1000 | Loss: 0.00005088
Iteration 137/1000 | Loss: 0.00005620
Iteration 138/1000 | Loss: 0.00009644
Iteration 139/1000 | Loss: 0.00007092
Iteration 140/1000 | Loss: 0.00008119
Iteration 141/1000 | Loss: 0.00005563
Iteration 142/1000 | Loss: 0.00006019
Iteration 143/1000 | Loss: 0.00006616
Iteration 144/1000 | Loss: 0.00005552
Iteration 145/1000 | Loss: 0.00006047
Iteration 146/1000 | Loss: 0.00010643
Iteration 147/1000 | Loss: 0.00008175
Iteration 148/1000 | Loss: 0.00004988
Iteration 149/1000 | Loss: 0.00006556
Iteration 150/1000 | Loss: 0.00004046
Iteration 151/1000 | Loss: 0.00022622
Iteration 152/1000 | Loss: 0.00041585
Iteration 153/1000 | Loss: 0.00031116
Iteration 154/1000 | Loss: 0.00036864
Iteration 155/1000 | Loss: 0.00039124
Iteration 156/1000 | Loss: 0.00041776
Iteration 157/1000 | Loss: 0.00007400
Iteration 158/1000 | Loss: 0.00003701
Iteration 159/1000 | Loss: 0.00010786
Iteration 160/1000 | Loss: 0.00004095
Iteration 161/1000 | Loss: 0.00003424
Iteration 162/1000 | Loss: 0.00003350
Iteration 163/1000 | Loss: 0.00003311
Iteration 164/1000 | Loss: 0.00003294
Iteration 165/1000 | Loss: 0.00003292
Iteration 166/1000 | Loss: 0.00003285
Iteration 167/1000 | Loss: 0.00003283
Iteration 168/1000 | Loss: 0.00003282
Iteration 169/1000 | Loss: 0.00003282
Iteration 170/1000 | Loss: 0.00003282
Iteration 171/1000 | Loss: 0.00003282
Iteration 172/1000 | Loss: 0.00003282
Iteration 173/1000 | Loss: 0.00003282
Iteration 174/1000 | Loss: 0.00003282
Iteration 175/1000 | Loss: 0.00003282
Iteration 176/1000 | Loss: 0.00003280
Iteration 177/1000 | Loss: 0.00003279
Iteration 178/1000 | Loss: 0.00003274
Iteration 179/1000 | Loss: 0.00003274
Iteration 180/1000 | Loss: 0.00003273
Iteration 181/1000 | Loss: 0.00003273
Iteration 182/1000 | Loss: 0.00003272
Iteration 183/1000 | Loss: 0.00003272
Iteration 184/1000 | Loss: 0.00003272
Iteration 185/1000 | Loss: 0.00003272
Iteration 186/1000 | Loss: 0.00003271
Iteration 187/1000 | Loss: 0.00003271
Iteration 188/1000 | Loss: 0.00003271
Iteration 189/1000 | Loss: 0.00003270
Iteration 190/1000 | Loss: 0.00003270
Iteration 191/1000 | Loss: 0.00003269
Iteration 192/1000 | Loss: 0.00003269
Iteration 193/1000 | Loss: 0.00003269
Iteration 194/1000 | Loss: 0.00003269
Iteration 195/1000 | Loss: 0.00003269
Iteration 196/1000 | Loss: 0.00003268
Iteration 197/1000 | Loss: 0.00003268
Iteration 198/1000 | Loss: 0.00003268
Iteration 199/1000 | Loss: 0.00003267
Iteration 200/1000 | Loss: 0.00003267
Iteration 201/1000 | Loss: 0.00003267
Iteration 202/1000 | Loss: 0.00003267
Iteration 203/1000 | Loss: 0.00003266
Iteration 204/1000 | Loss: 0.00003266
Iteration 205/1000 | Loss: 0.00003266
Iteration 206/1000 | Loss: 0.00003266
Iteration 207/1000 | Loss: 0.00003266
Iteration 208/1000 | Loss: 0.00003266
Iteration 209/1000 | Loss: 0.00003266
Iteration 210/1000 | Loss: 0.00003266
Iteration 211/1000 | Loss: 0.00003266
Iteration 212/1000 | Loss: 0.00003265
Iteration 213/1000 | Loss: 0.00003265
Iteration 214/1000 | Loss: 0.00003265
Iteration 215/1000 | Loss: 0.00003264
Iteration 216/1000 | Loss: 0.00003264
Iteration 217/1000 | Loss: 0.00003264
Iteration 218/1000 | Loss: 0.00003263
Iteration 219/1000 | Loss: 0.00003263
Iteration 220/1000 | Loss: 0.00003263
Iteration 221/1000 | Loss: 0.00003263
Iteration 222/1000 | Loss: 0.00003263
Iteration 223/1000 | Loss: 0.00003262
Iteration 224/1000 | Loss: 0.00003262
Iteration 225/1000 | Loss: 0.00003262
Iteration 226/1000 | Loss: 0.00003261
Iteration 227/1000 | Loss: 0.00003261
Iteration 228/1000 | Loss: 0.00003261
Iteration 229/1000 | Loss: 0.00003261
Iteration 230/1000 | Loss: 0.00003261
Iteration 231/1000 | Loss: 0.00003260
Iteration 232/1000 | Loss: 0.00003260
Iteration 233/1000 | Loss: 0.00003260
Iteration 234/1000 | Loss: 0.00003260
Iteration 235/1000 | Loss: 0.00003260
Iteration 236/1000 | Loss: 0.00003260
Iteration 237/1000 | Loss: 0.00003260
Iteration 238/1000 | Loss: 0.00003260
Iteration 239/1000 | Loss: 0.00003260
Iteration 240/1000 | Loss: 0.00003260
Iteration 241/1000 | Loss: 0.00003260
Iteration 242/1000 | Loss: 0.00003260
Iteration 243/1000 | Loss: 0.00003259
Iteration 244/1000 | Loss: 0.00003259
Iteration 245/1000 | Loss: 0.00003259
Iteration 246/1000 | Loss: 0.00003259
Iteration 247/1000 | Loss: 0.00003259
Iteration 248/1000 | Loss: 0.00003259
Iteration 249/1000 | Loss: 0.00003259
Iteration 250/1000 | Loss: 0.00003259
Iteration 251/1000 | Loss: 0.00003259
Iteration 252/1000 | Loss: 0.00003258
Iteration 253/1000 | Loss: 0.00003258
Iteration 254/1000 | Loss: 0.00003258
Iteration 255/1000 | Loss: 0.00003258
Iteration 256/1000 | Loss: 0.00003258
Iteration 257/1000 | Loss: 0.00003258
Iteration 258/1000 | Loss: 0.00003258
Iteration 259/1000 | Loss: 0.00003258
Iteration 260/1000 | Loss: 0.00003258
Iteration 261/1000 | Loss: 0.00003258
Iteration 262/1000 | Loss: 0.00003258
Iteration 263/1000 | Loss: 0.00003258
Iteration 264/1000 | Loss: 0.00003258
Iteration 265/1000 | Loss: 0.00003258
Iteration 266/1000 | Loss: 0.00003258
Iteration 267/1000 | Loss: 0.00003258
Iteration 268/1000 | Loss: 0.00003258
Iteration 269/1000 | Loss: 0.00003258
Iteration 270/1000 | Loss: 0.00003258
Iteration 271/1000 | Loss: 0.00003257
Iteration 272/1000 | Loss: 0.00003257
Iteration 273/1000 | Loss: 0.00003257
Iteration 274/1000 | Loss: 0.00003257
Iteration 275/1000 | Loss: 0.00003257
Iteration 276/1000 | Loss: 0.00003257
Iteration 277/1000 | Loss: 0.00003257
Iteration 278/1000 | Loss: 0.00003257
Iteration 279/1000 | Loss: 0.00003257
Iteration 280/1000 | Loss: 0.00003257
Iteration 281/1000 | Loss: 0.00003257
Iteration 282/1000 | Loss: 0.00003257
Iteration 283/1000 | Loss: 0.00003257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [3.257486969232559e-05, 3.257486969232559e-05, 3.257486969232559e-05, 3.257486969232559e-05, 3.257486969232559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.257486969232559e-05

Optimization complete. Final v2v error: 4.047085762023926 mm

Highest mean error: 13.584829330444336 mm for frame 170

Lowest mean error: 3.4093964099884033 mm for frame 178

Saving results

Total time: 322.1722741127014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126301
Iteration 2/25 | Loss: 0.00181422
Iteration 3/25 | Loss: 0.00134922
Iteration 4/25 | Loss: 0.00121471
Iteration 5/25 | Loss: 0.00112963
Iteration 6/25 | Loss: 0.00107741
Iteration 7/25 | Loss: 0.00106192
Iteration 8/25 | Loss: 0.00103320
Iteration 9/25 | Loss: 0.00101800
Iteration 10/25 | Loss: 0.00100547
Iteration 11/25 | Loss: 0.00100400
Iteration 12/25 | Loss: 0.00099582
Iteration 13/25 | Loss: 0.00099340
Iteration 14/25 | Loss: 0.00099675
Iteration 15/25 | Loss: 0.00099367
Iteration 16/25 | Loss: 0.00098915
Iteration 17/25 | Loss: 0.00098794
Iteration 18/25 | Loss: 0.00099181
Iteration 19/25 | Loss: 0.00098802
Iteration 20/25 | Loss: 0.00098708
Iteration 21/25 | Loss: 0.00098645
Iteration 22/25 | Loss: 0.00098568
Iteration 23/25 | Loss: 0.00098553
Iteration 24/25 | Loss: 0.00098553
Iteration 25/25 | Loss: 0.00098552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64623904
Iteration 2/25 | Loss: 0.00183881
Iteration 3/25 | Loss: 0.00183881
Iteration 4/25 | Loss: 0.00183881
Iteration 5/25 | Loss: 0.00183881
Iteration 6/25 | Loss: 0.00183881
Iteration 7/25 | Loss: 0.00183881
Iteration 8/25 | Loss: 0.00183881
Iteration 9/25 | Loss: 0.00183881
Iteration 10/25 | Loss: 0.00183881
Iteration 11/25 | Loss: 0.00183881
Iteration 12/25 | Loss: 0.00183881
Iteration 13/25 | Loss: 0.00183881
Iteration 14/25 | Loss: 0.00183881
Iteration 15/25 | Loss: 0.00183881
Iteration 16/25 | Loss: 0.00183881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018388101598247886, 0.0018388101598247886, 0.0018388101598247886, 0.0018388101598247886, 0.0018388101598247886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018388101598247886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183881
Iteration 2/1000 | Loss: 0.00008010
Iteration 3/1000 | Loss: 0.00005690
Iteration 4/1000 | Loss: 0.00004437
Iteration 5/1000 | Loss: 0.00003955
Iteration 6/1000 | Loss: 0.00003679
Iteration 7/1000 | Loss: 0.00003533
Iteration 8/1000 | Loss: 0.00003422
Iteration 9/1000 | Loss: 0.00135790
Iteration 10/1000 | Loss: 0.00019378
Iteration 11/1000 | Loss: 0.00003788
Iteration 12/1000 | Loss: 0.00002970
Iteration 13/1000 | Loss: 0.00002692
Iteration 14/1000 | Loss: 0.00002579
Iteration 15/1000 | Loss: 0.00002467
Iteration 16/1000 | Loss: 0.00002408
Iteration 17/1000 | Loss: 0.00002371
Iteration 18/1000 | Loss: 0.00002349
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002314
Iteration 21/1000 | Loss: 0.00002307
Iteration 22/1000 | Loss: 0.00002307
Iteration 23/1000 | Loss: 0.00002306
Iteration 24/1000 | Loss: 0.00002304
Iteration 25/1000 | Loss: 0.00002303
Iteration 26/1000 | Loss: 0.00002302
Iteration 27/1000 | Loss: 0.00002301
Iteration 28/1000 | Loss: 0.00002301
Iteration 29/1000 | Loss: 0.00002297
Iteration 30/1000 | Loss: 0.00002293
Iteration 31/1000 | Loss: 0.00002293
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00002291
Iteration 34/1000 | Loss: 0.00002290
Iteration 35/1000 | Loss: 0.00002290
Iteration 36/1000 | Loss: 0.00002290
Iteration 37/1000 | Loss: 0.00002289
Iteration 38/1000 | Loss: 0.00002289
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002286
Iteration 41/1000 | Loss: 0.00002285
Iteration 42/1000 | Loss: 0.00002284
Iteration 43/1000 | Loss: 0.00002284
Iteration 44/1000 | Loss: 0.00002283
Iteration 45/1000 | Loss: 0.00002282
Iteration 46/1000 | Loss: 0.00002282
Iteration 47/1000 | Loss: 0.00002282
Iteration 48/1000 | Loss: 0.00002282
Iteration 49/1000 | Loss: 0.00002282
Iteration 50/1000 | Loss: 0.00002282
Iteration 51/1000 | Loss: 0.00002282
Iteration 52/1000 | Loss: 0.00002282
Iteration 53/1000 | Loss: 0.00002281
Iteration 54/1000 | Loss: 0.00002281
Iteration 55/1000 | Loss: 0.00002281
Iteration 56/1000 | Loss: 0.00002281
Iteration 57/1000 | Loss: 0.00002281
Iteration 58/1000 | Loss: 0.00002281
Iteration 59/1000 | Loss: 0.00002281
Iteration 60/1000 | Loss: 0.00002281
Iteration 61/1000 | Loss: 0.00002280
Iteration 62/1000 | Loss: 0.00002280
Iteration 63/1000 | Loss: 0.00002279
Iteration 64/1000 | Loss: 0.00002279
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002278
Iteration 67/1000 | Loss: 0.00002278
Iteration 68/1000 | Loss: 0.00002277
Iteration 69/1000 | Loss: 0.00002277
Iteration 70/1000 | Loss: 0.00002277
Iteration 71/1000 | Loss: 0.00002277
Iteration 72/1000 | Loss: 0.00002277
Iteration 73/1000 | Loss: 0.00002277
Iteration 74/1000 | Loss: 0.00002276
Iteration 75/1000 | Loss: 0.00002276
Iteration 76/1000 | Loss: 0.00002276
Iteration 77/1000 | Loss: 0.00002276
Iteration 78/1000 | Loss: 0.00002276
Iteration 79/1000 | Loss: 0.00002276
Iteration 80/1000 | Loss: 0.00002276
Iteration 81/1000 | Loss: 0.00002276
Iteration 82/1000 | Loss: 0.00002276
Iteration 83/1000 | Loss: 0.00002276
Iteration 84/1000 | Loss: 0.00002276
Iteration 85/1000 | Loss: 0.00002276
Iteration 86/1000 | Loss: 0.00002276
Iteration 87/1000 | Loss: 0.00002276
Iteration 88/1000 | Loss: 0.00002276
Iteration 89/1000 | Loss: 0.00002276
Iteration 90/1000 | Loss: 0.00002275
Iteration 91/1000 | Loss: 0.00002275
Iteration 92/1000 | Loss: 0.00002275
Iteration 93/1000 | Loss: 0.00002275
Iteration 94/1000 | Loss: 0.00002275
Iteration 95/1000 | Loss: 0.00002275
Iteration 96/1000 | Loss: 0.00002275
Iteration 97/1000 | Loss: 0.00002275
Iteration 98/1000 | Loss: 0.00002275
Iteration 99/1000 | Loss: 0.00002275
Iteration 100/1000 | Loss: 0.00002275
Iteration 101/1000 | Loss: 0.00002275
Iteration 102/1000 | Loss: 0.00002275
Iteration 103/1000 | Loss: 0.00002274
Iteration 104/1000 | Loss: 0.00002274
Iteration 105/1000 | Loss: 0.00002274
Iteration 106/1000 | Loss: 0.00002274
Iteration 107/1000 | Loss: 0.00002273
Iteration 108/1000 | Loss: 0.00002273
Iteration 109/1000 | Loss: 0.00002273
Iteration 110/1000 | Loss: 0.00002273
Iteration 111/1000 | Loss: 0.00002273
Iteration 112/1000 | Loss: 0.00002273
Iteration 113/1000 | Loss: 0.00002272
Iteration 114/1000 | Loss: 0.00002272
Iteration 115/1000 | Loss: 0.00002272
Iteration 116/1000 | Loss: 0.00002272
Iteration 117/1000 | Loss: 0.00002272
Iteration 118/1000 | Loss: 0.00002272
Iteration 119/1000 | Loss: 0.00002272
Iteration 120/1000 | Loss: 0.00002272
Iteration 121/1000 | Loss: 0.00002272
Iteration 122/1000 | Loss: 0.00002272
Iteration 123/1000 | Loss: 0.00002272
Iteration 124/1000 | Loss: 0.00002272
Iteration 125/1000 | Loss: 0.00002272
Iteration 126/1000 | Loss: 0.00002272
Iteration 127/1000 | Loss: 0.00002272
Iteration 128/1000 | Loss: 0.00002272
Iteration 129/1000 | Loss: 0.00002272
Iteration 130/1000 | Loss: 0.00002272
Iteration 131/1000 | Loss: 0.00002272
Iteration 132/1000 | Loss: 0.00002272
Iteration 133/1000 | Loss: 0.00002272
Iteration 134/1000 | Loss: 0.00002272
Iteration 135/1000 | Loss: 0.00002272
Iteration 136/1000 | Loss: 0.00002272
Iteration 137/1000 | Loss: 0.00002272
Iteration 138/1000 | Loss: 0.00002272
Iteration 139/1000 | Loss: 0.00002272
Iteration 140/1000 | Loss: 0.00002272
Iteration 141/1000 | Loss: 0.00002272
Iteration 142/1000 | Loss: 0.00002272
Iteration 143/1000 | Loss: 0.00002272
Iteration 144/1000 | Loss: 0.00002272
Iteration 145/1000 | Loss: 0.00002272
Iteration 146/1000 | Loss: 0.00002272
Iteration 147/1000 | Loss: 0.00002272
Iteration 148/1000 | Loss: 0.00002272
Iteration 149/1000 | Loss: 0.00002272
Iteration 150/1000 | Loss: 0.00002272
Iteration 151/1000 | Loss: 0.00002272
Iteration 152/1000 | Loss: 0.00002272
Iteration 153/1000 | Loss: 0.00002272
Iteration 154/1000 | Loss: 0.00002272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.2716181774740107e-05, 2.2716181774740107e-05, 2.2716181774740107e-05, 2.2716181774740107e-05, 2.2716181774740107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2716181774740107e-05

Optimization complete. Final v2v error: 3.939615488052368 mm

Highest mean error: 9.699795722961426 mm for frame 26

Lowest mean error: 3.4862306118011475 mm for frame 39

Saving results

Total time: 78.23251295089722
