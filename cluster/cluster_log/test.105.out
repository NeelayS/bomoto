Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=105, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5880-5935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770486
Iteration 2/25 | Loss: 0.00146016
Iteration 3/25 | Loss: 0.00120380
Iteration 4/25 | Loss: 0.00115662
Iteration 5/25 | Loss: 0.00114161
Iteration 6/25 | Loss: 0.00114814
Iteration 7/25 | Loss: 0.00113777
Iteration 8/25 | Loss: 0.00113062
Iteration 9/25 | Loss: 0.00112694
Iteration 10/25 | Loss: 0.00112543
Iteration 11/25 | Loss: 0.00112432
Iteration 12/25 | Loss: 0.00112389
Iteration 13/25 | Loss: 0.00112370
Iteration 14/25 | Loss: 0.00112362
Iteration 15/25 | Loss: 0.00112361
Iteration 16/25 | Loss: 0.00112361
Iteration 17/25 | Loss: 0.00112361
Iteration 18/25 | Loss: 0.00112361
Iteration 19/25 | Loss: 0.00112361
Iteration 20/25 | Loss: 0.00112361
Iteration 21/25 | Loss: 0.00112361
Iteration 22/25 | Loss: 0.00112361
Iteration 23/25 | Loss: 0.00112361
Iteration 24/25 | Loss: 0.00112360
Iteration 25/25 | Loss: 0.00112360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.94157839
Iteration 2/25 | Loss: 0.00156670
Iteration 3/25 | Loss: 0.00156663
Iteration 4/25 | Loss: 0.00156663
Iteration 5/25 | Loss: 0.00156663
Iteration 6/25 | Loss: 0.00156663
Iteration 7/25 | Loss: 0.00156663
Iteration 8/25 | Loss: 0.00156663
Iteration 9/25 | Loss: 0.00156663
Iteration 10/25 | Loss: 0.00156663
Iteration 11/25 | Loss: 0.00156663
Iteration 12/25 | Loss: 0.00156663
Iteration 13/25 | Loss: 0.00156663
Iteration 14/25 | Loss: 0.00156663
Iteration 15/25 | Loss: 0.00156663
Iteration 16/25 | Loss: 0.00156663
Iteration 17/25 | Loss: 0.00156663
Iteration 18/25 | Loss: 0.00156663
Iteration 19/25 | Loss: 0.00156663
Iteration 20/25 | Loss: 0.00156663
Iteration 21/25 | Loss: 0.00156663
Iteration 22/25 | Loss: 0.00156663
Iteration 23/25 | Loss: 0.00156663
Iteration 24/25 | Loss: 0.00156663
Iteration 25/25 | Loss: 0.00156663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015666260151192546, 0.0015666260151192546, 0.0015666260151192546, 0.0015666260151192546, 0.0015666260151192546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015666260151192546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156663
Iteration 2/1000 | Loss: 0.00003729
Iteration 3/1000 | Loss: 0.00002600
Iteration 4/1000 | Loss: 0.00005410
Iteration 5/1000 | Loss: 0.00002034
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001904
Iteration 8/1000 | Loss: 0.00001864
Iteration 9/1000 | Loss: 0.00001834
Iteration 10/1000 | Loss: 0.00001812
Iteration 11/1000 | Loss: 0.00001804
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001792
Iteration 14/1000 | Loss: 0.00001790
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001779
Iteration 18/1000 | Loss: 0.00001778
Iteration 19/1000 | Loss: 0.00001777
Iteration 20/1000 | Loss: 0.00001776
Iteration 21/1000 | Loss: 0.00001774
Iteration 22/1000 | Loss: 0.00001774
Iteration 23/1000 | Loss: 0.00001773
Iteration 24/1000 | Loss: 0.00001772
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001769
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001768
Iteration 29/1000 | Loss: 0.00001767
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001766
Iteration 33/1000 | Loss: 0.00001765
Iteration 34/1000 | Loss: 0.00001765
Iteration 35/1000 | Loss: 0.00001764
Iteration 36/1000 | Loss: 0.00001764
Iteration 37/1000 | Loss: 0.00001763
Iteration 38/1000 | Loss: 0.00001761
Iteration 39/1000 | Loss: 0.00001761
Iteration 40/1000 | Loss: 0.00001761
Iteration 41/1000 | Loss: 0.00001761
Iteration 42/1000 | Loss: 0.00001761
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001760
Iteration 45/1000 | Loss: 0.00001760
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001760
Iteration 49/1000 | Loss: 0.00001760
Iteration 50/1000 | Loss: 0.00001760
Iteration 51/1000 | Loss: 0.00001759
Iteration 52/1000 | Loss: 0.00001759
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001758
Iteration 55/1000 | Loss: 0.00001758
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001756
Iteration 77/1000 | Loss: 0.00001756
Iteration 78/1000 | Loss: 0.00001756
Iteration 79/1000 | Loss: 0.00001756
Iteration 80/1000 | Loss: 0.00001756
Iteration 81/1000 | Loss: 0.00001756
Iteration 82/1000 | Loss: 0.00001756
Iteration 83/1000 | Loss: 0.00001756
Iteration 84/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.7560243577463552e-05, 1.7560243577463552e-05, 1.7560243577463552e-05, 1.7560243577463552e-05, 1.7560243577463552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7560243577463552e-05

Optimization complete. Final v2v error: 3.351916551589966 mm

Highest mean error: 12.147591590881348 mm for frame 158

Lowest mean error: 2.863114595413208 mm for frame 195

Saving results

Total time: 55.8615939617157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493777
Iteration 2/25 | Loss: 0.00133159
Iteration 3/25 | Loss: 0.00120643
Iteration 4/25 | Loss: 0.00119255
Iteration 5/25 | Loss: 0.00118934
Iteration 6/25 | Loss: 0.00118934
Iteration 7/25 | Loss: 0.00118934
Iteration 8/25 | Loss: 0.00118934
Iteration 9/25 | Loss: 0.00118934
Iteration 10/25 | Loss: 0.00118934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011893432820215821, 0.0011893432820215821, 0.0011893432820215821, 0.0011893432820215821, 0.0011893432820215821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011893432820215821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29881668
Iteration 2/25 | Loss: 0.00124878
Iteration 3/25 | Loss: 0.00124876
Iteration 4/25 | Loss: 0.00124876
Iteration 5/25 | Loss: 0.00124876
Iteration 6/25 | Loss: 0.00124876
Iteration 7/25 | Loss: 0.00124876
Iteration 8/25 | Loss: 0.00124876
Iteration 9/25 | Loss: 0.00124876
Iteration 10/25 | Loss: 0.00124876
Iteration 11/25 | Loss: 0.00124876
Iteration 12/25 | Loss: 0.00124876
Iteration 13/25 | Loss: 0.00124876
Iteration 14/25 | Loss: 0.00124876
Iteration 15/25 | Loss: 0.00124876
Iteration 16/25 | Loss: 0.00124876
Iteration 17/25 | Loss: 0.00124876
Iteration 18/25 | Loss: 0.00124876
Iteration 19/25 | Loss: 0.00124876
Iteration 20/25 | Loss: 0.00124876
Iteration 21/25 | Loss: 0.00124876
Iteration 22/25 | Loss: 0.00124876
Iteration 23/25 | Loss: 0.00124876
Iteration 24/25 | Loss: 0.00124876
Iteration 25/25 | Loss: 0.00124876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124876
Iteration 2/1000 | Loss: 0.00005611
Iteration 3/1000 | Loss: 0.00003441
Iteration 4/1000 | Loss: 0.00002590
Iteration 5/1000 | Loss: 0.00002314
Iteration 6/1000 | Loss: 0.00002179
Iteration 7/1000 | Loss: 0.00002102
Iteration 8/1000 | Loss: 0.00002055
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001932
Iteration 13/1000 | Loss: 0.00001929
Iteration 14/1000 | Loss: 0.00001929
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001918
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001909
Iteration 21/1000 | Loss: 0.00001907
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001898
Iteration 25/1000 | Loss: 0.00001895
Iteration 26/1000 | Loss: 0.00001895
Iteration 27/1000 | Loss: 0.00001895
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001894
Iteration 32/1000 | Loss: 0.00001894
Iteration 33/1000 | Loss: 0.00001894
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001894
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001894
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001891
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001888
Iteration 47/1000 | Loss: 0.00001888
Iteration 48/1000 | Loss: 0.00001888
Iteration 49/1000 | Loss: 0.00001888
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001888
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00001887
Iteration 55/1000 | Loss: 0.00001887
Iteration 56/1000 | Loss: 0.00001887
Iteration 57/1000 | Loss: 0.00001887
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001885
Iteration 61/1000 | Loss: 0.00001884
Iteration 62/1000 | Loss: 0.00001884
Iteration 63/1000 | Loss: 0.00001884
Iteration 64/1000 | Loss: 0.00001883
Iteration 65/1000 | Loss: 0.00001883
Iteration 66/1000 | Loss: 0.00001883
Iteration 67/1000 | Loss: 0.00001882
Iteration 68/1000 | Loss: 0.00001882
Iteration 69/1000 | Loss: 0.00001882
Iteration 70/1000 | Loss: 0.00001882
Iteration 71/1000 | Loss: 0.00001882
Iteration 72/1000 | Loss: 0.00001881
Iteration 73/1000 | Loss: 0.00001881
Iteration 74/1000 | Loss: 0.00001881
Iteration 75/1000 | Loss: 0.00001880
Iteration 76/1000 | Loss: 0.00001880
Iteration 77/1000 | Loss: 0.00001880
Iteration 78/1000 | Loss: 0.00001879
Iteration 79/1000 | Loss: 0.00001879
Iteration 80/1000 | Loss: 0.00001878
Iteration 81/1000 | Loss: 0.00001878
Iteration 82/1000 | Loss: 0.00001878
Iteration 83/1000 | Loss: 0.00001878
Iteration 84/1000 | Loss: 0.00001877
Iteration 85/1000 | Loss: 0.00001877
Iteration 86/1000 | Loss: 0.00001877
Iteration 87/1000 | Loss: 0.00001876
Iteration 88/1000 | Loss: 0.00001876
Iteration 89/1000 | Loss: 0.00001876
Iteration 90/1000 | Loss: 0.00001876
Iteration 91/1000 | Loss: 0.00001875
Iteration 92/1000 | Loss: 0.00001875
Iteration 93/1000 | Loss: 0.00001875
Iteration 94/1000 | Loss: 0.00001875
Iteration 95/1000 | Loss: 0.00001875
Iteration 96/1000 | Loss: 0.00001875
Iteration 97/1000 | Loss: 0.00001875
Iteration 98/1000 | Loss: 0.00001875
Iteration 99/1000 | Loss: 0.00001875
Iteration 100/1000 | Loss: 0.00001875
Iteration 101/1000 | Loss: 0.00001874
Iteration 102/1000 | Loss: 0.00001874
Iteration 103/1000 | Loss: 0.00001874
Iteration 104/1000 | Loss: 0.00001874
Iteration 105/1000 | Loss: 0.00001874
Iteration 106/1000 | Loss: 0.00001874
Iteration 107/1000 | Loss: 0.00001874
Iteration 108/1000 | Loss: 0.00001874
Iteration 109/1000 | Loss: 0.00001874
Iteration 110/1000 | Loss: 0.00001874
Iteration 111/1000 | Loss: 0.00001874
Iteration 112/1000 | Loss: 0.00001874
Iteration 113/1000 | Loss: 0.00001874
Iteration 114/1000 | Loss: 0.00001874
Iteration 115/1000 | Loss: 0.00001874
Iteration 116/1000 | Loss: 0.00001874
Iteration 117/1000 | Loss: 0.00001874
Iteration 118/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.874424378911499e-05, 1.874424378911499e-05, 1.874424378911499e-05, 1.874424378911499e-05, 1.874424378911499e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.874424378911499e-05

Optimization complete. Final v2v error: 3.6225976943969727 mm

Highest mean error: 4.112909317016602 mm for frame 209

Lowest mean error: 3.129387140274048 mm for frame 235

Saving results

Total time: 41.56355047225952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014807
Iteration 2/25 | Loss: 0.00426406
Iteration 3/25 | Loss: 0.00284470
Iteration 4/25 | Loss: 0.00246077
Iteration 5/25 | Loss: 0.00227545
Iteration 6/25 | Loss: 0.00224392
Iteration 7/25 | Loss: 0.00219605
Iteration 8/25 | Loss: 0.00210930
Iteration 9/25 | Loss: 0.00206227
Iteration 10/25 | Loss: 0.00196667
Iteration 11/25 | Loss: 0.00185720
Iteration 12/25 | Loss: 0.00184156
Iteration 13/25 | Loss: 0.00181630
Iteration 14/25 | Loss: 0.00181819
Iteration 15/25 | Loss: 0.00180665
Iteration 16/25 | Loss: 0.00181698
Iteration 17/25 | Loss: 0.00178950
Iteration 18/25 | Loss: 0.00178048
Iteration 19/25 | Loss: 0.00177894
Iteration 20/25 | Loss: 0.00177848
Iteration 21/25 | Loss: 0.00177845
Iteration 22/25 | Loss: 0.00177842
Iteration 23/25 | Loss: 0.00177842
Iteration 24/25 | Loss: 0.00177842
Iteration 25/25 | Loss: 0.00177842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27492404
Iteration 2/25 | Loss: 0.01052679
Iteration 3/25 | Loss: 0.00363638
Iteration 4/25 | Loss: 0.00363638
Iteration 5/25 | Loss: 0.00363638
Iteration 6/25 | Loss: 0.00363638
Iteration 7/25 | Loss: 0.00363638
Iteration 8/25 | Loss: 0.00363638
Iteration 9/25 | Loss: 0.00363638
Iteration 10/25 | Loss: 0.00363638
Iteration 11/25 | Loss: 0.00363638
Iteration 12/25 | Loss: 0.00363638
Iteration 13/25 | Loss: 0.00363638
Iteration 14/25 | Loss: 0.00363638
Iteration 15/25 | Loss: 0.00363638
Iteration 16/25 | Loss: 0.00363638
Iteration 17/25 | Loss: 0.00363638
Iteration 18/25 | Loss: 0.00363638
Iteration 19/25 | Loss: 0.00363638
Iteration 20/25 | Loss: 0.00363638
Iteration 21/25 | Loss: 0.00363638
Iteration 22/25 | Loss: 0.00363638
Iteration 23/25 | Loss: 0.00363638
Iteration 24/25 | Loss: 0.00363638
Iteration 25/25 | Loss: 0.00363638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00363638
Iteration 2/1000 | Loss: 0.01078258
Iteration 3/1000 | Loss: 0.00066917
Iteration 4/1000 | Loss: 0.00083609
Iteration 5/1000 | Loss: 0.00064580
Iteration 6/1000 | Loss: 0.00399997
Iteration 7/1000 | Loss: 0.00432962
Iteration 8/1000 | Loss: 0.00061298
Iteration 9/1000 | Loss: 0.00073985
Iteration 10/1000 | Loss: 0.00065933
Iteration 11/1000 | Loss: 0.00148099
Iteration 12/1000 | Loss: 0.00035195
Iteration 13/1000 | Loss: 0.00030736
Iteration 14/1000 | Loss: 0.00028012
Iteration 15/1000 | Loss: 0.00070203
Iteration 16/1000 | Loss: 0.01223089
Iteration 17/1000 | Loss: 0.00414752
Iteration 18/1000 | Loss: 0.00068775
Iteration 19/1000 | Loss: 0.00180558
Iteration 20/1000 | Loss: 0.00184771
Iteration 21/1000 | Loss: 0.00100686
Iteration 22/1000 | Loss: 0.00013745
Iteration 23/1000 | Loss: 0.00020150
Iteration 24/1000 | Loss: 0.00045267
Iteration 25/1000 | Loss: 0.00355986
Iteration 26/1000 | Loss: 0.00027518
Iteration 27/1000 | Loss: 0.00022736
Iteration 28/1000 | Loss: 0.00005248
Iteration 29/1000 | Loss: 0.00022695
Iteration 30/1000 | Loss: 0.00038407
Iteration 31/1000 | Loss: 0.00003919
Iteration 32/1000 | Loss: 0.00002964
Iteration 33/1000 | Loss: 0.00002554
Iteration 34/1000 | Loss: 0.00025100
Iteration 35/1000 | Loss: 0.00020390
Iteration 36/1000 | Loss: 0.00040814
Iteration 37/1000 | Loss: 0.00150167
Iteration 38/1000 | Loss: 0.00094988
Iteration 39/1000 | Loss: 0.00173841
Iteration 40/1000 | Loss: 0.00031517
Iteration 41/1000 | Loss: 0.00063959
Iteration 42/1000 | Loss: 0.00015003
Iteration 43/1000 | Loss: 0.00014903
Iteration 44/1000 | Loss: 0.00003022
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001913
Iteration 47/1000 | Loss: 0.00017682
Iteration 48/1000 | Loss: 0.00003571
Iteration 49/1000 | Loss: 0.00004650
Iteration 50/1000 | Loss: 0.00001829
Iteration 51/1000 | Loss: 0.00001800
Iteration 52/1000 | Loss: 0.00001756
Iteration 53/1000 | Loss: 0.00001727
Iteration 54/1000 | Loss: 0.00001707
Iteration 55/1000 | Loss: 0.00012309
Iteration 56/1000 | Loss: 0.00012483
Iteration 57/1000 | Loss: 0.00084936
Iteration 58/1000 | Loss: 0.00015819
Iteration 59/1000 | Loss: 0.00001783
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001682
Iteration 62/1000 | Loss: 0.00001675
Iteration 63/1000 | Loss: 0.00001674
Iteration 64/1000 | Loss: 0.00001670
Iteration 65/1000 | Loss: 0.00001670
Iteration 66/1000 | Loss: 0.00001669
Iteration 67/1000 | Loss: 0.00001666
Iteration 68/1000 | Loss: 0.00001666
Iteration 69/1000 | Loss: 0.00001665
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001661
Iteration 72/1000 | Loss: 0.00001661
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001660
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001659
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001659
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00011789
Iteration 91/1000 | Loss: 0.00012859
Iteration 92/1000 | Loss: 0.00003645
Iteration 93/1000 | Loss: 0.00010879
Iteration 94/1000 | Loss: 0.00014721
Iteration 95/1000 | Loss: 0.00006646
Iteration 96/1000 | Loss: 0.00002945
Iteration 97/1000 | Loss: 0.00008519
Iteration 98/1000 | Loss: 0.00005390
Iteration 99/1000 | Loss: 0.00002199
Iteration 100/1000 | Loss: 0.00001674
Iteration 101/1000 | Loss: 0.00002237
Iteration 102/1000 | Loss: 0.00001671
Iteration 103/1000 | Loss: 0.00001662
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001658
Iteration 108/1000 | Loss: 0.00001657
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001652
Iteration 111/1000 | Loss: 0.00001652
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001651
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001650
Iteration 128/1000 | Loss: 0.00001650
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001650
Iteration 131/1000 | Loss: 0.00001650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.649891783017665e-05, 1.649891783017665e-05, 1.649891783017665e-05, 1.649891783017665e-05, 1.649891783017665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.649891783017665e-05

Optimization complete. Final v2v error: 3.462731122970581 mm

Highest mean error: 3.632904291152954 mm for frame 47

Lowest mean error: 3.226990222930908 mm for frame 112

Saving results

Total time: 144.26574778556824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569658
Iteration 2/25 | Loss: 0.00138236
Iteration 3/25 | Loss: 0.00121240
Iteration 4/25 | Loss: 0.00119382
Iteration 5/25 | Loss: 0.00118914
Iteration 6/25 | Loss: 0.00118778
Iteration 7/25 | Loss: 0.00118778
Iteration 8/25 | Loss: 0.00118778
Iteration 9/25 | Loss: 0.00118778
Iteration 10/25 | Loss: 0.00118778
Iteration 11/25 | Loss: 0.00118778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011877783108502626, 0.0011877783108502626, 0.0011877783108502626, 0.0011877783108502626, 0.0011877783108502626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011877783108502626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52198327
Iteration 2/25 | Loss: 0.00143074
Iteration 3/25 | Loss: 0.00143073
Iteration 4/25 | Loss: 0.00143073
Iteration 5/25 | Loss: 0.00143073
Iteration 6/25 | Loss: 0.00143073
Iteration 7/25 | Loss: 0.00143073
Iteration 8/25 | Loss: 0.00143073
Iteration 9/25 | Loss: 0.00143073
Iteration 10/25 | Loss: 0.00143073
Iteration 11/25 | Loss: 0.00143073
Iteration 12/25 | Loss: 0.00143073
Iteration 13/25 | Loss: 0.00143073
Iteration 14/25 | Loss: 0.00143073
Iteration 15/25 | Loss: 0.00143073
Iteration 16/25 | Loss: 0.00143073
Iteration 17/25 | Loss: 0.00143073
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014307316159829497, 0.0014307316159829497, 0.0014307316159829497, 0.0014307316159829497, 0.0014307316159829497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014307316159829497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143073
Iteration 2/1000 | Loss: 0.00003830
Iteration 3/1000 | Loss: 0.00002426
Iteration 4/1000 | Loss: 0.00002098
Iteration 5/1000 | Loss: 0.00002008
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001869
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001785
Iteration 12/1000 | Loss: 0.00001783
Iteration 13/1000 | Loss: 0.00001783
Iteration 14/1000 | Loss: 0.00001783
Iteration 15/1000 | Loss: 0.00001782
Iteration 16/1000 | Loss: 0.00001781
Iteration 17/1000 | Loss: 0.00001781
Iteration 18/1000 | Loss: 0.00001780
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00001778
Iteration 22/1000 | Loss: 0.00001778
Iteration 23/1000 | Loss: 0.00001778
Iteration 24/1000 | Loss: 0.00001778
Iteration 25/1000 | Loss: 0.00001778
Iteration 26/1000 | Loss: 0.00001778
Iteration 27/1000 | Loss: 0.00001778
Iteration 28/1000 | Loss: 0.00001778
Iteration 29/1000 | Loss: 0.00001778
Iteration 30/1000 | Loss: 0.00001778
Iteration 31/1000 | Loss: 0.00001777
Iteration 32/1000 | Loss: 0.00001774
Iteration 33/1000 | Loss: 0.00001774
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001768
Iteration 38/1000 | Loss: 0.00001767
Iteration 39/1000 | Loss: 0.00001767
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001764
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001760
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001757
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001755
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001751
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001751
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001750
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001750
Iteration 90/1000 | Loss: 0.00001750
Iteration 91/1000 | Loss: 0.00001750
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001749
Iteration 94/1000 | Loss: 0.00001749
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001748
Iteration 97/1000 | Loss: 0.00001748
Iteration 98/1000 | Loss: 0.00001748
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001747
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001747
Iteration 107/1000 | Loss: 0.00001747
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001746
Iteration 114/1000 | Loss: 0.00001746
Iteration 115/1000 | Loss: 0.00001746
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001745
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001745
Iteration 122/1000 | Loss: 0.00001744
Iteration 123/1000 | Loss: 0.00001744
Iteration 124/1000 | Loss: 0.00001744
Iteration 125/1000 | Loss: 0.00001744
Iteration 126/1000 | Loss: 0.00001744
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001744
Iteration 132/1000 | Loss: 0.00001744
Iteration 133/1000 | Loss: 0.00001744
Iteration 134/1000 | Loss: 0.00001744
Iteration 135/1000 | Loss: 0.00001744
Iteration 136/1000 | Loss: 0.00001744
Iteration 137/1000 | Loss: 0.00001744
Iteration 138/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.7439077055314556e-05, 1.7439077055314556e-05, 1.7439077055314556e-05, 1.7439077055314556e-05, 1.7439077055314556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7439077055314556e-05

Optimization complete. Final v2v error: 3.5433692932128906 mm

Highest mean error: 4.105300426483154 mm for frame 103

Lowest mean error: 3.163317918777466 mm for frame 188

Saving results

Total time: 38.84084343910217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618619
Iteration 2/25 | Loss: 0.00130048
Iteration 3/25 | Loss: 0.00122305
Iteration 4/25 | Loss: 0.00120616
Iteration 5/25 | Loss: 0.00120258
Iteration 6/25 | Loss: 0.00120169
Iteration 7/25 | Loss: 0.00120169
Iteration 8/25 | Loss: 0.00120169
Iteration 9/25 | Loss: 0.00120169
Iteration 10/25 | Loss: 0.00120169
Iteration 11/25 | Loss: 0.00120169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001201686798594892, 0.001201686798594892, 0.001201686798594892, 0.001201686798594892, 0.001201686798594892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001201686798594892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67886138
Iteration 2/25 | Loss: 0.00161455
Iteration 3/25 | Loss: 0.00161453
Iteration 4/25 | Loss: 0.00161453
Iteration 5/25 | Loss: 0.00161453
Iteration 6/25 | Loss: 0.00161453
Iteration 7/25 | Loss: 0.00161453
Iteration 8/25 | Loss: 0.00161452
Iteration 9/25 | Loss: 0.00161452
Iteration 10/25 | Loss: 0.00161452
Iteration 11/25 | Loss: 0.00161452
Iteration 12/25 | Loss: 0.00161452
Iteration 13/25 | Loss: 0.00161452
Iteration 14/25 | Loss: 0.00161452
Iteration 15/25 | Loss: 0.00161452
Iteration 16/25 | Loss: 0.00161452
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016145240515470505, 0.0016145240515470505, 0.0016145240515470505, 0.0016145240515470505, 0.0016145240515470505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016145240515470505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161452
Iteration 2/1000 | Loss: 0.00010176
Iteration 3/1000 | Loss: 0.00005395
Iteration 4/1000 | Loss: 0.00003477
Iteration 5/1000 | Loss: 0.00002924
Iteration 6/1000 | Loss: 0.00002732
Iteration 7/1000 | Loss: 0.00002611
Iteration 8/1000 | Loss: 0.00002528
Iteration 9/1000 | Loss: 0.00002475
Iteration 10/1000 | Loss: 0.00002442
Iteration 11/1000 | Loss: 0.00002405
Iteration 12/1000 | Loss: 0.00002378
Iteration 13/1000 | Loss: 0.00002358
Iteration 14/1000 | Loss: 0.00002346
Iteration 15/1000 | Loss: 0.00002331
Iteration 16/1000 | Loss: 0.00002318
Iteration 17/1000 | Loss: 0.00002309
Iteration 18/1000 | Loss: 0.00002297
Iteration 19/1000 | Loss: 0.00002296
Iteration 20/1000 | Loss: 0.00002291
Iteration 21/1000 | Loss: 0.00002291
Iteration 22/1000 | Loss: 0.00002291
Iteration 23/1000 | Loss: 0.00002291
Iteration 24/1000 | Loss: 0.00002291
Iteration 25/1000 | Loss: 0.00002290
Iteration 26/1000 | Loss: 0.00002289
Iteration 27/1000 | Loss: 0.00002289
Iteration 28/1000 | Loss: 0.00002289
Iteration 29/1000 | Loss: 0.00002289
Iteration 30/1000 | Loss: 0.00002289
Iteration 31/1000 | Loss: 0.00002288
Iteration 32/1000 | Loss: 0.00002288
Iteration 33/1000 | Loss: 0.00002288
Iteration 34/1000 | Loss: 0.00002288
Iteration 35/1000 | Loss: 0.00002288
Iteration 36/1000 | Loss: 0.00002288
Iteration 37/1000 | Loss: 0.00002288
Iteration 38/1000 | Loss: 0.00002288
Iteration 39/1000 | Loss: 0.00002288
Iteration 40/1000 | Loss: 0.00002288
Iteration 41/1000 | Loss: 0.00002287
Iteration 42/1000 | Loss: 0.00002287
Iteration 43/1000 | Loss: 0.00002287
Iteration 44/1000 | Loss: 0.00002287
Iteration 45/1000 | Loss: 0.00002286
Iteration 46/1000 | Loss: 0.00002286
Iteration 47/1000 | Loss: 0.00002286
Iteration 48/1000 | Loss: 0.00002285
Iteration 49/1000 | Loss: 0.00002285
Iteration 50/1000 | Loss: 0.00002285
Iteration 51/1000 | Loss: 0.00002284
Iteration 52/1000 | Loss: 0.00002283
Iteration 53/1000 | Loss: 0.00002280
Iteration 54/1000 | Loss: 0.00002277
Iteration 55/1000 | Loss: 0.00002277
Iteration 56/1000 | Loss: 0.00002277
Iteration 57/1000 | Loss: 0.00002276
Iteration 58/1000 | Loss: 0.00002276
Iteration 59/1000 | Loss: 0.00002273
Iteration 60/1000 | Loss: 0.00002272
Iteration 61/1000 | Loss: 0.00002271
Iteration 62/1000 | Loss: 0.00002271
Iteration 63/1000 | Loss: 0.00002271
Iteration 64/1000 | Loss: 0.00002271
Iteration 65/1000 | Loss: 0.00002271
Iteration 66/1000 | Loss: 0.00002271
Iteration 67/1000 | Loss: 0.00002271
Iteration 68/1000 | Loss: 0.00002271
Iteration 69/1000 | Loss: 0.00002271
Iteration 70/1000 | Loss: 0.00002270
Iteration 71/1000 | Loss: 0.00002270
Iteration 72/1000 | Loss: 0.00002270
Iteration 73/1000 | Loss: 0.00002269
Iteration 74/1000 | Loss: 0.00002269
Iteration 75/1000 | Loss: 0.00002269
Iteration 76/1000 | Loss: 0.00002269
Iteration 77/1000 | Loss: 0.00002268
Iteration 78/1000 | Loss: 0.00002268
Iteration 79/1000 | Loss: 0.00002268
Iteration 80/1000 | Loss: 0.00002268
Iteration 81/1000 | Loss: 0.00002267
Iteration 82/1000 | Loss: 0.00002267
Iteration 83/1000 | Loss: 0.00002267
Iteration 84/1000 | Loss: 0.00002267
Iteration 85/1000 | Loss: 0.00002266
Iteration 86/1000 | Loss: 0.00002266
Iteration 87/1000 | Loss: 0.00002265
Iteration 88/1000 | Loss: 0.00002265
Iteration 89/1000 | Loss: 0.00002265
Iteration 90/1000 | Loss: 0.00002265
Iteration 91/1000 | Loss: 0.00002265
Iteration 92/1000 | Loss: 0.00002265
Iteration 93/1000 | Loss: 0.00002265
Iteration 94/1000 | Loss: 0.00002265
Iteration 95/1000 | Loss: 0.00002265
Iteration 96/1000 | Loss: 0.00002265
Iteration 97/1000 | Loss: 0.00002265
Iteration 98/1000 | Loss: 0.00002264
Iteration 99/1000 | Loss: 0.00002264
Iteration 100/1000 | Loss: 0.00002264
Iteration 101/1000 | Loss: 0.00002263
Iteration 102/1000 | Loss: 0.00002263
Iteration 103/1000 | Loss: 0.00002262
Iteration 104/1000 | Loss: 0.00002262
Iteration 105/1000 | Loss: 0.00002262
Iteration 106/1000 | Loss: 0.00002262
Iteration 107/1000 | Loss: 0.00002262
Iteration 108/1000 | Loss: 0.00002262
Iteration 109/1000 | Loss: 0.00002262
Iteration 110/1000 | Loss: 0.00002261
Iteration 111/1000 | Loss: 0.00002261
Iteration 112/1000 | Loss: 0.00002261
Iteration 113/1000 | Loss: 0.00002261
Iteration 114/1000 | Loss: 0.00002261
Iteration 115/1000 | Loss: 0.00002260
Iteration 116/1000 | Loss: 0.00002260
Iteration 117/1000 | Loss: 0.00002260
Iteration 118/1000 | Loss: 0.00002259
Iteration 119/1000 | Loss: 0.00002259
Iteration 120/1000 | Loss: 0.00002259
Iteration 121/1000 | Loss: 0.00002259
Iteration 122/1000 | Loss: 0.00002258
Iteration 123/1000 | Loss: 0.00002258
Iteration 124/1000 | Loss: 0.00002258
Iteration 125/1000 | Loss: 0.00002258
Iteration 126/1000 | Loss: 0.00002258
Iteration 127/1000 | Loss: 0.00002258
Iteration 128/1000 | Loss: 0.00002258
Iteration 129/1000 | Loss: 0.00002258
Iteration 130/1000 | Loss: 0.00002257
Iteration 131/1000 | Loss: 0.00002257
Iteration 132/1000 | Loss: 0.00002257
Iteration 133/1000 | Loss: 0.00002257
Iteration 134/1000 | Loss: 0.00002257
Iteration 135/1000 | Loss: 0.00002257
Iteration 136/1000 | Loss: 0.00002256
Iteration 137/1000 | Loss: 0.00002256
Iteration 138/1000 | Loss: 0.00002255
Iteration 139/1000 | Loss: 0.00002255
Iteration 140/1000 | Loss: 0.00002255
Iteration 141/1000 | Loss: 0.00002255
Iteration 142/1000 | Loss: 0.00002254
Iteration 143/1000 | Loss: 0.00002254
Iteration 144/1000 | Loss: 0.00002254
Iteration 145/1000 | Loss: 0.00002254
Iteration 146/1000 | Loss: 0.00002254
Iteration 147/1000 | Loss: 0.00002253
Iteration 148/1000 | Loss: 0.00002253
Iteration 149/1000 | Loss: 0.00002253
Iteration 150/1000 | Loss: 0.00002253
Iteration 151/1000 | Loss: 0.00002253
Iteration 152/1000 | Loss: 0.00002252
Iteration 153/1000 | Loss: 0.00002252
Iteration 154/1000 | Loss: 0.00002252
Iteration 155/1000 | Loss: 0.00002252
Iteration 156/1000 | Loss: 0.00002252
Iteration 157/1000 | Loss: 0.00002252
Iteration 158/1000 | Loss: 0.00002252
Iteration 159/1000 | Loss: 0.00002251
Iteration 160/1000 | Loss: 0.00002251
Iteration 161/1000 | Loss: 0.00002251
Iteration 162/1000 | Loss: 0.00002251
Iteration 163/1000 | Loss: 0.00002251
Iteration 164/1000 | Loss: 0.00002251
Iteration 165/1000 | Loss: 0.00002251
Iteration 166/1000 | Loss: 0.00002251
Iteration 167/1000 | Loss: 0.00002251
Iteration 168/1000 | Loss: 0.00002251
Iteration 169/1000 | Loss: 0.00002251
Iteration 170/1000 | Loss: 0.00002251
Iteration 171/1000 | Loss: 0.00002251
Iteration 172/1000 | Loss: 0.00002251
Iteration 173/1000 | Loss: 0.00002251
Iteration 174/1000 | Loss: 0.00002251
Iteration 175/1000 | Loss: 0.00002251
Iteration 176/1000 | Loss: 0.00002251
Iteration 177/1000 | Loss: 0.00002251
Iteration 178/1000 | Loss: 0.00002251
Iteration 179/1000 | Loss: 0.00002251
Iteration 180/1000 | Loss: 0.00002251
Iteration 181/1000 | Loss: 0.00002251
Iteration 182/1000 | Loss: 0.00002251
Iteration 183/1000 | Loss: 0.00002251
Iteration 184/1000 | Loss: 0.00002251
Iteration 185/1000 | Loss: 0.00002251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.2505084416479804e-05, 2.2505084416479804e-05, 2.2505084416479804e-05, 2.2505084416479804e-05, 2.2505084416479804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2505084416479804e-05

Optimization complete. Final v2v error: 3.931743621826172 mm

Highest mean error: 4.719408988952637 mm for frame 0

Lowest mean error: 3.817452907562256 mm for frame 103

Saving results

Total time: 46.9674277305603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050673
Iteration 2/25 | Loss: 0.00256182
Iteration 3/25 | Loss: 0.00206335
Iteration 4/25 | Loss: 0.00195074
Iteration 5/25 | Loss: 0.00194596
Iteration 6/25 | Loss: 0.00189081
Iteration 7/25 | Loss: 0.00189759
Iteration 8/25 | Loss: 0.00187120
Iteration 9/25 | Loss: 0.00186350
Iteration 10/25 | Loss: 0.00185967
Iteration 11/25 | Loss: 0.00185696
Iteration 12/25 | Loss: 0.00185497
Iteration 13/25 | Loss: 0.00186356
Iteration 14/25 | Loss: 0.00187329
Iteration 15/25 | Loss: 0.00189008
Iteration 16/25 | Loss: 0.00183425
Iteration 17/25 | Loss: 0.00179777
Iteration 18/25 | Loss: 0.00176600
Iteration 19/25 | Loss: 0.00175091
Iteration 20/25 | Loss: 0.00173110
Iteration 21/25 | Loss: 0.00173936
Iteration 22/25 | Loss: 0.00170828
Iteration 23/25 | Loss: 0.00170185
Iteration 24/25 | Loss: 0.00169701
Iteration 25/25 | Loss: 0.00169205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31223297
Iteration 2/25 | Loss: 0.00406673
Iteration 3/25 | Loss: 0.00406672
Iteration 4/25 | Loss: 0.00406672
Iteration 5/25 | Loss: 0.00406672
Iteration 6/25 | Loss: 0.00406672
Iteration 7/25 | Loss: 0.00406672
Iteration 8/25 | Loss: 0.00406672
Iteration 9/25 | Loss: 0.00406672
Iteration 10/25 | Loss: 0.00406672
Iteration 11/25 | Loss: 0.00406672
Iteration 12/25 | Loss: 0.00406672
Iteration 13/25 | Loss: 0.00406672
Iteration 14/25 | Loss: 0.00406672
Iteration 15/25 | Loss: 0.00406672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004066716879606247, 0.004066716879606247, 0.004066716879606247, 0.004066716879606247, 0.004066716879606247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004066716879606247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00406672
Iteration 2/1000 | Loss: 0.00596626
Iteration 3/1000 | Loss: 0.00240186
Iteration 4/1000 | Loss: 0.00148699
Iteration 5/1000 | Loss: 0.00188949
Iteration 6/1000 | Loss: 0.00205410
Iteration 7/1000 | Loss: 0.00238927
Iteration 8/1000 | Loss: 0.00153938
Iteration 9/1000 | Loss: 0.00158321
Iteration 10/1000 | Loss: 0.00104589
Iteration 11/1000 | Loss: 0.00144845
Iteration 12/1000 | Loss: 0.00029936
Iteration 13/1000 | Loss: 0.00042005
Iteration 14/1000 | Loss: 0.00142986
Iteration 15/1000 | Loss: 0.00037094
Iteration 16/1000 | Loss: 0.00068591
Iteration 17/1000 | Loss: 0.00038956
Iteration 18/1000 | Loss: 0.00051451
Iteration 19/1000 | Loss: 0.00046034
Iteration 20/1000 | Loss: 0.00018565
Iteration 21/1000 | Loss: 0.00007902
Iteration 22/1000 | Loss: 0.00028330
Iteration 23/1000 | Loss: 0.00006682
Iteration 24/1000 | Loss: 0.00018369
Iteration 25/1000 | Loss: 0.00006268
Iteration 26/1000 | Loss: 0.00005443
Iteration 27/1000 | Loss: 0.00009914
Iteration 28/1000 | Loss: 0.00016967
Iteration 29/1000 | Loss: 0.00005248
Iteration 30/1000 | Loss: 0.00065030
Iteration 31/1000 | Loss: 0.00017690
Iteration 32/1000 | Loss: 0.00014983
Iteration 33/1000 | Loss: 0.00020506
Iteration 34/1000 | Loss: 0.00018452
Iteration 35/1000 | Loss: 0.00011511
Iteration 36/1000 | Loss: 0.00016684
Iteration 37/1000 | Loss: 0.00015736
Iteration 38/1000 | Loss: 0.00024526
Iteration 39/1000 | Loss: 0.00018002
Iteration 40/1000 | Loss: 0.00010087
Iteration 41/1000 | Loss: 0.00008452
Iteration 42/1000 | Loss: 0.00006508
Iteration 43/1000 | Loss: 0.00006469
Iteration 44/1000 | Loss: 0.00012272
Iteration 45/1000 | Loss: 0.00005555
Iteration 46/1000 | Loss: 0.00025913
Iteration 47/1000 | Loss: 0.00023174
Iteration 48/1000 | Loss: 0.00014008
Iteration 49/1000 | Loss: 0.00024745
Iteration 50/1000 | Loss: 0.00016739
Iteration 51/1000 | Loss: 0.00019504
Iteration 52/1000 | Loss: 0.00015622
Iteration 53/1000 | Loss: 0.00014965
Iteration 54/1000 | Loss: 0.00016252
Iteration 55/1000 | Loss: 0.00005448
Iteration 56/1000 | Loss: 0.00014768
Iteration 57/1000 | Loss: 0.00016486
Iteration 58/1000 | Loss: 0.00065178
Iteration 59/1000 | Loss: 0.00040206
Iteration 60/1000 | Loss: 0.00036509
Iteration 61/1000 | Loss: 0.00015324
Iteration 62/1000 | Loss: 0.00003984
Iteration 63/1000 | Loss: 0.00009489
Iteration 64/1000 | Loss: 0.00008441
Iteration 65/1000 | Loss: 0.00013752
Iteration 66/1000 | Loss: 0.00004016
Iteration 67/1000 | Loss: 0.00007769
Iteration 68/1000 | Loss: 0.00018314
Iteration 69/1000 | Loss: 0.00003544
Iteration 70/1000 | Loss: 0.00012256
Iteration 71/1000 | Loss: 0.00016739
Iteration 72/1000 | Loss: 0.00067911
Iteration 73/1000 | Loss: 0.00036249
Iteration 74/1000 | Loss: 0.00046418
Iteration 75/1000 | Loss: 0.00013500
Iteration 76/1000 | Loss: 0.00007057
Iteration 77/1000 | Loss: 0.00007822
Iteration 78/1000 | Loss: 0.00003268
Iteration 79/1000 | Loss: 0.00011734
Iteration 80/1000 | Loss: 0.00003009
Iteration 81/1000 | Loss: 0.00002877
Iteration 82/1000 | Loss: 0.00010313
Iteration 83/1000 | Loss: 0.00008580
Iteration 84/1000 | Loss: 0.00013625
Iteration 85/1000 | Loss: 0.00005434
Iteration 86/1000 | Loss: 0.00002906
Iteration 87/1000 | Loss: 0.00008680
Iteration 88/1000 | Loss: 0.00009937
Iteration 89/1000 | Loss: 0.00010757
Iteration 90/1000 | Loss: 0.00009666
Iteration 91/1000 | Loss: 0.00002947
Iteration 92/1000 | Loss: 0.00002621
Iteration 93/1000 | Loss: 0.00002556
Iteration 94/1000 | Loss: 0.00002524
Iteration 95/1000 | Loss: 0.00002498
Iteration 96/1000 | Loss: 0.00002472
Iteration 97/1000 | Loss: 0.00002458
Iteration 98/1000 | Loss: 0.00002457
Iteration 99/1000 | Loss: 0.00002455
Iteration 100/1000 | Loss: 0.00002450
Iteration 101/1000 | Loss: 0.00002447
Iteration 102/1000 | Loss: 0.00002436
Iteration 103/1000 | Loss: 0.00002436
Iteration 104/1000 | Loss: 0.00002429
Iteration 105/1000 | Loss: 0.00002428
Iteration 106/1000 | Loss: 0.00002428
Iteration 107/1000 | Loss: 0.00002428
Iteration 108/1000 | Loss: 0.00002428
Iteration 109/1000 | Loss: 0.00002427
Iteration 110/1000 | Loss: 0.00002427
Iteration 111/1000 | Loss: 0.00002427
Iteration 112/1000 | Loss: 0.00002427
Iteration 113/1000 | Loss: 0.00002425
Iteration 114/1000 | Loss: 0.00002425
Iteration 115/1000 | Loss: 0.00002424
Iteration 116/1000 | Loss: 0.00002424
Iteration 117/1000 | Loss: 0.00002424
Iteration 118/1000 | Loss: 0.00002424
Iteration 119/1000 | Loss: 0.00002424
Iteration 120/1000 | Loss: 0.00002424
Iteration 121/1000 | Loss: 0.00002424
Iteration 122/1000 | Loss: 0.00002424
Iteration 123/1000 | Loss: 0.00002424
Iteration 124/1000 | Loss: 0.00002423
Iteration 125/1000 | Loss: 0.00002423
Iteration 126/1000 | Loss: 0.00002423
Iteration 127/1000 | Loss: 0.00002423
Iteration 128/1000 | Loss: 0.00002422
Iteration 129/1000 | Loss: 0.00002422
Iteration 130/1000 | Loss: 0.00002422
Iteration 131/1000 | Loss: 0.00002421
Iteration 132/1000 | Loss: 0.00002421
Iteration 133/1000 | Loss: 0.00002421
Iteration 134/1000 | Loss: 0.00002421
Iteration 135/1000 | Loss: 0.00002421
Iteration 136/1000 | Loss: 0.00002421
Iteration 137/1000 | Loss: 0.00002421
Iteration 138/1000 | Loss: 0.00002421
Iteration 139/1000 | Loss: 0.00002421
Iteration 140/1000 | Loss: 0.00002421
Iteration 141/1000 | Loss: 0.00002421
Iteration 142/1000 | Loss: 0.00002421
Iteration 143/1000 | Loss: 0.00002420
Iteration 144/1000 | Loss: 0.00002420
Iteration 145/1000 | Loss: 0.00002420
Iteration 146/1000 | Loss: 0.00002420
Iteration 147/1000 | Loss: 0.00002420
Iteration 148/1000 | Loss: 0.00002420
Iteration 149/1000 | Loss: 0.00002420
Iteration 150/1000 | Loss: 0.00002420
Iteration 151/1000 | Loss: 0.00002419
Iteration 152/1000 | Loss: 0.00002419
Iteration 153/1000 | Loss: 0.00002419
Iteration 154/1000 | Loss: 0.00002419
Iteration 155/1000 | Loss: 0.00002419
Iteration 156/1000 | Loss: 0.00002419
Iteration 157/1000 | Loss: 0.00002419
Iteration 158/1000 | Loss: 0.00002419
Iteration 159/1000 | Loss: 0.00002419
Iteration 160/1000 | Loss: 0.00002419
Iteration 161/1000 | Loss: 0.00002419
Iteration 162/1000 | Loss: 0.00002419
Iteration 163/1000 | Loss: 0.00002419
Iteration 164/1000 | Loss: 0.00002419
Iteration 165/1000 | Loss: 0.00002419
Iteration 166/1000 | Loss: 0.00002418
Iteration 167/1000 | Loss: 0.00002418
Iteration 168/1000 | Loss: 0.00002418
Iteration 169/1000 | Loss: 0.00002418
Iteration 170/1000 | Loss: 0.00002418
Iteration 171/1000 | Loss: 0.00002418
Iteration 172/1000 | Loss: 0.00002418
Iteration 173/1000 | Loss: 0.00002418
Iteration 174/1000 | Loss: 0.00002418
Iteration 175/1000 | Loss: 0.00002418
Iteration 176/1000 | Loss: 0.00002418
Iteration 177/1000 | Loss: 0.00002418
Iteration 178/1000 | Loss: 0.00002418
Iteration 179/1000 | Loss: 0.00002418
Iteration 180/1000 | Loss: 0.00002418
Iteration 181/1000 | Loss: 0.00002418
Iteration 182/1000 | Loss: 0.00002418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.417807081656065e-05, 2.417807081656065e-05, 2.417807081656065e-05, 2.417807081656065e-05, 2.417807081656065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.417807081656065e-05

Optimization complete. Final v2v error: 4.207573890686035 mm

Highest mean error: 7.020877838134766 mm for frame 70

Lowest mean error: 3.6676559448242188 mm for frame 2

Saving results

Total time: 202.22852325439453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099967
Iteration 2/25 | Loss: 0.01099967
Iteration 3/25 | Loss: 0.00233655
Iteration 4/25 | Loss: 0.00211879
Iteration 5/25 | Loss: 0.00147870
Iteration 6/25 | Loss: 0.00146962
Iteration 7/25 | Loss: 0.00130039
Iteration 8/25 | Loss: 0.00118080
Iteration 9/25 | Loss: 0.00112792
Iteration 10/25 | Loss: 0.00110767
Iteration 11/25 | Loss: 0.00109800
Iteration 12/25 | Loss: 0.00109632
Iteration 13/25 | Loss: 0.00109546
Iteration 14/25 | Loss: 0.00109527
Iteration 15/25 | Loss: 0.00109527
Iteration 16/25 | Loss: 0.00109527
Iteration 17/25 | Loss: 0.00109527
Iteration 18/25 | Loss: 0.00109527
Iteration 19/25 | Loss: 0.00109527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001095267478376627, 0.001095267478376627, 0.001095267478376627, 0.001095267478376627, 0.001095267478376627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001095267478376627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28959215
Iteration 2/25 | Loss: 0.00144154
Iteration 3/25 | Loss: 0.00144154
Iteration 4/25 | Loss: 0.00144154
Iteration 5/25 | Loss: 0.00144154
Iteration 6/25 | Loss: 0.00144154
Iteration 7/25 | Loss: 0.00144154
Iteration 8/25 | Loss: 0.00144154
Iteration 9/25 | Loss: 0.00144154
Iteration 10/25 | Loss: 0.00144154
Iteration 11/25 | Loss: 0.00144154
Iteration 12/25 | Loss: 0.00144154
Iteration 13/25 | Loss: 0.00144154
Iteration 14/25 | Loss: 0.00144154
Iteration 15/25 | Loss: 0.00144154
Iteration 16/25 | Loss: 0.00144154
Iteration 17/25 | Loss: 0.00144154
Iteration 18/25 | Loss: 0.00144154
Iteration 19/25 | Loss: 0.00144154
Iteration 20/25 | Loss: 0.00144154
Iteration 21/25 | Loss: 0.00144154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014415397308766842, 0.0014415397308766842, 0.0014415397308766842, 0.0014415397308766842, 0.0014415397308766842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014415397308766842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144154
Iteration 2/1000 | Loss: 0.00003348
Iteration 3/1000 | Loss: 0.00002165
Iteration 4/1000 | Loss: 0.00001804
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001594
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001571
Iteration 10/1000 | Loss: 0.00001559
Iteration 11/1000 | Loss: 0.00001559
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001552
Iteration 16/1000 | Loss: 0.00001551
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001550
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001548
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001548
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001547
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001544
Iteration 36/1000 | Loss: 0.00001544
Iteration 37/1000 | Loss: 0.00001544
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00001544
Iteration 40/1000 | Loss: 0.00001544
Iteration 41/1000 | Loss: 0.00001543
Iteration 42/1000 | Loss: 0.00001543
Iteration 43/1000 | Loss: 0.00001541
Iteration 44/1000 | Loss: 0.00001541
Iteration 45/1000 | Loss: 0.00001541
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001541
Iteration 51/1000 | Loss: 0.00001541
Iteration 52/1000 | Loss: 0.00001540
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001539
Iteration 55/1000 | Loss: 0.00001539
Iteration 56/1000 | Loss: 0.00001538
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001537
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001536
Iteration 61/1000 | Loss: 0.00001536
Iteration 62/1000 | Loss: 0.00001536
Iteration 63/1000 | Loss: 0.00001535
Iteration 64/1000 | Loss: 0.00001535
Iteration 65/1000 | Loss: 0.00001535
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001534
Iteration 69/1000 | Loss: 0.00001534
Iteration 70/1000 | Loss: 0.00001534
Iteration 71/1000 | Loss: 0.00001534
Iteration 72/1000 | Loss: 0.00001534
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001533
Iteration 77/1000 | Loss: 0.00001533
Iteration 78/1000 | Loss: 0.00001533
Iteration 79/1000 | Loss: 0.00001533
Iteration 80/1000 | Loss: 0.00001533
Iteration 81/1000 | Loss: 0.00001533
Iteration 82/1000 | Loss: 0.00001532
Iteration 83/1000 | Loss: 0.00001532
Iteration 84/1000 | Loss: 0.00001532
Iteration 85/1000 | Loss: 0.00001532
Iteration 86/1000 | Loss: 0.00001532
Iteration 87/1000 | Loss: 0.00001532
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.532266105641611e-05, 1.532266105641611e-05, 1.532266105641611e-05, 1.532266105641611e-05, 1.532266105641611e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.532266105641611e-05

Optimization complete. Final v2v error: 3.3423914909362793 mm

Highest mean error: 3.5383353233337402 mm for frame 99

Lowest mean error: 2.9829094409942627 mm for frame 0

Saving results

Total time: 40.20646071434021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374341
Iteration 2/25 | Loss: 0.00113252
Iteration 3/25 | Loss: 0.00107668
Iteration 4/25 | Loss: 0.00106910
Iteration 5/25 | Loss: 0.00106686
Iteration 6/25 | Loss: 0.00106623
Iteration 7/25 | Loss: 0.00106623
Iteration 8/25 | Loss: 0.00106623
Iteration 9/25 | Loss: 0.00106623
Iteration 10/25 | Loss: 0.00106623
Iteration 11/25 | Loss: 0.00106623
Iteration 12/25 | Loss: 0.00106623
Iteration 13/25 | Loss: 0.00106623
Iteration 14/25 | Loss: 0.00106623
Iteration 15/25 | Loss: 0.00106623
Iteration 16/25 | Loss: 0.00106623
Iteration 17/25 | Loss: 0.00106623
Iteration 18/25 | Loss: 0.00106623
Iteration 19/25 | Loss: 0.00106623
Iteration 20/25 | Loss: 0.00106623
Iteration 21/25 | Loss: 0.00106623
Iteration 22/25 | Loss: 0.00106623
Iteration 23/25 | Loss: 0.00106623
Iteration 24/25 | Loss: 0.00106623
Iteration 25/25 | Loss: 0.00106623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82375133
Iteration 2/25 | Loss: 0.00142608
Iteration 3/25 | Loss: 0.00142608
Iteration 4/25 | Loss: 0.00142608
Iteration 5/25 | Loss: 0.00142608
Iteration 6/25 | Loss: 0.00142608
Iteration 7/25 | Loss: 0.00142608
Iteration 8/25 | Loss: 0.00142608
Iteration 9/25 | Loss: 0.00142608
Iteration 10/25 | Loss: 0.00142607
Iteration 11/25 | Loss: 0.00142607
Iteration 12/25 | Loss: 0.00142607
Iteration 13/25 | Loss: 0.00142607
Iteration 14/25 | Loss: 0.00142607
Iteration 15/25 | Loss: 0.00142607
Iteration 16/25 | Loss: 0.00142607
Iteration 17/25 | Loss: 0.00142607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014260746538639069, 0.0014260746538639069, 0.0014260746538639069, 0.0014260746538639069, 0.0014260746538639069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014260746538639069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142607
Iteration 2/1000 | Loss: 0.00003563
Iteration 3/1000 | Loss: 0.00002012
Iteration 4/1000 | Loss: 0.00001563
Iteration 5/1000 | Loss: 0.00001416
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001275
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001251
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001246
Iteration 18/1000 | Loss: 0.00001246
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001245
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001236
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001235
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001234
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001231
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001231
Iteration 88/1000 | Loss: 0.00001231
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001231
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001230
Iteration 118/1000 | Loss: 0.00001230
Iteration 119/1000 | Loss: 0.00001230
Iteration 120/1000 | Loss: 0.00001230
Iteration 121/1000 | Loss: 0.00001230
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001230
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001230
Iteration 141/1000 | Loss: 0.00001230
Iteration 142/1000 | Loss: 0.00001230
Iteration 143/1000 | Loss: 0.00001230
Iteration 144/1000 | Loss: 0.00001230
Iteration 145/1000 | Loss: 0.00001230
Iteration 146/1000 | Loss: 0.00001230
Iteration 147/1000 | Loss: 0.00001230
Iteration 148/1000 | Loss: 0.00001230
Iteration 149/1000 | Loss: 0.00001230
Iteration 150/1000 | Loss: 0.00001230
Iteration 151/1000 | Loss: 0.00001230
Iteration 152/1000 | Loss: 0.00001230
Iteration 153/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.230425277753966e-05, 1.230425277753966e-05, 1.230425277753966e-05, 1.230425277753966e-05, 1.230425277753966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.230425277753966e-05

Optimization complete. Final v2v error: 2.9249706268310547 mm

Highest mean error: 3.3024089336395264 mm for frame 95

Lowest mean error: 2.6384212970733643 mm for frame 6

Saving results

Total time: 29.243205785751343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828057
Iteration 2/25 | Loss: 0.00137516
Iteration 3/25 | Loss: 0.00112336
Iteration 4/25 | Loss: 0.00111269
Iteration 5/25 | Loss: 0.00110994
Iteration 6/25 | Loss: 0.00110971
Iteration 7/25 | Loss: 0.00110971
Iteration 8/25 | Loss: 0.00110971
Iteration 9/25 | Loss: 0.00110971
Iteration 10/25 | Loss: 0.00110971
Iteration 11/25 | Loss: 0.00110971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011097120586782694, 0.0011097120586782694, 0.0011097120586782694, 0.0011097120586782694, 0.0011097120586782694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011097120586782694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08705068
Iteration 2/25 | Loss: 0.00135360
Iteration 3/25 | Loss: 0.00135360
Iteration 4/25 | Loss: 0.00135360
Iteration 5/25 | Loss: 0.00135360
Iteration 6/25 | Loss: 0.00135360
Iteration 7/25 | Loss: 0.00135360
Iteration 8/25 | Loss: 0.00135360
Iteration 9/25 | Loss: 0.00135360
Iteration 10/25 | Loss: 0.00135360
Iteration 11/25 | Loss: 0.00135360
Iteration 12/25 | Loss: 0.00135360
Iteration 13/25 | Loss: 0.00135360
Iteration 14/25 | Loss: 0.00135360
Iteration 15/25 | Loss: 0.00135360
Iteration 16/25 | Loss: 0.00135360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013535999460145831, 0.0013535999460145831, 0.0013535999460145831, 0.0013535999460145831, 0.0013535999460145831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013535999460145831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135360
Iteration 2/1000 | Loss: 0.00005805
Iteration 3/1000 | Loss: 0.00003217
Iteration 4/1000 | Loss: 0.00002651
Iteration 5/1000 | Loss: 0.00002447
Iteration 6/1000 | Loss: 0.00002330
Iteration 7/1000 | Loss: 0.00002228
Iteration 8/1000 | Loss: 0.00002150
Iteration 9/1000 | Loss: 0.00002098
Iteration 10/1000 | Loss: 0.00002058
Iteration 11/1000 | Loss: 0.00002032
Iteration 12/1000 | Loss: 0.00002014
Iteration 13/1000 | Loss: 0.00001993
Iteration 14/1000 | Loss: 0.00001990
Iteration 15/1000 | Loss: 0.00001975
Iteration 16/1000 | Loss: 0.00001968
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00001961
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001955
Iteration 21/1000 | Loss: 0.00001955
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001947
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001939
Iteration 27/1000 | Loss: 0.00001939
Iteration 28/1000 | Loss: 0.00001935
Iteration 29/1000 | Loss: 0.00001935
Iteration 30/1000 | Loss: 0.00001935
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001934
Iteration 33/1000 | Loss: 0.00001930
Iteration 34/1000 | Loss: 0.00001930
Iteration 35/1000 | Loss: 0.00001929
Iteration 36/1000 | Loss: 0.00001929
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001928
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001927
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001927
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001925
Iteration 53/1000 | Loss: 0.00001925
Iteration 54/1000 | Loss: 0.00001925
Iteration 55/1000 | Loss: 0.00001925
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001924
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001924
Iteration 62/1000 | Loss: 0.00001923
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001922
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001921
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001920
Iteration 69/1000 | Loss: 0.00001920
Iteration 70/1000 | Loss: 0.00001919
Iteration 71/1000 | Loss: 0.00001919
Iteration 72/1000 | Loss: 0.00001919
Iteration 73/1000 | Loss: 0.00001919
Iteration 74/1000 | Loss: 0.00001919
Iteration 75/1000 | Loss: 0.00001919
Iteration 76/1000 | Loss: 0.00001919
Iteration 77/1000 | Loss: 0.00001919
Iteration 78/1000 | Loss: 0.00001919
Iteration 79/1000 | Loss: 0.00001919
Iteration 80/1000 | Loss: 0.00001918
Iteration 81/1000 | Loss: 0.00001918
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001915
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001911
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001911
Iteration 95/1000 | Loss: 0.00001910
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001910
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001909
Iteration 101/1000 | Loss: 0.00001909
Iteration 102/1000 | Loss: 0.00001909
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001909
Iteration 106/1000 | Loss: 0.00001909
Iteration 107/1000 | Loss: 0.00001909
Iteration 108/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.9092802176601253e-05, 1.9092802176601253e-05, 1.9092802176601253e-05, 1.9092802176601253e-05, 1.9092802176601253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9092802176601253e-05

Optimization complete. Final v2v error: 3.475083589553833 mm

Highest mean error: 4.843198299407959 mm for frame 168

Lowest mean error: 2.6381847858428955 mm for frame 131

Saving results

Total time: 45.5360164642334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790811
Iteration 2/25 | Loss: 0.00174885
Iteration 3/25 | Loss: 0.00119367
Iteration 4/25 | Loss: 0.00112341
Iteration 5/25 | Loss: 0.00111638
Iteration 6/25 | Loss: 0.00110214
Iteration 7/25 | Loss: 0.00109728
Iteration 8/25 | Loss: 0.00109561
Iteration 9/25 | Loss: 0.00109433
Iteration 10/25 | Loss: 0.00109372
Iteration 11/25 | Loss: 0.00109355
Iteration 12/25 | Loss: 0.00109347
Iteration 13/25 | Loss: 0.00109347
Iteration 14/25 | Loss: 0.00109346
Iteration 15/25 | Loss: 0.00109346
Iteration 16/25 | Loss: 0.00109346
Iteration 17/25 | Loss: 0.00109346
Iteration 18/25 | Loss: 0.00109346
Iteration 19/25 | Loss: 0.00109346
Iteration 20/25 | Loss: 0.00109346
Iteration 21/25 | Loss: 0.00109346
Iteration 22/25 | Loss: 0.00109346
Iteration 23/25 | Loss: 0.00109346
Iteration 24/25 | Loss: 0.00109345
Iteration 25/25 | Loss: 0.00109345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85351348
Iteration 2/25 | Loss: 0.00127675
Iteration 3/25 | Loss: 0.00127675
Iteration 4/25 | Loss: 0.00127675
Iteration 5/25 | Loss: 0.00127675
Iteration 6/25 | Loss: 0.00127675
Iteration 7/25 | Loss: 0.00127675
Iteration 8/25 | Loss: 0.00127675
Iteration 9/25 | Loss: 0.00127675
Iteration 10/25 | Loss: 0.00127675
Iteration 11/25 | Loss: 0.00127675
Iteration 12/25 | Loss: 0.00127675
Iteration 13/25 | Loss: 0.00127675
Iteration 14/25 | Loss: 0.00127675
Iteration 15/25 | Loss: 0.00127675
Iteration 16/25 | Loss: 0.00127675
Iteration 17/25 | Loss: 0.00127675
Iteration 18/25 | Loss: 0.00127675
Iteration 19/25 | Loss: 0.00127675
Iteration 20/25 | Loss: 0.00127675
Iteration 21/25 | Loss: 0.00127675
Iteration 22/25 | Loss: 0.00127675
Iteration 23/25 | Loss: 0.00127675
Iteration 24/25 | Loss: 0.00127675
Iteration 25/25 | Loss: 0.00127675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127675
Iteration 2/1000 | Loss: 0.00003731
Iteration 3/1000 | Loss: 0.00002344
Iteration 4/1000 | Loss: 0.00001918
Iteration 5/1000 | Loss: 0.00001808
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001726
Iteration 8/1000 | Loss: 0.00001687
Iteration 9/1000 | Loss: 0.00001660
Iteration 10/1000 | Loss: 0.00001653
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001647
Iteration 13/1000 | Loss: 0.00001645
Iteration 14/1000 | Loss: 0.00001642
Iteration 15/1000 | Loss: 0.00001642
Iteration 16/1000 | Loss: 0.00001641
Iteration 17/1000 | Loss: 0.00001637
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001631
Iteration 21/1000 | Loss: 0.00001631
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001630
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001629
Iteration 26/1000 | Loss: 0.00001628
Iteration 27/1000 | Loss: 0.00001628
Iteration 28/1000 | Loss: 0.00001627
Iteration 29/1000 | Loss: 0.00001627
Iteration 30/1000 | Loss: 0.00001627
Iteration 31/1000 | Loss: 0.00001626
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00001625
Iteration 35/1000 | Loss: 0.00001625
Iteration 36/1000 | Loss: 0.00001625
Iteration 37/1000 | Loss: 0.00001625
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001625
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001622
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001617
Iteration 75/1000 | Loss: 0.00001617
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001615
Iteration 82/1000 | Loss: 0.00001615
Iteration 83/1000 | Loss: 0.00001615
Iteration 84/1000 | Loss: 0.00001615
Iteration 85/1000 | Loss: 0.00001615
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001614
Iteration 88/1000 | Loss: 0.00001614
Iteration 89/1000 | Loss: 0.00001614
Iteration 90/1000 | Loss: 0.00001614
Iteration 91/1000 | Loss: 0.00001614
Iteration 92/1000 | Loss: 0.00001614
Iteration 93/1000 | Loss: 0.00001614
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001612
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001611
Iteration 102/1000 | Loss: 0.00001611
Iteration 103/1000 | Loss: 0.00001611
Iteration 104/1000 | Loss: 0.00001611
Iteration 105/1000 | Loss: 0.00001610
Iteration 106/1000 | Loss: 0.00001610
Iteration 107/1000 | Loss: 0.00001610
Iteration 108/1000 | Loss: 0.00001610
Iteration 109/1000 | Loss: 0.00001610
Iteration 110/1000 | Loss: 0.00001610
Iteration 111/1000 | Loss: 0.00001610
Iteration 112/1000 | Loss: 0.00001609
Iteration 113/1000 | Loss: 0.00001609
Iteration 114/1000 | Loss: 0.00001609
Iteration 115/1000 | Loss: 0.00001609
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001608
Iteration 118/1000 | Loss: 0.00001608
Iteration 119/1000 | Loss: 0.00001608
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001608
Iteration 122/1000 | Loss: 0.00001608
Iteration 123/1000 | Loss: 0.00001608
Iteration 124/1000 | Loss: 0.00001608
Iteration 125/1000 | Loss: 0.00001608
Iteration 126/1000 | Loss: 0.00001608
Iteration 127/1000 | Loss: 0.00001608
Iteration 128/1000 | Loss: 0.00001608
Iteration 129/1000 | Loss: 0.00001608
Iteration 130/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.6078072803793475e-05, 1.6078072803793475e-05, 1.6078072803793475e-05, 1.6078072803793475e-05, 1.6078072803793475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6078072803793475e-05

Optimization complete. Final v2v error: 3.3960025310516357 mm

Highest mean error: 3.809325933456421 mm for frame 45

Lowest mean error: 3.0576999187469482 mm for frame 185

Saving results

Total time: 45.246471881866455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833918
Iteration 2/25 | Loss: 0.00203347
Iteration 3/25 | Loss: 0.00145629
Iteration 4/25 | Loss: 0.00139495
Iteration 5/25 | Loss: 0.00138276
Iteration 6/25 | Loss: 0.00137397
Iteration 7/25 | Loss: 0.00137456
Iteration 8/25 | Loss: 0.00137488
Iteration 9/25 | Loss: 0.00137169
Iteration 10/25 | Loss: 0.00137029
Iteration 11/25 | Loss: 0.00136995
Iteration 12/25 | Loss: 0.00136982
Iteration 13/25 | Loss: 0.00136981
Iteration 14/25 | Loss: 0.00136981
Iteration 15/25 | Loss: 0.00136981
Iteration 16/25 | Loss: 0.00136981
Iteration 17/25 | Loss: 0.00136980
Iteration 18/25 | Loss: 0.00136980
Iteration 19/25 | Loss: 0.00136980
Iteration 20/25 | Loss: 0.00136980
Iteration 21/25 | Loss: 0.00136980
Iteration 22/25 | Loss: 0.00136980
Iteration 23/25 | Loss: 0.00136980
Iteration 24/25 | Loss: 0.00136980
Iteration 25/25 | Loss: 0.00136980

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26256955
Iteration 2/25 | Loss: 0.00096400
Iteration 3/25 | Loss: 0.00096399
Iteration 4/25 | Loss: 0.00096399
Iteration 5/25 | Loss: 0.00096399
Iteration 6/25 | Loss: 0.00096399
Iteration 7/25 | Loss: 0.00096399
Iteration 8/25 | Loss: 0.00096399
Iteration 9/25 | Loss: 0.00096399
Iteration 10/25 | Loss: 0.00096399
Iteration 11/25 | Loss: 0.00096399
Iteration 12/25 | Loss: 0.00096399
Iteration 13/25 | Loss: 0.00096399
Iteration 14/25 | Loss: 0.00096399
Iteration 15/25 | Loss: 0.00096399
Iteration 16/25 | Loss: 0.00096399
Iteration 17/25 | Loss: 0.00096399
Iteration 18/25 | Loss: 0.00096399
Iteration 19/25 | Loss: 0.00096399
Iteration 20/25 | Loss: 0.00096399
Iteration 21/25 | Loss: 0.00096399
Iteration 22/25 | Loss: 0.00096399
Iteration 23/25 | Loss: 0.00096399
Iteration 24/25 | Loss: 0.00096399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000963990984018892, 0.000963990984018892, 0.000963990984018892, 0.000963990984018892, 0.000963990984018892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000963990984018892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096399
Iteration 2/1000 | Loss: 0.00021685
Iteration 3/1000 | Loss: 0.00004794
Iteration 4/1000 | Loss: 0.00003229
Iteration 5/1000 | Loss: 0.00002744
Iteration 6/1000 | Loss: 0.00002579
Iteration 7/1000 | Loss: 0.00002473
Iteration 8/1000 | Loss: 0.00002419
Iteration 9/1000 | Loss: 0.00002383
Iteration 10/1000 | Loss: 0.00002348
Iteration 11/1000 | Loss: 0.00002328
Iteration 12/1000 | Loss: 0.00002327
Iteration 13/1000 | Loss: 0.00002314
Iteration 14/1000 | Loss: 0.00002300
Iteration 15/1000 | Loss: 0.00002297
Iteration 16/1000 | Loss: 0.00002296
Iteration 17/1000 | Loss: 0.00002293
Iteration 18/1000 | Loss: 0.00002290
Iteration 19/1000 | Loss: 0.00002278
Iteration 20/1000 | Loss: 0.00002272
Iteration 21/1000 | Loss: 0.00002269
Iteration 22/1000 | Loss: 0.00002268
Iteration 23/1000 | Loss: 0.00002268
Iteration 24/1000 | Loss: 0.00002267
Iteration 25/1000 | Loss: 0.00002267
Iteration 26/1000 | Loss: 0.00002266
Iteration 27/1000 | Loss: 0.00002266
Iteration 28/1000 | Loss: 0.00002260
Iteration 29/1000 | Loss: 0.00002259
Iteration 30/1000 | Loss: 0.00002257
Iteration 31/1000 | Loss: 0.00002257
Iteration 32/1000 | Loss: 0.00002257
Iteration 33/1000 | Loss: 0.00002256
Iteration 34/1000 | Loss: 0.00002256
Iteration 35/1000 | Loss: 0.00002255
Iteration 36/1000 | Loss: 0.00002255
Iteration 37/1000 | Loss: 0.00002255
Iteration 38/1000 | Loss: 0.00002255
Iteration 39/1000 | Loss: 0.00002255
Iteration 40/1000 | Loss: 0.00002255
Iteration 41/1000 | Loss: 0.00002255
Iteration 42/1000 | Loss: 0.00002255
Iteration 43/1000 | Loss: 0.00002254
Iteration 44/1000 | Loss: 0.00002254
Iteration 45/1000 | Loss: 0.00002254
Iteration 46/1000 | Loss: 0.00002254
Iteration 47/1000 | Loss: 0.00002254
Iteration 48/1000 | Loss: 0.00002254
Iteration 49/1000 | Loss: 0.00002254
Iteration 50/1000 | Loss: 0.00002253
Iteration 51/1000 | Loss: 0.00002253
Iteration 52/1000 | Loss: 0.00002253
Iteration 53/1000 | Loss: 0.00002253
Iteration 54/1000 | Loss: 0.00002252
Iteration 55/1000 | Loss: 0.00002252
Iteration 56/1000 | Loss: 0.00002252
Iteration 57/1000 | Loss: 0.00002252
Iteration 58/1000 | Loss: 0.00002251
Iteration 59/1000 | Loss: 0.00002251
Iteration 60/1000 | Loss: 0.00002251
Iteration 61/1000 | Loss: 0.00002250
Iteration 62/1000 | Loss: 0.00002250
Iteration 63/1000 | Loss: 0.00002250
Iteration 64/1000 | Loss: 0.00002250
Iteration 65/1000 | Loss: 0.00002250
Iteration 66/1000 | Loss: 0.00002249
Iteration 67/1000 | Loss: 0.00002249
Iteration 68/1000 | Loss: 0.00002249
Iteration 69/1000 | Loss: 0.00002249
Iteration 70/1000 | Loss: 0.00002249
Iteration 71/1000 | Loss: 0.00002249
Iteration 72/1000 | Loss: 0.00002249
Iteration 73/1000 | Loss: 0.00002249
Iteration 74/1000 | Loss: 0.00002248
Iteration 75/1000 | Loss: 0.00002248
Iteration 76/1000 | Loss: 0.00002248
Iteration 77/1000 | Loss: 0.00002248
Iteration 78/1000 | Loss: 0.00002247
Iteration 79/1000 | Loss: 0.00002247
Iteration 80/1000 | Loss: 0.00002247
Iteration 81/1000 | Loss: 0.00002247
Iteration 82/1000 | Loss: 0.00002247
Iteration 83/1000 | Loss: 0.00002247
Iteration 84/1000 | Loss: 0.00002247
Iteration 85/1000 | Loss: 0.00002247
Iteration 86/1000 | Loss: 0.00002247
Iteration 87/1000 | Loss: 0.00002247
Iteration 88/1000 | Loss: 0.00002247
Iteration 89/1000 | Loss: 0.00002247
Iteration 90/1000 | Loss: 0.00002247
Iteration 91/1000 | Loss: 0.00002247
Iteration 92/1000 | Loss: 0.00002247
Iteration 93/1000 | Loss: 0.00002246
Iteration 94/1000 | Loss: 0.00002246
Iteration 95/1000 | Loss: 0.00002246
Iteration 96/1000 | Loss: 0.00002246
Iteration 97/1000 | Loss: 0.00002246
Iteration 98/1000 | Loss: 0.00002246
Iteration 99/1000 | Loss: 0.00002245
Iteration 100/1000 | Loss: 0.00002245
Iteration 101/1000 | Loss: 0.00002245
Iteration 102/1000 | Loss: 0.00002245
Iteration 103/1000 | Loss: 0.00002245
Iteration 104/1000 | Loss: 0.00002245
Iteration 105/1000 | Loss: 0.00002244
Iteration 106/1000 | Loss: 0.00002244
Iteration 107/1000 | Loss: 0.00002244
Iteration 108/1000 | Loss: 0.00002244
Iteration 109/1000 | Loss: 0.00002244
Iteration 110/1000 | Loss: 0.00002244
Iteration 111/1000 | Loss: 0.00002244
Iteration 112/1000 | Loss: 0.00002244
Iteration 113/1000 | Loss: 0.00002244
Iteration 114/1000 | Loss: 0.00002244
Iteration 115/1000 | Loss: 0.00002244
Iteration 116/1000 | Loss: 0.00002244
Iteration 117/1000 | Loss: 0.00002244
Iteration 118/1000 | Loss: 0.00002244
Iteration 119/1000 | Loss: 0.00002244
Iteration 120/1000 | Loss: 0.00002244
Iteration 121/1000 | Loss: 0.00002244
Iteration 122/1000 | Loss: 0.00002244
Iteration 123/1000 | Loss: 0.00002244
Iteration 124/1000 | Loss: 0.00002244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.2438212909037247e-05, 2.2438212909037247e-05, 2.2438212909037247e-05, 2.2438212909037247e-05, 2.2438212909037247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2438212909037247e-05

Optimization complete. Final v2v error: 4.131934642791748 mm

Highest mean error: 4.494111061096191 mm for frame 111

Lowest mean error: 3.884148597717285 mm for frame 44

Saving results

Total time: 56.60344123840332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422822
Iteration 2/25 | Loss: 0.00116078
Iteration 3/25 | Loss: 0.00109776
Iteration 4/25 | Loss: 0.00109159
Iteration 5/25 | Loss: 0.00109040
Iteration 6/25 | Loss: 0.00109040
Iteration 7/25 | Loss: 0.00109040
Iteration 8/25 | Loss: 0.00109040
Iteration 9/25 | Loss: 0.00109040
Iteration 10/25 | Loss: 0.00109040
Iteration 11/25 | Loss: 0.00109040
Iteration 12/25 | Loss: 0.00109040
Iteration 13/25 | Loss: 0.00109040
Iteration 14/25 | Loss: 0.00109040
Iteration 15/25 | Loss: 0.00109040
Iteration 16/25 | Loss: 0.00109040
Iteration 17/25 | Loss: 0.00109040
Iteration 18/25 | Loss: 0.00109040
Iteration 19/25 | Loss: 0.00109040
Iteration 20/25 | Loss: 0.00109040
Iteration 21/25 | Loss: 0.00109040
Iteration 22/25 | Loss: 0.00109040
Iteration 23/25 | Loss: 0.00109040
Iteration 24/25 | Loss: 0.00109040
Iteration 25/25 | Loss: 0.00109040

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.28938842
Iteration 2/25 | Loss: 0.00120136
Iteration 3/25 | Loss: 0.00120136
Iteration 4/25 | Loss: 0.00120136
Iteration 5/25 | Loss: 0.00120136
Iteration 6/25 | Loss: 0.00120136
Iteration 7/25 | Loss: 0.00120136
Iteration 8/25 | Loss: 0.00120136
Iteration 9/25 | Loss: 0.00120135
Iteration 10/25 | Loss: 0.00120135
Iteration 11/25 | Loss: 0.00120135
Iteration 12/25 | Loss: 0.00120135
Iteration 13/25 | Loss: 0.00120135
Iteration 14/25 | Loss: 0.00120135
Iteration 15/25 | Loss: 0.00120135
Iteration 16/25 | Loss: 0.00120135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012013542000204325, 0.0012013542000204325, 0.0012013542000204325, 0.0012013542000204325, 0.0012013542000204325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012013542000204325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120135
Iteration 2/1000 | Loss: 0.00003993
Iteration 3/1000 | Loss: 0.00002339
Iteration 4/1000 | Loss: 0.00001768
Iteration 5/1000 | Loss: 0.00001620
Iteration 6/1000 | Loss: 0.00001555
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001439
Iteration 10/1000 | Loss: 0.00001422
Iteration 11/1000 | Loss: 0.00001422
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001422
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001421
Iteration 16/1000 | Loss: 0.00001419
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001415
Iteration 20/1000 | Loss: 0.00001414
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001407
Iteration 23/1000 | Loss: 0.00001406
Iteration 24/1000 | Loss: 0.00001403
Iteration 25/1000 | Loss: 0.00001403
Iteration 26/1000 | Loss: 0.00001403
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001403
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001403
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001402
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001402
Iteration 38/1000 | Loss: 0.00001402
Iteration 39/1000 | Loss: 0.00001402
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001402
Iteration 43/1000 | Loss: 0.00001402
Iteration 44/1000 | Loss: 0.00001402
Iteration 45/1000 | Loss: 0.00001401
Iteration 46/1000 | Loss: 0.00001401
Iteration 47/1000 | Loss: 0.00001401
Iteration 48/1000 | Loss: 0.00001401
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001399
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001399
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001398
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001397
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001397
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001396
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001395
Iteration 79/1000 | Loss: 0.00001395
Iteration 80/1000 | Loss: 0.00001395
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001393
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001392
Iteration 92/1000 | Loss: 0.00001392
Iteration 93/1000 | Loss: 0.00001392
Iteration 94/1000 | Loss: 0.00001392
Iteration 95/1000 | Loss: 0.00001392
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001391
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001390
Iteration 101/1000 | Loss: 0.00001390
Iteration 102/1000 | Loss: 0.00001390
Iteration 103/1000 | Loss: 0.00001390
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001389
Iteration 107/1000 | Loss: 0.00001389
Iteration 108/1000 | Loss: 0.00001389
Iteration 109/1000 | Loss: 0.00001389
Iteration 110/1000 | Loss: 0.00001389
Iteration 111/1000 | Loss: 0.00001389
Iteration 112/1000 | Loss: 0.00001389
Iteration 113/1000 | Loss: 0.00001388
Iteration 114/1000 | Loss: 0.00001388
Iteration 115/1000 | Loss: 0.00001388
Iteration 116/1000 | Loss: 0.00001388
Iteration 117/1000 | Loss: 0.00001388
Iteration 118/1000 | Loss: 0.00001388
Iteration 119/1000 | Loss: 0.00001388
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001387
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001387
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001385
Iteration 144/1000 | Loss: 0.00001385
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00001383
Iteration 151/1000 | Loss: 0.00001383
Iteration 152/1000 | Loss: 0.00001383
Iteration 153/1000 | Loss: 0.00001383
Iteration 154/1000 | Loss: 0.00001383
Iteration 155/1000 | Loss: 0.00001383
Iteration 156/1000 | Loss: 0.00001383
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001383
Iteration 160/1000 | Loss: 0.00001382
Iteration 161/1000 | Loss: 0.00001382
Iteration 162/1000 | Loss: 0.00001382
Iteration 163/1000 | Loss: 0.00001382
Iteration 164/1000 | Loss: 0.00001382
Iteration 165/1000 | Loss: 0.00001382
Iteration 166/1000 | Loss: 0.00001382
Iteration 167/1000 | Loss: 0.00001382
Iteration 168/1000 | Loss: 0.00001382
Iteration 169/1000 | Loss: 0.00001382
Iteration 170/1000 | Loss: 0.00001382
Iteration 171/1000 | Loss: 0.00001382
Iteration 172/1000 | Loss: 0.00001382
Iteration 173/1000 | Loss: 0.00001382
Iteration 174/1000 | Loss: 0.00001382
Iteration 175/1000 | Loss: 0.00001382
Iteration 176/1000 | Loss: 0.00001381
Iteration 177/1000 | Loss: 0.00001381
Iteration 178/1000 | Loss: 0.00001381
Iteration 179/1000 | Loss: 0.00001381
Iteration 180/1000 | Loss: 0.00001381
Iteration 181/1000 | Loss: 0.00001381
Iteration 182/1000 | Loss: 0.00001381
Iteration 183/1000 | Loss: 0.00001381
Iteration 184/1000 | Loss: 0.00001381
Iteration 185/1000 | Loss: 0.00001381
Iteration 186/1000 | Loss: 0.00001381
Iteration 187/1000 | Loss: 0.00001381
Iteration 188/1000 | Loss: 0.00001380
Iteration 189/1000 | Loss: 0.00001380
Iteration 190/1000 | Loss: 0.00001380
Iteration 191/1000 | Loss: 0.00001380
Iteration 192/1000 | Loss: 0.00001380
Iteration 193/1000 | Loss: 0.00001380
Iteration 194/1000 | Loss: 0.00001380
Iteration 195/1000 | Loss: 0.00001380
Iteration 196/1000 | Loss: 0.00001380
Iteration 197/1000 | Loss: 0.00001380
Iteration 198/1000 | Loss: 0.00001380
Iteration 199/1000 | Loss: 0.00001380
Iteration 200/1000 | Loss: 0.00001380
Iteration 201/1000 | Loss: 0.00001380
Iteration 202/1000 | Loss: 0.00001380
Iteration 203/1000 | Loss: 0.00001380
Iteration 204/1000 | Loss: 0.00001380
Iteration 205/1000 | Loss: 0.00001380
Iteration 206/1000 | Loss: 0.00001380
Iteration 207/1000 | Loss: 0.00001380
Iteration 208/1000 | Loss: 0.00001380
Iteration 209/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.3802052308165003e-05, 1.3802052308165003e-05, 1.3802052308165003e-05, 1.3802052308165003e-05, 1.3802052308165003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3802052308165003e-05

Optimization complete. Final v2v error: 3.1797361373901367 mm

Highest mean error: 3.4724888801574707 mm for frame 68

Lowest mean error: 2.9337995052337646 mm for frame 1

Saving results

Total time: 36.053515911102295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736361
Iteration 2/25 | Loss: 0.00136962
Iteration 3/25 | Loss: 0.00122036
Iteration 4/25 | Loss: 0.00120439
Iteration 5/25 | Loss: 0.00120192
Iteration 6/25 | Loss: 0.00120192
Iteration 7/25 | Loss: 0.00120192
Iteration 8/25 | Loss: 0.00120192
Iteration 9/25 | Loss: 0.00120192
Iteration 10/25 | Loss: 0.00120192
Iteration 11/25 | Loss: 0.00120192
Iteration 12/25 | Loss: 0.00120192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012019198620691895, 0.0012019198620691895, 0.0012019198620691895, 0.0012019198620691895, 0.0012019198620691895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012019198620691895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.07023764
Iteration 2/25 | Loss: 0.00139958
Iteration 3/25 | Loss: 0.00139956
Iteration 4/25 | Loss: 0.00139956
Iteration 5/25 | Loss: 0.00139956
Iteration 6/25 | Loss: 0.00139956
Iteration 7/25 | Loss: 0.00139956
Iteration 8/25 | Loss: 0.00139956
Iteration 9/25 | Loss: 0.00139956
Iteration 10/25 | Loss: 0.00139956
Iteration 11/25 | Loss: 0.00139956
Iteration 12/25 | Loss: 0.00139956
Iteration 13/25 | Loss: 0.00139956
Iteration 14/25 | Loss: 0.00139956
Iteration 15/25 | Loss: 0.00139956
Iteration 16/25 | Loss: 0.00139956
Iteration 17/25 | Loss: 0.00139956
Iteration 18/25 | Loss: 0.00139956
Iteration 19/25 | Loss: 0.00139956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001399555359967053, 0.001399555359967053, 0.001399555359967053, 0.001399555359967053, 0.001399555359967053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001399555359967053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139956
Iteration 2/1000 | Loss: 0.00005292
Iteration 3/1000 | Loss: 0.00003009
Iteration 4/1000 | Loss: 0.00002579
Iteration 5/1000 | Loss: 0.00002427
Iteration 6/1000 | Loss: 0.00002299
Iteration 7/1000 | Loss: 0.00002223
Iteration 8/1000 | Loss: 0.00002172
Iteration 9/1000 | Loss: 0.00002130
Iteration 10/1000 | Loss: 0.00002099
Iteration 11/1000 | Loss: 0.00002088
Iteration 12/1000 | Loss: 0.00002088
Iteration 13/1000 | Loss: 0.00002088
Iteration 14/1000 | Loss: 0.00002088
Iteration 15/1000 | Loss: 0.00002071
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002048
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002046
Iteration 21/1000 | Loss: 0.00002046
Iteration 22/1000 | Loss: 0.00002045
Iteration 23/1000 | Loss: 0.00002045
Iteration 24/1000 | Loss: 0.00002044
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002042
Iteration 27/1000 | Loss: 0.00002042
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002041
Iteration 30/1000 | Loss: 0.00002041
Iteration 31/1000 | Loss: 0.00002041
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002040
Iteration 34/1000 | Loss: 0.00002040
Iteration 35/1000 | Loss: 0.00002040
Iteration 36/1000 | Loss: 0.00002039
Iteration 37/1000 | Loss: 0.00002039
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00002038
Iteration 40/1000 | Loss: 0.00002038
Iteration 41/1000 | Loss: 0.00002037
Iteration 42/1000 | Loss: 0.00002037
Iteration 43/1000 | Loss: 0.00002037
Iteration 44/1000 | Loss: 0.00002037
Iteration 45/1000 | Loss: 0.00002037
Iteration 46/1000 | Loss: 0.00002037
Iteration 47/1000 | Loss: 0.00002036
Iteration 48/1000 | Loss: 0.00002036
Iteration 49/1000 | Loss: 0.00002036
Iteration 50/1000 | Loss: 0.00002036
Iteration 51/1000 | Loss: 0.00002035
Iteration 52/1000 | Loss: 0.00002035
Iteration 53/1000 | Loss: 0.00002035
Iteration 54/1000 | Loss: 0.00002035
Iteration 55/1000 | Loss: 0.00002035
Iteration 56/1000 | Loss: 0.00002035
Iteration 57/1000 | Loss: 0.00002034
Iteration 58/1000 | Loss: 0.00002034
Iteration 59/1000 | Loss: 0.00002034
Iteration 60/1000 | Loss: 0.00002034
Iteration 61/1000 | Loss: 0.00002034
Iteration 62/1000 | Loss: 0.00002034
Iteration 63/1000 | Loss: 0.00002034
Iteration 64/1000 | Loss: 0.00002033
Iteration 65/1000 | Loss: 0.00002033
Iteration 66/1000 | Loss: 0.00002033
Iteration 67/1000 | Loss: 0.00002033
Iteration 68/1000 | Loss: 0.00002033
Iteration 69/1000 | Loss: 0.00002032
Iteration 70/1000 | Loss: 0.00002032
Iteration 71/1000 | Loss: 0.00002032
Iteration 72/1000 | Loss: 0.00002032
Iteration 73/1000 | Loss: 0.00002032
Iteration 74/1000 | Loss: 0.00002032
Iteration 75/1000 | Loss: 0.00002031
Iteration 76/1000 | Loss: 0.00002031
Iteration 77/1000 | Loss: 0.00002031
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00002031
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002030
Iteration 82/1000 | Loss: 0.00002030
Iteration 83/1000 | Loss: 0.00002030
Iteration 84/1000 | Loss: 0.00002030
Iteration 85/1000 | Loss: 0.00002030
Iteration 86/1000 | Loss: 0.00002030
Iteration 87/1000 | Loss: 0.00002030
Iteration 88/1000 | Loss: 0.00002030
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00002029
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002028
Iteration 100/1000 | Loss: 0.00002028
Iteration 101/1000 | Loss: 0.00002028
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002027
Iteration 105/1000 | Loss: 0.00002027
Iteration 106/1000 | Loss: 0.00002027
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002027
Iteration 111/1000 | Loss: 0.00002027
Iteration 112/1000 | Loss: 0.00002026
Iteration 113/1000 | Loss: 0.00002026
Iteration 114/1000 | Loss: 0.00002025
Iteration 115/1000 | Loss: 0.00002025
Iteration 116/1000 | Loss: 0.00002025
Iteration 117/1000 | Loss: 0.00002024
Iteration 118/1000 | Loss: 0.00002024
Iteration 119/1000 | Loss: 0.00002024
Iteration 120/1000 | Loss: 0.00002024
Iteration 121/1000 | Loss: 0.00002024
Iteration 122/1000 | Loss: 0.00002024
Iteration 123/1000 | Loss: 0.00002024
Iteration 124/1000 | Loss: 0.00002023
Iteration 125/1000 | Loss: 0.00002023
Iteration 126/1000 | Loss: 0.00002023
Iteration 127/1000 | Loss: 0.00002022
Iteration 128/1000 | Loss: 0.00002022
Iteration 129/1000 | Loss: 0.00002022
Iteration 130/1000 | Loss: 0.00002022
Iteration 131/1000 | Loss: 0.00002021
Iteration 132/1000 | Loss: 0.00002021
Iteration 133/1000 | Loss: 0.00002021
Iteration 134/1000 | Loss: 0.00002020
Iteration 135/1000 | Loss: 0.00002020
Iteration 136/1000 | Loss: 0.00002020
Iteration 137/1000 | Loss: 0.00002020
Iteration 138/1000 | Loss: 0.00002020
Iteration 139/1000 | Loss: 0.00002020
Iteration 140/1000 | Loss: 0.00002020
Iteration 141/1000 | Loss: 0.00002020
Iteration 142/1000 | Loss: 0.00002020
Iteration 143/1000 | Loss: 0.00002020
Iteration 144/1000 | Loss: 0.00002020
Iteration 145/1000 | Loss: 0.00002020
Iteration 146/1000 | Loss: 0.00002019
Iteration 147/1000 | Loss: 0.00002019
Iteration 148/1000 | Loss: 0.00002019
Iteration 149/1000 | Loss: 0.00002019
Iteration 150/1000 | Loss: 0.00002019
Iteration 151/1000 | Loss: 0.00002019
Iteration 152/1000 | Loss: 0.00002019
Iteration 153/1000 | Loss: 0.00002019
Iteration 154/1000 | Loss: 0.00002019
Iteration 155/1000 | Loss: 0.00002019
Iteration 156/1000 | Loss: 0.00002019
Iteration 157/1000 | Loss: 0.00002019
Iteration 158/1000 | Loss: 0.00002018
Iteration 159/1000 | Loss: 0.00002018
Iteration 160/1000 | Loss: 0.00002018
Iteration 161/1000 | Loss: 0.00002018
Iteration 162/1000 | Loss: 0.00002018
Iteration 163/1000 | Loss: 0.00002018
Iteration 164/1000 | Loss: 0.00002018
Iteration 165/1000 | Loss: 0.00002018
Iteration 166/1000 | Loss: 0.00002017
Iteration 167/1000 | Loss: 0.00002017
Iteration 168/1000 | Loss: 0.00002017
Iteration 169/1000 | Loss: 0.00002017
Iteration 170/1000 | Loss: 0.00002017
Iteration 171/1000 | Loss: 0.00002017
Iteration 172/1000 | Loss: 0.00002017
Iteration 173/1000 | Loss: 0.00002017
Iteration 174/1000 | Loss: 0.00002017
Iteration 175/1000 | Loss: 0.00002017
Iteration 176/1000 | Loss: 0.00002017
Iteration 177/1000 | Loss: 0.00002017
Iteration 178/1000 | Loss: 0.00002017
Iteration 179/1000 | Loss: 0.00002017
Iteration 180/1000 | Loss: 0.00002017
Iteration 181/1000 | Loss: 0.00002017
Iteration 182/1000 | Loss: 0.00002017
Iteration 183/1000 | Loss: 0.00002017
Iteration 184/1000 | Loss: 0.00002017
Iteration 185/1000 | Loss: 0.00002017
Iteration 186/1000 | Loss: 0.00002017
Iteration 187/1000 | Loss: 0.00002017
Iteration 188/1000 | Loss: 0.00002017
Iteration 189/1000 | Loss: 0.00002017
Iteration 190/1000 | Loss: 0.00002017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.017148472077679e-05, 2.017148472077679e-05, 2.017148472077679e-05, 2.017148472077679e-05, 2.017148472077679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.017148472077679e-05

Optimization complete. Final v2v error: 3.74021577835083 mm

Highest mean error: 4.349925518035889 mm for frame 75

Lowest mean error: 3.1615729331970215 mm for frame 239

Saving results

Total time: 43.40528082847595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886175
Iteration 2/25 | Loss: 0.00177705
Iteration 3/25 | Loss: 0.00134757
Iteration 4/25 | Loss: 0.00128092
Iteration 5/25 | Loss: 0.00126402
Iteration 6/25 | Loss: 0.00124700
Iteration 7/25 | Loss: 0.00123699
Iteration 8/25 | Loss: 0.00122623
Iteration 9/25 | Loss: 0.00122241
Iteration 10/25 | Loss: 0.00122137
Iteration 11/25 | Loss: 0.00122110
Iteration 12/25 | Loss: 0.00122104
Iteration 13/25 | Loss: 0.00122104
Iteration 14/25 | Loss: 0.00122104
Iteration 15/25 | Loss: 0.00122104
Iteration 16/25 | Loss: 0.00122104
Iteration 17/25 | Loss: 0.00122104
Iteration 18/25 | Loss: 0.00122104
Iteration 19/25 | Loss: 0.00122103
Iteration 20/25 | Loss: 0.00122103
Iteration 21/25 | Loss: 0.00122103
Iteration 22/25 | Loss: 0.00122103
Iteration 23/25 | Loss: 0.00122103
Iteration 24/25 | Loss: 0.00122103
Iteration 25/25 | Loss: 0.00122103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20138514
Iteration 2/25 | Loss: 0.00130356
Iteration 3/25 | Loss: 0.00130355
Iteration 4/25 | Loss: 0.00130355
Iteration 5/25 | Loss: 0.00130355
Iteration 6/25 | Loss: 0.00130355
Iteration 7/25 | Loss: 0.00130355
Iteration 8/25 | Loss: 0.00130355
Iteration 9/25 | Loss: 0.00130355
Iteration 10/25 | Loss: 0.00130355
Iteration 11/25 | Loss: 0.00130355
Iteration 12/25 | Loss: 0.00130355
Iteration 13/25 | Loss: 0.00130355
Iteration 14/25 | Loss: 0.00130355
Iteration 15/25 | Loss: 0.00130355
Iteration 16/25 | Loss: 0.00130355
Iteration 17/25 | Loss: 0.00130355
Iteration 18/25 | Loss: 0.00130355
Iteration 19/25 | Loss: 0.00130355
Iteration 20/25 | Loss: 0.00130355
Iteration 21/25 | Loss: 0.00130355
Iteration 22/25 | Loss: 0.00130355
Iteration 23/25 | Loss: 0.00130355
Iteration 24/25 | Loss: 0.00130355
Iteration 25/25 | Loss: 0.00130355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130355
Iteration 2/1000 | Loss: 0.00007095
Iteration 3/1000 | Loss: 0.00003330
Iteration 4/1000 | Loss: 0.00002692
Iteration 5/1000 | Loss: 0.00002343
Iteration 6/1000 | Loss: 0.00002142
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00001973
Iteration 9/1000 | Loss: 0.00001932
Iteration 10/1000 | Loss: 0.00001910
Iteration 11/1000 | Loss: 0.00001886
Iteration 12/1000 | Loss: 0.00001879
Iteration 13/1000 | Loss: 0.00001878
Iteration 14/1000 | Loss: 0.00001864
Iteration 15/1000 | Loss: 0.00001862
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001862
Iteration 18/1000 | Loss: 0.00001862
Iteration 19/1000 | Loss: 0.00001861
Iteration 20/1000 | Loss: 0.00001861
Iteration 21/1000 | Loss: 0.00001859
Iteration 22/1000 | Loss: 0.00001856
Iteration 23/1000 | Loss: 0.00001856
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001843
Iteration 27/1000 | Loss: 0.00001842
Iteration 28/1000 | Loss: 0.00001841
Iteration 29/1000 | Loss: 0.00001840
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00001840
Iteration 32/1000 | Loss: 0.00001839
Iteration 33/1000 | Loss: 0.00001839
Iteration 34/1000 | Loss: 0.00001839
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001838
Iteration 37/1000 | Loss: 0.00001838
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001837
Iteration 44/1000 | Loss: 0.00001837
Iteration 45/1000 | Loss: 0.00001836
Iteration 46/1000 | Loss: 0.00001836
Iteration 47/1000 | Loss: 0.00001836
Iteration 48/1000 | Loss: 0.00001836
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001835
Iteration 51/1000 | Loss: 0.00001835
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001834
Iteration 58/1000 | Loss: 0.00001834
Iteration 59/1000 | Loss: 0.00001834
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001833
Iteration 67/1000 | Loss: 0.00001833
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001832
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00001831
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001830
Iteration 83/1000 | Loss: 0.00001830
Iteration 84/1000 | Loss: 0.00001830
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001829
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001827
Iteration 114/1000 | Loss: 0.00001827
Iteration 115/1000 | Loss: 0.00001827
Iteration 116/1000 | Loss: 0.00001827
Iteration 117/1000 | Loss: 0.00001827
Iteration 118/1000 | Loss: 0.00001827
Iteration 119/1000 | Loss: 0.00001827
Iteration 120/1000 | Loss: 0.00001827
Iteration 121/1000 | Loss: 0.00001827
Iteration 122/1000 | Loss: 0.00001827
Iteration 123/1000 | Loss: 0.00001827
Iteration 124/1000 | Loss: 0.00001827
Iteration 125/1000 | Loss: 0.00001827
Iteration 126/1000 | Loss: 0.00001827
Iteration 127/1000 | Loss: 0.00001827
Iteration 128/1000 | Loss: 0.00001827
Iteration 129/1000 | Loss: 0.00001827
Iteration 130/1000 | Loss: 0.00001827
Iteration 131/1000 | Loss: 0.00001827
Iteration 132/1000 | Loss: 0.00001827
Iteration 133/1000 | Loss: 0.00001827
Iteration 134/1000 | Loss: 0.00001827
Iteration 135/1000 | Loss: 0.00001827
Iteration 136/1000 | Loss: 0.00001827
Iteration 137/1000 | Loss: 0.00001827
Iteration 138/1000 | Loss: 0.00001827
Iteration 139/1000 | Loss: 0.00001827
Iteration 140/1000 | Loss: 0.00001827
Iteration 141/1000 | Loss: 0.00001827
Iteration 142/1000 | Loss: 0.00001827
Iteration 143/1000 | Loss: 0.00001827
Iteration 144/1000 | Loss: 0.00001827
Iteration 145/1000 | Loss: 0.00001827
Iteration 146/1000 | Loss: 0.00001827
Iteration 147/1000 | Loss: 0.00001827
Iteration 148/1000 | Loss: 0.00001827
Iteration 149/1000 | Loss: 0.00001827
Iteration 150/1000 | Loss: 0.00001827
Iteration 151/1000 | Loss: 0.00001827
Iteration 152/1000 | Loss: 0.00001827
Iteration 153/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.8274604371981695e-05, 1.8274604371981695e-05, 1.8274604371981695e-05, 1.8274604371981695e-05, 1.8274604371981695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8274604371981695e-05

Optimization complete. Final v2v error: 3.6613574028015137 mm

Highest mean error: 3.965916633605957 mm for frame 110

Lowest mean error: 3.2135274410247803 mm for frame 28

Saving results

Total time: 46.72247385978699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391482
Iteration 2/25 | Loss: 0.00114567
Iteration 3/25 | Loss: 0.00106486
Iteration 4/25 | Loss: 0.00105556
Iteration 5/25 | Loss: 0.00105315
Iteration 6/25 | Loss: 0.00105292
Iteration 7/25 | Loss: 0.00105292
Iteration 8/25 | Loss: 0.00105292
Iteration 9/25 | Loss: 0.00105292
Iteration 10/25 | Loss: 0.00105292
Iteration 11/25 | Loss: 0.00105292
Iteration 12/25 | Loss: 0.00105292
Iteration 13/25 | Loss: 0.00105292
Iteration 14/25 | Loss: 0.00105292
Iteration 15/25 | Loss: 0.00105292
Iteration 16/25 | Loss: 0.00105292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010529200080782175, 0.0010529200080782175, 0.0010529200080782175, 0.0010529200080782175, 0.0010529200080782175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010529200080782175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98315954
Iteration 2/25 | Loss: 0.00138463
Iteration 3/25 | Loss: 0.00138462
Iteration 4/25 | Loss: 0.00138462
Iteration 5/25 | Loss: 0.00138462
Iteration 6/25 | Loss: 0.00138462
Iteration 7/25 | Loss: 0.00138462
Iteration 8/25 | Loss: 0.00138462
Iteration 9/25 | Loss: 0.00138462
Iteration 10/25 | Loss: 0.00138462
Iteration 11/25 | Loss: 0.00138462
Iteration 12/25 | Loss: 0.00138462
Iteration 13/25 | Loss: 0.00138462
Iteration 14/25 | Loss: 0.00138462
Iteration 15/25 | Loss: 0.00138462
Iteration 16/25 | Loss: 0.00138462
Iteration 17/25 | Loss: 0.00138462
Iteration 18/25 | Loss: 0.00138462
Iteration 19/25 | Loss: 0.00138462
Iteration 20/25 | Loss: 0.00138462
Iteration 21/25 | Loss: 0.00138462
Iteration 22/25 | Loss: 0.00138462
Iteration 23/25 | Loss: 0.00138462
Iteration 24/25 | Loss: 0.00138462
Iteration 25/25 | Loss: 0.00138462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138462
Iteration 2/1000 | Loss: 0.00002741
Iteration 3/1000 | Loss: 0.00001667
Iteration 4/1000 | Loss: 0.00001392
Iteration 5/1000 | Loss: 0.00001316
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001236
Iteration 8/1000 | Loss: 0.00001223
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001192
Iteration 11/1000 | Loss: 0.00001191
Iteration 12/1000 | Loss: 0.00001190
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001189
Iteration 15/1000 | Loss: 0.00001189
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001189
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001182
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001177
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001177
Iteration 45/1000 | Loss: 0.00001177
Iteration 46/1000 | Loss: 0.00001176
Iteration 47/1000 | Loss: 0.00001176
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001176
Iteration 52/1000 | Loss: 0.00001176
Iteration 53/1000 | Loss: 0.00001176
Iteration 54/1000 | Loss: 0.00001176
Iteration 55/1000 | Loss: 0.00001176
Iteration 56/1000 | Loss: 0.00001176
Iteration 57/1000 | Loss: 0.00001176
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001175
Iteration 63/1000 | Loss: 0.00001174
Iteration 64/1000 | Loss: 0.00001174
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001171
Iteration 109/1000 | Loss: 0.00001171
Iteration 110/1000 | Loss: 0.00001171
Iteration 111/1000 | Loss: 0.00001171
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001169
Iteration 135/1000 | Loss: 0.00001169
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001169
Iteration 140/1000 | Loss: 0.00001169
Iteration 141/1000 | Loss: 0.00001169
Iteration 142/1000 | Loss: 0.00001169
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.1690373867168091e-05, 1.1690373867168091e-05, 1.1690373867168091e-05, 1.1690373867168091e-05, 1.1690373867168091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1690373867168091e-05

Optimization complete. Final v2v error: 2.84501051902771 mm

Highest mean error: 3.053147315979004 mm for frame 133

Lowest mean error: 2.652388095855713 mm for frame 98

Saving results

Total time: 30.82163166999817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462864
Iteration 2/25 | Loss: 0.00131787
Iteration 3/25 | Loss: 0.00117158
Iteration 4/25 | Loss: 0.00114688
Iteration 5/25 | Loss: 0.00113900
Iteration 6/25 | Loss: 0.00113677
Iteration 7/25 | Loss: 0.00113677
Iteration 8/25 | Loss: 0.00113677
Iteration 9/25 | Loss: 0.00113677
Iteration 10/25 | Loss: 0.00113677
Iteration 11/25 | Loss: 0.00113677
Iteration 12/25 | Loss: 0.00113677
Iteration 13/25 | Loss: 0.00113677
Iteration 14/25 | Loss: 0.00113677
Iteration 15/25 | Loss: 0.00113677
Iteration 16/25 | Loss: 0.00113677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011367728002369404, 0.0011367728002369404, 0.0011367728002369404, 0.0011367728002369404, 0.0011367728002369404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011367728002369404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.20387149
Iteration 2/25 | Loss: 0.00169701
Iteration 3/25 | Loss: 0.00169701
Iteration 4/25 | Loss: 0.00169701
Iteration 5/25 | Loss: 0.00169700
Iteration 6/25 | Loss: 0.00169700
Iteration 7/25 | Loss: 0.00169700
Iteration 8/25 | Loss: 0.00169700
Iteration 9/25 | Loss: 0.00169700
Iteration 10/25 | Loss: 0.00169700
Iteration 11/25 | Loss: 0.00169700
Iteration 12/25 | Loss: 0.00169700
Iteration 13/25 | Loss: 0.00169700
Iteration 14/25 | Loss: 0.00169700
Iteration 15/25 | Loss: 0.00169700
Iteration 16/25 | Loss: 0.00169700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016970029100775719, 0.0016970029100775719, 0.0016970029100775719, 0.0016970029100775719, 0.0016970029100775719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016970029100775719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169700
Iteration 2/1000 | Loss: 0.00005587
Iteration 3/1000 | Loss: 0.00003306
Iteration 4/1000 | Loss: 0.00002410
Iteration 5/1000 | Loss: 0.00002120
Iteration 6/1000 | Loss: 0.00001967
Iteration 7/1000 | Loss: 0.00001876
Iteration 8/1000 | Loss: 0.00001821
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001757
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001719
Iteration 13/1000 | Loss: 0.00001715
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001710
Iteration 16/1000 | Loss: 0.00001703
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001694
Iteration 19/1000 | Loss: 0.00001694
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001689
Iteration 22/1000 | Loss: 0.00001689
Iteration 23/1000 | Loss: 0.00001688
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001686
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001684
Iteration 31/1000 | Loss: 0.00001683
Iteration 32/1000 | Loss: 0.00001683
Iteration 33/1000 | Loss: 0.00001683
Iteration 34/1000 | Loss: 0.00001682
Iteration 35/1000 | Loss: 0.00001682
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001681
Iteration 38/1000 | Loss: 0.00001680
Iteration 39/1000 | Loss: 0.00001680
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001679
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001678
Iteration 44/1000 | Loss: 0.00001678
Iteration 45/1000 | Loss: 0.00001678
Iteration 46/1000 | Loss: 0.00001678
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001678
Iteration 51/1000 | Loss: 0.00001678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [1.678209264355246e-05, 1.678209264355246e-05, 1.678209264355246e-05, 1.678209264355246e-05, 1.678209264355246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.678209264355246e-05

Optimization complete. Final v2v error: 3.3910434246063232 mm

Highest mean error: 4.180006504058838 mm for frame 89

Lowest mean error: 2.867325782775879 mm for frame 136

Saving results

Total time: 36.7414870262146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438662
Iteration 2/25 | Loss: 0.00125383
Iteration 3/25 | Loss: 0.00115131
Iteration 4/25 | Loss: 0.00113819
Iteration 5/25 | Loss: 0.00113333
Iteration 6/25 | Loss: 0.00113216
Iteration 7/25 | Loss: 0.00113216
Iteration 8/25 | Loss: 0.00113216
Iteration 9/25 | Loss: 0.00113216
Iteration 10/25 | Loss: 0.00113216
Iteration 11/25 | Loss: 0.00113216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011321634519845247, 0.0011321634519845247, 0.0011321634519845247, 0.0011321634519845247, 0.0011321634519845247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011321634519845247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29918981
Iteration 2/25 | Loss: 0.00176727
Iteration 3/25 | Loss: 0.00176727
Iteration 4/25 | Loss: 0.00176727
Iteration 5/25 | Loss: 0.00176727
Iteration 6/25 | Loss: 0.00176727
Iteration 7/25 | Loss: 0.00176727
Iteration 8/25 | Loss: 0.00176727
Iteration 9/25 | Loss: 0.00176727
Iteration 10/25 | Loss: 0.00176727
Iteration 11/25 | Loss: 0.00176727
Iteration 12/25 | Loss: 0.00176727
Iteration 13/25 | Loss: 0.00176727
Iteration 14/25 | Loss: 0.00176727
Iteration 15/25 | Loss: 0.00176727
Iteration 16/25 | Loss: 0.00176727
Iteration 17/25 | Loss: 0.00176727
Iteration 18/25 | Loss: 0.00176727
Iteration 19/25 | Loss: 0.00176727
Iteration 20/25 | Loss: 0.00176727
Iteration 21/25 | Loss: 0.00176727
Iteration 22/25 | Loss: 0.00176727
Iteration 23/25 | Loss: 0.00176727
Iteration 24/25 | Loss: 0.00176727
Iteration 25/25 | Loss: 0.00176727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176727
Iteration 2/1000 | Loss: 0.00005607
Iteration 3/1000 | Loss: 0.00002933
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002042
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001806
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001725
Iteration 12/1000 | Loss: 0.00001710
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001695
Iteration 15/1000 | Loss: 0.00001686
Iteration 16/1000 | Loss: 0.00001683
Iteration 17/1000 | Loss: 0.00001681
Iteration 18/1000 | Loss: 0.00001679
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001676
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001673
Iteration 24/1000 | Loss: 0.00001672
Iteration 25/1000 | Loss: 0.00001671
Iteration 26/1000 | Loss: 0.00001671
Iteration 27/1000 | Loss: 0.00001667
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001663
Iteration 32/1000 | Loss: 0.00001662
Iteration 33/1000 | Loss: 0.00001659
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001658
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001654
Iteration 42/1000 | Loss: 0.00001654
Iteration 43/1000 | Loss: 0.00001654
Iteration 44/1000 | Loss: 0.00001654
Iteration 45/1000 | Loss: 0.00001654
Iteration 46/1000 | Loss: 0.00001654
Iteration 47/1000 | Loss: 0.00001654
Iteration 48/1000 | Loss: 0.00001653
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001652
Iteration 51/1000 | Loss: 0.00001652
Iteration 52/1000 | Loss: 0.00001652
Iteration 53/1000 | Loss: 0.00001651
Iteration 54/1000 | Loss: 0.00001651
Iteration 55/1000 | Loss: 0.00001651
Iteration 56/1000 | Loss: 0.00001650
Iteration 57/1000 | Loss: 0.00001650
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001648
Iteration 65/1000 | Loss: 0.00001648
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001647
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001642
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001639
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001639
Iteration 107/1000 | Loss: 0.00001639
Iteration 108/1000 | Loss: 0.00001639
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001639
Iteration 124/1000 | Loss: 0.00001639
Iteration 125/1000 | Loss: 0.00001639
Iteration 126/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.6387615687563084e-05, 1.6387615687563084e-05, 1.6387615687563084e-05, 1.6387615687563084e-05, 1.6387615687563084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6387615687563084e-05

Optimization complete. Final v2v error: 3.3020923137664795 mm

Highest mean error: 3.9420645236968994 mm for frame 144

Lowest mean error: 2.708205461502075 mm for frame 191

Saving results

Total time: 42.479997873306274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075205
Iteration 2/25 | Loss: 0.01075205
Iteration 3/25 | Loss: 0.01075205
Iteration 4/25 | Loss: 0.01075204
Iteration 5/25 | Loss: 0.01075204
Iteration 6/25 | Loss: 0.01075204
Iteration 7/25 | Loss: 0.01075204
Iteration 8/25 | Loss: 0.01075204
Iteration 9/25 | Loss: 0.01075204
Iteration 10/25 | Loss: 0.01075203
Iteration 11/25 | Loss: 0.01075203
Iteration 12/25 | Loss: 0.01075203
Iteration 13/25 | Loss: 0.01075203
Iteration 14/25 | Loss: 0.01075203
Iteration 15/25 | Loss: 0.01075202
Iteration 16/25 | Loss: 0.01075202
Iteration 17/25 | Loss: 0.01075202
Iteration 18/25 | Loss: 0.01075202
Iteration 19/25 | Loss: 0.01075201
Iteration 20/25 | Loss: 0.01075201
Iteration 21/25 | Loss: 0.01075201
Iteration 22/25 | Loss: 0.01075201
Iteration 23/25 | Loss: 0.01075201
Iteration 24/25 | Loss: 0.01075200
Iteration 25/25 | Loss: 0.01075200

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48937833
Iteration 2/25 | Loss: 0.11154056
Iteration 3/25 | Loss: 0.11148240
Iteration 4/25 | Loss: 0.11132083
Iteration 5/25 | Loss: 0.11132077
Iteration 6/25 | Loss: 0.11132077
Iteration 7/25 | Loss: 0.11130469
Iteration 8/25 | Loss: 0.11130467
Iteration 9/25 | Loss: 0.11130466
Iteration 10/25 | Loss: 0.11130466
Iteration 11/25 | Loss: 0.11130466
Iteration 12/25 | Loss: 0.11130464
Iteration 13/25 | Loss: 0.11130465
Iteration 14/25 | Loss: 0.11130465
Iteration 15/25 | Loss: 0.11130465
Iteration 16/25 | Loss: 0.11130465
Iteration 17/25 | Loss: 0.11130465
Iteration 18/25 | Loss: 0.11130465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.11130464822053909, 0.11130464822053909, 0.11130464822053909, 0.11130464822053909, 0.11130464822053909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11130464822053909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11130465
Iteration 2/1000 | Loss: 0.00188408
Iteration 3/1000 | Loss: 0.00054257
Iteration 4/1000 | Loss: 0.00104994
Iteration 5/1000 | Loss: 0.00047070
Iteration 6/1000 | Loss: 0.00017370
Iteration 7/1000 | Loss: 0.00012400
Iteration 8/1000 | Loss: 0.00010824
Iteration 9/1000 | Loss: 0.00016886
Iteration 10/1000 | Loss: 0.00108044
Iteration 11/1000 | Loss: 0.00087563
Iteration 12/1000 | Loss: 0.00017355
Iteration 13/1000 | Loss: 0.00034190
Iteration 14/1000 | Loss: 0.00009448
Iteration 15/1000 | Loss: 0.00014541
Iteration 16/1000 | Loss: 0.00010926
Iteration 17/1000 | Loss: 0.00015853
Iteration 18/1000 | Loss: 0.00004265
Iteration 19/1000 | Loss: 0.00004855
Iteration 20/1000 | Loss: 0.00002263
Iteration 21/1000 | Loss: 0.00003982
Iteration 22/1000 | Loss: 0.00006574
Iteration 23/1000 | Loss: 0.00021305
Iteration 24/1000 | Loss: 0.00013155
Iteration 25/1000 | Loss: 0.00014412
Iteration 26/1000 | Loss: 0.00003202
Iteration 27/1000 | Loss: 0.00001905
Iteration 28/1000 | Loss: 0.00003396
Iteration 29/1000 | Loss: 0.00001830
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00005071
Iteration 32/1000 | Loss: 0.00003908
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001706
Iteration 35/1000 | Loss: 0.00003615
Iteration 36/1000 | Loss: 0.00001665
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001631
Iteration 39/1000 | Loss: 0.00002602
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00003106
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001584
Iteration 46/1000 | Loss: 0.00001584
Iteration 47/1000 | Loss: 0.00004398
Iteration 48/1000 | Loss: 0.00004696
Iteration 49/1000 | Loss: 0.00004089
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001540
Iteration 53/1000 | Loss: 0.00002066
Iteration 54/1000 | Loss: 0.00001540
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001535
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001532
Iteration 61/1000 | Loss: 0.00001531
Iteration 62/1000 | Loss: 0.00001531
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00001522
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001521
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001515
Iteration 79/1000 | Loss: 0.00001510
Iteration 80/1000 | Loss: 0.00001507
Iteration 81/1000 | Loss: 0.00001506
Iteration 82/1000 | Loss: 0.00001506
Iteration 83/1000 | Loss: 0.00001506
Iteration 84/1000 | Loss: 0.00001506
Iteration 85/1000 | Loss: 0.00001505
Iteration 86/1000 | Loss: 0.00001504
Iteration 87/1000 | Loss: 0.00001501
Iteration 88/1000 | Loss: 0.00001501
Iteration 89/1000 | Loss: 0.00001500
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001498
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001497
Iteration 94/1000 | Loss: 0.00001497
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001495
Iteration 98/1000 | Loss: 0.00001495
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001493
Iteration 101/1000 | Loss: 0.00001493
Iteration 102/1000 | Loss: 0.00001493
Iteration 103/1000 | Loss: 0.00001492
Iteration 104/1000 | Loss: 0.00001492
Iteration 105/1000 | Loss: 0.00001492
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001491
Iteration 108/1000 | Loss: 0.00001491
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001490
Iteration 113/1000 | Loss: 0.00001490
Iteration 114/1000 | Loss: 0.00001490
Iteration 115/1000 | Loss: 0.00001490
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001489
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001488
Iteration 122/1000 | Loss: 0.00001488
Iteration 123/1000 | Loss: 0.00001488
Iteration 124/1000 | Loss: 0.00001488
Iteration 125/1000 | Loss: 0.00001487
Iteration 126/1000 | Loss: 0.00001487
Iteration 127/1000 | Loss: 0.00001487
Iteration 128/1000 | Loss: 0.00001487
Iteration 129/1000 | Loss: 0.00001487
Iteration 130/1000 | Loss: 0.00001487
Iteration 131/1000 | Loss: 0.00001487
Iteration 132/1000 | Loss: 0.00001486
Iteration 133/1000 | Loss: 0.00001486
Iteration 134/1000 | Loss: 0.00001486
Iteration 135/1000 | Loss: 0.00001486
Iteration 136/1000 | Loss: 0.00001486
Iteration 137/1000 | Loss: 0.00001486
Iteration 138/1000 | Loss: 0.00001486
Iteration 139/1000 | Loss: 0.00001486
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001485
Iteration 143/1000 | Loss: 0.00001485
Iteration 144/1000 | Loss: 0.00002028
Iteration 145/1000 | Loss: 0.00002787
Iteration 146/1000 | Loss: 0.00001485
Iteration 147/1000 | Loss: 0.00001479
Iteration 148/1000 | Loss: 0.00001479
Iteration 149/1000 | Loss: 0.00001479
Iteration 150/1000 | Loss: 0.00001479
Iteration 151/1000 | Loss: 0.00002117
Iteration 152/1000 | Loss: 0.00001479
Iteration 153/1000 | Loss: 0.00001479
Iteration 154/1000 | Loss: 0.00001479
Iteration 155/1000 | Loss: 0.00001479
Iteration 156/1000 | Loss: 0.00001479
Iteration 157/1000 | Loss: 0.00001479
Iteration 158/1000 | Loss: 0.00001479
Iteration 159/1000 | Loss: 0.00001479
Iteration 160/1000 | Loss: 0.00001479
Iteration 161/1000 | Loss: 0.00001479
Iteration 162/1000 | Loss: 0.00001478
Iteration 163/1000 | Loss: 0.00001478
Iteration 164/1000 | Loss: 0.00001478
Iteration 165/1000 | Loss: 0.00001896
Iteration 166/1000 | Loss: 0.00001479
Iteration 167/1000 | Loss: 0.00001478
Iteration 168/1000 | Loss: 0.00001478
Iteration 169/1000 | Loss: 0.00001478
Iteration 170/1000 | Loss: 0.00001478
Iteration 171/1000 | Loss: 0.00001478
Iteration 172/1000 | Loss: 0.00001478
Iteration 173/1000 | Loss: 0.00001478
Iteration 174/1000 | Loss: 0.00001478
Iteration 175/1000 | Loss: 0.00001478
Iteration 176/1000 | Loss: 0.00001478
Iteration 177/1000 | Loss: 0.00001478
Iteration 178/1000 | Loss: 0.00001478
Iteration 179/1000 | Loss: 0.00001478
Iteration 180/1000 | Loss: 0.00001478
Iteration 181/1000 | Loss: 0.00001478
Iteration 182/1000 | Loss: 0.00001478
Iteration 183/1000 | Loss: 0.00001478
Iteration 184/1000 | Loss: 0.00001478
Iteration 185/1000 | Loss: 0.00001477
Iteration 186/1000 | Loss: 0.00001477
Iteration 187/1000 | Loss: 0.00001477
Iteration 188/1000 | Loss: 0.00001477
Iteration 189/1000 | Loss: 0.00001477
Iteration 190/1000 | Loss: 0.00001477
Iteration 191/1000 | Loss: 0.00001477
Iteration 192/1000 | Loss: 0.00001477
Iteration 193/1000 | Loss: 0.00001477
Iteration 194/1000 | Loss: 0.00001477
Iteration 195/1000 | Loss: 0.00001477
Iteration 196/1000 | Loss: 0.00001477
Iteration 197/1000 | Loss: 0.00001477
Iteration 198/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.4773630937270354e-05, 1.4773630937270354e-05, 1.4773630937270354e-05, 1.4773630937270354e-05, 1.4773630937270354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4773630937270354e-05

Optimization complete. Final v2v error: 3.2672104835510254 mm

Highest mean error: 3.6652491092681885 mm for frame 0

Lowest mean error: 3.0073840618133545 mm for frame 181

Saving results

Total time: 107.91969990730286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073189
Iteration 2/25 | Loss: 0.00191405
Iteration 3/25 | Loss: 0.00148700
Iteration 4/25 | Loss: 0.00141483
Iteration 5/25 | Loss: 0.00150182
Iteration 6/25 | Loss: 0.00145811
Iteration 7/25 | Loss: 0.00134667
Iteration 8/25 | Loss: 0.00125869
Iteration 9/25 | Loss: 0.00120435
Iteration 10/25 | Loss: 0.00118001
Iteration 11/25 | Loss: 0.00117938
Iteration 12/25 | Loss: 0.00117500
Iteration 13/25 | Loss: 0.00117116
Iteration 14/25 | Loss: 0.00115692
Iteration 15/25 | Loss: 0.00114579
Iteration 16/25 | Loss: 0.00113958
Iteration 17/25 | Loss: 0.00112949
Iteration 18/25 | Loss: 0.00112641
Iteration 19/25 | Loss: 0.00112851
Iteration 20/25 | Loss: 0.00112583
Iteration 21/25 | Loss: 0.00112222
Iteration 22/25 | Loss: 0.00112056
Iteration 23/25 | Loss: 0.00111880
Iteration 24/25 | Loss: 0.00111788
Iteration 25/25 | Loss: 0.00111373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33974171
Iteration 2/25 | Loss: 0.00167488
Iteration 3/25 | Loss: 0.00158765
Iteration 4/25 | Loss: 0.00158764
Iteration 5/25 | Loss: 0.00158764
Iteration 6/25 | Loss: 0.00158764
Iteration 7/25 | Loss: 0.00158764
Iteration 8/25 | Loss: 0.00158764
Iteration 9/25 | Loss: 0.00158764
Iteration 10/25 | Loss: 0.00158764
Iteration 11/25 | Loss: 0.00158764
Iteration 12/25 | Loss: 0.00158764
Iteration 13/25 | Loss: 0.00158764
Iteration 14/25 | Loss: 0.00158764
Iteration 15/25 | Loss: 0.00158764
Iteration 16/25 | Loss: 0.00158764
Iteration 17/25 | Loss: 0.00158764
Iteration 18/25 | Loss: 0.00158764
Iteration 19/25 | Loss: 0.00158764
Iteration 20/25 | Loss: 0.00158764
Iteration 21/25 | Loss: 0.00158764
Iteration 22/25 | Loss: 0.00158764
Iteration 23/25 | Loss: 0.00158764
Iteration 24/25 | Loss: 0.00158764
Iteration 25/25 | Loss: 0.00158764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158764
Iteration 2/1000 | Loss: 0.00032961
Iteration 3/1000 | Loss: 0.00023007
Iteration 4/1000 | Loss: 0.00024538
Iteration 5/1000 | Loss: 0.00029078
Iteration 6/1000 | Loss: 0.00038323
Iteration 7/1000 | Loss: 0.00035360
Iteration 8/1000 | Loss: 0.00046657
Iteration 9/1000 | Loss: 0.00052540
Iteration 10/1000 | Loss: 0.00066454
Iteration 11/1000 | Loss: 0.00007406
Iteration 12/1000 | Loss: 0.00015553
Iteration 13/1000 | Loss: 0.00007798
Iteration 14/1000 | Loss: 0.00055019
Iteration 15/1000 | Loss: 0.00030581
Iteration 16/1000 | Loss: 0.00012597
Iteration 17/1000 | Loss: 0.00016740
Iteration 18/1000 | Loss: 0.00030618
Iteration 19/1000 | Loss: 0.00023792
Iteration 20/1000 | Loss: 0.00022812
Iteration 21/1000 | Loss: 0.00011387
Iteration 22/1000 | Loss: 0.00010129
Iteration 23/1000 | Loss: 0.00066545
Iteration 24/1000 | Loss: 0.00041967
Iteration 25/1000 | Loss: 0.00028675
Iteration 26/1000 | Loss: 0.00007282
Iteration 27/1000 | Loss: 0.00013534
Iteration 28/1000 | Loss: 0.00015003
Iteration 29/1000 | Loss: 0.00008330
Iteration 30/1000 | Loss: 0.00014248
Iteration 31/1000 | Loss: 0.00009870
Iteration 32/1000 | Loss: 0.00004122
Iteration 33/1000 | Loss: 0.00010738
Iteration 34/1000 | Loss: 0.00007398
Iteration 35/1000 | Loss: 0.00010929
Iteration 36/1000 | Loss: 0.00015519
Iteration 37/1000 | Loss: 0.00055914
Iteration 38/1000 | Loss: 0.00036913
Iteration 39/1000 | Loss: 0.00028900
Iteration 40/1000 | Loss: 0.00010271
Iteration 41/1000 | Loss: 0.00013083
Iteration 42/1000 | Loss: 0.00005071
Iteration 43/1000 | Loss: 0.00013965
Iteration 44/1000 | Loss: 0.00042884
Iteration 45/1000 | Loss: 0.00049286
Iteration 46/1000 | Loss: 0.00054128
Iteration 47/1000 | Loss: 0.00015093
Iteration 48/1000 | Loss: 0.00005042
Iteration 49/1000 | Loss: 0.00008064
Iteration 50/1000 | Loss: 0.00003598
Iteration 51/1000 | Loss: 0.00004381
Iteration 52/1000 | Loss: 0.00004283
Iteration 53/1000 | Loss: 0.00004008
Iteration 54/1000 | Loss: 0.00015339
Iteration 55/1000 | Loss: 0.00004895
Iteration 56/1000 | Loss: 0.00007361
Iteration 57/1000 | Loss: 0.00004404
Iteration 58/1000 | Loss: 0.00004332
Iteration 59/1000 | Loss: 0.00003235
Iteration 60/1000 | Loss: 0.00002927
Iteration 61/1000 | Loss: 0.00003736
Iteration 62/1000 | Loss: 0.00017495
Iteration 63/1000 | Loss: 0.00003896
Iteration 64/1000 | Loss: 0.00003778
Iteration 65/1000 | Loss: 0.00003675
Iteration 66/1000 | Loss: 0.00003623
Iteration 67/1000 | Loss: 0.00031119
Iteration 68/1000 | Loss: 0.00009822
Iteration 69/1000 | Loss: 0.00003856
Iteration 70/1000 | Loss: 0.00016968
Iteration 71/1000 | Loss: 0.00010813
Iteration 72/1000 | Loss: 0.00023701
Iteration 73/1000 | Loss: 0.00016197
Iteration 74/1000 | Loss: 0.00013353
Iteration 75/1000 | Loss: 0.00015702
Iteration 76/1000 | Loss: 0.00022556
Iteration 77/1000 | Loss: 0.00005054
Iteration 78/1000 | Loss: 0.00004173
Iteration 79/1000 | Loss: 0.00004723
Iteration 80/1000 | Loss: 0.00007701
Iteration 81/1000 | Loss: 0.00012357
Iteration 82/1000 | Loss: 0.00011386
Iteration 83/1000 | Loss: 0.00017566
Iteration 84/1000 | Loss: 0.00061739
Iteration 85/1000 | Loss: 0.00049663
Iteration 86/1000 | Loss: 0.00004536
Iteration 87/1000 | Loss: 0.00003101
Iteration 88/1000 | Loss: 0.00044137
Iteration 89/1000 | Loss: 0.00023137
Iteration 90/1000 | Loss: 0.00010369
Iteration 91/1000 | Loss: 0.00014771
Iteration 92/1000 | Loss: 0.00002528
Iteration 93/1000 | Loss: 0.00015420
Iteration 94/1000 | Loss: 0.00010164
Iteration 95/1000 | Loss: 0.00002879
Iteration 96/1000 | Loss: 0.00002588
Iteration 97/1000 | Loss: 0.00012806
Iteration 98/1000 | Loss: 0.00004519
Iteration 99/1000 | Loss: 0.00010377
Iteration 100/1000 | Loss: 0.00002146
Iteration 101/1000 | Loss: 0.00002064
Iteration 102/1000 | Loss: 0.00001968
Iteration 103/1000 | Loss: 0.00007043
Iteration 104/1000 | Loss: 0.00032445
Iteration 105/1000 | Loss: 0.00011292
Iteration 106/1000 | Loss: 0.00013997
Iteration 107/1000 | Loss: 0.00007231
Iteration 108/1000 | Loss: 0.00009640
Iteration 109/1000 | Loss: 0.00008892
Iteration 110/1000 | Loss: 0.00014100
Iteration 111/1000 | Loss: 0.00005768
Iteration 112/1000 | Loss: 0.00006230
Iteration 113/1000 | Loss: 0.00007847
Iteration 114/1000 | Loss: 0.00004406
Iteration 115/1000 | Loss: 0.00009303
Iteration 116/1000 | Loss: 0.00012673
Iteration 117/1000 | Loss: 0.00006633
Iteration 118/1000 | Loss: 0.00009258
Iteration 119/1000 | Loss: 0.00006896
Iteration 120/1000 | Loss: 0.00006313
Iteration 121/1000 | Loss: 0.00002898
Iteration 122/1000 | Loss: 0.00007092
Iteration 123/1000 | Loss: 0.00002243
Iteration 124/1000 | Loss: 0.00002132
Iteration 125/1000 | Loss: 0.00006854
Iteration 126/1000 | Loss: 0.00040867
Iteration 127/1000 | Loss: 0.00059088
Iteration 128/1000 | Loss: 0.00019856
Iteration 129/1000 | Loss: 0.00017027
Iteration 130/1000 | Loss: 0.00004084
Iteration 131/1000 | Loss: 0.00002889
Iteration 132/1000 | Loss: 0.00016084
Iteration 133/1000 | Loss: 0.00015364
Iteration 134/1000 | Loss: 0.00032306
Iteration 135/1000 | Loss: 0.00015290
Iteration 136/1000 | Loss: 0.00012810
Iteration 137/1000 | Loss: 0.00012850
Iteration 138/1000 | Loss: 0.00011231
Iteration 139/1000 | Loss: 0.00012104
Iteration 140/1000 | Loss: 0.00002744
Iteration 141/1000 | Loss: 0.00020213
Iteration 142/1000 | Loss: 0.00002227
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001581
Iteration 145/1000 | Loss: 0.00002519
Iteration 146/1000 | Loss: 0.00001604
Iteration 147/1000 | Loss: 0.00001491
Iteration 148/1000 | Loss: 0.00001439
Iteration 149/1000 | Loss: 0.00001926
Iteration 150/1000 | Loss: 0.00002265
Iteration 151/1000 | Loss: 0.00002138
Iteration 152/1000 | Loss: 0.00001399
Iteration 153/1000 | Loss: 0.00001262
Iteration 154/1000 | Loss: 0.00001246
Iteration 155/1000 | Loss: 0.00001223
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001185
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001173
Iteration 160/1000 | Loss: 0.00001172
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001167
Iteration 164/1000 | Loss: 0.00001166
Iteration 165/1000 | Loss: 0.00001165
Iteration 166/1000 | Loss: 0.00001157
Iteration 167/1000 | Loss: 0.00001153
Iteration 168/1000 | Loss: 0.00001152
Iteration 169/1000 | Loss: 0.00001151
Iteration 170/1000 | Loss: 0.00001151
Iteration 171/1000 | Loss: 0.00001148
Iteration 172/1000 | Loss: 0.00001147
Iteration 173/1000 | Loss: 0.00001147
Iteration 174/1000 | Loss: 0.00001146
Iteration 175/1000 | Loss: 0.00001146
Iteration 176/1000 | Loss: 0.00001146
Iteration 177/1000 | Loss: 0.00001145
Iteration 178/1000 | Loss: 0.00001145
Iteration 179/1000 | Loss: 0.00001145
Iteration 180/1000 | Loss: 0.00001145
Iteration 181/1000 | Loss: 0.00001145
Iteration 182/1000 | Loss: 0.00001145
Iteration 183/1000 | Loss: 0.00001145
Iteration 184/1000 | Loss: 0.00001145
Iteration 185/1000 | Loss: 0.00001145
Iteration 186/1000 | Loss: 0.00001145
Iteration 187/1000 | Loss: 0.00001145
Iteration 188/1000 | Loss: 0.00001144
Iteration 189/1000 | Loss: 0.00001144
Iteration 190/1000 | Loss: 0.00001144
Iteration 191/1000 | Loss: 0.00001143
Iteration 192/1000 | Loss: 0.00001143
Iteration 193/1000 | Loss: 0.00001143
Iteration 194/1000 | Loss: 0.00001143
Iteration 195/1000 | Loss: 0.00001142
Iteration 196/1000 | Loss: 0.00001142
Iteration 197/1000 | Loss: 0.00001142
Iteration 198/1000 | Loss: 0.00001142
Iteration 199/1000 | Loss: 0.00001142
Iteration 200/1000 | Loss: 0.00001142
Iteration 201/1000 | Loss: 0.00001142
Iteration 202/1000 | Loss: 0.00001141
Iteration 203/1000 | Loss: 0.00001141
Iteration 204/1000 | Loss: 0.00001141
Iteration 205/1000 | Loss: 0.00001141
Iteration 206/1000 | Loss: 0.00001141
Iteration 207/1000 | Loss: 0.00001141
Iteration 208/1000 | Loss: 0.00001141
Iteration 209/1000 | Loss: 0.00001141
Iteration 210/1000 | Loss: 0.00001140
Iteration 211/1000 | Loss: 0.00001140
Iteration 212/1000 | Loss: 0.00001140
Iteration 213/1000 | Loss: 0.00001140
Iteration 214/1000 | Loss: 0.00001140
Iteration 215/1000 | Loss: 0.00001140
Iteration 216/1000 | Loss: 0.00001140
Iteration 217/1000 | Loss: 0.00001139
Iteration 218/1000 | Loss: 0.00001139
Iteration 219/1000 | Loss: 0.00001139
Iteration 220/1000 | Loss: 0.00001139
Iteration 221/1000 | Loss: 0.00001139
Iteration 222/1000 | Loss: 0.00001139
Iteration 223/1000 | Loss: 0.00001139
Iteration 224/1000 | Loss: 0.00001139
Iteration 225/1000 | Loss: 0.00001139
Iteration 226/1000 | Loss: 0.00001139
Iteration 227/1000 | Loss: 0.00001139
Iteration 228/1000 | Loss: 0.00001139
Iteration 229/1000 | Loss: 0.00001139
Iteration 230/1000 | Loss: 0.00001138
Iteration 231/1000 | Loss: 0.00001138
Iteration 232/1000 | Loss: 0.00001138
Iteration 233/1000 | Loss: 0.00001138
Iteration 234/1000 | Loss: 0.00001138
Iteration 235/1000 | Loss: 0.00001138
Iteration 236/1000 | Loss: 0.00001138
Iteration 237/1000 | Loss: 0.00001138
Iteration 238/1000 | Loss: 0.00001138
Iteration 239/1000 | Loss: 0.00001138
Iteration 240/1000 | Loss: 0.00001138
Iteration 241/1000 | Loss: 0.00001138
Iteration 242/1000 | Loss: 0.00001138
Iteration 243/1000 | Loss: 0.00001137
Iteration 244/1000 | Loss: 0.00001137
Iteration 245/1000 | Loss: 0.00001137
Iteration 246/1000 | Loss: 0.00001137
Iteration 247/1000 | Loss: 0.00001137
Iteration 248/1000 | Loss: 0.00001137
Iteration 249/1000 | Loss: 0.00001137
Iteration 250/1000 | Loss: 0.00001137
Iteration 251/1000 | Loss: 0.00001137
Iteration 252/1000 | Loss: 0.00001137
Iteration 253/1000 | Loss: 0.00001137
Iteration 254/1000 | Loss: 0.00001137
Iteration 255/1000 | Loss: 0.00001137
Iteration 256/1000 | Loss: 0.00001137
Iteration 257/1000 | Loss: 0.00001137
Iteration 258/1000 | Loss: 0.00001136
Iteration 259/1000 | Loss: 0.00001136
Iteration 260/1000 | Loss: 0.00001136
Iteration 261/1000 | Loss: 0.00001136
Iteration 262/1000 | Loss: 0.00001136
Iteration 263/1000 | Loss: 0.00001136
Iteration 264/1000 | Loss: 0.00001136
Iteration 265/1000 | Loss: 0.00001135
Iteration 266/1000 | Loss: 0.00001135
Iteration 267/1000 | Loss: 0.00001135
Iteration 268/1000 | Loss: 0.00001135
Iteration 269/1000 | Loss: 0.00001135
Iteration 270/1000 | Loss: 0.00001135
Iteration 271/1000 | Loss: 0.00001135
Iteration 272/1000 | Loss: 0.00001135
Iteration 273/1000 | Loss: 0.00001135
Iteration 274/1000 | Loss: 0.00001135
Iteration 275/1000 | Loss: 0.00001135
Iteration 276/1000 | Loss: 0.00001135
Iteration 277/1000 | Loss: 0.00001135
Iteration 278/1000 | Loss: 0.00001135
Iteration 279/1000 | Loss: 0.00001135
Iteration 280/1000 | Loss: 0.00001135
Iteration 281/1000 | Loss: 0.00001135
Iteration 282/1000 | Loss: 0.00001135
Iteration 283/1000 | Loss: 0.00001135
Iteration 284/1000 | Loss: 0.00001135
Iteration 285/1000 | Loss: 0.00001135
Iteration 286/1000 | Loss: 0.00001135
Iteration 287/1000 | Loss: 0.00001135
Iteration 288/1000 | Loss: 0.00001135
Iteration 289/1000 | Loss: 0.00001135
Iteration 290/1000 | Loss: 0.00001135
Iteration 291/1000 | Loss: 0.00001135
Iteration 292/1000 | Loss: 0.00001135
Iteration 293/1000 | Loss: 0.00001135
Iteration 294/1000 | Loss: 0.00001135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [1.134959347837139e-05, 1.134959347837139e-05, 1.134959347837139e-05, 1.134959347837139e-05, 1.134959347837139e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.134959347837139e-05

Optimization complete. Final v2v error: 2.7902398109436035 mm

Highest mean error: 4.6156415939331055 mm for frame 229

Lowest mean error: 2.466088056564331 mm for frame 85

Saving results

Total time: 316.6801154613495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1269/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1269/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492517
Iteration 2/25 | Loss: 0.00113881
Iteration 3/25 | Loss: 0.00107218
Iteration 4/25 | Loss: 0.00105837
Iteration 5/25 | Loss: 0.00105426
Iteration 6/25 | Loss: 0.00105325
Iteration 7/25 | Loss: 0.00105325
Iteration 8/25 | Loss: 0.00105325
Iteration 9/25 | Loss: 0.00105325
Iteration 10/25 | Loss: 0.00105325
Iteration 11/25 | Loss: 0.00105325
Iteration 12/25 | Loss: 0.00105325
Iteration 13/25 | Loss: 0.00105325
Iteration 14/25 | Loss: 0.00105325
Iteration 15/25 | Loss: 0.00105325
Iteration 16/25 | Loss: 0.00105325
Iteration 17/25 | Loss: 0.00105325
Iteration 18/25 | Loss: 0.00105325
Iteration 19/25 | Loss: 0.00105325
Iteration 20/25 | Loss: 0.00105325
Iteration 21/25 | Loss: 0.00105325
Iteration 22/25 | Loss: 0.00105325
Iteration 23/25 | Loss: 0.00105325
Iteration 24/25 | Loss: 0.00105325
Iteration 25/25 | Loss: 0.00105325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44513845
Iteration 2/25 | Loss: 0.00132113
Iteration 3/25 | Loss: 0.00132113
Iteration 4/25 | Loss: 0.00132113
Iteration 5/25 | Loss: 0.00132113
Iteration 6/25 | Loss: 0.00132113
Iteration 7/25 | Loss: 0.00132113
Iteration 8/25 | Loss: 0.00132113
Iteration 9/25 | Loss: 0.00132113
Iteration 10/25 | Loss: 0.00132113
Iteration 11/25 | Loss: 0.00132113
Iteration 12/25 | Loss: 0.00132113
Iteration 13/25 | Loss: 0.00132113
Iteration 14/25 | Loss: 0.00132113
Iteration 15/25 | Loss: 0.00132113
Iteration 16/25 | Loss: 0.00132113
Iteration 17/25 | Loss: 0.00132113
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013211272889748216, 0.0013211272889748216, 0.0013211272889748216, 0.0013211272889748216, 0.0013211272889748216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013211272889748216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132113
Iteration 2/1000 | Loss: 0.00002725
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001357
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001268
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001265
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001261
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001259
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001250
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001249
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001247
Iteration 41/1000 | Loss: 0.00001247
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001246
Iteration 44/1000 | Loss: 0.00001246
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001244
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001243
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001241
Iteration 62/1000 | Loss: 0.00001241
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001240
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001238
Iteration 82/1000 | Loss: 0.00001238
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001238
Iteration 89/1000 | Loss: 0.00001237
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001237
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001234
Iteration 113/1000 | Loss: 0.00001234
Iteration 114/1000 | Loss: 0.00001234
Iteration 115/1000 | Loss: 0.00001234
Iteration 116/1000 | Loss: 0.00001234
Iteration 117/1000 | Loss: 0.00001234
Iteration 118/1000 | Loss: 0.00001234
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.2341327419562731e-05, 1.2341327419562731e-05, 1.2341327419562731e-05, 1.2341327419562731e-05, 1.2341327419562731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2341327419562731e-05

Optimization complete. Final v2v error: 2.9741199016571045 mm

Highest mean error: 3.1758594512939453 mm for frame 157

Lowest mean error: 2.800234079360962 mm for frame 2

Saving results

Total time: 31.20094609260559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830740
Iteration 2/25 | Loss: 0.00105669
Iteration 3/25 | Loss: 0.00096393
Iteration 4/25 | Loss: 0.00093956
Iteration 5/25 | Loss: 0.00092880
Iteration 6/25 | Loss: 0.00092606
Iteration 7/25 | Loss: 0.00092588
Iteration 8/25 | Loss: 0.00092588
Iteration 9/25 | Loss: 0.00092588
Iteration 10/25 | Loss: 0.00092588
Iteration 11/25 | Loss: 0.00092588
Iteration 12/25 | Loss: 0.00092588
Iteration 13/25 | Loss: 0.00092588
Iteration 14/25 | Loss: 0.00092588
Iteration 15/25 | Loss: 0.00092588
Iteration 16/25 | Loss: 0.00092588
Iteration 17/25 | Loss: 0.00092588
Iteration 18/25 | Loss: 0.00092588
Iteration 19/25 | Loss: 0.00092588
Iteration 20/25 | Loss: 0.00092588
Iteration 21/25 | Loss: 0.00092588
Iteration 22/25 | Loss: 0.00092588
Iteration 23/25 | Loss: 0.00092588
Iteration 24/25 | Loss: 0.00092588
Iteration 25/25 | Loss: 0.00092588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.36953783
Iteration 2/25 | Loss: 0.00118263
Iteration 3/25 | Loss: 0.00118257
Iteration 4/25 | Loss: 0.00118257
Iteration 5/25 | Loss: 0.00118257
Iteration 6/25 | Loss: 0.00118257
Iteration 7/25 | Loss: 0.00118257
Iteration 8/25 | Loss: 0.00118257
Iteration 9/25 | Loss: 0.00118257
Iteration 10/25 | Loss: 0.00118257
Iteration 11/25 | Loss: 0.00118257
Iteration 12/25 | Loss: 0.00118257
Iteration 13/25 | Loss: 0.00118257
Iteration 14/25 | Loss: 0.00118257
Iteration 15/25 | Loss: 0.00118257
Iteration 16/25 | Loss: 0.00118257
Iteration 17/25 | Loss: 0.00118257
Iteration 18/25 | Loss: 0.00118257
Iteration 19/25 | Loss: 0.00118257
Iteration 20/25 | Loss: 0.00118257
Iteration 21/25 | Loss: 0.00118257
Iteration 22/25 | Loss: 0.00118257
Iteration 23/25 | Loss: 0.00118257
Iteration 24/25 | Loss: 0.00118257
Iteration 25/25 | Loss: 0.00118257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118257
Iteration 2/1000 | Loss: 0.00004820
Iteration 3/1000 | Loss: 0.00002170
Iteration 4/1000 | Loss: 0.00001767
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001540
Iteration 8/1000 | Loss: 0.00001522
Iteration 9/1000 | Loss: 0.00001502
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001466
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001442
Iteration 16/1000 | Loss: 0.00001439
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001435
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001434
Iteration 21/1000 | Loss: 0.00001433
Iteration 22/1000 | Loss: 0.00001431
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001426
Iteration 27/1000 | Loss: 0.00001426
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001423
Iteration 31/1000 | Loss: 0.00001423
Iteration 32/1000 | Loss: 0.00001422
Iteration 33/1000 | Loss: 0.00001422
Iteration 34/1000 | Loss: 0.00001422
Iteration 35/1000 | Loss: 0.00001422
Iteration 36/1000 | Loss: 0.00001422
Iteration 37/1000 | Loss: 0.00001422
Iteration 38/1000 | Loss: 0.00001421
Iteration 39/1000 | Loss: 0.00001420
Iteration 40/1000 | Loss: 0.00001419
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001418
Iteration 44/1000 | Loss: 0.00001418
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001415
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001414
Iteration 54/1000 | Loss: 0.00001414
Iteration 55/1000 | Loss: 0.00001413
Iteration 56/1000 | Loss: 0.00001413
Iteration 57/1000 | Loss: 0.00001413
Iteration 58/1000 | Loss: 0.00001412
Iteration 59/1000 | Loss: 0.00001412
Iteration 60/1000 | Loss: 0.00001412
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001411
Iteration 63/1000 | Loss: 0.00001411
Iteration 64/1000 | Loss: 0.00001410
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001410
Iteration 67/1000 | Loss: 0.00001410
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001409
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001409
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001408
Iteration 78/1000 | Loss: 0.00001408
Iteration 79/1000 | Loss: 0.00001408
Iteration 80/1000 | Loss: 0.00001408
Iteration 81/1000 | Loss: 0.00001408
Iteration 82/1000 | Loss: 0.00001408
Iteration 83/1000 | Loss: 0.00001408
Iteration 84/1000 | Loss: 0.00001408
Iteration 85/1000 | Loss: 0.00001408
Iteration 86/1000 | Loss: 0.00001408
Iteration 87/1000 | Loss: 0.00001408
Iteration 88/1000 | Loss: 0.00001408
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001407
Iteration 92/1000 | Loss: 0.00001407
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001407
Iteration 95/1000 | Loss: 0.00001407
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001406
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001406
Iteration 100/1000 | Loss: 0.00001406
Iteration 101/1000 | Loss: 0.00001406
Iteration 102/1000 | Loss: 0.00001406
Iteration 103/1000 | Loss: 0.00001406
Iteration 104/1000 | Loss: 0.00001406
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001405
Iteration 109/1000 | Loss: 0.00001405
Iteration 110/1000 | Loss: 0.00001404
Iteration 111/1000 | Loss: 0.00001404
Iteration 112/1000 | Loss: 0.00001404
Iteration 113/1000 | Loss: 0.00001404
Iteration 114/1000 | Loss: 0.00001404
Iteration 115/1000 | Loss: 0.00001404
Iteration 116/1000 | Loss: 0.00001404
Iteration 117/1000 | Loss: 0.00001404
Iteration 118/1000 | Loss: 0.00001404
Iteration 119/1000 | Loss: 0.00001404
Iteration 120/1000 | Loss: 0.00001403
Iteration 121/1000 | Loss: 0.00001403
Iteration 122/1000 | Loss: 0.00001403
Iteration 123/1000 | Loss: 0.00001403
Iteration 124/1000 | Loss: 0.00001403
Iteration 125/1000 | Loss: 0.00001403
Iteration 126/1000 | Loss: 0.00001403
Iteration 127/1000 | Loss: 0.00001403
Iteration 128/1000 | Loss: 0.00001403
Iteration 129/1000 | Loss: 0.00001403
Iteration 130/1000 | Loss: 0.00001403
Iteration 131/1000 | Loss: 0.00001403
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001402
Iteration 141/1000 | Loss: 0.00001402
Iteration 142/1000 | Loss: 0.00001401
Iteration 143/1000 | Loss: 0.00001401
Iteration 144/1000 | Loss: 0.00001401
Iteration 145/1000 | Loss: 0.00001401
Iteration 146/1000 | Loss: 0.00001401
Iteration 147/1000 | Loss: 0.00001401
Iteration 148/1000 | Loss: 0.00001401
Iteration 149/1000 | Loss: 0.00001401
Iteration 150/1000 | Loss: 0.00001401
Iteration 151/1000 | Loss: 0.00001401
Iteration 152/1000 | Loss: 0.00001401
Iteration 153/1000 | Loss: 0.00001401
Iteration 154/1000 | Loss: 0.00001401
Iteration 155/1000 | Loss: 0.00001401
Iteration 156/1000 | Loss: 0.00001401
Iteration 157/1000 | Loss: 0.00001401
Iteration 158/1000 | Loss: 0.00001401
Iteration 159/1000 | Loss: 0.00001401
Iteration 160/1000 | Loss: 0.00001401
Iteration 161/1000 | Loss: 0.00001401
Iteration 162/1000 | Loss: 0.00001401
Iteration 163/1000 | Loss: 0.00001401
Iteration 164/1000 | Loss: 0.00001401
Iteration 165/1000 | Loss: 0.00001401
Iteration 166/1000 | Loss: 0.00001401
Iteration 167/1000 | Loss: 0.00001401
Iteration 168/1000 | Loss: 0.00001401
Iteration 169/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4012840438226704e-05, 1.4012840438226704e-05, 1.4012840438226704e-05, 1.4012840438226704e-05, 1.4012840438226704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4012840438226704e-05

Optimization complete. Final v2v error: 3.239975690841675 mm

Highest mean error: 3.951524019241333 mm for frame 53

Lowest mean error: 2.5114307403564453 mm for frame 218

Saving results

Total time: 44.32042098045349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534809
Iteration 2/25 | Loss: 0.00121995
Iteration 3/25 | Loss: 0.00096628
Iteration 4/25 | Loss: 0.00094194
Iteration 5/25 | Loss: 0.00093729
Iteration 6/25 | Loss: 0.00093099
Iteration 7/25 | Loss: 0.00092582
Iteration 8/25 | Loss: 0.00092374
Iteration 9/25 | Loss: 0.00092276
Iteration 10/25 | Loss: 0.00092214
Iteration 11/25 | Loss: 0.00092175
Iteration 12/25 | Loss: 0.00092150
Iteration 13/25 | Loss: 0.00092146
Iteration 14/25 | Loss: 0.00092072
Iteration 15/25 | Loss: 0.00092000
Iteration 16/25 | Loss: 0.00091957
Iteration 17/25 | Loss: 0.00091941
Iteration 18/25 | Loss: 0.00091937
Iteration 19/25 | Loss: 0.00091936
Iteration 20/25 | Loss: 0.00091936
Iteration 21/25 | Loss: 0.00091936
Iteration 22/25 | Loss: 0.00091936
Iteration 23/25 | Loss: 0.00091936
Iteration 24/25 | Loss: 0.00091936
Iteration 25/25 | Loss: 0.00091936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63810754
Iteration 2/25 | Loss: 0.00063596
Iteration 3/25 | Loss: 0.00062465
Iteration 4/25 | Loss: 0.00062378
Iteration 5/25 | Loss: 0.00062378
Iteration 6/25 | Loss: 0.00062378
Iteration 7/25 | Loss: 0.00062378
Iteration 8/25 | Loss: 0.00062378
Iteration 9/25 | Loss: 0.00062378
Iteration 10/25 | Loss: 0.00062378
Iteration 11/25 | Loss: 0.00062378
Iteration 12/25 | Loss: 0.00062378
Iteration 13/25 | Loss: 0.00062378
Iteration 14/25 | Loss: 0.00062378
Iteration 15/25 | Loss: 0.00062378
Iteration 16/25 | Loss: 0.00062378
Iteration 17/25 | Loss: 0.00062378
Iteration 18/25 | Loss: 0.00062378
Iteration 19/25 | Loss: 0.00062378
Iteration 20/25 | Loss: 0.00062378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006237829802557826, 0.0006237829802557826, 0.0006237829802557826, 0.0006237829802557826, 0.0006237829802557826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006237829802557826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062378
Iteration 2/1000 | Loss: 0.00007053
Iteration 3/1000 | Loss: 0.00002536
Iteration 4/1000 | Loss: 0.00001974
Iteration 5/1000 | Loss: 0.00013800
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001700
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001464
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001372
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001301
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001298
Iteration 19/1000 | Loss: 0.00001297
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001289
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001286
Iteration 24/1000 | Loss: 0.00001286
Iteration 25/1000 | Loss: 0.00001286
Iteration 26/1000 | Loss: 0.00001286
Iteration 27/1000 | Loss: 0.00001285
Iteration 28/1000 | Loss: 0.00001285
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001282
Iteration 34/1000 | Loss: 0.00001282
Iteration 35/1000 | Loss: 0.00001282
Iteration 36/1000 | Loss: 0.00001282
Iteration 37/1000 | Loss: 0.00001282
Iteration 38/1000 | Loss: 0.00001282
Iteration 39/1000 | Loss: 0.00001281
Iteration 40/1000 | Loss: 0.00001281
Iteration 41/1000 | Loss: 0.00001281
Iteration 42/1000 | Loss: 0.00001280
Iteration 43/1000 | Loss: 0.00001280
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001279
Iteration 46/1000 | Loss: 0.00001279
Iteration 47/1000 | Loss: 0.00001279
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001278
Iteration 50/1000 | Loss: 0.00001277
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001276
Iteration 54/1000 | Loss: 0.00001276
Iteration 55/1000 | Loss: 0.00001276
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001275
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001273
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001273
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001273
Iteration 67/1000 | Loss: 0.00001273
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001273
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001269
Iteration 85/1000 | Loss: 0.00001269
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001268
Iteration 89/1000 | Loss: 0.00001268
Iteration 90/1000 | Loss: 0.00001267
Iteration 91/1000 | Loss: 0.00001267
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001266
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001264
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001263
Iteration 102/1000 | Loss: 0.00001263
Iteration 103/1000 | Loss: 0.00001263
Iteration 104/1000 | Loss: 0.00001263
Iteration 105/1000 | Loss: 0.00001262
Iteration 106/1000 | Loss: 0.00001262
Iteration 107/1000 | Loss: 0.00001262
Iteration 108/1000 | Loss: 0.00001262
Iteration 109/1000 | Loss: 0.00001262
Iteration 110/1000 | Loss: 0.00001262
Iteration 111/1000 | Loss: 0.00001262
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001262
Iteration 123/1000 | Loss: 0.00001262
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001262
Iteration 126/1000 | Loss: 0.00001262
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001262
Iteration 129/1000 | Loss: 0.00001262
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001262
Iteration 132/1000 | Loss: 0.00001262
Iteration 133/1000 | Loss: 0.00001262
Iteration 134/1000 | Loss: 0.00001262
Iteration 135/1000 | Loss: 0.00001262
Iteration 136/1000 | Loss: 0.00001262
Iteration 137/1000 | Loss: 0.00001262
Iteration 138/1000 | Loss: 0.00001262
Iteration 139/1000 | Loss: 0.00001262
Iteration 140/1000 | Loss: 0.00001262
Iteration 141/1000 | Loss: 0.00001262
Iteration 142/1000 | Loss: 0.00001262
Iteration 143/1000 | Loss: 0.00001262
Iteration 144/1000 | Loss: 0.00001262
Iteration 145/1000 | Loss: 0.00001262
Iteration 146/1000 | Loss: 0.00001262
Iteration 147/1000 | Loss: 0.00001262
Iteration 148/1000 | Loss: 0.00001262
Iteration 149/1000 | Loss: 0.00001262
Iteration 150/1000 | Loss: 0.00001262
Iteration 151/1000 | Loss: 0.00001262
Iteration 152/1000 | Loss: 0.00001262
Iteration 153/1000 | Loss: 0.00001262
Iteration 154/1000 | Loss: 0.00001262
Iteration 155/1000 | Loss: 0.00001262
Iteration 156/1000 | Loss: 0.00001262
Iteration 157/1000 | Loss: 0.00001262
Iteration 158/1000 | Loss: 0.00001262
Iteration 159/1000 | Loss: 0.00001262
Iteration 160/1000 | Loss: 0.00001262
Iteration 161/1000 | Loss: 0.00001262
Iteration 162/1000 | Loss: 0.00001262
Iteration 163/1000 | Loss: 0.00001262
Iteration 164/1000 | Loss: 0.00001262
Iteration 165/1000 | Loss: 0.00001262
Iteration 166/1000 | Loss: 0.00001262
Iteration 167/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.261790021089837e-05, 1.261790021089837e-05, 1.261790021089837e-05, 1.261790021089837e-05, 1.261790021089837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.261790021089837e-05

Optimization complete. Final v2v error: 3.045313596725464 mm

Highest mean error: 4.095434665679932 mm for frame 129

Lowest mean error: 2.548461675643921 mm for frame 1

Saving results

Total time: 69.23831224441528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609513
Iteration 2/25 | Loss: 0.00163126
Iteration 3/25 | Loss: 0.00110924
Iteration 4/25 | Loss: 0.00107739
Iteration 5/25 | Loss: 0.00107308
Iteration 6/25 | Loss: 0.00107120
Iteration 7/25 | Loss: 0.00107081
Iteration 8/25 | Loss: 0.00107081
Iteration 9/25 | Loss: 0.00107081
Iteration 10/25 | Loss: 0.00107081
Iteration 11/25 | Loss: 0.00107081
Iteration 12/25 | Loss: 0.00107081
Iteration 13/25 | Loss: 0.00107081
Iteration 14/25 | Loss: 0.00107081
Iteration 15/25 | Loss: 0.00107081
Iteration 16/25 | Loss: 0.00107081
Iteration 17/25 | Loss: 0.00107081
Iteration 18/25 | Loss: 0.00107081
Iteration 19/25 | Loss: 0.00107081
Iteration 20/25 | Loss: 0.00107081
Iteration 21/25 | Loss: 0.00107081
Iteration 22/25 | Loss: 0.00107081
Iteration 23/25 | Loss: 0.00107081
Iteration 24/25 | Loss: 0.00107081
Iteration 25/25 | Loss: 0.00107081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01670563
Iteration 2/25 | Loss: 0.00062442
Iteration 3/25 | Loss: 0.00062440
Iteration 4/25 | Loss: 0.00062440
Iteration 5/25 | Loss: 0.00062440
Iteration 6/25 | Loss: 0.00062440
Iteration 7/25 | Loss: 0.00062440
Iteration 8/25 | Loss: 0.00062440
Iteration 9/25 | Loss: 0.00062440
Iteration 10/25 | Loss: 0.00062440
Iteration 11/25 | Loss: 0.00062440
Iteration 12/25 | Loss: 0.00062440
Iteration 13/25 | Loss: 0.00062440
Iteration 14/25 | Loss: 0.00062440
Iteration 15/25 | Loss: 0.00062440
Iteration 16/25 | Loss: 0.00062440
Iteration 17/25 | Loss: 0.00062440
Iteration 18/25 | Loss: 0.00062440
Iteration 19/25 | Loss: 0.00062440
Iteration 20/25 | Loss: 0.00062440
Iteration 21/25 | Loss: 0.00062440
Iteration 22/25 | Loss: 0.00062440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006244003889150918, 0.0006244003889150918, 0.0006244003889150918, 0.0006244003889150918, 0.0006244003889150918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006244003889150918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062440
Iteration 2/1000 | Loss: 0.00009402
Iteration 3/1000 | Loss: 0.00005883
Iteration 4/1000 | Loss: 0.00004987
Iteration 5/1000 | Loss: 0.00004612
Iteration 6/1000 | Loss: 0.00004453
Iteration 7/1000 | Loss: 0.00004331
Iteration 8/1000 | Loss: 0.00004207
Iteration 9/1000 | Loss: 0.00004136
Iteration 10/1000 | Loss: 0.00004080
Iteration 11/1000 | Loss: 0.00004011
Iteration 12/1000 | Loss: 0.00003958
Iteration 13/1000 | Loss: 0.00003914
Iteration 14/1000 | Loss: 0.00003880
Iteration 15/1000 | Loss: 0.00003849
Iteration 16/1000 | Loss: 0.00003822
Iteration 17/1000 | Loss: 0.00003790
Iteration 18/1000 | Loss: 0.00003772
Iteration 19/1000 | Loss: 0.00003769
Iteration 20/1000 | Loss: 0.00003760
Iteration 21/1000 | Loss: 0.00003744
Iteration 22/1000 | Loss: 0.00003729
Iteration 23/1000 | Loss: 0.00003726
Iteration 24/1000 | Loss: 0.00003723
Iteration 25/1000 | Loss: 0.00003719
Iteration 26/1000 | Loss: 0.00003719
Iteration 27/1000 | Loss: 0.00003719
Iteration 28/1000 | Loss: 0.00003718
Iteration 29/1000 | Loss: 0.00003718
Iteration 30/1000 | Loss: 0.00003717
Iteration 31/1000 | Loss: 0.00003717
Iteration 32/1000 | Loss: 0.00003717
Iteration 33/1000 | Loss: 0.00003716
Iteration 34/1000 | Loss: 0.00003716
Iteration 35/1000 | Loss: 0.00003715
Iteration 36/1000 | Loss: 0.00003715
Iteration 37/1000 | Loss: 0.00003715
Iteration 38/1000 | Loss: 0.00003715
Iteration 39/1000 | Loss: 0.00003715
Iteration 40/1000 | Loss: 0.00003715
Iteration 41/1000 | Loss: 0.00003715
Iteration 42/1000 | Loss: 0.00003715
Iteration 43/1000 | Loss: 0.00003715
Iteration 44/1000 | Loss: 0.00003715
Iteration 45/1000 | Loss: 0.00003715
Iteration 46/1000 | Loss: 0.00003714
Iteration 47/1000 | Loss: 0.00003713
Iteration 48/1000 | Loss: 0.00003712
Iteration 49/1000 | Loss: 0.00003712
Iteration 50/1000 | Loss: 0.00003711
Iteration 51/1000 | Loss: 0.00003711
Iteration 52/1000 | Loss: 0.00003711
Iteration 53/1000 | Loss: 0.00003710
Iteration 54/1000 | Loss: 0.00003710
Iteration 55/1000 | Loss: 0.00003710
Iteration 56/1000 | Loss: 0.00003710
Iteration 57/1000 | Loss: 0.00003710
Iteration 58/1000 | Loss: 0.00003710
Iteration 59/1000 | Loss: 0.00003709
Iteration 60/1000 | Loss: 0.00003709
Iteration 61/1000 | Loss: 0.00003707
Iteration 62/1000 | Loss: 0.00003707
Iteration 63/1000 | Loss: 0.00003707
Iteration 64/1000 | Loss: 0.00003706
Iteration 65/1000 | Loss: 0.00003706
Iteration 66/1000 | Loss: 0.00003706
Iteration 67/1000 | Loss: 0.00003705
Iteration 68/1000 | Loss: 0.00003705
Iteration 69/1000 | Loss: 0.00003705
Iteration 70/1000 | Loss: 0.00003705
Iteration 71/1000 | Loss: 0.00003705
Iteration 72/1000 | Loss: 0.00003705
Iteration 73/1000 | Loss: 0.00003705
Iteration 74/1000 | Loss: 0.00003705
Iteration 75/1000 | Loss: 0.00003705
Iteration 76/1000 | Loss: 0.00003705
Iteration 77/1000 | Loss: 0.00003705
Iteration 78/1000 | Loss: 0.00003705
Iteration 79/1000 | Loss: 0.00003705
Iteration 80/1000 | Loss: 0.00003705
Iteration 81/1000 | Loss: 0.00003704
Iteration 82/1000 | Loss: 0.00003704
Iteration 83/1000 | Loss: 0.00003704
Iteration 84/1000 | Loss: 0.00003704
Iteration 85/1000 | Loss: 0.00003703
Iteration 86/1000 | Loss: 0.00003703
Iteration 87/1000 | Loss: 0.00003703
Iteration 88/1000 | Loss: 0.00003703
Iteration 89/1000 | Loss: 0.00003703
Iteration 90/1000 | Loss: 0.00003703
Iteration 91/1000 | Loss: 0.00003703
Iteration 92/1000 | Loss: 0.00003703
Iteration 93/1000 | Loss: 0.00003703
Iteration 94/1000 | Loss: 0.00003703
Iteration 95/1000 | Loss: 0.00003703
Iteration 96/1000 | Loss: 0.00003702
Iteration 97/1000 | Loss: 0.00003702
Iteration 98/1000 | Loss: 0.00003702
Iteration 99/1000 | Loss: 0.00003702
Iteration 100/1000 | Loss: 0.00003702
Iteration 101/1000 | Loss: 0.00003702
Iteration 102/1000 | Loss: 0.00003702
Iteration 103/1000 | Loss: 0.00003702
Iteration 104/1000 | Loss: 0.00003702
Iteration 105/1000 | Loss: 0.00003702
Iteration 106/1000 | Loss: 0.00003702
Iteration 107/1000 | Loss: 0.00003701
Iteration 108/1000 | Loss: 0.00003701
Iteration 109/1000 | Loss: 0.00003701
Iteration 110/1000 | Loss: 0.00003701
Iteration 111/1000 | Loss: 0.00003701
Iteration 112/1000 | Loss: 0.00003701
Iteration 113/1000 | Loss: 0.00003701
Iteration 114/1000 | Loss: 0.00003701
Iteration 115/1000 | Loss: 0.00003701
Iteration 116/1000 | Loss: 0.00003701
Iteration 117/1000 | Loss: 0.00003701
Iteration 118/1000 | Loss: 0.00003701
Iteration 119/1000 | Loss: 0.00003700
Iteration 120/1000 | Loss: 0.00003700
Iteration 121/1000 | Loss: 0.00003700
Iteration 122/1000 | Loss: 0.00003700
Iteration 123/1000 | Loss: 0.00003700
Iteration 124/1000 | Loss: 0.00003700
Iteration 125/1000 | Loss: 0.00003700
Iteration 126/1000 | Loss: 0.00003700
Iteration 127/1000 | Loss: 0.00003700
Iteration 128/1000 | Loss: 0.00003700
Iteration 129/1000 | Loss: 0.00003700
Iteration 130/1000 | Loss: 0.00003700
Iteration 131/1000 | Loss: 0.00003700
Iteration 132/1000 | Loss: 0.00003700
Iteration 133/1000 | Loss: 0.00003700
Iteration 134/1000 | Loss: 0.00003700
Iteration 135/1000 | Loss: 0.00003700
Iteration 136/1000 | Loss: 0.00003700
Iteration 137/1000 | Loss: 0.00003700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [3.700431261677295e-05, 3.700431261677295e-05, 3.700431261677295e-05, 3.700431261677295e-05, 3.700431261677295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.700431261677295e-05

Optimization complete. Final v2v error: 4.535773754119873 mm

Highest mean error: 6.037172317504883 mm for frame 151

Lowest mean error: 3.1489100456237793 mm for frame 0

Saving results

Total time: 48.93730092048645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381345
Iteration 2/25 | Loss: 0.00107691
Iteration 3/25 | Loss: 0.00092128
Iteration 4/25 | Loss: 0.00089877
Iteration 5/25 | Loss: 0.00089320
Iteration 6/25 | Loss: 0.00089161
Iteration 7/25 | Loss: 0.00089139
Iteration 8/25 | Loss: 0.00089139
Iteration 9/25 | Loss: 0.00089139
Iteration 10/25 | Loss: 0.00089139
Iteration 11/25 | Loss: 0.00089139
Iteration 12/25 | Loss: 0.00089139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008913923520594835, 0.0008913923520594835, 0.0008913923520594835, 0.0008913923520594835, 0.0008913923520594835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008913923520594835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32097864
Iteration 2/25 | Loss: 0.00059394
Iteration 3/25 | Loss: 0.00059394
Iteration 4/25 | Loss: 0.00059394
Iteration 5/25 | Loss: 0.00059394
Iteration 6/25 | Loss: 0.00059394
Iteration 7/25 | Loss: 0.00059394
Iteration 8/25 | Loss: 0.00059394
Iteration 9/25 | Loss: 0.00059394
Iteration 10/25 | Loss: 0.00059394
Iteration 11/25 | Loss: 0.00059394
Iteration 12/25 | Loss: 0.00059394
Iteration 13/25 | Loss: 0.00059394
Iteration 14/25 | Loss: 0.00059394
Iteration 15/25 | Loss: 0.00059394
Iteration 16/25 | Loss: 0.00059394
Iteration 17/25 | Loss: 0.00059394
Iteration 18/25 | Loss: 0.00059394
Iteration 19/25 | Loss: 0.00059394
Iteration 20/25 | Loss: 0.00059394
Iteration 21/25 | Loss: 0.00059394
Iteration 22/25 | Loss: 0.00059394
Iteration 23/25 | Loss: 0.00059394
Iteration 24/25 | Loss: 0.00059394
Iteration 25/25 | Loss: 0.00059394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059394
Iteration 2/1000 | Loss: 0.00006455
Iteration 3/1000 | Loss: 0.00003549
Iteration 4/1000 | Loss: 0.00002276
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001501
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001446
Iteration 14/1000 | Loss: 0.00001437
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001416
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001411
Iteration 21/1000 | Loss: 0.00001411
Iteration 22/1000 | Loss: 0.00001411
Iteration 23/1000 | Loss: 0.00001410
Iteration 24/1000 | Loss: 0.00001410
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001405
Iteration 34/1000 | Loss: 0.00001405
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001404
Iteration 37/1000 | Loss: 0.00001404
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001402
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001398
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001397
Iteration 47/1000 | Loss: 0.00001393
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001392
Iteration 50/1000 | Loss: 0.00001391
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001384
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001383
Iteration 69/1000 | Loss: 0.00001383
Iteration 70/1000 | Loss: 0.00001383
Iteration 71/1000 | Loss: 0.00001383
Iteration 72/1000 | Loss: 0.00001382
Iteration 73/1000 | Loss: 0.00001382
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001381
Iteration 76/1000 | Loss: 0.00001381
Iteration 77/1000 | Loss: 0.00001381
Iteration 78/1000 | Loss: 0.00001380
Iteration 79/1000 | Loss: 0.00001380
Iteration 80/1000 | Loss: 0.00001380
Iteration 81/1000 | Loss: 0.00001379
Iteration 82/1000 | Loss: 0.00001379
Iteration 83/1000 | Loss: 0.00001379
Iteration 84/1000 | Loss: 0.00001379
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001378
Iteration 87/1000 | Loss: 0.00001378
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001377
Iteration 103/1000 | Loss: 0.00001377
Iteration 104/1000 | Loss: 0.00001377
Iteration 105/1000 | Loss: 0.00001376
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001376
Iteration 108/1000 | Loss: 0.00001376
Iteration 109/1000 | Loss: 0.00001376
Iteration 110/1000 | Loss: 0.00001376
Iteration 111/1000 | Loss: 0.00001376
Iteration 112/1000 | Loss: 0.00001376
Iteration 113/1000 | Loss: 0.00001376
Iteration 114/1000 | Loss: 0.00001376
Iteration 115/1000 | Loss: 0.00001376
Iteration 116/1000 | Loss: 0.00001375
Iteration 117/1000 | Loss: 0.00001375
Iteration 118/1000 | Loss: 0.00001375
Iteration 119/1000 | Loss: 0.00001375
Iteration 120/1000 | Loss: 0.00001375
Iteration 121/1000 | Loss: 0.00001375
Iteration 122/1000 | Loss: 0.00001375
Iteration 123/1000 | Loss: 0.00001375
Iteration 124/1000 | Loss: 0.00001375
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001375
Iteration 127/1000 | Loss: 0.00001375
Iteration 128/1000 | Loss: 0.00001375
Iteration 129/1000 | Loss: 0.00001375
Iteration 130/1000 | Loss: 0.00001374
Iteration 131/1000 | Loss: 0.00001374
Iteration 132/1000 | Loss: 0.00001374
Iteration 133/1000 | Loss: 0.00001374
Iteration 134/1000 | Loss: 0.00001374
Iteration 135/1000 | Loss: 0.00001374
Iteration 136/1000 | Loss: 0.00001374
Iteration 137/1000 | Loss: 0.00001374
Iteration 138/1000 | Loss: 0.00001374
Iteration 139/1000 | Loss: 0.00001374
Iteration 140/1000 | Loss: 0.00001374
Iteration 141/1000 | Loss: 0.00001374
Iteration 142/1000 | Loss: 0.00001374
Iteration 143/1000 | Loss: 0.00001374
Iteration 144/1000 | Loss: 0.00001374
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001373
Iteration 149/1000 | Loss: 0.00001373
Iteration 150/1000 | Loss: 0.00001373
Iteration 151/1000 | Loss: 0.00001373
Iteration 152/1000 | Loss: 0.00001373
Iteration 153/1000 | Loss: 0.00001373
Iteration 154/1000 | Loss: 0.00001373
Iteration 155/1000 | Loss: 0.00001373
Iteration 156/1000 | Loss: 0.00001373
Iteration 157/1000 | Loss: 0.00001372
Iteration 158/1000 | Loss: 0.00001372
Iteration 159/1000 | Loss: 0.00001372
Iteration 160/1000 | Loss: 0.00001372
Iteration 161/1000 | Loss: 0.00001372
Iteration 162/1000 | Loss: 0.00001372
Iteration 163/1000 | Loss: 0.00001372
Iteration 164/1000 | Loss: 0.00001372
Iteration 165/1000 | Loss: 0.00001372
Iteration 166/1000 | Loss: 0.00001372
Iteration 167/1000 | Loss: 0.00001372
Iteration 168/1000 | Loss: 0.00001372
Iteration 169/1000 | Loss: 0.00001372
Iteration 170/1000 | Loss: 0.00001372
Iteration 171/1000 | Loss: 0.00001372
Iteration 172/1000 | Loss: 0.00001372
Iteration 173/1000 | Loss: 0.00001372
Iteration 174/1000 | Loss: 0.00001371
Iteration 175/1000 | Loss: 0.00001371
Iteration 176/1000 | Loss: 0.00001371
Iteration 177/1000 | Loss: 0.00001371
Iteration 178/1000 | Loss: 0.00001371
Iteration 179/1000 | Loss: 0.00001371
Iteration 180/1000 | Loss: 0.00001371
Iteration 181/1000 | Loss: 0.00001371
Iteration 182/1000 | Loss: 0.00001371
Iteration 183/1000 | Loss: 0.00001371
Iteration 184/1000 | Loss: 0.00001371
Iteration 185/1000 | Loss: 0.00001371
Iteration 186/1000 | Loss: 0.00001371
Iteration 187/1000 | Loss: 0.00001371
Iteration 188/1000 | Loss: 0.00001371
Iteration 189/1000 | Loss: 0.00001371
Iteration 190/1000 | Loss: 0.00001371
Iteration 191/1000 | Loss: 0.00001371
Iteration 192/1000 | Loss: 0.00001371
Iteration 193/1000 | Loss: 0.00001371
Iteration 194/1000 | Loss: 0.00001370
Iteration 195/1000 | Loss: 0.00001370
Iteration 196/1000 | Loss: 0.00001370
Iteration 197/1000 | Loss: 0.00001370
Iteration 198/1000 | Loss: 0.00001370
Iteration 199/1000 | Loss: 0.00001370
Iteration 200/1000 | Loss: 0.00001370
Iteration 201/1000 | Loss: 0.00001370
Iteration 202/1000 | Loss: 0.00001370
Iteration 203/1000 | Loss: 0.00001370
Iteration 204/1000 | Loss: 0.00001370
Iteration 205/1000 | Loss: 0.00001370
Iteration 206/1000 | Loss: 0.00001370
Iteration 207/1000 | Loss: 0.00001369
Iteration 208/1000 | Loss: 0.00001369
Iteration 209/1000 | Loss: 0.00001369
Iteration 210/1000 | Loss: 0.00001369
Iteration 211/1000 | Loss: 0.00001369
Iteration 212/1000 | Loss: 0.00001369
Iteration 213/1000 | Loss: 0.00001369
Iteration 214/1000 | Loss: 0.00001369
Iteration 215/1000 | Loss: 0.00001369
Iteration 216/1000 | Loss: 0.00001369
Iteration 217/1000 | Loss: 0.00001369
Iteration 218/1000 | Loss: 0.00001369
Iteration 219/1000 | Loss: 0.00001369
Iteration 220/1000 | Loss: 0.00001369
Iteration 221/1000 | Loss: 0.00001369
Iteration 222/1000 | Loss: 0.00001369
Iteration 223/1000 | Loss: 0.00001368
Iteration 224/1000 | Loss: 0.00001368
Iteration 225/1000 | Loss: 0.00001368
Iteration 226/1000 | Loss: 0.00001368
Iteration 227/1000 | Loss: 0.00001368
Iteration 228/1000 | Loss: 0.00001368
Iteration 229/1000 | Loss: 0.00001368
Iteration 230/1000 | Loss: 0.00001368
Iteration 231/1000 | Loss: 0.00001368
Iteration 232/1000 | Loss: 0.00001368
Iteration 233/1000 | Loss: 0.00001368
Iteration 234/1000 | Loss: 0.00001368
Iteration 235/1000 | Loss: 0.00001368
Iteration 236/1000 | Loss: 0.00001368
Iteration 237/1000 | Loss: 0.00001368
Iteration 238/1000 | Loss: 0.00001367
Iteration 239/1000 | Loss: 0.00001367
Iteration 240/1000 | Loss: 0.00001367
Iteration 241/1000 | Loss: 0.00001367
Iteration 242/1000 | Loss: 0.00001367
Iteration 243/1000 | Loss: 0.00001367
Iteration 244/1000 | Loss: 0.00001367
Iteration 245/1000 | Loss: 0.00001367
Iteration 246/1000 | Loss: 0.00001367
Iteration 247/1000 | Loss: 0.00001367
Iteration 248/1000 | Loss: 0.00001367
Iteration 249/1000 | Loss: 0.00001367
Iteration 250/1000 | Loss: 0.00001367
Iteration 251/1000 | Loss: 0.00001367
Iteration 252/1000 | Loss: 0.00001367
Iteration 253/1000 | Loss: 0.00001367
Iteration 254/1000 | Loss: 0.00001367
Iteration 255/1000 | Loss: 0.00001367
Iteration 256/1000 | Loss: 0.00001367
Iteration 257/1000 | Loss: 0.00001367
Iteration 258/1000 | Loss: 0.00001367
Iteration 259/1000 | Loss: 0.00001366
Iteration 260/1000 | Loss: 0.00001366
Iteration 261/1000 | Loss: 0.00001366
Iteration 262/1000 | Loss: 0.00001366
Iteration 263/1000 | Loss: 0.00001366
Iteration 264/1000 | Loss: 0.00001366
Iteration 265/1000 | Loss: 0.00001366
Iteration 266/1000 | Loss: 0.00001366
Iteration 267/1000 | Loss: 0.00001366
Iteration 268/1000 | Loss: 0.00001366
Iteration 269/1000 | Loss: 0.00001366
Iteration 270/1000 | Loss: 0.00001366
Iteration 271/1000 | Loss: 0.00001366
Iteration 272/1000 | Loss: 0.00001366
Iteration 273/1000 | Loss: 0.00001366
Iteration 274/1000 | Loss: 0.00001365
Iteration 275/1000 | Loss: 0.00001365
Iteration 276/1000 | Loss: 0.00001365
Iteration 277/1000 | Loss: 0.00001365
Iteration 278/1000 | Loss: 0.00001365
Iteration 279/1000 | Loss: 0.00001365
Iteration 280/1000 | Loss: 0.00001365
Iteration 281/1000 | Loss: 0.00001365
Iteration 282/1000 | Loss: 0.00001365
Iteration 283/1000 | Loss: 0.00001365
Iteration 284/1000 | Loss: 0.00001365
Iteration 285/1000 | Loss: 0.00001365
Iteration 286/1000 | Loss: 0.00001365
Iteration 287/1000 | Loss: 0.00001365
Iteration 288/1000 | Loss: 0.00001365
Iteration 289/1000 | Loss: 0.00001365
Iteration 290/1000 | Loss: 0.00001365
Iteration 291/1000 | Loss: 0.00001365
Iteration 292/1000 | Loss: 0.00001365
Iteration 293/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [1.3651942026626784e-05, 1.3651942026626784e-05, 1.3651942026626784e-05, 1.3651942026626784e-05, 1.3651942026626784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3651942026626784e-05

Optimization complete. Final v2v error: 3.1263740062713623 mm

Highest mean error: 3.4691145420074463 mm for frame 81

Lowest mean error: 2.656395196914673 mm for frame 14

Saving results

Total time: 46.56868124008179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988080
Iteration 2/25 | Loss: 0.00238625
Iteration 3/25 | Loss: 0.00158709
Iteration 4/25 | Loss: 0.00143637
Iteration 5/25 | Loss: 0.00141062
Iteration 6/25 | Loss: 0.00130010
Iteration 7/25 | Loss: 0.00124879
Iteration 8/25 | Loss: 0.00122937
Iteration 9/25 | Loss: 0.00119756
Iteration 10/25 | Loss: 0.00116975
Iteration 11/25 | Loss: 0.00114427
Iteration 12/25 | Loss: 0.00113942
Iteration 13/25 | Loss: 0.00113246
Iteration 14/25 | Loss: 0.00112218
Iteration 15/25 | Loss: 0.00112713
Iteration 16/25 | Loss: 0.00111641
Iteration 17/25 | Loss: 0.00111718
Iteration 18/25 | Loss: 0.00111931
Iteration 19/25 | Loss: 0.00111768
Iteration 20/25 | Loss: 0.00113658
Iteration 21/25 | Loss: 0.00112480
Iteration 22/25 | Loss: 0.00111906
Iteration 23/25 | Loss: 0.00111689
Iteration 24/25 | Loss: 0.00111522
Iteration 25/25 | Loss: 0.00110104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37079298
Iteration 2/25 | Loss: 0.00216115
Iteration 3/25 | Loss: 0.00191802
Iteration 4/25 | Loss: 0.00191802
Iteration 5/25 | Loss: 0.00191802
Iteration 6/25 | Loss: 0.00191802
Iteration 7/25 | Loss: 0.00191802
Iteration 8/25 | Loss: 0.00191802
Iteration 9/25 | Loss: 0.00191802
Iteration 10/25 | Loss: 0.00191802
Iteration 11/25 | Loss: 0.00191801
Iteration 12/25 | Loss: 0.00191801
Iteration 13/25 | Loss: 0.00191801
Iteration 14/25 | Loss: 0.00191801
Iteration 15/25 | Loss: 0.00191801
Iteration 16/25 | Loss: 0.00191801
Iteration 17/25 | Loss: 0.00191801
Iteration 18/25 | Loss: 0.00191801
Iteration 19/25 | Loss: 0.00191801
Iteration 20/25 | Loss: 0.00191801
Iteration 21/25 | Loss: 0.00191801
Iteration 22/25 | Loss: 0.00191801
Iteration 23/25 | Loss: 0.00191801
Iteration 24/25 | Loss: 0.00191801
Iteration 25/25 | Loss: 0.00191801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191801
Iteration 2/1000 | Loss: 0.00031096
Iteration 3/1000 | Loss: 0.00019616
Iteration 4/1000 | Loss: 0.00018822
Iteration 5/1000 | Loss: 0.00027213
Iteration 6/1000 | Loss: 0.00018090
Iteration 7/1000 | Loss: 0.00020798
Iteration 8/1000 | Loss: 0.00012551
Iteration 9/1000 | Loss: 0.00021843
Iteration 10/1000 | Loss: 0.00058055
Iteration 11/1000 | Loss: 0.00036046
Iteration 12/1000 | Loss: 0.00052110
Iteration 13/1000 | Loss: 0.00014846
Iteration 14/1000 | Loss: 0.00009412
Iteration 15/1000 | Loss: 0.00016627
Iteration 16/1000 | Loss: 0.00018654
Iteration 17/1000 | Loss: 0.00030846
Iteration 18/1000 | Loss: 0.00015869
Iteration 19/1000 | Loss: 0.00016452
Iteration 20/1000 | Loss: 0.00036239
Iteration 21/1000 | Loss: 0.00007173
Iteration 22/1000 | Loss: 0.00016363
Iteration 23/1000 | Loss: 0.00015860
Iteration 24/1000 | Loss: 0.00017276
Iteration 25/1000 | Loss: 0.00024101
Iteration 26/1000 | Loss: 0.00023385
Iteration 27/1000 | Loss: 0.00015039
Iteration 28/1000 | Loss: 0.00040835
Iteration 29/1000 | Loss: 0.00017912
Iteration 30/1000 | Loss: 0.00021026
Iteration 31/1000 | Loss: 0.00024872
Iteration 32/1000 | Loss: 0.00025768
Iteration 33/1000 | Loss: 0.00022409
Iteration 34/1000 | Loss: 0.00013455
Iteration 35/1000 | Loss: 0.00025478
Iteration 36/1000 | Loss: 0.00015122
Iteration 37/1000 | Loss: 0.00015389
Iteration 38/1000 | Loss: 0.00008128
Iteration 39/1000 | Loss: 0.00019337
Iteration 40/1000 | Loss: 0.00012337
Iteration 41/1000 | Loss: 0.00013768
Iteration 42/1000 | Loss: 0.00039041
Iteration 43/1000 | Loss: 0.00056940
Iteration 44/1000 | Loss: 0.00040510
Iteration 45/1000 | Loss: 0.00033992
Iteration 46/1000 | Loss: 0.00035847
Iteration 47/1000 | Loss: 0.00009647
Iteration 48/1000 | Loss: 0.00009431
Iteration 49/1000 | Loss: 0.00017275
Iteration 50/1000 | Loss: 0.00039357
Iteration 51/1000 | Loss: 0.00027188
Iteration 52/1000 | Loss: 0.00010270
Iteration 53/1000 | Loss: 0.00022033
Iteration 54/1000 | Loss: 0.00031961
Iteration 55/1000 | Loss: 0.00024680
Iteration 56/1000 | Loss: 0.00011724
Iteration 57/1000 | Loss: 0.00024136
Iteration 58/1000 | Loss: 0.00020579
Iteration 59/1000 | Loss: 0.00031625
Iteration 60/1000 | Loss: 0.00032309
Iteration 61/1000 | Loss: 0.00044756
Iteration 62/1000 | Loss: 0.00011181
Iteration 63/1000 | Loss: 0.00014434
Iteration 64/1000 | Loss: 0.00030247
Iteration 65/1000 | Loss: 0.00017478
Iteration 66/1000 | Loss: 0.00018648
Iteration 67/1000 | Loss: 0.00009168
Iteration 68/1000 | Loss: 0.00017823
Iteration 69/1000 | Loss: 0.00018801
Iteration 70/1000 | Loss: 0.00024398
Iteration 71/1000 | Loss: 0.00009756
Iteration 72/1000 | Loss: 0.00006887
Iteration 73/1000 | Loss: 0.00012379
Iteration 74/1000 | Loss: 0.00006663
Iteration 75/1000 | Loss: 0.00006381
Iteration 76/1000 | Loss: 0.00006181
Iteration 77/1000 | Loss: 0.00008725
Iteration 78/1000 | Loss: 0.00006165
Iteration 79/1000 | Loss: 0.00023481
Iteration 80/1000 | Loss: 0.00065270
Iteration 81/1000 | Loss: 0.00023096
Iteration 82/1000 | Loss: 0.00031716
Iteration 83/1000 | Loss: 0.00045871
Iteration 84/1000 | Loss: 0.00035451
Iteration 85/1000 | Loss: 0.00008960
Iteration 86/1000 | Loss: 0.00005843
Iteration 87/1000 | Loss: 0.00005411
Iteration 88/1000 | Loss: 0.00034901
Iteration 89/1000 | Loss: 0.00005443
Iteration 90/1000 | Loss: 0.00044501
Iteration 91/1000 | Loss: 0.00006538
Iteration 92/1000 | Loss: 0.00004920
Iteration 93/1000 | Loss: 0.00004685
Iteration 94/1000 | Loss: 0.00004587
Iteration 95/1000 | Loss: 0.00022082
Iteration 96/1000 | Loss: 0.00004517
Iteration 97/1000 | Loss: 0.00004473
Iteration 98/1000 | Loss: 0.00041736
Iteration 99/1000 | Loss: 0.00032954
Iteration 100/1000 | Loss: 0.00041223
Iteration 101/1000 | Loss: 0.00004722
Iteration 102/1000 | Loss: 0.00004222
Iteration 103/1000 | Loss: 0.00003989
Iteration 104/1000 | Loss: 0.00003792
Iteration 105/1000 | Loss: 0.00003628
Iteration 106/1000 | Loss: 0.00003549
Iteration 107/1000 | Loss: 0.00003504
Iteration 108/1000 | Loss: 0.00003455
Iteration 109/1000 | Loss: 0.00040286
Iteration 110/1000 | Loss: 0.00048831
Iteration 111/1000 | Loss: 0.00036953
Iteration 112/1000 | Loss: 0.00003439
Iteration 113/1000 | Loss: 0.00011459
Iteration 114/1000 | Loss: 0.00003277
Iteration 115/1000 | Loss: 0.00003176
Iteration 116/1000 | Loss: 0.00067682
Iteration 117/1000 | Loss: 0.00055016
Iteration 118/1000 | Loss: 0.00003196
Iteration 119/1000 | Loss: 0.00058961
Iteration 120/1000 | Loss: 0.00004217
Iteration 121/1000 | Loss: 0.00038013
Iteration 122/1000 | Loss: 0.00003529
Iteration 123/1000 | Loss: 0.00003096
Iteration 124/1000 | Loss: 0.00002844
Iteration 125/1000 | Loss: 0.00002679
Iteration 126/1000 | Loss: 0.00002585
Iteration 127/1000 | Loss: 0.00002531
Iteration 128/1000 | Loss: 0.00037622
Iteration 129/1000 | Loss: 0.00002799
Iteration 130/1000 | Loss: 0.00002508
Iteration 131/1000 | Loss: 0.00002420
Iteration 132/1000 | Loss: 0.00002340
Iteration 133/1000 | Loss: 0.00002239
Iteration 134/1000 | Loss: 0.00002197
Iteration 135/1000 | Loss: 0.00002182
Iteration 136/1000 | Loss: 0.00002181
Iteration 137/1000 | Loss: 0.00002177
Iteration 138/1000 | Loss: 0.00002160
Iteration 139/1000 | Loss: 0.00002147
Iteration 140/1000 | Loss: 0.00002146
Iteration 141/1000 | Loss: 0.00002143
Iteration 142/1000 | Loss: 0.00002142
Iteration 143/1000 | Loss: 0.00002142
Iteration 144/1000 | Loss: 0.00002141
Iteration 145/1000 | Loss: 0.00002141
Iteration 146/1000 | Loss: 0.00002140
Iteration 147/1000 | Loss: 0.00002140
Iteration 148/1000 | Loss: 0.00002139
Iteration 149/1000 | Loss: 0.00002139
Iteration 150/1000 | Loss: 0.00002139
Iteration 151/1000 | Loss: 0.00002139
Iteration 152/1000 | Loss: 0.00002138
Iteration 153/1000 | Loss: 0.00002138
Iteration 154/1000 | Loss: 0.00002137
Iteration 155/1000 | Loss: 0.00002137
Iteration 156/1000 | Loss: 0.00002137
Iteration 157/1000 | Loss: 0.00002137
Iteration 158/1000 | Loss: 0.00002136
Iteration 159/1000 | Loss: 0.00002136
Iteration 160/1000 | Loss: 0.00002135
Iteration 161/1000 | Loss: 0.00002135
Iteration 162/1000 | Loss: 0.00002135
Iteration 163/1000 | Loss: 0.00002135
Iteration 164/1000 | Loss: 0.00002135
Iteration 165/1000 | Loss: 0.00002134
Iteration 166/1000 | Loss: 0.00002134
Iteration 167/1000 | Loss: 0.00002134
Iteration 168/1000 | Loss: 0.00002133
Iteration 169/1000 | Loss: 0.00002133
Iteration 170/1000 | Loss: 0.00002133
Iteration 171/1000 | Loss: 0.00002132
Iteration 172/1000 | Loss: 0.00002132
Iteration 173/1000 | Loss: 0.00002132
Iteration 174/1000 | Loss: 0.00002132
Iteration 175/1000 | Loss: 0.00002132
Iteration 176/1000 | Loss: 0.00002131
Iteration 177/1000 | Loss: 0.00002131
Iteration 178/1000 | Loss: 0.00002131
Iteration 179/1000 | Loss: 0.00002131
Iteration 180/1000 | Loss: 0.00002130
Iteration 181/1000 | Loss: 0.00002130
Iteration 182/1000 | Loss: 0.00002128
Iteration 183/1000 | Loss: 0.00002127
Iteration 184/1000 | Loss: 0.00002127
Iteration 185/1000 | Loss: 0.00002126
Iteration 186/1000 | Loss: 0.00002126
Iteration 187/1000 | Loss: 0.00002126
Iteration 188/1000 | Loss: 0.00002125
Iteration 189/1000 | Loss: 0.00002125
Iteration 190/1000 | Loss: 0.00002125
Iteration 191/1000 | Loss: 0.00002125
Iteration 192/1000 | Loss: 0.00002125
Iteration 193/1000 | Loss: 0.00002124
Iteration 194/1000 | Loss: 0.00002124
Iteration 195/1000 | Loss: 0.00002124
Iteration 196/1000 | Loss: 0.00002124
Iteration 197/1000 | Loss: 0.00002124
Iteration 198/1000 | Loss: 0.00002124
Iteration 199/1000 | Loss: 0.00002124
Iteration 200/1000 | Loss: 0.00002124
Iteration 201/1000 | Loss: 0.00002124
Iteration 202/1000 | Loss: 0.00002123
Iteration 203/1000 | Loss: 0.00002123
Iteration 204/1000 | Loss: 0.00002123
Iteration 205/1000 | Loss: 0.00002123
Iteration 206/1000 | Loss: 0.00002123
Iteration 207/1000 | Loss: 0.00002123
Iteration 208/1000 | Loss: 0.00002123
Iteration 209/1000 | Loss: 0.00002123
Iteration 210/1000 | Loss: 0.00002122
Iteration 211/1000 | Loss: 0.00002122
Iteration 212/1000 | Loss: 0.00002122
Iteration 213/1000 | Loss: 0.00002122
Iteration 214/1000 | Loss: 0.00002122
Iteration 215/1000 | Loss: 0.00002122
Iteration 216/1000 | Loss: 0.00002122
Iteration 217/1000 | Loss: 0.00002122
Iteration 218/1000 | Loss: 0.00002122
Iteration 219/1000 | Loss: 0.00002121
Iteration 220/1000 | Loss: 0.00002121
Iteration 221/1000 | Loss: 0.00002121
Iteration 222/1000 | Loss: 0.00002121
Iteration 223/1000 | Loss: 0.00002121
Iteration 224/1000 | Loss: 0.00002121
Iteration 225/1000 | Loss: 0.00002121
Iteration 226/1000 | Loss: 0.00002121
Iteration 227/1000 | Loss: 0.00002121
Iteration 228/1000 | Loss: 0.00002121
Iteration 229/1000 | Loss: 0.00002121
Iteration 230/1000 | Loss: 0.00002121
Iteration 231/1000 | Loss: 0.00002121
Iteration 232/1000 | Loss: 0.00002121
Iteration 233/1000 | Loss: 0.00002121
Iteration 234/1000 | Loss: 0.00002121
Iteration 235/1000 | Loss: 0.00002121
Iteration 236/1000 | Loss: 0.00002121
Iteration 237/1000 | Loss: 0.00002120
Iteration 238/1000 | Loss: 0.00002120
Iteration 239/1000 | Loss: 0.00002120
Iteration 240/1000 | Loss: 0.00002120
Iteration 241/1000 | Loss: 0.00002120
Iteration 242/1000 | Loss: 0.00002120
Iteration 243/1000 | Loss: 0.00002120
Iteration 244/1000 | Loss: 0.00002120
Iteration 245/1000 | Loss: 0.00002120
Iteration 246/1000 | Loss: 0.00002120
Iteration 247/1000 | Loss: 0.00002120
Iteration 248/1000 | Loss: 0.00002119
Iteration 249/1000 | Loss: 0.00002119
Iteration 250/1000 | Loss: 0.00002119
Iteration 251/1000 | Loss: 0.00002119
Iteration 252/1000 | Loss: 0.00002119
Iteration 253/1000 | Loss: 0.00002119
Iteration 254/1000 | Loss: 0.00002119
Iteration 255/1000 | Loss: 0.00002119
Iteration 256/1000 | Loss: 0.00002119
Iteration 257/1000 | Loss: 0.00002119
Iteration 258/1000 | Loss: 0.00002119
Iteration 259/1000 | Loss: 0.00002119
Iteration 260/1000 | Loss: 0.00002119
Iteration 261/1000 | Loss: 0.00002118
Iteration 262/1000 | Loss: 0.00002118
Iteration 263/1000 | Loss: 0.00002118
Iteration 264/1000 | Loss: 0.00002118
Iteration 265/1000 | Loss: 0.00002118
Iteration 266/1000 | Loss: 0.00002118
Iteration 267/1000 | Loss: 0.00002118
Iteration 268/1000 | Loss: 0.00002118
Iteration 269/1000 | Loss: 0.00002118
Iteration 270/1000 | Loss: 0.00002118
Iteration 271/1000 | Loss: 0.00002118
Iteration 272/1000 | Loss: 0.00002118
Iteration 273/1000 | Loss: 0.00002118
Iteration 274/1000 | Loss: 0.00002118
Iteration 275/1000 | Loss: 0.00002118
Iteration 276/1000 | Loss: 0.00002118
Iteration 277/1000 | Loss: 0.00002118
Iteration 278/1000 | Loss: 0.00002118
Iteration 279/1000 | Loss: 0.00002118
Iteration 280/1000 | Loss: 0.00002118
Iteration 281/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [2.118013617291581e-05, 2.118013617291581e-05, 2.118013617291581e-05, 2.118013617291581e-05, 2.118013617291581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.118013617291581e-05

Optimization complete. Final v2v error: 3.8005306720733643 mm

Highest mean error: 4.398787975311279 mm for frame 69

Lowest mean error: 2.8738880157470703 mm for frame 0

Saving results

Total time: 254.0627465248108
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389816
Iteration 2/25 | Loss: 0.00107062
Iteration 3/25 | Loss: 0.00093100
Iteration 4/25 | Loss: 0.00092117
Iteration 5/25 | Loss: 0.00091817
Iteration 6/25 | Loss: 0.00091721
Iteration 7/25 | Loss: 0.00091721
Iteration 8/25 | Loss: 0.00091721
Iteration 9/25 | Loss: 0.00091721
Iteration 10/25 | Loss: 0.00091721
Iteration 11/25 | Loss: 0.00091721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009172072168439627, 0.0009172072168439627, 0.0009172072168439627, 0.0009172072168439627, 0.0009172072168439627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009172072168439627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40944743
Iteration 2/25 | Loss: 0.00075602
Iteration 3/25 | Loss: 0.00075602
Iteration 4/25 | Loss: 0.00075602
Iteration 5/25 | Loss: 0.00075602
Iteration 6/25 | Loss: 0.00075602
Iteration 7/25 | Loss: 0.00075602
Iteration 8/25 | Loss: 0.00075602
Iteration 9/25 | Loss: 0.00075602
Iteration 10/25 | Loss: 0.00075602
Iteration 11/25 | Loss: 0.00075602
Iteration 12/25 | Loss: 0.00075602
Iteration 13/25 | Loss: 0.00075602
Iteration 14/25 | Loss: 0.00075602
Iteration 15/25 | Loss: 0.00075602
Iteration 16/25 | Loss: 0.00075602
Iteration 17/25 | Loss: 0.00075602
Iteration 18/25 | Loss: 0.00075602
Iteration 19/25 | Loss: 0.00075602
Iteration 20/25 | Loss: 0.00075602
Iteration 21/25 | Loss: 0.00075602
Iteration 22/25 | Loss: 0.00075602
Iteration 23/25 | Loss: 0.00075602
Iteration 24/25 | Loss: 0.00075602
Iteration 25/25 | Loss: 0.00075602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075602
Iteration 2/1000 | Loss: 0.00006329
Iteration 3/1000 | Loss: 0.00003224
Iteration 4/1000 | Loss: 0.00001783
Iteration 5/1000 | Loss: 0.00001471
Iteration 6/1000 | Loss: 0.00001313
Iteration 7/1000 | Loss: 0.00001234
Iteration 8/1000 | Loss: 0.00001192
Iteration 9/1000 | Loss: 0.00001158
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001122
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001116
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001100
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001098
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001097
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001093
Iteration 30/1000 | Loss: 0.00001093
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001092
Iteration 36/1000 | Loss: 0.00001091
Iteration 37/1000 | Loss: 0.00001091
Iteration 38/1000 | Loss: 0.00001090
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001090
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001089
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001086
Iteration 49/1000 | Loss: 0.00001086
Iteration 50/1000 | Loss: 0.00001085
Iteration 51/1000 | Loss: 0.00001085
Iteration 52/1000 | Loss: 0.00001084
Iteration 53/1000 | Loss: 0.00001084
Iteration 54/1000 | Loss: 0.00001084
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001078
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001077
Iteration 62/1000 | Loss: 0.00001077
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001076
Iteration 65/1000 | Loss: 0.00001076
Iteration 66/1000 | Loss: 0.00001075
Iteration 67/1000 | Loss: 0.00001075
Iteration 68/1000 | Loss: 0.00001075
Iteration 69/1000 | Loss: 0.00001075
Iteration 70/1000 | Loss: 0.00001074
Iteration 71/1000 | Loss: 0.00001074
Iteration 72/1000 | Loss: 0.00001074
Iteration 73/1000 | Loss: 0.00001073
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001073
Iteration 76/1000 | Loss: 0.00001073
Iteration 77/1000 | Loss: 0.00001073
Iteration 78/1000 | Loss: 0.00001073
Iteration 79/1000 | Loss: 0.00001073
Iteration 80/1000 | Loss: 0.00001073
Iteration 81/1000 | Loss: 0.00001073
Iteration 82/1000 | Loss: 0.00001073
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001073
Iteration 86/1000 | Loss: 0.00001073
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001072
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001072
Iteration 92/1000 | Loss: 0.00001072
Iteration 93/1000 | Loss: 0.00001072
Iteration 94/1000 | Loss: 0.00001072
Iteration 95/1000 | Loss: 0.00001072
Iteration 96/1000 | Loss: 0.00001072
Iteration 97/1000 | Loss: 0.00001072
Iteration 98/1000 | Loss: 0.00001072
Iteration 99/1000 | Loss: 0.00001071
Iteration 100/1000 | Loss: 0.00001071
Iteration 101/1000 | Loss: 0.00001071
Iteration 102/1000 | Loss: 0.00001071
Iteration 103/1000 | Loss: 0.00001071
Iteration 104/1000 | Loss: 0.00001071
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001071
Iteration 110/1000 | Loss: 0.00001071
Iteration 111/1000 | Loss: 0.00001071
Iteration 112/1000 | Loss: 0.00001071
Iteration 113/1000 | Loss: 0.00001070
Iteration 114/1000 | Loss: 0.00001070
Iteration 115/1000 | Loss: 0.00001070
Iteration 116/1000 | Loss: 0.00001070
Iteration 117/1000 | Loss: 0.00001070
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001070
Iteration 120/1000 | Loss: 0.00001070
Iteration 121/1000 | Loss: 0.00001070
Iteration 122/1000 | Loss: 0.00001070
Iteration 123/1000 | Loss: 0.00001070
Iteration 124/1000 | Loss: 0.00001070
Iteration 125/1000 | Loss: 0.00001070
Iteration 126/1000 | Loss: 0.00001070
Iteration 127/1000 | Loss: 0.00001070
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001069
Iteration 132/1000 | Loss: 0.00001069
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001069
Iteration 138/1000 | Loss: 0.00001069
Iteration 139/1000 | Loss: 0.00001069
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001069
Iteration 143/1000 | Loss: 0.00001069
Iteration 144/1000 | Loss: 0.00001068
Iteration 145/1000 | Loss: 0.00001068
Iteration 146/1000 | Loss: 0.00001068
Iteration 147/1000 | Loss: 0.00001068
Iteration 148/1000 | Loss: 0.00001068
Iteration 149/1000 | Loss: 0.00001068
Iteration 150/1000 | Loss: 0.00001068
Iteration 151/1000 | Loss: 0.00001068
Iteration 152/1000 | Loss: 0.00001068
Iteration 153/1000 | Loss: 0.00001068
Iteration 154/1000 | Loss: 0.00001068
Iteration 155/1000 | Loss: 0.00001068
Iteration 156/1000 | Loss: 0.00001068
Iteration 157/1000 | Loss: 0.00001068
Iteration 158/1000 | Loss: 0.00001068
Iteration 159/1000 | Loss: 0.00001068
Iteration 160/1000 | Loss: 0.00001068
Iteration 161/1000 | Loss: 0.00001068
Iteration 162/1000 | Loss: 0.00001068
Iteration 163/1000 | Loss: 0.00001067
Iteration 164/1000 | Loss: 0.00001067
Iteration 165/1000 | Loss: 0.00001067
Iteration 166/1000 | Loss: 0.00001067
Iteration 167/1000 | Loss: 0.00001067
Iteration 168/1000 | Loss: 0.00001067
Iteration 169/1000 | Loss: 0.00001067
Iteration 170/1000 | Loss: 0.00001067
Iteration 171/1000 | Loss: 0.00001067
Iteration 172/1000 | Loss: 0.00001067
Iteration 173/1000 | Loss: 0.00001067
Iteration 174/1000 | Loss: 0.00001067
Iteration 175/1000 | Loss: 0.00001067
Iteration 176/1000 | Loss: 0.00001067
Iteration 177/1000 | Loss: 0.00001067
Iteration 178/1000 | Loss: 0.00001067
Iteration 179/1000 | Loss: 0.00001067
Iteration 180/1000 | Loss: 0.00001066
Iteration 181/1000 | Loss: 0.00001066
Iteration 182/1000 | Loss: 0.00001066
Iteration 183/1000 | Loss: 0.00001066
Iteration 184/1000 | Loss: 0.00001066
Iteration 185/1000 | Loss: 0.00001066
Iteration 186/1000 | Loss: 0.00001066
Iteration 187/1000 | Loss: 0.00001066
Iteration 188/1000 | Loss: 0.00001066
Iteration 189/1000 | Loss: 0.00001066
Iteration 190/1000 | Loss: 0.00001066
Iteration 191/1000 | Loss: 0.00001066
Iteration 192/1000 | Loss: 0.00001066
Iteration 193/1000 | Loss: 0.00001066
Iteration 194/1000 | Loss: 0.00001066
Iteration 195/1000 | Loss: 0.00001066
Iteration 196/1000 | Loss: 0.00001066
Iteration 197/1000 | Loss: 0.00001066
Iteration 198/1000 | Loss: 0.00001066
Iteration 199/1000 | Loss: 0.00001066
Iteration 200/1000 | Loss: 0.00001066
Iteration 201/1000 | Loss: 0.00001066
Iteration 202/1000 | Loss: 0.00001066
Iteration 203/1000 | Loss: 0.00001066
Iteration 204/1000 | Loss: 0.00001066
Iteration 205/1000 | Loss: 0.00001066
Iteration 206/1000 | Loss: 0.00001066
Iteration 207/1000 | Loss: 0.00001066
Iteration 208/1000 | Loss: 0.00001066
Iteration 209/1000 | Loss: 0.00001066
Iteration 210/1000 | Loss: 0.00001066
Iteration 211/1000 | Loss: 0.00001066
Iteration 212/1000 | Loss: 0.00001066
Iteration 213/1000 | Loss: 0.00001066
Iteration 214/1000 | Loss: 0.00001066
Iteration 215/1000 | Loss: 0.00001066
Iteration 216/1000 | Loss: 0.00001066
Iteration 217/1000 | Loss: 0.00001066
Iteration 218/1000 | Loss: 0.00001066
Iteration 219/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.0658800420060288e-05, 1.0658800420060288e-05, 1.0658800420060288e-05, 1.0658800420060288e-05, 1.0658800420060288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0658800420060288e-05

Optimization complete. Final v2v error: 2.8063607215881348 mm

Highest mean error: 3.458240032196045 mm for frame 106

Lowest mean error: 2.348026990890503 mm for frame 147

Saving results

Total time: 39.71222805976868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827705
Iteration 2/25 | Loss: 0.00121609
Iteration 3/25 | Loss: 0.00098425
Iteration 4/25 | Loss: 0.00094801
Iteration 5/25 | Loss: 0.00094117
Iteration 6/25 | Loss: 0.00093839
Iteration 7/25 | Loss: 0.00093813
Iteration 8/25 | Loss: 0.00093813
Iteration 9/25 | Loss: 0.00093813
Iteration 10/25 | Loss: 0.00093813
Iteration 11/25 | Loss: 0.00093813
Iteration 12/25 | Loss: 0.00093813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009381320560351014, 0.0009381320560351014, 0.0009381320560351014, 0.0009381320560351014, 0.0009381320560351014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009381320560351014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38470030
Iteration 2/25 | Loss: 0.00087016
Iteration 3/25 | Loss: 0.00087016
Iteration 4/25 | Loss: 0.00087016
Iteration 5/25 | Loss: 0.00087016
Iteration 6/25 | Loss: 0.00087016
Iteration 7/25 | Loss: 0.00087016
Iteration 8/25 | Loss: 0.00087016
Iteration 9/25 | Loss: 0.00087016
Iteration 10/25 | Loss: 0.00087016
Iteration 11/25 | Loss: 0.00087016
Iteration 12/25 | Loss: 0.00087016
Iteration 13/25 | Loss: 0.00087016
Iteration 14/25 | Loss: 0.00087016
Iteration 15/25 | Loss: 0.00087016
Iteration 16/25 | Loss: 0.00087016
Iteration 17/25 | Loss: 0.00087016
Iteration 18/25 | Loss: 0.00087016
Iteration 19/25 | Loss: 0.00087016
Iteration 20/25 | Loss: 0.00087016
Iteration 21/25 | Loss: 0.00087016
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008701594197191298, 0.0008701594197191298, 0.0008701594197191298, 0.0008701594197191298, 0.0008701594197191298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008701594197191298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087016
Iteration 2/1000 | Loss: 0.00006537
Iteration 3/1000 | Loss: 0.00002753
Iteration 4/1000 | Loss: 0.00002076
Iteration 5/1000 | Loss: 0.00001779
Iteration 6/1000 | Loss: 0.00001608
Iteration 7/1000 | Loss: 0.00001521
Iteration 8/1000 | Loss: 0.00001464
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001394
Iteration 11/1000 | Loss: 0.00001386
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001348
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001335
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001331
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001328
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00001323
Iteration 28/1000 | Loss: 0.00001323
Iteration 29/1000 | Loss: 0.00001322
Iteration 30/1000 | Loss: 0.00001321
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001319
Iteration 35/1000 | Loss: 0.00001319
Iteration 36/1000 | Loss: 0.00001318
Iteration 37/1000 | Loss: 0.00001318
Iteration 38/1000 | Loss: 0.00001317
Iteration 39/1000 | Loss: 0.00001317
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001308
Iteration 49/1000 | Loss: 0.00001308
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001306
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001305
Iteration 58/1000 | Loss: 0.00001305
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001304
Iteration 62/1000 | Loss: 0.00001304
Iteration 63/1000 | Loss: 0.00001304
Iteration 64/1000 | Loss: 0.00001304
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001302
Iteration 68/1000 | Loss: 0.00001302
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001302
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00001301
Iteration 73/1000 | Loss: 0.00001301
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00001301
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001297
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001296
Iteration 95/1000 | Loss: 0.00001296
Iteration 96/1000 | Loss: 0.00001296
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001296
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001296
Iteration 117/1000 | Loss: 0.00001296
Iteration 118/1000 | Loss: 0.00001296
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001296
Iteration 121/1000 | Loss: 0.00001296
Iteration 122/1000 | Loss: 0.00001296
Iteration 123/1000 | Loss: 0.00001296
Iteration 124/1000 | Loss: 0.00001296
Iteration 125/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.2961467291461304e-05, 1.2961467291461304e-05, 1.2961467291461304e-05, 1.2961467291461304e-05, 1.2961467291461304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2961467291461304e-05

Optimization complete. Final v2v error: 3.0942320823669434 mm

Highest mean error: 3.662710428237915 mm for frame 45

Lowest mean error: 2.723963499069214 mm for frame 151

Saving results

Total time: 39.188515424728394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389153
Iteration 2/25 | Loss: 0.00094752
Iteration 3/25 | Loss: 0.00087127
Iteration 4/25 | Loss: 0.00086177
Iteration 5/25 | Loss: 0.00085837
Iteration 6/25 | Loss: 0.00085721
Iteration 7/25 | Loss: 0.00085719
Iteration 8/25 | Loss: 0.00085719
Iteration 9/25 | Loss: 0.00085719
Iteration 10/25 | Loss: 0.00085719
Iteration 11/25 | Loss: 0.00085719
Iteration 12/25 | Loss: 0.00085719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008571934304200113, 0.0008571934304200113, 0.0008571934304200113, 0.0008571934304200113, 0.0008571934304200113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008571934304200113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.84108305
Iteration 2/25 | Loss: 0.00057736
Iteration 3/25 | Loss: 0.00057735
Iteration 4/25 | Loss: 0.00057735
Iteration 5/25 | Loss: 0.00057735
Iteration 6/25 | Loss: 0.00057735
Iteration 7/25 | Loss: 0.00057735
Iteration 8/25 | Loss: 0.00057735
Iteration 9/25 | Loss: 0.00057735
Iteration 10/25 | Loss: 0.00057735
Iteration 11/25 | Loss: 0.00057735
Iteration 12/25 | Loss: 0.00057735
Iteration 13/25 | Loss: 0.00057735
Iteration 14/25 | Loss: 0.00057735
Iteration 15/25 | Loss: 0.00057735
Iteration 16/25 | Loss: 0.00057735
Iteration 17/25 | Loss: 0.00057735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005773474113084376, 0.0005773474113084376, 0.0005773474113084376, 0.0005773474113084376, 0.0005773474113084376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005773474113084376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057735
Iteration 2/1000 | Loss: 0.00004554
Iteration 3/1000 | Loss: 0.00001571
Iteration 4/1000 | Loss: 0.00001279
Iteration 5/1000 | Loss: 0.00001126
Iteration 6/1000 | Loss: 0.00001074
Iteration 7/1000 | Loss: 0.00001025
Iteration 8/1000 | Loss: 0.00000995
Iteration 9/1000 | Loss: 0.00000989
Iteration 10/1000 | Loss: 0.00000969
Iteration 11/1000 | Loss: 0.00000968
Iteration 12/1000 | Loss: 0.00000962
Iteration 13/1000 | Loss: 0.00000961
Iteration 14/1000 | Loss: 0.00000952
Iteration 15/1000 | Loss: 0.00000944
Iteration 16/1000 | Loss: 0.00000943
Iteration 17/1000 | Loss: 0.00000942
Iteration 18/1000 | Loss: 0.00000941
Iteration 19/1000 | Loss: 0.00000937
Iteration 20/1000 | Loss: 0.00000936
Iteration 21/1000 | Loss: 0.00000936
Iteration 22/1000 | Loss: 0.00000936
Iteration 23/1000 | Loss: 0.00000935
Iteration 24/1000 | Loss: 0.00000935
Iteration 25/1000 | Loss: 0.00000935
Iteration 26/1000 | Loss: 0.00000934
Iteration 27/1000 | Loss: 0.00000933
Iteration 28/1000 | Loss: 0.00000932
Iteration 29/1000 | Loss: 0.00000932
Iteration 30/1000 | Loss: 0.00000931
Iteration 31/1000 | Loss: 0.00000931
Iteration 32/1000 | Loss: 0.00000931
Iteration 33/1000 | Loss: 0.00000930
Iteration 34/1000 | Loss: 0.00000930
Iteration 35/1000 | Loss: 0.00000930
Iteration 36/1000 | Loss: 0.00000930
Iteration 37/1000 | Loss: 0.00000930
Iteration 38/1000 | Loss: 0.00000930
Iteration 39/1000 | Loss: 0.00000930
Iteration 40/1000 | Loss: 0.00000930
Iteration 41/1000 | Loss: 0.00000930
Iteration 42/1000 | Loss: 0.00000930
Iteration 43/1000 | Loss: 0.00000929
Iteration 44/1000 | Loss: 0.00000929
Iteration 45/1000 | Loss: 0.00000927
Iteration 46/1000 | Loss: 0.00000927
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000927
Iteration 49/1000 | Loss: 0.00000927
Iteration 50/1000 | Loss: 0.00000927
Iteration 51/1000 | Loss: 0.00000927
Iteration 52/1000 | Loss: 0.00000927
Iteration 53/1000 | Loss: 0.00000927
Iteration 54/1000 | Loss: 0.00000927
Iteration 55/1000 | Loss: 0.00000927
Iteration 56/1000 | Loss: 0.00000926
Iteration 57/1000 | Loss: 0.00000926
Iteration 58/1000 | Loss: 0.00000925
Iteration 59/1000 | Loss: 0.00000925
Iteration 60/1000 | Loss: 0.00000925
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000925
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000924
Iteration 65/1000 | Loss: 0.00000924
Iteration 66/1000 | Loss: 0.00000924
Iteration 67/1000 | Loss: 0.00000924
Iteration 68/1000 | Loss: 0.00000924
Iteration 69/1000 | Loss: 0.00000924
Iteration 70/1000 | Loss: 0.00000924
Iteration 71/1000 | Loss: 0.00000924
Iteration 72/1000 | Loss: 0.00000924
Iteration 73/1000 | Loss: 0.00000923
Iteration 74/1000 | Loss: 0.00000923
Iteration 75/1000 | Loss: 0.00000923
Iteration 76/1000 | Loss: 0.00000923
Iteration 77/1000 | Loss: 0.00000923
Iteration 78/1000 | Loss: 0.00000923
Iteration 79/1000 | Loss: 0.00000922
Iteration 80/1000 | Loss: 0.00000922
Iteration 81/1000 | Loss: 0.00000922
Iteration 82/1000 | Loss: 0.00000922
Iteration 83/1000 | Loss: 0.00000921
Iteration 84/1000 | Loss: 0.00000921
Iteration 85/1000 | Loss: 0.00000921
Iteration 86/1000 | Loss: 0.00000921
Iteration 87/1000 | Loss: 0.00000921
Iteration 88/1000 | Loss: 0.00000921
Iteration 89/1000 | Loss: 0.00000921
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000920
Iteration 94/1000 | Loss: 0.00000920
Iteration 95/1000 | Loss: 0.00000920
Iteration 96/1000 | Loss: 0.00000920
Iteration 97/1000 | Loss: 0.00000920
Iteration 98/1000 | Loss: 0.00000920
Iteration 99/1000 | Loss: 0.00000920
Iteration 100/1000 | Loss: 0.00000920
Iteration 101/1000 | Loss: 0.00000920
Iteration 102/1000 | Loss: 0.00000920
Iteration 103/1000 | Loss: 0.00000919
Iteration 104/1000 | Loss: 0.00000919
Iteration 105/1000 | Loss: 0.00000919
Iteration 106/1000 | Loss: 0.00000919
Iteration 107/1000 | Loss: 0.00000919
Iteration 108/1000 | Loss: 0.00000919
Iteration 109/1000 | Loss: 0.00000919
Iteration 110/1000 | Loss: 0.00000919
Iteration 111/1000 | Loss: 0.00000919
Iteration 112/1000 | Loss: 0.00000919
Iteration 113/1000 | Loss: 0.00000919
Iteration 114/1000 | Loss: 0.00000919
Iteration 115/1000 | Loss: 0.00000919
Iteration 116/1000 | Loss: 0.00000918
Iteration 117/1000 | Loss: 0.00000918
Iteration 118/1000 | Loss: 0.00000918
Iteration 119/1000 | Loss: 0.00000918
Iteration 120/1000 | Loss: 0.00000918
Iteration 121/1000 | Loss: 0.00000918
Iteration 122/1000 | Loss: 0.00000918
Iteration 123/1000 | Loss: 0.00000918
Iteration 124/1000 | Loss: 0.00000918
Iteration 125/1000 | Loss: 0.00000918
Iteration 126/1000 | Loss: 0.00000918
Iteration 127/1000 | Loss: 0.00000918
Iteration 128/1000 | Loss: 0.00000918
Iteration 129/1000 | Loss: 0.00000918
Iteration 130/1000 | Loss: 0.00000918
Iteration 131/1000 | Loss: 0.00000918
Iteration 132/1000 | Loss: 0.00000918
Iteration 133/1000 | Loss: 0.00000918
Iteration 134/1000 | Loss: 0.00000918
Iteration 135/1000 | Loss: 0.00000918
Iteration 136/1000 | Loss: 0.00000918
Iteration 137/1000 | Loss: 0.00000918
Iteration 138/1000 | Loss: 0.00000918
Iteration 139/1000 | Loss: 0.00000918
Iteration 140/1000 | Loss: 0.00000918
Iteration 141/1000 | Loss: 0.00000918
Iteration 142/1000 | Loss: 0.00000918
Iteration 143/1000 | Loss: 0.00000918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [9.179424523608759e-06, 9.179424523608759e-06, 9.179424523608759e-06, 9.179424523608759e-06, 9.179424523608759e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.179424523608759e-06

Optimization complete. Final v2v error: 2.5950961112976074 mm

Highest mean error: 2.9025633335113525 mm for frame 94

Lowest mean error: 2.2503433227539062 mm for frame 0

Saving results

Total time: 33.16732382774353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01144203
Iteration 2/25 | Loss: 0.00177599
Iteration 3/25 | Loss: 0.00116509
Iteration 4/25 | Loss: 0.00112789
Iteration 5/25 | Loss: 0.00111927
Iteration 6/25 | Loss: 0.00111655
Iteration 7/25 | Loss: 0.00111655
Iteration 8/25 | Loss: 0.00111655
Iteration 9/25 | Loss: 0.00111655
Iteration 10/25 | Loss: 0.00111655
Iteration 11/25 | Loss: 0.00111655
Iteration 12/25 | Loss: 0.00111655
Iteration 13/25 | Loss: 0.00111655
Iteration 14/25 | Loss: 0.00111655
Iteration 15/25 | Loss: 0.00111655
Iteration 16/25 | Loss: 0.00111655
Iteration 17/25 | Loss: 0.00111655
Iteration 18/25 | Loss: 0.00111655
Iteration 19/25 | Loss: 0.00111655
Iteration 20/25 | Loss: 0.00111655
Iteration 21/25 | Loss: 0.00111655
Iteration 22/25 | Loss: 0.00111655
Iteration 23/25 | Loss: 0.00111655
Iteration 24/25 | Loss: 0.00111655
Iteration 25/25 | Loss: 0.00111655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97287750
Iteration 2/25 | Loss: 0.00070465
Iteration 3/25 | Loss: 0.00070461
Iteration 4/25 | Loss: 0.00070461
Iteration 5/25 | Loss: 0.00070461
Iteration 6/25 | Loss: 0.00070461
Iteration 7/25 | Loss: 0.00070461
Iteration 8/25 | Loss: 0.00070461
Iteration 9/25 | Loss: 0.00070461
Iteration 10/25 | Loss: 0.00070461
Iteration 11/25 | Loss: 0.00070461
Iteration 12/25 | Loss: 0.00070461
Iteration 13/25 | Loss: 0.00070461
Iteration 14/25 | Loss: 0.00070461
Iteration 15/25 | Loss: 0.00070461
Iteration 16/25 | Loss: 0.00070461
Iteration 17/25 | Loss: 0.00070461
Iteration 18/25 | Loss: 0.00070461
Iteration 19/25 | Loss: 0.00070461
Iteration 20/25 | Loss: 0.00070461
Iteration 21/25 | Loss: 0.00070461
Iteration 22/25 | Loss: 0.00070461
Iteration 23/25 | Loss: 0.00070461
Iteration 24/25 | Loss: 0.00070461
Iteration 25/25 | Loss: 0.00070461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070461
Iteration 2/1000 | Loss: 0.00006389
Iteration 3/1000 | Loss: 0.00004018
Iteration 4/1000 | Loss: 0.00003407
Iteration 5/1000 | Loss: 0.00003237
Iteration 6/1000 | Loss: 0.00003109
Iteration 7/1000 | Loss: 0.00002988
Iteration 8/1000 | Loss: 0.00002937
Iteration 9/1000 | Loss: 0.00002882
Iteration 10/1000 | Loss: 0.00002837
Iteration 11/1000 | Loss: 0.00002804
Iteration 12/1000 | Loss: 0.00002779
Iteration 13/1000 | Loss: 0.00002760
Iteration 14/1000 | Loss: 0.00002745
Iteration 15/1000 | Loss: 0.00002729
Iteration 16/1000 | Loss: 0.00002722
Iteration 17/1000 | Loss: 0.00002719
Iteration 18/1000 | Loss: 0.00002719
Iteration 19/1000 | Loss: 0.00002707
Iteration 20/1000 | Loss: 0.00002703
Iteration 21/1000 | Loss: 0.00002701
Iteration 22/1000 | Loss: 0.00002699
Iteration 23/1000 | Loss: 0.00002699
Iteration 24/1000 | Loss: 0.00002699
Iteration 25/1000 | Loss: 0.00002699
Iteration 26/1000 | Loss: 0.00002699
Iteration 27/1000 | Loss: 0.00002699
Iteration 28/1000 | Loss: 0.00002699
Iteration 29/1000 | Loss: 0.00002699
Iteration 30/1000 | Loss: 0.00002699
Iteration 31/1000 | Loss: 0.00002699
Iteration 32/1000 | Loss: 0.00002699
Iteration 33/1000 | Loss: 0.00002699
Iteration 34/1000 | Loss: 0.00002698
Iteration 35/1000 | Loss: 0.00002698
Iteration 36/1000 | Loss: 0.00002698
Iteration 37/1000 | Loss: 0.00002698
Iteration 38/1000 | Loss: 0.00002698
Iteration 39/1000 | Loss: 0.00002698
Iteration 40/1000 | Loss: 0.00002698
Iteration 41/1000 | Loss: 0.00002698
Iteration 42/1000 | Loss: 0.00002698
Iteration 43/1000 | Loss: 0.00002697
Iteration 44/1000 | Loss: 0.00002695
Iteration 45/1000 | Loss: 0.00002694
Iteration 46/1000 | Loss: 0.00002693
Iteration 47/1000 | Loss: 0.00002693
Iteration 48/1000 | Loss: 0.00002691
Iteration 49/1000 | Loss: 0.00002691
Iteration 50/1000 | Loss: 0.00002690
Iteration 51/1000 | Loss: 0.00002690
Iteration 52/1000 | Loss: 0.00002690
Iteration 53/1000 | Loss: 0.00002690
Iteration 54/1000 | Loss: 0.00002690
Iteration 55/1000 | Loss: 0.00002690
Iteration 56/1000 | Loss: 0.00002688
Iteration 57/1000 | Loss: 0.00002687
Iteration 58/1000 | Loss: 0.00002687
Iteration 59/1000 | Loss: 0.00002687
Iteration 60/1000 | Loss: 0.00002686
Iteration 61/1000 | Loss: 0.00002686
Iteration 62/1000 | Loss: 0.00002685
Iteration 63/1000 | Loss: 0.00002684
Iteration 64/1000 | Loss: 0.00002684
Iteration 65/1000 | Loss: 0.00002683
Iteration 66/1000 | Loss: 0.00002683
Iteration 67/1000 | Loss: 0.00002683
Iteration 68/1000 | Loss: 0.00002682
Iteration 69/1000 | Loss: 0.00002682
Iteration 70/1000 | Loss: 0.00002682
Iteration 71/1000 | Loss: 0.00002682
Iteration 72/1000 | Loss: 0.00002681
Iteration 73/1000 | Loss: 0.00002681
Iteration 74/1000 | Loss: 0.00002681
Iteration 75/1000 | Loss: 0.00002680
Iteration 76/1000 | Loss: 0.00002680
Iteration 77/1000 | Loss: 0.00002680
Iteration 78/1000 | Loss: 0.00002680
Iteration 79/1000 | Loss: 0.00002680
Iteration 80/1000 | Loss: 0.00002679
Iteration 81/1000 | Loss: 0.00002679
Iteration 82/1000 | Loss: 0.00002679
Iteration 83/1000 | Loss: 0.00002679
Iteration 84/1000 | Loss: 0.00002678
Iteration 85/1000 | Loss: 0.00002678
Iteration 86/1000 | Loss: 0.00002678
Iteration 87/1000 | Loss: 0.00002676
Iteration 88/1000 | Loss: 0.00002676
Iteration 89/1000 | Loss: 0.00002676
Iteration 90/1000 | Loss: 0.00002676
Iteration 91/1000 | Loss: 0.00002676
Iteration 92/1000 | Loss: 0.00002676
Iteration 93/1000 | Loss: 0.00002675
Iteration 94/1000 | Loss: 0.00002675
Iteration 95/1000 | Loss: 0.00002675
Iteration 96/1000 | Loss: 0.00002675
Iteration 97/1000 | Loss: 0.00002674
Iteration 98/1000 | Loss: 0.00002674
Iteration 99/1000 | Loss: 0.00002674
Iteration 100/1000 | Loss: 0.00002674
Iteration 101/1000 | Loss: 0.00002673
Iteration 102/1000 | Loss: 0.00002673
Iteration 103/1000 | Loss: 0.00002673
Iteration 104/1000 | Loss: 0.00002673
Iteration 105/1000 | Loss: 0.00002673
Iteration 106/1000 | Loss: 0.00002673
Iteration 107/1000 | Loss: 0.00002673
Iteration 108/1000 | Loss: 0.00002673
Iteration 109/1000 | Loss: 0.00002673
Iteration 110/1000 | Loss: 0.00002673
Iteration 111/1000 | Loss: 0.00002672
Iteration 112/1000 | Loss: 0.00002672
Iteration 113/1000 | Loss: 0.00002672
Iteration 114/1000 | Loss: 0.00002672
Iteration 115/1000 | Loss: 0.00002671
Iteration 116/1000 | Loss: 0.00002671
Iteration 117/1000 | Loss: 0.00002671
Iteration 118/1000 | Loss: 0.00002671
Iteration 119/1000 | Loss: 0.00002670
Iteration 120/1000 | Loss: 0.00002670
Iteration 121/1000 | Loss: 0.00002670
Iteration 122/1000 | Loss: 0.00002670
Iteration 123/1000 | Loss: 0.00002670
Iteration 124/1000 | Loss: 0.00002670
Iteration 125/1000 | Loss: 0.00002670
Iteration 126/1000 | Loss: 0.00002670
Iteration 127/1000 | Loss: 0.00002670
Iteration 128/1000 | Loss: 0.00002669
Iteration 129/1000 | Loss: 0.00002669
Iteration 130/1000 | Loss: 0.00002669
Iteration 131/1000 | Loss: 0.00002669
Iteration 132/1000 | Loss: 0.00002669
Iteration 133/1000 | Loss: 0.00002669
Iteration 134/1000 | Loss: 0.00002669
Iteration 135/1000 | Loss: 0.00002669
Iteration 136/1000 | Loss: 0.00002669
Iteration 137/1000 | Loss: 0.00002669
Iteration 138/1000 | Loss: 0.00002668
Iteration 139/1000 | Loss: 0.00002668
Iteration 140/1000 | Loss: 0.00002668
Iteration 141/1000 | Loss: 0.00002668
Iteration 142/1000 | Loss: 0.00002668
Iteration 143/1000 | Loss: 0.00002668
Iteration 144/1000 | Loss: 0.00002668
Iteration 145/1000 | Loss: 0.00002668
Iteration 146/1000 | Loss: 0.00002668
Iteration 147/1000 | Loss: 0.00002668
Iteration 148/1000 | Loss: 0.00002668
Iteration 149/1000 | Loss: 0.00002667
Iteration 150/1000 | Loss: 0.00002667
Iteration 151/1000 | Loss: 0.00002667
Iteration 152/1000 | Loss: 0.00002667
Iteration 153/1000 | Loss: 0.00002667
Iteration 154/1000 | Loss: 0.00002667
Iteration 155/1000 | Loss: 0.00002667
Iteration 156/1000 | Loss: 0.00002667
Iteration 157/1000 | Loss: 0.00002667
Iteration 158/1000 | Loss: 0.00002667
Iteration 159/1000 | Loss: 0.00002667
Iteration 160/1000 | Loss: 0.00002667
Iteration 161/1000 | Loss: 0.00002666
Iteration 162/1000 | Loss: 0.00002666
Iteration 163/1000 | Loss: 0.00002666
Iteration 164/1000 | Loss: 0.00002666
Iteration 165/1000 | Loss: 0.00002666
Iteration 166/1000 | Loss: 0.00002666
Iteration 167/1000 | Loss: 0.00002666
Iteration 168/1000 | Loss: 0.00002666
Iteration 169/1000 | Loss: 0.00002666
Iteration 170/1000 | Loss: 0.00002666
Iteration 171/1000 | Loss: 0.00002666
Iteration 172/1000 | Loss: 0.00002666
Iteration 173/1000 | Loss: 0.00002666
Iteration 174/1000 | Loss: 0.00002666
Iteration 175/1000 | Loss: 0.00002666
Iteration 176/1000 | Loss: 0.00002666
Iteration 177/1000 | Loss: 0.00002666
Iteration 178/1000 | Loss: 0.00002665
Iteration 179/1000 | Loss: 0.00002665
Iteration 180/1000 | Loss: 0.00002665
Iteration 181/1000 | Loss: 0.00002665
Iteration 182/1000 | Loss: 0.00002665
Iteration 183/1000 | Loss: 0.00002665
Iteration 184/1000 | Loss: 0.00002665
Iteration 185/1000 | Loss: 0.00002665
Iteration 186/1000 | Loss: 0.00002665
Iteration 187/1000 | Loss: 0.00002665
Iteration 188/1000 | Loss: 0.00002665
Iteration 189/1000 | Loss: 0.00002665
Iteration 190/1000 | Loss: 0.00002665
Iteration 191/1000 | Loss: 0.00002665
Iteration 192/1000 | Loss: 0.00002665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.6649402570910752e-05, 2.6649402570910752e-05, 2.6649402570910752e-05, 2.6649402570910752e-05, 2.6649402570910752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6649402570910752e-05

Optimization complete. Final v2v error: 4.144626140594482 mm

Highest mean error: 5.549792289733887 mm for frame 73

Lowest mean error: 3.334695816040039 mm for frame 33

Saving results

Total time: 54.86199712753296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891350
Iteration 2/25 | Loss: 0.00115785
Iteration 3/25 | Loss: 0.00095325
Iteration 4/25 | Loss: 0.00093806
Iteration 5/25 | Loss: 0.00093454
Iteration 6/25 | Loss: 0.00093333
Iteration 7/25 | Loss: 0.00093330
Iteration 8/25 | Loss: 0.00093330
Iteration 9/25 | Loss: 0.00093330
Iteration 10/25 | Loss: 0.00093330
Iteration 11/25 | Loss: 0.00093330
Iteration 12/25 | Loss: 0.00093330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009332950576208532, 0.0009332950576208532, 0.0009332950576208532, 0.0009332950576208532, 0.0009332950576208532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009332950576208532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44799340
Iteration 2/25 | Loss: 0.00055190
Iteration 3/25 | Loss: 0.00055190
Iteration 4/25 | Loss: 0.00055190
Iteration 5/25 | Loss: 0.00055190
Iteration 6/25 | Loss: 0.00055190
Iteration 7/25 | Loss: 0.00055190
Iteration 8/25 | Loss: 0.00055190
Iteration 9/25 | Loss: 0.00055190
Iteration 10/25 | Loss: 0.00055190
Iteration 11/25 | Loss: 0.00055190
Iteration 12/25 | Loss: 0.00055190
Iteration 13/25 | Loss: 0.00055190
Iteration 14/25 | Loss: 0.00055190
Iteration 15/25 | Loss: 0.00055190
Iteration 16/25 | Loss: 0.00055190
Iteration 17/25 | Loss: 0.00055190
Iteration 18/25 | Loss: 0.00055190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005518994876183569, 0.0005518994876183569, 0.0005518994876183569, 0.0005518994876183569, 0.0005518994876183569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005518994876183569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055190
Iteration 2/1000 | Loss: 0.00006538
Iteration 3/1000 | Loss: 0.00003048
Iteration 4/1000 | Loss: 0.00002266
Iteration 5/1000 | Loss: 0.00002004
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001839
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001673
Iteration 12/1000 | Loss: 0.00001651
Iteration 13/1000 | Loss: 0.00001635
Iteration 14/1000 | Loss: 0.00001633
Iteration 15/1000 | Loss: 0.00001632
Iteration 16/1000 | Loss: 0.00001626
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001624
Iteration 19/1000 | Loss: 0.00001624
Iteration 20/1000 | Loss: 0.00001624
Iteration 21/1000 | Loss: 0.00001624
Iteration 22/1000 | Loss: 0.00001623
Iteration 23/1000 | Loss: 0.00001623
Iteration 24/1000 | Loss: 0.00001621
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001619
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001615
Iteration 37/1000 | Loss: 0.00001615
Iteration 38/1000 | Loss: 0.00001615
Iteration 39/1000 | Loss: 0.00001614
Iteration 40/1000 | Loss: 0.00001614
Iteration 41/1000 | Loss: 0.00001614
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001612
Iteration 50/1000 | Loss: 0.00001612
Iteration 51/1000 | Loss: 0.00001612
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001612
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001611
Iteration 57/1000 | Loss: 0.00001611
Iteration 58/1000 | Loss: 0.00001611
Iteration 59/1000 | Loss: 0.00001611
Iteration 60/1000 | Loss: 0.00001610
Iteration 61/1000 | Loss: 0.00001610
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001610
Iteration 64/1000 | Loss: 0.00001610
Iteration 65/1000 | Loss: 0.00001610
Iteration 66/1000 | Loss: 0.00001610
Iteration 67/1000 | Loss: 0.00001610
Iteration 68/1000 | Loss: 0.00001610
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.610005165275652e-05, 1.610005165275652e-05, 1.610005165275652e-05, 1.610005165275652e-05, 1.610005165275652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.610005165275652e-05

Optimization complete. Final v2v error: 3.246333599090576 mm

Highest mean error: 4.795319080352783 mm for frame 60

Lowest mean error: 2.6950299739837646 mm for frame 152

Saving results

Total time: 32.49744439125061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815899
Iteration 2/25 | Loss: 0.00101270
Iteration 3/25 | Loss: 0.00090072
Iteration 4/25 | Loss: 0.00089034
Iteration 5/25 | Loss: 0.00088773
Iteration 6/25 | Loss: 0.00088746
Iteration 7/25 | Loss: 0.00088746
Iteration 8/25 | Loss: 0.00088746
Iteration 9/25 | Loss: 0.00088746
Iteration 10/25 | Loss: 0.00088746
Iteration 11/25 | Loss: 0.00088746
Iteration 12/25 | Loss: 0.00088746
Iteration 13/25 | Loss: 0.00088746
Iteration 14/25 | Loss: 0.00088746
Iteration 15/25 | Loss: 0.00088746
Iteration 16/25 | Loss: 0.00088746
Iteration 17/25 | Loss: 0.00088746
Iteration 18/25 | Loss: 0.00088746
Iteration 19/25 | Loss: 0.00088746
Iteration 20/25 | Loss: 0.00088746
Iteration 21/25 | Loss: 0.00088746
Iteration 22/25 | Loss: 0.00088746
Iteration 23/25 | Loss: 0.00088746
Iteration 24/25 | Loss: 0.00088746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008874601917341352, 0.0008874601917341352, 0.0008874601917341352, 0.0008874601917341352, 0.0008874601917341352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008874601917341352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37073350
Iteration 2/25 | Loss: 0.00054826
Iteration 3/25 | Loss: 0.00054826
Iteration 4/25 | Loss: 0.00054826
Iteration 5/25 | Loss: 0.00054826
Iteration 6/25 | Loss: 0.00054826
Iteration 7/25 | Loss: 0.00054826
Iteration 8/25 | Loss: 0.00054826
Iteration 9/25 | Loss: 0.00054826
Iteration 10/25 | Loss: 0.00054826
Iteration 11/25 | Loss: 0.00054826
Iteration 12/25 | Loss: 0.00054826
Iteration 13/25 | Loss: 0.00054826
Iteration 14/25 | Loss: 0.00054826
Iteration 15/25 | Loss: 0.00054826
Iteration 16/25 | Loss: 0.00054826
Iteration 17/25 | Loss: 0.00054826
Iteration 18/25 | Loss: 0.00054826
Iteration 19/25 | Loss: 0.00054826
Iteration 20/25 | Loss: 0.00054826
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005482558044604957, 0.0005482558044604957, 0.0005482558044604957, 0.0005482558044604957, 0.0005482558044604957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005482558044604957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054826
Iteration 2/1000 | Loss: 0.00006361
Iteration 3/1000 | Loss: 0.00002144
Iteration 4/1000 | Loss: 0.00001576
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001265
Iteration 7/1000 | Loss: 0.00001217
Iteration 8/1000 | Loss: 0.00001160
Iteration 9/1000 | Loss: 0.00001125
Iteration 10/1000 | Loss: 0.00001100
Iteration 11/1000 | Loss: 0.00001081
Iteration 12/1000 | Loss: 0.00001061
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001054
Iteration 15/1000 | Loss: 0.00001054
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001050
Iteration 18/1000 | Loss: 0.00001048
Iteration 19/1000 | Loss: 0.00001047
Iteration 20/1000 | Loss: 0.00001047
Iteration 21/1000 | Loss: 0.00001046
Iteration 22/1000 | Loss: 0.00001045
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001043
Iteration 26/1000 | Loss: 0.00001042
Iteration 27/1000 | Loss: 0.00001042
Iteration 28/1000 | Loss: 0.00001041
Iteration 29/1000 | Loss: 0.00001041
Iteration 30/1000 | Loss: 0.00001041
Iteration 31/1000 | Loss: 0.00001040
Iteration 32/1000 | Loss: 0.00001040
Iteration 33/1000 | Loss: 0.00001040
Iteration 34/1000 | Loss: 0.00001040
Iteration 35/1000 | Loss: 0.00001039
Iteration 36/1000 | Loss: 0.00001039
Iteration 37/1000 | Loss: 0.00001038
Iteration 38/1000 | Loss: 0.00001038
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001037
Iteration 41/1000 | Loss: 0.00001037
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001036
Iteration 44/1000 | Loss: 0.00001036
Iteration 45/1000 | Loss: 0.00001036
Iteration 46/1000 | Loss: 0.00001035
Iteration 47/1000 | Loss: 0.00001035
Iteration 48/1000 | Loss: 0.00001034
Iteration 49/1000 | Loss: 0.00001034
Iteration 50/1000 | Loss: 0.00001034
Iteration 51/1000 | Loss: 0.00001034
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001033
Iteration 57/1000 | Loss: 0.00001033
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001033
Iteration 60/1000 | Loss: 0.00001033
Iteration 61/1000 | Loss: 0.00001033
Iteration 62/1000 | Loss: 0.00001033
Iteration 63/1000 | Loss: 0.00001033
Iteration 64/1000 | Loss: 0.00001032
Iteration 65/1000 | Loss: 0.00001032
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001032
Iteration 68/1000 | Loss: 0.00001032
Iteration 69/1000 | Loss: 0.00001032
Iteration 70/1000 | Loss: 0.00001032
Iteration 71/1000 | Loss: 0.00001032
Iteration 72/1000 | Loss: 0.00001031
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001031
Iteration 75/1000 | Loss: 0.00001031
Iteration 76/1000 | Loss: 0.00001030
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001030
Iteration 81/1000 | Loss: 0.00001030
Iteration 82/1000 | Loss: 0.00001030
Iteration 83/1000 | Loss: 0.00001030
Iteration 84/1000 | Loss: 0.00001029
Iteration 85/1000 | Loss: 0.00001029
Iteration 86/1000 | Loss: 0.00001029
Iteration 87/1000 | Loss: 0.00001029
Iteration 88/1000 | Loss: 0.00001029
Iteration 89/1000 | Loss: 0.00001029
Iteration 90/1000 | Loss: 0.00001029
Iteration 91/1000 | Loss: 0.00001029
Iteration 92/1000 | Loss: 0.00001029
Iteration 93/1000 | Loss: 0.00001028
Iteration 94/1000 | Loss: 0.00001028
Iteration 95/1000 | Loss: 0.00001028
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001028
Iteration 98/1000 | Loss: 0.00001028
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001028
Iteration 101/1000 | Loss: 0.00001027
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001026
Iteration 107/1000 | Loss: 0.00001026
Iteration 108/1000 | Loss: 0.00001026
Iteration 109/1000 | Loss: 0.00001026
Iteration 110/1000 | Loss: 0.00001026
Iteration 111/1000 | Loss: 0.00001026
Iteration 112/1000 | Loss: 0.00001026
Iteration 113/1000 | Loss: 0.00001026
Iteration 114/1000 | Loss: 0.00001026
Iteration 115/1000 | Loss: 0.00001026
Iteration 116/1000 | Loss: 0.00001026
Iteration 117/1000 | Loss: 0.00001026
Iteration 118/1000 | Loss: 0.00001026
Iteration 119/1000 | Loss: 0.00001026
Iteration 120/1000 | Loss: 0.00001026
Iteration 121/1000 | Loss: 0.00001026
Iteration 122/1000 | Loss: 0.00001026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.0259893315378577e-05, 1.0259893315378577e-05, 1.0259893315378577e-05, 1.0259893315378577e-05, 1.0259893315378577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0259893315378577e-05

Optimization complete. Final v2v error: 2.77274489402771 mm

Highest mean error: 3.009667158126831 mm for frame 88

Lowest mean error: 2.4477853775024414 mm for frame 15

Saving results

Total time: 33.00950241088867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00672683
Iteration 2/25 | Loss: 0.00110746
Iteration 3/25 | Loss: 0.00096894
Iteration 4/25 | Loss: 0.00094177
Iteration 5/25 | Loss: 0.00093431
Iteration 6/25 | Loss: 0.00093268
Iteration 7/25 | Loss: 0.00093268
Iteration 8/25 | Loss: 0.00093268
Iteration 9/25 | Loss: 0.00093268
Iteration 10/25 | Loss: 0.00093268
Iteration 11/25 | Loss: 0.00093268
Iteration 12/25 | Loss: 0.00093268
Iteration 13/25 | Loss: 0.00093268
Iteration 14/25 | Loss: 0.00093268
Iteration 15/25 | Loss: 0.00093268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009326792205683887, 0.0009326792205683887, 0.0009326792205683887, 0.0009326792205683887, 0.0009326792205683887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009326792205683887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35942090
Iteration 2/25 | Loss: 0.00075007
Iteration 3/25 | Loss: 0.00075006
Iteration 4/25 | Loss: 0.00075006
Iteration 5/25 | Loss: 0.00075006
Iteration 6/25 | Loss: 0.00075006
Iteration 7/25 | Loss: 0.00075006
Iteration 8/25 | Loss: 0.00075006
Iteration 9/25 | Loss: 0.00075006
Iteration 10/25 | Loss: 0.00075006
Iteration 11/25 | Loss: 0.00075006
Iteration 12/25 | Loss: 0.00075006
Iteration 13/25 | Loss: 0.00075006
Iteration 14/25 | Loss: 0.00075006
Iteration 15/25 | Loss: 0.00075006
Iteration 16/25 | Loss: 0.00075006
Iteration 17/25 | Loss: 0.00075006
Iteration 18/25 | Loss: 0.00075006
Iteration 19/25 | Loss: 0.00075006
Iteration 20/25 | Loss: 0.00075006
Iteration 21/25 | Loss: 0.00075006
Iteration 22/25 | Loss: 0.00075006
Iteration 23/25 | Loss: 0.00075006
Iteration 24/25 | Loss: 0.00075006
Iteration 25/25 | Loss: 0.00075006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075006
Iteration 2/1000 | Loss: 0.00006809
Iteration 3/1000 | Loss: 0.00002465
Iteration 4/1000 | Loss: 0.00001909
Iteration 5/1000 | Loss: 0.00001779
Iteration 6/1000 | Loss: 0.00001699
Iteration 7/1000 | Loss: 0.00001658
Iteration 8/1000 | Loss: 0.00001642
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001591
Iteration 11/1000 | Loss: 0.00001566
Iteration 12/1000 | Loss: 0.00001561
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001559
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001556
Iteration 17/1000 | Loss: 0.00001552
Iteration 18/1000 | Loss: 0.00001548
Iteration 19/1000 | Loss: 0.00001548
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001548
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001547
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001546
Iteration 27/1000 | Loss: 0.00001546
Iteration 28/1000 | Loss: 0.00001546
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001544
Iteration 36/1000 | Loss: 0.00001544
Iteration 37/1000 | Loss: 0.00001544
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00001544
Iteration 40/1000 | Loss: 0.00001543
Iteration 41/1000 | Loss: 0.00001543
Iteration 42/1000 | Loss: 0.00001543
Iteration 43/1000 | Loss: 0.00001543
Iteration 44/1000 | Loss: 0.00001543
Iteration 45/1000 | Loss: 0.00001543
Iteration 46/1000 | Loss: 0.00001542
Iteration 47/1000 | Loss: 0.00001542
Iteration 48/1000 | Loss: 0.00001542
Iteration 49/1000 | Loss: 0.00001542
Iteration 50/1000 | Loss: 0.00001542
Iteration 51/1000 | Loss: 0.00001542
Iteration 52/1000 | Loss: 0.00001542
Iteration 53/1000 | Loss: 0.00001541
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001540
Iteration 61/1000 | Loss: 0.00001540
Iteration 62/1000 | Loss: 0.00001540
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001539
Iteration 66/1000 | Loss: 0.00001539
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001538
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001537
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001537
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.536517811473459e-05, 1.536517811473459e-05, 1.536517811473459e-05, 1.536517811473459e-05, 1.536517811473459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536517811473459e-05

Optimization complete. Final v2v error: 3.3567888736724854 mm

Highest mean error: 3.5415453910827637 mm for frame 98

Lowest mean error: 3.095521926879883 mm for frame 236

Saving results

Total time: 36.51345443725586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872240
Iteration 2/25 | Loss: 0.00135471
Iteration 3/25 | Loss: 0.00098524
Iteration 4/25 | Loss: 0.00092337
Iteration 5/25 | Loss: 0.00091409
Iteration 6/25 | Loss: 0.00091261
Iteration 7/25 | Loss: 0.00091226
Iteration 8/25 | Loss: 0.00091217
Iteration 9/25 | Loss: 0.00091217
Iteration 10/25 | Loss: 0.00091217
Iteration 11/25 | Loss: 0.00091217
Iteration 12/25 | Loss: 0.00091217
Iteration 13/25 | Loss: 0.00091217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009121735347434878, 0.0009121735347434878, 0.0009121735347434878, 0.0009121735347434878, 0.0009121735347434878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009121735347434878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32329369
Iteration 2/25 | Loss: 0.00050110
Iteration 3/25 | Loss: 0.00050109
Iteration 4/25 | Loss: 0.00050109
Iteration 5/25 | Loss: 0.00050109
Iteration 6/25 | Loss: 0.00050109
Iteration 7/25 | Loss: 0.00050109
Iteration 8/25 | Loss: 0.00050109
Iteration 9/25 | Loss: 0.00050109
Iteration 10/25 | Loss: 0.00050108
Iteration 11/25 | Loss: 0.00050108
Iteration 12/25 | Loss: 0.00050108
Iteration 13/25 | Loss: 0.00050108
Iteration 14/25 | Loss: 0.00050108
Iteration 15/25 | Loss: 0.00050108
Iteration 16/25 | Loss: 0.00050108
Iteration 17/25 | Loss: 0.00050108
Iteration 18/25 | Loss: 0.00050108
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005010848399251699, 0.0005010848399251699, 0.0005010848399251699, 0.0005010848399251699, 0.0005010848399251699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005010848399251699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050108
Iteration 2/1000 | Loss: 0.00005978
Iteration 3/1000 | Loss: 0.00002067
Iteration 4/1000 | Loss: 0.00001747
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001491
Iteration 7/1000 | Loss: 0.00001425
Iteration 8/1000 | Loss: 0.00001392
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001313
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001277
Iteration 23/1000 | Loss: 0.00001276
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001275
Iteration 26/1000 | Loss: 0.00001275
Iteration 27/1000 | Loss: 0.00001274
Iteration 28/1000 | Loss: 0.00001274
Iteration 29/1000 | Loss: 0.00001274
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001273
Iteration 34/1000 | Loss: 0.00001273
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001273
Iteration 37/1000 | Loss: 0.00001273
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001272
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001271
Iteration 44/1000 | Loss: 0.00001271
Iteration 45/1000 | Loss: 0.00001271
Iteration 46/1000 | Loss: 0.00001270
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00001270
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001269
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001268
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001267
Iteration 58/1000 | Loss: 0.00001267
Iteration 59/1000 | Loss: 0.00001267
Iteration 60/1000 | Loss: 0.00001267
Iteration 61/1000 | Loss: 0.00001267
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001266
Iteration 65/1000 | Loss: 0.00001266
Iteration 66/1000 | Loss: 0.00001266
Iteration 67/1000 | Loss: 0.00001266
Iteration 68/1000 | Loss: 0.00001266
Iteration 69/1000 | Loss: 0.00001266
Iteration 70/1000 | Loss: 0.00001266
Iteration 71/1000 | Loss: 0.00001266
Iteration 72/1000 | Loss: 0.00001266
Iteration 73/1000 | Loss: 0.00001266
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001265
Iteration 77/1000 | Loss: 0.00001265
Iteration 78/1000 | Loss: 0.00001265
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001264
Iteration 81/1000 | Loss: 0.00001263
Iteration 82/1000 | Loss: 0.00001263
Iteration 83/1000 | Loss: 0.00001263
Iteration 84/1000 | Loss: 0.00001263
Iteration 85/1000 | Loss: 0.00001263
Iteration 86/1000 | Loss: 0.00001263
Iteration 87/1000 | Loss: 0.00001262
Iteration 88/1000 | Loss: 0.00001262
Iteration 89/1000 | Loss: 0.00001262
Iteration 90/1000 | Loss: 0.00001262
Iteration 91/1000 | Loss: 0.00001262
Iteration 92/1000 | Loss: 0.00001262
Iteration 93/1000 | Loss: 0.00001262
Iteration 94/1000 | Loss: 0.00001262
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001262
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001262
Iteration 99/1000 | Loss: 0.00001262
Iteration 100/1000 | Loss: 0.00001262
Iteration 101/1000 | Loss: 0.00001262
Iteration 102/1000 | Loss: 0.00001262
Iteration 103/1000 | Loss: 0.00001261
Iteration 104/1000 | Loss: 0.00001261
Iteration 105/1000 | Loss: 0.00001261
Iteration 106/1000 | Loss: 0.00001261
Iteration 107/1000 | Loss: 0.00001261
Iteration 108/1000 | Loss: 0.00001261
Iteration 109/1000 | Loss: 0.00001261
Iteration 110/1000 | Loss: 0.00001261
Iteration 111/1000 | Loss: 0.00001261
Iteration 112/1000 | Loss: 0.00001261
Iteration 113/1000 | Loss: 0.00001261
Iteration 114/1000 | Loss: 0.00001261
Iteration 115/1000 | Loss: 0.00001261
Iteration 116/1000 | Loss: 0.00001261
Iteration 117/1000 | Loss: 0.00001261
Iteration 118/1000 | Loss: 0.00001261
Iteration 119/1000 | Loss: 0.00001261
Iteration 120/1000 | Loss: 0.00001261
Iteration 121/1000 | Loss: 0.00001261
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.2608306860784069e-05, 1.2608306860784069e-05, 1.2608306860784069e-05, 1.2608306860784069e-05, 1.2608306860784069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2608306860784069e-05

Optimization complete. Final v2v error: 3.001801013946533 mm

Highest mean error: 4.436537265777588 mm for frame 0

Lowest mean error: 2.4895293712615967 mm for frame 19

Saving results

Total time: 42.245277881622314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011954
Iteration 2/25 | Loss: 0.00243264
Iteration 3/25 | Loss: 0.00160306
Iteration 4/25 | Loss: 0.00142035
Iteration 5/25 | Loss: 0.00135087
Iteration 6/25 | Loss: 0.00129423
Iteration 7/25 | Loss: 0.00146269
Iteration 8/25 | Loss: 0.00110862
Iteration 9/25 | Loss: 0.00100268
Iteration 10/25 | Loss: 0.00097603
Iteration 11/25 | Loss: 0.00096344
Iteration 12/25 | Loss: 0.00096260
Iteration 13/25 | Loss: 0.00094983
Iteration 14/25 | Loss: 0.00094589
Iteration 15/25 | Loss: 0.00094439
Iteration 16/25 | Loss: 0.00095619
Iteration 17/25 | Loss: 0.00094106
Iteration 18/25 | Loss: 0.00094492
Iteration 19/25 | Loss: 0.00094649
Iteration 20/25 | Loss: 0.00094334
Iteration 21/25 | Loss: 0.00093723
Iteration 22/25 | Loss: 0.00094069
Iteration 23/25 | Loss: 0.00093743
Iteration 24/25 | Loss: 0.00093866
Iteration 25/25 | Loss: 0.00093674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36054897
Iteration 2/25 | Loss: 0.00076672
Iteration 3/25 | Loss: 0.00056036
Iteration 4/25 | Loss: 0.00050906
Iteration 5/25 | Loss: 0.00050905
Iteration 6/25 | Loss: 0.00050905
Iteration 7/25 | Loss: 0.00050905
Iteration 8/25 | Loss: 0.00050905
Iteration 9/25 | Loss: 0.00050905
Iteration 10/25 | Loss: 0.00050905
Iteration 11/25 | Loss: 0.00050905
Iteration 12/25 | Loss: 0.00050905
Iteration 13/25 | Loss: 0.00050905
Iteration 14/25 | Loss: 0.00050905
Iteration 15/25 | Loss: 0.00050905
Iteration 16/25 | Loss: 0.00050905
Iteration 17/25 | Loss: 0.00050905
Iteration 18/25 | Loss: 0.00050905
Iteration 19/25 | Loss: 0.00050905
Iteration 20/25 | Loss: 0.00050905
Iteration 21/25 | Loss: 0.00050905
Iteration 22/25 | Loss: 0.00050905
Iteration 23/25 | Loss: 0.00050905
Iteration 24/25 | Loss: 0.00050905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005090534104965627, 0.0005090534104965627, 0.0005090534104965627, 0.0005090534104965627, 0.0005090534104965627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005090534104965627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050905
Iteration 2/1000 | Loss: 0.00036657
Iteration 3/1000 | Loss: 0.00005127
Iteration 4/1000 | Loss: 0.00010299
Iteration 5/1000 | Loss: 0.00003532
Iteration 6/1000 | Loss: 0.00010286
Iteration 7/1000 | Loss: 0.00048352
Iteration 8/1000 | Loss: 0.00005621
Iteration 9/1000 | Loss: 0.00003290
Iteration 10/1000 | Loss: 0.00002317
Iteration 11/1000 | Loss: 0.00003949
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00004660
Iteration 14/1000 | Loss: 0.00111447
Iteration 15/1000 | Loss: 0.00741404
Iteration 16/1000 | Loss: 0.00389654
Iteration 17/1000 | Loss: 0.00291545
Iteration 18/1000 | Loss: 0.00201759
Iteration 19/1000 | Loss: 0.00075812
Iteration 20/1000 | Loss: 0.00155292
Iteration 21/1000 | Loss: 0.00072408
Iteration 22/1000 | Loss: 0.00018527
Iteration 23/1000 | Loss: 0.00148789
Iteration 24/1000 | Loss: 0.00003211
Iteration 25/1000 | Loss: 0.00004247
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00018493
Iteration 28/1000 | Loss: 0.00001347
Iteration 29/1000 | Loss: 0.00008779
Iteration 30/1000 | Loss: 0.00065621
Iteration 31/1000 | Loss: 0.00070820
Iteration 32/1000 | Loss: 0.00013319
Iteration 33/1000 | Loss: 0.00050285
Iteration 34/1000 | Loss: 0.00013309
Iteration 35/1000 | Loss: 0.00008015
Iteration 36/1000 | Loss: 0.00006554
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00002387
Iteration 39/1000 | Loss: 0.00002736
Iteration 40/1000 | Loss: 0.00003296
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001529
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00002935
Iteration 45/1000 | Loss: 0.00013985
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00002562
Iteration 52/1000 | Loss: 0.00008284
Iteration 53/1000 | Loss: 0.00002813
Iteration 54/1000 | Loss: 0.00002577
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001200
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001200
Iteration 62/1000 | Loss: 0.00001200
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001200
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001200
Iteration 72/1000 | Loss: 0.00001200
Iteration 73/1000 | Loss: 0.00001200
Iteration 74/1000 | Loss: 0.00001200
Iteration 75/1000 | Loss: 0.00001200
Iteration 76/1000 | Loss: 0.00001200
Iteration 77/1000 | Loss: 0.00001200
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001200
Iteration 83/1000 | Loss: 0.00001200
Iteration 84/1000 | Loss: 0.00001200
Iteration 85/1000 | Loss: 0.00001200
Iteration 86/1000 | Loss: 0.00001200
Iteration 87/1000 | Loss: 0.00001200
Iteration 88/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.1995954082522076e-05, 1.1995954082522076e-05, 1.1995954082522076e-05, 1.1995954082522076e-05, 1.1995954082522076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1995954082522076e-05

Optimization complete. Final v2v error: 2.956062078475952 mm

Highest mean error: 3.8291890621185303 mm for frame 82

Lowest mean error: 2.5579843521118164 mm for frame 3

Saving results

Total time: 118.35874509811401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830921
Iteration 2/25 | Loss: 0.00126912
Iteration 3/25 | Loss: 0.00096504
Iteration 4/25 | Loss: 0.00093823
Iteration 5/25 | Loss: 0.00093485
Iteration 6/25 | Loss: 0.00093457
Iteration 7/25 | Loss: 0.00093457
Iteration 8/25 | Loss: 0.00093457
Iteration 9/25 | Loss: 0.00093457
Iteration 10/25 | Loss: 0.00093457
Iteration 11/25 | Loss: 0.00093457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009345707949250937, 0.0009345707949250937, 0.0009345707949250937, 0.0009345707949250937, 0.0009345707949250937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009345707949250937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90754831
Iteration 2/25 | Loss: 0.00027210
Iteration 3/25 | Loss: 0.00027210
Iteration 4/25 | Loss: 0.00027210
Iteration 5/25 | Loss: 0.00027210
Iteration 6/25 | Loss: 0.00027210
Iteration 7/25 | Loss: 0.00027209
Iteration 8/25 | Loss: 0.00027209
Iteration 9/25 | Loss: 0.00027209
Iteration 10/25 | Loss: 0.00027209
Iteration 11/25 | Loss: 0.00027209
Iteration 12/25 | Loss: 0.00027209
Iteration 13/25 | Loss: 0.00027209
Iteration 14/25 | Loss: 0.00027209
Iteration 15/25 | Loss: 0.00027209
Iteration 16/25 | Loss: 0.00027209
Iteration 17/25 | Loss: 0.00027209
Iteration 18/25 | Loss: 0.00027209
Iteration 19/25 | Loss: 0.00027209
Iteration 20/25 | Loss: 0.00027209
Iteration 21/25 | Loss: 0.00027209
Iteration 22/25 | Loss: 0.00027209
Iteration 23/25 | Loss: 0.00027209
Iteration 24/25 | Loss: 0.00027209
Iteration 25/25 | Loss: 0.00027209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027209
Iteration 2/1000 | Loss: 0.00003423
Iteration 3/1000 | Loss: 0.00002108
Iteration 4/1000 | Loss: 0.00001896
Iteration 5/1000 | Loss: 0.00001832
Iteration 6/1000 | Loss: 0.00001767
Iteration 7/1000 | Loss: 0.00001733
Iteration 8/1000 | Loss: 0.00001699
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001669
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001662
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001658
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001654
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001653
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001652
Iteration 28/1000 | Loss: 0.00001652
Iteration 29/1000 | Loss: 0.00001652
Iteration 30/1000 | Loss: 0.00001652
Iteration 31/1000 | Loss: 0.00001652
Iteration 32/1000 | Loss: 0.00001652
Iteration 33/1000 | Loss: 0.00001652
Iteration 34/1000 | Loss: 0.00001651
Iteration 35/1000 | Loss: 0.00001651
Iteration 36/1000 | Loss: 0.00001651
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001651
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001651
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001650
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001650
Iteration 53/1000 | Loss: 0.00001650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [1.65048932103673e-05, 1.65048932103673e-05, 1.65048932103673e-05, 1.65048932103673e-05, 1.65048932103673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.65048932103673e-05

Optimization complete. Final v2v error: 3.362887144088745 mm

Highest mean error: 3.463245391845703 mm for frame 98

Lowest mean error: 3.231252670288086 mm for frame 15

Saving results

Total time: 28.0546395778656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832157
Iteration 2/25 | Loss: 0.00121478
Iteration 3/25 | Loss: 0.00106104
Iteration 4/25 | Loss: 0.00104822
Iteration 5/25 | Loss: 0.00104470
Iteration 6/25 | Loss: 0.00104353
Iteration 7/25 | Loss: 0.00104676
Iteration 8/25 | Loss: 0.00104899
Iteration 9/25 | Loss: 0.00104833
Iteration 10/25 | Loss: 0.00104670
Iteration 11/25 | Loss: 0.00104064
Iteration 12/25 | Loss: 0.00103855
Iteration 13/25 | Loss: 0.00104183
Iteration 14/25 | Loss: 0.00104108
Iteration 15/25 | Loss: 0.00104009
Iteration 16/25 | Loss: 0.00103667
Iteration 17/25 | Loss: 0.00103523
Iteration 18/25 | Loss: 0.00103497
Iteration 19/25 | Loss: 0.00103485
Iteration 20/25 | Loss: 0.00103478
Iteration 21/25 | Loss: 0.00103469
Iteration 22/25 | Loss: 0.00103465
Iteration 23/25 | Loss: 0.00103464
Iteration 24/25 | Loss: 0.00103463
Iteration 25/25 | Loss: 0.00103463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36461031
Iteration 2/25 | Loss: 0.00163718
Iteration 3/25 | Loss: 0.00163717
Iteration 4/25 | Loss: 0.00163717
Iteration 5/25 | Loss: 0.00163716
Iteration 6/25 | Loss: 0.00163716
Iteration 7/25 | Loss: 0.00163716
Iteration 8/25 | Loss: 0.00163716
Iteration 9/25 | Loss: 0.00163716
Iteration 10/25 | Loss: 0.00163716
Iteration 11/25 | Loss: 0.00163716
Iteration 12/25 | Loss: 0.00163716
Iteration 13/25 | Loss: 0.00163716
Iteration 14/25 | Loss: 0.00163716
Iteration 15/25 | Loss: 0.00163716
Iteration 16/25 | Loss: 0.00163716
Iteration 17/25 | Loss: 0.00163716
Iteration 18/25 | Loss: 0.00163716
Iteration 19/25 | Loss: 0.00163716
Iteration 20/25 | Loss: 0.00163716
Iteration 21/25 | Loss: 0.00163716
Iteration 22/25 | Loss: 0.00163716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001637162291444838, 0.001637162291444838, 0.001637162291444838, 0.001637162291444838, 0.001637162291444838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001637162291444838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163716
Iteration 2/1000 | Loss: 0.00017017
Iteration 3/1000 | Loss: 0.00010169
Iteration 4/1000 | Loss: 0.00008523
Iteration 5/1000 | Loss: 0.00007647
Iteration 6/1000 | Loss: 0.00007210
Iteration 7/1000 | Loss: 0.00006953
Iteration 8/1000 | Loss: 0.00006715
Iteration 9/1000 | Loss: 0.00006501
Iteration 10/1000 | Loss: 0.00006290
Iteration 11/1000 | Loss: 0.00006166
Iteration 12/1000 | Loss: 0.00006091
Iteration 13/1000 | Loss: 0.00006024
Iteration 14/1000 | Loss: 0.00005968
Iteration 15/1000 | Loss: 0.00005920
Iteration 16/1000 | Loss: 0.00005871
Iteration 17/1000 | Loss: 0.00005838
Iteration 18/1000 | Loss: 0.00036840
Iteration 19/1000 | Loss: 0.00006629
Iteration 20/1000 | Loss: 0.00005875
Iteration 21/1000 | Loss: 0.00005701
Iteration 22/1000 | Loss: 0.00005631
Iteration 23/1000 | Loss: 0.00005571
Iteration 24/1000 | Loss: 0.00005513
Iteration 25/1000 | Loss: 0.00005477
Iteration 26/1000 | Loss: 0.00005442
Iteration 27/1000 | Loss: 0.00005400
Iteration 28/1000 | Loss: 0.00005352
Iteration 29/1000 | Loss: 0.00005318
Iteration 30/1000 | Loss: 0.00005289
Iteration 31/1000 | Loss: 0.00005267
Iteration 32/1000 | Loss: 0.00005233
Iteration 33/1000 | Loss: 0.00005207
Iteration 34/1000 | Loss: 0.00005186
Iteration 35/1000 | Loss: 0.00005161
Iteration 36/1000 | Loss: 0.00005142
Iteration 37/1000 | Loss: 0.00005125
Iteration 38/1000 | Loss: 0.00005105
Iteration 39/1000 | Loss: 0.00005090
Iteration 40/1000 | Loss: 0.00005087
Iteration 41/1000 | Loss: 0.00005087
Iteration 42/1000 | Loss: 0.00005086
Iteration 43/1000 | Loss: 0.00005082
Iteration 44/1000 | Loss: 0.00005079
Iteration 45/1000 | Loss: 0.00005077
Iteration 46/1000 | Loss: 0.00005071
Iteration 47/1000 | Loss: 0.00005067
Iteration 48/1000 | Loss: 0.00005066
Iteration 49/1000 | Loss: 0.00005064
Iteration 50/1000 | Loss: 0.00005064
Iteration 51/1000 | Loss: 0.00005063
Iteration 52/1000 | Loss: 0.00005063
Iteration 53/1000 | Loss: 0.00005061
Iteration 54/1000 | Loss: 0.00005057
Iteration 55/1000 | Loss: 0.00005051
Iteration 56/1000 | Loss: 0.00005049
Iteration 57/1000 | Loss: 0.00005048
Iteration 58/1000 | Loss: 0.00005048
Iteration 59/1000 | Loss: 0.00005047
Iteration 60/1000 | Loss: 0.00005044
Iteration 61/1000 | Loss: 0.00005032
Iteration 62/1000 | Loss: 0.00005031
Iteration 63/1000 | Loss: 0.00005031
Iteration 64/1000 | Loss: 0.00005031
Iteration 65/1000 | Loss: 0.00005030
Iteration 66/1000 | Loss: 0.00005029
Iteration 67/1000 | Loss: 0.00005028
Iteration 68/1000 | Loss: 0.00005027
Iteration 69/1000 | Loss: 0.00005027
Iteration 70/1000 | Loss: 0.00005027
Iteration 71/1000 | Loss: 0.00005027
Iteration 72/1000 | Loss: 0.00005027
Iteration 73/1000 | Loss: 0.00005026
Iteration 74/1000 | Loss: 0.00005026
Iteration 75/1000 | Loss: 0.00005025
Iteration 76/1000 | Loss: 0.00005024
Iteration 77/1000 | Loss: 0.00005024
Iteration 78/1000 | Loss: 0.00005024
Iteration 79/1000 | Loss: 0.00005023
Iteration 80/1000 | Loss: 0.00005023
Iteration 81/1000 | Loss: 0.00005022
Iteration 82/1000 | Loss: 0.00005022
Iteration 83/1000 | Loss: 0.00005021
Iteration 84/1000 | Loss: 0.00005021
Iteration 85/1000 | Loss: 0.00005021
Iteration 86/1000 | Loss: 0.00005020
Iteration 87/1000 | Loss: 0.00005020
Iteration 88/1000 | Loss: 0.00005020
Iteration 89/1000 | Loss: 0.00005017
Iteration 90/1000 | Loss: 0.00005017
Iteration 91/1000 | Loss: 0.00005017
Iteration 92/1000 | Loss: 0.00005017
Iteration 93/1000 | Loss: 0.00005016
Iteration 94/1000 | Loss: 0.00005016
Iteration 95/1000 | Loss: 0.00005016
Iteration 96/1000 | Loss: 0.00005015
Iteration 97/1000 | Loss: 0.00005015
Iteration 98/1000 | Loss: 0.00005014
Iteration 99/1000 | Loss: 0.00005014
Iteration 100/1000 | Loss: 0.00005014
Iteration 101/1000 | Loss: 0.00005014
Iteration 102/1000 | Loss: 0.00005014
Iteration 103/1000 | Loss: 0.00005014
Iteration 104/1000 | Loss: 0.00005014
Iteration 105/1000 | Loss: 0.00005014
Iteration 106/1000 | Loss: 0.00005014
Iteration 107/1000 | Loss: 0.00005014
Iteration 108/1000 | Loss: 0.00005013
Iteration 109/1000 | Loss: 0.00005013
Iteration 110/1000 | Loss: 0.00005013
Iteration 111/1000 | Loss: 0.00005013
Iteration 112/1000 | Loss: 0.00005012
Iteration 113/1000 | Loss: 0.00005012
Iteration 114/1000 | Loss: 0.00005011
Iteration 115/1000 | Loss: 0.00005011
Iteration 116/1000 | Loss: 0.00005011
Iteration 117/1000 | Loss: 0.00005011
Iteration 118/1000 | Loss: 0.00005011
Iteration 119/1000 | Loss: 0.00005011
Iteration 120/1000 | Loss: 0.00005010
Iteration 121/1000 | Loss: 0.00005010
Iteration 122/1000 | Loss: 0.00005010
Iteration 123/1000 | Loss: 0.00005009
Iteration 124/1000 | Loss: 0.00005009
Iteration 125/1000 | Loss: 0.00005009
Iteration 126/1000 | Loss: 0.00005008
Iteration 127/1000 | Loss: 0.00005008
Iteration 128/1000 | Loss: 0.00005008
Iteration 129/1000 | Loss: 0.00005007
Iteration 130/1000 | Loss: 0.00005007
Iteration 131/1000 | Loss: 0.00005006
Iteration 132/1000 | Loss: 0.00005006
Iteration 133/1000 | Loss: 0.00005006
Iteration 134/1000 | Loss: 0.00005006
Iteration 135/1000 | Loss: 0.00005006
Iteration 136/1000 | Loss: 0.00005005
Iteration 137/1000 | Loss: 0.00005005
Iteration 138/1000 | Loss: 0.00005005
Iteration 139/1000 | Loss: 0.00005004
Iteration 140/1000 | Loss: 0.00005004
Iteration 141/1000 | Loss: 0.00005003
Iteration 142/1000 | Loss: 0.00005003
Iteration 143/1000 | Loss: 0.00005003
Iteration 144/1000 | Loss: 0.00005002
Iteration 145/1000 | Loss: 0.00005002
Iteration 146/1000 | Loss: 0.00005001
Iteration 147/1000 | Loss: 0.00005001
Iteration 148/1000 | Loss: 0.00005001
Iteration 149/1000 | Loss: 0.00005001
Iteration 150/1000 | Loss: 0.00005001
Iteration 151/1000 | Loss: 0.00005001
Iteration 152/1000 | Loss: 0.00005001
Iteration 153/1000 | Loss: 0.00005001
Iteration 154/1000 | Loss: 0.00005000
Iteration 155/1000 | Loss: 0.00005000
Iteration 156/1000 | Loss: 0.00005000
Iteration 157/1000 | Loss: 0.00004999
Iteration 158/1000 | Loss: 0.00004999
Iteration 159/1000 | Loss: 0.00004999
Iteration 160/1000 | Loss: 0.00004999
Iteration 161/1000 | Loss: 0.00004999
Iteration 162/1000 | Loss: 0.00004999
Iteration 163/1000 | Loss: 0.00004999
Iteration 164/1000 | Loss: 0.00004999
Iteration 165/1000 | Loss: 0.00004999
Iteration 166/1000 | Loss: 0.00004999
Iteration 167/1000 | Loss: 0.00004999
Iteration 168/1000 | Loss: 0.00004998
Iteration 169/1000 | Loss: 0.00004998
Iteration 170/1000 | Loss: 0.00004998
Iteration 171/1000 | Loss: 0.00004997
Iteration 172/1000 | Loss: 0.00004997
Iteration 173/1000 | Loss: 0.00004997
Iteration 174/1000 | Loss: 0.00004997
Iteration 175/1000 | Loss: 0.00004996
Iteration 176/1000 | Loss: 0.00004996
Iteration 177/1000 | Loss: 0.00004996
Iteration 178/1000 | Loss: 0.00004996
Iteration 179/1000 | Loss: 0.00004996
Iteration 180/1000 | Loss: 0.00004996
Iteration 181/1000 | Loss: 0.00004996
Iteration 182/1000 | Loss: 0.00004996
Iteration 183/1000 | Loss: 0.00004995
Iteration 184/1000 | Loss: 0.00004995
Iteration 185/1000 | Loss: 0.00004995
Iteration 186/1000 | Loss: 0.00004995
Iteration 187/1000 | Loss: 0.00004995
Iteration 188/1000 | Loss: 0.00004995
Iteration 189/1000 | Loss: 0.00004994
Iteration 190/1000 | Loss: 0.00004994
Iteration 191/1000 | Loss: 0.00004994
Iteration 192/1000 | Loss: 0.00004994
Iteration 193/1000 | Loss: 0.00004994
Iteration 194/1000 | Loss: 0.00004994
Iteration 195/1000 | Loss: 0.00004994
Iteration 196/1000 | Loss: 0.00004994
Iteration 197/1000 | Loss: 0.00004993
Iteration 198/1000 | Loss: 0.00004993
Iteration 199/1000 | Loss: 0.00004993
Iteration 200/1000 | Loss: 0.00004993
Iteration 201/1000 | Loss: 0.00004993
Iteration 202/1000 | Loss: 0.00004993
Iteration 203/1000 | Loss: 0.00004993
Iteration 204/1000 | Loss: 0.00004992
Iteration 205/1000 | Loss: 0.00004992
Iteration 206/1000 | Loss: 0.00004992
Iteration 207/1000 | Loss: 0.00004992
Iteration 208/1000 | Loss: 0.00004992
Iteration 209/1000 | Loss: 0.00004992
Iteration 210/1000 | Loss: 0.00004992
Iteration 211/1000 | Loss: 0.00004992
Iteration 212/1000 | Loss: 0.00004992
Iteration 213/1000 | Loss: 0.00004992
Iteration 214/1000 | Loss: 0.00004992
Iteration 215/1000 | Loss: 0.00004992
Iteration 216/1000 | Loss: 0.00004991
Iteration 217/1000 | Loss: 0.00004991
Iteration 218/1000 | Loss: 0.00004991
Iteration 219/1000 | Loss: 0.00004991
Iteration 220/1000 | Loss: 0.00004991
Iteration 221/1000 | Loss: 0.00004991
Iteration 222/1000 | Loss: 0.00004991
Iteration 223/1000 | Loss: 0.00004991
Iteration 224/1000 | Loss: 0.00004991
Iteration 225/1000 | Loss: 0.00004991
Iteration 226/1000 | Loss: 0.00004990
Iteration 227/1000 | Loss: 0.00004990
Iteration 228/1000 | Loss: 0.00004990
Iteration 229/1000 | Loss: 0.00004990
Iteration 230/1000 | Loss: 0.00004990
Iteration 231/1000 | Loss: 0.00004990
Iteration 232/1000 | Loss: 0.00004990
Iteration 233/1000 | Loss: 0.00004990
Iteration 234/1000 | Loss: 0.00004990
Iteration 235/1000 | Loss: 0.00004990
Iteration 236/1000 | Loss: 0.00004990
Iteration 237/1000 | Loss: 0.00004989
Iteration 238/1000 | Loss: 0.00004989
Iteration 239/1000 | Loss: 0.00004989
Iteration 240/1000 | Loss: 0.00004989
Iteration 241/1000 | Loss: 0.00004989
Iteration 242/1000 | Loss: 0.00004989
Iteration 243/1000 | Loss: 0.00004989
Iteration 244/1000 | Loss: 0.00004989
Iteration 245/1000 | Loss: 0.00004989
Iteration 246/1000 | Loss: 0.00004989
Iteration 247/1000 | Loss: 0.00004989
Iteration 248/1000 | Loss: 0.00004989
Iteration 249/1000 | Loss: 0.00004988
Iteration 250/1000 | Loss: 0.00004988
Iteration 251/1000 | Loss: 0.00004988
Iteration 252/1000 | Loss: 0.00004988
Iteration 253/1000 | Loss: 0.00004988
Iteration 254/1000 | Loss: 0.00004988
Iteration 255/1000 | Loss: 0.00004988
Iteration 256/1000 | Loss: 0.00004988
Iteration 257/1000 | Loss: 0.00004988
Iteration 258/1000 | Loss: 0.00004988
Iteration 259/1000 | Loss: 0.00004988
Iteration 260/1000 | Loss: 0.00004988
Iteration 261/1000 | Loss: 0.00004988
Iteration 262/1000 | Loss: 0.00004988
Iteration 263/1000 | Loss: 0.00004987
Iteration 264/1000 | Loss: 0.00004987
Iteration 265/1000 | Loss: 0.00004987
Iteration 266/1000 | Loss: 0.00004987
Iteration 267/1000 | Loss: 0.00004987
Iteration 268/1000 | Loss: 0.00004987
Iteration 269/1000 | Loss: 0.00004987
Iteration 270/1000 | Loss: 0.00004987
Iteration 271/1000 | Loss: 0.00004987
Iteration 272/1000 | Loss: 0.00004987
Iteration 273/1000 | Loss: 0.00004986
Iteration 274/1000 | Loss: 0.00004986
Iteration 275/1000 | Loss: 0.00004986
Iteration 276/1000 | Loss: 0.00004986
Iteration 277/1000 | Loss: 0.00004986
Iteration 278/1000 | Loss: 0.00004986
Iteration 279/1000 | Loss: 0.00004986
Iteration 280/1000 | Loss: 0.00004986
Iteration 281/1000 | Loss: 0.00004986
Iteration 282/1000 | Loss: 0.00004986
Iteration 283/1000 | Loss: 0.00004986
Iteration 284/1000 | Loss: 0.00004986
Iteration 285/1000 | Loss: 0.00004986
Iteration 286/1000 | Loss: 0.00004986
Iteration 287/1000 | Loss: 0.00004986
Iteration 288/1000 | Loss: 0.00004986
Iteration 289/1000 | Loss: 0.00004986
Iteration 290/1000 | Loss: 0.00004986
Iteration 291/1000 | Loss: 0.00004985
Iteration 292/1000 | Loss: 0.00004985
Iteration 293/1000 | Loss: 0.00004985
Iteration 294/1000 | Loss: 0.00004985
Iteration 295/1000 | Loss: 0.00004985
Iteration 296/1000 | Loss: 0.00004985
Iteration 297/1000 | Loss: 0.00004985
Iteration 298/1000 | Loss: 0.00004985
Iteration 299/1000 | Loss: 0.00004985
Iteration 300/1000 | Loss: 0.00004985
Iteration 301/1000 | Loss: 0.00004985
Iteration 302/1000 | Loss: 0.00004985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [4.984906263416633e-05, 4.984906263416633e-05, 4.984906263416633e-05, 4.984906263416633e-05, 4.984906263416633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.984906263416633e-05

Optimization complete. Final v2v error: 3.8136157989501953 mm

Highest mean error: 13.389533996582031 mm for frame 81

Lowest mean error: 2.6127443313598633 mm for frame 0

Saving results

Total time: 123.98074412345886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385872
Iteration 2/25 | Loss: 0.00097417
Iteration 3/25 | Loss: 0.00089540
Iteration 4/25 | Loss: 0.00088452
Iteration 5/25 | Loss: 0.00088048
Iteration 6/25 | Loss: 0.00087932
Iteration 7/25 | Loss: 0.00087930
Iteration 8/25 | Loss: 0.00087930
Iteration 9/25 | Loss: 0.00087930
Iteration 10/25 | Loss: 0.00087930
Iteration 11/25 | Loss: 0.00087930
Iteration 12/25 | Loss: 0.00087930
Iteration 13/25 | Loss: 0.00087930
Iteration 14/25 | Loss: 0.00087930
Iteration 15/25 | Loss: 0.00087930
Iteration 16/25 | Loss: 0.00087930
Iteration 17/25 | Loss: 0.00087930
Iteration 18/25 | Loss: 0.00087930
Iteration 19/25 | Loss: 0.00087930
Iteration 20/25 | Loss: 0.00087930
Iteration 21/25 | Loss: 0.00087930
Iteration 22/25 | Loss: 0.00087930
Iteration 23/25 | Loss: 0.00087930
Iteration 24/25 | Loss: 0.00087930
Iteration 25/25 | Loss: 0.00087930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53175426
Iteration 2/25 | Loss: 0.00054760
Iteration 3/25 | Loss: 0.00054759
Iteration 4/25 | Loss: 0.00054759
Iteration 5/25 | Loss: 0.00054759
Iteration 6/25 | Loss: 0.00054759
Iteration 7/25 | Loss: 0.00054759
Iteration 8/25 | Loss: 0.00054759
Iteration 9/25 | Loss: 0.00054758
Iteration 10/25 | Loss: 0.00054758
Iteration 11/25 | Loss: 0.00054758
Iteration 12/25 | Loss: 0.00054758
Iteration 13/25 | Loss: 0.00054758
Iteration 14/25 | Loss: 0.00054758
Iteration 15/25 | Loss: 0.00054758
Iteration 16/25 | Loss: 0.00054758
Iteration 17/25 | Loss: 0.00054758
Iteration 18/25 | Loss: 0.00054758
Iteration 19/25 | Loss: 0.00054758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000547584262676537, 0.000547584262676537, 0.000547584262676537, 0.000547584262676537, 0.000547584262676537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000547584262676537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054758
Iteration 2/1000 | Loss: 0.00002621
Iteration 3/1000 | Loss: 0.00001451
Iteration 4/1000 | Loss: 0.00001294
Iteration 5/1000 | Loss: 0.00001237
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001159
Iteration 8/1000 | Loss: 0.00001138
Iteration 9/1000 | Loss: 0.00001115
Iteration 10/1000 | Loss: 0.00001096
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001086
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001082
Iteration 15/1000 | Loss: 0.00001082
Iteration 16/1000 | Loss: 0.00001081
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001081
Iteration 19/1000 | Loss: 0.00001080
Iteration 20/1000 | Loss: 0.00001080
Iteration 21/1000 | Loss: 0.00001077
Iteration 22/1000 | Loss: 0.00001077
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001077
Iteration 26/1000 | Loss: 0.00001077
Iteration 27/1000 | Loss: 0.00001077
Iteration 28/1000 | Loss: 0.00001073
Iteration 29/1000 | Loss: 0.00001073
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001072
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001070
Iteration 38/1000 | Loss: 0.00001070
Iteration 39/1000 | Loss: 0.00001067
Iteration 40/1000 | Loss: 0.00001067
Iteration 41/1000 | Loss: 0.00001067
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001067
Iteration 45/1000 | Loss: 0.00001066
Iteration 46/1000 | Loss: 0.00001066
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001066
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001065
Iteration 55/1000 | Loss: 0.00001065
Iteration 56/1000 | Loss: 0.00001065
Iteration 57/1000 | Loss: 0.00001065
Iteration 58/1000 | Loss: 0.00001064
Iteration 59/1000 | Loss: 0.00001064
Iteration 60/1000 | Loss: 0.00001064
Iteration 61/1000 | Loss: 0.00001064
Iteration 62/1000 | Loss: 0.00001064
Iteration 63/1000 | Loss: 0.00001064
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001064
Iteration 67/1000 | Loss: 0.00001064
Iteration 68/1000 | Loss: 0.00001064
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001062
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001062
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001062
Iteration 82/1000 | Loss: 0.00001062
Iteration 83/1000 | Loss: 0.00001062
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001061
Iteration 86/1000 | Loss: 0.00001061
Iteration 87/1000 | Loss: 0.00001061
Iteration 88/1000 | Loss: 0.00001061
Iteration 89/1000 | Loss: 0.00001061
Iteration 90/1000 | Loss: 0.00001061
Iteration 91/1000 | Loss: 0.00001061
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001060
Iteration 94/1000 | Loss: 0.00001060
Iteration 95/1000 | Loss: 0.00001060
Iteration 96/1000 | Loss: 0.00001060
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Iteration 100/1000 | Loss: 0.00001059
Iteration 101/1000 | Loss: 0.00001059
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001058
Iteration 104/1000 | Loss: 0.00001058
Iteration 105/1000 | Loss: 0.00001058
Iteration 106/1000 | Loss: 0.00001058
Iteration 107/1000 | Loss: 0.00001058
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001057
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001057
Iteration 113/1000 | Loss: 0.00001057
Iteration 114/1000 | Loss: 0.00001057
Iteration 115/1000 | Loss: 0.00001057
Iteration 116/1000 | Loss: 0.00001057
Iteration 117/1000 | Loss: 0.00001057
Iteration 118/1000 | Loss: 0.00001057
Iteration 119/1000 | Loss: 0.00001057
Iteration 120/1000 | Loss: 0.00001057
Iteration 121/1000 | Loss: 0.00001057
Iteration 122/1000 | Loss: 0.00001056
Iteration 123/1000 | Loss: 0.00001056
Iteration 124/1000 | Loss: 0.00001056
Iteration 125/1000 | Loss: 0.00001056
Iteration 126/1000 | Loss: 0.00001056
Iteration 127/1000 | Loss: 0.00001056
Iteration 128/1000 | Loss: 0.00001056
Iteration 129/1000 | Loss: 0.00001056
Iteration 130/1000 | Loss: 0.00001056
Iteration 131/1000 | Loss: 0.00001056
Iteration 132/1000 | Loss: 0.00001055
Iteration 133/1000 | Loss: 0.00001055
Iteration 134/1000 | Loss: 0.00001055
Iteration 135/1000 | Loss: 0.00001055
Iteration 136/1000 | Loss: 0.00001055
Iteration 137/1000 | Loss: 0.00001054
Iteration 138/1000 | Loss: 0.00001054
Iteration 139/1000 | Loss: 0.00001054
Iteration 140/1000 | Loss: 0.00001054
Iteration 141/1000 | Loss: 0.00001054
Iteration 142/1000 | Loss: 0.00001054
Iteration 143/1000 | Loss: 0.00001054
Iteration 144/1000 | Loss: 0.00001054
Iteration 145/1000 | Loss: 0.00001054
Iteration 146/1000 | Loss: 0.00001054
Iteration 147/1000 | Loss: 0.00001054
Iteration 148/1000 | Loss: 0.00001054
Iteration 149/1000 | Loss: 0.00001054
Iteration 150/1000 | Loss: 0.00001053
Iteration 151/1000 | Loss: 0.00001053
Iteration 152/1000 | Loss: 0.00001053
Iteration 153/1000 | Loss: 0.00001053
Iteration 154/1000 | Loss: 0.00001053
Iteration 155/1000 | Loss: 0.00001053
Iteration 156/1000 | Loss: 0.00001053
Iteration 157/1000 | Loss: 0.00001053
Iteration 158/1000 | Loss: 0.00001053
Iteration 159/1000 | Loss: 0.00001053
Iteration 160/1000 | Loss: 0.00001053
Iteration 161/1000 | Loss: 0.00001053
Iteration 162/1000 | Loss: 0.00001053
Iteration 163/1000 | Loss: 0.00001053
Iteration 164/1000 | Loss: 0.00001053
Iteration 165/1000 | Loss: 0.00001053
Iteration 166/1000 | Loss: 0.00001053
Iteration 167/1000 | Loss: 0.00001053
Iteration 168/1000 | Loss: 0.00001052
Iteration 169/1000 | Loss: 0.00001052
Iteration 170/1000 | Loss: 0.00001052
Iteration 171/1000 | Loss: 0.00001052
Iteration 172/1000 | Loss: 0.00001052
Iteration 173/1000 | Loss: 0.00001052
Iteration 174/1000 | Loss: 0.00001052
Iteration 175/1000 | Loss: 0.00001052
Iteration 176/1000 | Loss: 0.00001052
Iteration 177/1000 | Loss: 0.00001052
Iteration 178/1000 | Loss: 0.00001052
Iteration 179/1000 | Loss: 0.00001052
Iteration 180/1000 | Loss: 0.00001052
Iteration 181/1000 | Loss: 0.00001052
Iteration 182/1000 | Loss: 0.00001052
Iteration 183/1000 | Loss: 0.00001052
Iteration 184/1000 | Loss: 0.00001052
Iteration 185/1000 | Loss: 0.00001052
Iteration 186/1000 | Loss: 0.00001052
Iteration 187/1000 | Loss: 0.00001052
Iteration 188/1000 | Loss: 0.00001052
Iteration 189/1000 | Loss: 0.00001052
Iteration 190/1000 | Loss: 0.00001052
Iteration 191/1000 | Loss: 0.00001052
Iteration 192/1000 | Loss: 0.00001052
Iteration 193/1000 | Loss: 0.00001052
Iteration 194/1000 | Loss: 0.00001052
Iteration 195/1000 | Loss: 0.00001052
Iteration 196/1000 | Loss: 0.00001052
Iteration 197/1000 | Loss: 0.00001052
Iteration 198/1000 | Loss: 0.00001052
Iteration 199/1000 | Loss: 0.00001052
Iteration 200/1000 | Loss: 0.00001052
Iteration 201/1000 | Loss: 0.00001052
Iteration 202/1000 | Loss: 0.00001052
Iteration 203/1000 | Loss: 0.00001052
Iteration 204/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0523286618990824e-05, 1.0523286618990824e-05, 1.0523286618990824e-05, 1.0523286618990824e-05, 1.0523286618990824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0523286618990824e-05

Optimization complete. Final v2v error: 2.8348379135131836 mm

Highest mean error: 3.2140724658966064 mm for frame 78

Lowest mean error: 2.3097543716430664 mm for frame 166

Saving results

Total time: 37.44187903404236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507957
Iteration 2/25 | Loss: 0.00115309
Iteration 3/25 | Loss: 0.00099135
Iteration 4/25 | Loss: 0.00094839
Iteration 5/25 | Loss: 0.00094294
Iteration 6/25 | Loss: 0.00094244
Iteration 7/25 | Loss: 0.00094244
Iteration 8/25 | Loss: 0.00094244
Iteration 9/25 | Loss: 0.00094244
Iteration 10/25 | Loss: 0.00094244
Iteration 11/25 | Loss: 0.00094244
Iteration 12/25 | Loss: 0.00094244
Iteration 13/25 | Loss: 0.00094244
Iteration 14/25 | Loss: 0.00094244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009424390154890716, 0.0009424390154890716, 0.0009424390154890716, 0.0009424390154890716, 0.0009424390154890716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009424390154890716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29832745
Iteration 2/25 | Loss: 0.00052683
Iteration 3/25 | Loss: 0.00052682
Iteration 4/25 | Loss: 0.00052682
Iteration 5/25 | Loss: 0.00052682
Iteration 6/25 | Loss: 0.00052682
Iteration 7/25 | Loss: 0.00052682
Iteration 8/25 | Loss: 0.00052682
Iteration 9/25 | Loss: 0.00052682
Iteration 10/25 | Loss: 0.00052682
Iteration 11/25 | Loss: 0.00052682
Iteration 12/25 | Loss: 0.00052682
Iteration 13/25 | Loss: 0.00052682
Iteration 14/25 | Loss: 0.00052682
Iteration 15/25 | Loss: 0.00052682
Iteration 16/25 | Loss: 0.00052682
Iteration 17/25 | Loss: 0.00052682
Iteration 18/25 | Loss: 0.00052682
Iteration 19/25 | Loss: 0.00052682
Iteration 20/25 | Loss: 0.00052682
Iteration 21/25 | Loss: 0.00052682
Iteration 22/25 | Loss: 0.00052682
Iteration 23/25 | Loss: 0.00052682
Iteration 24/25 | Loss: 0.00052682
Iteration 25/25 | Loss: 0.00052682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052682
Iteration 2/1000 | Loss: 0.00005282
Iteration 3/1000 | Loss: 0.00002579
Iteration 4/1000 | Loss: 0.00002073
Iteration 5/1000 | Loss: 0.00001902
Iteration 6/1000 | Loss: 0.00001808
Iteration 7/1000 | Loss: 0.00001735
Iteration 8/1000 | Loss: 0.00001695
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001661
Iteration 12/1000 | Loss: 0.00001641
Iteration 13/1000 | Loss: 0.00001639
Iteration 14/1000 | Loss: 0.00001627
Iteration 15/1000 | Loss: 0.00001623
Iteration 16/1000 | Loss: 0.00001619
Iteration 17/1000 | Loss: 0.00001613
Iteration 18/1000 | Loss: 0.00001608
Iteration 19/1000 | Loss: 0.00001602
Iteration 20/1000 | Loss: 0.00001602
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001600
Iteration 23/1000 | Loss: 0.00001599
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001599
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001596
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001596
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001596
Iteration 37/1000 | Loss: 0.00001596
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001595
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001594
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001593
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001593
Iteration 52/1000 | Loss: 0.00001593
Iteration 53/1000 | Loss: 0.00001593
Iteration 54/1000 | Loss: 0.00001593
Iteration 55/1000 | Loss: 0.00001593
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001592
Iteration 58/1000 | Loss: 0.00001592
Iteration 59/1000 | Loss: 0.00001592
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001591
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001588
Iteration 72/1000 | Loss: 0.00001588
Iteration 73/1000 | Loss: 0.00001588
Iteration 74/1000 | Loss: 0.00001588
Iteration 75/1000 | Loss: 0.00001588
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001586
Iteration 91/1000 | Loss: 0.00001586
Iteration 92/1000 | Loss: 0.00001586
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001583
Iteration 109/1000 | Loss: 0.00001583
Iteration 110/1000 | Loss: 0.00001583
Iteration 111/1000 | Loss: 0.00001582
Iteration 112/1000 | Loss: 0.00001582
Iteration 113/1000 | Loss: 0.00001582
Iteration 114/1000 | Loss: 0.00001580
Iteration 115/1000 | Loss: 0.00001580
Iteration 116/1000 | Loss: 0.00001580
Iteration 117/1000 | Loss: 0.00001580
Iteration 118/1000 | Loss: 0.00001580
Iteration 119/1000 | Loss: 0.00001580
Iteration 120/1000 | Loss: 0.00001580
Iteration 121/1000 | Loss: 0.00001580
Iteration 122/1000 | Loss: 0.00001580
Iteration 123/1000 | Loss: 0.00001579
Iteration 124/1000 | Loss: 0.00001579
Iteration 125/1000 | Loss: 0.00001579
Iteration 126/1000 | Loss: 0.00001579
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001577
Iteration 133/1000 | Loss: 0.00001577
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001576
Iteration 140/1000 | Loss: 0.00001575
Iteration 141/1000 | Loss: 0.00001575
Iteration 142/1000 | Loss: 0.00001575
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001574
Iteration 146/1000 | Loss: 0.00001574
Iteration 147/1000 | Loss: 0.00001574
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001574
Iteration 150/1000 | Loss: 0.00001574
Iteration 151/1000 | Loss: 0.00001574
Iteration 152/1000 | Loss: 0.00001574
Iteration 153/1000 | Loss: 0.00001574
Iteration 154/1000 | Loss: 0.00001574
Iteration 155/1000 | Loss: 0.00001573
Iteration 156/1000 | Loss: 0.00001573
Iteration 157/1000 | Loss: 0.00001573
Iteration 158/1000 | Loss: 0.00001573
Iteration 159/1000 | Loss: 0.00001573
Iteration 160/1000 | Loss: 0.00001573
Iteration 161/1000 | Loss: 0.00001573
Iteration 162/1000 | Loss: 0.00001573
Iteration 163/1000 | Loss: 0.00001573
Iteration 164/1000 | Loss: 0.00001573
Iteration 165/1000 | Loss: 0.00001573
Iteration 166/1000 | Loss: 0.00001573
Iteration 167/1000 | Loss: 0.00001573
Iteration 168/1000 | Loss: 0.00001573
Iteration 169/1000 | Loss: 0.00001573
Iteration 170/1000 | Loss: 0.00001573
Iteration 171/1000 | Loss: 0.00001573
Iteration 172/1000 | Loss: 0.00001573
Iteration 173/1000 | Loss: 0.00001573
Iteration 174/1000 | Loss: 0.00001573
Iteration 175/1000 | Loss: 0.00001573
Iteration 176/1000 | Loss: 0.00001573
Iteration 177/1000 | Loss: 0.00001573
Iteration 178/1000 | Loss: 0.00001573
Iteration 179/1000 | Loss: 0.00001573
Iteration 180/1000 | Loss: 0.00001573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.5730818631709553e-05, 1.5730818631709553e-05, 1.5730818631709553e-05, 1.5730818631709553e-05, 1.5730818631709553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5730818631709553e-05

Optimization complete. Final v2v error: 3.3272383213043213 mm

Highest mean error: 3.631162643432617 mm for frame 0

Lowest mean error: 3.172459125518799 mm for frame 36

Saving results

Total time: 37.73350143432617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00612393
Iteration 2/25 | Loss: 0.00154527
Iteration 3/25 | Loss: 0.00114624
Iteration 4/25 | Loss: 0.00112566
Iteration 5/25 | Loss: 0.00111834
Iteration 6/25 | Loss: 0.00111593
Iteration 7/25 | Loss: 0.00111542
Iteration 8/25 | Loss: 0.00111542
Iteration 9/25 | Loss: 0.00111542
Iteration 10/25 | Loss: 0.00111542
Iteration 11/25 | Loss: 0.00111542
Iteration 12/25 | Loss: 0.00111542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00111541582737118, 0.00111541582737118, 0.00111541582737118, 0.00111541582737118, 0.00111541582737118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00111541582737118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91661340
Iteration 2/25 | Loss: 0.00078949
Iteration 3/25 | Loss: 0.00078948
Iteration 4/25 | Loss: 0.00078948
Iteration 5/25 | Loss: 0.00078948
Iteration 6/25 | Loss: 0.00078948
Iteration 7/25 | Loss: 0.00078948
Iteration 8/25 | Loss: 0.00078948
Iteration 9/25 | Loss: 0.00078948
Iteration 10/25 | Loss: 0.00078948
Iteration 11/25 | Loss: 0.00078948
Iteration 12/25 | Loss: 0.00078948
Iteration 13/25 | Loss: 0.00078948
Iteration 14/25 | Loss: 0.00078948
Iteration 15/25 | Loss: 0.00078948
Iteration 16/25 | Loss: 0.00078948
Iteration 17/25 | Loss: 0.00078948
Iteration 18/25 | Loss: 0.00078948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007894794689491391, 0.0007894794689491391, 0.0007894794689491391, 0.0007894794689491391, 0.0007894794689491391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007894794689491391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078948
Iteration 2/1000 | Loss: 0.00010489
Iteration 3/1000 | Loss: 0.00005773
Iteration 4/1000 | Loss: 0.00004192
Iteration 5/1000 | Loss: 0.00003664
Iteration 6/1000 | Loss: 0.00003480
Iteration 7/1000 | Loss: 0.00003386
Iteration 8/1000 | Loss: 0.00003308
Iteration 9/1000 | Loss: 0.00003247
Iteration 10/1000 | Loss: 0.00003211
Iteration 11/1000 | Loss: 0.00003179
Iteration 12/1000 | Loss: 0.00003139
Iteration 13/1000 | Loss: 0.00003107
Iteration 14/1000 | Loss: 0.00003087
Iteration 15/1000 | Loss: 0.00003068
Iteration 16/1000 | Loss: 0.00003052
Iteration 17/1000 | Loss: 0.00003033
Iteration 18/1000 | Loss: 0.00003023
Iteration 19/1000 | Loss: 0.00003016
Iteration 20/1000 | Loss: 0.00003015
Iteration 21/1000 | Loss: 0.00003011
Iteration 22/1000 | Loss: 0.00003009
Iteration 23/1000 | Loss: 0.00003006
Iteration 24/1000 | Loss: 0.00003006
Iteration 25/1000 | Loss: 0.00003006
Iteration 26/1000 | Loss: 0.00003005
Iteration 27/1000 | Loss: 0.00003005
Iteration 28/1000 | Loss: 0.00003004
Iteration 29/1000 | Loss: 0.00003004
Iteration 30/1000 | Loss: 0.00003001
Iteration 31/1000 | Loss: 0.00003000
Iteration 32/1000 | Loss: 0.00002998
Iteration 33/1000 | Loss: 0.00002998
Iteration 34/1000 | Loss: 0.00002997
Iteration 35/1000 | Loss: 0.00002997
Iteration 36/1000 | Loss: 0.00002996
Iteration 37/1000 | Loss: 0.00002996
Iteration 38/1000 | Loss: 0.00002996
Iteration 39/1000 | Loss: 0.00002996
Iteration 40/1000 | Loss: 0.00002996
Iteration 41/1000 | Loss: 0.00002996
Iteration 42/1000 | Loss: 0.00002995
Iteration 43/1000 | Loss: 0.00002995
Iteration 44/1000 | Loss: 0.00002994
Iteration 45/1000 | Loss: 0.00002993
Iteration 46/1000 | Loss: 0.00002993
Iteration 47/1000 | Loss: 0.00002993
Iteration 48/1000 | Loss: 0.00002993
Iteration 49/1000 | Loss: 0.00002993
Iteration 50/1000 | Loss: 0.00002992
Iteration 51/1000 | Loss: 0.00002992
Iteration 52/1000 | Loss: 0.00002992
Iteration 53/1000 | Loss: 0.00002992
Iteration 54/1000 | Loss: 0.00002992
Iteration 55/1000 | Loss: 0.00002991
Iteration 56/1000 | Loss: 0.00002991
Iteration 57/1000 | Loss: 0.00002990
Iteration 58/1000 | Loss: 0.00002990
Iteration 59/1000 | Loss: 0.00002990
Iteration 60/1000 | Loss: 0.00002989
Iteration 61/1000 | Loss: 0.00002989
Iteration 62/1000 | Loss: 0.00002989
Iteration 63/1000 | Loss: 0.00002988
Iteration 64/1000 | Loss: 0.00002988
Iteration 65/1000 | Loss: 0.00002988
Iteration 66/1000 | Loss: 0.00002988
Iteration 67/1000 | Loss: 0.00002988
Iteration 68/1000 | Loss: 0.00002988
Iteration 69/1000 | Loss: 0.00002988
Iteration 70/1000 | Loss: 0.00002988
Iteration 71/1000 | Loss: 0.00002988
Iteration 72/1000 | Loss: 0.00002987
Iteration 73/1000 | Loss: 0.00002986
Iteration 74/1000 | Loss: 0.00002986
Iteration 75/1000 | Loss: 0.00002986
Iteration 76/1000 | Loss: 0.00002986
Iteration 77/1000 | Loss: 0.00002986
Iteration 78/1000 | Loss: 0.00002986
Iteration 79/1000 | Loss: 0.00002986
Iteration 80/1000 | Loss: 0.00002986
Iteration 81/1000 | Loss: 0.00002986
Iteration 82/1000 | Loss: 0.00002985
Iteration 83/1000 | Loss: 0.00002985
Iteration 84/1000 | Loss: 0.00002985
Iteration 85/1000 | Loss: 0.00002985
Iteration 86/1000 | Loss: 0.00002985
Iteration 87/1000 | Loss: 0.00002984
Iteration 88/1000 | Loss: 0.00002984
Iteration 89/1000 | Loss: 0.00002984
Iteration 90/1000 | Loss: 0.00002984
Iteration 91/1000 | Loss: 0.00002983
Iteration 92/1000 | Loss: 0.00002983
Iteration 93/1000 | Loss: 0.00002983
Iteration 94/1000 | Loss: 0.00002983
Iteration 95/1000 | Loss: 0.00002982
Iteration 96/1000 | Loss: 0.00002982
Iteration 97/1000 | Loss: 0.00002982
Iteration 98/1000 | Loss: 0.00002982
Iteration 99/1000 | Loss: 0.00002982
Iteration 100/1000 | Loss: 0.00002982
Iteration 101/1000 | Loss: 0.00002982
Iteration 102/1000 | Loss: 0.00002982
Iteration 103/1000 | Loss: 0.00002982
Iteration 104/1000 | Loss: 0.00002981
Iteration 105/1000 | Loss: 0.00002981
Iteration 106/1000 | Loss: 0.00002981
Iteration 107/1000 | Loss: 0.00002981
Iteration 108/1000 | Loss: 0.00002981
Iteration 109/1000 | Loss: 0.00002981
Iteration 110/1000 | Loss: 0.00002981
Iteration 111/1000 | Loss: 0.00002981
Iteration 112/1000 | Loss: 0.00002981
Iteration 113/1000 | Loss: 0.00002980
Iteration 114/1000 | Loss: 0.00002980
Iteration 115/1000 | Loss: 0.00002980
Iteration 116/1000 | Loss: 0.00002980
Iteration 117/1000 | Loss: 0.00002980
Iteration 118/1000 | Loss: 0.00002980
Iteration 119/1000 | Loss: 0.00002980
Iteration 120/1000 | Loss: 0.00002980
Iteration 121/1000 | Loss: 0.00002980
Iteration 122/1000 | Loss: 0.00002980
Iteration 123/1000 | Loss: 0.00002980
Iteration 124/1000 | Loss: 0.00002980
Iteration 125/1000 | Loss: 0.00002980
Iteration 126/1000 | Loss: 0.00002980
Iteration 127/1000 | Loss: 0.00002980
Iteration 128/1000 | Loss: 0.00002980
Iteration 129/1000 | Loss: 0.00002980
Iteration 130/1000 | Loss: 0.00002980
Iteration 131/1000 | Loss: 0.00002980
Iteration 132/1000 | Loss: 0.00002980
Iteration 133/1000 | Loss: 0.00002980
Iteration 134/1000 | Loss: 0.00002980
Iteration 135/1000 | Loss: 0.00002980
Iteration 136/1000 | Loss: 0.00002979
Iteration 137/1000 | Loss: 0.00002979
Iteration 138/1000 | Loss: 0.00002979
Iteration 139/1000 | Loss: 0.00002979
Iteration 140/1000 | Loss: 0.00002979
Iteration 141/1000 | Loss: 0.00002979
Iteration 142/1000 | Loss: 0.00002979
Iteration 143/1000 | Loss: 0.00002979
Iteration 144/1000 | Loss: 0.00002979
Iteration 145/1000 | Loss: 0.00002979
Iteration 146/1000 | Loss: 0.00002978
Iteration 147/1000 | Loss: 0.00002978
Iteration 148/1000 | Loss: 0.00002978
Iteration 149/1000 | Loss: 0.00002978
Iteration 150/1000 | Loss: 0.00002978
Iteration 151/1000 | Loss: 0.00002978
Iteration 152/1000 | Loss: 0.00002978
Iteration 153/1000 | Loss: 0.00002978
Iteration 154/1000 | Loss: 0.00002978
Iteration 155/1000 | Loss: 0.00002978
Iteration 156/1000 | Loss: 0.00002978
Iteration 157/1000 | Loss: 0.00002978
Iteration 158/1000 | Loss: 0.00002978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.9783610443701036e-05, 2.9783610443701036e-05, 2.9783610443701036e-05, 2.9783610443701036e-05, 2.9783610443701036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9783610443701036e-05

Optimization complete. Final v2v error: 4.200467586517334 mm

Highest mean error: 5.358720779418945 mm for frame 91

Lowest mean error: 3.0682578086853027 mm for frame 43

Saving results

Total time: 46.6181583404541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834499
Iteration 2/25 | Loss: 0.00104436
Iteration 3/25 | Loss: 0.00091075
Iteration 4/25 | Loss: 0.00090341
Iteration 5/25 | Loss: 0.00090158
Iteration 6/25 | Loss: 0.00090111
Iteration 7/25 | Loss: 0.00090111
Iteration 8/25 | Loss: 0.00090111
Iteration 9/25 | Loss: 0.00090111
Iteration 10/25 | Loss: 0.00090111
Iteration 11/25 | Loss: 0.00090111
Iteration 12/25 | Loss: 0.00090111
Iteration 13/25 | Loss: 0.00090111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000901113438885659, 0.000901113438885659, 0.000901113438885659, 0.000901113438885659, 0.000901113438885659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000901113438885659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36306107
Iteration 2/25 | Loss: 0.00064639
Iteration 3/25 | Loss: 0.00064639
Iteration 4/25 | Loss: 0.00064639
Iteration 5/25 | Loss: 0.00064639
Iteration 6/25 | Loss: 0.00064639
Iteration 7/25 | Loss: 0.00064638
Iteration 8/25 | Loss: 0.00064638
Iteration 9/25 | Loss: 0.00064638
Iteration 10/25 | Loss: 0.00064638
Iteration 11/25 | Loss: 0.00064638
Iteration 12/25 | Loss: 0.00064638
Iteration 13/25 | Loss: 0.00064638
Iteration 14/25 | Loss: 0.00064638
Iteration 15/25 | Loss: 0.00064638
Iteration 16/25 | Loss: 0.00064638
Iteration 17/25 | Loss: 0.00064638
Iteration 18/25 | Loss: 0.00064638
Iteration 19/25 | Loss: 0.00064638
Iteration 20/25 | Loss: 0.00064638
Iteration 21/25 | Loss: 0.00064638
Iteration 22/25 | Loss: 0.00064638
Iteration 23/25 | Loss: 0.00064638
Iteration 24/25 | Loss: 0.00064638
Iteration 25/25 | Loss: 0.00064638
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006463828030973673, 0.0006463828030973673, 0.0006463828030973673, 0.0006463828030973673, 0.0006463828030973673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006463828030973673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064638
Iteration 2/1000 | Loss: 0.00005625
Iteration 3/1000 | Loss: 0.00002462
Iteration 4/1000 | Loss: 0.00001574
Iteration 5/1000 | Loss: 0.00001352
Iteration 6/1000 | Loss: 0.00001271
Iteration 7/1000 | Loss: 0.00001207
Iteration 8/1000 | Loss: 0.00001147
Iteration 9/1000 | Loss: 0.00001127
Iteration 10/1000 | Loss: 0.00001102
Iteration 11/1000 | Loss: 0.00001074
Iteration 12/1000 | Loss: 0.00001067
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001058
Iteration 15/1000 | Loss: 0.00001056
Iteration 16/1000 | Loss: 0.00001054
Iteration 17/1000 | Loss: 0.00001049
Iteration 18/1000 | Loss: 0.00001048
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001035
Iteration 21/1000 | Loss: 0.00001032
Iteration 22/1000 | Loss: 0.00001032
Iteration 23/1000 | Loss: 0.00001031
Iteration 24/1000 | Loss: 0.00001031
Iteration 25/1000 | Loss: 0.00001031
Iteration 26/1000 | Loss: 0.00001025
Iteration 27/1000 | Loss: 0.00001025
Iteration 28/1000 | Loss: 0.00001025
Iteration 29/1000 | Loss: 0.00001025
Iteration 30/1000 | Loss: 0.00001021
Iteration 31/1000 | Loss: 0.00001021
Iteration 32/1000 | Loss: 0.00001020
Iteration 33/1000 | Loss: 0.00001020
Iteration 34/1000 | Loss: 0.00001020
Iteration 35/1000 | Loss: 0.00001019
Iteration 36/1000 | Loss: 0.00001019
Iteration 37/1000 | Loss: 0.00001019
Iteration 38/1000 | Loss: 0.00001019
Iteration 39/1000 | Loss: 0.00001018
Iteration 40/1000 | Loss: 0.00001018
Iteration 41/1000 | Loss: 0.00001018
Iteration 42/1000 | Loss: 0.00001017
Iteration 43/1000 | Loss: 0.00001017
Iteration 44/1000 | Loss: 0.00001016
Iteration 45/1000 | Loss: 0.00001016
Iteration 46/1000 | Loss: 0.00001016
Iteration 47/1000 | Loss: 0.00001016
Iteration 48/1000 | Loss: 0.00001016
Iteration 49/1000 | Loss: 0.00001015
Iteration 50/1000 | Loss: 0.00001015
Iteration 51/1000 | Loss: 0.00001015
Iteration 52/1000 | Loss: 0.00001015
Iteration 53/1000 | Loss: 0.00001015
Iteration 54/1000 | Loss: 0.00001015
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001014
Iteration 58/1000 | Loss: 0.00001014
Iteration 59/1000 | Loss: 0.00001013
Iteration 60/1000 | Loss: 0.00001013
Iteration 61/1000 | Loss: 0.00001013
Iteration 62/1000 | Loss: 0.00001013
Iteration 63/1000 | Loss: 0.00001012
Iteration 64/1000 | Loss: 0.00001012
Iteration 65/1000 | Loss: 0.00001012
Iteration 66/1000 | Loss: 0.00001012
Iteration 67/1000 | Loss: 0.00001012
Iteration 68/1000 | Loss: 0.00001012
Iteration 69/1000 | Loss: 0.00001012
Iteration 70/1000 | Loss: 0.00001011
Iteration 71/1000 | Loss: 0.00001011
Iteration 72/1000 | Loss: 0.00001011
Iteration 73/1000 | Loss: 0.00001011
Iteration 74/1000 | Loss: 0.00001011
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001011
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001010
Iteration 82/1000 | Loss: 0.00001010
Iteration 83/1000 | Loss: 0.00001010
Iteration 84/1000 | Loss: 0.00001010
Iteration 85/1000 | Loss: 0.00001010
Iteration 86/1000 | Loss: 0.00001010
Iteration 87/1000 | Loss: 0.00001010
Iteration 88/1000 | Loss: 0.00001010
Iteration 89/1000 | Loss: 0.00001010
Iteration 90/1000 | Loss: 0.00001009
Iteration 91/1000 | Loss: 0.00001009
Iteration 92/1000 | Loss: 0.00001009
Iteration 93/1000 | Loss: 0.00001009
Iteration 94/1000 | Loss: 0.00001009
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001009
Iteration 97/1000 | Loss: 0.00001009
Iteration 98/1000 | Loss: 0.00001009
Iteration 99/1000 | Loss: 0.00001009
Iteration 100/1000 | Loss: 0.00001009
Iteration 101/1000 | Loss: 0.00001009
Iteration 102/1000 | Loss: 0.00001009
Iteration 103/1000 | Loss: 0.00001009
Iteration 104/1000 | Loss: 0.00001009
Iteration 105/1000 | Loss: 0.00001009
Iteration 106/1000 | Loss: 0.00001009
Iteration 107/1000 | Loss: 0.00001009
Iteration 108/1000 | Loss: 0.00001009
Iteration 109/1000 | Loss: 0.00001009
Iteration 110/1000 | Loss: 0.00001009
Iteration 111/1000 | Loss: 0.00001009
Iteration 112/1000 | Loss: 0.00001009
Iteration 113/1000 | Loss: 0.00001009
Iteration 114/1000 | Loss: 0.00001009
Iteration 115/1000 | Loss: 0.00001008
Iteration 116/1000 | Loss: 0.00001008
Iteration 117/1000 | Loss: 0.00001008
Iteration 118/1000 | Loss: 0.00001008
Iteration 119/1000 | Loss: 0.00001008
Iteration 120/1000 | Loss: 0.00001008
Iteration 121/1000 | Loss: 0.00001008
Iteration 122/1000 | Loss: 0.00001008
Iteration 123/1000 | Loss: 0.00001008
Iteration 124/1000 | Loss: 0.00001008
Iteration 125/1000 | Loss: 0.00001008
Iteration 126/1000 | Loss: 0.00001008
Iteration 127/1000 | Loss: 0.00001008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.0084939276566729e-05, 1.0084939276566729e-05, 1.0084939276566729e-05, 1.0084939276566729e-05, 1.0084939276566729e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0084939276566729e-05

Optimization complete. Final v2v error: 2.700713872909546 mm

Highest mean error: 3.656601905822754 mm for frame 66

Lowest mean error: 2.176805019378662 mm for frame 184

Saving results

Total time: 36.08846187591553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049562
Iteration 2/25 | Loss: 0.00182202
Iteration 3/25 | Loss: 0.00136547
Iteration 4/25 | Loss: 0.00110886
Iteration 5/25 | Loss: 0.00119770
Iteration 6/25 | Loss: 0.00105521
Iteration 7/25 | Loss: 0.00100993
Iteration 8/25 | Loss: 0.00096853
Iteration 9/25 | Loss: 0.00094392
Iteration 10/25 | Loss: 0.00093458
Iteration 11/25 | Loss: 0.00092873
Iteration 12/25 | Loss: 0.00093005
Iteration 13/25 | Loss: 0.00092501
Iteration 14/25 | Loss: 0.00092311
Iteration 15/25 | Loss: 0.00092230
Iteration 16/25 | Loss: 0.00092202
Iteration 17/25 | Loss: 0.00092497
Iteration 18/25 | Loss: 0.00092436
Iteration 19/25 | Loss: 0.00092070
Iteration 20/25 | Loss: 0.00091895
Iteration 21/25 | Loss: 0.00091752
Iteration 22/25 | Loss: 0.00091725
Iteration 23/25 | Loss: 0.00091711
Iteration 24/25 | Loss: 0.00091705
Iteration 25/25 | Loss: 0.00091705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41464162
Iteration 2/25 | Loss: 0.00056114
Iteration 3/25 | Loss: 0.00056114
Iteration 4/25 | Loss: 0.00056114
Iteration 5/25 | Loss: 0.00056114
Iteration 6/25 | Loss: 0.00056114
Iteration 7/25 | Loss: 0.00056114
Iteration 8/25 | Loss: 0.00056114
Iteration 9/25 | Loss: 0.00056114
Iteration 10/25 | Loss: 0.00056114
Iteration 11/25 | Loss: 0.00056114
Iteration 12/25 | Loss: 0.00056114
Iteration 13/25 | Loss: 0.00056114
Iteration 14/25 | Loss: 0.00056114
Iteration 15/25 | Loss: 0.00056114
Iteration 16/25 | Loss: 0.00056114
Iteration 17/25 | Loss: 0.00056114
Iteration 18/25 | Loss: 0.00056114
Iteration 19/25 | Loss: 0.00056114
Iteration 20/25 | Loss: 0.00056114
Iteration 21/25 | Loss: 0.00056114
Iteration 22/25 | Loss: 0.00056114
Iteration 23/25 | Loss: 0.00056114
Iteration 24/25 | Loss: 0.00056114
Iteration 25/25 | Loss: 0.00056114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056114
Iteration 2/1000 | Loss: 0.00005257
Iteration 3/1000 | Loss: 0.00064124
Iteration 4/1000 | Loss: 0.00001895
Iteration 5/1000 | Loss: 0.00001647
Iteration 6/1000 | Loss: 0.00001553
Iteration 7/1000 | Loss: 0.00017738
Iteration 8/1000 | Loss: 0.00001492
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001379
Iteration 11/1000 | Loss: 0.00001357
Iteration 12/1000 | Loss: 0.00001335
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001308
Iteration 15/1000 | Loss: 0.00001304
Iteration 16/1000 | Loss: 0.00001302
Iteration 17/1000 | Loss: 0.00001302
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001292
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001292
Iteration 36/1000 | Loss: 0.00001292
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001291
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001288
Iteration 59/1000 | Loss: 0.00001288
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001287
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001287
Iteration 80/1000 | Loss: 0.00001287
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001283
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001283
Iteration 123/1000 | Loss: 0.00001283
Iteration 124/1000 | Loss: 0.00001283
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001283
Iteration 129/1000 | Loss: 0.00001283
Iteration 130/1000 | Loss: 0.00001283
Iteration 131/1000 | Loss: 0.00001283
Iteration 132/1000 | Loss: 0.00001283
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001283
Iteration 135/1000 | Loss: 0.00001283
Iteration 136/1000 | Loss: 0.00001283
Iteration 137/1000 | Loss: 0.00001283
Iteration 138/1000 | Loss: 0.00001283
Iteration 139/1000 | Loss: 0.00001283
Iteration 140/1000 | Loss: 0.00001283
Iteration 141/1000 | Loss: 0.00001283
Iteration 142/1000 | Loss: 0.00001283
Iteration 143/1000 | Loss: 0.00001283
Iteration 144/1000 | Loss: 0.00001283
Iteration 145/1000 | Loss: 0.00001283
Iteration 146/1000 | Loss: 0.00001283
Iteration 147/1000 | Loss: 0.00001283
Iteration 148/1000 | Loss: 0.00001283
Iteration 149/1000 | Loss: 0.00001283
Iteration 150/1000 | Loss: 0.00001283
Iteration 151/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.282954963244265e-05, 1.282954963244265e-05, 1.282954963244265e-05, 1.282954963244265e-05, 1.282954963244265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.282954963244265e-05

Optimization complete. Final v2v error: 3.0400359630584717 mm

Highest mean error: 3.325348377227783 mm for frame 43

Lowest mean error: 2.6462130546569824 mm for frame 21

Saving results

Total time: 67.49136805534363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628510
Iteration 2/25 | Loss: 0.00100715
Iteration 3/25 | Loss: 0.00089559
Iteration 4/25 | Loss: 0.00087889
Iteration 5/25 | Loss: 0.00087285
Iteration 6/25 | Loss: 0.00087094
Iteration 7/25 | Loss: 0.00087094
Iteration 8/25 | Loss: 0.00087094
Iteration 9/25 | Loss: 0.00087094
Iteration 10/25 | Loss: 0.00087094
Iteration 11/25 | Loss: 0.00087094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008709371322765946, 0.0008709371322765946, 0.0008709371322765946, 0.0008709371322765946, 0.0008709371322765946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008709371322765946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66732025
Iteration 2/25 | Loss: 0.00056957
Iteration 3/25 | Loss: 0.00056957
Iteration 4/25 | Loss: 0.00056957
Iteration 5/25 | Loss: 0.00056957
Iteration 6/25 | Loss: 0.00056957
Iteration 7/25 | Loss: 0.00056957
Iteration 8/25 | Loss: 0.00056957
Iteration 9/25 | Loss: 0.00056957
Iteration 10/25 | Loss: 0.00056957
Iteration 11/25 | Loss: 0.00056957
Iteration 12/25 | Loss: 0.00056957
Iteration 13/25 | Loss: 0.00056957
Iteration 14/25 | Loss: 0.00056957
Iteration 15/25 | Loss: 0.00056957
Iteration 16/25 | Loss: 0.00056957
Iteration 17/25 | Loss: 0.00056957
Iteration 18/25 | Loss: 0.00056957
Iteration 19/25 | Loss: 0.00056957
Iteration 20/25 | Loss: 0.00056957
Iteration 21/25 | Loss: 0.00056957
Iteration 22/25 | Loss: 0.00056957
Iteration 23/25 | Loss: 0.00056957
Iteration 24/25 | Loss: 0.00056957
Iteration 25/25 | Loss: 0.00056957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056957
Iteration 2/1000 | Loss: 0.00002902
Iteration 3/1000 | Loss: 0.00001613
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001340
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001227
Iteration 8/1000 | Loss: 0.00001192
Iteration 9/1000 | Loss: 0.00001163
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001125
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001122
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001122
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001118
Iteration 27/1000 | Loss: 0.00001118
Iteration 28/1000 | Loss: 0.00001118
Iteration 29/1000 | Loss: 0.00001117
Iteration 30/1000 | Loss: 0.00001114
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001114
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001113
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001113
Iteration 42/1000 | Loss: 0.00001112
Iteration 43/1000 | Loss: 0.00001112
Iteration 44/1000 | Loss: 0.00001111
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001109
Iteration 59/1000 | Loss: 0.00001109
Iteration 60/1000 | Loss: 0.00001108
Iteration 61/1000 | Loss: 0.00001108
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001107
Iteration 64/1000 | Loss: 0.00001107
Iteration 65/1000 | Loss: 0.00001107
Iteration 66/1000 | Loss: 0.00001107
Iteration 67/1000 | Loss: 0.00001107
Iteration 68/1000 | Loss: 0.00001107
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001106
Iteration 72/1000 | Loss: 0.00001106
Iteration 73/1000 | Loss: 0.00001104
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001099
Iteration 91/1000 | Loss: 0.00001099
Iteration 92/1000 | Loss: 0.00001099
Iteration 93/1000 | Loss: 0.00001098
Iteration 94/1000 | Loss: 0.00001098
Iteration 95/1000 | Loss: 0.00001098
Iteration 96/1000 | Loss: 0.00001098
Iteration 97/1000 | Loss: 0.00001098
Iteration 98/1000 | Loss: 0.00001098
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001097
Iteration 101/1000 | Loss: 0.00001097
Iteration 102/1000 | Loss: 0.00001097
Iteration 103/1000 | Loss: 0.00001097
Iteration 104/1000 | Loss: 0.00001096
Iteration 105/1000 | Loss: 0.00001096
Iteration 106/1000 | Loss: 0.00001096
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001096
Iteration 109/1000 | Loss: 0.00001096
Iteration 110/1000 | Loss: 0.00001096
Iteration 111/1000 | Loss: 0.00001096
Iteration 112/1000 | Loss: 0.00001096
Iteration 113/1000 | Loss: 0.00001096
Iteration 114/1000 | Loss: 0.00001096
Iteration 115/1000 | Loss: 0.00001096
Iteration 116/1000 | Loss: 0.00001096
Iteration 117/1000 | Loss: 0.00001095
Iteration 118/1000 | Loss: 0.00001095
Iteration 119/1000 | Loss: 0.00001095
Iteration 120/1000 | Loss: 0.00001095
Iteration 121/1000 | Loss: 0.00001095
Iteration 122/1000 | Loss: 0.00001095
Iteration 123/1000 | Loss: 0.00001095
Iteration 124/1000 | Loss: 0.00001095
Iteration 125/1000 | Loss: 0.00001095
Iteration 126/1000 | Loss: 0.00001095
Iteration 127/1000 | Loss: 0.00001095
Iteration 128/1000 | Loss: 0.00001095
Iteration 129/1000 | Loss: 0.00001095
Iteration 130/1000 | Loss: 0.00001095
Iteration 131/1000 | Loss: 0.00001095
Iteration 132/1000 | Loss: 0.00001094
Iteration 133/1000 | Loss: 0.00001094
Iteration 134/1000 | Loss: 0.00001094
Iteration 135/1000 | Loss: 0.00001094
Iteration 136/1000 | Loss: 0.00001094
Iteration 137/1000 | Loss: 0.00001094
Iteration 138/1000 | Loss: 0.00001094
Iteration 139/1000 | Loss: 0.00001094
Iteration 140/1000 | Loss: 0.00001094
Iteration 141/1000 | Loss: 0.00001094
Iteration 142/1000 | Loss: 0.00001094
Iteration 143/1000 | Loss: 0.00001094
Iteration 144/1000 | Loss: 0.00001094
Iteration 145/1000 | Loss: 0.00001094
Iteration 146/1000 | Loss: 0.00001093
Iteration 147/1000 | Loss: 0.00001093
Iteration 148/1000 | Loss: 0.00001093
Iteration 149/1000 | Loss: 0.00001093
Iteration 150/1000 | Loss: 0.00001093
Iteration 151/1000 | Loss: 0.00001093
Iteration 152/1000 | Loss: 0.00001093
Iteration 153/1000 | Loss: 0.00001092
Iteration 154/1000 | Loss: 0.00001092
Iteration 155/1000 | Loss: 0.00001092
Iteration 156/1000 | Loss: 0.00001092
Iteration 157/1000 | Loss: 0.00001092
Iteration 158/1000 | Loss: 0.00001092
Iteration 159/1000 | Loss: 0.00001092
Iteration 160/1000 | Loss: 0.00001092
Iteration 161/1000 | Loss: 0.00001092
Iteration 162/1000 | Loss: 0.00001092
Iteration 163/1000 | Loss: 0.00001092
Iteration 164/1000 | Loss: 0.00001092
Iteration 165/1000 | Loss: 0.00001091
Iteration 166/1000 | Loss: 0.00001091
Iteration 167/1000 | Loss: 0.00001091
Iteration 168/1000 | Loss: 0.00001091
Iteration 169/1000 | Loss: 0.00001091
Iteration 170/1000 | Loss: 0.00001091
Iteration 171/1000 | Loss: 0.00001091
Iteration 172/1000 | Loss: 0.00001091
Iteration 173/1000 | Loss: 0.00001090
Iteration 174/1000 | Loss: 0.00001090
Iteration 175/1000 | Loss: 0.00001090
Iteration 176/1000 | Loss: 0.00001090
Iteration 177/1000 | Loss: 0.00001090
Iteration 178/1000 | Loss: 0.00001090
Iteration 179/1000 | Loss: 0.00001090
Iteration 180/1000 | Loss: 0.00001090
Iteration 181/1000 | Loss: 0.00001090
Iteration 182/1000 | Loss: 0.00001090
Iteration 183/1000 | Loss: 0.00001090
Iteration 184/1000 | Loss: 0.00001090
Iteration 185/1000 | Loss: 0.00001090
Iteration 186/1000 | Loss: 0.00001089
Iteration 187/1000 | Loss: 0.00001089
Iteration 188/1000 | Loss: 0.00001089
Iteration 189/1000 | Loss: 0.00001089
Iteration 190/1000 | Loss: 0.00001089
Iteration 191/1000 | Loss: 0.00001088
Iteration 192/1000 | Loss: 0.00001088
Iteration 193/1000 | Loss: 0.00001088
Iteration 194/1000 | Loss: 0.00001088
Iteration 195/1000 | Loss: 0.00001088
Iteration 196/1000 | Loss: 0.00001088
Iteration 197/1000 | Loss: 0.00001088
Iteration 198/1000 | Loss: 0.00001088
Iteration 199/1000 | Loss: 0.00001088
Iteration 200/1000 | Loss: 0.00001088
Iteration 201/1000 | Loss: 0.00001088
Iteration 202/1000 | Loss: 0.00001088
Iteration 203/1000 | Loss: 0.00001088
Iteration 204/1000 | Loss: 0.00001088
Iteration 205/1000 | Loss: 0.00001088
Iteration 206/1000 | Loss: 0.00001087
Iteration 207/1000 | Loss: 0.00001087
Iteration 208/1000 | Loss: 0.00001087
Iteration 209/1000 | Loss: 0.00001087
Iteration 210/1000 | Loss: 0.00001087
Iteration 211/1000 | Loss: 0.00001087
Iteration 212/1000 | Loss: 0.00001087
Iteration 213/1000 | Loss: 0.00001087
Iteration 214/1000 | Loss: 0.00001087
Iteration 215/1000 | Loss: 0.00001087
Iteration 216/1000 | Loss: 0.00001087
Iteration 217/1000 | Loss: 0.00001087
Iteration 218/1000 | Loss: 0.00001087
Iteration 219/1000 | Loss: 0.00001087
Iteration 220/1000 | Loss: 0.00001087
Iteration 221/1000 | Loss: 0.00001087
Iteration 222/1000 | Loss: 0.00001087
Iteration 223/1000 | Loss: 0.00001087
Iteration 224/1000 | Loss: 0.00001087
Iteration 225/1000 | Loss: 0.00001087
Iteration 226/1000 | Loss: 0.00001086
Iteration 227/1000 | Loss: 0.00001086
Iteration 228/1000 | Loss: 0.00001086
Iteration 229/1000 | Loss: 0.00001086
Iteration 230/1000 | Loss: 0.00001086
Iteration 231/1000 | Loss: 0.00001086
Iteration 232/1000 | Loss: 0.00001086
Iteration 233/1000 | Loss: 0.00001086
Iteration 234/1000 | Loss: 0.00001086
Iteration 235/1000 | Loss: 0.00001086
Iteration 236/1000 | Loss: 0.00001086
Iteration 237/1000 | Loss: 0.00001086
Iteration 238/1000 | Loss: 0.00001086
Iteration 239/1000 | Loss: 0.00001086
Iteration 240/1000 | Loss: 0.00001086
Iteration 241/1000 | Loss: 0.00001086
Iteration 242/1000 | Loss: 0.00001086
Iteration 243/1000 | Loss: 0.00001086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.0864398973353673e-05, 1.0864398973353673e-05, 1.0864398973353673e-05, 1.0864398973353673e-05, 1.0864398973353673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0864398973353673e-05

Optimization complete. Final v2v error: 2.8560235500335693 mm

Highest mean error: 3.025545120239258 mm for frame 131

Lowest mean error: 2.619382858276367 mm for frame 160

Saving results

Total time: 39.934762954711914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059424
Iteration 2/25 | Loss: 0.00218668
Iteration 3/25 | Loss: 0.00179387
Iteration 4/25 | Loss: 0.00098572
Iteration 5/25 | Loss: 0.00095929
Iteration 6/25 | Loss: 0.00094923
Iteration 7/25 | Loss: 0.00094752
Iteration 8/25 | Loss: 0.00094720
Iteration 9/25 | Loss: 0.00094635
Iteration 10/25 | Loss: 0.00094720
Iteration 11/25 | Loss: 0.00094624
Iteration 12/25 | Loss: 0.00094624
Iteration 13/25 | Loss: 0.00094624
Iteration 14/25 | Loss: 0.00094624
Iteration 15/25 | Loss: 0.00094624
Iteration 16/25 | Loss: 0.00094624
Iteration 17/25 | Loss: 0.00094624
Iteration 18/25 | Loss: 0.00094623
Iteration 19/25 | Loss: 0.00094623
Iteration 20/25 | Loss: 0.00094623
Iteration 21/25 | Loss: 0.00094623
Iteration 22/25 | Loss: 0.00094623
Iteration 23/25 | Loss: 0.00094623
Iteration 24/25 | Loss: 0.00094623
Iteration 25/25 | Loss: 0.00094623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31599545
Iteration 2/25 | Loss: 0.00076204
Iteration 3/25 | Loss: 0.00076203
Iteration 4/25 | Loss: 0.00076203
Iteration 5/25 | Loss: 0.00076203
Iteration 6/25 | Loss: 0.00076203
Iteration 7/25 | Loss: 0.00076203
Iteration 8/25 | Loss: 0.00076203
Iteration 9/25 | Loss: 0.00076203
Iteration 10/25 | Loss: 0.00076203
Iteration 11/25 | Loss: 0.00076203
Iteration 12/25 | Loss: 0.00076203
Iteration 13/25 | Loss: 0.00076203
Iteration 14/25 | Loss: 0.00076203
Iteration 15/25 | Loss: 0.00076203
Iteration 16/25 | Loss: 0.00076203
Iteration 17/25 | Loss: 0.00076203
Iteration 18/25 | Loss: 0.00076203
Iteration 19/25 | Loss: 0.00076203
Iteration 20/25 | Loss: 0.00076203
Iteration 21/25 | Loss: 0.00076203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007620290853083134, 0.0007620290853083134, 0.0007620290853083134, 0.0007620290853083134, 0.0007620290853083134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007620290853083134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076203
Iteration 2/1000 | Loss: 0.00005378
Iteration 3/1000 | Loss: 0.00009492
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001663
Iteration 6/1000 | Loss: 0.00012109
Iteration 7/1000 | Loss: 0.00011546
Iteration 8/1000 | Loss: 0.00013026
Iteration 9/1000 | Loss: 0.00006465
Iteration 10/1000 | Loss: 0.00001537
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00008146
Iteration 13/1000 | Loss: 0.00008359
Iteration 14/1000 | Loss: 0.00006702
Iteration 15/1000 | Loss: 0.00011230
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00002423
Iteration 20/1000 | Loss: 0.00019605
Iteration 21/1000 | Loss: 0.00001901
Iteration 22/1000 | Loss: 0.00003948
Iteration 23/1000 | Loss: 0.00002367
Iteration 24/1000 | Loss: 0.00001361
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001320
Iteration 27/1000 | Loss: 0.00001318
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001316
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001314
Iteration 35/1000 | Loss: 0.00001314
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001314
Iteration 38/1000 | Loss: 0.00001314
Iteration 39/1000 | Loss: 0.00001314
Iteration 40/1000 | Loss: 0.00001314
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001314
Iteration 44/1000 | Loss: 0.00001313
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001312
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001402
Iteration 52/1000 | Loss: 0.00001303
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00001301
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001299
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001299
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001298
Iteration 104/1000 | Loss: 0.00001298
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001295
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001294
Iteration 112/1000 | Loss: 0.00001294
Iteration 113/1000 | Loss: 0.00001294
Iteration 114/1000 | Loss: 0.00001294
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001294
Iteration 118/1000 | Loss: 0.00001293
Iteration 119/1000 | Loss: 0.00001293
Iteration 120/1000 | Loss: 0.00001293
Iteration 121/1000 | Loss: 0.00001293
Iteration 122/1000 | Loss: 0.00001293
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001293
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001293
Iteration 127/1000 | Loss: 0.00001293
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001293
Iteration 131/1000 | Loss: 0.00001292
Iteration 132/1000 | Loss: 0.00001292
Iteration 133/1000 | Loss: 0.00001292
Iteration 134/1000 | Loss: 0.00001292
Iteration 135/1000 | Loss: 0.00001292
Iteration 136/1000 | Loss: 0.00001292
Iteration 137/1000 | Loss: 0.00001292
Iteration 138/1000 | Loss: 0.00001292
Iteration 139/1000 | Loss: 0.00001292
Iteration 140/1000 | Loss: 0.00001292
Iteration 141/1000 | Loss: 0.00001292
Iteration 142/1000 | Loss: 0.00001292
Iteration 143/1000 | Loss: 0.00001292
Iteration 144/1000 | Loss: 0.00001292
Iteration 145/1000 | Loss: 0.00001292
Iteration 146/1000 | Loss: 0.00001292
Iteration 147/1000 | Loss: 0.00001292
Iteration 148/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2921482266392559e-05, 1.2921482266392559e-05, 1.2921482266392559e-05, 1.2921482266392559e-05, 1.2921482266392559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2921482266392559e-05

Optimization complete. Final v2v error: 3.1136951446533203 mm

Highest mean error: 3.4147303104400635 mm for frame 36

Lowest mean error: 2.72584867477417 mm for frame 174

Saving results

Total time: 63.94679021835327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829175
Iteration 2/25 | Loss: 0.00107965
Iteration 3/25 | Loss: 0.00097177
Iteration 4/25 | Loss: 0.00094299
Iteration 5/25 | Loss: 0.00092867
Iteration 6/25 | Loss: 0.00092500
Iteration 7/25 | Loss: 0.00092354
Iteration 8/25 | Loss: 0.00092296
Iteration 9/25 | Loss: 0.00092296
Iteration 10/25 | Loss: 0.00092296
Iteration 11/25 | Loss: 0.00092296
Iteration 12/25 | Loss: 0.00092296
Iteration 13/25 | Loss: 0.00092296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009229604620486498, 0.0009229604620486498, 0.0009229604620486498, 0.0009229604620486498, 0.0009229604620486498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009229604620486498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59155142
Iteration 2/25 | Loss: 0.00094878
Iteration 3/25 | Loss: 0.00094878
Iteration 4/25 | Loss: 0.00094878
Iteration 5/25 | Loss: 0.00094878
Iteration 6/25 | Loss: 0.00094878
Iteration 7/25 | Loss: 0.00094878
Iteration 8/25 | Loss: 0.00094878
Iteration 9/25 | Loss: 0.00094878
Iteration 10/25 | Loss: 0.00094878
Iteration 11/25 | Loss: 0.00094878
Iteration 12/25 | Loss: 0.00094878
Iteration 13/25 | Loss: 0.00094878
Iteration 14/25 | Loss: 0.00094878
Iteration 15/25 | Loss: 0.00094878
Iteration 16/25 | Loss: 0.00094878
Iteration 17/25 | Loss: 0.00094878
Iteration 18/25 | Loss: 0.00094878
Iteration 19/25 | Loss: 0.00094878
Iteration 20/25 | Loss: 0.00094878
Iteration 21/25 | Loss: 0.00094878
Iteration 22/25 | Loss: 0.00094878
Iteration 23/25 | Loss: 0.00094878
Iteration 24/25 | Loss: 0.00094878
Iteration 25/25 | Loss: 0.00094878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094878
Iteration 2/1000 | Loss: 0.00007038
Iteration 3/1000 | Loss: 0.00003255
Iteration 4/1000 | Loss: 0.00002363
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00001868
Iteration 7/1000 | Loss: 0.00001791
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00001685
Iteration 11/1000 | Loss: 0.00001678
Iteration 12/1000 | Loss: 0.00001676
Iteration 13/1000 | Loss: 0.00001674
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001672
Iteration 16/1000 | Loss: 0.00001671
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001670
Iteration 19/1000 | Loss: 0.00001669
Iteration 20/1000 | Loss: 0.00001668
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001663
Iteration 23/1000 | Loss: 0.00001661
Iteration 24/1000 | Loss: 0.00001660
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001642
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001640
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001634
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001633
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001631
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001627
Iteration 69/1000 | Loss: 0.00001627
Iteration 70/1000 | Loss: 0.00001627
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001621
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001621
Iteration 112/1000 | Loss: 0.00001621
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001621
Iteration 121/1000 | Loss: 0.00001621
Iteration 122/1000 | Loss: 0.00001620
Iteration 123/1000 | Loss: 0.00001620
Iteration 124/1000 | Loss: 0.00001620
Iteration 125/1000 | Loss: 0.00001620
Iteration 126/1000 | Loss: 0.00001620
Iteration 127/1000 | Loss: 0.00001620
Iteration 128/1000 | Loss: 0.00001620
Iteration 129/1000 | Loss: 0.00001620
Iteration 130/1000 | Loss: 0.00001620
Iteration 131/1000 | Loss: 0.00001620
Iteration 132/1000 | Loss: 0.00001620
Iteration 133/1000 | Loss: 0.00001620
Iteration 134/1000 | Loss: 0.00001620
Iteration 135/1000 | Loss: 0.00001620
Iteration 136/1000 | Loss: 0.00001619
Iteration 137/1000 | Loss: 0.00001619
Iteration 138/1000 | Loss: 0.00001619
Iteration 139/1000 | Loss: 0.00001619
Iteration 140/1000 | Loss: 0.00001619
Iteration 141/1000 | Loss: 0.00001619
Iteration 142/1000 | Loss: 0.00001619
Iteration 143/1000 | Loss: 0.00001619
Iteration 144/1000 | Loss: 0.00001619
Iteration 145/1000 | Loss: 0.00001619
Iteration 146/1000 | Loss: 0.00001619
Iteration 147/1000 | Loss: 0.00001619
Iteration 148/1000 | Loss: 0.00001619
Iteration 149/1000 | Loss: 0.00001619
Iteration 150/1000 | Loss: 0.00001619
Iteration 151/1000 | Loss: 0.00001619
Iteration 152/1000 | Loss: 0.00001619
Iteration 153/1000 | Loss: 0.00001619
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001619
Iteration 158/1000 | Loss: 0.00001619
Iteration 159/1000 | Loss: 0.00001619
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001619
Iteration 164/1000 | Loss: 0.00001619
Iteration 165/1000 | Loss: 0.00001619
Iteration 166/1000 | Loss: 0.00001619
Iteration 167/1000 | Loss: 0.00001619
Iteration 168/1000 | Loss: 0.00001619
Iteration 169/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.6192765542655252e-05, 1.6192765542655252e-05, 1.6192765542655252e-05, 1.6192765542655252e-05, 1.6192765542655252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6192765542655252e-05

Optimization complete. Final v2v error: 3.3018996715545654 mm

Highest mean error: 4.964386940002441 mm for frame 136

Lowest mean error: 2.5140342712402344 mm for frame 112

Saving results

Total time: 38.97171497344971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_nl_5316/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_nl_5316/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784589
Iteration 2/25 | Loss: 0.00117232
Iteration 3/25 | Loss: 0.00094784
Iteration 4/25 | Loss: 0.00091430
Iteration 5/25 | Loss: 0.00090851
Iteration 6/25 | Loss: 0.00090744
Iteration 7/25 | Loss: 0.00090744
Iteration 8/25 | Loss: 0.00090744
Iteration 9/25 | Loss: 0.00090744
Iteration 10/25 | Loss: 0.00090744
Iteration 11/25 | Loss: 0.00090744
Iteration 12/25 | Loss: 0.00090744
Iteration 13/25 | Loss: 0.00090744
Iteration 14/25 | Loss: 0.00090744
Iteration 15/25 | Loss: 0.00090744
Iteration 16/25 | Loss: 0.00090744
Iteration 17/25 | Loss: 0.00090744
Iteration 18/25 | Loss: 0.00090744
Iteration 19/25 | Loss: 0.00090744
Iteration 20/25 | Loss: 0.00090744
Iteration 21/25 | Loss: 0.00090744
Iteration 22/25 | Loss: 0.00090744
Iteration 23/25 | Loss: 0.00090744
Iteration 24/25 | Loss: 0.00090744
Iteration 25/25 | Loss: 0.00090744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32392156
Iteration 2/25 | Loss: 0.00052117
Iteration 3/25 | Loss: 0.00052117
Iteration 4/25 | Loss: 0.00052117
Iteration 5/25 | Loss: 0.00052117
Iteration 6/25 | Loss: 0.00052117
Iteration 7/25 | Loss: 0.00052117
Iteration 8/25 | Loss: 0.00052117
Iteration 9/25 | Loss: 0.00052117
Iteration 10/25 | Loss: 0.00052117
Iteration 11/25 | Loss: 0.00052117
Iteration 12/25 | Loss: 0.00052117
Iteration 13/25 | Loss: 0.00052117
Iteration 14/25 | Loss: 0.00052117
Iteration 15/25 | Loss: 0.00052117
Iteration 16/25 | Loss: 0.00052117
Iteration 17/25 | Loss: 0.00052117
Iteration 18/25 | Loss: 0.00052117
Iteration 19/25 | Loss: 0.00052117
Iteration 20/25 | Loss: 0.00052117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005211704992689192, 0.0005211704992689192, 0.0005211704992689192, 0.0005211704992689192, 0.0005211704992689192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005211704992689192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052117
Iteration 2/1000 | Loss: 0.00004786
Iteration 3/1000 | Loss: 0.00002822
Iteration 4/1000 | Loss: 0.00001786
Iteration 5/1000 | Loss: 0.00001631
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001439
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001364
Iteration 10/1000 | Loss: 0.00001331
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001269
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001252
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001252
Iteration 25/1000 | Loss: 0.00001251
Iteration 26/1000 | Loss: 0.00001251
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001250
Iteration 32/1000 | Loss: 0.00001250
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001248
Iteration 37/1000 | Loss: 0.00001247
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00001246
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001245
Iteration 42/1000 | Loss: 0.00001245
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00001240
Iteration 58/1000 | Loss: 0.00001239
Iteration 59/1000 | Loss: 0.00001239
Iteration 60/1000 | Loss: 0.00001239
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001237
Iteration 82/1000 | Loss: 0.00001237
Iteration 83/1000 | Loss: 0.00001237
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001235
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001234
Iteration 113/1000 | Loss: 0.00001234
Iteration 114/1000 | Loss: 0.00001234
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001233
Iteration 118/1000 | Loss: 0.00001233
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001232
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001232
Iteration 132/1000 | Loss: 0.00001232
Iteration 133/1000 | Loss: 0.00001232
Iteration 134/1000 | Loss: 0.00001232
Iteration 135/1000 | Loss: 0.00001232
Iteration 136/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.2323436749284156e-05, 1.2323436749284156e-05, 1.2323436749284156e-05, 1.2323436749284156e-05, 1.2323436749284156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2323436749284156e-05

Optimization complete. Final v2v error: 3.003997564315796 mm

Highest mean error: 3.300936460494995 mm for frame 206

Lowest mean error: 2.746267557144165 mm for frame 27

Saving results

Total time: 41.171751499176025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862661
Iteration 2/25 | Loss: 0.00126420
Iteration 3/25 | Loss: 0.00098488
Iteration 4/25 | Loss: 0.00095655
Iteration 5/25 | Loss: 0.00094421
Iteration 6/25 | Loss: 0.00094004
Iteration 7/25 | Loss: 0.00093954
Iteration 8/25 | Loss: 0.00093954
Iteration 9/25 | Loss: 0.00093954
Iteration 10/25 | Loss: 0.00093954
Iteration 11/25 | Loss: 0.00093954
Iteration 12/25 | Loss: 0.00093954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009395449887961149, 0.0009395449887961149, 0.0009395449887961149, 0.0009395449887961149, 0.0009395449887961149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009395449887961149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12574148
Iteration 2/25 | Loss: 0.00072796
Iteration 3/25 | Loss: 0.00072796
Iteration 4/25 | Loss: 0.00072796
Iteration 5/25 | Loss: 0.00072796
Iteration 6/25 | Loss: 0.00072796
Iteration 7/25 | Loss: 0.00072796
Iteration 8/25 | Loss: 0.00072796
Iteration 9/25 | Loss: 0.00072796
Iteration 10/25 | Loss: 0.00072796
Iteration 11/25 | Loss: 0.00072796
Iteration 12/25 | Loss: 0.00072796
Iteration 13/25 | Loss: 0.00072796
Iteration 14/25 | Loss: 0.00072796
Iteration 15/25 | Loss: 0.00072796
Iteration 16/25 | Loss: 0.00072796
Iteration 17/25 | Loss: 0.00072796
Iteration 18/25 | Loss: 0.00072796
Iteration 19/25 | Loss: 0.00072796
Iteration 20/25 | Loss: 0.00072796
Iteration 21/25 | Loss: 0.00072796
Iteration 22/25 | Loss: 0.00072796
Iteration 23/25 | Loss: 0.00072796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007279588025994599, 0.0007279588025994599, 0.0007279588025994599, 0.0007279588025994599, 0.0007279588025994599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007279588025994599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072796
Iteration 2/1000 | Loss: 0.00008980
Iteration 3/1000 | Loss: 0.00005524
Iteration 4/1000 | Loss: 0.00004202
Iteration 5/1000 | Loss: 0.00003833
Iteration 6/1000 | Loss: 0.00003618
Iteration 7/1000 | Loss: 0.00003463
Iteration 8/1000 | Loss: 0.00003358
Iteration 9/1000 | Loss: 0.00003290
Iteration 10/1000 | Loss: 0.00003249
Iteration 11/1000 | Loss: 0.00003225
Iteration 12/1000 | Loss: 0.00003206
Iteration 13/1000 | Loss: 0.00003189
Iteration 14/1000 | Loss: 0.00003173
Iteration 15/1000 | Loss: 0.00003172
Iteration 16/1000 | Loss: 0.00003171
Iteration 17/1000 | Loss: 0.00003158
Iteration 18/1000 | Loss: 0.00003156
Iteration 19/1000 | Loss: 0.00003153
Iteration 20/1000 | Loss: 0.00003152
Iteration 21/1000 | Loss: 0.00003152
Iteration 22/1000 | Loss: 0.00003152
Iteration 23/1000 | Loss: 0.00003151
Iteration 24/1000 | Loss: 0.00003151
Iteration 25/1000 | Loss: 0.00003151
Iteration 26/1000 | Loss: 0.00003151
Iteration 27/1000 | Loss: 0.00003150
Iteration 28/1000 | Loss: 0.00003150
Iteration 29/1000 | Loss: 0.00003150
Iteration 30/1000 | Loss: 0.00003149
Iteration 31/1000 | Loss: 0.00003149
Iteration 32/1000 | Loss: 0.00003148
Iteration 33/1000 | Loss: 0.00003148
Iteration 34/1000 | Loss: 0.00003147
Iteration 35/1000 | Loss: 0.00003147
Iteration 36/1000 | Loss: 0.00003147
Iteration 37/1000 | Loss: 0.00003147
Iteration 38/1000 | Loss: 0.00003146
Iteration 39/1000 | Loss: 0.00003146
Iteration 40/1000 | Loss: 0.00003146
Iteration 41/1000 | Loss: 0.00003146
Iteration 42/1000 | Loss: 0.00003145
Iteration 43/1000 | Loss: 0.00003145
Iteration 44/1000 | Loss: 0.00003145
Iteration 45/1000 | Loss: 0.00003144
Iteration 46/1000 | Loss: 0.00003144
Iteration 47/1000 | Loss: 0.00003143
Iteration 48/1000 | Loss: 0.00003143
Iteration 49/1000 | Loss: 0.00003143
Iteration 50/1000 | Loss: 0.00003142
Iteration 51/1000 | Loss: 0.00003142
Iteration 52/1000 | Loss: 0.00003142
Iteration 53/1000 | Loss: 0.00003142
Iteration 54/1000 | Loss: 0.00003142
Iteration 55/1000 | Loss: 0.00003142
Iteration 56/1000 | Loss: 0.00003142
Iteration 57/1000 | Loss: 0.00003142
Iteration 58/1000 | Loss: 0.00003142
Iteration 59/1000 | Loss: 0.00003140
Iteration 60/1000 | Loss: 0.00003138
Iteration 61/1000 | Loss: 0.00003138
Iteration 62/1000 | Loss: 0.00003138
Iteration 63/1000 | Loss: 0.00003138
Iteration 64/1000 | Loss: 0.00003138
Iteration 65/1000 | Loss: 0.00003138
Iteration 66/1000 | Loss: 0.00003138
Iteration 67/1000 | Loss: 0.00003138
Iteration 68/1000 | Loss: 0.00003137
Iteration 69/1000 | Loss: 0.00003137
Iteration 70/1000 | Loss: 0.00003136
Iteration 71/1000 | Loss: 0.00003133
Iteration 72/1000 | Loss: 0.00003133
Iteration 73/1000 | Loss: 0.00003133
Iteration 74/1000 | Loss: 0.00003133
Iteration 75/1000 | Loss: 0.00003133
Iteration 76/1000 | Loss: 0.00003133
Iteration 77/1000 | Loss: 0.00003133
Iteration 78/1000 | Loss: 0.00003132
Iteration 79/1000 | Loss: 0.00003132
Iteration 80/1000 | Loss: 0.00003132
Iteration 81/1000 | Loss: 0.00003131
Iteration 82/1000 | Loss: 0.00003131
Iteration 83/1000 | Loss: 0.00003131
Iteration 84/1000 | Loss: 0.00003130
Iteration 85/1000 | Loss: 0.00003130
Iteration 86/1000 | Loss: 0.00003130
Iteration 87/1000 | Loss: 0.00003130
Iteration 88/1000 | Loss: 0.00003130
Iteration 89/1000 | Loss: 0.00003130
Iteration 90/1000 | Loss: 0.00003130
Iteration 91/1000 | Loss: 0.00003129
Iteration 92/1000 | Loss: 0.00003129
Iteration 93/1000 | Loss: 0.00003129
Iteration 94/1000 | Loss: 0.00003129
Iteration 95/1000 | Loss: 0.00003129
Iteration 96/1000 | Loss: 0.00003129
Iteration 97/1000 | Loss: 0.00003129
Iteration 98/1000 | Loss: 0.00003129
Iteration 99/1000 | Loss: 0.00003129
Iteration 100/1000 | Loss: 0.00003128
Iteration 101/1000 | Loss: 0.00003128
Iteration 102/1000 | Loss: 0.00003128
Iteration 103/1000 | Loss: 0.00003128
Iteration 104/1000 | Loss: 0.00003128
Iteration 105/1000 | Loss: 0.00003128
Iteration 106/1000 | Loss: 0.00003128
Iteration 107/1000 | Loss: 0.00003127
Iteration 108/1000 | Loss: 0.00003126
Iteration 109/1000 | Loss: 0.00003126
Iteration 110/1000 | Loss: 0.00003126
Iteration 111/1000 | Loss: 0.00003126
Iteration 112/1000 | Loss: 0.00003126
Iteration 113/1000 | Loss: 0.00003126
Iteration 114/1000 | Loss: 0.00003126
Iteration 115/1000 | Loss: 0.00003125
Iteration 116/1000 | Loss: 0.00003125
Iteration 117/1000 | Loss: 0.00003125
Iteration 118/1000 | Loss: 0.00003125
Iteration 119/1000 | Loss: 0.00003125
Iteration 120/1000 | Loss: 0.00003124
Iteration 121/1000 | Loss: 0.00003124
Iteration 122/1000 | Loss: 0.00003124
Iteration 123/1000 | Loss: 0.00003124
Iteration 124/1000 | Loss: 0.00003124
Iteration 125/1000 | Loss: 0.00003124
Iteration 126/1000 | Loss: 0.00003124
Iteration 127/1000 | Loss: 0.00003124
Iteration 128/1000 | Loss: 0.00003124
Iteration 129/1000 | Loss: 0.00003124
Iteration 130/1000 | Loss: 0.00003123
Iteration 131/1000 | Loss: 0.00003123
Iteration 132/1000 | Loss: 0.00003123
Iteration 133/1000 | Loss: 0.00003123
Iteration 134/1000 | Loss: 0.00003123
Iteration 135/1000 | Loss: 0.00003122
Iteration 136/1000 | Loss: 0.00003122
Iteration 137/1000 | Loss: 0.00003122
Iteration 138/1000 | Loss: 0.00003122
Iteration 139/1000 | Loss: 0.00003122
Iteration 140/1000 | Loss: 0.00003122
Iteration 141/1000 | Loss: 0.00003122
Iteration 142/1000 | Loss: 0.00003122
Iteration 143/1000 | Loss: 0.00003122
Iteration 144/1000 | Loss: 0.00003121
Iteration 145/1000 | Loss: 0.00003121
Iteration 146/1000 | Loss: 0.00003121
Iteration 147/1000 | Loss: 0.00003121
Iteration 148/1000 | Loss: 0.00003121
Iteration 149/1000 | Loss: 0.00003121
Iteration 150/1000 | Loss: 0.00003121
Iteration 151/1000 | Loss: 0.00003121
Iteration 152/1000 | Loss: 0.00003121
Iteration 153/1000 | Loss: 0.00003121
Iteration 154/1000 | Loss: 0.00003121
Iteration 155/1000 | Loss: 0.00003121
Iteration 156/1000 | Loss: 0.00003120
Iteration 157/1000 | Loss: 0.00003120
Iteration 158/1000 | Loss: 0.00003120
Iteration 159/1000 | Loss: 0.00003120
Iteration 160/1000 | Loss: 0.00003120
Iteration 161/1000 | Loss: 0.00003120
Iteration 162/1000 | Loss: 0.00003120
Iteration 163/1000 | Loss: 0.00003119
Iteration 164/1000 | Loss: 0.00003119
Iteration 165/1000 | Loss: 0.00003119
Iteration 166/1000 | Loss: 0.00003119
Iteration 167/1000 | Loss: 0.00003119
Iteration 168/1000 | Loss: 0.00003119
Iteration 169/1000 | Loss: 0.00003119
Iteration 170/1000 | Loss: 0.00003119
Iteration 171/1000 | Loss: 0.00003118
Iteration 172/1000 | Loss: 0.00003118
Iteration 173/1000 | Loss: 0.00003118
Iteration 174/1000 | Loss: 0.00003118
Iteration 175/1000 | Loss: 0.00003118
Iteration 176/1000 | Loss: 0.00003118
Iteration 177/1000 | Loss: 0.00003118
Iteration 178/1000 | Loss: 0.00003118
Iteration 179/1000 | Loss: 0.00003118
Iteration 180/1000 | Loss: 0.00003118
Iteration 181/1000 | Loss: 0.00003117
Iteration 182/1000 | Loss: 0.00003117
Iteration 183/1000 | Loss: 0.00003117
Iteration 184/1000 | Loss: 0.00003117
Iteration 185/1000 | Loss: 0.00003117
Iteration 186/1000 | Loss: 0.00003117
Iteration 187/1000 | Loss: 0.00003117
Iteration 188/1000 | Loss: 0.00003116
Iteration 189/1000 | Loss: 0.00003116
Iteration 190/1000 | Loss: 0.00003116
Iteration 191/1000 | Loss: 0.00003116
Iteration 192/1000 | Loss: 0.00003116
Iteration 193/1000 | Loss: 0.00003116
Iteration 194/1000 | Loss: 0.00003116
Iteration 195/1000 | Loss: 0.00003116
Iteration 196/1000 | Loss: 0.00003115
Iteration 197/1000 | Loss: 0.00003115
Iteration 198/1000 | Loss: 0.00003115
Iteration 199/1000 | Loss: 0.00003115
Iteration 200/1000 | Loss: 0.00003115
Iteration 201/1000 | Loss: 0.00003115
Iteration 202/1000 | Loss: 0.00003115
Iteration 203/1000 | Loss: 0.00003114
Iteration 204/1000 | Loss: 0.00003114
Iteration 205/1000 | Loss: 0.00003114
Iteration 206/1000 | Loss: 0.00003114
Iteration 207/1000 | Loss: 0.00003114
Iteration 208/1000 | Loss: 0.00003114
Iteration 209/1000 | Loss: 0.00003114
Iteration 210/1000 | Loss: 0.00003114
Iteration 211/1000 | Loss: 0.00003114
Iteration 212/1000 | Loss: 0.00003114
Iteration 213/1000 | Loss: 0.00003114
Iteration 214/1000 | Loss: 0.00003114
Iteration 215/1000 | Loss: 0.00003114
Iteration 216/1000 | Loss: 0.00003114
Iteration 217/1000 | Loss: 0.00003114
Iteration 218/1000 | Loss: 0.00003114
Iteration 219/1000 | Loss: 0.00003114
Iteration 220/1000 | Loss: 0.00003114
Iteration 221/1000 | Loss: 0.00003114
Iteration 222/1000 | Loss: 0.00003114
Iteration 223/1000 | Loss: 0.00003114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [3.113743878202513e-05, 3.113743878202513e-05, 3.113743878202513e-05, 3.113743878202513e-05, 3.113743878202513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.113743878202513e-05

Optimization complete. Final v2v error: 4.77652645111084 mm

Highest mean error: 6.356547832489014 mm for frame 77

Lowest mean error: 4.156511306762695 mm for frame 108

Saving results

Total time: 46.23983359336853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535371
Iteration 2/25 | Loss: 0.00127316
Iteration 3/25 | Loss: 0.00108179
Iteration 4/25 | Loss: 0.00104192
Iteration 5/25 | Loss: 0.00102876
Iteration 6/25 | Loss: 0.00102723
Iteration 7/25 | Loss: 0.00102723
Iteration 8/25 | Loss: 0.00102723
Iteration 9/25 | Loss: 0.00102723
Iteration 10/25 | Loss: 0.00102723
Iteration 11/25 | Loss: 0.00102723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010272313375025988, 0.0010272313375025988, 0.0010272313375025988, 0.0010272313375025988, 0.0010272313375025988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010272313375025988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67386734
Iteration 2/25 | Loss: 0.00082430
Iteration 3/25 | Loss: 0.00082430
Iteration 4/25 | Loss: 0.00082430
Iteration 5/25 | Loss: 0.00082430
Iteration 6/25 | Loss: 0.00082430
Iteration 7/25 | Loss: 0.00082430
Iteration 8/25 | Loss: 0.00082430
Iteration 9/25 | Loss: 0.00082430
Iteration 10/25 | Loss: 0.00082430
Iteration 11/25 | Loss: 0.00082430
Iteration 12/25 | Loss: 0.00082430
Iteration 13/25 | Loss: 0.00082430
Iteration 14/25 | Loss: 0.00082430
Iteration 15/25 | Loss: 0.00082430
Iteration 16/25 | Loss: 0.00082430
Iteration 17/25 | Loss: 0.00082430
Iteration 18/25 | Loss: 0.00082430
Iteration 19/25 | Loss: 0.00082430
Iteration 20/25 | Loss: 0.00082430
Iteration 21/25 | Loss: 0.00082430
Iteration 22/25 | Loss: 0.00082430
Iteration 23/25 | Loss: 0.00082430
Iteration 24/25 | Loss: 0.00082430
Iteration 25/25 | Loss: 0.00082430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082430
Iteration 2/1000 | Loss: 0.00009587
Iteration 3/1000 | Loss: 0.00005936
Iteration 4/1000 | Loss: 0.00005320
Iteration 5/1000 | Loss: 0.00005084
Iteration 6/1000 | Loss: 0.00004905
Iteration 7/1000 | Loss: 0.00004731
Iteration 8/1000 | Loss: 0.00004645
Iteration 9/1000 | Loss: 0.00004537
Iteration 10/1000 | Loss: 0.00004495
Iteration 11/1000 | Loss: 0.00004459
Iteration 12/1000 | Loss: 0.00004433
Iteration 13/1000 | Loss: 0.00004397
Iteration 14/1000 | Loss: 0.00004372
Iteration 15/1000 | Loss: 0.00004352
Iteration 16/1000 | Loss: 0.00004350
Iteration 17/1000 | Loss: 0.00004344
Iteration 18/1000 | Loss: 0.00004341
Iteration 19/1000 | Loss: 0.00004335
Iteration 20/1000 | Loss: 0.00004328
Iteration 21/1000 | Loss: 0.00004317
Iteration 22/1000 | Loss: 0.00004316
Iteration 23/1000 | Loss: 0.00004316
Iteration 24/1000 | Loss: 0.00004315
Iteration 25/1000 | Loss: 0.00004315
Iteration 26/1000 | Loss: 0.00004314
Iteration 27/1000 | Loss: 0.00004313
Iteration 28/1000 | Loss: 0.00004312
Iteration 29/1000 | Loss: 0.00004312
Iteration 30/1000 | Loss: 0.00004311
Iteration 31/1000 | Loss: 0.00004311
Iteration 32/1000 | Loss: 0.00004311
Iteration 33/1000 | Loss: 0.00004311
Iteration 34/1000 | Loss: 0.00004311
Iteration 35/1000 | Loss: 0.00004309
Iteration 36/1000 | Loss: 0.00004309
Iteration 37/1000 | Loss: 0.00004309
Iteration 38/1000 | Loss: 0.00004309
Iteration 39/1000 | Loss: 0.00004308
Iteration 40/1000 | Loss: 0.00004307
Iteration 41/1000 | Loss: 0.00004304
Iteration 42/1000 | Loss: 0.00004304
Iteration 43/1000 | Loss: 0.00004304
Iteration 44/1000 | Loss: 0.00004304
Iteration 45/1000 | Loss: 0.00004304
Iteration 46/1000 | Loss: 0.00004304
Iteration 47/1000 | Loss: 0.00004304
Iteration 48/1000 | Loss: 0.00004303
Iteration 49/1000 | Loss: 0.00004302
Iteration 50/1000 | Loss: 0.00004302
Iteration 51/1000 | Loss: 0.00004302
Iteration 52/1000 | Loss: 0.00004301
Iteration 53/1000 | Loss: 0.00004301
Iteration 54/1000 | Loss: 0.00004300
Iteration 55/1000 | Loss: 0.00004300
Iteration 56/1000 | Loss: 0.00004297
Iteration 57/1000 | Loss: 0.00004296
Iteration 58/1000 | Loss: 0.00004296
Iteration 59/1000 | Loss: 0.00004295
Iteration 60/1000 | Loss: 0.00004294
Iteration 61/1000 | Loss: 0.00004294
Iteration 62/1000 | Loss: 0.00004294
Iteration 63/1000 | Loss: 0.00004294
Iteration 64/1000 | Loss: 0.00004294
Iteration 65/1000 | Loss: 0.00004294
Iteration 66/1000 | Loss: 0.00004294
Iteration 67/1000 | Loss: 0.00004293
Iteration 68/1000 | Loss: 0.00004293
Iteration 69/1000 | Loss: 0.00004293
Iteration 70/1000 | Loss: 0.00004292
Iteration 71/1000 | Loss: 0.00004292
Iteration 72/1000 | Loss: 0.00004292
Iteration 73/1000 | Loss: 0.00004291
Iteration 74/1000 | Loss: 0.00004291
Iteration 75/1000 | Loss: 0.00004291
Iteration 76/1000 | Loss: 0.00004290
Iteration 77/1000 | Loss: 0.00004290
Iteration 78/1000 | Loss: 0.00004290
Iteration 79/1000 | Loss: 0.00004290
Iteration 80/1000 | Loss: 0.00004289
Iteration 81/1000 | Loss: 0.00004287
Iteration 82/1000 | Loss: 0.00004286
Iteration 83/1000 | Loss: 0.00004286
Iteration 84/1000 | Loss: 0.00004286
Iteration 85/1000 | Loss: 0.00004286
Iteration 86/1000 | Loss: 0.00004286
Iteration 87/1000 | Loss: 0.00004286
Iteration 88/1000 | Loss: 0.00004285
Iteration 89/1000 | Loss: 0.00004285
Iteration 90/1000 | Loss: 0.00004285
Iteration 91/1000 | Loss: 0.00004284
Iteration 92/1000 | Loss: 0.00004284
Iteration 93/1000 | Loss: 0.00004284
Iteration 94/1000 | Loss: 0.00004284
Iteration 95/1000 | Loss: 0.00004284
Iteration 96/1000 | Loss: 0.00004284
Iteration 97/1000 | Loss: 0.00004283
Iteration 98/1000 | Loss: 0.00004283
Iteration 99/1000 | Loss: 0.00004283
Iteration 100/1000 | Loss: 0.00004282
Iteration 101/1000 | Loss: 0.00004282
Iteration 102/1000 | Loss: 0.00004281
Iteration 103/1000 | Loss: 0.00004280
Iteration 104/1000 | Loss: 0.00004280
Iteration 105/1000 | Loss: 0.00004279
Iteration 106/1000 | Loss: 0.00004279
Iteration 107/1000 | Loss: 0.00004279
Iteration 108/1000 | Loss: 0.00004279
Iteration 109/1000 | Loss: 0.00004279
Iteration 110/1000 | Loss: 0.00004279
Iteration 111/1000 | Loss: 0.00004279
Iteration 112/1000 | Loss: 0.00004278
Iteration 113/1000 | Loss: 0.00004278
Iteration 114/1000 | Loss: 0.00004278
Iteration 115/1000 | Loss: 0.00004278
Iteration 116/1000 | Loss: 0.00004278
Iteration 117/1000 | Loss: 0.00004278
Iteration 118/1000 | Loss: 0.00004278
Iteration 119/1000 | Loss: 0.00004278
Iteration 120/1000 | Loss: 0.00004278
Iteration 121/1000 | Loss: 0.00004278
Iteration 122/1000 | Loss: 0.00004277
Iteration 123/1000 | Loss: 0.00004277
Iteration 124/1000 | Loss: 0.00004277
Iteration 125/1000 | Loss: 0.00004277
Iteration 126/1000 | Loss: 0.00004277
Iteration 127/1000 | Loss: 0.00004277
Iteration 128/1000 | Loss: 0.00004277
Iteration 129/1000 | Loss: 0.00004277
Iteration 130/1000 | Loss: 0.00004276
Iteration 131/1000 | Loss: 0.00004276
Iteration 132/1000 | Loss: 0.00004276
Iteration 133/1000 | Loss: 0.00004275
Iteration 134/1000 | Loss: 0.00004275
Iteration 135/1000 | Loss: 0.00004274
Iteration 136/1000 | Loss: 0.00004274
Iteration 137/1000 | Loss: 0.00004273
Iteration 138/1000 | Loss: 0.00004273
Iteration 139/1000 | Loss: 0.00004273
Iteration 140/1000 | Loss: 0.00004273
Iteration 141/1000 | Loss: 0.00004273
Iteration 142/1000 | Loss: 0.00004273
Iteration 143/1000 | Loss: 0.00004273
Iteration 144/1000 | Loss: 0.00004273
Iteration 145/1000 | Loss: 0.00004273
Iteration 146/1000 | Loss: 0.00004273
Iteration 147/1000 | Loss: 0.00004273
Iteration 148/1000 | Loss: 0.00004272
Iteration 149/1000 | Loss: 0.00004272
Iteration 150/1000 | Loss: 0.00004272
Iteration 151/1000 | Loss: 0.00004272
Iteration 152/1000 | Loss: 0.00004272
Iteration 153/1000 | Loss: 0.00004272
Iteration 154/1000 | Loss: 0.00004272
Iteration 155/1000 | Loss: 0.00004272
Iteration 156/1000 | Loss: 0.00004272
Iteration 157/1000 | Loss: 0.00004272
Iteration 158/1000 | Loss: 0.00004272
Iteration 159/1000 | Loss: 0.00004271
Iteration 160/1000 | Loss: 0.00004271
Iteration 161/1000 | Loss: 0.00004271
Iteration 162/1000 | Loss: 0.00004271
Iteration 163/1000 | Loss: 0.00004271
Iteration 164/1000 | Loss: 0.00004271
Iteration 165/1000 | Loss: 0.00004271
Iteration 166/1000 | Loss: 0.00004271
Iteration 167/1000 | Loss: 0.00004270
Iteration 168/1000 | Loss: 0.00004270
Iteration 169/1000 | Loss: 0.00004269
Iteration 170/1000 | Loss: 0.00004268
Iteration 171/1000 | Loss: 0.00004267
Iteration 172/1000 | Loss: 0.00004266
Iteration 173/1000 | Loss: 0.00004266
Iteration 174/1000 | Loss: 0.00004264
Iteration 175/1000 | Loss: 0.00004260
Iteration 176/1000 | Loss: 0.00004259
Iteration 177/1000 | Loss: 0.00004259
Iteration 178/1000 | Loss: 0.00004259
Iteration 179/1000 | Loss: 0.00004259
Iteration 180/1000 | Loss: 0.00004258
Iteration 181/1000 | Loss: 0.00004258
Iteration 182/1000 | Loss: 0.00004256
Iteration 183/1000 | Loss: 0.00004256
Iteration 184/1000 | Loss: 0.00004256
Iteration 185/1000 | Loss: 0.00004253
Iteration 186/1000 | Loss: 0.00004252
Iteration 187/1000 | Loss: 0.00004252
Iteration 188/1000 | Loss: 0.00004252
Iteration 189/1000 | Loss: 0.00004251
Iteration 190/1000 | Loss: 0.00004251
Iteration 191/1000 | Loss: 0.00004251
Iteration 192/1000 | Loss: 0.00004251
Iteration 193/1000 | Loss: 0.00004250
Iteration 194/1000 | Loss: 0.00004250
Iteration 195/1000 | Loss: 0.00004250
Iteration 196/1000 | Loss: 0.00004249
Iteration 197/1000 | Loss: 0.00004249
Iteration 198/1000 | Loss: 0.00004249
Iteration 199/1000 | Loss: 0.00004248
Iteration 200/1000 | Loss: 0.00004248
Iteration 201/1000 | Loss: 0.00004248
Iteration 202/1000 | Loss: 0.00004247
Iteration 203/1000 | Loss: 0.00004247
Iteration 204/1000 | Loss: 0.00004247
Iteration 205/1000 | Loss: 0.00004247
Iteration 206/1000 | Loss: 0.00004247
Iteration 207/1000 | Loss: 0.00004247
Iteration 208/1000 | Loss: 0.00004246
Iteration 209/1000 | Loss: 0.00004246
Iteration 210/1000 | Loss: 0.00004246
Iteration 211/1000 | Loss: 0.00004246
Iteration 212/1000 | Loss: 0.00004246
Iteration 213/1000 | Loss: 0.00004246
Iteration 214/1000 | Loss: 0.00004246
Iteration 215/1000 | Loss: 0.00004246
Iteration 216/1000 | Loss: 0.00004245
Iteration 217/1000 | Loss: 0.00004245
Iteration 218/1000 | Loss: 0.00004245
Iteration 219/1000 | Loss: 0.00004245
Iteration 220/1000 | Loss: 0.00004245
Iteration 221/1000 | Loss: 0.00004245
Iteration 222/1000 | Loss: 0.00004245
Iteration 223/1000 | Loss: 0.00004245
Iteration 224/1000 | Loss: 0.00004244
Iteration 225/1000 | Loss: 0.00004244
Iteration 226/1000 | Loss: 0.00004244
Iteration 227/1000 | Loss: 0.00004244
Iteration 228/1000 | Loss: 0.00004243
Iteration 229/1000 | Loss: 0.00004243
Iteration 230/1000 | Loss: 0.00004243
Iteration 231/1000 | Loss: 0.00004242
Iteration 232/1000 | Loss: 0.00004242
Iteration 233/1000 | Loss: 0.00004242
Iteration 234/1000 | Loss: 0.00004242
Iteration 235/1000 | Loss: 0.00004242
Iteration 236/1000 | Loss: 0.00004242
Iteration 237/1000 | Loss: 0.00004242
Iteration 238/1000 | Loss: 0.00004242
Iteration 239/1000 | Loss: 0.00004242
Iteration 240/1000 | Loss: 0.00004241
Iteration 241/1000 | Loss: 0.00004241
Iteration 242/1000 | Loss: 0.00004241
Iteration 243/1000 | Loss: 0.00004241
Iteration 244/1000 | Loss: 0.00004241
Iteration 245/1000 | Loss: 0.00004241
Iteration 246/1000 | Loss: 0.00004241
Iteration 247/1000 | Loss: 0.00004241
Iteration 248/1000 | Loss: 0.00004241
Iteration 249/1000 | Loss: 0.00004241
Iteration 250/1000 | Loss: 0.00004240
Iteration 251/1000 | Loss: 0.00004240
Iteration 252/1000 | Loss: 0.00004240
Iteration 253/1000 | Loss: 0.00004240
Iteration 254/1000 | Loss: 0.00004240
Iteration 255/1000 | Loss: 0.00004240
Iteration 256/1000 | Loss: 0.00004240
Iteration 257/1000 | Loss: 0.00004240
Iteration 258/1000 | Loss: 0.00004239
Iteration 259/1000 | Loss: 0.00004239
Iteration 260/1000 | Loss: 0.00004239
Iteration 261/1000 | Loss: 0.00004239
Iteration 262/1000 | Loss: 0.00004239
Iteration 263/1000 | Loss: 0.00004239
Iteration 264/1000 | Loss: 0.00004239
Iteration 265/1000 | Loss: 0.00004239
Iteration 266/1000 | Loss: 0.00004239
Iteration 267/1000 | Loss: 0.00004239
Iteration 268/1000 | Loss: 0.00004239
Iteration 269/1000 | Loss: 0.00004239
Iteration 270/1000 | Loss: 0.00004239
Iteration 271/1000 | Loss: 0.00004239
Iteration 272/1000 | Loss: 0.00004238
Iteration 273/1000 | Loss: 0.00004238
Iteration 274/1000 | Loss: 0.00004238
Iteration 275/1000 | Loss: 0.00004238
Iteration 276/1000 | Loss: 0.00004238
Iteration 277/1000 | Loss: 0.00004238
Iteration 278/1000 | Loss: 0.00004238
Iteration 279/1000 | Loss: 0.00004238
Iteration 280/1000 | Loss: 0.00004238
Iteration 281/1000 | Loss: 0.00004238
Iteration 282/1000 | Loss: 0.00004238
Iteration 283/1000 | Loss: 0.00004238
Iteration 284/1000 | Loss: 0.00004238
Iteration 285/1000 | Loss: 0.00004238
Iteration 286/1000 | Loss: 0.00004238
Iteration 287/1000 | Loss: 0.00004238
Iteration 288/1000 | Loss: 0.00004237
Iteration 289/1000 | Loss: 0.00004237
Iteration 290/1000 | Loss: 0.00004237
Iteration 291/1000 | Loss: 0.00004237
Iteration 292/1000 | Loss: 0.00004237
Iteration 293/1000 | Loss: 0.00004237
Iteration 294/1000 | Loss: 0.00004236
Iteration 295/1000 | Loss: 0.00004236
Iteration 296/1000 | Loss: 0.00004236
Iteration 297/1000 | Loss: 0.00004236
Iteration 298/1000 | Loss: 0.00004236
Iteration 299/1000 | Loss: 0.00004236
Iteration 300/1000 | Loss: 0.00004236
Iteration 301/1000 | Loss: 0.00004236
Iteration 302/1000 | Loss: 0.00004236
Iteration 303/1000 | Loss: 0.00004236
Iteration 304/1000 | Loss: 0.00004236
Iteration 305/1000 | Loss: 0.00004236
Iteration 306/1000 | Loss: 0.00004236
Iteration 307/1000 | Loss: 0.00004236
Iteration 308/1000 | Loss: 0.00004236
Iteration 309/1000 | Loss: 0.00004235
Iteration 310/1000 | Loss: 0.00004235
Iteration 311/1000 | Loss: 0.00004235
Iteration 312/1000 | Loss: 0.00004235
Iteration 313/1000 | Loss: 0.00004235
Iteration 314/1000 | Loss: 0.00004235
Iteration 315/1000 | Loss: 0.00004235
Iteration 316/1000 | Loss: 0.00004235
Iteration 317/1000 | Loss: 0.00004235
Iteration 318/1000 | Loss: 0.00004235
Iteration 319/1000 | Loss: 0.00004235
Iteration 320/1000 | Loss: 0.00004235
Iteration 321/1000 | Loss: 0.00004235
Iteration 322/1000 | Loss: 0.00004235
Iteration 323/1000 | Loss: 0.00004235
Iteration 324/1000 | Loss: 0.00004235
Iteration 325/1000 | Loss: 0.00004235
Iteration 326/1000 | Loss: 0.00004235
Iteration 327/1000 | Loss: 0.00004235
Iteration 328/1000 | Loss: 0.00004235
Iteration 329/1000 | Loss: 0.00004235
Iteration 330/1000 | Loss: 0.00004235
Iteration 331/1000 | Loss: 0.00004235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [4.2348256101831794e-05, 4.2348256101831794e-05, 4.2348256101831794e-05, 4.2348256101831794e-05, 4.2348256101831794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2348256101831794e-05

Optimization complete. Final v2v error: 5.555156230926514 mm

Highest mean error: 5.8007893562316895 mm for frame 8

Lowest mean error: 5.202374458312988 mm for frame 35

Saving results

Total time: 68.7763831615448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050478
Iteration 2/25 | Loss: 0.00139524
Iteration 3/25 | Loss: 0.00131190
Iteration 4/25 | Loss: 0.00105068
Iteration 5/25 | Loss: 0.00101725
Iteration 6/25 | Loss: 0.00101552
Iteration 7/25 | Loss: 0.00100572
Iteration 8/25 | Loss: 0.00099509
Iteration 9/25 | Loss: 0.00098941
Iteration 10/25 | Loss: 0.00098851
Iteration 11/25 | Loss: 0.00098867
Iteration 12/25 | Loss: 0.00098531
Iteration 13/25 | Loss: 0.00098309
Iteration 14/25 | Loss: 0.00098224
Iteration 15/25 | Loss: 0.00098193
Iteration 16/25 | Loss: 0.00098261
Iteration 17/25 | Loss: 0.00098226
Iteration 18/25 | Loss: 0.00098175
Iteration 19/25 | Loss: 0.00098065
Iteration 20/25 | Loss: 0.00098040
Iteration 21/25 | Loss: 0.00098025
Iteration 22/25 | Loss: 0.00098025
Iteration 23/25 | Loss: 0.00098025
Iteration 24/25 | Loss: 0.00098024
Iteration 25/25 | Loss: 0.00098024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15896273
Iteration 2/25 | Loss: 0.00064121
Iteration 3/25 | Loss: 0.00064120
Iteration 4/25 | Loss: 0.00064120
Iteration 5/25 | Loss: 0.00064120
Iteration 6/25 | Loss: 0.00064120
Iteration 7/25 | Loss: 0.00064120
Iteration 8/25 | Loss: 0.00064120
Iteration 9/25 | Loss: 0.00064120
Iteration 10/25 | Loss: 0.00064120
Iteration 11/25 | Loss: 0.00064120
Iteration 12/25 | Loss: 0.00064120
Iteration 13/25 | Loss: 0.00064120
Iteration 14/25 | Loss: 0.00064120
Iteration 15/25 | Loss: 0.00064120
Iteration 16/25 | Loss: 0.00064120
Iteration 17/25 | Loss: 0.00064120
Iteration 18/25 | Loss: 0.00064120
Iteration 19/25 | Loss: 0.00064120
Iteration 20/25 | Loss: 0.00064120
Iteration 21/25 | Loss: 0.00064120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00064119539456442, 0.00064119539456442, 0.00064119539456442, 0.00064119539456442, 0.00064119539456442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00064119539456442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064120
Iteration 2/1000 | Loss: 0.00007980
Iteration 3/1000 | Loss: 0.00004386
Iteration 4/1000 | Loss: 0.00003795
Iteration 5/1000 | Loss: 0.00003513
Iteration 6/1000 | Loss: 0.00003322
Iteration 7/1000 | Loss: 0.00003185
Iteration 8/1000 | Loss: 0.00003098
Iteration 9/1000 | Loss: 0.00003028
Iteration 10/1000 | Loss: 0.00002987
Iteration 11/1000 | Loss: 0.00002952
Iteration 12/1000 | Loss: 0.00002930
Iteration 13/1000 | Loss: 0.00002913
Iteration 14/1000 | Loss: 0.00002912
Iteration 15/1000 | Loss: 0.00002904
Iteration 16/1000 | Loss: 0.00002899
Iteration 17/1000 | Loss: 0.00002898
Iteration 18/1000 | Loss: 0.00002896
Iteration 19/1000 | Loss: 0.00002896
Iteration 20/1000 | Loss: 0.00002895
Iteration 21/1000 | Loss: 0.00002893
Iteration 22/1000 | Loss: 0.00002892
Iteration 23/1000 | Loss: 0.00002887
Iteration 24/1000 | Loss: 0.00002884
Iteration 25/1000 | Loss: 0.00002884
Iteration 26/1000 | Loss: 0.00002883
Iteration 27/1000 | Loss: 0.00002883
Iteration 28/1000 | Loss: 0.00002883
Iteration 29/1000 | Loss: 0.00002883
Iteration 30/1000 | Loss: 0.00002883
Iteration 31/1000 | Loss: 0.00002882
Iteration 32/1000 | Loss: 0.00002882
Iteration 33/1000 | Loss: 0.00002882
Iteration 34/1000 | Loss: 0.00002881
Iteration 35/1000 | Loss: 0.00002881
Iteration 36/1000 | Loss: 0.00002881
Iteration 37/1000 | Loss: 0.00002881
Iteration 38/1000 | Loss: 0.00002881
Iteration 39/1000 | Loss: 0.00002881
Iteration 40/1000 | Loss: 0.00002881
Iteration 41/1000 | Loss: 0.00002881
Iteration 42/1000 | Loss: 0.00002881
Iteration 43/1000 | Loss: 0.00002880
Iteration 44/1000 | Loss: 0.00002880
Iteration 45/1000 | Loss: 0.00002879
Iteration 46/1000 | Loss: 0.00002879
Iteration 47/1000 | Loss: 0.00002879
Iteration 48/1000 | Loss: 0.00002879
Iteration 49/1000 | Loss: 0.00002879
Iteration 50/1000 | Loss: 0.00002878
Iteration 51/1000 | Loss: 0.00002878
Iteration 52/1000 | Loss: 0.00002877
Iteration 53/1000 | Loss: 0.00002877
Iteration 54/1000 | Loss: 0.00002877
Iteration 55/1000 | Loss: 0.00002877
Iteration 56/1000 | Loss: 0.00002876
Iteration 57/1000 | Loss: 0.00002876
Iteration 58/1000 | Loss: 0.00002876
Iteration 59/1000 | Loss: 0.00002876
Iteration 60/1000 | Loss: 0.00002876
Iteration 61/1000 | Loss: 0.00002876
Iteration 62/1000 | Loss: 0.00002876
Iteration 63/1000 | Loss: 0.00002875
Iteration 64/1000 | Loss: 0.00002875
Iteration 65/1000 | Loss: 0.00002875
Iteration 66/1000 | Loss: 0.00002874
Iteration 67/1000 | Loss: 0.00002874
Iteration 68/1000 | Loss: 0.00002874
Iteration 69/1000 | Loss: 0.00002874
Iteration 70/1000 | Loss: 0.00002874
Iteration 71/1000 | Loss: 0.00002874
Iteration 72/1000 | Loss: 0.00002873
Iteration 73/1000 | Loss: 0.00002873
Iteration 74/1000 | Loss: 0.00002873
Iteration 75/1000 | Loss: 0.00002872
Iteration 76/1000 | Loss: 0.00002872
Iteration 77/1000 | Loss: 0.00002872
Iteration 78/1000 | Loss: 0.00002872
Iteration 79/1000 | Loss: 0.00002872
Iteration 80/1000 | Loss: 0.00002872
Iteration 81/1000 | Loss: 0.00002872
Iteration 82/1000 | Loss: 0.00002871
Iteration 83/1000 | Loss: 0.00002871
Iteration 84/1000 | Loss: 0.00002871
Iteration 85/1000 | Loss: 0.00002871
Iteration 86/1000 | Loss: 0.00002870
Iteration 87/1000 | Loss: 0.00002870
Iteration 88/1000 | Loss: 0.00002870
Iteration 89/1000 | Loss: 0.00002870
Iteration 90/1000 | Loss: 0.00002870
Iteration 91/1000 | Loss: 0.00002870
Iteration 92/1000 | Loss: 0.00002870
Iteration 93/1000 | Loss: 0.00002870
Iteration 94/1000 | Loss: 0.00002869
Iteration 95/1000 | Loss: 0.00002869
Iteration 96/1000 | Loss: 0.00002869
Iteration 97/1000 | Loss: 0.00002869
Iteration 98/1000 | Loss: 0.00002869
Iteration 99/1000 | Loss: 0.00002869
Iteration 100/1000 | Loss: 0.00002869
Iteration 101/1000 | Loss: 0.00002869
Iteration 102/1000 | Loss: 0.00002869
Iteration 103/1000 | Loss: 0.00002869
Iteration 104/1000 | Loss: 0.00002869
Iteration 105/1000 | Loss: 0.00002869
Iteration 106/1000 | Loss: 0.00002869
Iteration 107/1000 | Loss: 0.00002869
Iteration 108/1000 | Loss: 0.00002869
Iteration 109/1000 | Loss: 0.00002869
Iteration 110/1000 | Loss: 0.00002869
Iteration 111/1000 | Loss: 0.00002869
Iteration 112/1000 | Loss: 0.00002869
Iteration 113/1000 | Loss: 0.00002869
Iteration 114/1000 | Loss: 0.00002869
Iteration 115/1000 | Loss: 0.00002869
Iteration 116/1000 | Loss: 0.00002869
Iteration 117/1000 | Loss: 0.00002869
Iteration 118/1000 | Loss: 0.00002869
Iteration 119/1000 | Loss: 0.00002869
Iteration 120/1000 | Loss: 0.00002869
Iteration 121/1000 | Loss: 0.00002869
Iteration 122/1000 | Loss: 0.00002869
Iteration 123/1000 | Loss: 0.00002869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.868862429750152e-05, 2.868862429750152e-05, 2.868862429750152e-05, 2.868862429750152e-05, 2.868862429750152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.868862429750152e-05

Optimization complete. Final v2v error: 4.613108158111572 mm

Highest mean error: 5.1549506187438965 mm for frame 117

Lowest mean error: 4.303774356842041 mm for frame 224

Saving results

Total time: 73.6105546951294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00353931
Iteration 2/25 | Loss: 0.00095866
Iteration 3/25 | Loss: 0.00089100
Iteration 4/25 | Loss: 0.00087807
Iteration 5/25 | Loss: 0.00087451
Iteration 6/25 | Loss: 0.00087305
Iteration 7/25 | Loss: 0.00087292
Iteration 8/25 | Loss: 0.00087292
Iteration 9/25 | Loss: 0.00087292
Iteration 10/25 | Loss: 0.00087292
Iteration 11/25 | Loss: 0.00087292
Iteration 12/25 | Loss: 0.00087292
Iteration 13/25 | Loss: 0.00087292
Iteration 14/25 | Loss: 0.00087292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008729195105843246, 0.0008729195105843246, 0.0008729195105843246, 0.0008729195105843246, 0.0008729195105843246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008729195105843246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31252348
Iteration 2/25 | Loss: 0.00057754
Iteration 3/25 | Loss: 0.00057754
Iteration 4/25 | Loss: 0.00057754
Iteration 5/25 | Loss: 0.00057754
Iteration 6/25 | Loss: 0.00057754
Iteration 7/25 | Loss: 0.00057754
Iteration 8/25 | Loss: 0.00057754
Iteration 9/25 | Loss: 0.00057754
Iteration 10/25 | Loss: 0.00057754
Iteration 11/25 | Loss: 0.00057754
Iteration 12/25 | Loss: 0.00057754
Iteration 13/25 | Loss: 0.00057754
Iteration 14/25 | Loss: 0.00057754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005775387398898602, 0.0005775387398898602, 0.0005775387398898602, 0.0005775387398898602, 0.0005775387398898602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005775387398898602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057754
Iteration 2/1000 | Loss: 0.00006567
Iteration 3/1000 | Loss: 0.00003148
Iteration 4/1000 | Loss: 0.00002482
Iteration 5/1000 | Loss: 0.00002319
Iteration 6/1000 | Loss: 0.00002205
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002092
Iteration 9/1000 | Loss: 0.00002067
Iteration 10/1000 | Loss: 0.00002055
Iteration 11/1000 | Loss: 0.00002046
Iteration 12/1000 | Loss: 0.00002044
Iteration 13/1000 | Loss: 0.00002038
Iteration 14/1000 | Loss: 0.00002037
Iteration 15/1000 | Loss: 0.00002026
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002023
Iteration 18/1000 | Loss: 0.00002023
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002023
Iteration 21/1000 | Loss: 0.00002023
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002023
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002022
Iteration 27/1000 | Loss: 0.00002022
Iteration 28/1000 | Loss: 0.00002022
Iteration 29/1000 | Loss: 0.00002022
Iteration 30/1000 | Loss: 0.00002022
Iteration 31/1000 | Loss: 0.00002022
Iteration 32/1000 | Loss: 0.00002021
Iteration 33/1000 | Loss: 0.00002021
Iteration 34/1000 | Loss: 0.00002019
Iteration 35/1000 | Loss: 0.00002019
Iteration 36/1000 | Loss: 0.00002019
Iteration 37/1000 | Loss: 0.00002019
Iteration 38/1000 | Loss: 0.00002019
Iteration 39/1000 | Loss: 0.00002018
Iteration 40/1000 | Loss: 0.00002018
Iteration 41/1000 | Loss: 0.00002018
Iteration 42/1000 | Loss: 0.00002018
Iteration 43/1000 | Loss: 0.00002018
Iteration 44/1000 | Loss: 0.00002018
Iteration 45/1000 | Loss: 0.00002018
Iteration 46/1000 | Loss: 0.00002018
Iteration 47/1000 | Loss: 0.00002018
Iteration 48/1000 | Loss: 0.00002018
Iteration 49/1000 | Loss: 0.00002015
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002013
Iteration 52/1000 | Loss: 0.00002013
Iteration 53/1000 | Loss: 0.00002013
Iteration 54/1000 | Loss: 0.00002013
Iteration 55/1000 | Loss: 0.00002013
Iteration 56/1000 | Loss: 0.00002013
Iteration 57/1000 | Loss: 0.00002012
Iteration 58/1000 | Loss: 0.00002012
Iteration 59/1000 | Loss: 0.00002012
Iteration 60/1000 | Loss: 0.00002012
Iteration 61/1000 | Loss: 0.00002009
Iteration 62/1000 | Loss: 0.00002009
Iteration 63/1000 | Loss: 0.00002008
Iteration 64/1000 | Loss: 0.00002008
Iteration 65/1000 | Loss: 0.00002007
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002007
Iteration 71/1000 | Loss: 0.00002006
Iteration 72/1000 | Loss: 0.00002006
Iteration 73/1000 | Loss: 0.00002005
Iteration 74/1000 | Loss: 0.00002004
Iteration 75/1000 | Loss: 0.00002004
Iteration 76/1000 | Loss: 0.00002002
Iteration 77/1000 | Loss: 0.00002001
Iteration 78/1000 | Loss: 0.00002001
Iteration 79/1000 | Loss: 0.00002001
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001998
Iteration 83/1000 | Loss: 0.00001998
Iteration 84/1000 | Loss: 0.00001998
Iteration 85/1000 | Loss: 0.00001997
Iteration 86/1000 | Loss: 0.00001997
Iteration 87/1000 | Loss: 0.00001997
Iteration 88/1000 | Loss: 0.00001997
Iteration 89/1000 | Loss: 0.00001997
Iteration 90/1000 | Loss: 0.00001996
Iteration 91/1000 | Loss: 0.00001996
Iteration 92/1000 | Loss: 0.00001996
Iteration 93/1000 | Loss: 0.00001996
Iteration 94/1000 | Loss: 0.00001996
Iteration 95/1000 | Loss: 0.00001996
Iteration 96/1000 | Loss: 0.00001996
Iteration 97/1000 | Loss: 0.00001996
Iteration 98/1000 | Loss: 0.00001995
Iteration 99/1000 | Loss: 0.00001995
Iteration 100/1000 | Loss: 0.00001995
Iteration 101/1000 | Loss: 0.00001995
Iteration 102/1000 | Loss: 0.00001995
Iteration 103/1000 | Loss: 0.00001995
Iteration 104/1000 | Loss: 0.00001995
Iteration 105/1000 | Loss: 0.00001995
Iteration 106/1000 | Loss: 0.00001995
Iteration 107/1000 | Loss: 0.00001995
Iteration 108/1000 | Loss: 0.00001995
Iteration 109/1000 | Loss: 0.00001995
Iteration 110/1000 | Loss: 0.00001994
Iteration 111/1000 | Loss: 0.00001994
Iteration 112/1000 | Loss: 0.00001994
Iteration 113/1000 | Loss: 0.00001994
Iteration 114/1000 | Loss: 0.00001994
Iteration 115/1000 | Loss: 0.00001994
Iteration 116/1000 | Loss: 0.00001994
Iteration 117/1000 | Loss: 0.00001994
Iteration 118/1000 | Loss: 0.00001994
Iteration 119/1000 | Loss: 0.00001994
Iteration 120/1000 | Loss: 0.00001994
Iteration 121/1000 | Loss: 0.00001994
Iteration 122/1000 | Loss: 0.00001994
Iteration 123/1000 | Loss: 0.00001994
Iteration 124/1000 | Loss: 0.00001994
Iteration 125/1000 | Loss: 0.00001994
Iteration 126/1000 | Loss: 0.00001994
Iteration 127/1000 | Loss: 0.00001994
Iteration 128/1000 | Loss: 0.00001994
Iteration 129/1000 | Loss: 0.00001994
Iteration 130/1000 | Loss: 0.00001994
Iteration 131/1000 | Loss: 0.00001994
Iteration 132/1000 | Loss: 0.00001994
Iteration 133/1000 | Loss: 0.00001994
Iteration 134/1000 | Loss: 0.00001994
Iteration 135/1000 | Loss: 0.00001994
Iteration 136/1000 | Loss: 0.00001994
Iteration 137/1000 | Loss: 0.00001994
Iteration 138/1000 | Loss: 0.00001994
Iteration 139/1000 | Loss: 0.00001994
Iteration 140/1000 | Loss: 0.00001994
Iteration 141/1000 | Loss: 0.00001994
Iteration 142/1000 | Loss: 0.00001994
Iteration 143/1000 | Loss: 0.00001994
Iteration 144/1000 | Loss: 0.00001994
Iteration 145/1000 | Loss: 0.00001994
Iteration 146/1000 | Loss: 0.00001994
Iteration 147/1000 | Loss: 0.00001994
Iteration 148/1000 | Loss: 0.00001994
Iteration 149/1000 | Loss: 0.00001994
Iteration 150/1000 | Loss: 0.00001994
Iteration 151/1000 | Loss: 0.00001994
Iteration 152/1000 | Loss: 0.00001994
Iteration 153/1000 | Loss: 0.00001994
Iteration 154/1000 | Loss: 0.00001994
Iteration 155/1000 | Loss: 0.00001994
Iteration 156/1000 | Loss: 0.00001994
Iteration 157/1000 | Loss: 0.00001994
Iteration 158/1000 | Loss: 0.00001994
Iteration 159/1000 | Loss: 0.00001994
Iteration 160/1000 | Loss: 0.00001994
Iteration 161/1000 | Loss: 0.00001994
Iteration 162/1000 | Loss: 0.00001994
Iteration 163/1000 | Loss: 0.00001994
Iteration 164/1000 | Loss: 0.00001994
Iteration 165/1000 | Loss: 0.00001994
Iteration 166/1000 | Loss: 0.00001994
Iteration 167/1000 | Loss: 0.00001994
Iteration 168/1000 | Loss: 0.00001994
Iteration 169/1000 | Loss: 0.00001994
Iteration 170/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.9936054741265252e-05, 1.9936054741265252e-05, 1.9936054741265252e-05, 1.9936054741265252e-05, 1.9936054741265252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9936054741265252e-05

Optimization complete. Final v2v error: 3.9333672523498535 mm

Highest mean error: 4.163326740264893 mm for frame 157

Lowest mean error: 3.706611156463623 mm for frame 75

Saving results

Total time: 35.61925935745239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01131682
Iteration 2/25 | Loss: 0.01131682
Iteration 3/25 | Loss: 0.01131682
Iteration 4/25 | Loss: 0.00282617
Iteration 5/25 | Loss: 0.00172514
Iteration 6/25 | Loss: 0.00169509
Iteration 7/25 | Loss: 0.00171870
Iteration 8/25 | Loss: 0.00140767
Iteration 9/25 | Loss: 0.00136213
Iteration 10/25 | Loss: 0.00131113
Iteration 11/25 | Loss: 0.00128085
Iteration 12/25 | Loss: 0.00125063
Iteration 13/25 | Loss: 0.00124886
Iteration 14/25 | Loss: 0.00124581
Iteration 15/25 | Loss: 0.00122145
Iteration 16/25 | Loss: 0.00123571
Iteration 17/25 | Loss: 0.00121647
Iteration 18/25 | Loss: 0.00121296
Iteration 19/25 | Loss: 0.00121330
Iteration 20/25 | Loss: 0.00121536
Iteration 21/25 | Loss: 0.00121184
Iteration 22/25 | Loss: 0.00121551
Iteration 23/25 | Loss: 0.00121451
Iteration 24/25 | Loss: 0.00120213
Iteration 25/25 | Loss: 0.00120525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38185048
Iteration 2/25 | Loss: 0.00975443
Iteration 3/25 | Loss: 0.00849816
Iteration 4/25 | Loss: 0.00533667
Iteration 5/25 | Loss: 0.00478606
Iteration 6/25 | Loss: 0.00478566
Iteration 7/25 | Loss: 0.00478566
Iteration 8/25 | Loss: 0.00478566
Iteration 9/25 | Loss: 0.00478566
Iteration 10/25 | Loss: 0.00478565
Iteration 11/25 | Loss: 0.00478565
Iteration 12/25 | Loss: 0.00478565
Iteration 13/25 | Loss: 0.00478565
Iteration 14/25 | Loss: 0.00478565
Iteration 15/25 | Loss: 0.00478565
Iteration 16/25 | Loss: 0.00478565
Iteration 17/25 | Loss: 0.00478565
Iteration 18/25 | Loss: 0.00478565
Iteration 19/25 | Loss: 0.00478565
Iteration 20/25 | Loss: 0.00478565
Iteration 21/25 | Loss: 0.00478565
Iteration 22/25 | Loss: 0.00478565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0047856541350483894, 0.0047856541350483894, 0.0047856541350483894, 0.0047856541350483894, 0.0047856541350483894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0047856541350483894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00478565
Iteration 2/1000 | Loss: 0.00540993
Iteration 3/1000 | Loss: 0.00552362
Iteration 4/1000 | Loss: 0.00368164
Iteration 5/1000 | Loss: 0.00277857
Iteration 6/1000 | Loss: 0.00366150
Iteration 7/1000 | Loss: 0.00398804
Iteration 8/1000 | Loss: 0.00455464
Iteration 9/1000 | Loss: 0.00703848
Iteration 10/1000 | Loss: 0.00638606
Iteration 11/1000 | Loss: 0.00408139
Iteration 12/1000 | Loss: 0.00809100
Iteration 13/1000 | Loss: 0.01334000
Iteration 14/1000 | Loss: 0.00754592
Iteration 15/1000 | Loss: 0.01134646
Iteration 16/1000 | Loss: 0.01362529
Iteration 17/1000 | Loss: 0.00588928
Iteration 18/1000 | Loss: 0.01008175
Iteration 19/1000 | Loss: 0.00948164
Iteration 20/1000 | Loss: 0.01057930
Iteration 21/1000 | Loss: 0.00972046
Iteration 22/1000 | Loss: 0.00792687
Iteration 23/1000 | Loss: 0.00384063
Iteration 24/1000 | Loss: 0.00432913
Iteration 25/1000 | Loss: 0.00385859
Iteration 26/1000 | Loss: 0.00451592
Iteration 27/1000 | Loss: 0.00432090
Iteration 28/1000 | Loss: 0.00274162
Iteration 29/1000 | Loss: 0.00244606
Iteration 30/1000 | Loss: 0.00368587
Iteration 31/1000 | Loss: 0.00271228
Iteration 32/1000 | Loss: 0.00285067
Iteration 33/1000 | Loss: 0.00277173
Iteration 34/1000 | Loss: 0.00359882
Iteration 35/1000 | Loss: 0.00262766
Iteration 36/1000 | Loss: 0.00220818
Iteration 37/1000 | Loss: 0.00269224
Iteration 38/1000 | Loss: 0.00309894
Iteration 39/1000 | Loss: 0.00450073
Iteration 40/1000 | Loss: 0.00281229
Iteration 41/1000 | Loss: 0.00411449
Iteration 42/1000 | Loss: 0.00368461
Iteration 43/1000 | Loss: 0.00388241
Iteration 44/1000 | Loss: 0.00325967
Iteration 45/1000 | Loss: 0.00295698
Iteration 46/1000 | Loss: 0.00281600
Iteration 47/1000 | Loss: 0.00282731
Iteration 48/1000 | Loss: 0.00524608
Iteration 49/1000 | Loss: 0.00292825
Iteration 50/1000 | Loss: 0.00376294
Iteration 51/1000 | Loss: 0.00381632
Iteration 52/1000 | Loss: 0.00348469
Iteration 53/1000 | Loss: 0.00291651
Iteration 54/1000 | Loss: 0.00258744
Iteration 55/1000 | Loss: 0.00518054
Iteration 56/1000 | Loss: 0.00329910
Iteration 57/1000 | Loss: 0.00567509
Iteration 58/1000 | Loss: 0.00330404
Iteration 59/1000 | Loss: 0.00392340
Iteration 60/1000 | Loss: 0.00343086
Iteration 61/1000 | Loss: 0.00285953
Iteration 62/1000 | Loss: 0.00452725
Iteration 63/1000 | Loss: 0.00387658
Iteration 64/1000 | Loss: 0.00418939
Iteration 65/1000 | Loss: 0.00326362
Iteration 66/1000 | Loss: 0.00412820
Iteration 67/1000 | Loss: 0.00212446
Iteration 68/1000 | Loss: 0.00404880
Iteration 69/1000 | Loss: 0.00230211
Iteration 70/1000 | Loss: 0.00173777
Iteration 71/1000 | Loss: 0.00213431
Iteration 72/1000 | Loss: 0.00203247
Iteration 73/1000 | Loss: 0.00227407
Iteration 74/1000 | Loss: 0.00238438
Iteration 75/1000 | Loss: 0.00322071
Iteration 76/1000 | Loss: 0.00227193
Iteration 77/1000 | Loss: 0.00198300
Iteration 78/1000 | Loss: 0.00229077
Iteration 79/1000 | Loss: 0.00317013
Iteration 80/1000 | Loss: 0.00267758
Iteration 81/1000 | Loss: 0.00258138
Iteration 82/1000 | Loss: 0.00230913
Iteration 83/1000 | Loss: 0.00237582
Iteration 84/1000 | Loss: 0.00242658
Iteration 85/1000 | Loss: 0.00216650
Iteration 86/1000 | Loss: 0.00208942
Iteration 87/1000 | Loss: 0.00269224
Iteration 88/1000 | Loss: 0.00232342
Iteration 89/1000 | Loss: 0.00262742
Iteration 90/1000 | Loss: 0.00235221
Iteration 91/1000 | Loss: 0.00221413
Iteration 92/1000 | Loss: 0.00219593
Iteration 93/1000 | Loss: 0.00230258
Iteration 94/1000 | Loss: 0.00415021
Iteration 95/1000 | Loss: 0.00256108
Iteration 96/1000 | Loss: 0.00245548
Iteration 97/1000 | Loss: 0.00248599
Iteration 98/1000 | Loss: 0.00479151
Iteration 99/1000 | Loss: 0.00261341
Iteration 100/1000 | Loss: 0.00414068
Iteration 101/1000 | Loss: 0.00283533
Iteration 102/1000 | Loss: 0.00274430
Iteration 103/1000 | Loss: 0.00567404
Iteration 104/1000 | Loss: 0.00245511
Iteration 105/1000 | Loss: 0.00215651
Iteration 106/1000 | Loss: 0.00227728
Iteration 107/1000 | Loss: 0.00300694
Iteration 108/1000 | Loss: 0.00479635
Iteration 109/1000 | Loss: 0.00371794
Iteration 110/1000 | Loss: 0.00266021
Iteration 111/1000 | Loss: 0.00218585
Iteration 112/1000 | Loss: 0.00216374
Iteration 113/1000 | Loss: 0.00195098
Iteration 114/1000 | Loss: 0.00274613
Iteration 115/1000 | Loss: 0.00306590
Iteration 116/1000 | Loss: 0.00262347
Iteration 117/1000 | Loss: 0.00251769
Iteration 118/1000 | Loss: 0.00217907
Iteration 119/1000 | Loss: 0.00348258
Iteration 120/1000 | Loss: 0.00266190
Iteration 121/1000 | Loss: 0.00489729
Iteration 122/1000 | Loss: 0.00446887
Iteration 123/1000 | Loss: 0.00493055
Iteration 124/1000 | Loss: 0.00462110
Iteration 125/1000 | Loss: 0.00279879
Iteration 126/1000 | Loss: 0.00470804
Iteration 127/1000 | Loss: 0.00228504
Iteration 128/1000 | Loss: 0.00297674
Iteration 129/1000 | Loss: 0.00312909
Iteration 130/1000 | Loss: 0.00239574
Iteration 131/1000 | Loss: 0.00241103
Iteration 132/1000 | Loss: 0.00520907
Iteration 133/1000 | Loss: 0.00264489
Iteration 134/1000 | Loss: 0.00262305
Iteration 135/1000 | Loss: 0.00344510
Iteration 136/1000 | Loss: 0.00204966
Iteration 137/1000 | Loss: 0.00220413
Iteration 138/1000 | Loss: 0.00267392
Iteration 139/1000 | Loss: 0.00275295
Iteration 140/1000 | Loss: 0.00280135
Iteration 141/1000 | Loss: 0.00434069
Iteration 142/1000 | Loss: 0.00242661
Iteration 143/1000 | Loss: 0.00250730
Iteration 144/1000 | Loss: 0.00421302
Iteration 145/1000 | Loss: 0.00403864
Iteration 146/1000 | Loss: 0.00251352
Iteration 147/1000 | Loss: 0.00270667
Iteration 148/1000 | Loss: 0.00306735
Iteration 149/1000 | Loss: 0.00249282
Iteration 150/1000 | Loss: 0.00419371
Iteration 151/1000 | Loss: 0.00673539
Iteration 152/1000 | Loss: 0.00555979
Iteration 153/1000 | Loss: 0.00826792
Iteration 154/1000 | Loss: 0.00346890
Iteration 155/1000 | Loss: 0.00432241
Iteration 156/1000 | Loss: 0.00321541
Iteration 157/1000 | Loss: 0.00342104
Iteration 158/1000 | Loss: 0.00352821
Iteration 159/1000 | Loss: 0.00314012
Iteration 160/1000 | Loss: 0.00353955
Iteration 161/1000 | Loss: 0.00291929
Iteration 162/1000 | Loss: 0.00323256
Iteration 163/1000 | Loss: 0.00424934
Iteration 164/1000 | Loss: 0.00343647
Iteration 165/1000 | Loss: 0.00290002
Iteration 166/1000 | Loss: 0.00347328
Iteration 167/1000 | Loss: 0.00268860
Iteration 168/1000 | Loss: 0.00335956
Iteration 169/1000 | Loss: 0.00270974
Iteration 170/1000 | Loss: 0.00586897
Iteration 171/1000 | Loss: 0.00366579
Iteration 172/1000 | Loss: 0.00339215
Iteration 173/1000 | Loss: 0.00274811
Iteration 174/1000 | Loss: 0.00493415
Iteration 175/1000 | Loss: 0.00287124
Iteration 176/1000 | Loss: 0.00504800
Iteration 177/1000 | Loss: 0.00181664
Iteration 178/1000 | Loss: 0.00149715
Iteration 179/1000 | Loss: 0.00310327
Iteration 180/1000 | Loss: 0.00229000
Iteration 181/1000 | Loss: 0.00356215
Iteration 182/1000 | Loss: 0.00346676
Iteration 183/1000 | Loss: 0.00210108
Iteration 184/1000 | Loss: 0.00189266
Iteration 185/1000 | Loss: 0.00235101
Iteration 186/1000 | Loss: 0.00256654
Iteration 187/1000 | Loss: 0.00170634
Iteration 188/1000 | Loss: 0.00271936
Iteration 189/1000 | Loss: 0.00428393
Iteration 190/1000 | Loss: 0.00225301
Iteration 191/1000 | Loss: 0.00409360
Iteration 192/1000 | Loss: 0.00398375
Iteration 193/1000 | Loss: 0.00323884
Iteration 194/1000 | Loss: 0.00250453
Iteration 195/1000 | Loss: 0.00372801
Iteration 196/1000 | Loss: 0.00345721
Iteration 197/1000 | Loss: 0.00343235
Iteration 198/1000 | Loss: 0.00472642
Iteration 199/1000 | Loss: 0.00291252
Iteration 200/1000 | Loss: 0.00359599
Iteration 201/1000 | Loss: 0.00225946
Iteration 202/1000 | Loss: 0.00224028
Iteration 203/1000 | Loss: 0.00178990
Iteration 204/1000 | Loss: 0.00340959
Iteration 205/1000 | Loss: 0.00294725
Iteration 206/1000 | Loss: 0.00483567
Iteration 207/1000 | Loss: 0.00308826
Iteration 208/1000 | Loss: 0.00193701
Iteration 209/1000 | Loss: 0.00271821
Iteration 210/1000 | Loss: 0.00287301
Iteration 211/1000 | Loss: 0.00224311
Iteration 212/1000 | Loss: 0.00310640
Iteration 213/1000 | Loss: 0.00199022
Iteration 214/1000 | Loss: 0.00281919
Iteration 215/1000 | Loss: 0.00412766
Iteration 216/1000 | Loss: 0.00379456
Iteration 217/1000 | Loss: 0.00381407
Iteration 218/1000 | Loss: 0.00203632
Iteration 219/1000 | Loss: 0.00245262
Iteration 220/1000 | Loss: 0.00210909
Iteration 221/1000 | Loss: 0.00230241
Iteration 222/1000 | Loss: 0.00263017
Iteration 223/1000 | Loss: 0.00342364
Iteration 224/1000 | Loss: 0.00268631
Iteration 225/1000 | Loss: 0.00366758
Iteration 226/1000 | Loss: 0.00215697
Iteration 227/1000 | Loss: 0.00217352
Iteration 228/1000 | Loss: 0.00207280
Iteration 229/1000 | Loss: 0.00259204
Iteration 230/1000 | Loss: 0.00293114
Iteration 231/1000 | Loss: 0.00229338
Iteration 232/1000 | Loss: 0.00333435
Iteration 233/1000 | Loss: 0.00220366
Iteration 234/1000 | Loss: 0.00266057
Iteration 235/1000 | Loss: 0.00210398
Iteration 236/1000 | Loss: 0.00256771
Iteration 237/1000 | Loss: 0.00435496
Iteration 238/1000 | Loss: 0.00195202
Iteration 239/1000 | Loss: 0.00210379
Iteration 240/1000 | Loss: 0.00268286
Iteration 241/1000 | Loss: 0.00220784
Iteration 242/1000 | Loss: 0.00215311
Iteration 243/1000 | Loss: 0.00222568
Iteration 244/1000 | Loss: 0.00566940
Iteration 245/1000 | Loss: 0.00434190
Iteration 246/1000 | Loss: 0.00688794
Iteration 247/1000 | Loss: 0.00280574
Iteration 248/1000 | Loss: 0.00345319
Iteration 249/1000 | Loss: 0.00217289
Iteration 250/1000 | Loss: 0.00477266
Iteration 251/1000 | Loss: 0.00522217
Iteration 252/1000 | Loss: 0.00353301
Iteration 253/1000 | Loss: 0.00493321
Iteration 254/1000 | Loss: 0.00303238
Iteration 255/1000 | Loss: 0.00268627
Iteration 256/1000 | Loss: 0.00213713
Iteration 257/1000 | Loss: 0.00207083
Iteration 258/1000 | Loss: 0.00451335
Iteration 259/1000 | Loss: 0.00448781
Iteration 260/1000 | Loss: 0.00526377
Iteration 261/1000 | Loss: 0.00541948
Iteration 262/1000 | Loss: 0.00464623
Iteration 263/1000 | Loss: 0.00304676
Iteration 264/1000 | Loss: 0.00242731
Iteration 265/1000 | Loss: 0.00307523
Iteration 266/1000 | Loss: 0.00346117
Iteration 267/1000 | Loss: 0.00253269
Iteration 268/1000 | Loss: 0.00237460
Iteration 269/1000 | Loss: 0.00578609
Iteration 270/1000 | Loss: 0.00547239
Iteration 271/1000 | Loss: 0.00355823
Iteration 272/1000 | Loss: 0.00414099
Iteration 273/1000 | Loss: 0.00377308
Iteration 274/1000 | Loss: 0.00206539
Iteration 275/1000 | Loss: 0.00192816
Iteration 276/1000 | Loss: 0.00207163
Iteration 277/1000 | Loss: 0.00291914
Iteration 278/1000 | Loss: 0.00194929
Iteration 279/1000 | Loss: 0.00466948
Iteration 280/1000 | Loss: 0.00283711
Iteration 281/1000 | Loss: 0.00228604
Iteration 282/1000 | Loss: 0.00165705
Iteration 283/1000 | Loss: 0.00223116
Iteration 284/1000 | Loss: 0.00204686
Iteration 285/1000 | Loss: 0.00199393
Iteration 286/1000 | Loss: 0.00245125
Iteration 287/1000 | Loss: 0.00345881
Iteration 288/1000 | Loss: 0.00295597
Iteration 289/1000 | Loss: 0.00208657
Iteration 290/1000 | Loss: 0.00181484
Iteration 291/1000 | Loss: 0.00278253
Iteration 292/1000 | Loss: 0.00477620
Iteration 293/1000 | Loss: 0.00377640
Iteration 294/1000 | Loss: 0.00629750
Iteration 295/1000 | Loss: 0.00362139
Iteration 296/1000 | Loss: 0.00255177
Iteration 297/1000 | Loss: 0.00189985
Iteration 298/1000 | Loss: 0.00331579
Iteration 299/1000 | Loss: 0.00281727
Iteration 300/1000 | Loss: 0.00228879
Iteration 301/1000 | Loss: 0.00297519
Iteration 302/1000 | Loss: 0.00245854
Iteration 303/1000 | Loss: 0.00235808
Iteration 304/1000 | Loss: 0.00238401
Iteration 305/1000 | Loss: 0.00264357
Iteration 306/1000 | Loss: 0.00294740
Iteration 307/1000 | Loss: 0.00272500
Iteration 308/1000 | Loss: 0.00266637
Iteration 309/1000 | Loss: 0.00224444
Iteration 310/1000 | Loss: 0.00220091
Iteration 311/1000 | Loss: 0.00355260
Iteration 312/1000 | Loss: 0.00280036
Iteration 313/1000 | Loss: 0.00240983
Iteration 314/1000 | Loss: 0.00225588
Iteration 315/1000 | Loss: 0.00215902
Iteration 316/1000 | Loss: 0.00323383
Iteration 317/1000 | Loss: 0.00232522
Iteration 318/1000 | Loss: 0.00362532
Iteration 319/1000 | Loss: 0.00283646
Iteration 320/1000 | Loss: 0.00244112
Iteration 321/1000 | Loss: 0.00227209
Iteration 322/1000 | Loss: 0.00212526
Iteration 323/1000 | Loss: 0.00207501
Iteration 324/1000 | Loss: 0.00231710
Iteration 325/1000 | Loss: 0.00210600
Iteration 326/1000 | Loss: 0.00206588
Iteration 327/1000 | Loss: 0.00198402
Iteration 328/1000 | Loss: 0.00183871
Iteration 329/1000 | Loss: 0.00180433
Iteration 330/1000 | Loss: 0.00201636
Iteration 331/1000 | Loss: 0.00233101
Iteration 332/1000 | Loss: 0.00192085
Iteration 333/1000 | Loss: 0.00310135
Iteration 334/1000 | Loss: 0.00189258
Iteration 335/1000 | Loss: 0.00279342
Iteration 336/1000 | Loss: 0.00202489
Iteration 337/1000 | Loss: 0.00328028
Iteration 338/1000 | Loss: 0.00205115
Iteration 339/1000 | Loss: 0.00217236
Iteration 340/1000 | Loss: 0.00200007
Iteration 341/1000 | Loss: 0.00198744
Iteration 342/1000 | Loss: 0.00275083
Iteration 343/1000 | Loss: 0.00201587
Iteration 344/1000 | Loss: 0.00174747
Iteration 345/1000 | Loss: 0.00334472
Iteration 346/1000 | Loss: 0.00145682
Iteration 347/1000 | Loss: 0.00157733
Iteration 348/1000 | Loss: 0.00158189
Iteration 349/1000 | Loss: 0.00176841
Iteration 350/1000 | Loss: 0.00246607
Iteration 351/1000 | Loss: 0.00244127
Iteration 352/1000 | Loss: 0.00272881
Iteration 353/1000 | Loss: 0.00194580
Iteration 354/1000 | Loss: 0.00228605
Iteration 355/1000 | Loss: 0.00195148
Iteration 356/1000 | Loss: 0.00315497
Iteration 357/1000 | Loss: 0.00334697
Iteration 358/1000 | Loss: 0.00219348
Iteration 359/1000 | Loss: 0.00543370
Iteration 360/1000 | Loss: 0.00190880
Iteration 361/1000 | Loss: 0.00192101
Iteration 362/1000 | Loss: 0.00215897
Iteration 363/1000 | Loss: 0.00159976
Iteration 364/1000 | Loss: 0.00202430
Iteration 365/1000 | Loss: 0.00194932
Iteration 366/1000 | Loss: 0.00214106
Iteration 367/1000 | Loss: 0.00231665
Iteration 368/1000 | Loss: 0.00214623
Iteration 369/1000 | Loss: 0.00310623
Iteration 370/1000 | Loss: 0.00204175
Iteration 371/1000 | Loss: 0.00288273
Iteration 372/1000 | Loss: 0.00267308
Iteration 373/1000 | Loss: 0.00332758
Iteration 374/1000 | Loss: 0.00240193
Iteration 375/1000 | Loss: 0.00179343
Iteration 376/1000 | Loss: 0.00192241
Iteration 377/1000 | Loss: 0.00300181
Iteration 378/1000 | Loss: 0.00318206
Iteration 379/1000 | Loss: 0.00162813
Iteration 380/1000 | Loss: 0.00290785
Iteration 381/1000 | Loss: 0.00186971
Iteration 382/1000 | Loss: 0.00228383
Iteration 383/1000 | Loss: 0.00267822
Iteration 384/1000 | Loss: 0.00248120
Iteration 385/1000 | Loss: 0.00349443
Iteration 386/1000 | Loss: 0.00163690
Iteration 387/1000 | Loss: 0.00181403
Iteration 388/1000 | Loss: 0.00129181
Iteration 389/1000 | Loss: 0.00193496
Iteration 390/1000 | Loss: 0.00208506
Iteration 391/1000 | Loss: 0.00159031
Iteration 392/1000 | Loss: 0.00285114
Iteration 393/1000 | Loss: 0.00221508
Iteration 394/1000 | Loss: 0.00177031
Iteration 395/1000 | Loss: 0.00225597
Iteration 396/1000 | Loss: 0.00399365
Iteration 397/1000 | Loss: 0.00219287
Iteration 398/1000 | Loss: 0.00286723
Iteration 399/1000 | Loss: 0.00200141
Iteration 400/1000 | Loss: 0.00166904
Iteration 401/1000 | Loss: 0.00162703
Iteration 402/1000 | Loss: 0.00187507
Iteration 403/1000 | Loss: 0.00209282
Iteration 404/1000 | Loss: 0.00164275
Iteration 405/1000 | Loss: 0.00347170
Iteration 406/1000 | Loss: 0.00173243
Iteration 407/1000 | Loss: 0.00324454
Iteration 408/1000 | Loss: 0.00276960
Iteration 409/1000 | Loss: 0.00199150
Iteration 410/1000 | Loss: 0.00244571
Iteration 411/1000 | Loss: 0.00238288
Iteration 412/1000 | Loss: 0.00282817
Iteration 413/1000 | Loss: 0.00199761
Iteration 414/1000 | Loss: 0.00213519
Iteration 415/1000 | Loss: 0.00181807
Iteration 416/1000 | Loss: 0.00231567
Iteration 417/1000 | Loss: 0.00192673
Iteration 418/1000 | Loss: 0.00187764
Iteration 419/1000 | Loss: 0.00212699
Iteration 420/1000 | Loss: 0.00161449
Iteration 421/1000 | Loss: 0.00154472
Iteration 422/1000 | Loss: 0.00153321
Iteration 423/1000 | Loss: 0.00148976
Iteration 424/1000 | Loss: 0.00252203
Iteration 425/1000 | Loss: 0.00164286
Iteration 426/1000 | Loss: 0.00244512
Iteration 427/1000 | Loss: 0.00174805
Iteration 428/1000 | Loss: 0.00226584
Iteration 429/1000 | Loss: 0.00154807
Iteration 430/1000 | Loss: 0.00248305
Iteration 431/1000 | Loss: 0.00179821
Iteration 432/1000 | Loss: 0.00138626
Iteration 433/1000 | Loss: 0.00153552
Iteration 434/1000 | Loss: 0.00265585
Iteration 435/1000 | Loss: 0.00215603
Iteration 436/1000 | Loss: 0.00242845
Iteration 437/1000 | Loss: 0.00158456
Iteration 438/1000 | Loss: 0.00164326
Iteration 439/1000 | Loss: 0.00316390
Iteration 440/1000 | Loss: 0.00176564
Iteration 441/1000 | Loss: 0.00159772
Iteration 442/1000 | Loss: 0.00169353
Iteration 443/1000 | Loss: 0.00258111
Iteration 444/1000 | Loss: 0.00206333
Iteration 445/1000 | Loss: 0.00199011
Iteration 446/1000 | Loss: 0.00182945
Iteration 447/1000 | Loss: 0.00178908
Iteration 448/1000 | Loss: 0.00180255
Iteration 449/1000 | Loss: 0.00173874
Iteration 450/1000 | Loss: 0.00171016
Iteration 451/1000 | Loss: 0.00163104
Iteration 452/1000 | Loss: 0.00141917
Iteration 453/1000 | Loss: 0.00168558
Iteration 454/1000 | Loss: 0.00186357
Iteration 455/1000 | Loss: 0.00147283
Iteration 456/1000 | Loss: 0.00172309
Iteration 457/1000 | Loss: 0.00180231
Iteration 458/1000 | Loss: 0.00278412
Iteration 459/1000 | Loss: 0.00279609
Iteration 460/1000 | Loss: 0.00282349
Iteration 461/1000 | Loss: 0.00150270
Iteration 462/1000 | Loss: 0.00151288
Iteration 463/1000 | Loss: 0.00156059
Iteration 464/1000 | Loss: 0.00256045
Iteration 465/1000 | Loss: 0.00176957
Iteration 466/1000 | Loss: 0.00155127
Iteration 467/1000 | Loss: 0.00158149
Iteration 468/1000 | Loss: 0.00151023
Iteration 469/1000 | Loss: 0.00321883
Iteration 470/1000 | Loss: 0.00203259
Iteration 471/1000 | Loss: 0.00224118
Iteration 472/1000 | Loss: 0.00231392
Iteration 473/1000 | Loss: 0.00255486
Iteration 474/1000 | Loss: 0.00259636
Iteration 475/1000 | Loss: 0.00177984
Iteration 476/1000 | Loss: 0.00319926
Iteration 477/1000 | Loss: 0.00180618
Iteration 478/1000 | Loss: 0.00169320
Iteration 479/1000 | Loss: 0.00169408
Iteration 480/1000 | Loss: 0.00164319
Iteration 481/1000 | Loss: 0.00171728
Iteration 482/1000 | Loss: 0.00277427
Iteration 483/1000 | Loss: 0.00265103
Iteration 484/1000 | Loss: 0.00158058
Iteration 485/1000 | Loss: 0.00178571
Iteration 486/1000 | Loss: 0.00178024
Iteration 487/1000 | Loss: 0.00171261
Iteration 488/1000 | Loss: 0.00284048
Iteration 489/1000 | Loss: 0.00156943
Iteration 490/1000 | Loss: 0.00276895
Iteration 491/1000 | Loss: 0.00174569
Iteration 492/1000 | Loss: 0.00183022
Iteration 493/1000 | Loss: 0.00156029
Iteration 494/1000 | Loss: 0.00233596
Iteration 495/1000 | Loss: 0.00183438
Iteration 496/1000 | Loss: 0.00165597
Iteration 497/1000 | Loss: 0.00165348
Iteration 498/1000 | Loss: 0.00189863
Iteration 499/1000 | Loss: 0.00158574
Iteration 500/1000 | Loss: 0.00157817
Iteration 501/1000 | Loss: 0.00162614
Iteration 502/1000 | Loss: 0.00211179
Iteration 503/1000 | Loss: 0.00166521
Iteration 504/1000 | Loss: 0.00152568
Iteration 505/1000 | Loss: 0.00220733
Iteration 506/1000 | Loss: 0.00198171
Iteration 507/1000 | Loss: 0.00130441
Iteration 508/1000 | Loss: 0.00207617
Iteration 509/1000 | Loss: 0.00138508
Iteration 510/1000 | Loss: 0.00115587
Iteration 511/1000 | Loss: 0.00147423
Iteration 512/1000 | Loss: 0.00174512
Iteration 513/1000 | Loss: 0.00169101
Iteration 514/1000 | Loss: 0.00146280
Iteration 515/1000 | Loss: 0.00177944
Iteration 516/1000 | Loss: 0.00193163
Iteration 517/1000 | Loss: 0.00344016
Iteration 518/1000 | Loss: 0.00158354
Iteration 519/1000 | Loss: 0.00253466
Iteration 520/1000 | Loss: 0.00206506
Iteration 521/1000 | Loss: 0.00425758
Iteration 522/1000 | Loss: 0.00199433
Iteration 523/1000 | Loss: 0.00174862
Iteration 524/1000 | Loss: 0.00151425
Iteration 525/1000 | Loss: 0.00154702
Iteration 526/1000 | Loss: 0.00169669
Iteration 527/1000 | Loss: 0.00152609
Iteration 528/1000 | Loss: 0.00381512
Iteration 529/1000 | Loss: 0.00182763
Iteration 530/1000 | Loss: 0.00201344
Iteration 531/1000 | Loss: 0.00199313
Iteration 532/1000 | Loss: 0.00178055
Iteration 533/1000 | Loss: 0.00242378
Iteration 534/1000 | Loss: 0.00155088
Iteration 535/1000 | Loss: 0.00165992
Iteration 536/1000 | Loss: 0.00168043
Iteration 537/1000 | Loss: 0.00168906
Iteration 538/1000 | Loss: 0.00164232
Iteration 539/1000 | Loss: 0.00159679
Iteration 540/1000 | Loss: 0.00191781
Iteration 541/1000 | Loss: 0.00200383
Iteration 542/1000 | Loss: 0.00349783
Iteration 543/1000 | Loss: 0.00233594
Iteration 544/1000 | Loss: 0.00176584
Iteration 545/1000 | Loss: 0.00167912
Iteration 546/1000 | Loss: 0.00224843
Iteration 547/1000 | Loss: 0.00160935
Iteration 548/1000 | Loss: 0.00173214
Iteration 549/1000 | Loss: 0.00165868
Iteration 550/1000 | Loss: 0.00280785
Iteration 551/1000 | Loss: 0.00181701
Iteration 552/1000 | Loss: 0.00199469
Iteration 553/1000 | Loss: 0.00167952
Iteration 554/1000 | Loss: 0.00187469
Iteration 555/1000 | Loss: 0.00183849
Iteration 556/1000 | Loss: 0.00204483
Iteration 557/1000 | Loss: 0.00195074
Iteration 558/1000 | Loss: 0.00220397
Iteration 559/1000 | Loss: 0.00196986
Iteration 560/1000 | Loss: 0.00183994
Iteration 561/1000 | Loss: 0.00198699
Iteration 562/1000 | Loss: 0.00139619
Iteration 563/1000 | Loss: 0.00201168
Iteration 564/1000 | Loss: 0.00236577
Iteration 565/1000 | Loss: 0.00243171
Iteration 566/1000 | Loss: 0.00286471
Iteration 567/1000 | Loss: 0.00198371
Iteration 568/1000 | Loss: 0.00149551
Iteration 569/1000 | Loss: 0.00171062
Iteration 570/1000 | Loss: 0.00152180
Iteration 571/1000 | Loss: 0.00202259
Iteration 572/1000 | Loss: 0.00154153
Iteration 573/1000 | Loss: 0.00407701
Iteration 574/1000 | Loss: 0.00160090
Iteration 575/1000 | Loss: 0.00151932
Iteration 576/1000 | Loss: 0.00125875
Iteration 577/1000 | Loss: 0.00153488
Iteration 578/1000 | Loss: 0.00153680
Iteration 579/1000 | Loss: 0.00167677
Iteration 580/1000 | Loss: 0.00203085
Iteration 581/1000 | Loss: 0.00169826
Iteration 582/1000 | Loss: 0.00156691
Iteration 583/1000 | Loss: 0.00169529
Iteration 584/1000 | Loss: 0.00273199
Iteration 585/1000 | Loss: 0.00165600
Iteration 586/1000 | Loss: 0.00175170
Iteration 587/1000 | Loss: 0.00172266
Iteration 588/1000 | Loss: 0.00206101
Iteration 589/1000 | Loss: 0.00195970
Iteration 590/1000 | Loss: 0.00161419
Iteration 591/1000 | Loss: 0.00177695
Iteration 592/1000 | Loss: 0.00235756
Iteration 593/1000 | Loss: 0.00266067
Iteration 594/1000 | Loss: 0.00209565
Iteration 595/1000 | Loss: 0.00185239
Iteration 596/1000 | Loss: 0.00144260
Iteration 597/1000 | Loss: 0.00158976
Iteration 598/1000 | Loss: 0.00156119
Iteration 599/1000 | Loss: 0.00156327
Iteration 600/1000 | Loss: 0.00242719
Iteration 601/1000 | Loss: 0.00174544
Iteration 602/1000 | Loss: 0.00167630
Iteration 603/1000 | Loss: 0.00187654
Iteration 604/1000 | Loss: 0.00128760
Iteration 605/1000 | Loss: 0.00240457
Iteration 606/1000 | Loss: 0.00167328
Iteration 607/1000 | Loss: 0.00161284
Iteration 608/1000 | Loss: 0.00137628
Iteration 609/1000 | Loss: 0.00149331
Iteration 610/1000 | Loss: 0.00163428
Iteration 611/1000 | Loss: 0.00168079
Iteration 612/1000 | Loss: 0.00152404
Iteration 613/1000 | Loss: 0.00234013
Iteration 614/1000 | Loss: 0.00153259
Iteration 615/1000 | Loss: 0.00169962
Iteration 616/1000 | Loss: 0.00149181
Iteration 617/1000 | Loss: 0.00184506
Iteration 618/1000 | Loss: 0.00157007
Iteration 619/1000 | Loss: 0.00172995
Iteration 620/1000 | Loss: 0.00220860
Iteration 621/1000 | Loss: 0.00321543
Iteration 622/1000 | Loss: 0.00219971
Iteration 623/1000 | Loss: 0.00314312
Iteration 624/1000 | Loss: 0.00475103
Iteration 625/1000 | Loss: 0.00416205
Iteration 626/1000 | Loss: 0.00384310
Iteration 627/1000 | Loss: 0.00276798
Iteration 628/1000 | Loss: 0.00177771
Iteration 629/1000 | Loss: 0.00332043
Iteration 630/1000 | Loss: 0.00180750
Iteration 631/1000 | Loss: 0.00198028
Iteration 632/1000 | Loss: 0.00177480
Iteration 633/1000 | Loss: 0.00199786
Iteration 634/1000 | Loss: 0.00140108
Iteration 635/1000 | Loss: 0.00127386
Iteration 636/1000 | Loss: 0.00160238
Iteration 637/1000 | Loss: 0.00136183
Iteration 638/1000 | Loss: 0.00119782
Iteration 639/1000 | Loss: 0.00223963
Iteration 640/1000 | Loss: 0.00217163
Iteration 641/1000 | Loss: 0.00118295
Iteration 642/1000 | Loss: 0.00120685
Iteration 643/1000 | Loss: 0.00188158
Iteration 644/1000 | Loss: 0.00124038
Iteration 645/1000 | Loss: 0.00146252
Iteration 646/1000 | Loss: 0.00201221
Iteration 647/1000 | Loss: 0.00150083
Iteration 648/1000 | Loss: 0.00183294
Iteration 649/1000 | Loss: 0.00148260
Iteration 650/1000 | Loss: 0.00094468
Iteration 651/1000 | Loss: 0.00223748
Iteration 652/1000 | Loss: 0.00122313
Iteration 653/1000 | Loss: 0.00122502
Iteration 654/1000 | Loss: 0.00106533
Iteration 655/1000 | Loss: 0.00114250
Iteration 656/1000 | Loss: 0.00154583
Iteration 657/1000 | Loss: 0.00127622
Iteration 658/1000 | Loss: 0.00204708
Iteration 659/1000 | Loss: 0.00276061
Iteration 660/1000 | Loss: 0.00137071
Iteration 661/1000 | Loss: 0.00127693
Iteration 662/1000 | Loss: 0.00137527
Iteration 663/1000 | Loss: 0.00248738
Iteration 664/1000 | Loss: 0.00143603
Iteration 665/1000 | Loss: 0.00157102
Iteration 666/1000 | Loss: 0.00167527
Iteration 667/1000 | Loss: 0.00305035
Iteration 668/1000 | Loss: 0.00215242
Iteration 669/1000 | Loss: 0.00155943
Iteration 670/1000 | Loss: 0.00152129
Iteration 671/1000 | Loss: 0.00133777
Iteration 672/1000 | Loss: 0.00157771
Iteration 673/1000 | Loss: 0.00144582
Iteration 674/1000 | Loss: 0.00163700
Iteration 675/1000 | Loss: 0.00145595
Iteration 676/1000 | Loss: 0.00309642
Iteration 677/1000 | Loss: 0.00174083
Iteration 678/1000 | Loss: 0.00226169
Iteration 679/1000 | Loss: 0.00169861
Iteration 680/1000 | Loss: 0.00206541
Iteration 681/1000 | Loss: 0.00189180
Iteration 682/1000 | Loss: 0.00151020
Iteration 683/1000 | Loss: 0.00168203
Iteration 684/1000 | Loss: 0.00155869
Iteration 685/1000 | Loss: 0.00159836
Iteration 686/1000 | Loss: 0.00323986
Iteration 687/1000 | Loss: 0.00145639
Iteration 688/1000 | Loss: 0.00225609
Iteration 689/1000 | Loss: 0.00194178
Iteration 690/1000 | Loss: 0.00196138
Iteration 691/1000 | Loss: 0.00168991
Iteration 692/1000 | Loss: 0.00180448
Iteration 693/1000 | Loss: 0.00160916
Iteration 694/1000 | Loss: 0.00169193
Iteration 695/1000 | Loss: 0.00140459
Iteration 696/1000 | Loss: 0.00165020
Iteration 697/1000 | Loss: 0.00148189
Iteration 698/1000 | Loss: 0.00286632
Iteration 699/1000 | Loss: 0.00302670
Iteration 700/1000 | Loss: 0.00207067
Iteration 701/1000 | Loss: 0.00183248
Iteration 702/1000 | Loss: 0.00258773
Iteration 703/1000 | Loss: 0.00158334
Iteration 704/1000 | Loss: 0.00236050
Iteration 705/1000 | Loss: 0.00222896
Iteration 706/1000 | Loss: 0.00164430
Iteration 707/1000 | Loss: 0.00176119
Iteration 708/1000 | Loss: 0.00151683
Iteration 709/1000 | Loss: 0.00108587
Iteration 710/1000 | Loss: 0.00119068
Iteration 711/1000 | Loss: 0.00132760
Iteration 712/1000 | Loss: 0.00131911
Iteration 713/1000 | Loss: 0.00154817
Iteration 714/1000 | Loss: 0.00132180
Iteration 715/1000 | Loss: 0.00132406
Iteration 716/1000 | Loss: 0.00115717
Iteration 717/1000 | Loss: 0.00195947
Iteration 718/1000 | Loss: 0.00102395
Iteration 719/1000 | Loss: 0.00087560
Iteration 720/1000 | Loss: 0.00140840
Iteration 721/1000 | Loss: 0.00136975
Iteration 722/1000 | Loss: 0.00230247
Iteration 723/1000 | Loss: 0.00207011
Iteration 724/1000 | Loss: 0.00147244
Iteration 725/1000 | Loss: 0.00209908
Iteration 726/1000 | Loss: 0.00070542
Iteration 727/1000 | Loss: 0.00044513
Iteration 728/1000 | Loss: 0.00046791
Iteration 729/1000 | Loss: 0.00043219
Iteration 730/1000 | Loss: 0.00050222
Iteration 731/1000 | Loss: 0.00042010
Iteration 732/1000 | Loss: 0.00034064
Iteration 733/1000 | Loss: 0.00041656
Iteration 734/1000 | Loss: 0.00041026
Iteration 735/1000 | Loss: 0.00041173
Iteration 736/1000 | Loss: 0.00031475
Iteration 737/1000 | Loss: 0.00166060
Iteration 738/1000 | Loss: 0.00162334
Iteration 739/1000 | Loss: 0.00032100
Iteration 740/1000 | Loss: 0.00031751
Iteration 741/1000 | Loss: 0.00021526
Iteration 742/1000 | Loss: 0.00020730
Iteration 743/1000 | Loss: 0.00032885
Iteration 744/1000 | Loss: 0.00029205
Iteration 745/1000 | Loss: 0.00033765
Iteration 746/1000 | Loss: 0.00035307
Iteration 747/1000 | Loss: 0.00050909
Iteration 748/1000 | Loss: 0.00134208
Iteration 749/1000 | Loss: 0.00020139
Iteration 750/1000 | Loss: 0.00037651
Iteration 751/1000 | Loss: 0.00056055
Iteration 752/1000 | Loss: 0.00151197
Iteration 753/1000 | Loss: 0.00058262
Iteration 754/1000 | Loss: 0.00080793
Iteration 755/1000 | Loss: 0.00077149
Iteration 756/1000 | Loss: 0.00093504
Iteration 757/1000 | Loss: 0.00066384
Iteration 758/1000 | Loss: 0.00070403
Iteration 759/1000 | Loss: 0.00037463
Iteration 760/1000 | Loss: 0.00034061
Iteration 761/1000 | Loss: 0.00040482
Iteration 762/1000 | Loss: 0.00092342
Iteration 763/1000 | Loss: 0.00092135
Iteration 764/1000 | Loss: 0.00072026
Iteration 765/1000 | Loss: 0.00032182
Iteration 766/1000 | Loss: 0.00037229
Iteration 767/1000 | Loss: 0.00040651
Iteration 768/1000 | Loss: 0.00038102
Iteration 769/1000 | Loss: 0.00044766
Iteration 770/1000 | Loss: 0.00035797
Iteration 771/1000 | Loss: 0.00072189
Iteration 772/1000 | Loss: 0.00040124
Iteration 773/1000 | Loss: 0.00041667
Iteration 774/1000 | Loss: 0.00040793
Iteration 775/1000 | Loss: 0.00042517
Iteration 776/1000 | Loss: 0.00038755
Iteration 777/1000 | Loss: 0.00044857
Iteration 778/1000 | Loss: 0.00036739
Iteration 779/1000 | Loss: 0.00060408
Iteration 780/1000 | Loss: 0.00052606
Iteration 781/1000 | Loss: 0.00043383
Iteration 782/1000 | Loss: 0.00096507
Iteration 783/1000 | Loss: 0.00058453
Iteration 784/1000 | Loss: 0.00026149
Iteration 785/1000 | Loss: 0.00033482
Iteration 786/1000 | Loss: 0.00039886
Iteration 787/1000 | Loss: 0.00042415
Iteration 788/1000 | Loss: 0.00040609
Iteration 789/1000 | Loss: 0.00039288
Iteration 790/1000 | Loss: 0.00039933
Iteration 791/1000 | Loss: 0.00096463
Iteration 792/1000 | Loss: 0.00046331
Iteration 793/1000 | Loss: 0.00050737
Iteration 794/1000 | Loss: 0.00043156
Iteration 795/1000 | Loss: 0.00087610
Iteration 796/1000 | Loss: 0.00040341
Iteration 797/1000 | Loss: 0.00059070
Iteration 798/1000 | Loss: 0.00087359
Iteration 799/1000 | Loss: 0.00117917
Iteration 800/1000 | Loss: 0.00069635
Iteration 801/1000 | Loss: 0.00075150
Iteration 802/1000 | Loss: 0.00041675
Iteration 803/1000 | Loss: 0.00108769
Iteration 804/1000 | Loss: 0.00045337
Iteration 805/1000 | Loss: 0.00043463
Iteration 806/1000 | Loss: 0.00040936
Iteration 807/1000 | Loss: 0.00035221
Iteration 808/1000 | Loss: 0.00043672
Iteration 809/1000 | Loss: 0.00035984
Iteration 810/1000 | Loss: 0.00039435
Iteration 811/1000 | Loss: 0.00035680
Iteration 812/1000 | Loss: 0.00238063
Iteration 813/1000 | Loss: 0.00096211
Iteration 814/1000 | Loss: 0.00046287
Iteration 815/1000 | Loss: 0.00177934
Iteration 816/1000 | Loss: 0.00064409
Iteration 817/1000 | Loss: 0.00035275
Iteration 818/1000 | Loss: 0.00043002
Iteration 819/1000 | Loss: 0.00045724
Iteration 820/1000 | Loss: 0.00085247
Iteration 821/1000 | Loss: 0.00115610
Iteration 822/1000 | Loss: 0.00058687
Iteration 823/1000 | Loss: 0.00175955
Iteration 824/1000 | Loss: 0.00078437
Iteration 825/1000 | Loss: 0.00041722
Iteration 826/1000 | Loss: 0.00135397
Iteration 827/1000 | Loss: 0.00047534
Iteration 828/1000 | Loss: 0.00029003
Iteration 829/1000 | Loss: 0.00043568
Iteration 830/1000 | Loss: 0.00021129
Iteration 831/1000 | Loss: 0.00128667
Iteration 832/1000 | Loss: 0.00049415
Iteration 833/1000 | Loss: 0.00055705
Iteration 834/1000 | Loss: 0.00037803
Iteration 835/1000 | Loss: 0.00038550
Iteration 836/1000 | Loss: 0.00038224
Iteration 837/1000 | Loss: 0.00038625
Iteration 838/1000 | Loss: 0.00031543
Iteration 839/1000 | Loss: 0.00033409
Iteration 840/1000 | Loss: 0.00085335
Iteration 841/1000 | Loss: 0.00096635
Iteration 842/1000 | Loss: 0.00056667
Iteration 843/1000 | Loss: 0.00037689
Iteration 844/1000 | Loss: 0.00032815
Iteration 845/1000 | Loss: 0.00030338
Iteration 846/1000 | Loss: 0.00037743
Iteration 847/1000 | Loss: 0.00030503
Iteration 848/1000 | Loss: 0.00036813
Iteration 849/1000 | Loss: 0.00038218
Iteration 850/1000 | Loss: 0.00044599
Iteration 851/1000 | Loss: 0.00037010
Iteration 852/1000 | Loss: 0.00061469
Iteration 853/1000 | Loss: 0.00040599
Iteration 854/1000 | Loss: 0.00037535
Iteration 855/1000 | Loss: 0.00038498
Iteration 856/1000 | Loss: 0.00036501
Iteration 857/1000 | Loss: 0.00037319
Iteration 858/1000 | Loss: 0.00035490
Iteration 859/1000 | Loss: 0.00034570
Iteration 860/1000 | Loss: 0.00039472
Iteration 861/1000 | Loss: 0.00040817
Iteration 862/1000 | Loss: 0.00038274
Iteration 863/1000 | Loss: 0.00066069
Iteration 864/1000 | Loss: 0.00060815
Iteration 865/1000 | Loss: 0.00084741
Iteration 866/1000 | Loss: 0.00036292
Iteration 867/1000 | Loss: 0.00052361
Iteration 868/1000 | Loss: 0.00116749
Iteration 869/1000 | Loss: 0.00052745
Iteration 870/1000 | Loss: 0.00045548
Iteration 871/1000 | Loss: 0.00045972
Iteration 872/1000 | Loss: 0.00035639
Iteration 873/1000 | Loss: 0.00037418
Iteration 874/1000 | Loss: 0.00037472
Iteration 875/1000 | Loss: 0.00059669
Iteration 876/1000 | Loss: 0.00038861
Iteration 877/1000 | Loss: 0.00033869
Iteration 878/1000 | Loss: 0.00036047
Iteration 879/1000 | Loss: 0.00072413
Iteration 880/1000 | Loss: 0.00038700
Iteration 881/1000 | Loss: 0.00060812
Iteration 882/1000 | Loss: 0.00036145
Iteration 883/1000 | Loss: 0.00034343
Iteration 884/1000 | Loss: 0.00061249
Iteration 885/1000 | Loss: 0.00032610
Iteration 886/1000 | Loss: 0.00062218
Iteration 887/1000 | Loss: 0.00036050
Iteration 888/1000 | Loss: 0.00056727
Iteration 889/1000 | Loss: 0.00030475
Iteration 890/1000 | Loss: 0.00036408
Iteration 891/1000 | Loss: 0.00035982
Iteration 892/1000 | Loss: 0.00041484
Iteration 893/1000 | Loss: 0.00035652
Iteration 894/1000 | Loss: 0.00034944
Iteration 895/1000 | Loss: 0.00104830
Iteration 896/1000 | Loss: 0.00046720
Iteration 897/1000 | Loss: 0.00040407
Iteration 898/1000 | Loss: 0.00065850
Iteration 899/1000 | Loss: 0.00053957
Iteration 900/1000 | Loss: 0.00086286
Iteration 901/1000 | Loss: 0.00052405
Iteration 902/1000 | Loss: 0.00029299
Iteration 903/1000 | Loss: 0.00072997
Iteration 904/1000 | Loss: 0.00039189
Iteration 905/1000 | Loss: 0.00021881
Iteration 906/1000 | Loss: 0.00029950
Iteration 907/1000 | Loss: 0.00050390
Iteration 908/1000 | Loss: 0.00106070
Iteration 909/1000 | Loss: 0.00022144
Iteration 910/1000 | Loss: 0.00023405
Iteration 911/1000 | Loss: 0.00026017
Iteration 912/1000 | Loss: 0.00119644
Iteration 913/1000 | Loss: 0.00025242
Iteration 914/1000 | Loss: 0.00023206
Iteration 915/1000 | Loss: 0.00037718
Iteration 916/1000 | Loss: 0.00083979
Iteration 917/1000 | Loss: 0.00033432
Iteration 918/1000 | Loss: 0.00060335
Iteration 919/1000 | Loss: 0.00031972
Iteration 920/1000 | Loss: 0.00032357
Iteration 921/1000 | Loss: 0.00027924
Iteration 922/1000 | Loss: 0.00033384
Iteration 923/1000 | Loss: 0.00036119
Iteration 924/1000 | Loss: 0.00036154
Iteration 925/1000 | Loss: 0.00050208
Iteration 926/1000 | Loss: 0.00038509
Iteration 927/1000 | Loss: 0.00038207
Iteration 928/1000 | Loss: 0.00037619
Iteration 929/1000 | Loss: 0.00033801
Iteration 930/1000 | Loss: 0.00030181
Iteration 931/1000 | Loss: 0.00033965
Iteration 932/1000 | Loss: 0.00074986
Iteration 933/1000 | Loss: 0.00100578
Iteration 934/1000 | Loss: 0.00056371
Iteration 935/1000 | Loss: 0.00089676
Iteration 936/1000 | Loss: 0.00032676
Iteration 937/1000 | Loss: 0.00058246
Iteration 938/1000 | Loss: 0.00022240
Iteration 939/1000 | Loss: 0.00038576
Iteration 940/1000 | Loss: 0.00030960
Iteration 941/1000 | Loss: 0.00074912
Iteration 942/1000 | Loss: 0.00047508
Iteration 943/1000 | Loss: 0.00053362
Iteration 944/1000 | Loss: 0.00051140
Iteration 945/1000 | Loss: 0.00064347
Iteration 946/1000 | Loss: 0.00021980
Iteration 947/1000 | Loss: 0.00029164
Iteration 948/1000 | Loss: 0.00037959
Iteration 949/1000 | Loss: 0.00035435
Iteration 950/1000 | Loss: 0.00044303
Iteration 951/1000 | Loss: 0.00036684
Iteration 952/1000 | Loss: 0.00069511
Iteration 953/1000 | Loss: 0.00062953
Iteration 954/1000 | Loss: 0.00037337
Iteration 955/1000 | Loss: 0.00050002
Iteration 956/1000 | Loss: 0.00073821
Iteration 957/1000 | Loss: 0.00028253
Iteration 958/1000 | Loss: 0.00044823
Iteration 959/1000 | Loss: 0.00062290
Iteration 960/1000 | Loss: 0.00043495
Iteration 961/1000 | Loss: 0.00065431
Iteration 962/1000 | Loss: 0.00035760
Iteration 963/1000 | Loss: 0.00094468
Iteration 964/1000 | Loss: 0.00104995
Iteration 965/1000 | Loss: 0.00033029
Iteration 966/1000 | Loss: 0.00044328
Iteration 967/1000 | Loss: 0.00036024
Iteration 968/1000 | Loss: 0.00039841
Iteration 969/1000 | Loss: 0.00036322
Iteration 970/1000 | Loss: 0.00129460
Iteration 971/1000 | Loss: 0.00062669
Iteration 972/1000 | Loss: 0.00037066
Iteration 973/1000 | Loss: 0.00039016
Iteration 974/1000 | Loss: 0.00049102
Iteration 975/1000 | Loss: 0.00033927
Iteration 976/1000 | Loss: 0.00036850
Iteration 977/1000 | Loss: 0.00138636
Iteration 978/1000 | Loss: 0.00042054
Iteration 979/1000 | Loss: 0.00046853
Iteration 980/1000 | Loss: 0.00047402
Iteration 981/1000 | Loss: 0.00035758
Iteration 982/1000 | Loss: 0.00047837
Iteration 983/1000 | Loss: 0.00038596
Iteration 984/1000 | Loss: 0.00066133
Iteration 985/1000 | Loss: 0.00060226
Iteration 986/1000 | Loss: 0.00040980
Iteration 987/1000 | Loss: 0.00029313
Iteration 988/1000 | Loss: 0.00038415
Iteration 989/1000 | Loss: 0.00026233
Iteration 990/1000 | Loss: 0.00032374
Iteration 991/1000 | Loss: 0.00028482
Iteration 992/1000 | Loss: 0.00029460
Iteration 993/1000 | Loss: 0.00077725
Iteration 994/1000 | Loss: 0.00053294
Iteration 995/1000 | Loss: 0.00036083
Iteration 996/1000 | Loss: 0.00055782
Iteration 997/1000 | Loss: 0.00041789
Iteration 998/1000 | Loss: 0.00025773
Iteration 999/1000 | Loss: 0.00084346
Iteration 1000/1000 | Loss: 0.00031909

Optimization complete. Final v2v error: 6.721779823303223 mm

Highest mean error: 222.7435302734375 mm for frame 56

Lowest mean error: 3.8344454765319824 mm for frame 136

Saving results

Total time: 1646.3652336597443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086717
Iteration 2/25 | Loss: 0.00241302
Iteration 3/25 | Loss: 0.00160703
Iteration 4/25 | Loss: 0.00134596
Iteration 5/25 | Loss: 0.00125526
Iteration 6/25 | Loss: 0.00124666
Iteration 7/25 | Loss: 0.00118401
Iteration 8/25 | Loss: 0.00111160
Iteration 9/25 | Loss: 0.00106646
Iteration 10/25 | Loss: 0.00105638
Iteration 11/25 | Loss: 0.00103907
Iteration 12/25 | Loss: 0.00103082
Iteration 13/25 | Loss: 0.00103306
Iteration 14/25 | Loss: 0.00102001
Iteration 15/25 | Loss: 0.00100268
Iteration 16/25 | Loss: 0.00100813
Iteration 17/25 | Loss: 0.00099195
Iteration 18/25 | Loss: 0.00097956
Iteration 19/25 | Loss: 0.00097459
Iteration 20/25 | Loss: 0.00099192
Iteration 21/25 | Loss: 0.00098530
Iteration 22/25 | Loss: 0.00098380
Iteration 23/25 | Loss: 0.00097473
Iteration 24/25 | Loss: 0.00096560
Iteration 25/25 | Loss: 0.00096244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97450495
Iteration 2/25 | Loss: 0.00068188
Iteration 3/25 | Loss: 0.00068187
Iteration 4/25 | Loss: 0.00068187
Iteration 5/25 | Loss: 0.00068187
Iteration 6/25 | Loss: 0.00068187
Iteration 7/25 | Loss: 0.00068187
Iteration 8/25 | Loss: 0.00068187
Iteration 9/25 | Loss: 0.00068187
Iteration 10/25 | Loss: 0.00068187
Iteration 11/25 | Loss: 0.00068187
Iteration 12/25 | Loss: 0.00068187
Iteration 13/25 | Loss: 0.00068187
Iteration 14/25 | Loss: 0.00068187
Iteration 15/25 | Loss: 0.00068187
Iteration 16/25 | Loss: 0.00068187
Iteration 17/25 | Loss: 0.00068187
Iteration 18/25 | Loss: 0.00068187
Iteration 19/25 | Loss: 0.00068187
Iteration 20/25 | Loss: 0.00068187
Iteration 21/25 | Loss: 0.00068187
Iteration 22/25 | Loss: 0.00068187
Iteration 23/25 | Loss: 0.00068187
Iteration 24/25 | Loss: 0.00068187
Iteration 25/25 | Loss: 0.00068187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068187
Iteration 2/1000 | Loss: 0.00052024
Iteration 3/1000 | Loss: 0.00008275
Iteration 4/1000 | Loss: 0.00008140
Iteration 5/1000 | Loss: 0.00007350
Iteration 6/1000 | Loss: 0.00006617
Iteration 7/1000 | Loss: 0.00006755
Iteration 8/1000 | Loss: 0.00006721
Iteration 9/1000 | Loss: 0.00007069
Iteration 10/1000 | Loss: 0.00008134
Iteration 11/1000 | Loss: 0.00005708
Iteration 12/1000 | Loss: 0.00005666
Iteration 13/1000 | Loss: 0.00006298
Iteration 14/1000 | Loss: 0.00006032
Iteration 15/1000 | Loss: 0.00006014
Iteration 16/1000 | Loss: 0.00006745
Iteration 17/1000 | Loss: 0.00008222
Iteration 18/1000 | Loss: 0.00006258
Iteration 19/1000 | Loss: 0.00007801
Iteration 20/1000 | Loss: 0.00008989
Iteration 21/1000 | Loss: 0.00007823
Iteration 22/1000 | Loss: 0.00006162
Iteration 23/1000 | Loss: 0.00007515
Iteration 24/1000 | Loss: 0.00005964
Iteration 25/1000 | Loss: 0.00007730
Iteration 26/1000 | Loss: 0.00008202
Iteration 27/1000 | Loss: 0.00004327
Iteration 28/1000 | Loss: 0.00004033
Iteration 29/1000 | Loss: 0.00003949
Iteration 30/1000 | Loss: 0.00003903
Iteration 31/1000 | Loss: 0.00003871
Iteration 32/1000 | Loss: 0.00003842
Iteration 33/1000 | Loss: 0.00003884
Iteration 34/1000 | Loss: 0.00003834
Iteration 35/1000 | Loss: 0.00003803
Iteration 36/1000 | Loss: 0.00003784
Iteration 37/1000 | Loss: 0.00003780
Iteration 38/1000 | Loss: 0.00003780
Iteration 39/1000 | Loss: 0.00003780
Iteration 40/1000 | Loss: 0.00003779
Iteration 41/1000 | Loss: 0.00003813
Iteration 42/1000 | Loss: 0.00003795
Iteration 43/1000 | Loss: 0.00003804
Iteration 44/1000 | Loss: 0.00003794
Iteration 45/1000 | Loss: 0.00003801
Iteration 46/1000 | Loss: 0.00003793
Iteration 47/1000 | Loss: 0.00003802
Iteration 48/1000 | Loss: 0.00003781
Iteration 49/1000 | Loss: 0.00003780
Iteration 50/1000 | Loss: 0.00003780
Iteration 51/1000 | Loss: 0.00003780
Iteration 52/1000 | Loss: 0.00003780
Iteration 53/1000 | Loss: 0.00003780
Iteration 54/1000 | Loss: 0.00003779
Iteration 55/1000 | Loss: 0.00003779
Iteration 56/1000 | Loss: 0.00003794
Iteration 57/1000 | Loss: 0.00003794
Iteration 58/1000 | Loss: 0.00003791
Iteration 59/1000 | Loss: 0.00003776
Iteration 60/1000 | Loss: 0.00003776
Iteration 61/1000 | Loss: 0.00003775
Iteration 62/1000 | Loss: 0.00003775
Iteration 63/1000 | Loss: 0.00003774
Iteration 64/1000 | Loss: 0.00003768
Iteration 65/1000 | Loss: 0.00003768
Iteration 66/1000 | Loss: 0.00003766
Iteration 67/1000 | Loss: 0.00003765
Iteration 68/1000 | Loss: 0.00003765
Iteration 69/1000 | Loss: 0.00003765
Iteration 70/1000 | Loss: 0.00003765
Iteration 71/1000 | Loss: 0.00003764
Iteration 72/1000 | Loss: 0.00003764
Iteration 73/1000 | Loss: 0.00003764
Iteration 74/1000 | Loss: 0.00003763
Iteration 75/1000 | Loss: 0.00003763
Iteration 76/1000 | Loss: 0.00003763
Iteration 77/1000 | Loss: 0.00003763
Iteration 78/1000 | Loss: 0.00003763
Iteration 79/1000 | Loss: 0.00003763
Iteration 80/1000 | Loss: 0.00003763
Iteration 81/1000 | Loss: 0.00003763
Iteration 82/1000 | Loss: 0.00003763
Iteration 83/1000 | Loss: 0.00003763
Iteration 84/1000 | Loss: 0.00003763
Iteration 85/1000 | Loss: 0.00003762
Iteration 86/1000 | Loss: 0.00003762
Iteration 87/1000 | Loss: 0.00003762
Iteration 88/1000 | Loss: 0.00003762
Iteration 89/1000 | Loss: 0.00003762
Iteration 90/1000 | Loss: 0.00003762
Iteration 91/1000 | Loss: 0.00003762
Iteration 92/1000 | Loss: 0.00003762
Iteration 93/1000 | Loss: 0.00003762
Iteration 94/1000 | Loss: 0.00003762
Iteration 95/1000 | Loss: 0.00003762
Iteration 96/1000 | Loss: 0.00003762
Iteration 97/1000 | Loss: 0.00003762
Iteration 98/1000 | Loss: 0.00003762
Iteration 99/1000 | Loss: 0.00003762
Iteration 100/1000 | Loss: 0.00003762
Iteration 101/1000 | Loss: 0.00003762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [3.762050255318172e-05, 3.762050255318172e-05, 3.762050255318172e-05, 3.762050255318172e-05, 3.762050255318172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.762050255318172e-05

Optimization complete. Final v2v error: 4.856167316436768 mm

Highest mean error: 21.956096649169922 mm for frame 48

Lowest mean error: 4.486469745635986 mm for frame 77

Saving results

Total time: 108.8658173084259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450435
Iteration 2/25 | Loss: 0.00114346
Iteration 3/25 | Loss: 0.00105382
Iteration 4/25 | Loss: 0.00102602
Iteration 5/25 | Loss: 0.00101694
Iteration 6/25 | Loss: 0.00101548
Iteration 7/25 | Loss: 0.00101529
Iteration 8/25 | Loss: 0.00101529
Iteration 9/25 | Loss: 0.00101529
Iteration 10/25 | Loss: 0.00101529
Iteration 11/25 | Loss: 0.00101529
Iteration 12/25 | Loss: 0.00101529
Iteration 13/25 | Loss: 0.00101529
Iteration 14/25 | Loss: 0.00101529
Iteration 15/25 | Loss: 0.00101529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010152903851121664, 0.0010152903851121664, 0.0010152903851121664, 0.0010152903851121664, 0.0010152903851121664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010152903851121664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80222249
Iteration 2/25 | Loss: 0.00079067
Iteration 3/25 | Loss: 0.00079067
Iteration 4/25 | Loss: 0.00079067
Iteration 5/25 | Loss: 0.00079067
Iteration 6/25 | Loss: 0.00079067
Iteration 7/25 | Loss: 0.00079067
Iteration 8/25 | Loss: 0.00079067
Iteration 9/25 | Loss: 0.00079067
Iteration 10/25 | Loss: 0.00079067
Iteration 11/25 | Loss: 0.00079067
Iteration 12/25 | Loss: 0.00079067
Iteration 13/25 | Loss: 0.00079067
Iteration 14/25 | Loss: 0.00079067
Iteration 15/25 | Loss: 0.00079067
Iteration 16/25 | Loss: 0.00079067
Iteration 17/25 | Loss: 0.00079067
Iteration 18/25 | Loss: 0.00079067
Iteration 19/25 | Loss: 0.00079067
Iteration 20/25 | Loss: 0.00079067
Iteration 21/25 | Loss: 0.00079067
Iteration 22/25 | Loss: 0.00079067
Iteration 23/25 | Loss: 0.00079067
Iteration 24/25 | Loss: 0.00079067
Iteration 25/25 | Loss: 0.00079067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079067
Iteration 2/1000 | Loss: 0.00010795
Iteration 3/1000 | Loss: 0.00006698
Iteration 4/1000 | Loss: 0.00005399
Iteration 5/1000 | Loss: 0.00005060
Iteration 6/1000 | Loss: 0.00004807
Iteration 7/1000 | Loss: 0.00004568
Iteration 8/1000 | Loss: 0.00004414
Iteration 9/1000 | Loss: 0.00004333
Iteration 10/1000 | Loss: 0.00004298
Iteration 11/1000 | Loss: 0.00004262
Iteration 12/1000 | Loss: 0.00004230
Iteration 13/1000 | Loss: 0.00004217
Iteration 14/1000 | Loss: 0.00004208
Iteration 15/1000 | Loss: 0.00004204
Iteration 16/1000 | Loss: 0.00004203
Iteration 17/1000 | Loss: 0.00004202
Iteration 18/1000 | Loss: 0.00004201
Iteration 19/1000 | Loss: 0.00004200
Iteration 20/1000 | Loss: 0.00004199
Iteration 21/1000 | Loss: 0.00004199
Iteration 22/1000 | Loss: 0.00004198
Iteration 23/1000 | Loss: 0.00004198
Iteration 24/1000 | Loss: 0.00004197
Iteration 25/1000 | Loss: 0.00004197
Iteration 26/1000 | Loss: 0.00004196
Iteration 27/1000 | Loss: 0.00004196
Iteration 28/1000 | Loss: 0.00004195
Iteration 29/1000 | Loss: 0.00004195
Iteration 30/1000 | Loss: 0.00004195
Iteration 31/1000 | Loss: 0.00004194
Iteration 32/1000 | Loss: 0.00004194
Iteration 33/1000 | Loss: 0.00004194
Iteration 34/1000 | Loss: 0.00004193
Iteration 35/1000 | Loss: 0.00004192
Iteration 36/1000 | Loss: 0.00004192
Iteration 37/1000 | Loss: 0.00004192
Iteration 38/1000 | Loss: 0.00004192
Iteration 39/1000 | Loss: 0.00004191
Iteration 40/1000 | Loss: 0.00004191
Iteration 41/1000 | Loss: 0.00004191
Iteration 42/1000 | Loss: 0.00004190
Iteration 43/1000 | Loss: 0.00004188
Iteration 44/1000 | Loss: 0.00004187
Iteration 45/1000 | Loss: 0.00004187
Iteration 46/1000 | Loss: 0.00004185
Iteration 47/1000 | Loss: 0.00004184
Iteration 48/1000 | Loss: 0.00004184
Iteration 49/1000 | Loss: 0.00004184
Iteration 50/1000 | Loss: 0.00004183
Iteration 51/1000 | Loss: 0.00004182
Iteration 52/1000 | Loss: 0.00004182
Iteration 53/1000 | Loss: 0.00004182
Iteration 54/1000 | Loss: 0.00004181
Iteration 55/1000 | Loss: 0.00004181
Iteration 56/1000 | Loss: 0.00004181
Iteration 57/1000 | Loss: 0.00004180
Iteration 58/1000 | Loss: 0.00004179
Iteration 59/1000 | Loss: 0.00004179
Iteration 60/1000 | Loss: 0.00004179
Iteration 61/1000 | Loss: 0.00004179
Iteration 62/1000 | Loss: 0.00004179
Iteration 63/1000 | Loss: 0.00004179
Iteration 64/1000 | Loss: 0.00004179
Iteration 65/1000 | Loss: 0.00004179
Iteration 66/1000 | Loss: 0.00004179
Iteration 67/1000 | Loss: 0.00004179
Iteration 68/1000 | Loss: 0.00004178
Iteration 69/1000 | Loss: 0.00004178
Iteration 70/1000 | Loss: 0.00004178
Iteration 71/1000 | Loss: 0.00004178
Iteration 72/1000 | Loss: 0.00004178
Iteration 73/1000 | Loss: 0.00004178
Iteration 74/1000 | Loss: 0.00004178
Iteration 75/1000 | Loss: 0.00004178
Iteration 76/1000 | Loss: 0.00004178
Iteration 77/1000 | Loss: 0.00004177
Iteration 78/1000 | Loss: 0.00004177
Iteration 79/1000 | Loss: 0.00004177
Iteration 80/1000 | Loss: 0.00004177
Iteration 81/1000 | Loss: 0.00004177
Iteration 82/1000 | Loss: 0.00004177
Iteration 83/1000 | Loss: 0.00004177
Iteration 84/1000 | Loss: 0.00004177
Iteration 85/1000 | Loss: 0.00004177
Iteration 86/1000 | Loss: 0.00004177
Iteration 87/1000 | Loss: 0.00004177
Iteration 88/1000 | Loss: 0.00004177
Iteration 89/1000 | Loss: 0.00004177
Iteration 90/1000 | Loss: 0.00004177
Iteration 91/1000 | Loss: 0.00004177
Iteration 92/1000 | Loss: 0.00004177
Iteration 93/1000 | Loss: 0.00004177
Iteration 94/1000 | Loss: 0.00004177
Iteration 95/1000 | Loss: 0.00004177
Iteration 96/1000 | Loss: 0.00004177
Iteration 97/1000 | Loss: 0.00004177
Iteration 98/1000 | Loss: 0.00004177
Iteration 99/1000 | Loss: 0.00004177
Iteration 100/1000 | Loss: 0.00004177
Iteration 101/1000 | Loss: 0.00004177
Iteration 102/1000 | Loss: 0.00004177
Iteration 103/1000 | Loss: 0.00004177
Iteration 104/1000 | Loss: 0.00004177
Iteration 105/1000 | Loss: 0.00004177
Iteration 106/1000 | Loss: 0.00004177
Iteration 107/1000 | Loss: 0.00004177
Iteration 108/1000 | Loss: 0.00004177
Iteration 109/1000 | Loss: 0.00004177
Iteration 110/1000 | Loss: 0.00004177
Iteration 111/1000 | Loss: 0.00004177
Iteration 112/1000 | Loss: 0.00004177
Iteration 113/1000 | Loss: 0.00004177
Iteration 114/1000 | Loss: 0.00004177
Iteration 115/1000 | Loss: 0.00004177
Iteration 116/1000 | Loss: 0.00004177
Iteration 117/1000 | Loss: 0.00004177
Iteration 118/1000 | Loss: 0.00004177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [4.177139271632768e-05, 4.177139271632768e-05, 4.177139271632768e-05, 4.177139271632768e-05, 4.177139271632768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.177139271632768e-05

Optimization complete. Final v2v error: 5.45063591003418 mm

Highest mean error: 5.804146766662598 mm for frame 83

Lowest mean error: 4.836178779602051 mm for frame 39

Saving results

Total time: 36.56854248046875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080042
Iteration 2/25 | Loss: 0.00187144
Iteration 3/25 | Loss: 0.00136753
Iteration 4/25 | Loss: 0.00131751
Iteration 5/25 | Loss: 0.00122922
Iteration 6/25 | Loss: 0.00118360
Iteration 7/25 | Loss: 0.00119179
Iteration 8/25 | Loss: 0.00112437
Iteration 9/25 | Loss: 0.00110636
Iteration 10/25 | Loss: 0.00108791
Iteration 11/25 | Loss: 0.00106919
Iteration 12/25 | Loss: 0.00106556
Iteration 13/25 | Loss: 0.00106408
Iteration 14/25 | Loss: 0.00106487
Iteration 15/25 | Loss: 0.00106759
Iteration 16/25 | Loss: 0.00106505
Iteration 17/25 | Loss: 0.00106710
Iteration 18/25 | Loss: 0.00106175
Iteration 19/25 | Loss: 0.00106122
Iteration 20/25 | Loss: 0.00106402
Iteration 21/25 | Loss: 0.00106189
Iteration 22/25 | Loss: 0.00106392
Iteration 23/25 | Loss: 0.00106575
Iteration 24/25 | Loss: 0.00106236
Iteration 25/25 | Loss: 0.00105883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29470515
Iteration 2/25 | Loss: 0.00087356
Iteration 3/25 | Loss: 0.00061498
Iteration 4/25 | Loss: 0.00061498
Iteration 5/25 | Loss: 0.00061498
Iteration 6/25 | Loss: 0.00061498
Iteration 7/25 | Loss: 0.00061497
Iteration 8/25 | Loss: 0.00061497
Iteration 9/25 | Loss: 0.00061497
Iteration 10/25 | Loss: 0.00061497
Iteration 11/25 | Loss: 0.00061497
Iteration 12/25 | Loss: 0.00061497
Iteration 13/25 | Loss: 0.00061497
Iteration 14/25 | Loss: 0.00061497
Iteration 15/25 | Loss: 0.00061497
Iteration 16/25 | Loss: 0.00061497
Iteration 17/25 | Loss: 0.00061497
Iteration 18/25 | Loss: 0.00061497
Iteration 19/25 | Loss: 0.00061497
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006149740074761212, 0.0006149740074761212, 0.0006149740074761212, 0.0006149740074761212, 0.0006149740074761212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006149740074761212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061497
Iteration 2/1000 | Loss: 0.00033128
Iteration 3/1000 | Loss: 0.00022864
Iteration 4/1000 | Loss: 0.00011644
Iteration 5/1000 | Loss: 0.00009112
Iteration 6/1000 | Loss: 0.00007352
Iteration 7/1000 | Loss: 0.00008804
Iteration 8/1000 | Loss: 0.00027595
Iteration 9/1000 | Loss: 0.00028827
Iteration 10/1000 | Loss: 0.00023775
Iteration 11/1000 | Loss: 0.00009402
Iteration 12/1000 | Loss: 0.00041673
Iteration 13/1000 | Loss: 0.00030513
Iteration 14/1000 | Loss: 0.00031094
Iteration 15/1000 | Loss: 0.00025073
Iteration 16/1000 | Loss: 0.00024637
Iteration 17/1000 | Loss: 0.00014494
Iteration 18/1000 | Loss: 0.00008905
Iteration 19/1000 | Loss: 0.00008794
Iteration 20/1000 | Loss: 0.00009236
Iteration 21/1000 | Loss: 0.00009288
Iteration 22/1000 | Loss: 0.00008783
Iteration 23/1000 | Loss: 0.00006539
Iteration 24/1000 | Loss: 0.00015103
Iteration 25/1000 | Loss: 0.00041113
Iteration 26/1000 | Loss: 0.00030033
Iteration 27/1000 | Loss: 0.00018126
Iteration 28/1000 | Loss: 0.00007078
Iteration 29/1000 | Loss: 0.00017358
Iteration 30/1000 | Loss: 0.00010121
Iteration 31/1000 | Loss: 0.00006803
Iteration 32/1000 | Loss: 0.00005876
Iteration 33/1000 | Loss: 0.00006027
Iteration 34/1000 | Loss: 0.00005909
Iteration 35/1000 | Loss: 0.00021966
Iteration 36/1000 | Loss: 0.00027048
Iteration 37/1000 | Loss: 0.00015419
Iteration 38/1000 | Loss: 0.00035073
Iteration 39/1000 | Loss: 0.00020900
Iteration 40/1000 | Loss: 0.00017135
Iteration 41/1000 | Loss: 0.00016064
Iteration 42/1000 | Loss: 0.00007913
Iteration 43/1000 | Loss: 0.00016197
Iteration 44/1000 | Loss: 0.00016226
Iteration 45/1000 | Loss: 0.00010647
Iteration 46/1000 | Loss: 0.00011204
Iteration 47/1000 | Loss: 0.00016220
Iteration 48/1000 | Loss: 0.00007874
Iteration 49/1000 | Loss: 0.00006185
Iteration 50/1000 | Loss: 0.00005641
Iteration 51/1000 | Loss: 0.00005674
Iteration 52/1000 | Loss: 0.00016846
Iteration 53/1000 | Loss: 0.00007395
Iteration 54/1000 | Loss: 0.00037505
Iteration 55/1000 | Loss: 0.00022189
Iteration 56/1000 | Loss: 0.00048863
Iteration 57/1000 | Loss: 0.00018199
Iteration 58/1000 | Loss: 0.00005489
Iteration 59/1000 | Loss: 0.00006567
Iteration 60/1000 | Loss: 0.00010205
Iteration 61/1000 | Loss: 0.00015015
Iteration 62/1000 | Loss: 0.00005889
Iteration 63/1000 | Loss: 0.00009632
Iteration 64/1000 | Loss: 0.00006092
Iteration 65/1000 | Loss: 0.00014330
Iteration 66/1000 | Loss: 0.00013525
Iteration 67/1000 | Loss: 0.00011671
Iteration 68/1000 | Loss: 0.00009962
Iteration 69/1000 | Loss: 0.00004114
Iteration 70/1000 | Loss: 0.00004032
Iteration 71/1000 | Loss: 0.00014529
Iteration 72/1000 | Loss: 0.00004586
Iteration 73/1000 | Loss: 0.00004101
Iteration 74/1000 | Loss: 0.00003932
Iteration 75/1000 | Loss: 0.00003847
Iteration 76/1000 | Loss: 0.00003807
Iteration 77/1000 | Loss: 0.00021506
Iteration 78/1000 | Loss: 0.00004596
Iteration 79/1000 | Loss: 0.00004269
Iteration 80/1000 | Loss: 0.00004086
Iteration 81/1000 | Loss: 0.00021368
Iteration 82/1000 | Loss: 0.00009567
Iteration 83/1000 | Loss: 0.00007594
Iteration 84/1000 | Loss: 0.00017962
Iteration 85/1000 | Loss: 0.00006470
Iteration 86/1000 | Loss: 0.00004541
Iteration 87/1000 | Loss: 0.00004270
Iteration 88/1000 | Loss: 0.00004165
Iteration 89/1000 | Loss: 0.00011771
Iteration 90/1000 | Loss: 0.00005085
Iteration 91/1000 | Loss: 0.00004342
Iteration 92/1000 | Loss: 0.00004190
Iteration 93/1000 | Loss: 0.00004087
Iteration 94/1000 | Loss: 0.00004019
Iteration 95/1000 | Loss: 0.00017333
Iteration 96/1000 | Loss: 0.00005002
Iteration 97/1000 | Loss: 0.00006017
Iteration 98/1000 | Loss: 0.00004735
Iteration 99/1000 | Loss: 0.00004557
Iteration 100/1000 | Loss: 0.00005048
Iteration 101/1000 | Loss: 0.00003769
Iteration 102/1000 | Loss: 0.00003762
Iteration 103/1000 | Loss: 0.00003755
Iteration 104/1000 | Loss: 0.00003754
Iteration 105/1000 | Loss: 0.00003754
Iteration 106/1000 | Loss: 0.00003752
Iteration 107/1000 | Loss: 0.00003745
Iteration 108/1000 | Loss: 0.00003744
Iteration 109/1000 | Loss: 0.00003743
Iteration 110/1000 | Loss: 0.00003743
Iteration 111/1000 | Loss: 0.00003743
Iteration 112/1000 | Loss: 0.00003743
Iteration 113/1000 | Loss: 0.00003743
Iteration 114/1000 | Loss: 0.00003742
Iteration 115/1000 | Loss: 0.00003739
Iteration 116/1000 | Loss: 0.00003738
Iteration 117/1000 | Loss: 0.00003738
Iteration 118/1000 | Loss: 0.00003737
Iteration 119/1000 | Loss: 0.00003734
Iteration 120/1000 | Loss: 0.00003733
Iteration 121/1000 | Loss: 0.00004553
Iteration 122/1000 | Loss: 0.00005815
Iteration 123/1000 | Loss: 0.00004781
Iteration 124/1000 | Loss: 0.00004916
Iteration 125/1000 | Loss: 0.00004264
Iteration 126/1000 | Loss: 0.00003763
Iteration 127/1000 | Loss: 0.00003725
Iteration 128/1000 | Loss: 0.00004429
Iteration 129/1000 | Loss: 0.00005485
Iteration 130/1000 | Loss: 0.00004505
Iteration 131/1000 | Loss: 0.00003912
Iteration 132/1000 | Loss: 0.00003760
Iteration 133/1000 | Loss: 0.00004296
Iteration 134/1000 | Loss: 0.00004261
Iteration 135/1000 | Loss: 0.00004298
Iteration 136/1000 | Loss: 0.00004062
Iteration 137/1000 | Loss: 0.00004716
Iteration 138/1000 | Loss: 0.00004829
Iteration 139/1000 | Loss: 0.00003756
Iteration 140/1000 | Loss: 0.00004366
Iteration 141/1000 | Loss: 0.00004070
Iteration 142/1000 | Loss: 0.00004784
Iteration 143/1000 | Loss: 0.00004773
Iteration 144/1000 | Loss: 0.00003726
Iteration 145/1000 | Loss: 0.00003718
Iteration 146/1000 | Loss: 0.00004324
Iteration 147/1000 | Loss: 0.00004053
Iteration 148/1000 | Loss: 0.00004593
Iteration 149/1000 | Loss: 0.00004927
Iteration 150/1000 | Loss: 0.00005175
Iteration 151/1000 | Loss: 0.00004527
Iteration 152/1000 | Loss: 0.00004119
Iteration 153/1000 | Loss: 0.00003959
Iteration 154/1000 | Loss: 0.00003865
Iteration 155/1000 | Loss: 0.00003812
Iteration 156/1000 | Loss: 0.00003760
Iteration 157/1000 | Loss: 0.00003734
Iteration 158/1000 | Loss: 0.00003730
Iteration 159/1000 | Loss: 0.00003715
Iteration 160/1000 | Loss: 0.00003706
Iteration 161/1000 | Loss: 0.00003702
Iteration 162/1000 | Loss: 0.00003702
Iteration 163/1000 | Loss: 0.00003701
Iteration 164/1000 | Loss: 0.00003701
Iteration 165/1000 | Loss: 0.00003700
Iteration 166/1000 | Loss: 0.00003700
Iteration 167/1000 | Loss: 0.00003699
Iteration 168/1000 | Loss: 0.00003699
Iteration 169/1000 | Loss: 0.00003699
Iteration 170/1000 | Loss: 0.00003699
Iteration 171/1000 | Loss: 0.00003698
Iteration 172/1000 | Loss: 0.00003698
Iteration 173/1000 | Loss: 0.00003698
Iteration 174/1000 | Loss: 0.00003698
Iteration 175/1000 | Loss: 0.00003698
Iteration 176/1000 | Loss: 0.00003698
Iteration 177/1000 | Loss: 0.00003697
Iteration 178/1000 | Loss: 0.00003697
Iteration 179/1000 | Loss: 0.00003697
Iteration 180/1000 | Loss: 0.00003697
Iteration 181/1000 | Loss: 0.00003697
Iteration 182/1000 | Loss: 0.00003697
Iteration 183/1000 | Loss: 0.00003697
Iteration 184/1000 | Loss: 0.00003697
Iteration 185/1000 | Loss: 0.00003696
Iteration 186/1000 | Loss: 0.00003696
Iteration 187/1000 | Loss: 0.00003696
Iteration 188/1000 | Loss: 0.00003696
Iteration 189/1000 | Loss: 0.00003696
Iteration 190/1000 | Loss: 0.00003696
Iteration 191/1000 | Loss: 0.00003696
Iteration 192/1000 | Loss: 0.00003696
Iteration 193/1000 | Loss: 0.00003696
Iteration 194/1000 | Loss: 0.00003695
Iteration 195/1000 | Loss: 0.00003695
Iteration 196/1000 | Loss: 0.00003695
Iteration 197/1000 | Loss: 0.00003695
Iteration 198/1000 | Loss: 0.00003695
Iteration 199/1000 | Loss: 0.00003695
Iteration 200/1000 | Loss: 0.00003694
Iteration 201/1000 | Loss: 0.00003694
Iteration 202/1000 | Loss: 0.00003694
Iteration 203/1000 | Loss: 0.00003694
Iteration 204/1000 | Loss: 0.00003694
Iteration 205/1000 | Loss: 0.00003694
Iteration 206/1000 | Loss: 0.00003694
Iteration 207/1000 | Loss: 0.00003694
Iteration 208/1000 | Loss: 0.00003694
Iteration 209/1000 | Loss: 0.00003694
Iteration 210/1000 | Loss: 0.00003694
Iteration 211/1000 | Loss: 0.00003694
Iteration 212/1000 | Loss: 0.00003693
Iteration 213/1000 | Loss: 0.00003693
Iteration 214/1000 | Loss: 0.00003693
Iteration 215/1000 | Loss: 0.00003693
Iteration 216/1000 | Loss: 0.00003693
Iteration 217/1000 | Loss: 0.00003693
Iteration 218/1000 | Loss: 0.00003693
Iteration 219/1000 | Loss: 0.00003693
Iteration 220/1000 | Loss: 0.00003693
Iteration 221/1000 | Loss: 0.00003693
Iteration 222/1000 | Loss: 0.00003693
Iteration 223/1000 | Loss: 0.00003693
Iteration 224/1000 | Loss: 0.00003693
Iteration 225/1000 | Loss: 0.00003693
Iteration 226/1000 | Loss: 0.00003693
Iteration 227/1000 | Loss: 0.00003693
Iteration 228/1000 | Loss: 0.00003692
Iteration 229/1000 | Loss: 0.00003692
Iteration 230/1000 | Loss: 0.00003692
Iteration 231/1000 | Loss: 0.00003692
Iteration 232/1000 | Loss: 0.00003692
Iteration 233/1000 | Loss: 0.00003692
Iteration 234/1000 | Loss: 0.00003692
Iteration 235/1000 | Loss: 0.00003691
Iteration 236/1000 | Loss: 0.00003691
Iteration 237/1000 | Loss: 0.00003691
Iteration 238/1000 | Loss: 0.00003691
Iteration 239/1000 | Loss: 0.00003691
Iteration 240/1000 | Loss: 0.00003691
Iteration 241/1000 | Loss: 0.00003691
Iteration 242/1000 | Loss: 0.00003691
Iteration 243/1000 | Loss: 0.00003691
Iteration 244/1000 | Loss: 0.00003691
Iteration 245/1000 | Loss: 0.00003691
Iteration 246/1000 | Loss: 0.00003690
Iteration 247/1000 | Loss: 0.00003690
Iteration 248/1000 | Loss: 0.00003690
Iteration 249/1000 | Loss: 0.00003690
Iteration 250/1000 | Loss: 0.00003689
Iteration 251/1000 | Loss: 0.00003689
Iteration 252/1000 | Loss: 0.00003689
Iteration 253/1000 | Loss: 0.00003689
Iteration 254/1000 | Loss: 0.00003689
Iteration 255/1000 | Loss: 0.00003688
Iteration 256/1000 | Loss: 0.00003688
Iteration 257/1000 | Loss: 0.00003688
Iteration 258/1000 | Loss: 0.00003688
Iteration 259/1000 | Loss: 0.00003688
Iteration 260/1000 | Loss: 0.00003688
Iteration 261/1000 | Loss: 0.00003688
Iteration 262/1000 | Loss: 0.00003688
Iteration 263/1000 | Loss: 0.00003688
Iteration 264/1000 | Loss: 0.00003688
Iteration 265/1000 | Loss: 0.00003688
Iteration 266/1000 | Loss: 0.00003688
Iteration 267/1000 | Loss: 0.00003688
Iteration 268/1000 | Loss: 0.00003687
Iteration 269/1000 | Loss: 0.00003687
Iteration 270/1000 | Loss: 0.00003687
Iteration 271/1000 | Loss: 0.00003687
Iteration 272/1000 | Loss: 0.00003687
Iteration 273/1000 | Loss: 0.00003687
Iteration 274/1000 | Loss: 0.00003687
Iteration 275/1000 | Loss: 0.00003687
Iteration 276/1000 | Loss: 0.00003687
Iteration 277/1000 | Loss: 0.00003687
Iteration 278/1000 | Loss: 0.00003686
Iteration 279/1000 | Loss: 0.00003686
Iteration 280/1000 | Loss: 0.00003686
Iteration 281/1000 | Loss: 0.00003686
Iteration 282/1000 | Loss: 0.00003686
Iteration 283/1000 | Loss: 0.00003686
Iteration 284/1000 | Loss: 0.00003686
Iteration 285/1000 | Loss: 0.00003686
Iteration 286/1000 | Loss: 0.00003686
Iteration 287/1000 | Loss: 0.00003686
Iteration 288/1000 | Loss: 0.00003686
Iteration 289/1000 | Loss: 0.00003686
Iteration 290/1000 | Loss: 0.00003686
Iteration 291/1000 | Loss: 0.00003685
Iteration 292/1000 | Loss: 0.00003685
Iteration 293/1000 | Loss: 0.00003685
Iteration 294/1000 | Loss: 0.00003685
Iteration 295/1000 | Loss: 0.00003685
Iteration 296/1000 | Loss: 0.00003685
Iteration 297/1000 | Loss: 0.00003684
Iteration 298/1000 | Loss: 0.00003684
Iteration 299/1000 | Loss: 0.00003684
Iteration 300/1000 | Loss: 0.00003684
Iteration 301/1000 | Loss: 0.00003684
Iteration 302/1000 | Loss: 0.00003684
Iteration 303/1000 | Loss: 0.00003684
Iteration 304/1000 | Loss: 0.00003684
Iteration 305/1000 | Loss: 0.00003684
Iteration 306/1000 | Loss: 0.00003683
Iteration 307/1000 | Loss: 0.00003683
Iteration 308/1000 | Loss: 0.00003683
Iteration 309/1000 | Loss: 0.00003683
Iteration 310/1000 | Loss: 0.00003683
Iteration 311/1000 | Loss: 0.00003683
Iteration 312/1000 | Loss: 0.00003683
Iteration 313/1000 | Loss: 0.00003683
Iteration 314/1000 | Loss: 0.00003683
Iteration 315/1000 | Loss: 0.00003683
Iteration 316/1000 | Loss: 0.00003683
Iteration 317/1000 | Loss: 0.00003683
Iteration 318/1000 | Loss: 0.00003683
Iteration 319/1000 | Loss: 0.00003683
Iteration 320/1000 | Loss: 0.00003683
Iteration 321/1000 | Loss: 0.00003683
Iteration 322/1000 | Loss: 0.00003683
Iteration 323/1000 | Loss: 0.00003683
Iteration 324/1000 | Loss: 0.00003683
Iteration 325/1000 | Loss: 0.00003683
Iteration 326/1000 | Loss: 0.00003683
Iteration 327/1000 | Loss: 0.00003683
Iteration 328/1000 | Loss: 0.00003683
Iteration 329/1000 | Loss: 0.00003683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [3.682648093672469e-05, 3.682648093672469e-05, 3.682648093672469e-05, 3.682648093672469e-05, 3.682648093672469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.682648093672469e-05

Optimization complete. Final v2v error: 5.211595058441162 mm

Highest mean error: 9.859426498413086 mm for frame 218

Lowest mean error: 4.919766426086426 mm for frame 0

Saving results

Total time: 294.8871145248413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052451
Iteration 2/25 | Loss: 0.00323790
Iteration 3/25 | Loss: 0.00207709
Iteration 4/25 | Loss: 0.00182204
Iteration 5/25 | Loss: 0.00164498
Iteration 6/25 | Loss: 0.00150625
Iteration 7/25 | Loss: 0.00131945
Iteration 8/25 | Loss: 0.00126311
Iteration 9/25 | Loss: 0.00122240
Iteration 10/25 | Loss: 0.00119838
Iteration 11/25 | Loss: 0.00118209
Iteration 12/25 | Loss: 0.00117645
Iteration 13/25 | Loss: 0.00117173
Iteration 14/25 | Loss: 0.00116552
Iteration 15/25 | Loss: 0.00116208
Iteration 16/25 | Loss: 0.00115879
Iteration 17/25 | Loss: 0.00115726
Iteration 18/25 | Loss: 0.00115978
Iteration 19/25 | Loss: 0.00115595
Iteration 20/25 | Loss: 0.00115523
Iteration 21/25 | Loss: 0.00115508
Iteration 22/25 | Loss: 0.00115500
Iteration 23/25 | Loss: 0.00115500
Iteration 24/25 | Loss: 0.00115500
Iteration 25/25 | Loss: 0.00115500

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31000304
Iteration 2/25 | Loss: 0.00087858
Iteration 3/25 | Loss: 0.00079733
Iteration 4/25 | Loss: 0.00079733
Iteration 5/25 | Loss: 0.00079733
Iteration 6/25 | Loss: 0.00079733
Iteration 7/25 | Loss: 0.00079733
Iteration 8/25 | Loss: 0.00079733
Iteration 9/25 | Loss: 0.00079733
Iteration 10/25 | Loss: 0.00079733
Iteration 11/25 | Loss: 0.00079733
Iteration 12/25 | Loss: 0.00079733
Iteration 13/25 | Loss: 0.00079733
Iteration 14/25 | Loss: 0.00079733
Iteration 15/25 | Loss: 0.00079733
Iteration 16/25 | Loss: 0.00079733
Iteration 17/25 | Loss: 0.00079733
Iteration 18/25 | Loss: 0.00079733
Iteration 19/25 | Loss: 0.00079733
Iteration 20/25 | Loss: 0.00079733
Iteration 21/25 | Loss: 0.00079733
Iteration 22/25 | Loss: 0.00079733
Iteration 23/25 | Loss: 0.00079733
Iteration 24/25 | Loss: 0.00079733
Iteration 25/25 | Loss: 0.00079733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079733
Iteration 2/1000 | Loss: 0.00039752
Iteration 3/1000 | Loss: 0.00016349
Iteration 4/1000 | Loss: 0.00006320
Iteration 5/1000 | Loss: 0.00005577
Iteration 6/1000 | Loss: 0.00020437
Iteration 7/1000 | Loss: 0.00005527
Iteration 8/1000 | Loss: 0.00004962
Iteration 9/1000 | Loss: 0.00004687
Iteration 10/1000 | Loss: 0.00004511
Iteration 11/1000 | Loss: 0.00004390
Iteration 12/1000 | Loss: 0.00004315
Iteration 13/1000 | Loss: 0.00004254
Iteration 14/1000 | Loss: 0.00004214
Iteration 15/1000 | Loss: 0.00006469
Iteration 16/1000 | Loss: 0.00034634
Iteration 17/1000 | Loss: 0.00007950
Iteration 18/1000 | Loss: 0.00004196
Iteration 19/1000 | Loss: 0.00004144
Iteration 20/1000 | Loss: 0.00005970
Iteration 21/1000 | Loss: 0.00004123
Iteration 22/1000 | Loss: 0.00004115
Iteration 23/1000 | Loss: 0.00004109
Iteration 24/1000 | Loss: 0.00004103
Iteration 25/1000 | Loss: 0.00004102
Iteration 26/1000 | Loss: 0.00004098
Iteration 27/1000 | Loss: 0.00004092
Iteration 28/1000 | Loss: 0.00004090
Iteration 29/1000 | Loss: 0.00004089
Iteration 30/1000 | Loss: 0.00004089
Iteration 31/1000 | Loss: 0.00004088
Iteration 32/1000 | Loss: 0.00004088
Iteration 33/1000 | Loss: 0.00004088
Iteration 34/1000 | Loss: 0.00004088
Iteration 35/1000 | Loss: 0.00004088
Iteration 36/1000 | Loss: 0.00004088
Iteration 37/1000 | Loss: 0.00004088
Iteration 38/1000 | Loss: 0.00004088
Iteration 39/1000 | Loss: 0.00004088
Iteration 40/1000 | Loss: 0.00004088
Iteration 41/1000 | Loss: 0.00004088
Iteration 42/1000 | Loss: 0.00004088
Iteration 43/1000 | Loss: 0.00004087
Iteration 44/1000 | Loss: 0.00004087
Iteration 45/1000 | Loss: 0.00004086
Iteration 46/1000 | Loss: 0.00004086
Iteration 47/1000 | Loss: 0.00004086
Iteration 48/1000 | Loss: 0.00004086
Iteration 49/1000 | Loss: 0.00004086
Iteration 50/1000 | Loss: 0.00004085
Iteration 51/1000 | Loss: 0.00004085
Iteration 52/1000 | Loss: 0.00004085
Iteration 53/1000 | Loss: 0.00004085
Iteration 54/1000 | Loss: 0.00004085
Iteration 55/1000 | Loss: 0.00004085
Iteration 56/1000 | Loss: 0.00004085
Iteration 57/1000 | Loss: 0.00004085
Iteration 58/1000 | Loss: 0.00004084
Iteration 59/1000 | Loss: 0.00004083
Iteration 60/1000 | Loss: 0.00004082
Iteration 61/1000 | Loss: 0.00004082
Iteration 62/1000 | Loss: 0.00004082
Iteration 63/1000 | Loss: 0.00004081
Iteration 64/1000 | Loss: 0.00004081
Iteration 65/1000 | Loss: 0.00004081
Iteration 66/1000 | Loss: 0.00004081
Iteration 67/1000 | Loss: 0.00004080
Iteration 68/1000 | Loss: 0.00004080
Iteration 69/1000 | Loss: 0.00004079
Iteration 70/1000 | Loss: 0.00004079
Iteration 71/1000 | Loss: 0.00004079
Iteration 72/1000 | Loss: 0.00004079
Iteration 73/1000 | Loss: 0.00004079
Iteration 74/1000 | Loss: 0.00004079
Iteration 75/1000 | Loss: 0.00004079
Iteration 76/1000 | Loss: 0.00004079
Iteration 77/1000 | Loss: 0.00004078
Iteration 78/1000 | Loss: 0.00004078
Iteration 79/1000 | Loss: 0.00004078
Iteration 80/1000 | Loss: 0.00004078
Iteration 81/1000 | Loss: 0.00004078
Iteration 82/1000 | Loss: 0.00004078
Iteration 83/1000 | Loss: 0.00004077
Iteration 84/1000 | Loss: 0.00004077
Iteration 85/1000 | Loss: 0.00004077
Iteration 86/1000 | Loss: 0.00004077
Iteration 87/1000 | Loss: 0.00004077
Iteration 88/1000 | Loss: 0.00004077
Iteration 89/1000 | Loss: 0.00004076
Iteration 90/1000 | Loss: 0.00004076
Iteration 91/1000 | Loss: 0.00004076
Iteration 92/1000 | Loss: 0.00004076
Iteration 93/1000 | Loss: 0.00004076
Iteration 94/1000 | Loss: 0.00004076
Iteration 95/1000 | Loss: 0.00004076
Iteration 96/1000 | Loss: 0.00004076
Iteration 97/1000 | Loss: 0.00004076
Iteration 98/1000 | Loss: 0.00004076
Iteration 99/1000 | Loss: 0.00004076
Iteration 100/1000 | Loss: 0.00004076
Iteration 101/1000 | Loss: 0.00004076
Iteration 102/1000 | Loss: 0.00004076
Iteration 103/1000 | Loss: 0.00004076
Iteration 104/1000 | Loss: 0.00004076
Iteration 105/1000 | Loss: 0.00004076
Iteration 106/1000 | Loss: 0.00004076
Iteration 107/1000 | Loss: 0.00004076
Iteration 108/1000 | Loss: 0.00004076
Iteration 109/1000 | Loss: 0.00004076
Iteration 110/1000 | Loss: 0.00004076
Iteration 111/1000 | Loss: 0.00004076
Iteration 112/1000 | Loss: 0.00004076
Iteration 113/1000 | Loss: 0.00004076
Iteration 114/1000 | Loss: 0.00004076
Iteration 115/1000 | Loss: 0.00004076
Iteration 116/1000 | Loss: 0.00004076
Iteration 117/1000 | Loss: 0.00004076
Iteration 118/1000 | Loss: 0.00004076
Iteration 119/1000 | Loss: 0.00004076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [4.075708784512244e-05, 4.075708784512244e-05, 4.075708784512244e-05, 4.075708784512244e-05, 4.075708784512244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.075708784512244e-05

Optimization complete. Final v2v error: 5.555378437042236 mm

Highest mean error: 6.344519138336182 mm for frame 43

Lowest mean error: 4.748932838439941 mm for frame 154

Saving results

Total time: 90.38987827301025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081731
Iteration 2/25 | Loss: 0.00344930
Iteration 3/25 | Loss: 0.00218064
Iteration 4/25 | Loss: 0.00185012
Iteration 5/25 | Loss: 0.00173955
Iteration 6/25 | Loss: 0.00171912
Iteration 7/25 | Loss: 0.00172761
Iteration 8/25 | Loss: 0.00160023
Iteration 9/25 | Loss: 0.00147317
Iteration 10/25 | Loss: 0.00141099
Iteration 11/25 | Loss: 0.00138093
Iteration 12/25 | Loss: 0.00135334
Iteration 13/25 | Loss: 0.00134594
Iteration 14/25 | Loss: 0.00133115
Iteration 15/25 | Loss: 0.00131073
Iteration 16/25 | Loss: 0.00130625
Iteration 17/25 | Loss: 0.00130685
Iteration 18/25 | Loss: 0.00129417
Iteration 19/25 | Loss: 0.00128778
Iteration 20/25 | Loss: 0.00128636
Iteration 21/25 | Loss: 0.00128260
Iteration 22/25 | Loss: 0.00128247
Iteration 23/25 | Loss: 0.00128177
Iteration 24/25 | Loss: 0.00128142
Iteration 25/25 | Loss: 0.00128099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30957568
Iteration 2/25 | Loss: 0.00585819
Iteration 3/25 | Loss: 0.00521139
Iteration 4/25 | Loss: 0.00521138
Iteration 5/25 | Loss: 0.00521138
Iteration 6/25 | Loss: 0.00521138
Iteration 7/25 | Loss: 0.00521138
Iteration 8/25 | Loss: 0.00521138
Iteration 9/25 | Loss: 0.00521138
Iteration 10/25 | Loss: 0.00521138
Iteration 11/25 | Loss: 0.00521138
Iteration 12/25 | Loss: 0.00521138
Iteration 13/25 | Loss: 0.00521138
Iteration 14/25 | Loss: 0.00521138
Iteration 15/25 | Loss: 0.00521138
Iteration 16/25 | Loss: 0.00521138
Iteration 17/25 | Loss: 0.00521138
Iteration 18/25 | Loss: 0.00521138
Iteration 19/25 | Loss: 0.00521138
Iteration 20/25 | Loss: 0.00521138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0052113826386630535, 0.0052113826386630535, 0.0052113826386630535, 0.0052113826386630535, 0.0052113826386630535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0052113826386630535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00521138
Iteration 2/1000 | Loss: 0.00277021
Iteration 3/1000 | Loss: 0.00281573
Iteration 4/1000 | Loss: 0.00259906
Iteration 5/1000 | Loss: 0.00458227
Iteration 6/1000 | Loss: 0.01554735
Iteration 7/1000 | Loss: 0.00481269
Iteration 8/1000 | Loss: 0.00223718
Iteration 9/1000 | Loss: 0.00077516
Iteration 10/1000 | Loss: 0.00138902
Iteration 11/1000 | Loss: 0.00084147
Iteration 12/1000 | Loss: 0.00055453
Iteration 13/1000 | Loss: 0.00187364
Iteration 14/1000 | Loss: 0.00154845
Iteration 15/1000 | Loss: 0.00201718
Iteration 16/1000 | Loss: 0.00104637
Iteration 17/1000 | Loss: 0.00088933
Iteration 18/1000 | Loss: 0.00032397
Iteration 19/1000 | Loss: 0.00018274
Iteration 20/1000 | Loss: 0.00039007
Iteration 21/1000 | Loss: 0.00060690
Iteration 22/1000 | Loss: 0.00027810
Iteration 23/1000 | Loss: 0.00020940
Iteration 24/1000 | Loss: 0.00027207
Iteration 25/1000 | Loss: 0.00024834
Iteration 26/1000 | Loss: 0.00008656
Iteration 27/1000 | Loss: 0.00040251
Iteration 28/1000 | Loss: 0.00066236
Iteration 29/1000 | Loss: 0.00016513
Iteration 30/1000 | Loss: 0.00020367
Iteration 31/1000 | Loss: 0.00093036
Iteration 32/1000 | Loss: 0.00073349
Iteration 33/1000 | Loss: 0.00022094
Iteration 34/1000 | Loss: 0.00009302
Iteration 35/1000 | Loss: 0.00035554
Iteration 36/1000 | Loss: 0.00017666
Iteration 37/1000 | Loss: 0.00006309
Iteration 38/1000 | Loss: 0.00006720
Iteration 39/1000 | Loss: 0.00004866
Iteration 40/1000 | Loss: 0.00005494
Iteration 41/1000 | Loss: 0.00005604
Iteration 42/1000 | Loss: 0.00005568
Iteration 43/1000 | Loss: 0.00004948
Iteration 44/1000 | Loss: 0.00004338
Iteration 45/1000 | Loss: 0.00006378
Iteration 46/1000 | Loss: 0.00004671
Iteration 47/1000 | Loss: 0.00003978
Iteration 48/1000 | Loss: 0.00004137
Iteration 49/1000 | Loss: 0.00004638
Iteration 50/1000 | Loss: 0.00025238
Iteration 51/1000 | Loss: 0.00021011
Iteration 52/1000 | Loss: 0.00020985
Iteration 53/1000 | Loss: 0.00008324
Iteration 54/1000 | Loss: 0.00069770
Iteration 55/1000 | Loss: 0.00019075
Iteration 56/1000 | Loss: 0.00005333
Iteration 57/1000 | Loss: 0.00009241
Iteration 58/1000 | Loss: 0.00005329
Iteration 59/1000 | Loss: 0.00014246
Iteration 60/1000 | Loss: 0.00005454
Iteration 61/1000 | Loss: 0.00008070
Iteration 62/1000 | Loss: 0.00003651
Iteration 63/1000 | Loss: 0.00024189
Iteration 64/1000 | Loss: 0.00012341
Iteration 65/1000 | Loss: 0.00011983
Iteration 66/1000 | Loss: 0.00004172
Iteration 67/1000 | Loss: 0.00004627
Iteration 68/1000 | Loss: 0.00004480
Iteration 69/1000 | Loss: 0.00004180
Iteration 70/1000 | Loss: 0.00003619
Iteration 71/1000 | Loss: 0.00004893
Iteration 72/1000 | Loss: 0.00003525
Iteration 73/1000 | Loss: 0.00002962
Iteration 74/1000 | Loss: 0.00002807
Iteration 75/1000 | Loss: 0.00002706
Iteration 76/1000 | Loss: 0.00002622
Iteration 77/1000 | Loss: 0.00002542
Iteration 78/1000 | Loss: 0.00002488
Iteration 79/1000 | Loss: 0.00002458
Iteration 80/1000 | Loss: 0.00002431
Iteration 81/1000 | Loss: 0.00002417
Iteration 82/1000 | Loss: 0.00002403
Iteration 83/1000 | Loss: 0.00002401
Iteration 84/1000 | Loss: 0.00002398
Iteration 85/1000 | Loss: 0.00002397
Iteration 86/1000 | Loss: 0.00002396
Iteration 87/1000 | Loss: 0.00002396
Iteration 88/1000 | Loss: 0.00002395
Iteration 89/1000 | Loss: 0.00002395
Iteration 90/1000 | Loss: 0.00002393
Iteration 91/1000 | Loss: 0.00002393
Iteration 92/1000 | Loss: 0.00002392
Iteration 93/1000 | Loss: 0.00002392
Iteration 94/1000 | Loss: 0.00002391
Iteration 95/1000 | Loss: 0.00002389
Iteration 96/1000 | Loss: 0.00002389
Iteration 97/1000 | Loss: 0.00002389
Iteration 98/1000 | Loss: 0.00002388
Iteration 99/1000 | Loss: 0.00002388
Iteration 100/1000 | Loss: 0.00002387
Iteration 101/1000 | Loss: 0.00002386
Iteration 102/1000 | Loss: 0.00002385
Iteration 103/1000 | Loss: 0.00002385
Iteration 104/1000 | Loss: 0.00002385
Iteration 105/1000 | Loss: 0.00002385
Iteration 106/1000 | Loss: 0.00002385
Iteration 107/1000 | Loss: 0.00002384
Iteration 108/1000 | Loss: 0.00002384
Iteration 109/1000 | Loss: 0.00002384
Iteration 110/1000 | Loss: 0.00002383
Iteration 111/1000 | Loss: 0.00002383
Iteration 112/1000 | Loss: 0.00002383
Iteration 113/1000 | Loss: 0.00002382
Iteration 114/1000 | Loss: 0.00002382
Iteration 115/1000 | Loss: 0.00002382
Iteration 116/1000 | Loss: 0.00002382
Iteration 117/1000 | Loss: 0.00002382
Iteration 118/1000 | Loss: 0.00002382
Iteration 119/1000 | Loss: 0.00002382
Iteration 120/1000 | Loss: 0.00002382
Iteration 121/1000 | Loss: 0.00002382
Iteration 122/1000 | Loss: 0.00002382
Iteration 123/1000 | Loss: 0.00002382
Iteration 124/1000 | Loss: 0.00002382
Iteration 125/1000 | Loss: 0.00002381
Iteration 126/1000 | Loss: 0.00002381
Iteration 127/1000 | Loss: 0.00002381
Iteration 128/1000 | Loss: 0.00002381
Iteration 129/1000 | Loss: 0.00002380
Iteration 130/1000 | Loss: 0.00002378
Iteration 131/1000 | Loss: 0.00002378
Iteration 132/1000 | Loss: 0.00002377
Iteration 133/1000 | Loss: 0.00002377
Iteration 134/1000 | Loss: 0.00002376
Iteration 135/1000 | Loss: 0.00002376
Iteration 136/1000 | Loss: 0.00002376
Iteration 137/1000 | Loss: 0.00002375
Iteration 138/1000 | Loss: 0.00002375
Iteration 139/1000 | Loss: 0.00002375
Iteration 140/1000 | Loss: 0.00002374
Iteration 141/1000 | Loss: 0.00002374
Iteration 142/1000 | Loss: 0.00002374
Iteration 143/1000 | Loss: 0.00002374
Iteration 144/1000 | Loss: 0.00002374
Iteration 145/1000 | Loss: 0.00002374
Iteration 146/1000 | Loss: 0.00002374
Iteration 147/1000 | Loss: 0.00002374
Iteration 148/1000 | Loss: 0.00002373
Iteration 149/1000 | Loss: 0.00002373
Iteration 150/1000 | Loss: 0.00002373
Iteration 151/1000 | Loss: 0.00002372
Iteration 152/1000 | Loss: 0.00002372
Iteration 153/1000 | Loss: 0.00002372
Iteration 154/1000 | Loss: 0.00002372
Iteration 155/1000 | Loss: 0.00002372
Iteration 156/1000 | Loss: 0.00002372
Iteration 157/1000 | Loss: 0.00002372
Iteration 158/1000 | Loss: 0.00002372
Iteration 159/1000 | Loss: 0.00002372
Iteration 160/1000 | Loss: 0.00002371
Iteration 161/1000 | Loss: 0.00002371
Iteration 162/1000 | Loss: 0.00002371
Iteration 163/1000 | Loss: 0.00002371
Iteration 164/1000 | Loss: 0.00002371
Iteration 165/1000 | Loss: 0.00002371
Iteration 166/1000 | Loss: 0.00002371
Iteration 167/1000 | Loss: 0.00002371
Iteration 168/1000 | Loss: 0.00002371
Iteration 169/1000 | Loss: 0.00002371
Iteration 170/1000 | Loss: 0.00002371
Iteration 171/1000 | Loss: 0.00002370
Iteration 172/1000 | Loss: 0.00002370
Iteration 173/1000 | Loss: 0.00002370
Iteration 174/1000 | Loss: 0.00002370
Iteration 175/1000 | Loss: 0.00002370
Iteration 176/1000 | Loss: 0.00002370
Iteration 177/1000 | Loss: 0.00002370
Iteration 178/1000 | Loss: 0.00002370
Iteration 179/1000 | Loss: 0.00002370
Iteration 180/1000 | Loss: 0.00002370
Iteration 181/1000 | Loss: 0.00002370
Iteration 182/1000 | Loss: 0.00002370
Iteration 183/1000 | Loss: 0.00002370
Iteration 184/1000 | Loss: 0.00002370
Iteration 185/1000 | Loss: 0.00002370
Iteration 186/1000 | Loss: 0.00002370
Iteration 187/1000 | Loss: 0.00002370
Iteration 188/1000 | Loss: 0.00002370
Iteration 189/1000 | Loss: 0.00002370
Iteration 190/1000 | Loss: 0.00002370
Iteration 191/1000 | Loss: 0.00002370
Iteration 192/1000 | Loss: 0.00002370
Iteration 193/1000 | Loss: 0.00002370
Iteration 194/1000 | Loss: 0.00002370
Iteration 195/1000 | Loss: 0.00002370
Iteration 196/1000 | Loss: 0.00002370
Iteration 197/1000 | Loss: 0.00002370
Iteration 198/1000 | Loss: 0.00002370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.3699758457951248e-05, 2.3699758457951248e-05, 2.3699758457951248e-05, 2.3699758457951248e-05, 2.3699758457951248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3699758457951248e-05

Optimization complete. Final v2v error: 4.31231689453125 mm

Highest mean error: 4.971729755401611 mm for frame 17

Lowest mean error: 3.8156323432922363 mm for frame 180

Saving results

Total time: 194.85697221755981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995487
Iteration 2/25 | Loss: 0.00159553
Iteration 3/25 | Loss: 0.00117701
Iteration 4/25 | Loss: 0.00113837
Iteration 5/25 | Loss: 0.00112398
Iteration 6/25 | Loss: 0.00112178
Iteration 7/25 | Loss: 0.00112144
Iteration 8/25 | Loss: 0.00112144
Iteration 9/25 | Loss: 0.00112144
Iteration 10/25 | Loss: 0.00112144
Iteration 11/25 | Loss: 0.00112144
Iteration 12/25 | Loss: 0.00112144
Iteration 13/25 | Loss: 0.00112144
Iteration 14/25 | Loss: 0.00112144
Iteration 15/25 | Loss: 0.00112144
Iteration 16/25 | Loss: 0.00112144
Iteration 17/25 | Loss: 0.00112144
Iteration 18/25 | Loss: 0.00112144
Iteration 19/25 | Loss: 0.00112144
Iteration 20/25 | Loss: 0.00112144
Iteration 21/25 | Loss: 0.00112144
Iteration 22/25 | Loss: 0.00112144
Iteration 23/25 | Loss: 0.00112144
Iteration 24/25 | Loss: 0.00112144
Iteration 25/25 | Loss: 0.00112144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83101612
Iteration 2/25 | Loss: 0.00087026
Iteration 3/25 | Loss: 0.00087025
Iteration 4/25 | Loss: 0.00087025
Iteration 5/25 | Loss: 0.00087025
Iteration 6/25 | Loss: 0.00087025
Iteration 7/25 | Loss: 0.00087025
Iteration 8/25 | Loss: 0.00087025
Iteration 9/25 | Loss: 0.00087025
Iteration 10/25 | Loss: 0.00087025
Iteration 11/25 | Loss: 0.00087025
Iteration 12/25 | Loss: 0.00087025
Iteration 13/25 | Loss: 0.00087025
Iteration 14/25 | Loss: 0.00087025
Iteration 15/25 | Loss: 0.00087025
Iteration 16/25 | Loss: 0.00087025
Iteration 17/25 | Loss: 0.00087025
Iteration 18/25 | Loss: 0.00087025
Iteration 19/25 | Loss: 0.00087025
Iteration 20/25 | Loss: 0.00087025
Iteration 21/25 | Loss: 0.00087025
Iteration 22/25 | Loss: 0.00087025
Iteration 23/25 | Loss: 0.00087025
Iteration 24/25 | Loss: 0.00087025
Iteration 25/25 | Loss: 0.00087025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008702478953637183, 0.0008702478953637183, 0.0008702478953637183, 0.0008702478953637183, 0.0008702478953637183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008702478953637183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087025
Iteration 2/1000 | Loss: 0.00013215
Iteration 3/1000 | Loss: 0.00008508
Iteration 4/1000 | Loss: 0.00006446
Iteration 5/1000 | Loss: 0.00005752
Iteration 6/1000 | Loss: 0.00005422
Iteration 7/1000 | Loss: 0.00005162
Iteration 8/1000 | Loss: 0.00005035
Iteration 9/1000 | Loss: 0.00004934
Iteration 10/1000 | Loss: 0.00004870
Iteration 11/1000 | Loss: 0.00004828
Iteration 12/1000 | Loss: 0.00004788
Iteration 13/1000 | Loss: 0.00004756
Iteration 14/1000 | Loss: 0.00004733
Iteration 15/1000 | Loss: 0.00004710
Iteration 16/1000 | Loss: 0.00004698
Iteration 17/1000 | Loss: 0.00004697
Iteration 18/1000 | Loss: 0.00004693
Iteration 19/1000 | Loss: 0.00004687
Iteration 20/1000 | Loss: 0.00004683
Iteration 21/1000 | Loss: 0.00004682
Iteration 22/1000 | Loss: 0.00004679
Iteration 23/1000 | Loss: 0.00004672
Iteration 24/1000 | Loss: 0.00004665
Iteration 25/1000 | Loss: 0.00004664
Iteration 26/1000 | Loss: 0.00004663
Iteration 27/1000 | Loss: 0.00004663
Iteration 28/1000 | Loss: 0.00004662
Iteration 29/1000 | Loss: 0.00004661
Iteration 30/1000 | Loss: 0.00004661
Iteration 31/1000 | Loss: 0.00004660
Iteration 32/1000 | Loss: 0.00004660
Iteration 33/1000 | Loss: 0.00004660
Iteration 34/1000 | Loss: 0.00004659
Iteration 35/1000 | Loss: 0.00004656
Iteration 36/1000 | Loss: 0.00004656
Iteration 37/1000 | Loss: 0.00004656
Iteration 38/1000 | Loss: 0.00004656
Iteration 39/1000 | Loss: 0.00004655
Iteration 40/1000 | Loss: 0.00004655
Iteration 41/1000 | Loss: 0.00004654
Iteration 42/1000 | Loss: 0.00004653
Iteration 43/1000 | Loss: 0.00004652
Iteration 44/1000 | Loss: 0.00004651
Iteration 45/1000 | Loss: 0.00004651
Iteration 46/1000 | Loss: 0.00004651
Iteration 47/1000 | Loss: 0.00004651
Iteration 48/1000 | Loss: 0.00004651
Iteration 49/1000 | Loss: 0.00004651
Iteration 50/1000 | Loss: 0.00004651
Iteration 51/1000 | Loss: 0.00004651
Iteration 52/1000 | Loss: 0.00004648
Iteration 53/1000 | Loss: 0.00004648
Iteration 54/1000 | Loss: 0.00004648
Iteration 55/1000 | Loss: 0.00004648
Iteration 56/1000 | Loss: 0.00004648
Iteration 57/1000 | Loss: 0.00004648
Iteration 58/1000 | Loss: 0.00004648
Iteration 59/1000 | Loss: 0.00004647
Iteration 60/1000 | Loss: 0.00004647
Iteration 61/1000 | Loss: 0.00004647
Iteration 62/1000 | Loss: 0.00004647
Iteration 63/1000 | Loss: 0.00004647
Iteration 64/1000 | Loss: 0.00004647
Iteration 65/1000 | Loss: 0.00004647
Iteration 66/1000 | Loss: 0.00004647
Iteration 67/1000 | Loss: 0.00004647
Iteration 68/1000 | Loss: 0.00004647
Iteration 69/1000 | Loss: 0.00004647
Iteration 70/1000 | Loss: 0.00004646
Iteration 71/1000 | Loss: 0.00004646
Iteration 72/1000 | Loss: 0.00004646
Iteration 73/1000 | Loss: 0.00004645
Iteration 74/1000 | Loss: 0.00004645
Iteration 75/1000 | Loss: 0.00004645
Iteration 76/1000 | Loss: 0.00004644
Iteration 77/1000 | Loss: 0.00004644
Iteration 78/1000 | Loss: 0.00004644
Iteration 79/1000 | Loss: 0.00004643
Iteration 80/1000 | Loss: 0.00004643
Iteration 81/1000 | Loss: 0.00004643
Iteration 82/1000 | Loss: 0.00004643
Iteration 83/1000 | Loss: 0.00004643
Iteration 84/1000 | Loss: 0.00004642
Iteration 85/1000 | Loss: 0.00004642
Iteration 86/1000 | Loss: 0.00004642
Iteration 87/1000 | Loss: 0.00004641
Iteration 88/1000 | Loss: 0.00004641
Iteration 89/1000 | Loss: 0.00004640
Iteration 90/1000 | Loss: 0.00004640
Iteration 91/1000 | Loss: 0.00004640
Iteration 92/1000 | Loss: 0.00004639
Iteration 93/1000 | Loss: 0.00004639
Iteration 94/1000 | Loss: 0.00004639
Iteration 95/1000 | Loss: 0.00004639
Iteration 96/1000 | Loss: 0.00004639
Iteration 97/1000 | Loss: 0.00004639
Iteration 98/1000 | Loss: 0.00004639
Iteration 99/1000 | Loss: 0.00004639
Iteration 100/1000 | Loss: 0.00004639
Iteration 101/1000 | Loss: 0.00004639
Iteration 102/1000 | Loss: 0.00004638
Iteration 103/1000 | Loss: 0.00004638
Iteration 104/1000 | Loss: 0.00004637
Iteration 105/1000 | Loss: 0.00004637
Iteration 106/1000 | Loss: 0.00004637
Iteration 107/1000 | Loss: 0.00004637
Iteration 108/1000 | Loss: 0.00004636
Iteration 109/1000 | Loss: 0.00004636
Iteration 110/1000 | Loss: 0.00004636
Iteration 111/1000 | Loss: 0.00004636
Iteration 112/1000 | Loss: 0.00004636
Iteration 113/1000 | Loss: 0.00004636
Iteration 114/1000 | Loss: 0.00004636
Iteration 115/1000 | Loss: 0.00004636
Iteration 116/1000 | Loss: 0.00004636
Iteration 117/1000 | Loss: 0.00004636
Iteration 118/1000 | Loss: 0.00004636
Iteration 119/1000 | Loss: 0.00004636
Iteration 120/1000 | Loss: 0.00004635
Iteration 121/1000 | Loss: 0.00004635
Iteration 122/1000 | Loss: 0.00004635
Iteration 123/1000 | Loss: 0.00004635
Iteration 124/1000 | Loss: 0.00004635
Iteration 125/1000 | Loss: 0.00004634
Iteration 126/1000 | Loss: 0.00004634
Iteration 127/1000 | Loss: 0.00004634
Iteration 128/1000 | Loss: 0.00004634
Iteration 129/1000 | Loss: 0.00004634
Iteration 130/1000 | Loss: 0.00004633
Iteration 131/1000 | Loss: 0.00004633
Iteration 132/1000 | Loss: 0.00004633
Iteration 133/1000 | Loss: 0.00004633
Iteration 134/1000 | Loss: 0.00004633
Iteration 135/1000 | Loss: 0.00004633
Iteration 136/1000 | Loss: 0.00004633
Iteration 137/1000 | Loss: 0.00004633
Iteration 138/1000 | Loss: 0.00004633
Iteration 139/1000 | Loss: 0.00004632
Iteration 140/1000 | Loss: 0.00004632
Iteration 141/1000 | Loss: 0.00004632
Iteration 142/1000 | Loss: 0.00004632
Iteration 143/1000 | Loss: 0.00004632
Iteration 144/1000 | Loss: 0.00004632
Iteration 145/1000 | Loss: 0.00004632
Iteration 146/1000 | Loss: 0.00004632
Iteration 147/1000 | Loss: 0.00004632
Iteration 148/1000 | Loss: 0.00004631
Iteration 149/1000 | Loss: 0.00004631
Iteration 150/1000 | Loss: 0.00004631
Iteration 151/1000 | Loss: 0.00004631
Iteration 152/1000 | Loss: 0.00004631
Iteration 153/1000 | Loss: 0.00004631
Iteration 154/1000 | Loss: 0.00004630
Iteration 155/1000 | Loss: 0.00004630
Iteration 156/1000 | Loss: 0.00004630
Iteration 157/1000 | Loss: 0.00004630
Iteration 158/1000 | Loss: 0.00004629
Iteration 159/1000 | Loss: 0.00004629
Iteration 160/1000 | Loss: 0.00004629
Iteration 161/1000 | Loss: 0.00004629
Iteration 162/1000 | Loss: 0.00004628
Iteration 163/1000 | Loss: 0.00004628
Iteration 164/1000 | Loss: 0.00004628
Iteration 165/1000 | Loss: 0.00004628
Iteration 166/1000 | Loss: 0.00004628
Iteration 167/1000 | Loss: 0.00004628
Iteration 168/1000 | Loss: 0.00004628
Iteration 169/1000 | Loss: 0.00004628
Iteration 170/1000 | Loss: 0.00004627
Iteration 171/1000 | Loss: 0.00004627
Iteration 172/1000 | Loss: 0.00004627
Iteration 173/1000 | Loss: 0.00004627
Iteration 174/1000 | Loss: 0.00004627
Iteration 175/1000 | Loss: 0.00004627
Iteration 176/1000 | Loss: 0.00004627
Iteration 177/1000 | Loss: 0.00004626
Iteration 178/1000 | Loss: 0.00004626
Iteration 179/1000 | Loss: 0.00004626
Iteration 180/1000 | Loss: 0.00004626
Iteration 181/1000 | Loss: 0.00004626
Iteration 182/1000 | Loss: 0.00004626
Iteration 183/1000 | Loss: 0.00004626
Iteration 184/1000 | Loss: 0.00004625
Iteration 185/1000 | Loss: 0.00004625
Iteration 186/1000 | Loss: 0.00004625
Iteration 187/1000 | Loss: 0.00004625
Iteration 188/1000 | Loss: 0.00004625
Iteration 189/1000 | Loss: 0.00004624
Iteration 190/1000 | Loss: 0.00004624
Iteration 191/1000 | Loss: 0.00004624
Iteration 192/1000 | Loss: 0.00004624
Iteration 193/1000 | Loss: 0.00004623
Iteration 194/1000 | Loss: 0.00004623
Iteration 195/1000 | Loss: 0.00004623
Iteration 196/1000 | Loss: 0.00004623
Iteration 197/1000 | Loss: 0.00004623
Iteration 198/1000 | Loss: 0.00004622
Iteration 199/1000 | Loss: 0.00004622
Iteration 200/1000 | Loss: 0.00004622
Iteration 201/1000 | Loss: 0.00004622
Iteration 202/1000 | Loss: 0.00004622
Iteration 203/1000 | Loss: 0.00004622
Iteration 204/1000 | Loss: 0.00004622
Iteration 205/1000 | Loss: 0.00004621
Iteration 206/1000 | Loss: 0.00004621
Iteration 207/1000 | Loss: 0.00004621
Iteration 208/1000 | Loss: 0.00004621
Iteration 209/1000 | Loss: 0.00004620
Iteration 210/1000 | Loss: 0.00004620
Iteration 211/1000 | Loss: 0.00004620
Iteration 212/1000 | Loss: 0.00004620
Iteration 213/1000 | Loss: 0.00004620
Iteration 214/1000 | Loss: 0.00004620
Iteration 215/1000 | Loss: 0.00004620
Iteration 216/1000 | Loss: 0.00004619
Iteration 217/1000 | Loss: 0.00004619
Iteration 218/1000 | Loss: 0.00004619
Iteration 219/1000 | Loss: 0.00004619
Iteration 220/1000 | Loss: 0.00004619
Iteration 221/1000 | Loss: 0.00004619
Iteration 222/1000 | Loss: 0.00004619
Iteration 223/1000 | Loss: 0.00004619
Iteration 224/1000 | Loss: 0.00004618
Iteration 225/1000 | Loss: 0.00004618
Iteration 226/1000 | Loss: 0.00004618
Iteration 227/1000 | Loss: 0.00004618
Iteration 228/1000 | Loss: 0.00004618
Iteration 229/1000 | Loss: 0.00004618
Iteration 230/1000 | Loss: 0.00004618
Iteration 231/1000 | Loss: 0.00004618
Iteration 232/1000 | Loss: 0.00004617
Iteration 233/1000 | Loss: 0.00004617
Iteration 234/1000 | Loss: 0.00004617
Iteration 235/1000 | Loss: 0.00004617
Iteration 236/1000 | Loss: 0.00004617
Iteration 237/1000 | Loss: 0.00004617
Iteration 238/1000 | Loss: 0.00004617
Iteration 239/1000 | Loss: 0.00004617
Iteration 240/1000 | Loss: 0.00004617
Iteration 241/1000 | Loss: 0.00004617
Iteration 242/1000 | Loss: 0.00004617
Iteration 243/1000 | Loss: 0.00004617
Iteration 244/1000 | Loss: 0.00004616
Iteration 245/1000 | Loss: 0.00004616
Iteration 246/1000 | Loss: 0.00004616
Iteration 247/1000 | Loss: 0.00004616
Iteration 248/1000 | Loss: 0.00004616
Iteration 249/1000 | Loss: 0.00004616
Iteration 250/1000 | Loss: 0.00004616
Iteration 251/1000 | Loss: 0.00004615
Iteration 252/1000 | Loss: 0.00004615
Iteration 253/1000 | Loss: 0.00004615
Iteration 254/1000 | Loss: 0.00004615
Iteration 255/1000 | Loss: 0.00004615
Iteration 256/1000 | Loss: 0.00004614
Iteration 257/1000 | Loss: 0.00004614
Iteration 258/1000 | Loss: 0.00004614
Iteration 259/1000 | Loss: 0.00004614
Iteration 260/1000 | Loss: 0.00004614
Iteration 261/1000 | Loss: 0.00004614
Iteration 262/1000 | Loss: 0.00004613
Iteration 263/1000 | Loss: 0.00004613
Iteration 264/1000 | Loss: 0.00004613
Iteration 265/1000 | Loss: 0.00004613
Iteration 266/1000 | Loss: 0.00004613
Iteration 267/1000 | Loss: 0.00004612
Iteration 268/1000 | Loss: 0.00004612
Iteration 269/1000 | Loss: 0.00004612
Iteration 270/1000 | Loss: 0.00004612
Iteration 271/1000 | Loss: 0.00004612
Iteration 272/1000 | Loss: 0.00004612
Iteration 273/1000 | Loss: 0.00004612
Iteration 274/1000 | Loss: 0.00004612
Iteration 275/1000 | Loss: 0.00004611
Iteration 276/1000 | Loss: 0.00004611
Iteration 277/1000 | Loss: 0.00004611
Iteration 278/1000 | Loss: 0.00004611
Iteration 279/1000 | Loss: 0.00004611
Iteration 280/1000 | Loss: 0.00004611
Iteration 281/1000 | Loss: 0.00004611
Iteration 282/1000 | Loss: 0.00004611
Iteration 283/1000 | Loss: 0.00004611
Iteration 284/1000 | Loss: 0.00004611
Iteration 285/1000 | Loss: 0.00004611
Iteration 286/1000 | Loss: 0.00004611
Iteration 287/1000 | Loss: 0.00004611
Iteration 288/1000 | Loss: 0.00004611
Iteration 289/1000 | Loss: 0.00004611
Iteration 290/1000 | Loss: 0.00004610
Iteration 291/1000 | Loss: 0.00004610
Iteration 292/1000 | Loss: 0.00004610
Iteration 293/1000 | Loss: 0.00004610
Iteration 294/1000 | Loss: 0.00004610
Iteration 295/1000 | Loss: 0.00004610
Iteration 296/1000 | Loss: 0.00004610
Iteration 297/1000 | Loss: 0.00004610
Iteration 298/1000 | Loss: 0.00004610
Iteration 299/1000 | Loss: 0.00004610
Iteration 300/1000 | Loss: 0.00004610
Iteration 301/1000 | Loss: 0.00004610
Iteration 302/1000 | Loss: 0.00004610
Iteration 303/1000 | Loss: 0.00004610
Iteration 304/1000 | Loss: 0.00004610
Iteration 305/1000 | Loss: 0.00004610
Iteration 306/1000 | Loss: 0.00004609
Iteration 307/1000 | Loss: 0.00004609
Iteration 308/1000 | Loss: 0.00004609
Iteration 309/1000 | Loss: 0.00004609
Iteration 310/1000 | Loss: 0.00004609
Iteration 311/1000 | Loss: 0.00004609
Iteration 312/1000 | Loss: 0.00004609
Iteration 313/1000 | Loss: 0.00004609
Iteration 314/1000 | Loss: 0.00004609
Iteration 315/1000 | Loss: 0.00004609
Iteration 316/1000 | Loss: 0.00004609
Iteration 317/1000 | Loss: 0.00004609
Iteration 318/1000 | Loss: 0.00004609
Iteration 319/1000 | Loss: 0.00004609
Iteration 320/1000 | Loss: 0.00004608
Iteration 321/1000 | Loss: 0.00004608
Iteration 322/1000 | Loss: 0.00004608
Iteration 323/1000 | Loss: 0.00004608
Iteration 324/1000 | Loss: 0.00004608
Iteration 325/1000 | Loss: 0.00004608
Iteration 326/1000 | Loss: 0.00004608
Iteration 327/1000 | Loss: 0.00004608
Iteration 328/1000 | Loss: 0.00004608
Iteration 329/1000 | Loss: 0.00004608
Iteration 330/1000 | Loss: 0.00004608
Iteration 331/1000 | Loss: 0.00004608
Iteration 332/1000 | Loss: 0.00004608
Iteration 333/1000 | Loss: 0.00004608
Iteration 334/1000 | Loss: 0.00004608
Iteration 335/1000 | Loss: 0.00004608
Iteration 336/1000 | Loss: 0.00004608
Iteration 337/1000 | Loss: 0.00004608
Iteration 338/1000 | Loss: 0.00004608
Iteration 339/1000 | Loss: 0.00004608
Iteration 340/1000 | Loss: 0.00004607
Iteration 341/1000 | Loss: 0.00004607
Iteration 342/1000 | Loss: 0.00004607
Iteration 343/1000 | Loss: 0.00004607
Iteration 344/1000 | Loss: 0.00004607
Iteration 345/1000 | Loss: 0.00004607
Iteration 346/1000 | Loss: 0.00004607
Iteration 347/1000 | Loss: 0.00004607
Iteration 348/1000 | Loss: 0.00004607
Iteration 349/1000 | Loss: 0.00004607
Iteration 350/1000 | Loss: 0.00004607
Iteration 351/1000 | Loss: 0.00004607
Iteration 352/1000 | Loss: 0.00004607
Iteration 353/1000 | Loss: 0.00004607
Iteration 354/1000 | Loss: 0.00004607
Iteration 355/1000 | Loss: 0.00004607
Iteration 356/1000 | Loss: 0.00004607
Iteration 357/1000 | Loss: 0.00004607
Iteration 358/1000 | Loss: 0.00004607
Iteration 359/1000 | Loss: 0.00004607
Iteration 360/1000 | Loss: 0.00004606
Iteration 361/1000 | Loss: 0.00004606
Iteration 362/1000 | Loss: 0.00004606
Iteration 363/1000 | Loss: 0.00004606
Iteration 364/1000 | Loss: 0.00004606
Iteration 365/1000 | Loss: 0.00004606
Iteration 366/1000 | Loss: 0.00004606
Iteration 367/1000 | Loss: 0.00004606
Iteration 368/1000 | Loss: 0.00004606
Iteration 369/1000 | Loss: 0.00004606
Iteration 370/1000 | Loss: 0.00004606
Iteration 371/1000 | Loss: 0.00004606
Iteration 372/1000 | Loss: 0.00004606
Iteration 373/1000 | Loss: 0.00004606
Iteration 374/1000 | Loss: 0.00004606
Iteration 375/1000 | Loss: 0.00004606
Iteration 376/1000 | Loss: 0.00004606
Iteration 377/1000 | Loss: 0.00004606
Iteration 378/1000 | Loss: 0.00004606
Iteration 379/1000 | Loss: 0.00004606
Iteration 380/1000 | Loss: 0.00004606
Iteration 381/1000 | Loss: 0.00004606
Iteration 382/1000 | Loss: 0.00004606
Iteration 383/1000 | Loss: 0.00004606
Iteration 384/1000 | Loss: 0.00004606
Iteration 385/1000 | Loss: 0.00004606
Iteration 386/1000 | Loss: 0.00004606
Iteration 387/1000 | Loss: 0.00004606
Iteration 388/1000 | Loss: 0.00004606
Iteration 389/1000 | Loss: 0.00004606
Iteration 390/1000 | Loss: 0.00004606
Iteration 391/1000 | Loss: 0.00004606
Iteration 392/1000 | Loss: 0.00004606
Iteration 393/1000 | Loss: 0.00004606
Iteration 394/1000 | Loss: 0.00004606
Iteration 395/1000 | Loss: 0.00004606
Iteration 396/1000 | Loss: 0.00004606
Iteration 397/1000 | Loss: 0.00004606
Iteration 398/1000 | Loss: 0.00004606
Iteration 399/1000 | Loss: 0.00004606
Iteration 400/1000 | Loss: 0.00004606
Iteration 401/1000 | Loss: 0.00004606
Iteration 402/1000 | Loss: 0.00004606
Iteration 403/1000 | Loss: 0.00004606
Iteration 404/1000 | Loss: 0.00004606
Iteration 405/1000 | Loss: 0.00004606
Iteration 406/1000 | Loss: 0.00004606
Iteration 407/1000 | Loss: 0.00004606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 407. Stopping optimization.
Last 5 losses: [4.6061028115218505e-05, 4.6061028115218505e-05, 4.6061028115218505e-05, 4.6061028115218505e-05, 4.6061028115218505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6061028115218505e-05

Optimization complete. Final v2v error: 5.793825626373291 mm

Highest mean error: 6.6254191398620605 mm for frame 130

Lowest mean error: 5.149342060089111 mm for frame 38

Saving results

Total time: 63.02429461479187
