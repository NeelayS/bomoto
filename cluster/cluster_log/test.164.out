Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=164, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9184-9239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890129
Iteration 2/25 | Loss: 0.00284295
Iteration 3/25 | Loss: 0.00198178
Iteration 4/25 | Loss: 0.00197192
Iteration 5/25 | Loss: 0.00132699
Iteration 6/25 | Loss: 0.00125417
Iteration 7/25 | Loss: 0.00122089
Iteration 8/25 | Loss: 0.00118050
Iteration 9/25 | Loss: 0.00116795
Iteration 10/25 | Loss: 0.00116204
Iteration 11/25 | Loss: 0.00115101
Iteration 12/25 | Loss: 0.00114648
Iteration 13/25 | Loss: 0.00114439
Iteration 14/25 | Loss: 0.00114508
Iteration 15/25 | Loss: 0.00114223
Iteration 16/25 | Loss: 0.00114197
Iteration 17/25 | Loss: 0.00114260
Iteration 18/25 | Loss: 0.00114040
Iteration 19/25 | Loss: 0.00114020
Iteration 20/25 | Loss: 0.00114142
Iteration 21/25 | Loss: 0.00113876
Iteration 22/25 | Loss: 0.00113861
Iteration 23/25 | Loss: 0.00113853
Iteration 24/25 | Loss: 0.00113850
Iteration 25/25 | Loss: 0.00113849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34312725
Iteration 2/25 | Loss: 0.00041500
Iteration 3/25 | Loss: 0.00041493
Iteration 4/25 | Loss: 0.00041493
Iteration 5/25 | Loss: 0.00041493
Iteration 6/25 | Loss: 0.00041493
Iteration 7/25 | Loss: 0.00041493
Iteration 8/25 | Loss: 0.00041493
Iteration 9/25 | Loss: 0.00041493
Iteration 10/25 | Loss: 0.00041493
Iteration 11/25 | Loss: 0.00041493
Iteration 12/25 | Loss: 0.00041493
Iteration 13/25 | Loss: 0.00041493
Iteration 14/25 | Loss: 0.00041493
Iteration 15/25 | Loss: 0.00041493
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004149307496845722, 0.0004149307496845722, 0.0004149307496845722, 0.0004149307496845722, 0.0004149307496845722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004149307496845722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041493
Iteration 2/1000 | Loss: 0.00003690
Iteration 3/1000 | Loss: 0.00002351
Iteration 4/1000 | Loss: 0.00002053
Iteration 5/1000 | Loss: 0.00001946
Iteration 6/1000 | Loss: 0.00001882
Iteration 7/1000 | Loss: 0.00001867
Iteration 8/1000 | Loss: 0.00001860
Iteration 9/1000 | Loss: 0.00001847
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001836
Iteration 14/1000 | Loss: 0.00001836
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001829
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001829
Iteration 19/1000 | Loss: 0.00001829
Iteration 20/1000 | Loss: 0.00001829
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001827
Iteration 25/1000 | Loss: 0.00001827
Iteration 26/1000 | Loss: 0.00001826
Iteration 27/1000 | Loss: 0.00001822
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001822
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001821
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001821
Iteration 37/1000 | Loss: 0.00001821
Iteration 38/1000 | Loss: 0.00001821
Iteration 39/1000 | Loss: 0.00001818
Iteration 40/1000 | Loss: 0.00001818
Iteration 41/1000 | Loss: 0.00001818
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001818
Iteration 44/1000 | Loss: 0.00001818
Iteration 45/1000 | Loss: 0.00001818
Iteration 46/1000 | Loss: 0.00001818
Iteration 47/1000 | Loss: 0.00001818
Iteration 48/1000 | Loss: 0.00001818
Iteration 49/1000 | Loss: 0.00001818
Iteration 50/1000 | Loss: 0.00001818
Iteration 51/1000 | Loss: 0.00001818
Iteration 52/1000 | Loss: 0.00001818
Iteration 53/1000 | Loss: 0.00001818
Iteration 54/1000 | Loss: 0.00001818
Iteration 55/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.817584598029498e-05, 1.817584598029498e-05, 1.817584598029498e-05, 1.817584598029498e-05, 1.817584598029498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.817584598029498e-05

Optimization complete. Final v2v error: 3.5117831230163574 mm

Highest mean error: 3.8783469200134277 mm for frame 209

Lowest mean error: 3.243150472640991 mm for frame 4

Saving results

Total time: 65.3817880153656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820042
Iteration 2/25 | Loss: 0.00109972
Iteration 3/25 | Loss: 0.00098520
Iteration 4/25 | Loss: 0.00096817
Iteration 5/25 | Loss: 0.00096559
Iteration 6/25 | Loss: 0.00096559
Iteration 7/25 | Loss: 0.00096559
Iteration 8/25 | Loss: 0.00096559
Iteration 9/25 | Loss: 0.00096559
Iteration 10/25 | Loss: 0.00096559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009655911708250642, 0.0009655911708250642, 0.0009655911708250642, 0.0009655911708250642, 0.0009655911708250642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009655911708250642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36076438
Iteration 2/25 | Loss: 0.00087796
Iteration 3/25 | Loss: 0.00087796
Iteration 4/25 | Loss: 0.00087796
Iteration 5/25 | Loss: 0.00087796
Iteration 6/25 | Loss: 0.00087796
Iteration 7/25 | Loss: 0.00087796
Iteration 8/25 | Loss: 0.00087796
Iteration 9/25 | Loss: 0.00087796
Iteration 10/25 | Loss: 0.00087796
Iteration 11/25 | Loss: 0.00087796
Iteration 12/25 | Loss: 0.00087796
Iteration 13/25 | Loss: 0.00087796
Iteration 14/25 | Loss: 0.00087796
Iteration 15/25 | Loss: 0.00087796
Iteration 16/25 | Loss: 0.00087796
Iteration 17/25 | Loss: 0.00087796
Iteration 18/25 | Loss: 0.00087796
Iteration 19/25 | Loss: 0.00087796
Iteration 20/25 | Loss: 0.00087796
Iteration 21/25 | Loss: 0.00087796
Iteration 22/25 | Loss: 0.00087796
Iteration 23/25 | Loss: 0.00087796
Iteration 24/25 | Loss: 0.00087796
Iteration 25/25 | Loss: 0.00087796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087796
Iteration 2/1000 | Loss: 0.00003010
Iteration 3/1000 | Loss: 0.00001952
Iteration 4/1000 | Loss: 0.00001699
Iteration 5/1000 | Loss: 0.00001602
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001448
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001380
Iteration 11/1000 | Loss: 0.00001352
Iteration 12/1000 | Loss: 0.00001336
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001322
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001312
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001305
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001298
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001291
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [1.2912881174997892e-05, 1.2912881174997892e-05, 1.2912881174997892e-05, 1.2912881174997892e-05, 1.2912881174997892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2912881174997892e-05

Optimization complete. Final v2v error: 3.058598279953003 mm

Highest mean error: 3.3527281284332275 mm for frame 239

Lowest mean error: 2.6619949340820312 mm for frame 0

Saving results

Total time: 33.97409200668335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852825
Iteration 2/25 | Loss: 0.00140116
Iteration 3/25 | Loss: 0.00109503
Iteration 4/25 | Loss: 0.00106608
Iteration 5/25 | Loss: 0.00106318
Iteration 6/25 | Loss: 0.00106266
Iteration 7/25 | Loss: 0.00106266
Iteration 8/25 | Loss: 0.00106266
Iteration 9/25 | Loss: 0.00106266
Iteration 10/25 | Loss: 0.00106266
Iteration 11/25 | Loss: 0.00106266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010626553557813168, 0.0010626553557813168, 0.0010626553557813168, 0.0010626553557813168, 0.0010626553557813168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010626553557813168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40983653
Iteration 2/25 | Loss: 0.00064639
Iteration 3/25 | Loss: 0.00064639
Iteration 4/25 | Loss: 0.00064639
Iteration 5/25 | Loss: 0.00064639
Iteration 6/25 | Loss: 0.00064639
Iteration 7/25 | Loss: 0.00064638
Iteration 8/25 | Loss: 0.00064638
Iteration 9/25 | Loss: 0.00064638
Iteration 10/25 | Loss: 0.00064638
Iteration 11/25 | Loss: 0.00064638
Iteration 12/25 | Loss: 0.00064638
Iteration 13/25 | Loss: 0.00064638
Iteration 14/25 | Loss: 0.00064638
Iteration 15/25 | Loss: 0.00064638
Iteration 16/25 | Loss: 0.00064638
Iteration 17/25 | Loss: 0.00064638
Iteration 18/25 | Loss: 0.00064638
Iteration 19/25 | Loss: 0.00064638
Iteration 20/25 | Loss: 0.00064638
Iteration 21/25 | Loss: 0.00064638
Iteration 22/25 | Loss: 0.00064638
Iteration 23/25 | Loss: 0.00064638
Iteration 24/25 | Loss: 0.00064638
Iteration 25/25 | Loss: 0.00064638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064638
Iteration 2/1000 | Loss: 0.00004207
Iteration 3/1000 | Loss: 0.00003095
Iteration 4/1000 | Loss: 0.00002823
Iteration 5/1000 | Loss: 0.00002679
Iteration 6/1000 | Loss: 0.00002591
Iteration 7/1000 | Loss: 0.00002531
Iteration 8/1000 | Loss: 0.00002484
Iteration 9/1000 | Loss: 0.00002439
Iteration 10/1000 | Loss: 0.00002410
Iteration 11/1000 | Loss: 0.00002382
Iteration 12/1000 | Loss: 0.00002363
Iteration 13/1000 | Loss: 0.00002347
Iteration 14/1000 | Loss: 0.00002330
Iteration 15/1000 | Loss: 0.00002330
Iteration 16/1000 | Loss: 0.00002322
Iteration 17/1000 | Loss: 0.00002318
Iteration 18/1000 | Loss: 0.00002318
Iteration 19/1000 | Loss: 0.00002318
Iteration 20/1000 | Loss: 0.00002318
Iteration 21/1000 | Loss: 0.00002318
Iteration 22/1000 | Loss: 0.00002318
Iteration 23/1000 | Loss: 0.00002318
Iteration 24/1000 | Loss: 0.00002318
Iteration 25/1000 | Loss: 0.00002318
Iteration 26/1000 | Loss: 0.00002318
Iteration 27/1000 | Loss: 0.00002316
Iteration 28/1000 | Loss: 0.00002313
Iteration 29/1000 | Loss: 0.00002310
Iteration 30/1000 | Loss: 0.00002309
Iteration 31/1000 | Loss: 0.00002308
Iteration 32/1000 | Loss: 0.00002308
Iteration 33/1000 | Loss: 0.00002307
Iteration 34/1000 | Loss: 0.00002307
Iteration 35/1000 | Loss: 0.00002307
Iteration 36/1000 | Loss: 0.00002306
Iteration 37/1000 | Loss: 0.00002304
Iteration 38/1000 | Loss: 0.00002304
Iteration 39/1000 | Loss: 0.00002304
Iteration 40/1000 | Loss: 0.00002298
Iteration 41/1000 | Loss: 0.00002298
Iteration 42/1000 | Loss: 0.00002298
Iteration 43/1000 | Loss: 0.00002296
Iteration 44/1000 | Loss: 0.00002296
Iteration 45/1000 | Loss: 0.00002295
Iteration 46/1000 | Loss: 0.00002295
Iteration 47/1000 | Loss: 0.00002294
Iteration 48/1000 | Loss: 0.00002294
Iteration 49/1000 | Loss: 0.00002294
Iteration 50/1000 | Loss: 0.00002293
Iteration 51/1000 | Loss: 0.00002293
Iteration 52/1000 | Loss: 0.00002293
Iteration 53/1000 | Loss: 0.00002293
Iteration 54/1000 | Loss: 0.00002293
Iteration 55/1000 | Loss: 0.00002293
Iteration 56/1000 | Loss: 0.00002293
Iteration 57/1000 | Loss: 0.00002293
Iteration 58/1000 | Loss: 0.00002293
Iteration 59/1000 | Loss: 0.00002292
Iteration 60/1000 | Loss: 0.00002292
Iteration 61/1000 | Loss: 0.00002290
Iteration 62/1000 | Loss: 0.00002290
Iteration 63/1000 | Loss: 0.00002290
Iteration 64/1000 | Loss: 0.00002290
Iteration 65/1000 | Loss: 0.00002290
Iteration 66/1000 | Loss: 0.00002290
Iteration 67/1000 | Loss: 0.00002290
Iteration 68/1000 | Loss: 0.00002290
Iteration 69/1000 | Loss: 0.00002289
Iteration 70/1000 | Loss: 0.00002289
Iteration 71/1000 | Loss: 0.00002289
Iteration 72/1000 | Loss: 0.00002289
Iteration 73/1000 | Loss: 0.00002288
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002288
Iteration 77/1000 | Loss: 0.00002288
Iteration 78/1000 | Loss: 0.00002288
Iteration 79/1000 | Loss: 0.00002287
Iteration 80/1000 | Loss: 0.00002287
Iteration 81/1000 | Loss: 0.00002287
Iteration 82/1000 | Loss: 0.00002287
Iteration 83/1000 | Loss: 0.00002287
Iteration 84/1000 | Loss: 0.00002286
Iteration 85/1000 | Loss: 0.00002286
Iteration 86/1000 | Loss: 0.00002286
Iteration 87/1000 | Loss: 0.00002285
Iteration 88/1000 | Loss: 0.00002285
Iteration 89/1000 | Loss: 0.00002285
Iteration 90/1000 | Loss: 0.00002285
Iteration 91/1000 | Loss: 0.00002285
Iteration 92/1000 | Loss: 0.00002285
Iteration 93/1000 | Loss: 0.00002285
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00002283
Iteration 98/1000 | Loss: 0.00002283
Iteration 99/1000 | Loss: 0.00002283
Iteration 100/1000 | Loss: 0.00002283
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002283
Iteration 103/1000 | Loss: 0.00002282
Iteration 104/1000 | Loss: 0.00002282
Iteration 105/1000 | Loss: 0.00002281
Iteration 106/1000 | Loss: 0.00002281
Iteration 107/1000 | Loss: 0.00002280
Iteration 108/1000 | Loss: 0.00002280
Iteration 109/1000 | Loss: 0.00002280
Iteration 110/1000 | Loss: 0.00002279
Iteration 111/1000 | Loss: 0.00002279
Iteration 112/1000 | Loss: 0.00002279
Iteration 113/1000 | Loss: 0.00002279
Iteration 114/1000 | Loss: 0.00002279
Iteration 115/1000 | Loss: 0.00002278
Iteration 116/1000 | Loss: 0.00002278
Iteration 117/1000 | Loss: 0.00002278
Iteration 118/1000 | Loss: 0.00002277
Iteration 119/1000 | Loss: 0.00002277
Iteration 120/1000 | Loss: 0.00002277
Iteration 121/1000 | Loss: 0.00002277
Iteration 122/1000 | Loss: 0.00002277
Iteration 123/1000 | Loss: 0.00002277
Iteration 124/1000 | Loss: 0.00002277
Iteration 125/1000 | Loss: 0.00002277
Iteration 126/1000 | Loss: 0.00002276
Iteration 127/1000 | Loss: 0.00002276
Iteration 128/1000 | Loss: 0.00002276
Iteration 129/1000 | Loss: 0.00002276
Iteration 130/1000 | Loss: 0.00002275
Iteration 131/1000 | Loss: 0.00002275
Iteration 132/1000 | Loss: 0.00002275
Iteration 133/1000 | Loss: 0.00002274
Iteration 134/1000 | Loss: 0.00002274
Iteration 135/1000 | Loss: 0.00002274
Iteration 136/1000 | Loss: 0.00002274
Iteration 137/1000 | Loss: 0.00002274
Iteration 138/1000 | Loss: 0.00002274
Iteration 139/1000 | Loss: 0.00002273
Iteration 140/1000 | Loss: 0.00002273
Iteration 141/1000 | Loss: 0.00002273
Iteration 142/1000 | Loss: 0.00002272
Iteration 143/1000 | Loss: 0.00002272
Iteration 144/1000 | Loss: 0.00002272
Iteration 145/1000 | Loss: 0.00002272
Iteration 146/1000 | Loss: 0.00002272
Iteration 147/1000 | Loss: 0.00002272
Iteration 148/1000 | Loss: 0.00002272
Iteration 149/1000 | Loss: 0.00002272
Iteration 150/1000 | Loss: 0.00002271
Iteration 151/1000 | Loss: 0.00002271
Iteration 152/1000 | Loss: 0.00002271
Iteration 153/1000 | Loss: 0.00002271
Iteration 154/1000 | Loss: 0.00002271
Iteration 155/1000 | Loss: 0.00002271
Iteration 156/1000 | Loss: 0.00002271
Iteration 157/1000 | Loss: 0.00002271
Iteration 158/1000 | Loss: 0.00002271
Iteration 159/1000 | Loss: 0.00002270
Iteration 160/1000 | Loss: 0.00002270
Iteration 161/1000 | Loss: 0.00002270
Iteration 162/1000 | Loss: 0.00002269
Iteration 163/1000 | Loss: 0.00002269
Iteration 164/1000 | Loss: 0.00002269
Iteration 165/1000 | Loss: 0.00002269
Iteration 166/1000 | Loss: 0.00002269
Iteration 167/1000 | Loss: 0.00002269
Iteration 168/1000 | Loss: 0.00002269
Iteration 169/1000 | Loss: 0.00002269
Iteration 170/1000 | Loss: 0.00002269
Iteration 171/1000 | Loss: 0.00002269
Iteration 172/1000 | Loss: 0.00002268
Iteration 173/1000 | Loss: 0.00002268
Iteration 174/1000 | Loss: 0.00002268
Iteration 175/1000 | Loss: 0.00002268
Iteration 176/1000 | Loss: 0.00002268
Iteration 177/1000 | Loss: 0.00002267
Iteration 178/1000 | Loss: 0.00002267
Iteration 179/1000 | Loss: 0.00002267
Iteration 180/1000 | Loss: 0.00002267
Iteration 181/1000 | Loss: 0.00002266
Iteration 182/1000 | Loss: 0.00002266
Iteration 183/1000 | Loss: 0.00002266
Iteration 184/1000 | Loss: 0.00002266
Iteration 185/1000 | Loss: 0.00002266
Iteration 186/1000 | Loss: 0.00002266
Iteration 187/1000 | Loss: 0.00002266
Iteration 188/1000 | Loss: 0.00002266
Iteration 189/1000 | Loss: 0.00002265
Iteration 190/1000 | Loss: 0.00002265
Iteration 191/1000 | Loss: 0.00002265
Iteration 192/1000 | Loss: 0.00002265
Iteration 193/1000 | Loss: 0.00002265
Iteration 194/1000 | Loss: 0.00002265
Iteration 195/1000 | Loss: 0.00002265
Iteration 196/1000 | Loss: 0.00002265
Iteration 197/1000 | Loss: 0.00002265
Iteration 198/1000 | Loss: 0.00002265
Iteration 199/1000 | Loss: 0.00002265
Iteration 200/1000 | Loss: 0.00002265
Iteration 201/1000 | Loss: 0.00002265
Iteration 202/1000 | Loss: 0.00002265
Iteration 203/1000 | Loss: 0.00002265
Iteration 204/1000 | Loss: 0.00002265
Iteration 205/1000 | Loss: 0.00002265
Iteration 206/1000 | Loss: 0.00002265
Iteration 207/1000 | Loss: 0.00002265
Iteration 208/1000 | Loss: 0.00002265
Iteration 209/1000 | Loss: 0.00002265
Iteration 210/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.265443981741555e-05, 2.265443981741555e-05, 2.265443981741555e-05, 2.265443981741555e-05, 2.265443981741555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.265443981741555e-05

Optimization complete. Final v2v error: 3.750636339187622 mm

Highest mean error: 5.613636016845703 mm for frame 149

Lowest mean error: 2.4892616271972656 mm for frame 4

Saving results

Total time: 45.57556700706482
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556974
Iteration 2/25 | Loss: 0.00137981
Iteration 3/25 | Loss: 0.00111104
Iteration 4/25 | Loss: 0.00106210
Iteration 5/25 | Loss: 0.00105280
Iteration 6/25 | Loss: 0.00105026
Iteration 7/25 | Loss: 0.00106112
Iteration 8/25 | Loss: 0.00104058
Iteration 9/25 | Loss: 0.00103084
Iteration 10/25 | Loss: 0.00102638
Iteration 11/25 | Loss: 0.00102480
Iteration 12/25 | Loss: 0.00102431
Iteration 13/25 | Loss: 0.00102416
Iteration 14/25 | Loss: 0.00102414
Iteration 15/25 | Loss: 0.00102414
Iteration 16/25 | Loss: 0.00102413
Iteration 17/25 | Loss: 0.00102413
Iteration 18/25 | Loss: 0.00102413
Iteration 19/25 | Loss: 0.00102413
Iteration 20/25 | Loss: 0.00102413
Iteration 21/25 | Loss: 0.00102413
Iteration 22/25 | Loss: 0.00102413
Iteration 23/25 | Loss: 0.00102413
Iteration 24/25 | Loss: 0.00102413
Iteration 25/25 | Loss: 0.00102413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16338670
Iteration 2/25 | Loss: 0.00066870
Iteration 3/25 | Loss: 0.00066870
Iteration 4/25 | Loss: 0.00066870
Iteration 5/25 | Loss: 0.00066870
Iteration 6/25 | Loss: 0.00066870
Iteration 7/25 | Loss: 0.00066870
Iteration 8/25 | Loss: 0.00066870
Iteration 9/25 | Loss: 0.00066870
Iteration 10/25 | Loss: 0.00066870
Iteration 11/25 | Loss: 0.00066870
Iteration 12/25 | Loss: 0.00066870
Iteration 13/25 | Loss: 0.00066870
Iteration 14/25 | Loss: 0.00066870
Iteration 15/25 | Loss: 0.00066870
Iteration 16/25 | Loss: 0.00066870
Iteration 17/25 | Loss: 0.00066870
Iteration 18/25 | Loss: 0.00066870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006686979904770851, 0.0006686979904770851, 0.0006686979904770851, 0.0006686979904770851, 0.0006686979904770851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006686979904770851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066870
Iteration 2/1000 | Loss: 0.00003663
Iteration 3/1000 | Loss: 0.00002463
Iteration 4/1000 | Loss: 0.00002126
Iteration 5/1000 | Loss: 0.00001992
Iteration 6/1000 | Loss: 0.00001932
Iteration 7/1000 | Loss: 0.00001886
Iteration 8/1000 | Loss: 0.00001835
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001769
Iteration 11/1000 | Loss: 0.00001751
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001730
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001729
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001725
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001724
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001723
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001721
Iteration 28/1000 | Loss: 0.00001721
Iteration 29/1000 | Loss: 0.00001721
Iteration 30/1000 | Loss: 0.00001721
Iteration 31/1000 | Loss: 0.00001720
Iteration 32/1000 | Loss: 0.00001720
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001720
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001720
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001720
Iteration 41/1000 | Loss: 0.00001719
Iteration 42/1000 | Loss: 0.00001719
Iteration 43/1000 | Loss: 0.00001719
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001718
Iteration 48/1000 | Loss: 0.00001718
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.7179118003696203e-05, 1.7179118003696203e-05, 1.7179118003696203e-05, 1.7179118003696203e-05, 1.7179118003696203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7179118003696203e-05

Optimization complete. Final v2v error: 3.512608528137207 mm

Highest mean error: 3.8387207984924316 mm for frame 231

Lowest mean error: 3.1258327960968018 mm for frame 74

Saving results

Total time: 48.814252614974976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836937
Iteration 2/25 | Loss: 0.00177320
Iteration 3/25 | Loss: 0.00152761
Iteration 4/25 | Loss: 0.00160743
Iteration 5/25 | Loss: 0.00161047
Iteration 6/25 | Loss: 0.00156852
Iteration 7/25 | Loss: 0.00127415
Iteration 8/25 | Loss: 0.00109790
Iteration 9/25 | Loss: 0.00108266
Iteration 10/25 | Loss: 0.00104710
Iteration 11/25 | Loss: 0.00107669
Iteration 12/25 | Loss: 0.00103038
Iteration 13/25 | Loss: 0.00102169
Iteration 14/25 | Loss: 0.00102016
Iteration 15/25 | Loss: 0.00102010
Iteration 16/25 | Loss: 0.00102010
Iteration 17/25 | Loss: 0.00102010
Iteration 18/25 | Loss: 0.00102009
Iteration 19/25 | Loss: 0.00102009
Iteration 20/25 | Loss: 0.00102009
Iteration 21/25 | Loss: 0.00102009
Iteration 22/25 | Loss: 0.00102009
Iteration 23/25 | Loss: 0.00102009
Iteration 24/25 | Loss: 0.00102009
Iteration 25/25 | Loss: 0.00102009

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38869452
Iteration 2/25 | Loss: 0.00105312
Iteration 3/25 | Loss: 0.00105312
Iteration 4/25 | Loss: 0.00105312
Iteration 5/25 | Loss: 0.00105312
Iteration 6/25 | Loss: 0.00105312
Iteration 7/25 | Loss: 0.00105312
Iteration 8/25 | Loss: 0.00105312
Iteration 9/25 | Loss: 0.00105312
Iteration 10/25 | Loss: 0.00105312
Iteration 11/25 | Loss: 0.00105312
Iteration 12/25 | Loss: 0.00105312
Iteration 13/25 | Loss: 0.00105312
Iteration 14/25 | Loss: 0.00105312
Iteration 15/25 | Loss: 0.00105312
Iteration 16/25 | Loss: 0.00105312
Iteration 17/25 | Loss: 0.00105312
Iteration 18/25 | Loss: 0.00105312
Iteration 19/25 | Loss: 0.00105312
Iteration 20/25 | Loss: 0.00105312
Iteration 21/25 | Loss: 0.00105312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010531180305406451, 0.0010531180305406451, 0.0010531180305406451, 0.0010531180305406451, 0.0010531180305406451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010531180305406451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105312
Iteration 2/1000 | Loss: 0.00082986
Iteration 3/1000 | Loss: 0.00003003
Iteration 4/1000 | Loss: 0.00002044
Iteration 5/1000 | Loss: 0.00001793
Iteration 6/1000 | Loss: 0.00001697
Iteration 7/1000 | Loss: 0.00001641
Iteration 8/1000 | Loss: 0.00001604
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001536
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001525
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001518
Iteration 17/1000 | Loss: 0.00001518
Iteration 18/1000 | Loss: 0.00001517
Iteration 19/1000 | Loss: 0.00001515
Iteration 20/1000 | Loss: 0.00001515
Iteration 21/1000 | Loss: 0.00001515
Iteration 22/1000 | Loss: 0.00001515
Iteration 23/1000 | Loss: 0.00001514
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001513
Iteration 27/1000 | Loss: 0.00001511
Iteration 28/1000 | Loss: 0.00001511
Iteration 29/1000 | Loss: 0.00001510
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001509
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001507
Iteration 34/1000 | Loss: 0.00001506
Iteration 35/1000 | Loss: 0.00001506
Iteration 36/1000 | Loss: 0.00001505
Iteration 37/1000 | Loss: 0.00001505
Iteration 38/1000 | Loss: 0.00001505
Iteration 39/1000 | Loss: 0.00001504
Iteration 40/1000 | Loss: 0.00001504
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001499
Iteration 58/1000 | Loss: 0.00001499
Iteration 59/1000 | Loss: 0.00001499
Iteration 60/1000 | Loss: 0.00001499
Iteration 61/1000 | Loss: 0.00001499
Iteration 62/1000 | Loss: 0.00001498
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001498
Iteration 65/1000 | Loss: 0.00001498
Iteration 66/1000 | Loss: 0.00001498
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001498
Iteration 74/1000 | Loss: 0.00001498
Iteration 75/1000 | Loss: 0.00001498
Iteration 76/1000 | Loss: 0.00001498
Iteration 77/1000 | Loss: 0.00001497
Iteration 78/1000 | Loss: 0.00001497
Iteration 79/1000 | Loss: 0.00001497
Iteration 80/1000 | Loss: 0.00001497
Iteration 81/1000 | Loss: 0.00001497
Iteration 82/1000 | Loss: 0.00001497
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001497
Iteration 87/1000 | Loss: 0.00001497
Iteration 88/1000 | Loss: 0.00001497
Iteration 89/1000 | Loss: 0.00001497
Iteration 90/1000 | Loss: 0.00001496
Iteration 91/1000 | Loss: 0.00001496
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001496
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001496
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001496
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001496
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001496
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001496
Iteration 110/1000 | Loss: 0.00001495
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001495
Iteration 113/1000 | Loss: 0.00001495
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001495
Iteration 117/1000 | Loss: 0.00001495
Iteration 118/1000 | Loss: 0.00001495
Iteration 119/1000 | Loss: 0.00001495
Iteration 120/1000 | Loss: 0.00001495
Iteration 121/1000 | Loss: 0.00001495
Iteration 122/1000 | Loss: 0.00001494
Iteration 123/1000 | Loss: 0.00001494
Iteration 124/1000 | Loss: 0.00001494
Iteration 125/1000 | Loss: 0.00001494
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001493
Iteration 128/1000 | Loss: 0.00001493
Iteration 129/1000 | Loss: 0.00001493
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001492
Iteration 132/1000 | Loss: 0.00001492
Iteration 133/1000 | Loss: 0.00001492
Iteration 134/1000 | Loss: 0.00001492
Iteration 135/1000 | Loss: 0.00001492
Iteration 136/1000 | Loss: 0.00001492
Iteration 137/1000 | Loss: 0.00001492
Iteration 138/1000 | Loss: 0.00001492
Iteration 139/1000 | Loss: 0.00001492
Iteration 140/1000 | Loss: 0.00001492
Iteration 141/1000 | Loss: 0.00001492
Iteration 142/1000 | Loss: 0.00001492
Iteration 143/1000 | Loss: 0.00001491
Iteration 144/1000 | Loss: 0.00001491
Iteration 145/1000 | Loss: 0.00001491
Iteration 146/1000 | Loss: 0.00001491
Iteration 147/1000 | Loss: 0.00001491
Iteration 148/1000 | Loss: 0.00001491
Iteration 149/1000 | Loss: 0.00001491
Iteration 150/1000 | Loss: 0.00001491
Iteration 151/1000 | Loss: 0.00001491
Iteration 152/1000 | Loss: 0.00001491
Iteration 153/1000 | Loss: 0.00001491
Iteration 154/1000 | Loss: 0.00001491
Iteration 155/1000 | Loss: 0.00001491
Iteration 156/1000 | Loss: 0.00001490
Iteration 157/1000 | Loss: 0.00001490
Iteration 158/1000 | Loss: 0.00001490
Iteration 159/1000 | Loss: 0.00001490
Iteration 160/1000 | Loss: 0.00001490
Iteration 161/1000 | Loss: 0.00001490
Iteration 162/1000 | Loss: 0.00001490
Iteration 163/1000 | Loss: 0.00001490
Iteration 164/1000 | Loss: 0.00001490
Iteration 165/1000 | Loss: 0.00001490
Iteration 166/1000 | Loss: 0.00001490
Iteration 167/1000 | Loss: 0.00001490
Iteration 168/1000 | Loss: 0.00001489
Iteration 169/1000 | Loss: 0.00001489
Iteration 170/1000 | Loss: 0.00001489
Iteration 171/1000 | Loss: 0.00001489
Iteration 172/1000 | Loss: 0.00001489
Iteration 173/1000 | Loss: 0.00001489
Iteration 174/1000 | Loss: 0.00001489
Iteration 175/1000 | Loss: 0.00001489
Iteration 176/1000 | Loss: 0.00001489
Iteration 177/1000 | Loss: 0.00001489
Iteration 178/1000 | Loss: 0.00001489
Iteration 179/1000 | Loss: 0.00001489
Iteration 180/1000 | Loss: 0.00001489
Iteration 181/1000 | Loss: 0.00001489
Iteration 182/1000 | Loss: 0.00001489
Iteration 183/1000 | Loss: 0.00001489
Iteration 184/1000 | Loss: 0.00001489
Iteration 185/1000 | Loss: 0.00001489
Iteration 186/1000 | Loss: 0.00001489
Iteration 187/1000 | Loss: 0.00001489
Iteration 188/1000 | Loss: 0.00001489
Iteration 189/1000 | Loss: 0.00001489
Iteration 190/1000 | Loss: 0.00001489
Iteration 191/1000 | Loss: 0.00001489
Iteration 192/1000 | Loss: 0.00001489
Iteration 193/1000 | Loss: 0.00001489
Iteration 194/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.4892559192958288e-05, 1.4892559192958288e-05, 1.4892559192958288e-05, 1.4892559192958288e-05, 1.4892559192958288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4892559192958288e-05

Optimization complete. Final v2v error: 3.1862435340881348 mm

Highest mean error: 3.981942892074585 mm for frame 60

Lowest mean error: 2.5947864055633545 mm for frame 211

Saving results

Total time: 60.29415678977966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428831
Iteration 2/25 | Loss: 0.00107949
Iteration 3/25 | Loss: 0.00099318
Iteration 4/25 | Loss: 0.00097981
Iteration 5/25 | Loss: 0.00097602
Iteration 6/25 | Loss: 0.00097489
Iteration 7/25 | Loss: 0.00097484
Iteration 8/25 | Loss: 0.00097484
Iteration 9/25 | Loss: 0.00097484
Iteration 10/25 | Loss: 0.00097484
Iteration 11/25 | Loss: 0.00097484
Iteration 12/25 | Loss: 0.00097484
Iteration 13/25 | Loss: 0.00097484
Iteration 14/25 | Loss: 0.00097484
Iteration 15/25 | Loss: 0.00097484
Iteration 16/25 | Loss: 0.00097484
Iteration 17/25 | Loss: 0.00097484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009748448501341045, 0.0009748448501341045, 0.0009748448501341045, 0.0009748448501341045, 0.0009748448501341045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009748448501341045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39405918
Iteration 2/25 | Loss: 0.00089724
Iteration 3/25 | Loss: 0.00089724
Iteration 4/25 | Loss: 0.00089724
Iteration 5/25 | Loss: 0.00089724
Iteration 6/25 | Loss: 0.00089724
Iteration 7/25 | Loss: 0.00089724
Iteration 8/25 | Loss: 0.00089724
Iteration 9/25 | Loss: 0.00089724
Iteration 10/25 | Loss: 0.00089724
Iteration 11/25 | Loss: 0.00089724
Iteration 12/25 | Loss: 0.00089724
Iteration 13/25 | Loss: 0.00089724
Iteration 14/25 | Loss: 0.00089724
Iteration 15/25 | Loss: 0.00089724
Iteration 16/25 | Loss: 0.00089724
Iteration 17/25 | Loss: 0.00089724
Iteration 18/25 | Loss: 0.00089724
Iteration 19/25 | Loss: 0.00089724
Iteration 20/25 | Loss: 0.00089724
Iteration 21/25 | Loss: 0.00089724
Iteration 22/25 | Loss: 0.00089724
Iteration 23/25 | Loss: 0.00089724
Iteration 24/25 | Loss: 0.00089724
Iteration 25/25 | Loss: 0.00089724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089724
Iteration 2/1000 | Loss: 0.00002850
Iteration 3/1000 | Loss: 0.00001928
Iteration 4/1000 | Loss: 0.00001773
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001596
Iteration 9/1000 | Loss: 0.00001582
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00001574
Iteration 12/1000 | Loss: 0.00001571
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001568
Iteration 15/1000 | Loss: 0.00001568
Iteration 16/1000 | Loss: 0.00001568
Iteration 17/1000 | Loss: 0.00001568
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001564
Iteration 24/1000 | Loss: 0.00001564
Iteration 25/1000 | Loss: 0.00001564
Iteration 26/1000 | Loss: 0.00001563
Iteration 27/1000 | Loss: 0.00001563
Iteration 28/1000 | Loss: 0.00001563
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001562
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001561
Iteration 33/1000 | Loss: 0.00001560
Iteration 34/1000 | Loss: 0.00001560
Iteration 35/1000 | Loss: 0.00001560
Iteration 36/1000 | Loss: 0.00001560
Iteration 37/1000 | Loss: 0.00001559
Iteration 38/1000 | Loss: 0.00001559
Iteration 39/1000 | Loss: 0.00001559
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001557
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001557
Iteration 45/1000 | Loss: 0.00001556
Iteration 46/1000 | Loss: 0.00001556
Iteration 47/1000 | Loss: 0.00001556
Iteration 48/1000 | Loss: 0.00001556
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001554
Iteration 53/1000 | Loss: 0.00001554
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001552
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001552
Iteration 61/1000 | Loss: 0.00001552
Iteration 62/1000 | Loss: 0.00001552
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001551
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001551
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001550
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001550
Iteration 87/1000 | Loss: 0.00001550
Iteration 88/1000 | Loss: 0.00001550
Iteration 89/1000 | Loss: 0.00001550
Iteration 90/1000 | Loss: 0.00001550
Iteration 91/1000 | Loss: 0.00001550
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001550
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001549
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00001549
Iteration 102/1000 | Loss: 0.00001549
Iteration 103/1000 | Loss: 0.00001549
Iteration 104/1000 | Loss: 0.00001549
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001549
Iteration 117/1000 | Loss: 0.00001549
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001549
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001549
Iteration 135/1000 | Loss: 0.00001549
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001549
Iteration 141/1000 | Loss: 0.00001549
Iteration 142/1000 | Loss: 0.00001549
Iteration 143/1000 | Loss: 0.00001549
Iteration 144/1000 | Loss: 0.00001549
Iteration 145/1000 | Loss: 0.00001549
Iteration 146/1000 | Loss: 0.00001549
Iteration 147/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.5486768461414613e-05, 1.5486768461414613e-05, 1.5486768461414613e-05, 1.5486768461414613e-05, 1.5486768461414613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5486768461414613e-05

Optimization complete. Final v2v error: 3.2838950157165527 mm

Highest mean error: 3.561479330062866 mm for frame 113

Lowest mean error: 2.9060215950012207 mm for frame 42

Saving results

Total time: 30.73825740814209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490796
Iteration 2/25 | Loss: 0.00117979
Iteration 3/25 | Loss: 0.00105153
Iteration 4/25 | Loss: 0.00104671
Iteration 5/25 | Loss: 0.00104671
Iteration 6/25 | Loss: 0.00104671
Iteration 7/25 | Loss: 0.00104671
Iteration 8/25 | Loss: 0.00104671
Iteration 9/25 | Loss: 0.00104671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0010467052925378084, 0.0010467052925378084, 0.0010467052925378084, 0.0010467052925378084, 0.0010467052925378084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010467052925378084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85179657
Iteration 2/25 | Loss: 0.00084127
Iteration 3/25 | Loss: 0.00084127
Iteration 4/25 | Loss: 0.00084127
Iteration 5/25 | Loss: 0.00084127
Iteration 6/25 | Loss: 0.00084127
Iteration 7/25 | Loss: 0.00084127
Iteration 8/25 | Loss: 0.00084127
Iteration 9/25 | Loss: 0.00084127
Iteration 10/25 | Loss: 0.00084127
Iteration 11/25 | Loss: 0.00084127
Iteration 12/25 | Loss: 0.00084127
Iteration 13/25 | Loss: 0.00084127
Iteration 14/25 | Loss: 0.00084127
Iteration 15/25 | Loss: 0.00084127
Iteration 16/25 | Loss: 0.00084127
Iteration 17/25 | Loss: 0.00084127
Iteration 18/25 | Loss: 0.00084127
Iteration 19/25 | Loss: 0.00084127
Iteration 20/25 | Loss: 0.00084127
Iteration 21/25 | Loss: 0.00084127
Iteration 22/25 | Loss: 0.00084127
Iteration 23/25 | Loss: 0.00084127
Iteration 24/25 | Loss: 0.00084127
Iteration 25/25 | Loss: 0.00084127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084127
Iteration 2/1000 | Loss: 0.00003482
Iteration 3/1000 | Loss: 0.00002806
Iteration 4/1000 | Loss: 0.00002561
Iteration 5/1000 | Loss: 0.00002435
Iteration 6/1000 | Loss: 0.00002369
Iteration 7/1000 | Loss: 0.00002285
Iteration 8/1000 | Loss: 0.00002230
Iteration 9/1000 | Loss: 0.00002173
Iteration 10/1000 | Loss: 0.00002141
Iteration 11/1000 | Loss: 0.00002107
Iteration 12/1000 | Loss: 0.00002075
Iteration 13/1000 | Loss: 0.00002051
Iteration 14/1000 | Loss: 0.00002028
Iteration 15/1000 | Loss: 0.00002012
Iteration 16/1000 | Loss: 0.00002010
Iteration 17/1000 | Loss: 0.00002009
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001991
Iteration 20/1000 | Loss: 0.00001969
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001914
Iteration 24/1000 | Loss: 0.00001912
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001898
Iteration 34/1000 | Loss: 0.00001895
Iteration 35/1000 | Loss: 0.00001895
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001892
Iteration 48/1000 | Loss: 0.00001892
Iteration 49/1000 | Loss: 0.00001892
Iteration 50/1000 | Loss: 0.00001892
Iteration 51/1000 | Loss: 0.00001892
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001888
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001888
Iteration 61/1000 | Loss: 0.00001888
Iteration 62/1000 | Loss: 0.00001888
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001888
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001887
Iteration 70/1000 | Loss: 0.00001887
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001885
Iteration 74/1000 | Loss: 0.00001885
Iteration 75/1000 | Loss: 0.00001884
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001884
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001882
Iteration 88/1000 | Loss: 0.00001882
Iteration 89/1000 | Loss: 0.00001882
Iteration 90/1000 | Loss: 0.00001882
Iteration 91/1000 | Loss: 0.00001882
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001880
Iteration 98/1000 | Loss: 0.00001880
Iteration 99/1000 | Loss: 0.00001880
Iteration 100/1000 | Loss: 0.00001880
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001879
Iteration 109/1000 | Loss: 0.00001879
Iteration 110/1000 | Loss: 0.00001879
Iteration 111/1000 | Loss: 0.00001879
Iteration 112/1000 | Loss: 0.00001879
Iteration 113/1000 | Loss: 0.00001879
Iteration 114/1000 | Loss: 0.00001878
Iteration 115/1000 | Loss: 0.00001878
Iteration 116/1000 | Loss: 0.00001878
Iteration 117/1000 | Loss: 0.00001878
Iteration 118/1000 | Loss: 0.00001878
Iteration 119/1000 | Loss: 0.00001878
Iteration 120/1000 | Loss: 0.00001878
Iteration 121/1000 | Loss: 0.00001878
Iteration 122/1000 | Loss: 0.00001878
Iteration 123/1000 | Loss: 0.00001878
Iteration 124/1000 | Loss: 0.00001878
Iteration 125/1000 | Loss: 0.00001878
Iteration 126/1000 | Loss: 0.00001878
Iteration 127/1000 | Loss: 0.00001877
Iteration 128/1000 | Loss: 0.00001877
Iteration 129/1000 | Loss: 0.00001877
Iteration 130/1000 | Loss: 0.00001877
Iteration 131/1000 | Loss: 0.00001877
Iteration 132/1000 | Loss: 0.00001877
Iteration 133/1000 | Loss: 0.00001877
Iteration 134/1000 | Loss: 0.00001877
Iteration 135/1000 | Loss: 0.00001877
Iteration 136/1000 | Loss: 0.00001876
Iteration 137/1000 | Loss: 0.00001876
Iteration 138/1000 | Loss: 0.00001876
Iteration 139/1000 | Loss: 0.00001876
Iteration 140/1000 | Loss: 0.00001876
Iteration 141/1000 | Loss: 0.00001876
Iteration 142/1000 | Loss: 0.00001876
Iteration 143/1000 | Loss: 0.00001876
Iteration 144/1000 | Loss: 0.00001876
Iteration 145/1000 | Loss: 0.00001876
Iteration 146/1000 | Loss: 0.00001876
Iteration 147/1000 | Loss: 0.00001876
Iteration 148/1000 | Loss: 0.00001876
Iteration 149/1000 | Loss: 0.00001875
Iteration 150/1000 | Loss: 0.00001875
Iteration 151/1000 | Loss: 0.00001875
Iteration 152/1000 | Loss: 0.00001875
Iteration 153/1000 | Loss: 0.00001875
Iteration 154/1000 | Loss: 0.00001875
Iteration 155/1000 | Loss: 0.00001875
Iteration 156/1000 | Loss: 0.00001875
Iteration 157/1000 | Loss: 0.00001875
Iteration 158/1000 | Loss: 0.00001875
Iteration 159/1000 | Loss: 0.00001875
Iteration 160/1000 | Loss: 0.00001875
Iteration 161/1000 | Loss: 0.00001875
Iteration 162/1000 | Loss: 0.00001875
Iteration 163/1000 | Loss: 0.00001875
Iteration 164/1000 | Loss: 0.00001875
Iteration 165/1000 | Loss: 0.00001875
Iteration 166/1000 | Loss: 0.00001875
Iteration 167/1000 | Loss: 0.00001875
Iteration 168/1000 | Loss: 0.00001875
Iteration 169/1000 | Loss: 0.00001875
Iteration 170/1000 | Loss: 0.00001875
Iteration 171/1000 | Loss: 0.00001875
Iteration 172/1000 | Loss: 0.00001875
Iteration 173/1000 | Loss: 0.00001875
Iteration 174/1000 | Loss: 0.00001875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.8751270545180887e-05, 1.8751270545180887e-05, 1.8751270545180887e-05, 1.8751270545180887e-05, 1.8751270545180887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8751270545180887e-05

Optimization complete. Final v2v error: 3.6523611545562744 mm

Highest mean error: 3.8887875080108643 mm for frame 229

Lowest mean error: 3.367399215698242 mm for frame 163

Saving results

Total time: 53.11231875419617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862209
Iteration 2/25 | Loss: 0.00110844
Iteration 3/25 | Loss: 0.00095690
Iteration 4/25 | Loss: 0.00093777
Iteration 5/25 | Loss: 0.00093334
Iteration 6/25 | Loss: 0.00093230
Iteration 7/25 | Loss: 0.00093230
Iteration 8/25 | Loss: 0.00093230
Iteration 9/25 | Loss: 0.00093230
Iteration 10/25 | Loss: 0.00093230
Iteration 11/25 | Loss: 0.00093230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009323044214397669, 0.0009323044214397669, 0.0009323044214397669, 0.0009323044214397669, 0.0009323044214397669]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009323044214397669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40514517
Iteration 2/25 | Loss: 0.00060847
Iteration 3/25 | Loss: 0.00060845
Iteration 4/25 | Loss: 0.00060845
Iteration 5/25 | Loss: 0.00060845
Iteration 6/25 | Loss: 0.00060845
Iteration 7/25 | Loss: 0.00060845
Iteration 8/25 | Loss: 0.00060845
Iteration 9/25 | Loss: 0.00060845
Iteration 10/25 | Loss: 0.00060845
Iteration 11/25 | Loss: 0.00060845
Iteration 12/25 | Loss: 0.00060845
Iteration 13/25 | Loss: 0.00060845
Iteration 14/25 | Loss: 0.00060845
Iteration 15/25 | Loss: 0.00060845
Iteration 16/25 | Loss: 0.00060845
Iteration 17/25 | Loss: 0.00060845
Iteration 18/25 | Loss: 0.00060845
Iteration 19/25 | Loss: 0.00060845
Iteration 20/25 | Loss: 0.00060845
Iteration 21/25 | Loss: 0.00060845
Iteration 22/25 | Loss: 0.00060845
Iteration 23/25 | Loss: 0.00060845
Iteration 24/25 | Loss: 0.00060845
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006084496853873134, 0.0006084496853873134, 0.0006084496853873134, 0.0006084496853873134, 0.0006084496853873134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006084496853873134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060845
Iteration 2/1000 | Loss: 0.00002110
Iteration 3/1000 | Loss: 0.00001385
Iteration 4/1000 | Loss: 0.00001169
Iteration 5/1000 | Loss: 0.00001087
Iteration 6/1000 | Loss: 0.00001049
Iteration 7/1000 | Loss: 0.00001018
Iteration 8/1000 | Loss: 0.00000995
Iteration 9/1000 | Loss: 0.00000993
Iteration 10/1000 | Loss: 0.00000993
Iteration 11/1000 | Loss: 0.00000992
Iteration 12/1000 | Loss: 0.00000989
Iteration 13/1000 | Loss: 0.00000988
Iteration 14/1000 | Loss: 0.00000983
Iteration 15/1000 | Loss: 0.00000980
Iteration 16/1000 | Loss: 0.00000980
Iteration 17/1000 | Loss: 0.00000980
Iteration 18/1000 | Loss: 0.00000979
Iteration 19/1000 | Loss: 0.00000979
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000979
Iteration 22/1000 | Loss: 0.00000979
Iteration 23/1000 | Loss: 0.00000977
Iteration 24/1000 | Loss: 0.00000976
Iteration 25/1000 | Loss: 0.00000976
Iteration 26/1000 | Loss: 0.00000975
Iteration 27/1000 | Loss: 0.00000975
Iteration 28/1000 | Loss: 0.00000975
Iteration 29/1000 | Loss: 0.00000975
Iteration 30/1000 | Loss: 0.00000975
Iteration 31/1000 | Loss: 0.00000975
Iteration 32/1000 | Loss: 0.00000975
Iteration 33/1000 | Loss: 0.00000975
Iteration 34/1000 | Loss: 0.00000974
Iteration 35/1000 | Loss: 0.00000974
Iteration 36/1000 | Loss: 0.00000974
Iteration 37/1000 | Loss: 0.00000974
Iteration 38/1000 | Loss: 0.00000973
Iteration 39/1000 | Loss: 0.00000973
Iteration 40/1000 | Loss: 0.00000972
Iteration 41/1000 | Loss: 0.00000972
Iteration 42/1000 | Loss: 0.00000971
Iteration 43/1000 | Loss: 0.00000971
Iteration 44/1000 | Loss: 0.00000971
Iteration 45/1000 | Loss: 0.00000971
Iteration 46/1000 | Loss: 0.00000971
Iteration 47/1000 | Loss: 0.00000971
Iteration 48/1000 | Loss: 0.00000969
Iteration 49/1000 | Loss: 0.00000969
Iteration 50/1000 | Loss: 0.00000968
Iteration 51/1000 | Loss: 0.00000968
Iteration 52/1000 | Loss: 0.00000967
Iteration 53/1000 | Loss: 0.00000967
Iteration 54/1000 | Loss: 0.00000967
Iteration 55/1000 | Loss: 0.00000967
Iteration 56/1000 | Loss: 0.00000967
Iteration 57/1000 | Loss: 0.00000967
Iteration 58/1000 | Loss: 0.00000966
Iteration 59/1000 | Loss: 0.00000966
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000965
Iteration 62/1000 | Loss: 0.00000965
Iteration 63/1000 | Loss: 0.00000964
Iteration 64/1000 | Loss: 0.00000964
Iteration 65/1000 | Loss: 0.00000964
Iteration 66/1000 | Loss: 0.00000964
Iteration 67/1000 | Loss: 0.00000964
Iteration 68/1000 | Loss: 0.00000964
Iteration 69/1000 | Loss: 0.00000963
Iteration 70/1000 | Loss: 0.00000963
Iteration 71/1000 | Loss: 0.00000963
Iteration 72/1000 | Loss: 0.00000963
Iteration 73/1000 | Loss: 0.00000963
Iteration 74/1000 | Loss: 0.00000963
Iteration 75/1000 | Loss: 0.00000963
Iteration 76/1000 | Loss: 0.00000963
Iteration 77/1000 | Loss: 0.00000963
Iteration 78/1000 | Loss: 0.00000963
Iteration 79/1000 | Loss: 0.00000962
Iteration 80/1000 | Loss: 0.00000962
Iteration 81/1000 | Loss: 0.00000961
Iteration 82/1000 | Loss: 0.00000961
Iteration 83/1000 | Loss: 0.00000961
Iteration 84/1000 | Loss: 0.00000961
Iteration 85/1000 | Loss: 0.00000961
Iteration 86/1000 | Loss: 0.00000961
Iteration 87/1000 | Loss: 0.00000961
Iteration 88/1000 | Loss: 0.00000961
Iteration 89/1000 | Loss: 0.00000960
Iteration 90/1000 | Loss: 0.00000960
Iteration 91/1000 | Loss: 0.00000960
Iteration 92/1000 | Loss: 0.00000960
Iteration 93/1000 | Loss: 0.00000960
Iteration 94/1000 | Loss: 0.00000960
Iteration 95/1000 | Loss: 0.00000960
Iteration 96/1000 | Loss: 0.00000960
Iteration 97/1000 | Loss: 0.00000960
Iteration 98/1000 | Loss: 0.00000960
Iteration 99/1000 | Loss: 0.00000960
Iteration 100/1000 | Loss: 0.00000959
Iteration 101/1000 | Loss: 0.00000959
Iteration 102/1000 | Loss: 0.00000959
Iteration 103/1000 | Loss: 0.00000959
Iteration 104/1000 | Loss: 0.00000959
Iteration 105/1000 | Loss: 0.00000959
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000959
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [9.585570296621881e-06, 9.585570296621881e-06, 9.585570296621881e-06, 9.585570296621881e-06, 9.585570296621881e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.585570296621881e-06

Optimization complete. Final v2v error: 2.630290985107422 mm

Highest mean error: 2.940112352371216 mm for frame 158

Lowest mean error: 2.4572393894195557 mm for frame 65

Saving results

Total time: 29.760379791259766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845649
Iteration 2/25 | Loss: 0.00129357
Iteration 3/25 | Loss: 0.00112319
Iteration 4/25 | Loss: 0.00109663
Iteration 5/25 | Loss: 0.00109039
Iteration 6/25 | Loss: 0.00108948
Iteration 7/25 | Loss: 0.00108948
Iteration 8/25 | Loss: 0.00108948
Iteration 9/25 | Loss: 0.00108948
Iteration 10/25 | Loss: 0.00108948
Iteration 11/25 | Loss: 0.00108948
Iteration 12/25 | Loss: 0.00108948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010894836159422994, 0.0010894836159422994, 0.0010894836159422994, 0.0010894836159422994, 0.0010894836159422994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010894836159422994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93067980
Iteration 2/25 | Loss: 0.00058277
Iteration 3/25 | Loss: 0.00058276
Iteration 4/25 | Loss: 0.00058276
Iteration 5/25 | Loss: 0.00058276
Iteration 6/25 | Loss: 0.00058276
Iteration 7/25 | Loss: 0.00058276
Iteration 8/25 | Loss: 0.00058276
Iteration 9/25 | Loss: 0.00058276
Iteration 10/25 | Loss: 0.00058276
Iteration 11/25 | Loss: 0.00058276
Iteration 12/25 | Loss: 0.00058276
Iteration 13/25 | Loss: 0.00058276
Iteration 14/25 | Loss: 0.00058276
Iteration 15/25 | Loss: 0.00058276
Iteration 16/25 | Loss: 0.00058276
Iteration 17/25 | Loss: 0.00058276
Iteration 18/25 | Loss: 0.00058276
Iteration 19/25 | Loss: 0.00058276
Iteration 20/25 | Loss: 0.00058276
Iteration 21/25 | Loss: 0.00058276
Iteration 22/25 | Loss: 0.00058276
Iteration 23/25 | Loss: 0.00058276
Iteration 24/25 | Loss: 0.00058276
Iteration 25/25 | Loss: 0.00058276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058276
Iteration 2/1000 | Loss: 0.00004484
Iteration 3/1000 | Loss: 0.00003363
Iteration 4/1000 | Loss: 0.00003030
Iteration 5/1000 | Loss: 0.00002870
Iteration 6/1000 | Loss: 0.00002781
Iteration 7/1000 | Loss: 0.00002682
Iteration 8/1000 | Loss: 0.00002618
Iteration 9/1000 | Loss: 0.00002578
Iteration 10/1000 | Loss: 0.00002546
Iteration 11/1000 | Loss: 0.00002520
Iteration 12/1000 | Loss: 0.00002506
Iteration 13/1000 | Loss: 0.00002505
Iteration 14/1000 | Loss: 0.00002505
Iteration 15/1000 | Loss: 0.00002503
Iteration 16/1000 | Loss: 0.00002502
Iteration 17/1000 | Loss: 0.00002502
Iteration 18/1000 | Loss: 0.00002502
Iteration 19/1000 | Loss: 0.00002502
Iteration 20/1000 | Loss: 0.00002502
Iteration 21/1000 | Loss: 0.00002502
Iteration 22/1000 | Loss: 0.00002502
Iteration 23/1000 | Loss: 0.00002502
Iteration 24/1000 | Loss: 0.00002502
Iteration 25/1000 | Loss: 0.00002502
Iteration 26/1000 | Loss: 0.00002502
Iteration 27/1000 | Loss: 0.00002501
Iteration 28/1000 | Loss: 0.00002494
Iteration 29/1000 | Loss: 0.00002493
Iteration 30/1000 | Loss: 0.00002491
Iteration 31/1000 | Loss: 0.00002491
Iteration 32/1000 | Loss: 0.00002491
Iteration 33/1000 | Loss: 0.00002491
Iteration 34/1000 | Loss: 0.00002491
Iteration 35/1000 | Loss: 0.00002491
Iteration 36/1000 | Loss: 0.00002491
Iteration 37/1000 | Loss: 0.00002491
Iteration 38/1000 | Loss: 0.00002490
Iteration 39/1000 | Loss: 0.00002490
Iteration 40/1000 | Loss: 0.00002490
Iteration 41/1000 | Loss: 0.00002490
Iteration 42/1000 | Loss: 0.00002490
Iteration 43/1000 | Loss: 0.00002490
Iteration 44/1000 | Loss: 0.00002490
Iteration 45/1000 | Loss: 0.00002490
Iteration 46/1000 | Loss: 0.00002490
Iteration 47/1000 | Loss: 0.00002490
Iteration 48/1000 | Loss: 0.00002490
Iteration 49/1000 | Loss: 0.00002488
Iteration 50/1000 | Loss: 0.00002488
Iteration 51/1000 | Loss: 0.00002488
Iteration 52/1000 | Loss: 0.00002488
Iteration 53/1000 | Loss: 0.00002488
Iteration 54/1000 | Loss: 0.00002488
Iteration 55/1000 | Loss: 0.00002488
Iteration 56/1000 | Loss: 0.00002488
Iteration 57/1000 | Loss: 0.00002488
Iteration 58/1000 | Loss: 0.00002487
Iteration 59/1000 | Loss: 0.00002486
Iteration 60/1000 | Loss: 0.00002486
Iteration 61/1000 | Loss: 0.00002486
Iteration 62/1000 | Loss: 0.00002485
Iteration 63/1000 | Loss: 0.00002485
Iteration 64/1000 | Loss: 0.00002485
Iteration 65/1000 | Loss: 0.00002485
Iteration 66/1000 | Loss: 0.00002485
Iteration 67/1000 | Loss: 0.00002485
Iteration 68/1000 | Loss: 0.00002485
Iteration 69/1000 | Loss: 0.00002485
Iteration 70/1000 | Loss: 0.00002484
Iteration 71/1000 | Loss: 0.00002484
Iteration 72/1000 | Loss: 0.00002484
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00002484
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002484
Iteration 77/1000 | Loss: 0.00002484
Iteration 78/1000 | Loss: 0.00002484
Iteration 79/1000 | Loss: 0.00002484
Iteration 80/1000 | Loss: 0.00002484
Iteration 81/1000 | Loss: 0.00002484
Iteration 82/1000 | Loss: 0.00002484
Iteration 83/1000 | Loss: 0.00002484
Iteration 84/1000 | Loss: 0.00002484
Iteration 85/1000 | Loss: 0.00002484
Iteration 86/1000 | Loss: 0.00002484
Iteration 87/1000 | Loss: 0.00002484
Iteration 88/1000 | Loss: 0.00002484
Iteration 89/1000 | Loss: 0.00002484
Iteration 90/1000 | Loss: 0.00002484
Iteration 91/1000 | Loss: 0.00002484
Iteration 92/1000 | Loss: 0.00002484
Iteration 93/1000 | Loss: 0.00002484
Iteration 94/1000 | Loss: 0.00002484
Iteration 95/1000 | Loss: 0.00002484
Iteration 96/1000 | Loss: 0.00002484
Iteration 97/1000 | Loss: 0.00002484
Iteration 98/1000 | Loss: 0.00002484
Iteration 99/1000 | Loss: 0.00002484
Iteration 100/1000 | Loss: 0.00002484
Iteration 101/1000 | Loss: 0.00002484
Iteration 102/1000 | Loss: 0.00002484
Iteration 103/1000 | Loss: 0.00002484
Iteration 104/1000 | Loss: 0.00002484
Iteration 105/1000 | Loss: 0.00002484
Iteration 106/1000 | Loss: 0.00002484
Iteration 107/1000 | Loss: 0.00002484
Iteration 108/1000 | Loss: 0.00002484
Iteration 109/1000 | Loss: 0.00002484
Iteration 110/1000 | Loss: 0.00002484
Iteration 111/1000 | Loss: 0.00002484
Iteration 112/1000 | Loss: 0.00002484
Iteration 113/1000 | Loss: 0.00002484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.484222932253033e-05, 2.484222932253033e-05, 2.484222932253033e-05, 2.484222932253033e-05, 2.484222932253033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.484222932253033e-05

Optimization complete. Final v2v error: 4.2386155128479 mm

Highest mean error: 4.541660785675049 mm for frame 29

Lowest mean error: 4.112698554992676 mm for frame 13

Saving results

Total time: 31.179267406463623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423873
Iteration 2/25 | Loss: 0.00110294
Iteration 3/25 | Loss: 0.00096533
Iteration 4/25 | Loss: 0.00095609
Iteration 5/25 | Loss: 0.00095419
Iteration 6/25 | Loss: 0.00095401
Iteration 7/25 | Loss: 0.00095401
Iteration 8/25 | Loss: 0.00095401
Iteration 9/25 | Loss: 0.00095401
Iteration 10/25 | Loss: 0.00095401
Iteration 11/25 | Loss: 0.00095401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009540097671560943, 0.0009540097671560943, 0.0009540097671560943, 0.0009540097671560943, 0.0009540097671560943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009540097671560943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38896310
Iteration 2/25 | Loss: 0.00079732
Iteration 3/25 | Loss: 0.00079731
Iteration 4/25 | Loss: 0.00079731
Iteration 5/25 | Loss: 0.00079731
Iteration 6/25 | Loss: 0.00079731
Iteration 7/25 | Loss: 0.00079731
Iteration 8/25 | Loss: 0.00079731
Iteration 9/25 | Loss: 0.00079731
Iteration 10/25 | Loss: 0.00079731
Iteration 11/25 | Loss: 0.00079731
Iteration 12/25 | Loss: 0.00079731
Iteration 13/25 | Loss: 0.00079731
Iteration 14/25 | Loss: 0.00079731
Iteration 15/25 | Loss: 0.00079731
Iteration 16/25 | Loss: 0.00079731
Iteration 17/25 | Loss: 0.00079731
Iteration 18/25 | Loss: 0.00079731
Iteration 19/25 | Loss: 0.00079731
Iteration 20/25 | Loss: 0.00079731
Iteration 21/25 | Loss: 0.00079731
Iteration 22/25 | Loss: 0.00079731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007973082829266787, 0.0007973082829266787, 0.0007973082829266787, 0.0007973082829266787, 0.0007973082829266787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007973082829266787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079731
Iteration 2/1000 | Loss: 0.00002471
Iteration 3/1000 | Loss: 0.00001653
Iteration 4/1000 | Loss: 0.00001457
Iteration 5/1000 | Loss: 0.00001351
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001207
Iteration 11/1000 | Loss: 0.00001193
Iteration 12/1000 | Loss: 0.00001187
Iteration 13/1000 | Loss: 0.00001187
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001177
Iteration 16/1000 | Loss: 0.00001177
Iteration 17/1000 | Loss: 0.00001177
Iteration 18/1000 | Loss: 0.00001174
Iteration 19/1000 | Loss: 0.00001173
Iteration 20/1000 | Loss: 0.00001172
Iteration 21/1000 | Loss: 0.00001171
Iteration 22/1000 | Loss: 0.00001170
Iteration 23/1000 | Loss: 0.00001169
Iteration 24/1000 | Loss: 0.00001169
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001167
Iteration 27/1000 | Loss: 0.00001166
Iteration 28/1000 | Loss: 0.00001166
Iteration 29/1000 | Loss: 0.00001165
Iteration 30/1000 | Loss: 0.00001165
Iteration 31/1000 | Loss: 0.00001164
Iteration 32/1000 | Loss: 0.00001164
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001161
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001160
Iteration 41/1000 | Loss: 0.00001160
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001155
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001153
Iteration 63/1000 | Loss: 0.00001153
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001152
Iteration 67/1000 | Loss: 0.00001151
Iteration 68/1000 | Loss: 0.00001151
Iteration 69/1000 | Loss: 0.00001151
Iteration 70/1000 | Loss: 0.00001151
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001150
Iteration 73/1000 | Loss: 0.00001150
Iteration 74/1000 | Loss: 0.00001150
Iteration 75/1000 | Loss: 0.00001150
Iteration 76/1000 | Loss: 0.00001150
Iteration 77/1000 | Loss: 0.00001149
Iteration 78/1000 | Loss: 0.00001149
Iteration 79/1000 | Loss: 0.00001149
Iteration 80/1000 | Loss: 0.00001149
Iteration 81/1000 | Loss: 0.00001149
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001149
Iteration 89/1000 | Loss: 0.00001149
Iteration 90/1000 | Loss: 0.00001149
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001148
Iteration 101/1000 | Loss: 0.00001148
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001148
Iteration 107/1000 | Loss: 0.00001148
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001147
Iteration 110/1000 | Loss: 0.00001147
Iteration 111/1000 | Loss: 0.00001147
Iteration 112/1000 | Loss: 0.00001147
Iteration 113/1000 | Loss: 0.00001147
Iteration 114/1000 | Loss: 0.00001147
Iteration 115/1000 | Loss: 0.00001147
Iteration 116/1000 | Loss: 0.00001147
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001147
Iteration 125/1000 | Loss: 0.00001147
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001147
Iteration 128/1000 | Loss: 0.00001147
Iteration 129/1000 | Loss: 0.00001147
Iteration 130/1000 | Loss: 0.00001147
Iteration 131/1000 | Loss: 0.00001147
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001147
Iteration 135/1000 | Loss: 0.00001147
Iteration 136/1000 | Loss: 0.00001147
Iteration 137/1000 | Loss: 0.00001147
Iteration 138/1000 | Loss: 0.00001147
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001147
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001147
Iteration 147/1000 | Loss: 0.00001147
Iteration 148/1000 | Loss: 0.00001147
Iteration 149/1000 | Loss: 0.00001147
Iteration 150/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.1466517207736615e-05, 1.1466517207736615e-05, 1.1466517207736615e-05, 1.1466517207736615e-05, 1.1466517207736615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1466517207736615e-05

Optimization complete. Final v2v error: 2.815260887145996 mm

Highest mean error: 3.717761754989624 mm for frame 61

Lowest mean error: 2.512725830078125 mm for frame 0

Saving results

Total time: 36.69969201087952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911552
Iteration 2/25 | Loss: 0.00121029
Iteration 3/25 | Loss: 0.00105106
Iteration 4/25 | Loss: 0.00102529
Iteration 5/25 | Loss: 0.00101688
Iteration 6/25 | Loss: 0.00101398
Iteration 7/25 | Loss: 0.00101358
Iteration 8/25 | Loss: 0.00101358
Iteration 9/25 | Loss: 0.00101358
Iteration 10/25 | Loss: 0.00101358
Iteration 11/25 | Loss: 0.00101358
Iteration 12/25 | Loss: 0.00101358
Iteration 13/25 | Loss: 0.00101358
Iteration 14/25 | Loss: 0.00101358
Iteration 15/25 | Loss: 0.00101358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010135835036635399, 0.0010135835036635399, 0.0010135835036635399, 0.0010135835036635399, 0.0010135835036635399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010135835036635399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41541398
Iteration 2/25 | Loss: 0.00082305
Iteration 3/25 | Loss: 0.00082302
Iteration 4/25 | Loss: 0.00082302
Iteration 5/25 | Loss: 0.00082302
Iteration 6/25 | Loss: 0.00082302
Iteration 7/25 | Loss: 0.00082302
Iteration 8/25 | Loss: 0.00082302
Iteration 9/25 | Loss: 0.00082302
Iteration 10/25 | Loss: 0.00082302
Iteration 11/25 | Loss: 0.00082302
Iteration 12/25 | Loss: 0.00082302
Iteration 13/25 | Loss: 0.00082302
Iteration 14/25 | Loss: 0.00082302
Iteration 15/25 | Loss: 0.00082302
Iteration 16/25 | Loss: 0.00082302
Iteration 17/25 | Loss: 0.00082302
Iteration 18/25 | Loss: 0.00082302
Iteration 19/25 | Loss: 0.00082302
Iteration 20/25 | Loss: 0.00082302
Iteration 21/25 | Loss: 0.00082302
Iteration 22/25 | Loss: 0.00082302
Iteration 23/25 | Loss: 0.00082302
Iteration 24/25 | Loss: 0.00082302
Iteration 25/25 | Loss: 0.00082302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082302
Iteration 2/1000 | Loss: 0.00004142
Iteration 3/1000 | Loss: 0.00002684
Iteration 4/1000 | Loss: 0.00002412
Iteration 5/1000 | Loss: 0.00002226
Iteration 6/1000 | Loss: 0.00002104
Iteration 7/1000 | Loss: 0.00002035
Iteration 8/1000 | Loss: 0.00001979
Iteration 9/1000 | Loss: 0.00001927
Iteration 10/1000 | Loss: 0.00001889
Iteration 11/1000 | Loss: 0.00001872
Iteration 12/1000 | Loss: 0.00001854
Iteration 13/1000 | Loss: 0.00001852
Iteration 14/1000 | Loss: 0.00001848
Iteration 15/1000 | Loss: 0.00001841
Iteration 16/1000 | Loss: 0.00001839
Iteration 17/1000 | Loss: 0.00001838
Iteration 18/1000 | Loss: 0.00001834
Iteration 19/1000 | Loss: 0.00001829
Iteration 20/1000 | Loss: 0.00001826
Iteration 21/1000 | Loss: 0.00001826
Iteration 22/1000 | Loss: 0.00001826
Iteration 23/1000 | Loss: 0.00001826
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001825
Iteration 26/1000 | Loss: 0.00001825
Iteration 27/1000 | Loss: 0.00001825
Iteration 28/1000 | Loss: 0.00001825
Iteration 29/1000 | Loss: 0.00001824
Iteration 30/1000 | Loss: 0.00001824
Iteration 31/1000 | Loss: 0.00001823
Iteration 32/1000 | Loss: 0.00001823
Iteration 33/1000 | Loss: 0.00001822
Iteration 34/1000 | Loss: 0.00001822
Iteration 35/1000 | Loss: 0.00001822
Iteration 36/1000 | Loss: 0.00001822
Iteration 37/1000 | Loss: 0.00001821
Iteration 38/1000 | Loss: 0.00001821
Iteration 39/1000 | Loss: 0.00001821
Iteration 40/1000 | Loss: 0.00001820
Iteration 41/1000 | Loss: 0.00001820
Iteration 42/1000 | Loss: 0.00001820
Iteration 43/1000 | Loss: 0.00001820
Iteration 44/1000 | Loss: 0.00001820
Iteration 45/1000 | Loss: 0.00001820
Iteration 46/1000 | Loss: 0.00001820
Iteration 47/1000 | Loss: 0.00001820
Iteration 48/1000 | Loss: 0.00001820
Iteration 49/1000 | Loss: 0.00001820
Iteration 50/1000 | Loss: 0.00001820
Iteration 51/1000 | Loss: 0.00001820
Iteration 52/1000 | Loss: 0.00001819
Iteration 53/1000 | Loss: 0.00001819
Iteration 54/1000 | Loss: 0.00001819
Iteration 55/1000 | Loss: 0.00001819
Iteration 56/1000 | Loss: 0.00001819
Iteration 57/1000 | Loss: 0.00001819
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001819
Iteration 60/1000 | Loss: 0.00001819
Iteration 61/1000 | Loss: 0.00001819
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001818
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001818
Iteration 74/1000 | Loss: 0.00001818
Iteration 75/1000 | Loss: 0.00001818
Iteration 76/1000 | Loss: 0.00001818
Iteration 77/1000 | Loss: 0.00001818
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001818
Iteration 80/1000 | Loss: 0.00001818
Iteration 81/1000 | Loss: 0.00001818
Iteration 82/1000 | Loss: 0.00001818
Iteration 83/1000 | Loss: 0.00001818
Iteration 84/1000 | Loss: 0.00001818
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001818
Iteration 87/1000 | Loss: 0.00001818
Iteration 88/1000 | Loss: 0.00001818
Iteration 89/1000 | Loss: 0.00001818
Iteration 90/1000 | Loss: 0.00001818
Iteration 91/1000 | Loss: 0.00001818
Iteration 92/1000 | Loss: 0.00001818
Iteration 93/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.8177044694311917e-05, 1.8177044694311917e-05, 1.8177044694311917e-05, 1.8177044694311917e-05, 1.8177044694311917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8177044694311917e-05

Optimization complete. Final v2v error: 3.5268912315368652 mm

Highest mean error: 5.096114158630371 mm for frame 69

Lowest mean error: 2.7395641803741455 mm for frame 0

Saving results

Total time: 33.079108476638794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871454
Iteration 2/25 | Loss: 0.00128317
Iteration 3/25 | Loss: 0.00105150
Iteration 4/25 | Loss: 0.00096401
Iteration 5/25 | Loss: 0.00095843
Iteration 6/25 | Loss: 0.00095657
Iteration 7/25 | Loss: 0.00095520
Iteration 8/25 | Loss: 0.00095561
Iteration 9/25 | Loss: 0.00095503
Iteration 10/25 | Loss: 0.00095503
Iteration 11/25 | Loss: 0.00095503
Iteration 12/25 | Loss: 0.00095503
Iteration 13/25 | Loss: 0.00095502
Iteration 14/25 | Loss: 0.00095502
Iteration 15/25 | Loss: 0.00095502
Iteration 16/25 | Loss: 0.00095502
Iteration 17/25 | Loss: 0.00095502
Iteration 18/25 | Loss: 0.00095502
Iteration 19/25 | Loss: 0.00095502
Iteration 20/25 | Loss: 0.00095502
Iteration 21/25 | Loss: 0.00095502
Iteration 22/25 | Loss: 0.00095502
Iteration 23/25 | Loss: 0.00095502
Iteration 24/25 | Loss: 0.00095502
Iteration 25/25 | Loss: 0.00095501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75248885
Iteration 2/25 | Loss: 0.00087701
Iteration 3/25 | Loss: 0.00087701
Iteration 4/25 | Loss: 0.00087701
Iteration 5/25 | Loss: 0.00087701
Iteration 6/25 | Loss: 0.00087701
Iteration 7/25 | Loss: 0.00087701
Iteration 8/25 | Loss: 0.00087700
Iteration 9/25 | Loss: 0.00087516
Iteration 10/25 | Loss: 0.00087516
Iteration 11/25 | Loss: 0.00087516
Iteration 12/25 | Loss: 0.00087516
Iteration 13/25 | Loss: 0.00087516
Iteration 14/25 | Loss: 0.00087516
Iteration 15/25 | Loss: 0.00087516
Iteration 16/25 | Loss: 0.00087516
Iteration 17/25 | Loss: 0.00087516
Iteration 18/25 | Loss: 0.00087516
Iteration 19/25 | Loss: 0.00087516
Iteration 20/25 | Loss: 0.00087516
Iteration 21/25 | Loss: 0.00087516
Iteration 22/25 | Loss: 0.00087516
Iteration 23/25 | Loss: 0.00087516
Iteration 24/25 | Loss: 0.00087516
Iteration 25/25 | Loss: 0.00087516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087516
Iteration 2/1000 | Loss: 0.00002418
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00002365
Iteration 5/1000 | Loss: 0.00001759
Iteration 6/1000 | Loss: 0.00001758
Iteration 7/1000 | Loss: 0.00001249
Iteration 8/1000 | Loss: 0.00001217
Iteration 9/1000 | Loss: 0.00001211
Iteration 10/1000 | Loss: 0.00001197
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001179
Iteration 15/1000 | Loss: 0.00001179
Iteration 16/1000 | Loss: 0.00001179
Iteration 17/1000 | Loss: 0.00001179
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001179
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00001179
Iteration 22/1000 | Loss: 0.00001179
Iteration 23/1000 | Loss: 0.00001178
Iteration 24/1000 | Loss: 0.00001178
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001177
Iteration 27/1000 | Loss: 0.00001175
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001174
Iteration 30/1000 | Loss: 0.00001174
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001171
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001171
Iteration 38/1000 | Loss: 0.00001171
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001369
Iteration 54/1000 | Loss: 0.00001368
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00001160
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001160
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001160
Iteration 70/1000 | Loss: 0.00001160
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001159
Iteration 74/1000 | Loss: 0.00001159
Iteration 75/1000 | Loss: 0.00001159
Iteration 76/1000 | Loss: 0.00001159
Iteration 77/1000 | Loss: 0.00001159
Iteration 78/1000 | Loss: 0.00001159
Iteration 79/1000 | Loss: 0.00001159
Iteration 80/1000 | Loss: 0.00001159
Iteration 81/1000 | Loss: 0.00001158
Iteration 82/1000 | Loss: 0.00001158
Iteration 83/1000 | Loss: 0.00001158
Iteration 84/1000 | Loss: 0.00001158
Iteration 85/1000 | Loss: 0.00001158
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001179
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001155
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.1554819138837047e-05, 1.1554819138837047e-05, 1.1554819138837047e-05, 1.1554819138837047e-05, 1.1554819138837047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1554819138837047e-05

Optimization complete. Final v2v error: 2.879817485809326 mm

Highest mean error: 3.249386787414551 mm for frame 205

Lowest mean error: 2.4842593669891357 mm for frame 223

Saving results

Total time: 50.72608971595764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974246
Iteration 2/25 | Loss: 0.00143161
Iteration 3/25 | Loss: 0.00126339
Iteration 4/25 | Loss: 0.00109212
Iteration 5/25 | Loss: 0.00104714
Iteration 6/25 | Loss: 0.00101959
Iteration 7/25 | Loss: 0.00101271
Iteration 8/25 | Loss: 0.00100721
Iteration 9/25 | Loss: 0.00100110
Iteration 10/25 | Loss: 0.00100016
Iteration 11/25 | Loss: 0.00099966
Iteration 12/25 | Loss: 0.00099914
Iteration 13/25 | Loss: 0.00099900
Iteration 14/25 | Loss: 0.00099900
Iteration 15/25 | Loss: 0.00099900
Iteration 16/25 | Loss: 0.00099900
Iteration 17/25 | Loss: 0.00099900
Iteration 18/25 | Loss: 0.00099900
Iteration 19/25 | Loss: 0.00099900
Iteration 20/25 | Loss: 0.00099900
Iteration 21/25 | Loss: 0.00099900
Iteration 22/25 | Loss: 0.00099900
Iteration 23/25 | Loss: 0.00099900
Iteration 24/25 | Loss: 0.00099900
Iteration 25/25 | Loss: 0.00099900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51760566
Iteration 2/25 | Loss: 0.00084525
Iteration 3/25 | Loss: 0.00080979
Iteration 4/25 | Loss: 0.00080979
Iteration 5/25 | Loss: 0.00080979
Iteration 6/25 | Loss: 0.00080979
Iteration 7/25 | Loss: 0.00080979
Iteration 8/25 | Loss: 0.00080979
Iteration 9/25 | Loss: 0.00080979
Iteration 10/25 | Loss: 0.00080979
Iteration 11/25 | Loss: 0.00080979
Iteration 12/25 | Loss: 0.00080979
Iteration 13/25 | Loss: 0.00080979
Iteration 14/25 | Loss: 0.00080979
Iteration 15/25 | Loss: 0.00080979
Iteration 16/25 | Loss: 0.00080979
Iteration 17/25 | Loss: 0.00080979
Iteration 18/25 | Loss: 0.00080979
Iteration 19/25 | Loss: 0.00080979
Iteration 20/25 | Loss: 0.00080979
Iteration 21/25 | Loss: 0.00080979
Iteration 22/25 | Loss: 0.00080979
Iteration 23/25 | Loss: 0.00080979
Iteration 24/25 | Loss: 0.00080979
Iteration 25/25 | Loss: 0.00080979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080979
Iteration 2/1000 | Loss: 0.00006900
Iteration 3/1000 | Loss: 0.00009764
Iteration 4/1000 | Loss: 0.00002122
Iteration 5/1000 | Loss: 0.00001886
Iteration 6/1000 | Loss: 0.00001765
Iteration 7/1000 | Loss: 0.00005442
Iteration 8/1000 | Loss: 0.00011742
Iteration 9/1000 | Loss: 0.00004036
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00005863
Iteration 12/1000 | Loss: 0.00001546
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001498
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001478
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001467
Iteration 35/1000 | Loss: 0.00001467
Iteration 36/1000 | Loss: 0.00001467
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001466
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001465
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001463
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001462
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001460
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001459
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001459
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001458
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001456
Iteration 125/1000 | Loss: 0.00001456
Iteration 126/1000 | Loss: 0.00001456
Iteration 127/1000 | Loss: 0.00001456
Iteration 128/1000 | Loss: 0.00001456
Iteration 129/1000 | Loss: 0.00001456
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001455
Iteration 134/1000 | Loss: 0.00001455
Iteration 135/1000 | Loss: 0.00001455
Iteration 136/1000 | Loss: 0.00001455
Iteration 137/1000 | Loss: 0.00001455
Iteration 138/1000 | Loss: 0.00001455
Iteration 139/1000 | Loss: 0.00001455
Iteration 140/1000 | Loss: 0.00001455
Iteration 141/1000 | Loss: 0.00001455
Iteration 142/1000 | Loss: 0.00001455
Iteration 143/1000 | Loss: 0.00001455
Iteration 144/1000 | Loss: 0.00001455
Iteration 145/1000 | Loss: 0.00001455
Iteration 146/1000 | Loss: 0.00001454
Iteration 147/1000 | Loss: 0.00001454
Iteration 148/1000 | Loss: 0.00001454
Iteration 149/1000 | Loss: 0.00001454
Iteration 150/1000 | Loss: 0.00001454
Iteration 151/1000 | Loss: 0.00001454
Iteration 152/1000 | Loss: 0.00001454
Iteration 153/1000 | Loss: 0.00001454
Iteration 154/1000 | Loss: 0.00001454
Iteration 155/1000 | Loss: 0.00001454
Iteration 156/1000 | Loss: 0.00001454
Iteration 157/1000 | Loss: 0.00001453
Iteration 158/1000 | Loss: 0.00001453
Iteration 159/1000 | Loss: 0.00001453
Iteration 160/1000 | Loss: 0.00001453
Iteration 161/1000 | Loss: 0.00001453
Iteration 162/1000 | Loss: 0.00001453
Iteration 163/1000 | Loss: 0.00001453
Iteration 164/1000 | Loss: 0.00001453
Iteration 165/1000 | Loss: 0.00001453
Iteration 166/1000 | Loss: 0.00001453
Iteration 167/1000 | Loss: 0.00001453
Iteration 168/1000 | Loss: 0.00001453
Iteration 169/1000 | Loss: 0.00001452
Iteration 170/1000 | Loss: 0.00001452
Iteration 171/1000 | Loss: 0.00001452
Iteration 172/1000 | Loss: 0.00001452
Iteration 173/1000 | Loss: 0.00001452
Iteration 174/1000 | Loss: 0.00001452
Iteration 175/1000 | Loss: 0.00001452
Iteration 176/1000 | Loss: 0.00001452
Iteration 177/1000 | Loss: 0.00001452
Iteration 178/1000 | Loss: 0.00001452
Iteration 179/1000 | Loss: 0.00001452
Iteration 180/1000 | Loss: 0.00001452
Iteration 181/1000 | Loss: 0.00001452
Iteration 182/1000 | Loss: 0.00001452
Iteration 183/1000 | Loss: 0.00001452
Iteration 184/1000 | Loss: 0.00001452
Iteration 185/1000 | Loss: 0.00001452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.4516809642373119e-05, 1.4516809642373119e-05, 1.4516809642373119e-05, 1.4516809642373119e-05, 1.4516809642373119e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4516809642373119e-05

Optimization complete. Final v2v error: 3.2212681770324707 mm

Highest mean error: 3.8025870323181152 mm for frame 34

Lowest mean error: 2.7891592979431152 mm for frame 92

Saving results

Total time: 59.82946991920471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391119
Iteration 2/25 | Loss: 0.00109707
Iteration 3/25 | Loss: 0.00095445
Iteration 4/25 | Loss: 0.00094472
Iteration 5/25 | Loss: 0.00094342
Iteration 6/25 | Loss: 0.00094315
Iteration 7/25 | Loss: 0.00094315
Iteration 8/25 | Loss: 0.00094315
Iteration 9/25 | Loss: 0.00094315
Iteration 10/25 | Loss: 0.00094315
Iteration 11/25 | Loss: 0.00094315
Iteration 12/25 | Loss: 0.00094315
Iteration 13/25 | Loss: 0.00094315
Iteration 14/25 | Loss: 0.00094315
Iteration 15/25 | Loss: 0.00094315
Iteration 16/25 | Loss: 0.00094315
Iteration 17/25 | Loss: 0.00094315
Iteration 18/25 | Loss: 0.00094315
Iteration 19/25 | Loss: 0.00094315
Iteration 20/25 | Loss: 0.00094315
Iteration 21/25 | Loss: 0.00094315
Iteration 22/25 | Loss: 0.00094315
Iteration 23/25 | Loss: 0.00094315
Iteration 24/25 | Loss: 0.00094315
Iteration 25/25 | Loss: 0.00094315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38921845
Iteration 2/25 | Loss: 0.00063173
Iteration 3/25 | Loss: 0.00063173
Iteration 4/25 | Loss: 0.00063173
Iteration 5/25 | Loss: 0.00063173
Iteration 6/25 | Loss: 0.00063173
Iteration 7/25 | Loss: 0.00063173
Iteration 8/25 | Loss: 0.00063173
Iteration 9/25 | Loss: 0.00063173
Iteration 10/25 | Loss: 0.00063173
Iteration 11/25 | Loss: 0.00063173
Iteration 12/25 | Loss: 0.00063173
Iteration 13/25 | Loss: 0.00063173
Iteration 14/25 | Loss: 0.00063173
Iteration 15/25 | Loss: 0.00063173
Iteration 16/25 | Loss: 0.00063173
Iteration 17/25 | Loss: 0.00063173
Iteration 18/25 | Loss: 0.00063173
Iteration 19/25 | Loss: 0.00063173
Iteration 20/25 | Loss: 0.00063173
Iteration 21/25 | Loss: 0.00063173
Iteration 22/25 | Loss: 0.00063173
Iteration 23/25 | Loss: 0.00063173
Iteration 24/25 | Loss: 0.00063173
Iteration 25/25 | Loss: 0.00063173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063173
Iteration 2/1000 | Loss: 0.00002006
Iteration 3/1000 | Loss: 0.00001514
Iteration 4/1000 | Loss: 0.00001338
Iteration 5/1000 | Loss: 0.00001255
Iteration 6/1000 | Loss: 0.00001204
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001138
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001110
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001108
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001107
Iteration 16/1000 | Loss: 0.00001107
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001106
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001106
Iteration 21/1000 | Loss: 0.00001106
Iteration 22/1000 | Loss: 0.00001105
Iteration 23/1000 | Loss: 0.00001105
Iteration 24/1000 | Loss: 0.00001104
Iteration 25/1000 | Loss: 0.00001104
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001102
Iteration 28/1000 | Loss: 0.00001101
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001086
Iteration 32/1000 | Loss: 0.00001086
Iteration 33/1000 | Loss: 0.00001085
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001082
Iteration 37/1000 | Loss: 0.00001082
Iteration 38/1000 | Loss: 0.00001081
Iteration 39/1000 | Loss: 0.00001080
Iteration 40/1000 | Loss: 0.00001080
Iteration 41/1000 | Loss: 0.00001080
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001079
Iteration 45/1000 | Loss: 0.00001079
Iteration 46/1000 | Loss: 0.00001079
Iteration 47/1000 | Loss: 0.00001079
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001078
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001078
Iteration 59/1000 | Loss: 0.00001078
Iteration 60/1000 | Loss: 0.00001078
Iteration 61/1000 | Loss: 0.00001078
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001078
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001076
Iteration 68/1000 | Loss: 0.00001076
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001076
Iteration 72/1000 | Loss: 0.00001076
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001074
Iteration 79/1000 | Loss: 0.00001074
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001074
Iteration 83/1000 | Loss: 0.00001074
Iteration 84/1000 | Loss: 0.00001074
Iteration 85/1000 | Loss: 0.00001074
Iteration 86/1000 | Loss: 0.00001073
Iteration 87/1000 | Loss: 0.00001073
Iteration 88/1000 | Loss: 0.00001073
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001072
Iteration 92/1000 | Loss: 0.00001072
Iteration 93/1000 | Loss: 0.00001072
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001069
Iteration 106/1000 | Loss: 0.00001069
Iteration 107/1000 | Loss: 0.00001069
Iteration 108/1000 | Loss: 0.00001069
Iteration 109/1000 | Loss: 0.00001069
Iteration 110/1000 | Loss: 0.00001069
Iteration 111/1000 | Loss: 0.00001069
Iteration 112/1000 | Loss: 0.00001069
Iteration 113/1000 | Loss: 0.00001068
Iteration 114/1000 | Loss: 0.00001068
Iteration 115/1000 | Loss: 0.00001068
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001067
Iteration 118/1000 | Loss: 0.00001067
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001067
Iteration 121/1000 | Loss: 0.00001067
Iteration 122/1000 | Loss: 0.00001067
Iteration 123/1000 | Loss: 0.00001067
Iteration 124/1000 | Loss: 0.00001067
Iteration 125/1000 | Loss: 0.00001067
Iteration 126/1000 | Loss: 0.00001067
Iteration 127/1000 | Loss: 0.00001067
Iteration 128/1000 | Loss: 0.00001067
Iteration 129/1000 | Loss: 0.00001067
Iteration 130/1000 | Loss: 0.00001067
Iteration 131/1000 | Loss: 0.00001067
Iteration 132/1000 | Loss: 0.00001067
Iteration 133/1000 | Loss: 0.00001067
Iteration 134/1000 | Loss: 0.00001067
Iteration 135/1000 | Loss: 0.00001067
Iteration 136/1000 | Loss: 0.00001067
Iteration 137/1000 | Loss: 0.00001067
Iteration 138/1000 | Loss: 0.00001067
Iteration 139/1000 | Loss: 0.00001067
Iteration 140/1000 | Loss: 0.00001067
Iteration 141/1000 | Loss: 0.00001067
Iteration 142/1000 | Loss: 0.00001067
Iteration 143/1000 | Loss: 0.00001067
Iteration 144/1000 | Loss: 0.00001067
Iteration 145/1000 | Loss: 0.00001067
Iteration 146/1000 | Loss: 0.00001067
Iteration 147/1000 | Loss: 0.00001067
Iteration 148/1000 | Loss: 0.00001067
Iteration 149/1000 | Loss: 0.00001067
Iteration 150/1000 | Loss: 0.00001067
Iteration 151/1000 | Loss: 0.00001067
Iteration 152/1000 | Loss: 0.00001067
Iteration 153/1000 | Loss: 0.00001067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.066630102286581e-05, 1.066630102286581e-05, 1.066630102286581e-05, 1.066630102286581e-05, 1.066630102286581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.066630102286581e-05

Optimization complete. Final v2v error: 2.7767019271850586 mm

Highest mean error: 3.248349666595459 mm for frame 75

Lowest mean error: 2.3747663497924805 mm for frame 2

Saving results

Total time: 33.3140664100647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899862
Iteration 2/25 | Loss: 0.00108602
Iteration 3/25 | Loss: 0.00099169
Iteration 4/25 | Loss: 0.00098309
Iteration 5/25 | Loss: 0.00097985
Iteration 6/25 | Loss: 0.00097959
Iteration 7/25 | Loss: 0.00097959
Iteration 8/25 | Loss: 0.00097959
Iteration 9/25 | Loss: 0.00097959
Iteration 10/25 | Loss: 0.00097959
Iteration 11/25 | Loss: 0.00097959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009795858059078455, 0.0009795858059078455, 0.0009795858059078455, 0.0009795858059078455, 0.0009795858059078455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009795858059078455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35064483
Iteration 2/25 | Loss: 0.00073505
Iteration 3/25 | Loss: 0.00073496
Iteration 4/25 | Loss: 0.00073496
Iteration 5/25 | Loss: 0.00073496
Iteration 6/25 | Loss: 0.00073496
Iteration 7/25 | Loss: 0.00073496
Iteration 8/25 | Loss: 0.00073496
Iteration 9/25 | Loss: 0.00073496
Iteration 10/25 | Loss: 0.00073496
Iteration 11/25 | Loss: 0.00073496
Iteration 12/25 | Loss: 0.00073496
Iteration 13/25 | Loss: 0.00073496
Iteration 14/25 | Loss: 0.00073496
Iteration 15/25 | Loss: 0.00073496
Iteration 16/25 | Loss: 0.00073496
Iteration 17/25 | Loss: 0.00073496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007349615916609764, 0.0007349615916609764, 0.0007349615916609764, 0.0007349615916609764, 0.0007349615916609764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007349615916609764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073496
Iteration 2/1000 | Loss: 0.00002070
Iteration 3/1000 | Loss: 0.00001519
Iteration 4/1000 | Loss: 0.00001354
Iteration 5/1000 | Loss: 0.00001267
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001185
Iteration 9/1000 | Loss: 0.00001182
Iteration 10/1000 | Loss: 0.00001178
Iteration 11/1000 | Loss: 0.00001175
Iteration 12/1000 | Loss: 0.00001175
Iteration 13/1000 | Loss: 0.00001162
Iteration 14/1000 | Loss: 0.00001156
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001149
Iteration 19/1000 | Loss: 0.00001149
Iteration 20/1000 | Loss: 0.00001147
Iteration 21/1000 | Loss: 0.00001141
Iteration 22/1000 | Loss: 0.00001141
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001137
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001137
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001137
Iteration 31/1000 | Loss: 0.00001137
Iteration 32/1000 | Loss: 0.00001137
Iteration 33/1000 | Loss: 0.00001137
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001136
Iteration 36/1000 | Loss: 0.00001136
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001135
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001134
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001130
Iteration 71/1000 | Loss: 0.00001130
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001129
Iteration 76/1000 | Loss: 0.00001129
Iteration 77/1000 | Loss: 0.00001129
Iteration 78/1000 | Loss: 0.00001129
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001127
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001126
Iteration 91/1000 | Loss: 0.00001126
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001126
Iteration 111/1000 | Loss: 0.00001126
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001125
Iteration 124/1000 | Loss: 0.00001125
Iteration 125/1000 | Loss: 0.00001125
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001124
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001124
Iteration 137/1000 | Loss: 0.00001124
Iteration 138/1000 | Loss: 0.00001124
Iteration 139/1000 | Loss: 0.00001124
Iteration 140/1000 | Loss: 0.00001124
Iteration 141/1000 | Loss: 0.00001124
Iteration 142/1000 | Loss: 0.00001124
Iteration 143/1000 | Loss: 0.00001124
Iteration 144/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1241124411753844e-05, 1.1241124411753844e-05, 1.1241124411753844e-05, 1.1241124411753844e-05, 1.1241124411753844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1241124411753844e-05

Optimization complete. Final v2v error: 2.8721585273742676 mm

Highest mean error: 3.2691516876220703 mm for frame 64

Lowest mean error: 2.495863914489746 mm for frame 3

Saving results

Total time: 36.089860916137695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837411
Iteration 2/25 | Loss: 0.00105207
Iteration 3/25 | Loss: 0.00096843
Iteration 4/25 | Loss: 0.00096062
Iteration 5/25 | Loss: 0.00095820
Iteration 6/25 | Loss: 0.00095734
Iteration 7/25 | Loss: 0.00095734
Iteration 8/25 | Loss: 0.00095734
Iteration 9/25 | Loss: 0.00095734
Iteration 10/25 | Loss: 0.00095734
Iteration 11/25 | Loss: 0.00095734
Iteration 12/25 | Loss: 0.00095734
Iteration 13/25 | Loss: 0.00095734
Iteration 14/25 | Loss: 0.00095734
Iteration 15/25 | Loss: 0.00095734
Iteration 16/25 | Loss: 0.00095734
Iteration 17/25 | Loss: 0.00095734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009573358693160117, 0.0009573358693160117, 0.0009573358693160117, 0.0009573358693160117, 0.0009573358693160117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009573358693160117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41366839
Iteration 2/25 | Loss: 0.00086568
Iteration 3/25 | Loss: 0.00086568
Iteration 4/25 | Loss: 0.00086568
Iteration 5/25 | Loss: 0.00086568
Iteration 6/25 | Loss: 0.00086568
Iteration 7/25 | Loss: 0.00086568
Iteration 8/25 | Loss: 0.00086568
Iteration 9/25 | Loss: 0.00086568
Iteration 10/25 | Loss: 0.00086568
Iteration 11/25 | Loss: 0.00086568
Iteration 12/25 | Loss: 0.00086568
Iteration 13/25 | Loss: 0.00086568
Iteration 14/25 | Loss: 0.00086568
Iteration 15/25 | Loss: 0.00086568
Iteration 16/25 | Loss: 0.00086568
Iteration 17/25 | Loss: 0.00086568
Iteration 18/25 | Loss: 0.00086568
Iteration 19/25 | Loss: 0.00086568
Iteration 20/25 | Loss: 0.00086568
Iteration 21/25 | Loss: 0.00086568
Iteration 22/25 | Loss: 0.00086568
Iteration 23/25 | Loss: 0.00086568
Iteration 24/25 | Loss: 0.00086568
Iteration 25/25 | Loss: 0.00086568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086568
Iteration 2/1000 | Loss: 0.00002399
Iteration 3/1000 | Loss: 0.00001658
Iteration 4/1000 | Loss: 0.00001455
Iteration 5/1000 | Loss: 0.00001388
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001301
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001259
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001258
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00001255
Iteration 40/1000 | Loss: 0.00001254
Iteration 41/1000 | Loss: 0.00001254
Iteration 42/1000 | Loss: 0.00001254
Iteration 43/1000 | Loss: 0.00001254
Iteration 44/1000 | Loss: 0.00001254
Iteration 45/1000 | Loss: 0.00001254
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001253
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001252
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001251
Iteration 55/1000 | Loss: 0.00001251
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001250
Iteration 58/1000 | Loss: 0.00001250
Iteration 59/1000 | Loss: 0.00001250
Iteration 60/1000 | Loss: 0.00001250
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001249
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001248
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001248
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001248
Iteration 76/1000 | Loss: 0.00001248
Iteration 77/1000 | Loss: 0.00001248
Iteration 78/1000 | Loss: 0.00001248
Iteration 79/1000 | Loss: 0.00001248
Iteration 80/1000 | Loss: 0.00001248
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001248
Iteration 89/1000 | Loss: 0.00001248
Iteration 90/1000 | Loss: 0.00001248
Iteration 91/1000 | Loss: 0.00001248
Iteration 92/1000 | Loss: 0.00001248
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001247
Iteration 96/1000 | Loss: 0.00001247
Iteration 97/1000 | Loss: 0.00001247
Iteration 98/1000 | Loss: 0.00001247
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001247
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001247
Iteration 105/1000 | Loss: 0.00001247
Iteration 106/1000 | Loss: 0.00001247
Iteration 107/1000 | Loss: 0.00001247
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001246
Iteration 110/1000 | Loss: 0.00001246
Iteration 111/1000 | Loss: 0.00001246
Iteration 112/1000 | Loss: 0.00001246
Iteration 113/1000 | Loss: 0.00001246
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001246
Iteration 117/1000 | Loss: 0.00001246
Iteration 118/1000 | Loss: 0.00001246
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001246
Iteration 128/1000 | Loss: 0.00001246
Iteration 129/1000 | Loss: 0.00001246
Iteration 130/1000 | Loss: 0.00001246
Iteration 131/1000 | Loss: 0.00001246
Iteration 132/1000 | Loss: 0.00001246
Iteration 133/1000 | Loss: 0.00001246
Iteration 134/1000 | Loss: 0.00001246
Iteration 135/1000 | Loss: 0.00001246
Iteration 136/1000 | Loss: 0.00001246
Iteration 137/1000 | Loss: 0.00001246
Iteration 138/1000 | Loss: 0.00001246
Iteration 139/1000 | Loss: 0.00001246
Iteration 140/1000 | Loss: 0.00001246
Iteration 141/1000 | Loss: 0.00001246
Iteration 142/1000 | Loss: 0.00001246
Iteration 143/1000 | Loss: 0.00001246
Iteration 144/1000 | Loss: 0.00001246
Iteration 145/1000 | Loss: 0.00001246
Iteration 146/1000 | Loss: 0.00001246
Iteration 147/1000 | Loss: 0.00001246
Iteration 148/1000 | Loss: 0.00001246
Iteration 149/1000 | Loss: 0.00001246
Iteration 150/1000 | Loss: 0.00001246
Iteration 151/1000 | Loss: 0.00001246
Iteration 152/1000 | Loss: 0.00001246
Iteration 153/1000 | Loss: 0.00001246
Iteration 154/1000 | Loss: 0.00001246
Iteration 155/1000 | Loss: 0.00001246
Iteration 156/1000 | Loss: 0.00001246
Iteration 157/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2459060599212535e-05, 1.2459060599212535e-05, 1.2459060599212535e-05, 1.2459060599212535e-05, 1.2459060599212535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2459060599212535e-05

Optimization complete. Final v2v error: 3.0163464546203613 mm

Highest mean error: 3.2170498371124268 mm for frame 107

Lowest mean error: 2.7788450717926025 mm for frame 0

Saving results

Total time: 28.105835676193237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401823
Iteration 2/25 | Loss: 0.00133513
Iteration 3/25 | Loss: 0.00122464
Iteration 4/25 | Loss: 0.00121840
Iteration 5/25 | Loss: 0.00121773
Iteration 6/25 | Loss: 0.00121773
Iteration 7/25 | Loss: 0.00121773
Iteration 8/25 | Loss: 0.00121773
Iteration 9/25 | Loss: 0.00121773
Iteration 10/25 | Loss: 0.00121773
Iteration 11/25 | Loss: 0.00121773
Iteration 12/25 | Loss: 0.00121773
Iteration 13/25 | Loss: 0.00121773
Iteration 14/25 | Loss: 0.00121773
Iteration 15/25 | Loss: 0.00121773
Iteration 16/25 | Loss: 0.00121773
Iteration 17/25 | Loss: 0.00121773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012177268508821726, 0.0012177268508821726, 0.0012177268508821726, 0.0012177268508821726, 0.0012177268508821726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012177268508821726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27068758
Iteration 2/25 | Loss: 0.00273990
Iteration 3/25 | Loss: 0.00273990
Iteration 4/25 | Loss: 0.00273990
Iteration 5/25 | Loss: 0.00273990
Iteration 6/25 | Loss: 0.00273990
Iteration 7/25 | Loss: 0.00273990
Iteration 8/25 | Loss: 0.00273990
Iteration 9/25 | Loss: 0.00273990
Iteration 10/25 | Loss: 0.00273990
Iteration 11/25 | Loss: 0.00273990
Iteration 12/25 | Loss: 0.00273990
Iteration 13/25 | Loss: 0.00273990
Iteration 14/25 | Loss: 0.00273990
Iteration 15/25 | Loss: 0.00273990
Iteration 16/25 | Loss: 0.00273990
Iteration 17/25 | Loss: 0.00273990
Iteration 18/25 | Loss: 0.00273990
Iteration 19/25 | Loss: 0.00273990
Iteration 20/25 | Loss: 0.00273990
Iteration 21/25 | Loss: 0.00273990
Iteration 22/25 | Loss: 0.00273990
Iteration 23/25 | Loss: 0.00273990
Iteration 24/25 | Loss: 0.00273990
Iteration 25/25 | Loss: 0.00273990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273990
Iteration 2/1000 | Loss: 0.00003492
Iteration 3/1000 | Loss: 0.00002143
Iteration 4/1000 | Loss: 0.00001805
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001501
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001384
Iteration 9/1000 | Loss: 0.00001355
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001297
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001278
Iteration 18/1000 | Loss: 0.00001278
Iteration 19/1000 | Loss: 0.00001277
Iteration 20/1000 | Loss: 0.00001274
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001262
Iteration 25/1000 | Loss: 0.00001257
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001245
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001244
Iteration 79/1000 | Loss: 0.00001244
Iteration 80/1000 | Loss: 0.00001244
Iteration 81/1000 | Loss: 0.00001244
Iteration 82/1000 | Loss: 0.00001244
Iteration 83/1000 | Loss: 0.00001244
Iteration 84/1000 | Loss: 0.00001244
Iteration 85/1000 | Loss: 0.00001244
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001243
Iteration 88/1000 | Loss: 0.00001243
Iteration 89/1000 | Loss: 0.00001243
Iteration 90/1000 | Loss: 0.00001243
Iteration 91/1000 | Loss: 0.00001242
Iteration 92/1000 | Loss: 0.00001242
Iteration 93/1000 | Loss: 0.00001242
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001242
Iteration 99/1000 | Loss: 0.00001242
Iteration 100/1000 | Loss: 0.00001242
Iteration 101/1000 | Loss: 0.00001242
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001241
Iteration 106/1000 | Loss: 0.00001241
Iteration 107/1000 | Loss: 0.00001241
Iteration 108/1000 | Loss: 0.00001241
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001240
Iteration 113/1000 | Loss: 0.00001240
Iteration 114/1000 | Loss: 0.00001240
Iteration 115/1000 | Loss: 0.00001240
Iteration 116/1000 | Loss: 0.00001240
Iteration 117/1000 | Loss: 0.00001240
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001239
Iteration 120/1000 | Loss: 0.00001239
Iteration 121/1000 | Loss: 0.00001239
Iteration 122/1000 | Loss: 0.00001239
Iteration 123/1000 | Loss: 0.00001239
Iteration 124/1000 | Loss: 0.00001239
Iteration 125/1000 | Loss: 0.00001239
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001238
Iteration 131/1000 | Loss: 0.00001238
Iteration 132/1000 | Loss: 0.00001238
Iteration 133/1000 | Loss: 0.00001238
Iteration 134/1000 | Loss: 0.00001238
Iteration 135/1000 | Loss: 0.00001238
Iteration 136/1000 | Loss: 0.00001238
Iteration 137/1000 | Loss: 0.00001237
Iteration 138/1000 | Loss: 0.00001237
Iteration 139/1000 | Loss: 0.00001237
Iteration 140/1000 | Loss: 0.00001237
Iteration 141/1000 | Loss: 0.00001237
Iteration 142/1000 | Loss: 0.00001237
Iteration 143/1000 | Loss: 0.00001236
Iteration 144/1000 | Loss: 0.00001236
Iteration 145/1000 | Loss: 0.00001236
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001236
Iteration 149/1000 | Loss: 0.00001236
Iteration 150/1000 | Loss: 0.00001236
Iteration 151/1000 | Loss: 0.00001236
Iteration 152/1000 | Loss: 0.00001236
Iteration 153/1000 | Loss: 0.00001236
Iteration 154/1000 | Loss: 0.00001236
Iteration 155/1000 | Loss: 0.00001236
Iteration 156/1000 | Loss: 0.00001236
Iteration 157/1000 | Loss: 0.00001236
Iteration 158/1000 | Loss: 0.00001236
Iteration 159/1000 | Loss: 0.00001236
Iteration 160/1000 | Loss: 0.00001236
Iteration 161/1000 | Loss: 0.00001235
Iteration 162/1000 | Loss: 0.00001235
Iteration 163/1000 | Loss: 0.00001235
Iteration 164/1000 | Loss: 0.00001235
Iteration 165/1000 | Loss: 0.00001235
Iteration 166/1000 | Loss: 0.00001235
Iteration 167/1000 | Loss: 0.00001235
Iteration 168/1000 | Loss: 0.00001235
Iteration 169/1000 | Loss: 0.00001235
Iteration 170/1000 | Loss: 0.00001235
Iteration 171/1000 | Loss: 0.00001235
Iteration 172/1000 | Loss: 0.00001235
Iteration 173/1000 | Loss: 0.00001234
Iteration 174/1000 | Loss: 0.00001234
Iteration 175/1000 | Loss: 0.00001234
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001234
Iteration 178/1000 | Loss: 0.00001234
Iteration 179/1000 | Loss: 0.00001234
Iteration 180/1000 | Loss: 0.00001234
Iteration 181/1000 | Loss: 0.00001234
Iteration 182/1000 | Loss: 0.00001234
Iteration 183/1000 | Loss: 0.00001234
Iteration 184/1000 | Loss: 0.00001234
Iteration 185/1000 | Loss: 0.00001234
Iteration 186/1000 | Loss: 0.00001234
Iteration 187/1000 | Loss: 0.00001234
Iteration 188/1000 | Loss: 0.00001234
Iteration 189/1000 | Loss: 0.00001234
Iteration 190/1000 | Loss: 0.00001234
Iteration 191/1000 | Loss: 0.00001234
Iteration 192/1000 | Loss: 0.00001234
Iteration 193/1000 | Loss: 0.00001234
Iteration 194/1000 | Loss: 0.00001234
Iteration 195/1000 | Loss: 0.00001234
Iteration 196/1000 | Loss: 0.00001234
Iteration 197/1000 | Loss: 0.00001234
Iteration 198/1000 | Loss: 0.00001234
Iteration 199/1000 | Loss: 0.00001234
Iteration 200/1000 | Loss: 0.00001234
Iteration 201/1000 | Loss: 0.00001234
Iteration 202/1000 | Loss: 0.00001234
Iteration 203/1000 | Loss: 0.00001234
Iteration 204/1000 | Loss: 0.00001234
Iteration 205/1000 | Loss: 0.00001234
Iteration 206/1000 | Loss: 0.00001234
Iteration 207/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.234360934176948e-05, 1.234360934176948e-05, 1.234360934176948e-05, 1.234360934176948e-05, 1.234360934176948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.234360934176948e-05

Optimization complete. Final v2v error: 3.068152904510498 mm

Highest mean error: 3.4006125926971436 mm for frame 24

Lowest mean error: 2.7097420692443848 mm for frame 92

Saving results

Total time: 44.004509925842285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932111
Iteration 2/25 | Loss: 0.00145269
Iteration 3/25 | Loss: 0.00131923
Iteration 4/25 | Loss: 0.00130308
Iteration 5/25 | Loss: 0.00129684
Iteration 6/25 | Loss: 0.00129620
Iteration 7/25 | Loss: 0.00129620
Iteration 8/25 | Loss: 0.00129620
Iteration 9/25 | Loss: 0.00129620
Iteration 10/25 | Loss: 0.00129620
Iteration 11/25 | Loss: 0.00129620
Iteration 12/25 | Loss: 0.00129620
Iteration 13/25 | Loss: 0.00129620
Iteration 14/25 | Loss: 0.00129620
Iteration 15/25 | Loss: 0.00129620
Iteration 16/25 | Loss: 0.00129620
Iteration 17/25 | Loss: 0.00129620
Iteration 18/25 | Loss: 0.00129620
Iteration 19/25 | Loss: 0.00129620
Iteration 20/25 | Loss: 0.00129620
Iteration 21/25 | Loss: 0.00129620
Iteration 22/25 | Loss: 0.00129620
Iteration 23/25 | Loss: 0.00129620
Iteration 24/25 | Loss: 0.00129620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001296201371587813, 0.001296201371587813, 0.001296201371587813, 0.001296201371587813, 0.001296201371587813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001296201371587813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10096908
Iteration 2/25 | Loss: 0.00246561
Iteration 3/25 | Loss: 0.00246556
Iteration 4/25 | Loss: 0.00246556
Iteration 5/25 | Loss: 0.00246556
Iteration 6/25 | Loss: 0.00246556
Iteration 7/25 | Loss: 0.00246556
Iteration 8/25 | Loss: 0.00246556
Iteration 9/25 | Loss: 0.00246556
Iteration 10/25 | Loss: 0.00246555
Iteration 11/25 | Loss: 0.00246555
Iteration 12/25 | Loss: 0.00246555
Iteration 13/25 | Loss: 0.00246555
Iteration 14/25 | Loss: 0.00246555
Iteration 15/25 | Loss: 0.00246555
Iteration 16/25 | Loss: 0.00246555
Iteration 17/25 | Loss: 0.00246555
Iteration 18/25 | Loss: 0.00246555
Iteration 19/25 | Loss: 0.00246555
Iteration 20/25 | Loss: 0.00246555
Iteration 21/25 | Loss: 0.00246555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0024655545130372047, 0.0024655545130372047, 0.0024655545130372047, 0.0024655545130372047, 0.0024655545130372047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024655545130372047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246555
Iteration 2/1000 | Loss: 0.00004494
Iteration 3/1000 | Loss: 0.00003242
Iteration 4/1000 | Loss: 0.00002846
Iteration 5/1000 | Loss: 0.00002608
Iteration 6/1000 | Loss: 0.00002459
Iteration 7/1000 | Loss: 0.00002384
Iteration 8/1000 | Loss: 0.00002333
Iteration 9/1000 | Loss: 0.00002287
Iteration 10/1000 | Loss: 0.00002258
Iteration 11/1000 | Loss: 0.00002236
Iteration 12/1000 | Loss: 0.00002218
Iteration 13/1000 | Loss: 0.00002210
Iteration 14/1000 | Loss: 0.00002210
Iteration 15/1000 | Loss: 0.00002207
Iteration 16/1000 | Loss: 0.00002204
Iteration 17/1000 | Loss: 0.00002198
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002192
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002181
Iteration 22/1000 | Loss: 0.00002181
Iteration 23/1000 | Loss: 0.00002181
Iteration 24/1000 | Loss: 0.00002180
Iteration 25/1000 | Loss: 0.00002180
Iteration 26/1000 | Loss: 0.00002180
Iteration 27/1000 | Loss: 0.00002180
Iteration 28/1000 | Loss: 0.00002179
Iteration 29/1000 | Loss: 0.00002178
Iteration 30/1000 | Loss: 0.00002177
Iteration 31/1000 | Loss: 0.00002177
Iteration 32/1000 | Loss: 0.00002177
Iteration 33/1000 | Loss: 0.00002177
Iteration 34/1000 | Loss: 0.00002177
Iteration 35/1000 | Loss: 0.00002177
Iteration 36/1000 | Loss: 0.00002177
Iteration 37/1000 | Loss: 0.00002177
Iteration 38/1000 | Loss: 0.00002176
Iteration 39/1000 | Loss: 0.00002176
Iteration 40/1000 | Loss: 0.00002176
Iteration 41/1000 | Loss: 0.00002176
Iteration 42/1000 | Loss: 0.00002176
Iteration 43/1000 | Loss: 0.00002176
Iteration 44/1000 | Loss: 0.00002176
Iteration 45/1000 | Loss: 0.00002176
Iteration 46/1000 | Loss: 0.00002175
Iteration 47/1000 | Loss: 0.00002175
Iteration 48/1000 | Loss: 0.00002175
Iteration 49/1000 | Loss: 0.00002175
Iteration 50/1000 | Loss: 0.00002175
Iteration 51/1000 | Loss: 0.00002175
Iteration 52/1000 | Loss: 0.00002175
Iteration 53/1000 | Loss: 0.00002175
Iteration 54/1000 | Loss: 0.00002175
Iteration 55/1000 | Loss: 0.00002175
Iteration 56/1000 | Loss: 0.00002175
Iteration 57/1000 | Loss: 0.00002175
Iteration 58/1000 | Loss: 0.00002175
Iteration 59/1000 | Loss: 0.00002175
Iteration 60/1000 | Loss: 0.00002175
Iteration 61/1000 | Loss: 0.00002175
Iteration 62/1000 | Loss: 0.00002175
Iteration 63/1000 | Loss: 0.00002175
Iteration 64/1000 | Loss: 0.00002175
Iteration 65/1000 | Loss: 0.00002175
Iteration 66/1000 | Loss: 0.00002175
Iteration 67/1000 | Loss: 0.00002175
Iteration 68/1000 | Loss: 0.00002175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [2.1745525373262353e-05, 2.1745525373262353e-05, 2.1745525373262353e-05, 2.1745525373262353e-05, 2.1745525373262353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1745525373262353e-05

Optimization complete. Final v2v error: 3.8964321613311768 mm

Highest mean error: 4.209688663482666 mm for frame 126

Lowest mean error: 3.3993747234344482 mm for frame 27

Saving results

Total time: 36.25543928146362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486390
Iteration 2/25 | Loss: 0.00133403
Iteration 3/25 | Loss: 0.00123819
Iteration 4/25 | Loss: 0.00122336
Iteration 5/25 | Loss: 0.00121845
Iteration 6/25 | Loss: 0.00121740
Iteration 7/25 | Loss: 0.00121740
Iteration 8/25 | Loss: 0.00121740
Iteration 9/25 | Loss: 0.00121740
Iteration 10/25 | Loss: 0.00121740
Iteration 11/25 | Loss: 0.00121740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012173980940133333, 0.0012173980940133333, 0.0012173980940133333, 0.0012173980940133333, 0.0012173980940133333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012173980940133333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19404757
Iteration 2/25 | Loss: 0.00267964
Iteration 3/25 | Loss: 0.00267964
Iteration 4/25 | Loss: 0.00267964
Iteration 5/25 | Loss: 0.00267964
Iteration 6/25 | Loss: 0.00267964
Iteration 7/25 | Loss: 0.00267964
Iteration 8/25 | Loss: 0.00267964
Iteration 9/25 | Loss: 0.00267964
Iteration 10/25 | Loss: 0.00267963
Iteration 11/25 | Loss: 0.00267963
Iteration 12/25 | Loss: 0.00267963
Iteration 13/25 | Loss: 0.00267963
Iteration 14/25 | Loss: 0.00267963
Iteration 15/25 | Loss: 0.00267963
Iteration 16/25 | Loss: 0.00267963
Iteration 17/25 | Loss: 0.00267963
Iteration 18/25 | Loss: 0.00267963
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002679634839296341, 0.002679634839296341, 0.002679634839296341, 0.002679634839296341, 0.002679634839296341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002679634839296341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267963
Iteration 2/1000 | Loss: 0.00003272
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001702
Iteration 5/1000 | Loss: 0.00001584
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001402
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001377
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001352
Iteration 21/1000 | Loss: 0.00001349
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001341
Iteration 29/1000 | Loss: 0.00001341
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001340
Iteration 32/1000 | Loss: 0.00001340
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001339
Iteration 35/1000 | Loss: 0.00001339
Iteration 36/1000 | Loss: 0.00001338
Iteration 37/1000 | Loss: 0.00001338
Iteration 38/1000 | Loss: 0.00001337
Iteration 39/1000 | Loss: 0.00001337
Iteration 40/1000 | Loss: 0.00001336
Iteration 41/1000 | Loss: 0.00001336
Iteration 42/1000 | Loss: 0.00001336
Iteration 43/1000 | Loss: 0.00001336
Iteration 44/1000 | Loss: 0.00001336
Iteration 45/1000 | Loss: 0.00001336
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001335
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001334
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001332
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.3302192201081198e-05, 1.3302192201081198e-05, 1.3302192201081198e-05, 1.3302192201081198e-05, 1.3302192201081198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3302192201081198e-05

Optimization complete. Final v2v error: 3.1287059783935547 mm

Highest mean error: 3.6007280349731445 mm for frame 239

Lowest mean error: 2.6906235218048096 mm for frame 2

Saving results

Total time: 34.046637535095215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052836
Iteration 2/25 | Loss: 0.01052836
Iteration 3/25 | Loss: 0.00431779
Iteration 4/25 | Loss: 0.00255057
Iteration 5/25 | Loss: 0.00232527
Iteration 6/25 | Loss: 0.00219606
Iteration 7/25 | Loss: 0.00204644
Iteration 8/25 | Loss: 0.00203477
Iteration 9/25 | Loss: 0.00202436
Iteration 10/25 | Loss: 0.00203737
Iteration 11/25 | Loss: 0.00194480
Iteration 12/25 | Loss: 0.00189028
Iteration 13/25 | Loss: 0.00187982
Iteration 14/25 | Loss: 0.00186979
Iteration 15/25 | Loss: 0.00186852
Iteration 16/25 | Loss: 0.00186805
Iteration 17/25 | Loss: 0.00186769
Iteration 18/25 | Loss: 0.00186740
Iteration 19/25 | Loss: 0.00186712
Iteration 20/25 | Loss: 0.00186694
Iteration 21/25 | Loss: 0.00186685
Iteration 22/25 | Loss: 0.00186682
Iteration 23/25 | Loss: 0.00186680
Iteration 24/25 | Loss: 0.00186680
Iteration 25/25 | Loss: 0.00186680

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08674371
Iteration 2/25 | Loss: 0.02050951
Iteration 3/25 | Loss: 0.00581597
Iteration 4/25 | Loss: 0.00581596
Iteration 5/25 | Loss: 0.00581595
Iteration 6/25 | Loss: 0.00581595
Iteration 7/25 | Loss: 0.00581595
Iteration 8/25 | Loss: 0.00581595
Iteration 9/25 | Loss: 0.00581595
Iteration 10/25 | Loss: 0.00581595
Iteration 11/25 | Loss: 0.00581595
Iteration 12/25 | Loss: 0.00581595
Iteration 13/25 | Loss: 0.00581595
Iteration 14/25 | Loss: 0.00581595
Iteration 15/25 | Loss: 0.00581595
Iteration 16/25 | Loss: 0.00581595
Iteration 17/25 | Loss: 0.00581595
Iteration 18/25 | Loss: 0.00581595
Iteration 19/25 | Loss: 0.00581595
Iteration 20/25 | Loss: 0.00581595
Iteration 21/25 | Loss: 0.00581595
Iteration 22/25 | Loss: 0.00581595
Iteration 23/25 | Loss: 0.00581595
Iteration 24/25 | Loss: 0.00581595
Iteration 25/25 | Loss: 0.00581595
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.005815950222313404, 0.005815950222313404, 0.005815950222313404, 0.005815950222313404, 0.005815950222313404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005815950222313404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00581595
Iteration 2/1000 | Loss: 0.01130867
Iteration 3/1000 | Loss: 0.00072996
Iteration 4/1000 | Loss: 0.00052266
Iteration 5/1000 | Loss: 0.00473961
Iteration 6/1000 | Loss: 0.00048093
Iteration 7/1000 | Loss: 0.00048784
Iteration 8/1000 | Loss: 0.00056703
Iteration 9/1000 | Loss: 0.00197833
Iteration 10/1000 | Loss: 0.00075960
Iteration 11/1000 | Loss: 0.00230254
Iteration 12/1000 | Loss: 0.01066543
Iteration 13/1000 | Loss: 0.00425431
Iteration 14/1000 | Loss: 0.00124819
Iteration 15/1000 | Loss: 0.00082623
Iteration 16/1000 | Loss: 0.00039458
Iteration 17/1000 | Loss: 0.00045635
Iteration 18/1000 | Loss: 0.00010899
Iteration 19/1000 | Loss: 0.00018757
Iteration 20/1000 | Loss: 0.00041959
Iteration 21/1000 | Loss: 0.00032437
Iteration 22/1000 | Loss: 0.00008611
Iteration 23/1000 | Loss: 0.00012452
Iteration 24/1000 | Loss: 0.00004535
Iteration 25/1000 | Loss: 0.00024556
Iteration 26/1000 | Loss: 0.00019637
Iteration 27/1000 | Loss: 0.00034572
Iteration 28/1000 | Loss: 0.00003960
Iteration 29/1000 | Loss: 0.00005553
Iteration 30/1000 | Loss: 0.00002368
Iteration 31/1000 | Loss: 0.00002491
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00024857
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00012272
Iteration 37/1000 | Loss: 0.00001462
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00018579
Iteration 40/1000 | Loss: 0.00010502
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001326
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00014918
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001281
Iteration 49/1000 | Loss: 0.00008529
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001267
Iteration 53/1000 | Loss: 0.00001266
Iteration 54/1000 | Loss: 0.00001266
Iteration 55/1000 | Loss: 0.00001266
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001266
Iteration 61/1000 | Loss: 0.00001266
Iteration 62/1000 | Loss: 0.00001266
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001265
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001265
Iteration 68/1000 | Loss: 0.00001265
Iteration 69/1000 | Loss: 0.00001265
Iteration 70/1000 | Loss: 0.00001265
Iteration 71/1000 | Loss: 0.00001265
Iteration 72/1000 | Loss: 0.00001264
Iteration 73/1000 | Loss: 0.00001264
Iteration 74/1000 | Loss: 0.00001264
Iteration 75/1000 | Loss: 0.00001264
Iteration 76/1000 | Loss: 0.00001263
Iteration 77/1000 | Loss: 0.00001263
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001260
Iteration 91/1000 | Loss: 0.00001260
Iteration 92/1000 | Loss: 0.00001260
Iteration 93/1000 | Loss: 0.00001260
Iteration 94/1000 | Loss: 0.00001260
Iteration 95/1000 | Loss: 0.00001260
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001259
Iteration 104/1000 | Loss: 0.00001259
Iteration 105/1000 | Loss: 0.00001259
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001259
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001259
Iteration 113/1000 | Loss: 0.00001259
Iteration 114/1000 | Loss: 0.00001259
Iteration 115/1000 | Loss: 0.00001259
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001258
Iteration 120/1000 | Loss: 0.00001258
Iteration 121/1000 | Loss: 0.00001258
Iteration 122/1000 | Loss: 0.00001258
Iteration 123/1000 | Loss: 0.00001258
Iteration 124/1000 | Loss: 0.00001257
Iteration 125/1000 | Loss: 0.00001257
Iteration 126/1000 | Loss: 0.00001257
Iteration 127/1000 | Loss: 0.00001257
Iteration 128/1000 | Loss: 0.00001257
Iteration 129/1000 | Loss: 0.00001257
Iteration 130/1000 | Loss: 0.00001257
Iteration 131/1000 | Loss: 0.00001257
Iteration 132/1000 | Loss: 0.00001257
Iteration 133/1000 | Loss: 0.00001257
Iteration 134/1000 | Loss: 0.00001257
Iteration 135/1000 | Loss: 0.00001256
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001255
Iteration 138/1000 | Loss: 0.00001255
Iteration 139/1000 | Loss: 0.00001255
Iteration 140/1000 | Loss: 0.00001255
Iteration 141/1000 | Loss: 0.00001255
Iteration 142/1000 | Loss: 0.00001255
Iteration 143/1000 | Loss: 0.00001255
Iteration 144/1000 | Loss: 0.00001255
Iteration 145/1000 | Loss: 0.00001255
Iteration 146/1000 | Loss: 0.00001255
Iteration 147/1000 | Loss: 0.00001255
Iteration 148/1000 | Loss: 0.00001255
Iteration 149/1000 | Loss: 0.00001255
Iteration 150/1000 | Loss: 0.00001255
Iteration 151/1000 | Loss: 0.00001255
Iteration 152/1000 | Loss: 0.00001255
Iteration 153/1000 | Loss: 0.00001255
Iteration 154/1000 | Loss: 0.00001255
Iteration 155/1000 | Loss: 0.00001255
Iteration 156/1000 | Loss: 0.00001255
Iteration 157/1000 | Loss: 0.00001255
Iteration 158/1000 | Loss: 0.00001255
Iteration 159/1000 | Loss: 0.00001255
Iteration 160/1000 | Loss: 0.00001255
Iteration 161/1000 | Loss: 0.00001254
Iteration 162/1000 | Loss: 0.00001254
Iteration 163/1000 | Loss: 0.00001254
Iteration 164/1000 | Loss: 0.00001254
Iteration 165/1000 | Loss: 0.00001254
Iteration 166/1000 | Loss: 0.00001254
Iteration 167/1000 | Loss: 0.00001254
Iteration 168/1000 | Loss: 0.00001254
Iteration 169/1000 | Loss: 0.00001254
Iteration 170/1000 | Loss: 0.00001254
Iteration 171/1000 | Loss: 0.00001254
Iteration 172/1000 | Loss: 0.00001254
Iteration 173/1000 | Loss: 0.00001254
Iteration 174/1000 | Loss: 0.00001254
Iteration 175/1000 | Loss: 0.00001254
Iteration 176/1000 | Loss: 0.00001254
Iteration 177/1000 | Loss: 0.00001254
Iteration 178/1000 | Loss: 0.00001254
Iteration 179/1000 | Loss: 0.00001254
Iteration 180/1000 | Loss: 0.00001254
Iteration 181/1000 | Loss: 0.00001254
Iteration 182/1000 | Loss: 0.00001254
Iteration 183/1000 | Loss: 0.00001254
Iteration 184/1000 | Loss: 0.00001254
Iteration 185/1000 | Loss: 0.00001254
Iteration 186/1000 | Loss: 0.00001254
Iteration 187/1000 | Loss: 0.00001254
Iteration 188/1000 | Loss: 0.00001254
Iteration 189/1000 | Loss: 0.00001254
Iteration 190/1000 | Loss: 0.00001254
Iteration 191/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.253986465599155e-05, 1.253986465599155e-05, 1.253986465599155e-05, 1.253986465599155e-05, 1.253986465599155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.253986465599155e-05

Optimization complete. Final v2v error: 2.9777681827545166 mm

Highest mean error: 3.3545405864715576 mm for frame 1

Lowest mean error: 2.7109405994415283 mm for frame 102

Saving results

Total time: 110.68738794326782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128076
Iteration 2/25 | Loss: 0.00320617
Iteration 3/25 | Loss: 0.00229332
Iteration 4/25 | Loss: 0.00185222
Iteration 5/25 | Loss: 0.00191781
Iteration 6/25 | Loss: 0.00190124
Iteration 7/25 | Loss: 0.00185137
Iteration 8/25 | Loss: 0.00166858
Iteration 9/25 | Loss: 0.00154810
Iteration 10/25 | Loss: 0.00151304
Iteration 11/25 | Loss: 0.00145769
Iteration 12/25 | Loss: 0.00142610
Iteration 13/25 | Loss: 0.00142670
Iteration 14/25 | Loss: 0.00141858
Iteration 15/25 | Loss: 0.00139799
Iteration 16/25 | Loss: 0.00140810
Iteration 17/25 | Loss: 0.00140941
Iteration 18/25 | Loss: 0.00139829
Iteration 19/25 | Loss: 0.00138187
Iteration 20/25 | Loss: 0.00137104
Iteration 21/25 | Loss: 0.00136887
Iteration 22/25 | Loss: 0.00135523
Iteration 23/25 | Loss: 0.00133880
Iteration 24/25 | Loss: 0.00133232
Iteration 25/25 | Loss: 0.00131663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91879165
Iteration 2/25 | Loss: 0.00181724
Iteration 3/25 | Loss: 0.00176253
Iteration 4/25 | Loss: 0.00176253
Iteration 5/25 | Loss: 0.00176253
Iteration 6/25 | Loss: 0.00176253
Iteration 7/25 | Loss: 0.00176253
Iteration 8/25 | Loss: 0.00176253
Iteration 9/25 | Loss: 0.00176253
Iteration 10/25 | Loss: 0.00176253
Iteration 11/25 | Loss: 0.00176253
Iteration 12/25 | Loss: 0.00176253
Iteration 13/25 | Loss: 0.00176253
Iteration 14/25 | Loss: 0.00176253
Iteration 15/25 | Loss: 0.00176253
Iteration 16/25 | Loss: 0.00176253
Iteration 17/25 | Loss: 0.00176253
Iteration 18/25 | Loss: 0.00176253
Iteration 19/25 | Loss: 0.00176253
Iteration 20/25 | Loss: 0.00176253
Iteration 21/25 | Loss: 0.00176253
Iteration 22/25 | Loss: 0.00176253
Iteration 23/25 | Loss: 0.00176253
Iteration 24/25 | Loss: 0.00176253
Iteration 25/25 | Loss: 0.00176253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176253
Iteration 2/1000 | Loss: 0.00057218
Iteration 3/1000 | Loss: 0.00050355
Iteration 4/1000 | Loss: 0.00018505
Iteration 5/1000 | Loss: 0.00023135
Iteration 6/1000 | Loss: 0.00020472
Iteration 7/1000 | Loss: 0.00006863
Iteration 8/1000 | Loss: 0.00021913
Iteration 9/1000 | Loss: 0.00013205
Iteration 10/1000 | Loss: 0.00023820
Iteration 11/1000 | Loss: 0.00046805
Iteration 12/1000 | Loss: 0.00060191
Iteration 13/1000 | Loss: 0.00022515
Iteration 14/1000 | Loss: 0.00033302
Iteration 15/1000 | Loss: 0.00023356
Iteration 16/1000 | Loss: 0.00025075
Iteration 17/1000 | Loss: 0.00010250
Iteration 18/1000 | Loss: 0.00007075
Iteration 19/1000 | Loss: 0.00011461
Iteration 20/1000 | Loss: 0.00008687
Iteration 21/1000 | Loss: 0.00005516
Iteration 22/1000 | Loss: 0.00020958
Iteration 23/1000 | Loss: 0.00024159
Iteration 24/1000 | Loss: 0.00043690
Iteration 25/1000 | Loss: 0.00024164
Iteration 26/1000 | Loss: 0.00038843
Iteration 27/1000 | Loss: 0.00021110
Iteration 28/1000 | Loss: 0.00025194
Iteration 29/1000 | Loss: 0.00033900
Iteration 30/1000 | Loss: 0.00042486
Iteration 31/1000 | Loss: 0.00008201
Iteration 32/1000 | Loss: 0.00027015
Iteration 33/1000 | Loss: 0.00028047
Iteration 34/1000 | Loss: 0.00071676
Iteration 35/1000 | Loss: 0.00008804
Iteration 36/1000 | Loss: 0.00006682
Iteration 37/1000 | Loss: 0.00032659
Iteration 38/1000 | Loss: 0.00021109
Iteration 39/1000 | Loss: 0.00045147
Iteration 40/1000 | Loss: 0.00005899
Iteration 41/1000 | Loss: 0.00004917
Iteration 42/1000 | Loss: 0.00005390
Iteration 43/1000 | Loss: 0.00004625
Iteration 44/1000 | Loss: 0.00004398
Iteration 45/1000 | Loss: 0.00021498
Iteration 46/1000 | Loss: 0.00027733
Iteration 47/1000 | Loss: 0.00038726
Iteration 48/1000 | Loss: 0.00014561
Iteration 49/1000 | Loss: 0.00005103
Iteration 50/1000 | Loss: 0.00057284
Iteration 51/1000 | Loss: 0.00007814
Iteration 52/1000 | Loss: 0.00018926
Iteration 53/1000 | Loss: 0.00010083
Iteration 54/1000 | Loss: 0.00004282
Iteration 55/1000 | Loss: 0.00004144
Iteration 56/1000 | Loss: 0.00004044
Iteration 57/1000 | Loss: 0.00003968
Iteration 58/1000 | Loss: 0.00003915
Iteration 59/1000 | Loss: 0.00038398
Iteration 60/1000 | Loss: 0.00028240
Iteration 61/1000 | Loss: 0.00027076
Iteration 62/1000 | Loss: 0.00005359
Iteration 63/1000 | Loss: 0.00019486
Iteration 64/1000 | Loss: 0.00019043
Iteration 65/1000 | Loss: 0.00019554
Iteration 66/1000 | Loss: 0.00016206
Iteration 67/1000 | Loss: 0.00016910
Iteration 68/1000 | Loss: 0.00003866
Iteration 69/1000 | Loss: 0.00003467
Iteration 70/1000 | Loss: 0.00003349
Iteration 71/1000 | Loss: 0.00003222
Iteration 72/1000 | Loss: 0.00003157
Iteration 73/1000 | Loss: 0.00003109
Iteration 74/1000 | Loss: 0.00003084
Iteration 75/1000 | Loss: 0.00003073
Iteration 76/1000 | Loss: 0.00003071
Iteration 77/1000 | Loss: 0.00003063
Iteration 78/1000 | Loss: 0.00003062
Iteration 79/1000 | Loss: 0.00003058
Iteration 80/1000 | Loss: 0.00003058
Iteration 81/1000 | Loss: 0.00003054
Iteration 82/1000 | Loss: 0.00003054
Iteration 83/1000 | Loss: 0.00003054
Iteration 84/1000 | Loss: 0.00003050
Iteration 85/1000 | Loss: 0.00003050
Iteration 86/1000 | Loss: 0.00003050
Iteration 87/1000 | Loss: 0.00003050
Iteration 88/1000 | Loss: 0.00003049
Iteration 89/1000 | Loss: 0.00003048
Iteration 90/1000 | Loss: 0.00003048
Iteration 91/1000 | Loss: 0.00003045
Iteration 92/1000 | Loss: 0.00003045
Iteration 93/1000 | Loss: 0.00003044
Iteration 94/1000 | Loss: 0.00003044
Iteration 95/1000 | Loss: 0.00003044
Iteration 96/1000 | Loss: 0.00003044
Iteration 97/1000 | Loss: 0.00003044
Iteration 98/1000 | Loss: 0.00003044
Iteration 99/1000 | Loss: 0.00003044
Iteration 100/1000 | Loss: 0.00003044
Iteration 101/1000 | Loss: 0.00003044
Iteration 102/1000 | Loss: 0.00003044
Iteration 103/1000 | Loss: 0.00003044
Iteration 104/1000 | Loss: 0.00003044
Iteration 105/1000 | Loss: 0.00003044
Iteration 106/1000 | Loss: 0.00003044
Iteration 107/1000 | Loss: 0.00003044
Iteration 108/1000 | Loss: 0.00003043
Iteration 109/1000 | Loss: 0.00003043
Iteration 110/1000 | Loss: 0.00003042
Iteration 111/1000 | Loss: 0.00003042
Iteration 112/1000 | Loss: 0.00003042
Iteration 113/1000 | Loss: 0.00003042
Iteration 114/1000 | Loss: 0.00003042
Iteration 115/1000 | Loss: 0.00003042
Iteration 116/1000 | Loss: 0.00003042
Iteration 117/1000 | Loss: 0.00003042
Iteration 118/1000 | Loss: 0.00003042
Iteration 119/1000 | Loss: 0.00003042
Iteration 120/1000 | Loss: 0.00003042
Iteration 121/1000 | Loss: 0.00003041
Iteration 122/1000 | Loss: 0.00003040
Iteration 123/1000 | Loss: 0.00003040
Iteration 124/1000 | Loss: 0.00003039
Iteration 125/1000 | Loss: 0.00003039
Iteration 126/1000 | Loss: 0.00003039
Iteration 127/1000 | Loss: 0.00003039
Iteration 128/1000 | Loss: 0.00003039
Iteration 129/1000 | Loss: 0.00003039
Iteration 130/1000 | Loss: 0.00003039
Iteration 131/1000 | Loss: 0.00003039
Iteration 132/1000 | Loss: 0.00003038
Iteration 133/1000 | Loss: 0.00003037
Iteration 134/1000 | Loss: 0.00003037
Iteration 135/1000 | Loss: 0.00003037
Iteration 136/1000 | Loss: 0.00003037
Iteration 137/1000 | Loss: 0.00003036
Iteration 138/1000 | Loss: 0.00003036
Iteration 139/1000 | Loss: 0.00003036
Iteration 140/1000 | Loss: 0.00003036
Iteration 141/1000 | Loss: 0.00003036
Iteration 142/1000 | Loss: 0.00003036
Iteration 143/1000 | Loss: 0.00003036
Iteration 144/1000 | Loss: 0.00003036
Iteration 145/1000 | Loss: 0.00003036
Iteration 146/1000 | Loss: 0.00003036
Iteration 147/1000 | Loss: 0.00003036
Iteration 148/1000 | Loss: 0.00003036
Iteration 149/1000 | Loss: 0.00003036
Iteration 150/1000 | Loss: 0.00003035
Iteration 151/1000 | Loss: 0.00003035
Iteration 152/1000 | Loss: 0.00003035
Iteration 153/1000 | Loss: 0.00003035
Iteration 154/1000 | Loss: 0.00003035
Iteration 155/1000 | Loss: 0.00003035
Iteration 156/1000 | Loss: 0.00003035
Iteration 157/1000 | Loss: 0.00003035
Iteration 158/1000 | Loss: 0.00003035
Iteration 159/1000 | Loss: 0.00003035
Iteration 160/1000 | Loss: 0.00003035
Iteration 161/1000 | Loss: 0.00003035
Iteration 162/1000 | Loss: 0.00003035
Iteration 163/1000 | Loss: 0.00003035
Iteration 164/1000 | Loss: 0.00003035
Iteration 165/1000 | Loss: 0.00003035
Iteration 166/1000 | Loss: 0.00003034
Iteration 167/1000 | Loss: 0.00003034
Iteration 168/1000 | Loss: 0.00003034
Iteration 169/1000 | Loss: 0.00003034
Iteration 170/1000 | Loss: 0.00003034
Iteration 171/1000 | Loss: 0.00003034
Iteration 172/1000 | Loss: 0.00003034
Iteration 173/1000 | Loss: 0.00003034
Iteration 174/1000 | Loss: 0.00003034
Iteration 175/1000 | Loss: 0.00003034
Iteration 176/1000 | Loss: 0.00003034
Iteration 177/1000 | Loss: 0.00003034
Iteration 178/1000 | Loss: 0.00003034
Iteration 179/1000 | Loss: 0.00003034
Iteration 180/1000 | Loss: 0.00003034
Iteration 181/1000 | Loss: 0.00003034
Iteration 182/1000 | Loss: 0.00003034
Iteration 183/1000 | Loss: 0.00003034
Iteration 184/1000 | Loss: 0.00003034
Iteration 185/1000 | Loss: 0.00003034
Iteration 186/1000 | Loss: 0.00003034
Iteration 187/1000 | Loss: 0.00003034
Iteration 188/1000 | Loss: 0.00003034
Iteration 189/1000 | Loss: 0.00003033
Iteration 190/1000 | Loss: 0.00003033
Iteration 191/1000 | Loss: 0.00003033
Iteration 192/1000 | Loss: 0.00003033
Iteration 193/1000 | Loss: 0.00003033
Iteration 194/1000 | Loss: 0.00003033
Iteration 195/1000 | Loss: 0.00003033
Iteration 196/1000 | Loss: 0.00003033
Iteration 197/1000 | Loss: 0.00003033
Iteration 198/1000 | Loss: 0.00003033
Iteration 199/1000 | Loss: 0.00003033
Iteration 200/1000 | Loss: 0.00003033
Iteration 201/1000 | Loss: 0.00003033
Iteration 202/1000 | Loss: 0.00003033
Iteration 203/1000 | Loss: 0.00003033
Iteration 204/1000 | Loss: 0.00003033
Iteration 205/1000 | Loss: 0.00003033
Iteration 206/1000 | Loss: 0.00003033
Iteration 207/1000 | Loss: 0.00003032
Iteration 208/1000 | Loss: 0.00003032
Iteration 209/1000 | Loss: 0.00003032
Iteration 210/1000 | Loss: 0.00003032
Iteration 211/1000 | Loss: 0.00003032
Iteration 212/1000 | Loss: 0.00003032
Iteration 213/1000 | Loss: 0.00003032
Iteration 214/1000 | Loss: 0.00003032
Iteration 215/1000 | Loss: 0.00003032
Iteration 216/1000 | Loss: 0.00003032
Iteration 217/1000 | Loss: 0.00003032
Iteration 218/1000 | Loss: 0.00003032
Iteration 219/1000 | Loss: 0.00003032
Iteration 220/1000 | Loss: 0.00003032
Iteration 221/1000 | Loss: 0.00003032
Iteration 222/1000 | Loss: 0.00003032
Iteration 223/1000 | Loss: 0.00003031
Iteration 224/1000 | Loss: 0.00003031
Iteration 225/1000 | Loss: 0.00003031
Iteration 226/1000 | Loss: 0.00003031
Iteration 227/1000 | Loss: 0.00003031
Iteration 228/1000 | Loss: 0.00003031
Iteration 229/1000 | Loss: 0.00003031
Iteration 230/1000 | Loss: 0.00003031
Iteration 231/1000 | Loss: 0.00003031
Iteration 232/1000 | Loss: 0.00003031
Iteration 233/1000 | Loss: 0.00003031
Iteration 234/1000 | Loss: 0.00003031
Iteration 235/1000 | Loss: 0.00003031
Iteration 236/1000 | Loss: 0.00003031
Iteration 237/1000 | Loss: 0.00003031
Iteration 238/1000 | Loss: 0.00003031
Iteration 239/1000 | Loss: 0.00003031
Iteration 240/1000 | Loss: 0.00003031
Iteration 241/1000 | Loss: 0.00003031
Iteration 242/1000 | Loss: 0.00003031
Iteration 243/1000 | Loss: 0.00003031
Iteration 244/1000 | Loss: 0.00003031
Iteration 245/1000 | Loss: 0.00003031
Iteration 246/1000 | Loss: 0.00003031
Iteration 247/1000 | Loss: 0.00003031
Iteration 248/1000 | Loss: 0.00003031
Iteration 249/1000 | Loss: 0.00003031
Iteration 250/1000 | Loss: 0.00003031
Iteration 251/1000 | Loss: 0.00003031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [3.0308823625091463e-05, 3.0308823625091463e-05, 3.0308823625091463e-05, 3.0308823625091463e-05, 3.0308823625091463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0308823625091463e-05

Optimization complete. Final v2v error: 4.249667644500732 mm

Highest mean error: 4.92841100692749 mm for frame 64

Lowest mean error: 3.2487170696258545 mm for frame 15

Saving results

Total time: 183.29462885856628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836404
Iteration 2/25 | Loss: 0.00135081
Iteration 3/25 | Loss: 0.00124083
Iteration 4/25 | Loss: 0.00122943
Iteration 5/25 | Loss: 0.00122711
Iteration 6/25 | Loss: 0.00122696
Iteration 7/25 | Loss: 0.00122696
Iteration 8/25 | Loss: 0.00122696
Iteration 9/25 | Loss: 0.00122696
Iteration 10/25 | Loss: 0.00122696
Iteration 11/25 | Loss: 0.00122696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001226957654580474, 0.001226957654580474, 0.001226957654580474, 0.001226957654580474, 0.001226957654580474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001226957654580474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14221907
Iteration 2/25 | Loss: 0.00287645
Iteration 3/25 | Loss: 0.00287645
Iteration 4/25 | Loss: 0.00287645
Iteration 5/25 | Loss: 0.00287645
Iteration 6/25 | Loss: 0.00287645
Iteration 7/25 | Loss: 0.00287645
Iteration 8/25 | Loss: 0.00287645
Iteration 9/25 | Loss: 0.00287645
Iteration 10/25 | Loss: 0.00287645
Iteration 11/25 | Loss: 0.00287645
Iteration 12/25 | Loss: 0.00287645
Iteration 13/25 | Loss: 0.00287645
Iteration 14/25 | Loss: 0.00287645
Iteration 15/25 | Loss: 0.00287645
Iteration 16/25 | Loss: 0.00287645
Iteration 17/25 | Loss: 0.00287645
Iteration 18/25 | Loss: 0.00287645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0028764463495463133, 0.0028764463495463133, 0.0028764463495463133, 0.0028764463495463133, 0.0028764463495463133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028764463495463133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00287645
Iteration 2/1000 | Loss: 0.00003077
Iteration 3/1000 | Loss: 0.00002092
Iteration 4/1000 | Loss: 0.00001865
Iteration 5/1000 | Loss: 0.00001712
Iteration 6/1000 | Loss: 0.00001626
Iteration 7/1000 | Loss: 0.00001544
Iteration 8/1000 | Loss: 0.00001491
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001414
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001402
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001401
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001400
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001396
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001392
Iteration 32/1000 | Loss: 0.00001392
Iteration 33/1000 | Loss: 0.00001391
Iteration 34/1000 | Loss: 0.00001391
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001388
Iteration 46/1000 | Loss: 0.00001388
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001387
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001386
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001386
Iteration 62/1000 | Loss: 0.00001386
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00001385
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00001385
Iteration 69/1000 | Loss: 0.00001384
Iteration 70/1000 | Loss: 0.00001384
Iteration 71/1000 | Loss: 0.00001384
Iteration 72/1000 | Loss: 0.00001384
Iteration 73/1000 | Loss: 0.00001384
Iteration 74/1000 | Loss: 0.00001384
Iteration 75/1000 | Loss: 0.00001384
Iteration 76/1000 | Loss: 0.00001384
Iteration 77/1000 | Loss: 0.00001384
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001384
Iteration 83/1000 | Loss: 0.00001384
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001384
Iteration 89/1000 | Loss: 0.00001384
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.3838966879120562e-05, 1.3838966879120562e-05, 1.3838966879120562e-05, 1.3838966879120562e-05, 1.3838966879120562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3838966879120562e-05

Optimization complete. Final v2v error: 3.2089645862579346 mm

Highest mean error: 3.445347309112549 mm for frame 78

Lowest mean error: 2.7480475902557373 mm for frame 14

Saving results

Total time: 32.75836968421936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746786
Iteration 2/25 | Loss: 0.00150415
Iteration 3/25 | Loss: 0.00128870
Iteration 4/25 | Loss: 0.00127577
Iteration 5/25 | Loss: 0.00127551
Iteration 6/25 | Loss: 0.00127551
Iteration 7/25 | Loss: 0.00127551
Iteration 8/25 | Loss: 0.00127551
Iteration 9/25 | Loss: 0.00127551
Iteration 10/25 | Loss: 0.00127551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012755085481330752, 0.0012755085481330752, 0.0012755085481330752, 0.0012755085481330752, 0.0012755085481330752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012755085481330752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.75008821
Iteration 2/25 | Loss: 0.00259561
Iteration 3/25 | Loss: 0.00259559
Iteration 4/25 | Loss: 0.00259559
Iteration 5/25 | Loss: 0.00259559
Iteration 6/25 | Loss: 0.00259559
Iteration 7/25 | Loss: 0.00259559
Iteration 8/25 | Loss: 0.00259559
Iteration 9/25 | Loss: 0.00259559
Iteration 10/25 | Loss: 0.00259559
Iteration 11/25 | Loss: 0.00259559
Iteration 12/25 | Loss: 0.00259559
Iteration 13/25 | Loss: 0.00259559
Iteration 14/25 | Loss: 0.00259559
Iteration 15/25 | Loss: 0.00259559
Iteration 16/25 | Loss: 0.00259559
Iteration 17/25 | Loss: 0.00259559
Iteration 18/25 | Loss: 0.00259559
Iteration 19/25 | Loss: 0.00259559
Iteration 20/25 | Loss: 0.00259559
Iteration 21/25 | Loss: 0.00259559
Iteration 22/25 | Loss: 0.00259559
Iteration 23/25 | Loss: 0.00259559
Iteration 24/25 | Loss: 0.00259559
Iteration 25/25 | Loss: 0.00259559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259559
Iteration 2/1000 | Loss: 0.00004724
Iteration 3/1000 | Loss: 0.00003142
Iteration 4/1000 | Loss: 0.00002777
Iteration 5/1000 | Loss: 0.00002499
Iteration 6/1000 | Loss: 0.00002320
Iteration 7/1000 | Loss: 0.00002208
Iteration 8/1000 | Loss: 0.00002118
Iteration 9/1000 | Loss: 0.00002072
Iteration 10/1000 | Loss: 0.00002036
Iteration 11/1000 | Loss: 0.00002035
Iteration 12/1000 | Loss: 0.00002034
Iteration 13/1000 | Loss: 0.00002022
Iteration 14/1000 | Loss: 0.00002005
Iteration 15/1000 | Loss: 0.00002001
Iteration 16/1000 | Loss: 0.00002001
Iteration 17/1000 | Loss: 0.00002001
Iteration 18/1000 | Loss: 0.00002000
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00001996
Iteration 21/1000 | Loss: 0.00001996
Iteration 22/1000 | Loss: 0.00001996
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001996
Iteration 26/1000 | Loss: 0.00001995
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001991
Iteration 29/1000 | Loss: 0.00001990
Iteration 30/1000 | Loss: 0.00001989
Iteration 31/1000 | Loss: 0.00001989
Iteration 32/1000 | Loss: 0.00001989
Iteration 33/1000 | Loss: 0.00001989
Iteration 34/1000 | Loss: 0.00001989
Iteration 35/1000 | Loss: 0.00001989
Iteration 36/1000 | Loss: 0.00001989
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001988
Iteration 40/1000 | Loss: 0.00001988
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001988
Iteration 43/1000 | Loss: 0.00001987
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001985
Iteration 46/1000 | Loss: 0.00001985
Iteration 47/1000 | Loss: 0.00001985
Iteration 48/1000 | Loss: 0.00001985
Iteration 49/1000 | Loss: 0.00001984
Iteration 50/1000 | Loss: 0.00001984
Iteration 51/1000 | Loss: 0.00001984
Iteration 52/1000 | Loss: 0.00001984
Iteration 53/1000 | Loss: 0.00001983
Iteration 54/1000 | Loss: 0.00001983
Iteration 55/1000 | Loss: 0.00001982
Iteration 56/1000 | Loss: 0.00001982
Iteration 57/1000 | Loss: 0.00001982
Iteration 58/1000 | Loss: 0.00001981
Iteration 59/1000 | Loss: 0.00001981
Iteration 60/1000 | Loss: 0.00001980
Iteration 61/1000 | Loss: 0.00001980
Iteration 62/1000 | Loss: 0.00001980
Iteration 63/1000 | Loss: 0.00001980
Iteration 64/1000 | Loss: 0.00001980
Iteration 65/1000 | Loss: 0.00001980
Iteration 66/1000 | Loss: 0.00001980
Iteration 67/1000 | Loss: 0.00001980
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001979
Iteration 70/1000 | Loss: 0.00001979
Iteration 71/1000 | Loss: 0.00001979
Iteration 72/1000 | Loss: 0.00001979
Iteration 73/1000 | Loss: 0.00001979
Iteration 74/1000 | Loss: 0.00001979
Iteration 75/1000 | Loss: 0.00001979
Iteration 76/1000 | Loss: 0.00001979
Iteration 77/1000 | Loss: 0.00001979
Iteration 78/1000 | Loss: 0.00001979
Iteration 79/1000 | Loss: 0.00001979
Iteration 80/1000 | Loss: 0.00001979
Iteration 81/1000 | Loss: 0.00001979
Iteration 82/1000 | Loss: 0.00001979
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00001979
Iteration 85/1000 | Loss: 0.00001979
Iteration 86/1000 | Loss: 0.00001979
Iteration 87/1000 | Loss: 0.00001979
Iteration 88/1000 | Loss: 0.00001979
Iteration 89/1000 | Loss: 0.00001979
Iteration 90/1000 | Loss: 0.00001979
Iteration 91/1000 | Loss: 0.00001979
Iteration 92/1000 | Loss: 0.00001979
Iteration 93/1000 | Loss: 0.00001979
Iteration 94/1000 | Loss: 0.00001979
Iteration 95/1000 | Loss: 0.00001979
Iteration 96/1000 | Loss: 0.00001979
Iteration 97/1000 | Loss: 0.00001979
Iteration 98/1000 | Loss: 0.00001979
Iteration 99/1000 | Loss: 0.00001979
Iteration 100/1000 | Loss: 0.00001979
Iteration 101/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.9790788428508677e-05, 1.9790788428508677e-05, 1.9790788428508677e-05, 1.9790788428508677e-05, 1.9790788428508677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9790788428508677e-05

Optimization complete. Final v2v error: 3.7186970710754395 mm

Highest mean error: 4.0723652839660645 mm for frame 142

Lowest mean error: 3.371553421020508 mm for frame 235

Saving results

Total time: 33.34661555290222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560664
Iteration 2/25 | Loss: 0.00138069
Iteration 3/25 | Loss: 0.00124124
Iteration 4/25 | Loss: 0.00122294
Iteration 5/25 | Loss: 0.00121883
Iteration 6/25 | Loss: 0.00121845
Iteration 7/25 | Loss: 0.00121845
Iteration 8/25 | Loss: 0.00121845
Iteration 9/25 | Loss: 0.00121845
Iteration 10/25 | Loss: 0.00121845
Iteration 11/25 | Loss: 0.00121845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012184521183371544, 0.0012184521183371544, 0.0012184521183371544, 0.0012184521183371544, 0.0012184521183371544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012184521183371544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73926306
Iteration 2/25 | Loss: 0.00254647
Iteration 3/25 | Loss: 0.00254646
Iteration 4/25 | Loss: 0.00254646
Iteration 5/25 | Loss: 0.00254646
Iteration 6/25 | Loss: 0.00254646
Iteration 7/25 | Loss: 0.00254646
Iteration 8/25 | Loss: 0.00254646
Iteration 9/25 | Loss: 0.00254646
Iteration 10/25 | Loss: 0.00254646
Iteration 11/25 | Loss: 0.00254646
Iteration 12/25 | Loss: 0.00254646
Iteration 13/25 | Loss: 0.00254646
Iteration 14/25 | Loss: 0.00254646
Iteration 15/25 | Loss: 0.00254646
Iteration 16/25 | Loss: 0.00254646
Iteration 17/25 | Loss: 0.00254646
Iteration 18/25 | Loss: 0.00254646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002546460134908557, 0.002546460134908557, 0.002546460134908557, 0.002546460134908557, 0.002546460134908557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002546460134908557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254646
Iteration 2/1000 | Loss: 0.00003982
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00002312
Iteration 5/1000 | Loss: 0.00002106
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001739
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001692
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001689
Iteration 14/1000 | Loss: 0.00001688
Iteration 15/1000 | Loss: 0.00001677
Iteration 16/1000 | Loss: 0.00001676
Iteration 17/1000 | Loss: 0.00001668
Iteration 18/1000 | Loss: 0.00001667
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001661
Iteration 22/1000 | Loss: 0.00001660
Iteration 23/1000 | Loss: 0.00001658
Iteration 24/1000 | Loss: 0.00001657
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001654
Iteration 28/1000 | Loss: 0.00001653
Iteration 29/1000 | Loss: 0.00001653
Iteration 30/1000 | Loss: 0.00001653
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001640
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001637
Iteration 38/1000 | Loss: 0.00001637
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001636
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001636
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00001636
Iteration 53/1000 | Loss: 0.00001635
Iteration 54/1000 | Loss: 0.00001635
Iteration 55/1000 | Loss: 0.00001635
Iteration 56/1000 | Loss: 0.00001635
Iteration 57/1000 | Loss: 0.00001635
Iteration 58/1000 | Loss: 0.00001635
Iteration 59/1000 | Loss: 0.00001635
Iteration 60/1000 | Loss: 0.00001634
Iteration 61/1000 | Loss: 0.00001634
Iteration 62/1000 | Loss: 0.00001634
Iteration 63/1000 | Loss: 0.00001634
Iteration 64/1000 | Loss: 0.00001633
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001633
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001632
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001632
Iteration 75/1000 | Loss: 0.00001632
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001630
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001628
Iteration 97/1000 | Loss: 0.00001628
Iteration 98/1000 | Loss: 0.00001628
Iteration 99/1000 | Loss: 0.00001628
Iteration 100/1000 | Loss: 0.00001628
Iteration 101/1000 | Loss: 0.00001628
Iteration 102/1000 | Loss: 0.00001628
Iteration 103/1000 | Loss: 0.00001628
Iteration 104/1000 | Loss: 0.00001628
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001628
Iteration 107/1000 | Loss: 0.00001628
Iteration 108/1000 | Loss: 0.00001628
Iteration 109/1000 | Loss: 0.00001628
Iteration 110/1000 | Loss: 0.00001627
Iteration 111/1000 | Loss: 0.00001627
Iteration 112/1000 | Loss: 0.00001627
Iteration 113/1000 | Loss: 0.00001627
Iteration 114/1000 | Loss: 0.00001627
Iteration 115/1000 | Loss: 0.00001627
Iteration 116/1000 | Loss: 0.00001627
Iteration 117/1000 | Loss: 0.00001627
Iteration 118/1000 | Loss: 0.00001627
Iteration 119/1000 | Loss: 0.00001627
Iteration 120/1000 | Loss: 0.00001627
Iteration 121/1000 | Loss: 0.00001627
Iteration 122/1000 | Loss: 0.00001627
Iteration 123/1000 | Loss: 0.00001626
Iteration 124/1000 | Loss: 0.00001626
Iteration 125/1000 | Loss: 0.00001626
Iteration 126/1000 | Loss: 0.00001626
Iteration 127/1000 | Loss: 0.00001626
Iteration 128/1000 | Loss: 0.00001626
Iteration 129/1000 | Loss: 0.00001626
Iteration 130/1000 | Loss: 0.00001626
Iteration 131/1000 | Loss: 0.00001626
Iteration 132/1000 | Loss: 0.00001626
Iteration 133/1000 | Loss: 0.00001625
Iteration 134/1000 | Loss: 0.00001625
Iteration 135/1000 | Loss: 0.00001625
Iteration 136/1000 | Loss: 0.00001625
Iteration 137/1000 | Loss: 0.00001625
Iteration 138/1000 | Loss: 0.00001625
Iteration 139/1000 | Loss: 0.00001624
Iteration 140/1000 | Loss: 0.00001624
Iteration 141/1000 | Loss: 0.00001624
Iteration 142/1000 | Loss: 0.00001624
Iteration 143/1000 | Loss: 0.00001624
Iteration 144/1000 | Loss: 0.00001624
Iteration 145/1000 | Loss: 0.00001624
Iteration 146/1000 | Loss: 0.00001624
Iteration 147/1000 | Loss: 0.00001624
Iteration 148/1000 | Loss: 0.00001624
Iteration 149/1000 | Loss: 0.00001624
Iteration 150/1000 | Loss: 0.00001624
Iteration 151/1000 | Loss: 0.00001624
Iteration 152/1000 | Loss: 0.00001624
Iteration 153/1000 | Loss: 0.00001624
Iteration 154/1000 | Loss: 0.00001624
Iteration 155/1000 | Loss: 0.00001624
Iteration 156/1000 | Loss: 0.00001624
Iteration 157/1000 | Loss: 0.00001624
Iteration 158/1000 | Loss: 0.00001624
Iteration 159/1000 | Loss: 0.00001624
Iteration 160/1000 | Loss: 0.00001624
Iteration 161/1000 | Loss: 0.00001624
Iteration 162/1000 | Loss: 0.00001624
Iteration 163/1000 | Loss: 0.00001624
Iteration 164/1000 | Loss: 0.00001624
Iteration 165/1000 | Loss: 0.00001624
Iteration 166/1000 | Loss: 0.00001624
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001624
Iteration 169/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.623536263650749e-05, 1.623536263650749e-05, 1.623536263650749e-05, 1.623536263650749e-05, 1.623536263650749e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.623536263650749e-05

Optimization complete. Final v2v error: 3.3881759643554688 mm

Highest mean error: 4.09960412979126 mm for frame 124

Lowest mean error: 3.0747034549713135 mm for frame 64

Saving results

Total time: 37.98031568527222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088441
Iteration 2/25 | Loss: 0.00265038
Iteration 3/25 | Loss: 0.00202043
Iteration 4/25 | Loss: 0.00165922
Iteration 5/25 | Loss: 0.00161874
Iteration 6/25 | Loss: 0.00155604
Iteration 7/25 | Loss: 0.00155161
Iteration 8/25 | Loss: 0.00144853
Iteration 9/25 | Loss: 0.00135552
Iteration 10/25 | Loss: 0.00130539
Iteration 11/25 | Loss: 0.00126502
Iteration 12/25 | Loss: 0.00123969
Iteration 13/25 | Loss: 0.00121559
Iteration 14/25 | Loss: 0.00125142
Iteration 15/25 | Loss: 0.00119447
Iteration 16/25 | Loss: 0.00119380
Iteration 17/25 | Loss: 0.00119234
Iteration 18/25 | Loss: 0.00119834
Iteration 19/25 | Loss: 0.00119090
Iteration 20/25 | Loss: 0.00119219
Iteration 21/25 | Loss: 0.00118984
Iteration 22/25 | Loss: 0.00118500
Iteration 23/25 | Loss: 0.00118129
Iteration 24/25 | Loss: 0.00118226
Iteration 25/25 | Loss: 0.00117890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22003663
Iteration 2/25 | Loss: 0.00361298
Iteration 3/25 | Loss: 0.00288580
Iteration 4/25 | Loss: 0.00288580
Iteration 5/25 | Loss: 0.00288580
Iteration 6/25 | Loss: 0.00288579
Iteration 7/25 | Loss: 0.00288579
Iteration 8/25 | Loss: 0.00288579
Iteration 9/25 | Loss: 0.00288579
Iteration 10/25 | Loss: 0.00288579
Iteration 11/25 | Loss: 0.00288579
Iteration 12/25 | Loss: 0.00288579
Iteration 13/25 | Loss: 0.00288579
Iteration 14/25 | Loss: 0.00288579
Iteration 15/25 | Loss: 0.00288579
Iteration 16/25 | Loss: 0.00288579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0028857935685664415, 0.0028857935685664415, 0.0028857935685664415, 0.0028857935685664415, 0.0028857935685664415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028857935685664415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288579
Iteration 2/1000 | Loss: 0.00037575
Iteration 3/1000 | Loss: 0.00007639
Iteration 4/1000 | Loss: 0.00039649
Iteration 5/1000 | Loss: 0.00028336
Iteration 6/1000 | Loss: 0.00019542
Iteration 7/1000 | Loss: 0.00015639
Iteration 8/1000 | Loss: 0.00005959
Iteration 9/1000 | Loss: 0.00006343
Iteration 10/1000 | Loss: 0.00128698
Iteration 11/1000 | Loss: 0.00037734
Iteration 12/1000 | Loss: 0.00120605
Iteration 13/1000 | Loss: 0.00005442
Iteration 14/1000 | Loss: 0.00004121
Iteration 15/1000 | Loss: 0.00003684
Iteration 16/1000 | Loss: 0.00003485
Iteration 17/1000 | Loss: 0.00087103
Iteration 18/1000 | Loss: 0.00126233
Iteration 19/1000 | Loss: 0.00081636
Iteration 20/1000 | Loss: 0.00091708
Iteration 21/1000 | Loss: 0.00123658
Iteration 22/1000 | Loss: 0.00007915
Iteration 23/1000 | Loss: 0.00004274
Iteration 24/1000 | Loss: 0.00003011
Iteration 25/1000 | Loss: 0.00002458
Iteration 26/1000 | Loss: 0.00002051
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001661
Iteration 30/1000 | Loss: 0.00001607
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001529
Iteration 33/1000 | Loss: 0.00001513
Iteration 34/1000 | Loss: 0.00001488
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001458
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001449
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001443
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001434
Iteration 70/1000 | Loss: 0.00001434
Iteration 71/1000 | Loss: 0.00001434
Iteration 72/1000 | Loss: 0.00001434
Iteration 73/1000 | Loss: 0.00001434
Iteration 74/1000 | Loss: 0.00001434
Iteration 75/1000 | Loss: 0.00001433
Iteration 76/1000 | Loss: 0.00001433
Iteration 77/1000 | Loss: 0.00001433
Iteration 78/1000 | Loss: 0.00001433
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001432
Iteration 85/1000 | Loss: 0.00001432
Iteration 86/1000 | Loss: 0.00001432
Iteration 87/1000 | Loss: 0.00001432
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001431
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001431
Iteration 100/1000 | Loss: 0.00001431
Iteration 101/1000 | Loss: 0.00001431
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001431
Iteration 104/1000 | Loss: 0.00001431
Iteration 105/1000 | Loss: 0.00001431
Iteration 106/1000 | Loss: 0.00001431
Iteration 107/1000 | Loss: 0.00001431
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001430
Iteration 113/1000 | Loss: 0.00001430
Iteration 114/1000 | Loss: 0.00001430
Iteration 115/1000 | Loss: 0.00001430
Iteration 116/1000 | Loss: 0.00001430
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001430
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001430
Iteration 122/1000 | Loss: 0.00001430
Iteration 123/1000 | Loss: 0.00001430
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Iteration 126/1000 | Loss: 0.00001430
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001429
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001428
Iteration 137/1000 | Loss: 0.00001428
Iteration 138/1000 | Loss: 0.00001428
Iteration 139/1000 | Loss: 0.00001428
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001428
Iteration 146/1000 | Loss: 0.00001428
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001427
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001427
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001427
Iteration 162/1000 | Loss: 0.00001426
Iteration 163/1000 | Loss: 0.00001426
Iteration 164/1000 | Loss: 0.00001426
Iteration 165/1000 | Loss: 0.00001426
Iteration 166/1000 | Loss: 0.00001426
Iteration 167/1000 | Loss: 0.00001426
Iteration 168/1000 | Loss: 0.00001426
Iteration 169/1000 | Loss: 0.00001425
Iteration 170/1000 | Loss: 0.00001425
Iteration 171/1000 | Loss: 0.00001425
Iteration 172/1000 | Loss: 0.00001425
Iteration 173/1000 | Loss: 0.00001425
Iteration 174/1000 | Loss: 0.00001425
Iteration 175/1000 | Loss: 0.00001425
Iteration 176/1000 | Loss: 0.00001425
Iteration 177/1000 | Loss: 0.00001425
Iteration 178/1000 | Loss: 0.00001425
Iteration 179/1000 | Loss: 0.00001425
Iteration 180/1000 | Loss: 0.00001425
Iteration 181/1000 | Loss: 0.00001424
Iteration 182/1000 | Loss: 0.00001424
Iteration 183/1000 | Loss: 0.00001424
Iteration 184/1000 | Loss: 0.00001424
Iteration 185/1000 | Loss: 0.00001424
Iteration 186/1000 | Loss: 0.00001424
Iteration 187/1000 | Loss: 0.00001424
Iteration 188/1000 | Loss: 0.00001424
Iteration 189/1000 | Loss: 0.00001424
Iteration 190/1000 | Loss: 0.00001424
Iteration 191/1000 | Loss: 0.00001424
Iteration 192/1000 | Loss: 0.00001424
Iteration 193/1000 | Loss: 0.00001424
Iteration 194/1000 | Loss: 0.00001424
Iteration 195/1000 | Loss: 0.00001424
Iteration 196/1000 | Loss: 0.00001424
Iteration 197/1000 | Loss: 0.00001424
Iteration 198/1000 | Loss: 0.00001424
Iteration 199/1000 | Loss: 0.00001424
Iteration 200/1000 | Loss: 0.00001424
Iteration 201/1000 | Loss: 0.00001424
Iteration 202/1000 | Loss: 0.00001424
Iteration 203/1000 | Loss: 0.00001424
Iteration 204/1000 | Loss: 0.00001424
Iteration 205/1000 | Loss: 0.00001424
Iteration 206/1000 | Loss: 0.00001424
Iteration 207/1000 | Loss: 0.00001424
Iteration 208/1000 | Loss: 0.00001424
Iteration 209/1000 | Loss: 0.00001424
Iteration 210/1000 | Loss: 0.00001424
Iteration 211/1000 | Loss: 0.00001424
Iteration 212/1000 | Loss: 0.00001424
Iteration 213/1000 | Loss: 0.00001424
Iteration 214/1000 | Loss: 0.00001424
Iteration 215/1000 | Loss: 0.00001424
Iteration 216/1000 | Loss: 0.00001424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.423573758074781e-05, 1.423573758074781e-05, 1.423573758074781e-05, 1.423573758074781e-05, 1.423573758074781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.423573758074781e-05

Optimization complete. Final v2v error: 3.0853891372680664 mm

Highest mean error: 8.380437850952148 mm for frame 50

Lowest mean error: 2.8055264949798584 mm for frame 7

Saving results

Total time: 107.22791814804077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390935
Iteration 2/25 | Loss: 0.00167551
Iteration 3/25 | Loss: 0.00132600
Iteration 4/25 | Loss: 0.00128666
Iteration 5/25 | Loss: 0.00127829
Iteration 6/25 | Loss: 0.00127662
Iteration 7/25 | Loss: 0.00127634
Iteration 8/25 | Loss: 0.00127634
Iteration 9/25 | Loss: 0.00127634
Iteration 10/25 | Loss: 0.00127634
Iteration 11/25 | Loss: 0.00127634
Iteration 12/25 | Loss: 0.00127634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001276336726732552, 0.001276336726732552, 0.001276336726732552, 0.001276336726732552, 0.001276336726732552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001276336726732552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08263528
Iteration 2/25 | Loss: 0.00305884
Iteration 3/25 | Loss: 0.00305884
Iteration 4/25 | Loss: 0.00305884
Iteration 5/25 | Loss: 0.00305884
Iteration 6/25 | Loss: 0.00305884
Iteration 7/25 | Loss: 0.00305884
Iteration 8/25 | Loss: 0.00305884
Iteration 9/25 | Loss: 0.00305884
Iteration 10/25 | Loss: 0.00305884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.003058837726712227, 0.003058837726712227, 0.003058837726712227, 0.003058837726712227, 0.003058837726712227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003058837726712227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00305884
Iteration 2/1000 | Loss: 0.00004354
Iteration 3/1000 | Loss: 0.00002289
Iteration 4/1000 | Loss: 0.00001753
Iteration 5/1000 | Loss: 0.00001605
Iteration 6/1000 | Loss: 0.00001488
Iteration 7/1000 | Loss: 0.00001444
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001378
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001334
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001324
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001322
Iteration 23/1000 | Loss: 0.00001322
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001319
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001309
Iteration 45/1000 | Loss: 0.00001309
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001303
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001302
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001300
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001300
Iteration 66/1000 | Loss: 0.00001300
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001299
Iteration 69/1000 | Loss: 0.00001299
Iteration 70/1000 | Loss: 0.00001299
Iteration 71/1000 | Loss: 0.00001299
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001299
Iteration 76/1000 | Loss: 0.00001299
Iteration 77/1000 | Loss: 0.00001299
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001297
Iteration 85/1000 | Loss: 0.00001297
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001296
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001296
Iteration 91/1000 | Loss: 0.00001296
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001293
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001293
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001292
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001291
Iteration 113/1000 | Loss: 0.00001291
Iteration 114/1000 | Loss: 0.00001291
Iteration 115/1000 | Loss: 0.00001291
Iteration 116/1000 | Loss: 0.00001291
Iteration 117/1000 | Loss: 0.00001291
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001290
Iteration 121/1000 | Loss: 0.00001290
Iteration 122/1000 | Loss: 0.00001290
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001290
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001289
Iteration 128/1000 | Loss: 0.00001289
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001288
Iteration 134/1000 | Loss: 0.00001288
Iteration 135/1000 | Loss: 0.00001288
Iteration 136/1000 | Loss: 0.00001288
Iteration 137/1000 | Loss: 0.00001288
Iteration 138/1000 | Loss: 0.00001288
Iteration 139/1000 | Loss: 0.00001288
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001287
Iteration 143/1000 | Loss: 0.00001287
Iteration 144/1000 | Loss: 0.00001287
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001287
Iteration 149/1000 | Loss: 0.00001287
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001286
Iteration 153/1000 | Loss: 0.00001286
Iteration 154/1000 | Loss: 0.00001286
Iteration 155/1000 | Loss: 0.00001286
Iteration 156/1000 | Loss: 0.00001286
Iteration 157/1000 | Loss: 0.00001286
Iteration 158/1000 | Loss: 0.00001286
Iteration 159/1000 | Loss: 0.00001286
Iteration 160/1000 | Loss: 0.00001285
Iteration 161/1000 | Loss: 0.00001285
Iteration 162/1000 | Loss: 0.00001285
Iteration 163/1000 | Loss: 0.00001285
Iteration 164/1000 | Loss: 0.00001285
Iteration 165/1000 | Loss: 0.00001285
Iteration 166/1000 | Loss: 0.00001285
Iteration 167/1000 | Loss: 0.00001284
Iteration 168/1000 | Loss: 0.00001284
Iteration 169/1000 | Loss: 0.00001284
Iteration 170/1000 | Loss: 0.00001284
Iteration 171/1000 | Loss: 0.00001284
Iteration 172/1000 | Loss: 0.00001284
Iteration 173/1000 | Loss: 0.00001284
Iteration 174/1000 | Loss: 0.00001284
Iteration 175/1000 | Loss: 0.00001284
Iteration 176/1000 | Loss: 0.00001284
Iteration 177/1000 | Loss: 0.00001284
Iteration 178/1000 | Loss: 0.00001284
Iteration 179/1000 | Loss: 0.00001284
Iteration 180/1000 | Loss: 0.00001284
Iteration 181/1000 | Loss: 0.00001284
Iteration 182/1000 | Loss: 0.00001283
Iteration 183/1000 | Loss: 0.00001283
Iteration 184/1000 | Loss: 0.00001283
Iteration 185/1000 | Loss: 0.00001283
Iteration 186/1000 | Loss: 0.00001283
Iteration 187/1000 | Loss: 0.00001283
Iteration 188/1000 | Loss: 0.00001283
Iteration 189/1000 | Loss: 0.00001283
Iteration 190/1000 | Loss: 0.00001283
Iteration 191/1000 | Loss: 0.00001283
Iteration 192/1000 | Loss: 0.00001283
Iteration 193/1000 | Loss: 0.00001283
Iteration 194/1000 | Loss: 0.00001283
Iteration 195/1000 | Loss: 0.00001283
Iteration 196/1000 | Loss: 0.00001283
Iteration 197/1000 | Loss: 0.00001283
Iteration 198/1000 | Loss: 0.00001283
Iteration 199/1000 | Loss: 0.00001282
Iteration 200/1000 | Loss: 0.00001282
Iteration 201/1000 | Loss: 0.00001282
Iteration 202/1000 | Loss: 0.00001282
Iteration 203/1000 | Loss: 0.00001282
Iteration 204/1000 | Loss: 0.00001282
Iteration 205/1000 | Loss: 0.00001282
Iteration 206/1000 | Loss: 0.00001282
Iteration 207/1000 | Loss: 0.00001282
Iteration 208/1000 | Loss: 0.00001282
Iteration 209/1000 | Loss: 0.00001282
Iteration 210/1000 | Loss: 0.00001282
Iteration 211/1000 | Loss: 0.00001282
Iteration 212/1000 | Loss: 0.00001282
Iteration 213/1000 | Loss: 0.00001282
Iteration 214/1000 | Loss: 0.00001282
Iteration 215/1000 | Loss: 0.00001282
Iteration 216/1000 | Loss: 0.00001282
Iteration 217/1000 | Loss: 0.00001282
Iteration 218/1000 | Loss: 0.00001282
Iteration 219/1000 | Loss: 0.00001282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.2823470569856e-05, 1.2823470569856e-05, 1.2823470569856e-05, 1.2823470569856e-05, 1.2823470569856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2823470569856e-05

Optimization complete. Final v2v error: 3.0724616050720215 mm

Highest mean error: 3.2990176677703857 mm for frame 13

Lowest mean error: 2.8795387744903564 mm for frame 4

Saving results

Total time: 38.542155027389526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431637
Iteration 2/25 | Loss: 0.00128214
Iteration 3/25 | Loss: 0.00119578
Iteration 4/25 | Loss: 0.00118012
Iteration 5/25 | Loss: 0.00117582
Iteration 6/25 | Loss: 0.00117507
Iteration 7/25 | Loss: 0.00117507
Iteration 8/25 | Loss: 0.00117507
Iteration 9/25 | Loss: 0.00117507
Iteration 10/25 | Loss: 0.00117507
Iteration 11/25 | Loss: 0.00117507
Iteration 12/25 | Loss: 0.00117507
Iteration 13/25 | Loss: 0.00117507
Iteration 14/25 | Loss: 0.00117507
Iteration 15/25 | Loss: 0.00117507
Iteration 16/25 | Loss: 0.00117507
Iteration 17/25 | Loss: 0.00117507
Iteration 18/25 | Loss: 0.00117507
Iteration 19/25 | Loss: 0.00117507
Iteration 20/25 | Loss: 0.00117507
Iteration 21/25 | Loss: 0.00117507
Iteration 22/25 | Loss: 0.00117507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011750743724405766, 0.0011750743724405766, 0.0011750743724405766, 0.0011750743724405766, 0.0011750743724405766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011750743724405766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22976625
Iteration 2/25 | Loss: 0.00280755
Iteration 3/25 | Loss: 0.00280755
Iteration 4/25 | Loss: 0.00280755
Iteration 5/25 | Loss: 0.00280755
Iteration 6/25 | Loss: 0.00280755
Iteration 7/25 | Loss: 0.00280754
Iteration 8/25 | Loss: 0.00280754
Iteration 9/25 | Loss: 0.00280754
Iteration 10/25 | Loss: 0.00280754
Iteration 11/25 | Loss: 0.00280754
Iteration 12/25 | Loss: 0.00280754
Iteration 13/25 | Loss: 0.00280754
Iteration 14/25 | Loss: 0.00280754
Iteration 15/25 | Loss: 0.00280754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002807543845847249, 0.002807543845847249, 0.002807543845847249, 0.002807543845847249, 0.002807543845847249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002807543845847249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280754
Iteration 2/1000 | Loss: 0.00003773
Iteration 3/1000 | Loss: 0.00002349
Iteration 4/1000 | Loss: 0.00002118
Iteration 5/1000 | Loss: 0.00001930
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001682
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001566
Iteration 11/1000 | Loss: 0.00001551
Iteration 12/1000 | Loss: 0.00001539
Iteration 13/1000 | Loss: 0.00001538
Iteration 14/1000 | Loss: 0.00001537
Iteration 15/1000 | Loss: 0.00001536
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001529
Iteration 18/1000 | Loss: 0.00001529
Iteration 19/1000 | Loss: 0.00001529
Iteration 20/1000 | Loss: 0.00001528
Iteration 21/1000 | Loss: 0.00001517
Iteration 22/1000 | Loss: 0.00001516
Iteration 23/1000 | Loss: 0.00001515
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001514
Iteration 26/1000 | Loss: 0.00001512
Iteration 27/1000 | Loss: 0.00001508
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001508
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001507
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001507
Iteration 37/1000 | Loss: 0.00001507
Iteration 38/1000 | Loss: 0.00001507
Iteration 39/1000 | Loss: 0.00001507
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001506
Iteration 42/1000 | Loss: 0.00001506
Iteration 43/1000 | Loss: 0.00001505
Iteration 44/1000 | Loss: 0.00001505
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001505
Iteration 47/1000 | Loss: 0.00001505
Iteration 48/1000 | Loss: 0.00001505
Iteration 49/1000 | Loss: 0.00001505
Iteration 50/1000 | Loss: 0.00001505
Iteration 51/1000 | Loss: 0.00001505
Iteration 52/1000 | Loss: 0.00001505
Iteration 53/1000 | Loss: 0.00001505
Iteration 54/1000 | Loss: 0.00001505
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.5052554772410076e-05, 1.5052554772410076e-05, 1.5052554772410076e-05, 1.5052554772410076e-05, 1.5052554772410076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5052554772410076e-05

Optimization complete. Final v2v error: 3.2775299549102783 mm

Highest mean error: 3.6301186084747314 mm for frame 122

Lowest mean error: 2.9708895683288574 mm for frame 11

Saving results

Total time: 30.170480012893677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002564
Iteration 2/25 | Loss: 0.00266266
Iteration 3/25 | Loss: 0.00192521
Iteration 4/25 | Loss: 0.00188031
Iteration 5/25 | Loss: 0.00189811
Iteration 6/25 | Loss: 0.00193954
Iteration 7/25 | Loss: 0.00150969
Iteration 8/25 | Loss: 0.00142268
Iteration 9/25 | Loss: 0.00139768
Iteration 10/25 | Loss: 0.00139093
Iteration 11/25 | Loss: 0.00138473
Iteration 12/25 | Loss: 0.00137884
Iteration 13/25 | Loss: 0.00137002
Iteration 14/25 | Loss: 0.00136104
Iteration 15/25 | Loss: 0.00134749
Iteration 16/25 | Loss: 0.00134170
Iteration 17/25 | Loss: 0.00133973
Iteration 18/25 | Loss: 0.00133926
Iteration 19/25 | Loss: 0.00133901
Iteration 20/25 | Loss: 0.00133889
Iteration 21/25 | Loss: 0.00133882
Iteration 22/25 | Loss: 0.00133881
Iteration 23/25 | Loss: 0.00133881
Iteration 24/25 | Loss: 0.00133881
Iteration 25/25 | Loss: 0.00133881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10582018
Iteration 2/25 | Loss: 0.00298676
Iteration 3/25 | Loss: 0.00298676
Iteration 4/25 | Loss: 0.00298676
Iteration 5/25 | Loss: 0.00298676
Iteration 6/25 | Loss: 0.00298676
Iteration 7/25 | Loss: 0.00298676
Iteration 8/25 | Loss: 0.00298676
Iteration 9/25 | Loss: 0.00298676
Iteration 10/25 | Loss: 0.00298676
Iteration 11/25 | Loss: 0.00298676
Iteration 12/25 | Loss: 0.00298676
Iteration 13/25 | Loss: 0.00298676
Iteration 14/25 | Loss: 0.00298676
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0029867568518966436, 0.0029867568518966436, 0.0029867568518966436, 0.0029867568518966436, 0.0029867568518966436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029867568518966436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298676
Iteration 2/1000 | Loss: 0.00147943
Iteration 3/1000 | Loss: 0.00079219
Iteration 4/1000 | Loss: 0.00118246
Iteration 5/1000 | Loss: 0.00070262
Iteration 6/1000 | Loss: 0.00071120
Iteration 7/1000 | Loss: 0.00070165
Iteration 8/1000 | Loss: 0.00099350
Iteration 9/1000 | Loss: 0.00028089
Iteration 10/1000 | Loss: 0.00009958
Iteration 11/1000 | Loss: 0.00008110
Iteration 12/1000 | Loss: 0.00031665
Iteration 13/1000 | Loss: 0.00042568
Iteration 14/1000 | Loss: 0.00030992
Iteration 15/1000 | Loss: 0.00065141
Iteration 16/1000 | Loss: 0.00025344
Iteration 17/1000 | Loss: 0.00009481
Iteration 18/1000 | Loss: 0.00007959
Iteration 19/1000 | Loss: 0.00009256
Iteration 20/1000 | Loss: 0.00014358
Iteration 21/1000 | Loss: 0.00006311
Iteration 22/1000 | Loss: 0.00005212
Iteration 23/1000 | Loss: 0.00006228
Iteration 24/1000 | Loss: 0.00029709
Iteration 25/1000 | Loss: 0.00017534
Iteration 26/1000 | Loss: 0.00029318
Iteration 27/1000 | Loss: 0.00039065
Iteration 28/1000 | Loss: 0.00004739
Iteration 29/1000 | Loss: 0.00005004
Iteration 30/1000 | Loss: 0.00004305
Iteration 31/1000 | Loss: 0.00004210
Iteration 32/1000 | Loss: 0.00004061
Iteration 33/1000 | Loss: 0.00003900
Iteration 34/1000 | Loss: 0.00027430
Iteration 35/1000 | Loss: 0.00078231
Iteration 36/1000 | Loss: 0.00094594
Iteration 37/1000 | Loss: 0.00022029
Iteration 38/1000 | Loss: 0.00027695
Iteration 39/1000 | Loss: 0.00016941
Iteration 40/1000 | Loss: 0.00024506
Iteration 41/1000 | Loss: 0.00012719
Iteration 42/1000 | Loss: 0.00026914
Iteration 43/1000 | Loss: 0.00010419
Iteration 44/1000 | Loss: 0.00025053
Iteration 45/1000 | Loss: 0.00009565
Iteration 46/1000 | Loss: 0.00022656
Iteration 47/1000 | Loss: 0.00003713
Iteration 48/1000 | Loss: 0.00003323
Iteration 49/1000 | Loss: 0.00002947
Iteration 50/1000 | Loss: 0.00002762
Iteration 51/1000 | Loss: 0.00002588
Iteration 52/1000 | Loss: 0.00002444
Iteration 53/1000 | Loss: 0.00002358
Iteration 54/1000 | Loss: 0.00002295
Iteration 55/1000 | Loss: 0.00002230
Iteration 56/1000 | Loss: 0.00002170
Iteration 57/1000 | Loss: 0.00002101
Iteration 58/1000 | Loss: 0.00002045
Iteration 59/1000 | Loss: 0.00001985
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001908
Iteration 63/1000 | Loss: 0.00001907
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001903
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001900
Iteration 72/1000 | Loss: 0.00024230
Iteration 73/1000 | Loss: 0.00006200
Iteration 74/1000 | Loss: 0.00021829
Iteration 75/1000 | Loss: 0.00007746
Iteration 76/1000 | Loss: 0.00005076
Iteration 77/1000 | Loss: 0.00005404
Iteration 78/1000 | Loss: 0.00002373
Iteration 79/1000 | Loss: 0.00002098
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001634
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001611
Iteration 86/1000 | Loss: 0.00001599
Iteration 87/1000 | Loss: 0.00001597
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001594
Iteration 91/1000 | Loss: 0.00001592
Iteration 92/1000 | Loss: 0.00001592
Iteration 93/1000 | Loss: 0.00001592
Iteration 94/1000 | Loss: 0.00001592
Iteration 95/1000 | Loss: 0.00001591
Iteration 96/1000 | Loss: 0.00001591
Iteration 97/1000 | Loss: 0.00001590
Iteration 98/1000 | Loss: 0.00001590
Iteration 99/1000 | Loss: 0.00001590
Iteration 100/1000 | Loss: 0.00001590
Iteration 101/1000 | Loss: 0.00001590
Iteration 102/1000 | Loss: 0.00001590
Iteration 103/1000 | Loss: 0.00001589
Iteration 104/1000 | Loss: 0.00001589
Iteration 105/1000 | Loss: 0.00001588
Iteration 106/1000 | Loss: 0.00001588
Iteration 107/1000 | Loss: 0.00001588
Iteration 108/1000 | Loss: 0.00001588
Iteration 109/1000 | Loss: 0.00001587
Iteration 110/1000 | Loss: 0.00001587
Iteration 111/1000 | Loss: 0.00001587
Iteration 112/1000 | Loss: 0.00001587
Iteration 113/1000 | Loss: 0.00001587
Iteration 114/1000 | Loss: 0.00001587
Iteration 115/1000 | Loss: 0.00001587
Iteration 116/1000 | Loss: 0.00001586
Iteration 117/1000 | Loss: 0.00001586
Iteration 118/1000 | Loss: 0.00001586
Iteration 119/1000 | Loss: 0.00001585
Iteration 120/1000 | Loss: 0.00001585
Iteration 121/1000 | Loss: 0.00001585
Iteration 122/1000 | Loss: 0.00001585
Iteration 123/1000 | Loss: 0.00001585
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001584
Iteration 129/1000 | Loss: 0.00001583
Iteration 130/1000 | Loss: 0.00001583
Iteration 131/1000 | Loss: 0.00001583
Iteration 132/1000 | Loss: 0.00001583
Iteration 133/1000 | Loss: 0.00001583
Iteration 134/1000 | Loss: 0.00001583
Iteration 135/1000 | Loss: 0.00001583
Iteration 136/1000 | Loss: 0.00001582
Iteration 137/1000 | Loss: 0.00001582
Iteration 138/1000 | Loss: 0.00001582
Iteration 139/1000 | Loss: 0.00001582
Iteration 140/1000 | Loss: 0.00001582
Iteration 141/1000 | Loss: 0.00001582
Iteration 142/1000 | Loss: 0.00001581
Iteration 143/1000 | Loss: 0.00001581
Iteration 144/1000 | Loss: 0.00001581
Iteration 145/1000 | Loss: 0.00001581
Iteration 146/1000 | Loss: 0.00001581
Iteration 147/1000 | Loss: 0.00001581
Iteration 148/1000 | Loss: 0.00001581
Iteration 149/1000 | Loss: 0.00001581
Iteration 150/1000 | Loss: 0.00001581
Iteration 151/1000 | Loss: 0.00001581
Iteration 152/1000 | Loss: 0.00001581
Iteration 153/1000 | Loss: 0.00001581
Iteration 154/1000 | Loss: 0.00001581
Iteration 155/1000 | Loss: 0.00001581
Iteration 156/1000 | Loss: 0.00001580
Iteration 157/1000 | Loss: 0.00001580
Iteration 158/1000 | Loss: 0.00001580
Iteration 159/1000 | Loss: 0.00001580
Iteration 160/1000 | Loss: 0.00001580
Iteration 161/1000 | Loss: 0.00001580
Iteration 162/1000 | Loss: 0.00001580
Iteration 163/1000 | Loss: 0.00001580
Iteration 164/1000 | Loss: 0.00001580
Iteration 165/1000 | Loss: 0.00001580
Iteration 166/1000 | Loss: 0.00001580
Iteration 167/1000 | Loss: 0.00001580
Iteration 168/1000 | Loss: 0.00001580
Iteration 169/1000 | Loss: 0.00001580
Iteration 170/1000 | Loss: 0.00001580
Iteration 171/1000 | Loss: 0.00001580
Iteration 172/1000 | Loss: 0.00001580
Iteration 173/1000 | Loss: 0.00001580
Iteration 174/1000 | Loss: 0.00001580
Iteration 175/1000 | Loss: 0.00001580
Iteration 176/1000 | Loss: 0.00001580
Iteration 177/1000 | Loss: 0.00001580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.580392745381687e-05, 1.580392745381687e-05, 1.580392745381687e-05, 1.580392745381687e-05, 1.580392745381687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.580392745381687e-05

Optimization complete. Final v2v error: 3.2351837158203125 mm

Highest mean error: 5.9675374031066895 mm for frame 103

Lowest mean error: 2.772549867630005 mm for frame 121

Saving results

Total time: 144.9037470817566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437831
Iteration 2/25 | Loss: 0.00135156
Iteration 3/25 | Loss: 0.00122396
Iteration 4/25 | Loss: 0.00121061
Iteration 5/25 | Loss: 0.00120795
Iteration 6/25 | Loss: 0.00120795
Iteration 7/25 | Loss: 0.00120795
Iteration 8/25 | Loss: 0.00120795
Iteration 9/25 | Loss: 0.00120795
Iteration 10/25 | Loss: 0.00120795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012079476146027446, 0.0012079476146027446, 0.0012079476146027446, 0.0012079476146027446, 0.0012079476146027446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012079476146027446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15123701
Iteration 2/25 | Loss: 0.00268770
Iteration 3/25 | Loss: 0.00268770
Iteration 4/25 | Loss: 0.00268770
Iteration 5/25 | Loss: 0.00268770
Iteration 6/25 | Loss: 0.00268770
Iteration 7/25 | Loss: 0.00268770
Iteration 8/25 | Loss: 0.00268770
Iteration 9/25 | Loss: 0.00268770
Iteration 10/25 | Loss: 0.00268770
Iteration 11/25 | Loss: 0.00268770
Iteration 12/25 | Loss: 0.00268770
Iteration 13/25 | Loss: 0.00268770
Iteration 14/25 | Loss: 0.00268770
Iteration 15/25 | Loss: 0.00268770
Iteration 16/25 | Loss: 0.00268770
Iteration 17/25 | Loss: 0.00268770
Iteration 18/25 | Loss: 0.00268770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002687695901840925, 0.002687695901840925, 0.002687695901840925, 0.002687695901840925, 0.002687695901840925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002687695901840925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268770
Iteration 2/1000 | Loss: 0.00002467
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001550
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001377
Iteration 7/1000 | Loss: 0.00001339
Iteration 8/1000 | Loss: 0.00001293
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001264
Iteration 11/1000 | Loss: 0.00001244
Iteration 12/1000 | Loss: 0.00001244
Iteration 13/1000 | Loss: 0.00001232
Iteration 14/1000 | Loss: 0.00001231
Iteration 15/1000 | Loss: 0.00001222
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001219
Iteration 20/1000 | Loss: 0.00001219
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001218
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001215
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001214
Iteration 27/1000 | Loss: 0.00001213
Iteration 28/1000 | Loss: 0.00001213
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001211
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001209
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001207
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001204
Iteration 43/1000 | Loss: 0.00001204
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001203
Iteration 52/1000 | Loss: 0.00001203
Iteration 53/1000 | Loss: 0.00001203
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001202
Iteration 62/1000 | Loss: 0.00001202
Iteration 63/1000 | Loss: 0.00001202
Iteration 64/1000 | Loss: 0.00001202
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001201
Iteration 68/1000 | Loss: 0.00001201
Iteration 69/1000 | Loss: 0.00001201
Iteration 70/1000 | Loss: 0.00001201
Iteration 71/1000 | Loss: 0.00001200
Iteration 72/1000 | Loss: 0.00001200
Iteration 73/1000 | Loss: 0.00001200
Iteration 74/1000 | Loss: 0.00001200
Iteration 75/1000 | Loss: 0.00001200
Iteration 76/1000 | Loss: 0.00001200
Iteration 77/1000 | Loss: 0.00001199
Iteration 78/1000 | Loss: 0.00001199
Iteration 79/1000 | Loss: 0.00001199
Iteration 80/1000 | Loss: 0.00001199
Iteration 81/1000 | Loss: 0.00001199
Iteration 82/1000 | Loss: 0.00001199
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001199
Iteration 87/1000 | Loss: 0.00001199
Iteration 88/1000 | Loss: 0.00001199
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001198
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001197
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001195
Iteration 124/1000 | Loss: 0.00001195
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001194
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001194
Iteration 134/1000 | Loss: 0.00001194
Iteration 135/1000 | Loss: 0.00001194
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001194
Iteration 138/1000 | Loss: 0.00001194
Iteration 139/1000 | Loss: 0.00001194
Iteration 140/1000 | Loss: 0.00001194
Iteration 141/1000 | Loss: 0.00001194
Iteration 142/1000 | Loss: 0.00001194
Iteration 143/1000 | Loss: 0.00001194
Iteration 144/1000 | Loss: 0.00001194
Iteration 145/1000 | Loss: 0.00001194
Iteration 146/1000 | Loss: 0.00001194
Iteration 147/1000 | Loss: 0.00001194
Iteration 148/1000 | Loss: 0.00001194
Iteration 149/1000 | Loss: 0.00001194
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.1935208931390662e-05, 1.1935208931390662e-05, 1.1935208931390662e-05, 1.1935208931390662e-05, 1.1935208931390662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1935208931390662e-05

Optimization complete. Final v2v error: 2.943673849105835 mm

Highest mean error: 3.18658709526062 mm for frame 18

Lowest mean error: 2.6743156909942627 mm for frame 222

Saving results

Total time: 37.522470474243164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074698
Iteration 2/25 | Loss: 0.00177318
Iteration 3/25 | Loss: 0.00129295
Iteration 4/25 | Loss: 0.00127198
Iteration 5/25 | Loss: 0.00126786
Iteration 6/25 | Loss: 0.00126687
Iteration 7/25 | Loss: 0.00126687
Iteration 8/25 | Loss: 0.00126687
Iteration 9/25 | Loss: 0.00126687
Iteration 10/25 | Loss: 0.00126687
Iteration 11/25 | Loss: 0.00126687
Iteration 12/25 | Loss: 0.00126687
Iteration 13/25 | Loss: 0.00126687
Iteration 14/25 | Loss: 0.00126687
Iteration 15/25 | Loss: 0.00126687
Iteration 16/25 | Loss: 0.00126687
Iteration 17/25 | Loss: 0.00126687
Iteration 18/25 | Loss: 0.00126687
Iteration 19/25 | Loss: 0.00126687
Iteration 20/25 | Loss: 0.00126687
Iteration 21/25 | Loss: 0.00126687
Iteration 22/25 | Loss: 0.00126687
Iteration 23/25 | Loss: 0.00126687
Iteration 24/25 | Loss: 0.00126687
Iteration 25/25 | Loss: 0.00126687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43249282
Iteration 2/25 | Loss: 0.00172615
Iteration 3/25 | Loss: 0.00172615
Iteration 4/25 | Loss: 0.00172615
Iteration 5/25 | Loss: 0.00172615
Iteration 6/25 | Loss: 0.00172615
Iteration 7/25 | Loss: 0.00172615
Iteration 8/25 | Loss: 0.00172615
Iteration 9/25 | Loss: 0.00172615
Iteration 10/25 | Loss: 0.00172615
Iteration 11/25 | Loss: 0.00172614
Iteration 12/25 | Loss: 0.00172614
Iteration 13/25 | Loss: 0.00172614
Iteration 14/25 | Loss: 0.00172614
Iteration 15/25 | Loss: 0.00172614
Iteration 16/25 | Loss: 0.00172614
Iteration 17/25 | Loss: 0.00172614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017261446919292212, 0.0017261446919292212, 0.0017261446919292212, 0.0017261446919292212, 0.0017261446919292212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017261446919292212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172614
Iteration 2/1000 | Loss: 0.00004994
Iteration 3/1000 | Loss: 0.00003433
Iteration 4/1000 | Loss: 0.00003004
Iteration 5/1000 | Loss: 0.00002840
Iteration 6/1000 | Loss: 0.00002760
Iteration 7/1000 | Loss: 0.00002706
Iteration 8/1000 | Loss: 0.00002652
Iteration 9/1000 | Loss: 0.00002614
Iteration 10/1000 | Loss: 0.00002586
Iteration 11/1000 | Loss: 0.00002567
Iteration 12/1000 | Loss: 0.00002551
Iteration 13/1000 | Loss: 0.00002546
Iteration 14/1000 | Loss: 0.00002540
Iteration 15/1000 | Loss: 0.00002534
Iteration 16/1000 | Loss: 0.00002529
Iteration 17/1000 | Loss: 0.00002525
Iteration 18/1000 | Loss: 0.00002520
Iteration 19/1000 | Loss: 0.00002518
Iteration 20/1000 | Loss: 0.00002518
Iteration 21/1000 | Loss: 0.00002516
Iteration 22/1000 | Loss: 0.00002516
Iteration 23/1000 | Loss: 0.00002515
Iteration 24/1000 | Loss: 0.00002515
Iteration 25/1000 | Loss: 0.00002515
Iteration 26/1000 | Loss: 0.00002515
Iteration 27/1000 | Loss: 0.00002514
Iteration 28/1000 | Loss: 0.00002512
Iteration 29/1000 | Loss: 0.00002512
Iteration 30/1000 | Loss: 0.00002511
Iteration 31/1000 | Loss: 0.00002511
Iteration 32/1000 | Loss: 0.00002510
Iteration 33/1000 | Loss: 0.00002510
Iteration 34/1000 | Loss: 0.00002509
Iteration 35/1000 | Loss: 0.00002508
Iteration 36/1000 | Loss: 0.00002508
Iteration 37/1000 | Loss: 0.00002507
Iteration 38/1000 | Loss: 0.00002507
Iteration 39/1000 | Loss: 0.00002507
Iteration 40/1000 | Loss: 0.00002506
Iteration 41/1000 | Loss: 0.00002506
Iteration 42/1000 | Loss: 0.00002506
Iteration 43/1000 | Loss: 0.00002506
Iteration 44/1000 | Loss: 0.00002506
Iteration 45/1000 | Loss: 0.00002506
Iteration 46/1000 | Loss: 0.00002505
Iteration 47/1000 | Loss: 0.00002505
Iteration 48/1000 | Loss: 0.00002505
Iteration 49/1000 | Loss: 0.00002504
Iteration 50/1000 | Loss: 0.00002504
Iteration 51/1000 | Loss: 0.00002504
Iteration 52/1000 | Loss: 0.00002501
Iteration 53/1000 | Loss: 0.00002501
Iteration 54/1000 | Loss: 0.00002501
Iteration 55/1000 | Loss: 0.00002501
Iteration 56/1000 | Loss: 0.00002500
Iteration 57/1000 | Loss: 0.00002500
Iteration 58/1000 | Loss: 0.00002500
Iteration 59/1000 | Loss: 0.00002500
Iteration 60/1000 | Loss: 0.00002500
Iteration 61/1000 | Loss: 0.00002500
Iteration 62/1000 | Loss: 0.00002499
Iteration 63/1000 | Loss: 0.00002499
Iteration 64/1000 | Loss: 0.00002498
Iteration 65/1000 | Loss: 0.00002497
Iteration 66/1000 | Loss: 0.00002497
Iteration 67/1000 | Loss: 0.00002496
Iteration 68/1000 | Loss: 0.00002495
Iteration 69/1000 | Loss: 0.00002494
Iteration 70/1000 | Loss: 0.00002494
Iteration 71/1000 | Loss: 0.00002493
Iteration 72/1000 | Loss: 0.00002493
Iteration 73/1000 | Loss: 0.00002492
Iteration 74/1000 | Loss: 0.00002491
Iteration 75/1000 | Loss: 0.00002490
Iteration 76/1000 | Loss: 0.00002490
Iteration 77/1000 | Loss: 0.00002490
Iteration 78/1000 | Loss: 0.00002490
Iteration 79/1000 | Loss: 0.00002490
Iteration 80/1000 | Loss: 0.00002489
Iteration 81/1000 | Loss: 0.00002489
Iteration 82/1000 | Loss: 0.00002489
Iteration 83/1000 | Loss: 0.00002489
Iteration 84/1000 | Loss: 0.00002489
Iteration 85/1000 | Loss: 0.00002488
Iteration 86/1000 | Loss: 0.00002488
Iteration 87/1000 | Loss: 0.00002488
Iteration 88/1000 | Loss: 0.00002488
Iteration 89/1000 | Loss: 0.00002488
Iteration 90/1000 | Loss: 0.00002488
Iteration 91/1000 | Loss: 0.00002488
Iteration 92/1000 | Loss: 0.00002488
Iteration 93/1000 | Loss: 0.00002487
Iteration 94/1000 | Loss: 0.00002487
Iteration 95/1000 | Loss: 0.00002487
Iteration 96/1000 | Loss: 0.00002487
Iteration 97/1000 | Loss: 0.00002487
Iteration 98/1000 | Loss: 0.00002487
Iteration 99/1000 | Loss: 0.00002486
Iteration 100/1000 | Loss: 0.00002486
Iteration 101/1000 | Loss: 0.00002486
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002485
Iteration 105/1000 | Loss: 0.00002485
Iteration 106/1000 | Loss: 0.00002485
Iteration 107/1000 | Loss: 0.00002485
Iteration 108/1000 | Loss: 0.00002484
Iteration 109/1000 | Loss: 0.00002484
Iteration 110/1000 | Loss: 0.00002484
Iteration 111/1000 | Loss: 0.00002484
Iteration 112/1000 | Loss: 0.00002484
Iteration 113/1000 | Loss: 0.00002484
Iteration 114/1000 | Loss: 0.00002484
Iteration 115/1000 | Loss: 0.00002484
Iteration 116/1000 | Loss: 0.00002483
Iteration 117/1000 | Loss: 0.00002483
Iteration 118/1000 | Loss: 0.00002483
Iteration 119/1000 | Loss: 0.00002482
Iteration 120/1000 | Loss: 0.00002482
Iteration 121/1000 | Loss: 0.00002482
Iteration 122/1000 | Loss: 0.00002481
Iteration 123/1000 | Loss: 0.00002481
Iteration 124/1000 | Loss: 0.00002481
Iteration 125/1000 | Loss: 0.00002481
Iteration 126/1000 | Loss: 0.00002480
Iteration 127/1000 | Loss: 0.00002480
Iteration 128/1000 | Loss: 0.00002480
Iteration 129/1000 | Loss: 0.00002480
Iteration 130/1000 | Loss: 0.00002480
Iteration 131/1000 | Loss: 0.00002480
Iteration 132/1000 | Loss: 0.00002479
Iteration 133/1000 | Loss: 0.00002479
Iteration 134/1000 | Loss: 0.00002479
Iteration 135/1000 | Loss: 0.00002479
Iteration 136/1000 | Loss: 0.00002479
Iteration 137/1000 | Loss: 0.00002479
Iteration 138/1000 | Loss: 0.00002479
Iteration 139/1000 | Loss: 0.00002479
Iteration 140/1000 | Loss: 0.00002479
Iteration 141/1000 | Loss: 0.00002479
Iteration 142/1000 | Loss: 0.00002478
Iteration 143/1000 | Loss: 0.00002478
Iteration 144/1000 | Loss: 0.00002478
Iteration 145/1000 | Loss: 0.00002478
Iteration 146/1000 | Loss: 0.00002478
Iteration 147/1000 | Loss: 0.00002477
Iteration 148/1000 | Loss: 0.00002477
Iteration 149/1000 | Loss: 0.00002477
Iteration 150/1000 | Loss: 0.00002477
Iteration 151/1000 | Loss: 0.00002477
Iteration 152/1000 | Loss: 0.00002477
Iteration 153/1000 | Loss: 0.00002476
Iteration 154/1000 | Loss: 0.00002476
Iteration 155/1000 | Loss: 0.00002476
Iteration 156/1000 | Loss: 0.00002476
Iteration 157/1000 | Loss: 0.00002476
Iteration 158/1000 | Loss: 0.00002476
Iteration 159/1000 | Loss: 0.00002476
Iteration 160/1000 | Loss: 0.00002476
Iteration 161/1000 | Loss: 0.00002476
Iteration 162/1000 | Loss: 0.00002475
Iteration 163/1000 | Loss: 0.00002475
Iteration 164/1000 | Loss: 0.00002475
Iteration 165/1000 | Loss: 0.00002475
Iteration 166/1000 | Loss: 0.00002475
Iteration 167/1000 | Loss: 0.00002475
Iteration 168/1000 | Loss: 0.00002475
Iteration 169/1000 | Loss: 0.00002475
Iteration 170/1000 | Loss: 0.00002475
Iteration 171/1000 | Loss: 0.00002475
Iteration 172/1000 | Loss: 0.00002475
Iteration 173/1000 | Loss: 0.00002475
Iteration 174/1000 | Loss: 0.00002475
Iteration 175/1000 | Loss: 0.00002475
Iteration 176/1000 | Loss: 0.00002475
Iteration 177/1000 | Loss: 0.00002475
Iteration 178/1000 | Loss: 0.00002475
Iteration 179/1000 | Loss: 0.00002475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.475242581567727e-05, 2.475242581567727e-05, 2.475242581567727e-05, 2.475242581567727e-05, 2.475242581567727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.475242581567727e-05

Optimization complete. Final v2v error: 4.001759052276611 mm

Highest mean error: 5.005202770233154 mm for frame 26

Lowest mean error: 3.081059455871582 mm for frame 1

Saving results

Total time: 44.22751760482788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887609
Iteration 2/25 | Loss: 0.00133024
Iteration 3/25 | Loss: 0.00123084
Iteration 4/25 | Loss: 0.00122174
Iteration 5/25 | Loss: 0.00121973
Iteration 6/25 | Loss: 0.00121946
Iteration 7/25 | Loss: 0.00121946
Iteration 8/25 | Loss: 0.00121946
Iteration 9/25 | Loss: 0.00121946
Iteration 10/25 | Loss: 0.00121946
Iteration 11/25 | Loss: 0.00121946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001219455269165337, 0.001219455269165337, 0.001219455269165337, 0.001219455269165337, 0.001219455269165337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001219455269165337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.95810413
Iteration 2/25 | Loss: 0.00254440
Iteration 3/25 | Loss: 0.00254439
Iteration 4/25 | Loss: 0.00254439
Iteration 5/25 | Loss: 0.00254438
Iteration 6/25 | Loss: 0.00254438
Iteration 7/25 | Loss: 0.00254438
Iteration 8/25 | Loss: 0.00254438
Iteration 9/25 | Loss: 0.00254438
Iteration 10/25 | Loss: 0.00254438
Iteration 11/25 | Loss: 0.00254438
Iteration 12/25 | Loss: 0.00254438
Iteration 13/25 | Loss: 0.00254438
Iteration 14/25 | Loss: 0.00254438
Iteration 15/25 | Loss: 0.00254438
Iteration 16/25 | Loss: 0.00254438
Iteration 17/25 | Loss: 0.00254438
Iteration 18/25 | Loss: 0.00254438
Iteration 19/25 | Loss: 0.00254438
Iteration 20/25 | Loss: 0.00254438
Iteration 21/25 | Loss: 0.00254438
Iteration 22/25 | Loss: 0.00254438
Iteration 23/25 | Loss: 0.00254438
Iteration 24/25 | Loss: 0.00254438
Iteration 25/25 | Loss: 0.00254438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254438
Iteration 2/1000 | Loss: 0.00002809
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001861
Iteration 5/1000 | Loss: 0.00001712
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001590
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001542
Iteration 10/1000 | Loss: 0.00001537
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001519
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001512
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001506
Iteration 22/1000 | Loss: 0.00001505
Iteration 23/1000 | Loss: 0.00001505
Iteration 24/1000 | Loss: 0.00001505
Iteration 25/1000 | Loss: 0.00001503
Iteration 26/1000 | Loss: 0.00001502
Iteration 27/1000 | Loss: 0.00001501
Iteration 28/1000 | Loss: 0.00001501
Iteration 29/1000 | Loss: 0.00001500
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001499
Iteration 32/1000 | Loss: 0.00001498
Iteration 33/1000 | Loss: 0.00001498
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001498
Iteration 36/1000 | Loss: 0.00001498
Iteration 37/1000 | Loss: 0.00001498
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001498
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00001498
Iteration 42/1000 | Loss: 0.00001498
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001494
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001492
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001490
Iteration 63/1000 | Loss: 0.00001489
Iteration 64/1000 | Loss: 0.00001489
Iteration 65/1000 | Loss: 0.00001489
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001488
Iteration 68/1000 | Loss: 0.00001488
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001488
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001486
Iteration 80/1000 | Loss: 0.00001486
Iteration 81/1000 | Loss: 0.00001486
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001486
Iteration 86/1000 | Loss: 0.00001486
Iteration 87/1000 | Loss: 0.00001486
Iteration 88/1000 | Loss: 0.00001486
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001485
Iteration 91/1000 | Loss: 0.00001485
Iteration 92/1000 | Loss: 0.00001485
Iteration 93/1000 | Loss: 0.00001485
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001485
Iteration 99/1000 | Loss: 0.00001485
Iteration 100/1000 | Loss: 0.00001485
Iteration 101/1000 | Loss: 0.00001485
Iteration 102/1000 | Loss: 0.00001485
Iteration 103/1000 | Loss: 0.00001484
Iteration 104/1000 | Loss: 0.00001484
Iteration 105/1000 | Loss: 0.00001484
Iteration 106/1000 | Loss: 0.00001484
Iteration 107/1000 | Loss: 0.00001484
Iteration 108/1000 | Loss: 0.00001484
Iteration 109/1000 | Loss: 0.00001484
Iteration 110/1000 | Loss: 0.00001484
Iteration 111/1000 | Loss: 0.00001484
Iteration 112/1000 | Loss: 0.00001484
Iteration 113/1000 | Loss: 0.00001484
Iteration 114/1000 | Loss: 0.00001484
Iteration 115/1000 | Loss: 0.00001484
Iteration 116/1000 | Loss: 0.00001484
Iteration 117/1000 | Loss: 0.00001484
Iteration 118/1000 | Loss: 0.00001484
Iteration 119/1000 | Loss: 0.00001484
Iteration 120/1000 | Loss: 0.00001484
Iteration 121/1000 | Loss: 0.00001484
Iteration 122/1000 | Loss: 0.00001484
Iteration 123/1000 | Loss: 0.00001484
Iteration 124/1000 | Loss: 0.00001484
Iteration 125/1000 | Loss: 0.00001484
Iteration 126/1000 | Loss: 0.00001484
Iteration 127/1000 | Loss: 0.00001484
Iteration 128/1000 | Loss: 0.00001484
Iteration 129/1000 | Loss: 0.00001484
Iteration 130/1000 | Loss: 0.00001484
Iteration 131/1000 | Loss: 0.00001484
Iteration 132/1000 | Loss: 0.00001484
Iteration 133/1000 | Loss: 0.00001484
Iteration 134/1000 | Loss: 0.00001484
Iteration 135/1000 | Loss: 0.00001484
Iteration 136/1000 | Loss: 0.00001484
Iteration 137/1000 | Loss: 0.00001484
Iteration 138/1000 | Loss: 0.00001484
Iteration 139/1000 | Loss: 0.00001484
Iteration 140/1000 | Loss: 0.00001484
Iteration 141/1000 | Loss: 0.00001484
Iteration 142/1000 | Loss: 0.00001484
Iteration 143/1000 | Loss: 0.00001484
Iteration 144/1000 | Loss: 0.00001484
Iteration 145/1000 | Loss: 0.00001484
Iteration 146/1000 | Loss: 0.00001484
Iteration 147/1000 | Loss: 0.00001484
Iteration 148/1000 | Loss: 0.00001484
Iteration 149/1000 | Loss: 0.00001484
Iteration 150/1000 | Loss: 0.00001484
Iteration 151/1000 | Loss: 0.00001484
Iteration 152/1000 | Loss: 0.00001484
Iteration 153/1000 | Loss: 0.00001484
Iteration 154/1000 | Loss: 0.00001484
Iteration 155/1000 | Loss: 0.00001484
Iteration 156/1000 | Loss: 0.00001484
Iteration 157/1000 | Loss: 0.00001484
Iteration 158/1000 | Loss: 0.00001484
Iteration 159/1000 | Loss: 0.00001484
Iteration 160/1000 | Loss: 0.00001484
Iteration 161/1000 | Loss: 0.00001484
Iteration 162/1000 | Loss: 0.00001484
Iteration 163/1000 | Loss: 0.00001484
Iteration 164/1000 | Loss: 0.00001484
Iteration 165/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.484210224589333e-05, 1.484210224589333e-05, 1.484210224589333e-05, 1.484210224589333e-05, 1.484210224589333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.484210224589333e-05

Optimization complete. Final v2v error: 3.2418575286865234 mm

Highest mean error: 3.5666351318359375 mm for frame 53

Lowest mean error: 2.7473673820495605 mm for frame 27

Saving results

Total time: 32.307814598083496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046516
Iteration 2/25 | Loss: 0.00298938
Iteration 3/25 | Loss: 0.00192485
Iteration 4/25 | Loss: 0.00176265
Iteration 5/25 | Loss: 0.00169490
Iteration 6/25 | Loss: 0.00157133
Iteration 7/25 | Loss: 0.00151291
Iteration 8/25 | Loss: 0.00152142
Iteration 9/25 | Loss: 0.00147482
Iteration 10/25 | Loss: 0.00145211
Iteration 11/25 | Loss: 0.00143280
Iteration 12/25 | Loss: 0.00142765
Iteration 13/25 | Loss: 0.00142671
Iteration 14/25 | Loss: 0.00142659
Iteration 15/25 | Loss: 0.00141384
Iteration 16/25 | Loss: 0.00141082
Iteration 17/25 | Loss: 0.00140416
Iteration 18/25 | Loss: 0.00140438
Iteration 19/25 | Loss: 0.00139926
Iteration 20/25 | Loss: 0.00140122
Iteration 21/25 | Loss: 0.00140037
Iteration 22/25 | Loss: 0.00140512
Iteration 23/25 | Loss: 0.00140429
Iteration 24/25 | Loss: 0.00139942
Iteration 25/25 | Loss: 0.00140185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21582103
Iteration 2/25 | Loss: 0.00440075
Iteration 3/25 | Loss: 0.00440075
Iteration 4/25 | Loss: 0.00415253
Iteration 5/25 | Loss: 0.00415228
Iteration 6/25 | Loss: 0.00415228
Iteration 7/25 | Loss: 0.00415228
Iteration 8/25 | Loss: 0.00415228
Iteration 9/25 | Loss: 0.00415228
Iteration 10/25 | Loss: 0.00415228
Iteration 11/25 | Loss: 0.00415228
Iteration 12/25 | Loss: 0.00415228
Iteration 13/25 | Loss: 0.00415228
Iteration 14/25 | Loss: 0.00415228
Iteration 15/25 | Loss: 0.00415228
Iteration 16/25 | Loss: 0.00415228
Iteration 17/25 | Loss: 0.00415228
Iteration 18/25 | Loss: 0.00415228
Iteration 19/25 | Loss: 0.00415228
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004152277950197458, 0.004152277950197458, 0.004152277950197458, 0.004152277950197458, 0.004152277950197458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004152277950197458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00415228
Iteration 2/1000 | Loss: 0.00038966
Iteration 3/1000 | Loss: 0.00075529
Iteration 4/1000 | Loss: 0.00061968
Iteration 5/1000 | Loss: 0.00036484
Iteration 6/1000 | Loss: 0.00045067
Iteration 7/1000 | Loss: 0.00071711
Iteration 8/1000 | Loss: 0.00053146
Iteration 9/1000 | Loss: 0.00017107
Iteration 10/1000 | Loss: 0.00013122
Iteration 11/1000 | Loss: 0.00022962
Iteration 12/1000 | Loss: 0.00021224
Iteration 13/1000 | Loss: 0.00012147
Iteration 14/1000 | Loss: 0.00010439
Iteration 15/1000 | Loss: 0.00010451
Iteration 16/1000 | Loss: 0.00012222
Iteration 17/1000 | Loss: 0.00061498
Iteration 18/1000 | Loss: 0.00057395
Iteration 19/1000 | Loss: 0.00026927
Iteration 20/1000 | Loss: 0.00010364
Iteration 21/1000 | Loss: 0.00013410
Iteration 22/1000 | Loss: 0.00010781
Iteration 23/1000 | Loss: 0.00009213
Iteration 24/1000 | Loss: 0.00013572
Iteration 25/1000 | Loss: 0.00008830
Iteration 26/1000 | Loss: 0.00008577
Iteration 27/1000 | Loss: 0.00112156
Iteration 28/1000 | Loss: 0.00050768
Iteration 29/1000 | Loss: 0.00011853
Iteration 30/1000 | Loss: 0.00009879
Iteration 31/1000 | Loss: 0.00011785
Iteration 32/1000 | Loss: 0.00008502
Iteration 33/1000 | Loss: 0.00024097
Iteration 34/1000 | Loss: 0.00007573
Iteration 35/1000 | Loss: 0.00008487
Iteration 36/1000 | Loss: 0.00076838
Iteration 37/1000 | Loss: 0.00030131
Iteration 38/1000 | Loss: 0.00022399
Iteration 39/1000 | Loss: 0.00007891
Iteration 40/1000 | Loss: 0.00007036
Iteration 41/1000 | Loss: 0.00047694
Iteration 42/1000 | Loss: 0.00024181
Iteration 43/1000 | Loss: 0.00010057
Iteration 44/1000 | Loss: 0.00048971
Iteration 45/1000 | Loss: 0.00007397
Iteration 46/1000 | Loss: 0.00034648
Iteration 47/1000 | Loss: 0.00006279
Iteration 48/1000 | Loss: 0.00006047
Iteration 49/1000 | Loss: 0.00005882
Iteration 50/1000 | Loss: 0.00005799
Iteration 51/1000 | Loss: 0.00005983
Iteration 52/1000 | Loss: 0.00005805
Iteration 53/1000 | Loss: 0.00006082
Iteration 54/1000 | Loss: 0.00005639
Iteration 55/1000 | Loss: 0.00005620
Iteration 56/1000 | Loss: 0.00007783
Iteration 57/1000 | Loss: 0.00006803
Iteration 58/1000 | Loss: 0.00009094
Iteration 59/1000 | Loss: 0.00042940
Iteration 60/1000 | Loss: 0.00007926
Iteration 61/1000 | Loss: 0.00006328
Iteration 62/1000 | Loss: 0.00006563
Iteration 63/1000 | Loss: 0.00007707
Iteration 64/1000 | Loss: 0.00006065
Iteration 65/1000 | Loss: 0.00007180
Iteration 66/1000 | Loss: 0.00005422
Iteration 67/1000 | Loss: 0.00005972
Iteration 68/1000 | Loss: 0.00005686
Iteration 69/1000 | Loss: 0.00005490
Iteration 70/1000 | Loss: 0.00005283
Iteration 71/1000 | Loss: 0.00005905
Iteration 72/1000 | Loss: 0.00005281
Iteration 73/1000 | Loss: 0.00013304
Iteration 74/1000 | Loss: 0.00032567
Iteration 75/1000 | Loss: 0.00007816
Iteration 76/1000 | Loss: 0.00005827
Iteration 77/1000 | Loss: 0.00005604
Iteration 78/1000 | Loss: 0.00007562
Iteration 79/1000 | Loss: 0.00009568
Iteration 80/1000 | Loss: 0.00025092
Iteration 81/1000 | Loss: 0.00005080
Iteration 82/1000 | Loss: 0.00007470
Iteration 83/1000 | Loss: 0.00005002
Iteration 84/1000 | Loss: 0.00005517
Iteration 85/1000 | Loss: 0.00004955
Iteration 86/1000 | Loss: 0.00004933
Iteration 87/1000 | Loss: 0.00004924
Iteration 88/1000 | Loss: 0.00004924
Iteration 89/1000 | Loss: 0.00004964
Iteration 90/1000 | Loss: 0.00004963
Iteration 91/1000 | Loss: 0.00005001
Iteration 92/1000 | Loss: 0.00006367
Iteration 93/1000 | Loss: 0.00004909
Iteration 94/1000 | Loss: 0.00004906
Iteration 95/1000 | Loss: 0.00004906
Iteration 96/1000 | Loss: 0.00004905
Iteration 97/1000 | Loss: 0.00004905
Iteration 98/1000 | Loss: 0.00004904
Iteration 99/1000 | Loss: 0.00004904
Iteration 100/1000 | Loss: 0.00004904
Iteration 101/1000 | Loss: 0.00004903
Iteration 102/1000 | Loss: 0.00006100
Iteration 103/1000 | Loss: 0.00004903
Iteration 104/1000 | Loss: 0.00004903
Iteration 105/1000 | Loss: 0.00004902
Iteration 106/1000 | Loss: 0.00004902
Iteration 107/1000 | Loss: 0.00004902
Iteration 108/1000 | Loss: 0.00004902
Iteration 109/1000 | Loss: 0.00004902
Iteration 110/1000 | Loss: 0.00004902
Iteration 111/1000 | Loss: 0.00004902
Iteration 112/1000 | Loss: 0.00004902
Iteration 113/1000 | Loss: 0.00004902
Iteration 114/1000 | Loss: 0.00004902
Iteration 115/1000 | Loss: 0.00004901
Iteration 116/1000 | Loss: 0.00004901
Iteration 117/1000 | Loss: 0.00004901
Iteration 118/1000 | Loss: 0.00004901
Iteration 119/1000 | Loss: 0.00004901
Iteration 120/1000 | Loss: 0.00004900
Iteration 121/1000 | Loss: 0.00004900
Iteration 122/1000 | Loss: 0.00004900
Iteration 123/1000 | Loss: 0.00004900
Iteration 124/1000 | Loss: 0.00004900
Iteration 125/1000 | Loss: 0.00004899
Iteration 126/1000 | Loss: 0.00005414
Iteration 127/1000 | Loss: 0.00004895
Iteration 128/1000 | Loss: 0.00004895
Iteration 129/1000 | Loss: 0.00004895
Iteration 130/1000 | Loss: 0.00004894
Iteration 131/1000 | Loss: 0.00004894
Iteration 132/1000 | Loss: 0.00004894
Iteration 133/1000 | Loss: 0.00004894
Iteration 134/1000 | Loss: 0.00004894
Iteration 135/1000 | Loss: 0.00004894
Iteration 136/1000 | Loss: 0.00004893
Iteration 137/1000 | Loss: 0.00004893
Iteration 138/1000 | Loss: 0.00004893
Iteration 139/1000 | Loss: 0.00004893
Iteration 140/1000 | Loss: 0.00004893
Iteration 141/1000 | Loss: 0.00004893
Iteration 142/1000 | Loss: 0.00004893
Iteration 143/1000 | Loss: 0.00004893
Iteration 144/1000 | Loss: 0.00004893
Iteration 145/1000 | Loss: 0.00004893
Iteration 146/1000 | Loss: 0.00004893
Iteration 147/1000 | Loss: 0.00004893
Iteration 148/1000 | Loss: 0.00004893
Iteration 149/1000 | Loss: 0.00004893
Iteration 150/1000 | Loss: 0.00004893
Iteration 151/1000 | Loss: 0.00004893
Iteration 152/1000 | Loss: 0.00004893
Iteration 153/1000 | Loss: 0.00004893
Iteration 154/1000 | Loss: 0.00004892
Iteration 155/1000 | Loss: 0.00004892
Iteration 156/1000 | Loss: 0.00004892
Iteration 157/1000 | Loss: 0.00004892
Iteration 158/1000 | Loss: 0.00004892
Iteration 159/1000 | Loss: 0.00004892
Iteration 160/1000 | Loss: 0.00004892
Iteration 161/1000 | Loss: 0.00004892
Iteration 162/1000 | Loss: 0.00004892
Iteration 163/1000 | Loss: 0.00004892
Iteration 164/1000 | Loss: 0.00004892
Iteration 165/1000 | Loss: 0.00004892
Iteration 166/1000 | Loss: 0.00004891
Iteration 167/1000 | Loss: 0.00004891
Iteration 168/1000 | Loss: 0.00004891
Iteration 169/1000 | Loss: 0.00004891
Iteration 170/1000 | Loss: 0.00004891
Iteration 171/1000 | Loss: 0.00004891
Iteration 172/1000 | Loss: 0.00004891
Iteration 173/1000 | Loss: 0.00004891
Iteration 174/1000 | Loss: 0.00004891
Iteration 175/1000 | Loss: 0.00004891
Iteration 176/1000 | Loss: 0.00004891
Iteration 177/1000 | Loss: 0.00004891
Iteration 178/1000 | Loss: 0.00004891
Iteration 179/1000 | Loss: 0.00004891
Iteration 180/1000 | Loss: 0.00004891
Iteration 181/1000 | Loss: 0.00004891
Iteration 182/1000 | Loss: 0.00004891
Iteration 183/1000 | Loss: 0.00004890
Iteration 184/1000 | Loss: 0.00004890
Iteration 185/1000 | Loss: 0.00004890
Iteration 186/1000 | Loss: 0.00004890
Iteration 187/1000 | Loss: 0.00004890
Iteration 188/1000 | Loss: 0.00004890
Iteration 189/1000 | Loss: 0.00004890
Iteration 190/1000 | Loss: 0.00004890
Iteration 191/1000 | Loss: 0.00004889
Iteration 192/1000 | Loss: 0.00004889
Iteration 193/1000 | Loss: 0.00004889
Iteration 194/1000 | Loss: 0.00004889
Iteration 195/1000 | Loss: 0.00004889
Iteration 196/1000 | Loss: 0.00004888
Iteration 197/1000 | Loss: 0.00004888
Iteration 198/1000 | Loss: 0.00013683
Iteration 199/1000 | Loss: 0.00042253
Iteration 200/1000 | Loss: 0.00010102
Iteration 201/1000 | Loss: 0.00007240
Iteration 202/1000 | Loss: 0.00005050
Iteration 203/1000 | Loss: 0.00005157
Iteration 204/1000 | Loss: 0.00006770
Iteration 205/1000 | Loss: 0.00007687
Iteration 206/1000 | Loss: 0.00004945
Iteration 207/1000 | Loss: 0.00004775
Iteration 208/1000 | Loss: 0.00004756
Iteration 209/1000 | Loss: 0.00004756
Iteration 210/1000 | Loss: 0.00004747
Iteration 211/1000 | Loss: 0.00004746
Iteration 212/1000 | Loss: 0.00005115
Iteration 213/1000 | Loss: 0.00005024
Iteration 214/1000 | Loss: 0.00004738
Iteration 215/1000 | Loss: 0.00004738
Iteration 216/1000 | Loss: 0.00004738
Iteration 217/1000 | Loss: 0.00004738
Iteration 218/1000 | Loss: 0.00004737
Iteration 219/1000 | Loss: 0.00004737
Iteration 220/1000 | Loss: 0.00004737
Iteration 221/1000 | Loss: 0.00004737
Iteration 222/1000 | Loss: 0.00004737
Iteration 223/1000 | Loss: 0.00004737
Iteration 224/1000 | Loss: 0.00004737
Iteration 225/1000 | Loss: 0.00004737
Iteration 226/1000 | Loss: 0.00004736
Iteration 227/1000 | Loss: 0.00004736
Iteration 228/1000 | Loss: 0.00004736
Iteration 229/1000 | Loss: 0.00004736
Iteration 230/1000 | Loss: 0.00004736
Iteration 231/1000 | Loss: 0.00004736
Iteration 232/1000 | Loss: 0.00004736
Iteration 233/1000 | Loss: 0.00004735
Iteration 234/1000 | Loss: 0.00004846
Iteration 235/1000 | Loss: 0.00005081
Iteration 236/1000 | Loss: 0.00004797
Iteration 237/1000 | Loss: 0.00004732
Iteration 238/1000 | Loss: 0.00004732
Iteration 239/1000 | Loss: 0.00004732
Iteration 240/1000 | Loss: 0.00004732
Iteration 241/1000 | Loss: 0.00004732
Iteration 242/1000 | Loss: 0.00004732
Iteration 243/1000 | Loss: 0.00004731
Iteration 244/1000 | Loss: 0.00004731
Iteration 245/1000 | Loss: 0.00004731
Iteration 246/1000 | Loss: 0.00004731
Iteration 247/1000 | Loss: 0.00004731
Iteration 248/1000 | Loss: 0.00004731
Iteration 249/1000 | Loss: 0.00004731
Iteration 250/1000 | Loss: 0.00004731
Iteration 251/1000 | Loss: 0.00004754
Iteration 252/1000 | Loss: 0.00005006
Iteration 253/1000 | Loss: 0.00004729
Iteration 254/1000 | Loss: 0.00004729
Iteration 255/1000 | Loss: 0.00004729
Iteration 256/1000 | Loss: 0.00004729
Iteration 257/1000 | Loss: 0.00004729
Iteration 258/1000 | Loss: 0.00004729
Iteration 259/1000 | Loss: 0.00004729
Iteration 260/1000 | Loss: 0.00004729
Iteration 261/1000 | Loss: 0.00004728
Iteration 262/1000 | Loss: 0.00004728
Iteration 263/1000 | Loss: 0.00004728
Iteration 264/1000 | Loss: 0.00004728
Iteration 265/1000 | Loss: 0.00004728
Iteration 266/1000 | Loss: 0.00004728
Iteration 267/1000 | Loss: 0.00004728
Iteration 268/1000 | Loss: 0.00004728
Iteration 269/1000 | Loss: 0.00004728
Iteration 270/1000 | Loss: 0.00004728
Iteration 271/1000 | Loss: 0.00004728
Iteration 272/1000 | Loss: 0.00004728
Iteration 273/1000 | Loss: 0.00004728
Iteration 274/1000 | Loss: 0.00004728
Iteration 275/1000 | Loss: 0.00004728
Iteration 276/1000 | Loss: 0.00004728
Iteration 277/1000 | Loss: 0.00004728
Iteration 278/1000 | Loss: 0.00004728
Iteration 279/1000 | Loss: 0.00004728
Iteration 280/1000 | Loss: 0.00004728
Iteration 281/1000 | Loss: 0.00004728
Iteration 282/1000 | Loss: 0.00004728
Iteration 283/1000 | Loss: 0.00004728
Iteration 284/1000 | Loss: 0.00004728
Iteration 285/1000 | Loss: 0.00004728
Iteration 286/1000 | Loss: 0.00004728
Iteration 287/1000 | Loss: 0.00004728
Iteration 288/1000 | Loss: 0.00004728
Iteration 289/1000 | Loss: 0.00004728
Iteration 290/1000 | Loss: 0.00004728
Iteration 291/1000 | Loss: 0.00004728
Iteration 292/1000 | Loss: 0.00004728
Iteration 293/1000 | Loss: 0.00004728
Iteration 294/1000 | Loss: 0.00004728
Iteration 295/1000 | Loss: 0.00004728
Iteration 296/1000 | Loss: 0.00004728
Iteration 297/1000 | Loss: 0.00004728
Iteration 298/1000 | Loss: 0.00004728
Iteration 299/1000 | Loss: 0.00004728
Iteration 300/1000 | Loss: 0.00004728
Iteration 301/1000 | Loss: 0.00004728
Iteration 302/1000 | Loss: 0.00004728
Iteration 303/1000 | Loss: 0.00004728
Iteration 304/1000 | Loss: 0.00004728
Iteration 305/1000 | Loss: 0.00004728
Iteration 306/1000 | Loss: 0.00004728
Iteration 307/1000 | Loss: 0.00004728
Iteration 308/1000 | Loss: 0.00004728
Iteration 309/1000 | Loss: 0.00004728
Iteration 310/1000 | Loss: 0.00004728
Iteration 311/1000 | Loss: 0.00004728
Iteration 312/1000 | Loss: 0.00004728
Iteration 313/1000 | Loss: 0.00004728
Iteration 314/1000 | Loss: 0.00004728
Iteration 315/1000 | Loss: 0.00004728
Iteration 316/1000 | Loss: 0.00004728
Iteration 317/1000 | Loss: 0.00004728
Iteration 318/1000 | Loss: 0.00004728
Iteration 319/1000 | Loss: 0.00004728
Iteration 320/1000 | Loss: 0.00004728
Iteration 321/1000 | Loss: 0.00004728
Iteration 322/1000 | Loss: 0.00004728
Iteration 323/1000 | Loss: 0.00004728
Iteration 324/1000 | Loss: 0.00004728
Iteration 325/1000 | Loss: 0.00004728
Iteration 326/1000 | Loss: 0.00004728
Iteration 327/1000 | Loss: 0.00004728
Iteration 328/1000 | Loss: 0.00004728
Iteration 329/1000 | Loss: 0.00004728
Iteration 330/1000 | Loss: 0.00004728
Iteration 331/1000 | Loss: 0.00004728
Iteration 332/1000 | Loss: 0.00004728
Iteration 333/1000 | Loss: 0.00004728
Iteration 334/1000 | Loss: 0.00004728
Iteration 335/1000 | Loss: 0.00004728
Iteration 336/1000 | Loss: 0.00004728
Iteration 337/1000 | Loss: 0.00004728
Iteration 338/1000 | Loss: 0.00004728
Iteration 339/1000 | Loss: 0.00004728
Iteration 340/1000 | Loss: 0.00004728
Iteration 341/1000 | Loss: 0.00004728
Iteration 342/1000 | Loss: 0.00004728
Iteration 343/1000 | Loss: 0.00004728
Iteration 344/1000 | Loss: 0.00004728
Iteration 345/1000 | Loss: 0.00004728
Iteration 346/1000 | Loss: 0.00004728
Iteration 347/1000 | Loss: 0.00004728
Iteration 348/1000 | Loss: 0.00004728
Iteration 349/1000 | Loss: 0.00004728
Iteration 350/1000 | Loss: 0.00004728
Iteration 351/1000 | Loss: 0.00004728
Iteration 352/1000 | Loss: 0.00004728
Iteration 353/1000 | Loss: 0.00004728
Iteration 354/1000 | Loss: 0.00004728
Iteration 355/1000 | Loss: 0.00004728
Iteration 356/1000 | Loss: 0.00004728
Iteration 357/1000 | Loss: 0.00004728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [4.728421845356934e-05, 4.728421845356934e-05, 4.728421845356934e-05, 4.728421845356934e-05, 4.728421845356934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.728421845356934e-05

Optimization complete. Final v2v error: 4.097479343414307 mm

Highest mean error: 12.947020530700684 mm for frame 172

Lowest mean error: 2.71372389793396 mm for frame 168

Saving results

Total time: 237.32470750808716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088210
Iteration 2/25 | Loss: 0.01088210
Iteration 3/25 | Loss: 0.01088210
Iteration 4/25 | Loss: 0.01088210
Iteration 5/25 | Loss: 0.01088210
Iteration 6/25 | Loss: 0.01088209
Iteration 7/25 | Loss: 0.01088209
Iteration 8/25 | Loss: 0.01088209
Iteration 9/25 | Loss: 0.01088209
Iteration 10/25 | Loss: 0.01088209
Iteration 11/25 | Loss: 0.01088209
Iteration 12/25 | Loss: 0.01088209
Iteration 13/25 | Loss: 0.01088209
Iteration 14/25 | Loss: 0.01088208
Iteration 15/25 | Loss: 0.01088208
Iteration 16/25 | Loss: 0.01088208
Iteration 17/25 | Loss: 0.01088208
Iteration 18/25 | Loss: 0.01088208
Iteration 19/25 | Loss: 0.01088208
Iteration 20/25 | Loss: 0.01088207
Iteration 21/25 | Loss: 0.01088207
Iteration 22/25 | Loss: 0.01088207
Iteration 23/25 | Loss: 0.01088207
Iteration 24/25 | Loss: 0.01088207
Iteration 25/25 | Loss: 0.01088207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29367399
Iteration 2/25 | Loss: 0.09520975
Iteration 3/25 | Loss: 0.09464148
Iteration 4/25 | Loss: 0.09464184
Iteration 5/25 | Loss: 0.09456833
Iteration 6/25 | Loss: 0.09456829
Iteration 7/25 | Loss: 0.09456829
Iteration 8/25 | Loss: 0.09456829
Iteration 9/25 | Loss: 0.09456828
Iteration 10/25 | Loss: 0.09456827
Iteration 11/25 | Loss: 0.09456827
Iteration 12/25 | Loss: 0.09456826
Iteration 13/25 | Loss: 0.09456826
Iteration 14/25 | Loss: 0.09456825
Iteration 15/25 | Loss: 0.09456825
Iteration 16/25 | Loss: 0.09456825
Iteration 17/25 | Loss: 0.09456825
Iteration 18/25 | Loss: 0.09456825
Iteration 19/25 | Loss: 0.09456822
Iteration 20/25 | Loss: 0.09456755
Iteration 21/25 | Loss: 0.09456753
Iteration 22/25 | Loss: 0.09456752
Iteration 23/25 | Loss: 0.09456752
Iteration 24/25 | Loss: 0.09456752
Iteration 25/25 | Loss: 0.09456751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09456751
Iteration 2/1000 | Loss: 0.00719927
Iteration 3/1000 | Loss: 0.00165343
Iteration 4/1000 | Loss: 0.00264580
Iteration 5/1000 | Loss: 0.00057907
Iteration 6/1000 | Loss: 0.00056521
Iteration 7/1000 | Loss: 0.00302752
Iteration 8/1000 | Loss: 0.00051445
Iteration 9/1000 | Loss: 0.00031486
Iteration 10/1000 | Loss: 0.00031583
Iteration 11/1000 | Loss: 0.00022272
Iteration 12/1000 | Loss: 0.00013442
Iteration 13/1000 | Loss: 0.00039883
Iteration 14/1000 | Loss: 0.00066608
Iteration 15/1000 | Loss: 0.00040235
Iteration 16/1000 | Loss: 0.00041019
Iteration 17/1000 | Loss: 0.00007653
Iteration 18/1000 | Loss: 0.00004849
Iteration 19/1000 | Loss: 0.00011649
Iteration 20/1000 | Loss: 0.00019050
Iteration 21/1000 | Loss: 0.00034381
Iteration 22/1000 | Loss: 0.00004376
Iteration 23/1000 | Loss: 0.00007213
Iteration 24/1000 | Loss: 0.00003046
Iteration 25/1000 | Loss: 0.00006458
Iteration 26/1000 | Loss: 0.00003574
Iteration 27/1000 | Loss: 0.00006684
Iteration 28/1000 | Loss: 0.00017807
Iteration 29/1000 | Loss: 0.00038451
Iteration 30/1000 | Loss: 0.00011541
Iteration 31/1000 | Loss: 0.00002387
Iteration 32/1000 | Loss: 0.00002323
Iteration 33/1000 | Loss: 0.00018985
Iteration 34/1000 | Loss: 0.00002924
Iteration 35/1000 | Loss: 0.00002386
Iteration 36/1000 | Loss: 0.00007801
Iteration 37/1000 | Loss: 0.00002925
Iteration 38/1000 | Loss: 0.00013678
Iteration 39/1000 | Loss: 0.00006826
Iteration 40/1000 | Loss: 0.00002238
Iteration 41/1000 | Loss: 0.00012829
Iteration 42/1000 | Loss: 0.00002821
Iteration 43/1000 | Loss: 0.00018393
Iteration 44/1000 | Loss: 0.00002947
Iteration 45/1000 | Loss: 0.00015137
Iteration 46/1000 | Loss: 0.00002797
Iteration 47/1000 | Loss: 0.00008476
Iteration 48/1000 | Loss: 0.00002136
Iteration 49/1000 | Loss: 0.00002126
Iteration 50/1000 | Loss: 0.00002395
Iteration 51/1000 | Loss: 0.00011606
Iteration 52/1000 | Loss: 0.00011500
Iteration 53/1000 | Loss: 0.00002850
Iteration 54/1000 | Loss: 0.00004702
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002086
Iteration 57/1000 | Loss: 0.00007971
Iteration 58/1000 | Loss: 0.00018026
Iteration 59/1000 | Loss: 0.00011295
Iteration 60/1000 | Loss: 0.00002649
Iteration 61/1000 | Loss: 0.00019298
Iteration 62/1000 | Loss: 0.00025260
Iteration 63/1000 | Loss: 0.00006652
Iteration 64/1000 | Loss: 0.00002232
Iteration 65/1000 | Loss: 0.00002075
Iteration 66/1000 | Loss: 0.00002057
Iteration 67/1000 | Loss: 0.00002663
Iteration 68/1000 | Loss: 0.00002042
Iteration 69/1000 | Loss: 0.00002040
Iteration 70/1000 | Loss: 0.00008920
Iteration 71/1000 | Loss: 0.00002041
Iteration 72/1000 | Loss: 0.00002032
Iteration 73/1000 | Loss: 0.00002030
Iteration 74/1000 | Loss: 0.00002030
Iteration 75/1000 | Loss: 0.00002030
Iteration 76/1000 | Loss: 0.00002030
Iteration 77/1000 | Loss: 0.00002030
Iteration 78/1000 | Loss: 0.00002030
Iteration 79/1000 | Loss: 0.00002030
Iteration 80/1000 | Loss: 0.00002030
Iteration 81/1000 | Loss: 0.00002029
Iteration 82/1000 | Loss: 0.00002029
Iteration 83/1000 | Loss: 0.00002029
Iteration 84/1000 | Loss: 0.00002029
Iteration 85/1000 | Loss: 0.00002029
Iteration 86/1000 | Loss: 0.00002029
Iteration 87/1000 | Loss: 0.00002029
Iteration 88/1000 | Loss: 0.00002029
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002028
Iteration 94/1000 | Loss: 0.00002028
Iteration 95/1000 | Loss: 0.00002028
Iteration 96/1000 | Loss: 0.00002028
Iteration 97/1000 | Loss: 0.00002028
Iteration 98/1000 | Loss: 0.00002028
Iteration 99/1000 | Loss: 0.00002028
Iteration 100/1000 | Loss: 0.00002028
Iteration 101/1000 | Loss: 0.00002027
Iteration 102/1000 | Loss: 0.00007095
Iteration 103/1000 | Loss: 0.00002037
Iteration 104/1000 | Loss: 0.00002021
Iteration 105/1000 | Loss: 0.00002020
Iteration 106/1000 | Loss: 0.00002019
Iteration 107/1000 | Loss: 0.00002019
Iteration 108/1000 | Loss: 0.00002018
Iteration 109/1000 | Loss: 0.00002018
Iteration 110/1000 | Loss: 0.00002017
Iteration 111/1000 | Loss: 0.00002016
Iteration 112/1000 | Loss: 0.00002014
Iteration 113/1000 | Loss: 0.00002014
Iteration 114/1000 | Loss: 0.00002011
Iteration 115/1000 | Loss: 0.00002011
Iteration 116/1000 | Loss: 0.00002011
Iteration 117/1000 | Loss: 0.00002011
Iteration 118/1000 | Loss: 0.00002011
Iteration 119/1000 | Loss: 0.00002011
Iteration 120/1000 | Loss: 0.00002010
Iteration 121/1000 | Loss: 0.00002010
Iteration 122/1000 | Loss: 0.00002010
Iteration 123/1000 | Loss: 0.00002009
Iteration 124/1000 | Loss: 0.00002009
Iteration 125/1000 | Loss: 0.00002008
Iteration 126/1000 | Loss: 0.00012845
Iteration 127/1000 | Loss: 0.00002009
Iteration 128/1000 | Loss: 0.00002003
Iteration 129/1000 | Loss: 0.00002000
Iteration 130/1000 | Loss: 0.00002000
Iteration 131/1000 | Loss: 0.00002000
Iteration 132/1000 | Loss: 0.00002000
Iteration 133/1000 | Loss: 0.00002000
Iteration 134/1000 | Loss: 0.00002000
Iteration 135/1000 | Loss: 0.00002000
Iteration 136/1000 | Loss: 0.00002000
Iteration 137/1000 | Loss: 0.00002000
Iteration 138/1000 | Loss: 0.00001998
Iteration 139/1000 | Loss: 0.00001998
Iteration 140/1000 | Loss: 0.00001997
Iteration 141/1000 | Loss: 0.00001996
Iteration 142/1000 | Loss: 0.00001995
Iteration 143/1000 | Loss: 0.00001995
Iteration 144/1000 | Loss: 0.00001995
Iteration 145/1000 | Loss: 0.00001994
Iteration 146/1000 | Loss: 0.00001994
Iteration 147/1000 | Loss: 0.00001994
Iteration 148/1000 | Loss: 0.00001994
Iteration 149/1000 | Loss: 0.00001994
Iteration 150/1000 | Loss: 0.00001993
Iteration 151/1000 | Loss: 0.00001993
Iteration 152/1000 | Loss: 0.00001993
Iteration 153/1000 | Loss: 0.00001993
Iteration 154/1000 | Loss: 0.00001993
Iteration 155/1000 | Loss: 0.00001993
Iteration 156/1000 | Loss: 0.00001993
Iteration 157/1000 | Loss: 0.00001993
Iteration 158/1000 | Loss: 0.00001992
Iteration 159/1000 | Loss: 0.00001992
Iteration 160/1000 | Loss: 0.00001992
Iteration 161/1000 | Loss: 0.00001992
Iteration 162/1000 | Loss: 0.00001992
Iteration 163/1000 | Loss: 0.00001991
Iteration 164/1000 | Loss: 0.00001991
Iteration 165/1000 | Loss: 0.00001991
Iteration 166/1000 | Loss: 0.00001990
Iteration 167/1000 | Loss: 0.00001990
Iteration 168/1000 | Loss: 0.00001990
Iteration 169/1000 | Loss: 0.00008083
Iteration 170/1000 | Loss: 0.00010126
Iteration 171/1000 | Loss: 0.00009264
Iteration 172/1000 | Loss: 0.00004119
Iteration 173/1000 | Loss: 0.00002240
Iteration 174/1000 | Loss: 0.00001985
Iteration 175/1000 | Loss: 0.00001981
Iteration 176/1000 | Loss: 0.00001980
Iteration 177/1000 | Loss: 0.00001980
Iteration 178/1000 | Loss: 0.00010278
Iteration 179/1000 | Loss: 0.00010278
Iteration 180/1000 | Loss: 0.00005243
Iteration 181/1000 | Loss: 0.00003845
Iteration 182/1000 | Loss: 0.00002000
Iteration 183/1000 | Loss: 0.00001982
Iteration 184/1000 | Loss: 0.00001982
Iteration 185/1000 | Loss: 0.00001981
Iteration 186/1000 | Loss: 0.00001981
Iteration 187/1000 | Loss: 0.00001980
Iteration 188/1000 | Loss: 0.00001980
Iteration 189/1000 | Loss: 0.00001980
Iteration 190/1000 | Loss: 0.00001980
Iteration 191/1000 | Loss: 0.00001980
Iteration 192/1000 | Loss: 0.00001980
Iteration 193/1000 | Loss: 0.00001980
Iteration 194/1000 | Loss: 0.00001980
Iteration 195/1000 | Loss: 0.00001980
Iteration 196/1000 | Loss: 0.00001979
Iteration 197/1000 | Loss: 0.00001979
Iteration 198/1000 | Loss: 0.00001979
Iteration 199/1000 | Loss: 0.00001979
Iteration 200/1000 | Loss: 0.00001979
Iteration 201/1000 | Loss: 0.00001979
Iteration 202/1000 | Loss: 0.00001979
Iteration 203/1000 | Loss: 0.00001979
Iteration 204/1000 | Loss: 0.00001979
Iteration 205/1000 | Loss: 0.00001979
Iteration 206/1000 | Loss: 0.00001978
Iteration 207/1000 | Loss: 0.00001978
Iteration 208/1000 | Loss: 0.00001978
Iteration 209/1000 | Loss: 0.00001977
Iteration 210/1000 | Loss: 0.00001977
Iteration 211/1000 | Loss: 0.00001977
Iteration 212/1000 | Loss: 0.00001976
Iteration 213/1000 | Loss: 0.00001976
Iteration 214/1000 | Loss: 0.00001976
Iteration 215/1000 | Loss: 0.00001976
Iteration 216/1000 | Loss: 0.00001975
Iteration 217/1000 | Loss: 0.00001975
Iteration 218/1000 | Loss: 0.00001975
Iteration 219/1000 | Loss: 0.00001975
Iteration 220/1000 | Loss: 0.00001975
Iteration 221/1000 | Loss: 0.00001974
Iteration 222/1000 | Loss: 0.00001974
Iteration 223/1000 | Loss: 0.00001974
Iteration 224/1000 | Loss: 0.00001974
Iteration 225/1000 | Loss: 0.00001974
Iteration 226/1000 | Loss: 0.00001974
Iteration 227/1000 | Loss: 0.00001973
Iteration 228/1000 | Loss: 0.00001973
Iteration 229/1000 | Loss: 0.00001973
Iteration 230/1000 | Loss: 0.00001973
Iteration 231/1000 | Loss: 0.00001973
Iteration 232/1000 | Loss: 0.00001973
Iteration 233/1000 | Loss: 0.00001973
Iteration 234/1000 | Loss: 0.00001973
Iteration 235/1000 | Loss: 0.00001973
Iteration 236/1000 | Loss: 0.00001973
Iteration 237/1000 | Loss: 0.00001973
Iteration 238/1000 | Loss: 0.00001973
Iteration 239/1000 | Loss: 0.00001973
Iteration 240/1000 | Loss: 0.00001972
Iteration 241/1000 | Loss: 0.00001972
Iteration 242/1000 | Loss: 0.00001972
Iteration 243/1000 | Loss: 0.00001972
Iteration 244/1000 | Loss: 0.00001971
Iteration 245/1000 | Loss: 0.00001971
Iteration 246/1000 | Loss: 0.00001971
Iteration 247/1000 | Loss: 0.00001971
Iteration 248/1000 | Loss: 0.00001971
Iteration 249/1000 | Loss: 0.00001971
Iteration 250/1000 | Loss: 0.00001971
Iteration 251/1000 | Loss: 0.00001971
Iteration 252/1000 | Loss: 0.00001971
Iteration 253/1000 | Loss: 0.00001971
Iteration 254/1000 | Loss: 0.00001971
Iteration 255/1000 | Loss: 0.00001971
Iteration 256/1000 | Loss: 0.00001971
Iteration 257/1000 | Loss: 0.00001971
Iteration 258/1000 | Loss: 0.00001971
Iteration 259/1000 | Loss: 0.00001971
Iteration 260/1000 | Loss: 0.00001971
Iteration 261/1000 | Loss: 0.00001971
Iteration 262/1000 | Loss: 0.00001971
Iteration 263/1000 | Loss: 0.00001971
Iteration 264/1000 | Loss: 0.00001970
Iteration 265/1000 | Loss: 0.00001970
Iteration 266/1000 | Loss: 0.00001970
Iteration 267/1000 | Loss: 0.00001970
Iteration 268/1000 | Loss: 0.00001970
Iteration 269/1000 | Loss: 0.00001970
Iteration 270/1000 | Loss: 0.00001970
Iteration 271/1000 | Loss: 0.00001970
Iteration 272/1000 | Loss: 0.00001970
Iteration 273/1000 | Loss: 0.00001970
Iteration 274/1000 | Loss: 0.00001970
Iteration 275/1000 | Loss: 0.00001970
Iteration 276/1000 | Loss: 0.00001970
Iteration 277/1000 | Loss: 0.00001970
Iteration 278/1000 | Loss: 0.00001970
Iteration 279/1000 | Loss: 0.00001970
Iteration 280/1000 | Loss: 0.00001970
Iteration 281/1000 | Loss: 0.00001970
Iteration 282/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [1.9702836652868427e-05, 1.9702836652868427e-05, 1.9702836652868427e-05, 1.9702836652868427e-05, 1.9702836652868427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9702836652868427e-05

Optimization complete. Final v2v error: 3.711118221282959 mm

Highest mean error: 13.842467308044434 mm for frame 132

Lowest mean error: 3.3412632942199707 mm for frame 128

Saving results

Total time: 153.90238761901855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957726
Iteration 2/25 | Loss: 0.00150080
Iteration 3/25 | Loss: 0.00134042
Iteration 4/25 | Loss: 0.00131403
Iteration 5/25 | Loss: 0.00130426
Iteration 6/25 | Loss: 0.00130159
Iteration 7/25 | Loss: 0.00130036
Iteration 8/25 | Loss: 0.00130036
Iteration 9/25 | Loss: 0.00130036
Iteration 10/25 | Loss: 0.00130036
Iteration 11/25 | Loss: 0.00130036
Iteration 12/25 | Loss: 0.00130036
Iteration 13/25 | Loss: 0.00130036
Iteration 14/25 | Loss: 0.00130036
Iteration 15/25 | Loss: 0.00130036
Iteration 16/25 | Loss: 0.00130036
Iteration 17/25 | Loss: 0.00130036
Iteration 18/25 | Loss: 0.00130036
Iteration 19/25 | Loss: 0.00130036
Iteration 20/25 | Loss: 0.00130036
Iteration 21/25 | Loss: 0.00130036
Iteration 22/25 | Loss: 0.00130036
Iteration 23/25 | Loss: 0.00130036
Iteration 24/25 | Loss: 0.00130036
Iteration 25/25 | Loss: 0.00130036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.17921506
Iteration 2/25 | Loss: 0.00185062
Iteration 3/25 | Loss: 0.00185062
Iteration 4/25 | Loss: 0.00185061
Iteration 5/25 | Loss: 0.00185061
Iteration 6/25 | Loss: 0.00185061
Iteration 7/25 | Loss: 0.00185061
Iteration 8/25 | Loss: 0.00185061
Iteration 9/25 | Loss: 0.00185061
Iteration 10/25 | Loss: 0.00185061
Iteration 11/25 | Loss: 0.00185061
Iteration 12/25 | Loss: 0.00185061
Iteration 13/25 | Loss: 0.00185061
Iteration 14/25 | Loss: 0.00185061
Iteration 15/25 | Loss: 0.00185061
Iteration 16/25 | Loss: 0.00185061
Iteration 17/25 | Loss: 0.00185061
Iteration 18/25 | Loss: 0.00185061
Iteration 19/25 | Loss: 0.00185061
Iteration 20/25 | Loss: 0.00185061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018506132764741778, 0.0018506132764741778, 0.0018506132764741778, 0.0018506132764741778, 0.0018506132764741778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018506132764741778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185061
Iteration 2/1000 | Loss: 0.00006031
Iteration 3/1000 | Loss: 0.00003902
Iteration 4/1000 | Loss: 0.00003416
Iteration 5/1000 | Loss: 0.00003111
Iteration 6/1000 | Loss: 0.00002915
Iteration 7/1000 | Loss: 0.00002847
Iteration 8/1000 | Loss: 0.00002763
Iteration 9/1000 | Loss: 0.00002697
Iteration 10/1000 | Loss: 0.00002652
Iteration 11/1000 | Loss: 0.00002617
Iteration 12/1000 | Loss: 0.00002591
Iteration 13/1000 | Loss: 0.00002573
Iteration 14/1000 | Loss: 0.00002569
Iteration 15/1000 | Loss: 0.00002563
Iteration 16/1000 | Loss: 0.00002560
Iteration 17/1000 | Loss: 0.00002546
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00002539
Iteration 20/1000 | Loss: 0.00002536
Iteration 21/1000 | Loss: 0.00002535
Iteration 22/1000 | Loss: 0.00002533
Iteration 23/1000 | Loss: 0.00002532
Iteration 24/1000 | Loss: 0.00002532
Iteration 25/1000 | Loss: 0.00002531
Iteration 26/1000 | Loss: 0.00002530
Iteration 27/1000 | Loss: 0.00002527
Iteration 28/1000 | Loss: 0.00002527
Iteration 29/1000 | Loss: 0.00002527
Iteration 30/1000 | Loss: 0.00002526
Iteration 31/1000 | Loss: 0.00002526
Iteration 32/1000 | Loss: 0.00002526
Iteration 33/1000 | Loss: 0.00002526
Iteration 34/1000 | Loss: 0.00002526
Iteration 35/1000 | Loss: 0.00002526
Iteration 36/1000 | Loss: 0.00002526
Iteration 37/1000 | Loss: 0.00002526
Iteration 38/1000 | Loss: 0.00002525
Iteration 39/1000 | Loss: 0.00002524
Iteration 40/1000 | Loss: 0.00002523
Iteration 41/1000 | Loss: 0.00002522
Iteration 42/1000 | Loss: 0.00002520
Iteration 43/1000 | Loss: 0.00002520
Iteration 44/1000 | Loss: 0.00002520
Iteration 45/1000 | Loss: 0.00002520
Iteration 46/1000 | Loss: 0.00002520
Iteration 47/1000 | Loss: 0.00002520
Iteration 48/1000 | Loss: 0.00002520
Iteration 49/1000 | Loss: 0.00002520
Iteration 50/1000 | Loss: 0.00002520
Iteration 51/1000 | Loss: 0.00002519
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002517
Iteration 54/1000 | Loss: 0.00002517
Iteration 55/1000 | Loss: 0.00002517
Iteration 56/1000 | Loss: 0.00002517
Iteration 57/1000 | Loss: 0.00002517
Iteration 58/1000 | Loss: 0.00002517
Iteration 59/1000 | Loss: 0.00002517
Iteration 60/1000 | Loss: 0.00002516
Iteration 61/1000 | Loss: 0.00002516
Iteration 62/1000 | Loss: 0.00002516
Iteration 63/1000 | Loss: 0.00002516
Iteration 64/1000 | Loss: 0.00002516
Iteration 65/1000 | Loss: 0.00002516
Iteration 66/1000 | Loss: 0.00002516
Iteration 67/1000 | Loss: 0.00002516
Iteration 68/1000 | Loss: 0.00002516
Iteration 69/1000 | Loss: 0.00002516
Iteration 70/1000 | Loss: 0.00002516
Iteration 71/1000 | Loss: 0.00002515
Iteration 72/1000 | Loss: 0.00002515
Iteration 73/1000 | Loss: 0.00002515
Iteration 74/1000 | Loss: 0.00002514
Iteration 75/1000 | Loss: 0.00002514
Iteration 76/1000 | Loss: 0.00002513
Iteration 77/1000 | Loss: 0.00002513
Iteration 78/1000 | Loss: 0.00002513
Iteration 79/1000 | Loss: 0.00002512
Iteration 80/1000 | Loss: 0.00002512
Iteration 81/1000 | Loss: 0.00002511
Iteration 82/1000 | Loss: 0.00002511
Iteration 83/1000 | Loss: 0.00002510
Iteration 84/1000 | Loss: 0.00002510
Iteration 85/1000 | Loss: 0.00002510
Iteration 86/1000 | Loss: 0.00002510
Iteration 87/1000 | Loss: 0.00002510
Iteration 88/1000 | Loss: 0.00002510
Iteration 89/1000 | Loss: 0.00002510
Iteration 90/1000 | Loss: 0.00002510
Iteration 91/1000 | Loss: 0.00002510
Iteration 92/1000 | Loss: 0.00002509
Iteration 93/1000 | Loss: 0.00002509
Iteration 94/1000 | Loss: 0.00002509
Iteration 95/1000 | Loss: 0.00002509
Iteration 96/1000 | Loss: 0.00002508
Iteration 97/1000 | Loss: 0.00002508
Iteration 98/1000 | Loss: 0.00002508
Iteration 99/1000 | Loss: 0.00002508
Iteration 100/1000 | Loss: 0.00002507
Iteration 101/1000 | Loss: 0.00002507
Iteration 102/1000 | Loss: 0.00002507
Iteration 103/1000 | Loss: 0.00002507
Iteration 104/1000 | Loss: 0.00002507
Iteration 105/1000 | Loss: 0.00002507
Iteration 106/1000 | Loss: 0.00002507
Iteration 107/1000 | Loss: 0.00002507
Iteration 108/1000 | Loss: 0.00002507
Iteration 109/1000 | Loss: 0.00002507
Iteration 110/1000 | Loss: 0.00002507
Iteration 111/1000 | Loss: 0.00002507
Iteration 112/1000 | Loss: 0.00002507
Iteration 113/1000 | Loss: 0.00002507
Iteration 114/1000 | Loss: 0.00002507
Iteration 115/1000 | Loss: 0.00002507
Iteration 116/1000 | Loss: 0.00002507
Iteration 117/1000 | Loss: 0.00002507
Iteration 118/1000 | Loss: 0.00002507
Iteration 119/1000 | Loss: 0.00002507
Iteration 120/1000 | Loss: 0.00002507
Iteration 121/1000 | Loss: 0.00002507
Iteration 122/1000 | Loss: 0.00002507
Iteration 123/1000 | Loss: 0.00002507
Iteration 124/1000 | Loss: 0.00002507
Iteration 125/1000 | Loss: 0.00002507
Iteration 126/1000 | Loss: 0.00002507
Iteration 127/1000 | Loss: 0.00002507
Iteration 128/1000 | Loss: 0.00002507
Iteration 129/1000 | Loss: 0.00002507
Iteration 130/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.5074288714677095e-05, 2.5074288714677095e-05, 2.5074288714677095e-05, 2.5074288714677095e-05, 2.5074288714677095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5074288714677095e-05

Optimization complete. Final v2v error: 4.330318450927734 mm

Highest mean error: 4.575511932373047 mm for frame 54

Lowest mean error: 3.922029972076416 mm for frame 7

Saving results

Total time: 38.656094789505005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903956
Iteration 2/25 | Loss: 0.00150983
Iteration 3/25 | Loss: 0.00129859
Iteration 4/25 | Loss: 0.00128172
Iteration 5/25 | Loss: 0.00127746
Iteration 6/25 | Loss: 0.00127666
Iteration 7/25 | Loss: 0.00127666
Iteration 8/25 | Loss: 0.00127666
Iteration 9/25 | Loss: 0.00127666
Iteration 10/25 | Loss: 0.00127666
Iteration 11/25 | Loss: 0.00127666
Iteration 12/25 | Loss: 0.00127666
Iteration 13/25 | Loss: 0.00127666
Iteration 14/25 | Loss: 0.00127666
Iteration 15/25 | Loss: 0.00127666
Iteration 16/25 | Loss: 0.00127666
Iteration 17/25 | Loss: 0.00127666
Iteration 18/25 | Loss: 0.00127666
Iteration 19/25 | Loss: 0.00127666
Iteration 20/25 | Loss: 0.00127666
Iteration 21/25 | Loss: 0.00127666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012766564032062888, 0.0012766564032062888, 0.0012766564032062888, 0.0012766564032062888, 0.0012766564032062888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012766564032062888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14764905
Iteration 2/25 | Loss: 0.00220965
Iteration 3/25 | Loss: 0.00220965
Iteration 4/25 | Loss: 0.00220965
Iteration 5/25 | Loss: 0.00220965
Iteration 6/25 | Loss: 0.00220965
Iteration 7/25 | Loss: 0.00220965
Iteration 8/25 | Loss: 0.00220965
Iteration 9/25 | Loss: 0.00220965
Iteration 10/25 | Loss: 0.00220965
Iteration 11/25 | Loss: 0.00220965
Iteration 12/25 | Loss: 0.00220965
Iteration 13/25 | Loss: 0.00220965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0022096480242908, 0.0022096480242908, 0.0022096480242908, 0.0022096480242908, 0.0022096480242908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022096480242908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220965
Iteration 2/1000 | Loss: 0.00007337
Iteration 3/1000 | Loss: 0.00004464
Iteration 4/1000 | Loss: 0.00003709
Iteration 5/1000 | Loss: 0.00003467
Iteration 6/1000 | Loss: 0.00003265
Iteration 7/1000 | Loss: 0.00003116
Iteration 8/1000 | Loss: 0.00003007
Iteration 9/1000 | Loss: 0.00002914
Iteration 10/1000 | Loss: 0.00002843
Iteration 11/1000 | Loss: 0.00002780
Iteration 12/1000 | Loss: 0.00002738
Iteration 13/1000 | Loss: 0.00002707
Iteration 14/1000 | Loss: 0.00002690
Iteration 15/1000 | Loss: 0.00002688
Iteration 16/1000 | Loss: 0.00002677
Iteration 17/1000 | Loss: 0.00002665
Iteration 18/1000 | Loss: 0.00002662
Iteration 19/1000 | Loss: 0.00002659
Iteration 20/1000 | Loss: 0.00002658
Iteration 21/1000 | Loss: 0.00002657
Iteration 22/1000 | Loss: 0.00002657
Iteration 23/1000 | Loss: 0.00002656
Iteration 24/1000 | Loss: 0.00002656
Iteration 25/1000 | Loss: 0.00002656
Iteration 26/1000 | Loss: 0.00002655
Iteration 27/1000 | Loss: 0.00002654
Iteration 28/1000 | Loss: 0.00002653
Iteration 29/1000 | Loss: 0.00002653
Iteration 30/1000 | Loss: 0.00002653
Iteration 31/1000 | Loss: 0.00002652
Iteration 32/1000 | Loss: 0.00002652
Iteration 33/1000 | Loss: 0.00002652
Iteration 34/1000 | Loss: 0.00002651
Iteration 35/1000 | Loss: 0.00002651
Iteration 36/1000 | Loss: 0.00002651
Iteration 37/1000 | Loss: 0.00002651
Iteration 38/1000 | Loss: 0.00002651
Iteration 39/1000 | Loss: 0.00002651
Iteration 40/1000 | Loss: 0.00002651
Iteration 41/1000 | Loss: 0.00002650
Iteration 42/1000 | Loss: 0.00002650
Iteration 43/1000 | Loss: 0.00002650
Iteration 44/1000 | Loss: 0.00002647
Iteration 45/1000 | Loss: 0.00002647
Iteration 46/1000 | Loss: 0.00002646
Iteration 47/1000 | Loss: 0.00002646
Iteration 48/1000 | Loss: 0.00002645
Iteration 49/1000 | Loss: 0.00002645
Iteration 50/1000 | Loss: 0.00002645
Iteration 51/1000 | Loss: 0.00002643
Iteration 52/1000 | Loss: 0.00002643
Iteration 53/1000 | Loss: 0.00002643
Iteration 54/1000 | Loss: 0.00002642
Iteration 55/1000 | Loss: 0.00002642
Iteration 56/1000 | Loss: 0.00002642
Iteration 57/1000 | Loss: 0.00002642
Iteration 58/1000 | Loss: 0.00002642
Iteration 59/1000 | Loss: 0.00002642
Iteration 60/1000 | Loss: 0.00002642
Iteration 61/1000 | Loss: 0.00002642
Iteration 62/1000 | Loss: 0.00002642
Iteration 63/1000 | Loss: 0.00002641
Iteration 64/1000 | Loss: 0.00002641
Iteration 65/1000 | Loss: 0.00002640
Iteration 66/1000 | Loss: 0.00002640
Iteration 67/1000 | Loss: 0.00002640
Iteration 68/1000 | Loss: 0.00002640
Iteration 69/1000 | Loss: 0.00002640
Iteration 70/1000 | Loss: 0.00002640
Iteration 71/1000 | Loss: 0.00002639
Iteration 72/1000 | Loss: 0.00002639
Iteration 73/1000 | Loss: 0.00002639
Iteration 74/1000 | Loss: 0.00002638
Iteration 75/1000 | Loss: 0.00002638
Iteration 76/1000 | Loss: 0.00002638
Iteration 77/1000 | Loss: 0.00002638
Iteration 78/1000 | Loss: 0.00002637
Iteration 79/1000 | Loss: 0.00002637
Iteration 80/1000 | Loss: 0.00002637
Iteration 81/1000 | Loss: 0.00002636
Iteration 82/1000 | Loss: 0.00002636
Iteration 83/1000 | Loss: 0.00002636
Iteration 84/1000 | Loss: 0.00002636
Iteration 85/1000 | Loss: 0.00002636
Iteration 86/1000 | Loss: 0.00002636
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002636
Iteration 89/1000 | Loss: 0.00002636
Iteration 90/1000 | Loss: 0.00002636
Iteration 91/1000 | Loss: 0.00002636
Iteration 92/1000 | Loss: 0.00002636
Iteration 93/1000 | Loss: 0.00002636
Iteration 94/1000 | Loss: 0.00002636
Iteration 95/1000 | Loss: 0.00002635
Iteration 96/1000 | Loss: 0.00002634
Iteration 97/1000 | Loss: 0.00002634
Iteration 98/1000 | Loss: 0.00002634
Iteration 99/1000 | Loss: 0.00002634
Iteration 100/1000 | Loss: 0.00002633
Iteration 101/1000 | Loss: 0.00002633
Iteration 102/1000 | Loss: 0.00002633
Iteration 103/1000 | Loss: 0.00002633
Iteration 104/1000 | Loss: 0.00002633
Iteration 105/1000 | Loss: 0.00002633
Iteration 106/1000 | Loss: 0.00002633
Iteration 107/1000 | Loss: 0.00002633
Iteration 108/1000 | Loss: 0.00002633
Iteration 109/1000 | Loss: 0.00002633
Iteration 110/1000 | Loss: 0.00002633
Iteration 111/1000 | Loss: 0.00002632
Iteration 112/1000 | Loss: 0.00002632
Iteration 113/1000 | Loss: 0.00002632
Iteration 114/1000 | Loss: 0.00002632
Iteration 115/1000 | Loss: 0.00002632
Iteration 116/1000 | Loss: 0.00002632
Iteration 117/1000 | Loss: 0.00002632
Iteration 118/1000 | Loss: 0.00002632
Iteration 119/1000 | Loss: 0.00002631
Iteration 120/1000 | Loss: 0.00002631
Iteration 121/1000 | Loss: 0.00002631
Iteration 122/1000 | Loss: 0.00002631
Iteration 123/1000 | Loss: 0.00002631
Iteration 124/1000 | Loss: 0.00002631
Iteration 125/1000 | Loss: 0.00002631
Iteration 126/1000 | Loss: 0.00002630
Iteration 127/1000 | Loss: 0.00002630
Iteration 128/1000 | Loss: 0.00002630
Iteration 129/1000 | Loss: 0.00002630
Iteration 130/1000 | Loss: 0.00002630
Iteration 131/1000 | Loss: 0.00002630
Iteration 132/1000 | Loss: 0.00002630
Iteration 133/1000 | Loss: 0.00002629
Iteration 134/1000 | Loss: 0.00002629
Iteration 135/1000 | Loss: 0.00002629
Iteration 136/1000 | Loss: 0.00002629
Iteration 137/1000 | Loss: 0.00002629
Iteration 138/1000 | Loss: 0.00002629
Iteration 139/1000 | Loss: 0.00002628
Iteration 140/1000 | Loss: 0.00002628
Iteration 141/1000 | Loss: 0.00002628
Iteration 142/1000 | Loss: 0.00002628
Iteration 143/1000 | Loss: 0.00002628
Iteration 144/1000 | Loss: 0.00002628
Iteration 145/1000 | Loss: 0.00002628
Iteration 146/1000 | Loss: 0.00002628
Iteration 147/1000 | Loss: 0.00002628
Iteration 148/1000 | Loss: 0.00002628
Iteration 149/1000 | Loss: 0.00002627
Iteration 150/1000 | Loss: 0.00002627
Iteration 151/1000 | Loss: 0.00002627
Iteration 152/1000 | Loss: 0.00002626
Iteration 153/1000 | Loss: 0.00002626
Iteration 154/1000 | Loss: 0.00002626
Iteration 155/1000 | Loss: 0.00002626
Iteration 156/1000 | Loss: 0.00002626
Iteration 157/1000 | Loss: 0.00002626
Iteration 158/1000 | Loss: 0.00002626
Iteration 159/1000 | Loss: 0.00002626
Iteration 160/1000 | Loss: 0.00002626
Iteration 161/1000 | Loss: 0.00002626
Iteration 162/1000 | Loss: 0.00002626
Iteration 163/1000 | Loss: 0.00002626
Iteration 164/1000 | Loss: 0.00002626
Iteration 165/1000 | Loss: 0.00002626
Iteration 166/1000 | Loss: 0.00002626
Iteration 167/1000 | Loss: 0.00002626
Iteration 168/1000 | Loss: 0.00002626
Iteration 169/1000 | Loss: 0.00002626
Iteration 170/1000 | Loss: 0.00002626
Iteration 171/1000 | Loss: 0.00002626
Iteration 172/1000 | Loss: 0.00002626
Iteration 173/1000 | Loss: 0.00002626
Iteration 174/1000 | Loss: 0.00002626
Iteration 175/1000 | Loss: 0.00002626
Iteration 176/1000 | Loss: 0.00002626
Iteration 177/1000 | Loss: 0.00002626
Iteration 178/1000 | Loss: 0.00002626
Iteration 179/1000 | Loss: 0.00002626
Iteration 180/1000 | Loss: 0.00002626
Iteration 181/1000 | Loss: 0.00002626
Iteration 182/1000 | Loss: 0.00002626
Iteration 183/1000 | Loss: 0.00002626
Iteration 184/1000 | Loss: 0.00002626
Iteration 185/1000 | Loss: 0.00002626
Iteration 186/1000 | Loss: 0.00002626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.625674824230373e-05, 2.625674824230373e-05, 2.625674824230373e-05, 2.625674824230373e-05, 2.625674824230373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.625674824230373e-05

Optimization complete. Final v2v error: 4.109675884246826 mm

Highest mean error: 5.759734153747559 mm for frame 102

Lowest mean error: 3.180452585220337 mm for frame 138

Saving results

Total time: 47.520620822906494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953544
Iteration 2/25 | Loss: 0.00196771
Iteration 3/25 | Loss: 0.00154251
Iteration 4/25 | Loss: 0.00146756
Iteration 5/25 | Loss: 0.00143916
Iteration 6/25 | Loss: 0.00139779
Iteration 7/25 | Loss: 0.00136071
Iteration 8/25 | Loss: 0.00134310
Iteration 9/25 | Loss: 0.00132467
Iteration 10/25 | Loss: 0.00131898
Iteration 11/25 | Loss: 0.00132104
Iteration 12/25 | Loss: 0.00130692
Iteration 13/25 | Loss: 0.00130401
Iteration 14/25 | Loss: 0.00130342
Iteration 15/25 | Loss: 0.00130315
Iteration 16/25 | Loss: 0.00130666
Iteration 17/25 | Loss: 0.00129619
Iteration 18/25 | Loss: 0.00129579
Iteration 19/25 | Loss: 0.00129575
Iteration 20/25 | Loss: 0.00129575
Iteration 21/25 | Loss: 0.00129575
Iteration 22/25 | Loss: 0.00129575
Iteration 23/25 | Loss: 0.00129575
Iteration 24/25 | Loss: 0.00129575
Iteration 25/25 | Loss: 0.00129575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33677268
Iteration 2/25 | Loss: 0.00221669
Iteration 3/25 | Loss: 0.00221668
Iteration 4/25 | Loss: 0.00221668
Iteration 5/25 | Loss: 0.00221668
Iteration 6/25 | Loss: 0.00221668
Iteration 7/25 | Loss: 0.00221668
Iteration 8/25 | Loss: 0.00221667
Iteration 9/25 | Loss: 0.00221667
Iteration 10/25 | Loss: 0.00221667
Iteration 11/25 | Loss: 0.00221667
Iteration 12/25 | Loss: 0.00221667
Iteration 13/25 | Loss: 0.00221667
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0022166746202856302, 0.0022166746202856302, 0.0022166746202856302, 0.0022166746202856302, 0.0022166746202856302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022166746202856302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221667
Iteration 2/1000 | Loss: 0.00014278
Iteration 3/1000 | Loss: 0.00010196
Iteration 4/1000 | Loss: 0.00022196
Iteration 5/1000 | Loss: 0.00017704
Iteration 6/1000 | Loss: 0.00012868
Iteration 7/1000 | Loss: 0.00011089
Iteration 8/1000 | Loss: 0.00008717
Iteration 9/1000 | Loss: 0.00010476
Iteration 10/1000 | Loss: 0.00010221
Iteration 11/1000 | Loss: 0.00007451
Iteration 12/1000 | Loss: 0.00010453
Iteration 13/1000 | Loss: 0.00012776
Iteration 14/1000 | Loss: 0.00011023
Iteration 15/1000 | Loss: 0.00006358
Iteration 16/1000 | Loss: 0.00013818
Iteration 17/1000 | Loss: 0.00005888
Iteration 18/1000 | Loss: 0.00004995
Iteration 19/1000 | Loss: 0.00004440
Iteration 20/1000 | Loss: 0.00004067
Iteration 21/1000 | Loss: 0.00003839
Iteration 22/1000 | Loss: 0.00003685
Iteration 23/1000 | Loss: 0.00003571
Iteration 24/1000 | Loss: 0.00003488
Iteration 25/1000 | Loss: 0.00003437
Iteration 26/1000 | Loss: 0.00003410
Iteration 27/1000 | Loss: 0.00003362
Iteration 28/1000 | Loss: 0.00003309
Iteration 29/1000 | Loss: 0.00003259
Iteration 30/1000 | Loss: 0.00003202
Iteration 31/1000 | Loss: 0.00003149
Iteration 32/1000 | Loss: 0.00007374
Iteration 33/1000 | Loss: 0.00004277
Iteration 34/1000 | Loss: 0.00003629
Iteration 35/1000 | Loss: 0.00003356
Iteration 36/1000 | Loss: 0.00003206
Iteration 37/1000 | Loss: 0.00003097
Iteration 38/1000 | Loss: 0.00002990
Iteration 39/1000 | Loss: 0.00002905
Iteration 40/1000 | Loss: 0.00002829
Iteration 41/1000 | Loss: 0.00002790
Iteration 42/1000 | Loss: 0.00003829
Iteration 43/1000 | Loss: 0.00003115
Iteration 44/1000 | Loss: 0.00002973
Iteration 45/1000 | Loss: 0.00002880
Iteration 46/1000 | Loss: 0.00002813
Iteration 47/1000 | Loss: 0.00002748
Iteration 48/1000 | Loss: 0.00002723
Iteration 49/1000 | Loss: 0.00002708
Iteration 50/1000 | Loss: 0.00002707
Iteration 51/1000 | Loss: 0.00002704
Iteration 52/1000 | Loss: 0.00002704
Iteration 53/1000 | Loss: 0.00002703
Iteration 54/1000 | Loss: 0.00002703
Iteration 55/1000 | Loss: 0.00002702
Iteration 56/1000 | Loss: 0.00002702
Iteration 57/1000 | Loss: 0.00002702
Iteration 58/1000 | Loss: 0.00002701
Iteration 59/1000 | Loss: 0.00002701
Iteration 60/1000 | Loss: 0.00002700
Iteration 61/1000 | Loss: 0.00002699
Iteration 62/1000 | Loss: 0.00002699
Iteration 63/1000 | Loss: 0.00002699
Iteration 64/1000 | Loss: 0.00002698
Iteration 65/1000 | Loss: 0.00002698
Iteration 66/1000 | Loss: 0.00002698
Iteration 67/1000 | Loss: 0.00002698
Iteration 68/1000 | Loss: 0.00002697
Iteration 69/1000 | Loss: 0.00002697
Iteration 70/1000 | Loss: 0.00002697
Iteration 71/1000 | Loss: 0.00002697
Iteration 72/1000 | Loss: 0.00002697
Iteration 73/1000 | Loss: 0.00002697
Iteration 74/1000 | Loss: 0.00002697
Iteration 75/1000 | Loss: 0.00002697
Iteration 76/1000 | Loss: 0.00002697
Iteration 77/1000 | Loss: 0.00002697
Iteration 78/1000 | Loss: 0.00002697
Iteration 79/1000 | Loss: 0.00002696
Iteration 80/1000 | Loss: 0.00002696
Iteration 81/1000 | Loss: 0.00002696
Iteration 82/1000 | Loss: 0.00002695
Iteration 83/1000 | Loss: 0.00002695
Iteration 84/1000 | Loss: 0.00002695
Iteration 85/1000 | Loss: 0.00002695
Iteration 86/1000 | Loss: 0.00002694
Iteration 87/1000 | Loss: 0.00002694
Iteration 88/1000 | Loss: 0.00002694
Iteration 89/1000 | Loss: 0.00002694
Iteration 90/1000 | Loss: 0.00002694
Iteration 91/1000 | Loss: 0.00002694
Iteration 92/1000 | Loss: 0.00002694
Iteration 93/1000 | Loss: 0.00002694
Iteration 94/1000 | Loss: 0.00002694
Iteration 95/1000 | Loss: 0.00002694
Iteration 96/1000 | Loss: 0.00002693
Iteration 97/1000 | Loss: 0.00002693
Iteration 98/1000 | Loss: 0.00002693
Iteration 99/1000 | Loss: 0.00002693
Iteration 100/1000 | Loss: 0.00002693
Iteration 101/1000 | Loss: 0.00002693
Iteration 102/1000 | Loss: 0.00002692
Iteration 103/1000 | Loss: 0.00002692
Iteration 104/1000 | Loss: 0.00002692
Iteration 105/1000 | Loss: 0.00002692
Iteration 106/1000 | Loss: 0.00002691
Iteration 107/1000 | Loss: 0.00002691
Iteration 108/1000 | Loss: 0.00002691
Iteration 109/1000 | Loss: 0.00002691
Iteration 110/1000 | Loss: 0.00002691
Iteration 111/1000 | Loss: 0.00002690
Iteration 112/1000 | Loss: 0.00002690
Iteration 113/1000 | Loss: 0.00002690
Iteration 114/1000 | Loss: 0.00002689
Iteration 115/1000 | Loss: 0.00002689
Iteration 116/1000 | Loss: 0.00002689
Iteration 117/1000 | Loss: 0.00002689
Iteration 118/1000 | Loss: 0.00002689
Iteration 119/1000 | Loss: 0.00002689
Iteration 120/1000 | Loss: 0.00002689
Iteration 121/1000 | Loss: 0.00002689
Iteration 122/1000 | Loss: 0.00002689
Iteration 123/1000 | Loss: 0.00002689
Iteration 124/1000 | Loss: 0.00002689
Iteration 125/1000 | Loss: 0.00002689
Iteration 126/1000 | Loss: 0.00002689
Iteration 127/1000 | Loss: 0.00002689
Iteration 128/1000 | Loss: 0.00002689
Iteration 129/1000 | Loss: 0.00002689
Iteration 130/1000 | Loss: 0.00002689
Iteration 131/1000 | Loss: 0.00002689
Iteration 132/1000 | Loss: 0.00002689
Iteration 133/1000 | Loss: 0.00002689
Iteration 134/1000 | Loss: 0.00002689
Iteration 135/1000 | Loss: 0.00002689
Iteration 136/1000 | Loss: 0.00002689
Iteration 137/1000 | Loss: 0.00002688
Iteration 138/1000 | Loss: 0.00002688
Iteration 139/1000 | Loss: 0.00002688
Iteration 140/1000 | Loss: 0.00002688
Iteration 141/1000 | Loss: 0.00002688
Iteration 142/1000 | Loss: 0.00002688
Iteration 143/1000 | Loss: 0.00002688
Iteration 144/1000 | Loss: 0.00002688
Iteration 145/1000 | Loss: 0.00002688
Iteration 146/1000 | Loss: 0.00002688
Iteration 147/1000 | Loss: 0.00002688
Iteration 148/1000 | Loss: 0.00002688
Iteration 149/1000 | Loss: 0.00002688
Iteration 150/1000 | Loss: 0.00002688
Iteration 151/1000 | Loss: 0.00002688
Iteration 152/1000 | Loss: 0.00002688
Iteration 153/1000 | Loss: 0.00002688
Iteration 154/1000 | Loss: 0.00002688
Iteration 155/1000 | Loss: 0.00002688
Iteration 156/1000 | Loss: 0.00002688
Iteration 157/1000 | Loss: 0.00002688
Iteration 158/1000 | Loss: 0.00002688
Iteration 159/1000 | Loss: 0.00002688
Iteration 160/1000 | Loss: 0.00002688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.6883764803642407e-05, 2.6883764803642407e-05, 2.6883764803642407e-05, 2.6883764803642407e-05, 2.6883764803642407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6883764803642407e-05

Optimization complete. Final v2v error: 3.937825918197632 mm

Highest mean error: 8.337064743041992 mm for frame 117

Lowest mean error: 2.8632164001464844 mm for frame 74

Saving results

Total time: 111.2721300125122
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438592
Iteration 2/25 | Loss: 0.00133940
Iteration 3/25 | Loss: 0.00124107
Iteration 4/25 | Loss: 0.00122395
Iteration 5/25 | Loss: 0.00121821
Iteration 6/25 | Loss: 0.00121692
Iteration 7/25 | Loss: 0.00121692
Iteration 8/25 | Loss: 0.00121692
Iteration 9/25 | Loss: 0.00121692
Iteration 10/25 | Loss: 0.00121692
Iteration 11/25 | Loss: 0.00121692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001216915319673717, 0.001216915319673717, 0.001216915319673717, 0.001216915319673717, 0.001216915319673717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001216915319673717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19431329
Iteration 2/25 | Loss: 0.00276591
Iteration 3/25 | Loss: 0.00276591
Iteration 4/25 | Loss: 0.00276591
Iteration 5/25 | Loss: 0.00276591
Iteration 6/25 | Loss: 0.00276590
Iteration 7/25 | Loss: 0.00276590
Iteration 8/25 | Loss: 0.00276590
Iteration 9/25 | Loss: 0.00276590
Iteration 10/25 | Loss: 0.00276590
Iteration 11/25 | Loss: 0.00276590
Iteration 12/25 | Loss: 0.00276590
Iteration 13/25 | Loss: 0.00276590
Iteration 14/25 | Loss: 0.00276590
Iteration 15/25 | Loss: 0.00276590
Iteration 16/25 | Loss: 0.00276590
Iteration 17/25 | Loss: 0.00276590
Iteration 18/25 | Loss: 0.00276590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002765903016552329, 0.002765903016552329, 0.002765903016552329, 0.002765903016552329, 0.002765903016552329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002765903016552329

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276590
Iteration 2/1000 | Loss: 0.00003327
Iteration 3/1000 | Loss: 0.00002347
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00001988
Iteration 6/1000 | Loss: 0.00001877
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001746
Iteration 10/1000 | Loss: 0.00001742
Iteration 11/1000 | Loss: 0.00001723
Iteration 12/1000 | Loss: 0.00001720
Iteration 13/1000 | Loss: 0.00001714
Iteration 14/1000 | Loss: 0.00001712
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001706
Iteration 17/1000 | Loss: 0.00001705
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001702
Iteration 20/1000 | Loss: 0.00001701
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001700
Iteration 23/1000 | Loss: 0.00001699
Iteration 24/1000 | Loss: 0.00001698
Iteration 25/1000 | Loss: 0.00001698
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001696
Iteration 28/1000 | Loss: 0.00001696
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001695
Iteration 31/1000 | Loss: 0.00001694
Iteration 32/1000 | Loss: 0.00001694
Iteration 33/1000 | Loss: 0.00001694
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001693
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001690
Iteration 40/1000 | Loss: 0.00001689
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001687
Iteration 44/1000 | Loss: 0.00001686
Iteration 45/1000 | Loss: 0.00001685
Iteration 46/1000 | Loss: 0.00001685
Iteration 47/1000 | Loss: 0.00001685
Iteration 48/1000 | Loss: 0.00001685
Iteration 49/1000 | Loss: 0.00001685
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001684
Iteration 52/1000 | Loss: 0.00001684
Iteration 53/1000 | Loss: 0.00001684
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001684
Iteration 57/1000 | Loss: 0.00001684
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001683
Iteration 61/1000 | Loss: 0.00001683
Iteration 62/1000 | Loss: 0.00001683
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001683
Iteration 65/1000 | Loss: 0.00001683
Iteration 66/1000 | Loss: 0.00001683
Iteration 67/1000 | Loss: 0.00001682
Iteration 68/1000 | Loss: 0.00001682
Iteration 69/1000 | Loss: 0.00001682
Iteration 70/1000 | Loss: 0.00001682
Iteration 71/1000 | Loss: 0.00001682
Iteration 72/1000 | Loss: 0.00001682
Iteration 73/1000 | Loss: 0.00001682
Iteration 74/1000 | Loss: 0.00001682
Iteration 75/1000 | Loss: 0.00001682
Iteration 76/1000 | Loss: 0.00001682
Iteration 77/1000 | Loss: 0.00001682
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001681
Iteration 84/1000 | Loss: 0.00001681
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001681
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001680
Iteration 100/1000 | Loss: 0.00001680
Iteration 101/1000 | Loss: 0.00001680
Iteration 102/1000 | Loss: 0.00001680
Iteration 103/1000 | Loss: 0.00001680
Iteration 104/1000 | Loss: 0.00001680
Iteration 105/1000 | Loss: 0.00001680
Iteration 106/1000 | Loss: 0.00001680
Iteration 107/1000 | Loss: 0.00001680
Iteration 108/1000 | Loss: 0.00001680
Iteration 109/1000 | Loss: 0.00001680
Iteration 110/1000 | Loss: 0.00001680
Iteration 111/1000 | Loss: 0.00001680
Iteration 112/1000 | Loss: 0.00001680
Iteration 113/1000 | Loss: 0.00001680
Iteration 114/1000 | Loss: 0.00001680
Iteration 115/1000 | Loss: 0.00001680
Iteration 116/1000 | Loss: 0.00001680
Iteration 117/1000 | Loss: 0.00001680
Iteration 118/1000 | Loss: 0.00001680
Iteration 119/1000 | Loss: 0.00001680
Iteration 120/1000 | Loss: 0.00001680
Iteration 121/1000 | Loss: 0.00001680
Iteration 122/1000 | Loss: 0.00001680
Iteration 123/1000 | Loss: 0.00001680
Iteration 124/1000 | Loss: 0.00001680
Iteration 125/1000 | Loss: 0.00001680
Iteration 126/1000 | Loss: 0.00001680
Iteration 127/1000 | Loss: 0.00001680
Iteration 128/1000 | Loss: 0.00001680
Iteration 129/1000 | Loss: 0.00001680
Iteration 130/1000 | Loss: 0.00001680
Iteration 131/1000 | Loss: 0.00001680
Iteration 132/1000 | Loss: 0.00001680
Iteration 133/1000 | Loss: 0.00001680
Iteration 134/1000 | Loss: 0.00001680
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001680
Iteration 139/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.680179593677167e-05, 1.680179593677167e-05, 1.680179593677167e-05, 1.680179593677167e-05, 1.680179593677167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.680179593677167e-05

Optimization complete. Final v2v error: 3.434666395187378 mm

Highest mean error: 3.7067511081695557 mm for frame 18

Lowest mean error: 2.9248502254486084 mm for frame 39

Saving results

Total time: 31.893386125564575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834870
Iteration 2/25 | Loss: 0.00159055
Iteration 3/25 | Loss: 0.00129482
Iteration 4/25 | Loss: 0.00123707
Iteration 5/25 | Loss: 0.00123180
Iteration 6/25 | Loss: 0.00123129
Iteration 7/25 | Loss: 0.00123111
Iteration 8/25 | Loss: 0.00123111
Iteration 9/25 | Loss: 0.00123111
Iteration 10/25 | Loss: 0.00123111
Iteration 11/25 | Loss: 0.00123111
Iteration 12/25 | Loss: 0.00123111
Iteration 13/25 | Loss: 0.00123111
Iteration 14/25 | Loss: 0.00123111
Iteration 15/25 | Loss: 0.00123111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012311070458963513, 0.0012311070458963513, 0.0012311070458963513, 0.0012311070458963513, 0.0012311070458963513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012311070458963513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10830200
Iteration 2/25 | Loss: 0.00287384
Iteration 3/25 | Loss: 0.00287384
Iteration 4/25 | Loss: 0.00287384
Iteration 5/25 | Loss: 0.00287384
Iteration 6/25 | Loss: 0.00287384
Iteration 7/25 | Loss: 0.00287384
Iteration 8/25 | Loss: 0.00287384
Iteration 9/25 | Loss: 0.00287384
Iteration 10/25 | Loss: 0.00287384
Iteration 11/25 | Loss: 0.00287384
Iteration 12/25 | Loss: 0.00287384
Iteration 13/25 | Loss: 0.00287384
Iteration 14/25 | Loss: 0.00287384
Iteration 15/25 | Loss: 0.00287384
Iteration 16/25 | Loss: 0.00287384
Iteration 17/25 | Loss: 0.00287384
Iteration 18/25 | Loss: 0.00287384
Iteration 19/25 | Loss: 0.00287384
Iteration 20/25 | Loss: 0.00287384
Iteration 21/25 | Loss: 0.00287384
Iteration 22/25 | Loss: 0.00287384
Iteration 23/25 | Loss: 0.00287384
Iteration 24/25 | Loss: 0.00287384
Iteration 25/25 | Loss: 0.00287384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00287384
Iteration 2/1000 | Loss: 0.00004295
Iteration 3/1000 | Loss: 0.00002864
Iteration 4/1000 | Loss: 0.00002393
Iteration 5/1000 | Loss: 0.00002159
Iteration 6/1000 | Loss: 0.00001944
Iteration 7/1000 | Loss: 0.00001822
Iteration 8/1000 | Loss: 0.00001775
Iteration 9/1000 | Loss: 0.00001730
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001676
Iteration 12/1000 | Loss: 0.00001652
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001639
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001635
Iteration 20/1000 | Loss: 0.00001632
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001630
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001629
Iteration 26/1000 | Loss: 0.00001629
Iteration 27/1000 | Loss: 0.00001629
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001629
Iteration 31/1000 | Loss: 0.00001628
Iteration 32/1000 | Loss: 0.00001628
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001624
Iteration 41/1000 | Loss: 0.00001624
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001623
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001622
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001621
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001620
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001617
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001615
Iteration 85/1000 | Loss: 0.00001615
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001615
Iteration 93/1000 | Loss: 0.00001615
Iteration 94/1000 | Loss: 0.00001615
Iteration 95/1000 | Loss: 0.00001615
Iteration 96/1000 | Loss: 0.00001615
Iteration 97/1000 | Loss: 0.00001615
Iteration 98/1000 | Loss: 0.00001615
Iteration 99/1000 | Loss: 0.00001615
Iteration 100/1000 | Loss: 0.00001614
Iteration 101/1000 | Loss: 0.00001614
Iteration 102/1000 | Loss: 0.00001614
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001612
Iteration 108/1000 | Loss: 0.00001612
Iteration 109/1000 | Loss: 0.00001612
Iteration 110/1000 | Loss: 0.00001612
Iteration 111/1000 | Loss: 0.00001612
Iteration 112/1000 | Loss: 0.00001612
Iteration 113/1000 | Loss: 0.00001612
Iteration 114/1000 | Loss: 0.00001612
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001610
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001610
Iteration 125/1000 | Loss: 0.00001610
Iteration 126/1000 | Loss: 0.00001610
Iteration 127/1000 | Loss: 0.00001610
Iteration 128/1000 | Loss: 0.00001610
Iteration 129/1000 | Loss: 0.00001610
Iteration 130/1000 | Loss: 0.00001610
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001609
Iteration 133/1000 | Loss: 0.00001609
Iteration 134/1000 | Loss: 0.00001609
Iteration 135/1000 | Loss: 0.00001609
Iteration 136/1000 | Loss: 0.00001609
Iteration 137/1000 | Loss: 0.00001609
Iteration 138/1000 | Loss: 0.00001609
Iteration 139/1000 | Loss: 0.00001608
Iteration 140/1000 | Loss: 0.00001608
Iteration 141/1000 | Loss: 0.00001608
Iteration 142/1000 | Loss: 0.00001608
Iteration 143/1000 | Loss: 0.00001608
Iteration 144/1000 | Loss: 0.00001608
Iteration 145/1000 | Loss: 0.00001608
Iteration 146/1000 | Loss: 0.00001608
Iteration 147/1000 | Loss: 0.00001608
Iteration 148/1000 | Loss: 0.00001608
Iteration 149/1000 | Loss: 0.00001608
Iteration 150/1000 | Loss: 0.00001608
Iteration 151/1000 | Loss: 0.00001608
Iteration 152/1000 | Loss: 0.00001608
Iteration 153/1000 | Loss: 0.00001608
Iteration 154/1000 | Loss: 0.00001608
Iteration 155/1000 | Loss: 0.00001608
Iteration 156/1000 | Loss: 0.00001608
Iteration 157/1000 | Loss: 0.00001608
Iteration 158/1000 | Loss: 0.00001608
Iteration 159/1000 | Loss: 0.00001608
Iteration 160/1000 | Loss: 0.00001608
Iteration 161/1000 | Loss: 0.00001608
Iteration 162/1000 | Loss: 0.00001608
Iteration 163/1000 | Loss: 0.00001608
Iteration 164/1000 | Loss: 0.00001608
Iteration 165/1000 | Loss: 0.00001608
Iteration 166/1000 | Loss: 0.00001608
Iteration 167/1000 | Loss: 0.00001608
Iteration 168/1000 | Loss: 0.00001608
Iteration 169/1000 | Loss: 0.00001608
Iteration 170/1000 | Loss: 0.00001608
Iteration 171/1000 | Loss: 0.00001608
Iteration 172/1000 | Loss: 0.00001608
Iteration 173/1000 | Loss: 0.00001608
Iteration 174/1000 | Loss: 0.00001608
Iteration 175/1000 | Loss: 0.00001608
Iteration 176/1000 | Loss: 0.00001608
Iteration 177/1000 | Loss: 0.00001608
Iteration 178/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.608106686035171e-05, 1.608106686035171e-05, 1.608106686035171e-05, 1.608106686035171e-05, 1.608106686035171e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.608106686035171e-05

Optimization complete. Final v2v error: 3.3740952014923096 mm

Highest mean error: 3.6638753414154053 mm for frame 77

Lowest mean error: 3.0471432209014893 mm for frame 12

Saving results

Total time: 39.372140645980835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998601
Iteration 2/25 | Loss: 0.00333596
Iteration 3/25 | Loss: 0.00198398
Iteration 4/25 | Loss: 0.00180454
Iteration 5/25 | Loss: 0.00175814
Iteration 6/25 | Loss: 0.00163447
Iteration 7/25 | Loss: 0.00158242
Iteration 8/25 | Loss: 0.00153918
Iteration 9/25 | Loss: 0.00150866
Iteration 10/25 | Loss: 0.00150208
Iteration 11/25 | Loss: 0.00149514
Iteration 12/25 | Loss: 0.00148471
Iteration 13/25 | Loss: 0.00148001
Iteration 14/25 | Loss: 0.00147808
Iteration 15/25 | Loss: 0.00147749
Iteration 16/25 | Loss: 0.00147907
Iteration 17/25 | Loss: 0.00147486
Iteration 18/25 | Loss: 0.00147383
Iteration 19/25 | Loss: 0.00147336
Iteration 20/25 | Loss: 0.00147325
Iteration 21/25 | Loss: 0.00147322
Iteration 22/25 | Loss: 0.00147322
Iteration 23/25 | Loss: 0.00147322
Iteration 24/25 | Loss: 0.00147318
Iteration 25/25 | Loss: 0.00147312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67897648
Iteration 2/25 | Loss: 0.00246605
Iteration 3/25 | Loss: 0.00211140
Iteration 4/25 | Loss: 0.00211140
Iteration 5/25 | Loss: 0.00211140
Iteration 6/25 | Loss: 0.00211140
Iteration 7/25 | Loss: 0.00211140
Iteration 8/25 | Loss: 0.00211140
Iteration 9/25 | Loss: 0.00211140
Iteration 10/25 | Loss: 0.00211140
Iteration 11/25 | Loss: 0.00211140
Iteration 12/25 | Loss: 0.00211140
Iteration 13/25 | Loss: 0.00211140
Iteration 14/25 | Loss: 0.00211140
Iteration 15/25 | Loss: 0.00211140
Iteration 16/25 | Loss: 0.00211140
Iteration 17/25 | Loss: 0.00211140
Iteration 18/25 | Loss: 0.00211140
Iteration 19/25 | Loss: 0.00211140
Iteration 20/25 | Loss: 0.00211140
Iteration 21/25 | Loss: 0.00211140
Iteration 22/25 | Loss: 0.00211140
Iteration 23/25 | Loss: 0.00211140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0021113997790962458, 0.0021113997790962458, 0.0021113997790962458, 0.0021113997790962458, 0.0021113997790962458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021113997790962458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211140
Iteration 2/1000 | Loss: 0.00024305
Iteration 3/1000 | Loss: 0.00032635
Iteration 4/1000 | Loss: 0.00023045
Iteration 5/1000 | Loss: 0.00037556
Iteration 6/1000 | Loss: 0.00015803
Iteration 7/1000 | Loss: 0.00018770
Iteration 8/1000 | Loss: 0.00013580
Iteration 9/1000 | Loss: 0.00012872
Iteration 10/1000 | Loss: 0.00029968
Iteration 11/1000 | Loss: 0.00012382
Iteration 12/1000 | Loss: 0.00011939
Iteration 13/1000 | Loss: 0.00011746
Iteration 14/1000 | Loss: 0.00036906
Iteration 15/1000 | Loss: 0.00383553
Iteration 16/1000 | Loss: 0.00137619
Iteration 17/1000 | Loss: 0.00017877
Iteration 18/1000 | Loss: 0.00013161
Iteration 19/1000 | Loss: 0.00040371
Iteration 20/1000 | Loss: 0.00011063
Iteration 21/1000 | Loss: 0.00009395
Iteration 22/1000 | Loss: 0.00008063
Iteration 23/1000 | Loss: 0.00007374
Iteration 24/1000 | Loss: 0.00020851
Iteration 25/1000 | Loss: 0.00006715
Iteration 26/1000 | Loss: 0.00006428
Iteration 27/1000 | Loss: 0.00006144
Iteration 28/1000 | Loss: 0.00005944
Iteration 29/1000 | Loss: 0.00005783
Iteration 30/1000 | Loss: 0.00005639
Iteration 31/1000 | Loss: 0.00005545
Iteration 32/1000 | Loss: 0.00005478
Iteration 33/1000 | Loss: 0.00005431
Iteration 34/1000 | Loss: 0.00005403
Iteration 35/1000 | Loss: 0.00005385
Iteration 36/1000 | Loss: 0.00005376
Iteration 37/1000 | Loss: 0.00005375
Iteration 38/1000 | Loss: 0.00005374
Iteration 39/1000 | Loss: 0.00005374
Iteration 40/1000 | Loss: 0.00005374
Iteration 41/1000 | Loss: 0.00005373
Iteration 42/1000 | Loss: 0.00005368
Iteration 43/1000 | Loss: 0.00005358
Iteration 44/1000 | Loss: 0.00005347
Iteration 45/1000 | Loss: 0.00005342
Iteration 46/1000 | Loss: 0.00005342
Iteration 47/1000 | Loss: 0.00005342
Iteration 48/1000 | Loss: 0.00005340
Iteration 49/1000 | Loss: 0.00005339
Iteration 50/1000 | Loss: 0.00005339
Iteration 51/1000 | Loss: 0.00005337
Iteration 52/1000 | Loss: 0.00005336
Iteration 53/1000 | Loss: 0.00005326
Iteration 54/1000 | Loss: 0.00005326
Iteration 55/1000 | Loss: 0.00005325
Iteration 56/1000 | Loss: 0.00005325
Iteration 57/1000 | Loss: 0.00005325
Iteration 58/1000 | Loss: 0.00005324
Iteration 59/1000 | Loss: 0.00005324
Iteration 60/1000 | Loss: 0.00005323
Iteration 61/1000 | Loss: 0.00005323
Iteration 62/1000 | Loss: 0.00005323
Iteration 63/1000 | Loss: 0.00005323
Iteration 64/1000 | Loss: 0.00005323
Iteration 65/1000 | Loss: 0.00005323
Iteration 66/1000 | Loss: 0.00005323
Iteration 67/1000 | Loss: 0.00005323
Iteration 68/1000 | Loss: 0.00005323
Iteration 69/1000 | Loss: 0.00005323
Iteration 70/1000 | Loss: 0.00005323
Iteration 71/1000 | Loss: 0.00005323
Iteration 72/1000 | Loss: 0.00005323
Iteration 73/1000 | Loss: 0.00005322
Iteration 74/1000 | Loss: 0.00005322
Iteration 75/1000 | Loss: 0.00005322
Iteration 76/1000 | Loss: 0.00005322
Iteration 77/1000 | Loss: 0.00005322
Iteration 78/1000 | Loss: 0.00005322
Iteration 79/1000 | Loss: 0.00005322
Iteration 80/1000 | Loss: 0.00005322
Iteration 81/1000 | Loss: 0.00005322
Iteration 82/1000 | Loss: 0.00005321
Iteration 83/1000 | Loss: 0.00005321
Iteration 84/1000 | Loss: 0.00005321
Iteration 85/1000 | Loss: 0.00005321
Iteration 86/1000 | Loss: 0.00005321
Iteration 87/1000 | Loss: 0.00005321
Iteration 88/1000 | Loss: 0.00005321
Iteration 89/1000 | Loss: 0.00005321
Iteration 90/1000 | Loss: 0.00005321
Iteration 91/1000 | Loss: 0.00005321
Iteration 92/1000 | Loss: 0.00005321
Iteration 93/1000 | Loss: 0.00005320
Iteration 94/1000 | Loss: 0.00005320
Iteration 95/1000 | Loss: 0.00005320
Iteration 96/1000 | Loss: 0.00005320
Iteration 97/1000 | Loss: 0.00005320
Iteration 98/1000 | Loss: 0.00005319
Iteration 99/1000 | Loss: 0.00005319
Iteration 100/1000 | Loss: 0.00005319
Iteration 101/1000 | Loss: 0.00005319
Iteration 102/1000 | Loss: 0.00005319
Iteration 103/1000 | Loss: 0.00005318
Iteration 104/1000 | Loss: 0.00005318
Iteration 105/1000 | Loss: 0.00005318
Iteration 106/1000 | Loss: 0.00005318
Iteration 107/1000 | Loss: 0.00005318
Iteration 108/1000 | Loss: 0.00005318
Iteration 109/1000 | Loss: 0.00005317
Iteration 110/1000 | Loss: 0.00005317
Iteration 111/1000 | Loss: 0.00005317
Iteration 112/1000 | Loss: 0.00005317
Iteration 113/1000 | Loss: 0.00005317
Iteration 114/1000 | Loss: 0.00005317
Iteration 115/1000 | Loss: 0.00005317
Iteration 116/1000 | Loss: 0.00005316
Iteration 117/1000 | Loss: 0.00005316
Iteration 118/1000 | Loss: 0.00005316
Iteration 119/1000 | Loss: 0.00005316
Iteration 120/1000 | Loss: 0.00005316
Iteration 121/1000 | Loss: 0.00005316
Iteration 122/1000 | Loss: 0.00005315
Iteration 123/1000 | Loss: 0.00005315
Iteration 124/1000 | Loss: 0.00005315
Iteration 125/1000 | Loss: 0.00005315
Iteration 126/1000 | Loss: 0.00005314
Iteration 127/1000 | Loss: 0.00005314
Iteration 128/1000 | Loss: 0.00005314
Iteration 129/1000 | Loss: 0.00005314
Iteration 130/1000 | Loss: 0.00005314
Iteration 131/1000 | Loss: 0.00005314
Iteration 132/1000 | Loss: 0.00005314
Iteration 133/1000 | Loss: 0.00005314
Iteration 134/1000 | Loss: 0.00005314
Iteration 135/1000 | Loss: 0.00005314
Iteration 136/1000 | Loss: 0.00005314
Iteration 137/1000 | Loss: 0.00005314
Iteration 138/1000 | Loss: 0.00005314
Iteration 139/1000 | Loss: 0.00005313
Iteration 140/1000 | Loss: 0.00005313
Iteration 141/1000 | Loss: 0.00005313
Iteration 142/1000 | Loss: 0.00005313
Iteration 143/1000 | Loss: 0.00005313
Iteration 144/1000 | Loss: 0.00005313
Iteration 145/1000 | Loss: 0.00005313
Iteration 146/1000 | Loss: 0.00005313
Iteration 147/1000 | Loss: 0.00005312
Iteration 148/1000 | Loss: 0.00005312
Iteration 149/1000 | Loss: 0.00005312
Iteration 150/1000 | Loss: 0.00005312
Iteration 151/1000 | Loss: 0.00005312
Iteration 152/1000 | Loss: 0.00005312
Iteration 153/1000 | Loss: 0.00005312
Iteration 154/1000 | Loss: 0.00005312
Iteration 155/1000 | Loss: 0.00005312
Iteration 156/1000 | Loss: 0.00005312
Iteration 157/1000 | Loss: 0.00005312
Iteration 158/1000 | Loss: 0.00005312
Iteration 159/1000 | Loss: 0.00005312
Iteration 160/1000 | Loss: 0.00005312
Iteration 161/1000 | Loss: 0.00005312
Iteration 162/1000 | Loss: 0.00005312
Iteration 163/1000 | Loss: 0.00005312
Iteration 164/1000 | Loss: 0.00005311
Iteration 165/1000 | Loss: 0.00005311
Iteration 166/1000 | Loss: 0.00005311
Iteration 167/1000 | Loss: 0.00005311
Iteration 168/1000 | Loss: 0.00005311
Iteration 169/1000 | Loss: 0.00005311
Iteration 170/1000 | Loss: 0.00005311
Iteration 171/1000 | Loss: 0.00005311
Iteration 172/1000 | Loss: 0.00005311
Iteration 173/1000 | Loss: 0.00005311
Iteration 174/1000 | Loss: 0.00005311
Iteration 175/1000 | Loss: 0.00005311
Iteration 176/1000 | Loss: 0.00005311
Iteration 177/1000 | Loss: 0.00005311
Iteration 178/1000 | Loss: 0.00005311
Iteration 179/1000 | Loss: 0.00005311
Iteration 180/1000 | Loss: 0.00005311
Iteration 181/1000 | Loss: 0.00005311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [5.311409040587023e-05, 5.311409040587023e-05, 5.311409040587023e-05, 5.311409040587023e-05, 5.311409040587023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.311409040587023e-05

Optimization complete. Final v2v error: 5.207453727722168 mm

Highest mean error: 6.422231674194336 mm for frame 146

Lowest mean error: 3.765127658843994 mm for frame 107

Saving results

Total time: 118.36238241195679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021386
Iteration 2/25 | Loss: 0.00305599
Iteration 3/25 | Loss: 0.00223579
Iteration 4/25 | Loss: 0.00204746
Iteration 5/25 | Loss: 0.00190573
Iteration 6/25 | Loss: 0.00178690
Iteration 7/25 | Loss: 0.00174204
Iteration 8/25 | Loss: 0.00169628
Iteration 9/25 | Loss: 0.00166963
Iteration 10/25 | Loss: 0.00165472
Iteration 11/25 | Loss: 0.00165233
Iteration 12/25 | Loss: 0.00165360
Iteration 13/25 | Loss: 0.00163943
Iteration 14/25 | Loss: 0.00163965
Iteration 15/25 | Loss: 0.00163687
Iteration 16/25 | Loss: 0.00164297
Iteration 17/25 | Loss: 0.00163684
Iteration 18/25 | Loss: 0.00163310
Iteration 19/25 | Loss: 0.00163345
Iteration 20/25 | Loss: 0.00163082
Iteration 21/25 | Loss: 0.00163130
Iteration 22/25 | Loss: 0.00163050
Iteration 23/25 | Loss: 0.00162917
Iteration 24/25 | Loss: 0.00162964
Iteration 25/25 | Loss: 0.00162667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13002443
Iteration 2/25 | Loss: 0.00763220
Iteration 3/25 | Loss: 0.00698139
Iteration 4/25 | Loss: 0.00694268
Iteration 5/25 | Loss: 0.00694268
Iteration 6/25 | Loss: 0.00694268
Iteration 7/25 | Loss: 0.00694268
Iteration 8/25 | Loss: 0.00694268
Iteration 9/25 | Loss: 0.00694268
Iteration 10/25 | Loss: 0.00694268
Iteration 11/25 | Loss: 0.00694268
Iteration 12/25 | Loss: 0.00694268
Iteration 13/25 | Loss: 0.00694268
Iteration 14/25 | Loss: 0.00694268
Iteration 15/25 | Loss: 0.00694268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.006942677777260542, 0.006942677777260542, 0.006942677777260542, 0.006942677777260542, 0.006942677777260542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006942677777260542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00694268
Iteration 2/1000 | Loss: 0.00102959
Iteration 3/1000 | Loss: 0.00075613
Iteration 4/1000 | Loss: 0.00070648
Iteration 5/1000 | Loss: 0.00216034
Iteration 6/1000 | Loss: 0.00165309
Iteration 7/1000 | Loss: 0.00131607
Iteration 8/1000 | Loss: 0.00056213
Iteration 9/1000 | Loss: 0.00052774
Iteration 10/1000 | Loss: 0.00059408
Iteration 11/1000 | Loss: 0.00132940
Iteration 12/1000 | Loss: 0.00110803
Iteration 13/1000 | Loss: 0.00095822
Iteration 14/1000 | Loss: 0.00132582
Iteration 15/1000 | Loss: 0.00033547
Iteration 16/1000 | Loss: 0.00034323
Iteration 17/1000 | Loss: 0.00065620
Iteration 18/1000 | Loss: 0.00036128
Iteration 19/1000 | Loss: 0.00034914
Iteration 20/1000 | Loss: 0.00035692
Iteration 21/1000 | Loss: 0.00037292
Iteration 22/1000 | Loss: 0.00028731
Iteration 23/1000 | Loss: 0.00101866
Iteration 24/1000 | Loss: 0.00148978
Iteration 25/1000 | Loss: 0.00261742
Iteration 26/1000 | Loss: 0.00096591
Iteration 27/1000 | Loss: 0.00070795
Iteration 28/1000 | Loss: 0.00031976
Iteration 29/1000 | Loss: 0.00062385
Iteration 30/1000 | Loss: 0.00111159
Iteration 31/1000 | Loss: 0.00066235
Iteration 32/1000 | Loss: 0.00033843
Iteration 33/1000 | Loss: 0.00051843
Iteration 34/1000 | Loss: 0.00043518
Iteration 35/1000 | Loss: 0.00072450
Iteration 36/1000 | Loss: 0.00028754
Iteration 37/1000 | Loss: 0.00028304
Iteration 38/1000 | Loss: 0.00084986
Iteration 39/1000 | Loss: 0.00634263
Iteration 40/1000 | Loss: 0.00212206
Iteration 41/1000 | Loss: 0.00190898
Iteration 42/1000 | Loss: 0.00075521
Iteration 43/1000 | Loss: 0.00087871
Iteration 44/1000 | Loss: 0.00044067
Iteration 45/1000 | Loss: 0.00065269
Iteration 46/1000 | Loss: 0.00062689
Iteration 47/1000 | Loss: 0.00052556
Iteration 48/1000 | Loss: 0.00048736
Iteration 49/1000 | Loss: 0.00091905
Iteration 50/1000 | Loss: 0.00199260
Iteration 51/1000 | Loss: 0.00039409
Iteration 52/1000 | Loss: 0.00062707
Iteration 53/1000 | Loss: 0.00052657
Iteration 54/1000 | Loss: 0.00039289
Iteration 55/1000 | Loss: 0.00026884
Iteration 56/1000 | Loss: 0.00049438
Iteration 57/1000 | Loss: 0.00043258
Iteration 58/1000 | Loss: 0.00030153
Iteration 59/1000 | Loss: 0.00027373
Iteration 60/1000 | Loss: 0.00022089
Iteration 61/1000 | Loss: 0.00041457
Iteration 62/1000 | Loss: 0.00064225
Iteration 63/1000 | Loss: 0.00022723
Iteration 64/1000 | Loss: 0.00049762
Iteration 65/1000 | Loss: 0.00019949
Iteration 66/1000 | Loss: 0.00048516
Iteration 67/1000 | Loss: 0.00062712
Iteration 68/1000 | Loss: 0.00018483
Iteration 69/1000 | Loss: 0.00018476
Iteration 70/1000 | Loss: 0.00016557
Iteration 71/1000 | Loss: 0.00079973
Iteration 72/1000 | Loss: 0.00148046
Iteration 73/1000 | Loss: 0.00021495
Iteration 74/1000 | Loss: 0.00017509
Iteration 75/1000 | Loss: 0.00030819
Iteration 76/1000 | Loss: 0.00025955
Iteration 77/1000 | Loss: 0.00050875
Iteration 78/1000 | Loss: 0.00056450
Iteration 79/1000 | Loss: 0.00028453
Iteration 80/1000 | Loss: 0.00030963
Iteration 81/1000 | Loss: 0.00044992
Iteration 82/1000 | Loss: 0.00067201
Iteration 83/1000 | Loss: 0.00013522
Iteration 84/1000 | Loss: 0.00048469
Iteration 85/1000 | Loss: 0.00017880
Iteration 86/1000 | Loss: 0.00030397
Iteration 87/1000 | Loss: 0.00043841
Iteration 88/1000 | Loss: 0.00015416
Iteration 89/1000 | Loss: 0.00013774
Iteration 90/1000 | Loss: 0.00015554
Iteration 91/1000 | Loss: 0.00028190
Iteration 92/1000 | Loss: 0.00062955
Iteration 93/1000 | Loss: 0.00136628
Iteration 94/1000 | Loss: 0.00035207
Iteration 95/1000 | Loss: 0.00043411
Iteration 96/1000 | Loss: 0.00036536
Iteration 97/1000 | Loss: 0.00045697
Iteration 98/1000 | Loss: 0.00046269
Iteration 99/1000 | Loss: 0.00016925
Iteration 100/1000 | Loss: 0.00018965
Iteration 101/1000 | Loss: 0.00023084
Iteration 102/1000 | Loss: 0.00021200
Iteration 103/1000 | Loss: 0.00011999
Iteration 104/1000 | Loss: 0.00029661
Iteration 105/1000 | Loss: 0.00032920
Iteration 106/1000 | Loss: 0.00039501
Iteration 107/1000 | Loss: 0.00014002
Iteration 108/1000 | Loss: 0.00011394
Iteration 109/1000 | Loss: 0.00014338
Iteration 110/1000 | Loss: 0.00010669
Iteration 111/1000 | Loss: 0.00010347
Iteration 112/1000 | Loss: 0.00019852
Iteration 113/1000 | Loss: 0.00014136
Iteration 114/1000 | Loss: 0.00021578
Iteration 115/1000 | Loss: 0.00010046
Iteration 116/1000 | Loss: 0.00011794
Iteration 117/1000 | Loss: 0.00010438
Iteration 118/1000 | Loss: 0.00030312
Iteration 119/1000 | Loss: 0.00104574
Iteration 120/1000 | Loss: 0.00033757
Iteration 121/1000 | Loss: 0.00034340
Iteration 122/1000 | Loss: 0.00029740
Iteration 123/1000 | Loss: 0.00031561
Iteration 124/1000 | Loss: 0.00017982
Iteration 125/1000 | Loss: 0.00009948
Iteration 126/1000 | Loss: 0.00029052
Iteration 127/1000 | Loss: 0.00053979
Iteration 128/1000 | Loss: 0.00013781
Iteration 129/1000 | Loss: 0.00019612
Iteration 130/1000 | Loss: 0.00030085
Iteration 131/1000 | Loss: 0.00028027
Iteration 132/1000 | Loss: 0.00010060
Iteration 133/1000 | Loss: 0.00009408
Iteration 134/1000 | Loss: 0.00009119
Iteration 135/1000 | Loss: 0.00042321
Iteration 136/1000 | Loss: 0.00034297
Iteration 137/1000 | Loss: 0.00027579
Iteration 138/1000 | Loss: 0.00010207
Iteration 139/1000 | Loss: 0.00008599
Iteration 140/1000 | Loss: 0.00014482
Iteration 141/1000 | Loss: 0.00008366
Iteration 142/1000 | Loss: 0.00008766
Iteration 143/1000 | Loss: 0.00009371
Iteration 144/1000 | Loss: 0.00008289
Iteration 145/1000 | Loss: 0.00009060
Iteration 146/1000 | Loss: 0.00008040
Iteration 147/1000 | Loss: 0.00008031
Iteration 148/1000 | Loss: 0.00008355
Iteration 149/1000 | Loss: 0.00009580
Iteration 150/1000 | Loss: 0.00013072
Iteration 151/1000 | Loss: 0.00008165
Iteration 152/1000 | Loss: 0.00008184
Iteration 153/1000 | Loss: 0.00008151
Iteration 154/1000 | Loss: 0.00011532
Iteration 155/1000 | Loss: 0.00009020
Iteration 156/1000 | Loss: 0.00008204
Iteration 157/1000 | Loss: 0.00007954
Iteration 158/1000 | Loss: 0.00007954
Iteration 159/1000 | Loss: 0.00007954
Iteration 160/1000 | Loss: 0.00007946
Iteration 161/1000 | Loss: 0.00007946
Iteration 162/1000 | Loss: 0.00007946
Iteration 163/1000 | Loss: 0.00007946
Iteration 164/1000 | Loss: 0.00007946
Iteration 165/1000 | Loss: 0.00007946
Iteration 166/1000 | Loss: 0.00007946
Iteration 167/1000 | Loss: 0.00007945
Iteration 168/1000 | Loss: 0.00007945
Iteration 169/1000 | Loss: 0.00007945
Iteration 170/1000 | Loss: 0.00007945
Iteration 171/1000 | Loss: 0.00007945
Iteration 172/1000 | Loss: 0.00007945
Iteration 173/1000 | Loss: 0.00007945
Iteration 174/1000 | Loss: 0.00007945
Iteration 175/1000 | Loss: 0.00007945
Iteration 176/1000 | Loss: 0.00007945
Iteration 177/1000 | Loss: 0.00007945
Iteration 178/1000 | Loss: 0.00007945
Iteration 179/1000 | Loss: 0.00007945
Iteration 180/1000 | Loss: 0.00007945
Iteration 181/1000 | Loss: 0.00007945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [7.945374818518758e-05, 7.945374818518758e-05, 7.945374818518758e-05, 7.945374818518758e-05, 7.945374818518758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.945374818518758e-05

Optimization complete. Final v2v error: 4.295979022979736 mm

Highest mean error: 12.604928970336914 mm for frame 179

Lowest mean error: 2.718912363052368 mm for frame 222

Saving results

Total time: 302.17636728286743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_1930/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_1930/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822702
Iteration 2/25 | Loss: 0.00141373
Iteration 3/25 | Loss: 0.00127924
Iteration 4/25 | Loss: 0.00125798
Iteration 5/25 | Loss: 0.00125200
Iteration 6/25 | Loss: 0.00125001
Iteration 7/25 | Loss: 0.00124957
Iteration 8/25 | Loss: 0.00124957
Iteration 9/25 | Loss: 0.00124957
Iteration 10/25 | Loss: 0.00124957
Iteration 11/25 | Loss: 0.00124957
Iteration 12/25 | Loss: 0.00124957
Iteration 13/25 | Loss: 0.00124957
Iteration 14/25 | Loss: 0.00124957
Iteration 15/25 | Loss: 0.00124957
Iteration 16/25 | Loss: 0.00124957
Iteration 17/25 | Loss: 0.00124957
Iteration 18/25 | Loss: 0.00124957
Iteration 19/25 | Loss: 0.00124957
Iteration 20/25 | Loss: 0.00124957
Iteration 21/25 | Loss: 0.00124957
Iteration 22/25 | Loss: 0.00124957
Iteration 23/25 | Loss: 0.00124957
Iteration 24/25 | Loss: 0.00124957
Iteration 25/25 | Loss: 0.00124957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001249574706889689, 0.001249574706889689, 0.001249574706889689, 0.001249574706889689, 0.001249574706889689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001249574706889689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05906546
Iteration 2/25 | Loss: 0.00377643
Iteration 3/25 | Loss: 0.00377643
Iteration 4/25 | Loss: 0.00377643
Iteration 5/25 | Loss: 0.00377643
Iteration 6/25 | Loss: 0.00377643
Iteration 7/25 | Loss: 0.00377643
Iteration 8/25 | Loss: 0.00377643
Iteration 9/25 | Loss: 0.00377643
Iteration 10/25 | Loss: 0.00377643
Iteration 11/25 | Loss: 0.00377643
Iteration 12/25 | Loss: 0.00377643
Iteration 13/25 | Loss: 0.00377643
Iteration 14/25 | Loss: 0.00377643
Iteration 15/25 | Loss: 0.00377643
Iteration 16/25 | Loss: 0.00377643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003776425961405039, 0.003776425961405039, 0.003776425961405039, 0.003776425961405039, 0.003776425961405039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003776425961405039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00377643
Iteration 2/1000 | Loss: 0.00004109
Iteration 3/1000 | Loss: 0.00002817
Iteration 4/1000 | Loss: 0.00002308
Iteration 5/1000 | Loss: 0.00002148
Iteration 6/1000 | Loss: 0.00002036
Iteration 7/1000 | Loss: 0.00001925
Iteration 8/1000 | Loss: 0.00001874
Iteration 9/1000 | Loss: 0.00001834
Iteration 10/1000 | Loss: 0.00001794
Iteration 11/1000 | Loss: 0.00001765
Iteration 12/1000 | Loss: 0.00001753
Iteration 13/1000 | Loss: 0.00001733
Iteration 14/1000 | Loss: 0.00001714
Iteration 15/1000 | Loss: 0.00001714
Iteration 16/1000 | Loss: 0.00001713
Iteration 17/1000 | Loss: 0.00001710
Iteration 18/1000 | Loss: 0.00001709
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001705
Iteration 22/1000 | Loss: 0.00001704
Iteration 23/1000 | Loss: 0.00001704
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001699
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001698
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001697
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001697
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001696
Iteration 39/1000 | Loss: 0.00001696
Iteration 40/1000 | Loss: 0.00001696
Iteration 41/1000 | Loss: 0.00001696
Iteration 42/1000 | Loss: 0.00001696
Iteration 43/1000 | Loss: 0.00001696
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001694
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001694
Iteration 52/1000 | Loss: 0.00001693
Iteration 53/1000 | Loss: 0.00001693
Iteration 54/1000 | Loss: 0.00001693
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001692
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001690
Iteration 59/1000 | Loss: 0.00001690
Iteration 60/1000 | Loss: 0.00001690
Iteration 61/1000 | Loss: 0.00001690
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001689
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001688
Iteration 68/1000 | Loss: 0.00001688
Iteration 69/1000 | Loss: 0.00001687
Iteration 70/1000 | Loss: 0.00001687
Iteration 71/1000 | Loss: 0.00001686
Iteration 72/1000 | Loss: 0.00001686
Iteration 73/1000 | Loss: 0.00001686
Iteration 74/1000 | Loss: 0.00001686
Iteration 75/1000 | Loss: 0.00001686
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001685
Iteration 81/1000 | Loss: 0.00001685
Iteration 82/1000 | Loss: 0.00001685
Iteration 83/1000 | Loss: 0.00001685
Iteration 84/1000 | Loss: 0.00001685
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001684
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001684
Iteration 93/1000 | Loss: 0.00001684
Iteration 94/1000 | Loss: 0.00001683
Iteration 95/1000 | Loss: 0.00001683
Iteration 96/1000 | Loss: 0.00001683
Iteration 97/1000 | Loss: 0.00001683
Iteration 98/1000 | Loss: 0.00001683
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001682
Iteration 102/1000 | Loss: 0.00001682
Iteration 103/1000 | Loss: 0.00001682
Iteration 104/1000 | Loss: 0.00001681
Iteration 105/1000 | Loss: 0.00001681
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001680
Iteration 109/1000 | Loss: 0.00001680
Iteration 110/1000 | Loss: 0.00001680
Iteration 111/1000 | Loss: 0.00001680
Iteration 112/1000 | Loss: 0.00001679
Iteration 113/1000 | Loss: 0.00001679
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001679
Iteration 116/1000 | Loss: 0.00001678
Iteration 117/1000 | Loss: 0.00001678
Iteration 118/1000 | Loss: 0.00001678
Iteration 119/1000 | Loss: 0.00001677
Iteration 120/1000 | Loss: 0.00001677
Iteration 121/1000 | Loss: 0.00001677
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00001677
Iteration 124/1000 | Loss: 0.00001677
Iteration 125/1000 | Loss: 0.00001677
Iteration 126/1000 | Loss: 0.00001677
Iteration 127/1000 | Loss: 0.00001677
Iteration 128/1000 | Loss: 0.00001676
Iteration 129/1000 | Loss: 0.00001676
Iteration 130/1000 | Loss: 0.00001676
Iteration 131/1000 | Loss: 0.00001676
Iteration 132/1000 | Loss: 0.00001676
Iteration 133/1000 | Loss: 0.00001676
Iteration 134/1000 | Loss: 0.00001676
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001675
Iteration 137/1000 | Loss: 0.00001675
Iteration 138/1000 | Loss: 0.00001675
Iteration 139/1000 | Loss: 0.00001675
Iteration 140/1000 | Loss: 0.00001675
Iteration 141/1000 | Loss: 0.00001675
Iteration 142/1000 | Loss: 0.00001675
Iteration 143/1000 | Loss: 0.00001675
Iteration 144/1000 | Loss: 0.00001675
Iteration 145/1000 | Loss: 0.00001675
Iteration 146/1000 | Loss: 0.00001675
Iteration 147/1000 | Loss: 0.00001675
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001674
Iteration 151/1000 | Loss: 0.00001674
Iteration 152/1000 | Loss: 0.00001674
Iteration 153/1000 | Loss: 0.00001674
Iteration 154/1000 | Loss: 0.00001674
Iteration 155/1000 | Loss: 0.00001674
Iteration 156/1000 | Loss: 0.00001674
Iteration 157/1000 | Loss: 0.00001674
Iteration 158/1000 | Loss: 0.00001673
Iteration 159/1000 | Loss: 0.00001673
Iteration 160/1000 | Loss: 0.00001673
Iteration 161/1000 | Loss: 0.00001673
Iteration 162/1000 | Loss: 0.00001673
Iteration 163/1000 | Loss: 0.00001673
Iteration 164/1000 | Loss: 0.00001673
Iteration 165/1000 | Loss: 0.00001673
Iteration 166/1000 | Loss: 0.00001673
Iteration 167/1000 | Loss: 0.00001673
Iteration 168/1000 | Loss: 0.00001673
Iteration 169/1000 | Loss: 0.00001673
Iteration 170/1000 | Loss: 0.00001673
Iteration 171/1000 | Loss: 0.00001673
Iteration 172/1000 | Loss: 0.00001673
Iteration 173/1000 | Loss: 0.00001673
Iteration 174/1000 | Loss: 0.00001673
Iteration 175/1000 | Loss: 0.00001673
Iteration 176/1000 | Loss: 0.00001673
Iteration 177/1000 | Loss: 0.00001673
Iteration 178/1000 | Loss: 0.00001673
Iteration 179/1000 | Loss: 0.00001673
Iteration 180/1000 | Loss: 0.00001673
Iteration 181/1000 | Loss: 0.00001673
Iteration 182/1000 | Loss: 0.00001673
Iteration 183/1000 | Loss: 0.00001673
Iteration 184/1000 | Loss: 0.00001673
Iteration 185/1000 | Loss: 0.00001673
Iteration 186/1000 | Loss: 0.00001673
Iteration 187/1000 | Loss: 0.00001673
Iteration 188/1000 | Loss: 0.00001673
Iteration 189/1000 | Loss: 0.00001673
Iteration 190/1000 | Loss: 0.00001673
Iteration 191/1000 | Loss: 0.00001673
Iteration 192/1000 | Loss: 0.00001673
Iteration 193/1000 | Loss: 0.00001673
Iteration 194/1000 | Loss: 0.00001673
Iteration 195/1000 | Loss: 0.00001673
Iteration 196/1000 | Loss: 0.00001673
Iteration 197/1000 | Loss: 0.00001673
Iteration 198/1000 | Loss: 0.00001673
Iteration 199/1000 | Loss: 0.00001673
Iteration 200/1000 | Loss: 0.00001673
Iteration 201/1000 | Loss: 0.00001673
Iteration 202/1000 | Loss: 0.00001673
Iteration 203/1000 | Loss: 0.00001673
Iteration 204/1000 | Loss: 0.00001673
Iteration 205/1000 | Loss: 0.00001673
Iteration 206/1000 | Loss: 0.00001673
Iteration 207/1000 | Loss: 0.00001673
Iteration 208/1000 | Loss: 0.00001673
Iteration 209/1000 | Loss: 0.00001673
Iteration 210/1000 | Loss: 0.00001673
Iteration 211/1000 | Loss: 0.00001673
Iteration 212/1000 | Loss: 0.00001673
Iteration 213/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.672613507253118e-05, 1.672613507253118e-05, 1.672613507253118e-05, 1.672613507253118e-05, 1.672613507253118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.672613507253118e-05

Optimization complete. Final v2v error: 3.535280227661133 mm

Highest mean error: 3.778013229370117 mm for frame 46

Lowest mean error: 3.0558853149414062 mm for frame 109

Saving results

Total time: 40.9129683971405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958469
Iteration 2/25 | Loss: 0.00385610
Iteration 3/25 | Loss: 0.00290727
Iteration 4/25 | Loss: 0.00248170
Iteration 5/25 | Loss: 0.00244916
Iteration 6/25 | Loss: 0.00233158
Iteration 7/25 | Loss: 0.00218100
Iteration 8/25 | Loss: 0.00217818
Iteration 9/25 | Loss: 0.00202448
Iteration 10/25 | Loss: 0.00200241
Iteration 11/25 | Loss: 0.00191804
Iteration 12/25 | Loss: 0.00186009
Iteration 13/25 | Loss: 0.00183866
Iteration 14/25 | Loss: 0.00186669
Iteration 15/25 | Loss: 0.00182337
Iteration 16/25 | Loss: 0.00181710
Iteration 17/25 | Loss: 0.00180168
Iteration 18/25 | Loss: 0.00181646
Iteration 19/25 | Loss: 0.00183163
Iteration 20/25 | Loss: 0.00180817
Iteration 21/25 | Loss: 0.00178099
Iteration 22/25 | Loss: 0.00182616
Iteration 23/25 | Loss: 0.00178584
Iteration 24/25 | Loss: 0.00178639
Iteration 25/25 | Loss: 0.00175936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87909019
Iteration 2/25 | Loss: 0.00579346
Iteration 3/25 | Loss: 0.00579346
Iteration 4/25 | Loss: 0.00579346
Iteration 5/25 | Loss: 0.00579346
Iteration 6/25 | Loss: 0.00579346
Iteration 7/25 | Loss: 0.00579346
Iteration 8/25 | Loss: 0.00579346
Iteration 9/25 | Loss: 0.00579346
Iteration 10/25 | Loss: 0.00579346
Iteration 11/25 | Loss: 0.00579346
Iteration 12/25 | Loss: 0.00579346
Iteration 13/25 | Loss: 0.00579346
Iteration 14/25 | Loss: 0.00579346
Iteration 15/25 | Loss: 0.00579346
Iteration 16/25 | Loss: 0.00579346
Iteration 17/25 | Loss: 0.00579346
Iteration 18/25 | Loss: 0.00579346
Iteration 19/25 | Loss: 0.00579346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.005793455988168716, 0.005793455988168716, 0.005793455988168716, 0.005793455988168716, 0.005793455988168716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005793455988168716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00579346
Iteration 2/1000 | Loss: 0.00064607
Iteration 3/1000 | Loss: 0.00048871
Iteration 4/1000 | Loss: 0.00151702
Iteration 5/1000 | Loss: 0.00095447
Iteration 6/1000 | Loss: 0.00031976
Iteration 7/1000 | Loss: 0.00251706
Iteration 8/1000 | Loss: 0.00039957
Iteration 9/1000 | Loss: 0.00194580
Iteration 10/1000 | Loss: 0.00028695
Iteration 11/1000 | Loss: 0.00334788
Iteration 12/1000 | Loss: 0.00044229
Iteration 13/1000 | Loss: 0.00121249
Iteration 14/1000 | Loss: 0.00034128
Iteration 15/1000 | Loss: 0.00198859
Iteration 16/1000 | Loss: 0.00076631
Iteration 17/1000 | Loss: 0.00167564
Iteration 18/1000 | Loss: 0.00032993
Iteration 19/1000 | Loss: 0.00026768
Iteration 20/1000 | Loss: 0.00318705
Iteration 21/1000 | Loss: 0.01021145
Iteration 22/1000 | Loss: 0.01365448
Iteration 23/1000 | Loss: 0.00155485
Iteration 24/1000 | Loss: 0.00114336
Iteration 25/1000 | Loss: 0.00038988
Iteration 26/1000 | Loss: 0.00142711
Iteration 27/1000 | Loss: 0.00130940
Iteration 28/1000 | Loss: 0.00037795
Iteration 29/1000 | Loss: 0.00142134
Iteration 30/1000 | Loss: 0.00119906
Iteration 31/1000 | Loss: 0.00054960
Iteration 32/1000 | Loss: 0.00039777
Iteration 33/1000 | Loss: 0.00033153
Iteration 34/1000 | Loss: 0.00108694
Iteration 35/1000 | Loss: 0.00018980
Iteration 36/1000 | Loss: 0.00192278
Iteration 37/1000 | Loss: 0.00105774
Iteration 38/1000 | Loss: 0.00018857
Iteration 39/1000 | Loss: 0.00015392
Iteration 40/1000 | Loss: 0.00098692
Iteration 41/1000 | Loss: 0.00014736
Iteration 42/1000 | Loss: 0.00097440
Iteration 43/1000 | Loss: 0.00017934
Iteration 44/1000 | Loss: 0.00013463
Iteration 45/1000 | Loss: 0.00012793
Iteration 46/1000 | Loss: 0.00241866
Iteration 47/1000 | Loss: 0.00962291
Iteration 48/1000 | Loss: 0.00136636
Iteration 49/1000 | Loss: 0.00107012
Iteration 50/1000 | Loss: 0.00015150
Iteration 51/1000 | Loss: 0.00097482
Iteration 52/1000 | Loss: 0.00132981
Iteration 53/1000 | Loss: 0.00125916
Iteration 54/1000 | Loss: 0.00056201
Iteration 55/1000 | Loss: 0.00107939
Iteration 56/1000 | Loss: 0.00139498
Iteration 57/1000 | Loss: 0.00116864
Iteration 58/1000 | Loss: 0.00159425
Iteration 59/1000 | Loss: 0.00302779
Iteration 60/1000 | Loss: 0.00011712
Iteration 61/1000 | Loss: 0.00010222
Iteration 62/1000 | Loss: 0.00009636
Iteration 63/1000 | Loss: 0.00009296
Iteration 64/1000 | Loss: 0.00073784
Iteration 65/1000 | Loss: 0.00096219
Iteration 66/1000 | Loss: 0.00090743
Iteration 67/1000 | Loss: 0.00093614
Iteration 68/1000 | Loss: 0.00011410
Iteration 69/1000 | Loss: 0.00229722
Iteration 70/1000 | Loss: 0.00251580
Iteration 71/1000 | Loss: 0.00021481
Iteration 72/1000 | Loss: 0.00012862
Iteration 73/1000 | Loss: 0.00010897
Iteration 74/1000 | Loss: 0.00009740
Iteration 75/1000 | Loss: 0.00009045
Iteration 76/1000 | Loss: 0.00008445
Iteration 77/1000 | Loss: 0.00008373
Iteration 78/1000 | Loss: 0.00008127
Iteration 79/1000 | Loss: 0.00008034
Iteration 80/1000 | Loss: 0.00007948
Iteration 81/1000 | Loss: 0.00121958
Iteration 82/1000 | Loss: 0.00244032
Iteration 83/1000 | Loss: 0.00147814
Iteration 84/1000 | Loss: 0.00151688
Iteration 85/1000 | Loss: 0.00098795
Iteration 86/1000 | Loss: 0.00058364
Iteration 87/1000 | Loss: 0.00038420
Iteration 88/1000 | Loss: 0.00007401
Iteration 89/1000 | Loss: 0.00006924
Iteration 90/1000 | Loss: 0.00006362
Iteration 91/1000 | Loss: 0.00005940
Iteration 92/1000 | Loss: 0.00089799
Iteration 93/1000 | Loss: 0.00007339
Iteration 94/1000 | Loss: 0.00006110
Iteration 95/1000 | Loss: 0.00005385
Iteration 96/1000 | Loss: 0.00005107
Iteration 97/1000 | Loss: 0.00033693
Iteration 98/1000 | Loss: 0.00007026
Iteration 99/1000 | Loss: 0.00005683
Iteration 100/1000 | Loss: 0.00005164
Iteration 101/1000 | Loss: 0.00004802
Iteration 102/1000 | Loss: 0.00004582
Iteration 103/1000 | Loss: 0.00004509
Iteration 104/1000 | Loss: 0.00004417
Iteration 105/1000 | Loss: 0.00068021
Iteration 106/1000 | Loss: 0.00004814
Iteration 107/1000 | Loss: 0.00004436
Iteration 108/1000 | Loss: 0.00004274
Iteration 109/1000 | Loss: 0.00004111
Iteration 110/1000 | Loss: 0.00003976
Iteration 111/1000 | Loss: 0.00071533
Iteration 112/1000 | Loss: 0.00004449
Iteration 113/1000 | Loss: 0.00003990
Iteration 114/1000 | Loss: 0.00003835
Iteration 115/1000 | Loss: 0.00003703
Iteration 116/1000 | Loss: 0.00003601
Iteration 117/1000 | Loss: 0.00003548
Iteration 118/1000 | Loss: 0.00003498
Iteration 119/1000 | Loss: 0.00071481
Iteration 120/1000 | Loss: 0.00065678
Iteration 121/1000 | Loss: 0.00005277
Iteration 122/1000 | Loss: 0.00003737
Iteration 123/1000 | Loss: 0.00003463
Iteration 124/1000 | Loss: 0.00003265
Iteration 125/1000 | Loss: 0.00003084
Iteration 126/1000 | Loss: 0.00002994
Iteration 127/1000 | Loss: 0.00002919
Iteration 128/1000 | Loss: 0.00002878
Iteration 129/1000 | Loss: 0.00002845
Iteration 130/1000 | Loss: 0.00002819
Iteration 131/1000 | Loss: 0.00002810
Iteration 132/1000 | Loss: 0.00002791
Iteration 133/1000 | Loss: 0.00002776
Iteration 134/1000 | Loss: 0.00002765
Iteration 135/1000 | Loss: 0.00002761
Iteration 136/1000 | Loss: 0.00002761
Iteration 137/1000 | Loss: 0.00002758
Iteration 138/1000 | Loss: 0.00002758
Iteration 139/1000 | Loss: 0.00002758
Iteration 140/1000 | Loss: 0.00002757
Iteration 141/1000 | Loss: 0.00002756
Iteration 142/1000 | Loss: 0.00002755
Iteration 143/1000 | Loss: 0.00002754
Iteration 144/1000 | Loss: 0.00002753
Iteration 145/1000 | Loss: 0.00002753
Iteration 146/1000 | Loss: 0.00002753
Iteration 147/1000 | Loss: 0.00002752
Iteration 148/1000 | Loss: 0.00002752
Iteration 149/1000 | Loss: 0.00002751
Iteration 150/1000 | Loss: 0.00002750
Iteration 151/1000 | Loss: 0.00002750
Iteration 152/1000 | Loss: 0.00002749
Iteration 153/1000 | Loss: 0.00002748
Iteration 154/1000 | Loss: 0.00002748
Iteration 155/1000 | Loss: 0.00002748
Iteration 156/1000 | Loss: 0.00002747
Iteration 157/1000 | Loss: 0.00002747
Iteration 158/1000 | Loss: 0.00002746
Iteration 159/1000 | Loss: 0.00002746
Iteration 160/1000 | Loss: 0.00002745
Iteration 161/1000 | Loss: 0.00002745
Iteration 162/1000 | Loss: 0.00002745
Iteration 163/1000 | Loss: 0.00002745
Iteration 164/1000 | Loss: 0.00002744
Iteration 165/1000 | Loss: 0.00002744
Iteration 166/1000 | Loss: 0.00002744
Iteration 167/1000 | Loss: 0.00002744
Iteration 168/1000 | Loss: 0.00002744
Iteration 169/1000 | Loss: 0.00002744
Iteration 170/1000 | Loss: 0.00002744
Iteration 171/1000 | Loss: 0.00002744
Iteration 172/1000 | Loss: 0.00002744
Iteration 173/1000 | Loss: 0.00002744
Iteration 174/1000 | Loss: 0.00002744
Iteration 175/1000 | Loss: 0.00002744
Iteration 176/1000 | Loss: 0.00002744
Iteration 177/1000 | Loss: 0.00002744
Iteration 178/1000 | Loss: 0.00002744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.744285848166328e-05, 2.744285848166328e-05, 2.744285848166328e-05, 2.744285848166328e-05, 2.744285848166328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.744285848166328e-05

Optimization complete. Final v2v error: 3.852084159851074 mm

Highest mean error: 10.269059181213379 mm for frame 30

Lowest mean error: 2.7640957832336426 mm for frame 2

Saving results

Total time: 233.7611300945282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855644
Iteration 2/25 | Loss: 0.00139974
Iteration 3/25 | Loss: 0.00132584
Iteration 4/25 | Loss: 0.00131679
Iteration 5/25 | Loss: 0.00131434
Iteration 6/25 | Loss: 0.00131434
Iteration 7/25 | Loss: 0.00131434
Iteration 8/25 | Loss: 0.00131434
Iteration 9/25 | Loss: 0.00131434
Iteration 10/25 | Loss: 0.00131434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001314339810051024, 0.001314339810051024, 0.001314339810051024, 0.001314339810051024, 0.001314339810051024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001314339810051024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62163627
Iteration 2/25 | Loss: 0.00212526
Iteration 3/25 | Loss: 0.00212526
Iteration 4/25 | Loss: 0.00212526
Iteration 5/25 | Loss: 0.00212526
Iteration 6/25 | Loss: 0.00212526
Iteration 7/25 | Loss: 0.00212526
Iteration 8/25 | Loss: 0.00212526
Iteration 9/25 | Loss: 0.00212526
Iteration 10/25 | Loss: 0.00212526
Iteration 11/25 | Loss: 0.00212526
Iteration 12/25 | Loss: 0.00212526
Iteration 13/25 | Loss: 0.00212526
Iteration 14/25 | Loss: 0.00212526
Iteration 15/25 | Loss: 0.00212526
Iteration 16/25 | Loss: 0.00212526
Iteration 17/25 | Loss: 0.00212526
Iteration 18/25 | Loss: 0.00212526
Iteration 19/25 | Loss: 0.00212526
Iteration 20/25 | Loss: 0.00212526
Iteration 21/25 | Loss: 0.00212526
Iteration 22/25 | Loss: 0.00212526
Iteration 23/25 | Loss: 0.00212526
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002125257160514593, 0.002125257160514593, 0.002125257160514593, 0.002125257160514593, 0.002125257160514593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002125257160514593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212526
Iteration 2/1000 | Loss: 0.00002626
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001338
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001251
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001189
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001157
Iteration 14/1000 | Loss: 0.00001144
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001138
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001133
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001119
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001107
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001101
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001094
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001090
Iteration 35/1000 | Loss: 0.00001090
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001089
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001089
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001089
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001086
Iteration 48/1000 | Loss: 0.00001086
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001085
Iteration 51/1000 | Loss: 0.00001085
Iteration 52/1000 | Loss: 0.00001085
Iteration 53/1000 | Loss: 0.00001085
Iteration 54/1000 | Loss: 0.00001085
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001085
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001082
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001081
Iteration 73/1000 | Loss: 0.00001081
Iteration 74/1000 | Loss: 0.00001081
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001080
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001080
Iteration 79/1000 | Loss: 0.00001080
Iteration 80/1000 | Loss: 0.00001080
Iteration 81/1000 | Loss: 0.00001079
Iteration 82/1000 | Loss: 0.00001079
Iteration 83/1000 | Loss: 0.00001079
Iteration 84/1000 | Loss: 0.00001079
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001078
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001077
Iteration 92/1000 | Loss: 0.00001077
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001076
Iteration 104/1000 | Loss: 0.00001076
Iteration 105/1000 | Loss: 0.00001076
Iteration 106/1000 | Loss: 0.00001075
Iteration 107/1000 | Loss: 0.00001075
Iteration 108/1000 | Loss: 0.00001075
Iteration 109/1000 | Loss: 0.00001075
Iteration 110/1000 | Loss: 0.00001075
Iteration 111/1000 | Loss: 0.00001074
Iteration 112/1000 | Loss: 0.00001074
Iteration 113/1000 | Loss: 0.00001074
Iteration 114/1000 | Loss: 0.00001074
Iteration 115/1000 | Loss: 0.00001074
Iteration 116/1000 | Loss: 0.00001074
Iteration 117/1000 | Loss: 0.00001074
Iteration 118/1000 | Loss: 0.00001074
Iteration 119/1000 | Loss: 0.00001074
Iteration 120/1000 | Loss: 0.00001074
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001073
Iteration 123/1000 | Loss: 0.00001073
Iteration 124/1000 | Loss: 0.00001073
Iteration 125/1000 | Loss: 0.00001073
Iteration 126/1000 | Loss: 0.00001073
Iteration 127/1000 | Loss: 0.00001073
Iteration 128/1000 | Loss: 0.00001073
Iteration 129/1000 | Loss: 0.00001073
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001072
Iteration 134/1000 | Loss: 0.00001072
Iteration 135/1000 | Loss: 0.00001072
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001071
Iteration 138/1000 | Loss: 0.00001071
Iteration 139/1000 | Loss: 0.00001070
Iteration 140/1000 | Loss: 0.00001070
Iteration 141/1000 | Loss: 0.00001070
Iteration 142/1000 | Loss: 0.00001070
Iteration 143/1000 | Loss: 0.00001069
Iteration 144/1000 | Loss: 0.00001069
Iteration 145/1000 | Loss: 0.00001069
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001069
Iteration 148/1000 | Loss: 0.00001068
Iteration 149/1000 | Loss: 0.00001068
Iteration 150/1000 | Loss: 0.00001068
Iteration 151/1000 | Loss: 0.00001067
Iteration 152/1000 | Loss: 0.00001067
Iteration 153/1000 | Loss: 0.00001067
Iteration 154/1000 | Loss: 0.00001067
Iteration 155/1000 | Loss: 0.00001067
Iteration 156/1000 | Loss: 0.00001067
Iteration 157/1000 | Loss: 0.00001066
Iteration 158/1000 | Loss: 0.00001066
Iteration 159/1000 | Loss: 0.00001066
Iteration 160/1000 | Loss: 0.00001066
Iteration 161/1000 | Loss: 0.00001066
Iteration 162/1000 | Loss: 0.00001065
Iteration 163/1000 | Loss: 0.00001065
Iteration 164/1000 | Loss: 0.00001065
Iteration 165/1000 | Loss: 0.00001064
Iteration 166/1000 | Loss: 0.00001064
Iteration 167/1000 | Loss: 0.00001064
Iteration 168/1000 | Loss: 0.00001064
Iteration 169/1000 | Loss: 0.00001064
Iteration 170/1000 | Loss: 0.00001064
Iteration 171/1000 | Loss: 0.00001064
Iteration 172/1000 | Loss: 0.00001064
Iteration 173/1000 | Loss: 0.00001064
Iteration 174/1000 | Loss: 0.00001063
Iteration 175/1000 | Loss: 0.00001063
Iteration 176/1000 | Loss: 0.00001063
Iteration 177/1000 | Loss: 0.00001063
Iteration 178/1000 | Loss: 0.00001063
Iteration 179/1000 | Loss: 0.00001063
Iteration 180/1000 | Loss: 0.00001063
Iteration 181/1000 | Loss: 0.00001063
Iteration 182/1000 | Loss: 0.00001063
Iteration 183/1000 | Loss: 0.00001063
Iteration 184/1000 | Loss: 0.00001063
Iteration 185/1000 | Loss: 0.00001063
Iteration 186/1000 | Loss: 0.00001063
Iteration 187/1000 | Loss: 0.00001063
Iteration 188/1000 | Loss: 0.00001062
Iteration 189/1000 | Loss: 0.00001062
Iteration 190/1000 | Loss: 0.00001062
Iteration 191/1000 | Loss: 0.00001062
Iteration 192/1000 | Loss: 0.00001062
Iteration 193/1000 | Loss: 0.00001062
Iteration 194/1000 | Loss: 0.00001062
Iteration 195/1000 | Loss: 0.00001062
Iteration 196/1000 | Loss: 0.00001061
Iteration 197/1000 | Loss: 0.00001061
Iteration 198/1000 | Loss: 0.00001061
Iteration 199/1000 | Loss: 0.00001061
Iteration 200/1000 | Loss: 0.00001061
Iteration 201/1000 | Loss: 0.00001061
Iteration 202/1000 | Loss: 0.00001061
Iteration 203/1000 | Loss: 0.00001061
Iteration 204/1000 | Loss: 0.00001061
Iteration 205/1000 | Loss: 0.00001061
Iteration 206/1000 | Loss: 0.00001060
Iteration 207/1000 | Loss: 0.00001060
Iteration 208/1000 | Loss: 0.00001060
Iteration 209/1000 | Loss: 0.00001060
Iteration 210/1000 | Loss: 0.00001060
Iteration 211/1000 | Loss: 0.00001060
Iteration 212/1000 | Loss: 0.00001060
Iteration 213/1000 | Loss: 0.00001060
Iteration 214/1000 | Loss: 0.00001060
Iteration 215/1000 | Loss: 0.00001060
Iteration 216/1000 | Loss: 0.00001060
Iteration 217/1000 | Loss: 0.00001060
Iteration 218/1000 | Loss: 0.00001060
Iteration 219/1000 | Loss: 0.00001060
Iteration 220/1000 | Loss: 0.00001060
Iteration 221/1000 | Loss: 0.00001060
Iteration 222/1000 | Loss: 0.00001060
Iteration 223/1000 | Loss: 0.00001060
Iteration 224/1000 | Loss: 0.00001060
Iteration 225/1000 | Loss: 0.00001060
Iteration 226/1000 | Loss: 0.00001060
Iteration 227/1000 | Loss: 0.00001060
Iteration 228/1000 | Loss: 0.00001060
Iteration 229/1000 | Loss: 0.00001060
Iteration 230/1000 | Loss: 0.00001060
Iteration 231/1000 | Loss: 0.00001060
Iteration 232/1000 | Loss: 0.00001060
Iteration 233/1000 | Loss: 0.00001060
Iteration 234/1000 | Loss: 0.00001060
Iteration 235/1000 | Loss: 0.00001060
Iteration 236/1000 | Loss: 0.00001060
Iteration 237/1000 | Loss: 0.00001060
Iteration 238/1000 | Loss: 0.00001060
Iteration 239/1000 | Loss: 0.00001060
Iteration 240/1000 | Loss: 0.00001060
Iteration 241/1000 | Loss: 0.00001060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.059530313796131e-05, 1.059530313796131e-05, 1.059530313796131e-05, 1.059530313796131e-05, 1.059530313796131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.059530313796131e-05

Optimization complete. Final v2v error: 2.7887275218963623 mm

Highest mean error: 3.620664119720459 mm for frame 94

Lowest mean error: 2.581688642501831 mm for frame 153

Saving results

Total time: 45.209221601486206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437005
Iteration 2/25 | Loss: 0.00141056
Iteration 3/25 | Loss: 0.00133355
Iteration 4/25 | Loss: 0.00132136
Iteration 5/25 | Loss: 0.00131753
Iteration 6/25 | Loss: 0.00131675
Iteration 7/25 | Loss: 0.00131675
Iteration 8/25 | Loss: 0.00131675
Iteration 9/25 | Loss: 0.00131675
Iteration 10/25 | Loss: 0.00131675
Iteration 11/25 | Loss: 0.00131675
Iteration 12/25 | Loss: 0.00131675
Iteration 13/25 | Loss: 0.00131675
Iteration 14/25 | Loss: 0.00131675
Iteration 15/25 | Loss: 0.00131675
Iteration 16/25 | Loss: 0.00131675
Iteration 17/25 | Loss: 0.00131675
Iteration 18/25 | Loss: 0.00131675
Iteration 19/25 | Loss: 0.00131675
Iteration 20/25 | Loss: 0.00131675
Iteration 21/25 | Loss: 0.00131675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013167469296604395, 0.0013167469296604395, 0.0013167469296604395, 0.0013167469296604395, 0.0013167469296604395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013167469296604395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.47281742
Iteration 2/25 | Loss: 0.00215899
Iteration 3/25 | Loss: 0.00215897
Iteration 4/25 | Loss: 0.00215897
Iteration 5/25 | Loss: 0.00215897
Iteration 6/25 | Loss: 0.00215897
Iteration 7/25 | Loss: 0.00215897
Iteration 8/25 | Loss: 0.00215897
Iteration 9/25 | Loss: 0.00215897
Iteration 10/25 | Loss: 0.00215897
Iteration 11/25 | Loss: 0.00215897
Iteration 12/25 | Loss: 0.00215897
Iteration 13/25 | Loss: 0.00215897
Iteration 14/25 | Loss: 0.00215897
Iteration 15/25 | Loss: 0.00215897
Iteration 16/25 | Loss: 0.00215897
Iteration 17/25 | Loss: 0.00215897
Iteration 18/25 | Loss: 0.00215897
Iteration 19/25 | Loss: 0.00215897
Iteration 20/25 | Loss: 0.00215897
Iteration 21/25 | Loss: 0.00215897
Iteration 22/25 | Loss: 0.00215897
Iteration 23/25 | Loss: 0.00215897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002158968010917306, 0.002158968010917306, 0.002158968010917306, 0.002158968010917306, 0.002158968010917306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002158968010917306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215897
Iteration 2/1000 | Loss: 0.00002567
Iteration 3/1000 | Loss: 0.00001973
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00001612
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001225
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001213
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001206
Iteration 22/1000 | Loss: 0.00001206
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001185
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001180
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001176
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001175
Iteration 63/1000 | Loss: 0.00001174
Iteration 64/1000 | Loss: 0.00001174
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001171
Iteration 74/1000 | Loss: 0.00001171
Iteration 75/1000 | Loss: 0.00001170
Iteration 76/1000 | Loss: 0.00001170
Iteration 77/1000 | Loss: 0.00001170
Iteration 78/1000 | Loss: 0.00001170
Iteration 79/1000 | Loss: 0.00001170
Iteration 80/1000 | Loss: 0.00001169
Iteration 81/1000 | Loss: 0.00001169
Iteration 82/1000 | Loss: 0.00001169
Iteration 83/1000 | Loss: 0.00001169
Iteration 84/1000 | Loss: 0.00001169
Iteration 85/1000 | Loss: 0.00001169
Iteration 86/1000 | Loss: 0.00001168
Iteration 87/1000 | Loss: 0.00001168
Iteration 88/1000 | Loss: 0.00001168
Iteration 89/1000 | Loss: 0.00001168
Iteration 90/1000 | Loss: 0.00001168
Iteration 91/1000 | Loss: 0.00001167
Iteration 92/1000 | Loss: 0.00001167
Iteration 93/1000 | Loss: 0.00001167
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001166
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001166
Iteration 99/1000 | Loss: 0.00001166
Iteration 100/1000 | Loss: 0.00001166
Iteration 101/1000 | Loss: 0.00001166
Iteration 102/1000 | Loss: 0.00001166
Iteration 103/1000 | Loss: 0.00001166
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001165
Iteration 106/1000 | Loss: 0.00001165
Iteration 107/1000 | Loss: 0.00001165
Iteration 108/1000 | Loss: 0.00001165
Iteration 109/1000 | Loss: 0.00001164
Iteration 110/1000 | Loss: 0.00001164
Iteration 111/1000 | Loss: 0.00001164
Iteration 112/1000 | Loss: 0.00001164
Iteration 113/1000 | Loss: 0.00001164
Iteration 114/1000 | Loss: 0.00001164
Iteration 115/1000 | Loss: 0.00001164
Iteration 116/1000 | Loss: 0.00001164
Iteration 117/1000 | Loss: 0.00001164
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001164
Iteration 120/1000 | Loss: 0.00001164
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001163
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001162
Iteration 127/1000 | Loss: 0.00001162
Iteration 128/1000 | Loss: 0.00001162
Iteration 129/1000 | Loss: 0.00001162
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001162
Iteration 134/1000 | Loss: 0.00001162
Iteration 135/1000 | Loss: 0.00001162
Iteration 136/1000 | Loss: 0.00001162
Iteration 137/1000 | Loss: 0.00001162
Iteration 138/1000 | Loss: 0.00001161
Iteration 139/1000 | Loss: 0.00001161
Iteration 140/1000 | Loss: 0.00001161
Iteration 141/1000 | Loss: 0.00001161
Iteration 142/1000 | Loss: 0.00001161
Iteration 143/1000 | Loss: 0.00001161
Iteration 144/1000 | Loss: 0.00001161
Iteration 145/1000 | Loss: 0.00001160
Iteration 146/1000 | Loss: 0.00001160
Iteration 147/1000 | Loss: 0.00001160
Iteration 148/1000 | Loss: 0.00001160
Iteration 149/1000 | Loss: 0.00001160
Iteration 150/1000 | Loss: 0.00001160
Iteration 151/1000 | Loss: 0.00001160
Iteration 152/1000 | Loss: 0.00001160
Iteration 153/1000 | Loss: 0.00001160
Iteration 154/1000 | Loss: 0.00001160
Iteration 155/1000 | Loss: 0.00001160
Iteration 156/1000 | Loss: 0.00001160
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001159
Iteration 159/1000 | Loss: 0.00001159
Iteration 160/1000 | Loss: 0.00001159
Iteration 161/1000 | Loss: 0.00001159
Iteration 162/1000 | Loss: 0.00001159
Iteration 163/1000 | Loss: 0.00001159
Iteration 164/1000 | Loss: 0.00001159
Iteration 165/1000 | Loss: 0.00001159
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001158
Iteration 169/1000 | Loss: 0.00001158
Iteration 170/1000 | Loss: 0.00001158
Iteration 171/1000 | Loss: 0.00001158
Iteration 172/1000 | Loss: 0.00001158
Iteration 173/1000 | Loss: 0.00001158
Iteration 174/1000 | Loss: 0.00001158
Iteration 175/1000 | Loss: 0.00001157
Iteration 176/1000 | Loss: 0.00001157
Iteration 177/1000 | Loss: 0.00001157
Iteration 178/1000 | Loss: 0.00001157
Iteration 179/1000 | Loss: 0.00001157
Iteration 180/1000 | Loss: 0.00001157
Iteration 181/1000 | Loss: 0.00001157
Iteration 182/1000 | Loss: 0.00001157
Iteration 183/1000 | Loss: 0.00001157
Iteration 184/1000 | Loss: 0.00001157
Iteration 185/1000 | Loss: 0.00001157
Iteration 186/1000 | Loss: 0.00001157
Iteration 187/1000 | Loss: 0.00001157
Iteration 188/1000 | Loss: 0.00001157
Iteration 189/1000 | Loss: 0.00001157
Iteration 190/1000 | Loss: 0.00001157
Iteration 191/1000 | Loss: 0.00001157
Iteration 192/1000 | Loss: 0.00001156
Iteration 193/1000 | Loss: 0.00001156
Iteration 194/1000 | Loss: 0.00001156
Iteration 195/1000 | Loss: 0.00001156
Iteration 196/1000 | Loss: 0.00001156
Iteration 197/1000 | Loss: 0.00001156
Iteration 198/1000 | Loss: 0.00001156
Iteration 199/1000 | Loss: 0.00001156
Iteration 200/1000 | Loss: 0.00001156
Iteration 201/1000 | Loss: 0.00001156
Iteration 202/1000 | Loss: 0.00001156
Iteration 203/1000 | Loss: 0.00001156
Iteration 204/1000 | Loss: 0.00001156
Iteration 205/1000 | Loss: 0.00001156
Iteration 206/1000 | Loss: 0.00001156
Iteration 207/1000 | Loss: 0.00001156
Iteration 208/1000 | Loss: 0.00001156
Iteration 209/1000 | Loss: 0.00001156
Iteration 210/1000 | Loss: 0.00001156
Iteration 211/1000 | Loss: 0.00001156
Iteration 212/1000 | Loss: 0.00001156
Iteration 213/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.1556659956113435e-05, 1.1556659956113435e-05, 1.1556659956113435e-05, 1.1556659956113435e-05, 1.1556659956113435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1556659956113435e-05

Optimization complete. Final v2v error: 2.9585165977478027 mm

Highest mean error: 3.5699920654296875 mm for frame 92

Lowest mean error: 2.6901674270629883 mm for frame 206

Saving results

Total time: 53.66174530982971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722139
Iteration 2/25 | Loss: 0.00147475
Iteration 3/25 | Loss: 0.00142147
Iteration 4/25 | Loss: 0.00141534
Iteration 5/25 | Loss: 0.00141351
Iteration 6/25 | Loss: 0.00141351
Iteration 7/25 | Loss: 0.00141351
Iteration 8/25 | Loss: 0.00141351
Iteration 9/25 | Loss: 0.00141351
Iteration 10/25 | Loss: 0.00141351
Iteration 11/25 | Loss: 0.00141351
Iteration 12/25 | Loss: 0.00141351
Iteration 13/25 | Loss: 0.00141351
Iteration 14/25 | Loss: 0.00141351
Iteration 15/25 | Loss: 0.00141351
Iteration 16/25 | Loss: 0.00141351
Iteration 17/25 | Loss: 0.00141351
Iteration 18/25 | Loss: 0.00141351
Iteration 19/25 | Loss: 0.00141351
Iteration 20/25 | Loss: 0.00141351
Iteration 21/25 | Loss: 0.00141351
Iteration 22/25 | Loss: 0.00141351
Iteration 23/25 | Loss: 0.00141351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001413511112332344, 0.001413511112332344, 0.001413511112332344, 0.001413511112332344, 0.001413511112332344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001413511112332344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07529998
Iteration 2/25 | Loss: 0.00177628
Iteration 3/25 | Loss: 0.00177627
Iteration 4/25 | Loss: 0.00177627
Iteration 5/25 | Loss: 0.00177627
Iteration 6/25 | Loss: 0.00177627
Iteration 7/25 | Loss: 0.00177627
Iteration 8/25 | Loss: 0.00177627
Iteration 9/25 | Loss: 0.00177627
Iteration 10/25 | Loss: 0.00177627
Iteration 11/25 | Loss: 0.00177627
Iteration 12/25 | Loss: 0.00177627
Iteration 13/25 | Loss: 0.00177627
Iteration 14/25 | Loss: 0.00177627
Iteration 15/25 | Loss: 0.00177627
Iteration 16/25 | Loss: 0.00177627
Iteration 17/25 | Loss: 0.00177627
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017762694042176008, 0.0017762694042176008, 0.0017762694042176008, 0.0017762694042176008, 0.0017762694042176008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017762694042176008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177627
Iteration 2/1000 | Loss: 0.00003485
Iteration 3/1000 | Loss: 0.00002450
Iteration 4/1000 | Loss: 0.00002222
Iteration 5/1000 | Loss: 0.00002123
Iteration 6/1000 | Loss: 0.00002068
Iteration 7/1000 | Loss: 0.00002024
Iteration 8/1000 | Loss: 0.00001977
Iteration 9/1000 | Loss: 0.00001940
Iteration 10/1000 | Loss: 0.00001897
Iteration 11/1000 | Loss: 0.00001893
Iteration 12/1000 | Loss: 0.00001864
Iteration 13/1000 | Loss: 0.00001850
Iteration 14/1000 | Loss: 0.00001832
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001795
Iteration 19/1000 | Loss: 0.00001794
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001780
Iteration 30/1000 | Loss: 0.00001774
Iteration 31/1000 | Loss: 0.00001774
Iteration 32/1000 | Loss: 0.00001769
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001767
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001764
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001760
Iteration 45/1000 | Loss: 0.00001760
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001757
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001757
Iteration 50/1000 | Loss: 0.00001757
Iteration 51/1000 | Loss: 0.00001757
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001757
Iteration 56/1000 | Loss: 0.00001756
Iteration 57/1000 | Loss: 0.00001756
Iteration 58/1000 | Loss: 0.00001755
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001753
Iteration 62/1000 | Loss: 0.00001753
Iteration 63/1000 | Loss: 0.00001753
Iteration 64/1000 | Loss: 0.00001753
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001752
Iteration 69/1000 | Loss: 0.00001752
Iteration 70/1000 | Loss: 0.00001751
Iteration 71/1000 | Loss: 0.00001751
Iteration 72/1000 | Loss: 0.00001751
Iteration 73/1000 | Loss: 0.00001751
Iteration 74/1000 | Loss: 0.00001750
Iteration 75/1000 | Loss: 0.00001750
Iteration 76/1000 | Loss: 0.00001750
Iteration 77/1000 | Loss: 0.00001750
Iteration 78/1000 | Loss: 0.00001750
Iteration 79/1000 | Loss: 0.00001750
Iteration 80/1000 | Loss: 0.00001750
Iteration 81/1000 | Loss: 0.00001750
Iteration 82/1000 | Loss: 0.00001750
Iteration 83/1000 | Loss: 0.00001750
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001750
Iteration 87/1000 | Loss: 0.00001749
Iteration 88/1000 | Loss: 0.00001749
Iteration 89/1000 | Loss: 0.00001749
Iteration 90/1000 | Loss: 0.00001748
Iteration 91/1000 | Loss: 0.00001748
Iteration 92/1000 | Loss: 0.00001748
Iteration 93/1000 | Loss: 0.00001748
Iteration 94/1000 | Loss: 0.00001748
Iteration 95/1000 | Loss: 0.00001748
Iteration 96/1000 | Loss: 0.00001748
Iteration 97/1000 | Loss: 0.00001748
Iteration 98/1000 | Loss: 0.00001748
Iteration 99/1000 | Loss: 0.00001747
Iteration 100/1000 | Loss: 0.00001747
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001747
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001747
Iteration 107/1000 | Loss: 0.00001747
Iteration 108/1000 | Loss: 0.00001747
Iteration 109/1000 | Loss: 0.00001747
Iteration 110/1000 | Loss: 0.00001747
Iteration 111/1000 | Loss: 0.00001747
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001746
Iteration 114/1000 | Loss: 0.00001746
Iteration 115/1000 | Loss: 0.00001746
Iteration 116/1000 | Loss: 0.00001746
Iteration 117/1000 | Loss: 0.00001746
Iteration 118/1000 | Loss: 0.00001746
Iteration 119/1000 | Loss: 0.00001746
Iteration 120/1000 | Loss: 0.00001746
Iteration 121/1000 | Loss: 0.00001746
Iteration 122/1000 | Loss: 0.00001746
Iteration 123/1000 | Loss: 0.00001746
Iteration 124/1000 | Loss: 0.00001746
Iteration 125/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.745539702824317e-05, 1.745539702824317e-05, 1.745539702824317e-05, 1.745539702824317e-05, 1.745539702824317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.745539702824317e-05

Optimization complete. Final v2v error: 3.5045108795166016 mm

Highest mean error: 3.6797313690185547 mm for frame 3

Lowest mean error: 3.2501113414764404 mm for frame 131

Saving results

Total time: 37.248669147491455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391282
Iteration 2/25 | Loss: 0.00138347
Iteration 3/25 | Loss: 0.00131880
Iteration 4/25 | Loss: 0.00130901
Iteration 5/25 | Loss: 0.00130585
Iteration 6/25 | Loss: 0.00130492
Iteration 7/25 | Loss: 0.00130485
Iteration 8/25 | Loss: 0.00130485
Iteration 9/25 | Loss: 0.00130485
Iteration 10/25 | Loss: 0.00130485
Iteration 11/25 | Loss: 0.00130485
Iteration 12/25 | Loss: 0.00130485
Iteration 13/25 | Loss: 0.00130485
Iteration 14/25 | Loss: 0.00130485
Iteration 15/25 | Loss: 0.00130485
Iteration 16/25 | Loss: 0.00130485
Iteration 17/25 | Loss: 0.00130485
Iteration 18/25 | Loss: 0.00130485
Iteration 19/25 | Loss: 0.00130485
Iteration 20/25 | Loss: 0.00130485
Iteration 21/25 | Loss: 0.00130485
Iteration 22/25 | Loss: 0.00130485
Iteration 23/25 | Loss: 0.00130485
Iteration 24/25 | Loss: 0.00130485
Iteration 25/25 | Loss: 0.00130485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74666500
Iteration 2/25 | Loss: 0.00217429
Iteration 3/25 | Loss: 0.00217429
Iteration 4/25 | Loss: 0.00217429
Iteration 5/25 | Loss: 0.00217429
Iteration 6/25 | Loss: 0.00217429
Iteration 7/25 | Loss: 0.00217429
Iteration 8/25 | Loss: 0.00217429
Iteration 9/25 | Loss: 0.00217429
Iteration 10/25 | Loss: 0.00217429
Iteration 11/25 | Loss: 0.00217429
Iteration 12/25 | Loss: 0.00217429
Iteration 13/25 | Loss: 0.00217429
Iteration 14/25 | Loss: 0.00217429
Iteration 15/25 | Loss: 0.00217429
Iteration 16/25 | Loss: 0.00217429
Iteration 17/25 | Loss: 0.00217429
Iteration 18/25 | Loss: 0.00217429
Iteration 19/25 | Loss: 0.00217429
Iteration 20/25 | Loss: 0.00217429
Iteration 21/25 | Loss: 0.00217429
Iteration 22/25 | Loss: 0.00217429
Iteration 23/25 | Loss: 0.00217429
Iteration 24/25 | Loss: 0.00217429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0021742857061326504, 0.0021742857061326504, 0.0021742857061326504, 0.0021742857061326504, 0.0021742857061326504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021742857061326504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217429
Iteration 2/1000 | Loss: 0.00002013
Iteration 3/1000 | Loss: 0.00001528
Iteration 4/1000 | Loss: 0.00001411
Iteration 5/1000 | Loss: 0.00001338
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001216
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001129
Iteration 11/1000 | Loss: 0.00001113
Iteration 12/1000 | Loss: 0.00001106
Iteration 13/1000 | Loss: 0.00001100
Iteration 14/1000 | Loss: 0.00001091
Iteration 15/1000 | Loss: 0.00001089
Iteration 16/1000 | Loss: 0.00001087
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001081
Iteration 19/1000 | Loss: 0.00001081
Iteration 20/1000 | Loss: 0.00001074
Iteration 21/1000 | Loss: 0.00001070
Iteration 22/1000 | Loss: 0.00001070
Iteration 23/1000 | Loss: 0.00001070
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001068
Iteration 26/1000 | Loss: 0.00001064
Iteration 27/1000 | Loss: 0.00001063
Iteration 28/1000 | Loss: 0.00001063
Iteration 29/1000 | Loss: 0.00001063
Iteration 30/1000 | Loss: 0.00001062
Iteration 31/1000 | Loss: 0.00001062
Iteration 32/1000 | Loss: 0.00001061
Iteration 33/1000 | Loss: 0.00001061
Iteration 34/1000 | Loss: 0.00001059
Iteration 35/1000 | Loss: 0.00001059
Iteration 36/1000 | Loss: 0.00001058
Iteration 37/1000 | Loss: 0.00001058
Iteration 38/1000 | Loss: 0.00001058
Iteration 39/1000 | Loss: 0.00001056
Iteration 40/1000 | Loss: 0.00001053
Iteration 41/1000 | Loss: 0.00001053
Iteration 42/1000 | Loss: 0.00001053
Iteration 43/1000 | Loss: 0.00001053
Iteration 44/1000 | Loss: 0.00001053
Iteration 45/1000 | Loss: 0.00001053
Iteration 46/1000 | Loss: 0.00001052
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001051
Iteration 49/1000 | Loss: 0.00001048
Iteration 50/1000 | Loss: 0.00001048
Iteration 51/1000 | Loss: 0.00001048
Iteration 52/1000 | Loss: 0.00001047
Iteration 53/1000 | Loss: 0.00001047
Iteration 54/1000 | Loss: 0.00001047
Iteration 55/1000 | Loss: 0.00001047
Iteration 56/1000 | Loss: 0.00001047
Iteration 57/1000 | Loss: 0.00001047
Iteration 58/1000 | Loss: 0.00001047
Iteration 59/1000 | Loss: 0.00001047
Iteration 60/1000 | Loss: 0.00001047
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001046
Iteration 63/1000 | Loss: 0.00001046
Iteration 64/1000 | Loss: 0.00001046
Iteration 65/1000 | Loss: 0.00001044
Iteration 66/1000 | Loss: 0.00001043
Iteration 67/1000 | Loss: 0.00001043
Iteration 68/1000 | Loss: 0.00001042
Iteration 69/1000 | Loss: 0.00001042
Iteration 70/1000 | Loss: 0.00001042
Iteration 71/1000 | Loss: 0.00001041
Iteration 72/1000 | Loss: 0.00001041
Iteration 73/1000 | Loss: 0.00001041
Iteration 74/1000 | Loss: 0.00001041
Iteration 75/1000 | Loss: 0.00001041
Iteration 76/1000 | Loss: 0.00001041
Iteration 77/1000 | Loss: 0.00001040
Iteration 78/1000 | Loss: 0.00001040
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001039
Iteration 83/1000 | Loss: 0.00001038
Iteration 84/1000 | Loss: 0.00001038
Iteration 85/1000 | Loss: 0.00001038
Iteration 86/1000 | Loss: 0.00001038
Iteration 87/1000 | Loss: 0.00001038
Iteration 88/1000 | Loss: 0.00001037
Iteration 89/1000 | Loss: 0.00001037
Iteration 90/1000 | Loss: 0.00001037
Iteration 91/1000 | Loss: 0.00001037
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001037
Iteration 95/1000 | Loss: 0.00001036
Iteration 96/1000 | Loss: 0.00001036
Iteration 97/1000 | Loss: 0.00001036
Iteration 98/1000 | Loss: 0.00001036
Iteration 99/1000 | Loss: 0.00001036
Iteration 100/1000 | Loss: 0.00001036
Iteration 101/1000 | Loss: 0.00001036
Iteration 102/1000 | Loss: 0.00001035
Iteration 103/1000 | Loss: 0.00001035
Iteration 104/1000 | Loss: 0.00001035
Iteration 105/1000 | Loss: 0.00001035
Iteration 106/1000 | Loss: 0.00001035
Iteration 107/1000 | Loss: 0.00001035
Iteration 108/1000 | Loss: 0.00001035
Iteration 109/1000 | Loss: 0.00001035
Iteration 110/1000 | Loss: 0.00001034
Iteration 111/1000 | Loss: 0.00001034
Iteration 112/1000 | Loss: 0.00001034
Iteration 113/1000 | Loss: 0.00001034
Iteration 114/1000 | Loss: 0.00001034
Iteration 115/1000 | Loss: 0.00001034
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001033
Iteration 118/1000 | Loss: 0.00001033
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001033
Iteration 121/1000 | Loss: 0.00001033
Iteration 122/1000 | Loss: 0.00001033
Iteration 123/1000 | Loss: 0.00001032
Iteration 124/1000 | Loss: 0.00001032
Iteration 125/1000 | Loss: 0.00001032
Iteration 126/1000 | Loss: 0.00001032
Iteration 127/1000 | Loss: 0.00001032
Iteration 128/1000 | Loss: 0.00001032
Iteration 129/1000 | Loss: 0.00001032
Iteration 130/1000 | Loss: 0.00001031
Iteration 131/1000 | Loss: 0.00001031
Iteration 132/1000 | Loss: 0.00001031
Iteration 133/1000 | Loss: 0.00001031
Iteration 134/1000 | Loss: 0.00001031
Iteration 135/1000 | Loss: 0.00001031
Iteration 136/1000 | Loss: 0.00001031
Iteration 137/1000 | Loss: 0.00001030
Iteration 138/1000 | Loss: 0.00001030
Iteration 139/1000 | Loss: 0.00001030
Iteration 140/1000 | Loss: 0.00001030
Iteration 141/1000 | Loss: 0.00001030
Iteration 142/1000 | Loss: 0.00001030
Iteration 143/1000 | Loss: 0.00001030
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001030
Iteration 146/1000 | Loss: 0.00001029
Iteration 147/1000 | Loss: 0.00001029
Iteration 148/1000 | Loss: 0.00001029
Iteration 149/1000 | Loss: 0.00001029
Iteration 150/1000 | Loss: 0.00001029
Iteration 151/1000 | Loss: 0.00001029
Iteration 152/1000 | Loss: 0.00001029
Iteration 153/1000 | Loss: 0.00001029
Iteration 154/1000 | Loss: 0.00001029
Iteration 155/1000 | Loss: 0.00001028
Iteration 156/1000 | Loss: 0.00001028
Iteration 157/1000 | Loss: 0.00001028
Iteration 158/1000 | Loss: 0.00001028
Iteration 159/1000 | Loss: 0.00001028
Iteration 160/1000 | Loss: 0.00001028
Iteration 161/1000 | Loss: 0.00001028
Iteration 162/1000 | Loss: 0.00001028
Iteration 163/1000 | Loss: 0.00001028
Iteration 164/1000 | Loss: 0.00001028
Iteration 165/1000 | Loss: 0.00001028
Iteration 166/1000 | Loss: 0.00001028
Iteration 167/1000 | Loss: 0.00001028
Iteration 168/1000 | Loss: 0.00001028
Iteration 169/1000 | Loss: 0.00001028
Iteration 170/1000 | Loss: 0.00001027
Iteration 171/1000 | Loss: 0.00001027
Iteration 172/1000 | Loss: 0.00001027
Iteration 173/1000 | Loss: 0.00001027
Iteration 174/1000 | Loss: 0.00001026
Iteration 175/1000 | Loss: 0.00001026
Iteration 176/1000 | Loss: 0.00001026
Iteration 177/1000 | Loss: 0.00001026
Iteration 178/1000 | Loss: 0.00001026
Iteration 179/1000 | Loss: 0.00001026
Iteration 180/1000 | Loss: 0.00001026
Iteration 181/1000 | Loss: 0.00001026
Iteration 182/1000 | Loss: 0.00001026
Iteration 183/1000 | Loss: 0.00001025
Iteration 184/1000 | Loss: 0.00001025
Iteration 185/1000 | Loss: 0.00001025
Iteration 186/1000 | Loss: 0.00001025
Iteration 187/1000 | Loss: 0.00001025
Iteration 188/1000 | Loss: 0.00001025
Iteration 189/1000 | Loss: 0.00001025
Iteration 190/1000 | Loss: 0.00001025
Iteration 191/1000 | Loss: 0.00001025
Iteration 192/1000 | Loss: 0.00001025
Iteration 193/1000 | Loss: 0.00001025
Iteration 194/1000 | Loss: 0.00001025
Iteration 195/1000 | Loss: 0.00001025
Iteration 196/1000 | Loss: 0.00001025
Iteration 197/1000 | Loss: 0.00001024
Iteration 198/1000 | Loss: 0.00001024
Iteration 199/1000 | Loss: 0.00001024
Iteration 200/1000 | Loss: 0.00001024
Iteration 201/1000 | Loss: 0.00001024
Iteration 202/1000 | Loss: 0.00001024
Iteration 203/1000 | Loss: 0.00001024
Iteration 204/1000 | Loss: 0.00001024
Iteration 205/1000 | Loss: 0.00001024
Iteration 206/1000 | Loss: 0.00001024
Iteration 207/1000 | Loss: 0.00001024
Iteration 208/1000 | Loss: 0.00001024
Iteration 209/1000 | Loss: 0.00001024
Iteration 210/1000 | Loss: 0.00001023
Iteration 211/1000 | Loss: 0.00001023
Iteration 212/1000 | Loss: 0.00001023
Iteration 213/1000 | Loss: 0.00001023
Iteration 214/1000 | Loss: 0.00001023
Iteration 215/1000 | Loss: 0.00001023
Iteration 216/1000 | Loss: 0.00001023
Iteration 217/1000 | Loss: 0.00001023
Iteration 218/1000 | Loss: 0.00001022
Iteration 219/1000 | Loss: 0.00001022
Iteration 220/1000 | Loss: 0.00001022
Iteration 221/1000 | Loss: 0.00001022
Iteration 222/1000 | Loss: 0.00001022
Iteration 223/1000 | Loss: 0.00001022
Iteration 224/1000 | Loss: 0.00001022
Iteration 225/1000 | Loss: 0.00001022
Iteration 226/1000 | Loss: 0.00001022
Iteration 227/1000 | Loss: 0.00001022
Iteration 228/1000 | Loss: 0.00001022
Iteration 229/1000 | Loss: 0.00001022
Iteration 230/1000 | Loss: 0.00001022
Iteration 231/1000 | Loss: 0.00001022
Iteration 232/1000 | Loss: 0.00001022
Iteration 233/1000 | Loss: 0.00001022
Iteration 234/1000 | Loss: 0.00001022
Iteration 235/1000 | Loss: 0.00001022
Iteration 236/1000 | Loss: 0.00001022
Iteration 237/1000 | Loss: 0.00001022
Iteration 238/1000 | Loss: 0.00001022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.0224829566141125e-05, 1.0224829566141125e-05, 1.0224829566141125e-05, 1.0224829566141125e-05, 1.0224829566141125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0224829566141125e-05

Optimization complete. Final v2v error: 2.7808966636657715 mm

Highest mean error: 3.1855177879333496 mm for frame 55

Lowest mean error: 2.7024006843566895 mm for frame 3

Saving results

Total time: 43.9766206741333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841961
Iteration 2/25 | Loss: 0.00157960
Iteration 3/25 | Loss: 0.00144185
Iteration 4/25 | Loss: 0.00142428
Iteration 5/25 | Loss: 0.00142026
Iteration 6/25 | Loss: 0.00141985
Iteration 7/25 | Loss: 0.00141985
Iteration 8/25 | Loss: 0.00141985
Iteration 9/25 | Loss: 0.00141985
Iteration 10/25 | Loss: 0.00141985
Iteration 11/25 | Loss: 0.00141985
Iteration 12/25 | Loss: 0.00141985
Iteration 13/25 | Loss: 0.00141985
Iteration 14/25 | Loss: 0.00141985
Iteration 15/25 | Loss: 0.00141985
Iteration 16/25 | Loss: 0.00141985
Iteration 17/25 | Loss: 0.00141985
Iteration 18/25 | Loss: 0.00141985
Iteration 19/25 | Loss: 0.00141985
Iteration 20/25 | Loss: 0.00141985
Iteration 21/25 | Loss: 0.00141985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014198546996340156, 0.0014198546996340156, 0.0014198546996340156, 0.0014198546996340156, 0.0014198546996340156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014198546996340156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17797232
Iteration 2/25 | Loss: 0.00257477
Iteration 3/25 | Loss: 0.00257477
Iteration 4/25 | Loss: 0.00257476
Iteration 5/25 | Loss: 0.00257476
Iteration 6/25 | Loss: 0.00257476
Iteration 7/25 | Loss: 0.00257476
Iteration 8/25 | Loss: 0.00257476
Iteration 9/25 | Loss: 0.00257476
Iteration 10/25 | Loss: 0.00257476
Iteration 11/25 | Loss: 0.00257476
Iteration 12/25 | Loss: 0.00257476
Iteration 13/25 | Loss: 0.00257476
Iteration 14/25 | Loss: 0.00257476
Iteration 15/25 | Loss: 0.00257476
Iteration 16/25 | Loss: 0.00257476
Iteration 17/25 | Loss: 0.00257476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002574760699644685, 0.002574760699644685, 0.002574760699644685, 0.002574760699644685, 0.002574760699644685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002574760699644685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257476
Iteration 2/1000 | Loss: 0.00005914
Iteration 3/1000 | Loss: 0.00004896
Iteration 4/1000 | Loss: 0.00004056
Iteration 5/1000 | Loss: 0.00003811
Iteration 6/1000 | Loss: 0.00003603
Iteration 7/1000 | Loss: 0.00003504
Iteration 8/1000 | Loss: 0.00003430
Iteration 9/1000 | Loss: 0.00003378
Iteration 10/1000 | Loss: 0.00003332
Iteration 11/1000 | Loss: 0.00003297
Iteration 12/1000 | Loss: 0.00003267
Iteration 13/1000 | Loss: 0.00003264
Iteration 14/1000 | Loss: 0.00003239
Iteration 15/1000 | Loss: 0.00003215
Iteration 16/1000 | Loss: 0.00003203
Iteration 17/1000 | Loss: 0.00003202
Iteration 18/1000 | Loss: 0.00003198
Iteration 19/1000 | Loss: 0.00003196
Iteration 20/1000 | Loss: 0.00003196
Iteration 21/1000 | Loss: 0.00003187
Iteration 22/1000 | Loss: 0.00003174
Iteration 23/1000 | Loss: 0.00003173
Iteration 24/1000 | Loss: 0.00003172
Iteration 25/1000 | Loss: 0.00003169
Iteration 26/1000 | Loss: 0.00003168
Iteration 27/1000 | Loss: 0.00003168
Iteration 28/1000 | Loss: 0.00003168
Iteration 29/1000 | Loss: 0.00003167
Iteration 30/1000 | Loss: 0.00003167
Iteration 31/1000 | Loss: 0.00003166
Iteration 32/1000 | Loss: 0.00003165
Iteration 33/1000 | Loss: 0.00003165
Iteration 34/1000 | Loss: 0.00003164
Iteration 35/1000 | Loss: 0.00003164
Iteration 36/1000 | Loss: 0.00003164
Iteration 37/1000 | Loss: 0.00003163
Iteration 38/1000 | Loss: 0.00003161
Iteration 39/1000 | Loss: 0.00003160
Iteration 40/1000 | Loss: 0.00003160
Iteration 41/1000 | Loss: 0.00003159
Iteration 42/1000 | Loss: 0.00003159
Iteration 43/1000 | Loss: 0.00003159
Iteration 44/1000 | Loss: 0.00003158
Iteration 45/1000 | Loss: 0.00003158
Iteration 46/1000 | Loss: 0.00003158
Iteration 47/1000 | Loss: 0.00003158
Iteration 48/1000 | Loss: 0.00003155
Iteration 49/1000 | Loss: 0.00003155
Iteration 50/1000 | Loss: 0.00003155
Iteration 51/1000 | Loss: 0.00003155
Iteration 52/1000 | Loss: 0.00003154
Iteration 53/1000 | Loss: 0.00003154
Iteration 54/1000 | Loss: 0.00003154
Iteration 55/1000 | Loss: 0.00003154
Iteration 56/1000 | Loss: 0.00003154
Iteration 57/1000 | Loss: 0.00003154
Iteration 58/1000 | Loss: 0.00003154
Iteration 59/1000 | Loss: 0.00003154
Iteration 60/1000 | Loss: 0.00003154
Iteration 61/1000 | Loss: 0.00003152
Iteration 62/1000 | Loss: 0.00003152
Iteration 63/1000 | Loss: 0.00003151
Iteration 64/1000 | Loss: 0.00003151
Iteration 65/1000 | Loss: 0.00003151
Iteration 66/1000 | Loss: 0.00003151
Iteration 67/1000 | Loss: 0.00003150
Iteration 68/1000 | Loss: 0.00003150
Iteration 69/1000 | Loss: 0.00003150
Iteration 70/1000 | Loss: 0.00003150
Iteration 71/1000 | Loss: 0.00003149
Iteration 72/1000 | Loss: 0.00003149
Iteration 73/1000 | Loss: 0.00003148
Iteration 74/1000 | Loss: 0.00003148
Iteration 75/1000 | Loss: 0.00003147
Iteration 76/1000 | Loss: 0.00003147
Iteration 77/1000 | Loss: 0.00003147
Iteration 78/1000 | Loss: 0.00003147
Iteration 79/1000 | Loss: 0.00003146
Iteration 80/1000 | Loss: 0.00003146
Iteration 81/1000 | Loss: 0.00003146
Iteration 82/1000 | Loss: 0.00003146
Iteration 83/1000 | Loss: 0.00003145
Iteration 84/1000 | Loss: 0.00003145
Iteration 85/1000 | Loss: 0.00003144
Iteration 86/1000 | Loss: 0.00003143
Iteration 87/1000 | Loss: 0.00003143
Iteration 88/1000 | Loss: 0.00003143
Iteration 89/1000 | Loss: 0.00003142
Iteration 90/1000 | Loss: 0.00003142
Iteration 91/1000 | Loss: 0.00003141
Iteration 92/1000 | Loss: 0.00003141
Iteration 93/1000 | Loss: 0.00003141
Iteration 94/1000 | Loss: 0.00003141
Iteration 95/1000 | Loss: 0.00003141
Iteration 96/1000 | Loss: 0.00003141
Iteration 97/1000 | Loss: 0.00003140
Iteration 98/1000 | Loss: 0.00003140
Iteration 99/1000 | Loss: 0.00003140
Iteration 100/1000 | Loss: 0.00003140
Iteration 101/1000 | Loss: 0.00003140
Iteration 102/1000 | Loss: 0.00003140
Iteration 103/1000 | Loss: 0.00003139
Iteration 104/1000 | Loss: 0.00003139
Iteration 105/1000 | Loss: 0.00003139
Iteration 106/1000 | Loss: 0.00003139
Iteration 107/1000 | Loss: 0.00003138
Iteration 108/1000 | Loss: 0.00003138
Iteration 109/1000 | Loss: 0.00003138
Iteration 110/1000 | Loss: 0.00003138
Iteration 111/1000 | Loss: 0.00003138
Iteration 112/1000 | Loss: 0.00003138
Iteration 113/1000 | Loss: 0.00003138
Iteration 114/1000 | Loss: 0.00003137
Iteration 115/1000 | Loss: 0.00003137
Iteration 116/1000 | Loss: 0.00003137
Iteration 117/1000 | Loss: 0.00003136
Iteration 118/1000 | Loss: 0.00003136
Iteration 119/1000 | Loss: 0.00003136
Iteration 120/1000 | Loss: 0.00003136
Iteration 121/1000 | Loss: 0.00003136
Iteration 122/1000 | Loss: 0.00003135
Iteration 123/1000 | Loss: 0.00003135
Iteration 124/1000 | Loss: 0.00003135
Iteration 125/1000 | Loss: 0.00003135
Iteration 126/1000 | Loss: 0.00003135
Iteration 127/1000 | Loss: 0.00003135
Iteration 128/1000 | Loss: 0.00003135
Iteration 129/1000 | Loss: 0.00003135
Iteration 130/1000 | Loss: 0.00003135
Iteration 131/1000 | Loss: 0.00003135
Iteration 132/1000 | Loss: 0.00003135
Iteration 133/1000 | Loss: 0.00003135
Iteration 134/1000 | Loss: 0.00003134
Iteration 135/1000 | Loss: 0.00003133
Iteration 136/1000 | Loss: 0.00003133
Iteration 137/1000 | Loss: 0.00003133
Iteration 138/1000 | Loss: 0.00003133
Iteration 139/1000 | Loss: 0.00003133
Iteration 140/1000 | Loss: 0.00003132
Iteration 141/1000 | Loss: 0.00003132
Iteration 142/1000 | Loss: 0.00003132
Iteration 143/1000 | Loss: 0.00003132
Iteration 144/1000 | Loss: 0.00003132
Iteration 145/1000 | Loss: 0.00003132
Iteration 146/1000 | Loss: 0.00003132
Iteration 147/1000 | Loss: 0.00003132
Iteration 148/1000 | Loss: 0.00003132
Iteration 149/1000 | Loss: 0.00003132
Iteration 150/1000 | Loss: 0.00003132
Iteration 151/1000 | Loss: 0.00003132
Iteration 152/1000 | Loss: 0.00003132
Iteration 153/1000 | Loss: 0.00003132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.132044730591588e-05, 3.132044730591588e-05, 3.132044730591588e-05, 3.132044730591588e-05, 3.132044730591588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.132044730591588e-05

Optimization complete. Final v2v error: 4.674997806549072 mm

Highest mean error: 5.281718730926514 mm for frame 23

Lowest mean error: 4.12419319152832 mm for frame 84

Saving results

Total time: 40.839322090148926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816515
Iteration 2/25 | Loss: 0.00145492
Iteration 3/25 | Loss: 0.00137336
Iteration 4/25 | Loss: 0.00136330
Iteration 5/25 | Loss: 0.00136161
Iteration 6/25 | Loss: 0.00136161
Iteration 7/25 | Loss: 0.00136161
Iteration 8/25 | Loss: 0.00136161
Iteration 9/25 | Loss: 0.00136161
Iteration 10/25 | Loss: 0.00136161
Iteration 11/25 | Loss: 0.00136161
Iteration 12/25 | Loss: 0.00136161
Iteration 13/25 | Loss: 0.00136161
Iteration 14/25 | Loss: 0.00136161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013616130454465747, 0.0013616130454465747, 0.0013616130454465747, 0.0013616130454465747, 0.0013616130454465747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013616130454465747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19969845
Iteration 2/25 | Loss: 0.00166404
Iteration 3/25 | Loss: 0.00166398
Iteration 4/25 | Loss: 0.00166398
Iteration 5/25 | Loss: 0.00166398
Iteration 6/25 | Loss: 0.00166398
Iteration 7/25 | Loss: 0.00166398
Iteration 8/25 | Loss: 0.00166398
Iteration 9/25 | Loss: 0.00166397
Iteration 10/25 | Loss: 0.00166397
Iteration 11/25 | Loss: 0.00166397
Iteration 12/25 | Loss: 0.00166397
Iteration 13/25 | Loss: 0.00166397
Iteration 14/25 | Loss: 0.00166397
Iteration 15/25 | Loss: 0.00166397
Iteration 16/25 | Loss: 0.00166397
Iteration 17/25 | Loss: 0.00166397
Iteration 18/25 | Loss: 0.00166397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016639739042147994, 0.0016639739042147994, 0.0016639739042147994, 0.0016639739042147994, 0.0016639739042147994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016639739042147994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166397
Iteration 2/1000 | Loss: 0.00004026
Iteration 3/1000 | Loss: 0.00002652
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001734
Iteration 6/1000 | Loss: 0.00001638
Iteration 7/1000 | Loss: 0.00001566
Iteration 8/1000 | Loss: 0.00001518
Iteration 9/1000 | Loss: 0.00001470
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001394
Iteration 12/1000 | Loss: 0.00001358
Iteration 13/1000 | Loss: 0.00001323
Iteration 14/1000 | Loss: 0.00001296
Iteration 15/1000 | Loss: 0.00001277
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001234
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001218
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001216
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001216
Iteration 26/1000 | Loss: 0.00001216
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001215
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001189
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001185
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001179
Iteration 45/1000 | Loss: 0.00001178
Iteration 46/1000 | Loss: 0.00001177
Iteration 47/1000 | Loss: 0.00001176
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001176
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001175
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001174
Iteration 58/1000 | Loss: 0.00001174
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001172
Iteration 66/1000 | Loss: 0.00001172
Iteration 67/1000 | Loss: 0.00001172
Iteration 68/1000 | Loss: 0.00001171
Iteration 69/1000 | Loss: 0.00001171
Iteration 70/1000 | Loss: 0.00001171
Iteration 71/1000 | Loss: 0.00001171
Iteration 72/1000 | Loss: 0.00001170
Iteration 73/1000 | Loss: 0.00001170
Iteration 74/1000 | Loss: 0.00001170
Iteration 75/1000 | Loss: 0.00001170
Iteration 76/1000 | Loss: 0.00001169
Iteration 77/1000 | Loss: 0.00001169
Iteration 78/1000 | Loss: 0.00001169
Iteration 79/1000 | Loss: 0.00001169
Iteration 80/1000 | Loss: 0.00001169
Iteration 81/1000 | Loss: 0.00001169
Iteration 82/1000 | Loss: 0.00001169
Iteration 83/1000 | Loss: 0.00001169
Iteration 84/1000 | Loss: 0.00001169
Iteration 85/1000 | Loss: 0.00001169
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001169
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001169
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001169
Iteration 100/1000 | Loss: 0.00001169
Iteration 101/1000 | Loss: 0.00001169
Iteration 102/1000 | Loss: 0.00001169
Iteration 103/1000 | Loss: 0.00001169
Iteration 104/1000 | Loss: 0.00001169
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.1689888196997344e-05, 1.1689888196997344e-05, 1.1689888196997344e-05, 1.1689888196997344e-05, 1.1689888196997344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1689888196997344e-05

Optimization complete. Final v2v error: 2.95835542678833 mm

Highest mean error: 3.252861499786377 mm for frame 2

Lowest mean error: 2.7912232875823975 mm for frame 106

Saving results

Total time: 41.27727222442627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892143
Iteration 2/25 | Loss: 0.00178927
Iteration 3/25 | Loss: 0.00150442
Iteration 4/25 | Loss: 0.00145040
Iteration 5/25 | Loss: 0.00143161
Iteration 6/25 | Loss: 0.00142838
Iteration 7/25 | Loss: 0.00140984
Iteration 8/25 | Loss: 0.00140564
Iteration 9/25 | Loss: 0.00141242
Iteration 10/25 | Loss: 0.00140427
Iteration 11/25 | Loss: 0.00140367
Iteration 12/25 | Loss: 0.00140243
Iteration 13/25 | Loss: 0.00140153
Iteration 14/25 | Loss: 0.00140163
Iteration 15/25 | Loss: 0.00140103
Iteration 16/25 | Loss: 0.00140029
Iteration 17/25 | Loss: 0.00139980
Iteration 18/25 | Loss: 0.00139961
Iteration 19/25 | Loss: 0.00139952
Iteration 20/25 | Loss: 0.00139952
Iteration 21/25 | Loss: 0.00139952
Iteration 22/25 | Loss: 0.00139952
Iteration 23/25 | Loss: 0.00139951
Iteration 24/25 | Loss: 0.00139951
Iteration 25/25 | Loss: 0.00139951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.00153017
Iteration 2/25 | Loss: 0.00221018
Iteration 3/25 | Loss: 0.00221018
Iteration 4/25 | Loss: 0.00221018
Iteration 5/25 | Loss: 0.00221018
Iteration 6/25 | Loss: 0.00221017
Iteration 7/25 | Loss: 0.00221017
Iteration 8/25 | Loss: 0.00221017
Iteration 9/25 | Loss: 0.00221017
Iteration 10/25 | Loss: 0.00221017
Iteration 11/25 | Loss: 0.00221017
Iteration 12/25 | Loss: 0.00221017
Iteration 13/25 | Loss: 0.00221017
Iteration 14/25 | Loss: 0.00221017
Iteration 15/25 | Loss: 0.00221017
Iteration 16/25 | Loss: 0.00221017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002210173523053527, 0.002210173523053527, 0.002210173523053527, 0.002210173523053527, 0.002210173523053527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002210173523053527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221017
Iteration 2/1000 | Loss: 0.00004817
Iteration 3/1000 | Loss: 0.00003308
Iteration 4/1000 | Loss: 0.00002766
Iteration 5/1000 | Loss: 0.00002562
Iteration 6/1000 | Loss: 0.00002445
Iteration 7/1000 | Loss: 0.00002353
Iteration 8/1000 | Loss: 0.00002292
Iteration 9/1000 | Loss: 0.00002251
Iteration 10/1000 | Loss: 0.00002211
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002149
Iteration 13/1000 | Loss: 0.00002146
Iteration 14/1000 | Loss: 0.00002123
Iteration 15/1000 | Loss: 0.00002111
Iteration 16/1000 | Loss: 0.00002110
Iteration 17/1000 | Loss: 0.00002109
Iteration 18/1000 | Loss: 0.00002105
Iteration 19/1000 | Loss: 0.00002105
Iteration 20/1000 | Loss: 0.00002105
Iteration 21/1000 | Loss: 0.00002104
Iteration 22/1000 | Loss: 0.00002102
Iteration 23/1000 | Loss: 0.00002102
Iteration 24/1000 | Loss: 0.00002101
Iteration 25/1000 | Loss: 0.00002100
Iteration 26/1000 | Loss: 0.00002098
Iteration 27/1000 | Loss: 0.00002097
Iteration 28/1000 | Loss: 0.00002097
Iteration 29/1000 | Loss: 0.00002096
Iteration 30/1000 | Loss: 0.00002095
Iteration 31/1000 | Loss: 0.00002095
Iteration 32/1000 | Loss: 0.00002095
Iteration 33/1000 | Loss: 0.00002094
Iteration 34/1000 | Loss: 0.00002094
Iteration 35/1000 | Loss: 0.00002093
Iteration 36/1000 | Loss: 0.00002093
Iteration 37/1000 | Loss: 0.00002093
Iteration 38/1000 | Loss: 0.00002093
Iteration 39/1000 | Loss: 0.00002092
Iteration 40/1000 | Loss: 0.00002092
Iteration 41/1000 | Loss: 0.00002091
Iteration 42/1000 | Loss: 0.00002091
Iteration 43/1000 | Loss: 0.00002091
Iteration 44/1000 | Loss: 0.00002091
Iteration 45/1000 | Loss: 0.00002090
Iteration 46/1000 | Loss: 0.00002090
Iteration 47/1000 | Loss: 0.00002090
Iteration 48/1000 | Loss: 0.00002089
Iteration 49/1000 | Loss: 0.00002089
Iteration 50/1000 | Loss: 0.00002088
Iteration 51/1000 | Loss: 0.00002088
Iteration 52/1000 | Loss: 0.00002088
Iteration 53/1000 | Loss: 0.00002088
Iteration 54/1000 | Loss: 0.00002088
Iteration 55/1000 | Loss: 0.00002087
Iteration 56/1000 | Loss: 0.00002087
Iteration 57/1000 | Loss: 0.00002087
Iteration 58/1000 | Loss: 0.00002087
Iteration 59/1000 | Loss: 0.00002087
Iteration 60/1000 | Loss: 0.00002087
Iteration 61/1000 | Loss: 0.00002087
Iteration 62/1000 | Loss: 0.00002087
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002086
Iteration 65/1000 | Loss: 0.00002086
Iteration 66/1000 | Loss: 0.00002086
Iteration 67/1000 | Loss: 0.00002086
Iteration 68/1000 | Loss: 0.00002086
Iteration 69/1000 | Loss: 0.00002086
Iteration 70/1000 | Loss: 0.00002086
Iteration 71/1000 | Loss: 0.00002086
Iteration 72/1000 | Loss: 0.00002086
Iteration 73/1000 | Loss: 0.00002086
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002085
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002085
Iteration 80/1000 | Loss: 0.00002085
Iteration 81/1000 | Loss: 0.00002085
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002085
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002085
Iteration 87/1000 | Loss: 0.00002085
Iteration 88/1000 | Loss: 0.00002084
Iteration 89/1000 | Loss: 0.00002084
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002084
Iteration 93/1000 | Loss: 0.00002084
Iteration 94/1000 | Loss: 0.00002084
Iteration 95/1000 | Loss: 0.00002084
Iteration 96/1000 | Loss: 0.00002084
Iteration 97/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.0844387108809315e-05, 2.0844387108809315e-05, 2.0844387108809315e-05, 2.0844387108809315e-05, 2.0844387108809315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0844387108809315e-05

Optimization complete. Final v2v error: 3.859138011932373 mm

Highest mean error: 4.542648792266846 mm for frame 63

Lowest mean error: 3.4210309982299805 mm for frame 138

Saving results

Total time: 60.36788487434387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407813
Iteration 2/25 | Loss: 0.00139182
Iteration 3/25 | Loss: 0.00133573
Iteration 4/25 | Loss: 0.00132869
Iteration 5/25 | Loss: 0.00132689
Iteration 6/25 | Loss: 0.00132686
Iteration 7/25 | Loss: 0.00132685
Iteration 8/25 | Loss: 0.00132685
Iteration 9/25 | Loss: 0.00132685
Iteration 10/25 | Loss: 0.00132685
Iteration 11/25 | Loss: 0.00132685
Iteration 12/25 | Loss: 0.00132685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013268549228087068, 0.0013268549228087068, 0.0013268549228087068, 0.0013268549228087068, 0.0013268549228087068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013268549228087068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21186531
Iteration 2/25 | Loss: 0.00260956
Iteration 3/25 | Loss: 0.00260953
Iteration 4/25 | Loss: 0.00260953
Iteration 5/25 | Loss: 0.00260953
Iteration 6/25 | Loss: 0.00260953
Iteration 7/25 | Loss: 0.00260953
Iteration 8/25 | Loss: 0.00260953
Iteration 9/25 | Loss: 0.00260953
Iteration 10/25 | Loss: 0.00260953
Iteration 11/25 | Loss: 0.00260953
Iteration 12/25 | Loss: 0.00260953
Iteration 13/25 | Loss: 0.00260953
Iteration 14/25 | Loss: 0.00260953
Iteration 15/25 | Loss: 0.00260953
Iteration 16/25 | Loss: 0.00260953
Iteration 17/25 | Loss: 0.00260953
Iteration 18/25 | Loss: 0.00260953
Iteration 19/25 | Loss: 0.00260953
Iteration 20/25 | Loss: 0.00260953
Iteration 21/25 | Loss: 0.00260953
Iteration 22/25 | Loss: 0.00260953
Iteration 23/25 | Loss: 0.00260953
Iteration 24/25 | Loss: 0.00260953
Iteration 25/25 | Loss: 0.00260953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0026095302309840918, 0.0026095302309840918, 0.0026095302309840918, 0.0026095302309840918, 0.0026095302309840918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026095302309840918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260953
Iteration 2/1000 | Loss: 0.00002662
Iteration 3/1000 | Loss: 0.00001808
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001338
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001252
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00001198
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001143
Iteration 17/1000 | Loss: 0.00001142
Iteration 18/1000 | Loss: 0.00001138
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001119
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001096
Iteration 27/1000 | Loss: 0.00001096
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001087
Iteration 31/1000 | Loss: 0.00001086
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001075
Iteration 40/1000 | Loss: 0.00001071
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001061
Iteration 43/1000 | Loss: 0.00001061
Iteration 44/1000 | Loss: 0.00001060
Iteration 45/1000 | Loss: 0.00001059
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001055
Iteration 48/1000 | Loss: 0.00001054
Iteration 49/1000 | Loss: 0.00001053
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001047
Iteration 53/1000 | Loss: 0.00001044
Iteration 54/1000 | Loss: 0.00001044
Iteration 55/1000 | Loss: 0.00001044
Iteration 56/1000 | Loss: 0.00001044
Iteration 57/1000 | Loss: 0.00001043
Iteration 58/1000 | Loss: 0.00001043
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001041
Iteration 62/1000 | Loss: 0.00001041
Iteration 63/1000 | Loss: 0.00001041
Iteration 64/1000 | Loss: 0.00001041
Iteration 65/1000 | Loss: 0.00001041
Iteration 66/1000 | Loss: 0.00001041
Iteration 67/1000 | Loss: 0.00001041
Iteration 68/1000 | Loss: 0.00001040
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001039
Iteration 72/1000 | Loss: 0.00001039
Iteration 73/1000 | Loss: 0.00001039
Iteration 74/1000 | Loss: 0.00001039
Iteration 75/1000 | Loss: 0.00001038
Iteration 76/1000 | Loss: 0.00001038
Iteration 77/1000 | Loss: 0.00001038
Iteration 78/1000 | Loss: 0.00001038
Iteration 79/1000 | Loss: 0.00001038
Iteration 80/1000 | Loss: 0.00001038
Iteration 81/1000 | Loss: 0.00001038
Iteration 82/1000 | Loss: 0.00001037
Iteration 83/1000 | Loss: 0.00001037
Iteration 84/1000 | Loss: 0.00001037
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001034
Iteration 89/1000 | Loss: 0.00001034
Iteration 90/1000 | Loss: 0.00001034
Iteration 91/1000 | Loss: 0.00001034
Iteration 92/1000 | Loss: 0.00001034
Iteration 93/1000 | Loss: 0.00001034
Iteration 94/1000 | Loss: 0.00001034
Iteration 95/1000 | Loss: 0.00001033
Iteration 96/1000 | Loss: 0.00001033
Iteration 97/1000 | Loss: 0.00001033
Iteration 98/1000 | Loss: 0.00001033
Iteration 99/1000 | Loss: 0.00001032
Iteration 100/1000 | Loss: 0.00001032
Iteration 101/1000 | Loss: 0.00001032
Iteration 102/1000 | Loss: 0.00001032
Iteration 103/1000 | Loss: 0.00001032
Iteration 104/1000 | Loss: 0.00001032
Iteration 105/1000 | Loss: 0.00001032
Iteration 106/1000 | Loss: 0.00001031
Iteration 107/1000 | Loss: 0.00001031
Iteration 108/1000 | Loss: 0.00001031
Iteration 109/1000 | Loss: 0.00001031
Iteration 110/1000 | Loss: 0.00001030
Iteration 111/1000 | Loss: 0.00001030
Iteration 112/1000 | Loss: 0.00001030
Iteration 113/1000 | Loss: 0.00001030
Iteration 114/1000 | Loss: 0.00001030
Iteration 115/1000 | Loss: 0.00001030
Iteration 116/1000 | Loss: 0.00001030
Iteration 117/1000 | Loss: 0.00001030
Iteration 118/1000 | Loss: 0.00001030
Iteration 119/1000 | Loss: 0.00001029
Iteration 120/1000 | Loss: 0.00001029
Iteration 121/1000 | Loss: 0.00001029
Iteration 122/1000 | Loss: 0.00001029
Iteration 123/1000 | Loss: 0.00001029
Iteration 124/1000 | Loss: 0.00001029
Iteration 125/1000 | Loss: 0.00001029
Iteration 126/1000 | Loss: 0.00001029
Iteration 127/1000 | Loss: 0.00001028
Iteration 128/1000 | Loss: 0.00001028
Iteration 129/1000 | Loss: 0.00001028
Iteration 130/1000 | Loss: 0.00001028
Iteration 131/1000 | Loss: 0.00001028
Iteration 132/1000 | Loss: 0.00001028
Iteration 133/1000 | Loss: 0.00001028
Iteration 134/1000 | Loss: 0.00001028
Iteration 135/1000 | Loss: 0.00001027
Iteration 136/1000 | Loss: 0.00001027
Iteration 137/1000 | Loss: 0.00001027
Iteration 138/1000 | Loss: 0.00001027
Iteration 139/1000 | Loss: 0.00001027
Iteration 140/1000 | Loss: 0.00001027
Iteration 141/1000 | Loss: 0.00001027
Iteration 142/1000 | Loss: 0.00001027
Iteration 143/1000 | Loss: 0.00001027
Iteration 144/1000 | Loss: 0.00001027
Iteration 145/1000 | Loss: 0.00001027
Iteration 146/1000 | Loss: 0.00001026
Iteration 147/1000 | Loss: 0.00001026
Iteration 148/1000 | Loss: 0.00001026
Iteration 149/1000 | Loss: 0.00001026
Iteration 150/1000 | Loss: 0.00001026
Iteration 151/1000 | Loss: 0.00001025
Iteration 152/1000 | Loss: 0.00001025
Iteration 153/1000 | Loss: 0.00001025
Iteration 154/1000 | Loss: 0.00001025
Iteration 155/1000 | Loss: 0.00001025
Iteration 156/1000 | Loss: 0.00001025
Iteration 157/1000 | Loss: 0.00001025
Iteration 158/1000 | Loss: 0.00001024
Iteration 159/1000 | Loss: 0.00001024
Iteration 160/1000 | Loss: 0.00001024
Iteration 161/1000 | Loss: 0.00001024
Iteration 162/1000 | Loss: 0.00001024
Iteration 163/1000 | Loss: 0.00001023
Iteration 164/1000 | Loss: 0.00001023
Iteration 165/1000 | Loss: 0.00001023
Iteration 166/1000 | Loss: 0.00001023
Iteration 167/1000 | Loss: 0.00001022
Iteration 168/1000 | Loss: 0.00001022
Iteration 169/1000 | Loss: 0.00001022
Iteration 170/1000 | Loss: 0.00001022
Iteration 171/1000 | Loss: 0.00001022
Iteration 172/1000 | Loss: 0.00001022
Iteration 173/1000 | Loss: 0.00001022
Iteration 174/1000 | Loss: 0.00001022
Iteration 175/1000 | Loss: 0.00001022
Iteration 176/1000 | Loss: 0.00001022
Iteration 177/1000 | Loss: 0.00001022
Iteration 178/1000 | Loss: 0.00001021
Iteration 179/1000 | Loss: 0.00001021
Iteration 180/1000 | Loss: 0.00001021
Iteration 181/1000 | Loss: 0.00001021
Iteration 182/1000 | Loss: 0.00001021
Iteration 183/1000 | Loss: 0.00001021
Iteration 184/1000 | Loss: 0.00001021
Iteration 185/1000 | Loss: 0.00001021
Iteration 186/1000 | Loss: 0.00001020
Iteration 187/1000 | Loss: 0.00001020
Iteration 188/1000 | Loss: 0.00001020
Iteration 189/1000 | Loss: 0.00001020
Iteration 190/1000 | Loss: 0.00001020
Iteration 191/1000 | Loss: 0.00001020
Iteration 192/1000 | Loss: 0.00001020
Iteration 193/1000 | Loss: 0.00001020
Iteration 194/1000 | Loss: 0.00001020
Iteration 195/1000 | Loss: 0.00001020
Iteration 196/1000 | Loss: 0.00001020
Iteration 197/1000 | Loss: 0.00001020
Iteration 198/1000 | Loss: 0.00001020
Iteration 199/1000 | Loss: 0.00001020
Iteration 200/1000 | Loss: 0.00001019
Iteration 201/1000 | Loss: 0.00001019
Iteration 202/1000 | Loss: 0.00001019
Iteration 203/1000 | Loss: 0.00001019
Iteration 204/1000 | Loss: 0.00001019
Iteration 205/1000 | Loss: 0.00001019
Iteration 206/1000 | Loss: 0.00001019
Iteration 207/1000 | Loss: 0.00001019
Iteration 208/1000 | Loss: 0.00001019
Iteration 209/1000 | Loss: 0.00001019
Iteration 210/1000 | Loss: 0.00001019
Iteration 211/1000 | Loss: 0.00001019
Iteration 212/1000 | Loss: 0.00001019
Iteration 213/1000 | Loss: 0.00001019
Iteration 214/1000 | Loss: 0.00001019
Iteration 215/1000 | Loss: 0.00001019
Iteration 216/1000 | Loss: 0.00001019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.0190520697506145e-05, 1.0190520697506145e-05, 1.0190520697506145e-05, 1.0190520697506145e-05, 1.0190520697506145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0190520697506145e-05

Optimization complete. Final v2v error: 2.75221848487854 mm

Highest mean error: 3.0145347118377686 mm for frame 84

Lowest mean error: 2.586275339126587 mm for frame 4

Saving results

Total time: 45.85990571975708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973196
Iteration 2/25 | Loss: 0.00226710
Iteration 3/25 | Loss: 0.00181080
Iteration 4/25 | Loss: 0.00180066
Iteration 5/25 | Loss: 0.00172909
Iteration 6/25 | Loss: 0.00160097
Iteration 7/25 | Loss: 0.00162046
Iteration 8/25 | Loss: 0.00155480
Iteration 9/25 | Loss: 0.00154668
Iteration 10/25 | Loss: 0.00149385
Iteration 11/25 | Loss: 0.00146041
Iteration 12/25 | Loss: 0.00146899
Iteration 13/25 | Loss: 0.00142938
Iteration 14/25 | Loss: 0.00141211
Iteration 15/25 | Loss: 0.00142217
Iteration 16/25 | Loss: 0.00140858
Iteration 17/25 | Loss: 0.00140082
Iteration 18/25 | Loss: 0.00140824
Iteration 19/25 | Loss: 0.00140219
Iteration 20/25 | Loss: 0.00139463
Iteration 21/25 | Loss: 0.00139173
Iteration 22/25 | Loss: 0.00139362
Iteration 23/25 | Loss: 0.00139289
Iteration 24/25 | Loss: 0.00138733
Iteration 25/25 | Loss: 0.00138541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24572265
Iteration 2/25 | Loss: 0.00227624
Iteration 3/25 | Loss: 0.00227624
Iteration 4/25 | Loss: 0.00227624
Iteration 5/25 | Loss: 0.00227624
Iteration 6/25 | Loss: 0.00227624
Iteration 7/25 | Loss: 0.00227624
Iteration 8/25 | Loss: 0.00227624
Iteration 9/25 | Loss: 0.00227624
Iteration 10/25 | Loss: 0.00227624
Iteration 11/25 | Loss: 0.00227624
Iteration 12/25 | Loss: 0.00227624
Iteration 13/25 | Loss: 0.00227624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002276237355545163, 0.002276237355545163, 0.002276237355545163, 0.002276237355545163, 0.002276237355545163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002276237355545163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227624
Iteration 2/1000 | Loss: 0.00007793
Iteration 3/1000 | Loss: 0.00005548
Iteration 4/1000 | Loss: 0.00004274
Iteration 5/1000 | Loss: 0.00021626
Iteration 6/1000 | Loss: 0.00014042
Iteration 7/1000 | Loss: 0.00016054
Iteration 8/1000 | Loss: 0.00004969
Iteration 9/1000 | Loss: 0.00003937
Iteration 10/1000 | Loss: 0.00003514
Iteration 11/1000 | Loss: 0.00003319
Iteration 12/1000 | Loss: 0.00003145
Iteration 13/1000 | Loss: 0.00003039
Iteration 14/1000 | Loss: 0.00002974
Iteration 15/1000 | Loss: 0.00002914
Iteration 16/1000 | Loss: 0.00002857
Iteration 17/1000 | Loss: 0.00037237
Iteration 18/1000 | Loss: 0.00008835
Iteration 19/1000 | Loss: 0.00003012
Iteration 20/1000 | Loss: 0.00002517
Iteration 21/1000 | Loss: 0.00002198
Iteration 22/1000 | Loss: 0.00001971
Iteration 23/1000 | Loss: 0.00001879
Iteration 24/1000 | Loss: 0.00001777
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001668
Iteration 27/1000 | Loss: 0.00001636
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001512
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001500
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001497
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001493
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001486
Iteration 63/1000 | Loss: 0.00001486
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001484
Iteration 70/1000 | Loss: 0.00001484
Iteration 71/1000 | Loss: 0.00001484
Iteration 72/1000 | Loss: 0.00001484
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001483
Iteration 76/1000 | Loss: 0.00001483
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001483
Iteration 82/1000 | Loss: 0.00001483
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001483
Iteration 85/1000 | Loss: 0.00001483
Iteration 86/1000 | Loss: 0.00001483
Iteration 87/1000 | Loss: 0.00001483
Iteration 88/1000 | Loss: 0.00001482
Iteration 89/1000 | Loss: 0.00001482
Iteration 90/1000 | Loss: 0.00001481
Iteration 91/1000 | Loss: 0.00001481
Iteration 92/1000 | Loss: 0.00001481
Iteration 93/1000 | Loss: 0.00001481
Iteration 94/1000 | Loss: 0.00001481
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001480
Iteration 97/1000 | Loss: 0.00001480
Iteration 98/1000 | Loss: 0.00001480
Iteration 99/1000 | Loss: 0.00023632
Iteration 100/1000 | Loss: 0.00042986
Iteration 101/1000 | Loss: 0.00011009
Iteration 102/1000 | Loss: 0.00002703
Iteration 103/1000 | Loss: 0.00002436
Iteration 104/1000 | Loss: 0.00003447
Iteration 105/1000 | Loss: 0.00005395
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001578
Iteration 108/1000 | Loss: 0.00001574
Iteration 109/1000 | Loss: 0.00001555
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001536
Iteration 112/1000 | Loss: 0.00001534
Iteration 113/1000 | Loss: 0.00001534
Iteration 114/1000 | Loss: 0.00001533
Iteration 115/1000 | Loss: 0.00001533
Iteration 116/1000 | Loss: 0.00001529
Iteration 117/1000 | Loss: 0.00001528
Iteration 118/1000 | Loss: 0.00001527
Iteration 119/1000 | Loss: 0.00001527
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001524
Iteration 123/1000 | Loss: 0.00001524
Iteration 124/1000 | Loss: 0.00001524
Iteration 125/1000 | Loss: 0.00001524
Iteration 126/1000 | Loss: 0.00001524
Iteration 127/1000 | Loss: 0.00001524
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001523
Iteration 131/1000 | Loss: 0.00001523
Iteration 132/1000 | Loss: 0.00001523
Iteration 133/1000 | Loss: 0.00001523
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001523
Iteration 139/1000 | Loss: 0.00001522
Iteration 140/1000 | Loss: 0.00001522
Iteration 141/1000 | Loss: 0.00001522
Iteration 142/1000 | Loss: 0.00001521
Iteration 143/1000 | Loss: 0.00001521
Iteration 144/1000 | Loss: 0.00001520
Iteration 145/1000 | Loss: 0.00001520
Iteration 146/1000 | Loss: 0.00001520
Iteration 147/1000 | Loss: 0.00001519
Iteration 148/1000 | Loss: 0.00001519
Iteration 149/1000 | Loss: 0.00001518
Iteration 150/1000 | Loss: 0.00001518
Iteration 151/1000 | Loss: 0.00001517
Iteration 152/1000 | Loss: 0.00001517
Iteration 153/1000 | Loss: 0.00001516
Iteration 154/1000 | Loss: 0.00001516
Iteration 155/1000 | Loss: 0.00001516
Iteration 156/1000 | Loss: 0.00001515
Iteration 157/1000 | Loss: 0.00001515
Iteration 158/1000 | Loss: 0.00001515
Iteration 159/1000 | Loss: 0.00001515
Iteration 160/1000 | Loss: 0.00001515
Iteration 161/1000 | Loss: 0.00001513
Iteration 162/1000 | Loss: 0.00001513
Iteration 163/1000 | Loss: 0.00001513
Iteration 164/1000 | Loss: 0.00001513
Iteration 165/1000 | Loss: 0.00001513
Iteration 166/1000 | Loss: 0.00001513
Iteration 167/1000 | Loss: 0.00001512
Iteration 168/1000 | Loss: 0.00001512
Iteration 169/1000 | Loss: 0.00001512
Iteration 170/1000 | Loss: 0.00001512
Iteration 171/1000 | Loss: 0.00001512
Iteration 172/1000 | Loss: 0.00001512
Iteration 173/1000 | Loss: 0.00001511
Iteration 174/1000 | Loss: 0.00001510
Iteration 175/1000 | Loss: 0.00015910
Iteration 176/1000 | Loss: 0.00008395
Iteration 177/1000 | Loss: 0.00022942
Iteration 178/1000 | Loss: 0.00008239
Iteration 179/1000 | Loss: 0.00001849
Iteration 180/1000 | Loss: 0.00015718
Iteration 181/1000 | Loss: 0.00021820
Iteration 182/1000 | Loss: 0.00002949
Iteration 183/1000 | Loss: 0.00004811
Iteration 184/1000 | Loss: 0.00016578
Iteration 185/1000 | Loss: 0.00002571
Iteration 186/1000 | Loss: 0.00002095
Iteration 187/1000 | Loss: 0.00001673
Iteration 188/1000 | Loss: 0.00001577
Iteration 189/1000 | Loss: 0.00005018
Iteration 190/1000 | Loss: 0.00001887
Iteration 191/1000 | Loss: 0.00001483
Iteration 192/1000 | Loss: 0.00002527
Iteration 193/1000 | Loss: 0.00002746
Iteration 194/1000 | Loss: 0.00001701
Iteration 195/1000 | Loss: 0.00001472
Iteration 196/1000 | Loss: 0.00001426
Iteration 197/1000 | Loss: 0.00001426
Iteration 198/1000 | Loss: 0.00001426
Iteration 199/1000 | Loss: 0.00001425
Iteration 200/1000 | Loss: 0.00001425
Iteration 201/1000 | Loss: 0.00007904
Iteration 202/1000 | Loss: 0.00007904
Iteration 203/1000 | Loss: 0.00001508
Iteration 204/1000 | Loss: 0.00001422
Iteration 205/1000 | Loss: 0.00001414
Iteration 206/1000 | Loss: 0.00001413
Iteration 207/1000 | Loss: 0.00001412
Iteration 208/1000 | Loss: 0.00001412
Iteration 209/1000 | Loss: 0.00001411
Iteration 210/1000 | Loss: 0.00001411
Iteration 211/1000 | Loss: 0.00001411
Iteration 212/1000 | Loss: 0.00001410
Iteration 213/1000 | Loss: 0.00001410
Iteration 214/1000 | Loss: 0.00001410
Iteration 215/1000 | Loss: 0.00001409
Iteration 216/1000 | Loss: 0.00001409
Iteration 217/1000 | Loss: 0.00001408
Iteration 218/1000 | Loss: 0.00001408
Iteration 219/1000 | Loss: 0.00001408
Iteration 220/1000 | Loss: 0.00001408
Iteration 221/1000 | Loss: 0.00001408
Iteration 222/1000 | Loss: 0.00001407
Iteration 223/1000 | Loss: 0.00001407
Iteration 224/1000 | Loss: 0.00001407
Iteration 225/1000 | Loss: 0.00001407
Iteration 226/1000 | Loss: 0.00001407
Iteration 227/1000 | Loss: 0.00001406
Iteration 228/1000 | Loss: 0.00001406
Iteration 229/1000 | Loss: 0.00001406
Iteration 230/1000 | Loss: 0.00001406
Iteration 231/1000 | Loss: 0.00001406
Iteration 232/1000 | Loss: 0.00001406
Iteration 233/1000 | Loss: 0.00001406
Iteration 234/1000 | Loss: 0.00001406
Iteration 235/1000 | Loss: 0.00001406
Iteration 236/1000 | Loss: 0.00001406
Iteration 237/1000 | Loss: 0.00001406
Iteration 238/1000 | Loss: 0.00001405
Iteration 239/1000 | Loss: 0.00001405
Iteration 240/1000 | Loss: 0.00001405
Iteration 241/1000 | Loss: 0.00001405
Iteration 242/1000 | Loss: 0.00001405
Iteration 243/1000 | Loss: 0.00001405
Iteration 244/1000 | Loss: 0.00001405
Iteration 245/1000 | Loss: 0.00001405
Iteration 246/1000 | Loss: 0.00001405
Iteration 247/1000 | Loss: 0.00001405
Iteration 248/1000 | Loss: 0.00001405
Iteration 249/1000 | Loss: 0.00001405
Iteration 250/1000 | Loss: 0.00001405
Iteration 251/1000 | Loss: 0.00001404
Iteration 252/1000 | Loss: 0.00001404
Iteration 253/1000 | Loss: 0.00001404
Iteration 254/1000 | Loss: 0.00001404
Iteration 255/1000 | Loss: 0.00001404
Iteration 256/1000 | Loss: 0.00001404
Iteration 257/1000 | Loss: 0.00001404
Iteration 258/1000 | Loss: 0.00001404
Iteration 259/1000 | Loss: 0.00001404
Iteration 260/1000 | Loss: 0.00001404
Iteration 261/1000 | Loss: 0.00001404
Iteration 262/1000 | Loss: 0.00001404
Iteration 263/1000 | Loss: 0.00001404
Iteration 264/1000 | Loss: 0.00001404
Iteration 265/1000 | Loss: 0.00001404
Iteration 266/1000 | Loss: 0.00001404
Iteration 267/1000 | Loss: 0.00001403
Iteration 268/1000 | Loss: 0.00001403
Iteration 269/1000 | Loss: 0.00001403
Iteration 270/1000 | Loss: 0.00001403
Iteration 271/1000 | Loss: 0.00001403
Iteration 272/1000 | Loss: 0.00001403
Iteration 273/1000 | Loss: 0.00001403
Iteration 274/1000 | Loss: 0.00001403
Iteration 275/1000 | Loss: 0.00001402
Iteration 276/1000 | Loss: 0.00001402
Iteration 277/1000 | Loss: 0.00001402
Iteration 278/1000 | Loss: 0.00001402
Iteration 279/1000 | Loss: 0.00001402
Iteration 280/1000 | Loss: 0.00001402
Iteration 281/1000 | Loss: 0.00001402
Iteration 282/1000 | Loss: 0.00001402
Iteration 283/1000 | Loss: 0.00001402
Iteration 284/1000 | Loss: 0.00001402
Iteration 285/1000 | Loss: 0.00001402
Iteration 286/1000 | Loss: 0.00001402
Iteration 287/1000 | Loss: 0.00001401
Iteration 288/1000 | Loss: 0.00001401
Iteration 289/1000 | Loss: 0.00001401
Iteration 290/1000 | Loss: 0.00001401
Iteration 291/1000 | Loss: 0.00001401
Iteration 292/1000 | Loss: 0.00001401
Iteration 293/1000 | Loss: 0.00001401
Iteration 294/1000 | Loss: 0.00001401
Iteration 295/1000 | Loss: 0.00001401
Iteration 296/1000 | Loss: 0.00001401
Iteration 297/1000 | Loss: 0.00001401
Iteration 298/1000 | Loss: 0.00001401
Iteration 299/1000 | Loss: 0.00001401
Iteration 300/1000 | Loss: 0.00001401
Iteration 301/1000 | Loss: 0.00001401
Iteration 302/1000 | Loss: 0.00001401
Iteration 303/1000 | Loss: 0.00001401
Iteration 304/1000 | Loss: 0.00001401
Iteration 305/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [1.4012681276653893e-05, 1.4012681276653893e-05, 1.4012681276653893e-05, 1.4012681276653893e-05, 1.4012681276653893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4012681276653893e-05

Optimization complete. Final v2v error: 3.2430527210235596 mm

Highest mean error: 4.118781566619873 mm for frame 71

Lowest mean error: 2.8181488513946533 mm for frame 125

Saving results

Total time: 154.37523078918457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750727
Iteration 2/25 | Loss: 0.00178584
Iteration 3/25 | Loss: 0.00144706
Iteration 4/25 | Loss: 0.00141746
Iteration 5/25 | Loss: 0.00141360
Iteration 6/25 | Loss: 0.00141356
Iteration 7/25 | Loss: 0.00141356
Iteration 8/25 | Loss: 0.00141356
Iteration 9/25 | Loss: 0.00141356
Iteration 10/25 | Loss: 0.00141356
Iteration 11/25 | Loss: 0.00141356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014135614037513733, 0.0014135614037513733, 0.0014135614037513733, 0.0014135614037513733, 0.0014135614037513733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014135614037513733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11887777
Iteration 2/25 | Loss: 0.00181392
Iteration 3/25 | Loss: 0.00181392
Iteration 4/25 | Loss: 0.00181392
Iteration 5/25 | Loss: 0.00181392
Iteration 6/25 | Loss: 0.00181392
Iteration 7/25 | Loss: 0.00181392
Iteration 8/25 | Loss: 0.00181392
Iteration 9/25 | Loss: 0.00181392
Iteration 10/25 | Loss: 0.00181392
Iteration 11/25 | Loss: 0.00181392
Iteration 12/25 | Loss: 0.00181392
Iteration 13/25 | Loss: 0.00181392
Iteration 14/25 | Loss: 0.00181392
Iteration 15/25 | Loss: 0.00181392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018139188177883625, 0.0018139188177883625, 0.0018139188177883625, 0.0018139188177883625, 0.0018139188177883625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018139188177883625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181392
Iteration 2/1000 | Loss: 0.00004761
Iteration 3/1000 | Loss: 0.00003378
Iteration 4/1000 | Loss: 0.00002927
Iteration 5/1000 | Loss: 0.00002705
Iteration 6/1000 | Loss: 0.00002568
Iteration 7/1000 | Loss: 0.00002468
Iteration 8/1000 | Loss: 0.00002399
Iteration 9/1000 | Loss: 0.00002325
Iteration 10/1000 | Loss: 0.00002272
Iteration 11/1000 | Loss: 0.00002242
Iteration 12/1000 | Loss: 0.00002200
Iteration 13/1000 | Loss: 0.00002175
Iteration 14/1000 | Loss: 0.00002150
Iteration 15/1000 | Loss: 0.00002140
Iteration 16/1000 | Loss: 0.00002136
Iteration 17/1000 | Loss: 0.00002120
Iteration 18/1000 | Loss: 0.00002110
Iteration 19/1000 | Loss: 0.00002105
Iteration 20/1000 | Loss: 0.00002096
Iteration 21/1000 | Loss: 0.00002089
Iteration 22/1000 | Loss: 0.00002087
Iteration 23/1000 | Loss: 0.00002086
Iteration 24/1000 | Loss: 0.00002086
Iteration 25/1000 | Loss: 0.00002085
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002082
Iteration 28/1000 | Loss: 0.00002082
Iteration 29/1000 | Loss: 0.00002081
Iteration 30/1000 | Loss: 0.00002081
Iteration 31/1000 | Loss: 0.00002080
Iteration 32/1000 | Loss: 0.00002078
Iteration 33/1000 | Loss: 0.00002075
Iteration 34/1000 | Loss: 0.00002071
Iteration 35/1000 | Loss: 0.00002067
Iteration 36/1000 | Loss: 0.00002064
Iteration 37/1000 | Loss: 0.00002063
Iteration 38/1000 | Loss: 0.00002062
Iteration 39/1000 | Loss: 0.00002062
Iteration 40/1000 | Loss: 0.00002062
Iteration 41/1000 | Loss: 0.00002062
Iteration 42/1000 | Loss: 0.00002060
Iteration 43/1000 | Loss: 0.00002060
Iteration 44/1000 | Loss: 0.00002059
Iteration 45/1000 | Loss: 0.00002058
Iteration 46/1000 | Loss: 0.00002057
Iteration 47/1000 | Loss: 0.00002057
Iteration 48/1000 | Loss: 0.00002053
Iteration 49/1000 | Loss: 0.00002053
Iteration 50/1000 | Loss: 0.00002050
Iteration 51/1000 | Loss: 0.00002050
Iteration 52/1000 | Loss: 0.00002047
Iteration 53/1000 | Loss: 0.00002044
Iteration 54/1000 | Loss: 0.00002043
Iteration 55/1000 | Loss: 0.00002043
Iteration 56/1000 | Loss: 0.00002041
Iteration 57/1000 | Loss: 0.00002040
Iteration 58/1000 | Loss: 0.00002040
Iteration 59/1000 | Loss: 0.00002040
Iteration 60/1000 | Loss: 0.00002040
Iteration 61/1000 | Loss: 0.00002040
Iteration 62/1000 | Loss: 0.00002040
Iteration 63/1000 | Loss: 0.00002040
Iteration 64/1000 | Loss: 0.00002040
Iteration 65/1000 | Loss: 0.00002040
Iteration 66/1000 | Loss: 0.00002039
Iteration 67/1000 | Loss: 0.00002039
Iteration 68/1000 | Loss: 0.00002039
Iteration 69/1000 | Loss: 0.00002039
Iteration 70/1000 | Loss: 0.00002039
Iteration 71/1000 | Loss: 0.00002038
Iteration 72/1000 | Loss: 0.00002038
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002036
Iteration 79/1000 | Loss: 0.00002036
Iteration 80/1000 | Loss: 0.00002036
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00002035
Iteration 83/1000 | Loss: 0.00002035
Iteration 84/1000 | Loss: 0.00002035
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002034
Iteration 87/1000 | Loss: 0.00002034
Iteration 88/1000 | Loss: 0.00002034
Iteration 89/1000 | Loss: 0.00002034
Iteration 90/1000 | Loss: 0.00002034
Iteration 91/1000 | Loss: 0.00002034
Iteration 92/1000 | Loss: 0.00002034
Iteration 93/1000 | Loss: 0.00002033
Iteration 94/1000 | Loss: 0.00002033
Iteration 95/1000 | Loss: 0.00002033
Iteration 96/1000 | Loss: 0.00002033
Iteration 97/1000 | Loss: 0.00002033
Iteration 98/1000 | Loss: 0.00002033
Iteration 99/1000 | Loss: 0.00002033
Iteration 100/1000 | Loss: 0.00002033
Iteration 101/1000 | Loss: 0.00002033
Iteration 102/1000 | Loss: 0.00002033
Iteration 103/1000 | Loss: 0.00002033
Iteration 104/1000 | Loss: 0.00002033
Iteration 105/1000 | Loss: 0.00002033
Iteration 106/1000 | Loss: 0.00002032
Iteration 107/1000 | Loss: 0.00002032
Iteration 108/1000 | Loss: 0.00002032
Iteration 109/1000 | Loss: 0.00002032
Iteration 110/1000 | Loss: 0.00002032
Iteration 111/1000 | Loss: 0.00002032
Iteration 112/1000 | Loss: 0.00002032
Iteration 113/1000 | Loss: 0.00002031
Iteration 114/1000 | Loss: 0.00002031
Iteration 115/1000 | Loss: 0.00002031
Iteration 116/1000 | Loss: 0.00002031
Iteration 117/1000 | Loss: 0.00002031
Iteration 118/1000 | Loss: 0.00002031
Iteration 119/1000 | Loss: 0.00002031
Iteration 120/1000 | Loss: 0.00002031
Iteration 121/1000 | Loss: 0.00002030
Iteration 122/1000 | Loss: 0.00002030
Iteration 123/1000 | Loss: 0.00002030
Iteration 124/1000 | Loss: 0.00002029
Iteration 125/1000 | Loss: 0.00002029
Iteration 126/1000 | Loss: 0.00002029
Iteration 127/1000 | Loss: 0.00002029
Iteration 128/1000 | Loss: 0.00002028
Iteration 129/1000 | Loss: 0.00002028
Iteration 130/1000 | Loss: 0.00002028
Iteration 131/1000 | Loss: 0.00002027
Iteration 132/1000 | Loss: 0.00002027
Iteration 133/1000 | Loss: 0.00002027
Iteration 134/1000 | Loss: 0.00002026
Iteration 135/1000 | Loss: 0.00002026
Iteration 136/1000 | Loss: 0.00002026
Iteration 137/1000 | Loss: 0.00002025
Iteration 138/1000 | Loss: 0.00002025
Iteration 139/1000 | Loss: 0.00002025
Iteration 140/1000 | Loss: 0.00002025
Iteration 141/1000 | Loss: 0.00002025
Iteration 142/1000 | Loss: 0.00002025
Iteration 143/1000 | Loss: 0.00002025
Iteration 144/1000 | Loss: 0.00002025
Iteration 145/1000 | Loss: 0.00002025
Iteration 146/1000 | Loss: 0.00002025
Iteration 147/1000 | Loss: 0.00002025
Iteration 148/1000 | Loss: 0.00002025
Iteration 149/1000 | Loss: 0.00002025
Iteration 150/1000 | Loss: 0.00002024
Iteration 151/1000 | Loss: 0.00002024
Iteration 152/1000 | Loss: 0.00002024
Iteration 153/1000 | Loss: 0.00002023
Iteration 154/1000 | Loss: 0.00002023
Iteration 155/1000 | Loss: 0.00002023
Iteration 156/1000 | Loss: 0.00002023
Iteration 157/1000 | Loss: 0.00002023
Iteration 158/1000 | Loss: 0.00002023
Iteration 159/1000 | Loss: 0.00002023
Iteration 160/1000 | Loss: 0.00002023
Iteration 161/1000 | Loss: 0.00002023
Iteration 162/1000 | Loss: 0.00002023
Iteration 163/1000 | Loss: 0.00002023
Iteration 164/1000 | Loss: 0.00002023
Iteration 165/1000 | Loss: 0.00002023
Iteration 166/1000 | Loss: 0.00002023
Iteration 167/1000 | Loss: 0.00002023
Iteration 168/1000 | Loss: 0.00002023
Iteration 169/1000 | Loss: 0.00002023
Iteration 170/1000 | Loss: 0.00002023
Iteration 171/1000 | Loss: 0.00002023
Iteration 172/1000 | Loss: 0.00002023
Iteration 173/1000 | Loss: 0.00002023
Iteration 174/1000 | Loss: 0.00002023
Iteration 175/1000 | Loss: 0.00002023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.022544140345417e-05, 2.022544140345417e-05, 2.022544140345417e-05, 2.022544140345417e-05, 2.022544140345417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.022544140345417e-05

Optimization complete. Final v2v error: 3.772099018096924 mm

Highest mean error: 4.591836452484131 mm for frame 44

Lowest mean error: 3.263047695159912 mm for frame 181

Saving results

Total time: 47.227030515670776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877980
Iteration 2/25 | Loss: 0.00308321
Iteration 3/25 | Loss: 0.00216668
Iteration 4/25 | Loss: 0.00185816
Iteration 5/25 | Loss: 0.00190023
Iteration 6/25 | Loss: 0.00182949
Iteration 7/25 | Loss: 0.00178437
Iteration 8/25 | Loss: 0.00178778
Iteration 9/25 | Loss: 0.00172257
Iteration 10/25 | Loss: 0.00169594
Iteration 11/25 | Loss: 0.00170252
Iteration 12/25 | Loss: 0.00171087
Iteration 13/25 | Loss: 0.00170492
Iteration 14/25 | Loss: 0.00169635
Iteration 15/25 | Loss: 0.00169460
Iteration 16/25 | Loss: 0.00168197
Iteration 17/25 | Loss: 0.00166303
Iteration 18/25 | Loss: 0.00166158
Iteration 19/25 | Loss: 0.00166778
Iteration 20/25 | Loss: 0.00166665
Iteration 21/25 | Loss: 0.00166088
Iteration 22/25 | Loss: 0.00165276
Iteration 23/25 | Loss: 0.00165219
Iteration 24/25 | Loss: 0.00165130
Iteration 25/25 | Loss: 0.00164675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28355122
Iteration 2/25 | Loss: 0.00489253
Iteration 3/25 | Loss: 0.00429642
Iteration 4/25 | Loss: 0.00429642
Iteration 5/25 | Loss: 0.00429642
Iteration 6/25 | Loss: 0.00429642
Iteration 7/25 | Loss: 0.00429642
Iteration 8/25 | Loss: 0.00429641
Iteration 9/25 | Loss: 0.00429641
Iteration 10/25 | Loss: 0.00429641
Iteration 11/25 | Loss: 0.00429641
Iteration 12/25 | Loss: 0.00429641
Iteration 13/25 | Loss: 0.00429641
Iteration 14/25 | Loss: 0.00429641
Iteration 15/25 | Loss: 0.00429641
Iteration 16/25 | Loss: 0.00429641
Iteration 17/25 | Loss: 0.00429641
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0042964136227965355, 0.0042964136227965355, 0.0042964136227965355, 0.0042964136227965355, 0.0042964136227965355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0042964136227965355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00429641
Iteration 2/1000 | Loss: 0.00128022
Iteration 3/1000 | Loss: 0.00144813
Iteration 4/1000 | Loss: 0.00190564
Iteration 5/1000 | Loss: 0.00150947
Iteration 6/1000 | Loss: 0.00264770
Iteration 7/1000 | Loss: 0.00026096
Iteration 8/1000 | Loss: 0.00444244
Iteration 9/1000 | Loss: 0.00125878
Iteration 10/1000 | Loss: 0.00153910
Iteration 11/1000 | Loss: 0.00084008
Iteration 12/1000 | Loss: 0.00121871
Iteration 13/1000 | Loss: 0.00015233
Iteration 14/1000 | Loss: 0.00153791
Iteration 15/1000 | Loss: 0.00036498
Iteration 16/1000 | Loss: 0.00038435
Iteration 17/1000 | Loss: 0.00041470
Iteration 18/1000 | Loss: 0.00068828
Iteration 19/1000 | Loss: 0.00011303
Iteration 20/1000 | Loss: 0.00060835
Iteration 21/1000 | Loss: 0.00008455
Iteration 22/1000 | Loss: 0.00007295
Iteration 23/1000 | Loss: 0.00106570
Iteration 24/1000 | Loss: 0.00028527
Iteration 25/1000 | Loss: 0.00079201
Iteration 26/1000 | Loss: 0.00033335
Iteration 27/1000 | Loss: 0.00006591
Iteration 28/1000 | Loss: 0.00005854
Iteration 29/1000 | Loss: 0.00005395
Iteration 30/1000 | Loss: 0.00005133
Iteration 31/1000 | Loss: 0.00004803
Iteration 32/1000 | Loss: 0.00004577
Iteration 33/1000 | Loss: 0.00026949
Iteration 34/1000 | Loss: 0.00023109
Iteration 35/1000 | Loss: 0.00004270
Iteration 36/1000 | Loss: 0.00004163
Iteration 37/1000 | Loss: 0.00004089
Iteration 38/1000 | Loss: 0.00004374
Iteration 39/1000 | Loss: 0.00004033
Iteration 40/1000 | Loss: 0.00003967
Iteration 41/1000 | Loss: 0.00095537
Iteration 42/1000 | Loss: 0.00008632
Iteration 43/1000 | Loss: 0.00023785
Iteration 44/1000 | Loss: 0.00014673
Iteration 45/1000 | Loss: 0.00019194
Iteration 46/1000 | Loss: 0.00014680
Iteration 47/1000 | Loss: 0.00014156
Iteration 48/1000 | Loss: 0.00004644
Iteration 49/1000 | Loss: 0.00007139
Iteration 50/1000 | Loss: 0.00003903
Iteration 51/1000 | Loss: 0.00005103
Iteration 52/1000 | Loss: 0.00003614
Iteration 53/1000 | Loss: 0.00010516
Iteration 54/1000 | Loss: 0.00003519
Iteration 55/1000 | Loss: 0.00024859
Iteration 56/1000 | Loss: 0.00043831
Iteration 57/1000 | Loss: 0.00061904
Iteration 58/1000 | Loss: 0.00075866
Iteration 59/1000 | Loss: 0.00137623
Iteration 60/1000 | Loss: 0.00004641
Iteration 61/1000 | Loss: 0.00030878
Iteration 62/1000 | Loss: 0.00003735
Iteration 63/1000 | Loss: 0.00008926
Iteration 64/1000 | Loss: 0.00003367
Iteration 65/1000 | Loss: 0.00005280
Iteration 66/1000 | Loss: 0.00003225
Iteration 67/1000 | Loss: 0.00003155
Iteration 68/1000 | Loss: 0.00003110
Iteration 69/1000 | Loss: 0.00025625
Iteration 70/1000 | Loss: 0.00005602
Iteration 71/1000 | Loss: 0.00004774
Iteration 72/1000 | Loss: 0.00003506
Iteration 73/1000 | Loss: 0.00003366
Iteration 74/1000 | Loss: 0.00028395
Iteration 75/1000 | Loss: 0.00020702
Iteration 76/1000 | Loss: 0.00005829
Iteration 77/1000 | Loss: 0.00028840
Iteration 78/1000 | Loss: 0.00019692
Iteration 79/1000 | Loss: 0.00035104
Iteration 80/1000 | Loss: 0.00018458
Iteration 81/1000 | Loss: 0.00032708
Iteration 82/1000 | Loss: 0.00017986
Iteration 83/1000 | Loss: 0.00030893
Iteration 84/1000 | Loss: 0.00014257
Iteration 85/1000 | Loss: 0.00026313
Iteration 86/1000 | Loss: 0.00043138
Iteration 87/1000 | Loss: 0.00044985
Iteration 88/1000 | Loss: 0.00053613
Iteration 89/1000 | Loss: 0.00022559
Iteration 90/1000 | Loss: 0.00004909
Iteration 91/1000 | Loss: 0.00004380
Iteration 92/1000 | Loss: 0.00013167
Iteration 93/1000 | Loss: 0.00004032
Iteration 94/1000 | Loss: 0.00003881
Iteration 95/1000 | Loss: 0.00003743
Iteration 96/1000 | Loss: 0.00003651
Iteration 97/1000 | Loss: 0.00003589
Iteration 98/1000 | Loss: 0.00039963
Iteration 99/1000 | Loss: 0.00009951
Iteration 100/1000 | Loss: 0.00004009
Iteration 101/1000 | Loss: 0.00003642
Iteration 102/1000 | Loss: 0.00008299
Iteration 103/1000 | Loss: 0.00003346
Iteration 104/1000 | Loss: 0.00003240
Iteration 105/1000 | Loss: 0.00003183
Iteration 106/1000 | Loss: 0.00049031
Iteration 107/1000 | Loss: 0.00003469
Iteration 108/1000 | Loss: 0.00003168
Iteration 109/1000 | Loss: 0.00003008
Iteration 110/1000 | Loss: 0.00002898
Iteration 111/1000 | Loss: 0.00002826
Iteration 112/1000 | Loss: 0.00002771
Iteration 113/1000 | Loss: 0.00002721
Iteration 114/1000 | Loss: 0.00002689
Iteration 115/1000 | Loss: 0.00002660
Iteration 116/1000 | Loss: 0.00002626
Iteration 117/1000 | Loss: 0.00002601
Iteration 118/1000 | Loss: 0.00002589
Iteration 119/1000 | Loss: 0.00002588
Iteration 120/1000 | Loss: 0.00002575
Iteration 121/1000 | Loss: 0.00002561
Iteration 122/1000 | Loss: 0.00002546
Iteration 123/1000 | Loss: 0.00002541
Iteration 124/1000 | Loss: 0.00002540
Iteration 125/1000 | Loss: 0.00002540
Iteration 126/1000 | Loss: 0.00002540
Iteration 127/1000 | Loss: 0.00002540
Iteration 128/1000 | Loss: 0.00002540
Iteration 129/1000 | Loss: 0.00002540
Iteration 130/1000 | Loss: 0.00002540
Iteration 131/1000 | Loss: 0.00002540
Iteration 132/1000 | Loss: 0.00002540
Iteration 133/1000 | Loss: 0.00002540
Iteration 134/1000 | Loss: 0.00002540
Iteration 135/1000 | Loss: 0.00002540
Iteration 136/1000 | Loss: 0.00002539
Iteration 137/1000 | Loss: 0.00002539
Iteration 138/1000 | Loss: 0.00002539
Iteration 139/1000 | Loss: 0.00002539
Iteration 140/1000 | Loss: 0.00002539
Iteration 141/1000 | Loss: 0.00002539
Iteration 142/1000 | Loss: 0.00002539
Iteration 143/1000 | Loss: 0.00002538
Iteration 144/1000 | Loss: 0.00002538
Iteration 145/1000 | Loss: 0.00002537
Iteration 146/1000 | Loss: 0.00002537
Iteration 147/1000 | Loss: 0.00002537
Iteration 148/1000 | Loss: 0.00002536
Iteration 149/1000 | Loss: 0.00002536
Iteration 150/1000 | Loss: 0.00002536
Iteration 151/1000 | Loss: 0.00002535
Iteration 152/1000 | Loss: 0.00002535
Iteration 153/1000 | Loss: 0.00002535
Iteration 154/1000 | Loss: 0.00002534
Iteration 155/1000 | Loss: 0.00002534
Iteration 156/1000 | Loss: 0.00002533
Iteration 157/1000 | Loss: 0.00002533
Iteration 158/1000 | Loss: 0.00002532
Iteration 159/1000 | Loss: 0.00002532
Iteration 160/1000 | Loss: 0.00002532
Iteration 161/1000 | Loss: 0.00002530
Iteration 162/1000 | Loss: 0.00002530
Iteration 163/1000 | Loss: 0.00002530
Iteration 164/1000 | Loss: 0.00002530
Iteration 165/1000 | Loss: 0.00002530
Iteration 166/1000 | Loss: 0.00002530
Iteration 167/1000 | Loss: 0.00002530
Iteration 168/1000 | Loss: 0.00002530
Iteration 169/1000 | Loss: 0.00002529
Iteration 170/1000 | Loss: 0.00002529
Iteration 171/1000 | Loss: 0.00002529
Iteration 172/1000 | Loss: 0.00002528
Iteration 173/1000 | Loss: 0.00002528
Iteration 174/1000 | Loss: 0.00002528
Iteration 175/1000 | Loss: 0.00002528
Iteration 176/1000 | Loss: 0.00002528
Iteration 177/1000 | Loss: 0.00002528
Iteration 178/1000 | Loss: 0.00002527
Iteration 179/1000 | Loss: 0.00002527
Iteration 180/1000 | Loss: 0.00002527
Iteration 181/1000 | Loss: 0.00002527
Iteration 182/1000 | Loss: 0.00002527
Iteration 183/1000 | Loss: 0.00002526
Iteration 184/1000 | Loss: 0.00002526
Iteration 185/1000 | Loss: 0.00002526
Iteration 186/1000 | Loss: 0.00002526
Iteration 187/1000 | Loss: 0.00002526
Iteration 188/1000 | Loss: 0.00002525
Iteration 189/1000 | Loss: 0.00002525
Iteration 190/1000 | Loss: 0.00002525
Iteration 191/1000 | Loss: 0.00002524
Iteration 192/1000 | Loss: 0.00002524
Iteration 193/1000 | Loss: 0.00002524
Iteration 194/1000 | Loss: 0.00002524
Iteration 195/1000 | Loss: 0.00002524
Iteration 196/1000 | Loss: 0.00002523
Iteration 197/1000 | Loss: 0.00002523
Iteration 198/1000 | Loss: 0.00002523
Iteration 199/1000 | Loss: 0.00002523
Iteration 200/1000 | Loss: 0.00002523
Iteration 201/1000 | Loss: 0.00002523
Iteration 202/1000 | Loss: 0.00002523
Iteration 203/1000 | Loss: 0.00002523
Iteration 204/1000 | Loss: 0.00002523
Iteration 205/1000 | Loss: 0.00002523
Iteration 206/1000 | Loss: 0.00002523
Iteration 207/1000 | Loss: 0.00002523
Iteration 208/1000 | Loss: 0.00002523
Iteration 209/1000 | Loss: 0.00002523
Iteration 210/1000 | Loss: 0.00002523
Iteration 211/1000 | Loss: 0.00002523
Iteration 212/1000 | Loss: 0.00002523
Iteration 213/1000 | Loss: 0.00002523
Iteration 214/1000 | Loss: 0.00002523
Iteration 215/1000 | Loss: 0.00002523
Iteration 216/1000 | Loss: 0.00002523
Iteration 217/1000 | Loss: 0.00002523
Iteration 218/1000 | Loss: 0.00002523
Iteration 219/1000 | Loss: 0.00002523
Iteration 220/1000 | Loss: 0.00002523
Iteration 221/1000 | Loss: 0.00002523
Iteration 222/1000 | Loss: 0.00002523
Iteration 223/1000 | Loss: 0.00002523
Iteration 224/1000 | Loss: 0.00002523
Iteration 225/1000 | Loss: 0.00002523
Iteration 226/1000 | Loss: 0.00002523
Iteration 227/1000 | Loss: 0.00002523
Iteration 228/1000 | Loss: 0.00002523
Iteration 229/1000 | Loss: 0.00002523
Iteration 230/1000 | Loss: 0.00002523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [2.5226850993931293e-05, 2.5226850993931293e-05, 2.5226850993931293e-05, 2.5226850993931293e-05, 2.5226850993931293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5226850993931293e-05

Optimization complete. Final v2v error: 4.02174711227417 mm

Highest mean error: 7.789391994476318 mm for frame 58

Lowest mean error: 3.136439085006714 mm for frame 4

Saving results

Total time: 221.00069904327393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389194
Iteration 2/25 | Loss: 0.00140682
Iteration 3/25 | Loss: 0.00133571
Iteration 4/25 | Loss: 0.00132866
Iteration 5/25 | Loss: 0.00132724
Iteration 6/25 | Loss: 0.00132724
Iteration 7/25 | Loss: 0.00132724
Iteration 8/25 | Loss: 0.00132724
Iteration 9/25 | Loss: 0.00132724
Iteration 10/25 | Loss: 0.00132724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013272393262013793, 0.0013272393262013793, 0.0013272393262013793, 0.0013272393262013793, 0.0013272393262013793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013272393262013793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26540196
Iteration 2/25 | Loss: 0.00204390
Iteration 3/25 | Loss: 0.00204390
Iteration 4/25 | Loss: 0.00204390
Iteration 5/25 | Loss: 0.00204390
Iteration 6/25 | Loss: 0.00204390
Iteration 7/25 | Loss: 0.00204390
Iteration 8/25 | Loss: 0.00204390
Iteration 9/25 | Loss: 0.00204389
Iteration 10/25 | Loss: 0.00204389
Iteration 11/25 | Loss: 0.00204389
Iteration 12/25 | Loss: 0.00204389
Iteration 13/25 | Loss: 0.00204389
Iteration 14/25 | Loss: 0.00204389
Iteration 15/25 | Loss: 0.00204389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002043894724920392, 0.002043894724920392, 0.002043894724920392, 0.002043894724920392, 0.002043894724920392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002043894724920392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204389
Iteration 2/1000 | Loss: 0.00002458
Iteration 3/1000 | Loss: 0.00001724
Iteration 4/1000 | Loss: 0.00001553
Iteration 5/1000 | Loss: 0.00001463
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001343
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001267
Iteration 12/1000 | Loss: 0.00001246
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001197
Iteration 21/1000 | Loss: 0.00001194
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001181
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001167
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001164
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001162
Iteration 61/1000 | Loss: 0.00001162
Iteration 62/1000 | Loss: 0.00001161
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001160
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001156
Iteration 89/1000 | Loss: 0.00001156
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001155
Iteration 92/1000 | Loss: 0.00001155
Iteration 93/1000 | Loss: 0.00001155
Iteration 94/1000 | Loss: 0.00001154
Iteration 95/1000 | Loss: 0.00001154
Iteration 96/1000 | Loss: 0.00001154
Iteration 97/1000 | Loss: 0.00001154
Iteration 98/1000 | Loss: 0.00001154
Iteration 99/1000 | Loss: 0.00001154
Iteration 100/1000 | Loss: 0.00001154
Iteration 101/1000 | Loss: 0.00001154
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001152
Iteration 110/1000 | Loss: 0.00001152
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001152
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001152
Iteration 121/1000 | Loss: 0.00001152
Iteration 122/1000 | Loss: 0.00001152
Iteration 123/1000 | Loss: 0.00001152
Iteration 124/1000 | Loss: 0.00001152
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001152
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001151
Iteration 135/1000 | Loss: 0.00001151
Iteration 136/1000 | Loss: 0.00001151
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001150
Iteration 139/1000 | Loss: 0.00001150
Iteration 140/1000 | Loss: 0.00001150
Iteration 141/1000 | Loss: 0.00001149
Iteration 142/1000 | Loss: 0.00001149
Iteration 143/1000 | Loss: 0.00001149
Iteration 144/1000 | Loss: 0.00001149
Iteration 145/1000 | Loss: 0.00001149
Iteration 146/1000 | Loss: 0.00001148
Iteration 147/1000 | Loss: 0.00001148
Iteration 148/1000 | Loss: 0.00001148
Iteration 149/1000 | Loss: 0.00001147
Iteration 150/1000 | Loss: 0.00001147
Iteration 151/1000 | Loss: 0.00001147
Iteration 152/1000 | Loss: 0.00001147
Iteration 153/1000 | Loss: 0.00001147
Iteration 154/1000 | Loss: 0.00001147
Iteration 155/1000 | Loss: 0.00001146
Iteration 156/1000 | Loss: 0.00001146
Iteration 157/1000 | Loss: 0.00001145
Iteration 158/1000 | Loss: 0.00001144
Iteration 159/1000 | Loss: 0.00001144
Iteration 160/1000 | Loss: 0.00001144
Iteration 161/1000 | Loss: 0.00001144
Iteration 162/1000 | Loss: 0.00001144
Iteration 163/1000 | Loss: 0.00001144
Iteration 164/1000 | Loss: 0.00001144
Iteration 165/1000 | Loss: 0.00001144
Iteration 166/1000 | Loss: 0.00001144
Iteration 167/1000 | Loss: 0.00001143
Iteration 168/1000 | Loss: 0.00001143
Iteration 169/1000 | Loss: 0.00001143
Iteration 170/1000 | Loss: 0.00001143
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001142
Iteration 173/1000 | Loss: 0.00001142
Iteration 174/1000 | Loss: 0.00001142
Iteration 175/1000 | Loss: 0.00001141
Iteration 176/1000 | Loss: 0.00001141
Iteration 177/1000 | Loss: 0.00001141
Iteration 178/1000 | Loss: 0.00001141
Iteration 179/1000 | Loss: 0.00001141
Iteration 180/1000 | Loss: 0.00001141
Iteration 181/1000 | Loss: 0.00001141
Iteration 182/1000 | Loss: 0.00001141
Iteration 183/1000 | Loss: 0.00001141
Iteration 184/1000 | Loss: 0.00001141
Iteration 185/1000 | Loss: 0.00001140
Iteration 186/1000 | Loss: 0.00001140
Iteration 187/1000 | Loss: 0.00001140
Iteration 188/1000 | Loss: 0.00001140
Iteration 189/1000 | Loss: 0.00001140
Iteration 190/1000 | Loss: 0.00001140
Iteration 191/1000 | Loss: 0.00001140
Iteration 192/1000 | Loss: 0.00001140
Iteration 193/1000 | Loss: 0.00001140
Iteration 194/1000 | Loss: 0.00001140
Iteration 195/1000 | Loss: 0.00001140
Iteration 196/1000 | Loss: 0.00001140
Iteration 197/1000 | Loss: 0.00001140
Iteration 198/1000 | Loss: 0.00001139
Iteration 199/1000 | Loss: 0.00001139
Iteration 200/1000 | Loss: 0.00001139
Iteration 201/1000 | Loss: 0.00001139
Iteration 202/1000 | Loss: 0.00001139
Iteration 203/1000 | Loss: 0.00001139
Iteration 204/1000 | Loss: 0.00001139
Iteration 205/1000 | Loss: 0.00001139
Iteration 206/1000 | Loss: 0.00001138
Iteration 207/1000 | Loss: 0.00001138
Iteration 208/1000 | Loss: 0.00001138
Iteration 209/1000 | Loss: 0.00001138
Iteration 210/1000 | Loss: 0.00001138
Iteration 211/1000 | Loss: 0.00001138
Iteration 212/1000 | Loss: 0.00001138
Iteration 213/1000 | Loss: 0.00001138
Iteration 214/1000 | Loss: 0.00001138
Iteration 215/1000 | Loss: 0.00001138
Iteration 216/1000 | Loss: 0.00001138
Iteration 217/1000 | Loss: 0.00001138
Iteration 218/1000 | Loss: 0.00001138
Iteration 219/1000 | Loss: 0.00001138
Iteration 220/1000 | Loss: 0.00001138
Iteration 221/1000 | Loss: 0.00001138
Iteration 222/1000 | Loss: 0.00001137
Iteration 223/1000 | Loss: 0.00001137
Iteration 224/1000 | Loss: 0.00001137
Iteration 225/1000 | Loss: 0.00001137
Iteration 226/1000 | Loss: 0.00001137
Iteration 227/1000 | Loss: 0.00001137
Iteration 228/1000 | Loss: 0.00001137
Iteration 229/1000 | Loss: 0.00001136
Iteration 230/1000 | Loss: 0.00001136
Iteration 231/1000 | Loss: 0.00001136
Iteration 232/1000 | Loss: 0.00001136
Iteration 233/1000 | Loss: 0.00001136
Iteration 234/1000 | Loss: 0.00001136
Iteration 235/1000 | Loss: 0.00001136
Iteration 236/1000 | Loss: 0.00001136
Iteration 237/1000 | Loss: 0.00001136
Iteration 238/1000 | Loss: 0.00001136
Iteration 239/1000 | Loss: 0.00001136
Iteration 240/1000 | Loss: 0.00001135
Iteration 241/1000 | Loss: 0.00001135
Iteration 242/1000 | Loss: 0.00001135
Iteration 243/1000 | Loss: 0.00001135
Iteration 244/1000 | Loss: 0.00001135
Iteration 245/1000 | Loss: 0.00001135
Iteration 246/1000 | Loss: 0.00001135
Iteration 247/1000 | Loss: 0.00001135
Iteration 248/1000 | Loss: 0.00001135
Iteration 249/1000 | Loss: 0.00001135
Iteration 250/1000 | Loss: 0.00001135
Iteration 251/1000 | Loss: 0.00001134
Iteration 252/1000 | Loss: 0.00001134
Iteration 253/1000 | Loss: 0.00001134
Iteration 254/1000 | Loss: 0.00001134
Iteration 255/1000 | Loss: 0.00001134
Iteration 256/1000 | Loss: 0.00001134
Iteration 257/1000 | Loss: 0.00001134
Iteration 258/1000 | Loss: 0.00001134
Iteration 259/1000 | Loss: 0.00001134
Iteration 260/1000 | Loss: 0.00001134
Iteration 261/1000 | Loss: 0.00001134
Iteration 262/1000 | Loss: 0.00001134
Iteration 263/1000 | Loss: 0.00001134
Iteration 264/1000 | Loss: 0.00001134
Iteration 265/1000 | Loss: 0.00001134
Iteration 266/1000 | Loss: 0.00001134
Iteration 267/1000 | Loss: 0.00001134
Iteration 268/1000 | Loss: 0.00001134
Iteration 269/1000 | Loss: 0.00001134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.1335788258293178e-05, 1.1335788258293178e-05, 1.1335788258293178e-05, 1.1335788258293178e-05, 1.1335788258293178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1335788258293178e-05

Optimization complete. Final v2v error: 2.8862533569335938 mm

Highest mean error: 3.0989301204681396 mm for frame 58

Lowest mean error: 2.745443105697632 mm for frame 70

Saving results

Total time: 44.32494521141052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782637
Iteration 2/25 | Loss: 0.00138820
Iteration 3/25 | Loss: 0.00131483
Iteration 4/25 | Loss: 0.00130693
Iteration 5/25 | Loss: 0.00130474
Iteration 6/25 | Loss: 0.00130474
Iteration 7/25 | Loss: 0.00130474
Iteration 8/25 | Loss: 0.00130474
Iteration 9/25 | Loss: 0.00130474
Iteration 10/25 | Loss: 0.00130474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013047442771494389, 0.0013047442771494389, 0.0013047442771494389, 0.0013047442771494389, 0.0013047442771494389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013047442771494389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24423432
Iteration 2/25 | Loss: 0.00210460
Iteration 3/25 | Loss: 0.00210460
Iteration 4/25 | Loss: 0.00210460
Iteration 5/25 | Loss: 0.00210460
Iteration 6/25 | Loss: 0.00210460
Iteration 7/25 | Loss: 0.00210460
Iteration 8/25 | Loss: 0.00210460
Iteration 9/25 | Loss: 0.00210460
Iteration 10/25 | Loss: 0.00210460
Iteration 11/25 | Loss: 0.00210460
Iteration 12/25 | Loss: 0.00210460
Iteration 13/25 | Loss: 0.00210460
Iteration 14/25 | Loss: 0.00210460
Iteration 15/25 | Loss: 0.00210460
Iteration 16/25 | Loss: 0.00210460
Iteration 17/25 | Loss: 0.00210460
Iteration 18/25 | Loss: 0.00210460
Iteration 19/25 | Loss: 0.00210460
Iteration 20/25 | Loss: 0.00210460
Iteration 21/25 | Loss: 0.00210460
Iteration 22/25 | Loss: 0.00210460
Iteration 23/25 | Loss: 0.00210460
Iteration 24/25 | Loss: 0.00210460
Iteration 25/25 | Loss: 0.00210460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210460
Iteration 2/1000 | Loss: 0.00002434
Iteration 3/1000 | Loss: 0.00001713
Iteration 4/1000 | Loss: 0.00001476
Iteration 5/1000 | Loss: 0.00001368
Iteration 6/1000 | Loss: 0.00001305
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001116
Iteration 11/1000 | Loss: 0.00001099
Iteration 12/1000 | Loss: 0.00001078
Iteration 13/1000 | Loss: 0.00001072
Iteration 14/1000 | Loss: 0.00001070
Iteration 15/1000 | Loss: 0.00001068
Iteration 16/1000 | Loss: 0.00001067
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001054
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001048
Iteration 21/1000 | Loss: 0.00001044
Iteration 22/1000 | Loss: 0.00001043
Iteration 23/1000 | Loss: 0.00001041
Iteration 24/1000 | Loss: 0.00001040
Iteration 25/1000 | Loss: 0.00001040
Iteration 26/1000 | Loss: 0.00001037
Iteration 27/1000 | Loss: 0.00001035
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001033
Iteration 30/1000 | Loss: 0.00001032
Iteration 31/1000 | Loss: 0.00001032
Iteration 32/1000 | Loss: 0.00001029
Iteration 33/1000 | Loss: 0.00001028
Iteration 34/1000 | Loss: 0.00001028
Iteration 35/1000 | Loss: 0.00001027
Iteration 36/1000 | Loss: 0.00001027
Iteration 37/1000 | Loss: 0.00001026
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001024
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001022
Iteration 44/1000 | Loss: 0.00001022
Iteration 45/1000 | Loss: 0.00001019
Iteration 46/1000 | Loss: 0.00001017
Iteration 47/1000 | Loss: 0.00001016
Iteration 48/1000 | Loss: 0.00001016
Iteration 49/1000 | Loss: 0.00001016
Iteration 50/1000 | Loss: 0.00001015
Iteration 51/1000 | Loss: 0.00001015
Iteration 52/1000 | Loss: 0.00001015
Iteration 53/1000 | Loss: 0.00001014
Iteration 54/1000 | Loss: 0.00001014
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001013
Iteration 58/1000 | Loss: 0.00001011
Iteration 59/1000 | Loss: 0.00001011
Iteration 60/1000 | Loss: 0.00001011
Iteration 61/1000 | Loss: 0.00001011
Iteration 62/1000 | Loss: 0.00001011
Iteration 63/1000 | Loss: 0.00001011
Iteration 64/1000 | Loss: 0.00001010
Iteration 65/1000 | Loss: 0.00001010
Iteration 66/1000 | Loss: 0.00001010
Iteration 67/1000 | Loss: 0.00001010
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001008
Iteration 72/1000 | Loss: 0.00001007
Iteration 73/1000 | Loss: 0.00001006
Iteration 74/1000 | Loss: 0.00001006
Iteration 75/1000 | Loss: 0.00001005
Iteration 76/1000 | Loss: 0.00001004
Iteration 77/1000 | Loss: 0.00001004
Iteration 78/1000 | Loss: 0.00001003
Iteration 79/1000 | Loss: 0.00001003
Iteration 80/1000 | Loss: 0.00001002
Iteration 81/1000 | Loss: 0.00001002
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001001
Iteration 84/1000 | Loss: 0.00001000
Iteration 85/1000 | Loss: 0.00001000
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00000999
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000999
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000998
Iteration 100/1000 | Loss: 0.00000998
Iteration 101/1000 | Loss: 0.00000998
Iteration 102/1000 | Loss: 0.00000998
Iteration 103/1000 | Loss: 0.00000998
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000997
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000997
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000996
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000996
Iteration 120/1000 | Loss: 0.00000996
Iteration 121/1000 | Loss: 0.00000995
Iteration 122/1000 | Loss: 0.00000995
Iteration 123/1000 | Loss: 0.00000994
Iteration 124/1000 | Loss: 0.00000994
Iteration 125/1000 | Loss: 0.00000994
Iteration 126/1000 | Loss: 0.00000993
Iteration 127/1000 | Loss: 0.00000993
Iteration 128/1000 | Loss: 0.00000993
Iteration 129/1000 | Loss: 0.00000993
Iteration 130/1000 | Loss: 0.00000993
Iteration 131/1000 | Loss: 0.00000992
Iteration 132/1000 | Loss: 0.00000992
Iteration 133/1000 | Loss: 0.00000992
Iteration 134/1000 | Loss: 0.00000991
Iteration 135/1000 | Loss: 0.00000991
Iteration 136/1000 | Loss: 0.00000991
Iteration 137/1000 | Loss: 0.00000991
Iteration 138/1000 | Loss: 0.00000991
Iteration 139/1000 | Loss: 0.00000991
Iteration 140/1000 | Loss: 0.00000991
Iteration 141/1000 | Loss: 0.00000991
Iteration 142/1000 | Loss: 0.00000991
Iteration 143/1000 | Loss: 0.00000991
Iteration 144/1000 | Loss: 0.00000991
Iteration 145/1000 | Loss: 0.00000991
Iteration 146/1000 | Loss: 0.00000991
Iteration 147/1000 | Loss: 0.00000991
Iteration 148/1000 | Loss: 0.00000991
Iteration 149/1000 | Loss: 0.00000991
Iteration 150/1000 | Loss: 0.00000991
Iteration 151/1000 | Loss: 0.00000991
Iteration 152/1000 | Loss: 0.00000991
Iteration 153/1000 | Loss: 0.00000991
Iteration 154/1000 | Loss: 0.00000990
Iteration 155/1000 | Loss: 0.00000990
Iteration 156/1000 | Loss: 0.00000990
Iteration 157/1000 | Loss: 0.00000990
Iteration 158/1000 | Loss: 0.00000990
Iteration 159/1000 | Loss: 0.00000990
Iteration 160/1000 | Loss: 0.00000990
Iteration 161/1000 | Loss: 0.00000990
Iteration 162/1000 | Loss: 0.00000989
Iteration 163/1000 | Loss: 0.00000989
Iteration 164/1000 | Loss: 0.00000988
Iteration 165/1000 | Loss: 0.00000988
Iteration 166/1000 | Loss: 0.00000988
Iteration 167/1000 | Loss: 0.00000988
Iteration 168/1000 | Loss: 0.00000988
Iteration 169/1000 | Loss: 0.00000988
Iteration 170/1000 | Loss: 0.00000988
Iteration 171/1000 | Loss: 0.00000987
Iteration 172/1000 | Loss: 0.00000986
Iteration 173/1000 | Loss: 0.00000986
Iteration 174/1000 | Loss: 0.00000986
Iteration 175/1000 | Loss: 0.00000985
Iteration 176/1000 | Loss: 0.00000985
Iteration 177/1000 | Loss: 0.00000985
Iteration 178/1000 | Loss: 0.00000985
Iteration 179/1000 | Loss: 0.00000985
Iteration 180/1000 | Loss: 0.00000984
Iteration 181/1000 | Loss: 0.00000984
Iteration 182/1000 | Loss: 0.00000984
Iteration 183/1000 | Loss: 0.00000984
Iteration 184/1000 | Loss: 0.00000984
Iteration 185/1000 | Loss: 0.00000984
Iteration 186/1000 | Loss: 0.00000983
Iteration 187/1000 | Loss: 0.00000983
Iteration 188/1000 | Loss: 0.00000983
Iteration 189/1000 | Loss: 0.00000983
Iteration 190/1000 | Loss: 0.00000983
Iteration 191/1000 | Loss: 0.00000983
Iteration 192/1000 | Loss: 0.00000982
Iteration 193/1000 | Loss: 0.00000982
Iteration 194/1000 | Loss: 0.00000982
Iteration 195/1000 | Loss: 0.00000982
Iteration 196/1000 | Loss: 0.00000982
Iteration 197/1000 | Loss: 0.00000982
Iteration 198/1000 | Loss: 0.00000982
Iteration 199/1000 | Loss: 0.00000982
Iteration 200/1000 | Loss: 0.00000982
Iteration 201/1000 | Loss: 0.00000982
Iteration 202/1000 | Loss: 0.00000982
Iteration 203/1000 | Loss: 0.00000982
Iteration 204/1000 | Loss: 0.00000981
Iteration 205/1000 | Loss: 0.00000981
Iteration 206/1000 | Loss: 0.00000981
Iteration 207/1000 | Loss: 0.00000981
Iteration 208/1000 | Loss: 0.00000981
Iteration 209/1000 | Loss: 0.00000981
Iteration 210/1000 | Loss: 0.00000981
Iteration 211/1000 | Loss: 0.00000981
Iteration 212/1000 | Loss: 0.00000981
Iteration 213/1000 | Loss: 0.00000981
Iteration 214/1000 | Loss: 0.00000981
Iteration 215/1000 | Loss: 0.00000981
Iteration 216/1000 | Loss: 0.00000980
Iteration 217/1000 | Loss: 0.00000980
Iteration 218/1000 | Loss: 0.00000980
Iteration 219/1000 | Loss: 0.00000980
Iteration 220/1000 | Loss: 0.00000980
Iteration 221/1000 | Loss: 0.00000980
Iteration 222/1000 | Loss: 0.00000980
Iteration 223/1000 | Loss: 0.00000980
Iteration 224/1000 | Loss: 0.00000980
Iteration 225/1000 | Loss: 0.00000979
Iteration 226/1000 | Loss: 0.00000979
Iteration 227/1000 | Loss: 0.00000979
Iteration 228/1000 | Loss: 0.00000979
Iteration 229/1000 | Loss: 0.00000979
Iteration 230/1000 | Loss: 0.00000979
Iteration 231/1000 | Loss: 0.00000979
Iteration 232/1000 | Loss: 0.00000978
Iteration 233/1000 | Loss: 0.00000978
Iteration 234/1000 | Loss: 0.00000978
Iteration 235/1000 | Loss: 0.00000978
Iteration 236/1000 | Loss: 0.00000978
Iteration 237/1000 | Loss: 0.00000978
Iteration 238/1000 | Loss: 0.00000977
Iteration 239/1000 | Loss: 0.00000977
Iteration 240/1000 | Loss: 0.00000977
Iteration 241/1000 | Loss: 0.00000977
Iteration 242/1000 | Loss: 0.00000977
Iteration 243/1000 | Loss: 0.00000977
Iteration 244/1000 | Loss: 0.00000977
Iteration 245/1000 | Loss: 0.00000976
Iteration 246/1000 | Loss: 0.00000976
Iteration 247/1000 | Loss: 0.00000976
Iteration 248/1000 | Loss: 0.00000976
Iteration 249/1000 | Loss: 0.00000976
Iteration 250/1000 | Loss: 0.00000976
Iteration 251/1000 | Loss: 0.00000976
Iteration 252/1000 | Loss: 0.00000976
Iteration 253/1000 | Loss: 0.00000976
Iteration 254/1000 | Loss: 0.00000976
Iteration 255/1000 | Loss: 0.00000975
Iteration 256/1000 | Loss: 0.00000975
Iteration 257/1000 | Loss: 0.00000975
Iteration 258/1000 | Loss: 0.00000975
Iteration 259/1000 | Loss: 0.00000975
Iteration 260/1000 | Loss: 0.00000974
Iteration 261/1000 | Loss: 0.00000974
Iteration 262/1000 | Loss: 0.00000974
Iteration 263/1000 | Loss: 0.00000974
Iteration 264/1000 | Loss: 0.00000974
Iteration 265/1000 | Loss: 0.00000974
Iteration 266/1000 | Loss: 0.00000974
Iteration 267/1000 | Loss: 0.00000974
Iteration 268/1000 | Loss: 0.00000974
Iteration 269/1000 | Loss: 0.00000974
Iteration 270/1000 | Loss: 0.00000974
Iteration 271/1000 | Loss: 0.00000973
Iteration 272/1000 | Loss: 0.00000973
Iteration 273/1000 | Loss: 0.00000973
Iteration 274/1000 | Loss: 0.00000973
Iteration 275/1000 | Loss: 0.00000973
Iteration 276/1000 | Loss: 0.00000973
Iteration 277/1000 | Loss: 0.00000973
Iteration 278/1000 | Loss: 0.00000973
Iteration 279/1000 | Loss: 0.00000973
Iteration 280/1000 | Loss: 0.00000973
Iteration 281/1000 | Loss: 0.00000973
Iteration 282/1000 | Loss: 0.00000973
Iteration 283/1000 | Loss: 0.00000973
Iteration 284/1000 | Loss: 0.00000973
Iteration 285/1000 | Loss: 0.00000973
Iteration 286/1000 | Loss: 0.00000972
Iteration 287/1000 | Loss: 0.00000972
Iteration 288/1000 | Loss: 0.00000972
Iteration 289/1000 | Loss: 0.00000972
Iteration 290/1000 | Loss: 0.00000972
Iteration 291/1000 | Loss: 0.00000972
Iteration 292/1000 | Loss: 0.00000972
Iteration 293/1000 | Loss: 0.00000972
Iteration 294/1000 | Loss: 0.00000972
Iteration 295/1000 | Loss: 0.00000972
Iteration 296/1000 | Loss: 0.00000972
Iteration 297/1000 | Loss: 0.00000972
Iteration 298/1000 | Loss: 0.00000972
Iteration 299/1000 | Loss: 0.00000972
Iteration 300/1000 | Loss: 0.00000972
Iteration 301/1000 | Loss: 0.00000971
Iteration 302/1000 | Loss: 0.00000971
Iteration 303/1000 | Loss: 0.00000971
Iteration 304/1000 | Loss: 0.00000971
Iteration 305/1000 | Loss: 0.00000971
Iteration 306/1000 | Loss: 0.00000971
Iteration 307/1000 | Loss: 0.00000971
Iteration 308/1000 | Loss: 0.00000971
Iteration 309/1000 | Loss: 0.00000971
Iteration 310/1000 | Loss: 0.00000971
Iteration 311/1000 | Loss: 0.00000970
Iteration 312/1000 | Loss: 0.00000970
Iteration 313/1000 | Loss: 0.00000970
Iteration 314/1000 | Loss: 0.00000970
Iteration 315/1000 | Loss: 0.00000970
Iteration 316/1000 | Loss: 0.00000970
Iteration 317/1000 | Loss: 0.00000970
Iteration 318/1000 | Loss: 0.00000970
Iteration 319/1000 | Loss: 0.00000970
Iteration 320/1000 | Loss: 0.00000970
Iteration 321/1000 | Loss: 0.00000970
Iteration 322/1000 | Loss: 0.00000970
Iteration 323/1000 | Loss: 0.00000970
Iteration 324/1000 | Loss: 0.00000969
Iteration 325/1000 | Loss: 0.00000969
Iteration 326/1000 | Loss: 0.00000969
Iteration 327/1000 | Loss: 0.00000969
Iteration 328/1000 | Loss: 0.00000969
Iteration 329/1000 | Loss: 0.00000969
Iteration 330/1000 | Loss: 0.00000969
Iteration 331/1000 | Loss: 0.00000969
Iteration 332/1000 | Loss: 0.00000969
Iteration 333/1000 | Loss: 0.00000969
Iteration 334/1000 | Loss: 0.00000969
Iteration 335/1000 | Loss: 0.00000969
Iteration 336/1000 | Loss: 0.00000969
Iteration 337/1000 | Loss: 0.00000969
Iteration 338/1000 | Loss: 0.00000969
Iteration 339/1000 | Loss: 0.00000969
Iteration 340/1000 | Loss: 0.00000969
Iteration 341/1000 | Loss: 0.00000969
Iteration 342/1000 | Loss: 0.00000969
Iteration 343/1000 | Loss: 0.00000969
Iteration 344/1000 | Loss: 0.00000969
Iteration 345/1000 | Loss: 0.00000969
Iteration 346/1000 | Loss: 0.00000969
Iteration 347/1000 | Loss: 0.00000969
Iteration 348/1000 | Loss: 0.00000969
Iteration 349/1000 | Loss: 0.00000969
Iteration 350/1000 | Loss: 0.00000969
Iteration 351/1000 | Loss: 0.00000969
Iteration 352/1000 | Loss: 0.00000969
Iteration 353/1000 | Loss: 0.00000969
Iteration 354/1000 | Loss: 0.00000969
Iteration 355/1000 | Loss: 0.00000969
Iteration 356/1000 | Loss: 0.00000969
Iteration 357/1000 | Loss: 0.00000969
Iteration 358/1000 | Loss: 0.00000969
Iteration 359/1000 | Loss: 0.00000969
Iteration 360/1000 | Loss: 0.00000969
Iteration 361/1000 | Loss: 0.00000969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 361. Stopping optimization.
Last 5 losses: [9.68652057053987e-06, 9.68652057053987e-06, 9.68652057053987e-06, 9.68652057053987e-06, 9.68652057053987e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.68652057053987e-06

Optimization complete. Final v2v error: 2.6791749000549316 mm

Highest mean error: 2.85459041595459 mm for frame 49

Lowest mean error: 2.5385797023773193 mm for frame 136

Saving results

Total time: 51.2579607963562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774894
Iteration 2/25 | Loss: 0.00162550
Iteration 3/25 | Loss: 0.00137112
Iteration 4/25 | Loss: 0.00135129
Iteration 5/25 | Loss: 0.00134910
Iteration 6/25 | Loss: 0.00134910
Iteration 7/25 | Loss: 0.00134910
Iteration 8/25 | Loss: 0.00134910
Iteration 9/25 | Loss: 0.00134910
Iteration 10/25 | Loss: 0.00134910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013490981655195355, 0.0013490981655195355, 0.0013490981655195355, 0.0013490981655195355, 0.0013490981655195355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013490981655195355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22913289
Iteration 2/25 | Loss: 0.00192225
Iteration 3/25 | Loss: 0.00192225
Iteration 4/25 | Loss: 0.00192224
Iteration 5/25 | Loss: 0.00192224
Iteration 6/25 | Loss: 0.00192224
Iteration 7/25 | Loss: 0.00192224
Iteration 8/25 | Loss: 0.00192224
Iteration 9/25 | Loss: 0.00192224
Iteration 10/25 | Loss: 0.00192224
Iteration 11/25 | Loss: 0.00192224
Iteration 12/25 | Loss: 0.00192224
Iteration 13/25 | Loss: 0.00192224
Iteration 14/25 | Loss: 0.00192224
Iteration 15/25 | Loss: 0.00192224
Iteration 16/25 | Loss: 0.00192224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019222414121031761, 0.0019222414121031761, 0.0019222414121031761, 0.0019222414121031761, 0.0019222414121031761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019222414121031761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192224
Iteration 2/1000 | Loss: 0.00003075
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00001920
Iteration 5/1000 | Loss: 0.00001744
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001575
Iteration 8/1000 | Loss: 0.00001531
Iteration 9/1000 | Loss: 0.00001484
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001427
Iteration 12/1000 | Loss: 0.00001398
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001386
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001361
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001340
Iteration 25/1000 | Loss: 0.00001340
Iteration 26/1000 | Loss: 0.00001340
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001319
Iteration 33/1000 | Loss: 0.00001318
Iteration 34/1000 | Loss: 0.00001317
Iteration 35/1000 | Loss: 0.00001317
Iteration 36/1000 | Loss: 0.00001316
Iteration 37/1000 | Loss: 0.00001316
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001314
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001313
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001311
Iteration 45/1000 | Loss: 0.00001310
Iteration 46/1000 | Loss: 0.00001310
Iteration 47/1000 | Loss: 0.00001310
Iteration 48/1000 | Loss: 0.00001309
Iteration 49/1000 | Loss: 0.00001309
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001308
Iteration 53/1000 | Loss: 0.00001308
Iteration 54/1000 | Loss: 0.00001308
Iteration 55/1000 | Loss: 0.00001308
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001308
Iteration 58/1000 | Loss: 0.00001308
Iteration 59/1000 | Loss: 0.00001308
Iteration 60/1000 | Loss: 0.00001308
Iteration 61/1000 | Loss: 0.00001307
Iteration 62/1000 | Loss: 0.00001307
Iteration 63/1000 | Loss: 0.00001307
Iteration 64/1000 | Loss: 0.00001307
Iteration 65/1000 | Loss: 0.00001307
Iteration 66/1000 | Loss: 0.00001307
Iteration 67/1000 | Loss: 0.00001307
Iteration 68/1000 | Loss: 0.00001307
Iteration 69/1000 | Loss: 0.00001307
Iteration 70/1000 | Loss: 0.00001307
Iteration 71/1000 | Loss: 0.00001306
Iteration 72/1000 | Loss: 0.00001306
Iteration 73/1000 | Loss: 0.00001306
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001305
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001302
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001302
Iteration 88/1000 | Loss: 0.00001302
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001301
Iteration 91/1000 | Loss: 0.00001301
Iteration 92/1000 | Loss: 0.00001301
Iteration 93/1000 | Loss: 0.00001301
Iteration 94/1000 | Loss: 0.00001301
Iteration 95/1000 | Loss: 0.00001301
Iteration 96/1000 | Loss: 0.00001301
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001300
Iteration 101/1000 | Loss: 0.00001300
Iteration 102/1000 | Loss: 0.00001300
Iteration 103/1000 | Loss: 0.00001300
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001299
Iteration 106/1000 | Loss: 0.00001299
Iteration 107/1000 | Loss: 0.00001299
Iteration 108/1000 | Loss: 0.00001299
Iteration 109/1000 | Loss: 0.00001299
Iteration 110/1000 | Loss: 0.00001299
Iteration 111/1000 | Loss: 0.00001299
Iteration 112/1000 | Loss: 0.00001299
Iteration 113/1000 | Loss: 0.00001299
Iteration 114/1000 | Loss: 0.00001299
Iteration 115/1000 | Loss: 0.00001299
Iteration 116/1000 | Loss: 0.00001299
Iteration 117/1000 | Loss: 0.00001299
Iteration 118/1000 | Loss: 0.00001298
Iteration 119/1000 | Loss: 0.00001298
Iteration 120/1000 | Loss: 0.00001298
Iteration 121/1000 | Loss: 0.00001298
Iteration 122/1000 | Loss: 0.00001298
Iteration 123/1000 | Loss: 0.00001298
Iteration 124/1000 | Loss: 0.00001298
Iteration 125/1000 | Loss: 0.00001297
Iteration 126/1000 | Loss: 0.00001297
Iteration 127/1000 | Loss: 0.00001297
Iteration 128/1000 | Loss: 0.00001297
Iteration 129/1000 | Loss: 0.00001297
Iteration 130/1000 | Loss: 0.00001297
Iteration 131/1000 | Loss: 0.00001297
Iteration 132/1000 | Loss: 0.00001297
Iteration 133/1000 | Loss: 0.00001297
Iteration 134/1000 | Loss: 0.00001297
Iteration 135/1000 | Loss: 0.00001297
Iteration 136/1000 | Loss: 0.00001297
Iteration 137/1000 | Loss: 0.00001297
Iteration 138/1000 | Loss: 0.00001297
Iteration 139/1000 | Loss: 0.00001297
Iteration 140/1000 | Loss: 0.00001297
Iteration 141/1000 | Loss: 0.00001297
Iteration 142/1000 | Loss: 0.00001297
Iteration 143/1000 | Loss: 0.00001297
Iteration 144/1000 | Loss: 0.00001297
Iteration 145/1000 | Loss: 0.00001297
Iteration 146/1000 | Loss: 0.00001297
Iteration 147/1000 | Loss: 0.00001297
Iteration 148/1000 | Loss: 0.00001297
Iteration 149/1000 | Loss: 0.00001297
Iteration 150/1000 | Loss: 0.00001297
Iteration 151/1000 | Loss: 0.00001297
Iteration 152/1000 | Loss: 0.00001297
Iteration 153/1000 | Loss: 0.00001297
Iteration 154/1000 | Loss: 0.00001297
Iteration 155/1000 | Loss: 0.00001297
Iteration 156/1000 | Loss: 0.00001297
Iteration 157/1000 | Loss: 0.00001297
Iteration 158/1000 | Loss: 0.00001297
Iteration 159/1000 | Loss: 0.00001297
Iteration 160/1000 | Loss: 0.00001297
Iteration 161/1000 | Loss: 0.00001297
Iteration 162/1000 | Loss: 0.00001297
Iteration 163/1000 | Loss: 0.00001297
Iteration 164/1000 | Loss: 0.00001297
Iteration 165/1000 | Loss: 0.00001297
Iteration 166/1000 | Loss: 0.00001297
Iteration 167/1000 | Loss: 0.00001297
Iteration 168/1000 | Loss: 0.00001297
Iteration 169/1000 | Loss: 0.00001297
Iteration 170/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.2971070646017324e-05, 1.2971070646017324e-05, 1.2971070646017324e-05, 1.2971070646017324e-05, 1.2971070646017324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2971070646017324e-05

Optimization complete. Final v2v error: 3.0521881580352783 mm

Highest mean error: 3.4054105281829834 mm for frame 96

Lowest mean error: 2.7607553005218506 mm for frame 80

Saving results

Total time: 40.26057982444763
