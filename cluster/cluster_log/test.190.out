Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=190, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10640-10695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920838
Iteration 2/25 | Loss: 0.00207054
Iteration 3/25 | Loss: 0.00152078
Iteration 4/25 | Loss: 0.00143463
Iteration 5/25 | Loss: 0.00144560
Iteration 6/25 | Loss: 0.00141842
Iteration 7/25 | Loss: 0.00141319
Iteration 8/25 | Loss: 0.00138704
Iteration 9/25 | Loss: 0.00137811
Iteration 10/25 | Loss: 0.00137664
Iteration 11/25 | Loss: 0.00137728
Iteration 12/25 | Loss: 0.00137641
Iteration 13/25 | Loss: 0.00137547
Iteration 14/25 | Loss: 0.00137555
Iteration 15/25 | Loss: 0.00137321
Iteration 16/25 | Loss: 0.00136746
Iteration 17/25 | Loss: 0.00136631
Iteration 18/25 | Loss: 0.00136440
Iteration 19/25 | Loss: 0.00136156
Iteration 20/25 | Loss: 0.00136145
Iteration 21/25 | Loss: 0.00136056
Iteration 22/25 | Loss: 0.00136054
Iteration 23/25 | Loss: 0.00136011
Iteration 24/25 | Loss: 0.00135711
Iteration 25/25 | Loss: 0.00135635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.28063536
Iteration 2/25 | Loss: 0.00292949
Iteration 3/25 | Loss: 0.00292949
Iteration 4/25 | Loss: 0.00292949
Iteration 5/25 | Loss: 0.00292948
Iteration 6/25 | Loss: 0.00292948
Iteration 7/25 | Loss: 0.00292948
Iteration 8/25 | Loss: 0.00292948
Iteration 9/25 | Loss: 0.00292948
Iteration 10/25 | Loss: 0.00292948
Iteration 11/25 | Loss: 0.00292948
Iteration 12/25 | Loss: 0.00292948
Iteration 13/25 | Loss: 0.00292948
Iteration 14/25 | Loss: 0.00292948
Iteration 15/25 | Loss: 0.00292948
Iteration 16/25 | Loss: 0.00292948
Iteration 17/25 | Loss: 0.00292948
Iteration 18/25 | Loss: 0.00292948
Iteration 19/25 | Loss: 0.00292948
Iteration 20/25 | Loss: 0.00292948
Iteration 21/25 | Loss: 0.00292948
Iteration 22/25 | Loss: 0.00292948
Iteration 23/25 | Loss: 0.00292948
Iteration 24/25 | Loss: 0.00292948
Iteration 25/25 | Loss: 0.00292948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00292948
Iteration 2/1000 | Loss: 0.00030399
Iteration 3/1000 | Loss: 0.00127245
Iteration 4/1000 | Loss: 0.00146434
Iteration 5/1000 | Loss: 0.00114593
Iteration 6/1000 | Loss: 0.00413105
Iteration 7/1000 | Loss: 0.00048367
Iteration 8/1000 | Loss: 0.00076491
Iteration 9/1000 | Loss: 0.00094002
Iteration 10/1000 | Loss: 0.00015283
Iteration 11/1000 | Loss: 0.00010998
Iteration 12/1000 | Loss: 0.00006045
Iteration 13/1000 | Loss: 0.00004818
Iteration 14/1000 | Loss: 0.00004023
Iteration 15/1000 | Loss: 0.00003636
Iteration 16/1000 | Loss: 0.00003389
Iteration 17/1000 | Loss: 0.00003237
Iteration 18/1000 | Loss: 0.00003104
Iteration 19/1000 | Loss: 0.00003026
Iteration 20/1000 | Loss: 0.00002937
Iteration 21/1000 | Loss: 0.00002874
Iteration 22/1000 | Loss: 0.00094103
Iteration 23/1000 | Loss: 0.00069000
Iteration 24/1000 | Loss: 0.00046894
Iteration 25/1000 | Loss: 0.00061847
Iteration 26/1000 | Loss: 0.00004654
Iteration 27/1000 | Loss: 0.00003567
Iteration 28/1000 | Loss: 0.00002953
Iteration 29/1000 | Loss: 0.00002749
Iteration 30/1000 | Loss: 0.00002633
Iteration 31/1000 | Loss: 0.00002536
Iteration 32/1000 | Loss: 0.00002454
Iteration 33/1000 | Loss: 0.00002408
Iteration 34/1000 | Loss: 0.00002367
Iteration 35/1000 | Loss: 0.00002336
Iteration 36/1000 | Loss: 0.00002313
Iteration 37/1000 | Loss: 0.00002307
Iteration 38/1000 | Loss: 0.00002295
Iteration 39/1000 | Loss: 0.00035933
Iteration 40/1000 | Loss: 0.00139336
Iteration 41/1000 | Loss: 0.00029957
Iteration 42/1000 | Loss: 0.00002667
Iteration 43/1000 | Loss: 0.00002374
Iteration 44/1000 | Loss: 0.00002191
Iteration 45/1000 | Loss: 0.00002046
Iteration 46/1000 | Loss: 0.00070545
Iteration 47/1000 | Loss: 0.00011485
Iteration 48/1000 | Loss: 0.00003045
Iteration 49/1000 | Loss: 0.00002416
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00049048
Iteration 53/1000 | Loss: 0.00003017
Iteration 54/1000 | Loss: 0.00002433
Iteration 55/1000 | Loss: 0.00001891
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001423
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001419
Iteration 66/1000 | Loss: 0.00001412
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001411
Iteration 69/1000 | Loss: 0.00001411
Iteration 70/1000 | Loss: 0.00001411
Iteration 71/1000 | Loss: 0.00001410
Iteration 72/1000 | Loss: 0.00001410
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001404
Iteration 83/1000 | Loss: 0.00001403
Iteration 84/1000 | Loss: 0.00001403
Iteration 85/1000 | Loss: 0.00001403
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001402
Iteration 89/1000 | Loss: 0.00001402
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001402
Iteration 92/1000 | Loss: 0.00001402
Iteration 93/1000 | Loss: 0.00001402
Iteration 94/1000 | Loss: 0.00001402
Iteration 95/1000 | Loss: 0.00001402
Iteration 96/1000 | Loss: 0.00001401
Iteration 97/1000 | Loss: 0.00001401
Iteration 98/1000 | Loss: 0.00001401
Iteration 99/1000 | Loss: 0.00001400
Iteration 100/1000 | Loss: 0.00001400
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001399
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001396
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001396
Iteration 119/1000 | Loss: 0.00001396
Iteration 120/1000 | Loss: 0.00001396
Iteration 121/1000 | Loss: 0.00001396
Iteration 122/1000 | Loss: 0.00001396
Iteration 123/1000 | Loss: 0.00001396
Iteration 124/1000 | Loss: 0.00001396
Iteration 125/1000 | Loss: 0.00001396
Iteration 126/1000 | Loss: 0.00001395
Iteration 127/1000 | Loss: 0.00001395
Iteration 128/1000 | Loss: 0.00001395
Iteration 129/1000 | Loss: 0.00001395
Iteration 130/1000 | Loss: 0.00001394
Iteration 131/1000 | Loss: 0.00001394
Iteration 132/1000 | Loss: 0.00001394
Iteration 133/1000 | Loss: 0.00001394
Iteration 134/1000 | Loss: 0.00001394
Iteration 135/1000 | Loss: 0.00001394
Iteration 136/1000 | Loss: 0.00001393
Iteration 137/1000 | Loss: 0.00001393
Iteration 138/1000 | Loss: 0.00001393
Iteration 139/1000 | Loss: 0.00001393
Iteration 140/1000 | Loss: 0.00001393
Iteration 141/1000 | Loss: 0.00001393
Iteration 142/1000 | Loss: 0.00001393
Iteration 143/1000 | Loss: 0.00001393
Iteration 144/1000 | Loss: 0.00001392
Iteration 145/1000 | Loss: 0.00001392
Iteration 146/1000 | Loss: 0.00001392
Iteration 147/1000 | Loss: 0.00001392
Iteration 148/1000 | Loss: 0.00001392
Iteration 149/1000 | Loss: 0.00001392
Iteration 150/1000 | Loss: 0.00001392
Iteration 151/1000 | Loss: 0.00001392
Iteration 152/1000 | Loss: 0.00001392
Iteration 153/1000 | Loss: 0.00001391
Iteration 154/1000 | Loss: 0.00001391
Iteration 155/1000 | Loss: 0.00001391
Iteration 156/1000 | Loss: 0.00001391
Iteration 157/1000 | Loss: 0.00001391
Iteration 158/1000 | Loss: 0.00001391
Iteration 159/1000 | Loss: 0.00001391
Iteration 160/1000 | Loss: 0.00001391
Iteration 161/1000 | Loss: 0.00001391
Iteration 162/1000 | Loss: 0.00001391
Iteration 163/1000 | Loss: 0.00001390
Iteration 164/1000 | Loss: 0.00001390
Iteration 165/1000 | Loss: 0.00001390
Iteration 166/1000 | Loss: 0.00001390
Iteration 167/1000 | Loss: 0.00001390
Iteration 168/1000 | Loss: 0.00001390
Iteration 169/1000 | Loss: 0.00001390
Iteration 170/1000 | Loss: 0.00001390
Iteration 171/1000 | Loss: 0.00001390
Iteration 172/1000 | Loss: 0.00001390
Iteration 173/1000 | Loss: 0.00001389
Iteration 174/1000 | Loss: 0.00001389
Iteration 175/1000 | Loss: 0.00001389
Iteration 176/1000 | Loss: 0.00001389
Iteration 177/1000 | Loss: 0.00001389
Iteration 178/1000 | Loss: 0.00001389
Iteration 179/1000 | Loss: 0.00001389
Iteration 180/1000 | Loss: 0.00001389
Iteration 181/1000 | Loss: 0.00001389
Iteration 182/1000 | Loss: 0.00001389
Iteration 183/1000 | Loss: 0.00001389
Iteration 184/1000 | Loss: 0.00001389
Iteration 185/1000 | Loss: 0.00001389
Iteration 186/1000 | Loss: 0.00001389
Iteration 187/1000 | Loss: 0.00001389
Iteration 188/1000 | Loss: 0.00001389
Iteration 189/1000 | Loss: 0.00001389
Iteration 190/1000 | Loss: 0.00001389
Iteration 191/1000 | Loss: 0.00001389
Iteration 192/1000 | Loss: 0.00001389
Iteration 193/1000 | Loss: 0.00001389
Iteration 194/1000 | Loss: 0.00001389
Iteration 195/1000 | Loss: 0.00001389
Iteration 196/1000 | Loss: 0.00001389
Iteration 197/1000 | Loss: 0.00001389
Iteration 198/1000 | Loss: 0.00001389
Iteration 199/1000 | Loss: 0.00001389
Iteration 200/1000 | Loss: 0.00001389
Iteration 201/1000 | Loss: 0.00001389
Iteration 202/1000 | Loss: 0.00001389
Iteration 203/1000 | Loss: 0.00001389
Iteration 204/1000 | Loss: 0.00001389
Iteration 205/1000 | Loss: 0.00001389
Iteration 206/1000 | Loss: 0.00001389
Iteration 207/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.3885142834624276e-05, 1.3885142834624276e-05, 1.3885142834624276e-05, 1.3885142834624276e-05, 1.3885142834624276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3885142834624276e-05

Optimization complete. Final v2v error: 3.0786023139953613 mm

Highest mean error: 4.881520748138428 mm for frame 63

Lowest mean error: 2.501633405685425 mm for frame 105

Saving results

Total time: 147.9493818283081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00707455
Iteration 2/25 | Loss: 0.00127691
Iteration 3/25 | Loss: 0.00119091
Iteration 4/25 | Loss: 0.00117277
Iteration 5/25 | Loss: 0.00116661
Iteration 6/25 | Loss: 0.00116480
Iteration 7/25 | Loss: 0.00116475
Iteration 8/25 | Loss: 0.00116475
Iteration 9/25 | Loss: 0.00116475
Iteration 10/25 | Loss: 0.00116475
Iteration 11/25 | Loss: 0.00116475
Iteration 12/25 | Loss: 0.00116475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011647532228380442, 0.0011647532228380442, 0.0011647532228380442, 0.0011647532228380442, 0.0011647532228380442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011647532228380442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.15206814
Iteration 2/25 | Loss: 0.00121126
Iteration 3/25 | Loss: 0.00121125
Iteration 4/25 | Loss: 0.00121125
Iteration 5/25 | Loss: 0.00121125
Iteration 6/25 | Loss: 0.00121125
Iteration 7/25 | Loss: 0.00121125
Iteration 8/25 | Loss: 0.00121125
Iteration 9/25 | Loss: 0.00121125
Iteration 10/25 | Loss: 0.00121125
Iteration 11/25 | Loss: 0.00121125
Iteration 12/25 | Loss: 0.00121125
Iteration 13/25 | Loss: 0.00121125
Iteration 14/25 | Loss: 0.00121125
Iteration 15/25 | Loss: 0.00121125
Iteration 16/25 | Loss: 0.00121125
Iteration 17/25 | Loss: 0.00121125
Iteration 18/25 | Loss: 0.00121125
Iteration 19/25 | Loss: 0.00121125
Iteration 20/25 | Loss: 0.00121125
Iteration 21/25 | Loss: 0.00121125
Iteration 22/25 | Loss: 0.00121125
Iteration 23/25 | Loss: 0.00121125
Iteration 24/25 | Loss: 0.00121125
Iteration 25/25 | Loss: 0.00121125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121125
Iteration 2/1000 | Loss: 0.00002704
Iteration 3/1000 | Loss: 0.00001845
Iteration 4/1000 | Loss: 0.00001659
Iteration 5/1000 | Loss: 0.00001562
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00001410
Iteration 9/1000 | Loss: 0.00001375
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001309
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001285
Iteration 25/1000 | Loss: 0.00001285
Iteration 26/1000 | Loss: 0.00001280
Iteration 27/1000 | Loss: 0.00001278
Iteration 28/1000 | Loss: 0.00001278
Iteration 29/1000 | Loss: 0.00001277
Iteration 30/1000 | Loss: 0.00001276
Iteration 31/1000 | Loss: 0.00001275
Iteration 32/1000 | Loss: 0.00001275
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001273
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001273
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001269
Iteration 40/1000 | Loss: 0.00001269
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001268
Iteration 50/1000 | Loss: 0.00001268
Iteration 51/1000 | Loss: 0.00001268
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001267
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001266
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001264
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001264
Iteration 62/1000 | Loss: 0.00001263
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001261
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001261
Iteration 71/1000 | Loss: 0.00001261
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001261
Iteration 74/1000 | Loss: 0.00001260
Iteration 75/1000 | Loss: 0.00001260
Iteration 76/1000 | Loss: 0.00001260
Iteration 77/1000 | Loss: 0.00001260
Iteration 78/1000 | Loss: 0.00001260
Iteration 79/1000 | Loss: 0.00001260
Iteration 80/1000 | Loss: 0.00001260
Iteration 81/1000 | Loss: 0.00001260
Iteration 82/1000 | Loss: 0.00001259
Iteration 83/1000 | Loss: 0.00001259
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001258
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001255
Iteration 99/1000 | Loss: 0.00001255
Iteration 100/1000 | Loss: 0.00001255
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001254
Iteration 104/1000 | Loss: 0.00001254
Iteration 105/1000 | Loss: 0.00001254
Iteration 106/1000 | Loss: 0.00001254
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001253
Iteration 112/1000 | Loss: 0.00001253
Iteration 113/1000 | Loss: 0.00001253
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001253
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001253
Iteration 132/1000 | Loss: 0.00001253
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2527420949481893e-05, 1.2527420949481893e-05, 1.2527420949481893e-05, 1.2527420949481893e-05, 1.2527420949481893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2527420949481893e-05

Optimization complete. Final v2v error: 3.0648677349090576 mm

Highest mean error: 3.319040060043335 mm for frame 98

Lowest mean error: 2.823035717010498 mm for frame 3

Saving results

Total time: 38.783766746520996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358964
Iteration 2/25 | Loss: 0.00125998
Iteration 3/25 | Loss: 0.00115370
Iteration 4/25 | Loss: 0.00113696
Iteration 5/25 | Loss: 0.00113297
Iteration 6/25 | Loss: 0.00113250
Iteration 7/25 | Loss: 0.00113250
Iteration 8/25 | Loss: 0.00113250
Iteration 9/25 | Loss: 0.00113250
Iteration 10/25 | Loss: 0.00113250
Iteration 11/25 | Loss: 0.00113250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011325046652927995, 0.0011325046652927995, 0.0011325046652927995, 0.0011325046652927995, 0.0011325046652927995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011325046652927995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30320179
Iteration 2/25 | Loss: 0.00173115
Iteration 3/25 | Loss: 0.00173115
Iteration 4/25 | Loss: 0.00173115
Iteration 5/25 | Loss: 0.00173115
Iteration 6/25 | Loss: 0.00173115
Iteration 7/25 | Loss: 0.00173115
Iteration 8/25 | Loss: 0.00173115
Iteration 9/25 | Loss: 0.00173115
Iteration 10/25 | Loss: 0.00173115
Iteration 11/25 | Loss: 0.00173115
Iteration 12/25 | Loss: 0.00173115
Iteration 13/25 | Loss: 0.00173115
Iteration 14/25 | Loss: 0.00173115
Iteration 15/25 | Loss: 0.00173115
Iteration 16/25 | Loss: 0.00173115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017311499686911702, 0.0017311499686911702, 0.0017311499686911702, 0.0017311499686911702, 0.0017311499686911702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017311499686911702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173115
Iteration 2/1000 | Loss: 0.00004745
Iteration 3/1000 | Loss: 0.00003265
Iteration 4/1000 | Loss: 0.00002232
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001886
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001585
Iteration 11/1000 | Loss: 0.00001557
Iteration 12/1000 | Loss: 0.00001536
Iteration 13/1000 | Loss: 0.00001521
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001500
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001495
Iteration 21/1000 | Loss: 0.00001492
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001487
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001483
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001481
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001475
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001473
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001472
Iteration 39/1000 | Loss: 0.00001472
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001471
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001469
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001468
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001467
Iteration 53/1000 | Loss: 0.00001467
Iteration 54/1000 | Loss: 0.00001467
Iteration 55/1000 | Loss: 0.00001467
Iteration 56/1000 | Loss: 0.00001466
Iteration 57/1000 | Loss: 0.00001466
Iteration 58/1000 | Loss: 0.00001466
Iteration 59/1000 | Loss: 0.00001466
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001465
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001460
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001457
Iteration 88/1000 | Loss: 0.00001456
Iteration 89/1000 | Loss: 0.00001456
Iteration 90/1000 | Loss: 0.00001456
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001455
Iteration 94/1000 | Loss: 0.00001455
Iteration 95/1000 | Loss: 0.00001455
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001452
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001451
Iteration 108/1000 | Loss: 0.00001451
Iteration 109/1000 | Loss: 0.00001451
Iteration 110/1000 | Loss: 0.00001451
Iteration 111/1000 | Loss: 0.00001450
Iteration 112/1000 | Loss: 0.00001450
Iteration 113/1000 | Loss: 0.00001450
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001449
Iteration 117/1000 | Loss: 0.00001449
Iteration 118/1000 | Loss: 0.00001449
Iteration 119/1000 | Loss: 0.00001449
Iteration 120/1000 | Loss: 0.00001448
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001448
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001448
Iteration 128/1000 | Loss: 0.00001448
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001447
Iteration 133/1000 | Loss: 0.00001447
Iteration 134/1000 | Loss: 0.00001447
Iteration 135/1000 | Loss: 0.00001447
Iteration 136/1000 | Loss: 0.00001447
Iteration 137/1000 | Loss: 0.00001447
Iteration 138/1000 | Loss: 0.00001447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.4471842405328061e-05, 1.4471842405328061e-05, 1.4471842405328061e-05, 1.4471842405328061e-05, 1.4471842405328061e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4471842405328061e-05

Optimization complete. Final v2v error: 3.1948037147521973 mm

Highest mean error: 4.7909088134765625 mm for frame 151

Lowest mean error: 2.273352861404419 mm for frame 1

Saving results

Total time: 41.7225558757782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009217
Iteration 2/25 | Loss: 0.00265201
Iteration 3/25 | Loss: 0.00177816
Iteration 4/25 | Loss: 0.00146135
Iteration 5/25 | Loss: 0.00157707
Iteration 6/25 | Loss: 0.00146798
Iteration 7/25 | Loss: 0.00140406
Iteration 8/25 | Loss: 0.00132617
Iteration 9/25 | Loss: 0.00129356
Iteration 10/25 | Loss: 0.00127409
Iteration 11/25 | Loss: 0.00126414
Iteration 12/25 | Loss: 0.00125185
Iteration 13/25 | Loss: 0.00123281
Iteration 14/25 | Loss: 0.00121979
Iteration 15/25 | Loss: 0.00120998
Iteration 16/25 | Loss: 0.00120861
Iteration 17/25 | Loss: 0.00120536
Iteration 18/25 | Loss: 0.00119568
Iteration 19/25 | Loss: 0.00119515
Iteration 20/25 | Loss: 0.00119264
Iteration 21/25 | Loss: 0.00118863
Iteration 22/25 | Loss: 0.00119060
Iteration 23/25 | Loss: 0.00118845
Iteration 24/25 | Loss: 0.00118858
Iteration 25/25 | Loss: 0.00118439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30066419
Iteration 2/25 | Loss: 0.00137435
Iteration 3/25 | Loss: 0.00137435
Iteration 4/25 | Loss: 0.00137435
Iteration 5/25 | Loss: 0.00137435
Iteration 6/25 | Loss: 0.00137435
Iteration 7/25 | Loss: 0.00137435
Iteration 8/25 | Loss: 0.00137435
Iteration 9/25 | Loss: 0.00137435
Iteration 10/25 | Loss: 0.00137435
Iteration 11/25 | Loss: 0.00137435
Iteration 12/25 | Loss: 0.00137435
Iteration 13/25 | Loss: 0.00137435
Iteration 14/25 | Loss: 0.00137435
Iteration 15/25 | Loss: 0.00137435
Iteration 16/25 | Loss: 0.00137435
Iteration 17/25 | Loss: 0.00137435
Iteration 18/25 | Loss: 0.00137435
Iteration 19/25 | Loss: 0.00137435
Iteration 20/25 | Loss: 0.00137435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013743493473157287, 0.0013743493473157287, 0.0013743493473157287, 0.0013743493473157287, 0.0013743493473157287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013743493473157287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137435
Iteration 2/1000 | Loss: 0.00003953
Iteration 3/1000 | Loss: 0.00022534
Iteration 4/1000 | Loss: 0.00006352
Iteration 5/1000 | Loss: 0.00008048
Iteration 6/1000 | Loss: 0.00014509
Iteration 7/1000 | Loss: 0.00008939
Iteration 8/1000 | Loss: 0.00012403
Iteration 9/1000 | Loss: 0.00014767
Iteration 10/1000 | Loss: 0.00033264
Iteration 11/1000 | Loss: 0.00012426
Iteration 12/1000 | Loss: 0.00082409
Iteration 13/1000 | Loss: 0.00012564
Iteration 14/1000 | Loss: 0.00028915
Iteration 15/1000 | Loss: 0.00007723
Iteration 16/1000 | Loss: 0.00004396
Iteration 17/1000 | Loss: 0.00005517
Iteration 18/1000 | Loss: 0.00005970
Iteration 19/1000 | Loss: 0.00044458
Iteration 20/1000 | Loss: 0.00032475
Iteration 21/1000 | Loss: 0.00006877
Iteration 22/1000 | Loss: 0.00012096
Iteration 23/1000 | Loss: 0.00018025
Iteration 24/1000 | Loss: 0.00006413
Iteration 25/1000 | Loss: 0.00030827
Iteration 26/1000 | Loss: 0.00010364
Iteration 27/1000 | Loss: 0.00005532
Iteration 28/1000 | Loss: 0.00005440
Iteration 29/1000 | Loss: 0.00035798
Iteration 30/1000 | Loss: 0.00004499
Iteration 31/1000 | Loss: 0.00003723
Iteration 32/1000 | Loss: 0.00005137
Iteration 33/1000 | Loss: 0.00004566
Iteration 34/1000 | Loss: 0.00004647
Iteration 35/1000 | Loss: 0.00004078
Iteration 36/1000 | Loss: 0.00005795
Iteration 37/1000 | Loss: 0.00006757
Iteration 38/1000 | Loss: 0.00004859
Iteration 39/1000 | Loss: 0.00005885
Iteration 40/1000 | Loss: 0.00006040
Iteration 41/1000 | Loss: 0.00006793
Iteration 42/1000 | Loss: 0.00006209
Iteration 43/1000 | Loss: 0.00006375
Iteration 44/1000 | Loss: 0.00007129
Iteration 45/1000 | Loss: 0.00006770
Iteration 46/1000 | Loss: 0.00006114
Iteration 47/1000 | Loss: 0.00006358
Iteration 48/1000 | Loss: 0.00007891
Iteration 49/1000 | Loss: 0.00005147
Iteration 50/1000 | Loss: 0.00005402
Iteration 51/1000 | Loss: 0.00006767
Iteration 52/1000 | Loss: 0.00005758
Iteration 53/1000 | Loss: 0.00006231
Iteration 54/1000 | Loss: 0.00007597
Iteration 55/1000 | Loss: 0.00008293
Iteration 56/1000 | Loss: 0.00005519
Iteration 57/1000 | Loss: 0.00005629
Iteration 58/1000 | Loss: 0.00005608
Iteration 59/1000 | Loss: 0.00005165
Iteration 60/1000 | Loss: 0.00007014
Iteration 61/1000 | Loss: 0.00003068
Iteration 62/1000 | Loss: 0.00008869
Iteration 63/1000 | Loss: 0.00005547
Iteration 64/1000 | Loss: 0.00005844
Iteration 65/1000 | Loss: 0.00007094
Iteration 66/1000 | Loss: 0.00005247
Iteration 67/1000 | Loss: 0.00007758
Iteration 68/1000 | Loss: 0.00006067
Iteration 69/1000 | Loss: 0.00006358
Iteration 70/1000 | Loss: 0.00006449
Iteration 71/1000 | Loss: 0.00006553
Iteration 72/1000 | Loss: 0.00006396
Iteration 73/1000 | Loss: 0.00006819
Iteration 74/1000 | Loss: 0.00007571
Iteration 75/1000 | Loss: 0.00006477
Iteration 76/1000 | Loss: 0.00006347
Iteration 77/1000 | Loss: 0.00006339
Iteration 78/1000 | Loss: 0.00006927
Iteration 79/1000 | Loss: 0.00006140
Iteration 80/1000 | Loss: 0.00005759
Iteration 81/1000 | Loss: 0.00008584
Iteration 82/1000 | Loss: 0.00007982
Iteration 83/1000 | Loss: 0.00002259
Iteration 84/1000 | Loss: 0.00001735
Iteration 85/1000 | Loss: 0.00001471
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001253
Iteration 89/1000 | Loss: 0.00001242
Iteration 90/1000 | Loss: 0.00001221
Iteration 91/1000 | Loss: 0.00001218
Iteration 92/1000 | Loss: 0.00001195
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001147
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001132
Iteration 99/1000 | Loss: 0.00001128
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001125
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001124
Iteration 108/1000 | Loss: 0.00001124
Iteration 109/1000 | Loss: 0.00001124
Iteration 110/1000 | Loss: 0.00001123
Iteration 111/1000 | Loss: 0.00001123
Iteration 112/1000 | Loss: 0.00001123
Iteration 113/1000 | Loss: 0.00001123
Iteration 114/1000 | Loss: 0.00001123
Iteration 115/1000 | Loss: 0.00001122
Iteration 116/1000 | Loss: 0.00001122
Iteration 117/1000 | Loss: 0.00001122
Iteration 118/1000 | Loss: 0.00001122
Iteration 119/1000 | Loss: 0.00001121
Iteration 120/1000 | Loss: 0.00001121
Iteration 121/1000 | Loss: 0.00001121
Iteration 122/1000 | Loss: 0.00001121
Iteration 123/1000 | Loss: 0.00001121
Iteration 124/1000 | Loss: 0.00001121
Iteration 125/1000 | Loss: 0.00001121
Iteration 126/1000 | Loss: 0.00001121
Iteration 127/1000 | Loss: 0.00001121
Iteration 128/1000 | Loss: 0.00001120
Iteration 129/1000 | Loss: 0.00001120
Iteration 130/1000 | Loss: 0.00001120
Iteration 131/1000 | Loss: 0.00001120
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001119
Iteration 134/1000 | Loss: 0.00001119
Iteration 135/1000 | Loss: 0.00001119
Iteration 136/1000 | Loss: 0.00001119
Iteration 137/1000 | Loss: 0.00001119
Iteration 138/1000 | Loss: 0.00001119
Iteration 139/1000 | Loss: 0.00001119
Iteration 140/1000 | Loss: 0.00001119
Iteration 141/1000 | Loss: 0.00001119
Iteration 142/1000 | Loss: 0.00001119
Iteration 143/1000 | Loss: 0.00001119
Iteration 144/1000 | Loss: 0.00001118
Iteration 145/1000 | Loss: 0.00001118
Iteration 146/1000 | Loss: 0.00001118
Iteration 147/1000 | Loss: 0.00001118
Iteration 148/1000 | Loss: 0.00001118
Iteration 149/1000 | Loss: 0.00001118
Iteration 150/1000 | Loss: 0.00001118
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001117
Iteration 155/1000 | Loss: 0.00001117
Iteration 156/1000 | Loss: 0.00001116
Iteration 157/1000 | Loss: 0.00001116
Iteration 158/1000 | Loss: 0.00001116
Iteration 159/1000 | Loss: 0.00001116
Iteration 160/1000 | Loss: 0.00001115
Iteration 161/1000 | Loss: 0.00001115
Iteration 162/1000 | Loss: 0.00001115
Iteration 163/1000 | Loss: 0.00001115
Iteration 164/1000 | Loss: 0.00001115
Iteration 165/1000 | Loss: 0.00001115
Iteration 166/1000 | Loss: 0.00001115
Iteration 167/1000 | Loss: 0.00001115
Iteration 168/1000 | Loss: 0.00001115
Iteration 169/1000 | Loss: 0.00001114
Iteration 170/1000 | Loss: 0.00001114
Iteration 171/1000 | Loss: 0.00001114
Iteration 172/1000 | Loss: 0.00001114
Iteration 173/1000 | Loss: 0.00001114
Iteration 174/1000 | Loss: 0.00001114
Iteration 175/1000 | Loss: 0.00001114
Iteration 176/1000 | Loss: 0.00001114
Iteration 177/1000 | Loss: 0.00001114
Iteration 178/1000 | Loss: 0.00001114
Iteration 179/1000 | Loss: 0.00001114
Iteration 180/1000 | Loss: 0.00001114
Iteration 181/1000 | Loss: 0.00001114
Iteration 182/1000 | Loss: 0.00001114
Iteration 183/1000 | Loss: 0.00001114
Iteration 184/1000 | Loss: 0.00001114
Iteration 185/1000 | Loss: 0.00001114
Iteration 186/1000 | Loss: 0.00001114
Iteration 187/1000 | Loss: 0.00001114
Iteration 188/1000 | Loss: 0.00001114
Iteration 189/1000 | Loss: 0.00001114
Iteration 190/1000 | Loss: 0.00001114
Iteration 191/1000 | Loss: 0.00001114
Iteration 192/1000 | Loss: 0.00001114
Iteration 193/1000 | Loss: 0.00001114
Iteration 194/1000 | Loss: 0.00001114
Iteration 195/1000 | Loss: 0.00001114
Iteration 196/1000 | Loss: 0.00001114
Iteration 197/1000 | Loss: 0.00001114
Iteration 198/1000 | Loss: 0.00001114
Iteration 199/1000 | Loss: 0.00001114
Iteration 200/1000 | Loss: 0.00001114
Iteration 201/1000 | Loss: 0.00001114
Iteration 202/1000 | Loss: 0.00001114
Iteration 203/1000 | Loss: 0.00001114
Iteration 204/1000 | Loss: 0.00001114
Iteration 205/1000 | Loss: 0.00001114
Iteration 206/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.113500002247747e-05, 1.113500002247747e-05, 1.113500002247747e-05, 1.113500002247747e-05, 1.113500002247747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.113500002247747e-05

Optimization complete. Final v2v error: 2.825814723968506 mm

Highest mean error: 4.841113090515137 mm for frame 44

Lowest mean error: 2.4517626762390137 mm for frame 29

Saving results

Total time: 185.5785357952118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917060
Iteration 2/25 | Loss: 0.00169626
Iteration 3/25 | Loss: 0.00134165
Iteration 4/25 | Loss: 0.00131415
Iteration 5/25 | Loss: 0.00130498
Iteration 6/25 | Loss: 0.00130384
Iteration 7/25 | Loss: 0.00130384
Iteration 8/25 | Loss: 0.00130384
Iteration 9/25 | Loss: 0.00130384
Iteration 10/25 | Loss: 0.00130384
Iteration 11/25 | Loss: 0.00130384
Iteration 12/25 | Loss: 0.00130384
Iteration 13/25 | Loss: 0.00130384
Iteration 14/25 | Loss: 0.00130384
Iteration 15/25 | Loss: 0.00130384
Iteration 16/25 | Loss: 0.00130384
Iteration 17/25 | Loss: 0.00130384
Iteration 18/25 | Loss: 0.00130384
Iteration 19/25 | Loss: 0.00130384
Iteration 20/25 | Loss: 0.00130384
Iteration 21/25 | Loss: 0.00130384
Iteration 22/25 | Loss: 0.00130384
Iteration 23/25 | Loss: 0.00130384
Iteration 24/25 | Loss: 0.00130384
Iteration 25/25 | Loss: 0.00130384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95178950
Iteration 2/25 | Loss: 0.00141180
Iteration 3/25 | Loss: 0.00141180
Iteration 4/25 | Loss: 0.00141180
Iteration 5/25 | Loss: 0.00141180
Iteration 6/25 | Loss: 0.00141179
Iteration 7/25 | Loss: 0.00141179
Iteration 8/25 | Loss: 0.00141179
Iteration 9/25 | Loss: 0.00141179
Iteration 10/25 | Loss: 0.00141179
Iteration 11/25 | Loss: 0.00141179
Iteration 12/25 | Loss: 0.00141179
Iteration 13/25 | Loss: 0.00141179
Iteration 14/25 | Loss: 0.00141179
Iteration 15/25 | Loss: 0.00141179
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014117936370894313, 0.0014117936370894313, 0.0014117936370894313, 0.0014117936370894313, 0.0014117936370894313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014117936370894313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141179
Iteration 2/1000 | Loss: 0.00005751
Iteration 3/1000 | Loss: 0.00004064
Iteration 4/1000 | Loss: 0.00003714
Iteration 5/1000 | Loss: 0.00003522
Iteration 6/1000 | Loss: 0.00003429
Iteration 7/1000 | Loss: 0.00003336
Iteration 8/1000 | Loss: 0.00003290
Iteration 9/1000 | Loss: 0.00003243
Iteration 10/1000 | Loss: 0.00003210
Iteration 11/1000 | Loss: 0.00003180
Iteration 12/1000 | Loss: 0.00003151
Iteration 13/1000 | Loss: 0.00003125
Iteration 14/1000 | Loss: 0.00003102
Iteration 15/1000 | Loss: 0.00003078
Iteration 16/1000 | Loss: 0.00003068
Iteration 17/1000 | Loss: 0.00003063
Iteration 18/1000 | Loss: 0.00003057
Iteration 19/1000 | Loss: 0.00003042
Iteration 20/1000 | Loss: 0.00003028
Iteration 21/1000 | Loss: 0.00003026
Iteration 22/1000 | Loss: 0.00003021
Iteration 23/1000 | Loss: 0.00003019
Iteration 24/1000 | Loss: 0.00003018
Iteration 25/1000 | Loss: 0.00003011
Iteration 26/1000 | Loss: 0.00003010
Iteration 27/1000 | Loss: 0.00003010
Iteration 28/1000 | Loss: 0.00003004
Iteration 29/1000 | Loss: 0.00003003
Iteration 30/1000 | Loss: 0.00003002
Iteration 31/1000 | Loss: 0.00002999
Iteration 32/1000 | Loss: 0.00002998
Iteration 33/1000 | Loss: 0.00002998
Iteration 34/1000 | Loss: 0.00002996
Iteration 35/1000 | Loss: 0.00002995
Iteration 36/1000 | Loss: 0.00002995
Iteration 37/1000 | Loss: 0.00002993
Iteration 38/1000 | Loss: 0.00002992
Iteration 39/1000 | Loss: 0.00002992
Iteration 40/1000 | Loss: 0.00002992
Iteration 41/1000 | Loss: 0.00002991
Iteration 42/1000 | Loss: 0.00002991
Iteration 43/1000 | Loss: 0.00002991
Iteration 44/1000 | Loss: 0.00002990
Iteration 45/1000 | Loss: 0.00002990
Iteration 46/1000 | Loss: 0.00002990
Iteration 47/1000 | Loss: 0.00002990
Iteration 48/1000 | Loss: 0.00002990
Iteration 49/1000 | Loss: 0.00002989
Iteration 50/1000 | Loss: 0.00002989
Iteration 51/1000 | Loss: 0.00002988
Iteration 52/1000 | Loss: 0.00002988
Iteration 53/1000 | Loss: 0.00002988
Iteration 54/1000 | Loss: 0.00002988
Iteration 55/1000 | Loss: 0.00002988
Iteration 56/1000 | Loss: 0.00002987
Iteration 57/1000 | Loss: 0.00002987
Iteration 58/1000 | Loss: 0.00002987
Iteration 59/1000 | Loss: 0.00002987
Iteration 60/1000 | Loss: 0.00002987
Iteration 61/1000 | Loss: 0.00002987
Iteration 62/1000 | Loss: 0.00002987
Iteration 63/1000 | Loss: 0.00002987
Iteration 64/1000 | Loss: 0.00002987
Iteration 65/1000 | Loss: 0.00002987
Iteration 66/1000 | Loss: 0.00002986
Iteration 67/1000 | Loss: 0.00002986
Iteration 68/1000 | Loss: 0.00002986
Iteration 69/1000 | Loss: 0.00002985
Iteration 70/1000 | Loss: 0.00002985
Iteration 71/1000 | Loss: 0.00002985
Iteration 72/1000 | Loss: 0.00002985
Iteration 73/1000 | Loss: 0.00002984
Iteration 74/1000 | Loss: 0.00002984
Iteration 75/1000 | Loss: 0.00002984
Iteration 76/1000 | Loss: 0.00002984
Iteration 77/1000 | Loss: 0.00002984
Iteration 78/1000 | Loss: 0.00002983
Iteration 79/1000 | Loss: 0.00002983
Iteration 80/1000 | Loss: 0.00002983
Iteration 81/1000 | Loss: 0.00002983
Iteration 82/1000 | Loss: 0.00002983
Iteration 83/1000 | Loss: 0.00002983
Iteration 84/1000 | Loss: 0.00002983
Iteration 85/1000 | Loss: 0.00002982
Iteration 86/1000 | Loss: 0.00002982
Iteration 87/1000 | Loss: 0.00002982
Iteration 88/1000 | Loss: 0.00002982
Iteration 89/1000 | Loss: 0.00002982
Iteration 90/1000 | Loss: 0.00002982
Iteration 91/1000 | Loss: 0.00002982
Iteration 92/1000 | Loss: 0.00002981
Iteration 93/1000 | Loss: 0.00002981
Iteration 94/1000 | Loss: 0.00002981
Iteration 95/1000 | Loss: 0.00002981
Iteration 96/1000 | Loss: 0.00002981
Iteration 97/1000 | Loss: 0.00002981
Iteration 98/1000 | Loss: 0.00002980
Iteration 99/1000 | Loss: 0.00002980
Iteration 100/1000 | Loss: 0.00002980
Iteration 101/1000 | Loss: 0.00002980
Iteration 102/1000 | Loss: 0.00002980
Iteration 103/1000 | Loss: 0.00002980
Iteration 104/1000 | Loss: 0.00002980
Iteration 105/1000 | Loss: 0.00002980
Iteration 106/1000 | Loss: 0.00002980
Iteration 107/1000 | Loss: 0.00002980
Iteration 108/1000 | Loss: 0.00002980
Iteration 109/1000 | Loss: 0.00002980
Iteration 110/1000 | Loss: 0.00002979
Iteration 111/1000 | Loss: 0.00002979
Iteration 112/1000 | Loss: 0.00002979
Iteration 113/1000 | Loss: 0.00002979
Iteration 114/1000 | Loss: 0.00002979
Iteration 115/1000 | Loss: 0.00002978
Iteration 116/1000 | Loss: 0.00002978
Iteration 117/1000 | Loss: 0.00002978
Iteration 118/1000 | Loss: 0.00002978
Iteration 119/1000 | Loss: 0.00002978
Iteration 120/1000 | Loss: 0.00002978
Iteration 121/1000 | Loss: 0.00002977
Iteration 122/1000 | Loss: 0.00002977
Iteration 123/1000 | Loss: 0.00002977
Iteration 124/1000 | Loss: 0.00002977
Iteration 125/1000 | Loss: 0.00002977
Iteration 126/1000 | Loss: 0.00002977
Iteration 127/1000 | Loss: 0.00002977
Iteration 128/1000 | Loss: 0.00002977
Iteration 129/1000 | Loss: 0.00002976
Iteration 130/1000 | Loss: 0.00002976
Iteration 131/1000 | Loss: 0.00002976
Iteration 132/1000 | Loss: 0.00002976
Iteration 133/1000 | Loss: 0.00002976
Iteration 134/1000 | Loss: 0.00002976
Iteration 135/1000 | Loss: 0.00002976
Iteration 136/1000 | Loss: 0.00002976
Iteration 137/1000 | Loss: 0.00002976
Iteration 138/1000 | Loss: 0.00002976
Iteration 139/1000 | Loss: 0.00002976
Iteration 140/1000 | Loss: 0.00002976
Iteration 141/1000 | Loss: 0.00002976
Iteration 142/1000 | Loss: 0.00002976
Iteration 143/1000 | Loss: 0.00002975
Iteration 144/1000 | Loss: 0.00002975
Iteration 145/1000 | Loss: 0.00002975
Iteration 146/1000 | Loss: 0.00002975
Iteration 147/1000 | Loss: 0.00002975
Iteration 148/1000 | Loss: 0.00002975
Iteration 149/1000 | Loss: 0.00002975
Iteration 150/1000 | Loss: 0.00002975
Iteration 151/1000 | Loss: 0.00002975
Iteration 152/1000 | Loss: 0.00002975
Iteration 153/1000 | Loss: 0.00002975
Iteration 154/1000 | Loss: 0.00002975
Iteration 155/1000 | Loss: 0.00002975
Iteration 156/1000 | Loss: 0.00002975
Iteration 157/1000 | Loss: 0.00002975
Iteration 158/1000 | Loss: 0.00002974
Iteration 159/1000 | Loss: 0.00002974
Iteration 160/1000 | Loss: 0.00002974
Iteration 161/1000 | Loss: 0.00002974
Iteration 162/1000 | Loss: 0.00002974
Iteration 163/1000 | Loss: 0.00002974
Iteration 164/1000 | Loss: 0.00002974
Iteration 165/1000 | Loss: 0.00002974
Iteration 166/1000 | Loss: 0.00002974
Iteration 167/1000 | Loss: 0.00002974
Iteration 168/1000 | Loss: 0.00002974
Iteration 169/1000 | Loss: 0.00002974
Iteration 170/1000 | Loss: 0.00002974
Iteration 171/1000 | Loss: 0.00002974
Iteration 172/1000 | Loss: 0.00002974
Iteration 173/1000 | Loss: 0.00002974
Iteration 174/1000 | Loss: 0.00002974
Iteration 175/1000 | Loss: 0.00002974
Iteration 176/1000 | Loss: 0.00002974
Iteration 177/1000 | Loss: 0.00002973
Iteration 178/1000 | Loss: 0.00002973
Iteration 179/1000 | Loss: 0.00002973
Iteration 180/1000 | Loss: 0.00002973
Iteration 181/1000 | Loss: 0.00002973
Iteration 182/1000 | Loss: 0.00002973
Iteration 183/1000 | Loss: 0.00002973
Iteration 184/1000 | Loss: 0.00002973
Iteration 185/1000 | Loss: 0.00002973
Iteration 186/1000 | Loss: 0.00002973
Iteration 187/1000 | Loss: 0.00002973
Iteration 188/1000 | Loss: 0.00002973
Iteration 189/1000 | Loss: 0.00002973
Iteration 190/1000 | Loss: 0.00002973
Iteration 191/1000 | Loss: 0.00002973
Iteration 192/1000 | Loss: 0.00002973
Iteration 193/1000 | Loss: 0.00002973
Iteration 194/1000 | Loss: 0.00002973
Iteration 195/1000 | Loss: 0.00002973
Iteration 196/1000 | Loss: 0.00002972
Iteration 197/1000 | Loss: 0.00002972
Iteration 198/1000 | Loss: 0.00002972
Iteration 199/1000 | Loss: 0.00002972
Iteration 200/1000 | Loss: 0.00002972
Iteration 201/1000 | Loss: 0.00002972
Iteration 202/1000 | Loss: 0.00002972
Iteration 203/1000 | Loss: 0.00002972
Iteration 204/1000 | Loss: 0.00002972
Iteration 205/1000 | Loss: 0.00002972
Iteration 206/1000 | Loss: 0.00002972
Iteration 207/1000 | Loss: 0.00002972
Iteration 208/1000 | Loss: 0.00002972
Iteration 209/1000 | Loss: 0.00002972
Iteration 210/1000 | Loss: 0.00002972
Iteration 211/1000 | Loss: 0.00002972
Iteration 212/1000 | Loss: 0.00002972
Iteration 213/1000 | Loss: 0.00002972
Iteration 214/1000 | Loss: 0.00002972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.972390029754024e-05, 2.972390029754024e-05, 2.972390029754024e-05, 2.972390029754024e-05, 2.972390029754024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.972390029754024e-05

Optimization complete. Final v2v error: 4.4678850173950195 mm

Highest mean error: 5.627467155456543 mm for frame 118

Lowest mean error: 3.626805305480957 mm for frame 20

Saving results

Total time: 58.66320586204529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094954
Iteration 2/25 | Loss: 0.00359374
Iteration 3/25 | Loss: 0.00300674
Iteration 4/25 | Loss: 0.00211639
Iteration 5/25 | Loss: 0.00194310
Iteration 6/25 | Loss: 0.00197368
Iteration 7/25 | Loss: 0.00180378
Iteration 8/25 | Loss: 0.00171178
Iteration 9/25 | Loss: 0.00168043
Iteration 10/25 | Loss: 0.00167377
Iteration 11/25 | Loss: 0.00167147
Iteration 12/25 | Loss: 0.00166965
Iteration 13/25 | Loss: 0.00166338
Iteration 14/25 | Loss: 0.00166283
Iteration 15/25 | Loss: 0.00166275
Iteration 16/25 | Loss: 0.00166275
Iteration 17/25 | Loss: 0.00166275
Iteration 18/25 | Loss: 0.00166275
Iteration 19/25 | Loss: 0.00166275
Iteration 20/25 | Loss: 0.00166275
Iteration 21/25 | Loss: 0.00166274
Iteration 22/25 | Loss: 0.00166274
Iteration 23/25 | Loss: 0.00166274
Iteration 24/25 | Loss: 0.00166274
Iteration 25/25 | Loss: 0.00166274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.41404340
Iteration 2/25 | Loss: 0.00791249
Iteration 3/25 | Loss: 0.00178677
Iteration 4/25 | Loss: 0.00178677
Iteration 5/25 | Loss: 0.00178676
Iteration 6/25 | Loss: 0.00178676
Iteration 7/25 | Loss: 0.00178676
Iteration 8/25 | Loss: 0.00178676
Iteration 9/25 | Loss: 0.00178676
Iteration 10/25 | Loss: 0.00178676
Iteration 11/25 | Loss: 0.00178676
Iteration 12/25 | Loss: 0.00178676
Iteration 13/25 | Loss: 0.00178676
Iteration 14/25 | Loss: 0.00178676
Iteration 15/25 | Loss: 0.00178676
Iteration 16/25 | Loss: 0.00178676
Iteration 17/25 | Loss: 0.00178676
Iteration 18/25 | Loss: 0.00178676
Iteration 19/25 | Loss: 0.00178676
Iteration 20/25 | Loss: 0.00178676
Iteration 21/25 | Loss: 0.00178676
Iteration 22/25 | Loss: 0.00178676
Iteration 23/25 | Loss: 0.00178676
Iteration 24/25 | Loss: 0.00178676
Iteration 25/25 | Loss: 0.00178676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178676
Iteration 2/1000 | Loss: 0.00151029
Iteration 3/1000 | Loss: 0.00324500
Iteration 4/1000 | Loss: 0.00213091
Iteration 5/1000 | Loss: 0.00116341
Iteration 6/1000 | Loss: 0.00139526
Iteration 7/1000 | Loss: 0.00063054
Iteration 8/1000 | Loss: 0.00082228
Iteration 9/1000 | Loss: 0.00052379
Iteration 10/1000 | Loss: 0.00057451
Iteration 11/1000 | Loss: 0.00021252
Iteration 12/1000 | Loss: 0.00016512
Iteration 13/1000 | Loss: 0.00024954
Iteration 14/1000 | Loss: 0.00012716
Iteration 15/1000 | Loss: 0.00037234
Iteration 16/1000 | Loss: 0.00037868
Iteration 17/1000 | Loss: 0.00017109
Iteration 18/1000 | Loss: 0.00026331
Iteration 19/1000 | Loss: 0.00024945
Iteration 20/1000 | Loss: 0.00010887
Iteration 21/1000 | Loss: 0.00009526
Iteration 22/1000 | Loss: 0.00008736
Iteration 23/1000 | Loss: 0.00008051
Iteration 24/1000 | Loss: 0.00007584
Iteration 25/1000 | Loss: 0.00007088
Iteration 26/1000 | Loss: 0.00006848
Iteration 27/1000 | Loss: 0.00006688
Iteration 28/1000 | Loss: 0.00006553
Iteration 29/1000 | Loss: 0.00012471
Iteration 30/1000 | Loss: 0.00010460
Iteration 31/1000 | Loss: 0.00014223
Iteration 32/1000 | Loss: 0.00007173
Iteration 33/1000 | Loss: 0.00015869
Iteration 34/1000 | Loss: 0.00007320
Iteration 35/1000 | Loss: 0.00006587
Iteration 36/1000 | Loss: 0.00006417
Iteration 37/1000 | Loss: 0.00006265
Iteration 38/1000 | Loss: 0.00006122
Iteration 39/1000 | Loss: 0.00006065
Iteration 40/1000 | Loss: 0.00006022
Iteration 41/1000 | Loss: 0.00005961
Iteration 42/1000 | Loss: 0.00005900
Iteration 43/1000 | Loss: 0.00005846
Iteration 44/1000 | Loss: 0.00005801
Iteration 45/1000 | Loss: 0.00005769
Iteration 46/1000 | Loss: 0.00005741
Iteration 47/1000 | Loss: 0.00005726
Iteration 48/1000 | Loss: 0.00005725
Iteration 49/1000 | Loss: 0.00005709
Iteration 50/1000 | Loss: 0.00005688
Iteration 51/1000 | Loss: 0.00005658
Iteration 52/1000 | Loss: 0.00005632
Iteration 53/1000 | Loss: 0.00008456
Iteration 54/1000 | Loss: 0.00007105
Iteration 55/1000 | Loss: 0.00008815
Iteration 56/1000 | Loss: 0.00006969
Iteration 57/1000 | Loss: 0.00008957
Iteration 58/1000 | Loss: 0.00006849
Iteration 59/1000 | Loss: 0.00008833
Iteration 60/1000 | Loss: 0.00006699
Iteration 61/1000 | Loss: 0.00008715
Iteration 62/1000 | Loss: 0.00006542
Iteration 63/1000 | Loss: 0.00008851
Iteration 64/1000 | Loss: 0.00006397
Iteration 65/1000 | Loss: 0.00008666
Iteration 66/1000 | Loss: 0.00006466
Iteration 67/1000 | Loss: 0.00005619
Iteration 68/1000 | Loss: 0.00009045
Iteration 69/1000 | Loss: 0.00006494
Iteration 70/1000 | Loss: 0.00009807
Iteration 71/1000 | Loss: 0.00006513
Iteration 72/1000 | Loss: 0.00009032
Iteration 73/1000 | Loss: 0.00006524
Iteration 74/1000 | Loss: 0.00008023
Iteration 75/1000 | Loss: 0.00009326
Iteration 76/1000 | Loss: 0.00006685
Iteration 77/1000 | Loss: 0.00005942
Iteration 78/1000 | Loss: 0.00005798
Iteration 79/1000 | Loss: 0.00005639
Iteration 80/1000 | Loss: 0.00009048
Iteration 81/1000 | Loss: 0.00007771
Iteration 82/1000 | Loss: 0.00009059
Iteration 83/1000 | Loss: 0.00007272
Iteration 84/1000 | Loss: 0.00009085
Iteration 85/1000 | Loss: 0.00007614
Iteration 86/1000 | Loss: 0.00006200
Iteration 87/1000 | Loss: 0.00008121
Iteration 88/1000 | Loss: 0.00005813
Iteration 89/1000 | Loss: 0.00005764
Iteration 90/1000 | Loss: 0.00009130
Iteration 91/1000 | Loss: 0.00006929
Iteration 92/1000 | Loss: 0.00005928
Iteration 93/1000 | Loss: 0.00008439
Iteration 94/1000 | Loss: 0.00005727
Iteration 95/1000 | Loss: 0.00005665
Iteration 96/1000 | Loss: 0.00005638
Iteration 97/1000 | Loss: 0.00005618
Iteration 98/1000 | Loss: 0.00005603
Iteration 99/1000 | Loss: 0.00005586
Iteration 100/1000 | Loss: 0.00005570
Iteration 101/1000 | Loss: 0.00005560
Iteration 102/1000 | Loss: 0.00005558
Iteration 103/1000 | Loss: 0.00005554
Iteration 104/1000 | Loss: 0.00005553
Iteration 105/1000 | Loss: 0.00005553
Iteration 106/1000 | Loss: 0.00005553
Iteration 107/1000 | Loss: 0.00005553
Iteration 108/1000 | Loss: 0.00005553
Iteration 109/1000 | Loss: 0.00005551
Iteration 110/1000 | Loss: 0.00005551
Iteration 111/1000 | Loss: 0.00005549
Iteration 112/1000 | Loss: 0.00005549
Iteration 113/1000 | Loss: 0.00005546
Iteration 114/1000 | Loss: 0.00005541
Iteration 115/1000 | Loss: 0.00005541
Iteration 116/1000 | Loss: 0.00005541
Iteration 117/1000 | Loss: 0.00005541
Iteration 118/1000 | Loss: 0.00005541
Iteration 119/1000 | Loss: 0.00005541
Iteration 120/1000 | Loss: 0.00005540
Iteration 121/1000 | Loss: 0.00005538
Iteration 122/1000 | Loss: 0.00005538
Iteration 123/1000 | Loss: 0.00005538
Iteration 124/1000 | Loss: 0.00005537
Iteration 125/1000 | Loss: 0.00005537
Iteration 126/1000 | Loss: 0.00005536
Iteration 127/1000 | Loss: 0.00005535
Iteration 128/1000 | Loss: 0.00005535
Iteration 129/1000 | Loss: 0.00005535
Iteration 130/1000 | Loss: 0.00005535
Iteration 131/1000 | Loss: 0.00005534
Iteration 132/1000 | Loss: 0.00005534
Iteration 133/1000 | Loss: 0.00005534
Iteration 134/1000 | Loss: 0.00005534
Iteration 135/1000 | Loss: 0.00005534
Iteration 136/1000 | Loss: 0.00005534
Iteration 137/1000 | Loss: 0.00005534
Iteration 138/1000 | Loss: 0.00005533
Iteration 139/1000 | Loss: 0.00005533
Iteration 140/1000 | Loss: 0.00005533
Iteration 141/1000 | Loss: 0.00005531
Iteration 142/1000 | Loss: 0.00005531
Iteration 143/1000 | Loss: 0.00005531
Iteration 144/1000 | Loss: 0.00005531
Iteration 145/1000 | Loss: 0.00005531
Iteration 146/1000 | Loss: 0.00005531
Iteration 147/1000 | Loss: 0.00005531
Iteration 148/1000 | Loss: 0.00005531
Iteration 149/1000 | Loss: 0.00005531
Iteration 150/1000 | Loss: 0.00005531
Iteration 151/1000 | Loss: 0.00005530
Iteration 152/1000 | Loss: 0.00005530
Iteration 153/1000 | Loss: 0.00005530
Iteration 154/1000 | Loss: 0.00005530
Iteration 155/1000 | Loss: 0.00005529
Iteration 156/1000 | Loss: 0.00005529
Iteration 157/1000 | Loss: 0.00005529
Iteration 158/1000 | Loss: 0.00005529
Iteration 159/1000 | Loss: 0.00005529
Iteration 160/1000 | Loss: 0.00005529
Iteration 161/1000 | Loss: 0.00005529
Iteration 162/1000 | Loss: 0.00005529
Iteration 163/1000 | Loss: 0.00005529
Iteration 164/1000 | Loss: 0.00005529
Iteration 165/1000 | Loss: 0.00005529
Iteration 166/1000 | Loss: 0.00005529
Iteration 167/1000 | Loss: 0.00005529
Iteration 168/1000 | Loss: 0.00005528
Iteration 169/1000 | Loss: 0.00005528
Iteration 170/1000 | Loss: 0.00005528
Iteration 171/1000 | Loss: 0.00005527
Iteration 172/1000 | Loss: 0.00005527
Iteration 173/1000 | Loss: 0.00005527
Iteration 174/1000 | Loss: 0.00005527
Iteration 175/1000 | Loss: 0.00005527
Iteration 176/1000 | Loss: 0.00005527
Iteration 177/1000 | Loss: 0.00005527
Iteration 178/1000 | Loss: 0.00005527
Iteration 179/1000 | Loss: 0.00005527
Iteration 180/1000 | Loss: 0.00005526
Iteration 181/1000 | Loss: 0.00005526
Iteration 182/1000 | Loss: 0.00005526
Iteration 183/1000 | Loss: 0.00005526
Iteration 184/1000 | Loss: 0.00005526
Iteration 185/1000 | Loss: 0.00005526
Iteration 186/1000 | Loss: 0.00005526
Iteration 187/1000 | Loss: 0.00005526
Iteration 188/1000 | Loss: 0.00005526
Iteration 189/1000 | Loss: 0.00005526
Iteration 190/1000 | Loss: 0.00005526
Iteration 191/1000 | Loss: 0.00005526
Iteration 192/1000 | Loss: 0.00005526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [5.5258715292438865e-05, 5.5258715292438865e-05, 5.5258715292438865e-05, 5.5258715292438865e-05, 5.5258715292438865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.5258715292438865e-05

Optimization complete. Final v2v error: 5.841224670410156 mm

Highest mean error: 7.411564826965332 mm for frame 66

Lowest mean error: 4.651281833648682 mm for frame 121

Saving results

Total time: 179.6080732345581
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480246
Iteration 2/25 | Loss: 0.00128582
Iteration 3/25 | Loss: 0.00119502
Iteration 4/25 | Loss: 0.00117946
Iteration 5/25 | Loss: 0.00117273
Iteration 6/25 | Loss: 0.00117140
Iteration 7/25 | Loss: 0.00117140
Iteration 8/25 | Loss: 0.00117140
Iteration 9/25 | Loss: 0.00117140
Iteration 10/25 | Loss: 0.00117140
Iteration 11/25 | Loss: 0.00117140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001171402051113546, 0.001171402051113546, 0.001171402051113546, 0.001171402051113546, 0.001171402051113546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001171402051113546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75899619
Iteration 2/25 | Loss: 0.00124710
Iteration 3/25 | Loss: 0.00124710
Iteration 4/25 | Loss: 0.00124710
Iteration 5/25 | Loss: 0.00124710
Iteration 6/25 | Loss: 0.00124710
Iteration 7/25 | Loss: 0.00124710
Iteration 8/25 | Loss: 0.00124710
Iteration 9/25 | Loss: 0.00124710
Iteration 10/25 | Loss: 0.00124710
Iteration 11/25 | Loss: 0.00124710
Iteration 12/25 | Loss: 0.00124710
Iteration 13/25 | Loss: 0.00124710
Iteration 14/25 | Loss: 0.00124710
Iteration 15/25 | Loss: 0.00124710
Iteration 16/25 | Loss: 0.00124710
Iteration 17/25 | Loss: 0.00124710
Iteration 18/25 | Loss: 0.00124710
Iteration 19/25 | Loss: 0.00124710
Iteration 20/25 | Loss: 0.00124710
Iteration 21/25 | Loss: 0.00124710
Iteration 22/25 | Loss: 0.00124710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001247099251486361, 0.001247099251486361, 0.001247099251486361, 0.001247099251486361, 0.001247099251486361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001247099251486361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124710
Iteration 2/1000 | Loss: 0.00003358
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002077
Iteration 6/1000 | Loss: 0.00002013
Iteration 7/1000 | Loss: 0.00001955
Iteration 8/1000 | Loss: 0.00001916
Iteration 9/1000 | Loss: 0.00001878
Iteration 10/1000 | Loss: 0.00001853
Iteration 11/1000 | Loss: 0.00001829
Iteration 12/1000 | Loss: 0.00001809
Iteration 13/1000 | Loss: 0.00001793
Iteration 14/1000 | Loss: 0.00001792
Iteration 15/1000 | Loss: 0.00001786
Iteration 16/1000 | Loss: 0.00001772
Iteration 17/1000 | Loss: 0.00001763
Iteration 18/1000 | Loss: 0.00001762
Iteration 19/1000 | Loss: 0.00001757
Iteration 20/1000 | Loss: 0.00001751
Iteration 21/1000 | Loss: 0.00001751
Iteration 22/1000 | Loss: 0.00001750
Iteration 23/1000 | Loss: 0.00001747
Iteration 24/1000 | Loss: 0.00001740
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001733
Iteration 27/1000 | Loss: 0.00001733
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001731
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001729
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001723
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001714
Iteration 42/1000 | Loss: 0.00001714
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001713
Iteration 53/1000 | Loss: 0.00001710
Iteration 54/1000 | Loss: 0.00001710
Iteration 55/1000 | Loss: 0.00001709
Iteration 56/1000 | Loss: 0.00001709
Iteration 57/1000 | Loss: 0.00001709
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001709
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001709
Iteration 62/1000 | Loss: 0.00001709
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001709
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001708
Iteration 68/1000 | Loss: 0.00001708
Iteration 69/1000 | Loss: 0.00001708
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001707
Iteration 72/1000 | Loss: 0.00001707
Iteration 73/1000 | Loss: 0.00001707
Iteration 74/1000 | Loss: 0.00001707
Iteration 75/1000 | Loss: 0.00001707
Iteration 76/1000 | Loss: 0.00001707
Iteration 77/1000 | Loss: 0.00001706
Iteration 78/1000 | Loss: 0.00001706
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001700
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001700
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001699
Iteration 101/1000 | Loss: 0.00001699
Iteration 102/1000 | Loss: 0.00001699
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001697
Iteration 105/1000 | Loss: 0.00001697
Iteration 106/1000 | Loss: 0.00001697
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001693
Iteration 110/1000 | Loss: 0.00001693
Iteration 111/1000 | Loss: 0.00001693
Iteration 112/1000 | Loss: 0.00001693
Iteration 113/1000 | Loss: 0.00001693
Iteration 114/1000 | Loss: 0.00001693
Iteration 115/1000 | Loss: 0.00001693
Iteration 116/1000 | Loss: 0.00001693
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001693
Iteration 120/1000 | Loss: 0.00001693
Iteration 121/1000 | Loss: 0.00001693
Iteration 122/1000 | Loss: 0.00001692
Iteration 123/1000 | Loss: 0.00001692
Iteration 124/1000 | Loss: 0.00001691
Iteration 125/1000 | Loss: 0.00001691
Iteration 126/1000 | Loss: 0.00001690
Iteration 127/1000 | Loss: 0.00001690
Iteration 128/1000 | Loss: 0.00001690
Iteration 129/1000 | Loss: 0.00001690
Iteration 130/1000 | Loss: 0.00001690
Iteration 131/1000 | Loss: 0.00001689
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001688
Iteration 136/1000 | Loss: 0.00001688
Iteration 137/1000 | Loss: 0.00001688
Iteration 138/1000 | Loss: 0.00001688
Iteration 139/1000 | Loss: 0.00001688
Iteration 140/1000 | Loss: 0.00001688
Iteration 141/1000 | Loss: 0.00001687
Iteration 142/1000 | Loss: 0.00001687
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001682
Iteration 160/1000 | Loss: 0.00001682
Iteration 161/1000 | Loss: 0.00001682
Iteration 162/1000 | Loss: 0.00001682
Iteration 163/1000 | Loss: 0.00001682
Iteration 164/1000 | Loss: 0.00001681
Iteration 165/1000 | Loss: 0.00001681
Iteration 166/1000 | Loss: 0.00001681
Iteration 167/1000 | Loss: 0.00001681
Iteration 168/1000 | Loss: 0.00001681
Iteration 169/1000 | Loss: 0.00001681
Iteration 170/1000 | Loss: 0.00001681
Iteration 171/1000 | Loss: 0.00001681
Iteration 172/1000 | Loss: 0.00001681
Iteration 173/1000 | Loss: 0.00001680
Iteration 174/1000 | Loss: 0.00001680
Iteration 175/1000 | Loss: 0.00001680
Iteration 176/1000 | Loss: 0.00001680
Iteration 177/1000 | Loss: 0.00001680
Iteration 178/1000 | Loss: 0.00001680
Iteration 179/1000 | Loss: 0.00001680
Iteration 180/1000 | Loss: 0.00001680
Iteration 181/1000 | Loss: 0.00001680
Iteration 182/1000 | Loss: 0.00001680
Iteration 183/1000 | Loss: 0.00001680
Iteration 184/1000 | Loss: 0.00001680
Iteration 185/1000 | Loss: 0.00001679
Iteration 186/1000 | Loss: 0.00001679
Iteration 187/1000 | Loss: 0.00001679
Iteration 188/1000 | Loss: 0.00001679
Iteration 189/1000 | Loss: 0.00001679
Iteration 190/1000 | Loss: 0.00001679
Iteration 191/1000 | Loss: 0.00001679
Iteration 192/1000 | Loss: 0.00001679
Iteration 193/1000 | Loss: 0.00001679
Iteration 194/1000 | Loss: 0.00001679
Iteration 195/1000 | Loss: 0.00001679
Iteration 196/1000 | Loss: 0.00001679
Iteration 197/1000 | Loss: 0.00001678
Iteration 198/1000 | Loss: 0.00001678
Iteration 199/1000 | Loss: 0.00001678
Iteration 200/1000 | Loss: 0.00001678
Iteration 201/1000 | Loss: 0.00001678
Iteration 202/1000 | Loss: 0.00001678
Iteration 203/1000 | Loss: 0.00001678
Iteration 204/1000 | Loss: 0.00001678
Iteration 205/1000 | Loss: 0.00001678
Iteration 206/1000 | Loss: 0.00001678
Iteration 207/1000 | Loss: 0.00001678
Iteration 208/1000 | Loss: 0.00001678
Iteration 209/1000 | Loss: 0.00001678
Iteration 210/1000 | Loss: 0.00001678
Iteration 211/1000 | Loss: 0.00001678
Iteration 212/1000 | Loss: 0.00001678
Iteration 213/1000 | Loss: 0.00001678
Iteration 214/1000 | Loss: 0.00001678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.6782076272647828e-05, 1.6782076272647828e-05, 1.6782076272647828e-05, 1.6782076272647828e-05, 1.6782076272647828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6782076272647828e-05

Optimization complete. Final v2v error: 3.472938299179077 mm

Highest mean error: 4.194394111633301 mm for frame 255

Lowest mean error: 3.2911570072174072 mm for frame 68

Saving results

Total time: 57.78735661506653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835940
Iteration 2/25 | Loss: 0.00142460
Iteration 3/25 | Loss: 0.00119902
Iteration 4/25 | Loss: 0.00116976
Iteration 5/25 | Loss: 0.00116678
Iteration 6/25 | Loss: 0.00116678
Iteration 7/25 | Loss: 0.00116678
Iteration 8/25 | Loss: 0.00116678
Iteration 9/25 | Loss: 0.00116678
Iteration 10/25 | Loss: 0.00116678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011667825747281313, 0.0011667825747281313, 0.0011667825747281313, 0.0011667825747281313, 0.0011667825747281313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011667825747281313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29670620
Iteration 2/25 | Loss: 0.00127052
Iteration 3/25 | Loss: 0.00127051
Iteration 4/25 | Loss: 0.00127051
Iteration 5/25 | Loss: 0.00127051
Iteration 6/25 | Loss: 0.00127051
Iteration 7/25 | Loss: 0.00127051
Iteration 8/25 | Loss: 0.00127051
Iteration 9/25 | Loss: 0.00127051
Iteration 10/25 | Loss: 0.00127051
Iteration 11/25 | Loss: 0.00127051
Iteration 12/25 | Loss: 0.00127051
Iteration 13/25 | Loss: 0.00127051
Iteration 14/25 | Loss: 0.00127051
Iteration 15/25 | Loss: 0.00127051
Iteration 16/25 | Loss: 0.00127051
Iteration 17/25 | Loss: 0.00127051
Iteration 18/25 | Loss: 0.00127051
Iteration 19/25 | Loss: 0.00127051
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00127050606533885, 0.00127050606533885, 0.00127050606533885, 0.00127050606533885, 0.00127050606533885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00127050606533885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127051
Iteration 2/1000 | Loss: 0.00003348
Iteration 3/1000 | Loss: 0.00002197
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001442
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001312
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001204
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001180
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001163
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001143
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001134
Iteration 28/1000 | Loss: 0.00001133
Iteration 29/1000 | Loss: 0.00001131
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001130
Iteration 32/1000 | Loss: 0.00001129
Iteration 33/1000 | Loss: 0.00001129
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001123
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001120
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001117
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001116
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001107
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001105
Iteration 52/1000 | Loss: 0.00001105
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001104
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001101
Iteration 60/1000 | Loss: 0.00001100
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001093
Iteration 67/1000 | Loss: 0.00001093
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001090
Iteration 73/1000 | Loss: 0.00001089
Iteration 74/1000 | Loss: 0.00001089
Iteration 75/1000 | Loss: 0.00001089
Iteration 76/1000 | Loss: 0.00001089
Iteration 77/1000 | Loss: 0.00001088
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001087
Iteration 81/1000 | Loss: 0.00001087
Iteration 82/1000 | Loss: 0.00001087
Iteration 83/1000 | Loss: 0.00001086
Iteration 84/1000 | Loss: 0.00001086
Iteration 85/1000 | Loss: 0.00001086
Iteration 86/1000 | Loss: 0.00001085
Iteration 87/1000 | Loss: 0.00001085
Iteration 88/1000 | Loss: 0.00001085
Iteration 89/1000 | Loss: 0.00001085
Iteration 90/1000 | Loss: 0.00001085
Iteration 91/1000 | Loss: 0.00001085
Iteration 92/1000 | Loss: 0.00001085
Iteration 93/1000 | Loss: 0.00001084
Iteration 94/1000 | Loss: 0.00001084
Iteration 95/1000 | Loss: 0.00001084
Iteration 96/1000 | Loss: 0.00001084
Iteration 97/1000 | Loss: 0.00001084
Iteration 98/1000 | Loss: 0.00001083
Iteration 99/1000 | Loss: 0.00001083
Iteration 100/1000 | Loss: 0.00001083
Iteration 101/1000 | Loss: 0.00001083
Iteration 102/1000 | Loss: 0.00001083
Iteration 103/1000 | Loss: 0.00001083
Iteration 104/1000 | Loss: 0.00001083
Iteration 105/1000 | Loss: 0.00001082
Iteration 106/1000 | Loss: 0.00001082
Iteration 107/1000 | Loss: 0.00001082
Iteration 108/1000 | Loss: 0.00001081
Iteration 109/1000 | Loss: 0.00001081
Iteration 110/1000 | Loss: 0.00001081
Iteration 111/1000 | Loss: 0.00001080
Iteration 112/1000 | Loss: 0.00001080
Iteration 113/1000 | Loss: 0.00001080
Iteration 114/1000 | Loss: 0.00001080
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001080
Iteration 125/1000 | Loss: 0.00001080
Iteration 126/1000 | Loss: 0.00001080
Iteration 127/1000 | Loss: 0.00001080
Iteration 128/1000 | Loss: 0.00001080
Iteration 129/1000 | Loss: 0.00001080
Iteration 130/1000 | Loss: 0.00001080
Iteration 131/1000 | Loss: 0.00001080
Iteration 132/1000 | Loss: 0.00001080
Iteration 133/1000 | Loss: 0.00001080
Iteration 134/1000 | Loss: 0.00001080
Iteration 135/1000 | Loss: 0.00001080
Iteration 136/1000 | Loss: 0.00001080
Iteration 137/1000 | Loss: 0.00001080
Iteration 138/1000 | Loss: 0.00001080
Iteration 139/1000 | Loss: 0.00001080
Iteration 140/1000 | Loss: 0.00001080
Iteration 141/1000 | Loss: 0.00001080
Iteration 142/1000 | Loss: 0.00001080
Iteration 143/1000 | Loss: 0.00001080
Iteration 144/1000 | Loss: 0.00001080
Iteration 145/1000 | Loss: 0.00001080
Iteration 146/1000 | Loss: 0.00001080
Iteration 147/1000 | Loss: 0.00001080
Iteration 148/1000 | Loss: 0.00001080
Iteration 149/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.0799096344271675e-05, 1.0799096344271675e-05, 1.0799096344271675e-05, 1.0799096344271675e-05, 1.0799096344271675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0799096344271675e-05

Optimization complete. Final v2v error: 2.8188223838806152 mm

Highest mean error: 3.454230546951294 mm for frame 132

Lowest mean error: 2.4620320796966553 mm for frame 34

Saving results

Total time: 47.94782257080078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961494
Iteration 2/25 | Loss: 0.00961493
Iteration 3/25 | Loss: 0.00249386
Iteration 4/25 | Loss: 0.00186189
Iteration 5/25 | Loss: 0.00167342
Iteration 6/25 | Loss: 0.00158088
Iteration 7/25 | Loss: 0.00158181
Iteration 8/25 | Loss: 0.00147236
Iteration 9/25 | Loss: 0.00140513
Iteration 10/25 | Loss: 0.00137989
Iteration 11/25 | Loss: 0.00137111
Iteration 12/25 | Loss: 0.00136556
Iteration 13/25 | Loss: 0.00137506
Iteration 14/25 | Loss: 0.00137658
Iteration 15/25 | Loss: 0.00135469
Iteration 16/25 | Loss: 0.00135538
Iteration 17/25 | Loss: 0.00135741
Iteration 18/25 | Loss: 0.00135303
Iteration 19/25 | Loss: 0.00133930
Iteration 20/25 | Loss: 0.00133926
Iteration 21/25 | Loss: 0.00133707
Iteration 22/25 | Loss: 0.00134018
Iteration 23/25 | Loss: 0.00133699
Iteration 24/25 | Loss: 0.00133871
Iteration 25/25 | Loss: 0.00133553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27957666
Iteration 2/25 | Loss: 0.00351725
Iteration 3/25 | Loss: 0.00216491
Iteration 4/25 | Loss: 0.00216491
Iteration 5/25 | Loss: 0.00216491
Iteration 6/25 | Loss: 0.00216491
Iteration 7/25 | Loss: 0.00216491
Iteration 8/25 | Loss: 0.00216491
Iteration 9/25 | Loss: 0.00216491
Iteration 10/25 | Loss: 0.00216491
Iteration 11/25 | Loss: 0.00216491
Iteration 12/25 | Loss: 0.00216491
Iteration 13/25 | Loss: 0.00216491
Iteration 14/25 | Loss: 0.00216491
Iteration 15/25 | Loss: 0.00216491
Iteration 16/25 | Loss: 0.00216491
Iteration 17/25 | Loss: 0.00216491
Iteration 18/25 | Loss: 0.00216491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021649105474352837, 0.0021649105474352837, 0.0021649105474352837, 0.0021649105474352837, 0.0021649105474352837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021649105474352837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216491
Iteration 2/1000 | Loss: 0.00276437
Iteration 3/1000 | Loss: 0.00180806
Iteration 4/1000 | Loss: 0.00116182
Iteration 5/1000 | Loss: 0.00079583
Iteration 6/1000 | Loss: 0.00035052
Iteration 7/1000 | Loss: 0.00022506
Iteration 8/1000 | Loss: 0.00049288
Iteration 9/1000 | Loss: 0.00030516
Iteration 10/1000 | Loss: 0.00035250
Iteration 11/1000 | Loss: 0.00189841
Iteration 12/1000 | Loss: 0.00045719
Iteration 13/1000 | Loss: 0.00054583
Iteration 14/1000 | Loss: 0.00187872
Iteration 15/1000 | Loss: 0.00021094
Iteration 16/1000 | Loss: 0.00095963
Iteration 17/1000 | Loss: 0.00181866
Iteration 18/1000 | Loss: 0.00187492
Iteration 19/1000 | Loss: 0.00174090
Iteration 20/1000 | Loss: 0.00115889
Iteration 21/1000 | Loss: 0.00117033
Iteration 22/1000 | Loss: 0.00116591
Iteration 23/1000 | Loss: 0.00074285
Iteration 24/1000 | Loss: 0.00017727
Iteration 25/1000 | Loss: 0.00023321
Iteration 26/1000 | Loss: 0.00084884
Iteration 27/1000 | Loss: 0.00031126
Iteration 28/1000 | Loss: 0.00016307
Iteration 29/1000 | Loss: 0.00051480
Iteration 30/1000 | Loss: 0.00013591
Iteration 31/1000 | Loss: 0.00048766
Iteration 32/1000 | Loss: 0.00011413
Iteration 33/1000 | Loss: 0.00030300
Iteration 34/1000 | Loss: 0.00018659
Iteration 35/1000 | Loss: 0.00015184
Iteration 36/1000 | Loss: 0.00007178
Iteration 37/1000 | Loss: 0.00010769
Iteration 38/1000 | Loss: 0.00006932
Iteration 39/1000 | Loss: 0.00047428
Iteration 40/1000 | Loss: 0.00009394
Iteration 41/1000 | Loss: 0.00008448
Iteration 42/1000 | Loss: 0.00008674
Iteration 43/1000 | Loss: 0.00004952
Iteration 44/1000 | Loss: 0.00013878
Iteration 45/1000 | Loss: 0.00005625
Iteration 46/1000 | Loss: 0.00007230
Iteration 47/1000 | Loss: 0.00007835
Iteration 48/1000 | Loss: 0.00006477
Iteration 49/1000 | Loss: 0.00007676
Iteration 50/1000 | Loss: 0.00005350
Iteration 51/1000 | Loss: 0.00005452
Iteration 52/1000 | Loss: 0.00006532
Iteration 53/1000 | Loss: 0.00005901
Iteration 54/1000 | Loss: 0.00007342
Iteration 55/1000 | Loss: 0.00008050
Iteration 56/1000 | Loss: 0.00006415
Iteration 57/1000 | Loss: 0.00011545
Iteration 58/1000 | Loss: 0.00032599
Iteration 59/1000 | Loss: 0.00041082
Iteration 60/1000 | Loss: 0.00032651
Iteration 61/1000 | Loss: 0.00009649
Iteration 62/1000 | Loss: 0.00009424
Iteration 63/1000 | Loss: 0.00020029
Iteration 64/1000 | Loss: 0.00007597
Iteration 65/1000 | Loss: 0.00020825
Iteration 66/1000 | Loss: 0.00026932
Iteration 67/1000 | Loss: 0.00017685
Iteration 68/1000 | Loss: 0.00006176
Iteration 69/1000 | Loss: 0.00005235
Iteration 70/1000 | Loss: 0.00005101
Iteration 71/1000 | Loss: 0.00002726
Iteration 72/1000 | Loss: 0.00003043
Iteration 73/1000 | Loss: 0.00005846
Iteration 74/1000 | Loss: 0.00003504
Iteration 75/1000 | Loss: 0.00003931
Iteration 76/1000 | Loss: 0.00003717
Iteration 77/1000 | Loss: 0.00004249
Iteration 78/1000 | Loss: 0.00003466
Iteration 79/1000 | Loss: 0.00003345
Iteration 80/1000 | Loss: 0.00006635
Iteration 81/1000 | Loss: 0.00003660
Iteration 82/1000 | Loss: 0.00002825
Iteration 83/1000 | Loss: 0.00016038
Iteration 84/1000 | Loss: 0.00006485
Iteration 85/1000 | Loss: 0.00005484
Iteration 86/1000 | Loss: 0.00021436
Iteration 87/1000 | Loss: 0.00003631
Iteration 88/1000 | Loss: 0.00004285
Iteration 89/1000 | Loss: 0.00005001
Iteration 90/1000 | Loss: 0.00009275
Iteration 91/1000 | Loss: 0.00056669
Iteration 92/1000 | Loss: 0.00010613
Iteration 93/1000 | Loss: 0.00007893
Iteration 94/1000 | Loss: 0.00004163
Iteration 95/1000 | Loss: 0.00003932
Iteration 96/1000 | Loss: 0.00004126
Iteration 97/1000 | Loss: 0.00005875
Iteration 98/1000 | Loss: 0.00006851
Iteration 99/1000 | Loss: 0.00011242
Iteration 100/1000 | Loss: 0.00003428
Iteration 101/1000 | Loss: 0.00007012
Iteration 102/1000 | Loss: 0.00004043
Iteration 103/1000 | Loss: 0.00003463
Iteration 104/1000 | Loss: 0.00003430
Iteration 105/1000 | Loss: 0.00005065
Iteration 106/1000 | Loss: 0.00007420
Iteration 107/1000 | Loss: 0.00002611
Iteration 108/1000 | Loss: 0.00002896
Iteration 109/1000 | Loss: 0.00002084
Iteration 110/1000 | Loss: 0.00009261
Iteration 111/1000 | Loss: 0.00002066
Iteration 112/1000 | Loss: 0.00002056
Iteration 113/1000 | Loss: 0.00002052
Iteration 114/1000 | Loss: 0.00003416
Iteration 115/1000 | Loss: 0.00016207
Iteration 116/1000 | Loss: 0.00002458
Iteration 117/1000 | Loss: 0.00002046
Iteration 118/1000 | Loss: 0.00002044
Iteration 119/1000 | Loss: 0.00002418
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002035
Iteration 127/1000 | Loss: 0.00002035
Iteration 128/1000 | Loss: 0.00002035
Iteration 129/1000 | Loss: 0.00002035
Iteration 130/1000 | Loss: 0.00002035
Iteration 131/1000 | Loss: 0.00002035
Iteration 132/1000 | Loss: 0.00002035
Iteration 133/1000 | Loss: 0.00002035
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002034
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002033
Iteration 144/1000 | Loss: 0.00002619
Iteration 145/1000 | Loss: 0.00002037
Iteration 146/1000 | Loss: 0.00002026
Iteration 147/1000 | Loss: 0.00002026
Iteration 148/1000 | Loss: 0.00002026
Iteration 149/1000 | Loss: 0.00002026
Iteration 150/1000 | Loss: 0.00002026
Iteration 151/1000 | Loss: 0.00002026
Iteration 152/1000 | Loss: 0.00002026
Iteration 153/1000 | Loss: 0.00002026
Iteration 154/1000 | Loss: 0.00002026
Iteration 155/1000 | Loss: 0.00002026
Iteration 156/1000 | Loss: 0.00002026
Iteration 157/1000 | Loss: 0.00002025
Iteration 158/1000 | Loss: 0.00002025
Iteration 159/1000 | Loss: 0.00002025
Iteration 160/1000 | Loss: 0.00002024
Iteration 161/1000 | Loss: 0.00002024
Iteration 162/1000 | Loss: 0.00002024
Iteration 163/1000 | Loss: 0.00002024
Iteration 164/1000 | Loss: 0.00002024
Iteration 165/1000 | Loss: 0.00002024
Iteration 166/1000 | Loss: 0.00002024
Iteration 167/1000 | Loss: 0.00002024
Iteration 168/1000 | Loss: 0.00002023
Iteration 169/1000 | Loss: 0.00002023
Iteration 170/1000 | Loss: 0.00002023
Iteration 171/1000 | Loss: 0.00002022
Iteration 172/1000 | Loss: 0.00002022
Iteration 173/1000 | Loss: 0.00004685
Iteration 174/1000 | Loss: 0.00003749
Iteration 175/1000 | Loss: 0.00002044
Iteration 176/1000 | Loss: 0.00002222
Iteration 177/1000 | Loss: 0.00002133
Iteration 178/1000 | Loss: 0.00002118
Iteration 179/1000 | Loss: 0.00002118
Iteration 180/1000 | Loss: 0.00002118
Iteration 181/1000 | Loss: 0.00002118
Iteration 182/1000 | Loss: 0.00002118
Iteration 183/1000 | Loss: 0.00002118
Iteration 184/1000 | Loss: 0.00002118
Iteration 185/1000 | Loss: 0.00002118
Iteration 186/1000 | Loss: 0.00002118
Iteration 187/1000 | Loss: 0.00002118
Iteration 188/1000 | Loss: 0.00002118
Iteration 189/1000 | Loss: 0.00002118
Iteration 190/1000 | Loss: 0.00002118
Iteration 191/1000 | Loss: 0.00002118
Iteration 192/1000 | Loss: 0.00002118
Iteration 193/1000 | Loss: 0.00002118
Iteration 194/1000 | Loss: 0.00002118
Iteration 195/1000 | Loss: 0.00002118
Iteration 196/1000 | Loss: 0.00002118
Iteration 197/1000 | Loss: 0.00002118
Iteration 198/1000 | Loss: 0.00002118
Iteration 199/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.1176290829316713e-05, 2.1176290829316713e-05, 2.1176290829316713e-05, 2.1176290829316713e-05, 2.1176290829316713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1176290829316713e-05

Optimization complete. Final v2v error: 3.552039861679077 mm

Highest mean error: 15.189912796020508 mm for frame 108

Lowest mean error: 2.6739931106567383 mm for frame 107

Saving results

Total time: 250.5484778881073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999592
Iteration 2/25 | Loss: 0.00199542
Iteration 3/25 | Loss: 0.00157632
Iteration 4/25 | Loss: 0.00155070
Iteration 5/25 | Loss: 0.00154577
Iteration 6/25 | Loss: 0.00157057
Iteration 7/25 | Loss: 0.00151767
Iteration 8/25 | Loss: 0.00147859
Iteration 9/25 | Loss: 0.00143845
Iteration 10/25 | Loss: 0.00140830
Iteration 11/25 | Loss: 0.00136896
Iteration 12/25 | Loss: 0.00133874
Iteration 13/25 | Loss: 0.00133024
Iteration 14/25 | Loss: 0.00131148
Iteration 15/25 | Loss: 0.00130148
Iteration 16/25 | Loss: 0.00129046
Iteration 17/25 | Loss: 0.00128716
Iteration 18/25 | Loss: 0.00128518
Iteration 19/25 | Loss: 0.00128338
Iteration 20/25 | Loss: 0.00128995
Iteration 21/25 | Loss: 0.00128576
Iteration 22/25 | Loss: 0.00128473
Iteration 23/25 | Loss: 0.00127931
Iteration 24/25 | Loss: 0.00127324
Iteration 25/25 | Loss: 0.00128046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17432642
Iteration 2/25 | Loss: 0.00225692
Iteration 3/25 | Loss: 0.00225691
Iteration 4/25 | Loss: 0.00225690
Iteration 5/25 | Loss: 0.00225690
Iteration 6/25 | Loss: 0.00225690
Iteration 7/25 | Loss: 0.00225690
Iteration 8/25 | Loss: 0.00225690
Iteration 9/25 | Loss: 0.00225690
Iteration 10/25 | Loss: 0.00225690
Iteration 11/25 | Loss: 0.00225690
Iteration 12/25 | Loss: 0.00225690
Iteration 13/25 | Loss: 0.00225690
Iteration 14/25 | Loss: 0.00225690
Iteration 15/25 | Loss: 0.00225690
Iteration 16/25 | Loss: 0.00225690
Iteration 17/25 | Loss: 0.00225690
Iteration 18/25 | Loss: 0.00225690
Iteration 19/25 | Loss: 0.00225690
Iteration 20/25 | Loss: 0.00225690
Iteration 21/25 | Loss: 0.00225690
Iteration 22/25 | Loss: 0.00225690
Iteration 23/25 | Loss: 0.00225690
Iteration 24/25 | Loss: 0.00225690
Iteration 25/25 | Loss: 0.00225690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0022569012362509966, 0.0022569012362509966, 0.0022569012362509966, 0.0022569012362509966, 0.0022569012362509966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022569012362509966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225690
Iteration 2/1000 | Loss: 0.00071051
Iteration 3/1000 | Loss: 0.00038545
Iteration 4/1000 | Loss: 0.00027418
Iteration 5/1000 | Loss: 0.00033716
Iteration 6/1000 | Loss: 0.00024554
Iteration 7/1000 | Loss: 0.00018405
Iteration 8/1000 | Loss: 0.00032920
Iteration 9/1000 | Loss: 0.00044752
Iteration 10/1000 | Loss: 0.00026975
Iteration 11/1000 | Loss: 0.00023642
Iteration 12/1000 | Loss: 0.00076760
Iteration 13/1000 | Loss: 0.00036886
Iteration 14/1000 | Loss: 0.00030277
Iteration 15/1000 | Loss: 0.00031368
Iteration 16/1000 | Loss: 0.00018200
Iteration 17/1000 | Loss: 0.00044679
Iteration 18/1000 | Loss: 0.00028253
Iteration 19/1000 | Loss: 0.00014122
Iteration 20/1000 | Loss: 0.00016030
Iteration 21/1000 | Loss: 0.00017174
Iteration 22/1000 | Loss: 0.00025606
Iteration 23/1000 | Loss: 0.00028547
Iteration 24/1000 | Loss: 0.00034112
Iteration 25/1000 | Loss: 0.00054443
Iteration 26/1000 | Loss: 0.00053576
Iteration 27/1000 | Loss: 0.00056491
Iteration 28/1000 | Loss: 0.00066432
Iteration 29/1000 | Loss: 0.00063214
Iteration 30/1000 | Loss: 0.00030691
Iteration 31/1000 | Loss: 0.00047315
Iteration 32/1000 | Loss: 0.00010544
Iteration 33/1000 | Loss: 0.00045770
Iteration 34/1000 | Loss: 0.00056300
Iteration 35/1000 | Loss: 0.00026752
Iteration 36/1000 | Loss: 0.00021034
Iteration 37/1000 | Loss: 0.00025074
Iteration 38/1000 | Loss: 0.00067010
Iteration 39/1000 | Loss: 0.00042585
Iteration 40/1000 | Loss: 0.00019253
Iteration 41/1000 | Loss: 0.00072798
Iteration 42/1000 | Loss: 0.00033947
Iteration 43/1000 | Loss: 0.00061959
Iteration 44/1000 | Loss: 0.00033410
Iteration 45/1000 | Loss: 0.00028008
Iteration 46/1000 | Loss: 0.00023901
Iteration 47/1000 | Loss: 0.00025474
Iteration 48/1000 | Loss: 0.00051146
Iteration 49/1000 | Loss: 0.00031401
Iteration 50/1000 | Loss: 0.00026275
Iteration 51/1000 | Loss: 0.00022262
Iteration 52/1000 | Loss: 0.00018090
Iteration 53/1000 | Loss: 0.00019984
Iteration 54/1000 | Loss: 0.00020180
Iteration 55/1000 | Loss: 0.00017007
Iteration 56/1000 | Loss: 0.00021960
Iteration 57/1000 | Loss: 0.00016655
Iteration 58/1000 | Loss: 0.00017593
Iteration 59/1000 | Loss: 0.00013425
Iteration 60/1000 | Loss: 0.00020888
Iteration 61/1000 | Loss: 0.00012896
Iteration 62/1000 | Loss: 0.00011911
Iteration 63/1000 | Loss: 0.00009252
Iteration 64/1000 | Loss: 0.00014562
Iteration 65/1000 | Loss: 0.00014105
Iteration 66/1000 | Loss: 0.00014670
Iteration 67/1000 | Loss: 0.00016175
Iteration 68/1000 | Loss: 0.00015482
Iteration 69/1000 | Loss: 0.00008833
Iteration 70/1000 | Loss: 0.00022623
Iteration 71/1000 | Loss: 0.00033241
Iteration 72/1000 | Loss: 0.00008962
Iteration 73/1000 | Loss: 0.00009410
Iteration 74/1000 | Loss: 0.00008042
Iteration 75/1000 | Loss: 0.00008250
Iteration 76/1000 | Loss: 0.00008126
Iteration 77/1000 | Loss: 0.00008116
Iteration 78/1000 | Loss: 0.00007736
Iteration 79/1000 | Loss: 0.00007586
Iteration 80/1000 | Loss: 0.00007361
Iteration 81/1000 | Loss: 0.00049470
Iteration 82/1000 | Loss: 0.00041444
Iteration 83/1000 | Loss: 0.00046467
Iteration 84/1000 | Loss: 0.00039006
Iteration 85/1000 | Loss: 0.00050769
Iteration 86/1000 | Loss: 0.00013270
Iteration 87/1000 | Loss: 0.00015309
Iteration 88/1000 | Loss: 0.00016151
Iteration 89/1000 | Loss: 0.00016816
Iteration 90/1000 | Loss: 0.00011373
Iteration 91/1000 | Loss: 0.00011727
Iteration 92/1000 | Loss: 0.00011735
Iteration 93/1000 | Loss: 0.00013899
Iteration 94/1000 | Loss: 0.00012106
Iteration 95/1000 | Loss: 0.00012878
Iteration 96/1000 | Loss: 0.00014740
Iteration 97/1000 | Loss: 0.00021968
Iteration 98/1000 | Loss: 0.00017773
Iteration 99/1000 | Loss: 0.00011398
Iteration 100/1000 | Loss: 0.00017581
Iteration 101/1000 | Loss: 0.00010990
Iteration 102/1000 | Loss: 0.00010943
Iteration 103/1000 | Loss: 0.00012034
Iteration 104/1000 | Loss: 0.00012752
Iteration 105/1000 | Loss: 0.00017278
Iteration 106/1000 | Loss: 0.00015719
Iteration 107/1000 | Loss: 0.00021307
Iteration 108/1000 | Loss: 0.00017306
Iteration 109/1000 | Loss: 0.00015234
Iteration 110/1000 | Loss: 0.00013812
Iteration 111/1000 | Loss: 0.00014746
Iteration 112/1000 | Loss: 0.00011824
Iteration 113/1000 | Loss: 0.00018334
Iteration 114/1000 | Loss: 0.00017517
Iteration 115/1000 | Loss: 0.00016323
Iteration 116/1000 | Loss: 0.00013882
Iteration 117/1000 | Loss: 0.00014604
Iteration 118/1000 | Loss: 0.00014127
Iteration 119/1000 | Loss: 0.00013228
Iteration 120/1000 | Loss: 0.00014309
Iteration 121/1000 | Loss: 0.00012808
Iteration 122/1000 | Loss: 0.00013733
Iteration 123/1000 | Loss: 0.00017067
Iteration 124/1000 | Loss: 0.00015266
Iteration 125/1000 | Loss: 0.00013007
Iteration 126/1000 | Loss: 0.00014481
Iteration 127/1000 | Loss: 0.00011674
Iteration 128/1000 | Loss: 0.00013151
Iteration 129/1000 | Loss: 0.00011254
Iteration 130/1000 | Loss: 0.00006324
Iteration 131/1000 | Loss: 0.00005399
Iteration 132/1000 | Loss: 0.00006865
Iteration 133/1000 | Loss: 0.00007170
Iteration 134/1000 | Loss: 0.00007239
Iteration 135/1000 | Loss: 0.00006451
Iteration 136/1000 | Loss: 0.00005605
Iteration 137/1000 | Loss: 0.00006712
Iteration 138/1000 | Loss: 0.00007467
Iteration 139/1000 | Loss: 0.00006056
Iteration 140/1000 | Loss: 0.00007043
Iteration 141/1000 | Loss: 0.00006867
Iteration 142/1000 | Loss: 0.00006995
Iteration 143/1000 | Loss: 0.00007612
Iteration 144/1000 | Loss: 0.00007292
Iteration 145/1000 | Loss: 0.00007462
Iteration 146/1000 | Loss: 0.00017938
Iteration 147/1000 | Loss: 0.00007656
Iteration 148/1000 | Loss: 0.00008441
Iteration 149/1000 | Loss: 0.00007609
Iteration 150/1000 | Loss: 0.00008057
Iteration 151/1000 | Loss: 0.00008363
Iteration 152/1000 | Loss: 0.00007571
Iteration 153/1000 | Loss: 0.00008137
Iteration 154/1000 | Loss: 0.00007389
Iteration 155/1000 | Loss: 0.00018672
Iteration 156/1000 | Loss: 0.00015065
Iteration 157/1000 | Loss: 0.00019194
Iteration 158/1000 | Loss: 0.00013485
Iteration 159/1000 | Loss: 0.00009715
Iteration 160/1000 | Loss: 0.00007103
Iteration 161/1000 | Loss: 0.00007078
Iteration 162/1000 | Loss: 0.00009195
Iteration 163/1000 | Loss: 0.00011712
Iteration 164/1000 | Loss: 0.00009453
Iteration 165/1000 | Loss: 0.00020702
Iteration 166/1000 | Loss: 0.00015645
Iteration 167/1000 | Loss: 0.00016384
Iteration 168/1000 | Loss: 0.00011729
Iteration 169/1000 | Loss: 0.00017532
Iteration 170/1000 | Loss: 0.00013239
Iteration 171/1000 | Loss: 0.00010871
Iteration 172/1000 | Loss: 0.00021316
Iteration 173/1000 | Loss: 0.00007284
Iteration 174/1000 | Loss: 0.00007412
Iteration 175/1000 | Loss: 0.00006138
Iteration 176/1000 | Loss: 0.00006389
Iteration 177/1000 | Loss: 0.00005526
Iteration 178/1000 | Loss: 0.00005044
Iteration 179/1000 | Loss: 0.00006534
Iteration 180/1000 | Loss: 0.00006109
Iteration 181/1000 | Loss: 0.00007116
Iteration 182/1000 | Loss: 0.00007584
Iteration 183/1000 | Loss: 0.00007032
Iteration 184/1000 | Loss: 0.00006735
Iteration 185/1000 | Loss: 0.00007063
Iteration 186/1000 | Loss: 0.00007150
Iteration 187/1000 | Loss: 0.00007622
Iteration 188/1000 | Loss: 0.00006793
Iteration 189/1000 | Loss: 0.00008298
Iteration 190/1000 | Loss: 0.00006866
Iteration 191/1000 | Loss: 0.00006315
Iteration 192/1000 | Loss: 0.00007107
Iteration 193/1000 | Loss: 0.00007363
Iteration 194/1000 | Loss: 0.00007889
Iteration 195/1000 | Loss: 0.00007559
Iteration 196/1000 | Loss: 0.00007185
Iteration 197/1000 | Loss: 0.00007613
Iteration 198/1000 | Loss: 0.00007310
Iteration 199/1000 | Loss: 0.00007910
Iteration 200/1000 | Loss: 0.00007789
Iteration 201/1000 | Loss: 0.00008946
Iteration 202/1000 | Loss: 0.00007215
Iteration 203/1000 | Loss: 0.00007185
Iteration 204/1000 | Loss: 0.00006184
Iteration 205/1000 | Loss: 0.00008598
Iteration 206/1000 | Loss: 0.00006507
Iteration 207/1000 | Loss: 0.00007944
Iteration 208/1000 | Loss: 0.00006544
Iteration 209/1000 | Loss: 0.00008415
Iteration 210/1000 | Loss: 0.00006477
Iteration 211/1000 | Loss: 0.00006038
Iteration 212/1000 | Loss: 0.00006051
Iteration 213/1000 | Loss: 0.00007404
Iteration 214/1000 | Loss: 0.00007168
Iteration 215/1000 | Loss: 0.00017373
Iteration 216/1000 | Loss: 0.00018890
Iteration 217/1000 | Loss: 0.00017538
Iteration 218/1000 | Loss: 0.00013960
Iteration 219/1000 | Loss: 0.00017065
Iteration 220/1000 | Loss: 0.00011455
Iteration 221/1000 | Loss: 0.00018226
Iteration 222/1000 | Loss: 0.00007964
Iteration 223/1000 | Loss: 0.00007170
Iteration 224/1000 | Loss: 0.00006647
Iteration 225/1000 | Loss: 0.00004817
Iteration 226/1000 | Loss: 0.00005584
Iteration 227/1000 | Loss: 0.00005146
Iteration 228/1000 | Loss: 0.00021238
Iteration 229/1000 | Loss: 0.00008521
Iteration 230/1000 | Loss: 0.00010759
Iteration 231/1000 | Loss: 0.00013632
Iteration 232/1000 | Loss: 0.00014361
Iteration 233/1000 | Loss: 0.00012342
Iteration 234/1000 | Loss: 0.00005681
Iteration 235/1000 | Loss: 0.00005518
Iteration 236/1000 | Loss: 0.00014261
Iteration 237/1000 | Loss: 0.00014645
Iteration 238/1000 | Loss: 0.00006396
Iteration 239/1000 | Loss: 0.00007438
Iteration 240/1000 | Loss: 0.00006254
Iteration 241/1000 | Loss: 0.00008717
Iteration 242/1000 | Loss: 0.00014093
Iteration 243/1000 | Loss: 0.00014412
Iteration 244/1000 | Loss: 0.00012347
Iteration 245/1000 | Loss: 0.00021260
Iteration 246/1000 | Loss: 0.00005936
Iteration 247/1000 | Loss: 0.00006040
Iteration 248/1000 | Loss: 0.00009548
Iteration 249/1000 | Loss: 0.00005684
Iteration 250/1000 | Loss: 0.00005587
Iteration 251/1000 | Loss: 0.00005398
Iteration 252/1000 | Loss: 0.00005408
Iteration 253/1000 | Loss: 0.00005380
Iteration 254/1000 | Loss: 0.00005381
Iteration 255/1000 | Loss: 0.00015398
Iteration 256/1000 | Loss: 0.00011813
Iteration 257/1000 | Loss: 0.00011569
Iteration 258/1000 | Loss: 0.00006708
Iteration 259/1000 | Loss: 0.00008301
Iteration 260/1000 | Loss: 0.00006380
Iteration 261/1000 | Loss: 0.00006357
Iteration 262/1000 | Loss: 0.00006189
Iteration 263/1000 | Loss: 0.00005666
Iteration 264/1000 | Loss: 0.00017363
Iteration 265/1000 | Loss: 0.00015593
Iteration 266/1000 | Loss: 0.00008344
Iteration 267/1000 | Loss: 0.00010154
Iteration 268/1000 | Loss: 0.00005824
Iteration 269/1000 | Loss: 0.00005725
Iteration 270/1000 | Loss: 0.00005878
Iteration 271/1000 | Loss: 0.00005525
Iteration 272/1000 | Loss: 0.00010506
Iteration 273/1000 | Loss: 0.00011528
Iteration 274/1000 | Loss: 0.00017664
Iteration 275/1000 | Loss: 0.00014880
Iteration 276/1000 | Loss: 0.00020103
Iteration 277/1000 | Loss: 0.00015035
Iteration 278/1000 | Loss: 0.00008451
Iteration 279/1000 | Loss: 0.00011252
Iteration 280/1000 | Loss: 0.00006647
Iteration 281/1000 | Loss: 0.00010908
Iteration 282/1000 | Loss: 0.00017154
Iteration 283/1000 | Loss: 0.00033938
Iteration 284/1000 | Loss: 0.00024134
Iteration 285/1000 | Loss: 0.00013878
Iteration 286/1000 | Loss: 0.00022145
Iteration 287/1000 | Loss: 0.00015988
Iteration 288/1000 | Loss: 0.00020948
Iteration 289/1000 | Loss: 0.00012482
Iteration 290/1000 | Loss: 0.00007532
Iteration 291/1000 | Loss: 0.00006851
Iteration 292/1000 | Loss: 0.00005329
Iteration 293/1000 | Loss: 0.00006418
Iteration 294/1000 | Loss: 0.00006740
Iteration 295/1000 | Loss: 0.00006301
Iteration 296/1000 | Loss: 0.00008312
Iteration 297/1000 | Loss: 0.00006559
Iteration 298/1000 | Loss: 0.00007002
Iteration 299/1000 | Loss: 0.00006129
Iteration 300/1000 | Loss: 0.00006131
Iteration 301/1000 | Loss: 0.00006554
Iteration 302/1000 | Loss: 0.00005162
Iteration 303/1000 | Loss: 0.00006987
Iteration 304/1000 | Loss: 0.00007961
Iteration 305/1000 | Loss: 0.00006476
Iteration 306/1000 | Loss: 0.00005785
Iteration 307/1000 | Loss: 0.00007635
Iteration 308/1000 | Loss: 0.00007528
Iteration 309/1000 | Loss: 0.00007708
Iteration 310/1000 | Loss: 0.00007277
Iteration 311/1000 | Loss: 0.00005614
Iteration 312/1000 | Loss: 0.00005335
Iteration 313/1000 | Loss: 0.00005355
Iteration 314/1000 | Loss: 0.00005683
Iteration 315/1000 | Loss: 0.00005640
Iteration 316/1000 | Loss: 0.00006056
Iteration 317/1000 | Loss: 0.00004358
Iteration 318/1000 | Loss: 0.00005686
Iteration 319/1000 | Loss: 0.00005828
Iteration 320/1000 | Loss: 0.00005332
Iteration 321/1000 | Loss: 0.00004376
Iteration 322/1000 | Loss: 0.00004228
Iteration 323/1000 | Loss: 0.00004241
Iteration 324/1000 | Loss: 0.00004082
Iteration 325/1000 | Loss: 0.00018987
Iteration 326/1000 | Loss: 0.00018984
Iteration 327/1000 | Loss: 0.00010374
Iteration 328/1000 | Loss: 0.00006554
Iteration 329/1000 | Loss: 0.00004279
Iteration 330/1000 | Loss: 0.00004155
Iteration 331/1000 | Loss: 0.00004050
Iteration 332/1000 | Loss: 0.00004275
Iteration 333/1000 | Loss: 0.00003912
Iteration 334/1000 | Loss: 0.00004765
Iteration 335/1000 | Loss: 0.00003863
Iteration 336/1000 | Loss: 0.00003861
Iteration 337/1000 | Loss: 0.00003860
Iteration 338/1000 | Loss: 0.00003860
Iteration 339/1000 | Loss: 0.00003859
Iteration 340/1000 | Loss: 0.00003859
Iteration 341/1000 | Loss: 0.00003858
Iteration 342/1000 | Loss: 0.00003858
Iteration 343/1000 | Loss: 0.00003858
Iteration 344/1000 | Loss: 0.00003857
Iteration 345/1000 | Loss: 0.00003857
Iteration 346/1000 | Loss: 0.00003850
Iteration 347/1000 | Loss: 0.00003849
Iteration 348/1000 | Loss: 0.00003842
Iteration 349/1000 | Loss: 0.00003823
Iteration 350/1000 | Loss: 0.00003822
Iteration 351/1000 | Loss: 0.00003821
Iteration 352/1000 | Loss: 0.00003818
Iteration 353/1000 | Loss: 0.00003818
Iteration 354/1000 | Loss: 0.00003817
Iteration 355/1000 | Loss: 0.00003817
Iteration 356/1000 | Loss: 0.00003816
Iteration 357/1000 | Loss: 0.00003816
Iteration 358/1000 | Loss: 0.00003813
Iteration 359/1000 | Loss: 0.00003813
Iteration 360/1000 | Loss: 0.00003812
Iteration 361/1000 | Loss: 0.00003812
Iteration 362/1000 | Loss: 0.00003811
Iteration 363/1000 | Loss: 0.00003810
Iteration 364/1000 | Loss: 0.00003810
Iteration 365/1000 | Loss: 0.00003810
Iteration 366/1000 | Loss: 0.00003809
Iteration 367/1000 | Loss: 0.00003804
Iteration 368/1000 | Loss: 0.00003799
Iteration 369/1000 | Loss: 0.00003797
Iteration 370/1000 | Loss: 0.00003797
Iteration 371/1000 | Loss: 0.00003795
Iteration 372/1000 | Loss: 0.00003785
Iteration 373/1000 | Loss: 0.00003778
Iteration 374/1000 | Loss: 0.00003777
Iteration 375/1000 | Loss: 0.00003777
Iteration 376/1000 | Loss: 0.00020003
Iteration 377/1000 | Loss: 0.00011008
Iteration 378/1000 | Loss: 0.00015066
Iteration 379/1000 | Loss: 0.00004090
Iteration 380/1000 | Loss: 0.00003858
Iteration 381/1000 | Loss: 0.00003792
Iteration 382/1000 | Loss: 0.00003724
Iteration 383/1000 | Loss: 0.00003671
Iteration 384/1000 | Loss: 0.00003651
Iteration 385/1000 | Loss: 0.00003644
Iteration 386/1000 | Loss: 0.00003644
Iteration 387/1000 | Loss: 0.00003642
Iteration 388/1000 | Loss: 0.00003642
Iteration 389/1000 | Loss: 0.00003641
Iteration 390/1000 | Loss: 0.00003637
Iteration 391/1000 | Loss: 0.00003637
Iteration 392/1000 | Loss: 0.00003637
Iteration 393/1000 | Loss: 0.00003637
Iteration 394/1000 | Loss: 0.00003636
Iteration 395/1000 | Loss: 0.00003636
Iteration 396/1000 | Loss: 0.00003636
Iteration 397/1000 | Loss: 0.00003636
Iteration 398/1000 | Loss: 0.00003636
Iteration 399/1000 | Loss: 0.00003636
Iteration 400/1000 | Loss: 0.00003636
Iteration 401/1000 | Loss: 0.00003636
Iteration 402/1000 | Loss: 0.00003635
Iteration 403/1000 | Loss: 0.00003634
Iteration 404/1000 | Loss: 0.00003632
Iteration 405/1000 | Loss: 0.00003632
Iteration 406/1000 | Loss: 0.00003632
Iteration 407/1000 | Loss: 0.00003632
Iteration 408/1000 | Loss: 0.00003631
Iteration 409/1000 | Loss: 0.00003631
Iteration 410/1000 | Loss: 0.00003631
Iteration 411/1000 | Loss: 0.00003631
Iteration 412/1000 | Loss: 0.00003630
Iteration 413/1000 | Loss: 0.00003630
Iteration 414/1000 | Loss: 0.00003630
Iteration 415/1000 | Loss: 0.00003630
Iteration 416/1000 | Loss: 0.00003630
Iteration 417/1000 | Loss: 0.00003629
Iteration 418/1000 | Loss: 0.00003629
Iteration 419/1000 | Loss: 0.00003629
Iteration 420/1000 | Loss: 0.00003628
Iteration 421/1000 | Loss: 0.00003628
Iteration 422/1000 | Loss: 0.00003628
Iteration 423/1000 | Loss: 0.00003628
Iteration 424/1000 | Loss: 0.00003627
Iteration 425/1000 | Loss: 0.00003627
Iteration 426/1000 | Loss: 0.00003627
Iteration 427/1000 | Loss: 0.00003627
Iteration 428/1000 | Loss: 0.00003626
Iteration 429/1000 | Loss: 0.00003626
Iteration 430/1000 | Loss: 0.00003626
Iteration 431/1000 | Loss: 0.00003626
Iteration 432/1000 | Loss: 0.00003626
Iteration 433/1000 | Loss: 0.00003626
Iteration 434/1000 | Loss: 0.00003626
Iteration 435/1000 | Loss: 0.00003625
Iteration 436/1000 | Loss: 0.00003625
Iteration 437/1000 | Loss: 0.00003625
Iteration 438/1000 | Loss: 0.00003623
Iteration 439/1000 | Loss: 0.00020520
Iteration 440/1000 | Loss: 0.00010395
Iteration 441/1000 | Loss: 0.00016099
Iteration 442/1000 | Loss: 0.00032908
Iteration 443/1000 | Loss: 0.00021683
Iteration 444/1000 | Loss: 0.00016365
Iteration 445/1000 | Loss: 0.00016420
Iteration 446/1000 | Loss: 0.00005046
Iteration 447/1000 | Loss: 0.00004245
Iteration 448/1000 | Loss: 0.00015062
Iteration 449/1000 | Loss: 0.00004007
Iteration 450/1000 | Loss: 0.00003737
Iteration 451/1000 | Loss: 0.00003607
Iteration 452/1000 | Loss: 0.00003475
Iteration 453/1000 | Loss: 0.00003410
Iteration 454/1000 | Loss: 0.00003378
Iteration 455/1000 | Loss: 0.00003369
Iteration 456/1000 | Loss: 0.00003368
Iteration 457/1000 | Loss: 0.00003365
Iteration 458/1000 | Loss: 0.00003361
Iteration 459/1000 | Loss: 0.00003353
Iteration 460/1000 | Loss: 0.00003349
Iteration 461/1000 | Loss: 0.00003348
Iteration 462/1000 | Loss: 0.00003348
Iteration 463/1000 | Loss: 0.00003347
Iteration 464/1000 | Loss: 0.00003346
Iteration 465/1000 | Loss: 0.00003346
Iteration 466/1000 | Loss: 0.00003343
Iteration 467/1000 | Loss: 0.00003339
Iteration 468/1000 | Loss: 0.00003335
Iteration 469/1000 | Loss: 0.00003331
Iteration 470/1000 | Loss: 0.00003329
Iteration 471/1000 | Loss: 0.00003327
Iteration 472/1000 | Loss: 0.00030143
Iteration 473/1000 | Loss: 0.00010809
Iteration 474/1000 | Loss: 0.00026675
Iteration 475/1000 | Loss: 0.00104872
Iteration 476/1000 | Loss: 0.00207825
Iteration 477/1000 | Loss: 0.00132042
Iteration 478/1000 | Loss: 0.00167380
Iteration 479/1000 | Loss: 0.00009408
Iteration 480/1000 | Loss: 0.00006495
Iteration 481/1000 | Loss: 0.00004138
Iteration 482/1000 | Loss: 0.00004553
Iteration 483/1000 | Loss: 0.00078846
Iteration 484/1000 | Loss: 0.00052168
Iteration 485/1000 | Loss: 0.00097184
Iteration 486/1000 | Loss: 0.00073198
Iteration 487/1000 | Loss: 0.00046882
Iteration 488/1000 | Loss: 0.00064203
Iteration 489/1000 | Loss: 0.00005879
Iteration 490/1000 | Loss: 0.00012843
Iteration 491/1000 | Loss: 0.00003719
Iteration 492/1000 | Loss: 0.00003079
Iteration 493/1000 | Loss: 0.00002762
Iteration 494/1000 | Loss: 0.00002526
Iteration 495/1000 | Loss: 0.00002371
Iteration 496/1000 | Loss: 0.00054546
Iteration 497/1000 | Loss: 0.00015357
Iteration 498/1000 | Loss: 0.00002271
Iteration 499/1000 | Loss: 0.00048868
Iteration 500/1000 | Loss: 0.00031638
Iteration 501/1000 | Loss: 0.00003208
Iteration 502/1000 | Loss: 0.00047563
Iteration 503/1000 | Loss: 0.00015315
Iteration 504/1000 | Loss: 0.00002314
Iteration 505/1000 | Loss: 0.00002202
Iteration 506/1000 | Loss: 0.00044747
Iteration 507/1000 | Loss: 0.00015683
Iteration 508/1000 | Loss: 0.00042807
Iteration 509/1000 | Loss: 0.00017241
Iteration 510/1000 | Loss: 0.00036034
Iteration 511/1000 | Loss: 0.00009732
Iteration 512/1000 | Loss: 0.00007987
Iteration 513/1000 | Loss: 0.00002384
Iteration 514/1000 | Loss: 0.00003480
Iteration 515/1000 | Loss: 0.00002098
Iteration 516/1000 | Loss: 0.00002063
Iteration 517/1000 | Loss: 0.00002027
Iteration 518/1000 | Loss: 0.00051115
Iteration 519/1000 | Loss: 0.00002289
Iteration 520/1000 | Loss: 0.00001977
Iteration 521/1000 | Loss: 0.00001860
Iteration 522/1000 | Loss: 0.00001810
Iteration 523/1000 | Loss: 0.00001753
Iteration 524/1000 | Loss: 0.00001717
Iteration 525/1000 | Loss: 0.00001704
Iteration 526/1000 | Loss: 0.00001693
Iteration 527/1000 | Loss: 0.00001679
Iteration 528/1000 | Loss: 0.00001677
Iteration 529/1000 | Loss: 0.00001675
Iteration 530/1000 | Loss: 0.00001674
Iteration 531/1000 | Loss: 0.00001674
Iteration 532/1000 | Loss: 0.00001674
Iteration 533/1000 | Loss: 0.00001673
Iteration 534/1000 | Loss: 0.00001668
Iteration 535/1000 | Loss: 0.00001667
Iteration 536/1000 | Loss: 0.00001666
Iteration 537/1000 | Loss: 0.00001666
Iteration 538/1000 | Loss: 0.00001665
Iteration 539/1000 | Loss: 0.00001665
Iteration 540/1000 | Loss: 0.00001665
Iteration 541/1000 | Loss: 0.00001664
Iteration 542/1000 | Loss: 0.00001664
Iteration 543/1000 | Loss: 0.00001663
Iteration 544/1000 | Loss: 0.00001663
Iteration 545/1000 | Loss: 0.00001661
Iteration 546/1000 | Loss: 0.00001661
Iteration 547/1000 | Loss: 0.00001660
Iteration 548/1000 | Loss: 0.00001660
Iteration 549/1000 | Loss: 0.00001660
Iteration 550/1000 | Loss: 0.00001659
Iteration 551/1000 | Loss: 0.00001659
Iteration 552/1000 | Loss: 0.00001659
Iteration 553/1000 | Loss: 0.00001658
Iteration 554/1000 | Loss: 0.00001658
Iteration 555/1000 | Loss: 0.00001658
Iteration 556/1000 | Loss: 0.00001657
Iteration 557/1000 | Loss: 0.00001657
Iteration 558/1000 | Loss: 0.00001657
Iteration 559/1000 | Loss: 0.00001657
Iteration 560/1000 | Loss: 0.00001657
Iteration 561/1000 | Loss: 0.00001657
Iteration 562/1000 | Loss: 0.00001657
Iteration 563/1000 | Loss: 0.00001657
Iteration 564/1000 | Loss: 0.00001656
Iteration 565/1000 | Loss: 0.00001656
Iteration 566/1000 | Loss: 0.00001656
Iteration 567/1000 | Loss: 0.00001656
Iteration 568/1000 | Loss: 0.00001656
Iteration 569/1000 | Loss: 0.00001656
Iteration 570/1000 | Loss: 0.00001656
Iteration 571/1000 | Loss: 0.00001656
Iteration 572/1000 | Loss: 0.00001656
Iteration 573/1000 | Loss: 0.00001656
Iteration 574/1000 | Loss: 0.00001656
Iteration 575/1000 | Loss: 0.00001656
Iteration 576/1000 | Loss: 0.00001656
Iteration 577/1000 | Loss: 0.00001656
Iteration 578/1000 | Loss: 0.00001656
Iteration 579/1000 | Loss: 0.00001656
Iteration 580/1000 | Loss: 0.00001656
Iteration 581/1000 | Loss: 0.00001656
Iteration 582/1000 | Loss: 0.00001656
Iteration 583/1000 | Loss: 0.00001655
Iteration 584/1000 | Loss: 0.00001655
Iteration 585/1000 | Loss: 0.00001655
Iteration 586/1000 | Loss: 0.00001655
Iteration 587/1000 | Loss: 0.00001655
Iteration 588/1000 | Loss: 0.00001655
Iteration 589/1000 | Loss: 0.00001655
Iteration 590/1000 | Loss: 0.00001655
Iteration 591/1000 | Loss: 0.00001655
Iteration 592/1000 | Loss: 0.00001655
Iteration 593/1000 | Loss: 0.00001655
Iteration 594/1000 | Loss: 0.00001655
Iteration 595/1000 | Loss: 0.00001655
Iteration 596/1000 | Loss: 0.00001655
Iteration 597/1000 | Loss: 0.00001655
Iteration 598/1000 | Loss: 0.00001655
Iteration 599/1000 | Loss: 0.00001655
Iteration 600/1000 | Loss: 0.00001655
Iteration 601/1000 | Loss: 0.00001655
Iteration 602/1000 | Loss: 0.00001655
Iteration 603/1000 | Loss: 0.00001654
Iteration 604/1000 | Loss: 0.00001654
Iteration 605/1000 | Loss: 0.00001654
Iteration 606/1000 | Loss: 0.00001654
Iteration 607/1000 | Loss: 0.00001654
Iteration 608/1000 | Loss: 0.00001654
Iteration 609/1000 | Loss: 0.00001654
Iteration 610/1000 | Loss: 0.00001654
Iteration 611/1000 | Loss: 0.00001654
Iteration 612/1000 | Loss: 0.00001654
Iteration 613/1000 | Loss: 0.00001654
Iteration 614/1000 | Loss: 0.00001654
Iteration 615/1000 | Loss: 0.00001654
Iteration 616/1000 | Loss: 0.00001654
Iteration 617/1000 | Loss: 0.00001654
Iteration 618/1000 | Loss: 0.00001654
Iteration 619/1000 | Loss: 0.00001654
Iteration 620/1000 | Loss: 0.00001654
Iteration 621/1000 | Loss: 0.00001654
Iteration 622/1000 | Loss: 0.00001654
Iteration 623/1000 | Loss: 0.00001654
Iteration 624/1000 | Loss: 0.00001654
Iteration 625/1000 | Loss: 0.00001654
Iteration 626/1000 | Loss: 0.00001654
Iteration 627/1000 | Loss: 0.00001654
Iteration 628/1000 | Loss: 0.00001654
Iteration 629/1000 | Loss: 0.00001654
Iteration 630/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 630. Stopping optimization.
Last 5 losses: [1.6535279428353533e-05, 1.6535279428353533e-05, 1.6535279428353533e-05, 1.6535279428353533e-05, 1.6535279428353533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6535279428353533e-05

Optimization complete. Final v2v error: 3.403963327407837 mm

Highest mean error: 6.487353324890137 mm for frame 121

Lowest mean error: 3.155055046081543 mm for frame 216

Saving results

Total time: 747.6443722248077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999615
Iteration 2/25 | Loss: 0.00250856
Iteration 3/25 | Loss: 0.00223570
Iteration 4/25 | Loss: 0.00187599
Iteration 5/25 | Loss: 0.00165905
Iteration 6/25 | Loss: 0.00159782
Iteration 7/25 | Loss: 0.00155199
Iteration 8/25 | Loss: 0.00152939
Iteration 9/25 | Loss: 0.00153225
Iteration 10/25 | Loss: 0.00150730
Iteration 11/25 | Loss: 0.00152191
Iteration 12/25 | Loss: 0.00149747
Iteration 13/25 | Loss: 0.00152193
Iteration 14/25 | Loss: 0.00156384
Iteration 15/25 | Loss: 0.00156006
Iteration 16/25 | Loss: 0.00140916
Iteration 17/25 | Loss: 0.00137737
Iteration 18/25 | Loss: 0.00137410
Iteration 19/25 | Loss: 0.00137286
Iteration 20/25 | Loss: 0.00137405
Iteration 21/25 | Loss: 0.00136973
Iteration 22/25 | Loss: 0.00136787
Iteration 23/25 | Loss: 0.00135949
Iteration 24/25 | Loss: 0.00135777
Iteration 25/25 | Loss: 0.00135674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48704720
Iteration 2/25 | Loss: 0.00206185
Iteration 3/25 | Loss: 0.00206185
Iteration 4/25 | Loss: 0.00206184
Iteration 5/25 | Loss: 0.00206184
Iteration 6/25 | Loss: 0.00206184
Iteration 7/25 | Loss: 0.00206184
Iteration 8/25 | Loss: 0.00206184
Iteration 9/25 | Loss: 0.00206184
Iteration 10/25 | Loss: 0.00206184
Iteration 11/25 | Loss: 0.00206184
Iteration 12/25 | Loss: 0.00206184
Iteration 13/25 | Loss: 0.00206184
Iteration 14/25 | Loss: 0.00206184
Iteration 15/25 | Loss: 0.00206184
Iteration 16/25 | Loss: 0.00206184
Iteration 17/25 | Loss: 0.00206184
Iteration 18/25 | Loss: 0.00206184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002061841543763876, 0.002061841543763876, 0.002061841543763876, 0.002061841543763876, 0.002061841543763876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002061841543763876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206184
Iteration 2/1000 | Loss: 0.00208992
Iteration 3/1000 | Loss: 0.00064559
Iteration 4/1000 | Loss: 0.00036589
Iteration 5/1000 | Loss: 0.00026151
Iteration 6/1000 | Loss: 0.00013134
Iteration 7/1000 | Loss: 0.00016404
Iteration 8/1000 | Loss: 0.00132903
Iteration 9/1000 | Loss: 0.00024434
Iteration 10/1000 | Loss: 0.00053472
Iteration 11/1000 | Loss: 0.00045487
Iteration 12/1000 | Loss: 0.00007442
Iteration 13/1000 | Loss: 0.00082431
Iteration 14/1000 | Loss: 0.00036135
Iteration 15/1000 | Loss: 0.00086836
Iteration 16/1000 | Loss: 0.00028537
Iteration 17/1000 | Loss: 0.00029756
Iteration 18/1000 | Loss: 0.00031733
Iteration 19/1000 | Loss: 0.00023665
Iteration 20/1000 | Loss: 0.00023109
Iteration 21/1000 | Loss: 0.00019876
Iteration 22/1000 | Loss: 0.00029424
Iteration 23/1000 | Loss: 0.00020786
Iteration 24/1000 | Loss: 0.00029751
Iteration 25/1000 | Loss: 0.00023397
Iteration 26/1000 | Loss: 0.00023234
Iteration 27/1000 | Loss: 0.00003831
Iteration 28/1000 | Loss: 0.00003389
Iteration 29/1000 | Loss: 0.00003144
Iteration 30/1000 | Loss: 0.00002988
Iteration 31/1000 | Loss: 0.00050300
Iteration 32/1000 | Loss: 0.00002911
Iteration 33/1000 | Loss: 0.00002681
Iteration 34/1000 | Loss: 0.00002537
Iteration 35/1000 | Loss: 0.00002416
Iteration 36/1000 | Loss: 0.00002318
Iteration 37/1000 | Loss: 0.00002265
Iteration 38/1000 | Loss: 0.00044722
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002092
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00001917
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001825
Iteration 45/1000 | Loss: 0.00001813
Iteration 46/1000 | Loss: 0.00072004
Iteration 47/1000 | Loss: 0.00042958
Iteration 48/1000 | Loss: 0.00045666
Iteration 49/1000 | Loss: 0.00003112
Iteration 50/1000 | Loss: 0.00002041
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001644
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001580
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001555
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001544
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001509
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001506
Iteration 68/1000 | Loss: 0.00001503
Iteration 69/1000 | Loss: 0.00001499
Iteration 70/1000 | Loss: 0.00001499
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001497
Iteration 76/1000 | Loss: 0.00001497
Iteration 77/1000 | Loss: 0.00001496
Iteration 78/1000 | Loss: 0.00001496
Iteration 79/1000 | Loss: 0.00001496
Iteration 80/1000 | Loss: 0.00001496
Iteration 81/1000 | Loss: 0.00001495
Iteration 82/1000 | Loss: 0.00001495
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001494
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001492
Iteration 88/1000 | Loss: 0.00001492
Iteration 89/1000 | Loss: 0.00001491
Iteration 90/1000 | Loss: 0.00001491
Iteration 91/1000 | Loss: 0.00001491
Iteration 92/1000 | Loss: 0.00001490
Iteration 93/1000 | Loss: 0.00001490
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001489
Iteration 96/1000 | Loss: 0.00001489
Iteration 97/1000 | Loss: 0.00001489
Iteration 98/1000 | Loss: 0.00001489
Iteration 99/1000 | Loss: 0.00001489
Iteration 100/1000 | Loss: 0.00001489
Iteration 101/1000 | Loss: 0.00001488
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001488
Iteration 111/1000 | Loss: 0.00001488
Iteration 112/1000 | Loss: 0.00001488
Iteration 113/1000 | Loss: 0.00001488
Iteration 114/1000 | Loss: 0.00001488
Iteration 115/1000 | Loss: 0.00001488
Iteration 116/1000 | Loss: 0.00001488
Iteration 117/1000 | Loss: 0.00001488
Iteration 118/1000 | Loss: 0.00001488
Iteration 119/1000 | Loss: 0.00001488
Iteration 120/1000 | Loss: 0.00001488
Iteration 121/1000 | Loss: 0.00001488
Iteration 122/1000 | Loss: 0.00001488
Iteration 123/1000 | Loss: 0.00001488
Iteration 124/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.4883713447488844e-05, 1.4883713447488844e-05, 1.4883713447488844e-05, 1.4883713447488844e-05, 1.4883713447488844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4883713447488844e-05

Optimization complete. Final v2v error: 3.1453497409820557 mm

Highest mean error: 6.016104698181152 mm for frame 52

Lowest mean error: 2.5487194061279297 mm for frame 0

Saving results

Total time: 132.45976090431213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424499
Iteration 2/25 | Loss: 0.00122782
Iteration 3/25 | Loss: 0.00116314
Iteration 4/25 | Loss: 0.00115321
Iteration 5/25 | Loss: 0.00115022
Iteration 6/25 | Loss: 0.00115019
Iteration 7/25 | Loss: 0.00115019
Iteration 8/25 | Loss: 0.00115019
Iteration 9/25 | Loss: 0.00115019
Iteration 10/25 | Loss: 0.00115019
Iteration 11/25 | Loss: 0.00115019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011501926928758621, 0.0011501926928758621, 0.0011501926928758621, 0.0011501926928758621, 0.0011501926928758621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011501926928758621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30314374
Iteration 2/25 | Loss: 0.00107246
Iteration 3/25 | Loss: 0.00107246
Iteration 4/25 | Loss: 0.00107245
Iteration 5/25 | Loss: 0.00107245
Iteration 6/25 | Loss: 0.00107245
Iteration 7/25 | Loss: 0.00107245
Iteration 8/25 | Loss: 0.00107245
Iteration 9/25 | Loss: 0.00107245
Iteration 10/25 | Loss: 0.00107245
Iteration 11/25 | Loss: 0.00107245
Iteration 12/25 | Loss: 0.00107245
Iteration 13/25 | Loss: 0.00107245
Iteration 14/25 | Loss: 0.00107245
Iteration 15/25 | Loss: 0.00107245
Iteration 16/25 | Loss: 0.00107245
Iteration 17/25 | Loss: 0.00107245
Iteration 18/25 | Loss: 0.00107245
Iteration 19/25 | Loss: 0.00107245
Iteration 20/25 | Loss: 0.00107245
Iteration 21/25 | Loss: 0.00107245
Iteration 22/25 | Loss: 0.00107245
Iteration 23/25 | Loss: 0.00107245
Iteration 24/25 | Loss: 0.00107245
Iteration 25/25 | Loss: 0.00107245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107245
Iteration 2/1000 | Loss: 0.00001997
Iteration 3/1000 | Loss: 0.00001494
Iteration 4/1000 | Loss: 0.00001335
Iteration 5/1000 | Loss: 0.00001241
Iteration 6/1000 | Loss: 0.00001189
Iteration 7/1000 | Loss: 0.00001155
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001099
Iteration 10/1000 | Loss: 0.00001089
Iteration 11/1000 | Loss: 0.00001082
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001062
Iteration 14/1000 | Loss: 0.00001060
Iteration 15/1000 | Loss: 0.00001050
Iteration 16/1000 | Loss: 0.00001049
Iteration 17/1000 | Loss: 0.00001042
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001038
Iteration 20/1000 | Loss: 0.00001037
Iteration 21/1000 | Loss: 0.00001037
Iteration 22/1000 | Loss: 0.00001036
Iteration 23/1000 | Loss: 0.00001035
Iteration 24/1000 | Loss: 0.00001034
Iteration 25/1000 | Loss: 0.00001034
Iteration 26/1000 | Loss: 0.00001033
Iteration 27/1000 | Loss: 0.00001031
Iteration 28/1000 | Loss: 0.00001031
Iteration 29/1000 | Loss: 0.00001030
Iteration 30/1000 | Loss: 0.00001030
Iteration 31/1000 | Loss: 0.00001030
Iteration 32/1000 | Loss: 0.00001029
Iteration 33/1000 | Loss: 0.00001029
Iteration 34/1000 | Loss: 0.00001028
Iteration 35/1000 | Loss: 0.00001028
Iteration 36/1000 | Loss: 0.00001027
Iteration 37/1000 | Loss: 0.00001026
Iteration 38/1000 | Loss: 0.00001026
Iteration 39/1000 | Loss: 0.00001025
Iteration 40/1000 | Loss: 0.00001025
Iteration 41/1000 | Loss: 0.00001025
Iteration 42/1000 | Loss: 0.00001025
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001023
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001023
Iteration 47/1000 | Loss: 0.00001023
Iteration 48/1000 | Loss: 0.00001023
Iteration 49/1000 | Loss: 0.00001023
Iteration 50/1000 | Loss: 0.00001022
Iteration 51/1000 | Loss: 0.00001022
Iteration 52/1000 | Loss: 0.00001022
Iteration 53/1000 | Loss: 0.00001021
Iteration 54/1000 | Loss: 0.00001021
Iteration 55/1000 | Loss: 0.00001021
Iteration 56/1000 | Loss: 0.00001021
Iteration 57/1000 | Loss: 0.00001021
Iteration 58/1000 | Loss: 0.00001020
Iteration 59/1000 | Loss: 0.00001020
Iteration 60/1000 | Loss: 0.00001020
Iteration 61/1000 | Loss: 0.00001020
Iteration 62/1000 | Loss: 0.00001020
Iteration 63/1000 | Loss: 0.00001020
Iteration 64/1000 | Loss: 0.00001020
Iteration 65/1000 | Loss: 0.00001020
Iteration 66/1000 | Loss: 0.00001019
Iteration 67/1000 | Loss: 0.00001019
Iteration 68/1000 | Loss: 0.00001019
Iteration 69/1000 | Loss: 0.00001019
Iteration 70/1000 | Loss: 0.00001019
Iteration 71/1000 | Loss: 0.00001018
Iteration 72/1000 | Loss: 0.00001018
Iteration 73/1000 | Loss: 0.00001017
Iteration 74/1000 | Loss: 0.00001017
Iteration 75/1000 | Loss: 0.00001017
Iteration 76/1000 | Loss: 0.00001017
Iteration 77/1000 | Loss: 0.00001016
Iteration 78/1000 | Loss: 0.00001016
Iteration 79/1000 | Loss: 0.00001016
Iteration 80/1000 | Loss: 0.00001015
Iteration 81/1000 | Loss: 0.00001015
Iteration 82/1000 | Loss: 0.00001015
Iteration 83/1000 | Loss: 0.00001014
Iteration 84/1000 | Loss: 0.00001014
Iteration 85/1000 | Loss: 0.00001014
Iteration 86/1000 | Loss: 0.00001014
Iteration 87/1000 | Loss: 0.00001013
Iteration 88/1000 | Loss: 0.00001013
Iteration 89/1000 | Loss: 0.00001013
Iteration 90/1000 | Loss: 0.00001013
Iteration 91/1000 | Loss: 0.00001013
Iteration 92/1000 | Loss: 0.00001012
Iteration 93/1000 | Loss: 0.00001012
Iteration 94/1000 | Loss: 0.00001012
Iteration 95/1000 | Loss: 0.00001012
Iteration 96/1000 | Loss: 0.00001012
Iteration 97/1000 | Loss: 0.00001012
Iteration 98/1000 | Loss: 0.00001012
Iteration 99/1000 | Loss: 0.00001012
Iteration 100/1000 | Loss: 0.00001012
Iteration 101/1000 | Loss: 0.00001012
Iteration 102/1000 | Loss: 0.00001012
Iteration 103/1000 | Loss: 0.00001012
Iteration 104/1000 | Loss: 0.00001012
Iteration 105/1000 | Loss: 0.00001012
Iteration 106/1000 | Loss: 0.00001012
Iteration 107/1000 | Loss: 0.00001011
Iteration 108/1000 | Loss: 0.00001011
Iteration 109/1000 | Loss: 0.00001011
Iteration 110/1000 | Loss: 0.00001011
Iteration 111/1000 | Loss: 0.00001011
Iteration 112/1000 | Loss: 0.00001011
Iteration 113/1000 | Loss: 0.00001011
Iteration 114/1000 | Loss: 0.00001010
Iteration 115/1000 | Loss: 0.00001010
Iteration 116/1000 | Loss: 0.00001010
Iteration 117/1000 | Loss: 0.00001010
Iteration 118/1000 | Loss: 0.00001010
Iteration 119/1000 | Loss: 0.00001010
Iteration 120/1000 | Loss: 0.00001010
Iteration 121/1000 | Loss: 0.00001010
Iteration 122/1000 | Loss: 0.00001010
Iteration 123/1000 | Loss: 0.00001010
Iteration 124/1000 | Loss: 0.00001009
Iteration 125/1000 | Loss: 0.00001009
Iteration 126/1000 | Loss: 0.00001009
Iteration 127/1000 | Loss: 0.00001009
Iteration 128/1000 | Loss: 0.00001009
Iteration 129/1000 | Loss: 0.00001009
Iteration 130/1000 | Loss: 0.00001009
Iteration 131/1000 | Loss: 0.00001009
Iteration 132/1000 | Loss: 0.00001009
Iteration 133/1000 | Loss: 0.00001009
Iteration 134/1000 | Loss: 0.00001008
Iteration 135/1000 | Loss: 0.00001008
Iteration 136/1000 | Loss: 0.00001008
Iteration 137/1000 | Loss: 0.00001008
Iteration 138/1000 | Loss: 0.00001008
Iteration 139/1000 | Loss: 0.00001008
Iteration 140/1000 | Loss: 0.00001008
Iteration 141/1000 | Loss: 0.00001007
Iteration 142/1000 | Loss: 0.00001007
Iteration 143/1000 | Loss: 0.00001007
Iteration 144/1000 | Loss: 0.00001007
Iteration 145/1000 | Loss: 0.00001006
Iteration 146/1000 | Loss: 0.00001006
Iteration 147/1000 | Loss: 0.00001006
Iteration 148/1000 | Loss: 0.00001006
Iteration 149/1000 | Loss: 0.00001006
Iteration 150/1000 | Loss: 0.00001005
Iteration 151/1000 | Loss: 0.00001005
Iteration 152/1000 | Loss: 0.00001005
Iteration 153/1000 | Loss: 0.00001005
Iteration 154/1000 | Loss: 0.00001005
Iteration 155/1000 | Loss: 0.00001005
Iteration 156/1000 | Loss: 0.00001005
Iteration 157/1000 | Loss: 0.00001005
Iteration 158/1000 | Loss: 0.00001005
Iteration 159/1000 | Loss: 0.00001004
Iteration 160/1000 | Loss: 0.00001004
Iteration 161/1000 | Loss: 0.00001004
Iteration 162/1000 | Loss: 0.00001004
Iteration 163/1000 | Loss: 0.00001004
Iteration 164/1000 | Loss: 0.00001004
Iteration 165/1000 | Loss: 0.00001004
Iteration 166/1000 | Loss: 0.00001004
Iteration 167/1000 | Loss: 0.00001003
Iteration 168/1000 | Loss: 0.00001003
Iteration 169/1000 | Loss: 0.00001003
Iteration 170/1000 | Loss: 0.00001003
Iteration 171/1000 | Loss: 0.00001003
Iteration 172/1000 | Loss: 0.00001003
Iteration 173/1000 | Loss: 0.00001003
Iteration 174/1000 | Loss: 0.00001003
Iteration 175/1000 | Loss: 0.00001003
Iteration 176/1000 | Loss: 0.00001003
Iteration 177/1000 | Loss: 0.00001003
Iteration 178/1000 | Loss: 0.00001003
Iteration 179/1000 | Loss: 0.00001003
Iteration 180/1000 | Loss: 0.00001003
Iteration 181/1000 | Loss: 0.00001002
Iteration 182/1000 | Loss: 0.00001002
Iteration 183/1000 | Loss: 0.00001002
Iteration 184/1000 | Loss: 0.00001002
Iteration 185/1000 | Loss: 0.00001002
Iteration 186/1000 | Loss: 0.00001002
Iteration 187/1000 | Loss: 0.00001002
Iteration 188/1000 | Loss: 0.00001002
Iteration 189/1000 | Loss: 0.00001002
Iteration 190/1000 | Loss: 0.00001002
Iteration 191/1000 | Loss: 0.00001002
Iteration 192/1000 | Loss: 0.00001002
Iteration 193/1000 | Loss: 0.00001002
Iteration 194/1000 | Loss: 0.00001002
Iteration 195/1000 | Loss: 0.00001002
Iteration 196/1000 | Loss: 0.00001002
Iteration 197/1000 | Loss: 0.00001002
Iteration 198/1000 | Loss: 0.00001002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.0024924449680839e-05, 1.0024924449680839e-05, 1.0024924449680839e-05, 1.0024924449680839e-05, 1.0024924449680839e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0024924449680839e-05

Optimization complete. Final v2v error: 2.7177681922912598 mm

Highest mean error: 2.8660800457000732 mm for frame 183

Lowest mean error: 2.536318063735962 mm for frame 6

Saving results

Total time: 41.17007923126221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032227
Iteration 2/25 | Loss: 0.00338888
Iteration 3/25 | Loss: 0.00182843
Iteration 4/25 | Loss: 0.00164738
Iteration 5/25 | Loss: 0.00164114
Iteration 6/25 | Loss: 0.00145637
Iteration 7/25 | Loss: 0.00135400
Iteration 8/25 | Loss: 0.00129259
Iteration 9/25 | Loss: 0.00126827
Iteration 10/25 | Loss: 0.00125913
Iteration 11/25 | Loss: 0.00125792
Iteration 12/25 | Loss: 0.00124452
Iteration 13/25 | Loss: 0.00124116
Iteration 14/25 | Loss: 0.00123899
Iteration 15/25 | Loss: 0.00123623
Iteration 16/25 | Loss: 0.00123517
Iteration 17/25 | Loss: 0.00123483
Iteration 18/25 | Loss: 0.00123474
Iteration 19/25 | Loss: 0.00123474
Iteration 20/25 | Loss: 0.00123474
Iteration 21/25 | Loss: 0.00123474
Iteration 22/25 | Loss: 0.00123474
Iteration 23/25 | Loss: 0.00123474
Iteration 24/25 | Loss: 0.00123474
Iteration 25/25 | Loss: 0.00123474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65588367
Iteration 2/25 | Loss: 0.00145878
Iteration 3/25 | Loss: 0.00144768
Iteration 4/25 | Loss: 0.00144768
Iteration 5/25 | Loss: 0.00144768
Iteration 6/25 | Loss: 0.00144768
Iteration 7/25 | Loss: 0.00144768
Iteration 8/25 | Loss: 0.00144768
Iteration 9/25 | Loss: 0.00144768
Iteration 10/25 | Loss: 0.00144768
Iteration 11/25 | Loss: 0.00144768
Iteration 12/25 | Loss: 0.00144768
Iteration 13/25 | Loss: 0.00144768
Iteration 14/25 | Loss: 0.00144768
Iteration 15/25 | Loss: 0.00144768
Iteration 16/25 | Loss: 0.00144768
Iteration 17/25 | Loss: 0.00144768
Iteration 18/25 | Loss: 0.00144768
Iteration 19/25 | Loss: 0.00144768
Iteration 20/25 | Loss: 0.00144768
Iteration 21/25 | Loss: 0.00144768
Iteration 22/25 | Loss: 0.00144768
Iteration 23/25 | Loss: 0.00144768
Iteration 24/25 | Loss: 0.00144768
Iteration 25/25 | Loss: 0.00144768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144768
Iteration 2/1000 | Loss: 0.00009321
Iteration 3/1000 | Loss: 0.00007171
Iteration 4/1000 | Loss: 0.00005787
Iteration 5/1000 | Loss: 0.00006221
Iteration 6/1000 | Loss: 0.00005060
Iteration 7/1000 | Loss: 0.00004893
Iteration 8/1000 | Loss: 0.00004708
Iteration 9/1000 | Loss: 0.00004557
Iteration 10/1000 | Loss: 0.00007005
Iteration 11/1000 | Loss: 0.00004864
Iteration 12/1000 | Loss: 0.00093062
Iteration 13/1000 | Loss: 0.00198396
Iteration 14/1000 | Loss: 0.00020941
Iteration 15/1000 | Loss: 0.00059832
Iteration 16/1000 | Loss: 0.00004123
Iteration 17/1000 | Loss: 0.00035001
Iteration 18/1000 | Loss: 0.00005096
Iteration 19/1000 | Loss: 0.00008366
Iteration 20/1000 | Loss: 0.00011097
Iteration 21/1000 | Loss: 0.00003769
Iteration 22/1000 | Loss: 0.00001916
Iteration 23/1000 | Loss: 0.00007658
Iteration 24/1000 | Loss: 0.00009450
Iteration 25/1000 | Loss: 0.00005140
Iteration 26/1000 | Loss: 0.00002416
Iteration 27/1000 | Loss: 0.00001650
Iteration 28/1000 | Loss: 0.00010791
Iteration 29/1000 | Loss: 0.00003436
Iteration 30/1000 | Loss: 0.00003744
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00003797
Iteration 33/1000 | Loss: 0.00016691
Iteration 34/1000 | Loss: 0.00003721
Iteration 35/1000 | Loss: 0.00006060
Iteration 36/1000 | Loss: 0.00003383
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001298
Iteration 39/1000 | Loss: 0.00001287
Iteration 40/1000 | Loss: 0.00002236
Iteration 41/1000 | Loss: 0.00001274
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001269
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001259
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001257
Iteration 72/1000 | Loss: 0.00001256
Iteration 73/1000 | Loss: 0.00001256
Iteration 74/1000 | Loss: 0.00001255
Iteration 75/1000 | Loss: 0.00001255
Iteration 76/1000 | Loss: 0.00001255
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001252
Iteration 82/1000 | Loss: 0.00001252
Iteration 83/1000 | Loss: 0.00001251
Iteration 84/1000 | Loss: 0.00001251
Iteration 85/1000 | Loss: 0.00001251
Iteration 86/1000 | Loss: 0.00001251
Iteration 87/1000 | Loss: 0.00001251
Iteration 88/1000 | Loss: 0.00001251
Iteration 89/1000 | Loss: 0.00001250
Iteration 90/1000 | Loss: 0.00001250
Iteration 91/1000 | Loss: 0.00001250
Iteration 92/1000 | Loss: 0.00001249
Iteration 93/1000 | Loss: 0.00001249
Iteration 94/1000 | Loss: 0.00001249
Iteration 95/1000 | Loss: 0.00001249
Iteration 96/1000 | Loss: 0.00001249
Iteration 97/1000 | Loss: 0.00001248
Iteration 98/1000 | Loss: 0.00001248
Iteration 99/1000 | Loss: 0.00001248
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001248
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001248
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001248
Iteration 111/1000 | Loss: 0.00001248
Iteration 112/1000 | Loss: 0.00001248
Iteration 113/1000 | Loss: 0.00001248
Iteration 114/1000 | Loss: 0.00001248
Iteration 115/1000 | Loss: 0.00001248
Iteration 116/1000 | Loss: 0.00001248
Iteration 117/1000 | Loss: 0.00001248
Iteration 118/1000 | Loss: 0.00001248
Iteration 119/1000 | Loss: 0.00001248
Iteration 120/1000 | Loss: 0.00001248
Iteration 121/1000 | Loss: 0.00001248
Iteration 122/1000 | Loss: 0.00001248
Iteration 123/1000 | Loss: 0.00001248
Iteration 124/1000 | Loss: 0.00001248
Iteration 125/1000 | Loss: 0.00001248
Iteration 126/1000 | Loss: 0.00001248
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001248
Iteration 135/1000 | Loss: 0.00001248
Iteration 136/1000 | Loss: 0.00001248
Iteration 137/1000 | Loss: 0.00001248
Iteration 138/1000 | Loss: 0.00001248
Iteration 139/1000 | Loss: 0.00001248
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.2479011274990626e-05, 1.2479011274990626e-05, 1.2479011274990626e-05, 1.2479011274990626e-05, 1.2479011274990626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2479011274990626e-05

Optimization complete. Final v2v error: 2.9989233016967773 mm

Highest mean error: 4.215543746948242 mm for frame 78

Lowest mean error: 2.6469149589538574 mm for frame 23

Saving results

Total time: 93.11353015899658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878092
Iteration 2/25 | Loss: 0.00170953
Iteration 3/25 | Loss: 0.00137450
Iteration 4/25 | Loss: 0.00137118
Iteration 5/25 | Loss: 0.00133153
Iteration 6/25 | Loss: 0.00130505
Iteration 7/25 | Loss: 0.00129000
Iteration 8/25 | Loss: 0.00128576
Iteration 9/25 | Loss: 0.00128444
Iteration 10/25 | Loss: 0.00127152
Iteration 11/25 | Loss: 0.00125428
Iteration 12/25 | Loss: 0.00125114
Iteration 13/25 | Loss: 0.00124980
Iteration 14/25 | Loss: 0.00125236
Iteration 15/25 | Loss: 0.00125042
Iteration 16/25 | Loss: 0.00125183
Iteration 17/25 | Loss: 0.00125346
Iteration 18/25 | Loss: 0.00125230
Iteration 19/25 | Loss: 0.00124395
Iteration 20/25 | Loss: 0.00124330
Iteration 21/25 | Loss: 0.00124022
Iteration 22/25 | Loss: 0.00124126
Iteration 23/25 | Loss: 0.00124080
Iteration 24/25 | Loss: 0.00124191
Iteration 25/25 | Loss: 0.00124231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.61377478
Iteration 2/25 | Loss: 0.00194783
Iteration 3/25 | Loss: 0.00194774
Iteration 4/25 | Loss: 0.00194773
Iteration 5/25 | Loss: 0.00194773
Iteration 6/25 | Loss: 0.00194773
Iteration 7/25 | Loss: 0.00194773
Iteration 8/25 | Loss: 0.00194773
Iteration 9/25 | Loss: 0.00194773
Iteration 10/25 | Loss: 0.00194773
Iteration 11/25 | Loss: 0.00194773
Iteration 12/25 | Loss: 0.00194773
Iteration 13/25 | Loss: 0.00194773
Iteration 14/25 | Loss: 0.00194773
Iteration 15/25 | Loss: 0.00194773
Iteration 16/25 | Loss: 0.00194773
Iteration 17/25 | Loss: 0.00194773
Iteration 18/25 | Loss: 0.00194773
Iteration 19/25 | Loss: 0.00194773
Iteration 20/25 | Loss: 0.00194773
Iteration 21/25 | Loss: 0.00194773
Iteration 22/25 | Loss: 0.00194773
Iteration 23/25 | Loss: 0.00194773
Iteration 24/25 | Loss: 0.00194773
Iteration 25/25 | Loss: 0.00194773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194773
Iteration 2/1000 | Loss: 0.00011224
Iteration 3/1000 | Loss: 0.00018380
Iteration 4/1000 | Loss: 0.00008356
Iteration 5/1000 | Loss: 0.00011794
Iteration 6/1000 | Loss: 0.00009193
Iteration 7/1000 | Loss: 0.00014854
Iteration 8/1000 | Loss: 0.00013766
Iteration 9/1000 | Loss: 0.00016594
Iteration 10/1000 | Loss: 0.00012513
Iteration 11/1000 | Loss: 0.00007760
Iteration 12/1000 | Loss: 0.00010087
Iteration 13/1000 | Loss: 0.00009191
Iteration 14/1000 | Loss: 0.00016098
Iteration 15/1000 | Loss: 0.00023015
Iteration 16/1000 | Loss: 0.00018853
Iteration 17/1000 | Loss: 0.00013490
Iteration 18/1000 | Loss: 0.00013929
Iteration 19/1000 | Loss: 0.00010905
Iteration 20/1000 | Loss: 0.00004590
Iteration 21/1000 | Loss: 0.00021309
Iteration 22/1000 | Loss: 0.00019160
Iteration 23/1000 | Loss: 0.00018247
Iteration 24/1000 | Loss: 0.00018915
Iteration 25/1000 | Loss: 0.00005218
Iteration 26/1000 | Loss: 0.00038472
Iteration 27/1000 | Loss: 0.00013808
Iteration 28/1000 | Loss: 0.00009013
Iteration 29/1000 | Loss: 0.00004747
Iteration 30/1000 | Loss: 0.00007055
Iteration 31/1000 | Loss: 0.00004894
Iteration 32/1000 | Loss: 0.00007442
Iteration 33/1000 | Loss: 0.00007953
Iteration 34/1000 | Loss: 0.00005747
Iteration 35/1000 | Loss: 0.00010261
Iteration 36/1000 | Loss: 0.00004456
Iteration 37/1000 | Loss: 0.00005851
Iteration 38/1000 | Loss: 0.00006811
Iteration 39/1000 | Loss: 0.00007693
Iteration 40/1000 | Loss: 0.00006441
Iteration 41/1000 | Loss: 0.00005354
Iteration 42/1000 | Loss: 0.00007283
Iteration 43/1000 | Loss: 0.00007694
Iteration 44/1000 | Loss: 0.00009396
Iteration 45/1000 | Loss: 0.00006749
Iteration 46/1000 | Loss: 0.00018635
Iteration 47/1000 | Loss: 0.00010614
Iteration 48/1000 | Loss: 0.00017093
Iteration 49/1000 | Loss: 0.00010992
Iteration 50/1000 | Loss: 0.00010122
Iteration 51/1000 | Loss: 0.00003328
Iteration 52/1000 | Loss: 0.00003179
Iteration 53/1000 | Loss: 0.00012914
Iteration 54/1000 | Loss: 0.00012361
Iteration 55/1000 | Loss: 0.00040872
Iteration 56/1000 | Loss: 0.00003135
Iteration 57/1000 | Loss: 0.00003068
Iteration 58/1000 | Loss: 0.00012678
Iteration 59/1000 | Loss: 0.00016019
Iteration 60/1000 | Loss: 0.00012172
Iteration 61/1000 | Loss: 0.00006277
Iteration 62/1000 | Loss: 0.00009990
Iteration 63/1000 | Loss: 0.00008961
Iteration 64/1000 | Loss: 0.00008827
Iteration 65/1000 | Loss: 0.00007814
Iteration 66/1000 | Loss: 0.00008782
Iteration 67/1000 | Loss: 0.00015146
Iteration 68/1000 | Loss: 0.00010435
Iteration 69/1000 | Loss: 0.00021389
Iteration 70/1000 | Loss: 0.00015269
Iteration 71/1000 | Loss: 0.00010195
Iteration 72/1000 | Loss: 0.00011862
Iteration 73/1000 | Loss: 0.00012414
Iteration 74/1000 | Loss: 0.00015520
Iteration 75/1000 | Loss: 0.00012620
Iteration 76/1000 | Loss: 0.00004656
Iteration 77/1000 | Loss: 0.00003306
Iteration 78/1000 | Loss: 0.00009539
Iteration 79/1000 | Loss: 0.00005979
Iteration 80/1000 | Loss: 0.00013519
Iteration 81/1000 | Loss: 0.00012597
Iteration 82/1000 | Loss: 0.00009943
Iteration 83/1000 | Loss: 0.00009508
Iteration 84/1000 | Loss: 0.00005470
Iteration 85/1000 | Loss: 0.00009536
Iteration 86/1000 | Loss: 0.00011881
Iteration 87/1000 | Loss: 0.00014058
Iteration 88/1000 | Loss: 0.00011543
Iteration 89/1000 | Loss: 0.00012134
Iteration 90/1000 | Loss: 0.00012832
Iteration 91/1000 | Loss: 0.00012112
Iteration 92/1000 | Loss: 0.00010687
Iteration 93/1000 | Loss: 0.00011711
Iteration 94/1000 | Loss: 0.00009270
Iteration 95/1000 | Loss: 0.00005439
Iteration 96/1000 | Loss: 0.00005214
Iteration 97/1000 | Loss: 0.00008287
Iteration 98/1000 | Loss: 0.00025346
Iteration 99/1000 | Loss: 0.00008391
Iteration 100/1000 | Loss: 0.00008798
Iteration 101/1000 | Loss: 0.00003456
Iteration 102/1000 | Loss: 0.00009678
Iteration 103/1000 | Loss: 0.00004081
Iteration 104/1000 | Loss: 0.00010884
Iteration 105/1000 | Loss: 0.00010456
Iteration 106/1000 | Loss: 0.00020089
Iteration 107/1000 | Loss: 0.00007267
Iteration 108/1000 | Loss: 0.00009378
Iteration 109/1000 | Loss: 0.00008513
Iteration 110/1000 | Loss: 0.00004064
Iteration 111/1000 | Loss: 0.00007818
Iteration 112/1000 | Loss: 0.00023341
Iteration 113/1000 | Loss: 0.00012532
Iteration 114/1000 | Loss: 0.00014936
Iteration 115/1000 | Loss: 0.00014690
Iteration 116/1000 | Loss: 0.00012655
Iteration 117/1000 | Loss: 0.00011547
Iteration 118/1000 | Loss: 0.00003956
Iteration 119/1000 | Loss: 0.00013855
Iteration 120/1000 | Loss: 0.00013365
Iteration 121/1000 | Loss: 0.00010291
Iteration 122/1000 | Loss: 0.00009462
Iteration 123/1000 | Loss: 0.00013939
Iteration 124/1000 | Loss: 0.00008349
Iteration 125/1000 | Loss: 0.00006830
Iteration 126/1000 | Loss: 0.00006478
Iteration 127/1000 | Loss: 0.00010144
Iteration 128/1000 | Loss: 0.00009136
Iteration 129/1000 | Loss: 0.00009141
Iteration 130/1000 | Loss: 0.00006167
Iteration 131/1000 | Loss: 0.00008936
Iteration 132/1000 | Loss: 0.00011138
Iteration 133/1000 | Loss: 0.00011447
Iteration 134/1000 | Loss: 0.00005681
Iteration 135/1000 | Loss: 0.00006272
Iteration 136/1000 | Loss: 0.00005205
Iteration 137/1000 | Loss: 0.00003149
Iteration 138/1000 | Loss: 0.00004591
Iteration 139/1000 | Loss: 0.00004388
Iteration 140/1000 | Loss: 0.00003005
Iteration 141/1000 | Loss: 0.00005247
Iteration 142/1000 | Loss: 0.00007069
Iteration 143/1000 | Loss: 0.00013473
Iteration 144/1000 | Loss: 0.00009079
Iteration 145/1000 | Loss: 0.00002938
Iteration 146/1000 | Loss: 0.00008528
Iteration 147/1000 | Loss: 0.00002962
Iteration 148/1000 | Loss: 0.00002906
Iteration 149/1000 | Loss: 0.00002904
Iteration 150/1000 | Loss: 0.00002897
Iteration 151/1000 | Loss: 0.00002889
Iteration 152/1000 | Loss: 0.00018098
Iteration 153/1000 | Loss: 0.00003767
Iteration 154/1000 | Loss: 0.00010705
Iteration 155/1000 | Loss: 0.00031550
Iteration 156/1000 | Loss: 0.00008838
Iteration 157/1000 | Loss: 0.00015892
Iteration 158/1000 | Loss: 0.00003422
Iteration 159/1000 | Loss: 0.00003262
Iteration 160/1000 | Loss: 0.00007884
Iteration 161/1000 | Loss: 0.00004213
Iteration 162/1000 | Loss: 0.00003126
Iteration 163/1000 | Loss: 0.00003082
Iteration 164/1000 | Loss: 0.00015245
Iteration 165/1000 | Loss: 0.00007536
Iteration 166/1000 | Loss: 0.00018165
Iteration 167/1000 | Loss: 0.00010101
Iteration 168/1000 | Loss: 0.00003919
Iteration 169/1000 | Loss: 0.00014153
Iteration 170/1000 | Loss: 0.00008416
Iteration 171/1000 | Loss: 0.00004563
Iteration 172/1000 | Loss: 0.00014680
Iteration 173/1000 | Loss: 0.00008953
Iteration 174/1000 | Loss: 0.00009177
Iteration 175/1000 | Loss: 0.00007095
Iteration 176/1000 | Loss: 0.00004310
Iteration 177/1000 | Loss: 0.00003115
Iteration 178/1000 | Loss: 0.00015443
Iteration 179/1000 | Loss: 0.00007688
Iteration 180/1000 | Loss: 0.00013150
Iteration 181/1000 | Loss: 0.00007647
Iteration 182/1000 | Loss: 0.00007363
Iteration 183/1000 | Loss: 0.00011525
Iteration 184/1000 | Loss: 0.00006045
Iteration 185/1000 | Loss: 0.00008399
Iteration 186/1000 | Loss: 0.00049608
Iteration 187/1000 | Loss: 0.00004743
Iteration 188/1000 | Loss: 0.00011467
Iteration 189/1000 | Loss: 0.00042449
Iteration 190/1000 | Loss: 0.00048821
Iteration 191/1000 | Loss: 0.00030538
Iteration 192/1000 | Loss: 0.00008569
Iteration 193/1000 | Loss: 0.00024127
Iteration 194/1000 | Loss: 0.00017362
Iteration 195/1000 | Loss: 0.00009526
Iteration 196/1000 | Loss: 0.00012137
Iteration 197/1000 | Loss: 0.00012067
Iteration 198/1000 | Loss: 0.00009300
Iteration 199/1000 | Loss: 0.00007613
Iteration 200/1000 | Loss: 0.00011623
Iteration 201/1000 | Loss: 0.00013296
Iteration 202/1000 | Loss: 0.00047551
Iteration 203/1000 | Loss: 0.00014652
Iteration 204/1000 | Loss: 0.00030411
Iteration 205/1000 | Loss: 0.00023820
Iteration 206/1000 | Loss: 0.00020180
Iteration 207/1000 | Loss: 0.00026191
Iteration 208/1000 | Loss: 0.00010341
Iteration 209/1000 | Loss: 0.00010488
Iteration 210/1000 | Loss: 0.00049359
Iteration 211/1000 | Loss: 0.00060309
Iteration 212/1000 | Loss: 0.00052168
Iteration 213/1000 | Loss: 0.00006498
Iteration 214/1000 | Loss: 0.00004279
Iteration 215/1000 | Loss: 0.00067884
Iteration 216/1000 | Loss: 0.00077177
Iteration 217/1000 | Loss: 0.00054210
Iteration 218/1000 | Loss: 0.00059668
Iteration 219/1000 | Loss: 0.00045589
Iteration 220/1000 | Loss: 0.00004590
Iteration 221/1000 | Loss: 0.00050779
Iteration 222/1000 | Loss: 0.00021717
Iteration 223/1000 | Loss: 0.00003424
Iteration 224/1000 | Loss: 0.00008299
Iteration 225/1000 | Loss: 0.00017206
Iteration 226/1000 | Loss: 0.00019091
Iteration 227/1000 | Loss: 0.00033117
Iteration 228/1000 | Loss: 0.00017261
Iteration 229/1000 | Loss: 0.00018591
Iteration 230/1000 | Loss: 0.00010702
Iteration 231/1000 | Loss: 0.00017570
Iteration 232/1000 | Loss: 0.00012517
Iteration 233/1000 | Loss: 0.00014392
Iteration 234/1000 | Loss: 0.00004514
Iteration 235/1000 | Loss: 0.00019750
Iteration 236/1000 | Loss: 0.00027525
Iteration 237/1000 | Loss: 0.00008157
Iteration 238/1000 | Loss: 0.00015953
Iteration 239/1000 | Loss: 0.00017550
Iteration 240/1000 | Loss: 0.00014837
Iteration 241/1000 | Loss: 0.00013700
Iteration 242/1000 | Loss: 0.00018174
Iteration 243/1000 | Loss: 0.00020584
Iteration 244/1000 | Loss: 0.00020088
Iteration 245/1000 | Loss: 0.00017774
Iteration 246/1000 | Loss: 0.00057949
Iteration 247/1000 | Loss: 0.00008088
Iteration 248/1000 | Loss: 0.00005594
Iteration 249/1000 | Loss: 0.00004763
Iteration 250/1000 | Loss: 0.00005737
Iteration 251/1000 | Loss: 0.00003743
Iteration 252/1000 | Loss: 0.00003787
Iteration 253/1000 | Loss: 0.00004374
Iteration 254/1000 | Loss: 0.00003813
Iteration 255/1000 | Loss: 0.00004142
Iteration 256/1000 | Loss: 0.00003617
Iteration 257/1000 | Loss: 0.00003661
Iteration 258/1000 | Loss: 0.00003927
Iteration 259/1000 | Loss: 0.00003877
Iteration 260/1000 | Loss: 0.00003920
Iteration 261/1000 | Loss: 0.00006722
Iteration 262/1000 | Loss: 0.00005166
Iteration 263/1000 | Loss: 0.00004048
Iteration 264/1000 | Loss: 0.00005014
Iteration 265/1000 | Loss: 0.00004068
Iteration 266/1000 | Loss: 0.00003465
Iteration 267/1000 | Loss: 0.00006906
Iteration 268/1000 | Loss: 0.00004421
Iteration 269/1000 | Loss: 0.00004357
Iteration 270/1000 | Loss: 0.00004130
Iteration 271/1000 | Loss: 0.00003788
Iteration 272/1000 | Loss: 0.00003966
Iteration 273/1000 | Loss: 0.00003925
Iteration 274/1000 | Loss: 0.00003761
Iteration 275/1000 | Loss: 0.00004009
Iteration 276/1000 | Loss: 0.00003888
Iteration 277/1000 | Loss: 0.00003780
Iteration 278/1000 | Loss: 0.00003941
Iteration 279/1000 | Loss: 0.00003964
Iteration 280/1000 | Loss: 0.00003925
Iteration 281/1000 | Loss: 0.00003819
Iteration 282/1000 | Loss: 0.00003824
Iteration 283/1000 | Loss: 0.00003958
Iteration 284/1000 | Loss: 0.00003894
Iteration 285/1000 | Loss: 0.00026818
Iteration 286/1000 | Loss: 0.00018866
Iteration 287/1000 | Loss: 0.00005596
Iteration 288/1000 | Loss: 0.00004504
Iteration 289/1000 | Loss: 0.00010539
Iteration 290/1000 | Loss: 0.00004095
Iteration 291/1000 | Loss: 0.00004127
Iteration 292/1000 | Loss: 0.00003812
Iteration 293/1000 | Loss: 0.00004382
Iteration 294/1000 | Loss: 0.00004043
Iteration 295/1000 | Loss: 0.00004074
Iteration 296/1000 | Loss: 0.00018266
Iteration 297/1000 | Loss: 0.00003754
Iteration 298/1000 | Loss: 0.00005232
Iteration 299/1000 | Loss: 0.00003935
Iteration 300/1000 | Loss: 0.00003509
Iteration 301/1000 | Loss: 0.00003374
Iteration 302/1000 | Loss: 0.00003961
Iteration 303/1000 | Loss: 0.00003467
Iteration 304/1000 | Loss: 0.00004756
Iteration 305/1000 | Loss: 0.00003208
Iteration 306/1000 | Loss: 0.00002820
Iteration 307/1000 | Loss: 0.00002718
Iteration 308/1000 | Loss: 0.00002675
Iteration 309/1000 | Loss: 0.00002655
Iteration 310/1000 | Loss: 0.00002649
Iteration 311/1000 | Loss: 0.00002648
Iteration 312/1000 | Loss: 0.00002648
Iteration 313/1000 | Loss: 0.00002647
Iteration 314/1000 | Loss: 0.00002646
Iteration 315/1000 | Loss: 0.00002645
Iteration 316/1000 | Loss: 0.00002645
Iteration 317/1000 | Loss: 0.00002645
Iteration 318/1000 | Loss: 0.00002645
Iteration 319/1000 | Loss: 0.00002644
Iteration 320/1000 | Loss: 0.00002644
Iteration 321/1000 | Loss: 0.00002644
Iteration 322/1000 | Loss: 0.00002644
Iteration 323/1000 | Loss: 0.00002644
Iteration 324/1000 | Loss: 0.00002644
Iteration 325/1000 | Loss: 0.00002644
Iteration 326/1000 | Loss: 0.00002644
Iteration 327/1000 | Loss: 0.00002643
Iteration 328/1000 | Loss: 0.00002643
Iteration 329/1000 | Loss: 0.00002643
Iteration 330/1000 | Loss: 0.00002643
Iteration 331/1000 | Loss: 0.00002643
Iteration 332/1000 | Loss: 0.00002643
Iteration 333/1000 | Loss: 0.00002643
Iteration 334/1000 | Loss: 0.00002643
Iteration 335/1000 | Loss: 0.00002643
Iteration 336/1000 | Loss: 0.00002643
Iteration 337/1000 | Loss: 0.00002642
Iteration 338/1000 | Loss: 0.00002642
Iteration 339/1000 | Loss: 0.00002642
Iteration 340/1000 | Loss: 0.00002642
Iteration 341/1000 | Loss: 0.00002642
Iteration 342/1000 | Loss: 0.00002642
Iteration 343/1000 | Loss: 0.00002642
Iteration 344/1000 | Loss: 0.00002642
Iteration 345/1000 | Loss: 0.00002642
Iteration 346/1000 | Loss: 0.00002642
Iteration 347/1000 | Loss: 0.00002642
Iteration 348/1000 | Loss: 0.00002641
Iteration 349/1000 | Loss: 0.00002641
Iteration 350/1000 | Loss: 0.00002641
Iteration 351/1000 | Loss: 0.00002641
Iteration 352/1000 | Loss: 0.00002641
Iteration 353/1000 | Loss: 0.00002641
Iteration 354/1000 | Loss: 0.00002641
Iteration 355/1000 | Loss: 0.00002641
Iteration 356/1000 | Loss: 0.00002641
Iteration 357/1000 | Loss: 0.00002641
Iteration 358/1000 | Loss: 0.00002641
Iteration 359/1000 | Loss: 0.00002640
Iteration 360/1000 | Loss: 0.00002640
Iteration 361/1000 | Loss: 0.00002640
Iteration 362/1000 | Loss: 0.00002640
Iteration 363/1000 | Loss: 0.00002640
Iteration 364/1000 | Loss: 0.00002640
Iteration 365/1000 | Loss: 0.00002640
Iteration 366/1000 | Loss: 0.00002640
Iteration 367/1000 | Loss: 0.00002640
Iteration 368/1000 | Loss: 0.00002640
Iteration 369/1000 | Loss: 0.00002640
Iteration 370/1000 | Loss: 0.00002640
Iteration 371/1000 | Loss: 0.00002640
Iteration 372/1000 | Loss: 0.00002640
Iteration 373/1000 | Loss: 0.00002640
Iteration 374/1000 | Loss: 0.00002640
Iteration 375/1000 | Loss: 0.00002640
Iteration 376/1000 | Loss: 0.00002639
Iteration 377/1000 | Loss: 0.00002639
Iteration 378/1000 | Loss: 0.00002639
Iteration 379/1000 | Loss: 0.00002639
Iteration 380/1000 | Loss: 0.00002639
Iteration 381/1000 | Loss: 0.00002639
Iteration 382/1000 | Loss: 0.00002639
Iteration 383/1000 | Loss: 0.00002639
Iteration 384/1000 | Loss: 0.00002639
Iteration 385/1000 | Loss: 0.00002639
Iteration 386/1000 | Loss: 0.00002639
Iteration 387/1000 | Loss: 0.00002639
Iteration 388/1000 | Loss: 0.00002639
Iteration 389/1000 | Loss: 0.00002639
Iteration 390/1000 | Loss: 0.00002639
Iteration 391/1000 | Loss: 0.00002639
Iteration 392/1000 | Loss: 0.00002639
Iteration 393/1000 | Loss: 0.00002639
Iteration 394/1000 | Loss: 0.00002639
Iteration 395/1000 | Loss: 0.00002639
Iteration 396/1000 | Loss: 0.00002639
Iteration 397/1000 | Loss: 0.00002639
Iteration 398/1000 | Loss: 0.00002639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 398. Stopping optimization.
Last 5 losses: [2.639386548253242e-05, 2.639386548253242e-05, 2.639386548253242e-05, 2.639386548253242e-05, 2.639386548253242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.639386548253242e-05

Optimization complete. Final v2v error: 4.2083845138549805 mm

Highest mean error: 6.7697434425354 mm for frame 41

Lowest mean error: 2.9277374744415283 mm for frame 137

Saving results

Total time: 544.5096342563629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986311
Iteration 2/25 | Loss: 0.00134067
Iteration 3/25 | Loss: 0.00121688
Iteration 4/25 | Loss: 0.00120502
Iteration 5/25 | Loss: 0.00120195
Iteration 6/25 | Loss: 0.00120112
Iteration 7/25 | Loss: 0.00120109
Iteration 8/25 | Loss: 0.00120109
Iteration 9/25 | Loss: 0.00120109
Iteration 10/25 | Loss: 0.00120109
Iteration 11/25 | Loss: 0.00120109
Iteration 12/25 | Loss: 0.00120109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012010937789455056, 0.0012010937789455056, 0.0012010937789455056, 0.0012010937789455056, 0.0012010937789455056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012010937789455056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.99788046
Iteration 2/25 | Loss: 0.00131530
Iteration 3/25 | Loss: 0.00131530
Iteration 4/25 | Loss: 0.00131530
Iteration 5/25 | Loss: 0.00131530
Iteration 6/25 | Loss: 0.00131530
Iteration 7/25 | Loss: 0.00131530
Iteration 8/25 | Loss: 0.00131530
Iteration 9/25 | Loss: 0.00131530
Iteration 10/25 | Loss: 0.00131530
Iteration 11/25 | Loss: 0.00131530
Iteration 12/25 | Loss: 0.00131530
Iteration 13/25 | Loss: 0.00131530
Iteration 14/25 | Loss: 0.00131530
Iteration 15/25 | Loss: 0.00131530
Iteration 16/25 | Loss: 0.00131530
Iteration 17/25 | Loss: 0.00131530
Iteration 18/25 | Loss: 0.00131530
Iteration 19/25 | Loss: 0.00131530
Iteration 20/25 | Loss: 0.00131530
Iteration 21/25 | Loss: 0.00131530
Iteration 22/25 | Loss: 0.00131530
Iteration 23/25 | Loss: 0.00131530
Iteration 24/25 | Loss: 0.00131530
Iteration 25/25 | Loss: 0.00131530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013152994215488434, 0.0013152994215488434, 0.0013152994215488434, 0.0013152994215488434, 0.0013152994215488434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013152994215488434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131530
Iteration 2/1000 | Loss: 0.00002602
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001480
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001327
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001267
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001256
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001255
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001253
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001247
Iteration 33/1000 | Loss: 0.00001247
Iteration 34/1000 | Loss: 0.00001246
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001236
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001235
Iteration 51/1000 | Loss: 0.00001235
Iteration 52/1000 | Loss: 0.00001235
Iteration 53/1000 | Loss: 0.00001234
Iteration 54/1000 | Loss: 0.00001234
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001233
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001231
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001230
Iteration 67/1000 | Loss: 0.00001230
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001229
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001225
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001222
Iteration 85/1000 | Loss: 0.00001222
Iteration 86/1000 | Loss: 0.00001222
Iteration 87/1000 | Loss: 0.00001221
Iteration 88/1000 | Loss: 0.00001221
Iteration 89/1000 | Loss: 0.00001221
Iteration 90/1000 | Loss: 0.00001219
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001219
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001218
Iteration 97/1000 | Loss: 0.00001218
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001217
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001214
Iteration 116/1000 | Loss: 0.00001214
Iteration 117/1000 | Loss: 0.00001214
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2135651559219696e-05, 1.2135651559219696e-05, 1.2135651559219696e-05, 1.2135651559219696e-05, 1.2135651559219696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2135651559219696e-05

Optimization complete. Final v2v error: 2.9570083618164062 mm

Highest mean error: 3.208913803100586 mm for frame 92

Lowest mean error: 2.4946608543395996 mm for frame 148

Saving results

Total time: 37.996599435806274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052868
Iteration 2/25 | Loss: 0.01052868
Iteration 3/25 | Loss: 0.01052868
Iteration 4/25 | Loss: 0.01052867
Iteration 5/25 | Loss: 0.01052867
Iteration 6/25 | Loss: 0.01052867
Iteration 7/25 | Loss: 0.01052866
Iteration 8/25 | Loss: 0.00233433
Iteration 9/25 | Loss: 0.00185660
Iteration 10/25 | Loss: 0.00224705
Iteration 11/25 | Loss: 0.00160268
Iteration 12/25 | Loss: 0.00143253
Iteration 13/25 | Loss: 0.00136821
Iteration 14/25 | Loss: 0.00135449
Iteration 15/25 | Loss: 0.00135334
Iteration 16/25 | Loss: 0.00135312
Iteration 17/25 | Loss: 0.00135312
Iteration 18/25 | Loss: 0.00135312
Iteration 19/25 | Loss: 0.00135312
Iteration 20/25 | Loss: 0.00135312
Iteration 21/25 | Loss: 0.00135312
Iteration 22/25 | Loss: 0.00135312
Iteration 23/25 | Loss: 0.00135312
Iteration 24/25 | Loss: 0.00135312
Iteration 25/25 | Loss: 0.00135312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14290142
Iteration 2/25 | Loss: 0.00096243
Iteration 3/25 | Loss: 0.00096242
Iteration 4/25 | Loss: 0.00096242
Iteration 5/25 | Loss: 0.00096242
Iteration 6/25 | Loss: 0.00096242
Iteration 7/25 | Loss: 0.00096242
Iteration 8/25 | Loss: 0.00096242
Iteration 9/25 | Loss: 0.00096242
Iteration 10/25 | Loss: 0.00096242
Iteration 11/25 | Loss: 0.00096242
Iteration 12/25 | Loss: 0.00096242
Iteration 13/25 | Loss: 0.00096242
Iteration 14/25 | Loss: 0.00096242
Iteration 15/25 | Loss: 0.00096242
Iteration 16/25 | Loss: 0.00096242
Iteration 17/25 | Loss: 0.00096242
Iteration 18/25 | Loss: 0.00096242
Iteration 19/25 | Loss: 0.00096242
Iteration 20/25 | Loss: 0.00096242
Iteration 21/25 | Loss: 0.00096242
Iteration 22/25 | Loss: 0.00096242
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009624169324524701, 0.0009624169324524701, 0.0009624169324524701, 0.0009624169324524701, 0.0009624169324524701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009624169324524701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096242
Iteration 2/1000 | Loss: 0.00004168
Iteration 3/1000 | Loss: 0.00002981
Iteration 4/1000 | Loss: 0.00002669
Iteration 5/1000 | Loss: 0.00002590
Iteration 6/1000 | Loss: 0.00002531
Iteration 7/1000 | Loss: 0.00002504
Iteration 8/1000 | Loss: 0.00002479
Iteration 9/1000 | Loss: 0.00002458
Iteration 10/1000 | Loss: 0.00002458
Iteration 11/1000 | Loss: 0.00002457
Iteration 12/1000 | Loss: 0.00002447
Iteration 13/1000 | Loss: 0.00002428
Iteration 14/1000 | Loss: 0.00002428
Iteration 15/1000 | Loss: 0.00002427
Iteration 16/1000 | Loss: 0.00002422
Iteration 17/1000 | Loss: 0.00002422
Iteration 18/1000 | Loss: 0.00002415
Iteration 19/1000 | Loss: 0.00002414
Iteration 20/1000 | Loss: 0.00002414
Iteration 21/1000 | Loss: 0.00002413
Iteration 22/1000 | Loss: 0.00002413
Iteration 23/1000 | Loss: 0.00002413
Iteration 24/1000 | Loss: 0.00002413
Iteration 25/1000 | Loss: 0.00002413
Iteration 26/1000 | Loss: 0.00002413
Iteration 27/1000 | Loss: 0.00002412
Iteration 28/1000 | Loss: 0.00002412
Iteration 29/1000 | Loss: 0.00002411
Iteration 30/1000 | Loss: 0.00002411
Iteration 31/1000 | Loss: 0.00002410
Iteration 32/1000 | Loss: 0.00002410
Iteration 33/1000 | Loss: 0.00002408
Iteration 34/1000 | Loss: 0.00002408
Iteration 35/1000 | Loss: 0.00002408
Iteration 36/1000 | Loss: 0.00002408
Iteration 37/1000 | Loss: 0.00002408
Iteration 38/1000 | Loss: 0.00002408
Iteration 39/1000 | Loss: 0.00002408
Iteration 40/1000 | Loss: 0.00002408
Iteration 41/1000 | Loss: 0.00002407
Iteration 42/1000 | Loss: 0.00002407
Iteration 43/1000 | Loss: 0.00002407
Iteration 44/1000 | Loss: 0.00002407
Iteration 45/1000 | Loss: 0.00002407
Iteration 46/1000 | Loss: 0.00002406
Iteration 47/1000 | Loss: 0.00002406
Iteration 48/1000 | Loss: 0.00002406
Iteration 49/1000 | Loss: 0.00002406
Iteration 50/1000 | Loss: 0.00002405
Iteration 51/1000 | Loss: 0.00002405
Iteration 52/1000 | Loss: 0.00002405
Iteration 53/1000 | Loss: 0.00002405
Iteration 54/1000 | Loss: 0.00002405
Iteration 55/1000 | Loss: 0.00002405
Iteration 56/1000 | Loss: 0.00002404
Iteration 57/1000 | Loss: 0.00002404
Iteration 58/1000 | Loss: 0.00002404
Iteration 59/1000 | Loss: 0.00002404
Iteration 60/1000 | Loss: 0.00002403
Iteration 61/1000 | Loss: 0.00002403
Iteration 62/1000 | Loss: 0.00002403
Iteration 63/1000 | Loss: 0.00002403
Iteration 64/1000 | Loss: 0.00002403
Iteration 65/1000 | Loss: 0.00002403
Iteration 66/1000 | Loss: 0.00002403
Iteration 67/1000 | Loss: 0.00002402
Iteration 68/1000 | Loss: 0.00002402
Iteration 69/1000 | Loss: 0.00002402
Iteration 70/1000 | Loss: 0.00002402
Iteration 71/1000 | Loss: 0.00002401
Iteration 72/1000 | Loss: 0.00002401
Iteration 73/1000 | Loss: 0.00002401
Iteration 74/1000 | Loss: 0.00002401
Iteration 75/1000 | Loss: 0.00002401
Iteration 76/1000 | Loss: 0.00002401
Iteration 77/1000 | Loss: 0.00002401
Iteration 78/1000 | Loss: 0.00002401
Iteration 79/1000 | Loss: 0.00002400
Iteration 80/1000 | Loss: 0.00002400
Iteration 81/1000 | Loss: 0.00002400
Iteration 82/1000 | Loss: 0.00002400
Iteration 83/1000 | Loss: 0.00002400
Iteration 84/1000 | Loss: 0.00002400
Iteration 85/1000 | Loss: 0.00002400
Iteration 86/1000 | Loss: 0.00002400
Iteration 87/1000 | Loss: 0.00002399
Iteration 88/1000 | Loss: 0.00002399
Iteration 89/1000 | Loss: 0.00002399
Iteration 90/1000 | Loss: 0.00002399
Iteration 91/1000 | Loss: 0.00002399
Iteration 92/1000 | Loss: 0.00002399
Iteration 93/1000 | Loss: 0.00002399
Iteration 94/1000 | Loss: 0.00002399
Iteration 95/1000 | Loss: 0.00002399
Iteration 96/1000 | Loss: 0.00002399
Iteration 97/1000 | Loss: 0.00002399
Iteration 98/1000 | Loss: 0.00002399
Iteration 99/1000 | Loss: 0.00002399
Iteration 100/1000 | Loss: 0.00002399
Iteration 101/1000 | Loss: 0.00002399
Iteration 102/1000 | Loss: 0.00002399
Iteration 103/1000 | Loss: 0.00002399
Iteration 104/1000 | Loss: 0.00002399
Iteration 105/1000 | Loss: 0.00002399
Iteration 106/1000 | Loss: 0.00002399
Iteration 107/1000 | Loss: 0.00002399
Iteration 108/1000 | Loss: 0.00002399
Iteration 109/1000 | Loss: 0.00002399
Iteration 110/1000 | Loss: 0.00002399
Iteration 111/1000 | Loss: 0.00002399
Iteration 112/1000 | Loss: 0.00002399
Iteration 113/1000 | Loss: 0.00002399
Iteration 114/1000 | Loss: 0.00002399
Iteration 115/1000 | Loss: 0.00002399
Iteration 116/1000 | Loss: 0.00002399
Iteration 117/1000 | Loss: 0.00002399
Iteration 118/1000 | Loss: 0.00002399
Iteration 119/1000 | Loss: 0.00002399
Iteration 120/1000 | Loss: 0.00002399
Iteration 121/1000 | Loss: 0.00002399
Iteration 122/1000 | Loss: 0.00002399
Iteration 123/1000 | Loss: 0.00002399
Iteration 124/1000 | Loss: 0.00002399
Iteration 125/1000 | Loss: 0.00002399
Iteration 126/1000 | Loss: 0.00002399
Iteration 127/1000 | Loss: 0.00002399
Iteration 128/1000 | Loss: 0.00002399
Iteration 129/1000 | Loss: 0.00002399
Iteration 130/1000 | Loss: 0.00002399
Iteration 131/1000 | Loss: 0.00002399
Iteration 132/1000 | Loss: 0.00002399
Iteration 133/1000 | Loss: 0.00002399
Iteration 134/1000 | Loss: 0.00002399
Iteration 135/1000 | Loss: 0.00002399
Iteration 136/1000 | Loss: 0.00002399
Iteration 137/1000 | Loss: 0.00002399
Iteration 138/1000 | Loss: 0.00002399
Iteration 139/1000 | Loss: 0.00002399
Iteration 140/1000 | Loss: 0.00002399
Iteration 141/1000 | Loss: 0.00002399
Iteration 142/1000 | Loss: 0.00002399
Iteration 143/1000 | Loss: 0.00002399
Iteration 144/1000 | Loss: 0.00002399
Iteration 145/1000 | Loss: 0.00002399
Iteration 146/1000 | Loss: 0.00002399
Iteration 147/1000 | Loss: 0.00002399
Iteration 148/1000 | Loss: 0.00002399
Iteration 149/1000 | Loss: 0.00002399
Iteration 150/1000 | Loss: 0.00002399
Iteration 151/1000 | Loss: 0.00002399
Iteration 152/1000 | Loss: 0.00002399
Iteration 153/1000 | Loss: 0.00002399
Iteration 154/1000 | Loss: 0.00002399
Iteration 155/1000 | Loss: 0.00002399
Iteration 156/1000 | Loss: 0.00002399
Iteration 157/1000 | Loss: 0.00002399
Iteration 158/1000 | Loss: 0.00002399
Iteration 159/1000 | Loss: 0.00002399
Iteration 160/1000 | Loss: 0.00002399
Iteration 161/1000 | Loss: 0.00002399
Iteration 162/1000 | Loss: 0.00002399
Iteration 163/1000 | Loss: 0.00002399
Iteration 164/1000 | Loss: 0.00002399
Iteration 165/1000 | Loss: 0.00002399
Iteration 166/1000 | Loss: 0.00002399
Iteration 167/1000 | Loss: 0.00002399
Iteration 168/1000 | Loss: 0.00002399
Iteration 169/1000 | Loss: 0.00002399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.399387267359998e-05, 2.399387267359998e-05, 2.399387267359998e-05, 2.399387267359998e-05, 2.399387267359998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.399387267359998e-05

Optimization complete. Final v2v error: 4.015430450439453 mm

Highest mean error: 4.4865312576293945 mm for frame 239

Lowest mean error: 3.6607913970947266 mm for frame 18

Saving results

Total time: 45.1796350479126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947018
Iteration 2/25 | Loss: 0.00190685
Iteration 3/25 | Loss: 0.00142846
Iteration 4/25 | Loss: 0.00141561
Iteration 5/25 | Loss: 0.00141309
Iteration 6/25 | Loss: 0.00141281
Iteration 7/25 | Loss: 0.00141281
Iteration 8/25 | Loss: 0.00141281
Iteration 9/25 | Loss: 0.00141281
Iteration 10/25 | Loss: 0.00141281
Iteration 11/25 | Loss: 0.00141281
Iteration 12/25 | Loss: 0.00141281
Iteration 13/25 | Loss: 0.00141281
Iteration 14/25 | Loss: 0.00141281
Iteration 15/25 | Loss: 0.00141281
Iteration 16/25 | Loss: 0.00141281
Iteration 17/25 | Loss: 0.00141281
Iteration 18/25 | Loss: 0.00141281
Iteration 19/25 | Loss: 0.00141281
Iteration 20/25 | Loss: 0.00141281
Iteration 21/25 | Loss: 0.00141281
Iteration 22/25 | Loss: 0.00141281
Iteration 23/25 | Loss: 0.00141281
Iteration 24/25 | Loss: 0.00141281
Iteration 25/25 | Loss: 0.00141281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20551443
Iteration 2/25 | Loss: 0.00127692
Iteration 3/25 | Loss: 0.00127692
Iteration 4/25 | Loss: 0.00127692
Iteration 5/25 | Loss: 0.00127692
Iteration 6/25 | Loss: 0.00127692
Iteration 7/25 | Loss: 0.00127692
Iteration 8/25 | Loss: 0.00127691
Iteration 9/25 | Loss: 0.00127691
Iteration 10/25 | Loss: 0.00127691
Iteration 11/25 | Loss: 0.00127691
Iteration 12/25 | Loss: 0.00127691
Iteration 13/25 | Loss: 0.00127691
Iteration 14/25 | Loss: 0.00127691
Iteration 15/25 | Loss: 0.00127691
Iteration 16/25 | Loss: 0.00127691
Iteration 17/25 | Loss: 0.00127691
Iteration 18/25 | Loss: 0.00127691
Iteration 19/25 | Loss: 0.00127691
Iteration 20/25 | Loss: 0.00127691
Iteration 21/25 | Loss: 0.00127691
Iteration 22/25 | Loss: 0.00127691
Iteration 23/25 | Loss: 0.00127691
Iteration 24/25 | Loss: 0.00127691
Iteration 25/25 | Loss: 0.00127691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012769140303134918, 0.0012769140303134918, 0.0012769140303134918, 0.0012769140303134918, 0.0012769140303134918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012769140303134918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127691
Iteration 2/1000 | Loss: 0.00007809
Iteration 3/1000 | Loss: 0.00005001
Iteration 4/1000 | Loss: 0.00003880
Iteration 5/1000 | Loss: 0.00003653
Iteration 6/1000 | Loss: 0.00003531
Iteration 7/1000 | Loss: 0.00003490
Iteration 8/1000 | Loss: 0.00003415
Iteration 9/1000 | Loss: 0.00003348
Iteration 10/1000 | Loss: 0.00003298
Iteration 11/1000 | Loss: 0.00003248
Iteration 12/1000 | Loss: 0.00003213
Iteration 13/1000 | Loss: 0.00003178
Iteration 14/1000 | Loss: 0.00003145
Iteration 15/1000 | Loss: 0.00003120
Iteration 16/1000 | Loss: 0.00003096
Iteration 17/1000 | Loss: 0.00003082
Iteration 18/1000 | Loss: 0.00003074
Iteration 19/1000 | Loss: 0.00003065
Iteration 20/1000 | Loss: 0.00003057
Iteration 21/1000 | Loss: 0.00003049
Iteration 22/1000 | Loss: 0.00003032
Iteration 23/1000 | Loss: 0.00003028
Iteration 24/1000 | Loss: 0.00003027
Iteration 25/1000 | Loss: 0.00003020
Iteration 26/1000 | Loss: 0.00003020
Iteration 27/1000 | Loss: 0.00003013
Iteration 28/1000 | Loss: 0.00003013
Iteration 29/1000 | Loss: 0.00003012
Iteration 30/1000 | Loss: 0.00003011
Iteration 31/1000 | Loss: 0.00003011
Iteration 32/1000 | Loss: 0.00003010
Iteration 33/1000 | Loss: 0.00003009
Iteration 34/1000 | Loss: 0.00003009
Iteration 35/1000 | Loss: 0.00003005
Iteration 36/1000 | Loss: 0.00003004
Iteration 37/1000 | Loss: 0.00003002
Iteration 38/1000 | Loss: 0.00003001
Iteration 39/1000 | Loss: 0.00003001
Iteration 40/1000 | Loss: 0.00003001
Iteration 41/1000 | Loss: 0.00003001
Iteration 42/1000 | Loss: 0.00003001
Iteration 43/1000 | Loss: 0.00003001
Iteration 44/1000 | Loss: 0.00003001
Iteration 45/1000 | Loss: 0.00003001
Iteration 46/1000 | Loss: 0.00003001
Iteration 47/1000 | Loss: 0.00003001
Iteration 48/1000 | Loss: 0.00003001
Iteration 49/1000 | Loss: 0.00003000
Iteration 50/1000 | Loss: 0.00003000
Iteration 51/1000 | Loss: 0.00003000
Iteration 52/1000 | Loss: 0.00002999
Iteration 53/1000 | Loss: 0.00002998
Iteration 54/1000 | Loss: 0.00002997
Iteration 55/1000 | Loss: 0.00002992
Iteration 56/1000 | Loss: 0.00002992
Iteration 57/1000 | Loss: 0.00002992
Iteration 58/1000 | Loss: 0.00002992
Iteration 59/1000 | Loss: 0.00002991
Iteration 60/1000 | Loss: 0.00002991
Iteration 61/1000 | Loss: 0.00002991
Iteration 62/1000 | Loss: 0.00002991
Iteration 63/1000 | Loss: 0.00002991
Iteration 64/1000 | Loss: 0.00002991
Iteration 65/1000 | Loss: 0.00002991
Iteration 66/1000 | Loss: 0.00002991
Iteration 67/1000 | Loss: 0.00002991
Iteration 68/1000 | Loss: 0.00002990
Iteration 69/1000 | Loss: 0.00002990
Iteration 70/1000 | Loss: 0.00002990
Iteration 71/1000 | Loss: 0.00002990
Iteration 72/1000 | Loss: 0.00002990
Iteration 73/1000 | Loss: 0.00002990
Iteration 74/1000 | Loss: 0.00002990
Iteration 75/1000 | Loss: 0.00002990
Iteration 76/1000 | Loss: 0.00002990
Iteration 77/1000 | Loss: 0.00002990
Iteration 78/1000 | Loss: 0.00002990
Iteration 79/1000 | Loss: 0.00002990
Iteration 80/1000 | Loss: 0.00002989
Iteration 81/1000 | Loss: 0.00002989
Iteration 82/1000 | Loss: 0.00002989
Iteration 83/1000 | Loss: 0.00002989
Iteration 84/1000 | Loss: 0.00002988
Iteration 85/1000 | Loss: 0.00002988
Iteration 86/1000 | Loss: 0.00002988
Iteration 87/1000 | Loss: 0.00002987
Iteration 88/1000 | Loss: 0.00002987
Iteration 89/1000 | Loss: 0.00002987
Iteration 90/1000 | Loss: 0.00002987
Iteration 91/1000 | Loss: 0.00002986
Iteration 92/1000 | Loss: 0.00002986
Iteration 93/1000 | Loss: 0.00002986
Iteration 94/1000 | Loss: 0.00002986
Iteration 95/1000 | Loss: 0.00002986
Iteration 96/1000 | Loss: 0.00002986
Iteration 97/1000 | Loss: 0.00002986
Iteration 98/1000 | Loss: 0.00002986
Iteration 99/1000 | Loss: 0.00002986
Iteration 100/1000 | Loss: 0.00002986
Iteration 101/1000 | Loss: 0.00002985
Iteration 102/1000 | Loss: 0.00002985
Iteration 103/1000 | Loss: 0.00002985
Iteration 104/1000 | Loss: 0.00002985
Iteration 105/1000 | Loss: 0.00002985
Iteration 106/1000 | Loss: 0.00002985
Iteration 107/1000 | Loss: 0.00002985
Iteration 108/1000 | Loss: 0.00002984
Iteration 109/1000 | Loss: 0.00002984
Iteration 110/1000 | Loss: 0.00002984
Iteration 111/1000 | Loss: 0.00002984
Iteration 112/1000 | Loss: 0.00002984
Iteration 113/1000 | Loss: 0.00002984
Iteration 114/1000 | Loss: 0.00002984
Iteration 115/1000 | Loss: 0.00002984
Iteration 116/1000 | Loss: 0.00002984
Iteration 117/1000 | Loss: 0.00002984
Iteration 118/1000 | Loss: 0.00002984
Iteration 119/1000 | Loss: 0.00002984
Iteration 120/1000 | Loss: 0.00002984
Iteration 121/1000 | Loss: 0.00002984
Iteration 122/1000 | Loss: 0.00002984
Iteration 123/1000 | Loss: 0.00002984
Iteration 124/1000 | Loss: 0.00002984
Iteration 125/1000 | Loss: 0.00002983
Iteration 126/1000 | Loss: 0.00002983
Iteration 127/1000 | Loss: 0.00002983
Iteration 128/1000 | Loss: 0.00002983
Iteration 129/1000 | Loss: 0.00002983
Iteration 130/1000 | Loss: 0.00002983
Iteration 131/1000 | Loss: 0.00002983
Iteration 132/1000 | Loss: 0.00002983
Iteration 133/1000 | Loss: 0.00002982
Iteration 134/1000 | Loss: 0.00002982
Iteration 135/1000 | Loss: 0.00002982
Iteration 136/1000 | Loss: 0.00002982
Iteration 137/1000 | Loss: 0.00002982
Iteration 138/1000 | Loss: 0.00002982
Iteration 139/1000 | Loss: 0.00002982
Iteration 140/1000 | Loss: 0.00002982
Iteration 141/1000 | Loss: 0.00002982
Iteration 142/1000 | Loss: 0.00002982
Iteration 143/1000 | Loss: 0.00002982
Iteration 144/1000 | Loss: 0.00002981
Iteration 145/1000 | Loss: 0.00002981
Iteration 146/1000 | Loss: 0.00002981
Iteration 147/1000 | Loss: 0.00002981
Iteration 148/1000 | Loss: 0.00002981
Iteration 149/1000 | Loss: 0.00002981
Iteration 150/1000 | Loss: 0.00002980
Iteration 151/1000 | Loss: 0.00002980
Iteration 152/1000 | Loss: 0.00002980
Iteration 153/1000 | Loss: 0.00002980
Iteration 154/1000 | Loss: 0.00002980
Iteration 155/1000 | Loss: 0.00002980
Iteration 156/1000 | Loss: 0.00002980
Iteration 157/1000 | Loss: 0.00002980
Iteration 158/1000 | Loss: 0.00002980
Iteration 159/1000 | Loss: 0.00002980
Iteration 160/1000 | Loss: 0.00002980
Iteration 161/1000 | Loss: 0.00002980
Iteration 162/1000 | Loss: 0.00002980
Iteration 163/1000 | Loss: 0.00002980
Iteration 164/1000 | Loss: 0.00002979
Iteration 165/1000 | Loss: 0.00002979
Iteration 166/1000 | Loss: 0.00002979
Iteration 167/1000 | Loss: 0.00002979
Iteration 168/1000 | Loss: 0.00002979
Iteration 169/1000 | Loss: 0.00002979
Iteration 170/1000 | Loss: 0.00002979
Iteration 171/1000 | Loss: 0.00002979
Iteration 172/1000 | Loss: 0.00002979
Iteration 173/1000 | Loss: 0.00002979
Iteration 174/1000 | Loss: 0.00002979
Iteration 175/1000 | Loss: 0.00002979
Iteration 176/1000 | Loss: 0.00002979
Iteration 177/1000 | Loss: 0.00002978
Iteration 178/1000 | Loss: 0.00002978
Iteration 179/1000 | Loss: 0.00002978
Iteration 180/1000 | Loss: 0.00002978
Iteration 181/1000 | Loss: 0.00002978
Iteration 182/1000 | Loss: 0.00002978
Iteration 183/1000 | Loss: 0.00002978
Iteration 184/1000 | Loss: 0.00002978
Iteration 185/1000 | Loss: 0.00002978
Iteration 186/1000 | Loss: 0.00002978
Iteration 187/1000 | Loss: 0.00002978
Iteration 188/1000 | Loss: 0.00002978
Iteration 189/1000 | Loss: 0.00002978
Iteration 190/1000 | Loss: 0.00002978
Iteration 191/1000 | Loss: 0.00002978
Iteration 192/1000 | Loss: 0.00002978
Iteration 193/1000 | Loss: 0.00002978
Iteration 194/1000 | Loss: 0.00002978
Iteration 195/1000 | Loss: 0.00002978
Iteration 196/1000 | Loss: 0.00002978
Iteration 197/1000 | Loss: 0.00002978
Iteration 198/1000 | Loss: 0.00002978
Iteration 199/1000 | Loss: 0.00002977
Iteration 200/1000 | Loss: 0.00002977
Iteration 201/1000 | Loss: 0.00002977
Iteration 202/1000 | Loss: 0.00002977
Iteration 203/1000 | Loss: 0.00002977
Iteration 204/1000 | Loss: 0.00002977
Iteration 205/1000 | Loss: 0.00002977
Iteration 206/1000 | Loss: 0.00002977
Iteration 207/1000 | Loss: 0.00002977
Iteration 208/1000 | Loss: 0.00002977
Iteration 209/1000 | Loss: 0.00002977
Iteration 210/1000 | Loss: 0.00002977
Iteration 211/1000 | Loss: 0.00002977
Iteration 212/1000 | Loss: 0.00002977
Iteration 213/1000 | Loss: 0.00002977
Iteration 214/1000 | Loss: 0.00002977
Iteration 215/1000 | Loss: 0.00002977
Iteration 216/1000 | Loss: 0.00002977
Iteration 217/1000 | Loss: 0.00002977
Iteration 218/1000 | Loss: 0.00002977
Iteration 219/1000 | Loss: 0.00002977
Iteration 220/1000 | Loss: 0.00002977
Iteration 221/1000 | Loss: 0.00002977
Iteration 222/1000 | Loss: 0.00002977
Iteration 223/1000 | Loss: 0.00002977
Iteration 224/1000 | Loss: 0.00002977
Iteration 225/1000 | Loss: 0.00002977
Iteration 226/1000 | Loss: 0.00002977
Iteration 227/1000 | Loss: 0.00002977
Iteration 228/1000 | Loss: 0.00002977
Iteration 229/1000 | Loss: 0.00002977
Iteration 230/1000 | Loss: 0.00002977
Iteration 231/1000 | Loss: 0.00002977
Iteration 232/1000 | Loss: 0.00002977
Iteration 233/1000 | Loss: 0.00002977
Iteration 234/1000 | Loss: 0.00002977
Iteration 235/1000 | Loss: 0.00002977
Iteration 236/1000 | Loss: 0.00002977
Iteration 237/1000 | Loss: 0.00002977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [2.9766280931653455e-05, 2.9766280931653455e-05, 2.9766280931653455e-05, 2.9766280931653455e-05, 2.9766280931653455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9766280931653455e-05

Optimization complete. Final v2v error: 4.442788600921631 mm

Highest mean error: 5.117981910705566 mm for frame 90

Lowest mean error: 4.0749688148498535 mm for frame 59

Saving results

Total time: 53.21845746040344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00191314
Iteration 2/25 | Loss: 0.00117813
Iteration 3/25 | Loss: 0.00111537
Iteration 4/25 | Loss: 0.00110456
Iteration 5/25 | Loss: 0.00110057
Iteration 6/25 | Loss: 0.00109979
Iteration 7/25 | Loss: 0.00109979
Iteration 8/25 | Loss: 0.00109979
Iteration 9/25 | Loss: 0.00109979
Iteration 10/25 | Loss: 0.00109979
Iteration 11/25 | Loss: 0.00109979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00109979382250458, 0.00109979382250458, 0.00109979382250458, 0.00109979382250458, 0.00109979382250458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00109979382250458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27997160
Iteration 2/25 | Loss: 0.00220519
Iteration 3/25 | Loss: 0.00220519
Iteration 4/25 | Loss: 0.00220519
Iteration 5/25 | Loss: 0.00220519
Iteration 6/25 | Loss: 0.00220519
Iteration 7/25 | Loss: 0.00220519
Iteration 8/25 | Loss: 0.00220519
Iteration 9/25 | Loss: 0.00220519
Iteration 10/25 | Loss: 0.00220519
Iteration 11/25 | Loss: 0.00220519
Iteration 12/25 | Loss: 0.00220519
Iteration 13/25 | Loss: 0.00220519
Iteration 14/25 | Loss: 0.00220519
Iteration 15/25 | Loss: 0.00220519
Iteration 16/25 | Loss: 0.00220519
Iteration 17/25 | Loss: 0.00220519
Iteration 18/25 | Loss: 0.00220519
Iteration 19/25 | Loss: 0.00220519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0022051921114325523, 0.0022051921114325523, 0.0022051921114325523, 0.0022051921114325523, 0.0022051921114325523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022051921114325523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220519
Iteration 2/1000 | Loss: 0.00003058
Iteration 3/1000 | Loss: 0.00001850
Iteration 4/1000 | Loss: 0.00001426
Iteration 5/1000 | Loss: 0.00001322
Iteration 6/1000 | Loss: 0.00001208
Iteration 7/1000 | Loss: 0.00001159
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001108
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001045
Iteration 12/1000 | Loss: 0.00001034
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001033
Iteration 15/1000 | Loss: 0.00001033
Iteration 16/1000 | Loss: 0.00001029
Iteration 17/1000 | Loss: 0.00001026
Iteration 18/1000 | Loss: 0.00001024
Iteration 19/1000 | Loss: 0.00001024
Iteration 20/1000 | Loss: 0.00001017
Iteration 21/1000 | Loss: 0.00001016
Iteration 22/1000 | Loss: 0.00001015
Iteration 23/1000 | Loss: 0.00001015
Iteration 24/1000 | Loss: 0.00001010
Iteration 25/1000 | Loss: 0.00001009
Iteration 26/1000 | Loss: 0.00001009
Iteration 27/1000 | Loss: 0.00001009
Iteration 28/1000 | Loss: 0.00001009
Iteration 29/1000 | Loss: 0.00001008
Iteration 30/1000 | Loss: 0.00001008
Iteration 31/1000 | Loss: 0.00001007
Iteration 32/1000 | Loss: 0.00001006
Iteration 33/1000 | Loss: 0.00001005
Iteration 34/1000 | Loss: 0.00001005
Iteration 35/1000 | Loss: 0.00001005
Iteration 36/1000 | Loss: 0.00001004
Iteration 37/1000 | Loss: 0.00001004
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00001003
Iteration 41/1000 | Loss: 0.00001003
Iteration 42/1000 | Loss: 0.00001003
Iteration 43/1000 | Loss: 0.00001002
Iteration 44/1000 | Loss: 0.00001002
Iteration 45/1000 | Loss: 0.00001002
Iteration 46/1000 | Loss: 0.00001002
Iteration 47/1000 | Loss: 0.00001001
Iteration 48/1000 | Loss: 0.00001001
Iteration 49/1000 | Loss: 0.00001001
Iteration 50/1000 | Loss: 0.00001000
Iteration 51/1000 | Loss: 0.00001000
Iteration 52/1000 | Loss: 0.00001000
Iteration 53/1000 | Loss: 0.00001000
Iteration 54/1000 | Loss: 0.00001000
Iteration 55/1000 | Loss: 0.00001000
Iteration 56/1000 | Loss: 0.00000999
Iteration 57/1000 | Loss: 0.00000999
Iteration 58/1000 | Loss: 0.00000998
Iteration 59/1000 | Loss: 0.00000998
Iteration 60/1000 | Loss: 0.00000997
Iteration 61/1000 | Loss: 0.00000997
Iteration 62/1000 | Loss: 0.00000996
Iteration 63/1000 | Loss: 0.00000996
Iteration 64/1000 | Loss: 0.00000996
Iteration 65/1000 | Loss: 0.00000996
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000996
Iteration 68/1000 | Loss: 0.00000996
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000995
Iteration 71/1000 | Loss: 0.00000995
Iteration 72/1000 | Loss: 0.00000995
Iteration 73/1000 | Loss: 0.00000995
Iteration 74/1000 | Loss: 0.00000995
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000993
Iteration 79/1000 | Loss: 0.00000993
Iteration 80/1000 | Loss: 0.00000992
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000991
Iteration 83/1000 | Loss: 0.00000991
Iteration 84/1000 | Loss: 0.00000990
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000990
Iteration 87/1000 | Loss: 0.00000989
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000988
Iteration 90/1000 | Loss: 0.00000987
Iteration 91/1000 | Loss: 0.00000987
Iteration 92/1000 | Loss: 0.00000986
Iteration 93/1000 | Loss: 0.00000986
Iteration 94/1000 | Loss: 0.00000986
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000985
Iteration 97/1000 | Loss: 0.00000984
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000983
Iteration 101/1000 | Loss: 0.00000983
Iteration 102/1000 | Loss: 0.00000982
Iteration 103/1000 | Loss: 0.00000982
Iteration 104/1000 | Loss: 0.00000981
Iteration 105/1000 | Loss: 0.00000981
Iteration 106/1000 | Loss: 0.00000980
Iteration 107/1000 | Loss: 0.00000980
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000978
Iteration 111/1000 | Loss: 0.00000978
Iteration 112/1000 | Loss: 0.00000978
Iteration 113/1000 | Loss: 0.00000978
Iteration 114/1000 | Loss: 0.00000978
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000977
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000976
Iteration 129/1000 | Loss: 0.00000976
Iteration 130/1000 | Loss: 0.00000976
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000975
Iteration 133/1000 | Loss: 0.00000975
Iteration 134/1000 | Loss: 0.00000975
Iteration 135/1000 | Loss: 0.00000975
Iteration 136/1000 | Loss: 0.00000975
Iteration 137/1000 | Loss: 0.00000975
Iteration 138/1000 | Loss: 0.00000975
Iteration 139/1000 | Loss: 0.00000975
Iteration 140/1000 | Loss: 0.00000975
Iteration 141/1000 | Loss: 0.00000974
Iteration 142/1000 | Loss: 0.00000974
Iteration 143/1000 | Loss: 0.00000974
Iteration 144/1000 | Loss: 0.00000974
Iteration 145/1000 | Loss: 0.00000974
Iteration 146/1000 | Loss: 0.00000974
Iteration 147/1000 | Loss: 0.00000974
Iteration 148/1000 | Loss: 0.00000974
Iteration 149/1000 | Loss: 0.00000973
Iteration 150/1000 | Loss: 0.00000973
Iteration 151/1000 | Loss: 0.00000973
Iteration 152/1000 | Loss: 0.00000973
Iteration 153/1000 | Loss: 0.00000973
Iteration 154/1000 | Loss: 0.00000973
Iteration 155/1000 | Loss: 0.00000973
Iteration 156/1000 | Loss: 0.00000973
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000973
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000973
Iteration 164/1000 | Loss: 0.00000972
Iteration 165/1000 | Loss: 0.00000972
Iteration 166/1000 | Loss: 0.00000972
Iteration 167/1000 | Loss: 0.00000972
Iteration 168/1000 | Loss: 0.00000972
Iteration 169/1000 | Loss: 0.00000972
Iteration 170/1000 | Loss: 0.00000972
Iteration 171/1000 | Loss: 0.00000972
Iteration 172/1000 | Loss: 0.00000972
Iteration 173/1000 | Loss: 0.00000972
Iteration 174/1000 | Loss: 0.00000972
Iteration 175/1000 | Loss: 0.00000972
Iteration 176/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [9.720430171000771e-06, 9.720430171000771e-06, 9.720430171000771e-06, 9.720430171000771e-06, 9.720430171000771e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.720430171000771e-06

Optimization complete. Final v2v error: 2.721492290496826 mm

Highest mean error: 3.099821090698242 mm for frame 38

Lowest mean error: 2.4951040744781494 mm for frame 47

Saving results

Total time: 42.53966784477234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982947
Iteration 2/25 | Loss: 0.00226565
Iteration 3/25 | Loss: 0.00175663
Iteration 4/25 | Loss: 0.00162566
Iteration 5/25 | Loss: 0.00162074
Iteration 6/25 | Loss: 0.00157724
Iteration 7/25 | Loss: 0.00144610
Iteration 8/25 | Loss: 0.00132748
Iteration 9/25 | Loss: 0.00129523
Iteration 10/25 | Loss: 0.00125146
Iteration 11/25 | Loss: 0.00124242
Iteration 12/25 | Loss: 0.00122500
Iteration 13/25 | Loss: 0.00121285
Iteration 14/25 | Loss: 0.00120699
Iteration 15/25 | Loss: 0.00121197
Iteration 16/25 | Loss: 0.00121013
Iteration 17/25 | Loss: 0.00120432
Iteration 18/25 | Loss: 0.00119856
Iteration 19/25 | Loss: 0.00119641
Iteration 20/25 | Loss: 0.00119567
Iteration 21/25 | Loss: 0.00119528
Iteration 22/25 | Loss: 0.00119516
Iteration 23/25 | Loss: 0.00119505
Iteration 24/25 | Loss: 0.00119496
Iteration 25/25 | Loss: 0.00119481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40629601
Iteration 2/25 | Loss: 0.00155086
Iteration 3/25 | Loss: 0.00155086
Iteration 4/25 | Loss: 0.00155086
Iteration 5/25 | Loss: 0.00155086
Iteration 6/25 | Loss: 0.00155086
Iteration 7/25 | Loss: 0.00155086
Iteration 8/25 | Loss: 0.00155086
Iteration 9/25 | Loss: 0.00155086
Iteration 10/25 | Loss: 0.00155086
Iteration 11/25 | Loss: 0.00155086
Iteration 12/25 | Loss: 0.00155086
Iteration 13/25 | Loss: 0.00155086
Iteration 14/25 | Loss: 0.00155086
Iteration 15/25 | Loss: 0.00155086
Iteration 16/25 | Loss: 0.00155086
Iteration 17/25 | Loss: 0.00155086
Iteration 18/25 | Loss: 0.00155086
Iteration 19/25 | Loss: 0.00155086
Iteration 20/25 | Loss: 0.00155086
Iteration 21/25 | Loss: 0.00155086
Iteration 22/25 | Loss: 0.00155086
Iteration 23/25 | Loss: 0.00155086
Iteration 24/25 | Loss: 0.00155086
Iteration 25/25 | Loss: 0.00155086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155086
Iteration 2/1000 | Loss: 0.00008884
Iteration 3/1000 | Loss: 0.00029931
Iteration 4/1000 | Loss: 0.00100813
Iteration 5/1000 | Loss: 0.00030498
Iteration 6/1000 | Loss: 0.00011608
Iteration 7/1000 | Loss: 0.00003918
Iteration 8/1000 | Loss: 0.00044403
Iteration 9/1000 | Loss: 0.00042211
Iteration 10/1000 | Loss: 0.00014315
Iteration 11/1000 | Loss: 0.00005862
Iteration 12/1000 | Loss: 0.00003961
Iteration 13/1000 | Loss: 0.00087223
Iteration 14/1000 | Loss: 0.00078713
Iteration 15/1000 | Loss: 0.00045130
Iteration 16/1000 | Loss: 0.00087641
Iteration 17/1000 | Loss: 0.00006299
Iteration 18/1000 | Loss: 0.00019262
Iteration 19/1000 | Loss: 0.00016804
Iteration 20/1000 | Loss: 0.00016191
Iteration 21/1000 | Loss: 0.00027084
Iteration 22/1000 | Loss: 0.00022051
Iteration 23/1000 | Loss: 0.00024395
Iteration 24/1000 | Loss: 0.00029248
Iteration 25/1000 | Loss: 0.00022117
Iteration 26/1000 | Loss: 0.00027669
Iteration 27/1000 | Loss: 0.00022532
Iteration 28/1000 | Loss: 0.00002567
Iteration 29/1000 | Loss: 0.00002390
Iteration 30/1000 | Loss: 0.00002261
Iteration 31/1000 | Loss: 0.00046716
Iteration 32/1000 | Loss: 0.00015111
Iteration 33/1000 | Loss: 0.00002819
Iteration 34/1000 | Loss: 0.00002337
Iteration 35/1000 | Loss: 0.00042768
Iteration 36/1000 | Loss: 0.00003297
Iteration 37/1000 | Loss: 0.00002536
Iteration 38/1000 | Loss: 0.00002076
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001773
Iteration 42/1000 | Loss: 0.00003328
Iteration 43/1000 | Loss: 0.00002808
Iteration 44/1000 | Loss: 0.00002252
Iteration 45/1000 | Loss: 0.00001847
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00002517
Iteration 48/1000 | Loss: 0.00001952
Iteration 49/1000 | Loss: 0.00001635
Iteration 50/1000 | Loss: 0.00001634
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001633
Iteration 53/1000 | Loss: 0.00001633
Iteration 54/1000 | Loss: 0.00001633
Iteration 55/1000 | Loss: 0.00001633
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001629
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001624
Iteration 67/1000 | Loss: 0.00001623
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001614
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001612
Iteration 74/1000 | Loss: 0.00001611
Iteration 75/1000 | Loss: 0.00001603
Iteration 76/1000 | Loss: 0.00001602
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001594
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001591
Iteration 81/1000 | Loss: 0.00001590
Iteration 82/1000 | Loss: 0.00001584
Iteration 83/1000 | Loss: 0.00001578
Iteration 84/1000 | Loss: 0.00001575
Iteration 85/1000 | Loss: 0.00001569
Iteration 86/1000 | Loss: 0.00001569
Iteration 87/1000 | Loss: 0.00001569
Iteration 88/1000 | Loss: 0.00001568
Iteration 89/1000 | Loss: 0.00001568
Iteration 90/1000 | Loss: 0.00001568
Iteration 91/1000 | Loss: 0.00001568
Iteration 92/1000 | Loss: 0.00001568
Iteration 93/1000 | Loss: 0.00001568
Iteration 94/1000 | Loss: 0.00001568
Iteration 95/1000 | Loss: 0.00001568
Iteration 96/1000 | Loss: 0.00001566
Iteration 97/1000 | Loss: 0.00001566
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001565
Iteration 100/1000 | Loss: 0.00001565
Iteration 101/1000 | Loss: 0.00001565
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001564
Iteration 106/1000 | Loss: 0.00001564
Iteration 107/1000 | Loss: 0.00001564
Iteration 108/1000 | Loss: 0.00001563
Iteration 109/1000 | Loss: 0.00001563
Iteration 110/1000 | Loss: 0.00001563
Iteration 111/1000 | Loss: 0.00001563
Iteration 112/1000 | Loss: 0.00001562
Iteration 113/1000 | Loss: 0.00001562
Iteration 114/1000 | Loss: 0.00001562
Iteration 115/1000 | Loss: 0.00001562
Iteration 116/1000 | Loss: 0.00001561
Iteration 117/1000 | Loss: 0.00001561
Iteration 118/1000 | Loss: 0.00001561
Iteration 119/1000 | Loss: 0.00001561
Iteration 120/1000 | Loss: 0.00001560
Iteration 121/1000 | Loss: 0.00001560
Iteration 122/1000 | Loss: 0.00001560
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001560
Iteration 125/1000 | Loss: 0.00001560
Iteration 126/1000 | Loss: 0.00001560
Iteration 127/1000 | Loss: 0.00001560
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001556
Iteration 139/1000 | Loss: 0.00001556
Iteration 140/1000 | Loss: 0.00001556
Iteration 141/1000 | Loss: 0.00001556
Iteration 142/1000 | Loss: 0.00001556
Iteration 143/1000 | Loss: 0.00001555
Iteration 144/1000 | Loss: 0.00001555
Iteration 145/1000 | Loss: 0.00002455
Iteration 146/1000 | Loss: 0.00020141
Iteration 147/1000 | Loss: 0.00038600
Iteration 148/1000 | Loss: 0.00010729
Iteration 149/1000 | Loss: 0.00001486
Iteration 150/1000 | Loss: 0.00001587
Iteration 151/1000 | Loss: 0.00001459
Iteration 152/1000 | Loss: 0.00001345
Iteration 153/1000 | Loss: 0.00001578
Iteration 154/1000 | Loss: 0.00001273
Iteration 155/1000 | Loss: 0.00001244
Iteration 156/1000 | Loss: 0.00001228
Iteration 157/1000 | Loss: 0.00001228
Iteration 158/1000 | Loss: 0.00001228
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001938
Iteration 161/1000 | Loss: 0.00001342
Iteration 162/1000 | Loss: 0.00001212
Iteration 163/1000 | Loss: 0.00001206
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001203
Iteration 166/1000 | Loss: 0.00001203
Iteration 167/1000 | Loss: 0.00001203
Iteration 168/1000 | Loss: 0.00001203
Iteration 169/1000 | Loss: 0.00001203
Iteration 170/1000 | Loss: 0.00001203
Iteration 171/1000 | Loss: 0.00001203
Iteration 172/1000 | Loss: 0.00001203
Iteration 173/1000 | Loss: 0.00001203
Iteration 174/1000 | Loss: 0.00001203
Iteration 175/1000 | Loss: 0.00001202
Iteration 176/1000 | Loss: 0.00001202
Iteration 177/1000 | Loss: 0.00001202
Iteration 178/1000 | Loss: 0.00001202
Iteration 179/1000 | Loss: 0.00001202
Iteration 180/1000 | Loss: 0.00001202
Iteration 181/1000 | Loss: 0.00001202
Iteration 182/1000 | Loss: 0.00001202
Iteration 183/1000 | Loss: 0.00001202
Iteration 184/1000 | Loss: 0.00001202
Iteration 185/1000 | Loss: 0.00001202
Iteration 186/1000 | Loss: 0.00001202
Iteration 187/1000 | Loss: 0.00001202
Iteration 188/1000 | Loss: 0.00001202
Iteration 189/1000 | Loss: 0.00001202
Iteration 190/1000 | Loss: 0.00001202
Iteration 191/1000 | Loss: 0.00001202
Iteration 192/1000 | Loss: 0.00001202
Iteration 193/1000 | Loss: 0.00001202
Iteration 194/1000 | Loss: 0.00001202
Iteration 195/1000 | Loss: 0.00001202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.2015651009278372e-05, 1.2015651009278372e-05, 1.2015651009278372e-05, 1.2015651009278372e-05, 1.2015651009278372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2015651009278372e-05

Optimization complete. Final v2v error: 2.912785768508911 mm

Highest mean error: 4.74409818649292 mm for frame 79

Lowest mean error: 2.478996753692627 mm for frame 51

Saving results

Total time: 147.00549507141113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835634
Iteration 2/25 | Loss: 0.00127133
Iteration 3/25 | Loss: 0.00115545
Iteration 4/25 | Loss: 0.00114410
Iteration 5/25 | Loss: 0.00114210
Iteration 6/25 | Loss: 0.00114172
Iteration 7/25 | Loss: 0.00114172
Iteration 8/25 | Loss: 0.00114172
Iteration 9/25 | Loss: 0.00114172
Iteration 10/25 | Loss: 0.00114172
Iteration 11/25 | Loss: 0.00114172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011417160276323557, 0.0011417160276323557, 0.0011417160276323557, 0.0011417160276323557, 0.0011417160276323557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011417160276323557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29043663
Iteration 2/25 | Loss: 0.00110116
Iteration 3/25 | Loss: 0.00110113
Iteration 4/25 | Loss: 0.00110113
Iteration 5/25 | Loss: 0.00110113
Iteration 6/25 | Loss: 0.00110113
Iteration 7/25 | Loss: 0.00110113
Iteration 8/25 | Loss: 0.00110113
Iteration 9/25 | Loss: 0.00110113
Iteration 10/25 | Loss: 0.00110113
Iteration 11/25 | Loss: 0.00110113
Iteration 12/25 | Loss: 0.00110113
Iteration 13/25 | Loss: 0.00110113
Iteration 14/25 | Loss: 0.00110113
Iteration 15/25 | Loss: 0.00110113
Iteration 16/25 | Loss: 0.00110113
Iteration 17/25 | Loss: 0.00110113
Iteration 18/25 | Loss: 0.00110113
Iteration 19/25 | Loss: 0.00110113
Iteration 20/25 | Loss: 0.00110113
Iteration 21/25 | Loss: 0.00110113
Iteration 22/25 | Loss: 0.00110113
Iteration 23/25 | Loss: 0.00110113
Iteration 24/25 | Loss: 0.00110113
Iteration 25/25 | Loss: 0.00110113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110113
Iteration 2/1000 | Loss: 0.00002022
Iteration 3/1000 | Loss: 0.00001317
Iteration 4/1000 | Loss: 0.00001160
Iteration 5/1000 | Loss: 0.00001047
Iteration 6/1000 | Loss: 0.00000988
Iteration 7/1000 | Loss: 0.00000949
Iteration 8/1000 | Loss: 0.00000911
Iteration 9/1000 | Loss: 0.00000896
Iteration 10/1000 | Loss: 0.00000895
Iteration 11/1000 | Loss: 0.00000877
Iteration 12/1000 | Loss: 0.00000866
Iteration 13/1000 | Loss: 0.00000861
Iteration 14/1000 | Loss: 0.00000861
Iteration 15/1000 | Loss: 0.00000858
Iteration 16/1000 | Loss: 0.00000855
Iteration 17/1000 | Loss: 0.00000847
Iteration 18/1000 | Loss: 0.00000845
Iteration 19/1000 | Loss: 0.00000843
Iteration 20/1000 | Loss: 0.00000842
Iteration 21/1000 | Loss: 0.00000840
Iteration 22/1000 | Loss: 0.00000838
Iteration 23/1000 | Loss: 0.00000835
Iteration 24/1000 | Loss: 0.00000833
Iteration 25/1000 | Loss: 0.00000832
Iteration 26/1000 | Loss: 0.00000832
Iteration 27/1000 | Loss: 0.00000825
Iteration 28/1000 | Loss: 0.00000817
Iteration 29/1000 | Loss: 0.00000817
Iteration 30/1000 | Loss: 0.00000817
Iteration 31/1000 | Loss: 0.00000816
Iteration 32/1000 | Loss: 0.00000816
Iteration 33/1000 | Loss: 0.00000816
Iteration 34/1000 | Loss: 0.00000816
Iteration 35/1000 | Loss: 0.00000816
Iteration 36/1000 | Loss: 0.00000815
Iteration 37/1000 | Loss: 0.00000814
Iteration 38/1000 | Loss: 0.00000813
Iteration 39/1000 | Loss: 0.00000813
Iteration 40/1000 | Loss: 0.00000813
Iteration 41/1000 | Loss: 0.00000813
Iteration 42/1000 | Loss: 0.00000812
Iteration 43/1000 | Loss: 0.00000812
Iteration 44/1000 | Loss: 0.00000812
Iteration 45/1000 | Loss: 0.00000812
Iteration 46/1000 | Loss: 0.00000812
Iteration 47/1000 | Loss: 0.00000812
Iteration 48/1000 | Loss: 0.00000811
Iteration 49/1000 | Loss: 0.00000811
Iteration 50/1000 | Loss: 0.00000811
Iteration 51/1000 | Loss: 0.00000811
Iteration 52/1000 | Loss: 0.00000811
Iteration 53/1000 | Loss: 0.00000811
Iteration 54/1000 | Loss: 0.00000811
Iteration 55/1000 | Loss: 0.00000810
Iteration 56/1000 | Loss: 0.00000810
Iteration 57/1000 | Loss: 0.00000810
Iteration 58/1000 | Loss: 0.00000810
Iteration 59/1000 | Loss: 0.00000810
Iteration 60/1000 | Loss: 0.00000810
Iteration 61/1000 | Loss: 0.00000809
Iteration 62/1000 | Loss: 0.00000809
Iteration 63/1000 | Loss: 0.00000809
Iteration 64/1000 | Loss: 0.00000809
Iteration 65/1000 | Loss: 0.00000809
Iteration 66/1000 | Loss: 0.00000809
Iteration 67/1000 | Loss: 0.00000808
Iteration 68/1000 | Loss: 0.00000808
Iteration 69/1000 | Loss: 0.00000808
Iteration 70/1000 | Loss: 0.00000808
Iteration 71/1000 | Loss: 0.00000808
Iteration 72/1000 | Loss: 0.00000808
Iteration 73/1000 | Loss: 0.00000808
Iteration 74/1000 | Loss: 0.00000807
Iteration 75/1000 | Loss: 0.00000807
Iteration 76/1000 | Loss: 0.00000807
Iteration 77/1000 | Loss: 0.00000807
Iteration 78/1000 | Loss: 0.00000807
Iteration 79/1000 | Loss: 0.00000807
Iteration 80/1000 | Loss: 0.00000807
Iteration 81/1000 | Loss: 0.00000807
Iteration 82/1000 | Loss: 0.00000807
Iteration 83/1000 | Loss: 0.00000807
Iteration 84/1000 | Loss: 0.00000807
Iteration 85/1000 | Loss: 0.00000806
Iteration 86/1000 | Loss: 0.00000806
Iteration 87/1000 | Loss: 0.00000806
Iteration 88/1000 | Loss: 0.00000806
Iteration 89/1000 | Loss: 0.00000806
Iteration 90/1000 | Loss: 0.00000805
Iteration 91/1000 | Loss: 0.00000805
Iteration 92/1000 | Loss: 0.00000805
Iteration 93/1000 | Loss: 0.00000805
Iteration 94/1000 | Loss: 0.00000804
Iteration 95/1000 | Loss: 0.00000804
Iteration 96/1000 | Loss: 0.00000804
Iteration 97/1000 | Loss: 0.00000804
Iteration 98/1000 | Loss: 0.00000804
Iteration 99/1000 | Loss: 0.00000804
Iteration 100/1000 | Loss: 0.00000804
Iteration 101/1000 | Loss: 0.00000803
Iteration 102/1000 | Loss: 0.00000803
Iteration 103/1000 | Loss: 0.00000803
Iteration 104/1000 | Loss: 0.00000803
Iteration 105/1000 | Loss: 0.00000802
Iteration 106/1000 | Loss: 0.00000802
Iteration 107/1000 | Loss: 0.00000802
Iteration 108/1000 | Loss: 0.00000801
Iteration 109/1000 | Loss: 0.00000801
Iteration 110/1000 | Loss: 0.00000801
Iteration 111/1000 | Loss: 0.00000800
Iteration 112/1000 | Loss: 0.00000800
Iteration 113/1000 | Loss: 0.00000800
Iteration 114/1000 | Loss: 0.00000800
Iteration 115/1000 | Loss: 0.00000800
Iteration 116/1000 | Loss: 0.00000800
Iteration 117/1000 | Loss: 0.00000800
Iteration 118/1000 | Loss: 0.00000800
Iteration 119/1000 | Loss: 0.00000800
Iteration 120/1000 | Loss: 0.00000800
Iteration 121/1000 | Loss: 0.00000799
Iteration 122/1000 | Loss: 0.00000799
Iteration 123/1000 | Loss: 0.00000799
Iteration 124/1000 | Loss: 0.00000799
Iteration 125/1000 | Loss: 0.00000798
Iteration 126/1000 | Loss: 0.00000798
Iteration 127/1000 | Loss: 0.00000798
Iteration 128/1000 | Loss: 0.00000797
Iteration 129/1000 | Loss: 0.00000797
Iteration 130/1000 | Loss: 0.00000797
Iteration 131/1000 | Loss: 0.00000796
Iteration 132/1000 | Loss: 0.00000796
Iteration 133/1000 | Loss: 0.00000796
Iteration 134/1000 | Loss: 0.00000796
Iteration 135/1000 | Loss: 0.00000795
Iteration 136/1000 | Loss: 0.00000795
Iteration 137/1000 | Loss: 0.00000795
Iteration 138/1000 | Loss: 0.00000795
Iteration 139/1000 | Loss: 0.00000795
Iteration 140/1000 | Loss: 0.00000795
Iteration 141/1000 | Loss: 0.00000795
Iteration 142/1000 | Loss: 0.00000795
Iteration 143/1000 | Loss: 0.00000795
Iteration 144/1000 | Loss: 0.00000795
Iteration 145/1000 | Loss: 0.00000795
Iteration 146/1000 | Loss: 0.00000795
Iteration 147/1000 | Loss: 0.00000795
Iteration 148/1000 | Loss: 0.00000794
Iteration 149/1000 | Loss: 0.00000794
Iteration 150/1000 | Loss: 0.00000794
Iteration 151/1000 | Loss: 0.00000794
Iteration 152/1000 | Loss: 0.00000794
Iteration 153/1000 | Loss: 0.00000794
Iteration 154/1000 | Loss: 0.00000793
Iteration 155/1000 | Loss: 0.00000793
Iteration 156/1000 | Loss: 0.00000793
Iteration 157/1000 | Loss: 0.00000793
Iteration 158/1000 | Loss: 0.00000793
Iteration 159/1000 | Loss: 0.00000792
Iteration 160/1000 | Loss: 0.00000792
Iteration 161/1000 | Loss: 0.00000792
Iteration 162/1000 | Loss: 0.00000792
Iteration 163/1000 | Loss: 0.00000792
Iteration 164/1000 | Loss: 0.00000792
Iteration 165/1000 | Loss: 0.00000792
Iteration 166/1000 | Loss: 0.00000792
Iteration 167/1000 | Loss: 0.00000792
Iteration 168/1000 | Loss: 0.00000792
Iteration 169/1000 | Loss: 0.00000792
Iteration 170/1000 | Loss: 0.00000792
Iteration 171/1000 | Loss: 0.00000792
Iteration 172/1000 | Loss: 0.00000791
Iteration 173/1000 | Loss: 0.00000791
Iteration 174/1000 | Loss: 0.00000791
Iteration 175/1000 | Loss: 0.00000791
Iteration 176/1000 | Loss: 0.00000791
Iteration 177/1000 | Loss: 0.00000791
Iteration 178/1000 | Loss: 0.00000791
Iteration 179/1000 | Loss: 0.00000791
Iteration 180/1000 | Loss: 0.00000791
Iteration 181/1000 | Loss: 0.00000791
Iteration 182/1000 | Loss: 0.00000791
Iteration 183/1000 | Loss: 0.00000791
Iteration 184/1000 | Loss: 0.00000791
Iteration 185/1000 | Loss: 0.00000791
Iteration 186/1000 | Loss: 0.00000791
Iteration 187/1000 | Loss: 0.00000790
Iteration 188/1000 | Loss: 0.00000790
Iteration 189/1000 | Loss: 0.00000790
Iteration 190/1000 | Loss: 0.00000790
Iteration 191/1000 | Loss: 0.00000790
Iteration 192/1000 | Loss: 0.00000790
Iteration 193/1000 | Loss: 0.00000790
Iteration 194/1000 | Loss: 0.00000790
Iteration 195/1000 | Loss: 0.00000790
Iteration 196/1000 | Loss: 0.00000790
Iteration 197/1000 | Loss: 0.00000789
Iteration 198/1000 | Loss: 0.00000789
Iteration 199/1000 | Loss: 0.00000789
Iteration 200/1000 | Loss: 0.00000789
Iteration 201/1000 | Loss: 0.00000789
Iteration 202/1000 | Loss: 0.00000789
Iteration 203/1000 | Loss: 0.00000789
Iteration 204/1000 | Loss: 0.00000789
Iteration 205/1000 | Loss: 0.00000789
Iteration 206/1000 | Loss: 0.00000789
Iteration 207/1000 | Loss: 0.00000789
Iteration 208/1000 | Loss: 0.00000789
Iteration 209/1000 | Loss: 0.00000789
Iteration 210/1000 | Loss: 0.00000789
Iteration 211/1000 | Loss: 0.00000789
Iteration 212/1000 | Loss: 0.00000789
Iteration 213/1000 | Loss: 0.00000789
Iteration 214/1000 | Loss: 0.00000789
Iteration 215/1000 | Loss: 0.00000788
Iteration 216/1000 | Loss: 0.00000788
Iteration 217/1000 | Loss: 0.00000788
Iteration 218/1000 | Loss: 0.00000788
Iteration 219/1000 | Loss: 0.00000788
Iteration 220/1000 | Loss: 0.00000788
Iteration 221/1000 | Loss: 0.00000788
Iteration 222/1000 | Loss: 0.00000788
Iteration 223/1000 | Loss: 0.00000788
Iteration 224/1000 | Loss: 0.00000788
Iteration 225/1000 | Loss: 0.00000788
Iteration 226/1000 | Loss: 0.00000787
Iteration 227/1000 | Loss: 0.00000787
Iteration 228/1000 | Loss: 0.00000787
Iteration 229/1000 | Loss: 0.00000787
Iteration 230/1000 | Loss: 0.00000787
Iteration 231/1000 | Loss: 0.00000787
Iteration 232/1000 | Loss: 0.00000787
Iteration 233/1000 | Loss: 0.00000787
Iteration 234/1000 | Loss: 0.00000787
Iteration 235/1000 | Loss: 0.00000787
Iteration 236/1000 | Loss: 0.00000787
Iteration 237/1000 | Loss: 0.00000787
Iteration 238/1000 | Loss: 0.00000787
Iteration 239/1000 | Loss: 0.00000787
Iteration 240/1000 | Loss: 0.00000787
Iteration 241/1000 | Loss: 0.00000787
Iteration 242/1000 | Loss: 0.00000786
Iteration 243/1000 | Loss: 0.00000786
Iteration 244/1000 | Loss: 0.00000786
Iteration 245/1000 | Loss: 0.00000786
Iteration 246/1000 | Loss: 0.00000786
Iteration 247/1000 | Loss: 0.00000786
Iteration 248/1000 | Loss: 0.00000786
Iteration 249/1000 | Loss: 0.00000786
Iteration 250/1000 | Loss: 0.00000786
Iteration 251/1000 | Loss: 0.00000786
Iteration 252/1000 | Loss: 0.00000786
Iteration 253/1000 | Loss: 0.00000786
Iteration 254/1000 | Loss: 0.00000786
Iteration 255/1000 | Loss: 0.00000786
Iteration 256/1000 | Loss: 0.00000786
Iteration 257/1000 | Loss: 0.00000786
Iteration 258/1000 | Loss: 0.00000786
Iteration 259/1000 | Loss: 0.00000786
Iteration 260/1000 | Loss: 0.00000786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [7.85713109507924e-06, 7.85713109507924e-06, 7.85713109507924e-06, 7.85713109507924e-06, 7.85713109507924e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.85713109507924e-06

Optimization complete. Final v2v error: 2.4443628787994385 mm

Highest mean error: 2.6545016765594482 mm for frame 28

Lowest mean error: 2.3427822589874268 mm for frame 89

Saving results

Total time: 42.906513929367065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975180
Iteration 2/25 | Loss: 0.00256142
Iteration 3/25 | Loss: 0.00167721
Iteration 4/25 | Loss: 0.00157346
Iteration 5/25 | Loss: 0.00157264
Iteration 6/25 | Loss: 0.00149265
Iteration 7/25 | Loss: 0.00144664
Iteration 8/25 | Loss: 0.00129456
Iteration 9/25 | Loss: 0.00123144
Iteration 10/25 | Loss: 0.00119928
Iteration 11/25 | Loss: 0.00121283
Iteration 12/25 | Loss: 0.00116902
Iteration 13/25 | Loss: 0.00116000
Iteration 14/25 | Loss: 0.00115987
Iteration 15/25 | Loss: 0.00115761
Iteration 16/25 | Loss: 0.00116085
Iteration 17/25 | Loss: 0.00115741
Iteration 18/25 | Loss: 0.00115739
Iteration 19/25 | Loss: 0.00115738
Iteration 20/25 | Loss: 0.00115738
Iteration 21/25 | Loss: 0.00115738
Iteration 22/25 | Loss: 0.00115738
Iteration 23/25 | Loss: 0.00115738
Iteration 24/25 | Loss: 0.00115738
Iteration 25/25 | Loss: 0.00115738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29129696
Iteration 2/25 | Loss: 0.00144963
Iteration 3/25 | Loss: 0.00144963
Iteration 4/25 | Loss: 0.00120348
Iteration 5/25 | Loss: 0.00120347
Iteration 6/25 | Loss: 0.00120347
Iteration 7/25 | Loss: 0.00120347
Iteration 8/25 | Loss: 0.00120347
Iteration 9/25 | Loss: 0.00120347
Iteration 10/25 | Loss: 0.00120347
Iteration 11/25 | Loss: 0.00120347
Iteration 12/25 | Loss: 0.00120347
Iteration 13/25 | Loss: 0.00120347
Iteration 14/25 | Loss: 0.00120347
Iteration 15/25 | Loss: 0.00120347
Iteration 16/25 | Loss: 0.00120347
Iteration 17/25 | Loss: 0.00120347
Iteration 18/25 | Loss: 0.00120347
Iteration 19/25 | Loss: 0.00120347
Iteration 20/25 | Loss: 0.00120347
Iteration 21/25 | Loss: 0.00120347
Iteration 22/25 | Loss: 0.00120347
Iteration 23/25 | Loss: 0.00120347
Iteration 24/25 | Loss: 0.00120347
Iteration 25/25 | Loss: 0.00120347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120347
Iteration 2/1000 | Loss: 0.00023185
Iteration 3/1000 | Loss: 0.00024439
Iteration 4/1000 | Loss: 0.00090208
Iteration 5/1000 | Loss: 0.00017991
Iteration 6/1000 | Loss: 0.00001934
Iteration 7/1000 | Loss: 0.00002837
Iteration 8/1000 | Loss: 0.00005663
Iteration 9/1000 | Loss: 0.00001698
Iteration 10/1000 | Loss: 0.00002967
Iteration 11/1000 | Loss: 0.00006487
Iteration 12/1000 | Loss: 0.00002984
Iteration 13/1000 | Loss: 0.00012834
Iteration 14/1000 | Loss: 0.00009416
Iteration 15/1000 | Loss: 0.00001684
Iteration 16/1000 | Loss: 0.00004252
Iteration 17/1000 | Loss: 0.00002214
Iteration 18/1000 | Loss: 0.00001418
Iteration 19/1000 | Loss: 0.00005299
Iteration 20/1000 | Loss: 0.00008718
Iteration 21/1000 | Loss: 0.00002703
Iteration 22/1000 | Loss: 0.00003246
Iteration 23/1000 | Loss: 0.00001490
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00002724
Iteration 27/1000 | Loss: 0.00002560
Iteration 28/1000 | Loss: 0.00002560
Iteration 29/1000 | Loss: 0.00013037
Iteration 30/1000 | Loss: 0.00001678
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00004085
Iteration 34/1000 | Loss: 0.00002317
Iteration 35/1000 | Loss: 0.00001377
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001435
Iteration 47/1000 | Loss: 0.00001435
Iteration 48/1000 | Loss: 0.00001435
Iteration 49/1000 | Loss: 0.00002351
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001352
Iteration 52/1000 | Loss: 0.00001352
Iteration 53/1000 | Loss: 0.00001352
Iteration 54/1000 | Loss: 0.00001352
Iteration 55/1000 | Loss: 0.00001352
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001351
Iteration 58/1000 | Loss: 0.00001351
Iteration 59/1000 | Loss: 0.00001351
Iteration 60/1000 | Loss: 0.00001351
Iteration 61/1000 | Loss: 0.00001350
Iteration 62/1000 | Loss: 0.00001350
Iteration 63/1000 | Loss: 0.00001350
Iteration 64/1000 | Loss: 0.00001349
Iteration 65/1000 | Loss: 0.00001349
Iteration 66/1000 | Loss: 0.00001349
Iteration 67/1000 | Loss: 0.00001349
Iteration 68/1000 | Loss: 0.00001349
Iteration 69/1000 | Loss: 0.00001348
Iteration 70/1000 | Loss: 0.00001753
Iteration 71/1000 | Loss: 0.00001347
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001346
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001346
Iteration 82/1000 | Loss: 0.00001346
Iteration 83/1000 | Loss: 0.00001346
Iteration 84/1000 | Loss: 0.00001346
Iteration 85/1000 | Loss: 0.00001346
Iteration 86/1000 | Loss: 0.00001346
Iteration 87/1000 | Loss: 0.00001346
Iteration 88/1000 | Loss: 0.00001346
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001345
Iteration 96/1000 | Loss: 0.00001345
Iteration 97/1000 | Loss: 0.00001345
Iteration 98/1000 | Loss: 0.00001345
Iteration 99/1000 | Loss: 0.00001345
Iteration 100/1000 | Loss: 0.00001345
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001344
Iteration 103/1000 | Loss: 0.00001344
Iteration 104/1000 | Loss: 0.00001344
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001343
Iteration 111/1000 | Loss: 0.00001343
Iteration 112/1000 | Loss: 0.00001343
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001343
Iteration 115/1000 | Loss: 0.00001343
Iteration 116/1000 | Loss: 0.00001343
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001342
Iteration 121/1000 | Loss: 0.00001342
Iteration 122/1000 | Loss: 0.00001342
Iteration 123/1000 | Loss: 0.00001342
Iteration 124/1000 | Loss: 0.00001342
Iteration 125/1000 | Loss: 0.00001342
Iteration 126/1000 | Loss: 0.00001342
Iteration 127/1000 | Loss: 0.00001342
Iteration 128/1000 | Loss: 0.00001342
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001342
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001342
Iteration 134/1000 | Loss: 0.00001342
Iteration 135/1000 | Loss: 0.00001342
Iteration 136/1000 | Loss: 0.00001342
Iteration 137/1000 | Loss: 0.00001342
Iteration 138/1000 | Loss: 0.00001342
Iteration 139/1000 | Loss: 0.00001342
Iteration 140/1000 | Loss: 0.00001342
Iteration 141/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.3423962627712172e-05, 1.3423962627712172e-05, 1.3423962627712172e-05, 1.3423962627712172e-05, 1.3423962627712172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3423962627712172e-05

Optimization complete. Final v2v error: 3.0964229106903076 mm

Highest mean error: 3.6677000522613525 mm for frame 89

Lowest mean error: 2.7760112285614014 mm for frame 131

Saving results

Total time: 87.44900155067444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989161
Iteration 2/25 | Loss: 0.00243125
Iteration 3/25 | Loss: 0.00184868
Iteration 4/25 | Loss: 0.00171384
Iteration 5/25 | Loss: 0.00176152
Iteration 6/25 | Loss: 0.00181738
Iteration 7/25 | Loss: 0.00175562
Iteration 8/25 | Loss: 0.00160601
Iteration 9/25 | Loss: 0.00151436
Iteration 10/25 | Loss: 0.00147373
Iteration 11/25 | Loss: 0.00144765
Iteration 12/25 | Loss: 0.00144380
Iteration 13/25 | Loss: 0.00142420
Iteration 14/25 | Loss: 0.00142337
Iteration 15/25 | Loss: 0.00141010
Iteration 16/25 | Loss: 0.00140972
Iteration 17/25 | Loss: 0.00142249
Iteration 18/25 | Loss: 0.00141155
Iteration 19/25 | Loss: 0.00140796
Iteration 20/25 | Loss: 0.00140462
Iteration 21/25 | Loss: 0.00140277
Iteration 22/25 | Loss: 0.00140680
Iteration 23/25 | Loss: 0.00140767
Iteration 24/25 | Loss: 0.00140746
Iteration 25/25 | Loss: 0.00140362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30612934
Iteration 2/25 | Loss: 0.00346292
Iteration 3/25 | Loss: 0.00312834
Iteration 4/25 | Loss: 0.00312832
Iteration 5/25 | Loss: 0.00312832
Iteration 6/25 | Loss: 0.00315838
Iteration 7/25 | Loss: 0.00315838
Iteration 8/25 | Loss: 0.00315838
Iteration 9/25 | Loss: 0.00315838
Iteration 10/25 | Loss: 0.00315838
Iteration 11/25 | Loss: 0.00315838
Iteration 12/25 | Loss: 0.00315838
Iteration 13/25 | Loss: 0.00315838
Iteration 14/25 | Loss: 0.00315838
Iteration 15/25 | Loss: 0.00315838
Iteration 16/25 | Loss: 0.00315838
Iteration 17/25 | Loss: 0.00315838
Iteration 18/25 | Loss: 0.00315838
Iteration 19/25 | Loss: 0.00315838
Iteration 20/25 | Loss: 0.00315838
Iteration 21/25 | Loss: 0.00315838
Iteration 22/25 | Loss: 0.00315838
Iteration 23/25 | Loss: 0.00315838
Iteration 24/25 | Loss: 0.00315838
Iteration 25/25 | Loss: 0.00315838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00315838
Iteration 2/1000 | Loss: 0.00144988
Iteration 3/1000 | Loss: 0.00207163
Iteration 4/1000 | Loss: 0.00062729
Iteration 5/1000 | Loss: 0.00036307
Iteration 6/1000 | Loss: 0.00034454
Iteration 7/1000 | Loss: 0.00069716
Iteration 8/1000 | Loss: 0.00033330
Iteration 9/1000 | Loss: 0.00125180
Iteration 10/1000 | Loss: 0.00153666
Iteration 11/1000 | Loss: 0.00041229
Iteration 12/1000 | Loss: 0.00085100
Iteration 13/1000 | Loss: 0.00022519
Iteration 14/1000 | Loss: 0.00019704
Iteration 15/1000 | Loss: 0.00181559
Iteration 16/1000 | Loss: 0.00169969
Iteration 17/1000 | Loss: 0.00059778
Iteration 18/1000 | Loss: 0.00140986
Iteration 19/1000 | Loss: 0.00118518
Iteration 20/1000 | Loss: 0.00108910
Iteration 21/1000 | Loss: 0.00092918
Iteration 22/1000 | Loss: 0.00077517
Iteration 23/1000 | Loss: 0.00038467
Iteration 24/1000 | Loss: 0.00026792
Iteration 25/1000 | Loss: 0.00021198
Iteration 26/1000 | Loss: 0.00016792
Iteration 27/1000 | Loss: 0.00020773
Iteration 28/1000 | Loss: 0.00054443
Iteration 29/1000 | Loss: 0.00040452
Iteration 30/1000 | Loss: 0.00047771
Iteration 31/1000 | Loss: 0.00025041
Iteration 32/1000 | Loss: 0.00025887
Iteration 33/1000 | Loss: 0.00042859
Iteration 34/1000 | Loss: 0.00022622
Iteration 35/1000 | Loss: 0.00016280
Iteration 36/1000 | Loss: 0.00019497
Iteration 37/1000 | Loss: 0.00066624
Iteration 38/1000 | Loss: 0.00131023
Iteration 39/1000 | Loss: 0.00060676
Iteration 40/1000 | Loss: 0.00040249
Iteration 41/1000 | Loss: 0.00043856
Iteration 42/1000 | Loss: 0.00025984
Iteration 43/1000 | Loss: 0.00021258
Iteration 44/1000 | Loss: 0.00031813
Iteration 45/1000 | Loss: 0.00035468
Iteration 46/1000 | Loss: 0.00019158
Iteration 47/1000 | Loss: 0.00024168
Iteration 48/1000 | Loss: 0.00020897
Iteration 49/1000 | Loss: 0.00015004
Iteration 50/1000 | Loss: 0.00020577
Iteration 51/1000 | Loss: 0.00030553
Iteration 52/1000 | Loss: 0.00025008
Iteration 53/1000 | Loss: 0.00022480
Iteration 54/1000 | Loss: 0.00015661
Iteration 55/1000 | Loss: 0.00028358
Iteration 56/1000 | Loss: 0.00024229
Iteration 57/1000 | Loss: 0.00029261
Iteration 58/1000 | Loss: 0.00027927
Iteration 59/1000 | Loss: 0.00027772
Iteration 60/1000 | Loss: 0.00024874
Iteration 61/1000 | Loss: 0.00137085
Iteration 62/1000 | Loss: 0.00055100
Iteration 63/1000 | Loss: 0.00082386
Iteration 64/1000 | Loss: 0.00082243
Iteration 65/1000 | Loss: 0.00056618
Iteration 66/1000 | Loss: 0.00048820
Iteration 67/1000 | Loss: 0.00054661
Iteration 68/1000 | Loss: 0.00039182
Iteration 69/1000 | Loss: 0.00035764
Iteration 70/1000 | Loss: 0.00037409
Iteration 71/1000 | Loss: 0.00089001
Iteration 72/1000 | Loss: 0.00059960
Iteration 73/1000 | Loss: 0.00031856
Iteration 74/1000 | Loss: 0.00043239
Iteration 75/1000 | Loss: 0.00022414
Iteration 76/1000 | Loss: 0.00059961
Iteration 77/1000 | Loss: 0.00039013
Iteration 78/1000 | Loss: 0.00029036
Iteration 79/1000 | Loss: 0.00052500
Iteration 80/1000 | Loss: 0.00028117
Iteration 81/1000 | Loss: 0.00071064
Iteration 82/1000 | Loss: 0.00038582
Iteration 83/1000 | Loss: 0.00045893
Iteration 84/1000 | Loss: 0.00030276
Iteration 85/1000 | Loss: 0.00042902
Iteration 86/1000 | Loss: 0.00024129
Iteration 87/1000 | Loss: 0.00025111
Iteration 88/1000 | Loss: 0.00023746
Iteration 89/1000 | Loss: 0.00024537
Iteration 90/1000 | Loss: 0.00020105
Iteration 91/1000 | Loss: 0.00021308
Iteration 92/1000 | Loss: 0.00020117
Iteration 93/1000 | Loss: 0.00021200
Iteration 94/1000 | Loss: 0.00017166
Iteration 95/1000 | Loss: 0.00086358
Iteration 96/1000 | Loss: 0.00091217
Iteration 97/1000 | Loss: 0.00091722
Iteration 98/1000 | Loss: 0.00074622
Iteration 99/1000 | Loss: 0.00037354
Iteration 100/1000 | Loss: 0.00027658
Iteration 101/1000 | Loss: 0.00025303
Iteration 102/1000 | Loss: 0.00017614
Iteration 103/1000 | Loss: 0.00026438
Iteration 104/1000 | Loss: 0.00044718
Iteration 105/1000 | Loss: 0.00150726
Iteration 106/1000 | Loss: 0.00073747
Iteration 107/1000 | Loss: 0.00049978
Iteration 108/1000 | Loss: 0.00025985
Iteration 109/1000 | Loss: 0.00014368
Iteration 110/1000 | Loss: 0.00029204
Iteration 111/1000 | Loss: 0.00025283
Iteration 112/1000 | Loss: 0.00017223
Iteration 113/1000 | Loss: 0.00041234
Iteration 114/1000 | Loss: 0.00027260
Iteration 115/1000 | Loss: 0.00082942
Iteration 116/1000 | Loss: 0.00053818
Iteration 117/1000 | Loss: 0.00038092
Iteration 118/1000 | Loss: 0.00038514
Iteration 119/1000 | Loss: 0.00025570
Iteration 120/1000 | Loss: 0.00030601
Iteration 121/1000 | Loss: 0.00031526
Iteration 122/1000 | Loss: 0.00030196
Iteration 123/1000 | Loss: 0.00028775
Iteration 124/1000 | Loss: 0.00025920
Iteration 125/1000 | Loss: 0.00023513
Iteration 126/1000 | Loss: 0.00047750
Iteration 127/1000 | Loss: 0.00059979
Iteration 128/1000 | Loss: 0.00039750
Iteration 129/1000 | Loss: 0.00057766
Iteration 130/1000 | Loss: 0.00054116
Iteration 131/1000 | Loss: 0.00041932
Iteration 132/1000 | Loss: 0.00071450
Iteration 133/1000 | Loss: 0.00040425
Iteration 134/1000 | Loss: 0.00037466
Iteration 135/1000 | Loss: 0.00055725
Iteration 136/1000 | Loss: 0.00040995
Iteration 137/1000 | Loss: 0.00025359
Iteration 138/1000 | Loss: 0.00022081
Iteration 139/1000 | Loss: 0.00017132
Iteration 140/1000 | Loss: 0.00114097
Iteration 141/1000 | Loss: 0.00023340
Iteration 142/1000 | Loss: 0.00021255
Iteration 143/1000 | Loss: 0.00020598
Iteration 144/1000 | Loss: 0.00034898
Iteration 145/1000 | Loss: 0.00046053
Iteration 146/1000 | Loss: 0.00031766
Iteration 147/1000 | Loss: 0.00031348
Iteration 148/1000 | Loss: 0.00021949
Iteration 149/1000 | Loss: 0.00045078
Iteration 150/1000 | Loss: 0.00021435
Iteration 151/1000 | Loss: 0.00033873
Iteration 152/1000 | Loss: 0.00042638
Iteration 153/1000 | Loss: 0.00014927
Iteration 154/1000 | Loss: 0.00015081
Iteration 155/1000 | Loss: 0.00015496
Iteration 156/1000 | Loss: 0.00066321
Iteration 157/1000 | Loss: 0.00057405
Iteration 158/1000 | Loss: 0.00024524
Iteration 159/1000 | Loss: 0.00020815
Iteration 160/1000 | Loss: 0.00020276
Iteration 161/1000 | Loss: 0.00061344
Iteration 162/1000 | Loss: 0.00018405
Iteration 163/1000 | Loss: 0.00012992
Iteration 164/1000 | Loss: 0.00018545
Iteration 165/1000 | Loss: 0.00012022
Iteration 166/1000 | Loss: 0.00010343
Iteration 167/1000 | Loss: 0.00014979
Iteration 168/1000 | Loss: 0.00028185
Iteration 169/1000 | Loss: 0.00015569
Iteration 170/1000 | Loss: 0.00022528
Iteration 171/1000 | Loss: 0.00011872
Iteration 172/1000 | Loss: 0.00012932
Iteration 173/1000 | Loss: 0.00049431
Iteration 174/1000 | Loss: 0.00075776
Iteration 175/1000 | Loss: 0.00011286
Iteration 176/1000 | Loss: 0.00021949
Iteration 177/1000 | Loss: 0.00009412
Iteration 178/1000 | Loss: 0.00034301
Iteration 179/1000 | Loss: 0.00040947
Iteration 180/1000 | Loss: 0.00005823
Iteration 181/1000 | Loss: 0.00005091
Iteration 182/1000 | Loss: 0.00088815
Iteration 183/1000 | Loss: 0.00004582
Iteration 184/1000 | Loss: 0.00004113
Iteration 185/1000 | Loss: 0.00045588
Iteration 186/1000 | Loss: 0.00019890
Iteration 187/1000 | Loss: 0.00088930
Iteration 188/1000 | Loss: 0.00048232
Iteration 189/1000 | Loss: 0.00016426
Iteration 190/1000 | Loss: 0.00011972
Iteration 191/1000 | Loss: 0.00015035
Iteration 192/1000 | Loss: 0.00019760
Iteration 193/1000 | Loss: 0.00040047
Iteration 194/1000 | Loss: 0.00004243
Iteration 195/1000 | Loss: 0.00003681
Iteration 196/1000 | Loss: 0.00003369
Iteration 197/1000 | Loss: 0.00003167
Iteration 198/1000 | Loss: 0.00046525
Iteration 199/1000 | Loss: 0.00003217
Iteration 200/1000 | Loss: 0.00005887
Iteration 201/1000 | Loss: 0.00005884
Iteration 202/1000 | Loss: 0.00002805
Iteration 203/1000 | Loss: 0.00002646
Iteration 204/1000 | Loss: 0.00005261
Iteration 205/1000 | Loss: 0.00002526
Iteration 206/1000 | Loss: 0.00002468
Iteration 207/1000 | Loss: 0.00002438
Iteration 208/1000 | Loss: 0.00002409
Iteration 209/1000 | Loss: 0.00002387
Iteration 210/1000 | Loss: 0.00002375
Iteration 211/1000 | Loss: 0.00015314
Iteration 212/1000 | Loss: 0.00044655
Iteration 213/1000 | Loss: 0.00003571
Iteration 214/1000 | Loss: 0.00002559
Iteration 215/1000 | Loss: 0.00002384
Iteration 216/1000 | Loss: 0.00017139
Iteration 217/1000 | Loss: 0.00022888
Iteration 218/1000 | Loss: 0.00014332
Iteration 219/1000 | Loss: 0.00003185
Iteration 220/1000 | Loss: 0.00002438
Iteration 221/1000 | Loss: 0.00006710
Iteration 222/1000 | Loss: 0.00002094
Iteration 223/1000 | Loss: 0.00001967
Iteration 224/1000 | Loss: 0.00001834
Iteration 225/1000 | Loss: 0.00001765
Iteration 226/1000 | Loss: 0.00001725
Iteration 227/1000 | Loss: 0.00001694
Iteration 228/1000 | Loss: 0.00001674
Iteration 229/1000 | Loss: 0.00018616
Iteration 230/1000 | Loss: 0.00002156
Iteration 231/1000 | Loss: 0.00006129
Iteration 232/1000 | Loss: 0.00002048
Iteration 233/1000 | Loss: 0.00001700
Iteration 234/1000 | Loss: 0.00002351
Iteration 235/1000 | Loss: 0.00001568
Iteration 236/1000 | Loss: 0.00001536
Iteration 237/1000 | Loss: 0.00001512
Iteration 238/1000 | Loss: 0.00001508
Iteration 239/1000 | Loss: 0.00001503
Iteration 240/1000 | Loss: 0.00001500
Iteration 241/1000 | Loss: 0.00001499
Iteration 242/1000 | Loss: 0.00001498
Iteration 243/1000 | Loss: 0.00003659
Iteration 244/1000 | Loss: 0.00001618
Iteration 245/1000 | Loss: 0.00001488
Iteration 246/1000 | Loss: 0.00001487
Iteration 247/1000 | Loss: 0.00001486
Iteration 248/1000 | Loss: 0.00001486
Iteration 249/1000 | Loss: 0.00001486
Iteration 250/1000 | Loss: 0.00001486
Iteration 251/1000 | Loss: 0.00001486
Iteration 252/1000 | Loss: 0.00001486
Iteration 253/1000 | Loss: 0.00001486
Iteration 254/1000 | Loss: 0.00001486
Iteration 255/1000 | Loss: 0.00001486
Iteration 256/1000 | Loss: 0.00001486
Iteration 257/1000 | Loss: 0.00001486
Iteration 258/1000 | Loss: 0.00001486
Iteration 259/1000 | Loss: 0.00001485
Iteration 260/1000 | Loss: 0.00001485
Iteration 261/1000 | Loss: 0.00001485
Iteration 262/1000 | Loss: 0.00001485
Iteration 263/1000 | Loss: 0.00001485
Iteration 264/1000 | Loss: 0.00001485
Iteration 265/1000 | Loss: 0.00001485
Iteration 266/1000 | Loss: 0.00001484
Iteration 267/1000 | Loss: 0.00002422
Iteration 268/1000 | Loss: 0.00001478
Iteration 269/1000 | Loss: 0.00001474
Iteration 270/1000 | Loss: 0.00001473
Iteration 271/1000 | Loss: 0.00001473
Iteration 272/1000 | Loss: 0.00001473
Iteration 273/1000 | Loss: 0.00001473
Iteration 274/1000 | Loss: 0.00001473
Iteration 275/1000 | Loss: 0.00001473
Iteration 276/1000 | Loss: 0.00001473
Iteration 277/1000 | Loss: 0.00001473
Iteration 278/1000 | Loss: 0.00001472
Iteration 279/1000 | Loss: 0.00001472
Iteration 280/1000 | Loss: 0.00001472
Iteration 281/1000 | Loss: 0.00001471
Iteration 282/1000 | Loss: 0.00001471
Iteration 283/1000 | Loss: 0.00001471
Iteration 284/1000 | Loss: 0.00001471
Iteration 285/1000 | Loss: 0.00001471
Iteration 286/1000 | Loss: 0.00001471
Iteration 287/1000 | Loss: 0.00001471
Iteration 288/1000 | Loss: 0.00001471
Iteration 289/1000 | Loss: 0.00001471
Iteration 290/1000 | Loss: 0.00001471
Iteration 291/1000 | Loss: 0.00001471
Iteration 292/1000 | Loss: 0.00001470
Iteration 293/1000 | Loss: 0.00001470
Iteration 294/1000 | Loss: 0.00001470
Iteration 295/1000 | Loss: 0.00001470
Iteration 296/1000 | Loss: 0.00001470
Iteration 297/1000 | Loss: 0.00001470
Iteration 298/1000 | Loss: 0.00001469
Iteration 299/1000 | Loss: 0.00001469
Iteration 300/1000 | Loss: 0.00001468
Iteration 301/1000 | Loss: 0.00001467
Iteration 302/1000 | Loss: 0.00001467
Iteration 303/1000 | Loss: 0.00001467
Iteration 304/1000 | Loss: 0.00001467
Iteration 305/1000 | Loss: 0.00001467
Iteration 306/1000 | Loss: 0.00001467
Iteration 307/1000 | Loss: 0.00001467
Iteration 308/1000 | Loss: 0.00001467
Iteration 309/1000 | Loss: 0.00001467
Iteration 310/1000 | Loss: 0.00001467
Iteration 311/1000 | Loss: 0.00001466
Iteration 312/1000 | Loss: 0.00001466
Iteration 313/1000 | Loss: 0.00001466
Iteration 314/1000 | Loss: 0.00001466
Iteration 315/1000 | Loss: 0.00001466
Iteration 316/1000 | Loss: 0.00001466
Iteration 317/1000 | Loss: 0.00001466
Iteration 318/1000 | Loss: 0.00001465
Iteration 319/1000 | Loss: 0.00001465
Iteration 320/1000 | Loss: 0.00001465
Iteration 321/1000 | Loss: 0.00001464
Iteration 322/1000 | Loss: 0.00001464
Iteration 323/1000 | Loss: 0.00001463
Iteration 324/1000 | Loss: 0.00001463
Iteration 325/1000 | Loss: 0.00001463
Iteration 326/1000 | Loss: 0.00001463
Iteration 327/1000 | Loss: 0.00001463
Iteration 328/1000 | Loss: 0.00001463
Iteration 329/1000 | Loss: 0.00001463
Iteration 330/1000 | Loss: 0.00001463
Iteration 331/1000 | Loss: 0.00001463
Iteration 332/1000 | Loss: 0.00001462
Iteration 333/1000 | Loss: 0.00001462
Iteration 334/1000 | Loss: 0.00001462
Iteration 335/1000 | Loss: 0.00001462
Iteration 336/1000 | Loss: 0.00001461
Iteration 337/1000 | Loss: 0.00001461
Iteration 338/1000 | Loss: 0.00001461
Iteration 339/1000 | Loss: 0.00001460
Iteration 340/1000 | Loss: 0.00001460
Iteration 341/1000 | Loss: 0.00001460
Iteration 342/1000 | Loss: 0.00001460
Iteration 343/1000 | Loss: 0.00001460
Iteration 344/1000 | Loss: 0.00001460
Iteration 345/1000 | Loss: 0.00001460
Iteration 346/1000 | Loss: 0.00001460
Iteration 347/1000 | Loss: 0.00001459
Iteration 348/1000 | Loss: 0.00001459
Iteration 349/1000 | Loss: 0.00001459
Iteration 350/1000 | Loss: 0.00001459
Iteration 351/1000 | Loss: 0.00001459
Iteration 352/1000 | Loss: 0.00001459
Iteration 353/1000 | Loss: 0.00001459
Iteration 354/1000 | Loss: 0.00001459
Iteration 355/1000 | Loss: 0.00001459
Iteration 356/1000 | Loss: 0.00001459
Iteration 357/1000 | Loss: 0.00001459
Iteration 358/1000 | Loss: 0.00001459
Iteration 359/1000 | Loss: 0.00001459
Iteration 360/1000 | Loss: 0.00001459
Iteration 361/1000 | Loss: 0.00001459
Iteration 362/1000 | Loss: 0.00001459
Iteration 363/1000 | Loss: 0.00001459
Iteration 364/1000 | Loss: 0.00001459
Iteration 365/1000 | Loss: 0.00001459
Iteration 366/1000 | Loss: 0.00001459
Iteration 367/1000 | Loss: 0.00001459
Iteration 368/1000 | Loss: 0.00001459
Iteration 369/1000 | Loss: 0.00001459
Iteration 370/1000 | Loss: 0.00001459
Iteration 371/1000 | Loss: 0.00001459
Iteration 372/1000 | Loss: 0.00001459
Iteration 373/1000 | Loss: 0.00001459
Iteration 374/1000 | Loss: 0.00001459
Iteration 375/1000 | Loss: 0.00001459
Iteration 376/1000 | Loss: 0.00001459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 376. Stopping optimization.
Last 5 losses: [1.4588026715500746e-05, 1.4588026715500746e-05, 1.4588026715500746e-05, 1.4588026715500746e-05, 1.4588026715500746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4588026715500746e-05

Optimization complete. Final v2v error: 2.89166259765625 mm

Highest mean error: 11.745462417602539 mm for frame 222

Lowest mean error: 2.318204402923584 mm for frame 130

Saving results

Total time: 450.1819062232971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485651
Iteration 2/25 | Loss: 0.00129072
Iteration 3/25 | Loss: 0.00120079
Iteration 4/25 | Loss: 0.00118628
Iteration 5/25 | Loss: 0.00118164
Iteration 6/25 | Loss: 0.00118082
Iteration 7/25 | Loss: 0.00118082
Iteration 8/25 | Loss: 0.00118082
Iteration 9/25 | Loss: 0.00118082
Iteration 10/25 | Loss: 0.00118082
Iteration 11/25 | Loss: 0.00118082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011808203998953104, 0.0011808203998953104, 0.0011808203998953104, 0.0011808203998953104, 0.0011808203998953104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011808203998953104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48173201
Iteration 2/25 | Loss: 0.00132640
Iteration 3/25 | Loss: 0.00132639
Iteration 4/25 | Loss: 0.00132639
Iteration 5/25 | Loss: 0.00132639
Iteration 6/25 | Loss: 0.00132639
Iteration 7/25 | Loss: 0.00132639
Iteration 8/25 | Loss: 0.00132639
Iteration 9/25 | Loss: 0.00132639
Iteration 10/25 | Loss: 0.00132639
Iteration 11/25 | Loss: 0.00132639
Iteration 12/25 | Loss: 0.00132639
Iteration 13/25 | Loss: 0.00132639
Iteration 14/25 | Loss: 0.00132639
Iteration 15/25 | Loss: 0.00132639
Iteration 16/25 | Loss: 0.00132639
Iteration 17/25 | Loss: 0.00132639
Iteration 18/25 | Loss: 0.00132639
Iteration 19/25 | Loss: 0.00132639
Iteration 20/25 | Loss: 0.00132639
Iteration 21/25 | Loss: 0.00132639
Iteration 22/25 | Loss: 0.00132639
Iteration 23/25 | Loss: 0.00132639
Iteration 24/25 | Loss: 0.00132639
Iteration 25/25 | Loss: 0.00132639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132639
Iteration 2/1000 | Loss: 0.00003297
Iteration 3/1000 | Loss: 0.00001986
Iteration 4/1000 | Loss: 0.00001740
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00001474
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001402
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001386
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001355
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001341
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001340
Iteration 35/1000 | Loss: 0.00001337
Iteration 36/1000 | Loss: 0.00001337
Iteration 37/1000 | Loss: 0.00001336
Iteration 38/1000 | Loss: 0.00001336
Iteration 39/1000 | Loss: 0.00001334
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001331
Iteration 44/1000 | Loss: 0.00001331
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001326
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001325
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001324
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001323
Iteration 55/1000 | Loss: 0.00001323
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001322
Iteration 60/1000 | Loss: 0.00001322
Iteration 61/1000 | Loss: 0.00001322
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001322
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001320
Iteration 68/1000 | Loss: 0.00001320
Iteration 69/1000 | Loss: 0.00001320
Iteration 70/1000 | Loss: 0.00001320
Iteration 71/1000 | Loss: 0.00001320
Iteration 72/1000 | Loss: 0.00001319
Iteration 73/1000 | Loss: 0.00001319
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001318
Iteration 77/1000 | Loss: 0.00001317
Iteration 78/1000 | Loss: 0.00001317
Iteration 79/1000 | Loss: 0.00001317
Iteration 80/1000 | Loss: 0.00001317
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001317
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001315
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001313
Iteration 92/1000 | Loss: 0.00001313
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001309
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001306
Iteration 115/1000 | Loss: 0.00001306
Iteration 116/1000 | Loss: 0.00001306
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001306
Iteration 122/1000 | Loss: 0.00001306
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001305
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001305
Iteration 135/1000 | Loss: 0.00001305
Iteration 136/1000 | Loss: 0.00001305
Iteration 137/1000 | Loss: 0.00001305
Iteration 138/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.3052093891019467e-05, 1.3052093891019467e-05, 1.3052093891019467e-05, 1.3052093891019467e-05, 1.3052093891019467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3052093891019467e-05

Optimization complete. Final v2v error: 3.1140623092651367 mm

Highest mean error: 3.868037700653076 mm for frame 227

Lowest mean error: 2.7990524768829346 mm for frame 191

Saving results

Total time: 46.38325071334839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546675
Iteration 2/25 | Loss: 0.00121452
Iteration 3/25 | Loss: 0.00115009
Iteration 4/25 | Loss: 0.00114006
Iteration 5/25 | Loss: 0.00113637
Iteration 6/25 | Loss: 0.00113565
Iteration 7/25 | Loss: 0.00113565
Iteration 8/25 | Loss: 0.00113565
Iteration 9/25 | Loss: 0.00113565
Iteration 10/25 | Loss: 0.00113565
Iteration 11/25 | Loss: 0.00113565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011356525355949998, 0.0011356525355949998, 0.0011356525355949998, 0.0011356525355949998, 0.0011356525355949998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011356525355949998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76889610
Iteration 2/25 | Loss: 0.00120514
Iteration 3/25 | Loss: 0.00120513
Iteration 4/25 | Loss: 0.00120513
Iteration 5/25 | Loss: 0.00120513
Iteration 6/25 | Loss: 0.00120513
Iteration 7/25 | Loss: 0.00120513
Iteration 8/25 | Loss: 0.00120513
Iteration 9/25 | Loss: 0.00120513
Iteration 10/25 | Loss: 0.00120513
Iteration 11/25 | Loss: 0.00120513
Iteration 12/25 | Loss: 0.00120513
Iteration 13/25 | Loss: 0.00120513
Iteration 14/25 | Loss: 0.00120513
Iteration 15/25 | Loss: 0.00120513
Iteration 16/25 | Loss: 0.00120513
Iteration 17/25 | Loss: 0.00120513
Iteration 18/25 | Loss: 0.00120513
Iteration 19/25 | Loss: 0.00120513
Iteration 20/25 | Loss: 0.00120513
Iteration 21/25 | Loss: 0.00120513
Iteration 22/25 | Loss: 0.00120513
Iteration 23/25 | Loss: 0.00120513
Iteration 24/25 | Loss: 0.00120513
Iteration 25/25 | Loss: 0.00120513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120513
Iteration 2/1000 | Loss: 0.00002265
Iteration 3/1000 | Loss: 0.00001645
Iteration 4/1000 | Loss: 0.00001456
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001265
Iteration 8/1000 | Loss: 0.00001207
Iteration 9/1000 | Loss: 0.00001179
Iteration 10/1000 | Loss: 0.00001149
Iteration 11/1000 | Loss: 0.00001120
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001097
Iteration 14/1000 | Loss: 0.00001096
Iteration 15/1000 | Loss: 0.00001087
Iteration 16/1000 | Loss: 0.00001077
Iteration 17/1000 | Loss: 0.00001074
Iteration 18/1000 | Loss: 0.00001073
Iteration 19/1000 | Loss: 0.00001072
Iteration 20/1000 | Loss: 0.00001068
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001064
Iteration 23/1000 | Loss: 0.00001059
Iteration 24/1000 | Loss: 0.00001055
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001047
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001041
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001040
Iteration 36/1000 | Loss: 0.00001038
Iteration 37/1000 | Loss: 0.00001038
Iteration 38/1000 | Loss: 0.00001037
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001037
Iteration 41/1000 | Loss: 0.00001037
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001037
Iteration 44/1000 | Loss: 0.00001037
Iteration 45/1000 | Loss: 0.00001037
Iteration 46/1000 | Loss: 0.00001037
Iteration 47/1000 | Loss: 0.00001037
Iteration 48/1000 | Loss: 0.00001037
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001034
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001033
Iteration 54/1000 | Loss: 0.00001033
Iteration 55/1000 | Loss: 0.00001033
Iteration 56/1000 | Loss: 0.00001033
Iteration 57/1000 | Loss: 0.00001032
Iteration 58/1000 | Loss: 0.00001032
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001031
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001031
Iteration 66/1000 | Loss: 0.00001030
Iteration 67/1000 | Loss: 0.00001030
Iteration 68/1000 | Loss: 0.00001030
Iteration 69/1000 | Loss: 0.00001030
Iteration 70/1000 | Loss: 0.00001030
Iteration 71/1000 | Loss: 0.00001029
Iteration 72/1000 | Loss: 0.00001029
Iteration 73/1000 | Loss: 0.00001029
Iteration 74/1000 | Loss: 0.00001029
Iteration 75/1000 | Loss: 0.00001029
Iteration 76/1000 | Loss: 0.00001028
Iteration 77/1000 | Loss: 0.00001028
Iteration 78/1000 | Loss: 0.00001028
Iteration 79/1000 | Loss: 0.00001028
Iteration 80/1000 | Loss: 0.00001027
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001027
Iteration 86/1000 | Loss: 0.00001027
Iteration 87/1000 | Loss: 0.00001027
Iteration 88/1000 | Loss: 0.00001027
Iteration 89/1000 | Loss: 0.00001027
Iteration 90/1000 | Loss: 0.00001026
Iteration 91/1000 | Loss: 0.00001026
Iteration 92/1000 | Loss: 0.00001025
Iteration 93/1000 | Loss: 0.00001024
Iteration 94/1000 | Loss: 0.00001024
Iteration 95/1000 | Loss: 0.00001024
Iteration 96/1000 | Loss: 0.00001023
Iteration 97/1000 | Loss: 0.00001023
Iteration 98/1000 | Loss: 0.00001023
Iteration 99/1000 | Loss: 0.00001023
Iteration 100/1000 | Loss: 0.00001023
Iteration 101/1000 | Loss: 0.00001023
Iteration 102/1000 | Loss: 0.00001022
Iteration 103/1000 | Loss: 0.00001022
Iteration 104/1000 | Loss: 0.00001022
Iteration 105/1000 | Loss: 0.00001022
Iteration 106/1000 | Loss: 0.00001022
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001021
Iteration 112/1000 | Loss: 0.00001021
Iteration 113/1000 | Loss: 0.00001021
Iteration 114/1000 | Loss: 0.00001021
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001021
Iteration 118/1000 | Loss: 0.00001021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.0207147170149256e-05, 1.0207147170149256e-05, 1.0207147170149256e-05, 1.0207147170149256e-05, 1.0207147170149256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0207147170149256e-05

Optimization complete. Final v2v error: 2.7813048362731934 mm

Highest mean error: 3.152900218963623 mm for frame 137

Lowest mean error: 2.5874228477478027 mm for frame 157

Saving results

Total time: 38.959001541137695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527332
Iteration 2/25 | Loss: 0.00145878
Iteration 3/25 | Loss: 0.00127219
Iteration 4/25 | Loss: 0.00125388
Iteration 5/25 | Loss: 0.00125118
Iteration 6/25 | Loss: 0.00125082
Iteration 7/25 | Loss: 0.00125082
Iteration 8/25 | Loss: 0.00125082
Iteration 9/25 | Loss: 0.00125082
Iteration 10/25 | Loss: 0.00125082
Iteration 11/25 | Loss: 0.00125082
Iteration 12/25 | Loss: 0.00125082
Iteration 13/25 | Loss: 0.00125082
Iteration 14/25 | Loss: 0.00125082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001250819070264697, 0.001250819070264697, 0.001250819070264697, 0.001250819070264697, 0.001250819070264697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001250819070264697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41404021
Iteration 2/25 | Loss: 0.00092284
Iteration 3/25 | Loss: 0.00092283
Iteration 4/25 | Loss: 0.00092283
Iteration 5/25 | Loss: 0.00092283
Iteration 6/25 | Loss: 0.00092283
Iteration 7/25 | Loss: 0.00092283
Iteration 8/25 | Loss: 0.00092283
Iteration 9/25 | Loss: 0.00092283
Iteration 10/25 | Loss: 0.00092283
Iteration 11/25 | Loss: 0.00092283
Iteration 12/25 | Loss: 0.00092283
Iteration 13/25 | Loss: 0.00092283
Iteration 14/25 | Loss: 0.00092283
Iteration 15/25 | Loss: 0.00092283
Iteration 16/25 | Loss: 0.00092283
Iteration 17/25 | Loss: 0.00092283
Iteration 18/25 | Loss: 0.00092283
Iteration 19/25 | Loss: 0.00092283
Iteration 20/25 | Loss: 0.00092283
Iteration 21/25 | Loss: 0.00092283
Iteration 22/25 | Loss: 0.00092283
Iteration 23/25 | Loss: 0.00092283
Iteration 24/25 | Loss: 0.00092283
Iteration 25/25 | Loss: 0.00092283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092283
Iteration 2/1000 | Loss: 0.00004496
Iteration 3/1000 | Loss: 0.00003362
Iteration 4/1000 | Loss: 0.00002823
Iteration 5/1000 | Loss: 0.00002581
Iteration 6/1000 | Loss: 0.00002472
Iteration 7/1000 | Loss: 0.00002391
Iteration 8/1000 | Loss: 0.00002338
Iteration 9/1000 | Loss: 0.00002301
Iteration 10/1000 | Loss: 0.00002250
Iteration 11/1000 | Loss: 0.00002210
Iteration 12/1000 | Loss: 0.00002181
Iteration 13/1000 | Loss: 0.00002162
Iteration 14/1000 | Loss: 0.00002160
Iteration 15/1000 | Loss: 0.00002144
Iteration 16/1000 | Loss: 0.00002126
Iteration 17/1000 | Loss: 0.00002116
Iteration 18/1000 | Loss: 0.00002113
Iteration 19/1000 | Loss: 0.00002108
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00002107
Iteration 22/1000 | Loss: 0.00002106
Iteration 23/1000 | Loss: 0.00002105
Iteration 24/1000 | Loss: 0.00002105
Iteration 25/1000 | Loss: 0.00002105
Iteration 26/1000 | Loss: 0.00002104
Iteration 27/1000 | Loss: 0.00002104
Iteration 28/1000 | Loss: 0.00002104
Iteration 29/1000 | Loss: 0.00002103
Iteration 30/1000 | Loss: 0.00002102
Iteration 31/1000 | Loss: 0.00002102
Iteration 32/1000 | Loss: 0.00002101
Iteration 33/1000 | Loss: 0.00002101
Iteration 34/1000 | Loss: 0.00002100
Iteration 35/1000 | Loss: 0.00002100
Iteration 36/1000 | Loss: 0.00002099
Iteration 37/1000 | Loss: 0.00002099
Iteration 38/1000 | Loss: 0.00002098
Iteration 39/1000 | Loss: 0.00002098
Iteration 40/1000 | Loss: 0.00002098
Iteration 41/1000 | Loss: 0.00002098
Iteration 42/1000 | Loss: 0.00002097
Iteration 43/1000 | Loss: 0.00002097
Iteration 44/1000 | Loss: 0.00002096
Iteration 45/1000 | Loss: 0.00002096
Iteration 46/1000 | Loss: 0.00002096
Iteration 47/1000 | Loss: 0.00002095
Iteration 48/1000 | Loss: 0.00002095
Iteration 49/1000 | Loss: 0.00002095
Iteration 50/1000 | Loss: 0.00002095
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00002095
Iteration 53/1000 | Loss: 0.00002094
Iteration 54/1000 | Loss: 0.00002094
Iteration 55/1000 | Loss: 0.00002094
Iteration 56/1000 | Loss: 0.00002094
Iteration 57/1000 | Loss: 0.00002094
Iteration 58/1000 | Loss: 0.00002094
Iteration 59/1000 | Loss: 0.00002094
Iteration 60/1000 | Loss: 0.00002094
Iteration 61/1000 | Loss: 0.00002094
Iteration 62/1000 | Loss: 0.00002094
Iteration 63/1000 | Loss: 0.00002093
Iteration 64/1000 | Loss: 0.00002093
Iteration 65/1000 | Loss: 0.00002093
Iteration 66/1000 | Loss: 0.00002092
Iteration 67/1000 | Loss: 0.00002092
Iteration 68/1000 | Loss: 0.00002091
Iteration 69/1000 | Loss: 0.00002091
Iteration 70/1000 | Loss: 0.00002091
Iteration 71/1000 | Loss: 0.00002091
Iteration 72/1000 | Loss: 0.00002090
Iteration 73/1000 | Loss: 0.00002090
Iteration 74/1000 | Loss: 0.00002090
Iteration 75/1000 | Loss: 0.00002090
Iteration 76/1000 | Loss: 0.00002090
Iteration 77/1000 | Loss: 0.00002089
Iteration 78/1000 | Loss: 0.00002089
Iteration 79/1000 | Loss: 0.00002089
Iteration 80/1000 | Loss: 0.00002089
Iteration 81/1000 | Loss: 0.00002089
Iteration 82/1000 | Loss: 0.00002089
Iteration 83/1000 | Loss: 0.00002089
Iteration 84/1000 | Loss: 0.00002089
Iteration 85/1000 | Loss: 0.00002089
Iteration 86/1000 | Loss: 0.00002089
Iteration 87/1000 | Loss: 0.00002088
Iteration 88/1000 | Loss: 0.00002088
Iteration 89/1000 | Loss: 0.00002088
Iteration 90/1000 | Loss: 0.00002088
Iteration 91/1000 | Loss: 0.00002088
Iteration 92/1000 | Loss: 0.00002088
Iteration 93/1000 | Loss: 0.00002088
Iteration 94/1000 | Loss: 0.00002088
Iteration 95/1000 | Loss: 0.00002087
Iteration 96/1000 | Loss: 0.00002087
Iteration 97/1000 | Loss: 0.00002087
Iteration 98/1000 | Loss: 0.00002086
Iteration 99/1000 | Loss: 0.00002086
Iteration 100/1000 | Loss: 0.00002086
Iteration 101/1000 | Loss: 0.00002086
Iteration 102/1000 | Loss: 0.00002085
Iteration 103/1000 | Loss: 0.00002085
Iteration 104/1000 | Loss: 0.00002085
Iteration 105/1000 | Loss: 0.00002085
Iteration 106/1000 | Loss: 0.00002085
Iteration 107/1000 | Loss: 0.00002084
Iteration 108/1000 | Loss: 0.00002084
Iteration 109/1000 | Loss: 0.00002084
Iteration 110/1000 | Loss: 0.00002084
Iteration 111/1000 | Loss: 0.00002083
Iteration 112/1000 | Loss: 0.00002083
Iteration 113/1000 | Loss: 0.00002083
Iteration 114/1000 | Loss: 0.00002083
Iteration 115/1000 | Loss: 0.00002082
Iteration 116/1000 | Loss: 0.00002082
Iteration 117/1000 | Loss: 0.00002082
Iteration 118/1000 | Loss: 0.00002081
Iteration 119/1000 | Loss: 0.00002081
Iteration 120/1000 | Loss: 0.00002081
Iteration 121/1000 | Loss: 0.00002081
Iteration 122/1000 | Loss: 0.00002081
Iteration 123/1000 | Loss: 0.00002081
Iteration 124/1000 | Loss: 0.00002081
Iteration 125/1000 | Loss: 0.00002081
Iteration 126/1000 | Loss: 0.00002081
Iteration 127/1000 | Loss: 0.00002081
Iteration 128/1000 | Loss: 0.00002081
Iteration 129/1000 | Loss: 0.00002080
Iteration 130/1000 | Loss: 0.00002080
Iteration 131/1000 | Loss: 0.00002080
Iteration 132/1000 | Loss: 0.00002080
Iteration 133/1000 | Loss: 0.00002080
Iteration 134/1000 | Loss: 0.00002080
Iteration 135/1000 | Loss: 0.00002079
Iteration 136/1000 | Loss: 0.00002079
Iteration 137/1000 | Loss: 0.00002079
Iteration 138/1000 | Loss: 0.00002078
Iteration 139/1000 | Loss: 0.00002078
Iteration 140/1000 | Loss: 0.00002078
Iteration 141/1000 | Loss: 0.00002078
Iteration 142/1000 | Loss: 0.00002078
Iteration 143/1000 | Loss: 0.00002078
Iteration 144/1000 | Loss: 0.00002078
Iteration 145/1000 | Loss: 0.00002078
Iteration 146/1000 | Loss: 0.00002078
Iteration 147/1000 | Loss: 0.00002078
Iteration 148/1000 | Loss: 0.00002078
Iteration 149/1000 | Loss: 0.00002078
Iteration 150/1000 | Loss: 0.00002078
Iteration 151/1000 | Loss: 0.00002078
Iteration 152/1000 | Loss: 0.00002077
Iteration 153/1000 | Loss: 0.00002077
Iteration 154/1000 | Loss: 0.00002077
Iteration 155/1000 | Loss: 0.00002077
Iteration 156/1000 | Loss: 0.00002076
Iteration 157/1000 | Loss: 0.00002076
Iteration 158/1000 | Loss: 0.00002076
Iteration 159/1000 | Loss: 0.00002076
Iteration 160/1000 | Loss: 0.00002075
Iteration 161/1000 | Loss: 0.00002075
Iteration 162/1000 | Loss: 0.00002075
Iteration 163/1000 | Loss: 0.00002075
Iteration 164/1000 | Loss: 0.00002075
Iteration 165/1000 | Loss: 0.00002075
Iteration 166/1000 | Loss: 0.00002075
Iteration 167/1000 | Loss: 0.00002075
Iteration 168/1000 | Loss: 0.00002075
Iteration 169/1000 | Loss: 0.00002075
Iteration 170/1000 | Loss: 0.00002074
Iteration 171/1000 | Loss: 0.00002074
Iteration 172/1000 | Loss: 0.00002074
Iteration 173/1000 | Loss: 0.00002074
Iteration 174/1000 | Loss: 0.00002074
Iteration 175/1000 | Loss: 0.00002074
Iteration 176/1000 | Loss: 0.00002074
Iteration 177/1000 | Loss: 0.00002074
Iteration 178/1000 | Loss: 0.00002074
Iteration 179/1000 | Loss: 0.00002073
Iteration 180/1000 | Loss: 0.00002073
Iteration 181/1000 | Loss: 0.00002073
Iteration 182/1000 | Loss: 0.00002073
Iteration 183/1000 | Loss: 0.00002073
Iteration 184/1000 | Loss: 0.00002073
Iteration 185/1000 | Loss: 0.00002073
Iteration 186/1000 | Loss: 0.00002073
Iteration 187/1000 | Loss: 0.00002073
Iteration 188/1000 | Loss: 0.00002073
Iteration 189/1000 | Loss: 0.00002073
Iteration 190/1000 | Loss: 0.00002073
Iteration 191/1000 | Loss: 0.00002073
Iteration 192/1000 | Loss: 0.00002072
Iteration 193/1000 | Loss: 0.00002072
Iteration 194/1000 | Loss: 0.00002072
Iteration 195/1000 | Loss: 0.00002072
Iteration 196/1000 | Loss: 0.00002072
Iteration 197/1000 | Loss: 0.00002072
Iteration 198/1000 | Loss: 0.00002072
Iteration 199/1000 | Loss: 0.00002072
Iteration 200/1000 | Loss: 0.00002072
Iteration 201/1000 | Loss: 0.00002072
Iteration 202/1000 | Loss: 0.00002072
Iteration 203/1000 | Loss: 0.00002072
Iteration 204/1000 | Loss: 0.00002072
Iteration 205/1000 | Loss: 0.00002071
Iteration 206/1000 | Loss: 0.00002071
Iteration 207/1000 | Loss: 0.00002071
Iteration 208/1000 | Loss: 0.00002071
Iteration 209/1000 | Loss: 0.00002071
Iteration 210/1000 | Loss: 0.00002071
Iteration 211/1000 | Loss: 0.00002071
Iteration 212/1000 | Loss: 0.00002071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.071479320875369e-05, 2.071479320875369e-05, 2.071479320875369e-05, 2.071479320875369e-05, 2.071479320875369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.071479320875369e-05

Optimization complete. Final v2v error: 3.8122823238372803 mm

Highest mean error: 4.292634963989258 mm for frame 78

Lowest mean error: 3.4834694862365723 mm for frame 13

Saving results

Total time: 42.76226305961609
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551287
Iteration 2/25 | Loss: 0.00140265
Iteration 3/25 | Loss: 0.00122521
Iteration 4/25 | Loss: 0.00117176
Iteration 5/25 | Loss: 0.00116832
Iteration 6/25 | Loss: 0.00116396
Iteration 7/25 | Loss: 0.00116251
Iteration 8/25 | Loss: 0.00116019
Iteration 9/25 | Loss: 0.00115740
Iteration 10/25 | Loss: 0.00115635
Iteration 11/25 | Loss: 0.00115603
Iteration 12/25 | Loss: 0.00115590
Iteration 13/25 | Loss: 0.00115588
Iteration 14/25 | Loss: 0.00115588
Iteration 15/25 | Loss: 0.00115588
Iteration 16/25 | Loss: 0.00115588
Iteration 17/25 | Loss: 0.00115588
Iteration 18/25 | Loss: 0.00115588
Iteration 19/25 | Loss: 0.00115588
Iteration 20/25 | Loss: 0.00115588
Iteration 21/25 | Loss: 0.00115588
Iteration 22/25 | Loss: 0.00115588
Iteration 23/25 | Loss: 0.00115588
Iteration 24/25 | Loss: 0.00115587
Iteration 25/25 | Loss: 0.00115587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56752598
Iteration 2/25 | Loss: 0.00124496
Iteration 3/25 | Loss: 0.00124496
Iteration 4/25 | Loss: 0.00124496
Iteration 5/25 | Loss: 0.00124496
Iteration 6/25 | Loss: 0.00124496
Iteration 7/25 | Loss: 0.00124496
Iteration 8/25 | Loss: 0.00124496
Iteration 9/25 | Loss: 0.00124496
Iteration 10/25 | Loss: 0.00124496
Iteration 11/25 | Loss: 0.00124496
Iteration 12/25 | Loss: 0.00124496
Iteration 13/25 | Loss: 0.00124496
Iteration 14/25 | Loss: 0.00124496
Iteration 15/25 | Loss: 0.00124496
Iteration 16/25 | Loss: 0.00124496
Iteration 17/25 | Loss: 0.00124496
Iteration 18/25 | Loss: 0.00124496
Iteration 19/25 | Loss: 0.00124496
Iteration 20/25 | Loss: 0.00124496
Iteration 21/25 | Loss: 0.00124496
Iteration 22/25 | Loss: 0.00124496
Iteration 23/25 | Loss: 0.00124496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012449586065486073, 0.0012449586065486073, 0.0012449586065486073, 0.0012449586065486073, 0.0012449586065486073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012449586065486073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124496
Iteration 2/1000 | Loss: 0.00002133
Iteration 3/1000 | Loss: 0.00001743
Iteration 4/1000 | Loss: 0.00001622
Iteration 5/1000 | Loss: 0.00002130
Iteration 6/1000 | Loss: 0.00001727
Iteration 7/1000 | Loss: 0.00001474
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00002904
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00002148
Iteration 13/1000 | Loss: 0.00002642
Iteration 14/1000 | Loss: 0.00001474
Iteration 15/1000 | Loss: 0.00001362
Iteration 16/1000 | Loss: 0.00001310
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001308
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001300
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001282
Iteration 23/1000 | Loss: 0.00001281
Iteration 24/1000 | Loss: 0.00001281
Iteration 25/1000 | Loss: 0.00001281
Iteration 26/1000 | Loss: 0.00001281
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001281
Iteration 29/1000 | Loss: 0.00001275
Iteration 30/1000 | Loss: 0.00001275
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001991
Iteration 33/1000 | Loss: 0.00001435
Iteration 34/1000 | Loss: 0.00001638
Iteration 35/1000 | Loss: 0.00002031
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001618
Iteration 38/1000 | Loss: 0.00001267
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001267
Iteration 43/1000 | Loss: 0.00001267
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001266
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001265
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001264
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001264
Iteration 68/1000 | Loss: 0.00001264
Iteration 69/1000 | Loss: 0.00001264
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001263
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001263
Iteration 76/1000 | Loss: 0.00001262
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001261
Iteration 79/1000 | Loss: 0.00001261
Iteration 80/1000 | Loss: 0.00001260
Iteration 81/1000 | Loss: 0.00001260
Iteration 82/1000 | Loss: 0.00001260
Iteration 83/1000 | Loss: 0.00001260
Iteration 84/1000 | Loss: 0.00001259
Iteration 85/1000 | Loss: 0.00001259
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001256
Iteration 92/1000 | Loss: 0.00001256
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001249
Iteration 106/1000 | Loss: 0.00001249
Iteration 107/1000 | Loss: 0.00001249
Iteration 108/1000 | Loss: 0.00001249
Iteration 109/1000 | Loss: 0.00001249
Iteration 110/1000 | Loss: 0.00001249
Iteration 111/1000 | Loss: 0.00001249
Iteration 112/1000 | Loss: 0.00001248
Iteration 113/1000 | Loss: 0.00001379
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001247
Iteration 120/1000 | Loss: 0.00001247
Iteration 121/1000 | Loss: 0.00001247
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001246
Iteration 128/1000 | Loss: 0.00001246
Iteration 129/1000 | Loss: 0.00001246
Iteration 130/1000 | Loss: 0.00001246
Iteration 131/1000 | Loss: 0.00001246
Iteration 132/1000 | Loss: 0.00001246
Iteration 133/1000 | Loss: 0.00001246
Iteration 134/1000 | Loss: 0.00001245
Iteration 135/1000 | Loss: 0.00001245
Iteration 136/1000 | Loss: 0.00001245
Iteration 137/1000 | Loss: 0.00001245
Iteration 138/1000 | Loss: 0.00001244
Iteration 139/1000 | Loss: 0.00001244
Iteration 140/1000 | Loss: 0.00001244
Iteration 141/1000 | Loss: 0.00001244
Iteration 142/1000 | Loss: 0.00001244
Iteration 143/1000 | Loss: 0.00001244
Iteration 144/1000 | Loss: 0.00001244
Iteration 145/1000 | Loss: 0.00001244
Iteration 146/1000 | Loss: 0.00001244
Iteration 147/1000 | Loss: 0.00001244
Iteration 148/1000 | Loss: 0.00001244
Iteration 149/1000 | Loss: 0.00001244
Iteration 150/1000 | Loss: 0.00001244
Iteration 151/1000 | Loss: 0.00001244
Iteration 152/1000 | Loss: 0.00001244
Iteration 153/1000 | Loss: 0.00001244
Iteration 154/1000 | Loss: 0.00001244
Iteration 155/1000 | Loss: 0.00001244
Iteration 156/1000 | Loss: 0.00001244
Iteration 157/1000 | Loss: 0.00001244
Iteration 158/1000 | Loss: 0.00001244
Iteration 159/1000 | Loss: 0.00001244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.2443189916666597e-05, 1.2443189916666597e-05, 1.2443189916666597e-05, 1.2443189916666597e-05, 1.2443189916666597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2443189916666597e-05

Optimization complete. Final v2v error: 3.018181085586548 mm

Highest mean error: 3.7587971687316895 mm for frame 181

Lowest mean error: 2.7755000591278076 mm for frame 234

Saving results

Total time: 72.61928081512451
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381549
Iteration 2/25 | Loss: 0.00120338
Iteration 3/25 | Loss: 0.00114114
Iteration 4/25 | Loss: 0.00113162
Iteration 5/25 | Loss: 0.00112784
Iteration 6/25 | Loss: 0.00112658
Iteration 7/25 | Loss: 0.00112658
Iteration 8/25 | Loss: 0.00112658
Iteration 9/25 | Loss: 0.00112658
Iteration 10/25 | Loss: 0.00112658
Iteration 11/25 | Loss: 0.00112658
Iteration 12/25 | Loss: 0.00112658
Iteration 13/25 | Loss: 0.00112658
Iteration 14/25 | Loss: 0.00112658
Iteration 15/25 | Loss: 0.00112658
Iteration 16/25 | Loss: 0.00112658
Iteration 17/25 | Loss: 0.00112658
Iteration 18/25 | Loss: 0.00112658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011265786597505212, 0.0011265786597505212, 0.0011265786597505212, 0.0011265786597505212, 0.0011265786597505212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011265786597505212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.83745313
Iteration 2/25 | Loss: 0.00127288
Iteration 3/25 | Loss: 0.00127288
Iteration 4/25 | Loss: 0.00127288
Iteration 5/25 | Loss: 0.00127288
Iteration 6/25 | Loss: 0.00127288
Iteration 7/25 | Loss: 0.00127288
Iteration 8/25 | Loss: 0.00127288
Iteration 9/25 | Loss: 0.00127288
Iteration 10/25 | Loss: 0.00127288
Iteration 11/25 | Loss: 0.00127288
Iteration 12/25 | Loss: 0.00127288
Iteration 13/25 | Loss: 0.00127288
Iteration 14/25 | Loss: 0.00127288
Iteration 15/25 | Loss: 0.00127288
Iteration 16/25 | Loss: 0.00127288
Iteration 17/25 | Loss: 0.00127288
Iteration 18/25 | Loss: 0.00127288
Iteration 19/25 | Loss: 0.00127288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012728767469525337, 0.0012728767469525337, 0.0012728767469525337, 0.0012728767469525337, 0.0012728767469525337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012728767469525337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127288
Iteration 2/1000 | Loss: 0.00001932
Iteration 3/1000 | Loss: 0.00001305
Iteration 4/1000 | Loss: 0.00001190
Iteration 5/1000 | Loss: 0.00001120
Iteration 6/1000 | Loss: 0.00001070
Iteration 7/1000 | Loss: 0.00001042
Iteration 8/1000 | Loss: 0.00001018
Iteration 9/1000 | Loss: 0.00000989
Iteration 10/1000 | Loss: 0.00000977
Iteration 11/1000 | Loss: 0.00000967
Iteration 12/1000 | Loss: 0.00000960
Iteration 13/1000 | Loss: 0.00000955
Iteration 14/1000 | Loss: 0.00000954
Iteration 15/1000 | Loss: 0.00000950
Iteration 16/1000 | Loss: 0.00000950
Iteration 17/1000 | Loss: 0.00000947
Iteration 18/1000 | Loss: 0.00000947
Iteration 19/1000 | Loss: 0.00000944
Iteration 20/1000 | Loss: 0.00000943
Iteration 21/1000 | Loss: 0.00000940
Iteration 22/1000 | Loss: 0.00000935
Iteration 23/1000 | Loss: 0.00000934
Iteration 24/1000 | Loss: 0.00000934
Iteration 25/1000 | Loss: 0.00000930
Iteration 26/1000 | Loss: 0.00000927
Iteration 27/1000 | Loss: 0.00000926
Iteration 28/1000 | Loss: 0.00000926
Iteration 29/1000 | Loss: 0.00000925
Iteration 30/1000 | Loss: 0.00000924
Iteration 31/1000 | Loss: 0.00000924
Iteration 32/1000 | Loss: 0.00000923
Iteration 33/1000 | Loss: 0.00000922
Iteration 34/1000 | Loss: 0.00000921
Iteration 35/1000 | Loss: 0.00000920
Iteration 36/1000 | Loss: 0.00000920
Iteration 37/1000 | Loss: 0.00000918
Iteration 38/1000 | Loss: 0.00000915
Iteration 39/1000 | Loss: 0.00000915
Iteration 40/1000 | Loss: 0.00000915
Iteration 41/1000 | Loss: 0.00000915
Iteration 42/1000 | Loss: 0.00000915
Iteration 43/1000 | Loss: 0.00000914
Iteration 44/1000 | Loss: 0.00000914
Iteration 45/1000 | Loss: 0.00000914
Iteration 46/1000 | Loss: 0.00000914
Iteration 47/1000 | Loss: 0.00000914
Iteration 48/1000 | Loss: 0.00000914
Iteration 49/1000 | Loss: 0.00000914
Iteration 50/1000 | Loss: 0.00000914
Iteration 51/1000 | Loss: 0.00000912
Iteration 52/1000 | Loss: 0.00000911
Iteration 53/1000 | Loss: 0.00000910
Iteration 54/1000 | Loss: 0.00000910
Iteration 55/1000 | Loss: 0.00000910
Iteration 56/1000 | Loss: 0.00000910
Iteration 57/1000 | Loss: 0.00000909
Iteration 58/1000 | Loss: 0.00000909
Iteration 59/1000 | Loss: 0.00000909
Iteration 60/1000 | Loss: 0.00000908
Iteration 61/1000 | Loss: 0.00000907
Iteration 62/1000 | Loss: 0.00000906
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000905
Iteration 66/1000 | Loss: 0.00000905
Iteration 67/1000 | Loss: 0.00000905
Iteration 68/1000 | Loss: 0.00000905
Iteration 69/1000 | Loss: 0.00000905
Iteration 70/1000 | Loss: 0.00000905
Iteration 71/1000 | Loss: 0.00000905
Iteration 72/1000 | Loss: 0.00000904
Iteration 73/1000 | Loss: 0.00000904
Iteration 74/1000 | Loss: 0.00000904
Iteration 75/1000 | Loss: 0.00000904
Iteration 76/1000 | Loss: 0.00000904
Iteration 77/1000 | Loss: 0.00000903
Iteration 78/1000 | Loss: 0.00000903
Iteration 79/1000 | Loss: 0.00000903
Iteration 80/1000 | Loss: 0.00000903
Iteration 81/1000 | Loss: 0.00000902
Iteration 82/1000 | Loss: 0.00000902
Iteration 83/1000 | Loss: 0.00000902
Iteration 84/1000 | Loss: 0.00000902
Iteration 85/1000 | Loss: 0.00000902
Iteration 86/1000 | Loss: 0.00000901
Iteration 87/1000 | Loss: 0.00000901
Iteration 88/1000 | Loss: 0.00000901
Iteration 89/1000 | Loss: 0.00000901
Iteration 90/1000 | Loss: 0.00000900
Iteration 91/1000 | Loss: 0.00000900
Iteration 92/1000 | Loss: 0.00000900
Iteration 93/1000 | Loss: 0.00000900
Iteration 94/1000 | Loss: 0.00000900
Iteration 95/1000 | Loss: 0.00000899
Iteration 96/1000 | Loss: 0.00000899
Iteration 97/1000 | Loss: 0.00000899
Iteration 98/1000 | Loss: 0.00000899
Iteration 99/1000 | Loss: 0.00000899
Iteration 100/1000 | Loss: 0.00000899
Iteration 101/1000 | Loss: 0.00000899
Iteration 102/1000 | Loss: 0.00000898
Iteration 103/1000 | Loss: 0.00000898
Iteration 104/1000 | Loss: 0.00000898
Iteration 105/1000 | Loss: 0.00000898
Iteration 106/1000 | Loss: 0.00000897
Iteration 107/1000 | Loss: 0.00000897
Iteration 108/1000 | Loss: 0.00000897
Iteration 109/1000 | Loss: 0.00000897
Iteration 110/1000 | Loss: 0.00000897
Iteration 111/1000 | Loss: 0.00000897
Iteration 112/1000 | Loss: 0.00000897
Iteration 113/1000 | Loss: 0.00000897
Iteration 114/1000 | Loss: 0.00000896
Iteration 115/1000 | Loss: 0.00000896
Iteration 116/1000 | Loss: 0.00000896
Iteration 117/1000 | Loss: 0.00000896
Iteration 118/1000 | Loss: 0.00000896
Iteration 119/1000 | Loss: 0.00000896
Iteration 120/1000 | Loss: 0.00000896
Iteration 121/1000 | Loss: 0.00000896
Iteration 122/1000 | Loss: 0.00000896
Iteration 123/1000 | Loss: 0.00000896
Iteration 124/1000 | Loss: 0.00000896
Iteration 125/1000 | Loss: 0.00000895
Iteration 126/1000 | Loss: 0.00000895
Iteration 127/1000 | Loss: 0.00000895
Iteration 128/1000 | Loss: 0.00000895
Iteration 129/1000 | Loss: 0.00000894
Iteration 130/1000 | Loss: 0.00000894
Iteration 131/1000 | Loss: 0.00000894
Iteration 132/1000 | Loss: 0.00000894
Iteration 133/1000 | Loss: 0.00000894
Iteration 134/1000 | Loss: 0.00000894
Iteration 135/1000 | Loss: 0.00000893
Iteration 136/1000 | Loss: 0.00000893
Iteration 137/1000 | Loss: 0.00000893
Iteration 138/1000 | Loss: 0.00000893
Iteration 139/1000 | Loss: 0.00000893
Iteration 140/1000 | Loss: 0.00000893
Iteration 141/1000 | Loss: 0.00000893
Iteration 142/1000 | Loss: 0.00000893
Iteration 143/1000 | Loss: 0.00000893
Iteration 144/1000 | Loss: 0.00000893
Iteration 145/1000 | Loss: 0.00000893
Iteration 146/1000 | Loss: 0.00000893
Iteration 147/1000 | Loss: 0.00000893
Iteration 148/1000 | Loss: 0.00000893
Iteration 149/1000 | Loss: 0.00000893
Iteration 150/1000 | Loss: 0.00000893
Iteration 151/1000 | Loss: 0.00000893
Iteration 152/1000 | Loss: 0.00000892
Iteration 153/1000 | Loss: 0.00000892
Iteration 154/1000 | Loss: 0.00000892
Iteration 155/1000 | Loss: 0.00000892
Iteration 156/1000 | Loss: 0.00000892
Iteration 157/1000 | Loss: 0.00000892
Iteration 158/1000 | Loss: 0.00000892
Iteration 159/1000 | Loss: 0.00000892
Iteration 160/1000 | Loss: 0.00000892
Iteration 161/1000 | Loss: 0.00000892
Iteration 162/1000 | Loss: 0.00000892
Iteration 163/1000 | Loss: 0.00000891
Iteration 164/1000 | Loss: 0.00000891
Iteration 165/1000 | Loss: 0.00000891
Iteration 166/1000 | Loss: 0.00000891
Iteration 167/1000 | Loss: 0.00000891
Iteration 168/1000 | Loss: 0.00000891
Iteration 169/1000 | Loss: 0.00000891
Iteration 170/1000 | Loss: 0.00000891
Iteration 171/1000 | Loss: 0.00000891
Iteration 172/1000 | Loss: 0.00000891
Iteration 173/1000 | Loss: 0.00000891
Iteration 174/1000 | Loss: 0.00000891
Iteration 175/1000 | Loss: 0.00000891
Iteration 176/1000 | Loss: 0.00000891
Iteration 177/1000 | Loss: 0.00000891
Iteration 178/1000 | Loss: 0.00000891
Iteration 179/1000 | Loss: 0.00000891
Iteration 180/1000 | Loss: 0.00000891
Iteration 181/1000 | Loss: 0.00000891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [8.910647011362016e-06, 8.910647011362016e-06, 8.910647011362016e-06, 8.910647011362016e-06, 8.910647011362016e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.910647011362016e-06

Optimization complete. Final v2v error: 2.59761118888855 mm

Highest mean error: 2.7329978942871094 mm for frame 76

Lowest mean error: 2.4971611499786377 mm for frame 35

Saving results

Total time: 40.004698514938354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01144969
Iteration 2/25 | Loss: 0.00355793
Iteration 3/25 | Loss: 0.00220823
Iteration 4/25 | Loss: 0.00212678
Iteration 5/25 | Loss: 0.00196267
Iteration 6/25 | Loss: 0.00200831
Iteration 7/25 | Loss: 0.00217417
Iteration 8/25 | Loss: 0.00214717
Iteration 9/25 | Loss: 0.00195210
Iteration 10/25 | Loss: 0.00181856
Iteration 11/25 | Loss: 0.00171173
Iteration 12/25 | Loss: 0.00169731
Iteration 13/25 | Loss: 0.00168092
Iteration 14/25 | Loss: 0.00165386
Iteration 15/25 | Loss: 0.00164466
Iteration 16/25 | Loss: 0.00164188
Iteration 17/25 | Loss: 0.00164008
Iteration 18/25 | Loss: 0.00163972
Iteration 19/25 | Loss: 0.00163958
Iteration 20/25 | Loss: 0.00163954
Iteration 21/25 | Loss: 0.00163953
Iteration 22/25 | Loss: 0.00163953
Iteration 23/25 | Loss: 0.00163953
Iteration 24/25 | Loss: 0.00163953
Iteration 25/25 | Loss: 0.00163953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.34077772
Iteration 2/25 | Loss: 0.00149408
Iteration 3/25 | Loss: 0.00149408
Iteration 4/25 | Loss: 0.00149408
Iteration 5/25 | Loss: 0.00149407
Iteration 6/25 | Loss: 0.00149407
Iteration 7/25 | Loss: 0.00149407
Iteration 8/25 | Loss: 0.00149407
Iteration 9/25 | Loss: 0.00149407
Iteration 10/25 | Loss: 0.00149407
Iteration 11/25 | Loss: 0.00149407
Iteration 12/25 | Loss: 0.00149407
Iteration 13/25 | Loss: 0.00149407
Iteration 14/25 | Loss: 0.00149407
Iteration 15/25 | Loss: 0.00149407
Iteration 16/25 | Loss: 0.00149407
Iteration 17/25 | Loss: 0.00149407
Iteration 18/25 | Loss: 0.00149407
Iteration 19/25 | Loss: 0.00149407
Iteration 20/25 | Loss: 0.00149407
Iteration 21/25 | Loss: 0.00149407
Iteration 22/25 | Loss: 0.00149407
Iteration 23/25 | Loss: 0.00149407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014940722612664104, 0.0014940722612664104, 0.0014940722612664104, 0.0014940722612664104, 0.0014940722612664104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014940722612664104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149407
Iteration 2/1000 | Loss: 0.00009289
Iteration 3/1000 | Loss: 0.00006917
Iteration 4/1000 | Loss: 0.00006004
Iteration 5/1000 | Loss: 0.00005804
Iteration 6/1000 | Loss: 0.00005681
Iteration 7/1000 | Loss: 0.00005576
Iteration 8/1000 | Loss: 0.00005506
Iteration 9/1000 | Loss: 0.00005421
Iteration 10/1000 | Loss: 0.00005356
Iteration 11/1000 | Loss: 0.00005322
Iteration 12/1000 | Loss: 0.00005274
Iteration 13/1000 | Loss: 0.00005241
Iteration 14/1000 | Loss: 0.00005203
Iteration 15/1000 | Loss: 0.00005184
Iteration 16/1000 | Loss: 0.00005161
Iteration 17/1000 | Loss: 0.00005137
Iteration 18/1000 | Loss: 0.00005116
Iteration 19/1000 | Loss: 0.00005099
Iteration 20/1000 | Loss: 0.00005098
Iteration 21/1000 | Loss: 0.00005083
Iteration 22/1000 | Loss: 0.00005082
Iteration 23/1000 | Loss: 0.00005076
Iteration 24/1000 | Loss: 0.00005075
Iteration 25/1000 | Loss: 0.00005073
Iteration 26/1000 | Loss: 0.00005068
Iteration 27/1000 | Loss: 0.00005068
Iteration 28/1000 | Loss: 0.00005068
Iteration 29/1000 | Loss: 0.00005068
Iteration 30/1000 | Loss: 0.00005067
Iteration 31/1000 | Loss: 0.00005066
Iteration 32/1000 | Loss: 0.00005066
Iteration 33/1000 | Loss: 0.00005065
Iteration 34/1000 | Loss: 0.00005065
Iteration 35/1000 | Loss: 0.00005061
Iteration 36/1000 | Loss: 0.00005060
Iteration 37/1000 | Loss: 0.00005059
Iteration 38/1000 | Loss: 0.00005053
Iteration 39/1000 | Loss: 0.00005053
Iteration 40/1000 | Loss: 0.00005052
Iteration 41/1000 | Loss: 0.00005051
Iteration 42/1000 | Loss: 0.00005051
Iteration 43/1000 | Loss: 0.00005051
Iteration 44/1000 | Loss: 0.00005051
Iteration 45/1000 | Loss: 0.00005051
Iteration 46/1000 | Loss: 0.00005050
Iteration 47/1000 | Loss: 0.00005049
Iteration 48/1000 | Loss: 0.00005049
Iteration 49/1000 | Loss: 0.00005049
Iteration 50/1000 | Loss: 0.00005048
Iteration 51/1000 | Loss: 0.00005045
Iteration 52/1000 | Loss: 0.00005045
Iteration 53/1000 | Loss: 0.00005045
Iteration 54/1000 | Loss: 0.00005045
Iteration 55/1000 | Loss: 0.00005044
Iteration 56/1000 | Loss: 0.00005044
Iteration 57/1000 | Loss: 0.00005044
Iteration 58/1000 | Loss: 0.00005044
Iteration 59/1000 | Loss: 0.00005044
Iteration 60/1000 | Loss: 0.00005044
Iteration 61/1000 | Loss: 0.00005043
Iteration 62/1000 | Loss: 0.00005041
Iteration 63/1000 | Loss: 0.00005040
Iteration 64/1000 | Loss: 0.00005040
Iteration 65/1000 | Loss: 0.00005040
Iteration 66/1000 | Loss: 0.00005039
Iteration 67/1000 | Loss: 0.00005039
Iteration 68/1000 | Loss: 0.00005039
Iteration 69/1000 | Loss: 0.00005039
Iteration 70/1000 | Loss: 0.00005039
Iteration 71/1000 | Loss: 0.00005039
Iteration 72/1000 | Loss: 0.00005039
Iteration 73/1000 | Loss: 0.00005039
Iteration 74/1000 | Loss: 0.00005039
Iteration 75/1000 | Loss: 0.00005039
Iteration 76/1000 | Loss: 0.00005039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [5.0391310651320964e-05, 5.0391310651320964e-05, 5.0391310651320964e-05, 5.0391310651320964e-05, 5.0391310651320964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.0391310651320964e-05

Optimization complete. Final v2v error: 5.795060157775879 mm

Highest mean error: 6.084158420562744 mm for frame 42

Lowest mean error: 5.595520496368408 mm for frame 152

Saving results

Total time: 80.76574039459229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902038
Iteration 2/25 | Loss: 0.00188781
Iteration 3/25 | Loss: 0.00152079
Iteration 4/25 | Loss: 0.00145305
Iteration 5/25 | Loss: 0.00144256
Iteration 6/25 | Loss: 0.00137071
Iteration 7/25 | Loss: 0.00133425
Iteration 8/25 | Loss: 0.00131076
Iteration 9/25 | Loss: 0.00130404
Iteration 10/25 | Loss: 0.00131912
Iteration 11/25 | Loss: 0.00131567
Iteration 12/25 | Loss: 0.00129812
Iteration 13/25 | Loss: 0.00128763
Iteration 14/25 | Loss: 0.00127813
Iteration 15/25 | Loss: 0.00127475
Iteration 16/25 | Loss: 0.00127379
Iteration 17/25 | Loss: 0.00127349
Iteration 18/25 | Loss: 0.00127701
Iteration 19/25 | Loss: 0.00127349
Iteration 20/25 | Loss: 0.00127258
Iteration 21/25 | Loss: 0.00127226
Iteration 22/25 | Loss: 0.00127224
Iteration 23/25 | Loss: 0.00127224
Iteration 24/25 | Loss: 0.00127224
Iteration 25/25 | Loss: 0.00127224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33659005
Iteration 2/25 | Loss: 0.00128403
Iteration 3/25 | Loss: 0.00127906
Iteration 4/25 | Loss: 0.00127906
Iteration 5/25 | Loss: 0.00127906
Iteration 6/25 | Loss: 0.00127906
Iteration 7/25 | Loss: 0.00127906
Iteration 8/25 | Loss: 0.00127906
Iteration 9/25 | Loss: 0.00127906
Iteration 10/25 | Loss: 0.00127906
Iteration 11/25 | Loss: 0.00127905
Iteration 12/25 | Loss: 0.00127905
Iteration 13/25 | Loss: 0.00127905
Iteration 14/25 | Loss: 0.00127905
Iteration 15/25 | Loss: 0.00127905
Iteration 16/25 | Loss: 0.00127905
Iteration 17/25 | Loss: 0.00127905
Iteration 18/25 | Loss: 0.00127905
Iteration 19/25 | Loss: 0.00127905
Iteration 20/25 | Loss: 0.00127905
Iteration 21/25 | Loss: 0.00127905
Iteration 22/25 | Loss: 0.00127905
Iteration 23/25 | Loss: 0.00127905
Iteration 24/25 | Loss: 0.00127905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012790544424206018, 0.0012790544424206018, 0.0012790544424206018, 0.0012790544424206018, 0.0012790544424206018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012790544424206018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127905
Iteration 2/1000 | Loss: 0.00031468
Iteration 3/1000 | Loss: 0.00042683
Iteration 4/1000 | Loss: 0.00038349
Iteration 5/1000 | Loss: 0.00029686
Iteration 6/1000 | Loss: 0.00018715
Iteration 7/1000 | Loss: 0.00016160
Iteration 8/1000 | Loss: 0.00010882
Iteration 9/1000 | Loss: 0.00010266
Iteration 10/1000 | Loss: 0.00008354
Iteration 11/1000 | Loss: 0.00007453
Iteration 12/1000 | Loss: 0.00015732
Iteration 13/1000 | Loss: 0.00013885
Iteration 14/1000 | Loss: 0.00014646
Iteration 15/1000 | Loss: 0.00012678
Iteration 16/1000 | Loss: 0.00012682
Iteration 17/1000 | Loss: 0.00008899
Iteration 18/1000 | Loss: 0.00010572
Iteration 19/1000 | Loss: 0.00007460
Iteration 20/1000 | Loss: 0.00006587
Iteration 21/1000 | Loss: 0.00005955
Iteration 22/1000 | Loss: 0.00008775
Iteration 23/1000 | Loss: 0.00005707
Iteration 24/1000 | Loss: 0.00005285
Iteration 25/1000 | Loss: 0.00004661
Iteration 26/1000 | Loss: 0.00008578
Iteration 27/1000 | Loss: 0.00004914
Iteration 28/1000 | Loss: 0.00004355
Iteration 29/1000 | Loss: 0.00003990
Iteration 30/1000 | Loss: 0.00005034
Iteration 31/1000 | Loss: 0.00004455
Iteration 32/1000 | Loss: 0.00003557
Iteration 33/1000 | Loss: 0.00004461
Iteration 34/1000 | Loss: 0.00006956
Iteration 35/1000 | Loss: 0.00003956
Iteration 36/1000 | Loss: 0.00005903
Iteration 37/1000 | Loss: 0.00003265
Iteration 38/1000 | Loss: 0.00005141
Iteration 39/1000 | Loss: 0.00005402
Iteration 40/1000 | Loss: 0.00004576
Iteration 41/1000 | Loss: 0.00005435
Iteration 42/1000 | Loss: 0.00003493
Iteration 43/1000 | Loss: 0.00005085
Iteration 44/1000 | Loss: 0.00005916
Iteration 45/1000 | Loss: 0.00004629
Iteration 46/1000 | Loss: 0.00003540
Iteration 47/1000 | Loss: 0.00005376
Iteration 48/1000 | Loss: 0.00004607
Iteration 49/1000 | Loss: 0.00002955
Iteration 50/1000 | Loss: 0.00002887
Iteration 51/1000 | Loss: 0.00003245
Iteration 52/1000 | Loss: 0.00003849
Iteration 53/1000 | Loss: 0.00002948
Iteration 54/1000 | Loss: 0.00004123
Iteration 55/1000 | Loss: 0.00003159
Iteration 56/1000 | Loss: 0.00003259
Iteration 57/1000 | Loss: 0.00004907
Iteration 58/1000 | Loss: 0.00004736
Iteration 59/1000 | Loss: 0.00002942
Iteration 60/1000 | Loss: 0.00003388
Iteration 61/1000 | Loss: 0.00003213
Iteration 62/1000 | Loss: 0.00003140
Iteration 63/1000 | Loss: 0.00002976
Iteration 64/1000 | Loss: 0.00003483
Iteration 65/1000 | Loss: 0.00003703
Iteration 66/1000 | Loss: 0.00002884
Iteration 67/1000 | Loss: 0.00003311
Iteration 68/1000 | Loss: 0.00002972
Iteration 69/1000 | Loss: 0.00002843
Iteration 70/1000 | Loss: 0.00002909
Iteration 71/1000 | Loss: 0.00002904
Iteration 72/1000 | Loss: 0.00002812
Iteration 73/1000 | Loss: 0.00009502
Iteration 74/1000 | Loss: 0.00007168
Iteration 75/1000 | Loss: 0.00003948
Iteration 76/1000 | Loss: 0.00003232
Iteration 77/1000 | Loss: 0.00003309
Iteration 78/1000 | Loss: 0.00003097
Iteration 79/1000 | Loss: 0.00002975
Iteration 80/1000 | Loss: 0.00002568
Iteration 81/1000 | Loss: 0.00002567
Iteration 82/1000 | Loss: 0.00002567
Iteration 83/1000 | Loss: 0.00002567
Iteration 84/1000 | Loss: 0.00002567
Iteration 85/1000 | Loss: 0.00002567
Iteration 86/1000 | Loss: 0.00002567
Iteration 87/1000 | Loss: 0.00002567
Iteration 88/1000 | Loss: 0.00002567
Iteration 89/1000 | Loss: 0.00002567
Iteration 90/1000 | Loss: 0.00002567
Iteration 91/1000 | Loss: 0.00002567
Iteration 92/1000 | Loss: 0.00002567
Iteration 93/1000 | Loss: 0.00003073
Iteration 94/1000 | Loss: 0.00003574
Iteration 95/1000 | Loss: 0.00004932
Iteration 96/1000 | Loss: 0.00003483
Iteration 97/1000 | Loss: 0.00003076
Iteration 98/1000 | Loss: 0.00003118
Iteration 99/1000 | Loss: 0.00003519
Iteration 100/1000 | Loss: 0.00004807
Iteration 101/1000 | Loss: 0.00003749
Iteration 102/1000 | Loss: 0.00002589
Iteration 103/1000 | Loss: 0.00002566
Iteration 104/1000 | Loss: 0.00002564
Iteration 105/1000 | Loss: 0.00002556
Iteration 106/1000 | Loss: 0.00003373
Iteration 107/1000 | Loss: 0.00003292
Iteration 108/1000 | Loss: 0.00003728
Iteration 109/1000 | Loss: 0.00002991
Iteration 110/1000 | Loss: 0.00002963
Iteration 111/1000 | Loss: 0.00003036
Iteration 112/1000 | Loss: 0.00014627
Iteration 113/1000 | Loss: 0.00008222
Iteration 114/1000 | Loss: 0.00006531
Iteration 115/1000 | Loss: 0.00005865
Iteration 116/1000 | Loss: 0.00004406
Iteration 117/1000 | Loss: 0.00006793
Iteration 118/1000 | Loss: 0.00004765
Iteration 119/1000 | Loss: 0.00004682
Iteration 120/1000 | Loss: 0.00007673
Iteration 121/1000 | Loss: 0.00012395
Iteration 122/1000 | Loss: 0.00011428
Iteration 123/1000 | Loss: 0.00005724
Iteration 124/1000 | Loss: 0.00004207
Iteration 125/1000 | Loss: 0.00003045
Iteration 126/1000 | Loss: 0.00003007
Iteration 127/1000 | Loss: 0.00002708
Iteration 128/1000 | Loss: 0.00003776
Iteration 129/1000 | Loss: 0.00003685
Iteration 130/1000 | Loss: 0.00004796
Iteration 131/1000 | Loss: 0.00003366
Iteration 132/1000 | Loss: 0.00003574
Iteration 133/1000 | Loss: 0.00002884
Iteration 134/1000 | Loss: 0.00003215
Iteration 135/1000 | Loss: 0.00002839
Iteration 136/1000 | Loss: 0.00003821
Iteration 137/1000 | Loss: 0.00003931
Iteration 138/1000 | Loss: 0.00003688
Iteration 139/1000 | Loss: 0.00003340
Iteration 140/1000 | Loss: 0.00003545
Iteration 141/1000 | Loss: 0.00003218
Iteration 142/1000 | Loss: 0.00004299
Iteration 143/1000 | Loss: 0.00006817
Iteration 144/1000 | Loss: 0.00005952
Iteration 145/1000 | Loss: 0.00004659
Iteration 146/1000 | Loss: 0.00004462
Iteration 147/1000 | Loss: 0.00003649
Iteration 148/1000 | Loss: 0.00006174
Iteration 149/1000 | Loss: 0.00004004
Iteration 150/1000 | Loss: 0.00004184
Iteration 151/1000 | Loss: 0.00009088
Iteration 152/1000 | Loss: 0.00007577
Iteration 153/1000 | Loss: 0.00009938
Iteration 154/1000 | Loss: 0.00006159
Iteration 155/1000 | Loss: 0.00004261
Iteration 156/1000 | Loss: 0.00007266
Iteration 157/1000 | Loss: 0.00004667
Iteration 158/1000 | Loss: 0.00005681
Iteration 159/1000 | Loss: 0.00003142
Iteration 160/1000 | Loss: 0.00004131
Iteration 161/1000 | Loss: 0.00003830
Iteration 162/1000 | Loss: 0.00003915
Iteration 163/1000 | Loss: 0.00003360
Iteration 164/1000 | Loss: 0.00003934
Iteration 165/1000 | Loss: 0.00004008
Iteration 166/1000 | Loss: 0.00003530
Iteration 167/1000 | Loss: 0.00003309
Iteration 168/1000 | Loss: 0.00004133
Iteration 169/1000 | Loss: 0.00003694
Iteration 170/1000 | Loss: 0.00003402
Iteration 171/1000 | Loss: 0.00003929
Iteration 172/1000 | Loss: 0.00003447
Iteration 173/1000 | Loss: 0.00005895
Iteration 174/1000 | Loss: 0.00004044
Iteration 175/1000 | Loss: 0.00005330
Iteration 176/1000 | Loss: 0.00005783
Iteration 177/1000 | Loss: 0.00004691
Iteration 178/1000 | Loss: 0.00005353
Iteration 179/1000 | Loss: 0.00004079
Iteration 180/1000 | Loss: 0.00004146
Iteration 181/1000 | Loss: 0.00003221
Iteration 182/1000 | Loss: 0.00002750
Iteration 183/1000 | Loss: 0.00002988
Iteration 184/1000 | Loss: 0.00003884
Iteration 185/1000 | Loss: 0.00003867
Iteration 186/1000 | Loss: 0.00003481
Iteration 187/1000 | Loss: 0.00003734
Iteration 188/1000 | Loss: 0.00003479
Iteration 189/1000 | Loss: 0.00003691
Iteration 190/1000 | Loss: 0.00003529
Iteration 191/1000 | Loss: 0.00003418
Iteration 192/1000 | Loss: 0.00003728
Iteration 193/1000 | Loss: 0.00003626
Iteration 194/1000 | Loss: 0.00003626
Iteration 195/1000 | Loss: 0.00003733
Iteration 196/1000 | Loss: 0.00003585
Iteration 197/1000 | Loss: 0.00003628
Iteration 198/1000 | Loss: 0.00003545
Iteration 199/1000 | Loss: 0.00003544
Iteration 200/1000 | Loss: 0.00003480
Iteration 201/1000 | Loss: 0.00003550
Iteration 202/1000 | Loss: 0.00002674
Iteration 203/1000 | Loss: 0.00002649
Iteration 204/1000 | Loss: 0.00002642
Iteration 205/1000 | Loss: 0.00002638
Iteration 206/1000 | Loss: 0.00002629
Iteration 207/1000 | Loss: 0.00002621
Iteration 208/1000 | Loss: 0.00002619
Iteration 209/1000 | Loss: 0.00002619
Iteration 210/1000 | Loss: 0.00003600
Iteration 211/1000 | Loss: 0.00005882
Iteration 212/1000 | Loss: 0.00003592
Iteration 213/1000 | Loss: 0.00005015
Iteration 214/1000 | Loss: 0.00005536
Iteration 215/1000 | Loss: 0.00007591
Iteration 216/1000 | Loss: 0.00006690
Iteration 217/1000 | Loss: 0.00009995
Iteration 218/1000 | Loss: 0.00008183
Iteration 219/1000 | Loss: 0.00004803
Iteration 220/1000 | Loss: 0.00004338
Iteration 221/1000 | Loss: 0.00004539
Iteration 222/1000 | Loss: 0.00004146
Iteration 223/1000 | Loss: 0.00004500
Iteration 224/1000 | Loss: 0.00004143
Iteration 225/1000 | Loss: 0.00003035
Iteration 226/1000 | Loss: 0.00002871
Iteration 227/1000 | Loss: 0.00002758
Iteration 228/1000 | Loss: 0.00003790
Iteration 229/1000 | Loss: 0.00002812
Iteration 230/1000 | Loss: 0.00002738
Iteration 231/1000 | Loss: 0.00003084
Iteration 232/1000 | Loss: 0.00003493
Iteration 233/1000 | Loss: 0.00003041
Iteration 234/1000 | Loss: 0.00002719
Iteration 235/1000 | Loss: 0.00003090
Iteration 236/1000 | Loss: 0.00002768
Iteration 237/1000 | Loss: 0.00003276
Iteration 238/1000 | Loss: 0.00003586
Iteration 239/1000 | Loss: 0.00003880
Iteration 240/1000 | Loss: 0.00003402
Iteration 241/1000 | Loss: 0.00003707
Iteration 242/1000 | Loss: 0.00002764
Iteration 243/1000 | Loss: 0.00003103
Iteration 244/1000 | Loss: 0.00003141
Iteration 245/1000 | Loss: 0.00003292
Iteration 246/1000 | Loss: 0.00003688
Iteration 247/1000 | Loss: 0.00003292
Iteration 248/1000 | Loss: 0.00003674
Iteration 249/1000 | Loss: 0.00003236
Iteration 250/1000 | Loss: 0.00002803
Iteration 251/1000 | Loss: 0.00003396
Iteration 252/1000 | Loss: 0.00003342
Iteration 253/1000 | Loss: 0.00003237
Iteration 254/1000 | Loss: 0.00003174
Iteration 255/1000 | Loss: 0.00003705
Iteration 256/1000 | Loss: 0.00003409
Iteration 257/1000 | Loss: 0.00003767
Iteration 258/1000 | Loss: 0.00003194
Iteration 259/1000 | Loss: 0.00003690
Iteration 260/1000 | Loss: 0.00003352
Iteration 261/1000 | Loss: 0.00003789
Iteration 262/1000 | Loss: 0.00003137
Iteration 263/1000 | Loss: 0.00003305
Iteration 264/1000 | Loss: 0.00003528
Iteration 265/1000 | Loss: 0.00003340
Iteration 266/1000 | Loss: 0.00003835
Iteration 267/1000 | Loss: 0.00002973
Iteration 268/1000 | Loss: 0.00003866
Iteration 269/1000 | Loss: 0.00003147
Iteration 270/1000 | Loss: 0.00003137
Iteration 271/1000 | Loss: 0.00003688
Iteration 272/1000 | Loss: 0.00003992
Iteration 273/1000 | Loss: 0.00003197
Iteration 274/1000 | Loss: 0.00003172
Iteration 275/1000 | Loss: 0.00002862
Iteration 276/1000 | Loss: 0.00002895
Iteration 277/1000 | Loss: 0.00002784
Iteration 278/1000 | Loss: 0.00002841
Iteration 279/1000 | Loss: 0.00002813
Iteration 280/1000 | Loss: 0.00002812
Iteration 281/1000 | Loss: 0.00002812
Iteration 282/1000 | Loss: 0.00002848
Iteration 283/1000 | Loss: 0.00003529
Iteration 284/1000 | Loss: 0.00002924
Iteration 285/1000 | Loss: 0.00003037
Iteration 286/1000 | Loss: 0.00002864
Iteration 287/1000 | Loss: 0.00002822
Iteration 288/1000 | Loss: 0.00002791
Iteration 289/1000 | Loss: 0.00002878
Iteration 290/1000 | Loss: 0.00003555
Iteration 291/1000 | Loss: 0.00002823
Iteration 292/1000 | Loss: 0.00002844
Iteration 293/1000 | Loss: 0.00003613
Iteration 294/1000 | Loss: 0.00003343
Iteration 295/1000 | Loss: 0.00003598
Iteration 296/1000 | Loss: 0.00003295
Iteration 297/1000 | Loss: 0.00003294
Iteration 298/1000 | Loss: 0.00002989
Iteration 299/1000 | Loss: 0.00002838
Iteration 300/1000 | Loss: 0.00002909
Iteration 301/1000 | Loss: 0.00002730
Iteration 302/1000 | Loss: 0.00002854
Iteration 303/1000 | Loss: 0.00002764
Iteration 304/1000 | Loss: 0.00002837
Iteration 305/1000 | Loss: 0.00002774
Iteration 306/1000 | Loss: 0.00002813
Iteration 307/1000 | Loss: 0.00002768
Iteration 308/1000 | Loss: 0.00002742
Iteration 309/1000 | Loss: 0.00002774
Iteration 310/1000 | Loss: 0.00002791
Iteration 311/1000 | Loss: 0.00002765
Iteration 312/1000 | Loss: 0.00002785
Iteration 313/1000 | Loss: 0.00003468
Iteration 314/1000 | Loss: 0.00003333
Iteration 315/1000 | Loss: 0.00002746
Iteration 316/1000 | Loss: 0.00003461
Iteration 317/1000 | Loss: 0.00003652
Iteration 318/1000 | Loss: 0.00003219
Iteration 319/1000 | Loss: 0.00003438
Iteration 320/1000 | Loss: 0.00003693
Iteration 321/1000 | Loss: 0.00003672
Iteration 322/1000 | Loss: 0.00003299
Iteration 323/1000 | Loss: 0.00002861
Iteration 324/1000 | Loss: 0.00002899
Iteration 325/1000 | Loss: 0.00003461
Iteration 326/1000 | Loss: 0.00003476
Iteration 327/1000 | Loss: 0.00003418
Iteration 328/1000 | Loss: 0.00003395
Iteration 329/1000 | Loss: 0.00003455
Iteration 330/1000 | Loss: 0.00003243
Iteration 331/1000 | Loss: 0.00003403
Iteration 332/1000 | Loss: 0.00003529
Iteration 333/1000 | Loss: 0.00003615
Iteration 334/1000 | Loss: 0.00003608
Iteration 335/1000 | Loss: 0.00003574
Iteration 336/1000 | Loss: 0.00003486
Iteration 337/1000 | Loss: 0.00003246
Iteration 338/1000 | Loss: 0.00003422
Iteration 339/1000 | Loss: 0.00003463
Iteration 340/1000 | Loss: 0.00003593
Iteration 341/1000 | Loss: 0.00003500
Iteration 342/1000 | Loss: 0.00003456
Iteration 343/1000 | Loss: 0.00003519
Iteration 344/1000 | Loss: 0.00003474
Iteration 345/1000 | Loss: 0.00003497
Iteration 346/1000 | Loss: 0.00003396
Iteration 347/1000 | Loss: 0.00003415
Iteration 348/1000 | Loss: 0.00002939
Iteration 349/1000 | Loss: 0.00003288
Iteration 350/1000 | Loss: 0.00003187
Iteration 351/1000 | Loss: 0.00003032
Iteration 352/1000 | Loss: 0.00003333
Iteration 353/1000 | Loss: 0.00002861
Iteration 354/1000 | Loss: 0.00003547
Iteration 355/1000 | Loss: 0.00003359
Iteration 356/1000 | Loss: 0.00003545
Iteration 357/1000 | Loss: 0.00003394
Iteration 358/1000 | Loss: 0.00003526
Iteration 359/1000 | Loss: 0.00003502
Iteration 360/1000 | Loss: 0.00003469
Iteration 361/1000 | Loss: 0.00003496
Iteration 362/1000 | Loss: 0.00003474
Iteration 363/1000 | Loss: 0.00003526
Iteration 364/1000 | Loss: 0.00003395
Iteration 365/1000 | Loss: 0.00003190
Iteration 366/1000 | Loss: 0.00003502
Iteration 367/1000 | Loss: 0.00003212
Iteration 368/1000 | Loss: 0.00003391
Iteration 369/1000 | Loss: 0.00003257
Iteration 370/1000 | Loss: 0.00003088
Iteration 371/1000 | Loss: 0.00003255
Iteration 372/1000 | Loss: 0.00002542
Iteration 373/1000 | Loss: 0.00003635
Iteration 374/1000 | Loss: 0.00003552
Iteration 375/1000 | Loss: 0.00003820
Iteration 376/1000 | Loss: 0.00003553
Iteration 377/1000 | Loss: 0.00003225
Iteration 378/1000 | Loss: 0.00003558
Iteration 379/1000 | Loss: 0.00003587
Iteration 380/1000 | Loss: 0.00003606
Iteration 381/1000 | Loss: 0.00003644
Iteration 382/1000 | Loss: 0.00003364
Iteration 383/1000 | Loss: 0.00003696
Iteration 384/1000 | Loss: 0.00003531
Iteration 385/1000 | Loss: 0.00003454
Iteration 386/1000 | Loss: 0.00003484
Iteration 387/1000 | Loss: 0.00003126
Iteration 388/1000 | Loss: 0.00003125
Iteration 389/1000 | Loss: 0.00004164
Iteration 390/1000 | Loss: 0.00002650
Iteration 391/1000 | Loss: 0.00004963
Iteration 392/1000 | Loss: 0.00004737
Iteration 393/1000 | Loss: 0.00003940
Iteration 394/1000 | Loss: 0.00004426
Iteration 395/1000 | Loss: 0.00005101
Iteration 396/1000 | Loss: 0.00003577
Iteration 397/1000 | Loss: 0.00003246
Iteration 398/1000 | Loss: 0.00003039
Iteration 399/1000 | Loss: 0.00002838
Iteration 400/1000 | Loss: 0.00002721
Iteration 401/1000 | Loss: 0.00002618
Iteration 402/1000 | Loss: 0.00002552
Iteration 403/1000 | Loss: 0.00002510
Iteration 404/1000 | Loss: 0.00002483
Iteration 405/1000 | Loss: 0.00002475
Iteration 406/1000 | Loss: 0.00002473
Iteration 407/1000 | Loss: 0.00002473
Iteration 408/1000 | Loss: 0.00002467
Iteration 409/1000 | Loss: 0.00002463
Iteration 410/1000 | Loss: 0.00002446
Iteration 411/1000 | Loss: 0.00002444
Iteration 412/1000 | Loss: 0.00002443
Iteration 413/1000 | Loss: 0.00002441
Iteration 414/1000 | Loss: 0.00002439
Iteration 415/1000 | Loss: 0.00002438
Iteration 416/1000 | Loss: 0.00002437
Iteration 417/1000 | Loss: 0.00002437
Iteration 418/1000 | Loss: 0.00002436
Iteration 419/1000 | Loss: 0.00002436
Iteration 420/1000 | Loss: 0.00002435
Iteration 421/1000 | Loss: 0.00002434
Iteration 422/1000 | Loss: 0.00002434
Iteration 423/1000 | Loss: 0.00002434
Iteration 424/1000 | Loss: 0.00002434
Iteration 425/1000 | Loss: 0.00002433
Iteration 426/1000 | Loss: 0.00002433
Iteration 427/1000 | Loss: 0.00002433
Iteration 428/1000 | Loss: 0.00002433
Iteration 429/1000 | Loss: 0.00002433
Iteration 430/1000 | Loss: 0.00002433
Iteration 431/1000 | Loss: 0.00002433
Iteration 432/1000 | Loss: 0.00002432
Iteration 433/1000 | Loss: 0.00002432
Iteration 434/1000 | Loss: 0.00002432
Iteration 435/1000 | Loss: 0.00002431
Iteration 436/1000 | Loss: 0.00002431
Iteration 437/1000 | Loss: 0.00002431
Iteration 438/1000 | Loss: 0.00002431
Iteration 439/1000 | Loss: 0.00002430
Iteration 440/1000 | Loss: 0.00002430
Iteration 441/1000 | Loss: 0.00002430
Iteration 442/1000 | Loss: 0.00002429
Iteration 443/1000 | Loss: 0.00002429
Iteration 444/1000 | Loss: 0.00002429
Iteration 445/1000 | Loss: 0.00002428
Iteration 446/1000 | Loss: 0.00002428
Iteration 447/1000 | Loss: 0.00002428
Iteration 448/1000 | Loss: 0.00002428
Iteration 449/1000 | Loss: 0.00002428
Iteration 450/1000 | Loss: 0.00002427
Iteration 451/1000 | Loss: 0.00002427
Iteration 452/1000 | Loss: 0.00002427
Iteration 453/1000 | Loss: 0.00002427
Iteration 454/1000 | Loss: 0.00002427
Iteration 455/1000 | Loss: 0.00002426
Iteration 456/1000 | Loss: 0.00002426
Iteration 457/1000 | Loss: 0.00002426
Iteration 458/1000 | Loss: 0.00002426
Iteration 459/1000 | Loss: 0.00002426
Iteration 460/1000 | Loss: 0.00002426
Iteration 461/1000 | Loss: 0.00002426
Iteration 462/1000 | Loss: 0.00002426
Iteration 463/1000 | Loss: 0.00002425
Iteration 464/1000 | Loss: 0.00002425
Iteration 465/1000 | Loss: 0.00002425
Iteration 466/1000 | Loss: 0.00002424
Iteration 467/1000 | Loss: 0.00002424
Iteration 468/1000 | Loss: 0.00002424
Iteration 469/1000 | Loss: 0.00002424
Iteration 470/1000 | Loss: 0.00002424
Iteration 471/1000 | Loss: 0.00002424
Iteration 472/1000 | Loss: 0.00002424
Iteration 473/1000 | Loss: 0.00002423
Iteration 474/1000 | Loss: 0.00002423
Iteration 475/1000 | Loss: 0.00002423
Iteration 476/1000 | Loss: 0.00002423
Iteration 477/1000 | Loss: 0.00002423
Iteration 478/1000 | Loss: 0.00002423
Iteration 479/1000 | Loss: 0.00002422
Iteration 480/1000 | Loss: 0.00002422
Iteration 481/1000 | Loss: 0.00002422
Iteration 482/1000 | Loss: 0.00002422
Iteration 483/1000 | Loss: 0.00002422
Iteration 484/1000 | Loss: 0.00002422
Iteration 485/1000 | Loss: 0.00002422
Iteration 486/1000 | Loss: 0.00002422
Iteration 487/1000 | Loss: 0.00002421
Iteration 488/1000 | Loss: 0.00002421
Iteration 489/1000 | Loss: 0.00002421
Iteration 490/1000 | Loss: 0.00002421
Iteration 491/1000 | Loss: 0.00002420
Iteration 492/1000 | Loss: 0.00002420
Iteration 493/1000 | Loss: 0.00002420
Iteration 494/1000 | Loss: 0.00002420
Iteration 495/1000 | Loss: 0.00002420
Iteration 496/1000 | Loss: 0.00002420
Iteration 497/1000 | Loss: 0.00002420
Iteration 498/1000 | Loss: 0.00002420
Iteration 499/1000 | Loss: 0.00002419
Iteration 500/1000 | Loss: 0.00002419
Iteration 501/1000 | Loss: 0.00002419
Iteration 502/1000 | Loss: 0.00002419
Iteration 503/1000 | Loss: 0.00002419
Iteration 504/1000 | Loss: 0.00002419
Iteration 505/1000 | Loss: 0.00002419
Iteration 506/1000 | Loss: 0.00002419
Iteration 507/1000 | Loss: 0.00002419
Iteration 508/1000 | Loss: 0.00002419
Iteration 509/1000 | Loss: 0.00002419
Iteration 510/1000 | Loss: 0.00002419
Iteration 511/1000 | Loss: 0.00002419
Iteration 512/1000 | Loss: 0.00002418
Iteration 513/1000 | Loss: 0.00002418
Iteration 514/1000 | Loss: 0.00002418
Iteration 515/1000 | Loss: 0.00002418
Iteration 516/1000 | Loss: 0.00002418
Iteration 517/1000 | Loss: 0.00002418
Iteration 518/1000 | Loss: 0.00002418
Iteration 519/1000 | Loss: 0.00002418
Iteration 520/1000 | Loss: 0.00002418
Iteration 521/1000 | Loss: 0.00002417
Iteration 522/1000 | Loss: 0.00002417
Iteration 523/1000 | Loss: 0.00002417
Iteration 524/1000 | Loss: 0.00002417
Iteration 525/1000 | Loss: 0.00002417
Iteration 526/1000 | Loss: 0.00002417
Iteration 527/1000 | Loss: 0.00002417
Iteration 528/1000 | Loss: 0.00002417
Iteration 529/1000 | Loss: 0.00002417
Iteration 530/1000 | Loss: 0.00002417
Iteration 531/1000 | Loss: 0.00002417
Iteration 532/1000 | Loss: 0.00002417
Iteration 533/1000 | Loss: 0.00002417
Iteration 534/1000 | Loss: 0.00002416
Iteration 535/1000 | Loss: 0.00002416
Iteration 536/1000 | Loss: 0.00002416
Iteration 537/1000 | Loss: 0.00002416
Iteration 538/1000 | Loss: 0.00002416
Iteration 539/1000 | Loss: 0.00002416
Iteration 540/1000 | Loss: 0.00002415
Iteration 541/1000 | Loss: 0.00002415
Iteration 542/1000 | Loss: 0.00002415
Iteration 543/1000 | Loss: 0.00002415
Iteration 544/1000 | Loss: 0.00002414
Iteration 545/1000 | Loss: 0.00002414
Iteration 546/1000 | Loss: 0.00002414
Iteration 547/1000 | Loss: 0.00002414
Iteration 548/1000 | Loss: 0.00002414
Iteration 549/1000 | Loss: 0.00002414
Iteration 550/1000 | Loss: 0.00002414
Iteration 551/1000 | Loss: 0.00002414
Iteration 552/1000 | Loss: 0.00002413
Iteration 553/1000 | Loss: 0.00002413
Iteration 554/1000 | Loss: 0.00002413
Iteration 555/1000 | Loss: 0.00002413
Iteration 556/1000 | Loss: 0.00002413
Iteration 557/1000 | Loss: 0.00002413
Iteration 558/1000 | Loss: 0.00002413
Iteration 559/1000 | Loss: 0.00002413
Iteration 560/1000 | Loss: 0.00002413
Iteration 561/1000 | Loss: 0.00002413
Iteration 562/1000 | Loss: 0.00002412
Iteration 563/1000 | Loss: 0.00002412
Iteration 564/1000 | Loss: 0.00002412
Iteration 565/1000 | Loss: 0.00002412
Iteration 566/1000 | Loss: 0.00002412
Iteration 567/1000 | Loss: 0.00002412
Iteration 568/1000 | Loss: 0.00002411
Iteration 569/1000 | Loss: 0.00002411
Iteration 570/1000 | Loss: 0.00002411
Iteration 571/1000 | Loss: 0.00002411
Iteration 572/1000 | Loss: 0.00002411
Iteration 573/1000 | Loss: 0.00002411
Iteration 574/1000 | Loss: 0.00002411
Iteration 575/1000 | Loss: 0.00002411
Iteration 576/1000 | Loss: 0.00002411
Iteration 577/1000 | Loss: 0.00002411
Iteration 578/1000 | Loss: 0.00002411
Iteration 579/1000 | Loss: 0.00002411
Iteration 580/1000 | Loss: 0.00002410
Iteration 581/1000 | Loss: 0.00002410
Iteration 582/1000 | Loss: 0.00002410
Iteration 583/1000 | Loss: 0.00002410
Iteration 584/1000 | Loss: 0.00002410
Iteration 585/1000 | Loss: 0.00002409
Iteration 586/1000 | Loss: 0.00002409
Iteration 587/1000 | Loss: 0.00002409
Iteration 588/1000 | Loss: 0.00002409
Iteration 589/1000 | Loss: 0.00002409
Iteration 590/1000 | Loss: 0.00002409
Iteration 591/1000 | Loss: 0.00002409
Iteration 592/1000 | Loss: 0.00002409
Iteration 593/1000 | Loss: 0.00002409
Iteration 594/1000 | Loss: 0.00002409
Iteration 595/1000 | Loss: 0.00002409
Iteration 596/1000 | Loss: 0.00002409
Iteration 597/1000 | Loss: 0.00002409
Iteration 598/1000 | Loss: 0.00002409
Iteration 599/1000 | Loss: 0.00002408
Iteration 600/1000 | Loss: 0.00002408
Iteration 601/1000 | Loss: 0.00002408
Iteration 602/1000 | Loss: 0.00002408
Iteration 603/1000 | Loss: 0.00002408
Iteration 604/1000 | Loss: 0.00002408
Iteration 605/1000 | Loss: 0.00002408
Iteration 606/1000 | Loss: 0.00002408
Iteration 607/1000 | Loss: 0.00002408
Iteration 608/1000 | Loss: 0.00002408
Iteration 609/1000 | Loss: 0.00002408
Iteration 610/1000 | Loss: 0.00002408
Iteration 611/1000 | Loss: 0.00002408
Iteration 612/1000 | Loss: 0.00002408
Iteration 613/1000 | Loss: 0.00002408
Iteration 614/1000 | Loss: 0.00002408
Iteration 615/1000 | Loss: 0.00002407
Iteration 616/1000 | Loss: 0.00002407
Iteration 617/1000 | Loss: 0.00002407
Iteration 618/1000 | Loss: 0.00002407
Iteration 619/1000 | Loss: 0.00002407
Iteration 620/1000 | Loss: 0.00002407
Iteration 621/1000 | Loss: 0.00002407
Iteration 622/1000 | Loss: 0.00002407
Iteration 623/1000 | Loss: 0.00002407
Iteration 624/1000 | Loss: 0.00002407
Iteration 625/1000 | Loss: 0.00002407
Iteration 626/1000 | Loss: 0.00002407
Iteration 627/1000 | Loss: 0.00002407
Iteration 628/1000 | Loss: 0.00002407
Iteration 629/1000 | Loss: 0.00002407
Iteration 630/1000 | Loss: 0.00002407
Iteration 631/1000 | Loss: 0.00002407
Iteration 632/1000 | Loss: 0.00002407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 632. Stopping optimization.
Last 5 losses: [2.4068262064247392e-05, 2.4068262064247392e-05, 2.4068262064247392e-05, 2.4068262064247392e-05, 2.4068262064247392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4068262064247392e-05

Optimization complete. Final v2v error: 3.877568006515503 mm

Highest mean error: 7.886013031005859 mm for frame 113

Lowest mean error: 2.872012138366699 mm for frame 74

Saving results

Total time: 620.9065968990326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516298
Iteration 2/25 | Loss: 0.00160958
Iteration 3/25 | Loss: 0.00126753
Iteration 4/25 | Loss: 0.00124585
Iteration 5/25 | Loss: 0.00123979
Iteration 6/25 | Loss: 0.00123782
Iteration 7/25 | Loss: 0.00123718
Iteration 8/25 | Loss: 0.00123718
Iteration 9/25 | Loss: 0.00123718
Iteration 10/25 | Loss: 0.00123718
Iteration 11/25 | Loss: 0.00123718
Iteration 12/25 | Loss: 0.00123718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012371756602078676, 0.0012371756602078676, 0.0012371756602078676, 0.0012371756602078676, 0.0012371756602078676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012371756602078676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24906397
Iteration 2/25 | Loss: 0.00116482
Iteration 3/25 | Loss: 0.00116480
Iteration 4/25 | Loss: 0.00116480
Iteration 5/25 | Loss: 0.00116480
Iteration 6/25 | Loss: 0.00116480
Iteration 7/25 | Loss: 0.00116480
Iteration 8/25 | Loss: 0.00116480
Iteration 9/25 | Loss: 0.00116480
Iteration 10/25 | Loss: 0.00116480
Iteration 11/25 | Loss: 0.00116480
Iteration 12/25 | Loss: 0.00116480
Iteration 13/25 | Loss: 0.00116480
Iteration 14/25 | Loss: 0.00116480
Iteration 15/25 | Loss: 0.00116480
Iteration 16/25 | Loss: 0.00116480
Iteration 17/25 | Loss: 0.00116480
Iteration 18/25 | Loss: 0.00116480
Iteration 19/25 | Loss: 0.00116480
Iteration 20/25 | Loss: 0.00116480
Iteration 21/25 | Loss: 0.00116480
Iteration 22/25 | Loss: 0.00116480
Iteration 23/25 | Loss: 0.00116480
Iteration 24/25 | Loss: 0.00116480
Iteration 25/25 | Loss: 0.00116480

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116480
Iteration 2/1000 | Loss: 0.00005798
Iteration 3/1000 | Loss: 0.00003554
Iteration 4/1000 | Loss: 0.00002653
Iteration 5/1000 | Loss: 0.00002437
Iteration 6/1000 | Loss: 0.00002309
Iteration 7/1000 | Loss: 0.00002223
Iteration 8/1000 | Loss: 0.00002166
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002098
Iteration 11/1000 | Loss: 0.00002069
Iteration 12/1000 | Loss: 0.00002045
Iteration 13/1000 | Loss: 0.00002024
Iteration 14/1000 | Loss: 0.00002005
Iteration 15/1000 | Loss: 0.00001988
Iteration 16/1000 | Loss: 0.00001981
Iteration 17/1000 | Loss: 0.00001978
Iteration 18/1000 | Loss: 0.00001972
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001968
Iteration 21/1000 | Loss: 0.00001962
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001953
Iteration 28/1000 | Loss: 0.00001952
Iteration 29/1000 | Loss: 0.00001951
Iteration 30/1000 | Loss: 0.00001951
Iteration 31/1000 | Loss: 0.00001950
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001947
Iteration 36/1000 | Loss: 0.00001947
Iteration 37/1000 | Loss: 0.00001947
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001945
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001944
Iteration 45/1000 | Loss: 0.00001944
Iteration 46/1000 | Loss: 0.00001944
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001941
Iteration 49/1000 | Loss: 0.00001941
Iteration 50/1000 | Loss: 0.00001940
Iteration 51/1000 | Loss: 0.00001939
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001937
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001936
Iteration 64/1000 | Loss: 0.00001936
Iteration 65/1000 | Loss: 0.00001936
Iteration 66/1000 | Loss: 0.00001936
Iteration 67/1000 | Loss: 0.00001936
Iteration 68/1000 | Loss: 0.00001935
Iteration 69/1000 | Loss: 0.00001935
Iteration 70/1000 | Loss: 0.00001935
Iteration 71/1000 | Loss: 0.00001934
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001933
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00001933
Iteration 81/1000 | Loss: 0.00001933
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001933
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001932
Iteration 91/1000 | Loss: 0.00001932
Iteration 92/1000 | Loss: 0.00001931
Iteration 93/1000 | Loss: 0.00001931
Iteration 94/1000 | Loss: 0.00001931
Iteration 95/1000 | Loss: 0.00001931
Iteration 96/1000 | Loss: 0.00001931
Iteration 97/1000 | Loss: 0.00001931
Iteration 98/1000 | Loss: 0.00001931
Iteration 99/1000 | Loss: 0.00001931
Iteration 100/1000 | Loss: 0.00001931
Iteration 101/1000 | Loss: 0.00001931
Iteration 102/1000 | Loss: 0.00001931
Iteration 103/1000 | Loss: 0.00001931
Iteration 104/1000 | Loss: 0.00001930
Iteration 105/1000 | Loss: 0.00001930
Iteration 106/1000 | Loss: 0.00001930
Iteration 107/1000 | Loss: 0.00001930
Iteration 108/1000 | Loss: 0.00001930
Iteration 109/1000 | Loss: 0.00001930
Iteration 110/1000 | Loss: 0.00001930
Iteration 111/1000 | Loss: 0.00001930
Iteration 112/1000 | Loss: 0.00001930
Iteration 113/1000 | Loss: 0.00001929
Iteration 114/1000 | Loss: 0.00001929
Iteration 115/1000 | Loss: 0.00001929
Iteration 116/1000 | Loss: 0.00001929
Iteration 117/1000 | Loss: 0.00001929
Iteration 118/1000 | Loss: 0.00001929
Iteration 119/1000 | Loss: 0.00001929
Iteration 120/1000 | Loss: 0.00001929
Iteration 121/1000 | Loss: 0.00001929
Iteration 122/1000 | Loss: 0.00001929
Iteration 123/1000 | Loss: 0.00001929
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.928614619828295e-05, 1.928614619828295e-05, 1.928614619828295e-05, 1.928614619828295e-05, 1.928614619828295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.928614619828295e-05

Optimization complete. Final v2v error: 3.564197301864624 mm

Highest mean error: 5.610588073730469 mm for frame 58

Lowest mean error: 2.692416191101074 mm for frame 1

Saving results

Total time: 44.718684673309326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526715
Iteration 2/25 | Loss: 0.00124221
Iteration 3/25 | Loss: 0.00116163
Iteration 4/25 | Loss: 0.00115027
Iteration 5/25 | Loss: 0.00114706
Iteration 6/25 | Loss: 0.00114637
Iteration 7/25 | Loss: 0.00114637
Iteration 8/25 | Loss: 0.00114637
Iteration 9/25 | Loss: 0.00114637
Iteration 10/25 | Loss: 0.00114637
Iteration 11/25 | Loss: 0.00114637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011463707778602839, 0.0011463707778602839, 0.0011463707778602839, 0.0011463707778602839, 0.0011463707778602839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011463707778602839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.66091967
Iteration 2/25 | Loss: 0.00117870
Iteration 3/25 | Loss: 0.00117869
Iteration 4/25 | Loss: 0.00117869
Iteration 5/25 | Loss: 0.00117869
Iteration 6/25 | Loss: 0.00117869
Iteration 7/25 | Loss: 0.00117869
Iteration 8/25 | Loss: 0.00117869
Iteration 9/25 | Loss: 0.00117869
Iteration 10/25 | Loss: 0.00117869
Iteration 11/25 | Loss: 0.00117869
Iteration 12/25 | Loss: 0.00117869
Iteration 13/25 | Loss: 0.00117869
Iteration 14/25 | Loss: 0.00117869
Iteration 15/25 | Loss: 0.00117869
Iteration 16/25 | Loss: 0.00117869
Iteration 17/25 | Loss: 0.00117869
Iteration 18/25 | Loss: 0.00117869
Iteration 19/25 | Loss: 0.00117869
Iteration 20/25 | Loss: 0.00117869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00117868569213897, 0.00117868569213897, 0.00117868569213897, 0.00117868569213897, 0.00117868569213897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00117868569213897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117869
Iteration 2/1000 | Loss: 0.00002433
Iteration 3/1000 | Loss: 0.00001785
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001396
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001276
Iteration 8/1000 | Loss: 0.00001236
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001162
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001134
Iteration 15/1000 | Loss: 0.00001132
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001127
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001118
Iteration 23/1000 | Loss: 0.00001117
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001112
Iteration 29/1000 | Loss: 0.00001112
Iteration 30/1000 | Loss: 0.00001111
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001098
Iteration 41/1000 | Loss: 0.00001098
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001096
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001089
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001087
Iteration 59/1000 | Loss: 0.00001087
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001085
Iteration 64/1000 | Loss: 0.00001085
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001084
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001082
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001081
Iteration 74/1000 | Loss: 0.00001081
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001080
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001077
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001075
Iteration 98/1000 | Loss: 0.00001075
Iteration 99/1000 | Loss: 0.00001075
Iteration 100/1000 | Loss: 0.00001075
Iteration 101/1000 | Loss: 0.00001075
Iteration 102/1000 | Loss: 0.00001075
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001075
Iteration 107/1000 | Loss: 0.00001075
Iteration 108/1000 | Loss: 0.00001075
Iteration 109/1000 | Loss: 0.00001075
Iteration 110/1000 | Loss: 0.00001075
Iteration 111/1000 | Loss: 0.00001075
Iteration 112/1000 | Loss: 0.00001075
Iteration 113/1000 | Loss: 0.00001074
Iteration 114/1000 | Loss: 0.00001074
Iteration 115/1000 | Loss: 0.00001074
Iteration 116/1000 | Loss: 0.00001074
Iteration 117/1000 | Loss: 0.00001074
Iteration 118/1000 | Loss: 0.00001074
Iteration 119/1000 | Loss: 0.00001074
Iteration 120/1000 | Loss: 0.00001074
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001073
Iteration 124/1000 | Loss: 0.00001073
Iteration 125/1000 | Loss: 0.00001073
Iteration 126/1000 | Loss: 0.00001073
Iteration 127/1000 | Loss: 0.00001073
Iteration 128/1000 | Loss: 0.00001073
Iteration 129/1000 | Loss: 0.00001073
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001073
Iteration 134/1000 | Loss: 0.00001072
Iteration 135/1000 | Loss: 0.00001072
Iteration 136/1000 | Loss: 0.00001072
Iteration 137/1000 | Loss: 0.00001072
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001072
Iteration 141/1000 | Loss: 0.00001072
Iteration 142/1000 | Loss: 0.00001072
Iteration 143/1000 | Loss: 0.00001072
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001072
Iteration 146/1000 | Loss: 0.00001072
Iteration 147/1000 | Loss: 0.00001072
Iteration 148/1000 | Loss: 0.00001072
Iteration 149/1000 | Loss: 0.00001071
Iteration 150/1000 | Loss: 0.00001071
Iteration 151/1000 | Loss: 0.00001071
Iteration 152/1000 | Loss: 0.00001071
Iteration 153/1000 | Loss: 0.00001071
Iteration 154/1000 | Loss: 0.00001071
Iteration 155/1000 | Loss: 0.00001071
Iteration 156/1000 | Loss: 0.00001071
Iteration 157/1000 | Loss: 0.00001071
Iteration 158/1000 | Loss: 0.00001071
Iteration 159/1000 | Loss: 0.00001071
Iteration 160/1000 | Loss: 0.00001071
Iteration 161/1000 | Loss: 0.00001071
Iteration 162/1000 | Loss: 0.00001071
Iteration 163/1000 | Loss: 0.00001071
Iteration 164/1000 | Loss: 0.00001070
Iteration 165/1000 | Loss: 0.00001070
Iteration 166/1000 | Loss: 0.00001070
Iteration 167/1000 | Loss: 0.00001070
Iteration 168/1000 | Loss: 0.00001070
Iteration 169/1000 | Loss: 0.00001070
Iteration 170/1000 | Loss: 0.00001070
Iteration 171/1000 | Loss: 0.00001070
Iteration 172/1000 | Loss: 0.00001070
Iteration 173/1000 | Loss: 0.00001070
Iteration 174/1000 | Loss: 0.00001070
Iteration 175/1000 | Loss: 0.00001070
Iteration 176/1000 | Loss: 0.00001070
Iteration 177/1000 | Loss: 0.00001070
Iteration 178/1000 | Loss: 0.00001070
Iteration 179/1000 | Loss: 0.00001070
Iteration 180/1000 | Loss: 0.00001070
Iteration 181/1000 | Loss: 0.00001070
Iteration 182/1000 | Loss: 0.00001070
Iteration 183/1000 | Loss: 0.00001070
Iteration 184/1000 | Loss: 0.00001070
Iteration 185/1000 | Loss: 0.00001070
Iteration 186/1000 | Loss: 0.00001070
Iteration 187/1000 | Loss: 0.00001070
Iteration 188/1000 | Loss: 0.00001070
Iteration 189/1000 | Loss: 0.00001070
Iteration 190/1000 | Loss: 0.00001070
Iteration 191/1000 | Loss: 0.00001070
Iteration 192/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.0702696272346657e-05, 1.0702696272346657e-05, 1.0702696272346657e-05, 1.0702696272346657e-05, 1.0702696272346657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0702696272346657e-05

Optimization complete. Final v2v error: 2.8062241077423096 mm

Highest mean error: 3.5609383583068848 mm for frame 70

Lowest mean error: 2.4920732975006104 mm for frame 27

Saving results

Total time: 41.058889865875244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421053
Iteration 2/25 | Loss: 0.00127972
Iteration 3/25 | Loss: 0.00117508
Iteration 4/25 | Loss: 0.00116158
Iteration 5/25 | Loss: 0.00115823
Iteration 6/25 | Loss: 0.00115738
Iteration 7/25 | Loss: 0.00115738
Iteration 8/25 | Loss: 0.00115738
Iteration 9/25 | Loss: 0.00115738
Iteration 10/25 | Loss: 0.00115738
Iteration 11/25 | Loss: 0.00115738
Iteration 12/25 | Loss: 0.00115738
Iteration 13/25 | Loss: 0.00115738
Iteration 14/25 | Loss: 0.00115738
Iteration 15/25 | Loss: 0.00115737
Iteration 16/25 | Loss: 0.00115737
Iteration 17/25 | Loss: 0.00115737
Iteration 18/25 | Loss: 0.00115737
Iteration 19/25 | Loss: 0.00115737
Iteration 20/25 | Loss: 0.00115737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011573651572689414, 0.0011573651572689414, 0.0011573651572689414, 0.0011573651572689414, 0.0011573651572689414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011573651572689414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28231406
Iteration 2/25 | Loss: 0.00133506
Iteration 3/25 | Loss: 0.00133506
Iteration 4/25 | Loss: 0.00133506
Iteration 5/25 | Loss: 0.00133506
Iteration 6/25 | Loss: 0.00133505
Iteration 7/25 | Loss: 0.00133505
Iteration 8/25 | Loss: 0.00133505
Iteration 9/25 | Loss: 0.00133505
Iteration 10/25 | Loss: 0.00133505
Iteration 11/25 | Loss: 0.00133505
Iteration 12/25 | Loss: 0.00133505
Iteration 13/25 | Loss: 0.00133505
Iteration 14/25 | Loss: 0.00133505
Iteration 15/25 | Loss: 0.00133505
Iteration 16/25 | Loss: 0.00133505
Iteration 17/25 | Loss: 0.00133505
Iteration 18/25 | Loss: 0.00133505
Iteration 19/25 | Loss: 0.00133505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013350528897717595, 0.0013350528897717595, 0.0013350528897717595, 0.0013350528897717595, 0.0013350528897717595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013350528897717595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133505
Iteration 2/1000 | Loss: 0.00002792
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001482
Iteration 5/1000 | Loss: 0.00001380
Iteration 6/1000 | Loss: 0.00001282
Iteration 7/1000 | Loss: 0.00001234
Iteration 8/1000 | Loss: 0.00001189
Iteration 9/1000 | Loss: 0.00001178
Iteration 10/1000 | Loss: 0.00001155
Iteration 11/1000 | Loss: 0.00001126
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001108
Iteration 14/1000 | Loss: 0.00001100
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001087
Iteration 17/1000 | Loss: 0.00001086
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001072
Iteration 23/1000 | Loss: 0.00001072
Iteration 24/1000 | Loss: 0.00001068
Iteration 25/1000 | Loss: 0.00001064
Iteration 26/1000 | Loss: 0.00001064
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001064
Iteration 29/1000 | Loss: 0.00001063
Iteration 30/1000 | Loss: 0.00001061
Iteration 31/1000 | Loss: 0.00001060
Iteration 32/1000 | Loss: 0.00001060
Iteration 33/1000 | Loss: 0.00001059
Iteration 34/1000 | Loss: 0.00001059
Iteration 35/1000 | Loss: 0.00001059
Iteration 36/1000 | Loss: 0.00001059
Iteration 37/1000 | Loss: 0.00001059
Iteration 38/1000 | Loss: 0.00001058
Iteration 39/1000 | Loss: 0.00001058
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001057
Iteration 44/1000 | Loss: 0.00001057
Iteration 45/1000 | Loss: 0.00001056
Iteration 46/1000 | Loss: 0.00001055
Iteration 47/1000 | Loss: 0.00001054
Iteration 48/1000 | Loss: 0.00001054
Iteration 49/1000 | Loss: 0.00001054
Iteration 50/1000 | Loss: 0.00001053
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001051
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001050
Iteration 55/1000 | Loss: 0.00001050
Iteration 56/1000 | Loss: 0.00001050
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001049
Iteration 59/1000 | Loss: 0.00001049
Iteration 60/1000 | Loss: 0.00001048
Iteration 61/1000 | Loss: 0.00001048
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001047
Iteration 65/1000 | Loss: 0.00001046
Iteration 66/1000 | Loss: 0.00001046
Iteration 67/1000 | Loss: 0.00001045
Iteration 68/1000 | Loss: 0.00001045
Iteration 69/1000 | Loss: 0.00001044
Iteration 70/1000 | Loss: 0.00001044
Iteration 71/1000 | Loss: 0.00001044
Iteration 72/1000 | Loss: 0.00001043
Iteration 73/1000 | Loss: 0.00001043
Iteration 74/1000 | Loss: 0.00001043
Iteration 75/1000 | Loss: 0.00001042
Iteration 76/1000 | Loss: 0.00001042
Iteration 77/1000 | Loss: 0.00001041
Iteration 78/1000 | Loss: 0.00001041
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001039
Iteration 83/1000 | Loss: 0.00001039
Iteration 84/1000 | Loss: 0.00001039
Iteration 85/1000 | Loss: 0.00001038
Iteration 86/1000 | Loss: 0.00001038
Iteration 87/1000 | Loss: 0.00001038
Iteration 88/1000 | Loss: 0.00001038
Iteration 89/1000 | Loss: 0.00001038
Iteration 90/1000 | Loss: 0.00001038
Iteration 91/1000 | Loss: 0.00001038
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001037
Iteration 95/1000 | Loss: 0.00001037
Iteration 96/1000 | Loss: 0.00001037
Iteration 97/1000 | Loss: 0.00001037
Iteration 98/1000 | Loss: 0.00001037
Iteration 99/1000 | Loss: 0.00001037
Iteration 100/1000 | Loss: 0.00001037
Iteration 101/1000 | Loss: 0.00001037
Iteration 102/1000 | Loss: 0.00001037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.037402034853585e-05, 1.037402034853585e-05, 1.037402034853585e-05, 1.037402034853585e-05, 1.037402034853585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.037402034853585e-05

Optimization complete. Final v2v error: 2.765422821044922 mm

Highest mean error: 3.743708848953247 mm for frame 60

Lowest mean error: 2.4714550971984863 mm for frame 41

Saving results

Total time: 40.12509250640869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782068
Iteration 2/25 | Loss: 0.00148785
Iteration 3/25 | Loss: 0.00122922
Iteration 4/25 | Loss: 0.00119848
Iteration 5/25 | Loss: 0.00118902
Iteration 6/25 | Loss: 0.00118696
Iteration 7/25 | Loss: 0.00118628
Iteration 8/25 | Loss: 0.00118610
Iteration 9/25 | Loss: 0.00118600
Iteration 10/25 | Loss: 0.00118600
Iteration 11/25 | Loss: 0.00118600
Iteration 12/25 | Loss: 0.00118600
Iteration 13/25 | Loss: 0.00118599
Iteration 14/25 | Loss: 0.00118599
Iteration 15/25 | Loss: 0.00118599
Iteration 16/25 | Loss: 0.00118599
Iteration 17/25 | Loss: 0.00118599
Iteration 18/25 | Loss: 0.00118598
Iteration 19/25 | Loss: 0.00118598
Iteration 20/25 | Loss: 0.00118598
Iteration 21/25 | Loss: 0.00118598
Iteration 22/25 | Loss: 0.00118598
Iteration 23/25 | Loss: 0.00118598
Iteration 24/25 | Loss: 0.00118598
Iteration 25/25 | Loss: 0.00118598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07846236
Iteration 2/25 | Loss: 0.00134425
Iteration 3/25 | Loss: 0.00134424
Iteration 4/25 | Loss: 0.00134424
Iteration 5/25 | Loss: 0.00134424
Iteration 6/25 | Loss: 0.00134424
Iteration 7/25 | Loss: 0.00134424
Iteration 8/25 | Loss: 0.00134424
Iteration 9/25 | Loss: 0.00134424
Iteration 10/25 | Loss: 0.00134424
Iteration 11/25 | Loss: 0.00134424
Iteration 12/25 | Loss: 0.00134424
Iteration 13/25 | Loss: 0.00134424
Iteration 14/25 | Loss: 0.00134424
Iteration 15/25 | Loss: 0.00134424
Iteration 16/25 | Loss: 0.00134424
Iteration 17/25 | Loss: 0.00134424
Iteration 18/25 | Loss: 0.00134424
Iteration 19/25 | Loss: 0.00134424
Iteration 20/25 | Loss: 0.00134424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013442368945106864, 0.0013442368945106864, 0.0013442368945106864, 0.0013442368945106864, 0.0013442368945106864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013442368945106864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134424
Iteration 2/1000 | Loss: 0.00002263
Iteration 3/1000 | Loss: 0.00001821
Iteration 4/1000 | Loss: 0.00001707
Iteration 5/1000 | Loss: 0.00001952
Iteration 6/1000 | Loss: 0.00001606
Iteration 7/1000 | Loss: 0.00001811
Iteration 8/1000 | Loss: 0.00001688
Iteration 9/1000 | Loss: 0.00001569
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001763
Iteration 12/1000 | Loss: 0.00001437
Iteration 13/1000 | Loss: 0.00001426
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001424
Iteration 16/1000 | Loss: 0.00001424
Iteration 17/1000 | Loss: 0.00001424
Iteration 18/1000 | Loss: 0.00001423
Iteration 19/1000 | Loss: 0.00001423
Iteration 20/1000 | Loss: 0.00001423
Iteration 21/1000 | Loss: 0.00001423
Iteration 22/1000 | Loss: 0.00001423
Iteration 23/1000 | Loss: 0.00001423
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001423
Iteration 28/1000 | Loss: 0.00001423
Iteration 29/1000 | Loss: 0.00001423
Iteration 30/1000 | Loss: 0.00001423
Iteration 31/1000 | Loss: 0.00001423
Iteration 32/1000 | Loss: 0.00001423
Iteration 33/1000 | Loss: 0.00001423
Iteration 34/1000 | Loss: 0.00001423
Iteration 35/1000 | Loss: 0.00001423
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001423
Iteration 39/1000 | Loss: 0.00001423
Iteration 40/1000 | Loss: 0.00001423
Iteration 41/1000 | Loss: 0.00001423
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001423
Iteration 47/1000 | Loss: 0.00001423
Iteration 48/1000 | Loss: 0.00001423
Iteration 49/1000 | Loss: 0.00001423
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.422605237166863e-05, 1.422605237166863e-05, 1.422605237166863e-05, 1.422605237166863e-05, 1.422605237166863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.422605237166863e-05

Optimization complete. Final v2v error: 3.1565051078796387 mm

Highest mean error: 5.285215854644775 mm for frame 48

Lowest mean error: 2.8029236793518066 mm for frame 151

Saving results

Total time: 39.39455032348633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875985
Iteration 2/25 | Loss: 0.00125289
Iteration 3/25 | Loss: 0.00115632
Iteration 4/25 | Loss: 0.00114403
Iteration 5/25 | Loss: 0.00114109
Iteration 6/25 | Loss: 0.00114109
Iteration 7/25 | Loss: 0.00114109
Iteration 8/25 | Loss: 0.00114109
Iteration 9/25 | Loss: 0.00114109
Iteration 10/25 | Loss: 0.00114109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011410912266001105, 0.0011410912266001105, 0.0011410912266001105, 0.0011410912266001105, 0.0011410912266001105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011410912266001105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.72184181
Iteration 2/25 | Loss: 0.00130768
Iteration 3/25 | Loss: 0.00130768
Iteration 4/25 | Loss: 0.00130768
Iteration 5/25 | Loss: 0.00130767
Iteration 6/25 | Loss: 0.00130767
Iteration 7/25 | Loss: 0.00130767
Iteration 8/25 | Loss: 0.00130767
Iteration 9/25 | Loss: 0.00130767
Iteration 10/25 | Loss: 0.00130767
Iteration 11/25 | Loss: 0.00130767
Iteration 12/25 | Loss: 0.00130767
Iteration 13/25 | Loss: 0.00130767
Iteration 14/25 | Loss: 0.00130767
Iteration 15/25 | Loss: 0.00130767
Iteration 16/25 | Loss: 0.00130767
Iteration 17/25 | Loss: 0.00130767
Iteration 18/25 | Loss: 0.00130767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013076727045699954, 0.0013076727045699954, 0.0013076727045699954, 0.0013076727045699954, 0.0013076727045699954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013076727045699954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130767
Iteration 2/1000 | Loss: 0.00001866
Iteration 3/1000 | Loss: 0.00001386
Iteration 4/1000 | Loss: 0.00001219
Iteration 5/1000 | Loss: 0.00001143
Iteration 6/1000 | Loss: 0.00001091
Iteration 7/1000 | Loss: 0.00001046
Iteration 8/1000 | Loss: 0.00001045
Iteration 9/1000 | Loss: 0.00001024
Iteration 10/1000 | Loss: 0.00000992
Iteration 11/1000 | Loss: 0.00000978
Iteration 12/1000 | Loss: 0.00000974
Iteration 13/1000 | Loss: 0.00000961
Iteration 14/1000 | Loss: 0.00000953
Iteration 15/1000 | Loss: 0.00000949
Iteration 16/1000 | Loss: 0.00000948
Iteration 17/1000 | Loss: 0.00000948
Iteration 18/1000 | Loss: 0.00000948
Iteration 19/1000 | Loss: 0.00000948
Iteration 20/1000 | Loss: 0.00000948
Iteration 21/1000 | Loss: 0.00000948
Iteration 22/1000 | Loss: 0.00000940
Iteration 23/1000 | Loss: 0.00000940
Iteration 24/1000 | Loss: 0.00000939
Iteration 25/1000 | Loss: 0.00000938
Iteration 26/1000 | Loss: 0.00000937
Iteration 27/1000 | Loss: 0.00000937
Iteration 28/1000 | Loss: 0.00000936
Iteration 29/1000 | Loss: 0.00000928
Iteration 30/1000 | Loss: 0.00000917
Iteration 31/1000 | Loss: 0.00000916
Iteration 32/1000 | Loss: 0.00000915
Iteration 33/1000 | Loss: 0.00000914
Iteration 34/1000 | Loss: 0.00000914
Iteration 35/1000 | Loss: 0.00000913
Iteration 36/1000 | Loss: 0.00000912
Iteration 37/1000 | Loss: 0.00000911
Iteration 38/1000 | Loss: 0.00000907
Iteration 39/1000 | Loss: 0.00000906
Iteration 40/1000 | Loss: 0.00000905
Iteration 41/1000 | Loss: 0.00000905
Iteration 42/1000 | Loss: 0.00000904
Iteration 43/1000 | Loss: 0.00000903
Iteration 44/1000 | Loss: 0.00000903
Iteration 45/1000 | Loss: 0.00000902
Iteration 46/1000 | Loss: 0.00000901
Iteration 47/1000 | Loss: 0.00000900
Iteration 48/1000 | Loss: 0.00000900
Iteration 49/1000 | Loss: 0.00000900
Iteration 50/1000 | Loss: 0.00000900
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000899
Iteration 53/1000 | Loss: 0.00000899
Iteration 54/1000 | Loss: 0.00000897
Iteration 55/1000 | Loss: 0.00000896
Iteration 56/1000 | Loss: 0.00000896
Iteration 57/1000 | Loss: 0.00000895
Iteration 58/1000 | Loss: 0.00000895
Iteration 59/1000 | Loss: 0.00000894
Iteration 60/1000 | Loss: 0.00000894
Iteration 61/1000 | Loss: 0.00000894
Iteration 62/1000 | Loss: 0.00000893
Iteration 63/1000 | Loss: 0.00000893
Iteration 64/1000 | Loss: 0.00000892
Iteration 65/1000 | Loss: 0.00000892
Iteration 66/1000 | Loss: 0.00000892
Iteration 67/1000 | Loss: 0.00000891
Iteration 68/1000 | Loss: 0.00000891
Iteration 69/1000 | Loss: 0.00000891
Iteration 70/1000 | Loss: 0.00000891
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000890
Iteration 73/1000 | Loss: 0.00000890
Iteration 74/1000 | Loss: 0.00000890
Iteration 75/1000 | Loss: 0.00000890
Iteration 76/1000 | Loss: 0.00000890
Iteration 77/1000 | Loss: 0.00000890
Iteration 78/1000 | Loss: 0.00000890
Iteration 79/1000 | Loss: 0.00000890
Iteration 80/1000 | Loss: 0.00000890
Iteration 81/1000 | Loss: 0.00000889
Iteration 82/1000 | Loss: 0.00000889
Iteration 83/1000 | Loss: 0.00000889
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000885
Iteration 86/1000 | Loss: 0.00000885
Iteration 87/1000 | Loss: 0.00000885
Iteration 88/1000 | Loss: 0.00000885
Iteration 89/1000 | Loss: 0.00000885
Iteration 90/1000 | Loss: 0.00000885
Iteration 91/1000 | Loss: 0.00000884
Iteration 92/1000 | Loss: 0.00000882
Iteration 93/1000 | Loss: 0.00000882
Iteration 94/1000 | Loss: 0.00000881
Iteration 95/1000 | Loss: 0.00000881
Iteration 96/1000 | Loss: 0.00000881
Iteration 97/1000 | Loss: 0.00000880
Iteration 98/1000 | Loss: 0.00000880
Iteration 99/1000 | Loss: 0.00000880
Iteration 100/1000 | Loss: 0.00000880
Iteration 101/1000 | Loss: 0.00000880
Iteration 102/1000 | Loss: 0.00000880
Iteration 103/1000 | Loss: 0.00000880
Iteration 104/1000 | Loss: 0.00000880
Iteration 105/1000 | Loss: 0.00000880
Iteration 106/1000 | Loss: 0.00000879
Iteration 107/1000 | Loss: 0.00000879
Iteration 108/1000 | Loss: 0.00000879
Iteration 109/1000 | Loss: 0.00000879
Iteration 110/1000 | Loss: 0.00000878
Iteration 111/1000 | Loss: 0.00000877
Iteration 112/1000 | Loss: 0.00000877
Iteration 113/1000 | Loss: 0.00000877
Iteration 114/1000 | Loss: 0.00000877
Iteration 115/1000 | Loss: 0.00000877
Iteration 116/1000 | Loss: 0.00000876
Iteration 117/1000 | Loss: 0.00000876
Iteration 118/1000 | Loss: 0.00000876
Iteration 119/1000 | Loss: 0.00000875
Iteration 120/1000 | Loss: 0.00000875
Iteration 121/1000 | Loss: 0.00000875
Iteration 122/1000 | Loss: 0.00000875
Iteration 123/1000 | Loss: 0.00000874
Iteration 124/1000 | Loss: 0.00000874
Iteration 125/1000 | Loss: 0.00000874
Iteration 126/1000 | Loss: 0.00000873
Iteration 127/1000 | Loss: 0.00000873
Iteration 128/1000 | Loss: 0.00000872
Iteration 129/1000 | Loss: 0.00000872
Iteration 130/1000 | Loss: 0.00000871
Iteration 131/1000 | Loss: 0.00000871
Iteration 132/1000 | Loss: 0.00000871
Iteration 133/1000 | Loss: 0.00000871
Iteration 134/1000 | Loss: 0.00000871
Iteration 135/1000 | Loss: 0.00000871
Iteration 136/1000 | Loss: 0.00000871
Iteration 137/1000 | Loss: 0.00000871
Iteration 138/1000 | Loss: 0.00000870
Iteration 139/1000 | Loss: 0.00000870
Iteration 140/1000 | Loss: 0.00000870
Iteration 141/1000 | Loss: 0.00000870
Iteration 142/1000 | Loss: 0.00000870
Iteration 143/1000 | Loss: 0.00000870
Iteration 144/1000 | Loss: 0.00000869
Iteration 145/1000 | Loss: 0.00000869
Iteration 146/1000 | Loss: 0.00000868
Iteration 147/1000 | Loss: 0.00000868
Iteration 148/1000 | Loss: 0.00000868
Iteration 149/1000 | Loss: 0.00000868
Iteration 150/1000 | Loss: 0.00000868
Iteration 151/1000 | Loss: 0.00000868
Iteration 152/1000 | Loss: 0.00000868
Iteration 153/1000 | Loss: 0.00000868
Iteration 154/1000 | Loss: 0.00000868
Iteration 155/1000 | Loss: 0.00000868
Iteration 156/1000 | Loss: 0.00000868
Iteration 157/1000 | Loss: 0.00000868
Iteration 158/1000 | Loss: 0.00000868
Iteration 159/1000 | Loss: 0.00000867
Iteration 160/1000 | Loss: 0.00000867
Iteration 161/1000 | Loss: 0.00000867
Iteration 162/1000 | Loss: 0.00000867
Iteration 163/1000 | Loss: 0.00000867
Iteration 164/1000 | Loss: 0.00000867
Iteration 165/1000 | Loss: 0.00000867
Iteration 166/1000 | Loss: 0.00000867
Iteration 167/1000 | Loss: 0.00000867
Iteration 168/1000 | Loss: 0.00000867
Iteration 169/1000 | Loss: 0.00000867
Iteration 170/1000 | Loss: 0.00000867
Iteration 171/1000 | Loss: 0.00000866
Iteration 172/1000 | Loss: 0.00000866
Iteration 173/1000 | Loss: 0.00000866
Iteration 174/1000 | Loss: 0.00000866
Iteration 175/1000 | Loss: 0.00000866
Iteration 176/1000 | Loss: 0.00000866
Iteration 177/1000 | Loss: 0.00000866
Iteration 178/1000 | Loss: 0.00000866
Iteration 179/1000 | Loss: 0.00000865
Iteration 180/1000 | Loss: 0.00000865
Iteration 181/1000 | Loss: 0.00000865
Iteration 182/1000 | Loss: 0.00000865
Iteration 183/1000 | Loss: 0.00000865
Iteration 184/1000 | Loss: 0.00000865
Iteration 185/1000 | Loss: 0.00000865
Iteration 186/1000 | Loss: 0.00000865
Iteration 187/1000 | Loss: 0.00000865
Iteration 188/1000 | Loss: 0.00000865
Iteration 189/1000 | Loss: 0.00000865
Iteration 190/1000 | Loss: 0.00000865
Iteration 191/1000 | Loss: 0.00000865
Iteration 192/1000 | Loss: 0.00000864
Iteration 193/1000 | Loss: 0.00000864
Iteration 194/1000 | Loss: 0.00000864
Iteration 195/1000 | Loss: 0.00000864
Iteration 196/1000 | Loss: 0.00000864
Iteration 197/1000 | Loss: 0.00000864
Iteration 198/1000 | Loss: 0.00000864
Iteration 199/1000 | Loss: 0.00000863
Iteration 200/1000 | Loss: 0.00000863
Iteration 201/1000 | Loss: 0.00000863
Iteration 202/1000 | Loss: 0.00000863
Iteration 203/1000 | Loss: 0.00000863
Iteration 204/1000 | Loss: 0.00000863
Iteration 205/1000 | Loss: 0.00000863
Iteration 206/1000 | Loss: 0.00000863
Iteration 207/1000 | Loss: 0.00000863
Iteration 208/1000 | Loss: 0.00000863
Iteration 209/1000 | Loss: 0.00000863
Iteration 210/1000 | Loss: 0.00000863
Iteration 211/1000 | Loss: 0.00000863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [8.629829608253203e-06, 8.629829608253203e-06, 8.629829608253203e-06, 8.629829608253203e-06, 8.629829608253203e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.629829608253203e-06

Optimization complete. Final v2v error: 2.5139126777648926 mm

Highest mean error: 2.8275845050811768 mm for frame 228

Lowest mean error: 2.3108153343200684 mm for frame 77

Saving results

Total time: 48.202505350112915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856472
Iteration 2/25 | Loss: 0.00135252
Iteration 3/25 | Loss: 0.00126911
Iteration 4/25 | Loss: 0.00125018
Iteration 5/25 | Loss: 0.00124411
Iteration 6/25 | Loss: 0.00124305
Iteration 7/25 | Loss: 0.00124305
Iteration 8/25 | Loss: 0.00124305
Iteration 9/25 | Loss: 0.00124305
Iteration 10/25 | Loss: 0.00124305
Iteration 11/25 | Loss: 0.00124305
Iteration 12/25 | Loss: 0.00124305
Iteration 13/25 | Loss: 0.00124305
Iteration 14/25 | Loss: 0.00124305
Iteration 15/25 | Loss: 0.00124305
Iteration 16/25 | Loss: 0.00124305
Iteration 17/25 | Loss: 0.00124305
Iteration 18/25 | Loss: 0.00124305
Iteration 19/25 | Loss: 0.00124305
Iteration 20/25 | Loss: 0.00124305
Iteration 21/25 | Loss: 0.00124305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012430495116859674, 0.0012430495116859674, 0.0012430495116859674, 0.0012430495116859674, 0.0012430495116859674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012430495116859674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23099661
Iteration 2/25 | Loss: 0.00166698
Iteration 3/25 | Loss: 0.00166691
Iteration 4/25 | Loss: 0.00166691
Iteration 5/25 | Loss: 0.00166691
Iteration 6/25 | Loss: 0.00166691
Iteration 7/25 | Loss: 0.00166691
Iteration 8/25 | Loss: 0.00166691
Iteration 9/25 | Loss: 0.00166691
Iteration 10/25 | Loss: 0.00166691
Iteration 11/25 | Loss: 0.00166691
Iteration 12/25 | Loss: 0.00166691
Iteration 13/25 | Loss: 0.00166691
Iteration 14/25 | Loss: 0.00166691
Iteration 15/25 | Loss: 0.00166691
Iteration 16/25 | Loss: 0.00166691
Iteration 17/25 | Loss: 0.00166691
Iteration 18/25 | Loss: 0.00166691
Iteration 19/25 | Loss: 0.00166691
Iteration 20/25 | Loss: 0.00166691
Iteration 21/25 | Loss: 0.00166691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016669066390022635, 0.0016669066390022635, 0.0016669066390022635, 0.0016669066390022635, 0.0016669066390022635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016669066390022635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166691
Iteration 2/1000 | Loss: 0.00006638
Iteration 3/1000 | Loss: 0.00003549
Iteration 4/1000 | Loss: 0.00002787
Iteration 5/1000 | Loss: 0.00002574
Iteration 6/1000 | Loss: 0.00002461
Iteration 7/1000 | Loss: 0.00002380
Iteration 8/1000 | Loss: 0.00002304
Iteration 9/1000 | Loss: 0.00002262
Iteration 10/1000 | Loss: 0.00002225
Iteration 11/1000 | Loss: 0.00002196
Iteration 12/1000 | Loss: 0.00002181
Iteration 13/1000 | Loss: 0.00002167
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002155
Iteration 16/1000 | Loss: 0.00002141
Iteration 17/1000 | Loss: 0.00002141
Iteration 18/1000 | Loss: 0.00002135
Iteration 19/1000 | Loss: 0.00002130
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002124
Iteration 22/1000 | Loss: 0.00002124
Iteration 23/1000 | Loss: 0.00002123
Iteration 24/1000 | Loss: 0.00002123
Iteration 25/1000 | Loss: 0.00002122
Iteration 26/1000 | Loss: 0.00002122
Iteration 27/1000 | Loss: 0.00002121
Iteration 28/1000 | Loss: 0.00002121
Iteration 29/1000 | Loss: 0.00002120
Iteration 30/1000 | Loss: 0.00002120
Iteration 31/1000 | Loss: 0.00002120
Iteration 32/1000 | Loss: 0.00002119
Iteration 33/1000 | Loss: 0.00002119
Iteration 34/1000 | Loss: 0.00002119
Iteration 35/1000 | Loss: 0.00002117
Iteration 36/1000 | Loss: 0.00002116
Iteration 37/1000 | Loss: 0.00002116
Iteration 38/1000 | Loss: 0.00002116
Iteration 39/1000 | Loss: 0.00002116
Iteration 40/1000 | Loss: 0.00002116
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002115
Iteration 43/1000 | Loss: 0.00002115
Iteration 44/1000 | Loss: 0.00002113
Iteration 45/1000 | Loss: 0.00002113
Iteration 46/1000 | Loss: 0.00002112
Iteration 47/1000 | Loss: 0.00002111
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002109
Iteration 51/1000 | Loss: 0.00002108
Iteration 52/1000 | Loss: 0.00002107
Iteration 53/1000 | Loss: 0.00002107
Iteration 54/1000 | Loss: 0.00002106
Iteration 55/1000 | Loss: 0.00002106
Iteration 56/1000 | Loss: 0.00002106
Iteration 57/1000 | Loss: 0.00002105
Iteration 58/1000 | Loss: 0.00002105
Iteration 59/1000 | Loss: 0.00002104
Iteration 60/1000 | Loss: 0.00002103
Iteration 61/1000 | Loss: 0.00002102
Iteration 62/1000 | Loss: 0.00002102
Iteration 63/1000 | Loss: 0.00002100
Iteration 64/1000 | Loss: 0.00002100
Iteration 65/1000 | Loss: 0.00002099
Iteration 66/1000 | Loss: 0.00002099
Iteration 67/1000 | Loss: 0.00002099
Iteration 68/1000 | Loss: 0.00002098
Iteration 69/1000 | Loss: 0.00002097
Iteration 70/1000 | Loss: 0.00002097
Iteration 71/1000 | Loss: 0.00002095
Iteration 72/1000 | Loss: 0.00002095
Iteration 73/1000 | Loss: 0.00002095
Iteration 74/1000 | Loss: 0.00002095
Iteration 75/1000 | Loss: 0.00002095
Iteration 76/1000 | Loss: 0.00002095
Iteration 77/1000 | Loss: 0.00002095
Iteration 78/1000 | Loss: 0.00002095
Iteration 79/1000 | Loss: 0.00002095
Iteration 80/1000 | Loss: 0.00002095
Iteration 81/1000 | Loss: 0.00002094
Iteration 82/1000 | Loss: 0.00002094
Iteration 83/1000 | Loss: 0.00002094
Iteration 84/1000 | Loss: 0.00002093
Iteration 85/1000 | Loss: 0.00002093
Iteration 86/1000 | Loss: 0.00002093
Iteration 87/1000 | Loss: 0.00002093
Iteration 88/1000 | Loss: 0.00002090
Iteration 89/1000 | Loss: 0.00002090
Iteration 90/1000 | Loss: 0.00002089
Iteration 91/1000 | Loss: 0.00002089
Iteration 92/1000 | Loss: 0.00002089
Iteration 93/1000 | Loss: 0.00002089
Iteration 94/1000 | Loss: 0.00002088
Iteration 95/1000 | Loss: 0.00002088
Iteration 96/1000 | Loss: 0.00002088
Iteration 97/1000 | Loss: 0.00002087
Iteration 98/1000 | Loss: 0.00002087
Iteration 99/1000 | Loss: 0.00002087
Iteration 100/1000 | Loss: 0.00002086
Iteration 101/1000 | Loss: 0.00002086
Iteration 102/1000 | Loss: 0.00002086
Iteration 103/1000 | Loss: 0.00002086
Iteration 104/1000 | Loss: 0.00002085
Iteration 105/1000 | Loss: 0.00002085
Iteration 106/1000 | Loss: 0.00002085
Iteration 107/1000 | Loss: 0.00002084
Iteration 108/1000 | Loss: 0.00002084
Iteration 109/1000 | Loss: 0.00002084
Iteration 110/1000 | Loss: 0.00002084
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002084
Iteration 113/1000 | Loss: 0.00002084
Iteration 114/1000 | Loss: 0.00002084
Iteration 115/1000 | Loss: 0.00002084
Iteration 116/1000 | Loss: 0.00002084
Iteration 117/1000 | Loss: 0.00002084
Iteration 118/1000 | Loss: 0.00002084
Iteration 119/1000 | Loss: 0.00002084
Iteration 120/1000 | Loss: 0.00002083
Iteration 121/1000 | Loss: 0.00002083
Iteration 122/1000 | Loss: 0.00002083
Iteration 123/1000 | Loss: 0.00002083
Iteration 124/1000 | Loss: 0.00002082
Iteration 125/1000 | Loss: 0.00002082
Iteration 126/1000 | Loss: 0.00002082
Iteration 127/1000 | Loss: 0.00002082
Iteration 128/1000 | Loss: 0.00002082
Iteration 129/1000 | Loss: 0.00002082
Iteration 130/1000 | Loss: 0.00002082
Iteration 131/1000 | Loss: 0.00002082
Iteration 132/1000 | Loss: 0.00002082
Iteration 133/1000 | Loss: 0.00002081
Iteration 134/1000 | Loss: 0.00002081
Iteration 135/1000 | Loss: 0.00002081
Iteration 136/1000 | Loss: 0.00002081
Iteration 137/1000 | Loss: 0.00002081
Iteration 138/1000 | Loss: 0.00002081
Iteration 139/1000 | Loss: 0.00002081
Iteration 140/1000 | Loss: 0.00002081
Iteration 141/1000 | Loss: 0.00002081
Iteration 142/1000 | Loss: 0.00002081
Iteration 143/1000 | Loss: 0.00002080
Iteration 144/1000 | Loss: 0.00002080
Iteration 145/1000 | Loss: 0.00002080
Iteration 146/1000 | Loss: 0.00002080
Iteration 147/1000 | Loss: 0.00002080
Iteration 148/1000 | Loss: 0.00002079
Iteration 149/1000 | Loss: 0.00002079
Iteration 150/1000 | Loss: 0.00002079
Iteration 151/1000 | Loss: 0.00002079
Iteration 152/1000 | Loss: 0.00002079
Iteration 153/1000 | Loss: 0.00002079
Iteration 154/1000 | Loss: 0.00002079
Iteration 155/1000 | Loss: 0.00002079
Iteration 156/1000 | Loss: 0.00002079
Iteration 157/1000 | Loss: 0.00002079
Iteration 158/1000 | Loss: 0.00002079
Iteration 159/1000 | Loss: 0.00002079
Iteration 160/1000 | Loss: 0.00002079
Iteration 161/1000 | Loss: 0.00002078
Iteration 162/1000 | Loss: 0.00002078
Iteration 163/1000 | Loss: 0.00002078
Iteration 164/1000 | Loss: 0.00002078
Iteration 165/1000 | Loss: 0.00002077
Iteration 166/1000 | Loss: 0.00002077
Iteration 167/1000 | Loss: 0.00002077
Iteration 168/1000 | Loss: 0.00002077
Iteration 169/1000 | Loss: 0.00002077
Iteration 170/1000 | Loss: 0.00002077
Iteration 171/1000 | Loss: 0.00002077
Iteration 172/1000 | Loss: 0.00002077
Iteration 173/1000 | Loss: 0.00002077
Iteration 174/1000 | Loss: 0.00002077
Iteration 175/1000 | Loss: 0.00002077
Iteration 176/1000 | Loss: 0.00002076
Iteration 177/1000 | Loss: 0.00002076
Iteration 178/1000 | Loss: 0.00002076
Iteration 179/1000 | Loss: 0.00002076
Iteration 180/1000 | Loss: 0.00002076
Iteration 181/1000 | Loss: 0.00002076
Iteration 182/1000 | Loss: 0.00002076
Iteration 183/1000 | Loss: 0.00002076
Iteration 184/1000 | Loss: 0.00002076
Iteration 185/1000 | Loss: 0.00002076
Iteration 186/1000 | Loss: 0.00002075
Iteration 187/1000 | Loss: 0.00002075
Iteration 188/1000 | Loss: 0.00002075
Iteration 189/1000 | Loss: 0.00002075
Iteration 190/1000 | Loss: 0.00002075
Iteration 191/1000 | Loss: 0.00002075
Iteration 192/1000 | Loss: 0.00002075
Iteration 193/1000 | Loss: 0.00002075
Iteration 194/1000 | Loss: 0.00002075
Iteration 195/1000 | Loss: 0.00002075
Iteration 196/1000 | Loss: 0.00002075
Iteration 197/1000 | Loss: 0.00002075
Iteration 198/1000 | Loss: 0.00002075
Iteration 199/1000 | Loss: 0.00002075
Iteration 200/1000 | Loss: 0.00002075
Iteration 201/1000 | Loss: 0.00002075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.07491684705019e-05, 2.07491684705019e-05, 2.07491684705019e-05, 2.07491684705019e-05, 2.07491684705019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.07491684705019e-05

Optimization complete. Final v2v error: 3.8625004291534424 mm

Highest mean error: 4.166600227355957 mm for frame 110

Lowest mean error: 3.4485344886779785 mm for frame 0

Saving results

Total time: 44.428380489349365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00697673
Iteration 2/25 | Loss: 0.00144064
Iteration 3/25 | Loss: 0.00123511
Iteration 4/25 | Loss: 0.00121372
Iteration 5/25 | Loss: 0.00120593
Iteration 6/25 | Loss: 0.00120464
Iteration 7/25 | Loss: 0.00120162
Iteration 8/25 | Loss: 0.00119956
Iteration 9/25 | Loss: 0.00119759
Iteration 10/25 | Loss: 0.00119612
Iteration 11/25 | Loss: 0.00119570
Iteration 12/25 | Loss: 0.00119548
Iteration 13/25 | Loss: 0.00119545
Iteration 14/25 | Loss: 0.00119545
Iteration 15/25 | Loss: 0.00119544
Iteration 16/25 | Loss: 0.00119544
Iteration 17/25 | Loss: 0.00119544
Iteration 18/25 | Loss: 0.00119544
Iteration 19/25 | Loss: 0.00119544
Iteration 20/25 | Loss: 0.00119543
Iteration 21/25 | Loss: 0.00119543
Iteration 22/25 | Loss: 0.00119543
Iteration 23/25 | Loss: 0.00119543
Iteration 24/25 | Loss: 0.00119543
Iteration 25/25 | Loss: 0.00119543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.44077206
Iteration 2/25 | Loss: 0.00136050
Iteration 3/25 | Loss: 0.00135196
Iteration 4/25 | Loss: 0.00135196
Iteration 5/25 | Loss: 0.00135196
Iteration 6/25 | Loss: 0.00135196
Iteration 7/25 | Loss: 0.00135196
Iteration 8/25 | Loss: 0.00135196
Iteration 9/25 | Loss: 0.00135196
Iteration 10/25 | Loss: 0.00135196
Iteration 11/25 | Loss: 0.00135196
Iteration 12/25 | Loss: 0.00135196
Iteration 13/25 | Loss: 0.00135195
Iteration 14/25 | Loss: 0.00135195
Iteration 15/25 | Loss: 0.00135195
Iteration 16/25 | Loss: 0.00135195
Iteration 17/25 | Loss: 0.00135195
Iteration 18/25 | Loss: 0.00135195
Iteration 19/25 | Loss: 0.00135195
Iteration 20/25 | Loss: 0.00135195
Iteration 21/25 | Loss: 0.00135195
Iteration 22/25 | Loss: 0.00135195
Iteration 23/25 | Loss: 0.00135195
Iteration 24/25 | Loss: 0.00135195
Iteration 25/25 | Loss: 0.00135195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135195
Iteration 2/1000 | Loss: 0.00004055
Iteration 3/1000 | Loss: 0.00007158
Iteration 4/1000 | Loss: 0.00005772
Iteration 5/1000 | Loss: 0.00007373
Iteration 6/1000 | Loss: 0.00005758
Iteration 7/1000 | Loss: 0.00007509
Iteration 8/1000 | Loss: 0.00005670
Iteration 9/1000 | Loss: 0.00007571
Iteration 10/1000 | Loss: 0.00005851
Iteration 11/1000 | Loss: 0.00008872
Iteration 12/1000 | Loss: 0.00005763
Iteration 13/1000 | Loss: 0.00008543
Iteration 14/1000 | Loss: 0.00005753
Iteration 15/1000 | Loss: 0.00008287
Iteration 16/1000 | Loss: 0.00005586
Iteration 17/1000 | Loss: 0.00008230
Iteration 18/1000 | Loss: 0.00005612
Iteration 19/1000 | Loss: 0.00002187
Iteration 20/1000 | Loss: 0.00006262
Iteration 21/1000 | Loss: 0.00005675
Iteration 22/1000 | Loss: 0.00006441
Iteration 23/1000 | Loss: 0.00005517
Iteration 24/1000 | Loss: 0.00005455
Iteration 25/1000 | Loss: 0.00006239
Iteration 26/1000 | Loss: 0.00006122
Iteration 27/1000 | Loss: 0.00005966
Iteration 28/1000 | Loss: 0.00011127
Iteration 29/1000 | Loss: 0.00002694
Iteration 30/1000 | Loss: 0.00002192
Iteration 31/1000 | Loss: 0.00002014
Iteration 32/1000 | Loss: 0.00001919
Iteration 33/1000 | Loss: 0.00001856
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001753
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001707
Iteration 50/1000 | Loss: 0.00001706
Iteration 51/1000 | Loss: 0.00001706
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001703
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001698
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001696
Iteration 60/1000 | Loss: 0.00001695
Iteration 61/1000 | Loss: 0.00001694
Iteration 62/1000 | Loss: 0.00001694
Iteration 63/1000 | Loss: 0.00001694
Iteration 64/1000 | Loss: 0.00001693
Iteration 65/1000 | Loss: 0.00001693
Iteration 66/1000 | Loss: 0.00001693
Iteration 67/1000 | Loss: 0.00001693
Iteration 68/1000 | Loss: 0.00001692
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001690
Iteration 72/1000 | Loss: 0.00001689
Iteration 73/1000 | Loss: 0.00001689
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001688
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001686
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001683
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001682
Iteration 82/1000 | Loss: 0.00001682
Iteration 83/1000 | Loss: 0.00001681
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001677
Iteration 87/1000 | Loss: 0.00001677
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001675
Iteration 92/1000 | Loss: 0.00001675
Iteration 93/1000 | Loss: 0.00001675
Iteration 94/1000 | Loss: 0.00001674
Iteration 95/1000 | Loss: 0.00001674
Iteration 96/1000 | Loss: 0.00001674
Iteration 97/1000 | Loss: 0.00001674
Iteration 98/1000 | Loss: 0.00001674
Iteration 99/1000 | Loss: 0.00001674
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001673
Iteration 102/1000 | Loss: 0.00001673
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001671
Iteration 107/1000 | Loss: 0.00001671
Iteration 108/1000 | Loss: 0.00001671
Iteration 109/1000 | Loss: 0.00001671
Iteration 110/1000 | Loss: 0.00001670
Iteration 111/1000 | Loss: 0.00001670
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00001669
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001668
Iteration 116/1000 | Loss: 0.00001668
Iteration 117/1000 | Loss: 0.00001667
Iteration 118/1000 | Loss: 0.00001667
Iteration 119/1000 | Loss: 0.00001667
Iteration 120/1000 | Loss: 0.00001666
Iteration 121/1000 | Loss: 0.00001666
Iteration 122/1000 | Loss: 0.00001666
Iteration 123/1000 | Loss: 0.00001665
Iteration 124/1000 | Loss: 0.00001665
Iteration 125/1000 | Loss: 0.00001665
Iteration 126/1000 | Loss: 0.00001665
Iteration 127/1000 | Loss: 0.00001665
Iteration 128/1000 | Loss: 0.00001664
Iteration 129/1000 | Loss: 0.00001664
Iteration 130/1000 | Loss: 0.00001663
Iteration 131/1000 | Loss: 0.00001663
Iteration 132/1000 | Loss: 0.00001663
Iteration 133/1000 | Loss: 0.00001663
Iteration 134/1000 | Loss: 0.00001663
Iteration 135/1000 | Loss: 0.00001663
Iteration 136/1000 | Loss: 0.00001663
Iteration 137/1000 | Loss: 0.00001663
Iteration 138/1000 | Loss: 0.00001663
Iteration 139/1000 | Loss: 0.00001662
Iteration 140/1000 | Loss: 0.00001662
Iteration 141/1000 | Loss: 0.00001662
Iteration 142/1000 | Loss: 0.00001662
Iteration 143/1000 | Loss: 0.00001662
Iteration 144/1000 | Loss: 0.00001662
Iteration 145/1000 | Loss: 0.00001662
Iteration 146/1000 | Loss: 0.00001661
Iteration 147/1000 | Loss: 0.00001661
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00001661
Iteration 150/1000 | Loss: 0.00001661
Iteration 151/1000 | Loss: 0.00001660
Iteration 152/1000 | Loss: 0.00001660
Iteration 153/1000 | Loss: 0.00001660
Iteration 154/1000 | Loss: 0.00001660
Iteration 155/1000 | Loss: 0.00001659
Iteration 156/1000 | Loss: 0.00001659
Iteration 157/1000 | Loss: 0.00001659
Iteration 158/1000 | Loss: 0.00001658
Iteration 159/1000 | Loss: 0.00001658
Iteration 160/1000 | Loss: 0.00001658
Iteration 161/1000 | Loss: 0.00001657
Iteration 162/1000 | Loss: 0.00001657
Iteration 163/1000 | Loss: 0.00001657
Iteration 164/1000 | Loss: 0.00001657
Iteration 165/1000 | Loss: 0.00001656
Iteration 166/1000 | Loss: 0.00001656
Iteration 167/1000 | Loss: 0.00001656
Iteration 168/1000 | Loss: 0.00001656
Iteration 169/1000 | Loss: 0.00001656
Iteration 170/1000 | Loss: 0.00001655
Iteration 171/1000 | Loss: 0.00001655
Iteration 172/1000 | Loss: 0.00001655
Iteration 173/1000 | Loss: 0.00001654
Iteration 174/1000 | Loss: 0.00001654
Iteration 175/1000 | Loss: 0.00001654
Iteration 176/1000 | Loss: 0.00001654
Iteration 177/1000 | Loss: 0.00001653
Iteration 178/1000 | Loss: 0.00001653
Iteration 179/1000 | Loss: 0.00001653
Iteration 180/1000 | Loss: 0.00001653
Iteration 181/1000 | Loss: 0.00001653
Iteration 182/1000 | Loss: 0.00001652
Iteration 183/1000 | Loss: 0.00001652
Iteration 184/1000 | Loss: 0.00001652
Iteration 185/1000 | Loss: 0.00001651
Iteration 186/1000 | Loss: 0.00001651
Iteration 187/1000 | Loss: 0.00001651
Iteration 188/1000 | Loss: 0.00001651
Iteration 189/1000 | Loss: 0.00001650
Iteration 190/1000 | Loss: 0.00001650
Iteration 191/1000 | Loss: 0.00001650
Iteration 192/1000 | Loss: 0.00001650
Iteration 193/1000 | Loss: 0.00001650
Iteration 194/1000 | Loss: 0.00001650
Iteration 195/1000 | Loss: 0.00001650
Iteration 196/1000 | Loss: 0.00001650
Iteration 197/1000 | Loss: 0.00001649
Iteration 198/1000 | Loss: 0.00001649
Iteration 199/1000 | Loss: 0.00001649
Iteration 200/1000 | Loss: 0.00001649
Iteration 201/1000 | Loss: 0.00001649
Iteration 202/1000 | Loss: 0.00001649
Iteration 203/1000 | Loss: 0.00001649
Iteration 204/1000 | Loss: 0.00001649
Iteration 205/1000 | Loss: 0.00001648
Iteration 206/1000 | Loss: 0.00001648
Iteration 207/1000 | Loss: 0.00001648
Iteration 208/1000 | Loss: 0.00001647
Iteration 209/1000 | Loss: 0.00001647
Iteration 210/1000 | Loss: 0.00001647
Iteration 211/1000 | Loss: 0.00001646
Iteration 212/1000 | Loss: 0.00001646
Iteration 213/1000 | Loss: 0.00001646
Iteration 214/1000 | Loss: 0.00001645
Iteration 215/1000 | Loss: 0.00001645
Iteration 216/1000 | Loss: 0.00001645
Iteration 217/1000 | Loss: 0.00001645
Iteration 218/1000 | Loss: 0.00001645
Iteration 219/1000 | Loss: 0.00001645
Iteration 220/1000 | Loss: 0.00001644
Iteration 221/1000 | Loss: 0.00001644
Iteration 222/1000 | Loss: 0.00001644
Iteration 223/1000 | Loss: 0.00001644
Iteration 224/1000 | Loss: 0.00001643
Iteration 225/1000 | Loss: 0.00001643
Iteration 226/1000 | Loss: 0.00001643
Iteration 227/1000 | Loss: 0.00001643
Iteration 228/1000 | Loss: 0.00001643
Iteration 229/1000 | Loss: 0.00001643
Iteration 230/1000 | Loss: 0.00001643
Iteration 231/1000 | Loss: 0.00001643
Iteration 232/1000 | Loss: 0.00001642
Iteration 233/1000 | Loss: 0.00001642
Iteration 234/1000 | Loss: 0.00001642
Iteration 235/1000 | Loss: 0.00001642
Iteration 236/1000 | Loss: 0.00001642
Iteration 237/1000 | Loss: 0.00001642
Iteration 238/1000 | Loss: 0.00001642
Iteration 239/1000 | Loss: 0.00001642
Iteration 240/1000 | Loss: 0.00001642
Iteration 241/1000 | Loss: 0.00001641
Iteration 242/1000 | Loss: 0.00001641
Iteration 243/1000 | Loss: 0.00001641
Iteration 244/1000 | Loss: 0.00001641
Iteration 245/1000 | Loss: 0.00001641
Iteration 246/1000 | Loss: 0.00001641
Iteration 247/1000 | Loss: 0.00001641
Iteration 248/1000 | Loss: 0.00001640
Iteration 249/1000 | Loss: 0.00001640
Iteration 250/1000 | Loss: 0.00001640
Iteration 251/1000 | Loss: 0.00001640
Iteration 252/1000 | Loss: 0.00001640
Iteration 253/1000 | Loss: 0.00001639
Iteration 254/1000 | Loss: 0.00001639
Iteration 255/1000 | Loss: 0.00001639
Iteration 256/1000 | Loss: 0.00001638
Iteration 257/1000 | Loss: 0.00001638
Iteration 258/1000 | Loss: 0.00001638
Iteration 259/1000 | Loss: 0.00002136
Iteration 260/1000 | Loss: 0.00002136
Iteration 261/1000 | Loss: 0.00001826
Iteration 262/1000 | Loss: 0.00001687
Iteration 263/1000 | Loss: 0.00001653
Iteration 264/1000 | Loss: 0.00001628
Iteration 265/1000 | Loss: 0.00001616
Iteration 266/1000 | Loss: 0.00001613
Iteration 267/1000 | Loss: 0.00001611
Iteration 268/1000 | Loss: 0.00001611
Iteration 269/1000 | Loss: 0.00001610
Iteration 270/1000 | Loss: 0.00001610
Iteration 271/1000 | Loss: 0.00001609
Iteration 272/1000 | Loss: 0.00001609
Iteration 273/1000 | Loss: 0.00001609
Iteration 274/1000 | Loss: 0.00001609
Iteration 275/1000 | Loss: 0.00001609
Iteration 276/1000 | Loss: 0.00001608
Iteration 277/1000 | Loss: 0.00001608
Iteration 278/1000 | Loss: 0.00001608
Iteration 279/1000 | Loss: 0.00001608
Iteration 280/1000 | Loss: 0.00001608
Iteration 281/1000 | Loss: 0.00001608
Iteration 282/1000 | Loss: 0.00001608
Iteration 283/1000 | Loss: 0.00001608
Iteration 284/1000 | Loss: 0.00001608
Iteration 285/1000 | Loss: 0.00001608
Iteration 286/1000 | Loss: 0.00001608
Iteration 287/1000 | Loss: 0.00001607
Iteration 288/1000 | Loss: 0.00001607
Iteration 289/1000 | Loss: 0.00001607
Iteration 290/1000 | Loss: 0.00001606
Iteration 291/1000 | Loss: 0.00001606
Iteration 292/1000 | Loss: 0.00001606
Iteration 293/1000 | Loss: 0.00001606
Iteration 294/1000 | Loss: 0.00001606
Iteration 295/1000 | Loss: 0.00001606
Iteration 296/1000 | Loss: 0.00001606
Iteration 297/1000 | Loss: 0.00001606
Iteration 298/1000 | Loss: 0.00001606
Iteration 299/1000 | Loss: 0.00001606
Iteration 300/1000 | Loss: 0.00001605
Iteration 301/1000 | Loss: 0.00001605
Iteration 302/1000 | Loss: 0.00001605
Iteration 303/1000 | Loss: 0.00001605
Iteration 304/1000 | Loss: 0.00001605
Iteration 305/1000 | Loss: 0.00001605
Iteration 306/1000 | Loss: 0.00001605
Iteration 307/1000 | Loss: 0.00001605
Iteration 308/1000 | Loss: 0.00001605
Iteration 309/1000 | Loss: 0.00001605
Iteration 310/1000 | Loss: 0.00001605
Iteration 311/1000 | Loss: 0.00001605
Iteration 312/1000 | Loss: 0.00001605
Iteration 313/1000 | Loss: 0.00001605
Iteration 314/1000 | Loss: 0.00001605
Iteration 315/1000 | Loss: 0.00001605
Iteration 316/1000 | Loss: 0.00001605
Iteration 317/1000 | Loss: 0.00001604
Iteration 318/1000 | Loss: 0.00001604
Iteration 319/1000 | Loss: 0.00001604
Iteration 320/1000 | Loss: 0.00001604
Iteration 321/1000 | Loss: 0.00001604
Iteration 322/1000 | Loss: 0.00001604
Iteration 323/1000 | Loss: 0.00001604
Iteration 324/1000 | Loss: 0.00001604
Iteration 325/1000 | Loss: 0.00001604
Iteration 326/1000 | Loss: 0.00001604
Iteration 327/1000 | Loss: 0.00001604
Iteration 328/1000 | Loss: 0.00001604
Iteration 329/1000 | Loss: 0.00001604
Iteration 330/1000 | Loss: 0.00001604
Iteration 331/1000 | Loss: 0.00001604
Iteration 332/1000 | Loss: 0.00001604
Iteration 333/1000 | Loss: 0.00001603
Iteration 334/1000 | Loss: 0.00001603
Iteration 335/1000 | Loss: 0.00001603
Iteration 336/1000 | Loss: 0.00001603
Iteration 337/1000 | Loss: 0.00001603
Iteration 338/1000 | Loss: 0.00001603
Iteration 339/1000 | Loss: 0.00001603
Iteration 340/1000 | Loss: 0.00001603
Iteration 341/1000 | Loss: 0.00001603
Iteration 342/1000 | Loss: 0.00001603
Iteration 343/1000 | Loss: 0.00001603
Iteration 344/1000 | Loss: 0.00001603
Iteration 345/1000 | Loss: 0.00001603
Iteration 346/1000 | Loss: 0.00001603
Iteration 347/1000 | Loss: 0.00001602
Iteration 348/1000 | Loss: 0.00001602
Iteration 349/1000 | Loss: 0.00001602
Iteration 350/1000 | Loss: 0.00001602
Iteration 351/1000 | Loss: 0.00001602
Iteration 352/1000 | Loss: 0.00001602
Iteration 353/1000 | Loss: 0.00001602
Iteration 354/1000 | Loss: 0.00001602
Iteration 355/1000 | Loss: 0.00001602
Iteration 356/1000 | Loss: 0.00001602
Iteration 357/1000 | Loss: 0.00001602
Iteration 358/1000 | Loss: 0.00001602
Iteration 359/1000 | Loss: 0.00001602
Iteration 360/1000 | Loss: 0.00001602
Iteration 361/1000 | Loss: 0.00001602
Iteration 362/1000 | Loss: 0.00001601
Iteration 363/1000 | Loss: 0.00001601
Iteration 364/1000 | Loss: 0.00001601
Iteration 365/1000 | Loss: 0.00001601
Iteration 366/1000 | Loss: 0.00001601
Iteration 367/1000 | Loss: 0.00001601
Iteration 368/1000 | Loss: 0.00001601
Iteration 369/1000 | Loss: 0.00001601
Iteration 370/1000 | Loss: 0.00001601
Iteration 371/1000 | Loss: 0.00001601
Iteration 372/1000 | Loss: 0.00001601
Iteration 373/1000 | Loss: 0.00001601
Iteration 374/1000 | Loss: 0.00001601
Iteration 375/1000 | Loss: 0.00001601
Iteration 376/1000 | Loss: 0.00001601
Iteration 377/1000 | Loss: 0.00001601
Iteration 378/1000 | Loss: 0.00001601
Iteration 379/1000 | Loss: 0.00001601
Iteration 380/1000 | Loss: 0.00001601
Iteration 381/1000 | Loss: 0.00001601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 381. Stopping optimization.
Last 5 losses: [1.6010289982659742e-05, 1.6010289982659742e-05, 1.6010289982659742e-05, 1.6010289982659742e-05, 1.6010289982659742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6010289982659742e-05

Optimization complete. Final v2v error: 3.299419403076172 mm

Highest mean error: 5.3955397605896 mm for frame 88

Lowest mean error: 2.468217611312866 mm for frame 197

Saving results

Total time: 125.90759778022766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774234
Iteration 2/25 | Loss: 0.00152462
Iteration 3/25 | Loss: 0.00125491
Iteration 4/25 | Loss: 0.00122307
Iteration 5/25 | Loss: 0.00121869
Iteration 6/25 | Loss: 0.00121758
Iteration 7/25 | Loss: 0.00121725
Iteration 8/25 | Loss: 0.00121716
Iteration 9/25 | Loss: 0.00121716
Iteration 10/25 | Loss: 0.00121716
Iteration 11/25 | Loss: 0.00121716
Iteration 12/25 | Loss: 0.00121716
Iteration 13/25 | Loss: 0.00121716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012171596754342318, 0.0012171596754342318, 0.0012171596754342318, 0.0012171596754342318, 0.0012171596754342318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012171596754342318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23430192
Iteration 2/25 | Loss: 0.00126749
Iteration 3/25 | Loss: 0.00126747
Iteration 4/25 | Loss: 0.00126747
Iteration 5/25 | Loss: 0.00126747
Iteration 6/25 | Loss: 0.00126747
Iteration 7/25 | Loss: 0.00126747
Iteration 8/25 | Loss: 0.00126747
Iteration 9/25 | Loss: 0.00126747
Iteration 10/25 | Loss: 0.00126747
Iteration 11/25 | Loss: 0.00126747
Iteration 12/25 | Loss: 0.00126747
Iteration 13/25 | Loss: 0.00126747
Iteration 14/25 | Loss: 0.00126747
Iteration 15/25 | Loss: 0.00126747
Iteration 16/25 | Loss: 0.00126747
Iteration 17/25 | Loss: 0.00126747
Iteration 18/25 | Loss: 0.00126747
Iteration 19/25 | Loss: 0.00126747
Iteration 20/25 | Loss: 0.00126747
Iteration 21/25 | Loss: 0.00126747
Iteration 22/25 | Loss: 0.00126747
Iteration 23/25 | Loss: 0.00126747
Iteration 24/25 | Loss: 0.00126747
Iteration 25/25 | Loss: 0.00126747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126747
Iteration 2/1000 | Loss: 0.00005760
Iteration 3/1000 | Loss: 0.00003207
Iteration 4/1000 | Loss: 0.00002752
Iteration 5/1000 | Loss: 0.00002516
Iteration 6/1000 | Loss: 0.00002418
Iteration 7/1000 | Loss: 0.00003521
Iteration 8/1000 | Loss: 0.00002441
Iteration 9/1000 | Loss: 0.00002323
Iteration 10/1000 | Loss: 0.00002215
Iteration 11/1000 | Loss: 0.00002161
Iteration 12/1000 | Loss: 0.00002137
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002081
Iteration 15/1000 | Loss: 0.00002052
Iteration 16/1000 | Loss: 0.00002029
Iteration 17/1000 | Loss: 0.00002011
Iteration 18/1000 | Loss: 0.00001997
Iteration 19/1000 | Loss: 0.00001995
Iteration 20/1000 | Loss: 0.00001982
Iteration 21/1000 | Loss: 0.00001978
Iteration 22/1000 | Loss: 0.00001973
Iteration 23/1000 | Loss: 0.00001973
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001969
Iteration 26/1000 | Loss: 0.00001966
Iteration 27/1000 | Loss: 0.00001966
Iteration 28/1000 | Loss: 0.00001965
Iteration 29/1000 | Loss: 0.00001964
Iteration 30/1000 | Loss: 0.00001964
Iteration 31/1000 | Loss: 0.00001963
Iteration 32/1000 | Loss: 0.00001963
Iteration 33/1000 | Loss: 0.00001963
Iteration 34/1000 | Loss: 0.00001962
Iteration 35/1000 | Loss: 0.00001962
Iteration 36/1000 | Loss: 0.00001962
Iteration 37/1000 | Loss: 0.00001962
Iteration 38/1000 | Loss: 0.00001962
Iteration 39/1000 | Loss: 0.00001962
Iteration 40/1000 | Loss: 0.00001962
Iteration 41/1000 | Loss: 0.00001962
Iteration 42/1000 | Loss: 0.00001961
Iteration 43/1000 | Loss: 0.00001961
Iteration 44/1000 | Loss: 0.00001961
Iteration 45/1000 | Loss: 0.00001961
Iteration 46/1000 | Loss: 0.00001960
Iteration 47/1000 | Loss: 0.00001960
Iteration 48/1000 | Loss: 0.00001960
Iteration 49/1000 | Loss: 0.00001960
Iteration 50/1000 | Loss: 0.00001960
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001959
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001957
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001956
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001955
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001955
Iteration 66/1000 | Loss: 0.00001954
Iteration 67/1000 | Loss: 0.00001954
Iteration 68/1000 | Loss: 0.00001954
Iteration 69/1000 | Loss: 0.00001954
Iteration 70/1000 | Loss: 0.00001954
Iteration 71/1000 | Loss: 0.00001954
Iteration 72/1000 | Loss: 0.00001954
Iteration 73/1000 | Loss: 0.00001953
Iteration 74/1000 | Loss: 0.00001953
Iteration 75/1000 | Loss: 0.00001953
Iteration 76/1000 | Loss: 0.00001953
Iteration 77/1000 | Loss: 0.00001953
Iteration 78/1000 | Loss: 0.00001953
Iteration 79/1000 | Loss: 0.00001953
Iteration 80/1000 | Loss: 0.00001953
Iteration 81/1000 | Loss: 0.00001953
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001952
Iteration 84/1000 | Loss: 0.00001952
Iteration 85/1000 | Loss: 0.00001952
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001951
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001951
Iteration 97/1000 | Loss: 0.00001951
Iteration 98/1000 | Loss: 0.00001951
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001950
Iteration 101/1000 | Loss: 0.00001950
Iteration 102/1000 | Loss: 0.00001950
Iteration 103/1000 | Loss: 0.00001950
Iteration 104/1000 | Loss: 0.00001950
Iteration 105/1000 | Loss: 0.00001950
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00001950
Iteration 108/1000 | Loss: 0.00001950
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001950
Iteration 111/1000 | Loss: 0.00001950
Iteration 112/1000 | Loss: 0.00001950
Iteration 113/1000 | Loss: 0.00001950
Iteration 114/1000 | Loss: 0.00001949
Iteration 115/1000 | Loss: 0.00001949
Iteration 116/1000 | Loss: 0.00001949
Iteration 117/1000 | Loss: 0.00001949
Iteration 118/1000 | Loss: 0.00001949
Iteration 119/1000 | Loss: 0.00001949
Iteration 120/1000 | Loss: 0.00001949
Iteration 121/1000 | Loss: 0.00001949
Iteration 122/1000 | Loss: 0.00001949
Iteration 123/1000 | Loss: 0.00001949
Iteration 124/1000 | Loss: 0.00001948
Iteration 125/1000 | Loss: 0.00001948
Iteration 126/1000 | Loss: 0.00001948
Iteration 127/1000 | Loss: 0.00001948
Iteration 128/1000 | Loss: 0.00001948
Iteration 129/1000 | Loss: 0.00001947
Iteration 130/1000 | Loss: 0.00001947
Iteration 131/1000 | Loss: 0.00001947
Iteration 132/1000 | Loss: 0.00001947
Iteration 133/1000 | Loss: 0.00001947
Iteration 134/1000 | Loss: 0.00001947
Iteration 135/1000 | Loss: 0.00001947
Iteration 136/1000 | Loss: 0.00001947
Iteration 137/1000 | Loss: 0.00001947
Iteration 138/1000 | Loss: 0.00001947
Iteration 139/1000 | Loss: 0.00001947
Iteration 140/1000 | Loss: 0.00001947
Iteration 141/1000 | Loss: 0.00001947
Iteration 142/1000 | Loss: 0.00001947
Iteration 143/1000 | Loss: 0.00001947
Iteration 144/1000 | Loss: 0.00001946
Iteration 145/1000 | Loss: 0.00001946
Iteration 146/1000 | Loss: 0.00001946
Iteration 147/1000 | Loss: 0.00001946
Iteration 148/1000 | Loss: 0.00001946
Iteration 149/1000 | Loss: 0.00001946
Iteration 150/1000 | Loss: 0.00001946
Iteration 151/1000 | Loss: 0.00001946
Iteration 152/1000 | Loss: 0.00001945
Iteration 153/1000 | Loss: 0.00001945
Iteration 154/1000 | Loss: 0.00001945
Iteration 155/1000 | Loss: 0.00001945
Iteration 156/1000 | Loss: 0.00001945
Iteration 157/1000 | Loss: 0.00001945
Iteration 158/1000 | Loss: 0.00001945
Iteration 159/1000 | Loss: 0.00001945
Iteration 160/1000 | Loss: 0.00001945
Iteration 161/1000 | Loss: 0.00001944
Iteration 162/1000 | Loss: 0.00001944
Iteration 163/1000 | Loss: 0.00001944
Iteration 164/1000 | Loss: 0.00001944
Iteration 165/1000 | Loss: 0.00001944
Iteration 166/1000 | Loss: 0.00001944
Iteration 167/1000 | Loss: 0.00001944
Iteration 168/1000 | Loss: 0.00001944
Iteration 169/1000 | Loss: 0.00001944
Iteration 170/1000 | Loss: 0.00001944
Iteration 171/1000 | Loss: 0.00001944
Iteration 172/1000 | Loss: 0.00001944
Iteration 173/1000 | Loss: 0.00001944
Iteration 174/1000 | Loss: 0.00001944
Iteration 175/1000 | Loss: 0.00001944
Iteration 176/1000 | Loss: 0.00001944
Iteration 177/1000 | Loss: 0.00001944
Iteration 178/1000 | Loss: 0.00001943
Iteration 179/1000 | Loss: 0.00001943
Iteration 180/1000 | Loss: 0.00001943
Iteration 181/1000 | Loss: 0.00001943
Iteration 182/1000 | Loss: 0.00001943
Iteration 183/1000 | Loss: 0.00001943
Iteration 184/1000 | Loss: 0.00001943
Iteration 185/1000 | Loss: 0.00001943
Iteration 186/1000 | Loss: 0.00001943
Iteration 187/1000 | Loss: 0.00001943
Iteration 188/1000 | Loss: 0.00001943
Iteration 189/1000 | Loss: 0.00001943
Iteration 190/1000 | Loss: 0.00001943
Iteration 191/1000 | Loss: 0.00001943
Iteration 192/1000 | Loss: 0.00001942
Iteration 193/1000 | Loss: 0.00001942
Iteration 194/1000 | Loss: 0.00001942
Iteration 195/1000 | Loss: 0.00001942
Iteration 196/1000 | Loss: 0.00001942
Iteration 197/1000 | Loss: 0.00001942
Iteration 198/1000 | Loss: 0.00001942
Iteration 199/1000 | Loss: 0.00001942
Iteration 200/1000 | Loss: 0.00001942
Iteration 201/1000 | Loss: 0.00001942
Iteration 202/1000 | Loss: 0.00001942
Iteration 203/1000 | Loss: 0.00001942
Iteration 204/1000 | Loss: 0.00001942
Iteration 205/1000 | Loss: 0.00001942
Iteration 206/1000 | Loss: 0.00001942
Iteration 207/1000 | Loss: 0.00001942
Iteration 208/1000 | Loss: 0.00001942
Iteration 209/1000 | Loss: 0.00001942
Iteration 210/1000 | Loss: 0.00001942
Iteration 211/1000 | Loss: 0.00001942
Iteration 212/1000 | Loss: 0.00001942
Iteration 213/1000 | Loss: 0.00001942
Iteration 214/1000 | Loss: 0.00001942
Iteration 215/1000 | Loss: 0.00001942
Iteration 216/1000 | Loss: 0.00001942
Iteration 217/1000 | Loss: 0.00001942
Iteration 218/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.9417422663536854e-05, 1.9417422663536854e-05, 1.9417422663536854e-05, 1.9417422663536854e-05, 1.9417422663536854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9417422663536854e-05

Optimization complete. Final v2v error: 3.6652157306671143 mm

Highest mean error: 7.46297025680542 mm for frame 65

Lowest mean error: 2.7909247875213623 mm for frame 123

Saving results

Total time: 61.005537271499634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959431
Iteration 2/25 | Loss: 0.00222102
Iteration 3/25 | Loss: 0.00164861
Iteration 4/25 | Loss: 0.00155708
Iteration 5/25 | Loss: 0.00145559
Iteration 6/25 | Loss: 0.00144938
Iteration 7/25 | Loss: 0.00134337
Iteration 8/25 | Loss: 0.00131229
Iteration 9/25 | Loss: 0.00130511
Iteration 10/25 | Loss: 0.00130783
Iteration 11/25 | Loss: 0.00129761
Iteration 12/25 | Loss: 0.00129419
Iteration 13/25 | Loss: 0.00128751
Iteration 14/25 | Loss: 0.00128302
Iteration 15/25 | Loss: 0.00128029
Iteration 16/25 | Loss: 0.00127974
Iteration 17/25 | Loss: 0.00127965
Iteration 18/25 | Loss: 0.00127965
Iteration 19/25 | Loss: 0.00127965
Iteration 20/25 | Loss: 0.00127965
Iteration 21/25 | Loss: 0.00127965
Iteration 22/25 | Loss: 0.00127965
Iteration 23/25 | Loss: 0.00127965
Iteration 24/25 | Loss: 0.00127965
Iteration 25/25 | Loss: 0.00127964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24100924
Iteration 2/25 | Loss: 0.00118039
Iteration 3/25 | Loss: 0.00118039
Iteration 4/25 | Loss: 0.00118039
Iteration 5/25 | Loss: 0.00118038
Iteration 6/25 | Loss: 0.00118038
Iteration 7/25 | Loss: 0.00118038
Iteration 8/25 | Loss: 0.00118038
Iteration 9/25 | Loss: 0.00118038
Iteration 10/25 | Loss: 0.00118038
Iteration 11/25 | Loss: 0.00118038
Iteration 12/25 | Loss: 0.00118038
Iteration 13/25 | Loss: 0.00118038
Iteration 14/25 | Loss: 0.00118038
Iteration 15/25 | Loss: 0.00118038
Iteration 16/25 | Loss: 0.00118038
Iteration 17/25 | Loss: 0.00118038
Iteration 18/25 | Loss: 0.00118038
Iteration 19/25 | Loss: 0.00118038
Iteration 20/25 | Loss: 0.00118038
Iteration 21/25 | Loss: 0.00118038
Iteration 22/25 | Loss: 0.00118038
Iteration 23/25 | Loss: 0.00118038
Iteration 24/25 | Loss: 0.00118038
Iteration 25/25 | Loss: 0.00118038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118038
Iteration 2/1000 | Loss: 0.00005784
Iteration 3/1000 | Loss: 0.00004083
Iteration 4/1000 | Loss: 0.00003295
Iteration 5/1000 | Loss: 0.00003045
Iteration 6/1000 | Loss: 0.00002894
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002720
Iteration 9/1000 | Loss: 0.00002665
Iteration 10/1000 | Loss: 0.00002616
Iteration 11/1000 | Loss: 0.00002581
Iteration 12/1000 | Loss: 0.00002581
Iteration 13/1000 | Loss: 0.00002560
Iteration 14/1000 | Loss: 0.00002558
Iteration 15/1000 | Loss: 0.00002554
Iteration 16/1000 | Loss: 0.00002535
Iteration 17/1000 | Loss: 0.00002527
Iteration 18/1000 | Loss: 0.00002520
Iteration 19/1000 | Loss: 0.00002498
Iteration 20/1000 | Loss: 0.00002494
Iteration 21/1000 | Loss: 0.00002491
Iteration 22/1000 | Loss: 0.00002490
Iteration 23/1000 | Loss: 0.00002490
Iteration 24/1000 | Loss: 0.00002489
Iteration 25/1000 | Loss: 0.00002488
Iteration 26/1000 | Loss: 0.00002484
Iteration 27/1000 | Loss: 0.00002482
Iteration 28/1000 | Loss: 0.00002481
Iteration 29/1000 | Loss: 0.00002481
Iteration 30/1000 | Loss: 0.00002481
Iteration 31/1000 | Loss: 0.00002480
Iteration 32/1000 | Loss: 0.00002480
Iteration 33/1000 | Loss: 0.00002479
Iteration 34/1000 | Loss: 0.00002479
Iteration 35/1000 | Loss: 0.00002478
Iteration 36/1000 | Loss: 0.00002478
Iteration 37/1000 | Loss: 0.00002477
Iteration 38/1000 | Loss: 0.00002477
Iteration 39/1000 | Loss: 0.00002477
Iteration 40/1000 | Loss: 0.00002477
Iteration 41/1000 | Loss: 0.00002477
Iteration 42/1000 | Loss: 0.00002477
Iteration 43/1000 | Loss: 0.00002476
Iteration 44/1000 | Loss: 0.00002476
Iteration 45/1000 | Loss: 0.00002476
Iteration 46/1000 | Loss: 0.00002476
Iteration 47/1000 | Loss: 0.00002476
Iteration 48/1000 | Loss: 0.00002475
Iteration 49/1000 | Loss: 0.00002475
Iteration 50/1000 | Loss: 0.00002474
Iteration 51/1000 | Loss: 0.00002474
Iteration 52/1000 | Loss: 0.00002473
Iteration 53/1000 | Loss: 0.00002472
Iteration 54/1000 | Loss: 0.00002472
Iteration 55/1000 | Loss: 0.00002472
Iteration 56/1000 | Loss: 0.00002471
Iteration 57/1000 | Loss: 0.00002471
Iteration 58/1000 | Loss: 0.00002471
Iteration 59/1000 | Loss: 0.00002471
Iteration 60/1000 | Loss: 0.00002470
Iteration 61/1000 | Loss: 0.00002470
Iteration 62/1000 | Loss: 0.00002470
Iteration 63/1000 | Loss: 0.00002470
Iteration 64/1000 | Loss: 0.00002470
Iteration 65/1000 | Loss: 0.00002470
Iteration 66/1000 | Loss: 0.00002469
Iteration 67/1000 | Loss: 0.00002469
Iteration 68/1000 | Loss: 0.00002469
Iteration 69/1000 | Loss: 0.00002468
Iteration 70/1000 | Loss: 0.00002468
Iteration 71/1000 | Loss: 0.00002468
Iteration 72/1000 | Loss: 0.00002468
Iteration 73/1000 | Loss: 0.00002467
Iteration 74/1000 | Loss: 0.00002467
Iteration 75/1000 | Loss: 0.00002467
Iteration 76/1000 | Loss: 0.00002467
Iteration 77/1000 | Loss: 0.00002467
Iteration 78/1000 | Loss: 0.00002467
Iteration 79/1000 | Loss: 0.00002467
Iteration 80/1000 | Loss: 0.00002467
Iteration 81/1000 | Loss: 0.00002467
Iteration 82/1000 | Loss: 0.00002467
Iteration 83/1000 | Loss: 0.00002467
Iteration 84/1000 | Loss: 0.00002466
Iteration 85/1000 | Loss: 0.00002466
Iteration 86/1000 | Loss: 0.00002466
Iteration 87/1000 | Loss: 0.00002466
Iteration 88/1000 | Loss: 0.00002466
Iteration 89/1000 | Loss: 0.00002465
Iteration 90/1000 | Loss: 0.00002465
Iteration 91/1000 | Loss: 0.00002465
Iteration 92/1000 | Loss: 0.00002465
Iteration 93/1000 | Loss: 0.00002465
Iteration 94/1000 | Loss: 0.00002464
Iteration 95/1000 | Loss: 0.00002464
Iteration 96/1000 | Loss: 0.00002464
Iteration 97/1000 | Loss: 0.00002463
Iteration 98/1000 | Loss: 0.00002463
Iteration 99/1000 | Loss: 0.00002463
Iteration 100/1000 | Loss: 0.00002463
Iteration 101/1000 | Loss: 0.00002462
Iteration 102/1000 | Loss: 0.00002462
Iteration 103/1000 | Loss: 0.00002462
Iteration 104/1000 | Loss: 0.00002462
Iteration 105/1000 | Loss: 0.00002462
Iteration 106/1000 | Loss: 0.00002462
Iteration 107/1000 | Loss: 0.00002461
Iteration 108/1000 | Loss: 0.00002461
Iteration 109/1000 | Loss: 0.00002461
Iteration 110/1000 | Loss: 0.00002461
Iteration 111/1000 | Loss: 0.00002461
Iteration 112/1000 | Loss: 0.00002461
Iteration 113/1000 | Loss: 0.00002461
Iteration 114/1000 | Loss: 0.00002461
Iteration 115/1000 | Loss: 0.00002461
Iteration 116/1000 | Loss: 0.00002461
Iteration 117/1000 | Loss: 0.00002461
Iteration 118/1000 | Loss: 0.00002461
Iteration 119/1000 | Loss: 0.00002461
Iteration 120/1000 | Loss: 0.00002461
Iteration 121/1000 | Loss: 0.00002461
Iteration 122/1000 | Loss: 0.00002461
Iteration 123/1000 | Loss: 0.00002461
Iteration 124/1000 | Loss: 0.00002461
Iteration 125/1000 | Loss: 0.00002461
Iteration 126/1000 | Loss: 0.00002461
Iteration 127/1000 | Loss: 0.00002461
Iteration 128/1000 | Loss: 0.00002461
Iteration 129/1000 | Loss: 0.00002461
Iteration 130/1000 | Loss: 0.00002461
Iteration 131/1000 | Loss: 0.00002461
Iteration 132/1000 | Loss: 0.00002461
Iteration 133/1000 | Loss: 0.00002461
Iteration 134/1000 | Loss: 0.00002461
Iteration 135/1000 | Loss: 0.00002461
Iteration 136/1000 | Loss: 0.00002461
Iteration 137/1000 | Loss: 0.00002461
Iteration 138/1000 | Loss: 0.00002461
Iteration 139/1000 | Loss: 0.00002461
Iteration 140/1000 | Loss: 0.00002461
Iteration 141/1000 | Loss: 0.00002461
Iteration 142/1000 | Loss: 0.00002461
Iteration 143/1000 | Loss: 0.00002461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.460835275996942e-05, 2.460835275996942e-05, 2.460835275996942e-05, 2.460835275996942e-05, 2.460835275996942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.460835275996942e-05

Optimization complete. Final v2v error: 3.7669687271118164 mm

Highest mean error: 11.564166069030762 mm for frame 45

Lowest mean error: 3.3835766315460205 mm for frame 55

Saving results

Total time: 59.63729929924011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526983
Iteration 2/25 | Loss: 0.00133947
Iteration 3/25 | Loss: 0.00121625
Iteration 4/25 | Loss: 0.00119941
Iteration 5/25 | Loss: 0.00119210
Iteration 6/25 | Loss: 0.00119054
Iteration 7/25 | Loss: 0.00119054
Iteration 8/25 | Loss: 0.00119054
Iteration 9/25 | Loss: 0.00119054
Iteration 10/25 | Loss: 0.00119054
Iteration 11/25 | Loss: 0.00119054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011905430583283305, 0.0011905430583283305, 0.0011905430583283305, 0.0011905430583283305, 0.0011905430583283305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011905430583283305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72402877
Iteration 2/25 | Loss: 0.00101391
Iteration 3/25 | Loss: 0.00101391
Iteration 4/25 | Loss: 0.00101391
Iteration 5/25 | Loss: 0.00101391
Iteration 6/25 | Loss: 0.00101391
Iteration 7/25 | Loss: 0.00101391
Iteration 8/25 | Loss: 0.00101391
Iteration 9/25 | Loss: 0.00101391
Iteration 10/25 | Loss: 0.00101391
Iteration 11/25 | Loss: 0.00101391
Iteration 12/25 | Loss: 0.00101391
Iteration 13/25 | Loss: 0.00101391
Iteration 14/25 | Loss: 0.00101391
Iteration 15/25 | Loss: 0.00101391
Iteration 16/25 | Loss: 0.00101391
Iteration 17/25 | Loss: 0.00101391
Iteration 18/25 | Loss: 0.00101391
Iteration 19/25 | Loss: 0.00101391
Iteration 20/25 | Loss: 0.00101391
Iteration 21/25 | Loss: 0.00101391
Iteration 22/25 | Loss: 0.00101391
Iteration 23/25 | Loss: 0.00101391
Iteration 24/25 | Loss: 0.00101391
Iteration 25/25 | Loss: 0.00101391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101391
Iteration 2/1000 | Loss: 0.00003157
Iteration 3/1000 | Loss: 0.00002509
Iteration 4/1000 | Loss: 0.00002320
Iteration 5/1000 | Loss: 0.00002243
Iteration 6/1000 | Loss: 0.00002192
Iteration 7/1000 | Loss: 0.00002152
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00002073
Iteration 10/1000 | Loss: 0.00002041
Iteration 11/1000 | Loss: 0.00002013
Iteration 12/1000 | Loss: 0.00001983
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001917
Iteration 18/1000 | Loss: 0.00001903
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001875
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001874
Iteration 24/1000 | Loss: 0.00001873
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001873
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001871
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001870
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001869
Iteration 40/1000 | Loss: 0.00001869
Iteration 41/1000 | Loss: 0.00001868
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001862
Iteration 44/1000 | Loss: 0.00001862
Iteration 45/1000 | Loss: 0.00001861
Iteration 46/1000 | Loss: 0.00001861
Iteration 47/1000 | Loss: 0.00001861
Iteration 48/1000 | Loss: 0.00001860
Iteration 49/1000 | Loss: 0.00001860
Iteration 50/1000 | Loss: 0.00001860
Iteration 51/1000 | Loss: 0.00001859
Iteration 52/1000 | Loss: 0.00001859
Iteration 53/1000 | Loss: 0.00001859
Iteration 54/1000 | Loss: 0.00001858
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001857
Iteration 58/1000 | Loss: 0.00001857
Iteration 59/1000 | Loss: 0.00001857
Iteration 60/1000 | Loss: 0.00001857
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001855
Iteration 70/1000 | Loss: 0.00001855
Iteration 71/1000 | Loss: 0.00001855
Iteration 72/1000 | Loss: 0.00001855
Iteration 73/1000 | Loss: 0.00001855
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001854
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001854
Iteration 83/1000 | Loss: 0.00001854
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001853
Iteration 88/1000 | Loss: 0.00001853
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001852
Iteration 91/1000 | Loss: 0.00001852
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001849
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001848
Iteration 107/1000 | Loss: 0.00001848
Iteration 108/1000 | Loss: 0.00001848
Iteration 109/1000 | Loss: 0.00001848
Iteration 110/1000 | Loss: 0.00001848
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Iteration 113/1000 | Loss: 0.00001847
Iteration 114/1000 | Loss: 0.00001847
Iteration 115/1000 | Loss: 0.00001847
Iteration 116/1000 | Loss: 0.00001847
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00001846
Iteration 119/1000 | Loss: 0.00001846
Iteration 120/1000 | Loss: 0.00001846
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001846
Iteration 123/1000 | Loss: 0.00001846
Iteration 124/1000 | Loss: 0.00001846
Iteration 125/1000 | Loss: 0.00001846
Iteration 126/1000 | Loss: 0.00001846
Iteration 127/1000 | Loss: 0.00001846
Iteration 128/1000 | Loss: 0.00001846
Iteration 129/1000 | Loss: 0.00001846
Iteration 130/1000 | Loss: 0.00001846
Iteration 131/1000 | Loss: 0.00001846
Iteration 132/1000 | Loss: 0.00001846
Iteration 133/1000 | Loss: 0.00001846
Iteration 134/1000 | Loss: 0.00001846
Iteration 135/1000 | Loss: 0.00001845
Iteration 136/1000 | Loss: 0.00001845
Iteration 137/1000 | Loss: 0.00001845
Iteration 138/1000 | Loss: 0.00001845
Iteration 139/1000 | Loss: 0.00001845
Iteration 140/1000 | Loss: 0.00001845
Iteration 141/1000 | Loss: 0.00001845
Iteration 142/1000 | Loss: 0.00001845
Iteration 143/1000 | Loss: 0.00001845
Iteration 144/1000 | Loss: 0.00001845
Iteration 145/1000 | Loss: 0.00001845
Iteration 146/1000 | Loss: 0.00001845
Iteration 147/1000 | Loss: 0.00001845
Iteration 148/1000 | Loss: 0.00001845
Iteration 149/1000 | Loss: 0.00001845
Iteration 150/1000 | Loss: 0.00001845
Iteration 151/1000 | Loss: 0.00001844
Iteration 152/1000 | Loss: 0.00001844
Iteration 153/1000 | Loss: 0.00001844
Iteration 154/1000 | Loss: 0.00001844
Iteration 155/1000 | Loss: 0.00001844
Iteration 156/1000 | Loss: 0.00001844
Iteration 157/1000 | Loss: 0.00001844
Iteration 158/1000 | Loss: 0.00001844
Iteration 159/1000 | Loss: 0.00001844
Iteration 160/1000 | Loss: 0.00001844
Iteration 161/1000 | Loss: 0.00001844
Iteration 162/1000 | Loss: 0.00001844
Iteration 163/1000 | Loss: 0.00001844
Iteration 164/1000 | Loss: 0.00001843
Iteration 165/1000 | Loss: 0.00001843
Iteration 166/1000 | Loss: 0.00001843
Iteration 167/1000 | Loss: 0.00001843
Iteration 168/1000 | Loss: 0.00001843
Iteration 169/1000 | Loss: 0.00001843
Iteration 170/1000 | Loss: 0.00001843
Iteration 171/1000 | Loss: 0.00001843
Iteration 172/1000 | Loss: 0.00001843
Iteration 173/1000 | Loss: 0.00001843
Iteration 174/1000 | Loss: 0.00001843
Iteration 175/1000 | Loss: 0.00001843
Iteration 176/1000 | Loss: 0.00001843
Iteration 177/1000 | Loss: 0.00001843
Iteration 178/1000 | Loss: 0.00001843
Iteration 179/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.8433822333463468e-05, 1.8433822333463468e-05, 1.8433822333463468e-05, 1.8433822333463468e-05, 1.8433822333463468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8433822333463468e-05

Optimization complete. Final v2v error: 3.656064033508301 mm

Highest mean error: 4.188450813293457 mm for frame 266

Lowest mean error: 3.5955910682678223 mm for frame 59

Saving results

Total time: 53.06895399093628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500028
Iteration 2/25 | Loss: 0.00135479
Iteration 3/25 | Loss: 0.00123251
Iteration 4/25 | Loss: 0.00122007
Iteration 5/25 | Loss: 0.00121742
Iteration 6/25 | Loss: 0.00121657
Iteration 7/25 | Loss: 0.00121657
Iteration 8/25 | Loss: 0.00121657
Iteration 9/25 | Loss: 0.00121657
Iteration 10/25 | Loss: 0.00121657
Iteration 11/25 | Loss: 0.00121657
Iteration 12/25 | Loss: 0.00121657
Iteration 13/25 | Loss: 0.00121657
Iteration 14/25 | Loss: 0.00121657
Iteration 15/25 | Loss: 0.00121657
Iteration 16/25 | Loss: 0.00121657
Iteration 17/25 | Loss: 0.00121657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012165678199380636, 0.0012165678199380636, 0.0012165678199380636, 0.0012165678199380636, 0.0012165678199380636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012165678199380636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35866392
Iteration 2/25 | Loss: 0.00119800
Iteration 3/25 | Loss: 0.00119795
Iteration 4/25 | Loss: 0.00119795
Iteration 5/25 | Loss: 0.00119795
Iteration 6/25 | Loss: 0.00119795
Iteration 7/25 | Loss: 0.00119795
Iteration 8/25 | Loss: 0.00119795
Iteration 9/25 | Loss: 0.00119795
Iteration 10/25 | Loss: 0.00119795
Iteration 11/25 | Loss: 0.00119795
Iteration 12/25 | Loss: 0.00119795
Iteration 13/25 | Loss: 0.00119795
Iteration 14/25 | Loss: 0.00119795
Iteration 15/25 | Loss: 0.00119795
Iteration 16/25 | Loss: 0.00119795
Iteration 17/25 | Loss: 0.00119795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011979504488408566, 0.0011979504488408566, 0.0011979504488408566, 0.0011979504488408566, 0.0011979504488408566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011979504488408566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119795
Iteration 2/1000 | Loss: 0.00004915
Iteration 3/1000 | Loss: 0.00002719
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00001961
Iteration 6/1000 | Loss: 0.00001859
Iteration 7/1000 | Loss: 0.00001819
Iteration 8/1000 | Loss: 0.00001770
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001683
Iteration 11/1000 | Loss: 0.00001672
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001645
Iteration 14/1000 | Loss: 0.00001633
Iteration 15/1000 | Loss: 0.00001627
Iteration 16/1000 | Loss: 0.00001616
Iteration 17/1000 | Loss: 0.00001612
Iteration 18/1000 | Loss: 0.00001609
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001593
Iteration 26/1000 | Loss: 0.00001593
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001592
Iteration 29/1000 | Loss: 0.00001592
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001591
Iteration 32/1000 | Loss: 0.00001589
Iteration 33/1000 | Loss: 0.00001588
Iteration 34/1000 | Loss: 0.00001587
Iteration 35/1000 | Loss: 0.00001583
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001577
Iteration 41/1000 | Loss: 0.00001577
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001575
Iteration 47/1000 | Loss: 0.00001575
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001574
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001574
Iteration 57/1000 | Loss: 0.00001574
Iteration 58/1000 | Loss: 0.00001574
Iteration 59/1000 | Loss: 0.00001574
Iteration 60/1000 | Loss: 0.00001573
Iteration 61/1000 | Loss: 0.00001573
Iteration 62/1000 | Loss: 0.00001573
Iteration 63/1000 | Loss: 0.00001573
Iteration 64/1000 | Loss: 0.00001573
Iteration 65/1000 | Loss: 0.00001572
Iteration 66/1000 | Loss: 0.00001572
Iteration 67/1000 | Loss: 0.00001572
Iteration 68/1000 | Loss: 0.00001572
Iteration 69/1000 | Loss: 0.00001571
Iteration 70/1000 | Loss: 0.00001571
Iteration 71/1000 | Loss: 0.00001571
Iteration 72/1000 | Loss: 0.00001570
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001570
Iteration 75/1000 | Loss: 0.00001570
Iteration 76/1000 | Loss: 0.00001569
Iteration 77/1000 | Loss: 0.00001569
Iteration 78/1000 | Loss: 0.00001569
Iteration 79/1000 | Loss: 0.00001569
Iteration 80/1000 | Loss: 0.00001569
Iteration 81/1000 | Loss: 0.00001569
Iteration 82/1000 | Loss: 0.00001569
Iteration 83/1000 | Loss: 0.00001569
Iteration 84/1000 | Loss: 0.00001568
Iteration 85/1000 | Loss: 0.00001568
Iteration 86/1000 | Loss: 0.00001568
Iteration 87/1000 | Loss: 0.00001568
Iteration 88/1000 | Loss: 0.00001568
Iteration 89/1000 | Loss: 0.00001568
Iteration 90/1000 | Loss: 0.00001568
Iteration 91/1000 | Loss: 0.00001567
Iteration 92/1000 | Loss: 0.00001567
Iteration 93/1000 | Loss: 0.00001567
Iteration 94/1000 | Loss: 0.00001567
Iteration 95/1000 | Loss: 0.00001567
Iteration 96/1000 | Loss: 0.00001567
Iteration 97/1000 | Loss: 0.00001567
Iteration 98/1000 | Loss: 0.00001567
Iteration 99/1000 | Loss: 0.00001566
Iteration 100/1000 | Loss: 0.00001566
Iteration 101/1000 | Loss: 0.00001566
Iteration 102/1000 | Loss: 0.00001566
Iteration 103/1000 | Loss: 0.00001566
Iteration 104/1000 | Loss: 0.00001565
Iteration 105/1000 | Loss: 0.00001565
Iteration 106/1000 | Loss: 0.00001565
Iteration 107/1000 | Loss: 0.00001565
Iteration 108/1000 | Loss: 0.00001565
Iteration 109/1000 | Loss: 0.00001565
Iteration 110/1000 | Loss: 0.00001565
Iteration 111/1000 | Loss: 0.00001564
Iteration 112/1000 | Loss: 0.00001564
Iteration 113/1000 | Loss: 0.00001564
Iteration 114/1000 | Loss: 0.00001564
Iteration 115/1000 | Loss: 0.00001564
Iteration 116/1000 | Loss: 0.00001564
Iteration 117/1000 | Loss: 0.00001564
Iteration 118/1000 | Loss: 0.00001564
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001563
Iteration 122/1000 | Loss: 0.00001563
Iteration 123/1000 | Loss: 0.00001563
Iteration 124/1000 | Loss: 0.00001563
Iteration 125/1000 | Loss: 0.00001563
Iteration 126/1000 | Loss: 0.00001563
Iteration 127/1000 | Loss: 0.00001562
Iteration 128/1000 | Loss: 0.00001562
Iteration 129/1000 | Loss: 0.00001562
Iteration 130/1000 | Loss: 0.00001562
Iteration 131/1000 | Loss: 0.00001562
Iteration 132/1000 | Loss: 0.00001562
Iteration 133/1000 | Loss: 0.00001562
Iteration 134/1000 | Loss: 0.00001562
Iteration 135/1000 | Loss: 0.00001562
Iteration 136/1000 | Loss: 0.00001562
Iteration 137/1000 | Loss: 0.00001562
Iteration 138/1000 | Loss: 0.00001562
Iteration 139/1000 | Loss: 0.00001562
Iteration 140/1000 | Loss: 0.00001562
Iteration 141/1000 | Loss: 0.00001562
Iteration 142/1000 | Loss: 0.00001562
Iteration 143/1000 | Loss: 0.00001561
Iteration 144/1000 | Loss: 0.00001561
Iteration 145/1000 | Loss: 0.00001561
Iteration 146/1000 | Loss: 0.00001561
Iteration 147/1000 | Loss: 0.00001561
Iteration 148/1000 | Loss: 0.00001560
Iteration 149/1000 | Loss: 0.00001560
Iteration 150/1000 | Loss: 0.00001560
Iteration 151/1000 | Loss: 0.00001559
Iteration 152/1000 | Loss: 0.00001559
Iteration 153/1000 | Loss: 0.00001559
Iteration 154/1000 | Loss: 0.00001559
Iteration 155/1000 | Loss: 0.00001559
Iteration 156/1000 | Loss: 0.00001559
Iteration 157/1000 | Loss: 0.00001559
Iteration 158/1000 | Loss: 0.00001559
Iteration 159/1000 | Loss: 0.00001559
Iteration 160/1000 | Loss: 0.00001559
Iteration 161/1000 | Loss: 0.00001559
Iteration 162/1000 | Loss: 0.00001559
Iteration 163/1000 | Loss: 0.00001559
Iteration 164/1000 | Loss: 0.00001559
Iteration 165/1000 | Loss: 0.00001559
Iteration 166/1000 | Loss: 0.00001558
Iteration 167/1000 | Loss: 0.00001558
Iteration 168/1000 | Loss: 0.00001558
Iteration 169/1000 | Loss: 0.00001558
Iteration 170/1000 | Loss: 0.00001558
Iteration 171/1000 | Loss: 0.00001558
Iteration 172/1000 | Loss: 0.00001558
Iteration 173/1000 | Loss: 0.00001558
Iteration 174/1000 | Loss: 0.00001558
Iteration 175/1000 | Loss: 0.00001558
Iteration 176/1000 | Loss: 0.00001558
Iteration 177/1000 | Loss: 0.00001558
Iteration 178/1000 | Loss: 0.00001558
Iteration 179/1000 | Loss: 0.00001557
Iteration 180/1000 | Loss: 0.00001557
Iteration 181/1000 | Loss: 0.00001557
Iteration 182/1000 | Loss: 0.00001557
Iteration 183/1000 | Loss: 0.00001557
Iteration 184/1000 | Loss: 0.00001556
Iteration 185/1000 | Loss: 0.00001556
Iteration 186/1000 | Loss: 0.00001556
Iteration 187/1000 | Loss: 0.00001556
Iteration 188/1000 | Loss: 0.00001556
Iteration 189/1000 | Loss: 0.00001556
Iteration 190/1000 | Loss: 0.00001556
Iteration 191/1000 | Loss: 0.00001556
Iteration 192/1000 | Loss: 0.00001556
Iteration 193/1000 | Loss: 0.00001556
Iteration 194/1000 | Loss: 0.00001556
Iteration 195/1000 | Loss: 0.00001555
Iteration 196/1000 | Loss: 0.00001555
Iteration 197/1000 | Loss: 0.00001555
Iteration 198/1000 | Loss: 0.00001555
Iteration 199/1000 | Loss: 0.00001555
Iteration 200/1000 | Loss: 0.00001555
Iteration 201/1000 | Loss: 0.00001555
Iteration 202/1000 | Loss: 0.00001555
Iteration 203/1000 | Loss: 0.00001555
Iteration 204/1000 | Loss: 0.00001555
Iteration 205/1000 | Loss: 0.00001555
Iteration 206/1000 | Loss: 0.00001554
Iteration 207/1000 | Loss: 0.00001554
Iteration 208/1000 | Loss: 0.00001554
Iteration 209/1000 | Loss: 0.00001554
Iteration 210/1000 | Loss: 0.00001554
Iteration 211/1000 | Loss: 0.00001554
Iteration 212/1000 | Loss: 0.00001554
Iteration 213/1000 | Loss: 0.00001554
Iteration 214/1000 | Loss: 0.00001553
Iteration 215/1000 | Loss: 0.00001553
Iteration 216/1000 | Loss: 0.00001553
Iteration 217/1000 | Loss: 0.00001553
Iteration 218/1000 | Loss: 0.00001553
Iteration 219/1000 | Loss: 0.00001553
Iteration 220/1000 | Loss: 0.00001553
Iteration 221/1000 | Loss: 0.00001553
Iteration 222/1000 | Loss: 0.00001553
Iteration 223/1000 | Loss: 0.00001553
Iteration 224/1000 | Loss: 0.00001553
Iteration 225/1000 | Loss: 0.00001553
Iteration 226/1000 | Loss: 0.00001553
Iteration 227/1000 | Loss: 0.00001553
Iteration 228/1000 | Loss: 0.00001553
Iteration 229/1000 | Loss: 0.00001553
Iteration 230/1000 | Loss: 0.00001552
Iteration 231/1000 | Loss: 0.00001552
Iteration 232/1000 | Loss: 0.00001552
Iteration 233/1000 | Loss: 0.00001552
Iteration 234/1000 | Loss: 0.00001552
Iteration 235/1000 | Loss: 0.00001552
Iteration 236/1000 | Loss: 0.00001552
Iteration 237/1000 | Loss: 0.00001552
Iteration 238/1000 | Loss: 0.00001552
Iteration 239/1000 | Loss: 0.00001552
Iteration 240/1000 | Loss: 0.00001552
Iteration 241/1000 | Loss: 0.00001552
Iteration 242/1000 | Loss: 0.00001552
Iteration 243/1000 | Loss: 0.00001552
Iteration 244/1000 | Loss: 0.00001552
Iteration 245/1000 | Loss: 0.00001552
Iteration 246/1000 | Loss: 0.00001552
Iteration 247/1000 | Loss: 0.00001552
Iteration 248/1000 | Loss: 0.00001552
Iteration 249/1000 | Loss: 0.00001551
Iteration 250/1000 | Loss: 0.00001551
Iteration 251/1000 | Loss: 0.00001551
Iteration 252/1000 | Loss: 0.00001551
Iteration 253/1000 | Loss: 0.00001551
Iteration 254/1000 | Loss: 0.00001551
Iteration 255/1000 | Loss: 0.00001551
Iteration 256/1000 | Loss: 0.00001551
Iteration 257/1000 | Loss: 0.00001551
Iteration 258/1000 | Loss: 0.00001551
Iteration 259/1000 | Loss: 0.00001551
Iteration 260/1000 | Loss: 0.00001551
Iteration 261/1000 | Loss: 0.00001551
Iteration 262/1000 | Loss: 0.00001551
Iteration 263/1000 | Loss: 0.00001551
Iteration 264/1000 | Loss: 0.00001551
Iteration 265/1000 | Loss: 0.00001551
Iteration 266/1000 | Loss: 0.00001551
Iteration 267/1000 | Loss: 0.00001551
Iteration 268/1000 | Loss: 0.00001551
Iteration 269/1000 | Loss: 0.00001550
Iteration 270/1000 | Loss: 0.00001550
Iteration 271/1000 | Loss: 0.00001550
Iteration 272/1000 | Loss: 0.00001550
Iteration 273/1000 | Loss: 0.00001550
Iteration 274/1000 | Loss: 0.00001550
Iteration 275/1000 | Loss: 0.00001550
Iteration 276/1000 | Loss: 0.00001550
Iteration 277/1000 | Loss: 0.00001550
Iteration 278/1000 | Loss: 0.00001550
Iteration 279/1000 | Loss: 0.00001550
Iteration 280/1000 | Loss: 0.00001550
Iteration 281/1000 | Loss: 0.00001550
Iteration 282/1000 | Loss: 0.00001550
Iteration 283/1000 | Loss: 0.00001550
Iteration 284/1000 | Loss: 0.00001550
Iteration 285/1000 | Loss: 0.00001549
Iteration 286/1000 | Loss: 0.00001549
Iteration 287/1000 | Loss: 0.00001549
Iteration 288/1000 | Loss: 0.00001549
Iteration 289/1000 | Loss: 0.00001549
Iteration 290/1000 | Loss: 0.00001549
Iteration 291/1000 | Loss: 0.00001549
Iteration 292/1000 | Loss: 0.00001549
Iteration 293/1000 | Loss: 0.00001549
Iteration 294/1000 | Loss: 0.00001549
Iteration 295/1000 | Loss: 0.00001549
Iteration 296/1000 | Loss: 0.00001549
Iteration 297/1000 | Loss: 0.00001548
Iteration 298/1000 | Loss: 0.00001548
Iteration 299/1000 | Loss: 0.00001548
Iteration 300/1000 | Loss: 0.00001548
Iteration 301/1000 | Loss: 0.00001548
Iteration 302/1000 | Loss: 0.00001548
Iteration 303/1000 | Loss: 0.00001548
Iteration 304/1000 | Loss: 0.00001548
Iteration 305/1000 | Loss: 0.00001548
Iteration 306/1000 | Loss: 0.00001548
Iteration 307/1000 | Loss: 0.00001547
Iteration 308/1000 | Loss: 0.00001547
Iteration 309/1000 | Loss: 0.00001547
Iteration 310/1000 | Loss: 0.00001547
Iteration 311/1000 | Loss: 0.00001547
Iteration 312/1000 | Loss: 0.00001547
Iteration 313/1000 | Loss: 0.00001547
Iteration 314/1000 | Loss: 0.00001547
Iteration 315/1000 | Loss: 0.00001547
Iteration 316/1000 | Loss: 0.00001547
Iteration 317/1000 | Loss: 0.00001547
Iteration 318/1000 | Loss: 0.00001547
Iteration 319/1000 | Loss: 0.00001547
Iteration 320/1000 | Loss: 0.00001547
Iteration 321/1000 | Loss: 0.00001547
Iteration 322/1000 | Loss: 0.00001547
Iteration 323/1000 | Loss: 0.00001547
Iteration 324/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 324. Stopping optimization.
Last 5 losses: [1.5472889572265558e-05, 1.5472889572265558e-05, 1.5472889572265558e-05, 1.5472889572265558e-05, 1.5472889572265558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5472889572265558e-05

Optimization complete. Final v2v error: 3.0969810485839844 mm

Highest mean error: 5.022074222564697 mm for frame 60

Lowest mean error: 2.4425394535064697 mm for frame 86

Saving results

Total time: 49.96187353134155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535770
Iteration 2/25 | Loss: 0.00127154
Iteration 3/25 | Loss: 0.00118879
Iteration 4/25 | Loss: 0.00117261
Iteration 5/25 | Loss: 0.00116838
Iteration 6/25 | Loss: 0.00116798
Iteration 7/25 | Loss: 0.00116798
Iteration 8/25 | Loss: 0.00116798
Iteration 9/25 | Loss: 0.00116798
Iteration 10/25 | Loss: 0.00116798
Iteration 11/25 | Loss: 0.00116798
Iteration 12/25 | Loss: 0.00116798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001167980721220374, 0.001167980721220374, 0.001167980721220374, 0.001167980721220374, 0.001167980721220374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001167980721220374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89802718
Iteration 2/25 | Loss: 0.00115653
Iteration 3/25 | Loss: 0.00115653
Iteration 4/25 | Loss: 0.00115653
Iteration 5/25 | Loss: 0.00115652
Iteration 6/25 | Loss: 0.00115652
Iteration 7/25 | Loss: 0.00115652
Iteration 8/25 | Loss: 0.00115652
Iteration 9/25 | Loss: 0.00115652
Iteration 10/25 | Loss: 0.00115652
Iteration 11/25 | Loss: 0.00115652
Iteration 12/25 | Loss: 0.00115652
Iteration 13/25 | Loss: 0.00115652
Iteration 14/25 | Loss: 0.00115652
Iteration 15/25 | Loss: 0.00115652
Iteration 16/25 | Loss: 0.00115652
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011565233580768108, 0.0011565233580768108, 0.0011565233580768108, 0.0011565233580768108, 0.0011565233580768108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011565233580768108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115652
Iteration 2/1000 | Loss: 0.00002906
Iteration 3/1000 | Loss: 0.00002172
Iteration 4/1000 | Loss: 0.00001956
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001688
Iteration 8/1000 | Loss: 0.00001635
Iteration 9/1000 | Loss: 0.00001604
Iteration 10/1000 | Loss: 0.00001570
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001541
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001497
Iteration 15/1000 | Loss: 0.00001496
Iteration 16/1000 | Loss: 0.00001490
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001472
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001460
Iteration 27/1000 | Loss: 0.00001459
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001457
Iteration 30/1000 | Loss: 0.00001456
Iteration 31/1000 | Loss: 0.00001455
Iteration 32/1000 | Loss: 0.00001455
Iteration 33/1000 | Loss: 0.00001454
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00001440
Iteration 38/1000 | Loss: 0.00001439
Iteration 39/1000 | Loss: 0.00001439
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001437
Iteration 50/1000 | Loss: 0.00001436
Iteration 51/1000 | Loss: 0.00001436
Iteration 52/1000 | Loss: 0.00001436
Iteration 53/1000 | Loss: 0.00001435
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001434
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001434
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001433
Iteration 60/1000 | Loss: 0.00001432
Iteration 61/1000 | Loss: 0.00001432
Iteration 62/1000 | Loss: 0.00001432
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001431
Iteration 66/1000 | Loss: 0.00001430
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001429
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001428
Iteration 71/1000 | Loss: 0.00001428
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001427
Iteration 75/1000 | Loss: 0.00001427
Iteration 76/1000 | Loss: 0.00001427
Iteration 77/1000 | Loss: 0.00001426
Iteration 78/1000 | Loss: 0.00001426
Iteration 79/1000 | Loss: 0.00001426
Iteration 80/1000 | Loss: 0.00001426
Iteration 81/1000 | Loss: 0.00001426
Iteration 82/1000 | Loss: 0.00001426
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001425
Iteration 86/1000 | Loss: 0.00001425
Iteration 87/1000 | Loss: 0.00001425
Iteration 88/1000 | Loss: 0.00001424
Iteration 89/1000 | Loss: 0.00001424
Iteration 90/1000 | Loss: 0.00001424
Iteration 91/1000 | Loss: 0.00001424
Iteration 92/1000 | Loss: 0.00001424
Iteration 93/1000 | Loss: 0.00001424
Iteration 94/1000 | Loss: 0.00001424
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001423
Iteration 100/1000 | Loss: 0.00001423
Iteration 101/1000 | Loss: 0.00001423
Iteration 102/1000 | Loss: 0.00001423
Iteration 103/1000 | Loss: 0.00001423
Iteration 104/1000 | Loss: 0.00001423
Iteration 105/1000 | Loss: 0.00001423
Iteration 106/1000 | Loss: 0.00001423
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001422
Iteration 109/1000 | Loss: 0.00001422
Iteration 110/1000 | Loss: 0.00001422
Iteration 111/1000 | Loss: 0.00001422
Iteration 112/1000 | Loss: 0.00001421
Iteration 113/1000 | Loss: 0.00001421
Iteration 114/1000 | Loss: 0.00001421
Iteration 115/1000 | Loss: 0.00001421
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001420
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001419
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001419
Iteration 131/1000 | Loss: 0.00001419
Iteration 132/1000 | Loss: 0.00001419
Iteration 133/1000 | Loss: 0.00001419
Iteration 134/1000 | Loss: 0.00001419
Iteration 135/1000 | Loss: 0.00001419
Iteration 136/1000 | Loss: 0.00001418
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001418
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001418
Iteration 141/1000 | Loss: 0.00001418
Iteration 142/1000 | Loss: 0.00001418
Iteration 143/1000 | Loss: 0.00001417
Iteration 144/1000 | Loss: 0.00001417
Iteration 145/1000 | Loss: 0.00001417
Iteration 146/1000 | Loss: 0.00001417
Iteration 147/1000 | Loss: 0.00001417
Iteration 148/1000 | Loss: 0.00001417
Iteration 149/1000 | Loss: 0.00001417
Iteration 150/1000 | Loss: 0.00001417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.4174729585647583e-05, 1.4174729585647583e-05, 1.4174729585647583e-05, 1.4174729585647583e-05, 1.4174729585647583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4174729585647583e-05

Optimization complete. Final v2v error: 3.223374128341675 mm

Highest mean error: 3.7209970951080322 mm for frame 126

Lowest mean error: 2.8792412281036377 mm for frame 84

Saving results

Total time: 43.76639747619629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497303
Iteration 2/25 | Loss: 0.00128507
Iteration 3/25 | Loss: 0.00120458
Iteration 4/25 | Loss: 0.00119531
Iteration 5/25 | Loss: 0.00119354
Iteration 6/25 | Loss: 0.00119354
Iteration 7/25 | Loss: 0.00119354
Iteration 8/25 | Loss: 0.00119354
Iteration 9/25 | Loss: 0.00119354
Iteration 10/25 | Loss: 0.00119354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011935399379581213, 0.0011935399379581213, 0.0011935399379581213, 0.0011935399379581213, 0.0011935399379581213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011935399379581213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29743350
Iteration 2/25 | Loss: 0.00100977
Iteration 3/25 | Loss: 0.00100975
Iteration 4/25 | Loss: 0.00100975
Iteration 5/25 | Loss: 0.00100975
Iteration 6/25 | Loss: 0.00100975
Iteration 7/25 | Loss: 0.00100975
Iteration 8/25 | Loss: 0.00100975
Iteration 9/25 | Loss: 0.00100975
Iteration 10/25 | Loss: 0.00100975
Iteration 11/25 | Loss: 0.00100975
Iteration 12/25 | Loss: 0.00100975
Iteration 13/25 | Loss: 0.00100975
Iteration 14/25 | Loss: 0.00100975
Iteration 15/25 | Loss: 0.00100975
Iteration 16/25 | Loss: 0.00100975
Iteration 17/25 | Loss: 0.00100975
Iteration 18/25 | Loss: 0.00100975
Iteration 19/25 | Loss: 0.00100975
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010097462218254805, 0.0010097462218254805, 0.0010097462218254805, 0.0010097462218254805, 0.0010097462218254805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010097462218254805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100975
Iteration 2/1000 | Loss: 0.00002812
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001502
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00001378
Iteration 7/1000 | Loss: 0.00001343
Iteration 8/1000 | Loss: 0.00001312
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001221
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001200
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001196
Iteration 21/1000 | Loss: 0.00001196
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001188
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001180
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001170
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001169
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001168
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001159
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001158
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001154
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001152
Iteration 122/1000 | Loss: 0.00001152
Iteration 123/1000 | Loss: 0.00001152
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001151
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001150
Iteration 133/1000 | Loss: 0.00001150
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001150
Iteration 136/1000 | Loss: 0.00001150
Iteration 137/1000 | Loss: 0.00001150
Iteration 138/1000 | Loss: 0.00001150
Iteration 139/1000 | Loss: 0.00001150
Iteration 140/1000 | Loss: 0.00001150
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001150
Iteration 144/1000 | Loss: 0.00001150
Iteration 145/1000 | Loss: 0.00001150
Iteration 146/1000 | Loss: 0.00001150
Iteration 147/1000 | Loss: 0.00001150
Iteration 148/1000 | Loss: 0.00001150
Iteration 149/1000 | Loss: 0.00001150
Iteration 150/1000 | Loss: 0.00001150
Iteration 151/1000 | Loss: 0.00001150
Iteration 152/1000 | Loss: 0.00001150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.1496033039293252e-05, 1.1496033039293252e-05, 1.1496033039293252e-05, 1.1496033039293252e-05, 1.1496033039293252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1496033039293252e-05

Optimization complete. Final v2v error: 2.8648622035980225 mm

Highest mean error: 3.088454008102417 mm for frame 173

Lowest mean error: 2.5887341499328613 mm for frame 223

Saving results

Total time: 44.08676266670227
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468745
Iteration 2/25 | Loss: 0.00123833
Iteration 3/25 | Loss: 0.00115219
Iteration 4/25 | Loss: 0.00113759
Iteration 5/25 | Loss: 0.00113382
Iteration 6/25 | Loss: 0.00113382
Iteration 7/25 | Loss: 0.00113382
Iteration 8/25 | Loss: 0.00113382
Iteration 9/25 | Loss: 0.00113382
Iteration 10/25 | Loss: 0.00113382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011338167823851109, 0.0011338167823851109, 0.0011338167823851109, 0.0011338167823851109, 0.0011338167823851109]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011338167823851109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75946522
Iteration 2/25 | Loss: 0.00119565
Iteration 3/25 | Loss: 0.00119564
Iteration 4/25 | Loss: 0.00119564
Iteration 5/25 | Loss: 0.00119564
Iteration 6/25 | Loss: 0.00119564
Iteration 7/25 | Loss: 0.00119564
Iteration 8/25 | Loss: 0.00119564
Iteration 9/25 | Loss: 0.00119564
Iteration 10/25 | Loss: 0.00119564
Iteration 11/25 | Loss: 0.00119564
Iteration 12/25 | Loss: 0.00119564
Iteration 13/25 | Loss: 0.00119564
Iteration 14/25 | Loss: 0.00119564
Iteration 15/25 | Loss: 0.00119564
Iteration 16/25 | Loss: 0.00119564
Iteration 17/25 | Loss: 0.00119564
Iteration 18/25 | Loss: 0.00119564
Iteration 19/25 | Loss: 0.00119564
Iteration 20/25 | Loss: 0.00119564
Iteration 21/25 | Loss: 0.00119564
Iteration 22/25 | Loss: 0.00119564
Iteration 23/25 | Loss: 0.00119564
Iteration 24/25 | Loss: 0.00119564
Iteration 25/25 | Loss: 0.00119564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011956385569646955, 0.0011956385569646955, 0.0011956385569646955, 0.0011956385569646955, 0.0011956385569646955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011956385569646955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119564
Iteration 2/1000 | Loss: 0.00001769
Iteration 3/1000 | Loss: 0.00001422
Iteration 4/1000 | Loss: 0.00001301
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001191
Iteration 7/1000 | Loss: 0.00001140
Iteration 8/1000 | Loss: 0.00001110
Iteration 9/1000 | Loss: 0.00001100
Iteration 10/1000 | Loss: 0.00001088
Iteration 11/1000 | Loss: 0.00001085
Iteration 12/1000 | Loss: 0.00001069
Iteration 13/1000 | Loss: 0.00001058
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001055
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001049
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001012
Iteration 21/1000 | Loss: 0.00001006
Iteration 22/1000 | Loss: 0.00001005
Iteration 23/1000 | Loss: 0.00001003
Iteration 24/1000 | Loss: 0.00000994
Iteration 25/1000 | Loss: 0.00000994
Iteration 26/1000 | Loss: 0.00000991
Iteration 27/1000 | Loss: 0.00000991
Iteration 28/1000 | Loss: 0.00000990
Iteration 29/1000 | Loss: 0.00000988
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000986
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000984
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000979
Iteration 41/1000 | Loss: 0.00000978
Iteration 42/1000 | Loss: 0.00000978
Iteration 43/1000 | Loss: 0.00000978
Iteration 44/1000 | Loss: 0.00000978
Iteration 45/1000 | Loss: 0.00000978
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000978
Iteration 49/1000 | Loss: 0.00000978
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000978
Iteration 52/1000 | Loss: 0.00000978
Iteration 53/1000 | Loss: 0.00000978
Iteration 54/1000 | Loss: 0.00000978
Iteration 55/1000 | Loss: 0.00000978
Iteration 56/1000 | Loss: 0.00000978
Iteration 57/1000 | Loss: 0.00000978
Iteration 58/1000 | Loss: 0.00000978
Iteration 59/1000 | Loss: 0.00000978
Iteration 60/1000 | Loss: 0.00000978
Iteration 61/1000 | Loss: 0.00000978
Iteration 62/1000 | Loss: 0.00000978
Iteration 63/1000 | Loss: 0.00000978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [9.778847015695646e-06, 9.778847015695646e-06, 9.778847015695646e-06, 9.778847015695646e-06, 9.778847015695646e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.778847015695646e-06

Optimization complete. Final v2v error: 2.7588250637054443 mm

Highest mean error: 2.9697234630584717 mm for frame 110

Lowest mean error: 2.593188524246216 mm for frame 2

Saving results

Total time: 35.436333656311035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842352
Iteration 2/25 | Loss: 0.00174081
Iteration 3/25 | Loss: 0.00152153
Iteration 4/25 | Loss: 0.00151082
Iteration 5/25 | Loss: 0.00150812
Iteration 6/25 | Loss: 0.00150776
Iteration 7/25 | Loss: 0.00150776
Iteration 8/25 | Loss: 0.00150776
Iteration 9/25 | Loss: 0.00150776
Iteration 10/25 | Loss: 0.00150776
Iteration 11/25 | Loss: 0.00150776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015077629359439015, 0.0015077629359439015, 0.0015077629359439015, 0.0015077629359439015, 0.0015077629359439015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015077629359439015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.47151905
Iteration 2/25 | Loss: 0.00119817
Iteration 3/25 | Loss: 0.00119816
Iteration 4/25 | Loss: 0.00119816
Iteration 5/25 | Loss: 0.00119816
Iteration 6/25 | Loss: 0.00119816
Iteration 7/25 | Loss: 0.00119816
Iteration 8/25 | Loss: 0.00119816
Iteration 9/25 | Loss: 0.00119816
Iteration 10/25 | Loss: 0.00119816
Iteration 11/25 | Loss: 0.00119816
Iteration 12/25 | Loss: 0.00119816
Iteration 13/25 | Loss: 0.00119816
Iteration 14/25 | Loss: 0.00119816
Iteration 15/25 | Loss: 0.00119816
Iteration 16/25 | Loss: 0.00119816
Iteration 17/25 | Loss: 0.00119816
Iteration 18/25 | Loss: 0.00119816
Iteration 19/25 | Loss: 0.00119816
Iteration 20/25 | Loss: 0.00119816
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011981617426499724, 0.0011981617426499724, 0.0011981617426499724, 0.0011981617426499724, 0.0011981617426499724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011981617426499724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119816
Iteration 2/1000 | Loss: 0.00010839
Iteration 3/1000 | Loss: 0.00006769
Iteration 4/1000 | Loss: 0.00005136
Iteration 5/1000 | Loss: 0.00004762
Iteration 6/1000 | Loss: 0.00004606
Iteration 7/1000 | Loss: 0.00004522
Iteration 8/1000 | Loss: 0.00004449
Iteration 9/1000 | Loss: 0.00004357
Iteration 10/1000 | Loss: 0.00004276
Iteration 11/1000 | Loss: 0.00004203
Iteration 12/1000 | Loss: 0.00004145
Iteration 13/1000 | Loss: 0.00004100
Iteration 14/1000 | Loss: 0.00004049
Iteration 15/1000 | Loss: 0.00004009
Iteration 16/1000 | Loss: 0.00003976
Iteration 17/1000 | Loss: 0.00003954
Iteration 18/1000 | Loss: 0.00003931
Iteration 19/1000 | Loss: 0.00003917
Iteration 20/1000 | Loss: 0.00003904
Iteration 21/1000 | Loss: 0.00003904
Iteration 22/1000 | Loss: 0.00003904
Iteration 23/1000 | Loss: 0.00003904
Iteration 24/1000 | Loss: 0.00003904
Iteration 25/1000 | Loss: 0.00003904
Iteration 26/1000 | Loss: 0.00003904
Iteration 27/1000 | Loss: 0.00003904
Iteration 28/1000 | Loss: 0.00003904
Iteration 29/1000 | Loss: 0.00003904
Iteration 30/1000 | Loss: 0.00003903
Iteration 31/1000 | Loss: 0.00003903
Iteration 32/1000 | Loss: 0.00003903
Iteration 33/1000 | Loss: 0.00003902
Iteration 34/1000 | Loss: 0.00003902
Iteration 35/1000 | Loss: 0.00003893
Iteration 36/1000 | Loss: 0.00003885
Iteration 37/1000 | Loss: 0.00003877
Iteration 38/1000 | Loss: 0.00003876
Iteration 39/1000 | Loss: 0.00003870
Iteration 40/1000 | Loss: 0.00003865
Iteration 41/1000 | Loss: 0.00003862
Iteration 42/1000 | Loss: 0.00003861
Iteration 43/1000 | Loss: 0.00003861
Iteration 44/1000 | Loss: 0.00003861
Iteration 45/1000 | Loss: 0.00003861
Iteration 46/1000 | Loss: 0.00003861
Iteration 47/1000 | Loss: 0.00003861
Iteration 48/1000 | Loss: 0.00003860
Iteration 49/1000 | Loss: 0.00003860
Iteration 50/1000 | Loss: 0.00003858
Iteration 51/1000 | Loss: 0.00003858
Iteration 52/1000 | Loss: 0.00003857
Iteration 53/1000 | Loss: 0.00003857
Iteration 54/1000 | Loss: 0.00003856
Iteration 55/1000 | Loss: 0.00003856
Iteration 56/1000 | Loss: 0.00003856
Iteration 57/1000 | Loss: 0.00003855
Iteration 58/1000 | Loss: 0.00003855
Iteration 59/1000 | Loss: 0.00003855
Iteration 60/1000 | Loss: 0.00003854
Iteration 61/1000 | Loss: 0.00003854
Iteration 62/1000 | Loss: 0.00003854
Iteration 63/1000 | Loss: 0.00003853
Iteration 64/1000 | Loss: 0.00003853
Iteration 65/1000 | Loss: 0.00003853
Iteration 66/1000 | Loss: 0.00003853
Iteration 67/1000 | Loss: 0.00003853
Iteration 68/1000 | Loss: 0.00003852
Iteration 69/1000 | Loss: 0.00003852
Iteration 70/1000 | Loss: 0.00003852
Iteration 71/1000 | Loss: 0.00003852
Iteration 72/1000 | Loss: 0.00003852
Iteration 73/1000 | Loss: 0.00003852
Iteration 74/1000 | Loss: 0.00003852
Iteration 75/1000 | Loss: 0.00003852
Iteration 76/1000 | Loss: 0.00003852
Iteration 77/1000 | Loss: 0.00003852
Iteration 78/1000 | Loss: 0.00003852
Iteration 79/1000 | Loss: 0.00003851
Iteration 80/1000 | Loss: 0.00003851
Iteration 81/1000 | Loss: 0.00003851
Iteration 82/1000 | Loss: 0.00003851
Iteration 83/1000 | Loss: 0.00003851
Iteration 84/1000 | Loss: 0.00003851
Iteration 85/1000 | Loss: 0.00003851
Iteration 86/1000 | Loss: 0.00003850
Iteration 87/1000 | Loss: 0.00003850
Iteration 88/1000 | Loss: 0.00003849
Iteration 89/1000 | Loss: 0.00003849
Iteration 90/1000 | Loss: 0.00003849
Iteration 91/1000 | Loss: 0.00003849
Iteration 92/1000 | Loss: 0.00003849
Iteration 93/1000 | Loss: 0.00003848
Iteration 94/1000 | Loss: 0.00003848
Iteration 95/1000 | Loss: 0.00003848
Iteration 96/1000 | Loss: 0.00003847
Iteration 97/1000 | Loss: 0.00003847
Iteration 98/1000 | Loss: 0.00003847
Iteration 99/1000 | Loss: 0.00003846
Iteration 100/1000 | Loss: 0.00003846
Iteration 101/1000 | Loss: 0.00003846
Iteration 102/1000 | Loss: 0.00003846
Iteration 103/1000 | Loss: 0.00003846
Iteration 104/1000 | Loss: 0.00003846
Iteration 105/1000 | Loss: 0.00003846
Iteration 106/1000 | Loss: 0.00003846
Iteration 107/1000 | Loss: 0.00003846
Iteration 108/1000 | Loss: 0.00003846
Iteration 109/1000 | Loss: 0.00003845
Iteration 110/1000 | Loss: 0.00003845
Iteration 111/1000 | Loss: 0.00003845
Iteration 112/1000 | Loss: 0.00003845
Iteration 113/1000 | Loss: 0.00003845
Iteration 114/1000 | Loss: 0.00003845
Iteration 115/1000 | Loss: 0.00003845
Iteration 116/1000 | Loss: 0.00003845
Iteration 117/1000 | Loss: 0.00003845
Iteration 118/1000 | Loss: 0.00003845
Iteration 119/1000 | Loss: 0.00003845
Iteration 120/1000 | Loss: 0.00003844
Iteration 121/1000 | Loss: 0.00003844
Iteration 122/1000 | Loss: 0.00003844
Iteration 123/1000 | Loss: 0.00003844
Iteration 124/1000 | Loss: 0.00003844
Iteration 125/1000 | Loss: 0.00003844
Iteration 126/1000 | Loss: 0.00003844
Iteration 127/1000 | Loss: 0.00003843
Iteration 128/1000 | Loss: 0.00003843
Iteration 129/1000 | Loss: 0.00003843
Iteration 130/1000 | Loss: 0.00003843
Iteration 131/1000 | Loss: 0.00003843
Iteration 132/1000 | Loss: 0.00003843
Iteration 133/1000 | Loss: 0.00003843
Iteration 134/1000 | Loss: 0.00003843
Iteration 135/1000 | Loss: 0.00003843
Iteration 136/1000 | Loss: 0.00003842
Iteration 137/1000 | Loss: 0.00003842
Iteration 138/1000 | Loss: 0.00003842
Iteration 139/1000 | Loss: 0.00003842
Iteration 140/1000 | Loss: 0.00003842
Iteration 141/1000 | Loss: 0.00003842
Iteration 142/1000 | Loss: 0.00003841
Iteration 143/1000 | Loss: 0.00003841
Iteration 144/1000 | Loss: 0.00003841
Iteration 145/1000 | Loss: 0.00003841
Iteration 146/1000 | Loss: 0.00003841
Iteration 147/1000 | Loss: 0.00003841
Iteration 148/1000 | Loss: 0.00003841
Iteration 149/1000 | Loss: 0.00003841
Iteration 150/1000 | Loss: 0.00003840
Iteration 151/1000 | Loss: 0.00003840
Iteration 152/1000 | Loss: 0.00003840
Iteration 153/1000 | Loss: 0.00003840
Iteration 154/1000 | Loss: 0.00003840
Iteration 155/1000 | Loss: 0.00003840
Iteration 156/1000 | Loss: 0.00003840
Iteration 157/1000 | Loss: 0.00003840
Iteration 158/1000 | Loss: 0.00003840
Iteration 159/1000 | Loss: 0.00003840
Iteration 160/1000 | Loss: 0.00003840
Iteration 161/1000 | Loss: 0.00003840
Iteration 162/1000 | Loss: 0.00003840
Iteration 163/1000 | Loss: 0.00003840
Iteration 164/1000 | Loss: 0.00003840
Iteration 165/1000 | Loss: 0.00003839
Iteration 166/1000 | Loss: 0.00003839
Iteration 167/1000 | Loss: 0.00003839
Iteration 168/1000 | Loss: 0.00003839
Iteration 169/1000 | Loss: 0.00003839
Iteration 170/1000 | Loss: 0.00003839
Iteration 171/1000 | Loss: 0.00003839
Iteration 172/1000 | Loss: 0.00003839
Iteration 173/1000 | Loss: 0.00003839
Iteration 174/1000 | Loss: 0.00003839
Iteration 175/1000 | Loss: 0.00003839
Iteration 176/1000 | Loss: 0.00003839
Iteration 177/1000 | Loss: 0.00003839
Iteration 178/1000 | Loss: 0.00003839
Iteration 179/1000 | Loss: 0.00003839
Iteration 180/1000 | Loss: 0.00003839
Iteration 181/1000 | Loss: 0.00003839
Iteration 182/1000 | Loss: 0.00003839
Iteration 183/1000 | Loss: 0.00003839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [3.839334749500267e-05, 3.839334749500267e-05, 3.839334749500267e-05, 3.839334749500267e-05, 3.839334749500267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.839334749500267e-05

Optimization complete. Final v2v error: 5.093752384185791 mm

Highest mean error: 6.1947760581970215 mm for frame 143

Lowest mean error: 4.790557384490967 mm for frame 37

Saving results

Total time: 52.23051357269287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795369
Iteration 2/25 | Loss: 0.00127258
Iteration 3/25 | Loss: 0.00118851
Iteration 4/25 | Loss: 0.00117644
Iteration 5/25 | Loss: 0.00117347
Iteration 6/25 | Loss: 0.00117295
Iteration 7/25 | Loss: 0.00117295
Iteration 8/25 | Loss: 0.00117295
Iteration 9/25 | Loss: 0.00117295
Iteration 10/25 | Loss: 0.00117295
Iteration 11/25 | Loss: 0.00117295
Iteration 12/25 | Loss: 0.00117295
Iteration 13/25 | Loss: 0.00117295
Iteration 14/25 | Loss: 0.00117295
Iteration 15/25 | Loss: 0.00117295
Iteration 16/25 | Loss: 0.00117295
Iteration 17/25 | Loss: 0.00117295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011729543330147862, 0.0011729543330147862, 0.0011729543330147862, 0.0011729543330147862, 0.0011729543330147862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011729543330147862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34605706
Iteration 2/25 | Loss: 0.00112226
Iteration 3/25 | Loss: 0.00112226
Iteration 4/25 | Loss: 0.00112225
Iteration 5/25 | Loss: 0.00112225
Iteration 6/25 | Loss: 0.00112225
Iteration 7/25 | Loss: 0.00112225
Iteration 8/25 | Loss: 0.00112225
Iteration 9/25 | Loss: 0.00112225
Iteration 10/25 | Loss: 0.00112225
Iteration 11/25 | Loss: 0.00112225
Iteration 12/25 | Loss: 0.00112225
Iteration 13/25 | Loss: 0.00112225
Iteration 14/25 | Loss: 0.00112225
Iteration 15/25 | Loss: 0.00112225
Iteration 16/25 | Loss: 0.00112225
Iteration 17/25 | Loss: 0.00112225
Iteration 18/25 | Loss: 0.00112225
Iteration 19/25 | Loss: 0.00112225
Iteration 20/25 | Loss: 0.00112225
Iteration 21/25 | Loss: 0.00112225
Iteration 22/25 | Loss: 0.00112225
Iteration 23/25 | Loss: 0.00112225
Iteration 24/25 | Loss: 0.00112225
Iteration 25/25 | Loss: 0.00112225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112225
Iteration 2/1000 | Loss: 0.00002965
Iteration 3/1000 | Loss: 0.00002081
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001647
Iteration 6/1000 | Loss: 0.00001569
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001470
Iteration 9/1000 | Loss: 0.00001437
Iteration 10/1000 | Loss: 0.00001402
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001314
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001310
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00001294
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001290
Iteration 28/1000 | Loss: 0.00001290
Iteration 29/1000 | Loss: 0.00001290
Iteration 30/1000 | Loss: 0.00001289
Iteration 31/1000 | Loss: 0.00001289
Iteration 32/1000 | Loss: 0.00001287
Iteration 33/1000 | Loss: 0.00001287
Iteration 34/1000 | Loss: 0.00001286
Iteration 35/1000 | Loss: 0.00001285
Iteration 36/1000 | Loss: 0.00001285
Iteration 37/1000 | Loss: 0.00001285
Iteration 38/1000 | Loss: 0.00001285
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001285
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001283
Iteration 46/1000 | Loss: 0.00001283
Iteration 47/1000 | Loss: 0.00001281
Iteration 48/1000 | Loss: 0.00001281
Iteration 49/1000 | Loss: 0.00001281
Iteration 50/1000 | Loss: 0.00001281
Iteration 51/1000 | Loss: 0.00001281
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001280
Iteration 54/1000 | Loss: 0.00001280
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001280
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001280
Iteration 61/1000 | Loss: 0.00001280
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001280
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001277
Iteration 67/1000 | Loss: 0.00001277
Iteration 68/1000 | Loss: 0.00001277
Iteration 69/1000 | Loss: 0.00001276
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001276
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001274
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001273
Iteration 77/1000 | Loss: 0.00001273
Iteration 78/1000 | Loss: 0.00001273
Iteration 79/1000 | Loss: 0.00001272
Iteration 80/1000 | Loss: 0.00001272
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001272
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001272
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001270
Iteration 93/1000 | Loss: 0.00001270
Iteration 94/1000 | Loss: 0.00001270
Iteration 95/1000 | Loss: 0.00001270
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001270
Iteration 98/1000 | Loss: 0.00001269
Iteration 99/1000 | Loss: 0.00001269
Iteration 100/1000 | Loss: 0.00001269
Iteration 101/1000 | Loss: 0.00001269
Iteration 102/1000 | Loss: 0.00001269
Iteration 103/1000 | Loss: 0.00001268
Iteration 104/1000 | Loss: 0.00001268
Iteration 105/1000 | Loss: 0.00001267
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001267
Iteration 108/1000 | Loss: 0.00001267
Iteration 109/1000 | Loss: 0.00001267
Iteration 110/1000 | Loss: 0.00001267
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001266
Iteration 113/1000 | Loss: 0.00001266
Iteration 114/1000 | Loss: 0.00001266
Iteration 115/1000 | Loss: 0.00001266
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001266
Iteration 119/1000 | Loss: 0.00001265
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001265
Iteration 123/1000 | Loss: 0.00001265
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Iteration 129/1000 | Loss: 0.00001265
Iteration 130/1000 | Loss: 0.00001265
Iteration 131/1000 | Loss: 0.00001265
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001263
Iteration 138/1000 | Loss: 0.00001263
Iteration 139/1000 | Loss: 0.00001263
Iteration 140/1000 | Loss: 0.00001263
Iteration 141/1000 | Loss: 0.00001263
Iteration 142/1000 | Loss: 0.00001263
Iteration 143/1000 | Loss: 0.00001263
Iteration 144/1000 | Loss: 0.00001263
Iteration 145/1000 | Loss: 0.00001263
Iteration 146/1000 | Loss: 0.00001263
Iteration 147/1000 | Loss: 0.00001263
Iteration 148/1000 | Loss: 0.00001263
Iteration 149/1000 | Loss: 0.00001262
Iteration 150/1000 | Loss: 0.00001262
Iteration 151/1000 | Loss: 0.00001262
Iteration 152/1000 | Loss: 0.00001262
Iteration 153/1000 | Loss: 0.00001262
Iteration 154/1000 | Loss: 0.00001262
Iteration 155/1000 | Loss: 0.00001262
Iteration 156/1000 | Loss: 0.00001262
Iteration 157/1000 | Loss: 0.00001262
Iteration 158/1000 | Loss: 0.00001262
Iteration 159/1000 | Loss: 0.00001262
Iteration 160/1000 | Loss: 0.00001262
Iteration 161/1000 | Loss: 0.00001262
Iteration 162/1000 | Loss: 0.00001262
Iteration 163/1000 | Loss: 0.00001262
Iteration 164/1000 | Loss: 0.00001262
Iteration 165/1000 | Loss: 0.00001262
Iteration 166/1000 | Loss: 0.00001262
Iteration 167/1000 | Loss: 0.00001262
Iteration 168/1000 | Loss: 0.00001262
Iteration 169/1000 | Loss: 0.00001262
Iteration 170/1000 | Loss: 0.00001262
Iteration 171/1000 | Loss: 0.00001262
Iteration 172/1000 | Loss: 0.00001262
Iteration 173/1000 | Loss: 0.00001262
Iteration 174/1000 | Loss: 0.00001262
Iteration 175/1000 | Loss: 0.00001262
Iteration 176/1000 | Loss: 0.00001262
Iteration 177/1000 | Loss: 0.00001262
Iteration 178/1000 | Loss: 0.00001262
Iteration 179/1000 | Loss: 0.00001262
Iteration 180/1000 | Loss: 0.00001262
Iteration 181/1000 | Loss: 0.00001262
Iteration 182/1000 | Loss: 0.00001262
Iteration 183/1000 | Loss: 0.00001262
Iteration 184/1000 | Loss: 0.00001262
Iteration 185/1000 | Loss: 0.00001262
Iteration 186/1000 | Loss: 0.00001262
Iteration 187/1000 | Loss: 0.00001262
Iteration 188/1000 | Loss: 0.00001262
Iteration 189/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.2615377272595651e-05, 1.2615377272595651e-05, 1.2615377272595651e-05, 1.2615377272595651e-05, 1.2615377272595651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2615377272595651e-05

Optimization complete. Final v2v error: 3.065535068511963 mm

Highest mean error: 3.7797932624816895 mm for frame 86

Lowest mean error: 2.892810583114624 mm for frame 45

Saving results

Total time: 40.74633765220642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018171
Iteration 2/25 | Loss: 0.00275879
Iteration 3/25 | Loss: 0.00212666
Iteration 4/25 | Loss: 0.00210066
Iteration 5/25 | Loss: 0.00184235
Iteration 6/25 | Loss: 0.00164570
Iteration 7/25 | Loss: 0.00153847
Iteration 8/25 | Loss: 0.00149365
Iteration 9/25 | Loss: 0.00148178
Iteration 10/25 | Loss: 0.00146298
Iteration 11/25 | Loss: 0.00144537
Iteration 12/25 | Loss: 0.00143782
Iteration 13/25 | Loss: 0.00143603
Iteration 14/25 | Loss: 0.00143467
Iteration 15/25 | Loss: 0.00143271
Iteration 16/25 | Loss: 0.00143268
Iteration 17/25 | Loss: 0.00143249
Iteration 18/25 | Loss: 0.00143276
Iteration 19/25 | Loss: 0.00143242
Iteration 20/25 | Loss: 0.00143228
Iteration 21/25 | Loss: 0.00143267
Iteration 22/25 | Loss: 0.00143321
Iteration 23/25 | Loss: 0.00143110
Iteration 24/25 | Loss: 0.00143226
Iteration 25/25 | Loss: 0.00143343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40956259
Iteration 2/25 | Loss: 0.00372056
Iteration 3/25 | Loss: 0.00310062
Iteration 4/25 | Loss: 0.00310062
Iteration 5/25 | Loss: 0.00310062
Iteration 6/25 | Loss: 0.00310062
Iteration 7/25 | Loss: 0.00310062
Iteration 8/25 | Loss: 0.00310062
Iteration 9/25 | Loss: 0.00310062
Iteration 10/25 | Loss: 0.00310062
Iteration 11/25 | Loss: 0.00310062
Iteration 12/25 | Loss: 0.00310062
Iteration 13/25 | Loss: 0.00310062
Iteration 14/25 | Loss: 0.00310062
Iteration 15/25 | Loss: 0.00310062
Iteration 16/25 | Loss: 0.00310062
Iteration 17/25 | Loss: 0.00310062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0031006168574094772, 0.0031006168574094772, 0.0031006168574094772, 0.0031006168574094772, 0.0031006168574094772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031006168574094772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310062
Iteration 2/1000 | Loss: 0.00213560
Iteration 3/1000 | Loss: 0.00241008
Iteration 4/1000 | Loss: 0.00020696
Iteration 5/1000 | Loss: 0.00018888
Iteration 6/1000 | Loss: 0.00044790
Iteration 7/1000 | Loss: 0.00016335
Iteration 8/1000 | Loss: 0.00075690
Iteration 9/1000 | Loss: 0.00013983
Iteration 10/1000 | Loss: 0.00110084
Iteration 11/1000 | Loss: 0.00013355
Iteration 12/1000 | Loss: 0.00013847
Iteration 13/1000 | Loss: 0.00012850
Iteration 14/1000 | Loss: 0.00012972
Iteration 15/1000 | Loss: 0.00012809
Iteration 16/1000 | Loss: 0.00011990
Iteration 17/1000 | Loss: 0.00012769
Iteration 18/1000 | Loss: 0.00074839
Iteration 19/1000 | Loss: 0.00011999
Iteration 20/1000 | Loss: 0.00011743
Iteration 21/1000 | Loss: 0.00011875
Iteration 22/1000 | Loss: 0.00011804
Iteration 23/1000 | Loss: 0.00011695
Iteration 24/1000 | Loss: 0.00011592
Iteration 25/1000 | Loss: 0.00011857
Iteration 26/1000 | Loss: 0.00011684
Iteration 27/1000 | Loss: 0.00038602
Iteration 28/1000 | Loss: 0.00122505
Iteration 29/1000 | Loss: 0.00099751
Iteration 30/1000 | Loss: 0.00109488
Iteration 31/1000 | Loss: 0.00201052
Iteration 32/1000 | Loss: 0.00029995
Iteration 33/1000 | Loss: 0.00011670
Iteration 34/1000 | Loss: 0.00010702
Iteration 35/1000 | Loss: 0.00010111
Iteration 36/1000 | Loss: 0.00009789
Iteration 37/1000 | Loss: 0.00009525
Iteration 38/1000 | Loss: 0.00009398
Iteration 39/1000 | Loss: 0.00009278
Iteration 40/1000 | Loss: 0.00009192
Iteration 41/1000 | Loss: 0.00009120
Iteration 42/1000 | Loss: 0.00009048
Iteration 43/1000 | Loss: 0.00027269
Iteration 44/1000 | Loss: 0.00009573
Iteration 45/1000 | Loss: 0.00009050
Iteration 46/1000 | Loss: 0.00008919
Iteration 47/1000 | Loss: 0.00008790
Iteration 48/1000 | Loss: 0.00028119
Iteration 49/1000 | Loss: 0.00019004
Iteration 50/1000 | Loss: 0.00011448
Iteration 51/1000 | Loss: 0.00021563
Iteration 52/1000 | Loss: 0.00069651
Iteration 53/1000 | Loss: 0.00039042
Iteration 54/1000 | Loss: 0.00026687
Iteration 55/1000 | Loss: 0.00015634
Iteration 56/1000 | Loss: 0.00019401
Iteration 57/1000 | Loss: 0.00022791
Iteration 58/1000 | Loss: 0.00009440
Iteration 59/1000 | Loss: 0.00015506
Iteration 60/1000 | Loss: 0.00008799
Iteration 61/1000 | Loss: 0.00008680
Iteration 62/1000 | Loss: 0.00044249
Iteration 63/1000 | Loss: 0.00012856
Iteration 64/1000 | Loss: 0.00039736
Iteration 65/1000 | Loss: 0.00149890
Iteration 66/1000 | Loss: 0.00161292
Iteration 67/1000 | Loss: 0.00015324
Iteration 68/1000 | Loss: 0.00009098
Iteration 69/1000 | Loss: 0.00027728
Iteration 70/1000 | Loss: 0.00036194
Iteration 71/1000 | Loss: 0.00012295
Iteration 72/1000 | Loss: 0.00029301
Iteration 73/1000 | Loss: 0.00044778
Iteration 74/1000 | Loss: 0.00074602
Iteration 75/1000 | Loss: 0.00034243
Iteration 76/1000 | Loss: 0.00074374
Iteration 77/1000 | Loss: 0.00029001
Iteration 78/1000 | Loss: 0.00020963
Iteration 79/1000 | Loss: 0.00012974
Iteration 80/1000 | Loss: 0.00009940
Iteration 81/1000 | Loss: 0.00009109
Iteration 82/1000 | Loss: 0.00010630
Iteration 83/1000 | Loss: 0.00023620
Iteration 84/1000 | Loss: 0.00012891
Iteration 85/1000 | Loss: 0.00022154
Iteration 86/1000 | Loss: 0.00013822
Iteration 87/1000 | Loss: 0.00008586
Iteration 88/1000 | Loss: 0.00008368
Iteration 89/1000 | Loss: 0.00023656
Iteration 90/1000 | Loss: 0.00054543
Iteration 91/1000 | Loss: 0.00011506
Iteration 92/1000 | Loss: 0.00047051
Iteration 93/1000 | Loss: 0.00016618
Iteration 94/1000 | Loss: 0.00008291
Iteration 95/1000 | Loss: 0.00008062
Iteration 96/1000 | Loss: 0.00040568
Iteration 97/1000 | Loss: 0.00008827
Iteration 98/1000 | Loss: 0.00008401
Iteration 99/1000 | Loss: 0.00007971
Iteration 100/1000 | Loss: 0.00007878
Iteration 101/1000 | Loss: 0.00026814
Iteration 102/1000 | Loss: 0.00008483
Iteration 103/1000 | Loss: 0.00007706
Iteration 104/1000 | Loss: 0.00007570
Iteration 105/1000 | Loss: 0.00007509
Iteration 106/1000 | Loss: 0.00007457
Iteration 107/1000 | Loss: 0.00007411
Iteration 108/1000 | Loss: 0.00022477
Iteration 109/1000 | Loss: 0.00020201
Iteration 110/1000 | Loss: 0.00021856
Iteration 111/1000 | Loss: 0.00023655
Iteration 112/1000 | Loss: 0.00020330
Iteration 113/1000 | Loss: 0.00008202
Iteration 114/1000 | Loss: 0.00024236
Iteration 115/1000 | Loss: 0.00048091
Iteration 116/1000 | Loss: 0.00019361
Iteration 117/1000 | Loss: 0.00007880
Iteration 118/1000 | Loss: 0.00007588
Iteration 119/1000 | Loss: 0.00007491
Iteration 120/1000 | Loss: 0.00009173
Iteration 121/1000 | Loss: 0.00008882
Iteration 122/1000 | Loss: 0.00009002
Iteration 123/1000 | Loss: 0.00007424
Iteration 124/1000 | Loss: 0.00007351
Iteration 125/1000 | Loss: 0.00007324
Iteration 126/1000 | Loss: 0.00007290
Iteration 127/1000 | Loss: 0.00007250
Iteration 128/1000 | Loss: 0.00007218
Iteration 129/1000 | Loss: 0.00007200
Iteration 130/1000 | Loss: 0.00007197
Iteration 131/1000 | Loss: 0.00007190
Iteration 132/1000 | Loss: 0.00007179
Iteration 133/1000 | Loss: 0.00007173
Iteration 134/1000 | Loss: 0.00007169
Iteration 135/1000 | Loss: 0.00007168
Iteration 136/1000 | Loss: 0.00007165
Iteration 137/1000 | Loss: 0.00007148
Iteration 138/1000 | Loss: 0.00007125
Iteration 139/1000 | Loss: 0.00007108
Iteration 140/1000 | Loss: 0.00007101
Iteration 141/1000 | Loss: 0.00007099
Iteration 142/1000 | Loss: 0.00007098
Iteration 143/1000 | Loss: 0.00007094
Iteration 144/1000 | Loss: 0.00007094
Iteration 145/1000 | Loss: 0.00007091
Iteration 146/1000 | Loss: 0.00007091
Iteration 147/1000 | Loss: 0.00007090
Iteration 148/1000 | Loss: 0.00007090
Iteration 149/1000 | Loss: 0.00007090
Iteration 150/1000 | Loss: 0.00007089
Iteration 151/1000 | Loss: 0.00007089
Iteration 152/1000 | Loss: 0.00007088
Iteration 153/1000 | Loss: 0.00007088
Iteration 154/1000 | Loss: 0.00007087
Iteration 155/1000 | Loss: 0.00007086
Iteration 156/1000 | Loss: 0.00007085
Iteration 157/1000 | Loss: 0.00007085
Iteration 158/1000 | Loss: 0.00007085
Iteration 159/1000 | Loss: 0.00007084
Iteration 160/1000 | Loss: 0.00007084
Iteration 161/1000 | Loss: 0.00007084
Iteration 162/1000 | Loss: 0.00007084
Iteration 163/1000 | Loss: 0.00007083
Iteration 164/1000 | Loss: 0.00007083
Iteration 165/1000 | Loss: 0.00007083
Iteration 166/1000 | Loss: 0.00007083
Iteration 167/1000 | Loss: 0.00007083
Iteration 168/1000 | Loss: 0.00007083
Iteration 169/1000 | Loss: 0.00007082
Iteration 170/1000 | Loss: 0.00007082
Iteration 171/1000 | Loss: 0.00007082
Iteration 172/1000 | Loss: 0.00007082
Iteration 173/1000 | Loss: 0.00007082
Iteration 174/1000 | Loss: 0.00007082
Iteration 175/1000 | Loss: 0.00007082
Iteration 176/1000 | Loss: 0.00007082
Iteration 177/1000 | Loss: 0.00007082
Iteration 178/1000 | Loss: 0.00007082
Iteration 179/1000 | Loss: 0.00007082
Iteration 180/1000 | Loss: 0.00007081
Iteration 181/1000 | Loss: 0.00007081
Iteration 182/1000 | Loss: 0.00007081
Iteration 183/1000 | Loss: 0.00007081
Iteration 184/1000 | Loss: 0.00007081
Iteration 185/1000 | Loss: 0.00007081
Iteration 186/1000 | Loss: 0.00007081
Iteration 187/1000 | Loss: 0.00007081
Iteration 188/1000 | Loss: 0.00007081
Iteration 189/1000 | Loss: 0.00007081
Iteration 190/1000 | Loss: 0.00007081
Iteration 191/1000 | Loss: 0.00007080
Iteration 192/1000 | Loss: 0.00007080
Iteration 193/1000 | Loss: 0.00007080
Iteration 194/1000 | Loss: 0.00007080
Iteration 195/1000 | Loss: 0.00007080
Iteration 196/1000 | Loss: 0.00007080
Iteration 197/1000 | Loss: 0.00007080
Iteration 198/1000 | Loss: 0.00007080
Iteration 199/1000 | Loss: 0.00007080
Iteration 200/1000 | Loss: 0.00007080
Iteration 201/1000 | Loss: 0.00007080
Iteration 202/1000 | Loss: 0.00007080
Iteration 203/1000 | Loss: 0.00007080
Iteration 204/1000 | Loss: 0.00007080
Iteration 205/1000 | Loss: 0.00007080
Iteration 206/1000 | Loss: 0.00007079
Iteration 207/1000 | Loss: 0.00007079
Iteration 208/1000 | Loss: 0.00007079
Iteration 209/1000 | Loss: 0.00007079
Iteration 210/1000 | Loss: 0.00007079
Iteration 211/1000 | Loss: 0.00007079
Iteration 212/1000 | Loss: 0.00007079
Iteration 213/1000 | Loss: 0.00007078
Iteration 214/1000 | Loss: 0.00007078
Iteration 215/1000 | Loss: 0.00007078
Iteration 216/1000 | Loss: 0.00007078
Iteration 217/1000 | Loss: 0.00007078
Iteration 218/1000 | Loss: 0.00007078
Iteration 219/1000 | Loss: 0.00007077
Iteration 220/1000 | Loss: 0.00007077
Iteration 221/1000 | Loss: 0.00007077
Iteration 222/1000 | Loss: 0.00007077
Iteration 223/1000 | Loss: 0.00007076
Iteration 224/1000 | Loss: 0.00007076
Iteration 225/1000 | Loss: 0.00007075
Iteration 226/1000 | Loss: 0.00007075
Iteration 227/1000 | Loss: 0.00007074
Iteration 228/1000 | Loss: 0.00007074
Iteration 229/1000 | Loss: 0.00007074
Iteration 230/1000 | Loss: 0.00007074
Iteration 231/1000 | Loss: 0.00007074
Iteration 232/1000 | Loss: 0.00007074
Iteration 233/1000 | Loss: 0.00007073
Iteration 234/1000 | Loss: 0.00007073
Iteration 235/1000 | Loss: 0.00007073
Iteration 236/1000 | Loss: 0.00007072
Iteration 237/1000 | Loss: 0.00007072
Iteration 238/1000 | Loss: 0.00007072
Iteration 239/1000 | Loss: 0.00007072
Iteration 240/1000 | Loss: 0.00007072
Iteration 241/1000 | Loss: 0.00007072
Iteration 242/1000 | Loss: 0.00007072
Iteration 243/1000 | Loss: 0.00007072
Iteration 244/1000 | Loss: 0.00007072
Iteration 245/1000 | Loss: 0.00007072
Iteration 246/1000 | Loss: 0.00007072
Iteration 247/1000 | Loss: 0.00007071
Iteration 248/1000 | Loss: 0.00007071
Iteration 249/1000 | Loss: 0.00007071
Iteration 250/1000 | Loss: 0.00007071
Iteration 251/1000 | Loss: 0.00007071
Iteration 252/1000 | Loss: 0.00007071
Iteration 253/1000 | Loss: 0.00007071
Iteration 254/1000 | Loss: 0.00007071
Iteration 255/1000 | Loss: 0.00007070
Iteration 256/1000 | Loss: 0.00007070
Iteration 257/1000 | Loss: 0.00007070
Iteration 258/1000 | Loss: 0.00007070
Iteration 259/1000 | Loss: 0.00007070
Iteration 260/1000 | Loss: 0.00007070
Iteration 261/1000 | Loss: 0.00007070
Iteration 262/1000 | Loss: 0.00007070
Iteration 263/1000 | Loss: 0.00007070
Iteration 264/1000 | Loss: 0.00007070
Iteration 265/1000 | Loss: 0.00007070
Iteration 266/1000 | Loss: 0.00007070
Iteration 267/1000 | Loss: 0.00007070
Iteration 268/1000 | Loss: 0.00007070
Iteration 269/1000 | Loss: 0.00007070
Iteration 270/1000 | Loss: 0.00007069
Iteration 271/1000 | Loss: 0.00007069
Iteration 272/1000 | Loss: 0.00007069
Iteration 273/1000 | Loss: 0.00007069
Iteration 274/1000 | Loss: 0.00007069
Iteration 275/1000 | Loss: 0.00007069
Iteration 276/1000 | Loss: 0.00007069
Iteration 277/1000 | Loss: 0.00007069
Iteration 278/1000 | Loss: 0.00007069
Iteration 279/1000 | Loss: 0.00007069
Iteration 280/1000 | Loss: 0.00007068
Iteration 281/1000 | Loss: 0.00007068
Iteration 282/1000 | Loss: 0.00007068
Iteration 283/1000 | Loss: 0.00007068
Iteration 284/1000 | Loss: 0.00007068
Iteration 285/1000 | Loss: 0.00007068
Iteration 286/1000 | Loss: 0.00007068
Iteration 287/1000 | Loss: 0.00007068
Iteration 288/1000 | Loss: 0.00007067
Iteration 289/1000 | Loss: 0.00007067
Iteration 290/1000 | Loss: 0.00007067
Iteration 291/1000 | Loss: 0.00007067
Iteration 292/1000 | Loss: 0.00007067
Iteration 293/1000 | Loss: 0.00007067
Iteration 294/1000 | Loss: 0.00007067
Iteration 295/1000 | Loss: 0.00007067
Iteration 296/1000 | Loss: 0.00007067
Iteration 297/1000 | Loss: 0.00007067
Iteration 298/1000 | Loss: 0.00007067
Iteration 299/1000 | Loss: 0.00007067
Iteration 300/1000 | Loss: 0.00007067
Iteration 301/1000 | Loss: 0.00007067
Iteration 302/1000 | Loss: 0.00007067
Iteration 303/1000 | Loss: 0.00007067
Iteration 304/1000 | Loss: 0.00007067
Iteration 305/1000 | Loss: 0.00007066
Iteration 306/1000 | Loss: 0.00007066
Iteration 307/1000 | Loss: 0.00007066
Iteration 308/1000 | Loss: 0.00007066
Iteration 309/1000 | Loss: 0.00007066
Iteration 310/1000 | Loss: 0.00007066
Iteration 311/1000 | Loss: 0.00007066
Iteration 312/1000 | Loss: 0.00007066
Iteration 313/1000 | Loss: 0.00007066
Iteration 314/1000 | Loss: 0.00007066
Iteration 315/1000 | Loss: 0.00007065
Iteration 316/1000 | Loss: 0.00007065
Iteration 317/1000 | Loss: 0.00007065
Iteration 318/1000 | Loss: 0.00007065
Iteration 319/1000 | Loss: 0.00007065
Iteration 320/1000 | Loss: 0.00007065
Iteration 321/1000 | Loss: 0.00007065
Iteration 322/1000 | Loss: 0.00007065
Iteration 323/1000 | Loss: 0.00007065
Iteration 324/1000 | Loss: 0.00007065
Iteration 325/1000 | Loss: 0.00007065
Iteration 326/1000 | Loss: 0.00007065
Iteration 327/1000 | Loss: 0.00007065
Iteration 328/1000 | Loss: 0.00007065
Iteration 329/1000 | Loss: 0.00007065
Iteration 330/1000 | Loss: 0.00007065
Iteration 331/1000 | Loss: 0.00007064
Iteration 332/1000 | Loss: 0.00007064
Iteration 333/1000 | Loss: 0.00007064
Iteration 334/1000 | Loss: 0.00007064
Iteration 335/1000 | Loss: 0.00007064
Iteration 336/1000 | Loss: 0.00007064
Iteration 337/1000 | Loss: 0.00007064
Iteration 338/1000 | Loss: 0.00007064
Iteration 339/1000 | Loss: 0.00007064
Iteration 340/1000 | Loss: 0.00007064
Iteration 341/1000 | Loss: 0.00007064
Iteration 342/1000 | Loss: 0.00007064
Iteration 343/1000 | Loss: 0.00007064
Iteration 344/1000 | Loss: 0.00007064
Iteration 345/1000 | Loss: 0.00007064
Iteration 346/1000 | Loss: 0.00007064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 346. Stopping optimization.
Last 5 losses: [7.06380233168602e-05, 7.06380233168602e-05, 7.06380233168602e-05, 7.06380233168602e-05, 7.06380233168602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.06380233168602e-05

Optimization complete. Final v2v error: 4.758627414703369 mm

Highest mean error: 12.160676956176758 mm for frame 49

Lowest mean error: 3.172513961791992 mm for frame 18

Saving results

Total time: 260.309517621994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862360
Iteration 2/25 | Loss: 0.00176805
Iteration 3/25 | Loss: 0.00144618
Iteration 4/25 | Loss: 0.00137075
Iteration 5/25 | Loss: 0.00142386
Iteration 6/25 | Loss: 0.00134230
Iteration 7/25 | Loss: 0.00129378
Iteration 8/25 | Loss: 0.00128528
Iteration 9/25 | Loss: 0.00128094
Iteration 10/25 | Loss: 0.00127021
Iteration 11/25 | Loss: 0.00124131
Iteration 12/25 | Loss: 0.00123528
Iteration 13/25 | Loss: 0.00122809
Iteration 14/25 | Loss: 0.00122959
Iteration 15/25 | Loss: 0.00122610
Iteration 16/25 | Loss: 0.00122324
Iteration 17/25 | Loss: 0.00122217
Iteration 18/25 | Loss: 0.00122203
Iteration 19/25 | Loss: 0.00122248
Iteration 20/25 | Loss: 0.00122138
Iteration 21/25 | Loss: 0.00122059
Iteration 22/25 | Loss: 0.00122038
Iteration 23/25 | Loss: 0.00122037
Iteration 24/25 | Loss: 0.00122036
Iteration 25/25 | Loss: 0.00122036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.46585655
Iteration 2/25 | Loss: 0.00128728
Iteration 3/25 | Loss: 0.00128722
Iteration 4/25 | Loss: 0.00128721
Iteration 5/25 | Loss: 0.00128721
Iteration 6/25 | Loss: 0.00128721
Iteration 7/25 | Loss: 0.00128721
Iteration 8/25 | Loss: 0.00128721
Iteration 9/25 | Loss: 0.00128721
Iteration 10/25 | Loss: 0.00128721
Iteration 11/25 | Loss: 0.00128721
Iteration 12/25 | Loss: 0.00128721
Iteration 13/25 | Loss: 0.00128721
Iteration 14/25 | Loss: 0.00128721
Iteration 15/25 | Loss: 0.00128721
Iteration 16/25 | Loss: 0.00128721
Iteration 17/25 | Loss: 0.00128721
Iteration 18/25 | Loss: 0.00128721
Iteration 19/25 | Loss: 0.00128721
Iteration 20/25 | Loss: 0.00128721
Iteration 21/25 | Loss: 0.00128721
Iteration 22/25 | Loss: 0.00128721
Iteration 23/25 | Loss: 0.00128721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012872111983597279, 0.0012872111983597279, 0.0012872111983597279, 0.0012872111983597279, 0.0012872111983597279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012872111983597279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128721
Iteration 2/1000 | Loss: 0.00005927
Iteration 3/1000 | Loss: 0.00015726
Iteration 4/1000 | Loss: 0.00003034
Iteration 5/1000 | Loss: 0.00004879
Iteration 6/1000 | Loss: 0.00003918
Iteration 7/1000 | Loss: 0.00004154
Iteration 8/1000 | Loss: 0.00003790
Iteration 9/1000 | Loss: 0.00003283
Iteration 10/1000 | Loss: 0.00003059
Iteration 11/1000 | Loss: 0.00002047
Iteration 12/1000 | Loss: 0.00001887
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00001735
Iteration 15/1000 | Loss: 0.00001697
Iteration 16/1000 | Loss: 0.00001670
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001621
Iteration 20/1000 | Loss: 0.00001617
Iteration 21/1000 | Loss: 0.00001601
Iteration 22/1000 | Loss: 0.00001588
Iteration 23/1000 | Loss: 0.00001582
Iteration 24/1000 | Loss: 0.00001581
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001578
Iteration 27/1000 | Loss: 0.00001577
Iteration 28/1000 | Loss: 0.00001577
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001567
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001566
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001562
Iteration 40/1000 | Loss: 0.00001562
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001561
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001556
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001556
Iteration 54/1000 | Loss: 0.00001556
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001556
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001555
Iteration 60/1000 | Loss: 0.00001555
Iteration 61/1000 | Loss: 0.00001555
Iteration 62/1000 | Loss: 0.00001555
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001555
Iteration 65/1000 | Loss: 0.00001554
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001554
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001553
Iteration 75/1000 | Loss: 0.00001553
Iteration 76/1000 | Loss: 0.00001553
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001550
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001550
Iteration 87/1000 | Loss: 0.00001550
Iteration 88/1000 | Loss: 0.00001550
Iteration 89/1000 | Loss: 0.00001550
Iteration 90/1000 | Loss: 0.00001550
Iteration 91/1000 | Loss: 0.00001549
Iteration 92/1000 | Loss: 0.00001549
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001548
Iteration 98/1000 | Loss: 0.00001548
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001547
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001547
Iteration 109/1000 | Loss: 0.00001547
Iteration 110/1000 | Loss: 0.00001547
Iteration 111/1000 | Loss: 0.00001547
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001547
Iteration 115/1000 | Loss: 0.00001547
Iteration 116/1000 | Loss: 0.00001547
Iteration 117/1000 | Loss: 0.00001547
Iteration 118/1000 | Loss: 0.00001546
Iteration 119/1000 | Loss: 0.00001546
Iteration 120/1000 | Loss: 0.00001546
Iteration 121/1000 | Loss: 0.00001546
Iteration 122/1000 | Loss: 0.00001546
Iteration 123/1000 | Loss: 0.00001546
Iteration 124/1000 | Loss: 0.00001546
Iteration 125/1000 | Loss: 0.00001546
Iteration 126/1000 | Loss: 0.00001546
Iteration 127/1000 | Loss: 0.00001545
Iteration 128/1000 | Loss: 0.00001545
Iteration 129/1000 | Loss: 0.00001545
Iteration 130/1000 | Loss: 0.00001545
Iteration 131/1000 | Loss: 0.00001545
Iteration 132/1000 | Loss: 0.00001545
Iteration 133/1000 | Loss: 0.00001544
Iteration 134/1000 | Loss: 0.00001544
Iteration 135/1000 | Loss: 0.00001544
Iteration 136/1000 | Loss: 0.00001544
Iteration 137/1000 | Loss: 0.00001544
Iteration 138/1000 | Loss: 0.00001544
Iteration 139/1000 | Loss: 0.00001543
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001543
Iteration 142/1000 | Loss: 0.00001543
Iteration 143/1000 | Loss: 0.00001543
Iteration 144/1000 | Loss: 0.00001543
Iteration 145/1000 | Loss: 0.00001543
Iteration 146/1000 | Loss: 0.00001542
Iteration 147/1000 | Loss: 0.00001542
Iteration 148/1000 | Loss: 0.00001542
Iteration 149/1000 | Loss: 0.00001542
Iteration 150/1000 | Loss: 0.00001542
Iteration 151/1000 | Loss: 0.00001542
Iteration 152/1000 | Loss: 0.00001541
Iteration 153/1000 | Loss: 0.00001541
Iteration 154/1000 | Loss: 0.00001541
Iteration 155/1000 | Loss: 0.00001541
Iteration 156/1000 | Loss: 0.00001541
Iteration 157/1000 | Loss: 0.00001541
Iteration 158/1000 | Loss: 0.00001541
Iteration 159/1000 | Loss: 0.00001541
Iteration 160/1000 | Loss: 0.00001541
Iteration 161/1000 | Loss: 0.00001540
Iteration 162/1000 | Loss: 0.00001540
Iteration 163/1000 | Loss: 0.00001540
Iteration 164/1000 | Loss: 0.00001540
Iteration 165/1000 | Loss: 0.00001540
Iteration 166/1000 | Loss: 0.00001539
Iteration 167/1000 | Loss: 0.00001539
Iteration 168/1000 | Loss: 0.00001539
Iteration 169/1000 | Loss: 0.00001539
Iteration 170/1000 | Loss: 0.00001538
Iteration 171/1000 | Loss: 0.00001538
Iteration 172/1000 | Loss: 0.00001538
Iteration 173/1000 | Loss: 0.00001538
Iteration 174/1000 | Loss: 0.00001537
Iteration 175/1000 | Loss: 0.00001537
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001537
Iteration 178/1000 | Loss: 0.00001537
Iteration 179/1000 | Loss: 0.00001537
Iteration 180/1000 | Loss: 0.00001536
Iteration 181/1000 | Loss: 0.00001536
Iteration 182/1000 | Loss: 0.00001536
Iteration 183/1000 | Loss: 0.00001536
Iteration 184/1000 | Loss: 0.00001536
Iteration 185/1000 | Loss: 0.00001536
Iteration 186/1000 | Loss: 0.00001536
Iteration 187/1000 | Loss: 0.00001535
Iteration 188/1000 | Loss: 0.00001535
Iteration 189/1000 | Loss: 0.00001535
Iteration 190/1000 | Loss: 0.00001535
Iteration 191/1000 | Loss: 0.00001535
Iteration 192/1000 | Loss: 0.00001535
Iteration 193/1000 | Loss: 0.00001535
Iteration 194/1000 | Loss: 0.00001535
Iteration 195/1000 | Loss: 0.00001535
Iteration 196/1000 | Loss: 0.00001535
Iteration 197/1000 | Loss: 0.00001534
Iteration 198/1000 | Loss: 0.00001534
Iteration 199/1000 | Loss: 0.00001534
Iteration 200/1000 | Loss: 0.00001534
Iteration 201/1000 | Loss: 0.00001534
Iteration 202/1000 | Loss: 0.00001534
Iteration 203/1000 | Loss: 0.00001534
Iteration 204/1000 | Loss: 0.00001534
Iteration 205/1000 | Loss: 0.00001534
Iteration 206/1000 | Loss: 0.00001534
Iteration 207/1000 | Loss: 0.00001534
Iteration 208/1000 | Loss: 0.00001534
Iteration 209/1000 | Loss: 0.00001534
Iteration 210/1000 | Loss: 0.00001534
Iteration 211/1000 | Loss: 0.00001534
Iteration 212/1000 | Loss: 0.00001534
Iteration 213/1000 | Loss: 0.00001534
Iteration 214/1000 | Loss: 0.00001534
Iteration 215/1000 | Loss: 0.00001533
Iteration 216/1000 | Loss: 0.00001533
Iteration 217/1000 | Loss: 0.00001533
Iteration 218/1000 | Loss: 0.00001533
Iteration 219/1000 | Loss: 0.00001533
Iteration 220/1000 | Loss: 0.00001532
Iteration 221/1000 | Loss: 0.00001532
Iteration 222/1000 | Loss: 0.00001532
Iteration 223/1000 | Loss: 0.00001532
Iteration 224/1000 | Loss: 0.00001532
Iteration 225/1000 | Loss: 0.00001532
Iteration 226/1000 | Loss: 0.00001532
Iteration 227/1000 | Loss: 0.00001532
Iteration 228/1000 | Loss: 0.00001532
Iteration 229/1000 | Loss: 0.00001531
Iteration 230/1000 | Loss: 0.00001531
Iteration 231/1000 | Loss: 0.00001531
Iteration 232/1000 | Loss: 0.00001531
Iteration 233/1000 | Loss: 0.00001531
Iteration 234/1000 | Loss: 0.00001531
Iteration 235/1000 | Loss: 0.00001531
Iteration 236/1000 | Loss: 0.00001530
Iteration 237/1000 | Loss: 0.00001530
Iteration 238/1000 | Loss: 0.00001530
Iteration 239/1000 | Loss: 0.00001530
Iteration 240/1000 | Loss: 0.00001530
Iteration 241/1000 | Loss: 0.00001530
Iteration 242/1000 | Loss: 0.00001530
Iteration 243/1000 | Loss: 0.00001530
Iteration 244/1000 | Loss: 0.00001530
Iteration 245/1000 | Loss: 0.00001530
Iteration 246/1000 | Loss: 0.00001530
Iteration 247/1000 | Loss: 0.00001530
Iteration 248/1000 | Loss: 0.00001530
Iteration 249/1000 | Loss: 0.00001530
Iteration 250/1000 | Loss: 0.00001530
Iteration 251/1000 | Loss: 0.00001529
Iteration 252/1000 | Loss: 0.00001529
Iteration 253/1000 | Loss: 0.00001529
Iteration 254/1000 | Loss: 0.00001529
Iteration 255/1000 | Loss: 0.00001529
Iteration 256/1000 | Loss: 0.00001529
Iteration 257/1000 | Loss: 0.00001529
Iteration 258/1000 | Loss: 0.00001529
Iteration 259/1000 | Loss: 0.00001528
Iteration 260/1000 | Loss: 0.00001528
Iteration 261/1000 | Loss: 0.00001528
Iteration 262/1000 | Loss: 0.00001528
Iteration 263/1000 | Loss: 0.00001527
Iteration 264/1000 | Loss: 0.00001527
Iteration 265/1000 | Loss: 0.00001527
Iteration 266/1000 | Loss: 0.00001527
Iteration 267/1000 | Loss: 0.00001527
Iteration 268/1000 | Loss: 0.00001527
Iteration 269/1000 | Loss: 0.00001527
Iteration 270/1000 | Loss: 0.00001527
Iteration 271/1000 | Loss: 0.00001527
Iteration 272/1000 | Loss: 0.00001526
Iteration 273/1000 | Loss: 0.00001526
Iteration 274/1000 | Loss: 0.00001526
Iteration 275/1000 | Loss: 0.00001526
Iteration 276/1000 | Loss: 0.00001526
Iteration 277/1000 | Loss: 0.00001526
Iteration 278/1000 | Loss: 0.00001526
Iteration 279/1000 | Loss: 0.00001526
Iteration 280/1000 | Loss: 0.00001526
Iteration 281/1000 | Loss: 0.00001526
Iteration 282/1000 | Loss: 0.00001526
Iteration 283/1000 | Loss: 0.00001526
Iteration 284/1000 | Loss: 0.00001526
Iteration 285/1000 | Loss: 0.00001526
Iteration 286/1000 | Loss: 0.00001526
Iteration 287/1000 | Loss: 0.00001526
Iteration 288/1000 | Loss: 0.00001526
Iteration 289/1000 | Loss: 0.00001526
Iteration 290/1000 | Loss: 0.00001526
Iteration 291/1000 | Loss: 0.00001526
Iteration 292/1000 | Loss: 0.00001526
Iteration 293/1000 | Loss: 0.00001525
Iteration 294/1000 | Loss: 0.00001525
Iteration 295/1000 | Loss: 0.00001525
Iteration 296/1000 | Loss: 0.00001525
Iteration 297/1000 | Loss: 0.00001525
Iteration 298/1000 | Loss: 0.00001525
Iteration 299/1000 | Loss: 0.00001525
Iteration 300/1000 | Loss: 0.00001525
Iteration 301/1000 | Loss: 0.00001525
Iteration 302/1000 | Loss: 0.00001525
Iteration 303/1000 | Loss: 0.00001525
Iteration 304/1000 | Loss: 0.00001525
Iteration 305/1000 | Loss: 0.00001525
Iteration 306/1000 | Loss: 0.00001525
Iteration 307/1000 | Loss: 0.00001525
Iteration 308/1000 | Loss: 0.00001525
Iteration 309/1000 | Loss: 0.00001525
Iteration 310/1000 | Loss: 0.00001525
Iteration 311/1000 | Loss: 0.00001525
Iteration 312/1000 | Loss: 0.00001525
Iteration 313/1000 | Loss: 0.00001525
Iteration 314/1000 | Loss: 0.00001525
Iteration 315/1000 | Loss: 0.00001525
Iteration 316/1000 | Loss: 0.00001525
Iteration 317/1000 | Loss: 0.00001525
Iteration 318/1000 | Loss: 0.00001525
Iteration 319/1000 | Loss: 0.00001525
Iteration 320/1000 | Loss: 0.00001525
Iteration 321/1000 | Loss: 0.00001525
Iteration 322/1000 | Loss: 0.00001525
Iteration 323/1000 | Loss: 0.00001525
Iteration 324/1000 | Loss: 0.00001525
Iteration 325/1000 | Loss: 0.00001525
Iteration 326/1000 | Loss: 0.00001525
Iteration 327/1000 | Loss: 0.00001525
Iteration 328/1000 | Loss: 0.00001525
Iteration 329/1000 | Loss: 0.00001525
Iteration 330/1000 | Loss: 0.00001525
Iteration 331/1000 | Loss: 0.00001525
Iteration 332/1000 | Loss: 0.00001525
Iteration 333/1000 | Loss: 0.00001525
Iteration 334/1000 | Loss: 0.00001525
Iteration 335/1000 | Loss: 0.00001525
Iteration 336/1000 | Loss: 0.00001525
Iteration 337/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [1.5252679986588191e-05, 1.5252679986588191e-05, 1.5252679986588191e-05, 1.5252679986588191e-05, 1.5252679986588191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5252679986588191e-05

Optimization complete. Final v2v error: 3.2551991939544678 mm

Highest mean error: 3.8553466796875 mm for frame 104

Lowest mean error: 2.828251361846924 mm for frame 52

Saving results

Total time: 93.25800561904907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421392
Iteration 2/25 | Loss: 0.00125410
Iteration 3/25 | Loss: 0.00117601
Iteration 4/25 | Loss: 0.00115632
Iteration 5/25 | Loss: 0.00114924
Iteration 6/25 | Loss: 0.00114753
Iteration 7/25 | Loss: 0.00114753
Iteration 8/25 | Loss: 0.00114753
Iteration 9/25 | Loss: 0.00114753
Iteration 10/25 | Loss: 0.00114753
Iteration 11/25 | Loss: 0.00114753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011475294595584273, 0.0011475294595584273, 0.0011475294595584273, 0.0011475294595584273, 0.0011475294595584273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011475294595584273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30631912
Iteration 2/25 | Loss: 0.00120254
Iteration 3/25 | Loss: 0.00120254
Iteration 4/25 | Loss: 0.00120254
Iteration 5/25 | Loss: 0.00120254
Iteration 6/25 | Loss: 0.00120254
Iteration 7/25 | Loss: 0.00120254
Iteration 8/25 | Loss: 0.00120254
Iteration 9/25 | Loss: 0.00120254
Iteration 10/25 | Loss: 0.00120254
Iteration 11/25 | Loss: 0.00120254
Iteration 12/25 | Loss: 0.00120254
Iteration 13/25 | Loss: 0.00120254
Iteration 14/25 | Loss: 0.00120254
Iteration 15/25 | Loss: 0.00120254
Iteration 16/25 | Loss: 0.00120254
Iteration 17/25 | Loss: 0.00120254
Iteration 18/25 | Loss: 0.00120254
Iteration 19/25 | Loss: 0.00120254
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012025401229038835, 0.0012025401229038835, 0.0012025401229038835, 0.0012025401229038835, 0.0012025401229038835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012025401229038835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120254
Iteration 2/1000 | Loss: 0.00002504
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001417
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001263
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001207
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001180
Iteration 25/1000 | Loss: 0.00001180
Iteration 26/1000 | Loss: 0.00001179
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001162
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001160
Iteration 71/1000 | Loss: 0.00001160
Iteration 72/1000 | Loss: 0.00001160
Iteration 73/1000 | Loss: 0.00001159
Iteration 74/1000 | Loss: 0.00001159
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001157
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001156
Iteration 87/1000 | Loss: 0.00001156
Iteration 88/1000 | Loss: 0.00001156
Iteration 89/1000 | Loss: 0.00001155
Iteration 90/1000 | Loss: 0.00001155
Iteration 91/1000 | Loss: 0.00001155
Iteration 92/1000 | Loss: 0.00001155
Iteration 93/1000 | Loss: 0.00001155
Iteration 94/1000 | Loss: 0.00001155
Iteration 95/1000 | Loss: 0.00001154
Iteration 96/1000 | Loss: 0.00001154
Iteration 97/1000 | Loss: 0.00001154
Iteration 98/1000 | Loss: 0.00001154
Iteration 99/1000 | Loss: 0.00001154
Iteration 100/1000 | Loss: 0.00001154
Iteration 101/1000 | Loss: 0.00001154
Iteration 102/1000 | Loss: 0.00001154
Iteration 103/1000 | Loss: 0.00001154
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001152
Iteration 110/1000 | Loss: 0.00001152
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001151
Iteration 117/1000 | Loss: 0.00001151
Iteration 118/1000 | Loss: 0.00001151
Iteration 119/1000 | Loss: 0.00001151
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001150
Iteration 122/1000 | Loss: 0.00001150
Iteration 123/1000 | Loss: 0.00001149
Iteration 124/1000 | Loss: 0.00001149
Iteration 125/1000 | Loss: 0.00001149
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001148
Iteration 128/1000 | Loss: 0.00001148
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001148
Iteration 131/1000 | Loss: 0.00001148
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001148
Iteration 137/1000 | Loss: 0.00001148
Iteration 138/1000 | Loss: 0.00001148
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001147
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001147
Iteration 147/1000 | Loss: 0.00001147
Iteration 148/1000 | Loss: 0.00001147
Iteration 149/1000 | Loss: 0.00001146
Iteration 150/1000 | Loss: 0.00001146
Iteration 151/1000 | Loss: 0.00001146
Iteration 152/1000 | Loss: 0.00001146
Iteration 153/1000 | Loss: 0.00001146
Iteration 154/1000 | Loss: 0.00001146
Iteration 155/1000 | Loss: 0.00001146
Iteration 156/1000 | Loss: 0.00001146
Iteration 157/1000 | Loss: 0.00001146
Iteration 158/1000 | Loss: 0.00001146
Iteration 159/1000 | Loss: 0.00001146
Iteration 160/1000 | Loss: 0.00001146
Iteration 161/1000 | Loss: 0.00001146
Iteration 162/1000 | Loss: 0.00001146
Iteration 163/1000 | Loss: 0.00001146
Iteration 164/1000 | Loss: 0.00001146
Iteration 165/1000 | Loss: 0.00001146
Iteration 166/1000 | Loss: 0.00001146
Iteration 167/1000 | Loss: 0.00001146
Iteration 168/1000 | Loss: 0.00001146
Iteration 169/1000 | Loss: 0.00001145
Iteration 170/1000 | Loss: 0.00001145
Iteration 171/1000 | Loss: 0.00001145
Iteration 172/1000 | Loss: 0.00001145
Iteration 173/1000 | Loss: 0.00001145
Iteration 174/1000 | Loss: 0.00001145
Iteration 175/1000 | Loss: 0.00001145
Iteration 176/1000 | Loss: 0.00001145
Iteration 177/1000 | Loss: 0.00001145
Iteration 178/1000 | Loss: 0.00001145
Iteration 179/1000 | Loss: 0.00001145
Iteration 180/1000 | Loss: 0.00001145
Iteration 181/1000 | Loss: 0.00001145
Iteration 182/1000 | Loss: 0.00001145
Iteration 183/1000 | Loss: 0.00001145
Iteration 184/1000 | Loss: 0.00001145
Iteration 185/1000 | Loss: 0.00001145
Iteration 186/1000 | Loss: 0.00001145
Iteration 187/1000 | Loss: 0.00001145
Iteration 188/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.1449186786194332e-05, 1.1449186786194332e-05, 1.1449186786194332e-05, 1.1449186786194332e-05, 1.1449186786194332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1449186786194332e-05

Optimization complete. Final v2v error: 2.9474663734436035 mm

Highest mean error: 3.1824419498443604 mm for frame 104

Lowest mean error: 2.7828097343444824 mm for frame 158

Saving results

Total time: 39.83856725692749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051742
Iteration 2/25 | Loss: 0.01051742
Iteration 3/25 | Loss: 0.01051742
Iteration 4/25 | Loss: 0.01051741
Iteration 5/25 | Loss: 0.00238666
Iteration 6/25 | Loss: 0.00177570
Iteration 7/25 | Loss: 0.00137496
Iteration 8/25 | Loss: 0.00131535
Iteration 9/25 | Loss: 0.00129731
Iteration 10/25 | Loss: 0.00128376
Iteration 11/25 | Loss: 0.00127830
Iteration 12/25 | Loss: 0.00128279
Iteration 13/25 | Loss: 0.00128149
Iteration 14/25 | Loss: 0.00127393
Iteration 15/25 | Loss: 0.00127263
Iteration 16/25 | Loss: 0.00127214
Iteration 17/25 | Loss: 0.00127140
Iteration 18/25 | Loss: 0.00127105
Iteration 19/25 | Loss: 0.00127348
Iteration 20/25 | Loss: 0.00127271
Iteration 21/25 | Loss: 0.00127101
Iteration 22/25 | Loss: 0.00126878
Iteration 23/25 | Loss: 0.00126666
Iteration 24/25 | Loss: 0.00126587
Iteration 25/25 | Loss: 0.00126578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27210510
Iteration 2/25 | Loss: 0.00199486
Iteration 3/25 | Loss: 0.00199486
Iteration 4/25 | Loss: 0.00199485
Iteration 5/25 | Loss: 0.00199485
Iteration 6/25 | Loss: 0.00199485
Iteration 7/25 | Loss: 0.00199485
Iteration 8/25 | Loss: 0.00199485
Iteration 9/25 | Loss: 0.00199485
Iteration 10/25 | Loss: 0.00199485
Iteration 11/25 | Loss: 0.00199485
Iteration 12/25 | Loss: 0.00199485
Iteration 13/25 | Loss: 0.00199485
Iteration 14/25 | Loss: 0.00199485
Iteration 15/25 | Loss: 0.00199485
Iteration 16/25 | Loss: 0.00199485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019948517438024282, 0.0019948517438024282, 0.0019948517438024282, 0.0019948517438024282, 0.0019948517438024282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019948517438024282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199485
Iteration 2/1000 | Loss: 0.00009436
Iteration 3/1000 | Loss: 0.00005098
Iteration 4/1000 | Loss: 0.00003971
Iteration 5/1000 | Loss: 0.00003901
Iteration 6/1000 | Loss: 0.00003346
Iteration 7/1000 | Loss: 0.00003316
Iteration 8/1000 | Loss: 0.00003242
Iteration 9/1000 | Loss: 0.00003100
Iteration 10/1000 | Loss: 0.00003062
Iteration 11/1000 | Loss: 0.00019700
Iteration 12/1000 | Loss: 0.00008121
Iteration 13/1000 | Loss: 0.00003611
Iteration 14/1000 | Loss: 0.00003180
Iteration 15/1000 | Loss: 0.00003053
Iteration 16/1000 | Loss: 0.00002992
Iteration 17/1000 | Loss: 0.00002974
Iteration 18/1000 | Loss: 0.00016644
Iteration 19/1000 | Loss: 0.00003022
Iteration 20/1000 | Loss: 0.00002968
Iteration 21/1000 | Loss: 0.00002959
Iteration 22/1000 | Loss: 0.00002947
Iteration 23/1000 | Loss: 0.00002934
Iteration 24/1000 | Loss: 0.00002933
Iteration 25/1000 | Loss: 0.00002932
Iteration 26/1000 | Loss: 0.00002932
Iteration 27/1000 | Loss: 0.00002931
Iteration 28/1000 | Loss: 0.00002931
Iteration 29/1000 | Loss: 0.00002931
Iteration 30/1000 | Loss: 0.00002931
Iteration 31/1000 | Loss: 0.00002930
Iteration 32/1000 | Loss: 0.00002930
Iteration 33/1000 | Loss: 0.00002930
Iteration 34/1000 | Loss: 0.00002929
Iteration 35/1000 | Loss: 0.00002929
Iteration 36/1000 | Loss: 0.00002929
Iteration 37/1000 | Loss: 0.00002928
Iteration 38/1000 | Loss: 0.00002927
Iteration 39/1000 | Loss: 0.00002925
Iteration 40/1000 | Loss: 0.00002924
Iteration 41/1000 | Loss: 0.00002924
Iteration 42/1000 | Loss: 0.00002923
Iteration 43/1000 | Loss: 0.00002923
Iteration 44/1000 | Loss: 0.00002922
Iteration 45/1000 | Loss: 0.00002921
Iteration 46/1000 | Loss: 0.00002905
Iteration 47/1000 | Loss: 0.00002904
Iteration 48/1000 | Loss: 0.00002903
Iteration 49/1000 | Loss: 0.00002902
Iteration 50/1000 | Loss: 0.00002900
Iteration 51/1000 | Loss: 0.00002899
Iteration 52/1000 | Loss: 0.00002899
Iteration 53/1000 | Loss: 0.00002895
Iteration 54/1000 | Loss: 0.00002894
Iteration 55/1000 | Loss: 0.00002891
Iteration 56/1000 | Loss: 0.00002890
Iteration 57/1000 | Loss: 0.00002890
Iteration 58/1000 | Loss: 0.00002890
Iteration 59/1000 | Loss: 0.00002890
Iteration 60/1000 | Loss: 0.00002889
Iteration 61/1000 | Loss: 0.00002888
Iteration 62/1000 | Loss: 0.00002887
Iteration 63/1000 | Loss: 0.00002887
Iteration 64/1000 | Loss: 0.00002887
Iteration 65/1000 | Loss: 0.00002887
Iteration 66/1000 | Loss: 0.00002886
Iteration 67/1000 | Loss: 0.00002886
Iteration 68/1000 | Loss: 0.00002886
Iteration 69/1000 | Loss: 0.00002885
Iteration 70/1000 | Loss: 0.00020725
Iteration 71/1000 | Loss: 0.00020724
Iteration 72/1000 | Loss: 0.00006175
Iteration 73/1000 | Loss: 0.00003772
Iteration 74/1000 | Loss: 0.00003033
Iteration 75/1000 | Loss: 0.00002958
Iteration 76/1000 | Loss: 0.00003171
Iteration 77/1000 | Loss: 0.00002879
Iteration 78/1000 | Loss: 0.00012776
Iteration 79/1000 | Loss: 0.00003092
Iteration 80/1000 | Loss: 0.00002983
Iteration 81/1000 | Loss: 0.00002894
Iteration 82/1000 | Loss: 0.00013169
Iteration 83/1000 | Loss: 0.00003002
Iteration 84/1000 | Loss: 0.00002884
Iteration 85/1000 | Loss: 0.00002880
Iteration 86/1000 | Loss: 0.00002880
Iteration 87/1000 | Loss: 0.00002879
Iteration 88/1000 | Loss: 0.00002878
Iteration 89/1000 | Loss: 0.00002878
Iteration 90/1000 | Loss: 0.00002878
Iteration 91/1000 | Loss: 0.00002877
Iteration 92/1000 | Loss: 0.00002876
Iteration 93/1000 | Loss: 0.00002876
Iteration 94/1000 | Loss: 0.00002876
Iteration 95/1000 | Loss: 0.00002876
Iteration 96/1000 | Loss: 0.00002876
Iteration 97/1000 | Loss: 0.00002876
Iteration 98/1000 | Loss: 0.00002876
Iteration 99/1000 | Loss: 0.00002876
Iteration 100/1000 | Loss: 0.00002876
Iteration 101/1000 | Loss: 0.00002876
Iteration 102/1000 | Loss: 0.00002876
Iteration 103/1000 | Loss: 0.00002876
Iteration 104/1000 | Loss: 0.00002876
Iteration 105/1000 | Loss: 0.00002875
Iteration 106/1000 | Loss: 0.00002875
Iteration 107/1000 | Loss: 0.00002875
Iteration 108/1000 | Loss: 0.00002875
Iteration 109/1000 | Loss: 0.00002875
Iteration 110/1000 | Loss: 0.00002875
Iteration 111/1000 | Loss: 0.00002875
Iteration 112/1000 | Loss: 0.00002875
Iteration 113/1000 | Loss: 0.00002875
Iteration 114/1000 | Loss: 0.00002875
Iteration 115/1000 | Loss: 0.00002875
Iteration 116/1000 | Loss: 0.00002874
Iteration 117/1000 | Loss: 0.00002874
Iteration 118/1000 | Loss: 0.00002874
Iteration 119/1000 | Loss: 0.00002873
Iteration 120/1000 | Loss: 0.00002873
Iteration 121/1000 | Loss: 0.00002873
Iteration 122/1000 | Loss: 0.00002873
Iteration 123/1000 | Loss: 0.00002873
Iteration 124/1000 | Loss: 0.00002873
Iteration 125/1000 | Loss: 0.00002873
Iteration 126/1000 | Loss: 0.00002873
Iteration 127/1000 | Loss: 0.00002873
Iteration 128/1000 | Loss: 0.00002872
Iteration 129/1000 | Loss: 0.00002872
Iteration 130/1000 | Loss: 0.00002872
Iteration 131/1000 | Loss: 0.00002872
Iteration 132/1000 | Loss: 0.00002872
Iteration 133/1000 | Loss: 0.00002872
Iteration 134/1000 | Loss: 0.00002872
Iteration 135/1000 | Loss: 0.00002872
Iteration 136/1000 | Loss: 0.00002872
Iteration 137/1000 | Loss: 0.00002871
Iteration 138/1000 | Loss: 0.00002871
Iteration 139/1000 | Loss: 0.00002871
Iteration 140/1000 | Loss: 0.00002871
Iteration 141/1000 | Loss: 0.00002871
Iteration 142/1000 | Loss: 0.00002871
Iteration 143/1000 | Loss: 0.00002871
Iteration 144/1000 | Loss: 0.00002871
Iteration 145/1000 | Loss: 0.00002871
Iteration 146/1000 | Loss: 0.00002871
Iteration 147/1000 | Loss: 0.00002871
Iteration 148/1000 | Loss: 0.00002871
Iteration 149/1000 | Loss: 0.00002871
Iteration 150/1000 | Loss: 0.00002871
Iteration 151/1000 | Loss: 0.00002870
Iteration 152/1000 | Loss: 0.00002870
Iteration 153/1000 | Loss: 0.00002870
Iteration 154/1000 | Loss: 0.00002870
Iteration 155/1000 | Loss: 0.00002869
Iteration 156/1000 | Loss: 0.00002869
Iteration 157/1000 | Loss: 0.00002869
Iteration 158/1000 | Loss: 0.00002869
Iteration 159/1000 | Loss: 0.00002869
Iteration 160/1000 | Loss: 0.00002869
Iteration 161/1000 | Loss: 0.00002869
Iteration 162/1000 | Loss: 0.00002869
Iteration 163/1000 | Loss: 0.00002869
Iteration 164/1000 | Loss: 0.00002868
Iteration 165/1000 | Loss: 0.00002868
Iteration 166/1000 | Loss: 0.00002868
Iteration 167/1000 | Loss: 0.00002868
Iteration 168/1000 | Loss: 0.00002868
Iteration 169/1000 | Loss: 0.00002868
Iteration 170/1000 | Loss: 0.00002868
Iteration 171/1000 | Loss: 0.00002868
Iteration 172/1000 | Loss: 0.00002868
Iteration 173/1000 | Loss: 0.00002868
Iteration 174/1000 | Loss: 0.00002868
Iteration 175/1000 | Loss: 0.00002868
Iteration 176/1000 | Loss: 0.00002868
Iteration 177/1000 | Loss: 0.00002868
Iteration 178/1000 | Loss: 0.00002868
Iteration 179/1000 | Loss: 0.00002868
Iteration 180/1000 | Loss: 0.00002867
Iteration 181/1000 | Loss: 0.00002867
Iteration 182/1000 | Loss: 0.00002867
Iteration 183/1000 | Loss: 0.00002867
Iteration 184/1000 | Loss: 0.00002867
Iteration 185/1000 | Loss: 0.00002867
Iteration 186/1000 | Loss: 0.00002867
Iteration 187/1000 | Loss: 0.00002867
Iteration 188/1000 | Loss: 0.00002867
Iteration 189/1000 | Loss: 0.00002867
Iteration 190/1000 | Loss: 0.00002867
Iteration 191/1000 | Loss: 0.00002867
Iteration 192/1000 | Loss: 0.00002867
Iteration 193/1000 | Loss: 0.00002867
Iteration 194/1000 | Loss: 0.00002867
Iteration 195/1000 | Loss: 0.00002867
Iteration 196/1000 | Loss: 0.00002867
Iteration 197/1000 | Loss: 0.00002867
Iteration 198/1000 | Loss: 0.00002867
Iteration 199/1000 | Loss: 0.00002867
Iteration 200/1000 | Loss: 0.00002867
Iteration 201/1000 | Loss: 0.00002867
Iteration 202/1000 | Loss: 0.00002867
Iteration 203/1000 | Loss: 0.00002867
Iteration 204/1000 | Loss: 0.00002867
Iteration 205/1000 | Loss: 0.00002867
Iteration 206/1000 | Loss: 0.00002867
Iteration 207/1000 | Loss: 0.00002867
Iteration 208/1000 | Loss: 0.00002867
Iteration 209/1000 | Loss: 0.00002867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.8667733204201795e-05, 2.8667733204201795e-05, 2.8667733204201795e-05, 2.8667733204201795e-05, 2.8667733204201795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8667733204201795e-05

Optimization complete. Final v2v error: 4.427399635314941 mm

Highest mean error: 16.414886474609375 mm for frame 138

Lowest mean error: 3.2541263103485107 mm for frame 0

Saving results

Total time: 104.81218123435974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370942
Iteration 2/25 | Loss: 0.00119522
Iteration 3/25 | Loss: 0.00112684
Iteration 4/25 | Loss: 0.00111671
Iteration 5/25 | Loss: 0.00111362
Iteration 6/25 | Loss: 0.00111303
Iteration 7/25 | Loss: 0.00111303
Iteration 8/25 | Loss: 0.00111303
Iteration 9/25 | Loss: 0.00111303
Iteration 10/25 | Loss: 0.00111303
Iteration 11/25 | Loss: 0.00111303
Iteration 12/25 | Loss: 0.00111303
Iteration 13/25 | Loss: 0.00111303
Iteration 14/25 | Loss: 0.00111303
Iteration 15/25 | Loss: 0.00111303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011130310595035553, 0.0011130310595035553, 0.0011130310595035553, 0.0011130310595035553, 0.0011130310595035553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011130310595035553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39354873
Iteration 2/25 | Loss: 0.00131287
Iteration 3/25 | Loss: 0.00131287
Iteration 4/25 | Loss: 0.00131287
Iteration 5/25 | Loss: 0.00131287
Iteration 6/25 | Loss: 0.00131287
Iteration 7/25 | Loss: 0.00131287
Iteration 8/25 | Loss: 0.00131287
Iteration 9/25 | Loss: 0.00131287
Iteration 10/25 | Loss: 0.00131287
Iteration 11/25 | Loss: 0.00131287
Iteration 12/25 | Loss: 0.00131287
Iteration 13/25 | Loss: 0.00131287
Iteration 14/25 | Loss: 0.00131287
Iteration 15/25 | Loss: 0.00131287
Iteration 16/25 | Loss: 0.00131287
Iteration 17/25 | Loss: 0.00131287
Iteration 18/25 | Loss: 0.00131287
Iteration 19/25 | Loss: 0.00131287
Iteration 20/25 | Loss: 0.00131287
Iteration 21/25 | Loss: 0.00131287
Iteration 22/25 | Loss: 0.00131287
Iteration 23/25 | Loss: 0.00131287
Iteration 24/25 | Loss: 0.00131287
Iteration 25/25 | Loss: 0.00131287
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013128696009516716, 0.0013128696009516716, 0.0013128696009516716, 0.0013128696009516716, 0.0013128696009516716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013128696009516716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131287
Iteration 2/1000 | Loss: 0.00001687
Iteration 3/1000 | Loss: 0.00001166
Iteration 4/1000 | Loss: 0.00001042
Iteration 5/1000 | Loss: 0.00000989
Iteration 6/1000 | Loss: 0.00000941
Iteration 7/1000 | Loss: 0.00000900
Iteration 8/1000 | Loss: 0.00000891
Iteration 9/1000 | Loss: 0.00000871
Iteration 10/1000 | Loss: 0.00000862
Iteration 11/1000 | Loss: 0.00000841
Iteration 12/1000 | Loss: 0.00000841
Iteration 13/1000 | Loss: 0.00000840
Iteration 14/1000 | Loss: 0.00000826
Iteration 15/1000 | Loss: 0.00000826
Iteration 16/1000 | Loss: 0.00000826
Iteration 17/1000 | Loss: 0.00000825
Iteration 18/1000 | Loss: 0.00000825
Iteration 19/1000 | Loss: 0.00000824
Iteration 20/1000 | Loss: 0.00000815
Iteration 21/1000 | Loss: 0.00000810
Iteration 22/1000 | Loss: 0.00000810
Iteration 23/1000 | Loss: 0.00000810
Iteration 24/1000 | Loss: 0.00000810
Iteration 25/1000 | Loss: 0.00000810
Iteration 26/1000 | Loss: 0.00000810
Iteration 27/1000 | Loss: 0.00000810
Iteration 28/1000 | Loss: 0.00000809
Iteration 29/1000 | Loss: 0.00000809
Iteration 30/1000 | Loss: 0.00000808
Iteration 31/1000 | Loss: 0.00000805
Iteration 32/1000 | Loss: 0.00000805
Iteration 33/1000 | Loss: 0.00000805
Iteration 34/1000 | Loss: 0.00000805
Iteration 35/1000 | Loss: 0.00000804
Iteration 36/1000 | Loss: 0.00000804
Iteration 37/1000 | Loss: 0.00000804
Iteration 38/1000 | Loss: 0.00000803
Iteration 39/1000 | Loss: 0.00000803
Iteration 40/1000 | Loss: 0.00000802
Iteration 41/1000 | Loss: 0.00000802
Iteration 42/1000 | Loss: 0.00000801
Iteration 43/1000 | Loss: 0.00000801
Iteration 44/1000 | Loss: 0.00000800
Iteration 45/1000 | Loss: 0.00000800
Iteration 46/1000 | Loss: 0.00000799
Iteration 47/1000 | Loss: 0.00000794
Iteration 48/1000 | Loss: 0.00000794
Iteration 49/1000 | Loss: 0.00000794
Iteration 50/1000 | Loss: 0.00000794
Iteration 51/1000 | Loss: 0.00000794
Iteration 52/1000 | Loss: 0.00000793
Iteration 53/1000 | Loss: 0.00000792
Iteration 54/1000 | Loss: 0.00000792
Iteration 55/1000 | Loss: 0.00000791
Iteration 56/1000 | Loss: 0.00000790
Iteration 57/1000 | Loss: 0.00000790
Iteration 58/1000 | Loss: 0.00000790
Iteration 59/1000 | Loss: 0.00000790
Iteration 60/1000 | Loss: 0.00000790
Iteration 61/1000 | Loss: 0.00000789
Iteration 62/1000 | Loss: 0.00000788
Iteration 63/1000 | Loss: 0.00000788
Iteration 64/1000 | Loss: 0.00000788
Iteration 65/1000 | Loss: 0.00000787
Iteration 66/1000 | Loss: 0.00000787
Iteration 67/1000 | Loss: 0.00000787
Iteration 68/1000 | Loss: 0.00000787
Iteration 69/1000 | Loss: 0.00000787
Iteration 70/1000 | Loss: 0.00000787
Iteration 71/1000 | Loss: 0.00000785
Iteration 72/1000 | Loss: 0.00000785
Iteration 73/1000 | Loss: 0.00000784
Iteration 74/1000 | Loss: 0.00000784
Iteration 75/1000 | Loss: 0.00000784
Iteration 76/1000 | Loss: 0.00000784
Iteration 77/1000 | Loss: 0.00000784
Iteration 78/1000 | Loss: 0.00000784
Iteration 79/1000 | Loss: 0.00000784
Iteration 80/1000 | Loss: 0.00000784
Iteration 81/1000 | Loss: 0.00000784
Iteration 82/1000 | Loss: 0.00000783
Iteration 83/1000 | Loss: 0.00000783
Iteration 84/1000 | Loss: 0.00000783
Iteration 85/1000 | Loss: 0.00000783
Iteration 86/1000 | Loss: 0.00000783
Iteration 87/1000 | Loss: 0.00000782
Iteration 88/1000 | Loss: 0.00000782
Iteration 89/1000 | Loss: 0.00000781
Iteration 90/1000 | Loss: 0.00000780
Iteration 91/1000 | Loss: 0.00000779
Iteration 92/1000 | Loss: 0.00000779
Iteration 93/1000 | Loss: 0.00000779
Iteration 94/1000 | Loss: 0.00000778
Iteration 95/1000 | Loss: 0.00000778
Iteration 96/1000 | Loss: 0.00000778
Iteration 97/1000 | Loss: 0.00000777
Iteration 98/1000 | Loss: 0.00000777
Iteration 99/1000 | Loss: 0.00000777
Iteration 100/1000 | Loss: 0.00000777
Iteration 101/1000 | Loss: 0.00000777
Iteration 102/1000 | Loss: 0.00000777
Iteration 103/1000 | Loss: 0.00000776
Iteration 104/1000 | Loss: 0.00000776
Iteration 105/1000 | Loss: 0.00000776
Iteration 106/1000 | Loss: 0.00000776
Iteration 107/1000 | Loss: 0.00000776
Iteration 108/1000 | Loss: 0.00000776
Iteration 109/1000 | Loss: 0.00000775
Iteration 110/1000 | Loss: 0.00000775
Iteration 111/1000 | Loss: 0.00000775
Iteration 112/1000 | Loss: 0.00000774
Iteration 113/1000 | Loss: 0.00000774
Iteration 114/1000 | Loss: 0.00000774
Iteration 115/1000 | Loss: 0.00000774
Iteration 116/1000 | Loss: 0.00000774
Iteration 117/1000 | Loss: 0.00000774
Iteration 118/1000 | Loss: 0.00000773
Iteration 119/1000 | Loss: 0.00000773
Iteration 120/1000 | Loss: 0.00000773
Iteration 121/1000 | Loss: 0.00000773
Iteration 122/1000 | Loss: 0.00000773
Iteration 123/1000 | Loss: 0.00000773
Iteration 124/1000 | Loss: 0.00000772
Iteration 125/1000 | Loss: 0.00000772
Iteration 126/1000 | Loss: 0.00000772
Iteration 127/1000 | Loss: 0.00000771
Iteration 128/1000 | Loss: 0.00000771
Iteration 129/1000 | Loss: 0.00000771
Iteration 130/1000 | Loss: 0.00000771
Iteration 131/1000 | Loss: 0.00000771
Iteration 132/1000 | Loss: 0.00000771
Iteration 133/1000 | Loss: 0.00000771
Iteration 134/1000 | Loss: 0.00000771
Iteration 135/1000 | Loss: 0.00000771
Iteration 136/1000 | Loss: 0.00000771
Iteration 137/1000 | Loss: 0.00000771
Iteration 138/1000 | Loss: 0.00000771
Iteration 139/1000 | Loss: 0.00000771
Iteration 140/1000 | Loss: 0.00000771
Iteration 141/1000 | Loss: 0.00000771
Iteration 142/1000 | Loss: 0.00000771
Iteration 143/1000 | Loss: 0.00000771
Iteration 144/1000 | Loss: 0.00000771
Iteration 145/1000 | Loss: 0.00000771
Iteration 146/1000 | Loss: 0.00000771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [7.70923088566633e-06, 7.70923088566633e-06, 7.70923088566633e-06, 7.70923088566633e-06, 7.70923088566633e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.70923088566633e-06

Optimization complete. Final v2v error: 2.4238080978393555 mm

Highest mean error: 2.620495319366455 mm for frame 15

Lowest mean error: 2.280158519744873 mm for frame 32

Saving results

Total time: 34.11010217666626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393251
Iteration 2/25 | Loss: 0.00123648
Iteration 3/25 | Loss: 0.00116706
Iteration 4/25 | Loss: 0.00115812
Iteration 5/25 | Loss: 0.00115573
Iteration 6/25 | Loss: 0.00115573
Iteration 7/25 | Loss: 0.00115573
Iteration 8/25 | Loss: 0.00115573
Iteration 9/25 | Loss: 0.00115573
Iteration 10/25 | Loss: 0.00115573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011557277757674456, 0.0011557277757674456, 0.0011557277757674456, 0.0011557277757674456, 0.0011557277757674456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011557277757674456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30696738
Iteration 2/25 | Loss: 0.00120863
Iteration 3/25 | Loss: 0.00120863
Iteration 4/25 | Loss: 0.00120863
Iteration 5/25 | Loss: 0.00120863
Iteration 6/25 | Loss: 0.00120863
Iteration 7/25 | Loss: 0.00120863
Iteration 8/25 | Loss: 0.00120863
Iteration 9/25 | Loss: 0.00120863
Iteration 10/25 | Loss: 0.00120863
Iteration 11/25 | Loss: 0.00120863
Iteration 12/25 | Loss: 0.00120863
Iteration 13/25 | Loss: 0.00120863
Iteration 14/25 | Loss: 0.00120863
Iteration 15/25 | Loss: 0.00120863
Iteration 16/25 | Loss: 0.00120863
Iteration 17/25 | Loss: 0.00120863
Iteration 18/25 | Loss: 0.00120863
Iteration 19/25 | Loss: 0.00120863
Iteration 20/25 | Loss: 0.00120863
Iteration 21/25 | Loss: 0.00120863
Iteration 22/25 | Loss: 0.00120863
Iteration 23/25 | Loss: 0.00120863
Iteration 24/25 | Loss: 0.00120863
Iteration 25/25 | Loss: 0.00120863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120863
Iteration 2/1000 | Loss: 0.00001675
Iteration 3/1000 | Loss: 0.00001327
Iteration 4/1000 | Loss: 0.00001235
Iteration 5/1000 | Loss: 0.00001165
Iteration 6/1000 | Loss: 0.00001126
Iteration 7/1000 | Loss: 0.00001087
Iteration 8/1000 | Loss: 0.00001062
Iteration 9/1000 | Loss: 0.00001048
Iteration 10/1000 | Loss: 0.00001029
Iteration 11/1000 | Loss: 0.00001023
Iteration 12/1000 | Loss: 0.00001008
Iteration 13/1000 | Loss: 0.00001004
Iteration 14/1000 | Loss: 0.00000996
Iteration 15/1000 | Loss: 0.00000991
Iteration 16/1000 | Loss: 0.00000987
Iteration 17/1000 | Loss: 0.00000986
Iteration 18/1000 | Loss: 0.00000986
Iteration 19/1000 | Loss: 0.00000985
Iteration 20/1000 | Loss: 0.00000984
Iteration 21/1000 | Loss: 0.00000984
Iteration 22/1000 | Loss: 0.00000983
Iteration 23/1000 | Loss: 0.00000980
Iteration 24/1000 | Loss: 0.00000978
Iteration 25/1000 | Loss: 0.00000969
Iteration 26/1000 | Loss: 0.00000968
Iteration 27/1000 | Loss: 0.00000967
Iteration 28/1000 | Loss: 0.00000965
Iteration 29/1000 | Loss: 0.00000965
Iteration 30/1000 | Loss: 0.00000964
Iteration 31/1000 | Loss: 0.00000964
Iteration 32/1000 | Loss: 0.00000964
Iteration 33/1000 | Loss: 0.00000963
Iteration 34/1000 | Loss: 0.00000963
Iteration 35/1000 | Loss: 0.00000963
Iteration 36/1000 | Loss: 0.00000963
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000962
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000962
Iteration 41/1000 | Loss: 0.00000962
Iteration 42/1000 | Loss: 0.00000962
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000962
Iteration 46/1000 | Loss: 0.00000962
Iteration 47/1000 | Loss: 0.00000962
Iteration 48/1000 | Loss: 0.00000961
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000960
Iteration 51/1000 | Loss: 0.00000960
Iteration 52/1000 | Loss: 0.00000960
Iteration 53/1000 | Loss: 0.00000960
Iteration 54/1000 | Loss: 0.00000960
Iteration 55/1000 | Loss: 0.00000960
Iteration 56/1000 | Loss: 0.00000960
Iteration 57/1000 | Loss: 0.00000960
Iteration 58/1000 | Loss: 0.00000959
Iteration 59/1000 | Loss: 0.00000959
Iteration 60/1000 | Loss: 0.00000959
Iteration 61/1000 | Loss: 0.00000958
Iteration 62/1000 | Loss: 0.00000958
Iteration 63/1000 | Loss: 0.00000957
Iteration 64/1000 | Loss: 0.00000957
Iteration 65/1000 | Loss: 0.00000956
Iteration 66/1000 | Loss: 0.00000956
Iteration 67/1000 | Loss: 0.00000956
Iteration 68/1000 | Loss: 0.00000956
Iteration 69/1000 | Loss: 0.00000956
Iteration 70/1000 | Loss: 0.00000955
Iteration 71/1000 | Loss: 0.00000955
Iteration 72/1000 | Loss: 0.00000955
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000955
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000955
Iteration 80/1000 | Loss: 0.00000954
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000953
Iteration 83/1000 | Loss: 0.00000953
Iteration 84/1000 | Loss: 0.00000952
Iteration 85/1000 | Loss: 0.00000951
Iteration 86/1000 | Loss: 0.00000951
Iteration 87/1000 | Loss: 0.00000951
Iteration 88/1000 | Loss: 0.00000950
Iteration 89/1000 | Loss: 0.00000950
Iteration 90/1000 | Loss: 0.00000950
Iteration 91/1000 | Loss: 0.00000950
Iteration 92/1000 | Loss: 0.00000949
Iteration 93/1000 | Loss: 0.00000948
Iteration 94/1000 | Loss: 0.00000948
Iteration 95/1000 | Loss: 0.00000947
Iteration 96/1000 | Loss: 0.00000947
Iteration 97/1000 | Loss: 0.00000947
Iteration 98/1000 | Loss: 0.00000947
Iteration 99/1000 | Loss: 0.00000946
Iteration 100/1000 | Loss: 0.00000946
Iteration 101/1000 | Loss: 0.00000946
Iteration 102/1000 | Loss: 0.00000946
Iteration 103/1000 | Loss: 0.00000946
Iteration 104/1000 | Loss: 0.00000946
Iteration 105/1000 | Loss: 0.00000946
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000945
Iteration 108/1000 | Loss: 0.00000945
Iteration 109/1000 | Loss: 0.00000945
Iteration 110/1000 | Loss: 0.00000944
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000944
Iteration 113/1000 | Loss: 0.00000944
Iteration 114/1000 | Loss: 0.00000943
Iteration 115/1000 | Loss: 0.00000943
Iteration 116/1000 | Loss: 0.00000943
Iteration 117/1000 | Loss: 0.00000943
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000942
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000942
Iteration 125/1000 | Loss: 0.00000942
Iteration 126/1000 | Loss: 0.00000942
Iteration 127/1000 | Loss: 0.00000942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [9.419599336979445e-06, 9.419599336979445e-06, 9.419599336979445e-06, 9.419599336979445e-06, 9.419599336979445e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.419599336979445e-06

Optimization complete. Final v2v error: 2.6759819984436035 mm

Highest mean error: 2.8575973510742188 mm for frame 126

Lowest mean error: 2.559189558029175 mm for frame 227

Saving results

Total time: 40.03221583366394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902054
Iteration 2/25 | Loss: 0.00187553
Iteration 3/25 | Loss: 0.00151819
Iteration 4/25 | Loss: 0.00145107
Iteration 5/25 | Loss: 0.00144169
Iteration 6/25 | Loss: 0.00135937
Iteration 7/25 | Loss: 0.00132747
Iteration 8/25 | Loss: 0.00131280
Iteration 9/25 | Loss: 0.00130872
Iteration 10/25 | Loss: 0.00132416
Iteration 11/25 | Loss: 0.00130685
Iteration 12/25 | Loss: 0.00130613
Iteration 13/25 | Loss: 0.00130801
Iteration 14/25 | Loss: 0.00129312
Iteration 15/25 | Loss: 0.00128682
Iteration 16/25 | Loss: 0.00128516
Iteration 17/25 | Loss: 0.00128472
Iteration 18/25 | Loss: 0.00128458
Iteration 19/25 | Loss: 0.00128457
Iteration 20/25 | Loss: 0.00128457
Iteration 21/25 | Loss: 0.00128456
Iteration 22/25 | Loss: 0.00128456
Iteration 23/25 | Loss: 0.00128456
Iteration 24/25 | Loss: 0.00128456
Iteration 25/25 | Loss: 0.00128456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33580494
Iteration 2/25 | Loss: 0.00137214
Iteration 3/25 | Loss: 0.00136838
Iteration 4/25 | Loss: 0.00136838
Iteration 5/25 | Loss: 0.00136838
Iteration 6/25 | Loss: 0.00136838
Iteration 7/25 | Loss: 0.00136838
Iteration 8/25 | Loss: 0.00136838
Iteration 9/25 | Loss: 0.00136838
Iteration 10/25 | Loss: 0.00136838
Iteration 11/25 | Loss: 0.00136838
Iteration 12/25 | Loss: 0.00136838
Iteration 13/25 | Loss: 0.00136838
Iteration 14/25 | Loss: 0.00136838
Iteration 15/25 | Loss: 0.00136838
Iteration 16/25 | Loss: 0.00136838
Iteration 17/25 | Loss: 0.00136838
Iteration 18/25 | Loss: 0.00136838
Iteration 19/25 | Loss: 0.00136838
Iteration 20/25 | Loss: 0.00136838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013683777069672942, 0.0013683777069672942, 0.0013683777069672942, 0.0013683777069672942, 0.0013683777069672942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013683777069672942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136838
Iteration 2/1000 | Loss: 0.00036617
Iteration 3/1000 | Loss: 0.00045985
Iteration 4/1000 | Loss: 0.00023250
Iteration 5/1000 | Loss: 0.00018009
Iteration 6/1000 | Loss: 0.00012853
Iteration 7/1000 | Loss: 0.00010360
Iteration 8/1000 | Loss: 0.00009169
Iteration 9/1000 | Loss: 0.00008418
Iteration 10/1000 | Loss: 0.00024192
Iteration 11/1000 | Loss: 0.00011877
Iteration 12/1000 | Loss: 0.00010649
Iteration 13/1000 | Loss: 0.00014414
Iteration 14/1000 | Loss: 0.00009717
Iteration 15/1000 | Loss: 0.00008633
Iteration 16/1000 | Loss: 0.00012638
Iteration 17/1000 | Loss: 0.00010651
Iteration 18/1000 | Loss: 0.00007765
Iteration 19/1000 | Loss: 0.00007151
Iteration 20/1000 | Loss: 0.00006375
Iteration 21/1000 | Loss: 0.00009911
Iteration 22/1000 | Loss: 0.00006044
Iteration 23/1000 | Loss: 0.00005604
Iteration 24/1000 | Loss: 0.00005023
Iteration 25/1000 | Loss: 0.00004657
Iteration 26/1000 | Loss: 0.00009249
Iteration 27/1000 | Loss: 0.00005220
Iteration 28/1000 | Loss: 0.00006057
Iteration 29/1000 | Loss: 0.00005537
Iteration 30/1000 | Loss: 0.00004896
Iteration 31/1000 | Loss: 0.00004510
Iteration 32/1000 | Loss: 0.00004192
Iteration 33/1000 | Loss: 0.00003930
Iteration 34/1000 | Loss: 0.00003725
Iteration 35/1000 | Loss: 0.00003604
Iteration 36/1000 | Loss: 0.00007043
Iteration 37/1000 | Loss: 0.00013872
Iteration 38/1000 | Loss: 0.00007378
Iteration 39/1000 | Loss: 0.00004727
Iteration 40/1000 | Loss: 0.00003771
Iteration 41/1000 | Loss: 0.00004604
Iteration 42/1000 | Loss: 0.00004538
Iteration 43/1000 | Loss: 0.00004035
Iteration 44/1000 | Loss: 0.00003795
Iteration 45/1000 | Loss: 0.00004433
Iteration 46/1000 | Loss: 0.00003399
Iteration 47/1000 | Loss: 0.00003952
Iteration 48/1000 | Loss: 0.00003517
Iteration 49/1000 | Loss: 0.00003403
Iteration 50/1000 | Loss: 0.00003281
Iteration 51/1000 | Loss: 0.00003210
Iteration 52/1000 | Loss: 0.00003138
Iteration 53/1000 | Loss: 0.00004744
Iteration 54/1000 | Loss: 0.00006993
Iteration 55/1000 | Loss: 0.00006208
Iteration 56/1000 | Loss: 0.00003773
Iteration 57/1000 | Loss: 0.00003633
Iteration 58/1000 | Loss: 0.00004809
Iteration 59/1000 | Loss: 0.00004145
Iteration 60/1000 | Loss: 0.00003648
Iteration 61/1000 | Loss: 0.00003183
Iteration 62/1000 | Loss: 0.00005741
Iteration 63/1000 | Loss: 0.00005617
Iteration 64/1000 | Loss: 0.00005446
Iteration 65/1000 | Loss: 0.00002837
Iteration 66/1000 | Loss: 0.00002749
Iteration 67/1000 | Loss: 0.00002723
Iteration 68/1000 | Loss: 0.00002716
Iteration 69/1000 | Loss: 0.00002697
Iteration 70/1000 | Loss: 0.00002695
Iteration 71/1000 | Loss: 0.00002684
Iteration 72/1000 | Loss: 0.00002679
Iteration 73/1000 | Loss: 0.00002677
Iteration 74/1000 | Loss: 0.00002676
Iteration 75/1000 | Loss: 0.00002675
Iteration 76/1000 | Loss: 0.00002675
Iteration 77/1000 | Loss: 0.00002675
Iteration 78/1000 | Loss: 0.00002675
Iteration 79/1000 | Loss: 0.00002675
Iteration 80/1000 | Loss: 0.00002674
Iteration 81/1000 | Loss: 0.00002674
Iteration 82/1000 | Loss: 0.00002674
Iteration 83/1000 | Loss: 0.00002673
Iteration 84/1000 | Loss: 0.00002673
Iteration 85/1000 | Loss: 0.00002673
Iteration 86/1000 | Loss: 0.00002672
Iteration 87/1000 | Loss: 0.00002672
Iteration 88/1000 | Loss: 0.00002671
Iteration 89/1000 | Loss: 0.00002671
Iteration 90/1000 | Loss: 0.00002671
Iteration 91/1000 | Loss: 0.00002670
Iteration 92/1000 | Loss: 0.00002670
Iteration 93/1000 | Loss: 0.00002670
Iteration 94/1000 | Loss: 0.00002669
Iteration 95/1000 | Loss: 0.00002669
Iteration 96/1000 | Loss: 0.00002667
Iteration 97/1000 | Loss: 0.00002667
Iteration 98/1000 | Loss: 0.00002667
Iteration 99/1000 | Loss: 0.00002667
Iteration 100/1000 | Loss: 0.00002667
Iteration 101/1000 | Loss: 0.00002667
Iteration 102/1000 | Loss: 0.00002667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.6667732527130283e-05, 2.6667732527130283e-05, 2.6667732527130283e-05, 2.6667732527130283e-05, 2.6667732527130283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6667732527130283e-05

Optimization complete. Final v2v error: 3.9308531284332275 mm

Highest mean error: 12.420018196105957 mm for frame 120

Lowest mean error: 2.8787786960601807 mm for frame 74

Saving results

Total time: 139.37641429901123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002645
Iteration 2/25 | Loss: 0.01002645
Iteration 3/25 | Loss: 0.01002645
Iteration 4/25 | Loss: 0.01002645
Iteration 5/25 | Loss: 0.01002644
Iteration 6/25 | Loss: 0.01002644
Iteration 7/25 | Loss: 0.01002644
Iteration 8/25 | Loss: 0.01002644
Iteration 9/25 | Loss: 0.01002644
Iteration 10/25 | Loss: 0.01002644
Iteration 11/25 | Loss: 0.01002644
Iteration 12/25 | Loss: 0.01002644
Iteration 13/25 | Loss: 0.01002644
Iteration 14/25 | Loss: 0.01002644
Iteration 15/25 | Loss: 0.01002644
Iteration 16/25 | Loss: 0.01002644
Iteration 17/25 | Loss: 0.01002644
Iteration 18/25 | Loss: 0.01002643
Iteration 19/25 | Loss: 0.01002643
Iteration 20/25 | Loss: 0.01002643
Iteration 21/25 | Loss: 0.01002643
Iteration 22/25 | Loss: 0.01002643
Iteration 23/25 | Loss: 0.01002643
Iteration 24/25 | Loss: 0.01002643
Iteration 25/25 | Loss: 0.01002643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52633584
Iteration 2/25 | Loss: 0.12010398
Iteration 3/25 | Loss: 0.11854301
Iteration 4/25 | Loss: 0.11906613
Iteration 5/25 | Loss: 0.11789817
Iteration 6/25 | Loss: 0.11789817
Iteration 7/25 | Loss: 0.11789817
Iteration 8/25 | Loss: 0.11789817
Iteration 9/25 | Loss: 0.11789817
Iteration 10/25 | Loss: 0.11789817
Iteration 11/25 | Loss: 0.11789817
Iteration 12/25 | Loss: 0.11789817
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.11789816617965698, 0.11789816617965698, 0.11789816617965698, 0.11789816617965698, 0.11789816617965698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11789816617965698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11789817
Iteration 2/1000 | Loss: 0.00297205
Iteration 3/1000 | Loss: 0.00336818
Iteration 4/1000 | Loss: 0.00102198
Iteration 5/1000 | Loss: 0.00040578
Iteration 6/1000 | Loss: 0.00284762
Iteration 7/1000 | Loss: 0.00042642
Iteration 8/1000 | Loss: 0.00011301
Iteration 9/1000 | Loss: 0.00007367
Iteration 10/1000 | Loss: 0.00007846
Iteration 11/1000 | Loss: 0.00007241
Iteration 12/1000 | Loss: 0.00057890
Iteration 13/1000 | Loss: 0.00007588
Iteration 14/1000 | Loss: 0.00003843
Iteration 15/1000 | Loss: 0.00053305
Iteration 16/1000 | Loss: 0.00013865
Iteration 17/1000 | Loss: 0.00011873
Iteration 18/1000 | Loss: 0.00006750
Iteration 19/1000 | Loss: 0.00015718
Iteration 20/1000 | Loss: 0.00022654
Iteration 21/1000 | Loss: 0.00347466
Iteration 22/1000 | Loss: 0.00152512
Iteration 23/1000 | Loss: 0.00067097
Iteration 24/1000 | Loss: 0.00003298
Iteration 25/1000 | Loss: 0.00020924
Iteration 26/1000 | Loss: 0.00002546
Iteration 27/1000 | Loss: 0.00007285
Iteration 28/1000 | Loss: 0.00067186
Iteration 29/1000 | Loss: 0.00008774
Iteration 30/1000 | Loss: 0.00030403
Iteration 31/1000 | Loss: 0.00004404
Iteration 32/1000 | Loss: 0.00023138
Iteration 33/1000 | Loss: 0.00008577
Iteration 34/1000 | Loss: 0.00002435
Iteration 35/1000 | Loss: 0.00004583
Iteration 36/1000 | Loss: 0.00006615
Iteration 37/1000 | Loss: 0.00004216
Iteration 38/1000 | Loss: 0.00007261
Iteration 39/1000 | Loss: 0.00003917
Iteration 40/1000 | Loss: 0.00005776
Iteration 41/1000 | Loss: 0.00012184
Iteration 42/1000 | Loss: 0.00002963
Iteration 43/1000 | Loss: 0.00010397
Iteration 44/1000 | Loss: 0.00003582
Iteration 45/1000 | Loss: 0.00004948
Iteration 46/1000 | Loss: 0.00002347
Iteration 47/1000 | Loss: 0.00004132
Iteration 48/1000 | Loss: 0.00001684
Iteration 49/1000 | Loss: 0.00002841
Iteration 50/1000 | Loss: 0.00001557
Iteration 51/1000 | Loss: 0.00002497
Iteration 52/1000 | Loss: 0.00024018
Iteration 53/1000 | Loss: 0.00002063
Iteration 54/1000 | Loss: 0.00001970
Iteration 55/1000 | Loss: 0.00001539
Iteration 56/1000 | Loss: 0.00003679
Iteration 57/1000 | Loss: 0.00003903
Iteration 58/1000 | Loss: 0.00002053
Iteration 59/1000 | Loss: 0.00001747
Iteration 60/1000 | Loss: 0.00001451
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00002026
Iteration 65/1000 | Loss: 0.00003074
Iteration 66/1000 | Loss: 0.00003047
Iteration 67/1000 | Loss: 0.00001735
Iteration 68/1000 | Loss: 0.00001783
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001820
Iteration 71/1000 | Loss: 0.00001427
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001413
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001410
Iteration 81/1000 | Loss: 0.00001410
Iteration 82/1000 | Loss: 0.00001410
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001410
Iteration 87/1000 | Loss: 0.00003174
Iteration 88/1000 | Loss: 0.00007592
Iteration 89/1000 | Loss: 0.00003572
Iteration 90/1000 | Loss: 0.00003024
Iteration 91/1000 | Loss: 0.00006448
Iteration 92/1000 | Loss: 0.00001759
Iteration 93/1000 | Loss: 0.00002947
Iteration 94/1000 | Loss: 0.00004502
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00002189
Iteration 97/1000 | Loss: 0.00001490
Iteration 98/1000 | Loss: 0.00002274
Iteration 99/1000 | Loss: 0.00001537
Iteration 100/1000 | Loss: 0.00001575
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001955
Iteration 103/1000 | Loss: 0.00002284
Iteration 104/1000 | Loss: 0.00001651
Iteration 105/1000 | Loss: 0.00002494
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001416
Iteration 108/1000 | Loss: 0.00001576
Iteration 109/1000 | Loss: 0.00002528
Iteration 110/1000 | Loss: 0.00002669
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001396
Iteration 113/1000 | Loss: 0.00001561
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001395
Iteration 117/1000 | Loss: 0.00001395
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001394
Iteration 120/1000 | Loss: 0.00001394
Iteration 121/1000 | Loss: 0.00001394
Iteration 122/1000 | Loss: 0.00001394
Iteration 123/1000 | Loss: 0.00001634
Iteration 124/1000 | Loss: 0.00001876
Iteration 125/1000 | Loss: 0.00002691
Iteration 126/1000 | Loss: 0.00002414
Iteration 127/1000 | Loss: 0.00002333
Iteration 128/1000 | Loss: 0.00007186
Iteration 129/1000 | Loss: 0.00002748
Iteration 130/1000 | Loss: 0.00004882
Iteration 131/1000 | Loss: 0.00001393
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001391
Iteration 135/1000 | Loss: 0.00001391
Iteration 136/1000 | Loss: 0.00001391
Iteration 137/1000 | Loss: 0.00001391
Iteration 138/1000 | Loss: 0.00001391
Iteration 139/1000 | Loss: 0.00001391
Iteration 140/1000 | Loss: 0.00001391
Iteration 141/1000 | Loss: 0.00001391
Iteration 142/1000 | Loss: 0.00001391
Iteration 143/1000 | Loss: 0.00001391
Iteration 144/1000 | Loss: 0.00001391
Iteration 145/1000 | Loss: 0.00001504
Iteration 146/1000 | Loss: 0.00001504
Iteration 147/1000 | Loss: 0.00001504
Iteration 148/1000 | Loss: 0.00001504
Iteration 149/1000 | Loss: 0.00008361
Iteration 150/1000 | Loss: 0.00001811
Iteration 151/1000 | Loss: 0.00001915
Iteration 152/1000 | Loss: 0.00001391
Iteration 153/1000 | Loss: 0.00001565
Iteration 154/1000 | Loss: 0.00001397
Iteration 155/1000 | Loss: 0.00001844
Iteration 156/1000 | Loss: 0.00001535
Iteration 157/1000 | Loss: 0.00001390
Iteration 158/1000 | Loss: 0.00001390
Iteration 159/1000 | Loss: 0.00001389
Iteration 160/1000 | Loss: 0.00001389
Iteration 161/1000 | Loss: 0.00001389
Iteration 162/1000 | Loss: 0.00001389
Iteration 163/1000 | Loss: 0.00001389
Iteration 164/1000 | Loss: 0.00001389
Iteration 165/1000 | Loss: 0.00001389
Iteration 166/1000 | Loss: 0.00001388
Iteration 167/1000 | Loss: 0.00001388
Iteration 168/1000 | Loss: 0.00001388
Iteration 169/1000 | Loss: 0.00001388
Iteration 170/1000 | Loss: 0.00001597
Iteration 171/1000 | Loss: 0.00001534
Iteration 172/1000 | Loss: 0.00001437
Iteration 173/1000 | Loss: 0.00001388
Iteration 174/1000 | Loss: 0.00001388
Iteration 175/1000 | Loss: 0.00001388
Iteration 176/1000 | Loss: 0.00001388
Iteration 177/1000 | Loss: 0.00001388
Iteration 178/1000 | Loss: 0.00001388
Iteration 179/1000 | Loss: 0.00001388
Iteration 180/1000 | Loss: 0.00001388
Iteration 181/1000 | Loss: 0.00001388
Iteration 182/1000 | Loss: 0.00001388
Iteration 183/1000 | Loss: 0.00001388
Iteration 184/1000 | Loss: 0.00001388
Iteration 185/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.3875045624445193e-05, 1.3875045624445193e-05, 1.3875045624445193e-05, 1.3875045624445193e-05, 1.3875045624445193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3875045624445193e-05

Optimization complete. Final v2v error: 3.1632604598999023 mm

Highest mean error: 3.422116279602051 mm for frame 40

Lowest mean error: 3.0605597496032715 mm for frame 194

Saving results

Total time: 178.84083676338196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865567
Iteration 2/25 | Loss: 0.00171786
Iteration 3/25 | Loss: 0.00136764
Iteration 4/25 | Loss: 0.00132882
Iteration 5/25 | Loss: 0.00134043
Iteration 6/25 | Loss: 0.00129708
Iteration 7/25 | Loss: 0.00125983
Iteration 8/25 | Loss: 0.00124006
Iteration 9/25 | Loss: 0.00122484
Iteration 10/25 | Loss: 0.00121659
Iteration 11/25 | Loss: 0.00121543
Iteration 12/25 | Loss: 0.00121236
Iteration 13/25 | Loss: 0.00121176
Iteration 14/25 | Loss: 0.00121162
Iteration 15/25 | Loss: 0.00121159
Iteration 16/25 | Loss: 0.00121156
Iteration 17/25 | Loss: 0.00121154
Iteration 18/25 | Loss: 0.00121154
Iteration 19/25 | Loss: 0.00121154
Iteration 20/25 | Loss: 0.00121154
Iteration 21/25 | Loss: 0.00121154
Iteration 22/25 | Loss: 0.00121153
Iteration 23/25 | Loss: 0.00121153
Iteration 24/25 | Loss: 0.00121153
Iteration 25/25 | Loss: 0.00121153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87204438
Iteration 2/25 | Loss: 0.00066993
Iteration 3/25 | Loss: 0.00066993
Iteration 4/25 | Loss: 0.00066992
Iteration 5/25 | Loss: 0.00066992
Iteration 6/25 | Loss: 0.00066992
Iteration 7/25 | Loss: 0.00066992
Iteration 8/25 | Loss: 0.00066992
Iteration 9/25 | Loss: 0.00066992
Iteration 10/25 | Loss: 0.00066992
Iteration 11/25 | Loss: 0.00066992
Iteration 12/25 | Loss: 0.00066992
Iteration 13/25 | Loss: 0.00066992
Iteration 14/25 | Loss: 0.00066992
Iteration 15/25 | Loss: 0.00066992
Iteration 16/25 | Loss: 0.00066992
Iteration 17/25 | Loss: 0.00066992
Iteration 18/25 | Loss: 0.00066992
Iteration 19/25 | Loss: 0.00066992
Iteration 20/25 | Loss: 0.00066992
Iteration 21/25 | Loss: 0.00066992
Iteration 22/25 | Loss: 0.00066992
Iteration 23/25 | Loss: 0.00066992
Iteration 24/25 | Loss: 0.00066992
Iteration 25/25 | Loss: 0.00066992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066992
Iteration 2/1000 | Loss: 0.00003776
Iteration 3/1000 | Loss: 0.00003084
Iteration 4/1000 | Loss: 0.00002870
Iteration 5/1000 | Loss: 0.00002755
Iteration 6/1000 | Loss: 0.00002682
Iteration 7/1000 | Loss: 0.00002616
Iteration 8/1000 | Loss: 0.00002588
Iteration 9/1000 | Loss: 0.00002548
Iteration 10/1000 | Loss: 0.00002523
Iteration 11/1000 | Loss: 0.00002506
Iteration 12/1000 | Loss: 0.00002501
Iteration 13/1000 | Loss: 0.00002501
Iteration 14/1000 | Loss: 0.00002501
Iteration 15/1000 | Loss: 0.00002499
Iteration 16/1000 | Loss: 0.00002484
Iteration 17/1000 | Loss: 0.00002484
Iteration 18/1000 | Loss: 0.00002479
Iteration 19/1000 | Loss: 0.00002479
Iteration 20/1000 | Loss: 0.00002476
Iteration 21/1000 | Loss: 0.00002469
Iteration 22/1000 | Loss: 0.00002468
Iteration 23/1000 | Loss: 0.00002468
Iteration 24/1000 | Loss: 0.00002467
Iteration 25/1000 | Loss: 0.00002467
Iteration 26/1000 | Loss: 0.00002467
Iteration 27/1000 | Loss: 0.00002467
Iteration 28/1000 | Loss: 0.00002466
Iteration 29/1000 | Loss: 0.00002466
Iteration 30/1000 | Loss: 0.00002466
Iteration 31/1000 | Loss: 0.00002466
Iteration 32/1000 | Loss: 0.00002466
Iteration 33/1000 | Loss: 0.00002466
Iteration 34/1000 | Loss: 0.00002466
Iteration 35/1000 | Loss: 0.00002465
Iteration 36/1000 | Loss: 0.00002465
Iteration 37/1000 | Loss: 0.00002465
Iteration 38/1000 | Loss: 0.00002465
Iteration 39/1000 | Loss: 0.00002465
Iteration 40/1000 | Loss: 0.00002465
Iteration 41/1000 | Loss: 0.00002465
Iteration 42/1000 | Loss: 0.00002464
Iteration 43/1000 | Loss: 0.00002464
Iteration 44/1000 | Loss: 0.00002464
Iteration 45/1000 | Loss: 0.00002464
Iteration 46/1000 | Loss: 0.00002464
Iteration 47/1000 | Loss: 0.00002464
Iteration 48/1000 | Loss: 0.00002463
Iteration 49/1000 | Loss: 0.00002463
Iteration 50/1000 | Loss: 0.00002463
Iteration 51/1000 | Loss: 0.00002463
Iteration 52/1000 | Loss: 0.00002463
Iteration 53/1000 | Loss: 0.00002463
Iteration 54/1000 | Loss: 0.00002463
Iteration 55/1000 | Loss: 0.00002463
Iteration 56/1000 | Loss: 0.00002462
Iteration 57/1000 | Loss: 0.00002462
Iteration 58/1000 | Loss: 0.00002462
Iteration 59/1000 | Loss: 0.00002462
Iteration 60/1000 | Loss: 0.00002461
Iteration 61/1000 | Loss: 0.00002461
Iteration 62/1000 | Loss: 0.00002461
Iteration 63/1000 | Loss: 0.00002461
Iteration 64/1000 | Loss: 0.00002461
Iteration 65/1000 | Loss: 0.00002461
Iteration 66/1000 | Loss: 0.00002461
Iteration 67/1000 | Loss: 0.00002461
Iteration 68/1000 | Loss: 0.00002461
Iteration 69/1000 | Loss: 0.00002461
Iteration 70/1000 | Loss: 0.00002461
Iteration 71/1000 | Loss: 0.00002461
Iteration 72/1000 | Loss: 0.00002461
Iteration 73/1000 | Loss: 0.00002461
Iteration 74/1000 | Loss: 0.00002461
Iteration 75/1000 | Loss: 0.00002461
Iteration 76/1000 | Loss: 0.00002461
Iteration 77/1000 | Loss: 0.00002461
Iteration 78/1000 | Loss: 0.00002461
Iteration 79/1000 | Loss: 0.00002461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.4610815671621822e-05, 2.4610815671621822e-05, 2.4610815671621822e-05, 2.4610815671621822e-05, 2.4610815671621822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4610815671621822e-05

Optimization complete. Final v2v error: 4.178223609924316 mm

Highest mean error: 4.284584999084473 mm for frame 48

Lowest mean error: 4.080945014953613 mm for frame 14

Saving results

Total time: 50.7305006980896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962196
Iteration 2/25 | Loss: 0.00317839
Iteration 3/25 | Loss: 0.00217659
Iteration 4/25 | Loss: 0.00201382
Iteration 5/25 | Loss: 0.00189001
Iteration 6/25 | Loss: 0.00184588
Iteration 7/25 | Loss: 0.00190841
Iteration 8/25 | Loss: 0.00185456
Iteration 9/25 | Loss: 0.00178417
Iteration 10/25 | Loss: 0.00166063
Iteration 11/25 | Loss: 0.00156829
Iteration 12/25 | Loss: 0.00153132
Iteration 13/25 | Loss: 0.00149100
Iteration 14/25 | Loss: 0.00147763
Iteration 15/25 | Loss: 0.00145184
Iteration 16/25 | Loss: 0.00143871
Iteration 17/25 | Loss: 0.00142301
Iteration 18/25 | Loss: 0.00140690
Iteration 19/25 | Loss: 0.00141969
Iteration 20/25 | Loss: 0.00140877
Iteration 21/25 | Loss: 0.00138758
Iteration 22/25 | Loss: 0.00137001
Iteration 23/25 | Loss: 0.00136676
Iteration 24/25 | Loss: 0.00136270
Iteration 25/25 | Loss: 0.00136111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26551831
Iteration 2/25 | Loss: 0.00239253
Iteration 3/25 | Loss: 0.00226223
Iteration 4/25 | Loss: 0.00226223
Iteration 5/25 | Loss: 0.00226222
Iteration 6/25 | Loss: 0.00226222
Iteration 7/25 | Loss: 0.00226222
Iteration 8/25 | Loss: 0.00226222
Iteration 9/25 | Loss: 0.00226222
Iteration 10/25 | Loss: 0.00226222
Iteration 11/25 | Loss: 0.00226222
Iteration 12/25 | Loss: 0.00226222
Iteration 13/25 | Loss: 0.00226222
Iteration 14/25 | Loss: 0.00226222
Iteration 15/25 | Loss: 0.00226222
Iteration 16/25 | Loss: 0.00226222
Iteration 17/25 | Loss: 0.00226222
Iteration 18/25 | Loss: 0.00226222
Iteration 19/25 | Loss: 0.00226222
Iteration 20/25 | Loss: 0.00226222
Iteration 21/25 | Loss: 0.00226222
Iteration 22/25 | Loss: 0.00226222
Iteration 23/25 | Loss: 0.00226222
Iteration 24/25 | Loss: 0.00226222
Iteration 25/25 | Loss: 0.00226222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226222
Iteration 2/1000 | Loss: 0.00062530
Iteration 3/1000 | Loss: 0.00030585
Iteration 4/1000 | Loss: 0.00026036
Iteration 5/1000 | Loss: 0.00015562
Iteration 6/1000 | Loss: 0.00019547
Iteration 7/1000 | Loss: 0.00022137
Iteration 8/1000 | Loss: 0.00032025
Iteration 9/1000 | Loss: 0.00036008
Iteration 10/1000 | Loss: 0.00013664
Iteration 11/1000 | Loss: 0.00012154
Iteration 12/1000 | Loss: 0.00011449
Iteration 13/1000 | Loss: 0.00013914
Iteration 14/1000 | Loss: 0.00013870
Iteration 15/1000 | Loss: 0.00018376
Iteration 16/1000 | Loss: 0.00026833
Iteration 17/1000 | Loss: 0.00013945
Iteration 18/1000 | Loss: 0.00051576
Iteration 19/1000 | Loss: 0.00017424
Iteration 20/1000 | Loss: 0.00029655
Iteration 21/1000 | Loss: 0.00011338
Iteration 22/1000 | Loss: 0.00012125
Iteration 23/1000 | Loss: 0.00011573
Iteration 24/1000 | Loss: 0.00040534
Iteration 25/1000 | Loss: 0.00011143
Iteration 26/1000 | Loss: 0.00010709
Iteration 27/1000 | Loss: 0.00017354
Iteration 28/1000 | Loss: 0.00013483
Iteration 29/1000 | Loss: 0.00113153
Iteration 30/1000 | Loss: 0.00262612
Iteration 31/1000 | Loss: 0.00307623
Iteration 32/1000 | Loss: 0.00031625
Iteration 33/1000 | Loss: 0.00019218
Iteration 34/1000 | Loss: 0.00012960
Iteration 35/1000 | Loss: 0.00013866
Iteration 36/1000 | Loss: 0.00014106
Iteration 37/1000 | Loss: 0.00036498
Iteration 38/1000 | Loss: 0.00009608
Iteration 39/1000 | Loss: 0.00011368
Iteration 40/1000 | Loss: 0.00009759
Iteration 41/1000 | Loss: 0.00008665
Iteration 42/1000 | Loss: 0.00014300
Iteration 43/1000 | Loss: 0.00007150
Iteration 44/1000 | Loss: 0.00011987
Iteration 45/1000 | Loss: 0.00013556
Iteration 46/1000 | Loss: 0.00034385
Iteration 47/1000 | Loss: 0.00009529
Iteration 48/1000 | Loss: 0.00012087
Iteration 49/1000 | Loss: 0.00009224
Iteration 50/1000 | Loss: 0.00010152
Iteration 51/1000 | Loss: 0.00013213
Iteration 52/1000 | Loss: 0.00010693
Iteration 53/1000 | Loss: 0.00008091
Iteration 54/1000 | Loss: 0.00008339
Iteration 55/1000 | Loss: 0.00006960
Iteration 56/1000 | Loss: 0.00010972
Iteration 57/1000 | Loss: 0.00007121
Iteration 58/1000 | Loss: 0.00008686
Iteration 59/1000 | Loss: 0.00006598
Iteration 60/1000 | Loss: 0.00007594
Iteration 61/1000 | Loss: 0.00026898
Iteration 62/1000 | Loss: 0.00008319
Iteration 63/1000 | Loss: 0.00009109
Iteration 64/1000 | Loss: 0.00017753
Iteration 65/1000 | Loss: 0.00010481
Iteration 66/1000 | Loss: 0.00007635
Iteration 67/1000 | Loss: 0.00010545
Iteration 68/1000 | Loss: 0.00011716
Iteration 69/1000 | Loss: 0.00007035
Iteration 70/1000 | Loss: 0.00008490
Iteration 71/1000 | Loss: 0.00010603
Iteration 72/1000 | Loss: 0.00015449
Iteration 73/1000 | Loss: 0.00019282
Iteration 74/1000 | Loss: 0.00008169
Iteration 75/1000 | Loss: 0.00005315
Iteration 76/1000 | Loss: 0.00025074
Iteration 77/1000 | Loss: 0.00006273
Iteration 78/1000 | Loss: 0.00009311
Iteration 79/1000 | Loss: 0.00005738
Iteration 80/1000 | Loss: 0.00005816
Iteration 81/1000 | Loss: 0.00005656
Iteration 82/1000 | Loss: 0.00006271
Iteration 83/1000 | Loss: 0.00004962
Iteration 84/1000 | Loss: 0.00005159
Iteration 85/1000 | Loss: 0.00005030
Iteration 86/1000 | Loss: 0.00005320
Iteration 87/1000 | Loss: 0.00009611
Iteration 88/1000 | Loss: 0.00004640
Iteration 89/1000 | Loss: 0.00006126
Iteration 90/1000 | Loss: 0.00006023
Iteration 91/1000 | Loss: 0.00004966
Iteration 92/1000 | Loss: 0.00005191
Iteration 93/1000 | Loss: 0.00012794
Iteration 94/1000 | Loss: 0.00004909
Iteration 95/1000 | Loss: 0.00008435
Iteration 96/1000 | Loss: 0.00004173
Iteration 97/1000 | Loss: 0.00005233
Iteration 98/1000 | Loss: 0.00003841
Iteration 99/1000 | Loss: 0.00003785
Iteration 100/1000 | Loss: 0.00014151
Iteration 101/1000 | Loss: 0.00004871
Iteration 102/1000 | Loss: 0.00003716
Iteration 103/1000 | Loss: 0.00013306
Iteration 104/1000 | Loss: 0.00006388
Iteration 105/1000 | Loss: 0.00005316
Iteration 106/1000 | Loss: 0.00003641
Iteration 107/1000 | Loss: 0.00003613
Iteration 108/1000 | Loss: 0.00003582
Iteration 109/1000 | Loss: 0.00003553
Iteration 110/1000 | Loss: 0.00003529
Iteration 111/1000 | Loss: 0.00003499
Iteration 112/1000 | Loss: 0.00010270
Iteration 113/1000 | Loss: 0.00009388
Iteration 114/1000 | Loss: 0.00003488
Iteration 115/1000 | Loss: 0.00008866
Iteration 116/1000 | Loss: 0.00008866
Iteration 117/1000 | Loss: 0.00005281
Iteration 118/1000 | Loss: 0.00004151
Iteration 119/1000 | Loss: 0.00005149
Iteration 120/1000 | Loss: 0.00003466
Iteration 121/1000 | Loss: 0.00003461
Iteration 122/1000 | Loss: 0.00003460
Iteration 123/1000 | Loss: 0.00003456
Iteration 124/1000 | Loss: 0.00003456
Iteration 125/1000 | Loss: 0.00003456
Iteration 126/1000 | Loss: 0.00003455
Iteration 127/1000 | Loss: 0.00003455
Iteration 128/1000 | Loss: 0.00003455
Iteration 129/1000 | Loss: 0.00003455
Iteration 130/1000 | Loss: 0.00003454
Iteration 131/1000 | Loss: 0.00003453
Iteration 132/1000 | Loss: 0.00003452
Iteration 133/1000 | Loss: 0.00003452
Iteration 134/1000 | Loss: 0.00003450
Iteration 135/1000 | Loss: 0.00003448
Iteration 136/1000 | Loss: 0.00003441
Iteration 137/1000 | Loss: 0.00003441
Iteration 138/1000 | Loss: 0.00003440
Iteration 139/1000 | Loss: 0.00003440
Iteration 140/1000 | Loss: 0.00003440
Iteration 141/1000 | Loss: 0.00003439
Iteration 142/1000 | Loss: 0.00003439
Iteration 143/1000 | Loss: 0.00003438
Iteration 144/1000 | Loss: 0.00003438
Iteration 145/1000 | Loss: 0.00003437
Iteration 146/1000 | Loss: 0.00003437
Iteration 147/1000 | Loss: 0.00003436
Iteration 148/1000 | Loss: 0.00003436
Iteration 149/1000 | Loss: 0.00003435
Iteration 150/1000 | Loss: 0.00003435
Iteration 151/1000 | Loss: 0.00003434
Iteration 152/1000 | Loss: 0.00003434
Iteration 153/1000 | Loss: 0.00003434
Iteration 154/1000 | Loss: 0.00003433
Iteration 155/1000 | Loss: 0.00003433
Iteration 156/1000 | Loss: 0.00003433
Iteration 157/1000 | Loss: 0.00003433
Iteration 158/1000 | Loss: 0.00003432
Iteration 159/1000 | Loss: 0.00003432
Iteration 160/1000 | Loss: 0.00003432
Iteration 161/1000 | Loss: 0.00003432
Iteration 162/1000 | Loss: 0.00003432
Iteration 163/1000 | Loss: 0.00003432
Iteration 164/1000 | Loss: 0.00003432
Iteration 165/1000 | Loss: 0.00003432
Iteration 166/1000 | Loss: 0.00003431
Iteration 167/1000 | Loss: 0.00003431
Iteration 168/1000 | Loss: 0.00003431
Iteration 169/1000 | Loss: 0.00003430
Iteration 170/1000 | Loss: 0.00003430
Iteration 171/1000 | Loss: 0.00003430
Iteration 172/1000 | Loss: 0.00003430
Iteration 173/1000 | Loss: 0.00003429
Iteration 174/1000 | Loss: 0.00003429
Iteration 175/1000 | Loss: 0.00003429
Iteration 176/1000 | Loss: 0.00003427
Iteration 177/1000 | Loss: 0.00003425
Iteration 178/1000 | Loss: 0.00003425
Iteration 179/1000 | Loss: 0.00003425
Iteration 180/1000 | Loss: 0.00003424
Iteration 181/1000 | Loss: 0.00003424
Iteration 182/1000 | Loss: 0.00003423
Iteration 183/1000 | Loss: 0.00003422
Iteration 184/1000 | Loss: 0.00003422
Iteration 185/1000 | Loss: 0.00003421
Iteration 186/1000 | Loss: 0.00003421
Iteration 187/1000 | Loss: 0.00003421
Iteration 188/1000 | Loss: 0.00003420
Iteration 189/1000 | Loss: 0.00003420
Iteration 190/1000 | Loss: 0.00003420
Iteration 191/1000 | Loss: 0.00003419
Iteration 192/1000 | Loss: 0.00003419
Iteration 193/1000 | Loss: 0.00003419
Iteration 194/1000 | Loss: 0.00003419
Iteration 195/1000 | Loss: 0.00003419
Iteration 196/1000 | Loss: 0.00003419
Iteration 197/1000 | Loss: 0.00003418
Iteration 198/1000 | Loss: 0.00003418
Iteration 199/1000 | Loss: 0.00003418
Iteration 200/1000 | Loss: 0.00003418
Iteration 201/1000 | Loss: 0.00003417
Iteration 202/1000 | Loss: 0.00003417
Iteration 203/1000 | Loss: 0.00003417
Iteration 204/1000 | Loss: 0.00003417
Iteration 205/1000 | Loss: 0.00003417
Iteration 206/1000 | Loss: 0.00003417
Iteration 207/1000 | Loss: 0.00003417
Iteration 208/1000 | Loss: 0.00003417
Iteration 209/1000 | Loss: 0.00003417
Iteration 210/1000 | Loss: 0.00003417
Iteration 211/1000 | Loss: 0.00003416
Iteration 212/1000 | Loss: 0.00003416
Iteration 213/1000 | Loss: 0.00003416
Iteration 214/1000 | Loss: 0.00003416
Iteration 215/1000 | Loss: 0.00003416
Iteration 216/1000 | Loss: 0.00003416
Iteration 217/1000 | Loss: 0.00003415
Iteration 218/1000 | Loss: 0.00003415
Iteration 219/1000 | Loss: 0.00003415
Iteration 220/1000 | Loss: 0.00003415
Iteration 221/1000 | Loss: 0.00003415
Iteration 222/1000 | Loss: 0.00003415
Iteration 223/1000 | Loss: 0.00003414
Iteration 224/1000 | Loss: 0.00003414
Iteration 225/1000 | Loss: 0.00003414
Iteration 226/1000 | Loss: 0.00003414
Iteration 227/1000 | Loss: 0.00003414
Iteration 228/1000 | Loss: 0.00003414
Iteration 229/1000 | Loss: 0.00003414
Iteration 230/1000 | Loss: 0.00003414
Iteration 231/1000 | Loss: 0.00003414
Iteration 232/1000 | Loss: 0.00003413
Iteration 233/1000 | Loss: 0.00003412
Iteration 234/1000 | Loss: 0.00003412
Iteration 235/1000 | Loss: 0.00003412
Iteration 236/1000 | Loss: 0.00003412
Iteration 237/1000 | Loss: 0.00003412
Iteration 238/1000 | Loss: 0.00003412
Iteration 239/1000 | Loss: 0.00003412
Iteration 240/1000 | Loss: 0.00003412
Iteration 241/1000 | Loss: 0.00003411
Iteration 242/1000 | Loss: 0.00003411
Iteration 243/1000 | Loss: 0.00003410
Iteration 244/1000 | Loss: 0.00003409
Iteration 245/1000 | Loss: 0.00003409
Iteration 246/1000 | Loss: 0.00003408
Iteration 247/1000 | Loss: 0.00003408
Iteration 248/1000 | Loss: 0.00003408
Iteration 249/1000 | Loss: 0.00003408
Iteration 250/1000 | Loss: 0.00003408
Iteration 251/1000 | Loss: 0.00003408
Iteration 252/1000 | Loss: 0.00003408
Iteration 253/1000 | Loss: 0.00003408
Iteration 254/1000 | Loss: 0.00003408
Iteration 255/1000 | Loss: 0.00003408
Iteration 256/1000 | Loss: 0.00003408
Iteration 257/1000 | Loss: 0.00003408
Iteration 258/1000 | Loss: 0.00003408
Iteration 259/1000 | Loss: 0.00003408
Iteration 260/1000 | Loss: 0.00003408
Iteration 261/1000 | Loss: 0.00003408
Iteration 262/1000 | Loss: 0.00003407
Iteration 263/1000 | Loss: 0.00003407
Iteration 264/1000 | Loss: 0.00003407
Iteration 265/1000 | Loss: 0.00003407
Iteration 266/1000 | Loss: 0.00003407
Iteration 267/1000 | Loss: 0.00003407
Iteration 268/1000 | Loss: 0.00003407
Iteration 269/1000 | Loss: 0.00003407
Iteration 270/1000 | Loss: 0.00003407
Iteration 271/1000 | Loss: 0.00003407
Iteration 272/1000 | Loss: 0.00003406
Iteration 273/1000 | Loss: 0.00003406
Iteration 274/1000 | Loss: 0.00003406
Iteration 275/1000 | Loss: 0.00003406
Iteration 276/1000 | Loss: 0.00003406
Iteration 277/1000 | Loss: 0.00003406
Iteration 278/1000 | Loss: 0.00003406
Iteration 279/1000 | Loss: 0.00003406
Iteration 280/1000 | Loss: 0.00003406
Iteration 281/1000 | Loss: 0.00003406
Iteration 282/1000 | Loss: 0.00003406
Iteration 283/1000 | Loss: 0.00003406
Iteration 284/1000 | Loss: 0.00003405
Iteration 285/1000 | Loss: 0.00003405
Iteration 286/1000 | Loss: 0.00003405
Iteration 287/1000 | Loss: 0.00003405
Iteration 288/1000 | Loss: 0.00003405
Iteration 289/1000 | Loss: 0.00003405
Iteration 290/1000 | Loss: 0.00003405
Iteration 291/1000 | Loss: 0.00003404
Iteration 292/1000 | Loss: 0.00003404
Iteration 293/1000 | Loss: 0.00003404
Iteration 294/1000 | Loss: 0.00003404
Iteration 295/1000 | Loss: 0.00003404
Iteration 296/1000 | Loss: 0.00003404
Iteration 297/1000 | Loss: 0.00003404
Iteration 298/1000 | Loss: 0.00003404
Iteration 299/1000 | Loss: 0.00003404
Iteration 300/1000 | Loss: 0.00003404
Iteration 301/1000 | Loss: 0.00003404
Iteration 302/1000 | Loss: 0.00003404
Iteration 303/1000 | Loss: 0.00003404
Iteration 304/1000 | Loss: 0.00003404
Iteration 305/1000 | Loss: 0.00003404
Iteration 306/1000 | Loss: 0.00003404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [3.4039581805700436e-05, 3.4039581805700436e-05, 3.4039581805700436e-05, 3.4039581805700436e-05, 3.4039581805700436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4039581805700436e-05

Optimization complete. Final v2v error: 3.7051899433135986 mm

Highest mean error: 11.109564781188965 mm for frame 45

Lowest mean error: 3.0491650104522705 mm for frame 8

Saving results

Total time: 228.97452211380005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800109
Iteration 2/25 | Loss: 0.00153737
Iteration 3/25 | Loss: 0.00134633
Iteration 4/25 | Loss: 0.00133274
Iteration 5/25 | Loss: 0.00126715
Iteration 6/25 | Loss: 0.00128516
Iteration 7/25 | Loss: 0.00126268
Iteration 8/25 | Loss: 0.00125287
Iteration 9/25 | Loss: 0.00125253
Iteration 10/25 | Loss: 0.00125239
Iteration 11/25 | Loss: 0.00125237
Iteration 12/25 | Loss: 0.00125237
Iteration 13/25 | Loss: 0.00125237
Iteration 14/25 | Loss: 0.00125237
Iteration 15/25 | Loss: 0.00125237
Iteration 16/25 | Loss: 0.00125236
Iteration 17/25 | Loss: 0.00125236
Iteration 18/25 | Loss: 0.00125236
Iteration 19/25 | Loss: 0.00125236
Iteration 20/25 | Loss: 0.00125236
Iteration 21/25 | Loss: 0.00125236
Iteration 22/25 | Loss: 0.00125236
Iteration 23/25 | Loss: 0.00125236
Iteration 24/25 | Loss: 0.00125235
Iteration 25/25 | Loss: 0.00125235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.07466030
Iteration 2/25 | Loss: 0.00146422
Iteration 3/25 | Loss: 0.00146421
Iteration 4/25 | Loss: 0.00146421
Iteration 5/25 | Loss: 0.00146420
Iteration 6/25 | Loss: 0.00146420
Iteration 7/25 | Loss: 0.00146420
Iteration 8/25 | Loss: 0.00146420
Iteration 9/25 | Loss: 0.00146420
Iteration 10/25 | Loss: 0.00146420
Iteration 11/25 | Loss: 0.00146420
Iteration 12/25 | Loss: 0.00146420
Iteration 13/25 | Loss: 0.00146420
Iteration 14/25 | Loss: 0.00146420
Iteration 15/25 | Loss: 0.00146420
Iteration 16/25 | Loss: 0.00146420
Iteration 17/25 | Loss: 0.00146420
Iteration 18/25 | Loss: 0.00146420
Iteration 19/25 | Loss: 0.00146420
Iteration 20/25 | Loss: 0.00146420
Iteration 21/25 | Loss: 0.00146420
Iteration 22/25 | Loss: 0.00146420
Iteration 23/25 | Loss: 0.00146420
Iteration 24/25 | Loss: 0.00146420
Iteration 25/25 | Loss: 0.00146420

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146420
Iteration 2/1000 | Loss: 0.00002849
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00002140
Iteration 5/1000 | Loss: 0.00002054
Iteration 6/1000 | Loss: 0.00020812
Iteration 7/1000 | Loss: 0.00003090
Iteration 8/1000 | Loss: 0.00002342
Iteration 9/1000 | Loss: 0.00002193
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001976
Iteration 12/1000 | Loss: 0.00001961
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001928
Iteration 15/1000 | Loss: 0.00001911
Iteration 16/1000 | Loss: 0.00001909
Iteration 17/1000 | Loss: 0.00001893
Iteration 18/1000 | Loss: 0.00001891
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001884
Iteration 21/1000 | Loss: 0.00001884
Iteration 22/1000 | Loss: 0.00001884
Iteration 23/1000 | Loss: 0.00001884
Iteration 24/1000 | Loss: 0.00001884
Iteration 25/1000 | Loss: 0.00001884
Iteration 26/1000 | Loss: 0.00001883
Iteration 27/1000 | Loss: 0.00001883
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00001879
Iteration 31/1000 | Loss: 0.00017748
Iteration 32/1000 | Loss: 0.00002559
Iteration 33/1000 | Loss: 0.00002193
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001872
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001869
Iteration 41/1000 | Loss: 0.00001868
Iteration 42/1000 | Loss: 0.00001867
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001864
Iteration 45/1000 | Loss: 0.00001864
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001863
Iteration 48/1000 | Loss: 0.00001863
Iteration 49/1000 | Loss: 0.00001863
Iteration 50/1000 | Loss: 0.00001862
Iteration 51/1000 | Loss: 0.00001862
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001859
Iteration 62/1000 | Loss: 0.00001859
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001855
Iteration 76/1000 | Loss: 0.00001855
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00021919
Iteration 80/1000 | Loss: 0.00002464
Iteration 81/1000 | Loss: 0.00002142
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001854
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001851
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001850
Iteration 104/1000 | Loss: 0.00001850
Iteration 105/1000 | Loss: 0.00001849
Iteration 106/1000 | Loss: 0.00001849
Iteration 107/1000 | Loss: 0.00001849
Iteration 108/1000 | Loss: 0.00001849
Iteration 109/1000 | Loss: 0.00001849
Iteration 110/1000 | Loss: 0.00001849
Iteration 111/1000 | Loss: 0.00001849
Iteration 112/1000 | Loss: 0.00001849
Iteration 113/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.849391446739901e-05, 1.849391446739901e-05, 1.849391446739901e-05, 1.849391446739901e-05, 1.849391446739901e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.849391446739901e-05

Optimization complete. Final v2v error: 3.608242988586426 mm

Highest mean error: 4.39158296585083 mm for frame 133

Lowest mean error: 3.0117859840393066 mm for frame 177

Saving results

Total time: 66.54709267616272
