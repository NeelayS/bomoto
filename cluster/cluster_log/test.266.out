Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=266, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14896-14951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451714
Iteration 2/25 | Loss: 0.00154482
Iteration 3/25 | Loss: 0.00144090
Iteration 4/25 | Loss: 0.00142907
Iteration 5/25 | Loss: 0.00142523
Iteration 6/25 | Loss: 0.00142422
Iteration 7/25 | Loss: 0.00142422
Iteration 8/25 | Loss: 0.00142422
Iteration 9/25 | Loss: 0.00142422
Iteration 10/25 | Loss: 0.00142422
Iteration 11/25 | Loss: 0.00142422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014242216711863875, 0.0014242216711863875, 0.0014242216711863875, 0.0014242216711863875, 0.0014242216711863875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014242216711863875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53185332
Iteration 2/25 | Loss: 0.00182431
Iteration 3/25 | Loss: 0.00182431
Iteration 4/25 | Loss: 0.00182431
Iteration 5/25 | Loss: 0.00182431
Iteration 6/25 | Loss: 0.00182431
Iteration 7/25 | Loss: 0.00182430
Iteration 8/25 | Loss: 0.00182430
Iteration 9/25 | Loss: 0.00182430
Iteration 10/25 | Loss: 0.00182430
Iteration 11/25 | Loss: 0.00182430
Iteration 12/25 | Loss: 0.00182430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018243049271404743, 0.0018243049271404743, 0.0018243049271404743, 0.0018243049271404743, 0.0018243049271404743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018243049271404743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182430
Iteration 2/1000 | Loss: 0.00005516
Iteration 3/1000 | Loss: 0.00003731
Iteration 4/1000 | Loss: 0.00003173
Iteration 5/1000 | Loss: 0.00002841
Iteration 6/1000 | Loss: 0.00002743
Iteration 7/1000 | Loss: 0.00002616
Iteration 8/1000 | Loss: 0.00002561
Iteration 9/1000 | Loss: 0.00002525
Iteration 10/1000 | Loss: 0.00002501
Iteration 11/1000 | Loss: 0.00002480
Iteration 12/1000 | Loss: 0.00002472
Iteration 13/1000 | Loss: 0.00002467
Iteration 14/1000 | Loss: 0.00002463
Iteration 15/1000 | Loss: 0.00002461
Iteration 16/1000 | Loss: 0.00002457
Iteration 17/1000 | Loss: 0.00002454
Iteration 18/1000 | Loss: 0.00002451
Iteration 19/1000 | Loss: 0.00002451
Iteration 20/1000 | Loss: 0.00002451
Iteration 21/1000 | Loss: 0.00002451
Iteration 22/1000 | Loss: 0.00002451
Iteration 23/1000 | Loss: 0.00002451
Iteration 24/1000 | Loss: 0.00002450
Iteration 25/1000 | Loss: 0.00002450
Iteration 26/1000 | Loss: 0.00002450
Iteration 27/1000 | Loss: 0.00002450
Iteration 28/1000 | Loss: 0.00002450
Iteration 29/1000 | Loss: 0.00002450
Iteration 30/1000 | Loss: 0.00002449
Iteration 31/1000 | Loss: 0.00002448
Iteration 32/1000 | Loss: 0.00002447
Iteration 33/1000 | Loss: 0.00002446
Iteration 34/1000 | Loss: 0.00002446
Iteration 35/1000 | Loss: 0.00002445
Iteration 36/1000 | Loss: 0.00002445
Iteration 37/1000 | Loss: 0.00002445
Iteration 38/1000 | Loss: 0.00002444
Iteration 39/1000 | Loss: 0.00002444
Iteration 40/1000 | Loss: 0.00002443
Iteration 41/1000 | Loss: 0.00002443
Iteration 42/1000 | Loss: 0.00002443
Iteration 43/1000 | Loss: 0.00002442
Iteration 44/1000 | Loss: 0.00002442
Iteration 45/1000 | Loss: 0.00002441
Iteration 46/1000 | Loss: 0.00002441
Iteration 47/1000 | Loss: 0.00002441
Iteration 48/1000 | Loss: 0.00002440
Iteration 49/1000 | Loss: 0.00002440
Iteration 50/1000 | Loss: 0.00002440
Iteration 51/1000 | Loss: 0.00002439
Iteration 52/1000 | Loss: 0.00002439
Iteration 53/1000 | Loss: 0.00002439
Iteration 54/1000 | Loss: 0.00002438
Iteration 55/1000 | Loss: 0.00002438
Iteration 56/1000 | Loss: 0.00002438
Iteration 57/1000 | Loss: 0.00002437
Iteration 58/1000 | Loss: 0.00002437
Iteration 59/1000 | Loss: 0.00002437
Iteration 60/1000 | Loss: 0.00002436
Iteration 61/1000 | Loss: 0.00002436
Iteration 62/1000 | Loss: 0.00002436
Iteration 63/1000 | Loss: 0.00002435
Iteration 64/1000 | Loss: 0.00002435
Iteration 65/1000 | Loss: 0.00002435
Iteration 66/1000 | Loss: 0.00002434
Iteration 67/1000 | Loss: 0.00002434
Iteration 68/1000 | Loss: 0.00002434
Iteration 69/1000 | Loss: 0.00002434
Iteration 70/1000 | Loss: 0.00002433
Iteration 71/1000 | Loss: 0.00002433
Iteration 72/1000 | Loss: 0.00002433
Iteration 73/1000 | Loss: 0.00002433
Iteration 74/1000 | Loss: 0.00002433
Iteration 75/1000 | Loss: 0.00002432
Iteration 76/1000 | Loss: 0.00002432
Iteration 77/1000 | Loss: 0.00002432
Iteration 78/1000 | Loss: 0.00002431
Iteration 79/1000 | Loss: 0.00002431
Iteration 80/1000 | Loss: 0.00002430
Iteration 81/1000 | Loss: 0.00002430
Iteration 82/1000 | Loss: 0.00002430
Iteration 83/1000 | Loss: 0.00002430
Iteration 84/1000 | Loss: 0.00002429
Iteration 85/1000 | Loss: 0.00002429
Iteration 86/1000 | Loss: 0.00002429
Iteration 87/1000 | Loss: 0.00002429
Iteration 88/1000 | Loss: 0.00002429
Iteration 89/1000 | Loss: 0.00002429
Iteration 90/1000 | Loss: 0.00002428
Iteration 91/1000 | Loss: 0.00002428
Iteration 92/1000 | Loss: 0.00002428
Iteration 93/1000 | Loss: 0.00002428
Iteration 94/1000 | Loss: 0.00002428
Iteration 95/1000 | Loss: 0.00002427
Iteration 96/1000 | Loss: 0.00002427
Iteration 97/1000 | Loss: 0.00002427
Iteration 98/1000 | Loss: 0.00002427
Iteration 99/1000 | Loss: 0.00002427
Iteration 100/1000 | Loss: 0.00002427
Iteration 101/1000 | Loss: 0.00002427
Iteration 102/1000 | Loss: 0.00002427
Iteration 103/1000 | Loss: 0.00002427
Iteration 104/1000 | Loss: 0.00002427
Iteration 105/1000 | Loss: 0.00002427
Iteration 106/1000 | Loss: 0.00002427
Iteration 107/1000 | Loss: 0.00002427
Iteration 108/1000 | Loss: 0.00002426
Iteration 109/1000 | Loss: 0.00002426
Iteration 110/1000 | Loss: 0.00002426
Iteration 111/1000 | Loss: 0.00002426
Iteration 112/1000 | Loss: 0.00002426
Iteration 113/1000 | Loss: 0.00002426
Iteration 114/1000 | Loss: 0.00002426
Iteration 115/1000 | Loss: 0.00002426
Iteration 116/1000 | Loss: 0.00002426
Iteration 117/1000 | Loss: 0.00002426
Iteration 118/1000 | Loss: 0.00002426
Iteration 119/1000 | Loss: 0.00002425
Iteration 120/1000 | Loss: 0.00002425
Iteration 121/1000 | Loss: 0.00002425
Iteration 122/1000 | Loss: 0.00002425
Iteration 123/1000 | Loss: 0.00002425
Iteration 124/1000 | Loss: 0.00002425
Iteration 125/1000 | Loss: 0.00002425
Iteration 126/1000 | Loss: 0.00002425
Iteration 127/1000 | Loss: 0.00002425
Iteration 128/1000 | Loss: 0.00002425
Iteration 129/1000 | Loss: 0.00002425
Iteration 130/1000 | Loss: 0.00002425
Iteration 131/1000 | Loss: 0.00002425
Iteration 132/1000 | Loss: 0.00002425
Iteration 133/1000 | Loss: 0.00002425
Iteration 134/1000 | Loss: 0.00002425
Iteration 135/1000 | Loss: 0.00002425
Iteration 136/1000 | Loss: 0.00002425
Iteration 137/1000 | Loss: 0.00002425
Iteration 138/1000 | Loss: 0.00002425
Iteration 139/1000 | Loss: 0.00002425
Iteration 140/1000 | Loss: 0.00002425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.4253164156107232e-05, 2.4253164156107232e-05, 2.4253164156107232e-05, 2.4253164156107232e-05, 2.4253164156107232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4253164156107232e-05

Optimization complete. Final v2v error: 4.273193836212158 mm

Highest mean error: 4.943203926086426 mm for frame 88

Lowest mean error: 3.971156597137451 mm for frame 180

Saving results

Total time: 37.53890323638916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499091
Iteration 2/25 | Loss: 0.00157790
Iteration 3/25 | Loss: 0.00150070
Iteration 4/25 | Loss: 0.00148318
Iteration 5/25 | Loss: 0.00147641
Iteration 6/25 | Loss: 0.00147528
Iteration 7/25 | Loss: 0.00147528
Iteration 8/25 | Loss: 0.00147528
Iteration 9/25 | Loss: 0.00147528
Iteration 10/25 | Loss: 0.00147528
Iteration 11/25 | Loss: 0.00147528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014752831775695086, 0.0014752831775695086, 0.0014752831775695086, 0.0014752831775695086, 0.0014752831775695086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014752831775695086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51151764
Iteration 2/25 | Loss: 0.00192577
Iteration 3/25 | Loss: 0.00192577
Iteration 4/25 | Loss: 0.00192577
Iteration 5/25 | Loss: 0.00192577
Iteration 6/25 | Loss: 0.00192577
Iteration 7/25 | Loss: 0.00192577
Iteration 8/25 | Loss: 0.00192577
Iteration 9/25 | Loss: 0.00192577
Iteration 10/25 | Loss: 0.00192577
Iteration 11/25 | Loss: 0.00192577
Iteration 12/25 | Loss: 0.00192577
Iteration 13/25 | Loss: 0.00192577
Iteration 14/25 | Loss: 0.00192577
Iteration 15/25 | Loss: 0.00192577
Iteration 16/25 | Loss: 0.00192577
Iteration 17/25 | Loss: 0.00192577
Iteration 18/25 | Loss: 0.00192577
Iteration 19/25 | Loss: 0.00192577
Iteration 20/25 | Loss: 0.00192577
Iteration 21/25 | Loss: 0.00192577
Iteration 22/25 | Loss: 0.00192577
Iteration 23/25 | Loss: 0.00192577
Iteration 24/25 | Loss: 0.00192577
Iteration 25/25 | Loss: 0.00192577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192577
Iteration 2/1000 | Loss: 0.00006585
Iteration 3/1000 | Loss: 0.00004989
Iteration 4/1000 | Loss: 0.00004014
Iteration 5/1000 | Loss: 0.00003742
Iteration 6/1000 | Loss: 0.00003643
Iteration 7/1000 | Loss: 0.00003582
Iteration 8/1000 | Loss: 0.00003535
Iteration 9/1000 | Loss: 0.00003504
Iteration 10/1000 | Loss: 0.00003497
Iteration 11/1000 | Loss: 0.00003478
Iteration 12/1000 | Loss: 0.00003465
Iteration 13/1000 | Loss: 0.00003452
Iteration 14/1000 | Loss: 0.00003451
Iteration 15/1000 | Loss: 0.00003450
Iteration 16/1000 | Loss: 0.00003449
Iteration 17/1000 | Loss: 0.00003449
Iteration 18/1000 | Loss: 0.00003448
Iteration 19/1000 | Loss: 0.00003445
Iteration 20/1000 | Loss: 0.00003444
Iteration 21/1000 | Loss: 0.00003444
Iteration 22/1000 | Loss: 0.00003442
Iteration 23/1000 | Loss: 0.00003441
Iteration 24/1000 | Loss: 0.00003437
Iteration 25/1000 | Loss: 0.00003431
Iteration 26/1000 | Loss: 0.00003430
Iteration 27/1000 | Loss: 0.00003424
Iteration 28/1000 | Loss: 0.00003421
Iteration 29/1000 | Loss: 0.00003421
Iteration 30/1000 | Loss: 0.00003418
Iteration 31/1000 | Loss: 0.00003416
Iteration 32/1000 | Loss: 0.00003413
Iteration 33/1000 | Loss: 0.00003413
Iteration 34/1000 | Loss: 0.00003413
Iteration 35/1000 | Loss: 0.00003412
Iteration 36/1000 | Loss: 0.00003412
Iteration 37/1000 | Loss: 0.00003412
Iteration 38/1000 | Loss: 0.00003411
Iteration 39/1000 | Loss: 0.00003410
Iteration 40/1000 | Loss: 0.00003410
Iteration 41/1000 | Loss: 0.00003410
Iteration 42/1000 | Loss: 0.00003410
Iteration 43/1000 | Loss: 0.00003410
Iteration 44/1000 | Loss: 0.00003410
Iteration 45/1000 | Loss: 0.00003410
Iteration 46/1000 | Loss: 0.00003410
Iteration 47/1000 | Loss: 0.00003410
Iteration 48/1000 | Loss: 0.00003410
Iteration 49/1000 | Loss: 0.00003409
Iteration 50/1000 | Loss: 0.00003409
Iteration 51/1000 | Loss: 0.00003409
Iteration 52/1000 | Loss: 0.00003408
Iteration 53/1000 | Loss: 0.00003408
Iteration 54/1000 | Loss: 0.00003408
Iteration 55/1000 | Loss: 0.00003407
Iteration 56/1000 | Loss: 0.00003407
Iteration 57/1000 | Loss: 0.00003407
Iteration 58/1000 | Loss: 0.00003407
Iteration 59/1000 | Loss: 0.00003407
Iteration 60/1000 | Loss: 0.00003407
Iteration 61/1000 | Loss: 0.00003407
Iteration 62/1000 | Loss: 0.00003407
Iteration 63/1000 | Loss: 0.00003407
Iteration 64/1000 | Loss: 0.00003407
Iteration 65/1000 | Loss: 0.00003407
Iteration 66/1000 | Loss: 0.00003407
Iteration 67/1000 | Loss: 0.00003407
Iteration 68/1000 | Loss: 0.00003406
Iteration 69/1000 | Loss: 0.00003406
Iteration 70/1000 | Loss: 0.00003406
Iteration 71/1000 | Loss: 0.00003405
Iteration 72/1000 | Loss: 0.00003405
Iteration 73/1000 | Loss: 0.00003405
Iteration 74/1000 | Loss: 0.00003405
Iteration 75/1000 | Loss: 0.00003405
Iteration 76/1000 | Loss: 0.00003405
Iteration 77/1000 | Loss: 0.00003405
Iteration 78/1000 | Loss: 0.00003405
Iteration 79/1000 | Loss: 0.00003405
Iteration 80/1000 | Loss: 0.00003405
Iteration 81/1000 | Loss: 0.00003405
Iteration 82/1000 | Loss: 0.00003405
Iteration 83/1000 | Loss: 0.00003405
Iteration 84/1000 | Loss: 0.00003405
Iteration 85/1000 | Loss: 0.00003404
Iteration 86/1000 | Loss: 0.00003404
Iteration 87/1000 | Loss: 0.00003404
Iteration 88/1000 | Loss: 0.00003403
Iteration 89/1000 | Loss: 0.00003403
Iteration 90/1000 | Loss: 0.00003403
Iteration 91/1000 | Loss: 0.00003403
Iteration 92/1000 | Loss: 0.00003403
Iteration 93/1000 | Loss: 0.00003403
Iteration 94/1000 | Loss: 0.00003403
Iteration 95/1000 | Loss: 0.00003403
Iteration 96/1000 | Loss: 0.00003402
Iteration 97/1000 | Loss: 0.00003402
Iteration 98/1000 | Loss: 0.00003402
Iteration 99/1000 | Loss: 0.00003402
Iteration 100/1000 | Loss: 0.00003402
Iteration 101/1000 | Loss: 0.00003402
Iteration 102/1000 | Loss: 0.00003402
Iteration 103/1000 | Loss: 0.00003401
Iteration 104/1000 | Loss: 0.00003401
Iteration 105/1000 | Loss: 0.00003401
Iteration 106/1000 | Loss: 0.00003401
Iteration 107/1000 | Loss: 0.00003401
Iteration 108/1000 | Loss: 0.00003401
Iteration 109/1000 | Loss: 0.00003401
Iteration 110/1000 | Loss: 0.00003401
Iteration 111/1000 | Loss: 0.00003401
Iteration 112/1000 | Loss: 0.00003401
Iteration 113/1000 | Loss: 0.00003401
Iteration 114/1000 | Loss: 0.00003401
Iteration 115/1000 | Loss: 0.00003401
Iteration 116/1000 | Loss: 0.00003401
Iteration 117/1000 | Loss: 0.00003401
Iteration 118/1000 | Loss: 0.00003400
Iteration 119/1000 | Loss: 0.00003400
Iteration 120/1000 | Loss: 0.00003400
Iteration 121/1000 | Loss: 0.00003400
Iteration 122/1000 | Loss: 0.00003400
Iteration 123/1000 | Loss: 0.00003400
Iteration 124/1000 | Loss: 0.00003400
Iteration 125/1000 | Loss: 0.00003400
Iteration 126/1000 | Loss: 0.00003400
Iteration 127/1000 | Loss: 0.00003400
Iteration 128/1000 | Loss: 0.00003400
Iteration 129/1000 | Loss: 0.00003400
Iteration 130/1000 | Loss: 0.00003400
Iteration 131/1000 | Loss: 0.00003400
Iteration 132/1000 | Loss: 0.00003400
Iteration 133/1000 | Loss: 0.00003400
Iteration 134/1000 | Loss: 0.00003400
Iteration 135/1000 | Loss: 0.00003399
Iteration 136/1000 | Loss: 0.00003399
Iteration 137/1000 | Loss: 0.00003399
Iteration 138/1000 | Loss: 0.00003399
Iteration 139/1000 | Loss: 0.00003399
Iteration 140/1000 | Loss: 0.00003399
Iteration 141/1000 | Loss: 0.00003399
Iteration 142/1000 | Loss: 0.00003399
Iteration 143/1000 | Loss: 0.00003399
Iteration 144/1000 | Loss: 0.00003399
Iteration 145/1000 | Loss: 0.00003399
Iteration 146/1000 | Loss: 0.00003399
Iteration 147/1000 | Loss: 0.00003399
Iteration 148/1000 | Loss: 0.00003399
Iteration 149/1000 | Loss: 0.00003399
Iteration 150/1000 | Loss: 0.00003399
Iteration 151/1000 | Loss: 0.00003399
Iteration 152/1000 | Loss: 0.00003399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [3.399460547370836e-05, 3.399460547370836e-05, 3.399460547370836e-05, 3.399460547370836e-05, 3.399460547370836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.399460547370836e-05

Optimization complete. Final v2v error: 5.062116622924805 mm

Highest mean error: 5.268191337585449 mm for frame 27

Lowest mean error: 4.869428634643555 mm for frame 46

Saving results

Total time: 38.167004108428955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813411
Iteration 2/25 | Loss: 0.00159881
Iteration 3/25 | Loss: 0.00151786
Iteration 4/25 | Loss: 0.00150255
Iteration 5/25 | Loss: 0.00149734
Iteration 6/25 | Loss: 0.00149589
Iteration 7/25 | Loss: 0.00149566
Iteration 8/25 | Loss: 0.00149566
Iteration 9/25 | Loss: 0.00149566
Iteration 10/25 | Loss: 0.00149566
Iteration 11/25 | Loss: 0.00149566
Iteration 12/25 | Loss: 0.00149566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014956569066271186, 0.0014956569066271186, 0.0014956569066271186, 0.0014956569066271186, 0.0014956569066271186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014956569066271186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36600447
Iteration 2/25 | Loss: 0.00206320
Iteration 3/25 | Loss: 0.00206320
Iteration 4/25 | Loss: 0.00206320
Iteration 5/25 | Loss: 0.00206320
Iteration 6/25 | Loss: 0.00206320
Iteration 7/25 | Loss: 0.00206320
Iteration 8/25 | Loss: 0.00206320
Iteration 9/25 | Loss: 0.00206320
Iteration 10/25 | Loss: 0.00206320
Iteration 11/25 | Loss: 0.00206320
Iteration 12/25 | Loss: 0.00206320
Iteration 13/25 | Loss: 0.00206320
Iteration 14/25 | Loss: 0.00206320
Iteration 15/25 | Loss: 0.00206320
Iteration 16/25 | Loss: 0.00206320
Iteration 17/25 | Loss: 0.00206320
Iteration 18/25 | Loss: 0.00206320
Iteration 19/25 | Loss: 0.00206320
Iteration 20/25 | Loss: 0.00206320
Iteration 21/25 | Loss: 0.00206320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002063195453956723, 0.002063195453956723, 0.002063195453956723, 0.002063195453956723, 0.002063195453956723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002063195453956723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206320
Iteration 2/1000 | Loss: 0.00005935
Iteration 3/1000 | Loss: 0.00004037
Iteration 4/1000 | Loss: 0.00003490
Iteration 5/1000 | Loss: 0.00003200
Iteration 6/1000 | Loss: 0.00003098
Iteration 7/1000 | Loss: 0.00003046
Iteration 8/1000 | Loss: 0.00003017
Iteration 9/1000 | Loss: 0.00002996
Iteration 10/1000 | Loss: 0.00002993
Iteration 11/1000 | Loss: 0.00002989
Iteration 12/1000 | Loss: 0.00002980
Iteration 13/1000 | Loss: 0.00002980
Iteration 14/1000 | Loss: 0.00002978
Iteration 15/1000 | Loss: 0.00002977
Iteration 16/1000 | Loss: 0.00002974
Iteration 17/1000 | Loss: 0.00002974
Iteration 18/1000 | Loss: 0.00002974
Iteration 19/1000 | Loss: 0.00002974
Iteration 20/1000 | Loss: 0.00002974
Iteration 21/1000 | Loss: 0.00002970
Iteration 22/1000 | Loss: 0.00002970
Iteration 23/1000 | Loss: 0.00002970
Iteration 24/1000 | Loss: 0.00002969
Iteration 25/1000 | Loss: 0.00002969
Iteration 26/1000 | Loss: 0.00002969
Iteration 27/1000 | Loss: 0.00002969
Iteration 28/1000 | Loss: 0.00002969
Iteration 29/1000 | Loss: 0.00002968
Iteration 30/1000 | Loss: 0.00002968
Iteration 31/1000 | Loss: 0.00002967
Iteration 32/1000 | Loss: 0.00002967
Iteration 33/1000 | Loss: 0.00002967
Iteration 34/1000 | Loss: 0.00002967
Iteration 35/1000 | Loss: 0.00002967
Iteration 36/1000 | Loss: 0.00002967
Iteration 37/1000 | Loss: 0.00002967
Iteration 38/1000 | Loss: 0.00002967
Iteration 39/1000 | Loss: 0.00002967
Iteration 40/1000 | Loss: 0.00002966
Iteration 41/1000 | Loss: 0.00002966
Iteration 42/1000 | Loss: 0.00002966
Iteration 43/1000 | Loss: 0.00002965
Iteration 44/1000 | Loss: 0.00002965
Iteration 45/1000 | Loss: 0.00002965
Iteration 46/1000 | Loss: 0.00002965
Iteration 47/1000 | Loss: 0.00002964
Iteration 48/1000 | Loss: 0.00002964
Iteration 49/1000 | Loss: 0.00002964
Iteration 50/1000 | Loss: 0.00002964
Iteration 51/1000 | Loss: 0.00002964
Iteration 52/1000 | Loss: 0.00002964
Iteration 53/1000 | Loss: 0.00002963
Iteration 54/1000 | Loss: 0.00002963
Iteration 55/1000 | Loss: 0.00002963
Iteration 56/1000 | Loss: 0.00002963
Iteration 57/1000 | Loss: 0.00002963
Iteration 58/1000 | Loss: 0.00002963
Iteration 59/1000 | Loss: 0.00002963
Iteration 60/1000 | Loss: 0.00002963
Iteration 61/1000 | Loss: 0.00002963
Iteration 62/1000 | Loss: 0.00002962
Iteration 63/1000 | Loss: 0.00002962
Iteration 64/1000 | Loss: 0.00002962
Iteration 65/1000 | Loss: 0.00002962
Iteration 66/1000 | Loss: 0.00002962
Iteration 67/1000 | Loss: 0.00002962
Iteration 68/1000 | Loss: 0.00002962
Iteration 69/1000 | Loss: 0.00002961
Iteration 70/1000 | Loss: 0.00002961
Iteration 71/1000 | Loss: 0.00002961
Iteration 72/1000 | Loss: 0.00002960
Iteration 73/1000 | Loss: 0.00002960
Iteration 74/1000 | Loss: 0.00002960
Iteration 75/1000 | Loss: 0.00002960
Iteration 76/1000 | Loss: 0.00002959
Iteration 77/1000 | Loss: 0.00002959
Iteration 78/1000 | Loss: 0.00002959
Iteration 79/1000 | Loss: 0.00002958
Iteration 80/1000 | Loss: 0.00002958
Iteration 81/1000 | Loss: 0.00002958
Iteration 82/1000 | Loss: 0.00002958
Iteration 83/1000 | Loss: 0.00002958
Iteration 84/1000 | Loss: 0.00002958
Iteration 85/1000 | Loss: 0.00002958
Iteration 86/1000 | Loss: 0.00002957
Iteration 87/1000 | Loss: 0.00002957
Iteration 88/1000 | Loss: 0.00002957
Iteration 89/1000 | Loss: 0.00002957
Iteration 90/1000 | Loss: 0.00002957
Iteration 91/1000 | Loss: 0.00002957
Iteration 92/1000 | Loss: 0.00002956
Iteration 93/1000 | Loss: 0.00002956
Iteration 94/1000 | Loss: 0.00002956
Iteration 95/1000 | Loss: 0.00002956
Iteration 96/1000 | Loss: 0.00002956
Iteration 97/1000 | Loss: 0.00002956
Iteration 98/1000 | Loss: 0.00002956
Iteration 99/1000 | Loss: 0.00002955
Iteration 100/1000 | Loss: 0.00002955
Iteration 101/1000 | Loss: 0.00002955
Iteration 102/1000 | Loss: 0.00002955
Iteration 103/1000 | Loss: 0.00002955
Iteration 104/1000 | Loss: 0.00002955
Iteration 105/1000 | Loss: 0.00002955
Iteration 106/1000 | Loss: 0.00002955
Iteration 107/1000 | Loss: 0.00002954
Iteration 108/1000 | Loss: 0.00002954
Iteration 109/1000 | Loss: 0.00002954
Iteration 110/1000 | Loss: 0.00002954
Iteration 111/1000 | Loss: 0.00002953
Iteration 112/1000 | Loss: 0.00002953
Iteration 113/1000 | Loss: 0.00002953
Iteration 114/1000 | Loss: 0.00002953
Iteration 115/1000 | Loss: 0.00002953
Iteration 116/1000 | Loss: 0.00002952
Iteration 117/1000 | Loss: 0.00002952
Iteration 118/1000 | Loss: 0.00002952
Iteration 119/1000 | Loss: 0.00002951
Iteration 120/1000 | Loss: 0.00002951
Iteration 121/1000 | Loss: 0.00002951
Iteration 122/1000 | Loss: 0.00002951
Iteration 123/1000 | Loss: 0.00002951
Iteration 124/1000 | Loss: 0.00002951
Iteration 125/1000 | Loss: 0.00002951
Iteration 126/1000 | Loss: 0.00002951
Iteration 127/1000 | Loss: 0.00002951
Iteration 128/1000 | Loss: 0.00002950
Iteration 129/1000 | Loss: 0.00002950
Iteration 130/1000 | Loss: 0.00002950
Iteration 131/1000 | Loss: 0.00002949
Iteration 132/1000 | Loss: 0.00002949
Iteration 133/1000 | Loss: 0.00002949
Iteration 134/1000 | Loss: 0.00002949
Iteration 135/1000 | Loss: 0.00002949
Iteration 136/1000 | Loss: 0.00002949
Iteration 137/1000 | Loss: 0.00002949
Iteration 138/1000 | Loss: 0.00002948
Iteration 139/1000 | Loss: 0.00002948
Iteration 140/1000 | Loss: 0.00002948
Iteration 141/1000 | Loss: 0.00002948
Iteration 142/1000 | Loss: 0.00002948
Iteration 143/1000 | Loss: 0.00002948
Iteration 144/1000 | Loss: 0.00002948
Iteration 145/1000 | Loss: 0.00002947
Iteration 146/1000 | Loss: 0.00002947
Iteration 147/1000 | Loss: 0.00002947
Iteration 148/1000 | Loss: 0.00002947
Iteration 149/1000 | Loss: 0.00002947
Iteration 150/1000 | Loss: 0.00002947
Iteration 151/1000 | Loss: 0.00002947
Iteration 152/1000 | Loss: 0.00002947
Iteration 153/1000 | Loss: 0.00002947
Iteration 154/1000 | Loss: 0.00002947
Iteration 155/1000 | Loss: 0.00002947
Iteration 156/1000 | Loss: 0.00002947
Iteration 157/1000 | Loss: 0.00002946
Iteration 158/1000 | Loss: 0.00002946
Iteration 159/1000 | Loss: 0.00002946
Iteration 160/1000 | Loss: 0.00002946
Iteration 161/1000 | Loss: 0.00002946
Iteration 162/1000 | Loss: 0.00002946
Iteration 163/1000 | Loss: 0.00002946
Iteration 164/1000 | Loss: 0.00002946
Iteration 165/1000 | Loss: 0.00002946
Iteration 166/1000 | Loss: 0.00002946
Iteration 167/1000 | Loss: 0.00002946
Iteration 168/1000 | Loss: 0.00002946
Iteration 169/1000 | Loss: 0.00002946
Iteration 170/1000 | Loss: 0.00002946
Iteration 171/1000 | Loss: 0.00002946
Iteration 172/1000 | Loss: 0.00002946
Iteration 173/1000 | Loss: 0.00002946
Iteration 174/1000 | Loss: 0.00002946
Iteration 175/1000 | Loss: 0.00002946
Iteration 176/1000 | Loss: 0.00002946
Iteration 177/1000 | Loss: 0.00002946
Iteration 178/1000 | Loss: 0.00002946
Iteration 179/1000 | Loss: 0.00002946
Iteration 180/1000 | Loss: 0.00002946
Iteration 181/1000 | Loss: 0.00002946
Iteration 182/1000 | Loss: 0.00002946
Iteration 183/1000 | Loss: 0.00002946
Iteration 184/1000 | Loss: 0.00002946
Iteration 185/1000 | Loss: 0.00002946
Iteration 186/1000 | Loss: 0.00002946
Iteration 187/1000 | Loss: 0.00002946
Iteration 188/1000 | Loss: 0.00002946
Iteration 189/1000 | Loss: 0.00002946
Iteration 190/1000 | Loss: 0.00002946
Iteration 191/1000 | Loss: 0.00002946
Iteration 192/1000 | Loss: 0.00002946
Iteration 193/1000 | Loss: 0.00002946
Iteration 194/1000 | Loss: 0.00002946
Iteration 195/1000 | Loss: 0.00002946
Iteration 196/1000 | Loss: 0.00002946
Iteration 197/1000 | Loss: 0.00002946
Iteration 198/1000 | Loss: 0.00002946
Iteration 199/1000 | Loss: 0.00002946
Iteration 200/1000 | Loss: 0.00002946
Iteration 201/1000 | Loss: 0.00002946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.9458205972332507e-05, 2.9458205972332507e-05, 2.9458205972332507e-05, 2.9458205972332507e-05, 2.9458205972332507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9458205972332507e-05

Optimization complete. Final v2v error: 4.832496643066406 mm

Highest mean error: 4.998007297515869 mm for frame 161

Lowest mean error: 4.590116024017334 mm for frame 67

Saving results

Total time: 36.21397662162781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741161
Iteration 2/25 | Loss: 0.00177426
Iteration 3/25 | Loss: 0.00145332
Iteration 4/25 | Loss: 0.00141481
Iteration 5/25 | Loss: 0.00140571
Iteration 6/25 | Loss: 0.00140351
Iteration 7/25 | Loss: 0.00140284
Iteration 8/25 | Loss: 0.00140259
Iteration 9/25 | Loss: 0.00140246
Iteration 10/25 | Loss: 0.00140244
Iteration 11/25 | Loss: 0.00140244
Iteration 12/25 | Loss: 0.00140244
Iteration 13/25 | Loss: 0.00140244
Iteration 14/25 | Loss: 0.00140244
Iteration 15/25 | Loss: 0.00140244
Iteration 16/25 | Loss: 0.00140244
Iteration 17/25 | Loss: 0.00140244
Iteration 18/25 | Loss: 0.00140243
Iteration 19/25 | Loss: 0.00140243
Iteration 20/25 | Loss: 0.00140243
Iteration 21/25 | Loss: 0.00140243
Iteration 22/25 | Loss: 0.00140243
Iteration 23/25 | Loss: 0.00140243
Iteration 24/25 | Loss: 0.00140243
Iteration 25/25 | Loss: 0.00140243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.94894361
Iteration 2/25 | Loss: 0.00166746
Iteration 3/25 | Loss: 0.00166738
Iteration 4/25 | Loss: 0.00166738
Iteration 5/25 | Loss: 0.00166737
Iteration 6/25 | Loss: 0.00166737
Iteration 7/25 | Loss: 0.00166737
Iteration 8/25 | Loss: 0.00166737
Iteration 9/25 | Loss: 0.00166737
Iteration 10/25 | Loss: 0.00166737
Iteration 11/25 | Loss: 0.00166737
Iteration 12/25 | Loss: 0.00166737
Iteration 13/25 | Loss: 0.00166737
Iteration 14/25 | Loss: 0.00166737
Iteration 15/25 | Loss: 0.00166737
Iteration 16/25 | Loss: 0.00166737
Iteration 17/25 | Loss: 0.00166737
Iteration 18/25 | Loss: 0.00166737
Iteration 19/25 | Loss: 0.00166737
Iteration 20/25 | Loss: 0.00166737
Iteration 21/25 | Loss: 0.00166737
Iteration 22/25 | Loss: 0.00166737
Iteration 23/25 | Loss: 0.00166737
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016673729987815022, 0.0016673729987815022, 0.0016673729987815022, 0.0016673729987815022, 0.0016673729987815022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016673729987815022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166737
Iteration 2/1000 | Loss: 0.00007003
Iteration 3/1000 | Loss: 0.00004684
Iteration 4/1000 | Loss: 0.00004007
Iteration 5/1000 | Loss: 0.00003557
Iteration 6/1000 | Loss: 0.00003338
Iteration 7/1000 | Loss: 0.00003250
Iteration 8/1000 | Loss: 0.00003201
Iteration 9/1000 | Loss: 0.00003165
Iteration 10/1000 | Loss: 0.00003127
Iteration 11/1000 | Loss: 0.00003105
Iteration 12/1000 | Loss: 0.00003104
Iteration 13/1000 | Loss: 0.00003099
Iteration 14/1000 | Loss: 0.00003094
Iteration 15/1000 | Loss: 0.00003091
Iteration 16/1000 | Loss: 0.00003091
Iteration 17/1000 | Loss: 0.00003087
Iteration 18/1000 | Loss: 0.00003084
Iteration 19/1000 | Loss: 0.00003083
Iteration 20/1000 | Loss: 0.00003083
Iteration 21/1000 | Loss: 0.00003082
Iteration 22/1000 | Loss: 0.00003081
Iteration 23/1000 | Loss: 0.00003080
Iteration 24/1000 | Loss: 0.00003080
Iteration 25/1000 | Loss: 0.00003079
Iteration 26/1000 | Loss: 0.00003077
Iteration 27/1000 | Loss: 0.00003077
Iteration 28/1000 | Loss: 0.00003077
Iteration 29/1000 | Loss: 0.00003077
Iteration 30/1000 | Loss: 0.00003076
Iteration 31/1000 | Loss: 0.00003076
Iteration 32/1000 | Loss: 0.00003075
Iteration 33/1000 | Loss: 0.00003074
Iteration 34/1000 | Loss: 0.00003074
Iteration 35/1000 | Loss: 0.00003074
Iteration 36/1000 | Loss: 0.00003074
Iteration 37/1000 | Loss: 0.00003074
Iteration 38/1000 | Loss: 0.00003073
Iteration 39/1000 | Loss: 0.00003073
Iteration 40/1000 | Loss: 0.00003072
Iteration 41/1000 | Loss: 0.00003072
Iteration 42/1000 | Loss: 0.00003072
Iteration 43/1000 | Loss: 0.00003072
Iteration 44/1000 | Loss: 0.00003072
Iteration 45/1000 | Loss: 0.00003072
Iteration 46/1000 | Loss: 0.00003072
Iteration 47/1000 | Loss: 0.00003072
Iteration 48/1000 | Loss: 0.00003071
Iteration 49/1000 | Loss: 0.00003071
Iteration 50/1000 | Loss: 0.00003071
Iteration 51/1000 | Loss: 0.00003070
Iteration 52/1000 | Loss: 0.00003070
Iteration 53/1000 | Loss: 0.00003070
Iteration 54/1000 | Loss: 0.00003070
Iteration 55/1000 | Loss: 0.00003069
Iteration 56/1000 | Loss: 0.00003069
Iteration 57/1000 | Loss: 0.00003068
Iteration 58/1000 | Loss: 0.00003068
Iteration 59/1000 | Loss: 0.00003068
Iteration 60/1000 | Loss: 0.00003067
Iteration 61/1000 | Loss: 0.00003067
Iteration 62/1000 | Loss: 0.00003067
Iteration 63/1000 | Loss: 0.00003067
Iteration 64/1000 | Loss: 0.00003066
Iteration 65/1000 | Loss: 0.00003066
Iteration 66/1000 | Loss: 0.00003066
Iteration 67/1000 | Loss: 0.00003066
Iteration 68/1000 | Loss: 0.00003065
Iteration 69/1000 | Loss: 0.00003065
Iteration 70/1000 | Loss: 0.00003065
Iteration 71/1000 | Loss: 0.00003065
Iteration 72/1000 | Loss: 0.00003064
Iteration 73/1000 | Loss: 0.00003064
Iteration 74/1000 | Loss: 0.00003064
Iteration 75/1000 | Loss: 0.00003064
Iteration 76/1000 | Loss: 0.00003064
Iteration 77/1000 | Loss: 0.00003064
Iteration 78/1000 | Loss: 0.00003064
Iteration 79/1000 | Loss: 0.00003064
Iteration 80/1000 | Loss: 0.00003063
Iteration 81/1000 | Loss: 0.00003063
Iteration 82/1000 | Loss: 0.00003063
Iteration 83/1000 | Loss: 0.00003063
Iteration 84/1000 | Loss: 0.00003062
Iteration 85/1000 | Loss: 0.00003062
Iteration 86/1000 | Loss: 0.00003062
Iteration 87/1000 | Loss: 0.00003061
Iteration 88/1000 | Loss: 0.00003061
Iteration 89/1000 | Loss: 0.00003061
Iteration 90/1000 | Loss: 0.00003061
Iteration 91/1000 | Loss: 0.00003060
Iteration 92/1000 | Loss: 0.00003060
Iteration 93/1000 | Loss: 0.00003060
Iteration 94/1000 | Loss: 0.00003060
Iteration 95/1000 | Loss: 0.00003059
Iteration 96/1000 | Loss: 0.00003059
Iteration 97/1000 | Loss: 0.00003059
Iteration 98/1000 | Loss: 0.00003058
Iteration 99/1000 | Loss: 0.00003058
Iteration 100/1000 | Loss: 0.00003058
Iteration 101/1000 | Loss: 0.00003058
Iteration 102/1000 | Loss: 0.00003057
Iteration 103/1000 | Loss: 0.00003057
Iteration 104/1000 | Loss: 0.00003057
Iteration 105/1000 | Loss: 0.00003057
Iteration 106/1000 | Loss: 0.00003057
Iteration 107/1000 | Loss: 0.00003057
Iteration 108/1000 | Loss: 0.00003056
Iteration 109/1000 | Loss: 0.00003056
Iteration 110/1000 | Loss: 0.00003055
Iteration 111/1000 | Loss: 0.00003055
Iteration 112/1000 | Loss: 0.00003054
Iteration 113/1000 | Loss: 0.00003054
Iteration 114/1000 | Loss: 0.00003054
Iteration 115/1000 | Loss: 0.00003054
Iteration 116/1000 | Loss: 0.00003054
Iteration 117/1000 | Loss: 0.00003054
Iteration 118/1000 | Loss: 0.00003054
Iteration 119/1000 | Loss: 0.00003053
Iteration 120/1000 | Loss: 0.00003053
Iteration 121/1000 | Loss: 0.00003052
Iteration 122/1000 | Loss: 0.00003052
Iteration 123/1000 | Loss: 0.00003052
Iteration 124/1000 | Loss: 0.00003052
Iteration 125/1000 | Loss: 0.00003051
Iteration 126/1000 | Loss: 0.00003051
Iteration 127/1000 | Loss: 0.00003051
Iteration 128/1000 | Loss: 0.00003051
Iteration 129/1000 | Loss: 0.00003050
Iteration 130/1000 | Loss: 0.00003050
Iteration 131/1000 | Loss: 0.00003050
Iteration 132/1000 | Loss: 0.00003050
Iteration 133/1000 | Loss: 0.00003050
Iteration 134/1000 | Loss: 0.00003049
Iteration 135/1000 | Loss: 0.00003049
Iteration 136/1000 | Loss: 0.00003049
Iteration 137/1000 | Loss: 0.00003048
Iteration 138/1000 | Loss: 0.00003048
Iteration 139/1000 | Loss: 0.00003048
Iteration 140/1000 | Loss: 0.00003047
Iteration 141/1000 | Loss: 0.00003047
Iteration 142/1000 | Loss: 0.00003046
Iteration 143/1000 | Loss: 0.00003045
Iteration 144/1000 | Loss: 0.00003045
Iteration 145/1000 | Loss: 0.00003045
Iteration 146/1000 | Loss: 0.00003045
Iteration 147/1000 | Loss: 0.00003045
Iteration 148/1000 | Loss: 0.00003044
Iteration 149/1000 | Loss: 0.00003044
Iteration 150/1000 | Loss: 0.00003043
Iteration 151/1000 | Loss: 0.00003043
Iteration 152/1000 | Loss: 0.00003043
Iteration 153/1000 | Loss: 0.00003043
Iteration 154/1000 | Loss: 0.00003043
Iteration 155/1000 | Loss: 0.00003042
Iteration 156/1000 | Loss: 0.00003042
Iteration 157/1000 | Loss: 0.00003042
Iteration 158/1000 | Loss: 0.00003042
Iteration 159/1000 | Loss: 0.00003042
Iteration 160/1000 | Loss: 0.00003042
Iteration 161/1000 | Loss: 0.00003042
Iteration 162/1000 | Loss: 0.00003042
Iteration 163/1000 | Loss: 0.00003042
Iteration 164/1000 | Loss: 0.00003042
Iteration 165/1000 | Loss: 0.00003042
Iteration 166/1000 | Loss: 0.00003041
Iteration 167/1000 | Loss: 0.00003041
Iteration 168/1000 | Loss: 0.00003041
Iteration 169/1000 | Loss: 0.00003041
Iteration 170/1000 | Loss: 0.00003041
Iteration 171/1000 | Loss: 0.00003041
Iteration 172/1000 | Loss: 0.00003040
Iteration 173/1000 | Loss: 0.00003040
Iteration 174/1000 | Loss: 0.00003040
Iteration 175/1000 | Loss: 0.00003040
Iteration 176/1000 | Loss: 0.00003040
Iteration 177/1000 | Loss: 0.00003040
Iteration 178/1000 | Loss: 0.00003040
Iteration 179/1000 | Loss: 0.00003040
Iteration 180/1000 | Loss: 0.00003039
Iteration 181/1000 | Loss: 0.00003039
Iteration 182/1000 | Loss: 0.00003039
Iteration 183/1000 | Loss: 0.00003039
Iteration 184/1000 | Loss: 0.00003039
Iteration 185/1000 | Loss: 0.00003039
Iteration 186/1000 | Loss: 0.00003039
Iteration 187/1000 | Loss: 0.00003039
Iteration 188/1000 | Loss: 0.00003039
Iteration 189/1000 | Loss: 0.00003039
Iteration 190/1000 | Loss: 0.00003039
Iteration 191/1000 | Loss: 0.00003038
Iteration 192/1000 | Loss: 0.00003038
Iteration 193/1000 | Loss: 0.00003038
Iteration 194/1000 | Loss: 0.00003038
Iteration 195/1000 | Loss: 0.00003038
Iteration 196/1000 | Loss: 0.00003038
Iteration 197/1000 | Loss: 0.00003038
Iteration 198/1000 | Loss: 0.00003038
Iteration 199/1000 | Loss: 0.00003038
Iteration 200/1000 | Loss: 0.00003037
Iteration 201/1000 | Loss: 0.00003037
Iteration 202/1000 | Loss: 0.00003037
Iteration 203/1000 | Loss: 0.00003037
Iteration 204/1000 | Loss: 0.00003037
Iteration 205/1000 | Loss: 0.00003037
Iteration 206/1000 | Loss: 0.00003037
Iteration 207/1000 | Loss: 0.00003037
Iteration 208/1000 | Loss: 0.00003037
Iteration 209/1000 | Loss: 0.00003037
Iteration 210/1000 | Loss: 0.00003037
Iteration 211/1000 | Loss: 0.00003037
Iteration 212/1000 | Loss: 0.00003037
Iteration 213/1000 | Loss: 0.00003037
Iteration 214/1000 | Loss: 0.00003037
Iteration 215/1000 | Loss: 0.00003037
Iteration 216/1000 | Loss: 0.00003037
Iteration 217/1000 | Loss: 0.00003037
Iteration 218/1000 | Loss: 0.00003037
Iteration 219/1000 | Loss: 0.00003037
Iteration 220/1000 | Loss: 0.00003037
Iteration 221/1000 | Loss: 0.00003037
Iteration 222/1000 | Loss: 0.00003037
Iteration 223/1000 | Loss: 0.00003037
Iteration 224/1000 | Loss: 0.00003037
Iteration 225/1000 | Loss: 0.00003037
Iteration 226/1000 | Loss: 0.00003037
Iteration 227/1000 | Loss: 0.00003037
Iteration 228/1000 | Loss: 0.00003037
Iteration 229/1000 | Loss: 0.00003037
Iteration 230/1000 | Loss: 0.00003037
Iteration 231/1000 | Loss: 0.00003037
Iteration 232/1000 | Loss: 0.00003037
Iteration 233/1000 | Loss: 0.00003037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [3.036861562577542e-05, 3.036861562577542e-05, 3.036861562577542e-05, 3.036861562577542e-05, 3.036861562577542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.036861562577542e-05

Optimization complete. Final v2v error: 4.733731746673584 mm

Highest mean error: 11.039505958557129 mm for frame 37

Lowest mean error: 4.257077693939209 mm for frame 11

Saving results

Total time: 57.022693395614624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808612
Iteration 2/25 | Loss: 0.00153414
Iteration 3/25 | Loss: 0.00136192
Iteration 4/25 | Loss: 0.00134065
Iteration 5/25 | Loss: 0.00133911
Iteration 6/25 | Loss: 0.00134095
Iteration 7/25 | Loss: 0.00133784
Iteration 8/25 | Loss: 0.00132265
Iteration 9/25 | Loss: 0.00132214
Iteration 10/25 | Loss: 0.00131576
Iteration 11/25 | Loss: 0.00131388
Iteration 12/25 | Loss: 0.00131275
Iteration 13/25 | Loss: 0.00131244
Iteration 14/25 | Loss: 0.00131234
Iteration 15/25 | Loss: 0.00131232
Iteration 16/25 | Loss: 0.00131231
Iteration 17/25 | Loss: 0.00131231
Iteration 18/25 | Loss: 0.00131231
Iteration 19/25 | Loss: 0.00131231
Iteration 20/25 | Loss: 0.00131231
Iteration 21/25 | Loss: 0.00131231
Iteration 22/25 | Loss: 0.00131231
Iteration 23/25 | Loss: 0.00131231
Iteration 24/25 | Loss: 0.00131230
Iteration 25/25 | Loss: 0.00131230

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37661505
Iteration 2/25 | Loss: 0.00081789
Iteration 3/25 | Loss: 0.00081774
Iteration 4/25 | Loss: 0.00081774
Iteration 5/25 | Loss: 0.00081774
Iteration 6/25 | Loss: 0.00081774
Iteration 7/25 | Loss: 0.00081774
Iteration 8/25 | Loss: 0.00081774
Iteration 9/25 | Loss: 0.00081774
Iteration 10/25 | Loss: 0.00081774
Iteration 11/25 | Loss: 0.00081774
Iteration 12/25 | Loss: 0.00081774
Iteration 13/25 | Loss: 0.00081774
Iteration 14/25 | Loss: 0.00081774
Iteration 15/25 | Loss: 0.00081774
Iteration 16/25 | Loss: 0.00081774
Iteration 17/25 | Loss: 0.00081774
Iteration 18/25 | Loss: 0.00081774
Iteration 19/25 | Loss: 0.00081774
Iteration 20/25 | Loss: 0.00081774
Iteration 21/25 | Loss: 0.00081774
Iteration 22/25 | Loss: 0.00081774
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008177359704859555, 0.0008177359704859555, 0.0008177359704859555, 0.0008177359704859555, 0.0008177359704859555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008177359704859555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081774
Iteration 2/1000 | Loss: 0.00005011
Iteration 3/1000 | Loss: 0.00003413
Iteration 4/1000 | Loss: 0.00003105
Iteration 5/1000 | Loss: 0.00002922
Iteration 6/1000 | Loss: 0.00002783
Iteration 7/1000 | Loss: 0.00002694
Iteration 8/1000 | Loss: 0.00002628
Iteration 9/1000 | Loss: 0.00002583
Iteration 10/1000 | Loss: 0.00002540
Iteration 11/1000 | Loss: 0.00002504
Iteration 12/1000 | Loss: 0.00002484
Iteration 13/1000 | Loss: 0.00002467
Iteration 14/1000 | Loss: 0.00002456
Iteration 15/1000 | Loss: 0.00002451
Iteration 16/1000 | Loss: 0.00002444
Iteration 17/1000 | Loss: 0.00002440
Iteration 18/1000 | Loss: 0.00002440
Iteration 19/1000 | Loss: 0.00002439
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002437
Iteration 22/1000 | Loss: 0.00002436
Iteration 23/1000 | Loss: 0.00002434
Iteration 24/1000 | Loss: 0.00002431
Iteration 25/1000 | Loss: 0.00002431
Iteration 26/1000 | Loss: 0.00002430
Iteration 27/1000 | Loss: 0.00002429
Iteration 28/1000 | Loss: 0.00002429
Iteration 29/1000 | Loss: 0.00002428
Iteration 30/1000 | Loss: 0.00002428
Iteration 31/1000 | Loss: 0.00002427
Iteration 32/1000 | Loss: 0.00002427
Iteration 33/1000 | Loss: 0.00002427
Iteration 34/1000 | Loss: 0.00002427
Iteration 35/1000 | Loss: 0.00002427
Iteration 36/1000 | Loss: 0.00002426
Iteration 37/1000 | Loss: 0.00002426
Iteration 38/1000 | Loss: 0.00002426
Iteration 39/1000 | Loss: 0.00002425
Iteration 40/1000 | Loss: 0.00002425
Iteration 41/1000 | Loss: 0.00002424
Iteration 42/1000 | Loss: 0.00002424
Iteration 43/1000 | Loss: 0.00002423
Iteration 44/1000 | Loss: 0.00002423
Iteration 45/1000 | Loss: 0.00002423
Iteration 46/1000 | Loss: 0.00002422
Iteration 47/1000 | Loss: 0.00002422
Iteration 48/1000 | Loss: 0.00002422
Iteration 49/1000 | Loss: 0.00002421
Iteration 50/1000 | Loss: 0.00002421
Iteration 51/1000 | Loss: 0.00002420
Iteration 52/1000 | Loss: 0.00002420
Iteration 53/1000 | Loss: 0.00002420
Iteration 54/1000 | Loss: 0.00002419
Iteration 55/1000 | Loss: 0.00002419
Iteration 56/1000 | Loss: 0.00002419
Iteration 57/1000 | Loss: 0.00002419
Iteration 58/1000 | Loss: 0.00002418
Iteration 59/1000 | Loss: 0.00002418
Iteration 60/1000 | Loss: 0.00002418
Iteration 61/1000 | Loss: 0.00002417
Iteration 62/1000 | Loss: 0.00002417
Iteration 63/1000 | Loss: 0.00002417
Iteration 64/1000 | Loss: 0.00002416
Iteration 65/1000 | Loss: 0.00002416
Iteration 66/1000 | Loss: 0.00002416
Iteration 67/1000 | Loss: 0.00002416
Iteration 68/1000 | Loss: 0.00002416
Iteration 69/1000 | Loss: 0.00002416
Iteration 70/1000 | Loss: 0.00002416
Iteration 71/1000 | Loss: 0.00002416
Iteration 72/1000 | Loss: 0.00002416
Iteration 73/1000 | Loss: 0.00002416
Iteration 74/1000 | Loss: 0.00002416
Iteration 75/1000 | Loss: 0.00002416
Iteration 76/1000 | Loss: 0.00002415
Iteration 77/1000 | Loss: 0.00002415
Iteration 78/1000 | Loss: 0.00002415
Iteration 79/1000 | Loss: 0.00002415
Iteration 80/1000 | Loss: 0.00002414
Iteration 81/1000 | Loss: 0.00002414
Iteration 82/1000 | Loss: 0.00002414
Iteration 83/1000 | Loss: 0.00002414
Iteration 84/1000 | Loss: 0.00002414
Iteration 85/1000 | Loss: 0.00002413
Iteration 86/1000 | Loss: 0.00002413
Iteration 87/1000 | Loss: 0.00002413
Iteration 88/1000 | Loss: 0.00002413
Iteration 89/1000 | Loss: 0.00002413
Iteration 90/1000 | Loss: 0.00002413
Iteration 91/1000 | Loss: 0.00002412
Iteration 92/1000 | Loss: 0.00002412
Iteration 93/1000 | Loss: 0.00002412
Iteration 94/1000 | Loss: 0.00002412
Iteration 95/1000 | Loss: 0.00002412
Iteration 96/1000 | Loss: 0.00002411
Iteration 97/1000 | Loss: 0.00002411
Iteration 98/1000 | Loss: 0.00002411
Iteration 99/1000 | Loss: 0.00002411
Iteration 100/1000 | Loss: 0.00002411
Iteration 101/1000 | Loss: 0.00002411
Iteration 102/1000 | Loss: 0.00002411
Iteration 103/1000 | Loss: 0.00002411
Iteration 104/1000 | Loss: 0.00002411
Iteration 105/1000 | Loss: 0.00002411
Iteration 106/1000 | Loss: 0.00002410
Iteration 107/1000 | Loss: 0.00002410
Iteration 108/1000 | Loss: 0.00002410
Iteration 109/1000 | Loss: 0.00002410
Iteration 110/1000 | Loss: 0.00002410
Iteration 111/1000 | Loss: 0.00002410
Iteration 112/1000 | Loss: 0.00002410
Iteration 113/1000 | Loss: 0.00002410
Iteration 114/1000 | Loss: 0.00002410
Iteration 115/1000 | Loss: 0.00002410
Iteration 116/1000 | Loss: 0.00002410
Iteration 117/1000 | Loss: 0.00002409
Iteration 118/1000 | Loss: 0.00002409
Iteration 119/1000 | Loss: 0.00002409
Iteration 120/1000 | Loss: 0.00002409
Iteration 121/1000 | Loss: 0.00002409
Iteration 122/1000 | Loss: 0.00002409
Iteration 123/1000 | Loss: 0.00002409
Iteration 124/1000 | Loss: 0.00002409
Iteration 125/1000 | Loss: 0.00002408
Iteration 126/1000 | Loss: 0.00002408
Iteration 127/1000 | Loss: 0.00002408
Iteration 128/1000 | Loss: 0.00002408
Iteration 129/1000 | Loss: 0.00002408
Iteration 130/1000 | Loss: 0.00002408
Iteration 131/1000 | Loss: 0.00002408
Iteration 132/1000 | Loss: 0.00002407
Iteration 133/1000 | Loss: 0.00002407
Iteration 134/1000 | Loss: 0.00002407
Iteration 135/1000 | Loss: 0.00002407
Iteration 136/1000 | Loss: 0.00002407
Iteration 137/1000 | Loss: 0.00002407
Iteration 138/1000 | Loss: 0.00002407
Iteration 139/1000 | Loss: 0.00002407
Iteration 140/1000 | Loss: 0.00002407
Iteration 141/1000 | Loss: 0.00002406
Iteration 142/1000 | Loss: 0.00002406
Iteration 143/1000 | Loss: 0.00002406
Iteration 144/1000 | Loss: 0.00002406
Iteration 145/1000 | Loss: 0.00002406
Iteration 146/1000 | Loss: 0.00002406
Iteration 147/1000 | Loss: 0.00002406
Iteration 148/1000 | Loss: 0.00002405
Iteration 149/1000 | Loss: 0.00002405
Iteration 150/1000 | Loss: 0.00002405
Iteration 151/1000 | Loss: 0.00002405
Iteration 152/1000 | Loss: 0.00002405
Iteration 153/1000 | Loss: 0.00002405
Iteration 154/1000 | Loss: 0.00002405
Iteration 155/1000 | Loss: 0.00002405
Iteration 156/1000 | Loss: 0.00002405
Iteration 157/1000 | Loss: 0.00002405
Iteration 158/1000 | Loss: 0.00002405
Iteration 159/1000 | Loss: 0.00002405
Iteration 160/1000 | Loss: 0.00002405
Iteration 161/1000 | Loss: 0.00002405
Iteration 162/1000 | Loss: 0.00002405
Iteration 163/1000 | Loss: 0.00002405
Iteration 164/1000 | Loss: 0.00002404
Iteration 165/1000 | Loss: 0.00002404
Iteration 166/1000 | Loss: 0.00002404
Iteration 167/1000 | Loss: 0.00002404
Iteration 168/1000 | Loss: 0.00002404
Iteration 169/1000 | Loss: 0.00002404
Iteration 170/1000 | Loss: 0.00002404
Iteration 171/1000 | Loss: 0.00002404
Iteration 172/1000 | Loss: 0.00002404
Iteration 173/1000 | Loss: 0.00002404
Iteration 174/1000 | Loss: 0.00002404
Iteration 175/1000 | Loss: 0.00002404
Iteration 176/1000 | Loss: 0.00002404
Iteration 177/1000 | Loss: 0.00002404
Iteration 178/1000 | Loss: 0.00002404
Iteration 179/1000 | Loss: 0.00002404
Iteration 180/1000 | Loss: 0.00002404
Iteration 181/1000 | Loss: 0.00002404
Iteration 182/1000 | Loss: 0.00002404
Iteration 183/1000 | Loss: 0.00002404
Iteration 184/1000 | Loss: 0.00002404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.4044778911047615e-05, 2.4044778911047615e-05, 2.4044778911047615e-05, 2.4044778911047615e-05, 2.4044778911047615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4044778911047615e-05

Optimization complete. Final v2v error: 4.0506463050842285 mm

Highest mean error: 4.965477466583252 mm for frame 158

Lowest mean error: 3.364813804626465 mm for frame 101

Saving results

Total time: 65.13414096832275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081069
Iteration 2/25 | Loss: 0.01081069
Iteration 3/25 | Loss: 0.01081069
Iteration 4/25 | Loss: 0.01081069
Iteration 5/25 | Loss: 0.01081069
Iteration 6/25 | Loss: 0.01081069
Iteration 7/25 | Loss: 0.01081069
Iteration 8/25 | Loss: 0.01081069
Iteration 9/25 | Loss: 0.01081068
Iteration 10/25 | Loss: 0.01081068
Iteration 11/25 | Loss: 0.01081068
Iteration 12/25 | Loss: 0.01081068
Iteration 13/25 | Loss: 0.01081068
Iteration 14/25 | Loss: 0.01081068
Iteration 15/25 | Loss: 0.01081068
Iteration 16/25 | Loss: 0.01081068
Iteration 17/25 | Loss: 0.01081068
Iteration 18/25 | Loss: 0.01081068
Iteration 19/25 | Loss: 0.01081068
Iteration 20/25 | Loss: 0.01081068
Iteration 21/25 | Loss: 0.01081067
Iteration 22/25 | Loss: 0.01081067
Iteration 23/25 | Loss: 0.01081067
Iteration 24/25 | Loss: 0.01081067
Iteration 25/25 | Loss: 0.01081067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76663673
Iteration 2/25 | Loss: 0.15400684
Iteration 3/25 | Loss: 0.15399118
Iteration 4/25 | Loss: 0.15399116
Iteration 5/25 | Loss: 0.15399116
Iteration 6/25 | Loss: 0.15399116
Iteration 7/25 | Loss: 0.15399116
Iteration 8/25 | Loss: 0.15399116
Iteration 9/25 | Loss: 0.15399116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.15399116277694702, 0.15399116277694702, 0.15399116277694702, 0.15399116277694702, 0.15399116277694702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15399116277694702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15399116
Iteration 2/1000 | Loss: 0.00387817
Iteration 3/1000 | Loss: 0.00082328
Iteration 4/1000 | Loss: 0.00034277
Iteration 5/1000 | Loss: 0.00011950
Iteration 6/1000 | Loss: 0.00009010
Iteration 7/1000 | Loss: 0.00004931
Iteration 8/1000 | Loss: 0.00060138
Iteration 9/1000 | Loss: 0.00005615
Iteration 10/1000 | Loss: 0.00004398
Iteration 11/1000 | Loss: 0.00006534
Iteration 12/1000 | Loss: 0.00007023
Iteration 13/1000 | Loss: 0.00015517
Iteration 14/1000 | Loss: 0.00003204
Iteration 15/1000 | Loss: 0.00005699
Iteration 16/1000 | Loss: 0.00004599
Iteration 17/1000 | Loss: 0.00002568
Iteration 18/1000 | Loss: 0.00010296
Iteration 19/1000 | Loss: 0.00008304
Iteration 20/1000 | Loss: 0.00002434
Iteration 21/1000 | Loss: 0.00007961
Iteration 22/1000 | Loss: 0.00002337
Iteration 23/1000 | Loss: 0.00002789
Iteration 24/1000 | Loss: 0.00009837
Iteration 25/1000 | Loss: 0.00002181
Iteration 26/1000 | Loss: 0.00002079
Iteration 27/1000 | Loss: 0.00002817
Iteration 28/1000 | Loss: 0.00001959
Iteration 29/1000 | Loss: 0.00019411
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00007849
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00005916
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00006230
Iteration 37/1000 | Loss: 0.00003607
Iteration 38/1000 | Loss: 0.00007719
Iteration 39/1000 | Loss: 0.00001825
Iteration 40/1000 | Loss: 0.00001798
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00004807
Iteration 43/1000 | Loss: 0.00002278
Iteration 44/1000 | Loss: 0.00004977
Iteration 45/1000 | Loss: 0.00002252
Iteration 46/1000 | Loss: 0.00004208
Iteration 47/1000 | Loss: 0.00021107
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001774
Iteration 50/1000 | Loss: 0.00001773
Iteration 51/1000 | Loss: 0.00001769
Iteration 52/1000 | Loss: 0.00001769
Iteration 53/1000 | Loss: 0.00001769
Iteration 54/1000 | Loss: 0.00001768
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001768
Iteration 57/1000 | Loss: 0.00005622
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001763
Iteration 60/1000 | Loss: 0.00001763
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001760
Iteration 63/1000 | Loss: 0.00001760
Iteration 64/1000 | Loss: 0.00001760
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00002829
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001748
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001747
Iteration 75/1000 | Loss: 0.00001747
Iteration 76/1000 | Loss: 0.00001747
Iteration 77/1000 | Loss: 0.00001747
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001747
Iteration 81/1000 | Loss: 0.00001747
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00001746
Iteration 84/1000 | Loss: 0.00001746
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00001746
Iteration 88/1000 | Loss: 0.00001746
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001745
Iteration 92/1000 | Loss: 0.00001745
Iteration 93/1000 | Loss: 0.00001745
Iteration 94/1000 | Loss: 0.00001745
Iteration 95/1000 | Loss: 0.00001745
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001745
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001744
Iteration 101/1000 | Loss: 0.00001744
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001744
Iteration 108/1000 | Loss: 0.00001744
Iteration 109/1000 | Loss: 0.00001744
Iteration 110/1000 | Loss: 0.00001744
Iteration 111/1000 | Loss: 0.00001744
Iteration 112/1000 | Loss: 0.00001744
Iteration 113/1000 | Loss: 0.00001744
Iteration 114/1000 | Loss: 0.00001744
Iteration 115/1000 | Loss: 0.00001744
Iteration 116/1000 | Loss: 0.00001744
Iteration 117/1000 | Loss: 0.00001744
Iteration 118/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.7438434952055104e-05, 1.7438434952055104e-05, 1.7438434952055104e-05, 1.7438434952055104e-05, 1.7438434952055104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7438434952055104e-05

Optimization complete. Final v2v error: 3.5549259185791016 mm

Highest mean error: 3.9965217113494873 mm for frame 200

Lowest mean error: 3.341482162475586 mm for frame 111

Saving results

Total time: 88.3291666507721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909156
Iteration 2/25 | Loss: 0.00158129
Iteration 3/25 | Loss: 0.00146252
Iteration 4/25 | Loss: 0.00138134
Iteration 5/25 | Loss: 0.00137771
Iteration 6/25 | Loss: 0.00134177
Iteration 7/25 | Loss: 0.00134263
Iteration 8/25 | Loss: 0.00133996
Iteration 9/25 | Loss: 0.00133461
Iteration 10/25 | Loss: 0.00133315
Iteration 11/25 | Loss: 0.00133327
Iteration 12/25 | Loss: 0.00133050
Iteration 13/25 | Loss: 0.00132868
Iteration 14/25 | Loss: 0.00133324
Iteration 15/25 | Loss: 0.00133091
Iteration 16/25 | Loss: 0.00133241
Iteration 17/25 | Loss: 0.00132895
Iteration 18/25 | Loss: 0.00133101
Iteration 19/25 | Loss: 0.00133098
Iteration 20/25 | Loss: 0.00133080
Iteration 21/25 | Loss: 0.00133080
Iteration 22/25 | Loss: 0.00133321
Iteration 23/25 | Loss: 0.00133071
Iteration 24/25 | Loss: 0.00133040
Iteration 25/25 | Loss: 0.00133046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35506845
Iteration 2/25 | Loss: 0.00269815
Iteration 3/25 | Loss: 0.00082455
Iteration 4/25 | Loss: 0.00082455
Iteration 5/25 | Loss: 0.00082455
Iteration 6/25 | Loss: 0.00082455
Iteration 7/25 | Loss: 0.00082455
Iteration 8/25 | Loss: 0.00082455
Iteration 9/25 | Loss: 0.00082455
Iteration 10/25 | Loss: 0.00082455
Iteration 11/25 | Loss: 0.00082455
Iteration 12/25 | Loss: 0.00082455
Iteration 13/25 | Loss: 0.00082455
Iteration 14/25 | Loss: 0.00082455
Iteration 15/25 | Loss: 0.00082455
Iteration 16/25 | Loss: 0.00082455
Iteration 17/25 | Loss: 0.00082455
Iteration 18/25 | Loss: 0.00082455
Iteration 19/25 | Loss: 0.00082455
Iteration 20/25 | Loss: 0.00082455
Iteration 21/25 | Loss: 0.00082455
Iteration 22/25 | Loss: 0.00082455
Iteration 23/25 | Loss: 0.00082455
Iteration 24/25 | Loss: 0.00082455
Iteration 25/25 | Loss: 0.00082455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082455
Iteration 2/1000 | Loss: 0.00008081
Iteration 3/1000 | Loss: 0.00014484
Iteration 4/1000 | Loss: 0.00004726
Iteration 5/1000 | Loss: 0.00111971
Iteration 6/1000 | Loss: 0.00007991
Iteration 7/1000 | Loss: 0.00009007
Iteration 8/1000 | Loss: 0.00014848
Iteration 9/1000 | Loss: 0.00014799
Iteration 10/1000 | Loss: 0.00013339
Iteration 11/1000 | Loss: 0.00013711
Iteration 12/1000 | Loss: 0.00020755
Iteration 13/1000 | Loss: 0.00013409
Iteration 14/1000 | Loss: 0.00020627
Iteration 15/1000 | Loss: 0.00011570
Iteration 16/1000 | Loss: 0.00019538
Iteration 17/1000 | Loss: 0.00008290
Iteration 18/1000 | Loss: 0.00010544
Iteration 19/1000 | Loss: 0.00018306
Iteration 20/1000 | Loss: 0.00014505
Iteration 21/1000 | Loss: 0.00017075
Iteration 22/1000 | Loss: 0.00013764
Iteration 23/1000 | Loss: 0.00014477
Iteration 24/1000 | Loss: 0.00013970
Iteration 25/1000 | Loss: 0.00014202
Iteration 26/1000 | Loss: 0.00014448
Iteration 27/1000 | Loss: 0.00012516
Iteration 28/1000 | Loss: 0.00124506
Iteration 29/1000 | Loss: 0.00032887
Iteration 30/1000 | Loss: 0.00011117
Iteration 31/1000 | Loss: 0.00010973
Iteration 32/1000 | Loss: 0.00017776
Iteration 33/1000 | Loss: 0.00017174
Iteration 34/1000 | Loss: 0.00017150
Iteration 35/1000 | Loss: 0.00016877
Iteration 36/1000 | Loss: 0.00017934
Iteration 37/1000 | Loss: 0.00017021
Iteration 38/1000 | Loss: 0.00016867
Iteration 39/1000 | Loss: 0.00015789
Iteration 40/1000 | Loss: 0.00016224
Iteration 41/1000 | Loss: 0.00017029
Iteration 42/1000 | Loss: 0.00033759
Iteration 43/1000 | Loss: 0.00012535
Iteration 44/1000 | Loss: 0.00025585
Iteration 45/1000 | Loss: 0.00004047
Iteration 46/1000 | Loss: 0.00029559
Iteration 47/1000 | Loss: 0.00003501
Iteration 48/1000 | Loss: 0.00003182
Iteration 49/1000 | Loss: 0.00003026
Iteration 50/1000 | Loss: 0.00002946
Iteration 51/1000 | Loss: 0.00002851
Iteration 52/1000 | Loss: 0.00018589
Iteration 53/1000 | Loss: 0.00002781
Iteration 54/1000 | Loss: 0.00002695
Iteration 55/1000 | Loss: 0.00002620
Iteration 56/1000 | Loss: 0.00002572
Iteration 57/1000 | Loss: 0.00002516
Iteration 58/1000 | Loss: 0.00002484
Iteration 59/1000 | Loss: 0.00002450
Iteration 60/1000 | Loss: 0.00002442
Iteration 61/1000 | Loss: 0.00002423
Iteration 62/1000 | Loss: 0.00002415
Iteration 63/1000 | Loss: 0.00002412
Iteration 64/1000 | Loss: 0.00002412
Iteration 65/1000 | Loss: 0.00002410
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002407
Iteration 68/1000 | Loss: 0.00002397
Iteration 69/1000 | Loss: 0.00002390
Iteration 70/1000 | Loss: 0.00002385
Iteration 71/1000 | Loss: 0.00002385
Iteration 72/1000 | Loss: 0.00002385
Iteration 73/1000 | Loss: 0.00002385
Iteration 74/1000 | Loss: 0.00002384
Iteration 75/1000 | Loss: 0.00002384
Iteration 76/1000 | Loss: 0.00002384
Iteration 77/1000 | Loss: 0.00002383
Iteration 78/1000 | Loss: 0.00002382
Iteration 79/1000 | Loss: 0.00002381
Iteration 80/1000 | Loss: 0.00002381
Iteration 81/1000 | Loss: 0.00002381
Iteration 82/1000 | Loss: 0.00002380
Iteration 83/1000 | Loss: 0.00002380
Iteration 84/1000 | Loss: 0.00002379
Iteration 85/1000 | Loss: 0.00002379
Iteration 86/1000 | Loss: 0.00002378
Iteration 87/1000 | Loss: 0.00002378
Iteration 88/1000 | Loss: 0.00002377
Iteration 89/1000 | Loss: 0.00002374
Iteration 90/1000 | Loss: 0.00002373
Iteration 91/1000 | Loss: 0.00002373
Iteration 92/1000 | Loss: 0.00002371
Iteration 93/1000 | Loss: 0.00002371
Iteration 94/1000 | Loss: 0.00002371
Iteration 95/1000 | Loss: 0.00002370
Iteration 96/1000 | Loss: 0.00002370
Iteration 97/1000 | Loss: 0.00002368
Iteration 98/1000 | Loss: 0.00002367
Iteration 99/1000 | Loss: 0.00002367
Iteration 100/1000 | Loss: 0.00002366
Iteration 101/1000 | Loss: 0.00002365
Iteration 102/1000 | Loss: 0.00002364
Iteration 103/1000 | Loss: 0.00002364
Iteration 104/1000 | Loss: 0.00002364
Iteration 105/1000 | Loss: 0.00002364
Iteration 106/1000 | Loss: 0.00002364
Iteration 107/1000 | Loss: 0.00002364
Iteration 108/1000 | Loss: 0.00002364
Iteration 109/1000 | Loss: 0.00002364
Iteration 110/1000 | Loss: 0.00002364
Iteration 111/1000 | Loss: 0.00002363
Iteration 112/1000 | Loss: 0.00002363
Iteration 113/1000 | Loss: 0.00002362
Iteration 114/1000 | Loss: 0.00002362
Iteration 115/1000 | Loss: 0.00002361
Iteration 116/1000 | Loss: 0.00002361
Iteration 117/1000 | Loss: 0.00002361
Iteration 118/1000 | Loss: 0.00002361
Iteration 119/1000 | Loss: 0.00002361
Iteration 120/1000 | Loss: 0.00002360
Iteration 121/1000 | Loss: 0.00002360
Iteration 122/1000 | Loss: 0.00002360
Iteration 123/1000 | Loss: 0.00002359
Iteration 124/1000 | Loss: 0.00002359
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002358
Iteration 127/1000 | Loss: 0.00002358
Iteration 128/1000 | Loss: 0.00002358
Iteration 129/1000 | Loss: 0.00002357
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002357
Iteration 132/1000 | Loss: 0.00002357
Iteration 133/1000 | Loss: 0.00002357
Iteration 134/1000 | Loss: 0.00002357
Iteration 135/1000 | Loss: 0.00002357
Iteration 136/1000 | Loss: 0.00002356
Iteration 137/1000 | Loss: 0.00002356
Iteration 138/1000 | Loss: 0.00002356
Iteration 139/1000 | Loss: 0.00002356
Iteration 140/1000 | Loss: 0.00002356
Iteration 141/1000 | Loss: 0.00002356
Iteration 142/1000 | Loss: 0.00002356
Iteration 143/1000 | Loss: 0.00002356
Iteration 144/1000 | Loss: 0.00002356
Iteration 145/1000 | Loss: 0.00002356
Iteration 146/1000 | Loss: 0.00002356
Iteration 147/1000 | Loss: 0.00002356
Iteration 148/1000 | Loss: 0.00002356
Iteration 149/1000 | Loss: 0.00002356
Iteration 150/1000 | Loss: 0.00002356
Iteration 151/1000 | Loss: 0.00002356
Iteration 152/1000 | Loss: 0.00002355
Iteration 153/1000 | Loss: 0.00002355
Iteration 154/1000 | Loss: 0.00002355
Iteration 155/1000 | Loss: 0.00002355
Iteration 156/1000 | Loss: 0.00002355
Iteration 157/1000 | Loss: 0.00002355
Iteration 158/1000 | Loss: 0.00002355
Iteration 159/1000 | Loss: 0.00002355
Iteration 160/1000 | Loss: 0.00002355
Iteration 161/1000 | Loss: 0.00002354
Iteration 162/1000 | Loss: 0.00002354
Iteration 163/1000 | Loss: 0.00002354
Iteration 164/1000 | Loss: 0.00002354
Iteration 165/1000 | Loss: 0.00002354
Iteration 166/1000 | Loss: 0.00002353
Iteration 167/1000 | Loss: 0.00002353
Iteration 168/1000 | Loss: 0.00002353
Iteration 169/1000 | Loss: 0.00002353
Iteration 170/1000 | Loss: 0.00002353
Iteration 171/1000 | Loss: 0.00002353
Iteration 172/1000 | Loss: 0.00002352
Iteration 173/1000 | Loss: 0.00002352
Iteration 174/1000 | Loss: 0.00002352
Iteration 175/1000 | Loss: 0.00002351
Iteration 176/1000 | Loss: 0.00002351
Iteration 177/1000 | Loss: 0.00002351
Iteration 178/1000 | Loss: 0.00002350
Iteration 179/1000 | Loss: 0.00002350
Iteration 180/1000 | Loss: 0.00002349
Iteration 181/1000 | Loss: 0.00002349
Iteration 182/1000 | Loss: 0.00002348
Iteration 183/1000 | Loss: 0.00002348
Iteration 184/1000 | Loss: 0.00002348
Iteration 185/1000 | Loss: 0.00002347
Iteration 186/1000 | Loss: 0.00002347
Iteration 187/1000 | Loss: 0.00002347
Iteration 188/1000 | Loss: 0.00002346
Iteration 189/1000 | Loss: 0.00002346
Iteration 190/1000 | Loss: 0.00002346
Iteration 191/1000 | Loss: 0.00002346
Iteration 192/1000 | Loss: 0.00002346
Iteration 193/1000 | Loss: 0.00002346
Iteration 194/1000 | Loss: 0.00002346
Iteration 195/1000 | Loss: 0.00002346
Iteration 196/1000 | Loss: 0.00002346
Iteration 197/1000 | Loss: 0.00002346
Iteration 198/1000 | Loss: 0.00002346
Iteration 199/1000 | Loss: 0.00002346
Iteration 200/1000 | Loss: 0.00002346
Iteration 201/1000 | Loss: 0.00002345
Iteration 202/1000 | Loss: 0.00002345
Iteration 203/1000 | Loss: 0.00002345
Iteration 204/1000 | Loss: 0.00002345
Iteration 205/1000 | Loss: 0.00002345
Iteration 206/1000 | Loss: 0.00002345
Iteration 207/1000 | Loss: 0.00002345
Iteration 208/1000 | Loss: 0.00002345
Iteration 209/1000 | Loss: 0.00002344
Iteration 210/1000 | Loss: 0.00002344
Iteration 211/1000 | Loss: 0.00002344
Iteration 212/1000 | Loss: 0.00002344
Iteration 213/1000 | Loss: 0.00002344
Iteration 214/1000 | Loss: 0.00002344
Iteration 215/1000 | Loss: 0.00002344
Iteration 216/1000 | Loss: 0.00002344
Iteration 217/1000 | Loss: 0.00002343
Iteration 218/1000 | Loss: 0.00002343
Iteration 219/1000 | Loss: 0.00002343
Iteration 220/1000 | Loss: 0.00002343
Iteration 221/1000 | Loss: 0.00002343
Iteration 222/1000 | Loss: 0.00002343
Iteration 223/1000 | Loss: 0.00002343
Iteration 224/1000 | Loss: 0.00002343
Iteration 225/1000 | Loss: 0.00002343
Iteration 226/1000 | Loss: 0.00002343
Iteration 227/1000 | Loss: 0.00002343
Iteration 228/1000 | Loss: 0.00002343
Iteration 229/1000 | Loss: 0.00002343
Iteration 230/1000 | Loss: 0.00002343
Iteration 231/1000 | Loss: 0.00002343
Iteration 232/1000 | Loss: 0.00002343
Iteration 233/1000 | Loss: 0.00002343
Iteration 234/1000 | Loss: 0.00002343
Iteration 235/1000 | Loss: 0.00002343
Iteration 236/1000 | Loss: 0.00002343
Iteration 237/1000 | Loss: 0.00002343
Iteration 238/1000 | Loss: 0.00002343
Iteration 239/1000 | Loss: 0.00002343
Iteration 240/1000 | Loss: 0.00002343
Iteration 241/1000 | Loss: 0.00002343
Iteration 242/1000 | Loss: 0.00002343
Iteration 243/1000 | Loss: 0.00002343
Iteration 244/1000 | Loss: 0.00002343
Iteration 245/1000 | Loss: 0.00002343
Iteration 246/1000 | Loss: 0.00002343
Iteration 247/1000 | Loss: 0.00002343
Iteration 248/1000 | Loss: 0.00002343
Iteration 249/1000 | Loss: 0.00002343
Iteration 250/1000 | Loss: 0.00002343
Iteration 251/1000 | Loss: 0.00002343
Iteration 252/1000 | Loss: 0.00002343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [2.343027517781593e-05, 2.343027517781593e-05, 2.343027517781593e-05, 2.343027517781593e-05, 2.343027517781593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.343027517781593e-05

Optimization complete. Final v2v error: 4.076453685760498 mm

Highest mean error: 5.674386024475098 mm for frame 136

Lowest mean error: 3.500675916671753 mm for frame 33

Saving results

Total time: 144.77752804756165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00711489
Iteration 2/25 | Loss: 0.00139873
Iteration 3/25 | Loss: 0.00127426
Iteration 4/25 | Loss: 0.00124287
Iteration 5/25 | Loss: 0.00123530
Iteration 6/25 | Loss: 0.00124009
Iteration 7/25 | Loss: 0.00123061
Iteration 8/25 | Loss: 0.00123445
Iteration 9/25 | Loss: 0.00122954
Iteration 10/25 | Loss: 0.00122926
Iteration 11/25 | Loss: 0.00122913
Iteration 12/25 | Loss: 0.00122907
Iteration 13/25 | Loss: 0.00122906
Iteration 14/25 | Loss: 0.00122906
Iteration 15/25 | Loss: 0.00122906
Iteration 16/25 | Loss: 0.00122906
Iteration 17/25 | Loss: 0.00122906
Iteration 18/25 | Loss: 0.00122905
Iteration 19/25 | Loss: 0.00122905
Iteration 20/25 | Loss: 0.00122905
Iteration 21/25 | Loss: 0.00122905
Iteration 22/25 | Loss: 0.00122905
Iteration 23/25 | Loss: 0.00122905
Iteration 24/25 | Loss: 0.00122905
Iteration 25/25 | Loss: 0.00122905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.88841391
Iteration 2/25 | Loss: 0.00086655
Iteration 3/25 | Loss: 0.00086655
Iteration 4/25 | Loss: 0.00086655
Iteration 5/25 | Loss: 0.00086655
Iteration 6/25 | Loss: 0.00086655
Iteration 7/25 | Loss: 0.00086655
Iteration 8/25 | Loss: 0.00086655
Iteration 9/25 | Loss: 0.00086655
Iteration 10/25 | Loss: 0.00086655
Iteration 11/25 | Loss: 0.00086655
Iteration 12/25 | Loss: 0.00086654
Iteration 13/25 | Loss: 0.00086654
Iteration 14/25 | Loss: 0.00086654
Iteration 15/25 | Loss: 0.00086654
Iteration 16/25 | Loss: 0.00086654
Iteration 17/25 | Loss: 0.00086654
Iteration 18/25 | Loss: 0.00086654
Iteration 19/25 | Loss: 0.00086654
Iteration 20/25 | Loss: 0.00086654
Iteration 21/25 | Loss: 0.00086654
Iteration 22/25 | Loss: 0.00086654
Iteration 23/25 | Loss: 0.00086654
Iteration 24/25 | Loss: 0.00086654
Iteration 25/25 | Loss: 0.00086654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086654
Iteration 2/1000 | Loss: 0.00002080
Iteration 3/1000 | Loss: 0.00001621
Iteration 4/1000 | Loss: 0.00001469
Iteration 5/1000 | Loss: 0.00001395
Iteration 6/1000 | Loss: 0.00006536
Iteration 7/1000 | Loss: 0.00001340
Iteration 8/1000 | Loss: 0.00001307
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001255
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001239
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001238
Iteration 18/1000 | Loss: 0.00001236
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001227
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001220
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00006104
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001213
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001211
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001208
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00001208
Iteration 44/1000 | Loss: 0.00001208
Iteration 45/1000 | Loss: 0.00001208
Iteration 46/1000 | Loss: 0.00001208
Iteration 47/1000 | Loss: 0.00001208
Iteration 48/1000 | Loss: 0.00001208
Iteration 49/1000 | Loss: 0.00001207
Iteration 50/1000 | Loss: 0.00001207
Iteration 51/1000 | Loss: 0.00001207
Iteration 52/1000 | Loss: 0.00001206
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00006011
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001188
Iteration 65/1000 | Loss: 0.00001188
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001184
Iteration 73/1000 | Loss: 0.00001184
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001183
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001182
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001179
Iteration 93/1000 | Loss: 0.00001179
Iteration 94/1000 | Loss: 0.00001179
Iteration 95/1000 | Loss: 0.00001178
Iteration 96/1000 | Loss: 0.00001178
Iteration 97/1000 | Loss: 0.00001178
Iteration 98/1000 | Loss: 0.00001178
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001177
Iteration 101/1000 | Loss: 0.00001177
Iteration 102/1000 | Loss: 0.00001177
Iteration 103/1000 | Loss: 0.00001177
Iteration 104/1000 | Loss: 0.00001177
Iteration 105/1000 | Loss: 0.00001177
Iteration 106/1000 | Loss: 0.00001176
Iteration 107/1000 | Loss: 0.00001176
Iteration 108/1000 | Loss: 0.00001176
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001175
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001172
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001171
Iteration 124/1000 | Loss: 0.00001171
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00008116
Iteration 132/1000 | Loss: 0.00002067
Iteration 133/1000 | Loss: 0.00001180
Iteration 134/1000 | Loss: 0.00001422
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001170
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001488
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001170
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001167
Iteration 154/1000 | Loss: 0.00001167
Iteration 155/1000 | Loss: 0.00001167
Iteration 156/1000 | Loss: 0.00001167
Iteration 157/1000 | Loss: 0.00001167
Iteration 158/1000 | Loss: 0.00001167
Iteration 159/1000 | Loss: 0.00001166
Iteration 160/1000 | Loss: 0.00001166
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001166
Iteration 163/1000 | Loss: 0.00001166
Iteration 164/1000 | Loss: 0.00001166
Iteration 165/1000 | Loss: 0.00001166
Iteration 166/1000 | Loss: 0.00001166
Iteration 167/1000 | Loss: 0.00001165
Iteration 168/1000 | Loss: 0.00001165
Iteration 169/1000 | Loss: 0.00001165
Iteration 170/1000 | Loss: 0.00001164
Iteration 171/1000 | Loss: 0.00001164
Iteration 172/1000 | Loss: 0.00001164
Iteration 173/1000 | Loss: 0.00001164
Iteration 174/1000 | Loss: 0.00001164
Iteration 175/1000 | Loss: 0.00001164
Iteration 176/1000 | Loss: 0.00001164
Iteration 177/1000 | Loss: 0.00001164
Iteration 178/1000 | Loss: 0.00001164
Iteration 179/1000 | Loss: 0.00001164
Iteration 180/1000 | Loss: 0.00001164
Iteration 181/1000 | Loss: 0.00001164
Iteration 182/1000 | Loss: 0.00001164
Iteration 183/1000 | Loss: 0.00001164
Iteration 184/1000 | Loss: 0.00001163
Iteration 185/1000 | Loss: 0.00001163
Iteration 186/1000 | Loss: 0.00001163
Iteration 187/1000 | Loss: 0.00001163
Iteration 188/1000 | Loss: 0.00001162
Iteration 189/1000 | Loss: 0.00001162
Iteration 190/1000 | Loss: 0.00001162
Iteration 191/1000 | Loss: 0.00001162
Iteration 192/1000 | Loss: 0.00001162
Iteration 193/1000 | Loss: 0.00001162
Iteration 194/1000 | Loss: 0.00001162
Iteration 195/1000 | Loss: 0.00001162
Iteration 196/1000 | Loss: 0.00001162
Iteration 197/1000 | Loss: 0.00001162
Iteration 198/1000 | Loss: 0.00001162
Iteration 199/1000 | Loss: 0.00001162
Iteration 200/1000 | Loss: 0.00001162
Iteration 201/1000 | Loss: 0.00001161
Iteration 202/1000 | Loss: 0.00001161
Iteration 203/1000 | Loss: 0.00001161
Iteration 204/1000 | Loss: 0.00001161
Iteration 205/1000 | Loss: 0.00001161
Iteration 206/1000 | Loss: 0.00001161
Iteration 207/1000 | Loss: 0.00001161
Iteration 208/1000 | Loss: 0.00001161
Iteration 209/1000 | Loss: 0.00001161
Iteration 210/1000 | Loss: 0.00001161
Iteration 211/1000 | Loss: 0.00001161
Iteration 212/1000 | Loss: 0.00001160
Iteration 213/1000 | Loss: 0.00001160
Iteration 214/1000 | Loss: 0.00001160
Iteration 215/1000 | Loss: 0.00001160
Iteration 216/1000 | Loss: 0.00001160
Iteration 217/1000 | Loss: 0.00001160
Iteration 218/1000 | Loss: 0.00001160
Iteration 219/1000 | Loss: 0.00001160
Iteration 220/1000 | Loss: 0.00001160
Iteration 221/1000 | Loss: 0.00001160
Iteration 222/1000 | Loss: 0.00001160
Iteration 223/1000 | Loss: 0.00001160
Iteration 224/1000 | Loss: 0.00001160
Iteration 225/1000 | Loss: 0.00001160
Iteration 226/1000 | Loss: 0.00001160
Iteration 227/1000 | Loss: 0.00001160
Iteration 228/1000 | Loss: 0.00001160
Iteration 229/1000 | Loss: 0.00001160
Iteration 230/1000 | Loss: 0.00001160
Iteration 231/1000 | Loss: 0.00001160
Iteration 232/1000 | Loss: 0.00001160
Iteration 233/1000 | Loss: 0.00001160
Iteration 234/1000 | Loss: 0.00001160
Iteration 235/1000 | Loss: 0.00001160
Iteration 236/1000 | Loss: 0.00001160
Iteration 237/1000 | Loss: 0.00001160
Iteration 238/1000 | Loss: 0.00001160
Iteration 239/1000 | Loss: 0.00001160
Iteration 240/1000 | Loss: 0.00001160
Iteration 241/1000 | Loss: 0.00001160
Iteration 242/1000 | Loss: 0.00001160
Iteration 243/1000 | Loss: 0.00001160
Iteration 244/1000 | Loss: 0.00001160
Iteration 245/1000 | Loss: 0.00001160
Iteration 246/1000 | Loss: 0.00001160
Iteration 247/1000 | Loss: 0.00001160
Iteration 248/1000 | Loss: 0.00001160
Iteration 249/1000 | Loss: 0.00001160
Iteration 250/1000 | Loss: 0.00001160
Iteration 251/1000 | Loss: 0.00001160
Iteration 252/1000 | Loss: 0.00001160
Iteration 253/1000 | Loss: 0.00001160
Iteration 254/1000 | Loss: 0.00001160
Iteration 255/1000 | Loss: 0.00001160
Iteration 256/1000 | Loss: 0.00001160
Iteration 257/1000 | Loss: 0.00001160
Iteration 258/1000 | Loss: 0.00001160
Iteration 259/1000 | Loss: 0.00001160
Iteration 260/1000 | Loss: 0.00001160
Iteration 261/1000 | Loss: 0.00001160
Iteration 262/1000 | Loss: 0.00001160
Iteration 263/1000 | Loss: 0.00001160
Iteration 264/1000 | Loss: 0.00001160
Iteration 265/1000 | Loss: 0.00001160
Iteration 266/1000 | Loss: 0.00001160
Iteration 267/1000 | Loss: 0.00001160
Iteration 268/1000 | Loss: 0.00001160
Iteration 269/1000 | Loss: 0.00001160
Iteration 270/1000 | Loss: 0.00001160
Iteration 271/1000 | Loss: 0.00001160
Iteration 272/1000 | Loss: 0.00001160
Iteration 273/1000 | Loss: 0.00001160
Iteration 274/1000 | Loss: 0.00001160
Iteration 275/1000 | Loss: 0.00001160
Iteration 276/1000 | Loss: 0.00001160
Iteration 277/1000 | Loss: 0.00001160
Iteration 278/1000 | Loss: 0.00001160
Iteration 279/1000 | Loss: 0.00001160
Iteration 280/1000 | Loss: 0.00001160
Iteration 281/1000 | Loss: 0.00001160
Iteration 282/1000 | Loss: 0.00001160
Iteration 283/1000 | Loss: 0.00001160
Iteration 284/1000 | Loss: 0.00001160
Iteration 285/1000 | Loss: 0.00001160
Iteration 286/1000 | Loss: 0.00001160
Iteration 287/1000 | Loss: 0.00001160
Iteration 288/1000 | Loss: 0.00001160
Iteration 289/1000 | Loss: 0.00001160
Iteration 290/1000 | Loss: 0.00001160
Iteration 291/1000 | Loss: 0.00001160
Iteration 292/1000 | Loss: 0.00001160
Iteration 293/1000 | Loss: 0.00001160
Iteration 294/1000 | Loss: 0.00001160
Iteration 295/1000 | Loss: 0.00001160
Iteration 296/1000 | Loss: 0.00001160
Iteration 297/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [1.1597057891776785e-05, 1.1597057891776785e-05, 1.1597057891776785e-05, 1.1597057891776785e-05, 1.1597057891776785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1597057891776785e-05

Optimization complete. Final v2v error: 2.926151990890503 mm

Highest mean error: 3.3982186317443848 mm for frame 145

Lowest mean error: 2.7493834495544434 mm for frame 26

Saving results

Total time: 73.90933585166931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815966
Iteration 2/25 | Loss: 0.00153663
Iteration 3/25 | Loss: 0.00129639
Iteration 4/25 | Loss: 0.00128795
Iteration 5/25 | Loss: 0.00128558
Iteration 6/25 | Loss: 0.00128558
Iteration 7/25 | Loss: 0.00128558
Iteration 8/25 | Loss: 0.00128558
Iteration 9/25 | Loss: 0.00128558
Iteration 10/25 | Loss: 0.00128558
Iteration 11/25 | Loss: 0.00128558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00128558452706784, 0.00128558452706784, 0.00128558452706784, 0.00128558452706784, 0.00128558452706784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00128558452706784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40937161
Iteration 2/25 | Loss: 0.00078817
Iteration 3/25 | Loss: 0.00078817
Iteration 4/25 | Loss: 0.00078817
Iteration 5/25 | Loss: 0.00078816
Iteration 6/25 | Loss: 0.00078816
Iteration 7/25 | Loss: 0.00078816
Iteration 8/25 | Loss: 0.00078816
Iteration 9/25 | Loss: 0.00078816
Iteration 10/25 | Loss: 0.00078816
Iteration 11/25 | Loss: 0.00078816
Iteration 12/25 | Loss: 0.00078816
Iteration 13/25 | Loss: 0.00078816
Iteration 14/25 | Loss: 0.00078816
Iteration 15/25 | Loss: 0.00078816
Iteration 16/25 | Loss: 0.00078816
Iteration 17/25 | Loss: 0.00078816
Iteration 18/25 | Loss: 0.00078816
Iteration 19/25 | Loss: 0.00078816
Iteration 20/25 | Loss: 0.00078816
Iteration 21/25 | Loss: 0.00078816
Iteration 22/25 | Loss: 0.00078816
Iteration 23/25 | Loss: 0.00078816
Iteration 24/25 | Loss: 0.00078816
Iteration 25/25 | Loss: 0.00078816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078816
Iteration 2/1000 | Loss: 0.00003166
Iteration 3/1000 | Loss: 0.00002257
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001935
Iteration 6/1000 | Loss: 0.00001855
Iteration 7/1000 | Loss: 0.00001787
Iteration 8/1000 | Loss: 0.00001738
Iteration 9/1000 | Loss: 0.00001698
Iteration 10/1000 | Loss: 0.00001683
Iteration 11/1000 | Loss: 0.00001666
Iteration 12/1000 | Loss: 0.00001650
Iteration 13/1000 | Loss: 0.00001649
Iteration 14/1000 | Loss: 0.00001648
Iteration 15/1000 | Loss: 0.00001646
Iteration 16/1000 | Loss: 0.00001634
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001631
Iteration 19/1000 | Loss: 0.00001630
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001627
Iteration 24/1000 | Loss: 0.00001627
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001626
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001625
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001612
Iteration 36/1000 | Loss: 0.00001612
Iteration 37/1000 | Loss: 0.00001611
Iteration 38/1000 | Loss: 0.00001611
Iteration 39/1000 | Loss: 0.00001611
Iteration 40/1000 | Loss: 0.00001611
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001611
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001611
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001610
Iteration 50/1000 | Loss: 0.00001610
Iteration 51/1000 | Loss: 0.00001609
Iteration 52/1000 | Loss: 0.00001609
Iteration 53/1000 | Loss: 0.00001609
Iteration 54/1000 | Loss: 0.00001608
Iteration 55/1000 | Loss: 0.00001608
Iteration 56/1000 | Loss: 0.00001608
Iteration 57/1000 | Loss: 0.00001608
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001607
Iteration 61/1000 | Loss: 0.00001607
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001606
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001606
Iteration 67/1000 | Loss: 0.00001606
Iteration 68/1000 | Loss: 0.00001606
Iteration 69/1000 | Loss: 0.00001606
Iteration 70/1000 | Loss: 0.00001606
Iteration 71/1000 | Loss: 0.00001606
Iteration 72/1000 | Loss: 0.00001606
Iteration 73/1000 | Loss: 0.00001606
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001606
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001606
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001606
Iteration 84/1000 | Loss: 0.00001606
Iteration 85/1000 | Loss: 0.00001606
Iteration 86/1000 | Loss: 0.00001606
Iteration 87/1000 | Loss: 0.00001606
Iteration 88/1000 | Loss: 0.00001606
Iteration 89/1000 | Loss: 0.00001606
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001606
Iteration 93/1000 | Loss: 0.00001606
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001606
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.6055619198596105e-05, 1.6055619198596105e-05, 1.6055619198596105e-05, 1.6055619198596105e-05, 1.6055619198596105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6055619198596105e-05

Optimization complete. Final v2v error: 3.38419771194458 mm

Highest mean error: 3.62741756439209 mm for frame 54

Lowest mean error: 3.1794593334198 mm for frame 110

Saving results

Total time: 33.30388617515564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534134
Iteration 2/25 | Loss: 0.00149627
Iteration 3/25 | Loss: 0.00135739
Iteration 4/25 | Loss: 0.00134251
Iteration 5/25 | Loss: 0.00133916
Iteration 6/25 | Loss: 0.00133916
Iteration 7/25 | Loss: 0.00133916
Iteration 8/25 | Loss: 0.00133916
Iteration 9/25 | Loss: 0.00133916
Iteration 10/25 | Loss: 0.00133916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013391552492976189, 0.0013391552492976189, 0.0013391552492976189, 0.0013391552492976189, 0.0013391552492976189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013391552492976189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80942088
Iteration 2/25 | Loss: 0.00087110
Iteration 3/25 | Loss: 0.00087109
Iteration 4/25 | Loss: 0.00087109
Iteration 5/25 | Loss: 0.00087109
Iteration 6/25 | Loss: 0.00087109
Iteration 7/25 | Loss: 0.00087109
Iteration 8/25 | Loss: 0.00087109
Iteration 9/25 | Loss: 0.00087109
Iteration 10/25 | Loss: 0.00087109
Iteration 11/25 | Loss: 0.00087109
Iteration 12/25 | Loss: 0.00087109
Iteration 13/25 | Loss: 0.00087109
Iteration 14/25 | Loss: 0.00087109
Iteration 15/25 | Loss: 0.00087109
Iteration 16/25 | Loss: 0.00087109
Iteration 17/25 | Loss: 0.00087109
Iteration 18/25 | Loss: 0.00087109
Iteration 19/25 | Loss: 0.00087109
Iteration 20/25 | Loss: 0.00087109
Iteration 21/25 | Loss: 0.00087109
Iteration 22/25 | Loss: 0.00087109
Iteration 23/25 | Loss: 0.00087109
Iteration 24/25 | Loss: 0.00087109
Iteration 25/25 | Loss: 0.00087109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087109
Iteration 2/1000 | Loss: 0.00004313
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00002107
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001884
Iteration 7/1000 | Loss: 0.00001822
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001746
Iteration 10/1000 | Loss: 0.00001711
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001662
Iteration 13/1000 | Loss: 0.00001639
Iteration 14/1000 | Loss: 0.00001624
Iteration 15/1000 | Loss: 0.00001616
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00001593
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001577
Iteration 20/1000 | Loss: 0.00001575
Iteration 21/1000 | Loss: 0.00001570
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001555
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001543
Iteration 26/1000 | Loss: 0.00001542
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001538
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001535
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001534
Iteration 40/1000 | Loss: 0.00001534
Iteration 41/1000 | Loss: 0.00001534
Iteration 42/1000 | Loss: 0.00001534
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001534
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001533
Iteration 49/1000 | Loss: 0.00001533
Iteration 50/1000 | Loss: 0.00001533
Iteration 51/1000 | Loss: 0.00001532
Iteration 52/1000 | Loss: 0.00001532
Iteration 53/1000 | Loss: 0.00001532
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001530
Iteration 56/1000 | Loss: 0.00001530
Iteration 57/1000 | Loss: 0.00001530
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001526
Iteration 75/1000 | Loss: 0.00001526
Iteration 76/1000 | Loss: 0.00001526
Iteration 77/1000 | Loss: 0.00001526
Iteration 78/1000 | Loss: 0.00001526
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001523
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001522
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001521
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001520
Iteration 101/1000 | Loss: 0.00001520
Iteration 102/1000 | Loss: 0.00001520
Iteration 103/1000 | Loss: 0.00001520
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001520
Iteration 106/1000 | Loss: 0.00001520
Iteration 107/1000 | Loss: 0.00001519
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001518
Iteration 112/1000 | Loss: 0.00001518
Iteration 113/1000 | Loss: 0.00001518
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001517
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001517
Iteration 127/1000 | Loss: 0.00001517
Iteration 128/1000 | Loss: 0.00001517
Iteration 129/1000 | Loss: 0.00001517
Iteration 130/1000 | Loss: 0.00001517
Iteration 131/1000 | Loss: 0.00001516
Iteration 132/1000 | Loss: 0.00001516
Iteration 133/1000 | Loss: 0.00001516
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001514
Iteration 138/1000 | Loss: 0.00001514
Iteration 139/1000 | Loss: 0.00001514
Iteration 140/1000 | Loss: 0.00001513
Iteration 141/1000 | Loss: 0.00001513
Iteration 142/1000 | Loss: 0.00001513
Iteration 143/1000 | Loss: 0.00001513
Iteration 144/1000 | Loss: 0.00001513
Iteration 145/1000 | Loss: 0.00001512
Iteration 146/1000 | Loss: 0.00001512
Iteration 147/1000 | Loss: 0.00001512
Iteration 148/1000 | Loss: 0.00001512
Iteration 149/1000 | Loss: 0.00001512
Iteration 150/1000 | Loss: 0.00001512
Iteration 151/1000 | Loss: 0.00001512
Iteration 152/1000 | Loss: 0.00001511
Iteration 153/1000 | Loss: 0.00001511
Iteration 154/1000 | Loss: 0.00001511
Iteration 155/1000 | Loss: 0.00001510
Iteration 156/1000 | Loss: 0.00001510
Iteration 157/1000 | Loss: 0.00001510
Iteration 158/1000 | Loss: 0.00001510
Iteration 159/1000 | Loss: 0.00001510
Iteration 160/1000 | Loss: 0.00001509
Iteration 161/1000 | Loss: 0.00001509
Iteration 162/1000 | Loss: 0.00001509
Iteration 163/1000 | Loss: 0.00001509
Iteration 164/1000 | Loss: 0.00001509
Iteration 165/1000 | Loss: 0.00001509
Iteration 166/1000 | Loss: 0.00001508
Iteration 167/1000 | Loss: 0.00001508
Iteration 168/1000 | Loss: 0.00001508
Iteration 169/1000 | Loss: 0.00001508
Iteration 170/1000 | Loss: 0.00001508
Iteration 171/1000 | Loss: 0.00001508
Iteration 172/1000 | Loss: 0.00001508
Iteration 173/1000 | Loss: 0.00001508
Iteration 174/1000 | Loss: 0.00001508
Iteration 175/1000 | Loss: 0.00001508
Iteration 176/1000 | Loss: 0.00001508
Iteration 177/1000 | Loss: 0.00001508
Iteration 178/1000 | Loss: 0.00001508
Iteration 179/1000 | Loss: 0.00001508
Iteration 180/1000 | Loss: 0.00001508
Iteration 181/1000 | Loss: 0.00001508
Iteration 182/1000 | Loss: 0.00001508
Iteration 183/1000 | Loss: 0.00001508
Iteration 184/1000 | Loss: 0.00001508
Iteration 185/1000 | Loss: 0.00001508
Iteration 186/1000 | Loss: 0.00001508
Iteration 187/1000 | Loss: 0.00001508
Iteration 188/1000 | Loss: 0.00001508
Iteration 189/1000 | Loss: 0.00001508
Iteration 190/1000 | Loss: 0.00001508
Iteration 191/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.5079756849445403e-05, 1.5079756849445403e-05, 1.5079756849445403e-05, 1.5079756849445403e-05, 1.5079756849445403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5079756849445403e-05

Optimization complete. Final v2v error: 3.3081154823303223 mm

Highest mean error: 3.849285125732422 mm for frame 239

Lowest mean error: 3.2523298263549805 mm for frame 136

Saving results

Total time: 53.18029236793518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806355
Iteration 2/25 | Loss: 0.00152949
Iteration 3/25 | Loss: 0.00130434
Iteration 4/25 | Loss: 0.00128132
Iteration 5/25 | Loss: 0.00127751
Iteration 6/25 | Loss: 0.00127573
Iteration 7/25 | Loss: 0.00127473
Iteration 8/25 | Loss: 0.00128052
Iteration 9/25 | Loss: 0.00127773
Iteration 10/25 | Loss: 0.00127752
Iteration 11/25 | Loss: 0.00127966
Iteration 12/25 | Loss: 0.00127879
Iteration 13/25 | Loss: 0.00128032
Iteration 14/25 | Loss: 0.00127431
Iteration 15/25 | Loss: 0.00127076
Iteration 16/25 | Loss: 0.00126784
Iteration 17/25 | Loss: 0.00126763
Iteration 18/25 | Loss: 0.00126627
Iteration 19/25 | Loss: 0.00126671
Iteration 20/25 | Loss: 0.00126635
Iteration 21/25 | Loss: 0.00126691
Iteration 22/25 | Loss: 0.00126695
Iteration 23/25 | Loss: 0.00126641
Iteration 24/25 | Loss: 0.00126668
Iteration 25/25 | Loss: 0.00126661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42947769
Iteration 2/25 | Loss: 0.00086557
Iteration 3/25 | Loss: 0.00086557
Iteration 4/25 | Loss: 0.00086557
Iteration 5/25 | Loss: 0.00086556
Iteration 6/25 | Loss: 0.00086556
Iteration 7/25 | Loss: 0.00086556
Iteration 8/25 | Loss: 0.00086556
Iteration 9/25 | Loss: 0.00086556
Iteration 10/25 | Loss: 0.00086556
Iteration 11/25 | Loss: 0.00086556
Iteration 12/25 | Loss: 0.00086556
Iteration 13/25 | Loss: 0.00086556
Iteration 14/25 | Loss: 0.00086556
Iteration 15/25 | Loss: 0.00086556
Iteration 16/25 | Loss: 0.00086556
Iteration 17/25 | Loss: 0.00086556
Iteration 18/25 | Loss: 0.00086556
Iteration 19/25 | Loss: 0.00086556
Iteration 20/25 | Loss: 0.00086556
Iteration 21/25 | Loss: 0.00086556
Iteration 22/25 | Loss: 0.00086556
Iteration 23/25 | Loss: 0.00086556
Iteration 24/25 | Loss: 0.00086556
Iteration 25/25 | Loss: 0.00086556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086556
Iteration 2/1000 | Loss: 0.00004262
Iteration 3/1000 | Loss: 0.00006076
Iteration 4/1000 | Loss: 0.00006237
Iteration 5/1000 | Loss: 0.00004365
Iteration 6/1000 | Loss: 0.00008139
Iteration 7/1000 | Loss: 0.00004687
Iteration 8/1000 | Loss: 0.00006729
Iteration 9/1000 | Loss: 0.00006807
Iteration 10/1000 | Loss: 0.00006784
Iteration 11/1000 | Loss: 0.00006978
Iteration 12/1000 | Loss: 0.00006176
Iteration 13/1000 | Loss: 0.00006838
Iteration 14/1000 | Loss: 0.00006674
Iteration 15/1000 | Loss: 0.00007428
Iteration 16/1000 | Loss: 0.00006465
Iteration 17/1000 | Loss: 0.00007974
Iteration 18/1000 | Loss: 0.00005637
Iteration 19/1000 | Loss: 0.00004498
Iteration 20/1000 | Loss: 0.00004987
Iteration 21/1000 | Loss: 0.00005436
Iteration 22/1000 | Loss: 0.00008687
Iteration 23/1000 | Loss: 0.00007073
Iteration 24/1000 | Loss: 0.00005488
Iteration 25/1000 | Loss: 0.00003528
Iteration 26/1000 | Loss: 0.00005665
Iteration 27/1000 | Loss: 0.00005656
Iteration 28/1000 | Loss: 0.00006030
Iteration 29/1000 | Loss: 0.00007741
Iteration 30/1000 | Loss: 0.00006718
Iteration 31/1000 | Loss: 0.00007389
Iteration 32/1000 | Loss: 0.00005294
Iteration 33/1000 | Loss: 0.00004548
Iteration 34/1000 | Loss: 0.00006330
Iteration 35/1000 | Loss: 0.00007949
Iteration 36/1000 | Loss: 0.00006365
Iteration 37/1000 | Loss: 0.00006351
Iteration 38/1000 | Loss: 0.00002509
Iteration 39/1000 | Loss: 0.00002717
Iteration 40/1000 | Loss: 0.00004369
Iteration 41/1000 | Loss: 0.00007516
Iteration 42/1000 | Loss: 0.00005240
Iteration 43/1000 | Loss: 0.00006584
Iteration 44/1000 | Loss: 0.00005507
Iteration 45/1000 | Loss: 0.00005513
Iteration 46/1000 | Loss: 0.00005467
Iteration 47/1000 | Loss: 0.00005679
Iteration 48/1000 | Loss: 0.00005546
Iteration 49/1000 | Loss: 0.00006074
Iteration 50/1000 | Loss: 0.00005381
Iteration 51/1000 | Loss: 0.00005602
Iteration 52/1000 | Loss: 0.00004995
Iteration 53/1000 | Loss: 0.00006908
Iteration 54/1000 | Loss: 0.00005040
Iteration 55/1000 | Loss: 0.00006811
Iteration 56/1000 | Loss: 0.00005244
Iteration 57/1000 | Loss: 0.00005876
Iteration 58/1000 | Loss: 0.00004983
Iteration 59/1000 | Loss: 0.00008391
Iteration 60/1000 | Loss: 0.00006021
Iteration 61/1000 | Loss: 0.00002786
Iteration 62/1000 | Loss: 0.00001995
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001650
Iteration 65/1000 | Loss: 0.00001591
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001452
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001315
Iteration 74/1000 | Loss: 0.00001313
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001302
Iteration 80/1000 | Loss: 0.00001302
Iteration 81/1000 | Loss: 0.00001301
Iteration 82/1000 | Loss: 0.00001301
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001301
Iteration 86/1000 | Loss: 0.00001300
Iteration 87/1000 | Loss: 0.00001300
Iteration 88/1000 | Loss: 0.00001300
Iteration 89/1000 | Loss: 0.00001300
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001295
Iteration 104/1000 | Loss: 0.00001295
Iteration 105/1000 | Loss: 0.00001295
Iteration 106/1000 | Loss: 0.00001295
Iteration 107/1000 | Loss: 0.00001295
Iteration 108/1000 | Loss: 0.00001295
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001294
Iteration 112/1000 | Loss: 0.00001294
Iteration 113/1000 | Loss: 0.00001294
Iteration 114/1000 | Loss: 0.00001294
Iteration 115/1000 | Loss: 0.00001293
Iteration 116/1000 | Loss: 0.00001293
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001292
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001289
Iteration 122/1000 | Loss: 0.00001288
Iteration 123/1000 | Loss: 0.00001285
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001284
Iteration 126/1000 | Loss: 0.00001281
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001275
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001273
Iteration 131/1000 | Loss: 0.00001273
Iteration 132/1000 | Loss: 0.00001273
Iteration 133/1000 | Loss: 0.00001272
Iteration 134/1000 | Loss: 0.00001272
Iteration 135/1000 | Loss: 0.00001272
Iteration 136/1000 | Loss: 0.00001271
Iteration 137/1000 | Loss: 0.00001271
Iteration 138/1000 | Loss: 0.00001271
Iteration 139/1000 | Loss: 0.00001270
Iteration 140/1000 | Loss: 0.00001270
Iteration 141/1000 | Loss: 0.00001269
Iteration 142/1000 | Loss: 0.00001268
Iteration 143/1000 | Loss: 0.00001267
Iteration 144/1000 | Loss: 0.00001267
Iteration 145/1000 | Loss: 0.00001267
Iteration 146/1000 | Loss: 0.00001266
Iteration 147/1000 | Loss: 0.00001266
Iteration 148/1000 | Loss: 0.00001265
Iteration 149/1000 | Loss: 0.00001263
Iteration 150/1000 | Loss: 0.00001263
Iteration 151/1000 | Loss: 0.00001262
Iteration 152/1000 | Loss: 0.00001262
Iteration 153/1000 | Loss: 0.00001262
Iteration 154/1000 | Loss: 0.00001262
Iteration 155/1000 | Loss: 0.00001261
Iteration 156/1000 | Loss: 0.00001261
Iteration 157/1000 | Loss: 0.00001260
Iteration 158/1000 | Loss: 0.00001260
Iteration 159/1000 | Loss: 0.00001260
Iteration 160/1000 | Loss: 0.00001259
Iteration 161/1000 | Loss: 0.00001259
Iteration 162/1000 | Loss: 0.00001259
Iteration 163/1000 | Loss: 0.00001259
Iteration 164/1000 | Loss: 0.00001259
Iteration 165/1000 | Loss: 0.00001258
Iteration 166/1000 | Loss: 0.00001258
Iteration 167/1000 | Loss: 0.00001258
Iteration 168/1000 | Loss: 0.00001258
Iteration 169/1000 | Loss: 0.00001257
Iteration 170/1000 | Loss: 0.00001257
Iteration 171/1000 | Loss: 0.00001257
Iteration 172/1000 | Loss: 0.00001257
Iteration 173/1000 | Loss: 0.00001256
Iteration 174/1000 | Loss: 0.00001256
Iteration 175/1000 | Loss: 0.00001256
Iteration 176/1000 | Loss: 0.00001256
Iteration 177/1000 | Loss: 0.00001256
Iteration 178/1000 | Loss: 0.00001256
Iteration 179/1000 | Loss: 0.00001256
Iteration 180/1000 | Loss: 0.00001256
Iteration 181/1000 | Loss: 0.00001255
Iteration 182/1000 | Loss: 0.00001255
Iteration 183/1000 | Loss: 0.00001255
Iteration 184/1000 | Loss: 0.00001255
Iteration 185/1000 | Loss: 0.00001255
Iteration 186/1000 | Loss: 0.00001254
Iteration 187/1000 | Loss: 0.00001254
Iteration 188/1000 | Loss: 0.00001254
Iteration 189/1000 | Loss: 0.00001254
Iteration 190/1000 | Loss: 0.00001254
Iteration 191/1000 | Loss: 0.00001254
Iteration 192/1000 | Loss: 0.00001254
Iteration 193/1000 | Loss: 0.00001254
Iteration 194/1000 | Loss: 0.00001254
Iteration 195/1000 | Loss: 0.00001253
Iteration 196/1000 | Loss: 0.00001253
Iteration 197/1000 | Loss: 0.00001253
Iteration 198/1000 | Loss: 0.00001253
Iteration 199/1000 | Loss: 0.00001253
Iteration 200/1000 | Loss: 0.00001253
Iteration 201/1000 | Loss: 0.00001253
Iteration 202/1000 | Loss: 0.00001253
Iteration 203/1000 | Loss: 0.00001253
Iteration 204/1000 | Loss: 0.00001253
Iteration 205/1000 | Loss: 0.00001253
Iteration 206/1000 | Loss: 0.00001253
Iteration 207/1000 | Loss: 0.00001252
Iteration 208/1000 | Loss: 0.00001252
Iteration 209/1000 | Loss: 0.00001252
Iteration 210/1000 | Loss: 0.00001252
Iteration 211/1000 | Loss: 0.00001252
Iteration 212/1000 | Loss: 0.00001252
Iteration 213/1000 | Loss: 0.00001252
Iteration 214/1000 | Loss: 0.00001252
Iteration 215/1000 | Loss: 0.00001252
Iteration 216/1000 | Loss: 0.00001252
Iteration 217/1000 | Loss: 0.00001252
Iteration 218/1000 | Loss: 0.00001251
Iteration 219/1000 | Loss: 0.00001251
Iteration 220/1000 | Loss: 0.00001251
Iteration 221/1000 | Loss: 0.00001251
Iteration 222/1000 | Loss: 0.00001251
Iteration 223/1000 | Loss: 0.00001251
Iteration 224/1000 | Loss: 0.00001251
Iteration 225/1000 | Loss: 0.00001251
Iteration 226/1000 | Loss: 0.00001251
Iteration 227/1000 | Loss: 0.00001251
Iteration 228/1000 | Loss: 0.00001251
Iteration 229/1000 | Loss: 0.00001251
Iteration 230/1000 | Loss: 0.00001251
Iteration 231/1000 | Loss: 0.00001251
Iteration 232/1000 | Loss: 0.00001251
Iteration 233/1000 | Loss: 0.00001251
Iteration 234/1000 | Loss: 0.00001251
Iteration 235/1000 | Loss: 0.00001251
Iteration 236/1000 | Loss: 0.00001251
Iteration 237/1000 | Loss: 0.00001251
Iteration 238/1000 | Loss: 0.00001251
Iteration 239/1000 | Loss: 0.00001251
Iteration 240/1000 | Loss: 0.00001251
Iteration 241/1000 | Loss: 0.00001251
Iteration 242/1000 | Loss: 0.00001251
Iteration 243/1000 | Loss: 0.00001251
Iteration 244/1000 | Loss: 0.00001250
Iteration 245/1000 | Loss: 0.00001250
Iteration 246/1000 | Loss: 0.00001250
Iteration 247/1000 | Loss: 0.00001250
Iteration 248/1000 | Loss: 0.00001250
Iteration 249/1000 | Loss: 0.00001250
Iteration 250/1000 | Loss: 0.00001250
Iteration 251/1000 | Loss: 0.00001250
Iteration 252/1000 | Loss: 0.00001250
Iteration 253/1000 | Loss: 0.00001250
Iteration 254/1000 | Loss: 0.00001250
Iteration 255/1000 | Loss: 0.00001250
Iteration 256/1000 | Loss: 0.00001250
Iteration 257/1000 | Loss: 0.00001250
Iteration 258/1000 | Loss: 0.00001249
Iteration 259/1000 | Loss: 0.00001249
Iteration 260/1000 | Loss: 0.00001249
Iteration 261/1000 | Loss: 0.00001249
Iteration 262/1000 | Loss: 0.00001249
Iteration 263/1000 | Loss: 0.00001249
Iteration 264/1000 | Loss: 0.00001249
Iteration 265/1000 | Loss: 0.00001249
Iteration 266/1000 | Loss: 0.00001249
Iteration 267/1000 | Loss: 0.00001249
Iteration 268/1000 | Loss: 0.00001249
Iteration 269/1000 | Loss: 0.00001249
Iteration 270/1000 | Loss: 0.00001249
Iteration 271/1000 | Loss: 0.00001249
Iteration 272/1000 | Loss: 0.00001248
Iteration 273/1000 | Loss: 0.00001248
Iteration 274/1000 | Loss: 0.00001248
Iteration 275/1000 | Loss: 0.00001248
Iteration 276/1000 | Loss: 0.00001248
Iteration 277/1000 | Loss: 0.00001247
Iteration 278/1000 | Loss: 0.00001247
Iteration 279/1000 | Loss: 0.00001247
Iteration 280/1000 | Loss: 0.00001247
Iteration 281/1000 | Loss: 0.00001247
Iteration 282/1000 | Loss: 0.00001247
Iteration 283/1000 | Loss: 0.00001247
Iteration 284/1000 | Loss: 0.00001247
Iteration 285/1000 | Loss: 0.00001247
Iteration 286/1000 | Loss: 0.00001247
Iteration 287/1000 | Loss: 0.00001246
Iteration 288/1000 | Loss: 0.00001246
Iteration 289/1000 | Loss: 0.00001246
Iteration 290/1000 | Loss: 0.00001246
Iteration 291/1000 | Loss: 0.00001246
Iteration 292/1000 | Loss: 0.00001246
Iteration 293/1000 | Loss: 0.00001246
Iteration 294/1000 | Loss: 0.00001246
Iteration 295/1000 | Loss: 0.00001246
Iteration 296/1000 | Loss: 0.00001246
Iteration 297/1000 | Loss: 0.00001246
Iteration 298/1000 | Loss: 0.00001246
Iteration 299/1000 | Loss: 0.00001245
Iteration 300/1000 | Loss: 0.00001245
Iteration 301/1000 | Loss: 0.00001245
Iteration 302/1000 | Loss: 0.00001245
Iteration 303/1000 | Loss: 0.00001245
Iteration 304/1000 | Loss: 0.00001245
Iteration 305/1000 | Loss: 0.00001245
Iteration 306/1000 | Loss: 0.00001245
Iteration 307/1000 | Loss: 0.00001245
Iteration 308/1000 | Loss: 0.00001245
Iteration 309/1000 | Loss: 0.00001245
Iteration 310/1000 | Loss: 0.00001245
Iteration 311/1000 | Loss: 0.00001245
Iteration 312/1000 | Loss: 0.00001245
Iteration 313/1000 | Loss: 0.00001245
Iteration 314/1000 | Loss: 0.00001245
Iteration 315/1000 | Loss: 0.00001245
Iteration 316/1000 | Loss: 0.00001245
Iteration 317/1000 | Loss: 0.00001245
Iteration 318/1000 | Loss: 0.00001245
Iteration 319/1000 | Loss: 0.00001245
Iteration 320/1000 | Loss: 0.00001245
Iteration 321/1000 | Loss: 0.00001245
Iteration 322/1000 | Loss: 0.00001245
Iteration 323/1000 | Loss: 0.00001245
Iteration 324/1000 | Loss: 0.00001245
Iteration 325/1000 | Loss: 0.00001245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [1.244784380105557e-05, 1.244784380105557e-05, 1.244784380105557e-05, 1.244784380105557e-05, 1.244784380105557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.244784380105557e-05

Optimization complete. Final v2v error: 3.01888370513916 mm

Highest mean error: 3.811319351196289 mm for frame 66

Lowest mean error: 2.8438327312469482 mm for frame 186

Saving results

Total time: 169.74532771110535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027979
Iteration 2/25 | Loss: 0.00243335
Iteration 3/25 | Loss: 0.00246377
Iteration 4/25 | Loss: 0.00222748
Iteration 5/25 | Loss: 0.00204103
Iteration 6/25 | Loss: 0.00168864
Iteration 7/25 | Loss: 0.00165668
Iteration 8/25 | Loss: 0.00157433
Iteration 9/25 | Loss: 0.00149677
Iteration 10/25 | Loss: 0.00148126
Iteration 11/25 | Loss: 0.00148651
Iteration 12/25 | Loss: 0.00147137
Iteration 13/25 | Loss: 0.00145324
Iteration 14/25 | Loss: 0.00142514
Iteration 15/25 | Loss: 0.00142609
Iteration 16/25 | Loss: 0.00142602
Iteration 17/25 | Loss: 0.00142086
Iteration 18/25 | Loss: 0.00143492
Iteration 19/25 | Loss: 0.00140586
Iteration 20/25 | Loss: 0.00140586
Iteration 21/25 | Loss: 0.00139769
Iteration 22/25 | Loss: 0.00140270
Iteration 23/25 | Loss: 0.00139845
Iteration 24/25 | Loss: 0.00138928
Iteration 25/25 | Loss: 0.00138787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37074018
Iteration 2/25 | Loss: 0.00194997
Iteration 3/25 | Loss: 0.00109145
Iteration 4/25 | Loss: 0.00109144
Iteration 5/25 | Loss: 0.00109144
Iteration 6/25 | Loss: 0.00109144
Iteration 7/25 | Loss: 0.00109144
Iteration 8/25 | Loss: 0.00109144
Iteration 9/25 | Loss: 0.00109144
Iteration 10/25 | Loss: 0.00109144
Iteration 11/25 | Loss: 0.00109144
Iteration 12/25 | Loss: 0.00109144
Iteration 13/25 | Loss: 0.00109144
Iteration 14/25 | Loss: 0.00109144
Iteration 15/25 | Loss: 0.00109144
Iteration 16/25 | Loss: 0.00109144
Iteration 17/25 | Loss: 0.00109144
Iteration 18/25 | Loss: 0.00109144
Iteration 19/25 | Loss: 0.00109144
Iteration 20/25 | Loss: 0.00109144
Iteration 21/25 | Loss: 0.00109144
Iteration 22/25 | Loss: 0.00109144
Iteration 23/25 | Loss: 0.00109144
Iteration 24/25 | Loss: 0.00109144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010914411395788193, 0.0010914411395788193, 0.0010914411395788193, 0.0010914411395788193, 0.0010914411395788193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010914411395788193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109144
Iteration 2/1000 | Loss: 0.00028237
Iteration 3/1000 | Loss: 0.00036804
Iteration 4/1000 | Loss: 0.00044954
Iteration 5/1000 | Loss: 0.00036415
Iteration 6/1000 | Loss: 0.00009345
Iteration 7/1000 | Loss: 0.00030960
Iteration 8/1000 | Loss: 0.00004788
Iteration 9/1000 | Loss: 0.00053791
Iteration 10/1000 | Loss: 0.00006694
Iteration 11/1000 | Loss: 0.00005782
Iteration 12/1000 | Loss: 0.00030807
Iteration 13/1000 | Loss: 0.00003953
Iteration 14/1000 | Loss: 0.00003750
Iteration 15/1000 | Loss: 0.00025382
Iteration 16/1000 | Loss: 0.00009647
Iteration 17/1000 | Loss: 0.00016943
Iteration 18/1000 | Loss: 0.00006387
Iteration 19/1000 | Loss: 0.00021743
Iteration 20/1000 | Loss: 0.00003857
Iteration 21/1000 | Loss: 0.00003596
Iteration 22/1000 | Loss: 0.00023291
Iteration 23/1000 | Loss: 0.00007238
Iteration 24/1000 | Loss: 0.00012682
Iteration 25/1000 | Loss: 0.00003356
Iteration 26/1000 | Loss: 0.00013673
Iteration 27/1000 | Loss: 0.00027015
Iteration 28/1000 | Loss: 0.00004328
Iteration 29/1000 | Loss: 0.00007871
Iteration 30/1000 | Loss: 0.00011516
Iteration 31/1000 | Loss: 0.00031286
Iteration 32/1000 | Loss: 0.00033594
Iteration 33/1000 | Loss: 0.00007702
Iteration 34/1000 | Loss: 0.00014201
Iteration 35/1000 | Loss: 0.00009336
Iteration 36/1000 | Loss: 0.00026669
Iteration 37/1000 | Loss: 0.00003376
Iteration 38/1000 | Loss: 0.00008456
Iteration 39/1000 | Loss: 0.00003496
Iteration 40/1000 | Loss: 0.00003940
Iteration 41/1000 | Loss: 0.00003291
Iteration 42/1000 | Loss: 0.00007650
Iteration 43/1000 | Loss: 0.00003282
Iteration 44/1000 | Loss: 0.00005774
Iteration 45/1000 | Loss: 0.00003264
Iteration 46/1000 | Loss: 0.00003264
Iteration 47/1000 | Loss: 0.00005773
Iteration 48/1000 | Loss: 0.00019360
Iteration 49/1000 | Loss: 0.00004802
Iteration 50/1000 | Loss: 0.00003914
Iteration 51/1000 | Loss: 0.00003914
Iteration 52/1000 | Loss: 0.00045561
Iteration 53/1000 | Loss: 0.00007359
Iteration 54/1000 | Loss: 0.00051339
Iteration 55/1000 | Loss: 0.00004176
Iteration 56/1000 | Loss: 0.00005390
Iteration 57/1000 | Loss: 0.00003250
Iteration 58/1000 | Loss: 0.00003220
Iteration 59/1000 | Loss: 0.00009893
Iteration 60/1000 | Loss: 0.00007244
Iteration 61/1000 | Loss: 0.00003216
Iteration 62/1000 | Loss: 0.00005605
Iteration 63/1000 | Loss: 0.00005605
Iteration 64/1000 | Loss: 0.00029664
Iteration 65/1000 | Loss: 0.00009105
Iteration 66/1000 | Loss: 0.00005878
Iteration 67/1000 | Loss: 0.00007796
Iteration 68/1000 | Loss: 0.00003747
Iteration 69/1000 | Loss: 0.00003537
Iteration 70/1000 | Loss: 0.00004152
Iteration 71/1000 | Loss: 0.00006844
Iteration 72/1000 | Loss: 0.00015377
Iteration 73/1000 | Loss: 0.00023275
Iteration 74/1000 | Loss: 0.00012885
Iteration 75/1000 | Loss: 0.00005987
Iteration 76/1000 | Loss: 0.00010089
Iteration 77/1000 | Loss: 0.00020041
Iteration 78/1000 | Loss: 0.00010387
Iteration 79/1000 | Loss: 0.00010637
Iteration 80/1000 | Loss: 0.00014501
Iteration 81/1000 | Loss: 0.00005986
Iteration 82/1000 | Loss: 0.00007932
Iteration 83/1000 | Loss: 0.00002870
Iteration 84/1000 | Loss: 0.00008153
Iteration 85/1000 | Loss: 0.00015242
Iteration 86/1000 | Loss: 0.00095658
Iteration 87/1000 | Loss: 0.00004475
Iteration 88/1000 | Loss: 0.00015907
Iteration 89/1000 | Loss: 0.00003125
Iteration 90/1000 | Loss: 0.00004476
Iteration 91/1000 | Loss: 0.00002812
Iteration 92/1000 | Loss: 0.00004869
Iteration 93/1000 | Loss: 0.00002797
Iteration 94/1000 | Loss: 0.00004157
Iteration 95/1000 | Loss: 0.00002785
Iteration 96/1000 | Loss: 0.00005273
Iteration 97/1000 | Loss: 0.00007921
Iteration 98/1000 | Loss: 0.00003814
Iteration 99/1000 | Loss: 0.00002787
Iteration 100/1000 | Loss: 0.00002771
Iteration 101/1000 | Loss: 0.00002770
Iteration 102/1000 | Loss: 0.00002770
Iteration 103/1000 | Loss: 0.00002769
Iteration 104/1000 | Loss: 0.00002769
Iteration 105/1000 | Loss: 0.00002769
Iteration 106/1000 | Loss: 0.00002769
Iteration 107/1000 | Loss: 0.00002769
Iteration 108/1000 | Loss: 0.00002769
Iteration 109/1000 | Loss: 0.00002769
Iteration 110/1000 | Loss: 0.00002769
Iteration 111/1000 | Loss: 0.00002769
Iteration 112/1000 | Loss: 0.00002769
Iteration 113/1000 | Loss: 0.00002768
Iteration 114/1000 | Loss: 0.00002768
Iteration 115/1000 | Loss: 0.00002767
Iteration 116/1000 | Loss: 0.00002767
Iteration 117/1000 | Loss: 0.00002767
Iteration 118/1000 | Loss: 0.00002766
Iteration 119/1000 | Loss: 0.00002765
Iteration 120/1000 | Loss: 0.00002765
Iteration 121/1000 | Loss: 0.00002764
Iteration 122/1000 | Loss: 0.00002764
Iteration 123/1000 | Loss: 0.00002764
Iteration 124/1000 | Loss: 0.00002764
Iteration 125/1000 | Loss: 0.00002763
Iteration 126/1000 | Loss: 0.00002763
Iteration 127/1000 | Loss: 0.00002763
Iteration 128/1000 | Loss: 0.00002763
Iteration 129/1000 | Loss: 0.00002763
Iteration 130/1000 | Loss: 0.00002763
Iteration 131/1000 | Loss: 0.00002763
Iteration 132/1000 | Loss: 0.00002762
Iteration 133/1000 | Loss: 0.00002762
Iteration 134/1000 | Loss: 0.00002762
Iteration 135/1000 | Loss: 0.00002762
Iteration 136/1000 | Loss: 0.00002761
Iteration 137/1000 | Loss: 0.00002761
Iteration 138/1000 | Loss: 0.00002761
Iteration 139/1000 | Loss: 0.00002761
Iteration 140/1000 | Loss: 0.00002761
Iteration 141/1000 | Loss: 0.00002761
Iteration 142/1000 | Loss: 0.00002761
Iteration 143/1000 | Loss: 0.00002761
Iteration 144/1000 | Loss: 0.00002761
Iteration 145/1000 | Loss: 0.00002760
Iteration 146/1000 | Loss: 0.00002760
Iteration 147/1000 | Loss: 0.00002760
Iteration 148/1000 | Loss: 0.00002760
Iteration 149/1000 | Loss: 0.00002759
Iteration 150/1000 | Loss: 0.00002759
Iteration 151/1000 | Loss: 0.00002759
Iteration 152/1000 | Loss: 0.00002759
Iteration 153/1000 | Loss: 0.00005827
Iteration 154/1000 | Loss: 0.00003344
Iteration 155/1000 | Loss: 0.00003822
Iteration 156/1000 | Loss: 0.00005897
Iteration 157/1000 | Loss: 0.00008518
Iteration 158/1000 | Loss: 0.00004084
Iteration 159/1000 | Loss: 0.00003034
Iteration 160/1000 | Loss: 0.00005941
Iteration 161/1000 | Loss: 0.00002765
Iteration 162/1000 | Loss: 0.00004074
Iteration 163/1000 | Loss: 0.00003472
Iteration 164/1000 | Loss: 0.00002758
Iteration 165/1000 | Loss: 0.00002757
Iteration 166/1000 | Loss: 0.00002757
Iteration 167/1000 | Loss: 0.00002757
Iteration 168/1000 | Loss: 0.00002756
Iteration 169/1000 | Loss: 0.00002756
Iteration 170/1000 | Loss: 0.00002756
Iteration 171/1000 | Loss: 0.00002756
Iteration 172/1000 | Loss: 0.00002756
Iteration 173/1000 | Loss: 0.00002756
Iteration 174/1000 | Loss: 0.00002756
Iteration 175/1000 | Loss: 0.00002756
Iteration 176/1000 | Loss: 0.00002756
Iteration 177/1000 | Loss: 0.00002755
Iteration 178/1000 | Loss: 0.00002755
Iteration 179/1000 | Loss: 0.00002755
Iteration 180/1000 | Loss: 0.00002755
Iteration 181/1000 | Loss: 0.00002755
Iteration 182/1000 | Loss: 0.00002754
Iteration 183/1000 | Loss: 0.00002754
Iteration 184/1000 | Loss: 0.00002754
Iteration 185/1000 | Loss: 0.00002754
Iteration 186/1000 | Loss: 0.00002754
Iteration 187/1000 | Loss: 0.00002754
Iteration 188/1000 | Loss: 0.00002754
Iteration 189/1000 | Loss: 0.00002754
Iteration 190/1000 | Loss: 0.00002753
Iteration 191/1000 | Loss: 0.00002753
Iteration 192/1000 | Loss: 0.00006030
Iteration 193/1000 | Loss: 0.00005740
Iteration 194/1000 | Loss: 0.00002760
Iteration 195/1000 | Loss: 0.00002756
Iteration 196/1000 | Loss: 0.00002755
Iteration 197/1000 | Loss: 0.00002755
Iteration 198/1000 | Loss: 0.00002755
Iteration 199/1000 | Loss: 0.00002754
Iteration 200/1000 | Loss: 0.00002754
Iteration 201/1000 | Loss: 0.00002753
Iteration 202/1000 | Loss: 0.00002753
Iteration 203/1000 | Loss: 0.00002753
Iteration 204/1000 | Loss: 0.00002753
Iteration 205/1000 | Loss: 0.00002753
Iteration 206/1000 | Loss: 0.00002752
Iteration 207/1000 | Loss: 0.00002752
Iteration 208/1000 | Loss: 0.00002752
Iteration 209/1000 | Loss: 0.00002752
Iteration 210/1000 | Loss: 0.00002752
Iteration 211/1000 | Loss: 0.00002752
Iteration 212/1000 | Loss: 0.00002752
Iteration 213/1000 | Loss: 0.00002752
Iteration 214/1000 | Loss: 0.00002752
Iteration 215/1000 | Loss: 0.00002752
Iteration 216/1000 | Loss: 0.00002752
Iteration 217/1000 | Loss: 0.00002752
Iteration 218/1000 | Loss: 0.00002752
Iteration 219/1000 | Loss: 0.00002751
Iteration 220/1000 | Loss: 0.00002751
Iteration 221/1000 | Loss: 0.00002751
Iteration 222/1000 | Loss: 0.00002751
Iteration 223/1000 | Loss: 0.00002751
Iteration 224/1000 | Loss: 0.00002751
Iteration 225/1000 | Loss: 0.00002751
Iteration 226/1000 | Loss: 0.00002751
Iteration 227/1000 | Loss: 0.00002751
Iteration 228/1000 | Loss: 0.00002751
Iteration 229/1000 | Loss: 0.00002751
Iteration 230/1000 | Loss: 0.00002751
Iteration 231/1000 | Loss: 0.00002751
Iteration 232/1000 | Loss: 0.00002751
Iteration 233/1000 | Loss: 0.00002751
Iteration 234/1000 | Loss: 0.00002751
Iteration 235/1000 | Loss: 0.00002751
Iteration 236/1000 | Loss: 0.00002751
Iteration 237/1000 | Loss: 0.00002751
Iteration 238/1000 | Loss: 0.00002751
Iteration 239/1000 | Loss: 0.00002751
Iteration 240/1000 | Loss: 0.00002751
Iteration 241/1000 | Loss: 0.00002751
Iteration 242/1000 | Loss: 0.00002751
Iteration 243/1000 | Loss: 0.00002751
Iteration 244/1000 | Loss: 0.00002751
Iteration 245/1000 | Loss: 0.00002751
Iteration 246/1000 | Loss: 0.00002751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.751236934273038e-05, 2.751236934273038e-05, 2.751236934273038e-05, 2.751236934273038e-05, 2.751236934273038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.751236934273038e-05

Optimization complete. Final v2v error: 3.9333155155181885 mm

Highest mean error: 11.596240997314453 mm for frame 41

Lowest mean error: 3.574558734893799 mm for frame 6

Saving results

Total time: 196.57237434387207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993507
Iteration 2/25 | Loss: 0.00993507
Iteration 3/25 | Loss: 0.00993507
Iteration 4/25 | Loss: 0.00993507
Iteration 5/25 | Loss: 0.00993506
Iteration 6/25 | Loss: 0.00993506
Iteration 7/25 | Loss: 0.00993506
Iteration 8/25 | Loss: 0.00993506
Iteration 9/25 | Loss: 0.00993505
Iteration 10/25 | Loss: 0.00993505
Iteration 11/25 | Loss: 0.00993505
Iteration 12/25 | Loss: 0.00993505
Iteration 13/25 | Loss: 0.00993505
Iteration 14/25 | Loss: 0.00993505
Iteration 15/25 | Loss: 0.00993505
Iteration 16/25 | Loss: 0.00993504
Iteration 17/25 | Loss: 0.00993504
Iteration 18/25 | Loss: 0.00993504
Iteration 19/25 | Loss: 0.00993504
Iteration 20/25 | Loss: 0.00993504
Iteration 21/25 | Loss: 0.00993504
Iteration 22/25 | Loss: 0.00993503
Iteration 23/25 | Loss: 0.00993503
Iteration 24/25 | Loss: 0.00993503
Iteration 25/25 | Loss: 0.00993503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75632060
Iteration 2/25 | Loss: 0.16321397
Iteration 3/25 | Loss: 0.16198044
Iteration 4/25 | Loss: 0.16369538
Iteration 5/25 | Loss: 0.16137007
Iteration 6/25 | Loss: 0.16136947
Iteration 7/25 | Loss: 0.16136946
Iteration 8/25 | Loss: 0.16136946
Iteration 9/25 | Loss: 0.16136943
Iteration 10/25 | Loss: 0.16136943
Iteration 11/25 | Loss: 0.16136943
Iteration 12/25 | Loss: 0.16136943
Iteration 13/25 | Loss: 0.16136943
Iteration 14/25 | Loss: 0.16136943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.1613694280385971, 0.1613694280385971, 0.1613694280385971, 0.1613694280385971, 0.1613694280385971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1613694280385971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16136943
Iteration 2/1000 | Loss: 0.00644230
Iteration 3/1000 | Loss: 0.00323008
Iteration 4/1000 | Loss: 0.00080776
Iteration 5/1000 | Loss: 0.00044108
Iteration 6/1000 | Loss: 0.00058087
Iteration 7/1000 | Loss: 0.00025632
Iteration 8/1000 | Loss: 0.00028036
Iteration 9/1000 | Loss: 0.00017471
Iteration 10/1000 | Loss: 0.00009641
Iteration 11/1000 | Loss: 0.00011863
Iteration 12/1000 | Loss: 0.00044042
Iteration 13/1000 | Loss: 0.00033676
Iteration 14/1000 | Loss: 0.00005843
Iteration 15/1000 | Loss: 0.00010898
Iteration 16/1000 | Loss: 0.00009420
Iteration 17/1000 | Loss: 0.00004307
Iteration 18/1000 | Loss: 0.00018785
Iteration 19/1000 | Loss: 0.00004884
Iteration 20/1000 | Loss: 0.00003776
Iteration 21/1000 | Loss: 0.00003587
Iteration 22/1000 | Loss: 0.00006957
Iteration 23/1000 | Loss: 0.00003256
Iteration 24/1000 | Loss: 0.00003108
Iteration 25/1000 | Loss: 0.00022064
Iteration 26/1000 | Loss: 0.00002988
Iteration 27/1000 | Loss: 0.00002900
Iteration 28/1000 | Loss: 0.00002825
Iteration 29/1000 | Loss: 0.00013427
Iteration 30/1000 | Loss: 0.00015455
Iteration 31/1000 | Loss: 0.00075378
Iteration 32/1000 | Loss: 0.00002923
Iteration 33/1000 | Loss: 0.00007083
Iteration 34/1000 | Loss: 0.00037062
Iteration 35/1000 | Loss: 0.00006559
Iteration 36/1000 | Loss: 0.00005014
Iteration 37/1000 | Loss: 0.00002667
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00002563
Iteration 40/1000 | Loss: 0.00002527
Iteration 41/1000 | Loss: 0.00002501
Iteration 42/1000 | Loss: 0.00002469
Iteration 43/1000 | Loss: 0.00002458
Iteration 44/1000 | Loss: 0.00027885
Iteration 45/1000 | Loss: 0.00015384
Iteration 46/1000 | Loss: 0.00002541
Iteration 47/1000 | Loss: 0.00002468
Iteration 48/1000 | Loss: 0.00002435
Iteration 49/1000 | Loss: 0.00002430
Iteration 50/1000 | Loss: 0.00002428
Iteration 51/1000 | Loss: 0.00002428
Iteration 52/1000 | Loss: 0.00002428
Iteration 53/1000 | Loss: 0.00002428
Iteration 54/1000 | Loss: 0.00002427
Iteration 55/1000 | Loss: 0.00002427
Iteration 56/1000 | Loss: 0.00002427
Iteration 57/1000 | Loss: 0.00002427
Iteration 58/1000 | Loss: 0.00002427
Iteration 59/1000 | Loss: 0.00002427
Iteration 60/1000 | Loss: 0.00002427
Iteration 61/1000 | Loss: 0.00002427
Iteration 62/1000 | Loss: 0.00002427
Iteration 63/1000 | Loss: 0.00002427
Iteration 64/1000 | Loss: 0.00002426
Iteration 65/1000 | Loss: 0.00002426
Iteration 66/1000 | Loss: 0.00002425
Iteration 67/1000 | Loss: 0.00002425
Iteration 68/1000 | Loss: 0.00002425
Iteration 69/1000 | Loss: 0.00002425
Iteration 70/1000 | Loss: 0.00002425
Iteration 71/1000 | Loss: 0.00002425
Iteration 72/1000 | Loss: 0.00002425
Iteration 73/1000 | Loss: 0.00002425
Iteration 74/1000 | Loss: 0.00002424
Iteration 75/1000 | Loss: 0.00002423
Iteration 76/1000 | Loss: 0.00002423
Iteration 77/1000 | Loss: 0.00002423
Iteration 78/1000 | Loss: 0.00002422
Iteration 79/1000 | Loss: 0.00002422
Iteration 80/1000 | Loss: 0.00002421
Iteration 81/1000 | Loss: 0.00002421
Iteration 82/1000 | Loss: 0.00002421
Iteration 83/1000 | Loss: 0.00002421
Iteration 84/1000 | Loss: 0.00002421
Iteration 85/1000 | Loss: 0.00002421
Iteration 86/1000 | Loss: 0.00002421
Iteration 87/1000 | Loss: 0.00002420
Iteration 88/1000 | Loss: 0.00002420
Iteration 89/1000 | Loss: 0.00002420
Iteration 90/1000 | Loss: 0.00002420
Iteration 91/1000 | Loss: 0.00002420
Iteration 92/1000 | Loss: 0.00002420
Iteration 93/1000 | Loss: 0.00002419
Iteration 94/1000 | Loss: 0.00002419
Iteration 95/1000 | Loss: 0.00002419
Iteration 96/1000 | Loss: 0.00002419
Iteration 97/1000 | Loss: 0.00002418
Iteration 98/1000 | Loss: 0.00002418
Iteration 99/1000 | Loss: 0.00002418
Iteration 100/1000 | Loss: 0.00002417
Iteration 101/1000 | Loss: 0.00002417
Iteration 102/1000 | Loss: 0.00002417
Iteration 103/1000 | Loss: 0.00002416
Iteration 104/1000 | Loss: 0.00002416
Iteration 105/1000 | Loss: 0.00002415
Iteration 106/1000 | Loss: 0.00002415
Iteration 107/1000 | Loss: 0.00002415
Iteration 108/1000 | Loss: 0.00002414
Iteration 109/1000 | Loss: 0.00002414
Iteration 110/1000 | Loss: 0.00002413
Iteration 111/1000 | Loss: 0.00002412
Iteration 112/1000 | Loss: 0.00002411
Iteration 113/1000 | Loss: 0.00002410
Iteration 114/1000 | Loss: 0.00002410
Iteration 115/1000 | Loss: 0.00002410
Iteration 116/1000 | Loss: 0.00002410
Iteration 117/1000 | Loss: 0.00002410
Iteration 118/1000 | Loss: 0.00002410
Iteration 119/1000 | Loss: 0.00002410
Iteration 120/1000 | Loss: 0.00002410
Iteration 121/1000 | Loss: 0.00002409
Iteration 122/1000 | Loss: 0.00002409
Iteration 123/1000 | Loss: 0.00002409
Iteration 124/1000 | Loss: 0.00002409
Iteration 125/1000 | Loss: 0.00002409
Iteration 126/1000 | Loss: 0.00002408
Iteration 127/1000 | Loss: 0.00002408
Iteration 128/1000 | Loss: 0.00002407
Iteration 129/1000 | Loss: 0.00002407
Iteration 130/1000 | Loss: 0.00002407
Iteration 131/1000 | Loss: 0.00002406
Iteration 132/1000 | Loss: 0.00002406
Iteration 133/1000 | Loss: 0.00002405
Iteration 134/1000 | Loss: 0.00002405
Iteration 135/1000 | Loss: 0.00002404
Iteration 136/1000 | Loss: 0.00002404
Iteration 137/1000 | Loss: 0.00015613
Iteration 138/1000 | Loss: 0.00002827
Iteration 139/1000 | Loss: 0.00002535
Iteration 140/1000 | Loss: 0.00013425
Iteration 141/1000 | Loss: 0.00002412
Iteration 142/1000 | Loss: 0.00002401
Iteration 143/1000 | Loss: 0.00002401
Iteration 144/1000 | Loss: 0.00002401
Iteration 145/1000 | Loss: 0.00002401
Iteration 146/1000 | Loss: 0.00002400
Iteration 147/1000 | Loss: 0.00002400
Iteration 148/1000 | Loss: 0.00002400
Iteration 149/1000 | Loss: 0.00002400
Iteration 150/1000 | Loss: 0.00002400
Iteration 151/1000 | Loss: 0.00002400
Iteration 152/1000 | Loss: 0.00002400
Iteration 153/1000 | Loss: 0.00002399
Iteration 154/1000 | Loss: 0.00002399
Iteration 155/1000 | Loss: 0.00002399
Iteration 156/1000 | Loss: 0.00002399
Iteration 157/1000 | Loss: 0.00002399
Iteration 158/1000 | Loss: 0.00002399
Iteration 159/1000 | Loss: 0.00002399
Iteration 160/1000 | Loss: 0.00002398
Iteration 161/1000 | Loss: 0.00002398
Iteration 162/1000 | Loss: 0.00002398
Iteration 163/1000 | Loss: 0.00002398
Iteration 164/1000 | Loss: 0.00002398
Iteration 165/1000 | Loss: 0.00002398
Iteration 166/1000 | Loss: 0.00002398
Iteration 167/1000 | Loss: 0.00002398
Iteration 168/1000 | Loss: 0.00002398
Iteration 169/1000 | Loss: 0.00002398
Iteration 170/1000 | Loss: 0.00002398
Iteration 171/1000 | Loss: 0.00002398
Iteration 172/1000 | Loss: 0.00002398
Iteration 173/1000 | Loss: 0.00002398
Iteration 174/1000 | Loss: 0.00002398
Iteration 175/1000 | Loss: 0.00002398
Iteration 176/1000 | Loss: 0.00002397
Iteration 177/1000 | Loss: 0.00002397
Iteration 178/1000 | Loss: 0.00002397
Iteration 179/1000 | Loss: 0.00002397
Iteration 180/1000 | Loss: 0.00002397
Iteration 181/1000 | Loss: 0.00002397
Iteration 182/1000 | Loss: 0.00002397
Iteration 183/1000 | Loss: 0.00002397
Iteration 184/1000 | Loss: 0.00002397
Iteration 185/1000 | Loss: 0.00002397
Iteration 186/1000 | Loss: 0.00002397
Iteration 187/1000 | Loss: 0.00002397
Iteration 188/1000 | Loss: 0.00002397
Iteration 189/1000 | Loss: 0.00002397
Iteration 190/1000 | Loss: 0.00002397
Iteration 191/1000 | Loss: 0.00002396
Iteration 192/1000 | Loss: 0.00002396
Iteration 193/1000 | Loss: 0.00002396
Iteration 194/1000 | Loss: 0.00002396
Iteration 195/1000 | Loss: 0.00002396
Iteration 196/1000 | Loss: 0.00002396
Iteration 197/1000 | Loss: 0.00002396
Iteration 198/1000 | Loss: 0.00002396
Iteration 199/1000 | Loss: 0.00002396
Iteration 200/1000 | Loss: 0.00002396
Iteration 201/1000 | Loss: 0.00002396
Iteration 202/1000 | Loss: 0.00002396
Iteration 203/1000 | Loss: 0.00002396
Iteration 204/1000 | Loss: 0.00002395
Iteration 205/1000 | Loss: 0.00002395
Iteration 206/1000 | Loss: 0.00002395
Iteration 207/1000 | Loss: 0.00002395
Iteration 208/1000 | Loss: 0.00002395
Iteration 209/1000 | Loss: 0.00002394
Iteration 210/1000 | Loss: 0.00002394
Iteration 211/1000 | Loss: 0.00002394
Iteration 212/1000 | Loss: 0.00002394
Iteration 213/1000 | Loss: 0.00002394
Iteration 214/1000 | Loss: 0.00002394
Iteration 215/1000 | Loss: 0.00002394
Iteration 216/1000 | Loss: 0.00002393
Iteration 217/1000 | Loss: 0.00002393
Iteration 218/1000 | Loss: 0.00002393
Iteration 219/1000 | Loss: 0.00002393
Iteration 220/1000 | Loss: 0.00002393
Iteration 221/1000 | Loss: 0.00002393
Iteration 222/1000 | Loss: 0.00002393
Iteration 223/1000 | Loss: 0.00002393
Iteration 224/1000 | Loss: 0.00002393
Iteration 225/1000 | Loss: 0.00002393
Iteration 226/1000 | Loss: 0.00002393
Iteration 227/1000 | Loss: 0.00002393
Iteration 228/1000 | Loss: 0.00002393
Iteration 229/1000 | Loss: 0.00002393
Iteration 230/1000 | Loss: 0.00002393
Iteration 231/1000 | Loss: 0.00002393
Iteration 232/1000 | Loss: 0.00002393
Iteration 233/1000 | Loss: 0.00002393
Iteration 234/1000 | Loss: 0.00002393
Iteration 235/1000 | Loss: 0.00002393
Iteration 236/1000 | Loss: 0.00002393
Iteration 237/1000 | Loss: 0.00002393
Iteration 238/1000 | Loss: 0.00002392
Iteration 239/1000 | Loss: 0.00002392
Iteration 240/1000 | Loss: 0.00002392
Iteration 241/1000 | Loss: 0.00002392
Iteration 242/1000 | Loss: 0.00002392
Iteration 243/1000 | Loss: 0.00002392
Iteration 244/1000 | Loss: 0.00002392
Iteration 245/1000 | Loss: 0.00002392
Iteration 246/1000 | Loss: 0.00002392
Iteration 247/1000 | Loss: 0.00002392
Iteration 248/1000 | Loss: 0.00002391
Iteration 249/1000 | Loss: 0.00002391
Iteration 250/1000 | Loss: 0.00002391
Iteration 251/1000 | Loss: 0.00002391
Iteration 252/1000 | Loss: 0.00002391
Iteration 253/1000 | Loss: 0.00002391
Iteration 254/1000 | Loss: 0.00002391
Iteration 255/1000 | Loss: 0.00002391
Iteration 256/1000 | Loss: 0.00002391
Iteration 257/1000 | Loss: 0.00002391
Iteration 258/1000 | Loss: 0.00002391
Iteration 259/1000 | Loss: 0.00002391
Iteration 260/1000 | Loss: 0.00002391
Iteration 261/1000 | Loss: 0.00002391
Iteration 262/1000 | Loss: 0.00002391
Iteration 263/1000 | Loss: 0.00002390
Iteration 264/1000 | Loss: 0.00015589
Iteration 265/1000 | Loss: 0.00002516
Iteration 266/1000 | Loss: 0.00006831
Iteration 267/1000 | Loss: 0.00002512
Iteration 268/1000 | Loss: 0.00002421
Iteration 269/1000 | Loss: 0.00014014
Iteration 270/1000 | Loss: 0.00063602
Iteration 271/1000 | Loss: 0.00005903
Iteration 272/1000 | Loss: 0.00004128
Iteration 273/1000 | Loss: 0.00002644
Iteration 274/1000 | Loss: 0.00007149
Iteration 275/1000 | Loss: 0.00002413
Iteration 276/1000 | Loss: 0.00002395
Iteration 277/1000 | Loss: 0.00002394
Iteration 278/1000 | Loss: 0.00002392
Iteration 279/1000 | Loss: 0.00002392
Iteration 280/1000 | Loss: 0.00002391
Iteration 281/1000 | Loss: 0.00002391
Iteration 282/1000 | Loss: 0.00002390
Iteration 283/1000 | Loss: 0.00002389
Iteration 284/1000 | Loss: 0.00002389
Iteration 285/1000 | Loss: 0.00002388
Iteration 286/1000 | Loss: 0.00002388
Iteration 287/1000 | Loss: 0.00002388
Iteration 288/1000 | Loss: 0.00002388
Iteration 289/1000 | Loss: 0.00002388
Iteration 290/1000 | Loss: 0.00002388
Iteration 291/1000 | Loss: 0.00002387
Iteration 292/1000 | Loss: 0.00002387
Iteration 293/1000 | Loss: 0.00002387
Iteration 294/1000 | Loss: 0.00002387
Iteration 295/1000 | Loss: 0.00002387
Iteration 296/1000 | Loss: 0.00002387
Iteration 297/1000 | Loss: 0.00002386
Iteration 298/1000 | Loss: 0.00002386
Iteration 299/1000 | Loss: 0.00002386
Iteration 300/1000 | Loss: 0.00002386
Iteration 301/1000 | Loss: 0.00002386
Iteration 302/1000 | Loss: 0.00002386
Iteration 303/1000 | Loss: 0.00002386
Iteration 304/1000 | Loss: 0.00002386
Iteration 305/1000 | Loss: 0.00002386
Iteration 306/1000 | Loss: 0.00002386
Iteration 307/1000 | Loss: 0.00002386
Iteration 308/1000 | Loss: 0.00002386
Iteration 309/1000 | Loss: 0.00002386
Iteration 310/1000 | Loss: 0.00002386
Iteration 311/1000 | Loss: 0.00002386
Iteration 312/1000 | Loss: 0.00002386
Iteration 313/1000 | Loss: 0.00002386
Iteration 314/1000 | Loss: 0.00002386
Iteration 315/1000 | Loss: 0.00002386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [2.3860031433287077e-05, 2.3860031433287077e-05, 2.3860031433287077e-05, 2.3860031433287077e-05, 2.3860031433287077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3860031433287077e-05

Optimization complete. Final v2v error: 4.174386501312256 mm

Highest mean error: 4.7570037841796875 mm for frame 230

Lowest mean error: 3.858140468597412 mm for frame 176

Saving results

Total time: 131.8933503627777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954487
Iteration 2/25 | Loss: 0.00306806
Iteration 3/25 | Loss: 0.00231101
Iteration 4/25 | Loss: 0.00210634
Iteration 5/25 | Loss: 0.00198941
Iteration 6/25 | Loss: 0.00184002
Iteration 7/25 | Loss: 0.00213475
Iteration 8/25 | Loss: 0.00214850
Iteration 9/25 | Loss: 0.00153074
Iteration 10/25 | Loss: 0.00140882
Iteration 11/25 | Loss: 0.00137573
Iteration 12/25 | Loss: 0.00137293
Iteration 13/25 | Loss: 0.00137935
Iteration 14/25 | Loss: 0.00137199
Iteration 15/25 | Loss: 0.00136974
Iteration 16/25 | Loss: 0.00136927
Iteration 17/25 | Loss: 0.00136923
Iteration 18/25 | Loss: 0.00136923
Iteration 19/25 | Loss: 0.00136923
Iteration 20/25 | Loss: 0.00136922
Iteration 21/25 | Loss: 0.00136922
Iteration 22/25 | Loss: 0.00136922
Iteration 23/25 | Loss: 0.00136922
Iteration 24/25 | Loss: 0.00136922
Iteration 25/25 | Loss: 0.00136922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.12662125
Iteration 2/25 | Loss: 0.00139554
Iteration 3/25 | Loss: 0.00074390
Iteration 4/25 | Loss: 0.00074390
Iteration 5/25 | Loss: 0.00074390
Iteration 6/25 | Loss: 0.00074390
Iteration 7/25 | Loss: 0.00074390
Iteration 8/25 | Loss: 0.00074390
Iteration 9/25 | Loss: 0.00074390
Iteration 10/25 | Loss: 0.00074390
Iteration 11/25 | Loss: 0.00074390
Iteration 12/25 | Loss: 0.00074390
Iteration 13/25 | Loss: 0.00074390
Iteration 14/25 | Loss: 0.00074390
Iteration 15/25 | Loss: 0.00074390
Iteration 16/25 | Loss: 0.00074390
Iteration 17/25 | Loss: 0.00074390
Iteration 18/25 | Loss: 0.00074390
Iteration 19/25 | Loss: 0.00074390
Iteration 20/25 | Loss: 0.00074390
Iteration 21/25 | Loss: 0.00074390
Iteration 22/25 | Loss: 0.00074390
Iteration 23/25 | Loss: 0.00074390
Iteration 24/25 | Loss: 0.00074390
Iteration 25/25 | Loss: 0.00074390

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074390
Iteration 2/1000 | Loss: 0.00006835
Iteration 3/1000 | Loss: 0.00004897
Iteration 4/1000 | Loss: 0.00004409
Iteration 5/1000 | Loss: 0.00004158
Iteration 6/1000 | Loss: 0.00050132
Iteration 7/1000 | Loss: 0.00004804
Iteration 8/1000 | Loss: 0.00003628
Iteration 9/1000 | Loss: 0.00042922
Iteration 10/1000 | Loss: 0.00003033
Iteration 11/1000 | Loss: 0.00002719
Iteration 12/1000 | Loss: 0.00002585
Iteration 13/1000 | Loss: 0.00002495
Iteration 14/1000 | Loss: 0.00002415
Iteration 15/1000 | Loss: 0.00002361
Iteration 16/1000 | Loss: 0.00002316
Iteration 17/1000 | Loss: 0.00002270
Iteration 18/1000 | Loss: 0.00002241
Iteration 19/1000 | Loss: 0.00002224
Iteration 20/1000 | Loss: 0.00002217
Iteration 21/1000 | Loss: 0.00002213
Iteration 22/1000 | Loss: 0.00002213
Iteration 23/1000 | Loss: 0.00002209
Iteration 24/1000 | Loss: 0.00002209
Iteration 25/1000 | Loss: 0.00002208
Iteration 26/1000 | Loss: 0.00002207
Iteration 27/1000 | Loss: 0.00002206
Iteration 28/1000 | Loss: 0.00002202
Iteration 29/1000 | Loss: 0.00002202
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002200
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002200
Iteration 34/1000 | Loss: 0.00002199
Iteration 35/1000 | Loss: 0.00002197
Iteration 36/1000 | Loss: 0.00002197
Iteration 37/1000 | Loss: 0.00002197
Iteration 38/1000 | Loss: 0.00002197
Iteration 39/1000 | Loss: 0.00002197
Iteration 40/1000 | Loss: 0.00002196
Iteration 41/1000 | Loss: 0.00002196
Iteration 42/1000 | Loss: 0.00002196
Iteration 43/1000 | Loss: 0.00002195
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002195
Iteration 46/1000 | Loss: 0.00002194
Iteration 47/1000 | Loss: 0.00002194
Iteration 48/1000 | Loss: 0.00002191
Iteration 49/1000 | Loss: 0.00002191
Iteration 50/1000 | Loss: 0.00002191
Iteration 51/1000 | Loss: 0.00002191
Iteration 52/1000 | Loss: 0.00002191
Iteration 53/1000 | Loss: 0.00002191
Iteration 54/1000 | Loss: 0.00002191
Iteration 55/1000 | Loss: 0.00002190
Iteration 56/1000 | Loss: 0.00002190
Iteration 57/1000 | Loss: 0.00002190
Iteration 58/1000 | Loss: 0.00002190
Iteration 59/1000 | Loss: 0.00002190
Iteration 60/1000 | Loss: 0.00002190
Iteration 61/1000 | Loss: 0.00002190
Iteration 62/1000 | Loss: 0.00002190
Iteration 63/1000 | Loss: 0.00002190
Iteration 64/1000 | Loss: 0.00002190
Iteration 65/1000 | Loss: 0.00002190
Iteration 66/1000 | Loss: 0.00002190
Iteration 67/1000 | Loss: 0.00002190
Iteration 68/1000 | Loss: 0.00002190
Iteration 69/1000 | Loss: 0.00002190
Iteration 70/1000 | Loss: 0.00002190
Iteration 71/1000 | Loss: 0.00002190
Iteration 72/1000 | Loss: 0.00002190
Iteration 73/1000 | Loss: 0.00002190
Iteration 74/1000 | Loss: 0.00002190
Iteration 75/1000 | Loss: 0.00002190
Iteration 76/1000 | Loss: 0.00002190
Iteration 77/1000 | Loss: 0.00002190
Iteration 78/1000 | Loss: 0.00002190
Iteration 79/1000 | Loss: 0.00002190
Iteration 80/1000 | Loss: 0.00002190
Iteration 81/1000 | Loss: 0.00002190
Iteration 82/1000 | Loss: 0.00002190
Iteration 83/1000 | Loss: 0.00002190
Iteration 84/1000 | Loss: 0.00002190
Iteration 85/1000 | Loss: 0.00002190
Iteration 86/1000 | Loss: 0.00002190
Iteration 87/1000 | Loss: 0.00002190
Iteration 88/1000 | Loss: 0.00002190
Iteration 89/1000 | Loss: 0.00002190
Iteration 90/1000 | Loss: 0.00002190
Iteration 91/1000 | Loss: 0.00002190
Iteration 92/1000 | Loss: 0.00002190
Iteration 93/1000 | Loss: 0.00002190
Iteration 94/1000 | Loss: 0.00002190
Iteration 95/1000 | Loss: 0.00002190
Iteration 96/1000 | Loss: 0.00002190
Iteration 97/1000 | Loss: 0.00002190
Iteration 98/1000 | Loss: 0.00002190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.1903859305894002e-05, 2.1903859305894002e-05, 2.1903859305894002e-05, 2.1903859305894002e-05, 2.1903859305894002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1903859305894002e-05

Optimization complete. Final v2v error: 3.9427592754364014 mm

Highest mean error: 4.395840167999268 mm for frame 58

Lowest mean error: 3.483192205429077 mm for frame 124

Saving results

Total time: 59.16275238990784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438841
Iteration 2/25 | Loss: 0.00140258
Iteration 3/25 | Loss: 0.00132364
Iteration 4/25 | Loss: 0.00130776
Iteration 5/25 | Loss: 0.00130197
Iteration 6/25 | Loss: 0.00130105
Iteration 7/25 | Loss: 0.00130105
Iteration 8/25 | Loss: 0.00130105
Iteration 9/25 | Loss: 0.00130105
Iteration 10/25 | Loss: 0.00130105
Iteration 11/25 | Loss: 0.00130105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00130104657728225, 0.00130104657728225, 0.00130104657728225, 0.00130104657728225, 0.00130104657728225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00130104657728225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84033501
Iteration 2/25 | Loss: 0.00098618
Iteration 3/25 | Loss: 0.00098617
Iteration 4/25 | Loss: 0.00098617
Iteration 5/25 | Loss: 0.00098617
Iteration 6/25 | Loss: 0.00098617
Iteration 7/25 | Loss: 0.00098617
Iteration 8/25 | Loss: 0.00098617
Iteration 9/25 | Loss: 0.00098617
Iteration 10/25 | Loss: 0.00098617
Iteration 11/25 | Loss: 0.00098617
Iteration 12/25 | Loss: 0.00098617
Iteration 13/25 | Loss: 0.00098617
Iteration 14/25 | Loss: 0.00098617
Iteration 15/25 | Loss: 0.00098617
Iteration 16/25 | Loss: 0.00098617
Iteration 17/25 | Loss: 0.00098617
Iteration 18/25 | Loss: 0.00098617
Iteration 19/25 | Loss: 0.00098617
Iteration 20/25 | Loss: 0.00098617
Iteration 21/25 | Loss: 0.00098617
Iteration 22/25 | Loss: 0.00098617
Iteration 23/25 | Loss: 0.00098617
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009861711878329515, 0.0009861711878329515, 0.0009861711878329515, 0.0009861711878329515, 0.0009861711878329515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009861711878329515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098617
Iteration 2/1000 | Loss: 0.00003593
Iteration 3/1000 | Loss: 0.00002331
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00002094
Iteration 6/1000 | Loss: 0.00002047
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001975
Iteration 10/1000 | Loss: 0.00001975
Iteration 11/1000 | Loss: 0.00001973
Iteration 12/1000 | Loss: 0.00001967
Iteration 13/1000 | Loss: 0.00001966
Iteration 14/1000 | Loss: 0.00001960
Iteration 15/1000 | Loss: 0.00001959
Iteration 16/1000 | Loss: 0.00001955
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001953
Iteration 19/1000 | Loss: 0.00001953
Iteration 20/1000 | Loss: 0.00001950
Iteration 21/1000 | Loss: 0.00001942
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001932
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001931
Iteration 32/1000 | Loss: 0.00001930
Iteration 33/1000 | Loss: 0.00001930
Iteration 34/1000 | Loss: 0.00001930
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001928
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001927
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001926
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001926
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001925
Iteration 48/1000 | Loss: 0.00001925
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001919
Iteration 60/1000 | Loss: 0.00001919
Iteration 61/1000 | Loss: 0.00001919
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00001918
Iteration 64/1000 | Loss: 0.00001918
Iteration 65/1000 | Loss: 0.00001918
Iteration 66/1000 | Loss: 0.00001918
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001915
Iteration 74/1000 | Loss: 0.00001915
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001915
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001913
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001912
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001911
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001911
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001910
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001910
Iteration 101/1000 | Loss: 0.00001909
Iteration 102/1000 | Loss: 0.00001909
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001908
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001907
Iteration 110/1000 | Loss: 0.00001907
Iteration 111/1000 | Loss: 0.00001907
Iteration 112/1000 | Loss: 0.00001907
Iteration 113/1000 | Loss: 0.00001907
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001906
Iteration 116/1000 | Loss: 0.00001906
Iteration 117/1000 | Loss: 0.00001906
Iteration 118/1000 | Loss: 0.00001906
Iteration 119/1000 | Loss: 0.00001905
Iteration 120/1000 | Loss: 0.00001905
Iteration 121/1000 | Loss: 0.00001905
Iteration 122/1000 | Loss: 0.00001905
Iteration 123/1000 | Loss: 0.00001905
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001904
Iteration 126/1000 | Loss: 0.00001904
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001904
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001902
Iteration 140/1000 | Loss: 0.00001902
Iteration 141/1000 | Loss: 0.00001902
Iteration 142/1000 | Loss: 0.00001902
Iteration 143/1000 | Loss: 0.00001902
Iteration 144/1000 | Loss: 0.00001902
Iteration 145/1000 | Loss: 0.00001901
Iteration 146/1000 | Loss: 0.00001901
Iteration 147/1000 | Loss: 0.00001900
Iteration 148/1000 | Loss: 0.00001900
Iteration 149/1000 | Loss: 0.00001900
Iteration 150/1000 | Loss: 0.00001900
Iteration 151/1000 | Loss: 0.00001900
Iteration 152/1000 | Loss: 0.00001900
Iteration 153/1000 | Loss: 0.00001900
Iteration 154/1000 | Loss: 0.00001900
Iteration 155/1000 | Loss: 0.00001900
Iteration 156/1000 | Loss: 0.00001900
Iteration 157/1000 | Loss: 0.00001900
Iteration 158/1000 | Loss: 0.00001900
Iteration 159/1000 | Loss: 0.00001900
Iteration 160/1000 | Loss: 0.00001900
Iteration 161/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.8996175640495494e-05, 1.8996175640495494e-05, 1.8996175640495494e-05, 1.8996175640495494e-05, 1.8996175640495494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8996175640495494e-05

Optimization complete. Final v2v error: 3.688793182373047 mm

Highest mean error: 4.322216987609863 mm for frame 157

Lowest mean error: 3.5740466117858887 mm for frame 64

Saving results

Total time: 40.1000497341156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488769
Iteration 2/25 | Loss: 0.00131777
Iteration 3/25 | Loss: 0.00125328
Iteration 4/25 | Loss: 0.00124138
Iteration 5/25 | Loss: 0.00123727
Iteration 6/25 | Loss: 0.00123709
Iteration 7/25 | Loss: 0.00123709
Iteration 8/25 | Loss: 0.00123709
Iteration 9/25 | Loss: 0.00123709
Iteration 10/25 | Loss: 0.00123709
Iteration 11/25 | Loss: 0.00123709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012370927724987268, 0.0012370927724987268, 0.0012370927724987268, 0.0012370927724987268, 0.0012370927724987268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012370927724987268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.40687442
Iteration 2/25 | Loss: 0.00081198
Iteration 3/25 | Loss: 0.00081197
Iteration 4/25 | Loss: 0.00081197
Iteration 5/25 | Loss: 0.00081197
Iteration 6/25 | Loss: 0.00081197
Iteration 7/25 | Loss: 0.00081197
Iteration 8/25 | Loss: 0.00081197
Iteration 9/25 | Loss: 0.00081197
Iteration 10/25 | Loss: 0.00081197
Iteration 11/25 | Loss: 0.00081197
Iteration 12/25 | Loss: 0.00081197
Iteration 13/25 | Loss: 0.00081197
Iteration 14/25 | Loss: 0.00081197
Iteration 15/25 | Loss: 0.00081197
Iteration 16/25 | Loss: 0.00081197
Iteration 17/25 | Loss: 0.00081197
Iteration 18/25 | Loss: 0.00081197
Iteration 19/25 | Loss: 0.00081197
Iteration 20/25 | Loss: 0.00081197
Iteration 21/25 | Loss: 0.00081197
Iteration 22/25 | Loss: 0.00081197
Iteration 23/25 | Loss: 0.00081197
Iteration 24/25 | Loss: 0.00081197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008119700360111892, 0.0008119700360111892, 0.0008119700360111892, 0.0008119700360111892, 0.0008119700360111892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008119700360111892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081197
Iteration 2/1000 | Loss: 0.00002269
Iteration 3/1000 | Loss: 0.00001748
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001551
Iteration 6/1000 | Loss: 0.00001496
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001420
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001403
Iteration 13/1000 | Loss: 0.00001401
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001398
Iteration 16/1000 | Loss: 0.00001397
Iteration 17/1000 | Loss: 0.00001397
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001384
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001377
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001377
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001373
Iteration 32/1000 | Loss: 0.00001373
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001371
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001368
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001367
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001366
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001353
Iteration 48/1000 | Loss: 0.00001353
Iteration 49/1000 | Loss: 0.00001353
Iteration 50/1000 | Loss: 0.00001353
Iteration 51/1000 | Loss: 0.00001353
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001352
Iteration 55/1000 | Loss: 0.00001352
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001348
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001347
Iteration 65/1000 | Loss: 0.00001346
Iteration 66/1000 | Loss: 0.00001346
Iteration 67/1000 | Loss: 0.00001346
Iteration 68/1000 | Loss: 0.00001346
Iteration 69/1000 | Loss: 0.00001346
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00001345
Iteration 72/1000 | Loss: 0.00001345
Iteration 73/1000 | Loss: 0.00001345
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001336
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001336
Iteration 100/1000 | Loss: 0.00001336
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001334
Iteration 105/1000 | Loss: 0.00001334
Iteration 106/1000 | Loss: 0.00001334
Iteration 107/1000 | Loss: 0.00001334
Iteration 108/1000 | Loss: 0.00001334
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001333
Iteration 121/1000 | Loss: 0.00001333
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001332
Iteration 126/1000 | Loss: 0.00001332
Iteration 127/1000 | Loss: 0.00001332
Iteration 128/1000 | Loss: 0.00001332
Iteration 129/1000 | Loss: 0.00001332
Iteration 130/1000 | Loss: 0.00001332
Iteration 131/1000 | Loss: 0.00001332
Iteration 132/1000 | Loss: 0.00001331
Iteration 133/1000 | Loss: 0.00001331
Iteration 134/1000 | Loss: 0.00001331
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001331
Iteration 137/1000 | Loss: 0.00001331
Iteration 138/1000 | Loss: 0.00001331
Iteration 139/1000 | Loss: 0.00001331
Iteration 140/1000 | Loss: 0.00001331
Iteration 141/1000 | Loss: 0.00001331
Iteration 142/1000 | Loss: 0.00001331
Iteration 143/1000 | Loss: 0.00001331
Iteration 144/1000 | Loss: 0.00001331
Iteration 145/1000 | Loss: 0.00001331
Iteration 146/1000 | Loss: 0.00001331
Iteration 147/1000 | Loss: 0.00001331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.3313923773239367e-05, 1.3313923773239367e-05, 1.3313923773239367e-05, 1.3313923773239367e-05, 1.3313923773239367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3313923773239367e-05

Optimization complete. Final v2v error: 3.153932809829712 mm

Highest mean error: 3.402294158935547 mm for frame 170

Lowest mean error: 3.0449910163879395 mm for frame 89

Saving results

Total time: 40.15171790122986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805249
Iteration 2/25 | Loss: 0.00134725
Iteration 3/25 | Loss: 0.00127708
Iteration 4/25 | Loss: 0.00126210
Iteration 5/25 | Loss: 0.00125791
Iteration 6/25 | Loss: 0.00125716
Iteration 7/25 | Loss: 0.00125716
Iteration 8/25 | Loss: 0.00125716
Iteration 9/25 | Loss: 0.00125716
Iteration 10/25 | Loss: 0.00125716
Iteration 11/25 | Loss: 0.00125716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012571564875543118, 0.0012571564875543118, 0.0012571564875543118, 0.0012571564875543118, 0.0012571564875543118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012571564875543118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26967764
Iteration 2/25 | Loss: 0.00090191
Iteration 3/25 | Loss: 0.00090190
Iteration 4/25 | Loss: 0.00090190
Iteration 5/25 | Loss: 0.00090190
Iteration 6/25 | Loss: 0.00090190
Iteration 7/25 | Loss: 0.00090190
Iteration 8/25 | Loss: 0.00090190
Iteration 9/25 | Loss: 0.00090190
Iteration 10/25 | Loss: 0.00090190
Iteration 11/25 | Loss: 0.00090190
Iteration 12/25 | Loss: 0.00090190
Iteration 13/25 | Loss: 0.00090190
Iteration 14/25 | Loss: 0.00090190
Iteration 15/25 | Loss: 0.00090190
Iteration 16/25 | Loss: 0.00090190
Iteration 17/25 | Loss: 0.00090190
Iteration 18/25 | Loss: 0.00090190
Iteration 19/25 | Loss: 0.00090190
Iteration 20/25 | Loss: 0.00090190
Iteration 21/25 | Loss: 0.00090190
Iteration 22/25 | Loss: 0.00090190
Iteration 23/25 | Loss: 0.00090190
Iteration 24/25 | Loss: 0.00090190
Iteration 25/25 | Loss: 0.00090190

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090190
Iteration 2/1000 | Loss: 0.00004916
Iteration 3/1000 | Loss: 0.00003286
Iteration 4/1000 | Loss: 0.00002859
Iteration 5/1000 | Loss: 0.00002736
Iteration 6/1000 | Loss: 0.00002622
Iteration 7/1000 | Loss: 0.00002556
Iteration 8/1000 | Loss: 0.00002493
Iteration 9/1000 | Loss: 0.00002449
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002387
Iteration 12/1000 | Loss: 0.00002361
Iteration 13/1000 | Loss: 0.00002355
Iteration 14/1000 | Loss: 0.00002338
Iteration 15/1000 | Loss: 0.00002327
Iteration 16/1000 | Loss: 0.00002326
Iteration 17/1000 | Loss: 0.00002326
Iteration 18/1000 | Loss: 0.00002324
Iteration 19/1000 | Loss: 0.00002324
Iteration 20/1000 | Loss: 0.00002324
Iteration 21/1000 | Loss: 0.00002323
Iteration 22/1000 | Loss: 0.00002323
Iteration 23/1000 | Loss: 0.00002322
Iteration 24/1000 | Loss: 0.00002319
Iteration 25/1000 | Loss: 0.00002319
Iteration 26/1000 | Loss: 0.00002319
Iteration 27/1000 | Loss: 0.00002318
Iteration 28/1000 | Loss: 0.00002318
Iteration 29/1000 | Loss: 0.00002318
Iteration 30/1000 | Loss: 0.00002313
Iteration 31/1000 | Loss: 0.00002313
Iteration 32/1000 | Loss: 0.00002311
Iteration 33/1000 | Loss: 0.00002311
Iteration 34/1000 | Loss: 0.00002311
Iteration 35/1000 | Loss: 0.00002311
Iteration 36/1000 | Loss: 0.00002310
Iteration 37/1000 | Loss: 0.00002307
Iteration 38/1000 | Loss: 0.00002305
Iteration 39/1000 | Loss: 0.00002305
Iteration 40/1000 | Loss: 0.00002304
Iteration 41/1000 | Loss: 0.00002304
Iteration 42/1000 | Loss: 0.00002304
Iteration 43/1000 | Loss: 0.00002303
Iteration 44/1000 | Loss: 0.00002303
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002302
Iteration 47/1000 | Loss: 0.00002302
Iteration 48/1000 | Loss: 0.00002301
Iteration 49/1000 | Loss: 0.00002301
Iteration 50/1000 | Loss: 0.00002301
Iteration 51/1000 | Loss: 0.00002301
Iteration 52/1000 | Loss: 0.00002301
Iteration 53/1000 | Loss: 0.00002300
Iteration 54/1000 | Loss: 0.00002300
Iteration 55/1000 | Loss: 0.00002300
Iteration 56/1000 | Loss: 0.00002300
Iteration 57/1000 | Loss: 0.00002299
Iteration 58/1000 | Loss: 0.00002299
Iteration 59/1000 | Loss: 0.00002299
Iteration 60/1000 | Loss: 0.00002299
Iteration 61/1000 | Loss: 0.00002299
Iteration 62/1000 | Loss: 0.00002299
Iteration 63/1000 | Loss: 0.00002299
Iteration 64/1000 | Loss: 0.00002299
Iteration 65/1000 | Loss: 0.00002299
Iteration 66/1000 | Loss: 0.00002298
Iteration 67/1000 | Loss: 0.00002298
Iteration 68/1000 | Loss: 0.00002298
Iteration 69/1000 | Loss: 0.00002297
Iteration 70/1000 | Loss: 0.00002297
Iteration 71/1000 | Loss: 0.00002297
Iteration 72/1000 | Loss: 0.00002297
Iteration 73/1000 | Loss: 0.00002297
Iteration 74/1000 | Loss: 0.00002297
Iteration 75/1000 | Loss: 0.00002297
Iteration 76/1000 | Loss: 0.00002297
Iteration 77/1000 | Loss: 0.00002297
Iteration 78/1000 | Loss: 0.00002296
Iteration 79/1000 | Loss: 0.00002296
Iteration 80/1000 | Loss: 0.00002296
Iteration 81/1000 | Loss: 0.00002296
Iteration 82/1000 | Loss: 0.00002296
Iteration 83/1000 | Loss: 0.00002296
Iteration 84/1000 | Loss: 0.00002296
Iteration 85/1000 | Loss: 0.00002295
Iteration 86/1000 | Loss: 0.00002295
Iteration 87/1000 | Loss: 0.00002295
Iteration 88/1000 | Loss: 0.00002294
Iteration 89/1000 | Loss: 0.00002294
Iteration 90/1000 | Loss: 0.00002294
Iteration 91/1000 | Loss: 0.00002293
Iteration 92/1000 | Loss: 0.00002293
Iteration 93/1000 | Loss: 0.00002293
Iteration 94/1000 | Loss: 0.00002293
Iteration 95/1000 | Loss: 0.00002293
Iteration 96/1000 | Loss: 0.00002292
Iteration 97/1000 | Loss: 0.00002291
Iteration 98/1000 | Loss: 0.00002291
Iteration 99/1000 | Loss: 0.00002291
Iteration 100/1000 | Loss: 0.00002290
Iteration 101/1000 | Loss: 0.00002290
Iteration 102/1000 | Loss: 0.00002290
Iteration 103/1000 | Loss: 0.00002290
Iteration 104/1000 | Loss: 0.00002290
Iteration 105/1000 | Loss: 0.00002290
Iteration 106/1000 | Loss: 0.00002289
Iteration 107/1000 | Loss: 0.00002289
Iteration 108/1000 | Loss: 0.00002289
Iteration 109/1000 | Loss: 0.00002289
Iteration 110/1000 | Loss: 0.00002289
Iteration 111/1000 | Loss: 0.00002289
Iteration 112/1000 | Loss: 0.00002289
Iteration 113/1000 | Loss: 0.00002289
Iteration 114/1000 | Loss: 0.00002289
Iteration 115/1000 | Loss: 0.00002288
Iteration 116/1000 | Loss: 0.00002288
Iteration 117/1000 | Loss: 0.00002288
Iteration 118/1000 | Loss: 0.00002288
Iteration 119/1000 | Loss: 0.00002288
Iteration 120/1000 | Loss: 0.00002288
Iteration 121/1000 | Loss: 0.00002288
Iteration 122/1000 | Loss: 0.00002288
Iteration 123/1000 | Loss: 0.00002288
Iteration 124/1000 | Loss: 0.00002288
Iteration 125/1000 | Loss: 0.00002288
Iteration 126/1000 | Loss: 0.00002288
Iteration 127/1000 | Loss: 0.00002288
Iteration 128/1000 | Loss: 0.00002287
Iteration 129/1000 | Loss: 0.00002287
Iteration 130/1000 | Loss: 0.00002287
Iteration 131/1000 | Loss: 0.00002287
Iteration 132/1000 | Loss: 0.00002286
Iteration 133/1000 | Loss: 0.00002286
Iteration 134/1000 | Loss: 0.00002286
Iteration 135/1000 | Loss: 0.00002286
Iteration 136/1000 | Loss: 0.00002286
Iteration 137/1000 | Loss: 0.00002286
Iteration 138/1000 | Loss: 0.00002286
Iteration 139/1000 | Loss: 0.00002286
Iteration 140/1000 | Loss: 0.00002286
Iteration 141/1000 | Loss: 0.00002286
Iteration 142/1000 | Loss: 0.00002286
Iteration 143/1000 | Loss: 0.00002286
Iteration 144/1000 | Loss: 0.00002286
Iteration 145/1000 | Loss: 0.00002286
Iteration 146/1000 | Loss: 0.00002285
Iteration 147/1000 | Loss: 0.00002285
Iteration 148/1000 | Loss: 0.00002285
Iteration 149/1000 | Loss: 0.00002285
Iteration 150/1000 | Loss: 0.00002285
Iteration 151/1000 | Loss: 0.00002285
Iteration 152/1000 | Loss: 0.00002285
Iteration 153/1000 | Loss: 0.00002285
Iteration 154/1000 | Loss: 0.00002285
Iteration 155/1000 | Loss: 0.00002285
Iteration 156/1000 | Loss: 0.00002285
Iteration 157/1000 | Loss: 0.00002285
Iteration 158/1000 | Loss: 0.00002285
Iteration 159/1000 | Loss: 0.00002284
Iteration 160/1000 | Loss: 0.00002284
Iteration 161/1000 | Loss: 0.00002284
Iteration 162/1000 | Loss: 0.00002284
Iteration 163/1000 | Loss: 0.00002284
Iteration 164/1000 | Loss: 0.00002284
Iteration 165/1000 | Loss: 0.00002284
Iteration 166/1000 | Loss: 0.00002284
Iteration 167/1000 | Loss: 0.00002284
Iteration 168/1000 | Loss: 0.00002284
Iteration 169/1000 | Loss: 0.00002284
Iteration 170/1000 | Loss: 0.00002284
Iteration 171/1000 | Loss: 0.00002283
Iteration 172/1000 | Loss: 0.00002283
Iteration 173/1000 | Loss: 0.00002283
Iteration 174/1000 | Loss: 0.00002283
Iteration 175/1000 | Loss: 0.00002283
Iteration 176/1000 | Loss: 0.00002283
Iteration 177/1000 | Loss: 0.00002283
Iteration 178/1000 | Loss: 0.00002283
Iteration 179/1000 | Loss: 0.00002283
Iteration 180/1000 | Loss: 0.00002283
Iteration 181/1000 | Loss: 0.00002283
Iteration 182/1000 | Loss: 0.00002283
Iteration 183/1000 | Loss: 0.00002283
Iteration 184/1000 | Loss: 0.00002283
Iteration 185/1000 | Loss: 0.00002283
Iteration 186/1000 | Loss: 0.00002283
Iteration 187/1000 | Loss: 0.00002283
Iteration 188/1000 | Loss: 0.00002283
Iteration 189/1000 | Loss: 0.00002283
Iteration 190/1000 | Loss: 0.00002283
Iteration 191/1000 | Loss: 0.00002283
Iteration 192/1000 | Loss: 0.00002283
Iteration 193/1000 | Loss: 0.00002283
Iteration 194/1000 | Loss: 0.00002283
Iteration 195/1000 | Loss: 0.00002283
Iteration 196/1000 | Loss: 0.00002283
Iteration 197/1000 | Loss: 0.00002283
Iteration 198/1000 | Loss: 0.00002283
Iteration 199/1000 | Loss: 0.00002283
Iteration 200/1000 | Loss: 0.00002283
Iteration 201/1000 | Loss: 0.00002283
Iteration 202/1000 | Loss: 0.00002283
Iteration 203/1000 | Loss: 0.00002283
Iteration 204/1000 | Loss: 0.00002283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.2833002731204033e-05, 2.2833002731204033e-05, 2.2833002731204033e-05, 2.2833002731204033e-05, 2.2833002731204033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2833002731204033e-05

Optimization complete. Final v2v error: 4.0872883796691895 mm

Highest mean error: 4.448420524597168 mm for frame 0

Lowest mean error: 3.8981211185455322 mm for frame 189

Saving results

Total time: 42.95341348648071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460380
Iteration 2/25 | Loss: 0.00148336
Iteration 3/25 | Loss: 0.00133012
Iteration 4/25 | Loss: 0.00131708
Iteration 5/25 | Loss: 0.00131435
Iteration 6/25 | Loss: 0.00131348
Iteration 7/25 | Loss: 0.00131337
Iteration 8/25 | Loss: 0.00131337
Iteration 9/25 | Loss: 0.00131337
Iteration 10/25 | Loss: 0.00131337
Iteration 11/25 | Loss: 0.00131337
Iteration 12/25 | Loss: 0.00131337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001313370419666171, 0.001313370419666171, 0.001313370419666171, 0.001313370419666171, 0.001313370419666171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001313370419666171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49953187
Iteration 2/25 | Loss: 0.00084445
Iteration 3/25 | Loss: 0.00084444
Iteration 4/25 | Loss: 0.00084443
Iteration 5/25 | Loss: 0.00084443
Iteration 6/25 | Loss: 0.00084443
Iteration 7/25 | Loss: 0.00084443
Iteration 8/25 | Loss: 0.00084443
Iteration 9/25 | Loss: 0.00084443
Iteration 10/25 | Loss: 0.00084443
Iteration 11/25 | Loss: 0.00084443
Iteration 12/25 | Loss: 0.00084443
Iteration 13/25 | Loss: 0.00084443
Iteration 14/25 | Loss: 0.00084443
Iteration 15/25 | Loss: 0.00084443
Iteration 16/25 | Loss: 0.00084443
Iteration 17/25 | Loss: 0.00084443
Iteration 18/25 | Loss: 0.00084443
Iteration 19/25 | Loss: 0.00084443
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008444323902949691, 0.0008444323902949691, 0.0008444323902949691, 0.0008444323902949691, 0.0008444323902949691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008444323902949691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084443
Iteration 2/1000 | Loss: 0.00004676
Iteration 3/1000 | Loss: 0.00002784
Iteration 4/1000 | Loss: 0.00002296
Iteration 5/1000 | Loss: 0.00002167
Iteration 6/1000 | Loss: 0.00002065
Iteration 7/1000 | Loss: 0.00002002
Iteration 8/1000 | Loss: 0.00001946
Iteration 9/1000 | Loss: 0.00001906
Iteration 10/1000 | Loss: 0.00001890
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001850
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001825
Iteration 15/1000 | Loss: 0.00001809
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001791
Iteration 20/1000 | Loss: 0.00001788
Iteration 21/1000 | Loss: 0.00001784
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001781
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001779
Iteration 35/1000 | Loss: 0.00001779
Iteration 36/1000 | Loss: 0.00001779
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001778
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001776
Iteration 44/1000 | Loss: 0.00001776
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001776
Iteration 47/1000 | Loss: 0.00001775
Iteration 48/1000 | Loss: 0.00001775
Iteration 49/1000 | Loss: 0.00001774
Iteration 50/1000 | Loss: 0.00001774
Iteration 51/1000 | Loss: 0.00001773
Iteration 52/1000 | Loss: 0.00001773
Iteration 53/1000 | Loss: 0.00001773
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001769
Iteration 57/1000 | Loss: 0.00001769
Iteration 58/1000 | Loss: 0.00001768
Iteration 59/1000 | Loss: 0.00001767
Iteration 60/1000 | Loss: 0.00001767
Iteration 61/1000 | Loss: 0.00001767
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001766
Iteration 64/1000 | Loss: 0.00001765
Iteration 65/1000 | Loss: 0.00001765
Iteration 66/1000 | Loss: 0.00001765
Iteration 67/1000 | Loss: 0.00001765
Iteration 68/1000 | Loss: 0.00001765
Iteration 69/1000 | Loss: 0.00001765
Iteration 70/1000 | Loss: 0.00001764
Iteration 71/1000 | Loss: 0.00001764
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001762
Iteration 77/1000 | Loss: 0.00001762
Iteration 78/1000 | Loss: 0.00001761
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001759
Iteration 85/1000 | Loss: 0.00001759
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001756
Iteration 95/1000 | Loss: 0.00001756
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001755
Iteration 99/1000 | Loss: 0.00001755
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001752
Iteration 111/1000 | Loss: 0.00001752
Iteration 112/1000 | Loss: 0.00001752
Iteration 113/1000 | Loss: 0.00001751
Iteration 114/1000 | Loss: 0.00001751
Iteration 115/1000 | Loss: 0.00001751
Iteration 116/1000 | Loss: 0.00001751
Iteration 117/1000 | Loss: 0.00001751
Iteration 118/1000 | Loss: 0.00001751
Iteration 119/1000 | Loss: 0.00001751
Iteration 120/1000 | Loss: 0.00001751
Iteration 121/1000 | Loss: 0.00001751
Iteration 122/1000 | Loss: 0.00001751
Iteration 123/1000 | Loss: 0.00001751
Iteration 124/1000 | Loss: 0.00001751
Iteration 125/1000 | Loss: 0.00001751
Iteration 126/1000 | Loss: 0.00001751
Iteration 127/1000 | Loss: 0.00001751
Iteration 128/1000 | Loss: 0.00001751
Iteration 129/1000 | Loss: 0.00001751
Iteration 130/1000 | Loss: 0.00001751
Iteration 131/1000 | Loss: 0.00001751
Iteration 132/1000 | Loss: 0.00001751
Iteration 133/1000 | Loss: 0.00001751
Iteration 134/1000 | Loss: 0.00001751
Iteration 135/1000 | Loss: 0.00001751
Iteration 136/1000 | Loss: 0.00001751
Iteration 137/1000 | Loss: 0.00001751
Iteration 138/1000 | Loss: 0.00001751
Iteration 139/1000 | Loss: 0.00001750
Iteration 140/1000 | Loss: 0.00001750
Iteration 141/1000 | Loss: 0.00001750
Iteration 142/1000 | Loss: 0.00001750
Iteration 143/1000 | Loss: 0.00001750
Iteration 144/1000 | Loss: 0.00001750
Iteration 145/1000 | Loss: 0.00001750
Iteration 146/1000 | Loss: 0.00001750
Iteration 147/1000 | Loss: 0.00001750
Iteration 148/1000 | Loss: 0.00001750
Iteration 149/1000 | Loss: 0.00001750
Iteration 150/1000 | Loss: 0.00001750
Iteration 151/1000 | Loss: 0.00001750
Iteration 152/1000 | Loss: 0.00001750
Iteration 153/1000 | Loss: 0.00001750
Iteration 154/1000 | Loss: 0.00001750
Iteration 155/1000 | Loss: 0.00001750
Iteration 156/1000 | Loss: 0.00001750
Iteration 157/1000 | Loss: 0.00001750
Iteration 158/1000 | Loss: 0.00001750
Iteration 159/1000 | Loss: 0.00001750
Iteration 160/1000 | Loss: 0.00001750
Iteration 161/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.7504724382888526e-05, 1.7504724382888526e-05, 1.7504724382888526e-05, 1.7504724382888526e-05, 1.7504724382888526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7504724382888526e-05

Optimization complete. Final v2v error: 3.4634063243865967 mm

Highest mean error: 3.9749698638916016 mm for frame 15

Lowest mean error: 2.7423009872436523 mm for frame 0

Saving results

Total time: 40.17501902580261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925477
Iteration 2/25 | Loss: 0.00277331
Iteration 3/25 | Loss: 0.00219151
Iteration 4/25 | Loss: 0.00186241
Iteration 5/25 | Loss: 0.00176263
Iteration 6/25 | Loss: 0.00240366
Iteration 7/25 | Loss: 0.00198983
Iteration 8/25 | Loss: 0.00172650
Iteration 9/25 | Loss: 0.00167404
Iteration 10/25 | Loss: 0.00164255
Iteration 11/25 | Loss: 0.00164626
Iteration 12/25 | Loss: 0.00165527
Iteration 13/25 | Loss: 0.00160348
Iteration 14/25 | Loss: 0.00154748
Iteration 15/25 | Loss: 0.00155654
Iteration 16/25 | Loss: 0.00163981
Iteration 17/25 | Loss: 0.00164412
Iteration 18/25 | Loss: 0.00151610
Iteration 19/25 | Loss: 0.00143394
Iteration 20/25 | Loss: 0.00138669
Iteration 21/25 | Loss: 0.00137003
Iteration 22/25 | Loss: 0.00136683
Iteration 23/25 | Loss: 0.00136789
Iteration 24/25 | Loss: 0.00136531
Iteration 25/25 | Loss: 0.00137773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41512036
Iteration 2/25 | Loss: 0.00076080
Iteration 3/25 | Loss: 0.00076080
Iteration 4/25 | Loss: 0.00076080
Iteration 5/25 | Loss: 0.00076080
Iteration 6/25 | Loss: 0.00076080
Iteration 7/25 | Loss: 0.00076080
Iteration 8/25 | Loss: 0.00076080
Iteration 9/25 | Loss: 0.00076080
Iteration 10/25 | Loss: 0.00076080
Iteration 11/25 | Loss: 0.00076080
Iteration 12/25 | Loss: 0.00076080
Iteration 13/25 | Loss: 0.00076080
Iteration 14/25 | Loss: 0.00076080
Iteration 15/25 | Loss: 0.00076080
Iteration 16/25 | Loss: 0.00076080
Iteration 17/25 | Loss: 0.00076080
Iteration 18/25 | Loss: 0.00076080
Iteration 19/25 | Loss: 0.00076080
Iteration 20/25 | Loss: 0.00076080
Iteration 21/25 | Loss: 0.00076080
Iteration 22/25 | Loss: 0.00076080
Iteration 23/25 | Loss: 0.00076080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007607993902638555, 0.0007607993902638555, 0.0007607993902638555, 0.0007607993902638555, 0.0007607993902638555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007607993902638555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076080
Iteration 2/1000 | Loss: 0.00007375
Iteration 3/1000 | Loss: 0.00005198
Iteration 4/1000 | Loss: 0.00004260
Iteration 5/1000 | Loss: 0.00003819
Iteration 6/1000 | Loss: 0.00003435
Iteration 7/1000 | Loss: 0.00003144
Iteration 8/1000 | Loss: 0.00003001
Iteration 9/1000 | Loss: 0.00002920
Iteration 10/1000 | Loss: 0.00002871
Iteration 11/1000 | Loss: 0.00002829
Iteration 12/1000 | Loss: 0.00002786
Iteration 13/1000 | Loss: 0.00002750
Iteration 14/1000 | Loss: 0.00004857
Iteration 15/1000 | Loss: 0.00087567
Iteration 16/1000 | Loss: 0.00005118
Iteration 17/1000 | Loss: 0.00003855
Iteration 18/1000 | Loss: 0.00003150
Iteration 19/1000 | Loss: 0.00002757
Iteration 20/1000 | Loss: 0.00002458
Iteration 21/1000 | Loss: 0.00002317
Iteration 22/1000 | Loss: 0.00002245
Iteration 23/1000 | Loss: 0.00002175
Iteration 24/1000 | Loss: 0.00002132
Iteration 25/1000 | Loss: 0.00002111
Iteration 26/1000 | Loss: 0.00002103
Iteration 27/1000 | Loss: 0.00002102
Iteration 28/1000 | Loss: 0.00002100
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002099
Iteration 31/1000 | Loss: 0.00002099
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002098
Iteration 34/1000 | Loss: 0.00002097
Iteration 35/1000 | Loss: 0.00002094
Iteration 36/1000 | Loss: 0.00002092
Iteration 37/1000 | Loss: 0.00002088
Iteration 38/1000 | Loss: 0.00002088
Iteration 39/1000 | Loss: 0.00002088
Iteration 40/1000 | Loss: 0.00002088
Iteration 41/1000 | Loss: 0.00002087
Iteration 42/1000 | Loss: 0.00002087
Iteration 43/1000 | Loss: 0.00002087
Iteration 44/1000 | Loss: 0.00002087
Iteration 45/1000 | Loss: 0.00002087
Iteration 46/1000 | Loss: 0.00002087
Iteration 47/1000 | Loss: 0.00002087
Iteration 48/1000 | Loss: 0.00002084
Iteration 49/1000 | Loss: 0.00002083
Iteration 50/1000 | Loss: 0.00002083
Iteration 51/1000 | Loss: 0.00002083
Iteration 52/1000 | Loss: 0.00002083
Iteration 53/1000 | Loss: 0.00002083
Iteration 54/1000 | Loss: 0.00002083
Iteration 55/1000 | Loss: 0.00002083
Iteration 56/1000 | Loss: 0.00002083
Iteration 57/1000 | Loss: 0.00002083
Iteration 58/1000 | Loss: 0.00002083
Iteration 59/1000 | Loss: 0.00002083
Iteration 60/1000 | Loss: 0.00002082
Iteration 61/1000 | Loss: 0.00002082
Iteration 62/1000 | Loss: 0.00002082
Iteration 63/1000 | Loss: 0.00002081
Iteration 64/1000 | Loss: 0.00002080
Iteration 65/1000 | Loss: 0.00002080
Iteration 66/1000 | Loss: 0.00002080
Iteration 67/1000 | Loss: 0.00002079
Iteration 68/1000 | Loss: 0.00002079
Iteration 69/1000 | Loss: 0.00002079
Iteration 70/1000 | Loss: 0.00002079
Iteration 71/1000 | Loss: 0.00002078
Iteration 72/1000 | Loss: 0.00002078
Iteration 73/1000 | Loss: 0.00002078
Iteration 74/1000 | Loss: 0.00002077
Iteration 75/1000 | Loss: 0.00002077
Iteration 76/1000 | Loss: 0.00002076
Iteration 77/1000 | Loss: 0.00002076
Iteration 78/1000 | Loss: 0.00002076
Iteration 79/1000 | Loss: 0.00002076
Iteration 80/1000 | Loss: 0.00002076
Iteration 81/1000 | Loss: 0.00002076
Iteration 82/1000 | Loss: 0.00002076
Iteration 83/1000 | Loss: 0.00002076
Iteration 84/1000 | Loss: 0.00002075
Iteration 85/1000 | Loss: 0.00002075
Iteration 86/1000 | Loss: 0.00002075
Iteration 87/1000 | Loss: 0.00002074
Iteration 88/1000 | Loss: 0.00002074
Iteration 89/1000 | Loss: 0.00002074
Iteration 90/1000 | Loss: 0.00002074
Iteration 91/1000 | Loss: 0.00002074
Iteration 92/1000 | Loss: 0.00002073
Iteration 93/1000 | Loss: 0.00002073
Iteration 94/1000 | Loss: 0.00002073
Iteration 95/1000 | Loss: 0.00002073
Iteration 96/1000 | Loss: 0.00002073
Iteration 97/1000 | Loss: 0.00002073
Iteration 98/1000 | Loss: 0.00002073
Iteration 99/1000 | Loss: 0.00002073
Iteration 100/1000 | Loss: 0.00002073
Iteration 101/1000 | Loss: 0.00002073
Iteration 102/1000 | Loss: 0.00002073
Iteration 103/1000 | Loss: 0.00002073
Iteration 104/1000 | Loss: 0.00002072
Iteration 105/1000 | Loss: 0.00002072
Iteration 106/1000 | Loss: 0.00002072
Iteration 107/1000 | Loss: 0.00002072
Iteration 108/1000 | Loss: 0.00002072
Iteration 109/1000 | Loss: 0.00002072
Iteration 110/1000 | Loss: 0.00002072
Iteration 111/1000 | Loss: 0.00002071
Iteration 112/1000 | Loss: 0.00002071
Iteration 113/1000 | Loss: 0.00002071
Iteration 114/1000 | Loss: 0.00002070
Iteration 115/1000 | Loss: 0.00002070
Iteration 116/1000 | Loss: 0.00002069
Iteration 117/1000 | Loss: 0.00002069
Iteration 118/1000 | Loss: 0.00002069
Iteration 119/1000 | Loss: 0.00002069
Iteration 120/1000 | Loss: 0.00002069
Iteration 121/1000 | Loss: 0.00002068
Iteration 122/1000 | Loss: 0.00002068
Iteration 123/1000 | Loss: 0.00002068
Iteration 124/1000 | Loss: 0.00002068
Iteration 125/1000 | Loss: 0.00002068
Iteration 126/1000 | Loss: 0.00002068
Iteration 127/1000 | Loss: 0.00002068
Iteration 128/1000 | Loss: 0.00002068
Iteration 129/1000 | Loss: 0.00002067
Iteration 130/1000 | Loss: 0.00002067
Iteration 131/1000 | Loss: 0.00002067
Iteration 132/1000 | Loss: 0.00002067
Iteration 133/1000 | Loss: 0.00002067
Iteration 134/1000 | Loss: 0.00002066
Iteration 135/1000 | Loss: 0.00002066
Iteration 136/1000 | Loss: 0.00002066
Iteration 137/1000 | Loss: 0.00002066
Iteration 138/1000 | Loss: 0.00002066
Iteration 139/1000 | Loss: 0.00002066
Iteration 140/1000 | Loss: 0.00002066
Iteration 141/1000 | Loss: 0.00002066
Iteration 142/1000 | Loss: 0.00002065
Iteration 143/1000 | Loss: 0.00002065
Iteration 144/1000 | Loss: 0.00002065
Iteration 145/1000 | Loss: 0.00002065
Iteration 146/1000 | Loss: 0.00002065
Iteration 147/1000 | Loss: 0.00002065
Iteration 148/1000 | Loss: 0.00002064
Iteration 149/1000 | Loss: 0.00002064
Iteration 150/1000 | Loss: 0.00002064
Iteration 151/1000 | Loss: 0.00002063
Iteration 152/1000 | Loss: 0.00002063
Iteration 153/1000 | Loss: 0.00002063
Iteration 154/1000 | Loss: 0.00002063
Iteration 155/1000 | Loss: 0.00002062
Iteration 156/1000 | Loss: 0.00002062
Iteration 157/1000 | Loss: 0.00002062
Iteration 158/1000 | Loss: 0.00002062
Iteration 159/1000 | Loss: 0.00002062
Iteration 160/1000 | Loss: 0.00002062
Iteration 161/1000 | Loss: 0.00002062
Iteration 162/1000 | Loss: 0.00002062
Iteration 163/1000 | Loss: 0.00002062
Iteration 164/1000 | Loss: 0.00002061
Iteration 165/1000 | Loss: 0.00002061
Iteration 166/1000 | Loss: 0.00002061
Iteration 167/1000 | Loss: 0.00002061
Iteration 168/1000 | Loss: 0.00002061
Iteration 169/1000 | Loss: 0.00002061
Iteration 170/1000 | Loss: 0.00002061
Iteration 171/1000 | Loss: 0.00002061
Iteration 172/1000 | Loss: 0.00002061
Iteration 173/1000 | Loss: 0.00002061
Iteration 174/1000 | Loss: 0.00002061
Iteration 175/1000 | Loss: 0.00002061
Iteration 176/1000 | Loss: 0.00002061
Iteration 177/1000 | Loss: 0.00002061
Iteration 178/1000 | Loss: 0.00002061
Iteration 179/1000 | Loss: 0.00002060
Iteration 180/1000 | Loss: 0.00002060
Iteration 181/1000 | Loss: 0.00002060
Iteration 182/1000 | Loss: 0.00002060
Iteration 183/1000 | Loss: 0.00002060
Iteration 184/1000 | Loss: 0.00002060
Iteration 185/1000 | Loss: 0.00002060
Iteration 186/1000 | Loss: 0.00002060
Iteration 187/1000 | Loss: 0.00002060
Iteration 188/1000 | Loss: 0.00002060
Iteration 189/1000 | Loss: 0.00002060
Iteration 190/1000 | Loss: 0.00002060
Iteration 191/1000 | Loss: 0.00002060
Iteration 192/1000 | Loss: 0.00002060
Iteration 193/1000 | Loss: 0.00002060
Iteration 194/1000 | Loss: 0.00002060
Iteration 195/1000 | Loss: 0.00002060
Iteration 196/1000 | Loss: 0.00002060
Iteration 197/1000 | Loss: 0.00002059
Iteration 198/1000 | Loss: 0.00002059
Iteration 199/1000 | Loss: 0.00002059
Iteration 200/1000 | Loss: 0.00002059
Iteration 201/1000 | Loss: 0.00002059
Iteration 202/1000 | Loss: 0.00002059
Iteration 203/1000 | Loss: 0.00002059
Iteration 204/1000 | Loss: 0.00002059
Iteration 205/1000 | Loss: 0.00002059
Iteration 206/1000 | Loss: 0.00002059
Iteration 207/1000 | Loss: 0.00002059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [2.0594865418388508e-05, 2.0594865418388508e-05, 2.0594865418388508e-05, 2.0594865418388508e-05, 2.0594865418388508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0594865418388508e-05

Optimization complete. Final v2v error: 3.7559237480163574 mm

Highest mean error: 5.905150890350342 mm for frame 70

Lowest mean error: 3.6283655166625977 mm for frame 42

Saving results

Total time: 91.84643077850342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987097
Iteration 2/25 | Loss: 0.00198432
Iteration 3/25 | Loss: 0.00158018
Iteration 4/25 | Loss: 0.00149553
Iteration 5/25 | Loss: 0.00150456
Iteration 6/25 | Loss: 0.00152280
Iteration 7/25 | Loss: 0.00144134
Iteration 8/25 | Loss: 0.00142171
Iteration 9/25 | Loss: 0.00140661
Iteration 10/25 | Loss: 0.00140944
Iteration 11/25 | Loss: 0.00140124
Iteration 12/25 | Loss: 0.00140524
Iteration 13/25 | Loss: 0.00139782
Iteration 14/25 | Loss: 0.00139091
Iteration 15/25 | Loss: 0.00138898
Iteration 16/25 | Loss: 0.00138681
Iteration 17/25 | Loss: 0.00138512
Iteration 18/25 | Loss: 0.00138448
Iteration 19/25 | Loss: 0.00138423
Iteration 20/25 | Loss: 0.00138418
Iteration 21/25 | Loss: 0.00138418
Iteration 22/25 | Loss: 0.00138418
Iteration 23/25 | Loss: 0.00138418
Iteration 24/25 | Loss: 0.00138417
Iteration 25/25 | Loss: 0.00138417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97065163
Iteration 2/25 | Loss: 0.00089371
Iteration 3/25 | Loss: 0.00089371
Iteration 4/25 | Loss: 0.00089371
Iteration 5/25 | Loss: 0.00089370
Iteration 6/25 | Loss: 0.00089370
Iteration 7/25 | Loss: 0.00089370
Iteration 8/25 | Loss: 0.00089370
Iteration 9/25 | Loss: 0.00089370
Iteration 10/25 | Loss: 0.00089370
Iteration 11/25 | Loss: 0.00089370
Iteration 12/25 | Loss: 0.00089370
Iteration 13/25 | Loss: 0.00089370
Iteration 14/25 | Loss: 0.00089370
Iteration 15/25 | Loss: 0.00089370
Iteration 16/25 | Loss: 0.00089370
Iteration 17/25 | Loss: 0.00089370
Iteration 18/25 | Loss: 0.00089370
Iteration 19/25 | Loss: 0.00089370
Iteration 20/25 | Loss: 0.00089370
Iteration 21/25 | Loss: 0.00089370
Iteration 22/25 | Loss: 0.00089370
Iteration 23/25 | Loss: 0.00089370
Iteration 24/25 | Loss: 0.00089370
Iteration 25/25 | Loss: 0.00089370

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089370
Iteration 2/1000 | Loss: 0.00004838
Iteration 3/1000 | Loss: 0.00003363
Iteration 4/1000 | Loss: 0.00003091
Iteration 5/1000 | Loss: 0.00002990
Iteration 6/1000 | Loss: 0.00002923
Iteration 7/1000 | Loss: 0.00002868
Iteration 8/1000 | Loss: 0.00002801
Iteration 9/1000 | Loss: 0.00002760
Iteration 10/1000 | Loss: 0.00002718
Iteration 11/1000 | Loss: 0.00002687
Iteration 12/1000 | Loss: 0.00002666
Iteration 13/1000 | Loss: 0.00002643
Iteration 14/1000 | Loss: 0.00002626
Iteration 15/1000 | Loss: 0.00002623
Iteration 16/1000 | Loss: 0.00002620
Iteration 17/1000 | Loss: 0.00002616
Iteration 18/1000 | Loss: 0.00002613
Iteration 19/1000 | Loss: 0.00002612
Iteration 20/1000 | Loss: 0.00002611
Iteration 21/1000 | Loss: 0.00002607
Iteration 22/1000 | Loss: 0.00002604
Iteration 23/1000 | Loss: 0.00002601
Iteration 24/1000 | Loss: 0.00002600
Iteration 25/1000 | Loss: 0.00002600
Iteration 26/1000 | Loss: 0.00002599
Iteration 27/1000 | Loss: 0.00002599
Iteration 28/1000 | Loss: 0.00002593
Iteration 29/1000 | Loss: 0.00002593
Iteration 30/1000 | Loss: 0.00002591
Iteration 31/1000 | Loss: 0.00002590
Iteration 32/1000 | Loss: 0.00002589
Iteration 33/1000 | Loss: 0.00002589
Iteration 34/1000 | Loss: 0.00002589
Iteration 35/1000 | Loss: 0.00002589
Iteration 36/1000 | Loss: 0.00002589
Iteration 37/1000 | Loss: 0.00002589
Iteration 38/1000 | Loss: 0.00002589
Iteration 39/1000 | Loss: 0.00002589
Iteration 40/1000 | Loss: 0.00002589
Iteration 41/1000 | Loss: 0.00002588
Iteration 42/1000 | Loss: 0.00002588
Iteration 43/1000 | Loss: 0.00002588
Iteration 44/1000 | Loss: 0.00002588
Iteration 45/1000 | Loss: 0.00002588
Iteration 46/1000 | Loss: 0.00002588
Iteration 47/1000 | Loss: 0.00002588
Iteration 48/1000 | Loss: 0.00002588
Iteration 49/1000 | Loss: 0.00002588
Iteration 50/1000 | Loss: 0.00002588
Iteration 51/1000 | Loss: 0.00002587
Iteration 52/1000 | Loss: 0.00002587
Iteration 53/1000 | Loss: 0.00002586
Iteration 54/1000 | Loss: 0.00002585
Iteration 55/1000 | Loss: 0.00002585
Iteration 56/1000 | Loss: 0.00002585
Iteration 57/1000 | Loss: 0.00002584
Iteration 58/1000 | Loss: 0.00002584
Iteration 59/1000 | Loss: 0.00002584
Iteration 60/1000 | Loss: 0.00002584
Iteration 61/1000 | Loss: 0.00002584
Iteration 62/1000 | Loss: 0.00002584
Iteration 63/1000 | Loss: 0.00002584
Iteration 64/1000 | Loss: 0.00002584
Iteration 65/1000 | Loss: 0.00002584
Iteration 66/1000 | Loss: 0.00002584
Iteration 67/1000 | Loss: 0.00002584
Iteration 68/1000 | Loss: 0.00002583
Iteration 69/1000 | Loss: 0.00002583
Iteration 70/1000 | Loss: 0.00002583
Iteration 71/1000 | Loss: 0.00002581
Iteration 72/1000 | Loss: 0.00002581
Iteration 73/1000 | Loss: 0.00002581
Iteration 74/1000 | Loss: 0.00002581
Iteration 75/1000 | Loss: 0.00002581
Iteration 76/1000 | Loss: 0.00002580
Iteration 77/1000 | Loss: 0.00002580
Iteration 78/1000 | Loss: 0.00002580
Iteration 79/1000 | Loss: 0.00002580
Iteration 80/1000 | Loss: 0.00002579
Iteration 81/1000 | Loss: 0.00002579
Iteration 82/1000 | Loss: 0.00002579
Iteration 83/1000 | Loss: 0.00002579
Iteration 84/1000 | Loss: 0.00002579
Iteration 85/1000 | Loss: 0.00002578
Iteration 86/1000 | Loss: 0.00002578
Iteration 87/1000 | Loss: 0.00002578
Iteration 88/1000 | Loss: 0.00002577
Iteration 89/1000 | Loss: 0.00002577
Iteration 90/1000 | Loss: 0.00002577
Iteration 91/1000 | Loss: 0.00002576
Iteration 92/1000 | Loss: 0.00002576
Iteration 93/1000 | Loss: 0.00002576
Iteration 94/1000 | Loss: 0.00002576
Iteration 95/1000 | Loss: 0.00002576
Iteration 96/1000 | Loss: 0.00002576
Iteration 97/1000 | Loss: 0.00002576
Iteration 98/1000 | Loss: 0.00002575
Iteration 99/1000 | Loss: 0.00002575
Iteration 100/1000 | Loss: 0.00002575
Iteration 101/1000 | Loss: 0.00002575
Iteration 102/1000 | Loss: 0.00002575
Iteration 103/1000 | Loss: 0.00002574
Iteration 104/1000 | Loss: 0.00002574
Iteration 105/1000 | Loss: 0.00002574
Iteration 106/1000 | Loss: 0.00002574
Iteration 107/1000 | Loss: 0.00002573
Iteration 108/1000 | Loss: 0.00002573
Iteration 109/1000 | Loss: 0.00002572
Iteration 110/1000 | Loss: 0.00002572
Iteration 111/1000 | Loss: 0.00002572
Iteration 112/1000 | Loss: 0.00002571
Iteration 113/1000 | Loss: 0.00002571
Iteration 114/1000 | Loss: 0.00002571
Iteration 115/1000 | Loss: 0.00002570
Iteration 116/1000 | Loss: 0.00002570
Iteration 117/1000 | Loss: 0.00002570
Iteration 118/1000 | Loss: 0.00002569
Iteration 119/1000 | Loss: 0.00002569
Iteration 120/1000 | Loss: 0.00002569
Iteration 121/1000 | Loss: 0.00002568
Iteration 122/1000 | Loss: 0.00002568
Iteration 123/1000 | Loss: 0.00002568
Iteration 124/1000 | Loss: 0.00002568
Iteration 125/1000 | Loss: 0.00002568
Iteration 126/1000 | Loss: 0.00002567
Iteration 127/1000 | Loss: 0.00002567
Iteration 128/1000 | Loss: 0.00002567
Iteration 129/1000 | Loss: 0.00002566
Iteration 130/1000 | Loss: 0.00002566
Iteration 131/1000 | Loss: 0.00002566
Iteration 132/1000 | Loss: 0.00002566
Iteration 133/1000 | Loss: 0.00002566
Iteration 134/1000 | Loss: 0.00002565
Iteration 135/1000 | Loss: 0.00002565
Iteration 136/1000 | Loss: 0.00002565
Iteration 137/1000 | Loss: 0.00002565
Iteration 138/1000 | Loss: 0.00002565
Iteration 139/1000 | Loss: 0.00002565
Iteration 140/1000 | Loss: 0.00002565
Iteration 141/1000 | Loss: 0.00002565
Iteration 142/1000 | Loss: 0.00002565
Iteration 143/1000 | Loss: 0.00002565
Iteration 144/1000 | Loss: 0.00002565
Iteration 145/1000 | Loss: 0.00002565
Iteration 146/1000 | Loss: 0.00002565
Iteration 147/1000 | Loss: 0.00002565
Iteration 148/1000 | Loss: 0.00002565
Iteration 149/1000 | Loss: 0.00002564
Iteration 150/1000 | Loss: 0.00002564
Iteration 151/1000 | Loss: 0.00002564
Iteration 152/1000 | Loss: 0.00002564
Iteration 153/1000 | Loss: 0.00002563
Iteration 154/1000 | Loss: 0.00002563
Iteration 155/1000 | Loss: 0.00002563
Iteration 156/1000 | Loss: 0.00002563
Iteration 157/1000 | Loss: 0.00002563
Iteration 158/1000 | Loss: 0.00002562
Iteration 159/1000 | Loss: 0.00002562
Iteration 160/1000 | Loss: 0.00002562
Iteration 161/1000 | Loss: 0.00002562
Iteration 162/1000 | Loss: 0.00002562
Iteration 163/1000 | Loss: 0.00002562
Iteration 164/1000 | Loss: 0.00002562
Iteration 165/1000 | Loss: 0.00002562
Iteration 166/1000 | Loss: 0.00002561
Iteration 167/1000 | Loss: 0.00002561
Iteration 168/1000 | Loss: 0.00002561
Iteration 169/1000 | Loss: 0.00002561
Iteration 170/1000 | Loss: 0.00002561
Iteration 171/1000 | Loss: 0.00002561
Iteration 172/1000 | Loss: 0.00002561
Iteration 173/1000 | Loss: 0.00002561
Iteration 174/1000 | Loss: 0.00002561
Iteration 175/1000 | Loss: 0.00002561
Iteration 176/1000 | Loss: 0.00002561
Iteration 177/1000 | Loss: 0.00002561
Iteration 178/1000 | Loss: 0.00002561
Iteration 179/1000 | Loss: 0.00002561
Iteration 180/1000 | Loss: 0.00002561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.560777465987485e-05, 2.560777465987485e-05, 2.560777465987485e-05, 2.560777465987485e-05, 2.560777465987485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.560777465987485e-05

Optimization complete. Final v2v error: 4.0307488441467285 mm

Highest mean error: 6.05272102355957 mm for frame 103

Lowest mean error: 3.554990530014038 mm for frame 182

Saving results

Total time: 69.84209847450256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510240
Iteration 2/25 | Loss: 0.00141367
Iteration 3/25 | Loss: 0.00132631
Iteration 4/25 | Loss: 0.00131178
Iteration 5/25 | Loss: 0.00130659
Iteration 6/25 | Loss: 0.00130633
Iteration 7/25 | Loss: 0.00130633
Iteration 8/25 | Loss: 0.00130633
Iteration 9/25 | Loss: 0.00130633
Iteration 10/25 | Loss: 0.00130633
Iteration 11/25 | Loss: 0.00130633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001306328922510147, 0.001306328922510147, 0.001306328922510147, 0.001306328922510147, 0.001306328922510147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001306328922510147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86822474
Iteration 2/25 | Loss: 0.00079255
Iteration 3/25 | Loss: 0.00079255
Iteration 4/25 | Loss: 0.00079255
Iteration 5/25 | Loss: 0.00079255
Iteration 6/25 | Loss: 0.00079254
Iteration 7/25 | Loss: 0.00079254
Iteration 8/25 | Loss: 0.00079254
Iteration 9/25 | Loss: 0.00079254
Iteration 10/25 | Loss: 0.00079254
Iteration 11/25 | Loss: 0.00079254
Iteration 12/25 | Loss: 0.00079254
Iteration 13/25 | Loss: 0.00079254
Iteration 14/25 | Loss: 0.00079254
Iteration 15/25 | Loss: 0.00079254
Iteration 16/25 | Loss: 0.00079254
Iteration 17/25 | Loss: 0.00079254
Iteration 18/25 | Loss: 0.00079254
Iteration 19/25 | Loss: 0.00079254
Iteration 20/25 | Loss: 0.00079254
Iteration 21/25 | Loss: 0.00079254
Iteration 22/25 | Loss: 0.00079254
Iteration 23/25 | Loss: 0.00079254
Iteration 24/25 | Loss: 0.00079254
Iteration 25/25 | Loss: 0.00079254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079254
Iteration 2/1000 | Loss: 0.00004209
Iteration 3/1000 | Loss: 0.00002565
Iteration 4/1000 | Loss: 0.00002352
Iteration 5/1000 | Loss: 0.00002239
Iteration 6/1000 | Loss: 0.00002154
Iteration 7/1000 | Loss: 0.00002094
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002021
Iteration 10/1000 | Loss: 0.00001992
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001933
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001894
Iteration 15/1000 | Loss: 0.00001874
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001859
Iteration 19/1000 | Loss: 0.00001853
Iteration 20/1000 | Loss: 0.00001841
Iteration 21/1000 | Loss: 0.00001841
Iteration 22/1000 | Loss: 0.00001838
Iteration 23/1000 | Loss: 0.00001832
Iteration 24/1000 | Loss: 0.00001831
Iteration 25/1000 | Loss: 0.00001828
Iteration 26/1000 | Loss: 0.00001827
Iteration 27/1000 | Loss: 0.00001827
Iteration 28/1000 | Loss: 0.00001826
Iteration 29/1000 | Loss: 0.00001826
Iteration 30/1000 | Loss: 0.00001824
Iteration 31/1000 | Loss: 0.00001824
Iteration 32/1000 | Loss: 0.00001823
Iteration 33/1000 | Loss: 0.00001822
Iteration 34/1000 | Loss: 0.00001822
Iteration 35/1000 | Loss: 0.00001822
Iteration 36/1000 | Loss: 0.00001819
Iteration 37/1000 | Loss: 0.00001819
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001818
Iteration 44/1000 | Loss: 0.00001818
Iteration 45/1000 | Loss: 0.00001818
Iteration 46/1000 | Loss: 0.00001818
Iteration 47/1000 | Loss: 0.00001817
Iteration 48/1000 | Loss: 0.00001817
Iteration 49/1000 | Loss: 0.00001817
Iteration 50/1000 | Loss: 0.00001816
Iteration 51/1000 | Loss: 0.00001816
Iteration 52/1000 | Loss: 0.00001816
Iteration 53/1000 | Loss: 0.00001816
Iteration 54/1000 | Loss: 0.00001816
Iteration 55/1000 | Loss: 0.00001816
Iteration 56/1000 | Loss: 0.00001816
Iteration 57/1000 | Loss: 0.00001815
Iteration 58/1000 | Loss: 0.00001815
Iteration 59/1000 | Loss: 0.00001815
Iteration 60/1000 | Loss: 0.00001815
Iteration 61/1000 | Loss: 0.00001815
Iteration 62/1000 | Loss: 0.00001815
Iteration 63/1000 | Loss: 0.00001815
Iteration 64/1000 | Loss: 0.00001815
Iteration 65/1000 | Loss: 0.00001814
Iteration 66/1000 | Loss: 0.00001814
Iteration 67/1000 | Loss: 0.00001814
Iteration 68/1000 | Loss: 0.00001814
Iteration 69/1000 | Loss: 0.00001814
Iteration 70/1000 | Loss: 0.00001813
Iteration 71/1000 | Loss: 0.00001813
Iteration 72/1000 | Loss: 0.00001813
Iteration 73/1000 | Loss: 0.00001812
Iteration 74/1000 | Loss: 0.00001812
Iteration 75/1000 | Loss: 0.00001812
Iteration 76/1000 | Loss: 0.00001812
Iteration 77/1000 | Loss: 0.00001812
Iteration 78/1000 | Loss: 0.00001812
Iteration 79/1000 | Loss: 0.00001812
Iteration 80/1000 | Loss: 0.00001812
Iteration 81/1000 | Loss: 0.00001811
Iteration 82/1000 | Loss: 0.00001811
Iteration 83/1000 | Loss: 0.00001811
Iteration 84/1000 | Loss: 0.00001811
Iteration 85/1000 | Loss: 0.00001810
Iteration 86/1000 | Loss: 0.00001810
Iteration 87/1000 | Loss: 0.00001810
Iteration 88/1000 | Loss: 0.00001809
Iteration 89/1000 | Loss: 0.00001809
Iteration 90/1000 | Loss: 0.00001809
Iteration 91/1000 | Loss: 0.00001809
Iteration 92/1000 | Loss: 0.00001809
Iteration 93/1000 | Loss: 0.00001809
Iteration 94/1000 | Loss: 0.00001809
Iteration 95/1000 | Loss: 0.00001808
Iteration 96/1000 | Loss: 0.00001808
Iteration 97/1000 | Loss: 0.00001808
Iteration 98/1000 | Loss: 0.00001808
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001806
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001805
Iteration 107/1000 | Loss: 0.00001805
Iteration 108/1000 | Loss: 0.00001805
Iteration 109/1000 | Loss: 0.00001805
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001805
Iteration 117/1000 | Loss: 0.00001805
Iteration 118/1000 | Loss: 0.00001805
Iteration 119/1000 | Loss: 0.00001805
Iteration 120/1000 | Loss: 0.00001805
Iteration 121/1000 | Loss: 0.00001804
Iteration 122/1000 | Loss: 0.00001804
Iteration 123/1000 | Loss: 0.00001804
Iteration 124/1000 | Loss: 0.00001804
Iteration 125/1000 | Loss: 0.00001804
Iteration 126/1000 | Loss: 0.00001804
Iteration 127/1000 | Loss: 0.00001804
Iteration 128/1000 | Loss: 0.00001804
Iteration 129/1000 | Loss: 0.00001804
Iteration 130/1000 | Loss: 0.00001804
Iteration 131/1000 | Loss: 0.00001804
Iteration 132/1000 | Loss: 0.00001804
Iteration 133/1000 | Loss: 0.00001804
Iteration 134/1000 | Loss: 0.00001804
Iteration 135/1000 | Loss: 0.00001804
Iteration 136/1000 | Loss: 0.00001803
Iteration 137/1000 | Loss: 0.00001803
Iteration 138/1000 | Loss: 0.00001803
Iteration 139/1000 | Loss: 0.00001803
Iteration 140/1000 | Loss: 0.00001803
Iteration 141/1000 | Loss: 0.00001803
Iteration 142/1000 | Loss: 0.00001802
Iteration 143/1000 | Loss: 0.00001802
Iteration 144/1000 | Loss: 0.00001802
Iteration 145/1000 | Loss: 0.00001802
Iteration 146/1000 | Loss: 0.00001801
Iteration 147/1000 | Loss: 0.00001801
Iteration 148/1000 | Loss: 0.00001801
Iteration 149/1000 | Loss: 0.00001801
Iteration 150/1000 | Loss: 0.00001801
Iteration 151/1000 | Loss: 0.00001801
Iteration 152/1000 | Loss: 0.00001800
Iteration 153/1000 | Loss: 0.00001800
Iteration 154/1000 | Loss: 0.00001800
Iteration 155/1000 | Loss: 0.00001800
Iteration 156/1000 | Loss: 0.00001800
Iteration 157/1000 | Loss: 0.00001799
Iteration 158/1000 | Loss: 0.00001799
Iteration 159/1000 | Loss: 0.00001799
Iteration 160/1000 | Loss: 0.00001799
Iteration 161/1000 | Loss: 0.00001799
Iteration 162/1000 | Loss: 0.00001799
Iteration 163/1000 | Loss: 0.00001798
Iteration 164/1000 | Loss: 0.00001798
Iteration 165/1000 | Loss: 0.00001798
Iteration 166/1000 | Loss: 0.00001798
Iteration 167/1000 | Loss: 0.00001798
Iteration 168/1000 | Loss: 0.00001797
Iteration 169/1000 | Loss: 0.00001797
Iteration 170/1000 | Loss: 0.00001797
Iteration 171/1000 | Loss: 0.00001797
Iteration 172/1000 | Loss: 0.00001797
Iteration 173/1000 | Loss: 0.00001797
Iteration 174/1000 | Loss: 0.00001797
Iteration 175/1000 | Loss: 0.00001797
Iteration 176/1000 | Loss: 0.00001797
Iteration 177/1000 | Loss: 0.00001797
Iteration 178/1000 | Loss: 0.00001797
Iteration 179/1000 | Loss: 0.00001797
Iteration 180/1000 | Loss: 0.00001796
Iteration 181/1000 | Loss: 0.00001796
Iteration 182/1000 | Loss: 0.00001796
Iteration 183/1000 | Loss: 0.00001796
Iteration 184/1000 | Loss: 0.00001796
Iteration 185/1000 | Loss: 0.00001796
Iteration 186/1000 | Loss: 0.00001796
Iteration 187/1000 | Loss: 0.00001796
Iteration 188/1000 | Loss: 0.00001796
Iteration 189/1000 | Loss: 0.00001796
Iteration 190/1000 | Loss: 0.00001796
Iteration 191/1000 | Loss: 0.00001796
Iteration 192/1000 | Loss: 0.00001796
Iteration 193/1000 | Loss: 0.00001796
Iteration 194/1000 | Loss: 0.00001796
Iteration 195/1000 | Loss: 0.00001796
Iteration 196/1000 | Loss: 0.00001796
Iteration 197/1000 | Loss: 0.00001796
Iteration 198/1000 | Loss: 0.00001796
Iteration 199/1000 | Loss: 0.00001796
Iteration 200/1000 | Loss: 0.00001795
Iteration 201/1000 | Loss: 0.00001795
Iteration 202/1000 | Loss: 0.00001795
Iteration 203/1000 | Loss: 0.00001795
Iteration 204/1000 | Loss: 0.00001795
Iteration 205/1000 | Loss: 0.00001795
Iteration 206/1000 | Loss: 0.00001795
Iteration 207/1000 | Loss: 0.00001795
Iteration 208/1000 | Loss: 0.00001795
Iteration 209/1000 | Loss: 0.00001795
Iteration 210/1000 | Loss: 0.00001795
Iteration 211/1000 | Loss: 0.00001795
Iteration 212/1000 | Loss: 0.00001795
Iteration 213/1000 | Loss: 0.00001795
Iteration 214/1000 | Loss: 0.00001795
Iteration 215/1000 | Loss: 0.00001795
Iteration 216/1000 | Loss: 0.00001795
Iteration 217/1000 | Loss: 0.00001795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.79464987013489e-05, 1.79464987013489e-05, 1.79464987013489e-05, 1.79464987013489e-05, 1.79464987013489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.79464987013489e-05

Optimization complete. Final v2v error: 3.548567056655884 mm

Highest mean error: 3.9281396865844727 mm for frame 0

Lowest mean error: 3.502408504486084 mm for frame 266

Saving results

Total time: 54.614505767822266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491507
Iteration 2/25 | Loss: 0.00152816
Iteration 3/25 | Loss: 0.00135767
Iteration 4/25 | Loss: 0.00134071
Iteration 5/25 | Loss: 0.00133735
Iteration 6/25 | Loss: 0.00133669
Iteration 7/25 | Loss: 0.00133669
Iteration 8/25 | Loss: 0.00133669
Iteration 9/25 | Loss: 0.00133669
Iteration 10/25 | Loss: 0.00133669
Iteration 11/25 | Loss: 0.00133669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013366888742893934, 0.0013366888742893934, 0.0013366888742893934, 0.0013366888742893934, 0.0013366888742893934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013366888742893934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53031361
Iteration 2/25 | Loss: 0.00091436
Iteration 3/25 | Loss: 0.00091432
Iteration 4/25 | Loss: 0.00091432
Iteration 5/25 | Loss: 0.00091432
Iteration 6/25 | Loss: 0.00091432
Iteration 7/25 | Loss: 0.00091432
Iteration 8/25 | Loss: 0.00091432
Iteration 9/25 | Loss: 0.00091432
Iteration 10/25 | Loss: 0.00091432
Iteration 11/25 | Loss: 0.00091432
Iteration 12/25 | Loss: 0.00091432
Iteration 13/25 | Loss: 0.00091432
Iteration 14/25 | Loss: 0.00091432
Iteration 15/25 | Loss: 0.00091432
Iteration 16/25 | Loss: 0.00091432
Iteration 17/25 | Loss: 0.00091432
Iteration 18/25 | Loss: 0.00091432
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009143175557255745, 0.0009143175557255745, 0.0009143175557255745, 0.0009143175557255745, 0.0009143175557255745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009143175557255745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091432
Iteration 2/1000 | Loss: 0.00004328
Iteration 3/1000 | Loss: 0.00003039
Iteration 4/1000 | Loss: 0.00002452
Iteration 5/1000 | Loss: 0.00002246
Iteration 6/1000 | Loss: 0.00002146
Iteration 7/1000 | Loss: 0.00002058
Iteration 8/1000 | Loss: 0.00001986
Iteration 9/1000 | Loss: 0.00001932
Iteration 10/1000 | Loss: 0.00001892
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001822
Iteration 14/1000 | Loss: 0.00001821
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001820
Iteration 17/1000 | Loss: 0.00001815
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001806
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001802
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001801
Iteration 28/1000 | Loss: 0.00001801
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001801
Iteration 33/1000 | Loss: 0.00001800
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001800
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001798
Iteration 41/1000 | Loss: 0.00001798
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001795
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001795
Iteration 46/1000 | Loss: 0.00001795
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001795
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001795
Iteration 51/1000 | Loss: 0.00001795
Iteration 52/1000 | Loss: 0.00001795
Iteration 53/1000 | Loss: 0.00001795
Iteration 54/1000 | Loss: 0.00001794
Iteration 55/1000 | Loss: 0.00001794
Iteration 56/1000 | Loss: 0.00001794
Iteration 57/1000 | Loss: 0.00001794
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001793
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001793
Iteration 64/1000 | Loss: 0.00001793
Iteration 65/1000 | Loss: 0.00001793
Iteration 66/1000 | Loss: 0.00001792
Iteration 67/1000 | Loss: 0.00001792
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001792
Iteration 70/1000 | Loss: 0.00001792
Iteration 71/1000 | Loss: 0.00001792
Iteration 72/1000 | Loss: 0.00001792
Iteration 73/1000 | Loss: 0.00001792
Iteration 74/1000 | Loss: 0.00001792
Iteration 75/1000 | Loss: 0.00001792
Iteration 76/1000 | Loss: 0.00001792
Iteration 77/1000 | Loss: 0.00001792
Iteration 78/1000 | Loss: 0.00001792
Iteration 79/1000 | Loss: 0.00001791
Iteration 80/1000 | Loss: 0.00001791
Iteration 81/1000 | Loss: 0.00001791
Iteration 82/1000 | Loss: 0.00001790
Iteration 83/1000 | Loss: 0.00001790
Iteration 84/1000 | Loss: 0.00001790
Iteration 85/1000 | Loss: 0.00001790
Iteration 86/1000 | Loss: 0.00001790
Iteration 87/1000 | Loss: 0.00001790
Iteration 88/1000 | Loss: 0.00001790
Iteration 89/1000 | Loss: 0.00001790
Iteration 90/1000 | Loss: 0.00001790
Iteration 91/1000 | Loss: 0.00001790
Iteration 92/1000 | Loss: 0.00001790
Iteration 93/1000 | Loss: 0.00001790
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001789
Iteration 96/1000 | Loss: 0.00001789
Iteration 97/1000 | Loss: 0.00001789
Iteration 98/1000 | Loss: 0.00001789
Iteration 99/1000 | Loss: 0.00001789
Iteration 100/1000 | Loss: 0.00001789
Iteration 101/1000 | Loss: 0.00001789
Iteration 102/1000 | Loss: 0.00001789
Iteration 103/1000 | Loss: 0.00001789
Iteration 104/1000 | Loss: 0.00001789
Iteration 105/1000 | Loss: 0.00001789
Iteration 106/1000 | Loss: 0.00001789
Iteration 107/1000 | Loss: 0.00001788
Iteration 108/1000 | Loss: 0.00001788
Iteration 109/1000 | Loss: 0.00001788
Iteration 110/1000 | Loss: 0.00001788
Iteration 111/1000 | Loss: 0.00001788
Iteration 112/1000 | Loss: 0.00001788
Iteration 113/1000 | Loss: 0.00001788
Iteration 114/1000 | Loss: 0.00001788
Iteration 115/1000 | Loss: 0.00001787
Iteration 116/1000 | Loss: 0.00001787
Iteration 117/1000 | Loss: 0.00001787
Iteration 118/1000 | Loss: 0.00001787
Iteration 119/1000 | Loss: 0.00001787
Iteration 120/1000 | Loss: 0.00001787
Iteration 121/1000 | Loss: 0.00001787
Iteration 122/1000 | Loss: 0.00001787
Iteration 123/1000 | Loss: 0.00001787
Iteration 124/1000 | Loss: 0.00001787
Iteration 125/1000 | Loss: 0.00001787
Iteration 126/1000 | Loss: 0.00001787
Iteration 127/1000 | Loss: 0.00001787
Iteration 128/1000 | Loss: 0.00001787
Iteration 129/1000 | Loss: 0.00001787
Iteration 130/1000 | Loss: 0.00001787
Iteration 131/1000 | Loss: 0.00001787
Iteration 132/1000 | Loss: 0.00001787
Iteration 133/1000 | Loss: 0.00001787
Iteration 134/1000 | Loss: 0.00001786
Iteration 135/1000 | Loss: 0.00001786
Iteration 136/1000 | Loss: 0.00001786
Iteration 137/1000 | Loss: 0.00001786
Iteration 138/1000 | Loss: 0.00001786
Iteration 139/1000 | Loss: 0.00001786
Iteration 140/1000 | Loss: 0.00001786
Iteration 141/1000 | Loss: 0.00001786
Iteration 142/1000 | Loss: 0.00001786
Iteration 143/1000 | Loss: 0.00001786
Iteration 144/1000 | Loss: 0.00001786
Iteration 145/1000 | Loss: 0.00001786
Iteration 146/1000 | Loss: 0.00001786
Iteration 147/1000 | Loss: 0.00001786
Iteration 148/1000 | Loss: 0.00001786
Iteration 149/1000 | Loss: 0.00001786
Iteration 150/1000 | Loss: 0.00001786
Iteration 151/1000 | Loss: 0.00001786
Iteration 152/1000 | Loss: 0.00001786
Iteration 153/1000 | Loss: 0.00001786
Iteration 154/1000 | Loss: 0.00001786
Iteration 155/1000 | Loss: 0.00001786
Iteration 156/1000 | Loss: 0.00001786
Iteration 157/1000 | Loss: 0.00001786
Iteration 158/1000 | Loss: 0.00001786
Iteration 159/1000 | Loss: 0.00001786
Iteration 160/1000 | Loss: 0.00001786
Iteration 161/1000 | Loss: 0.00001786
Iteration 162/1000 | Loss: 0.00001786
Iteration 163/1000 | Loss: 0.00001786
Iteration 164/1000 | Loss: 0.00001786
Iteration 165/1000 | Loss: 0.00001786
Iteration 166/1000 | Loss: 0.00001786
Iteration 167/1000 | Loss: 0.00001786
Iteration 168/1000 | Loss: 0.00001786
Iteration 169/1000 | Loss: 0.00001786
Iteration 170/1000 | Loss: 0.00001786
Iteration 171/1000 | Loss: 0.00001786
Iteration 172/1000 | Loss: 0.00001786
Iteration 173/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.786476423148997e-05, 1.786476423148997e-05, 1.786476423148997e-05, 1.786476423148997e-05, 1.786476423148997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.786476423148997e-05

Optimization complete. Final v2v error: 3.5404739379882812 mm

Highest mean error: 4.124960899353027 mm for frame 69

Lowest mean error: 3.0129268169403076 mm for frame 93

Saving results

Total time: 35.559205293655396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500207
Iteration 2/25 | Loss: 0.00148430
Iteration 3/25 | Loss: 0.00134609
Iteration 4/25 | Loss: 0.00131554
Iteration 5/25 | Loss: 0.00130857
Iteration 6/25 | Loss: 0.00130654
Iteration 7/25 | Loss: 0.00130593
Iteration 8/25 | Loss: 0.00130567
Iteration 9/25 | Loss: 0.00130558
Iteration 10/25 | Loss: 0.00130558
Iteration 11/25 | Loss: 0.00130558
Iteration 12/25 | Loss: 0.00130558
Iteration 13/25 | Loss: 0.00130558
Iteration 14/25 | Loss: 0.00130558
Iteration 15/25 | Loss: 0.00130558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013055776944383979, 0.0013055776944383979, 0.0013055776944383979, 0.0013055776944383979, 0.0013055776944383979]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013055776944383979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46731639
Iteration 2/25 | Loss: 0.00114685
Iteration 3/25 | Loss: 0.00114684
Iteration 4/25 | Loss: 0.00114684
Iteration 5/25 | Loss: 0.00114684
Iteration 6/25 | Loss: 0.00114684
Iteration 7/25 | Loss: 0.00114684
Iteration 8/25 | Loss: 0.00114684
Iteration 9/25 | Loss: 0.00114684
Iteration 10/25 | Loss: 0.00114684
Iteration 11/25 | Loss: 0.00114684
Iteration 12/25 | Loss: 0.00114684
Iteration 13/25 | Loss: 0.00114684
Iteration 14/25 | Loss: 0.00114684
Iteration 15/25 | Loss: 0.00114684
Iteration 16/25 | Loss: 0.00114684
Iteration 17/25 | Loss: 0.00114684
Iteration 18/25 | Loss: 0.00114684
Iteration 19/25 | Loss: 0.00114684
Iteration 20/25 | Loss: 0.00114684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001146839582361281, 0.001146839582361281, 0.001146839582361281, 0.001146839582361281, 0.001146839582361281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001146839582361281

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114684
Iteration 2/1000 | Loss: 0.00006368
Iteration 3/1000 | Loss: 0.00008646
Iteration 4/1000 | Loss: 0.00004084
Iteration 5/1000 | Loss: 0.00003538
Iteration 6/1000 | Loss: 0.00006604
Iteration 7/1000 | Loss: 0.00006708
Iteration 8/1000 | Loss: 0.00006298
Iteration 9/1000 | Loss: 0.00006380
Iteration 10/1000 | Loss: 0.00006877
Iteration 11/1000 | Loss: 0.00005980
Iteration 12/1000 | Loss: 0.00005961
Iteration 13/1000 | Loss: 0.00005713
Iteration 14/1000 | Loss: 0.00004973
Iteration 15/1000 | Loss: 0.00002660
Iteration 16/1000 | Loss: 0.00002547
Iteration 17/1000 | Loss: 0.00002476
Iteration 18/1000 | Loss: 0.00002449
Iteration 19/1000 | Loss: 0.00002428
Iteration 20/1000 | Loss: 0.00002403
Iteration 21/1000 | Loss: 0.00002384
Iteration 22/1000 | Loss: 0.00002372
Iteration 23/1000 | Loss: 0.00002368
Iteration 24/1000 | Loss: 0.00002368
Iteration 25/1000 | Loss: 0.00002367
Iteration 26/1000 | Loss: 0.00002366
Iteration 27/1000 | Loss: 0.00002366
Iteration 28/1000 | Loss: 0.00002365
Iteration 29/1000 | Loss: 0.00002364
Iteration 30/1000 | Loss: 0.00002364
Iteration 31/1000 | Loss: 0.00002364
Iteration 32/1000 | Loss: 0.00002363
Iteration 33/1000 | Loss: 0.00002363
Iteration 34/1000 | Loss: 0.00002361
Iteration 35/1000 | Loss: 0.00002361
Iteration 36/1000 | Loss: 0.00002360
Iteration 37/1000 | Loss: 0.00002360
Iteration 38/1000 | Loss: 0.00002359
Iteration 39/1000 | Loss: 0.00002355
Iteration 40/1000 | Loss: 0.00002351
Iteration 41/1000 | Loss: 0.00002351
Iteration 42/1000 | Loss: 0.00002350
Iteration 43/1000 | Loss: 0.00002349
Iteration 44/1000 | Loss: 0.00002349
Iteration 45/1000 | Loss: 0.00002348
Iteration 46/1000 | Loss: 0.00002347
Iteration 47/1000 | Loss: 0.00002346
Iteration 48/1000 | Loss: 0.00002345
Iteration 49/1000 | Loss: 0.00002344
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002340
Iteration 52/1000 | Loss: 0.00002337
Iteration 53/1000 | Loss: 0.00002336
Iteration 54/1000 | Loss: 0.00002336
Iteration 55/1000 | Loss: 0.00002335
Iteration 56/1000 | Loss: 0.00002334
Iteration 57/1000 | Loss: 0.00002334
Iteration 58/1000 | Loss: 0.00002334
Iteration 59/1000 | Loss: 0.00002334
Iteration 60/1000 | Loss: 0.00002333
Iteration 61/1000 | Loss: 0.00002333
Iteration 62/1000 | Loss: 0.00002333
Iteration 63/1000 | Loss: 0.00002332
Iteration 64/1000 | Loss: 0.00002331
Iteration 65/1000 | Loss: 0.00002331
Iteration 66/1000 | Loss: 0.00002331
Iteration 67/1000 | Loss: 0.00002331
Iteration 68/1000 | Loss: 0.00002331
Iteration 69/1000 | Loss: 0.00002330
Iteration 70/1000 | Loss: 0.00002330
Iteration 71/1000 | Loss: 0.00002330
Iteration 72/1000 | Loss: 0.00002330
Iteration 73/1000 | Loss: 0.00002330
Iteration 74/1000 | Loss: 0.00002330
Iteration 75/1000 | Loss: 0.00002330
Iteration 76/1000 | Loss: 0.00002329
Iteration 77/1000 | Loss: 0.00002329
Iteration 78/1000 | Loss: 0.00002329
Iteration 79/1000 | Loss: 0.00002329
Iteration 80/1000 | Loss: 0.00002329
Iteration 81/1000 | Loss: 0.00002328
Iteration 82/1000 | Loss: 0.00002328
Iteration 83/1000 | Loss: 0.00002328
Iteration 84/1000 | Loss: 0.00002328
Iteration 85/1000 | Loss: 0.00002328
Iteration 86/1000 | Loss: 0.00002328
Iteration 87/1000 | Loss: 0.00002328
Iteration 88/1000 | Loss: 0.00002327
Iteration 89/1000 | Loss: 0.00002327
Iteration 90/1000 | Loss: 0.00002327
Iteration 91/1000 | Loss: 0.00002327
Iteration 92/1000 | Loss: 0.00002327
Iteration 93/1000 | Loss: 0.00002327
Iteration 94/1000 | Loss: 0.00002327
Iteration 95/1000 | Loss: 0.00002327
Iteration 96/1000 | Loss: 0.00002327
Iteration 97/1000 | Loss: 0.00002327
Iteration 98/1000 | Loss: 0.00002327
Iteration 99/1000 | Loss: 0.00002327
Iteration 100/1000 | Loss: 0.00002327
Iteration 101/1000 | Loss: 0.00002327
Iteration 102/1000 | Loss: 0.00002327
Iteration 103/1000 | Loss: 0.00002327
Iteration 104/1000 | Loss: 0.00002327
Iteration 105/1000 | Loss: 0.00002326
Iteration 106/1000 | Loss: 0.00002326
Iteration 107/1000 | Loss: 0.00002326
Iteration 108/1000 | Loss: 0.00002326
Iteration 109/1000 | Loss: 0.00002326
Iteration 110/1000 | Loss: 0.00002326
Iteration 111/1000 | Loss: 0.00002326
Iteration 112/1000 | Loss: 0.00002326
Iteration 113/1000 | Loss: 0.00002326
Iteration 114/1000 | Loss: 0.00002326
Iteration 115/1000 | Loss: 0.00002326
Iteration 116/1000 | Loss: 0.00002326
Iteration 117/1000 | Loss: 0.00002326
Iteration 118/1000 | Loss: 0.00002326
Iteration 119/1000 | Loss: 0.00002326
Iteration 120/1000 | Loss: 0.00002325
Iteration 121/1000 | Loss: 0.00002325
Iteration 122/1000 | Loss: 0.00002325
Iteration 123/1000 | Loss: 0.00002325
Iteration 124/1000 | Loss: 0.00002325
Iteration 125/1000 | Loss: 0.00002325
Iteration 126/1000 | Loss: 0.00002325
Iteration 127/1000 | Loss: 0.00002325
Iteration 128/1000 | Loss: 0.00002325
Iteration 129/1000 | Loss: 0.00002325
Iteration 130/1000 | Loss: 0.00002325
Iteration 131/1000 | Loss: 0.00002325
Iteration 132/1000 | Loss: 0.00002325
Iteration 133/1000 | Loss: 0.00002325
Iteration 134/1000 | Loss: 0.00002325
Iteration 135/1000 | Loss: 0.00002325
Iteration 136/1000 | Loss: 0.00002325
Iteration 137/1000 | Loss: 0.00002325
Iteration 138/1000 | Loss: 0.00002325
Iteration 139/1000 | Loss: 0.00002325
Iteration 140/1000 | Loss: 0.00002325
Iteration 141/1000 | Loss: 0.00002325
Iteration 142/1000 | Loss: 0.00002325
Iteration 143/1000 | Loss: 0.00002325
Iteration 144/1000 | Loss: 0.00002325
Iteration 145/1000 | Loss: 0.00002325
Iteration 146/1000 | Loss: 0.00002325
Iteration 147/1000 | Loss: 0.00002325
Iteration 148/1000 | Loss: 0.00002325
Iteration 149/1000 | Loss: 0.00002325
Iteration 150/1000 | Loss: 0.00002325
Iteration 151/1000 | Loss: 0.00002325
Iteration 152/1000 | Loss: 0.00002325
Iteration 153/1000 | Loss: 0.00002325
Iteration 154/1000 | Loss: 0.00002325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.324769229744561e-05, 2.324769229744561e-05, 2.324769229744561e-05, 2.324769229744561e-05, 2.324769229744561e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.324769229744561e-05

Optimization complete. Final v2v error: 3.929398536682129 mm

Highest mean error: 5.632204055786133 mm for frame 94

Lowest mean error: 2.9868059158325195 mm for frame 49

Saving results

Total time: 54.03135442733765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712604
Iteration 2/25 | Loss: 0.00151737
Iteration 3/25 | Loss: 0.00142774
Iteration 4/25 | Loss: 0.00142109
Iteration 5/25 | Loss: 0.00142059
Iteration 6/25 | Loss: 0.00142059
Iteration 7/25 | Loss: 0.00142059
Iteration 8/25 | Loss: 0.00142059
Iteration 9/25 | Loss: 0.00142059
Iteration 10/25 | Loss: 0.00142059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014205933548510075, 0.0014205933548510075, 0.0014205933548510075, 0.0014205933548510075, 0.0014205933548510075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014205933548510075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53308713
Iteration 2/25 | Loss: 0.00086174
Iteration 3/25 | Loss: 0.00086172
Iteration 4/25 | Loss: 0.00086172
Iteration 5/25 | Loss: 0.00086172
Iteration 6/25 | Loss: 0.00086172
Iteration 7/25 | Loss: 0.00086172
Iteration 8/25 | Loss: 0.00086172
Iteration 9/25 | Loss: 0.00086172
Iteration 10/25 | Loss: 0.00086172
Iteration 11/25 | Loss: 0.00086172
Iteration 12/25 | Loss: 0.00086172
Iteration 13/25 | Loss: 0.00086172
Iteration 14/25 | Loss: 0.00086172
Iteration 15/25 | Loss: 0.00086172
Iteration 16/25 | Loss: 0.00086172
Iteration 17/25 | Loss: 0.00086172
Iteration 18/25 | Loss: 0.00086172
Iteration 19/25 | Loss: 0.00086172
Iteration 20/25 | Loss: 0.00086172
Iteration 21/25 | Loss: 0.00086172
Iteration 22/25 | Loss: 0.00086172
Iteration 23/25 | Loss: 0.00086172
Iteration 24/25 | Loss: 0.00086172
Iteration 25/25 | Loss: 0.00086172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086172
Iteration 2/1000 | Loss: 0.00004363
Iteration 3/1000 | Loss: 0.00003151
Iteration 4/1000 | Loss: 0.00002897
Iteration 5/1000 | Loss: 0.00002764
Iteration 6/1000 | Loss: 0.00002701
Iteration 7/1000 | Loss: 0.00002658
Iteration 8/1000 | Loss: 0.00002619
Iteration 9/1000 | Loss: 0.00002616
Iteration 10/1000 | Loss: 0.00002595
Iteration 11/1000 | Loss: 0.00002572
Iteration 12/1000 | Loss: 0.00002541
Iteration 13/1000 | Loss: 0.00002521
Iteration 14/1000 | Loss: 0.00002508
Iteration 15/1000 | Loss: 0.00002487
Iteration 16/1000 | Loss: 0.00002473
Iteration 17/1000 | Loss: 0.00002471
Iteration 18/1000 | Loss: 0.00002467
Iteration 19/1000 | Loss: 0.00002466
Iteration 20/1000 | Loss: 0.00002466
Iteration 21/1000 | Loss: 0.00002455
Iteration 22/1000 | Loss: 0.00002453
Iteration 23/1000 | Loss: 0.00002447
Iteration 24/1000 | Loss: 0.00002446
Iteration 25/1000 | Loss: 0.00002437
Iteration 26/1000 | Loss: 0.00002436
Iteration 27/1000 | Loss: 0.00002435
Iteration 28/1000 | Loss: 0.00002434
Iteration 29/1000 | Loss: 0.00002433
Iteration 30/1000 | Loss: 0.00002433
Iteration 31/1000 | Loss: 0.00002433
Iteration 32/1000 | Loss: 0.00002433
Iteration 33/1000 | Loss: 0.00002433
Iteration 34/1000 | Loss: 0.00002433
Iteration 35/1000 | Loss: 0.00002433
Iteration 36/1000 | Loss: 0.00002433
Iteration 37/1000 | Loss: 0.00002433
Iteration 38/1000 | Loss: 0.00002433
Iteration 39/1000 | Loss: 0.00002432
Iteration 40/1000 | Loss: 0.00002432
Iteration 41/1000 | Loss: 0.00002432
Iteration 42/1000 | Loss: 0.00002432
Iteration 43/1000 | Loss: 0.00002432
Iteration 44/1000 | Loss: 0.00002432
Iteration 45/1000 | Loss: 0.00002432
Iteration 46/1000 | Loss: 0.00002431
Iteration 47/1000 | Loss: 0.00002430
Iteration 48/1000 | Loss: 0.00002430
Iteration 49/1000 | Loss: 0.00002430
Iteration 50/1000 | Loss: 0.00002428
Iteration 51/1000 | Loss: 0.00002428
Iteration 52/1000 | Loss: 0.00002428
Iteration 53/1000 | Loss: 0.00002428
Iteration 54/1000 | Loss: 0.00002427
Iteration 55/1000 | Loss: 0.00002427
Iteration 56/1000 | Loss: 0.00002426
Iteration 57/1000 | Loss: 0.00002426
Iteration 58/1000 | Loss: 0.00002426
Iteration 59/1000 | Loss: 0.00002426
Iteration 60/1000 | Loss: 0.00002426
Iteration 61/1000 | Loss: 0.00002425
Iteration 62/1000 | Loss: 0.00002425
Iteration 63/1000 | Loss: 0.00002425
Iteration 64/1000 | Loss: 0.00002425
Iteration 65/1000 | Loss: 0.00002425
Iteration 66/1000 | Loss: 0.00002425
Iteration 67/1000 | Loss: 0.00002425
Iteration 68/1000 | Loss: 0.00002425
Iteration 69/1000 | Loss: 0.00002425
Iteration 70/1000 | Loss: 0.00002425
Iteration 71/1000 | Loss: 0.00002425
Iteration 72/1000 | Loss: 0.00002425
Iteration 73/1000 | Loss: 0.00002425
Iteration 74/1000 | Loss: 0.00002424
Iteration 75/1000 | Loss: 0.00002423
Iteration 76/1000 | Loss: 0.00002423
Iteration 77/1000 | Loss: 0.00002423
Iteration 78/1000 | Loss: 0.00002422
Iteration 79/1000 | Loss: 0.00002422
Iteration 80/1000 | Loss: 0.00002422
Iteration 81/1000 | Loss: 0.00002422
Iteration 82/1000 | Loss: 0.00002421
Iteration 83/1000 | Loss: 0.00002421
Iteration 84/1000 | Loss: 0.00002421
Iteration 85/1000 | Loss: 0.00002421
Iteration 86/1000 | Loss: 0.00002421
Iteration 87/1000 | Loss: 0.00002421
Iteration 88/1000 | Loss: 0.00002421
Iteration 89/1000 | Loss: 0.00002421
Iteration 90/1000 | Loss: 0.00002421
Iteration 91/1000 | Loss: 0.00002420
Iteration 92/1000 | Loss: 0.00002420
Iteration 93/1000 | Loss: 0.00002420
Iteration 94/1000 | Loss: 0.00002420
Iteration 95/1000 | Loss: 0.00002420
Iteration 96/1000 | Loss: 0.00002420
Iteration 97/1000 | Loss: 0.00002419
Iteration 98/1000 | Loss: 0.00002419
Iteration 99/1000 | Loss: 0.00002419
Iteration 100/1000 | Loss: 0.00002418
Iteration 101/1000 | Loss: 0.00002418
Iteration 102/1000 | Loss: 0.00002418
Iteration 103/1000 | Loss: 0.00002417
Iteration 104/1000 | Loss: 0.00002417
Iteration 105/1000 | Loss: 0.00002417
Iteration 106/1000 | Loss: 0.00002417
Iteration 107/1000 | Loss: 0.00002415
Iteration 108/1000 | Loss: 0.00002414
Iteration 109/1000 | Loss: 0.00002413
Iteration 110/1000 | Loss: 0.00002413
Iteration 111/1000 | Loss: 0.00002413
Iteration 112/1000 | Loss: 0.00002413
Iteration 113/1000 | Loss: 0.00002413
Iteration 114/1000 | Loss: 0.00002413
Iteration 115/1000 | Loss: 0.00002413
Iteration 116/1000 | Loss: 0.00002413
Iteration 117/1000 | Loss: 0.00002412
Iteration 118/1000 | Loss: 0.00002412
Iteration 119/1000 | Loss: 0.00002412
Iteration 120/1000 | Loss: 0.00002411
Iteration 121/1000 | Loss: 0.00002411
Iteration 122/1000 | Loss: 0.00002411
Iteration 123/1000 | Loss: 0.00002411
Iteration 124/1000 | Loss: 0.00002411
Iteration 125/1000 | Loss: 0.00002411
Iteration 126/1000 | Loss: 0.00002411
Iteration 127/1000 | Loss: 0.00002411
Iteration 128/1000 | Loss: 0.00002410
Iteration 129/1000 | Loss: 0.00002410
Iteration 130/1000 | Loss: 0.00002410
Iteration 131/1000 | Loss: 0.00002410
Iteration 132/1000 | Loss: 0.00002410
Iteration 133/1000 | Loss: 0.00002410
Iteration 134/1000 | Loss: 0.00002410
Iteration 135/1000 | Loss: 0.00002410
Iteration 136/1000 | Loss: 0.00002409
Iteration 137/1000 | Loss: 0.00002409
Iteration 138/1000 | Loss: 0.00002409
Iteration 139/1000 | Loss: 0.00002409
Iteration 140/1000 | Loss: 0.00002409
Iteration 141/1000 | Loss: 0.00002409
Iteration 142/1000 | Loss: 0.00002409
Iteration 143/1000 | Loss: 0.00002408
Iteration 144/1000 | Loss: 0.00002408
Iteration 145/1000 | Loss: 0.00002408
Iteration 146/1000 | Loss: 0.00002408
Iteration 147/1000 | Loss: 0.00002408
Iteration 148/1000 | Loss: 0.00002408
Iteration 149/1000 | Loss: 0.00002408
Iteration 150/1000 | Loss: 0.00002408
Iteration 151/1000 | Loss: 0.00002408
Iteration 152/1000 | Loss: 0.00002408
Iteration 153/1000 | Loss: 0.00002408
Iteration 154/1000 | Loss: 0.00002408
Iteration 155/1000 | Loss: 0.00002408
Iteration 156/1000 | Loss: 0.00002408
Iteration 157/1000 | Loss: 0.00002408
Iteration 158/1000 | Loss: 0.00002408
Iteration 159/1000 | Loss: 0.00002408
Iteration 160/1000 | Loss: 0.00002408
Iteration 161/1000 | Loss: 0.00002408
Iteration 162/1000 | Loss: 0.00002408
Iteration 163/1000 | Loss: 0.00002408
Iteration 164/1000 | Loss: 0.00002408
Iteration 165/1000 | Loss: 0.00002408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.407603278697934e-05, 2.407603278697934e-05, 2.407603278697934e-05, 2.407603278697934e-05, 2.407603278697934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.407603278697934e-05

Optimization complete. Final v2v error: 3.8896396160125732 mm

Highest mean error: 4.3363447189331055 mm for frame 98

Lowest mean error: 3.4643988609313965 mm for frame 183

Saving results

Total time: 46.52552247047424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367020
Iteration 2/25 | Loss: 0.00130439
Iteration 3/25 | Loss: 0.00123333
Iteration 4/25 | Loss: 0.00122113
Iteration 5/25 | Loss: 0.00121842
Iteration 6/25 | Loss: 0.00121810
Iteration 7/25 | Loss: 0.00121810
Iteration 8/25 | Loss: 0.00121810
Iteration 9/25 | Loss: 0.00121810
Iteration 10/25 | Loss: 0.00121810
Iteration 11/25 | Loss: 0.00121810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001218097866512835, 0.001218097866512835, 0.001218097866512835, 0.001218097866512835, 0.001218097866512835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001218097866512835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47075355
Iteration 2/25 | Loss: 0.00090059
Iteration 3/25 | Loss: 0.00090059
Iteration 4/25 | Loss: 0.00090059
Iteration 5/25 | Loss: 0.00090059
Iteration 6/25 | Loss: 0.00090059
Iteration 7/25 | Loss: 0.00090059
Iteration 8/25 | Loss: 0.00090059
Iteration 9/25 | Loss: 0.00090059
Iteration 10/25 | Loss: 0.00090059
Iteration 11/25 | Loss: 0.00090058
Iteration 12/25 | Loss: 0.00090058
Iteration 13/25 | Loss: 0.00090058
Iteration 14/25 | Loss: 0.00090058
Iteration 15/25 | Loss: 0.00090058
Iteration 16/25 | Loss: 0.00090058
Iteration 17/25 | Loss: 0.00090058
Iteration 18/25 | Loss: 0.00090058
Iteration 19/25 | Loss: 0.00090058
Iteration 20/25 | Loss: 0.00090058
Iteration 21/25 | Loss: 0.00090058
Iteration 22/25 | Loss: 0.00090058
Iteration 23/25 | Loss: 0.00090058
Iteration 24/25 | Loss: 0.00090058
Iteration 25/25 | Loss: 0.00090058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090058
Iteration 2/1000 | Loss: 0.00003083
Iteration 3/1000 | Loss: 0.00001749
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001420
Iteration 6/1000 | Loss: 0.00001369
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001251
Iteration 14/1000 | Loss: 0.00001242
Iteration 15/1000 | Loss: 0.00001242
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001237
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001234
Iteration 23/1000 | Loss: 0.00001233
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001228
Iteration 26/1000 | Loss: 0.00001228
Iteration 27/1000 | Loss: 0.00001227
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001213
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001210
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001206
Iteration 67/1000 | Loss: 0.00001205
Iteration 68/1000 | Loss: 0.00001205
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001204
Iteration 75/1000 | Loss: 0.00001204
Iteration 76/1000 | Loss: 0.00001204
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001203
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001199
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001195
Iteration 111/1000 | Loss: 0.00001195
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001191
Iteration 130/1000 | Loss: 0.00001191
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001190
Iteration 133/1000 | Loss: 0.00001190
Iteration 134/1000 | Loss: 0.00001190
Iteration 135/1000 | Loss: 0.00001190
Iteration 136/1000 | Loss: 0.00001189
Iteration 137/1000 | Loss: 0.00001189
Iteration 138/1000 | Loss: 0.00001189
Iteration 139/1000 | Loss: 0.00001188
Iteration 140/1000 | Loss: 0.00001188
Iteration 141/1000 | Loss: 0.00001188
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001187
Iteration 147/1000 | Loss: 0.00001187
Iteration 148/1000 | Loss: 0.00001187
Iteration 149/1000 | Loss: 0.00001187
Iteration 150/1000 | Loss: 0.00001187
Iteration 151/1000 | Loss: 0.00001186
Iteration 152/1000 | Loss: 0.00001186
Iteration 153/1000 | Loss: 0.00001186
Iteration 154/1000 | Loss: 0.00001185
Iteration 155/1000 | Loss: 0.00001185
Iteration 156/1000 | Loss: 0.00001185
Iteration 157/1000 | Loss: 0.00001184
Iteration 158/1000 | Loss: 0.00001184
Iteration 159/1000 | Loss: 0.00001184
Iteration 160/1000 | Loss: 0.00001184
Iteration 161/1000 | Loss: 0.00001184
Iteration 162/1000 | Loss: 0.00001184
Iteration 163/1000 | Loss: 0.00001184
Iteration 164/1000 | Loss: 0.00001184
Iteration 165/1000 | Loss: 0.00001184
Iteration 166/1000 | Loss: 0.00001184
Iteration 167/1000 | Loss: 0.00001184
Iteration 168/1000 | Loss: 0.00001184
Iteration 169/1000 | Loss: 0.00001183
Iteration 170/1000 | Loss: 0.00001183
Iteration 171/1000 | Loss: 0.00001183
Iteration 172/1000 | Loss: 0.00001183
Iteration 173/1000 | Loss: 0.00001183
Iteration 174/1000 | Loss: 0.00001183
Iteration 175/1000 | Loss: 0.00001183
Iteration 176/1000 | Loss: 0.00001182
Iteration 177/1000 | Loss: 0.00001182
Iteration 178/1000 | Loss: 0.00001182
Iteration 179/1000 | Loss: 0.00001182
Iteration 180/1000 | Loss: 0.00001181
Iteration 181/1000 | Loss: 0.00001181
Iteration 182/1000 | Loss: 0.00001181
Iteration 183/1000 | Loss: 0.00001181
Iteration 184/1000 | Loss: 0.00001181
Iteration 185/1000 | Loss: 0.00001181
Iteration 186/1000 | Loss: 0.00001181
Iteration 187/1000 | Loss: 0.00001181
Iteration 188/1000 | Loss: 0.00001181
Iteration 189/1000 | Loss: 0.00001181
Iteration 190/1000 | Loss: 0.00001181
Iteration 191/1000 | Loss: 0.00001181
Iteration 192/1000 | Loss: 0.00001181
Iteration 193/1000 | Loss: 0.00001181
Iteration 194/1000 | Loss: 0.00001181
Iteration 195/1000 | Loss: 0.00001181
Iteration 196/1000 | Loss: 0.00001181
Iteration 197/1000 | Loss: 0.00001181
Iteration 198/1000 | Loss: 0.00001181
Iteration 199/1000 | Loss: 0.00001181
Iteration 200/1000 | Loss: 0.00001181
Iteration 201/1000 | Loss: 0.00001181
Iteration 202/1000 | Loss: 0.00001181
Iteration 203/1000 | Loss: 0.00001181
Iteration 204/1000 | Loss: 0.00001181
Iteration 205/1000 | Loss: 0.00001180
Iteration 206/1000 | Loss: 0.00001180
Iteration 207/1000 | Loss: 0.00001180
Iteration 208/1000 | Loss: 0.00001180
Iteration 209/1000 | Loss: 0.00001180
Iteration 210/1000 | Loss: 0.00001180
Iteration 211/1000 | Loss: 0.00001180
Iteration 212/1000 | Loss: 0.00001180
Iteration 213/1000 | Loss: 0.00001180
Iteration 214/1000 | Loss: 0.00001180
Iteration 215/1000 | Loss: 0.00001180
Iteration 216/1000 | Loss: 0.00001180
Iteration 217/1000 | Loss: 0.00001180
Iteration 218/1000 | Loss: 0.00001180
Iteration 219/1000 | Loss: 0.00001180
Iteration 220/1000 | Loss: 0.00001180
Iteration 221/1000 | Loss: 0.00001180
Iteration 222/1000 | Loss: 0.00001180
Iteration 223/1000 | Loss: 0.00001180
Iteration 224/1000 | Loss: 0.00001179
Iteration 225/1000 | Loss: 0.00001179
Iteration 226/1000 | Loss: 0.00001179
Iteration 227/1000 | Loss: 0.00001179
Iteration 228/1000 | Loss: 0.00001179
Iteration 229/1000 | Loss: 0.00001179
Iteration 230/1000 | Loss: 0.00001179
Iteration 231/1000 | Loss: 0.00001179
Iteration 232/1000 | Loss: 0.00001179
Iteration 233/1000 | Loss: 0.00001179
Iteration 234/1000 | Loss: 0.00001179
Iteration 235/1000 | Loss: 0.00001179
Iteration 236/1000 | Loss: 0.00001179
Iteration 237/1000 | Loss: 0.00001179
Iteration 238/1000 | Loss: 0.00001178
Iteration 239/1000 | Loss: 0.00001178
Iteration 240/1000 | Loss: 0.00001178
Iteration 241/1000 | Loss: 0.00001178
Iteration 242/1000 | Loss: 0.00001178
Iteration 243/1000 | Loss: 0.00001178
Iteration 244/1000 | Loss: 0.00001178
Iteration 245/1000 | Loss: 0.00001178
Iteration 246/1000 | Loss: 0.00001178
Iteration 247/1000 | Loss: 0.00001178
Iteration 248/1000 | Loss: 0.00001178
Iteration 249/1000 | Loss: 0.00001178
Iteration 250/1000 | Loss: 0.00001178
Iteration 251/1000 | Loss: 0.00001178
Iteration 252/1000 | Loss: 0.00001178
Iteration 253/1000 | Loss: 0.00001178
Iteration 254/1000 | Loss: 0.00001178
Iteration 255/1000 | Loss: 0.00001178
Iteration 256/1000 | Loss: 0.00001178
Iteration 257/1000 | Loss: 0.00001178
Iteration 258/1000 | Loss: 0.00001178
Iteration 259/1000 | Loss: 0.00001178
Iteration 260/1000 | Loss: 0.00001178
Iteration 261/1000 | Loss: 0.00001178
Iteration 262/1000 | Loss: 0.00001178
Iteration 263/1000 | Loss: 0.00001178
Iteration 264/1000 | Loss: 0.00001178
Iteration 265/1000 | Loss: 0.00001178
Iteration 266/1000 | Loss: 0.00001178
Iteration 267/1000 | Loss: 0.00001178
Iteration 268/1000 | Loss: 0.00001178
Iteration 269/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.1780965905927587e-05, 1.1780965905927587e-05, 1.1780965905927587e-05, 1.1780965905927587e-05, 1.1780965905927587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1780965905927587e-05

Optimization complete. Final v2v error: 2.9590208530426025 mm

Highest mean error: 3.239366054534912 mm for frame 103

Lowest mean error: 2.7453227043151855 mm for frame 137

Saving results

Total time: 41.52081632614136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839021
Iteration 2/25 | Loss: 0.00130594
Iteration 3/25 | Loss: 0.00124305
Iteration 4/25 | Loss: 0.00123551
Iteration 5/25 | Loss: 0.00123306
Iteration 6/25 | Loss: 0.00123261
Iteration 7/25 | Loss: 0.00123261
Iteration 8/25 | Loss: 0.00123261
Iteration 9/25 | Loss: 0.00123261
Iteration 10/25 | Loss: 0.00123261
Iteration 11/25 | Loss: 0.00123261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001232612645253539, 0.001232612645253539, 0.001232612645253539, 0.001232612645253539, 0.001232612645253539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001232612645253539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74219239
Iteration 2/25 | Loss: 0.00091087
Iteration 3/25 | Loss: 0.00091087
Iteration 4/25 | Loss: 0.00091086
Iteration 5/25 | Loss: 0.00091086
Iteration 6/25 | Loss: 0.00091086
Iteration 7/25 | Loss: 0.00091086
Iteration 8/25 | Loss: 0.00091086
Iteration 9/25 | Loss: 0.00091086
Iteration 10/25 | Loss: 0.00091086
Iteration 11/25 | Loss: 0.00091086
Iteration 12/25 | Loss: 0.00091086
Iteration 13/25 | Loss: 0.00091086
Iteration 14/25 | Loss: 0.00091086
Iteration 15/25 | Loss: 0.00091086
Iteration 16/25 | Loss: 0.00091086
Iteration 17/25 | Loss: 0.00091086
Iteration 18/25 | Loss: 0.00091086
Iteration 19/25 | Loss: 0.00091086
Iteration 20/25 | Loss: 0.00091086
Iteration 21/25 | Loss: 0.00091086
Iteration 22/25 | Loss: 0.00091086
Iteration 23/25 | Loss: 0.00091086
Iteration 24/25 | Loss: 0.00091086
Iteration 25/25 | Loss: 0.00091086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091086
Iteration 2/1000 | Loss: 0.00002359
Iteration 3/1000 | Loss: 0.00001705
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001300
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001217
Iteration 9/1000 | Loss: 0.00001214
Iteration 10/1000 | Loss: 0.00001196
Iteration 11/1000 | Loss: 0.00001179
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001147
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001140
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001138
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001137
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001131
Iteration 31/1000 | Loss: 0.00001131
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001130
Iteration 34/1000 | Loss: 0.00001130
Iteration 35/1000 | Loss: 0.00001130
Iteration 36/1000 | Loss: 0.00001130
Iteration 37/1000 | Loss: 0.00001130
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001129
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001126
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001124
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001122
Iteration 54/1000 | Loss: 0.00001122
Iteration 55/1000 | Loss: 0.00001122
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001120
Iteration 64/1000 | Loss: 0.00001120
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001117
Iteration 74/1000 | Loss: 0.00001117
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001116
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001116
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001116
Iteration 84/1000 | Loss: 0.00001116
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001108
Iteration 114/1000 | Loss: 0.00001108
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001106
Iteration 123/1000 | Loss: 0.00001106
Iteration 124/1000 | Loss: 0.00001106
Iteration 125/1000 | Loss: 0.00001106
Iteration 126/1000 | Loss: 0.00001106
Iteration 127/1000 | Loss: 0.00001105
Iteration 128/1000 | Loss: 0.00001105
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001105
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001104
Iteration 135/1000 | Loss: 0.00001104
Iteration 136/1000 | Loss: 0.00001104
Iteration 137/1000 | Loss: 0.00001104
Iteration 138/1000 | Loss: 0.00001104
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001103
Iteration 141/1000 | Loss: 0.00001103
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Iteration 144/1000 | Loss: 0.00001103
Iteration 145/1000 | Loss: 0.00001103
Iteration 146/1000 | Loss: 0.00001103
Iteration 147/1000 | Loss: 0.00001103
Iteration 148/1000 | Loss: 0.00001103
Iteration 149/1000 | Loss: 0.00001103
Iteration 150/1000 | Loss: 0.00001103
Iteration 151/1000 | Loss: 0.00001103
Iteration 152/1000 | Loss: 0.00001103
Iteration 153/1000 | Loss: 0.00001103
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001103
Iteration 156/1000 | Loss: 0.00001103
Iteration 157/1000 | Loss: 0.00001103
Iteration 158/1000 | Loss: 0.00001103
Iteration 159/1000 | Loss: 0.00001103
Iteration 160/1000 | Loss: 0.00001103
Iteration 161/1000 | Loss: 0.00001103
Iteration 162/1000 | Loss: 0.00001102
Iteration 163/1000 | Loss: 0.00001102
Iteration 164/1000 | Loss: 0.00001102
Iteration 165/1000 | Loss: 0.00001102
Iteration 166/1000 | Loss: 0.00001102
Iteration 167/1000 | Loss: 0.00001102
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001102
Iteration 176/1000 | Loss: 0.00001102
Iteration 177/1000 | Loss: 0.00001102
Iteration 178/1000 | Loss: 0.00001102
Iteration 179/1000 | Loss: 0.00001102
Iteration 180/1000 | Loss: 0.00001102
Iteration 181/1000 | Loss: 0.00001102
Iteration 182/1000 | Loss: 0.00001102
Iteration 183/1000 | Loss: 0.00001102
Iteration 184/1000 | Loss: 0.00001102
Iteration 185/1000 | Loss: 0.00001102
Iteration 186/1000 | Loss: 0.00001102
Iteration 187/1000 | Loss: 0.00001102
Iteration 188/1000 | Loss: 0.00001102
Iteration 189/1000 | Loss: 0.00001102
Iteration 190/1000 | Loss: 0.00001102
Iteration 191/1000 | Loss: 0.00001102
Iteration 192/1000 | Loss: 0.00001102
Iteration 193/1000 | Loss: 0.00001102
Iteration 194/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.1017118595191278e-05, 1.1017118595191278e-05, 1.1017118595191278e-05, 1.1017118595191278e-05, 1.1017118595191278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1017118595191278e-05

Optimization complete. Final v2v error: 2.8526737689971924 mm

Highest mean error: 3.3288629055023193 mm for frame 91

Lowest mean error: 2.6462032794952393 mm for frame 129

Saving results

Total time: 37.72266149520874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438640
Iteration 2/25 | Loss: 0.00135788
Iteration 3/25 | Loss: 0.00128822
Iteration 4/25 | Loss: 0.00127936
Iteration 5/25 | Loss: 0.00127685
Iteration 6/25 | Loss: 0.00127650
Iteration 7/25 | Loss: 0.00127650
Iteration 8/25 | Loss: 0.00127650
Iteration 9/25 | Loss: 0.00127650
Iteration 10/25 | Loss: 0.00127650
Iteration 11/25 | Loss: 0.00127650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012765047140419483, 0.0012765047140419483, 0.0012765047140419483, 0.0012765047140419483, 0.0012765047140419483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012765047140419483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.65467167
Iteration 2/25 | Loss: 0.00091223
Iteration 3/25 | Loss: 0.00091223
Iteration 4/25 | Loss: 0.00091223
Iteration 5/25 | Loss: 0.00091223
Iteration 6/25 | Loss: 0.00091223
Iteration 7/25 | Loss: 0.00091223
Iteration 8/25 | Loss: 0.00091223
Iteration 9/25 | Loss: 0.00091223
Iteration 10/25 | Loss: 0.00091223
Iteration 11/25 | Loss: 0.00091223
Iteration 12/25 | Loss: 0.00091223
Iteration 13/25 | Loss: 0.00091223
Iteration 14/25 | Loss: 0.00091223
Iteration 15/25 | Loss: 0.00091223
Iteration 16/25 | Loss: 0.00091223
Iteration 17/25 | Loss: 0.00091223
Iteration 18/25 | Loss: 0.00091223
Iteration 19/25 | Loss: 0.00091223
Iteration 20/25 | Loss: 0.00091223
Iteration 21/25 | Loss: 0.00091223
Iteration 22/25 | Loss: 0.00091223
Iteration 23/25 | Loss: 0.00091223
Iteration 24/25 | Loss: 0.00091223
Iteration 25/25 | Loss: 0.00091223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091223
Iteration 2/1000 | Loss: 0.00003018
Iteration 3/1000 | Loss: 0.00002152
Iteration 4/1000 | Loss: 0.00001871
Iteration 5/1000 | Loss: 0.00001784
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001634
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001538
Iteration 19/1000 | Loss: 0.00001538
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001535
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001527
Iteration 28/1000 | Loss: 0.00001527
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001523
Iteration 35/1000 | Loss: 0.00001522
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001521
Iteration 40/1000 | Loss: 0.00001520
Iteration 41/1000 | Loss: 0.00001520
Iteration 42/1000 | Loss: 0.00001519
Iteration 43/1000 | Loss: 0.00001519
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001517
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001512
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001512
Iteration 58/1000 | Loss: 0.00001511
Iteration 59/1000 | Loss: 0.00001511
Iteration 60/1000 | Loss: 0.00001510
Iteration 61/1000 | Loss: 0.00001509
Iteration 62/1000 | Loss: 0.00001508
Iteration 63/1000 | Loss: 0.00001508
Iteration 64/1000 | Loss: 0.00001508
Iteration 65/1000 | Loss: 0.00001507
Iteration 66/1000 | Loss: 0.00001507
Iteration 67/1000 | Loss: 0.00001507
Iteration 68/1000 | Loss: 0.00001506
Iteration 69/1000 | Loss: 0.00001506
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001505
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001503
Iteration 79/1000 | Loss: 0.00001503
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001501
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001501
Iteration 85/1000 | Loss: 0.00001500
Iteration 86/1000 | Loss: 0.00001500
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001497
Iteration 95/1000 | Loss: 0.00001497
Iteration 96/1000 | Loss: 0.00001497
Iteration 97/1000 | Loss: 0.00001497
Iteration 98/1000 | Loss: 0.00001497
Iteration 99/1000 | Loss: 0.00001497
Iteration 100/1000 | Loss: 0.00001497
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001496
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001496
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001496
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001496
Iteration 110/1000 | Loss: 0.00001495
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001495
Iteration 113/1000 | Loss: 0.00001495
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001494
Iteration 117/1000 | Loss: 0.00001494
Iteration 118/1000 | Loss: 0.00001494
Iteration 119/1000 | Loss: 0.00001494
Iteration 120/1000 | Loss: 0.00001494
Iteration 121/1000 | Loss: 0.00001493
Iteration 122/1000 | Loss: 0.00001493
Iteration 123/1000 | Loss: 0.00001493
Iteration 124/1000 | Loss: 0.00001493
Iteration 125/1000 | Loss: 0.00001493
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001492
Iteration 128/1000 | Loss: 0.00001492
Iteration 129/1000 | Loss: 0.00001492
Iteration 130/1000 | Loss: 0.00001492
Iteration 131/1000 | Loss: 0.00001492
Iteration 132/1000 | Loss: 0.00001492
Iteration 133/1000 | Loss: 0.00001492
Iteration 134/1000 | Loss: 0.00001492
Iteration 135/1000 | Loss: 0.00001492
Iteration 136/1000 | Loss: 0.00001492
Iteration 137/1000 | Loss: 0.00001491
Iteration 138/1000 | Loss: 0.00001491
Iteration 139/1000 | Loss: 0.00001491
Iteration 140/1000 | Loss: 0.00001491
Iteration 141/1000 | Loss: 0.00001491
Iteration 142/1000 | Loss: 0.00001491
Iteration 143/1000 | Loss: 0.00001491
Iteration 144/1000 | Loss: 0.00001491
Iteration 145/1000 | Loss: 0.00001491
Iteration 146/1000 | Loss: 0.00001491
Iteration 147/1000 | Loss: 0.00001491
Iteration 148/1000 | Loss: 0.00001490
Iteration 149/1000 | Loss: 0.00001490
Iteration 150/1000 | Loss: 0.00001490
Iteration 151/1000 | Loss: 0.00001490
Iteration 152/1000 | Loss: 0.00001490
Iteration 153/1000 | Loss: 0.00001490
Iteration 154/1000 | Loss: 0.00001490
Iteration 155/1000 | Loss: 0.00001490
Iteration 156/1000 | Loss: 0.00001490
Iteration 157/1000 | Loss: 0.00001490
Iteration 158/1000 | Loss: 0.00001490
Iteration 159/1000 | Loss: 0.00001490
Iteration 160/1000 | Loss: 0.00001490
Iteration 161/1000 | Loss: 0.00001490
Iteration 162/1000 | Loss: 0.00001490
Iteration 163/1000 | Loss: 0.00001490
Iteration 164/1000 | Loss: 0.00001490
Iteration 165/1000 | Loss: 0.00001489
Iteration 166/1000 | Loss: 0.00001489
Iteration 167/1000 | Loss: 0.00001489
Iteration 168/1000 | Loss: 0.00001489
Iteration 169/1000 | Loss: 0.00001489
Iteration 170/1000 | Loss: 0.00001489
Iteration 171/1000 | Loss: 0.00001489
Iteration 172/1000 | Loss: 0.00001489
Iteration 173/1000 | Loss: 0.00001489
Iteration 174/1000 | Loss: 0.00001489
Iteration 175/1000 | Loss: 0.00001489
Iteration 176/1000 | Loss: 0.00001489
Iteration 177/1000 | Loss: 0.00001489
Iteration 178/1000 | Loss: 0.00001489
Iteration 179/1000 | Loss: 0.00001489
Iteration 180/1000 | Loss: 0.00001489
Iteration 181/1000 | Loss: 0.00001489
Iteration 182/1000 | Loss: 0.00001489
Iteration 183/1000 | Loss: 0.00001488
Iteration 184/1000 | Loss: 0.00001488
Iteration 185/1000 | Loss: 0.00001488
Iteration 186/1000 | Loss: 0.00001488
Iteration 187/1000 | Loss: 0.00001488
Iteration 188/1000 | Loss: 0.00001488
Iteration 189/1000 | Loss: 0.00001488
Iteration 190/1000 | Loss: 0.00001488
Iteration 191/1000 | Loss: 0.00001488
Iteration 192/1000 | Loss: 0.00001488
Iteration 193/1000 | Loss: 0.00001488
Iteration 194/1000 | Loss: 0.00001487
Iteration 195/1000 | Loss: 0.00001487
Iteration 196/1000 | Loss: 0.00001487
Iteration 197/1000 | Loss: 0.00001487
Iteration 198/1000 | Loss: 0.00001487
Iteration 199/1000 | Loss: 0.00001487
Iteration 200/1000 | Loss: 0.00001487
Iteration 201/1000 | Loss: 0.00001487
Iteration 202/1000 | Loss: 0.00001487
Iteration 203/1000 | Loss: 0.00001487
Iteration 204/1000 | Loss: 0.00001486
Iteration 205/1000 | Loss: 0.00001486
Iteration 206/1000 | Loss: 0.00001486
Iteration 207/1000 | Loss: 0.00001486
Iteration 208/1000 | Loss: 0.00001486
Iteration 209/1000 | Loss: 0.00001486
Iteration 210/1000 | Loss: 0.00001486
Iteration 211/1000 | Loss: 0.00001486
Iteration 212/1000 | Loss: 0.00001485
Iteration 213/1000 | Loss: 0.00001485
Iteration 214/1000 | Loss: 0.00001485
Iteration 215/1000 | Loss: 0.00001485
Iteration 216/1000 | Loss: 0.00001485
Iteration 217/1000 | Loss: 0.00001485
Iteration 218/1000 | Loss: 0.00001485
Iteration 219/1000 | Loss: 0.00001485
Iteration 220/1000 | Loss: 0.00001485
Iteration 221/1000 | Loss: 0.00001485
Iteration 222/1000 | Loss: 0.00001485
Iteration 223/1000 | Loss: 0.00001485
Iteration 224/1000 | Loss: 0.00001485
Iteration 225/1000 | Loss: 0.00001485
Iteration 226/1000 | Loss: 0.00001485
Iteration 227/1000 | Loss: 0.00001485
Iteration 228/1000 | Loss: 0.00001485
Iteration 229/1000 | Loss: 0.00001484
Iteration 230/1000 | Loss: 0.00001484
Iteration 231/1000 | Loss: 0.00001484
Iteration 232/1000 | Loss: 0.00001484
Iteration 233/1000 | Loss: 0.00001484
Iteration 234/1000 | Loss: 0.00001484
Iteration 235/1000 | Loss: 0.00001484
Iteration 236/1000 | Loss: 0.00001484
Iteration 237/1000 | Loss: 0.00001484
Iteration 238/1000 | Loss: 0.00001484
Iteration 239/1000 | Loss: 0.00001484
Iteration 240/1000 | Loss: 0.00001484
Iteration 241/1000 | Loss: 0.00001484
Iteration 242/1000 | Loss: 0.00001484
Iteration 243/1000 | Loss: 0.00001484
Iteration 244/1000 | Loss: 0.00001484
Iteration 245/1000 | Loss: 0.00001484
Iteration 246/1000 | Loss: 0.00001484
Iteration 247/1000 | Loss: 0.00001484
Iteration 248/1000 | Loss: 0.00001484
Iteration 249/1000 | Loss: 0.00001484
Iteration 250/1000 | Loss: 0.00001484
Iteration 251/1000 | Loss: 0.00001484
Iteration 252/1000 | Loss: 0.00001484
Iteration 253/1000 | Loss: 0.00001484
Iteration 254/1000 | Loss: 0.00001484
Iteration 255/1000 | Loss: 0.00001484
Iteration 256/1000 | Loss: 0.00001484
Iteration 257/1000 | Loss: 0.00001484
Iteration 258/1000 | Loss: 0.00001484
Iteration 259/1000 | Loss: 0.00001484
Iteration 260/1000 | Loss: 0.00001484
Iteration 261/1000 | Loss: 0.00001484
Iteration 262/1000 | Loss: 0.00001484
Iteration 263/1000 | Loss: 0.00001484
Iteration 264/1000 | Loss: 0.00001484
Iteration 265/1000 | Loss: 0.00001484
Iteration 266/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.4839130017207935e-05, 1.4839130017207935e-05, 1.4839130017207935e-05, 1.4839130017207935e-05, 1.4839130017207935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4839130017207935e-05

Optimization complete. Final v2v error: 3.274533271789551 mm

Highest mean error: 3.6716089248657227 mm for frame 82

Lowest mean error: 3.050814390182495 mm for frame 24

Saving results

Total time: 42.300498962402344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499572
Iteration 2/25 | Loss: 0.00139582
Iteration 3/25 | Loss: 0.00128204
Iteration 4/25 | Loss: 0.00126575
Iteration 5/25 | Loss: 0.00126187
Iteration 6/25 | Loss: 0.00126059
Iteration 7/25 | Loss: 0.00126056
Iteration 8/25 | Loss: 0.00126056
Iteration 9/25 | Loss: 0.00126056
Iteration 10/25 | Loss: 0.00126056
Iteration 11/25 | Loss: 0.00126056
Iteration 12/25 | Loss: 0.00126056
Iteration 13/25 | Loss: 0.00126056
Iteration 14/25 | Loss: 0.00126056
Iteration 15/25 | Loss: 0.00126056
Iteration 16/25 | Loss: 0.00126056
Iteration 17/25 | Loss: 0.00126056
Iteration 18/25 | Loss: 0.00126056
Iteration 19/25 | Loss: 0.00126056
Iteration 20/25 | Loss: 0.00126056
Iteration 21/25 | Loss: 0.00126056
Iteration 22/25 | Loss: 0.00126056
Iteration 23/25 | Loss: 0.00126056
Iteration 24/25 | Loss: 0.00126056
Iteration 25/25 | Loss: 0.00126056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03962398
Iteration 2/25 | Loss: 0.00061505
Iteration 3/25 | Loss: 0.00061505
Iteration 4/25 | Loss: 0.00061505
Iteration 5/25 | Loss: 0.00061505
Iteration 6/25 | Loss: 0.00061505
Iteration 7/25 | Loss: 0.00061505
Iteration 8/25 | Loss: 0.00061505
Iteration 9/25 | Loss: 0.00061505
Iteration 10/25 | Loss: 0.00061505
Iteration 11/25 | Loss: 0.00061505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006150513072498143, 0.0006150513072498143, 0.0006150513072498143, 0.0006150513072498143, 0.0006150513072498143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006150513072498143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061505
Iteration 2/1000 | Loss: 0.00004899
Iteration 3/1000 | Loss: 0.00003303
Iteration 4/1000 | Loss: 0.00002754
Iteration 5/1000 | Loss: 0.00002563
Iteration 6/1000 | Loss: 0.00002444
Iteration 7/1000 | Loss: 0.00002352
Iteration 8/1000 | Loss: 0.00002281
Iteration 9/1000 | Loss: 0.00002224
Iteration 10/1000 | Loss: 0.00002188
Iteration 11/1000 | Loss: 0.00002155
Iteration 12/1000 | Loss: 0.00002121
Iteration 13/1000 | Loss: 0.00002090
Iteration 14/1000 | Loss: 0.00002064
Iteration 15/1000 | Loss: 0.00002032
Iteration 16/1000 | Loss: 0.00002012
Iteration 17/1000 | Loss: 0.00002002
Iteration 18/1000 | Loss: 0.00002002
Iteration 19/1000 | Loss: 0.00002001
Iteration 20/1000 | Loss: 0.00001999
Iteration 21/1000 | Loss: 0.00001993
Iteration 22/1000 | Loss: 0.00001990
Iteration 23/1000 | Loss: 0.00001989
Iteration 24/1000 | Loss: 0.00001989
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001988
Iteration 27/1000 | Loss: 0.00001984
Iteration 28/1000 | Loss: 0.00001984
Iteration 29/1000 | Loss: 0.00001984
Iteration 30/1000 | Loss: 0.00001984
Iteration 31/1000 | Loss: 0.00001983
Iteration 32/1000 | Loss: 0.00001983
Iteration 33/1000 | Loss: 0.00001983
Iteration 34/1000 | Loss: 0.00001983
Iteration 35/1000 | Loss: 0.00001983
Iteration 36/1000 | Loss: 0.00001983
Iteration 37/1000 | Loss: 0.00001983
Iteration 38/1000 | Loss: 0.00001983
Iteration 39/1000 | Loss: 0.00001983
Iteration 40/1000 | Loss: 0.00001982
Iteration 41/1000 | Loss: 0.00001981
Iteration 42/1000 | Loss: 0.00001980
Iteration 43/1000 | Loss: 0.00001979
Iteration 44/1000 | Loss: 0.00001976
Iteration 45/1000 | Loss: 0.00001975
Iteration 46/1000 | Loss: 0.00001975
Iteration 47/1000 | Loss: 0.00001975
Iteration 48/1000 | Loss: 0.00001975
Iteration 49/1000 | Loss: 0.00001974
Iteration 50/1000 | Loss: 0.00001974
Iteration 51/1000 | Loss: 0.00001974
Iteration 52/1000 | Loss: 0.00001973
Iteration 53/1000 | Loss: 0.00001973
Iteration 54/1000 | Loss: 0.00001973
Iteration 55/1000 | Loss: 0.00001972
Iteration 56/1000 | Loss: 0.00001972
Iteration 57/1000 | Loss: 0.00001972
Iteration 58/1000 | Loss: 0.00001972
Iteration 59/1000 | Loss: 0.00001972
Iteration 60/1000 | Loss: 0.00001972
Iteration 61/1000 | Loss: 0.00001972
Iteration 62/1000 | Loss: 0.00001972
Iteration 63/1000 | Loss: 0.00001972
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001972
Iteration 68/1000 | Loss: 0.00001972
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001972
Iteration 74/1000 | Loss: 0.00001972
Iteration 75/1000 | Loss: 0.00001971
Iteration 76/1000 | Loss: 0.00001971
Iteration 77/1000 | Loss: 0.00001971
Iteration 78/1000 | Loss: 0.00001971
Iteration 79/1000 | Loss: 0.00001971
Iteration 80/1000 | Loss: 0.00001971
Iteration 81/1000 | Loss: 0.00001971
Iteration 82/1000 | Loss: 0.00001971
Iteration 83/1000 | Loss: 0.00001971
Iteration 84/1000 | Loss: 0.00001971
Iteration 85/1000 | Loss: 0.00001971
Iteration 86/1000 | Loss: 0.00001971
Iteration 87/1000 | Loss: 0.00001970
Iteration 88/1000 | Loss: 0.00001970
Iteration 89/1000 | Loss: 0.00001970
Iteration 90/1000 | Loss: 0.00001970
Iteration 91/1000 | Loss: 0.00001970
Iteration 92/1000 | Loss: 0.00001970
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001969
Iteration 100/1000 | Loss: 0.00001969
Iteration 101/1000 | Loss: 0.00001969
Iteration 102/1000 | Loss: 0.00001969
Iteration 103/1000 | Loss: 0.00001969
Iteration 104/1000 | Loss: 0.00001969
Iteration 105/1000 | Loss: 0.00001969
Iteration 106/1000 | Loss: 0.00001969
Iteration 107/1000 | Loss: 0.00001969
Iteration 108/1000 | Loss: 0.00001969
Iteration 109/1000 | Loss: 0.00001968
Iteration 110/1000 | Loss: 0.00001968
Iteration 111/1000 | Loss: 0.00001968
Iteration 112/1000 | Loss: 0.00001968
Iteration 113/1000 | Loss: 0.00001967
Iteration 114/1000 | Loss: 0.00001967
Iteration 115/1000 | Loss: 0.00001967
Iteration 116/1000 | Loss: 0.00001967
Iteration 117/1000 | Loss: 0.00001967
Iteration 118/1000 | Loss: 0.00001967
Iteration 119/1000 | Loss: 0.00001967
Iteration 120/1000 | Loss: 0.00001967
Iteration 121/1000 | Loss: 0.00001967
Iteration 122/1000 | Loss: 0.00001967
Iteration 123/1000 | Loss: 0.00001967
Iteration 124/1000 | Loss: 0.00001967
Iteration 125/1000 | Loss: 0.00001967
Iteration 126/1000 | Loss: 0.00001967
Iteration 127/1000 | Loss: 0.00001967
Iteration 128/1000 | Loss: 0.00001966
Iteration 129/1000 | Loss: 0.00001966
Iteration 130/1000 | Loss: 0.00001966
Iteration 131/1000 | Loss: 0.00001966
Iteration 132/1000 | Loss: 0.00001966
Iteration 133/1000 | Loss: 0.00001966
Iteration 134/1000 | Loss: 0.00001966
Iteration 135/1000 | Loss: 0.00001966
Iteration 136/1000 | Loss: 0.00001966
Iteration 137/1000 | Loss: 0.00001966
Iteration 138/1000 | Loss: 0.00001966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.966311174328439e-05, 1.966311174328439e-05, 1.966311174328439e-05, 1.966311174328439e-05, 1.966311174328439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.966311174328439e-05

Optimization complete. Final v2v error: 3.751413106918335 mm

Highest mean error: 3.7849273681640625 mm for frame 4

Lowest mean error: 3.7237513065338135 mm for frame 53

Saving results

Total time: 39.378002882003784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00571947
Iteration 2/25 | Loss: 0.00142439
Iteration 3/25 | Loss: 0.00130765
Iteration 4/25 | Loss: 0.00129322
Iteration 5/25 | Loss: 0.00128960
Iteration 6/25 | Loss: 0.00128878
Iteration 7/25 | Loss: 0.00128878
Iteration 8/25 | Loss: 0.00128878
Iteration 9/25 | Loss: 0.00128878
Iteration 10/25 | Loss: 0.00128878
Iteration 11/25 | Loss: 0.00128878
Iteration 12/25 | Loss: 0.00128878
Iteration 13/25 | Loss: 0.00128878
Iteration 14/25 | Loss: 0.00128878
Iteration 15/25 | Loss: 0.00128878
Iteration 16/25 | Loss: 0.00128878
Iteration 17/25 | Loss: 0.00128878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012887819902971387, 0.0012887819902971387, 0.0012887819902971387, 0.0012887819902971387, 0.0012887819902971387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012887819902971387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.00657701
Iteration 2/25 | Loss: 0.00096065
Iteration 3/25 | Loss: 0.00096064
Iteration 4/25 | Loss: 0.00096064
Iteration 5/25 | Loss: 0.00096064
Iteration 6/25 | Loss: 0.00096063
Iteration 7/25 | Loss: 0.00096063
Iteration 8/25 | Loss: 0.00096063
Iteration 9/25 | Loss: 0.00096063
Iteration 10/25 | Loss: 0.00096063
Iteration 11/25 | Loss: 0.00096063
Iteration 12/25 | Loss: 0.00096063
Iteration 13/25 | Loss: 0.00096063
Iteration 14/25 | Loss: 0.00096063
Iteration 15/25 | Loss: 0.00096063
Iteration 16/25 | Loss: 0.00096063
Iteration 17/25 | Loss: 0.00096063
Iteration 18/25 | Loss: 0.00096063
Iteration 19/25 | Loss: 0.00096063
Iteration 20/25 | Loss: 0.00096063
Iteration 21/25 | Loss: 0.00096063
Iteration 22/25 | Loss: 0.00096063
Iteration 23/25 | Loss: 0.00096063
Iteration 24/25 | Loss: 0.00096063
Iteration 25/25 | Loss: 0.00096063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096063
Iteration 2/1000 | Loss: 0.00003220
Iteration 3/1000 | Loss: 0.00001997
Iteration 4/1000 | Loss: 0.00001783
Iteration 5/1000 | Loss: 0.00001720
Iteration 6/1000 | Loss: 0.00001673
Iteration 7/1000 | Loss: 0.00001643
Iteration 8/1000 | Loss: 0.00001618
Iteration 9/1000 | Loss: 0.00001603
Iteration 10/1000 | Loss: 0.00001592
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001586
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001569
Iteration 16/1000 | Loss: 0.00001565
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001562
Iteration 20/1000 | Loss: 0.00001553
Iteration 21/1000 | Loss: 0.00001552
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001545
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001541
Iteration 27/1000 | Loss: 0.00001540
Iteration 28/1000 | Loss: 0.00001540
Iteration 29/1000 | Loss: 0.00001539
Iteration 30/1000 | Loss: 0.00001539
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001538
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001533
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001532
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001528
Iteration 40/1000 | Loss: 0.00001528
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001528
Iteration 44/1000 | Loss: 0.00001528
Iteration 45/1000 | Loss: 0.00001528
Iteration 46/1000 | Loss: 0.00001528
Iteration 47/1000 | Loss: 0.00001528
Iteration 48/1000 | Loss: 0.00001528
Iteration 49/1000 | Loss: 0.00001528
Iteration 50/1000 | Loss: 0.00001528
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001528
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001526
Iteration 55/1000 | Loss: 0.00001526
Iteration 56/1000 | Loss: 0.00001526
Iteration 57/1000 | Loss: 0.00001525
Iteration 58/1000 | Loss: 0.00001525
Iteration 59/1000 | Loss: 0.00001525
Iteration 60/1000 | Loss: 0.00001525
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001524
Iteration 65/1000 | Loss: 0.00001524
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001522
Iteration 71/1000 | Loss: 0.00001522
Iteration 72/1000 | Loss: 0.00001522
Iteration 73/1000 | Loss: 0.00001522
Iteration 74/1000 | Loss: 0.00001522
Iteration 75/1000 | Loss: 0.00001522
Iteration 76/1000 | Loss: 0.00001521
Iteration 77/1000 | Loss: 0.00001521
Iteration 78/1000 | Loss: 0.00001521
Iteration 79/1000 | Loss: 0.00001521
Iteration 80/1000 | Loss: 0.00001521
Iteration 81/1000 | Loss: 0.00001521
Iteration 82/1000 | Loss: 0.00001521
Iteration 83/1000 | Loss: 0.00001521
Iteration 84/1000 | Loss: 0.00001521
Iteration 85/1000 | Loss: 0.00001521
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001520
Iteration 88/1000 | Loss: 0.00001520
Iteration 89/1000 | Loss: 0.00001520
Iteration 90/1000 | Loss: 0.00001519
Iteration 91/1000 | Loss: 0.00001519
Iteration 92/1000 | Loss: 0.00001519
Iteration 93/1000 | Loss: 0.00001519
Iteration 94/1000 | Loss: 0.00001519
Iteration 95/1000 | Loss: 0.00001519
Iteration 96/1000 | Loss: 0.00001519
Iteration 97/1000 | Loss: 0.00001519
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001519
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.5185528354777489e-05, 1.5185528354777489e-05, 1.5185528354777489e-05, 1.5185528354777489e-05, 1.5185528354777489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5185528354777489e-05

Optimization complete. Final v2v error: 3.3028981685638428 mm

Highest mean error: 3.5488228797912598 mm for frame 98

Lowest mean error: 3.0672192573547363 mm for frame 109

Saving results

Total time: 31.591598510742188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884173
Iteration 2/25 | Loss: 0.00229370
Iteration 3/25 | Loss: 0.00196084
Iteration 4/25 | Loss: 0.00195771
Iteration 5/25 | Loss: 0.00175401
Iteration 6/25 | Loss: 0.00150027
Iteration 7/25 | Loss: 0.00139544
Iteration 8/25 | Loss: 0.00137038
Iteration 9/25 | Loss: 0.00135638
Iteration 10/25 | Loss: 0.00134976
Iteration 11/25 | Loss: 0.00135116
Iteration 12/25 | Loss: 0.00133391
Iteration 13/25 | Loss: 0.00132650
Iteration 14/25 | Loss: 0.00132050
Iteration 15/25 | Loss: 0.00131594
Iteration 16/25 | Loss: 0.00131484
Iteration 17/25 | Loss: 0.00131446
Iteration 18/25 | Loss: 0.00131440
Iteration 19/25 | Loss: 0.00131440
Iteration 20/25 | Loss: 0.00131440
Iteration 21/25 | Loss: 0.00131439
Iteration 22/25 | Loss: 0.00131439
Iteration 23/25 | Loss: 0.00131439
Iteration 24/25 | Loss: 0.00131439
Iteration 25/25 | Loss: 0.00131439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40798426
Iteration 2/25 | Loss: 0.00064334
Iteration 3/25 | Loss: 0.00064333
Iteration 4/25 | Loss: 0.00064333
Iteration 5/25 | Loss: 0.00064333
Iteration 6/25 | Loss: 0.00064333
Iteration 7/25 | Loss: 0.00064333
Iteration 8/25 | Loss: 0.00064333
Iteration 9/25 | Loss: 0.00064333
Iteration 10/25 | Loss: 0.00064333
Iteration 11/25 | Loss: 0.00064333
Iteration 12/25 | Loss: 0.00064333
Iteration 13/25 | Loss: 0.00064333
Iteration 14/25 | Loss: 0.00064333
Iteration 15/25 | Loss: 0.00064333
Iteration 16/25 | Loss: 0.00064333
Iteration 17/25 | Loss: 0.00064333
Iteration 18/25 | Loss: 0.00064333
Iteration 19/25 | Loss: 0.00064333
Iteration 20/25 | Loss: 0.00064333
Iteration 21/25 | Loss: 0.00064333
Iteration 22/25 | Loss: 0.00064333
Iteration 23/25 | Loss: 0.00064333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006433286471292377, 0.0006433286471292377, 0.0006433286471292377, 0.0006433286471292377, 0.0006433286471292377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006433286471292377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064333
Iteration 2/1000 | Loss: 0.00065430
Iteration 3/1000 | Loss: 0.00083248
Iteration 4/1000 | Loss: 0.00041848
Iteration 5/1000 | Loss: 0.00006987
Iteration 6/1000 | Loss: 0.00004938
Iteration 7/1000 | Loss: 0.00003921
Iteration 8/1000 | Loss: 0.00003542
Iteration 9/1000 | Loss: 0.00003289
Iteration 10/1000 | Loss: 0.00003092
Iteration 11/1000 | Loss: 0.00002954
Iteration 12/1000 | Loss: 0.00002859
Iteration 13/1000 | Loss: 0.00002776
Iteration 14/1000 | Loss: 0.00002721
Iteration 15/1000 | Loss: 0.00002682
Iteration 16/1000 | Loss: 0.00002643
Iteration 17/1000 | Loss: 0.00002610
Iteration 18/1000 | Loss: 0.00002590
Iteration 19/1000 | Loss: 0.00002573
Iteration 20/1000 | Loss: 0.00002555
Iteration 21/1000 | Loss: 0.00002548
Iteration 22/1000 | Loss: 0.00002525
Iteration 23/1000 | Loss: 0.00002502
Iteration 24/1000 | Loss: 0.00002500
Iteration 25/1000 | Loss: 0.00028602
Iteration 26/1000 | Loss: 0.00028863
Iteration 27/1000 | Loss: 0.00025067
Iteration 28/1000 | Loss: 0.00025490
Iteration 29/1000 | Loss: 0.00016519
Iteration 30/1000 | Loss: 0.00002970
Iteration 31/1000 | Loss: 0.00002777
Iteration 32/1000 | Loss: 0.00002699
Iteration 33/1000 | Loss: 0.00002653
Iteration 34/1000 | Loss: 0.00002618
Iteration 35/1000 | Loss: 0.00002580
Iteration 36/1000 | Loss: 0.00002550
Iteration 37/1000 | Loss: 0.00002526
Iteration 38/1000 | Loss: 0.00002523
Iteration 39/1000 | Loss: 0.00002507
Iteration 40/1000 | Loss: 0.00002502
Iteration 41/1000 | Loss: 0.00002501
Iteration 42/1000 | Loss: 0.00002500
Iteration 43/1000 | Loss: 0.00002498
Iteration 44/1000 | Loss: 0.00027804
Iteration 45/1000 | Loss: 0.00071029
Iteration 46/1000 | Loss: 0.00028590
Iteration 47/1000 | Loss: 0.00006348
Iteration 48/1000 | Loss: 0.00004609
Iteration 49/1000 | Loss: 0.00003613
Iteration 50/1000 | Loss: 0.00003300
Iteration 51/1000 | Loss: 0.00003182
Iteration 52/1000 | Loss: 0.00003044
Iteration 53/1000 | Loss: 0.00002948
Iteration 54/1000 | Loss: 0.00002848
Iteration 55/1000 | Loss: 0.00002794
Iteration 56/1000 | Loss: 0.00002740
Iteration 57/1000 | Loss: 0.00002696
Iteration 58/1000 | Loss: 0.00002660
Iteration 59/1000 | Loss: 0.00002634
Iteration 60/1000 | Loss: 0.00002615
Iteration 61/1000 | Loss: 0.00002595
Iteration 62/1000 | Loss: 0.00002573
Iteration 63/1000 | Loss: 0.00002569
Iteration 64/1000 | Loss: 0.00002568
Iteration 65/1000 | Loss: 0.00002560
Iteration 66/1000 | Loss: 0.00002543
Iteration 67/1000 | Loss: 0.00002539
Iteration 68/1000 | Loss: 0.00002530
Iteration 69/1000 | Loss: 0.00002529
Iteration 70/1000 | Loss: 0.00002520
Iteration 71/1000 | Loss: 0.00002508
Iteration 72/1000 | Loss: 0.00002499
Iteration 73/1000 | Loss: 0.00002481
Iteration 74/1000 | Loss: 0.00002478
Iteration 75/1000 | Loss: 0.00002473
Iteration 76/1000 | Loss: 0.00002460
Iteration 77/1000 | Loss: 0.00002452
Iteration 78/1000 | Loss: 0.00002449
Iteration 79/1000 | Loss: 0.00002442
Iteration 80/1000 | Loss: 0.00002435
Iteration 81/1000 | Loss: 0.00002432
Iteration 82/1000 | Loss: 0.00002431
Iteration 83/1000 | Loss: 0.00002431
Iteration 84/1000 | Loss: 0.00002430
Iteration 85/1000 | Loss: 0.00002429
Iteration 86/1000 | Loss: 0.00002429
Iteration 87/1000 | Loss: 0.00002428
Iteration 88/1000 | Loss: 0.00002428
Iteration 89/1000 | Loss: 0.00002427
Iteration 90/1000 | Loss: 0.00002427
Iteration 91/1000 | Loss: 0.00002427
Iteration 92/1000 | Loss: 0.00002426
Iteration 93/1000 | Loss: 0.00002426
Iteration 94/1000 | Loss: 0.00002423
Iteration 95/1000 | Loss: 0.00002422
Iteration 96/1000 | Loss: 0.00002422
Iteration 97/1000 | Loss: 0.00002420
Iteration 98/1000 | Loss: 0.00002419
Iteration 99/1000 | Loss: 0.00002417
Iteration 100/1000 | Loss: 0.00002417
Iteration 101/1000 | Loss: 0.00002416
Iteration 102/1000 | Loss: 0.00002416
Iteration 103/1000 | Loss: 0.00002415
Iteration 104/1000 | Loss: 0.00002414
Iteration 105/1000 | Loss: 0.00002413
Iteration 106/1000 | Loss: 0.00002412
Iteration 107/1000 | Loss: 0.00002411
Iteration 108/1000 | Loss: 0.00002410
Iteration 109/1000 | Loss: 0.00002409
Iteration 110/1000 | Loss: 0.00002409
Iteration 111/1000 | Loss: 0.00002404
Iteration 112/1000 | Loss: 0.00029467
Iteration 113/1000 | Loss: 0.00003362
Iteration 114/1000 | Loss: 0.00003013
Iteration 115/1000 | Loss: 0.00002837
Iteration 116/1000 | Loss: 0.00002658
Iteration 117/1000 | Loss: 0.00002599
Iteration 118/1000 | Loss: 0.00027466
Iteration 119/1000 | Loss: 0.00051978
Iteration 120/1000 | Loss: 0.00043864
Iteration 121/1000 | Loss: 0.00006518
Iteration 122/1000 | Loss: 0.00005138
Iteration 123/1000 | Loss: 0.00004183
Iteration 124/1000 | Loss: 0.00003806
Iteration 125/1000 | Loss: 0.00003524
Iteration 126/1000 | Loss: 0.00003295
Iteration 127/1000 | Loss: 0.00003124
Iteration 128/1000 | Loss: 0.00003015
Iteration 129/1000 | Loss: 0.00002939
Iteration 130/1000 | Loss: 0.00002876
Iteration 131/1000 | Loss: 0.00002826
Iteration 132/1000 | Loss: 0.00002779
Iteration 133/1000 | Loss: 0.00002751
Iteration 134/1000 | Loss: 0.00002726
Iteration 135/1000 | Loss: 0.00002700
Iteration 136/1000 | Loss: 0.00002677
Iteration 137/1000 | Loss: 0.00002677
Iteration 138/1000 | Loss: 0.00002662
Iteration 139/1000 | Loss: 0.00002639
Iteration 140/1000 | Loss: 0.00002615
Iteration 141/1000 | Loss: 0.00002587
Iteration 142/1000 | Loss: 0.00002560
Iteration 143/1000 | Loss: 0.00002540
Iteration 144/1000 | Loss: 0.00002522
Iteration 145/1000 | Loss: 0.00002513
Iteration 146/1000 | Loss: 0.00002512
Iteration 147/1000 | Loss: 0.00002512
Iteration 148/1000 | Loss: 0.00002511
Iteration 149/1000 | Loss: 0.00002509
Iteration 150/1000 | Loss: 0.00002508
Iteration 151/1000 | Loss: 0.00002507
Iteration 152/1000 | Loss: 0.00002506
Iteration 153/1000 | Loss: 0.00002506
Iteration 154/1000 | Loss: 0.00002503
Iteration 155/1000 | Loss: 0.00002499
Iteration 156/1000 | Loss: 0.00002498
Iteration 157/1000 | Loss: 0.00002494
Iteration 158/1000 | Loss: 0.00002489
Iteration 159/1000 | Loss: 0.00002486
Iteration 160/1000 | Loss: 0.00002481
Iteration 161/1000 | Loss: 0.00002480
Iteration 162/1000 | Loss: 0.00002479
Iteration 163/1000 | Loss: 0.00002479
Iteration 164/1000 | Loss: 0.00002479
Iteration 165/1000 | Loss: 0.00002478
Iteration 166/1000 | Loss: 0.00002478
Iteration 167/1000 | Loss: 0.00002477
Iteration 168/1000 | Loss: 0.00002476
Iteration 169/1000 | Loss: 0.00002461
Iteration 170/1000 | Loss: 0.00002458
Iteration 171/1000 | Loss: 0.00002444
Iteration 172/1000 | Loss: 0.00002434
Iteration 173/1000 | Loss: 0.00002425
Iteration 174/1000 | Loss: 0.00028195
Iteration 175/1000 | Loss: 0.00048423
Iteration 176/1000 | Loss: 0.00042080
Iteration 177/1000 | Loss: 0.00005778
Iteration 178/1000 | Loss: 0.00004651
Iteration 179/1000 | Loss: 0.00004004
Iteration 180/1000 | Loss: 0.00003825
Iteration 181/1000 | Loss: 0.00003697
Iteration 182/1000 | Loss: 0.00003591
Iteration 183/1000 | Loss: 0.00003451
Iteration 184/1000 | Loss: 0.00003316
Iteration 185/1000 | Loss: 0.00003109
Iteration 186/1000 | Loss: 0.00002907
Iteration 187/1000 | Loss: 0.00002786
Iteration 188/1000 | Loss: 0.00002692
Iteration 189/1000 | Loss: 0.00002634
Iteration 190/1000 | Loss: 0.00002572
Iteration 191/1000 | Loss: 0.00002533
Iteration 192/1000 | Loss: 0.00002495
Iteration 193/1000 | Loss: 0.00002476
Iteration 194/1000 | Loss: 0.00002453
Iteration 195/1000 | Loss: 0.00002426
Iteration 196/1000 | Loss: 0.00002391
Iteration 197/1000 | Loss: 0.00002352
Iteration 198/1000 | Loss: 0.00002323
Iteration 199/1000 | Loss: 0.00002294
Iteration 200/1000 | Loss: 0.00005290
Iteration 201/1000 | Loss: 0.00003145
Iteration 202/1000 | Loss: 0.00005224
Iteration 203/1000 | Loss: 0.00003173
Iteration 204/1000 | Loss: 0.00002328
Iteration 205/1000 | Loss: 0.00005328
Iteration 206/1000 | Loss: 0.00004331
Iteration 207/1000 | Loss: 0.00005171
Iteration 208/1000 | Loss: 0.00002848
Iteration 209/1000 | Loss: 0.00003724
Iteration 210/1000 | Loss: 0.00002386
Iteration 211/1000 | Loss: 0.00007358
Iteration 212/1000 | Loss: 0.00004504
Iteration 213/1000 | Loss: 0.00004303
Iteration 214/1000 | Loss: 0.00003390
Iteration 215/1000 | Loss: 0.00004467
Iteration 216/1000 | Loss: 0.00003226
Iteration 217/1000 | Loss: 0.00004218
Iteration 218/1000 | Loss: 0.00003015
Iteration 219/1000 | Loss: 0.00007296
Iteration 220/1000 | Loss: 0.00005023
Iteration 221/1000 | Loss: 0.00004286
Iteration 222/1000 | Loss: 0.00002978
Iteration 223/1000 | Loss: 0.00004088
Iteration 224/1000 | Loss: 0.00002910
Iteration 225/1000 | Loss: 0.00007320
Iteration 226/1000 | Loss: 0.00003047
Iteration 227/1000 | Loss: 0.00004162
Iteration 228/1000 | Loss: 0.00007005
Iteration 229/1000 | Loss: 0.00003973
Iteration 230/1000 | Loss: 0.00006526
Iteration 231/1000 | Loss: 0.00002721
Iteration 232/1000 | Loss: 0.00003632
Iteration 233/1000 | Loss: 0.00002912
Iteration 234/1000 | Loss: 0.00002342
Iteration 235/1000 | Loss: 0.00002294
Iteration 236/1000 | Loss: 0.00002253
Iteration 237/1000 | Loss: 0.00002222
Iteration 238/1000 | Loss: 0.00002210
Iteration 239/1000 | Loss: 0.00002209
Iteration 240/1000 | Loss: 0.00002192
Iteration 241/1000 | Loss: 0.00002182
Iteration 242/1000 | Loss: 0.00002181
Iteration 243/1000 | Loss: 0.00002180
Iteration 244/1000 | Loss: 0.00002180
Iteration 245/1000 | Loss: 0.00002179
Iteration 246/1000 | Loss: 0.00002179
Iteration 247/1000 | Loss: 0.00002179
Iteration 248/1000 | Loss: 0.00002179
Iteration 249/1000 | Loss: 0.00002179
Iteration 250/1000 | Loss: 0.00002179
Iteration 251/1000 | Loss: 0.00002179
Iteration 252/1000 | Loss: 0.00002179
Iteration 253/1000 | Loss: 0.00002179
Iteration 254/1000 | Loss: 0.00002179
Iteration 255/1000 | Loss: 0.00002178
Iteration 256/1000 | Loss: 0.00002178
Iteration 257/1000 | Loss: 0.00002178
Iteration 258/1000 | Loss: 0.00002177
Iteration 259/1000 | Loss: 0.00002176
Iteration 260/1000 | Loss: 0.00002176
Iteration 261/1000 | Loss: 0.00002176
Iteration 262/1000 | Loss: 0.00002175
Iteration 263/1000 | Loss: 0.00002175
Iteration 264/1000 | Loss: 0.00002175
Iteration 265/1000 | Loss: 0.00002174
Iteration 266/1000 | Loss: 0.00002174
Iteration 267/1000 | Loss: 0.00002174
Iteration 268/1000 | Loss: 0.00002174
Iteration 269/1000 | Loss: 0.00002174
Iteration 270/1000 | Loss: 0.00002174
Iteration 271/1000 | Loss: 0.00002173
Iteration 272/1000 | Loss: 0.00002173
Iteration 273/1000 | Loss: 0.00002173
Iteration 274/1000 | Loss: 0.00002173
Iteration 275/1000 | Loss: 0.00002172
Iteration 276/1000 | Loss: 0.00002172
Iteration 277/1000 | Loss: 0.00002172
Iteration 278/1000 | Loss: 0.00002172
Iteration 279/1000 | Loss: 0.00002172
Iteration 280/1000 | Loss: 0.00002172
Iteration 281/1000 | Loss: 0.00002172
Iteration 282/1000 | Loss: 0.00002172
Iteration 283/1000 | Loss: 0.00002172
Iteration 284/1000 | Loss: 0.00002172
Iteration 285/1000 | Loss: 0.00002172
Iteration 286/1000 | Loss: 0.00002172
Iteration 287/1000 | Loss: 0.00002172
Iteration 288/1000 | Loss: 0.00002172
Iteration 289/1000 | Loss: 0.00002172
Iteration 290/1000 | Loss: 0.00002172
Iteration 291/1000 | Loss: 0.00002171
Iteration 292/1000 | Loss: 0.00002171
Iteration 293/1000 | Loss: 0.00002171
Iteration 294/1000 | Loss: 0.00002171
Iteration 295/1000 | Loss: 0.00002171
Iteration 296/1000 | Loss: 0.00002171
Iteration 297/1000 | Loss: 0.00002171
Iteration 298/1000 | Loss: 0.00002171
Iteration 299/1000 | Loss: 0.00002171
Iteration 300/1000 | Loss: 0.00002171
Iteration 301/1000 | Loss: 0.00002171
Iteration 302/1000 | Loss: 0.00002171
Iteration 303/1000 | Loss: 0.00002171
Iteration 304/1000 | Loss: 0.00002171
Iteration 305/1000 | Loss: 0.00002170
Iteration 306/1000 | Loss: 0.00002170
Iteration 307/1000 | Loss: 0.00002169
Iteration 308/1000 | Loss: 0.00002169
Iteration 309/1000 | Loss: 0.00002169
Iteration 310/1000 | Loss: 0.00002169
Iteration 311/1000 | Loss: 0.00002169
Iteration 312/1000 | Loss: 0.00002168
Iteration 313/1000 | Loss: 0.00002168
Iteration 314/1000 | Loss: 0.00002168
Iteration 315/1000 | Loss: 0.00002168
Iteration 316/1000 | Loss: 0.00002168
Iteration 317/1000 | Loss: 0.00002168
Iteration 318/1000 | Loss: 0.00002167
Iteration 319/1000 | Loss: 0.00002167
Iteration 320/1000 | Loss: 0.00002167
Iteration 321/1000 | Loss: 0.00002167
Iteration 322/1000 | Loss: 0.00002167
Iteration 323/1000 | Loss: 0.00002167
Iteration 324/1000 | Loss: 0.00002167
Iteration 325/1000 | Loss: 0.00002167
Iteration 326/1000 | Loss: 0.00002167
Iteration 327/1000 | Loss: 0.00002167
Iteration 328/1000 | Loss: 0.00002167
Iteration 329/1000 | Loss: 0.00002167
Iteration 330/1000 | Loss: 0.00002167
Iteration 331/1000 | Loss: 0.00002167
Iteration 332/1000 | Loss: 0.00002167
Iteration 333/1000 | Loss: 0.00002166
Iteration 334/1000 | Loss: 0.00002166
Iteration 335/1000 | Loss: 0.00002166
Iteration 336/1000 | Loss: 0.00002166
Iteration 337/1000 | Loss: 0.00002166
Iteration 338/1000 | Loss: 0.00002166
Iteration 339/1000 | Loss: 0.00002166
Iteration 340/1000 | Loss: 0.00002166
Iteration 341/1000 | Loss: 0.00002165
Iteration 342/1000 | Loss: 0.00002165
Iteration 343/1000 | Loss: 0.00002164
Iteration 344/1000 | Loss: 0.00002164
Iteration 345/1000 | Loss: 0.00002164
Iteration 346/1000 | Loss: 0.00002164
Iteration 347/1000 | Loss: 0.00002164
Iteration 348/1000 | Loss: 0.00002164
Iteration 349/1000 | Loss: 0.00002164
Iteration 350/1000 | Loss: 0.00002164
Iteration 351/1000 | Loss: 0.00002164
Iteration 352/1000 | Loss: 0.00002163
Iteration 353/1000 | Loss: 0.00002163
Iteration 354/1000 | Loss: 0.00002163
Iteration 355/1000 | Loss: 0.00002163
Iteration 356/1000 | Loss: 0.00002163
Iteration 357/1000 | Loss: 0.00002163
Iteration 358/1000 | Loss: 0.00002163
Iteration 359/1000 | Loss: 0.00002163
Iteration 360/1000 | Loss: 0.00002163
Iteration 361/1000 | Loss: 0.00002163
Iteration 362/1000 | Loss: 0.00002163
Iteration 363/1000 | Loss: 0.00002162
Iteration 364/1000 | Loss: 0.00002162
Iteration 365/1000 | Loss: 0.00002162
Iteration 366/1000 | Loss: 0.00002162
Iteration 367/1000 | Loss: 0.00002162
Iteration 368/1000 | Loss: 0.00002162
Iteration 369/1000 | Loss: 0.00002161
Iteration 370/1000 | Loss: 0.00002161
Iteration 371/1000 | Loss: 0.00002161
Iteration 372/1000 | Loss: 0.00002161
Iteration 373/1000 | Loss: 0.00002161
Iteration 374/1000 | Loss: 0.00002161
Iteration 375/1000 | Loss: 0.00002161
Iteration 376/1000 | Loss: 0.00002161
Iteration 377/1000 | Loss: 0.00002161
Iteration 378/1000 | Loss: 0.00002161
Iteration 379/1000 | Loss: 0.00002161
Iteration 380/1000 | Loss: 0.00002160
Iteration 381/1000 | Loss: 0.00002160
Iteration 382/1000 | Loss: 0.00002160
Iteration 383/1000 | Loss: 0.00002160
Iteration 384/1000 | Loss: 0.00002160
Iteration 385/1000 | Loss: 0.00002160
Iteration 386/1000 | Loss: 0.00002159
Iteration 387/1000 | Loss: 0.00002159
Iteration 388/1000 | Loss: 0.00002159
Iteration 389/1000 | Loss: 0.00002159
Iteration 390/1000 | Loss: 0.00002159
Iteration 391/1000 | Loss: 0.00002159
Iteration 392/1000 | Loss: 0.00002159
Iteration 393/1000 | Loss: 0.00002159
Iteration 394/1000 | Loss: 0.00002159
Iteration 395/1000 | Loss: 0.00002159
Iteration 396/1000 | Loss: 0.00002159
Iteration 397/1000 | Loss: 0.00002159
Iteration 398/1000 | Loss: 0.00002159
Iteration 399/1000 | Loss: 0.00002159
Iteration 400/1000 | Loss: 0.00002159
Iteration 401/1000 | Loss: 0.00002158
Iteration 402/1000 | Loss: 0.00002158
Iteration 403/1000 | Loss: 0.00002158
Iteration 404/1000 | Loss: 0.00002158
Iteration 405/1000 | Loss: 0.00002158
Iteration 406/1000 | Loss: 0.00002158
Iteration 407/1000 | Loss: 0.00002158
Iteration 408/1000 | Loss: 0.00002158
Iteration 409/1000 | Loss: 0.00002158
Iteration 410/1000 | Loss: 0.00002158
Iteration 411/1000 | Loss: 0.00002158
Iteration 412/1000 | Loss: 0.00002158
Iteration 413/1000 | Loss: 0.00002158
Iteration 414/1000 | Loss: 0.00002158
Iteration 415/1000 | Loss: 0.00002158
Iteration 416/1000 | Loss: 0.00002158
Iteration 417/1000 | Loss: 0.00002158
Iteration 418/1000 | Loss: 0.00002158
Iteration 419/1000 | Loss: 0.00002158
Iteration 420/1000 | Loss: 0.00002157
Iteration 421/1000 | Loss: 0.00002157
Iteration 422/1000 | Loss: 0.00002157
Iteration 423/1000 | Loss: 0.00002157
Iteration 424/1000 | Loss: 0.00002157
Iteration 425/1000 | Loss: 0.00002157
Iteration 426/1000 | Loss: 0.00002157
Iteration 427/1000 | Loss: 0.00002157
Iteration 428/1000 | Loss: 0.00002157
Iteration 429/1000 | Loss: 0.00002157
Iteration 430/1000 | Loss: 0.00002157
Iteration 431/1000 | Loss: 0.00002157
Iteration 432/1000 | Loss: 0.00002157
Iteration 433/1000 | Loss: 0.00002157
Iteration 434/1000 | Loss: 0.00002157
Iteration 435/1000 | Loss: 0.00002157
Iteration 436/1000 | Loss: 0.00002157
Iteration 437/1000 | Loss: 0.00002157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 437. Stopping optimization.
Last 5 losses: [2.1569478121818975e-05, 2.1569478121818975e-05, 2.1569478121818975e-05, 2.1569478121818975e-05, 2.1569478121818975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1569478121818975e-05

Optimization complete. Final v2v error: 3.871234178543091 mm

Highest mean error: 5.695481300354004 mm for frame 89

Lowest mean error: 3.5395700931549072 mm for frame 41

Saving results

Total time: 271.2020175457001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00729589
Iteration 2/25 | Loss: 0.00183278
Iteration 3/25 | Loss: 0.00146590
Iteration 4/25 | Loss: 0.00140793
Iteration 5/25 | Loss: 0.00139242
Iteration 6/25 | Loss: 0.00139059
Iteration 7/25 | Loss: 0.00138761
Iteration 8/25 | Loss: 0.00138545
Iteration 9/25 | Loss: 0.00138435
Iteration 10/25 | Loss: 0.00138394
Iteration 11/25 | Loss: 0.00138385
Iteration 12/25 | Loss: 0.00138378
Iteration 13/25 | Loss: 0.00138542
Iteration 14/25 | Loss: 0.00138122
Iteration 15/25 | Loss: 0.00137901
Iteration 16/25 | Loss: 0.00140126
Iteration 17/25 | Loss: 0.00137621
Iteration 18/25 | Loss: 0.00137536
Iteration 19/25 | Loss: 0.00137122
Iteration 20/25 | Loss: 0.00137227
Iteration 21/25 | Loss: 0.00137277
Iteration 22/25 | Loss: 0.00137264
Iteration 23/25 | Loss: 0.00136921
Iteration 24/25 | Loss: 0.00137868
Iteration 25/25 | Loss: 0.00137411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.36924267
Iteration 2/25 | Loss: 0.00105085
Iteration 3/25 | Loss: 0.00105046
Iteration 4/25 | Loss: 0.00105046
Iteration 5/25 | Loss: 0.00105045
Iteration 6/25 | Loss: 0.00105045
Iteration 7/25 | Loss: 0.00105045
Iteration 8/25 | Loss: 0.00105045
Iteration 9/25 | Loss: 0.00105045
Iteration 10/25 | Loss: 0.00105045
Iteration 11/25 | Loss: 0.00105045
Iteration 12/25 | Loss: 0.00105045
Iteration 13/25 | Loss: 0.00105045
Iteration 14/25 | Loss: 0.00105045
Iteration 15/25 | Loss: 0.00105045
Iteration 16/25 | Loss: 0.00105045
Iteration 17/25 | Loss: 0.00105045
Iteration 18/25 | Loss: 0.00105045
Iteration 19/25 | Loss: 0.00105045
Iteration 20/25 | Loss: 0.00105045
Iteration 21/25 | Loss: 0.00105045
Iteration 22/25 | Loss: 0.00105045
Iteration 23/25 | Loss: 0.00105045
Iteration 24/25 | Loss: 0.00105045
Iteration 25/25 | Loss: 0.00105045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105045
Iteration 2/1000 | Loss: 0.00008374
Iteration 3/1000 | Loss: 0.00022581
Iteration 4/1000 | Loss: 0.00020498
Iteration 5/1000 | Loss: 0.00022698
Iteration 6/1000 | Loss: 0.00017397
Iteration 7/1000 | Loss: 0.00020195
Iteration 8/1000 | Loss: 0.00015690
Iteration 9/1000 | Loss: 0.00021591
Iteration 10/1000 | Loss: 0.00015776
Iteration 11/1000 | Loss: 0.00013994
Iteration 12/1000 | Loss: 0.00016644
Iteration 13/1000 | Loss: 0.00013062
Iteration 14/1000 | Loss: 0.00013640
Iteration 15/1000 | Loss: 0.00003587
Iteration 16/1000 | Loss: 0.00021386
Iteration 17/1000 | Loss: 0.00020159
Iteration 18/1000 | Loss: 0.00012083
Iteration 19/1000 | Loss: 0.00021745
Iteration 20/1000 | Loss: 0.00014725
Iteration 21/1000 | Loss: 0.00021914
Iteration 22/1000 | Loss: 0.00014211
Iteration 23/1000 | Loss: 0.00015868
Iteration 24/1000 | Loss: 0.00006505
Iteration 25/1000 | Loss: 0.00013728
Iteration 26/1000 | Loss: 0.00018681
Iteration 27/1000 | Loss: 0.00015174
Iteration 28/1000 | Loss: 0.00005764
Iteration 29/1000 | Loss: 0.00012370
Iteration 30/1000 | Loss: 0.00014133
Iteration 31/1000 | Loss: 0.00013389
Iteration 32/1000 | Loss: 0.00014700
Iteration 33/1000 | Loss: 0.00013176
Iteration 34/1000 | Loss: 0.00010262
Iteration 35/1000 | Loss: 0.00021087
Iteration 36/1000 | Loss: 0.00007029
Iteration 37/1000 | Loss: 0.00003862
Iteration 38/1000 | Loss: 0.00008372
Iteration 39/1000 | Loss: 0.00017548
Iteration 40/1000 | Loss: 0.00009020
Iteration 41/1000 | Loss: 0.00014015
Iteration 42/1000 | Loss: 0.00010188
Iteration 43/1000 | Loss: 0.00005989
Iteration 44/1000 | Loss: 0.00004806
Iteration 45/1000 | Loss: 0.00014167
Iteration 46/1000 | Loss: 0.00012583
Iteration 47/1000 | Loss: 0.00010733
Iteration 48/1000 | Loss: 0.00008847
Iteration 49/1000 | Loss: 0.00008247
Iteration 50/1000 | Loss: 0.00004747
Iteration 51/1000 | Loss: 0.00003765
Iteration 52/1000 | Loss: 0.00004042
Iteration 53/1000 | Loss: 0.00009725
Iteration 54/1000 | Loss: 0.00007697
Iteration 55/1000 | Loss: 0.00009555
Iteration 56/1000 | Loss: 0.00007560
Iteration 57/1000 | Loss: 0.00012878
Iteration 58/1000 | Loss: 0.00007405
Iteration 59/1000 | Loss: 0.00023769
Iteration 60/1000 | Loss: 0.00004397
Iteration 61/1000 | Loss: 0.00003692
Iteration 62/1000 | Loss: 0.00031388
Iteration 63/1000 | Loss: 0.00009091
Iteration 64/1000 | Loss: 0.00004320
Iteration 65/1000 | Loss: 0.00003226
Iteration 66/1000 | Loss: 0.00003062
Iteration 67/1000 | Loss: 0.00002980
Iteration 68/1000 | Loss: 0.00002909
Iteration 69/1000 | Loss: 0.00002845
Iteration 70/1000 | Loss: 0.00002789
Iteration 71/1000 | Loss: 0.00002755
Iteration 72/1000 | Loss: 0.00002734
Iteration 73/1000 | Loss: 0.00002734
Iteration 74/1000 | Loss: 0.00002728
Iteration 75/1000 | Loss: 0.00002721
Iteration 76/1000 | Loss: 0.00006321
Iteration 77/1000 | Loss: 0.00003839
Iteration 78/1000 | Loss: 0.00003743
Iteration 79/1000 | Loss: 0.00004413
Iteration 80/1000 | Loss: 0.00003636
Iteration 81/1000 | Loss: 0.00004470
Iteration 82/1000 | Loss: 0.00003930
Iteration 83/1000 | Loss: 0.00004327
Iteration 84/1000 | Loss: 0.00004121
Iteration 85/1000 | Loss: 0.00004330
Iteration 86/1000 | Loss: 0.00004258
Iteration 87/1000 | Loss: 0.00004227
Iteration 88/1000 | Loss: 0.00004164
Iteration 89/1000 | Loss: 0.00004140
Iteration 90/1000 | Loss: 0.00003728
Iteration 91/1000 | Loss: 0.00004011
Iteration 92/1000 | Loss: 0.00003565
Iteration 93/1000 | Loss: 0.00003743
Iteration 94/1000 | Loss: 0.00003559
Iteration 95/1000 | Loss: 0.00004084
Iteration 96/1000 | Loss: 0.00005570
Iteration 97/1000 | Loss: 0.00003490
Iteration 98/1000 | Loss: 0.00003067
Iteration 99/1000 | Loss: 0.00002793
Iteration 100/1000 | Loss: 0.00002732
Iteration 101/1000 | Loss: 0.00002691
Iteration 102/1000 | Loss: 0.00002665
Iteration 103/1000 | Loss: 0.00002638
Iteration 104/1000 | Loss: 0.00002610
Iteration 105/1000 | Loss: 0.00002588
Iteration 106/1000 | Loss: 0.00002580
Iteration 107/1000 | Loss: 0.00002576
Iteration 108/1000 | Loss: 0.00002576
Iteration 109/1000 | Loss: 0.00002575
Iteration 110/1000 | Loss: 0.00002575
Iteration 111/1000 | Loss: 0.00002575
Iteration 112/1000 | Loss: 0.00002574
Iteration 113/1000 | Loss: 0.00002574
Iteration 114/1000 | Loss: 0.00002573
Iteration 115/1000 | Loss: 0.00002573
Iteration 116/1000 | Loss: 0.00002570
Iteration 117/1000 | Loss: 0.00002570
Iteration 118/1000 | Loss: 0.00002564
Iteration 119/1000 | Loss: 0.00002564
Iteration 120/1000 | Loss: 0.00002562
Iteration 121/1000 | Loss: 0.00002558
Iteration 122/1000 | Loss: 0.00002558
Iteration 123/1000 | Loss: 0.00002556
Iteration 124/1000 | Loss: 0.00002556
Iteration 125/1000 | Loss: 0.00002556
Iteration 126/1000 | Loss: 0.00002555
Iteration 127/1000 | Loss: 0.00002555
Iteration 128/1000 | Loss: 0.00002555
Iteration 129/1000 | Loss: 0.00002555
Iteration 130/1000 | Loss: 0.00002555
Iteration 131/1000 | Loss: 0.00002554
Iteration 132/1000 | Loss: 0.00002554
Iteration 133/1000 | Loss: 0.00002554
Iteration 134/1000 | Loss: 0.00002553
Iteration 135/1000 | Loss: 0.00002553
Iteration 136/1000 | Loss: 0.00002553
Iteration 137/1000 | Loss: 0.00002552
Iteration 138/1000 | Loss: 0.00002552
Iteration 139/1000 | Loss: 0.00002552
Iteration 140/1000 | Loss: 0.00002552
Iteration 141/1000 | Loss: 0.00002552
Iteration 142/1000 | Loss: 0.00002552
Iteration 143/1000 | Loss: 0.00002551
Iteration 144/1000 | Loss: 0.00002551
Iteration 145/1000 | Loss: 0.00002551
Iteration 146/1000 | Loss: 0.00002550
Iteration 147/1000 | Loss: 0.00002550
Iteration 148/1000 | Loss: 0.00002550
Iteration 149/1000 | Loss: 0.00002549
Iteration 150/1000 | Loss: 0.00002549
Iteration 151/1000 | Loss: 0.00002549
Iteration 152/1000 | Loss: 0.00002549
Iteration 153/1000 | Loss: 0.00002549
Iteration 154/1000 | Loss: 0.00002548
Iteration 155/1000 | Loss: 0.00002548
Iteration 156/1000 | Loss: 0.00002548
Iteration 157/1000 | Loss: 0.00002547
Iteration 158/1000 | Loss: 0.00002547
Iteration 159/1000 | Loss: 0.00002547
Iteration 160/1000 | Loss: 0.00002547
Iteration 161/1000 | Loss: 0.00002547
Iteration 162/1000 | Loss: 0.00002546
Iteration 163/1000 | Loss: 0.00002546
Iteration 164/1000 | Loss: 0.00002545
Iteration 165/1000 | Loss: 0.00002545
Iteration 166/1000 | Loss: 0.00002543
Iteration 167/1000 | Loss: 0.00002542
Iteration 168/1000 | Loss: 0.00002542
Iteration 169/1000 | Loss: 0.00002541
Iteration 170/1000 | Loss: 0.00002541
Iteration 171/1000 | Loss: 0.00002541
Iteration 172/1000 | Loss: 0.00002540
Iteration 173/1000 | Loss: 0.00002540
Iteration 174/1000 | Loss: 0.00002540
Iteration 175/1000 | Loss: 0.00002540
Iteration 176/1000 | Loss: 0.00002540
Iteration 177/1000 | Loss: 0.00002539
Iteration 178/1000 | Loss: 0.00002539
Iteration 179/1000 | Loss: 0.00002539
Iteration 180/1000 | Loss: 0.00002539
Iteration 181/1000 | Loss: 0.00002539
Iteration 182/1000 | Loss: 0.00002539
Iteration 183/1000 | Loss: 0.00002539
Iteration 184/1000 | Loss: 0.00002538
Iteration 185/1000 | Loss: 0.00002538
Iteration 186/1000 | Loss: 0.00002538
Iteration 187/1000 | Loss: 0.00002538
Iteration 188/1000 | Loss: 0.00002538
Iteration 189/1000 | Loss: 0.00002538
Iteration 190/1000 | Loss: 0.00002538
Iteration 191/1000 | Loss: 0.00002538
Iteration 192/1000 | Loss: 0.00002538
Iteration 193/1000 | Loss: 0.00002538
Iteration 194/1000 | Loss: 0.00002538
Iteration 195/1000 | Loss: 0.00002538
Iteration 196/1000 | Loss: 0.00002538
Iteration 197/1000 | Loss: 0.00002538
Iteration 198/1000 | Loss: 0.00002538
Iteration 199/1000 | Loss: 0.00002538
Iteration 200/1000 | Loss: 0.00002538
Iteration 201/1000 | Loss: 0.00002538
Iteration 202/1000 | Loss: 0.00002537
Iteration 203/1000 | Loss: 0.00002537
Iteration 204/1000 | Loss: 0.00002537
Iteration 205/1000 | Loss: 0.00002537
Iteration 206/1000 | Loss: 0.00002537
Iteration 207/1000 | Loss: 0.00002537
Iteration 208/1000 | Loss: 0.00002537
Iteration 209/1000 | Loss: 0.00002537
Iteration 210/1000 | Loss: 0.00002537
Iteration 211/1000 | Loss: 0.00002537
Iteration 212/1000 | Loss: 0.00002537
Iteration 213/1000 | Loss: 0.00002537
Iteration 214/1000 | Loss: 0.00002537
Iteration 215/1000 | Loss: 0.00002537
Iteration 216/1000 | Loss: 0.00002537
Iteration 217/1000 | Loss: 0.00002537
Iteration 218/1000 | Loss: 0.00002537
Iteration 219/1000 | Loss: 0.00002537
Iteration 220/1000 | Loss: 0.00002537
Iteration 221/1000 | Loss: 0.00002537
Iteration 222/1000 | Loss: 0.00002537
Iteration 223/1000 | Loss: 0.00002537
Iteration 224/1000 | Loss: 0.00002537
Iteration 225/1000 | Loss: 0.00002537
Iteration 226/1000 | Loss: 0.00002537
Iteration 227/1000 | Loss: 0.00002537
Iteration 228/1000 | Loss: 0.00002537
Iteration 229/1000 | Loss: 0.00002537
Iteration 230/1000 | Loss: 0.00002537
Iteration 231/1000 | Loss: 0.00002537
Iteration 232/1000 | Loss: 0.00002537
Iteration 233/1000 | Loss: 0.00002537
Iteration 234/1000 | Loss: 0.00002537
Iteration 235/1000 | Loss: 0.00002537
Iteration 236/1000 | Loss: 0.00002537
Iteration 237/1000 | Loss: 0.00002537
Iteration 238/1000 | Loss: 0.00002537
Iteration 239/1000 | Loss: 0.00002537
Iteration 240/1000 | Loss: 0.00002537
Iteration 241/1000 | Loss: 0.00002537
Iteration 242/1000 | Loss: 0.00002537
Iteration 243/1000 | Loss: 0.00002537
Iteration 244/1000 | Loss: 0.00002537
Iteration 245/1000 | Loss: 0.00002537
Iteration 246/1000 | Loss: 0.00002537
Iteration 247/1000 | Loss: 0.00002537
Iteration 248/1000 | Loss: 0.00002537
Iteration 249/1000 | Loss: 0.00002537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.537350337661337e-05, 2.537350337661337e-05, 2.537350337661337e-05, 2.537350337661337e-05, 2.537350337661337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.537350337661337e-05

Optimization complete. Final v2v error: 4.142254829406738 mm

Highest mean error: 7.124587535858154 mm for frame 64

Lowest mean error: 3.1537368297576904 mm for frame 109

Saving results

Total time: 195.93253803253174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960848
Iteration 2/25 | Loss: 0.00279880
Iteration 3/25 | Loss: 0.00196688
Iteration 4/25 | Loss: 0.00177261
Iteration 5/25 | Loss: 0.00163367
Iteration 6/25 | Loss: 0.00146514
Iteration 7/25 | Loss: 0.00149489
Iteration 8/25 | Loss: 0.00139493
Iteration 9/25 | Loss: 0.00138045
Iteration 10/25 | Loss: 0.00134610
Iteration 11/25 | Loss: 0.00134772
Iteration 12/25 | Loss: 0.00134939
Iteration 13/25 | Loss: 0.00132293
Iteration 14/25 | Loss: 0.00130973
Iteration 15/25 | Loss: 0.00130938
Iteration 16/25 | Loss: 0.00130814
Iteration 17/25 | Loss: 0.00130245
Iteration 18/25 | Loss: 0.00130148
Iteration 19/25 | Loss: 0.00130138
Iteration 20/25 | Loss: 0.00130134
Iteration 21/25 | Loss: 0.00130134
Iteration 22/25 | Loss: 0.00130134
Iteration 23/25 | Loss: 0.00130134
Iteration 24/25 | Loss: 0.00130134
Iteration 25/25 | Loss: 0.00130134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40090740
Iteration 2/25 | Loss: 0.00083313
Iteration 3/25 | Loss: 0.00078868
Iteration 4/25 | Loss: 0.00078868
Iteration 5/25 | Loss: 0.00078868
Iteration 6/25 | Loss: 0.00078868
Iteration 7/25 | Loss: 0.00078868
Iteration 8/25 | Loss: 0.00078868
Iteration 9/25 | Loss: 0.00078868
Iteration 10/25 | Loss: 0.00078868
Iteration 11/25 | Loss: 0.00078868
Iteration 12/25 | Loss: 0.00078868
Iteration 13/25 | Loss: 0.00078868
Iteration 14/25 | Loss: 0.00078868
Iteration 15/25 | Loss: 0.00078868
Iteration 16/25 | Loss: 0.00078868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007886778330430388, 0.0007886778330430388, 0.0007886778330430388, 0.0007886778330430388, 0.0007886778330430388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007886778330430388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078868
Iteration 2/1000 | Loss: 0.00008027
Iteration 3/1000 | Loss: 0.00006395
Iteration 4/1000 | Loss: 0.00006894
Iteration 5/1000 | Loss: 0.00028527
Iteration 6/1000 | Loss: 0.00005187
Iteration 7/1000 | Loss: 0.00008849
Iteration 8/1000 | Loss: 0.00004381
Iteration 9/1000 | Loss: 0.00006531
Iteration 10/1000 | Loss: 0.00006366
Iteration 11/1000 | Loss: 0.00012999
Iteration 12/1000 | Loss: 0.00018651
Iteration 13/1000 | Loss: 0.00004388
Iteration 14/1000 | Loss: 0.00005469
Iteration 15/1000 | Loss: 0.00004545
Iteration 16/1000 | Loss: 0.00003744
Iteration 17/1000 | Loss: 0.00002672
Iteration 18/1000 | Loss: 0.00008655
Iteration 19/1000 | Loss: 0.00003115
Iteration 20/1000 | Loss: 0.00005252
Iteration 21/1000 | Loss: 0.00002415
Iteration 22/1000 | Loss: 0.00002369
Iteration 23/1000 | Loss: 0.00004291
Iteration 24/1000 | Loss: 0.00004372
Iteration 25/1000 | Loss: 0.00002284
Iteration 26/1000 | Loss: 0.00002549
Iteration 27/1000 | Loss: 0.00003438
Iteration 28/1000 | Loss: 0.00002265
Iteration 29/1000 | Loss: 0.00002210
Iteration 30/1000 | Loss: 0.00002210
Iteration 31/1000 | Loss: 0.00002886
Iteration 32/1000 | Loss: 0.00002191
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002186
Iteration 35/1000 | Loss: 0.00002183
Iteration 36/1000 | Loss: 0.00002183
Iteration 37/1000 | Loss: 0.00002183
Iteration 38/1000 | Loss: 0.00002182
Iteration 39/1000 | Loss: 0.00002939
Iteration 40/1000 | Loss: 0.00002219
Iteration 41/1000 | Loss: 0.00003024
Iteration 42/1000 | Loss: 0.00027022
Iteration 43/1000 | Loss: 0.00002487
Iteration 44/1000 | Loss: 0.00002244
Iteration 45/1000 | Loss: 0.00005597
Iteration 46/1000 | Loss: 0.00002235
Iteration 47/1000 | Loss: 0.00002166
Iteration 48/1000 | Loss: 0.00002165
Iteration 49/1000 | Loss: 0.00002165
Iteration 50/1000 | Loss: 0.00002165
Iteration 51/1000 | Loss: 0.00002165
Iteration 52/1000 | Loss: 0.00002165
Iteration 53/1000 | Loss: 0.00002165
Iteration 54/1000 | Loss: 0.00002164
Iteration 55/1000 | Loss: 0.00002164
Iteration 56/1000 | Loss: 0.00002163
Iteration 57/1000 | Loss: 0.00002161
Iteration 58/1000 | Loss: 0.00002160
Iteration 59/1000 | Loss: 0.00003431
Iteration 60/1000 | Loss: 0.00002295
Iteration 61/1000 | Loss: 0.00002511
Iteration 62/1000 | Loss: 0.00002391
Iteration 63/1000 | Loss: 0.00002489
Iteration 64/1000 | Loss: 0.00002197
Iteration 65/1000 | Loss: 0.00002160
Iteration 66/1000 | Loss: 0.00002197
Iteration 67/1000 | Loss: 0.00002248
Iteration 68/1000 | Loss: 0.00002210
Iteration 69/1000 | Loss: 0.00002829
Iteration 70/1000 | Loss: 0.00002148
Iteration 71/1000 | Loss: 0.00002147
Iteration 72/1000 | Loss: 0.00002147
Iteration 73/1000 | Loss: 0.00002147
Iteration 74/1000 | Loss: 0.00002147
Iteration 75/1000 | Loss: 0.00002147
Iteration 76/1000 | Loss: 0.00002147
Iteration 77/1000 | Loss: 0.00002147
Iteration 78/1000 | Loss: 0.00002147
Iteration 79/1000 | Loss: 0.00002147
Iteration 80/1000 | Loss: 0.00002147
Iteration 81/1000 | Loss: 0.00002147
Iteration 82/1000 | Loss: 0.00002147
Iteration 83/1000 | Loss: 0.00002147
Iteration 84/1000 | Loss: 0.00002146
Iteration 85/1000 | Loss: 0.00002146
Iteration 86/1000 | Loss: 0.00002146
Iteration 87/1000 | Loss: 0.00002146
Iteration 88/1000 | Loss: 0.00002146
Iteration 89/1000 | Loss: 0.00002146
Iteration 90/1000 | Loss: 0.00002146
Iteration 91/1000 | Loss: 0.00002146
Iteration 92/1000 | Loss: 0.00002146
Iteration 93/1000 | Loss: 0.00002146
Iteration 94/1000 | Loss: 0.00002146
Iteration 95/1000 | Loss: 0.00002146
Iteration 96/1000 | Loss: 0.00002146
Iteration 97/1000 | Loss: 0.00002146
Iteration 98/1000 | Loss: 0.00002145
Iteration 99/1000 | Loss: 0.00002145
Iteration 100/1000 | Loss: 0.00002145
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002145
Iteration 107/1000 | Loss: 0.00002145
Iteration 108/1000 | Loss: 0.00002145
Iteration 109/1000 | Loss: 0.00002145
Iteration 110/1000 | Loss: 0.00002145
Iteration 111/1000 | Loss: 0.00002145
Iteration 112/1000 | Loss: 0.00002144
Iteration 113/1000 | Loss: 0.00002144
Iteration 114/1000 | Loss: 0.00002144
Iteration 115/1000 | Loss: 0.00002144
Iteration 116/1000 | Loss: 0.00002144
Iteration 117/1000 | Loss: 0.00002144
Iteration 118/1000 | Loss: 0.00002144
Iteration 119/1000 | Loss: 0.00002143
Iteration 120/1000 | Loss: 0.00002143
Iteration 121/1000 | Loss: 0.00002143
Iteration 122/1000 | Loss: 0.00002143
Iteration 123/1000 | Loss: 0.00002143
Iteration 124/1000 | Loss: 0.00002143
Iteration 125/1000 | Loss: 0.00002143
Iteration 126/1000 | Loss: 0.00002143
Iteration 127/1000 | Loss: 0.00002143
Iteration 128/1000 | Loss: 0.00002143
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002143
Iteration 132/1000 | Loss: 0.00002143
Iteration 133/1000 | Loss: 0.00002142
Iteration 134/1000 | Loss: 0.00002142
Iteration 135/1000 | Loss: 0.00002142
Iteration 136/1000 | Loss: 0.00002142
Iteration 137/1000 | Loss: 0.00002142
Iteration 138/1000 | Loss: 0.00002142
Iteration 139/1000 | Loss: 0.00002142
Iteration 140/1000 | Loss: 0.00002142
Iteration 141/1000 | Loss: 0.00002142
Iteration 142/1000 | Loss: 0.00002142
Iteration 143/1000 | Loss: 0.00002142
Iteration 144/1000 | Loss: 0.00002142
Iteration 145/1000 | Loss: 0.00002142
Iteration 146/1000 | Loss: 0.00002142
Iteration 147/1000 | Loss: 0.00002142
Iteration 148/1000 | Loss: 0.00002142
Iteration 149/1000 | Loss: 0.00002142
Iteration 150/1000 | Loss: 0.00002142
Iteration 151/1000 | Loss: 0.00002142
Iteration 152/1000 | Loss: 0.00002142
Iteration 153/1000 | Loss: 0.00002142
Iteration 154/1000 | Loss: 0.00002141
Iteration 155/1000 | Loss: 0.00002141
Iteration 156/1000 | Loss: 0.00002141
Iteration 157/1000 | Loss: 0.00002141
Iteration 158/1000 | Loss: 0.00002141
Iteration 159/1000 | Loss: 0.00002141
Iteration 160/1000 | Loss: 0.00002141
Iteration 161/1000 | Loss: 0.00002141
Iteration 162/1000 | Loss: 0.00002141
Iteration 163/1000 | Loss: 0.00002141
Iteration 164/1000 | Loss: 0.00002141
Iteration 165/1000 | Loss: 0.00002141
Iteration 166/1000 | Loss: 0.00002141
Iteration 167/1000 | Loss: 0.00002141
Iteration 168/1000 | Loss: 0.00002141
Iteration 169/1000 | Loss: 0.00002140
Iteration 170/1000 | Loss: 0.00002140
Iteration 171/1000 | Loss: 0.00002140
Iteration 172/1000 | Loss: 0.00002139
Iteration 173/1000 | Loss: 0.00002139
Iteration 174/1000 | Loss: 0.00002139
Iteration 175/1000 | Loss: 0.00002139
Iteration 176/1000 | Loss: 0.00002139
Iteration 177/1000 | Loss: 0.00002139
Iteration 178/1000 | Loss: 0.00002139
Iteration 179/1000 | Loss: 0.00002139
Iteration 180/1000 | Loss: 0.00002139
Iteration 181/1000 | Loss: 0.00002139
Iteration 182/1000 | Loss: 0.00002139
Iteration 183/1000 | Loss: 0.00002139
Iteration 184/1000 | Loss: 0.00002138
Iteration 185/1000 | Loss: 0.00002138
Iteration 186/1000 | Loss: 0.00002138
Iteration 187/1000 | Loss: 0.00002138
Iteration 188/1000 | Loss: 0.00002138
Iteration 189/1000 | Loss: 0.00002138
Iteration 190/1000 | Loss: 0.00002138
Iteration 191/1000 | Loss: 0.00002138
Iteration 192/1000 | Loss: 0.00002138
Iteration 193/1000 | Loss: 0.00002138
Iteration 194/1000 | Loss: 0.00002138
Iteration 195/1000 | Loss: 0.00002138
Iteration 196/1000 | Loss: 0.00002138
Iteration 197/1000 | Loss: 0.00002138
Iteration 198/1000 | Loss: 0.00002138
Iteration 199/1000 | Loss: 0.00002138
Iteration 200/1000 | Loss: 0.00003654
Iteration 201/1000 | Loss: 0.00002139
Iteration 202/1000 | Loss: 0.00002138
Iteration 203/1000 | Loss: 0.00002138
Iteration 204/1000 | Loss: 0.00002138
Iteration 205/1000 | Loss: 0.00002138
Iteration 206/1000 | Loss: 0.00002138
Iteration 207/1000 | Loss: 0.00002138
Iteration 208/1000 | Loss: 0.00002137
Iteration 209/1000 | Loss: 0.00002137
Iteration 210/1000 | Loss: 0.00002137
Iteration 211/1000 | Loss: 0.00002137
Iteration 212/1000 | Loss: 0.00002137
Iteration 213/1000 | Loss: 0.00002137
Iteration 214/1000 | Loss: 0.00002137
Iteration 215/1000 | Loss: 0.00002136
Iteration 216/1000 | Loss: 0.00002136
Iteration 217/1000 | Loss: 0.00002136
Iteration 218/1000 | Loss: 0.00002136
Iteration 219/1000 | Loss: 0.00002136
Iteration 220/1000 | Loss: 0.00002136
Iteration 221/1000 | Loss: 0.00002136
Iteration 222/1000 | Loss: 0.00002136
Iteration 223/1000 | Loss: 0.00002136
Iteration 224/1000 | Loss: 0.00002136
Iteration 225/1000 | Loss: 0.00002136
Iteration 226/1000 | Loss: 0.00002136
Iteration 227/1000 | Loss: 0.00002136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.136361217708327e-05, 2.136361217708327e-05, 2.136361217708327e-05, 2.136361217708327e-05, 2.136361217708327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.136361217708327e-05

Optimization complete. Final v2v error: 3.855837345123291 mm

Highest mean error: 4.552226543426514 mm for frame 170

Lowest mean error: 3.359783172607422 mm for frame 56

Saving results

Total time: 123.64668226242065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068861
Iteration 2/25 | Loss: 0.00307133
Iteration 3/25 | Loss: 0.00215269
Iteration 4/25 | Loss: 0.00208248
Iteration 5/25 | Loss: 0.00163480
Iteration 6/25 | Loss: 0.00156043
Iteration 7/25 | Loss: 0.00144421
Iteration 8/25 | Loss: 0.00137533
Iteration 9/25 | Loss: 0.00134440
Iteration 10/25 | Loss: 0.00132713
Iteration 11/25 | Loss: 0.00129303
Iteration 12/25 | Loss: 0.00130053
Iteration 13/25 | Loss: 0.00127743
Iteration 14/25 | Loss: 0.00127698
Iteration 15/25 | Loss: 0.00127672
Iteration 16/25 | Loss: 0.00127662
Iteration 17/25 | Loss: 0.00127662
Iteration 18/25 | Loss: 0.00127661
Iteration 19/25 | Loss: 0.00127661
Iteration 20/25 | Loss: 0.00127661
Iteration 21/25 | Loss: 0.00127661
Iteration 22/25 | Loss: 0.00127661
Iteration 23/25 | Loss: 0.00127661
Iteration 24/25 | Loss: 0.00127661
Iteration 25/25 | Loss: 0.00127661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53612161
Iteration 2/25 | Loss: 0.00083301
Iteration 3/25 | Loss: 0.00083300
Iteration 4/25 | Loss: 0.00083300
Iteration 5/25 | Loss: 0.00083300
Iteration 6/25 | Loss: 0.00083299
Iteration 7/25 | Loss: 0.00083299
Iteration 8/25 | Loss: 0.00083299
Iteration 9/25 | Loss: 0.00083299
Iteration 10/25 | Loss: 0.00083299
Iteration 11/25 | Loss: 0.00083299
Iteration 12/25 | Loss: 0.00083299
Iteration 13/25 | Loss: 0.00083299
Iteration 14/25 | Loss: 0.00083299
Iteration 15/25 | Loss: 0.00083299
Iteration 16/25 | Loss: 0.00083299
Iteration 17/25 | Loss: 0.00083299
Iteration 18/25 | Loss: 0.00083299
Iteration 19/25 | Loss: 0.00083299
Iteration 20/25 | Loss: 0.00083299
Iteration 21/25 | Loss: 0.00083299
Iteration 22/25 | Loss: 0.00083299
Iteration 23/25 | Loss: 0.00083299
Iteration 24/25 | Loss: 0.00083299
Iteration 25/25 | Loss: 0.00083299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083299
Iteration 2/1000 | Loss: 0.00016010
Iteration 3/1000 | Loss: 0.00016104
Iteration 4/1000 | Loss: 0.00002643
Iteration 5/1000 | Loss: 0.00002393
Iteration 6/1000 | Loss: 0.00002265
Iteration 7/1000 | Loss: 0.00002188
Iteration 8/1000 | Loss: 0.00002128
Iteration 9/1000 | Loss: 0.00002083
Iteration 10/1000 | Loss: 0.00002049
Iteration 11/1000 | Loss: 0.00002018
Iteration 12/1000 | Loss: 0.00001993
Iteration 13/1000 | Loss: 0.00001974
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001963
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001937
Iteration 19/1000 | Loss: 0.00001937
Iteration 20/1000 | Loss: 0.00001935
Iteration 21/1000 | Loss: 0.00001934
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00001933
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001931
Iteration 28/1000 | Loss: 0.00001930
Iteration 29/1000 | Loss: 0.00001927
Iteration 30/1000 | Loss: 0.00001921
Iteration 31/1000 | Loss: 0.00001921
Iteration 32/1000 | Loss: 0.00001920
Iteration 33/1000 | Loss: 0.00001919
Iteration 34/1000 | Loss: 0.00001917
Iteration 35/1000 | Loss: 0.00001916
Iteration 36/1000 | Loss: 0.00001915
Iteration 37/1000 | Loss: 0.00001915
Iteration 38/1000 | Loss: 0.00001914
Iteration 39/1000 | Loss: 0.00001914
Iteration 40/1000 | Loss: 0.00001913
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001910
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001908
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001907
Iteration 52/1000 | Loss: 0.00001907
Iteration 53/1000 | Loss: 0.00001907
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001904
Iteration 71/1000 | Loss: 0.00001904
Iteration 72/1000 | Loss: 0.00001903
Iteration 73/1000 | Loss: 0.00001903
Iteration 74/1000 | Loss: 0.00001903
Iteration 75/1000 | Loss: 0.00001902
Iteration 76/1000 | Loss: 0.00001902
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001900
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001897
Iteration 88/1000 | Loss: 0.00001897
Iteration 89/1000 | Loss: 0.00001897
Iteration 90/1000 | Loss: 0.00001896
Iteration 91/1000 | Loss: 0.00001896
Iteration 92/1000 | Loss: 0.00001896
Iteration 93/1000 | Loss: 0.00001896
Iteration 94/1000 | Loss: 0.00001896
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001894
Iteration 99/1000 | Loss: 0.00001894
Iteration 100/1000 | Loss: 0.00001894
Iteration 101/1000 | Loss: 0.00001894
Iteration 102/1000 | Loss: 0.00001893
Iteration 103/1000 | Loss: 0.00001893
Iteration 104/1000 | Loss: 0.00001892
Iteration 105/1000 | Loss: 0.00001892
Iteration 106/1000 | Loss: 0.00001892
Iteration 107/1000 | Loss: 0.00001891
Iteration 108/1000 | Loss: 0.00001891
Iteration 109/1000 | Loss: 0.00001891
Iteration 110/1000 | Loss: 0.00001891
Iteration 111/1000 | Loss: 0.00001891
Iteration 112/1000 | Loss: 0.00001890
Iteration 113/1000 | Loss: 0.00001890
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00001888
Iteration 117/1000 | Loss: 0.00001888
Iteration 118/1000 | Loss: 0.00001888
Iteration 119/1000 | Loss: 0.00001888
Iteration 120/1000 | Loss: 0.00001887
Iteration 121/1000 | Loss: 0.00001887
Iteration 122/1000 | Loss: 0.00001887
Iteration 123/1000 | Loss: 0.00001887
Iteration 124/1000 | Loss: 0.00001887
Iteration 125/1000 | Loss: 0.00001887
Iteration 126/1000 | Loss: 0.00001887
Iteration 127/1000 | Loss: 0.00001887
Iteration 128/1000 | Loss: 0.00001887
Iteration 129/1000 | Loss: 0.00001887
Iteration 130/1000 | Loss: 0.00001887
Iteration 131/1000 | Loss: 0.00001886
Iteration 132/1000 | Loss: 0.00001886
Iteration 133/1000 | Loss: 0.00001886
Iteration 134/1000 | Loss: 0.00001886
Iteration 135/1000 | Loss: 0.00001886
Iteration 136/1000 | Loss: 0.00001886
Iteration 137/1000 | Loss: 0.00001886
Iteration 138/1000 | Loss: 0.00001886
Iteration 139/1000 | Loss: 0.00001886
Iteration 140/1000 | Loss: 0.00001885
Iteration 141/1000 | Loss: 0.00001885
Iteration 142/1000 | Loss: 0.00001885
Iteration 143/1000 | Loss: 0.00001885
Iteration 144/1000 | Loss: 0.00001885
Iteration 145/1000 | Loss: 0.00001884
Iteration 146/1000 | Loss: 0.00001884
Iteration 147/1000 | Loss: 0.00001884
Iteration 148/1000 | Loss: 0.00001884
Iteration 149/1000 | Loss: 0.00001884
Iteration 150/1000 | Loss: 0.00001884
Iteration 151/1000 | Loss: 0.00001884
Iteration 152/1000 | Loss: 0.00001884
Iteration 153/1000 | Loss: 0.00001884
Iteration 154/1000 | Loss: 0.00001884
Iteration 155/1000 | Loss: 0.00001884
Iteration 156/1000 | Loss: 0.00001884
Iteration 157/1000 | Loss: 0.00001884
Iteration 158/1000 | Loss: 0.00001884
Iteration 159/1000 | Loss: 0.00001883
Iteration 160/1000 | Loss: 0.00001883
Iteration 161/1000 | Loss: 0.00001883
Iteration 162/1000 | Loss: 0.00001883
Iteration 163/1000 | Loss: 0.00001883
Iteration 164/1000 | Loss: 0.00001883
Iteration 165/1000 | Loss: 0.00001883
Iteration 166/1000 | Loss: 0.00001883
Iteration 167/1000 | Loss: 0.00001883
Iteration 168/1000 | Loss: 0.00001883
Iteration 169/1000 | Loss: 0.00001883
Iteration 170/1000 | Loss: 0.00001883
Iteration 171/1000 | Loss: 0.00001883
Iteration 172/1000 | Loss: 0.00001883
Iteration 173/1000 | Loss: 0.00001883
Iteration 174/1000 | Loss: 0.00001883
Iteration 175/1000 | Loss: 0.00001883
Iteration 176/1000 | Loss: 0.00001883
Iteration 177/1000 | Loss: 0.00001883
Iteration 178/1000 | Loss: 0.00001882
Iteration 179/1000 | Loss: 0.00001882
Iteration 180/1000 | Loss: 0.00001882
Iteration 181/1000 | Loss: 0.00001882
Iteration 182/1000 | Loss: 0.00001882
Iteration 183/1000 | Loss: 0.00001882
Iteration 184/1000 | Loss: 0.00001882
Iteration 185/1000 | Loss: 0.00001882
Iteration 186/1000 | Loss: 0.00001882
Iteration 187/1000 | Loss: 0.00001882
Iteration 188/1000 | Loss: 0.00001882
Iteration 189/1000 | Loss: 0.00001882
Iteration 190/1000 | Loss: 0.00001882
Iteration 191/1000 | Loss: 0.00001882
Iteration 192/1000 | Loss: 0.00001882
Iteration 193/1000 | Loss: 0.00001882
Iteration 194/1000 | Loss: 0.00001882
Iteration 195/1000 | Loss: 0.00001882
Iteration 196/1000 | Loss: 0.00001882
Iteration 197/1000 | Loss: 0.00001882
Iteration 198/1000 | Loss: 0.00001882
Iteration 199/1000 | Loss: 0.00001882
Iteration 200/1000 | Loss: 0.00001882
Iteration 201/1000 | Loss: 0.00001882
Iteration 202/1000 | Loss: 0.00001881
Iteration 203/1000 | Loss: 0.00001881
Iteration 204/1000 | Loss: 0.00001881
Iteration 205/1000 | Loss: 0.00001881
Iteration 206/1000 | Loss: 0.00001881
Iteration 207/1000 | Loss: 0.00001881
Iteration 208/1000 | Loss: 0.00001881
Iteration 209/1000 | Loss: 0.00001881
Iteration 210/1000 | Loss: 0.00001881
Iteration 211/1000 | Loss: 0.00001881
Iteration 212/1000 | Loss: 0.00001881
Iteration 213/1000 | Loss: 0.00001881
Iteration 214/1000 | Loss: 0.00001881
Iteration 215/1000 | Loss: 0.00001881
Iteration 216/1000 | Loss: 0.00001881
Iteration 217/1000 | Loss: 0.00001881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.8810955225490034e-05, 1.8810955225490034e-05, 1.8810955225490034e-05, 1.8810955225490034e-05, 1.8810955225490034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8810955225490034e-05

Optimization complete. Final v2v error: 3.6176247596740723 mm

Highest mean error: 4.6445183753967285 mm for frame 95

Lowest mean error: 3.132002592086792 mm for frame 158

Saving results

Total time: 66.15503144264221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796819
Iteration 2/25 | Loss: 0.00135438
Iteration 3/25 | Loss: 0.00126476
Iteration 4/25 | Loss: 0.00125093
Iteration 5/25 | Loss: 0.00124707
Iteration 6/25 | Loss: 0.00124672
Iteration 7/25 | Loss: 0.00124672
Iteration 8/25 | Loss: 0.00124672
Iteration 9/25 | Loss: 0.00124672
Iteration 10/25 | Loss: 0.00124672
Iteration 11/25 | Loss: 0.00124672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012467228807508945, 0.0012467228807508945, 0.0012467228807508945, 0.0012467228807508945, 0.0012467228807508945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012467228807508945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.54424858
Iteration 2/25 | Loss: 0.00082114
Iteration 3/25 | Loss: 0.00082112
Iteration 4/25 | Loss: 0.00082112
Iteration 5/25 | Loss: 0.00082112
Iteration 6/25 | Loss: 0.00082112
Iteration 7/25 | Loss: 0.00082112
Iteration 8/25 | Loss: 0.00082112
Iteration 9/25 | Loss: 0.00082112
Iteration 10/25 | Loss: 0.00082112
Iteration 11/25 | Loss: 0.00082112
Iteration 12/25 | Loss: 0.00082112
Iteration 13/25 | Loss: 0.00082112
Iteration 14/25 | Loss: 0.00082112
Iteration 15/25 | Loss: 0.00082112
Iteration 16/25 | Loss: 0.00082112
Iteration 17/25 | Loss: 0.00082112
Iteration 18/25 | Loss: 0.00082112
Iteration 19/25 | Loss: 0.00082112
Iteration 20/25 | Loss: 0.00082112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008211155654862523, 0.0008211155654862523, 0.0008211155654862523, 0.0008211155654862523, 0.0008211155654862523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008211155654862523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082112
Iteration 2/1000 | Loss: 0.00003567
Iteration 3/1000 | Loss: 0.00002587
Iteration 4/1000 | Loss: 0.00002164
Iteration 5/1000 | Loss: 0.00002046
Iteration 6/1000 | Loss: 0.00001933
Iteration 7/1000 | Loss: 0.00001868
Iteration 8/1000 | Loss: 0.00001818
Iteration 9/1000 | Loss: 0.00001778
Iteration 10/1000 | Loss: 0.00001746
Iteration 11/1000 | Loss: 0.00001727
Iteration 12/1000 | Loss: 0.00001712
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001705
Iteration 15/1000 | Loss: 0.00001704
Iteration 16/1000 | Loss: 0.00001703
Iteration 17/1000 | Loss: 0.00001702
Iteration 18/1000 | Loss: 0.00001700
Iteration 19/1000 | Loss: 0.00001693
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001685
Iteration 22/1000 | Loss: 0.00001679
Iteration 23/1000 | Loss: 0.00001676
Iteration 24/1000 | Loss: 0.00001675
Iteration 25/1000 | Loss: 0.00001675
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001674
Iteration 28/1000 | Loss: 0.00001673
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001672
Iteration 31/1000 | Loss: 0.00001672
Iteration 32/1000 | Loss: 0.00001672
Iteration 33/1000 | Loss: 0.00001671
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001670
Iteration 38/1000 | Loss: 0.00001670
Iteration 39/1000 | Loss: 0.00001667
Iteration 40/1000 | Loss: 0.00001667
Iteration 41/1000 | Loss: 0.00001667
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001667
Iteration 44/1000 | Loss: 0.00001667
Iteration 45/1000 | Loss: 0.00001666
Iteration 46/1000 | Loss: 0.00001665
Iteration 47/1000 | Loss: 0.00001665
Iteration 48/1000 | Loss: 0.00001664
Iteration 49/1000 | Loss: 0.00001664
Iteration 50/1000 | Loss: 0.00001663
Iteration 51/1000 | Loss: 0.00001663
Iteration 52/1000 | Loss: 0.00001663
Iteration 53/1000 | Loss: 0.00001662
Iteration 54/1000 | Loss: 0.00001662
Iteration 55/1000 | Loss: 0.00001661
Iteration 56/1000 | Loss: 0.00001661
Iteration 57/1000 | Loss: 0.00001661
Iteration 58/1000 | Loss: 0.00001661
Iteration 59/1000 | Loss: 0.00001660
Iteration 60/1000 | Loss: 0.00001660
Iteration 61/1000 | Loss: 0.00001660
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001659
Iteration 64/1000 | Loss: 0.00001659
Iteration 65/1000 | Loss: 0.00001659
Iteration 66/1000 | Loss: 0.00001658
Iteration 67/1000 | Loss: 0.00001658
Iteration 68/1000 | Loss: 0.00001658
Iteration 69/1000 | Loss: 0.00001657
Iteration 70/1000 | Loss: 0.00001657
Iteration 71/1000 | Loss: 0.00001656
Iteration 72/1000 | Loss: 0.00001656
Iteration 73/1000 | Loss: 0.00001656
Iteration 74/1000 | Loss: 0.00001655
Iteration 75/1000 | Loss: 0.00001655
Iteration 76/1000 | Loss: 0.00001654
Iteration 77/1000 | Loss: 0.00001654
Iteration 78/1000 | Loss: 0.00001654
Iteration 79/1000 | Loss: 0.00001654
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001654
Iteration 83/1000 | Loss: 0.00001654
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001653
Iteration 86/1000 | Loss: 0.00001653
Iteration 87/1000 | Loss: 0.00001653
Iteration 88/1000 | Loss: 0.00001653
Iteration 89/1000 | Loss: 0.00001653
Iteration 90/1000 | Loss: 0.00001653
Iteration 91/1000 | Loss: 0.00001653
Iteration 92/1000 | Loss: 0.00001653
Iteration 93/1000 | Loss: 0.00001652
Iteration 94/1000 | Loss: 0.00001652
Iteration 95/1000 | Loss: 0.00001652
Iteration 96/1000 | Loss: 0.00001651
Iteration 97/1000 | Loss: 0.00001651
Iteration 98/1000 | Loss: 0.00001651
Iteration 99/1000 | Loss: 0.00001651
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001650
Iteration 102/1000 | Loss: 0.00001650
Iteration 103/1000 | Loss: 0.00001650
Iteration 104/1000 | Loss: 0.00001650
Iteration 105/1000 | Loss: 0.00001650
Iteration 106/1000 | Loss: 0.00001650
Iteration 107/1000 | Loss: 0.00001650
Iteration 108/1000 | Loss: 0.00001650
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001649
Iteration 111/1000 | Loss: 0.00001649
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001649
Iteration 114/1000 | Loss: 0.00001649
Iteration 115/1000 | Loss: 0.00001648
Iteration 116/1000 | Loss: 0.00001648
Iteration 117/1000 | Loss: 0.00001648
Iteration 118/1000 | Loss: 0.00001648
Iteration 119/1000 | Loss: 0.00001648
Iteration 120/1000 | Loss: 0.00001648
Iteration 121/1000 | Loss: 0.00001648
Iteration 122/1000 | Loss: 0.00001647
Iteration 123/1000 | Loss: 0.00001647
Iteration 124/1000 | Loss: 0.00001647
Iteration 125/1000 | Loss: 0.00001647
Iteration 126/1000 | Loss: 0.00001646
Iteration 127/1000 | Loss: 0.00001646
Iteration 128/1000 | Loss: 0.00001646
Iteration 129/1000 | Loss: 0.00001646
Iteration 130/1000 | Loss: 0.00001645
Iteration 131/1000 | Loss: 0.00001645
Iteration 132/1000 | Loss: 0.00001645
Iteration 133/1000 | Loss: 0.00001645
Iteration 134/1000 | Loss: 0.00001644
Iteration 135/1000 | Loss: 0.00001644
Iteration 136/1000 | Loss: 0.00001644
Iteration 137/1000 | Loss: 0.00001644
Iteration 138/1000 | Loss: 0.00001644
Iteration 139/1000 | Loss: 0.00001643
Iteration 140/1000 | Loss: 0.00001643
Iteration 141/1000 | Loss: 0.00001643
Iteration 142/1000 | Loss: 0.00001642
Iteration 143/1000 | Loss: 0.00001642
Iteration 144/1000 | Loss: 0.00001642
Iteration 145/1000 | Loss: 0.00001642
Iteration 146/1000 | Loss: 0.00001642
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001642
Iteration 149/1000 | Loss: 0.00001641
Iteration 150/1000 | Loss: 0.00001641
Iteration 151/1000 | Loss: 0.00001641
Iteration 152/1000 | Loss: 0.00001641
Iteration 153/1000 | Loss: 0.00001641
Iteration 154/1000 | Loss: 0.00001641
Iteration 155/1000 | Loss: 0.00001641
Iteration 156/1000 | Loss: 0.00001641
Iteration 157/1000 | Loss: 0.00001641
Iteration 158/1000 | Loss: 0.00001641
Iteration 159/1000 | Loss: 0.00001641
Iteration 160/1000 | Loss: 0.00001641
Iteration 161/1000 | Loss: 0.00001640
Iteration 162/1000 | Loss: 0.00001640
Iteration 163/1000 | Loss: 0.00001640
Iteration 164/1000 | Loss: 0.00001640
Iteration 165/1000 | Loss: 0.00001640
Iteration 166/1000 | Loss: 0.00001640
Iteration 167/1000 | Loss: 0.00001640
Iteration 168/1000 | Loss: 0.00001640
Iteration 169/1000 | Loss: 0.00001640
Iteration 170/1000 | Loss: 0.00001640
Iteration 171/1000 | Loss: 0.00001640
Iteration 172/1000 | Loss: 0.00001639
Iteration 173/1000 | Loss: 0.00001639
Iteration 174/1000 | Loss: 0.00001639
Iteration 175/1000 | Loss: 0.00001639
Iteration 176/1000 | Loss: 0.00001639
Iteration 177/1000 | Loss: 0.00001639
Iteration 178/1000 | Loss: 0.00001639
Iteration 179/1000 | Loss: 0.00001639
Iteration 180/1000 | Loss: 0.00001639
Iteration 181/1000 | Loss: 0.00001639
Iteration 182/1000 | Loss: 0.00001639
Iteration 183/1000 | Loss: 0.00001638
Iteration 184/1000 | Loss: 0.00001638
Iteration 185/1000 | Loss: 0.00001638
Iteration 186/1000 | Loss: 0.00001638
Iteration 187/1000 | Loss: 0.00001638
Iteration 188/1000 | Loss: 0.00001638
Iteration 189/1000 | Loss: 0.00001638
Iteration 190/1000 | Loss: 0.00001638
Iteration 191/1000 | Loss: 0.00001638
Iteration 192/1000 | Loss: 0.00001638
Iteration 193/1000 | Loss: 0.00001638
Iteration 194/1000 | Loss: 0.00001638
Iteration 195/1000 | Loss: 0.00001638
Iteration 196/1000 | Loss: 0.00001638
Iteration 197/1000 | Loss: 0.00001638
Iteration 198/1000 | Loss: 0.00001637
Iteration 199/1000 | Loss: 0.00001637
Iteration 200/1000 | Loss: 0.00001637
Iteration 201/1000 | Loss: 0.00001637
Iteration 202/1000 | Loss: 0.00001637
Iteration 203/1000 | Loss: 0.00001637
Iteration 204/1000 | Loss: 0.00001637
Iteration 205/1000 | Loss: 0.00001637
Iteration 206/1000 | Loss: 0.00001637
Iteration 207/1000 | Loss: 0.00001637
Iteration 208/1000 | Loss: 0.00001637
Iteration 209/1000 | Loss: 0.00001637
Iteration 210/1000 | Loss: 0.00001637
Iteration 211/1000 | Loss: 0.00001637
Iteration 212/1000 | Loss: 0.00001637
Iteration 213/1000 | Loss: 0.00001637
Iteration 214/1000 | Loss: 0.00001637
Iteration 215/1000 | Loss: 0.00001637
Iteration 216/1000 | Loss: 0.00001637
Iteration 217/1000 | Loss: 0.00001636
Iteration 218/1000 | Loss: 0.00001636
Iteration 219/1000 | Loss: 0.00001636
Iteration 220/1000 | Loss: 0.00001636
Iteration 221/1000 | Loss: 0.00001636
Iteration 222/1000 | Loss: 0.00001636
Iteration 223/1000 | Loss: 0.00001636
Iteration 224/1000 | Loss: 0.00001636
Iteration 225/1000 | Loss: 0.00001636
Iteration 226/1000 | Loss: 0.00001636
Iteration 227/1000 | Loss: 0.00001636
Iteration 228/1000 | Loss: 0.00001636
Iteration 229/1000 | Loss: 0.00001636
Iteration 230/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.6362813767045736e-05, 1.6362813767045736e-05, 1.6362813767045736e-05, 1.6362813767045736e-05, 1.6362813767045736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6362813767045736e-05

Optimization complete. Final v2v error: 3.400409698486328 mm

Highest mean error: 4.086538314819336 mm for frame 146

Lowest mean error: 3.081474542617798 mm for frame 37

Saving results

Total time: 41.48404097557068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786777
Iteration 2/25 | Loss: 0.00171749
Iteration 3/25 | Loss: 0.00141202
Iteration 4/25 | Loss: 0.00137589
Iteration 5/25 | Loss: 0.00137280
Iteration 6/25 | Loss: 0.00137280
Iteration 7/25 | Loss: 0.00137280
Iteration 8/25 | Loss: 0.00137280
Iteration 9/25 | Loss: 0.00137280
Iteration 10/25 | Loss: 0.00137280
Iteration 11/25 | Loss: 0.00137280
Iteration 12/25 | Loss: 0.00137280
Iteration 13/25 | Loss: 0.00137280
Iteration 14/25 | Loss: 0.00137280
Iteration 15/25 | Loss: 0.00137280
Iteration 16/25 | Loss: 0.00137280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013727974146604538, 0.0013727974146604538, 0.0013727974146604538, 0.0013727974146604538, 0.0013727974146604538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013727974146604538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46913493
Iteration 2/25 | Loss: 0.00074469
Iteration 3/25 | Loss: 0.00074468
Iteration 4/25 | Loss: 0.00074467
Iteration 5/25 | Loss: 0.00074467
Iteration 6/25 | Loss: 0.00074467
Iteration 7/25 | Loss: 0.00074467
Iteration 8/25 | Loss: 0.00074467
Iteration 9/25 | Loss: 0.00074467
Iteration 10/25 | Loss: 0.00074467
Iteration 11/25 | Loss: 0.00074467
Iteration 12/25 | Loss: 0.00074467
Iteration 13/25 | Loss: 0.00074467
Iteration 14/25 | Loss: 0.00074467
Iteration 15/25 | Loss: 0.00074467
Iteration 16/25 | Loss: 0.00074467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007446730160154402, 0.0007446730160154402, 0.0007446730160154402, 0.0007446730160154402, 0.0007446730160154402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007446730160154402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074467
Iteration 2/1000 | Loss: 0.00007150
Iteration 3/1000 | Loss: 0.00003829
Iteration 4/1000 | Loss: 0.00003504
Iteration 5/1000 | Loss: 0.00003363
Iteration 6/1000 | Loss: 0.00003257
Iteration 7/1000 | Loss: 0.00003180
Iteration 8/1000 | Loss: 0.00003103
Iteration 9/1000 | Loss: 0.00003040
Iteration 10/1000 | Loss: 0.00002995
Iteration 11/1000 | Loss: 0.00002968
Iteration 12/1000 | Loss: 0.00002947
Iteration 13/1000 | Loss: 0.00002930
Iteration 14/1000 | Loss: 0.00002922
Iteration 15/1000 | Loss: 0.00002920
Iteration 16/1000 | Loss: 0.00002919
Iteration 17/1000 | Loss: 0.00002916
Iteration 18/1000 | Loss: 0.00002913
Iteration 19/1000 | Loss: 0.00002913
Iteration 20/1000 | Loss: 0.00002910
Iteration 21/1000 | Loss: 0.00002910
Iteration 22/1000 | Loss: 0.00002910
Iteration 23/1000 | Loss: 0.00002910
Iteration 24/1000 | Loss: 0.00002910
Iteration 25/1000 | Loss: 0.00002910
Iteration 26/1000 | Loss: 0.00002909
Iteration 27/1000 | Loss: 0.00002909
Iteration 28/1000 | Loss: 0.00002909
Iteration 29/1000 | Loss: 0.00002908
Iteration 30/1000 | Loss: 0.00002908
Iteration 31/1000 | Loss: 0.00002908
Iteration 32/1000 | Loss: 0.00002908
Iteration 33/1000 | Loss: 0.00002907
Iteration 34/1000 | Loss: 0.00002907
Iteration 35/1000 | Loss: 0.00002907
Iteration 36/1000 | Loss: 0.00002907
Iteration 37/1000 | Loss: 0.00002907
Iteration 38/1000 | Loss: 0.00002907
Iteration 39/1000 | Loss: 0.00002906
Iteration 40/1000 | Loss: 0.00002906
Iteration 41/1000 | Loss: 0.00002906
Iteration 42/1000 | Loss: 0.00002906
Iteration 43/1000 | Loss: 0.00002906
Iteration 44/1000 | Loss: 0.00002905
Iteration 45/1000 | Loss: 0.00002905
Iteration 46/1000 | Loss: 0.00002904
Iteration 47/1000 | Loss: 0.00002904
Iteration 48/1000 | Loss: 0.00002904
Iteration 49/1000 | Loss: 0.00002904
Iteration 50/1000 | Loss: 0.00002904
Iteration 51/1000 | Loss: 0.00002904
Iteration 52/1000 | Loss: 0.00002903
Iteration 53/1000 | Loss: 0.00002903
Iteration 54/1000 | Loss: 0.00002903
Iteration 55/1000 | Loss: 0.00002903
Iteration 56/1000 | Loss: 0.00002903
Iteration 57/1000 | Loss: 0.00002902
Iteration 58/1000 | Loss: 0.00002902
Iteration 59/1000 | Loss: 0.00002902
Iteration 60/1000 | Loss: 0.00002902
Iteration 61/1000 | Loss: 0.00002902
Iteration 62/1000 | Loss: 0.00002901
Iteration 63/1000 | Loss: 0.00002901
Iteration 64/1000 | Loss: 0.00002901
Iteration 65/1000 | Loss: 0.00002901
Iteration 66/1000 | Loss: 0.00002900
Iteration 67/1000 | Loss: 0.00002900
Iteration 68/1000 | Loss: 0.00002900
Iteration 69/1000 | Loss: 0.00002900
Iteration 70/1000 | Loss: 0.00002900
Iteration 71/1000 | Loss: 0.00002900
Iteration 72/1000 | Loss: 0.00002900
Iteration 73/1000 | Loss: 0.00002900
Iteration 74/1000 | Loss: 0.00002900
Iteration 75/1000 | Loss: 0.00002900
Iteration 76/1000 | Loss: 0.00002900
Iteration 77/1000 | Loss: 0.00002899
Iteration 78/1000 | Loss: 0.00002899
Iteration 79/1000 | Loss: 0.00002899
Iteration 80/1000 | Loss: 0.00002899
Iteration 81/1000 | Loss: 0.00002899
Iteration 82/1000 | Loss: 0.00002898
Iteration 83/1000 | Loss: 0.00002898
Iteration 84/1000 | Loss: 0.00002898
Iteration 85/1000 | Loss: 0.00002898
Iteration 86/1000 | Loss: 0.00002898
Iteration 87/1000 | Loss: 0.00002898
Iteration 88/1000 | Loss: 0.00002898
Iteration 89/1000 | Loss: 0.00002898
Iteration 90/1000 | Loss: 0.00002898
Iteration 91/1000 | Loss: 0.00002898
Iteration 92/1000 | Loss: 0.00002898
Iteration 93/1000 | Loss: 0.00002898
Iteration 94/1000 | Loss: 0.00002898
Iteration 95/1000 | Loss: 0.00002898
Iteration 96/1000 | Loss: 0.00002897
Iteration 97/1000 | Loss: 0.00002897
Iteration 98/1000 | Loss: 0.00002897
Iteration 99/1000 | Loss: 0.00002897
Iteration 100/1000 | Loss: 0.00002897
Iteration 101/1000 | Loss: 0.00002897
Iteration 102/1000 | Loss: 0.00002897
Iteration 103/1000 | Loss: 0.00002897
Iteration 104/1000 | Loss: 0.00002897
Iteration 105/1000 | Loss: 0.00002897
Iteration 106/1000 | Loss: 0.00002896
Iteration 107/1000 | Loss: 0.00002896
Iteration 108/1000 | Loss: 0.00002896
Iteration 109/1000 | Loss: 0.00002896
Iteration 110/1000 | Loss: 0.00002896
Iteration 111/1000 | Loss: 0.00002896
Iteration 112/1000 | Loss: 0.00002896
Iteration 113/1000 | Loss: 0.00002896
Iteration 114/1000 | Loss: 0.00002896
Iteration 115/1000 | Loss: 0.00002896
Iteration 116/1000 | Loss: 0.00002896
Iteration 117/1000 | Loss: 0.00002896
Iteration 118/1000 | Loss: 0.00002896
Iteration 119/1000 | Loss: 0.00002896
Iteration 120/1000 | Loss: 0.00002896
Iteration 121/1000 | Loss: 0.00002896
Iteration 122/1000 | Loss: 0.00002896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.8957454560440965e-05, 2.8957454560440965e-05, 2.8957454560440965e-05, 2.8957454560440965e-05, 2.8957454560440965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8957454560440965e-05

Optimization complete. Final v2v error: 4.560792446136475 mm

Highest mean error: 5.184608459472656 mm for frame 8

Lowest mean error: 3.789565086364746 mm for frame 237

Saving results

Total time: 38.84760522842407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018722
Iteration 2/25 | Loss: 0.01018722
Iteration 3/25 | Loss: 0.00411498
Iteration 4/25 | Loss: 0.00278349
Iteration 5/25 | Loss: 0.00204656
Iteration 6/25 | Loss: 0.00191458
Iteration 7/25 | Loss: 0.00160020
Iteration 8/25 | Loss: 0.00150025
Iteration 9/25 | Loss: 0.00141274
Iteration 10/25 | Loss: 0.00137144
Iteration 11/25 | Loss: 0.00135910
Iteration 12/25 | Loss: 0.00136360
Iteration 13/25 | Loss: 0.00135372
Iteration 14/25 | Loss: 0.00135159
Iteration 15/25 | Loss: 0.00134991
Iteration 16/25 | Loss: 0.00134393
Iteration 17/25 | Loss: 0.00134810
Iteration 18/25 | Loss: 0.00134618
Iteration 19/25 | Loss: 0.00134374
Iteration 20/25 | Loss: 0.00134567
Iteration 21/25 | Loss: 0.00134224
Iteration 22/25 | Loss: 0.00133805
Iteration 23/25 | Loss: 0.00133287
Iteration 24/25 | Loss: 0.00133032
Iteration 25/25 | Loss: 0.00132860

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39154577
Iteration 2/25 | Loss: 0.00133443
Iteration 3/25 | Loss: 0.00121412
Iteration 4/25 | Loss: 0.00121412
Iteration 5/25 | Loss: 0.00121412
Iteration 6/25 | Loss: 0.00121412
Iteration 7/25 | Loss: 0.00121412
Iteration 8/25 | Loss: 0.00121412
Iteration 9/25 | Loss: 0.00121412
Iteration 10/25 | Loss: 0.00121412
Iteration 11/25 | Loss: 0.00121412
Iteration 12/25 | Loss: 0.00121412
Iteration 13/25 | Loss: 0.00121412
Iteration 14/25 | Loss: 0.00121412
Iteration 15/25 | Loss: 0.00121412
Iteration 16/25 | Loss: 0.00121412
Iteration 17/25 | Loss: 0.00121412
Iteration 18/25 | Loss: 0.00121412
Iteration 19/25 | Loss: 0.00121412
Iteration 20/25 | Loss: 0.00121412
Iteration 21/25 | Loss: 0.00121412
Iteration 22/25 | Loss: 0.00121412
Iteration 23/25 | Loss: 0.00121412
Iteration 24/25 | Loss: 0.00121412
Iteration 25/25 | Loss: 0.00121412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121412
Iteration 2/1000 | Loss: 0.00027037
Iteration 3/1000 | Loss: 0.00024186
Iteration 4/1000 | Loss: 0.00008130
Iteration 5/1000 | Loss: 0.00006861
Iteration 6/1000 | Loss: 0.00074279
Iteration 7/1000 | Loss: 0.00025131
Iteration 8/1000 | Loss: 0.00026931
Iteration 9/1000 | Loss: 0.00012951
Iteration 10/1000 | Loss: 0.00021773
Iteration 11/1000 | Loss: 0.00025564
Iteration 12/1000 | Loss: 0.00020007
Iteration 13/1000 | Loss: 0.00014570
Iteration 14/1000 | Loss: 0.00021927
Iteration 15/1000 | Loss: 0.00015850
Iteration 16/1000 | Loss: 0.00013809
Iteration 17/1000 | Loss: 0.00042883
Iteration 18/1000 | Loss: 0.00021105
Iteration 19/1000 | Loss: 0.00016580
Iteration 20/1000 | Loss: 0.00010990
Iteration 21/1000 | Loss: 0.00005757
Iteration 22/1000 | Loss: 0.00021210
Iteration 23/1000 | Loss: 0.00017058
Iteration 24/1000 | Loss: 0.00020125
Iteration 25/1000 | Loss: 0.00020921
Iteration 26/1000 | Loss: 0.00019188
Iteration 27/1000 | Loss: 0.00021699
Iteration 28/1000 | Loss: 0.00020225
Iteration 29/1000 | Loss: 0.00018458
Iteration 30/1000 | Loss: 0.00018235
Iteration 31/1000 | Loss: 0.00017383
Iteration 32/1000 | Loss: 0.00012075
Iteration 33/1000 | Loss: 0.00005160
Iteration 34/1000 | Loss: 0.00004947
Iteration 35/1000 | Loss: 0.00004882
Iteration 36/1000 | Loss: 0.00004823
Iteration 37/1000 | Loss: 0.00004744
Iteration 38/1000 | Loss: 0.00081873
Iteration 39/1000 | Loss: 0.00066313
Iteration 40/1000 | Loss: 0.00006256
Iteration 41/1000 | Loss: 0.00005041
Iteration 42/1000 | Loss: 0.00004453
Iteration 43/1000 | Loss: 0.00004145
Iteration 44/1000 | Loss: 0.00004025
Iteration 45/1000 | Loss: 0.00003919
Iteration 46/1000 | Loss: 0.00003869
Iteration 47/1000 | Loss: 0.00032211
Iteration 48/1000 | Loss: 0.00004251
Iteration 49/1000 | Loss: 0.00003898
Iteration 50/1000 | Loss: 0.00075055
Iteration 51/1000 | Loss: 0.00004481
Iteration 52/1000 | Loss: 0.00003877
Iteration 53/1000 | Loss: 0.00003549
Iteration 54/1000 | Loss: 0.00003412
Iteration 55/1000 | Loss: 0.00003331
Iteration 56/1000 | Loss: 0.00003277
Iteration 57/1000 | Loss: 0.00003234
Iteration 58/1000 | Loss: 0.00003203
Iteration 59/1000 | Loss: 0.00003170
Iteration 60/1000 | Loss: 0.00094080
Iteration 61/1000 | Loss: 0.00099112
Iteration 62/1000 | Loss: 0.00004328
Iteration 63/1000 | Loss: 0.00003166
Iteration 64/1000 | Loss: 0.00003124
Iteration 65/1000 | Loss: 0.00088023
Iteration 66/1000 | Loss: 0.00021457
Iteration 67/1000 | Loss: 0.00003175
Iteration 68/1000 | Loss: 0.00079528
Iteration 69/1000 | Loss: 0.00017419
Iteration 70/1000 | Loss: 0.00092770
Iteration 71/1000 | Loss: 0.00017213
Iteration 72/1000 | Loss: 0.00003386
Iteration 73/1000 | Loss: 0.00044078
Iteration 74/1000 | Loss: 0.00009747
Iteration 75/1000 | Loss: 0.00037734
Iteration 76/1000 | Loss: 0.00004510
Iteration 77/1000 | Loss: 0.00003383
Iteration 78/1000 | Loss: 0.00003122
Iteration 79/1000 | Loss: 0.00002996
Iteration 80/1000 | Loss: 0.00003228
Iteration 81/1000 | Loss: 0.00002967
Iteration 82/1000 | Loss: 0.00002878
Iteration 83/1000 | Loss: 0.00002858
Iteration 84/1000 | Loss: 0.00002854
Iteration 85/1000 | Loss: 0.00002854
Iteration 86/1000 | Loss: 0.00002853
Iteration 87/1000 | Loss: 0.00002853
Iteration 88/1000 | Loss: 0.00002851
Iteration 89/1000 | Loss: 0.00002833
Iteration 90/1000 | Loss: 0.00002829
Iteration 91/1000 | Loss: 0.00002825
Iteration 92/1000 | Loss: 0.00002825
Iteration 93/1000 | Loss: 0.00002824
Iteration 94/1000 | Loss: 0.00002823
Iteration 95/1000 | Loss: 0.00002822
Iteration 96/1000 | Loss: 0.00002821
Iteration 97/1000 | Loss: 0.00002820
Iteration 98/1000 | Loss: 0.00002820
Iteration 99/1000 | Loss: 0.00002819
Iteration 100/1000 | Loss: 0.00002819
Iteration 101/1000 | Loss: 0.00002818
Iteration 102/1000 | Loss: 0.00002818
Iteration 103/1000 | Loss: 0.00002818
Iteration 104/1000 | Loss: 0.00002818
Iteration 105/1000 | Loss: 0.00002817
Iteration 106/1000 | Loss: 0.00002817
Iteration 107/1000 | Loss: 0.00002816
Iteration 108/1000 | Loss: 0.00002816
Iteration 109/1000 | Loss: 0.00002816
Iteration 110/1000 | Loss: 0.00002816
Iteration 111/1000 | Loss: 0.00002816
Iteration 112/1000 | Loss: 0.00002816
Iteration 113/1000 | Loss: 0.00002816
Iteration 114/1000 | Loss: 0.00002815
Iteration 115/1000 | Loss: 0.00002815
Iteration 116/1000 | Loss: 0.00002815
Iteration 117/1000 | Loss: 0.00002814
Iteration 118/1000 | Loss: 0.00002813
Iteration 119/1000 | Loss: 0.00002813
Iteration 120/1000 | Loss: 0.00002812
Iteration 121/1000 | Loss: 0.00002807
Iteration 122/1000 | Loss: 0.00002804
Iteration 123/1000 | Loss: 0.00002804
Iteration 124/1000 | Loss: 0.00002802
Iteration 125/1000 | Loss: 0.00002802
Iteration 126/1000 | Loss: 0.00002802
Iteration 127/1000 | Loss: 0.00002801
Iteration 128/1000 | Loss: 0.00002801
Iteration 129/1000 | Loss: 0.00002800
Iteration 130/1000 | Loss: 0.00002799
Iteration 131/1000 | Loss: 0.00002799
Iteration 132/1000 | Loss: 0.00002798
Iteration 133/1000 | Loss: 0.00002798
Iteration 134/1000 | Loss: 0.00002798
Iteration 135/1000 | Loss: 0.00002798
Iteration 136/1000 | Loss: 0.00002798
Iteration 137/1000 | Loss: 0.00002797
Iteration 138/1000 | Loss: 0.00002797
Iteration 139/1000 | Loss: 0.00002797
Iteration 140/1000 | Loss: 0.00002797
Iteration 141/1000 | Loss: 0.00002797
Iteration 142/1000 | Loss: 0.00002797
Iteration 143/1000 | Loss: 0.00002797
Iteration 144/1000 | Loss: 0.00002797
Iteration 145/1000 | Loss: 0.00002796
Iteration 146/1000 | Loss: 0.00002796
Iteration 147/1000 | Loss: 0.00002796
Iteration 148/1000 | Loss: 0.00002795
Iteration 149/1000 | Loss: 0.00002795
Iteration 150/1000 | Loss: 0.00002795
Iteration 151/1000 | Loss: 0.00002795
Iteration 152/1000 | Loss: 0.00002794
Iteration 153/1000 | Loss: 0.00002794
Iteration 154/1000 | Loss: 0.00002793
Iteration 155/1000 | Loss: 0.00002793
Iteration 156/1000 | Loss: 0.00002793
Iteration 157/1000 | Loss: 0.00002792
Iteration 158/1000 | Loss: 0.00002792
Iteration 159/1000 | Loss: 0.00002791
Iteration 160/1000 | Loss: 0.00002791
Iteration 161/1000 | Loss: 0.00002791
Iteration 162/1000 | Loss: 0.00002791
Iteration 163/1000 | Loss: 0.00002790
Iteration 164/1000 | Loss: 0.00002790
Iteration 165/1000 | Loss: 0.00002790
Iteration 166/1000 | Loss: 0.00002789
Iteration 167/1000 | Loss: 0.00002789
Iteration 168/1000 | Loss: 0.00002789
Iteration 169/1000 | Loss: 0.00002788
Iteration 170/1000 | Loss: 0.00002788
Iteration 171/1000 | Loss: 0.00002788
Iteration 172/1000 | Loss: 0.00002788
Iteration 173/1000 | Loss: 0.00002788
Iteration 174/1000 | Loss: 0.00002787
Iteration 175/1000 | Loss: 0.00002787
Iteration 176/1000 | Loss: 0.00002787
Iteration 177/1000 | Loss: 0.00002787
Iteration 178/1000 | Loss: 0.00002787
Iteration 179/1000 | Loss: 0.00002787
Iteration 180/1000 | Loss: 0.00002787
Iteration 181/1000 | Loss: 0.00002787
Iteration 182/1000 | Loss: 0.00002786
Iteration 183/1000 | Loss: 0.00002786
Iteration 184/1000 | Loss: 0.00002786
Iteration 185/1000 | Loss: 0.00002786
Iteration 186/1000 | Loss: 0.00002786
Iteration 187/1000 | Loss: 0.00002786
Iteration 188/1000 | Loss: 0.00002786
Iteration 189/1000 | Loss: 0.00002785
Iteration 190/1000 | Loss: 0.00002785
Iteration 191/1000 | Loss: 0.00002785
Iteration 192/1000 | Loss: 0.00002785
Iteration 193/1000 | Loss: 0.00002785
Iteration 194/1000 | Loss: 0.00002785
Iteration 195/1000 | Loss: 0.00002785
Iteration 196/1000 | Loss: 0.00002784
Iteration 197/1000 | Loss: 0.00002784
Iteration 198/1000 | Loss: 0.00002784
Iteration 199/1000 | Loss: 0.00002784
Iteration 200/1000 | Loss: 0.00002784
Iteration 201/1000 | Loss: 0.00002784
Iteration 202/1000 | Loss: 0.00002784
Iteration 203/1000 | Loss: 0.00002784
Iteration 204/1000 | Loss: 0.00002784
Iteration 205/1000 | Loss: 0.00002784
Iteration 206/1000 | Loss: 0.00002784
Iteration 207/1000 | Loss: 0.00002783
Iteration 208/1000 | Loss: 0.00002783
Iteration 209/1000 | Loss: 0.00002783
Iteration 210/1000 | Loss: 0.00002783
Iteration 211/1000 | Loss: 0.00002783
Iteration 212/1000 | Loss: 0.00002783
Iteration 213/1000 | Loss: 0.00002783
Iteration 214/1000 | Loss: 0.00002783
Iteration 215/1000 | Loss: 0.00002783
Iteration 216/1000 | Loss: 0.00002783
Iteration 217/1000 | Loss: 0.00002783
Iteration 218/1000 | Loss: 0.00002783
Iteration 219/1000 | Loss: 0.00002783
Iteration 220/1000 | Loss: 0.00002783
Iteration 221/1000 | Loss: 0.00002783
Iteration 222/1000 | Loss: 0.00002783
Iteration 223/1000 | Loss: 0.00002783
Iteration 224/1000 | Loss: 0.00002783
Iteration 225/1000 | Loss: 0.00002783
Iteration 226/1000 | Loss: 0.00002783
Iteration 227/1000 | Loss: 0.00002783
Iteration 228/1000 | Loss: 0.00002783
Iteration 229/1000 | Loss: 0.00002783
Iteration 230/1000 | Loss: 0.00002783
Iteration 231/1000 | Loss: 0.00002783
Iteration 232/1000 | Loss: 0.00002783
Iteration 233/1000 | Loss: 0.00002783
Iteration 234/1000 | Loss: 0.00002783
Iteration 235/1000 | Loss: 0.00002783
Iteration 236/1000 | Loss: 0.00002783
Iteration 237/1000 | Loss: 0.00002783
Iteration 238/1000 | Loss: 0.00002783
Iteration 239/1000 | Loss: 0.00002783
Iteration 240/1000 | Loss: 0.00002783
Iteration 241/1000 | Loss: 0.00002783
Iteration 242/1000 | Loss: 0.00002783
Iteration 243/1000 | Loss: 0.00002783
Iteration 244/1000 | Loss: 0.00002783
Iteration 245/1000 | Loss: 0.00002783
Iteration 246/1000 | Loss: 0.00002783
Iteration 247/1000 | Loss: 0.00002783
Iteration 248/1000 | Loss: 0.00002783
Iteration 249/1000 | Loss: 0.00002783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.783465970424004e-05, 2.783465970424004e-05, 2.783465970424004e-05, 2.783465970424004e-05, 2.783465970424004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.783465970424004e-05

Optimization complete. Final v2v error: 4.493432521820068 mm

Highest mean error: 5.298162937164307 mm for frame 146

Lowest mean error: 3.419243812561035 mm for frame 11

Saving results

Total time: 174.03913950920105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388381
Iteration 2/25 | Loss: 0.00133152
Iteration 3/25 | Loss: 0.00125214
Iteration 4/25 | Loss: 0.00124446
Iteration 5/25 | Loss: 0.00124140
Iteration 6/25 | Loss: 0.00124140
Iteration 7/25 | Loss: 0.00124140
Iteration 8/25 | Loss: 0.00124140
Iteration 9/25 | Loss: 0.00124140
Iteration 10/25 | Loss: 0.00124140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012414016528055072, 0.0012414016528055072, 0.0012414016528055072, 0.0012414016528055072, 0.0012414016528055072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012414016528055072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57570970
Iteration 2/25 | Loss: 0.00078570
Iteration 3/25 | Loss: 0.00078569
Iteration 4/25 | Loss: 0.00078569
Iteration 5/25 | Loss: 0.00078569
Iteration 6/25 | Loss: 0.00078569
Iteration 7/25 | Loss: 0.00078569
Iteration 8/25 | Loss: 0.00078569
Iteration 9/25 | Loss: 0.00078569
Iteration 10/25 | Loss: 0.00078569
Iteration 11/25 | Loss: 0.00078569
Iteration 12/25 | Loss: 0.00078569
Iteration 13/25 | Loss: 0.00078569
Iteration 14/25 | Loss: 0.00078569
Iteration 15/25 | Loss: 0.00078569
Iteration 16/25 | Loss: 0.00078569
Iteration 17/25 | Loss: 0.00078569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007856912561692297, 0.0007856912561692297, 0.0007856912561692297, 0.0007856912561692297, 0.0007856912561692297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007856912561692297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078569
Iteration 2/1000 | Loss: 0.00003335
Iteration 3/1000 | Loss: 0.00001909
Iteration 4/1000 | Loss: 0.00001710
Iteration 5/1000 | Loss: 0.00001576
Iteration 6/1000 | Loss: 0.00001463
Iteration 7/1000 | Loss: 0.00001408
Iteration 8/1000 | Loss: 0.00001364
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001267
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001254
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001242
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001240
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001235
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001233
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001231
Iteration 61/1000 | Loss: 0.00001231
Iteration 62/1000 | Loss: 0.00001231
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001224
Iteration 78/1000 | Loss: 0.00001224
Iteration 79/1000 | Loss: 0.00001224
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001222
Iteration 87/1000 | Loss: 0.00001222
Iteration 88/1000 | Loss: 0.00001221
Iteration 89/1000 | Loss: 0.00001221
Iteration 90/1000 | Loss: 0.00001221
Iteration 91/1000 | Loss: 0.00001221
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001220
Iteration 95/1000 | Loss: 0.00001220
Iteration 96/1000 | Loss: 0.00001220
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001220
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001220
Iteration 101/1000 | Loss: 0.00001219
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001219
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00001218
Iteration 107/1000 | Loss: 0.00001218
Iteration 108/1000 | Loss: 0.00001218
Iteration 109/1000 | Loss: 0.00001218
Iteration 110/1000 | Loss: 0.00001218
Iteration 111/1000 | Loss: 0.00001217
Iteration 112/1000 | Loss: 0.00001217
Iteration 113/1000 | Loss: 0.00001217
Iteration 114/1000 | Loss: 0.00001217
Iteration 115/1000 | Loss: 0.00001217
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001215
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001215
Iteration 124/1000 | Loss: 0.00001215
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Iteration 128/1000 | Loss: 0.00001214
Iteration 129/1000 | Loss: 0.00001214
Iteration 130/1000 | Loss: 0.00001214
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001213
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Iteration 141/1000 | Loss: 0.00001211
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001211
Iteration 146/1000 | Loss: 0.00001211
Iteration 147/1000 | Loss: 0.00001211
Iteration 148/1000 | Loss: 0.00001211
Iteration 149/1000 | Loss: 0.00001211
Iteration 150/1000 | Loss: 0.00001211
Iteration 151/1000 | Loss: 0.00001211
Iteration 152/1000 | Loss: 0.00001211
Iteration 153/1000 | Loss: 0.00001211
Iteration 154/1000 | Loss: 0.00001210
Iteration 155/1000 | Loss: 0.00001210
Iteration 156/1000 | Loss: 0.00001210
Iteration 157/1000 | Loss: 0.00001210
Iteration 158/1000 | Loss: 0.00001210
Iteration 159/1000 | Loss: 0.00001210
Iteration 160/1000 | Loss: 0.00001210
Iteration 161/1000 | Loss: 0.00001210
Iteration 162/1000 | Loss: 0.00001209
Iteration 163/1000 | Loss: 0.00001209
Iteration 164/1000 | Loss: 0.00001209
Iteration 165/1000 | Loss: 0.00001209
Iteration 166/1000 | Loss: 0.00001208
Iteration 167/1000 | Loss: 0.00001208
Iteration 168/1000 | Loss: 0.00001208
Iteration 169/1000 | Loss: 0.00001208
Iteration 170/1000 | Loss: 0.00001208
Iteration 171/1000 | Loss: 0.00001208
Iteration 172/1000 | Loss: 0.00001208
Iteration 173/1000 | Loss: 0.00001208
Iteration 174/1000 | Loss: 0.00001208
Iteration 175/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.2081099157512654e-05, 1.2081099157512654e-05, 1.2081099157512654e-05, 1.2081099157512654e-05, 1.2081099157512654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2081099157512654e-05

Optimization complete. Final v2v error: 2.978823661804199 mm

Highest mean error: 3.3291454315185547 mm for frame 128

Lowest mean error: 2.791226863861084 mm for frame 37

Saving results

Total time: 42.50903010368347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394000
Iteration 2/25 | Loss: 0.00131225
Iteration 3/25 | Loss: 0.00124744
Iteration 4/25 | Loss: 0.00123781
Iteration 5/25 | Loss: 0.00123525
Iteration 6/25 | Loss: 0.00123525
Iteration 7/25 | Loss: 0.00123525
Iteration 8/25 | Loss: 0.00123525
Iteration 9/25 | Loss: 0.00123525
Iteration 10/25 | Loss: 0.00123525
Iteration 11/25 | Loss: 0.00123525
Iteration 12/25 | Loss: 0.00123525
Iteration 13/25 | Loss: 0.00123525
Iteration 14/25 | Loss: 0.00123525
Iteration 15/25 | Loss: 0.00123525
Iteration 16/25 | Loss: 0.00123525
Iteration 17/25 | Loss: 0.00123525
Iteration 18/25 | Loss: 0.00123525
Iteration 19/25 | Loss: 0.00123525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001235245494171977, 0.001235245494171977, 0.001235245494171977, 0.001235245494171977, 0.001235245494171977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001235245494171977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44033766
Iteration 2/25 | Loss: 0.00077752
Iteration 3/25 | Loss: 0.00077752
Iteration 4/25 | Loss: 0.00077752
Iteration 5/25 | Loss: 0.00077752
Iteration 6/25 | Loss: 0.00077752
Iteration 7/25 | Loss: 0.00077752
Iteration 8/25 | Loss: 0.00077752
Iteration 9/25 | Loss: 0.00077752
Iteration 10/25 | Loss: 0.00077752
Iteration 11/25 | Loss: 0.00077752
Iteration 12/25 | Loss: 0.00077752
Iteration 13/25 | Loss: 0.00077752
Iteration 14/25 | Loss: 0.00077752
Iteration 15/25 | Loss: 0.00077752
Iteration 16/25 | Loss: 0.00077752
Iteration 17/25 | Loss: 0.00077752
Iteration 18/25 | Loss: 0.00077752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007775199483148754, 0.0007775199483148754, 0.0007775199483148754, 0.0007775199483148754, 0.0007775199483148754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007775199483148754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077752
Iteration 2/1000 | Loss: 0.00002669
Iteration 3/1000 | Loss: 0.00001914
Iteration 4/1000 | Loss: 0.00001716
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001551
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001449
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001438
Iteration 12/1000 | Loss: 0.00001438
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00001402
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001394
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001392
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001388
Iteration 34/1000 | Loss: 0.00001386
Iteration 35/1000 | Loss: 0.00001385
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001382
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001381
Iteration 43/1000 | Loss: 0.00001381
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001375
Iteration 50/1000 | Loss: 0.00001375
Iteration 51/1000 | Loss: 0.00001374
Iteration 52/1000 | Loss: 0.00001374
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001373
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001368
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001365
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001365
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001364
Iteration 88/1000 | Loss: 0.00001364
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001363
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001362
Iteration 97/1000 | Loss: 0.00001362
Iteration 98/1000 | Loss: 0.00001362
Iteration 99/1000 | Loss: 0.00001362
Iteration 100/1000 | Loss: 0.00001362
Iteration 101/1000 | Loss: 0.00001362
Iteration 102/1000 | Loss: 0.00001362
Iteration 103/1000 | Loss: 0.00001361
Iteration 104/1000 | Loss: 0.00001361
Iteration 105/1000 | Loss: 0.00001361
Iteration 106/1000 | Loss: 0.00001361
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001360
Iteration 109/1000 | Loss: 0.00001360
Iteration 110/1000 | Loss: 0.00001360
Iteration 111/1000 | Loss: 0.00001359
Iteration 112/1000 | Loss: 0.00001357
Iteration 113/1000 | Loss: 0.00001357
Iteration 114/1000 | Loss: 0.00001357
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001357
Iteration 118/1000 | Loss: 0.00001357
Iteration 119/1000 | Loss: 0.00001357
Iteration 120/1000 | Loss: 0.00001356
Iteration 121/1000 | Loss: 0.00001356
Iteration 122/1000 | Loss: 0.00001356
Iteration 123/1000 | Loss: 0.00001356
Iteration 124/1000 | Loss: 0.00001356
Iteration 125/1000 | Loss: 0.00001355
Iteration 126/1000 | Loss: 0.00001355
Iteration 127/1000 | Loss: 0.00001355
Iteration 128/1000 | Loss: 0.00001355
Iteration 129/1000 | Loss: 0.00001354
Iteration 130/1000 | Loss: 0.00001354
Iteration 131/1000 | Loss: 0.00001354
Iteration 132/1000 | Loss: 0.00001354
Iteration 133/1000 | Loss: 0.00001353
Iteration 134/1000 | Loss: 0.00001353
Iteration 135/1000 | Loss: 0.00001353
Iteration 136/1000 | Loss: 0.00001353
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Iteration 140/1000 | Loss: 0.00001352
Iteration 141/1000 | Loss: 0.00001352
Iteration 142/1000 | Loss: 0.00001352
Iteration 143/1000 | Loss: 0.00001351
Iteration 144/1000 | Loss: 0.00001351
Iteration 145/1000 | Loss: 0.00001351
Iteration 146/1000 | Loss: 0.00001351
Iteration 147/1000 | Loss: 0.00001351
Iteration 148/1000 | Loss: 0.00001351
Iteration 149/1000 | Loss: 0.00001351
Iteration 150/1000 | Loss: 0.00001351
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001350
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001349
Iteration 158/1000 | Loss: 0.00001349
Iteration 159/1000 | Loss: 0.00001349
Iteration 160/1000 | Loss: 0.00001348
Iteration 161/1000 | Loss: 0.00001348
Iteration 162/1000 | Loss: 0.00001348
Iteration 163/1000 | Loss: 0.00001348
Iteration 164/1000 | Loss: 0.00001348
Iteration 165/1000 | Loss: 0.00001347
Iteration 166/1000 | Loss: 0.00001347
Iteration 167/1000 | Loss: 0.00001347
Iteration 168/1000 | Loss: 0.00001347
Iteration 169/1000 | Loss: 0.00001347
Iteration 170/1000 | Loss: 0.00001347
Iteration 171/1000 | Loss: 0.00001347
Iteration 172/1000 | Loss: 0.00001347
Iteration 173/1000 | Loss: 0.00001347
Iteration 174/1000 | Loss: 0.00001347
Iteration 175/1000 | Loss: 0.00001347
Iteration 176/1000 | Loss: 0.00001347
Iteration 177/1000 | Loss: 0.00001347
Iteration 178/1000 | Loss: 0.00001347
Iteration 179/1000 | Loss: 0.00001347
Iteration 180/1000 | Loss: 0.00001347
Iteration 181/1000 | Loss: 0.00001346
Iteration 182/1000 | Loss: 0.00001346
Iteration 183/1000 | Loss: 0.00001346
Iteration 184/1000 | Loss: 0.00001346
Iteration 185/1000 | Loss: 0.00001346
Iteration 186/1000 | Loss: 0.00001345
Iteration 187/1000 | Loss: 0.00001345
Iteration 188/1000 | Loss: 0.00001345
Iteration 189/1000 | Loss: 0.00001345
Iteration 190/1000 | Loss: 0.00001345
Iteration 191/1000 | Loss: 0.00001345
Iteration 192/1000 | Loss: 0.00001345
Iteration 193/1000 | Loss: 0.00001345
Iteration 194/1000 | Loss: 0.00001345
Iteration 195/1000 | Loss: 0.00001345
Iteration 196/1000 | Loss: 0.00001345
Iteration 197/1000 | Loss: 0.00001345
Iteration 198/1000 | Loss: 0.00001345
Iteration 199/1000 | Loss: 0.00001345
Iteration 200/1000 | Loss: 0.00001345
Iteration 201/1000 | Loss: 0.00001345
Iteration 202/1000 | Loss: 0.00001345
Iteration 203/1000 | Loss: 0.00001345
Iteration 204/1000 | Loss: 0.00001345
Iteration 205/1000 | Loss: 0.00001345
Iteration 206/1000 | Loss: 0.00001345
Iteration 207/1000 | Loss: 0.00001345
Iteration 208/1000 | Loss: 0.00001345
Iteration 209/1000 | Loss: 0.00001345
Iteration 210/1000 | Loss: 0.00001345
Iteration 211/1000 | Loss: 0.00001345
Iteration 212/1000 | Loss: 0.00001345
Iteration 213/1000 | Loss: 0.00001345
Iteration 214/1000 | Loss: 0.00001345
Iteration 215/1000 | Loss: 0.00001345
Iteration 216/1000 | Loss: 0.00001345
Iteration 217/1000 | Loss: 0.00001345
Iteration 218/1000 | Loss: 0.00001345
Iteration 219/1000 | Loss: 0.00001345
Iteration 220/1000 | Loss: 0.00001345
Iteration 221/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.3447648598230444e-05, 1.3447648598230444e-05, 1.3447648598230444e-05, 1.3447648598230444e-05, 1.3447648598230444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3447648598230444e-05

Optimization complete. Final v2v error: 3.114217758178711 mm

Highest mean error: 3.226961374282837 mm for frame 183

Lowest mean error: 3.047559976577759 mm for frame 204

Saving results

Total time: 44.83163380622864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809016
Iteration 2/25 | Loss: 0.00156994
Iteration 3/25 | Loss: 0.00137313
Iteration 4/25 | Loss: 0.00135645
Iteration 5/25 | Loss: 0.00135140
Iteration 6/25 | Loss: 0.00134976
Iteration 7/25 | Loss: 0.00134948
Iteration 8/25 | Loss: 0.00134948
Iteration 9/25 | Loss: 0.00134948
Iteration 10/25 | Loss: 0.00134948
Iteration 11/25 | Loss: 0.00134948
Iteration 12/25 | Loss: 0.00134948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013494774466380477, 0.0013494774466380477, 0.0013494774466380477, 0.0013494774466380477, 0.0013494774466380477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013494774466380477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31438875
Iteration 2/25 | Loss: 0.00124650
Iteration 3/25 | Loss: 0.00124650
Iteration 4/25 | Loss: 0.00124650
Iteration 5/25 | Loss: 0.00124650
Iteration 6/25 | Loss: 0.00124650
Iteration 7/25 | Loss: 0.00124649
Iteration 8/25 | Loss: 0.00124649
Iteration 9/25 | Loss: 0.00124649
Iteration 10/25 | Loss: 0.00124649
Iteration 11/25 | Loss: 0.00124649
Iteration 12/25 | Loss: 0.00124649
Iteration 13/25 | Loss: 0.00124649
Iteration 14/25 | Loss: 0.00124649
Iteration 15/25 | Loss: 0.00124649
Iteration 16/25 | Loss: 0.00124649
Iteration 17/25 | Loss: 0.00124649
Iteration 18/25 | Loss: 0.00124649
Iteration 19/25 | Loss: 0.00124649
Iteration 20/25 | Loss: 0.00124649
Iteration 21/25 | Loss: 0.00124649
Iteration 22/25 | Loss: 0.00124649
Iteration 23/25 | Loss: 0.00124649
Iteration 24/25 | Loss: 0.00124649
Iteration 25/25 | Loss: 0.00124649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124649
Iteration 2/1000 | Loss: 0.00006818
Iteration 3/1000 | Loss: 0.00005207
Iteration 4/1000 | Loss: 0.00004291
Iteration 5/1000 | Loss: 0.00004072
Iteration 6/1000 | Loss: 0.00003975
Iteration 7/1000 | Loss: 0.00003865
Iteration 8/1000 | Loss: 0.00003783
Iteration 9/1000 | Loss: 0.00003686
Iteration 10/1000 | Loss: 0.00003618
Iteration 11/1000 | Loss: 0.00003572
Iteration 12/1000 | Loss: 0.00003532
Iteration 13/1000 | Loss: 0.00003507
Iteration 14/1000 | Loss: 0.00003480
Iteration 15/1000 | Loss: 0.00003471
Iteration 16/1000 | Loss: 0.00003463
Iteration 17/1000 | Loss: 0.00003447
Iteration 18/1000 | Loss: 0.00003437
Iteration 19/1000 | Loss: 0.00003437
Iteration 20/1000 | Loss: 0.00003437
Iteration 21/1000 | Loss: 0.00003437
Iteration 22/1000 | Loss: 0.00003437
Iteration 23/1000 | Loss: 0.00003437
Iteration 24/1000 | Loss: 0.00003437
Iteration 25/1000 | Loss: 0.00003437
Iteration 26/1000 | Loss: 0.00003436
Iteration 27/1000 | Loss: 0.00003436
Iteration 28/1000 | Loss: 0.00003436
Iteration 29/1000 | Loss: 0.00003436
Iteration 30/1000 | Loss: 0.00003434
Iteration 31/1000 | Loss: 0.00003433
Iteration 32/1000 | Loss: 0.00003432
Iteration 33/1000 | Loss: 0.00003432
Iteration 34/1000 | Loss: 0.00003432
Iteration 35/1000 | Loss: 0.00003432
Iteration 36/1000 | Loss: 0.00003431
Iteration 37/1000 | Loss: 0.00003431
Iteration 38/1000 | Loss: 0.00003430
Iteration 39/1000 | Loss: 0.00003430
Iteration 40/1000 | Loss: 0.00003430
Iteration 41/1000 | Loss: 0.00003429
Iteration 42/1000 | Loss: 0.00003429
Iteration 43/1000 | Loss: 0.00003429
Iteration 44/1000 | Loss: 0.00003424
Iteration 45/1000 | Loss: 0.00003424
Iteration 46/1000 | Loss: 0.00003423
Iteration 47/1000 | Loss: 0.00003423
Iteration 48/1000 | Loss: 0.00003423
Iteration 49/1000 | Loss: 0.00003423
Iteration 50/1000 | Loss: 0.00003423
Iteration 51/1000 | Loss: 0.00003423
Iteration 52/1000 | Loss: 0.00003423
Iteration 53/1000 | Loss: 0.00003422
Iteration 54/1000 | Loss: 0.00003422
Iteration 55/1000 | Loss: 0.00003422
Iteration 56/1000 | Loss: 0.00003422
Iteration 57/1000 | Loss: 0.00003422
Iteration 58/1000 | Loss: 0.00003422
Iteration 59/1000 | Loss: 0.00003422
Iteration 60/1000 | Loss: 0.00003421
Iteration 61/1000 | Loss: 0.00003421
Iteration 62/1000 | Loss: 0.00003421
Iteration 63/1000 | Loss: 0.00003421
Iteration 64/1000 | Loss: 0.00003421
Iteration 65/1000 | Loss: 0.00003421
Iteration 66/1000 | Loss: 0.00003421
Iteration 67/1000 | Loss: 0.00003421
Iteration 68/1000 | Loss: 0.00003421
Iteration 69/1000 | Loss: 0.00003420
Iteration 70/1000 | Loss: 0.00003420
Iteration 71/1000 | Loss: 0.00003419
Iteration 72/1000 | Loss: 0.00003419
Iteration 73/1000 | Loss: 0.00003418
Iteration 74/1000 | Loss: 0.00003418
Iteration 75/1000 | Loss: 0.00003418
Iteration 76/1000 | Loss: 0.00003418
Iteration 77/1000 | Loss: 0.00003418
Iteration 78/1000 | Loss: 0.00003418
Iteration 79/1000 | Loss: 0.00003418
Iteration 80/1000 | Loss: 0.00003418
Iteration 81/1000 | Loss: 0.00003418
Iteration 82/1000 | Loss: 0.00003418
Iteration 83/1000 | Loss: 0.00003417
Iteration 84/1000 | Loss: 0.00003417
Iteration 85/1000 | Loss: 0.00003416
Iteration 86/1000 | Loss: 0.00003416
Iteration 87/1000 | Loss: 0.00003416
Iteration 88/1000 | Loss: 0.00003415
Iteration 89/1000 | Loss: 0.00003415
Iteration 90/1000 | Loss: 0.00003415
Iteration 91/1000 | Loss: 0.00003414
Iteration 92/1000 | Loss: 0.00003413
Iteration 93/1000 | Loss: 0.00003413
Iteration 94/1000 | Loss: 0.00003413
Iteration 95/1000 | Loss: 0.00003412
Iteration 96/1000 | Loss: 0.00003412
Iteration 97/1000 | Loss: 0.00003412
Iteration 98/1000 | Loss: 0.00003412
Iteration 99/1000 | Loss: 0.00003412
Iteration 100/1000 | Loss: 0.00003412
Iteration 101/1000 | Loss: 0.00003412
Iteration 102/1000 | Loss: 0.00003412
Iteration 103/1000 | Loss: 0.00003412
Iteration 104/1000 | Loss: 0.00003412
Iteration 105/1000 | Loss: 0.00003412
Iteration 106/1000 | Loss: 0.00003412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [3.412108344491571e-05, 3.412108344491571e-05, 3.412108344491571e-05, 3.412108344491571e-05, 3.412108344491571e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.412108344491571e-05

Optimization complete. Final v2v error: 4.740187168121338 mm

Highest mean error: 4.809035301208496 mm for frame 33

Lowest mean error: 4.620179176330566 mm for frame 91

Saving results

Total time: 40.26792621612549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408518
Iteration 2/25 | Loss: 0.00132572
Iteration 3/25 | Loss: 0.00123936
Iteration 4/25 | Loss: 0.00122659
Iteration 5/25 | Loss: 0.00122221
Iteration 6/25 | Loss: 0.00122107
Iteration 7/25 | Loss: 0.00122091
Iteration 8/25 | Loss: 0.00122091
Iteration 9/25 | Loss: 0.00122091
Iteration 10/25 | Loss: 0.00122091
Iteration 11/25 | Loss: 0.00122091
Iteration 12/25 | Loss: 0.00122091
Iteration 13/25 | Loss: 0.00122091
Iteration 14/25 | Loss: 0.00122091
Iteration 15/25 | Loss: 0.00122091
Iteration 16/25 | Loss: 0.00122091
Iteration 17/25 | Loss: 0.00122091
Iteration 18/25 | Loss: 0.00122091
Iteration 19/25 | Loss: 0.00122091
Iteration 20/25 | Loss: 0.00122091
Iteration 21/25 | Loss: 0.00122091
Iteration 22/25 | Loss: 0.00122091
Iteration 23/25 | Loss: 0.00122091
Iteration 24/25 | Loss: 0.00122091
Iteration 25/25 | Loss: 0.00122091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02056074
Iteration 2/25 | Loss: 0.00077408
Iteration 3/25 | Loss: 0.00077407
Iteration 4/25 | Loss: 0.00077407
Iteration 5/25 | Loss: 0.00077407
Iteration 6/25 | Loss: 0.00077407
Iteration 7/25 | Loss: 0.00077407
Iteration 8/25 | Loss: 0.00077407
Iteration 9/25 | Loss: 0.00077407
Iteration 10/25 | Loss: 0.00077407
Iteration 11/25 | Loss: 0.00077407
Iteration 12/25 | Loss: 0.00077407
Iteration 13/25 | Loss: 0.00077407
Iteration 14/25 | Loss: 0.00077407
Iteration 15/25 | Loss: 0.00077407
Iteration 16/25 | Loss: 0.00077407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007740699802525342, 0.0007740699802525342, 0.0007740699802525342, 0.0007740699802525342, 0.0007740699802525342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007740699802525342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077407
Iteration 2/1000 | Loss: 0.00003905
Iteration 3/1000 | Loss: 0.00002620
Iteration 4/1000 | Loss: 0.00002066
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001796
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001641
Iteration 10/1000 | Loss: 0.00001621
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001596
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001555
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001538
Iteration 20/1000 | Loss: 0.00001524
Iteration 21/1000 | Loss: 0.00001517
Iteration 22/1000 | Loss: 0.00001512
Iteration 23/1000 | Loss: 0.00001505
Iteration 24/1000 | Loss: 0.00001505
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001492
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001491
Iteration 33/1000 | Loss: 0.00001488
Iteration 34/1000 | Loss: 0.00001486
Iteration 35/1000 | Loss: 0.00001485
Iteration 36/1000 | Loss: 0.00001485
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001482
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00001479
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001479
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001477
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001474
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001473
Iteration 75/1000 | Loss: 0.00001473
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001471
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001471
Iteration 81/1000 | Loss: 0.00001471
Iteration 82/1000 | Loss: 0.00001471
Iteration 83/1000 | Loss: 0.00001470
Iteration 84/1000 | Loss: 0.00001470
Iteration 85/1000 | Loss: 0.00001469
Iteration 86/1000 | Loss: 0.00001469
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001469
Iteration 89/1000 | Loss: 0.00001469
Iteration 90/1000 | Loss: 0.00001469
Iteration 91/1000 | Loss: 0.00001468
Iteration 92/1000 | Loss: 0.00001468
Iteration 93/1000 | Loss: 0.00001468
Iteration 94/1000 | Loss: 0.00001468
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001468
Iteration 97/1000 | Loss: 0.00001468
Iteration 98/1000 | Loss: 0.00001468
Iteration 99/1000 | Loss: 0.00001468
Iteration 100/1000 | Loss: 0.00001468
Iteration 101/1000 | Loss: 0.00001468
Iteration 102/1000 | Loss: 0.00001468
Iteration 103/1000 | Loss: 0.00001468
Iteration 104/1000 | Loss: 0.00001468
Iteration 105/1000 | Loss: 0.00001468
Iteration 106/1000 | Loss: 0.00001468
Iteration 107/1000 | Loss: 0.00001468
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001468
Iteration 119/1000 | Loss: 0.00001468
Iteration 120/1000 | Loss: 0.00001468
Iteration 121/1000 | Loss: 0.00001468
Iteration 122/1000 | Loss: 0.00001468
Iteration 123/1000 | Loss: 0.00001468
Iteration 124/1000 | Loss: 0.00001468
Iteration 125/1000 | Loss: 0.00001468
Iteration 126/1000 | Loss: 0.00001468
Iteration 127/1000 | Loss: 0.00001468
Iteration 128/1000 | Loss: 0.00001468
Iteration 129/1000 | Loss: 0.00001468
Iteration 130/1000 | Loss: 0.00001468
Iteration 131/1000 | Loss: 0.00001468
Iteration 132/1000 | Loss: 0.00001468
Iteration 133/1000 | Loss: 0.00001468
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001468
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4678723346150946e-05, 1.4678723346150946e-05, 1.4678723346150946e-05, 1.4678723346150946e-05, 1.4678723346150946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4678723346150946e-05

Optimization complete. Final v2v error: 3.271083116531372 mm

Highest mean error: 4.238122463226318 mm for frame 57

Lowest mean error: 3.00193190574646 mm for frame 91

Saving results

Total time: 42.00419330596924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043981
Iteration 2/25 | Loss: 0.01043981
Iteration 3/25 | Loss: 0.00272062
Iteration 4/25 | Loss: 0.00188671
Iteration 5/25 | Loss: 0.00186946
Iteration 6/25 | Loss: 0.00177942
Iteration 7/25 | Loss: 0.00163803
Iteration 8/25 | Loss: 0.00160926
Iteration 9/25 | Loss: 0.00150966
Iteration 10/25 | Loss: 0.00147319
Iteration 11/25 | Loss: 0.00138720
Iteration 12/25 | Loss: 0.00135552
Iteration 13/25 | Loss: 0.00132841
Iteration 14/25 | Loss: 0.00131557
Iteration 15/25 | Loss: 0.00129669
Iteration 16/25 | Loss: 0.00127598
Iteration 17/25 | Loss: 0.00126171
Iteration 18/25 | Loss: 0.00124895
Iteration 19/25 | Loss: 0.00125196
Iteration 20/25 | Loss: 0.00124931
Iteration 21/25 | Loss: 0.00125059
Iteration 22/25 | Loss: 0.00124236
Iteration 23/25 | Loss: 0.00123835
Iteration 24/25 | Loss: 0.00123958
Iteration 25/25 | Loss: 0.00124138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48242402
Iteration 2/25 | Loss: 0.00123118
Iteration 3/25 | Loss: 0.00123117
Iteration 4/25 | Loss: 0.00092807
Iteration 5/25 | Loss: 0.00092804
Iteration 6/25 | Loss: 0.00092411
Iteration 7/25 | Loss: 0.00092411
Iteration 8/25 | Loss: 0.00092411
Iteration 9/25 | Loss: 0.00092411
Iteration 10/25 | Loss: 0.00092411
Iteration 11/25 | Loss: 0.00092411
Iteration 12/25 | Loss: 0.00092411
Iteration 13/25 | Loss: 0.00092411
Iteration 14/25 | Loss: 0.00092411
Iteration 15/25 | Loss: 0.00092411
Iteration 16/25 | Loss: 0.00092411
Iteration 17/25 | Loss: 0.00092411
Iteration 18/25 | Loss: 0.00092411
Iteration 19/25 | Loss: 0.00092411
Iteration 20/25 | Loss: 0.00092411
Iteration 21/25 | Loss: 0.00092411
Iteration 22/25 | Loss: 0.00092411
Iteration 23/25 | Loss: 0.00092411
Iteration 24/25 | Loss: 0.00092411
Iteration 25/25 | Loss: 0.00092411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092411
Iteration 2/1000 | Loss: 0.00093161
Iteration 3/1000 | Loss: 0.00025738
Iteration 4/1000 | Loss: 0.00040677
Iteration 5/1000 | Loss: 0.00082582
Iteration 6/1000 | Loss: 0.00036682
Iteration 7/1000 | Loss: 0.00070209
Iteration 8/1000 | Loss: 0.00042385
Iteration 9/1000 | Loss: 0.00007740
Iteration 10/1000 | Loss: 0.00007520
Iteration 11/1000 | Loss: 0.00044700
Iteration 12/1000 | Loss: 0.00035186
Iteration 13/1000 | Loss: 0.00035828
Iteration 14/1000 | Loss: 0.00097247
Iteration 15/1000 | Loss: 0.00005360
Iteration 16/1000 | Loss: 0.00005880
Iteration 17/1000 | Loss: 0.00005612
Iteration 18/1000 | Loss: 0.00003250
Iteration 19/1000 | Loss: 0.00046384
Iteration 20/1000 | Loss: 0.00009796
Iteration 21/1000 | Loss: 0.00003885
Iteration 22/1000 | Loss: 0.00033127
Iteration 23/1000 | Loss: 0.00077163
Iteration 24/1000 | Loss: 0.00023178
Iteration 25/1000 | Loss: 0.00003507
Iteration 26/1000 | Loss: 0.00006011
Iteration 27/1000 | Loss: 0.00002657
Iteration 28/1000 | Loss: 0.00032371
Iteration 29/1000 | Loss: 0.00114228
Iteration 30/1000 | Loss: 0.00005367
Iteration 31/1000 | Loss: 0.00003079
Iteration 32/1000 | Loss: 0.00002529
Iteration 33/1000 | Loss: 0.00026251
Iteration 34/1000 | Loss: 0.00003112
Iteration 35/1000 | Loss: 0.00002468
Iteration 36/1000 | Loss: 0.00003344
Iteration 37/1000 | Loss: 0.00002442
Iteration 38/1000 | Loss: 0.00002434
Iteration 39/1000 | Loss: 0.00002433
Iteration 40/1000 | Loss: 0.00002433
Iteration 41/1000 | Loss: 0.00002433
Iteration 42/1000 | Loss: 0.00002433
Iteration 43/1000 | Loss: 0.00002433
Iteration 44/1000 | Loss: 0.00017441
Iteration 45/1000 | Loss: 0.00004426
Iteration 46/1000 | Loss: 0.00005655
Iteration 47/1000 | Loss: 0.00006107
Iteration 48/1000 | Loss: 0.00002308
Iteration 49/1000 | Loss: 0.00002289
Iteration 50/1000 | Loss: 0.00003654
Iteration 51/1000 | Loss: 0.00002245
Iteration 52/1000 | Loss: 0.00068604
Iteration 53/1000 | Loss: 0.00068604
Iteration 54/1000 | Loss: 0.00069094
Iteration 55/1000 | Loss: 0.00010781
Iteration 56/1000 | Loss: 0.00003255
Iteration 57/1000 | Loss: 0.00009819
Iteration 58/1000 | Loss: 0.00006767
Iteration 59/1000 | Loss: 0.00002233
Iteration 60/1000 | Loss: 0.00003727
Iteration 61/1000 | Loss: 0.00035825
Iteration 62/1000 | Loss: 0.00004117
Iteration 63/1000 | Loss: 0.00018484
Iteration 64/1000 | Loss: 0.00004562
Iteration 65/1000 | Loss: 0.00003191
Iteration 66/1000 | Loss: 0.00002218
Iteration 67/1000 | Loss: 0.00002177
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002176
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002174
Iteration 74/1000 | Loss: 0.00002174
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002174
Iteration 78/1000 | Loss: 0.00002173
Iteration 79/1000 | Loss: 0.00002173
Iteration 80/1000 | Loss: 0.00002173
Iteration 81/1000 | Loss: 0.00002173
Iteration 82/1000 | Loss: 0.00002172
Iteration 83/1000 | Loss: 0.00002172
Iteration 84/1000 | Loss: 0.00002171
Iteration 85/1000 | Loss: 0.00002171
Iteration 86/1000 | Loss: 0.00002171
Iteration 87/1000 | Loss: 0.00002171
Iteration 88/1000 | Loss: 0.00002171
Iteration 89/1000 | Loss: 0.00002171
Iteration 90/1000 | Loss: 0.00002170
Iteration 91/1000 | Loss: 0.00002854
Iteration 92/1000 | Loss: 0.00002171
Iteration 93/1000 | Loss: 0.00002170
Iteration 94/1000 | Loss: 0.00003291
Iteration 95/1000 | Loss: 0.00002364
Iteration 96/1000 | Loss: 0.00002548
Iteration 97/1000 | Loss: 0.00002169
Iteration 98/1000 | Loss: 0.00002168
Iteration 99/1000 | Loss: 0.00002168
Iteration 100/1000 | Loss: 0.00002167
Iteration 101/1000 | Loss: 0.00002167
Iteration 102/1000 | Loss: 0.00002167
Iteration 103/1000 | Loss: 0.00002167
Iteration 104/1000 | Loss: 0.00002167
Iteration 105/1000 | Loss: 0.00002166
Iteration 106/1000 | Loss: 0.00002166
Iteration 107/1000 | Loss: 0.00002166
Iteration 108/1000 | Loss: 0.00002166
Iteration 109/1000 | Loss: 0.00002165
Iteration 110/1000 | Loss: 0.00002165
Iteration 111/1000 | Loss: 0.00002165
Iteration 112/1000 | Loss: 0.00002165
Iteration 113/1000 | Loss: 0.00002165
Iteration 114/1000 | Loss: 0.00002165
Iteration 115/1000 | Loss: 0.00002164
Iteration 116/1000 | Loss: 0.00002164
Iteration 117/1000 | Loss: 0.00002164
Iteration 118/1000 | Loss: 0.00002164
Iteration 119/1000 | Loss: 0.00002164
Iteration 120/1000 | Loss: 0.00002164
Iteration 121/1000 | Loss: 0.00002164
Iteration 122/1000 | Loss: 0.00002164
Iteration 123/1000 | Loss: 0.00002164
Iteration 124/1000 | Loss: 0.00002164
Iteration 125/1000 | Loss: 0.00002164
Iteration 126/1000 | Loss: 0.00002164
Iteration 127/1000 | Loss: 0.00002164
Iteration 128/1000 | Loss: 0.00002163
Iteration 129/1000 | Loss: 0.00002163
Iteration 130/1000 | Loss: 0.00002163
Iteration 131/1000 | Loss: 0.00002163
Iteration 132/1000 | Loss: 0.00002163
Iteration 133/1000 | Loss: 0.00002163
Iteration 134/1000 | Loss: 0.00002162
Iteration 135/1000 | Loss: 0.00002162
Iteration 136/1000 | Loss: 0.00002162
Iteration 137/1000 | Loss: 0.00002162
Iteration 138/1000 | Loss: 0.00002161
Iteration 139/1000 | Loss: 0.00002161
Iteration 140/1000 | Loss: 0.00002161
Iteration 141/1000 | Loss: 0.00002161
Iteration 142/1000 | Loss: 0.00002161
Iteration 143/1000 | Loss: 0.00002161
Iteration 144/1000 | Loss: 0.00002161
Iteration 145/1000 | Loss: 0.00002161
Iteration 146/1000 | Loss: 0.00002161
Iteration 147/1000 | Loss: 0.00002161
Iteration 148/1000 | Loss: 0.00002161
Iteration 149/1000 | Loss: 0.00002161
Iteration 150/1000 | Loss: 0.00002161
Iteration 151/1000 | Loss: 0.00002161
Iteration 152/1000 | Loss: 0.00002161
Iteration 153/1000 | Loss: 0.00002161
Iteration 154/1000 | Loss: 0.00002160
Iteration 155/1000 | Loss: 0.00002160
Iteration 156/1000 | Loss: 0.00002160
Iteration 157/1000 | Loss: 0.00002160
Iteration 158/1000 | Loss: 0.00002160
Iteration 159/1000 | Loss: 0.00002160
Iteration 160/1000 | Loss: 0.00002160
Iteration 161/1000 | Loss: 0.00002160
Iteration 162/1000 | Loss: 0.00002160
Iteration 163/1000 | Loss: 0.00002160
Iteration 164/1000 | Loss: 0.00002160
Iteration 165/1000 | Loss: 0.00002160
Iteration 166/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.1601941625704058e-05, 2.1601941625704058e-05, 2.1601941625704058e-05, 2.1601941625704058e-05, 2.1601941625704058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1601941625704058e-05

Optimization complete. Final v2v error: 3.93022084236145 mm

Highest mean error: 5.443972587585449 mm for frame 93

Lowest mean error: 3.163968563079834 mm for frame 215

Saving results

Total time: 158.59972310066223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788349
Iteration 2/25 | Loss: 0.00141574
Iteration 3/25 | Loss: 0.00126916
Iteration 4/25 | Loss: 0.00125874
Iteration 5/25 | Loss: 0.00125520
Iteration 6/25 | Loss: 0.00125455
Iteration 7/25 | Loss: 0.00125455
Iteration 8/25 | Loss: 0.00125455
Iteration 9/25 | Loss: 0.00125455
Iteration 10/25 | Loss: 0.00125455
Iteration 11/25 | Loss: 0.00125455
Iteration 12/25 | Loss: 0.00125455
Iteration 13/25 | Loss: 0.00125455
Iteration 14/25 | Loss: 0.00125455
Iteration 15/25 | Loss: 0.00125455
Iteration 16/25 | Loss: 0.00125455
Iteration 17/25 | Loss: 0.00125455
Iteration 18/25 | Loss: 0.00125455
Iteration 19/25 | Loss: 0.00125455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012545495992526412, 0.0012545495992526412, 0.0012545495992526412, 0.0012545495992526412, 0.0012545495992526412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012545495992526412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21146977
Iteration 2/25 | Loss: 0.00088503
Iteration 3/25 | Loss: 0.00088503
Iteration 4/25 | Loss: 0.00088503
Iteration 5/25 | Loss: 0.00088503
Iteration 6/25 | Loss: 0.00088502
Iteration 7/25 | Loss: 0.00088502
Iteration 8/25 | Loss: 0.00088502
Iteration 9/25 | Loss: 0.00088502
Iteration 10/25 | Loss: 0.00088502
Iteration 11/25 | Loss: 0.00088502
Iteration 12/25 | Loss: 0.00088502
Iteration 13/25 | Loss: 0.00088502
Iteration 14/25 | Loss: 0.00088502
Iteration 15/25 | Loss: 0.00088502
Iteration 16/25 | Loss: 0.00088502
Iteration 17/25 | Loss: 0.00088502
Iteration 18/25 | Loss: 0.00088502
Iteration 19/25 | Loss: 0.00088502
Iteration 20/25 | Loss: 0.00088502
Iteration 21/25 | Loss: 0.00088502
Iteration 22/25 | Loss: 0.00088502
Iteration 23/25 | Loss: 0.00088502
Iteration 24/25 | Loss: 0.00088502
Iteration 25/25 | Loss: 0.00088502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088502
Iteration 2/1000 | Loss: 0.00005445
Iteration 3/1000 | Loss: 0.00003212
Iteration 4/1000 | Loss: 0.00002502
Iteration 5/1000 | Loss: 0.00002205
Iteration 6/1000 | Loss: 0.00002050
Iteration 7/1000 | Loss: 0.00001928
Iteration 8/1000 | Loss: 0.00001858
Iteration 9/1000 | Loss: 0.00001795
Iteration 10/1000 | Loss: 0.00001740
Iteration 11/1000 | Loss: 0.00001733
Iteration 12/1000 | Loss: 0.00001705
Iteration 13/1000 | Loss: 0.00001674
Iteration 14/1000 | Loss: 0.00001659
Iteration 15/1000 | Loss: 0.00001652
Iteration 16/1000 | Loss: 0.00001646
Iteration 17/1000 | Loss: 0.00001646
Iteration 18/1000 | Loss: 0.00001646
Iteration 19/1000 | Loss: 0.00001645
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001644
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001623
Iteration 24/1000 | Loss: 0.00001618
Iteration 25/1000 | Loss: 0.00001616
Iteration 26/1000 | Loss: 0.00001606
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001591
Iteration 30/1000 | Loss: 0.00001590
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001587
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001579
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001579
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001575
Iteration 57/1000 | Loss: 0.00001575
Iteration 58/1000 | Loss: 0.00001575
Iteration 59/1000 | Loss: 0.00001574
Iteration 60/1000 | Loss: 0.00001574
Iteration 61/1000 | Loss: 0.00001574
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001571
Iteration 65/1000 | Loss: 0.00001571
Iteration 66/1000 | Loss: 0.00001571
Iteration 67/1000 | Loss: 0.00001570
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001570
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001569
Iteration 73/1000 | Loss: 0.00001569
Iteration 74/1000 | Loss: 0.00001568
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001567
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001566
Iteration 85/1000 | Loss: 0.00001566
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001565
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001561
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001557
Iteration 119/1000 | Loss: 0.00001557
Iteration 120/1000 | Loss: 0.00001557
Iteration 121/1000 | Loss: 0.00001557
Iteration 122/1000 | Loss: 0.00001557
Iteration 123/1000 | Loss: 0.00001556
Iteration 124/1000 | Loss: 0.00001556
Iteration 125/1000 | Loss: 0.00001556
Iteration 126/1000 | Loss: 0.00001556
Iteration 127/1000 | Loss: 0.00001556
Iteration 128/1000 | Loss: 0.00001556
Iteration 129/1000 | Loss: 0.00001556
Iteration 130/1000 | Loss: 0.00001556
Iteration 131/1000 | Loss: 0.00001556
Iteration 132/1000 | Loss: 0.00001556
Iteration 133/1000 | Loss: 0.00001556
Iteration 134/1000 | Loss: 0.00001556
Iteration 135/1000 | Loss: 0.00001556
Iteration 136/1000 | Loss: 0.00001556
Iteration 137/1000 | Loss: 0.00001556
Iteration 138/1000 | Loss: 0.00001556
Iteration 139/1000 | Loss: 0.00001556
Iteration 140/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.555936069053132e-05, 1.555936069053132e-05, 1.555936069053132e-05, 1.555936069053132e-05, 1.555936069053132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.555936069053132e-05

Optimization complete. Final v2v error: 3.2763442993164062 mm

Highest mean error: 4.454316139221191 mm for frame 64

Lowest mean error: 2.8039729595184326 mm for frame 97

Saving results

Total time: 42.92241859436035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736405
Iteration 2/25 | Loss: 0.00211567
Iteration 3/25 | Loss: 0.00177967
Iteration 4/25 | Loss: 0.00168234
Iteration 5/25 | Loss: 0.00165728
Iteration 6/25 | Loss: 0.00156304
Iteration 7/25 | Loss: 0.00154805
Iteration 8/25 | Loss: 0.00154547
Iteration 9/25 | Loss: 0.00154109
Iteration 10/25 | Loss: 0.00154006
Iteration 11/25 | Loss: 0.00153996
Iteration 12/25 | Loss: 0.00153993
Iteration 13/25 | Loss: 0.00153993
Iteration 14/25 | Loss: 0.00153992
Iteration 15/25 | Loss: 0.00153992
Iteration 16/25 | Loss: 0.00153992
Iteration 17/25 | Loss: 0.00153992
Iteration 18/25 | Loss: 0.00153992
Iteration 19/25 | Loss: 0.00153992
Iteration 20/25 | Loss: 0.00153991
Iteration 21/25 | Loss: 0.00153991
Iteration 22/25 | Loss: 0.00153991
Iteration 23/25 | Loss: 0.00153991
Iteration 24/25 | Loss: 0.00153991
Iteration 25/25 | Loss: 0.00153991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42244935
Iteration 2/25 | Loss: 0.00114145
Iteration 3/25 | Loss: 0.00114143
Iteration 4/25 | Loss: 0.00114143
Iteration 5/25 | Loss: 0.00114143
Iteration 6/25 | Loss: 0.00114143
Iteration 7/25 | Loss: 0.00114143
Iteration 8/25 | Loss: 0.00114143
Iteration 9/25 | Loss: 0.00114143
Iteration 10/25 | Loss: 0.00114143
Iteration 11/25 | Loss: 0.00114143
Iteration 12/25 | Loss: 0.00114143
Iteration 13/25 | Loss: 0.00114143
Iteration 14/25 | Loss: 0.00114143
Iteration 15/25 | Loss: 0.00114143
Iteration 16/25 | Loss: 0.00114143
Iteration 17/25 | Loss: 0.00114143
Iteration 18/25 | Loss: 0.00114143
Iteration 19/25 | Loss: 0.00114143
Iteration 20/25 | Loss: 0.00114143
Iteration 21/25 | Loss: 0.00114143
Iteration 22/25 | Loss: 0.00114143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011414282489567995, 0.0011414282489567995, 0.0011414282489567995, 0.0011414282489567995, 0.0011414282489567995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011414282489567995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114143
Iteration 2/1000 | Loss: 0.00009731
Iteration 3/1000 | Loss: 0.00007063
Iteration 4/1000 | Loss: 0.00006388
Iteration 5/1000 | Loss: 0.00006041
Iteration 6/1000 | Loss: 0.00005801
Iteration 7/1000 | Loss: 0.00005660
Iteration 8/1000 | Loss: 0.00005542
Iteration 9/1000 | Loss: 0.00005465
Iteration 10/1000 | Loss: 0.00005396
Iteration 11/1000 | Loss: 0.00005343
Iteration 12/1000 | Loss: 0.00005320
Iteration 13/1000 | Loss: 0.00005299
Iteration 14/1000 | Loss: 0.00005282
Iteration 15/1000 | Loss: 0.00005274
Iteration 16/1000 | Loss: 0.00005270
Iteration 17/1000 | Loss: 0.00005263
Iteration 18/1000 | Loss: 0.00005257
Iteration 19/1000 | Loss: 0.00005256
Iteration 20/1000 | Loss: 0.00005255
Iteration 21/1000 | Loss: 0.00005254
Iteration 22/1000 | Loss: 0.00005253
Iteration 23/1000 | Loss: 0.00005251
Iteration 24/1000 | Loss: 0.00005250
Iteration 25/1000 | Loss: 0.00005250
Iteration 26/1000 | Loss: 0.00005249
Iteration 27/1000 | Loss: 0.00005249
Iteration 28/1000 | Loss: 0.00005249
Iteration 29/1000 | Loss: 0.00005248
Iteration 30/1000 | Loss: 0.00005248
Iteration 31/1000 | Loss: 0.00005247
Iteration 32/1000 | Loss: 0.00005247
Iteration 33/1000 | Loss: 0.00005246
Iteration 34/1000 | Loss: 0.00005246
Iteration 35/1000 | Loss: 0.00005245
Iteration 36/1000 | Loss: 0.00005245
Iteration 37/1000 | Loss: 0.00005245
Iteration 38/1000 | Loss: 0.00005245
Iteration 39/1000 | Loss: 0.00005244
Iteration 40/1000 | Loss: 0.00005244
Iteration 41/1000 | Loss: 0.00005243
Iteration 42/1000 | Loss: 0.00005242
Iteration 43/1000 | Loss: 0.00005242
Iteration 44/1000 | Loss: 0.00005242
Iteration 45/1000 | Loss: 0.00005242
Iteration 46/1000 | Loss: 0.00005242
Iteration 47/1000 | Loss: 0.00005242
Iteration 48/1000 | Loss: 0.00005241
Iteration 49/1000 | Loss: 0.00005241
Iteration 50/1000 | Loss: 0.00005239
Iteration 51/1000 | Loss: 0.00005238
Iteration 52/1000 | Loss: 0.00005238
Iteration 53/1000 | Loss: 0.00005238
Iteration 54/1000 | Loss: 0.00005238
Iteration 55/1000 | Loss: 0.00005238
Iteration 56/1000 | Loss: 0.00005237
Iteration 57/1000 | Loss: 0.00005237
Iteration 58/1000 | Loss: 0.00005236
Iteration 59/1000 | Loss: 0.00005236
Iteration 60/1000 | Loss: 0.00005235
Iteration 61/1000 | Loss: 0.00005235
Iteration 62/1000 | Loss: 0.00005235
Iteration 63/1000 | Loss: 0.00005235
Iteration 64/1000 | Loss: 0.00005235
Iteration 65/1000 | Loss: 0.00005235
Iteration 66/1000 | Loss: 0.00005235
Iteration 67/1000 | Loss: 0.00005235
Iteration 68/1000 | Loss: 0.00005235
Iteration 69/1000 | Loss: 0.00005235
Iteration 70/1000 | Loss: 0.00005235
Iteration 71/1000 | Loss: 0.00005235
Iteration 72/1000 | Loss: 0.00005234
Iteration 73/1000 | Loss: 0.00005234
Iteration 74/1000 | Loss: 0.00005233
Iteration 75/1000 | Loss: 0.00005233
Iteration 76/1000 | Loss: 0.00005233
Iteration 77/1000 | Loss: 0.00005233
Iteration 78/1000 | Loss: 0.00005233
Iteration 79/1000 | Loss: 0.00005232
Iteration 80/1000 | Loss: 0.00005232
Iteration 81/1000 | Loss: 0.00005232
Iteration 82/1000 | Loss: 0.00005231
Iteration 83/1000 | Loss: 0.00005231
Iteration 84/1000 | Loss: 0.00005231
Iteration 85/1000 | Loss: 0.00005231
Iteration 86/1000 | Loss: 0.00005231
Iteration 87/1000 | Loss: 0.00005231
Iteration 88/1000 | Loss: 0.00005231
Iteration 89/1000 | Loss: 0.00005231
Iteration 90/1000 | Loss: 0.00005231
Iteration 91/1000 | Loss: 0.00005231
Iteration 92/1000 | Loss: 0.00005231
Iteration 93/1000 | Loss: 0.00005231
Iteration 94/1000 | Loss: 0.00005231
Iteration 95/1000 | Loss: 0.00005231
Iteration 96/1000 | Loss: 0.00005231
Iteration 97/1000 | Loss: 0.00005231
Iteration 98/1000 | Loss: 0.00005231
Iteration 99/1000 | Loss: 0.00005231
Iteration 100/1000 | Loss: 0.00005231
Iteration 101/1000 | Loss: 0.00005231
Iteration 102/1000 | Loss: 0.00005231
Iteration 103/1000 | Loss: 0.00005231
Iteration 104/1000 | Loss: 0.00005231
Iteration 105/1000 | Loss: 0.00005231
Iteration 106/1000 | Loss: 0.00005231
Iteration 107/1000 | Loss: 0.00005231
Iteration 108/1000 | Loss: 0.00005231
Iteration 109/1000 | Loss: 0.00005231
Iteration 110/1000 | Loss: 0.00005231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [5.230674287304282e-05, 5.230674287304282e-05, 5.230674287304282e-05, 5.230674287304282e-05, 5.230674287304282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.230674287304282e-05

Optimization complete. Final v2v error: 5.956024646759033 mm

Highest mean error: 6.8743696212768555 mm for frame 39

Lowest mean error: 5.183326721191406 mm for frame 217

Saving results

Total time: 55.92304539680481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016996
Iteration 2/25 | Loss: 0.00219249
Iteration 3/25 | Loss: 0.00162568
Iteration 4/25 | Loss: 0.00154232
Iteration 5/25 | Loss: 0.00173945
Iteration 6/25 | Loss: 0.00144337
Iteration 7/25 | Loss: 0.00134214
Iteration 8/25 | Loss: 0.00135207
Iteration 9/25 | Loss: 0.00132105
Iteration 10/25 | Loss: 0.00131688
Iteration 11/25 | Loss: 0.00131167
Iteration 12/25 | Loss: 0.00131297
Iteration 13/25 | Loss: 0.00131845
Iteration 14/25 | Loss: 0.00131025
Iteration 15/25 | Loss: 0.00131155
Iteration 16/25 | Loss: 0.00130582
Iteration 17/25 | Loss: 0.00129666
Iteration 18/25 | Loss: 0.00129346
Iteration 19/25 | Loss: 0.00129290
Iteration 20/25 | Loss: 0.00129280
Iteration 21/25 | Loss: 0.00129279
Iteration 22/25 | Loss: 0.00129279
Iteration 23/25 | Loss: 0.00129279
Iteration 24/25 | Loss: 0.00129279
Iteration 25/25 | Loss: 0.00129279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48276138
Iteration 2/25 | Loss: 0.00381418
Iteration 3/25 | Loss: 0.00135515
Iteration 4/25 | Loss: 0.00135515
Iteration 5/25 | Loss: 0.00135514
Iteration 6/25 | Loss: 0.00135514
Iteration 7/25 | Loss: 0.00135514
Iteration 8/25 | Loss: 0.00135514
Iteration 9/25 | Loss: 0.00135514
Iteration 10/25 | Loss: 0.00135514
Iteration 11/25 | Loss: 0.00135514
Iteration 12/25 | Loss: 0.00135514
Iteration 13/25 | Loss: 0.00135514
Iteration 14/25 | Loss: 0.00135514
Iteration 15/25 | Loss: 0.00135514
Iteration 16/25 | Loss: 0.00135514
Iteration 17/25 | Loss: 0.00135514
Iteration 18/25 | Loss: 0.00135514
Iteration 19/25 | Loss: 0.00135514
Iteration 20/25 | Loss: 0.00135514
Iteration 21/25 | Loss: 0.00135514
Iteration 22/25 | Loss: 0.00135514
Iteration 23/25 | Loss: 0.00135514
Iteration 24/25 | Loss: 0.00135514
Iteration 25/25 | Loss: 0.00135514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135514
Iteration 2/1000 | Loss: 0.00010829
Iteration 3/1000 | Loss: 0.00006794
Iteration 4/1000 | Loss: 0.00005700
Iteration 5/1000 | Loss: 0.00016291
Iteration 6/1000 | Loss: 0.00005122
Iteration 7/1000 | Loss: 0.00004670
Iteration 8/1000 | Loss: 0.00020511
Iteration 9/1000 | Loss: 0.00005471
Iteration 10/1000 | Loss: 0.00009182
Iteration 11/1000 | Loss: 0.00004350
Iteration 12/1000 | Loss: 0.00012104
Iteration 13/1000 | Loss: 0.00004280
Iteration 14/1000 | Loss: 0.00004082
Iteration 15/1000 | Loss: 0.00003993
Iteration 16/1000 | Loss: 0.00003929
Iteration 17/1000 | Loss: 0.00003877
Iteration 18/1000 | Loss: 0.00018124
Iteration 19/1000 | Loss: 0.00004658
Iteration 20/1000 | Loss: 0.00005546
Iteration 21/1000 | Loss: 0.00003835
Iteration 22/1000 | Loss: 0.00003834
Iteration 23/1000 | Loss: 0.00003833
Iteration 24/1000 | Loss: 0.00003825
Iteration 25/1000 | Loss: 0.00003823
Iteration 26/1000 | Loss: 0.00003823
Iteration 27/1000 | Loss: 0.00003822
Iteration 28/1000 | Loss: 0.00003822
Iteration 29/1000 | Loss: 0.00009223
Iteration 30/1000 | Loss: 0.00005946
Iteration 31/1000 | Loss: 0.00003818
Iteration 32/1000 | Loss: 0.00007155
Iteration 33/1000 | Loss: 0.00003834
Iteration 34/1000 | Loss: 0.00003811
Iteration 35/1000 | Loss: 0.00003807
Iteration 36/1000 | Loss: 0.00007185
Iteration 37/1000 | Loss: 0.00004015
Iteration 38/1000 | Loss: 0.00003794
Iteration 39/1000 | Loss: 0.00007573
Iteration 40/1000 | Loss: 0.00003788
Iteration 41/1000 | Loss: 0.00003777
Iteration 42/1000 | Loss: 0.00003776
Iteration 43/1000 | Loss: 0.00003772
Iteration 44/1000 | Loss: 0.00003771
Iteration 45/1000 | Loss: 0.00003771
Iteration 46/1000 | Loss: 0.00003771
Iteration 47/1000 | Loss: 0.00003763
Iteration 48/1000 | Loss: 0.00015107
Iteration 49/1000 | Loss: 0.00003792
Iteration 50/1000 | Loss: 0.00003761
Iteration 51/1000 | Loss: 0.00003758
Iteration 52/1000 | Loss: 0.00003758
Iteration 53/1000 | Loss: 0.00003758
Iteration 54/1000 | Loss: 0.00003758
Iteration 55/1000 | Loss: 0.00003758
Iteration 56/1000 | Loss: 0.00003758
Iteration 57/1000 | Loss: 0.00003758
Iteration 58/1000 | Loss: 0.00003758
Iteration 59/1000 | Loss: 0.00003758
Iteration 60/1000 | Loss: 0.00003758
Iteration 61/1000 | Loss: 0.00003757
Iteration 62/1000 | Loss: 0.00003757
Iteration 63/1000 | Loss: 0.00003757
Iteration 64/1000 | Loss: 0.00003757
Iteration 65/1000 | Loss: 0.00003757
Iteration 66/1000 | Loss: 0.00003757
Iteration 67/1000 | Loss: 0.00003757
Iteration 68/1000 | Loss: 0.00003757
Iteration 69/1000 | Loss: 0.00003757
Iteration 70/1000 | Loss: 0.00003757
Iteration 71/1000 | Loss: 0.00003757
Iteration 72/1000 | Loss: 0.00003757
Iteration 73/1000 | Loss: 0.00003756
Iteration 74/1000 | Loss: 0.00003756
Iteration 75/1000 | Loss: 0.00003756
Iteration 76/1000 | Loss: 0.00003756
Iteration 77/1000 | Loss: 0.00003756
Iteration 78/1000 | Loss: 0.00003756
Iteration 79/1000 | Loss: 0.00003756
Iteration 80/1000 | Loss: 0.00003756
Iteration 81/1000 | Loss: 0.00003756
Iteration 82/1000 | Loss: 0.00003756
Iteration 83/1000 | Loss: 0.00003756
Iteration 84/1000 | Loss: 0.00003756
Iteration 85/1000 | Loss: 0.00003755
Iteration 86/1000 | Loss: 0.00003755
Iteration 87/1000 | Loss: 0.00003755
Iteration 88/1000 | Loss: 0.00003755
Iteration 89/1000 | Loss: 0.00003755
Iteration 90/1000 | Loss: 0.00003755
Iteration 91/1000 | Loss: 0.00003754
Iteration 92/1000 | Loss: 0.00003754
Iteration 93/1000 | Loss: 0.00003754
Iteration 94/1000 | Loss: 0.00003754
Iteration 95/1000 | Loss: 0.00003754
Iteration 96/1000 | Loss: 0.00003753
Iteration 97/1000 | Loss: 0.00003753
Iteration 98/1000 | Loss: 0.00003753
Iteration 99/1000 | Loss: 0.00003753
Iteration 100/1000 | Loss: 0.00003753
Iteration 101/1000 | Loss: 0.00003753
Iteration 102/1000 | Loss: 0.00003753
Iteration 103/1000 | Loss: 0.00003752
Iteration 104/1000 | Loss: 0.00003752
Iteration 105/1000 | Loss: 0.00003752
Iteration 106/1000 | Loss: 0.00003752
Iteration 107/1000 | Loss: 0.00003752
Iteration 108/1000 | Loss: 0.00003752
Iteration 109/1000 | Loss: 0.00003752
Iteration 110/1000 | Loss: 0.00003752
Iteration 111/1000 | Loss: 0.00003751
Iteration 112/1000 | Loss: 0.00003751
Iteration 113/1000 | Loss: 0.00003751
Iteration 114/1000 | Loss: 0.00003751
Iteration 115/1000 | Loss: 0.00003751
Iteration 116/1000 | Loss: 0.00003750
Iteration 117/1000 | Loss: 0.00003750
Iteration 118/1000 | Loss: 0.00003750
Iteration 119/1000 | Loss: 0.00003750
Iteration 120/1000 | Loss: 0.00003749
Iteration 121/1000 | Loss: 0.00003749
Iteration 122/1000 | Loss: 0.00003749
Iteration 123/1000 | Loss: 0.00003749
Iteration 124/1000 | Loss: 0.00003749
Iteration 125/1000 | Loss: 0.00003749
Iteration 126/1000 | Loss: 0.00003749
Iteration 127/1000 | Loss: 0.00003749
Iteration 128/1000 | Loss: 0.00003748
Iteration 129/1000 | Loss: 0.00003748
Iteration 130/1000 | Loss: 0.00003748
Iteration 131/1000 | Loss: 0.00003748
Iteration 132/1000 | Loss: 0.00003748
Iteration 133/1000 | Loss: 0.00003748
Iteration 134/1000 | Loss: 0.00003748
Iteration 135/1000 | Loss: 0.00003747
Iteration 136/1000 | Loss: 0.00003747
Iteration 137/1000 | Loss: 0.00003747
Iteration 138/1000 | Loss: 0.00003747
Iteration 139/1000 | Loss: 0.00003747
Iteration 140/1000 | Loss: 0.00003746
Iteration 141/1000 | Loss: 0.00003746
Iteration 142/1000 | Loss: 0.00003746
Iteration 143/1000 | Loss: 0.00003746
Iteration 144/1000 | Loss: 0.00003745
Iteration 145/1000 | Loss: 0.00003745
Iteration 146/1000 | Loss: 0.00003745
Iteration 147/1000 | Loss: 0.00003745
Iteration 148/1000 | Loss: 0.00003745
Iteration 149/1000 | Loss: 0.00003745
Iteration 150/1000 | Loss: 0.00003745
Iteration 151/1000 | Loss: 0.00003745
Iteration 152/1000 | Loss: 0.00003744
Iteration 153/1000 | Loss: 0.00003744
Iteration 154/1000 | Loss: 0.00003744
Iteration 155/1000 | Loss: 0.00003744
Iteration 156/1000 | Loss: 0.00003744
Iteration 157/1000 | Loss: 0.00003744
Iteration 158/1000 | Loss: 0.00003744
Iteration 159/1000 | Loss: 0.00003743
Iteration 160/1000 | Loss: 0.00003743
Iteration 161/1000 | Loss: 0.00003743
Iteration 162/1000 | Loss: 0.00003743
Iteration 163/1000 | Loss: 0.00003742
Iteration 164/1000 | Loss: 0.00003742
Iteration 165/1000 | Loss: 0.00003742
Iteration 166/1000 | Loss: 0.00003742
Iteration 167/1000 | Loss: 0.00003742
Iteration 168/1000 | Loss: 0.00003742
Iteration 169/1000 | Loss: 0.00003742
Iteration 170/1000 | Loss: 0.00003742
Iteration 171/1000 | Loss: 0.00003742
Iteration 172/1000 | Loss: 0.00003742
Iteration 173/1000 | Loss: 0.00003742
Iteration 174/1000 | Loss: 0.00003742
Iteration 175/1000 | Loss: 0.00003742
Iteration 176/1000 | Loss: 0.00003742
Iteration 177/1000 | Loss: 0.00003742
Iteration 178/1000 | Loss: 0.00003742
Iteration 179/1000 | Loss: 0.00003742
Iteration 180/1000 | Loss: 0.00003742
Iteration 181/1000 | Loss: 0.00003742
Iteration 182/1000 | Loss: 0.00003742
Iteration 183/1000 | Loss: 0.00003742
Iteration 184/1000 | Loss: 0.00003742
Iteration 185/1000 | Loss: 0.00003742
Iteration 186/1000 | Loss: 0.00003742
Iteration 187/1000 | Loss: 0.00003742
Iteration 188/1000 | Loss: 0.00003742
Iteration 189/1000 | Loss: 0.00003742
Iteration 190/1000 | Loss: 0.00003742
Iteration 191/1000 | Loss: 0.00003742
Iteration 192/1000 | Loss: 0.00003742
Iteration 193/1000 | Loss: 0.00003742
Iteration 194/1000 | Loss: 0.00003742
Iteration 195/1000 | Loss: 0.00003742
Iteration 196/1000 | Loss: 0.00003742
Iteration 197/1000 | Loss: 0.00003742
Iteration 198/1000 | Loss: 0.00003742
Iteration 199/1000 | Loss: 0.00003742
Iteration 200/1000 | Loss: 0.00003742
Iteration 201/1000 | Loss: 0.00003742
Iteration 202/1000 | Loss: 0.00003742
Iteration 203/1000 | Loss: 0.00003742
Iteration 204/1000 | Loss: 0.00003742
Iteration 205/1000 | Loss: 0.00003742
Iteration 206/1000 | Loss: 0.00003742
Iteration 207/1000 | Loss: 0.00003742
Iteration 208/1000 | Loss: 0.00003742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [3.741812543012202e-05, 3.741812543012202e-05, 3.741812543012202e-05, 3.741812543012202e-05, 3.741812543012202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.741812543012202e-05

Optimization complete. Final v2v error: 3.669259786605835 mm

Highest mean error: 12.676919937133789 mm for frame 97

Lowest mean error: 2.8487894535064697 mm for frame 131

Saving results

Total time: 94.3931074142456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872263
Iteration 2/25 | Loss: 0.00141726
Iteration 3/25 | Loss: 0.00130605
Iteration 4/25 | Loss: 0.00128506
Iteration 5/25 | Loss: 0.00128082
Iteration 6/25 | Loss: 0.00127962
Iteration 7/25 | Loss: 0.00127962
Iteration 8/25 | Loss: 0.00127962
Iteration 9/25 | Loss: 0.00127962
Iteration 10/25 | Loss: 0.00127962
Iteration 11/25 | Loss: 0.00127962
Iteration 12/25 | Loss: 0.00127962
Iteration 13/25 | Loss: 0.00127962
Iteration 14/25 | Loss: 0.00127962
Iteration 15/25 | Loss: 0.00127962
Iteration 16/25 | Loss: 0.00127962
Iteration 17/25 | Loss: 0.00127962
Iteration 18/25 | Loss: 0.00127962
Iteration 19/25 | Loss: 0.00127962
Iteration 20/25 | Loss: 0.00127962
Iteration 21/25 | Loss: 0.00127962
Iteration 22/25 | Loss: 0.00127962
Iteration 23/25 | Loss: 0.00127962
Iteration 24/25 | Loss: 0.00127962
Iteration 25/25 | Loss: 0.00127962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44782507
Iteration 2/25 | Loss: 0.00080427
Iteration 3/25 | Loss: 0.00080427
Iteration 4/25 | Loss: 0.00080427
Iteration 5/25 | Loss: 0.00080427
Iteration 6/25 | Loss: 0.00080427
Iteration 7/25 | Loss: 0.00080427
Iteration 8/25 | Loss: 0.00080427
Iteration 9/25 | Loss: 0.00080427
Iteration 10/25 | Loss: 0.00080427
Iteration 11/25 | Loss: 0.00080427
Iteration 12/25 | Loss: 0.00080427
Iteration 13/25 | Loss: 0.00080427
Iteration 14/25 | Loss: 0.00080427
Iteration 15/25 | Loss: 0.00080427
Iteration 16/25 | Loss: 0.00080427
Iteration 17/25 | Loss: 0.00080427
Iteration 18/25 | Loss: 0.00080427
Iteration 19/25 | Loss: 0.00080427
Iteration 20/25 | Loss: 0.00080427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008042680565267801, 0.0008042680565267801, 0.0008042680565267801, 0.0008042680565267801, 0.0008042680565267801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008042680565267801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080427
Iteration 2/1000 | Loss: 0.00003328
Iteration 3/1000 | Loss: 0.00002324
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00002053
Iteration 7/1000 | Loss: 0.00002012
Iteration 8/1000 | Loss: 0.00001993
Iteration 9/1000 | Loss: 0.00001991
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001943
Iteration 13/1000 | Loss: 0.00001941
Iteration 14/1000 | Loss: 0.00001939
Iteration 15/1000 | Loss: 0.00001938
Iteration 16/1000 | Loss: 0.00001928
Iteration 17/1000 | Loss: 0.00001917
Iteration 18/1000 | Loss: 0.00001915
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001909
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001904
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001903
Iteration 29/1000 | Loss: 0.00001902
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001900
Iteration 32/1000 | Loss: 0.00001900
Iteration 33/1000 | Loss: 0.00001900
Iteration 34/1000 | Loss: 0.00001900
Iteration 35/1000 | Loss: 0.00001900
Iteration 36/1000 | Loss: 0.00001900
Iteration 37/1000 | Loss: 0.00001900
Iteration 38/1000 | Loss: 0.00001900
Iteration 39/1000 | Loss: 0.00001900
Iteration 40/1000 | Loss: 0.00001899
Iteration 41/1000 | Loss: 0.00001899
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001899
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001897
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001897
Iteration 48/1000 | Loss: 0.00001897
Iteration 49/1000 | Loss: 0.00001897
Iteration 50/1000 | Loss: 0.00001897
Iteration 51/1000 | Loss: 0.00001897
Iteration 52/1000 | Loss: 0.00001897
Iteration 53/1000 | Loss: 0.00001896
Iteration 54/1000 | Loss: 0.00001896
Iteration 55/1000 | Loss: 0.00001895
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001894
Iteration 58/1000 | Loss: 0.00001894
Iteration 59/1000 | Loss: 0.00001893
Iteration 60/1000 | Loss: 0.00001893
Iteration 61/1000 | Loss: 0.00001893
Iteration 62/1000 | Loss: 0.00001893
Iteration 63/1000 | Loss: 0.00001893
Iteration 64/1000 | Loss: 0.00001893
Iteration 65/1000 | Loss: 0.00001893
Iteration 66/1000 | Loss: 0.00001893
Iteration 67/1000 | Loss: 0.00001893
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001892
Iteration 70/1000 | Loss: 0.00001892
Iteration 71/1000 | Loss: 0.00001891
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001888
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001885
Iteration 84/1000 | Loss: 0.00001885
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001885
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001883
Iteration 102/1000 | Loss: 0.00001883
Iteration 103/1000 | Loss: 0.00001883
Iteration 104/1000 | Loss: 0.00001883
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001882
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Iteration 122/1000 | Loss: 0.00001881
Iteration 123/1000 | Loss: 0.00001881
Iteration 124/1000 | Loss: 0.00001881
Iteration 125/1000 | Loss: 0.00001881
Iteration 126/1000 | Loss: 0.00001881
Iteration 127/1000 | Loss: 0.00001881
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001880
Iteration 131/1000 | Loss: 0.00001880
Iteration 132/1000 | Loss: 0.00001880
Iteration 133/1000 | Loss: 0.00001880
Iteration 134/1000 | Loss: 0.00001880
Iteration 135/1000 | Loss: 0.00001880
Iteration 136/1000 | Loss: 0.00001880
Iteration 137/1000 | Loss: 0.00001880
Iteration 138/1000 | Loss: 0.00001880
Iteration 139/1000 | Loss: 0.00001880
Iteration 140/1000 | Loss: 0.00001880
Iteration 141/1000 | Loss: 0.00001880
Iteration 142/1000 | Loss: 0.00001880
Iteration 143/1000 | Loss: 0.00001880
Iteration 144/1000 | Loss: 0.00001880
Iteration 145/1000 | Loss: 0.00001880
Iteration 146/1000 | Loss: 0.00001880
Iteration 147/1000 | Loss: 0.00001880
Iteration 148/1000 | Loss: 0.00001880
Iteration 149/1000 | Loss: 0.00001880
Iteration 150/1000 | Loss: 0.00001880
Iteration 151/1000 | Loss: 0.00001880
Iteration 152/1000 | Loss: 0.00001880
Iteration 153/1000 | Loss: 0.00001880
Iteration 154/1000 | Loss: 0.00001880
Iteration 155/1000 | Loss: 0.00001880
Iteration 156/1000 | Loss: 0.00001880
Iteration 157/1000 | Loss: 0.00001880
Iteration 158/1000 | Loss: 0.00001880
Iteration 159/1000 | Loss: 0.00001880
Iteration 160/1000 | Loss: 0.00001880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.879553747130558e-05, 1.879553747130558e-05, 1.879553747130558e-05, 1.879553747130558e-05, 1.879553747130558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.879553747130558e-05

Optimization complete. Final v2v error: 3.6724212169647217 mm

Highest mean error: 3.99306321144104 mm for frame 6

Lowest mean error: 3.3632800579071045 mm for frame 34

Saving results

Total time: 35.285860538482666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923199
Iteration 2/25 | Loss: 0.00174925
Iteration 3/25 | Loss: 0.00140999
Iteration 4/25 | Loss: 0.00138643
Iteration 5/25 | Loss: 0.00138156
Iteration 6/25 | Loss: 0.00138027
Iteration 7/25 | Loss: 0.00138027
Iteration 8/25 | Loss: 0.00138027
Iteration 9/25 | Loss: 0.00138027
Iteration 10/25 | Loss: 0.00138027
Iteration 11/25 | Loss: 0.00138027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013802734902128577, 0.0013802734902128577, 0.0013802734902128577, 0.0013802734902128577, 0.0013802734902128577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013802734902128577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98964971
Iteration 2/25 | Loss: 0.00087031
Iteration 3/25 | Loss: 0.00087029
Iteration 4/25 | Loss: 0.00087029
Iteration 5/25 | Loss: 0.00087029
Iteration 6/25 | Loss: 0.00087029
Iteration 7/25 | Loss: 0.00087029
Iteration 8/25 | Loss: 0.00087029
Iteration 9/25 | Loss: 0.00087029
Iteration 10/25 | Loss: 0.00087029
Iteration 11/25 | Loss: 0.00087029
Iteration 12/25 | Loss: 0.00087029
Iteration 13/25 | Loss: 0.00087029
Iteration 14/25 | Loss: 0.00087029
Iteration 15/25 | Loss: 0.00087029
Iteration 16/25 | Loss: 0.00087029
Iteration 17/25 | Loss: 0.00087029
Iteration 18/25 | Loss: 0.00087029
Iteration 19/25 | Loss: 0.00087029
Iteration 20/25 | Loss: 0.00087029
Iteration 21/25 | Loss: 0.00087029
Iteration 22/25 | Loss: 0.00087029
Iteration 23/25 | Loss: 0.00087029
Iteration 24/25 | Loss: 0.00087029
Iteration 25/25 | Loss: 0.00087029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087029
Iteration 2/1000 | Loss: 0.00005026
Iteration 3/1000 | Loss: 0.00003419
Iteration 4/1000 | Loss: 0.00002935
Iteration 5/1000 | Loss: 0.00002738
Iteration 6/1000 | Loss: 0.00002593
Iteration 7/1000 | Loss: 0.00002521
Iteration 8/1000 | Loss: 0.00002438
Iteration 9/1000 | Loss: 0.00002385
Iteration 10/1000 | Loss: 0.00002354
Iteration 11/1000 | Loss: 0.00002329
Iteration 12/1000 | Loss: 0.00002308
Iteration 13/1000 | Loss: 0.00002289
Iteration 14/1000 | Loss: 0.00002271
Iteration 15/1000 | Loss: 0.00002255
Iteration 16/1000 | Loss: 0.00002253
Iteration 17/1000 | Loss: 0.00002248
Iteration 18/1000 | Loss: 0.00002247
Iteration 19/1000 | Loss: 0.00002237
Iteration 20/1000 | Loss: 0.00002232
Iteration 21/1000 | Loss: 0.00002225
Iteration 22/1000 | Loss: 0.00002222
Iteration 23/1000 | Loss: 0.00002221
Iteration 24/1000 | Loss: 0.00002215
Iteration 25/1000 | Loss: 0.00002214
Iteration 26/1000 | Loss: 0.00002212
Iteration 27/1000 | Loss: 0.00002209
Iteration 28/1000 | Loss: 0.00002207
Iteration 29/1000 | Loss: 0.00002206
Iteration 30/1000 | Loss: 0.00002205
Iteration 31/1000 | Loss: 0.00002204
Iteration 32/1000 | Loss: 0.00002203
Iteration 33/1000 | Loss: 0.00002203
Iteration 34/1000 | Loss: 0.00002202
Iteration 35/1000 | Loss: 0.00002202
Iteration 36/1000 | Loss: 0.00002201
Iteration 37/1000 | Loss: 0.00002201
Iteration 38/1000 | Loss: 0.00002198
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002195
Iteration 41/1000 | Loss: 0.00002195
Iteration 42/1000 | Loss: 0.00002195
Iteration 43/1000 | Loss: 0.00002195
Iteration 44/1000 | Loss: 0.00002194
Iteration 45/1000 | Loss: 0.00002194
Iteration 46/1000 | Loss: 0.00002193
Iteration 47/1000 | Loss: 0.00002193
Iteration 48/1000 | Loss: 0.00002193
Iteration 49/1000 | Loss: 0.00002193
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002193
Iteration 53/1000 | Loss: 0.00002193
Iteration 54/1000 | Loss: 0.00002192
Iteration 55/1000 | Loss: 0.00002192
Iteration 56/1000 | Loss: 0.00002192
Iteration 57/1000 | Loss: 0.00002192
Iteration 58/1000 | Loss: 0.00002191
Iteration 59/1000 | Loss: 0.00002191
Iteration 60/1000 | Loss: 0.00002191
Iteration 61/1000 | Loss: 0.00002190
Iteration 62/1000 | Loss: 0.00002190
Iteration 63/1000 | Loss: 0.00002190
Iteration 64/1000 | Loss: 0.00002189
Iteration 65/1000 | Loss: 0.00002189
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002188
Iteration 69/1000 | Loss: 0.00002188
Iteration 70/1000 | Loss: 0.00002188
Iteration 71/1000 | Loss: 0.00002187
Iteration 72/1000 | Loss: 0.00002187
Iteration 73/1000 | Loss: 0.00002187
Iteration 74/1000 | Loss: 0.00002187
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002187
Iteration 79/1000 | Loss: 0.00002186
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002186
Iteration 83/1000 | Loss: 0.00002186
Iteration 84/1000 | Loss: 0.00002186
Iteration 85/1000 | Loss: 0.00002185
Iteration 86/1000 | Loss: 0.00002185
Iteration 87/1000 | Loss: 0.00002185
Iteration 88/1000 | Loss: 0.00002185
Iteration 89/1000 | Loss: 0.00002185
Iteration 90/1000 | Loss: 0.00002185
Iteration 91/1000 | Loss: 0.00002185
Iteration 92/1000 | Loss: 0.00002185
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002185
Iteration 95/1000 | Loss: 0.00002185
Iteration 96/1000 | Loss: 0.00002184
Iteration 97/1000 | Loss: 0.00002184
Iteration 98/1000 | Loss: 0.00002184
Iteration 99/1000 | Loss: 0.00002184
Iteration 100/1000 | Loss: 0.00002184
Iteration 101/1000 | Loss: 0.00002184
Iteration 102/1000 | Loss: 0.00002184
Iteration 103/1000 | Loss: 0.00002184
Iteration 104/1000 | Loss: 0.00002183
Iteration 105/1000 | Loss: 0.00002183
Iteration 106/1000 | Loss: 0.00002183
Iteration 107/1000 | Loss: 0.00002183
Iteration 108/1000 | Loss: 0.00002183
Iteration 109/1000 | Loss: 0.00002183
Iteration 110/1000 | Loss: 0.00002183
Iteration 111/1000 | Loss: 0.00002183
Iteration 112/1000 | Loss: 0.00002183
Iteration 113/1000 | Loss: 0.00002183
Iteration 114/1000 | Loss: 0.00002183
Iteration 115/1000 | Loss: 0.00002183
Iteration 116/1000 | Loss: 0.00002182
Iteration 117/1000 | Loss: 0.00002182
Iteration 118/1000 | Loss: 0.00002182
Iteration 119/1000 | Loss: 0.00002182
Iteration 120/1000 | Loss: 0.00002182
Iteration 121/1000 | Loss: 0.00002182
Iteration 122/1000 | Loss: 0.00002182
Iteration 123/1000 | Loss: 0.00002182
Iteration 124/1000 | Loss: 0.00002182
Iteration 125/1000 | Loss: 0.00002182
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002182
Iteration 128/1000 | Loss: 0.00002182
Iteration 129/1000 | Loss: 0.00002182
Iteration 130/1000 | Loss: 0.00002182
Iteration 131/1000 | Loss: 0.00002182
Iteration 132/1000 | Loss: 0.00002181
Iteration 133/1000 | Loss: 0.00002181
Iteration 134/1000 | Loss: 0.00002181
Iteration 135/1000 | Loss: 0.00002181
Iteration 136/1000 | Loss: 0.00002181
Iteration 137/1000 | Loss: 0.00002181
Iteration 138/1000 | Loss: 0.00002181
Iteration 139/1000 | Loss: 0.00002181
Iteration 140/1000 | Loss: 0.00002181
Iteration 141/1000 | Loss: 0.00002181
Iteration 142/1000 | Loss: 0.00002181
Iteration 143/1000 | Loss: 0.00002181
Iteration 144/1000 | Loss: 0.00002181
Iteration 145/1000 | Loss: 0.00002181
Iteration 146/1000 | Loss: 0.00002180
Iteration 147/1000 | Loss: 0.00002180
Iteration 148/1000 | Loss: 0.00002180
Iteration 149/1000 | Loss: 0.00002180
Iteration 150/1000 | Loss: 0.00002180
Iteration 151/1000 | Loss: 0.00002180
Iteration 152/1000 | Loss: 0.00002180
Iteration 153/1000 | Loss: 0.00002179
Iteration 154/1000 | Loss: 0.00002179
Iteration 155/1000 | Loss: 0.00002179
Iteration 156/1000 | Loss: 0.00002179
Iteration 157/1000 | Loss: 0.00002179
Iteration 158/1000 | Loss: 0.00002179
Iteration 159/1000 | Loss: 0.00002179
Iteration 160/1000 | Loss: 0.00002179
Iteration 161/1000 | Loss: 0.00002179
Iteration 162/1000 | Loss: 0.00002179
Iteration 163/1000 | Loss: 0.00002179
Iteration 164/1000 | Loss: 0.00002179
Iteration 165/1000 | Loss: 0.00002179
Iteration 166/1000 | Loss: 0.00002179
Iteration 167/1000 | Loss: 0.00002179
Iteration 168/1000 | Loss: 0.00002179
Iteration 169/1000 | Loss: 0.00002179
Iteration 170/1000 | Loss: 0.00002179
Iteration 171/1000 | Loss: 0.00002178
Iteration 172/1000 | Loss: 0.00002178
Iteration 173/1000 | Loss: 0.00002178
Iteration 174/1000 | Loss: 0.00002178
Iteration 175/1000 | Loss: 0.00002178
Iteration 176/1000 | Loss: 0.00002178
Iteration 177/1000 | Loss: 0.00002178
Iteration 178/1000 | Loss: 0.00002178
Iteration 179/1000 | Loss: 0.00002178
Iteration 180/1000 | Loss: 0.00002178
Iteration 181/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.1783276679343544e-05, 2.1783276679343544e-05, 2.1783276679343544e-05, 2.1783276679343544e-05, 2.1783276679343544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1783276679343544e-05

Optimization complete. Final v2v error: 3.918524742126465 mm

Highest mean error: 4.456234455108643 mm for frame 136

Lowest mean error: 3.231881618499756 mm for frame 27

Saving results

Total time: 44.67358446121216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770135
Iteration 2/25 | Loss: 0.00142086
Iteration 3/25 | Loss: 0.00127371
Iteration 4/25 | Loss: 0.00125917
Iteration 5/25 | Loss: 0.00125703
Iteration 6/25 | Loss: 0.00125703
Iteration 7/25 | Loss: 0.00125703
Iteration 8/25 | Loss: 0.00125703
Iteration 9/25 | Loss: 0.00125703
Iteration 10/25 | Loss: 0.00125703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012570337858051062, 0.0012570337858051062, 0.0012570337858051062, 0.0012570337858051062, 0.0012570337858051062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012570337858051062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42338812
Iteration 2/25 | Loss: 0.00078319
Iteration 3/25 | Loss: 0.00078319
Iteration 4/25 | Loss: 0.00078319
Iteration 5/25 | Loss: 0.00078319
Iteration 6/25 | Loss: 0.00078319
Iteration 7/25 | Loss: 0.00078319
Iteration 8/25 | Loss: 0.00078319
Iteration 9/25 | Loss: 0.00078319
Iteration 10/25 | Loss: 0.00078319
Iteration 11/25 | Loss: 0.00078319
Iteration 12/25 | Loss: 0.00078319
Iteration 13/25 | Loss: 0.00078319
Iteration 14/25 | Loss: 0.00078319
Iteration 15/25 | Loss: 0.00078319
Iteration 16/25 | Loss: 0.00078319
Iteration 17/25 | Loss: 0.00078319
Iteration 18/25 | Loss: 0.00078319
Iteration 19/25 | Loss: 0.00078319
Iteration 20/25 | Loss: 0.00078319
Iteration 21/25 | Loss: 0.00078319
Iteration 22/25 | Loss: 0.00078319
Iteration 23/25 | Loss: 0.00078319
Iteration 24/25 | Loss: 0.00078319
Iteration 25/25 | Loss: 0.00078319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078319
Iteration 2/1000 | Loss: 0.00002650
Iteration 3/1000 | Loss: 0.00001781
Iteration 4/1000 | Loss: 0.00001642
Iteration 5/1000 | Loss: 0.00001532
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001346
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001294
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001274
Iteration 22/1000 | Loss: 0.00001269
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001266
Iteration 26/1000 | Loss: 0.00001266
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001266
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001266
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001265
Iteration 38/1000 | Loss: 0.00001265
Iteration 39/1000 | Loss: 0.00001264
Iteration 40/1000 | Loss: 0.00001264
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001260
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001259
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001251
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001245
Iteration 60/1000 | Loss: 0.00001245
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00001243
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001239
Iteration 72/1000 | Loss: 0.00001239
Iteration 73/1000 | Loss: 0.00001238
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001227
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001225
Iteration 116/1000 | Loss: 0.00001225
Iteration 117/1000 | Loss: 0.00001225
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001224
Iteration 120/1000 | Loss: 0.00001224
Iteration 121/1000 | Loss: 0.00001223
Iteration 122/1000 | Loss: 0.00001223
Iteration 123/1000 | Loss: 0.00001223
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001223
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001222
Iteration 131/1000 | Loss: 0.00001222
Iteration 132/1000 | Loss: 0.00001222
Iteration 133/1000 | Loss: 0.00001222
Iteration 134/1000 | Loss: 0.00001222
Iteration 135/1000 | Loss: 0.00001222
Iteration 136/1000 | Loss: 0.00001222
Iteration 137/1000 | Loss: 0.00001222
Iteration 138/1000 | Loss: 0.00001222
Iteration 139/1000 | Loss: 0.00001222
Iteration 140/1000 | Loss: 0.00001222
Iteration 141/1000 | Loss: 0.00001222
Iteration 142/1000 | Loss: 0.00001222
Iteration 143/1000 | Loss: 0.00001222
Iteration 144/1000 | Loss: 0.00001222
Iteration 145/1000 | Loss: 0.00001222
Iteration 146/1000 | Loss: 0.00001222
Iteration 147/1000 | Loss: 0.00001222
Iteration 148/1000 | Loss: 0.00001222
Iteration 149/1000 | Loss: 0.00001222
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001222
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.2224502825119998e-05, 1.2224502825119998e-05, 1.2224502825119998e-05, 1.2224502825119998e-05, 1.2224502825119998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2224502825119998e-05

Optimization complete. Final v2v error: 2.9892375469207764 mm

Highest mean error: 3.374514102935791 mm for frame 124

Lowest mean error: 2.8130784034729004 mm for frame 174

Saving results

Total time: 40.5382285118103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993686
Iteration 2/25 | Loss: 0.00335009
Iteration 3/25 | Loss: 0.00219260
Iteration 4/25 | Loss: 0.00200668
Iteration 5/25 | Loss: 0.00192135
Iteration 6/25 | Loss: 0.00183447
Iteration 7/25 | Loss: 0.00146311
Iteration 8/25 | Loss: 0.00137232
Iteration 9/25 | Loss: 0.00132477
Iteration 10/25 | Loss: 0.00129752
Iteration 11/25 | Loss: 0.00129225
Iteration 12/25 | Loss: 0.00129934
Iteration 13/25 | Loss: 0.00128286
Iteration 14/25 | Loss: 0.00128067
Iteration 15/25 | Loss: 0.00128634
Iteration 16/25 | Loss: 0.00127966
Iteration 17/25 | Loss: 0.00127423
Iteration 18/25 | Loss: 0.00127597
Iteration 19/25 | Loss: 0.00129090
Iteration 20/25 | Loss: 0.00128816
Iteration 21/25 | Loss: 0.00128258
Iteration 22/25 | Loss: 0.00128469
Iteration 23/25 | Loss: 0.00127922
Iteration 24/25 | Loss: 0.00127408
Iteration 25/25 | Loss: 0.00127488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39788973
Iteration 2/25 | Loss: 0.00100794
Iteration 3/25 | Loss: 0.00093208
Iteration 4/25 | Loss: 0.00093208
Iteration 5/25 | Loss: 0.00093208
Iteration 6/25 | Loss: 0.00093208
Iteration 7/25 | Loss: 0.00093208
Iteration 8/25 | Loss: 0.00093208
Iteration 9/25 | Loss: 0.00093208
Iteration 10/25 | Loss: 0.00093208
Iteration 11/25 | Loss: 0.00093208
Iteration 12/25 | Loss: 0.00093208
Iteration 13/25 | Loss: 0.00093208
Iteration 14/25 | Loss: 0.00093208
Iteration 15/25 | Loss: 0.00093208
Iteration 16/25 | Loss: 0.00093208
Iteration 17/25 | Loss: 0.00093208
Iteration 18/25 | Loss: 0.00093208
Iteration 19/25 | Loss: 0.00093208
Iteration 20/25 | Loss: 0.00093208
Iteration 21/25 | Loss: 0.00093208
Iteration 22/25 | Loss: 0.00093208
Iteration 23/25 | Loss: 0.00093208
Iteration 24/25 | Loss: 0.00093208
Iteration 25/25 | Loss: 0.00093208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093208
Iteration 2/1000 | Loss: 0.00007683
Iteration 3/1000 | Loss: 0.00011952
Iteration 4/1000 | Loss: 0.00006620
Iteration 5/1000 | Loss: 0.00005614
Iteration 6/1000 | Loss: 0.00007147
Iteration 7/1000 | Loss: 0.00006491
Iteration 8/1000 | Loss: 0.00005284
Iteration 9/1000 | Loss: 0.00019526
Iteration 10/1000 | Loss: 0.00010423
Iteration 11/1000 | Loss: 0.00008589
Iteration 12/1000 | Loss: 0.00009809
Iteration 13/1000 | Loss: 0.00011420
Iteration 14/1000 | Loss: 0.00015026
Iteration 15/1000 | Loss: 0.00010072
Iteration 16/1000 | Loss: 0.00011624
Iteration 17/1000 | Loss: 0.00009411
Iteration 18/1000 | Loss: 0.00011148
Iteration 19/1000 | Loss: 0.00012585
Iteration 20/1000 | Loss: 0.00009806
Iteration 21/1000 | Loss: 0.00009134
Iteration 22/1000 | Loss: 0.00009209
Iteration 23/1000 | Loss: 0.00009136
Iteration 24/1000 | Loss: 0.00008997
Iteration 25/1000 | Loss: 0.00009098
Iteration 26/1000 | Loss: 0.00009625
Iteration 27/1000 | Loss: 0.00016935
Iteration 28/1000 | Loss: 0.00015812
Iteration 29/1000 | Loss: 0.00008963
Iteration 30/1000 | Loss: 0.00008796
Iteration 31/1000 | Loss: 0.00008884
Iteration 32/1000 | Loss: 0.00011185
Iteration 33/1000 | Loss: 0.00004852
Iteration 34/1000 | Loss: 0.00032340
Iteration 35/1000 | Loss: 0.00008353
Iteration 36/1000 | Loss: 0.00007484
Iteration 37/1000 | Loss: 0.00008818
Iteration 38/1000 | Loss: 0.00005957
Iteration 39/1000 | Loss: 0.00006821
Iteration 40/1000 | Loss: 0.00010342
Iteration 41/1000 | Loss: 0.00006559
Iteration 42/1000 | Loss: 0.00007912
Iteration 43/1000 | Loss: 0.00007868
Iteration 44/1000 | Loss: 0.00010365
Iteration 45/1000 | Loss: 0.00007882
Iteration 46/1000 | Loss: 0.00009361
Iteration 47/1000 | Loss: 0.00007474
Iteration 48/1000 | Loss: 0.00008191
Iteration 49/1000 | Loss: 0.00005160
Iteration 50/1000 | Loss: 0.00008896
Iteration 51/1000 | Loss: 0.00010950
Iteration 52/1000 | Loss: 0.00008321
Iteration 53/1000 | Loss: 0.00010070
Iteration 54/1000 | Loss: 0.00009025
Iteration 55/1000 | Loss: 0.00012059
Iteration 56/1000 | Loss: 0.00010401
Iteration 57/1000 | Loss: 0.00007257
Iteration 58/1000 | Loss: 0.00008340
Iteration 59/1000 | Loss: 0.00011020
Iteration 60/1000 | Loss: 0.00005815
Iteration 61/1000 | Loss: 0.00012526
Iteration 62/1000 | Loss: 0.00005670
Iteration 63/1000 | Loss: 0.00003781
Iteration 64/1000 | Loss: 0.00005858
Iteration 65/1000 | Loss: 0.00007622
Iteration 66/1000 | Loss: 0.00007553
Iteration 67/1000 | Loss: 0.00036736
Iteration 68/1000 | Loss: 0.00013168
Iteration 69/1000 | Loss: 0.00003767
Iteration 70/1000 | Loss: 0.00005341
Iteration 71/1000 | Loss: 0.00002517
Iteration 72/1000 | Loss: 0.00003478
Iteration 73/1000 | Loss: 0.00004195
Iteration 74/1000 | Loss: 0.00004970
Iteration 75/1000 | Loss: 0.00002861
Iteration 76/1000 | Loss: 0.00012425
Iteration 77/1000 | Loss: 0.00004016
Iteration 78/1000 | Loss: 0.00002165
Iteration 79/1000 | Loss: 0.00002031
Iteration 80/1000 | Loss: 0.00002726
Iteration 81/1000 | Loss: 0.00003097
Iteration 82/1000 | Loss: 0.00004256
Iteration 83/1000 | Loss: 0.00004727
Iteration 84/1000 | Loss: 0.00004405
Iteration 85/1000 | Loss: 0.00004280
Iteration 86/1000 | Loss: 0.00009186
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00005458
Iteration 89/1000 | Loss: 0.00004469
Iteration 90/1000 | Loss: 0.00004434
Iteration 91/1000 | Loss: 0.00004450
Iteration 92/1000 | Loss: 0.00005436
Iteration 93/1000 | Loss: 0.00006507
Iteration 94/1000 | Loss: 0.00005680
Iteration 95/1000 | Loss: 0.00004290
Iteration 96/1000 | Loss: 0.00005143
Iteration 97/1000 | Loss: 0.00004240
Iteration 98/1000 | Loss: 0.00005077
Iteration 99/1000 | Loss: 0.00002952
Iteration 100/1000 | Loss: 0.00003741
Iteration 101/1000 | Loss: 0.00004166
Iteration 102/1000 | Loss: 0.00004241
Iteration 103/1000 | Loss: 0.00004245
Iteration 104/1000 | Loss: 0.00006096
Iteration 105/1000 | Loss: 0.00004555
Iteration 106/1000 | Loss: 0.00005927
Iteration 107/1000 | Loss: 0.00004537
Iteration 108/1000 | Loss: 0.00006071
Iteration 109/1000 | Loss: 0.00004614
Iteration 110/1000 | Loss: 0.00004239
Iteration 111/1000 | Loss: 0.00021979
Iteration 112/1000 | Loss: 0.00008552
Iteration 113/1000 | Loss: 0.00010797
Iteration 114/1000 | Loss: 0.00002840
Iteration 115/1000 | Loss: 0.00003578
Iteration 116/1000 | Loss: 0.00005890
Iteration 117/1000 | Loss: 0.00003339
Iteration 118/1000 | Loss: 0.00004648
Iteration 119/1000 | Loss: 0.00003093
Iteration 120/1000 | Loss: 0.00003467
Iteration 121/1000 | Loss: 0.00003031
Iteration 122/1000 | Loss: 0.00003296
Iteration 123/1000 | Loss: 0.00005451
Iteration 124/1000 | Loss: 0.00003963
Iteration 125/1000 | Loss: 0.00004176
Iteration 126/1000 | Loss: 0.00007482
Iteration 127/1000 | Loss: 0.00004034
Iteration 128/1000 | Loss: 0.00003502
Iteration 129/1000 | Loss: 0.00003312
Iteration 130/1000 | Loss: 0.00002976
Iteration 131/1000 | Loss: 0.00004909
Iteration 132/1000 | Loss: 0.00002979
Iteration 133/1000 | Loss: 0.00003234
Iteration 134/1000 | Loss: 0.00002917
Iteration 135/1000 | Loss: 0.00006902
Iteration 136/1000 | Loss: 0.00044785
Iteration 137/1000 | Loss: 0.00057183
Iteration 138/1000 | Loss: 0.00003284
Iteration 139/1000 | Loss: 0.00004020
Iteration 140/1000 | Loss: 0.00002879
Iteration 141/1000 | Loss: 0.00007083
Iteration 142/1000 | Loss: 0.00003634
Iteration 143/1000 | Loss: 0.00005871
Iteration 144/1000 | Loss: 0.00002927
Iteration 145/1000 | Loss: 0.00004598
Iteration 146/1000 | Loss: 0.00003657
Iteration 147/1000 | Loss: 0.00004035
Iteration 148/1000 | Loss: 0.00005893
Iteration 149/1000 | Loss: 0.00003691
Iteration 150/1000 | Loss: 0.00002695
Iteration 151/1000 | Loss: 0.00003827
Iteration 152/1000 | Loss: 0.00003228
Iteration 153/1000 | Loss: 0.00003727
Iteration 154/1000 | Loss: 0.00002811
Iteration 155/1000 | Loss: 0.00003412
Iteration 156/1000 | Loss: 0.00003873
Iteration 157/1000 | Loss: 0.00002523
Iteration 158/1000 | Loss: 0.00003445
Iteration 159/1000 | Loss: 0.00003445
Iteration 160/1000 | Loss: 0.00007317
Iteration 161/1000 | Loss: 0.00003277
Iteration 162/1000 | Loss: 0.00006088
Iteration 163/1000 | Loss: 0.00003909
Iteration 164/1000 | Loss: 0.00003461
Iteration 165/1000 | Loss: 0.00006958
Iteration 166/1000 | Loss: 0.00002700
Iteration 167/1000 | Loss: 0.00003593
Iteration 168/1000 | Loss: 0.00002621
Iteration 169/1000 | Loss: 0.00003461
Iteration 170/1000 | Loss: 0.00003836
Iteration 171/1000 | Loss: 0.00003487
Iteration 172/1000 | Loss: 0.00003647
Iteration 173/1000 | Loss: 0.00007865
Iteration 174/1000 | Loss: 0.00004949
Iteration 175/1000 | Loss: 0.00005167
Iteration 176/1000 | Loss: 0.00002778
Iteration 177/1000 | Loss: 0.00002187
Iteration 178/1000 | Loss: 0.00003122
Iteration 179/1000 | Loss: 0.00002157
Iteration 180/1000 | Loss: 0.00003145
Iteration 181/1000 | Loss: 0.00003143
Iteration 182/1000 | Loss: 0.00003115
Iteration 183/1000 | Loss: 0.00003175
Iteration 184/1000 | Loss: 0.00003072
Iteration 185/1000 | Loss: 0.00003112
Iteration 186/1000 | Loss: 0.00003044
Iteration 187/1000 | Loss: 0.00002680
Iteration 188/1000 | Loss: 0.00028670
Iteration 189/1000 | Loss: 0.00004527
Iteration 190/1000 | Loss: 0.00006339
Iteration 191/1000 | Loss: 0.00002813
Iteration 192/1000 | Loss: 0.00001687
Iteration 193/1000 | Loss: 0.00001649
Iteration 194/1000 | Loss: 0.00001635
Iteration 195/1000 | Loss: 0.00001629
Iteration 196/1000 | Loss: 0.00001613
Iteration 197/1000 | Loss: 0.00001597
Iteration 198/1000 | Loss: 0.00001596
Iteration 199/1000 | Loss: 0.00001595
Iteration 200/1000 | Loss: 0.00001592
Iteration 201/1000 | Loss: 0.00001592
Iteration 202/1000 | Loss: 0.00001591
Iteration 203/1000 | Loss: 0.00001588
Iteration 204/1000 | Loss: 0.00001588
Iteration 205/1000 | Loss: 0.00001585
Iteration 206/1000 | Loss: 0.00001585
Iteration 207/1000 | Loss: 0.00001585
Iteration 208/1000 | Loss: 0.00001585
Iteration 209/1000 | Loss: 0.00001585
Iteration 210/1000 | Loss: 0.00001584
Iteration 211/1000 | Loss: 0.00001584
Iteration 212/1000 | Loss: 0.00001584
Iteration 213/1000 | Loss: 0.00001584
Iteration 214/1000 | Loss: 0.00001584
Iteration 215/1000 | Loss: 0.00001584
Iteration 216/1000 | Loss: 0.00001584
Iteration 217/1000 | Loss: 0.00001583
Iteration 218/1000 | Loss: 0.00001583
Iteration 219/1000 | Loss: 0.00001583
Iteration 220/1000 | Loss: 0.00001583
Iteration 221/1000 | Loss: 0.00001583
Iteration 222/1000 | Loss: 0.00001583
Iteration 223/1000 | Loss: 0.00001583
Iteration 224/1000 | Loss: 0.00001583
Iteration 225/1000 | Loss: 0.00001583
Iteration 226/1000 | Loss: 0.00001583
Iteration 227/1000 | Loss: 0.00001582
Iteration 228/1000 | Loss: 0.00001582
Iteration 229/1000 | Loss: 0.00001582
Iteration 230/1000 | Loss: 0.00001582
Iteration 231/1000 | Loss: 0.00001582
Iteration 232/1000 | Loss: 0.00001581
Iteration 233/1000 | Loss: 0.00001581
Iteration 234/1000 | Loss: 0.00001581
Iteration 235/1000 | Loss: 0.00001581
Iteration 236/1000 | Loss: 0.00001580
Iteration 237/1000 | Loss: 0.00001580
Iteration 238/1000 | Loss: 0.00001580
Iteration 239/1000 | Loss: 0.00001579
Iteration 240/1000 | Loss: 0.00001579
Iteration 241/1000 | Loss: 0.00001579
Iteration 242/1000 | Loss: 0.00001578
Iteration 243/1000 | Loss: 0.00001578
Iteration 244/1000 | Loss: 0.00001577
Iteration 245/1000 | Loss: 0.00001577
Iteration 246/1000 | Loss: 0.00001577
Iteration 247/1000 | Loss: 0.00001576
Iteration 248/1000 | Loss: 0.00001576
Iteration 249/1000 | Loss: 0.00001576
Iteration 250/1000 | Loss: 0.00001575
Iteration 251/1000 | Loss: 0.00001575
Iteration 252/1000 | Loss: 0.00001575
Iteration 253/1000 | Loss: 0.00001574
Iteration 254/1000 | Loss: 0.00001573
Iteration 255/1000 | Loss: 0.00001573
Iteration 256/1000 | Loss: 0.00001572
Iteration 257/1000 | Loss: 0.00001572
Iteration 258/1000 | Loss: 0.00001572
Iteration 259/1000 | Loss: 0.00001572
Iteration 260/1000 | Loss: 0.00001571
Iteration 261/1000 | Loss: 0.00001570
Iteration 262/1000 | Loss: 0.00001569
Iteration 263/1000 | Loss: 0.00001569
Iteration 264/1000 | Loss: 0.00001569
Iteration 265/1000 | Loss: 0.00001569
Iteration 266/1000 | Loss: 0.00001569
Iteration 267/1000 | Loss: 0.00001569
Iteration 268/1000 | Loss: 0.00001568
Iteration 269/1000 | Loss: 0.00001568
Iteration 270/1000 | Loss: 0.00001568
Iteration 271/1000 | Loss: 0.00001568
Iteration 272/1000 | Loss: 0.00001568
Iteration 273/1000 | Loss: 0.00001568
Iteration 274/1000 | Loss: 0.00001568
Iteration 275/1000 | Loss: 0.00001568
Iteration 276/1000 | Loss: 0.00001568
Iteration 277/1000 | Loss: 0.00001567
Iteration 278/1000 | Loss: 0.00001566
Iteration 279/1000 | Loss: 0.00001566
Iteration 280/1000 | Loss: 0.00001566
Iteration 281/1000 | Loss: 0.00001566
Iteration 282/1000 | Loss: 0.00001566
Iteration 283/1000 | Loss: 0.00001566
Iteration 284/1000 | Loss: 0.00001566
Iteration 285/1000 | Loss: 0.00001566
Iteration 286/1000 | Loss: 0.00001565
Iteration 287/1000 | Loss: 0.00001565
Iteration 288/1000 | Loss: 0.00001565
Iteration 289/1000 | Loss: 0.00001565
Iteration 290/1000 | Loss: 0.00001565
Iteration 291/1000 | Loss: 0.00001565
Iteration 292/1000 | Loss: 0.00001564
Iteration 293/1000 | Loss: 0.00001564
Iteration 294/1000 | Loss: 0.00001564
Iteration 295/1000 | Loss: 0.00001564
Iteration 296/1000 | Loss: 0.00001564
Iteration 297/1000 | Loss: 0.00001564
Iteration 298/1000 | Loss: 0.00001564
Iteration 299/1000 | Loss: 0.00001563
Iteration 300/1000 | Loss: 0.00001563
Iteration 301/1000 | Loss: 0.00001563
Iteration 302/1000 | Loss: 0.00001563
Iteration 303/1000 | Loss: 0.00001563
Iteration 304/1000 | Loss: 0.00001563
Iteration 305/1000 | Loss: 0.00001563
Iteration 306/1000 | Loss: 0.00001562
Iteration 307/1000 | Loss: 0.00001562
Iteration 308/1000 | Loss: 0.00001562
Iteration 309/1000 | Loss: 0.00001561
Iteration 310/1000 | Loss: 0.00001561
Iteration 311/1000 | Loss: 0.00001560
Iteration 312/1000 | Loss: 0.00001560
Iteration 313/1000 | Loss: 0.00001560
Iteration 314/1000 | Loss: 0.00001559
Iteration 315/1000 | Loss: 0.00001559
Iteration 316/1000 | Loss: 0.00001559
Iteration 317/1000 | Loss: 0.00001559
Iteration 318/1000 | Loss: 0.00001558
Iteration 319/1000 | Loss: 0.00001558
Iteration 320/1000 | Loss: 0.00001558
Iteration 321/1000 | Loss: 0.00001558
Iteration 322/1000 | Loss: 0.00001558
Iteration 323/1000 | Loss: 0.00001557
Iteration 324/1000 | Loss: 0.00001557
Iteration 325/1000 | Loss: 0.00001557
Iteration 326/1000 | Loss: 0.00001557
Iteration 327/1000 | Loss: 0.00001557
Iteration 328/1000 | Loss: 0.00001557
Iteration 329/1000 | Loss: 0.00001557
Iteration 330/1000 | Loss: 0.00001557
Iteration 331/1000 | Loss: 0.00001557
Iteration 332/1000 | Loss: 0.00001557
Iteration 333/1000 | Loss: 0.00001557
Iteration 334/1000 | Loss: 0.00001556
Iteration 335/1000 | Loss: 0.00001556
Iteration 336/1000 | Loss: 0.00001556
Iteration 337/1000 | Loss: 0.00001556
Iteration 338/1000 | Loss: 0.00001556
Iteration 339/1000 | Loss: 0.00001556
Iteration 340/1000 | Loss: 0.00001556
Iteration 341/1000 | Loss: 0.00001556
Iteration 342/1000 | Loss: 0.00001555
Iteration 343/1000 | Loss: 0.00001555
Iteration 344/1000 | Loss: 0.00001555
Iteration 345/1000 | Loss: 0.00001555
Iteration 346/1000 | Loss: 0.00001555
Iteration 347/1000 | Loss: 0.00001555
Iteration 348/1000 | Loss: 0.00001555
Iteration 349/1000 | Loss: 0.00001555
Iteration 350/1000 | Loss: 0.00001555
Iteration 351/1000 | Loss: 0.00001555
Iteration 352/1000 | Loss: 0.00001555
Iteration 353/1000 | Loss: 0.00001555
Iteration 354/1000 | Loss: 0.00001555
Iteration 355/1000 | Loss: 0.00001554
Iteration 356/1000 | Loss: 0.00001554
Iteration 357/1000 | Loss: 0.00001554
Iteration 358/1000 | Loss: 0.00001554
Iteration 359/1000 | Loss: 0.00001554
Iteration 360/1000 | Loss: 0.00001554
Iteration 361/1000 | Loss: 0.00001554
Iteration 362/1000 | Loss: 0.00001554
Iteration 363/1000 | Loss: 0.00001554
Iteration 364/1000 | Loss: 0.00001553
Iteration 365/1000 | Loss: 0.00001553
Iteration 366/1000 | Loss: 0.00001553
Iteration 367/1000 | Loss: 0.00001553
Iteration 368/1000 | Loss: 0.00001553
Iteration 369/1000 | Loss: 0.00001553
Iteration 370/1000 | Loss: 0.00001553
Iteration 371/1000 | Loss: 0.00001553
Iteration 372/1000 | Loss: 0.00001553
Iteration 373/1000 | Loss: 0.00001553
Iteration 374/1000 | Loss: 0.00001553
Iteration 375/1000 | Loss: 0.00001553
Iteration 376/1000 | Loss: 0.00001553
Iteration 377/1000 | Loss: 0.00001553
Iteration 378/1000 | Loss: 0.00001552
Iteration 379/1000 | Loss: 0.00001552
Iteration 380/1000 | Loss: 0.00001552
Iteration 381/1000 | Loss: 0.00001552
Iteration 382/1000 | Loss: 0.00001552
Iteration 383/1000 | Loss: 0.00001552
Iteration 384/1000 | Loss: 0.00001552
Iteration 385/1000 | Loss: 0.00001552
Iteration 386/1000 | Loss: 0.00001552
Iteration 387/1000 | Loss: 0.00001552
Iteration 388/1000 | Loss: 0.00001552
Iteration 389/1000 | Loss: 0.00001552
Iteration 390/1000 | Loss: 0.00001552
Iteration 391/1000 | Loss: 0.00001551
Iteration 392/1000 | Loss: 0.00001551
Iteration 393/1000 | Loss: 0.00001551
Iteration 394/1000 | Loss: 0.00001551
Iteration 395/1000 | Loss: 0.00001551
Iteration 396/1000 | Loss: 0.00001551
Iteration 397/1000 | Loss: 0.00001551
Iteration 398/1000 | Loss: 0.00001551
Iteration 399/1000 | Loss: 0.00001551
Iteration 400/1000 | Loss: 0.00001551
Iteration 401/1000 | Loss: 0.00001551
Iteration 402/1000 | Loss: 0.00001551
Iteration 403/1000 | Loss: 0.00001551
Iteration 404/1000 | Loss: 0.00001551
Iteration 405/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 405. Stopping optimization.
Last 5 losses: [1.551397144794464e-05, 1.551397144794464e-05, 1.551397144794464e-05, 1.551397144794464e-05, 1.551397144794464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.551397144794464e-05

Optimization complete. Final v2v error: 3.2401299476623535 mm

Highest mean error: 6.6167402267456055 mm for frame 105

Lowest mean error: 2.8570127487182617 mm for frame 84

Saving results

Total time: 320.1701395511627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955560
Iteration 2/25 | Loss: 0.00955559
Iteration 3/25 | Loss: 0.00339709
Iteration 4/25 | Loss: 0.00223737
Iteration 5/25 | Loss: 0.00203123
Iteration 6/25 | Loss: 0.00183754
Iteration 7/25 | Loss: 0.00193236
Iteration 8/25 | Loss: 0.00191568
Iteration 9/25 | Loss: 0.00181784
Iteration 10/25 | Loss: 0.00169596
Iteration 11/25 | Loss: 0.00161589
Iteration 12/25 | Loss: 0.00158844
Iteration 13/25 | Loss: 0.00157940
Iteration 14/25 | Loss: 0.00157368
Iteration 15/25 | Loss: 0.00155847
Iteration 16/25 | Loss: 0.00155803
Iteration 17/25 | Loss: 0.00155634
Iteration 18/25 | Loss: 0.00154775
Iteration 19/25 | Loss: 0.00154524
Iteration 20/25 | Loss: 0.00154552
Iteration 21/25 | Loss: 0.00154778
Iteration 22/25 | Loss: 0.00154312
Iteration 23/25 | Loss: 0.00154166
Iteration 24/25 | Loss: 0.00154159
Iteration 25/25 | Loss: 0.00154148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42302811
Iteration 2/25 | Loss: 0.00272298
Iteration 3/25 | Loss: 0.00272297
Iteration 4/25 | Loss: 0.00261510
Iteration 5/25 | Loss: 0.00261507
Iteration 6/25 | Loss: 0.00261506
Iteration 7/25 | Loss: 0.00261506
Iteration 8/25 | Loss: 0.00261506
Iteration 9/25 | Loss: 0.00261506
Iteration 10/25 | Loss: 0.00261506
Iteration 11/25 | Loss: 0.00261506
Iteration 12/25 | Loss: 0.00261506
Iteration 13/25 | Loss: 0.00261506
Iteration 14/25 | Loss: 0.00261506
Iteration 15/25 | Loss: 0.00261506
Iteration 16/25 | Loss: 0.00261506
Iteration 17/25 | Loss: 0.00261506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026150608900934458, 0.0026150608900934458, 0.0026150608900934458, 0.0026150608900934458, 0.0026150608900934458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026150608900934458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261506
Iteration 2/1000 | Loss: 0.00079039
Iteration 3/1000 | Loss: 0.00317301
Iteration 4/1000 | Loss: 0.00040316
Iteration 5/1000 | Loss: 0.00067281
Iteration 6/1000 | Loss: 0.00049668
Iteration 7/1000 | Loss: 0.00059978
Iteration 8/1000 | Loss: 0.00058949
Iteration 9/1000 | Loss: 0.00022123
Iteration 10/1000 | Loss: 0.00053033
Iteration 11/1000 | Loss: 0.00076660
Iteration 12/1000 | Loss: 0.00055451
Iteration 13/1000 | Loss: 0.00044848
Iteration 14/1000 | Loss: 0.00054641
Iteration 15/1000 | Loss: 0.00033418
Iteration 16/1000 | Loss: 0.00020823
Iteration 17/1000 | Loss: 0.00019787
Iteration 18/1000 | Loss: 0.00025907
Iteration 19/1000 | Loss: 0.00016672
Iteration 20/1000 | Loss: 0.00041207
Iteration 21/1000 | Loss: 0.00031031
Iteration 22/1000 | Loss: 0.00024382
Iteration 23/1000 | Loss: 0.00044166
Iteration 24/1000 | Loss: 0.00276401
Iteration 25/1000 | Loss: 0.00026569
Iteration 26/1000 | Loss: 0.00020017
Iteration 27/1000 | Loss: 0.00016955
Iteration 28/1000 | Loss: 0.00013720
Iteration 29/1000 | Loss: 0.00036020
Iteration 30/1000 | Loss: 0.00011637
Iteration 31/1000 | Loss: 0.00031259
Iteration 32/1000 | Loss: 0.00059759
Iteration 33/1000 | Loss: 0.00048103
Iteration 34/1000 | Loss: 0.00035565
Iteration 35/1000 | Loss: 0.00019341
Iteration 36/1000 | Loss: 0.00041994
Iteration 37/1000 | Loss: 0.00022165
Iteration 38/1000 | Loss: 0.00024463
Iteration 39/1000 | Loss: 0.00039876
Iteration 40/1000 | Loss: 0.00021743
Iteration 41/1000 | Loss: 0.00110997
Iteration 42/1000 | Loss: 0.00265715
Iteration 43/1000 | Loss: 0.00182787
Iteration 44/1000 | Loss: 0.00148582
Iteration 45/1000 | Loss: 0.00160433
Iteration 46/1000 | Loss: 0.00267862
Iteration 47/1000 | Loss: 0.00037821
Iteration 48/1000 | Loss: 0.00017012
Iteration 49/1000 | Loss: 0.00017328
Iteration 50/1000 | Loss: 0.00037911
Iteration 51/1000 | Loss: 0.00118172
Iteration 52/1000 | Loss: 0.00132975
Iteration 53/1000 | Loss: 0.00027964
Iteration 54/1000 | Loss: 0.00006454
Iteration 55/1000 | Loss: 0.00007471
Iteration 56/1000 | Loss: 0.00008893
Iteration 57/1000 | Loss: 0.00043805
Iteration 58/1000 | Loss: 0.00007646
Iteration 59/1000 | Loss: 0.00004780
Iteration 60/1000 | Loss: 0.00003823
Iteration 61/1000 | Loss: 0.00003266
Iteration 62/1000 | Loss: 0.00002894
Iteration 63/1000 | Loss: 0.00003430
Iteration 64/1000 | Loss: 0.00002561
Iteration 65/1000 | Loss: 0.00058173
Iteration 66/1000 | Loss: 0.00003028
Iteration 67/1000 | Loss: 0.00002465
Iteration 68/1000 | Loss: 0.00003544
Iteration 69/1000 | Loss: 0.00002253
Iteration 70/1000 | Loss: 0.00002124
Iteration 71/1000 | Loss: 0.00002048
Iteration 72/1000 | Loss: 0.00002785
Iteration 73/1000 | Loss: 0.00002101
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001880
Iteration 79/1000 | Loss: 0.00001875
Iteration 80/1000 | Loss: 0.00001873
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001870
Iteration 83/1000 | Loss: 0.00001870
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001859
Iteration 87/1000 | Loss: 0.00001854
Iteration 88/1000 | Loss: 0.00001853
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001849
Iteration 104/1000 | Loss: 0.00001849
Iteration 105/1000 | Loss: 0.00001849
Iteration 106/1000 | Loss: 0.00001848
Iteration 107/1000 | Loss: 0.00001848
Iteration 108/1000 | Loss: 0.00001848
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Iteration 113/1000 | Loss: 0.00001847
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001845
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001844
Iteration 120/1000 | Loss: 0.00001843
Iteration 121/1000 | Loss: 0.00001843
Iteration 122/1000 | Loss: 0.00001843
Iteration 123/1000 | Loss: 0.00001842
Iteration 124/1000 | Loss: 0.00001842
Iteration 125/1000 | Loss: 0.00001842
Iteration 126/1000 | Loss: 0.00001842
Iteration 127/1000 | Loss: 0.00001842
Iteration 128/1000 | Loss: 0.00001842
Iteration 129/1000 | Loss: 0.00001842
Iteration 130/1000 | Loss: 0.00001842
Iteration 131/1000 | Loss: 0.00001842
Iteration 132/1000 | Loss: 0.00001842
Iteration 133/1000 | Loss: 0.00001841
Iteration 134/1000 | Loss: 0.00001841
Iteration 135/1000 | Loss: 0.00001841
Iteration 136/1000 | Loss: 0.00001841
Iteration 137/1000 | Loss: 0.00001841
Iteration 138/1000 | Loss: 0.00001841
Iteration 139/1000 | Loss: 0.00001841
Iteration 140/1000 | Loss: 0.00001840
Iteration 141/1000 | Loss: 0.00001840
Iteration 142/1000 | Loss: 0.00001840
Iteration 143/1000 | Loss: 0.00002410
Iteration 144/1000 | Loss: 0.00001840
Iteration 145/1000 | Loss: 0.00001835
Iteration 146/1000 | Loss: 0.00001835
Iteration 147/1000 | Loss: 0.00001835
Iteration 148/1000 | Loss: 0.00001835
Iteration 149/1000 | Loss: 0.00001835
Iteration 150/1000 | Loss: 0.00001835
Iteration 151/1000 | Loss: 0.00001835
Iteration 152/1000 | Loss: 0.00001835
Iteration 153/1000 | Loss: 0.00001835
Iteration 154/1000 | Loss: 0.00001835
Iteration 155/1000 | Loss: 0.00001835
Iteration 156/1000 | Loss: 0.00001835
Iteration 157/1000 | Loss: 0.00001835
Iteration 158/1000 | Loss: 0.00001835
Iteration 159/1000 | Loss: 0.00001834
Iteration 160/1000 | Loss: 0.00001834
Iteration 161/1000 | Loss: 0.00001834
Iteration 162/1000 | Loss: 0.00001834
Iteration 163/1000 | Loss: 0.00001834
Iteration 164/1000 | Loss: 0.00001834
Iteration 165/1000 | Loss: 0.00001834
Iteration 166/1000 | Loss: 0.00001834
Iteration 167/1000 | Loss: 0.00001834
Iteration 168/1000 | Loss: 0.00001834
Iteration 169/1000 | Loss: 0.00001834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.834025897551328e-05, 1.834025897551328e-05, 1.834025897551328e-05, 1.834025897551328e-05, 1.834025897551328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.834025897551328e-05

Optimization complete. Final v2v error: 3.4792399406433105 mm

Highest mean error: 10.753931045532227 mm for frame 230

Lowest mean error: 3.069228410720825 mm for frame 234

Saving results

Total time: 183.74531602859497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433236
Iteration 2/25 | Loss: 0.00136993
Iteration 3/25 | Loss: 0.00129266
Iteration 4/25 | Loss: 0.00127761
Iteration 5/25 | Loss: 0.00127241
Iteration 6/25 | Loss: 0.00127175
Iteration 7/25 | Loss: 0.00127174
Iteration 8/25 | Loss: 0.00127174
Iteration 9/25 | Loss: 0.00127174
Iteration 10/25 | Loss: 0.00127174
Iteration 11/25 | Loss: 0.00127174
Iteration 12/25 | Loss: 0.00127174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012717412319034338, 0.0012717412319034338, 0.0012717412319034338, 0.0012717412319034338, 0.0012717412319034338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012717412319034338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96047223
Iteration 2/25 | Loss: 0.00086423
Iteration 3/25 | Loss: 0.00086423
Iteration 4/25 | Loss: 0.00086423
Iteration 5/25 | Loss: 0.00086423
Iteration 6/25 | Loss: 0.00086423
Iteration 7/25 | Loss: 0.00086423
Iteration 8/25 | Loss: 0.00086423
Iteration 9/25 | Loss: 0.00086423
Iteration 10/25 | Loss: 0.00086423
Iteration 11/25 | Loss: 0.00086423
Iteration 12/25 | Loss: 0.00086423
Iteration 13/25 | Loss: 0.00086423
Iteration 14/25 | Loss: 0.00086423
Iteration 15/25 | Loss: 0.00086423
Iteration 16/25 | Loss: 0.00086423
Iteration 17/25 | Loss: 0.00086423
Iteration 18/25 | Loss: 0.00086423
Iteration 19/25 | Loss: 0.00086423
Iteration 20/25 | Loss: 0.00086423
Iteration 21/25 | Loss: 0.00086423
Iteration 22/25 | Loss: 0.00086423
Iteration 23/25 | Loss: 0.00086423
Iteration 24/25 | Loss: 0.00086423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008642274769954383, 0.0008642274769954383, 0.0008642274769954383, 0.0008642274769954383, 0.0008642274769954383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008642274769954383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086423
Iteration 2/1000 | Loss: 0.00003128
Iteration 3/1000 | Loss: 0.00002332
Iteration 4/1000 | Loss: 0.00002155
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00001990
Iteration 8/1000 | Loss: 0.00001948
Iteration 9/1000 | Loss: 0.00001923
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001874
Iteration 12/1000 | Loss: 0.00001872
Iteration 13/1000 | Loss: 0.00001849
Iteration 14/1000 | Loss: 0.00001837
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001831
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001826
Iteration 19/1000 | Loss: 0.00001825
Iteration 20/1000 | Loss: 0.00001824
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001823
Iteration 23/1000 | Loss: 0.00001821
Iteration 24/1000 | Loss: 0.00001819
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001818
Iteration 27/1000 | Loss: 0.00001818
Iteration 28/1000 | Loss: 0.00001817
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001816
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001807
Iteration 37/1000 | Loss: 0.00001806
Iteration 38/1000 | Loss: 0.00001805
Iteration 39/1000 | Loss: 0.00001804
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001801
Iteration 43/1000 | Loss: 0.00001800
Iteration 44/1000 | Loss: 0.00001800
Iteration 45/1000 | Loss: 0.00001800
Iteration 46/1000 | Loss: 0.00001800
Iteration 47/1000 | Loss: 0.00001800
Iteration 48/1000 | Loss: 0.00001798
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001796
Iteration 54/1000 | Loss: 0.00001796
Iteration 55/1000 | Loss: 0.00001796
Iteration 56/1000 | Loss: 0.00001796
Iteration 57/1000 | Loss: 0.00001796
Iteration 58/1000 | Loss: 0.00001796
Iteration 59/1000 | Loss: 0.00001796
Iteration 60/1000 | Loss: 0.00001795
Iteration 61/1000 | Loss: 0.00001795
Iteration 62/1000 | Loss: 0.00001794
Iteration 63/1000 | Loss: 0.00001794
Iteration 64/1000 | Loss: 0.00001793
Iteration 65/1000 | Loss: 0.00001793
Iteration 66/1000 | Loss: 0.00001792
Iteration 67/1000 | Loss: 0.00001792
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001792
Iteration 70/1000 | Loss: 0.00001792
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001790
Iteration 77/1000 | Loss: 0.00001789
Iteration 78/1000 | Loss: 0.00001789
Iteration 79/1000 | Loss: 0.00001789
Iteration 80/1000 | Loss: 0.00001789
Iteration 81/1000 | Loss: 0.00001789
Iteration 82/1000 | Loss: 0.00001789
Iteration 83/1000 | Loss: 0.00001789
Iteration 84/1000 | Loss: 0.00001789
Iteration 85/1000 | Loss: 0.00001789
Iteration 86/1000 | Loss: 0.00001788
Iteration 87/1000 | Loss: 0.00001788
Iteration 88/1000 | Loss: 0.00001788
Iteration 89/1000 | Loss: 0.00001788
Iteration 90/1000 | Loss: 0.00001787
Iteration 91/1000 | Loss: 0.00001787
Iteration 92/1000 | Loss: 0.00001786
Iteration 93/1000 | Loss: 0.00001786
Iteration 94/1000 | Loss: 0.00001786
Iteration 95/1000 | Loss: 0.00001786
Iteration 96/1000 | Loss: 0.00001786
Iteration 97/1000 | Loss: 0.00001786
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001783
Iteration 111/1000 | Loss: 0.00001783
Iteration 112/1000 | Loss: 0.00001783
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001782
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001781
Iteration 131/1000 | Loss: 0.00001781
Iteration 132/1000 | Loss: 0.00001781
Iteration 133/1000 | Loss: 0.00001781
Iteration 134/1000 | Loss: 0.00001781
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001780
Iteration 137/1000 | Loss: 0.00001780
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001780
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001780
Iteration 146/1000 | Loss: 0.00001780
Iteration 147/1000 | Loss: 0.00001780
Iteration 148/1000 | Loss: 0.00001780
Iteration 149/1000 | Loss: 0.00001780
Iteration 150/1000 | Loss: 0.00001780
Iteration 151/1000 | Loss: 0.00001780
Iteration 152/1000 | Loss: 0.00001780
Iteration 153/1000 | Loss: 0.00001780
Iteration 154/1000 | Loss: 0.00001780
Iteration 155/1000 | Loss: 0.00001780
Iteration 156/1000 | Loss: 0.00001779
Iteration 157/1000 | Loss: 0.00001779
Iteration 158/1000 | Loss: 0.00001779
Iteration 159/1000 | Loss: 0.00001779
Iteration 160/1000 | Loss: 0.00001779
Iteration 161/1000 | Loss: 0.00001779
Iteration 162/1000 | Loss: 0.00001779
Iteration 163/1000 | Loss: 0.00001779
Iteration 164/1000 | Loss: 0.00001779
Iteration 165/1000 | Loss: 0.00001779
Iteration 166/1000 | Loss: 0.00001779
Iteration 167/1000 | Loss: 0.00001779
Iteration 168/1000 | Loss: 0.00001779
Iteration 169/1000 | Loss: 0.00001779
Iteration 170/1000 | Loss: 0.00001779
Iteration 171/1000 | Loss: 0.00001779
Iteration 172/1000 | Loss: 0.00001779
Iteration 173/1000 | Loss: 0.00001779
Iteration 174/1000 | Loss: 0.00001779
Iteration 175/1000 | Loss: 0.00001779
Iteration 176/1000 | Loss: 0.00001779
Iteration 177/1000 | Loss: 0.00001778
Iteration 178/1000 | Loss: 0.00001778
Iteration 179/1000 | Loss: 0.00001778
Iteration 180/1000 | Loss: 0.00001778
Iteration 181/1000 | Loss: 0.00001778
Iteration 182/1000 | Loss: 0.00001778
Iteration 183/1000 | Loss: 0.00001778
Iteration 184/1000 | Loss: 0.00001778
Iteration 185/1000 | Loss: 0.00001778
Iteration 186/1000 | Loss: 0.00001778
Iteration 187/1000 | Loss: 0.00001778
Iteration 188/1000 | Loss: 0.00001778
Iteration 189/1000 | Loss: 0.00001778
Iteration 190/1000 | Loss: 0.00001778
Iteration 191/1000 | Loss: 0.00001778
Iteration 192/1000 | Loss: 0.00001778
Iteration 193/1000 | Loss: 0.00001778
Iteration 194/1000 | Loss: 0.00001778
Iteration 195/1000 | Loss: 0.00001778
Iteration 196/1000 | Loss: 0.00001778
Iteration 197/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.778192563506309e-05, 1.778192563506309e-05, 1.778192563506309e-05, 1.778192563506309e-05, 1.778192563506309e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.778192563506309e-05

Optimization complete. Final v2v error: 3.5826821327209473 mm

Highest mean error: 4.168987274169922 mm for frame 20

Lowest mean error: 3.3004038333892822 mm for frame 43

Saving results

Total time: 39.861677408218384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389632
Iteration 2/25 | Loss: 0.00140162
Iteration 3/25 | Loss: 0.00127442
Iteration 4/25 | Loss: 0.00125106
Iteration 5/25 | Loss: 0.00124403
Iteration 6/25 | Loss: 0.00124262
Iteration 7/25 | Loss: 0.00124243
Iteration 8/25 | Loss: 0.00124243
Iteration 9/25 | Loss: 0.00124243
Iteration 10/25 | Loss: 0.00124243
Iteration 11/25 | Loss: 0.00124243
Iteration 12/25 | Loss: 0.00124243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012424287851899862, 0.0012424287851899862, 0.0012424287851899862, 0.0012424287851899862, 0.0012424287851899862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012424287851899862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39377844
Iteration 2/25 | Loss: 0.00071674
Iteration 3/25 | Loss: 0.00071674
Iteration 4/25 | Loss: 0.00071674
Iteration 5/25 | Loss: 0.00071674
Iteration 6/25 | Loss: 0.00071674
Iteration 7/25 | Loss: 0.00071674
Iteration 8/25 | Loss: 0.00071674
Iteration 9/25 | Loss: 0.00071674
Iteration 10/25 | Loss: 0.00071674
Iteration 11/25 | Loss: 0.00071674
Iteration 12/25 | Loss: 0.00071674
Iteration 13/25 | Loss: 0.00071674
Iteration 14/25 | Loss: 0.00071674
Iteration 15/25 | Loss: 0.00071674
Iteration 16/25 | Loss: 0.00071674
Iteration 17/25 | Loss: 0.00071674
Iteration 18/25 | Loss: 0.00071674
Iteration 19/25 | Loss: 0.00071674
Iteration 20/25 | Loss: 0.00071674
Iteration 21/25 | Loss: 0.00071674
Iteration 22/25 | Loss: 0.00071674
Iteration 23/25 | Loss: 0.00071674
Iteration 24/25 | Loss: 0.00071674
Iteration 25/25 | Loss: 0.00071674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071674
Iteration 2/1000 | Loss: 0.00005430
Iteration 3/1000 | Loss: 0.00003530
Iteration 4/1000 | Loss: 0.00002846
Iteration 5/1000 | Loss: 0.00002570
Iteration 6/1000 | Loss: 0.00002414
Iteration 7/1000 | Loss: 0.00002274
Iteration 8/1000 | Loss: 0.00002186
Iteration 9/1000 | Loss: 0.00002118
Iteration 10/1000 | Loss: 0.00002077
Iteration 11/1000 | Loss: 0.00002040
Iteration 12/1000 | Loss: 0.00002011
Iteration 13/1000 | Loss: 0.00001994
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001959
Iteration 16/1000 | Loss: 0.00001951
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001946
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001929
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001928
Iteration 24/1000 | Loss: 0.00001927
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001926
Iteration 27/1000 | Loss: 0.00001924
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001923
Iteration 30/1000 | Loss: 0.00001921
Iteration 31/1000 | Loss: 0.00001917
Iteration 32/1000 | Loss: 0.00001916
Iteration 33/1000 | Loss: 0.00001916
Iteration 34/1000 | Loss: 0.00001916
Iteration 35/1000 | Loss: 0.00001915
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001914
Iteration 38/1000 | Loss: 0.00001913
Iteration 39/1000 | Loss: 0.00001913
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001912
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001911
Iteration 45/1000 | Loss: 0.00001911
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001910
Iteration 48/1000 | Loss: 0.00001910
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001909
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001907
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001903
Iteration 67/1000 | Loss: 0.00001903
Iteration 68/1000 | Loss: 0.00001903
Iteration 69/1000 | Loss: 0.00001902
Iteration 70/1000 | Loss: 0.00001902
Iteration 71/1000 | Loss: 0.00001902
Iteration 72/1000 | Loss: 0.00001901
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001900
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001899
Iteration 86/1000 | Loss: 0.00001899
Iteration 87/1000 | Loss: 0.00001899
Iteration 88/1000 | Loss: 0.00001899
Iteration 89/1000 | Loss: 0.00001899
Iteration 90/1000 | Loss: 0.00001899
Iteration 91/1000 | Loss: 0.00001898
Iteration 92/1000 | Loss: 0.00001898
Iteration 93/1000 | Loss: 0.00001898
Iteration 94/1000 | Loss: 0.00001897
Iteration 95/1000 | Loss: 0.00001897
Iteration 96/1000 | Loss: 0.00001897
Iteration 97/1000 | Loss: 0.00001897
Iteration 98/1000 | Loss: 0.00001897
Iteration 99/1000 | Loss: 0.00001897
Iteration 100/1000 | Loss: 0.00001897
Iteration 101/1000 | Loss: 0.00001896
Iteration 102/1000 | Loss: 0.00001896
Iteration 103/1000 | Loss: 0.00001896
Iteration 104/1000 | Loss: 0.00001896
Iteration 105/1000 | Loss: 0.00001896
Iteration 106/1000 | Loss: 0.00001896
Iteration 107/1000 | Loss: 0.00001896
Iteration 108/1000 | Loss: 0.00001895
Iteration 109/1000 | Loss: 0.00001895
Iteration 110/1000 | Loss: 0.00001895
Iteration 111/1000 | Loss: 0.00001895
Iteration 112/1000 | Loss: 0.00001895
Iteration 113/1000 | Loss: 0.00001895
Iteration 114/1000 | Loss: 0.00001894
Iteration 115/1000 | Loss: 0.00001894
Iteration 116/1000 | Loss: 0.00001894
Iteration 117/1000 | Loss: 0.00001894
Iteration 118/1000 | Loss: 0.00001894
Iteration 119/1000 | Loss: 0.00001894
Iteration 120/1000 | Loss: 0.00001894
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001894
Iteration 123/1000 | Loss: 0.00001894
Iteration 124/1000 | Loss: 0.00001894
Iteration 125/1000 | Loss: 0.00001894
Iteration 126/1000 | Loss: 0.00001894
Iteration 127/1000 | Loss: 0.00001893
Iteration 128/1000 | Loss: 0.00001893
Iteration 129/1000 | Loss: 0.00001893
Iteration 130/1000 | Loss: 0.00001893
Iteration 131/1000 | Loss: 0.00001893
Iteration 132/1000 | Loss: 0.00001893
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001893
Iteration 136/1000 | Loss: 0.00001893
Iteration 137/1000 | Loss: 0.00001893
Iteration 138/1000 | Loss: 0.00001893
Iteration 139/1000 | Loss: 0.00001893
Iteration 140/1000 | Loss: 0.00001893
Iteration 141/1000 | Loss: 0.00001892
Iteration 142/1000 | Loss: 0.00001892
Iteration 143/1000 | Loss: 0.00001892
Iteration 144/1000 | Loss: 0.00001892
Iteration 145/1000 | Loss: 0.00001892
Iteration 146/1000 | Loss: 0.00001892
Iteration 147/1000 | Loss: 0.00001892
Iteration 148/1000 | Loss: 0.00001892
Iteration 149/1000 | Loss: 0.00001892
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001892
Iteration 152/1000 | Loss: 0.00001892
Iteration 153/1000 | Loss: 0.00001892
Iteration 154/1000 | Loss: 0.00001891
Iteration 155/1000 | Loss: 0.00001891
Iteration 156/1000 | Loss: 0.00001891
Iteration 157/1000 | Loss: 0.00001891
Iteration 158/1000 | Loss: 0.00001891
Iteration 159/1000 | Loss: 0.00001891
Iteration 160/1000 | Loss: 0.00001891
Iteration 161/1000 | Loss: 0.00001891
Iteration 162/1000 | Loss: 0.00001891
Iteration 163/1000 | Loss: 0.00001891
Iteration 164/1000 | Loss: 0.00001891
Iteration 165/1000 | Loss: 0.00001891
Iteration 166/1000 | Loss: 0.00001891
Iteration 167/1000 | Loss: 0.00001891
Iteration 168/1000 | Loss: 0.00001891
Iteration 169/1000 | Loss: 0.00001890
Iteration 170/1000 | Loss: 0.00001890
Iteration 171/1000 | Loss: 0.00001890
Iteration 172/1000 | Loss: 0.00001890
Iteration 173/1000 | Loss: 0.00001890
Iteration 174/1000 | Loss: 0.00001890
Iteration 175/1000 | Loss: 0.00001889
Iteration 176/1000 | Loss: 0.00001889
Iteration 177/1000 | Loss: 0.00001889
Iteration 178/1000 | Loss: 0.00001889
Iteration 179/1000 | Loss: 0.00001889
Iteration 180/1000 | Loss: 0.00001889
Iteration 181/1000 | Loss: 0.00001889
Iteration 182/1000 | Loss: 0.00001889
Iteration 183/1000 | Loss: 0.00001889
Iteration 184/1000 | Loss: 0.00001888
Iteration 185/1000 | Loss: 0.00001888
Iteration 186/1000 | Loss: 0.00001888
Iteration 187/1000 | Loss: 0.00001888
Iteration 188/1000 | Loss: 0.00001888
Iteration 189/1000 | Loss: 0.00001888
Iteration 190/1000 | Loss: 0.00001888
Iteration 191/1000 | Loss: 0.00001888
Iteration 192/1000 | Loss: 0.00001888
Iteration 193/1000 | Loss: 0.00001888
Iteration 194/1000 | Loss: 0.00001888
Iteration 195/1000 | Loss: 0.00001888
Iteration 196/1000 | Loss: 0.00001888
Iteration 197/1000 | Loss: 0.00001888
Iteration 198/1000 | Loss: 0.00001887
Iteration 199/1000 | Loss: 0.00001887
Iteration 200/1000 | Loss: 0.00001887
Iteration 201/1000 | Loss: 0.00001887
Iteration 202/1000 | Loss: 0.00001887
Iteration 203/1000 | Loss: 0.00001887
Iteration 204/1000 | Loss: 0.00001887
Iteration 205/1000 | Loss: 0.00001886
Iteration 206/1000 | Loss: 0.00001886
Iteration 207/1000 | Loss: 0.00001886
Iteration 208/1000 | Loss: 0.00001886
Iteration 209/1000 | Loss: 0.00001886
Iteration 210/1000 | Loss: 0.00001886
Iteration 211/1000 | Loss: 0.00001885
Iteration 212/1000 | Loss: 0.00001885
Iteration 213/1000 | Loss: 0.00001885
Iteration 214/1000 | Loss: 0.00001885
Iteration 215/1000 | Loss: 0.00001885
Iteration 216/1000 | Loss: 0.00001885
Iteration 217/1000 | Loss: 0.00001885
Iteration 218/1000 | Loss: 0.00001885
Iteration 219/1000 | Loss: 0.00001885
Iteration 220/1000 | Loss: 0.00001885
Iteration 221/1000 | Loss: 0.00001885
Iteration 222/1000 | Loss: 0.00001885
Iteration 223/1000 | Loss: 0.00001885
Iteration 224/1000 | Loss: 0.00001885
Iteration 225/1000 | Loss: 0.00001885
Iteration 226/1000 | Loss: 0.00001885
Iteration 227/1000 | Loss: 0.00001885
Iteration 228/1000 | Loss: 0.00001884
Iteration 229/1000 | Loss: 0.00001884
Iteration 230/1000 | Loss: 0.00001884
Iteration 231/1000 | Loss: 0.00001884
Iteration 232/1000 | Loss: 0.00001884
Iteration 233/1000 | Loss: 0.00001884
Iteration 234/1000 | Loss: 0.00001884
Iteration 235/1000 | Loss: 0.00001884
Iteration 236/1000 | Loss: 0.00001884
Iteration 237/1000 | Loss: 0.00001884
Iteration 238/1000 | Loss: 0.00001884
Iteration 239/1000 | Loss: 0.00001884
Iteration 240/1000 | Loss: 0.00001884
Iteration 241/1000 | Loss: 0.00001884
Iteration 242/1000 | Loss: 0.00001884
Iteration 243/1000 | Loss: 0.00001884
Iteration 244/1000 | Loss: 0.00001884
Iteration 245/1000 | Loss: 0.00001884
Iteration 246/1000 | Loss: 0.00001884
Iteration 247/1000 | Loss: 0.00001884
Iteration 248/1000 | Loss: 0.00001884
Iteration 249/1000 | Loss: 0.00001883
Iteration 250/1000 | Loss: 0.00001883
Iteration 251/1000 | Loss: 0.00001883
Iteration 252/1000 | Loss: 0.00001883
Iteration 253/1000 | Loss: 0.00001883
Iteration 254/1000 | Loss: 0.00001883
Iteration 255/1000 | Loss: 0.00001883
Iteration 256/1000 | Loss: 0.00001883
Iteration 257/1000 | Loss: 0.00001883
Iteration 258/1000 | Loss: 0.00001883
Iteration 259/1000 | Loss: 0.00001883
Iteration 260/1000 | Loss: 0.00001883
Iteration 261/1000 | Loss: 0.00001883
Iteration 262/1000 | Loss: 0.00001883
Iteration 263/1000 | Loss: 0.00001883
Iteration 264/1000 | Loss: 0.00001883
Iteration 265/1000 | Loss: 0.00001883
Iteration 266/1000 | Loss: 0.00001883
Iteration 267/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.8833799913409166e-05, 1.8833799913409166e-05, 1.8833799913409166e-05, 1.8833799913409166e-05, 1.8833799913409166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8833799913409166e-05

Optimization complete. Final v2v error: 3.6576218605041504 mm

Highest mean error: 4.363283157348633 mm for frame 73

Lowest mean error: 3.18489408493042 mm for frame 83

Saving results

Total time: 47.67339062690735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931229
Iteration 2/25 | Loss: 0.00221548
Iteration 3/25 | Loss: 0.00165573
Iteration 4/25 | Loss: 0.00153610
Iteration 5/25 | Loss: 0.00148574
Iteration 6/25 | Loss: 0.00141914
Iteration 7/25 | Loss: 0.00139270
Iteration 8/25 | Loss: 0.00137190
Iteration 9/25 | Loss: 0.00134036
Iteration 10/25 | Loss: 0.00133731
Iteration 11/25 | Loss: 0.00132917
Iteration 12/25 | Loss: 0.00132480
Iteration 13/25 | Loss: 0.00132245
Iteration 14/25 | Loss: 0.00131917
Iteration 15/25 | Loss: 0.00131806
Iteration 16/25 | Loss: 0.00131767
Iteration 17/25 | Loss: 0.00131747
Iteration 18/25 | Loss: 0.00131724
Iteration 19/25 | Loss: 0.00131679
Iteration 20/25 | Loss: 0.00131580
Iteration 21/25 | Loss: 0.00131693
Iteration 22/25 | Loss: 0.00131311
Iteration 23/25 | Loss: 0.00131617
Iteration 24/25 | Loss: 0.00131945
Iteration 25/25 | Loss: 0.00131429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49399340
Iteration 2/25 | Loss: 0.00098200
Iteration 3/25 | Loss: 0.00071290
Iteration 4/25 | Loss: 0.00071289
Iteration 5/25 | Loss: 0.00071289
Iteration 6/25 | Loss: 0.00071289
Iteration 7/25 | Loss: 0.00071289
Iteration 8/25 | Loss: 0.00071289
Iteration 9/25 | Loss: 0.00071289
Iteration 10/25 | Loss: 0.00071289
Iteration 11/25 | Loss: 0.00071289
Iteration 12/25 | Loss: 0.00071289
Iteration 13/25 | Loss: 0.00071289
Iteration 14/25 | Loss: 0.00071289
Iteration 15/25 | Loss: 0.00071289
Iteration 16/25 | Loss: 0.00071289
Iteration 17/25 | Loss: 0.00071289
Iteration 18/25 | Loss: 0.00071289
Iteration 19/25 | Loss: 0.00071289
Iteration 20/25 | Loss: 0.00071289
Iteration 21/25 | Loss: 0.00071289
Iteration 22/25 | Loss: 0.00071289
Iteration 23/25 | Loss: 0.00071289
Iteration 24/25 | Loss: 0.00071289
Iteration 25/25 | Loss: 0.00071289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071289
Iteration 2/1000 | Loss: 0.00005623
Iteration 3/1000 | Loss: 0.00050491
Iteration 4/1000 | Loss: 0.00022484
Iteration 5/1000 | Loss: 0.00003044
Iteration 6/1000 | Loss: 0.00010805
Iteration 7/1000 | Loss: 0.00010079
Iteration 8/1000 | Loss: 0.00004239
Iteration 9/1000 | Loss: 0.00002549
Iteration 10/1000 | Loss: 0.00002469
Iteration 11/1000 | Loss: 0.00002424
Iteration 12/1000 | Loss: 0.00032303
Iteration 13/1000 | Loss: 0.00002629
Iteration 14/1000 | Loss: 0.00002355
Iteration 15/1000 | Loss: 0.00002294
Iteration 16/1000 | Loss: 0.00002223
Iteration 17/1000 | Loss: 0.00002169
Iteration 18/1000 | Loss: 0.00002143
Iteration 19/1000 | Loss: 0.00002127
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002120
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002089
Iteration 24/1000 | Loss: 0.00002086
Iteration 25/1000 | Loss: 0.00002085
Iteration 26/1000 | Loss: 0.00002085
Iteration 27/1000 | Loss: 0.00002085
Iteration 28/1000 | Loss: 0.00002084
Iteration 29/1000 | Loss: 0.00002083
Iteration 30/1000 | Loss: 0.00002083
Iteration 31/1000 | Loss: 0.00002080
Iteration 32/1000 | Loss: 0.00002080
Iteration 33/1000 | Loss: 0.00002080
Iteration 34/1000 | Loss: 0.00002075
Iteration 35/1000 | Loss: 0.00002074
Iteration 36/1000 | Loss: 0.00002073
Iteration 37/1000 | Loss: 0.00002073
Iteration 38/1000 | Loss: 0.00002073
Iteration 39/1000 | Loss: 0.00002073
Iteration 40/1000 | Loss: 0.00002073
Iteration 41/1000 | Loss: 0.00002073
Iteration 42/1000 | Loss: 0.00002073
Iteration 43/1000 | Loss: 0.00002073
Iteration 44/1000 | Loss: 0.00002073
Iteration 45/1000 | Loss: 0.00002073
Iteration 46/1000 | Loss: 0.00002073
Iteration 47/1000 | Loss: 0.00002072
Iteration 48/1000 | Loss: 0.00002072
Iteration 49/1000 | Loss: 0.00002072
Iteration 50/1000 | Loss: 0.00002072
Iteration 51/1000 | Loss: 0.00002072
Iteration 52/1000 | Loss: 0.00002072
Iteration 53/1000 | Loss: 0.00002072
Iteration 54/1000 | Loss: 0.00002071
Iteration 55/1000 | Loss: 0.00002071
Iteration 56/1000 | Loss: 0.00002070
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002070
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002066
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002065
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002065
Iteration 85/1000 | Loss: 0.00002064
Iteration 86/1000 | Loss: 0.00002064
Iteration 87/1000 | Loss: 0.00002064
Iteration 88/1000 | Loss: 0.00002064
Iteration 89/1000 | Loss: 0.00002064
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002064
Iteration 93/1000 | Loss: 0.00002063
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002063
Iteration 97/1000 | Loss: 0.00002063
Iteration 98/1000 | Loss: 0.00002063
Iteration 99/1000 | Loss: 0.00002063
Iteration 100/1000 | Loss: 0.00002063
Iteration 101/1000 | Loss: 0.00002063
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002063
Iteration 105/1000 | Loss: 0.00002063
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002063
Iteration 108/1000 | Loss: 0.00002063
Iteration 109/1000 | Loss: 0.00002063
Iteration 110/1000 | Loss: 0.00002063
Iteration 111/1000 | Loss: 0.00002063
Iteration 112/1000 | Loss: 0.00002063
Iteration 113/1000 | Loss: 0.00002063
Iteration 114/1000 | Loss: 0.00002063
Iteration 115/1000 | Loss: 0.00002063
Iteration 116/1000 | Loss: 0.00002063
Iteration 117/1000 | Loss: 0.00002063
Iteration 118/1000 | Loss: 0.00002063
Iteration 119/1000 | Loss: 0.00002063
Iteration 120/1000 | Loss: 0.00002063
Iteration 121/1000 | Loss: 0.00002063
Iteration 122/1000 | Loss: 0.00002063
Iteration 123/1000 | Loss: 0.00002063
Iteration 124/1000 | Loss: 0.00002063
Iteration 125/1000 | Loss: 0.00002063
Iteration 126/1000 | Loss: 0.00002063
Iteration 127/1000 | Loss: 0.00002063
Iteration 128/1000 | Loss: 0.00002063
Iteration 129/1000 | Loss: 0.00002063
Iteration 130/1000 | Loss: 0.00002063
Iteration 131/1000 | Loss: 0.00002063
Iteration 132/1000 | Loss: 0.00002063
Iteration 133/1000 | Loss: 0.00002063
Iteration 134/1000 | Loss: 0.00002063
Iteration 135/1000 | Loss: 0.00002063
Iteration 136/1000 | Loss: 0.00002063
Iteration 137/1000 | Loss: 0.00002063
Iteration 138/1000 | Loss: 0.00002063
Iteration 139/1000 | Loss: 0.00002063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.0634215616155416e-05, 2.0634215616155416e-05, 2.0634215616155416e-05, 2.0634215616155416e-05, 2.0634215616155416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0634215616155416e-05

Optimization complete. Final v2v error: 3.8862080574035645 mm

Highest mean error: 4.875393390655518 mm for frame 133

Lowest mean error: 3.4717864990234375 mm for frame 129

Saving results

Total time: 94.56877851486206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995410
Iteration 2/25 | Loss: 0.00995410
Iteration 3/25 | Loss: 0.00995409
Iteration 4/25 | Loss: 0.00295879
Iteration 5/25 | Loss: 0.00224197
Iteration 6/25 | Loss: 0.00202549
Iteration 7/25 | Loss: 0.00180563
Iteration 8/25 | Loss: 0.00169440
Iteration 9/25 | Loss: 0.00166848
Iteration 10/25 | Loss: 0.00159243
Iteration 11/25 | Loss: 0.00155193
Iteration 12/25 | Loss: 0.00155146
Iteration 13/25 | Loss: 0.00154094
Iteration 14/25 | Loss: 0.00156545
Iteration 15/25 | Loss: 0.00152640
Iteration 16/25 | Loss: 0.00152800
Iteration 17/25 | Loss: 0.00152135
Iteration 18/25 | Loss: 0.00151910
Iteration 19/25 | Loss: 0.00151989
Iteration 20/25 | Loss: 0.00150242
Iteration 21/25 | Loss: 0.00149478
Iteration 22/25 | Loss: 0.00149790
Iteration 23/25 | Loss: 0.00151950
Iteration 24/25 | Loss: 0.00148751
Iteration 25/25 | Loss: 0.00149285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61425948
Iteration 2/25 | Loss: 0.00259782
Iteration 3/25 | Loss: 0.00212002
Iteration 4/25 | Loss: 0.00212002
Iteration 5/25 | Loss: 0.00212002
Iteration 6/25 | Loss: 0.00212002
Iteration 7/25 | Loss: 0.00212002
Iteration 8/25 | Loss: 0.00212002
Iteration 9/25 | Loss: 0.00212002
Iteration 10/25 | Loss: 0.00212002
Iteration 11/25 | Loss: 0.00212002
Iteration 12/25 | Loss: 0.00212002
Iteration 13/25 | Loss: 0.00212002
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0021200214978307486, 0.0021200214978307486, 0.0021200214978307486, 0.0021200214978307486, 0.0021200214978307486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021200214978307486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212002
Iteration 2/1000 | Loss: 0.00156989
Iteration 3/1000 | Loss: 0.00217190
Iteration 4/1000 | Loss: 0.00102731
Iteration 5/1000 | Loss: 0.00067229
Iteration 6/1000 | Loss: 0.00047071
Iteration 7/1000 | Loss: 0.00034904
Iteration 8/1000 | Loss: 0.00051668
Iteration 9/1000 | Loss: 0.00083312
Iteration 10/1000 | Loss: 0.00050610
Iteration 11/1000 | Loss: 0.00104543
Iteration 12/1000 | Loss: 0.00029435
Iteration 13/1000 | Loss: 0.00049631
Iteration 14/1000 | Loss: 0.00029993
Iteration 15/1000 | Loss: 0.00034676
Iteration 16/1000 | Loss: 0.00013194
Iteration 17/1000 | Loss: 0.00013529
Iteration 18/1000 | Loss: 0.00053124
Iteration 19/1000 | Loss: 0.00049781
Iteration 20/1000 | Loss: 0.00074006
Iteration 21/1000 | Loss: 0.00106719
Iteration 22/1000 | Loss: 0.00011606
Iteration 23/1000 | Loss: 0.00029322
Iteration 24/1000 | Loss: 0.00042586
Iteration 25/1000 | Loss: 0.00019725
Iteration 26/1000 | Loss: 0.00029202
Iteration 27/1000 | Loss: 0.00019821
Iteration 28/1000 | Loss: 0.00023602
Iteration 29/1000 | Loss: 0.00025652
Iteration 30/1000 | Loss: 0.00012661
Iteration 31/1000 | Loss: 0.00021870
Iteration 32/1000 | Loss: 0.00019839
Iteration 33/1000 | Loss: 0.00020148
Iteration 34/1000 | Loss: 0.00031996
Iteration 35/1000 | Loss: 0.00117763
Iteration 36/1000 | Loss: 0.00047437
Iteration 37/1000 | Loss: 0.00077515
Iteration 38/1000 | Loss: 0.00059853
Iteration 39/1000 | Loss: 0.00020258
Iteration 40/1000 | Loss: 0.00022371
Iteration 41/1000 | Loss: 0.00029612
Iteration 42/1000 | Loss: 0.00011311
Iteration 43/1000 | Loss: 0.00089327
Iteration 44/1000 | Loss: 0.00071243
Iteration 45/1000 | Loss: 0.00013672
Iteration 46/1000 | Loss: 0.00011037
Iteration 47/1000 | Loss: 0.00015958
Iteration 48/1000 | Loss: 0.00012467
Iteration 49/1000 | Loss: 0.00015122
Iteration 50/1000 | Loss: 0.00014919
Iteration 51/1000 | Loss: 0.00046133
Iteration 52/1000 | Loss: 0.00029979
Iteration 53/1000 | Loss: 0.00022685
Iteration 54/1000 | Loss: 0.00010278
Iteration 55/1000 | Loss: 0.00009817
Iteration 56/1000 | Loss: 0.00012620
Iteration 57/1000 | Loss: 0.00012333
Iteration 58/1000 | Loss: 0.00015501
Iteration 59/1000 | Loss: 0.00009157
Iteration 60/1000 | Loss: 0.00030987
Iteration 61/1000 | Loss: 0.00052443
Iteration 62/1000 | Loss: 0.00020327
Iteration 63/1000 | Loss: 0.00013897
Iteration 64/1000 | Loss: 0.00019119
Iteration 65/1000 | Loss: 0.00069513
Iteration 66/1000 | Loss: 0.00060547
Iteration 67/1000 | Loss: 0.00081948
Iteration 68/1000 | Loss: 0.00071647
Iteration 69/1000 | Loss: 0.00036800
Iteration 70/1000 | Loss: 0.00027098
Iteration 71/1000 | Loss: 0.00015092
Iteration 72/1000 | Loss: 0.00039386
Iteration 73/1000 | Loss: 0.00015763
Iteration 74/1000 | Loss: 0.00024552
Iteration 75/1000 | Loss: 0.00036147
Iteration 76/1000 | Loss: 0.00065059
Iteration 77/1000 | Loss: 0.00128880
Iteration 78/1000 | Loss: 0.00144644
Iteration 79/1000 | Loss: 0.00046591
Iteration 80/1000 | Loss: 0.00025754
Iteration 81/1000 | Loss: 0.00054063
Iteration 82/1000 | Loss: 0.00007571
Iteration 83/1000 | Loss: 0.00018707
Iteration 84/1000 | Loss: 0.00006346
Iteration 85/1000 | Loss: 0.00013977
Iteration 86/1000 | Loss: 0.00022356
Iteration 87/1000 | Loss: 0.00008851
Iteration 88/1000 | Loss: 0.00011076
Iteration 89/1000 | Loss: 0.00022575
Iteration 90/1000 | Loss: 0.00009682
Iteration 91/1000 | Loss: 0.00004907
Iteration 92/1000 | Loss: 0.00012054
Iteration 93/1000 | Loss: 0.00058675
Iteration 94/1000 | Loss: 0.00004875
Iteration 95/1000 | Loss: 0.00005964
Iteration 96/1000 | Loss: 0.00013083
Iteration 97/1000 | Loss: 0.00004291
Iteration 98/1000 | Loss: 0.00018105
Iteration 99/1000 | Loss: 0.00012349
Iteration 100/1000 | Loss: 0.00019215
Iteration 101/1000 | Loss: 0.00025092
Iteration 102/1000 | Loss: 0.00034592
Iteration 103/1000 | Loss: 0.00037833
Iteration 104/1000 | Loss: 0.00019731
Iteration 105/1000 | Loss: 0.00012941
Iteration 106/1000 | Loss: 0.00030087
Iteration 107/1000 | Loss: 0.00005350
Iteration 108/1000 | Loss: 0.00006478
Iteration 109/1000 | Loss: 0.00004225
Iteration 110/1000 | Loss: 0.00007768
Iteration 111/1000 | Loss: 0.00004929
Iteration 112/1000 | Loss: 0.00003964
Iteration 113/1000 | Loss: 0.00005393
Iteration 114/1000 | Loss: 0.00003842
Iteration 115/1000 | Loss: 0.00018285
Iteration 116/1000 | Loss: 0.00005628
Iteration 117/1000 | Loss: 0.00008000
Iteration 118/1000 | Loss: 0.00012750
Iteration 119/1000 | Loss: 0.00006250
Iteration 120/1000 | Loss: 0.00005288
Iteration 121/1000 | Loss: 0.00004267
Iteration 122/1000 | Loss: 0.00003723
Iteration 123/1000 | Loss: 0.00007408
Iteration 124/1000 | Loss: 0.00003632
Iteration 125/1000 | Loss: 0.00003616
Iteration 126/1000 | Loss: 0.00003559
Iteration 127/1000 | Loss: 0.00003528
Iteration 128/1000 | Loss: 0.00003492
Iteration 129/1000 | Loss: 0.00006609
Iteration 130/1000 | Loss: 0.00003480
Iteration 131/1000 | Loss: 0.00003448
Iteration 132/1000 | Loss: 0.00003444
Iteration 133/1000 | Loss: 0.00006994
Iteration 134/1000 | Loss: 0.00003396
Iteration 135/1000 | Loss: 0.00008286
Iteration 136/1000 | Loss: 0.00003374
Iteration 137/1000 | Loss: 0.00003353
Iteration 138/1000 | Loss: 0.00006205
Iteration 139/1000 | Loss: 0.00003328
Iteration 140/1000 | Loss: 0.00003316
Iteration 141/1000 | Loss: 0.00003312
Iteration 142/1000 | Loss: 0.00003310
Iteration 143/1000 | Loss: 0.00003310
Iteration 144/1000 | Loss: 0.00003308
Iteration 145/1000 | Loss: 0.00003306
Iteration 146/1000 | Loss: 0.00003288
Iteration 147/1000 | Loss: 0.00003273
Iteration 148/1000 | Loss: 0.00003265
Iteration 149/1000 | Loss: 0.00003262
Iteration 150/1000 | Loss: 0.00005827
Iteration 151/1000 | Loss: 0.00003527
Iteration 152/1000 | Loss: 0.00003251
Iteration 153/1000 | Loss: 0.00003251
Iteration 154/1000 | Loss: 0.00003251
Iteration 155/1000 | Loss: 0.00003250
Iteration 156/1000 | Loss: 0.00003250
Iteration 157/1000 | Loss: 0.00003250
Iteration 158/1000 | Loss: 0.00003249
Iteration 159/1000 | Loss: 0.00003249
Iteration 160/1000 | Loss: 0.00003249
Iteration 161/1000 | Loss: 0.00003851
Iteration 162/1000 | Loss: 0.00012225
Iteration 163/1000 | Loss: 0.00003468
Iteration 164/1000 | Loss: 0.00003271
Iteration 165/1000 | Loss: 0.00007966
Iteration 166/1000 | Loss: 0.00004205
Iteration 167/1000 | Loss: 0.00003247
Iteration 168/1000 | Loss: 0.00003241
Iteration 169/1000 | Loss: 0.00003241
Iteration 170/1000 | Loss: 0.00003241
Iteration 171/1000 | Loss: 0.00003241
Iteration 172/1000 | Loss: 0.00003241
Iteration 173/1000 | Loss: 0.00003241
Iteration 174/1000 | Loss: 0.00003241
Iteration 175/1000 | Loss: 0.00003241
Iteration 176/1000 | Loss: 0.00004048
Iteration 177/1000 | Loss: 0.00003403
Iteration 178/1000 | Loss: 0.00003884
Iteration 179/1000 | Loss: 0.00003516
Iteration 180/1000 | Loss: 0.00003291
Iteration 181/1000 | Loss: 0.00003291
Iteration 182/1000 | Loss: 0.00003235
Iteration 183/1000 | Loss: 0.00003235
Iteration 184/1000 | Loss: 0.00003235
Iteration 185/1000 | Loss: 0.00003235
Iteration 186/1000 | Loss: 0.00003235
Iteration 187/1000 | Loss: 0.00003234
Iteration 188/1000 | Loss: 0.00003234
Iteration 189/1000 | Loss: 0.00003234
Iteration 190/1000 | Loss: 0.00003234
Iteration 191/1000 | Loss: 0.00003234
Iteration 192/1000 | Loss: 0.00003234
Iteration 193/1000 | Loss: 0.00003234
Iteration 194/1000 | Loss: 0.00003234
Iteration 195/1000 | Loss: 0.00003234
Iteration 196/1000 | Loss: 0.00003234
Iteration 197/1000 | Loss: 0.00003234
Iteration 198/1000 | Loss: 0.00003234
Iteration 199/1000 | Loss: 0.00003234
Iteration 200/1000 | Loss: 0.00003234
Iteration 201/1000 | Loss: 0.00003234
Iteration 202/1000 | Loss: 0.00003233
Iteration 203/1000 | Loss: 0.00003233
Iteration 204/1000 | Loss: 0.00003233
Iteration 205/1000 | Loss: 0.00003233
Iteration 206/1000 | Loss: 0.00003233
Iteration 207/1000 | Loss: 0.00003233
Iteration 208/1000 | Loss: 0.00003233
Iteration 209/1000 | Loss: 0.00003233
Iteration 210/1000 | Loss: 0.00003233
Iteration 211/1000 | Loss: 0.00003455
Iteration 212/1000 | Loss: 0.00003232
Iteration 213/1000 | Loss: 0.00003232
Iteration 214/1000 | Loss: 0.00003231
Iteration 215/1000 | Loss: 0.00003230
Iteration 216/1000 | Loss: 0.00003230
Iteration 217/1000 | Loss: 0.00003230
Iteration 218/1000 | Loss: 0.00003230
Iteration 219/1000 | Loss: 0.00003230
Iteration 220/1000 | Loss: 0.00003230
Iteration 221/1000 | Loss: 0.00003230
Iteration 222/1000 | Loss: 0.00003229
Iteration 223/1000 | Loss: 0.00003229
Iteration 224/1000 | Loss: 0.00003229
Iteration 225/1000 | Loss: 0.00003229
Iteration 226/1000 | Loss: 0.00003229
Iteration 227/1000 | Loss: 0.00003229
Iteration 228/1000 | Loss: 0.00003229
Iteration 229/1000 | Loss: 0.00003229
Iteration 230/1000 | Loss: 0.00003229
Iteration 231/1000 | Loss: 0.00003229
Iteration 232/1000 | Loss: 0.00003229
Iteration 233/1000 | Loss: 0.00003229
Iteration 234/1000 | Loss: 0.00003229
Iteration 235/1000 | Loss: 0.00003229
Iteration 236/1000 | Loss: 0.00003228
Iteration 237/1000 | Loss: 0.00003228
Iteration 238/1000 | Loss: 0.00003228
Iteration 239/1000 | Loss: 0.00003228
Iteration 240/1000 | Loss: 0.00003227
Iteration 241/1000 | Loss: 0.00003227
Iteration 242/1000 | Loss: 0.00003227
Iteration 243/1000 | Loss: 0.00003227
Iteration 244/1000 | Loss: 0.00003227
Iteration 245/1000 | Loss: 0.00003227
Iteration 246/1000 | Loss: 0.00003227
Iteration 247/1000 | Loss: 0.00003227
Iteration 248/1000 | Loss: 0.00003227
Iteration 249/1000 | Loss: 0.00003227
Iteration 250/1000 | Loss: 0.00003227
Iteration 251/1000 | Loss: 0.00003226
Iteration 252/1000 | Loss: 0.00003226
Iteration 253/1000 | Loss: 0.00003226
Iteration 254/1000 | Loss: 0.00003226
Iteration 255/1000 | Loss: 0.00003226
Iteration 256/1000 | Loss: 0.00003226
Iteration 257/1000 | Loss: 0.00003225
Iteration 258/1000 | Loss: 0.00003225
Iteration 259/1000 | Loss: 0.00003225
Iteration 260/1000 | Loss: 0.00003225
Iteration 261/1000 | Loss: 0.00003225
Iteration 262/1000 | Loss: 0.00003225
Iteration 263/1000 | Loss: 0.00003225
Iteration 264/1000 | Loss: 0.00003225
Iteration 265/1000 | Loss: 0.00003224
Iteration 266/1000 | Loss: 0.00003224
Iteration 267/1000 | Loss: 0.00003224
Iteration 268/1000 | Loss: 0.00003224
Iteration 269/1000 | Loss: 0.00003224
Iteration 270/1000 | Loss: 0.00003224
Iteration 271/1000 | Loss: 0.00003223
Iteration 272/1000 | Loss: 0.00003223
Iteration 273/1000 | Loss: 0.00003223
Iteration 274/1000 | Loss: 0.00003223
Iteration 275/1000 | Loss: 0.00003222
Iteration 276/1000 | Loss: 0.00003222
Iteration 277/1000 | Loss: 0.00003222
Iteration 278/1000 | Loss: 0.00003222
Iteration 279/1000 | Loss: 0.00003222
Iteration 280/1000 | Loss: 0.00003222
Iteration 281/1000 | Loss: 0.00003222
Iteration 282/1000 | Loss: 0.00003222
Iteration 283/1000 | Loss: 0.00003222
Iteration 284/1000 | Loss: 0.00003222
Iteration 285/1000 | Loss: 0.00003222
Iteration 286/1000 | Loss: 0.00003221
Iteration 287/1000 | Loss: 0.00003221
Iteration 288/1000 | Loss: 0.00003221
Iteration 289/1000 | Loss: 0.00003221
Iteration 290/1000 | Loss: 0.00003221
Iteration 291/1000 | Loss: 0.00003221
Iteration 292/1000 | Loss: 0.00003221
Iteration 293/1000 | Loss: 0.00003221
Iteration 294/1000 | Loss: 0.00003220
Iteration 295/1000 | Loss: 0.00003220
Iteration 296/1000 | Loss: 0.00003220
Iteration 297/1000 | Loss: 0.00003220
Iteration 298/1000 | Loss: 0.00003220
Iteration 299/1000 | Loss: 0.00003220
Iteration 300/1000 | Loss: 0.00003220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [3.2204650779021904e-05, 3.2204650779021904e-05, 3.2204650779021904e-05, 3.2204650779021904e-05, 3.2204650779021904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2204650779021904e-05

Optimization complete. Final v2v error: 3.7633578777313232 mm

Highest mean error: 10.983407020568848 mm for frame 46

Lowest mean error: 2.885742425918579 mm for frame 22

Saving results

Total time: 299.69731545448303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00683493
Iteration 2/25 | Loss: 0.00179964
Iteration 3/25 | Loss: 0.00151764
Iteration 4/25 | Loss: 0.00142778
Iteration 5/25 | Loss: 0.00141470
Iteration 6/25 | Loss: 0.00140045
Iteration 7/25 | Loss: 0.00139336
Iteration 8/25 | Loss: 0.00139838
Iteration 9/25 | Loss: 0.00139231
Iteration 10/25 | Loss: 0.00138759
Iteration 11/25 | Loss: 0.00138290
Iteration 12/25 | Loss: 0.00137929
Iteration 13/25 | Loss: 0.00138887
Iteration 14/25 | Loss: 0.00138691
Iteration 15/25 | Loss: 0.00138859
Iteration 16/25 | Loss: 0.00138411
Iteration 17/25 | Loss: 0.00138467
Iteration 18/25 | Loss: 0.00138358
Iteration 19/25 | Loss: 0.00138289
Iteration 20/25 | Loss: 0.00138208
Iteration 21/25 | Loss: 0.00137978
Iteration 22/25 | Loss: 0.00138499
Iteration 23/25 | Loss: 0.00138333
Iteration 24/25 | Loss: 0.00138501
Iteration 25/25 | Loss: 0.00138332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31518698
Iteration 2/25 | Loss: 0.00137207
Iteration 3/25 | Loss: 0.00118652
Iteration 4/25 | Loss: 0.00118651
Iteration 5/25 | Loss: 0.00118651
Iteration 6/25 | Loss: 0.00118651
Iteration 7/25 | Loss: 0.00118651
Iteration 8/25 | Loss: 0.00118651
Iteration 9/25 | Loss: 0.00118651
Iteration 10/25 | Loss: 0.00118651
Iteration 11/25 | Loss: 0.00118651
Iteration 12/25 | Loss: 0.00118651
Iteration 13/25 | Loss: 0.00118651
Iteration 14/25 | Loss: 0.00118651
Iteration 15/25 | Loss: 0.00118651
Iteration 16/25 | Loss: 0.00118651
Iteration 17/25 | Loss: 0.00118651
Iteration 18/25 | Loss: 0.00118651
Iteration 19/25 | Loss: 0.00118651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011865113629028201, 0.0011865113629028201, 0.0011865113629028201, 0.0011865113629028201, 0.0011865113629028201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011865113629028201

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118651
Iteration 2/1000 | Loss: 0.00007462
Iteration 3/1000 | Loss: 0.00025194
Iteration 4/1000 | Loss: 0.00088498
Iteration 5/1000 | Loss: 0.00101234
Iteration 6/1000 | Loss: 0.00076214
Iteration 7/1000 | Loss: 0.00315810
Iteration 8/1000 | Loss: 0.00150874
Iteration 9/1000 | Loss: 0.00073494
Iteration 10/1000 | Loss: 0.00006719
Iteration 11/1000 | Loss: 0.00007569
Iteration 12/1000 | Loss: 0.00012506
Iteration 13/1000 | Loss: 0.00042665
Iteration 14/1000 | Loss: 0.00014624
Iteration 15/1000 | Loss: 0.00007680
Iteration 16/1000 | Loss: 0.00016609
Iteration 17/1000 | Loss: 0.00014860
Iteration 18/1000 | Loss: 0.00003664
Iteration 19/1000 | Loss: 0.00036288
Iteration 20/1000 | Loss: 0.00018838
Iteration 21/1000 | Loss: 0.00003133
Iteration 22/1000 | Loss: 0.00002831
Iteration 23/1000 | Loss: 0.00002687
Iteration 24/1000 | Loss: 0.00009273
Iteration 25/1000 | Loss: 0.00002552
Iteration 26/1000 | Loss: 0.00002460
Iteration 27/1000 | Loss: 0.00002374
Iteration 28/1000 | Loss: 0.00002327
Iteration 29/1000 | Loss: 0.00002290
Iteration 30/1000 | Loss: 0.00011641
Iteration 31/1000 | Loss: 0.00002528
Iteration 32/1000 | Loss: 0.00002319
Iteration 33/1000 | Loss: 0.00002235
Iteration 34/1000 | Loss: 0.00002153
Iteration 35/1000 | Loss: 0.00002102
Iteration 36/1000 | Loss: 0.00002077
Iteration 37/1000 | Loss: 0.00002066
Iteration 38/1000 | Loss: 0.00002048
Iteration 39/1000 | Loss: 0.00002034
Iteration 40/1000 | Loss: 0.00002023
Iteration 41/1000 | Loss: 0.00003524
Iteration 42/1000 | Loss: 0.00002176
Iteration 43/1000 | Loss: 0.00002068
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001971
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00001957
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001955
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001943
Iteration 53/1000 | Loss: 0.00001942
Iteration 54/1000 | Loss: 0.00001941
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001941
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001940
Iteration 59/1000 | Loss: 0.00001940
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001939
Iteration 66/1000 | Loss: 0.00001939
Iteration 67/1000 | Loss: 0.00001939
Iteration 68/1000 | Loss: 0.00001939
Iteration 69/1000 | Loss: 0.00001939
Iteration 70/1000 | Loss: 0.00001938
Iteration 71/1000 | Loss: 0.00001938
Iteration 72/1000 | Loss: 0.00001938
Iteration 73/1000 | Loss: 0.00001938
Iteration 74/1000 | Loss: 0.00001938
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001936
Iteration 86/1000 | Loss: 0.00001936
Iteration 87/1000 | Loss: 0.00001936
Iteration 88/1000 | Loss: 0.00001936
Iteration 89/1000 | Loss: 0.00001936
Iteration 90/1000 | Loss: 0.00001936
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.9361377781024203e-05, 1.9361377781024203e-05, 1.9361377781024203e-05, 1.9361377781024203e-05, 1.9361377781024203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9361377781024203e-05

Optimization complete. Final v2v error: 3.647134780883789 mm

Highest mean error: 4.899497985839844 mm for frame 20

Lowest mean error: 3.2004876136779785 mm for frame 133

Saving results

Total time: 113.89830994606018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084248
Iteration 2/25 | Loss: 0.00162433
Iteration 3/25 | Loss: 0.00140296
Iteration 4/25 | Loss: 0.00138983
Iteration 5/25 | Loss: 0.00138782
Iteration 6/25 | Loss: 0.00139361
Iteration 7/25 | Loss: 0.00138771
Iteration 8/25 | Loss: 0.00139256
Iteration 9/25 | Loss: 0.00138743
Iteration 10/25 | Loss: 0.00138704
Iteration 11/25 | Loss: 0.00138702
Iteration 12/25 | Loss: 0.00138702
Iteration 13/25 | Loss: 0.00138702
Iteration 14/25 | Loss: 0.00138702
Iteration 15/25 | Loss: 0.00138702
Iteration 16/25 | Loss: 0.00138702
Iteration 17/25 | Loss: 0.00138702
Iteration 18/25 | Loss: 0.00138702
Iteration 19/25 | Loss: 0.00138702
Iteration 20/25 | Loss: 0.00138702
Iteration 21/25 | Loss: 0.00138702
Iteration 22/25 | Loss: 0.00138702
Iteration 23/25 | Loss: 0.00138702
Iteration 24/25 | Loss: 0.00138702
Iteration 25/25 | Loss: 0.00138701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89700973
Iteration 2/25 | Loss: 0.00070704
Iteration 3/25 | Loss: 0.00070704
Iteration 4/25 | Loss: 0.00070704
Iteration 5/25 | Loss: 0.00070704
Iteration 6/25 | Loss: 0.00070704
Iteration 7/25 | Loss: 0.00070704
Iteration 8/25 | Loss: 0.00070704
Iteration 9/25 | Loss: 0.00070704
Iteration 10/25 | Loss: 0.00070704
Iteration 11/25 | Loss: 0.00070704
Iteration 12/25 | Loss: 0.00070704
Iteration 13/25 | Loss: 0.00070704
Iteration 14/25 | Loss: 0.00070704
Iteration 15/25 | Loss: 0.00070704
Iteration 16/25 | Loss: 0.00070704
Iteration 17/25 | Loss: 0.00070704
Iteration 18/25 | Loss: 0.00070704
Iteration 19/25 | Loss: 0.00070704
Iteration 20/25 | Loss: 0.00070704
Iteration 21/25 | Loss: 0.00070704
Iteration 22/25 | Loss: 0.00070704
Iteration 23/25 | Loss: 0.00070704
Iteration 24/25 | Loss: 0.00070704
Iteration 25/25 | Loss: 0.00070704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070704
Iteration 2/1000 | Loss: 0.00004180
Iteration 3/1000 | Loss: 0.00003021
Iteration 4/1000 | Loss: 0.00002634
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002453
Iteration 7/1000 | Loss: 0.00002397
Iteration 8/1000 | Loss: 0.00002356
Iteration 9/1000 | Loss: 0.00002321
Iteration 10/1000 | Loss: 0.00002286
Iteration 11/1000 | Loss: 0.00002265
Iteration 12/1000 | Loss: 0.00002253
Iteration 13/1000 | Loss: 0.00002237
Iteration 14/1000 | Loss: 0.00002224
Iteration 15/1000 | Loss: 0.00002214
Iteration 16/1000 | Loss: 0.00002213
Iteration 17/1000 | Loss: 0.00002213
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002212
Iteration 20/1000 | Loss: 0.00002211
Iteration 21/1000 | Loss: 0.00002211
Iteration 22/1000 | Loss: 0.00002211
Iteration 23/1000 | Loss: 0.00002211
Iteration 24/1000 | Loss: 0.00002210
Iteration 25/1000 | Loss: 0.00002209
Iteration 26/1000 | Loss: 0.00002209
Iteration 27/1000 | Loss: 0.00002209
Iteration 28/1000 | Loss: 0.00002209
Iteration 29/1000 | Loss: 0.00002209
Iteration 30/1000 | Loss: 0.00002209
Iteration 31/1000 | Loss: 0.00002208
Iteration 32/1000 | Loss: 0.00002208
Iteration 33/1000 | Loss: 0.00002208
Iteration 34/1000 | Loss: 0.00002208
Iteration 35/1000 | Loss: 0.00002208
Iteration 36/1000 | Loss: 0.00002208
Iteration 37/1000 | Loss: 0.00002208
Iteration 38/1000 | Loss: 0.00002207
Iteration 39/1000 | Loss: 0.00002207
Iteration 40/1000 | Loss: 0.00002207
Iteration 41/1000 | Loss: 0.00002207
Iteration 42/1000 | Loss: 0.00002206
Iteration 43/1000 | Loss: 0.00002206
Iteration 44/1000 | Loss: 0.00002206
Iteration 45/1000 | Loss: 0.00002206
Iteration 46/1000 | Loss: 0.00002206
Iteration 47/1000 | Loss: 0.00002205
Iteration 48/1000 | Loss: 0.00002205
Iteration 49/1000 | Loss: 0.00002205
Iteration 50/1000 | Loss: 0.00002204
Iteration 51/1000 | Loss: 0.00002204
Iteration 52/1000 | Loss: 0.00002204
Iteration 53/1000 | Loss: 0.00002203
Iteration 54/1000 | Loss: 0.00002203
Iteration 55/1000 | Loss: 0.00002203
Iteration 56/1000 | Loss: 0.00002202
Iteration 57/1000 | Loss: 0.00002202
Iteration 58/1000 | Loss: 0.00002202
Iteration 59/1000 | Loss: 0.00002202
Iteration 60/1000 | Loss: 0.00002201
Iteration 61/1000 | Loss: 0.00002200
Iteration 62/1000 | Loss: 0.00002200
Iteration 63/1000 | Loss: 0.00002200
Iteration 64/1000 | Loss: 0.00002200
Iteration 65/1000 | Loss: 0.00002200
Iteration 66/1000 | Loss: 0.00002200
Iteration 67/1000 | Loss: 0.00002200
Iteration 68/1000 | Loss: 0.00002200
Iteration 69/1000 | Loss: 0.00002200
Iteration 70/1000 | Loss: 0.00002200
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00002199
Iteration 74/1000 | Loss: 0.00002199
Iteration 75/1000 | Loss: 0.00002199
Iteration 76/1000 | Loss: 0.00002199
Iteration 77/1000 | Loss: 0.00002198
Iteration 78/1000 | Loss: 0.00002198
Iteration 79/1000 | Loss: 0.00002198
Iteration 80/1000 | Loss: 0.00002198
Iteration 81/1000 | Loss: 0.00002198
Iteration 82/1000 | Loss: 0.00002198
Iteration 83/1000 | Loss: 0.00002197
Iteration 84/1000 | Loss: 0.00002197
Iteration 85/1000 | Loss: 0.00002197
Iteration 86/1000 | Loss: 0.00002196
Iteration 87/1000 | Loss: 0.00002196
Iteration 88/1000 | Loss: 0.00002196
Iteration 89/1000 | Loss: 0.00002195
Iteration 90/1000 | Loss: 0.00002195
Iteration 91/1000 | Loss: 0.00002195
Iteration 92/1000 | Loss: 0.00002195
Iteration 93/1000 | Loss: 0.00002195
Iteration 94/1000 | Loss: 0.00002195
Iteration 95/1000 | Loss: 0.00002195
Iteration 96/1000 | Loss: 0.00002195
Iteration 97/1000 | Loss: 0.00002195
Iteration 98/1000 | Loss: 0.00002194
Iteration 99/1000 | Loss: 0.00002194
Iteration 100/1000 | Loss: 0.00002194
Iteration 101/1000 | Loss: 0.00002194
Iteration 102/1000 | Loss: 0.00002194
Iteration 103/1000 | Loss: 0.00002194
Iteration 104/1000 | Loss: 0.00002194
Iteration 105/1000 | Loss: 0.00002194
Iteration 106/1000 | Loss: 0.00002194
Iteration 107/1000 | Loss: 0.00002194
Iteration 108/1000 | Loss: 0.00002194
Iteration 109/1000 | Loss: 0.00002194
Iteration 110/1000 | Loss: 0.00002194
Iteration 111/1000 | Loss: 0.00002194
Iteration 112/1000 | Loss: 0.00002194
Iteration 113/1000 | Loss: 0.00002192
Iteration 114/1000 | Loss: 0.00002192
Iteration 115/1000 | Loss: 0.00002192
Iteration 116/1000 | Loss: 0.00002192
Iteration 117/1000 | Loss: 0.00002192
Iteration 118/1000 | Loss: 0.00002192
Iteration 119/1000 | Loss: 0.00002192
Iteration 120/1000 | Loss: 0.00002192
Iteration 121/1000 | Loss: 0.00002192
Iteration 122/1000 | Loss: 0.00002192
Iteration 123/1000 | Loss: 0.00002192
Iteration 124/1000 | Loss: 0.00002192
Iteration 125/1000 | Loss: 0.00002192
Iteration 126/1000 | Loss: 0.00002191
Iteration 127/1000 | Loss: 0.00002191
Iteration 128/1000 | Loss: 0.00002191
Iteration 129/1000 | Loss: 0.00002191
Iteration 130/1000 | Loss: 0.00002191
Iteration 131/1000 | Loss: 0.00002191
Iteration 132/1000 | Loss: 0.00002191
Iteration 133/1000 | Loss: 0.00002191
Iteration 134/1000 | Loss: 0.00002191
Iteration 135/1000 | Loss: 0.00002190
Iteration 136/1000 | Loss: 0.00002190
Iteration 137/1000 | Loss: 0.00002190
Iteration 138/1000 | Loss: 0.00002190
Iteration 139/1000 | Loss: 0.00002190
Iteration 140/1000 | Loss: 0.00002190
Iteration 141/1000 | Loss: 0.00002190
Iteration 142/1000 | Loss: 0.00002190
Iteration 143/1000 | Loss: 0.00002190
Iteration 144/1000 | Loss: 0.00002190
Iteration 145/1000 | Loss: 0.00002190
Iteration 146/1000 | Loss: 0.00002190
Iteration 147/1000 | Loss: 0.00002190
Iteration 148/1000 | Loss: 0.00002190
Iteration 149/1000 | Loss: 0.00002189
Iteration 150/1000 | Loss: 0.00002189
Iteration 151/1000 | Loss: 0.00002189
Iteration 152/1000 | Loss: 0.00002189
Iteration 153/1000 | Loss: 0.00002189
Iteration 154/1000 | Loss: 0.00002189
Iteration 155/1000 | Loss: 0.00002189
Iteration 156/1000 | Loss: 0.00002188
Iteration 157/1000 | Loss: 0.00002188
Iteration 158/1000 | Loss: 0.00002188
Iteration 159/1000 | Loss: 0.00002188
Iteration 160/1000 | Loss: 0.00002188
Iteration 161/1000 | Loss: 0.00002188
Iteration 162/1000 | Loss: 0.00002188
Iteration 163/1000 | Loss: 0.00002188
Iteration 164/1000 | Loss: 0.00002188
Iteration 165/1000 | Loss: 0.00002188
Iteration 166/1000 | Loss: 0.00002188
Iteration 167/1000 | Loss: 0.00002188
Iteration 168/1000 | Loss: 0.00002188
Iteration 169/1000 | Loss: 0.00002188
Iteration 170/1000 | Loss: 0.00002188
Iteration 171/1000 | Loss: 0.00002188
Iteration 172/1000 | Loss: 0.00002188
Iteration 173/1000 | Loss: 0.00002188
Iteration 174/1000 | Loss: 0.00002188
Iteration 175/1000 | Loss: 0.00002188
Iteration 176/1000 | Loss: 0.00002188
Iteration 177/1000 | Loss: 0.00002188
Iteration 178/1000 | Loss: 0.00002188
Iteration 179/1000 | Loss: 0.00002188
Iteration 180/1000 | Loss: 0.00002188
Iteration 181/1000 | Loss: 0.00002188
Iteration 182/1000 | Loss: 0.00002188
Iteration 183/1000 | Loss: 0.00002188
Iteration 184/1000 | Loss: 0.00002188
Iteration 185/1000 | Loss: 0.00002188
Iteration 186/1000 | Loss: 0.00002188
Iteration 187/1000 | Loss: 0.00002188
Iteration 188/1000 | Loss: 0.00002188
Iteration 189/1000 | Loss: 0.00002188
Iteration 190/1000 | Loss: 0.00002188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.1883752197027206e-05, 2.1883752197027206e-05, 2.1883752197027206e-05, 2.1883752197027206e-05, 2.1883752197027206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1883752197027206e-05

Optimization complete. Final v2v error: 3.7977843284606934 mm

Highest mean error: 4.262279987335205 mm for frame 104

Lowest mean error: 3.4663145542144775 mm for frame 33

Saving results

Total time: 45.71197152137756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423122
Iteration 2/25 | Loss: 0.00135068
Iteration 3/25 | Loss: 0.00127591
Iteration 4/25 | Loss: 0.00127301
Iteration 5/25 | Loss: 0.00127301
Iteration 6/25 | Loss: 0.00127301
Iteration 7/25 | Loss: 0.00127301
Iteration 8/25 | Loss: 0.00127301
Iteration 9/25 | Loss: 0.00127301
Iteration 10/25 | Loss: 0.00127301
Iteration 11/25 | Loss: 0.00127301
Iteration 12/25 | Loss: 0.00127301
Iteration 13/25 | Loss: 0.00127301
Iteration 14/25 | Loss: 0.00127301
Iteration 15/25 | Loss: 0.00127301
Iteration 16/25 | Loss: 0.00127301
Iteration 17/25 | Loss: 0.00127301
Iteration 18/25 | Loss: 0.00127301
Iteration 19/25 | Loss: 0.00127301
Iteration 20/25 | Loss: 0.00127301
Iteration 21/25 | Loss: 0.00127301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012730105081573129, 0.0012730105081573129, 0.0012730105081573129, 0.0012730105081573129, 0.0012730105081573129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012730105081573129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52488637
Iteration 2/25 | Loss: 0.00073636
Iteration 3/25 | Loss: 0.00073636
Iteration 4/25 | Loss: 0.00073636
Iteration 5/25 | Loss: 0.00073636
Iteration 6/25 | Loss: 0.00073636
Iteration 7/25 | Loss: 0.00073636
Iteration 8/25 | Loss: 0.00073636
Iteration 9/25 | Loss: 0.00073636
Iteration 10/25 | Loss: 0.00073636
Iteration 11/25 | Loss: 0.00073636
Iteration 12/25 | Loss: 0.00073636
Iteration 13/25 | Loss: 0.00073636
Iteration 14/25 | Loss: 0.00073636
Iteration 15/25 | Loss: 0.00073636
Iteration 16/25 | Loss: 0.00073636
Iteration 17/25 | Loss: 0.00073636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007363592158071697, 0.0007363592158071697, 0.0007363592158071697, 0.0007363592158071697, 0.0007363592158071697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007363592158071697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073636
Iteration 2/1000 | Loss: 0.00002717
Iteration 3/1000 | Loss: 0.00001770
Iteration 4/1000 | Loss: 0.00001626
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001493
Iteration 7/1000 | Loss: 0.00001466
Iteration 8/1000 | Loss: 0.00001465
Iteration 9/1000 | Loss: 0.00001439
Iteration 10/1000 | Loss: 0.00001401
Iteration 11/1000 | Loss: 0.00001395
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001353
Iteration 15/1000 | Loss: 0.00001348
Iteration 16/1000 | Loss: 0.00001346
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001329
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001326
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001320
Iteration 39/1000 | Loss: 0.00001320
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001307
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001304
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001303
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001301
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001297
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001280
Iteration 61/1000 | Loss: 0.00001280
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001278
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001277
Iteration 67/1000 | Loss: 0.00001276
Iteration 68/1000 | Loss: 0.00001275
Iteration 69/1000 | Loss: 0.00001275
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001274
Iteration 72/1000 | Loss: 0.00001272
Iteration 73/1000 | Loss: 0.00001272
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001270
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001269
Iteration 85/1000 | Loss: 0.00001269
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001268
Iteration 88/1000 | Loss: 0.00001268
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001267
Iteration 91/1000 | Loss: 0.00001267
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001267
Iteration 95/1000 | Loss: 0.00001266
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001265
Iteration 98/1000 | Loss: 0.00001265
Iteration 99/1000 | Loss: 0.00001265
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001264
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001263
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001260
Iteration 128/1000 | Loss: 0.00001260
Iteration 129/1000 | Loss: 0.00001260
Iteration 130/1000 | Loss: 0.00001260
Iteration 131/1000 | Loss: 0.00001260
Iteration 132/1000 | Loss: 0.00001260
Iteration 133/1000 | Loss: 0.00001260
Iteration 134/1000 | Loss: 0.00001259
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001258
Iteration 142/1000 | Loss: 0.00001258
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001258
Iteration 145/1000 | Loss: 0.00001258
Iteration 146/1000 | Loss: 0.00001258
Iteration 147/1000 | Loss: 0.00001257
Iteration 148/1000 | Loss: 0.00001257
Iteration 149/1000 | Loss: 0.00001257
Iteration 150/1000 | Loss: 0.00001257
Iteration 151/1000 | Loss: 0.00001257
Iteration 152/1000 | Loss: 0.00001257
Iteration 153/1000 | Loss: 0.00001257
Iteration 154/1000 | Loss: 0.00001257
Iteration 155/1000 | Loss: 0.00001256
Iteration 156/1000 | Loss: 0.00001256
Iteration 157/1000 | Loss: 0.00001256
Iteration 158/1000 | Loss: 0.00001256
Iteration 159/1000 | Loss: 0.00001256
Iteration 160/1000 | Loss: 0.00001256
Iteration 161/1000 | Loss: 0.00001255
Iteration 162/1000 | Loss: 0.00001255
Iteration 163/1000 | Loss: 0.00001255
Iteration 164/1000 | Loss: 0.00001255
Iteration 165/1000 | Loss: 0.00001255
Iteration 166/1000 | Loss: 0.00001255
Iteration 167/1000 | Loss: 0.00001255
Iteration 168/1000 | Loss: 0.00001255
Iteration 169/1000 | Loss: 0.00001254
Iteration 170/1000 | Loss: 0.00001254
Iteration 171/1000 | Loss: 0.00001254
Iteration 172/1000 | Loss: 0.00001253
Iteration 173/1000 | Loss: 0.00001253
Iteration 174/1000 | Loss: 0.00001253
Iteration 175/1000 | Loss: 0.00001252
Iteration 176/1000 | Loss: 0.00001252
Iteration 177/1000 | Loss: 0.00001252
Iteration 178/1000 | Loss: 0.00001252
Iteration 179/1000 | Loss: 0.00001252
Iteration 180/1000 | Loss: 0.00001251
Iteration 181/1000 | Loss: 0.00001251
Iteration 182/1000 | Loss: 0.00001251
Iteration 183/1000 | Loss: 0.00001251
Iteration 184/1000 | Loss: 0.00001251
Iteration 185/1000 | Loss: 0.00001251
Iteration 186/1000 | Loss: 0.00001250
Iteration 187/1000 | Loss: 0.00001250
Iteration 188/1000 | Loss: 0.00001250
Iteration 189/1000 | Loss: 0.00001250
Iteration 190/1000 | Loss: 0.00001250
Iteration 191/1000 | Loss: 0.00001250
Iteration 192/1000 | Loss: 0.00001250
Iteration 193/1000 | Loss: 0.00001250
Iteration 194/1000 | Loss: 0.00001249
Iteration 195/1000 | Loss: 0.00001249
Iteration 196/1000 | Loss: 0.00001249
Iteration 197/1000 | Loss: 0.00001249
Iteration 198/1000 | Loss: 0.00001249
Iteration 199/1000 | Loss: 0.00001249
Iteration 200/1000 | Loss: 0.00001249
Iteration 201/1000 | Loss: 0.00001249
Iteration 202/1000 | Loss: 0.00001249
Iteration 203/1000 | Loss: 0.00001249
Iteration 204/1000 | Loss: 0.00001249
Iteration 205/1000 | Loss: 0.00001249
Iteration 206/1000 | Loss: 0.00001249
Iteration 207/1000 | Loss: 0.00001249
Iteration 208/1000 | Loss: 0.00001249
Iteration 209/1000 | Loss: 0.00001249
Iteration 210/1000 | Loss: 0.00001249
Iteration 211/1000 | Loss: 0.00001249
Iteration 212/1000 | Loss: 0.00001249
Iteration 213/1000 | Loss: 0.00001249
Iteration 214/1000 | Loss: 0.00001249
Iteration 215/1000 | Loss: 0.00001249
Iteration 216/1000 | Loss: 0.00001249
Iteration 217/1000 | Loss: 0.00001249
Iteration 218/1000 | Loss: 0.00001249
Iteration 219/1000 | Loss: 0.00001249
Iteration 220/1000 | Loss: 0.00001249
Iteration 221/1000 | Loss: 0.00001249
Iteration 222/1000 | Loss: 0.00001249
Iteration 223/1000 | Loss: 0.00001249
Iteration 224/1000 | Loss: 0.00001249
Iteration 225/1000 | Loss: 0.00001249
Iteration 226/1000 | Loss: 0.00001249
Iteration 227/1000 | Loss: 0.00001249
Iteration 228/1000 | Loss: 0.00001249
Iteration 229/1000 | Loss: 0.00001249
Iteration 230/1000 | Loss: 0.00001249
Iteration 231/1000 | Loss: 0.00001249
Iteration 232/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.249226897925837e-05, 1.249226897925837e-05, 1.249226897925837e-05, 1.249226897925837e-05, 1.249226897925837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.249226897925837e-05

Optimization complete. Final v2v error: 3.014932632446289 mm

Highest mean error: 3.22542667388916 mm for frame 163

Lowest mean error: 2.9021530151367188 mm for frame 200

Saving results

Total time: 48.16444730758667
