Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=257, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14392-14447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512390
Iteration 2/25 | Loss: 0.00159237
Iteration 3/25 | Loss: 0.00139869
Iteration 4/25 | Loss: 0.00137861
Iteration 5/25 | Loss: 0.00137219
Iteration 6/25 | Loss: 0.00137013
Iteration 7/25 | Loss: 0.00136953
Iteration 8/25 | Loss: 0.00136952
Iteration 9/25 | Loss: 0.00136952
Iteration 10/25 | Loss: 0.00136952
Iteration 11/25 | Loss: 0.00136952
Iteration 12/25 | Loss: 0.00136952
Iteration 13/25 | Loss: 0.00136952
Iteration 14/25 | Loss: 0.00136952
Iteration 15/25 | Loss: 0.00136952
Iteration 16/25 | Loss: 0.00136952
Iteration 17/25 | Loss: 0.00136952
Iteration 18/25 | Loss: 0.00136952
Iteration 19/25 | Loss: 0.00136952
Iteration 20/25 | Loss: 0.00136952
Iteration 21/25 | Loss: 0.00136952
Iteration 22/25 | Loss: 0.00136952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013695203233510256, 0.0013695203233510256, 0.0013695203233510256, 0.0013695203233510256, 0.0013695203233510256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013695203233510256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47158921
Iteration 2/25 | Loss: 0.00101330
Iteration 3/25 | Loss: 0.00101330
Iteration 4/25 | Loss: 0.00101330
Iteration 5/25 | Loss: 0.00101330
Iteration 6/25 | Loss: 0.00101330
Iteration 7/25 | Loss: 0.00101330
Iteration 8/25 | Loss: 0.00101329
Iteration 9/25 | Loss: 0.00101329
Iteration 10/25 | Loss: 0.00101329
Iteration 11/25 | Loss: 0.00101329
Iteration 12/25 | Loss: 0.00101329
Iteration 13/25 | Loss: 0.00101329
Iteration 14/25 | Loss: 0.00101329
Iteration 15/25 | Loss: 0.00101329
Iteration 16/25 | Loss: 0.00101329
Iteration 17/25 | Loss: 0.00101329
Iteration 18/25 | Loss: 0.00101329
Iteration 19/25 | Loss: 0.00101329
Iteration 20/25 | Loss: 0.00101329
Iteration 21/25 | Loss: 0.00101329
Iteration 22/25 | Loss: 0.00101329
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010132943280041218, 0.0010132943280041218, 0.0010132943280041218, 0.0010132943280041218, 0.0010132943280041218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010132943280041218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101329
Iteration 2/1000 | Loss: 0.00005556
Iteration 3/1000 | Loss: 0.00003909
Iteration 4/1000 | Loss: 0.00003513
Iteration 5/1000 | Loss: 0.00003248
Iteration 6/1000 | Loss: 0.00003110
Iteration 7/1000 | Loss: 0.00003023
Iteration 8/1000 | Loss: 0.00002978
Iteration 9/1000 | Loss: 0.00002947
Iteration 10/1000 | Loss: 0.00002923
Iteration 11/1000 | Loss: 0.00002894
Iteration 12/1000 | Loss: 0.00002874
Iteration 13/1000 | Loss: 0.00002864
Iteration 14/1000 | Loss: 0.00002852
Iteration 15/1000 | Loss: 0.00002851
Iteration 16/1000 | Loss: 0.00002851
Iteration 17/1000 | Loss: 0.00002851
Iteration 18/1000 | Loss: 0.00002851
Iteration 19/1000 | Loss: 0.00002848
Iteration 20/1000 | Loss: 0.00002846
Iteration 21/1000 | Loss: 0.00002845
Iteration 22/1000 | Loss: 0.00002845
Iteration 23/1000 | Loss: 0.00002844
Iteration 24/1000 | Loss: 0.00002844
Iteration 25/1000 | Loss: 0.00002844
Iteration 26/1000 | Loss: 0.00002844
Iteration 27/1000 | Loss: 0.00002844
Iteration 28/1000 | Loss: 0.00002842
Iteration 29/1000 | Loss: 0.00002842
Iteration 30/1000 | Loss: 0.00002842
Iteration 31/1000 | Loss: 0.00002842
Iteration 32/1000 | Loss: 0.00002842
Iteration 33/1000 | Loss: 0.00002842
Iteration 34/1000 | Loss: 0.00002842
Iteration 35/1000 | Loss: 0.00002842
Iteration 36/1000 | Loss: 0.00002841
Iteration 37/1000 | Loss: 0.00002840
Iteration 38/1000 | Loss: 0.00002840
Iteration 39/1000 | Loss: 0.00002840
Iteration 40/1000 | Loss: 0.00002840
Iteration 41/1000 | Loss: 0.00002840
Iteration 42/1000 | Loss: 0.00002840
Iteration 43/1000 | Loss: 0.00002840
Iteration 44/1000 | Loss: 0.00002840
Iteration 45/1000 | Loss: 0.00002840
Iteration 46/1000 | Loss: 0.00002840
Iteration 47/1000 | Loss: 0.00002839
Iteration 48/1000 | Loss: 0.00002839
Iteration 49/1000 | Loss: 0.00002839
Iteration 50/1000 | Loss: 0.00002839
Iteration 51/1000 | Loss: 0.00002839
Iteration 52/1000 | Loss: 0.00002839
Iteration 53/1000 | Loss: 0.00002838
Iteration 54/1000 | Loss: 0.00002838
Iteration 55/1000 | Loss: 0.00002838
Iteration 56/1000 | Loss: 0.00002838
Iteration 57/1000 | Loss: 0.00002838
Iteration 58/1000 | Loss: 0.00002838
Iteration 59/1000 | Loss: 0.00002837
Iteration 60/1000 | Loss: 0.00002837
Iteration 61/1000 | Loss: 0.00002837
Iteration 62/1000 | Loss: 0.00002837
Iteration 63/1000 | Loss: 0.00002837
Iteration 64/1000 | Loss: 0.00002837
Iteration 65/1000 | Loss: 0.00002837
Iteration 66/1000 | Loss: 0.00002837
Iteration 67/1000 | Loss: 0.00002837
Iteration 68/1000 | Loss: 0.00002836
Iteration 69/1000 | Loss: 0.00002836
Iteration 70/1000 | Loss: 0.00002836
Iteration 71/1000 | Loss: 0.00002836
Iteration 72/1000 | Loss: 0.00002836
Iteration 73/1000 | Loss: 0.00002836
Iteration 74/1000 | Loss: 0.00002836
Iteration 75/1000 | Loss: 0.00002836
Iteration 76/1000 | Loss: 0.00002836
Iteration 77/1000 | Loss: 0.00002836
Iteration 78/1000 | Loss: 0.00002836
Iteration 79/1000 | Loss: 0.00002836
Iteration 80/1000 | Loss: 0.00002835
Iteration 81/1000 | Loss: 0.00002835
Iteration 82/1000 | Loss: 0.00002835
Iteration 83/1000 | Loss: 0.00002835
Iteration 84/1000 | Loss: 0.00002834
Iteration 85/1000 | Loss: 0.00002834
Iteration 86/1000 | Loss: 0.00002834
Iteration 87/1000 | Loss: 0.00002834
Iteration 88/1000 | Loss: 0.00002834
Iteration 89/1000 | Loss: 0.00002834
Iteration 90/1000 | Loss: 0.00002834
Iteration 91/1000 | Loss: 0.00002834
Iteration 92/1000 | Loss: 0.00002834
Iteration 93/1000 | Loss: 0.00002834
Iteration 94/1000 | Loss: 0.00002834
Iteration 95/1000 | Loss: 0.00002834
Iteration 96/1000 | Loss: 0.00002834
Iteration 97/1000 | Loss: 0.00002834
Iteration 98/1000 | Loss: 0.00002834
Iteration 99/1000 | Loss: 0.00002834
Iteration 100/1000 | Loss: 0.00002834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.834055521816481e-05, 2.834055521816481e-05, 2.834055521816481e-05, 2.834055521816481e-05, 2.834055521816481e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.834055521816481e-05

Optimization complete. Final v2v error: 4.67390775680542 mm

Highest mean error: 5.668410778045654 mm for frame 52

Lowest mean error: 4.126235485076904 mm for frame 2

Saving results

Total time: 37.58352017402649
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440551
Iteration 2/25 | Loss: 0.00152861
Iteration 3/25 | Loss: 0.00139999
Iteration 4/25 | Loss: 0.00138645
Iteration 5/25 | Loss: 0.00138093
Iteration 6/25 | Loss: 0.00137917
Iteration 7/25 | Loss: 0.00137890
Iteration 8/25 | Loss: 0.00137890
Iteration 9/25 | Loss: 0.00137890
Iteration 10/25 | Loss: 0.00137890
Iteration 11/25 | Loss: 0.00137890
Iteration 12/25 | Loss: 0.00137890
Iteration 13/25 | Loss: 0.00137890
Iteration 14/25 | Loss: 0.00137890
Iteration 15/25 | Loss: 0.00137890
Iteration 16/25 | Loss: 0.00137890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013788993237540126, 0.0013788993237540126, 0.0013788993237540126, 0.0013788993237540126, 0.0013788993237540126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013788993237540126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39151275
Iteration 2/25 | Loss: 0.00115442
Iteration 3/25 | Loss: 0.00115442
Iteration 4/25 | Loss: 0.00115442
Iteration 5/25 | Loss: 0.00115442
Iteration 6/25 | Loss: 0.00115442
Iteration 7/25 | Loss: 0.00115442
Iteration 8/25 | Loss: 0.00115442
Iteration 9/25 | Loss: 0.00115442
Iteration 10/25 | Loss: 0.00115442
Iteration 11/25 | Loss: 0.00115442
Iteration 12/25 | Loss: 0.00115442
Iteration 13/25 | Loss: 0.00115442
Iteration 14/25 | Loss: 0.00115442
Iteration 15/25 | Loss: 0.00115442
Iteration 16/25 | Loss: 0.00115442
Iteration 17/25 | Loss: 0.00115442
Iteration 18/25 | Loss: 0.00115442
Iteration 19/25 | Loss: 0.00115442
Iteration 20/25 | Loss: 0.00115442
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011544207809492946, 0.0011544207809492946, 0.0011544207809492946, 0.0011544207809492946, 0.0011544207809492946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011544207809492946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115442
Iteration 2/1000 | Loss: 0.00006135
Iteration 3/1000 | Loss: 0.00004673
Iteration 4/1000 | Loss: 0.00004040
Iteration 5/1000 | Loss: 0.00003637
Iteration 6/1000 | Loss: 0.00003471
Iteration 7/1000 | Loss: 0.00003390
Iteration 8/1000 | Loss: 0.00003331
Iteration 9/1000 | Loss: 0.00003302
Iteration 10/1000 | Loss: 0.00003277
Iteration 11/1000 | Loss: 0.00003268
Iteration 12/1000 | Loss: 0.00003243
Iteration 13/1000 | Loss: 0.00003243
Iteration 14/1000 | Loss: 0.00003240
Iteration 15/1000 | Loss: 0.00003225
Iteration 16/1000 | Loss: 0.00003224
Iteration 17/1000 | Loss: 0.00003224
Iteration 18/1000 | Loss: 0.00003222
Iteration 19/1000 | Loss: 0.00003222
Iteration 20/1000 | Loss: 0.00003222
Iteration 21/1000 | Loss: 0.00003221
Iteration 22/1000 | Loss: 0.00003219
Iteration 23/1000 | Loss: 0.00003218
Iteration 24/1000 | Loss: 0.00003218
Iteration 25/1000 | Loss: 0.00003217
Iteration 26/1000 | Loss: 0.00003215
Iteration 27/1000 | Loss: 0.00003215
Iteration 28/1000 | Loss: 0.00003214
Iteration 29/1000 | Loss: 0.00003209
Iteration 30/1000 | Loss: 0.00003209
Iteration 31/1000 | Loss: 0.00003207
Iteration 32/1000 | Loss: 0.00003207
Iteration 33/1000 | Loss: 0.00003206
Iteration 34/1000 | Loss: 0.00003206
Iteration 35/1000 | Loss: 0.00003206
Iteration 36/1000 | Loss: 0.00003205
Iteration 37/1000 | Loss: 0.00003204
Iteration 38/1000 | Loss: 0.00003204
Iteration 39/1000 | Loss: 0.00003203
Iteration 40/1000 | Loss: 0.00003203
Iteration 41/1000 | Loss: 0.00003202
Iteration 42/1000 | Loss: 0.00003201
Iteration 43/1000 | Loss: 0.00003201
Iteration 44/1000 | Loss: 0.00003199
Iteration 45/1000 | Loss: 0.00003199
Iteration 46/1000 | Loss: 0.00003198
Iteration 47/1000 | Loss: 0.00003198
Iteration 48/1000 | Loss: 0.00003195
Iteration 49/1000 | Loss: 0.00003195
Iteration 50/1000 | Loss: 0.00003195
Iteration 51/1000 | Loss: 0.00003195
Iteration 52/1000 | Loss: 0.00003195
Iteration 53/1000 | Loss: 0.00003195
Iteration 54/1000 | Loss: 0.00003195
Iteration 55/1000 | Loss: 0.00003195
Iteration 56/1000 | Loss: 0.00003194
Iteration 57/1000 | Loss: 0.00003193
Iteration 58/1000 | Loss: 0.00003192
Iteration 59/1000 | Loss: 0.00003192
Iteration 60/1000 | Loss: 0.00003192
Iteration 61/1000 | Loss: 0.00003191
Iteration 62/1000 | Loss: 0.00003191
Iteration 63/1000 | Loss: 0.00003191
Iteration 64/1000 | Loss: 0.00003191
Iteration 65/1000 | Loss: 0.00003190
Iteration 66/1000 | Loss: 0.00003190
Iteration 67/1000 | Loss: 0.00003190
Iteration 68/1000 | Loss: 0.00003190
Iteration 69/1000 | Loss: 0.00003189
Iteration 70/1000 | Loss: 0.00003189
Iteration 71/1000 | Loss: 0.00003188
Iteration 72/1000 | Loss: 0.00003188
Iteration 73/1000 | Loss: 0.00003188
Iteration 74/1000 | Loss: 0.00003188
Iteration 75/1000 | Loss: 0.00003187
Iteration 76/1000 | Loss: 0.00003187
Iteration 77/1000 | Loss: 0.00003187
Iteration 78/1000 | Loss: 0.00003187
Iteration 79/1000 | Loss: 0.00003187
Iteration 80/1000 | Loss: 0.00003186
Iteration 81/1000 | Loss: 0.00003186
Iteration 82/1000 | Loss: 0.00003186
Iteration 83/1000 | Loss: 0.00003186
Iteration 84/1000 | Loss: 0.00003186
Iteration 85/1000 | Loss: 0.00003186
Iteration 86/1000 | Loss: 0.00003185
Iteration 87/1000 | Loss: 0.00003185
Iteration 88/1000 | Loss: 0.00003185
Iteration 89/1000 | Loss: 0.00003184
Iteration 90/1000 | Loss: 0.00003184
Iteration 91/1000 | Loss: 0.00003184
Iteration 92/1000 | Loss: 0.00003183
Iteration 93/1000 | Loss: 0.00003183
Iteration 94/1000 | Loss: 0.00003183
Iteration 95/1000 | Loss: 0.00003183
Iteration 96/1000 | Loss: 0.00003183
Iteration 97/1000 | Loss: 0.00003183
Iteration 98/1000 | Loss: 0.00003183
Iteration 99/1000 | Loss: 0.00003183
Iteration 100/1000 | Loss: 0.00003183
Iteration 101/1000 | Loss: 0.00003183
Iteration 102/1000 | Loss: 0.00003183
Iteration 103/1000 | Loss: 0.00003183
Iteration 104/1000 | Loss: 0.00003183
Iteration 105/1000 | Loss: 0.00003183
Iteration 106/1000 | Loss: 0.00003183
Iteration 107/1000 | Loss: 0.00003182
Iteration 108/1000 | Loss: 0.00003182
Iteration 109/1000 | Loss: 0.00003182
Iteration 110/1000 | Loss: 0.00003182
Iteration 111/1000 | Loss: 0.00003182
Iteration 112/1000 | Loss: 0.00003182
Iteration 113/1000 | Loss: 0.00003182
Iteration 114/1000 | Loss: 0.00003182
Iteration 115/1000 | Loss: 0.00003181
Iteration 116/1000 | Loss: 0.00003181
Iteration 117/1000 | Loss: 0.00003181
Iteration 118/1000 | Loss: 0.00003181
Iteration 119/1000 | Loss: 0.00003181
Iteration 120/1000 | Loss: 0.00003181
Iteration 121/1000 | Loss: 0.00003181
Iteration 122/1000 | Loss: 0.00003181
Iteration 123/1000 | Loss: 0.00003181
Iteration 124/1000 | Loss: 0.00003181
Iteration 125/1000 | Loss: 0.00003181
Iteration 126/1000 | Loss: 0.00003181
Iteration 127/1000 | Loss: 0.00003180
Iteration 128/1000 | Loss: 0.00003180
Iteration 129/1000 | Loss: 0.00003180
Iteration 130/1000 | Loss: 0.00003180
Iteration 131/1000 | Loss: 0.00003180
Iteration 132/1000 | Loss: 0.00003180
Iteration 133/1000 | Loss: 0.00003180
Iteration 134/1000 | Loss: 0.00003180
Iteration 135/1000 | Loss: 0.00003180
Iteration 136/1000 | Loss: 0.00003180
Iteration 137/1000 | Loss: 0.00003180
Iteration 138/1000 | Loss: 0.00003180
Iteration 139/1000 | Loss: 0.00003180
Iteration 140/1000 | Loss: 0.00003180
Iteration 141/1000 | Loss: 0.00003180
Iteration 142/1000 | Loss: 0.00003180
Iteration 143/1000 | Loss: 0.00003180
Iteration 144/1000 | Loss: 0.00003180
Iteration 145/1000 | Loss: 0.00003180
Iteration 146/1000 | Loss: 0.00003180
Iteration 147/1000 | Loss: 0.00003180
Iteration 148/1000 | Loss: 0.00003180
Iteration 149/1000 | Loss: 0.00003180
Iteration 150/1000 | Loss: 0.00003180
Iteration 151/1000 | Loss: 0.00003180
Iteration 152/1000 | Loss: 0.00003180
Iteration 153/1000 | Loss: 0.00003180
Iteration 154/1000 | Loss: 0.00003180
Iteration 155/1000 | Loss: 0.00003180
Iteration 156/1000 | Loss: 0.00003180
Iteration 157/1000 | Loss: 0.00003180
Iteration 158/1000 | Loss: 0.00003180
Iteration 159/1000 | Loss: 0.00003180
Iteration 160/1000 | Loss: 0.00003180
Iteration 161/1000 | Loss: 0.00003180
Iteration 162/1000 | Loss: 0.00003180
Iteration 163/1000 | Loss: 0.00003180
Iteration 164/1000 | Loss: 0.00003180
Iteration 165/1000 | Loss: 0.00003180
Iteration 166/1000 | Loss: 0.00003180
Iteration 167/1000 | Loss: 0.00003180
Iteration 168/1000 | Loss: 0.00003180
Iteration 169/1000 | Loss: 0.00003180
Iteration 170/1000 | Loss: 0.00003180
Iteration 171/1000 | Loss: 0.00003180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.18000020342879e-05, 3.18000020342879e-05, 3.18000020342879e-05, 3.18000020342879e-05, 3.18000020342879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.18000020342879e-05

Optimization complete. Final v2v error: 5.002862930297852 mm

Highest mean error: 5.4130072593688965 mm for frame 15

Lowest mean error: 4.851354598999023 mm for frame 31

Saving results

Total time: 39.106239795684814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469099
Iteration 2/25 | Loss: 0.00144482
Iteration 3/25 | Loss: 0.00138350
Iteration 4/25 | Loss: 0.00137570
Iteration 5/25 | Loss: 0.00137222
Iteration 6/25 | Loss: 0.00137110
Iteration 7/25 | Loss: 0.00137076
Iteration 8/25 | Loss: 0.00137076
Iteration 9/25 | Loss: 0.00137076
Iteration 10/25 | Loss: 0.00137076
Iteration 11/25 | Loss: 0.00137076
Iteration 12/25 | Loss: 0.00137076
Iteration 13/25 | Loss: 0.00137076
Iteration 14/25 | Loss: 0.00137076
Iteration 15/25 | Loss: 0.00137076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013707558391615748, 0.0013707558391615748, 0.0013707558391615748, 0.0013707558391615748, 0.0013707558391615748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013707558391615748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74534023
Iteration 2/25 | Loss: 0.00104074
Iteration 3/25 | Loss: 0.00104074
Iteration 4/25 | Loss: 0.00104074
Iteration 5/25 | Loss: 0.00104074
Iteration 6/25 | Loss: 0.00104074
Iteration 7/25 | Loss: 0.00104074
Iteration 8/25 | Loss: 0.00104074
Iteration 9/25 | Loss: 0.00104074
Iteration 10/25 | Loss: 0.00104074
Iteration 11/25 | Loss: 0.00104074
Iteration 12/25 | Loss: 0.00104074
Iteration 13/25 | Loss: 0.00104074
Iteration 14/25 | Loss: 0.00104074
Iteration 15/25 | Loss: 0.00104074
Iteration 16/25 | Loss: 0.00104074
Iteration 17/25 | Loss: 0.00104074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010407359804958105, 0.0010407359804958105, 0.0010407359804958105, 0.0010407359804958105, 0.0010407359804958105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010407359804958105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104074
Iteration 2/1000 | Loss: 0.00006440
Iteration 3/1000 | Loss: 0.00004677
Iteration 4/1000 | Loss: 0.00004043
Iteration 5/1000 | Loss: 0.00003748
Iteration 6/1000 | Loss: 0.00003606
Iteration 7/1000 | Loss: 0.00003505
Iteration 8/1000 | Loss: 0.00003435
Iteration 9/1000 | Loss: 0.00003393
Iteration 10/1000 | Loss: 0.00003366
Iteration 11/1000 | Loss: 0.00003347
Iteration 12/1000 | Loss: 0.00003333
Iteration 13/1000 | Loss: 0.00003319
Iteration 14/1000 | Loss: 0.00003312
Iteration 15/1000 | Loss: 0.00003311
Iteration 16/1000 | Loss: 0.00003310
Iteration 17/1000 | Loss: 0.00003306
Iteration 18/1000 | Loss: 0.00003306
Iteration 19/1000 | Loss: 0.00003303
Iteration 20/1000 | Loss: 0.00003303
Iteration 21/1000 | Loss: 0.00003302
Iteration 22/1000 | Loss: 0.00003301
Iteration 23/1000 | Loss: 0.00003301
Iteration 24/1000 | Loss: 0.00003295
Iteration 25/1000 | Loss: 0.00003293
Iteration 26/1000 | Loss: 0.00003292
Iteration 27/1000 | Loss: 0.00003291
Iteration 28/1000 | Loss: 0.00003291
Iteration 29/1000 | Loss: 0.00003290
Iteration 30/1000 | Loss: 0.00003290
Iteration 31/1000 | Loss: 0.00003290
Iteration 32/1000 | Loss: 0.00003290
Iteration 33/1000 | Loss: 0.00003289
Iteration 34/1000 | Loss: 0.00003289
Iteration 35/1000 | Loss: 0.00003289
Iteration 36/1000 | Loss: 0.00003288
Iteration 37/1000 | Loss: 0.00003288
Iteration 38/1000 | Loss: 0.00003288
Iteration 39/1000 | Loss: 0.00003288
Iteration 40/1000 | Loss: 0.00003287
Iteration 41/1000 | Loss: 0.00003287
Iteration 42/1000 | Loss: 0.00003287
Iteration 43/1000 | Loss: 0.00003287
Iteration 44/1000 | Loss: 0.00003287
Iteration 45/1000 | Loss: 0.00003287
Iteration 46/1000 | Loss: 0.00003286
Iteration 47/1000 | Loss: 0.00003286
Iteration 48/1000 | Loss: 0.00003286
Iteration 49/1000 | Loss: 0.00003286
Iteration 50/1000 | Loss: 0.00003286
Iteration 51/1000 | Loss: 0.00003285
Iteration 52/1000 | Loss: 0.00003285
Iteration 53/1000 | Loss: 0.00003285
Iteration 54/1000 | Loss: 0.00003285
Iteration 55/1000 | Loss: 0.00003285
Iteration 56/1000 | Loss: 0.00003284
Iteration 57/1000 | Loss: 0.00003284
Iteration 58/1000 | Loss: 0.00003284
Iteration 59/1000 | Loss: 0.00003284
Iteration 60/1000 | Loss: 0.00003284
Iteration 61/1000 | Loss: 0.00003284
Iteration 62/1000 | Loss: 0.00003283
Iteration 63/1000 | Loss: 0.00003283
Iteration 64/1000 | Loss: 0.00003283
Iteration 65/1000 | Loss: 0.00003283
Iteration 66/1000 | Loss: 0.00003283
Iteration 67/1000 | Loss: 0.00003283
Iteration 68/1000 | Loss: 0.00003283
Iteration 69/1000 | Loss: 0.00003282
Iteration 70/1000 | Loss: 0.00003282
Iteration 71/1000 | Loss: 0.00003282
Iteration 72/1000 | Loss: 0.00003282
Iteration 73/1000 | Loss: 0.00003282
Iteration 74/1000 | Loss: 0.00003282
Iteration 75/1000 | Loss: 0.00003282
Iteration 76/1000 | Loss: 0.00003282
Iteration 77/1000 | Loss: 0.00003282
Iteration 78/1000 | Loss: 0.00003282
Iteration 79/1000 | Loss: 0.00003281
Iteration 80/1000 | Loss: 0.00003281
Iteration 81/1000 | Loss: 0.00003281
Iteration 82/1000 | Loss: 0.00003281
Iteration 83/1000 | Loss: 0.00003280
Iteration 84/1000 | Loss: 0.00003280
Iteration 85/1000 | Loss: 0.00003280
Iteration 86/1000 | Loss: 0.00003280
Iteration 87/1000 | Loss: 0.00003280
Iteration 88/1000 | Loss: 0.00003279
Iteration 89/1000 | Loss: 0.00003279
Iteration 90/1000 | Loss: 0.00003278
Iteration 91/1000 | Loss: 0.00003278
Iteration 92/1000 | Loss: 0.00003278
Iteration 93/1000 | Loss: 0.00003278
Iteration 94/1000 | Loss: 0.00003278
Iteration 95/1000 | Loss: 0.00003278
Iteration 96/1000 | Loss: 0.00003278
Iteration 97/1000 | Loss: 0.00003277
Iteration 98/1000 | Loss: 0.00003277
Iteration 99/1000 | Loss: 0.00003277
Iteration 100/1000 | Loss: 0.00003277
Iteration 101/1000 | Loss: 0.00003277
Iteration 102/1000 | Loss: 0.00003276
Iteration 103/1000 | Loss: 0.00003276
Iteration 104/1000 | Loss: 0.00003276
Iteration 105/1000 | Loss: 0.00003276
Iteration 106/1000 | Loss: 0.00003276
Iteration 107/1000 | Loss: 0.00003276
Iteration 108/1000 | Loss: 0.00003276
Iteration 109/1000 | Loss: 0.00003276
Iteration 110/1000 | Loss: 0.00003276
Iteration 111/1000 | Loss: 0.00003275
Iteration 112/1000 | Loss: 0.00003275
Iteration 113/1000 | Loss: 0.00003275
Iteration 114/1000 | Loss: 0.00003275
Iteration 115/1000 | Loss: 0.00003274
Iteration 116/1000 | Loss: 0.00003274
Iteration 117/1000 | Loss: 0.00003274
Iteration 118/1000 | Loss: 0.00003274
Iteration 119/1000 | Loss: 0.00003273
Iteration 120/1000 | Loss: 0.00003273
Iteration 121/1000 | Loss: 0.00003273
Iteration 122/1000 | Loss: 0.00003273
Iteration 123/1000 | Loss: 0.00003272
Iteration 124/1000 | Loss: 0.00003272
Iteration 125/1000 | Loss: 0.00003272
Iteration 126/1000 | Loss: 0.00003272
Iteration 127/1000 | Loss: 0.00003271
Iteration 128/1000 | Loss: 0.00003271
Iteration 129/1000 | Loss: 0.00003271
Iteration 130/1000 | Loss: 0.00003271
Iteration 131/1000 | Loss: 0.00003271
Iteration 132/1000 | Loss: 0.00003271
Iteration 133/1000 | Loss: 0.00003271
Iteration 134/1000 | Loss: 0.00003271
Iteration 135/1000 | Loss: 0.00003271
Iteration 136/1000 | Loss: 0.00003271
Iteration 137/1000 | Loss: 0.00003270
Iteration 138/1000 | Loss: 0.00003270
Iteration 139/1000 | Loss: 0.00003270
Iteration 140/1000 | Loss: 0.00003270
Iteration 141/1000 | Loss: 0.00003270
Iteration 142/1000 | Loss: 0.00003270
Iteration 143/1000 | Loss: 0.00003270
Iteration 144/1000 | Loss: 0.00003270
Iteration 145/1000 | Loss: 0.00003269
Iteration 146/1000 | Loss: 0.00003269
Iteration 147/1000 | Loss: 0.00003269
Iteration 148/1000 | Loss: 0.00003269
Iteration 149/1000 | Loss: 0.00003269
Iteration 150/1000 | Loss: 0.00003269
Iteration 151/1000 | Loss: 0.00003269
Iteration 152/1000 | Loss: 0.00003269
Iteration 153/1000 | Loss: 0.00003269
Iteration 154/1000 | Loss: 0.00003269
Iteration 155/1000 | Loss: 0.00003269
Iteration 156/1000 | Loss: 0.00003269
Iteration 157/1000 | Loss: 0.00003269
Iteration 158/1000 | Loss: 0.00003268
Iteration 159/1000 | Loss: 0.00003268
Iteration 160/1000 | Loss: 0.00003268
Iteration 161/1000 | Loss: 0.00003268
Iteration 162/1000 | Loss: 0.00003268
Iteration 163/1000 | Loss: 0.00003268
Iteration 164/1000 | Loss: 0.00003268
Iteration 165/1000 | Loss: 0.00003268
Iteration 166/1000 | Loss: 0.00003268
Iteration 167/1000 | Loss: 0.00003268
Iteration 168/1000 | Loss: 0.00003268
Iteration 169/1000 | Loss: 0.00003268
Iteration 170/1000 | Loss: 0.00003267
Iteration 171/1000 | Loss: 0.00003267
Iteration 172/1000 | Loss: 0.00003267
Iteration 173/1000 | Loss: 0.00003267
Iteration 174/1000 | Loss: 0.00003267
Iteration 175/1000 | Loss: 0.00003267
Iteration 176/1000 | Loss: 0.00003267
Iteration 177/1000 | Loss: 0.00003267
Iteration 178/1000 | Loss: 0.00003267
Iteration 179/1000 | Loss: 0.00003267
Iteration 180/1000 | Loss: 0.00003267
Iteration 181/1000 | Loss: 0.00003267
Iteration 182/1000 | Loss: 0.00003267
Iteration 183/1000 | Loss: 0.00003267
Iteration 184/1000 | Loss: 0.00003267
Iteration 185/1000 | Loss: 0.00003267
Iteration 186/1000 | Loss: 0.00003267
Iteration 187/1000 | Loss: 0.00003267
Iteration 188/1000 | Loss: 0.00003266
Iteration 189/1000 | Loss: 0.00003266
Iteration 190/1000 | Loss: 0.00003266
Iteration 191/1000 | Loss: 0.00003266
Iteration 192/1000 | Loss: 0.00003266
Iteration 193/1000 | Loss: 0.00003266
Iteration 194/1000 | Loss: 0.00003266
Iteration 195/1000 | Loss: 0.00003266
Iteration 196/1000 | Loss: 0.00003266
Iteration 197/1000 | Loss: 0.00003266
Iteration 198/1000 | Loss: 0.00003266
Iteration 199/1000 | Loss: 0.00003266
Iteration 200/1000 | Loss: 0.00003266
Iteration 201/1000 | Loss: 0.00003266
Iteration 202/1000 | Loss: 0.00003266
Iteration 203/1000 | Loss: 0.00003266
Iteration 204/1000 | Loss: 0.00003266
Iteration 205/1000 | Loss: 0.00003266
Iteration 206/1000 | Loss: 0.00003266
Iteration 207/1000 | Loss: 0.00003266
Iteration 208/1000 | Loss: 0.00003266
Iteration 209/1000 | Loss: 0.00003266
Iteration 210/1000 | Loss: 0.00003266
Iteration 211/1000 | Loss: 0.00003266
Iteration 212/1000 | Loss: 0.00003266
Iteration 213/1000 | Loss: 0.00003266
Iteration 214/1000 | Loss: 0.00003266
Iteration 215/1000 | Loss: 0.00003266
Iteration 216/1000 | Loss: 0.00003266
Iteration 217/1000 | Loss: 0.00003266
Iteration 218/1000 | Loss: 0.00003266
Iteration 219/1000 | Loss: 0.00003266
Iteration 220/1000 | Loss: 0.00003266
Iteration 221/1000 | Loss: 0.00003266
Iteration 222/1000 | Loss: 0.00003266
Iteration 223/1000 | Loss: 0.00003266
Iteration 224/1000 | Loss: 0.00003266
Iteration 225/1000 | Loss: 0.00003266
Iteration 226/1000 | Loss: 0.00003266
Iteration 227/1000 | Loss: 0.00003266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [3.265848499722779e-05, 3.265848499722779e-05, 3.265848499722779e-05, 3.265848499722779e-05, 3.265848499722779e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.265848499722779e-05

Optimization complete. Final v2v error: 5.030161380767822 mm

Highest mean error: 5.871910572052002 mm for frame 71

Lowest mean error: 4.475444316864014 mm for frame 100

Saving results

Total time: 42.64256739616394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918156
Iteration 2/25 | Loss: 0.00158847
Iteration 3/25 | Loss: 0.00139603
Iteration 4/25 | Loss: 0.00137767
Iteration 5/25 | Loss: 0.00136902
Iteration 6/25 | Loss: 0.00136735
Iteration 7/25 | Loss: 0.00136733
Iteration 8/25 | Loss: 0.00136733
Iteration 9/25 | Loss: 0.00136733
Iteration 10/25 | Loss: 0.00136733
Iteration 11/25 | Loss: 0.00136733
Iteration 12/25 | Loss: 0.00136733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013673255452886224, 0.0013673255452886224, 0.0013673255452886224, 0.0013673255452886224, 0.0013673255452886224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013673255452886224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.38827467
Iteration 2/25 | Loss: 0.00109133
Iteration 3/25 | Loss: 0.00109127
Iteration 4/25 | Loss: 0.00109127
Iteration 5/25 | Loss: 0.00109127
Iteration 6/25 | Loss: 0.00109127
Iteration 7/25 | Loss: 0.00109127
Iteration 8/25 | Loss: 0.00109127
Iteration 9/25 | Loss: 0.00109127
Iteration 10/25 | Loss: 0.00109127
Iteration 11/25 | Loss: 0.00109127
Iteration 12/25 | Loss: 0.00109127
Iteration 13/25 | Loss: 0.00109127
Iteration 14/25 | Loss: 0.00109127
Iteration 15/25 | Loss: 0.00109127
Iteration 16/25 | Loss: 0.00109127
Iteration 17/25 | Loss: 0.00109127
Iteration 18/25 | Loss: 0.00109127
Iteration 19/25 | Loss: 0.00109127
Iteration 20/25 | Loss: 0.00109127
Iteration 21/25 | Loss: 0.00109127
Iteration 22/25 | Loss: 0.00109127
Iteration 23/25 | Loss: 0.00109127
Iteration 24/25 | Loss: 0.00109127
Iteration 25/25 | Loss: 0.00109127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109127
Iteration 2/1000 | Loss: 0.00005441
Iteration 3/1000 | Loss: 0.00003909
Iteration 4/1000 | Loss: 0.00003561
Iteration 5/1000 | Loss: 0.00003276
Iteration 6/1000 | Loss: 0.00003147
Iteration 7/1000 | Loss: 0.00003075
Iteration 8/1000 | Loss: 0.00003023
Iteration 9/1000 | Loss: 0.00002979
Iteration 10/1000 | Loss: 0.00002953
Iteration 11/1000 | Loss: 0.00002940
Iteration 12/1000 | Loss: 0.00002937
Iteration 13/1000 | Loss: 0.00002936
Iteration 14/1000 | Loss: 0.00002936
Iteration 15/1000 | Loss: 0.00002936
Iteration 16/1000 | Loss: 0.00002935
Iteration 17/1000 | Loss: 0.00002934
Iteration 18/1000 | Loss: 0.00002931
Iteration 19/1000 | Loss: 0.00002930
Iteration 20/1000 | Loss: 0.00002930
Iteration 21/1000 | Loss: 0.00002928
Iteration 22/1000 | Loss: 0.00002928
Iteration 23/1000 | Loss: 0.00002928
Iteration 24/1000 | Loss: 0.00002926
Iteration 25/1000 | Loss: 0.00002925
Iteration 26/1000 | Loss: 0.00002925
Iteration 27/1000 | Loss: 0.00002923
Iteration 28/1000 | Loss: 0.00002922
Iteration 29/1000 | Loss: 0.00002921
Iteration 30/1000 | Loss: 0.00002921
Iteration 31/1000 | Loss: 0.00002921
Iteration 32/1000 | Loss: 0.00002921
Iteration 33/1000 | Loss: 0.00002921
Iteration 34/1000 | Loss: 0.00002921
Iteration 35/1000 | Loss: 0.00002920
Iteration 36/1000 | Loss: 0.00002920
Iteration 37/1000 | Loss: 0.00002920
Iteration 38/1000 | Loss: 0.00002920
Iteration 39/1000 | Loss: 0.00002920
Iteration 40/1000 | Loss: 0.00002920
Iteration 41/1000 | Loss: 0.00002919
Iteration 42/1000 | Loss: 0.00002919
Iteration 43/1000 | Loss: 0.00002918
Iteration 44/1000 | Loss: 0.00002918
Iteration 45/1000 | Loss: 0.00002918
Iteration 46/1000 | Loss: 0.00002917
Iteration 47/1000 | Loss: 0.00002917
Iteration 48/1000 | Loss: 0.00002916
Iteration 49/1000 | Loss: 0.00002916
Iteration 50/1000 | Loss: 0.00002915
Iteration 51/1000 | Loss: 0.00002915
Iteration 52/1000 | Loss: 0.00002915
Iteration 53/1000 | Loss: 0.00002914
Iteration 54/1000 | Loss: 0.00002914
Iteration 55/1000 | Loss: 0.00002914
Iteration 56/1000 | Loss: 0.00002913
Iteration 57/1000 | Loss: 0.00002913
Iteration 58/1000 | Loss: 0.00002912
Iteration 59/1000 | Loss: 0.00002912
Iteration 60/1000 | Loss: 0.00002912
Iteration 61/1000 | Loss: 0.00002911
Iteration 62/1000 | Loss: 0.00002911
Iteration 63/1000 | Loss: 0.00002911
Iteration 64/1000 | Loss: 0.00002910
Iteration 65/1000 | Loss: 0.00002910
Iteration 66/1000 | Loss: 0.00002909
Iteration 67/1000 | Loss: 0.00002909
Iteration 68/1000 | Loss: 0.00002909
Iteration 69/1000 | Loss: 0.00002909
Iteration 70/1000 | Loss: 0.00002908
Iteration 71/1000 | Loss: 0.00002908
Iteration 72/1000 | Loss: 0.00002908
Iteration 73/1000 | Loss: 0.00002908
Iteration 74/1000 | Loss: 0.00002908
Iteration 75/1000 | Loss: 0.00002908
Iteration 76/1000 | Loss: 0.00002907
Iteration 77/1000 | Loss: 0.00002907
Iteration 78/1000 | Loss: 0.00002907
Iteration 79/1000 | Loss: 0.00002907
Iteration 80/1000 | Loss: 0.00002907
Iteration 81/1000 | Loss: 0.00002907
Iteration 82/1000 | Loss: 0.00002907
Iteration 83/1000 | Loss: 0.00002906
Iteration 84/1000 | Loss: 0.00002906
Iteration 85/1000 | Loss: 0.00002906
Iteration 86/1000 | Loss: 0.00002906
Iteration 87/1000 | Loss: 0.00002906
Iteration 88/1000 | Loss: 0.00002906
Iteration 89/1000 | Loss: 0.00002906
Iteration 90/1000 | Loss: 0.00002906
Iteration 91/1000 | Loss: 0.00002906
Iteration 92/1000 | Loss: 0.00002906
Iteration 93/1000 | Loss: 0.00002906
Iteration 94/1000 | Loss: 0.00002906
Iteration 95/1000 | Loss: 0.00002906
Iteration 96/1000 | Loss: 0.00002905
Iteration 97/1000 | Loss: 0.00002905
Iteration 98/1000 | Loss: 0.00002905
Iteration 99/1000 | Loss: 0.00002905
Iteration 100/1000 | Loss: 0.00002905
Iteration 101/1000 | Loss: 0.00002905
Iteration 102/1000 | Loss: 0.00002905
Iteration 103/1000 | Loss: 0.00002905
Iteration 104/1000 | Loss: 0.00002905
Iteration 105/1000 | Loss: 0.00002905
Iteration 106/1000 | Loss: 0.00002905
Iteration 107/1000 | Loss: 0.00002905
Iteration 108/1000 | Loss: 0.00002905
Iteration 109/1000 | Loss: 0.00002905
Iteration 110/1000 | Loss: 0.00002905
Iteration 111/1000 | Loss: 0.00002905
Iteration 112/1000 | Loss: 0.00002905
Iteration 113/1000 | Loss: 0.00002905
Iteration 114/1000 | Loss: 0.00002905
Iteration 115/1000 | Loss: 0.00002905
Iteration 116/1000 | Loss: 0.00002905
Iteration 117/1000 | Loss: 0.00002905
Iteration 118/1000 | Loss: 0.00002905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.904833309003152e-05, 2.904833309003152e-05, 2.904833309003152e-05, 2.904833309003152e-05, 2.904833309003152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.904833309003152e-05

Optimization complete. Final v2v error: 4.685215473175049 mm

Highest mean error: 5.311822414398193 mm for frame 29

Lowest mean error: 4.2636189460754395 mm for frame 78

Saving results

Total time: 37.733200550079346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903772
Iteration 2/25 | Loss: 0.00160626
Iteration 3/25 | Loss: 0.00150646
Iteration 4/25 | Loss: 0.00146867
Iteration 5/25 | Loss: 0.00145364
Iteration 6/25 | Loss: 0.00145062
Iteration 7/25 | Loss: 0.00144980
Iteration 8/25 | Loss: 0.00144980
Iteration 9/25 | Loss: 0.00144980
Iteration 10/25 | Loss: 0.00144980
Iteration 11/25 | Loss: 0.00144980
Iteration 12/25 | Loss: 0.00144980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001449795439839363, 0.001449795439839363, 0.001449795439839363, 0.001449795439839363, 0.001449795439839363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001449795439839363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98149824
Iteration 2/25 | Loss: 0.00093764
Iteration 3/25 | Loss: 0.00093764
Iteration 4/25 | Loss: 0.00093764
Iteration 5/25 | Loss: 0.00093764
Iteration 6/25 | Loss: 0.00093764
Iteration 7/25 | Loss: 0.00093764
Iteration 8/25 | Loss: 0.00093764
Iteration 9/25 | Loss: 0.00093764
Iteration 10/25 | Loss: 0.00093764
Iteration 11/25 | Loss: 0.00093764
Iteration 12/25 | Loss: 0.00093764
Iteration 13/25 | Loss: 0.00093764
Iteration 14/25 | Loss: 0.00093764
Iteration 15/25 | Loss: 0.00093764
Iteration 16/25 | Loss: 0.00093764
Iteration 17/25 | Loss: 0.00093764
Iteration 18/25 | Loss: 0.00093764
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000937637232709676, 0.000937637232709676, 0.000937637232709676, 0.000937637232709676, 0.000937637232709676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000937637232709676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093764
Iteration 2/1000 | Loss: 0.00005920
Iteration 3/1000 | Loss: 0.00005076
Iteration 4/1000 | Loss: 0.00004751
Iteration 5/1000 | Loss: 0.00004618
Iteration 6/1000 | Loss: 0.00004517
Iteration 7/1000 | Loss: 0.00004447
Iteration 8/1000 | Loss: 0.00004389
Iteration 9/1000 | Loss: 0.00004350
Iteration 10/1000 | Loss: 0.00004334
Iteration 11/1000 | Loss: 0.00004332
Iteration 12/1000 | Loss: 0.00004331
Iteration 13/1000 | Loss: 0.00004326
Iteration 14/1000 | Loss: 0.00004325
Iteration 15/1000 | Loss: 0.00004324
Iteration 16/1000 | Loss: 0.00004323
Iteration 17/1000 | Loss: 0.00004322
Iteration 18/1000 | Loss: 0.00004321
Iteration 19/1000 | Loss: 0.00004318
Iteration 20/1000 | Loss: 0.00004317
Iteration 21/1000 | Loss: 0.00004316
Iteration 22/1000 | Loss: 0.00004315
Iteration 23/1000 | Loss: 0.00004315
Iteration 24/1000 | Loss: 0.00004314
Iteration 25/1000 | Loss: 0.00004314
Iteration 26/1000 | Loss: 0.00004314
Iteration 27/1000 | Loss: 0.00004314
Iteration 28/1000 | Loss: 0.00004312
Iteration 29/1000 | Loss: 0.00004312
Iteration 30/1000 | Loss: 0.00004310
Iteration 31/1000 | Loss: 0.00004310
Iteration 32/1000 | Loss: 0.00004310
Iteration 33/1000 | Loss: 0.00004310
Iteration 34/1000 | Loss: 0.00004310
Iteration 35/1000 | Loss: 0.00004309
Iteration 36/1000 | Loss: 0.00004309
Iteration 37/1000 | Loss: 0.00004308
Iteration 38/1000 | Loss: 0.00004307
Iteration 39/1000 | Loss: 0.00004307
Iteration 40/1000 | Loss: 0.00004306
Iteration 41/1000 | Loss: 0.00004305
Iteration 42/1000 | Loss: 0.00004304
Iteration 43/1000 | Loss: 0.00004304
Iteration 44/1000 | Loss: 0.00004303
Iteration 45/1000 | Loss: 0.00004303
Iteration 46/1000 | Loss: 0.00004303
Iteration 47/1000 | Loss: 0.00004302
Iteration 48/1000 | Loss: 0.00004302
Iteration 49/1000 | Loss: 0.00004302
Iteration 50/1000 | Loss: 0.00004302
Iteration 51/1000 | Loss: 0.00004302
Iteration 52/1000 | Loss: 0.00004302
Iteration 53/1000 | Loss: 0.00004302
Iteration 54/1000 | Loss: 0.00004301
Iteration 55/1000 | Loss: 0.00004301
Iteration 56/1000 | Loss: 0.00004301
Iteration 57/1000 | Loss: 0.00004301
Iteration 58/1000 | Loss: 0.00004301
Iteration 59/1000 | Loss: 0.00004301
Iteration 60/1000 | Loss: 0.00004301
Iteration 61/1000 | Loss: 0.00004301
Iteration 62/1000 | Loss: 0.00004300
Iteration 63/1000 | Loss: 0.00004300
Iteration 64/1000 | Loss: 0.00004300
Iteration 65/1000 | Loss: 0.00004300
Iteration 66/1000 | Loss: 0.00004300
Iteration 67/1000 | Loss: 0.00004299
Iteration 68/1000 | Loss: 0.00004299
Iteration 69/1000 | Loss: 0.00004299
Iteration 70/1000 | Loss: 0.00004299
Iteration 71/1000 | Loss: 0.00004299
Iteration 72/1000 | Loss: 0.00004299
Iteration 73/1000 | Loss: 0.00004299
Iteration 74/1000 | Loss: 0.00004298
Iteration 75/1000 | Loss: 0.00004298
Iteration 76/1000 | Loss: 0.00004298
Iteration 77/1000 | Loss: 0.00004298
Iteration 78/1000 | Loss: 0.00004298
Iteration 79/1000 | Loss: 0.00004298
Iteration 80/1000 | Loss: 0.00004298
Iteration 81/1000 | Loss: 0.00004298
Iteration 82/1000 | Loss: 0.00004298
Iteration 83/1000 | Loss: 0.00004297
Iteration 84/1000 | Loss: 0.00004297
Iteration 85/1000 | Loss: 0.00004297
Iteration 86/1000 | Loss: 0.00004297
Iteration 87/1000 | Loss: 0.00004297
Iteration 88/1000 | Loss: 0.00004297
Iteration 89/1000 | Loss: 0.00004296
Iteration 90/1000 | Loss: 0.00004296
Iteration 91/1000 | Loss: 0.00004296
Iteration 92/1000 | Loss: 0.00004296
Iteration 93/1000 | Loss: 0.00004296
Iteration 94/1000 | Loss: 0.00004296
Iteration 95/1000 | Loss: 0.00004296
Iteration 96/1000 | Loss: 0.00004296
Iteration 97/1000 | Loss: 0.00004295
Iteration 98/1000 | Loss: 0.00004295
Iteration 99/1000 | Loss: 0.00004295
Iteration 100/1000 | Loss: 0.00004295
Iteration 101/1000 | Loss: 0.00004295
Iteration 102/1000 | Loss: 0.00004295
Iteration 103/1000 | Loss: 0.00004295
Iteration 104/1000 | Loss: 0.00004295
Iteration 105/1000 | Loss: 0.00004295
Iteration 106/1000 | Loss: 0.00004294
Iteration 107/1000 | Loss: 0.00004294
Iteration 108/1000 | Loss: 0.00004294
Iteration 109/1000 | Loss: 0.00004294
Iteration 110/1000 | Loss: 0.00004294
Iteration 111/1000 | Loss: 0.00004294
Iteration 112/1000 | Loss: 0.00004294
Iteration 113/1000 | Loss: 0.00004294
Iteration 114/1000 | Loss: 0.00004294
Iteration 115/1000 | Loss: 0.00004294
Iteration 116/1000 | Loss: 0.00004294
Iteration 117/1000 | Loss: 0.00004294
Iteration 118/1000 | Loss: 0.00004294
Iteration 119/1000 | Loss: 0.00004294
Iteration 120/1000 | Loss: 0.00004294
Iteration 121/1000 | Loss: 0.00004294
Iteration 122/1000 | Loss: 0.00004293
Iteration 123/1000 | Loss: 0.00004293
Iteration 124/1000 | Loss: 0.00004293
Iteration 125/1000 | Loss: 0.00004293
Iteration 126/1000 | Loss: 0.00004293
Iteration 127/1000 | Loss: 0.00004293
Iteration 128/1000 | Loss: 0.00004293
Iteration 129/1000 | Loss: 0.00004293
Iteration 130/1000 | Loss: 0.00004293
Iteration 131/1000 | Loss: 0.00004293
Iteration 132/1000 | Loss: 0.00004293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [4.293289384804666e-05, 4.293289384804666e-05, 4.293289384804666e-05, 4.293289384804666e-05, 4.293289384804666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.293289384804666e-05

Optimization complete. Final v2v error: 5.685312747955322 mm

Highest mean error: 6.141336917877197 mm for frame 140

Lowest mean error: 5.124677658081055 mm for frame 1

Saving results

Total time: 39.14335107803345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931087
Iteration 2/25 | Loss: 0.00150673
Iteration 3/25 | Loss: 0.00141715
Iteration 4/25 | Loss: 0.00138501
Iteration 5/25 | Loss: 0.00137918
Iteration 6/25 | Loss: 0.00137741
Iteration 7/25 | Loss: 0.00137741
Iteration 8/25 | Loss: 0.00137741
Iteration 9/25 | Loss: 0.00137741
Iteration 10/25 | Loss: 0.00137741
Iteration 11/25 | Loss: 0.00137741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013774082763120532, 0.0013774082763120532, 0.0013774082763120532, 0.0013774082763120532, 0.0013774082763120532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013774082763120532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27735591
Iteration 2/25 | Loss: 0.00099184
Iteration 3/25 | Loss: 0.00099183
Iteration 4/25 | Loss: 0.00099183
Iteration 5/25 | Loss: 0.00099183
Iteration 6/25 | Loss: 0.00099183
Iteration 7/25 | Loss: 0.00099183
Iteration 8/25 | Loss: 0.00099183
Iteration 9/25 | Loss: 0.00099183
Iteration 10/25 | Loss: 0.00099183
Iteration 11/25 | Loss: 0.00099183
Iteration 12/25 | Loss: 0.00099183
Iteration 13/25 | Loss: 0.00099183
Iteration 14/25 | Loss: 0.00099183
Iteration 15/25 | Loss: 0.00099183
Iteration 16/25 | Loss: 0.00099183
Iteration 17/25 | Loss: 0.00099183
Iteration 18/25 | Loss: 0.00099183
Iteration 19/25 | Loss: 0.00099183
Iteration 20/25 | Loss: 0.00099183
Iteration 21/25 | Loss: 0.00099183
Iteration 22/25 | Loss: 0.00099183
Iteration 23/25 | Loss: 0.00099183
Iteration 24/25 | Loss: 0.00099183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000991828041151166, 0.000991828041151166, 0.000991828041151166, 0.000991828041151166, 0.000991828041151166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991828041151166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099183
Iteration 2/1000 | Loss: 0.00004311
Iteration 3/1000 | Loss: 0.00003434
Iteration 4/1000 | Loss: 0.00003052
Iteration 5/1000 | Loss: 0.00002882
Iteration 6/1000 | Loss: 0.00002787
Iteration 7/1000 | Loss: 0.00002709
Iteration 8/1000 | Loss: 0.00002670
Iteration 9/1000 | Loss: 0.00002641
Iteration 10/1000 | Loss: 0.00002619
Iteration 11/1000 | Loss: 0.00002615
Iteration 12/1000 | Loss: 0.00002609
Iteration 13/1000 | Loss: 0.00002609
Iteration 14/1000 | Loss: 0.00002605
Iteration 15/1000 | Loss: 0.00002600
Iteration 16/1000 | Loss: 0.00002599
Iteration 17/1000 | Loss: 0.00002599
Iteration 18/1000 | Loss: 0.00002598
Iteration 19/1000 | Loss: 0.00002598
Iteration 20/1000 | Loss: 0.00002598
Iteration 21/1000 | Loss: 0.00002598
Iteration 22/1000 | Loss: 0.00002598
Iteration 23/1000 | Loss: 0.00002597
Iteration 24/1000 | Loss: 0.00002596
Iteration 25/1000 | Loss: 0.00002596
Iteration 26/1000 | Loss: 0.00002595
Iteration 27/1000 | Loss: 0.00002594
Iteration 28/1000 | Loss: 0.00002594
Iteration 29/1000 | Loss: 0.00002594
Iteration 30/1000 | Loss: 0.00002594
Iteration 31/1000 | Loss: 0.00002593
Iteration 32/1000 | Loss: 0.00002593
Iteration 33/1000 | Loss: 0.00002593
Iteration 34/1000 | Loss: 0.00002593
Iteration 35/1000 | Loss: 0.00002593
Iteration 36/1000 | Loss: 0.00002592
Iteration 37/1000 | Loss: 0.00002592
Iteration 38/1000 | Loss: 0.00002592
Iteration 39/1000 | Loss: 0.00002591
Iteration 40/1000 | Loss: 0.00002591
Iteration 41/1000 | Loss: 0.00002591
Iteration 42/1000 | Loss: 0.00002591
Iteration 43/1000 | Loss: 0.00002591
Iteration 44/1000 | Loss: 0.00002591
Iteration 45/1000 | Loss: 0.00002590
Iteration 46/1000 | Loss: 0.00002590
Iteration 47/1000 | Loss: 0.00002590
Iteration 48/1000 | Loss: 0.00002590
Iteration 49/1000 | Loss: 0.00002589
Iteration 50/1000 | Loss: 0.00002589
Iteration 51/1000 | Loss: 0.00002589
Iteration 52/1000 | Loss: 0.00002589
Iteration 53/1000 | Loss: 0.00002589
Iteration 54/1000 | Loss: 0.00002589
Iteration 55/1000 | Loss: 0.00002589
Iteration 56/1000 | Loss: 0.00002589
Iteration 57/1000 | Loss: 0.00002589
Iteration 58/1000 | Loss: 0.00002588
Iteration 59/1000 | Loss: 0.00002588
Iteration 60/1000 | Loss: 0.00002588
Iteration 61/1000 | Loss: 0.00002588
Iteration 62/1000 | Loss: 0.00002588
Iteration 63/1000 | Loss: 0.00002587
Iteration 64/1000 | Loss: 0.00002587
Iteration 65/1000 | Loss: 0.00002587
Iteration 66/1000 | Loss: 0.00002586
Iteration 67/1000 | Loss: 0.00002586
Iteration 68/1000 | Loss: 0.00002586
Iteration 69/1000 | Loss: 0.00002586
Iteration 70/1000 | Loss: 0.00002586
Iteration 71/1000 | Loss: 0.00002586
Iteration 72/1000 | Loss: 0.00002586
Iteration 73/1000 | Loss: 0.00002585
Iteration 74/1000 | Loss: 0.00002585
Iteration 75/1000 | Loss: 0.00002585
Iteration 76/1000 | Loss: 0.00002584
Iteration 77/1000 | Loss: 0.00002584
Iteration 78/1000 | Loss: 0.00002584
Iteration 79/1000 | Loss: 0.00002584
Iteration 80/1000 | Loss: 0.00002583
Iteration 81/1000 | Loss: 0.00002583
Iteration 82/1000 | Loss: 0.00002583
Iteration 83/1000 | Loss: 0.00002583
Iteration 84/1000 | Loss: 0.00002583
Iteration 85/1000 | Loss: 0.00002583
Iteration 86/1000 | Loss: 0.00002583
Iteration 87/1000 | Loss: 0.00002583
Iteration 88/1000 | Loss: 0.00002583
Iteration 89/1000 | Loss: 0.00002583
Iteration 90/1000 | Loss: 0.00002583
Iteration 91/1000 | Loss: 0.00002583
Iteration 92/1000 | Loss: 0.00002582
Iteration 93/1000 | Loss: 0.00002582
Iteration 94/1000 | Loss: 0.00002582
Iteration 95/1000 | Loss: 0.00002582
Iteration 96/1000 | Loss: 0.00002582
Iteration 97/1000 | Loss: 0.00002582
Iteration 98/1000 | Loss: 0.00002582
Iteration 99/1000 | Loss: 0.00002582
Iteration 100/1000 | Loss: 0.00002582
Iteration 101/1000 | Loss: 0.00002582
Iteration 102/1000 | Loss: 0.00002582
Iteration 103/1000 | Loss: 0.00002582
Iteration 104/1000 | Loss: 0.00002581
Iteration 105/1000 | Loss: 0.00002581
Iteration 106/1000 | Loss: 0.00002581
Iteration 107/1000 | Loss: 0.00002581
Iteration 108/1000 | Loss: 0.00002581
Iteration 109/1000 | Loss: 0.00002581
Iteration 110/1000 | Loss: 0.00002581
Iteration 111/1000 | Loss: 0.00002581
Iteration 112/1000 | Loss: 0.00002581
Iteration 113/1000 | Loss: 0.00002581
Iteration 114/1000 | Loss: 0.00002581
Iteration 115/1000 | Loss: 0.00002581
Iteration 116/1000 | Loss: 0.00002581
Iteration 117/1000 | Loss: 0.00002580
Iteration 118/1000 | Loss: 0.00002580
Iteration 119/1000 | Loss: 0.00002580
Iteration 120/1000 | Loss: 0.00002580
Iteration 121/1000 | Loss: 0.00002580
Iteration 122/1000 | Loss: 0.00002580
Iteration 123/1000 | Loss: 0.00002580
Iteration 124/1000 | Loss: 0.00002580
Iteration 125/1000 | Loss: 0.00002580
Iteration 126/1000 | Loss: 0.00002580
Iteration 127/1000 | Loss: 0.00002580
Iteration 128/1000 | Loss: 0.00002580
Iteration 129/1000 | Loss: 0.00002580
Iteration 130/1000 | Loss: 0.00002580
Iteration 131/1000 | Loss: 0.00002580
Iteration 132/1000 | Loss: 0.00002580
Iteration 133/1000 | Loss: 0.00002580
Iteration 134/1000 | Loss: 0.00002580
Iteration 135/1000 | Loss: 0.00002580
Iteration 136/1000 | Loss: 0.00002580
Iteration 137/1000 | Loss: 0.00002580
Iteration 138/1000 | Loss: 0.00002580
Iteration 139/1000 | Loss: 0.00002580
Iteration 140/1000 | Loss: 0.00002580
Iteration 141/1000 | Loss: 0.00002580
Iteration 142/1000 | Loss: 0.00002580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.5800738512771204e-05, 2.5800738512771204e-05, 2.5800738512771204e-05, 2.5800738512771204e-05, 2.5800738512771204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5800738512771204e-05

Optimization complete. Final v2v error: 4.488491535186768 mm

Highest mean error: 4.814793586730957 mm for frame 14

Lowest mean error: 4.232778549194336 mm for frame 84

Saving results

Total time: 37.9292049407959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00613087
Iteration 2/25 | Loss: 0.00147702
Iteration 3/25 | Loss: 0.00139616
Iteration 4/25 | Loss: 0.00138315
Iteration 5/25 | Loss: 0.00137837
Iteration 6/25 | Loss: 0.00137686
Iteration 7/25 | Loss: 0.00137686
Iteration 8/25 | Loss: 0.00137686
Iteration 9/25 | Loss: 0.00137686
Iteration 10/25 | Loss: 0.00137686
Iteration 11/25 | Loss: 0.00137686
Iteration 12/25 | Loss: 0.00137686
Iteration 13/25 | Loss: 0.00137686
Iteration 14/25 | Loss: 0.00137686
Iteration 15/25 | Loss: 0.00137686
Iteration 16/25 | Loss: 0.00137686
Iteration 17/25 | Loss: 0.00137686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013768624048680067, 0.0013768624048680067, 0.0013768624048680067, 0.0013768624048680067, 0.0013768624048680067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013768624048680067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91197920
Iteration 2/25 | Loss: 0.00107237
Iteration 3/25 | Loss: 0.00107235
Iteration 4/25 | Loss: 0.00107235
Iteration 5/25 | Loss: 0.00107235
Iteration 6/25 | Loss: 0.00107235
Iteration 7/25 | Loss: 0.00107235
Iteration 8/25 | Loss: 0.00107235
Iteration 9/25 | Loss: 0.00107235
Iteration 10/25 | Loss: 0.00107235
Iteration 11/25 | Loss: 0.00107235
Iteration 12/25 | Loss: 0.00107235
Iteration 13/25 | Loss: 0.00107235
Iteration 14/25 | Loss: 0.00107235
Iteration 15/25 | Loss: 0.00107235
Iteration 16/25 | Loss: 0.00107235
Iteration 17/25 | Loss: 0.00107235
Iteration 18/25 | Loss: 0.00107235
Iteration 19/25 | Loss: 0.00107235
Iteration 20/25 | Loss: 0.00107235
Iteration 21/25 | Loss: 0.00107235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010723464656621218, 0.0010723464656621218, 0.0010723464656621218, 0.0010723464656621218, 0.0010723464656621218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010723464656621218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107235
Iteration 2/1000 | Loss: 0.00004398
Iteration 3/1000 | Loss: 0.00003645
Iteration 4/1000 | Loss: 0.00003361
Iteration 5/1000 | Loss: 0.00003169
Iteration 6/1000 | Loss: 0.00003057
Iteration 7/1000 | Loss: 0.00002997
Iteration 8/1000 | Loss: 0.00002966
Iteration 9/1000 | Loss: 0.00002943
Iteration 10/1000 | Loss: 0.00002930
Iteration 11/1000 | Loss: 0.00002918
Iteration 12/1000 | Loss: 0.00002916
Iteration 13/1000 | Loss: 0.00002915
Iteration 14/1000 | Loss: 0.00002914
Iteration 15/1000 | Loss: 0.00002914
Iteration 16/1000 | Loss: 0.00002913
Iteration 17/1000 | Loss: 0.00002913
Iteration 18/1000 | Loss: 0.00002913
Iteration 19/1000 | Loss: 0.00002913
Iteration 20/1000 | Loss: 0.00002913
Iteration 21/1000 | Loss: 0.00002913
Iteration 22/1000 | Loss: 0.00002912
Iteration 23/1000 | Loss: 0.00002912
Iteration 24/1000 | Loss: 0.00002912
Iteration 25/1000 | Loss: 0.00002911
Iteration 26/1000 | Loss: 0.00002911
Iteration 27/1000 | Loss: 0.00002911
Iteration 28/1000 | Loss: 0.00002911
Iteration 29/1000 | Loss: 0.00002911
Iteration 30/1000 | Loss: 0.00002911
Iteration 31/1000 | Loss: 0.00002910
Iteration 32/1000 | Loss: 0.00002910
Iteration 33/1000 | Loss: 0.00002910
Iteration 34/1000 | Loss: 0.00002909
Iteration 35/1000 | Loss: 0.00002909
Iteration 36/1000 | Loss: 0.00002909
Iteration 37/1000 | Loss: 0.00002908
Iteration 38/1000 | Loss: 0.00002908
Iteration 39/1000 | Loss: 0.00002908
Iteration 40/1000 | Loss: 0.00002908
Iteration 41/1000 | Loss: 0.00002908
Iteration 42/1000 | Loss: 0.00002907
Iteration 43/1000 | Loss: 0.00002907
Iteration 44/1000 | Loss: 0.00002907
Iteration 45/1000 | Loss: 0.00002906
Iteration 46/1000 | Loss: 0.00002906
Iteration 47/1000 | Loss: 0.00002906
Iteration 48/1000 | Loss: 0.00002906
Iteration 49/1000 | Loss: 0.00002906
Iteration 50/1000 | Loss: 0.00002905
Iteration 51/1000 | Loss: 0.00002905
Iteration 52/1000 | Loss: 0.00002905
Iteration 53/1000 | Loss: 0.00002905
Iteration 54/1000 | Loss: 0.00002903
Iteration 55/1000 | Loss: 0.00002903
Iteration 56/1000 | Loss: 0.00002903
Iteration 57/1000 | Loss: 0.00002902
Iteration 58/1000 | Loss: 0.00002902
Iteration 59/1000 | Loss: 0.00002902
Iteration 60/1000 | Loss: 0.00002901
Iteration 61/1000 | Loss: 0.00002901
Iteration 62/1000 | Loss: 0.00002901
Iteration 63/1000 | Loss: 0.00002901
Iteration 64/1000 | Loss: 0.00002901
Iteration 65/1000 | Loss: 0.00002901
Iteration 66/1000 | Loss: 0.00002900
Iteration 67/1000 | Loss: 0.00002900
Iteration 68/1000 | Loss: 0.00002900
Iteration 69/1000 | Loss: 0.00002900
Iteration 70/1000 | Loss: 0.00002900
Iteration 71/1000 | Loss: 0.00002900
Iteration 72/1000 | Loss: 0.00002900
Iteration 73/1000 | Loss: 0.00002900
Iteration 74/1000 | Loss: 0.00002900
Iteration 75/1000 | Loss: 0.00002900
Iteration 76/1000 | Loss: 0.00002899
Iteration 77/1000 | Loss: 0.00002899
Iteration 78/1000 | Loss: 0.00002899
Iteration 79/1000 | Loss: 0.00002899
Iteration 80/1000 | Loss: 0.00002899
Iteration 81/1000 | Loss: 0.00002899
Iteration 82/1000 | Loss: 0.00002899
Iteration 83/1000 | Loss: 0.00002898
Iteration 84/1000 | Loss: 0.00002898
Iteration 85/1000 | Loss: 0.00002898
Iteration 86/1000 | Loss: 0.00002898
Iteration 87/1000 | Loss: 0.00002898
Iteration 88/1000 | Loss: 0.00002898
Iteration 89/1000 | Loss: 0.00002898
Iteration 90/1000 | Loss: 0.00002898
Iteration 91/1000 | Loss: 0.00002898
Iteration 92/1000 | Loss: 0.00002898
Iteration 93/1000 | Loss: 0.00002897
Iteration 94/1000 | Loss: 0.00002897
Iteration 95/1000 | Loss: 0.00002897
Iteration 96/1000 | Loss: 0.00002897
Iteration 97/1000 | Loss: 0.00002897
Iteration 98/1000 | Loss: 0.00002896
Iteration 99/1000 | Loss: 0.00002896
Iteration 100/1000 | Loss: 0.00002896
Iteration 101/1000 | Loss: 0.00002895
Iteration 102/1000 | Loss: 0.00002895
Iteration 103/1000 | Loss: 0.00002895
Iteration 104/1000 | Loss: 0.00002895
Iteration 105/1000 | Loss: 0.00002895
Iteration 106/1000 | Loss: 0.00002894
Iteration 107/1000 | Loss: 0.00002894
Iteration 108/1000 | Loss: 0.00002894
Iteration 109/1000 | Loss: 0.00002894
Iteration 110/1000 | Loss: 0.00002893
Iteration 111/1000 | Loss: 0.00002893
Iteration 112/1000 | Loss: 0.00002893
Iteration 113/1000 | Loss: 0.00002893
Iteration 114/1000 | Loss: 0.00002893
Iteration 115/1000 | Loss: 0.00002893
Iteration 116/1000 | Loss: 0.00002893
Iteration 117/1000 | Loss: 0.00002892
Iteration 118/1000 | Loss: 0.00002892
Iteration 119/1000 | Loss: 0.00002892
Iteration 120/1000 | Loss: 0.00002892
Iteration 121/1000 | Loss: 0.00002892
Iteration 122/1000 | Loss: 0.00002892
Iteration 123/1000 | Loss: 0.00002892
Iteration 124/1000 | Loss: 0.00002892
Iteration 125/1000 | Loss: 0.00002892
Iteration 126/1000 | Loss: 0.00002891
Iteration 127/1000 | Loss: 0.00002891
Iteration 128/1000 | Loss: 0.00002891
Iteration 129/1000 | Loss: 0.00002891
Iteration 130/1000 | Loss: 0.00002891
Iteration 131/1000 | Loss: 0.00002891
Iteration 132/1000 | Loss: 0.00002891
Iteration 133/1000 | Loss: 0.00002891
Iteration 134/1000 | Loss: 0.00002891
Iteration 135/1000 | Loss: 0.00002891
Iteration 136/1000 | Loss: 0.00002890
Iteration 137/1000 | Loss: 0.00002890
Iteration 138/1000 | Loss: 0.00002890
Iteration 139/1000 | Loss: 0.00002890
Iteration 140/1000 | Loss: 0.00002890
Iteration 141/1000 | Loss: 0.00002890
Iteration 142/1000 | Loss: 0.00002890
Iteration 143/1000 | Loss: 0.00002890
Iteration 144/1000 | Loss: 0.00002890
Iteration 145/1000 | Loss: 0.00002890
Iteration 146/1000 | Loss: 0.00002890
Iteration 147/1000 | Loss: 0.00002890
Iteration 148/1000 | Loss: 0.00002890
Iteration 149/1000 | Loss: 0.00002890
Iteration 150/1000 | Loss: 0.00002890
Iteration 151/1000 | Loss: 0.00002890
Iteration 152/1000 | Loss: 0.00002890
Iteration 153/1000 | Loss: 0.00002889
Iteration 154/1000 | Loss: 0.00002889
Iteration 155/1000 | Loss: 0.00002889
Iteration 156/1000 | Loss: 0.00002889
Iteration 157/1000 | Loss: 0.00002889
Iteration 158/1000 | Loss: 0.00002889
Iteration 159/1000 | Loss: 0.00002889
Iteration 160/1000 | Loss: 0.00002889
Iteration 161/1000 | Loss: 0.00002889
Iteration 162/1000 | Loss: 0.00002889
Iteration 163/1000 | Loss: 0.00002889
Iteration 164/1000 | Loss: 0.00002889
Iteration 165/1000 | Loss: 0.00002889
Iteration 166/1000 | Loss: 0.00002889
Iteration 167/1000 | Loss: 0.00002889
Iteration 168/1000 | Loss: 0.00002889
Iteration 169/1000 | Loss: 0.00002889
Iteration 170/1000 | Loss: 0.00002889
Iteration 171/1000 | Loss: 0.00002889
Iteration 172/1000 | Loss: 0.00002889
Iteration 173/1000 | Loss: 0.00002889
Iteration 174/1000 | Loss: 0.00002889
Iteration 175/1000 | Loss: 0.00002889
Iteration 176/1000 | Loss: 0.00002889
Iteration 177/1000 | Loss: 0.00002889
Iteration 178/1000 | Loss: 0.00002889
Iteration 179/1000 | Loss: 0.00002889
Iteration 180/1000 | Loss: 0.00002889
Iteration 181/1000 | Loss: 0.00002889
Iteration 182/1000 | Loss: 0.00002889
Iteration 183/1000 | Loss: 0.00002889
Iteration 184/1000 | Loss: 0.00002889
Iteration 185/1000 | Loss: 0.00002889
Iteration 186/1000 | Loss: 0.00002889
Iteration 187/1000 | Loss: 0.00002889
Iteration 188/1000 | Loss: 0.00002889
Iteration 189/1000 | Loss: 0.00002889
Iteration 190/1000 | Loss: 0.00002889
Iteration 191/1000 | Loss: 0.00002889
Iteration 192/1000 | Loss: 0.00002889
Iteration 193/1000 | Loss: 0.00002889
Iteration 194/1000 | Loss: 0.00002889
Iteration 195/1000 | Loss: 0.00002889
Iteration 196/1000 | Loss: 0.00002889
Iteration 197/1000 | Loss: 0.00002889
Iteration 198/1000 | Loss: 0.00002889
Iteration 199/1000 | Loss: 0.00002889
Iteration 200/1000 | Loss: 0.00002889
Iteration 201/1000 | Loss: 0.00002889
Iteration 202/1000 | Loss: 0.00002889
Iteration 203/1000 | Loss: 0.00002889
Iteration 204/1000 | Loss: 0.00002889
Iteration 205/1000 | Loss: 0.00002889
Iteration 206/1000 | Loss: 0.00002889
Iteration 207/1000 | Loss: 0.00002889
Iteration 208/1000 | Loss: 0.00002889
Iteration 209/1000 | Loss: 0.00002889
Iteration 210/1000 | Loss: 0.00002889
Iteration 211/1000 | Loss: 0.00002889
Iteration 212/1000 | Loss: 0.00002889
Iteration 213/1000 | Loss: 0.00002889
Iteration 214/1000 | Loss: 0.00002889
Iteration 215/1000 | Loss: 0.00002889
Iteration 216/1000 | Loss: 0.00002889
Iteration 217/1000 | Loss: 0.00002889
Iteration 218/1000 | Loss: 0.00002889
Iteration 219/1000 | Loss: 0.00002889
Iteration 220/1000 | Loss: 0.00002889
Iteration 221/1000 | Loss: 0.00002889
Iteration 222/1000 | Loss: 0.00002889
Iteration 223/1000 | Loss: 0.00002889
Iteration 224/1000 | Loss: 0.00002889
Iteration 225/1000 | Loss: 0.00002889
Iteration 226/1000 | Loss: 0.00002889
Iteration 227/1000 | Loss: 0.00002889
Iteration 228/1000 | Loss: 0.00002889
Iteration 229/1000 | Loss: 0.00002889
Iteration 230/1000 | Loss: 0.00002889
Iteration 231/1000 | Loss: 0.00002889
Iteration 232/1000 | Loss: 0.00002889
Iteration 233/1000 | Loss: 0.00002889
Iteration 234/1000 | Loss: 0.00002889
Iteration 235/1000 | Loss: 0.00002889
Iteration 236/1000 | Loss: 0.00002889
Iteration 237/1000 | Loss: 0.00002889
Iteration 238/1000 | Loss: 0.00002889
Iteration 239/1000 | Loss: 0.00002889
Iteration 240/1000 | Loss: 0.00002889
Iteration 241/1000 | Loss: 0.00002889
Iteration 242/1000 | Loss: 0.00002889
Iteration 243/1000 | Loss: 0.00002889
Iteration 244/1000 | Loss: 0.00002889
Iteration 245/1000 | Loss: 0.00002889
Iteration 246/1000 | Loss: 0.00002889
Iteration 247/1000 | Loss: 0.00002889
Iteration 248/1000 | Loss: 0.00002889
Iteration 249/1000 | Loss: 0.00002889
Iteration 250/1000 | Loss: 0.00002889
Iteration 251/1000 | Loss: 0.00002889
Iteration 252/1000 | Loss: 0.00002889
Iteration 253/1000 | Loss: 0.00002889
Iteration 254/1000 | Loss: 0.00002889
Iteration 255/1000 | Loss: 0.00002889
Iteration 256/1000 | Loss: 0.00002889
Iteration 257/1000 | Loss: 0.00002889
Iteration 258/1000 | Loss: 0.00002889
Iteration 259/1000 | Loss: 0.00002889
Iteration 260/1000 | Loss: 0.00002889
Iteration 261/1000 | Loss: 0.00002889
Iteration 262/1000 | Loss: 0.00002889
Iteration 263/1000 | Loss: 0.00002889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.8893602575408295e-05, 2.8893602575408295e-05, 2.8893602575408295e-05, 2.8893602575408295e-05, 2.8893602575408295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8893602575408295e-05

Optimization complete. Final v2v error: 4.715915679931641 mm

Highest mean error: 5.0806965827941895 mm for frame 80

Lowest mean error: 4.342558860778809 mm for frame 166

Saving results

Total time: 36.61392879486084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108734
Iteration 2/25 | Loss: 0.00229593
Iteration 3/25 | Loss: 0.00175004
Iteration 4/25 | Loss: 0.00156754
Iteration 5/25 | Loss: 0.00147826
Iteration 6/25 | Loss: 0.00141227
Iteration 7/25 | Loss: 0.00136883
Iteration 8/25 | Loss: 0.00135531
Iteration 9/25 | Loss: 0.00133502
Iteration 10/25 | Loss: 0.00133636
Iteration 11/25 | Loss: 0.00133073
Iteration 12/25 | Loss: 0.00130944
Iteration 13/25 | Loss: 0.00130407
Iteration 14/25 | Loss: 0.00130249
Iteration 15/25 | Loss: 0.00130151
Iteration 16/25 | Loss: 0.00130066
Iteration 17/25 | Loss: 0.00130340
Iteration 18/25 | Loss: 0.00129991
Iteration 19/25 | Loss: 0.00129814
Iteration 20/25 | Loss: 0.00129711
Iteration 21/25 | Loss: 0.00129700
Iteration 22/25 | Loss: 0.00129699
Iteration 23/25 | Loss: 0.00129699
Iteration 24/25 | Loss: 0.00129699
Iteration 25/25 | Loss: 0.00129699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41140151
Iteration 2/25 | Loss: 0.00121583
Iteration 3/25 | Loss: 0.00121583
Iteration 4/25 | Loss: 0.00121583
Iteration 5/25 | Loss: 0.00121583
Iteration 6/25 | Loss: 0.00121583
Iteration 7/25 | Loss: 0.00121583
Iteration 8/25 | Loss: 0.00121583
Iteration 9/25 | Loss: 0.00121583
Iteration 10/25 | Loss: 0.00121583
Iteration 11/25 | Loss: 0.00121583
Iteration 12/25 | Loss: 0.00121583
Iteration 13/25 | Loss: 0.00121583
Iteration 14/25 | Loss: 0.00121583
Iteration 15/25 | Loss: 0.00121583
Iteration 16/25 | Loss: 0.00121583
Iteration 17/25 | Loss: 0.00121583
Iteration 18/25 | Loss: 0.00121583
Iteration 19/25 | Loss: 0.00121583
Iteration 20/25 | Loss: 0.00121583
Iteration 21/25 | Loss: 0.00121583
Iteration 22/25 | Loss: 0.00121583
Iteration 23/25 | Loss: 0.00121583
Iteration 24/25 | Loss: 0.00121583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012158270692452788, 0.0012158270692452788, 0.0012158270692452788, 0.0012158270692452788, 0.0012158270692452788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012158270692452788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121583
Iteration 2/1000 | Loss: 0.00051281
Iteration 3/1000 | Loss: 0.00068398
Iteration 4/1000 | Loss: 0.00011359
Iteration 5/1000 | Loss: 0.00161033
Iteration 6/1000 | Loss: 0.00309488
Iteration 7/1000 | Loss: 0.00222448
Iteration 8/1000 | Loss: 0.00014869
Iteration 9/1000 | Loss: 0.00009669
Iteration 10/1000 | Loss: 0.00007570
Iteration 11/1000 | Loss: 0.00006550
Iteration 12/1000 | Loss: 0.00005995
Iteration 13/1000 | Loss: 0.00005785
Iteration 14/1000 | Loss: 0.00005665
Iteration 15/1000 | Loss: 0.00005549
Iteration 16/1000 | Loss: 0.00005453
Iteration 17/1000 | Loss: 0.00005377
Iteration 18/1000 | Loss: 0.00005324
Iteration 19/1000 | Loss: 0.00005294
Iteration 20/1000 | Loss: 0.00005269
Iteration 21/1000 | Loss: 0.00005264
Iteration 22/1000 | Loss: 0.00005258
Iteration 23/1000 | Loss: 0.00005255
Iteration 24/1000 | Loss: 0.00005254
Iteration 25/1000 | Loss: 0.00005254
Iteration 26/1000 | Loss: 0.00005253
Iteration 27/1000 | Loss: 0.00005253
Iteration 28/1000 | Loss: 0.00005252
Iteration 29/1000 | Loss: 0.00005252
Iteration 30/1000 | Loss: 0.00005251
Iteration 31/1000 | Loss: 0.00005251
Iteration 32/1000 | Loss: 0.00005250
Iteration 33/1000 | Loss: 0.00005250
Iteration 34/1000 | Loss: 0.00005250
Iteration 35/1000 | Loss: 0.00005250
Iteration 36/1000 | Loss: 0.00005249
Iteration 37/1000 | Loss: 0.00005249
Iteration 38/1000 | Loss: 0.00005247
Iteration 39/1000 | Loss: 0.00005247
Iteration 40/1000 | Loss: 0.00005246
Iteration 41/1000 | Loss: 0.00005244
Iteration 42/1000 | Loss: 0.00005244
Iteration 43/1000 | Loss: 0.00005244
Iteration 44/1000 | Loss: 0.00005244
Iteration 45/1000 | Loss: 0.00005244
Iteration 46/1000 | Loss: 0.00005243
Iteration 47/1000 | Loss: 0.00005243
Iteration 48/1000 | Loss: 0.00005243
Iteration 49/1000 | Loss: 0.00005243
Iteration 50/1000 | Loss: 0.00005243
Iteration 51/1000 | Loss: 0.00005243
Iteration 52/1000 | Loss: 0.00005243
Iteration 53/1000 | Loss: 0.00005243
Iteration 54/1000 | Loss: 0.00005242
Iteration 55/1000 | Loss: 0.00005242
Iteration 56/1000 | Loss: 0.00005242
Iteration 57/1000 | Loss: 0.00005242
Iteration 58/1000 | Loss: 0.00005242
Iteration 59/1000 | Loss: 0.00005241
Iteration 60/1000 | Loss: 0.00005241
Iteration 61/1000 | Loss: 0.00005240
Iteration 62/1000 | Loss: 0.00005240
Iteration 63/1000 | Loss: 0.00005239
Iteration 64/1000 | Loss: 0.00005239
Iteration 65/1000 | Loss: 0.00005239
Iteration 66/1000 | Loss: 0.00005238
Iteration 67/1000 | Loss: 0.00005238
Iteration 68/1000 | Loss: 0.00005238
Iteration 69/1000 | Loss: 0.00005238
Iteration 70/1000 | Loss: 0.00005238
Iteration 71/1000 | Loss: 0.00005238
Iteration 72/1000 | Loss: 0.00005238
Iteration 73/1000 | Loss: 0.00005237
Iteration 74/1000 | Loss: 0.00005237
Iteration 75/1000 | Loss: 0.00005237
Iteration 76/1000 | Loss: 0.00005237
Iteration 77/1000 | Loss: 0.00005237
Iteration 78/1000 | Loss: 0.00005237
Iteration 79/1000 | Loss: 0.00005237
Iteration 80/1000 | Loss: 0.00005236
Iteration 81/1000 | Loss: 0.00005236
Iteration 82/1000 | Loss: 0.00005236
Iteration 83/1000 | Loss: 0.00005236
Iteration 84/1000 | Loss: 0.00005236
Iteration 85/1000 | Loss: 0.00005236
Iteration 86/1000 | Loss: 0.00005236
Iteration 87/1000 | Loss: 0.00005236
Iteration 88/1000 | Loss: 0.00005236
Iteration 89/1000 | Loss: 0.00005236
Iteration 90/1000 | Loss: 0.00005236
Iteration 91/1000 | Loss: 0.00005235
Iteration 92/1000 | Loss: 0.00005235
Iteration 93/1000 | Loss: 0.00005235
Iteration 94/1000 | Loss: 0.00005235
Iteration 95/1000 | Loss: 0.00005234
Iteration 96/1000 | Loss: 0.00005234
Iteration 97/1000 | Loss: 0.00005234
Iteration 98/1000 | Loss: 0.00005234
Iteration 99/1000 | Loss: 0.00005234
Iteration 100/1000 | Loss: 0.00005234
Iteration 101/1000 | Loss: 0.00005233
Iteration 102/1000 | Loss: 0.00005233
Iteration 103/1000 | Loss: 0.00005233
Iteration 104/1000 | Loss: 0.00005233
Iteration 105/1000 | Loss: 0.00005233
Iteration 106/1000 | Loss: 0.00005233
Iteration 107/1000 | Loss: 0.00005233
Iteration 108/1000 | Loss: 0.00005233
Iteration 109/1000 | Loss: 0.00005233
Iteration 110/1000 | Loss: 0.00005232
Iteration 111/1000 | Loss: 0.00005232
Iteration 112/1000 | Loss: 0.00005232
Iteration 113/1000 | Loss: 0.00005232
Iteration 114/1000 | Loss: 0.00005232
Iteration 115/1000 | Loss: 0.00005232
Iteration 116/1000 | Loss: 0.00005232
Iteration 117/1000 | Loss: 0.00005232
Iteration 118/1000 | Loss: 0.00005232
Iteration 119/1000 | Loss: 0.00005232
Iteration 120/1000 | Loss: 0.00005232
Iteration 121/1000 | Loss: 0.00005232
Iteration 122/1000 | Loss: 0.00005232
Iteration 123/1000 | Loss: 0.00005232
Iteration 124/1000 | Loss: 0.00005232
Iteration 125/1000 | Loss: 0.00005232
Iteration 126/1000 | Loss: 0.00005231
Iteration 127/1000 | Loss: 0.00005231
Iteration 128/1000 | Loss: 0.00005231
Iteration 129/1000 | Loss: 0.00005231
Iteration 130/1000 | Loss: 0.00005231
Iteration 131/1000 | Loss: 0.00005231
Iteration 132/1000 | Loss: 0.00005231
Iteration 133/1000 | Loss: 0.00005231
Iteration 134/1000 | Loss: 0.00005231
Iteration 135/1000 | Loss: 0.00005231
Iteration 136/1000 | Loss: 0.00005231
Iteration 137/1000 | Loss: 0.00005231
Iteration 138/1000 | Loss: 0.00005231
Iteration 139/1000 | Loss: 0.00005231
Iteration 140/1000 | Loss: 0.00005231
Iteration 141/1000 | Loss: 0.00005231
Iteration 142/1000 | Loss: 0.00005231
Iteration 143/1000 | Loss: 0.00005231
Iteration 144/1000 | Loss: 0.00005231
Iteration 145/1000 | Loss: 0.00005231
Iteration 146/1000 | Loss: 0.00005231
Iteration 147/1000 | Loss: 0.00005231
Iteration 148/1000 | Loss: 0.00005231
Iteration 149/1000 | Loss: 0.00005231
Iteration 150/1000 | Loss: 0.00005231
Iteration 151/1000 | Loss: 0.00005231
Iteration 152/1000 | Loss: 0.00005231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [5.230631722952239e-05, 5.230631722952239e-05, 5.230631722952239e-05, 5.230631722952239e-05, 5.230631722952239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.230631722952239e-05

Optimization complete. Final v2v error: 5.334064960479736 mm

Highest mean error: 23.045072555541992 mm for frame 8

Lowest mean error: 4.5435967445373535 mm for frame 108

Saving results

Total time: 73.66946291923523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841776
Iteration 2/25 | Loss: 0.00121285
Iteration 3/25 | Loss: 0.00114451
Iteration 4/25 | Loss: 0.00113598
Iteration 5/25 | Loss: 0.00113287
Iteration 6/25 | Loss: 0.00113222
Iteration 7/25 | Loss: 0.00113222
Iteration 8/25 | Loss: 0.00113222
Iteration 9/25 | Loss: 0.00113222
Iteration 10/25 | Loss: 0.00113222
Iteration 11/25 | Loss: 0.00113222
Iteration 12/25 | Loss: 0.00113222
Iteration 13/25 | Loss: 0.00113222
Iteration 14/25 | Loss: 0.00113222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011322236387059093, 0.0011322236387059093, 0.0011322236387059093, 0.0011322236387059093, 0.0011322236387059093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011322236387059093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75792086
Iteration 2/25 | Loss: 0.00091853
Iteration 3/25 | Loss: 0.00091853
Iteration 4/25 | Loss: 0.00091853
Iteration 5/25 | Loss: 0.00091853
Iteration 6/25 | Loss: 0.00091853
Iteration 7/25 | Loss: 0.00091853
Iteration 8/25 | Loss: 0.00091853
Iteration 9/25 | Loss: 0.00091853
Iteration 10/25 | Loss: 0.00091853
Iteration 11/25 | Loss: 0.00091853
Iteration 12/25 | Loss: 0.00091853
Iteration 13/25 | Loss: 0.00091853
Iteration 14/25 | Loss: 0.00091853
Iteration 15/25 | Loss: 0.00091853
Iteration 16/25 | Loss: 0.00091853
Iteration 17/25 | Loss: 0.00091853
Iteration 18/25 | Loss: 0.00091853
Iteration 19/25 | Loss: 0.00091853
Iteration 20/25 | Loss: 0.00091853
Iteration 21/25 | Loss: 0.00091853
Iteration 22/25 | Loss: 0.00091853
Iteration 23/25 | Loss: 0.00091853
Iteration 24/25 | Loss: 0.00091853
Iteration 25/25 | Loss: 0.00091853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091853
Iteration 2/1000 | Loss: 0.00001914
Iteration 3/1000 | Loss: 0.00001351
Iteration 4/1000 | Loss: 0.00001205
Iteration 5/1000 | Loss: 0.00001146
Iteration 6/1000 | Loss: 0.00001095
Iteration 7/1000 | Loss: 0.00001058
Iteration 8/1000 | Loss: 0.00001035
Iteration 9/1000 | Loss: 0.00001017
Iteration 10/1000 | Loss: 0.00001017
Iteration 11/1000 | Loss: 0.00000996
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000983
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000979
Iteration 16/1000 | Loss: 0.00000979
Iteration 17/1000 | Loss: 0.00000977
Iteration 18/1000 | Loss: 0.00000976
Iteration 19/1000 | Loss: 0.00000975
Iteration 20/1000 | Loss: 0.00000973
Iteration 21/1000 | Loss: 0.00000973
Iteration 22/1000 | Loss: 0.00000972
Iteration 23/1000 | Loss: 0.00000969
Iteration 24/1000 | Loss: 0.00000969
Iteration 25/1000 | Loss: 0.00000969
Iteration 26/1000 | Loss: 0.00000968
Iteration 27/1000 | Loss: 0.00000968
Iteration 28/1000 | Loss: 0.00000967
Iteration 29/1000 | Loss: 0.00000967
Iteration 30/1000 | Loss: 0.00000967
Iteration 31/1000 | Loss: 0.00000967
Iteration 32/1000 | Loss: 0.00000967
Iteration 33/1000 | Loss: 0.00000967
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000964
Iteration 36/1000 | Loss: 0.00000964
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000963
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000962
Iteration 41/1000 | Loss: 0.00000962
Iteration 42/1000 | Loss: 0.00000961
Iteration 43/1000 | Loss: 0.00000960
Iteration 44/1000 | Loss: 0.00000960
Iteration 45/1000 | Loss: 0.00000959
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000959
Iteration 48/1000 | Loss: 0.00000958
Iteration 49/1000 | Loss: 0.00000958
Iteration 50/1000 | Loss: 0.00000958
Iteration 51/1000 | Loss: 0.00000956
Iteration 52/1000 | Loss: 0.00000956
Iteration 53/1000 | Loss: 0.00000955
Iteration 54/1000 | Loss: 0.00000955
Iteration 55/1000 | Loss: 0.00000955
Iteration 56/1000 | Loss: 0.00000955
Iteration 57/1000 | Loss: 0.00000955
Iteration 58/1000 | Loss: 0.00000955
Iteration 59/1000 | Loss: 0.00000954
Iteration 60/1000 | Loss: 0.00000954
Iteration 61/1000 | Loss: 0.00000953
Iteration 62/1000 | Loss: 0.00000953
Iteration 63/1000 | Loss: 0.00000953
Iteration 64/1000 | Loss: 0.00000952
Iteration 65/1000 | Loss: 0.00000952
Iteration 66/1000 | Loss: 0.00000952
Iteration 67/1000 | Loss: 0.00000951
Iteration 68/1000 | Loss: 0.00000951
Iteration 69/1000 | Loss: 0.00000950
Iteration 70/1000 | Loss: 0.00000950
Iteration 71/1000 | Loss: 0.00000950
Iteration 72/1000 | Loss: 0.00000950
Iteration 73/1000 | Loss: 0.00000949
Iteration 74/1000 | Loss: 0.00000949
Iteration 75/1000 | Loss: 0.00000949
Iteration 76/1000 | Loss: 0.00000949
Iteration 77/1000 | Loss: 0.00000949
Iteration 78/1000 | Loss: 0.00000948
Iteration 79/1000 | Loss: 0.00000948
Iteration 80/1000 | Loss: 0.00000948
Iteration 81/1000 | Loss: 0.00000948
Iteration 82/1000 | Loss: 0.00000948
Iteration 83/1000 | Loss: 0.00000948
Iteration 84/1000 | Loss: 0.00000948
Iteration 85/1000 | Loss: 0.00000948
Iteration 86/1000 | Loss: 0.00000947
Iteration 87/1000 | Loss: 0.00000947
Iteration 88/1000 | Loss: 0.00000947
Iteration 89/1000 | Loss: 0.00000947
Iteration 90/1000 | Loss: 0.00000946
Iteration 91/1000 | Loss: 0.00000946
Iteration 92/1000 | Loss: 0.00000946
Iteration 93/1000 | Loss: 0.00000946
Iteration 94/1000 | Loss: 0.00000946
Iteration 95/1000 | Loss: 0.00000946
Iteration 96/1000 | Loss: 0.00000946
Iteration 97/1000 | Loss: 0.00000946
Iteration 98/1000 | Loss: 0.00000946
Iteration 99/1000 | Loss: 0.00000945
Iteration 100/1000 | Loss: 0.00000945
Iteration 101/1000 | Loss: 0.00000945
Iteration 102/1000 | Loss: 0.00000944
Iteration 103/1000 | Loss: 0.00000944
Iteration 104/1000 | Loss: 0.00000944
Iteration 105/1000 | Loss: 0.00000943
Iteration 106/1000 | Loss: 0.00000943
Iteration 107/1000 | Loss: 0.00000943
Iteration 108/1000 | Loss: 0.00000943
Iteration 109/1000 | Loss: 0.00000943
Iteration 110/1000 | Loss: 0.00000943
Iteration 111/1000 | Loss: 0.00000943
Iteration 112/1000 | Loss: 0.00000943
Iteration 113/1000 | Loss: 0.00000943
Iteration 114/1000 | Loss: 0.00000943
Iteration 115/1000 | Loss: 0.00000943
Iteration 116/1000 | Loss: 0.00000943
Iteration 117/1000 | Loss: 0.00000943
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000942
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000942
Iteration 125/1000 | Loss: 0.00000942
Iteration 126/1000 | Loss: 0.00000941
Iteration 127/1000 | Loss: 0.00000941
Iteration 128/1000 | Loss: 0.00000941
Iteration 129/1000 | Loss: 0.00000941
Iteration 130/1000 | Loss: 0.00000941
Iteration 131/1000 | Loss: 0.00000941
Iteration 132/1000 | Loss: 0.00000940
Iteration 133/1000 | Loss: 0.00000940
Iteration 134/1000 | Loss: 0.00000940
Iteration 135/1000 | Loss: 0.00000940
Iteration 136/1000 | Loss: 0.00000940
Iteration 137/1000 | Loss: 0.00000940
Iteration 138/1000 | Loss: 0.00000940
Iteration 139/1000 | Loss: 0.00000940
Iteration 140/1000 | Loss: 0.00000940
Iteration 141/1000 | Loss: 0.00000940
Iteration 142/1000 | Loss: 0.00000939
Iteration 143/1000 | Loss: 0.00000939
Iteration 144/1000 | Loss: 0.00000939
Iteration 145/1000 | Loss: 0.00000939
Iteration 146/1000 | Loss: 0.00000939
Iteration 147/1000 | Loss: 0.00000938
Iteration 148/1000 | Loss: 0.00000938
Iteration 149/1000 | Loss: 0.00000938
Iteration 150/1000 | Loss: 0.00000938
Iteration 151/1000 | Loss: 0.00000938
Iteration 152/1000 | Loss: 0.00000938
Iteration 153/1000 | Loss: 0.00000938
Iteration 154/1000 | Loss: 0.00000938
Iteration 155/1000 | Loss: 0.00000938
Iteration 156/1000 | Loss: 0.00000938
Iteration 157/1000 | Loss: 0.00000937
Iteration 158/1000 | Loss: 0.00000937
Iteration 159/1000 | Loss: 0.00000937
Iteration 160/1000 | Loss: 0.00000937
Iteration 161/1000 | Loss: 0.00000936
Iteration 162/1000 | Loss: 0.00000936
Iteration 163/1000 | Loss: 0.00000936
Iteration 164/1000 | Loss: 0.00000936
Iteration 165/1000 | Loss: 0.00000936
Iteration 166/1000 | Loss: 0.00000936
Iteration 167/1000 | Loss: 0.00000936
Iteration 168/1000 | Loss: 0.00000936
Iteration 169/1000 | Loss: 0.00000936
Iteration 170/1000 | Loss: 0.00000936
Iteration 171/1000 | Loss: 0.00000936
Iteration 172/1000 | Loss: 0.00000936
Iteration 173/1000 | Loss: 0.00000936
Iteration 174/1000 | Loss: 0.00000936
Iteration 175/1000 | Loss: 0.00000935
Iteration 176/1000 | Loss: 0.00000935
Iteration 177/1000 | Loss: 0.00000935
Iteration 178/1000 | Loss: 0.00000935
Iteration 179/1000 | Loss: 0.00000935
Iteration 180/1000 | Loss: 0.00000935
Iteration 181/1000 | Loss: 0.00000935
Iteration 182/1000 | Loss: 0.00000935
Iteration 183/1000 | Loss: 0.00000935
Iteration 184/1000 | Loss: 0.00000934
Iteration 185/1000 | Loss: 0.00000934
Iteration 186/1000 | Loss: 0.00000934
Iteration 187/1000 | Loss: 0.00000934
Iteration 188/1000 | Loss: 0.00000934
Iteration 189/1000 | Loss: 0.00000934
Iteration 190/1000 | Loss: 0.00000934
Iteration 191/1000 | Loss: 0.00000934
Iteration 192/1000 | Loss: 0.00000934
Iteration 193/1000 | Loss: 0.00000934
Iteration 194/1000 | Loss: 0.00000934
Iteration 195/1000 | Loss: 0.00000934
Iteration 196/1000 | Loss: 0.00000934
Iteration 197/1000 | Loss: 0.00000934
Iteration 198/1000 | Loss: 0.00000934
Iteration 199/1000 | Loss: 0.00000934
Iteration 200/1000 | Loss: 0.00000934
Iteration 201/1000 | Loss: 0.00000934
Iteration 202/1000 | Loss: 0.00000934
Iteration 203/1000 | Loss: 0.00000934
Iteration 204/1000 | Loss: 0.00000934
Iteration 205/1000 | Loss: 0.00000934
Iteration 206/1000 | Loss: 0.00000934
Iteration 207/1000 | Loss: 0.00000934
Iteration 208/1000 | Loss: 0.00000934
Iteration 209/1000 | Loss: 0.00000934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [9.343880265078042e-06, 9.343880265078042e-06, 9.343880265078042e-06, 9.343880265078042e-06, 9.343880265078042e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.343880265078042e-06

Optimization complete. Final v2v error: 2.6350088119506836 mm

Highest mean error: 3.008019208908081 mm for frame 63

Lowest mean error: 2.494406223297119 mm for frame 119

Saving results

Total time: 37.57087159156799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997928
Iteration 2/25 | Loss: 0.00997928
Iteration 3/25 | Loss: 0.00997928
Iteration 4/25 | Loss: 0.00997928
Iteration 5/25 | Loss: 0.00997928
Iteration 6/25 | Loss: 0.00997928
Iteration 7/25 | Loss: 0.00997927
Iteration 8/25 | Loss: 0.00997927
Iteration 9/25 | Loss: 0.00997927
Iteration 10/25 | Loss: 0.00997927
Iteration 11/25 | Loss: 0.00997926
Iteration 12/25 | Loss: 0.00997926
Iteration 13/25 | Loss: 0.00997926
Iteration 14/25 | Loss: 0.00997926
Iteration 15/25 | Loss: 0.00997926
Iteration 16/25 | Loss: 0.00997925
Iteration 17/25 | Loss: 0.00997925
Iteration 18/25 | Loss: 0.00997925
Iteration 19/25 | Loss: 0.00997925
Iteration 20/25 | Loss: 0.00997925
Iteration 21/25 | Loss: 0.00997924
Iteration 22/25 | Loss: 0.00997924
Iteration 23/25 | Loss: 0.00997924
Iteration 24/25 | Loss: 0.00997924
Iteration 25/25 | Loss: 0.00997924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68011582
Iteration 2/25 | Loss: 0.16457233
Iteration 3/25 | Loss: 0.16654451
Iteration 4/25 | Loss: 0.16748628
Iteration 5/25 | Loss: 0.15944001
Iteration 6/25 | Loss: 0.15941884
Iteration 7/25 | Loss: 0.15914342
Iteration 8/25 | Loss: 0.15914342
Iteration 9/25 | Loss: 0.15914340
Iteration 10/25 | Loss: 0.15914339
Iteration 11/25 | Loss: 0.15914340
Iteration 12/25 | Loss: 0.15914339
Iteration 13/25 | Loss: 0.15914339
Iteration 14/25 | Loss: 0.15914339
Iteration 15/25 | Loss: 0.15932831
Iteration 16/25 | Loss: 0.15932831
Iteration 17/25 | Loss: 0.15914351
Iteration 18/25 | Loss: 0.15914346
Iteration 19/25 | Loss: 0.15914346
Iteration 20/25 | Loss: 0.15914342
Iteration 21/25 | Loss: 0.15914342
Iteration 22/25 | Loss: 0.15914340
Iteration 23/25 | Loss: 0.15914339
Iteration 24/25 | Loss: 0.15914339
Iteration 25/25 | Loss: 0.15914339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15914339
Iteration 2/1000 | Loss: 0.00651893
Iteration 3/1000 | Loss: 0.00281191
Iteration 4/1000 | Loss: 0.00068194
Iteration 5/1000 | Loss: 0.00094903
Iteration 6/1000 | Loss: 0.00045146
Iteration 7/1000 | Loss: 0.00032011
Iteration 8/1000 | Loss: 0.00122968
Iteration 9/1000 | Loss: 0.00041789
Iteration 10/1000 | Loss: 0.00009565
Iteration 11/1000 | Loss: 0.00019918
Iteration 12/1000 | Loss: 0.00006126
Iteration 13/1000 | Loss: 0.00039167
Iteration 14/1000 | Loss: 0.00004912
Iteration 15/1000 | Loss: 0.00004262
Iteration 16/1000 | Loss: 0.00003922
Iteration 17/1000 | Loss: 0.00019005
Iteration 18/1000 | Loss: 0.00003489
Iteration 19/1000 | Loss: 0.00015983
Iteration 20/1000 | Loss: 0.00003222
Iteration 21/1000 | Loss: 0.00016302
Iteration 22/1000 | Loss: 0.00002949
Iteration 23/1000 | Loss: 0.00013152
Iteration 24/1000 | Loss: 0.00002761
Iteration 25/1000 | Loss: 0.00002673
Iteration 26/1000 | Loss: 0.00002586
Iteration 27/1000 | Loss: 0.00015462
Iteration 28/1000 | Loss: 0.00005527
Iteration 29/1000 | Loss: 0.00002476
Iteration 30/1000 | Loss: 0.00014744
Iteration 31/1000 | Loss: 0.00015444
Iteration 32/1000 | Loss: 0.00009723
Iteration 33/1000 | Loss: 0.00018853
Iteration 34/1000 | Loss: 0.00004123
Iteration 35/1000 | Loss: 0.00002967
Iteration 36/1000 | Loss: 0.00002460
Iteration 37/1000 | Loss: 0.00003560
Iteration 38/1000 | Loss: 0.00002564
Iteration 39/1000 | Loss: 0.00002520
Iteration 40/1000 | Loss: 0.00002296
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00030643
Iteration 43/1000 | Loss: 0.00007266
Iteration 44/1000 | Loss: 0.00002255
Iteration 45/1000 | Loss: 0.00027067
Iteration 46/1000 | Loss: 0.00004785
Iteration 47/1000 | Loss: 0.00002406
Iteration 48/1000 | Loss: 0.00011330
Iteration 49/1000 | Loss: 0.00013953
Iteration 50/1000 | Loss: 0.00009366
Iteration 51/1000 | Loss: 0.00005693
Iteration 52/1000 | Loss: 0.00004737
Iteration 53/1000 | Loss: 0.00002185
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002184
Iteration 56/1000 | Loss: 0.00002184
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00002182
Iteration 59/1000 | Loss: 0.00002172
Iteration 60/1000 | Loss: 0.00002154
Iteration 61/1000 | Loss: 0.00002154
Iteration 62/1000 | Loss: 0.00002154
Iteration 63/1000 | Loss: 0.00002154
Iteration 64/1000 | Loss: 0.00002145
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002141
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002140
Iteration 69/1000 | Loss: 0.00002140
Iteration 70/1000 | Loss: 0.00002140
Iteration 71/1000 | Loss: 0.00002140
Iteration 72/1000 | Loss: 0.00002140
Iteration 73/1000 | Loss: 0.00002138
Iteration 74/1000 | Loss: 0.00002138
Iteration 75/1000 | Loss: 0.00002137
Iteration 76/1000 | Loss: 0.00002137
Iteration 77/1000 | Loss: 0.00002136
Iteration 78/1000 | Loss: 0.00002136
Iteration 79/1000 | Loss: 0.00002135
Iteration 80/1000 | Loss: 0.00002135
Iteration 81/1000 | Loss: 0.00002132
Iteration 82/1000 | Loss: 0.00002132
Iteration 83/1000 | Loss: 0.00002131
Iteration 84/1000 | Loss: 0.00002128
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002125
Iteration 87/1000 | Loss: 0.00002123
Iteration 88/1000 | Loss: 0.00002122
Iteration 89/1000 | Loss: 0.00002122
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002121
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002119
Iteration 96/1000 | Loss: 0.00002119
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002118
Iteration 99/1000 | Loss: 0.00002118
Iteration 100/1000 | Loss: 0.00002117
Iteration 101/1000 | Loss: 0.00002117
Iteration 102/1000 | Loss: 0.00002116
Iteration 103/1000 | Loss: 0.00002116
Iteration 104/1000 | Loss: 0.00002115
Iteration 105/1000 | Loss: 0.00002115
Iteration 106/1000 | Loss: 0.00002115
Iteration 107/1000 | Loss: 0.00002114
Iteration 108/1000 | Loss: 0.00002114
Iteration 109/1000 | Loss: 0.00002113
Iteration 110/1000 | Loss: 0.00002113
Iteration 111/1000 | Loss: 0.00002113
Iteration 112/1000 | Loss: 0.00002112
Iteration 113/1000 | Loss: 0.00002112
Iteration 114/1000 | Loss: 0.00002112
Iteration 115/1000 | Loss: 0.00002112
Iteration 116/1000 | Loss: 0.00002112
Iteration 117/1000 | Loss: 0.00002112
Iteration 118/1000 | Loss: 0.00002112
Iteration 119/1000 | Loss: 0.00002112
Iteration 120/1000 | Loss: 0.00002112
Iteration 121/1000 | Loss: 0.00002111
Iteration 122/1000 | Loss: 0.00002111
Iteration 123/1000 | Loss: 0.00002111
Iteration 124/1000 | Loss: 0.00002111
Iteration 125/1000 | Loss: 0.00002111
Iteration 126/1000 | Loss: 0.00002111
Iteration 127/1000 | Loss: 0.00002111
Iteration 128/1000 | Loss: 0.00002111
Iteration 129/1000 | Loss: 0.00002111
Iteration 130/1000 | Loss: 0.00002111
Iteration 131/1000 | Loss: 0.00002111
Iteration 132/1000 | Loss: 0.00002111
Iteration 133/1000 | Loss: 0.00002111
Iteration 134/1000 | Loss: 0.00002111
Iteration 135/1000 | Loss: 0.00002111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.1106008716742508e-05, 2.1106008716742508e-05, 2.1106008716742508e-05, 2.1106008716742508e-05, 2.1106008716742508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1106008716742508e-05

Optimization complete. Final v2v error: 3.9296326637268066 mm

Highest mean error: 4.5241312980651855 mm for frame 230

Lowest mean error: 3.60308575630188 mm for frame 178

Saving results

Total time: 113.92790126800537
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845354
Iteration 2/25 | Loss: 0.00137183
Iteration 3/25 | Loss: 0.00123158
Iteration 4/25 | Loss: 0.00121691
Iteration 5/25 | Loss: 0.00121075
Iteration 6/25 | Loss: 0.00120903
Iteration 7/25 | Loss: 0.00120860
Iteration 8/25 | Loss: 0.00120860
Iteration 9/25 | Loss: 0.00120860
Iteration 10/25 | Loss: 0.00120860
Iteration 11/25 | Loss: 0.00120860
Iteration 12/25 | Loss: 0.00120860
Iteration 13/25 | Loss: 0.00120860
Iteration 14/25 | Loss: 0.00120860
Iteration 15/25 | Loss: 0.00120860
Iteration 16/25 | Loss: 0.00120860
Iteration 17/25 | Loss: 0.00120860
Iteration 18/25 | Loss: 0.00120860
Iteration 19/25 | Loss: 0.00120860
Iteration 20/25 | Loss: 0.00120860
Iteration 21/25 | Loss: 0.00120860
Iteration 22/25 | Loss: 0.00120860
Iteration 23/25 | Loss: 0.00120860
Iteration 24/25 | Loss: 0.00120860
Iteration 25/25 | Loss: 0.00120860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012085968628525734, 0.0012085968628525734, 0.0012085968628525734, 0.0012085968628525734, 0.0012085968628525734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012085968628525734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37829208
Iteration 2/25 | Loss: 0.00095769
Iteration 3/25 | Loss: 0.00095769
Iteration 4/25 | Loss: 0.00095769
Iteration 5/25 | Loss: 0.00095769
Iteration 6/25 | Loss: 0.00095769
Iteration 7/25 | Loss: 0.00095769
Iteration 8/25 | Loss: 0.00095769
Iteration 9/25 | Loss: 0.00095769
Iteration 10/25 | Loss: 0.00095769
Iteration 11/25 | Loss: 0.00095769
Iteration 12/25 | Loss: 0.00095769
Iteration 13/25 | Loss: 0.00095769
Iteration 14/25 | Loss: 0.00095769
Iteration 15/25 | Loss: 0.00095769
Iteration 16/25 | Loss: 0.00095769
Iteration 17/25 | Loss: 0.00095769
Iteration 18/25 | Loss: 0.00095769
Iteration 19/25 | Loss: 0.00095769
Iteration 20/25 | Loss: 0.00095769
Iteration 21/25 | Loss: 0.00095769
Iteration 22/25 | Loss: 0.00095769
Iteration 23/25 | Loss: 0.00095769
Iteration 24/25 | Loss: 0.00095769
Iteration 25/25 | Loss: 0.00095769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009576853481121361, 0.0009576853481121361, 0.0009576853481121361, 0.0009576853481121361, 0.0009576853481121361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009576853481121361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095769
Iteration 2/1000 | Loss: 0.00005722
Iteration 3/1000 | Loss: 0.00003425
Iteration 4/1000 | Loss: 0.00002468
Iteration 5/1000 | Loss: 0.00002211
Iteration 6/1000 | Loss: 0.00002086
Iteration 7/1000 | Loss: 0.00001987
Iteration 8/1000 | Loss: 0.00001927
Iteration 9/1000 | Loss: 0.00001889
Iteration 10/1000 | Loss: 0.00001849
Iteration 11/1000 | Loss: 0.00001813
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001774
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001740
Iteration 17/1000 | Loss: 0.00001737
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001724
Iteration 31/1000 | Loss: 0.00001723
Iteration 32/1000 | Loss: 0.00001723
Iteration 33/1000 | Loss: 0.00001721
Iteration 34/1000 | Loss: 0.00001719
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001714
Iteration 41/1000 | Loss: 0.00001710
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001709
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001702
Iteration 59/1000 | Loss: 0.00001702
Iteration 60/1000 | Loss: 0.00001702
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001701
Iteration 68/1000 | Loss: 0.00001701
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001696
Iteration 88/1000 | Loss: 0.00001696
Iteration 89/1000 | Loss: 0.00001696
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001695
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001695
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001694
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001694
Iteration 124/1000 | Loss: 0.00001694
Iteration 125/1000 | Loss: 0.00001694
Iteration 126/1000 | Loss: 0.00001694
Iteration 127/1000 | Loss: 0.00001693
Iteration 128/1000 | Loss: 0.00001693
Iteration 129/1000 | Loss: 0.00001693
Iteration 130/1000 | Loss: 0.00001693
Iteration 131/1000 | Loss: 0.00001693
Iteration 132/1000 | Loss: 0.00001693
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001693
Iteration 135/1000 | Loss: 0.00001693
Iteration 136/1000 | Loss: 0.00001693
Iteration 137/1000 | Loss: 0.00001693
Iteration 138/1000 | Loss: 0.00001692
Iteration 139/1000 | Loss: 0.00001692
Iteration 140/1000 | Loss: 0.00001692
Iteration 141/1000 | Loss: 0.00001692
Iteration 142/1000 | Loss: 0.00001692
Iteration 143/1000 | Loss: 0.00001692
Iteration 144/1000 | Loss: 0.00001691
Iteration 145/1000 | Loss: 0.00001691
Iteration 146/1000 | Loss: 0.00001691
Iteration 147/1000 | Loss: 0.00001691
Iteration 148/1000 | Loss: 0.00001691
Iteration 149/1000 | Loss: 0.00001691
Iteration 150/1000 | Loss: 0.00001691
Iteration 151/1000 | Loss: 0.00001691
Iteration 152/1000 | Loss: 0.00001691
Iteration 153/1000 | Loss: 0.00001691
Iteration 154/1000 | Loss: 0.00001691
Iteration 155/1000 | Loss: 0.00001690
Iteration 156/1000 | Loss: 0.00001690
Iteration 157/1000 | Loss: 0.00001690
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001690
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001690
Iteration 162/1000 | Loss: 0.00001690
Iteration 163/1000 | Loss: 0.00001690
Iteration 164/1000 | Loss: 0.00001690
Iteration 165/1000 | Loss: 0.00001689
Iteration 166/1000 | Loss: 0.00001689
Iteration 167/1000 | Loss: 0.00001689
Iteration 168/1000 | Loss: 0.00001689
Iteration 169/1000 | Loss: 0.00001689
Iteration 170/1000 | Loss: 0.00001688
Iteration 171/1000 | Loss: 0.00001688
Iteration 172/1000 | Loss: 0.00001688
Iteration 173/1000 | Loss: 0.00001688
Iteration 174/1000 | Loss: 0.00001688
Iteration 175/1000 | Loss: 0.00001688
Iteration 176/1000 | Loss: 0.00001688
Iteration 177/1000 | Loss: 0.00001687
Iteration 178/1000 | Loss: 0.00001687
Iteration 179/1000 | Loss: 0.00001687
Iteration 180/1000 | Loss: 0.00001687
Iteration 181/1000 | Loss: 0.00001687
Iteration 182/1000 | Loss: 0.00001687
Iteration 183/1000 | Loss: 0.00001687
Iteration 184/1000 | Loss: 0.00001687
Iteration 185/1000 | Loss: 0.00001687
Iteration 186/1000 | Loss: 0.00001687
Iteration 187/1000 | Loss: 0.00001687
Iteration 188/1000 | Loss: 0.00001686
Iteration 189/1000 | Loss: 0.00001686
Iteration 190/1000 | Loss: 0.00001686
Iteration 191/1000 | Loss: 0.00001686
Iteration 192/1000 | Loss: 0.00001686
Iteration 193/1000 | Loss: 0.00001686
Iteration 194/1000 | Loss: 0.00001686
Iteration 195/1000 | Loss: 0.00001686
Iteration 196/1000 | Loss: 0.00001686
Iteration 197/1000 | Loss: 0.00001686
Iteration 198/1000 | Loss: 0.00001685
Iteration 199/1000 | Loss: 0.00001685
Iteration 200/1000 | Loss: 0.00001685
Iteration 201/1000 | Loss: 0.00001685
Iteration 202/1000 | Loss: 0.00001685
Iteration 203/1000 | Loss: 0.00001684
Iteration 204/1000 | Loss: 0.00001684
Iteration 205/1000 | Loss: 0.00001684
Iteration 206/1000 | Loss: 0.00001684
Iteration 207/1000 | Loss: 0.00001684
Iteration 208/1000 | Loss: 0.00001684
Iteration 209/1000 | Loss: 0.00001684
Iteration 210/1000 | Loss: 0.00001684
Iteration 211/1000 | Loss: 0.00001684
Iteration 212/1000 | Loss: 0.00001683
Iteration 213/1000 | Loss: 0.00001683
Iteration 214/1000 | Loss: 0.00001683
Iteration 215/1000 | Loss: 0.00001683
Iteration 216/1000 | Loss: 0.00001682
Iteration 217/1000 | Loss: 0.00001682
Iteration 218/1000 | Loss: 0.00001682
Iteration 219/1000 | Loss: 0.00001682
Iteration 220/1000 | Loss: 0.00001682
Iteration 221/1000 | Loss: 0.00001682
Iteration 222/1000 | Loss: 0.00001682
Iteration 223/1000 | Loss: 0.00001682
Iteration 224/1000 | Loss: 0.00001682
Iteration 225/1000 | Loss: 0.00001682
Iteration 226/1000 | Loss: 0.00001682
Iteration 227/1000 | Loss: 0.00001682
Iteration 228/1000 | Loss: 0.00001682
Iteration 229/1000 | Loss: 0.00001682
Iteration 230/1000 | Loss: 0.00001682
Iteration 231/1000 | Loss: 0.00001682
Iteration 232/1000 | Loss: 0.00001682
Iteration 233/1000 | Loss: 0.00001682
Iteration 234/1000 | Loss: 0.00001682
Iteration 235/1000 | Loss: 0.00001682
Iteration 236/1000 | Loss: 0.00001682
Iteration 237/1000 | Loss: 0.00001681
Iteration 238/1000 | Loss: 0.00001681
Iteration 239/1000 | Loss: 0.00001681
Iteration 240/1000 | Loss: 0.00001681
Iteration 241/1000 | Loss: 0.00001681
Iteration 242/1000 | Loss: 0.00001681
Iteration 243/1000 | Loss: 0.00001681
Iteration 244/1000 | Loss: 0.00001681
Iteration 245/1000 | Loss: 0.00001681
Iteration 246/1000 | Loss: 0.00001681
Iteration 247/1000 | Loss: 0.00001681
Iteration 248/1000 | Loss: 0.00001681
Iteration 249/1000 | Loss: 0.00001681
Iteration 250/1000 | Loss: 0.00001681
Iteration 251/1000 | Loss: 0.00001681
Iteration 252/1000 | Loss: 0.00001681
Iteration 253/1000 | Loss: 0.00001681
Iteration 254/1000 | Loss: 0.00001681
Iteration 255/1000 | Loss: 0.00001681
Iteration 256/1000 | Loss: 0.00001681
Iteration 257/1000 | Loss: 0.00001681
Iteration 258/1000 | Loss: 0.00001681
Iteration 259/1000 | Loss: 0.00001681
Iteration 260/1000 | Loss: 0.00001681
Iteration 261/1000 | Loss: 0.00001681
Iteration 262/1000 | Loss: 0.00001681
Iteration 263/1000 | Loss: 0.00001681
Iteration 264/1000 | Loss: 0.00001681
Iteration 265/1000 | Loss: 0.00001681
Iteration 266/1000 | Loss: 0.00001681
Iteration 267/1000 | Loss: 0.00001681
Iteration 268/1000 | Loss: 0.00001681
Iteration 269/1000 | Loss: 0.00001681
Iteration 270/1000 | Loss: 0.00001681
Iteration 271/1000 | Loss: 0.00001681
Iteration 272/1000 | Loss: 0.00001681
Iteration 273/1000 | Loss: 0.00001681
Iteration 274/1000 | Loss: 0.00001681
Iteration 275/1000 | Loss: 0.00001681
Iteration 276/1000 | Loss: 0.00001681
Iteration 277/1000 | Loss: 0.00001681
Iteration 278/1000 | Loss: 0.00001681
Iteration 279/1000 | Loss: 0.00001681
Iteration 280/1000 | Loss: 0.00001681
Iteration 281/1000 | Loss: 0.00001681
Iteration 282/1000 | Loss: 0.00001681
Iteration 283/1000 | Loss: 0.00001681
Iteration 284/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.6810248780529946e-05, 1.6810248780529946e-05, 1.6810248780529946e-05, 1.6810248780529946e-05, 1.6810248780529946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6810248780529946e-05

Optimization complete. Final v2v error: 3.393415927886963 mm

Highest mean error: 5.56074333190918 mm for frame 70

Lowest mean error: 2.630140781402588 mm for frame 126

Saving results

Total time: 50.50664520263672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483945
Iteration 2/25 | Loss: 0.00135642
Iteration 3/25 | Loss: 0.00120443
Iteration 4/25 | Loss: 0.00118704
Iteration 5/25 | Loss: 0.00117944
Iteration 6/25 | Loss: 0.00117814
Iteration 7/25 | Loss: 0.00117814
Iteration 8/25 | Loss: 0.00117814
Iteration 9/25 | Loss: 0.00117814
Iteration 10/25 | Loss: 0.00117814
Iteration 11/25 | Loss: 0.00117814
Iteration 12/25 | Loss: 0.00117814
Iteration 13/25 | Loss: 0.00117814
Iteration 14/25 | Loss: 0.00117814
Iteration 15/25 | Loss: 0.00117814
Iteration 16/25 | Loss: 0.00117814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011781409848481417, 0.0011781409848481417, 0.0011781409848481417, 0.0011781409848481417, 0.0011781409848481417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011781409848481417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79614842
Iteration 2/25 | Loss: 0.00086257
Iteration 3/25 | Loss: 0.00086257
Iteration 4/25 | Loss: 0.00086257
Iteration 5/25 | Loss: 0.00086257
Iteration 6/25 | Loss: 0.00086257
Iteration 7/25 | Loss: 0.00086257
Iteration 8/25 | Loss: 0.00086257
Iteration 9/25 | Loss: 0.00086257
Iteration 10/25 | Loss: 0.00086257
Iteration 11/25 | Loss: 0.00086257
Iteration 12/25 | Loss: 0.00086257
Iteration 13/25 | Loss: 0.00086257
Iteration 14/25 | Loss: 0.00086257
Iteration 15/25 | Loss: 0.00086257
Iteration 16/25 | Loss: 0.00086257
Iteration 17/25 | Loss: 0.00086257
Iteration 18/25 | Loss: 0.00086257
Iteration 19/25 | Loss: 0.00086257
Iteration 20/25 | Loss: 0.00086257
Iteration 21/25 | Loss: 0.00086257
Iteration 22/25 | Loss: 0.00086257
Iteration 23/25 | Loss: 0.00086257
Iteration 24/25 | Loss: 0.00086257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008625690825283527, 0.0008625690825283527, 0.0008625690825283527, 0.0008625690825283527, 0.0008625690825283527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008625690825283527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086257
Iteration 2/1000 | Loss: 0.00003806
Iteration 3/1000 | Loss: 0.00002546
Iteration 4/1000 | Loss: 0.00002323
Iteration 5/1000 | Loss: 0.00002207
Iteration 6/1000 | Loss: 0.00002123
Iteration 7/1000 | Loss: 0.00002054
Iteration 8/1000 | Loss: 0.00002018
Iteration 9/1000 | Loss: 0.00001974
Iteration 10/1000 | Loss: 0.00001939
Iteration 11/1000 | Loss: 0.00001913
Iteration 12/1000 | Loss: 0.00001899
Iteration 13/1000 | Loss: 0.00001884
Iteration 14/1000 | Loss: 0.00001868
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001864
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001862
Iteration 19/1000 | Loss: 0.00001849
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001836
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001827
Iteration 25/1000 | Loss: 0.00001826
Iteration 26/1000 | Loss: 0.00001826
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001817
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001806
Iteration 32/1000 | Loss: 0.00001806
Iteration 33/1000 | Loss: 0.00001806
Iteration 34/1000 | Loss: 0.00001805
Iteration 35/1000 | Loss: 0.00001801
Iteration 36/1000 | Loss: 0.00001800
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001797
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001792
Iteration 42/1000 | Loss: 0.00001792
Iteration 43/1000 | Loss: 0.00001791
Iteration 44/1000 | Loss: 0.00001791
Iteration 45/1000 | Loss: 0.00001791
Iteration 46/1000 | Loss: 0.00001791
Iteration 47/1000 | Loss: 0.00001790
Iteration 48/1000 | Loss: 0.00001789
Iteration 49/1000 | Loss: 0.00001788
Iteration 50/1000 | Loss: 0.00001788
Iteration 51/1000 | Loss: 0.00001787
Iteration 52/1000 | Loss: 0.00001787
Iteration 53/1000 | Loss: 0.00001787
Iteration 54/1000 | Loss: 0.00001787
Iteration 55/1000 | Loss: 0.00001787
Iteration 56/1000 | Loss: 0.00001786
Iteration 57/1000 | Loss: 0.00001786
Iteration 58/1000 | Loss: 0.00001786
Iteration 59/1000 | Loss: 0.00001786
Iteration 60/1000 | Loss: 0.00001786
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001786
Iteration 64/1000 | Loss: 0.00001786
Iteration 65/1000 | Loss: 0.00001785
Iteration 66/1000 | Loss: 0.00001785
Iteration 67/1000 | Loss: 0.00001785
Iteration 68/1000 | Loss: 0.00001785
Iteration 69/1000 | Loss: 0.00001785
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001785
Iteration 74/1000 | Loss: 0.00001785
Iteration 75/1000 | Loss: 0.00001785
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001784
Iteration 80/1000 | Loss: 0.00001784
Iteration 81/1000 | Loss: 0.00001784
Iteration 82/1000 | Loss: 0.00001784
Iteration 83/1000 | Loss: 0.00001784
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001783
Iteration 86/1000 | Loss: 0.00001783
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001781
Iteration 91/1000 | Loss: 0.00001781
Iteration 92/1000 | Loss: 0.00001780
Iteration 93/1000 | Loss: 0.00001780
Iteration 94/1000 | Loss: 0.00001779
Iteration 95/1000 | Loss: 0.00001779
Iteration 96/1000 | Loss: 0.00001779
Iteration 97/1000 | Loss: 0.00001779
Iteration 98/1000 | Loss: 0.00001779
Iteration 99/1000 | Loss: 0.00001779
Iteration 100/1000 | Loss: 0.00001779
Iteration 101/1000 | Loss: 0.00001779
Iteration 102/1000 | Loss: 0.00001778
Iteration 103/1000 | Loss: 0.00001778
Iteration 104/1000 | Loss: 0.00001777
Iteration 105/1000 | Loss: 0.00001776
Iteration 106/1000 | Loss: 0.00001775
Iteration 107/1000 | Loss: 0.00001775
Iteration 108/1000 | Loss: 0.00001774
Iteration 109/1000 | Loss: 0.00001774
Iteration 110/1000 | Loss: 0.00001773
Iteration 111/1000 | Loss: 0.00001773
Iteration 112/1000 | Loss: 0.00001772
Iteration 113/1000 | Loss: 0.00001772
Iteration 114/1000 | Loss: 0.00001772
Iteration 115/1000 | Loss: 0.00001772
Iteration 116/1000 | Loss: 0.00001772
Iteration 117/1000 | Loss: 0.00001771
Iteration 118/1000 | Loss: 0.00001771
Iteration 119/1000 | Loss: 0.00001771
Iteration 120/1000 | Loss: 0.00001770
Iteration 121/1000 | Loss: 0.00001770
Iteration 122/1000 | Loss: 0.00001769
Iteration 123/1000 | Loss: 0.00001769
Iteration 124/1000 | Loss: 0.00001769
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001768
Iteration 127/1000 | Loss: 0.00001768
Iteration 128/1000 | Loss: 0.00001768
Iteration 129/1000 | Loss: 0.00001768
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001767
Iteration 135/1000 | Loss: 0.00001767
Iteration 136/1000 | Loss: 0.00001766
Iteration 137/1000 | Loss: 0.00001766
Iteration 138/1000 | Loss: 0.00001766
Iteration 139/1000 | Loss: 0.00001766
Iteration 140/1000 | Loss: 0.00001766
Iteration 141/1000 | Loss: 0.00001765
Iteration 142/1000 | Loss: 0.00001765
Iteration 143/1000 | Loss: 0.00001765
Iteration 144/1000 | Loss: 0.00001765
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001763
Iteration 151/1000 | Loss: 0.00001763
Iteration 152/1000 | Loss: 0.00001763
Iteration 153/1000 | Loss: 0.00001763
Iteration 154/1000 | Loss: 0.00001763
Iteration 155/1000 | Loss: 0.00001763
Iteration 156/1000 | Loss: 0.00001763
Iteration 157/1000 | Loss: 0.00001762
Iteration 158/1000 | Loss: 0.00001762
Iteration 159/1000 | Loss: 0.00001762
Iteration 160/1000 | Loss: 0.00001762
Iteration 161/1000 | Loss: 0.00001762
Iteration 162/1000 | Loss: 0.00001762
Iteration 163/1000 | Loss: 0.00001762
Iteration 164/1000 | Loss: 0.00001762
Iteration 165/1000 | Loss: 0.00001762
Iteration 166/1000 | Loss: 0.00001762
Iteration 167/1000 | Loss: 0.00001762
Iteration 168/1000 | Loss: 0.00001761
Iteration 169/1000 | Loss: 0.00001761
Iteration 170/1000 | Loss: 0.00001761
Iteration 171/1000 | Loss: 0.00001761
Iteration 172/1000 | Loss: 0.00001761
Iteration 173/1000 | Loss: 0.00001761
Iteration 174/1000 | Loss: 0.00001761
Iteration 175/1000 | Loss: 0.00001761
Iteration 176/1000 | Loss: 0.00001761
Iteration 177/1000 | Loss: 0.00001760
Iteration 178/1000 | Loss: 0.00001760
Iteration 179/1000 | Loss: 0.00001760
Iteration 180/1000 | Loss: 0.00001760
Iteration 181/1000 | Loss: 0.00001760
Iteration 182/1000 | Loss: 0.00001760
Iteration 183/1000 | Loss: 0.00001760
Iteration 184/1000 | Loss: 0.00001760
Iteration 185/1000 | Loss: 0.00001760
Iteration 186/1000 | Loss: 0.00001760
Iteration 187/1000 | Loss: 0.00001760
Iteration 188/1000 | Loss: 0.00001760
Iteration 189/1000 | Loss: 0.00001759
Iteration 190/1000 | Loss: 0.00001759
Iteration 191/1000 | Loss: 0.00001759
Iteration 192/1000 | Loss: 0.00001759
Iteration 193/1000 | Loss: 0.00001759
Iteration 194/1000 | Loss: 0.00001759
Iteration 195/1000 | Loss: 0.00001759
Iteration 196/1000 | Loss: 0.00001759
Iteration 197/1000 | Loss: 0.00001759
Iteration 198/1000 | Loss: 0.00001759
Iteration 199/1000 | Loss: 0.00001759
Iteration 200/1000 | Loss: 0.00001759
Iteration 201/1000 | Loss: 0.00001759
Iteration 202/1000 | Loss: 0.00001759
Iteration 203/1000 | Loss: 0.00001759
Iteration 204/1000 | Loss: 0.00001759
Iteration 205/1000 | Loss: 0.00001759
Iteration 206/1000 | Loss: 0.00001759
Iteration 207/1000 | Loss: 0.00001759
Iteration 208/1000 | Loss: 0.00001759
Iteration 209/1000 | Loss: 0.00001759
Iteration 210/1000 | Loss: 0.00001759
Iteration 211/1000 | Loss: 0.00001759
Iteration 212/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.7594948076293804e-05, 1.7594948076293804e-05, 1.7594948076293804e-05, 1.7594948076293804e-05, 1.7594948076293804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7594948076293804e-05

Optimization complete. Final v2v error: 3.5473384857177734 mm

Highest mean error: 4.25927734375 mm for frame 2

Lowest mean error: 3.3500723838806152 mm for frame 33

Saving results

Total time: 57.54667568206787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537881
Iteration 2/25 | Loss: 0.00142154
Iteration 3/25 | Loss: 0.00125018
Iteration 4/25 | Loss: 0.00124101
Iteration 5/25 | Loss: 0.00123917
Iteration 6/25 | Loss: 0.00123884
Iteration 7/25 | Loss: 0.00123884
Iteration 8/25 | Loss: 0.00123884
Iteration 9/25 | Loss: 0.00123884
Iteration 10/25 | Loss: 0.00123884
Iteration 11/25 | Loss: 0.00123884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012388444738462567, 0.0012388444738462567, 0.0012388444738462567, 0.0012388444738462567, 0.0012388444738462567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012388444738462567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77793288
Iteration 2/25 | Loss: 0.00083962
Iteration 3/25 | Loss: 0.00083962
Iteration 4/25 | Loss: 0.00083962
Iteration 5/25 | Loss: 0.00083962
Iteration 6/25 | Loss: 0.00083961
Iteration 7/25 | Loss: 0.00083961
Iteration 8/25 | Loss: 0.00083961
Iteration 9/25 | Loss: 0.00083961
Iteration 10/25 | Loss: 0.00083961
Iteration 11/25 | Loss: 0.00083961
Iteration 12/25 | Loss: 0.00083961
Iteration 13/25 | Loss: 0.00083961
Iteration 14/25 | Loss: 0.00083961
Iteration 15/25 | Loss: 0.00083961
Iteration 16/25 | Loss: 0.00083961
Iteration 17/25 | Loss: 0.00083961
Iteration 18/25 | Loss: 0.00083961
Iteration 19/25 | Loss: 0.00083961
Iteration 20/25 | Loss: 0.00083961
Iteration 21/25 | Loss: 0.00083961
Iteration 22/25 | Loss: 0.00083961
Iteration 23/25 | Loss: 0.00083961
Iteration 24/25 | Loss: 0.00083961
Iteration 25/25 | Loss: 0.00083961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083961
Iteration 2/1000 | Loss: 0.00003441
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00001949
Iteration 5/1000 | Loss: 0.00001825
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00001714
Iteration 8/1000 | Loss: 0.00001672
Iteration 9/1000 | Loss: 0.00001651
Iteration 10/1000 | Loss: 0.00001631
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001560
Iteration 15/1000 | Loss: 0.00001548
Iteration 16/1000 | Loss: 0.00001547
Iteration 17/1000 | Loss: 0.00001547
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001532
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001530
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001497
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001488
Iteration 28/1000 | Loss: 0.00001488
Iteration 29/1000 | Loss: 0.00001487
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001478
Iteration 40/1000 | Loss: 0.00001478
Iteration 41/1000 | Loss: 0.00001478
Iteration 42/1000 | Loss: 0.00001476
Iteration 43/1000 | Loss: 0.00001476
Iteration 44/1000 | Loss: 0.00001476
Iteration 45/1000 | Loss: 0.00001476
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001475
Iteration 48/1000 | Loss: 0.00001475
Iteration 49/1000 | Loss: 0.00001475
Iteration 50/1000 | Loss: 0.00001475
Iteration 51/1000 | Loss: 0.00001474
Iteration 52/1000 | Loss: 0.00001474
Iteration 53/1000 | Loss: 0.00001474
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001472
Iteration 56/1000 | Loss: 0.00001470
Iteration 57/1000 | Loss: 0.00001470
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001470
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001468
Iteration 65/1000 | Loss: 0.00001468
Iteration 66/1000 | Loss: 0.00001467
Iteration 67/1000 | Loss: 0.00001467
Iteration 68/1000 | Loss: 0.00001466
Iteration 69/1000 | Loss: 0.00001466
Iteration 70/1000 | Loss: 0.00001466
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001463
Iteration 74/1000 | Loss: 0.00001462
Iteration 75/1000 | Loss: 0.00001462
Iteration 76/1000 | Loss: 0.00001462
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001457
Iteration 89/1000 | Loss: 0.00001457
Iteration 90/1000 | Loss: 0.00001457
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001455
Iteration 97/1000 | Loss: 0.00001455
Iteration 98/1000 | Loss: 0.00001455
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001453
Iteration 107/1000 | Loss: 0.00001453
Iteration 108/1000 | Loss: 0.00001453
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001452
Iteration 112/1000 | Loss: 0.00001452
Iteration 113/1000 | Loss: 0.00001452
Iteration 114/1000 | Loss: 0.00001452
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001451
Iteration 122/1000 | Loss: 0.00001451
Iteration 123/1000 | Loss: 0.00001450
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001450
Iteration 128/1000 | Loss: 0.00001450
Iteration 129/1000 | Loss: 0.00001450
Iteration 130/1000 | Loss: 0.00001450
Iteration 131/1000 | Loss: 0.00001450
Iteration 132/1000 | Loss: 0.00001450
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001450
Iteration 135/1000 | Loss: 0.00001450
Iteration 136/1000 | Loss: 0.00001450
Iteration 137/1000 | Loss: 0.00001450
Iteration 138/1000 | Loss: 0.00001450
Iteration 139/1000 | Loss: 0.00001450
Iteration 140/1000 | Loss: 0.00001450
Iteration 141/1000 | Loss: 0.00001450
Iteration 142/1000 | Loss: 0.00001450
Iteration 143/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4501310943160206e-05, 1.4501310943160206e-05, 1.4501310943160206e-05, 1.4501310943160206e-05, 1.4501310943160206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4501310943160206e-05

Optimization complete. Final v2v error: 3.1683239936828613 mm

Highest mean error: 3.491835355758667 mm for frame 16

Lowest mean error: 3.029381275177002 mm for frame 0

Saving results

Total time: 45.8107271194458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763392
Iteration 2/25 | Loss: 0.00126285
Iteration 3/25 | Loss: 0.00116472
Iteration 4/25 | Loss: 0.00114876
Iteration 5/25 | Loss: 0.00114385
Iteration 6/25 | Loss: 0.00114268
Iteration 7/25 | Loss: 0.00114263
Iteration 8/25 | Loss: 0.00114263
Iteration 9/25 | Loss: 0.00114263
Iteration 10/25 | Loss: 0.00114263
Iteration 11/25 | Loss: 0.00114263
Iteration 12/25 | Loss: 0.00114263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001142630586400628, 0.001142630586400628, 0.001142630586400628, 0.001142630586400628, 0.001142630586400628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001142630586400628

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35689878
Iteration 2/25 | Loss: 0.00098800
Iteration 3/25 | Loss: 0.00098799
Iteration 4/25 | Loss: 0.00098799
Iteration 5/25 | Loss: 0.00098799
Iteration 6/25 | Loss: 0.00098799
Iteration 7/25 | Loss: 0.00098799
Iteration 8/25 | Loss: 0.00098799
Iteration 9/25 | Loss: 0.00098799
Iteration 10/25 | Loss: 0.00098798
Iteration 11/25 | Loss: 0.00098798
Iteration 12/25 | Loss: 0.00098798
Iteration 13/25 | Loss: 0.00098798
Iteration 14/25 | Loss: 0.00098798
Iteration 15/25 | Loss: 0.00098798
Iteration 16/25 | Loss: 0.00098798
Iteration 17/25 | Loss: 0.00098798
Iteration 18/25 | Loss: 0.00098798
Iteration 19/25 | Loss: 0.00098798
Iteration 20/25 | Loss: 0.00098798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009879845893010497, 0.0009879845893010497, 0.0009879845893010497, 0.0009879845893010497, 0.0009879845893010497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009879845893010497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098798
Iteration 2/1000 | Loss: 0.00003719
Iteration 3/1000 | Loss: 0.00002329
Iteration 4/1000 | Loss: 0.00001694
Iteration 5/1000 | Loss: 0.00001488
Iteration 6/1000 | Loss: 0.00001380
Iteration 7/1000 | Loss: 0.00001322
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001201
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001179
Iteration 16/1000 | Loss: 0.00001175
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001171
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001163
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001161
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001160
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001159
Iteration 35/1000 | Loss: 0.00001159
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001157
Iteration 38/1000 | Loss: 0.00001157
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001154
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001153
Iteration 43/1000 | Loss: 0.00001153
Iteration 44/1000 | Loss: 0.00001152
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001151
Iteration 47/1000 | Loss: 0.00001151
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001151
Iteration 50/1000 | Loss: 0.00001151
Iteration 51/1000 | Loss: 0.00001151
Iteration 52/1000 | Loss: 0.00001151
Iteration 53/1000 | Loss: 0.00001150
Iteration 54/1000 | Loss: 0.00001150
Iteration 55/1000 | Loss: 0.00001150
Iteration 56/1000 | Loss: 0.00001149
Iteration 57/1000 | Loss: 0.00001149
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001147
Iteration 65/1000 | Loss: 0.00001147
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001146
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001145
Iteration 81/1000 | Loss: 0.00001145
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001144
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001144
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001143
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001142
Iteration 103/1000 | Loss: 0.00001142
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001142
Iteration 108/1000 | Loss: 0.00001142
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001141
Iteration 111/1000 | Loss: 0.00001141
Iteration 112/1000 | Loss: 0.00001141
Iteration 113/1000 | Loss: 0.00001140
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001140
Iteration 116/1000 | Loss: 0.00001140
Iteration 117/1000 | Loss: 0.00001140
Iteration 118/1000 | Loss: 0.00001140
Iteration 119/1000 | Loss: 0.00001140
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001139
Iteration 125/1000 | Loss: 0.00001139
Iteration 126/1000 | Loss: 0.00001138
Iteration 127/1000 | Loss: 0.00001138
Iteration 128/1000 | Loss: 0.00001138
Iteration 129/1000 | Loss: 0.00001138
Iteration 130/1000 | Loss: 0.00001138
Iteration 131/1000 | Loss: 0.00001138
Iteration 132/1000 | Loss: 0.00001138
Iteration 133/1000 | Loss: 0.00001137
Iteration 134/1000 | Loss: 0.00001137
Iteration 135/1000 | Loss: 0.00001137
Iteration 136/1000 | Loss: 0.00001137
Iteration 137/1000 | Loss: 0.00001137
Iteration 138/1000 | Loss: 0.00001137
Iteration 139/1000 | Loss: 0.00001137
Iteration 140/1000 | Loss: 0.00001136
Iteration 141/1000 | Loss: 0.00001136
Iteration 142/1000 | Loss: 0.00001136
Iteration 143/1000 | Loss: 0.00001136
Iteration 144/1000 | Loss: 0.00001136
Iteration 145/1000 | Loss: 0.00001136
Iteration 146/1000 | Loss: 0.00001136
Iteration 147/1000 | Loss: 0.00001136
Iteration 148/1000 | Loss: 0.00001136
Iteration 149/1000 | Loss: 0.00001135
Iteration 150/1000 | Loss: 0.00001135
Iteration 151/1000 | Loss: 0.00001135
Iteration 152/1000 | Loss: 0.00001135
Iteration 153/1000 | Loss: 0.00001135
Iteration 154/1000 | Loss: 0.00001135
Iteration 155/1000 | Loss: 0.00001135
Iteration 156/1000 | Loss: 0.00001135
Iteration 157/1000 | Loss: 0.00001135
Iteration 158/1000 | Loss: 0.00001135
Iteration 159/1000 | Loss: 0.00001135
Iteration 160/1000 | Loss: 0.00001135
Iteration 161/1000 | Loss: 0.00001135
Iteration 162/1000 | Loss: 0.00001134
Iteration 163/1000 | Loss: 0.00001134
Iteration 164/1000 | Loss: 0.00001134
Iteration 165/1000 | Loss: 0.00001134
Iteration 166/1000 | Loss: 0.00001134
Iteration 167/1000 | Loss: 0.00001134
Iteration 168/1000 | Loss: 0.00001134
Iteration 169/1000 | Loss: 0.00001134
Iteration 170/1000 | Loss: 0.00001133
Iteration 171/1000 | Loss: 0.00001133
Iteration 172/1000 | Loss: 0.00001133
Iteration 173/1000 | Loss: 0.00001133
Iteration 174/1000 | Loss: 0.00001133
Iteration 175/1000 | Loss: 0.00001133
Iteration 176/1000 | Loss: 0.00001133
Iteration 177/1000 | Loss: 0.00001133
Iteration 178/1000 | Loss: 0.00001133
Iteration 179/1000 | Loss: 0.00001133
Iteration 180/1000 | Loss: 0.00001133
Iteration 181/1000 | Loss: 0.00001133
Iteration 182/1000 | Loss: 0.00001133
Iteration 183/1000 | Loss: 0.00001133
Iteration 184/1000 | Loss: 0.00001133
Iteration 185/1000 | Loss: 0.00001133
Iteration 186/1000 | Loss: 0.00001133
Iteration 187/1000 | Loss: 0.00001133
Iteration 188/1000 | Loss: 0.00001132
Iteration 189/1000 | Loss: 0.00001132
Iteration 190/1000 | Loss: 0.00001132
Iteration 191/1000 | Loss: 0.00001132
Iteration 192/1000 | Loss: 0.00001132
Iteration 193/1000 | Loss: 0.00001132
Iteration 194/1000 | Loss: 0.00001132
Iteration 195/1000 | Loss: 0.00001132
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001132
Iteration 203/1000 | Loss: 0.00001132
Iteration 204/1000 | Loss: 0.00001132
Iteration 205/1000 | Loss: 0.00001132
Iteration 206/1000 | Loss: 0.00001132
Iteration 207/1000 | Loss: 0.00001132
Iteration 208/1000 | Loss: 0.00001132
Iteration 209/1000 | Loss: 0.00001132
Iteration 210/1000 | Loss: 0.00001132
Iteration 211/1000 | Loss: 0.00001132
Iteration 212/1000 | Loss: 0.00001132
Iteration 213/1000 | Loss: 0.00001132
Iteration 214/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.1315773917885963e-05, 1.1315773917885963e-05, 1.1315773917885963e-05, 1.1315773917885963e-05, 1.1315773917885963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1315773917885963e-05

Optimization complete. Final v2v error: 2.8789069652557373 mm

Highest mean error: 3.3206357955932617 mm for frame 106

Lowest mean error: 2.5784523487091064 mm for frame 30

Saving results

Total time: 41.476964235305786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951910
Iteration 2/25 | Loss: 0.00206611
Iteration 3/25 | Loss: 0.00146755
Iteration 4/25 | Loss: 0.00145106
Iteration 5/25 | Loss: 0.00144811
Iteration 6/25 | Loss: 0.00144723
Iteration 7/25 | Loss: 0.00144723
Iteration 8/25 | Loss: 0.00144723
Iteration 9/25 | Loss: 0.00144723
Iteration 10/25 | Loss: 0.00144723
Iteration 11/25 | Loss: 0.00144723
Iteration 12/25 | Loss: 0.00144723
Iteration 13/25 | Loss: 0.00144723
Iteration 14/25 | Loss: 0.00144723
Iteration 15/25 | Loss: 0.00144723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001447233953513205, 0.001447233953513205, 0.001447233953513205, 0.001447233953513205, 0.001447233953513205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001447233953513205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66653228
Iteration 2/25 | Loss: 0.00097114
Iteration 3/25 | Loss: 0.00097114
Iteration 4/25 | Loss: 0.00097114
Iteration 5/25 | Loss: 0.00097114
Iteration 6/25 | Loss: 0.00097114
Iteration 7/25 | Loss: 0.00097114
Iteration 8/25 | Loss: 0.00097114
Iteration 9/25 | Loss: 0.00097114
Iteration 10/25 | Loss: 0.00097114
Iteration 11/25 | Loss: 0.00097114
Iteration 12/25 | Loss: 0.00097114
Iteration 13/25 | Loss: 0.00097114
Iteration 14/25 | Loss: 0.00097114
Iteration 15/25 | Loss: 0.00097114
Iteration 16/25 | Loss: 0.00097114
Iteration 17/25 | Loss: 0.00097114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009711398743093014, 0.0009711398743093014, 0.0009711398743093014, 0.0009711398743093014, 0.0009711398743093014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009711398743093014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097114
Iteration 2/1000 | Loss: 0.00008136
Iteration 3/1000 | Loss: 0.00005507
Iteration 4/1000 | Loss: 0.00004468
Iteration 5/1000 | Loss: 0.00004178
Iteration 6/1000 | Loss: 0.00004062
Iteration 7/1000 | Loss: 0.00003988
Iteration 8/1000 | Loss: 0.00003896
Iteration 9/1000 | Loss: 0.00003797
Iteration 10/1000 | Loss: 0.00003727
Iteration 11/1000 | Loss: 0.00003666
Iteration 12/1000 | Loss: 0.00003621
Iteration 13/1000 | Loss: 0.00003580
Iteration 14/1000 | Loss: 0.00003545
Iteration 15/1000 | Loss: 0.00003521
Iteration 16/1000 | Loss: 0.00003501
Iteration 17/1000 | Loss: 0.00003496
Iteration 18/1000 | Loss: 0.00003480
Iteration 19/1000 | Loss: 0.00003476
Iteration 20/1000 | Loss: 0.00003474
Iteration 21/1000 | Loss: 0.00003460
Iteration 22/1000 | Loss: 0.00003457
Iteration 23/1000 | Loss: 0.00003438
Iteration 24/1000 | Loss: 0.00003424
Iteration 25/1000 | Loss: 0.00003423
Iteration 26/1000 | Loss: 0.00003411
Iteration 27/1000 | Loss: 0.00003404
Iteration 28/1000 | Loss: 0.00003404
Iteration 29/1000 | Loss: 0.00003404
Iteration 30/1000 | Loss: 0.00003403
Iteration 31/1000 | Loss: 0.00003403
Iteration 32/1000 | Loss: 0.00003403
Iteration 33/1000 | Loss: 0.00003403
Iteration 34/1000 | Loss: 0.00003403
Iteration 35/1000 | Loss: 0.00003403
Iteration 36/1000 | Loss: 0.00003403
Iteration 37/1000 | Loss: 0.00003403
Iteration 38/1000 | Loss: 0.00003403
Iteration 39/1000 | Loss: 0.00003403
Iteration 40/1000 | Loss: 0.00003402
Iteration 41/1000 | Loss: 0.00003402
Iteration 42/1000 | Loss: 0.00003402
Iteration 43/1000 | Loss: 0.00003402
Iteration 44/1000 | Loss: 0.00003402
Iteration 45/1000 | Loss: 0.00003402
Iteration 46/1000 | Loss: 0.00003402
Iteration 47/1000 | Loss: 0.00003401
Iteration 48/1000 | Loss: 0.00003401
Iteration 49/1000 | Loss: 0.00003401
Iteration 50/1000 | Loss: 0.00003401
Iteration 51/1000 | Loss: 0.00003401
Iteration 52/1000 | Loss: 0.00003393
Iteration 53/1000 | Loss: 0.00003392
Iteration 54/1000 | Loss: 0.00003392
Iteration 55/1000 | Loss: 0.00003392
Iteration 56/1000 | Loss: 0.00003392
Iteration 57/1000 | Loss: 0.00003392
Iteration 58/1000 | Loss: 0.00003391
Iteration 59/1000 | Loss: 0.00003391
Iteration 60/1000 | Loss: 0.00003390
Iteration 61/1000 | Loss: 0.00003390
Iteration 62/1000 | Loss: 0.00003389
Iteration 63/1000 | Loss: 0.00003386
Iteration 64/1000 | Loss: 0.00003386
Iteration 65/1000 | Loss: 0.00003386
Iteration 66/1000 | Loss: 0.00003386
Iteration 67/1000 | Loss: 0.00003386
Iteration 68/1000 | Loss: 0.00003385
Iteration 69/1000 | Loss: 0.00003385
Iteration 70/1000 | Loss: 0.00003385
Iteration 71/1000 | Loss: 0.00003385
Iteration 72/1000 | Loss: 0.00003384
Iteration 73/1000 | Loss: 0.00003384
Iteration 74/1000 | Loss: 0.00003384
Iteration 75/1000 | Loss: 0.00003384
Iteration 76/1000 | Loss: 0.00003384
Iteration 77/1000 | Loss: 0.00003384
Iteration 78/1000 | Loss: 0.00003383
Iteration 79/1000 | Loss: 0.00003383
Iteration 80/1000 | Loss: 0.00003383
Iteration 81/1000 | Loss: 0.00003383
Iteration 82/1000 | Loss: 0.00003383
Iteration 83/1000 | Loss: 0.00003383
Iteration 84/1000 | Loss: 0.00003383
Iteration 85/1000 | Loss: 0.00003383
Iteration 86/1000 | Loss: 0.00003383
Iteration 87/1000 | Loss: 0.00003383
Iteration 88/1000 | Loss: 0.00003382
Iteration 89/1000 | Loss: 0.00003382
Iteration 90/1000 | Loss: 0.00003382
Iteration 91/1000 | Loss: 0.00003382
Iteration 92/1000 | Loss: 0.00003382
Iteration 93/1000 | Loss: 0.00003382
Iteration 94/1000 | Loss: 0.00003382
Iteration 95/1000 | Loss: 0.00003381
Iteration 96/1000 | Loss: 0.00003381
Iteration 97/1000 | Loss: 0.00003380
Iteration 98/1000 | Loss: 0.00003380
Iteration 99/1000 | Loss: 0.00003380
Iteration 100/1000 | Loss: 0.00003379
Iteration 101/1000 | Loss: 0.00003379
Iteration 102/1000 | Loss: 0.00003379
Iteration 103/1000 | Loss: 0.00003379
Iteration 104/1000 | Loss: 0.00003379
Iteration 105/1000 | Loss: 0.00003379
Iteration 106/1000 | Loss: 0.00003379
Iteration 107/1000 | Loss: 0.00003378
Iteration 108/1000 | Loss: 0.00003378
Iteration 109/1000 | Loss: 0.00003378
Iteration 110/1000 | Loss: 0.00003378
Iteration 111/1000 | Loss: 0.00003378
Iteration 112/1000 | Loss: 0.00003378
Iteration 113/1000 | Loss: 0.00003378
Iteration 114/1000 | Loss: 0.00003378
Iteration 115/1000 | Loss: 0.00003377
Iteration 116/1000 | Loss: 0.00003377
Iteration 117/1000 | Loss: 0.00003377
Iteration 118/1000 | Loss: 0.00003377
Iteration 119/1000 | Loss: 0.00003377
Iteration 120/1000 | Loss: 0.00003377
Iteration 121/1000 | Loss: 0.00003377
Iteration 122/1000 | Loss: 0.00003377
Iteration 123/1000 | Loss: 0.00003377
Iteration 124/1000 | Loss: 0.00003377
Iteration 125/1000 | Loss: 0.00003377
Iteration 126/1000 | Loss: 0.00003377
Iteration 127/1000 | Loss: 0.00003376
Iteration 128/1000 | Loss: 0.00003376
Iteration 129/1000 | Loss: 0.00003376
Iteration 130/1000 | Loss: 0.00003376
Iteration 131/1000 | Loss: 0.00003376
Iteration 132/1000 | Loss: 0.00003376
Iteration 133/1000 | Loss: 0.00003376
Iteration 134/1000 | Loss: 0.00003376
Iteration 135/1000 | Loss: 0.00003376
Iteration 136/1000 | Loss: 0.00003376
Iteration 137/1000 | Loss: 0.00003376
Iteration 138/1000 | Loss: 0.00003376
Iteration 139/1000 | Loss: 0.00003376
Iteration 140/1000 | Loss: 0.00003376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [3.376406311872415e-05, 3.376406311872415e-05, 3.376406311872415e-05, 3.376406311872415e-05, 3.376406311872415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.376406311872415e-05

Optimization complete. Final v2v error: 4.791192531585693 mm

Highest mean error: 5.429976940155029 mm for frame 59

Lowest mean error: 4.36667537689209 mm for frame 114

Saving results

Total time: 51.29496502876282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098611
Iteration 2/25 | Loss: 0.01098611
Iteration 3/25 | Loss: 0.00244102
Iteration 4/25 | Loss: 0.00160707
Iteration 5/25 | Loss: 0.00155467
Iteration 6/25 | Loss: 0.00171839
Iteration 7/25 | Loss: 0.00185467
Iteration 8/25 | Loss: 0.00136604
Iteration 9/25 | Loss: 0.00134416
Iteration 10/25 | Loss: 0.00133698
Iteration 11/25 | Loss: 0.00133385
Iteration 12/25 | Loss: 0.00133228
Iteration 13/25 | Loss: 0.00133103
Iteration 14/25 | Loss: 0.00133075
Iteration 15/25 | Loss: 0.00133068
Iteration 16/25 | Loss: 0.00133068
Iteration 17/25 | Loss: 0.00133068
Iteration 18/25 | Loss: 0.00133068
Iteration 19/25 | Loss: 0.00133068
Iteration 20/25 | Loss: 0.00133068
Iteration 21/25 | Loss: 0.00133068
Iteration 22/25 | Loss: 0.00133068
Iteration 23/25 | Loss: 0.00133068
Iteration 24/25 | Loss: 0.00133068
Iteration 25/25 | Loss: 0.00133067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33415997
Iteration 2/25 | Loss: 0.00100577
Iteration 3/25 | Loss: 0.00100577
Iteration 4/25 | Loss: 0.00100577
Iteration 5/25 | Loss: 0.00100577
Iteration 6/25 | Loss: 0.00100577
Iteration 7/25 | Loss: 0.00100577
Iteration 8/25 | Loss: 0.00100577
Iteration 9/25 | Loss: 0.00100577
Iteration 10/25 | Loss: 0.00100577
Iteration 11/25 | Loss: 0.00100577
Iteration 12/25 | Loss: 0.00100577
Iteration 13/25 | Loss: 0.00100577
Iteration 14/25 | Loss: 0.00100577
Iteration 15/25 | Loss: 0.00100577
Iteration 16/25 | Loss: 0.00100577
Iteration 17/25 | Loss: 0.00100577
Iteration 18/25 | Loss: 0.00100577
Iteration 19/25 | Loss: 0.00100577
Iteration 20/25 | Loss: 0.00100577
Iteration 21/25 | Loss: 0.00100577
Iteration 22/25 | Loss: 0.00100577
Iteration 23/25 | Loss: 0.00100577
Iteration 24/25 | Loss: 0.00100577
Iteration 25/25 | Loss: 0.00100577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100577
Iteration 2/1000 | Loss: 0.00005605
Iteration 3/1000 | Loss: 0.00003283
Iteration 4/1000 | Loss: 0.00002842
Iteration 5/1000 | Loss: 0.00002651
Iteration 6/1000 | Loss: 0.00002559
Iteration 7/1000 | Loss: 0.00002491
Iteration 8/1000 | Loss: 0.00011748
Iteration 9/1000 | Loss: 0.00013987
Iteration 10/1000 | Loss: 0.00002804
Iteration 11/1000 | Loss: 0.00002401
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002143
Iteration 14/1000 | Loss: 0.00002080
Iteration 15/1000 | Loss: 0.00002051
Iteration 16/1000 | Loss: 0.00002027
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00001999
Iteration 19/1000 | Loss: 0.00001980
Iteration 20/1000 | Loss: 0.00001961
Iteration 21/1000 | Loss: 0.00001948
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001946
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001944
Iteration 26/1000 | Loss: 0.00001943
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001943
Iteration 29/1000 | Loss: 0.00001943
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001942
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001942
Iteration 35/1000 | Loss: 0.00001942
Iteration 36/1000 | Loss: 0.00001942
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001942
Iteration 39/1000 | Loss: 0.00001942
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001941
Iteration 45/1000 | Loss: 0.00001940
Iteration 46/1000 | Loss: 0.00001940
Iteration 47/1000 | Loss: 0.00001940
Iteration 48/1000 | Loss: 0.00001940
Iteration 49/1000 | Loss: 0.00001940
Iteration 50/1000 | Loss: 0.00001940
Iteration 51/1000 | Loss: 0.00001940
Iteration 52/1000 | Loss: 0.00001940
Iteration 53/1000 | Loss: 0.00001940
Iteration 54/1000 | Loss: 0.00001940
Iteration 55/1000 | Loss: 0.00001940
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001940
Iteration 59/1000 | Loss: 0.00001939
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001938
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001938
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001938
Iteration 71/1000 | Loss: 0.00001938
Iteration 72/1000 | Loss: 0.00001938
Iteration 73/1000 | Loss: 0.00001938
Iteration 74/1000 | Loss: 0.00001938
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001936
Iteration 86/1000 | Loss: 0.00001936
Iteration 87/1000 | Loss: 0.00001936
Iteration 88/1000 | Loss: 0.00001936
Iteration 89/1000 | Loss: 0.00001936
Iteration 90/1000 | Loss: 0.00001936
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001935
Iteration 93/1000 | Loss: 0.00001935
Iteration 94/1000 | Loss: 0.00001935
Iteration 95/1000 | Loss: 0.00001935
Iteration 96/1000 | Loss: 0.00001935
Iteration 97/1000 | Loss: 0.00001935
Iteration 98/1000 | Loss: 0.00001935
Iteration 99/1000 | Loss: 0.00001935
Iteration 100/1000 | Loss: 0.00001935
Iteration 101/1000 | Loss: 0.00001935
Iteration 102/1000 | Loss: 0.00001935
Iteration 103/1000 | Loss: 0.00001935
Iteration 104/1000 | Loss: 0.00001935
Iteration 105/1000 | Loss: 0.00001935
Iteration 106/1000 | Loss: 0.00001935
Iteration 107/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.9350163711351342e-05, 1.9350163711351342e-05, 1.9350163711351342e-05, 1.9350163711351342e-05, 1.9350163711351342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9350163711351342e-05

Optimization complete. Final v2v error: 3.5751874446868896 mm

Highest mean error: 4.366953372955322 mm for frame 108

Lowest mean error: 3.0808024406433105 mm for frame 151

Saving results

Total time: 66.5546305179596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628770
Iteration 2/25 | Loss: 0.00146767
Iteration 3/25 | Loss: 0.00127392
Iteration 4/25 | Loss: 0.00123735
Iteration 5/25 | Loss: 0.00122102
Iteration 6/25 | Loss: 0.00122518
Iteration 7/25 | Loss: 0.00121956
Iteration 8/25 | Loss: 0.00121560
Iteration 9/25 | Loss: 0.00121516
Iteration 10/25 | Loss: 0.00121489
Iteration 11/25 | Loss: 0.00121458
Iteration 12/25 | Loss: 0.00121508
Iteration 13/25 | Loss: 0.00121699
Iteration 14/25 | Loss: 0.00121372
Iteration 15/25 | Loss: 0.00121015
Iteration 16/25 | Loss: 0.00120944
Iteration 17/25 | Loss: 0.00121365
Iteration 18/25 | Loss: 0.00120698
Iteration 19/25 | Loss: 0.00120587
Iteration 20/25 | Loss: 0.00120559
Iteration 21/25 | Loss: 0.00120554
Iteration 22/25 | Loss: 0.00120554
Iteration 23/25 | Loss: 0.00120553
Iteration 24/25 | Loss: 0.00120553
Iteration 25/25 | Loss: 0.00120553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66213679
Iteration 2/25 | Loss: 0.00122970
Iteration 3/25 | Loss: 0.00122969
Iteration 4/25 | Loss: 0.00122969
Iteration 5/25 | Loss: 0.00122969
Iteration 6/25 | Loss: 0.00122969
Iteration 7/25 | Loss: 0.00122969
Iteration 8/25 | Loss: 0.00122969
Iteration 9/25 | Loss: 0.00122969
Iteration 10/25 | Loss: 0.00122969
Iteration 11/25 | Loss: 0.00122969
Iteration 12/25 | Loss: 0.00122969
Iteration 13/25 | Loss: 0.00122969
Iteration 14/25 | Loss: 0.00122969
Iteration 15/25 | Loss: 0.00122969
Iteration 16/25 | Loss: 0.00122969
Iteration 17/25 | Loss: 0.00122969
Iteration 18/25 | Loss: 0.00122969
Iteration 19/25 | Loss: 0.00122969
Iteration 20/25 | Loss: 0.00122969
Iteration 21/25 | Loss: 0.00122969
Iteration 22/25 | Loss: 0.00122969
Iteration 23/25 | Loss: 0.00122969
Iteration 24/25 | Loss: 0.00122969
Iteration 25/25 | Loss: 0.00122969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122969
Iteration 2/1000 | Loss: 0.00006190
Iteration 3/1000 | Loss: 0.00004379
Iteration 4/1000 | Loss: 0.00003253
Iteration 5/1000 | Loss: 0.00002975
Iteration 6/1000 | Loss: 0.00002854
Iteration 7/1000 | Loss: 0.00002704
Iteration 8/1000 | Loss: 0.00021408
Iteration 9/1000 | Loss: 0.00005179
Iteration 10/1000 | Loss: 0.00002948
Iteration 11/1000 | Loss: 0.00010364
Iteration 12/1000 | Loss: 0.00002548
Iteration 13/1000 | Loss: 0.00002489
Iteration 14/1000 | Loss: 0.00002452
Iteration 15/1000 | Loss: 0.00002424
Iteration 16/1000 | Loss: 0.00002400
Iteration 17/1000 | Loss: 0.00029575
Iteration 18/1000 | Loss: 0.00003280
Iteration 19/1000 | Loss: 0.00002945
Iteration 20/1000 | Loss: 0.00011656
Iteration 21/1000 | Loss: 0.00002563
Iteration 22/1000 | Loss: 0.00005397
Iteration 23/1000 | Loss: 0.00002424
Iteration 24/1000 | Loss: 0.00002364
Iteration 25/1000 | Loss: 0.00002346
Iteration 26/1000 | Loss: 0.00002343
Iteration 27/1000 | Loss: 0.00002340
Iteration 28/1000 | Loss: 0.00002330
Iteration 29/1000 | Loss: 0.00002329
Iteration 30/1000 | Loss: 0.00002328
Iteration 31/1000 | Loss: 0.00002327
Iteration 32/1000 | Loss: 0.00002326
Iteration 33/1000 | Loss: 0.00002326
Iteration 34/1000 | Loss: 0.00002325
Iteration 35/1000 | Loss: 0.00002325
Iteration 36/1000 | Loss: 0.00002325
Iteration 37/1000 | Loss: 0.00002324
Iteration 38/1000 | Loss: 0.00002315
Iteration 39/1000 | Loss: 0.00002314
Iteration 40/1000 | Loss: 0.00002311
Iteration 41/1000 | Loss: 0.00002309
Iteration 42/1000 | Loss: 0.00002308
Iteration 43/1000 | Loss: 0.00002307
Iteration 44/1000 | Loss: 0.00002304
Iteration 45/1000 | Loss: 0.00002303
Iteration 46/1000 | Loss: 0.00002300
Iteration 47/1000 | Loss: 0.00002298
Iteration 48/1000 | Loss: 0.00002297
Iteration 49/1000 | Loss: 0.00002297
Iteration 50/1000 | Loss: 0.00002291
Iteration 51/1000 | Loss: 0.00002289
Iteration 52/1000 | Loss: 0.00002287
Iteration 53/1000 | Loss: 0.00002283
Iteration 54/1000 | Loss: 0.00002281
Iteration 55/1000 | Loss: 0.00002281
Iteration 56/1000 | Loss: 0.00002280
Iteration 57/1000 | Loss: 0.00002280
Iteration 58/1000 | Loss: 0.00002280
Iteration 59/1000 | Loss: 0.00002280
Iteration 60/1000 | Loss: 0.00002279
Iteration 61/1000 | Loss: 0.00002279
Iteration 62/1000 | Loss: 0.00002279
Iteration 63/1000 | Loss: 0.00002279
Iteration 64/1000 | Loss: 0.00002279
Iteration 65/1000 | Loss: 0.00002278
Iteration 66/1000 | Loss: 0.00002278
Iteration 67/1000 | Loss: 0.00002278
Iteration 68/1000 | Loss: 0.00002277
Iteration 69/1000 | Loss: 0.00002277
Iteration 70/1000 | Loss: 0.00002277
Iteration 71/1000 | Loss: 0.00002276
Iteration 72/1000 | Loss: 0.00002276
Iteration 73/1000 | Loss: 0.00002276
Iteration 74/1000 | Loss: 0.00002276
Iteration 75/1000 | Loss: 0.00002275
Iteration 76/1000 | Loss: 0.00002275
Iteration 77/1000 | Loss: 0.00002275
Iteration 78/1000 | Loss: 0.00002275
Iteration 79/1000 | Loss: 0.00002274
Iteration 80/1000 | Loss: 0.00002274
Iteration 81/1000 | Loss: 0.00002274
Iteration 82/1000 | Loss: 0.00002274
Iteration 83/1000 | Loss: 0.00002274
Iteration 84/1000 | Loss: 0.00002274
Iteration 85/1000 | Loss: 0.00002274
Iteration 86/1000 | Loss: 0.00002274
Iteration 87/1000 | Loss: 0.00002274
Iteration 88/1000 | Loss: 0.00002274
Iteration 89/1000 | Loss: 0.00002274
Iteration 90/1000 | Loss: 0.00002274
Iteration 91/1000 | Loss: 0.00002274
Iteration 92/1000 | Loss: 0.00002273
Iteration 93/1000 | Loss: 0.00002273
Iteration 94/1000 | Loss: 0.00002273
Iteration 95/1000 | Loss: 0.00002273
Iteration 96/1000 | Loss: 0.00002273
Iteration 97/1000 | Loss: 0.00002273
Iteration 98/1000 | Loss: 0.00002273
Iteration 99/1000 | Loss: 0.00002273
Iteration 100/1000 | Loss: 0.00002273
Iteration 101/1000 | Loss: 0.00002273
Iteration 102/1000 | Loss: 0.00002273
Iteration 103/1000 | Loss: 0.00002272
Iteration 104/1000 | Loss: 0.00002272
Iteration 105/1000 | Loss: 0.00002272
Iteration 106/1000 | Loss: 0.00002272
Iteration 107/1000 | Loss: 0.00002271
Iteration 108/1000 | Loss: 0.00002271
Iteration 109/1000 | Loss: 0.00002271
Iteration 110/1000 | Loss: 0.00002271
Iteration 111/1000 | Loss: 0.00002271
Iteration 112/1000 | Loss: 0.00002271
Iteration 113/1000 | Loss: 0.00002270
Iteration 114/1000 | Loss: 0.00002270
Iteration 115/1000 | Loss: 0.00002270
Iteration 116/1000 | Loss: 0.00002270
Iteration 117/1000 | Loss: 0.00002270
Iteration 118/1000 | Loss: 0.00002270
Iteration 119/1000 | Loss: 0.00002270
Iteration 120/1000 | Loss: 0.00002270
Iteration 121/1000 | Loss: 0.00002269
Iteration 122/1000 | Loss: 0.00002269
Iteration 123/1000 | Loss: 0.00002269
Iteration 124/1000 | Loss: 0.00002269
Iteration 125/1000 | Loss: 0.00002269
Iteration 126/1000 | Loss: 0.00002268
Iteration 127/1000 | Loss: 0.00002268
Iteration 128/1000 | Loss: 0.00002268
Iteration 129/1000 | Loss: 0.00002267
Iteration 130/1000 | Loss: 0.00002267
Iteration 131/1000 | Loss: 0.00002267
Iteration 132/1000 | Loss: 0.00002266
Iteration 133/1000 | Loss: 0.00002266
Iteration 134/1000 | Loss: 0.00002266
Iteration 135/1000 | Loss: 0.00002265
Iteration 136/1000 | Loss: 0.00002265
Iteration 137/1000 | Loss: 0.00002265
Iteration 138/1000 | Loss: 0.00002265
Iteration 139/1000 | Loss: 0.00002265
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00002265
Iteration 142/1000 | Loss: 0.00002264
Iteration 143/1000 | Loss: 0.00002264
Iteration 144/1000 | Loss: 0.00002264
Iteration 145/1000 | Loss: 0.00002264
Iteration 146/1000 | Loss: 0.00002263
Iteration 147/1000 | Loss: 0.00002262
Iteration 148/1000 | Loss: 0.00002262
Iteration 149/1000 | Loss: 0.00002262
Iteration 150/1000 | Loss: 0.00002261
Iteration 151/1000 | Loss: 0.00002261
Iteration 152/1000 | Loss: 0.00002261
Iteration 153/1000 | Loss: 0.00002261
Iteration 154/1000 | Loss: 0.00002261
Iteration 155/1000 | Loss: 0.00002260
Iteration 156/1000 | Loss: 0.00002260
Iteration 157/1000 | Loss: 0.00002260
Iteration 158/1000 | Loss: 0.00002259
Iteration 159/1000 | Loss: 0.00002259
Iteration 160/1000 | Loss: 0.00002259
Iteration 161/1000 | Loss: 0.00002258
Iteration 162/1000 | Loss: 0.00002258
Iteration 163/1000 | Loss: 0.00002258
Iteration 164/1000 | Loss: 0.00002258
Iteration 165/1000 | Loss: 0.00002258
Iteration 166/1000 | Loss: 0.00002257
Iteration 167/1000 | Loss: 0.00002257
Iteration 168/1000 | Loss: 0.00002257
Iteration 169/1000 | Loss: 0.00002257
Iteration 170/1000 | Loss: 0.00002257
Iteration 171/1000 | Loss: 0.00002257
Iteration 172/1000 | Loss: 0.00002257
Iteration 173/1000 | Loss: 0.00002257
Iteration 174/1000 | Loss: 0.00002256
Iteration 175/1000 | Loss: 0.00002256
Iteration 176/1000 | Loss: 0.00002256
Iteration 177/1000 | Loss: 0.00002256
Iteration 178/1000 | Loss: 0.00002256
Iteration 179/1000 | Loss: 0.00002256
Iteration 180/1000 | Loss: 0.00002256
Iteration 181/1000 | Loss: 0.00002256
Iteration 182/1000 | Loss: 0.00002256
Iteration 183/1000 | Loss: 0.00002256
Iteration 184/1000 | Loss: 0.00002256
Iteration 185/1000 | Loss: 0.00002255
Iteration 186/1000 | Loss: 0.00002255
Iteration 187/1000 | Loss: 0.00002255
Iteration 188/1000 | Loss: 0.00002255
Iteration 189/1000 | Loss: 0.00002255
Iteration 190/1000 | Loss: 0.00002255
Iteration 191/1000 | Loss: 0.00002255
Iteration 192/1000 | Loss: 0.00002255
Iteration 193/1000 | Loss: 0.00002255
Iteration 194/1000 | Loss: 0.00002255
Iteration 195/1000 | Loss: 0.00002254
Iteration 196/1000 | Loss: 0.00002254
Iteration 197/1000 | Loss: 0.00002254
Iteration 198/1000 | Loss: 0.00002254
Iteration 199/1000 | Loss: 0.00002254
Iteration 200/1000 | Loss: 0.00002254
Iteration 201/1000 | Loss: 0.00002254
Iteration 202/1000 | Loss: 0.00002254
Iteration 203/1000 | Loss: 0.00002254
Iteration 204/1000 | Loss: 0.00002254
Iteration 205/1000 | Loss: 0.00002254
Iteration 206/1000 | Loss: 0.00002254
Iteration 207/1000 | Loss: 0.00002253
Iteration 208/1000 | Loss: 0.00002253
Iteration 209/1000 | Loss: 0.00002253
Iteration 210/1000 | Loss: 0.00002253
Iteration 211/1000 | Loss: 0.00002253
Iteration 212/1000 | Loss: 0.00002253
Iteration 213/1000 | Loss: 0.00002253
Iteration 214/1000 | Loss: 0.00002253
Iteration 215/1000 | Loss: 0.00002253
Iteration 216/1000 | Loss: 0.00002253
Iteration 217/1000 | Loss: 0.00002253
Iteration 218/1000 | Loss: 0.00002253
Iteration 219/1000 | Loss: 0.00002253
Iteration 220/1000 | Loss: 0.00002252
Iteration 221/1000 | Loss: 0.00002252
Iteration 222/1000 | Loss: 0.00002252
Iteration 223/1000 | Loss: 0.00002252
Iteration 224/1000 | Loss: 0.00002252
Iteration 225/1000 | Loss: 0.00002252
Iteration 226/1000 | Loss: 0.00002252
Iteration 227/1000 | Loss: 0.00002252
Iteration 228/1000 | Loss: 0.00002252
Iteration 229/1000 | Loss: 0.00002252
Iteration 230/1000 | Loss: 0.00002252
Iteration 231/1000 | Loss: 0.00002252
Iteration 232/1000 | Loss: 0.00002252
Iteration 233/1000 | Loss: 0.00002251
Iteration 234/1000 | Loss: 0.00002251
Iteration 235/1000 | Loss: 0.00002251
Iteration 236/1000 | Loss: 0.00002251
Iteration 237/1000 | Loss: 0.00002251
Iteration 238/1000 | Loss: 0.00002251
Iteration 239/1000 | Loss: 0.00002251
Iteration 240/1000 | Loss: 0.00002251
Iteration 241/1000 | Loss: 0.00002251
Iteration 242/1000 | Loss: 0.00002251
Iteration 243/1000 | Loss: 0.00002251
Iteration 244/1000 | Loss: 0.00002251
Iteration 245/1000 | Loss: 0.00002251
Iteration 246/1000 | Loss: 0.00002251
Iteration 247/1000 | Loss: 0.00002251
Iteration 248/1000 | Loss: 0.00002250
Iteration 249/1000 | Loss: 0.00002250
Iteration 250/1000 | Loss: 0.00002250
Iteration 251/1000 | Loss: 0.00002250
Iteration 252/1000 | Loss: 0.00002250
Iteration 253/1000 | Loss: 0.00002250
Iteration 254/1000 | Loss: 0.00002250
Iteration 255/1000 | Loss: 0.00002250
Iteration 256/1000 | Loss: 0.00002250
Iteration 257/1000 | Loss: 0.00002250
Iteration 258/1000 | Loss: 0.00002250
Iteration 259/1000 | Loss: 0.00002250
Iteration 260/1000 | Loss: 0.00002250
Iteration 261/1000 | Loss: 0.00002250
Iteration 262/1000 | Loss: 0.00002249
Iteration 263/1000 | Loss: 0.00002249
Iteration 264/1000 | Loss: 0.00002249
Iteration 265/1000 | Loss: 0.00002249
Iteration 266/1000 | Loss: 0.00002249
Iteration 267/1000 | Loss: 0.00002249
Iteration 268/1000 | Loss: 0.00002249
Iteration 269/1000 | Loss: 0.00002249
Iteration 270/1000 | Loss: 0.00002249
Iteration 271/1000 | Loss: 0.00002249
Iteration 272/1000 | Loss: 0.00002249
Iteration 273/1000 | Loss: 0.00002249
Iteration 274/1000 | Loss: 0.00002249
Iteration 275/1000 | Loss: 0.00002249
Iteration 276/1000 | Loss: 0.00002249
Iteration 277/1000 | Loss: 0.00002249
Iteration 278/1000 | Loss: 0.00002249
Iteration 279/1000 | Loss: 0.00002249
Iteration 280/1000 | Loss: 0.00002249
Iteration 281/1000 | Loss: 0.00002249
Iteration 282/1000 | Loss: 0.00002249
Iteration 283/1000 | Loss: 0.00002249
Iteration 284/1000 | Loss: 0.00002249
Iteration 285/1000 | Loss: 0.00002249
Iteration 286/1000 | Loss: 0.00002249
Iteration 287/1000 | Loss: 0.00002249
Iteration 288/1000 | Loss: 0.00002249
Iteration 289/1000 | Loss: 0.00002249
Iteration 290/1000 | Loss: 0.00002249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [2.2487909518531524e-05, 2.2487909518531524e-05, 2.2487909518531524e-05, 2.2487909518531524e-05, 2.2487909518531524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2487909518531524e-05

Optimization complete. Final v2v error: 3.9325523376464844 mm

Highest mean error: 4.904603004455566 mm for frame 59

Lowest mean error: 3.229119062423706 mm for frame 81

Saving results

Total time: 93.06538963317871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392369
Iteration 2/25 | Loss: 0.00116795
Iteration 3/25 | Loss: 0.00111452
Iteration 4/25 | Loss: 0.00110689
Iteration 5/25 | Loss: 0.00110520
Iteration 6/25 | Loss: 0.00110520
Iteration 7/25 | Loss: 0.00110520
Iteration 8/25 | Loss: 0.00110520
Iteration 9/25 | Loss: 0.00110520
Iteration 10/25 | Loss: 0.00110520
Iteration 11/25 | Loss: 0.00110520
Iteration 12/25 | Loss: 0.00110520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001105197356082499, 0.001105197356082499, 0.001105197356082499, 0.001105197356082499, 0.001105197356082499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001105197356082499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41378188
Iteration 2/25 | Loss: 0.00081657
Iteration 3/25 | Loss: 0.00081656
Iteration 4/25 | Loss: 0.00081656
Iteration 5/25 | Loss: 0.00081656
Iteration 6/25 | Loss: 0.00081656
Iteration 7/25 | Loss: 0.00081656
Iteration 8/25 | Loss: 0.00081656
Iteration 9/25 | Loss: 0.00081656
Iteration 10/25 | Loss: 0.00081656
Iteration 11/25 | Loss: 0.00081656
Iteration 12/25 | Loss: 0.00081656
Iteration 13/25 | Loss: 0.00081656
Iteration 14/25 | Loss: 0.00081656
Iteration 15/25 | Loss: 0.00081656
Iteration 16/25 | Loss: 0.00081656
Iteration 17/25 | Loss: 0.00081656
Iteration 18/25 | Loss: 0.00081656
Iteration 19/25 | Loss: 0.00081656
Iteration 20/25 | Loss: 0.00081656
Iteration 21/25 | Loss: 0.00081656
Iteration 22/25 | Loss: 0.00081656
Iteration 23/25 | Loss: 0.00081656
Iteration 24/25 | Loss: 0.00081656
Iteration 25/25 | Loss: 0.00081656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081656
Iteration 2/1000 | Loss: 0.00001726
Iteration 3/1000 | Loss: 0.00001346
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001157
Iteration 6/1000 | Loss: 0.00001111
Iteration 7/1000 | Loss: 0.00001081
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001039
Iteration 10/1000 | Loss: 0.00001034
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001008
Iteration 14/1000 | Loss: 0.00001008
Iteration 15/1000 | Loss: 0.00001003
Iteration 16/1000 | Loss: 0.00001003
Iteration 17/1000 | Loss: 0.00001001
Iteration 18/1000 | Loss: 0.00000994
Iteration 19/1000 | Loss: 0.00000994
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000993
Iteration 22/1000 | Loss: 0.00000987
Iteration 23/1000 | Loss: 0.00000981
Iteration 24/1000 | Loss: 0.00000980
Iteration 25/1000 | Loss: 0.00000980
Iteration 26/1000 | Loss: 0.00000979
Iteration 27/1000 | Loss: 0.00000975
Iteration 28/1000 | Loss: 0.00000971
Iteration 29/1000 | Loss: 0.00000970
Iteration 30/1000 | Loss: 0.00000970
Iteration 31/1000 | Loss: 0.00000967
Iteration 32/1000 | Loss: 0.00000966
Iteration 33/1000 | Loss: 0.00000966
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000964
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000961
Iteration 41/1000 | Loss: 0.00000960
Iteration 42/1000 | Loss: 0.00000960
Iteration 43/1000 | Loss: 0.00000960
Iteration 44/1000 | Loss: 0.00000959
Iteration 45/1000 | Loss: 0.00000959
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000958
Iteration 48/1000 | Loss: 0.00000958
Iteration 49/1000 | Loss: 0.00000957
Iteration 50/1000 | Loss: 0.00000957
Iteration 51/1000 | Loss: 0.00000957
Iteration 52/1000 | Loss: 0.00000956
Iteration 53/1000 | Loss: 0.00000956
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000955
Iteration 57/1000 | Loss: 0.00000954
Iteration 58/1000 | Loss: 0.00000954
Iteration 59/1000 | Loss: 0.00000954
Iteration 60/1000 | Loss: 0.00000953
Iteration 61/1000 | Loss: 0.00000953
Iteration 62/1000 | Loss: 0.00000953
Iteration 63/1000 | Loss: 0.00000953
Iteration 64/1000 | Loss: 0.00000953
Iteration 65/1000 | Loss: 0.00000952
Iteration 66/1000 | Loss: 0.00000952
Iteration 67/1000 | Loss: 0.00000952
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000948
Iteration 70/1000 | Loss: 0.00000948
Iteration 71/1000 | Loss: 0.00000948
Iteration 72/1000 | Loss: 0.00000948
Iteration 73/1000 | Loss: 0.00000948
Iteration 74/1000 | Loss: 0.00000947
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000944
Iteration 78/1000 | Loss: 0.00000944
Iteration 79/1000 | Loss: 0.00000944
Iteration 80/1000 | Loss: 0.00000943
Iteration 81/1000 | Loss: 0.00000943
Iteration 82/1000 | Loss: 0.00000943
Iteration 83/1000 | Loss: 0.00000943
Iteration 84/1000 | Loss: 0.00000943
Iteration 85/1000 | Loss: 0.00000943
Iteration 86/1000 | Loss: 0.00000942
Iteration 87/1000 | Loss: 0.00000942
Iteration 88/1000 | Loss: 0.00000942
Iteration 89/1000 | Loss: 0.00000942
Iteration 90/1000 | Loss: 0.00000942
Iteration 91/1000 | Loss: 0.00000942
Iteration 92/1000 | Loss: 0.00000942
Iteration 93/1000 | Loss: 0.00000941
Iteration 94/1000 | Loss: 0.00000941
Iteration 95/1000 | Loss: 0.00000940
Iteration 96/1000 | Loss: 0.00000940
Iteration 97/1000 | Loss: 0.00000940
Iteration 98/1000 | Loss: 0.00000940
Iteration 99/1000 | Loss: 0.00000939
Iteration 100/1000 | Loss: 0.00000939
Iteration 101/1000 | Loss: 0.00000939
Iteration 102/1000 | Loss: 0.00000939
Iteration 103/1000 | Loss: 0.00000939
Iteration 104/1000 | Loss: 0.00000939
Iteration 105/1000 | Loss: 0.00000939
Iteration 106/1000 | Loss: 0.00000939
Iteration 107/1000 | Loss: 0.00000938
Iteration 108/1000 | Loss: 0.00000938
Iteration 109/1000 | Loss: 0.00000937
Iteration 110/1000 | Loss: 0.00000937
Iteration 111/1000 | Loss: 0.00000936
Iteration 112/1000 | Loss: 0.00000936
Iteration 113/1000 | Loss: 0.00000936
Iteration 114/1000 | Loss: 0.00000935
Iteration 115/1000 | Loss: 0.00000935
Iteration 116/1000 | Loss: 0.00000935
Iteration 117/1000 | Loss: 0.00000935
Iteration 118/1000 | Loss: 0.00000934
Iteration 119/1000 | Loss: 0.00000934
Iteration 120/1000 | Loss: 0.00000934
Iteration 121/1000 | Loss: 0.00000934
Iteration 122/1000 | Loss: 0.00000934
Iteration 123/1000 | Loss: 0.00000934
Iteration 124/1000 | Loss: 0.00000934
Iteration 125/1000 | Loss: 0.00000934
Iteration 126/1000 | Loss: 0.00000934
Iteration 127/1000 | Loss: 0.00000934
Iteration 128/1000 | Loss: 0.00000934
Iteration 129/1000 | Loss: 0.00000934
Iteration 130/1000 | Loss: 0.00000933
Iteration 131/1000 | Loss: 0.00000933
Iteration 132/1000 | Loss: 0.00000933
Iteration 133/1000 | Loss: 0.00000932
Iteration 134/1000 | Loss: 0.00000932
Iteration 135/1000 | Loss: 0.00000932
Iteration 136/1000 | Loss: 0.00000932
Iteration 137/1000 | Loss: 0.00000932
Iteration 138/1000 | Loss: 0.00000932
Iteration 139/1000 | Loss: 0.00000931
Iteration 140/1000 | Loss: 0.00000931
Iteration 141/1000 | Loss: 0.00000931
Iteration 142/1000 | Loss: 0.00000931
Iteration 143/1000 | Loss: 0.00000931
Iteration 144/1000 | Loss: 0.00000931
Iteration 145/1000 | Loss: 0.00000931
Iteration 146/1000 | Loss: 0.00000931
Iteration 147/1000 | Loss: 0.00000930
Iteration 148/1000 | Loss: 0.00000930
Iteration 149/1000 | Loss: 0.00000930
Iteration 150/1000 | Loss: 0.00000930
Iteration 151/1000 | Loss: 0.00000930
Iteration 152/1000 | Loss: 0.00000930
Iteration 153/1000 | Loss: 0.00000930
Iteration 154/1000 | Loss: 0.00000930
Iteration 155/1000 | Loss: 0.00000930
Iteration 156/1000 | Loss: 0.00000930
Iteration 157/1000 | Loss: 0.00000930
Iteration 158/1000 | Loss: 0.00000930
Iteration 159/1000 | Loss: 0.00000930
Iteration 160/1000 | Loss: 0.00000930
Iteration 161/1000 | Loss: 0.00000930
Iteration 162/1000 | Loss: 0.00000930
Iteration 163/1000 | Loss: 0.00000930
Iteration 164/1000 | Loss: 0.00000930
Iteration 165/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [9.29616635403363e-06, 9.29616635403363e-06, 9.29616635403363e-06, 9.29616635403363e-06, 9.29616635403363e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.29616635403363e-06

Optimization complete. Final v2v error: 2.647197723388672 mm

Highest mean error: 2.844390630722046 mm for frame 127

Lowest mean error: 2.541914463043213 mm for frame 30

Saving results

Total time: 42.094117403030396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885926
Iteration 2/25 | Loss: 0.00173028
Iteration 3/25 | Loss: 0.00139571
Iteration 4/25 | Loss: 0.00128659
Iteration 5/25 | Loss: 0.00132160
Iteration 6/25 | Loss: 0.00128596
Iteration 7/25 | Loss: 0.00125097
Iteration 8/25 | Loss: 0.00122425
Iteration 9/25 | Loss: 0.00120252
Iteration 10/25 | Loss: 0.00120954
Iteration 11/25 | Loss: 0.00118746
Iteration 12/25 | Loss: 0.00118450
Iteration 13/25 | Loss: 0.00116783
Iteration 14/25 | Loss: 0.00117151
Iteration 15/25 | Loss: 0.00116595
Iteration 16/25 | Loss: 0.00117018
Iteration 17/25 | Loss: 0.00116590
Iteration 18/25 | Loss: 0.00116582
Iteration 19/25 | Loss: 0.00116582
Iteration 20/25 | Loss: 0.00116582
Iteration 21/25 | Loss: 0.00116582
Iteration 22/25 | Loss: 0.00116581
Iteration 23/25 | Loss: 0.00116581
Iteration 24/25 | Loss: 0.00116581
Iteration 25/25 | Loss: 0.00116581

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39749539
Iteration 2/25 | Loss: 0.00080122
Iteration 3/25 | Loss: 0.00080122
Iteration 4/25 | Loss: 0.00080122
Iteration 5/25 | Loss: 0.00080122
Iteration 6/25 | Loss: 0.00080122
Iteration 7/25 | Loss: 0.00080122
Iteration 8/25 | Loss: 0.00080122
Iteration 9/25 | Loss: 0.00080122
Iteration 10/25 | Loss: 0.00080122
Iteration 11/25 | Loss: 0.00080122
Iteration 12/25 | Loss: 0.00080122
Iteration 13/25 | Loss: 0.00080122
Iteration 14/25 | Loss: 0.00080122
Iteration 15/25 | Loss: 0.00080122
Iteration 16/25 | Loss: 0.00080122
Iteration 17/25 | Loss: 0.00080122
Iteration 18/25 | Loss: 0.00080122
Iteration 19/25 | Loss: 0.00080122
Iteration 20/25 | Loss: 0.00080122
Iteration 21/25 | Loss: 0.00080122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000801216228865087, 0.000801216228865087, 0.000801216228865087, 0.000801216228865087, 0.000801216228865087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000801216228865087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080122
Iteration 2/1000 | Loss: 0.00003425
Iteration 3/1000 | Loss: 0.00002113
Iteration 4/1000 | Loss: 0.00001770
Iteration 5/1000 | Loss: 0.00001671
Iteration 6/1000 | Loss: 0.00007604
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00001502
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001459
Iteration 13/1000 | Loss: 0.00009703
Iteration 14/1000 | Loss: 0.00001440
Iteration 15/1000 | Loss: 0.00001431
Iteration 16/1000 | Loss: 0.00001428
Iteration 17/1000 | Loss: 0.00001427
Iteration 18/1000 | Loss: 0.00001427
Iteration 19/1000 | Loss: 0.00001426
Iteration 20/1000 | Loss: 0.00001426
Iteration 21/1000 | Loss: 0.00001425
Iteration 22/1000 | Loss: 0.00001423
Iteration 23/1000 | Loss: 0.00001422
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001411
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001403
Iteration 32/1000 | Loss: 0.00001403
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001402
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001400
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001399
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001396
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001394
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001393
Iteration 69/1000 | Loss: 0.00001392
Iteration 70/1000 | Loss: 0.00001391
Iteration 71/1000 | Loss: 0.00001391
Iteration 72/1000 | Loss: 0.00001390
Iteration 73/1000 | Loss: 0.00001390
Iteration 74/1000 | Loss: 0.00001390
Iteration 75/1000 | Loss: 0.00001389
Iteration 76/1000 | Loss: 0.00001389
Iteration 77/1000 | Loss: 0.00001389
Iteration 78/1000 | Loss: 0.00001389
Iteration 79/1000 | Loss: 0.00001389
Iteration 80/1000 | Loss: 0.00001389
Iteration 81/1000 | Loss: 0.00001388
Iteration 82/1000 | Loss: 0.00001388
Iteration 83/1000 | Loss: 0.00001388
Iteration 84/1000 | Loss: 0.00001388
Iteration 85/1000 | Loss: 0.00001388
Iteration 86/1000 | Loss: 0.00001388
Iteration 87/1000 | Loss: 0.00001387
Iteration 88/1000 | Loss: 0.00001387
Iteration 89/1000 | Loss: 0.00001387
Iteration 90/1000 | Loss: 0.00001387
Iteration 91/1000 | Loss: 0.00001387
Iteration 92/1000 | Loss: 0.00001387
Iteration 93/1000 | Loss: 0.00001387
Iteration 94/1000 | Loss: 0.00001386
Iteration 95/1000 | Loss: 0.00001386
Iteration 96/1000 | Loss: 0.00001386
Iteration 97/1000 | Loss: 0.00001386
Iteration 98/1000 | Loss: 0.00001386
Iteration 99/1000 | Loss: 0.00001386
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001385
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001384
Iteration 104/1000 | Loss: 0.00001384
Iteration 105/1000 | Loss: 0.00001383
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001382
Iteration 112/1000 | Loss: 0.00001382
Iteration 113/1000 | Loss: 0.00001382
Iteration 114/1000 | Loss: 0.00001382
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001382
Iteration 117/1000 | Loss: 0.00001382
Iteration 118/1000 | Loss: 0.00001382
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001381
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001380
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001380
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001379
Iteration 168/1000 | Loss: 0.00001379
Iteration 169/1000 | Loss: 0.00001379
Iteration 170/1000 | Loss: 0.00001378
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001378
Iteration 175/1000 | Loss: 0.00001378
Iteration 176/1000 | Loss: 0.00001378
Iteration 177/1000 | Loss: 0.00001378
Iteration 178/1000 | Loss: 0.00001378
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001377
Iteration 184/1000 | Loss: 0.00001377
Iteration 185/1000 | Loss: 0.00001377
Iteration 186/1000 | Loss: 0.00001377
Iteration 187/1000 | Loss: 0.00001377
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001377
Iteration 192/1000 | Loss: 0.00001377
Iteration 193/1000 | Loss: 0.00001377
Iteration 194/1000 | Loss: 0.00001377
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001377
Iteration 200/1000 | Loss: 0.00001377
Iteration 201/1000 | Loss: 0.00001377
Iteration 202/1000 | Loss: 0.00001377
Iteration 203/1000 | Loss: 0.00001377
Iteration 204/1000 | Loss: 0.00001377
Iteration 205/1000 | Loss: 0.00001377
Iteration 206/1000 | Loss: 0.00001377
Iteration 207/1000 | Loss: 0.00001377
Iteration 208/1000 | Loss: 0.00001377
Iteration 209/1000 | Loss: 0.00001377
Iteration 210/1000 | Loss: 0.00001377
Iteration 211/1000 | Loss: 0.00001377
Iteration 212/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.3769030374533031e-05, 1.3769030374533031e-05, 1.3769030374533031e-05, 1.3769030374533031e-05, 1.3769030374533031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3769030374533031e-05

Optimization complete. Final v2v error: 3.1798946857452393 mm

Highest mean error: 4.38987398147583 mm for frame 75

Lowest mean error: 2.9093875885009766 mm for frame 105

Saving results

Total time: 66.1333281993866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934342
Iteration 2/25 | Loss: 0.00183660
Iteration 3/25 | Loss: 0.00158959
Iteration 4/25 | Loss: 0.00155885
Iteration 5/25 | Loss: 0.00155527
Iteration 6/25 | Loss: 0.00148488
Iteration 7/25 | Loss: 0.00146678
Iteration 8/25 | Loss: 0.00146965
Iteration 9/25 | Loss: 0.00146673
Iteration 10/25 | Loss: 0.00146668
Iteration 11/25 | Loss: 0.00146293
Iteration 12/25 | Loss: 0.00147254
Iteration 13/25 | Loss: 0.00145164
Iteration 14/25 | Loss: 0.00145003
Iteration 15/25 | Loss: 0.00144958
Iteration 16/25 | Loss: 0.00144818
Iteration 17/25 | Loss: 0.00144787
Iteration 18/25 | Loss: 0.00144774
Iteration 19/25 | Loss: 0.00144763
Iteration 20/25 | Loss: 0.00144757
Iteration 21/25 | Loss: 0.00144753
Iteration 22/25 | Loss: 0.00144741
Iteration 23/25 | Loss: 0.00144707
Iteration 24/25 | Loss: 0.00144928
Iteration 25/25 | Loss: 0.00144535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87989569
Iteration 2/25 | Loss: 0.00255518
Iteration 3/25 | Loss: 0.00223664
Iteration 4/25 | Loss: 0.00223664
Iteration 5/25 | Loss: 0.00223664
Iteration 6/25 | Loss: 0.00223664
Iteration 7/25 | Loss: 0.00223663
Iteration 8/25 | Loss: 0.00223663
Iteration 9/25 | Loss: 0.00223663
Iteration 10/25 | Loss: 0.00223663
Iteration 11/25 | Loss: 0.00223663
Iteration 12/25 | Loss: 0.00223663
Iteration 13/25 | Loss: 0.00223663
Iteration 14/25 | Loss: 0.00223663
Iteration 15/25 | Loss: 0.00223663
Iteration 16/25 | Loss: 0.00223663
Iteration 17/25 | Loss: 0.00223663
Iteration 18/25 | Loss: 0.00223663
Iteration 19/25 | Loss: 0.00223663
Iteration 20/25 | Loss: 0.00223663
Iteration 21/25 | Loss: 0.00223663
Iteration 22/25 | Loss: 0.00223663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0022366330958902836, 0.0022366330958902836, 0.0022366330958902836, 0.0022366330958902836, 0.0022366330958902836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022366330958902836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223663
Iteration 2/1000 | Loss: 0.00023478
Iteration 3/1000 | Loss: 0.00217490
Iteration 4/1000 | Loss: 0.00168958
Iteration 5/1000 | Loss: 0.00261186
Iteration 6/1000 | Loss: 0.00026841
Iteration 7/1000 | Loss: 0.00071698
Iteration 8/1000 | Loss: 0.00059063
Iteration 9/1000 | Loss: 0.00038086
Iteration 10/1000 | Loss: 0.00061279
Iteration 11/1000 | Loss: 0.00071144
Iteration 12/1000 | Loss: 0.00007919
Iteration 13/1000 | Loss: 0.00006627
Iteration 14/1000 | Loss: 0.00031745
Iteration 15/1000 | Loss: 0.00006536
Iteration 16/1000 | Loss: 0.00005503
Iteration 17/1000 | Loss: 0.00101382
Iteration 18/1000 | Loss: 0.00102935
Iteration 19/1000 | Loss: 0.00106856
Iteration 20/1000 | Loss: 0.00044391
Iteration 21/1000 | Loss: 0.00005886
Iteration 22/1000 | Loss: 0.00004997
Iteration 23/1000 | Loss: 0.00004744
Iteration 24/1000 | Loss: 0.00004543
Iteration 25/1000 | Loss: 0.00038806
Iteration 26/1000 | Loss: 0.00024359
Iteration 27/1000 | Loss: 0.00005012
Iteration 28/1000 | Loss: 0.00004332
Iteration 29/1000 | Loss: 0.00033119
Iteration 30/1000 | Loss: 0.00004524
Iteration 31/1000 | Loss: 0.00003832
Iteration 32/1000 | Loss: 0.00009577
Iteration 33/1000 | Loss: 0.00003762
Iteration 34/1000 | Loss: 0.00003515
Iteration 35/1000 | Loss: 0.00003448
Iteration 36/1000 | Loss: 0.00003390
Iteration 37/1000 | Loss: 0.00026371
Iteration 38/1000 | Loss: 0.00003758
Iteration 39/1000 | Loss: 0.00003349
Iteration 40/1000 | Loss: 0.00003165
Iteration 41/1000 | Loss: 0.00003086
Iteration 42/1000 | Loss: 0.00003049
Iteration 43/1000 | Loss: 0.00003016
Iteration 44/1000 | Loss: 0.00003003
Iteration 45/1000 | Loss: 0.00003002
Iteration 46/1000 | Loss: 0.00003002
Iteration 47/1000 | Loss: 0.00002982
Iteration 48/1000 | Loss: 0.00002977
Iteration 49/1000 | Loss: 0.00002962
Iteration 50/1000 | Loss: 0.00002952
Iteration 51/1000 | Loss: 0.00002948
Iteration 52/1000 | Loss: 0.00002940
Iteration 53/1000 | Loss: 0.00002934
Iteration 54/1000 | Loss: 0.00002934
Iteration 55/1000 | Loss: 0.00002932
Iteration 56/1000 | Loss: 0.00002932
Iteration 57/1000 | Loss: 0.00002931
Iteration 58/1000 | Loss: 0.00002931
Iteration 59/1000 | Loss: 0.00002931
Iteration 60/1000 | Loss: 0.00002931
Iteration 61/1000 | Loss: 0.00002931
Iteration 62/1000 | Loss: 0.00002931
Iteration 63/1000 | Loss: 0.00002931
Iteration 64/1000 | Loss: 0.00002930
Iteration 65/1000 | Loss: 0.00002930
Iteration 66/1000 | Loss: 0.00002929
Iteration 67/1000 | Loss: 0.00002929
Iteration 68/1000 | Loss: 0.00002929
Iteration 69/1000 | Loss: 0.00002929
Iteration 70/1000 | Loss: 0.00002929
Iteration 71/1000 | Loss: 0.00002928
Iteration 72/1000 | Loss: 0.00002928
Iteration 73/1000 | Loss: 0.00002927
Iteration 74/1000 | Loss: 0.00002926
Iteration 75/1000 | Loss: 0.00002917
Iteration 76/1000 | Loss: 0.00002913
Iteration 77/1000 | Loss: 0.00002913
Iteration 78/1000 | Loss: 0.00002911
Iteration 79/1000 | Loss: 0.00002911
Iteration 80/1000 | Loss: 0.00002910
Iteration 81/1000 | Loss: 0.00002910
Iteration 82/1000 | Loss: 0.00002908
Iteration 83/1000 | Loss: 0.00002907
Iteration 84/1000 | Loss: 0.00002907
Iteration 85/1000 | Loss: 0.00002907
Iteration 86/1000 | Loss: 0.00002907
Iteration 87/1000 | Loss: 0.00002907
Iteration 88/1000 | Loss: 0.00002907
Iteration 89/1000 | Loss: 0.00002907
Iteration 90/1000 | Loss: 0.00002907
Iteration 91/1000 | Loss: 0.00002907
Iteration 92/1000 | Loss: 0.00002907
Iteration 93/1000 | Loss: 0.00002906
Iteration 94/1000 | Loss: 0.00002906
Iteration 95/1000 | Loss: 0.00002906
Iteration 96/1000 | Loss: 0.00002906
Iteration 97/1000 | Loss: 0.00002906
Iteration 98/1000 | Loss: 0.00002906
Iteration 99/1000 | Loss: 0.00002906
Iteration 100/1000 | Loss: 0.00002906
Iteration 101/1000 | Loss: 0.00002906
Iteration 102/1000 | Loss: 0.00002906
Iteration 103/1000 | Loss: 0.00002905
Iteration 104/1000 | Loss: 0.00002904
Iteration 105/1000 | Loss: 0.00002904
Iteration 106/1000 | Loss: 0.00002903
Iteration 107/1000 | Loss: 0.00002903
Iteration 108/1000 | Loss: 0.00002903
Iteration 109/1000 | Loss: 0.00002902
Iteration 110/1000 | Loss: 0.00002902
Iteration 111/1000 | Loss: 0.00002902
Iteration 112/1000 | Loss: 0.00002902
Iteration 113/1000 | Loss: 0.00002901
Iteration 114/1000 | Loss: 0.00002901
Iteration 115/1000 | Loss: 0.00002901
Iteration 116/1000 | Loss: 0.00002900
Iteration 117/1000 | Loss: 0.00002900
Iteration 118/1000 | Loss: 0.00002900
Iteration 119/1000 | Loss: 0.00002900
Iteration 120/1000 | Loss: 0.00002900
Iteration 121/1000 | Loss: 0.00002899
Iteration 122/1000 | Loss: 0.00002899
Iteration 123/1000 | Loss: 0.00002899
Iteration 124/1000 | Loss: 0.00002899
Iteration 125/1000 | Loss: 0.00002899
Iteration 126/1000 | Loss: 0.00002899
Iteration 127/1000 | Loss: 0.00002898
Iteration 128/1000 | Loss: 0.00002898
Iteration 129/1000 | Loss: 0.00002898
Iteration 130/1000 | Loss: 0.00002898
Iteration 131/1000 | Loss: 0.00002898
Iteration 132/1000 | Loss: 0.00002898
Iteration 133/1000 | Loss: 0.00002898
Iteration 134/1000 | Loss: 0.00002898
Iteration 135/1000 | Loss: 0.00002897
Iteration 136/1000 | Loss: 0.00002897
Iteration 137/1000 | Loss: 0.00002897
Iteration 138/1000 | Loss: 0.00002897
Iteration 139/1000 | Loss: 0.00002897
Iteration 140/1000 | Loss: 0.00002897
Iteration 141/1000 | Loss: 0.00002896
Iteration 142/1000 | Loss: 0.00002896
Iteration 143/1000 | Loss: 0.00002896
Iteration 144/1000 | Loss: 0.00002896
Iteration 145/1000 | Loss: 0.00002896
Iteration 146/1000 | Loss: 0.00002896
Iteration 147/1000 | Loss: 0.00002896
Iteration 148/1000 | Loss: 0.00002896
Iteration 149/1000 | Loss: 0.00002896
Iteration 150/1000 | Loss: 0.00002895
Iteration 151/1000 | Loss: 0.00002895
Iteration 152/1000 | Loss: 0.00002895
Iteration 153/1000 | Loss: 0.00002895
Iteration 154/1000 | Loss: 0.00002895
Iteration 155/1000 | Loss: 0.00002894
Iteration 156/1000 | Loss: 0.00002894
Iteration 157/1000 | Loss: 0.00002894
Iteration 158/1000 | Loss: 0.00002893
Iteration 159/1000 | Loss: 0.00002893
Iteration 160/1000 | Loss: 0.00002893
Iteration 161/1000 | Loss: 0.00002893
Iteration 162/1000 | Loss: 0.00002892
Iteration 163/1000 | Loss: 0.00002892
Iteration 164/1000 | Loss: 0.00002892
Iteration 165/1000 | Loss: 0.00002891
Iteration 166/1000 | Loss: 0.00002891
Iteration 167/1000 | Loss: 0.00002891
Iteration 168/1000 | Loss: 0.00002891
Iteration 169/1000 | Loss: 0.00002890
Iteration 170/1000 | Loss: 0.00002890
Iteration 171/1000 | Loss: 0.00002890
Iteration 172/1000 | Loss: 0.00002890
Iteration 173/1000 | Loss: 0.00002890
Iteration 174/1000 | Loss: 0.00002890
Iteration 175/1000 | Loss: 0.00002890
Iteration 176/1000 | Loss: 0.00002890
Iteration 177/1000 | Loss: 0.00002890
Iteration 178/1000 | Loss: 0.00002890
Iteration 179/1000 | Loss: 0.00002889
Iteration 180/1000 | Loss: 0.00002889
Iteration 181/1000 | Loss: 0.00002889
Iteration 182/1000 | Loss: 0.00002889
Iteration 183/1000 | Loss: 0.00002889
Iteration 184/1000 | Loss: 0.00002889
Iteration 185/1000 | Loss: 0.00002889
Iteration 186/1000 | Loss: 0.00002889
Iteration 187/1000 | Loss: 0.00002889
Iteration 188/1000 | Loss: 0.00002889
Iteration 189/1000 | Loss: 0.00002889
Iteration 190/1000 | Loss: 0.00002889
Iteration 191/1000 | Loss: 0.00002889
Iteration 192/1000 | Loss: 0.00002889
Iteration 193/1000 | Loss: 0.00002889
Iteration 194/1000 | Loss: 0.00002889
Iteration 195/1000 | Loss: 0.00002889
Iteration 196/1000 | Loss: 0.00002889
Iteration 197/1000 | Loss: 0.00002889
Iteration 198/1000 | Loss: 0.00002889
Iteration 199/1000 | Loss: 0.00002889
Iteration 200/1000 | Loss: 0.00002888
Iteration 201/1000 | Loss: 0.00002888
Iteration 202/1000 | Loss: 0.00002888
Iteration 203/1000 | Loss: 0.00002888
Iteration 204/1000 | Loss: 0.00002888
Iteration 205/1000 | Loss: 0.00002888
Iteration 206/1000 | Loss: 0.00002888
Iteration 207/1000 | Loss: 0.00002888
Iteration 208/1000 | Loss: 0.00002887
Iteration 209/1000 | Loss: 0.00002887
Iteration 210/1000 | Loss: 0.00002887
Iteration 211/1000 | Loss: 0.00002887
Iteration 212/1000 | Loss: 0.00002887
Iteration 213/1000 | Loss: 0.00002887
Iteration 214/1000 | Loss: 0.00002887
Iteration 215/1000 | Loss: 0.00002887
Iteration 216/1000 | Loss: 0.00002887
Iteration 217/1000 | Loss: 0.00002887
Iteration 218/1000 | Loss: 0.00002887
Iteration 219/1000 | Loss: 0.00002887
Iteration 220/1000 | Loss: 0.00002887
Iteration 221/1000 | Loss: 0.00002887
Iteration 222/1000 | Loss: 0.00002887
Iteration 223/1000 | Loss: 0.00002887
Iteration 224/1000 | Loss: 0.00002887
Iteration 225/1000 | Loss: 0.00002887
Iteration 226/1000 | Loss: 0.00002887
Iteration 227/1000 | Loss: 0.00002887
Iteration 228/1000 | Loss: 0.00002887
Iteration 229/1000 | Loss: 0.00002887
Iteration 230/1000 | Loss: 0.00002887
Iteration 231/1000 | Loss: 0.00002887
Iteration 232/1000 | Loss: 0.00002887
Iteration 233/1000 | Loss: 0.00002887
Iteration 234/1000 | Loss: 0.00002887
Iteration 235/1000 | Loss: 0.00002887
Iteration 236/1000 | Loss: 0.00002887
Iteration 237/1000 | Loss: 0.00002887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [2.8865784770459868e-05, 2.8865784770459868e-05, 2.8865784770459868e-05, 2.8865784770459868e-05, 2.8865784770459868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8865784770459868e-05

Optimization complete. Final v2v error: 4.43192195892334 mm

Highest mean error: 5.230547904968262 mm for frame 105

Lowest mean error: 3.310528516769409 mm for frame 208

Saving results

Total time: 147.05576300621033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994689
Iteration 2/25 | Loss: 0.00164143
Iteration 3/25 | Loss: 0.00133014
Iteration 4/25 | Loss: 0.00130530
Iteration 5/25 | Loss: 0.00129727
Iteration 6/25 | Loss: 0.00129520
Iteration 7/25 | Loss: 0.00129520
Iteration 8/25 | Loss: 0.00129520
Iteration 9/25 | Loss: 0.00129520
Iteration 10/25 | Loss: 0.00129520
Iteration 11/25 | Loss: 0.00129520
Iteration 12/25 | Loss: 0.00129520
Iteration 13/25 | Loss: 0.00129520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00129519565962255, 0.00129519565962255, 0.00129519565962255, 0.00129519565962255, 0.00129519565962255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00129519565962255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02220142
Iteration 2/25 | Loss: 0.00112380
Iteration 3/25 | Loss: 0.00112378
Iteration 4/25 | Loss: 0.00112378
Iteration 5/25 | Loss: 0.00112378
Iteration 6/25 | Loss: 0.00112378
Iteration 7/25 | Loss: 0.00112378
Iteration 8/25 | Loss: 0.00112378
Iteration 9/25 | Loss: 0.00112378
Iteration 10/25 | Loss: 0.00112378
Iteration 11/25 | Loss: 0.00112378
Iteration 12/25 | Loss: 0.00112378
Iteration 13/25 | Loss: 0.00112378
Iteration 14/25 | Loss: 0.00112378
Iteration 15/25 | Loss: 0.00112378
Iteration 16/25 | Loss: 0.00112378
Iteration 17/25 | Loss: 0.00112378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011237767757847905, 0.0011237767757847905, 0.0011237767757847905, 0.0011237767757847905, 0.0011237767757847905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011237767757847905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112378
Iteration 2/1000 | Loss: 0.00005653
Iteration 3/1000 | Loss: 0.00003456
Iteration 4/1000 | Loss: 0.00002942
Iteration 5/1000 | Loss: 0.00002755
Iteration 6/1000 | Loss: 0.00002671
Iteration 7/1000 | Loss: 0.00002593
Iteration 8/1000 | Loss: 0.00002534
Iteration 9/1000 | Loss: 0.00002490
Iteration 10/1000 | Loss: 0.00002453
Iteration 11/1000 | Loss: 0.00002428
Iteration 12/1000 | Loss: 0.00002409
Iteration 13/1000 | Loss: 0.00002407
Iteration 14/1000 | Loss: 0.00002401
Iteration 15/1000 | Loss: 0.00002385
Iteration 16/1000 | Loss: 0.00002376
Iteration 17/1000 | Loss: 0.00002374
Iteration 18/1000 | Loss: 0.00002373
Iteration 19/1000 | Loss: 0.00002370
Iteration 20/1000 | Loss: 0.00002369
Iteration 21/1000 | Loss: 0.00002369
Iteration 22/1000 | Loss: 0.00002368
Iteration 23/1000 | Loss: 0.00002367
Iteration 24/1000 | Loss: 0.00002366
Iteration 25/1000 | Loss: 0.00002366
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002361
Iteration 28/1000 | Loss: 0.00002361
Iteration 29/1000 | Loss: 0.00002361
Iteration 30/1000 | Loss: 0.00002360
Iteration 31/1000 | Loss: 0.00002360
Iteration 32/1000 | Loss: 0.00002360
Iteration 33/1000 | Loss: 0.00002359
Iteration 34/1000 | Loss: 0.00002356
Iteration 35/1000 | Loss: 0.00002351
Iteration 36/1000 | Loss: 0.00002350
Iteration 37/1000 | Loss: 0.00002344
Iteration 38/1000 | Loss: 0.00002344
Iteration 39/1000 | Loss: 0.00002343
Iteration 40/1000 | Loss: 0.00002342
Iteration 41/1000 | Loss: 0.00002341
Iteration 42/1000 | Loss: 0.00002339
Iteration 43/1000 | Loss: 0.00002336
Iteration 44/1000 | Loss: 0.00002336
Iteration 45/1000 | Loss: 0.00002335
Iteration 46/1000 | Loss: 0.00002334
Iteration 47/1000 | Loss: 0.00002334
Iteration 48/1000 | Loss: 0.00002333
Iteration 49/1000 | Loss: 0.00002333
Iteration 50/1000 | Loss: 0.00002333
Iteration 51/1000 | Loss: 0.00002331
Iteration 52/1000 | Loss: 0.00002331
Iteration 53/1000 | Loss: 0.00002330
Iteration 54/1000 | Loss: 0.00002330
Iteration 55/1000 | Loss: 0.00002330
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002328
Iteration 58/1000 | Loss: 0.00002328
Iteration 59/1000 | Loss: 0.00002327
Iteration 60/1000 | Loss: 0.00002327
Iteration 61/1000 | Loss: 0.00002326
Iteration 62/1000 | Loss: 0.00002326
Iteration 63/1000 | Loss: 0.00002326
Iteration 64/1000 | Loss: 0.00002325
Iteration 65/1000 | Loss: 0.00002325
Iteration 66/1000 | Loss: 0.00002325
Iteration 67/1000 | Loss: 0.00002324
Iteration 68/1000 | Loss: 0.00002324
Iteration 69/1000 | Loss: 0.00002324
Iteration 70/1000 | Loss: 0.00002323
Iteration 71/1000 | Loss: 0.00002323
Iteration 72/1000 | Loss: 0.00002322
Iteration 73/1000 | Loss: 0.00002322
Iteration 74/1000 | Loss: 0.00002322
Iteration 75/1000 | Loss: 0.00002321
Iteration 76/1000 | Loss: 0.00002321
Iteration 77/1000 | Loss: 0.00002321
Iteration 78/1000 | Loss: 0.00002321
Iteration 79/1000 | Loss: 0.00002321
Iteration 80/1000 | Loss: 0.00002320
Iteration 81/1000 | Loss: 0.00002320
Iteration 82/1000 | Loss: 0.00002320
Iteration 83/1000 | Loss: 0.00002319
Iteration 84/1000 | Loss: 0.00002319
Iteration 85/1000 | Loss: 0.00002319
Iteration 86/1000 | Loss: 0.00002318
Iteration 87/1000 | Loss: 0.00002318
Iteration 88/1000 | Loss: 0.00002318
Iteration 89/1000 | Loss: 0.00002318
Iteration 90/1000 | Loss: 0.00002318
Iteration 91/1000 | Loss: 0.00002318
Iteration 92/1000 | Loss: 0.00002317
Iteration 93/1000 | Loss: 0.00002317
Iteration 94/1000 | Loss: 0.00002317
Iteration 95/1000 | Loss: 0.00002317
Iteration 96/1000 | Loss: 0.00002317
Iteration 97/1000 | Loss: 0.00002317
Iteration 98/1000 | Loss: 0.00002316
Iteration 99/1000 | Loss: 0.00002316
Iteration 100/1000 | Loss: 0.00002316
Iteration 101/1000 | Loss: 0.00002316
Iteration 102/1000 | Loss: 0.00002316
Iteration 103/1000 | Loss: 0.00002315
Iteration 104/1000 | Loss: 0.00002315
Iteration 105/1000 | Loss: 0.00002315
Iteration 106/1000 | Loss: 0.00002315
Iteration 107/1000 | Loss: 0.00002314
Iteration 108/1000 | Loss: 0.00002314
Iteration 109/1000 | Loss: 0.00002314
Iteration 110/1000 | Loss: 0.00002313
Iteration 111/1000 | Loss: 0.00002313
Iteration 112/1000 | Loss: 0.00002313
Iteration 113/1000 | Loss: 0.00002313
Iteration 114/1000 | Loss: 0.00002312
Iteration 115/1000 | Loss: 0.00002312
Iteration 116/1000 | Loss: 0.00002312
Iteration 117/1000 | Loss: 0.00002312
Iteration 118/1000 | Loss: 0.00002312
Iteration 119/1000 | Loss: 0.00002312
Iteration 120/1000 | Loss: 0.00002312
Iteration 121/1000 | Loss: 0.00002312
Iteration 122/1000 | Loss: 0.00002311
Iteration 123/1000 | Loss: 0.00002311
Iteration 124/1000 | Loss: 0.00002311
Iteration 125/1000 | Loss: 0.00002311
Iteration 126/1000 | Loss: 0.00002311
Iteration 127/1000 | Loss: 0.00002310
Iteration 128/1000 | Loss: 0.00002310
Iteration 129/1000 | Loss: 0.00002310
Iteration 130/1000 | Loss: 0.00002310
Iteration 131/1000 | Loss: 0.00002310
Iteration 132/1000 | Loss: 0.00002310
Iteration 133/1000 | Loss: 0.00002310
Iteration 134/1000 | Loss: 0.00002310
Iteration 135/1000 | Loss: 0.00002310
Iteration 136/1000 | Loss: 0.00002309
Iteration 137/1000 | Loss: 0.00002309
Iteration 138/1000 | Loss: 0.00002309
Iteration 139/1000 | Loss: 0.00002309
Iteration 140/1000 | Loss: 0.00002309
Iteration 141/1000 | Loss: 0.00002309
Iteration 142/1000 | Loss: 0.00002309
Iteration 143/1000 | Loss: 0.00002309
Iteration 144/1000 | Loss: 0.00002309
Iteration 145/1000 | Loss: 0.00002309
Iteration 146/1000 | Loss: 0.00002309
Iteration 147/1000 | Loss: 0.00002309
Iteration 148/1000 | Loss: 0.00002309
Iteration 149/1000 | Loss: 0.00002309
Iteration 150/1000 | Loss: 0.00002309
Iteration 151/1000 | Loss: 0.00002308
Iteration 152/1000 | Loss: 0.00002308
Iteration 153/1000 | Loss: 0.00002308
Iteration 154/1000 | Loss: 0.00002308
Iteration 155/1000 | Loss: 0.00002308
Iteration 156/1000 | Loss: 0.00002308
Iteration 157/1000 | Loss: 0.00002308
Iteration 158/1000 | Loss: 0.00002308
Iteration 159/1000 | Loss: 0.00002308
Iteration 160/1000 | Loss: 0.00002308
Iteration 161/1000 | Loss: 0.00002308
Iteration 162/1000 | Loss: 0.00002307
Iteration 163/1000 | Loss: 0.00002307
Iteration 164/1000 | Loss: 0.00002307
Iteration 165/1000 | Loss: 0.00002307
Iteration 166/1000 | Loss: 0.00002307
Iteration 167/1000 | Loss: 0.00002307
Iteration 168/1000 | Loss: 0.00002307
Iteration 169/1000 | Loss: 0.00002307
Iteration 170/1000 | Loss: 0.00002307
Iteration 171/1000 | Loss: 0.00002307
Iteration 172/1000 | Loss: 0.00002307
Iteration 173/1000 | Loss: 0.00002307
Iteration 174/1000 | Loss: 0.00002307
Iteration 175/1000 | Loss: 0.00002307
Iteration 176/1000 | Loss: 0.00002306
Iteration 177/1000 | Loss: 0.00002306
Iteration 178/1000 | Loss: 0.00002306
Iteration 179/1000 | Loss: 0.00002306
Iteration 180/1000 | Loss: 0.00002306
Iteration 181/1000 | Loss: 0.00002306
Iteration 182/1000 | Loss: 0.00002306
Iteration 183/1000 | Loss: 0.00002306
Iteration 184/1000 | Loss: 0.00002306
Iteration 185/1000 | Loss: 0.00002306
Iteration 186/1000 | Loss: 0.00002306
Iteration 187/1000 | Loss: 0.00002306
Iteration 188/1000 | Loss: 0.00002306
Iteration 189/1000 | Loss: 0.00002306
Iteration 190/1000 | Loss: 0.00002306
Iteration 191/1000 | Loss: 0.00002306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.3064048946253024e-05, 2.3064048946253024e-05, 2.3064048946253024e-05, 2.3064048946253024e-05, 2.3064048946253024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3064048946253024e-05

Optimization complete. Final v2v error: 3.9734785556793213 mm

Highest mean error: 4.823857307434082 mm for frame 77

Lowest mean error: 3.3548450469970703 mm for frame 37

Saving results

Total time: 47.20001935958862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989939
Iteration 2/25 | Loss: 0.00989939
Iteration 3/25 | Loss: 0.00197924
Iteration 4/25 | Loss: 0.00158248
Iteration 5/25 | Loss: 0.00151379
Iteration 6/25 | Loss: 0.00153502
Iteration 7/25 | Loss: 0.00171818
Iteration 8/25 | Loss: 0.00158891
Iteration 9/25 | Loss: 0.00143676
Iteration 10/25 | Loss: 0.00139945
Iteration 11/25 | Loss: 0.00139331
Iteration 12/25 | Loss: 0.00138884
Iteration 13/25 | Loss: 0.00138265
Iteration 14/25 | Loss: 0.00138153
Iteration 15/25 | Loss: 0.00137910
Iteration 16/25 | Loss: 0.00137505
Iteration 17/25 | Loss: 0.00137165
Iteration 18/25 | Loss: 0.00137297
Iteration 19/25 | Loss: 0.00136765
Iteration 20/25 | Loss: 0.00136839
Iteration 21/25 | Loss: 0.00136508
Iteration 22/25 | Loss: 0.00136642
Iteration 23/25 | Loss: 0.00136563
Iteration 24/25 | Loss: 0.00136703
Iteration 25/25 | Loss: 0.00136321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88559020
Iteration 2/25 | Loss: 0.00109245
Iteration 3/25 | Loss: 0.00109244
Iteration 4/25 | Loss: 0.00109244
Iteration 5/25 | Loss: 0.00109244
Iteration 6/25 | Loss: 0.00109244
Iteration 7/25 | Loss: 0.00109244
Iteration 8/25 | Loss: 0.00109244
Iteration 9/25 | Loss: 0.00109244
Iteration 10/25 | Loss: 0.00109244
Iteration 11/25 | Loss: 0.00109244
Iteration 12/25 | Loss: 0.00109244
Iteration 13/25 | Loss: 0.00109244
Iteration 14/25 | Loss: 0.00109244
Iteration 15/25 | Loss: 0.00109244
Iteration 16/25 | Loss: 0.00109244
Iteration 17/25 | Loss: 0.00109244
Iteration 18/25 | Loss: 0.00109244
Iteration 19/25 | Loss: 0.00109244
Iteration 20/25 | Loss: 0.00109244
Iteration 21/25 | Loss: 0.00109244
Iteration 22/25 | Loss: 0.00109244
Iteration 23/25 | Loss: 0.00109244
Iteration 24/25 | Loss: 0.00109244
Iteration 25/25 | Loss: 0.00109244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109244
Iteration 2/1000 | Loss: 0.00012868
Iteration 3/1000 | Loss: 0.00012398
Iteration 4/1000 | Loss: 0.00011045
Iteration 5/1000 | Loss: 0.00011034
Iteration 6/1000 | Loss: 0.00011338
Iteration 7/1000 | Loss: 0.00013944
Iteration 8/1000 | Loss: 0.00010072
Iteration 9/1000 | Loss: 0.00010334
Iteration 10/1000 | Loss: 0.00009389
Iteration 11/1000 | Loss: 0.00008883
Iteration 12/1000 | Loss: 0.00010220
Iteration 13/1000 | Loss: 0.00007270
Iteration 14/1000 | Loss: 0.00008375
Iteration 15/1000 | Loss: 0.00010570
Iteration 16/1000 | Loss: 0.00007312
Iteration 17/1000 | Loss: 0.00011242
Iteration 18/1000 | Loss: 0.00009536
Iteration 19/1000 | Loss: 0.00010080
Iteration 20/1000 | Loss: 0.00009177
Iteration 21/1000 | Loss: 0.00011588
Iteration 22/1000 | Loss: 0.00016637
Iteration 23/1000 | Loss: 0.00011120
Iteration 24/1000 | Loss: 0.00011117
Iteration 25/1000 | Loss: 0.00011802
Iteration 26/1000 | Loss: 0.00011131
Iteration 27/1000 | Loss: 0.00010228
Iteration 28/1000 | Loss: 0.00011311
Iteration 29/1000 | Loss: 0.00011068
Iteration 30/1000 | Loss: 0.00007798
Iteration 31/1000 | Loss: 0.00006817
Iteration 32/1000 | Loss: 0.00011101
Iteration 33/1000 | Loss: 0.00011061
Iteration 34/1000 | Loss: 0.00011066
Iteration 35/1000 | Loss: 0.00009598
Iteration 36/1000 | Loss: 0.00012679
Iteration 37/1000 | Loss: 0.00007491
Iteration 38/1000 | Loss: 0.00007264
Iteration 39/1000 | Loss: 0.00009429
Iteration 40/1000 | Loss: 0.00006796
Iteration 41/1000 | Loss: 0.00008195
Iteration 42/1000 | Loss: 0.00008078
Iteration 43/1000 | Loss: 0.00008375
Iteration 44/1000 | Loss: 0.00007375
Iteration 45/1000 | Loss: 0.00009056
Iteration 46/1000 | Loss: 0.00011324
Iteration 47/1000 | Loss: 0.00010899
Iteration 48/1000 | Loss: 0.00009430
Iteration 49/1000 | Loss: 0.00008771
Iteration 50/1000 | Loss: 0.00008896
Iteration 51/1000 | Loss: 0.00008445
Iteration 52/1000 | Loss: 0.00009078
Iteration 53/1000 | Loss: 0.00006562
Iteration 54/1000 | Loss: 0.00007561
Iteration 55/1000 | Loss: 0.00006245
Iteration 56/1000 | Loss: 0.00008936
Iteration 57/1000 | Loss: 0.00007837
Iteration 58/1000 | Loss: 0.00008593
Iteration 59/1000 | Loss: 0.00008134
Iteration 60/1000 | Loss: 0.00008308
Iteration 61/1000 | Loss: 0.00007390
Iteration 62/1000 | Loss: 0.00007130
Iteration 63/1000 | Loss: 0.00008385
Iteration 64/1000 | Loss: 0.00006078
Iteration 65/1000 | Loss: 0.00006364
Iteration 66/1000 | Loss: 0.00006950
Iteration 67/1000 | Loss: 0.00007799
Iteration 68/1000 | Loss: 0.00007625
Iteration 69/1000 | Loss: 0.00008930
Iteration 70/1000 | Loss: 0.00009221
Iteration 71/1000 | Loss: 0.00007826
Iteration 72/1000 | Loss: 0.00005394
Iteration 73/1000 | Loss: 0.00006039
Iteration 74/1000 | Loss: 0.00007337
Iteration 75/1000 | Loss: 0.00006809
Iteration 76/1000 | Loss: 0.00006158
Iteration 77/1000 | Loss: 0.00006498
Iteration 78/1000 | Loss: 0.00006414
Iteration 79/1000 | Loss: 0.00006693
Iteration 80/1000 | Loss: 0.00004890
Iteration 81/1000 | Loss: 0.00008496
Iteration 82/1000 | Loss: 0.00006247
Iteration 83/1000 | Loss: 0.00006019
Iteration 84/1000 | Loss: 0.00006297
Iteration 85/1000 | Loss: 0.00005683
Iteration 86/1000 | Loss: 0.00005094
Iteration 87/1000 | Loss: 0.00004463
Iteration 88/1000 | Loss: 0.00004066
Iteration 89/1000 | Loss: 0.00004065
Iteration 90/1000 | Loss: 0.00005009
Iteration 91/1000 | Loss: 0.00005092
Iteration 92/1000 | Loss: 0.00004712
Iteration 93/1000 | Loss: 0.00005379
Iteration 94/1000 | Loss: 0.00004712
Iteration 95/1000 | Loss: 0.00006146
Iteration 96/1000 | Loss: 0.00005765
Iteration 97/1000 | Loss: 0.00006488
Iteration 98/1000 | Loss: 0.00005899
Iteration 99/1000 | Loss: 0.00007116
Iteration 100/1000 | Loss: 0.00006243
Iteration 101/1000 | Loss: 0.00006254
Iteration 102/1000 | Loss: 0.00004614
Iteration 103/1000 | Loss: 0.00003886
Iteration 104/1000 | Loss: 0.00004762
Iteration 105/1000 | Loss: 0.00007136
Iteration 106/1000 | Loss: 0.00006104
Iteration 107/1000 | Loss: 0.00004116
Iteration 108/1000 | Loss: 0.00006971
Iteration 109/1000 | Loss: 0.00005844
Iteration 110/1000 | Loss: 0.00006691
Iteration 111/1000 | Loss: 0.00006152
Iteration 112/1000 | Loss: 0.00006813
Iteration 113/1000 | Loss: 0.00006438
Iteration 114/1000 | Loss: 0.00007177
Iteration 115/1000 | Loss: 0.00006073
Iteration 116/1000 | Loss: 0.00007215
Iteration 117/1000 | Loss: 0.00005628
Iteration 118/1000 | Loss: 0.00007265
Iteration 119/1000 | Loss: 0.00007774
Iteration 120/1000 | Loss: 0.00007205
Iteration 121/1000 | Loss: 0.00005200
Iteration 122/1000 | Loss: 0.00004904
Iteration 123/1000 | Loss: 0.00005913
Iteration 124/1000 | Loss: 0.00005105
Iteration 125/1000 | Loss: 0.00006170
Iteration 126/1000 | Loss: 0.00006166
Iteration 127/1000 | Loss: 0.00006536
Iteration 128/1000 | Loss: 0.00005766
Iteration 129/1000 | Loss: 0.00005967
Iteration 130/1000 | Loss: 0.00005807
Iteration 131/1000 | Loss: 0.00005129
Iteration 132/1000 | Loss: 0.00005179
Iteration 133/1000 | Loss: 0.00006807
Iteration 134/1000 | Loss: 0.00005968
Iteration 135/1000 | Loss: 0.00005995
Iteration 136/1000 | Loss: 0.00005886
Iteration 137/1000 | Loss: 0.00008143
Iteration 138/1000 | Loss: 0.00006823
Iteration 139/1000 | Loss: 0.00006578
Iteration 140/1000 | Loss: 0.00005972
Iteration 141/1000 | Loss: 0.00005973
Iteration 142/1000 | Loss: 0.00007031
Iteration 143/1000 | Loss: 0.00008612
Iteration 144/1000 | Loss: 0.00006216
Iteration 145/1000 | Loss: 0.00007485
Iteration 146/1000 | Loss: 0.00007919
Iteration 147/1000 | Loss: 0.00007891
Iteration 148/1000 | Loss: 0.00007127
Iteration 149/1000 | Loss: 0.00006842
Iteration 150/1000 | Loss: 0.00007155
Iteration 151/1000 | Loss: 0.00007629
Iteration 152/1000 | Loss: 0.00007947
Iteration 153/1000 | Loss: 0.00007905
Iteration 154/1000 | Loss: 0.00007240
Iteration 155/1000 | Loss: 0.00008580
Iteration 156/1000 | Loss: 0.00006861
Iteration 157/1000 | Loss: 0.00007447
Iteration 158/1000 | Loss: 0.00008881
Iteration 159/1000 | Loss: 0.00008457
Iteration 160/1000 | Loss: 0.00007013
Iteration 161/1000 | Loss: 0.00007388
Iteration 162/1000 | Loss: 0.00006908
Iteration 163/1000 | Loss: 0.00007233
Iteration 164/1000 | Loss: 0.00006519
Iteration 165/1000 | Loss: 0.00008348
Iteration 166/1000 | Loss: 0.00007514
Iteration 167/1000 | Loss: 0.00008508
Iteration 168/1000 | Loss: 0.00007590
Iteration 169/1000 | Loss: 0.00007590
Iteration 170/1000 | Loss: 0.00008456
Iteration 171/1000 | Loss: 0.00005766
Iteration 172/1000 | Loss: 0.00004357
Iteration 173/1000 | Loss: 0.00006487
Iteration 174/1000 | Loss: 0.00005079
Iteration 175/1000 | Loss: 0.00005613
Iteration 176/1000 | Loss: 0.00006058
Iteration 177/1000 | Loss: 0.00007421
Iteration 178/1000 | Loss: 0.00006212
Iteration 179/1000 | Loss: 0.00005628
Iteration 180/1000 | Loss: 0.00006137
Iteration 181/1000 | Loss: 0.00005987
Iteration 182/1000 | Loss: 0.00004679
Iteration 183/1000 | Loss: 0.00006313
Iteration 184/1000 | Loss: 0.00005515
Iteration 185/1000 | Loss: 0.00008225
Iteration 186/1000 | Loss: 0.00007479
Iteration 187/1000 | Loss: 0.00007723
Iteration 188/1000 | Loss: 0.00005133
Iteration 189/1000 | Loss: 0.00003222
Iteration 190/1000 | Loss: 0.00004424
Iteration 191/1000 | Loss: 0.00004148
Iteration 192/1000 | Loss: 0.00003861
Iteration 193/1000 | Loss: 0.00004394
Iteration 194/1000 | Loss: 0.00004451
Iteration 195/1000 | Loss: 0.00004623
Iteration 196/1000 | Loss: 0.00004950
Iteration 197/1000 | Loss: 0.00004801
Iteration 198/1000 | Loss: 0.00004746
Iteration 199/1000 | Loss: 0.00004854
Iteration 200/1000 | Loss: 0.00004109
Iteration 201/1000 | Loss: 0.00003607
Iteration 202/1000 | Loss: 0.00003652
Iteration 203/1000 | Loss: 0.00003401
Iteration 204/1000 | Loss: 0.00004727
Iteration 205/1000 | Loss: 0.00004253
Iteration 206/1000 | Loss: 0.00004456
Iteration 207/1000 | Loss: 0.00002942
Iteration 208/1000 | Loss: 0.00003423
Iteration 209/1000 | Loss: 0.00003657
Iteration 210/1000 | Loss: 0.00004077
Iteration 211/1000 | Loss: 0.00004448
Iteration 212/1000 | Loss: 0.00004089
Iteration 213/1000 | Loss: 0.00004201
Iteration 214/1000 | Loss: 0.00004410
Iteration 215/1000 | Loss: 0.00003603
Iteration 216/1000 | Loss: 0.00003851
Iteration 217/1000 | Loss: 0.00003236
Iteration 218/1000 | Loss: 0.00002780
Iteration 219/1000 | Loss: 0.00003401
Iteration 220/1000 | Loss: 0.00003911
Iteration 221/1000 | Loss: 0.00004496
Iteration 222/1000 | Loss: 0.00004642
Iteration 223/1000 | Loss: 0.00004815
Iteration 224/1000 | Loss: 0.00004674
Iteration 225/1000 | Loss: 0.00005841
Iteration 226/1000 | Loss: 0.00003539
Iteration 227/1000 | Loss: 0.00003079
Iteration 228/1000 | Loss: 0.00003309
Iteration 229/1000 | Loss: 0.00002710
Iteration 230/1000 | Loss: 0.00003788
Iteration 231/1000 | Loss: 0.00003635
Iteration 232/1000 | Loss: 0.00003552
Iteration 233/1000 | Loss: 0.00003478
Iteration 234/1000 | Loss: 0.00003219
Iteration 235/1000 | Loss: 0.00003216
Iteration 236/1000 | Loss: 0.00003147
Iteration 237/1000 | Loss: 0.00003323
Iteration 238/1000 | Loss: 0.00003146
Iteration 239/1000 | Loss: 0.00004049
Iteration 240/1000 | Loss: 0.00003595
Iteration 241/1000 | Loss: 0.00003006
Iteration 242/1000 | Loss: 0.00002838
Iteration 243/1000 | Loss: 0.00002588
Iteration 244/1000 | Loss: 0.00002529
Iteration 245/1000 | Loss: 0.00002489
Iteration 246/1000 | Loss: 0.00002431
Iteration 247/1000 | Loss: 0.00002387
Iteration 248/1000 | Loss: 0.00002354
Iteration 249/1000 | Loss: 0.00002319
Iteration 250/1000 | Loss: 0.00002290
Iteration 251/1000 | Loss: 0.00002271
Iteration 252/1000 | Loss: 0.00002269
Iteration 253/1000 | Loss: 0.00002264
Iteration 254/1000 | Loss: 0.00002255
Iteration 255/1000 | Loss: 0.00002253
Iteration 256/1000 | Loss: 0.00002253
Iteration 257/1000 | Loss: 0.00002243
Iteration 258/1000 | Loss: 0.00002234
Iteration 259/1000 | Loss: 0.00002231
Iteration 260/1000 | Loss: 0.00002231
Iteration 261/1000 | Loss: 0.00002230
Iteration 262/1000 | Loss: 0.00002230
Iteration 263/1000 | Loss: 0.00002229
Iteration 264/1000 | Loss: 0.00002229
Iteration 265/1000 | Loss: 0.00002229
Iteration 266/1000 | Loss: 0.00002229
Iteration 267/1000 | Loss: 0.00002229
Iteration 268/1000 | Loss: 0.00002229
Iteration 269/1000 | Loss: 0.00002227
Iteration 270/1000 | Loss: 0.00002225
Iteration 271/1000 | Loss: 0.00002225
Iteration 272/1000 | Loss: 0.00002225
Iteration 273/1000 | Loss: 0.00002225
Iteration 274/1000 | Loss: 0.00002225
Iteration 275/1000 | Loss: 0.00002225
Iteration 276/1000 | Loss: 0.00002225
Iteration 277/1000 | Loss: 0.00002225
Iteration 278/1000 | Loss: 0.00002225
Iteration 279/1000 | Loss: 0.00002225
Iteration 280/1000 | Loss: 0.00002224
Iteration 281/1000 | Loss: 0.00002224
Iteration 282/1000 | Loss: 0.00002224
Iteration 283/1000 | Loss: 0.00002224
Iteration 284/1000 | Loss: 0.00002223
Iteration 285/1000 | Loss: 0.00002222
Iteration 286/1000 | Loss: 0.00002222
Iteration 287/1000 | Loss: 0.00002222
Iteration 288/1000 | Loss: 0.00002222
Iteration 289/1000 | Loss: 0.00002222
Iteration 290/1000 | Loss: 0.00002222
Iteration 291/1000 | Loss: 0.00002222
Iteration 292/1000 | Loss: 0.00002222
Iteration 293/1000 | Loss: 0.00002222
Iteration 294/1000 | Loss: 0.00002221
Iteration 295/1000 | Loss: 0.00002221
Iteration 296/1000 | Loss: 0.00002221
Iteration 297/1000 | Loss: 0.00002221
Iteration 298/1000 | Loss: 0.00002221
Iteration 299/1000 | Loss: 0.00002221
Iteration 300/1000 | Loss: 0.00002221
Iteration 301/1000 | Loss: 0.00002221
Iteration 302/1000 | Loss: 0.00002221
Iteration 303/1000 | Loss: 0.00002221
Iteration 304/1000 | Loss: 0.00002221
Iteration 305/1000 | Loss: 0.00002221
Iteration 306/1000 | Loss: 0.00002221
Iteration 307/1000 | Loss: 0.00002221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [2.221387876488734e-05, 2.221387876488734e-05, 2.221387876488734e-05, 2.221387876488734e-05, 2.221387876488734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.221387876488734e-05

Optimization complete. Final v2v error: 3.7525839805603027 mm

Highest mean error: 5.7492499351501465 mm for frame 36

Lowest mean error: 3.4606308937072754 mm for frame 2

Saving results

Total time: 465.7811846733093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693159
Iteration 2/25 | Loss: 0.00154599
Iteration 3/25 | Loss: 0.00128803
Iteration 4/25 | Loss: 0.00126031
Iteration 5/25 | Loss: 0.00127015
Iteration 6/25 | Loss: 0.00126218
Iteration 7/25 | Loss: 0.00123980
Iteration 8/25 | Loss: 0.00122657
Iteration 9/25 | Loss: 0.00122515
Iteration 10/25 | Loss: 0.00122473
Iteration 11/25 | Loss: 0.00123791
Iteration 12/25 | Loss: 0.00122394
Iteration 13/25 | Loss: 0.00121996
Iteration 14/25 | Loss: 0.00121930
Iteration 15/25 | Loss: 0.00121927
Iteration 16/25 | Loss: 0.00121927
Iteration 17/25 | Loss: 0.00121927
Iteration 18/25 | Loss: 0.00121927
Iteration 19/25 | Loss: 0.00121927
Iteration 20/25 | Loss: 0.00121926
Iteration 21/25 | Loss: 0.00121926
Iteration 22/25 | Loss: 0.00121926
Iteration 23/25 | Loss: 0.00121926
Iteration 24/25 | Loss: 0.00121926
Iteration 25/25 | Loss: 0.00121926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.63800240
Iteration 2/25 | Loss: 0.00066821
Iteration 3/25 | Loss: 0.00066821
Iteration 4/25 | Loss: 0.00066821
Iteration 5/25 | Loss: 0.00066821
Iteration 6/25 | Loss: 0.00066821
Iteration 7/25 | Loss: 0.00066821
Iteration 8/25 | Loss: 0.00066821
Iteration 9/25 | Loss: 0.00066821
Iteration 10/25 | Loss: 0.00066821
Iteration 11/25 | Loss: 0.00066821
Iteration 12/25 | Loss: 0.00066821
Iteration 13/25 | Loss: 0.00066821
Iteration 14/25 | Loss: 0.00066821
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006682118400931358, 0.0006682118400931358, 0.0006682118400931358, 0.0006682118400931358, 0.0006682118400931358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006682118400931358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066821
Iteration 2/1000 | Loss: 0.00003164
Iteration 3/1000 | Loss: 0.00001957
Iteration 4/1000 | Loss: 0.00001709
Iteration 5/1000 | Loss: 0.00001629
Iteration 6/1000 | Loss: 0.00001587
Iteration 7/1000 | Loss: 0.00001541
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001477
Iteration 10/1000 | Loss: 0.00001458
Iteration 11/1000 | Loss: 0.00001444
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001439
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001431
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001420
Iteration 19/1000 | Loss: 0.00001418
Iteration 20/1000 | Loss: 0.00001418
Iteration 21/1000 | Loss: 0.00001417
Iteration 22/1000 | Loss: 0.00001415
Iteration 23/1000 | Loss: 0.00001415
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001415
Iteration 26/1000 | Loss: 0.00001415
Iteration 27/1000 | Loss: 0.00001415
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001411
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001409
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001406
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001406
Iteration 46/1000 | Loss: 0.00001406
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001406
Iteration 50/1000 | Loss: 0.00001406
Iteration 51/1000 | Loss: 0.00001406
Iteration 52/1000 | Loss: 0.00001406
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001405
Iteration 55/1000 | Loss: 0.00001405
Iteration 56/1000 | Loss: 0.00001405
Iteration 57/1000 | Loss: 0.00001405
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001404
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001403
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001403
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001402
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001400
Iteration 78/1000 | Loss: 0.00001400
Iteration 79/1000 | Loss: 0.00001400
Iteration 80/1000 | Loss: 0.00001400
Iteration 81/1000 | Loss: 0.00001400
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001399
Iteration 85/1000 | Loss: 0.00001399
Iteration 86/1000 | Loss: 0.00001399
Iteration 87/1000 | Loss: 0.00001398
Iteration 88/1000 | Loss: 0.00001398
Iteration 89/1000 | Loss: 0.00001398
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001395
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001395
Iteration 105/1000 | Loss: 0.00001394
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001392
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Iteration 126/1000 | Loss: 0.00001391
Iteration 127/1000 | Loss: 0.00001390
Iteration 128/1000 | Loss: 0.00001390
Iteration 129/1000 | Loss: 0.00001390
Iteration 130/1000 | Loss: 0.00001390
Iteration 131/1000 | Loss: 0.00001390
Iteration 132/1000 | Loss: 0.00001390
Iteration 133/1000 | Loss: 0.00001390
Iteration 134/1000 | Loss: 0.00001390
Iteration 135/1000 | Loss: 0.00001390
Iteration 136/1000 | Loss: 0.00001390
Iteration 137/1000 | Loss: 0.00001390
Iteration 138/1000 | Loss: 0.00001389
Iteration 139/1000 | Loss: 0.00001389
Iteration 140/1000 | Loss: 0.00001389
Iteration 141/1000 | Loss: 0.00001389
Iteration 142/1000 | Loss: 0.00001389
Iteration 143/1000 | Loss: 0.00001389
Iteration 144/1000 | Loss: 0.00001388
Iteration 145/1000 | Loss: 0.00001388
Iteration 146/1000 | Loss: 0.00001388
Iteration 147/1000 | Loss: 0.00001388
Iteration 148/1000 | Loss: 0.00001388
Iteration 149/1000 | Loss: 0.00001388
Iteration 150/1000 | Loss: 0.00001388
Iteration 151/1000 | Loss: 0.00001387
Iteration 152/1000 | Loss: 0.00001387
Iteration 153/1000 | Loss: 0.00001387
Iteration 154/1000 | Loss: 0.00001386
Iteration 155/1000 | Loss: 0.00001386
Iteration 156/1000 | Loss: 0.00001386
Iteration 157/1000 | Loss: 0.00001386
Iteration 158/1000 | Loss: 0.00001386
Iteration 159/1000 | Loss: 0.00001385
Iteration 160/1000 | Loss: 0.00001385
Iteration 161/1000 | Loss: 0.00001385
Iteration 162/1000 | Loss: 0.00001385
Iteration 163/1000 | Loss: 0.00001385
Iteration 164/1000 | Loss: 0.00001385
Iteration 165/1000 | Loss: 0.00001385
Iteration 166/1000 | Loss: 0.00001385
Iteration 167/1000 | Loss: 0.00001384
Iteration 168/1000 | Loss: 0.00001384
Iteration 169/1000 | Loss: 0.00001384
Iteration 170/1000 | Loss: 0.00001384
Iteration 171/1000 | Loss: 0.00001384
Iteration 172/1000 | Loss: 0.00001384
Iteration 173/1000 | Loss: 0.00001384
Iteration 174/1000 | Loss: 0.00001384
Iteration 175/1000 | Loss: 0.00001384
Iteration 176/1000 | Loss: 0.00001384
Iteration 177/1000 | Loss: 0.00001384
Iteration 178/1000 | Loss: 0.00001384
Iteration 179/1000 | Loss: 0.00001383
Iteration 180/1000 | Loss: 0.00001383
Iteration 181/1000 | Loss: 0.00001382
Iteration 182/1000 | Loss: 0.00001382
Iteration 183/1000 | Loss: 0.00001382
Iteration 184/1000 | Loss: 0.00001382
Iteration 185/1000 | Loss: 0.00001382
Iteration 186/1000 | Loss: 0.00001381
Iteration 187/1000 | Loss: 0.00001381
Iteration 188/1000 | Loss: 0.00001381
Iteration 189/1000 | Loss: 0.00001381
Iteration 190/1000 | Loss: 0.00001381
Iteration 191/1000 | Loss: 0.00001381
Iteration 192/1000 | Loss: 0.00001381
Iteration 193/1000 | Loss: 0.00001381
Iteration 194/1000 | Loss: 0.00001381
Iteration 195/1000 | Loss: 0.00001380
Iteration 196/1000 | Loss: 0.00001380
Iteration 197/1000 | Loss: 0.00001380
Iteration 198/1000 | Loss: 0.00001380
Iteration 199/1000 | Loss: 0.00001380
Iteration 200/1000 | Loss: 0.00001380
Iteration 201/1000 | Loss: 0.00001380
Iteration 202/1000 | Loss: 0.00001380
Iteration 203/1000 | Loss: 0.00001380
Iteration 204/1000 | Loss: 0.00001380
Iteration 205/1000 | Loss: 0.00001380
Iteration 206/1000 | Loss: 0.00001380
Iteration 207/1000 | Loss: 0.00001380
Iteration 208/1000 | Loss: 0.00001380
Iteration 209/1000 | Loss: 0.00001379
Iteration 210/1000 | Loss: 0.00001379
Iteration 211/1000 | Loss: 0.00001379
Iteration 212/1000 | Loss: 0.00001379
Iteration 213/1000 | Loss: 0.00001379
Iteration 214/1000 | Loss: 0.00001379
Iteration 215/1000 | Loss: 0.00001379
Iteration 216/1000 | Loss: 0.00001379
Iteration 217/1000 | Loss: 0.00001379
Iteration 218/1000 | Loss: 0.00001379
Iteration 219/1000 | Loss: 0.00001379
Iteration 220/1000 | Loss: 0.00001379
Iteration 221/1000 | Loss: 0.00001379
Iteration 222/1000 | Loss: 0.00001379
Iteration 223/1000 | Loss: 0.00001379
Iteration 224/1000 | Loss: 0.00001379
Iteration 225/1000 | Loss: 0.00001379
Iteration 226/1000 | Loss: 0.00001379
Iteration 227/1000 | Loss: 0.00001379
Iteration 228/1000 | Loss: 0.00001379
Iteration 229/1000 | Loss: 0.00001379
Iteration 230/1000 | Loss: 0.00001379
Iteration 231/1000 | Loss: 0.00001379
Iteration 232/1000 | Loss: 0.00001379
Iteration 233/1000 | Loss: 0.00001379
Iteration 234/1000 | Loss: 0.00001379
Iteration 235/1000 | Loss: 0.00001379
Iteration 236/1000 | Loss: 0.00001379
Iteration 237/1000 | Loss: 0.00001379
Iteration 238/1000 | Loss: 0.00001379
Iteration 239/1000 | Loss: 0.00001379
Iteration 240/1000 | Loss: 0.00001379
Iteration 241/1000 | Loss: 0.00001379
Iteration 242/1000 | Loss: 0.00001379
Iteration 243/1000 | Loss: 0.00001379
Iteration 244/1000 | Loss: 0.00001379
Iteration 245/1000 | Loss: 0.00001379
Iteration 246/1000 | Loss: 0.00001379
Iteration 247/1000 | Loss: 0.00001379
Iteration 248/1000 | Loss: 0.00001379
Iteration 249/1000 | Loss: 0.00001379
Iteration 250/1000 | Loss: 0.00001379
Iteration 251/1000 | Loss: 0.00001379
Iteration 252/1000 | Loss: 0.00001379
Iteration 253/1000 | Loss: 0.00001379
Iteration 254/1000 | Loss: 0.00001379
Iteration 255/1000 | Loss: 0.00001379
Iteration 256/1000 | Loss: 0.00001379
Iteration 257/1000 | Loss: 0.00001379
Iteration 258/1000 | Loss: 0.00001379
Iteration 259/1000 | Loss: 0.00001379
Iteration 260/1000 | Loss: 0.00001379
Iteration 261/1000 | Loss: 0.00001379
Iteration 262/1000 | Loss: 0.00001379
Iteration 263/1000 | Loss: 0.00001379
Iteration 264/1000 | Loss: 0.00001379
Iteration 265/1000 | Loss: 0.00001379
Iteration 266/1000 | Loss: 0.00001379
Iteration 267/1000 | Loss: 0.00001379
Iteration 268/1000 | Loss: 0.00001379
Iteration 269/1000 | Loss: 0.00001379
Iteration 270/1000 | Loss: 0.00001379
Iteration 271/1000 | Loss: 0.00001379
Iteration 272/1000 | Loss: 0.00001379
Iteration 273/1000 | Loss: 0.00001379
Iteration 274/1000 | Loss: 0.00001379
Iteration 275/1000 | Loss: 0.00001379
Iteration 276/1000 | Loss: 0.00001379
Iteration 277/1000 | Loss: 0.00001379
Iteration 278/1000 | Loss: 0.00001379
Iteration 279/1000 | Loss: 0.00001379
Iteration 280/1000 | Loss: 0.00001379
Iteration 281/1000 | Loss: 0.00001379
Iteration 282/1000 | Loss: 0.00001379
Iteration 283/1000 | Loss: 0.00001379
Iteration 284/1000 | Loss: 0.00001379
Iteration 285/1000 | Loss: 0.00001379
Iteration 286/1000 | Loss: 0.00001379
Iteration 287/1000 | Loss: 0.00001379
Iteration 288/1000 | Loss: 0.00001379
Iteration 289/1000 | Loss: 0.00001379
Iteration 290/1000 | Loss: 0.00001379
Iteration 291/1000 | Loss: 0.00001379
Iteration 292/1000 | Loss: 0.00001379
Iteration 293/1000 | Loss: 0.00001379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [1.379239620291628e-05, 1.379239620291628e-05, 1.379239620291628e-05, 1.379239620291628e-05, 1.379239620291628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.379239620291628e-05

Optimization complete. Final v2v error: 3.1418299674987793 mm

Highest mean error: 3.972250461578369 mm for frame 3

Lowest mean error: 2.8480350971221924 mm for frame 94

Saving results

Total time: 56.6508252620697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022166
Iteration 2/25 | Loss: 0.00217037
Iteration 3/25 | Loss: 0.00169971
Iteration 4/25 | Loss: 0.00155982
Iteration 5/25 | Loss: 0.00146962
Iteration 6/25 | Loss: 0.00141615
Iteration 7/25 | Loss: 0.00142003
Iteration 8/25 | Loss: 0.00143362
Iteration 9/25 | Loss: 0.00139764
Iteration 10/25 | Loss: 0.00138247
Iteration 11/25 | Loss: 0.00136957
Iteration 12/25 | Loss: 0.00136056
Iteration 13/25 | Loss: 0.00134051
Iteration 14/25 | Loss: 0.00133358
Iteration 15/25 | Loss: 0.00131724
Iteration 16/25 | Loss: 0.00131046
Iteration 17/25 | Loss: 0.00130158
Iteration 18/25 | Loss: 0.00129379
Iteration 19/25 | Loss: 0.00128328
Iteration 20/25 | Loss: 0.00128433
Iteration 21/25 | Loss: 0.00128658
Iteration 22/25 | Loss: 0.00127711
Iteration 23/25 | Loss: 0.00126886
Iteration 24/25 | Loss: 0.00126779
Iteration 25/25 | Loss: 0.00126751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37145269
Iteration 2/25 | Loss: 0.00129733
Iteration 3/25 | Loss: 0.00102907
Iteration 4/25 | Loss: 0.00102907
Iteration 5/25 | Loss: 0.00102907
Iteration 6/25 | Loss: 0.00102907
Iteration 7/25 | Loss: 0.00102907
Iteration 8/25 | Loss: 0.00102907
Iteration 9/25 | Loss: 0.00102907
Iteration 10/25 | Loss: 0.00102907
Iteration 11/25 | Loss: 0.00102907
Iteration 12/25 | Loss: 0.00102907
Iteration 13/25 | Loss: 0.00102907
Iteration 14/25 | Loss: 0.00102907
Iteration 15/25 | Loss: 0.00102907
Iteration 16/25 | Loss: 0.00102907
Iteration 17/25 | Loss: 0.00102907
Iteration 18/25 | Loss: 0.00102907
Iteration 19/25 | Loss: 0.00102907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010290680220350623, 0.0010290680220350623, 0.0010290680220350623, 0.0010290680220350623, 0.0010290680220350623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010290680220350623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102907
Iteration 2/1000 | Loss: 0.00016247
Iteration 3/1000 | Loss: 0.00065125
Iteration 4/1000 | Loss: 0.00032511
Iteration 5/1000 | Loss: 0.00044849
Iteration 6/1000 | Loss: 0.00053183
Iteration 7/1000 | Loss: 0.00042385
Iteration 8/1000 | Loss: 0.00058968
Iteration 9/1000 | Loss: 0.00038606
Iteration 10/1000 | Loss: 0.00053439
Iteration 11/1000 | Loss: 0.00055729
Iteration 12/1000 | Loss: 0.00033616
Iteration 13/1000 | Loss: 0.00064448
Iteration 14/1000 | Loss: 0.00043059
Iteration 15/1000 | Loss: 0.00030084
Iteration 16/1000 | Loss: 0.00012551
Iteration 17/1000 | Loss: 0.00005840
Iteration 18/1000 | Loss: 0.00013124
Iteration 19/1000 | Loss: 0.00020583
Iteration 20/1000 | Loss: 0.00034097
Iteration 21/1000 | Loss: 0.00031059
Iteration 22/1000 | Loss: 0.00039510
Iteration 23/1000 | Loss: 0.00032534
Iteration 24/1000 | Loss: 0.00016288
Iteration 25/1000 | Loss: 0.00015828
Iteration 26/1000 | Loss: 0.00016303
Iteration 27/1000 | Loss: 0.00071901
Iteration 28/1000 | Loss: 0.00063095
Iteration 29/1000 | Loss: 0.00061501
Iteration 30/1000 | Loss: 0.00025227
Iteration 31/1000 | Loss: 0.00031084
Iteration 32/1000 | Loss: 0.00025990
Iteration 33/1000 | Loss: 0.00019721
Iteration 34/1000 | Loss: 0.00004214
Iteration 35/1000 | Loss: 0.00011321
Iteration 36/1000 | Loss: 0.00012932
Iteration 37/1000 | Loss: 0.00010336
Iteration 38/1000 | Loss: 0.00005221
Iteration 39/1000 | Loss: 0.00011194
Iteration 40/1000 | Loss: 0.00013236
Iteration 41/1000 | Loss: 0.00048907
Iteration 42/1000 | Loss: 0.00004608
Iteration 43/1000 | Loss: 0.00005708
Iteration 44/1000 | Loss: 0.00009894
Iteration 45/1000 | Loss: 0.00007278
Iteration 46/1000 | Loss: 0.00004593
Iteration 47/1000 | Loss: 0.00011391
Iteration 48/1000 | Loss: 0.00079707
Iteration 49/1000 | Loss: 0.00034514
Iteration 50/1000 | Loss: 0.00013301
Iteration 51/1000 | Loss: 0.00031477
Iteration 52/1000 | Loss: 0.00040237
Iteration 53/1000 | Loss: 0.00013366
Iteration 54/1000 | Loss: 0.00014711
Iteration 55/1000 | Loss: 0.00007473
Iteration 56/1000 | Loss: 0.00007293
Iteration 57/1000 | Loss: 0.00005668
Iteration 58/1000 | Loss: 0.00028458
Iteration 59/1000 | Loss: 0.00032927
Iteration 60/1000 | Loss: 0.00041635
Iteration 61/1000 | Loss: 0.00068884
Iteration 62/1000 | Loss: 0.00018438
Iteration 63/1000 | Loss: 0.00013389
Iteration 64/1000 | Loss: 0.00016918
Iteration 65/1000 | Loss: 0.00008532
Iteration 66/1000 | Loss: 0.00009241
Iteration 67/1000 | Loss: 0.00003823
Iteration 68/1000 | Loss: 0.00010239
Iteration 69/1000 | Loss: 0.00011739
Iteration 70/1000 | Loss: 0.00014356
Iteration 71/1000 | Loss: 0.00043970
Iteration 72/1000 | Loss: 0.00016681
Iteration 73/1000 | Loss: 0.00011210
Iteration 74/1000 | Loss: 0.00014816
Iteration 75/1000 | Loss: 0.00007569
Iteration 76/1000 | Loss: 0.00005855
Iteration 77/1000 | Loss: 0.00004966
Iteration 78/1000 | Loss: 0.00003343
Iteration 79/1000 | Loss: 0.00030309
Iteration 80/1000 | Loss: 0.00006336
Iteration 81/1000 | Loss: 0.00003247
Iteration 82/1000 | Loss: 0.00003218
Iteration 83/1000 | Loss: 0.00003173
Iteration 84/1000 | Loss: 0.00003130
Iteration 85/1000 | Loss: 0.00003100
Iteration 86/1000 | Loss: 0.00025699
Iteration 87/1000 | Loss: 0.00007788
Iteration 88/1000 | Loss: 0.00051871
Iteration 89/1000 | Loss: 0.00022179
Iteration 90/1000 | Loss: 0.00081520
Iteration 91/1000 | Loss: 0.00007142
Iteration 92/1000 | Loss: 0.00021317
Iteration 93/1000 | Loss: 0.00007872
Iteration 94/1000 | Loss: 0.00014012
Iteration 95/1000 | Loss: 0.00012890
Iteration 96/1000 | Loss: 0.00009016
Iteration 97/1000 | Loss: 0.00021170
Iteration 98/1000 | Loss: 0.00021945
Iteration 99/1000 | Loss: 0.00038140
Iteration 100/1000 | Loss: 0.00019824
Iteration 101/1000 | Loss: 0.00018177
Iteration 102/1000 | Loss: 0.00011959
Iteration 103/1000 | Loss: 0.00035617
Iteration 104/1000 | Loss: 0.00008361
Iteration 105/1000 | Loss: 0.00016790
Iteration 106/1000 | Loss: 0.00010699
Iteration 107/1000 | Loss: 0.00013944
Iteration 108/1000 | Loss: 0.00011943
Iteration 109/1000 | Loss: 0.00010095
Iteration 110/1000 | Loss: 0.00018667
Iteration 111/1000 | Loss: 0.00010538
Iteration 112/1000 | Loss: 0.00009933
Iteration 113/1000 | Loss: 0.00045331
Iteration 114/1000 | Loss: 0.00015994
Iteration 115/1000 | Loss: 0.00040204
Iteration 116/1000 | Loss: 0.00031221
Iteration 117/1000 | Loss: 0.00029273
Iteration 118/1000 | Loss: 0.00011984
Iteration 119/1000 | Loss: 0.00010556
Iteration 120/1000 | Loss: 0.00018104
Iteration 121/1000 | Loss: 0.00008275
Iteration 122/1000 | Loss: 0.00014561
Iteration 123/1000 | Loss: 0.00016760
Iteration 124/1000 | Loss: 0.00021659
Iteration 125/1000 | Loss: 0.00034096
Iteration 126/1000 | Loss: 0.00015730
Iteration 127/1000 | Loss: 0.00011772
Iteration 128/1000 | Loss: 0.00007267
Iteration 129/1000 | Loss: 0.00008641
Iteration 130/1000 | Loss: 0.00038750
Iteration 131/1000 | Loss: 0.00024636
Iteration 132/1000 | Loss: 0.00020788
Iteration 133/1000 | Loss: 0.00016548
Iteration 134/1000 | Loss: 0.00007650
Iteration 135/1000 | Loss: 0.00007518
Iteration 136/1000 | Loss: 0.00037330
Iteration 137/1000 | Loss: 0.00034774
Iteration 138/1000 | Loss: 0.00013095
Iteration 139/1000 | Loss: 0.00014674
Iteration 140/1000 | Loss: 0.00015083
Iteration 141/1000 | Loss: 0.00022497
Iteration 142/1000 | Loss: 0.00013182
Iteration 143/1000 | Loss: 0.00012472
Iteration 144/1000 | Loss: 0.00014386
Iteration 145/1000 | Loss: 0.00019749
Iteration 146/1000 | Loss: 0.00065035
Iteration 147/1000 | Loss: 0.00026191
Iteration 148/1000 | Loss: 0.00020999
Iteration 149/1000 | Loss: 0.00030177
Iteration 150/1000 | Loss: 0.00019645
Iteration 151/1000 | Loss: 0.00029431
Iteration 152/1000 | Loss: 0.00013888
Iteration 153/1000 | Loss: 0.00013829
Iteration 154/1000 | Loss: 0.00015071
Iteration 155/1000 | Loss: 0.00011316
Iteration 156/1000 | Loss: 0.00009769
Iteration 157/1000 | Loss: 0.00009962
Iteration 158/1000 | Loss: 0.00010099
Iteration 159/1000 | Loss: 0.00008010
Iteration 160/1000 | Loss: 0.00011276
Iteration 161/1000 | Loss: 0.00007564
Iteration 162/1000 | Loss: 0.00011830
Iteration 163/1000 | Loss: 0.00020527
Iteration 164/1000 | Loss: 0.00030605
Iteration 165/1000 | Loss: 0.00009845
Iteration 166/1000 | Loss: 0.00014750
Iteration 167/1000 | Loss: 0.00023117
Iteration 168/1000 | Loss: 0.00021365
Iteration 169/1000 | Loss: 0.00016482
Iteration 170/1000 | Loss: 0.00210473
Iteration 171/1000 | Loss: 0.00010180
Iteration 172/1000 | Loss: 0.00054010
Iteration 173/1000 | Loss: 0.00009706
Iteration 174/1000 | Loss: 0.00018131
Iteration 175/1000 | Loss: 0.00041693
Iteration 176/1000 | Loss: 0.00012759
Iteration 177/1000 | Loss: 0.00011557
Iteration 178/1000 | Loss: 0.00013465
Iteration 179/1000 | Loss: 0.00023361
Iteration 180/1000 | Loss: 0.00026714
Iteration 181/1000 | Loss: 0.00018480
Iteration 182/1000 | Loss: 0.00030573
Iteration 183/1000 | Loss: 0.00029498
Iteration 184/1000 | Loss: 0.00018604
Iteration 185/1000 | Loss: 0.00020792
Iteration 186/1000 | Loss: 0.00037797
Iteration 187/1000 | Loss: 0.00023869
Iteration 188/1000 | Loss: 0.00030990
Iteration 189/1000 | Loss: 0.00041209
Iteration 190/1000 | Loss: 0.00024600
Iteration 191/1000 | Loss: 0.00039666
Iteration 192/1000 | Loss: 0.00092030
Iteration 193/1000 | Loss: 0.00071422
Iteration 194/1000 | Loss: 0.00003969
Iteration 195/1000 | Loss: 0.00028194
Iteration 196/1000 | Loss: 0.00025013
Iteration 197/1000 | Loss: 0.00042421
Iteration 198/1000 | Loss: 0.00009062
Iteration 199/1000 | Loss: 0.00027710
Iteration 200/1000 | Loss: 0.00003908
Iteration 201/1000 | Loss: 0.00010086
Iteration 202/1000 | Loss: 0.00003358
Iteration 203/1000 | Loss: 0.00003644
Iteration 204/1000 | Loss: 0.00002841
Iteration 205/1000 | Loss: 0.00005088
Iteration 206/1000 | Loss: 0.00002758
Iteration 207/1000 | Loss: 0.00002715
Iteration 208/1000 | Loss: 0.00002683
Iteration 209/1000 | Loss: 0.00002626
Iteration 210/1000 | Loss: 0.00014554
Iteration 211/1000 | Loss: 0.00015910
Iteration 212/1000 | Loss: 0.00002649
Iteration 213/1000 | Loss: 0.00002583
Iteration 214/1000 | Loss: 0.00028320
Iteration 215/1000 | Loss: 0.00003278
Iteration 216/1000 | Loss: 0.00002767
Iteration 217/1000 | Loss: 0.00002624
Iteration 218/1000 | Loss: 0.00002537
Iteration 219/1000 | Loss: 0.00011471
Iteration 220/1000 | Loss: 0.00002520
Iteration 221/1000 | Loss: 0.00002493
Iteration 222/1000 | Loss: 0.00002468
Iteration 223/1000 | Loss: 0.00002449
Iteration 224/1000 | Loss: 0.00002438
Iteration 225/1000 | Loss: 0.00002437
Iteration 226/1000 | Loss: 0.00002430
Iteration 227/1000 | Loss: 0.00002429
Iteration 228/1000 | Loss: 0.00002429
Iteration 229/1000 | Loss: 0.00002429
Iteration 230/1000 | Loss: 0.00002427
Iteration 231/1000 | Loss: 0.00002424
Iteration 232/1000 | Loss: 0.00002420
Iteration 233/1000 | Loss: 0.00010301
Iteration 234/1000 | Loss: 0.00004400
Iteration 235/1000 | Loss: 0.00003461
Iteration 236/1000 | Loss: 0.00002407
Iteration 237/1000 | Loss: 0.00002407
Iteration 238/1000 | Loss: 0.00002407
Iteration 239/1000 | Loss: 0.00002406
Iteration 240/1000 | Loss: 0.00002406
Iteration 241/1000 | Loss: 0.00002406
Iteration 242/1000 | Loss: 0.00002406
Iteration 243/1000 | Loss: 0.00002405
Iteration 244/1000 | Loss: 0.00002405
Iteration 245/1000 | Loss: 0.00002404
Iteration 246/1000 | Loss: 0.00002404
Iteration 247/1000 | Loss: 0.00002404
Iteration 248/1000 | Loss: 0.00002403
Iteration 249/1000 | Loss: 0.00002403
Iteration 250/1000 | Loss: 0.00002403
Iteration 251/1000 | Loss: 0.00002403
Iteration 252/1000 | Loss: 0.00002402
Iteration 253/1000 | Loss: 0.00002402
Iteration 254/1000 | Loss: 0.00002402
Iteration 255/1000 | Loss: 0.00002402
Iteration 256/1000 | Loss: 0.00002402
Iteration 257/1000 | Loss: 0.00002402
Iteration 258/1000 | Loss: 0.00002401
Iteration 259/1000 | Loss: 0.00002401
Iteration 260/1000 | Loss: 0.00002401
Iteration 261/1000 | Loss: 0.00002401
Iteration 262/1000 | Loss: 0.00002401
Iteration 263/1000 | Loss: 0.00002400
Iteration 264/1000 | Loss: 0.00002400
Iteration 265/1000 | Loss: 0.00002400
Iteration 266/1000 | Loss: 0.00002400
Iteration 267/1000 | Loss: 0.00002400
Iteration 268/1000 | Loss: 0.00002400
Iteration 269/1000 | Loss: 0.00002400
Iteration 270/1000 | Loss: 0.00002400
Iteration 271/1000 | Loss: 0.00002400
Iteration 272/1000 | Loss: 0.00002400
Iteration 273/1000 | Loss: 0.00002400
Iteration 274/1000 | Loss: 0.00002400
Iteration 275/1000 | Loss: 0.00002400
Iteration 276/1000 | Loss: 0.00002400
Iteration 277/1000 | Loss: 0.00002400
Iteration 278/1000 | Loss: 0.00002400
Iteration 279/1000 | Loss: 0.00002400
Iteration 280/1000 | Loss: 0.00002400
Iteration 281/1000 | Loss: 0.00002400
Iteration 282/1000 | Loss: 0.00002400
Iteration 283/1000 | Loss: 0.00002400
Iteration 284/1000 | Loss: 0.00002400
Iteration 285/1000 | Loss: 0.00002400
Iteration 286/1000 | Loss: 0.00002400
Iteration 287/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [2.3996444724616595e-05, 2.3996444724616595e-05, 2.3996444724616595e-05, 2.3996444724616595e-05, 2.3996444724616595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3996444724616595e-05

Optimization complete. Final v2v error: 4.136774063110352 mm

Highest mean error: 6.573180198669434 mm for frame 155

Lowest mean error: 3.6150944232940674 mm for frame 67

Saving results

Total time: 392.28523087501526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553445
Iteration 2/25 | Loss: 0.00140561
Iteration 3/25 | Loss: 0.00122517
Iteration 4/25 | Loss: 0.00116928
Iteration 5/25 | Loss: 0.00116263
Iteration 6/25 | Loss: 0.00116096
Iteration 7/25 | Loss: 0.00115889
Iteration 8/25 | Loss: 0.00115566
Iteration 9/25 | Loss: 0.00115444
Iteration 10/25 | Loss: 0.00115250
Iteration 11/25 | Loss: 0.00115201
Iteration 12/25 | Loss: 0.00115182
Iteration 13/25 | Loss: 0.00115179
Iteration 14/25 | Loss: 0.00115178
Iteration 15/25 | Loss: 0.00115178
Iteration 16/25 | Loss: 0.00115177
Iteration 17/25 | Loss: 0.00115177
Iteration 18/25 | Loss: 0.00115176
Iteration 19/25 | Loss: 0.00115176
Iteration 20/25 | Loss: 0.00115176
Iteration 21/25 | Loss: 0.00115176
Iteration 22/25 | Loss: 0.00115175
Iteration 23/25 | Loss: 0.00115175
Iteration 24/25 | Loss: 0.00115175
Iteration 25/25 | Loss: 0.00115175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63130283
Iteration 2/25 | Loss: 0.00080502
Iteration 3/25 | Loss: 0.00080502
Iteration 4/25 | Loss: 0.00080502
Iteration 5/25 | Loss: 0.00080502
Iteration 6/25 | Loss: 0.00080502
Iteration 7/25 | Loss: 0.00080502
Iteration 8/25 | Loss: 0.00080502
Iteration 9/25 | Loss: 0.00080502
Iteration 10/25 | Loss: 0.00080502
Iteration 11/25 | Loss: 0.00080502
Iteration 12/25 | Loss: 0.00080502
Iteration 13/25 | Loss: 0.00080502
Iteration 14/25 | Loss: 0.00080502
Iteration 15/25 | Loss: 0.00080502
Iteration 16/25 | Loss: 0.00080502
Iteration 17/25 | Loss: 0.00080502
Iteration 18/25 | Loss: 0.00080502
Iteration 19/25 | Loss: 0.00080502
Iteration 20/25 | Loss: 0.00080502
Iteration 21/25 | Loss: 0.00080502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008050182368606329, 0.0008050182368606329, 0.0008050182368606329, 0.0008050182368606329, 0.0008050182368606329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008050182368606329

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080502
Iteration 2/1000 | Loss: 0.00002248
Iteration 3/1000 | Loss: 0.00001838
Iteration 4/1000 | Loss: 0.00001713
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001487
Iteration 10/1000 | Loss: 0.00002769
Iteration 11/1000 | Loss: 0.00001454
Iteration 12/1000 | Loss: 0.00001440
Iteration 13/1000 | Loss: 0.00002415
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001411
Iteration 18/1000 | Loss: 0.00001411
Iteration 19/1000 | Loss: 0.00001411
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001388
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001385
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001380
Iteration 28/1000 | Loss: 0.00001380
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001378
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001371
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001370
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001368
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001367
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001366
Iteration 54/1000 | Loss: 0.00001366
Iteration 55/1000 | Loss: 0.00001365
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001363
Iteration 63/1000 | Loss: 0.00001363
Iteration 64/1000 | Loss: 0.00001362
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001361
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001361
Iteration 72/1000 | Loss: 0.00001361
Iteration 73/1000 | Loss: 0.00001361
Iteration 74/1000 | Loss: 0.00001360
Iteration 75/1000 | Loss: 0.00001360
Iteration 76/1000 | Loss: 0.00001360
Iteration 77/1000 | Loss: 0.00001360
Iteration 78/1000 | Loss: 0.00001360
Iteration 79/1000 | Loss: 0.00001360
Iteration 80/1000 | Loss: 0.00001360
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001425
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001352
Iteration 89/1000 | Loss: 0.00001352
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001352
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001352
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.351801347482251e-05, 1.351801347482251e-05, 1.351801347482251e-05, 1.351801347482251e-05, 1.351801347482251e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.351801347482251e-05

Optimization complete. Final v2v error: 3.1172382831573486 mm

Highest mean error: 3.888216018676758 mm for frame 182

Lowest mean error: 2.8905649185180664 mm for frame 215

Saving results

Total time: 63.24307584762573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399787
Iteration 2/25 | Loss: 0.00123124
Iteration 3/25 | Loss: 0.00115098
Iteration 4/25 | Loss: 0.00114616
Iteration 5/25 | Loss: 0.00114341
Iteration 6/25 | Loss: 0.00114337
Iteration 7/25 | Loss: 0.00114337
Iteration 8/25 | Loss: 0.00114337
Iteration 9/25 | Loss: 0.00114337
Iteration 10/25 | Loss: 0.00114337
Iteration 11/25 | Loss: 0.00114337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011433683102950454, 0.0011433683102950454, 0.0011433683102950454, 0.0011433683102950454, 0.0011433683102950454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011433683102950454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60909331
Iteration 2/25 | Loss: 0.00069962
Iteration 3/25 | Loss: 0.00069961
Iteration 4/25 | Loss: 0.00069961
Iteration 5/25 | Loss: 0.00069961
Iteration 6/25 | Loss: 0.00069961
Iteration 7/25 | Loss: 0.00069961
Iteration 8/25 | Loss: 0.00069961
Iteration 9/25 | Loss: 0.00069961
Iteration 10/25 | Loss: 0.00069961
Iteration 11/25 | Loss: 0.00069961
Iteration 12/25 | Loss: 0.00069961
Iteration 13/25 | Loss: 0.00069961
Iteration 14/25 | Loss: 0.00069961
Iteration 15/25 | Loss: 0.00069961
Iteration 16/25 | Loss: 0.00069961
Iteration 17/25 | Loss: 0.00069961
Iteration 18/25 | Loss: 0.00069961
Iteration 19/25 | Loss: 0.00069961
Iteration 20/25 | Loss: 0.00069961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006996059673838317, 0.0006996059673838317, 0.0006996059673838317, 0.0006996059673838317, 0.0006996059673838317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006996059673838317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069961
Iteration 2/1000 | Loss: 0.00002343
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001336
Iteration 5/1000 | Loss: 0.00001239
Iteration 6/1000 | Loss: 0.00001183
Iteration 7/1000 | Loss: 0.00001146
Iteration 8/1000 | Loss: 0.00001115
Iteration 9/1000 | Loss: 0.00001078
Iteration 10/1000 | Loss: 0.00001056
Iteration 11/1000 | Loss: 0.00001042
Iteration 12/1000 | Loss: 0.00001031
Iteration 13/1000 | Loss: 0.00001026
Iteration 14/1000 | Loss: 0.00001020
Iteration 15/1000 | Loss: 0.00001016
Iteration 16/1000 | Loss: 0.00001015
Iteration 17/1000 | Loss: 0.00001014
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001008
Iteration 23/1000 | Loss: 0.00001008
Iteration 24/1000 | Loss: 0.00001008
Iteration 25/1000 | Loss: 0.00001008
Iteration 26/1000 | Loss: 0.00001008
Iteration 27/1000 | Loss: 0.00001008
Iteration 28/1000 | Loss: 0.00001007
Iteration 29/1000 | Loss: 0.00001007
Iteration 30/1000 | Loss: 0.00001006
Iteration 31/1000 | Loss: 0.00001006
Iteration 32/1000 | Loss: 0.00001005
Iteration 33/1000 | Loss: 0.00001005
Iteration 34/1000 | Loss: 0.00001005
Iteration 35/1000 | Loss: 0.00001004
Iteration 36/1000 | Loss: 0.00001004
Iteration 37/1000 | Loss: 0.00001004
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001004
Iteration 40/1000 | Loss: 0.00001004
Iteration 41/1000 | Loss: 0.00001003
Iteration 42/1000 | Loss: 0.00001003
Iteration 43/1000 | Loss: 0.00001003
Iteration 44/1000 | Loss: 0.00001003
Iteration 45/1000 | Loss: 0.00001003
Iteration 46/1000 | Loss: 0.00001003
Iteration 47/1000 | Loss: 0.00001003
Iteration 48/1000 | Loss: 0.00001002
Iteration 49/1000 | Loss: 0.00001002
Iteration 50/1000 | Loss: 0.00001001
Iteration 51/1000 | Loss: 0.00001001
Iteration 52/1000 | Loss: 0.00001001
Iteration 53/1000 | Loss: 0.00001001
Iteration 54/1000 | Loss: 0.00001000
Iteration 55/1000 | Loss: 0.00001000
Iteration 56/1000 | Loss: 0.00001000
Iteration 57/1000 | Loss: 0.00001000
Iteration 58/1000 | Loss: 0.00001000
Iteration 59/1000 | Loss: 0.00001000
Iteration 60/1000 | Loss: 0.00000999
Iteration 61/1000 | Loss: 0.00000999
Iteration 62/1000 | Loss: 0.00000999
Iteration 63/1000 | Loss: 0.00000999
Iteration 64/1000 | Loss: 0.00000999
Iteration 65/1000 | Loss: 0.00000999
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000998
Iteration 70/1000 | Loss: 0.00000998
Iteration 71/1000 | Loss: 0.00000998
Iteration 72/1000 | Loss: 0.00000997
Iteration 73/1000 | Loss: 0.00000997
Iteration 74/1000 | Loss: 0.00000997
Iteration 75/1000 | Loss: 0.00000997
Iteration 76/1000 | Loss: 0.00000997
Iteration 77/1000 | Loss: 0.00000997
Iteration 78/1000 | Loss: 0.00000997
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000997
Iteration 81/1000 | Loss: 0.00000997
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000996
Iteration 85/1000 | Loss: 0.00000996
Iteration 86/1000 | Loss: 0.00000996
Iteration 87/1000 | Loss: 0.00000996
Iteration 88/1000 | Loss: 0.00000996
Iteration 89/1000 | Loss: 0.00000996
Iteration 90/1000 | Loss: 0.00000996
Iteration 91/1000 | Loss: 0.00000996
Iteration 92/1000 | Loss: 0.00000996
Iteration 93/1000 | Loss: 0.00000996
Iteration 94/1000 | Loss: 0.00000996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [9.964009223040193e-06, 9.964009223040193e-06, 9.964009223040193e-06, 9.964009223040193e-06, 9.964009223040193e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.964009223040193e-06

Optimization complete. Final v2v error: 2.700373888015747 mm

Highest mean error: 3.1266696453094482 mm for frame 203

Lowest mean error: 2.447441577911377 mm for frame 109

Saving results

Total time: 35.5105767250061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400551
Iteration 2/25 | Loss: 0.00126840
Iteration 3/25 | Loss: 0.00116468
Iteration 4/25 | Loss: 0.00115382
Iteration 5/25 | Loss: 0.00115144
Iteration 6/25 | Loss: 0.00115071
Iteration 7/25 | Loss: 0.00115053
Iteration 8/25 | Loss: 0.00115053
Iteration 9/25 | Loss: 0.00115053
Iteration 10/25 | Loss: 0.00115053
Iteration 11/25 | Loss: 0.00115053
Iteration 12/25 | Loss: 0.00115053
Iteration 13/25 | Loss: 0.00115053
Iteration 14/25 | Loss: 0.00115053
Iteration 15/25 | Loss: 0.00115053
Iteration 16/25 | Loss: 0.00115053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011505342554301023, 0.0011505342554301023, 0.0011505342554301023, 0.0011505342554301023, 0.0011505342554301023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011505342554301023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45833981
Iteration 2/25 | Loss: 0.00082282
Iteration 3/25 | Loss: 0.00082282
Iteration 4/25 | Loss: 0.00082282
Iteration 5/25 | Loss: 0.00082282
Iteration 6/25 | Loss: 0.00082282
Iteration 7/25 | Loss: 0.00082282
Iteration 8/25 | Loss: 0.00082282
Iteration 9/25 | Loss: 0.00082281
Iteration 10/25 | Loss: 0.00082281
Iteration 11/25 | Loss: 0.00082281
Iteration 12/25 | Loss: 0.00082281
Iteration 13/25 | Loss: 0.00082281
Iteration 14/25 | Loss: 0.00082281
Iteration 15/25 | Loss: 0.00082281
Iteration 16/25 | Loss: 0.00082281
Iteration 17/25 | Loss: 0.00082281
Iteration 18/25 | Loss: 0.00082281
Iteration 19/25 | Loss: 0.00082281
Iteration 20/25 | Loss: 0.00082281
Iteration 21/25 | Loss: 0.00082281
Iteration 22/25 | Loss: 0.00082281
Iteration 23/25 | Loss: 0.00082281
Iteration 24/25 | Loss: 0.00082281
Iteration 25/25 | Loss: 0.00082281

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082281
Iteration 2/1000 | Loss: 0.00002959
Iteration 3/1000 | Loss: 0.00001973
Iteration 4/1000 | Loss: 0.00001540
Iteration 5/1000 | Loss: 0.00001425
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001236
Iteration 10/1000 | Loss: 0.00001221
Iteration 11/1000 | Loss: 0.00001218
Iteration 12/1000 | Loss: 0.00001211
Iteration 13/1000 | Loss: 0.00001207
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001196
Iteration 17/1000 | Loss: 0.00001193
Iteration 18/1000 | Loss: 0.00001189
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001177
Iteration 29/1000 | Loss: 0.00001175
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001171
Iteration 33/1000 | Loss: 0.00001169
Iteration 34/1000 | Loss: 0.00001169
Iteration 35/1000 | Loss: 0.00001169
Iteration 36/1000 | Loss: 0.00001168
Iteration 37/1000 | Loss: 0.00001168
Iteration 38/1000 | Loss: 0.00001168
Iteration 39/1000 | Loss: 0.00001167
Iteration 40/1000 | Loss: 0.00001164
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001161
Iteration 50/1000 | Loss: 0.00001160
Iteration 51/1000 | Loss: 0.00001159
Iteration 52/1000 | Loss: 0.00001159
Iteration 53/1000 | Loss: 0.00001159
Iteration 54/1000 | Loss: 0.00001158
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001157
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001153
Iteration 73/1000 | Loss: 0.00001153
Iteration 74/1000 | Loss: 0.00001153
Iteration 75/1000 | Loss: 0.00001153
Iteration 76/1000 | Loss: 0.00001153
Iteration 77/1000 | Loss: 0.00001153
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001152
Iteration 81/1000 | Loss: 0.00001152
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001151
Iteration 84/1000 | Loss: 0.00001151
Iteration 85/1000 | Loss: 0.00001151
Iteration 86/1000 | Loss: 0.00001151
Iteration 87/1000 | Loss: 0.00001150
Iteration 88/1000 | Loss: 0.00001150
Iteration 89/1000 | Loss: 0.00001150
Iteration 90/1000 | Loss: 0.00001150
Iteration 91/1000 | Loss: 0.00001150
Iteration 92/1000 | Loss: 0.00001150
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001149
Iteration 97/1000 | Loss: 0.00001149
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001148
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001147
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001141
Iteration 134/1000 | Loss: 0.00001141
Iteration 135/1000 | Loss: 0.00001141
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001138
Iteration 146/1000 | Loss: 0.00001138
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001138
Iteration 149/1000 | Loss: 0.00001138
Iteration 150/1000 | Loss: 0.00001138
Iteration 151/1000 | Loss: 0.00001138
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001136
Iteration 159/1000 | Loss: 0.00001136
Iteration 160/1000 | Loss: 0.00001136
Iteration 161/1000 | Loss: 0.00001136
Iteration 162/1000 | Loss: 0.00001136
Iteration 163/1000 | Loss: 0.00001136
Iteration 164/1000 | Loss: 0.00001135
Iteration 165/1000 | Loss: 0.00001135
Iteration 166/1000 | Loss: 0.00001135
Iteration 167/1000 | Loss: 0.00001135
Iteration 168/1000 | Loss: 0.00001135
Iteration 169/1000 | Loss: 0.00001135
Iteration 170/1000 | Loss: 0.00001135
Iteration 171/1000 | Loss: 0.00001135
Iteration 172/1000 | Loss: 0.00001135
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001134
Iteration 175/1000 | Loss: 0.00001134
Iteration 176/1000 | Loss: 0.00001134
Iteration 177/1000 | Loss: 0.00001134
Iteration 178/1000 | Loss: 0.00001134
Iteration 179/1000 | Loss: 0.00001133
Iteration 180/1000 | Loss: 0.00001133
Iteration 181/1000 | Loss: 0.00001133
Iteration 182/1000 | Loss: 0.00001133
Iteration 183/1000 | Loss: 0.00001133
Iteration 184/1000 | Loss: 0.00001133
Iteration 185/1000 | Loss: 0.00001133
Iteration 186/1000 | Loss: 0.00001133
Iteration 187/1000 | Loss: 0.00001133
Iteration 188/1000 | Loss: 0.00001133
Iteration 189/1000 | Loss: 0.00001133
Iteration 190/1000 | Loss: 0.00001133
Iteration 191/1000 | Loss: 0.00001133
Iteration 192/1000 | Loss: 0.00001132
Iteration 193/1000 | Loss: 0.00001132
Iteration 194/1000 | Loss: 0.00001132
Iteration 195/1000 | Loss: 0.00001132
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001132
Iteration 203/1000 | Loss: 0.00001132
Iteration 204/1000 | Loss: 0.00001132
Iteration 205/1000 | Loss: 0.00001132
Iteration 206/1000 | Loss: 0.00001132
Iteration 207/1000 | Loss: 0.00001132
Iteration 208/1000 | Loss: 0.00001132
Iteration 209/1000 | Loss: 0.00001132
Iteration 210/1000 | Loss: 0.00001132
Iteration 211/1000 | Loss: 0.00001132
Iteration 212/1000 | Loss: 0.00001132
Iteration 213/1000 | Loss: 0.00001132
Iteration 214/1000 | Loss: 0.00001132
Iteration 215/1000 | Loss: 0.00001132
Iteration 216/1000 | Loss: 0.00001132
Iteration 217/1000 | Loss: 0.00001132
Iteration 218/1000 | Loss: 0.00001132
Iteration 219/1000 | Loss: 0.00001132
Iteration 220/1000 | Loss: 0.00001132
Iteration 221/1000 | Loss: 0.00001132
Iteration 222/1000 | Loss: 0.00001132
Iteration 223/1000 | Loss: 0.00001132
Iteration 224/1000 | Loss: 0.00001132
Iteration 225/1000 | Loss: 0.00001132
Iteration 226/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.1315067240502685e-05, 1.1315067240502685e-05, 1.1315067240502685e-05, 1.1315067240502685e-05, 1.1315067240502685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1315067240502685e-05

Optimization complete. Final v2v error: 2.824599504470825 mm

Highest mean error: 4.2131218910217285 mm for frame 60

Lowest mean error: 2.555352210998535 mm for frame 99

Saving results

Total time: 44.262068033218384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965181
Iteration 2/25 | Loss: 0.00169083
Iteration 3/25 | Loss: 0.00141923
Iteration 4/25 | Loss: 0.00138302
Iteration 5/25 | Loss: 0.00137300
Iteration 6/25 | Loss: 0.00137071
Iteration 7/25 | Loss: 0.00137015
Iteration 8/25 | Loss: 0.00137015
Iteration 9/25 | Loss: 0.00137015
Iteration 10/25 | Loss: 0.00137015
Iteration 11/25 | Loss: 0.00137015
Iteration 12/25 | Loss: 0.00137015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001370151643641293, 0.001370151643641293, 0.001370151643641293, 0.001370151643641293, 0.001370151643641293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001370151643641293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25883031
Iteration 2/25 | Loss: 0.00104819
Iteration 3/25 | Loss: 0.00104816
Iteration 4/25 | Loss: 0.00104816
Iteration 5/25 | Loss: 0.00104815
Iteration 6/25 | Loss: 0.00104815
Iteration 7/25 | Loss: 0.00104815
Iteration 8/25 | Loss: 0.00104815
Iteration 9/25 | Loss: 0.00104815
Iteration 10/25 | Loss: 0.00104815
Iteration 11/25 | Loss: 0.00104815
Iteration 12/25 | Loss: 0.00104815
Iteration 13/25 | Loss: 0.00104815
Iteration 14/25 | Loss: 0.00104815
Iteration 15/25 | Loss: 0.00104815
Iteration 16/25 | Loss: 0.00104815
Iteration 17/25 | Loss: 0.00104815
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010481530334800482, 0.0010481530334800482, 0.0010481530334800482, 0.0010481530334800482, 0.0010481530334800482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010481530334800482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104815
Iteration 2/1000 | Loss: 0.00008247
Iteration 3/1000 | Loss: 0.00005356
Iteration 4/1000 | Loss: 0.00004564
Iteration 5/1000 | Loss: 0.00004316
Iteration 6/1000 | Loss: 0.00004156
Iteration 7/1000 | Loss: 0.00004048
Iteration 8/1000 | Loss: 0.00003969
Iteration 9/1000 | Loss: 0.00003911
Iteration 10/1000 | Loss: 0.00003858
Iteration 11/1000 | Loss: 0.00003822
Iteration 12/1000 | Loss: 0.00003792
Iteration 13/1000 | Loss: 0.00003765
Iteration 14/1000 | Loss: 0.00003743
Iteration 15/1000 | Loss: 0.00003726
Iteration 16/1000 | Loss: 0.00003723
Iteration 17/1000 | Loss: 0.00003710
Iteration 18/1000 | Loss: 0.00003705
Iteration 19/1000 | Loss: 0.00003699
Iteration 20/1000 | Loss: 0.00003694
Iteration 21/1000 | Loss: 0.00003692
Iteration 22/1000 | Loss: 0.00003691
Iteration 23/1000 | Loss: 0.00003690
Iteration 24/1000 | Loss: 0.00003689
Iteration 25/1000 | Loss: 0.00003686
Iteration 26/1000 | Loss: 0.00003684
Iteration 27/1000 | Loss: 0.00003680
Iteration 28/1000 | Loss: 0.00003676
Iteration 29/1000 | Loss: 0.00003676
Iteration 30/1000 | Loss: 0.00003676
Iteration 31/1000 | Loss: 0.00003676
Iteration 32/1000 | Loss: 0.00003676
Iteration 33/1000 | Loss: 0.00003675
Iteration 34/1000 | Loss: 0.00003675
Iteration 35/1000 | Loss: 0.00003675
Iteration 36/1000 | Loss: 0.00003675
Iteration 37/1000 | Loss: 0.00003675
Iteration 38/1000 | Loss: 0.00003674
Iteration 39/1000 | Loss: 0.00003670
Iteration 40/1000 | Loss: 0.00003668
Iteration 41/1000 | Loss: 0.00003666
Iteration 42/1000 | Loss: 0.00003665
Iteration 43/1000 | Loss: 0.00003663
Iteration 44/1000 | Loss: 0.00003663
Iteration 45/1000 | Loss: 0.00003662
Iteration 46/1000 | Loss: 0.00003661
Iteration 47/1000 | Loss: 0.00003660
Iteration 48/1000 | Loss: 0.00003659
Iteration 49/1000 | Loss: 0.00003659
Iteration 50/1000 | Loss: 0.00003658
Iteration 51/1000 | Loss: 0.00003658
Iteration 52/1000 | Loss: 0.00003658
Iteration 53/1000 | Loss: 0.00003657
Iteration 54/1000 | Loss: 0.00003657
Iteration 55/1000 | Loss: 0.00003657
Iteration 56/1000 | Loss: 0.00003656
Iteration 57/1000 | Loss: 0.00003656
Iteration 58/1000 | Loss: 0.00003656
Iteration 59/1000 | Loss: 0.00003655
Iteration 60/1000 | Loss: 0.00003655
Iteration 61/1000 | Loss: 0.00003655
Iteration 62/1000 | Loss: 0.00003655
Iteration 63/1000 | Loss: 0.00003655
Iteration 64/1000 | Loss: 0.00003654
Iteration 65/1000 | Loss: 0.00003654
Iteration 66/1000 | Loss: 0.00003654
Iteration 67/1000 | Loss: 0.00003653
Iteration 68/1000 | Loss: 0.00003653
Iteration 69/1000 | Loss: 0.00003653
Iteration 70/1000 | Loss: 0.00003653
Iteration 71/1000 | Loss: 0.00003653
Iteration 72/1000 | Loss: 0.00003652
Iteration 73/1000 | Loss: 0.00003652
Iteration 74/1000 | Loss: 0.00003652
Iteration 75/1000 | Loss: 0.00003652
Iteration 76/1000 | Loss: 0.00003652
Iteration 77/1000 | Loss: 0.00003651
Iteration 78/1000 | Loss: 0.00003651
Iteration 79/1000 | Loss: 0.00003651
Iteration 80/1000 | Loss: 0.00003651
Iteration 81/1000 | Loss: 0.00003651
Iteration 82/1000 | Loss: 0.00003651
Iteration 83/1000 | Loss: 0.00003651
Iteration 84/1000 | Loss: 0.00003650
Iteration 85/1000 | Loss: 0.00003650
Iteration 86/1000 | Loss: 0.00003650
Iteration 87/1000 | Loss: 0.00003649
Iteration 88/1000 | Loss: 0.00003649
Iteration 89/1000 | Loss: 0.00003649
Iteration 90/1000 | Loss: 0.00003649
Iteration 91/1000 | Loss: 0.00003649
Iteration 92/1000 | Loss: 0.00003648
Iteration 93/1000 | Loss: 0.00003648
Iteration 94/1000 | Loss: 0.00003648
Iteration 95/1000 | Loss: 0.00003648
Iteration 96/1000 | Loss: 0.00003647
Iteration 97/1000 | Loss: 0.00003647
Iteration 98/1000 | Loss: 0.00003647
Iteration 99/1000 | Loss: 0.00003647
Iteration 100/1000 | Loss: 0.00003647
Iteration 101/1000 | Loss: 0.00003647
Iteration 102/1000 | Loss: 0.00003646
Iteration 103/1000 | Loss: 0.00003646
Iteration 104/1000 | Loss: 0.00003646
Iteration 105/1000 | Loss: 0.00003646
Iteration 106/1000 | Loss: 0.00003646
Iteration 107/1000 | Loss: 0.00003646
Iteration 108/1000 | Loss: 0.00003646
Iteration 109/1000 | Loss: 0.00003646
Iteration 110/1000 | Loss: 0.00003646
Iteration 111/1000 | Loss: 0.00003646
Iteration 112/1000 | Loss: 0.00003645
Iteration 113/1000 | Loss: 0.00003645
Iteration 114/1000 | Loss: 0.00003645
Iteration 115/1000 | Loss: 0.00003645
Iteration 116/1000 | Loss: 0.00003645
Iteration 117/1000 | Loss: 0.00003645
Iteration 118/1000 | Loss: 0.00003645
Iteration 119/1000 | Loss: 0.00003645
Iteration 120/1000 | Loss: 0.00003645
Iteration 121/1000 | Loss: 0.00003645
Iteration 122/1000 | Loss: 0.00003644
Iteration 123/1000 | Loss: 0.00003644
Iteration 124/1000 | Loss: 0.00003644
Iteration 125/1000 | Loss: 0.00003644
Iteration 126/1000 | Loss: 0.00003644
Iteration 127/1000 | Loss: 0.00003644
Iteration 128/1000 | Loss: 0.00003643
Iteration 129/1000 | Loss: 0.00003643
Iteration 130/1000 | Loss: 0.00003643
Iteration 131/1000 | Loss: 0.00003643
Iteration 132/1000 | Loss: 0.00003643
Iteration 133/1000 | Loss: 0.00003643
Iteration 134/1000 | Loss: 0.00003643
Iteration 135/1000 | Loss: 0.00003643
Iteration 136/1000 | Loss: 0.00003643
Iteration 137/1000 | Loss: 0.00003643
Iteration 138/1000 | Loss: 0.00003643
Iteration 139/1000 | Loss: 0.00003643
Iteration 140/1000 | Loss: 0.00003642
Iteration 141/1000 | Loss: 0.00003642
Iteration 142/1000 | Loss: 0.00003642
Iteration 143/1000 | Loss: 0.00003642
Iteration 144/1000 | Loss: 0.00003642
Iteration 145/1000 | Loss: 0.00003642
Iteration 146/1000 | Loss: 0.00003642
Iteration 147/1000 | Loss: 0.00003642
Iteration 148/1000 | Loss: 0.00003642
Iteration 149/1000 | Loss: 0.00003642
Iteration 150/1000 | Loss: 0.00003642
Iteration 151/1000 | Loss: 0.00003642
Iteration 152/1000 | Loss: 0.00003642
Iteration 153/1000 | Loss: 0.00003642
Iteration 154/1000 | Loss: 0.00003642
Iteration 155/1000 | Loss: 0.00003642
Iteration 156/1000 | Loss: 0.00003642
Iteration 157/1000 | Loss: 0.00003642
Iteration 158/1000 | Loss: 0.00003642
Iteration 159/1000 | Loss: 0.00003641
Iteration 160/1000 | Loss: 0.00003641
Iteration 161/1000 | Loss: 0.00003641
Iteration 162/1000 | Loss: 0.00003641
Iteration 163/1000 | Loss: 0.00003641
Iteration 164/1000 | Loss: 0.00003641
Iteration 165/1000 | Loss: 0.00003641
Iteration 166/1000 | Loss: 0.00003641
Iteration 167/1000 | Loss: 0.00003641
Iteration 168/1000 | Loss: 0.00003641
Iteration 169/1000 | Loss: 0.00003641
Iteration 170/1000 | Loss: 0.00003641
Iteration 171/1000 | Loss: 0.00003641
Iteration 172/1000 | Loss: 0.00003641
Iteration 173/1000 | Loss: 0.00003641
Iteration 174/1000 | Loss: 0.00003641
Iteration 175/1000 | Loss: 0.00003641
Iteration 176/1000 | Loss: 0.00003641
Iteration 177/1000 | Loss: 0.00003641
Iteration 178/1000 | Loss: 0.00003641
Iteration 179/1000 | Loss: 0.00003641
Iteration 180/1000 | Loss: 0.00003641
Iteration 181/1000 | Loss: 0.00003641
Iteration 182/1000 | Loss: 0.00003641
Iteration 183/1000 | Loss: 0.00003641
Iteration 184/1000 | Loss: 0.00003641
Iteration 185/1000 | Loss: 0.00003641
Iteration 186/1000 | Loss: 0.00003641
Iteration 187/1000 | Loss: 0.00003641
Iteration 188/1000 | Loss: 0.00003641
Iteration 189/1000 | Loss: 0.00003641
Iteration 190/1000 | Loss: 0.00003641
Iteration 191/1000 | Loss: 0.00003641
Iteration 192/1000 | Loss: 0.00003641
Iteration 193/1000 | Loss: 0.00003641
Iteration 194/1000 | Loss: 0.00003641
Iteration 195/1000 | Loss: 0.00003641
Iteration 196/1000 | Loss: 0.00003641
Iteration 197/1000 | Loss: 0.00003641
Iteration 198/1000 | Loss: 0.00003641
Iteration 199/1000 | Loss: 0.00003641
Iteration 200/1000 | Loss: 0.00003641
Iteration 201/1000 | Loss: 0.00003641
Iteration 202/1000 | Loss: 0.00003641
Iteration 203/1000 | Loss: 0.00003641
Iteration 204/1000 | Loss: 0.00003641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [3.6412853660294786e-05, 3.6412853660294786e-05, 3.6412853660294786e-05, 3.6412853660294786e-05, 3.6412853660294786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6412853660294786e-05

Optimization complete. Final v2v error: 4.822264671325684 mm

Highest mean error: 5.986981391906738 mm for frame 123

Lowest mean error: 3.84740948677063 mm for frame 78

Saving results

Total time: 56.95579791069031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416946
Iteration 2/25 | Loss: 0.00121441
Iteration 3/25 | Loss: 0.00116117
Iteration 4/25 | Loss: 0.00115514
Iteration 5/25 | Loss: 0.00115328
Iteration 6/25 | Loss: 0.00115305
Iteration 7/25 | Loss: 0.00115305
Iteration 8/25 | Loss: 0.00115305
Iteration 9/25 | Loss: 0.00115305
Iteration 10/25 | Loss: 0.00115305
Iteration 11/25 | Loss: 0.00115305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011530485935509205, 0.0011530485935509205, 0.0011530485935509205, 0.0011530485935509205, 0.0011530485935509205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011530485935509205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37602472
Iteration 2/25 | Loss: 0.00082172
Iteration 3/25 | Loss: 0.00082171
Iteration 4/25 | Loss: 0.00082171
Iteration 5/25 | Loss: 0.00082171
Iteration 6/25 | Loss: 0.00082171
Iteration 7/25 | Loss: 0.00082171
Iteration 8/25 | Loss: 0.00082171
Iteration 9/25 | Loss: 0.00082171
Iteration 10/25 | Loss: 0.00082171
Iteration 11/25 | Loss: 0.00082171
Iteration 12/25 | Loss: 0.00082171
Iteration 13/25 | Loss: 0.00082171
Iteration 14/25 | Loss: 0.00082171
Iteration 15/25 | Loss: 0.00082171
Iteration 16/25 | Loss: 0.00082171
Iteration 17/25 | Loss: 0.00082171
Iteration 18/25 | Loss: 0.00082171
Iteration 19/25 | Loss: 0.00082171
Iteration 20/25 | Loss: 0.00082171
Iteration 21/25 | Loss: 0.00082171
Iteration 22/25 | Loss: 0.00082171
Iteration 23/25 | Loss: 0.00082171
Iteration 24/25 | Loss: 0.00082171
Iteration 25/25 | Loss: 0.00082171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082171
Iteration 2/1000 | Loss: 0.00002289
Iteration 3/1000 | Loss: 0.00001444
Iteration 4/1000 | Loss: 0.00001311
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001193
Iteration 8/1000 | Loss: 0.00001169
Iteration 9/1000 | Loss: 0.00001158
Iteration 10/1000 | Loss: 0.00001137
Iteration 11/1000 | Loss: 0.00001127
Iteration 12/1000 | Loss: 0.00001124
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001108
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001103
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001096
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001096
Iteration 22/1000 | Loss: 0.00001095
Iteration 23/1000 | Loss: 0.00001095
Iteration 24/1000 | Loss: 0.00001095
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001094
Iteration 28/1000 | Loss: 0.00001094
Iteration 29/1000 | Loss: 0.00001094
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001092
Iteration 36/1000 | Loss: 0.00001091
Iteration 37/1000 | Loss: 0.00001088
Iteration 38/1000 | Loss: 0.00001087
Iteration 39/1000 | Loss: 0.00001086
Iteration 40/1000 | Loss: 0.00001085
Iteration 41/1000 | Loss: 0.00001084
Iteration 42/1000 | Loss: 0.00001084
Iteration 43/1000 | Loss: 0.00001084
Iteration 44/1000 | Loss: 0.00001084
Iteration 45/1000 | Loss: 0.00001084
Iteration 46/1000 | Loss: 0.00001084
Iteration 47/1000 | Loss: 0.00001083
Iteration 48/1000 | Loss: 0.00001083
Iteration 49/1000 | Loss: 0.00001083
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001082
Iteration 54/1000 | Loss: 0.00001082
Iteration 55/1000 | Loss: 0.00001082
Iteration 56/1000 | Loss: 0.00001082
Iteration 57/1000 | Loss: 0.00001082
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001082
Iteration 60/1000 | Loss: 0.00001082
Iteration 61/1000 | Loss: 0.00001082
Iteration 62/1000 | Loss: 0.00001082
Iteration 63/1000 | Loss: 0.00001082
Iteration 64/1000 | Loss: 0.00001082
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001078
Iteration 78/1000 | Loss: 0.00001078
Iteration 79/1000 | Loss: 0.00001078
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001078
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001077
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001075
Iteration 102/1000 | Loss: 0.00001075
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001074
Iteration 105/1000 | Loss: 0.00001074
Iteration 106/1000 | Loss: 0.00001074
Iteration 107/1000 | Loss: 0.00001073
Iteration 108/1000 | Loss: 0.00001073
Iteration 109/1000 | Loss: 0.00001072
Iteration 110/1000 | Loss: 0.00001072
Iteration 111/1000 | Loss: 0.00001072
Iteration 112/1000 | Loss: 0.00001072
Iteration 113/1000 | Loss: 0.00001072
Iteration 114/1000 | Loss: 0.00001072
Iteration 115/1000 | Loss: 0.00001072
Iteration 116/1000 | Loss: 0.00001072
Iteration 117/1000 | Loss: 0.00001071
Iteration 118/1000 | Loss: 0.00001071
Iteration 119/1000 | Loss: 0.00001071
Iteration 120/1000 | Loss: 0.00001071
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001070
Iteration 135/1000 | Loss: 0.00001070
Iteration 136/1000 | Loss: 0.00001070
Iteration 137/1000 | Loss: 0.00001070
Iteration 138/1000 | Loss: 0.00001070
Iteration 139/1000 | Loss: 0.00001069
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001069
Iteration 143/1000 | Loss: 0.00001069
Iteration 144/1000 | Loss: 0.00001069
Iteration 145/1000 | Loss: 0.00001069
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001069
Iteration 148/1000 | Loss: 0.00001068
Iteration 149/1000 | Loss: 0.00001068
Iteration 150/1000 | Loss: 0.00001068
Iteration 151/1000 | Loss: 0.00001068
Iteration 152/1000 | Loss: 0.00001068
Iteration 153/1000 | Loss: 0.00001067
Iteration 154/1000 | Loss: 0.00001067
Iteration 155/1000 | Loss: 0.00001067
Iteration 156/1000 | Loss: 0.00001066
Iteration 157/1000 | Loss: 0.00001066
Iteration 158/1000 | Loss: 0.00001066
Iteration 159/1000 | Loss: 0.00001066
Iteration 160/1000 | Loss: 0.00001066
Iteration 161/1000 | Loss: 0.00001065
Iteration 162/1000 | Loss: 0.00001065
Iteration 163/1000 | Loss: 0.00001065
Iteration 164/1000 | Loss: 0.00001065
Iteration 165/1000 | Loss: 0.00001064
Iteration 166/1000 | Loss: 0.00001064
Iteration 167/1000 | Loss: 0.00001064
Iteration 168/1000 | Loss: 0.00001064
Iteration 169/1000 | Loss: 0.00001064
Iteration 170/1000 | Loss: 0.00001064
Iteration 171/1000 | Loss: 0.00001064
Iteration 172/1000 | Loss: 0.00001064
Iteration 173/1000 | Loss: 0.00001064
Iteration 174/1000 | Loss: 0.00001064
Iteration 175/1000 | Loss: 0.00001064
Iteration 176/1000 | Loss: 0.00001064
Iteration 177/1000 | Loss: 0.00001064
Iteration 178/1000 | Loss: 0.00001064
Iteration 179/1000 | Loss: 0.00001064
Iteration 180/1000 | Loss: 0.00001064
Iteration 181/1000 | Loss: 0.00001063
Iteration 182/1000 | Loss: 0.00001063
Iteration 183/1000 | Loss: 0.00001063
Iteration 184/1000 | Loss: 0.00001063
Iteration 185/1000 | Loss: 0.00001063
Iteration 186/1000 | Loss: 0.00001063
Iteration 187/1000 | Loss: 0.00001063
Iteration 188/1000 | Loss: 0.00001063
Iteration 189/1000 | Loss: 0.00001063
Iteration 190/1000 | Loss: 0.00001063
Iteration 191/1000 | Loss: 0.00001063
Iteration 192/1000 | Loss: 0.00001063
Iteration 193/1000 | Loss: 0.00001063
Iteration 194/1000 | Loss: 0.00001063
Iteration 195/1000 | Loss: 0.00001063
Iteration 196/1000 | Loss: 0.00001063
Iteration 197/1000 | Loss: 0.00001063
Iteration 198/1000 | Loss: 0.00001063
Iteration 199/1000 | Loss: 0.00001063
Iteration 200/1000 | Loss: 0.00001063
Iteration 201/1000 | Loss: 0.00001063
Iteration 202/1000 | Loss: 0.00001063
Iteration 203/1000 | Loss: 0.00001063
Iteration 204/1000 | Loss: 0.00001063
Iteration 205/1000 | Loss: 0.00001063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.0632068551785778e-05, 1.0632068551785778e-05, 1.0632068551785778e-05, 1.0632068551785778e-05, 1.0632068551785778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0632068551785778e-05

Optimization complete. Final v2v error: 2.809339761734009 mm

Highest mean error: 2.8782310485839844 mm for frame 15

Lowest mean error: 2.7247228622436523 mm for frame 27

Saving results

Total time: 36.93240022659302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496060
Iteration 2/25 | Loss: 0.00143754
Iteration 3/25 | Loss: 0.00124733
Iteration 4/25 | Loss: 0.00122607
Iteration 5/25 | Loss: 0.00121979
Iteration 6/25 | Loss: 0.00121763
Iteration 7/25 | Loss: 0.00121693
Iteration 8/25 | Loss: 0.00121666
Iteration 9/25 | Loss: 0.00121666
Iteration 10/25 | Loss: 0.00121666
Iteration 11/25 | Loss: 0.00121666
Iteration 12/25 | Loss: 0.00121666
Iteration 13/25 | Loss: 0.00121666
Iteration 14/25 | Loss: 0.00121666
Iteration 15/25 | Loss: 0.00121666
Iteration 16/25 | Loss: 0.00121666
Iteration 17/25 | Loss: 0.00121666
Iteration 18/25 | Loss: 0.00121666
Iteration 19/25 | Loss: 0.00121666
Iteration 20/25 | Loss: 0.00121666
Iteration 21/25 | Loss: 0.00121666
Iteration 22/25 | Loss: 0.00121666
Iteration 23/25 | Loss: 0.00121666
Iteration 24/25 | Loss: 0.00121666
Iteration 25/25 | Loss: 0.00121666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36920166
Iteration 2/25 | Loss: 0.00108597
Iteration 3/25 | Loss: 0.00108596
Iteration 4/25 | Loss: 0.00108596
Iteration 5/25 | Loss: 0.00108596
Iteration 6/25 | Loss: 0.00108596
Iteration 7/25 | Loss: 0.00108596
Iteration 8/25 | Loss: 0.00108596
Iteration 9/25 | Loss: 0.00108596
Iteration 10/25 | Loss: 0.00108596
Iteration 11/25 | Loss: 0.00108596
Iteration 12/25 | Loss: 0.00108596
Iteration 13/25 | Loss: 0.00108596
Iteration 14/25 | Loss: 0.00108596
Iteration 15/25 | Loss: 0.00108596
Iteration 16/25 | Loss: 0.00108596
Iteration 17/25 | Loss: 0.00108596
Iteration 18/25 | Loss: 0.00108596
Iteration 19/25 | Loss: 0.00108596
Iteration 20/25 | Loss: 0.00108596
Iteration 21/25 | Loss: 0.00108596
Iteration 22/25 | Loss: 0.00108596
Iteration 23/25 | Loss: 0.00108596
Iteration 24/25 | Loss: 0.00108596
Iteration 25/25 | Loss: 0.00108596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108596
Iteration 2/1000 | Loss: 0.00006483
Iteration 3/1000 | Loss: 0.00004098
Iteration 4/1000 | Loss: 0.00003351
Iteration 5/1000 | Loss: 0.00003080
Iteration 6/1000 | Loss: 0.00002951
Iteration 7/1000 | Loss: 0.00002860
Iteration 8/1000 | Loss: 0.00002775
Iteration 9/1000 | Loss: 0.00002702
Iteration 10/1000 | Loss: 0.00002648
Iteration 11/1000 | Loss: 0.00002601
Iteration 12/1000 | Loss: 0.00002573
Iteration 13/1000 | Loss: 0.00002539
Iteration 14/1000 | Loss: 0.00002514
Iteration 15/1000 | Loss: 0.00002500
Iteration 16/1000 | Loss: 0.00002488
Iteration 17/1000 | Loss: 0.00002476
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002470
Iteration 20/1000 | Loss: 0.00002467
Iteration 21/1000 | Loss: 0.00002467
Iteration 22/1000 | Loss: 0.00002465
Iteration 23/1000 | Loss: 0.00002464
Iteration 24/1000 | Loss: 0.00002464
Iteration 25/1000 | Loss: 0.00002463
Iteration 26/1000 | Loss: 0.00002463
Iteration 27/1000 | Loss: 0.00002462
Iteration 28/1000 | Loss: 0.00002462
Iteration 29/1000 | Loss: 0.00002460
Iteration 30/1000 | Loss: 0.00002460
Iteration 31/1000 | Loss: 0.00002460
Iteration 32/1000 | Loss: 0.00002459
Iteration 33/1000 | Loss: 0.00002459
Iteration 34/1000 | Loss: 0.00002456
Iteration 35/1000 | Loss: 0.00002456
Iteration 36/1000 | Loss: 0.00002455
Iteration 37/1000 | Loss: 0.00002455
Iteration 38/1000 | Loss: 0.00002455
Iteration 39/1000 | Loss: 0.00002455
Iteration 40/1000 | Loss: 0.00002455
Iteration 41/1000 | Loss: 0.00002454
Iteration 42/1000 | Loss: 0.00002454
Iteration 43/1000 | Loss: 0.00002453
Iteration 44/1000 | Loss: 0.00002453
Iteration 45/1000 | Loss: 0.00002451
Iteration 46/1000 | Loss: 0.00002450
Iteration 47/1000 | Loss: 0.00002450
Iteration 48/1000 | Loss: 0.00002450
Iteration 49/1000 | Loss: 0.00002449
Iteration 50/1000 | Loss: 0.00002448
Iteration 51/1000 | Loss: 0.00002448
Iteration 52/1000 | Loss: 0.00002448
Iteration 53/1000 | Loss: 0.00002448
Iteration 54/1000 | Loss: 0.00002448
Iteration 55/1000 | Loss: 0.00002448
Iteration 56/1000 | Loss: 0.00002448
Iteration 57/1000 | Loss: 0.00002448
Iteration 58/1000 | Loss: 0.00002447
Iteration 59/1000 | Loss: 0.00002447
Iteration 60/1000 | Loss: 0.00002447
Iteration 61/1000 | Loss: 0.00002447
Iteration 62/1000 | Loss: 0.00002447
Iteration 63/1000 | Loss: 0.00002447
Iteration 64/1000 | Loss: 0.00002447
Iteration 65/1000 | Loss: 0.00002445
Iteration 66/1000 | Loss: 0.00002445
Iteration 67/1000 | Loss: 0.00002445
Iteration 68/1000 | Loss: 0.00002444
Iteration 69/1000 | Loss: 0.00002444
Iteration 70/1000 | Loss: 0.00002444
Iteration 71/1000 | Loss: 0.00002444
Iteration 72/1000 | Loss: 0.00002443
Iteration 73/1000 | Loss: 0.00002443
Iteration 74/1000 | Loss: 0.00002443
Iteration 75/1000 | Loss: 0.00002442
Iteration 76/1000 | Loss: 0.00002442
Iteration 77/1000 | Loss: 0.00002442
Iteration 78/1000 | Loss: 0.00002441
Iteration 79/1000 | Loss: 0.00002441
Iteration 80/1000 | Loss: 0.00002441
Iteration 81/1000 | Loss: 0.00002441
Iteration 82/1000 | Loss: 0.00002440
Iteration 83/1000 | Loss: 0.00002439
Iteration 84/1000 | Loss: 0.00002439
Iteration 85/1000 | Loss: 0.00002439
Iteration 86/1000 | Loss: 0.00002439
Iteration 87/1000 | Loss: 0.00002438
Iteration 88/1000 | Loss: 0.00002438
Iteration 89/1000 | Loss: 0.00002438
Iteration 90/1000 | Loss: 0.00002438
Iteration 91/1000 | Loss: 0.00002438
Iteration 92/1000 | Loss: 0.00002438
Iteration 93/1000 | Loss: 0.00002438
Iteration 94/1000 | Loss: 0.00002438
Iteration 95/1000 | Loss: 0.00002438
Iteration 96/1000 | Loss: 0.00002438
Iteration 97/1000 | Loss: 0.00002438
Iteration 98/1000 | Loss: 0.00002438
Iteration 99/1000 | Loss: 0.00002438
Iteration 100/1000 | Loss: 0.00002438
Iteration 101/1000 | Loss: 0.00002437
Iteration 102/1000 | Loss: 0.00002437
Iteration 103/1000 | Loss: 0.00002437
Iteration 104/1000 | Loss: 0.00002437
Iteration 105/1000 | Loss: 0.00002436
Iteration 106/1000 | Loss: 0.00002436
Iteration 107/1000 | Loss: 0.00002436
Iteration 108/1000 | Loss: 0.00002436
Iteration 109/1000 | Loss: 0.00002436
Iteration 110/1000 | Loss: 0.00002436
Iteration 111/1000 | Loss: 0.00002435
Iteration 112/1000 | Loss: 0.00002435
Iteration 113/1000 | Loss: 0.00002435
Iteration 114/1000 | Loss: 0.00002434
Iteration 115/1000 | Loss: 0.00002434
Iteration 116/1000 | Loss: 0.00002434
Iteration 117/1000 | Loss: 0.00002434
Iteration 118/1000 | Loss: 0.00002434
Iteration 119/1000 | Loss: 0.00002434
Iteration 120/1000 | Loss: 0.00002434
Iteration 121/1000 | Loss: 0.00002434
Iteration 122/1000 | Loss: 0.00002433
Iteration 123/1000 | Loss: 0.00002433
Iteration 124/1000 | Loss: 0.00002433
Iteration 125/1000 | Loss: 0.00002433
Iteration 126/1000 | Loss: 0.00002433
Iteration 127/1000 | Loss: 0.00002433
Iteration 128/1000 | Loss: 0.00002433
Iteration 129/1000 | Loss: 0.00002433
Iteration 130/1000 | Loss: 0.00002433
Iteration 131/1000 | Loss: 0.00002433
Iteration 132/1000 | Loss: 0.00002433
Iteration 133/1000 | Loss: 0.00002433
Iteration 134/1000 | Loss: 0.00002433
Iteration 135/1000 | Loss: 0.00002432
Iteration 136/1000 | Loss: 0.00002432
Iteration 137/1000 | Loss: 0.00002432
Iteration 138/1000 | Loss: 0.00002432
Iteration 139/1000 | Loss: 0.00002432
Iteration 140/1000 | Loss: 0.00002432
Iteration 141/1000 | Loss: 0.00002432
Iteration 142/1000 | Loss: 0.00002432
Iteration 143/1000 | Loss: 0.00002432
Iteration 144/1000 | Loss: 0.00002432
Iteration 145/1000 | Loss: 0.00002432
Iteration 146/1000 | Loss: 0.00002431
Iteration 147/1000 | Loss: 0.00002431
Iteration 148/1000 | Loss: 0.00002431
Iteration 149/1000 | Loss: 0.00002431
Iteration 150/1000 | Loss: 0.00002431
Iteration 151/1000 | Loss: 0.00002431
Iteration 152/1000 | Loss: 0.00002430
Iteration 153/1000 | Loss: 0.00002430
Iteration 154/1000 | Loss: 0.00002430
Iteration 155/1000 | Loss: 0.00002430
Iteration 156/1000 | Loss: 0.00002430
Iteration 157/1000 | Loss: 0.00002430
Iteration 158/1000 | Loss: 0.00002430
Iteration 159/1000 | Loss: 0.00002430
Iteration 160/1000 | Loss: 0.00002430
Iteration 161/1000 | Loss: 0.00002430
Iteration 162/1000 | Loss: 0.00002430
Iteration 163/1000 | Loss: 0.00002430
Iteration 164/1000 | Loss: 0.00002430
Iteration 165/1000 | Loss: 0.00002429
Iteration 166/1000 | Loss: 0.00002429
Iteration 167/1000 | Loss: 0.00002429
Iteration 168/1000 | Loss: 0.00002429
Iteration 169/1000 | Loss: 0.00002429
Iteration 170/1000 | Loss: 0.00002429
Iteration 171/1000 | Loss: 0.00002429
Iteration 172/1000 | Loss: 0.00002429
Iteration 173/1000 | Loss: 0.00002429
Iteration 174/1000 | Loss: 0.00002429
Iteration 175/1000 | Loss: 0.00002428
Iteration 176/1000 | Loss: 0.00002428
Iteration 177/1000 | Loss: 0.00002428
Iteration 178/1000 | Loss: 0.00002428
Iteration 179/1000 | Loss: 0.00002428
Iteration 180/1000 | Loss: 0.00002428
Iteration 181/1000 | Loss: 0.00002428
Iteration 182/1000 | Loss: 0.00002428
Iteration 183/1000 | Loss: 0.00002428
Iteration 184/1000 | Loss: 0.00002428
Iteration 185/1000 | Loss: 0.00002428
Iteration 186/1000 | Loss: 0.00002428
Iteration 187/1000 | Loss: 0.00002427
Iteration 188/1000 | Loss: 0.00002427
Iteration 189/1000 | Loss: 0.00002427
Iteration 190/1000 | Loss: 0.00002427
Iteration 191/1000 | Loss: 0.00002427
Iteration 192/1000 | Loss: 0.00002427
Iteration 193/1000 | Loss: 0.00002427
Iteration 194/1000 | Loss: 0.00002427
Iteration 195/1000 | Loss: 0.00002427
Iteration 196/1000 | Loss: 0.00002427
Iteration 197/1000 | Loss: 0.00002427
Iteration 198/1000 | Loss: 0.00002426
Iteration 199/1000 | Loss: 0.00002426
Iteration 200/1000 | Loss: 0.00002426
Iteration 201/1000 | Loss: 0.00002426
Iteration 202/1000 | Loss: 0.00002426
Iteration 203/1000 | Loss: 0.00002426
Iteration 204/1000 | Loss: 0.00002426
Iteration 205/1000 | Loss: 0.00002426
Iteration 206/1000 | Loss: 0.00002426
Iteration 207/1000 | Loss: 0.00002426
Iteration 208/1000 | Loss: 0.00002426
Iteration 209/1000 | Loss: 0.00002425
Iteration 210/1000 | Loss: 0.00002425
Iteration 211/1000 | Loss: 0.00002425
Iteration 212/1000 | Loss: 0.00002425
Iteration 213/1000 | Loss: 0.00002425
Iteration 214/1000 | Loss: 0.00002425
Iteration 215/1000 | Loss: 0.00002425
Iteration 216/1000 | Loss: 0.00002425
Iteration 217/1000 | Loss: 0.00002425
Iteration 218/1000 | Loss: 0.00002425
Iteration 219/1000 | Loss: 0.00002425
Iteration 220/1000 | Loss: 0.00002425
Iteration 221/1000 | Loss: 0.00002424
Iteration 222/1000 | Loss: 0.00002424
Iteration 223/1000 | Loss: 0.00002424
Iteration 224/1000 | Loss: 0.00002424
Iteration 225/1000 | Loss: 0.00002424
Iteration 226/1000 | Loss: 0.00002424
Iteration 227/1000 | Loss: 0.00002424
Iteration 228/1000 | Loss: 0.00002424
Iteration 229/1000 | Loss: 0.00002424
Iteration 230/1000 | Loss: 0.00002424
Iteration 231/1000 | Loss: 0.00002424
Iteration 232/1000 | Loss: 0.00002424
Iteration 233/1000 | Loss: 0.00002424
Iteration 234/1000 | Loss: 0.00002423
Iteration 235/1000 | Loss: 0.00002423
Iteration 236/1000 | Loss: 0.00002423
Iteration 237/1000 | Loss: 0.00002423
Iteration 238/1000 | Loss: 0.00002423
Iteration 239/1000 | Loss: 0.00002423
Iteration 240/1000 | Loss: 0.00002423
Iteration 241/1000 | Loss: 0.00002423
Iteration 242/1000 | Loss: 0.00002423
Iteration 243/1000 | Loss: 0.00002423
Iteration 244/1000 | Loss: 0.00002423
Iteration 245/1000 | Loss: 0.00002423
Iteration 246/1000 | Loss: 0.00002423
Iteration 247/1000 | Loss: 0.00002423
Iteration 248/1000 | Loss: 0.00002423
Iteration 249/1000 | Loss: 0.00002423
Iteration 250/1000 | Loss: 0.00002423
Iteration 251/1000 | Loss: 0.00002423
Iteration 252/1000 | Loss: 0.00002423
Iteration 253/1000 | Loss: 0.00002423
Iteration 254/1000 | Loss: 0.00002423
Iteration 255/1000 | Loss: 0.00002423
Iteration 256/1000 | Loss: 0.00002423
Iteration 257/1000 | Loss: 0.00002423
Iteration 258/1000 | Loss: 0.00002423
Iteration 259/1000 | Loss: 0.00002423
Iteration 260/1000 | Loss: 0.00002423
Iteration 261/1000 | Loss: 0.00002423
Iteration 262/1000 | Loss: 0.00002423
Iteration 263/1000 | Loss: 0.00002423
Iteration 264/1000 | Loss: 0.00002423
Iteration 265/1000 | Loss: 0.00002423
Iteration 266/1000 | Loss: 0.00002423
Iteration 267/1000 | Loss: 0.00002423
Iteration 268/1000 | Loss: 0.00002423
Iteration 269/1000 | Loss: 0.00002423
Iteration 270/1000 | Loss: 0.00002423
Iteration 271/1000 | Loss: 0.00002423
Iteration 272/1000 | Loss: 0.00002423
Iteration 273/1000 | Loss: 0.00002423
Iteration 274/1000 | Loss: 0.00002423
Iteration 275/1000 | Loss: 0.00002423
Iteration 276/1000 | Loss: 0.00002423
Iteration 277/1000 | Loss: 0.00002423
Iteration 278/1000 | Loss: 0.00002423
Iteration 279/1000 | Loss: 0.00002423
Iteration 280/1000 | Loss: 0.00002423
Iteration 281/1000 | Loss: 0.00002423
Iteration 282/1000 | Loss: 0.00002423
Iteration 283/1000 | Loss: 0.00002423
Iteration 284/1000 | Loss: 0.00002423
Iteration 285/1000 | Loss: 0.00002423
Iteration 286/1000 | Loss: 0.00002423
Iteration 287/1000 | Loss: 0.00002423
Iteration 288/1000 | Loss: 0.00002423
Iteration 289/1000 | Loss: 0.00002423
Iteration 290/1000 | Loss: 0.00002423
Iteration 291/1000 | Loss: 0.00002423
Iteration 292/1000 | Loss: 0.00002423
Iteration 293/1000 | Loss: 0.00002423
Iteration 294/1000 | Loss: 0.00002423
Iteration 295/1000 | Loss: 0.00002423
Iteration 296/1000 | Loss: 0.00002423
Iteration 297/1000 | Loss: 0.00002423
Iteration 298/1000 | Loss: 0.00002423
Iteration 299/1000 | Loss: 0.00002423
Iteration 300/1000 | Loss: 0.00002423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [2.4225832021329552e-05, 2.4225832021329552e-05, 2.4225832021329552e-05, 2.4225832021329552e-05, 2.4225832021329552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4225832021329552e-05

Optimization complete. Final v2v error: 4.013477325439453 mm

Highest mean error: 5.234822750091553 mm for frame 106

Lowest mean error: 2.8776962757110596 mm for frame 75

Saving results

Total time: 54.10655093193054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932888
Iteration 2/25 | Loss: 0.00187737
Iteration 3/25 | Loss: 0.00179915
Iteration 4/25 | Loss: 0.00148649
Iteration 5/25 | Loss: 0.00144868
Iteration 6/25 | Loss: 0.00134326
Iteration 7/25 | Loss: 0.00134132
Iteration 8/25 | Loss: 0.00134082
Iteration 9/25 | Loss: 0.00134082
Iteration 10/25 | Loss: 0.00134082
Iteration 11/25 | Loss: 0.00134082
Iteration 12/25 | Loss: 0.00134082
Iteration 13/25 | Loss: 0.00134082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013408224331215024, 0.0013408224331215024, 0.0013408224331215024, 0.0013408224331215024, 0.0013408224331215024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013408224331215024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25369418
Iteration 2/25 | Loss: 0.00077642
Iteration 3/25 | Loss: 0.00077642
Iteration 4/25 | Loss: 0.00077642
Iteration 5/25 | Loss: 0.00077642
Iteration 6/25 | Loss: 0.00077642
Iteration 7/25 | Loss: 0.00077642
Iteration 8/25 | Loss: 0.00077642
Iteration 9/25 | Loss: 0.00077642
Iteration 10/25 | Loss: 0.00077642
Iteration 11/25 | Loss: 0.00077642
Iteration 12/25 | Loss: 0.00077642
Iteration 13/25 | Loss: 0.00077642
Iteration 14/25 | Loss: 0.00077642
Iteration 15/25 | Loss: 0.00077642
Iteration 16/25 | Loss: 0.00077642
Iteration 17/25 | Loss: 0.00077642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007764204638078809, 0.0007764204638078809, 0.0007764204638078809, 0.0007764204638078809, 0.0007764204638078809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007764204638078809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077642
Iteration 2/1000 | Loss: 0.00035166
Iteration 3/1000 | Loss: 0.00043192
Iteration 4/1000 | Loss: 0.00009767
Iteration 5/1000 | Loss: 0.00004742
Iteration 6/1000 | Loss: 0.00003819
Iteration 7/1000 | Loss: 0.00034790
Iteration 8/1000 | Loss: 0.00042337
Iteration 9/1000 | Loss: 0.00003948
Iteration 10/1000 | Loss: 0.00003404
Iteration 11/1000 | Loss: 0.00003266
Iteration 12/1000 | Loss: 0.00003191
Iteration 13/1000 | Loss: 0.00032493
Iteration 14/1000 | Loss: 0.00005351
Iteration 15/1000 | Loss: 0.00005967
Iteration 16/1000 | Loss: 0.00003081
Iteration 17/1000 | Loss: 0.00003035
Iteration 18/1000 | Loss: 0.00030630
Iteration 19/1000 | Loss: 0.00043322
Iteration 20/1000 | Loss: 0.00010016
Iteration 21/1000 | Loss: 0.00003022
Iteration 22/1000 | Loss: 0.00002984
Iteration 23/1000 | Loss: 0.00002961
Iteration 24/1000 | Loss: 0.00002954
Iteration 25/1000 | Loss: 0.00002947
Iteration 26/1000 | Loss: 0.00002939
Iteration 27/1000 | Loss: 0.00002938
Iteration 28/1000 | Loss: 0.00002938
Iteration 29/1000 | Loss: 0.00002937
Iteration 30/1000 | Loss: 0.00002934
Iteration 31/1000 | Loss: 0.00002928
Iteration 32/1000 | Loss: 0.00002921
Iteration 33/1000 | Loss: 0.00002920
Iteration 34/1000 | Loss: 0.00002917
Iteration 35/1000 | Loss: 0.00002912
Iteration 36/1000 | Loss: 0.00002912
Iteration 37/1000 | Loss: 0.00002912
Iteration 38/1000 | Loss: 0.00002912
Iteration 39/1000 | Loss: 0.00002911
Iteration 40/1000 | Loss: 0.00002911
Iteration 41/1000 | Loss: 0.00002911
Iteration 42/1000 | Loss: 0.00002911
Iteration 43/1000 | Loss: 0.00002911
Iteration 44/1000 | Loss: 0.00002911
Iteration 45/1000 | Loss: 0.00002911
Iteration 46/1000 | Loss: 0.00002911
Iteration 47/1000 | Loss: 0.00002911
Iteration 48/1000 | Loss: 0.00002911
Iteration 49/1000 | Loss: 0.00002910
Iteration 50/1000 | Loss: 0.00002910
Iteration 51/1000 | Loss: 0.00002908
Iteration 52/1000 | Loss: 0.00002907
Iteration 53/1000 | Loss: 0.00002907
Iteration 54/1000 | Loss: 0.00002907
Iteration 55/1000 | Loss: 0.00002907
Iteration 56/1000 | Loss: 0.00002907
Iteration 57/1000 | Loss: 0.00002907
Iteration 58/1000 | Loss: 0.00002907
Iteration 59/1000 | Loss: 0.00002907
Iteration 60/1000 | Loss: 0.00002907
Iteration 61/1000 | Loss: 0.00002907
Iteration 62/1000 | Loss: 0.00002907
Iteration 63/1000 | Loss: 0.00002907
Iteration 64/1000 | Loss: 0.00002907
Iteration 65/1000 | Loss: 0.00002906
Iteration 66/1000 | Loss: 0.00002906
Iteration 67/1000 | Loss: 0.00002906
Iteration 68/1000 | Loss: 0.00002906
Iteration 69/1000 | Loss: 0.00002906
Iteration 70/1000 | Loss: 0.00002905
Iteration 71/1000 | Loss: 0.00002905
Iteration 72/1000 | Loss: 0.00002905
Iteration 73/1000 | Loss: 0.00002905
Iteration 74/1000 | Loss: 0.00002905
Iteration 75/1000 | Loss: 0.00002905
Iteration 76/1000 | Loss: 0.00002905
Iteration 77/1000 | Loss: 0.00002904
Iteration 78/1000 | Loss: 0.00002904
Iteration 79/1000 | Loss: 0.00002904
Iteration 80/1000 | Loss: 0.00002904
Iteration 81/1000 | Loss: 0.00002904
Iteration 82/1000 | Loss: 0.00002904
Iteration 83/1000 | Loss: 0.00002904
Iteration 84/1000 | Loss: 0.00002904
Iteration 85/1000 | Loss: 0.00002904
Iteration 86/1000 | Loss: 0.00002904
Iteration 87/1000 | Loss: 0.00002904
Iteration 88/1000 | Loss: 0.00002904
Iteration 89/1000 | Loss: 0.00002904
Iteration 90/1000 | Loss: 0.00002904
Iteration 91/1000 | Loss: 0.00002903
Iteration 92/1000 | Loss: 0.00002903
Iteration 93/1000 | Loss: 0.00002903
Iteration 94/1000 | Loss: 0.00002903
Iteration 95/1000 | Loss: 0.00002903
Iteration 96/1000 | Loss: 0.00002902
Iteration 97/1000 | Loss: 0.00002902
Iteration 98/1000 | Loss: 0.00002902
Iteration 99/1000 | Loss: 0.00002902
Iteration 100/1000 | Loss: 0.00002902
Iteration 101/1000 | Loss: 0.00002902
Iteration 102/1000 | Loss: 0.00002902
Iteration 103/1000 | Loss: 0.00002902
Iteration 104/1000 | Loss: 0.00002902
Iteration 105/1000 | Loss: 0.00002901
Iteration 106/1000 | Loss: 0.00002901
Iteration 107/1000 | Loss: 0.00002901
Iteration 108/1000 | Loss: 0.00002901
Iteration 109/1000 | Loss: 0.00002901
Iteration 110/1000 | Loss: 0.00002900
Iteration 111/1000 | Loss: 0.00002900
Iteration 112/1000 | Loss: 0.00002900
Iteration 113/1000 | Loss: 0.00002900
Iteration 114/1000 | Loss: 0.00002900
Iteration 115/1000 | Loss: 0.00002900
Iteration 116/1000 | Loss: 0.00002900
Iteration 117/1000 | Loss: 0.00002900
Iteration 118/1000 | Loss: 0.00002900
Iteration 119/1000 | Loss: 0.00002900
Iteration 120/1000 | Loss: 0.00002900
Iteration 121/1000 | Loss: 0.00002900
Iteration 122/1000 | Loss: 0.00002900
Iteration 123/1000 | Loss: 0.00002900
Iteration 124/1000 | Loss: 0.00002900
Iteration 125/1000 | Loss: 0.00002900
Iteration 126/1000 | Loss: 0.00002899
Iteration 127/1000 | Loss: 0.00002899
Iteration 128/1000 | Loss: 0.00002899
Iteration 129/1000 | Loss: 0.00002899
Iteration 130/1000 | Loss: 0.00002899
Iteration 131/1000 | Loss: 0.00002899
Iteration 132/1000 | Loss: 0.00002899
Iteration 133/1000 | Loss: 0.00002899
Iteration 134/1000 | Loss: 0.00002899
Iteration 135/1000 | Loss: 0.00002899
Iteration 136/1000 | Loss: 0.00002899
Iteration 137/1000 | Loss: 0.00002899
Iteration 138/1000 | Loss: 0.00002899
Iteration 139/1000 | Loss: 0.00002899
Iteration 140/1000 | Loss: 0.00002898
Iteration 141/1000 | Loss: 0.00002898
Iteration 142/1000 | Loss: 0.00002898
Iteration 143/1000 | Loss: 0.00002898
Iteration 144/1000 | Loss: 0.00002898
Iteration 145/1000 | Loss: 0.00002898
Iteration 146/1000 | Loss: 0.00002898
Iteration 147/1000 | Loss: 0.00002898
Iteration 148/1000 | Loss: 0.00002898
Iteration 149/1000 | Loss: 0.00002897
Iteration 150/1000 | Loss: 0.00002897
Iteration 151/1000 | Loss: 0.00002897
Iteration 152/1000 | Loss: 0.00002897
Iteration 153/1000 | Loss: 0.00002897
Iteration 154/1000 | Loss: 0.00002897
Iteration 155/1000 | Loss: 0.00002897
Iteration 156/1000 | Loss: 0.00002897
Iteration 157/1000 | Loss: 0.00002897
Iteration 158/1000 | Loss: 0.00002897
Iteration 159/1000 | Loss: 0.00002897
Iteration 160/1000 | Loss: 0.00002897
Iteration 161/1000 | Loss: 0.00002897
Iteration 162/1000 | Loss: 0.00002897
Iteration 163/1000 | Loss: 0.00002897
Iteration 164/1000 | Loss: 0.00002897
Iteration 165/1000 | Loss: 0.00002897
Iteration 166/1000 | Loss: 0.00002897
Iteration 167/1000 | Loss: 0.00002897
Iteration 168/1000 | Loss: 0.00002897
Iteration 169/1000 | Loss: 0.00002897
Iteration 170/1000 | Loss: 0.00002897
Iteration 171/1000 | Loss: 0.00002897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.896839396271389e-05, 2.896839396271389e-05, 2.896839396271389e-05, 2.896839396271389e-05, 2.896839396271389e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.896839396271389e-05

Optimization complete. Final v2v error: 4.577522277832031 mm

Highest mean error: 4.995445251464844 mm for frame 144

Lowest mean error: 4.1403985023498535 mm for frame 204

Saving results

Total time: 66.72607398033142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441583
Iteration 2/25 | Loss: 0.00124755
Iteration 3/25 | Loss: 0.00116910
Iteration 4/25 | Loss: 0.00116290
Iteration 5/25 | Loss: 0.00116062
Iteration 6/25 | Loss: 0.00116016
Iteration 7/25 | Loss: 0.00116016
Iteration 8/25 | Loss: 0.00116016
Iteration 9/25 | Loss: 0.00116016
Iteration 10/25 | Loss: 0.00116016
Iteration 11/25 | Loss: 0.00116016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011601572623476386, 0.0011601572623476386, 0.0011601572623476386, 0.0011601572623476386, 0.0011601572623476386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011601572623476386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37150681
Iteration 2/25 | Loss: 0.00096177
Iteration 3/25 | Loss: 0.00096176
Iteration 4/25 | Loss: 0.00096176
Iteration 5/25 | Loss: 0.00096176
Iteration 6/25 | Loss: 0.00096176
Iteration 7/25 | Loss: 0.00096176
Iteration 8/25 | Loss: 0.00096176
Iteration 9/25 | Loss: 0.00096176
Iteration 10/25 | Loss: 0.00096176
Iteration 11/25 | Loss: 0.00096176
Iteration 12/25 | Loss: 0.00096176
Iteration 13/25 | Loss: 0.00096176
Iteration 14/25 | Loss: 0.00096176
Iteration 15/25 | Loss: 0.00096176
Iteration 16/25 | Loss: 0.00096176
Iteration 17/25 | Loss: 0.00096176
Iteration 18/25 | Loss: 0.00096176
Iteration 19/25 | Loss: 0.00096176
Iteration 20/25 | Loss: 0.00096176
Iteration 21/25 | Loss: 0.00096176
Iteration 22/25 | Loss: 0.00096176
Iteration 23/25 | Loss: 0.00096176
Iteration 24/25 | Loss: 0.00096176
Iteration 25/25 | Loss: 0.00096176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096176
Iteration 2/1000 | Loss: 0.00002464
Iteration 3/1000 | Loss: 0.00001532
Iteration 4/1000 | Loss: 0.00001330
Iteration 5/1000 | Loss: 0.00001265
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001161
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001103
Iteration 11/1000 | Loss: 0.00001099
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001090
Iteration 14/1000 | Loss: 0.00001089
Iteration 15/1000 | Loss: 0.00001085
Iteration 16/1000 | Loss: 0.00001082
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001079
Iteration 22/1000 | Loss: 0.00001079
Iteration 23/1000 | Loss: 0.00001079
Iteration 24/1000 | Loss: 0.00001079
Iteration 25/1000 | Loss: 0.00001079
Iteration 26/1000 | Loss: 0.00001078
Iteration 27/1000 | Loss: 0.00001078
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001077
Iteration 30/1000 | Loss: 0.00001077
Iteration 31/1000 | Loss: 0.00001076
Iteration 32/1000 | Loss: 0.00001076
Iteration 33/1000 | Loss: 0.00001074
Iteration 34/1000 | Loss: 0.00001074
Iteration 35/1000 | Loss: 0.00001074
Iteration 36/1000 | Loss: 0.00001074
Iteration 37/1000 | Loss: 0.00001074
Iteration 38/1000 | Loss: 0.00001074
Iteration 39/1000 | Loss: 0.00001074
Iteration 40/1000 | Loss: 0.00001073
Iteration 41/1000 | Loss: 0.00001073
Iteration 42/1000 | Loss: 0.00001072
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001070
Iteration 46/1000 | Loss: 0.00001070
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001070
Iteration 50/1000 | Loss: 0.00001069
Iteration 51/1000 | Loss: 0.00001069
Iteration 52/1000 | Loss: 0.00001068
Iteration 53/1000 | Loss: 0.00001067
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001066
Iteration 56/1000 | Loss: 0.00001066
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001065
Iteration 60/1000 | Loss: 0.00001065
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001064
Iteration 63/1000 | Loss: 0.00001064
Iteration 64/1000 | Loss: 0.00001063
Iteration 65/1000 | Loss: 0.00001063
Iteration 66/1000 | Loss: 0.00001062
Iteration 67/1000 | Loss: 0.00001062
Iteration 68/1000 | Loss: 0.00001062
Iteration 69/1000 | Loss: 0.00001062
Iteration 70/1000 | Loss: 0.00001061
Iteration 71/1000 | Loss: 0.00001061
Iteration 72/1000 | Loss: 0.00001061
Iteration 73/1000 | Loss: 0.00001061
Iteration 74/1000 | Loss: 0.00001060
Iteration 75/1000 | Loss: 0.00001060
Iteration 76/1000 | Loss: 0.00001060
Iteration 77/1000 | Loss: 0.00001060
Iteration 78/1000 | Loss: 0.00001060
Iteration 79/1000 | Loss: 0.00001060
Iteration 80/1000 | Loss: 0.00001059
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001059
Iteration 83/1000 | Loss: 0.00001059
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001058
Iteration 86/1000 | Loss: 0.00001058
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001057
Iteration 89/1000 | Loss: 0.00001057
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001057
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001057
Iteration 94/1000 | Loss: 0.00001057
Iteration 95/1000 | Loss: 0.00001057
Iteration 96/1000 | Loss: 0.00001057
Iteration 97/1000 | Loss: 0.00001057
Iteration 98/1000 | Loss: 0.00001056
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001056
Iteration 102/1000 | Loss: 0.00001056
Iteration 103/1000 | Loss: 0.00001056
Iteration 104/1000 | Loss: 0.00001056
Iteration 105/1000 | Loss: 0.00001055
Iteration 106/1000 | Loss: 0.00001055
Iteration 107/1000 | Loss: 0.00001055
Iteration 108/1000 | Loss: 0.00001055
Iteration 109/1000 | Loss: 0.00001055
Iteration 110/1000 | Loss: 0.00001055
Iteration 111/1000 | Loss: 0.00001055
Iteration 112/1000 | Loss: 0.00001054
Iteration 113/1000 | Loss: 0.00001054
Iteration 114/1000 | Loss: 0.00001054
Iteration 115/1000 | Loss: 0.00001054
Iteration 116/1000 | Loss: 0.00001053
Iteration 117/1000 | Loss: 0.00001053
Iteration 118/1000 | Loss: 0.00001053
Iteration 119/1000 | Loss: 0.00001053
Iteration 120/1000 | Loss: 0.00001053
Iteration 121/1000 | Loss: 0.00001053
Iteration 122/1000 | Loss: 0.00001052
Iteration 123/1000 | Loss: 0.00001052
Iteration 124/1000 | Loss: 0.00001052
Iteration 125/1000 | Loss: 0.00001051
Iteration 126/1000 | Loss: 0.00001051
Iteration 127/1000 | Loss: 0.00001051
Iteration 128/1000 | Loss: 0.00001050
Iteration 129/1000 | Loss: 0.00001050
Iteration 130/1000 | Loss: 0.00001050
Iteration 131/1000 | Loss: 0.00001049
Iteration 132/1000 | Loss: 0.00001049
Iteration 133/1000 | Loss: 0.00001049
Iteration 134/1000 | Loss: 0.00001049
Iteration 135/1000 | Loss: 0.00001049
Iteration 136/1000 | Loss: 0.00001048
Iteration 137/1000 | Loss: 0.00001048
Iteration 138/1000 | Loss: 0.00001048
Iteration 139/1000 | Loss: 0.00001048
Iteration 140/1000 | Loss: 0.00001048
Iteration 141/1000 | Loss: 0.00001048
Iteration 142/1000 | Loss: 0.00001048
Iteration 143/1000 | Loss: 0.00001048
Iteration 144/1000 | Loss: 0.00001048
Iteration 145/1000 | Loss: 0.00001047
Iteration 146/1000 | Loss: 0.00001047
Iteration 147/1000 | Loss: 0.00001046
Iteration 148/1000 | Loss: 0.00001046
Iteration 149/1000 | Loss: 0.00001046
Iteration 150/1000 | Loss: 0.00001046
Iteration 151/1000 | Loss: 0.00001046
Iteration 152/1000 | Loss: 0.00001045
Iteration 153/1000 | Loss: 0.00001044
Iteration 154/1000 | Loss: 0.00001044
Iteration 155/1000 | Loss: 0.00001044
Iteration 156/1000 | Loss: 0.00001044
Iteration 157/1000 | Loss: 0.00001043
Iteration 158/1000 | Loss: 0.00001043
Iteration 159/1000 | Loss: 0.00001043
Iteration 160/1000 | Loss: 0.00001043
Iteration 161/1000 | Loss: 0.00001043
Iteration 162/1000 | Loss: 0.00001042
Iteration 163/1000 | Loss: 0.00001042
Iteration 164/1000 | Loss: 0.00001042
Iteration 165/1000 | Loss: 0.00001042
Iteration 166/1000 | Loss: 0.00001042
Iteration 167/1000 | Loss: 0.00001042
Iteration 168/1000 | Loss: 0.00001042
Iteration 169/1000 | Loss: 0.00001042
Iteration 170/1000 | Loss: 0.00001042
Iteration 171/1000 | Loss: 0.00001042
Iteration 172/1000 | Loss: 0.00001042
Iteration 173/1000 | Loss: 0.00001042
Iteration 174/1000 | Loss: 0.00001041
Iteration 175/1000 | Loss: 0.00001041
Iteration 176/1000 | Loss: 0.00001041
Iteration 177/1000 | Loss: 0.00001041
Iteration 178/1000 | Loss: 0.00001041
Iteration 179/1000 | Loss: 0.00001041
Iteration 180/1000 | Loss: 0.00001041
Iteration 181/1000 | Loss: 0.00001041
Iteration 182/1000 | Loss: 0.00001041
Iteration 183/1000 | Loss: 0.00001041
Iteration 184/1000 | Loss: 0.00001041
Iteration 185/1000 | Loss: 0.00001041
Iteration 186/1000 | Loss: 0.00001041
Iteration 187/1000 | Loss: 0.00001041
Iteration 188/1000 | Loss: 0.00001041
Iteration 189/1000 | Loss: 0.00001041
Iteration 190/1000 | Loss: 0.00001041
Iteration 191/1000 | Loss: 0.00001041
Iteration 192/1000 | Loss: 0.00001041
Iteration 193/1000 | Loss: 0.00001041
Iteration 194/1000 | Loss: 0.00001041
Iteration 195/1000 | Loss: 0.00001041
Iteration 196/1000 | Loss: 0.00001041
Iteration 197/1000 | Loss: 0.00001041
Iteration 198/1000 | Loss: 0.00001041
Iteration 199/1000 | Loss: 0.00001041
Iteration 200/1000 | Loss: 0.00001041
Iteration 201/1000 | Loss: 0.00001041
Iteration 202/1000 | Loss: 0.00001041
Iteration 203/1000 | Loss: 0.00001041
Iteration 204/1000 | Loss: 0.00001041
Iteration 205/1000 | Loss: 0.00001041
Iteration 206/1000 | Loss: 0.00001041
Iteration 207/1000 | Loss: 0.00001041
Iteration 208/1000 | Loss: 0.00001041
Iteration 209/1000 | Loss: 0.00001041
Iteration 210/1000 | Loss: 0.00001041
Iteration 211/1000 | Loss: 0.00001041
Iteration 212/1000 | Loss: 0.00001041
Iteration 213/1000 | Loss: 0.00001041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.0412972187623382e-05, 1.0412972187623382e-05, 1.0412972187623382e-05, 1.0412972187623382e-05, 1.0412972187623382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0412972187623382e-05

Optimization complete. Final v2v error: 2.727137565612793 mm

Highest mean error: 3.058565378189087 mm for frame 109

Lowest mean error: 2.5040204524993896 mm for frame 18

Saving results

Total time: 38.52954363822937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003906
Iteration 2/25 | Loss: 0.00370726
Iteration 3/25 | Loss: 0.00226623
Iteration 4/25 | Loss: 0.00175691
Iteration 5/25 | Loss: 0.00170808
Iteration 6/25 | Loss: 0.00168977
Iteration 7/25 | Loss: 0.00166328
Iteration 8/25 | Loss: 0.00164254
Iteration 9/25 | Loss: 0.00161336
Iteration 10/25 | Loss: 0.00160878
Iteration 11/25 | Loss: 0.00159901
Iteration 12/25 | Loss: 0.00158691
Iteration 13/25 | Loss: 0.00158933
Iteration 14/25 | Loss: 0.00158908
Iteration 15/25 | Loss: 0.00158149
Iteration 16/25 | Loss: 0.00158434
Iteration 17/25 | Loss: 0.00158125
Iteration 18/25 | Loss: 0.00157863
Iteration 19/25 | Loss: 0.00158258
Iteration 20/25 | Loss: 0.00157379
Iteration 21/25 | Loss: 0.00157764
Iteration 22/25 | Loss: 0.00156803
Iteration 23/25 | Loss: 0.00156131
Iteration 24/25 | Loss: 0.00155087
Iteration 25/25 | Loss: 0.00154802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.97793293
Iteration 2/25 | Loss: 0.00516340
Iteration 3/25 | Loss: 0.00514927
Iteration 4/25 | Loss: 0.00514926
Iteration 5/25 | Loss: 0.00514926
Iteration 6/25 | Loss: 0.00514926
Iteration 7/25 | Loss: 0.00514926
Iteration 8/25 | Loss: 0.00514926
Iteration 9/25 | Loss: 0.00514926
Iteration 10/25 | Loss: 0.00514926
Iteration 11/25 | Loss: 0.00514926
Iteration 12/25 | Loss: 0.00514926
Iteration 13/25 | Loss: 0.00514926
Iteration 14/25 | Loss: 0.00514926
Iteration 15/25 | Loss: 0.00514926
Iteration 16/25 | Loss: 0.00514926
Iteration 17/25 | Loss: 0.00514926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.005149263888597488, 0.005149263888597488, 0.005149263888597488, 0.005149263888597488, 0.005149263888597488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005149263888597488

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00514926
Iteration 2/1000 | Loss: 0.00188444
Iteration 3/1000 | Loss: 0.00147192
Iteration 4/1000 | Loss: 0.00105831
Iteration 5/1000 | Loss: 0.00254238
Iteration 6/1000 | Loss: 0.00145531
Iteration 7/1000 | Loss: 0.00257671
Iteration 8/1000 | Loss: 0.00098730
Iteration 9/1000 | Loss: 0.00020880
Iteration 10/1000 | Loss: 0.00132540
Iteration 11/1000 | Loss: 0.00066782
Iteration 12/1000 | Loss: 0.00092168
Iteration 13/1000 | Loss: 0.00087180
Iteration 14/1000 | Loss: 0.00034506
Iteration 15/1000 | Loss: 0.00119068
Iteration 16/1000 | Loss: 0.00023130
Iteration 17/1000 | Loss: 0.00008930
Iteration 18/1000 | Loss: 0.00007336
Iteration 19/1000 | Loss: 0.00086135
Iteration 20/1000 | Loss: 0.00016731
Iteration 21/1000 | Loss: 0.00007150
Iteration 22/1000 | Loss: 0.00013974
Iteration 23/1000 | Loss: 0.00030710
Iteration 24/1000 | Loss: 0.00006992
Iteration 25/1000 | Loss: 0.00005785
Iteration 26/1000 | Loss: 0.00005193
Iteration 27/1000 | Loss: 0.00004804
Iteration 28/1000 | Loss: 0.00004530
Iteration 29/1000 | Loss: 0.00004275
Iteration 30/1000 | Loss: 0.00004144
Iteration 31/1000 | Loss: 0.00004045
Iteration 32/1000 | Loss: 0.00019534
Iteration 33/1000 | Loss: 0.00004317
Iteration 34/1000 | Loss: 0.00003961
Iteration 35/1000 | Loss: 0.00003828
Iteration 36/1000 | Loss: 0.00023748
Iteration 37/1000 | Loss: 0.00041567
Iteration 38/1000 | Loss: 0.00008914
Iteration 39/1000 | Loss: 0.00004747
Iteration 40/1000 | Loss: 0.00003523
Iteration 41/1000 | Loss: 0.00003267
Iteration 42/1000 | Loss: 0.00003091
Iteration 43/1000 | Loss: 0.00002986
Iteration 44/1000 | Loss: 0.00024881
Iteration 45/1000 | Loss: 0.00021920
Iteration 46/1000 | Loss: 0.00019223
Iteration 47/1000 | Loss: 0.00020492
Iteration 48/1000 | Loss: 0.00020226
Iteration 49/1000 | Loss: 0.00019873
Iteration 50/1000 | Loss: 0.00017233
Iteration 51/1000 | Loss: 0.00020360
Iteration 52/1000 | Loss: 0.00021464
Iteration 53/1000 | Loss: 0.00022458
Iteration 54/1000 | Loss: 0.00021592
Iteration 55/1000 | Loss: 0.00035400
Iteration 56/1000 | Loss: 0.00016150
Iteration 57/1000 | Loss: 0.00042891
Iteration 58/1000 | Loss: 0.00012746
Iteration 59/1000 | Loss: 0.00002995
Iteration 60/1000 | Loss: 0.00002812
Iteration 61/1000 | Loss: 0.00024195
Iteration 62/1000 | Loss: 0.00003895
Iteration 63/1000 | Loss: 0.00002825
Iteration 64/1000 | Loss: 0.00023955
Iteration 65/1000 | Loss: 0.00068443
Iteration 66/1000 | Loss: 0.00013045
Iteration 67/1000 | Loss: 0.00012738
Iteration 68/1000 | Loss: 0.00010230
Iteration 69/1000 | Loss: 0.00011543
Iteration 70/1000 | Loss: 0.00003443
Iteration 71/1000 | Loss: 0.00014388
Iteration 72/1000 | Loss: 0.00015209
Iteration 73/1000 | Loss: 0.00021557
Iteration 74/1000 | Loss: 0.00014232
Iteration 75/1000 | Loss: 0.00025896
Iteration 76/1000 | Loss: 0.00004275
Iteration 77/1000 | Loss: 0.00003201
Iteration 78/1000 | Loss: 0.00002713
Iteration 79/1000 | Loss: 0.00002502
Iteration 80/1000 | Loss: 0.00002304
Iteration 81/1000 | Loss: 0.00002206
Iteration 82/1000 | Loss: 0.00002149
Iteration 83/1000 | Loss: 0.00002106
Iteration 84/1000 | Loss: 0.00002083
Iteration 85/1000 | Loss: 0.00002059
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002039
Iteration 88/1000 | Loss: 0.00002019
Iteration 89/1000 | Loss: 0.00002019
Iteration 90/1000 | Loss: 0.00002015
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002006
Iteration 93/1000 | Loss: 0.00002006
Iteration 94/1000 | Loss: 0.00002006
Iteration 95/1000 | Loss: 0.00002005
Iteration 96/1000 | Loss: 0.00002005
Iteration 97/1000 | Loss: 0.00002004
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002002
Iteration 101/1000 | Loss: 0.00002001
Iteration 102/1000 | Loss: 0.00002000
Iteration 103/1000 | Loss: 0.00002000
Iteration 104/1000 | Loss: 0.00001999
Iteration 105/1000 | Loss: 0.00001999
Iteration 106/1000 | Loss: 0.00001999
Iteration 107/1000 | Loss: 0.00001999
Iteration 108/1000 | Loss: 0.00001998
Iteration 109/1000 | Loss: 0.00001998
Iteration 110/1000 | Loss: 0.00001998
Iteration 111/1000 | Loss: 0.00001998
Iteration 112/1000 | Loss: 0.00001998
Iteration 113/1000 | Loss: 0.00001998
Iteration 114/1000 | Loss: 0.00001998
Iteration 115/1000 | Loss: 0.00001997
Iteration 116/1000 | Loss: 0.00034615
Iteration 117/1000 | Loss: 0.00002143
Iteration 118/1000 | Loss: 0.00002041
Iteration 119/1000 | Loss: 0.00001929
Iteration 120/1000 | Loss: 0.00001873
Iteration 121/1000 | Loss: 0.00001839
Iteration 122/1000 | Loss: 0.00001818
Iteration 123/1000 | Loss: 0.00001799
Iteration 124/1000 | Loss: 0.00001795
Iteration 125/1000 | Loss: 0.00001788
Iteration 126/1000 | Loss: 0.00001787
Iteration 127/1000 | Loss: 0.00001787
Iteration 128/1000 | Loss: 0.00001786
Iteration 129/1000 | Loss: 0.00001785
Iteration 130/1000 | Loss: 0.00001784
Iteration 131/1000 | Loss: 0.00001784
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001782
Iteration 135/1000 | Loss: 0.00001781
Iteration 136/1000 | Loss: 0.00001781
Iteration 137/1000 | Loss: 0.00001781
Iteration 138/1000 | Loss: 0.00001781
Iteration 139/1000 | Loss: 0.00001781
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001781
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001780
Iteration 148/1000 | Loss: 0.00001780
Iteration 149/1000 | Loss: 0.00001780
Iteration 150/1000 | Loss: 0.00001780
Iteration 151/1000 | Loss: 0.00001780
Iteration 152/1000 | Loss: 0.00001780
Iteration 153/1000 | Loss: 0.00001780
Iteration 154/1000 | Loss: 0.00001780
Iteration 155/1000 | Loss: 0.00001780
Iteration 156/1000 | Loss: 0.00001780
Iteration 157/1000 | Loss: 0.00001780
Iteration 158/1000 | Loss: 0.00001780
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001780
Iteration 161/1000 | Loss: 0.00001780
Iteration 162/1000 | Loss: 0.00001780
Iteration 163/1000 | Loss: 0.00001780
Iteration 164/1000 | Loss: 0.00001780
Iteration 165/1000 | Loss: 0.00001780
Iteration 166/1000 | Loss: 0.00001780
Iteration 167/1000 | Loss: 0.00001780
Iteration 168/1000 | Loss: 0.00001780
Iteration 169/1000 | Loss: 0.00001780
Iteration 170/1000 | Loss: 0.00001780
Iteration 171/1000 | Loss: 0.00001780
Iteration 172/1000 | Loss: 0.00001780
Iteration 173/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.7801294234232046e-05, 1.7801294234232046e-05, 1.7801294234232046e-05, 1.7801294234232046e-05, 1.7801294234232046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7801294234232046e-05

Optimization complete. Final v2v error: 3.37058424949646 mm

Highest mean error: 5.079439163208008 mm for frame 63

Lowest mean error: 2.688944101333618 mm for frame 140

Saving results

Total time: 190.24144172668457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752125
Iteration 2/25 | Loss: 0.00133974
Iteration 3/25 | Loss: 0.00122689
Iteration 4/25 | Loss: 0.00121179
Iteration 5/25 | Loss: 0.00120495
Iteration 6/25 | Loss: 0.00120398
Iteration 7/25 | Loss: 0.00120398
Iteration 8/25 | Loss: 0.00120398
Iteration 9/25 | Loss: 0.00120398
Iteration 10/25 | Loss: 0.00120398
Iteration 11/25 | Loss: 0.00120398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012039849534630775, 0.0012039849534630775, 0.0012039849534630775, 0.0012039849534630775, 0.0012039849534630775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012039849534630775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47696936
Iteration 2/25 | Loss: 0.00093914
Iteration 3/25 | Loss: 0.00093914
Iteration 4/25 | Loss: 0.00093914
Iteration 5/25 | Loss: 0.00093914
Iteration 6/25 | Loss: 0.00093914
Iteration 7/25 | Loss: 0.00093914
Iteration 8/25 | Loss: 0.00093914
Iteration 9/25 | Loss: 0.00093914
Iteration 10/25 | Loss: 0.00093914
Iteration 11/25 | Loss: 0.00093914
Iteration 12/25 | Loss: 0.00093914
Iteration 13/25 | Loss: 0.00093913
Iteration 14/25 | Loss: 0.00093913
Iteration 15/25 | Loss: 0.00093913
Iteration 16/25 | Loss: 0.00093913
Iteration 17/25 | Loss: 0.00093913
Iteration 18/25 | Loss: 0.00093913
Iteration 19/25 | Loss: 0.00093913
Iteration 20/25 | Loss: 0.00093913
Iteration 21/25 | Loss: 0.00093913
Iteration 22/25 | Loss: 0.00093913
Iteration 23/25 | Loss: 0.00093913
Iteration 24/25 | Loss: 0.00093913
Iteration 25/25 | Loss: 0.00093913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093913
Iteration 2/1000 | Loss: 0.00003576
Iteration 3/1000 | Loss: 0.00002323
Iteration 4/1000 | Loss: 0.00002041
Iteration 5/1000 | Loss: 0.00001921
Iteration 6/1000 | Loss: 0.00001842
Iteration 7/1000 | Loss: 0.00001799
Iteration 8/1000 | Loss: 0.00001771
Iteration 9/1000 | Loss: 0.00001737
Iteration 10/1000 | Loss: 0.00001716
Iteration 11/1000 | Loss: 0.00001700
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001681
Iteration 14/1000 | Loss: 0.00001671
Iteration 15/1000 | Loss: 0.00001670
Iteration 16/1000 | Loss: 0.00001667
Iteration 17/1000 | Loss: 0.00001661
Iteration 18/1000 | Loss: 0.00001661
Iteration 19/1000 | Loss: 0.00001660
Iteration 20/1000 | Loss: 0.00001660
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001636
Iteration 26/1000 | Loss: 0.00001632
Iteration 27/1000 | Loss: 0.00001623
Iteration 28/1000 | Loss: 0.00001622
Iteration 29/1000 | Loss: 0.00001620
Iteration 30/1000 | Loss: 0.00001620
Iteration 31/1000 | Loss: 0.00001619
Iteration 32/1000 | Loss: 0.00001619
Iteration 33/1000 | Loss: 0.00001614
Iteration 34/1000 | Loss: 0.00001614
Iteration 35/1000 | Loss: 0.00001613
Iteration 36/1000 | Loss: 0.00001613
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001613
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00001612
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001608
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001608
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001608
Iteration 53/1000 | Loss: 0.00001608
Iteration 54/1000 | Loss: 0.00001608
Iteration 55/1000 | Loss: 0.00001608
Iteration 56/1000 | Loss: 0.00001607
Iteration 57/1000 | Loss: 0.00001607
Iteration 58/1000 | Loss: 0.00001607
Iteration 59/1000 | Loss: 0.00001606
Iteration 60/1000 | Loss: 0.00001606
Iteration 61/1000 | Loss: 0.00001606
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001604
Iteration 64/1000 | Loss: 0.00001604
Iteration 65/1000 | Loss: 0.00001604
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001603
Iteration 68/1000 | Loss: 0.00001603
Iteration 69/1000 | Loss: 0.00001603
Iteration 70/1000 | Loss: 0.00001603
Iteration 71/1000 | Loss: 0.00001602
Iteration 72/1000 | Loss: 0.00001602
Iteration 73/1000 | Loss: 0.00001602
Iteration 74/1000 | Loss: 0.00001602
Iteration 75/1000 | Loss: 0.00001601
Iteration 76/1000 | Loss: 0.00001601
Iteration 77/1000 | Loss: 0.00001601
Iteration 78/1000 | Loss: 0.00001601
Iteration 79/1000 | Loss: 0.00001601
Iteration 80/1000 | Loss: 0.00001601
Iteration 81/1000 | Loss: 0.00001600
Iteration 82/1000 | Loss: 0.00001600
Iteration 83/1000 | Loss: 0.00001600
Iteration 84/1000 | Loss: 0.00001600
Iteration 85/1000 | Loss: 0.00001599
Iteration 86/1000 | Loss: 0.00001599
Iteration 87/1000 | Loss: 0.00001599
Iteration 88/1000 | Loss: 0.00001599
Iteration 89/1000 | Loss: 0.00001599
Iteration 90/1000 | Loss: 0.00001598
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001598
Iteration 93/1000 | Loss: 0.00001598
Iteration 94/1000 | Loss: 0.00001597
Iteration 95/1000 | Loss: 0.00001597
Iteration 96/1000 | Loss: 0.00001597
Iteration 97/1000 | Loss: 0.00001597
Iteration 98/1000 | Loss: 0.00001597
Iteration 99/1000 | Loss: 0.00001597
Iteration 100/1000 | Loss: 0.00001597
Iteration 101/1000 | Loss: 0.00001596
Iteration 102/1000 | Loss: 0.00001596
Iteration 103/1000 | Loss: 0.00001596
Iteration 104/1000 | Loss: 0.00001596
Iteration 105/1000 | Loss: 0.00001596
Iteration 106/1000 | Loss: 0.00001595
Iteration 107/1000 | Loss: 0.00001595
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001593
Iteration 117/1000 | Loss: 0.00001593
Iteration 118/1000 | Loss: 0.00001593
Iteration 119/1000 | Loss: 0.00001593
Iteration 120/1000 | Loss: 0.00001593
Iteration 121/1000 | Loss: 0.00001593
Iteration 122/1000 | Loss: 0.00001593
Iteration 123/1000 | Loss: 0.00001593
Iteration 124/1000 | Loss: 0.00001593
Iteration 125/1000 | Loss: 0.00001592
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001592
Iteration 129/1000 | Loss: 0.00001592
Iteration 130/1000 | Loss: 0.00001592
Iteration 131/1000 | Loss: 0.00001592
Iteration 132/1000 | Loss: 0.00001592
Iteration 133/1000 | Loss: 0.00001592
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001591
Iteration 138/1000 | Loss: 0.00001591
Iteration 139/1000 | Loss: 0.00001591
Iteration 140/1000 | Loss: 0.00001591
Iteration 141/1000 | Loss: 0.00001591
Iteration 142/1000 | Loss: 0.00001591
Iteration 143/1000 | Loss: 0.00001591
Iteration 144/1000 | Loss: 0.00001591
Iteration 145/1000 | Loss: 0.00001591
Iteration 146/1000 | Loss: 0.00001591
Iteration 147/1000 | Loss: 0.00001590
Iteration 148/1000 | Loss: 0.00001590
Iteration 149/1000 | Loss: 0.00001590
Iteration 150/1000 | Loss: 0.00001590
Iteration 151/1000 | Loss: 0.00001590
Iteration 152/1000 | Loss: 0.00001590
Iteration 153/1000 | Loss: 0.00001590
Iteration 154/1000 | Loss: 0.00001590
Iteration 155/1000 | Loss: 0.00001590
Iteration 156/1000 | Loss: 0.00001590
Iteration 157/1000 | Loss: 0.00001590
Iteration 158/1000 | Loss: 0.00001590
Iteration 159/1000 | Loss: 0.00001590
Iteration 160/1000 | Loss: 0.00001590
Iteration 161/1000 | Loss: 0.00001589
Iteration 162/1000 | Loss: 0.00001589
Iteration 163/1000 | Loss: 0.00001589
Iteration 164/1000 | Loss: 0.00001589
Iteration 165/1000 | Loss: 0.00001589
Iteration 166/1000 | Loss: 0.00001589
Iteration 167/1000 | Loss: 0.00001589
Iteration 168/1000 | Loss: 0.00001589
Iteration 169/1000 | Loss: 0.00001589
Iteration 170/1000 | Loss: 0.00001589
Iteration 171/1000 | Loss: 0.00001589
Iteration 172/1000 | Loss: 0.00001589
Iteration 173/1000 | Loss: 0.00001589
Iteration 174/1000 | Loss: 0.00001589
Iteration 175/1000 | Loss: 0.00001589
Iteration 176/1000 | Loss: 0.00001589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.588904342497699e-05, 1.588904342497699e-05, 1.588904342497699e-05, 1.588904342497699e-05, 1.588904342497699e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.588904342497699e-05

Optimization complete. Final v2v error: 3.388132333755493 mm

Highest mean error: 4.12821102142334 mm for frame 198

Lowest mean error: 2.9067835807800293 mm for frame 9

Saving results

Total time: 48.71926021575928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801865
Iteration 2/25 | Loss: 0.00134161
Iteration 3/25 | Loss: 0.00119881
Iteration 4/25 | Loss: 0.00118255
Iteration 5/25 | Loss: 0.00117735
Iteration 6/25 | Loss: 0.00117620
Iteration 7/25 | Loss: 0.00117620
Iteration 8/25 | Loss: 0.00117620
Iteration 9/25 | Loss: 0.00117620
Iteration 10/25 | Loss: 0.00117620
Iteration 11/25 | Loss: 0.00117620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011761992936953902, 0.0011761992936953902, 0.0011761992936953902, 0.0011761992936953902, 0.0011761992936953902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011761992936953902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24355030
Iteration 2/25 | Loss: 0.00074959
Iteration 3/25 | Loss: 0.00074957
Iteration 4/25 | Loss: 0.00074957
Iteration 5/25 | Loss: 0.00074956
Iteration 6/25 | Loss: 0.00074956
Iteration 7/25 | Loss: 0.00074956
Iteration 8/25 | Loss: 0.00074956
Iteration 9/25 | Loss: 0.00074956
Iteration 10/25 | Loss: 0.00074956
Iteration 11/25 | Loss: 0.00074956
Iteration 12/25 | Loss: 0.00074956
Iteration 13/25 | Loss: 0.00074956
Iteration 14/25 | Loss: 0.00074956
Iteration 15/25 | Loss: 0.00074956
Iteration 16/25 | Loss: 0.00074956
Iteration 17/25 | Loss: 0.00074956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007495632162317634, 0.0007495632162317634, 0.0007495632162317634, 0.0007495632162317634, 0.0007495632162317634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007495632162317634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074956
Iteration 2/1000 | Loss: 0.00003284
Iteration 3/1000 | Loss: 0.00002380
Iteration 4/1000 | Loss: 0.00002101
Iteration 5/1000 | Loss: 0.00001991
Iteration 6/1000 | Loss: 0.00001876
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001762
Iteration 9/1000 | Loss: 0.00001728
Iteration 10/1000 | Loss: 0.00001696
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001659
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001638
Iteration 16/1000 | Loss: 0.00001621
Iteration 17/1000 | Loss: 0.00001621
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001612
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001608
Iteration 28/1000 | Loss: 0.00001608
Iteration 29/1000 | Loss: 0.00001607
Iteration 30/1000 | Loss: 0.00001607
Iteration 31/1000 | Loss: 0.00001604
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001593
Iteration 41/1000 | Loss: 0.00001593
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001592
Iteration 45/1000 | Loss: 0.00001592
Iteration 46/1000 | Loss: 0.00001592
Iteration 47/1000 | Loss: 0.00001592
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001591
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001589
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001587
Iteration 62/1000 | Loss: 0.00001587
Iteration 63/1000 | Loss: 0.00001587
Iteration 64/1000 | Loss: 0.00001587
Iteration 65/1000 | Loss: 0.00001587
Iteration 66/1000 | Loss: 0.00001587
Iteration 67/1000 | Loss: 0.00001586
Iteration 68/1000 | Loss: 0.00001586
Iteration 69/1000 | Loss: 0.00001586
Iteration 70/1000 | Loss: 0.00001586
Iteration 71/1000 | Loss: 0.00001586
Iteration 72/1000 | Loss: 0.00001586
Iteration 73/1000 | Loss: 0.00001586
Iteration 74/1000 | Loss: 0.00001585
Iteration 75/1000 | Loss: 0.00001585
Iteration 76/1000 | Loss: 0.00001585
Iteration 77/1000 | Loss: 0.00001585
Iteration 78/1000 | Loss: 0.00001584
Iteration 79/1000 | Loss: 0.00001584
Iteration 80/1000 | Loss: 0.00001584
Iteration 81/1000 | Loss: 0.00001584
Iteration 82/1000 | Loss: 0.00001583
Iteration 83/1000 | Loss: 0.00001583
Iteration 84/1000 | Loss: 0.00001583
Iteration 85/1000 | Loss: 0.00001583
Iteration 86/1000 | Loss: 0.00001583
Iteration 87/1000 | Loss: 0.00001583
Iteration 88/1000 | Loss: 0.00001583
Iteration 89/1000 | Loss: 0.00001583
Iteration 90/1000 | Loss: 0.00001583
Iteration 91/1000 | Loss: 0.00001583
Iteration 92/1000 | Loss: 0.00001582
Iteration 93/1000 | Loss: 0.00001582
Iteration 94/1000 | Loss: 0.00001582
Iteration 95/1000 | Loss: 0.00001582
Iteration 96/1000 | Loss: 0.00001582
Iteration 97/1000 | Loss: 0.00001582
Iteration 98/1000 | Loss: 0.00001582
Iteration 99/1000 | Loss: 0.00001582
Iteration 100/1000 | Loss: 0.00001582
Iteration 101/1000 | Loss: 0.00001582
Iteration 102/1000 | Loss: 0.00001582
Iteration 103/1000 | Loss: 0.00001582
Iteration 104/1000 | Loss: 0.00001581
Iteration 105/1000 | Loss: 0.00001581
Iteration 106/1000 | Loss: 0.00001581
Iteration 107/1000 | Loss: 0.00001581
Iteration 108/1000 | Loss: 0.00001581
Iteration 109/1000 | Loss: 0.00001581
Iteration 110/1000 | Loss: 0.00001581
Iteration 111/1000 | Loss: 0.00001581
Iteration 112/1000 | Loss: 0.00001581
Iteration 113/1000 | Loss: 0.00001581
Iteration 114/1000 | Loss: 0.00001581
Iteration 115/1000 | Loss: 0.00001581
Iteration 116/1000 | Loss: 0.00001581
Iteration 117/1000 | Loss: 0.00001581
Iteration 118/1000 | Loss: 0.00001581
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001580
Iteration 123/1000 | Loss: 0.00001580
Iteration 124/1000 | Loss: 0.00001580
Iteration 125/1000 | Loss: 0.00001580
Iteration 126/1000 | Loss: 0.00001580
Iteration 127/1000 | Loss: 0.00001580
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001580
Iteration 134/1000 | Loss: 0.00001580
Iteration 135/1000 | Loss: 0.00001580
Iteration 136/1000 | Loss: 0.00001580
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001579
Iteration 147/1000 | Loss: 0.00001579
Iteration 148/1000 | Loss: 0.00001579
Iteration 149/1000 | Loss: 0.00001579
Iteration 150/1000 | Loss: 0.00001579
Iteration 151/1000 | Loss: 0.00001578
Iteration 152/1000 | Loss: 0.00001578
Iteration 153/1000 | Loss: 0.00001578
Iteration 154/1000 | Loss: 0.00001578
Iteration 155/1000 | Loss: 0.00001578
Iteration 156/1000 | Loss: 0.00001578
Iteration 157/1000 | Loss: 0.00001578
Iteration 158/1000 | Loss: 0.00001578
Iteration 159/1000 | Loss: 0.00001578
Iteration 160/1000 | Loss: 0.00001578
Iteration 161/1000 | Loss: 0.00001577
Iteration 162/1000 | Loss: 0.00001577
Iteration 163/1000 | Loss: 0.00001577
Iteration 164/1000 | Loss: 0.00001577
Iteration 165/1000 | Loss: 0.00001577
Iteration 166/1000 | Loss: 0.00001577
Iteration 167/1000 | Loss: 0.00001577
Iteration 168/1000 | Loss: 0.00001577
Iteration 169/1000 | Loss: 0.00001577
Iteration 170/1000 | Loss: 0.00001577
Iteration 171/1000 | Loss: 0.00001577
Iteration 172/1000 | Loss: 0.00001577
Iteration 173/1000 | Loss: 0.00001577
Iteration 174/1000 | Loss: 0.00001577
Iteration 175/1000 | Loss: 0.00001577
Iteration 176/1000 | Loss: 0.00001577
Iteration 177/1000 | Loss: 0.00001577
Iteration 178/1000 | Loss: 0.00001577
Iteration 179/1000 | Loss: 0.00001577
Iteration 180/1000 | Loss: 0.00001577
Iteration 181/1000 | Loss: 0.00001577
Iteration 182/1000 | Loss: 0.00001577
Iteration 183/1000 | Loss: 0.00001577
Iteration 184/1000 | Loss: 0.00001577
Iteration 185/1000 | Loss: 0.00001577
Iteration 186/1000 | Loss: 0.00001577
Iteration 187/1000 | Loss: 0.00001577
Iteration 188/1000 | Loss: 0.00001577
Iteration 189/1000 | Loss: 0.00001577
Iteration 190/1000 | Loss: 0.00001577
Iteration 191/1000 | Loss: 0.00001577
Iteration 192/1000 | Loss: 0.00001577
Iteration 193/1000 | Loss: 0.00001577
Iteration 194/1000 | Loss: 0.00001577
Iteration 195/1000 | Loss: 0.00001577
Iteration 196/1000 | Loss: 0.00001577
Iteration 197/1000 | Loss: 0.00001577
Iteration 198/1000 | Loss: 0.00001577
Iteration 199/1000 | Loss: 0.00001577
Iteration 200/1000 | Loss: 0.00001577
Iteration 201/1000 | Loss: 0.00001577
Iteration 202/1000 | Loss: 0.00001577
Iteration 203/1000 | Loss: 0.00001577
Iteration 204/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.5769643141538836e-05, 1.5769643141538836e-05, 1.5769643141538836e-05, 1.5769643141538836e-05, 1.5769643141538836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5769643141538836e-05

Optimization complete. Final v2v error: 3.296380043029785 mm

Highest mean error: 4.079370021820068 mm for frame 60

Lowest mean error: 2.9582760334014893 mm for frame 198

Saving results

Total time: 48.046950578689575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385973
Iteration 2/25 | Loss: 0.00120919
Iteration 3/25 | Loss: 0.00113698
Iteration 4/25 | Loss: 0.00112655
Iteration 5/25 | Loss: 0.00112283
Iteration 6/25 | Loss: 0.00112187
Iteration 7/25 | Loss: 0.00112185
Iteration 8/25 | Loss: 0.00112185
Iteration 9/25 | Loss: 0.00112185
Iteration 10/25 | Loss: 0.00112185
Iteration 11/25 | Loss: 0.00112185
Iteration 12/25 | Loss: 0.00112185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001121854642406106, 0.001121854642406106, 0.001121854642406106, 0.001121854642406106, 0.001121854642406106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001121854642406106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86796021
Iteration 2/25 | Loss: 0.00086650
Iteration 3/25 | Loss: 0.00086650
Iteration 4/25 | Loss: 0.00086650
Iteration 5/25 | Loss: 0.00086650
Iteration 6/25 | Loss: 0.00086650
Iteration 7/25 | Loss: 0.00086650
Iteration 8/25 | Loss: 0.00086650
Iteration 9/25 | Loss: 0.00086650
Iteration 10/25 | Loss: 0.00086650
Iteration 11/25 | Loss: 0.00086650
Iteration 12/25 | Loss: 0.00086650
Iteration 13/25 | Loss: 0.00086650
Iteration 14/25 | Loss: 0.00086650
Iteration 15/25 | Loss: 0.00086650
Iteration 16/25 | Loss: 0.00086650
Iteration 17/25 | Loss: 0.00086650
Iteration 18/25 | Loss: 0.00086650
Iteration 19/25 | Loss: 0.00086650
Iteration 20/25 | Loss: 0.00086650
Iteration 21/25 | Loss: 0.00086650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008665003697387874, 0.0008665003697387874, 0.0008665003697387874, 0.0008665003697387874, 0.0008665003697387874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008665003697387874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086650
Iteration 2/1000 | Loss: 0.00001914
Iteration 3/1000 | Loss: 0.00001350
Iteration 4/1000 | Loss: 0.00001243
Iteration 5/1000 | Loss: 0.00001166
Iteration 6/1000 | Loss: 0.00001129
Iteration 7/1000 | Loss: 0.00001086
Iteration 8/1000 | Loss: 0.00001070
Iteration 9/1000 | Loss: 0.00001049
Iteration 10/1000 | Loss: 0.00001041
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001024
Iteration 16/1000 | Loss: 0.00001019
Iteration 17/1000 | Loss: 0.00001012
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001006
Iteration 20/1000 | Loss: 0.00001006
Iteration 21/1000 | Loss: 0.00001006
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001005
Iteration 26/1000 | Loss: 0.00001005
Iteration 27/1000 | Loss: 0.00001003
Iteration 28/1000 | Loss: 0.00001003
Iteration 29/1000 | Loss: 0.00001002
Iteration 30/1000 | Loss: 0.00001001
Iteration 31/1000 | Loss: 0.00001001
Iteration 32/1000 | Loss: 0.00001001
Iteration 33/1000 | Loss: 0.00001001
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00001000
Iteration 36/1000 | Loss: 0.00001000
Iteration 37/1000 | Loss: 0.00001000
Iteration 38/1000 | Loss: 0.00001000
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000997
Iteration 42/1000 | Loss: 0.00000997
Iteration 43/1000 | Loss: 0.00000996
Iteration 44/1000 | Loss: 0.00000996
Iteration 45/1000 | Loss: 0.00000995
Iteration 46/1000 | Loss: 0.00000995
Iteration 47/1000 | Loss: 0.00000995
Iteration 48/1000 | Loss: 0.00000995
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000994
Iteration 51/1000 | Loss: 0.00000994
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000992
Iteration 54/1000 | Loss: 0.00000992
Iteration 55/1000 | Loss: 0.00000991
Iteration 56/1000 | Loss: 0.00000991
Iteration 57/1000 | Loss: 0.00000991
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000991
Iteration 63/1000 | Loss: 0.00000991
Iteration 64/1000 | Loss: 0.00000991
Iteration 65/1000 | Loss: 0.00000991
Iteration 66/1000 | Loss: 0.00000990
Iteration 67/1000 | Loss: 0.00000990
Iteration 68/1000 | Loss: 0.00000989
Iteration 69/1000 | Loss: 0.00000989
Iteration 70/1000 | Loss: 0.00000988
Iteration 71/1000 | Loss: 0.00000988
Iteration 72/1000 | Loss: 0.00000988
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000987
Iteration 75/1000 | Loss: 0.00000987
Iteration 76/1000 | Loss: 0.00000987
Iteration 77/1000 | Loss: 0.00000987
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000986
Iteration 81/1000 | Loss: 0.00000986
Iteration 82/1000 | Loss: 0.00000986
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000986
Iteration 86/1000 | Loss: 0.00000986
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000985
Iteration 89/1000 | Loss: 0.00000985
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000984
Iteration 94/1000 | Loss: 0.00000984
Iteration 95/1000 | Loss: 0.00000984
Iteration 96/1000 | Loss: 0.00000984
Iteration 97/1000 | Loss: 0.00000984
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000982
Iteration 100/1000 | Loss: 0.00000982
Iteration 101/1000 | Loss: 0.00000982
Iteration 102/1000 | Loss: 0.00000981
Iteration 103/1000 | Loss: 0.00000981
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000978
Iteration 110/1000 | Loss: 0.00000978
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000976
Iteration 115/1000 | Loss: 0.00000976
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000975
Iteration 121/1000 | Loss: 0.00000975
Iteration 122/1000 | Loss: 0.00000975
Iteration 123/1000 | Loss: 0.00000975
Iteration 124/1000 | Loss: 0.00000974
Iteration 125/1000 | Loss: 0.00000974
Iteration 126/1000 | Loss: 0.00000974
Iteration 127/1000 | Loss: 0.00000973
Iteration 128/1000 | Loss: 0.00000973
Iteration 129/1000 | Loss: 0.00000973
Iteration 130/1000 | Loss: 0.00000973
Iteration 131/1000 | Loss: 0.00000973
Iteration 132/1000 | Loss: 0.00000973
Iteration 133/1000 | Loss: 0.00000972
Iteration 134/1000 | Loss: 0.00000972
Iteration 135/1000 | Loss: 0.00000972
Iteration 136/1000 | Loss: 0.00000972
Iteration 137/1000 | Loss: 0.00000972
Iteration 138/1000 | Loss: 0.00000972
Iteration 139/1000 | Loss: 0.00000971
Iteration 140/1000 | Loss: 0.00000971
Iteration 141/1000 | Loss: 0.00000971
Iteration 142/1000 | Loss: 0.00000970
Iteration 143/1000 | Loss: 0.00000970
Iteration 144/1000 | Loss: 0.00000970
Iteration 145/1000 | Loss: 0.00000970
Iteration 146/1000 | Loss: 0.00000970
Iteration 147/1000 | Loss: 0.00000970
Iteration 148/1000 | Loss: 0.00000970
Iteration 149/1000 | Loss: 0.00000970
Iteration 150/1000 | Loss: 0.00000970
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000969
Iteration 156/1000 | Loss: 0.00000969
Iteration 157/1000 | Loss: 0.00000969
Iteration 158/1000 | Loss: 0.00000969
Iteration 159/1000 | Loss: 0.00000969
Iteration 160/1000 | Loss: 0.00000968
Iteration 161/1000 | Loss: 0.00000968
Iteration 162/1000 | Loss: 0.00000968
Iteration 163/1000 | Loss: 0.00000968
Iteration 164/1000 | Loss: 0.00000968
Iteration 165/1000 | Loss: 0.00000968
Iteration 166/1000 | Loss: 0.00000968
Iteration 167/1000 | Loss: 0.00000968
Iteration 168/1000 | Loss: 0.00000968
Iteration 169/1000 | Loss: 0.00000968
Iteration 170/1000 | Loss: 0.00000968
Iteration 171/1000 | Loss: 0.00000967
Iteration 172/1000 | Loss: 0.00000967
Iteration 173/1000 | Loss: 0.00000967
Iteration 174/1000 | Loss: 0.00000967
Iteration 175/1000 | Loss: 0.00000967
Iteration 176/1000 | Loss: 0.00000967
Iteration 177/1000 | Loss: 0.00000967
Iteration 178/1000 | Loss: 0.00000967
Iteration 179/1000 | Loss: 0.00000967
Iteration 180/1000 | Loss: 0.00000967
Iteration 181/1000 | Loss: 0.00000967
Iteration 182/1000 | Loss: 0.00000966
Iteration 183/1000 | Loss: 0.00000966
Iteration 184/1000 | Loss: 0.00000966
Iteration 185/1000 | Loss: 0.00000966
Iteration 186/1000 | Loss: 0.00000966
Iteration 187/1000 | Loss: 0.00000966
Iteration 188/1000 | Loss: 0.00000966
Iteration 189/1000 | Loss: 0.00000966
Iteration 190/1000 | Loss: 0.00000966
Iteration 191/1000 | Loss: 0.00000966
Iteration 192/1000 | Loss: 0.00000966
Iteration 193/1000 | Loss: 0.00000966
Iteration 194/1000 | Loss: 0.00000966
Iteration 195/1000 | Loss: 0.00000966
Iteration 196/1000 | Loss: 0.00000966
Iteration 197/1000 | Loss: 0.00000966
Iteration 198/1000 | Loss: 0.00000966
Iteration 199/1000 | Loss: 0.00000966
Iteration 200/1000 | Loss: 0.00000966
Iteration 201/1000 | Loss: 0.00000966
Iteration 202/1000 | Loss: 0.00000966
Iteration 203/1000 | Loss: 0.00000966
Iteration 204/1000 | Loss: 0.00000966
Iteration 205/1000 | Loss: 0.00000966
Iteration 206/1000 | Loss: 0.00000965
Iteration 207/1000 | Loss: 0.00000965
Iteration 208/1000 | Loss: 0.00000965
Iteration 209/1000 | Loss: 0.00000965
Iteration 210/1000 | Loss: 0.00000965
Iteration 211/1000 | Loss: 0.00000965
Iteration 212/1000 | Loss: 0.00000965
Iteration 213/1000 | Loss: 0.00000965
Iteration 214/1000 | Loss: 0.00000965
Iteration 215/1000 | Loss: 0.00000965
Iteration 216/1000 | Loss: 0.00000965
Iteration 217/1000 | Loss: 0.00000965
Iteration 218/1000 | Loss: 0.00000965
Iteration 219/1000 | Loss: 0.00000965
Iteration 220/1000 | Loss: 0.00000965
Iteration 221/1000 | Loss: 0.00000965
Iteration 222/1000 | Loss: 0.00000965
Iteration 223/1000 | Loss: 0.00000965
Iteration 224/1000 | Loss: 0.00000965
Iteration 225/1000 | Loss: 0.00000964
Iteration 226/1000 | Loss: 0.00000964
Iteration 227/1000 | Loss: 0.00000964
Iteration 228/1000 | Loss: 0.00000964
Iteration 229/1000 | Loss: 0.00000964
Iteration 230/1000 | Loss: 0.00000964
Iteration 231/1000 | Loss: 0.00000964
Iteration 232/1000 | Loss: 0.00000964
Iteration 233/1000 | Loss: 0.00000964
Iteration 234/1000 | Loss: 0.00000964
Iteration 235/1000 | Loss: 0.00000964
Iteration 236/1000 | Loss: 0.00000964
Iteration 237/1000 | Loss: 0.00000964
Iteration 238/1000 | Loss: 0.00000964
Iteration 239/1000 | Loss: 0.00000964
Iteration 240/1000 | Loss: 0.00000964
Iteration 241/1000 | Loss: 0.00000964
Iteration 242/1000 | Loss: 0.00000964
Iteration 243/1000 | Loss: 0.00000964
Iteration 244/1000 | Loss: 0.00000964
Iteration 245/1000 | Loss: 0.00000964
Iteration 246/1000 | Loss: 0.00000964
Iteration 247/1000 | Loss: 0.00000964
Iteration 248/1000 | Loss: 0.00000964
Iteration 249/1000 | Loss: 0.00000963
Iteration 250/1000 | Loss: 0.00000963
Iteration 251/1000 | Loss: 0.00000963
Iteration 252/1000 | Loss: 0.00000963
Iteration 253/1000 | Loss: 0.00000963
Iteration 254/1000 | Loss: 0.00000963
Iteration 255/1000 | Loss: 0.00000963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [9.63498587225331e-06, 9.63498587225331e-06, 9.63498587225331e-06, 9.63498587225331e-06, 9.63498587225331e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.63498587225331e-06

Optimization complete. Final v2v error: 2.6688973903656006 mm

Highest mean error: 3.0944857597351074 mm for frame 55

Lowest mean error: 2.5781090259552 mm for frame 3

Saving results

Total time: 41.14623165130615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032758
Iteration 2/25 | Loss: 0.01032758
Iteration 3/25 | Loss: 0.01032758
Iteration 4/25 | Loss: 0.01032758
Iteration 5/25 | Loss: 0.01032758
Iteration 6/25 | Loss: 0.01032758
Iteration 7/25 | Loss: 0.01032758
Iteration 8/25 | Loss: 0.01032758
Iteration 9/25 | Loss: 0.01032758
Iteration 10/25 | Loss: 0.01032758
Iteration 11/25 | Loss: 0.01032758
Iteration 12/25 | Loss: 0.01032758
Iteration 13/25 | Loss: 0.01032758
Iteration 14/25 | Loss: 0.01032757
Iteration 15/25 | Loss: 0.01032757
Iteration 16/25 | Loss: 0.01032757
Iteration 17/25 | Loss: 0.01032757
Iteration 18/25 | Loss: 0.01032757
Iteration 19/25 | Loss: 0.01032757
Iteration 20/25 | Loss: 0.01032757
Iteration 21/25 | Loss: 0.01032757
Iteration 22/25 | Loss: 0.01032757
Iteration 23/25 | Loss: 0.01032757
Iteration 24/25 | Loss: 0.01032757
Iteration 25/25 | Loss: 0.01032757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95355284
Iteration 2/25 | Loss: 0.05593380
Iteration 3/25 | Loss: 0.05592281
Iteration 4/25 | Loss: 0.05592280
Iteration 5/25 | Loss: 0.05592279
Iteration 6/25 | Loss: 0.05592278
Iteration 7/25 | Loss: 0.05592278
Iteration 8/25 | Loss: 0.05592278
Iteration 9/25 | Loss: 0.05592278
Iteration 10/25 | Loss: 0.05592278
Iteration 11/25 | Loss: 0.05592278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.05592277646064758, 0.05592277646064758, 0.05592277646064758, 0.05592277646064758, 0.05592277646064758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05592277646064758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05592278
Iteration 2/1000 | Loss: 0.00437117
Iteration 3/1000 | Loss: 0.00183695
Iteration 4/1000 | Loss: 0.00040674
Iteration 5/1000 | Loss: 0.00283853
Iteration 6/1000 | Loss: 0.00917456
Iteration 7/1000 | Loss: 0.00149704
Iteration 8/1000 | Loss: 0.00217484
Iteration 9/1000 | Loss: 0.00307786
Iteration 10/1000 | Loss: 0.00075074
Iteration 11/1000 | Loss: 0.00017435
Iteration 12/1000 | Loss: 0.00021576
Iteration 13/1000 | Loss: 0.00092552
Iteration 14/1000 | Loss: 0.00026941
Iteration 15/1000 | Loss: 0.00013461
Iteration 16/1000 | Loss: 0.00013138
Iteration 17/1000 | Loss: 0.00017331
Iteration 18/1000 | Loss: 0.00016560
Iteration 19/1000 | Loss: 0.00023691
Iteration 20/1000 | Loss: 0.00012714
Iteration 21/1000 | Loss: 0.00005600
Iteration 22/1000 | Loss: 0.00072654
Iteration 23/1000 | Loss: 0.00024246
Iteration 24/1000 | Loss: 0.00004754
Iteration 25/1000 | Loss: 0.00009270
Iteration 26/1000 | Loss: 0.00005923
Iteration 27/1000 | Loss: 0.00014943
Iteration 28/1000 | Loss: 0.00014421
Iteration 29/1000 | Loss: 0.00012564
Iteration 30/1000 | Loss: 0.00010442
Iteration 31/1000 | Loss: 0.00083316
Iteration 32/1000 | Loss: 0.00047424
Iteration 33/1000 | Loss: 0.00098354
Iteration 34/1000 | Loss: 0.00052990
Iteration 35/1000 | Loss: 0.00015746
Iteration 36/1000 | Loss: 0.00014715
Iteration 37/1000 | Loss: 0.00490844
Iteration 38/1000 | Loss: 0.00123390
Iteration 39/1000 | Loss: 0.00096510
Iteration 40/1000 | Loss: 0.00064582
Iteration 41/1000 | Loss: 0.00019289
Iteration 42/1000 | Loss: 0.00008136
Iteration 43/1000 | Loss: 0.00004392
Iteration 44/1000 | Loss: 0.00007588
Iteration 45/1000 | Loss: 0.00008437
Iteration 46/1000 | Loss: 0.00007902
Iteration 47/1000 | Loss: 0.00002473
Iteration 48/1000 | Loss: 0.00003047
Iteration 49/1000 | Loss: 0.00003700
Iteration 50/1000 | Loss: 0.00003549
Iteration 51/1000 | Loss: 0.00007948
Iteration 52/1000 | Loss: 0.00007045
Iteration 53/1000 | Loss: 0.00002076
Iteration 54/1000 | Loss: 0.00002149
Iteration 55/1000 | Loss: 0.00004431
Iteration 56/1000 | Loss: 0.00035617
Iteration 57/1000 | Loss: 0.00025927
Iteration 58/1000 | Loss: 0.00006081
Iteration 59/1000 | Loss: 0.00006117
Iteration 60/1000 | Loss: 0.00014708
Iteration 61/1000 | Loss: 0.00013573
Iteration 62/1000 | Loss: 0.00002683
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002701
Iteration 65/1000 | Loss: 0.00004775
Iteration 66/1000 | Loss: 0.00003133
Iteration 67/1000 | Loss: 0.00003877
Iteration 68/1000 | Loss: 0.00009043
Iteration 69/1000 | Loss: 0.00026926
Iteration 70/1000 | Loss: 0.00004518
Iteration 71/1000 | Loss: 0.00001794
Iteration 72/1000 | Loss: 0.00008263
Iteration 73/1000 | Loss: 0.00011385
Iteration 74/1000 | Loss: 0.00002197
Iteration 75/1000 | Loss: 0.00013988
Iteration 76/1000 | Loss: 0.00015283
Iteration 77/1000 | Loss: 0.00005780
Iteration 78/1000 | Loss: 0.00003525
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00003892
Iteration 81/1000 | Loss: 0.00003025
Iteration 82/1000 | Loss: 0.00001749
Iteration 83/1000 | Loss: 0.00002603
Iteration 84/1000 | Loss: 0.00006448
Iteration 85/1000 | Loss: 0.00007950
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00007787
Iteration 88/1000 | Loss: 0.00015187
Iteration 89/1000 | Loss: 0.00015775
Iteration 90/1000 | Loss: 0.00037047
Iteration 91/1000 | Loss: 0.00018008
Iteration 92/1000 | Loss: 0.00031450
Iteration 93/1000 | Loss: 0.00005412
Iteration 94/1000 | Loss: 0.00036284
Iteration 95/1000 | Loss: 0.00005563
Iteration 96/1000 | Loss: 0.00007111
Iteration 97/1000 | Loss: 0.00009581
Iteration 98/1000 | Loss: 0.00001589
Iteration 99/1000 | Loss: 0.00001582
Iteration 100/1000 | Loss: 0.00001580
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00001578
Iteration 103/1000 | Loss: 0.00001578
Iteration 104/1000 | Loss: 0.00001578
Iteration 105/1000 | Loss: 0.00001578
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00001577
Iteration 108/1000 | Loss: 0.00003608
Iteration 109/1000 | Loss: 0.00001633
Iteration 110/1000 | Loss: 0.00001576
Iteration 111/1000 | Loss: 0.00001575
Iteration 112/1000 | Loss: 0.00001575
Iteration 113/1000 | Loss: 0.00003603
Iteration 114/1000 | Loss: 0.00002694
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001965
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001565
Iteration 119/1000 | Loss: 0.00001565
Iteration 120/1000 | Loss: 0.00001565
Iteration 121/1000 | Loss: 0.00001565
Iteration 122/1000 | Loss: 0.00001565
Iteration 123/1000 | Loss: 0.00001564
Iteration 124/1000 | Loss: 0.00001564
Iteration 125/1000 | Loss: 0.00001564
Iteration 126/1000 | Loss: 0.00001564
Iteration 127/1000 | Loss: 0.00001564
Iteration 128/1000 | Loss: 0.00001564
Iteration 129/1000 | Loss: 0.00001564
Iteration 130/1000 | Loss: 0.00001564
Iteration 131/1000 | Loss: 0.00001564
Iteration 132/1000 | Loss: 0.00001564
Iteration 133/1000 | Loss: 0.00001564
Iteration 134/1000 | Loss: 0.00001564
Iteration 135/1000 | Loss: 0.00001563
Iteration 136/1000 | Loss: 0.00001563
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001563
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001563
Iteration 143/1000 | Loss: 0.00001563
Iteration 144/1000 | Loss: 0.00001562
Iteration 145/1000 | Loss: 0.00001562
Iteration 146/1000 | Loss: 0.00001562
Iteration 147/1000 | Loss: 0.00001562
Iteration 148/1000 | Loss: 0.00001562
Iteration 149/1000 | Loss: 0.00001562
Iteration 150/1000 | Loss: 0.00001562
Iteration 151/1000 | Loss: 0.00001562
Iteration 152/1000 | Loss: 0.00001562
Iteration 153/1000 | Loss: 0.00001561
Iteration 154/1000 | Loss: 0.00001561
Iteration 155/1000 | Loss: 0.00001561
Iteration 156/1000 | Loss: 0.00001561
Iteration 157/1000 | Loss: 0.00001561
Iteration 158/1000 | Loss: 0.00001561
Iteration 159/1000 | Loss: 0.00001561
Iteration 160/1000 | Loss: 0.00001561
Iteration 161/1000 | Loss: 0.00001561
Iteration 162/1000 | Loss: 0.00001561
Iteration 163/1000 | Loss: 0.00001561
Iteration 164/1000 | Loss: 0.00001561
Iteration 165/1000 | Loss: 0.00001560
Iteration 166/1000 | Loss: 0.00001560
Iteration 167/1000 | Loss: 0.00001560
Iteration 168/1000 | Loss: 0.00001560
Iteration 169/1000 | Loss: 0.00001560
Iteration 170/1000 | Loss: 0.00001560
Iteration 171/1000 | Loss: 0.00001560
Iteration 172/1000 | Loss: 0.00001560
Iteration 173/1000 | Loss: 0.00005037
Iteration 174/1000 | Loss: 0.00008669
Iteration 175/1000 | Loss: 0.00003010
Iteration 176/1000 | Loss: 0.00001565
Iteration 177/1000 | Loss: 0.00003235
Iteration 178/1000 | Loss: 0.00001561
Iteration 179/1000 | Loss: 0.00001561
Iteration 180/1000 | Loss: 0.00001560
Iteration 181/1000 | Loss: 0.00002463
Iteration 182/1000 | Loss: 0.00001722
Iteration 183/1000 | Loss: 0.00001558
Iteration 184/1000 | Loss: 0.00001558
Iteration 185/1000 | Loss: 0.00001558
Iteration 186/1000 | Loss: 0.00001558
Iteration 187/1000 | Loss: 0.00001558
Iteration 188/1000 | Loss: 0.00001558
Iteration 189/1000 | Loss: 0.00001558
Iteration 190/1000 | Loss: 0.00001558
Iteration 191/1000 | Loss: 0.00001558
Iteration 192/1000 | Loss: 0.00001558
Iteration 193/1000 | Loss: 0.00001558
Iteration 194/1000 | Loss: 0.00001823
Iteration 195/1000 | Loss: 0.00004401
Iteration 196/1000 | Loss: 0.00002261
Iteration 197/1000 | Loss: 0.00001560
Iteration 198/1000 | Loss: 0.00001978
Iteration 199/1000 | Loss: 0.00001559
Iteration 200/1000 | Loss: 0.00001892
Iteration 201/1000 | Loss: 0.00001558
Iteration 202/1000 | Loss: 0.00001557
Iteration 203/1000 | Loss: 0.00001557
Iteration 204/1000 | Loss: 0.00001557
Iteration 205/1000 | Loss: 0.00001557
Iteration 206/1000 | Loss: 0.00001557
Iteration 207/1000 | Loss: 0.00001557
Iteration 208/1000 | Loss: 0.00001557
Iteration 209/1000 | Loss: 0.00001557
Iteration 210/1000 | Loss: 0.00001557
Iteration 211/1000 | Loss: 0.00001557
Iteration 212/1000 | Loss: 0.00001557
Iteration 213/1000 | Loss: 0.00001557
Iteration 214/1000 | Loss: 0.00001557
Iteration 215/1000 | Loss: 0.00001556
Iteration 216/1000 | Loss: 0.00001886
Iteration 217/1000 | Loss: 0.00001556
Iteration 218/1000 | Loss: 0.00001556
Iteration 219/1000 | Loss: 0.00001556
Iteration 220/1000 | Loss: 0.00001555
Iteration 221/1000 | Loss: 0.00001555
Iteration 222/1000 | Loss: 0.00001555
Iteration 223/1000 | Loss: 0.00001555
Iteration 224/1000 | Loss: 0.00001555
Iteration 225/1000 | Loss: 0.00001555
Iteration 226/1000 | Loss: 0.00001555
Iteration 227/1000 | Loss: 0.00001555
Iteration 228/1000 | Loss: 0.00001555
Iteration 229/1000 | Loss: 0.00001555
Iteration 230/1000 | Loss: 0.00001555
Iteration 231/1000 | Loss: 0.00001555
Iteration 232/1000 | Loss: 0.00001554
Iteration 233/1000 | Loss: 0.00001554
Iteration 234/1000 | Loss: 0.00001554
Iteration 235/1000 | Loss: 0.00002284
Iteration 236/1000 | Loss: 0.00062202
Iteration 237/1000 | Loss: 0.00013235
Iteration 238/1000 | Loss: 0.00001936
Iteration 239/1000 | Loss: 0.00002076
Iteration 240/1000 | Loss: 0.00001570
Iteration 241/1000 | Loss: 0.00002094
Iteration 242/1000 | Loss: 0.00001557
Iteration 243/1000 | Loss: 0.00001556
Iteration 244/1000 | Loss: 0.00001703
Iteration 245/1000 | Loss: 0.00002668
Iteration 246/1000 | Loss: 0.00014882
Iteration 247/1000 | Loss: 0.00001551
Iteration 248/1000 | Loss: 0.00001551
Iteration 249/1000 | Loss: 0.00001551
Iteration 250/1000 | Loss: 0.00001551
Iteration 251/1000 | Loss: 0.00001551
Iteration 252/1000 | Loss: 0.00001551
Iteration 253/1000 | Loss: 0.00001550
Iteration 254/1000 | Loss: 0.00001550
Iteration 255/1000 | Loss: 0.00001550
Iteration 256/1000 | Loss: 0.00001550
Iteration 257/1000 | Loss: 0.00001550
Iteration 258/1000 | Loss: 0.00001550
Iteration 259/1000 | Loss: 0.00001550
Iteration 260/1000 | Loss: 0.00001550
Iteration 261/1000 | Loss: 0.00001550
Iteration 262/1000 | Loss: 0.00001550
Iteration 263/1000 | Loss: 0.00001550
Iteration 264/1000 | Loss: 0.00001550
Iteration 265/1000 | Loss: 0.00001550
Iteration 266/1000 | Loss: 0.00001550
Iteration 267/1000 | Loss: 0.00001550
Iteration 268/1000 | Loss: 0.00001550
Iteration 269/1000 | Loss: 0.00001550
Iteration 270/1000 | Loss: 0.00001550
Iteration 271/1000 | Loss: 0.00001550
Iteration 272/1000 | Loss: 0.00001550
Iteration 273/1000 | Loss: 0.00001550
Iteration 274/1000 | Loss: 0.00001550
Iteration 275/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.5504263501497917e-05, 1.5504263501497917e-05, 1.5504263501497917e-05, 1.5504263501497917e-05, 1.5504263501497917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5504263501497917e-05

Optimization complete. Final v2v error: 3.2512824535369873 mm

Highest mean error: 4.585062503814697 mm for frame 72

Lowest mean error: 2.7355706691741943 mm for frame 152

Saving results

Total time: 213.97805976867676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973540
Iteration 2/25 | Loss: 0.00197761
Iteration 3/25 | Loss: 0.00168919
Iteration 4/25 | Loss: 0.00154827
Iteration 5/25 | Loss: 0.00151525
Iteration 6/25 | Loss: 0.00144090
Iteration 7/25 | Loss: 0.00139308
Iteration 8/25 | Loss: 0.00130531
Iteration 9/25 | Loss: 0.00129880
Iteration 10/25 | Loss: 0.00128352
Iteration 11/25 | Loss: 0.00129101
Iteration 12/25 | Loss: 0.00129005
Iteration 13/25 | Loss: 0.00129014
Iteration 14/25 | Loss: 0.00127136
Iteration 15/25 | Loss: 0.00134699
Iteration 16/25 | Loss: 0.00129691
Iteration 17/25 | Loss: 0.00127770
Iteration 18/25 | Loss: 0.00127877
Iteration 19/25 | Loss: 0.00125422
Iteration 20/25 | Loss: 0.00125162
Iteration 21/25 | Loss: 0.00125013
Iteration 22/25 | Loss: 0.00125632
Iteration 23/25 | Loss: 0.00125364
Iteration 24/25 | Loss: 0.00124854
Iteration 25/25 | Loss: 0.00124580

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79355550
Iteration 2/25 | Loss: 0.00121191
Iteration 3/25 | Loss: 0.00121191
Iteration 4/25 | Loss: 0.00121191
Iteration 5/25 | Loss: 0.00121191
Iteration 6/25 | Loss: 0.00121191
Iteration 7/25 | Loss: 0.00121191
Iteration 8/25 | Loss: 0.00121191
Iteration 9/25 | Loss: 0.00121191
Iteration 10/25 | Loss: 0.00121191
Iteration 11/25 | Loss: 0.00121190
Iteration 12/25 | Loss: 0.00121190
Iteration 13/25 | Loss: 0.00121190
Iteration 14/25 | Loss: 0.00121190
Iteration 15/25 | Loss: 0.00121190
Iteration 16/25 | Loss: 0.00121190
Iteration 17/25 | Loss: 0.00121190
Iteration 18/25 | Loss: 0.00121190
Iteration 19/25 | Loss: 0.00121190
Iteration 20/25 | Loss: 0.00121190
Iteration 21/25 | Loss: 0.00121190
Iteration 22/25 | Loss: 0.00121190
Iteration 23/25 | Loss: 0.00121190
Iteration 24/25 | Loss: 0.00121190
Iteration 25/25 | Loss: 0.00121190

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121190
Iteration 2/1000 | Loss: 0.00008100
Iteration 3/1000 | Loss: 0.00031732
Iteration 4/1000 | Loss: 0.00033969
Iteration 5/1000 | Loss: 0.00006447
Iteration 6/1000 | Loss: 0.00025838
Iteration 7/1000 | Loss: 0.00021741
Iteration 8/1000 | Loss: 0.00004911
Iteration 9/1000 | Loss: 0.00004645
Iteration 10/1000 | Loss: 0.00010995
Iteration 11/1000 | Loss: 0.00012665
Iteration 12/1000 | Loss: 0.00011277
Iteration 13/1000 | Loss: 0.00003955
Iteration 14/1000 | Loss: 0.00024723
Iteration 15/1000 | Loss: 0.00020196
Iteration 16/1000 | Loss: 0.00069978
Iteration 17/1000 | Loss: 0.00005085
Iteration 18/1000 | Loss: 0.00003299
Iteration 19/1000 | Loss: 0.00002759
Iteration 20/1000 | Loss: 0.00002299
Iteration 21/1000 | Loss: 0.00002049
Iteration 22/1000 | Loss: 0.00002580
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001683
Iteration 26/1000 | Loss: 0.00001636
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001570
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001534
Iteration 31/1000 | Loss: 0.00001515
Iteration 32/1000 | Loss: 0.00001514
Iteration 33/1000 | Loss: 0.00001513
Iteration 34/1000 | Loss: 0.00001513
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001511
Iteration 38/1000 | Loss: 0.00001511
Iteration 39/1000 | Loss: 0.00001510
Iteration 40/1000 | Loss: 0.00001510
Iteration 41/1000 | Loss: 0.00001508
Iteration 42/1000 | Loss: 0.00001507
Iteration 43/1000 | Loss: 0.00001504
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001500
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001495
Iteration 49/1000 | Loss: 0.00001495
Iteration 50/1000 | Loss: 0.00001494
Iteration 51/1000 | Loss: 0.00001494
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001493
Iteration 56/1000 | Loss: 0.00001493
Iteration 57/1000 | Loss: 0.00001492
Iteration 58/1000 | Loss: 0.00001492
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001491
Iteration 61/1000 | Loss: 0.00001491
Iteration 62/1000 | Loss: 0.00001491
Iteration 63/1000 | Loss: 0.00001491
Iteration 64/1000 | Loss: 0.00001491
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00001490
Iteration 67/1000 | Loss: 0.00001490
Iteration 68/1000 | Loss: 0.00001490
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001489
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001488
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001487
Iteration 81/1000 | Loss: 0.00001487
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001486
Iteration 86/1000 | Loss: 0.00001486
Iteration 87/1000 | Loss: 0.00001485
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001485
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001484
Iteration 94/1000 | Loss: 0.00001483
Iteration 95/1000 | Loss: 0.00001483
Iteration 96/1000 | Loss: 0.00001483
Iteration 97/1000 | Loss: 0.00001483
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001481
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001480
Iteration 115/1000 | Loss: 0.00001480
Iteration 116/1000 | Loss: 0.00001480
Iteration 117/1000 | Loss: 0.00001480
Iteration 118/1000 | Loss: 0.00001480
Iteration 119/1000 | Loss: 0.00001480
Iteration 120/1000 | Loss: 0.00001480
Iteration 121/1000 | Loss: 0.00001479
Iteration 122/1000 | Loss: 0.00001479
Iteration 123/1000 | Loss: 0.00001479
Iteration 124/1000 | Loss: 0.00001479
Iteration 125/1000 | Loss: 0.00001479
Iteration 126/1000 | Loss: 0.00001479
Iteration 127/1000 | Loss: 0.00001479
Iteration 128/1000 | Loss: 0.00001479
Iteration 129/1000 | Loss: 0.00001479
Iteration 130/1000 | Loss: 0.00001479
Iteration 131/1000 | Loss: 0.00001478
Iteration 132/1000 | Loss: 0.00001478
Iteration 133/1000 | Loss: 0.00001478
Iteration 134/1000 | Loss: 0.00001478
Iteration 135/1000 | Loss: 0.00001478
Iteration 136/1000 | Loss: 0.00001478
Iteration 137/1000 | Loss: 0.00001478
Iteration 138/1000 | Loss: 0.00001477
Iteration 139/1000 | Loss: 0.00001477
Iteration 140/1000 | Loss: 0.00001477
Iteration 141/1000 | Loss: 0.00001477
Iteration 142/1000 | Loss: 0.00001477
Iteration 143/1000 | Loss: 0.00001477
Iteration 144/1000 | Loss: 0.00001477
Iteration 145/1000 | Loss: 0.00001477
Iteration 146/1000 | Loss: 0.00001477
Iteration 147/1000 | Loss: 0.00001477
Iteration 148/1000 | Loss: 0.00001477
Iteration 149/1000 | Loss: 0.00001477
Iteration 150/1000 | Loss: 0.00001477
Iteration 151/1000 | Loss: 0.00001477
Iteration 152/1000 | Loss: 0.00001477
Iteration 153/1000 | Loss: 0.00001477
Iteration 154/1000 | Loss: 0.00001477
Iteration 155/1000 | Loss: 0.00001477
Iteration 156/1000 | Loss: 0.00001477
Iteration 157/1000 | Loss: 0.00001477
Iteration 158/1000 | Loss: 0.00001477
Iteration 159/1000 | Loss: 0.00001477
Iteration 160/1000 | Loss: 0.00001477
Iteration 161/1000 | Loss: 0.00001477
Iteration 162/1000 | Loss: 0.00001477
Iteration 163/1000 | Loss: 0.00001477
Iteration 164/1000 | Loss: 0.00001477
Iteration 165/1000 | Loss: 0.00001477
Iteration 166/1000 | Loss: 0.00001477
Iteration 167/1000 | Loss: 0.00001477
Iteration 168/1000 | Loss: 0.00001477
Iteration 169/1000 | Loss: 0.00001477
Iteration 170/1000 | Loss: 0.00001477
Iteration 171/1000 | Loss: 0.00001477
Iteration 172/1000 | Loss: 0.00001477
Iteration 173/1000 | Loss: 0.00001477
Iteration 174/1000 | Loss: 0.00001477
Iteration 175/1000 | Loss: 0.00001477
Iteration 176/1000 | Loss: 0.00001477
Iteration 177/1000 | Loss: 0.00001477
Iteration 178/1000 | Loss: 0.00001477
Iteration 179/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.4772435861232225e-05, 1.4772435861232225e-05, 1.4772435861232225e-05, 1.4772435861232225e-05, 1.4772435861232225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4772435861232225e-05

Optimization complete. Final v2v error: 3.225764274597168 mm

Highest mean error: 4.289855480194092 mm for frame 64

Lowest mean error: 2.85983943939209 mm for frame 107

Saving results

Total time: 99.76197171211243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375377
Iteration 2/25 | Loss: 0.00126389
Iteration 3/25 | Loss: 0.00116512
Iteration 4/25 | Loss: 0.00114493
Iteration 5/25 | Loss: 0.00113808
Iteration 6/25 | Loss: 0.00113653
Iteration 7/25 | Loss: 0.00113622
Iteration 8/25 | Loss: 0.00113622
Iteration 9/25 | Loss: 0.00113622
Iteration 10/25 | Loss: 0.00113622
Iteration 11/25 | Loss: 0.00113622
Iteration 12/25 | Loss: 0.00113622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011362193617969751, 0.0011362193617969751, 0.0011362193617969751, 0.0011362193617969751, 0.0011362193617969751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011362193617969751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30971646
Iteration 2/25 | Loss: 0.00089417
Iteration 3/25 | Loss: 0.00089417
Iteration 4/25 | Loss: 0.00089417
Iteration 5/25 | Loss: 0.00089417
Iteration 6/25 | Loss: 0.00089417
Iteration 7/25 | Loss: 0.00089417
Iteration 8/25 | Loss: 0.00089417
Iteration 9/25 | Loss: 0.00089417
Iteration 10/25 | Loss: 0.00089417
Iteration 11/25 | Loss: 0.00089417
Iteration 12/25 | Loss: 0.00089417
Iteration 13/25 | Loss: 0.00089417
Iteration 14/25 | Loss: 0.00089417
Iteration 15/25 | Loss: 0.00089417
Iteration 16/25 | Loss: 0.00089417
Iteration 17/25 | Loss: 0.00089417
Iteration 18/25 | Loss: 0.00089417
Iteration 19/25 | Loss: 0.00089417
Iteration 20/25 | Loss: 0.00089417
Iteration 21/25 | Loss: 0.00089417
Iteration 22/25 | Loss: 0.00089417
Iteration 23/25 | Loss: 0.00089417
Iteration 24/25 | Loss: 0.00089417
Iteration 25/25 | Loss: 0.00089417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089417
Iteration 2/1000 | Loss: 0.00005046
Iteration 3/1000 | Loss: 0.00003465
Iteration 4/1000 | Loss: 0.00002607
Iteration 5/1000 | Loss: 0.00002354
Iteration 6/1000 | Loss: 0.00002176
Iteration 7/1000 | Loss: 0.00002011
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00001876
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001811
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001765
Iteration 14/1000 | Loss: 0.00001757
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001738
Iteration 17/1000 | Loss: 0.00001736
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001729
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001720
Iteration 24/1000 | Loss: 0.00001716
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001708
Iteration 32/1000 | Loss: 0.00001706
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001704
Iteration 36/1000 | Loss: 0.00001704
Iteration 37/1000 | Loss: 0.00001704
Iteration 38/1000 | Loss: 0.00001704
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001701
Iteration 47/1000 | Loss: 0.00001701
Iteration 48/1000 | Loss: 0.00001700
Iteration 49/1000 | Loss: 0.00001699
Iteration 50/1000 | Loss: 0.00001699
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001699
Iteration 53/1000 | Loss: 0.00001699
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001698
Iteration 57/1000 | Loss: 0.00001698
Iteration 58/1000 | Loss: 0.00001698
Iteration 59/1000 | Loss: 0.00001698
Iteration 60/1000 | Loss: 0.00001698
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001697
Iteration 65/1000 | Loss: 0.00001697
Iteration 66/1000 | Loss: 0.00001697
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001696
Iteration 69/1000 | Loss: 0.00001696
Iteration 70/1000 | Loss: 0.00001696
Iteration 71/1000 | Loss: 0.00001696
Iteration 72/1000 | Loss: 0.00001696
Iteration 73/1000 | Loss: 0.00001696
Iteration 74/1000 | Loss: 0.00001696
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001695
Iteration 77/1000 | Loss: 0.00001695
Iteration 78/1000 | Loss: 0.00001694
Iteration 79/1000 | Loss: 0.00001694
Iteration 80/1000 | Loss: 0.00001694
Iteration 81/1000 | Loss: 0.00001693
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001692
Iteration 85/1000 | Loss: 0.00001692
Iteration 86/1000 | Loss: 0.00001692
Iteration 87/1000 | Loss: 0.00001691
Iteration 88/1000 | Loss: 0.00001691
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001690
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001689
Iteration 98/1000 | Loss: 0.00001689
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001688
Iteration 101/1000 | Loss: 0.00001688
Iteration 102/1000 | Loss: 0.00001688
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001688
Iteration 109/1000 | Loss: 0.00001688
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001687
Iteration 120/1000 | Loss: 0.00001687
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001685
Iteration 129/1000 | Loss: 0.00001685
Iteration 130/1000 | Loss: 0.00001685
Iteration 131/1000 | Loss: 0.00001685
Iteration 132/1000 | Loss: 0.00001685
Iteration 133/1000 | Loss: 0.00001685
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001684
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001682
Iteration 173/1000 | Loss: 0.00001682
Iteration 174/1000 | Loss: 0.00001682
Iteration 175/1000 | Loss: 0.00001682
Iteration 176/1000 | Loss: 0.00001682
Iteration 177/1000 | Loss: 0.00001682
Iteration 178/1000 | Loss: 0.00001682
Iteration 179/1000 | Loss: 0.00001682
Iteration 180/1000 | Loss: 0.00001682
Iteration 181/1000 | Loss: 0.00001681
Iteration 182/1000 | Loss: 0.00001681
Iteration 183/1000 | Loss: 0.00001681
Iteration 184/1000 | Loss: 0.00001681
Iteration 185/1000 | Loss: 0.00001681
Iteration 186/1000 | Loss: 0.00001681
Iteration 187/1000 | Loss: 0.00001681
Iteration 188/1000 | Loss: 0.00001681
Iteration 189/1000 | Loss: 0.00001681
Iteration 190/1000 | Loss: 0.00001681
Iteration 191/1000 | Loss: 0.00001681
Iteration 192/1000 | Loss: 0.00001681
Iteration 193/1000 | Loss: 0.00001681
Iteration 194/1000 | Loss: 0.00001681
Iteration 195/1000 | Loss: 0.00001681
Iteration 196/1000 | Loss: 0.00001681
Iteration 197/1000 | Loss: 0.00001680
Iteration 198/1000 | Loss: 0.00001680
Iteration 199/1000 | Loss: 0.00001680
Iteration 200/1000 | Loss: 0.00001680
Iteration 201/1000 | Loss: 0.00001680
Iteration 202/1000 | Loss: 0.00001680
Iteration 203/1000 | Loss: 0.00001680
Iteration 204/1000 | Loss: 0.00001680
Iteration 205/1000 | Loss: 0.00001680
Iteration 206/1000 | Loss: 0.00001680
Iteration 207/1000 | Loss: 0.00001680
Iteration 208/1000 | Loss: 0.00001680
Iteration 209/1000 | Loss: 0.00001680
Iteration 210/1000 | Loss: 0.00001680
Iteration 211/1000 | Loss: 0.00001679
Iteration 212/1000 | Loss: 0.00001679
Iteration 213/1000 | Loss: 0.00001679
Iteration 214/1000 | Loss: 0.00001679
Iteration 215/1000 | Loss: 0.00001679
Iteration 216/1000 | Loss: 0.00001679
Iteration 217/1000 | Loss: 0.00001679
Iteration 218/1000 | Loss: 0.00001679
Iteration 219/1000 | Loss: 0.00001679
Iteration 220/1000 | Loss: 0.00001679
Iteration 221/1000 | Loss: 0.00001679
Iteration 222/1000 | Loss: 0.00001679
Iteration 223/1000 | Loss: 0.00001679
Iteration 224/1000 | Loss: 0.00001679
Iteration 225/1000 | Loss: 0.00001679
Iteration 226/1000 | Loss: 0.00001679
Iteration 227/1000 | Loss: 0.00001679
Iteration 228/1000 | Loss: 0.00001679
Iteration 229/1000 | Loss: 0.00001679
Iteration 230/1000 | Loss: 0.00001679
Iteration 231/1000 | Loss: 0.00001679
Iteration 232/1000 | Loss: 0.00001678
Iteration 233/1000 | Loss: 0.00001678
Iteration 234/1000 | Loss: 0.00001678
Iteration 235/1000 | Loss: 0.00001678
Iteration 236/1000 | Loss: 0.00001678
Iteration 237/1000 | Loss: 0.00001678
Iteration 238/1000 | Loss: 0.00001678
Iteration 239/1000 | Loss: 0.00001678
Iteration 240/1000 | Loss: 0.00001678
Iteration 241/1000 | Loss: 0.00001678
Iteration 242/1000 | Loss: 0.00001678
Iteration 243/1000 | Loss: 0.00001678
Iteration 244/1000 | Loss: 0.00001678
Iteration 245/1000 | Loss: 0.00001678
Iteration 246/1000 | Loss: 0.00001678
Iteration 247/1000 | Loss: 0.00001678
Iteration 248/1000 | Loss: 0.00001678
Iteration 249/1000 | Loss: 0.00001678
Iteration 250/1000 | Loss: 0.00001678
Iteration 251/1000 | Loss: 0.00001678
Iteration 252/1000 | Loss: 0.00001677
Iteration 253/1000 | Loss: 0.00001677
Iteration 254/1000 | Loss: 0.00001677
Iteration 255/1000 | Loss: 0.00001677
Iteration 256/1000 | Loss: 0.00001677
Iteration 257/1000 | Loss: 0.00001677
Iteration 258/1000 | Loss: 0.00001677
Iteration 259/1000 | Loss: 0.00001677
Iteration 260/1000 | Loss: 0.00001677
Iteration 261/1000 | Loss: 0.00001677
Iteration 262/1000 | Loss: 0.00001677
Iteration 263/1000 | Loss: 0.00001677
Iteration 264/1000 | Loss: 0.00001677
Iteration 265/1000 | Loss: 0.00001677
Iteration 266/1000 | Loss: 0.00001677
Iteration 267/1000 | Loss: 0.00001677
Iteration 268/1000 | Loss: 0.00001677
Iteration 269/1000 | Loss: 0.00001677
Iteration 270/1000 | Loss: 0.00001677
Iteration 271/1000 | Loss: 0.00001677
Iteration 272/1000 | Loss: 0.00001676
Iteration 273/1000 | Loss: 0.00001676
Iteration 274/1000 | Loss: 0.00001676
Iteration 275/1000 | Loss: 0.00001676
Iteration 276/1000 | Loss: 0.00001676
Iteration 277/1000 | Loss: 0.00001676
Iteration 278/1000 | Loss: 0.00001676
Iteration 279/1000 | Loss: 0.00001676
Iteration 280/1000 | Loss: 0.00001676
Iteration 281/1000 | Loss: 0.00001676
Iteration 282/1000 | Loss: 0.00001676
Iteration 283/1000 | Loss: 0.00001676
Iteration 284/1000 | Loss: 0.00001676
Iteration 285/1000 | Loss: 0.00001676
Iteration 286/1000 | Loss: 0.00001676
Iteration 287/1000 | Loss: 0.00001676
Iteration 288/1000 | Loss: 0.00001676
Iteration 289/1000 | Loss: 0.00001676
Iteration 290/1000 | Loss: 0.00001676
Iteration 291/1000 | Loss: 0.00001676
Iteration 292/1000 | Loss: 0.00001676
Iteration 293/1000 | Loss: 0.00001676
Iteration 294/1000 | Loss: 0.00001676
Iteration 295/1000 | Loss: 0.00001676
Iteration 296/1000 | Loss: 0.00001676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [1.6759948266553693e-05, 1.6759948266553693e-05, 1.6759948266553693e-05, 1.6759948266553693e-05, 1.6759948266553693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6759948266553693e-05

Optimization complete. Final v2v error: 3.421536684036255 mm

Highest mean error: 4.03791618347168 mm for frame 93

Lowest mean error: 2.747187614440918 mm for frame 11

Saving results

Total time: 50.848066329956055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807163
Iteration 2/25 | Loss: 0.00138862
Iteration 3/25 | Loss: 0.00124593
Iteration 4/25 | Loss: 0.00122786
Iteration 5/25 | Loss: 0.00122334
Iteration 6/25 | Loss: 0.00122216
Iteration 7/25 | Loss: 0.00122179
Iteration 8/25 | Loss: 0.00122179
Iteration 9/25 | Loss: 0.00122179
Iteration 10/25 | Loss: 0.00122179
Iteration 11/25 | Loss: 0.00122179
Iteration 12/25 | Loss: 0.00122179
Iteration 13/25 | Loss: 0.00122179
Iteration 14/25 | Loss: 0.00122179
Iteration 15/25 | Loss: 0.00122179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001221788115799427, 0.001221788115799427, 0.001221788115799427, 0.001221788115799427, 0.001221788115799427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001221788115799427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36006808
Iteration 2/25 | Loss: 0.00192533
Iteration 3/25 | Loss: 0.00192532
Iteration 4/25 | Loss: 0.00192532
Iteration 5/25 | Loss: 0.00192532
Iteration 6/25 | Loss: 0.00192532
Iteration 7/25 | Loss: 0.00192532
Iteration 8/25 | Loss: 0.00192532
Iteration 9/25 | Loss: 0.00192532
Iteration 10/25 | Loss: 0.00192532
Iteration 11/25 | Loss: 0.00192532
Iteration 12/25 | Loss: 0.00192532
Iteration 13/25 | Loss: 0.00192532
Iteration 14/25 | Loss: 0.00192532
Iteration 15/25 | Loss: 0.00192532
Iteration 16/25 | Loss: 0.00192532
Iteration 17/25 | Loss: 0.00192532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019253187347203493, 0.0019253187347203493, 0.0019253187347203493, 0.0019253187347203493, 0.0019253187347203493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019253187347203493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192532
Iteration 2/1000 | Loss: 0.00035746
Iteration 3/1000 | Loss: 0.00014582
Iteration 4/1000 | Loss: 0.00007760
Iteration 5/1000 | Loss: 0.00006872
Iteration 6/1000 | Loss: 0.00006359
Iteration 7/1000 | Loss: 0.00006079
Iteration 8/1000 | Loss: 0.00017436
Iteration 9/1000 | Loss: 0.00006158
Iteration 10/1000 | Loss: 0.00023290
Iteration 11/1000 | Loss: 0.00011614
Iteration 12/1000 | Loss: 0.00011036
Iteration 13/1000 | Loss: 0.00006266
Iteration 14/1000 | Loss: 0.00006292
Iteration 15/1000 | Loss: 0.00015304
Iteration 16/1000 | Loss: 0.00014663
Iteration 17/1000 | Loss: 0.00016593
Iteration 18/1000 | Loss: 0.00007214
Iteration 19/1000 | Loss: 0.00005928
Iteration 20/1000 | Loss: 0.00005957
Iteration 21/1000 | Loss: 0.00014663
Iteration 22/1000 | Loss: 0.00019421
Iteration 23/1000 | Loss: 0.00014647
Iteration 24/1000 | Loss: 0.00007379
Iteration 25/1000 | Loss: 0.00010074
Iteration 26/1000 | Loss: 0.00011318
Iteration 27/1000 | Loss: 0.00021602
Iteration 28/1000 | Loss: 0.00012663
Iteration 29/1000 | Loss: 0.00017858
Iteration 30/1000 | Loss: 0.00017093
Iteration 31/1000 | Loss: 0.00020374
Iteration 32/1000 | Loss: 0.00014223
Iteration 33/1000 | Loss: 0.00022750
Iteration 34/1000 | Loss: 0.00013490
Iteration 35/1000 | Loss: 0.00005537
Iteration 36/1000 | Loss: 0.00014333
Iteration 37/1000 | Loss: 0.00016848
Iteration 38/1000 | Loss: 0.00011900
Iteration 39/1000 | Loss: 0.00006077
Iteration 40/1000 | Loss: 0.00015965
Iteration 41/1000 | Loss: 0.00006322
Iteration 42/1000 | Loss: 0.00005749
Iteration 43/1000 | Loss: 0.00010549
Iteration 44/1000 | Loss: 0.00017225
Iteration 45/1000 | Loss: 0.00017126
Iteration 46/1000 | Loss: 0.00006319
Iteration 47/1000 | Loss: 0.00015173
Iteration 48/1000 | Loss: 0.00006686
Iteration 49/1000 | Loss: 0.00005855
Iteration 50/1000 | Loss: 0.00026178
Iteration 51/1000 | Loss: 0.00014815
Iteration 52/1000 | Loss: 0.00014149
Iteration 53/1000 | Loss: 0.00009870
Iteration 54/1000 | Loss: 0.00022904
Iteration 55/1000 | Loss: 0.00005434
Iteration 56/1000 | Loss: 0.00007243
Iteration 57/1000 | Loss: 0.00005655
Iteration 58/1000 | Loss: 0.00005373
Iteration 59/1000 | Loss: 0.00005307
Iteration 60/1000 | Loss: 0.00005251
Iteration 61/1000 | Loss: 0.00005211
Iteration 62/1000 | Loss: 0.00005187
Iteration 63/1000 | Loss: 0.00005159
Iteration 64/1000 | Loss: 0.00005129
Iteration 65/1000 | Loss: 0.00005114
Iteration 66/1000 | Loss: 0.00005113
Iteration 67/1000 | Loss: 0.00005091
Iteration 68/1000 | Loss: 0.00005070
Iteration 69/1000 | Loss: 0.00005045
Iteration 70/1000 | Loss: 0.00005008
Iteration 71/1000 | Loss: 0.00004964
Iteration 72/1000 | Loss: 0.00004926
Iteration 73/1000 | Loss: 0.00004881
Iteration 74/1000 | Loss: 0.00004840
Iteration 75/1000 | Loss: 0.00004796
Iteration 76/1000 | Loss: 0.00004748
Iteration 77/1000 | Loss: 0.00004724
Iteration 78/1000 | Loss: 0.00004699
Iteration 79/1000 | Loss: 0.00004688
Iteration 80/1000 | Loss: 0.00004663
Iteration 81/1000 | Loss: 0.00004639
Iteration 82/1000 | Loss: 0.00004622
Iteration 83/1000 | Loss: 0.00004598
Iteration 84/1000 | Loss: 0.00004579
Iteration 85/1000 | Loss: 0.00004563
Iteration 86/1000 | Loss: 0.00004561
Iteration 87/1000 | Loss: 0.00004545
Iteration 88/1000 | Loss: 0.00004531
Iteration 89/1000 | Loss: 0.00004526
Iteration 90/1000 | Loss: 0.00004526
Iteration 91/1000 | Loss: 0.00004525
Iteration 92/1000 | Loss: 0.00004525
Iteration 93/1000 | Loss: 0.00004524
Iteration 94/1000 | Loss: 0.00004523
Iteration 95/1000 | Loss: 0.00004523
Iteration 96/1000 | Loss: 0.00004523
Iteration 97/1000 | Loss: 0.00004522
Iteration 98/1000 | Loss: 0.00004522
Iteration 99/1000 | Loss: 0.00004521
Iteration 100/1000 | Loss: 0.00004520
Iteration 101/1000 | Loss: 0.00004520
Iteration 102/1000 | Loss: 0.00004520
Iteration 103/1000 | Loss: 0.00004520
Iteration 104/1000 | Loss: 0.00004520
Iteration 105/1000 | Loss: 0.00004520
Iteration 106/1000 | Loss: 0.00004520
Iteration 107/1000 | Loss: 0.00004520
Iteration 108/1000 | Loss: 0.00004519
Iteration 109/1000 | Loss: 0.00004519
Iteration 110/1000 | Loss: 0.00004519
Iteration 111/1000 | Loss: 0.00004518
Iteration 112/1000 | Loss: 0.00004518
Iteration 113/1000 | Loss: 0.00004517
Iteration 114/1000 | Loss: 0.00004517
Iteration 115/1000 | Loss: 0.00004517
Iteration 116/1000 | Loss: 0.00004517
Iteration 117/1000 | Loss: 0.00004517
Iteration 118/1000 | Loss: 0.00004517
Iteration 119/1000 | Loss: 0.00004517
Iteration 120/1000 | Loss: 0.00004517
Iteration 121/1000 | Loss: 0.00004517
Iteration 122/1000 | Loss: 0.00004517
Iteration 123/1000 | Loss: 0.00004517
Iteration 124/1000 | Loss: 0.00004517
Iteration 125/1000 | Loss: 0.00004517
Iteration 126/1000 | Loss: 0.00004516
Iteration 127/1000 | Loss: 0.00004516
Iteration 128/1000 | Loss: 0.00004515
Iteration 129/1000 | Loss: 0.00004515
Iteration 130/1000 | Loss: 0.00004514
Iteration 131/1000 | Loss: 0.00004514
Iteration 132/1000 | Loss: 0.00004514
Iteration 133/1000 | Loss: 0.00004514
Iteration 134/1000 | Loss: 0.00004514
Iteration 135/1000 | Loss: 0.00004514
Iteration 136/1000 | Loss: 0.00004514
Iteration 137/1000 | Loss: 0.00004513
Iteration 138/1000 | Loss: 0.00004513
Iteration 139/1000 | Loss: 0.00004513
Iteration 140/1000 | Loss: 0.00004513
Iteration 141/1000 | Loss: 0.00004512
Iteration 142/1000 | Loss: 0.00004511
Iteration 143/1000 | Loss: 0.00004511
Iteration 144/1000 | Loss: 0.00004509
Iteration 145/1000 | Loss: 0.00004509
Iteration 146/1000 | Loss: 0.00004509
Iteration 147/1000 | Loss: 0.00004509
Iteration 148/1000 | Loss: 0.00004509
Iteration 149/1000 | Loss: 0.00004509
Iteration 150/1000 | Loss: 0.00004508
Iteration 151/1000 | Loss: 0.00004508
Iteration 152/1000 | Loss: 0.00004507
Iteration 153/1000 | Loss: 0.00004507
Iteration 154/1000 | Loss: 0.00004507
Iteration 155/1000 | Loss: 0.00004507
Iteration 156/1000 | Loss: 0.00004506
Iteration 157/1000 | Loss: 0.00004506
Iteration 158/1000 | Loss: 0.00004506
Iteration 159/1000 | Loss: 0.00004506
Iteration 160/1000 | Loss: 0.00004505
Iteration 161/1000 | Loss: 0.00004505
Iteration 162/1000 | Loss: 0.00004505
Iteration 163/1000 | Loss: 0.00004504
Iteration 164/1000 | Loss: 0.00004504
Iteration 165/1000 | Loss: 0.00004504
Iteration 166/1000 | Loss: 0.00004504
Iteration 167/1000 | Loss: 0.00004504
Iteration 168/1000 | Loss: 0.00004504
Iteration 169/1000 | Loss: 0.00004503
Iteration 170/1000 | Loss: 0.00004503
Iteration 171/1000 | Loss: 0.00004503
Iteration 172/1000 | Loss: 0.00004503
Iteration 173/1000 | Loss: 0.00004503
Iteration 174/1000 | Loss: 0.00004503
Iteration 175/1000 | Loss: 0.00004503
Iteration 176/1000 | Loss: 0.00004502
Iteration 177/1000 | Loss: 0.00004502
Iteration 178/1000 | Loss: 0.00004502
Iteration 179/1000 | Loss: 0.00004502
Iteration 180/1000 | Loss: 0.00004502
Iteration 181/1000 | Loss: 0.00004501
Iteration 182/1000 | Loss: 0.00004501
Iteration 183/1000 | Loss: 0.00004501
Iteration 184/1000 | Loss: 0.00004501
Iteration 185/1000 | Loss: 0.00004500
Iteration 186/1000 | Loss: 0.00004500
Iteration 187/1000 | Loss: 0.00004500
Iteration 188/1000 | Loss: 0.00004500
Iteration 189/1000 | Loss: 0.00004500
Iteration 190/1000 | Loss: 0.00004500
Iteration 191/1000 | Loss: 0.00004500
Iteration 192/1000 | Loss: 0.00004499
Iteration 193/1000 | Loss: 0.00004499
Iteration 194/1000 | Loss: 0.00004499
Iteration 195/1000 | Loss: 0.00004499
Iteration 196/1000 | Loss: 0.00004499
Iteration 197/1000 | Loss: 0.00004499
Iteration 198/1000 | Loss: 0.00004499
Iteration 199/1000 | Loss: 0.00004499
Iteration 200/1000 | Loss: 0.00004499
Iteration 201/1000 | Loss: 0.00004499
Iteration 202/1000 | Loss: 0.00004499
Iteration 203/1000 | Loss: 0.00004498
Iteration 204/1000 | Loss: 0.00004498
Iteration 205/1000 | Loss: 0.00004498
Iteration 206/1000 | Loss: 0.00004498
Iteration 207/1000 | Loss: 0.00004498
Iteration 208/1000 | Loss: 0.00004498
Iteration 209/1000 | Loss: 0.00004498
Iteration 210/1000 | Loss: 0.00004498
Iteration 211/1000 | Loss: 0.00004498
Iteration 212/1000 | Loss: 0.00004497
Iteration 213/1000 | Loss: 0.00004497
Iteration 214/1000 | Loss: 0.00004497
Iteration 215/1000 | Loss: 0.00004497
Iteration 216/1000 | Loss: 0.00004497
Iteration 217/1000 | Loss: 0.00004497
Iteration 218/1000 | Loss: 0.00004497
Iteration 219/1000 | Loss: 0.00004496
Iteration 220/1000 | Loss: 0.00004496
Iteration 221/1000 | Loss: 0.00004496
Iteration 222/1000 | Loss: 0.00004496
Iteration 223/1000 | Loss: 0.00004496
Iteration 224/1000 | Loss: 0.00004496
Iteration 225/1000 | Loss: 0.00004496
Iteration 226/1000 | Loss: 0.00004496
Iteration 227/1000 | Loss: 0.00004496
Iteration 228/1000 | Loss: 0.00004496
Iteration 229/1000 | Loss: 0.00004496
Iteration 230/1000 | Loss: 0.00004495
Iteration 231/1000 | Loss: 0.00004495
Iteration 232/1000 | Loss: 0.00004495
Iteration 233/1000 | Loss: 0.00004495
Iteration 234/1000 | Loss: 0.00004495
Iteration 235/1000 | Loss: 0.00004495
Iteration 236/1000 | Loss: 0.00004495
Iteration 237/1000 | Loss: 0.00004494
Iteration 238/1000 | Loss: 0.00004494
Iteration 239/1000 | Loss: 0.00004494
Iteration 240/1000 | Loss: 0.00004494
Iteration 241/1000 | Loss: 0.00004494
Iteration 242/1000 | Loss: 0.00004494
Iteration 243/1000 | Loss: 0.00004494
Iteration 244/1000 | Loss: 0.00004494
Iteration 245/1000 | Loss: 0.00004494
Iteration 246/1000 | Loss: 0.00004494
Iteration 247/1000 | Loss: 0.00004493
Iteration 248/1000 | Loss: 0.00004493
Iteration 249/1000 | Loss: 0.00004493
Iteration 250/1000 | Loss: 0.00004493
Iteration 251/1000 | Loss: 0.00004493
Iteration 252/1000 | Loss: 0.00004493
Iteration 253/1000 | Loss: 0.00004493
Iteration 254/1000 | Loss: 0.00004493
Iteration 255/1000 | Loss: 0.00004493
Iteration 256/1000 | Loss: 0.00004493
Iteration 257/1000 | Loss: 0.00004492
Iteration 258/1000 | Loss: 0.00004492
Iteration 259/1000 | Loss: 0.00004492
Iteration 260/1000 | Loss: 0.00004492
Iteration 261/1000 | Loss: 0.00004492
Iteration 262/1000 | Loss: 0.00004492
Iteration 263/1000 | Loss: 0.00004492
Iteration 264/1000 | Loss: 0.00004492
Iteration 265/1000 | Loss: 0.00004492
Iteration 266/1000 | Loss: 0.00004492
Iteration 267/1000 | Loss: 0.00004492
Iteration 268/1000 | Loss: 0.00004492
Iteration 269/1000 | Loss: 0.00004492
Iteration 270/1000 | Loss: 0.00004492
Iteration 271/1000 | Loss: 0.00004492
Iteration 272/1000 | Loss: 0.00004492
Iteration 273/1000 | Loss: 0.00004492
Iteration 274/1000 | Loss: 0.00004491
Iteration 275/1000 | Loss: 0.00004491
Iteration 276/1000 | Loss: 0.00004491
Iteration 277/1000 | Loss: 0.00004491
Iteration 278/1000 | Loss: 0.00004491
Iteration 279/1000 | Loss: 0.00004491
Iteration 280/1000 | Loss: 0.00004491
Iteration 281/1000 | Loss: 0.00004491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [4.491439540288411e-05, 4.491439540288411e-05, 4.491439540288411e-05, 4.491439540288411e-05, 4.491439540288411e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.491439540288411e-05

Optimization complete. Final v2v error: 3.39202880859375 mm

Highest mean error: 11.344681739807129 mm for frame 85

Lowest mean error: 2.347964286804199 mm for frame 38

Saving results

Total time: 155.36001777648926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067727
Iteration 2/25 | Loss: 0.00318416
Iteration 3/25 | Loss: 0.00232733
Iteration 4/25 | Loss: 0.00200209
Iteration 5/25 | Loss: 0.00194885
Iteration 6/25 | Loss: 0.00180486
Iteration 7/25 | Loss: 0.00186806
Iteration 8/25 | Loss: 0.00169981
Iteration 9/25 | Loss: 0.00161738
Iteration 10/25 | Loss: 0.00158547
Iteration 11/25 | Loss: 0.00155686
Iteration 12/25 | Loss: 0.00154089
Iteration 13/25 | Loss: 0.00151374
Iteration 14/25 | Loss: 0.00151616
Iteration 15/25 | Loss: 0.00151532
Iteration 16/25 | Loss: 0.00151992
Iteration 17/25 | Loss: 0.00151577
Iteration 18/25 | Loss: 0.00151161
Iteration 19/25 | Loss: 0.00149141
Iteration 20/25 | Loss: 0.00146812
Iteration 21/25 | Loss: 0.00146074
Iteration 22/25 | Loss: 0.00145201
Iteration 23/25 | Loss: 0.00144993
Iteration 24/25 | Loss: 0.00144896
Iteration 25/25 | Loss: 0.00144890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07370210
Iteration 2/25 | Loss: 0.00117289
Iteration 3/25 | Loss: 0.00102070
Iteration 4/25 | Loss: 0.00102070
Iteration 5/25 | Loss: 0.00102070
Iteration 6/25 | Loss: 0.00102070
Iteration 7/25 | Loss: 0.00102070
Iteration 8/25 | Loss: 0.00102070
Iteration 9/25 | Loss: 0.00102070
Iteration 10/25 | Loss: 0.00102070
Iteration 11/25 | Loss: 0.00102070
Iteration 12/25 | Loss: 0.00102070
Iteration 13/25 | Loss: 0.00102070
Iteration 14/25 | Loss: 0.00102070
Iteration 15/25 | Loss: 0.00102070
Iteration 16/25 | Loss: 0.00102070
Iteration 17/25 | Loss: 0.00102070
Iteration 18/25 | Loss: 0.00102070
Iteration 19/25 | Loss: 0.00102070
Iteration 20/25 | Loss: 0.00102070
Iteration 21/25 | Loss: 0.00102070
Iteration 22/25 | Loss: 0.00102070
Iteration 23/25 | Loss: 0.00102070
Iteration 24/25 | Loss: 0.00102070
Iteration 25/25 | Loss: 0.00102070

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102070
Iteration 2/1000 | Loss: 0.00028676
Iteration 3/1000 | Loss: 0.00014342
Iteration 4/1000 | Loss: 0.00015447
Iteration 5/1000 | Loss: 0.00011834
Iteration 6/1000 | Loss: 0.00012896
Iteration 7/1000 | Loss: 0.00011656
Iteration 8/1000 | Loss: 0.00007689
Iteration 9/1000 | Loss: 0.00017938
Iteration 10/1000 | Loss: 0.00122450
Iteration 11/1000 | Loss: 0.00175043
Iteration 12/1000 | Loss: 0.00047957
Iteration 13/1000 | Loss: 0.00039007
Iteration 14/1000 | Loss: 0.00035859
Iteration 15/1000 | Loss: 0.00046655
Iteration 16/1000 | Loss: 0.00074280
Iteration 17/1000 | Loss: 0.00029113
Iteration 18/1000 | Loss: 0.00011951
Iteration 19/1000 | Loss: 0.00012743
Iteration 20/1000 | Loss: 0.00014775
Iteration 21/1000 | Loss: 0.00032299
Iteration 22/1000 | Loss: 0.00105287
Iteration 23/1000 | Loss: 0.00028382
Iteration 24/1000 | Loss: 0.00117894
Iteration 25/1000 | Loss: 0.00014251
Iteration 26/1000 | Loss: 0.00012129
Iteration 27/1000 | Loss: 0.00050359
Iteration 28/1000 | Loss: 0.00053593
Iteration 29/1000 | Loss: 0.00016116
Iteration 30/1000 | Loss: 0.00015392
Iteration 31/1000 | Loss: 0.00031647
Iteration 32/1000 | Loss: 0.00013298
Iteration 33/1000 | Loss: 0.00016225
Iteration 34/1000 | Loss: 0.00016333
Iteration 35/1000 | Loss: 0.00034753
Iteration 36/1000 | Loss: 0.00022853
Iteration 37/1000 | Loss: 0.00018310
Iteration 38/1000 | Loss: 0.00018983
Iteration 39/1000 | Loss: 0.00035904
Iteration 40/1000 | Loss: 0.00072753
Iteration 41/1000 | Loss: 0.00095399
Iteration 42/1000 | Loss: 0.00031127
Iteration 43/1000 | Loss: 0.00018975
Iteration 44/1000 | Loss: 0.00047397
Iteration 45/1000 | Loss: 0.00035809
Iteration 46/1000 | Loss: 0.00029960
Iteration 47/1000 | Loss: 0.00088775
Iteration 48/1000 | Loss: 0.00050979
Iteration 49/1000 | Loss: 0.00022014
Iteration 50/1000 | Loss: 0.00075906
Iteration 51/1000 | Loss: 0.00188122
Iteration 52/1000 | Loss: 0.00189022
Iteration 53/1000 | Loss: 0.00044560
Iteration 54/1000 | Loss: 0.00019489
Iteration 55/1000 | Loss: 0.00013353
Iteration 56/1000 | Loss: 0.00015899
Iteration 57/1000 | Loss: 0.00008284
Iteration 58/1000 | Loss: 0.00013876
Iteration 59/1000 | Loss: 0.00011945
Iteration 60/1000 | Loss: 0.00006808
Iteration 61/1000 | Loss: 0.00031452
Iteration 62/1000 | Loss: 0.00034841
Iteration 63/1000 | Loss: 0.00027642
Iteration 64/1000 | Loss: 0.00020186
Iteration 65/1000 | Loss: 0.00009822
Iteration 66/1000 | Loss: 0.00013192
Iteration 67/1000 | Loss: 0.00035677
Iteration 68/1000 | Loss: 0.00046901
Iteration 69/1000 | Loss: 0.00015801
Iteration 70/1000 | Loss: 0.00025738
Iteration 71/1000 | Loss: 0.00020968
Iteration 72/1000 | Loss: 0.00009095
Iteration 73/1000 | Loss: 0.00005638
Iteration 74/1000 | Loss: 0.00008168
Iteration 75/1000 | Loss: 0.00079108
Iteration 76/1000 | Loss: 0.00039729
Iteration 77/1000 | Loss: 0.00049994
Iteration 78/1000 | Loss: 0.00033583
Iteration 79/1000 | Loss: 0.00017003
Iteration 80/1000 | Loss: 0.00008715
Iteration 81/1000 | Loss: 0.00032571
Iteration 82/1000 | Loss: 0.00009347
Iteration 83/1000 | Loss: 0.00010316
Iteration 84/1000 | Loss: 0.00005182
Iteration 85/1000 | Loss: 0.00005057
Iteration 86/1000 | Loss: 0.00005403
Iteration 87/1000 | Loss: 0.00015568
Iteration 88/1000 | Loss: 0.00010967
Iteration 89/1000 | Loss: 0.00014234
Iteration 90/1000 | Loss: 0.00012112
Iteration 91/1000 | Loss: 0.00015851
Iteration 92/1000 | Loss: 0.00039747
Iteration 93/1000 | Loss: 0.00010604
Iteration 94/1000 | Loss: 0.00017638
Iteration 95/1000 | Loss: 0.00019341
Iteration 96/1000 | Loss: 0.00007042
Iteration 97/1000 | Loss: 0.00016770
Iteration 98/1000 | Loss: 0.00013777
Iteration 99/1000 | Loss: 0.00015123
Iteration 100/1000 | Loss: 0.00007900
Iteration 101/1000 | Loss: 0.00004646
Iteration 102/1000 | Loss: 0.00004525
Iteration 103/1000 | Loss: 0.00015595
Iteration 104/1000 | Loss: 0.00005128
Iteration 105/1000 | Loss: 0.00014368
Iteration 106/1000 | Loss: 0.00005494
Iteration 107/1000 | Loss: 0.00012619
Iteration 108/1000 | Loss: 0.00004962
Iteration 109/1000 | Loss: 0.00012250
Iteration 110/1000 | Loss: 0.00004841
Iteration 111/1000 | Loss: 0.00011920
Iteration 112/1000 | Loss: 0.00006452
Iteration 113/1000 | Loss: 0.00010517
Iteration 114/1000 | Loss: 0.00004535
Iteration 115/1000 | Loss: 0.00004467
Iteration 116/1000 | Loss: 0.00004405
Iteration 117/1000 | Loss: 0.00004372
Iteration 118/1000 | Loss: 0.00025426
Iteration 119/1000 | Loss: 0.00035460
Iteration 120/1000 | Loss: 0.00004514
Iteration 121/1000 | Loss: 0.00004374
Iteration 122/1000 | Loss: 0.00004322
Iteration 123/1000 | Loss: 0.00004307
Iteration 124/1000 | Loss: 0.00004288
Iteration 125/1000 | Loss: 0.00004279
Iteration 126/1000 | Loss: 0.00004262
Iteration 127/1000 | Loss: 0.00026288
Iteration 128/1000 | Loss: 0.00027296
Iteration 129/1000 | Loss: 0.00004734
Iteration 130/1000 | Loss: 0.00004331
Iteration 131/1000 | Loss: 0.00004263
Iteration 132/1000 | Loss: 0.00004240
Iteration 133/1000 | Loss: 0.00004237
Iteration 134/1000 | Loss: 0.00004227
Iteration 135/1000 | Loss: 0.00040516
Iteration 136/1000 | Loss: 0.00018419
Iteration 137/1000 | Loss: 0.00005146
Iteration 138/1000 | Loss: 0.00016320
Iteration 139/1000 | Loss: 0.00013576
Iteration 140/1000 | Loss: 0.00005374
Iteration 141/1000 | Loss: 0.00004278
Iteration 142/1000 | Loss: 0.00004248
Iteration 143/1000 | Loss: 0.00024034
Iteration 144/1000 | Loss: 0.00042247
Iteration 145/1000 | Loss: 0.00007590
Iteration 146/1000 | Loss: 0.00023363
Iteration 147/1000 | Loss: 0.00013839
Iteration 148/1000 | Loss: 0.00005138
Iteration 149/1000 | Loss: 0.00004745
Iteration 150/1000 | Loss: 0.00008297
Iteration 151/1000 | Loss: 0.00006759
Iteration 152/1000 | Loss: 0.00004517
Iteration 153/1000 | Loss: 0.00032458
Iteration 154/1000 | Loss: 0.00012949
Iteration 155/1000 | Loss: 0.00018242
Iteration 156/1000 | Loss: 0.00045090
Iteration 157/1000 | Loss: 0.00024464
Iteration 158/1000 | Loss: 0.00005086
Iteration 159/1000 | Loss: 0.00007729
Iteration 160/1000 | Loss: 0.00006170
Iteration 161/1000 | Loss: 0.00005545
Iteration 162/1000 | Loss: 0.00006580
Iteration 163/1000 | Loss: 0.00005142
Iteration 164/1000 | Loss: 0.00012283
Iteration 165/1000 | Loss: 0.00010667
Iteration 166/1000 | Loss: 0.00036496
Iteration 167/1000 | Loss: 0.00020877
Iteration 168/1000 | Loss: 0.00011328
Iteration 169/1000 | Loss: 0.00014073
Iteration 170/1000 | Loss: 0.00004803
Iteration 171/1000 | Loss: 0.00011893
Iteration 172/1000 | Loss: 0.00011156
Iteration 173/1000 | Loss: 0.00016822
Iteration 174/1000 | Loss: 0.00025497
Iteration 175/1000 | Loss: 0.00010788
Iteration 176/1000 | Loss: 0.00006356
Iteration 177/1000 | Loss: 0.00005076
Iteration 178/1000 | Loss: 0.00021582
Iteration 179/1000 | Loss: 0.00017702
Iteration 180/1000 | Loss: 0.00004692
Iteration 181/1000 | Loss: 0.00005506
Iteration 182/1000 | Loss: 0.00010008
Iteration 183/1000 | Loss: 0.00005631
Iteration 184/1000 | Loss: 0.00005392
Iteration 185/1000 | Loss: 0.00006801
Iteration 186/1000 | Loss: 0.00005218
Iteration 187/1000 | Loss: 0.00008482
Iteration 188/1000 | Loss: 0.00007791
Iteration 189/1000 | Loss: 0.00005094
Iteration 190/1000 | Loss: 0.00004226
Iteration 191/1000 | Loss: 0.00004042
Iteration 192/1000 | Loss: 0.00003973
Iteration 193/1000 | Loss: 0.00004475
Iteration 194/1000 | Loss: 0.00004251
Iteration 195/1000 | Loss: 0.00003898
Iteration 196/1000 | Loss: 0.00007162
Iteration 197/1000 | Loss: 0.00007075
Iteration 198/1000 | Loss: 0.00012618
Iteration 199/1000 | Loss: 0.00008111
Iteration 200/1000 | Loss: 0.00010760
Iteration 201/1000 | Loss: 0.00004619
Iteration 202/1000 | Loss: 0.00004179
Iteration 203/1000 | Loss: 0.00003787
Iteration 204/1000 | Loss: 0.00004282
Iteration 205/1000 | Loss: 0.00003714
Iteration 206/1000 | Loss: 0.00005498
Iteration 207/1000 | Loss: 0.00016442
Iteration 208/1000 | Loss: 0.00010661
Iteration 209/1000 | Loss: 0.00005619
Iteration 210/1000 | Loss: 0.00009123
Iteration 211/1000 | Loss: 0.00007398
Iteration 212/1000 | Loss: 0.00007757
Iteration 213/1000 | Loss: 0.00005530
Iteration 214/1000 | Loss: 0.00004688
Iteration 215/1000 | Loss: 0.00008696
Iteration 216/1000 | Loss: 0.00008380
Iteration 217/1000 | Loss: 0.00007666
Iteration 218/1000 | Loss: 0.00011077
Iteration 219/1000 | Loss: 0.00004711
Iteration 220/1000 | Loss: 0.00009137
Iteration 221/1000 | Loss: 0.00007681
Iteration 222/1000 | Loss: 0.00003733
Iteration 223/1000 | Loss: 0.00004750
Iteration 224/1000 | Loss: 0.00005302
Iteration 225/1000 | Loss: 0.00005817
Iteration 226/1000 | Loss: 0.00005605
Iteration 227/1000 | Loss: 0.00006655
Iteration 228/1000 | Loss: 0.00005548
Iteration 229/1000 | Loss: 0.00007147
Iteration 230/1000 | Loss: 0.00007544
Iteration 231/1000 | Loss: 0.00005914
Iteration 232/1000 | Loss: 0.00005295
Iteration 233/1000 | Loss: 0.00004337
Iteration 234/1000 | Loss: 0.00007364
Iteration 235/1000 | Loss: 0.00007629
Iteration 236/1000 | Loss: 0.00005217
Iteration 237/1000 | Loss: 0.00006952
Iteration 238/1000 | Loss: 0.00006341
Iteration 239/1000 | Loss: 0.00008029
Iteration 240/1000 | Loss: 0.00006646
Iteration 241/1000 | Loss: 0.00007061
Iteration 242/1000 | Loss: 0.00007254
Iteration 243/1000 | Loss: 0.00008498
Iteration 244/1000 | Loss: 0.00006618
Iteration 245/1000 | Loss: 0.00006930
Iteration 246/1000 | Loss: 0.00004682
Iteration 247/1000 | Loss: 0.00004343
Iteration 248/1000 | Loss: 0.00003941
Iteration 249/1000 | Loss: 0.00005027
Iteration 250/1000 | Loss: 0.00004914
Iteration 251/1000 | Loss: 0.00003979
Iteration 252/1000 | Loss: 0.00003826
Iteration 253/1000 | Loss: 0.00003650
Iteration 254/1000 | Loss: 0.00003647
Iteration 255/1000 | Loss: 0.00006874
Iteration 256/1000 | Loss: 0.00005736
Iteration 257/1000 | Loss: 0.00005380
Iteration 258/1000 | Loss: 0.00004524
Iteration 259/1000 | Loss: 0.00003844
Iteration 260/1000 | Loss: 0.00006717
Iteration 261/1000 | Loss: 0.00003660
Iteration 262/1000 | Loss: 0.00005265
Iteration 263/1000 | Loss: 0.00004635
Iteration 264/1000 | Loss: 0.00005126
Iteration 265/1000 | Loss: 0.00004626
Iteration 266/1000 | Loss: 0.00004071
Iteration 267/1000 | Loss: 0.00005296
Iteration 268/1000 | Loss: 0.00004612
Iteration 269/1000 | Loss: 0.00005672
Iteration 270/1000 | Loss: 0.00004502
Iteration 271/1000 | Loss: 0.00005382
Iteration 272/1000 | Loss: 0.00009271
Iteration 273/1000 | Loss: 0.00009013
Iteration 274/1000 | Loss: 0.00005809
Iteration 275/1000 | Loss: 0.00005356
Iteration 276/1000 | Loss: 0.00005298
Iteration 277/1000 | Loss: 0.00005995
Iteration 278/1000 | Loss: 0.00006529
Iteration 279/1000 | Loss: 0.00005548
Iteration 280/1000 | Loss: 0.00004630
Iteration 281/1000 | Loss: 0.00007973
Iteration 282/1000 | Loss: 0.00005529
Iteration 283/1000 | Loss: 0.00005294
Iteration 284/1000 | Loss: 0.00005222
Iteration 285/1000 | Loss: 0.00007056
Iteration 286/1000 | Loss: 0.00006556
Iteration 287/1000 | Loss: 0.00004444
Iteration 288/1000 | Loss: 0.00004186
Iteration 289/1000 | Loss: 0.00005636
Iteration 290/1000 | Loss: 0.00003957
Iteration 291/1000 | Loss: 0.00003870
Iteration 292/1000 | Loss: 0.00007384
Iteration 293/1000 | Loss: 0.00004942
Iteration 294/1000 | Loss: 0.00007100
Iteration 295/1000 | Loss: 0.00008702
Iteration 296/1000 | Loss: 0.00006788
Iteration 297/1000 | Loss: 0.00005009
Iteration 298/1000 | Loss: 0.00005326
Iteration 299/1000 | Loss: 0.00008070
Iteration 300/1000 | Loss: 0.00005224
Iteration 301/1000 | Loss: 0.00004208
Iteration 302/1000 | Loss: 0.00004000
Iteration 303/1000 | Loss: 0.00007652
Iteration 304/1000 | Loss: 0.00003867
Iteration 305/1000 | Loss: 0.00003799
Iteration 306/1000 | Loss: 0.00005372
Iteration 307/1000 | Loss: 0.00003727
Iteration 308/1000 | Loss: 0.00003682
Iteration 309/1000 | Loss: 0.00007785
Iteration 310/1000 | Loss: 0.00003633
Iteration 311/1000 | Loss: 0.00003619
Iteration 312/1000 | Loss: 0.00003615
Iteration 313/1000 | Loss: 0.00003612
Iteration 314/1000 | Loss: 0.00003612
Iteration 315/1000 | Loss: 0.00003612
Iteration 316/1000 | Loss: 0.00003612
Iteration 317/1000 | Loss: 0.00003612
Iteration 318/1000 | Loss: 0.00003612
Iteration 319/1000 | Loss: 0.00003612
Iteration 320/1000 | Loss: 0.00003612
Iteration 321/1000 | Loss: 0.00003612
Iteration 322/1000 | Loss: 0.00003611
Iteration 323/1000 | Loss: 0.00003611
Iteration 324/1000 | Loss: 0.00003611
Iteration 325/1000 | Loss: 0.00003611
Iteration 326/1000 | Loss: 0.00003610
Iteration 327/1000 | Loss: 0.00003610
Iteration 328/1000 | Loss: 0.00003610
Iteration 329/1000 | Loss: 0.00003610
Iteration 330/1000 | Loss: 0.00003609
Iteration 331/1000 | Loss: 0.00003609
Iteration 332/1000 | Loss: 0.00003608
Iteration 333/1000 | Loss: 0.00003608
Iteration 334/1000 | Loss: 0.00003607
Iteration 335/1000 | Loss: 0.00003607
Iteration 336/1000 | Loss: 0.00003606
Iteration 337/1000 | Loss: 0.00003604
Iteration 338/1000 | Loss: 0.00003596
Iteration 339/1000 | Loss: 0.00003596
Iteration 340/1000 | Loss: 0.00003596
Iteration 341/1000 | Loss: 0.00003596
Iteration 342/1000 | Loss: 0.00003595
Iteration 343/1000 | Loss: 0.00003595
Iteration 344/1000 | Loss: 0.00003595
Iteration 345/1000 | Loss: 0.00003594
Iteration 346/1000 | Loss: 0.00003594
Iteration 347/1000 | Loss: 0.00003594
Iteration 348/1000 | Loss: 0.00003593
Iteration 349/1000 | Loss: 0.00003593
Iteration 350/1000 | Loss: 0.00003593
Iteration 351/1000 | Loss: 0.00003593
Iteration 352/1000 | Loss: 0.00003593
Iteration 353/1000 | Loss: 0.00003592
Iteration 354/1000 | Loss: 0.00003589
Iteration 355/1000 | Loss: 0.00003589
Iteration 356/1000 | Loss: 0.00003589
Iteration 357/1000 | Loss: 0.00003589
Iteration 358/1000 | Loss: 0.00003589
Iteration 359/1000 | Loss: 0.00003589
Iteration 360/1000 | Loss: 0.00003589
Iteration 361/1000 | Loss: 0.00003589
Iteration 362/1000 | Loss: 0.00003589
Iteration 363/1000 | Loss: 0.00003589
Iteration 364/1000 | Loss: 0.00003588
Iteration 365/1000 | Loss: 0.00003588
Iteration 366/1000 | Loss: 0.00003588
Iteration 367/1000 | Loss: 0.00003588
Iteration 368/1000 | Loss: 0.00003588
Iteration 369/1000 | Loss: 0.00003588
Iteration 370/1000 | Loss: 0.00003587
Iteration 371/1000 | Loss: 0.00003587
Iteration 372/1000 | Loss: 0.00003587
Iteration 373/1000 | Loss: 0.00003587
Iteration 374/1000 | Loss: 0.00003587
Iteration 375/1000 | Loss: 0.00003587
Iteration 376/1000 | Loss: 0.00003587
Iteration 377/1000 | Loss: 0.00003587
Iteration 378/1000 | Loss: 0.00003587
Iteration 379/1000 | Loss: 0.00003586
Iteration 380/1000 | Loss: 0.00003586
Iteration 381/1000 | Loss: 0.00003586
Iteration 382/1000 | Loss: 0.00003586
Iteration 383/1000 | Loss: 0.00003586
Iteration 384/1000 | Loss: 0.00003586
Iteration 385/1000 | Loss: 0.00003586
Iteration 386/1000 | Loss: 0.00003586
Iteration 387/1000 | Loss: 0.00003586
Iteration 388/1000 | Loss: 0.00003586
Iteration 389/1000 | Loss: 0.00003586
Iteration 390/1000 | Loss: 0.00003586
Iteration 391/1000 | Loss: 0.00003586
Iteration 392/1000 | Loss: 0.00003585
Iteration 393/1000 | Loss: 0.00003585
Iteration 394/1000 | Loss: 0.00003585
Iteration 395/1000 | Loss: 0.00003585
Iteration 396/1000 | Loss: 0.00003585
Iteration 397/1000 | Loss: 0.00003585
Iteration 398/1000 | Loss: 0.00003585
Iteration 399/1000 | Loss: 0.00003585
Iteration 400/1000 | Loss: 0.00003585
Iteration 401/1000 | Loss: 0.00003585
Iteration 402/1000 | Loss: 0.00003585
Iteration 403/1000 | Loss: 0.00003585
Iteration 404/1000 | Loss: 0.00003585
Iteration 405/1000 | Loss: 0.00003585
Iteration 406/1000 | Loss: 0.00003585
Iteration 407/1000 | Loss: 0.00003585
Iteration 408/1000 | Loss: 0.00003585
Iteration 409/1000 | Loss: 0.00003585
Iteration 410/1000 | Loss: 0.00003585
Iteration 411/1000 | Loss: 0.00003585
Iteration 412/1000 | Loss: 0.00003585
Iteration 413/1000 | Loss: 0.00003585
Iteration 414/1000 | Loss: 0.00003585
Iteration 415/1000 | Loss: 0.00003585
Iteration 416/1000 | Loss: 0.00003585
Iteration 417/1000 | Loss: 0.00003585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 417. Stopping optimization.
Last 5 losses: [3.584520163713023e-05, 3.584520163713023e-05, 3.584520163713023e-05, 3.584520163713023e-05, 3.584520163713023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.584520163713023e-05

Optimization complete. Final v2v error: 4.717613220214844 mm

Highest mean error: 6.3851399421691895 mm for frame 125

Lowest mean error: 3.771691083908081 mm for frame 0

Saving results

Total time: 554.9552869796753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958628
Iteration 2/25 | Loss: 0.00216032
Iteration 3/25 | Loss: 0.00158361
Iteration 4/25 | Loss: 0.00152698
Iteration 5/25 | Loss: 0.00144975
Iteration 6/25 | Loss: 0.00141988
Iteration 7/25 | Loss: 0.00134598
Iteration 8/25 | Loss: 0.00130074
Iteration 9/25 | Loss: 0.00128553
Iteration 10/25 | Loss: 0.00128902
Iteration 11/25 | Loss: 0.00128498
Iteration 12/25 | Loss: 0.00128393
Iteration 13/25 | Loss: 0.00127650
Iteration 14/25 | Loss: 0.00127504
Iteration 15/25 | Loss: 0.00127561
Iteration 16/25 | Loss: 0.00127516
Iteration 17/25 | Loss: 0.00127518
Iteration 18/25 | Loss: 0.00127484
Iteration 19/25 | Loss: 0.00127494
Iteration 20/25 | Loss: 0.00127378
Iteration 21/25 | Loss: 0.00127484
Iteration 22/25 | Loss: 0.00127436
Iteration 23/25 | Loss: 0.00127407
Iteration 24/25 | Loss: 0.00127463
Iteration 25/25 | Loss: 0.00127486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30970240
Iteration 2/25 | Loss: 0.00073255
Iteration 3/25 | Loss: 0.00073255
Iteration 4/25 | Loss: 0.00073255
Iteration 5/25 | Loss: 0.00073255
Iteration 6/25 | Loss: 0.00073255
Iteration 7/25 | Loss: 0.00073255
Iteration 8/25 | Loss: 0.00073255
Iteration 9/25 | Loss: 0.00073255
Iteration 10/25 | Loss: 0.00073255
Iteration 11/25 | Loss: 0.00073255
Iteration 12/25 | Loss: 0.00073255
Iteration 13/25 | Loss: 0.00073255
Iteration 14/25 | Loss: 0.00073255
Iteration 15/25 | Loss: 0.00073255
Iteration 16/25 | Loss: 0.00073255
Iteration 17/25 | Loss: 0.00073255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007325486512854695, 0.0007325486512854695, 0.0007325486512854695, 0.0007325486512854695, 0.0007325486512854695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007325486512854695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073255
Iteration 2/1000 | Loss: 0.00004668
Iteration 3/1000 | Loss: 0.00004981
Iteration 4/1000 | Loss: 0.00002760
Iteration 5/1000 | Loss: 0.00005179
Iteration 6/1000 | Loss: 0.00003609
Iteration 7/1000 | Loss: 0.00003947
Iteration 8/1000 | Loss: 0.00004573
Iteration 9/1000 | Loss: 0.00004643
Iteration 10/1000 | Loss: 0.00004461
Iteration 11/1000 | Loss: 0.00004023
Iteration 12/1000 | Loss: 0.00005193
Iteration 13/1000 | Loss: 0.00004656
Iteration 14/1000 | Loss: 0.00003162
Iteration 15/1000 | Loss: 0.00003843
Iteration 16/1000 | Loss: 0.00003646
Iteration 17/1000 | Loss: 0.00004616
Iteration 18/1000 | Loss: 0.00003412
Iteration 19/1000 | Loss: 0.00004517
Iteration 20/1000 | Loss: 0.00004537
Iteration 21/1000 | Loss: 0.00004452
Iteration 22/1000 | Loss: 0.00005474
Iteration 23/1000 | Loss: 0.00005407
Iteration 24/1000 | Loss: 0.00005113
Iteration 25/1000 | Loss: 0.00005386
Iteration 26/1000 | Loss: 0.00005388
Iteration 27/1000 | Loss: 0.00005327
Iteration 28/1000 | Loss: 0.00002798
Iteration 29/1000 | Loss: 0.00002391
Iteration 30/1000 | Loss: 0.00002173
Iteration 31/1000 | Loss: 0.00002098
Iteration 32/1000 | Loss: 0.00002068
Iteration 33/1000 | Loss: 0.00002058
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002042
Iteration 36/1000 | Loss: 0.00002028
Iteration 37/1000 | Loss: 0.00002028
Iteration 38/1000 | Loss: 0.00002027
Iteration 39/1000 | Loss: 0.00002025
Iteration 40/1000 | Loss: 0.00002024
Iteration 41/1000 | Loss: 0.00002015
Iteration 42/1000 | Loss: 0.00002013
Iteration 43/1000 | Loss: 0.00002008
Iteration 44/1000 | Loss: 0.00002004
Iteration 45/1000 | Loss: 0.00002004
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00002001
Iteration 48/1000 | Loss: 0.00002000
Iteration 49/1000 | Loss: 0.00002000
Iteration 50/1000 | Loss: 0.00001999
Iteration 51/1000 | Loss: 0.00001999
Iteration 52/1000 | Loss: 0.00001997
Iteration 53/1000 | Loss: 0.00001997
Iteration 54/1000 | Loss: 0.00001997
Iteration 55/1000 | Loss: 0.00001996
Iteration 56/1000 | Loss: 0.00001996
Iteration 57/1000 | Loss: 0.00001996
Iteration 58/1000 | Loss: 0.00001996
Iteration 59/1000 | Loss: 0.00001996
Iteration 60/1000 | Loss: 0.00001995
Iteration 61/1000 | Loss: 0.00001995
Iteration 62/1000 | Loss: 0.00001995
Iteration 63/1000 | Loss: 0.00001994
Iteration 64/1000 | Loss: 0.00001994
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001993
Iteration 67/1000 | Loss: 0.00001993
Iteration 68/1000 | Loss: 0.00001993
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001992
Iteration 72/1000 | Loss: 0.00001991
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001990
Iteration 75/1000 | Loss: 0.00001990
Iteration 76/1000 | Loss: 0.00001990
Iteration 77/1000 | Loss: 0.00001990
Iteration 78/1000 | Loss: 0.00001990
Iteration 79/1000 | Loss: 0.00001989
Iteration 80/1000 | Loss: 0.00001989
Iteration 81/1000 | Loss: 0.00001989
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001989
Iteration 84/1000 | Loss: 0.00001989
Iteration 85/1000 | Loss: 0.00001989
Iteration 86/1000 | Loss: 0.00001989
Iteration 87/1000 | Loss: 0.00001989
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001988
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001988
Iteration 93/1000 | Loss: 0.00001988
Iteration 94/1000 | Loss: 0.00001988
Iteration 95/1000 | Loss: 0.00001987
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001987
Iteration 98/1000 | Loss: 0.00001986
Iteration 99/1000 | Loss: 0.00001986
Iteration 100/1000 | Loss: 0.00001986
Iteration 101/1000 | Loss: 0.00001986
Iteration 102/1000 | Loss: 0.00001986
Iteration 103/1000 | Loss: 0.00001986
Iteration 104/1000 | Loss: 0.00001986
Iteration 105/1000 | Loss: 0.00001986
Iteration 106/1000 | Loss: 0.00001986
Iteration 107/1000 | Loss: 0.00001986
Iteration 108/1000 | Loss: 0.00001986
Iteration 109/1000 | Loss: 0.00001986
Iteration 110/1000 | Loss: 0.00001986
Iteration 111/1000 | Loss: 0.00001986
Iteration 112/1000 | Loss: 0.00001986
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001986
Iteration 116/1000 | Loss: 0.00001986
Iteration 117/1000 | Loss: 0.00001986
Iteration 118/1000 | Loss: 0.00001986
Iteration 119/1000 | Loss: 0.00001986
Iteration 120/1000 | Loss: 0.00001986
Iteration 121/1000 | Loss: 0.00001986
Iteration 122/1000 | Loss: 0.00001986
Iteration 123/1000 | Loss: 0.00001986
Iteration 124/1000 | Loss: 0.00001986
Iteration 125/1000 | Loss: 0.00001986
Iteration 126/1000 | Loss: 0.00001986
Iteration 127/1000 | Loss: 0.00001986
Iteration 128/1000 | Loss: 0.00001986
Iteration 129/1000 | Loss: 0.00001986
Iteration 130/1000 | Loss: 0.00001986
Iteration 131/1000 | Loss: 0.00001986
Iteration 132/1000 | Loss: 0.00001986
Iteration 133/1000 | Loss: 0.00001986
Iteration 134/1000 | Loss: 0.00001986
Iteration 135/1000 | Loss: 0.00001986
Iteration 136/1000 | Loss: 0.00001986
Iteration 137/1000 | Loss: 0.00001986
Iteration 138/1000 | Loss: 0.00001986
Iteration 139/1000 | Loss: 0.00001986
Iteration 140/1000 | Loss: 0.00001986
Iteration 141/1000 | Loss: 0.00001986
Iteration 142/1000 | Loss: 0.00001986
Iteration 143/1000 | Loss: 0.00001986
Iteration 144/1000 | Loss: 0.00001986
Iteration 145/1000 | Loss: 0.00001986
Iteration 146/1000 | Loss: 0.00001986
Iteration 147/1000 | Loss: 0.00001986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.985771450563334e-05, 1.985771450563334e-05, 1.985771450563334e-05, 1.985771450563334e-05, 1.985771450563334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.985771450563334e-05

Optimization complete. Final v2v error: 3.7383925914764404 mm

Highest mean error: 4.205991744995117 mm for frame 3

Lowest mean error: 3.457779884338379 mm for frame 55

Saving results

Total time: 101.80855202674866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997956
Iteration 2/25 | Loss: 0.00997956
Iteration 3/25 | Loss: 0.00997956
Iteration 4/25 | Loss: 0.00997956
Iteration 5/25 | Loss: 0.00997955
Iteration 6/25 | Loss: 0.00997955
Iteration 7/25 | Loss: 0.00997955
Iteration 8/25 | Loss: 0.00997955
Iteration 9/25 | Loss: 0.00997954
Iteration 10/25 | Loss: 0.00301983
Iteration 11/25 | Loss: 0.00203865
Iteration 12/25 | Loss: 0.00190475
Iteration 13/25 | Loss: 0.00173623
Iteration 14/25 | Loss: 0.00169058
Iteration 15/25 | Loss: 0.00165850
Iteration 16/25 | Loss: 0.00160162
Iteration 17/25 | Loss: 0.00152303
Iteration 18/25 | Loss: 0.00150412
Iteration 19/25 | Loss: 0.00148144
Iteration 20/25 | Loss: 0.00149802
Iteration 21/25 | Loss: 0.00146982
Iteration 22/25 | Loss: 0.00143688
Iteration 23/25 | Loss: 0.00141865
Iteration 24/25 | Loss: 0.00141021
Iteration 25/25 | Loss: 0.00140675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35661614
Iteration 2/25 | Loss: 0.00443307
Iteration 3/25 | Loss: 0.00361231
Iteration 4/25 | Loss: 0.00361228
Iteration 5/25 | Loss: 0.00361228
Iteration 6/25 | Loss: 0.00361228
Iteration 7/25 | Loss: 0.00361228
Iteration 8/25 | Loss: 0.00361228
Iteration 9/25 | Loss: 0.00361228
Iteration 10/25 | Loss: 0.00361228
Iteration 11/25 | Loss: 0.00361228
Iteration 12/25 | Loss: 0.00361228
Iteration 13/25 | Loss: 0.00361228
Iteration 14/25 | Loss: 0.00361228
Iteration 15/25 | Loss: 0.00361228
Iteration 16/25 | Loss: 0.00361228
Iteration 17/25 | Loss: 0.00361228
Iteration 18/25 | Loss: 0.00361228
Iteration 19/25 | Loss: 0.00361228
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0036122784949839115, 0.0036122784949839115, 0.0036122784949839115, 0.0036122784949839115, 0.0036122784949839115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036122784949839115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00361228
Iteration 2/1000 | Loss: 0.00049363
Iteration 3/1000 | Loss: 0.00149520
Iteration 4/1000 | Loss: 0.00064679
Iteration 5/1000 | Loss: 0.00045787
Iteration 6/1000 | Loss: 0.00019481
Iteration 7/1000 | Loss: 0.00017866
Iteration 8/1000 | Loss: 0.00051805
Iteration 9/1000 | Loss: 0.00064609
Iteration 10/1000 | Loss: 0.00018249
Iteration 11/1000 | Loss: 0.00017357
Iteration 12/1000 | Loss: 0.00017586
Iteration 13/1000 | Loss: 0.00033589
Iteration 14/1000 | Loss: 0.00031698
Iteration 15/1000 | Loss: 0.00015839
Iteration 16/1000 | Loss: 0.00051756
Iteration 17/1000 | Loss: 0.00052618
Iteration 18/1000 | Loss: 0.00013399
Iteration 19/1000 | Loss: 0.00013219
Iteration 20/1000 | Loss: 0.00055144
Iteration 21/1000 | Loss: 0.00014006
Iteration 22/1000 | Loss: 0.00012882
Iteration 23/1000 | Loss: 0.00013169
Iteration 24/1000 | Loss: 0.00012705
Iteration 25/1000 | Loss: 0.00014677
Iteration 26/1000 | Loss: 0.00013768
Iteration 27/1000 | Loss: 0.00013454
Iteration 28/1000 | Loss: 0.00021598
Iteration 29/1000 | Loss: 0.00013557
Iteration 30/1000 | Loss: 0.00054506
Iteration 31/1000 | Loss: 0.00050899
Iteration 32/1000 | Loss: 0.00036221
Iteration 33/1000 | Loss: 0.00035355
Iteration 34/1000 | Loss: 0.00026867
Iteration 35/1000 | Loss: 0.00012958
Iteration 36/1000 | Loss: 0.00012828
Iteration 37/1000 | Loss: 0.00011283
Iteration 38/1000 | Loss: 0.00013100
Iteration 39/1000 | Loss: 0.00011775
Iteration 40/1000 | Loss: 0.00011679
Iteration 41/1000 | Loss: 0.00011565
Iteration 42/1000 | Loss: 0.00013578
Iteration 43/1000 | Loss: 0.00013546
Iteration 44/1000 | Loss: 0.00014684
Iteration 45/1000 | Loss: 0.00011950
Iteration 46/1000 | Loss: 0.00011487
Iteration 47/1000 | Loss: 0.00012673
Iteration 48/1000 | Loss: 0.00013052
Iteration 49/1000 | Loss: 0.00011032
Iteration 50/1000 | Loss: 0.00012979
Iteration 51/1000 | Loss: 0.00012755
Iteration 52/1000 | Loss: 0.00014605
Iteration 53/1000 | Loss: 0.00012848
Iteration 54/1000 | Loss: 0.00014430
Iteration 55/1000 | Loss: 0.00012322
Iteration 56/1000 | Loss: 0.00012974
Iteration 57/1000 | Loss: 0.00018589
Iteration 58/1000 | Loss: 0.00015122
Iteration 59/1000 | Loss: 0.00013251
Iteration 60/1000 | Loss: 0.00015738
Iteration 61/1000 | Loss: 0.00021785
Iteration 62/1000 | Loss: 0.00012984
Iteration 63/1000 | Loss: 0.00013018
Iteration 64/1000 | Loss: 0.00012571
Iteration 65/1000 | Loss: 0.00013304
Iteration 66/1000 | Loss: 0.00013328
Iteration 67/1000 | Loss: 0.00011055
Iteration 68/1000 | Loss: 0.00012743
Iteration 69/1000 | Loss: 0.00013245
Iteration 70/1000 | Loss: 0.00012525
Iteration 71/1000 | Loss: 0.00011921
Iteration 72/1000 | Loss: 0.00012434
Iteration 73/1000 | Loss: 0.00016115
Iteration 74/1000 | Loss: 0.00012410
Iteration 75/1000 | Loss: 0.00012028
Iteration 76/1000 | Loss: 0.00010660
Iteration 77/1000 | Loss: 0.00011682
Iteration 78/1000 | Loss: 0.00011968
Iteration 79/1000 | Loss: 0.00012156
Iteration 80/1000 | Loss: 0.00013008
Iteration 81/1000 | Loss: 0.00012374
Iteration 82/1000 | Loss: 0.00013514
Iteration 83/1000 | Loss: 0.00012037
Iteration 84/1000 | Loss: 0.00013376
Iteration 85/1000 | Loss: 0.00014409
Iteration 86/1000 | Loss: 0.00012220
Iteration 87/1000 | Loss: 0.00012353
Iteration 88/1000 | Loss: 0.00012295
Iteration 89/1000 | Loss: 0.00012039
Iteration 90/1000 | Loss: 0.00011778
Iteration 91/1000 | Loss: 0.00011218
Iteration 92/1000 | Loss: 0.00012862
Iteration 93/1000 | Loss: 0.00011063
Iteration 94/1000 | Loss: 0.00011689
Iteration 95/1000 | Loss: 0.00011216
Iteration 96/1000 | Loss: 0.00011549
Iteration 97/1000 | Loss: 0.00053820
Iteration 98/1000 | Loss: 0.00011520
Iteration 99/1000 | Loss: 0.00053554
Iteration 100/1000 | Loss: 0.00461861
Iteration 101/1000 | Loss: 0.01024223
Iteration 102/1000 | Loss: 0.00445626
Iteration 103/1000 | Loss: 0.00222508
Iteration 104/1000 | Loss: 0.00150856
Iteration 105/1000 | Loss: 0.00088758
Iteration 106/1000 | Loss: 0.00139021
Iteration 107/1000 | Loss: 0.00017551
Iteration 108/1000 | Loss: 0.00043754
Iteration 109/1000 | Loss: 0.00018646
Iteration 110/1000 | Loss: 0.00023382
Iteration 111/1000 | Loss: 0.00041950
Iteration 112/1000 | Loss: 0.00015053
Iteration 113/1000 | Loss: 0.00062075
Iteration 114/1000 | Loss: 0.00012964
Iteration 115/1000 | Loss: 0.00006693
Iteration 116/1000 | Loss: 0.00027547
Iteration 117/1000 | Loss: 0.00011460
Iteration 118/1000 | Loss: 0.00005402
Iteration 119/1000 | Loss: 0.00005267
Iteration 120/1000 | Loss: 0.00008932
Iteration 121/1000 | Loss: 0.00014236
Iteration 122/1000 | Loss: 0.00027132
Iteration 123/1000 | Loss: 0.00008300
Iteration 124/1000 | Loss: 0.00025043
Iteration 125/1000 | Loss: 0.00005418
Iteration 126/1000 | Loss: 0.00019673
Iteration 127/1000 | Loss: 0.00018712
Iteration 128/1000 | Loss: 0.00033080
Iteration 129/1000 | Loss: 0.00027626
Iteration 130/1000 | Loss: 0.00013849
Iteration 131/1000 | Loss: 0.00006430
Iteration 132/1000 | Loss: 0.00050969
Iteration 133/1000 | Loss: 0.00030873
Iteration 134/1000 | Loss: 0.00005063
Iteration 135/1000 | Loss: 0.00003656
Iteration 136/1000 | Loss: 0.00004803
Iteration 137/1000 | Loss: 0.00010707
Iteration 138/1000 | Loss: 0.00003130
Iteration 139/1000 | Loss: 0.00003893
Iteration 140/1000 | Loss: 0.00021857
Iteration 141/1000 | Loss: 0.00042569
Iteration 142/1000 | Loss: 0.00009683
Iteration 143/1000 | Loss: 0.00002952
Iteration 144/1000 | Loss: 0.00003041
Iteration 145/1000 | Loss: 0.00022862
Iteration 146/1000 | Loss: 0.00040626
Iteration 147/1000 | Loss: 0.00031397
Iteration 148/1000 | Loss: 0.00004568
Iteration 149/1000 | Loss: 0.00003184
Iteration 150/1000 | Loss: 0.00002626
Iteration 151/1000 | Loss: 0.00002429
Iteration 152/1000 | Loss: 0.00002282
Iteration 153/1000 | Loss: 0.00002620
Iteration 154/1000 | Loss: 0.00002102
Iteration 155/1000 | Loss: 0.00002967
Iteration 156/1000 | Loss: 0.00002011
Iteration 157/1000 | Loss: 0.00001973
Iteration 158/1000 | Loss: 0.00003033
Iteration 159/1000 | Loss: 0.00001927
Iteration 160/1000 | Loss: 0.00001982
Iteration 161/1000 | Loss: 0.00014726
Iteration 162/1000 | Loss: 0.00008938
Iteration 163/1000 | Loss: 0.00002860
Iteration 164/1000 | Loss: 0.00002270
Iteration 165/1000 | Loss: 0.00014341
Iteration 166/1000 | Loss: 0.00008475
Iteration 167/1000 | Loss: 0.00016088
Iteration 168/1000 | Loss: 0.00007734
Iteration 169/1000 | Loss: 0.00014920
Iteration 170/1000 | Loss: 0.00005998
Iteration 171/1000 | Loss: 0.00011390
Iteration 172/1000 | Loss: 0.00011713
Iteration 173/1000 | Loss: 0.00010374
Iteration 174/1000 | Loss: 0.00002005
Iteration 175/1000 | Loss: 0.00002125
Iteration 176/1000 | Loss: 0.00001839
Iteration 177/1000 | Loss: 0.00002058
Iteration 178/1000 | Loss: 0.00002042
Iteration 179/1000 | Loss: 0.00001767
Iteration 180/1000 | Loss: 0.00001863
Iteration 181/1000 | Loss: 0.00001824
Iteration 182/1000 | Loss: 0.00002279
Iteration 183/1000 | Loss: 0.00002659
Iteration 184/1000 | Loss: 0.00002657
Iteration 185/1000 | Loss: 0.00006550
Iteration 186/1000 | Loss: 0.00002376
Iteration 187/1000 | Loss: 0.00002370
Iteration 188/1000 | Loss: 0.00001916
Iteration 189/1000 | Loss: 0.00002263
Iteration 190/1000 | Loss: 0.00003056
Iteration 191/1000 | Loss: 0.00002554
Iteration 192/1000 | Loss: 0.00002554
Iteration 193/1000 | Loss: 0.00002699
Iteration 194/1000 | Loss: 0.00001846
Iteration 195/1000 | Loss: 0.00002138
Iteration 196/1000 | Loss: 0.00001804
Iteration 197/1000 | Loss: 0.00022226
Iteration 198/1000 | Loss: 0.00015410
Iteration 199/1000 | Loss: 0.00013424
Iteration 200/1000 | Loss: 0.00006960
Iteration 201/1000 | Loss: 0.00003168
Iteration 202/1000 | Loss: 0.00002710
Iteration 203/1000 | Loss: 0.00003257
Iteration 204/1000 | Loss: 0.00007941
Iteration 205/1000 | Loss: 0.00021469
Iteration 206/1000 | Loss: 0.00034284
Iteration 207/1000 | Loss: 0.00011858
Iteration 208/1000 | Loss: 0.00002704
Iteration 209/1000 | Loss: 0.00002320
Iteration 210/1000 | Loss: 0.00002046
Iteration 211/1000 | Loss: 0.00001907
Iteration 212/1000 | Loss: 0.00001780
Iteration 213/1000 | Loss: 0.00002608
Iteration 214/1000 | Loss: 0.00001794
Iteration 215/1000 | Loss: 0.00001866
Iteration 216/1000 | Loss: 0.00001567
Iteration 217/1000 | Loss: 0.00001558
Iteration 218/1000 | Loss: 0.00001557
Iteration 219/1000 | Loss: 0.00001547
Iteration 220/1000 | Loss: 0.00001541
Iteration 221/1000 | Loss: 0.00001541
Iteration 222/1000 | Loss: 0.00001541
Iteration 223/1000 | Loss: 0.00001541
Iteration 224/1000 | Loss: 0.00001541
Iteration 225/1000 | Loss: 0.00001540
Iteration 226/1000 | Loss: 0.00001538
Iteration 227/1000 | Loss: 0.00001536
Iteration 228/1000 | Loss: 0.00001535
Iteration 229/1000 | Loss: 0.00001534
Iteration 230/1000 | Loss: 0.00001533
Iteration 231/1000 | Loss: 0.00001533
Iteration 232/1000 | Loss: 0.00001532
Iteration 233/1000 | Loss: 0.00001532
Iteration 234/1000 | Loss: 0.00001532
Iteration 235/1000 | Loss: 0.00001530
Iteration 236/1000 | Loss: 0.00001529
Iteration 237/1000 | Loss: 0.00001529
Iteration 238/1000 | Loss: 0.00001529
Iteration 239/1000 | Loss: 0.00001528
Iteration 240/1000 | Loss: 0.00001528
Iteration 241/1000 | Loss: 0.00001528
Iteration 242/1000 | Loss: 0.00001527
Iteration 243/1000 | Loss: 0.00001527
Iteration 244/1000 | Loss: 0.00001527
Iteration 245/1000 | Loss: 0.00001526
Iteration 246/1000 | Loss: 0.00001526
Iteration 247/1000 | Loss: 0.00001526
Iteration 248/1000 | Loss: 0.00001525
Iteration 249/1000 | Loss: 0.00001525
Iteration 250/1000 | Loss: 0.00001524
Iteration 251/1000 | Loss: 0.00001524
Iteration 252/1000 | Loss: 0.00001524
Iteration 253/1000 | Loss: 0.00001524
Iteration 254/1000 | Loss: 0.00001524
Iteration 255/1000 | Loss: 0.00001524
Iteration 256/1000 | Loss: 0.00001524
Iteration 257/1000 | Loss: 0.00001524
Iteration 258/1000 | Loss: 0.00001523
Iteration 259/1000 | Loss: 0.00001523
Iteration 260/1000 | Loss: 0.00001523
Iteration 261/1000 | Loss: 0.00001523
Iteration 262/1000 | Loss: 0.00001523
Iteration 263/1000 | Loss: 0.00001523
Iteration 264/1000 | Loss: 0.00001523
Iteration 265/1000 | Loss: 0.00001523
Iteration 266/1000 | Loss: 0.00001523
Iteration 267/1000 | Loss: 0.00001522
Iteration 268/1000 | Loss: 0.00001522
Iteration 269/1000 | Loss: 0.00001522
Iteration 270/1000 | Loss: 0.00001522
Iteration 271/1000 | Loss: 0.00001522
Iteration 272/1000 | Loss: 0.00001522
Iteration 273/1000 | Loss: 0.00001522
Iteration 274/1000 | Loss: 0.00001522
Iteration 275/1000 | Loss: 0.00001522
Iteration 276/1000 | Loss: 0.00001522
Iteration 277/1000 | Loss: 0.00001522
Iteration 278/1000 | Loss: 0.00001522
Iteration 279/1000 | Loss: 0.00001521
Iteration 280/1000 | Loss: 0.00001521
Iteration 281/1000 | Loss: 0.00001521
Iteration 282/1000 | Loss: 0.00001521
Iteration 283/1000 | Loss: 0.00001521
Iteration 284/1000 | Loss: 0.00001521
Iteration 285/1000 | Loss: 0.00001521
Iteration 286/1000 | Loss: 0.00001521
Iteration 287/1000 | Loss: 0.00001521
Iteration 288/1000 | Loss: 0.00001521
Iteration 289/1000 | Loss: 0.00001521
Iteration 290/1000 | Loss: 0.00001521
Iteration 291/1000 | Loss: 0.00001521
Iteration 292/1000 | Loss: 0.00001521
Iteration 293/1000 | Loss: 0.00001521
Iteration 294/1000 | Loss: 0.00001521
Iteration 295/1000 | Loss: 0.00001521
Iteration 296/1000 | Loss: 0.00001521
Iteration 297/1000 | Loss: 0.00001521
Iteration 298/1000 | Loss: 0.00001521
Iteration 299/1000 | Loss: 0.00001520
Iteration 300/1000 | Loss: 0.00001520
Iteration 301/1000 | Loss: 0.00001520
Iteration 302/1000 | Loss: 0.00001520
Iteration 303/1000 | Loss: 0.00001520
Iteration 304/1000 | Loss: 0.00001520
Iteration 305/1000 | Loss: 0.00001520
Iteration 306/1000 | Loss: 0.00001520
Iteration 307/1000 | Loss: 0.00001520
Iteration 308/1000 | Loss: 0.00001520
Iteration 309/1000 | Loss: 0.00001520
Iteration 310/1000 | Loss: 0.00001520
Iteration 311/1000 | Loss: 0.00001520
Iteration 312/1000 | Loss: 0.00001520
Iteration 313/1000 | Loss: 0.00001520
Iteration 314/1000 | Loss: 0.00001520
Iteration 315/1000 | Loss: 0.00001520
Iteration 316/1000 | Loss: 0.00001520
Iteration 317/1000 | Loss: 0.00001520
Iteration 318/1000 | Loss: 0.00001520
Iteration 319/1000 | Loss: 0.00001519
Iteration 320/1000 | Loss: 0.00001519
Iteration 321/1000 | Loss: 0.00001519
Iteration 322/1000 | Loss: 0.00001519
Iteration 323/1000 | Loss: 0.00001519
Iteration 324/1000 | Loss: 0.00001519
Iteration 325/1000 | Loss: 0.00001519
Iteration 326/1000 | Loss: 0.00001519
Iteration 327/1000 | Loss: 0.00001519
Iteration 328/1000 | Loss: 0.00001519
Iteration 329/1000 | Loss: 0.00001519
Iteration 330/1000 | Loss: 0.00001519
Iteration 331/1000 | Loss: 0.00001519
Iteration 332/1000 | Loss: 0.00001519
Iteration 333/1000 | Loss: 0.00001519
Iteration 334/1000 | Loss: 0.00001519
Iteration 335/1000 | Loss: 0.00001519
Iteration 336/1000 | Loss: 0.00001519
Iteration 337/1000 | Loss: 0.00001519
Iteration 338/1000 | Loss: 0.00001519
Iteration 339/1000 | Loss: 0.00001519
Iteration 340/1000 | Loss: 0.00001519
Iteration 341/1000 | Loss: 0.00001519
Iteration 342/1000 | Loss: 0.00001519
Iteration 343/1000 | Loss: 0.00001519
Iteration 344/1000 | Loss: 0.00001519
Iteration 345/1000 | Loss: 0.00001519
Iteration 346/1000 | Loss: 0.00001519
Iteration 347/1000 | Loss: 0.00001519
Iteration 348/1000 | Loss: 0.00001519
Iteration 349/1000 | Loss: 0.00001519
Iteration 350/1000 | Loss: 0.00001519
Iteration 351/1000 | Loss: 0.00001519
Iteration 352/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 352. Stopping optimization.
Last 5 losses: [1.518755016149953e-05, 1.518755016149953e-05, 1.518755016149953e-05, 1.518755016149953e-05, 1.518755016149953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.518755016149953e-05

Optimization complete. Final v2v error: 3.0031795501708984 mm

Highest mean error: 10.470745086669922 mm for frame 65

Lowest mean error: 2.5068187713623047 mm for frame 126

Saving results

Total time: 397.54750967025757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807511
Iteration 2/25 | Loss: 0.00123275
Iteration 3/25 | Loss: 0.00113828
Iteration 4/25 | Loss: 0.00112840
Iteration 5/25 | Loss: 0.00112623
Iteration 6/25 | Loss: 0.00112607
Iteration 7/25 | Loss: 0.00112607
Iteration 8/25 | Loss: 0.00112607
Iteration 9/25 | Loss: 0.00112607
Iteration 10/25 | Loss: 0.00112607
Iteration 11/25 | Loss: 0.00112607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011260658502578735, 0.0011260658502578735, 0.0011260658502578735, 0.0011260658502578735, 0.0011260658502578735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011260658502578735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36166072
Iteration 2/25 | Loss: 0.00082690
Iteration 3/25 | Loss: 0.00082690
Iteration 4/25 | Loss: 0.00082690
Iteration 5/25 | Loss: 0.00082690
Iteration 6/25 | Loss: 0.00082690
Iteration 7/25 | Loss: 0.00082690
Iteration 8/25 | Loss: 0.00082690
Iteration 9/25 | Loss: 0.00082690
Iteration 10/25 | Loss: 0.00082690
Iteration 11/25 | Loss: 0.00082690
Iteration 12/25 | Loss: 0.00082690
Iteration 13/25 | Loss: 0.00082690
Iteration 14/25 | Loss: 0.00082690
Iteration 15/25 | Loss: 0.00082690
Iteration 16/25 | Loss: 0.00082690
Iteration 17/25 | Loss: 0.00082690
Iteration 18/25 | Loss: 0.00082690
Iteration 19/25 | Loss: 0.00082690
Iteration 20/25 | Loss: 0.00082690
Iteration 21/25 | Loss: 0.00082690
Iteration 22/25 | Loss: 0.00082690
Iteration 23/25 | Loss: 0.00082690
Iteration 24/25 | Loss: 0.00082690
Iteration 25/25 | Loss: 0.00082690

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082690
Iteration 2/1000 | Loss: 0.00002524
Iteration 3/1000 | Loss: 0.00001636
Iteration 4/1000 | Loss: 0.00001338
Iteration 5/1000 | Loss: 0.00001247
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001133
Iteration 8/1000 | Loss: 0.00001093
Iteration 9/1000 | Loss: 0.00001085
Iteration 10/1000 | Loss: 0.00001063
Iteration 11/1000 | Loss: 0.00001046
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001034
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001032
Iteration 16/1000 | Loss: 0.00001032
Iteration 17/1000 | Loss: 0.00001031
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001029
Iteration 20/1000 | Loss: 0.00001028
Iteration 21/1000 | Loss: 0.00001027
Iteration 22/1000 | Loss: 0.00001026
Iteration 23/1000 | Loss: 0.00001026
Iteration 24/1000 | Loss: 0.00001026
Iteration 25/1000 | Loss: 0.00001025
Iteration 26/1000 | Loss: 0.00001025
Iteration 27/1000 | Loss: 0.00001024
Iteration 28/1000 | Loss: 0.00001024
Iteration 29/1000 | Loss: 0.00001022
Iteration 30/1000 | Loss: 0.00001021
Iteration 31/1000 | Loss: 0.00001021
Iteration 32/1000 | Loss: 0.00001020
Iteration 33/1000 | Loss: 0.00001019
Iteration 34/1000 | Loss: 0.00001015
Iteration 35/1000 | Loss: 0.00001015
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001011
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001008
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001007
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001005
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001002
Iteration 58/1000 | Loss: 0.00001001
Iteration 59/1000 | Loss: 0.00001001
Iteration 60/1000 | Loss: 0.00001001
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001001
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00001000
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000999
Iteration 72/1000 | Loss: 0.00000999
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000999
Iteration 75/1000 | Loss: 0.00000999
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000996
Iteration 78/1000 | Loss: 0.00000996
Iteration 79/1000 | Loss: 0.00000995
Iteration 80/1000 | Loss: 0.00000995
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000994
Iteration 85/1000 | Loss: 0.00000993
Iteration 86/1000 | Loss: 0.00000992
Iteration 87/1000 | Loss: 0.00000991
Iteration 88/1000 | Loss: 0.00000990
Iteration 89/1000 | Loss: 0.00000990
Iteration 90/1000 | Loss: 0.00000989
Iteration 91/1000 | Loss: 0.00000989
Iteration 92/1000 | Loss: 0.00000989
Iteration 93/1000 | Loss: 0.00000988
Iteration 94/1000 | Loss: 0.00000988
Iteration 95/1000 | Loss: 0.00000988
Iteration 96/1000 | Loss: 0.00000987
Iteration 97/1000 | Loss: 0.00000987
Iteration 98/1000 | Loss: 0.00000987
Iteration 99/1000 | Loss: 0.00000986
Iteration 100/1000 | Loss: 0.00000986
Iteration 101/1000 | Loss: 0.00000985
Iteration 102/1000 | Loss: 0.00000985
Iteration 103/1000 | Loss: 0.00000985
Iteration 104/1000 | Loss: 0.00000985
Iteration 105/1000 | Loss: 0.00000985
Iteration 106/1000 | Loss: 0.00000985
Iteration 107/1000 | Loss: 0.00000984
Iteration 108/1000 | Loss: 0.00000984
Iteration 109/1000 | Loss: 0.00000983
Iteration 110/1000 | Loss: 0.00000983
Iteration 111/1000 | Loss: 0.00000983
Iteration 112/1000 | Loss: 0.00000982
Iteration 113/1000 | Loss: 0.00000982
Iteration 114/1000 | Loss: 0.00000982
Iteration 115/1000 | Loss: 0.00000982
Iteration 116/1000 | Loss: 0.00000982
Iteration 117/1000 | Loss: 0.00000982
Iteration 118/1000 | Loss: 0.00000982
Iteration 119/1000 | Loss: 0.00000981
Iteration 120/1000 | Loss: 0.00000981
Iteration 121/1000 | Loss: 0.00000980
Iteration 122/1000 | Loss: 0.00000979
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000979
Iteration 125/1000 | Loss: 0.00000979
Iteration 126/1000 | Loss: 0.00000979
Iteration 127/1000 | Loss: 0.00000979
Iteration 128/1000 | Loss: 0.00000979
Iteration 129/1000 | Loss: 0.00000979
Iteration 130/1000 | Loss: 0.00000979
Iteration 131/1000 | Loss: 0.00000979
Iteration 132/1000 | Loss: 0.00000979
Iteration 133/1000 | Loss: 0.00000979
Iteration 134/1000 | Loss: 0.00000978
Iteration 135/1000 | Loss: 0.00000978
Iteration 136/1000 | Loss: 0.00000978
Iteration 137/1000 | Loss: 0.00000978
Iteration 138/1000 | Loss: 0.00000978
Iteration 139/1000 | Loss: 0.00000978
Iteration 140/1000 | Loss: 0.00000978
Iteration 141/1000 | Loss: 0.00000977
Iteration 142/1000 | Loss: 0.00000977
Iteration 143/1000 | Loss: 0.00000977
Iteration 144/1000 | Loss: 0.00000977
Iteration 145/1000 | Loss: 0.00000977
Iteration 146/1000 | Loss: 0.00000977
Iteration 147/1000 | Loss: 0.00000977
Iteration 148/1000 | Loss: 0.00000977
Iteration 149/1000 | Loss: 0.00000976
Iteration 150/1000 | Loss: 0.00000976
Iteration 151/1000 | Loss: 0.00000976
Iteration 152/1000 | Loss: 0.00000976
Iteration 153/1000 | Loss: 0.00000975
Iteration 154/1000 | Loss: 0.00000975
Iteration 155/1000 | Loss: 0.00000975
Iteration 156/1000 | Loss: 0.00000975
Iteration 157/1000 | Loss: 0.00000975
Iteration 158/1000 | Loss: 0.00000975
Iteration 159/1000 | Loss: 0.00000975
Iteration 160/1000 | Loss: 0.00000975
Iteration 161/1000 | Loss: 0.00000974
Iteration 162/1000 | Loss: 0.00000974
Iteration 163/1000 | Loss: 0.00000974
Iteration 164/1000 | Loss: 0.00000974
Iteration 165/1000 | Loss: 0.00000974
Iteration 166/1000 | Loss: 0.00000974
Iteration 167/1000 | Loss: 0.00000974
Iteration 168/1000 | Loss: 0.00000974
Iteration 169/1000 | Loss: 0.00000974
Iteration 170/1000 | Loss: 0.00000973
Iteration 171/1000 | Loss: 0.00000973
Iteration 172/1000 | Loss: 0.00000973
Iteration 173/1000 | Loss: 0.00000973
Iteration 174/1000 | Loss: 0.00000972
Iteration 175/1000 | Loss: 0.00000972
Iteration 176/1000 | Loss: 0.00000972
Iteration 177/1000 | Loss: 0.00000972
Iteration 178/1000 | Loss: 0.00000971
Iteration 179/1000 | Loss: 0.00000971
Iteration 180/1000 | Loss: 0.00000971
Iteration 181/1000 | Loss: 0.00000971
Iteration 182/1000 | Loss: 0.00000971
Iteration 183/1000 | Loss: 0.00000971
Iteration 184/1000 | Loss: 0.00000970
Iteration 185/1000 | Loss: 0.00000970
Iteration 186/1000 | Loss: 0.00000970
Iteration 187/1000 | Loss: 0.00000970
Iteration 188/1000 | Loss: 0.00000970
Iteration 189/1000 | Loss: 0.00000970
Iteration 190/1000 | Loss: 0.00000970
Iteration 191/1000 | Loss: 0.00000970
Iteration 192/1000 | Loss: 0.00000970
Iteration 193/1000 | Loss: 0.00000970
Iteration 194/1000 | Loss: 0.00000970
Iteration 195/1000 | Loss: 0.00000970
Iteration 196/1000 | Loss: 0.00000970
Iteration 197/1000 | Loss: 0.00000970
Iteration 198/1000 | Loss: 0.00000970
Iteration 199/1000 | Loss: 0.00000970
Iteration 200/1000 | Loss: 0.00000970
Iteration 201/1000 | Loss: 0.00000969
Iteration 202/1000 | Loss: 0.00000969
Iteration 203/1000 | Loss: 0.00000969
Iteration 204/1000 | Loss: 0.00000969
Iteration 205/1000 | Loss: 0.00000969
Iteration 206/1000 | Loss: 0.00000969
Iteration 207/1000 | Loss: 0.00000969
Iteration 208/1000 | Loss: 0.00000969
Iteration 209/1000 | Loss: 0.00000969
Iteration 210/1000 | Loss: 0.00000969
Iteration 211/1000 | Loss: 0.00000969
Iteration 212/1000 | Loss: 0.00000969
Iteration 213/1000 | Loss: 0.00000969
Iteration 214/1000 | Loss: 0.00000969
Iteration 215/1000 | Loss: 0.00000969
Iteration 216/1000 | Loss: 0.00000969
Iteration 217/1000 | Loss: 0.00000968
Iteration 218/1000 | Loss: 0.00000968
Iteration 219/1000 | Loss: 0.00000968
Iteration 220/1000 | Loss: 0.00000968
Iteration 221/1000 | Loss: 0.00000968
Iteration 222/1000 | Loss: 0.00000968
Iteration 223/1000 | Loss: 0.00000968
Iteration 224/1000 | Loss: 0.00000968
Iteration 225/1000 | Loss: 0.00000968
Iteration 226/1000 | Loss: 0.00000968
Iteration 227/1000 | Loss: 0.00000968
Iteration 228/1000 | Loss: 0.00000968
Iteration 229/1000 | Loss: 0.00000968
Iteration 230/1000 | Loss: 0.00000968
Iteration 231/1000 | Loss: 0.00000968
Iteration 232/1000 | Loss: 0.00000968
Iteration 233/1000 | Loss: 0.00000968
Iteration 234/1000 | Loss: 0.00000968
Iteration 235/1000 | Loss: 0.00000967
Iteration 236/1000 | Loss: 0.00000967
Iteration 237/1000 | Loss: 0.00000967
Iteration 238/1000 | Loss: 0.00000967
Iteration 239/1000 | Loss: 0.00000967
Iteration 240/1000 | Loss: 0.00000967
Iteration 241/1000 | Loss: 0.00000967
Iteration 242/1000 | Loss: 0.00000967
Iteration 243/1000 | Loss: 0.00000967
Iteration 244/1000 | Loss: 0.00000967
Iteration 245/1000 | Loss: 0.00000967
Iteration 246/1000 | Loss: 0.00000967
Iteration 247/1000 | Loss: 0.00000966
Iteration 248/1000 | Loss: 0.00000966
Iteration 249/1000 | Loss: 0.00000966
Iteration 250/1000 | Loss: 0.00000966
Iteration 251/1000 | Loss: 0.00000966
Iteration 252/1000 | Loss: 0.00000966
Iteration 253/1000 | Loss: 0.00000966
Iteration 254/1000 | Loss: 0.00000966
Iteration 255/1000 | Loss: 0.00000966
Iteration 256/1000 | Loss: 0.00000965
Iteration 257/1000 | Loss: 0.00000965
Iteration 258/1000 | Loss: 0.00000965
Iteration 259/1000 | Loss: 0.00000965
Iteration 260/1000 | Loss: 0.00000965
Iteration 261/1000 | Loss: 0.00000965
Iteration 262/1000 | Loss: 0.00000965
Iteration 263/1000 | Loss: 0.00000965
Iteration 264/1000 | Loss: 0.00000965
Iteration 265/1000 | Loss: 0.00000965
Iteration 266/1000 | Loss: 0.00000964
Iteration 267/1000 | Loss: 0.00000964
Iteration 268/1000 | Loss: 0.00000964
Iteration 269/1000 | Loss: 0.00000964
Iteration 270/1000 | Loss: 0.00000964
Iteration 271/1000 | Loss: 0.00000964
Iteration 272/1000 | Loss: 0.00000964
Iteration 273/1000 | Loss: 0.00000964
Iteration 274/1000 | Loss: 0.00000964
Iteration 275/1000 | Loss: 0.00000964
Iteration 276/1000 | Loss: 0.00000964
Iteration 277/1000 | Loss: 0.00000964
Iteration 278/1000 | Loss: 0.00000964
Iteration 279/1000 | Loss: 0.00000963
Iteration 280/1000 | Loss: 0.00000963
Iteration 281/1000 | Loss: 0.00000963
Iteration 282/1000 | Loss: 0.00000963
Iteration 283/1000 | Loss: 0.00000963
Iteration 284/1000 | Loss: 0.00000963
Iteration 285/1000 | Loss: 0.00000963
Iteration 286/1000 | Loss: 0.00000963
Iteration 287/1000 | Loss: 0.00000963
Iteration 288/1000 | Loss: 0.00000963
Iteration 289/1000 | Loss: 0.00000963
Iteration 290/1000 | Loss: 0.00000963
Iteration 291/1000 | Loss: 0.00000963
Iteration 292/1000 | Loss: 0.00000963
Iteration 293/1000 | Loss: 0.00000963
Iteration 294/1000 | Loss: 0.00000963
Iteration 295/1000 | Loss: 0.00000963
Iteration 296/1000 | Loss: 0.00000963
Iteration 297/1000 | Loss: 0.00000963
Iteration 298/1000 | Loss: 0.00000962
Iteration 299/1000 | Loss: 0.00000962
Iteration 300/1000 | Loss: 0.00000962
Iteration 301/1000 | Loss: 0.00000962
Iteration 302/1000 | Loss: 0.00000962
Iteration 303/1000 | Loss: 0.00000962
Iteration 304/1000 | Loss: 0.00000962
Iteration 305/1000 | Loss: 0.00000962
Iteration 306/1000 | Loss: 0.00000962
Iteration 307/1000 | Loss: 0.00000962
Iteration 308/1000 | Loss: 0.00000962
Iteration 309/1000 | Loss: 0.00000962
Iteration 310/1000 | Loss: 0.00000962
Iteration 311/1000 | Loss: 0.00000962
Iteration 312/1000 | Loss: 0.00000962
Iteration 313/1000 | Loss: 0.00000962
Iteration 314/1000 | Loss: 0.00000962
Iteration 315/1000 | Loss: 0.00000962
Iteration 316/1000 | Loss: 0.00000962
Iteration 317/1000 | Loss: 0.00000962
Iteration 318/1000 | Loss: 0.00000962
Iteration 319/1000 | Loss: 0.00000962
Iteration 320/1000 | Loss: 0.00000962
Iteration 321/1000 | Loss: 0.00000962
Iteration 322/1000 | Loss: 0.00000962
Iteration 323/1000 | Loss: 0.00000962
Iteration 324/1000 | Loss: 0.00000962
Iteration 325/1000 | Loss: 0.00000962
Iteration 326/1000 | Loss: 0.00000962
Iteration 327/1000 | Loss: 0.00000962
Iteration 328/1000 | Loss: 0.00000962
Iteration 329/1000 | Loss: 0.00000962
Iteration 330/1000 | Loss: 0.00000962
Iteration 331/1000 | Loss: 0.00000962
Iteration 332/1000 | Loss: 0.00000962
Iteration 333/1000 | Loss: 0.00000962
Iteration 334/1000 | Loss: 0.00000962
Iteration 335/1000 | Loss: 0.00000962
Iteration 336/1000 | Loss: 0.00000962
Iteration 337/1000 | Loss: 0.00000962
Iteration 338/1000 | Loss: 0.00000962
Iteration 339/1000 | Loss: 0.00000962
Iteration 340/1000 | Loss: 0.00000962
Iteration 341/1000 | Loss: 0.00000962
Iteration 342/1000 | Loss: 0.00000962
Iteration 343/1000 | Loss: 0.00000962
Iteration 344/1000 | Loss: 0.00000962
Iteration 345/1000 | Loss: 0.00000962
Iteration 346/1000 | Loss: 0.00000962
Iteration 347/1000 | Loss: 0.00000962
Iteration 348/1000 | Loss: 0.00000962
Iteration 349/1000 | Loss: 0.00000962
Iteration 350/1000 | Loss: 0.00000962
Iteration 351/1000 | Loss: 0.00000962
Iteration 352/1000 | Loss: 0.00000962
Iteration 353/1000 | Loss: 0.00000962
Iteration 354/1000 | Loss: 0.00000962
Iteration 355/1000 | Loss: 0.00000962
Iteration 356/1000 | Loss: 0.00000962
Iteration 357/1000 | Loss: 0.00000962
Iteration 358/1000 | Loss: 0.00000962
Iteration 359/1000 | Loss: 0.00000962
Iteration 360/1000 | Loss: 0.00000962
Iteration 361/1000 | Loss: 0.00000962
Iteration 362/1000 | Loss: 0.00000962
Iteration 363/1000 | Loss: 0.00000962
Iteration 364/1000 | Loss: 0.00000962
Iteration 365/1000 | Loss: 0.00000962
Iteration 366/1000 | Loss: 0.00000962
Iteration 367/1000 | Loss: 0.00000962
Iteration 368/1000 | Loss: 0.00000962
Iteration 369/1000 | Loss: 0.00000962
Iteration 370/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 370. Stopping optimization.
Last 5 losses: [9.61581554292934e-06, 9.61581554292934e-06, 9.61581554292934e-06, 9.61581554292934e-06, 9.61581554292934e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.61581554292934e-06

Optimization complete. Final v2v error: 2.6322696208953857 mm

Highest mean error: 2.832662343978882 mm for frame 54

Lowest mean error: 2.467323064804077 mm for frame 160

Saving results

Total time: 48.48116755485535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520567
Iteration 2/25 | Loss: 0.00133388
Iteration 3/25 | Loss: 0.00122022
Iteration 4/25 | Loss: 0.00120393
Iteration 5/25 | Loss: 0.00119809
Iteration 6/25 | Loss: 0.00119669
Iteration 7/25 | Loss: 0.00119669
Iteration 8/25 | Loss: 0.00119669
Iteration 9/25 | Loss: 0.00119669
Iteration 10/25 | Loss: 0.00119669
Iteration 11/25 | Loss: 0.00119669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011966886231675744, 0.0011966886231675744, 0.0011966886231675744, 0.0011966886231675744, 0.0011966886231675744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011966886231675744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17351294
Iteration 2/25 | Loss: 0.00121150
Iteration 3/25 | Loss: 0.00121149
Iteration 4/25 | Loss: 0.00121149
Iteration 5/25 | Loss: 0.00121149
Iteration 6/25 | Loss: 0.00121149
Iteration 7/25 | Loss: 0.00121149
Iteration 8/25 | Loss: 0.00121149
Iteration 9/25 | Loss: 0.00121149
Iteration 10/25 | Loss: 0.00121149
Iteration 11/25 | Loss: 0.00121149
Iteration 12/25 | Loss: 0.00121149
Iteration 13/25 | Loss: 0.00121149
Iteration 14/25 | Loss: 0.00121149
Iteration 15/25 | Loss: 0.00121149
Iteration 16/25 | Loss: 0.00121149
Iteration 17/25 | Loss: 0.00121149
Iteration 18/25 | Loss: 0.00121149
Iteration 19/25 | Loss: 0.00121149
Iteration 20/25 | Loss: 0.00121149
Iteration 21/25 | Loss: 0.00121149
Iteration 22/25 | Loss: 0.00121149
Iteration 23/25 | Loss: 0.00121149
Iteration 24/25 | Loss: 0.00121149
Iteration 25/25 | Loss: 0.00121149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121149
Iteration 2/1000 | Loss: 0.00002976
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001397
Iteration 5/1000 | Loss: 0.00001304
Iteration 6/1000 | Loss: 0.00001250
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001193
Iteration 9/1000 | Loss: 0.00001171
Iteration 10/1000 | Loss: 0.00001161
Iteration 11/1000 | Loss: 0.00001144
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001124
Iteration 14/1000 | Loss: 0.00001120
Iteration 15/1000 | Loss: 0.00001117
Iteration 16/1000 | Loss: 0.00001114
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001109
Iteration 23/1000 | Loss: 0.00001108
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001107
Iteration 27/1000 | Loss: 0.00001107
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001105
Iteration 30/1000 | Loss: 0.00001105
Iteration 31/1000 | Loss: 0.00001105
Iteration 32/1000 | Loss: 0.00001105
Iteration 33/1000 | Loss: 0.00001104
Iteration 34/1000 | Loss: 0.00001104
Iteration 35/1000 | Loss: 0.00001104
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001103
Iteration 38/1000 | Loss: 0.00001103
Iteration 39/1000 | Loss: 0.00001103
Iteration 40/1000 | Loss: 0.00001102
Iteration 41/1000 | Loss: 0.00001102
Iteration 42/1000 | Loss: 0.00001102
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001101
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001096
Iteration 51/1000 | Loss: 0.00001096
Iteration 52/1000 | Loss: 0.00001095
Iteration 53/1000 | Loss: 0.00001095
Iteration 54/1000 | Loss: 0.00001095
Iteration 55/1000 | Loss: 0.00001095
Iteration 56/1000 | Loss: 0.00001094
Iteration 57/1000 | Loss: 0.00001094
Iteration 58/1000 | Loss: 0.00001094
Iteration 59/1000 | Loss: 0.00001094
Iteration 60/1000 | Loss: 0.00001094
Iteration 61/1000 | Loss: 0.00001094
Iteration 62/1000 | Loss: 0.00001094
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001093
Iteration 67/1000 | Loss: 0.00001093
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001092
Iteration 72/1000 | Loss: 0.00001092
Iteration 73/1000 | Loss: 0.00001092
Iteration 74/1000 | Loss: 0.00001092
Iteration 75/1000 | Loss: 0.00001092
Iteration 76/1000 | Loss: 0.00001092
Iteration 77/1000 | Loss: 0.00001092
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001091
Iteration 80/1000 | Loss: 0.00001091
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001091
Iteration 83/1000 | Loss: 0.00001090
Iteration 84/1000 | Loss: 0.00001090
Iteration 85/1000 | Loss: 0.00001090
Iteration 86/1000 | Loss: 0.00001090
Iteration 87/1000 | Loss: 0.00001090
Iteration 88/1000 | Loss: 0.00001090
Iteration 89/1000 | Loss: 0.00001089
Iteration 90/1000 | Loss: 0.00001089
Iteration 91/1000 | Loss: 0.00001089
Iteration 92/1000 | Loss: 0.00001089
Iteration 93/1000 | Loss: 0.00001089
Iteration 94/1000 | Loss: 0.00001089
Iteration 95/1000 | Loss: 0.00001089
Iteration 96/1000 | Loss: 0.00001089
Iteration 97/1000 | Loss: 0.00001089
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001088
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001088
Iteration 106/1000 | Loss: 0.00001088
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001088
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001087
Iteration 126/1000 | Loss: 0.00001087
Iteration 127/1000 | Loss: 0.00001086
Iteration 128/1000 | Loss: 0.00001086
Iteration 129/1000 | Loss: 0.00001086
Iteration 130/1000 | Loss: 0.00001086
Iteration 131/1000 | Loss: 0.00001086
Iteration 132/1000 | Loss: 0.00001086
Iteration 133/1000 | Loss: 0.00001086
Iteration 134/1000 | Loss: 0.00001086
Iteration 135/1000 | Loss: 0.00001086
Iteration 136/1000 | Loss: 0.00001086
Iteration 137/1000 | Loss: 0.00001086
Iteration 138/1000 | Loss: 0.00001086
Iteration 139/1000 | Loss: 0.00001086
Iteration 140/1000 | Loss: 0.00001086
Iteration 141/1000 | Loss: 0.00001085
Iteration 142/1000 | Loss: 0.00001085
Iteration 143/1000 | Loss: 0.00001085
Iteration 144/1000 | Loss: 0.00001085
Iteration 145/1000 | Loss: 0.00001085
Iteration 146/1000 | Loss: 0.00001085
Iteration 147/1000 | Loss: 0.00001085
Iteration 148/1000 | Loss: 0.00001085
Iteration 149/1000 | Loss: 0.00001085
Iteration 150/1000 | Loss: 0.00001085
Iteration 151/1000 | Loss: 0.00001085
Iteration 152/1000 | Loss: 0.00001085
Iteration 153/1000 | Loss: 0.00001085
Iteration 154/1000 | Loss: 0.00001085
Iteration 155/1000 | Loss: 0.00001085
Iteration 156/1000 | Loss: 0.00001085
Iteration 157/1000 | Loss: 0.00001085
Iteration 158/1000 | Loss: 0.00001085
Iteration 159/1000 | Loss: 0.00001085
Iteration 160/1000 | Loss: 0.00001085
Iteration 161/1000 | Loss: 0.00001085
Iteration 162/1000 | Loss: 0.00001085
Iteration 163/1000 | Loss: 0.00001085
Iteration 164/1000 | Loss: 0.00001085
Iteration 165/1000 | Loss: 0.00001085
Iteration 166/1000 | Loss: 0.00001085
Iteration 167/1000 | Loss: 0.00001085
Iteration 168/1000 | Loss: 0.00001085
Iteration 169/1000 | Loss: 0.00001085
Iteration 170/1000 | Loss: 0.00001085
Iteration 171/1000 | Loss: 0.00001085
Iteration 172/1000 | Loss: 0.00001085
Iteration 173/1000 | Loss: 0.00001085
Iteration 174/1000 | Loss: 0.00001085
Iteration 175/1000 | Loss: 0.00001085
Iteration 176/1000 | Loss: 0.00001085
Iteration 177/1000 | Loss: 0.00001085
Iteration 178/1000 | Loss: 0.00001085
Iteration 179/1000 | Loss: 0.00001085
Iteration 180/1000 | Loss: 0.00001085
Iteration 181/1000 | Loss: 0.00001085
Iteration 182/1000 | Loss: 0.00001085
Iteration 183/1000 | Loss: 0.00001085
Iteration 184/1000 | Loss: 0.00001085
Iteration 185/1000 | Loss: 0.00001085
Iteration 186/1000 | Loss: 0.00001085
Iteration 187/1000 | Loss: 0.00001085
Iteration 188/1000 | Loss: 0.00001085
Iteration 189/1000 | Loss: 0.00001085
Iteration 190/1000 | Loss: 0.00001085
Iteration 191/1000 | Loss: 0.00001085
Iteration 192/1000 | Loss: 0.00001085
Iteration 193/1000 | Loss: 0.00001085
Iteration 194/1000 | Loss: 0.00001085
Iteration 195/1000 | Loss: 0.00001085
Iteration 196/1000 | Loss: 0.00001085
Iteration 197/1000 | Loss: 0.00001085
Iteration 198/1000 | Loss: 0.00001085
Iteration 199/1000 | Loss: 0.00001085
Iteration 200/1000 | Loss: 0.00001085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.0851246770471334e-05, 1.0851246770471334e-05, 1.0851246770471334e-05, 1.0851246770471334e-05, 1.0851246770471334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0851246770471334e-05

Optimization complete. Final v2v error: 2.8383922576904297 mm

Highest mean error: 3.188488245010376 mm for frame 141

Lowest mean error: 2.738954544067383 mm for frame 44

Saving results

Total time: 36.55636525154114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052906
Iteration 2/25 | Loss: 0.00170964
Iteration 3/25 | Loss: 0.00139419
Iteration 4/25 | Loss: 0.00136349
Iteration 5/25 | Loss: 0.00135201
Iteration 6/25 | Loss: 0.00134967
Iteration 7/25 | Loss: 0.00134967
Iteration 8/25 | Loss: 0.00134967
Iteration 9/25 | Loss: 0.00134967
Iteration 10/25 | Loss: 0.00134967
Iteration 11/25 | Loss: 0.00134967
Iteration 12/25 | Loss: 0.00134967
Iteration 13/25 | Loss: 0.00134967
Iteration 14/25 | Loss: 0.00134967
Iteration 15/25 | Loss: 0.00134967
Iteration 16/25 | Loss: 0.00134967
Iteration 17/25 | Loss: 0.00134967
Iteration 18/25 | Loss: 0.00134967
Iteration 19/25 | Loss: 0.00134967
Iteration 20/25 | Loss: 0.00134967
Iteration 21/25 | Loss: 0.00134967
Iteration 22/25 | Loss: 0.00134967
Iteration 23/25 | Loss: 0.00134967
Iteration 24/25 | Loss: 0.00134967
Iteration 25/25 | Loss: 0.00134967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24187768
Iteration 2/25 | Loss: 0.00082356
Iteration 3/25 | Loss: 0.00082356
Iteration 4/25 | Loss: 0.00082356
Iteration 5/25 | Loss: 0.00082356
Iteration 6/25 | Loss: 0.00082356
Iteration 7/25 | Loss: 0.00082356
Iteration 8/25 | Loss: 0.00082356
Iteration 9/25 | Loss: 0.00082355
Iteration 10/25 | Loss: 0.00082355
Iteration 11/25 | Loss: 0.00082355
Iteration 12/25 | Loss: 0.00082355
Iteration 13/25 | Loss: 0.00082355
Iteration 14/25 | Loss: 0.00082355
Iteration 15/25 | Loss: 0.00082355
Iteration 16/25 | Loss: 0.00082355
Iteration 17/25 | Loss: 0.00082355
Iteration 18/25 | Loss: 0.00082355
Iteration 19/25 | Loss: 0.00082355
Iteration 20/25 | Loss: 0.00082355
Iteration 21/25 | Loss: 0.00082355
Iteration 22/25 | Loss: 0.00082355
Iteration 23/25 | Loss: 0.00082355
Iteration 24/25 | Loss: 0.00082355
Iteration 25/25 | Loss: 0.00082355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082355
Iteration 2/1000 | Loss: 0.00004773
Iteration 3/1000 | Loss: 0.00003198
Iteration 4/1000 | Loss: 0.00002928
Iteration 5/1000 | Loss: 0.00002811
Iteration 6/1000 | Loss: 0.00002752
Iteration 7/1000 | Loss: 0.00002709
Iteration 8/1000 | Loss: 0.00002670
Iteration 9/1000 | Loss: 0.00002644
Iteration 10/1000 | Loss: 0.00002626
Iteration 11/1000 | Loss: 0.00002623
Iteration 12/1000 | Loss: 0.00002613
Iteration 13/1000 | Loss: 0.00002612
Iteration 14/1000 | Loss: 0.00002612
Iteration 15/1000 | Loss: 0.00002610
Iteration 16/1000 | Loss: 0.00002608
Iteration 17/1000 | Loss: 0.00002608
Iteration 18/1000 | Loss: 0.00002607
Iteration 19/1000 | Loss: 0.00002607
Iteration 20/1000 | Loss: 0.00002607
Iteration 21/1000 | Loss: 0.00002606
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00002605
Iteration 24/1000 | Loss: 0.00002605
Iteration 25/1000 | Loss: 0.00002604
Iteration 26/1000 | Loss: 0.00002604
Iteration 27/1000 | Loss: 0.00002604
Iteration 28/1000 | Loss: 0.00002604
Iteration 29/1000 | Loss: 0.00002603
Iteration 30/1000 | Loss: 0.00002603
Iteration 31/1000 | Loss: 0.00002603
Iteration 32/1000 | Loss: 0.00002603
Iteration 33/1000 | Loss: 0.00002603
Iteration 34/1000 | Loss: 0.00002603
Iteration 35/1000 | Loss: 0.00002603
Iteration 36/1000 | Loss: 0.00002602
Iteration 37/1000 | Loss: 0.00002602
Iteration 38/1000 | Loss: 0.00002602
Iteration 39/1000 | Loss: 0.00002601
Iteration 40/1000 | Loss: 0.00002601
Iteration 41/1000 | Loss: 0.00002601
Iteration 42/1000 | Loss: 0.00002601
Iteration 43/1000 | Loss: 0.00002601
Iteration 44/1000 | Loss: 0.00002601
Iteration 45/1000 | Loss: 0.00002600
Iteration 46/1000 | Loss: 0.00002600
Iteration 47/1000 | Loss: 0.00002600
Iteration 48/1000 | Loss: 0.00002600
Iteration 49/1000 | Loss: 0.00002600
Iteration 50/1000 | Loss: 0.00002600
Iteration 51/1000 | Loss: 0.00002600
Iteration 52/1000 | Loss: 0.00002600
Iteration 53/1000 | Loss: 0.00002599
Iteration 54/1000 | Loss: 0.00002599
Iteration 55/1000 | Loss: 0.00002599
Iteration 56/1000 | Loss: 0.00002599
Iteration 57/1000 | Loss: 0.00002599
Iteration 58/1000 | Loss: 0.00002599
Iteration 59/1000 | Loss: 0.00002598
Iteration 60/1000 | Loss: 0.00002597
Iteration 61/1000 | Loss: 0.00002597
Iteration 62/1000 | Loss: 0.00002596
Iteration 63/1000 | Loss: 0.00002596
Iteration 64/1000 | Loss: 0.00002595
Iteration 65/1000 | Loss: 0.00002594
Iteration 66/1000 | Loss: 0.00002594
Iteration 67/1000 | Loss: 0.00002593
Iteration 68/1000 | Loss: 0.00002593
Iteration 69/1000 | Loss: 0.00002593
Iteration 70/1000 | Loss: 0.00002592
Iteration 71/1000 | Loss: 0.00002592
Iteration 72/1000 | Loss: 0.00002592
Iteration 73/1000 | Loss: 0.00002591
Iteration 74/1000 | Loss: 0.00002591
Iteration 75/1000 | Loss: 0.00002591
Iteration 76/1000 | Loss: 0.00002591
Iteration 77/1000 | Loss: 0.00002590
Iteration 78/1000 | Loss: 0.00002590
Iteration 79/1000 | Loss: 0.00002590
Iteration 80/1000 | Loss: 0.00002590
Iteration 81/1000 | Loss: 0.00002590
Iteration 82/1000 | Loss: 0.00002590
Iteration 83/1000 | Loss: 0.00002590
Iteration 84/1000 | Loss: 0.00002589
Iteration 85/1000 | Loss: 0.00002589
Iteration 86/1000 | Loss: 0.00002588
Iteration 87/1000 | Loss: 0.00002588
Iteration 88/1000 | Loss: 0.00002588
Iteration 89/1000 | Loss: 0.00002587
Iteration 90/1000 | Loss: 0.00002587
Iteration 91/1000 | Loss: 0.00002587
Iteration 92/1000 | Loss: 0.00002586
Iteration 93/1000 | Loss: 0.00002586
Iteration 94/1000 | Loss: 0.00002585
Iteration 95/1000 | Loss: 0.00002585
Iteration 96/1000 | Loss: 0.00002585
Iteration 97/1000 | Loss: 0.00002585
Iteration 98/1000 | Loss: 0.00002585
Iteration 99/1000 | Loss: 0.00002584
Iteration 100/1000 | Loss: 0.00002584
Iteration 101/1000 | Loss: 0.00002584
Iteration 102/1000 | Loss: 0.00002584
Iteration 103/1000 | Loss: 0.00002584
Iteration 104/1000 | Loss: 0.00002584
Iteration 105/1000 | Loss: 0.00002584
Iteration 106/1000 | Loss: 0.00002584
Iteration 107/1000 | Loss: 0.00002583
Iteration 108/1000 | Loss: 0.00002582
Iteration 109/1000 | Loss: 0.00002582
Iteration 110/1000 | Loss: 0.00002581
Iteration 111/1000 | Loss: 0.00002581
Iteration 112/1000 | Loss: 0.00002581
Iteration 113/1000 | Loss: 0.00002581
Iteration 114/1000 | Loss: 0.00002581
Iteration 115/1000 | Loss: 0.00002581
Iteration 116/1000 | Loss: 0.00002581
Iteration 117/1000 | Loss: 0.00002581
Iteration 118/1000 | Loss: 0.00002581
Iteration 119/1000 | Loss: 0.00002581
Iteration 120/1000 | Loss: 0.00002581
Iteration 121/1000 | Loss: 0.00002581
Iteration 122/1000 | Loss: 0.00002581
Iteration 123/1000 | Loss: 0.00002581
Iteration 124/1000 | Loss: 0.00002581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.581160697445739e-05, 2.581160697445739e-05, 2.581160697445739e-05, 2.581160697445739e-05, 2.581160697445739e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.581160697445739e-05

Optimization complete. Final v2v error: 4.0925679206848145 mm

Highest mean error: 4.688258647918701 mm for frame 151

Lowest mean error: 3.5896666049957275 mm for frame 7

Saving results

Total time: 36.83122682571411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722992
Iteration 2/25 | Loss: 0.00149378
Iteration 3/25 | Loss: 0.00132931
Iteration 4/25 | Loss: 0.00131515
Iteration 5/25 | Loss: 0.00130971
Iteration 6/25 | Loss: 0.00130879
Iteration 7/25 | Loss: 0.00130879
Iteration 8/25 | Loss: 0.00130879
Iteration 9/25 | Loss: 0.00130879
Iteration 10/25 | Loss: 0.00130879
Iteration 11/25 | Loss: 0.00130879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013087874976918101, 0.0013087874976918101, 0.0013087874976918101, 0.0013087874976918101, 0.0013087874976918101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013087874976918101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79413080
Iteration 2/25 | Loss: 0.00095246
Iteration 3/25 | Loss: 0.00095240
Iteration 4/25 | Loss: 0.00095240
Iteration 5/25 | Loss: 0.00095240
Iteration 6/25 | Loss: 0.00095240
Iteration 7/25 | Loss: 0.00095240
Iteration 8/25 | Loss: 0.00095240
Iteration 9/25 | Loss: 0.00095240
Iteration 10/25 | Loss: 0.00095240
Iteration 11/25 | Loss: 0.00095240
Iteration 12/25 | Loss: 0.00095240
Iteration 13/25 | Loss: 0.00095240
Iteration 14/25 | Loss: 0.00095240
Iteration 15/25 | Loss: 0.00095240
Iteration 16/25 | Loss: 0.00095240
Iteration 17/25 | Loss: 0.00095240
Iteration 18/25 | Loss: 0.00095240
Iteration 19/25 | Loss: 0.00095240
Iteration 20/25 | Loss: 0.00095240
Iteration 21/25 | Loss: 0.00095240
Iteration 22/25 | Loss: 0.00095240
Iteration 23/25 | Loss: 0.00095240
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009524011402390897, 0.0009524011402390897, 0.0009524011402390897, 0.0009524011402390897, 0.0009524011402390897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009524011402390897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095240
Iteration 2/1000 | Loss: 0.00007321
Iteration 3/1000 | Loss: 0.00004712
Iteration 4/1000 | Loss: 0.00003885
Iteration 5/1000 | Loss: 0.00003567
Iteration 6/1000 | Loss: 0.00003394
Iteration 7/1000 | Loss: 0.00003324
Iteration 8/1000 | Loss: 0.00003260
Iteration 9/1000 | Loss: 0.00003195
Iteration 10/1000 | Loss: 0.00003141
Iteration 11/1000 | Loss: 0.00003107
Iteration 12/1000 | Loss: 0.00003074
Iteration 13/1000 | Loss: 0.00003046
Iteration 14/1000 | Loss: 0.00003023
Iteration 15/1000 | Loss: 0.00002998
Iteration 16/1000 | Loss: 0.00002975
Iteration 17/1000 | Loss: 0.00002959
Iteration 18/1000 | Loss: 0.00002949
Iteration 19/1000 | Loss: 0.00002946
Iteration 20/1000 | Loss: 0.00002943
Iteration 21/1000 | Loss: 0.00002943
Iteration 22/1000 | Loss: 0.00002942
Iteration 23/1000 | Loss: 0.00002942
Iteration 24/1000 | Loss: 0.00002939
Iteration 25/1000 | Loss: 0.00002926
Iteration 26/1000 | Loss: 0.00002916
Iteration 27/1000 | Loss: 0.00002915
Iteration 28/1000 | Loss: 0.00002911
Iteration 29/1000 | Loss: 0.00002908
Iteration 30/1000 | Loss: 0.00002908
Iteration 31/1000 | Loss: 0.00002907
Iteration 32/1000 | Loss: 0.00002904
Iteration 33/1000 | Loss: 0.00002903
Iteration 34/1000 | Loss: 0.00002901
Iteration 35/1000 | Loss: 0.00002901
Iteration 36/1000 | Loss: 0.00002901
Iteration 37/1000 | Loss: 0.00002901
Iteration 38/1000 | Loss: 0.00002901
Iteration 39/1000 | Loss: 0.00002900
Iteration 40/1000 | Loss: 0.00002900
Iteration 41/1000 | Loss: 0.00002900
Iteration 42/1000 | Loss: 0.00002897
Iteration 43/1000 | Loss: 0.00002897
Iteration 44/1000 | Loss: 0.00002897
Iteration 45/1000 | Loss: 0.00002897
Iteration 46/1000 | Loss: 0.00002897
Iteration 47/1000 | Loss: 0.00002897
Iteration 48/1000 | Loss: 0.00002897
Iteration 49/1000 | Loss: 0.00002896
Iteration 50/1000 | Loss: 0.00002896
Iteration 51/1000 | Loss: 0.00002896
Iteration 52/1000 | Loss: 0.00002896
Iteration 53/1000 | Loss: 0.00002896
Iteration 54/1000 | Loss: 0.00002896
Iteration 55/1000 | Loss: 0.00002896
Iteration 56/1000 | Loss: 0.00002893
Iteration 57/1000 | Loss: 0.00002893
Iteration 58/1000 | Loss: 0.00002893
Iteration 59/1000 | Loss: 0.00002893
Iteration 60/1000 | Loss: 0.00002893
Iteration 61/1000 | Loss: 0.00002893
Iteration 62/1000 | Loss: 0.00002893
Iteration 63/1000 | Loss: 0.00002893
Iteration 64/1000 | Loss: 0.00002893
Iteration 65/1000 | Loss: 0.00002892
Iteration 66/1000 | Loss: 0.00002892
Iteration 67/1000 | Loss: 0.00002892
Iteration 68/1000 | Loss: 0.00002892
Iteration 69/1000 | Loss: 0.00002891
Iteration 70/1000 | Loss: 0.00002890
Iteration 71/1000 | Loss: 0.00002890
Iteration 72/1000 | Loss: 0.00002890
Iteration 73/1000 | Loss: 0.00002889
Iteration 74/1000 | Loss: 0.00002888
Iteration 75/1000 | Loss: 0.00002887
Iteration 76/1000 | Loss: 0.00002885
Iteration 77/1000 | Loss: 0.00002885
Iteration 78/1000 | Loss: 0.00002885
Iteration 79/1000 | Loss: 0.00002885
Iteration 80/1000 | Loss: 0.00002885
Iteration 81/1000 | Loss: 0.00002884
Iteration 82/1000 | Loss: 0.00002884
Iteration 83/1000 | Loss: 0.00002882
Iteration 84/1000 | Loss: 0.00002882
Iteration 85/1000 | Loss: 0.00002882
Iteration 86/1000 | Loss: 0.00002882
Iteration 87/1000 | Loss: 0.00002882
Iteration 88/1000 | Loss: 0.00002882
Iteration 89/1000 | Loss: 0.00002882
Iteration 90/1000 | Loss: 0.00002882
Iteration 91/1000 | Loss: 0.00002882
Iteration 92/1000 | Loss: 0.00002881
Iteration 93/1000 | Loss: 0.00002881
Iteration 94/1000 | Loss: 0.00002881
Iteration 95/1000 | Loss: 0.00002881
Iteration 96/1000 | Loss: 0.00002881
Iteration 97/1000 | Loss: 0.00002881
Iteration 98/1000 | Loss: 0.00002881
Iteration 99/1000 | Loss: 0.00002881
Iteration 100/1000 | Loss: 0.00002881
Iteration 101/1000 | Loss: 0.00002881
Iteration 102/1000 | Loss: 0.00002880
Iteration 103/1000 | Loss: 0.00002880
Iteration 104/1000 | Loss: 0.00002880
Iteration 105/1000 | Loss: 0.00002880
Iteration 106/1000 | Loss: 0.00002880
Iteration 107/1000 | Loss: 0.00002880
Iteration 108/1000 | Loss: 0.00002880
Iteration 109/1000 | Loss: 0.00002879
Iteration 110/1000 | Loss: 0.00002879
Iteration 111/1000 | Loss: 0.00002879
Iteration 112/1000 | Loss: 0.00002879
Iteration 113/1000 | Loss: 0.00002879
Iteration 114/1000 | Loss: 0.00002878
Iteration 115/1000 | Loss: 0.00002878
Iteration 116/1000 | Loss: 0.00002878
Iteration 117/1000 | Loss: 0.00002878
Iteration 118/1000 | Loss: 0.00002878
Iteration 119/1000 | Loss: 0.00002878
Iteration 120/1000 | Loss: 0.00002878
Iteration 121/1000 | Loss: 0.00002878
Iteration 122/1000 | Loss: 0.00002878
Iteration 123/1000 | Loss: 0.00002878
Iteration 124/1000 | Loss: 0.00002878
Iteration 125/1000 | Loss: 0.00002877
Iteration 126/1000 | Loss: 0.00002877
Iteration 127/1000 | Loss: 0.00002877
Iteration 128/1000 | Loss: 0.00002877
Iteration 129/1000 | Loss: 0.00002877
Iteration 130/1000 | Loss: 0.00002876
Iteration 131/1000 | Loss: 0.00002876
Iteration 132/1000 | Loss: 0.00002876
Iteration 133/1000 | Loss: 0.00002876
Iteration 134/1000 | Loss: 0.00002876
Iteration 135/1000 | Loss: 0.00002876
Iteration 136/1000 | Loss: 0.00002876
Iteration 137/1000 | Loss: 0.00002876
Iteration 138/1000 | Loss: 0.00002876
Iteration 139/1000 | Loss: 0.00002876
Iteration 140/1000 | Loss: 0.00002875
Iteration 141/1000 | Loss: 0.00002875
Iteration 142/1000 | Loss: 0.00002875
Iteration 143/1000 | Loss: 0.00002875
Iteration 144/1000 | Loss: 0.00002875
Iteration 145/1000 | Loss: 0.00002875
Iteration 146/1000 | Loss: 0.00002875
Iteration 147/1000 | Loss: 0.00002875
Iteration 148/1000 | Loss: 0.00002874
Iteration 149/1000 | Loss: 0.00002874
Iteration 150/1000 | Loss: 0.00002874
Iteration 151/1000 | Loss: 0.00002874
Iteration 152/1000 | Loss: 0.00002874
Iteration 153/1000 | Loss: 0.00002874
Iteration 154/1000 | Loss: 0.00002874
Iteration 155/1000 | Loss: 0.00002873
Iteration 156/1000 | Loss: 0.00002873
Iteration 157/1000 | Loss: 0.00002873
Iteration 158/1000 | Loss: 0.00002873
Iteration 159/1000 | Loss: 0.00002873
Iteration 160/1000 | Loss: 0.00002872
Iteration 161/1000 | Loss: 0.00002872
Iteration 162/1000 | Loss: 0.00002872
Iteration 163/1000 | Loss: 0.00002872
Iteration 164/1000 | Loss: 0.00002872
Iteration 165/1000 | Loss: 0.00002872
Iteration 166/1000 | Loss: 0.00002872
Iteration 167/1000 | Loss: 0.00002872
Iteration 168/1000 | Loss: 0.00002872
Iteration 169/1000 | Loss: 0.00002872
Iteration 170/1000 | Loss: 0.00002872
Iteration 171/1000 | Loss: 0.00002872
Iteration 172/1000 | Loss: 0.00002872
Iteration 173/1000 | Loss: 0.00002872
Iteration 174/1000 | Loss: 0.00002872
Iteration 175/1000 | Loss: 0.00002872
Iteration 176/1000 | Loss: 0.00002872
Iteration 177/1000 | Loss: 0.00002872
Iteration 178/1000 | Loss: 0.00002872
Iteration 179/1000 | Loss: 0.00002872
Iteration 180/1000 | Loss: 0.00002872
Iteration 181/1000 | Loss: 0.00002872
Iteration 182/1000 | Loss: 0.00002872
Iteration 183/1000 | Loss: 0.00002872
Iteration 184/1000 | Loss: 0.00002872
Iteration 185/1000 | Loss: 0.00002872
Iteration 186/1000 | Loss: 0.00002872
Iteration 187/1000 | Loss: 0.00002872
Iteration 188/1000 | Loss: 0.00002872
Iteration 189/1000 | Loss: 0.00002872
Iteration 190/1000 | Loss: 0.00002872
Iteration 191/1000 | Loss: 0.00002872
Iteration 192/1000 | Loss: 0.00002872
Iteration 193/1000 | Loss: 0.00002872
Iteration 194/1000 | Loss: 0.00002872
Iteration 195/1000 | Loss: 0.00002872
Iteration 196/1000 | Loss: 0.00002872
Iteration 197/1000 | Loss: 0.00002872
Iteration 198/1000 | Loss: 0.00002872
Iteration 199/1000 | Loss: 0.00002872
Iteration 200/1000 | Loss: 0.00002872
Iteration 201/1000 | Loss: 0.00002872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.871639844670426e-05, 2.871639844670426e-05, 2.871639844670426e-05, 2.871639844670426e-05, 2.871639844670426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.871639844670426e-05

Optimization complete. Final v2v error: 4.234170436859131 mm

Highest mean error: 5.681875705718994 mm for frame 127

Lowest mean error: 3.2939748764038086 mm for frame 0

Saving results

Total time: 51.57616567611694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925149
Iteration 2/25 | Loss: 0.00379108
Iteration 3/25 | Loss: 0.00268419
Iteration 4/25 | Loss: 0.00248755
Iteration 5/25 | Loss: 0.00272648
Iteration 6/25 | Loss: 0.00244002
Iteration 7/25 | Loss: 0.00221475
Iteration 8/25 | Loss: 0.00221320
Iteration 9/25 | Loss: 0.00204256
Iteration 10/25 | Loss: 0.00204599
Iteration 11/25 | Loss: 0.00189044
Iteration 12/25 | Loss: 0.00189977
Iteration 13/25 | Loss: 0.00184857
Iteration 14/25 | Loss: 0.00179304
Iteration 15/25 | Loss: 0.00177571
Iteration 16/25 | Loss: 0.00174501
Iteration 17/25 | Loss: 0.00174669
Iteration 18/25 | Loss: 0.00167262
Iteration 19/25 | Loss: 0.00164549
Iteration 20/25 | Loss: 0.00167289
Iteration 21/25 | Loss: 0.00169626
Iteration 22/25 | Loss: 0.00166899
Iteration 23/25 | Loss: 0.00163615
Iteration 24/25 | Loss: 0.00163051
Iteration 25/25 | Loss: 0.00159326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.91802692
Iteration 2/25 | Loss: 0.00577684
Iteration 3/25 | Loss: 0.00535117
Iteration 4/25 | Loss: 0.00535117
Iteration 5/25 | Loss: 0.00535117
Iteration 6/25 | Loss: 0.00535117
Iteration 7/25 | Loss: 0.00535117
Iteration 8/25 | Loss: 0.00535117
Iteration 9/25 | Loss: 0.00535117
Iteration 10/25 | Loss: 0.00535117
Iteration 11/25 | Loss: 0.00535117
Iteration 12/25 | Loss: 0.00535117
Iteration 13/25 | Loss: 0.00535117
Iteration 14/25 | Loss: 0.00535117
Iteration 15/25 | Loss: 0.00535117
Iteration 16/25 | Loss: 0.00535117
Iteration 17/25 | Loss: 0.00535116
Iteration 18/25 | Loss: 0.00535116
Iteration 19/25 | Loss: 0.00535117
Iteration 20/25 | Loss: 0.00535117
Iteration 21/25 | Loss: 0.00535117
Iteration 22/25 | Loss: 0.00535116
Iteration 23/25 | Loss: 0.00535116
Iteration 24/25 | Loss: 0.00535116
Iteration 25/25 | Loss: 0.00535116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00535116
Iteration 2/1000 | Loss: 0.00320857
Iteration 3/1000 | Loss: 0.00162090
Iteration 4/1000 | Loss: 0.00157988
Iteration 5/1000 | Loss: 0.00186033
Iteration 6/1000 | Loss: 0.00302653
Iteration 7/1000 | Loss: 0.00408151
Iteration 8/1000 | Loss: 0.00885710
Iteration 9/1000 | Loss: 0.00719124
Iteration 10/1000 | Loss: 0.00420830
Iteration 11/1000 | Loss: 0.00120328
Iteration 12/1000 | Loss: 0.00352128
Iteration 13/1000 | Loss: 0.00307161
Iteration 14/1000 | Loss: 0.00259867
Iteration 15/1000 | Loss: 0.00275889
Iteration 16/1000 | Loss: 0.00176528
Iteration 17/1000 | Loss: 0.00309919
Iteration 18/1000 | Loss: 0.00138199
Iteration 19/1000 | Loss: 0.00085385
Iteration 20/1000 | Loss: 0.00107985
Iteration 21/1000 | Loss: 0.00066293
Iteration 22/1000 | Loss: 0.00053403
Iteration 23/1000 | Loss: 0.00101852
Iteration 24/1000 | Loss: 0.00104794
Iteration 25/1000 | Loss: 0.00087030
Iteration 26/1000 | Loss: 0.00064344
Iteration 27/1000 | Loss: 0.00074221
Iteration 28/1000 | Loss: 0.00071519
Iteration 29/1000 | Loss: 0.00018067
Iteration 30/1000 | Loss: 0.00016670
Iteration 31/1000 | Loss: 0.00045675
Iteration 32/1000 | Loss: 0.00017572
Iteration 33/1000 | Loss: 0.00014379
Iteration 34/1000 | Loss: 0.00013874
Iteration 35/1000 | Loss: 0.00062745
Iteration 36/1000 | Loss: 0.00030816
Iteration 37/1000 | Loss: 0.00013175
Iteration 38/1000 | Loss: 0.00057100
Iteration 39/1000 | Loss: 0.00044405
Iteration 40/1000 | Loss: 0.00053352
Iteration 41/1000 | Loss: 0.00060621
Iteration 42/1000 | Loss: 0.00061454
Iteration 43/1000 | Loss: 0.00060630
Iteration 44/1000 | Loss: 0.00104827
Iteration 45/1000 | Loss: 0.00098476
Iteration 46/1000 | Loss: 0.00069125
Iteration 47/1000 | Loss: 0.00202920
Iteration 48/1000 | Loss: 0.00099538
Iteration 49/1000 | Loss: 0.00365499
Iteration 50/1000 | Loss: 0.00113429
Iteration 51/1000 | Loss: 0.00049855
Iteration 52/1000 | Loss: 0.00062999
Iteration 53/1000 | Loss: 0.00147756
Iteration 54/1000 | Loss: 0.00053208
Iteration 55/1000 | Loss: 0.00025415
Iteration 56/1000 | Loss: 0.00053259
Iteration 57/1000 | Loss: 0.00016480
Iteration 58/1000 | Loss: 0.00159409
Iteration 59/1000 | Loss: 0.00016198
Iteration 60/1000 | Loss: 0.00012766
Iteration 61/1000 | Loss: 0.00011982
Iteration 62/1000 | Loss: 0.00011293
Iteration 63/1000 | Loss: 0.00010948
Iteration 64/1000 | Loss: 0.00010854
Iteration 65/1000 | Loss: 0.00010637
Iteration 66/1000 | Loss: 0.00018622
Iteration 67/1000 | Loss: 0.00109108
Iteration 68/1000 | Loss: 0.00023230
Iteration 69/1000 | Loss: 0.00026038
Iteration 70/1000 | Loss: 0.00012263
Iteration 71/1000 | Loss: 0.00063209
Iteration 72/1000 | Loss: 0.00011313
Iteration 73/1000 | Loss: 0.00018699
Iteration 74/1000 | Loss: 0.00010597
Iteration 75/1000 | Loss: 0.00010413
Iteration 76/1000 | Loss: 0.00010288
Iteration 77/1000 | Loss: 0.00010118
Iteration 78/1000 | Loss: 0.00009975
Iteration 79/1000 | Loss: 0.00009962
Iteration 80/1000 | Loss: 0.00027730
Iteration 81/1000 | Loss: 0.00012973
Iteration 82/1000 | Loss: 0.00029372
Iteration 83/1000 | Loss: 0.00012611
Iteration 84/1000 | Loss: 0.00010639
Iteration 85/1000 | Loss: 0.00019348
Iteration 86/1000 | Loss: 0.00011130
Iteration 87/1000 | Loss: 0.00012486
Iteration 88/1000 | Loss: 0.00010787
Iteration 89/1000 | Loss: 0.00010704
Iteration 90/1000 | Loss: 0.00009700
Iteration 91/1000 | Loss: 0.00009764
Iteration 92/1000 | Loss: 0.00009719
Iteration 93/1000 | Loss: 0.00035036
Iteration 94/1000 | Loss: 0.00023359
Iteration 95/1000 | Loss: 0.00009736
Iteration 96/1000 | Loss: 0.00009532
Iteration 97/1000 | Loss: 0.00030320
Iteration 98/1000 | Loss: 0.00009582
Iteration 99/1000 | Loss: 0.00021512
Iteration 100/1000 | Loss: 0.00009989
Iteration 101/1000 | Loss: 0.00009801
Iteration 102/1000 | Loss: 0.00009536
Iteration 103/1000 | Loss: 0.00009532
Iteration 104/1000 | Loss: 0.00009339
Iteration 105/1000 | Loss: 0.00009182
Iteration 106/1000 | Loss: 0.00009281
Iteration 107/1000 | Loss: 0.00009153
Iteration 108/1000 | Loss: 0.00009186
Iteration 109/1000 | Loss: 0.00009081
Iteration 110/1000 | Loss: 0.00020734
Iteration 111/1000 | Loss: 0.00010006
Iteration 112/1000 | Loss: 0.00009406
Iteration 113/1000 | Loss: 0.00009284
Iteration 114/1000 | Loss: 0.00009045
Iteration 115/1000 | Loss: 0.00052654
Iteration 116/1000 | Loss: 0.00041967
Iteration 117/1000 | Loss: 0.00010369
Iteration 118/1000 | Loss: 0.00039887
Iteration 119/1000 | Loss: 0.00008798
Iteration 120/1000 | Loss: 0.00008765
Iteration 121/1000 | Loss: 0.00008698
Iteration 122/1000 | Loss: 0.00008601
Iteration 123/1000 | Loss: 0.00008518
Iteration 124/1000 | Loss: 0.00008666
Iteration 125/1000 | Loss: 0.00008674
Iteration 126/1000 | Loss: 0.00008661
Iteration 127/1000 | Loss: 0.00008628
Iteration 128/1000 | Loss: 0.00008580
Iteration 129/1000 | Loss: 0.00075401
Iteration 130/1000 | Loss: 0.00015042
Iteration 131/1000 | Loss: 0.00069883
Iteration 132/1000 | Loss: 0.00020749
Iteration 133/1000 | Loss: 0.00034747
Iteration 134/1000 | Loss: 0.00028896
Iteration 135/1000 | Loss: 0.00044165
Iteration 136/1000 | Loss: 0.00013895
Iteration 137/1000 | Loss: 0.00032745
Iteration 138/1000 | Loss: 0.00008795
Iteration 139/1000 | Loss: 0.00008368
Iteration 140/1000 | Loss: 0.00008372
Iteration 141/1000 | Loss: 0.00008398
Iteration 142/1000 | Loss: 0.00008347
Iteration 143/1000 | Loss: 0.00008376
Iteration 144/1000 | Loss: 0.00008306
Iteration 145/1000 | Loss: 0.00008313
Iteration 146/1000 | Loss: 0.00008271
Iteration 147/1000 | Loss: 0.00008303
Iteration 148/1000 | Loss: 0.00040913
Iteration 149/1000 | Loss: 0.00008563
Iteration 150/1000 | Loss: 0.00018387
Iteration 151/1000 | Loss: 0.00011419
Iteration 152/1000 | Loss: 0.00008311
Iteration 153/1000 | Loss: 0.00008104
Iteration 154/1000 | Loss: 0.00008350
Iteration 155/1000 | Loss: 0.00008185
Iteration 156/1000 | Loss: 0.00008081
Iteration 157/1000 | Loss: 0.00008268
Iteration 158/1000 | Loss: 0.00008157
Iteration 159/1000 | Loss: 0.00008092
Iteration 160/1000 | Loss: 0.00008020
Iteration 161/1000 | Loss: 0.00007866
Iteration 162/1000 | Loss: 0.00008109
Iteration 163/1000 | Loss: 0.00008047
Iteration 164/1000 | Loss: 0.00008057
Iteration 165/1000 | Loss: 0.00032078
Iteration 166/1000 | Loss: 0.00164136
Iteration 167/1000 | Loss: 0.00209454
Iteration 168/1000 | Loss: 0.00217950
Iteration 169/1000 | Loss: 0.00146281
Iteration 170/1000 | Loss: 0.00181614
Iteration 171/1000 | Loss: 0.00053325
Iteration 172/1000 | Loss: 0.00046578
Iteration 173/1000 | Loss: 0.00012133
Iteration 174/1000 | Loss: 0.00009288
Iteration 175/1000 | Loss: 0.00008205
Iteration 176/1000 | Loss: 0.00007576
Iteration 177/1000 | Loss: 0.00052504
Iteration 178/1000 | Loss: 0.00039531
Iteration 179/1000 | Loss: 0.00008009
Iteration 180/1000 | Loss: 0.00006974
Iteration 181/1000 | Loss: 0.00006626
Iteration 182/1000 | Loss: 0.00006632
Iteration 183/1000 | Loss: 0.00006334
Iteration 184/1000 | Loss: 0.00006333
Iteration 185/1000 | Loss: 0.00006125
Iteration 186/1000 | Loss: 0.00009973
Iteration 187/1000 | Loss: 0.00005908
Iteration 188/1000 | Loss: 0.00005826
Iteration 189/1000 | Loss: 0.00005764
Iteration 190/1000 | Loss: 0.00005698
Iteration 191/1000 | Loss: 0.00017989
Iteration 192/1000 | Loss: 0.00005676
Iteration 193/1000 | Loss: 0.00005604
Iteration 194/1000 | Loss: 0.00005578
Iteration 195/1000 | Loss: 0.00005561
Iteration 196/1000 | Loss: 0.00005530
Iteration 197/1000 | Loss: 0.00005513
Iteration 198/1000 | Loss: 0.00005511
Iteration 199/1000 | Loss: 0.00005494
Iteration 200/1000 | Loss: 0.00005488
Iteration 201/1000 | Loss: 0.00005484
Iteration 202/1000 | Loss: 0.00005483
Iteration 203/1000 | Loss: 0.00005482
Iteration 204/1000 | Loss: 0.00005481
Iteration 205/1000 | Loss: 0.00005481
Iteration 206/1000 | Loss: 0.00005480
Iteration 207/1000 | Loss: 0.00005480
Iteration 208/1000 | Loss: 0.00005480
Iteration 209/1000 | Loss: 0.00005479
Iteration 210/1000 | Loss: 0.00005479
Iteration 211/1000 | Loss: 0.00005479
Iteration 212/1000 | Loss: 0.00005479
Iteration 213/1000 | Loss: 0.00005479
Iteration 214/1000 | Loss: 0.00005478
Iteration 215/1000 | Loss: 0.00005478
Iteration 216/1000 | Loss: 0.00005478
Iteration 217/1000 | Loss: 0.00005478
Iteration 218/1000 | Loss: 0.00005478
Iteration 219/1000 | Loss: 0.00005477
Iteration 220/1000 | Loss: 0.00005477
Iteration 221/1000 | Loss: 0.00005477
Iteration 222/1000 | Loss: 0.00005476
Iteration 223/1000 | Loss: 0.00005476
Iteration 224/1000 | Loss: 0.00005475
Iteration 225/1000 | Loss: 0.00005475
Iteration 226/1000 | Loss: 0.00005475
Iteration 227/1000 | Loss: 0.00005475
Iteration 228/1000 | Loss: 0.00005475
Iteration 229/1000 | Loss: 0.00005475
Iteration 230/1000 | Loss: 0.00005474
Iteration 231/1000 | Loss: 0.00005474
Iteration 232/1000 | Loss: 0.00005474
Iteration 233/1000 | Loss: 0.00005474
Iteration 234/1000 | Loss: 0.00005474
Iteration 235/1000 | Loss: 0.00005474
Iteration 236/1000 | Loss: 0.00005474
Iteration 237/1000 | Loss: 0.00005474
Iteration 238/1000 | Loss: 0.00005474
Iteration 239/1000 | Loss: 0.00005474
Iteration 240/1000 | Loss: 0.00005474
Iteration 241/1000 | Loss: 0.00005474
Iteration 242/1000 | Loss: 0.00005474
Iteration 243/1000 | Loss: 0.00005473
Iteration 244/1000 | Loss: 0.00005473
Iteration 245/1000 | Loss: 0.00005473
Iteration 246/1000 | Loss: 0.00005473
Iteration 247/1000 | Loss: 0.00005473
Iteration 248/1000 | Loss: 0.00005473
Iteration 249/1000 | Loss: 0.00005473
Iteration 250/1000 | Loss: 0.00005473
Iteration 251/1000 | Loss: 0.00005473
Iteration 252/1000 | Loss: 0.00005473
Iteration 253/1000 | Loss: 0.00005472
Iteration 254/1000 | Loss: 0.00005472
Iteration 255/1000 | Loss: 0.00005472
Iteration 256/1000 | Loss: 0.00005472
Iteration 257/1000 | Loss: 0.00005472
Iteration 258/1000 | Loss: 0.00005472
Iteration 259/1000 | Loss: 0.00005472
Iteration 260/1000 | Loss: 0.00005472
Iteration 261/1000 | Loss: 0.00005472
Iteration 262/1000 | Loss: 0.00005472
Iteration 263/1000 | Loss: 0.00005472
Iteration 264/1000 | Loss: 0.00005472
Iteration 265/1000 | Loss: 0.00005472
Iteration 266/1000 | Loss: 0.00005472
Iteration 267/1000 | Loss: 0.00005471
Iteration 268/1000 | Loss: 0.00005471
Iteration 269/1000 | Loss: 0.00005471
Iteration 270/1000 | Loss: 0.00005471
Iteration 271/1000 | Loss: 0.00005471
Iteration 272/1000 | Loss: 0.00005470
Iteration 273/1000 | Loss: 0.00005470
Iteration 274/1000 | Loss: 0.00005470
Iteration 275/1000 | Loss: 0.00005470
Iteration 276/1000 | Loss: 0.00005469
Iteration 277/1000 | Loss: 0.00005469
Iteration 278/1000 | Loss: 0.00005469
Iteration 279/1000 | Loss: 0.00005468
Iteration 280/1000 | Loss: 0.00005468
Iteration 281/1000 | Loss: 0.00005468
Iteration 282/1000 | Loss: 0.00005468
Iteration 283/1000 | Loss: 0.00005467
Iteration 284/1000 | Loss: 0.00005467
Iteration 285/1000 | Loss: 0.00005467
Iteration 286/1000 | Loss: 0.00005467
Iteration 287/1000 | Loss: 0.00005467
Iteration 288/1000 | Loss: 0.00005467
Iteration 289/1000 | Loss: 0.00005466
Iteration 290/1000 | Loss: 0.00005466
Iteration 291/1000 | Loss: 0.00005466
Iteration 292/1000 | Loss: 0.00005466
Iteration 293/1000 | Loss: 0.00005466
Iteration 294/1000 | Loss: 0.00005466
Iteration 295/1000 | Loss: 0.00005465
Iteration 296/1000 | Loss: 0.00005465
Iteration 297/1000 | Loss: 0.00005465
Iteration 298/1000 | Loss: 0.00005464
Iteration 299/1000 | Loss: 0.00005464
Iteration 300/1000 | Loss: 0.00005464
Iteration 301/1000 | Loss: 0.00005463
Iteration 302/1000 | Loss: 0.00005463
Iteration 303/1000 | Loss: 0.00005463
Iteration 304/1000 | Loss: 0.00005463
Iteration 305/1000 | Loss: 0.00005462
Iteration 306/1000 | Loss: 0.00005462
Iteration 307/1000 | Loss: 0.00005462
Iteration 308/1000 | Loss: 0.00005462
Iteration 309/1000 | Loss: 0.00005461
Iteration 310/1000 | Loss: 0.00005461
Iteration 311/1000 | Loss: 0.00005461
Iteration 312/1000 | Loss: 0.00005461
Iteration 313/1000 | Loss: 0.00005460
Iteration 314/1000 | Loss: 0.00005460
Iteration 315/1000 | Loss: 0.00005459
Iteration 316/1000 | Loss: 0.00005458
Iteration 317/1000 | Loss: 0.00005458
Iteration 318/1000 | Loss: 0.00005458
Iteration 319/1000 | Loss: 0.00005458
Iteration 320/1000 | Loss: 0.00005457
Iteration 321/1000 | Loss: 0.00005457
Iteration 322/1000 | Loss: 0.00005457
Iteration 323/1000 | Loss: 0.00005456
Iteration 324/1000 | Loss: 0.00005456
Iteration 325/1000 | Loss: 0.00005456
Iteration 326/1000 | Loss: 0.00005455
Iteration 327/1000 | Loss: 0.00005454
Iteration 328/1000 | Loss: 0.00005454
Iteration 329/1000 | Loss: 0.00005454
Iteration 330/1000 | Loss: 0.00005452
Iteration 331/1000 | Loss: 0.00005452
Iteration 332/1000 | Loss: 0.00005452
Iteration 333/1000 | Loss: 0.00005452
Iteration 334/1000 | Loss: 0.00005452
Iteration 335/1000 | Loss: 0.00005452
Iteration 336/1000 | Loss: 0.00005452
Iteration 337/1000 | Loss: 0.00005452
Iteration 338/1000 | Loss: 0.00005452
Iteration 339/1000 | Loss: 0.00005451
Iteration 340/1000 | Loss: 0.00023869
Iteration 341/1000 | Loss: 0.00005754
Iteration 342/1000 | Loss: 0.00005561
Iteration 343/1000 | Loss: 0.00005472
Iteration 344/1000 | Loss: 0.00005386
Iteration 345/1000 | Loss: 0.00005327
Iteration 346/1000 | Loss: 0.00005300
Iteration 347/1000 | Loss: 0.00005283
Iteration 348/1000 | Loss: 0.00005282
Iteration 349/1000 | Loss: 0.00005279
Iteration 350/1000 | Loss: 0.00005278
Iteration 351/1000 | Loss: 0.00005278
Iteration 352/1000 | Loss: 0.00005277
Iteration 353/1000 | Loss: 0.00005276
Iteration 354/1000 | Loss: 0.00005276
Iteration 355/1000 | Loss: 0.00005275
Iteration 356/1000 | Loss: 0.00005274
Iteration 357/1000 | Loss: 0.00005267
Iteration 358/1000 | Loss: 0.00005267
Iteration 359/1000 | Loss: 0.00005267
Iteration 360/1000 | Loss: 0.00005266
Iteration 361/1000 | Loss: 0.00005266
Iteration 362/1000 | Loss: 0.00005265
Iteration 363/1000 | Loss: 0.00005264
Iteration 364/1000 | Loss: 0.00005263
Iteration 365/1000 | Loss: 0.00005262
Iteration 366/1000 | Loss: 0.00005261
Iteration 367/1000 | Loss: 0.00005261
Iteration 368/1000 | Loss: 0.00005261
Iteration 369/1000 | Loss: 0.00005261
Iteration 370/1000 | Loss: 0.00005261
Iteration 371/1000 | Loss: 0.00005261
Iteration 372/1000 | Loss: 0.00005261
Iteration 373/1000 | Loss: 0.00005260
Iteration 374/1000 | Loss: 0.00005260
Iteration 375/1000 | Loss: 0.00005260
Iteration 376/1000 | Loss: 0.00005260
Iteration 377/1000 | Loss: 0.00005259
Iteration 378/1000 | Loss: 0.00005259
Iteration 379/1000 | Loss: 0.00005259
Iteration 380/1000 | Loss: 0.00005259
Iteration 381/1000 | Loss: 0.00005259
Iteration 382/1000 | Loss: 0.00005258
Iteration 383/1000 | Loss: 0.00005258
Iteration 384/1000 | Loss: 0.00005258
Iteration 385/1000 | Loss: 0.00005258
Iteration 386/1000 | Loss: 0.00005258
Iteration 387/1000 | Loss: 0.00005257
Iteration 388/1000 | Loss: 0.00005257
Iteration 389/1000 | Loss: 0.00005256
Iteration 390/1000 | Loss: 0.00005256
Iteration 391/1000 | Loss: 0.00005256
Iteration 392/1000 | Loss: 0.00005255
Iteration 393/1000 | Loss: 0.00005255
Iteration 394/1000 | Loss: 0.00005255
Iteration 395/1000 | Loss: 0.00005255
Iteration 396/1000 | Loss: 0.00005255
Iteration 397/1000 | Loss: 0.00005255
Iteration 398/1000 | Loss: 0.00005255
Iteration 399/1000 | Loss: 0.00005255
Iteration 400/1000 | Loss: 0.00005254
Iteration 401/1000 | Loss: 0.00005254
Iteration 402/1000 | Loss: 0.00005254
Iteration 403/1000 | Loss: 0.00005254
Iteration 404/1000 | Loss: 0.00005254
Iteration 405/1000 | Loss: 0.00005254
Iteration 406/1000 | Loss: 0.00005254
Iteration 407/1000 | Loss: 0.00005254
Iteration 408/1000 | Loss: 0.00005253
Iteration 409/1000 | Loss: 0.00005253
Iteration 410/1000 | Loss: 0.00005253
Iteration 411/1000 | Loss: 0.00005253
Iteration 412/1000 | Loss: 0.00005252
Iteration 413/1000 | Loss: 0.00005252
Iteration 414/1000 | Loss: 0.00005252
Iteration 415/1000 | Loss: 0.00005252
Iteration 416/1000 | Loss: 0.00005252
Iteration 417/1000 | Loss: 0.00005251
Iteration 418/1000 | Loss: 0.00005251
Iteration 419/1000 | Loss: 0.00005251
Iteration 420/1000 | Loss: 0.00005251
Iteration 421/1000 | Loss: 0.00005251
Iteration 422/1000 | Loss: 0.00005251
Iteration 423/1000 | Loss: 0.00005251
Iteration 424/1000 | Loss: 0.00005251
Iteration 425/1000 | Loss: 0.00005251
Iteration 426/1000 | Loss: 0.00005251
Iteration 427/1000 | Loss: 0.00005251
Iteration 428/1000 | Loss: 0.00005251
Iteration 429/1000 | Loss: 0.00005251
Iteration 430/1000 | Loss: 0.00005251
Iteration 431/1000 | Loss: 0.00005251
Iteration 432/1000 | Loss: 0.00005251
Iteration 433/1000 | Loss: 0.00005251
Iteration 434/1000 | Loss: 0.00005251
Iteration 435/1000 | Loss: 0.00005250
Iteration 436/1000 | Loss: 0.00005250
Iteration 437/1000 | Loss: 0.00005250
Iteration 438/1000 | Loss: 0.00005250
Iteration 439/1000 | Loss: 0.00005250
Iteration 440/1000 | Loss: 0.00005250
Iteration 441/1000 | Loss: 0.00005250
Iteration 442/1000 | Loss: 0.00005250
Iteration 443/1000 | Loss: 0.00005249
Iteration 444/1000 | Loss: 0.00005249
Iteration 445/1000 | Loss: 0.00005249
Iteration 446/1000 | Loss: 0.00005249
Iteration 447/1000 | Loss: 0.00005249
Iteration 448/1000 | Loss: 0.00005249
Iteration 449/1000 | Loss: 0.00005249
Iteration 450/1000 | Loss: 0.00005249
Iteration 451/1000 | Loss: 0.00005249
Iteration 452/1000 | Loss: 0.00005248
Iteration 453/1000 | Loss: 0.00005248
Iteration 454/1000 | Loss: 0.00005248
Iteration 455/1000 | Loss: 0.00005248
Iteration 456/1000 | Loss: 0.00005248
Iteration 457/1000 | Loss: 0.00005248
Iteration 458/1000 | Loss: 0.00005248
Iteration 459/1000 | Loss: 0.00005248
Iteration 460/1000 | Loss: 0.00005248
Iteration 461/1000 | Loss: 0.00005247
Iteration 462/1000 | Loss: 0.00005247
Iteration 463/1000 | Loss: 0.00005247
Iteration 464/1000 | Loss: 0.00005247
Iteration 465/1000 | Loss: 0.00005247
Iteration 466/1000 | Loss: 0.00005247
Iteration 467/1000 | Loss: 0.00005247
Iteration 468/1000 | Loss: 0.00005247
Iteration 469/1000 | Loss: 0.00005247
Iteration 470/1000 | Loss: 0.00005247
Iteration 471/1000 | Loss: 0.00005247
Iteration 472/1000 | Loss: 0.00005246
Iteration 473/1000 | Loss: 0.00005246
Iteration 474/1000 | Loss: 0.00005246
Iteration 475/1000 | Loss: 0.00005246
Iteration 476/1000 | Loss: 0.00005246
Iteration 477/1000 | Loss: 0.00005246
Iteration 478/1000 | Loss: 0.00005246
Iteration 479/1000 | Loss: 0.00005246
Iteration 480/1000 | Loss: 0.00005246
Iteration 481/1000 | Loss: 0.00005246
Iteration 482/1000 | Loss: 0.00005246
Iteration 483/1000 | Loss: 0.00005245
Iteration 484/1000 | Loss: 0.00005245
Iteration 485/1000 | Loss: 0.00005245
Iteration 486/1000 | Loss: 0.00005245
Iteration 487/1000 | Loss: 0.00005245
Iteration 488/1000 | Loss: 0.00005245
Iteration 489/1000 | Loss: 0.00005245
Iteration 490/1000 | Loss: 0.00005245
Iteration 491/1000 | Loss: 0.00005245
Iteration 492/1000 | Loss: 0.00005245
Iteration 493/1000 | Loss: 0.00005245
Iteration 494/1000 | Loss: 0.00005245
Iteration 495/1000 | Loss: 0.00005245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 495. Stopping optimization.
Last 5 losses: [5.245253487373702e-05, 5.245253487373702e-05, 5.245253487373702e-05, 5.245253487373702e-05, 5.245253487373702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.245253487373702e-05

Optimization complete. Final v2v error: 4.052256107330322 mm

Highest mean error: 11.643229484558105 mm for frame 76

Lowest mean error: 2.6430885791778564 mm for frame 154

Saving results

Total time: 411.5167407989502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009782
Iteration 2/25 | Loss: 0.00156797
Iteration 3/25 | Loss: 0.00136412
Iteration 4/25 | Loss: 0.00130231
Iteration 5/25 | Loss: 0.00130761
Iteration 6/25 | Loss: 0.00128560
Iteration 7/25 | Loss: 0.00126970
Iteration 8/25 | Loss: 0.00126468
Iteration 9/25 | Loss: 0.00126377
Iteration 10/25 | Loss: 0.00123637
Iteration 11/25 | Loss: 0.00122615
Iteration 12/25 | Loss: 0.00123106
Iteration 13/25 | Loss: 0.00122766
Iteration 14/25 | Loss: 0.00120426
Iteration 15/25 | Loss: 0.00119736
Iteration 16/25 | Loss: 0.00119195
Iteration 17/25 | Loss: 0.00119249
Iteration 18/25 | Loss: 0.00119209
Iteration 19/25 | Loss: 0.00119386
Iteration 20/25 | Loss: 0.00119214
Iteration 21/25 | Loss: 0.00119233
Iteration 22/25 | Loss: 0.00119076
Iteration 23/25 | Loss: 0.00118891
Iteration 24/25 | Loss: 0.00118985
Iteration 25/25 | Loss: 0.00118982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.95363355
Iteration 2/25 | Loss: 0.00116650
Iteration 3/25 | Loss: 0.00116650
Iteration 4/25 | Loss: 0.00109821
Iteration 5/25 | Loss: 0.00109820
Iteration 6/25 | Loss: 0.00109819
Iteration 7/25 | Loss: 0.00109819
Iteration 8/25 | Loss: 0.00109819
Iteration 9/25 | Loss: 0.00109819
Iteration 10/25 | Loss: 0.00109819
Iteration 11/25 | Loss: 0.00109819
Iteration 12/25 | Loss: 0.00109819
Iteration 13/25 | Loss: 0.00109819
Iteration 14/25 | Loss: 0.00109819
Iteration 15/25 | Loss: 0.00109819
Iteration 16/25 | Loss: 0.00109819
Iteration 17/25 | Loss: 0.00109819
Iteration 18/25 | Loss: 0.00109819
Iteration 19/25 | Loss: 0.00109819
Iteration 20/25 | Loss: 0.00109819
Iteration 21/25 | Loss: 0.00109819
Iteration 22/25 | Loss: 0.00109819
Iteration 23/25 | Loss: 0.00109819
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010981913655996323, 0.0010981913655996323, 0.0010981913655996323, 0.0010981913655996323, 0.0010981913655996323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010981913655996323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109819
Iteration 2/1000 | Loss: 0.00018694
Iteration 3/1000 | Loss: 0.00020612
Iteration 4/1000 | Loss: 0.00002476
Iteration 5/1000 | Loss: 0.00005133
Iteration 6/1000 | Loss: 0.00015190
Iteration 7/1000 | Loss: 0.00032181
Iteration 8/1000 | Loss: 0.00011022
Iteration 9/1000 | Loss: 0.00012877
Iteration 10/1000 | Loss: 0.00010776
Iteration 11/1000 | Loss: 0.00012339
Iteration 12/1000 | Loss: 0.00016761
Iteration 13/1000 | Loss: 0.00014352
Iteration 14/1000 | Loss: 0.00002003
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001508
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00009692
Iteration 19/1000 | Loss: 0.00001627
Iteration 20/1000 | Loss: 0.00014949
Iteration 21/1000 | Loss: 0.00005881
Iteration 22/1000 | Loss: 0.00007672
Iteration 23/1000 | Loss: 0.00006460
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00014942
Iteration 26/1000 | Loss: 0.00006718
Iteration 27/1000 | Loss: 0.00012643
Iteration 28/1000 | Loss: 0.00011574
Iteration 29/1000 | Loss: 0.00011140
Iteration 30/1000 | Loss: 0.00016597
Iteration 31/1000 | Loss: 0.00010855
Iteration 32/1000 | Loss: 0.00002646
Iteration 33/1000 | Loss: 0.00016004
Iteration 34/1000 | Loss: 0.00014970
Iteration 35/1000 | Loss: 0.00001919
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00017521
Iteration 39/1000 | Loss: 0.00010721
Iteration 40/1000 | Loss: 0.00015058
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001566
Iteration 43/1000 | Loss: 0.00014073
Iteration 44/1000 | Loss: 0.00005162
Iteration 45/1000 | Loss: 0.00002053
Iteration 46/1000 | Loss: 0.00001729
Iteration 47/1000 | Loss: 0.00006710
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001818
Iteration 50/1000 | Loss: 0.00002379
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00002332
Iteration 57/1000 | Loss: 0.00001296
Iteration 58/1000 | Loss: 0.00001292
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00003759
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001257
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001256
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001250
Iteration 81/1000 | Loss: 0.00001250
Iteration 82/1000 | Loss: 0.00001250
Iteration 83/1000 | Loss: 0.00001249
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00003253
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001243
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001240
Iteration 96/1000 | Loss: 0.00001240
Iteration 97/1000 | Loss: 0.00001240
Iteration 98/1000 | Loss: 0.00001240
Iteration 99/1000 | Loss: 0.00001240
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001240
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001239
Iteration 106/1000 | Loss: 0.00001239
Iteration 107/1000 | Loss: 0.00001239
Iteration 108/1000 | Loss: 0.00001239
Iteration 109/1000 | Loss: 0.00001239
Iteration 110/1000 | Loss: 0.00001239
Iteration 111/1000 | Loss: 0.00001239
Iteration 112/1000 | Loss: 0.00001239
Iteration 113/1000 | Loss: 0.00001239
Iteration 114/1000 | Loss: 0.00001239
Iteration 115/1000 | Loss: 0.00001239
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001239
Iteration 120/1000 | Loss: 0.00001239
Iteration 121/1000 | Loss: 0.00001239
Iteration 122/1000 | Loss: 0.00001239
Iteration 123/1000 | Loss: 0.00001239
Iteration 124/1000 | Loss: 0.00001239
Iteration 125/1000 | Loss: 0.00001239
Iteration 126/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.2386954040266573e-05, 1.2386954040266573e-05, 1.2386954040266573e-05, 1.2386954040266573e-05, 1.2386954040266573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2386954040266573e-05

Optimization complete. Final v2v error: 2.960759401321411 mm

Highest mean error: 5.3418474197387695 mm for frame 138

Lowest mean error: 2.650993585586548 mm for frame 73

Saving results

Total time: 158.29936909675598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485003
Iteration 2/25 | Loss: 0.00127521
Iteration 3/25 | Loss: 0.00119642
Iteration 4/25 | Loss: 0.00118541
Iteration 5/25 | Loss: 0.00118235
Iteration 6/25 | Loss: 0.00118192
Iteration 7/25 | Loss: 0.00118192
Iteration 8/25 | Loss: 0.00118192
Iteration 9/25 | Loss: 0.00118192
Iteration 10/25 | Loss: 0.00118192
Iteration 11/25 | Loss: 0.00118192
Iteration 12/25 | Loss: 0.00118192
Iteration 13/25 | Loss: 0.00118192
Iteration 14/25 | Loss: 0.00118192
Iteration 15/25 | Loss: 0.00118192
Iteration 16/25 | Loss: 0.00118192
Iteration 17/25 | Loss: 0.00118192
Iteration 18/25 | Loss: 0.00118192
Iteration 19/25 | Loss: 0.00118192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011819233186542988, 0.0011819233186542988, 0.0011819233186542988, 0.0011819233186542988, 0.0011819233186542988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011819233186542988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38863444
Iteration 2/25 | Loss: 0.00081065
Iteration 3/25 | Loss: 0.00081061
Iteration 4/25 | Loss: 0.00081061
Iteration 5/25 | Loss: 0.00081061
Iteration 6/25 | Loss: 0.00081061
Iteration 7/25 | Loss: 0.00081061
Iteration 8/25 | Loss: 0.00081061
Iteration 9/25 | Loss: 0.00081061
Iteration 10/25 | Loss: 0.00081061
Iteration 11/25 | Loss: 0.00081061
Iteration 12/25 | Loss: 0.00081061
Iteration 13/25 | Loss: 0.00081061
Iteration 14/25 | Loss: 0.00081061
Iteration 15/25 | Loss: 0.00081061
Iteration 16/25 | Loss: 0.00081061
Iteration 17/25 | Loss: 0.00081061
Iteration 18/25 | Loss: 0.00081061
Iteration 19/25 | Loss: 0.00081061
Iteration 20/25 | Loss: 0.00081061
Iteration 21/25 | Loss: 0.00081061
Iteration 22/25 | Loss: 0.00081061
Iteration 23/25 | Loss: 0.00081061
Iteration 24/25 | Loss: 0.00081061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008106101886369288, 0.0008106101886369288, 0.0008106101886369288, 0.0008106101886369288, 0.0008106101886369288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008106101886369288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081061
Iteration 2/1000 | Loss: 0.00002813
Iteration 3/1000 | Loss: 0.00001992
Iteration 4/1000 | Loss: 0.00001685
Iteration 5/1000 | Loss: 0.00001589
Iteration 6/1000 | Loss: 0.00001522
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001450
Iteration 9/1000 | Loss: 0.00001436
Iteration 10/1000 | Loss: 0.00001415
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001374
Iteration 14/1000 | Loss: 0.00001367
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001366
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001355
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001335
Iteration 51/1000 | Loss: 0.00001335
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001334
Iteration 57/1000 | Loss: 0.00001334
Iteration 58/1000 | Loss: 0.00001334
Iteration 59/1000 | Loss: 0.00001334
Iteration 60/1000 | Loss: 0.00001334
Iteration 61/1000 | Loss: 0.00001333
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001331
Iteration 66/1000 | Loss: 0.00001331
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001329
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001328
Iteration 78/1000 | Loss: 0.00001328
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001322
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001320
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001319
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001319
Iteration 111/1000 | Loss: 0.00001319
Iteration 112/1000 | Loss: 0.00001319
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001319
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001317
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001316
Iteration 123/1000 | Loss: 0.00001315
Iteration 124/1000 | Loss: 0.00001315
Iteration 125/1000 | Loss: 0.00001314
Iteration 126/1000 | Loss: 0.00001314
Iteration 127/1000 | Loss: 0.00001313
Iteration 128/1000 | Loss: 0.00001313
Iteration 129/1000 | Loss: 0.00001312
Iteration 130/1000 | Loss: 0.00001312
Iteration 131/1000 | Loss: 0.00001312
Iteration 132/1000 | Loss: 0.00001312
Iteration 133/1000 | Loss: 0.00001312
Iteration 134/1000 | Loss: 0.00001312
Iteration 135/1000 | Loss: 0.00001312
Iteration 136/1000 | Loss: 0.00001310
Iteration 137/1000 | Loss: 0.00001309
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001309
Iteration 140/1000 | Loss: 0.00001309
Iteration 141/1000 | Loss: 0.00001309
Iteration 142/1000 | Loss: 0.00001309
Iteration 143/1000 | Loss: 0.00001309
Iteration 144/1000 | Loss: 0.00001309
Iteration 145/1000 | Loss: 0.00001309
Iteration 146/1000 | Loss: 0.00001309
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001309
Iteration 149/1000 | Loss: 0.00001309
Iteration 150/1000 | Loss: 0.00001308
Iteration 151/1000 | Loss: 0.00001308
Iteration 152/1000 | Loss: 0.00001308
Iteration 153/1000 | Loss: 0.00001308
Iteration 154/1000 | Loss: 0.00001307
Iteration 155/1000 | Loss: 0.00001307
Iteration 156/1000 | Loss: 0.00001307
Iteration 157/1000 | Loss: 0.00001307
Iteration 158/1000 | Loss: 0.00001307
Iteration 159/1000 | Loss: 0.00001307
Iteration 160/1000 | Loss: 0.00001307
Iteration 161/1000 | Loss: 0.00001307
Iteration 162/1000 | Loss: 0.00001307
Iteration 163/1000 | Loss: 0.00001307
Iteration 164/1000 | Loss: 0.00001306
Iteration 165/1000 | Loss: 0.00001306
Iteration 166/1000 | Loss: 0.00001306
Iteration 167/1000 | Loss: 0.00001306
Iteration 168/1000 | Loss: 0.00001306
Iteration 169/1000 | Loss: 0.00001306
Iteration 170/1000 | Loss: 0.00001306
Iteration 171/1000 | Loss: 0.00001306
Iteration 172/1000 | Loss: 0.00001306
Iteration 173/1000 | Loss: 0.00001306
Iteration 174/1000 | Loss: 0.00001306
Iteration 175/1000 | Loss: 0.00001306
Iteration 176/1000 | Loss: 0.00001306
Iteration 177/1000 | Loss: 0.00001306
Iteration 178/1000 | Loss: 0.00001306
Iteration 179/1000 | Loss: 0.00001306
Iteration 180/1000 | Loss: 0.00001306
Iteration 181/1000 | Loss: 0.00001305
Iteration 182/1000 | Loss: 0.00001305
Iteration 183/1000 | Loss: 0.00001305
Iteration 184/1000 | Loss: 0.00001305
Iteration 185/1000 | Loss: 0.00001305
Iteration 186/1000 | Loss: 0.00001305
Iteration 187/1000 | Loss: 0.00001305
Iteration 188/1000 | Loss: 0.00001305
Iteration 189/1000 | Loss: 0.00001305
Iteration 190/1000 | Loss: 0.00001305
Iteration 191/1000 | Loss: 0.00001305
Iteration 192/1000 | Loss: 0.00001305
Iteration 193/1000 | Loss: 0.00001305
Iteration 194/1000 | Loss: 0.00001305
Iteration 195/1000 | Loss: 0.00001304
Iteration 196/1000 | Loss: 0.00001304
Iteration 197/1000 | Loss: 0.00001304
Iteration 198/1000 | Loss: 0.00001304
Iteration 199/1000 | Loss: 0.00001304
Iteration 200/1000 | Loss: 0.00001304
Iteration 201/1000 | Loss: 0.00001304
Iteration 202/1000 | Loss: 0.00001304
Iteration 203/1000 | Loss: 0.00001304
Iteration 204/1000 | Loss: 0.00001304
Iteration 205/1000 | Loss: 0.00001304
Iteration 206/1000 | Loss: 0.00001304
Iteration 207/1000 | Loss: 0.00001304
Iteration 208/1000 | Loss: 0.00001304
Iteration 209/1000 | Loss: 0.00001304
Iteration 210/1000 | Loss: 0.00001304
Iteration 211/1000 | Loss: 0.00001304
Iteration 212/1000 | Loss: 0.00001304
Iteration 213/1000 | Loss: 0.00001304
Iteration 214/1000 | Loss: 0.00001304
Iteration 215/1000 | Loss: 0.00001304
Iteration 216/1000 | Loss: 0.00001304
Iteration 217/1000 | Loss: 0.00001304
Iteration 218/1000 | Loss: 0.00001304
Iteration 219/1000 | Loss: 0.00001304
Iteration 220/1000 | Loss: 0.00001304
Iteration 221/1000 | Loss: 0.00001304
Iteration 222/1000 | Loss: 0.00001304
Iteration 223/1000 | Loss: 0.00001304
Iteration 224/1000 | Loss: 0.00001304
Iteration 225/1000 | Loss: 0.00001304
Iteration 226/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.3040889825788327e-05, 1.3040889825788327e-05, 1.3040889825788327e-05, 1.3040889825788327e-05, 1.3040889825788327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3040889825788327e-05

Optimization complete. Final v2v error: 3.042205810546875 mm

Highest mean error: 3.4781665802001953 mm for frame 48

Lowest mean error: 2.7658708095550537 mm for frame 147

Saving results

Total time: 41.96150279045105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434625
Iteration 2/25 | Loss: 0.00176714
Iteration 3/25 | Loss: 0.00147534
Iteration 4/25 | Loss: 0.00141270
Iteration 5/25 | Loss: 0.00140303
Iteration 6/25 | Loss: 0.00140309
Iteration 7/25 | Loss: 0.00140967
Iteration 8/25 | Loss: 0.00144792
Iteration 9/25 | Loss: 0.00149671
Iteration 10/25 | Loss: 0.00139100
Iteration 11/25 | Loss: 0.00133775
Iteration 12/25 | Loss: 0.00132759
Iteration 13/25 | Loss: 0.00132602
Iteration 14/25 | Loss: 0.00132577
Iteration 15/25 | Loss: 0.00132577
Iteration 16/25 | Loss: 0.00132577
Iteration 17/25 | Loss: 0.00132577
Iteration 18/25 | Loss: 0.00132577
Iteration 19/25 | Loss: 0.00132577
Iteration 20/25 | Loss: 0.00132577
Iteration 21/25 | Loss: 0.00132577
Iteration 22/25 | Loss: 0.00132577
Iteration 23/25 | Loss: 0.00132577
Iteration 24/25 | Loss: 0.00132577
Iteration 25/25 | Loss: 0.00132577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33731294
Iteration 2/25 | Loss: 0.00097357
Iteration 3/25 | Loss: 0.00097357
Iteration 4/25 | Loss: 0.00097356
Iteration 5/25 | Loss: 0.00097356
Iteration 6/25 | Loss: 0.00097356
Iteration 7/25 | Loss: 0.00097356
Iteration 8/25 | Loss: 0.00097356
Iteration 9/25 | Loss: 0.00097356
Iteration 10/25 | Loss: 0.00097356
Iteration 11/25 | Loss: 0.00097356
Iteration 12/25 | Loss: 0.00097356
Iteration 13/25 | Loss: 0.00097356
Iteration 14/25 | Loss: 0.00097356
Iteration 15/25 | Loss: 0.00097356
Iteration 16/25 | Loss: 0.00097356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009735635248944163, 0.0009735635248944163, 0.0009735635248944163, 0.0009735635248944163, 0.0009735635248944163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009735635248944163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097356
Iteration 2/1000 | Loss: 0.00512372
Iteration 3/1000 | Loss: 0.00302740
Iteration 4/1000 | Loss: 0.00334487
Iteration 5/1000 | Loss: 0.00203389
Iteration 6/1000 | Loss: 0.00526268
Iteration 7/1000 | Loss: 0.00296154
Iteration 8/1000 | Loss: 0.00601607
Iteration 9/1000 | Loss: 0.00280704
Iteration 10/1000 | Loss: 0.00660170
Iteration 11/1000 | Loss: 0.00316337
Iteration 12/1000 | Loss: 0.00651677
Iteration 13/1000 | Loss: 0.00286757
Iteration 14/1000 | Loss: 0.00516218
Iteration 15/1000 | Loss: 0.00275676
Iteration 16/1000 | Loss: 0.00289543
Iteration 17/1000 | Loss: 0.00143216
Iteration 18/1000 | Loss: 0.00232505
Iteration 19/1000 | Loss: 0.00023100
Iteration 20/1000 | Loss: 0.00018459
Iteration 21/1000 | Loss: 0.00015575
Iteration 22/1000 | Loss: 0.00037050
Iteration 23/1000 | Loss: 0.00026147
Iteration 24/1000 | Loss: 0.00033151
Iteration 25/1000 | Loss: 0.00031351
Iteration 26/1000 | Loss: 0.00017485
Iteration 27/1000 | Loss: 0.00022931
Iteration 28/1000 | Loss: 0.00028997
Iteration 29/1000 | Loss: 0.00013432
Iteration 30/1000 | Loss: 0.00018914
Iteration 31/1000 | Loss: 0.00037217
Iteration 32/1000 | Loss: 0.00024125
Iteration 33/1000 | Loss: 0.00009223
Iteration 34/1000 | Loss: 0.00023855
Iteration 35/1000 | Loss: 0.00018400
Iteration 36/1000 | Loss: 0.00020899
Iteration 37/1000 | Loss: 0.00010095
Iteration 38/1000 | Loss: 0.00007685
Iteration 39/1000 | Loss: 0.00006681
Iteration 40/1000 | Loss: 0.00006248
Iteration 41/1000 | Loss: 0.00011497
Iteration 42/1000 | Loss: 0.00007172
Iteration 43/1000 | Loss: 0.00023485
Iteration 44/1000 | Loss: 0.00011249
Iteration 45/1000 | Loss: 0.00016137
Iteration 46/1000 | Loss: 0.00010289
Iteration 47/1000 | Loss: 0.00013693
Iteration 48/1000 | Loss: 0.00009368
Iteration 49/1000 | Loss: 0.00012106
Iteration 50/1000 | Loss: 0.00009290
Iteration 51/1000 | Loss: 0.00008502
Iteration 52/1000 | Loss: 0.00006191
Iteration 53/1000 | Loss: 0.00005231
Iteration 54/1000 | Loss: 0.00005099
Iteration 55/1000 | Loss: 0.00005001
Iteration 56/1000 | Loss: 0.00004900
Iteration 57/1000 | Loss: 0.00017246
Iteration 58/1000 | Loss: 0.00006043
Iteration 59/1000 | Loss: 0.00005078
Iteration 60/1000 | Loss: 0.00004883
Iteration 61/1000 | Loss: 0.00004748
Iteration 62/1000 | Loss: 0.00004660
Iteration 63/1000 | Loss: 0.00004576
Iteration 64/1000 | Loss: 0.00004466
Iteration 65/1000 | Loss: 0.00004354
Iteration 66/1000 | Loss: 0.00004289
Iteration 67/1000 | Loss: 0.00004247
Iteration 68/1000 | Loss: 0.00004191
Iteration 69/1000 | Loss: 0.00004167
Iteration 70/1000 | Loss: 0.00004144
Iteration 71/1000 | Loss: 0.00004115
Iteration 72/1000 | Loss: 0.00004090
Iteration 73/1000 | Loss: 0.00004071
Iteration 74/1000 | Loss: 0.00004068
Iteration 75/1000 | Loss: 0.00004062
Iteration 76/1000 | Loss: 0.00004051
Iteration 77/1000 | Loss: 0.00004048
Iteration 78/1000 | Loss: 0.00004046
Iteration 79/1000 | Loss: 0.00004046
Iteration 80/1000 | Loss: 0.00004045
Iteration 81/1000 | Loss: 0.00004045
Iteration 82/1000 | Loss: 0.00004045
Iteration 83/1000 | Loss: 0.00004044
Iteration 84/1000 | Loss: 0.00004044
Iteration 85/1000 | Loss: 0.00004044
Iteration 86/1000 | Loss: 0.00004043
Iteration 87/1000 | Loss: 0.00004043
Iteration 88/1000 | Loss: 0.00004042
Iteration 89/1000 | Loss: 0.00004042
Iteration 90/1000 | Loss: 0.00004042
Iteration 91/1000 | Loss: 0.00004039
Iteration 92/1000 | Loss: 0.00004039
Iteration 93/1000 | Loss: 0.00004035
Iteration 94/1000 | Loss: 0.00004035
Iteration 95/1000 | Loss: 0.00004033
Iteration 96/1000 | Loss: 0.00004033
Iteration 97/1000 | Loss: 0.00004033
Iteration 98/1000 | Loss: 0.00004033
Iteration 99/1000 | Loss: 0.00004032
Iteration 100/1000 | Loss: 0.00004032
Iteration 101/1000 | Loss: 0.00004031
Iteration 102/1000 | Loss: 0.00004031
Iteration 103/1000 | Loss: 0.00004029
Iteration 104/1000 | Loss: 0.00004029
Iteration 105/1000 | Loss: 0.00004028
Iteration 106/1000 | Loss: 0.00004028
Iteration 107/1000 | Loss: 0.00004028
Iteration 108/1000 | Loss: 0.00004028
Iteration 109/1000 | Loss: 0.00004028
Iteration 110/1000 | Loss: 0.00004028
Iteration 111/1000 | Loss: 0.00004028
Iteration 112/1000 | Loss: 0.00004027
Iteration 113/1000 | Loss: 0.00004027
Iteration 114/1000 | Loss: 0.00004027
Iteration 115/1000 | Loss: 0.00004027
Iteration 116/1000 | Loss: 0.00004026
Iteration 117/1000 | Loss: 0.00004026
Iteration 118/1000 | Loss: 0.00004026
Iteration 119/1000 | Loss: 0.00004026
Iteration 120/1000 | Loss: 0.00004026
Iteration 121/1000 | Loss: 0.00004025
Iteration 122/1000 | Loss: 0.00004025
Iteration 123/1000 | Loss: 0.00004025
Iteration 124/1000 | Loss: 0.00004025
Iteration 125/1000 | Loss: 0.00004025
Iteration 126/1000 | Loss: 0.00004025
Iteration 127/1000 | Loss: 0.00004025
Iteration 128/1000 | Loss: 0.00004025
Iteration 129/1000 | Loss: 0.00004025
Iteration 130/1000 | Loss: 0.00004025
Iteration 131/1000 | Loss: 0.00004025
Iteration 132/1000 | Loss: 0.00004025
Iteration 133/1000 | Loss: 0.00004025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [4.025323869427666e-05, 4.025323869427666e-05, 4.025323869427666e-05, 4.025323869427666e-05, 4.025323869427666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.025323869427666e-05

Optimization complete. Final v2v error: 4.748932361602783 mm

Highest mean error: 6.353513717651367 mm for frame 126

Lowest mean error: 4.147977828979492 mm for frame 214

Saving results

Total time: 152.8381371498108
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049849
Iteration 2/25 | Loss: 0.00151799
Iteration 3/25 | Loss: 0.00120990
Iteration 4/25 | Loss: 0.00117428
Iteration 5/25 | Loss: 0.00117749
Iteration 6/25 | Loss: 0.00117178
Iteration 7/25 | Loss: 0.00117065
Iteration 8/25 | Loss: 0.00117368
Iteration 9/25 | Loss: 0.00117358
Iteration 10/25 | Loss: 0.00117084
Iteration 11/25 | Loss: 0.00117239
Iteration 12/25 | Loss: 0.00117835
Iteration 13/25 | Loss: 0.00117341
Iteration 14/25 | Loss: 0.00117371
Iteration 15/25 | Loss: 0.00117747
Iteration 16/25 | Loss: 0.00117555
Iteration 17/25 | Loss: 0.00117289
Iteration 18/25 | Loss: 0.00117340
Iteration 19/25 | Loss: 0.00117297
Iteration 20/25 | Loss: 0.00117402
Iteration 21/25 | Loss: 0.00117350
Iteration 22/25 | Loss: 0.00117481
Iteration 23/25 | Loss: 0.00117685
Iteration 24/25 | Loss: 0.00117404
Iteration 25/25 | Loss: 0.00117344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25986886
Iteration 2/25 | Loss: 0.00103830
Iteration 3/25 | Loss: 0.00103830
Iteration 4/25 | Loss: 0.00103830
Iteration 5/25 | Loss: 0.00103830
Iteration 6/25 | Loss: 0.00103830
Iteration 7/25 | Loss: 0.00103830
Iteration 8/25 | Loss: 0.00103830
Iteration 9/25 | Loss: 0.00103830
Iteration 10/25 | Loss: 0.00103830
Iteration 11/25 | Loss: 0.00103830
Iteration 12/25 | Loss: 0.00103830
Iteration 13/25 | Loss: 0.00103830
Iteration 14/25 | Loss: 0.00103830
Iteration 15/25 | Loss: 0.00103830
Iteration 16/25 | Loss: 0.00103830
Iteration 17/25 | Loss: 0.00103830
Iteration 18/25 | Loss: 0.00103830
Iteration 19/25 | Loss: 0.00103830
Iteration 20/25 | Loss: 0.00103830
Iteration 21/25 | Loss: 0.00103830
Iteration 22/25 | Loss: 0.00103830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010382973123341799, 0.0010382973123341799, 0.0010382973123341799, 0.0010382973123341799, 0.0010382973123341799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010382973123341799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103830
Iteration 2/1000 | Loss: 0.00037309
Iteration 3/1000 | Loss: 0.00031709
Iteration 4/1000 | Loss: 0.00042165
Iteration 5/1000 | Loss: 0.00040545
Iteration 6/1000 | Loss: 0.00034499
Iteration 7/1000 | Loss: 0.00043192
Iteration 8/1000 | Loss: 0.00042219
Iteration 9/1000 | Loss: 0.00044432
Iteration 10/1000 | Loss: 0.00035365
Iteration 11/1000 | Loss: 0.00035355
Iteration 12/1000 | Loss: 0.00048875
Iteration 13/1000 | Loss: 0.00041208
Iteration 14/1000 | Loss: 0.00043506
Iteration 15/1000 | Loss: 0.00041094
Iteration 16/1000 | Loss: 0.00041990
Iteration 17/1000 | Loss: 0.00023609
Iteration 18/1000 | Loss: 0.00042068
Iteration 19/1000 | Loss: 0.00022014
Iteration 20/1000 | Loss: 0.00030396
Iteration 21/1000 | Loss: 0.00029386
Iteration 22/1000 | Loss: 0.00024845
Iteration 23/1000 | Loss: 0.00029983
Iteration 24/1000 | Loss: 0.00030239
Iteration 25/1000 | Loss: 0.00022186
Iteration 26/1000 | Loss: 0.00029375
Iteration 27/1000 | Loss: 0.00028815
Iteration 28/1000 | Loss: 0.00030366
Iteration 29/1000 | Loss: 0.00074488
Iteration 30/1000 | Loss: 0.00069755
Iteration 31/1000 | Loss: 0.00063703
Iteration 32/1000 | Loss: 0.00083729
Iteration 33/1000 | Loss: 0.00031797
Iteration 34/1000 | Loss: 0.00036233
Iteration 35/1000 | Loss: 0.00021959
Iteration 36/1000 | Loss: 0.00049714
Iteration 37/1000 | Loss: 0.00053324
Iteration 38/1000 | Loss: 0.00025628
Iteration 39/1000 | Loss: 0.00036228
Iteration 40/1000 | Loss: 0.00048425
Iteration 41/1000 | Loss: 0.00030540
Iteration 42/1000 | Loss: 0.00040657
Iteration 43/1000 | Loss: 0.00042324
Iteration 44/1000 | Loss: 0.00047910
Iteration 45/1000 | Loss: 0.00034854
Iteration 46/1000 | Loss: 0.00039488
Iteration 47/1000 | Loss: 0.00035713
Iteration 48/1000 | Loss: 0.00047709
Iteration 49/1000 | Loss: 0.00030866
Iteration 50/1000 | Loss: 0.00046198
Iteration 51/1000 | Loss: 0.00036920
Iteration 52/1000 | Loss: 0.00008684
Iteration 53/1000 | Loss: 0.00016237
Iteration 54/1000 | Loss: 0.00022070
Iteration 55/1000 | Loss: 0.00004179
Iteration 56/1000 | Loss: 0.00020825
Iteration 57/1000 | Loss: 0.00010192
Iteration 58/1000 | Loss: 0.00015385
Iteration 59/1000 | Loss: 0.00015115
Iteration 60/1000 | Loss: 0.00014130
Iteration 61/1000 | Loss: 0.00017462
Iteration 62/1000 | Loss: 0.00015351
Iteration 63/1000 | Loss: 0.00016389
Iteration 64/1000 | Loss: 0.00010979
Iteration 65/1000 | Loss: 0.00004548
Iteration 66/1000 | Loss: 0.00013305
Iteration 67/1000 | Loss: 0.00013086
Iteration 68/1000 | Loss: 0.00013672
Iteration 69/1000 | Loss: 0.00020010
Iteration 70/1000 | Loss: 0.00012342
Iteration 71/1000 | Loss: 0.00018237
Iteration 72/1000 | Loss: 0.00015742
Iteration 73/1000 | Loss: 0.00018974
Iteration 74/1000 | Loss: 0.00011888
Iteration 75/1000 | Loss: 0.00008180
Iteration 76/1000 | Loss: 0.00003149
Iteration 77/1000 | Loss: 0.00002442
Iteration 78/1000 | Loss: 0.00002946
Iteration 79/1000 | Loss: 0.00008192
Iteration 80/1000 | Loss: 0.00010620
Iteration 81/1000 | Loss: 0.00004453
Iteration 82/1000 | Loss: 0.00002449
Iteration 83/1000 | Loss: 0.00013408
Iteration 84/1000 | Loss: 0.00008266
Iteration 85/1000 | Loss: 0.00010827
Iteration 86/1000 | Loss: 0.00025708
Iteration 87/1000 | Loss: 0.00037142
Iteration 88/1000 | Loss: 0.00009567
Iteration 89/1000 | Loss: 0.00008699
Iteration 90/1000 | Loss: 0.00011204
Iteration 91/1000 | Loss: 0.00035587
Iteration 92/1000 | Loss: 0.00021160
Iteration 93/1000 | Loss: 0.00027226
Iteration 94/1000 | Loss: 0.00018036
Iteration 95/1000 | Loss: 0.00004835
Iteration 96/1000 | Loss: 0.00012953
Iteration 97/1000 | Loss: 0.00004485
Iteration 98/1000 | Loss: 0.00006441
Iteration 99/1000 | Loss: 0.00006907
Iteration 100/1000 | Loss: 0.00011442
Iteration 101/1000 | Loss: 0.00011980
Iteration 102/1000 | Loss: 0.00013476
Iteration 103/1000 | Loss: 0.00007306
Iteration 104/1000 | Loss: 0.00007916
Iteration 105/1000 | Loss: 0.00010651
Iteration 106/1000 | Loss: 0.00013604
Iteration 107/1000 | Loss: 0.00009834
Iteration 108/1000 | Loss: 0.00010513
Iteration 109/1000 | Loss: 0.00010709
Iteration 110/1000 | Loss: 0.00010274
Iteration 111/1000 | Loss: 0.00006967
Iteration 112/1000 | Loss: 0.00020739
Iteration 113/1000 | Loss: 0.00002681
Iteration 114/1000 | Loss: 0.00001947
Iteration 115/1000 | Loss: 0.00003233
Iteration 116/1000 | Loss: 0.00001814
Iteration 117/1000 | Loss: 0.00002517
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00002083
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00002040
Iteration 122/1000 | Loss: 0.00001825
Iteration 123/1000 | Loss: 0.00002078
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001804
Iteration 126/1000 | Loss: 0.00001825
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00002781
Iteration 129/1000 | Loss: 0.00001933
Iteration 130/1000 | Loss: 0.00002302
Iteration 131/1000 | Loss: 0.00002019
Iteration 132/1000 | Loss: 0.00002207
Iteration 133/1000 | Loss: 0.00002286
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00002433
Iteration 136/1000 | Loss: 0.00002250
Iteration 137/1000 | Loss: 0.00002529
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00002297
Iteration 140/1000 | Loss: 0.00002284
Iteration 141/1000 | Loss: 0.00002058
Iteration 142/1000 | Loss: 0.00003347
Iteration 143/1000 | Loss: 0.00002157
Iteration 144/1000 | Loss: 0.00002240
Iteration 145/1000 | Loss: 0.00001691
Iteration 146/1000 | Loss: 0.00001980
Iteration 147/1000 | Loss: 0.00002347
Iteration 148/1000 | Loss: 0.00002086
Iteration 149/1000 | Loss: 0.00002205
Iteration 150/1000 | Loss: 0.00002062
Iteration 151/1000 | Loss: 0.00002035
Iteration 152/1000 | Loss: 0.00002997
Iteration 153/1000 | Loss: 0.00001417
Iteration 154/1000 | Loss: 0.00001184
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001123
Iteration 157/1000 | Loss: 0.00001122
Iteration 158/1000 | Loss: 0.00001122
Iteration 159/1000 | Loss: 0.00001115
Iteration 160/1000 | Loss: 0.00001112
Iteration 161/1000 | Loss: 0.00001112
Iteration 162/1000 | Loss: 0.00001112
Iteration 163/1000 | Loss: 0.00001112
Iteration 164/1000 | Loss: 0.00001112
Iteration 165/1000 | Loss: 0.00001112
Iteration 166/1000 | Loss: 0.00001112
Iteration 167/1000 | Loss: 0.00001111
Iteration 168/1000 | Loss: 0.00001111
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001106
Iteration 173/1000 | Loss: 0.00001105
Iteration 174/1000 | Loss: 0.00001105
Iteration 175/1000 | Loss: 0.00001104
Iteration 176/1000 | Loss: 0.00001104
Iteration 177/1000 | Loss: 0.00001104
Iteration 178/1000 | Loss: 0.00001103
Iteration 179/1000 | Loss: 0.00001103
Iteration 180/1000 | Loss: 0.00001095
Iteration 181/1000 | Loss: 0.00001093
Iteration 182/1000 | Loss: 0.00001093
Iteration 183/1000 | Loss: 0.00001093
Iteration 184/1000 | Loss: 0.00001092
Iteration 185/1000 | Loss: 0.00001092
Iteration 186/1000 | Loss: 0.00001092
Iteration 187/1000 | Loss: 0.00001092
Iteration 188/1000 | Loss: 0.00001092
Iteration 189/1000 | Loss: 0.00001091
Iteration 190/1000 | Loss: 0.00001091
Iteration 191/1000 | Loss: 0.00001091
Iteration 192/1000 | Loss: 0.00001090
Iteration 193/1000 | Loss: 0.00001090
Iteration 194/1000 | Loss: 0.00001090
Iteration 195/1000 | Loss: 0.00001089
Iteration 196/1000 | Loss: 0.00001089
Iteration 197/1000 | Loss: 0.00001089
Iteration 198/1000 | Loss: 0.00001088
Iteration 199/1000 | Loss: 0.00001087
Iteration 200/1000 | Loss: 0.00001087
Iteration 201/1000 | Loss: 0.00001087
Iteration 202/1000 | Loss: 0.00001086
Iteration 203/1000 | Loss: 0.00001086
Iteration 204/1000 | Loss: 0.00001086
Iteration 205/1000 | Loss: 0.00001085
Iteration 206/1000 | Loss: 0.00001085
Iteration 207/1000 | Loss: 0.00001085
Iteration 208/1000 | Loss: 0.00001085
Iteration 209/1000 | Loss: 0.00001085
Iteration 210/1000 | Loss: 0.00001085
Iteration 211/1000 | Loss: 0.00001085
Iteration 212/1000 | Loss: 0.00001085
Iteration 213/1000 | Loss: 0.00001085
Iteration 214/1000 | Loss: 0.00001084
Iteration 215/1000 | Loss: 0.00001084
Iteration 216/1000 | Loss: 0.00001084
Iteration 217/1000 | Loss: 0.00001084
Iteration 218/1000 | Loss: 0.00001084
Iteration 219/1000 | Loss: 0.00001084
Iteration 220/1000 | Loss: 0.00001084
Iteration 221/1000 | Loss: 0.00001084
Iteration 222/1000 | Loss: 0.00001084
Iteration 223/1000 | Loss: 0.00001084
Iteration 224/1000 | Loss: 0.00001083
Iteration 225/1000 | Loss: 0.00001083
Iteration 226/1000 | Loss: 0.00001083
Iteration 227/1000 | Loss: 0.00001083
Iteration 228/1000 | Loss: 0.00001083
Iteration 229/1000 | Loss: 0.00001083
Iteration 230/1000 | Loss: 0.00001083
Iteration 231/1000 | Loss: 0.00001083
Iteration 232/1000 | Loss: 0.00001083
Iteration 233/1000 | Loss: 0.00001083
Iteration 234/1000 | Loss: 0.00001083
Iteration 235/1000 | Loss: 0.00001083
Iteration 236/1000 | Loss: 0.00001083
Iteration 237/1000 | Loss: 0.00001083
Iteration 238/1000 | Loss: 0.00001083
Iteration 239/1000 | Loss: 0.00001083
Iteration 240/1000 | Loss: 0.00001083
Iteration 241/1000 | Loss: 0.00001083
Iteration 242/1000 | Loss: 0.00001083
Iteration 243/1000 | Loss: 0.00001083
Iteration 244/1000 | Loss: 0.00001083
Iteration 245/1000 | Loss: 0.00001083
Iteration 246/1000 | Loss: 0.00001083
Iteration 247/1000 | Loss: 0.00001083
Iteration 248/1000 | Loss: 0.00001083
Iteration 249/1000 | Loss: 0.00001083
Iteration 250/1000 | Loss: 0.00001083
Iteration 251/1000 | Loss: 0.00001083
Iteration 252/1000 | Loss: 0.00001083
Iteration 253/1000 | Loss: 0.00001083
Iteration 254/1000 | Loss: 0.00001083
Iteration 255/1000 | Loss: 0.00001083
Iteration 256/1000 | Loss: 0.00001083
Iteration 257/1000 | Loss: 0.00001083
Iteration 258/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.082835478882771e-05, 1.082835478882771e-05, 1.082835478882771e-05, 1.082835478882771e-05, 1.082835478882771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.082835478882771e-05

Optimization complete. Final v2v error: 2.806447744369507 mm

Highest mean error: 4.999041557312012 mm for frame 54

Lowest mean error: 2.653406858444214 mm for frame 193

Saving results

Total time: 321.6152126789093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826200
Iteration 2/25 | Loss: 0.00158668
Iteration 3/25 | Loss: 0.00131091
Iteration 4/25 | Loss: 0.00129284
Iteration 5/25 | Loss: 0.00129060
Iteration 6/25 | Loss: 0.00129060
Iteration 7/25 | Loss: 0.00129060
Iteration 8/25 | Loss: 0.00129060
Iteration 9/25 | Loss: 0.00129060
Iteration 10/25 | Loss: 0.00129060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001290603308007121, 0.001290603308007121, 0.001290603308007121, 0.001290603308007121, 0.001290603308007121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001290603308007121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64390361
Iteration 2/25 | Loss: 0.00102109
Iteration 3/25 | Loss: 0.00102109
Iteration 4/25 | Loss: 0.00102109
Iteration 5/25 | Loss: 0.00102109
Iteration 6/25 | Loss: 0.00102109
Iteration 7/25 | Loss: 0.00102109
Iteration 8/25 | Loss: 0.00102108
Iteration 9/25 | Loss: 0.00102108
Iteration 10/25 | Loss: 0.00102108
Iteration 11/25 | Loss: 0.00102108
Iteration 12/25 | Loss: 0.00102108
Iteration 13/25 | Loss: 0.00102108
Iteration 14/25 | Loss: 0.00102108
Iteration 15/25 | Loss: 0.00102108
Iteration 16/25 | Loss: 0.00102108
Iteration 17/25 | Loss: 0.00102108
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010210840264335275, 0.0010210840264335275, 0.0010210840264335275, 0.0010210840264335275, 0.0010210840264335275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010210840264335275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102108
Iteration 2/1000 | Loss: 0.00003934
Iteration 3/1000 | Loss: 0.00002332
Iteration 4/1000 | Loss: 0.00002005
Iteration 5/1000 | Loss: 0.00001884
Iteration 6/1000 | Loss: 0.00001817
Iteration 7/1000 | Loss: 0.00001776
Iteration 8/1000 | Loss: 0.00001753
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001705
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001676
Iteration 14/1000 | Loss: 0.00001676
Iteration 15/1000 | Loss: 0.00001670
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001664
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001655
Iteration 31/1000 | Loss: 0.00001655
Iteration 32/1000 | Loss: 0.00001654
Iteration 33/1000 | Loss: 0.00001654
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001650
Iteration 39/1000 | Loss: 0.00001650
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001649
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001648
Iteration 48/1000 | Loss: 0.00001648
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001647
Iteration 51/1000 | Loss: 0.00001647
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001645
Iteration 60/1000 | Loss: 0.00001645
Iteration 61/1000 | Loss: 0.00001645
Iteration 62/1000 | Loss: 0.00001645
Iteration 63/1000 | Loss: 0.00001645
Iteration 64/1000 | Loss: 0.00001645
Iteration 65/1000 | Loss: 0.00001645
Iteration 66/1000 | Loss: 0.00001645
Iteration 67/1000 | Loss: 0.00001645
Iteration 68/1000 | Loss: 0.00001645
Iteration 69/1000 | Loss: 0.00001645
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.6450288967462257e-05, 1.6450288967462257e-05, 1.6450288967462257e-05, 1.6450288967462257e-05, 1.6450288967462257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6450288967462257e-05

Optimization complete. Final v2v error: 3.450381278991699 mm

Highest mean error: 3.7129580974578857 mm for frame 181

Lowest mean error: 3.1006550788879395 mm for frame 12

Saving results

Total time: 33.401538372039795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009850
Iteration 2/25 | Loss: 0.01009850
Iteration 3/25 | Loss: 0.01009850
Iteration 4/25 | Loss: 0.01009850
Iteration 5/25 | Loss: 0.01009850
Iteration 6/25 | Loss: 0.01009849
Iteration 7/25 | Loss: 0.01009849
Iteration 8/25 | Loss: 0.01009849
Iteration 9/25 | Loss: 0.01009849
Iteration 10/25 | Loss: 0.01009849
Iteration 11/25 | Loss: 0.01009849
Iteration 12/25 | Loss: 0.01009849
Iteration 13/25 | Loss: 0.01009848
Iteration 14/25 | Loss: 0.01009848
Iteration 15/25 | Loss: 0.01009848
Iteration 16/25 | Loss: 0.01009848
Iteration 17/25 | Loss: 0.01009848
Iteration 18/25 | Loss: 0.01009847
Iteration 19/25 | Loss: 0.01009847
Iteration 20/25 | Loss: 0.01009847
Iteration 21/25 | Loss: 0.01009847
Iteration 22/25 | Loss: 0.01009847
Iteration 23/25 | Loss: 0.01009847
Iteration 24/25 | Loss: 0.01009846
Iteration 25/25 | Loss: 0.01009846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52979195
Iteration 2/25 | Loss: 0.13810396
Iteration 3/25 | Loss: 0.13718735
Iteration 4/25 | Loss: 0.13412811
Iteration 5/25 | Loss: 0.13319653
Iteration 6/25 | Loss: 0.13311210
Iteration 7/25 | Loss: 0.13311207
Iteration 8/25 | Loss: 0.13311204
Iteration 9/25 | Loss: 0.13311203
Iteration 10/25 | Loss: 0.13311203
Iteration 11/25 | Loss: 0.13311203
Iteration 12/25 | Loss: 0.13311203
Iteration 13/25 | Loss: 0.13311203
Iteration 14/25 | Loss: 0.13311201
Iteration 15/25 | Loss: 0.13311201
Iteration 16/25 | Loss: 0.13311201
Iteration 17/25 | Loss: 0.13311201
Iteration 18/25 | Loss: 0.13311201
Iteration 19/25 | Loss: 0.13311201
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.13311201333999634, 0.13311201333999634, 0.13311201333999634, 0.13311201333999634, 0.13311201333999634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.13311201333999634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13311201
Iteration 2/1000 | Loss: 0.00288868
Iteration 3/1000 | Loss: 0.00158959
Iteration 4/1000 | Loss: 0.01569100
Iteration 5/1000 | Loss: 0.00170627
Iteration 6/1000 | Loss: 0.00349033
Iteration 7/1000 | Loss: 0.00201592
Iteration 8/1000 | Loss: 0.00589742
Iteration 9/1000 | Loss: 0.00282200
Iteration 10/1000 | Loss: 0.00048520
Iteration 11/1000 | Loss: 0.00008550
Iteration 12/1000 | Loss: 0.00005548
Iteration 13/1000 | Loss: 0.00004206
Iteration 14/1000 | Loss: 0.00049953
Iteration 15/1000 | Loss: 0.00003284
Iteration 16/1000 | Loss: 0.00002860
Iteration 17/1000 | Loss: 0.00022644
Iteration 18/1000 | Loss: 0.00002504
Iteration 19/1000 | Loss: 0.00002366
Iteration 20/1000 | Loss: 0.00022367
Iteration 21/1000 | Loss: 0.00002494
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002161
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00009253
Iteration 26/1000 | Loss: 0.00002159
Iteration 27/1000 | Loss: 0.00001901
Iteration 28/1000 | Loss: 0.00001854
Iteration 29/1000 | Loss: 0.00025698
Iteration 30/1000 | Loss: 0.00040198
Iteration 31/1000 | Loss: 0.00016740
Iteration 32/1000 | Loss: 0.00002582
Iteration 33/1000 | Loss: 0.00003945
Iteration 34/1000 | Loss: 0.00002472
Iteration 35/1000 | Loss: 0.00001951
Iteration 36/1000 | Loss: 0.00001883
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001772
Iteration 39/1000 | Loss: 0.00038875
Iteration 40/1000 | Loss: 0.00019080
Iteration 41/1000 | Loss: 0.00002458
Iteration 42/1000 | Loss: 0.00002207
Iteration 43/1000 | Loss: 0.00008532
Iteration 44/1000 | Loss: 0.00001745
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001592
Iteration 47/1000 | Loss: 0.00009443
Iteration 48/1000 | Loss: 0.00001667
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001513
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001487
Iteration 54/1000 | Loss: 0.00008704
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001470
Iteration 58/1000 | Loss: 0.00001469
Iteration 59/1000 | Loss: 0.00001469
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001467
Iteration 64/1000 | Loss: 0.00001467
Iteration 65/1000 | Loss: 0.00001466
Iteration 66/1000 | Loss: 0.00001466
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001458
Iteration 70/1000 | Loss: 0.00001457
Iteration 71/1000 | Loss: 0.00001457
Iteration 72/1000 | Loss: 0.00001457
Iteration 73/1000 | Loss: 0.00001457
Iteration 74/1000 | Loss: 0.00001457
Iteration 75/1000 | Loss: 0.00001457
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001457
Iteration 80/1000 | Loss: 0.00001457
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001456
Iteration 84/1000 | Loss: 0.00011648
Iteration 85/1000 | Loss: 0.00011648
Iteration 86/1000 | Loss: 0.00002977
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001452
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001445
Iteration 99/1000 | Loss: 0.00001445
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001444
Iteration 103/1000 | Loss: 0.00001444
Iteration 104/1000 | Loss: 0.00001444
Iteration 105/1000 | Loss: 0.00001444
Iteration 106/1000 | Loss: 0.00001444
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001443
Iteration 110/1000 | Loss: 0.00001443
Iteration 111/1000 | Loss: 0.00001443
Iteration 112/1000 | Loss: 0.00001443
Iteration 113/1000 | Loss: 0.00001443
Iteration 114/1000 | Loss: 0.00001443
Iteration 115/1000 | Loss: 0.00001443
Iteration 116/1000 | Loss: 0.00001443
Iteration 117/1000 | Loss: 0.00001443
Iteration 118/1000 | Loss: 0.00001442
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001442
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001442
Iteration 125/1000 | Loss: 0.00001442
Iteration 126/1000 | Loss: 0.00001442
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Iteration 131/1000 | Loss: 0.00001440
Iteration 132/1000 | Loss: 0.00001440
Iteration 133/1000 | Loss: 0.00001440
Iteration 134/1000 | Loss: 0.00001440
Iteration 135/1000 | Loss: 0.00001440
Iteration 136/1000 | Loss: 0.00001440
Iteration 137/1000 | Loss: 0.00001440
Iteration 138/1000 | Loss: 0.00001440
Iteration 139/1000 | Loss: 0.00001440
Iteration 140/1000 | Loss: 0.00001440
Iteration 141/1000 | Loss: 0.00001440
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001439
Iteration 144/1000 | Loss: 0.00001439
Iteration 145/1000 | Loss: 0.00001439
Iteration 146/1000 | Loss: 0.00001439
Iteration 147/1000 | Loss: 0.00001439
Iteration 148/1000 | Loss: 0.00001439
Iteration 149/1000 | Loss: 0.00001439
Iteration 150/1000 | Loss: 0.00001439
Iteration 151/1000 | Loss: 0.00001439
Iteration 152/1000 | Loss: 0.00001439
Iteration 153/1000 | Loss: 0.00001439
Iteration 154/1000 | Loss: 0.00001439
Iteration 155/1000 | Loss: 0.00001439
Iteration 156/1000 | Loss: 0.00001439
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001439
Iteration 162/1000 | Loss: 0.00001439
Iteration 163/1000 | Loss: 0.00001439
Iteration 164/1000 | Loss: 0.00001439
Iteration 165/1000 | Loss: 0.00001439
Iteration 166/1000 | Loss: 0.00001439
Iteration 167/1000 | Loss: 0.00001439
Iteration 168/1000 | Loss: 0.00001439
Iteration 169/1000 | Loss: 0.00001439
Iteration 170/1000 | Loss: 0.00001439
Iteration 171/1000 | Loss: 0.00001439
Iteration 172/1000 | Loss: 0.00001439
Iteration 173/1000 | Loss: 0.00001439
Iteration 174/1000 | Loss: 0.00001439
Iteration 175/1000 | Loss: 0.00001439
Iteration 176/1000 | Loss: 0.00001439
Iteration 177/1000 | Loss: 0.00001439
Iteration 178/1000 | Loss: 0.00001439
Iteration 179/1000 | Loss: 0.00001439
Iteration 180/1000 | Loss: 0.00001439
Iteration 181/1000 | Loss: 0.00001439
Iteration 182/1000 | Loss: 0.00001439
Iteration 183/1000 | Loss: 0.00001439
Iteration 184/1000 | Loss: 0.00001439
Iteration 185/1000 | Loss: 0.00001439
Iteration 186/1000 | Loss: 0.00001439
Iteration 187/1000 | Loss: 0.00001439
Iteration 188/1000 | Loss: 0.00001439
Iteration 189/1000 | Loss: 0.00001439
Iteration 190/1000 | Loss: 0.00001439
Iteration 191/1000 | Loss: 0.00001439
Iteration 192/1000 | Loss: 0.00001439
Iteration 193/1000 | Loss: 0.00001439
Iteration 194/1000 | Loss: 0.00001439
Iteration 195/1000 | Loss: 0.00001439
Iteration 196/1000 | Loss: 0.00001439
Iteration 197/1000 | Loss: 0.00001439
Iteration 198/1000 | Loss: 0.00001439
Iteration 199/1000 | Loss: 0.00001439
Iteration 200/1000 | Loss: 0.00001439
Iteration 201/1000 | Loss: 0.00001439
Iteration 202/1000 | Loss: 0.00001439
Iteration 203/1000 | Loss: 0.00001439
Iteration 204/1000 | Loss: 0.00001439
Iteration 205/1000 | Loss: 0.00001439
Iteration 206/1000 | Loss: 0.00001439
Iteration 207/1000 | Loss: 0.00001439
Iteration 208/1000 | Loss: 0.00001439
Iteration 209/1000 | Loss: 0.00001439
Iteration 210/1000 | Loss: 0.00001439
Iteration 211/1000 | Loss: 0.00001439
Iteration 212/1000 | Loss: 0.00001439
Iteration 213/1000 | Loss: 0.00001439
Iteration 214/1000 | Loss: 0.00001439
Iteration 215/1000 | Loss: 0.00001439
Iteration 216/1000 | Loss: 0.00001439
Iteration 217/1000 | Loss: 0.00001439
Iteration 218/1000 | Loss: 0.00001439
Iteration 219/1000 | Loss: 0.00001439
Iteration 220/1000 | Loss: 0.00001439
Iteration 221/1000 | Loss: 0.00001439
Iteration 222/1000 | Loss: 0.00001439
Iteration 223/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.4391619515663479e-05, 1.4391619515663479e-05, 1.4391619515663479e-05, 1.4391619515663479e-05, 1.4391619515663479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4391619515663479e-05

Optimization complete. Final v2v error: 3.2532787322998047 mm

Highest mean error: 3.8026366233825684 mm for frame 204

Lowest mean error: 2.882746458053589 mm for frame 57

Saving results

Total time: 117.13765001296997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00342491
Iteration 2/25 | Loss: 0.00127822
Iteration 3/25 | Loss: 0.00114935
Iteration 4/25 | Loss: 0.00112521
Iteration 5/25 | Loss: 0.00111887
Iteration 6/25 | Loss: 0.00111659
Iteration 7/25 | Loss: 0.00111647
Iteration 8/25 | Loss: 0.00111647
Iteration 9/25 | Loss: 0.00111647
Iteration 10/25 | Loss: 0.00111647
Iteration 11/25 | Loss: 0.00111647
Iteration 12/25 | Loss: 0.00111647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011164720635861158, 0.0011164720635861158, 0.0011164720635861158, 0.0011164720635861158, 0.0011164720635861158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011164720635861158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33144736
Iteration 2/25 | Loss: 0.00091431
Iteration 3/25 | Loss: 0.00091431
Iteration 4/25 | Loss: 0.00091431
Iteration 5/25 | Loss: 0.00091431
Iteration 6/25 | Loss: 0.00091431
Iteration 7/25 | Loss: 0.00091431
Iteration 8/25 | Loss: 0.00091431
Iteration 9/25 | Loss: 0.00091431
Iteration 10/25 | Loss: 0.00091431
Iteration 11/25 | Loss: 0.00091431
Iteration 12/25 | Loss: 0.00091431
Iteration 13/25 | Loss: 0.00091431
Iteration 14/25 | Loss: 0.00091431
Iteration 15/25 | Loss: 0.00091431
Iteration 16/25 | Loss: 0.00091431
Iteration 17/25 | Loss: 0.00091431
Iteration 18/25 | Loss: 0.00091431
Iteration 19/25 | Loss: 0.00091431
Iteration 20/25 | Loss: 0.00091431
Iteration 21/25 | Loss: 0.00091431
Iteration 22/25 | Loss: 0.00091431
Iteration 23/25 | Loss: 0.00091431
Iteration 24/25 | Loss: 0.00091431
Iteration 25/25 | Loss: 0.00091431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091431
Iteration 2/1000 | Loss: 0.00002553
Iteration 3/1000 | Loss: 0.00001763
Iteration 4/1000 | Loss: 0.00001586
Iteration 5/1000 | Loss: 0.00001480
Iteration 6/1000 | Loss: 0.00001396
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001301
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001257
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001256
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001238
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001226
Iteration 45/1000 | Loss: 0.00001226
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001225
Iteration 48/1000 | Loss: 0.00001225
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001223
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001219
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001218
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001212
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001211
Iteration 78/1000 | Loss: 0.00001211
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001206
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001202
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001201
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001201
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001201
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001200
Iteration 138/1000 | Loss: 0.00001200
Iteration 139/1000 | Loss: 0.00001200
Iteration 140/1000 | Loss: 0.00001200
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001197
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001197
Iteration 160/1000 | Loss: 0.00001196
Iteration 161/1000 | Loss: 0.00001196
Iteration 162/1000 | Loss: 0.00001196
Iteration 163/1000 | Loss: 0.00001195
Iteration 164/1000 | Loss: 0.00001195
Iteration 165/1000 | Loss: 0.00001195
Iteration 166/1000 | Loss: 0.00001195
Iteration 167/1000 | Loss: 0.00001195
Iteration 168/1000 | Loss: 0.00001195
Iteration 169/1000 | Loss: 0.00001194
Iteration 170/1000 | Loss: 0.00001194
Iteration 171/1000 | Loss: 0.00001194
Iteration 172/1000 | Loss: 0.00001194
Iteration 173/1000 | Loss: 0.00001194
Iteration 174/1000 | Loss: 0.00001194
Iteration 175/1000 | Loss: 0.00001194
Iteration 176/1000 | Loss: 0.00001194
Iteration 177/1000 | Loss: 0.00001194
Iteration 178/1000 | Loss: 0.00001194
Iteration 179/1000 | Loss: 0.00001194
Iteration 180/1000 | Loss: 0.00001194
Iteration 181/1000 | Loss: 0.00001193
Iteration 182/1000 | Loss: 0.00001193
Iteration 183/1000 | Loss: 0.00001193
Iteration 184/1000 | Loss: 0.00001193
Iteration 185/1000 | Loss: 0.00001193
Iteration 186/1000 | Loss: 0.00001193
Iteration 187/1000 | Loss: 0.00001193
Iteration 188/1000 | Loss: 0.00001192
Iteration 189/1000 | Loss: 0.00001192
Iteration 190/1000 | Loss: 0.00001192
Iteration 191/1000 | Loss: 0.00001192
Iteration 192/1000 | Loss: 0.00001192
Iteration 193/1000 | Loss: 0.00001191
Iteration 194/1000 | Loss: 0.00001191
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001190
Iteration 197/1000 | Loss: 0.00001190
Iteration 198/1000 | Loss: 0.00001190
Iteration 199/1000 | Loss: 0.00001190
Iteration 200/1000 | Loss: 0.00001190
Iteration 201/1000 | Loss: 0.00001190
Iteration 202/1000 | Loss: 0.00001190
Iteration 203/1000 | Loss: 0.00001189
Iteration 204/1000 | Loss: 0.00001189
Iteration 205/1000 | Loss: 0.00001189
Iteration 206/1000 | Loss: 0.00001189
Iteration 207/1000 | Loss: 0.00001189
Iteration 208/1000 | Loss: 0.00001188
Iteration 209/1000 | Loss: 0.00001188
Iteration 210/1000 | Loss: 0.00001188
Iteration 211/1000 | Loss: 0.00001188
Iteration 212/1000 | Loss: 0.00001188
Iteration 213/1000 | Loss: 0.00001187
Iteration 214/1000 | Loss: 0.00001187
Iteration 215/1000 | Loss: 0.00001187
Iteration 216/1000 | Loss: 0.00001187
Iteration 217/1000 | Loss: 0.00001187
Iteration 218/1000 | Loss: 0.00001187
Iteration 219/1000 | Loss: 0.00001187
Iteration 220/1000 | Loss: 0.00001187
Iteration 221/1000 | Loss: 0.00001187
Iteration 222/1000 | Loss: 0.00001187
Iteration 223/1000 | Loss: 0.00001187
Iteration 224/1000 | Loss: 0.00001187
Iteration 225/1000 | Loss: 0.00001186
Iteration 226/1000 | Loss: 0.00001186
Iteration 227/1000 | Loss: 0.00001186
Iteration 228/1000 | Loss: 0.00001186
Iteration 229/1000 | Loss: 0.00001186
Iteration 230/1000 | Loss: 0.00001186
Iteration 231/1000 | Loss: 0.00001186
Iteration 232/1000 | Loss: 0.00001186
Iteration 233/1000 | Loss: 0.00001185
Iteration 234/1000 | Loss: 0.00001185
Iteration 235/1000 | Loss: 0.00001185
Iteration 236/1000 | Loss: 0.00001185
Iteration 237/1000 | Loss: 0.00001185
Iteration 238/1000 | Loss: 0.00001185
Iteration 239/1000 | Loss: 0.00001184
Iteration 240/1000 | Loss: 0.00001184
Iteration 241/1000 | Loss: 0.00001184
Iteration 242/1000 | Loss: 0.00001184
Iteration 243/1000 | Loss: 0.00001184
Iteration 244/1000 | Loss: 0.00001184
Iteration 245/1000 | Loss: 0.00001184
Iteration 246/1000 | Loss: 0.00001184
Iteration 247/1000 | Loss: 0.00001184
Iteration 248/1000 | Loss: 0.00001184
Iteration 249/1000 | Loss: 0.00001184
Iteration 250/1000 | Loss: 0.00001184
Iteration 251/1000 | Loss: 0.00001184
Iteration 252/1000 | Loss: 0.00001184
Iteration 253/1000 | Loss: 0.00001184
Iteration 254/1000 | Loss: 0.00001184
Iteration 255/1000 | Loss: 0.00001184
Iteration 256/1000 | Loss: 0.00001184
Iteration 257/1000 | Loss: 0.00001184
Iteration 258/1000 | Loss: 0.00001184
Iteration 259/1000 | Loss: 0.00001184
Iteration 260/1000 | Loss: 0.00001184
Iteration 261/1000 | Loss: 0.00001184
Iteration 262/1000 | Loss: 0.00001184
Iteration 263/1000 | Loss: 0.00001184
Iteration 264/1000 | Loss: 0.00001184
Iteration 265/1000 | Loss: 0.00001184
Iteration 266/1000 | Loss: 0.00001184
Iteration 267/1000 | Loss: 0.00001184
Iteration 268/1000 | Loss: 0.00001184
Iteration 269/1000 | Loss: 0.00001184
Iteration 270/1000 | Loss: 0.00001184
Iteration 271/1000 | Loss: 0.00001184
Iteration 272/1000 | Loss: 0.00001184
Iteration 273/1000 | Loss: 0.00001184
Iteration 274/1000 | Loss: 0.00001184
Iteration 275/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.184199300041655e-05, 1.184199300041655e-05, 1.184199300041655e-05, 1.184199300041655e-05, 1.184199300041655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.184199300041655e-05

Optimization complete. Final v2v error: 2.95813250541687 mm

Highest mean error: 3.550614356994629 mm for frame 75

Lowest mean error: 2.661947727203369 mm for frame 254

Saving results

Total time: 53.73717427253723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404739
Iteration 2/25 | Loss: 0.00127049
Iteration 3/25 | Loss: 0.00115113
Iteration 4/25 | Loss: 0.00113436
Iteration 5/25 | Loss: 0.00113155
Iteration 6/25 | Loss: 0.00113139
Iteration 7/25 | Loss: 0.00113133
Iteration 8/25 | Loss: 0.00113133
Iteration 9/25 | Loss: 0.00113133
Iteration 10/25 | Loss: 0.00113133
Iteration 11/25 | Loss: 0.00113133
Iteration 12/25 | Loss: 0.00113133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011313259601593018, 0.0011313259601593018, 0.0011313259601593018, 0.0011313259601593018, 0.0011313259601593018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011313259601593018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34964049
Iteration 2/25 | Loss: 0.00071525
Iteration 3/25 | Loss: 0.00071524
Iteration 4/25 | Loss: 0.00071524
Iteration 5/25 | Loss: 0.00071524
Iteration 6/25 | Loss: 0.00071524
Iteration 7/25 | Loss: 0.00071524
Iteration 8/25 | Loss: 0.00071524
Iteration 9/25 | Loss: 0.00071524
Iteration 10/25 | Loss: 0.00071524
Iteration 11/25 | Loss: 0.00071524
Iteration 12/25 | Loss: 0.00071524
Iteration 13/25 | Loss: 0.00071524
Iteration 14/25 | Loss: 0.00071524
Iteration 15/25 | Loss: 0.00071524
Iteration 16/25 | Loss: 0.00071524
Iteration 17/25 | Loss: 0.00071524
Iteration 18/25 | Loss: 0.00071524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007152408943511546, 0.0007152408943511546, 0.0007152408943511546, 0.0007152408943511546, 0.0007152408943511546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007152408943511546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071524
Iteration 2/1000 | Loss: 0.00002809
Iteration 3/1000 | Loss: 0.00002083
Iteration 4/1000 | Loss: 0.00001778
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001555
Iteration 7/1000 | Loss: 0.00001496
Iteration 8/1000 | Loss: 0.00001453
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001387
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001359
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001309
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001292
Iteration 27/1000 | Loss: 0.00001290
Iteration 28/1000 | Loss: 0.00001289
Iteration 29/1000 | Loss: 0.00001289
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001282
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001277
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001277
Iteration 38/1000 | Loss: 0.00001277
Iteration 39/1000 | Loss: 0.00001277
Iteration 40/1000 | Loss: 0.00001277
Iteration 41/1000 | Loss: 0.00001276
Iteration 42/1000 | Loss: 0.00001276
Iteration 43/1000 | Loss: 0.00001276
Iteration 44/1000 | Loss: 0.00001275
Iteration 45/1000 | Loss: 0.00001275
Iteration 46/1000 | Loss: 0.00001273
Iteration 47/1000 | Loss: 0.00001272
Iteration 48/1000 | Loss: 0.00001272
Iteration 49/1000 | Loss: 0.00001272
Iteration 50/1000 | Loss: 0.00001271
Iteration 51/1000 | Loss: 0.00001271
Iteration 52/1000 | Loss: 0.00001271
Iteration 53/1000 | Loss: 0.00001270
Iteration 54/1000 | Loss: 0.00001270
Iteration 55/1000 | Loss: 0.00001269
Iteration 56/1000 | Loss: 0.00001269
Iteration 57/1000 | Loss: 0.00001268
Iteration 58/1000 | Loss: 0.00001268
Iteration 59/1000 | Loss: 0.00001268
Iteration 60/1000 | Loss: 0.00001267
Iteration 61/1000 | Loss: 0.00001266
Iteration 62/1000 | Loss: 0.00001266
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001265
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001265
Iteration 68/1000 | Loss: 0.00001265
Iteration 69/1000 | Loss: 0.00001265
Iteration 70/1000 | Loss: 0.00001265
Iteration 71/1000 | Loss: 0.00001265
Iteration 72/1000 | Loss: 0.00001265
Iteration 73/1000 | Loss: 0.00001265
Iteration 74/1000 | Loss: 0.00001264
Iteration 75/1000 | Loss: 0.00001264
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001264
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001264
Iteration 81/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.2644392882066313e-05, 1.2644392882066313e-05, 1.2644392882066313e-05, 1.2644392882066313e-05, 1.2644392882066313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2644392882066313e-05

Optimization complete. Final v2v error: 3.022441864013672 mm

Highest mean error: 3.610382318496704 mm for frame 104

Lowest mean error: 2.674315929412842 mm for frame 18

Saving results

Total time: 37.32890820503235
