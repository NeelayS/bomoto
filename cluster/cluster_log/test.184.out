Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=184, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10304-10359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464285
Iteration 2/25 | Loss: 0.00097077
Iteration 3/25 | Loss: 0.00078150
Iteration 4/25 | Loss: 0.00076336
Iteration 5/25 | Loss: 0.00075639
Iteration 6/25 | Loss: 0.00075368
Iteration 7/25 | Loss: 0.00075314
Iteration 8/25 | Loss: 0.00075314
Iteration 9/25 | Loss: 0.00075314
Iteration 10/25 | Loss: 0.00075314
Iteration 11/25 | Loss: 0.00075314
Iteration 12/25 | Loss: 0.00075314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000753144093323499, 0.000753144093323499, 0.000753144093323499, 0.000753144093323499, 0.000753144093323499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000753144093323499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55691004
Iteration 2/25 | Loss: 0.00088371
Iteration 3/25 | Loss: 0.00088371
Iteration 4/25 | Loss: 0.00088371
Iteration 5/25 | Loss: 0.00088371
Iteration 6/25 | Loss: 0.00088371
Iteration 7/25 | Loss: 0.00088371
Iteration 8/25 | Loss: 0.00088371
Iteration 9/25 | Loss: 0.00088371
Iteration 10/25 | Loss: 0.00088371
Iteration 11/25 | Loss: 0.00088371
Iteration 12/25 | Loss: 0.00088371
Iteration 13/25 | Loss: 0.00088371
Iteration 14/25 | Loss: 0.00088371
Iteration 15/25 | Loss: 0.00088371
Iteration 16/25 | Loss: 0.00088371
Iteration 17/25 | Loss: 0.00088371
Iteration 18/25 | Loss: 0.00088371
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008837057976052165, 0.0008837057976052165, 0.0008837057976052165, 0.0008837057976052165, 0.0008837057976052165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008837057976052165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088371
Iteration 2/1000 | Loss: 0.00002878
Iteration 3/1000 | Loss: 0.00002115
Iteration 4/1000 | Loss: 0.00002001
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001825
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001728
Iteration 10/1000 | Loss: 0.00001721
Iteration 11/1000 | Loss: 0.00001713
Iteration 12/1000 | Loss: 0.00001709
Iteration 13/1000 | Loss: 0.00001709
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001708
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001708
Iteration 19/1000 | Loss: 0.00001708
Iteration 20/1000 | Loss: 0.00001708
Iteration 21/1000 | Loss: 0.00001707
Iteration 22/1000 | Loss: 0.00001704
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001703
Iteration 27/1000 | Loss: 0.00001702
Iteration 28/1000 | Loss: 0.00001702
Iteration 29/1000 | Loss: 0.00001702
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 39. Stopping optimization.
Last 5 losses: [1.701828296063468e-05, 1.701828296063468e-05, 1.701828296063468e-05, 1.701828296063468e-05, 1.701828296063468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.701828296063468e-05

Optimization complete. Final v2v error: 3.4846229553222656 mm

Highest mean error: 3.8898236751556396 mm for frame 15

Lowest mean error: 3.009410858154297 mm for frame 190

Saving results

Total time: 32.012781381607056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539675
Iteration 2/25 | Loss: 0.00122433
Iteration 3/25 | Loss: 0.00083440
Iteration 4/25 | Loss: 0.00078441
Iteration 5/25 | Loss: 0.00077136
Iteration 6/25 | Loss: 0.00076832
Iteration 7/25 | Loss: 0.00076804
Iteration 8/25 | Loss: 0.00076804
Iteration 9/25 | Loss: 0.00076804
Iteration 10/25 | Loss: 0.00076804
Iteration 11/25 | Loss: 0.00076804
Iteration 12/25 | Loss: 0.00076804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007680388516746461, 0.0007680388516746461, 0.0007680388516746461, 0.0007680388516746461, 0.0007680388516746461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007680388516746461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70351756
Iteration 2/25 | Loss: 0.00060037
Iteration 3/25 | Loss: 0.00060037
Iteration 4/25 | Loss: 0.00060037
Iteration 5/25 | Loss: 0.00060037
Iteration 6/25 | Loss: 0.00060037
Iteration 7/25 | Loss: 0.00060037
Iteration 8/25 | Loss: 0.00060037
Iteration 9/25 | Loss: 0.00060036
Iteration 10/25 | Loss: 0.00060036
Iteration 11/25 | Loss: 0.00060036
Iteration 12/25 | Loss: 0.00060036
Iteration 13/25 | Loss: 0.00060036
Iteration 14/25 | Loss: 0.00060036
Iteration 15/25 | Loss: 0.00060036
Iteration 16/25 | Loss: 0.00060036
Iteration 17/25 | Loss: 0.00060036
Iteration 18/25 | Loss: 0.00060036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00060036446666345, 0.00060036446666345, 0.00060036446666345, 0.00060036446666345, 0.00060036446666345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00060036446666345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060036
Iteration 2/1000 | Loss: 0.00003957
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002386
Iteration 5/1000 | Loss: 0.00002257
Iteration 6/1000 | Loss: 0.00002185
Iteration 7/1000 | Loss: 0.00002108
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00001989
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001978
Iteration 14/1000 | Loss: 0.00001960
Iteration 15/1000 | Loss: 0.00001953
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001947
Iteration 22/1000 | Loss: 0.00001945
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001939
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001930
Iteration 28/1000 | Loss: 0.00001930
Iteration 29/1000 | Loss: 0.00001929
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001918
Iteration 38/1000 | Loss: 0.00001918
Iteration 39/1000 | Loss: 0.00001916
Iteration 40/1000 | Loss: 0.00001916
Iteration 41/1000 | Loss: 0.00001916
Iteration 42/1000 | Loss: 0.00001916
Iteration 43/1000 | Loss: 0.00001916
Iteration 44/1000 | Loss: 0.00001916
Iteration 45/1000 | Loss: 0.00001916
Iteration 46/1000 | Loss: 0.00001915
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001914
Iteration 58/1000 | Loss: 0.00001914
Iteration 59/1000 | Loss: 0.00001914
Iteration 60/1000 | Loss: 0.00001913
Iteration 61/1000 | Loss: 0.00001913
Iteration 62/1000 | Loss: 0.00001913
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001912
Iteration 65/1000 | Loss: 0.00001912
Iteration 66/1000 | Loss: 0.00001912
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001912
Iteration 69/1000 | Loss: 0.00001912
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001911
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001911
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001910
Iteration 80/1000 | Loss: 0.00001909
Iteration 81/1000 | Loss: 0.00001909
Iteration 82/1000 | Loss: 0.00001909
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001909
Iteration 85/1000 | Loss: 0.00001909
Iteration 86/1000 | Loss: 0.00001908
Iteration 87/1000 | Loss: 0.00001908
Iteration 88/1000 | Loss: 0.00001908
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001905
Iteration 93/1000 | Loss: 0.00001905
Iteration 94/1000 | Loss: 0.00001905
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001904
Iteration 97/1000 | Loss: 0.00001904
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001903
Iteration 100/1000 | Loss: 0.00001902
Iteration 101/1000 | Loss: 0.00001902
Iteration 102/1000 | Loss: 0.00001901
Iteration 103/1000 | Loss: 0.00001901
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001899
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001898
Iteration 111/1000 | Loss: 0.00001898
Iteration 112/1000 | Loss: 0.00001898
Iteration 113/1000 | Loss: 0.00001898
Iteration 114/1000 | Loss: 0.00001897
Iteration 115/1000 | Loss: 0.00001897
Iteration 116/1000 | Loss: 0.00001897
Iteration 117/1000 | Loss: 0.00001897
Iteration 118/1000 | Loss: 0.00001896
Iteration 119/1000 | Loss: 0.00001896
Iteration 120/1000 | Loss: 0.00001896
Iteration 121/1000 | Loss: 0.00001896
Iteration 122/1000 | Loss: 0.00001895
Iteration 123/1000 | Loss: 0.00001895
Iteration 124/1000 | Loss: 0.00001895
Iteration 125/1000 | Loss: 0.00001895
Iteration 126/1000 | Loss: 0.00001894
Iteration 127/1000 | Loss: 0.00001894
Iteration 128/1000 | Loss: 0.00001894
Iteration 129/1000 | Loss: 0.00001894
Iteration 130/1000 | Loss: 0.00001894
Iteration 131/1000 | Loss: 0.00001894
Iteration 132/1000 | Loss: 0.00001894
Iteration 133/1000 | Loss: 0.00001894
Iteration 134/1000 | Loss: 0.00001894
Iteration 135/1000 | Loss: 0.00001894
Iteration 136/1000 | Loss: 0.00001893
Iteration 137/1000 | Loss: 0.00001893
Iteration 138/1000 | Loss: 0.00001893
Iteration 139/1000 | Loss: 0.00001893
Iteration 140/1000 | Loss: 0.00001893
Iteration 141/1000 | Loss: 0.00001893
Iteration 142/1000 | Loss: 0.00001892
Iteration 143/1000 | Loss: 0.00001892
Iteration 144/1000 | Loss: 0.00001892
Iteration 145/1000 | Loss: 0.00001892
Iteration 146/1000 | Loss: 0.00001892
Iteration 147/1000 | Loss: 0.00001892
Iteration 148/1000 | Loss: 0.00001892
Iteration 149/1000 | Loss: 0.00001892
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001891
Iteration 152/1000 | Loss: 0.00001891
Iteration 153/1000 | Loss: 0.00001891
Iteration 154/1000 | Loss: 0.00001891
Iteration 155/1000 | Loss: 0.00001891
Iteration 156/1000 | Loss: 0.00001891
Iteration 157/1000 | Loss: 0.00001891
Iteration 158/1000 | Loss: 0.00001891
Iteration 159/1000 | Loss: 0.00001891
Iteration 160/1000 | Loss: 0.00001891
Iteration 161/1000 | Loss: 0.00001891
Iteration 162/1000 | Loss: 0.00001890
Iteration 163/1000 | Loss: 0.00001890
Iteration 164/1000 | Loss: 0.00001890
Iteration 165/1000 | Loss: 0.00001890
Iteration 166/1000 | Loss: 0.00001890
Iteration 167/1000 | Loss: 0.00001890
Iteration 168/1000 | Loss: 0.00001890
Iteration 169/1000 | Loss: 0.00001890
Iteration 170/1000 | Loss: 0.00001890
Iteration 171/1000 | Loss: 0.00001890
Iteration 172/1000 | Loss: 0.00001890
Iteration 173/1000 | Loss: 0.00001890
Iteration 174/1000 | Loss: 0.00001890
Iteration 175/1000 | Loss: 0.00001890
Iteration 176/1000 | Loss: 0.00001890
Iteration 177/1000 | Loss: 0.00001890
Iteration 178/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.889822851808276e-05, 1.889822851808276e-05, 1.889822851808276e-05, 1.889822851808276e-05, 1.889822851808276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.889822851808276e-05

Optimization complete. Final v2v error: 3.6723926067352295 mm

Highest mean error: 4.032740116119385 mm for frame 26

Lowest mean error: 3.4893805980682373 mm for frame 11

Saving results

Total time: 51.6216778755188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088848
Iteration 2/25 | Loss: 0.01088848
Iteration 3/25 | Loss: 0.01088848
Iteration 4/25 | Loss: 0.01088848
Iteration 5/25 | Loss: 0.01088848
Iteration 6/25 | Loss: 0.01088848
Iteration 7/25 | Loss: 0.01088848
Iteration 8/25 | Loss: 0.01088848
Iteration 9/25 | Loss: 0.01088848
Iteration 10/25 | Loss: 0.01088848
Iteration 11/25 | Loss: 0.01088848
Iteration 12/25 | Loss: 0.01088847
Iteration 13/25 | Loss: 0.01088847
Iteration 14/25 | Loss: 0.01088847
Iteration 15/25 | Loss: 0.01088847
Iteration 16/25 | Loss: 0.01088847
Iteration 17/25 | Loss: 0.01088847
Iteration 18/25 | Loss: 0.01088847
Iteration 19/25 | Loss: 0.01088847
Iteration 20/25 | Loss: 0.01088847
Iteration 21/25 | Loss: 0.01088847
Iteration 22/25 | Loss: 0.01088847
Iteration 23/25 | Loss: 0.01088846
Iteration 24/25 | Loss: 0.01088846
Iteration 25/25 | Loss: 0.01088846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95232439
Iteration 2/25 | Loss: 0.06595811
Iteration 3/25 | Loss: 0.06570770
Iteration 4/25 | Loss: 0.06570769
Iteration 5/25 | Loss: 0.06570769
Iteration 6/25 | Loss: 0.06570768
Iteration 7/25 | Loss: 0.06570768
Iteration 8/25 | Loss: 0.06570768
Iteration 9/25 | Loss: 0.06570768
Iteration 10/25 | Loss: 0.06570768
Iteration 11/25 | Loss: 0.06570768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06570767611265182, 0.06570767611265182, 0.06570767611265182, 0.06570767611265182, 0.06570767611265182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06570767611265182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06570768
Iteration 2/1000 | Loss: 0.00807853
Iteration 3/1000 | Loss: 0.00252133
Iteration 4/1000 | Loss: 0.00104594
Iteration 5/1000 | Loss: 0.00161974
Iteration 6/1000 | Loss: 0.00384821
Iteration 7/1000 | Loss: 0.00953689
Iteration 8/1000 | Loss: 0.00608026
Iteration 9/1000 | Loss: 0.00089753
Iteration 10/1000 | Loss: 0.00169880
Iteration 11/1000 | Loss: 0.00508918
Iteration 12/1000 | Loss: 0.00369776
Iteration 13/1000 | Loss: 0.00036449
Iteration 14/1000 | Loss: 0.00012509
Iteration 15/1000 | Loss: 0.00070963
Iteration 16/1000 | Loss: 0.00009962
Iteration 17/1000 | Loss: 0.00057573
Iteration 18/1000 | Loss: 0.00057604
Iteration 19/1000 | Loss: 0.00117563
Iteration 20/1000 | Loss: 0.00046787
Iteration 21/1000 | Loss: 0.00020041
Iteration 22/1000 | Loss: 0.00319337
Iteration 23/1000 | Loss: 0.00021718
Iteration 24/1000 | Loss: 0.00014680
Iteration 25/1000 | Loss: 0.00012515
Iteration 26/1000 | Loss: 0.00018092
Iteration 27/1000 | Loss: 0.00006570
Iteration 28/1000 | Loss: 0.00054257
Iteration 29/1000 | Loss: 0.00026427
Iteration 30/1000 | Loss: 0.00032248
Iteration 31/1000 | Loss: 0.00018245
Iteration 32/1000 | Loss: 0.00034033
Iteration 33/1000 | Loss: 0.00036649
Iteration 34/1000 | Loss: 0.00017660
Iteration 35/1000 | Loss: 0.00033201
Iteration 36/1000 | Loss: 0.00144209
Iteration 37/1000 | Loss: 0.00013501
Iteration 38/1000 | Loss: 0.00023226
Iteration 39/1000 | Loss: 0.00027797
Iteration 40/1000 | Loss: 0.00034460
Iteration 41/1000 | Loss: 0.00052780
Iteration 42/1000 | Loss: 0.00011723
Iteration 43/1000 | Loss: 0.00065315
Iteration 44/1000 | Loss: 0.00026915
Iteration 45/1000 | Loss: 0.00010700
Iteration 46/1000 | Loss: 0.00006716
Iteration 47/1000 | Loss: 0.00006559
Iteration 48/1000 | Loss: 0.00009518
Iteration 49/1000 | Loss: 0.00034965
Iteration 50/1000 | Loss: 0.00040507
Iteration 51/1000 | Loss: 0.00083800
Iteration 52/1000 | Loss: 0.00004053
Iteration 53/1000 | Loss: 0.00003844
Iteration 54/1000 | Loss: 0.00016166
Iteration 55/1000 | Loss: 0.00055026
Iteration 56/1000 | Loss: 0.00012440
Iteration 57/1000 | Loss: 0.00007185
Iteration 58/1000 | Loss: 0.00003583
Iteration 59/1000 | Loss: 0.00013291
Iteration 60/1000 | Loss: 0.00009815
Iteration 61/1000 | Loss: 0.00006765
Iteration 62/1000 | Loss: 0.00010057
Iteration 63/1000 | Loss: 0.00061608
Iteration 64/1000 | Loss: 0.00017057
Iteration 65/1000 | Loss: 0.00010079
Iteration 66/1000 | Loss: 0.00009669
Iteration 67/1000 | Loss: 0.00004444
Iteration 68/1000 | Loss: 0.00003540
Iteration 69/1000 | Loss: 0.00003163
Iteration 70/1000 | Loss: 0.00010788
Iteration 71/1000 | Loss: 0.00021831
Iteration 72/1000 | Loss: 0.00005639
Iteration 73/1000 | Loss: 0.00003593
Iteration 74/1000 | Loss: 0.00007512
Iteration 75/1000 | Loss: 0.00003921
Iteration 76/1000 | Loss: 0.00009055
Iteration 77/1000 | Loss: 0.00003065
Iteration 78/1000 | Loss: 0.00005860
Iteration 79/1000 | Loss: 0.00007466
Iteration 80/1000 | Loss: 0.00002957
Iteration 81/1000 | Loss: 0.00003659
Iteration 82/1000 | Loss: 0.00002936
Iteration 83/1000 | Loss: 0.00002935
Iteration 84/1000 | Loss: 0.00005582
Iteration 85/1000 | Loss: 0.00005971
Iteration 86/1000 | Loss: 0.00058400
Iteration 87/1000 | Loss: 0.00022880
Iteration 88/1000 | Loss: 0.00020913
Iteration 89/1000 | Loss: 0.00003199
Iteration 90/1000 | Loss: 0.00003022
Iteration 91/1000 | Loss: 0.00002896
Iteration 92/1000 | Loss: 0.00002896
Iteration 93/1000 | Loss: 0.00005489
Iteration 94/1000 | Loss: 0.00003271
Iteration 95/1000 | Loss: 0.00002882
Iteration 96/1000 | Loss: 0.00003044
Iteration 97/1000 | Loss: 0.00005772
Iteration 98/1000 | Loss: 0.00003168
Iteration 99/1000 | Loss: 0.00003389
Iteration 100/1000 | Loss: 0.00002871
Iteration 101/1000 | Loss: 0.00002870
Iteration 102/1000 | Loss: 0.00002870
Iteration 103/1000 | Loss: 0.00002870
Iteration 104/1000 | Loss: 0.00002870
Iteration 105/1000 | Loss: 0.00002870
Iteration 106/1000 | Loss: 0.00002870
Iteration 107/1000 | Loss: 0.00002870
Iteration 108/1000 | Loss: 0.00002870
Iteration 109/1000 | Loss: 0.00002870
Iteration 110/1000 | Loss: 0.00002869
Iteration 111/1000 | Loss: 0.00002867
Iteration 112/1000 | Loss: 0.00006596
Iteration 113/1000 | Loss: 0.00003321
Iteration 114/1000 | Loss: 0.00005457
Iteration 115/1000 | Loss: 0.00003917
Iteration 116/1000 | Loss: 0.00003589
Iteration 117/1000 | Loss: 0.00002855
Iteration 118/1000 | Loss: 0.00002855
Iteration 119/1000 | Loss: 0.00002852
Iteration 120/1000 | Loss: 0.00002846
Iteration 121/1000 | Loss: 0.00002840
Iteration 122/1000 | Loss: 0.00002840
Iteration 123/1000 | Loss: 0.00002839
Iteration 124/1000 | Loss: 0.00002839
Iteration 125/1000 | Loss: 0.00002836
Iteration 126/1000 | Loss: 0.00002836
Iteration 127/1000 | Loss: 0.00002836
Iteration 128/1000 | Loss: 0.00002835
Iteration 129/1000 | Loss: 0.00002834
Iteration 130/1000 | Loss: 0.00002834
Iteration 131/1000 | Loss: 0.00002832
Iteration 132/1000 | Loss: 0.00002832
Iteration 133/1000 | Loss: 0.00002831
Iteration 134/1000 | Loss: 0.00002831
Iteration 135/1000 | Loss: 0.00002829
Iteration 136/1000 | Loss: 0.00002829
Iteration 137/1000 | Loss: 0.00002828
Iteration 138/1000 | Loss: 0.00002827
Iteration 139/1000 | Loss: 0.00002827
Iteration 140/1000 | Loss: 0.00002826
Iteration 141/1000 | Loss: 0.00002826
Iteration 142/1000 | Loss: 0.00002826
Iteration 143/1000 | Loss: 0.00002826
Iteration 144/1000 | Loss: 0.00002825
Iteration 145/1000 | Loss: 0.00002825
Iteration 146/1000 | Loss: 0.00002825
Iteration 147/1000 | Loss: 0.00002825
Iteration 148/1000 | Loss: 0.00002825
Iteration 149/1000 | Loss: 0.00002825
Iteration 150/1000 | Loss: 0.00002825
Iteration 151/1000 | Loss: 0.00002825
Iteration 152/1000 | Loss: 0.00002824
Iteration 153/1000 | Loss: 0.00002824
Iteration 154/1000 | Loss: 0.00002824
Iteration 155/1000 | Loss: 0.00002824
Iteration 156/1000 | Loss: 0.00002824
Iteration 157/1000 | Loss: 0.00002824
Iteration 158/1000 | Loss: 0.00002823
Iteration 159/1000 | Loss: 0.00006273
Iteration 160/1000 | Loss: 0.00002848
Iteration 161/1000 | Loss: 0.00002820
Iteration 162/1000 | Loss: 0.00002820
Iteration 163/1000 | Loss: 0.00002819
Iteration 164/1000 | Loss: 0.00002819
Iteration 165/1000 | Loss: 0.00002819
Iteration 166/1000 | Loss: 0.00002819
Iteration 167/1000 | Loss: 0.00002818
Iteration 168/1000 | Loss: 0.00002818
Iteration 169/1000 | Loss: 0.00002818
Iteration 170/1000 | Loss: 0.00002818
Iteration 171/1000 | Loss: 0.00002818
Iteration 172/1000 | Loss: 0.00002818
Iteration 173/1000 | Loss: 0.00002818
Iteration 174/1000 | Loss: 0.00002818
Iteration 175/1000 | Loss: 0.00002818
Iteration 176/1000 | Loss: 0.00002818
Iteration 177/1000 | Loss: 0.00002818
Iteration 178/1000 | Loss: 0.00002817
Iteration 179/1000 | Loss: 0.00002817
Iteration 180/1000 | Loss: 0.00002817
Iteration 181/1000 | Loss: 0.00002817
Iteration 182/1000 | Loss: 0.00002817
Iteration 183/1000 | Loss: 0.00002817
Iteration 184/1000 | Loss: 0.00002817
Iteration 185/1000 | Loss: 0.00002817
Iteration 186/1000 | Loss: 0.00002817
Iteration 187/1000 | Loss: 0.00002817
Iteration 188/1000 | Loss: 0.00002817
Iteration 189/1000 | Loss: 0.00002816
Iteration 190/1000 | Loss: 0.00002816
Iteration 191/1000 | Loss: 0.00002816
Iteration 192/1000 | Loss: 0.00002816
Iteration 193/1000 | Loss: 0.00002816
Iteration 194/1000 | Loss: 0.00002816
Iteration 195/1000 | Loss: 0.00002816
Iteration 196/1000 | Loss: 0.00002816
Iteration 197/1000 | Loss: 0.00002816
Iteration 198/1000 | Loss: 0.00002816
Iteration 199/1000 | Loss: 0.00002816
Iteration 200/1000 | Loss: 0.00006281
Iteration 201/1000 | Loss: 0.00002818
Iteration 202/1000 | Loss: 0.00002813
Iteration 203/1000 | Loss: 0.00002813
Iteration 204/1000 | Loss: 0.00002813
Iteration 205/1000 | Loss: 0.00002813
Iteration 206/1000 | Loss: 0.00002813
Iteration 207/1000 | Loss: 0.00002813
Iteration 208/1000 | Loss: 0.00002813
Iteration 209/1000 | Loss: 0.00002813
Iteration 210/1000 | Loss: 0.00002813
Iteration 211/1000 | Loss: 0.00002813
Iteration 212/1000 | Loss: 0.00002812
Iteration 213/1000 | Loss: 0.00002812
Iteration 214/1000 | Loss: 0.00002812
Iteration 215/1000 | Loss: 0.00002812
Iteration 216/1000 | Loss: 0.00002812
Iteration 217/1000 | Loss: 0.00002812
Iteration 218/1000 | Loss: 0.00002812
Iteration 219/1000 | Loss: 0.00002812
Iteration 220/1000 | Loss: 0.00002812
Iteration 221/1000 | Loss: 0.00002812
Iteration 222/1000 | Loss: 0.00002812
Iteration 223/1000 | Loss: 0.00002812
Iteration 224/1000 | Loss: 0.00002812
Iteration 225/1000 | Loss: 0.00002812
Iteration 226/1000 | Loss: 0.00002812
Iteration 227/1000 | Loss: 0.00002812
Iteration 228/1000 | Loss: 0.00002812
Iteration 229/1000 | Loss: 0.00002812
Iteration 230/1000 | Loss: 0.00002812
Iteration 231/1000 | Loss: 0.00002812
Iteration 232/1000 | Loss: 0.00002812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [2.8118309273850173e-05, 2.8118309273850173e-05, 2.8118309273850173e-05, 2.8118309273850173e-05, 2.8118309273850173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8118309273850173e-05

Optimization complete. Final v2v error: 3.756728172302246 mm

Highest mean error: 17.838510513305664 mm for frame 41

Lowest mean error: 2.809566020965576 mm for frame 94

Saving results

Total time: 184.40733861923218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460907
Iteration 2/25 | Loss: 0.00120287
Iteration 3/25 | Loss: 0.00085259
Iteration 4/25 | Loss: 0.00079968
Iteration 5/25 | Loss: 0.00079614
Iteration 6/25 | Loss: 0.00079540
Iteration 7/25 | Loss: 0.00079540
Iteration 8/25 | Loss: 0.00079540
Iteration 9/25 | Loss: 0.00079540
Iteration 10/25 | Loss: 0.00079540
Iteration 11/25 | Loss: 0.00079540
Iteration 12/25 | Loss: 0.00079540
Iteration 13/25 | Loss: 0.00079540
Iteration 14/25 | Loss: 0.00079540
Iteration 15/25 | Loss: 0.00079540
Iteration 16/25 | Loss: 0.00079540
Iteration 17/25 | Loss: 0.00079540
Iteration 18/25 | Loss: 0.00079540
Iteration 19/25 | Loss: 0.00079540
Iteration 20/25 | Loss: 0.00079540
Iteration 21/25 | Loss: 0.00079540
Iteration 22/25 | Loss: 0.00079540
Iteration 23/25 | Loss: 0.00079540
Iteration 24/25 | Loss: 0.00079540
Iteration 25/25 | Loss: 0.00079540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007954007596708834, 0.0007954007596708834, 0.0007954007596708834, 0.0007954007596708834, 0.0007954007596708834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007954007596708834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55282557
Iteration 2/25 | Loss: 0.00106808
Iteration 3/25 | Loss: 0.00106808
Iteration 4/25 | Loss: 0.00106808
Iteration 5/25 | Loss: 0.00106808
Iteration 6/25 | Loss: 0.00106808
Iteration 7/25 | Loss: 0.00106808
Iteration 8/25 | Loss: 0.00106808
Iteration 9/25 | Loss: 0.00106808
Iteration 10/25 | Loss: 0.00106808
Iteration 11/25 | Loss: 0.00106808
Iteration 12/25 | Loss: 0.00106808
Iteration 13/25 | Loss: 0.00106808
Iteration 14/25 | Loss: 0.00106808
Iteration 15/25 | Loss: 0.00106808
Iteration 16/25 | Loss: 0.00106808
Iteration 17/25 | Loss: 0.00106808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001068078214302659, 0.001068078214302659, 0.001068078214302659, 0.001068078214302659, 0.001068078214302659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001068078214302659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106808
Iteration 2/1000 | Loss: 0.00003175
Iteration 3/1000 | Loss: 0.00002204
Iteration 4/1000 | Loss: 0.00002028
Iteration 5/1000 | Loss: 0.00001939
Iteration 6/1000 | Loss: 0.00001871
Iteration 7/1000 | Loss: 0.00001832
Iteration 8/1000 | Loss: 0.00001806
Iteration 9/1000 | Loss: 0.00001804
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001793
Iteration 12/1000 | Loss: 0.00001793
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001778
Iteration 16/1000 | Loss: 0.00001772
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001770
Iteration 19/1000 | Loss: 0.00001770
Iteration 20/1000 | Loss: 0.00001769
Iteration 21/1000 | Loss: 0.00001769
Iteration 22/1000 | Loss: 0.00001768
Iteration 23/1000 | Loss: 0.00001768
Iteration 24/1000 | Loss: 0.00001768
Iteration 25/1000 | Loss: 0.00001768
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001766
Iteration 28/1000 | Loss: 0.00001766
Iteration 29/1000 | Loss: 0.00001765
Iteration 30/1000 | Loss: 0.00001764
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001764
Iteration 33/1000 | Loss: 0.00001764
Iteration 34/1000 | Loss: 0.00001764
Iteration 35/1000 | Loss: 0.00001764
Iteration 36/1000 | Loss: 0.00001763
Iteration 37/1000 | Loss: 0.00001763
Iteration 38/1000 | Loss: 0.00001763
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001763
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001762
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001762
Iteration 59/1000 | Loss: 0.00001762
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001762
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001762
Iteration 68/1000 | Loss: 0.00001762
Iteration 69/1000 | Loss: 0.00001762
Iteration 70/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.7616375771467574e-05, 1.7616375771467574e-05, 1.7616375771467574e-05, 1.7616375771467574e-05, 1.7616375771467574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7616375771467574e-05

Optimization complete. Final v2v error: 3.4773244857788086 mm

Highest mean error: 3.724513053894043 mm for frame 47

Lowest mean error: 3.321687698364258 mm for frame 231

Saving results

Total time: 29.30428695678711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980940
Iteration 2/25 | Loss: 0.00128057
Iteration 3/25 | Loss: 0.00098368
Iteration 4/25 | Loss: 0.00092034
Iteration 5/25 | Loss: 0.00088745
Iteration 6/25 | Loss: 0.00089096
Iteration 7/25 | Loss: 0.00087906
Iteration 8/25 | Loss: 0.00087623
Iteration 9/25 | Loss: 0.00087503
Iteration 10/25 | Loss: 0.00087459
Iteration 11/25 | Loss: 0.00087439
Iteration 12/25 | Loss: 0.00087419
Iteration 13/25 | Loss: 0.00087409
Iteration 14/25 | Loss: 0.00087409
Iteration 15/25 | Loss: 0.00087409
Iteration 16/25 | Loss: 0.00087409
Iteration 17/25 | Loss: 0.00087408
Iteration 18/25 | Loss: 0.00087408
Iteration 19/25 | Loss: 0.00087408
Iteration 20/25 | Loss: 0.00087407
Iteration 21/25 | Loss: 0.00087407
Iteration 22/25 | Loss: 0.00087407
Iteration 23/25 | Loss: 0.00087407
Iteration 24/25 | Loss: 0.00087407
Iteration 25/25 | Loss: 0.00087407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18122005
Iteration 2/25 | Loss: 0.00051806
Iteration 3/25 | Loss: 0.00051802
Iteration 4/25 | Loss: 0.00051802
Iteration 5/25 | Loss: 0.00051801
Iteration 6/25 | Loss: 0.00051801
Iteration 7/25 | Loss: 0.00051801
Iteration 8/25 | Loss: 0.00051801
Iteration 9/25 | Loss: 0.00051801
Iteration 10/25 | Loss: 0.00051801
Iteration 11/25 | Loss: 0.00051801
Iteration 12/25 | Loss: 0.00051801
Iteration 13/25 | Loss: 0.00051801
Iteration 14/25 | Loss: 0.00051801
Iteration 15/25 | Loss: 0.00051801
Iteration 16/25 | Loss: 0.00051801
Iteration 17/25 | Loss: 0.00051801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005180126172490418, 0.0005180126172490418, 0.0005180126172490418, 0.0005180126172490418, 0.0005180126172490418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005180126172490418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051801
Iteration 2/1000 | Loss: 0.00005815
Iteration 3/1000 | Loss: 0.00003948
Iteration 4/1000 | Loss: 0.00003452
Iteration 5/1000 | Loss: 0.00003275
Iteration 6/1000 | Loss: 0.00003202
Iteration 7/1000 | Loss: 0.00003124
Iteration 8/1000 | Loss: 0.00003065
Iteration 9/1000 | Loss: 0.00003027
Iteration 10/1000 | Loss: 0.00002997
Iteration 11/1000 | Loss: 0.00002976
Iteration 12/1000 | Loss: 0.00002959
Iteration 13/1000 | Loss: 0.00002944
Iteration 14/1000 | Loss: 0.00002943
Iteration 15/1000 | Loss: 0.00002943
Iteration 16/1000 | Loss: 0.00002942
Iteration 17/1000 | Loss: 0.00002939
Iteration 18/1000 | Loss: 0.00002938
Iteration 19/1000 | Loss: 0.00002938
Iteration 20/1000 | Loss: 0.00002937
Iteration 21/1000 | Loss: 0.00002937
Iteration 22/1000 | Loss: 0.00002937
Iteration 23/1000 | Loss: 0.00002937
Iteration 24/1000 | Loss: 0.00002936
Iteration 25/1000 | Loss: 0.00002936
Iteration 26/1000 | Loss: 0.00002935
Iteration 27/1000 | Loss: 0.00002935
Iteration 28/1000 | Loss: 0.00002935
Iteration 29/1000 | Loss: 0.00002935
Iteration 30/1000 | Loss: 0.00002935
Iteration 31/1000 | Loss: 0.00002935
Iteration 32/1000 | Loss: 0.00002935
Iteration 33/1000 | Loss: 0.00002935
Iteration 34/1000 | Loss: 0.00002935
Iteration 35/1000 | Loss: 0.00002935
Iteration 36/1000 | Loss: 0.00002935
Iteration 37/1000 | Loss: 0.00002935
Iteration 38/1000 | Loss: 0.00002934
Iteration 39/1000 | Loss: 0.00002934
Iteration 40/1000 | Loss: 0.00002934
Iteration 41/1000 | Loss: 0.00002934
Iteration 42/1000 | Loss: 0.00002934
Iteration 43/1000 | Loss: 0.00002934
Iteration 44/1000 | Loss: 0.00002934
Iteration 45/1000 | Loss: 0.00002934
Iteration 46/1000 | Loss: 0.00002933
Iteration 47/1000 | Loss: 0.00002933
Iteration 48/1000 | Loss: 0.00002933
Iteration 49/1000 | Loss: 0.00002933
Iteration 50/1000 | Loss: 0.00002933
Iteration 51/1000 | Loss: 0.00002933
Iteration 52/1000 | Loss: 0.00002933
Iteration 53/1000 | Loss: 0.00002933
Iteration 54/1000 | Loss: 0.00002932
Iteration 55/1000 | Loss: 0.00002932
Iteration 56/1000 | Loss: 0.00002932
Iteration 57/1000 | Loss: 0.00002932
Iteration 58/1000 | Loss: 0.00002932
Iteration 59/1000 | Loss: 0.00002932
Iteration 60/1000 | Loss: 0.00002932
Iteration 61/1000 | Loss: 0.00002932
Iteration 62/1000 | Loss: 0.00002932
Iteration 63/1000 | Loss: 0.00002932
Iteration 64/1000 | Loss: 0.00002932
Iteration 65/1000 | Loss: 0.00002932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [2.9319759050849825e-05, 2.9319759050849825e-05, 2.9319759050849825e-05, 2.9319759050849825e-05, 2.9319759050849825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9319759050849825e-05

Optimization complete. Final v2v error: 4.386344909667969 mm

Highest mean error: 5.076970100402832 mm for frame 106

Lowest mean error: 3.827366352081299 mm for frame 144

Saving results

Total time: 50.28301692008972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977826
Iteration 2/25 | Loss: 0.00245942
Iteration 3/25 | Loss: 0.00180139
Iteration 4/25 | Loss: 0.00162471
Iteration 5/25 | Loss: 0.00137240
Iteration 6/25 | Loss: 0.00121760
Iteration 7/25 | Loss: 0.00118740
Iteration 8/25 | Loss: 0.00133345
Iteration 9/25 | Loss: 0.00206513
Iteration 10/25 | Loss: 0.00119243
Iteration 11/25 | Loss: 0.00091041
Iteration 12/25 | Loss: 0.00087740
Iteration 13/25 | Loss: 0.00086464
Iteration 14/25 | Loss: 0.00086309
Iteration 15/25 | Loss: 0.00086618
Iteration 16/25 | Loss: 0.00086038
Iteration 17/25 | Loss: 0.00085645
Iteration 18/25 | Loss: 0.00085356
Iteration 19/25 | Loss: 0.00085040
Iteration 20/25 | Loss: 0.00084923
Iteration 21/25 | Loss: 0.00084889
Iteration 22/25 | Loss: 0.00084870
Iteration 23/25 | Loss: 0.00084857
Iteration 24/25 | Loss: 0.00084852
Iteration 25/25 | Loss: 0.00084852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48889053
Iteration 2/25 | Loss: 0.00096360
Iteration 3/25 | Loss: 0.00096360
Iteration 4/25 | Loss: 0.00096360
Iteration 5/25 | Loss: 0.00096360
Iteration 6/25 | Loss: 0.00096360
Iteration 7/25 | Loss: 0.00096360
Iteration 8/25 | Loss: 0.00096360
Iteration 9/25 | Loss: 0.00096360
Iteration 10/25 | Loss: 0.00096360
Iteration 11/25 | Loss: 0.00096360
Iteration 12/25 | Loss: 0.00096360
Iteration 13/25 | Loss: 0.00096360
Iteration 14/25 | Loss: 0.00096360
Iteration 15/25 | Loss: 0.00096360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009635952301323414, 0.0009635952301323414, 0.0009635952301323414, 0.0009635952301323414, 0.0009635952301323414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009635952301323414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096360
Iteration 2/1000 | Loss: 0.00004203
Iteration 3/1000 | Loss: 0.00002782
Iteration 4/1000 | Loss: 0.00002613
Iteration 5/1000 | Loss: 0.00002507
Iteration 6/1000 | Loss: 0.00002439
Iteration 7/1000 | Loss: 0.00002388
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002343
Iteration 10/1000 | Loss: 0.00002337
Iteration 11/1000 | Loss: 0.00002337
Iteration 12/1000 | Loss: 0.00002336
Iteration 13/1000 | Loss: 0.00002335
Iteration 14/1000 | Loss: 0.00002335
Iteration 15/1000 | Loss: 0.00002332
Iteration 16/1000 | Loss: 0.00002329
Iteration 17/1000 | Loss: 0.00002329
Iteration 18/1000 | Loss: 0.00002322
Iteration 19/1000 | Loss: 0.00002321
Iteration 20/1000 | Loss: 0.00002321
Iteration 21/1000 | Loss: 0.00002318
Iteration 22/1000 | Loss: 0.00002318
Iteration 23/1000 | Loss: 0.00002318
Iteration 24/1000 | Loss: 0.00002318
Iteration 25/1000 | Loss: 0.00002317
Iteration 26/1000 | Loss: 0.00002317
Iteration 27/1000 | Loss: 0.00002317
Iteration 28/1000 | Loss: 0.00002317
Iteration 29/1000 | Loss: 0.00002317
Iteration 30/1000 | Loss: 0.00002317
Iteration 31/1000 | Loss: 0.00002316
Iteration 32/1000 | Loss: 0.00002316
Iteration 33/1000 | Loss: 0.00002316
Iteration 34/1000 | Loss: 0.00002316
Iteration 35/1000 | Loss: 0.00002315
Iteration 36/1000 | Loss: 0.00002315
Iteration 37/1000 | Loss: 0.00002315
Iteration 38/1000 | Loss: 0.00002315
Iteration 39/1000 | Loss: 0.00002315
Iteration 40/1000 | Loss: 0.00002315
Iteration 41/1000 | Loss: 0.00002315
Iteration 42/1000 | Loss: 0.00002315
Iteration 43/1000 | Loss: 0.00002315
Iteration 44/1000 | Loss: 0.00002315
Iteration 45/1000 | Loss: 0.00002315
Iteration 46/1000 | Loss: 0.00002314
Iteration 47/1000 | Loss: 0.00002314
Iteration 48/1000 | Loss: 0.00002314
Iteration 49/1000 | Loss: 0.00002314
Iteration 50/1000 | Loss: 0.00002314
Iteration 51/1000 | Loss: 0.00002314
Iteration 52/1000 | Loss: 0.00002314
Iteration 53/1000 | Loss: 0.00002314
Iteration 54/1000 | Loss: 0.00002313
Iteration 55/1000 | Loss: 0.00002313
Iteration 56/1000 | Loss: 0.00002313
Iteration 57/1000 | Loss: 0.00002313
Iteration 58/1000 | Loss: 0.00002313
Iteration 59/1000 | Loss: 0.00002313
Iteration 60/1000 | Loss: 0.00002312
Iteration 61/1000 | Loss: 0.00002312
Iteration 62/1000 | Loss: 0.00002312
Iteration 63/1000 | Loss: 0.00002312
Iteration 64/1000 | Loss: 0.00002312
Iteration 65/1000 | Loss: 0.00002312
Iteration 66/1000 | Loss: 0.00002312
Iteration 67/1000 | Loss: 0.00002312
Iteration 68/1000 | Loss: 0.00002312
Iteration 69/1000 | Loss: 0.00002312
Iteration 70/1000 | Loss: 0.00002312
Iteration 71/1000 | Loss: 0.00002312
Iteration 72/1000 | Loss: 0.00002312
Iteration 73/1000 | Loss: 0.00002312
Iteration 74/1000 | Loss: 0.00002312
Iteration 75/1000 | Loss: 0.00002312
Iteration 76/1000 | Loss: 0.00002312
Iteration 77/1000 | Loss: 0.00002311
Iteration 78/1000 | Loss: 0.00002311
Iteration 79/1000 | Loss: 0.00002311
Iteration 80/1000 | Loss: 0.00002311
Iteration 81/1000 | Loss: 0.00002311
Iteration 82/1000 | Loss: 0.00002311
Iteration 83/1000 | Loss: 0.00002311
Iteration 84/1000 | Loss: 0.00002311
Iteration 85/1000 | Loss: 0.00002311
Iteration 86/1000 | Loss: 0.00002311
Iteration 87/1000 | Loss: 0.00002311
Iteration 88/1000 | Loss: 0.00002311
Iteration 89/1000 | Loss: 0.00002310
Iteration 90/1000 | Loss: 0.00002310
Iteration 91/1000 | Loss: 0.00002310
Iteration 92/1000 | Loss: 0.00002310
Iteration 93/1000 | Loss: 0.00002310
Iteration 94/1000 | Loss: 0.00002310
Iteration 95/1000 | Loss: 0.00002310
Iteration 96/1000 | Loss: 0.00002310
Iteration 97/1000 | Loss: 0.00002310
Iteration 98/1000 | Loss: 0.00002310
Iteration 99/1000 | Loss: 0.00002310
Iteration 100/1000 | Loss: 0.00002310
Iteration 101/1000 | Loss: 0.00002310
Iteration 102/1000 | Loss: 0.00002310
Iteration 103/1000 | Loss: 0.00002310
Iteration 104/1000 | Loss: 0.00002310
Iteration 105/1000 | Loss: 0.00002310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.3102573322830722e-05, 2.3102573322830722e-05, 2.3102573322830722e-05, 2.3102573322830722e-05, 2.3102573322830722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3102573322830722e-05

Optimization complete. Final v2v error: 3.9604344367980957 mm

Highest mean error: 4.092381477355957 mm for frame 82

Lowest mean error: 3.7641632556915283 mm for frame 193

Saving results

Total time: 63.626983404159546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834730
Iteration 2/25 | Loss: 0.00101457
Iteration 3/25 | Loss: 0.00079679
Iteration 4/25 | Loss: 0.00075713
Iteration 5/25 | Loss: 0.00074860
Iteration 6/25 | Loss: 0.00074611
Iteration 7/25 | Loss: 0.00074587
Iteration 8/25 | Loss: 0.00074587
Iteration 9/25 | Loss: 0.00074585
Iteration 10/25 | Loss: 0.00074585
Iteration 11/25 | Loss: 0.00074585
Iteration 12/25 | Loss: 0.00074585
Iteration 13/25 | Loss: 0.00074585
Iteration 14/25 | Loss: 0.00074585
Iteration 15/25 | Loss: 0.00074585
Iteration 16/25 | Loss: 0.00074585
Iteration 17/25 | Loss: 0.00074585
Iteration 18/25 | Loss: 0.00074585
Iteration 19/25 | Loss: 0.00074585
Iteration 20/25 | Loss: 0.00074585
Iteration 21/25 | Loss: 0.00074585
Iteration 22/25 | Loss: 0.00074585
Iteration 23/25 | Loss: 0.00074585
Iteration 24/25 | Loss: 0.00074585
Iteration 25/25 | Loss: 0.00074585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49892938
Iteration 2/25 | Loss: 0.00087368
Iteration 3/25 | Loss: 0.00087363
Iteration 4/25 | Loss: 0.00087363
Iteration 5/25 | Loss: 0.00087363
Iteration 6/25 | Loss: 0.00087363
Iteration 7/25 | Loss: 0.00087363
Iteration 8/25 | Loss: 0.00087363
Iteration 9/25 | Loss: 0.00087363
Iteration 10/25 | Loss: 0.00087363
Iteration 11/25 | Loss: 0.00087363
Iteration 12/25 | Loss: 0.00087363
Iteration 13/25 | Loss: 0.00087363
Iteration 14/25 | Loss: 0.00087363
Iteration 15/25 | Loss: 0.00087363
Iteration 16/25 | Loss: 0.00087363
Iteration 17/25 | Loss: 0.00087363
Iteration 18/25 | Loss: 0.00087363
Iteration 19/25 | Loss: 0.00087363
Iteration 20/25 | Loss: 0.00087363
Iteration 21/25 | Loss: 0.00087363
Iteration 22/25 | Loss: 0.00087363
Iteration 23/25 | Loss: 0.00087363
Iteration 24/25 | Loss: 0.00087363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008736262680031359, 0.0008736262680031359, 0.0008736262680031359, 0.0008736262680031359, 0.0008736262680031359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008736262680031359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087363
Iteration 2/1000 | Loss: 0.00003512
Iteration 3/1000 | Loss: 0.00002500
Iteration 4/1000 | Loss: 0.00002066
Iteration 5/1000 | Loss: 0.00001904
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001720
Iteration 8/1000 | Loss: 0.00001673
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001614
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001571
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001564
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001558
Iteration 20/1000 | Loss: 0.00001555
Iteration 21/1000 | Loss: 0.00001550
Iteration 22/1000 | Loss: 0.00001549
Iteration 23/1000 | Loss: 0.00001548
Iteration 24/1000 | Loss: 0.00001547
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001546
Iteration 27/1000 | Loss: 0.00001545
Iteration 28/1000 | Loss: 0.00001545
Iteration 29/1000 | Loss: 0.00001544
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001543
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001542
Iteration 34/1000 | Loss: 0.00001541
Iteration 35/1000 | Loss: 0.00001541
Iteration 36/1000 | Loss: 0.00001540
Iteration 37/1000 | Loss: 0.00001539
Iteration 38/1000 | Loss: 0.00001538
Iteration 39/1000 | Loss: 0.00001537
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001537
Iteration 44/1000 | Loss: 0.00001537
Iteration 45/1000 | Loss: 0.00001537
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001536
Iteration 48/1000 | Loss: 0.00001536
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001535
Iteration 51/1000 | Loss: 0.00001535
Iteration 52/1000 | Loss: 0.00001535
Iteration 53/1000 | Loss: 0.00001535
Iteration 54/1000 | Loss: 0.00001535
Iteration 55/1000 | Loss: 0.00001534
Iteration 56/1000 | Loss: 0.00001534
Iteration 57/1000 | Loss: 0.00001534
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001533
Iteration 65/1000 | Loss: 0.00001533
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001532
Iteration 68/1000 | Loss: 0.00001532
Iteration 69/1000 | Loss: 0.00001532
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001532
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001531
Iteration 77/1000 | Loss: 0.00001531
Iteration 78/1000 | Loss: 0.00001531
Iteration 79/1000 | Loss: 0.00001531
Iteration 80/1000 | Loss: 0.00001531
Iteration 81/1000 | Loss: 0.00001531
Iteration 82/1000 | Loss: 0.00001531
Iteration 83/1000 | Loss: 0.00001531
Iteration 84/1000 | Loss: 0.00001530
Iteration 85/1000 | Loss: 0.00001530
Iteration 86/1000 | Loss: 0.00001530
Iteration 87/1000 | Loss: 0.00001530
Iteration 88/1000 | Loss: 0.00001530
Iteration 89/1000 | Loss: 0.00001530
Iteration 90/1000 | Loss: 0.00001530
Iteration 91/1000 | Loss: 0.00001530
Iteration 92/1000 | Loss: 0.00001529
Iteration 93/1000 | Loss: 0.00001529
Iteration 94/1000 | Loss: 0.00001528
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001528
Iteration 97/1000 | Loss: 0.00001528
Iteration 98/1000 | Loss: 0.00001528
Iteration 99/1000 | Loss: 0.00001528
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001526
Iteration 106/1000 | Loss: 0.00001526
Iteration 107/1000 | Loss: 0.00001526
Iteration 108/1000 | Loss: 0.00001526
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001525
Iteration 111/1000 | Loss: 0.00001525
Iteration 112/1000 | Loss: 0.00001525
Iteration 113/1000 | Loss: 0.00001525
Iteration 114/1000 | Loss: 0.00001524
Iteration 115/1000 | Loss: 0.00001524
Iteration 116/1000 | Loss: 0.00001524
Iteration 117/1000 | Loss: 0.00001524
Iteration 118/1000 | Loss: 0.00001524
Iteration 119/1000 | Loss: 0.00001524
Iteration 120/1000 | Loss: 0.00001523
Iteration 121/1000 | Loss: 0.00001523
Iteration 122/1000 | Loss: 0.00001523
Iteration 123/1000 | Loss: 0.00001523
Iteration 124/1000 | Loss: 0.00001523
Iteration 125/1000 | Loss: 0.00001523
Iteration 126/1000 | Loss: 0.00001523
Iteration 127/1000 | Loss: 0.00001522
Iteration 128/1000 | Loss: 0.00001522
Iteration 129/1000 | Loss: 0.00001522
Iteration 130/1000 | Loss: 0.00001522
Iteration 131/1000 | Loss: 0.00001522
Iteration 132/1000 | Loss: 0.00001522
Iteration 133/1000 | Loss: 0.00001522
Iteration 134/1000 | Loss: 0.00001522
Iteration 135/1000 | Loss: 0.00001522
Iteration 136/1000 | Loss: 0.00001522
Iteration 137/1000 | Loss: 0.00001522
Iteration 138/1000 | Loss: 0.00001522
Iteration 139/1000 | Loss: 0.00001522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.5220694876916241e-05, 1.5220694876916241e-05, 1.5220694876916241e-05, 1.5220694876916241e-05, 1.5220694876916241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5220694876916241e-05

Optimization complete. Final v2v error: 3.2898850440979004 mm

Highest mean error: 4.604948997497559 mm for frame 116

Lowest mean error: 2.812293529510498 mm for frame 181

Saving results

Total time: 44.31060075759888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891701
Iteration 2/25 | Loss: 0.00124384
Iteration 3/25 | Loss: 0.00094515
Iteration 4/25 | Loss: 0.00090693
Iteration 5/25 | Loss: 0.00089973
Iteration 6/25 | Loss: 0.00089899
Iteration 7/25 | Loss: 0.00089899
Iteration 8/25 | Loss: 0.00089899
Iteration 9/25 | Loss: 0.00089899
Iteration 10/25 | Loss: 0.00089899
Iteration 11/25 | Loss: 0.00089899
Iteration 12/25 | Loss: 0.00089899
Iteration 13/25 | Loss: 0.00089899
Iteration 14/25 | Loss: 0.00089899
Iteration 15/25 | Loss: 0.00089899
Iteration 16/25 | Loss: 0.00089899
Iteration 17/25 | Loss: 0.00089899
Iteration 18/25 | Loss: 0.00089899
Iteration 19/25 | Loss: 0.00089899
Iteration 20/25 | Loss: 0.00089899
Iteration 21/25 | Loss: 0.00089899
Iteration 22/25 | Loss: 0.00089899
Iteration 23/25 | Loss: 0.00089899
Iteration 24/25 | Loss: 0.00089899
Iteration 25/25 | Loss: 0.00089899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52475369
Iteration 2/25 | Loss: 0.00108111
Iteration 3/25 | Loss: 0.00108111
Iteration 4/25 | Loss: 0.00108111
Iteration 5/25 | Loss: 0.00108111
Iteration 6/25 | Loss: 0.00108111
Iteration 7/25 | Loss: 0.00108111
Iteration 8/25 | Loss: 0.00108111
Iteration 9/25 | Loss: 0.00108111
Iteration 10/25 | Loss: 0.00108111
Iteration 11/25 | Loss: 0.00108111
Iteration 12/25 | Loss: 0.00108111
Iteration 13/25 | Loss: 0.00108111
Iteration 14/25 | Loss: 0.00108111
Iteration 15/25 | Loss: 0.00108111
Iteration 16/25 | Loss: 0.00108111
Iteration 17/25 | Loss: 0.00108111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001081105088815093, 0.001081105088815093, 0.001081105088815093, 0.001081105088815093, 0.001081105088815093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001081105088815093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108111
Iteration 2/1000 | Loss: 0.00005794
Iteration 3/1000 | Loss: 0.00004622
Iteration 4/1000 | Loss: 0.00004155
Iteration 5/1000 | Loss: 0.00003942
Iteration 6/1000 | Loss: 0.00003793
Iteration 7/1000 | Loss: 0.00003701
Iteration 8/1000 | Loss: 0.00003621
Iteration 9/1000 | Loss: 0.00003582
Iteration 10/1000 | Loss: 0.00003550
Iteration 11/1000 | Loss: 0.00003529
Iteration 12/1000 | Loss: 0.00003513
Iteration 13/1000 | Loss: 0.00003511
Iteration 14/1000 | Loss: 0.00003506
Iteration 15/1000 | Loss: 0.00003505
Iteration 16/1000 | Loss: 0.00003505
Iteration 17/1000 | Loss: 0.00003504
Iteration 18/1000 | Loss: 0.00003503
Iteration 19/1000 | Loss: 0.00003496
Iteration 20/1000 | Loss: 0.00003492
Iteration 21/1000 | Loss: 0.00003491
Iteration 22/1000 | Loss: 0.00003491
Iteration 23/1000 | Loss: 0.00003490
Iteration 24/1000 | Loss: 0.00003490
Iteration 25/1000 | Loss: 0.00003489
Iteration 26/1000 | Loss: 0.00003489
Iteration 27/1000 | Loss: 0.00003488
Iteration 28/1000 | Loss: 0.00003488
Iteration 29/1000 | Loss: 0.00003486
Iteration 30/1000 | Loss: 0.00003485
Iteration 31/1000 | Loss: 0.00003485
Iteration 32/1000 | Loss: 0.00003483
Iteration 33/1000 | Loss: 0.00003483
Iteration 34/1000 | Loss: 0.00003482
Iteration 35/1000 | Loss: 0.00003482
Iteration 36/1000 | Loss: 0.00003481
Iteration 37/1000 | Loss: 0.00003481
Iteration 38/1000 | Loss: 0.00003480
Iteration 39/1000 | Loss: 0.00003480
Iteration 40/1000 | Loss: 0.00003479
Iteration 41/1000 | Loss: 0.00003479
Iteration 42/1000 | Loss: 0.00003479
Iteration 43/1000 | Loss: 0.00003479
Iteration 44/1000 | Loss: 0.00003478
Iteration 45/1000 | Loss: 0.00003478
Iteration 46/1000 | Loss: 0.00003478
Iteration 47/1000 | Loss: 0.00003477
Iteration 48/1000 | Loss: 0.00003476
Iteration 49/1000 | Loss: 0.00003476
Iteration 50/1000 | Loss: 0.00003476
Iteration 51/1000 | Loss: 0.00003476
Iteration 52/1000 | Loss: 0.00003476
Iteration 53/1000 | Loss: 0.00003476
Iteration 54/1000 | Loss: 0.00003476
Iteration 55/1000 | Loss: 0.00003476
Iteration 56/1000 | Loss: 0.00003476
Iteration 57/1000 | Loss: 0.00003476
Iteration 58/1000 | Loss: 0.00003475
Iteration 59/1000 | Loss: 0.00003475
Iteration 60/1000 | Loss: 0.00003475
Iteration 61/1000 | Loss: 0.00003473
Iteration 62/1000 | Loss: 0.00003473
Iteration 63/1000 | Loss: 0.00003473
Iteration 64/1000 | Loss: 0.00003473
Iteration 65/1000 | Loss: 0.00003472
Iteration 66/1000 | Loss: 0.00003472
Iteration 67/1000 | Loss: 0.00003472
Iteration 68/1000 | Loss: 0.00003472
Iteration 69/1000 | Loss: 0.00003472
Iteration 70/1000 | Loss: 0.00003472
Iteration 71/1000 | Loss: 0.00003472
Iteration 72/1000 | Loss: 0.00003472
Iteration 73/1000 | Loss: 0.00003472
Iteration 74/1000 | Loss: 0.00003471
Iteration 75/1000 | Loss: 0.00003471
Iteration 76/1000 | Loss: 0.00003470
Iteration 77/1000 | Loss: 0.00003470
Iteration 78/1000 | Loss: 0.00003470
Iteration 79/1000 | Loss: 0.00003470
Iteration 80/1000 | Loss: 0.00003470
Iteration 81/1000 | Loss: 0.00003470
Iteration 82/1000 | Loss: 0.00003470
Iteration 83/1000 | Loss: 0.00003469
Iteration 84/1000 | Loss: 0.00003469
Iteration 85/1000 | Loss: 0.00003469
Iteration 86/1000 | Loss: 0.00003469
Iteration 87/1000 | Loss: 0.00003469
Iteration 88/1000 | Loss: 0.00003469
Iteration 89/1000 | Loss: 0.00003469
Iteration 90/1000 | Loss: 0.00003469
Iteration 91/1000 | Loss: 0.00003469
Iteration 92/1000 | Loss: 0.00003469
Iteration 93/1000 | Loss: 0.00003469
Iteration 94/1000 | Loss: 0.00003469
Iteration 95/1000 | Loss: 0.00003469
Iteration 96/1000 | Loss: 0.00003469
Iteration 97/1000 | Loss: 0.00003469
Iteration 98/1000 | Loss: 0.00003469
Iteration 99/1000 | Loss: 0.00003469
Iteration 100/1000 | Loss: 0.00003469
Iteration 101/1000 | Loss: 0.00003469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [3.468699287623167e-05, 3.468699287623167e-05, 3.468699287623167e-05, 3.468699287623167e-05, 3.468699287623167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.468699287623167e-05

Optimization complete. Final v2v error: 4.9122700691223145 mm

Highest mean error: 5.332329273223877 mm for frame 23

Lowest mean error: 4.4820966720581055 mm for frame 82

Saving results

Total time: 33.81035780906677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842320
Iteration 2/25 | Loss: 0.00130896
Iteration 3/25 | Loss: 0.00093517
Iteration 4/25 | Loss: 0.00089168
Iteration 5/25 | Loss: 0.00088746
Iteration 6/25 | Loss: 0.00088618
Iteration 7/25 | Loss: 0.00088612
Iteration 8/25 | Loss: 0.00088612
Iteration 9/25 | Loss: 0.00088612
Iteration 10/25 | Loss: 0.00088612
Iteration 11/25 | Loss: 0.00088612
Iteration 12/25 | Loss: 0.00088612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008861204260028899, 0.0008861204260028899, 0.0008861204260028899, 0.0008861204260028899, 0.0008861204260028899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008861204260028899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53733766
Iteration 2/25 | Loss: 0.00095300
Iteration 3/25 | Loss: 0.00095300
Iteration 4/25 | Loss: 0.00095300
Iteration 5/25 | Loss: 0.00095300
Iteration 6/25 | Loss: 0.00095300
Iteration 7/25 | Loss: 0.00095300
Iteration 8/25 | Loss: 0.00095300
Iteration 9/25 | Loss: 0.00095300
Iteration 10/25 | Loss: 0.00095300
Iteration 11/25 | Loss: 0.00095300
Iteration 12/25 | Loss: 0.00095300
Iteration 13/25 | Loss: 0.00095300
Iteration 14/25 | Loss: 0.00095300
Iteration 15/25 | Loss: 0.00095300
Iteration 16/25 | Loss: 0.00095300
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009529997478239238, 0.0009529997478239238, 0.0009529997478239238, 0.0009529997478239238, 0.0009529997478239238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009529997478239238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095300
Iteration 2/1000 | Loss: 0.00005625
Iteration 3/1000 | Loss: 0.00004402
Iteration 4/1000 | Loss: 0.00004076
Iteration 5/1000 | Loss: 0.00003933
Iteration 6/1000 | Loss: 0.00003780
Iteration 7/1000 | Loss: 0.00003654
Iteration 8/1000 | Loss: 0.00003563
Iteration 9/1000 | Loss: 0.00003517
Iteration 10/1000 | Loss: 0.00003487
Iteration 11/1000 | Loss: 0.00003465
Iteration 12/1000 | Loss: 0.00003448
Iteration 13/1000 | Loss: 0.00003444
Iteration 14/1000 | Loss: 0.00003444
Iteration 15/1000 | Loss: 0.00003442
Iteration 16/1000 | Loss: 0.00003442
Iteration 17/1000 | Loss: 0.00003441
Iteration 18/1000 | Loss: 0.00003441
Iteration 19/1000 | Loss: 0.00003441
Iteration 20/1000 | Loss: 0.00003441
Iteration 21/1000 | Loss: 0.00003441
Iteration 22/1000 | Loss: 0.00003441
Iteration 23/1000 | Loss: 0.00003441
Iteration 24/1000 | Loss: 0.00003440
Iteration 25/1000 | Loss: 0.00003440
Iteration 26/1000 | Loss: 0.00003440
Iteration 27/1000 | Loss: 0.00003440
Iteration 28/1000 | Loss: 0.00003440
Iteration 29/1000 | Loss: 0.00003440
Iteration 30/1000 | Loss: 0.00003440
Iteration 31/1000 | Loss: 0.00003439
Iteration 32/1000 | Loss: 0.00003439
Iteration 33/1000 | Loss: 0.00003439
Iteration 34/1000 | Loss: 0.00003438
Iteration 35/1000 | Loss: 0.00003438
Iteration 36/1000 | Loss: 0.00003438
Iteration 37/1000 | Loss: 0.00003438
Iteration 38/1000 | Loss: 0.00003437
Iteration 39/1000 | Loss: 0.00003437
Iteration 40/1000 | Loss: 0.00003437
Iteration 41/1000 | Loss: 0.00003436
Iteration 42/1000 | Loss: 0.00003436
Iteration 43/1000 | Loss: 0.00003436
Iteration 44/1000 | Loss: 0.00003436
Iteration 45/1000 | Loss: 0.00003436
Iteration 46/1000 | Loss: 0.00003436
Iteration 47/1000 | Loss: 0.00003436
Iteration 48/1000 | Loss: 0.00003436
Iteration 49/1000 | Loss: 0.00003436
Iteration 50/1000 | Loss: 0.00003435
Iteration 51/1000 | Loss: 0.00003435
Iteration 52/1000 | Loss: 0.00003435
Iteration 53/1000 | Loss: 0.00003435
Iteration 54/1000 | Loss: 0.00003435
Iteration 55/1000 | Loss: 0.00003435
Iteration 56/1000 | Loss: 0.00003435
Iteration 57/1000 | Loss: 0.00003435
Iteration 58/1000 | Loss: 0.00003434
Iteration 59/1000 | Loss: 0.00003434
Iteration 60/1000 | Loss: 0.00003434
Iteration 61/1000 | Loss: 0.00003434
Iteration 62/1000 | Loss: 0.00003434
Iteration 63/1000 | Loss: 0.00003433
Iteration 64/1000 | Loss: 0.00003433
Iteration 65/1000 | Loss: 0.00003433
Iteration 66/1000 | Loss: 0.00003433
Iteration 67/1000 | Loss: 0.00003433
Iteration 68/1000 | Loss: 0.00003433
Iteration 69/1000 | Loss: 0.00003433
Iteration 70/1000 | Loss: 0.00003433
Iteration 71/1000 | Loss: 0.00003432
Iteration 72/1000 | Loss: 0.00003432
Iteration 73/1000 | Loss: 0.00003432
Iteration 74/1000 | Loss: 0.00003431
Iteration 75/1000 | Loss: 0.00003431
Iteration 76/1000 | Loss: 0.00003431
Iteration 77/1000 | Loss: 0.00003431
Iteration 78/1000 | Loss: 0.00003431
Iteration 79/1000 | Loss: 0.00003431
Iteration 80/1000 | Loss: 0.00003431
Iteration 81/1000 | Loss: 0.00003431
Iteration 82/1000 | Loss: 0.00003430
Iteration 83/1000 | Loss: 0.00003430
Iteration 84/1000 | Loss: 0.00003430
Iteration 85/1000 | Loss: 0.00003430
Iteration 86/1000 | Loss: 0.00003430
Iteration 87/1000 | Loss: 0.00003430
Iteration 88/1000 | Loss: 0.00003430
Iteration 89/1000 | Loss: 0.00003430
Iteration 90/1000 | Loss: 0.00003430
Iteration 91/1000 | Loss: 0.00003430
Iteration 92/1000 | Loss: 0.00003429
Iteration 93/1000 | Loss: 0.00003429
Iteration 94/1000 | Loss: 0.00003429
Iteration 95/1000 | Loss: 0.00003429
Iteration 96/1000 | Loss: 0.00003429
Iteration 97/1000 | Loss: 0.00003429
Iteration 98/1000 | Loss: 0.00003429
Iteration 99/1000 | Loss: 0.00003429
Iteration 100/1000 | Loss: 0.00003429
Iteration 101/1000 | Loss: 0.00003429
Iteration 102/1000 | Loss: 0.00003429
Iteration 103/1000 | Loss: 0.00003429
Iteration 104/1000 | Loss: 0.00003428
Iteration 105/1000 | Loss: 0.00003428
Iteration 106/1000 | Loss: 0.00003428
Iteration 107/1000 | Loss: 0.00003428
Iteration 108/1000 | Loss: 0.00003428
Iteration 109/1000 | Loss: 0.00003428
Iteration 110/1000 | Loss: 0.00003428
Iteration 111/1000 | Loss: 0.00003428
Iteration 112/1000 | Loss: 0.00003428
Iteration 113/1000 | Loss: 0.00003428
Iteration 114/1000 | Loss: 0.00003428
Iteration 115/1000 | Loss: 0.00003427
Iteration 116/1000 | Loss: 0.00003427
Iteration 117/1000 | Loss: 0.00003427
Iteration 118/1000 | Loss: 0.00003427
Iteration 119/1000 | Loss: 0.00003427
Iteration 120/1000 | Loss: 0.00003427
Iteration 121/1000 | Loss: 0.00003427
Iteration 122/1000 | Loss: 0.00003427
Iteration 123/1000 | Loss: 0.00003427
Iteration 124/1000 | Loss: 0.00003427
Iteration 125/1000 | Loss: 0.00003427
Iteration 126/1000 | Loss: 0.00003426
Iteration 127/1000 | Loss: 0.00003426
Iteration 128/1000 | Loss: 0.00003426
Iteration 129/1000 | Loss: 0.00003426
Iteration 130/1000 | Loss: 0.00003426
Iteration 131/1000 | Loss: 0.00003426
Iteration 132/1000 | Loss: 0.00003426
Iteration 133/1000 | Loss: 0.00003426
Iteration 134/1000 | Loss: 0.00003425
Iteration 135/1000 | Loss: 0.00003425
Iteration 136/1000 | Loss: 0.00003425
Iteration 137/1000 | Loss: 0.00003425
Iteration 138/1000 | Loss: 0.00003425
Iteration 139/1000 | Loss: 0.00003425
Iteration 140/1000 | Loss: 0.00003425
Iteration 141/1000 | Loss: 0.00003425
Iteration 142/1000 | Loss: 0.00003425
Iteration 143/1000 | Loss: 0.00003425
Iteration 144/1000 | Loss: 0.00003425
Iteration 145/1000 | Loss: 0.00003424
Iteration 146/1000 | Loss: 0.00003424
Iteration 147/1000 | Loss: 0.00003424
Iteration 148/1000 | Loss: 0.00003424
Iteration 149/1000 | Loss: 0.00003424
Iteration 150/1000 | Loss: 0.00003424
Iteration 151/1000 | Loss: 0.00003424
Iteration 152/1000 | Loss: 0.00003424
Iteration 153/1000 | Loss: 0.00003424
Iteration 154/1000 | Loss: 0.00003424
Iteration 155/1000 | Loss: 0.00003424
Iteration 156/1000 | Loss: 0.00003424
Iteration 157/1000 | Loss: 0.00003424
Iteration 158/1000 | Loss: 0.00003423
Iteration 159/1000 | Loss: 0.00003423
Iteration 160/1000 | Loss: 0.00003423
Iteration 161/1000 | Loss: 0.00003423
Iteration 162/1000 | Loss: 0.00003423
Iteration 163/1000 | Loss: 0.00003423
Iteration 164/1000 | Loss: 0.00003423
Iteration 165/1000 | Loss: 0.00003423
Iteration 166/1000 | Loss: 0.00003423
Iteration 167/1000 | Loss: 0.00003423
Iteration 168/1000 | Loss: 0.00003423
Iteration 169/1000 | Loss: 0.00003423
Iteration 170/1000 | Loss: 0.00003422
Iteration 171/1000 | Loss: 0.00003422
Iteration 172/1000 | Loss: 0.00003422
Iteration 173/1000 | Loss: 0.00003422
Iteration 174/1000 | Loss: 0.00003422
Iteration 175/1000 | Loss: 0.00003422
Iteration 176/1000 | Loss: 0.00003422
Iteration 177/1000 | Loss: 0.00003422
Iteration 178/1000 | Loss: 0.00003422
Iteration 179/1000 | Loss: 0.00003422
Iteration 180/1000 | Loss: 0.00003421
Iteration 181/1000 | Loss: 0.00003421
Iteration 182/1000 | Loss: 0.00003421
Iteration 183/1000 | Loss: 0.00003421
Iteration 184/1000 | Loss: 0.00003421
Iteration 185/1000 | Loss: 0.00003421
Iteration 186/1000 | Loss: 0.00003420
Iteration 187/1000 | Loss: 0.00003420
Iteration 188/1000 | Loss: 0.00003420
Iteration 189/1000 | Loss: 0.00003420
Iteration 190/1000 | Loss: 0.00003420
Iteration 191/1000 | Loss: 0.00003420
Iteration 192/1000 | Loss: 0.00003420
Iteration 193/1000 | Loss: 0.00003420
Iteration 194/1000 | Loss: 0.00003419
Iteration 195/1000 | Loss: 0.00003419
Iteration 196/1000 | Loss: 0.00003419
Iteration 197/1000 | Loss: 0.00003419
Iteration 198/1000 | Loss: 0.00003419
Iteration 199/1000 | Loss: 0.00003419
Iteration 200/1000 | Loss: 0.00003419
Iteration 201/1000 | Loss: 0.00003419
Iteration 202/1000 | Loss: 0.00003419
Iteration 203/1000 | Loss: 0.00003419
Iteration 204/1000 | Loss: 0.00003418
Iteration 205/1000 | Loss: 0.00003418
Iteration 206/1000 | Loss: 0.00003418
Iteration 207/1000 | Loss: 0.00003418
Iteration 208/1000 | Loss: 0.00003418
Iteration 209/1000 | Loss: 0.00003418
Iteration 210/1000 | Loss: 0.00003418
Iteration 211/1000 | Loss: 0.00003418
Iteration 212/1000 | Loss: 0.00003418
Iteration 213/1000 | Loss: 0.00003418
Iteration 214/1000 | Loss: 0.00003418
Iteration 215/1000 | Loss: 0.00003418
Iteration 216/1000 | Loss: 0.00003418
Iteration 217/1000 | Loss: 0.00003418
Iteration 218/1000 | Loss: 0.00003418
Iteration 219/1000 | Loss: 0.00003418
Iteration 220/1000 | Loss: 0.00003418
Iteration 221/1000 | Loss: 0.00003418
Iteration 222/1000 | Loss: 0.00003418
Iteration 223/1000 | Loss: 0.00003418
Iteration 224/1000 | Loss: 0.00003418
Iteration 225/1000 | Loss: 0.00003418
Iteration 226/1000 | Loss: 0.00003418
Iteration 227/1000 | Loss: 0.00003418
Iteration 228/1000 | Loss: 0.00003418
Iteration 229/1000 | Loss: 0.00003418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [3.418098276597448e-05, 3.418098276597448e-05, 3.418098276597448e-05, 3.418098276597448e-05, 3.418098276597448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.418098276597448e-05

Optimization complete. Final v2v error: 4.691563129425049 mm

Highest mean error: 4.942099094390869 mm for frame 40

Lowest mean error: 4.320899963378906 mm for frame 120

Saving results

Total time: 37.62843990325928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936230
Iteration 2/25 | Loss: 0.00085418
Iteration 3/25 | Loss: 0.00075227
Iteration 4/25 | Loss: 0.00072970
Iteration 5/25 | Loss: 0.00072556
Iteration 6/25 | Loss: 0.00072513
Iteration 7/25 | Loss: 0.00072513
Iteration 8/25 | Loss: 0.00072513
Iteration 9/25 | Loss: 0.00072513
Iteration 10/25 | Loss: 0.00072513
Iteration 11/25 | Loss: 0.00072513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007251338684000075, 0.0007251338684000075, 0.0007251338684000075, 0.0007251338684000075, 0.0007251338684000075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007251338684000075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59857535
Iteration 2/25 | Loss: 0.00080967
Iteration 3/25 | Loss: 0.00080967
Iteration 4/25 | Loss: 0.00080967
Iteration 5/25 | Loss: 0.00080967
Iteration 6/25 | Loss: 0.00080967
Iteration 7/25 | Loss: 0.00080967
Iteration 8/25 | Loss: 0.00080967
Iteration 9/25 | Loss: 0.00080967
Iteration 10/25 | Loss: 0.00080967
Iteration 11/25 | Loss: 0.00080967
Iteration 12/25 | Loss: 0.00080967
Iteration 13/25 | Loss: 0.00080967
Iteration 14/25 | Loss: 0.00080967
Iteration 15/25 | Loss: 0.00080967
Iteration 16/25 | Loss: 0.00080967
Iteration 17/25 | Loss: 0.00080967
Iteration 18/25 | Loss: 0.00080967
Iteration 19/25 | Loss: 0.00080967
Iteration 20/25 | Loss: 0.00080967
Iteration 21/25 | Loss: 0.00080967
Iteration 22/25 | Loss: 0.00080967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008096706587821245, 0.0008096706587821245, 0.0008096706587821245, 0.0008096706587821245, 0.0008096706587821245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008096706587821245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080967
Iteration 2/1000 | Loss: 0.00002744
Iteration 3/1000 | Loss: 0.00002027
Iteration 4/1000 | Loss: 0.00001903
Iteration 5/1000 | Loss: 0.00001843
Iteration 6/1000 | Loss: 0.00001789
Iteration 7/1000 | Loss: 0.00001755
Iteration 8/1000 | Loss: 0.00001728
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001711
Iteration 11/1000 | Loss: 0.00001710
Iteration 12/1000 | Loss: 0.00001709
Iteration 13/1000 | Loss: 0.00001708
Iteration 14/1000 | Loss: 0.00001704
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001698
Iteration 17/1000 | Loss: 0.00001698
Iteration 18/1000 | Loss: 0.00001697
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001696
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001693
Iteration 24/1000 | Loss: 0.00001693
Iteration 25/1000 | Loss: 0.00001693
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001692
Iteration 28/1000 | Loss: 0.00001687
Iteration 29/1000 | Loss: 0.00001686
Iteration 30/1000 | Loss: 0.00001686
Iteration 31/1000 | Loss: 0.00001685
Iteration 32/1000 | Loss: 0.00001684
Iteration 33/1000 | Loss: 0.00001682
Iteration 34/1000 | Loss: 0.00001682
Iteration 35/1000 | Loss: 0.00001681
Iteration 36/1000 | Loss: 0.00001681
Iteration 37/1000 | Loss: 0.00001681
Iteration 38/1000 | Loss: 0.00001680
Iteration 39/1000 | Loss: 0.00001680
Iteration 40/1000 | Loss: 0.00001680
Iteration 41/1000 | Loss: 0.00001680
Iteration 42/1000 | Loss: 0.00001680
Iteration 43/1000 | Loss: 0.00001680
Iteration 44/1000 | Loss: 0.00001680
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001679
Iteration 47/1000 | Loss: 0.00001679
Iteration 48/1000 | Loss: 0.00001679
Iteration 49/1000 | Loss: 0.00001679
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001679
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001674
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001673
Iteration 69/1000 | Loss: 0.00001673
Iteration 70/1000 | Loss: 0.00001673
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001663
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001657
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001657
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001656
Iteration 85/1000 | Loss: 0.00001656
Iteration 86/1000 | Loss: 0.00001656
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001656
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.6564601537538692e-05, 1.6564601537538692e-05, 1.6564601537538692e-05, 1.6564601537538692e-05, 1.6564601537538692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6564601537538692e-05

Optimization complete. Final v2v error: 3.446549654006958 mm

Highest mean error: 3.9314627647399902 mm for frame 144

Lowest mean error: 3.377922534942627 mm for frame 136

Saving results

Total time: 34.87257242202759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436834
Iteration 2/25 | Loss: 0.00107093
Iteration 3/25 | Loss: 0.00077461
Iteration 4/25 | Loss: 0.00073950
Iteration 5/25 | Loss: 0.00073025
Iteration 6/25 | Loss: 0.00072655
Iteration 7/25 | Loss: 0.00072556
Iteration 8/25 | Loss: 0.00072532
Iteration 9/25 | Loss: 0.00072532
Iteration 10/25 | Loss: 0.00072532
Iteration 11/25 | Loss: 0.00072532
Iteration 12/25 | Loss: 0.00072532
Iteration 13/25 | Loss: 0.00072532
Iteration 14/25 | Loss: 0.00072532
Iteration 15/25 | Loss: 0.00072532
Iteration 16/25 | Loss: 0.00072532
Iteration 17/25 | Loss: 0.00072532
Iteration 18/25 | Loss: 0.00072532
Iteration 19/25 | Loss: 0.00072532
Iteration 20/25 | Loss: 0.00072532
Iteration 21/25 | Loss: 0.00072532
Iteration 22/25 | Loss: 0.00072532
Iteration 23/25 | Loss: 0.00072532
Iteration 24/25 | Loss: 0.00072532
Iteration 25/25 | Loss: 0.00072532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000725318503100425, 0.000725318503100425, 0.000725318503100425, 0.000725318503100425, 0.000725318503100425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000725318503100425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57053709
Iteration 2/25 | Loss: 0.00077056
Iteration 3/25 | Loss: 0.00077054
Iteration 4/25 | Loss: 0.00077054
Iteration 5/25 | Loss: 0.00077054
Iteration 6/25 | Loss: 0.00077054
Iteration 7/25 | Loss: 0.00077054
Iteration 8/25 | Loss: 0.00077054
Iteration 9/25 | Loss: 0.00077054
Iteration 10/25 | Loss: 0.00077054
Iteration 11/25 | Loss: 0.00077054
Iteration 12/25 | Loss: 0.00077054
Iteration 13/25 | Loss: 0.00077054
Iteration 14/25 | Loss: 0.00077054
Iteration 15/25 | Loss: 0.00077054
Iteration 16/25 | Loss: 0.00077054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007705370080657303, 0.0007705370080657303, 0.0007705370080657303, 0.0007705370080657303, 0.0007705370080657303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007705370080657303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077054
Iteration 2/1000 | Loss: 0.00003741
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00001714
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001475
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001418
Iteration 10/1000 | Loss: 0.00001400
Iteration 11/1000 | Loss: 0.00001398
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001368
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001367
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001366
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001363
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001363
Iteration 41/1000 | Loss: 0.00001363
Iteration 42/1000 | Loss: 0.00001363
Iteration 43/1000 | Loss: 0.00001363
Iteration 44/1000 | Loss: 0.00001363
Iteration 45/1000 | Loss: 0.00001363
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001362
Iteration 48/1000 | Loss: 0.00001362
Iteration 49/1000 | Loss: 0.00001362
Iteration 50/1000 | Loss: 0.00001362
Iteration 51/1000 | Loss: 0.00001362
Iteration 52/1000 | Loss: 0.00001362
Iteration 53/1000 | Loss: 0.00001361
Iteration 54/1000 | Loss: 0.00001361
Iteration 55/1000 | Loss: 0.00001361
Iteration 56/1000 | Loss: 0.00001361
Iteration 57/1000 | Loss: 0.00001361
Iteration 58/1000 | Loss: 0.00001361
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001360
Iteration 67/1000 | Loss: 0.00001360
Iteration 68/1000 | Loss: 0.00001360
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001360
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001358
Iteration 73/1000 | Loss: 0.00001358
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001356
Iteration 78/1000 | Loss: 0.00001356
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001355
Iteration 87/1000 | Loss: 0.00001355
Iteration 88/1000 | Loss: 0.00001355
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001355
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001355
Iteration 104/1000 | Loss: 0.00001355
Iteration 105/1000 | Loss: 0.00001355
Iteration 106/1000 | Loss: 0.00001355
Iteration 107/1000 | Loss: 0.00001355
Iteration 108/1000 | Loss: 0.00001355
Iteration 109/1000 | Loss: 0.00001355
Iteration 110/1000 | Loss: 0.00001355
Iteration 111/1000 | Loss: 0.00001355
Iteration 112/1000 | Loss: 0.00001355
Iteration 113/1000 | Loss: 0.00001355
Iteration 114/1000 | Loss: 0.00001355
Iteration 115/1000 | Loss: 0.00001355
Iteration 116/1000 | Loss: 0.00001355
Iteration 117/1000 | Loss: 0.00001355
Iteration 118/1000 | Loss: 0.00001355
Iteration 119/1000 | Loss: 0.00001355
Iteration 120/1000 | Loss: 0.00001355
Iteration 121/1000 | Loss: 0.00001355
Iteration 122/1000 | Loss: 0.00001355
Iteration 123/1000 | Loss: 0.00001355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.354918822471518e-05, 1.354918822471518e-05, 1.354918822471518e-05, 1.354918822471518e-05, 1.354918822471518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.354918822471518e-05

Optimization complete. Final v2v error: 3.02221941947937 mm

Highest mean error: 4.0209150314331055 mm for frame 56

Lowest mean error: 2.3709332942962646 mm for frame 119

Saving results

Total time: 34.84186577796936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391242
Iteration 2/25 | Loss: 0.00082641
Iteration 3/25 | Loss: 0.00072119
Iteration 4/25 | Loss: 0.00070151
Iteration 5/25 | Loss: 0.00069512
Iteration 6/25 | Loss: 0.00069333
Iteration 7/25 | Loss: 0.00069321
Iteration 8/25 | Loss: 0.00069321
Iteration 9/25 | Loss: 0.00069321
Iteration 10/25 | Loss: 0.00069321
Iteration 11/25 | Loss: 0.00069321
Iteration 12/25 | Loss: 0.00069321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006932093529030681, 0.0006932093529030681, 0.0006932093529030681, 0.0006932093529030681, 0.0006932093529030681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006932093529030681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00906324
Iteration 2/25 | Loss: 0.00083955
Iteration 3/25 | Loss: 0.00083955
Iteration 4/25 | Loss: 0.00083955
Iteration 5/25 | Loss: 0.00083955
Iteration 6/25 | Loss: 0.00083955
Iteration 7/25 | Loss: 0.00083955
Iteration 8/25 | Loss: 0.00083955
Iteration 9/25 | Loss: 0.00083955
Iteration 10/25 | Loss: 0.00083955
Iteration 11/25 | Loss: 0.00083955
Iteration 12/25 | Loss: 0.00083955
Iteration 13/25 | Loss: 0.00083955
Iteration 14/25 | Loss: 0.00083955
Iteration 15/25 | Loss: 0.00083955
Iteration 16/25 | Loss: 0.00083955
Iteration 17/25 | Loss: 0.00083955
Iteration 18/25 | Loss: 0.00083955
Iteration 19/25 | Loss: 0.00083955
Iteration 20/25 | Loss: 0.00083955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008395503973588347, 0.0008395503973588347, 0.0008395503973588347, 0.0008395503973588347, 0.0008395503973588347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008395503973588347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083955
Iteration 2/1000 | Loss: 0.00002388
Iteration 3/1000 | Loss: 0.00001668
Iteration 4/1000 | Loss: 0.00001558
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001442
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001406
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001379
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001370
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001362
Iteration 18/1000 | Loss: 0.00001361
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001328
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001326
Iteration 92/1000 | Loss: 0.00001326
Iteration 93/1000 | Loss: 0.00001326
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001325
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001324
Iteration 120/1000 | Loss: 0.00001324
Iteration 121/1000 | Loss: 0.00001324
Iteration 122/1000 | Loss: 0.00001324
Iteration 123/1000 | Loss: 0.00001324
Iteration 124/1000 | Loss: 0.00001324
Iteration 125/1000 | Loss: 0.00001324
Iteration 126/1000 | Loss: 0.00001324
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001324
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.324205004493706e-05, 1.324205004493706e-05, 1.324205004493706e-05, 1.324205004493706e-05, 1.324205004493706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.324205004493706e-05

Optimization complete. Final v2v error: 3.1128971576690674 mm

Highest mean error: 3.3073341846466064 mm for frame 114

Lowest mean error: 2.9712109565734863 mm for frame 139

Saving results

Total time: 37.25837230682373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978072
Iteration 2/25 | Loss: 0.00288268
Iteration 3/25 | Loss: 0.00164755
Iteration 4/25 | Loss: 0.00141985
Iteration 5/25 | Loss: 0.00136258
Iteration 6/25 | Loss: 0.00133263
Iteration 7/25 | Loss: 0.00133985
Iteration 8/25 | Loss: 0.00118315
Iteration 9/25 | Loss: 0.00107944
Iteration 10/25 | Loss: 0.00105517
Iteration 11/25 | Loss: 0.00103600
Iteration 12/25 | Loss: 0.00101425
Iteration 13/25 | Loss: 0.00101053
Iteration 14/25 | Loss: 0.00100188
Iteration 15/25 | Loss: 0.00100734
Iteration 16/25 | Loss: 0.00099092
Iteration 17/25 | Loss: 0.00098273
Iteration 18/25 | Loss: 0.00097014
Iteration 19/25 | Loss: 0.00095430
Iteration 20/25 | Loss: 0.00094799
Iteration 21/25 | Loss: 0.00094162
Iteration 22/25 | Loss: 0.00093682
Iteration 23/25 | Loss: 0.00092928
Iteration 24/25 | Loss: 0.00093523
Iteration 25/25 | Loss: 0.00093241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28340578
Iteration 2/25 | Loss: 0.00232453
Iteration 3/25 | Loss: 0.00229963
Iteration 4/25 | Loss: 0.00229959
Iteration 5/25 | Loss: 0.00229959
Iteration 6/25 | Loss: 0.00229959
Iteration 7/25 | Loss: 0.00229959
Iteration 8/25 | Loss: 0.00229959
Iteration 9/25 | Loss: 0.00229959
Iteration 10/25 | Loss: 0.00229959
Iteration 11/25 | Loss: 0.00229959
Iteration 12/25 | Loss: 0.00229959
Iteration 13/25 | Loss: 0.00229959
Iteration 14/25 | Loss: 0.00229959
Iteration 15/25 | Loss: 0.00229959
Iteration 16/25 | Loss: 0.00229959
Iteration 17/25 | Loss: 0.00229959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002299591666087508, 0.002299591666087508, 0.002299591666087508, 0.002299591666087508, 0.002299591666087508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002299591666087508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229959
Iteration 2/1000 | Loss: 0.00136056
Iteration 3/1000 | Loss: 0.00032714
Iteration 4/1000 | Loss: 0.00097797
Iteration 5/1000 | Loss: 0.00172234
Iteration 6/1000 | Loss: 0.00048395
Iteration 7/1000 | Loss: 0.00091600
Iteration 8/1000 | Loss: 0.00099399
Iteration 9/1000 | Loss: 0.00066995
Iteration 10/1000 | Loss: 0.00061824
Iteration 11/1000 | Loss: 0.00050180
Iteration 12/1000 | Loss: 0.00024504
Iteration 13/1000 | Loss: 0.00021536
Iteration 14/1000 | Loss: 0.00058475
Iteration 15/1000 | Loss: 0.00082616
Iteration 16/1000 | Loss: 0.00048423
Iteration 17/1000 | Loss: 0.00018653
Iteration 18/1000 | Loss: 0.00020596
Iteration 19/1000 | Loss: 0.00023082
Iteration 20/1000 | Loss: 0.00025114
Iteration 21/1000 | Loss: 0.00040654
Iteration 22/1000 | Loss: 0.00042858
Iteration 23/1000 | Loss: 0.00051232
Iteration 24/1000 | Loss: 0.00013015
Iteration 25/1000 | Loss: 0.00064034
Iteration 26/1000 | Loss: 0.00089438
Iteration 27/1000 | Loss: 0.00030884
Iteration 28/1000 | Loss: 0.00107890
Iteration 29/1000 | Loss: 0.00091917
Iteration 30/1000 | Loss: 0.00038779
Iteration 31/1000 | Loss: 0.00023077
Iteration 32/1000 | Loss: 0.00008017
Iteration 33/1000 | Loss: 0.00046070
Iteration 34/1000 | Loss: 0.00042368
Iteration 35/1000 | Loss: 0.00048298
Iteration 36/1000 | Loss: 0.00061293
Iteration 37/1000 | Loss: 0.00061228
Iteration 38/1000 | Loss: 0.00029494
Iteration 39/1000 | Loss: 0.00027106
Iteration 40/1000 | Loss: 0.00008628
Iteration 41/1000 | Loss: 0.00033199
Iteration 42/1000 | Loss: 0.00077632
Iteration 43/1000 | Loss: 0.00012488
Iteration 44/1000 | Loss: 0.00028541
Iteration 45/1000 | Loss: 0.00019339
Iteration 46/1000 | Loss: 0.00014073
Iteration 47/1000 | Loss: 0.00008611
Iteration 48/1000 | Loss: 0.00008693
Iteration 49/1000 | Loss: 0.00028690
Iteration 50/1000 | Loss: 0.00020862
Iteration 51/1000 | Loss: 0.00033693
Iteration 52/1000 | Loss: 0.00035879
Iteration 53/1000 | Loss: 0.00008506
Iteration 54/1000 | Loss: 0.00048872
Iteration 55/1000 | Loss: 0.00023917
Iteration 56/1000 | Loss: 0.00013068
Iteration 57/1000 | Loss: 0.00011547
Iteration 58/1000 | Loss: 0.00007724
Iteration 59/1000 | Loss: 0.00008777
Iteration 60/1000 | Loss: 0.00007497
Iteration 61/1000 | Loss: 0.00101385
Iteration 62/1000 | Loss: 0.00021249
Iteration 63/1000 | Loss: 0.00023165
Iteration 64/1000 | Loss: 0.00006861
Iteration 65/1000 | Loss: 0.00006302
Iteration 66/1000 | Loss: 0.00005998
Iteration 67/1000 | Loss: 0.00007526
Iteration 68/1000 | Loss: 0.00016822
Iteration 69/1000 | Loss: 0.00005915
Iteration 70/1000 | Loss: 0.00189440
Iteration 71/1000 | Loss: 0.00157522
Iteration 72/1000 | Loss: 0.00191456
Iteration 73/1000 | Loss: 0.00074856
Iteration 74/1000 | Loss: 0.00121146
Iteration 75/1000 | Loss: 0.00080712
Iteration 76/1000 | Loss: 0.00007232
Iteration 77/1000 | Loss: 0.00006174
Iteration 78/1000 | Loss: 0.00036113
Iteration 79/1000 | Loss: 0.00030966
Iteration 80/1000 | Loss: 0.00087978
Iteration 81/1000 | Loss: 0.00035824
Iteration 82/1000 | Loss: 0.00008584
Iteration 83/1000 | Loss: 0.00005929
Iteration 84/1000 | Loss: 0.00006121
Iteration 85/1000 | Loss: 0.00005549
Iteration 86/1000 | Loss: 0.00004800
Iteration 87/1000 | Loss: 0.00004930
Iteration 88/1000 | Loss: 0.00038377
Iteration 89/1000 | Loss: 0.00020891
Iteration 90/1000 | Loss: 0.00016770
Iteration 91/1000 | Loss: 0.00017630
Iteration 92/1000 | Loss: 0.00043024
Iteration 93/1000 | Loss: 0.00023415
Iteration 94/1000 | Loss: 0.00021091
Iteration 95/1000 | Loss: 0.00046792
Iteration 96/1000 | Loss: 0.00040570
Iteration 97/1000 | Loss: 0.00043422
Iteration 98/1000 | Loss: 0.00030328
Iteration 99/1000 | Loss: 0.00024074
Iteration 100/1000 | Loss: 0.00018271
Iteration 101/1000 | Loss: 0.00040143
Iteration 102/1000 | Loss: 0.00011160
Iteration 103/1000 | Loss: 0.00012765
Iteration 104/1000 | Loss: 0.00029939
Iteration 105/1000 | Loss: 0.00035914
Iteration 106/1000 | Loss: 0.00041055
Iteration 107/1000 | Loss: 0.00039678
Iteration 108/1000 | Loss: 0.00044662
Iteration 109/1000 | Loss: 0.00044433
Iteration 110/1000 | Loss: 0.00019739
Iteration 111/1000 | Loss: 0.00037046
Iteration 112/1000 | Loss: 0.00007330
Iteration 113/1000 | Loss: 0.00006018
Iteration 114/1000 | Loss: 0.00004846
Iteration 115/1000 | Loss: 0.00004614
Iteration 116/1000 | Loss: 0.00004550
Iteration 117/1000 | Loss: 0.00004498
Iteration 118/1000 | Loss: 0.00004424
Iteration 119/1000 | Loss: 0.00048982
Iteration 120/1000 | Loss: 0.00004620
Iteration 121/1000 | Loss: 0.00128333
Iteration 122/1000 | Loss: 0.00079160
Iteration 123/1000 | Loss: 0.00006210
Iteration 124/1000 | Loss: 0.00004560
Iteration 125/1000 | Loss: 0.00041607
Iteration 126/1000 | Loss: 0.00038782
Iteration 127/1000 | Loss: 0.00037197
Iteration 128/1000 | Loss: 0.00041421
Iteration 129/1000 | Loss: 0.00007491
Iteration 130/1000 | Loss: 0.00005536
Iteration 131/1000 | Loss: 0.00004799
Iteration 132/1000 | Loss: 0.00004347
Iteration 133/1000 | Loss: 0.00081099
Iteration 134/1000 | Loss: 0.00005310
Iteration 135/1000 | Loss: 0.00056392
Iteration 136/1000 | Loss: 0.00005144
Iteration 137/1000 | Loss: 0.00004385
Iteration 138/1000 | Loss: 0.00077297
Iteration 139/1000 | Loss: 0.00006631
Iteration 140/1000 | Loss: 0.00005267
Iteration 141/1000 | Loss: 0.00019018
Iteration 142/1000 | Loss: 0.00034026
Iteration 143/1000 | Loss: 0.00019143
Iteration 144/1000 | Loss: 0.00105316
Iteration 145/1000 | Loss: 0.00066186
Iteration 146/1000 | Loss: 0.00026425
Iteration 147/1000 | Loss: 0.00101871
Iteration 148/1000 | Loss: 0.00099934
Iteration 149/1000 | Loss: 0.00028234
Iteration 150/1000 | Loss: 0.00031000
Iteration 151/1000 | Loss: 0.00024980
Iteration 152/1000 | Loss: 0.00091347
Iteration 153/1000 | Loss: 0.00037850
Iteration 154/1000 | Loss: 0.00097086
Iteration 155/1000 | Loss: 0.00010125
Iteration 156/1000 | Loss: 0.00051442
Iteration 157/1000 | Loss: 0.00044188
Iteration 158/1000 | Loss: 0.00056456
Iteration 159/1000 | Loss: 0.00046286
Iteration 160/1000 | Loss: 0.00043575
Iteration 161/1000 | Loss: 0.00061930
Iteration 162/1000 | Loss: 0.00054320
Iteration 163/1000 | Loss: 0.00012052
Iteration 164/1000 | Loss: 0.00019416
Iteration 165/1000 | Loss: 0.00004972
Iteration 166/1000 | Loss: 0.00004374
Iteration 167/1000 | Loss: 0.00004037
Iteration 168/1000 | Loss: 0.00005400
Iteration 169/1000 | Loss: 0.00007014
Iteration 170/1000 | Loss: 0.00004787
Iteration 171/1000 | Loss: 0.00006629
Iteration 172/1000 | Loss: 0.00006127
Iteration 173/1000 | Loss: 0.00005792
Iteration 174/1000 | Loss: 0.00005838
Iteration 175/1000 | Loss: 0.00006051
Iteration 176/1000 | Loss: 0.00005923
Iteration 177/1000 | Loss: 0.00005699
Iteration 178/1000 | Loss: 0.00005911
Iteration 179/1000 | Loss: 0.00005697
Iteration 180/1000 | Loss: 0.00005958
Iteration 181/1000 | Loss: 0.00005355
Iteration 182/1000 | Loss: 0.00005333
Iteration 183/1000 | Loss: 0.00005234
Iteration 184/1000 | Loss: 0.00005043
Iteration 185/1000 | Loss: 0.00005740
Iteration 186/1000 | Loss: 0.00005298
Iteration 187/1000 | Loss: 0.00005014
Iteration 188/1000 | Loss: 0.00005292
Iteration 189/1000 | Loss: 0.00005482
Iteration 190/1000 | Loss: 0.00005616
Iteration 191/1000 | Loss: 0.00004903
Iteration 192/1000 | Loss: 0.00005281
Iteration 193/1000 | Loss: 0.00005729
Iteration 194/1000 | Loss: 0.00006290
Iteration 195/1000 | Loss: 0.00006930
Iteration 196/1000 | Loss: 0.00005891
Iteration 197/1000 | Loss: 0.00005859
Iteration 198/1000 | Loss: 0.00005998
Iteration 199/1000 | Loss: 0.00005746
Iteration 200/1000 | Loss: 0.00006009
Iteration 201/1000 | Loss: 0.00005068
Iteration 202/1000 | Loss: 0.00004498
Iteration 203/1000 | Loss: 0.00006767
Iteration 204/1000 | Loss: 0.00004692
Iteration 205/1000 | Loss: 0.00004071
Iteration 206/1000 | Loss: 0.00003756
Iteration 207/1000 | Loss: 0.00004524
Iteration 208/1000 | Loss: 0.00006640
Iteration 209/1000 | Loss: 0.00006628
Iteration 210/1000 | Loss: 0.00008796
Iteration 211/1000 | Loss: 0.00004181
Iteration 212/1000 | Loss: 0.00006624
Iteration 213/1000 | Loss: 0.00003982
Iteration 214/1000 | Loss: 0.00006545
Iteration 215/1000 | Loss: 0.00007755
Iteration 216/1000 | Loss: 0.00004540
Iteration 217/1000 | Loss: 0.00005879
Iteration 218/1000 | Loss: 0.00007848
Iteration 219/1000 | Loss: 0.00004405
Iteration 220/1000 | Loss: 0.00006901
Iteration 221/1000 | Loss: 0.00007596
Iteration 222/1000 | Loss: 0.00003952
Iteration 223/1000 | Loss: 0.00009078
Iteration 224/1000 | Loss: 0.00005147
Iteration 225/1000 | Loss: 0.00007712
Iteration 226/1000 | Loss: 0.00005110
Iteration 227/1000 | Loss: 0.00007331
Iteration 228/1000 | Loss: 0.00005185
Iteration 229/1000 | Loss: 0.00007267
Iteration 230/1000 | Loss: 0.00004866
Iteration 231/1000 | Loss: 0.00003596
Iteration 232/1000 | Loss: 0.00003506
Iteration 233/1000 | Loss: 0.00003431
Iteration 234/1000 | Loss: 0.00003354
Iteration 235/1000 | Loss: 0.00053496
Iteration 236/1000 | Loss: 0.00107491
Iteration 237/1000 | Loss: 0.00005335
Iteration 238/1000 | Loss: 0.00058941
Iteration 239/1000 | Loss: 0.00003569
Iteration 240/1000 | Loss: 0.00003387
Iteration 241/1000 | Loss: 0.00003282
Iteration 242/1000 | Loss: 0.00003102
Iteration 243/1000 | Loss: 0.00003007
Iteration 244/1000 | Loss: 0.00002965
Iteration 245/1000 | Loss: 0.00002944
Iteration 246/1000 | Loss: 0.00002931
Iteration 247/1000 | Loss: 0.00002928
Iteration 248/1000 | Loss: 0.00002927
Iteration 249/1000 | Loss: 0.00002926
Iteration 250/1000 | Loss: 0.00002925
Iteration 251/1000 | Loss: 0.00002925
Iteration 252/1000 | Loss: 0.00002924
Iteration 253/1000 | Loss: 0.00002923
Iteration 254/1000 | Loss: 0.00002923
Iteration 255/1000 | Loss: 0.00002923
Iteration 256/1000 | Loss: 0.00002922
Iteration 257/1000 | Loss: 0.00002922
Iteration 258/1000 | Loss: 0.00002921
Iteration 259/1000 | Loss: 0.00002921
Iteration 260/1000 | Loss: 0.00002918
Iteration 261/1000 | Loss: 0.00002914
Iteration 262/1000 | Loss: 0.00002909
Iteration 263/1000 | Loss: 0.00002909
Iteration 264/1000 | Loss: 0.00002908
Iteration 265/1000 | Loss: 0.00002908
Iteration 266/1000 | Loss: 0.00002908
Iteration 267/1000 | Loss: 0.00002908
Iteration 268/1000 | Loss: 0.00002908
Iteration 269/1000 | Loss: 0.00002908
Iteration 270/1000 | Loss: 0.00002907
Iteration 271/1000 | Loss: 0.00002907
Iteration 272/1000 | Loss: 0.00002907
Iteration 273/1000 | Loss: 0.00002906
Iteration 274/1000 | Loss: 0.00002906
Iteration 275/1000 | Loss: 0.00002905
Iteration 276/1000 | Loss: 0.00002905
Iteration 277/1000 | Loss: 0.00002904
Iteration 278/1000 | Loss: 0.00002904
Iteration 279/1000 | Loss: 0.00002904
Iteration 280/1000 | Loss: 0.00002904
Iteration 281/1000 | Loss: 0.00002904
Iteration 282/1000 | Loss: 0.00002904
Iteration 283/1000 | Loss: 0.00002904
Iteration 284/1000 | Loss: 0.00002903
Iteration 285/1000 | Loss: 0.00002903
Iteration 286/1000 | Loss: 0.00002900
Iteration 287/1000 | Loss: 0.00002900
Iteration 288/1000 | Loss: 0.00002900
Iteration 289/1000 | Loss: 0.00002900
Iteration 290/1000 | Loss: 0.00002899
Iteration 291/1000 | Loss: 0.00002899
Iteration 292/1000 | Loss: 0.00002899
Iteration 293/1000 | Loss: 0.00002899
Iteration 294/1000 | Loss: 0.00002898
Iteration 295/1000 | Loss: 0.00002898
Iteration 296/1000 | Loss: 0.00002897
Iteration 297/1000 | Loss: 0.00002897
Iteration 298/1000 | Loss: 0.00002897
Iteration 299/1000 | Loss: 0.00002896
Iteration 300/1000 | Loss: 0.00002896
Iteration 301/1000 | Loss: 0.00002896
Iteration 302/1000 | Loss: 0.00002896
Iteration 303/1000 | Loss: 0.00002896
Iteration 304/1000 | Loss: 0.00002896
Iteration 305/1000 | Loss: 0.00002896
Iteration 306/1000 | Loss: 0.00002896
Iteration 307/1000 | Loss: 0.00002896
Iteration 308/1000 | Loss: 0.00002896
Iteration 309/1000 | Loss: 0.00002896
Iteration 310/1000 | Loss: 0.00002896
Iteration 311/1000 | Loss: 0.00002896
Iteration 312/1000 | Loss: 0.00002896
Iteration 313/1000 | Loss: 0.00002896
Iteration 314/1000 | Loss: 0.00002895
Iteration 315/1000 | Loss: 0.00002895
Iteration 316/1000 | Loss: 0.00002895
Iteration 317/1000 | Loss: 0.00002895
Iteration 318/1000 | Loss: 0.00002895
Iteration 319/1000 | Loss: 0.00002895
Iteration 320/1000 | Loss: 0.00002895
Iteration 321/1000 | Loss: 0.00002895
Iteration 322/1000 | Loss: 0.00002895
Iteration 323/1000 | Loss: 0.00002894
Iteration 324/1000 | Loss: 0.00002894
Iteration 325/1000 | Loss: 0.00002894
Iteration 326/1000 | Loss: 0.00002894
Iteration 327/1000 | Loss: 0.00002894
Iteration 328/1000 | Loss: 0.00002894
Iteration 329/1000 | Loss: 0.00002894
Iteration 330/1000 | Loss: 0.00002894
Iteration 331/1000 | Loss: 0.00002894
Iteration 332/1000 | Loss: 0.00002894
Iteration 333/1000 | Loss: 0.00002894
Iteration 334/1000 | Loss: 0.00002894
Iteration 335/1000 | Loss: 0.00002894
Iteration 336/1000 | Loss: 0.00002894
Iteration 337/1000 | Loss: 0.00002894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [2.8940414267708547e-05, 2.8940414267708547e-05, 2.8940414267708547e-05, 2.8940414267708547e-05, 2.8940414267708547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8940414267708547e-05

Optimization complete. Final v2v error: 4.142795085906982 mm

Highest mean error: 11.891623497009277 mm for frame 20

Lowest mean error: 3.0794601440429688 mm for frame 92

Saving results

Total time: 406.5637366771698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00287285
Iteration 2/25 | Loss: 0.00099978
Iteration 3/25 | Loss: 0.00080055
Iteration 4/25 | Loss: 0.00076700
Iteration 5/25 | Loss: 0.00075785
Iteration 6/25 | Loss: 0.00075649
Iteration 7/25 | Loss: 0.00075613
Iteration 8/25 | Loss: 0.00075613
Iteration 9/25 | Loss: 0.00075613
Iteration 10/25 | Loss: 0.00075613
Iteration 11/25 | Loss: 0.00075613
Iteration 12/25 | Loss: 0.00075613
Iteration 13/25 | Loss: 0.00075613
Iteration 14/25 | Loss: 0.00075613
Iteration 15/25 | Loss: 0.00075613
Iteration 16/25 | Loss: 0.00075613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007561279344372451, 0.0007561279344372451, 0.0007561279344372451, 0.0007561279344372451, 0.0007561279344372451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007561279344372451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58559680
Iteration 2/25 | Loss: 0.00099534
Iteration 3/25 | Loss: 0.00099533
Iteration 4/25 | Loss: 0.00099533
Iteration 5/25 | Loss: 0.00099533
Iteration 6/25 | Loss: 0.00099533
Iteration 7/25 | Loss: 0.00099533
Iteration 8/25 | Loss: 0.00099533
Iteration 9/25 | Loss: 0.00099533
Iteration 10/25 | Loss: 0.00099533
Iteration 11/25 | Loss: 0.00099533
Iteration 12/25 | Loss: 0.00099533
Iteration 13/25 | Loss: 0.00099533
Iteration 14/25 | Loss: 0.00099533
Iteration 15/25 | Loss: 0.00099533
Iteration 16/25 | Loss: 0.00099533
Iteration 17/25 | Loss: 0.00099533
Iteration 18/25 | Loss: 0.00099533
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009953281842172146, 0.0009953281842172146, 0.0009953281842172146, 0.0009953281842172146, 0.0009953281842172146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009953281842172146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099533
Iteration 2/1000 | Loss: 0.00004351
Iteration 3/1000 | Loss: 0.00003142
Iteration 4/1000 | Loss: 0.00002531
Iteration 5/1000 | Loss: 0.00002340
Iteration 6/1000 | Loss: 0.00002248
Iteration 7/1000 | Loss: 0.00002189
Iteration 8/1000 | Loss: 0.00002144
Iteration 9/1000 | Loss: 0.00002099
Iteration 10/1000 | Loss: 0.00002067
Iteration 11/1000 | Loss: 0.00002038
Iteration 12/1000 | Loss: 0.00002017
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001990
Iteration 15/1000 | Loss: 0.00001990
Iteration 16/1000 | Loss: 0.00001985
Iteration 17/1000 | Loss: 0.00001983
Iteration 18/1000 | Loss: 0.00001981
Iteration 19/1000 | Loss: 0.00001980
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001978
Iteration 23/1000 | Loss: 0.00001977
Iteration 24/1000 | Loss: 0.00001976
Iteration 25/1000 | Loss: 0.00001976
Iteration 26/1000 | Loss: 0.00001975
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001974
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001974
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00001973
Iteration 34/1000 | Loss: 0.00001972
Iteration 35/1000 | Loss: 0.00001972
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001971
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001970
Iteration 42/1000 | Loss: 0.00001970
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00001969
Iteration 45/1000 | Loss: 0.00001969
Iteration 46/1000 | Loss: 0.00001968
Iteration 47/1000 | Loss: 0.00001968
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001967
Iteration 50/1000 | Loss: 0.00001967
Iteration 51/1000 | Loss: 0.00001967
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001966
Iteration 64/1000 | Loss: 0.00001966
Iteration 65/1000 | Loss: 0.00001966
Iteration 66/1000 | Loss: 0.00001966
Iteration 67/1000 | Loss: 0.00001965
Iteration 68/1000 | Loss: 0.00001965
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001965
Iteration 74/1000 | Loss: 0.00001965
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001964
Iteration 78/1000 | Loss: 0.00001964
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001961
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001960
Iteration 95/1000 | Loss: 0.00001960
Iteration 96/1000 | Loss: 0.00001960
Iteration 97/1000 | Loss: 0.00001960
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001959
Iteration 100/1000 | Loss: 0.00001959
Iteration 101/1000 | Loss: 0.00001959
Iteration 102/1000 | Loss: 0.00001959
Iteration 103/1000 | Loss: 0.00001959
Iteration 104/1000 | Loss: 0.00001959
Iteration 105/1000 | Loss: 0.00001959
Iteration 106/1000 | Loss: 0.00001959
Iteration 107/1000 | Loss: 0.00001959
Iteration 108/1000 | Loss: 0.00001959
Iteration 109/1000 | Loss: 0.00001958
Iteration 110/1000 | Loss: 0.00001958
Iteration 111/1000 | Loss: 0.00001958
Iteration 112/1000 | Loss: 0.00001958
Iteration 113/1000 | Loss: 0.00001958
Iteration 114/1000 | Loss: 0.00001958
Iteration 115/1000 | Loss: 0.00001958
Iteration 116/1000 | Loss: 0.00001958
Iteration 117/1000 | Loss: 0.00001958
Iteration 118/1000 | Loss: 0.00001958
Iteration 119/1000 | Loss: 0.00001958
Iteration 120/1000 | Loss: 0.00001958
Iteration 121/1000 | Loss: 0.00001958
Iteration 122/1000 | Loss: 0.00001958
Iteration 123/1000 | Loss: 0.00001958
Iteration 124/1000 | Loss: 0.00001958
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001957
Iteration 127/1000 | Loss: 0.00001957
Iteration 128/1000 | Loss: 0.00001957
Iteration 129/1000 | Loss: 0.00001957
Iteration 130/1000 | Loss: 0.00001957
Iteration 131/1000 | Loss: 0.00001957
Iteration 132/1000 | Loss: 0.00001957
Iteration 133/1000 | Loss: 0.00001956
Iteration 134/1000 | Loss: 0.00001956
Iteration 135/1000 | Loss: 0.00001956
Iteration 136/1000 | Loss: 0.00001956
Iteration 137/1000 | Loss: 0.00001956
Iteration 138/1000 | Loss: 0.00001956
Iteration 139/1000 | Loss: 0.00001956
Iteration 140/1000 | Loss: 0.00001956
Iteration 141/1000 | Loss: 0.00001956
Iteration 142/1000 | Loss: 0.00001956
Iteration 143/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.9560049622668885e-05, 1.9560049622668885e-05, 1.9560049622668885e-05, 1.9560049622668885e-05, 1.9560049622668885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9560049622668885e-05

Optimization complete. Final v2v error: 3.644404649734497 mm

Highest mean error: 3.981771945953369 mm for frame 209

Lowest mean error: 3.3080880641937256 mm for frame 7

Saving results

Total time: 45.078078508377075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865883
Iteration 2/25 | Loss: 0.00084944
Iteration 3/25 | Loss: 0.00072398
Iteration 4/25 | Loss: 0.00070173
Iteration 5/25 | Loss: 0.00069408
Iteration 6/25 | Loss: 0.00069274
Iteration 7/25 | Loss: 0.00069229
Iteration 8/25 | Loss: 0.00069229
Iteration 9/25 | Loss: 0.00069229
Iteration 10/25 | Loss: 0.00069229
Iteration 11/25 | Loss: 0.00069229
Iteration 12/25 | Loss: 0.00069229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006922913598828018, 0.0006922913598828018, 0.0006922913598828018, 0.0006922913598828018, 0.0006922913598828018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006922913598828018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61050248
Iteration 2/25 | Loss: 0.00082998
Iteration 3/25 | Loss: 0.00082998
Iteration 4/25 | Loss: 0.00082998
Iteration 5/25 | Loss: 0.00082998
Iteration 6/25 | Loss: 0.00082998
Iteration 7/25 | Loss: 0.00082998
Iteration 8/25 | Loss: 0.00082998
Iteration 9/25 | Loss: 0.00082998
Iteration 10/25 | Loss: 0.00082998
Iteration 11/25 | Loss: 0.00082998
Iteration 12/25 | Loss: 0.00082998
Iteration 13/25 | Loss: 0.00082998
Iteration 14/25 | Loss: 0.00082998
Iteration 15/25 | Loss: 0.00082998
Iteration 16/25 | Loss: 0.00082998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008299805922433734, 0.0008299805922433734, 0.0008299805922433734, 0.0008299805922433734, 0.0008299805922433734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008299805922433734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082998
Iteration 2/1000 | Loss: 0.00002389
Iteration 3/1000 | Loss: 0.00001483
Iteration 4/1000 | Loss: 0.00001387
Iteration 5/1000 | Loss: 0.00001323
Iteration 6/1000 | Loss: 0.00001303
Iteration 7/1000 | Loss: 0.00001281
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001251
Iteration 11/1000 | Loss: 0.00001250
Iteration 12/1000 | Loss: 0.00001243
Iteration 13/1000 | Loss: 0.00001243
Iteration 14/1000 | Loss: 0.00001242
Iteration 15/1000 | Loss: 0.00001233
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001228
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001223
Iteration 23/1000 | Loss: 0.00001222
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001220
Iteration 27/1000 | Loss: 0.00001219
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001212
Iteration 38/1000 | Loss: 0.00001212
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001212
Iteration 41/1000 | Loss: 0.00001212
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001211
Iteration 46/1000 | Loss: 0.00001211
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001210
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001206
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001205
Iteration 66/1000 | Loss: 0.00001205
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001201
Iteration 78/1000 | Loss: 0.00001201
Iteration 79/1000 | Loss: 0.00001201
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001200
Iteration 83/1000 | Loss: 0.00001200
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001199
Iteration 87/1000 | Loss: 0.00001198
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001197
Iteration 92/1000 | Loss: 0.00001197
Iteration 93/1000 | Loss: 0.00001197
Iteration 94/1000 | Loss: 0.00001197
Iteration 95/1000 | Loss: 0.00001196
Iteration 96/1000 | Loss: 0.00001196
Iteration 97/1000 | Loss: 0.00001196
Iteration 98/1000 | Loss: 0.00001195
Iteration 99/1000 | Loss: 0.00001195
Iteration 100/1000 | Loss: 0.00001195
Iteration 101/1000 | Loss: 0.00001194
Iteration 102/1000 | Loss: 0.00001194
Iteration 103/1000 | Loss: 0.00001194
Iteration 104/1000 | Loss: 0.00001194
Iteration 105/1000 | Loss: 0.00001193
Iteration 106/1000 | Loss: 0.00001193
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001191
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001190
Iteration 127/1000 | Loss: 0.00001190
Iteration 128/1000 | Loss: 0.00001190
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001189
Iteration 135/1000 | Loss: 0.00001189
Iteration 136/1000 | Loss: 0.00001189
Iteration 137/1000 | Loss: 0.00001189
Iteration 138/1000 | Loss: 0.00001189
Iteration 139/1000 | Loss: 0.00001189
Iteration 140/1000 | Loss: 0.00001189
Iteration 141/1000 | Loss: 0.00001189
Iteration 142/1000 | Loss: 0.00001189
Iteration 143/1000 | Loss: 0.00001189
Iteration 144/1000 | Loss: 0.00001189
Iteration 145/1000 | Loss: 0.00001189
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001188
Iteration 149/1000 | Loss: 0.00001188
Iteration 150/1000 | Loss: 0.00001188
Iteration 151/1000 | Loss: 0.00001188
Iteration 152/1000 | Loss: 0.00001188
Iteration 153/1000 | Loss: 0.00001188
Iteration 154/1000 | Loss: 0.00001188
Iteration 155/1000 | Loss: 0.00001188
Iteration 156/1000 | Loss: 0.00001188
Iteration 157/1000 | Loss: 0.00001188
Iteration 158/1000 | Loss: 0.00001188
Iteration 159/1000 | Loss: 0.00001188
Iteration 160/1000 | Loss: 0.00001188
Iteration 161/1000 | Loss: 0.00001188
Iteration 162/1000 | Loss: 0.00001188
Iteration 163/1000 | Loss: 0.00001188
Iteration 164/1000 | Loss: 0.00001188
Iteration 165/1000 | Loss: 0.00001188
Iteration 166/1000 | Loss: 0.00001188
Iteration 167/1000 | Loss: 0.00001188
Iteration 168/1000 | Loss: 0.00001188
Iteration 169/1000 | Loss: 0.00001188
Iteration 170/1000 | Loss: 0.00001188
Iteration 171/1000 | Loss: 0.00001188
Iteration 172/1000 | Loss: 0.00001188
Iteration 173/1000 | Loss: 0.00001188
Iteration 174/1000 | Loss: 0.00001188
Iteration 175/1000 | Loss: 0.00001188
Iteration 176/1000 | Loss: 0.00001188
Iteration 177/1000 | Loss: 0.00001188
Iteration 178/1000 | Loss: 0.00001187
Iteration 179/1000 | Loss: 0.00001187
Iteration 180/1000 | Loss: 0.00001187
Iteration 181/1000 | Loss: 0.00001187
Iteration 182/1000 | Loss: 0.00001187
Iteration 183/1000 | Loss: 0.00001187
Iteration 184/1000 | Loss: 0.00001187
Iteration 185/1000 | Loss: 0.00001187
Iteration 186/1000 | Loss: 0.00001187
Iteration 187/1000 | Loss: 0.00001187
Iteration 188/1000 | Loss: 0.00001187
Iteration 189/1000 | Loss: 0.00001187
Iteration 190/1000 | Loss: 0.00001187
Iteration 191/1000 | Loss: 0.00001187
Iteration 192/1000 | Loss: 0.00001187
Iteration 193/1000 | Loss: 0.00001187
Iteration 194/1000 | Loss: 0.00001187
Iteration 195/1000 | Loss: 0.00001187
Iteration 196/1000 | Loss: 0.00001187
Iteration 197/1000 | Loss: 0.00001187
Iteration 198/1000 | Loss: 0.00001187
Iteration 199/1000 | Loss: 0.00001187
Iteration 200/1000 | Loss: 0.00001187
Iteration 201/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.1874426490976475e-05, 1.1874426490976475e-05, 1.1874426490976475e-05, 1.1874426490976475e-05, 1.1874426490976475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1874426490976475e-05

Optimization complete. Final v2v error: 2.9382123947143555 mm

Highest mean error: 3.339081287384033 mm for frame 99

Lowest mean error: 2.7721872329711914 mm for frame 13

Saving results

Total time: 36.850775480270386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755798
Iteration 2/25 | Loss: 0.00086922
Iteration 3/25 | Loss: 0.00075357
Iteration 4/25 | Loss: 0.00073340
Iteration 5/25 | Loss: 0.00072649
Iteration 6/25 | Loss: 0.00072445
Iteration 7/25 | Loss: 0.00072384
Iteration 8/25 | Loss: 0.00072384
Iteration 9/25 | Loss: 0.00072384
Iteration 10/25 | Loss: 0.00072384
Iteration 11/25 | Loss: 0.00072384
Iteration 12/25 | Loss: 0.00072384
Iteration 13/25 | Loss: 0.00072384
Iteration 14/25 | Loss: 0.00072384
Iteration 15/25 | Loss: 0.00072384
Iteration 16/25 | Loss: 0.00072384
Iteration 17/25 | Loss: 0.00072384
Iteration 18/25 | Loss: 0.00072384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007238410762511194, 0.0007238410762511194, 0.0007238410762511194, 0.0007238410762511194, 0.0007238410762511194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007238410762511194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.63132954
Iteration 2/25 | Loss: 0.00070617
Iteration 3/25 | Loss: 0.00070616
Iteration 4/25 | Loss: 0.00070616
Iteration 5/25 | Loss: 0.00070615
Iteration 6/25 | Loss: 0.00070615
Iteration 7/25 | Loss: 0.00070615
Iteration 8/25 | Loss: 0.00070615
Iteration 9/25 | Loss: 0.00070615
Iteration 10/25 | Loss: 0.00070615
Iteration 11/25 | Loss: 0.00070615
Iteration 12/25 | Loss: 0.00070615
Iteration 13/25 | Loss: 0.00070615
Iteration 14/25 | Loss: 0.00070615
Iteration 15/25 | Loss: 0.00070615
Iteration 16/25 | Loss: 0.00070615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007061531650833786, 0.0007061531650833786, 0.0007061531650833786, 0.0007061531650833786, 0.0007061531650833786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007061531650833786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070615
Iteration 2/1000 | Loss: 0.00003835
Iteration 3/1000 | Loss: 0.00002424
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001759
Iteration 6/1000 | Loss: 0.00001686
Iteration 7/1000 | Loss: 0.00001639
Iteration 8/1000 | Loss: 0.00001608
Iteration 9/1000 | Loss: 0.00001585
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001565
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001539
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001537
Iteration 18/1000 | Loss: 0.00001530
Iteration 19/1000 | Loss: 0.00001528
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001526
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00001523
Iteration 24/1000 | Loss: 0.00001523
Iteration 25/1000 | Loss: 0.00001522
Iteration 26/1000 | Loss: 0.00001522
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001520
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001518
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001517
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001509
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001508
Iteration 52/1000 | Loss: 0.00001508
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001508
Iteration 55/1000 | Loss: 0.00001508
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001507
Iteration 58/1000 | Loss: 0.00001507
Iteration 59/1000 | Loss: 0.00001507
Iteration 60/1000 | Loss: 0.00001506
Iteration 61/1000 | Loss: 0.00001506
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001506
Iteration 64/1000 | Loss: 0.00001506
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001502
Iteration 73/1000 | Loss: 0.00001502
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001501
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001500
Iteration 81/1000 | Loss: 0.00001500
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001499
Iteration 93/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.4992690012149978e-05, 1.4992690012149978e-05, 1.4992690012149978e-05, 1.4992690012149978e-05, 1.4992690012149978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4992690012149978e-05

Optimization complete. Final v2v error: 3.2971134185791016 mm

Highest mean error: 3.6868438720703125 mm for frame 115

Lowest mean error: 2.988508462905884 mm for frame 12

Saving results

Total time: 34.806419134140015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908048
Iteration 2/25 | Loss: 0.00140911
Iteration 3/25 | Loss: 0.00103089
Iteration 4/25 | Loss: 0.00095915
Iteration 5/25 | Loss: 0.00094794
Iteration 6/25 | Loss: 0.00094551
Iteration 7/25 | Loss: 0.00095100
Iteration 8/25 | Loss: 0.00094169
Iteration 9/25 | Loss: 0.00093463
Iteration 10/25 | Loss: 0.00093250
Iteration 11/25 | Loss: 0.00093112
Iteration 12/25 | Loss: 0.00094341
Iteration 13/25 | Loss: 0.00094170
Iteration 14/25 | Loss: 0.00092082
Iteration 15/25 | Loss: 0.00090550
Iteration 16/25 | Loss: 0.00089877
Iteration 17/25 | Loss: 0.00089789
Iteration 18/25 | Loss: 0.00089532
Iteration 19/25 | Loss: 0.00089736
Iteration 20/25 | Loss: 0.00089437
Iteration 21/25 | Loss: 0.00089183
Iteration 22/25 | Loss: 0.00088970
Iteration 23/25 | Loss: 0.00088832
Iteration 24/25 | Loss: 0.00088716
Iteration 25/25 | Loss: 0.00088648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04908872
Iteration 2/25 | Loss: 0.00073031
Iteration 3/25 | Loss: 0.00073030
Iteration 4/25 | Loss: 0.00073030
Iteration 5/25 | Loss: 0.00073030
Iteration 6/25 | Loss: 0.00073030
Iteration 7/25 | Loss: 0.00073030
Iteration 8/25 | Loss: 0.00073030
Iteration 9/25 | Loss: 0.00073030
Iteration 10/25 | Loss: 0.00073030
Iteration 11/25 | Loss: 0.00073030
Iteration 12/25 | Loss: 0.00073030
Iteration 13/25 | Loss: 0.00073030
Iteration 14/25 | Loss: 0.00073030
Iteration 15/25 | Loss: 0.00073030
Iteration 16/25 | Loss: 0.00073030
Iteration 17/25 | Loss: 0.00073030
Iteration 18/25 | Loss: 0.00073030
Iteration 19/25 | Loss: 0.00073030
Iteration 20/25 | Loss: 0.00073030
Iteration 21/25 | Loss: 0.00073030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007303006132133305, 0.0007303006132133305, 0.0007303006132133305, 0.0007303006132133305, 0.0007303006132133305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007303006132133305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073030
Iteration 2/1000 | Loss: 0.00005119
Iteration 3/1000 | Loss: 0.00003874
Iteration 4/1000 | Loss: 0.00003650
Iteration 5/1000 | Loss: 0.00003449
Iteration 6/1000 | Loss: 0.00003334
Iteration 7/1000 | Loss: 0.00003234
Iteration 8/1000 | Loss: 0.00003183
Iteration 9/1000 | Loss: 0.00003161
Iteration 10/1000 | Loss: 0.00003138
Iteration 11/1000 | Loss: 0.00003128
Iteration 12/1000 | Loss: 0.00003126
Iteration 13/1000 | Loss: 0.00003126
Iteration 14/1000 | Loss: 0.00003126
Iteration 15/1000 | Loss: 0.00003119
Iteration 16/1000 | Loss: 0.00003116
Iteration 17/1000 | Loss: 0.00003116
Iteration 18/1000 | Loss: 0.00003116
Iteration 19/1000 | Loss: 0.00003116
Iteration 20/1000 | Loss: 0.00003116
Iteration 21/1000 | Loss: 0.00003116
Iteration 22/1000 | Loss: 0.00003116
Iteration 23/1000 | Loss: 0.00003116
Iteration 24/1000 | Loss: 0.00003116
Iteration 25/1000 | Loss: 0.00003115
Iteration 26/1000 | Loss: 0.00003115
Iteration 27/1000 | Loss: 0.00003114
Iteration 28/1000 | Loss: 0.00003114
Iteration 29/1000 | Loss: 0.00003113
Iteration 30/1000 | Loss: 0.00003113
Iteration 31/1000 | Loss: 0.00003112
Iteration 32/1000 | Loss: 0.00003107
Iteration 33/1000 | Loss: 0.00003106
Iteration 34/1000 | Loss: 0.00003104
Iteration 35/1000 | Loss: 0.00003104
Iteration 36/1000 | Loss: 0.00003104
Iteration 37/1000 | Loss: 0.00003104
Iteration 38/1000 | Loss: 0.00003104
Iteration 39/1000 | Loss: 0.00003104
Iteration 40/1000 | Loss: 0.00003104
Iteration 41/1000 | Loss: 0.00003104
Iteration 42/1000 | Loss: 0.00003104
Iteration 43/1000 | Loss: 0.00003104
Iteration 44/1000 | Loss: 0.00003104
Iteration 45/1000 | Loss: 0.00003104
Iteration 46/1000 | Loss: 0.00003103
Iteration 47/1000 | Loss: 0.00003103
Iteration 48/1000 | Loss: 0.00003103
Iteration 49/1000 | Loss: 0.00003103
Iteration 50/1000 | Loss: 0.00003103
Iteration 51/1000 | Loss: 0.00003103
Iteration 52/1000 | Loss: 0.00003103
Iteration 53/1000 | Loss: 0.00003103
Iteration 54/1000 | Loss: 0.00003103
Iteration 55/1000 | Loss: 0.00003103
Iteration 56/1000 | Loss: 0.00003103
Iteration 57/1000 | Loss: 0.00003103
Iteration 58/1000 | Loss: 0.00003103
Iteration 59/1000 | Loss: 0.00003103
Iteration 60/1000 | Loss: 0.00003103
Iteration 61/1000 | Loss: 0.00003103
Iteration 62/1000 | Loss: 0.00003102
Iteration 63/1000 | Loss: 0.00003102
Iteration 64/1000 | Loss: 0.00003102
Iteration 65/1000 | Loss: 0.00003102
Iteration 66/1000 | Loss: 0.00003102
Iteration 67/1000 | Loss: 0.00003102
Iteration 68/1000 | Loss: 0.00003102
Iteration 69/1000 | Loss: 0.00003102
Iteration 70/1000 | Loss: 0.00003102
Iteration 71/1000 | Loss: 0.00003102
Iteration 72/1000 | Loss: 0.00003102
Iteration 73/1000 | Loss: 0.00003102
Iteration 74/1000 | Loss: 0.00003102
Iteration 75/1000 | Loss: 0.00003101
Iteration 76/1000 | Loss: 0.00003101
Iteration 77/1000 | Loss: 0.00003101
Iteration 78/1000 | Loss: 0.00003101
Iteration 79/1000 | Loss: 0.00003100
Iteration 80/1000 | Loss: 0.00003100
Iteration 81/1000 | Loss: 0.00003100
Iteration 82/1000 | Loss: 0.00003100
Iteration 83/1000 | Loss: 0.00003100
Iteration 84/1000 | Loss: 0.00003100
Iteration 85/1000 | Loss: 0.00003100
Iteration 86/1000 | Loss: 0.00003099
Iteration 87/1000 | Loss: 0.00003099
Iteration 88/1000 | Loss: 0.00003099
Iteration 89/1000 | Loss: 0.00003099
Iteration 90/1000 | Loss: 0.00003099
Iteration 91/1000 | Loss: 0.00003098
Iteration 92/1000 | Loss: 0.00003098
Iteration 93/1000 | Loss: 0.00003098
Iteration 94/1000 | Loss: 0.00003098
Iteration 95/1000 | Loss: 0.00003098
Iteration 96/1000 | Loss: 0.00003098
Iteration 97/1000 | Loss: 0.00003098
Iteration 98/1000 | Loss: 0.00003098
Iteration 99/1000 | Loss: 0.00003098
Iteration 100/1000 | Loss: 0.00003098
Iteration 101/1000 | Loss: 0.00003098
Iteration 102/1000 | Loss: 0.00003098
Iteration 103/1000 | Loss: 0.00003098
Iteration 104/1000 | Loss: 0.00003098
Iteration 105/1000 | Loss: 0.00003098
Iteration 106/1000 | Loss: 0.00003098
Iteration 107/1000 | Loss: 0.00003098
Iteration 108/1000 | Loss: 0.00003098
Iteration 109/1000 | Loss: 0.00003098
Iteration 110/1000 | Loss: 0.00003098
Iteration 111/1000 | Loss: 0.00003098
Iteration 112/1000 | Loss: 0.00003098
Iteration 113/1000 | Loss: 0.00003098
Iteration 114/1000 | Loss: 0.00003098
Iteration 115/1000 | Loss: 0.00003098
Iteration 116/1000 | Loss: 0.00003098
Iteration 117/1000 | Loss: 0.00003098
Iteration 118/1000 | Loss: 0.00003098
Iteration 119/1000 | Loss: 0.00003098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [3.0978964787209406e-05, 3.0978964787209406e-05, 3.0978964787209406e-05, 3.0978964787209406e-05, 3.0978964787209406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0978964787209406e-05

Optimization complete. Final v2v error: 4.654812335968018 mm

Highest mean error: 4.899607181549072 mm for frame 123

Lowest mean error: 4.495829105377197 mm for frame 99

Saving results

Total time: 67.42355561256409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859899
Iteration 2/25 | Loss: 0.00163806
Iteration 3/25 | Loss: 0.00113058
Iteration 4/25 | Loss: 0.00110159
Iteration 5/25 | Loss: 0.00109527
Iteration 6/25 | Loss: 0.00109368
Iteration 7/25 | Loss: 0.00109354
Iteration 8/25 | Loss: 0.00109354
Iteration 9/25 | Loss: 0.00109354
Iteration 10/25 | Loss: 0.00109354
Iteration 11/25 | Loss: 0.00109354
Iteration 12/25 | Loss: 0.00109354
Iteration 13/25 | Loss: 0.00109354
Iteration 14/25 | Loss: 0.00109354
Iteration 15/25 | Loss: 0.00109354
Iteration 16/25 | Loss: 0.00109354
Iteration 17/25 | Loss: 0.00109354
Iteration 18/25 | Loss: 0.00109354
Iteration 19/25 | Loss: 0.00109354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010935430182144046, 0.0010935430182144046, 0.0010935430182144046, 0.0010935430182144046, 0.0010935430182144046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010935430182144046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39597979
Iteration 2/25 | Loss: 0.00043783
Iteration 3/25 | Loss: 0.00043783
Iteration 4/25 | Loss: 0.00043783
Iteration 5/25 | Loss: 0.00043783
Iteration 6/25 | Loss: 0.00043782
Iteration 7/25 | Loss: 0.00043782
Iteration 8/25 | Loss: 0.00043782
Iteration 9/25 | Loss: 0.00043782
Iteration 10/25 | Loss: 0.00043782
Iteration 11/25 | Loss: 0.00043782
Iteration 12/25 | Loss: 0.00043782
Iteration 13/25 | Loss: 0.00043782
Iteration 14/25 | Loss: 0.00043782
Iteration 15/25 | Loss: 0.00043782
Iteration 16/25 | Loss: 0.00043782
Iteration 17/25 | Loss: 0.00043782
Iteration 18/25 | Loss: 0.00043782
Iteration 19/25 | Loss: 0.00043782
Iteration 20/25 | Loss: 0.00043782
Iteration 21/25 | Loss: 0.00043782
Iteration 22/25 | Loss: 0.00043782
Iteration 23/25 | Loss: 0.00043782
Iteration 24/25 | Loss: 0.00043782
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004378235316835344, 0.0004378235316835344, 0.0004378235316835344, 0.0004378235316835344, 0.0004378235316835344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004378235316835344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043782
Iteration 2/1000 | Loss: 0.00006853
Iteration 3/1000 | Loss: 0.00005291
Iteration 4/1000 | Loss: 0.00004615
Iteration 5/1000 | Loss: 0.00004334
Iteration 6/1000 | Loss: 0.00004232
Iteration 7/1000 | Loss: 0.00004160
Iteration 8/1000 | Loss: 0.00004084
Iteration 9/1000 | Loss: 0.00004046
Iteration 10/1000 | Loss: 0.00004013
Iteration 11/1000 | Loss: 0.00003966
Iteration 12/1000 | Loss: 0.00003923
Iteration 13/1000 | Loss: 0.00003893
Iteration 14/1000 | Loss: 0.00003873
Iteration 15/1000 | Loss: 0.00003867
Iteration 16/1000 | Loss: 0.00003853
Iteration 17/1000 | Loss: 0.00003845
Iteration 18/1000 | Loss: 0.00003837
Iteration 19/1000 | Loss: 0.00003837
Iteration 20/1000 | Loss: 0.00003835
Iteration 21/1000 | Loss: 0.00003825
Iteration 22/1000 | Loss: 0.00003823
Iteration 23/1000 | Loss: 0.00003821
Iteration 24/1000 | Loss: 0.00003820
Iteration 25/1000 | Loss: 0.00003820
Iteration 26/1000 | Loss: 0.00003819
Iteration 27/1000 | Loss: 0.00003819
Iteration 28/1000 | Loss: 0.00003819
Iteration 29/1000 | Loss: 0.00003819
Iteration 30/1000 | Loss: 0.00003819
Iteration 31/1000 | Loss: 0.00003819
Iteration 32/1000 | Loss: 0.00003819
Iteration 33/1000 | Loss: 0.00003819
Iteration 34/1000 | Loss: 0.00003819
Iteration 35/1000 | Loss: 0.00003819
Iteration 36/1000 | Loss: 0.00003819
Iteration 37/1000 | Loss: 0.00003818
Iteration 38/1000 | Loss: 0.00003818
Iteration 39/1000 | Loss: 0.00003818
Iteration 40/1000 | Loss: 0.00003818
Iteration 41/1000 | Loss: 0.00003816
Iteration 42/1000 | Loss: 0.00003815
Iteration 43/1000 | Loss: 0.00003815
Iteration 44/1000 | Loss: 0.00003815
Iteration 45/1000 | Loss: 0.00003815
Iteration 46/1000 | Loss: 0.00003815
Iteration 47/1000 | Loss: 0.00003815
Iteration 48/1000 | Loss: 0.00003814
Iteration 49/1000 | Loss: 0.00003814
Iteration 50/1000 | Loss: 0.00003814
Iteration 51/1000 | Loss: 0.00003814
Iteration 52/1000 | Loss: 0.00003814
Iteration 53/1000 | Loss: 0.00003814
Iteration 54/1000 | Loss: 0.00003814
Iteration 55/1000 | Loss: 0.00003813
Iteration 56/1000 | Loss: 0.00003813
Iteration 57/1000 | Loss: 0.00003813
Iteration 58/1000 | Loss: 0.00003813
Iteration 59/1000 | Loss: 0.00003813
Iteration 60/1000 | Loss: 0.00003813
Iteration 61/1000 | Loss: 0.00003812
Iteration 62/1000 | Loss: 0.00003812
Iteration 63/1000 | Loss: 0.00003812
Iteration 64/1000 | Loss: 0.00003812
Iteration 65/1000 | Loss: 0.00003812
Iteration 66/1000 | Loss: 0.00003812
Iteration 67/1000 | Loss: 0.00003812
Iteration 68/1000 | Loss: 0.00003812
Iteration 69/1000 | Loss: 0.00003812
Iteration 70/1000 | Loss: 0.00003812
Iteration 71/1000 | Loss: 0.00003811
Iteration 72/1000 | Loss: 0.00003811
Iteration 73/1000 | Loss: 0.00003811
Iteration 74/1000 | Loss: 0.00003811
Iteration 75/1000 | Loss: 0.00003811
Iteration 76/1000 | Loss: 0.00003810
Iteration 77/1000 | Loss: 0.00003810
Iteration 78/1000 | Loss: 0.00003810
Iteration 79/1000 | Loss: 0.00003810
Iteration 80/1000 | Loss: 0.00003810
Iteration 81/1000 | Loss: 0.00003810
Iteration 82/1000 | Loss: 0.00003810
Iteration 83/1000 | Loss: 0.00003810
Iteration 84/1000 | Loss: 0.00003809
Iteration 85/1000 | Loss: 0.00003809
Iteration 86/1000 | Loss: 0.00003809
Iteration 87/1000 | Loss: 0.00003809
Iteration 88/1000 | Loss: 0.00003809
Iteration 89/1000 | Loss: 0.00003809
Iteration 90/1000 | Loss: 0.00003809
Iteration 91/1000 | Loss: 0.00003809
Iteration 92/1000 | Loss: 0.00003809
Iteration 93/1000 | Loss: 0.00003809
Iteration 94/1000 | Loss: 0.00003809
Iteration 95/1000 | Loss: 0.00003808
Iteration 96/1000 | Loss: 0.00003808
Iteration 97/1000 | Loss: 0.00003808
Iteration 98/1000 | Loss: 0.00003808
Iteration 99/1000 | Loss: 0.00003808
Iteration 100/1000 | Loss: 0.00003808
Iteration 101/1000 | Loss: 0.00003808
Iteration 102/1000 | Loss: 0.00003808
Iteration 103/1000 | Loss: 0.00003808
Iteration 104/1000 | Loss: 0.00003808
Iteration 105/1000 | Loss: 0.00003808
Iteration 106/1000 | Loss: 0.00003808
Iteration 107/1000 | Loss: 0.00003808
Iteration 108/1000 | Loss: 0.00003808
Iteration 109/1000 | Loss: 0.00003808
Iteration 110/1000 | Loss: 0.00003808
Iteration 111/1000 | Loss: 0.00003808
Iteration 112/1000 | Loss: 0.00003808
Iteration 113/1000 | Loss: 0.00003808
Iteration 114/1000 | Loss: 0.00003808
Iteration 115/1000 | Loss: 0.00003808
Iteration 116/1000 | Loss: 0.00003808
Iteration 117/1000 | Loss: 0.00003808
Iteration 118/1000 | Loss: 0.00003808
Iteration 119/1000 | Loss: 0.00003808
Iteration 120/1000 | Loss: 0.00003808
Iteration 121/1000 | Loss: 0.00003808
Iteration 122/1000 | Loss: 0.00003808
Iteration 123/1000 | Loss: 0.00003808
Iteration 124/1000 | Loss: 0.00003808
Iteration 125/1000 | Loss: 0.00003808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.807610846706666e-05, 3.807610846706666e-05, 3.807610846706666e-05, 3.807610846706666e-05, 3.807610846706666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.807610846706666e-05

Optimization complete. Final v2v error: 4.7800445556640625 mm

Highest mean error: 4.965439319610596 mm for frame 93

Lowest mean error: 4.5524091720581055 mm for frame 131

Saving results

Total time: 40.89653921127319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00675680
Iteration 2/25 | Loss: 0.00091015
Iteration 3/25 | Loss: 0.00078293
Iteration 4/25 | Loss: 0.00076409
Iteration 5/25 | Loss: 0.00076108
Iteration 6/25 | Loss: 0.00076100
Iteration 7/25 | Loss: 0.00076100
Iteration 8/25 | Loss: 0.00076100
Iteration 9/25 | Loss: 0.00076100
Iteration 10/25 | Loss: 0.00076100
Iteration 11/25 | Loss: 0.00076100
Iteration 12/25 | Loss: 0.00076100
Iteration 13/25 | Loss: 0.00076100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000761002826038748, 0.000761002826038748, 0.000761002826038748, 0.000761002826038748, 0.000761002826038748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000761002826038748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.20277596
Iteration 2/25 | Loss: 0.00082546
Iteration 3/25 | Loss: 0.00082538
Iteration 4/25 | Loss: 0.00082538
Iteration 5/25 | Loss: 0.00082538
Iteration 6/25 | Loss: 0.00082538
Iteration 7/25 | Loss: 0.00082538
Iteration 8/25 | Loss: 0.00082538
Iteration 9/25 | Loss: 0.00082538
Iteration 10/25 | Loss: 0.00082538
Iteration 11/25 | Loss: 0.00082538
Iteration 12/25 | Loss: 0.00082537
Iteration 13/25 | Loss: 0.00082537
Iteration 14/25 | Loss: 0.00082537
Iteration 15/25 | Loss: 0.00082537
Iteration 16/25 | Loss: 0.00082537
Iteration 17/25 | Loss: 0.00082537
Iteration 18/25 | Loss: 0.00082537
Iteration 19/25 | Loss: 0.00082537
Iteration 20/25 | Loss: 0.00082537
Iteration 21/25 | Loss: 0.00082537
Iteration 22/25 | Loss: 0.00082537
Iteration 23/25 | Loss: 0.00082537
Iteration 24/25 | Loss: 0.00082537
Iteration 25/25 | Loss: 0.00082537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082537
Iteration 2/1000 | Loss: 0.00003955
Iteration 3/1000 | Loss: 0.00002753
Iteration 4/1000 | Loss: 0.00002559
Iteration 5/1000 | Loss: 0.00002441
Iteration 6/1000 | Loss: 0.00002379
Iteration 7/1000 | Loss: 0.00002311
Iteration 8/1000 | Loss: 0.00002269
Iteration 9/1000 | Loss: 0.00002237
Iteration 10/1000 | Loss: 0.00002230
Iteration 11/1000 | Loss: 0.00002214
Iteration 12/1000 | Loss: 0.00002213
Iteration 13/1000 | Loss: 0.00002209
Iteration 14/1000 | Loss: 0.00002204
Iteration 15/1000 | Loss: 0.00002203
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002193
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002192
Iteration 20/1000 | Loss: 0.00002190
Iteration 21/1000 | Loss: 0.00002190
Iteration 22/1000 | Loss: 0.00002190
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002189
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002188
Iteration 27/1000 | Loss: 0.00002188
Iteration 28/1000 | Loss: 0.00002187
Iteration 29/1000 | Loss: 0.00002187
Iteration 30/1000 | Loss: 0.00002187
Iteration 31/1000 | Loss: 0.00002187
Iteration 32/1000 | Loss: 0.00002187
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002187
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002187
Iteration 39/1000 | Loss: 0.00002187
Iteration 40/1000 | Loss: 0.00002187
Iteration 41/1000 | Loss: 0.00002187
Iteration 42/1000 | Loss: 0.00002187
Iteration 43/1000 | Loss: 0.00002187
Iteration 44/1000 | Loss: 0.00002187
Iteration 45/1000 | Loss: 0.00002187
Iteration 46/1000 | Loss: 0.00002187
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002187
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002187
Iteration 51/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [2.1870993805350736e-05, 2.1870993805350736e-05, 2.1870993805350736e-05, 2.1870993805350736e-05, 2.1870993805350736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1870993805350736e-05

Optimization complete. Final v2v error: 3.9488327503204346 mm

Highest mean error: 4.270132064819336 mm for frame 52

Lowest mean error: 3.707231044769287 mm for frame 155

Saving results

Total time: 26.64033317565918
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995763
Iteration 2/25 | Loss: 0.00351983
Iteration 3/25 | Loss: 0.00294107
Iteration 4/25 | Loss: 0.00229262
Iteration 5/25 | Loss: 0.00185467
Iteration 6/25 | Loss: 0.00155826
Iteration 7/25 | Loss: 0.00153791
Iteration 8/25 | Loss: 0.00145085
Iteration 9/25 | Loss: 0.00165956
Iteration 10/25 | Loss: 0.00195878
Iteration 11/25 | Loss: 0.00174534
Iteration 12/25 | Loss: 0.00167679
Iteration 13/25 | Loss: 0.00161263
Iteration 14/25 | Loss: 0.00164243
Iteration 15/25 | Loss: 0.00156619
Iteration 16/25 | Loss: 0.00147231
Iteration 17/25 | Loss: 0.00147692
Iteration 18/25 | Loss: 0.00157848
Iteration 19/25 | Loss: 0.00159351
Iteration 20/25 | Loss: 0.00154012
Iteration 21/25 | Loss: 0.00153768
Iteration 22/25 | Loss: 0.00152529
Iteration 23/25 | Loss: 0.00153467
Iteration 24/25 | Loss: 0.00144512
Iteration 25/25 | Loss: 0.00148963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50188351
Iteration 2/25 | Loss: 0.01080520
Iteration 3/25 | Loss: 0.00767482
Iteration 4/25 | Loss: 0.00767467
Iteration 5/25 | Loss: 0.00767467
Iteration 6/25 | Loss: 0.00767467
Iteration 7/25 | Loss: 0.00767467
Iteration 8/25 | Loss: 0.00767467
Iteration 9/25 | Loss: 0.00767467
Iteration 10/25 | Loss: 0.00767467
Iteration 11/25 | Loss: 0.00767467
Iteration 12/25 | Loss: 0.00767467
Iteration 13/25 | Loss: 0.00767467
Iteration 14/25 | Loss: 0.00767467
Iteration 15/25 | Loss: 0.00767467
Iteration 16/25 | Loss: 0.00767467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.007674668449908495, 0.007674668449908495, 0.007674668449908495, 0.007674668449908495, 0.007674668449908495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007674668449908495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00767467
Iteration 2/1000 | Loss: 0.00642438
Iteration 3/1000 | Loss: 0.00432147
Iteration 4/1000 | Loss: 0.00509882
Iteration 5/1000 | Loss: 0.00358575
Iteration 6/1000 | Loss: 0.00319650
Iteration 7/1000 | Loss: 0.00160180
Iteration 8/1000 | Loss: 0.00238537
Iteration 9/1000 | Loss: 0.00294062
Iteration 10/1000 | Loss: 0.00245701
Iteration 11/1000 | Loss: 0.00238971
Iteration 12/1000 | Loss: 0.00297605
Iteration 13/1000 | Loss: 0.00180532
Iteration 14/1000 | Loss: 0.00131179
Iteration 15/1000 | Loss: 0.00110878
Iteration 16/1000 | Loss: 0.00146306
Iteration 17/1000 | Loss: 0.00240965
Iteration 18/1000 | Loss: 0.00260737
Iteration 19/1000 | Loss: 0.00505539
Iteration 20/1000 | Loss: 0.00234001
Iteration 21/1000 | Loss: 0.00130737
Iteration 22/1000 | Loss: 0.00194643
Iteration 23/1000 | Loss: 0.00234348
Iteration 24/1000 | Loss: 0.00171258
Iteration 25/1000 | Loss: 0.00240063
Iteration 26/1000 | Loss: 0.00126053
Iteration 27/1000 | Loss: 0.00149469
Iteration 28/1000 | Loss: 0.00109042
Iteration 29/1000 | Loss: 0.00106805
Iteration 30/1000 | Loss: 0.00097204
Iteration 31/1000 | Loss: 0.00160467
Iteration 32/1000 | Loss: 0.00124898
Iteration 33/1000 | Loss: 0.00095356
Iteration 34/1000 | Loss: 0.00160960
Iteration 35/1000 | Loss: 0.00081457
Iteration 36/1000 | Loss: 0.00092437
Iteration 37/1000 | Loss: 0.00233138
Iteration 38/1000 | Loss: 0.00256028
Iteration 39/1000 | Loss: 0.00197535
Iteration 40/1000 | Loss: 0.00159343
Iteration 41/1000 | Loss: 0.00147953
Iteration 42/1000 | Loss: 0.00190321
Iteration 43/1000 | Loss: 0.00130810
Iteration 44/1000 | Loss: 0.00156827
Iteration 45/1000 | Loss: 0.00190129
Iteration 46/1000 | Loss: 0.00196358
Iteration 47/1000 | Loss: 0.00088521
Iteration 48/1000 | Loss: 0.00193566
Iteration 49/1000 | Loss: 0.00116008
Iteration 50/1000 | Loss: 0.00323601
Iteration 51/1000 | Loss: 0.00298075
Iteration 52/1000 | Loss: 0.00125025
Iteration 53/1000 | Loss: 0.00120423
Iteration 54/1000 | Loss: 0.00218052
Iteration 55/1000 | Loss: 0.00211385
Iteration 56/1000 | Loss: 0.00093038
Iteration 57/1000 | Loss: 0.00224127
Iteration 58/1000 | Loss: 0.00116597
Iteration 59/1000 | Loss: 0.00132929
Iteration 60/1000 | Loss: 0.00154761
Iteration 61/1000 | Loss: 0.00076036
Iteration 62/1000 | Loss: 0.00074452
Iteration 63/1000 | Loss: 0.00064385
Iteration 64/1000 | Loss: 0.00071581
Iteration 65/1000 | Loss: 0.00107457
Iteration 66/1000 | Loss: 0.00070644
Iteration 67/1000 | Loss: 0.00056049
Iteration 68/1000 | Loss: 0.00043030
Iteration 69/1000 | Loss: 0.00165233
Iteration 70/1000 | Loss: 0.00027908
Iteration 71/1000 | Loss: 0.00124083
Iteration 72/1000 | Loss: 0.00226476
Iteration 73/1000 | Loss: 0.00174270
Iteration 74/1000 | Loss: 0.00103722
Iteration 75/1000 | Loss: 0.00128372
Iteration 76/1000 | Loss: 0.00216099
Iteration 77/1000 | Loss: 0.00136547
Iteration 78/1000 | Loss: 0.00187572
Iteration 79/1000 | Loss: 0.00068566
Iteration 80/1000 | Loss: 0.00060831
Iteration 81/1000 | Loss: 0.00212753
Iteration 82/1000 | Loss: 0.00174247
Iteration 83/1000 | Loss: 0.00076807
Iteration 84/1000 | Loss: 0.00071775
Iteration 85/1000 | Loss: 0.00064157
Iteration 86/1000 | Loss: 0.00047761
Iteration 87/1000 | Loss: 0.00060934
Iteration 88/1000 | Loss: 0.00043841
Iteration 89/1000 | Loss: 0.00044869
Iteration 90/1000 | Loss: 0.00047576
Iteration 91/1000 | Loss: 0.00040187
Iteration 92/1000 | Loss: 0.00042671
Iteration 93/1000 | Loss: 0.00045686
Iteration 94/1000 | Loss: 0.00085383
Iteration 95/1000 | Loss: 0.00060804
Iteration 96/1000 | Loss: 0.00031023
Iteration 97/1000 | Loss: 0.00036266
Iteration 98/1000 | Loss: 0.00039376
Iteration 99/1000 | Loss: 0.00080433
Iteration 100/1000 | Loss: 0.00060260
Iteration 101/1000 | Loss: 0.00087847
Iteration 102/1000 | Loss: 0.00076730
Iteration 103/1000 | Loss: 0.00127401
Iteration 104/1000 | Loss: 0.00047030
Iteration 105/1000 | Loss: 0.00029350
Iteration 106/1000 | Loss: 0.00011645
Iteration 107/1000 | Loss: 0.00036237
Iteration 108/1000 | Loss: 0.00038572
Iteration 109/1000 | Loss: 0.00020393
Iteration 110/1000 | Loss: 0.00013708
Iteration 111/1000 | Loss: 0.00010644
Iteration 112/1000 | Loss: 0.00034890
Iteration 113/1000 | Loss: 0.00056608
Iteration 114/1000 | Loss: 0.00139511
Iteration 115/1000 | Loss: 0.00037210
Iteration 116/1000 | Loss: 0.00050952
Iteration 117/1000 | Loss: 0.00095673
Iteration 118/1000 | Loss: 0.00045704
Iteration 119/1000 | Loss: 0.00017052
Iteration 120/1000 | Loss: 0.00019252
Iteration 121/1000 | Loss: 0.00016166
Iteration 122/1000 | Loss: 0.00040893
Iteration 123/1000 | Loss: 0.00076802
Iteration 124/1000 | Loss: 0.00061372
Iteration 125/1000 | Loss: 0.00012154
Iteration 126/1000 | Loss: 0.00009503
Iteration 127/1000 | Loss: 0.00098472
Iteration 128/1000 | Loss: 0.00137111
Iteration 129/1000 | Loss: 0.00045860
Iteration 130/1000 | Loss: 0.00039973
Iteration 131/1000 | Loss: 0.00017630
Iteration 132/1000 | Loss: 0.00059881
Iteration 133/1000 | Loss: 0.00071450
Iteration 134/1000 | Loss: 0.00023253
Iteration 135/1000 | Loss: 0.00019823
Iteration 136/1000 | Loss: 0.00035964
Iteration 137/1000 | Loss: 0.00062855
Iteration 138/1000 | Loss: 0.00009391
Iteration 139/1000 | Loss: 0.00035174
Iteration 140/1000 | Loss: 0.00282435
Iteration 141/1000 | Loss: 0.00132953
Iteration 142/1000 | Loss: 0.00068960
Iteration 143/1000 | Loss: 0.00166601
Iteration 144/1000 | Loss: 0.00130205
Iteration 145/1000 | Loss: 0.00013821
Iteration 146/1000 | Loss: 0.00010683
Iteration 147/1000 | Loss: 0.00035724
Iteration 148/1000 | Loss: 0.00079681
Iteration 149/1000 | Loss: 0.00064526
Iteration 150/1000 | Loss: 0.00058081
Iteration 151/1000 | Loss: 0.00067890
Iteration 152/1000 | Loss: 0.00065301
Iteration 153/1000 | Loss: 0.00074201
Iteration 154/1000 | Loss: 0.00031295
Iteration 155/1000 | Loss: 0.00012259
Iteration 156/1000 | Loss: 0.00061932
Iteration 157/1000 | Loss: 0.00053140
Iteration 158/1000 | Loss: 0.00021218
Iteration 159/1000 | Loss: 0.00040122
Iteration 160/1000 | Loss: 0.00009729
Iteration 161/1000 | Loss: 0.00027970
Iteration 162/1000 | Loss: 0.00046775
Iteration 163/1000 | Loss: 0.00022524
Iteration 164/1000 | Loss: 0.00056785
Iteration 165/1000 | Loss: 0.00033265
Iteration 166/1000 | Loss: 0.00036787
Iteration 167/1000 | Loss: 0.00089983
Iteration 168/1000 | Loss: 0.00019988
Iteration 169/1000 | Loss: 0.00016263
Iteration 170/1000 | Loss: 0.00031779
Iteration 171/1000 | Loss: 0.00019994
Iteration 172/1000 | Loss: 0.00021001
Iteration 173/1000 | Loss: 0.00033224
Iteration 174/1000 | Loss: 0.00020760
Iteration 175/1000 | Loss: 0.00066279
Iteration 176/1000 | Loss: 0.00009314
Iteration 177/1000 | Loss: 0.00033544
Iteration 178/1000 | Loss: 0.00109971
Iteration 179/1000 | Loss: 0.00046696
Iteration 180/1000 | Loss: 0.00069756
Iteration 181/1000 | Loss: 0.00164306
Iteration 182/1000 | Loss: 0.00036886
Iteration 183/1000 | Loss: 0.00017980
Iteration 184/1000 | Loss: 0.00035088
Iteration 185/1000 | Loss: 0.00090004
Iteration 186/1000 | Loss: 0.00036944
Iteration 187/1000 | Loss: 0.00015759
Iteration 188/1000 | Loss: 0.00018575
Iteration 189/1000 | Loss: 0.00016204
Iteration 190/1000 | Loss: 0.00010706
Iteration 191/1000 | Loss: 0.00038437
Iteration 192/1000 | Loss: 0.00010426
Iteration 193/1000 | Loss: 0.00014242
Iteration 194/1000 | Loss: 0.00038365
Iteration 195/1000 | Loss: 0.00087644
Iteration 196/1000 | Loss: 0.00113692
Iteration 197/1000 | Loss: 0.00046114
Iteration 198/1000 | Loss: 0.00041634
Iteration 199/1000 | Loss: 0.00025775
Iteration 200/1000 | Loss: 0.00027766
Iteration 201/1000 | Loss: 0.00068550
Iteration 202/1000 | Loss: 0.00086772
Iteration 203/1000 | Loss: 0.00072228
Iteration 204/1000 | Loss: 0.00113642
Iteration 205/1000 | Loss: 0.00113656
Iteration 206/1000 | Loss: 0.00084183
Iteration 207/1000 | Loss: 0.00009807
Iteration 208/1000 | Loss: 0.00038387
Iteration 209/1000 | Loss: 0.00046211
Iteration 210/1000 | Loss: 0.00054218
Iteration 211/1000 | Loss: 0.00066597
Iteration 212/1000 | Loss: 0.00108771
Iteration 213/1000 | Loss: 0.00025100
Iteration 214/1000 | Loss: 0.00030326
Iteration 215/1000 | Loss: 0.00026888
Iteration 216/1000 | Loss: 0.00020205
Iteration 217/1000 | Loss: 0.00009895
Iteration 218/1000 | Loss: 0.00008331
Iteration 219/1000 | Loss: 0.00007879
Iteration 220/1000 | Loss: 0.00023108
Iteration 221/1000 | Loss: 0.00040687
Iteration 222/1000 | Loss: 0.00077591
Iteration 223/1000 | Loss: 0.00016959
Iteration 224/1000 | Loss: 0.00070177
Iteration 225/1000 | Loss: 0.00012921
Iteration 226/1000 | Loss: 0.00064591
Iteration 227/1000 | Loss: 0.00028357
Iteration 228/1000 | Loss: 0.00008124
Iteration 229/1000 | Loss: 0.00010653
Iteration 230/1000 | Loss: 0.00006672
Iteration 231/1000 | Loss: 0.00091287
Iteration 232/1000 | Loss: 0.00089225
Iteration 233/1000 | Loss: 0.00008347
Iteration 234/1000 | Loss: 0.00007793
Iteration 235/1000 | Loss: 0.00006996
Iteration 236/1000 | Loss: 0.00008283
Iteration 237/1000 | Loss: 0.00006203
Iteration 238/1000 | Loss: 0.00035676
Iteration 239/1000 | Loss: 0.00010161
Iteration 240/1000 | Loss: 0.00008993
Iteration 241/1000 | Loss: 0.00007046
Iteration 242/1000 | Loss: 0.00035312
Iteration 243/1000 | Loss: 0.00008339
Iteration 244/1000 | Loss: 0.00006190
Iteration 245/1000 | Loss: 0.00099219
Iteration 246/1000 | Loss: 0.00007591
Iteration 247/1000 | Loss: 0.00008358
Iteration 248/1000 | Loss: 0.00144337
Iteration 249/1000 | Loss: 0.00393346
Iteration 250/1000 | Loss: 0.00049097
Iteration 251/1000 | Loss: 0.00032724
Iteration 252/1000 | Loss: 0.00019616
Iteration 253/1000 | Loss: 0.00047181
Iteration 254/1000 | Loss: 0.00010007
Iteration 255/1000 | Loss: 0.00031303
Iteration 256/1000 | Loss: 0.00009457
Iteration 257/1000 | Loss: 0.00024639
Iteration 258/1000 | Loss: 0.00074772
Iteration 259/1000 | Loss: 0.00010111
Iteration 260/1000 | Loss: 0.00012944
Iteration 261/1000 | Loss: 0.00087996
Iteration 262/1000 | Loss: 0.00076399
Iteration 263/1000 | Loss: 0.00011469
Iteration 264/1000 | Loss: 0.00048624
Iteration 265/1000 | Loss: 0.00011679
Iteration 266/1000 | Loss: 0.00008001
Iteration 267/1000 | Loss: 0.00035299
Iteration 268/1000 | Loss: 0.00072281
Iteration 269/1000 | Loss: 0.00044800
Iteration 270/1000 | Loss: 0.00036753
Iteration 271/1000 | Loss: 0.00013976
Iteration 272/1000 | Loss: 0.00023370
Iteration 273/1000 | Loss: 0.00025654
Iteration 274/1000 | Loss: 0.00041628
Iteration 275/1000 | Loss: 0.00036648
Iteration 276/1000 | Loss: 0.00010497
Iteration 277/1000 | Loss: 0.00006808
Iteration 278/1000 | Loss: 0.00006191
Iteration 279/1000 | Loss: 0.00008163
Iteration 280/1000 | Loss: 0.00007071
Iteration 281/1000 | Loss: 0.00005764
Iteration 282/1000 | Loss: 0.00059396
Iteration 283/1000 | Loss: 0.00037462
Iteration 284/1000 | Loss: 0.00037519
Iteration 285/1000 | Loss: 0.00049553
Iteration 286/1000 | Loss: 0.00032282
Iteration 287/1000 | Loss: 0.00007287
Iteration 288/1000 | Loss: 0.00093051
Iteration 289/1000 | Loss: 0.00034935
Iteration 290/1000 | Loss: 0.00021231
Iteration 291/1000 | Loss: 0.00007946
Iteration 292/1000 | Loss: 0.00007783
Iteration 293/1000 | Loss: 0.00006609
Iteration 294/1000 | Loss: 0.00007230
Iteration 295/1000 | Loss: 0.00036225
Iteration 296/1000 | Loss: 0.00037789
Iteration 297/1000 | Loss: 0.00029092
Iteration 298/1000 | Loss: 0.00026291
Iteration 299/1000 | Loss: 0.00020685
Iteration 300/1000 | Loss: 0.00016448
Iteration 301/1000 | Loss: 0.00005919
Iteration 302/1000 | Loss: 0.00007358
Iteration 303/1000 | Loss: 0.00095307
Iteration 304/1000 | Loss: 0.00027671
Iteration 305/1000 | Loss: 0.00095911
Iteration 306/1000 | Loss: 0.00011883
Iteration 307/1000 | Loss: 0.00007784
Iteration 308/1000 | Loss: 0.00050041
Iteration 309/1000 | Loss: 0.00048631
Iteration 310/1000 | Loss: 0.00053664
Iteration 311/1000 | Loss: 0.00046939
Iteration 312/1000 | Loss: 0.00049649
Iteration 313/1000 | Loss: 0.00078379
Iteration 314/1000 | Loss: 0.00054358
Iteration 315/1000 | Loss: 0.00065982
Iteration 316/1000 | Loss: 0.00043349
Iteration 317/1000 | Loss: 0.00006449
Iteration 318/1000 | Loss: 0.00005600
Iteration 319/1000 | Loss: 0.00024721
Iteration 320/1000 | Loss: 0.00006128
Iteration 321/1000 | Loss: 0.00005802
Iteration 322/1000 | Loss: 0.00006339
Iteration 323/1000 | Loss: 0.00006314
Iteration 324/1000 | Loss: 0.00032588
Iteration 325/1000 | Loss: 0.00005854
Iteration 326/1000 | Loss: 0.00005981
Iteration 327/1000 | Loss: 0.00033346
Iteration 328/1000 | Loss: 0.00009600
Iteration 329/1000 | Loss: 0.00007570
Iteration 330/1000 | Loss: 0.00005897
Iteration 331/1000 | Loss: 0.00005888
Iteration 332/1000 | Loss: 0.00005938
Iteration 333/1000 | Loss: 0.00005681
Iteration 334/1000 | Loss: 0.00005686
Iteration 335/1000 | Loss: 0.00007012
Iteration 336/1000 | Loss: 0.00006230
Iteration 337/1000 | Loss: 0.00031098
Iteration 338/1000 | Loss: 0.00028423
Iteration 339/1000 | Loss: 0.00009199
Iteration 340/1000 | Loss: 0.00005530
Iteration 341/1000 | Loss: 0.00005454
Iteration 342/1000 | Loss: 0.00006087
Iteration 343/1000 | Loss: 0.00006405
Iteration 344/1000 | Loss: 0.00005113
Iteration 345/1000 | Loss: 0.00005827
Iteration 346/1000 | Loss: 0.00004907
Iteration 347/1000 | Loss: 0.00006020
Iteration 348/1000 | Loss: 0.00032917
Iteration 349/1000 | Loss: 0.00008199
Iteration 350/1000 | Loss: 0.00005736
Iteration 351/1000 | Loss: 0.00006258
Iteration 352/1000 | Loss: 0.00005270
Iteration 353/1000 | Loss: 0.00004296
Iteration 354/1000 | Loss: 0.00032317
Iteration 355/1000 | Loss: 0.00179345
Iteration 356/1000 | Loss: 0.00037773
Iteration 357/1000 | Loss: 0.00008282
Iteration 358/1000 | Loss: 0.00005984
Iteration 359/1000 | Loss: 0.00005446
Iteration 360/1000 | Loss: 0.00005191
Iteration 361/1000 | Loss: 0.00004871
Iteration 362/1000 | Loss: 0.00004679
Iteration 363/1000 | Loss: 0.00004450
Iteration 364/1000 | Loss: 0.00004204
Iteration 365/1000 | Loss: 0.00029714
Iteration 366/1000 | Loss: 0.00119132
Iteration 367/1000 | Loss: 0.00017740
Iteration 368/1000 | Loss: 0.00066683
Iteration 369/1000 | Loss: 0.00316451
Iteration 370/1000 | Loss: 0.00048121
Iteration 371/1000 | Loss: 0.00010158
Iteration 372/1000 | Loss: 0.00007407
Iteration 373/1000 | Loss: 0.00059847
Iteration 374/1000 | Loss: 0.00057932
Iteration 375/1000 | Loss: 0.00054319
Iteration 376/1000 | Loss: 0.00026643
Iteration 377/1000 | Loss: 0.00102774
Iteration 378/1000 | Loss: 0.00035740
Iteration 379/1000 | Loss: 0.00009190
Iteration 380/1000 | Loss: 0.00007180
Iteration 381/1000 | Loss: 0.00033371
Iteration 382/1000 | Loss: 0.00046599
Iteration 383/1000 | Loss: 0.00011812
Iteration 384/1000 | Loss: 0.00006131
Iteration 385/1000 | Loss: 0.00005801
Iteration 386/1000 | Loss: 0.00005269
Iteration 387/1000 | Loss: 0.00004942
Iteration 388/1000 | Loss: 0.00028626
Iteration 389/1000 | Loss: 0.00243965
Iteration 390/1000 | Loss: 0.00380212
Iteration 391/1000 | Loss: 0.00098039
Iteration 392/1000 | Loss: 0.00247196
Iteration 393/1000 | Loss: 0.00237326
Iteration 394/1000 | Loss: 0.00188969
Iteration 395/1000 | Loss: 0.00167975
Iteration 396/1000 | Loss: 0.00190396
Iteration 397/1000 | Loss: 0.00171698
Iteration 398/1000 | Loss: 0.00164746
Iteration 399/1000 | Loss: 0.00184975
Iteration 400/1000 | Loss: 0.00151240
Iteration 401/1000 | Loss: 0.00073238
Iteration 402/1000 | Loss: 0.00080410
Iteration 403/1000 | Loss: 0.00070328
Iteration 404/1000 | Loss: 0.00010546
Iteration 405/1000 | Loss: 0.00030068
Iteration 406/1000 | Loss: 0.00040273
Iteration 407/1000 | Loss: 0.00037616
Iteration 408/1000 | Loss: 0.00038258
Iteration 409/1000 | Loss: 0.00034932
Iteration 410/1000 | Loss: 0.00037286
Iteration 411/1000 | Loss: 0.00050699
Iteration 412/1000 | Loss: 0.00022130
Iteration 413/1000 | Loss: 0.00009080
Iteration 414/1000 | Loss: 0.00041736
Iteration 415/1000 | Loss: 0.00075053
Iteration 416/1000 | Loss: 0.00106872
Iteration 417/1000 | Loss: 0.00047512
Iteration 418/1000 | Loss: 0.00014882
Iteration 419/1000 | Loss: 0.00016965
Iteration 420/1000 | Loss: 0.00013415
Iteration 421/1000 | Loss: 0.00007172
Iteration 422/1000 | Loss: 0.00111363
Iteration 423/1000 | Loss: 0.00026916
Iteration 424/1000 | Loss: 0.00005423
Iteration 425/1000 | Loss: 0.00005127
Iteration 426/1000 | Loss: 0.00022266
Iteration 427/1000 | Loss: 0.00037571
Iteration 428/1000 | Loss: 0.00028444
Iteration 429/1000 | Loss: 0.00027023
Iteration 430/1000 | Loss: 0.00017827
Iteration 431/1000 | Loss: 0.00020672
Iteration 432/1000 | Loss: 0.00021289
Iteration 433/1000 | Loss: 0.00031288
Iteration 434/1000 | Loss: 0.00034816
Iteration 435/1000 | Loss: 0.00068792
Iteration 436/1000 | Loss: 0.00044049
Iteration 437/1000 | Loss: 0.00005195
Iteration 438/1000 | Loss: 0.00050236
Iteration 439/1000 | Loss: 0.00089073
Iteration 440/1000 | Loss: 0.00006849
Iteration 441/1000 | Loss: 0.00080939
Iteration 442/1000 | Loss: 0.00050974
Iteration 443/1000 | Loss: 0.00080691
Iteration 444/1000 | Loss: 0.00047270
Iteration 445/1000 | Loss: 0.00030695
Iteration 446/1000 | Loss: 0.00031840
Iteration 447/1000 | Loss: 0.00026976
Iteration 448/1000 | Loss: 0.00014440
Iteration 449/1000 | Loss: 0.00004245
Iteration 450/1000 | Loss: 0.00003947
Iteration 451/1000 | Loss: 0.00003635
Iteration 452/1000 | Loss: 0.00003483
Iteration 453/1000 | Loss: 0.00029861
Iteration 454/1000 | Loss: 0.00025154
Iteration 455/1000 | Loss: 0.00078169
Iteration 456/1000 | Loss: 0.00032494
Iteration 457/1000 | Loss: 0.00023670
Iteration 458/1000 | Loss: 0.00028384
Iteration 459/1000 | Loss: 0.00078388
Iteration 460/1000 | Loss: 0.00048665
Iteration 461/1000 | Loss: 0.00030556
Iteration 462/1000 | Loss: 0.00017034
Iteration 463/1000 | Loss: 0.00004539
Iteration 464/1000 | Loss: 0.00053426
Iteration 465/1000 | Loss: 0.00023936
Iteration 466/1000 | Loss: 0.00060497
Iteration 467/1000 | Loss: 0.00004835
Iteration 468/1000 | Loss: 0.00062165
Iteration 469/1000 | Loss: 0.00055117
Iteration 470/1000 | Loss: 0.00095951
Iteration 471/1000 | Loss: 0.00086709
Iteration 472/1000 | Loss: 0.00007185
Iteration 473/1000 | Loss: 0.00004739
Iteration 474/1000 | Loss: 0.00004039
Iteration 475/1000 | Loss: 0.00003822
Iteration 476/1000 | Loss: 0.00003694
Iteration 477/1000 | Loss: 0.00003558
Iteration 478/1000 | Loss: 0.00003477
Iteration 479/1000 | Loss: 0.00059676
Iteration 480/1000 | Loss: 0.00006608
Iteration 481/1000 | Loss: 0.00005066
Iteration 482/1000 | Loss: 0.00004162
Iteration 483/1000 | Loss: 0.00003869
Iteration 484/1000 | Loss: 0.00003687
Iteration 485/1000 | Loss: 0.00003571
Iteration 486/1000 | Loss: 0.00003491
Iteration 487/1000 | Loss: 0.00030167
Iteration 488/1000 | Loss: 0.00030163
Iteration 489/1000 | Loss: 0.00336676
Iteration 490/1000 | Loss: 0.00248717
Iteration 491/1000 | Loss: 0.00055606
Iteration 492/1000 | Loss: 0.00049161
Iteration 493/1000 | Loss: 0.00051823
Iteration 494/1000 | Loss: 0.00042870
Iteration 495/1000 | Loss: 0.00003922
Iteration 496/1000 | Loss: 0.00003649
Iteration 497/1000 | Loss: 0.00003525
Iteration 498/1000 | Loss: 0.00003445
Iteration 499/1000 | Loss: 0.00030839
Iteration 500/1000 | Loss: 0.00251715
Iteration 501/1000 | Loss: 0.00129829
Iteration 502/1000 | Loss: 0.00052269
Iteration 503/1000 | Loss: 0.00041772
Iteration 504/1000 | Loss: 0.00022502
Iteration 505/1000 | Loss: 0.00012156
Iteration 506/1000 | Loss: 0.00025024
Iteration 507/1000 | Loss: 0.00005494
Iteration 508/1000 | Loss: 0.00004326
Iteration 509/1000 | Loss: 0.00003995
Iteration 510/1000 | Loss: 0.00003778
Iteration 511/1000 | Loss: 0.00003645
Iteration 512/1000 | Loss: 0.00003480
Iteration 513/1000 | Loss: 0.00059382
Iteration 514/1000 | Loss: 0.00035580
Iteration 515/1000 | Loss: 0.00003454
Iteration 516/1000 | Loss: 0.00003306
Iteration 517/1000 | Loss: 0.00003256
Iteration 518/1000 | Loss: 0.00114062
Iteration 519/1000 | Loss: 0.00367164
Iteration 520/1000 | Loss: 0.00248316
Iteration 521/1000 | Loss: 0.00051791
Iteration 522/1000 | Loss: 0.00035647
Iteration 523/1000 | Loss: 0.00115664
Iteration 524/1000 | Loss: 0.00047885
Iteration 525/1000 | Loss: 0.00032124
Iteration 526/1000 | Loss: 0.00023600
Iteration 527/1000 | Loss: 0.00005824
Iteration 528/1000 | Loss: 0.00004991
Iteration 529/1000 | Loss: 0.00004626
Iteration 530/1000 | Loss: 0.00004356
Iteration 531/1000 | Loss: 0.00004141
Iteration 532/1000 | Loss: 0.00167238
Iteration 533/1000 | Loss: 0.00288251
Iteration 534/1000 | Loss: 0.00074929
Iteration 535/1000 | Loss: 0.00140902
Iteration 536/1000 | Loss: 0.00050567
Iteration 537/1000 | Loss: 0.00108092
Iteration 538/1000 | Loss: 0.00164970
Iteration 539/1000 | Loss: 0.00319505
Iteration 540/1000 | Loss: 0.00073689
Iteration 541/1000 | Loss: 0.00065103
Iteration 542/1000 | Loss: 0.00056177
Iteration 543/1000 | Loss: 0.00012371
Iteration 544/1000 | Loss: 0.00037172
Iteration 545/1000 | Loss: 0.00104632
Iteration 546/1000 | Loss: 0.00132517
Iteration 547/1000 | Loss: 0.00088869
Iteration 548/1000 | Loss: 0.00079777
Iteration 549/1000 | Loss: 0.00054281
Iteration 550/1000 | Loss: 0.00009026
Iteration 551/1000 | Loss: 0.00007797
Iteration 552/1000 | Loss: 0.00009004
Iteration 553/1000 | Loss: 0.00050073
Iteration 554/1000 | Loss: 0.00007441
Iteration 555/1000 | Loss: 0.00054742
Iteration 556/1000 | Loss: 0.00017635
Iteration 557/1000 | Loss: 0.00020233
Iteration 558/1000 | Loss: 0.00038158
Iteration 559/1000 | Loss: 0.00099781
Iteration 560/1000 | Loss: 0.00048954
Iteration 561/1000 | Loss: 0.00048082
Iteration 562/1000 | Loss: 0.00030804
Iteration 563/1000 | Loss: 0.00006132
Iteration 564/1000 | Loss: 0.00023388
Iteration 565/1000 | Loss: 0.00039197
Iteration 566/1000 | Loss: 0.00034489
Iteration 567/1000 | Loss: 0.00026567
Iteration 568/1000 | Loss: 0.00027189
Iteration 569/1000 | Loss: 0.00005878
Iteration 570/1000 | Loss: 0.00008902
Iteration 571/1000 | Loss: 0.00008527
Iteration 572/1000 | Loss: 0.00008692
Iteration 573/1000 | Loss: 0.00005471
Iteration 574/1000 | Loss: 0.00030856
Iteration 575/1000 | Loss: 0.00030926
Iteration 576/1000 | Loss: 0.00007269
Iteration 577/1000 | Loss: 0.00004969
Iteration 578/1000 | Loss: 0.00004440
Iteration 579/1000 | Loss: 0.00004141
Iteration 580/1000 | Loss: 0.00003932
Iteration 581/1000 | Loss: 0.00032182
Iteration 582/1000 | Loss: 0.00004325
Iteration 583/1000 | Loss: 0.00042137
Iteration 584/1000 | Loss: 0.00016306
Iteration 585/1000 | Loss: 0.00004163
Iteration 586/1000 | Loss: 0.00003832
Iteration 587/1000 | Loss: 0.00004777
Iteration 588/1000 | Loss: 0.00062817
Iteration 589/1000 | Loss: 0.00026344
Iteration 590/1000 | Loss: 0.00005965
Iteration 591/1000 | Loss: 0.00010140
Iteration 592/1000 | Loss: 0.00018766
Iteration 593/1000 | Loss: 0.00017769
Iteration 594/1000 | Loss: 0.00022948
Iteration 595/1000 | Loss: 0.00025048
Iteration 596/1000 | Loss: 0.00004439
Iteration 597/1000 | Loss: 0.00003950
Iteration 598/1000 | Loss: 0.00003700
Iteration 599/1000 | Loss: 0.00003419
Iteration 600/1000 | Loss: 0.00003229
Iteration 601/1000 | Loss: 0.00003120
Iteration 602/1000 | Loss: 0.00003019
Iteration 603/1000 | Loss: 0.00002943
Iteration 604/1000 | Loss: 0.00059161
Iteration 605/1000 | Loss: 0.00059158
Iteration 606/1000 | Loss: 0.00045073
Iteration 607/1000 | Loss: 0.00013300
Iteration 608/1000 | Loss: 0.00004056
Iteration 609/1000 | Loss: 0.00003581
Iteration 610/1000 | Loss: 0.00003343
Iteration 611/1000 | Loss: 0.00003178
Iteration 612/1000 | Loss: 0.00003094
Iteration 613/1000 | Loss: 0.00003028
Iteration 614/1000 | Loss: 0.00002964
Iteration 615/1000 | Loss: 0.00002908
Iteration 616/1000 | Loss: 0.00002883
Iteration 617/1000 | Loss: 0.00002863
Iteration 618/1000 | Loss: 0.00030234
Iteration 619/1000 | Loss: 0.00278040
Iteration 620/1000 | Loss: 0.00094811
Iteration 621/1000 | Loss: 0.00003712
Iteration 622/1000 | Loss: 0.00003222
Iteration 623/1000 | Loss: 0.00003076
Iteration 624/1000 | Loss: 0.00030492
Iteration 625/1000 | Loss: 0.00003952
Iteration 626/1000 | Loss: 0.00003305
Iteration 627/1000 | Loss: 0.00031080
Iteration 628/1000 | Loss: 0.00004036
Iteration 629/1000 | Loss: 0.00057791
Iteration 630/1000 | Loss: 0.00235800
Iteration 631/1000 | Loss: 0.00087436
Iteration 632/1000 | Loss: 0.00017291
Iteration 633/1000 | Loss: 0.00015596
Iteration 634/1000 | Loss: 0.00011620
Iteration 635/1000 | Loss: 0.00013034
Iteration 636/1000 | Loss: 0.00009084
Iteration 637/1000 | Loss: 0.00014339
Iteration 638/1000 | Loss: 0.00020759
Iteration 639/1000 | Loss: 0.00011929
Iteration 640/1000 | Loss: 0.00019199
Iteration 641/1000 | Loss: 0.00024481
Iteration 642/1000 | Loss: 0.00005098
Iteration 643/1000 | Loss: 0.00024642
Iteration 644/1000 | Loss: 0.00004483
Iteration 645/1000 | Loss: 0.00004017
Iteration 646/1000 | Loss: 0.00003907
Iteration 647/1000 | Loss: 0.00003769
Iteration 648/1000 | Loss: 0.00003631
Iteration 649/1000 | Loss: 0.00003525
Iteration 650/1000 | Loss: 0.00084159
Iteration 651/1000 | Loss: 0.00087794
Iteration 652/1000 | Loss: 0.00050700
Iteration 653/1000 | Loss: 0.00005489
Iteration 654/1000 | Loss: 0.00004314
Iteration 655/1000 | Loss: 0.00004026
Iteration 656/1000 | Loss: 0.00003792
Iteration 657/1000 | Loss: 0.00003659
Iteration 658/1000 | Loss: 0.00003533
Iteration 659/1000 | Loss: 0.00084969
Iteration 660/1000 | Loss: 0.00115887
Iteration 661/1000 | Loss: 0.00014354
Iteration 662/1000 | Loss: 0.00006240
Iteration 663/1000 | Loss: 0.00004898
Iteration 664/1000 | Loss: 0.00068662
Iteration 665/1000 | Loss: 0.00017331
Iteration 666/1000 | Loss: 0.00004258
Iteration 667/1000 | Loss: 0.00003876
Iteration 668/1000 | Loss: 0.00003659
Iteration 669/1000 | Loss: 0.00003501
Iteration 670/1000 | Loss: 0.00087118
Iteration 671/1000 | Loss: 0.00038398
Iteration 672/1000 | Loss: 0.00003567
Iteration 673/1000 | Loss: 0.00003326
Iteration 674/1000 | Loss: 0.00064568
Iteration 675/1000 | Loss: 0.00147485
Iteration 676/1000 | Loss: 0.00085615
Iteration 677/1000 | Loss: 0.00004459
Iteration 678/1000 | Loss: 0.00003683
Iteration 679/1000 | Loss: 0.00003492
Iteration 680/1000 | Loss: 0.00025349
Iteration 681/1000 | Loss: 0.00015808
Iteration 682/1000 | Loss: 0.00003280
Iteration 683/1000 | Loss: 0.00003189
Iteration 684/1000 | Loss: 0.00003099
Iteration 685/1000 | Loss: 0.00030273
Iteration 686/1000 | Loss: 0.00003940
Iteration 687/1000 | Loss: 0.00056104
Iteration 688/1000 | Loss: 0.00032029
Iteration 689/1000 | Loss: 0.00015602
Iteration 690/1000 | Loss: 0.00004552
Iteration 691/1000 | Loss: 0.00003876
Iteration 692/1000 | Loss: 0.00003569
Iteration 693/1000 | Loss: 0.00003390
Iteration 694/1000 | Loss: 0.00003267
Iteration 695/1000 | Loss: 0.00003190
Iteration 696/1000 | Loss: 0.00113457
Iteration 697/1000 | Loss: 0.00110523
Iteration 698/1000 | Loss: 0.00045098
Iteration 699/1000 | Loss: 0.00008633
Iteration 700/1000 | Loss: 0.00004761
Iteration 701/1000 | Loss: 0.00004186
Iteration 702/1000 | Loss: 0.00026267
Iteration 703/1000 | Loss: 0.00016350
Iteration 704/1000 | Loss: 0.00003670
Iteration 705/1000 | Loss: 0.00053499
Iteration 706/1000 | Loss: 0.00029382
Iteration 707/1000 | Loss: 0.00038587
Iteration 708/1000 | Loss: 0.00043854
Iteration 709/1000 | Loss: 0.00024881
Iteration 710/1000 | Loss: 0.00023718
Iteration 711/1000 | Loss: 0.00018142
Iteration 712/1000 | Loss: 0.00012812
Iteration 713/1000 | Loss: 0.00023369
Iteration 714/1000 | Loss: 0.00126457
Iteration 715/1000 | Loss: 0.00008265
Iteration 716/1000 | Loss: 0.00005277
Iteration 717/1000 | Loss: 0.00004246
Iteration 718/1000 | Loss: 0.00003895
Iteration 719/1000 | Loss: 0.00003670
Iteration 720/1000 | Loss: 0.00003531
Iteration 721/1000 | Loss: 0.00038237
Iteration 722/1000 | Loss: 0.00014658
Iteration 723/1000 | Loss: 0.00038607
Iteration 724/1000 | Loss: 0.00015610
Iteration 725/1000 | Loss: 0.00006940
Iteration 726/1000 | Loss: 0.00024087
Iteration 727/1000 | Loss: 0.00014095
Iteration 728/1000 | Loss: 0.00008021
Iteration 729/1000 | Loss: 0.00019105
Iteration 730/1000 | Loss: 0.00012563
Iteration 731/1000 | Loss: 0.00003686
Iteration 732/1000 | Loss: 0.00003310
Iteration 733/1000 | Loss: 0.00003161
Iteration 734/1000 | Loss: 0.00003082
Iteration 735/1000 | Loss: 0.00003011
Iteration 736/1000 | Loss: 0.00082498
Iteration 737/1000 | Loss: 0.00078421
Iteration 738/1000 | Loss: 0.00140402
Iteration 739/1000 | Loss: 0.00032090
Iteration 740/1000 | Loss: 0.00028874
Iteration 741/1000 | Loss: 0.00004250
Iteration 742/1000 | Loss: 0.00012867
Iteration 743/1000 | Loss: 0.00018534
Iteration 744/1000 | Loss: 0.00003506
Iteration 745/1000 | Loss: 0.00003241
Iteration 746/1000 | Loss: 0.00003149
Iteration 747/1000 | Loss: 0.00003067
Iteration 748/1000 | Loss: 0.00027388
Iteration 749/1000 | Loss: 0.00018058
Iteration 750/1000 | Loss: 0.00025659
Iteration 751/1000 | Loss: 0.00003812
Iteration 752/1000 | Loss: 0.00003313
Iteration 753/1000 | Loss: 0.00003170
Iteration 754/1000 | Loss: 0.00003108
Iteration 755/1000 | Loss: 0.00029926
Iteration 756/1000 | Loss: 0.00004024
Iteration 757/1000 | Loss: 0.00003115
Iteration 758/1000 | Loss: 0.00030637
Iteration 759/1000 | Loss: 0.00003800
Iteration 760/1000 | Loss: 0.00003123
Iteration 761/1000 | Loss: 0.00002945
Iteration 762/1000 | Loss: 0.00002875
Iteration 763/1000 | Loss: 0.00002824
Iteration 764/1000 | Loss: 0.00002787
Iteration 765/1000 | Loss: 0.00057206
Iteration 766/1000 | Loss: 0.00004575
Iteration 767/1000 | Loss: 0.00003445
Iteration 768/1000 | Loss: 0.00003156
Iteration 769/1000 | Loss: 0.00003062
Iteration 770/1000 | Loss: 0.00056687
Iteration 771/1000 | Loss: 0.00025337
Iteration 772/1000 | Loss: 0.00002988
Iteration 773/1000 | Loss: 0.00002913
Iteration 774/1000 | Loss: 0.00084798
Iteration 775/1000 | Loss: 0.00215333
Iteration 776/1000 | Loss: 0.00037471
Iteration 777/1000 | Loss: 0.00052247
Iteration 778/1000 | Loss: 0.00054348
Iteration 779/1000 | Loss: 0.00007804
Iteration 780/1000 | Loss: 0.00080910
Iteration 781/1000 | Loss: 0.00078123
Iteration 782/1000 | Loss: 0.00010299
Iteration 783/1000 | Loss: 0.00007657
Iteration 784/1000 | Loss: 0.00006519
Iteration 785/1000 | Loss: 0.00005825
Iteration 786/1000 | Loss: 0.00004865
Iteration 787/1000 | Loss: 0.00012807
Iteration 788/1000 | Loss: 0.00014606
Iteration 789/1000 | Loss: 0.00014976
Iteration 790/1000 | Loss: 0.00008765
Iteration 791/1000 | Loss: 0.00004492
Iteration 792/1000 | Loss: 0.00009897
Iteration 793/1000 | Loss: 0.00006330
Iteration 794/1000 | Loss: 0.00009089
Iteration 795/1000 | Loss: 0.00006012
Iteration 796/1000 | Loss: 0.00008467
Iteration 797/1000 | Loss: 0.00003516
Iteration 798/1000 | Loss: 0.00003298
Iteration 799/1000 | Loss: 0.00003101
Iteration 800/1000 | Loss: 0.00002900
Iteration 801/1000 | Loss: 0.00002786
Iteration 802/1000 | Loss: 0.00002725
Iteration 803/1000 | Loss: 0.00002669
Iteration 804/1000 | Loss: 0.00002616
Iteration 805/1000 | Loss: 0.00002562
Iteration 806/1000 | Loss: 0.00002522
Iteration 807/1000 | Loss: 0.00002485
Iteration 808/1000 | Loss: 0.00002446
Iteration 809/1000 | Loss: 0.00002416
Iteration 810/1000 | Loss: 0.00002384
Iteration 811/1000 | Loss: 0.00002365
Iteration 812/1000 | Loss: 0.00002361
Iteration 813/1000 | Loss: 0.00002345
Iteration 814/1000 | Loss: 0.00002336
Iteration 815/1000 | Loss: 0.00002334
Iteration 816/1000 | Loss: 0.00002329
Iteration 817/1000 | Loss: 0.00002329
Iteration 818/1000 | Loss: 0.00002329
Iteration 819/1000 | Loss: 0.00002329
Iteration 820/1000 | Loss: 0.00002329
Iteration 821/1000 | Loss: 0.00002329
Iteration 822/1000 | Loss: 0.00002328
Iteration 823/1000 | Loss: 0.00002328
Iteration 824/1000 | Loss: 0.00002327
Iteration 825/1000 | Loss: 0.00002327
Iteration 826/1000 | Loss: 0.00002326
Iteration 827/1000 | Loss: 0.00002326
Iteration 828/1000 | Loss: 0.00002326
Iteration 829/1000 | Loss: 0.00002325
Iteration 830/1000 | Loss: 0.00002325
Iteration 831/1000 | Loss: 0.00002324
Iteration 832/1000 | Loss: 0.00002321
Iteration 833/1000 | Loss: 0.00002320
Iteration 834/1000 | Loss: 0.00002320
Iteration 835/1000 | Loss: 0.00002320
Iteration 836/1000 | Loss: 0.00002320
Iteration 837/1000 | Loss: 0.00002319
Iteration 838/1000 | Loss: 0.00002319
Iteration 839/1000 | Loss: 0.00002319
Iteration 840/1000 | Loss: 0.00002319
Iteration 841/1000 | Loss: 0.00002318
Iteration 842/1000 | Loss: 0.00002318
Iteration 843/1000 | Loss: 0.00002316
Iteration 844/1000 | Loss: 0.00002316
Iteration 845/1000 | Loss: 0.00002316
Iteration 846/1000 | Loss: 0.00002316
Iteration 847/1000 | Loss: 0.00002315
Iteration 848/1000 | Loss: 0.00029275
Iteration 849/1000 | Loss: 0.00141610
Iteration 850/1000 | Loss: 0.00029933
Iteration 851/1000 | Loss: 0.00159385
Iteration 852/1000 | Loss: 0.00119286
Iteration 853/1000 | Loss: 0.00097106
Iteration 854/1000 | Loss: 0.00065584
Iteration 855/1000 | Loss: 0.00006022
Iteration 856/1000 | Loss: 0.00004850
Iteration 857/1000 | Loss: 0.00004350
Iteration 858/1000 | Loss: 0.00004026
Iteration 859/1000 | Loss: 0.00003830
Iteration 860/1000 | Loss: 0.00003634
Iteration 861/1000 | Loss: 0.00113392
Iteration 862/1000 | Loss: 0.00052493
Iteration 863/1000 | Loss: 0.00050243
Iteration 864/1000 | Loss: 0.00026996
Iteration 865/1000 | Loss: 0.00095752
Iteration 866/1000 | Loss: 0.00123601
Iteration 867/1000 | Loss: 0.00042451
Iteration 868/1000 | Loss: 0.00042684
Iteration 869/1000 | Loss: 0.00027458
Iteration 870/1000 | Loss: 0.00061490
Iteration 871/1000 | Loss: 0.00063626
Iteration 872/1000 | Loss: 0.00065057
Iteration 873/1000 | Loss: 0.00055011
Iteration 874/1000 | Loss: 0.00051985
Iteration 875/1000 | Loss: 0.00016669
Iteration 876/1000 | Loss: 0.00015130
Iteration 877/1000 | Loss: 0.00012123
Iteration 878/1000 | Loss: 0.00005310
Iteration 879/1000 | Loss: 0.00040948
Iteration 880/1000 | Loss: 0.00098781
Iteration 881/1000 | Loss: 0.00031924
Iteration 882/1000 | Loss: 0.00029323
Iteration 883/1000 | Loss: 0.00020178
Iteration 884/1000 | Loss: 0.00005213
Iteration 885/1000 | Loss: 0.00004604
Iteration 886/1000 | Loss: 0.00031826
Iteration 887/1000 | Loss: 0.00109626
Iteration 888/1000 | Loss: 0.00030384
Iteration 889/1000 | Loss: 0.00027813
Iteration 890/1000 | Loss: 0.00023089
Iteration 891/1000 | Loss: 0.00006302
Iteration 892/1000 | Loss: 0.00014124
Iteration 893/1000 | Loss: 0.00011334
Iteration 894/1000 | Loss: 0.00003958
Iteration 895/1000 | Loss: 0.00003514
Iteration 896/1000 | Loss: 0.00056368
Iteration 897/1000 | Loss: 0.00003428
Iteration 898/1000 | Loss: 0.00002971
Iteration 899/1000 | Loss: 0.00002859
Iteration 900/1000 | Loss: 0.00002762
Iteration 901/1000 | Loss: 0.00029952
Iteration 902/1000 | Loss: 0.00011838
Iteration 903/1000 | Loss: 0.00002706
Iteration 904/1000 | Loss: 0.00033417
Iteration 905/1000 | Loss: 0.00013137
Iteration 906/1000 | Loss: 0.00022572
Iteration 907/1000 | Loss: 0.00014095
Iteration 908/1000 | Loss: 0.00026816
Iteration 909/1000 | Loss: 0.00013206
Iteration 910/1000 | Loss: 0.00025583
Iteration 911/1000 | Loss: 0.00017947
Iteration 912/1000 | Loss: 0.00027453
Iteration 913/1000 | Loss: 0.00012538
Iteration 914/1000 | Loss: 0.00003361
Iteration 915/1000 | Loss: 0.00021820
Iteration 916/1000 | Loss: 0.00014537
Iteration 917/1000 | Loss: 0.00002993
Iteration 918/1000 | Loss: 0.00022752
Iteration 919/1000 | Loss: 0.00025301
Iteration 920/1000 | Loss: 0.00069471
Iteration 921/1000 | Loss: 0.00031869
Iteration 922/1000 | Loss: 0.00042490
Iteration 923/1000 | Loss: 0.00017381
Iteration 924/1000 | Loss: 0.00020241
Iteration 925/1000 | Loss: 0.00033175
Iteration 926/1000 | Loss: 0.00003355
Iteration 927/1000 | Loss: 0.00002839
Iteration 928/1000 | Loss: 0.00029092
Iteration 929/1000 | Loss: 0.00003012
Iteration 930/1000 | Loss: 0.00002805
Iteration 931/1000 | Loss: 0.00002683
Iteration 932/1000 | Loss: 0.00029630
Iteration 933/1000 | Loss: 0.00003647
Iteration 934/1000 | Loss: 0.00003109
Iteration 935/1000 | Loss: 0.00002797
Iteration 936/1000 | Loss: 0.00002694
Iteration 937/1000 | Loss: 0.00002603
Iteration 938/1000 | Loss: 0.00002544
Iteration 939/1000 | Loss: 0.00002491
Iteration 940/1000 | Loss: 0.00034893
Iteration 941/1000 | Loss: 0.00006012
Iteration 942/1000 | Loss: 0.00002424
Iteration 943/1000 | Loss: 0.00058710
Iteration 944/1000 | Loss: 0.00043800
Iteration 945/1000 | Loss: 0.00012877
Iteration 946/1000 | Loss: 0.00003916
Iteration 947/1000 | Loss: 0.00003297
Iteration 948/1000 | Loss: 0.00003020
Iteration 949/1000 | Loss: 0.00002801
Iteration 950/1000 | Loss: 0.00030949
Iteration 951/1000 | Loss: 0.00004256
Iteration 952/1000 | Loss: 0.00024144
Iteration 953/1000 | Loss: 0.00003414
Iteration 954/1000 | Loss: 0.00002906
Iteration 955/1000 | Loss: 0.00031837
Iteration 956/1000 | Loss: 0.00026051
Iteration 957/1000 | Loss: 0.00026221
Iteration 958/1000 | Loss: 0.00003785
Iteration 959/1000 | Loss: 0.00003148
Iteration 960/1000 | Loss: 0.00039814
Iteration 961/1000 | Loss: 0.00046079
Iteration 962/1000 | Loss: 0.00003079
Iteration 963/1000 | Loss: 0.00002475
Iteration 964/1000 | Loss: 0.00002217
Iteration 965/1000 | Loss: 0.00002147
Iteration 966/1000 | Loss: 0.00002114
Iteration 967/1000 | Loss: 0.00002097
Iteration 968/1000 | Loss: 0.00002068
Iteration 969/1000 | Loss: 0.00002060
Iteration 970/1000 | Loss: 0.00002052
Iteration 971/1000 | Loss: 0.00002043
Iteration 972/1000 | Loss: 0.00002043
Iteration 973/1000 | Loss: 0.00002043
Iteration 974/1000 | Loss: 0.00002043
Iteration 975/1000 | Loss: 0.00002043
Iteration 976/1000 | Loss: 0.00002034
Iteration 977/1000 | Loss: 0.00002030
Iteration 978/1000 | Loss: 0.00002029
Iteration 979/1000 | Loss: 0.00002028
Iteration 980/1000 | Loss: 0.00002028
Iteration 981/1000 | Loss: 0.00002028
Iteration 982/1000 | Loss: 0.00002027
Iteration 983/1000 | Loss: 0.00002026
Iteration 984/1000 | Loss: 0.00002026
Iteration 985/1000 | Loss: 0.00002026
Iteration 986/1000 | Loss: 0.00002025
Iteration 987/1000 | Loss: 0.00002025
Iteration 988/1000 | Loss: 0.00002025
Iteration 989/1000 | Loss: 0.00002025
Iteration 990/1000 | Loss: 0.00002025
Iteration 991/1000 | Loss: 0.00002025
Iteration 992/1000 | Loss: 0.00002022
Iteration 993/1000 | Loss: 0.00002022
Iteration 994/1000 | Loss: 0.00002022
Iteration 995/1000 | Loss: 0.00002022
Iteration 996/1000 | Loss: 0.00002021
Iteration 997/1000 | Loss: 0.00002021
Iteration 998/1000 | Loss: 0.00002021
Iteration 999/1000 | Loss: 0.00002020
Iteration 1000/1000 | Loss: 0.00002019

Optimization complete. Final v2v error: 3.844616651535034 mm

Highest mean error: 4.8219451904296875 mm for frame 75

Lowest mean error: 3.764981985092163 mm for frame 113

Saving results

Total time: 1363.5841090679169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402121
Iteration 2/25 | Loss: 0.00093072
Iteration 3/25 | Loss: 0.00075722
Iteration 4/25 | Loss: 0.00072274
Iteration 5/25 | Loss: 0.00071459
Iteration 6/25 | Loss: 0.00071146
Iteration 7/25 | Loss: 0.00071068
Iteration 8/25 | Loss: 0.00071066
Iteration 9/25 | Loss: 0.00071066
Iteration 10/25 | Loss: 0.00071066
Iteration 11/25 | Loss: 0.00071066
Iteration 12/25 | Loss: 0.00071066
Iteration 13/25 | Loss: 0.00071066
Iteration 14/25 | Loss: 0.00071066
Iteration 15/25 | Loss: 0.00071066
Iteration 16/25 | Loss: 0.00071066
Iteration 17/25 | Loss: 0.00071066
Iteration 18/25 | Loss: 0.00071066
Iteration 19/25 | Loss: 0.00071066
Iteration 20/25 | Loss: 0.00071066
Iteration 21/25 | Loss: 0.00071066
Iteration 22/25 | Loss: 0.00071066
Iteration 23/25 | Loss: 0.00071066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007106603006832302, 0.0007106603006832302, 0.0007106603006832302, 0.0007106603006832302, 0.0007106603006832302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007106603006832302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52870476
Iteration 2/25 | Loss: 0.00081308
Iteration 3/25 | Loss: 0.00081307
Iteration 4/25 | Loss: 0.00081307
Iteration 5/25 | Loss: 0.00081307
Iteration 6/25 | Loss: 0.00081307
Iteration 7/25 | Loss: 0.00081307
Iteration 8/25 | Loss: 0.00081307
Iteration 9/25 | Loss: 0.00081307
Iteration 10/25 | Loss: 0.00081307
Iteration 11/25 | Loss: 0.00081307
Iteration 12/25 | Loss: 0.00081307
Iteration 13/25 | Loss: 0.00081307
Iteration 14/25 | Loss: 0.00081307
Iteration 15/25 | Loss: 0.00081307
Iteration 16/25 | Loss: 0.00081307
Iteration 17/25 | Loss: 0.00081307
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008130697533488274, 0.0008130697533488274, 0.0008130697533488274, 0.0008130697533488274, 0.0008130697533488274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008130697533488274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081307
Iteration 2/1000 | Loss: 0.00003173
Iteration 3/1000 | Loss: 0.00002122
Iteration 4/1000 | Loss: 0.00001899
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001700
Iteration 7/1000 | Loss: 0.00001655
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001599
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001550
Iteration 13/1000 | Loss: 0.00001541
Iteration 14/1000 | Loss: 0.00001534
Iteration 15/1000 | Loss: 0.00001534
Iteration 16/1000 | Loss: 0.00001532
Iteration 17/1000 | Loss: 0.00001527
Iteration 18/1000 | Loss: 0.00001526
Iteration 19/1000 | Loss: 0.00001526
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001525
Iteration 22/1000 | Loss: 0.00001522
Iteration 23/1000 | Loss: 0.00001521
Iteration 24/1000 | Loss: 0.00001521
Iteration 25/1000 | Loss: 0.00001520
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001520
Iteration 29/1000 | Loss: 0.00001520
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001519
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001518
Iteration 35/1000 | Loss: 0.00001518
Iteration 36/1000 | Loss: 0.00001518
Iteration 37/1000 | Loss: 0.00001518
Iteration 38/1000 | Loss: 0.00001517
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001517
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001516
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001512
Iteration 68/1000 | Loss: 0.00001512
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001511
Iteration 73/1000 | Loss: 0.00001511
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001509
Iteration 77/1000 | Loss: 0.00001508
Iteration 78/1000 | Loss: 0.00001508
Iteration 79/1000 | Loss: 0.00001507
Iteration 80/1000 | Loss: 0.00001507
Iteration 81/1000 | Loss: 0.00001507
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001506
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001505
Iteration 87/1000 | Loss: 0.00001505
Iteration 88/1000 | Loss: 0.00001505
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001505
Iteration 92/1000 | Loss: 0.00001505
Iteration 93/1000 | Loss: 0.00001505
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001504
Iteration 98/1000 | Loss: 0.00001504
Iteration 99/1000 | Loss: 0.00001504
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001503
Iteration 102/1000 | Loss: 0.00001503
Iteration 103/1000 | Loss: 0.00001503
Iteration 104/1000 | Loss: 0.00001503
Iteration 105/1000 | Loss: 0.00001503
Iteration 106/1000 | Loss: 0.00001503
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001503
Iteration 111/1000 | Loss: 0.00001503
Iteration 112/1000 | Loss: 0.00001503
Iteration 113/1000 | Loss: 0.00001503
Iteration 114/1000 | Loss: 0.00001503
Iteration 115/1000 | Loss: 0.00001503
Iteration 116/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.5033306226541754e-05, 1.5033306226541754e-05, 1.5033306226541754e-05, 1.5033306226541754e-05, 1.5033306226541754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5033306226541754e-05

Optimization complete. Final v2v error: 3.243079662322998 mm

Highest mean error: 3.6514320373535156 mm for frame 100

Lowest mean error: 2.9370040893554688 mm for frame 19

Saving results

Total time: 37.229217767715454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909948
Iteration 2/25 | Loss: 0.00125586
Iteration 3/25 | Loss: 0.00113675
Iteration 4/25 | Loss: 0.00112842
Iteration 5/25 | Loss: 0.00112545
Iteration 6/25 | Loss: 0.00112512
Iteration 7/25 | Loss: 0.00112512
Iteration 8/25 | Loss: 0.00112512
Iteration 9/25 | Loss: 0.00112512
Iteration 10/25 | Loss: 0.00112512
Iteration 11/25 | Loss: 0.00112512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011251179967075586, 0.0011251179967075586, 0.0011251179967075586, 0.0011251179967075586, 0.0011251179967075586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011251179967075586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41025758
Iteration 2/25 | Loss: 0.00058206
Iteration 3/25 | Loss: 0.00058206
Iteration 4/25 | Loss: 0.00058206
Iteration 5/25 | Loss: 0.00058206
Iteration 6/25 | Loss: 0.00058205
Iteration 7/25 | Loss: 0.00058205
Iteration 8/25 | Loss: 0.00058205
Iteration 9/25 | Loss: 0.00058205
Iteration 10/25 | Loss: 0.00058205
Iteration 11/25 | Loss: 0.00058205
Iteration 12/25 | Loss: 0.00058205
Iteration 13/25 | Loss: 0.00058205
Iteration 14/25 | Loss: 0.00058205
Iteration 15/25 | Loss: 0.00058205
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005820542573928833, 0.0005820542573928833, 0.0005820542573928833, 0.0005820542573928833, 0.0005820542573928833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005820542573928833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058205
Iteration 2/1000 | Loss: 0.00003295
Iteration 3/1000 | Loss: 0.00001944
Iteration 4/1000 | Loss: 0.00001744
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00001565
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001518
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001507
Iteration 11/1000 | Loss: 0.00001501
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001482
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001476
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001476
Iteration 20/1000 | Loss: 0.00001476
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001469
Iteration 26/1000 | Loss: 0.00001469
Iteration 27/1000 | Loss: 0.00001469
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001468
Iteration 30/1000 | Loss: 0.00001468
Iteration 31/1000 | Loss: 0.00001468
Iteration 32/1000 | Loss: 0.00001468
Iteration 33/1000 | Loss: 0.00001468
Iteration 34/1000 | Loss: 0.00001467
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001462
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001461
Iteration 43/1000 | Loss: 0.00001461
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001460
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001459
Iteration 51/1000 | Loss: 0.00001459
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001458
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001455
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001455
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001454
Iteration 84/1000 | Loss: 0.00001454
Iteration 85/1000 | Loss: 0.00001454
Iteration 86/1000 | Loss: 0.00001454
Iteration 87/1000 | Loss: 0.00001453
Iteration 88/1000 | Loss: 0.00001453
Iteration 89/1000 | Loss: 0.00001453
Iteration 90/1000 | Loss: 0.00001453
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001452
Iteration 94/1000 | Loss: 0.00001452
Iteration 95/1000 | Loss: 0.00001452
Iteration 96/1000 | Loss: 0.00001452
Iteration 97/1000 | Loss: 0.00001452
Iteration 98/1000 | Loss: 0.00001452
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001452
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001452
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001451
Iteration 109/1000 | Loss: 0.00001451
Iteration 110/1000 | Loss: 0.00001451
Iteration 111/1000 | Loss: 0.00001451
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001451
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001451
Iteration 122/1000 | Loss: 0.00001451
Iteration 123/1000 | Loss: 0.00001451
Iteration 124/1000 | Loss: 0.00001451
Iteration 125/1000 | Loss: 0.00001451
Iteration 126/1000 | Loss: 0.00001451
Iteration 127/1000 | Loss: 0.00001451
Iteration 128/1000 | Loss: 0.00001451
Iteration 129/1000 | Loss: 0.00001451
Iteration 130/1000 | Loss: 0.00001450
Iteration 131/1000 | Loss: 0.00001450
Iteration 132/1000 | Loss: 0.00001450
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001450
Iteration 135/1000 | Loss: 0.00001450
Iteration 136/1000 | Loss: 0.00001450
Iteration 137/1000 | Loss: 0.00001450
Iteration 138/1000 | Loss: 0.00001450
Iteration 139/1000 | Loss: 0.00001450
Iteration 140/1000 | Loss: 0.00001450
Iteration 141/1000 | Loss: 0.00001450
Iteration 142/1000 | Loss: 0.00001450
Iteration 143/1000 | Loss: 0.00001450
Iteration 144/1000 | Loss: 0.00001450
Iteration 145/1000 | Loss: 0.00001450
Iteration 146/1000 | Loss: 0.00001450
Iteration 147/1000 | Loss: 0.00001450
Iteration 148/1000 | Loss: 0.00001450
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001450
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001450
Iteration 153/1000 | Loss: 0.00001450
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001450
Iteration 158/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.4503540114674252e-05, 1.4503540114674252e-05, 1.4503540114674252e-05, 1.4503540114674252e-05, 1.4503540114674252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4503540114674252e-05

Optimization complete. Final v2v error: 3.2663183212280273 mm

Highest mean error: 3.4349286556243896 mm for frame 49

Lowest mean error: 2.9622414112091064 mm for frame 129

Saving results

Total time: 29.702152967453003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581757
Iteration 2/25 | Loss: 0.00145851
Iteration 3/25 | Loss: 0.00132378
Iteration 4/25 | Loss: 0.00130688
Iteration 5/25 | Loss: 0.00130388
Iteration 6/25 | Loss: 0.00130388
Iteration 7/25 | Loss: 0.00130388
Iteration 8/25 | Loss: 0.00130388
Iteration 9/25 | Loss: 0.00130388
Iteration 10/25 | Loss: 0.00130388
Iteration 11/25 | Loss: 0.00130388
Iteration 12/25 | Loss: 0.00130388
Iteration 13/25 | Loss: 0.00130388
Iteration 14/25 | Loss: 0.00130388
Iteration 15/25 | Loss: 0.00130388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013038786128163338, 0.0013038786128163338, 0.0013038786128163338, 0.0013038786128163338, 0.0013038786128163338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013038786128163338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34475386
Iteration 2/25 | Loss: 0.00064778
Iteration 3/25 | Loss: 0.00064778
Iteration 4/25 | Loss: 0.00064778
Iteration 5/25 | Loss: 0.00064777
Iteration 6/25 | Loss: 0.00064777
Iteration 7/25 | Loss: 0.00064777
Iteration 8/25 | Loss: 0.00064777
Iteration 9/25 | Loss: 0.00064777
Iteration 10/25 | Loss: 0.00064777
Iteration 11/25 | Loss: 0.00064777
Iteration 12/25 | Loss: 0.00064777
Iteration 13/25 | Loss: 0.00064777
Iteration 14/25 | Loss: 0.00064777
Iteration 15/25 | Loss: 0.00064777
Iteration 16/25 | Loss: 0.00064777
Iteration 17/25 | Loss: 0.00064777
Iteration 18/25 | Loss: 0.00064777
Iteration 19/25 | Loss: 0.00064777
Iteration 20/25 | Loss: 0.00064777
Iteration 21/25 | Loss: 0.00064777
Iteration 22/25 | Loss: 0.00064777
Iteration 23/25 | Loss: 0.00064777
Iteration 24/25 | Loss: 0.00064777
Iteration 25/25 | Loss: 0.00064777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064777
Iteration 2/1000 | Loss: 0.00006089
Iteration 3/1000 | Loss: 0.00004324
Iteration 4/1000 | Loss: 0.00004056
Iteration 5/1000 | Loss: 0.00003891
Iteration 6/1000 | Loss: 0.00003781
Iteration 7/1000 | Loss: 0.00003681
Iteration 8/1000 | Loss: 0.00003611
Iteration 9/1000 | Loss: 0.00003579
Iteration 10/1000 | Loss: 0.00003565
Iteration 11/1000 | Loss: 0.00003545
Iteration 12/1000 | Loss: 0.00003539
Iteration 13/1000 | Loss: 0.00003527
Iteration 14/1000 | Loss: 0.00003525
Iteration 15/1000 | Loss: 0.00003525
Iteration 16/1000 | Loss: 0.00003523
Iteration 17/1000 | Loss: 0.00003522
Iteration 18/1000 | Loss: 0.00003518
Iteration 19/1000 | Loss: 0.00003517
Iteration 20/1000 | Loss: 0.00003517
Iteration 21/1000 | Loss: 0.00003516
Iteration 22/1000 | Loss: 0.00003514
Iteration 23/1000 | Loss: 0.00003514
Iteration 24/1000 | Loss: 0.00003513
Iteration 25/1000 | Loss: 0.00003508
Iteration 26/1000 | Loss: 0.00003506
Iteration 27/1000 | Loss: 0.00003506
Iteration 28/1000 | Loss: 0.00003502
Iteration 29/1000 | Loss: 0.00003502
Iteration 30/1000 | Loss: 0.00003501
Iteration 31/1000 | Loss: 0.00003501
Iteration 32/1000 | Loss: 0.00003500
Iteration 33/1000 | Loss: 0.00003499
Iteration 34/1000 | Loss: 0.00003499
Iteration 35/1000 | Loss: 0.00003499
Iteration 36/1000 | Loss: 0.00003499
Iteration 37/1000 | Loss: 0.00003499
Iteration 38/1000 | Loss: 0.00003499
Iteration 39/1000 | Loss: 0.00003499
Iteration 40/1000 | Loss: 0.00003498
Iteration 41/1000 | Loss: 0.00003498
Iteration 42/1000 | Loss: 0.00003498
Iteration 43/1000 | Loss: 0.00003498
Iteration 44/1000 | Loss: 0.00003497
Iteration 45/1000 | Loss: 0.00003496
Iteration 46/1000 | Loss: 0.00003496
Iteration 47/1000 | Loss: 0.00003495
Iteration 48/1000 | Loss: 0.00003495
Iteration 49/1000 | Loss: 0.00003493
Iteration 50/1000 | Loss: 0.00003492
Iteration 51/1000 | Loss: 0.00003492
Iteration 52/1000 | Loss: 0.00003491
Iteration 53/1000 | Loss: 0.00003490
Iteration 54/1000 | Loss: 0.00003490
Iteration 55/1000 | Loss: 0.00003490
Iteration 56/1000 | Loss: 0.00003489
Iteration 57/1000 | Loss: 0.00003489
Iteration 58/1000 | Loss: 0.00003489
Iteration 59/1000 | Loss: 0.00003489
Iteration 60/1000 | Loss: 0.00003488
Iteration 61/1000 | Loss: 0.00003488
Iteration 62/1000 | Loss: 0.00003488
Iteration 63/1000 | Loss: 0.00003488
Iteration 64/1000 | Loss: 0.00003488
Iteration 65/1000 | Loss: 0.00003488
Iteration 66/1000 | Loss: 0.00003488
Iteration 67/1000 | Loss: 0.00003488
Iteration 68/1000 | Loss: 0.00003487
Iteration 69/1000 | Loss: 0.00003487
Iteration 70/1000 | Loss: 0.00003487
Iteration 71/1000 | Loss: 0.00003487
Iteration 72/1000 | Loss: 0.00003487
Iteration 73/1000 | Loss: 0.00003487
Iteration 74/1000 | Loss: 0.00003487
Iteration 75/1000 | Loss: 0.00003487
Iteration 76/1000 | Loss: 0.00003487
Iteration 77/1000 | Loss: 0.00003487
Iteration 78/1000 | Loss: 0.00003487
Iteration 79/1000 | Loss: 0.00003487
Iteration 80/1000 | Loss: 0.00003487
Iteration 81/1000 | Loss: 0.00003487
Iteration 82/1000 | Loss: 0.00003487
Iteration 83/1000 | Loss: 0.00003487
Iteration 84/1000 | Loss: 0.00003487
Iteration 85/1000 | Loss: 0.00003487
Iteration 86/1000 | Loss: 0.00003487
Iteration 87/1000 | Loss: 0.00003487
Iteration 88/1000 | Loss: 0.00003487
Iteration 89/1000 | Loss: 0.00003487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [3.486593050183728e-05, 3.486593050183728e-05, 3.486593050183728e-05, 3.486593050183728e-05, 3.486593050183728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.486593050183728e-05

Optimization complete. Final v2v error: 4.797159194946289 mm

Highest mean error: 5.300839900970459 mm for frame 181

Lowest mean error: 4.415122032165527 mm for frame 121

Saving results

Total time: 37.57716751098633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491257
Iteration 2/25 | Loss: 0.00148930
Iteration 3/25 | Loss: 0.00124130
Iteration 4/25 | Loss: 0.00121504
Iteration 5/25 | Loss: 0.00121217
Iteration 6/25 | Loss: 0.00121155
Iteration 7/25 | Loss: 0.00121155
Iteration 8/25 | Loss: 0.00121155
Iteration 9/25 | Loss: 0.00121155
Iteration 10/25 | Loss: 0.00121155
Iteration 11/25 | Loss: 0.00121155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012115509016439319, 0.0012115509016439319, 0.0012115509016439319, 0.0012115509016439319, 0.0012115509016439319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012115509016439319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33453524
Iteration 2/25 | Loss: 0.00057462
Iteration 3/25 | Loss: 0.00057461
Iteration 4/25 | Loss: 0.00057460
Iteration 5/25 | Loss: 0.00057460
Iteration 6/25 | Loss: 0.00057460
Iteration 7/25 | Loss: 0.00057460
Iteration 8/25 | Loss: 0.00057460
Iteration 9/25 | Loss: 0.00057460
Iteration 10/25 | Loss: 0.00057460
Iteration 11/25 | Loss: 0.00057460
Iteration 12/25 | Loss: 0.00057460
Iteration 13/25 | Loss: 0.00057460
Iteration 14/25 | Loss: 0.00057460
Iteration 15/25 | Loss: 0.00057460
Iteration 16/25 | Loss: 0.00057460
Iteration 17/25 | Loss: 0.00057460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000574602629058063, 0.000574602629058063, 0.000574602629058063, 0.000574602629058063, 0.000574602629058063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000574602629058063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057460
Iteration 2/1000 | Loss: 0.00005617
Iteration 3/1000 | Loss: 0.00002389
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00002008
Iteration 6/1000 | Loss: 0.00001947
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001870
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001821
Iteration 12/1000 | Loss: 0.00001816
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001812
Iteration 15/1000 | Loss: 0.00001811
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001808
Iteration 18/1000 | Loss: 0.00001807
Iteration 19/1000 | Loss: 0.00001803
Iteration 20/1000 | Loss: 0.00001800
Iteration 21/1000 | Loss: 0.00001799
Iteration 22/1000 | Loss: 0.00001798
Iteration 23/1000 | Loss: 0.00001798
Iteration 24/1000 | Loss: 0.00001797
Iteration 25/1000 | Loss: 0.00001797
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001796
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00001795
Iteration 30/1000 | Loss: 0.00001795
Iteration 31/1000 | Loss: 0.00001794
Iteration 32/1000 | Loss: 0.00001794
Iteration 33/1000 | Loss: 0.00001793
Iteration 34/1000 | Loss: 0.00001793
Iteration 35/1000 | Loss: 0.00001793
Iteration 36/1000 | Loss: 0.00001792
Iteration 37/1000 | Loss: 0.00001792
Iteration 38/1000 | Loss: 0.00001792
Iteration 39/1000 | Loss: 0.00001792
Iteration 40/1000 | Loss: 0.00001791
Iteration 41/1000 | Loss: 0.00001791
Iteration 42/1000 | Loss: 0.00001789
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001786
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001779
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001777
Iteration 67/1000 | Loss: 0.00001777
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001777
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001776
Iteration 73/1000 | Loss: 0.00001776
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001775
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001771
Iteration 113/1000 | Loss: 0.00001771
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001770
Iteration 116/1000 | Loss: 0.00001770
Iteration 117/1000 | Loss: 0.00001769
Iteration 118/1000 | Loss: 0.00001769
Iteration 119/1000 | Loss: 0.00001769
Iteration 120/1000 | Loss: 0.00001769
Iteration 121/1000 | Loss: 0.00001769
Iteration 122/1000 | Loss: 0.00001769
Iteration 123/1000 | Loss: 0.00001768
Iteration 124/1000 | Loss: 0.00001768
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001767
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001766
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001766
Iteration 132/1000 | Loss: 0.00001766
Iteration 133/1000 | Loss: 0.00001766
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001765
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001765
Iteration 139/1000 | Loss: 0.00001765
Iteration 140/1000 | Loss: 0.00001765
Iteration 141/1000 | Loss: 0.00001765
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001763
Iteration 148/1000 | Loss: 0.00001763
Iteration 149/1000 | Loss: 0.00001763
Iteration 150/1000 | Loss: 0.00001763
Iteration 151/1000 | Loss: 0.00001763
Iteration 152/1000 | Loss: 0.00001763
Iteration 153/1000 | Loss: 0.00001763
Iteration 154/1000 | Loss: 0.00001763
Iteration 155/1000 | Loss: 0.00001763
Iteration 156/1000 | Loss: 0.00001763
Iteration 157/1000 | Loss: 0.00001763
Iteration 158/1000 | Loss: 0.00001763
Iteration 159/1000 | Loss: 0.00001763
Iteration 160/1000 | Loss: 0.00001763
Iteration 161/1000 | Loss: 0.00001763
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.7631195078138262e-05, 1.7631195078138262e-05, 1.7631195078138262e-05, 1.7631195078138262e-05, 1.7631195078138262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7631195078138262e-05

Optimization complete. Final v2v error: 3.539215087890625 mm

Highest mean error: 3.8632123470306396 mm for frame 186

Lowest mean error: 2.9892122745513916 mm for frame 238

Saving results

Total time: 41.06384778022766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835648
Iteration 2/25 | Loss: 0.00161551
Iteration 3/25 | Loss: 0.00132492
Iteration 4/25 | Loss: 0.00129429
Iteration 5/25 | Loss: 0.00128437
Iteration 6/25 | Loss: 0.00128247
Iteration 7/25 | Loss: 0.00128229
Iteration 8/25 | Loss: 0.00128229
Iteration 9/25 | Loss: 0.00128229
Iteration 10/25 | Loss: 0.00128229
Iteration 11/25 | Loss: 0.00128229
Iteration 12/25 | Loss: 0.00128229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012822853168472648, 0.0012822853168472648, 0.0012822853168472648, 0.0012822853168472648, 0.0012822853168472648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012822853168472648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32932878
Iteration 2/25 | Loss: 0.00052546
Iteration 3/25 | Loss: 0.00052543
Iteration 4/25 | Loss: 0.00052543
Iteration 5/25 | Loss: 0.00052542
Iteration 6/25 | Loss: 0.00052542
Iteration 7/25 | Loss: 0.00052542
Iteration 8/25 | Loss: 0.00052542
Iteration 9/25 | Loss: 0.00052542
Iteration 10/25 | Loss: 0.00052542
Iteration 11/25 | Loss: 0.00052542
Iteration 12/25 | Loss: 0.00052542
Iteration 13/25 | Loss: 0.00052542
Iteration 14/25 | Loss: 0.00052542
Iteration 15/25 | Loss: 0.00052542
Iteration 16/25 | Loss: 0.00052542
Iteration 17/25 | Loss: 0.00052542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000525423267390579, 0.000525423267390579, 0.000525423267390579, 0.000525423267390579, 0.000525423267390579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000525423267390579

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052542
Iteration 2/1000 | Loss: 0.00007308
Iteration 3/1000 | Loss: 0.00005163
Iteration 4/1000 | Loss: 0.00004510
Iteration 5/1000 | Loss: 0.00004261
Iteration 6/1000 | Loss: 0.00004158
Iteration 7/1000 | Loss: 0.00004064
Iteration 8/1000 | Loss: 0.00003974
Iteration 9/1000 | Loss: 0.00003922
Iteration 10/1000 | Loss: 0.00003884
Iteration 11/1000 | Loss: 0.00003846
Iteration 12/1000 | Loss: 0.00003809
Iteration 13/1000 | Loss: 0.00003781
Iteration 14/1000 | Loss: 0.00003756
Iteration 15/1000 | Loss: 0.00003739
Iteration 16/1000 | Loss: 0.00003723
Iteration 17/1000 | Loss: 0.00003710
Iteration 18/1000 | Loss: 0.00003708
Iteration 19/1000 | Loss: 0.00003707
Iteration 20/1000 | Loss: 0.00003706
Iteration 21/1000 | Loss: 0.00003705
Iteration 22/1000 | Loss: 0.00003705
Iteration 23/1000 | Loss: 0.00003705
Iteration 24/1000 | Loss: 0.00003705
Iteration 25/1000 | Loss: 0.00003704
Iteration 26/1000 | Loss: 0.00003703
Iteration 27/1000 | Loss: 0.00003701
Iteration 28/1000 | Loss: 0.00003698
Iteration 29/1000 | Loss: 0.00003695
Iteration 30/1000 | Loss: 0.00003692
Iteration 31/1000 | Loss: 0.00003691
Iteration 32/1000 | Loss: 0.00003691
Iteration 33/1000 | Loss: 0.00003691
Iteration 34/1000 | Loss: 0.00003690
Iteration 35/1000 | Loss: 0.00003690
Iteration 36/1000 | Loss: 0.00003690
Iteration 37/1000 | Loss: 0.00003690
Iteration 38/1000 | Loss: 0.00003690
Iteration 39/1000 | Loss: 0.00003690
Iteration 40/1000 | Loss: 0.00003689
Iteration 41/1000 | Loss: 0.00003688
Iteration 42/1000 | Loss: 0.00003688
Iteration 43/1000 | Loss: 0.00003687
Iteration 44/1000 | Loss: 0.00003687
Iteration 45/1000 | Loss: 0.00003687
Iteration 46/1000 | Loss: 0.00003687
Iteration 47/1000 | Loss: 0.00003687
Iteration 48/1000 | Loss: 0.00003687
Iteration 49/1000 | Loss: 0.00003687
Iteration 50/1000 | Loss: 0.00003687
Iteration 51/1000 | Loss: 0.00003687
Iteration 52/1000 | Loss: 0.00003687
Iteration 53/1000 | Loss: 0.00003687
Iteration 54/1000 | Loss: 0.00003686
Iteration 55/1000 | Loss: 0.00003686
Iteration 56/1000 | Loss: 0.00003686
Iteration 57/1000 | Loss: 0.00003686
Iteration 58/1000 | Loss: 0.00003686
Iteration 59/1000 | Loss: 0.00003685
Iteration 60/1000 | Loss: 0.00003685
Iteration 61/1000 | Loss: 0.00003685
Iteration 62/1000 | Loss: 0.00003685
Iteration 63/1000 | Loss: 0.00003685
Iteration 64/1000 | Loss: 0.00003685
Iteration 65/1000 | Loss: 0.00003685
Iteration 66/1000 | Loss: 0.00003685
Iteration 67/1000 | Loss: 0.00003685
Iteration 68/1000 | Loss: 0.00003684
Iteration 69/1000 | Loss: 0.00003684
Iteration 70/1000 | Loss: 0.00003684
Iteration 71/1000 | Loss: 0.00003684
Iteration 72/1000 | Loss: 0.00003684
Iteration 73/1000 | Loss: 0.00003684
Iteration 74/1000 | Loss: 0.00003684
Iteration 75/1000 | Loss: 0.00003684
Iteration 76/1000 | Loss: 0.00003684
Iteration 77/1000 | Loss: 0.00003684
Iteration 78/1000 | Loss: 0.00003684
Iteration 79/1000 | Loss: 0.00003683
Iteration 80/1000 | Loss: 0.00003683
Iteration 81/1000 | Loss: 0.00003683
Iteration 82/1000 | Loss: 0.00003683
Iteration 83/1000 | Loss: 0.00003683
Iteration 84/1000 | Loss: 0.00003683
Iteration 85/1000 | Loss: 0.00003683
Iteration 86/1000 | Loss: 0.00003683
Iteration 87/1000 | Loss: 0.00003683
Iteration 88/1000 | Loss: 0.00003683
Iteration 89/1000 | Loss: 0.00003683
Iteration 90/1000 | Loss: 0.00003683
Iteration 91/1000 | Loss: 0.00003682
Iteration 92/1000 | Loss: 0.00003682
Iteration 93/1000 | Loss: 0.00003682
Iteration 94/1000 | Loss: 0.00003682
Iteration 95/1000 | Loss: 0.00003682
Iteration 96/1000 | Loss: 0.00003682
Iteration 97/1000 | Loss: 0.00003682
Iteration 98/1000 | Loss: 0.00003682
Iteration 99/1000 | Loss: 0.00003682
Iteration 100/1000 | Loss: 0.00003682
Iteration 101/1000 | Loss: 0.00003682
Iteration 102/1000 | Loss: 0.00003682
Iteration 103/1000 | Loss: 0.00003682
Iteration 104/1000 | Loss: 0.00003682
Iteration 105/1000 | Loss: 0.00003681
Iteration 106/1000 | Loss: 0.00003681
Iteration 107/1000 | Loss: 0.00003680
Iteration 108/1000 | Loss: 0.00003680
Iteration 109/1000 | Loss: 0.00003680
Iteration 110/1000 | Loss: 0.00003679
Iteration 111/1000 | Loss: 0.00003679
Iteration 112/1000 | Loss: 0.00003679
Iteration 113/1000 | Loss: 0.00003679
Iteration 114/1000 | Loss: 0.00003679
Iteration 115/1000 | Loss: 0.00003679
Iteration 116/1000 | Loss: 0.00003679
Iteration 117/1000 | Loss: 0.00003678
Iteration 118/1000 | Loss: 0.00003678
Iteration 119/1000 | Loss: 0.00003678
Iteration 120/1000 | Loss: 0.00003678
Iteration 121/1000 | Loss: 0.00003678
Iteration 122/1000 | Loss: 0.00003678
Iteration 123/1000 | Loss: 0.00003678
Iteration 124/1000 | Loss: 0.00003678
Iteration 125/1000 | Loss: 0.00003677
Iteration 126/1000 | Loss: 0.00003677
Iteration 127/1000 | Loss: 0.00003677
Iteration 128/1000 | Loss: 0.00003677
Iteration 129/1000 | Loss: 0.00003677
Iteration 130/1000 | Loss: 0.00003677
Iteration 131/1000 | Loss: 0.00003677
Iteration 132/1000 | Loss: 0.00003677
Iteration 133/1000 | Loss: 0.00003677
Iteration 134/1000 | Loss: 0.00003677
Iteration 135/1000 | Loss: 0.00003677
Iteration 136/1000 | Loss: 0.00003677
Iteration 137/1000 | Loss: 0.00003677
Iteration 138/1000 | Loss: 0.00003676
Iteration 139/1000 | Loss: 0.00003676
Iteration 140/1000 | Loss: 0.00003676
Iteration 141/1000 | Loss: 0.00003676
Iteration 142/1000 | Loss: 0.00003676
Iteration 143/1000 | Loss: 0.00003676
Iteration 144/1000 | Loss: 0.00003676
Iteration 145/1000 | Loss: 0.00003676
Iteration 146/1000 | Loss: 0.00003676
Iteration 147/1000 | Loss: 0.00003676
Iteration 148/1000 | Loss: 0.00003675
Iteration 149/1000 | Loss: 0.00003675
Iteration 150/1000 | Loss: 0.00003675
Iteration 151/1000 | Loss: 0.00003675
Iteration 152/1000 | Loss: 0.00003675
Iteration 153/1000 | Loss: 0.00003675
Iteration 154/1000 | Loss: 0.00003675
Iteration 155/1000 | Loss: 0.00003675
Iteration 156/1000 | Loss: 0.00003675
Iteration 157/1000 | Loss: 0.00003675
Iteration 158/1000 | Loss: 0.00003675
Iteration 159/1000 | Loss: 0.00003675
Iteration 160/1000 | Loss: 0.00003675
Iteration 161/1000 | Loss: 0.00003674
Iteration 162/1000 | Loss: 0.00003674
Iteration 163/1000 | Loss: 0.00003674
Iteration 164/1000 | Loss: 0.00003674
Iteration 165/1000 | Loss: 0.00003674
Iteration 166/1000 | Loss: 0.00003674
Iteration 167/1000 | Loss: 0.00003674
Iteration 168/1000 | Loss: 0.00003674
Iteration 169/1000 | Loss: 0.00003674
Iteration 170/1000 | Loss: 0.00003674
Iteration 171/1000 | Loss: 0.00003674
Iteration 172/1000 | Loss: 0.00003674
Iteration 173/1000 | Loss: 0.00003674
Iteration 174/1000 | Loss: 0.00003674
Iteration 175/1000 | Loss: 0.00003674
Iteration 176/1000 | Loss: 0.00003674
Iteration 177/1000 | Loss: 0.00003674
Iteration 178/1000 | Loss: 0.00003674
Iteration 179/1000 | Loss: 0.00003674
Iteration 180/1000 | Loss: 0.00003674
Iteration 181/1000 | Loss: 0.00003674
Iteration 182/1000 | Loss: 0.00003674
Iteration 183/1000 | Loss: 0.00003674
Iteration 184/1000 | Loss: 0.00003674
Iteration 185/1000 | Loss: 0.00003674
Iteration 186/1000 | Loss: 0.00003674
Iteration 187/1000 | Loss: 0.00003674
Iteration 188/1000 | Loss: 0.00003674
Iteration 189/1000 | Loss: 0.00003674
Iteration 190/1000 | Loss: 0.00003674
Iteration 191/1000 | Loss: 0.00003674
Iteration 192/1000 | Loss: 0.00003674
Iteration 193/1000 | Loss: 0.00003674
Iteration 194/1000 | Loss: 0.00003674
Iteration 195/1000 | Loss: 0.00003674
Iteration 196/1000 | Loss: 0.00003674
Iteration 197/1000 | Loss: 0.00003674
Iteration 198/1000 | Loss: 0.00003674
Iteration 199/1000 | Loss: 0.00003674
Iteration 200/1000 | Loss: 0.00003674
Iteration 201/1000 | Loss: 0.00003674
Iteration 202/1000 | Loss: 0.00003674
Iteration 203/1000 | Loss: 0.00003674
Iteration 204/1000 | Loss: 0.00003674
Iteration 205/1000 | Loss: 0.00003674
Iteration 206/1000 | Loss: 0.00003674
Iteration 207/1000 | Loss: 0.00003674
Iteration 208/1000 | Loss: 0.00003674
Iteration 209/1000 | Loss: 0.00003674
Iteration 210/1000 | Loss: 0.00003674
Iteration 211/1000 | Loss: 0.00003674
Iteration 212/1000 | Loss: 0.00003674
Iteration 213/1000 | Loss: 0.00003674
Iteration 214/1000 | Loss: 0.00003674
Iteration 215/1000 | Loss: 0.00003674
Iteration 216/1000 | Loss: 0.00003674
Iteration 217/1000 | Loss: 0.00003674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [3.673885657917708e-05, 3.673885657917708e-05, 3.673885657917708e-05, 3.673885657917708e-05, 3.673885657917708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.673885657917708e-05

Optimization complete. Final v2v error: 4.790882587432861 mm

Highest mean error: 5.3940558433532715 mm for frame 55

Lowest mean error: 3.506847858428955 mm for frame 1

Saving results

Total time: 46.23032903671265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885194
Iteration 2/25 | Loss: 0.00156340
Iteration 3/25 | Loss: 0.00129432
Iteration 4/25 | Loss: 0.00125489
Iteration 5/25 | Loss: 0.00124983
Iteration 6/25 | Loss: 0.00124756
Iteration 7/25 | Loss: 0.00123831
Iteration 8/25 | Loss: 0.00123688
Iteration 9/25 | Loss: 0.00123117
Iteration 10/25 | Loss: 0.00123063
Iteration 11/25 | Loss: 0.00122467
Iteration 12/25 | Loss: 0.00122334
Iteration 13/25 | Loss: 0.00122227
Iteration 14/25 | Loss: 0.00122148
Iteration 15/25 | Loss: 0.00122110
Iteration 16/25 | Loss: 0.00122436
Iteration 17/25 | Loss: 0.00122446
Iteration 18/25 | Loss: 0.00122092
Iteration 19/25 | Loss: 0.00121823
Iteration 20/25 | Loss: 0.00121800
Iteration 21/25 | Loss: 0.00121795
Iteration 22/25 | Loss: 0.00121795
Iteration 23/25 | Loss: 0.00121795
Iteration 24/25 | Loss: 0.00121795
Iteration 25/25 | Loss: 0.00121795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37402880
Iteration 2/25 | Loss: 0.00076983
Iteration 3/25 | Loss: 0.00076982
Iteration 4/25 | Loss: 0.00076982
Iteration 5/25 | Loss: 0.00076981
Iteration 6/25 | Loss: 0.00076981
Iteration 7/25 | Loss: 0.00076981
Iteration 8/25 | Loss: 0.00076981
Iteration 9/25 | Loss: 0.00076981
Iteration 10/25 | Loss: 0.00076981
Iteration 11/25 | Loss: 0.00076981
Iteration 12/25 | Loss: 0.00076981
Iteration 13/25 | Loss: 0.00076981
Iteration 14/25 | Loss: 0.00076981
Iteration 15/25 | Loss: 0.00076981
Iteration 16/25 | Loss: 0.00076981
Iteration 17/25 | Loss: 0.00076981
Iteration 18/25 | Loss: 0.00076981
Iteration 19/25 | Loss: 0.00076981
Iteration 20/25 | Loss: 0.00076981
Iteration 21/25 | Loss: 0.00076981
Iteration 22/25 | Loss: 0.00076981
Iteration 23/25 | Loss: 0.00076981
Iteration 24/25 | Loss: 0.00076981
Iteration 25/25 | Loss: 0.00076981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007698131958022714, 0.0007698131958022714, 0.0007698131958022714, 0.0007698131958022714, 0.0007698131958022714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007698131958022714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076981
Iteration 2/1000 | Loss: 0.00008483
Iteration 3/1000 | Loss: 0.00005920
Iteration 4/1000 | Loss: 0.00005355
Iteration 5/1000 | Loss: 0.00004929
Iteration 6/1000 | Loss: 0.00004627
Iteration 7/1000 | Loss: 0.00004441
Iteration 8/1000 | Loss: 0.00004212
Iteration 9/1000 | Loss: 0.00003995
Iteration 10/1000 | Loss: 0.00003801
Iteration 11/1000 | Loss: 0.00054628
Iteration 12/1000 | Loss: 0.00010572
Iteration 13/1000 | Loss: 0.00003828
Iteration 14/1000 | Loss: 0.00003595
Iteration 15/1000 | Loss: 0.00051197
Iteration 16/1000 | Loss: 0.00006028
Iteration 17/1000 | Loss: 0.00003564
Iteration 18/1000 | Loss: 0.00003457
Iteration 19/1000 | Loss: 0.00003392
Iteration 20/1000 | Loss: 0.00039166
Iteration 21/1000 | Loss: 0.00131067
Iteration 22/1000 | Loss: 0.00018581
Iteration 23/1000 | Loss: 0.00006234
Iteration 24/1000 | Loss: 0.00016170
Iteration 25/1000 | Loss: 0.00019574
Iteration 26/1000 | Loss: 0.00104270
Iteration 27/1000 | Loss: 0.00021918
Iteration 28/1000 | Loss: 0.00031017
Iteration 29/1000 | Loss: 0.00019273
Iteration 30/1000 | Loss: 0.00014275
Iteration 31/1000 | Loss: 0.00026139
Iteration 32/1000 | Loss: 0.00008507
Iteration 33/1000 | Loss: 0.00015941
Iteration 34/1000 | Loss: 0.00012404
Iteration 35/1000 | Loss: 0.00007218
Iteration 36/1000 | Loss: 0.00020487
Iteration 37/1000 | Loss: 0.00049385
Iteration 38/1000 | Loss: 0.00070457
Iteration 39/1000 | Loss: 0.00054498
Iteration 40/1000 | Loss: 0.00064232
Iteration 41/1000 | Loss: 0.00050475
Iteration 42/1000 | Loss: 0.00040299
Iteration 43/1000 | Loss: 0.00045486
Iteration 44/1000 | Loss: 0.00079970
Iteration 45/1000 | Loss: 0.00026621
Iteration 46/1000 | Loss: 0.00005986
Iteration 47/1000 | Loss: 0.00010563
Iteration 48/1000 | Loss: 0.00012682
Iteration 49/1000 | Loss: 0.00027179
Iteration 50/1000 | Loss: 0.00022791
Iteration 51/1000 | Loss: 0.00030195
Iteration 52/1000 | Loss: 0.00031737
Iteration 53/1000 | Loss: 0.00017416
Iteration 54/1000 | Loss: 0.00018685
Iteration 55/1000 | Loss: 0.00044393
Iteration 56/1000 | Loss: 0.00020320
Iteration 57/1000 | Loss: 0.00005638
Iteration 58/1000 | Loss: 0.00003619
Iteration 59/1000 | Loss: 0.00003360
Iteration 60/1000 | Loss: 0.00053720
Iteration 61/1000 | Loss: 0.00008067
Iteration 62/1000 | Loss: 0.00004136
Iteration 63/1000 | Loss: 0.00013442
Iteration 64/1000 | Loss: 0.00033435
Iteration 65/1000 | Loss: 0.00018637
Iteration 66/1000 | Loss: 0.00029042
Iteration 67/1000 | Loss: 0.00003985
Iteration 68/1000 | Loss: 0.00003255
Iteration 69/1000 | Loss: 0.00029514
Iteration 70/1000 | Loss: 0.00003236
Iteration 71/1000 | Loss: 0.00002967
Iteration 72/1000 | Loss: 0.00002902
Iteration 73/1000 | Loss: 0.00002874
Iteration 74/1000 | Loss: 0.00002856
Iteration 75/1000 | Loss: 0.00041462
Iteration 76/1000 | Loss: 0.00026911
Iteration 77/1000 | Loss: 0.00028571
Iteration 78/1000 | Loss: 0.00004849
Iteration 79/1000 | Loss: 0.00003278
Iteration 80/1000 | Loss: 0.00002924
Iteration 81/1000 | Loss: 0.00002771
Iteration 82/1000 | Loss: 0.00002685
Iteration 83/1000 | Loss: 0.00002608
Iteration 84/1000 | Loss: 0.00002577
Iteration 85/1000 | Loss: 0.00002560
Iteration 86/1000 | Loss: 0.00002560
Iteration 87/1000 | Loss: 0.00002556
Iteration 88/1000 | Loss: 0.00002556
Iteration 89/1000 | Loss: 0.00002550
Iteration 90/1000 | Loss: 0.00002549
Iteration 91/1000 | Loss: 0.00002549
Iteration 92/1000 | Loss: 0.00002548
Iteration 93/1000 | Loss: 0.00002548
Iteration 94/1000 | Loss: 0.00002547
Iteration 95/1000 | Loss: 0.00002547
Iteration 96/1000 | Loss: 0.00002547
Iteration 97/1000 | Loss: 0.00002547
Iteration 98/1000 | Loss: 0.00002546
Iteration 99/1000 | Loss: 0.00002546
Iteration 100/1000 | Loss: 0.00002546
Iteration 101/1000 | Loss: 0.00002545
Iteration 102/1000 | Loss: 0.00002545
Iteration 103/1000 | Loss: 0.00002545
Iteration 104/1000 | Loss: 0.00002545
Iteration 105/1000 | Loss: 0.00002545
Iteration 106/1000 | Loss: 0.00002545
Iteration 107/1000 | Loss: 0.00002545
Iteration 108/1000 | Loss: 0.00002545
Iteration 109/1000 | Loss: 0.00002544
Iteration 110/1000 | Loss: 0.00002544
Iteration 111/1000 | Loss: 0.00002544
Iteration 112/1000 | Loss: 0.00002543
Iteration 113/1000 | Loss: 0.00002543
Iteration 114/1000 | Loss: 0.00002543
Iteration 115/1000 | Loss: 0.00002543
Iteration 116/1000 | Loss: 0.00002541
Iteration 117/1000 | Loss: 0.00002541
Iteration 118/1000 | Loss: 0.00002541
Iteration 119/1000 | Loss: 0.00002541
Iteration 120/1000 | Loss: 0.00002541
Iteration 121/1000 | Loss: 0.00002541
Iteration 122/1000 | Loss: 0.00002541
Iteration 123/1000 | Loss: 0.00002541
Iteration 124/1000 | Loss: 0.00002541
Iteration 125/1000 | Loss: 0.00002541
Iteration 126/1000 | Loss: 0.00002540
Iteration 127/1000 | Loss: 0.00002540
Iteration 128/1000 | Loss: 0.00002540
Iteration 129/1000 | Loss: 0.00002540
Iteration 130/1000 | Loss: 0.00002540
Iteration 131/1000 | Loss: 0.00002540
Iteration 132/1000 | Loss: 0.00002540
Iteration 133/1000 | Loss: 0.00002540
Iteration 134/1000 | Loss: 0.00002540
Iteration 135/1000 | Loss: 0.00002539
Iteration 136/1000 | Loss: 0.00002539
Iteration 137/1000 | Loss: 0.00002539
Iteration 138/1000 | Loss: 0.00002538
Iteration 139/1000 | Loss: 0.00002538
Iteration 140/1000 | Loss: 0.00002538
Iteration 141/1000 | Loss: 0.00002538
Iteration 142/1000 | Loss: 0.00002537
Iteration 143/1000 | Loss: 0.00002537
Iteration 144/1000 | Loss: 0.00002537
Iteration 145/1000 | Loss: 0.00002537
Iteration 146/1000 | Loss: 0.00002537
Iteration 147/1000 | Loss: 0.00002537
Iteration 148/1000 | Loss: 0.00002537
Iteration 149/1000 | Loss: 0.00002537
Iteration 150/1000 | Loss: 0.00002536
Iteration 151/1000 | Loss: 0.00002536
Iteration 152/1000 | Loss: 0.00002536
Iteration 153/1000 | Loss: 0.00002536
Iteration 154/1000 | Loss: 0.00002536
Iteration 155/1000 | Loss: 0.00002536
Iteration 156/1000 | Loss: 0.00002536
Iteration 157/1000 | Loss: 0.00002536
Iteration 158/1000 | Loss: 0.00002536
Iteration 159/1000 | Loss: 0.00002536
Iteration 160/1000 | Loss: 0.00002536
Iteration 161/1000 | Loss: 0.00002536
Iteration 162/1000 | Loss: 0.00002536
Iteration 163/1000 | Loss: 0.00002536
Iteration 164/1000 | Loss: 0.00002536
Iteration 165/1000 | Loss: 0.00002536
Iteration 166/1000 | Loss: 0.00002536
Iteration 167/1000 | Loss: 0.00002536
Iteration 168/1000 | Loss: 0.00002536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.536159081500955e-05, 2.536159081500955e-05, 2.536159081500955e-05, 2.536159081500955e-05, 2.536159081500955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.536159081500955e-05

Optimization complete. Final v2v error: 3.9492337703704834 mm

Highest mean error: 13.110586166381836 mm for frame 168

Lowest mean error: 3.0792503356933594 mm for frame 145

Saving results

Total time: 184.9818079471588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903812
Iteration 2/25 | Loss: 0.00126307
Iteration 3/25 | Loss: 0.00114758
Iteration 4/25 | Loss: 0.00113990
Iteration 5/25 | Loss: 0.00113699
Iteration 6/25 | Loss: 0.00113653
Iteration 7/25 | Loss: 0.00113653
Iteration 8/25 | Loss: 0.00113653
Iteration 9/25 | Loss: 0.00113653
Iteration 10/25 | Loss: 0.00113653
Iteration 11/25 | Loss: 0.00113653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011365319369360805, 0.0011365319369360805, 0.0011365319369360805, 0.0011365319369360805, 0.0011365319369360805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011365319369360805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32957649
Iteration 2/25 | Loss: 0.00066323
Iteration 3/25 | Loss: 0.00066322
Iteration 4/25 | Loss: 0.00066322
Iteration 5/25 | Loss: 0.00066322
Iteration 6/25 | Loss: 0.00066322
Iteration 7/25 | Loss: 0.00066322
Iteration 8/25 | Loss: 0.00066322
Iteration 9/25 | Loss: 0.00066322
Iteration 10/25 | Loss: 0.00066322
Iteration 11/25 | Loss: 0.00066322
Iteration 12/25 | Loss: 0.00066322
Iteration 13/25 | Loss: 0.00066322
Iteration 14/25 | Loss: 0.00066322
Iteration 15/25 | Loss: 0.00066322
Iteration 16/25 | Loss: 0.00066322
Iteration 17/25 | Loss: 0.00066322
Iteration 18/25 | Loss: 0.00066322
Iteration 19/25 | Loss: 0.00066322
Iteration 20/25 | Loss: 0.00066322
Iteration 21/25 | Loss: 0.00066322
Iteration 22/25 | Loss: 0.00066322
Iteration 23/25 | Loss: 0.00066322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006632168078795075, 0.0006632168078795075, 0.0006632168078795075, 0.0006632168078795075, 0.0006632168078795075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006632168078795075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066322
Iteration 2/1000 | Loss: 0.00003224
Iteration 3/1000 | Loss: 0.00001866
Iteration 4/1000 | Loss: 0.00001720
Iteration 5/1000 | Loss: 0.00001581
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001411
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001395
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001395
Iteration 17/1000 | Loss: 0.00001395
Iteration 18/1000 | Loss: 0.00001395
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001380
Iteration 24/1000 | Loss: 0.00001380
Iteration 25/1000 | Loss: 0.00001379
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00001378
Iteration 30/1000 | Loss: 0.00001378
Iteration 31/1000 | Loss: 0.00001378
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001377
Iteration 35/1000 | Loss: 0.00001377
Iteration 36/1000 | Loss: 0.00001377
Iteration 37/1000 | Loss: 0.00001377
Iteration 38/1000 | Loss: 0.00001377
Iteration 39/1000 | Loss: 0.00001377
Iteration 40/1000 | Loss: 0.00001377
Iteration 41/1000 | Loss: 0.00001377
Iteration 42/1000 | Loss: 0.00001377
Iteration 43/1000 | Loss: 0.00001377
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001376
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001374
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001374
Iteration 62/1000 | Loss: 0.00001374
Iteration 63/1000 | Loss: 0.00001374
Iteration 64/1000 | Loss: 0.00001373
Iteration 65/1000 | Loss: 0.00001373
Iteration 66/1000 | Loss: 0.00001373
Iteration 67/1000 | Loss: 0.00001373
Iteration 68/1000 | Loss: 0.00001373
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001372
Iteration 71/1000 | Loss: 0.00001372
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001372
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001372
Iteration 79/1000 | Loss: 0.00001372
Iteration 80/1000 | Loss: 0.00001372
Iteration 81/1000 | Loss: 0.00001372
Iteration 82/1000 | Loss: 0.00001372
Iteration 83/1000 | Loss: 0.00001372
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001372
Iteration 88/1000 | Loss: 0.00001372
Iteration 89/1000 | Loss: 0.00001372
Iteration 90/1000 | Loss: 0.00001372
Iteration 91/1000 | Loss: 0.00001372
Iteration 92/1000 | Loss: 0.00001372
Iteration 93/1000 | Loss: 0.00001372
Iteration 94/1000 | Loss: 0.00001372
Iteration 95/1000 | Loss: 0.00001372
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001372
Iteration 102/1000 | Loss: 0.00001372
Iteration 103/1000 | Loss: 0.00001372
Iteration 104/1000 | Loss: 0.00001372
Iteration 105/1000 | Loss: 0.00001372
Iteration 106/1000 | Loss: 0.00001372
Iteration 107/1000 | Loss: 0.00001372
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001372
Iteration 110/1000 | Loss: 0.00001372
Iteration 111/1000 | Loss: 0.00001372
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.3717337424168363e-05, 1.3717337424168363e-05, 1.3717337424168363e-05, 1.3717337424168363e-05, 1.3717337424168363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3717337424168363e-05

Optimization complete. Final v2v error: 3.1218037605285645 mm

Highest mean error: 3.4772064685821533 mm for frame 60

Lowest mean error: 2.8071322441101074 mm for frame 75

Saving results

Total time: 29.030730724334717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455937
Iteration 2/25 | Loss: 0.00132745
Iteration 3/25 | Loss: 0.00123002
Iteration 4/25 | Loss: 0.00120779
Iteration 5/25 | Loss: 0.00119823
Iteration 6/25 | Loss: 0.00119607
Iteration 7/25 | Loss: 0.00119598
Iteration 8/25 | Loss: 0.00119539
Iteration 9/25 | Loss: 0.00119539
Iteration 10/25 | Loss: 0.00119539
Iteration 11/25 | Loss: 0.00119539
Iteration 12/25 | Loss: 0.00119539
Iteration 13/25 | Loss: 0.00119539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011953875655308366, 0.0011953875655308366, 0.0011953875655308366, 0.0011953875655308366, 0.0011953875655308366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011953875655308366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73552394
Iteration 2/25 | Loss: 0.00061357
Iteration 3/25 | Loss: 0.00061357
Iteration 4/25 | Loss: 0.00061357
Iteration 5/25 | Loss: 0.00061357
Iteration 6/25 | Loss: 0.00061357
Iteration 7/25 | Loss: 0.00061357
Iteration 8/25 | Loss: 0.00061357
Iteration 9/25 | Loss: 0.00061357
Iteration 10/25 | Loss: 0.00061357
Iteration 11/25 | Loss: 0.00061357
Iteration 12/25 | Loss: 0.00061357
Iteration 13/25 | Loss: 0.00061357
Iteration 14/25 | Loss: 0.00061357
Iteration 15/25 | Loss: 0.00061357
Iteration 16/25 | Loss: 0.00061357
Iteration 17/25 | Loss: 0.00061357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006135652074590325, 0.0006135652074590325, 0.0006135652074590325, 0.0006135652074590325, 0.0006135652074590325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006135652074590325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061357
Iteration 2/1000 | Loss: 0.00004607
Iteration 3/1000 | Loss: 0.00002649
Iteration 4/1000 | Loss: 0.00002461
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002228
Iteration 7/1000 | Loss: 0.00002149
Iteration 8/1000 | Loss: 0.00002098
Iteration 9/1000 | Loss: 0.00002068
Iteration 10/1000 | Loss: 0.00002045
Iteration 11/1000 | Loss: 0.00002027
Iteration 12/1000 | Loss: 0.00002018
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002014
Iteration 15/1000 | Loss: 0.00002013
Iteration 16/1000 | Loss: 0.00002013
Iteration 17/1000 | Loss: 0.00002012
Iteration 18/1000 | Loss: 0.00002011
Iteration 19/1000 | Loss: 0.00002011
Iteration 20/1000 | Loss: 0.00002010
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002009
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00002004
Iteration 25/1000 | Loss: 0.00002004
Iteration 26/1000 | Loss: 0.00002004
Iteration 27/1000 | Loss: 0.00002004
Iteration 28/1000 | Loss: 0.00002004
Iteration 29/1000 | Loss: 0.00002003
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00002001
Iteration 32/1000 | Loss: 0.00002001
Iteration 33/1000 | Loss: 0.00002001
Iteration 34/1000 | Loss: 0.00002001
Iteration 35/1000 | Loss: 0.00002001
Iteration 36/1000 | Loss: 0.00002001
Iteration 37/1000 | Loss: 0.00002001
Iteration 38/1000 | Loss: 0.00002001
Iteration 39/1000 | Loss: 0.00002001
Iteration 40/1000 | Loss: 0.00002001
Iteration 41/1000 | Loss: 0.00002000
Iteration 42/1000 | Loss: 0.00002000
Iteration 43/1000 | Loss: 0.00002000
Iteration 44/1000 | Loss: 0.00001999
Iteration 45/1000 | Loss: 0.00001999
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001998
Iteration 48/1000 | Loss: 0.00001998
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001997
Iteration 51/1000 | Loss: 0.00001997
Iteration 52/1000 | Loss: 0.00001997
Iteration 53/1000 | Loss: 0.00001997
Iteration 54/1000 | Loss: 0.00001997
Iteration 55/1000 | Loss: 0.00001997
Iteration 56/1000 | Loss: 0.00001997
Iteration 57/1000 | Loss: 0.00001997
Iteration 58/1000 | Loss: 0.00001996
Iteration 59/1000 | Loss: 0.00001996
Iteration 60/1000 | Loss: 0.00001996
Iteration 61/1000 | Loss: 0.00001996
Iteration 62/1000 | Loss: 0.00001995
Iteration 63/1000 | Loss: 0.00001995
Iteration 64/1000 | Loss: 0.00001994
Iteration 65/1000 | Loss: 0.00001994
Iteration 66/1000 | Loss: 0.00001994
Iteration 67/1000 | Loss: 0.00001994
Iteration 68/1000 | Loss: 0.00001994
Iteration 69/1000 | Loss: 0.00001994
Iteration 70/1000 | Loss: 0.00001993
Iteration 71/1000 | Loss: 0.00001993
Iteration 72/1000 | Loss: 0.00001993
Iteration 73/1000 | Loss: 0.00001993
Iteration 74/1000 | Loss: 0.00001993
Iteration 75/1000 | Loss: 0.00001993
Iteration 76/1000 | Loss: 0.00001993
Iteration 77/1000 | Loss: 0.00001993
Iteration 78/1000 | Loss: 0.00001993
Iteration 79/1000 | Loss: 0.00001993
Iteration 80/1000 | Loss: 0.00001993
Iteration 81/1000 | Loss: 0.00001992
Iteration 82/1000 | Loss: 0.00001992
Iteration 83/1000 | Loss: 0.00001992
Iteration 84/1000 | Loss: 0.00001992
Iteration 85/1000 | Loss: 0.00001992
Iteration 86/1000 | Loss: 0.00001992
Iteration 87/1000 | Loss: 0.00001992
Iteration 88/1000 | Loss: 0.00001992
Iteration 89/1000 | Loss: 0.00001992
Iteration 90/1000 | Loss: 0.00001991
Iteration 91/1000 | Loss: 0.00001991
Iteration 92/1000 | Loss: 0.00001991
Iteration 93/1000 | Loss: 0.00001990
Iteration 94/1000 | Loss: 0.00001990
Iteration 95/1000 | Loss: 0.00001990
Iteration 96/1000 | Loss: 0.00001990
Iteration 97/1000 | Loss: 0.00001990
Iteration 98/1000 | Loss: 0.00001990
Iteration 99/1000 | Loss: 0.00001989
Iteration 100/1000 | Loss: 0.00001989
Iteration 101/1000 | Loss: 0.00001989
Iteration 102/1000 | Loss: 0.00001989
Iteration 103/1000 | Loss: 0.00001989
Iteration 104/1000 | Loss: 0.00001989
Iteration 105/1000 | Loss: 0.00001989
Iteration 106/1000 | Loss: 0.00001989
Iteration 107/1000 | Loss: 0.00001988
Iteration 108/1000 | Loss: 0.00001988
Iteration 109/1000 | Loss: 0.00001988
Iteration 110/1000 | Loss: 0.00001988
Iteration 111/1000 | Loss: 0.00001988
Iteration 112/1000 | Loss: 0.00001988
Iteration 113/1000 | Loss: 0.00001988
Iteration 114/1000 | Loss: 0.00001988
Iteration 115/1000 | Loss: 0.00001988
Iteration 116/1000 | Loss: 0.00001988
Iteration 117/1000 | Loss: 0.00001988
Iteration 118/1000 | Loss: 0.00001988
Iteration 119/1000 | Loss: 0.00001988
Iteration 120/1000 | Loss: 0.00001988
Iteration 121/1000 | Loss: 0.00001988
Iteration 122/1000 | Loss: 0.00001988
Iteration 123/1000 | Loss: 0.00001988
Iteration 124/1000 | Loss: 0.00001988
Iteration 125/1000 | Loss: 0.00001988
Iteration 126/1000 | Loss: 0.00001988
Iteration 127/1000 | Loss: 0.00001987
Iteration 128/1000 | Loss: 0.00001987
Iteration 129/1000 | Loss: 0.00001987
Iteration 130/1000 | Loss: 0.00001987
Iteration 131/1000 | Loss: 0.00001987
Iteration 132/1000 | Loss: 0.00001987
Iteration 133/1000 | Loss: 0.00001987
Iteration 134/1000 | Loss: 0.00001987
Iteration 135/1000 | Loss: 0.00001987
Iteration 136/1000 | Loss: 0.00001987
Iteration 137/1000 | Loss: 0.00001987
Iteration 138/1000 | Loss: 0.00001987
Iteration 139/1000 | Loss: 0.00001987
Iteration 140/1000 | Loss: 0.00001986
Iteration 141/1000 | Loss: 0.00001986
Iteration 142/1000 | Loss: 0.00001986
Iteration 143/1000 | Loss: 0.00001986
Iteration 144/1000 | Loss: 0.00001986
Iteration 145/1000 | Loss: 0.00001986
Iteration 146/1000 | Loss: 0.00001986
Iteration 147/1000 | Loss: 0.00001986
Iteration 148/1000 | Loss: 0.00001986
Iteration 149/1000 | Loss: 0.00001986
Iteration 150/1000 | Loss: 0.00001986
Iteration 151/1000 | Loss: 0.00001986
Iteration 152/1000 | Loss: 0.00001986
Iteration 153/1000 | Loss: 0.00001986
Iteration 154/1000 | Loss: 0.00001985
Iteration 155/1000 | Loss: 0.00001985
Iteration 156/1000 | Loss: 0.00001985
Iteration 157/1000 | Loss: 0.00001985
Iteration 158/1000 | Loss: 0.00001985
Iteration 159/1000 | Loss: 0.00001985
Iteration 160/1000 | Loss: 0.00001985
Iteration 161/1000 | Loss: 0.00001985
Iteration 162/1000 | Loss: 0.00001985
Iteration 163/1000 | Loss: 0.00001984
Iteration 164/1000 | Loss: 0.00001984
Iteration 165/1000 | Loss: 0.00001984
Iteration 166/1000 | Loss: 0.00001984
Iteration 167/1000 | Loss: 0.00001984
Iteration 168/1000 | Loss: 0.00001984
Iteration 169/1000 | Loss: 0.00001984
Iteration 170/1000 | Loss: 0.00001984
Iteration 171/1000 | Loss: 0.00001983
Iteration 172/1000 | Loss: 0.00001983
Iteration 173/1000 | Loss: 0.00001983
Iteration 174/1000 | Loss: 0.00001983
Iteration 175/1000 | Loss: 0.00001983
Iteration 176/1000 | Loss: 0.00001983
Iteration 177/1000 | Loss: 0.00001983
Iteration 178/1000 | Loss: 0.00001983
Iteration 179/1000 | Loss: 0.00001983
Iteration 180/1000 | Loss: 0.00001983
Iteration 181/1000 | Loss: 0.00001983
Iteration 182/1000 | Loss: 0.00001982
Iteration 183/1000 | Loss: 0.00001982
Iteration 184/1000 | Loss: 0.00001982
Iteration 185/1000 | Loss: 0.00001982
Iteration 186/1000 | Loss: 0.00001982
Iteration 187/1000 | Loss: 0.00001982
Iteration 188/1000 | Loss: 0.00001982
Iteration 189/1000 | Loss: 0.00001982
Iteration 190/1000 | Loss: 0.00001982
Iteration 191/1000 | Loss: 0.00001982
Iteration 192/1000 | Loss: 0.00001982
Iteration 193/1000 | Loss: 0.00001982
Iteration 194/1000 | Loss: 0.00001982
Iteration 195/1000 | Loss: 0.00001982
Iteration 196/1000 | Loss: 0.00001982
Iteration 197/1000 | Loss: 0.00001982
Iteration 198/1000 | Loss: 0.00001982
Iteration 199/1000 | Loss: 0.00001982
Iteration 200/1000 | Loss: 0.00001982
Iteration 201/1000 | Loss: 0.00001982
Iteration 202/1000 | Loss: 0.00001982
Iteration 203/1000 | Loss: 0.00001982
Iteration 204/1000 | Loss: 0.00001982
Iteration 205/1000 | Loss: 0.00001982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.9816387066384777e-05, 1.9816387066384777e-05, 1.9816387066384777e-05, 1.9816387066384777e-05, 1.9816387066384777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9816387066384777e-05

Optimization complete. Final v2v error: 3.8455820083618164 mm

Highest mean error: 4.129454612731934 mm for frame 99

Lowest mean error: 3.509809970855713 mm for frame 26

Saving results

Total time: 44.73237204551697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900091
Iteration 2/25 | Loss: 0.00156872
Iteration 3/25 | Loss: 0.00129999
Iteration 4/25 | Loss: 0.00125754
Iteration 5/25 | Loss: 0.00125111
Iteration 6/25 | Loss: 0.00124980
Iteration 7/25 | Loss: 0.00124964
Iteration 8/25 | Loss: 0.00124964
Iteration 9/25 | Loss: 0.00124964
Iteration 10/25 | Loss: 0.00124964
Iteration 11/25 | Loss: 0.00124964
Iteration 12/25 | Loss: 0.00124964
Iteration 13/25 | Loss: 0.00124964
Iteration 14/25 | Loss: 0.00124964
Iteration 15/25 | Loss: 0.00124964
Iteration 16/25 | Loss: 0.00124964
Iteration 17/25 | Loss: 0.00124964
Iteration 18/25 | Loss: 0.00124964
Iteration 19/25 | Loss: 0.00124964
Iteration 20/25 | Loss: 0.00124964
Iteration 21/25 | Loss: 0.00124964
Iteration 22/25 | Loss: 0.00124964
Iteration 23/25 | Loss: 0.00124964
Iteration 24/25 | Loss: 0.00124964
Iteration 25/25 | Loss: 0.00124964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93258554
Iteration 2/25 | Loss: 0.00056431
Iteration 3/25 | Loss: 0.00056430
Iteration 4/25 | Loss: 0.00056429
Iteration 5/25 | Loss: 0.00056429
Iteration 6/25 | Loss: 0.00056429
Iteration 7/25 | Loss: 0.00056429
Iteration 8/25 | Loss: 0.00056429
Iteration 9/25 | Loss: 0.00056429
Iteration 10/25 | Loss: 0.00056429
Iteration 11/25 | Loss: 0.00056429
Iteration 12/25 | Loss: 0.00056429
Iteration 13/25 | Loss: 0.00056429
Iteration 14/25 | Loss: 0.00056429
Iteration 15/25 | Loss: 0.00056429
Iteration 16/25 | Loss: 0.00056429
Iteration 17/25 | Loss: 0.00056429
Iteration 18/25 | Loss: 0.00056429
Iteration 19/25 | Loss: 0.00056429
Iteration 20/25 | Loss: 0.00056429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005642924224957824, 0.0005642924224957824, 0.0005642924224957824, 0.0005642924224957824, 0.0005642924224957824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005642924224957824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056429
Iteration 2/1000 | Loss: 0.00006667
Iteration 3/1000 | Loss: 0.00004205
Iteration 4/1000 | Loss: 0.00003809
Iteration 5/1000 | Loss: 0.00003397
Iteration 6/1000 | Loss: 0.00003180
Iteration 7/1000 | Loss: 0.00003028
Iteration 8/1000 | Loss: 0.00002931
Iteration 9/1000 | Loss: 0.00002865
Iteration 10/1000 | Loss: 0.00002821
Iteration 11/1000 | Loss: 0.00002795
Iteration 12/1000 | Loss: 0.00002776
Iteration 13/1000 | Loss: 0.00002758
Iteration 14/1000 | Loss: 0.00002757
Iteration 15/1000 | Loss: 0.00002756
Iteration 16/1000 | Loss: 0.00002755
Iteration 17/1000 | Loss: 0.00002755
Iteration 18/1000 | Loss: 0.00002754
Iteration 19/1000 | Loss: 0.00002754
Iteration 20/1000 | Loss: 0.00002750
Iteration 21/1000 | Loss: 0.00002750
Iteration 22/1000 | Loss: 0.00002750
Iteration 23/1000 | Loss: 0.00002750
Iteration 24/1000 | Loss: 0.00002750
Iteration 25/1000 | Loss: 0.00002749
Iteration 26/1000 | Loss: 0.00002748
Iteration 27/1000 | Loss: 0.00002748
Iteration 28/1000 | Loss: 0.00002748
Iteration 29/1000 | Loss: 0.00002747
Iteration 30/1000 | Loss: 0.00002747
Iteration 31/1000 | Loss: 0.00002747
Iteration 32/1000 | Loss: 0.00002746
Iteration 33/1000 | Loss: 0.00002746
Iteration 34/1000 | Loss: 0.00002746
Iteration 35/1000 | Loss: 0.00002746
Iteration 36/1000 | Loss: 0.00002746
Iteration 37/1000 | Loss: 0.00002746
Iteration 38/1000 | Loss: 0.00002745
Iteration 39/1000 | Loss: 0.00002745
Iteration 40/1000 | Loss: 0.00002745
Iteration 41/1000 | Loss: 0.00002745
Iteration 42/1000 | Loss: 0.00002744
Iteration 43/1000 | Loss: 0.00002744
Iteration 44/1000 | Loss: 0.00002744
Iteration 45/1000 | Loss: 0.00002743
Iteration 46/1000 | Loss: 0.00002743
Iteration 47/1000 | Loss: 0.00002743
Iteration 48/1000 | Loss: 0.00002743
Iteration 49/1000 | Loss: 0.00002743
Iteration 50/1000 | Loss: 0.00002743
Iteration 51/1000 | Loss: 0.00002743
Iteration 52/1000 | Loss: 0.00002743
Iteration 53/1000 | Loss: 0.00002743
Iteration 54/1000 | Loss: 0.00002743
Iteration 55/1000 | Loss: 0.00002743
Iteration 56/1000 | Loss: 0.00002743
Iteration 57/1000 | Loss: 0.00002743
Iteration 58/1000 | Loss: 0.00002743
Iteration 59/1000 | Loss: 0.00002743
Iteration 60/1000 | Loss: 0.00002743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [2.743088589340914e-05, 2.743088589340914e-05, 2.743088589340914e-05, 2.743088589340914e-05, 2.743088589340914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.743088589340914e-05

Optimization complete. Final v2v error: 4.40282678604126 mm

Highest mean error: 4.869758129119873 mm for frame 131

Lowest mean error: 4.175780296325684 mm for frame 33

Saving results

Total time: 31.445232629776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982586
Iteration 2/25 | Loss: 0.00203671
Iteration 3/25 | Loss: 0.00156407
Iteration 4/25 | Loss: 0.00147423
Iteration 5/25 | Loss: 0.00142871
Iteration 6/25 | Loss: 0.00137504
Iteration 7/25 | Loss: 0.00134578
Iteration 8/25 | Loss: 0.00133525
Iteration 9/25 | Loss: 0.00133079
Iteration 10/25 | Loss: 0.00131884
Iteration 11/25 | Loss: 0.00131214
Iteration 12/25 | Loss: 0.00131085
Iteration 13/25 | Loss: 0.00131061
Iteration 14/25 | Loss: 0.00131393
Iteration 15/25 | Loss: 0.00130371
Iteration 16/25 | Loss: 0.00130312
Iteration 17/25 | Loss: 0.00130306
Iteration 18/25 | Loss: 0.00130306
Iteration 19/25 | Loss: 0.00130306
Iteration 20/25 | Loss: 0.00130305
Iteration 21/25 | Loss: 0.00130305
Iteration 22/25 | Loss: 0.00130305
Iteration 23/25 | Loss: 0.00130305
Iteration 24/25 | Loss: 0.00130305
Iteration 25/25 | Loss: 0.00130305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32044673
Iteration 2/25 | Loss: 0.00083961
Iteration 3/25 | Loss: 0.00083784
Iteration 4/25 | Loss: 0.00083784
Iteration 5/25 | Loss: 0.00083784
Iteration 6/25 | Loss: 0.00083784
Iteration 7/25 | Loss: 0.00083784
Iteration 8/25 | Loss: 0.00083784
Iteration 9/25 | Loss: 0.00083784
Iteration 10/25 | Loss: 0.00083784
Iteration 11/25 | Loss: 0.00083784
Iteration 12/25 | Loss: 0.00083784
Iteration 13/25 | Loss: 0.00083784
Iteration 14/25 | Loss: 0.00083784
Iteration 15/25 | Loss: 0.00083784
Iteration 16/25 | Loss: 0.00083784
Iteration 17/25 | Loss: 0.00083784
Iteration 18/25 | Loss: 0.00083784
Iteration 19/25 | Loss: 0.00083784
Iteration 20/25 | Loss: 0.00083784
Iteration 21/25 | Loss: 0.00083784
Iteration 22/25 | Loss: 0.00083784
Iteration 23/25 | Loss: 0.00083784
Iteration 24/25 | Loss: 0.00083784
Iteration 25/25 | Loss: 0.00083784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083784
Iteration 2/1000 | Loss: 0.00012857
Iteration 3/1000 | Loss: 0.00010699
Iteration 4/1000 | Loss: 0.00006667
Iteration 5/1000 | Loss: 0.00005866
Iteration 6/1000 | Loss: 0.00005336
Iteration 7/1000 | Loss: 0.00004963
Iteration 8/1000 | Loss: 0.00004760
Iteration 9/1000 | Loss: 0.00004684
Iteration 10/1000 | Loss: 0.00004587
Iteration 11/1000 | Loss: 0.00004492
Iteration 12/1000 | Loss: 0.00004416
Iteration 13/1000 | Loss: 0.00004363
Iteration 14/1000 | Loss: 0.00004318
Iteration 15/1000 | Loss: 0.00004268
Iteration 16/1000 | Loss: 0.00004220
Iteration 17/1000 | Loss: 0.00004146
Iteration 18/1000 | Loss: 0.00004075
Iteration 19/1000 | Loss: 0.00003991
Iteration 20/1000 | Loss: 0.00004624
Iteration 21/1000 | Loss: 0.00004235
Iteration 22/1000 | Loss: 0.00004128
Iteration 23/1000 | Loss: 0.00004048
Iteration 24/1000 | Loss: 0.00003952
Iteration 25/1000 | Loss: 0.00003828
Iteration 26/1000 | Loss: 0.00003725
Iteration 27/1000 | Loss: 0.00003663
Iteration 28/1000 | Loss: 0.00003642
Iteration 29/1000 | Loss: 0.00003625
Iteration 30/1000 | Loss: 0.00003622
Iteration 31/1000 | Loss: 0.00003622
Iteration 32/1000 | Loss: 0.00003622
Iteration 33/1000 | Loss: 0.00003619
Iteration 34/1000 | Loss: 0.00003619
Iteration 35/1000 | Loss: 0.00003619
Iteration 36/1000 | Loss: 0.00003619
Iteration 37/1000 | Loss: 0.00003619
Iteration 38/1000 | Loss: 0.00003619
Iteration 39/1000 | Loss: 0.00003619
Iteration 40/1000 | Loss: 0.00003618
Iteration 41/1000 | Loss: 0.00003618
Iteration 42/1000 | Loss: 0.00003618
Iteration 43/1000 | Loss: 0.00003618
Iteration 44/1000 | Loss: 0.00003618
Iteration 45/1000 | Loss: 0.00003618
Iteration 46/1000 | Loss: 0.00003618
Iteration 47/1000 | Loss: 0.00003616
Iteration 48/1000 | Loss: 0.00003616
Iteration 49/1000 | Loss: 0.00003616
Iteration 50/1000 | Loss: 0.00003616
Iteration 51/1000 | Loss: 0.00003615
Iteration 52/1000 | Loss: 0.00003615
Iteration 53/1000 | Loss: 0.00003615
Iteration 54/1000 | Loss: 0.00003615
Iteration 55/1000 | Loss: 0.00003615
Iteration 56/1000 | Loss: 0.00003615
Iteration 57/1000 | Loss: 0.00003615
Iteration 58/1000 | Loss: 0.00003615
Iteration 59/1000 | Loss: 0.00003614
Iteration 60/1000 | Loss: 0.00003614
Iteration 61/1000 | Loss: 0.00003614
Iteration 62/1000 | Loss: 0.00003614
Iteration 63/1000 | Loss: 0.00003614
Iteration 64/1000 | Loss: 0.00003614
Iteration 65/1000 | Loss: 0.00003613
Iteration 66/1000 | Loss: 0.00003613
Iteration 67/1000 | Loss: 0.00003613
Iteration 68/1000 | Loss: 0.00003613
Iteration 69/1000 | Loss: 0.00003613
Iteration 70/1000 | Loss: 0.00003612
Iteration 71/1000 | Loss: 0.00003612
Iteration 72/1000 | Loss: 0.00003612
Iteration 73/1000 | Loss: 0.00003612
Iteration 74/1000 | Loss: 0.00003611
Iteration 75/1000 | Loss: 0.00003611
Iteration 76/1000 | Loss: 0.00003611
Iteration 77/1000 | Loss: 0.00003611
Iteration 78/1000 | Loss: 0.00003611
Iteration 79/1000 | Loss: 0.00003611
Iteration 80/1000 | Loss: 0.00003610
Iteration 81/1000 | Loss: 0.00003610
Iteration 82/1000 | Loss: 0.00003610
Iteration 83/1000 | Loss: 0.00003609
Iteration 84/1000 | Loss: 0.00003609
Iteration 85/1000 | Loss: 0.00003609
Iteration 86/1000 | Loss: 0.00003609
Iteration 87/1000 | Loss: 0.00003608
Iteration 88/1000 | Loss: 0.00003608
Iteration 89/1000 | Loss: 0.00003608
Iteration 90/1000 | Loss: 0.00003608
Iteration 91/1000 | Loss: 0.00003607
Iteration 92/1000 | Loss: 0.00003607
Iteration 93/1000 | Loss: 0.00003606
Iteration 94/1000 | Loss: 0.00003606
Iteration 95/1000 | Loss: 0.00003606
Iteration 96/1000 | Loss: 0.00003606
Iteration 97/1000 | Loss: 0.00003606
Iteration 98/1000 | Loss: 0.00003606
Iteration 99/1000 | Loss: 0.00003606
Iteration 100/1000 | Loss: 0.00003605
Iteration 101/1000 | Loss: 0.00003605
Iteration 102/1000 | Loss: 0.00003605
Iteration 103/1000 | Loss: 0.00003605
Iteration 104/1000 | Loss: 0.00003605
Iteration 105/1000 | Loss: 0.00003605
Iteration 106/1000 | Loss: 0.00003605
Iteration 107/1000 | Loss: 0.00003604
Iteration 108/1000 | Loss: 0.00003604
Iteration 109/1000 | Loss: 0.00003604
Iteration 110/1000 | Loss: 0.00003604
Iteration 111/1000 | Loss: 0.00003604
Iteration 112/1000 | Loss: 0.00003604
Iteration 113/1000 | Loss: 0.00003604
Iteration 114/1000 | Loss: 0.00003604
Iteration 115/1000 | Loss: 0.00003604
Iteration 116/1000 | Loss: 0.00003604
Iteration 117/1000 | Loss: 0.00003604
Iteration 118/1000 | Loss: 0.00003604
Iteration 119/1000 | Loss: 0.00003604
Iteration 120/1000 | Loss: 0.00003604
Iteration 121/1000 | Loss: 0.00003604
Iteration 122/1000 | Loss: 0.00003604
Iteration 123/1000 | Loss: 0.00003604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.603824370657094e-05, 3.603824370657094e-05, 3.603824370657094e-05, 3.603824370657094e-05, 3.603824370657094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.603824370657094e-05

Optimization complete. Final v2v error: 4.692188262939453 mm

Highest mean error: 8.09212875366211 mm for frame 117

Lowest mean error: 3.390718698501587 mm for frame 197

Saving results

Total time: 81.14374470710754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892166
Iteration 2/25 | Loss: 0.00157933
Iteration 3/25 | Loss: 0.00124013
Iteration 4/25 | Loss: 0.00119056
Iteration 5/25 | Loss: 0.00118089
Iteration 6/25 | Loss: 0.00117849
Iteration 7/25 | Loss: 0.00117775
Iteration 8/25 | Loss: 0.00117755
Iteration 9/25 | Loss: 0.00117745
Iteration 10/25 | Loss: 0.00117743
Iteration 11/25 | Loss: 0.00117743
Iteration 12/25 | Loss: 0.00117743
Iteration 13/25 | Loss: 0.00117743
Iteration 14/25 | Loss: 0.00117743
Iteration 15/25 | Loss: 0.00117743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011774312006309628, 0.0011774312006309628, 0.0011774312006309628, 0.0011774312006309628, 0.0011774312006309628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011774312006309628

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32429290
Iteration 2/25 | Loss: 0.00061863
Iteration 3/25 | Loss: 0.00061863
Iteration 4/25 | Loss: 0.00061863
Iteration 5/25 | Loss: 0.00061863
Iteration 6/25 | Loss: 0.00061863
Iteration 7/25 | Loss: 0.00061863
Iteration 8/25 | Loss: 0.00061863
Iteration 9/25 | Loss: 0.00061863
Iteration 10/25 | Loss: 0.00061863
Iteration 11/25 | Loss: 0.00061863
Iteration 12/25 | Loss: 0.00061863
Iteration 13/25 | Loss: 0.00061863
Iteration 14/25 | Loss: 0.00061863
Iteration 15/25 | Loss: 0.00061863
Iteration 16/25 | Loss: 0.00061863
Iteration 17/25 | Loss: 0.00061863
Iteration 18/25 | Loss: 0.00061863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006186292157508433, 0.0006186292157508433, 0.0006186292157508433, 0.0006186292157508433, 0.0006186292157508433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006186292157508433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061863
Iteration 2/1000 | Loss: 0.00003922
Iteration 3/1000 | Loss: 0.00002371
Iteration 4/1000 | Loss: 0.00001974
Iteration 5/1000 | Loss: 0.00001818
Iteration 6/1000 | Loss: 0.00001740
Iteration 7/1000 | Loss: 0.00001678
Iteration 8/1000 | Loss: 0.00001634
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001621
Iteration 11/1000 | Loss: 0.00003115
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001860
Iteration 14/1000 | Loss: 0.00003086
Iteration 15/1000 | Loss: 0.00001692
Iteration 16/1000 | Loss: 0.00003282
Iteration 17/1000 | Loss: 0.00002683
Iteration 18/1000 | Loss: 0.00002947
Iteration 19/1000 | Loss: 0.00003852
Iteration 20/1000 | Loss: 0.00002933
Iteration 21/1000 | Loss: 0.00003231
Iteration 22/1000 | Loss: 0.00003069
Iteration 23/1000 | Loss: 0.00003039
Iteration 24/1000 | Loss: 0.00003056
Iteration 25/1000 | Loss: 0.00003198
Iteration 26/1000 | Loss: 0.00003067
Iteration 27/1000 | Loss: 0.00002642
Iteration 28/1000 | Loss: 0.00003044
Iteration 29/1000 | Loss: 0.00002041
Iteration 30/1000 | Loss: 0.00002644
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001619
Iteration 33/1000 | Loss: 0.00001547
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001496
Iteration 39/1000 | Loss: 0.00001490
Iteration 40/1000 | Loss: 0.00001489
Iteration 41/1000 | Loss: 0.00001488
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001476
Iteration 45/1000 | Loss: 0.00001476
Iteration 46/1000 | Loss: 0.00001476
Iteration 47/1000 | Loss: 0.00001476
Iteration 48/1000 | Loss: 0.00001476
Iteration 49/1000 | Loss: 0.00001476
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001476
Iteration 52/1000 | Loss: 0.00001476
Iteration 53/1000 | Loss: 0.00001476
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001475
Iteration 58/1000 | Loss: 0.00001475
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001474
Iteration 61/1000 | Loss: 0.00001474
Iteration 62/1000 | Loss: 0.00001474
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001473
Iteration 65/1000 | Loss: 0.00001473
Iteration 66/1000 | Loss: 0.00001473
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001472
Iteration 69/1000 | Loss: 0.00001472
Iteration 70/1000 | Loss: 0.00001472
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001472
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001471
Iteration 75/1000 | Loss: 0.00001471
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001469
Iteration 86/1000 | Loss: 0.00001469
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001469
Iteration 89/1000 | Loss: 0.00001468
Iteration 90/1000 | Loss: 0.00001468
Iteration 91/1000 | Loss: 0.00001468
Iteration 92/1000 | Loss: 0.00001468
Iteration 93/1000 | Loss: 0.00001468
Iteration 94/1000 | Loss: 0.00001468
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001468
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001467
Iteration 99/1000 | Loss: 0.00001467
Iteration 100/1000 | Loss: 0.00001467
Iteration 101/1000 | Loss: 0.00001467
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001466
Iteration 106/1000 | Loss: 0.00001466
Iteration 107/1000 | Loss: 0.00001466
Iteration 108/1000 | Loss: 0.00001466
Iteration 109/1000 | Loss: 0.00001465
Iteration 110/1000 | Loss: 0.00001465
Iteration 111/1000 | Loss: 0.00001465
Iteration 112/1000 | Loss: 0.00001465
Iteration 113/1000 | Loss: 0.00001465
Iteration 114/1000 | Loss: 0.00001465
Iteration 115/1000 | Loss: 0.00001465
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001464
Iteration 118/1000 | Loss: 0.00001464
Iteration 119/1000 | Loss: 0.00001464
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001462
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001462
Iteration 130/1000 | Loss: 0.00001462
Iteration 131/1000 | Loss: 0.00001461
Iteration 132/1000 | Loss: 0.00001461
Iteration 133/1000 | Loss: 0.00001461
Iteration 134/1000 | Loss: 0.00001461
Iteration 135/1000 | Loss: 0.00001461
Iteration 136/1000 | Loss: 0.00001461
Iteration 137/1000 | Loss: 0.00001461
Iteration 138/1000 | Loss: 0.00001461
Iteration 139/1000 | Loss: 0.00001461
Iteration 140/1000 | Loss: 0.00001460
Iteration 141/1000 | Loss: 0.00001460
Iteration 142/1000 | Loss: 0.00001460
Iteration 143/1000 | Loss: 0.00001460
Iteration 144/1000 | Loss: 0.00001460
Iteration 145/1000 | Loss: 0.00001460
Iteration 146/1000 | Loss: 0.00001460
Iteration 147/1000 | Loss: 0.00001460
Iteration 148/1000 | Loss: 0.00001460
Iteration 149/1000 | Loss: 0.00001460
Iteration 150/1000 | Loss: 0.00001460
Iteration 151/1000 | Loss: 0.00001460
Iteration 152/1000 | Loss: 0.00001460
Iteration 153/1000 | Loss: 0.00001460
Iteration 154/1000 | Loss: 0.00001460
Iteration 155/1000 | Loss: 0.00001459
Iteration 156/1000 | Loss: 0.00001459
Iteration 157/1000 | Loss: 0.00001459
Iteration 158/1000 | Loss: 0.00001459
Iteration 159/1000 | Loss: 0.00001459
Iteration 160/1000 | Loss: 0.00001459
Iteration 161/1000 | Loss: 0.00001459
Iteration 162/1000 | Loss: 0.00001459
Iteration 163/1000 | Loss: 0.00001459
Iteration 164/1000 | Loss: 0.00001459
Iteration 165/1000 | Loss: 0.00001459
Iteration 166/1000 | Loss: 0.00001459
Iteration 167/1000 | Loss: 0.00001459
Iteration 168/1000 | Loss: 0.00001459
Iteration 169/1000 | Loss: 0.00001459
Iteration 170/1000 | Loss: 0.00001459
Iteration 171/1000 | Loss: 0.00001459
Iteration 172/1000 | Loss: 0.00001459
Iteration 173/1000 | Loss: 0.00001459
Iteration 174/1000 | Loss: 0.00001459
Iteration 175/1000 | Loss: 0.00001459
Iteration 176/1000 | Loss: 0.00001459
Iteration 177/1000 | Loss: 0.00001459
Iteration 178/1000 | Loss: 0.00001459
Iteration 179/1000 | Loss: 0.00001459
Iteration 180/1000 | Loss: 0.00001459
Iteration 181/1000 | Loss: 0.00001459
Iteration 182/1000 | Loss: 0.00001459
Iteration 183/1000 | Loss: 0.00001459
Iteration 184/1000 | Loss: 0.00001459
Iteration 185/1000 | Loss: 0.00001459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.4591410035791341e-05, 1.4591410035791341e-05, 1.4591410035791341e-05, 1.4591410035791341e-05, 1.4591410035791341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4591410035791341e-05

Optimization complete. Final v2v error: 3.3051486015319824 mm

Highest mean error: 4.222211837768555 mm for frame 100

Lowest mean error: 2.8799939155578613 mm for frame 4

Saving results

Total time: 85.28137493133545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387814
Iteration 2/25 | Loss: 0.00117999
Iteration 3/25 | Loss: 0.00111639
Iteration 4/25 | Loss: 0.00110808
Iteration 5/25 | Loss: 0.00110459
Iteration 6/25 | Loss: 0.00110369
Iteration 7/25 | Loss: 0.00110369
Iteration 8/25 | Loss: 0.00110369
Iteration 9/25 | Loss: 0.00110369
Iteration 10/25 | Loss: 0.00110369
Iteration 11/25 | Loss: 0.00110369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011036861687898636, 0.0011036861687898636, 0.0011036861687898636, 0.0011036861687898636, 0.0011036861687898636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011036861687898636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42463517
Iteration 2/25 | Loss: 0.00055160
Iteration 3/25 | Loss: 0.00055160
Iteration 4/25 | Loss: 0.00055160
Iteration 5/25 | Loss: 0.00055160
Iteration 6/25 | Loss: 0.00055160
Iteration 7/25 | Loss: 0.00055160
Iteration 8/25 | Loss: 0.00055160
Iteration 9/25 | Loss: 0.00055160
Iteration 10/25 | Loss: 0.00055160
Iteration 11/25 | Loss: 0.00055160
Iteration 12/25 | Loss: 0.00055160
Iteration 13/25 | Loss: 0.00055160
Iteration 14/25 | Loss: 0.00055160
Iteration 15/25 | Loss: 0.00055160
Iteration 16/25 | Loss: 0.00055160
Iteration 17/25 | Loss: 0.00055160
Iteration 18/25 | Loss: 0.00055160
Iteration 19/25 | Loss: 0.00055160
Iteration 20/25 | Loss: 0.00055160
Iteration 21/25 | Loss: 0.00055160
Iteration 22/25 | Loss: 0.00055160
Iteration 23/25 | Loss: 0.00055160
Iteration 24/25 | Loss: 0.00055160
Iteration 25/25 | Loss: 0.00055160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055160
Iteration 2/1000 | Loss: 0.00002726
Iteration 3/1000 | Loss: 0.00001615
Iteration 4/1000 | Loss: 0.00001495
Iteration 5/1000 | Loss: 0.00001386
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001306
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001293
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001279
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001277
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001274
Iteration 25/1000 | Loss: 0.00001274
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001271
Iteration 28/1000 | Loss: 0.00001271
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001270
Iteration 31/1000 | Loss: 0.00001270
Iteration 32/1000 | Loss: 0.00001270
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001265
Iteration 38/1000 | Loss: 0.00001264
Iteration 39/1000 | Loss: 0.00001264
Iteration 40/1000 | Loss: 0.00001264
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001264
Iteration 43/1000 | Loss: 0.00001264
Iteration 44/1000 | Loss: 0.00001264
Iteration 45/1000 | Loss: 0.00001263
Iteration 46/1000 | Loss: 0.00001262
Iteration 47/1000 | Loss: 0.00001262
Iteration 48/1000 | Loss: 0.00001261
Iteration 49/1000 | Loss: 0.00001261
Iteration 50/1000 | Loss: 0.00001261
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001258
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001258
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001258
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001257
Iteration 67/1000 | Loss: 0.00001257
Iteration 68/1000 | Loss: 0.00001257
Iteration 69/1000 | Loss: 0.00001257
Iteration 70/1000 | Loss: 0.00001256
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001256
Iteration 73/1000 | Loss: 0.00001256
Iteration 74/1000 | Loss: 0.00001255
Iteration 75/1000 | Loss: 0.00001255
Iteration 76/1000 | Loss: 0.00001255
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001254
Iteration 82/1000 | Loss: 0.00001254
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001253
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001251
Iteration 101/1000 | Loss: 0.00001251
Iteration 102/1000 | Loss: 0.00001251
Iteration 103/1000 | Loss: 0.00001251
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001250
Iteration 106/1000 | Loss: 0.00001250
Iteration 107/1000 | Loss: 0.00001250
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001250
Iteration 110/1000 | Loss: 0.00001250
Iteration 111/1000 | Loss: 0.00001250
Iteration 112/1000 | Loss: 0.00001250
Iteration 113/1000 | Loss: 0.00001249
Iteration 114/1000 | Loss: 0.00001249
Iteration 115/1000 | Loss: 0.00001249
Iteration 116/1000 | Loss: 0.00001249
Iteration 117/1000 | Loss: 0.00001249
Iteration 118/1000 | Loss: 0.00001249
Iteration 119/1000 | Loss: 0.00001249
Iteration 120/1000 | Loss: 0.00001249
Iteration 121/1000 | Loss: 0.00001249
Iteration 122/1000 | Loss: 0.00001249
Iteration 123/1000 | Loss: 0.00001249
Iteration 124/1000 | Loss: 0.00001249
Iteration 125/1000 | Loss: 0.00001249
Iteration 126/1000 | Loss: 0.00001249
Iteration 127/1000 | Loss: 0.00001249
Iteration 128/1000 | Loss: 0.00001249
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001248
Iteration 135/1000 | Loss: 0.00001248
Iteration 136/1000 | Loss: 0.00001248
Iteration 137/1000 | Loss: 0.00001247
Iteration 138/1000 | Loss: 0.00001247
Iteration 139/1000 | Loss: 0.00001247
Iteration 140/1000 | Loss: 0.00001247
Iteration 141/1000 | Loss: 0.00001247
Iteration 142/1000 | Loss: 0.00001247
Iteration 143/1000 | Loss: 0.00001247
Iteration 144/1000 | Loss: 0.00001247
Iteration 145/1000 | Loss: 0.00001247
Iteration 146/1000 | Loss: 0.00001247
Iteration 147/1000 | Loss: 0.00001247
Iteration 148/1000 | Loss: 0.00001247
Iteration 149/1000 | Loss: 0.00001247
Iteration 150/1000 | Loss: 0.00001247
Iteration 151/1000 | Loss: 0.00001247
Iteration 152/1000 | Loss: 0.00001247
Iteration 153/1000 | Loss: 0.00001246
Iteration 154/1000 | Loss: 0.00001246
Iteration 155/1000 | Loss: 0.00001246
Iteration 156/1000 | Loss: 0.00001246
Iteration 157/1000 | Loss: 0.00001246
Iteration 158/1000 | Loss: 0.00001246
Iteration 159/1000 | Loss: 0.00001246
Iteration 160/1000 | Loss: 0.00001245
Iteration 161/1000 | Loss: 0.00001245
Iteration 162/1000 | Loss: 0.00001245
Iteration 163/1000 | Loss: 0.00001245
Iteration 164/1000 | Loss: 0.00001245
Iteration 165/1000 | Loss: 0.00001245
Iteration 166/1000 | Loss: 0.00001245
Iteration 167/1000 | Loss: 0.00001245
Iteration 168/1000 | Loss: 0.00001245
Iteration 169/1000 | Loss: 0.00001245
Iteration 170/1000 | Loss: 0.00001245
Iteration 171/1000 | Loss: 0.00001245
Iteration 172/1000 | Loss: 0.00001245
Iteration 173/1000 | Loss: 0.00001245
Iteration 174/1000 | Loss: 0.00001245
Iteration 175/1000 | Loss: 0.00001245
Iteration 176/1000 | Loss: 0.00001245
Iteration 177/1000 | Loss: 0.00001245
Iteration 178/1000 | Loss: 0.00001245
Iteration 179/1000 | Loss: 0.00001245
Iteration 180/1000 | Loss: 0.00001245
Iteration 181/1000 | Loss: 0.00001245
Iteration 182/1000 | Loss: 0.00001245
Iteration 183/1000 | Loss: 0.00001245
Iteration 184/1000 | Loss: 0.00001245
Iteration 185/1000 | Loss: 0.00001245
Iteration 186/1000 | Loss: 0.00001245
Iteration 187/1000 | Loss: 0.00001245
Iteration 188/1000 | Loss: 0.00001245
Iteration 189/1000 | Loss: 0.00001245
Iteration 190/1000 | Loss: 0.00001245
Iteration 191/1000 | Loss: 0.00001245
Iteration 192/1000 | Loss: 0.00001245
Iteration 193/1000 | Loss: 0.00001245
Iteration 194/1000 | Loss: 0.00001245
Iteration 195/1000 | Loss: 0.00001245
Iteration 196/1000 | Loss: 0.00001245
Iteration 197/1000 | Loss: 0.00001245
Iteration 198/1000 | Loss: 0.00001245
Iteration 199/1000 | Loss: 0.00001245
Iteration 200/1000 | Loss: 0.00001245
Iteration 201/1000 | Loss: 0.00001245
Iteration 202/1000 | Loss: 0.00001245
Iteration 203/1000 | Loss: 0.00001245
Iteration 204/1000 | Loss: 0.00001245
Iteration 205/1000 | Loss: 0.00001245
Iteration 206/1000 | Loss: 0.00001245
Iteration 207/1000 | Loss: 0.00001245
Iteration 208/1000 | Loss: 0.00001245
Iteration 209/1000 | Loss: 0.00001245
Iteration 210/1000 | Loss: 0.00001245
Iteration 211/1000 | Loss: 0.00001245
Iteration 212/1000 | Loss: 0.00001245
Iteration 213/1000 | Loss: 0.00001245
Iteration 214/1000 | Loss: 0.00001245
Iteration 215/1000 | Loss: 0.00001245
Iteration 216/1000 | Loss: 0.00001245
Iteration 217/1000 | Loss: 0.00001245
Iteration 218/1000 | Loss: 0.00001245
Iteration 219/1000 | Loss: 0.00001245
Iteration 220/1000 | Loss: 0.00001245
Iteration 221/1000 | Loss: 0.00001245
Iteration 222/1000 | Loss: 0.00001245
Iteration 223/1000 | Loss: 0.00001245
Iteration 224/1000 | Loss: 0.00001245
Iteration 225/1000 | Loss: 0.00001245
Iteration 226/1000 | Loss: 0.00001245
Iteration 227/1000 | Loss: 0.00001245
Iteration 228/1000 | Loss: 0.00001245
Iteration 229/1000 | Loss: 0.00001245
Iteration 230/1000 | Loss: 0.00001245
Iteration 231/1000 | Loss: 0.00001245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.2452605915314052e-05, 1.2452605915314052e-05, 1.2452605915314052e-05, 1.2452605915314052e-05, 1.2452605915314052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2452605915314052e-05

Optimization complete. Final v2v error: 3.029834032058716 mm

Highest mean error: 3.297220230102539 mm for frame 113

Lowest mean error: 2.637091875076294 mm for frame 4

Saving results

Total time: 33.69549107551575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00689333
Iteration 2/25 | Loss: 0.00156181
Iteration 3/25 | Loss: 0.00131292
Iteration 4/25 | Loss: 0.00127343
Iteration 5/25 | Loss: 0.00126110
Iteration 6/25 | Loss: 0.00125431
Iteration 7/25 | Loss: 0.00125097
Iteration 8/25 | Loss: 0.00125672
Iteration 9/25 | Loss: 0.00125123
Iteration 10/25 | Loss: 0.00124763
Iteration 11/25 | Loss: 0.00124679
Iteration 12/25 | Loss: 0.00124489
Iteration 13/25 | Loss: 0.00124267
Iteration 14/25 | Loss: 0.00124236
Iteration 15/25 | Loss: 0.00124230
Iteration 16/25 | Loss: 0.00124230
Iteration 17/25 | Loss: 0.00124230
Iteration 18/25 | Loss: 0.00124230
Iteration 19/25 | Loss: 0.00124230
Iteration 20/25 | Loss: 0.00124230
Iteration 21/25 | Loss: 0.00124229
Iteration 22/25 | Loss: 0.00124229
Iteration 23/25 | Loss: 0.00124229
Iteration 24/25 | Loss: 0.00124229
Iteration 25/25 | Loss: 0.00124229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11649871
Iteration 2/25 | Loss: 0.00064024
Iteration 3/25 | Loss: 0.00063998
Iteration 4/25 | Loss: 0.00063998
Iteration 5/25 | Loss: 0.00063998
Iteration 6/25 | Loss: 0.00063998
Iteration 7/25 | Loss: 0.00063998
Iteration 8/25 | Loss: 0.00063998
Iteration 9/25 | Loss: 0.00063998
Iteration 10/25 | Loss: 0.00063998
Iteration 11/25 | Loss: 0.00063998
Iteration 12/25 | Loss: 0.00063998
Iteration 13/25 | Loss: 0.00063998
Iteration 14/25 | Loss: 0.00063998
Iteration 15/25 | Loss: 0.00063998
Iteration 16/25 | Loss: 0.00063998
Iteration 17/25 | Loss: 0.00063998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000639978563413024, 0.000639978563413024, 0.000639978563413024, 0.000639978563413024, 0.000639978563413024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000639978563413024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063998
Iteration 2/1000 | Loss: 0.00006327
Iteration 3/1000 | Loss: 0.00003855
Iteration 4/1000 | Loss: 0.00003368
Iteration 5/1000 | Loss: 0.00003212
Iteration 6/1000 | Loss: 0.00003048
Iteration 7/1000 | Loss: 0.00002933
Iteration 8/1000 | Loss: 0.00002854
Iteration 9/1000 | Loss: 0.00002776
Iteration 10/1000 | Loss: 0.00002723
Iteration 11/1000 | Loss: 0.00002676
Iteration 12/1000 | Loss: 0.00002640
Iteration 13/1000 | Loss: 0.00002605
Iteration 14/1000 | Loss: 0.00002580
Iteration 15/1000 | Loss: 0.00002561
Iteration 16/1000 | Loss: 0.00002546
Iteration 17/1000 | Loss: 0.00002545
Iteration 18/1000 | Loss: 0.00002539
Iteration 19/1000 | Loss: 0.00002538
Iteration 20/1000 | Loss: 0.00002537
Iteration 21/1000 | Loss: 0.00002537
Iteration 22/1000 | Loss: 0.00002533
Iteration 23/1000 | Loss: 0.00002532
Iteration 24/1000 | Loss: 0.00002532
Iteration 25/1000 | Loss: 0.00002531
Iteration 26/1000 | Loss: 0.00002531
Iteration 27/1000 | Loss: 0.00002531
Iteration 28/1000 | Loss: 0.00002530
Iteration 29/1000 | Loss: 0.00002530
Iteration 30/1000 | Loss: 0.00002529
Iteration 31/1000 | Loss: 0.00002527
Iteration 32/1000 | Loss: 0.00002526
Iteration 33/1000 | Loss: 0.00002525
Iteration 34/1000 | Loss: 0.00002525
Iteration 35/1000 | Loss: 0.00002524
Iteration 36/1000 | Loss: 0.00002524
Iteration 37/1000 | Loss: 0.00002523
Iteration 38/1000 | Loss: 0.00002523
Iteration 39/1000 | Loss: 0.00002523
Iteration 40/1000 | Loss: 0.00002522
Iteration 41/1000 | Loss: 0.00002522
Iteration 42/1000 | Loss: 0.00002521
Iteration 43/1000 | Loss: 0.00002519
Iteration 44/1000 | Loss: 0.00002519
Iteration 45/1000 | Loss: 0.00002519
Iteration 46/1000 | Loss: 0.00002518
Iteration 47/1000 | Loss: 0.00002518
Iteration 48/1000 | Loss: 0.00002518
Iteration 49/1000 | Loss: 0.00002518
Iteration 50/1000 | Loss: 0.00002517
Iteration 51/1000 | Loss: 0.00002517
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002517
Iteration 54/1000 | Loss: 0.00002517
Iteration 55/1000 | Loss: 0.00002516
Iteration 56/1000 | Loss: 0.00002516
Iteration 57/1000 | Loss: 0.00002515
Iteration 58/1000 | Loss: 0.00002515
Iteration 59/1000 | Loss: 0.00002515
Iteration 60/1000 | Loss: 0.00002514
Iteration 61/1000 | Loss: 0.00002514
Iteration 62/1000 | Loss: 0.00002514
Iteration 63/1000 | Loss: 0.00002513
Iteration 64/1000 | Loss: 0.00002513
Iteration 65/1000 | Loss: 0.00002513
Iteration 66/1000 | Loss: 0.00002513
Iteration 67/1000 | Loss: 0.00002512
Iteration 68/1000 | Loss: 0.00002512
Iteration 69/1000 | Loss: 0.00002512
Iteration 70/1000 | Loss: 0.00002511
Iteration 71/1000 | Loss: 0.00002511
Iteration 72/1000 | Loss: 0.00002511
Iteration 73/1000 | Loss: 0.00002510
Iteration 74/1000 | Loss: 0.00002510
Iteration 75/1000 | Loss: 0.00002509
Iteration 76/1000 | Loss: 0.00002509
Iteration 77/1000 | Loss: 0.00002509
Iteration 78/1000 | Loss: 0.00002509
Iteration 79/1000 | Loss: 0.00002509
Iteration 80/1000 | Loss: 0.00002508
Iteration 81/1000 | Loss: 0.00002508
Iteration 82/1000 | Loss: 0.00002508
Iteration 83/1000 | Loss: 0.00002508
Iteration 84/1000 | Loss: 0.00002508
Iteration 85/1000 | Loss: 0.00002507
Iteration 86/1000 | Loss: 0.00002507
Iteration 87/1000 | Loss: 0.00002507
Iteration 88/1000 | Loss: 0.00002507
Iteration 89/1000 | Loss: 0.00002507
Iteration 90/1000 | Loss: 0.00002506
Iteration 91/1000 | Loss: 0.00002506
Iteration 92/1000 | Loss: 0.00002506
Iteration 93/1000 | Loss: 0.00002505
Iteration 94/1000 | Loss: 0.00002505
Iteration 95/1000 | Loss: 0.00002505
Iteration 96/1000 | Loss: 0.00002505
Iteration 97/1000 | Loss: 0.00002505
Iteration 98/1000 | Loss: 0.00002505
Iteration 99/1000 | Loss: 0.00002505
Iteration 100/1000 | Loss: 0.00002505
Iteration 101/1000 | Loss: 0.00002504
Iteration 102/1000 | Loss: 0.00002504
Iteration 103/1000 | Loss: 0.00002504
Iteration 104/1000 | Loss: 0.00002504
Iteration 105/1000 | Loss: 0.00002504
Iteration 106/1000 | Loss: 0.00002503
Iteration 107/1000 | Loss: 0.00002503
Iteration 108/1000 | Loss: 0.00002503
Iteration 109/1000 | Loss: 0.00002503
Iteration 110/1000 | Loss: 0.00002503
Iteration 111/1000 | Loss: 0.00002502
Iteration 112/1000 | Loss: 0.00002502
Iteration 113/1000 | Loss: 0.00002502
Iteration 114/1000 | Loss: 0.00002502
Iteration 115/1000 | Loss: 0.00002502
Iteration 116/1000 | Loss: 0.00002502
Iteration 117/1000 | Loss: 0.00002502
Iteration 118/1000 | Loss: 0.00002502
Iteration 119/1000 | Loss: 0.00002502
Iteration 120/1000 | Loss: 0.00002502
Iteration 121/1000 | Loss: 0.00002502
Iteration 122/1000 | Loss: 0.00002502
Iteration 123/1000 | Loss: 0.00002502
Iteration 124/1000 | Loss: 0.00002502
Iteration 125/1000 | Loss: 0.00002501
Iteration 126/1000 | Loss: 0.00002501
Iteration 127/1000 | Loss: 0.00002501
Iteration 128/1000 | Loss: 0.00002501
Iteration 129/1000 | Loss: 0.00002501
Iteration 130/1000 | Loss: 0.00002501
Iteration 131/1000 | Loss: 0.00002501
Iteration 132/1000 | Loss: 0.00002501
Iteration 133/1000 | Loss: 0.00002501
Iteration 134/1000 | Loss: 0.00002501
Iteration 135/1000 | Loss: 0.00002501
Iteration 136/1000 | Loss: 0.00002501
Iteration 137/1000 | Loss: 0.00002501
Iteration 138/1000 | Loss: 0.00002501
Iteration 139/1000 | Loss: 0.00002501
Iteration 140/1000 | Loss: 0.00002501
Iteration 141/1000 | Loss: 0.00002501
Iteration 142/1000 | Loss: 0.00002501
Iteration 143/1000 | Loss: 0.00002501
Iteration 144/1000 | Loss: 0.00002501
Iteration 145/1000 | Loss: 0.00002501
Iteration 146/1000 | Loss: 0.00002501
Iteration 147/1000 | Loss: 0.00002501
Iteration 148/1000 | Loss: 0.00002501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.5009827368194237e-05, 2.5009827368194237e-05, 2.5009827368194237e-05, 2.5009827368194237e-05, 2.5009827368194237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5009827368194237e-05

Optimization complete. Final v2v error: 4.093428611755371 mm

Highest mean error: 6.216987609863281 mm for frame 119

Lowest mean error: 3.1308062076568604 mm for frame 11

Saving results

Total time: 66.72277522087097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092438
Iteration 2/25 | Loss: 0.00285916
Iteration 3/25 | Loss: 0.00222702
Iteration 4/25 | Loss: 0.00201478
Iteration 5/25 | Loss: 0.00187777
Iteration 6/25 | Loss: 0.00157161
Iteration 7/25 | Loss: 0.00144703
Iteration 8/25 | Loss: 0.00134712
Iteration 9/25 | Loss: 0.00129903
Iteration 10/25 | Loss: 0.00125978
Iteration 11/25 | Loss: 0.00127699
Iteration 12/25 | Loss: 0.00123796
Iteration 13/25 | Loss: 0.00121910
Iteration 14/25 | Loss: 0.00123439
Iteration 15/25 | Loss: 0.00120484
Iteration 16/25 | Loss: 0.00120876
Iteration 17/25 | Loss: 0.00118392
Iteration 18/25 | Loss: 0.00119450
Iteration 19/25 | Loss: 0.00119130
Iteration 20/25 | Loss: 0.00118834
Iteration 21/25 | Loss: 0.00120055
Iteration 22/25 | Loss: 0.00119830
Iteration 23/25 | Loss: 0.00118059
Iteration 24/25 | Loss: 0.00117714
Iteration 25/25 | Loss: 0.00119726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36304390
Iteration 2/25 | Loss: 0.00369139
Iteration 3/25 | Loss: 0.00352564
Iteration 4/25 | Loss: 0.00352563
Iteration 5/25 | Loss: 0.00352563
Iteration 6/25 | Loss: 0.00352563
Iteration 7/25 | Loss: 0.00352563
Iteration 8/25 | Loss: 0.00352563
Iteration 9/25 | Loss: 0.00352563
Iteration 10/25 | Loss: 0.00352563
Iteration 11/25 | Loss: 0.00352563
Iteration 12/25 | Loss: 0.00352563
Iteration 13/25 | Loss: 0.00352563
Iteration 14/25 | Loss: 0.00352563
Iteration 15/25 | Loss: 0.00352563
Iteration 16/25 | Loss: 0.00352563
Iteration 17/25 | Loss: 0.00352563
Iteration 18/25 | Loss: 0.00352563
Iteration 19/25 | Loss: 0.00352563
Iteration 20/25 | Loss: 0.00352563
Iteration 21/25 | Loss: 0.00352563
Iteration 22/25 | Loss: 0.00352563
Iteration 23/25 | Loss: 0.00352563
Iteration 24/25 | Loss: 0.00352563
Iteration 25/25 | Loss: 0.00352563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00352563
Iteration 2/1000 | Loss: 0.00404241
Iteration 3/1000 | Loss: 0.00287220
Iteration 4/1000 | Loss: 0.00362172
Iteration 5/1000 | Loss: 0.00469857
Iteration 6/1000 | Loss: 0.00415348
Iteration 7/1000 | Loss: 0.00355090
Iteration 8/1000 | Loss: 0.00425993
Iteration 9/1000 | Loss: 0.00384761
Iteration 10/1000 | Loss: 0.00413509
Iteration 11/1000 | Loss: 0.00429037
Iteration 12/1000 | Loss: 0.00494653
Iteration 13/1000 | Loss: 0.00371525
Iteration 14/1000 | Loss: 0.00535217
Iteration 15/1000 | Loss: 0.00440045
Iteration 16/1000 | Loss: 0.00442840
Iteration 17/1000 | Loss: 0.00402972
Iteration 18/1000 | Loss: 0.00490089
Iteration 19/1000 | Loss: 0.00427100
Iteration 20/1000 | Loss: 0.00438577
Iteration 21/1000 | Loss: 0.00449792
Iteration 22/1000 | Loss: 0.00547986
Iteration 23/1000 | Loss: 0.00652651
Iteration 24/1000 | Loss: 0.00479236
Iteration 25/1000 | Loss: 0.00465811
Iteration 26/1000 | Loss: 0.00496166
Iteration 27/1000 | Loss: 0.00411997
Iteration 28/1000 | Loss: 0.00454348
Iteration 29/1000 | Loss: 0.00384496
Iteration 30/1000 | Loss: 0.00546439
Iteration 31/1000 | Loss: 0.00410926
Iteration 32/1000 | Loss: 0.00476053
Iteration 33/1000 | Loss: 0.00454744
Iteration 34/1000 | Loss: 0.00498309
Iteration 35/1000 | Loss: 0.00494942
Iteration 36/1000 | Loss: 0.00477727
Iteration 37/1000 | Loss: 0.00384414
Iteration 38/1000 | Loss: 0.00368759
Iteration 39/1000 | Loss: 0.00413397
Iteration 40/1000 | Loss: 0.00443428
Iteration 41/1000 | Loss: 0.00501122
Iteration 42/1000 | Loss: 0.00535698
Iteration 43/1000 | Loss: 0.00604478
Iteration 44/1000 | Loss: 0.00410309
Iteration 45/1000 | Loss: 0.00416432
Iteration 46/1000 | Loss: 0.00352950
Iteration 47/1000 | Loss: 0.00386387
Iteration 48/1000 | Loss: 0.00409726
Iteration 49/1000 | Loss: 0.00419418
Iteration 50/1000 | Loss: 0.00428006
Iteration 51/1000 | Loss: 0.00459753
Iteration 52/1000 | Loss: 0.00448037
Iteration 53/1000 | Loss: 0.00291604
Iteration 54/1000 | Loss: 0.00432507
Iteration 55/1000 | Loss: 0.00436870
Iteration 56/1000 | Loss: 0.00412924
Iteration 57/1000 | Loss: 0.00443528
Iteration 58/1000 | Loss: 0.00494491
Iteration 59/1000 | Loss: 0.00495770
Iteration 60/1000 | Loss: 0.00636738
Iteration 61/1000 | Loss: 0.00475153
Iteration 62/1000 | Loss: 0.00508709
Iteration 63/1000 | Loss: 0.00459810
Iteration 64/1000 | Loss: 0.00450504
Iteration 65/1000 | Loss: 0.00530162
Iteration 66/1000 | Loss: 0.00453329
Iteration 67/1000 | Loss: 0.00421275
Iteration 68/1000 | Loss: 0.00397205
Iteration 69/1000 | Loss: 0.00375926
Iteration 70/1000 | Loss: 0.00407192
Iteration 71/1000 | Loss: 0.00413908
Iteration 72/1000 | Loss: 0.00451119
Iteration 73/1000 | Loss: 0.00547504
Iteration 74/1000 | Loss: 0.00389095
Iteration 75/1000 | Loss: 0.00393885
Iteration 76/1000 | Loss: 0.00390510
Iteration 77/1000 | Loss: 0.00367166
Iteration 78/1000 | Loss: 0.00415922
Iteration 79/1000 | Loss: 0.00407212
Iteration 80/1000 | Loss: 0.00404130
Iteration 81/1000 | Loss: 0.00382547
Iteration 82/1000 | Loss: 0.00367148
Iteration 83/1000 | Loss: 0.00375868
Iteration 84/1000 | Loss: 0.00414033
Iteration 85/1000 | Loss: 0.00580156
Iteration 86/1000 | Loss: 0.00448107
Iteration 87/1000 | Loss: 0.00383350
Iteration 88/1000 | Loss: 0.00290606
Iteration 89/1000 | Loss: 0.00322006
Iteration 90/1000 | Loss: 0.00343188
Iteration 91/1000 | Loss: 0.00396376
Iteration 92/1000 | Loss: 0.00422643
Iteration 93/1000 | Loss: 0.00443875
Iteration 94/1000 | Loss: 0.00418207
Iteration 95/1000 | Loss: 0.00503650
Iteration 96/1000 | Loss: 0.00358571
Iteration 97/1000 | Loss: 0.00406090
Iteration 98/1000 | Loss: 0.00333842
Iteration 99/1000 | Loss: 0.00359949
Iteration 100/1000 | Loss: 0.00379359
Iteration 101/1000 | Loss: 0.00379573
Iteration 102/1000 | Loss: 0.00459913
Iteration 103/1000 | Loss: 0.00410918
Iteration 104/1000 | Loss: 0.00603151
Iteration 105/1000 | Loss: 0.00506204
Iteration 106/1000 | Loss: 0.00465786
Iteration 107/1000 | Loss: 0.00446043
Iteration 108/1000 | Loss: 0.00409682
Iteration 109/1000 | Loss: 0.00396742
Iteration 110/1000 | Loss: 0.00412819
Iteration 111/1000 | Loss: 0.00386829
Iteration 112/1000 | Loss: 0.00387013
Iteration 113/1000 | Loss: 0.00438331
Iteration 114/1000 | Loss: 0.00408647
Iteration 115/1000 | Loss: 0.00348640
Iteration 116/1000 | Loss: 0.00444570
Iteration 117/1000 | Loss: 0.00392175
Iteration 118/1000 | Loss: 0.00355732
Iteration 119/1000 | Loss: 0.00318825
Iteration 120/1000 | Loss: 0.00325198
Iteration 121/1000 | Loss: 0.00238564
Iteration 122/1000 | Loss: 0.00278348
Iteration 123/1000 | Loss: 0.00257683
Iteration 124/1000 | Loss: 0.00285992
Iteration 125/1000 | Loss: 0.00337532
Iteration 126/1000 | Loss: 0.00322179
Iteration 127/1000 | Loss: 0.00334578
Iteration 128/1000 | Loss: 0.00355351
Iteration 129/1000 | Loss: 0.00389352
Iteration 130/1000 | Loss: 0.00293709
Iteration 131/1000 | Loss: 0.00268718
Iteration 132/1000 | Loss: 0.00296449
Iteration 133/1000 | Loss: 0.00359985
Iteration 134/1000 | Loss: 0.00353082
Iteration 135/1000 | Loss: 0.00479161
Iteration 136/1000 | Loss: 0.00337716
Iteration 137/1000 | Loss: 0.00422104
Iteration 138/1000 | Loss: 0.00338967
Iteration 139/1000 | Loss: 0.00265554
Iteration 140/1000 | Loss: 0.00325047
Iteration 141/1000 | Loss: 0.00411101
Iteration 142/1000 | Loss: 0.00541051
Iteration 143/1000 | Loss: 0.00566709
Iteration 144/1000 | Loss: 0.00413658
Iteration 145/1000 | Loss: 0.00312435
Iteration 146/1000 | Loss: 0.00272581
Iteration 147/1000 | Loss: 0.00267701
Iteration 148/1000 | Loss: 0.00263609
Iteration 149/1000 | Loss: 0.00303871
Iteration 150/1000 | Loss: 0.00288961
Iteration 151/1000 | Loss: 0.00365042
Iteration 152/1000 | Loss: 0.00313203
Iteration 153/1000 | Loss: 0.00316763
Iteration 154/1000 | Loss: 0.00265517
Iteration 155/1000 | Loss: 0.00219248
Iteration 156/1000 | Loss: 0.00348843
Iteration 157/1000 | Loss: 0.00268975
Iteration 158/1000 | Loss: 0.00191319
Iteration 159/1000 | Loss: 0.00133278
Iteration 160/1000 | Loss: 0.00193654
Iteration 161/1000 | Loss: 0.00313498
Iteration 162/1000 | Loss: 0.00233496
Iteration 163/1000 | Loss: 0.00211190
Iteration 164/1000 | Loss: 0.00243782
Iteration 165/1000 | Loss: 0.00227291
Iteration 166/1000 | Loss: 0.00226366
Iteration 167/1000 | Loss: 0.00210979
Iteration 168/1000 | Loss: 0.00318062
Iteration 169/1000 | Loss: 0.00249280
Iteration 170/1000 | Loss: 0.00187999
Iteration 171/1000 | Loss: 0.00183835
Iteration 172/1000 | Loss: 0.00195695
Iteration 173/1000 | Loss: 0.00168663
Iteration 174/1000 | Loss: 0.00167921
Iteration 175/1000 | Loss: 0.00171590
Iteration 176/1000 | Loss: 0.00202134
Iteration 177/1000 | Loss: 0.00171037
Iteration 178/1000 | Loss: 0.00206997
Iteration 179/1000 | Loss: 0.00175404
Iteration 180/1000 | Loss: 0.00165243
Iteration 181/1000 | Loss: 0.00182869
Iteration 182/1000 | Loss: 0.00153169
Iteration 183/1000 | Loss: 0.00187881
Iteration 184/1000 | Loss: 0.00193564
Iteration 185/1000 | Loss: 0.00205818
Iteration 186/1000 | Loss: 0.00167884
Iteration 187/1000 | Loss: 0.00138965
Iteration 188/1000 | Loss: 0.00160724
Iteration 189/1000 | Loss: 0.00168634
Iteration 190/1000 | Loss: 0.00182733
Iteration 191/1000 | Loss: 0.00208843
Iteration 192/1000 | Loss: 0.00188711
Iteration 193/1000 | Loss: 0.00198176
Iteration 194/1000 | Loss: 0.00195925
Iteration 195/1000 | Loss: 0.00195350
Iteration 196/1000 | Loss: 0.00186124
Iteration 197/1000 | Loss: 0.00183422
Iteration 198/1000 | Loss: 0.00195817
Iteration 199/1000 | Loss: 0.00233352
Iteration 200/1000 | Loss: 0.00177923
Iteration 201/1000 | Loss: 0.00191244
Iteration 202/1000 | Loss: 0.00179377
Iteration 203/1000 | Loss: 0.00232315
Iteration 204/1000 | Loss: 0.00229843
Iteration 205/1000 | Loss: 0.00216534
Iteration 206/1000 | Loss: 0.00253090
Iteration 207/1000 | Loss: 0.00382015
Iteration 208/1000 | Loss: 0.00218212
Iteration 209/1000 | Loss: 0.00198032
Iteration 210/1000 | Loss: 0.00196260
Iteration 211/1000 | Loss: 0.00186417
Iteration 212/1000 | Loss: 0.00383920
Iteration 213/1000 | Loss: 0.00534936
Iteration 214/1000 | Loss: 0.00313585
Iteration 215/1000 | Loss: 0.00506687
Iteration 216/1000 | Loss: 0.00282554
Iteration 217/1000 | Loss: 0.00159280
Iteration 218/1000 | Loss: 0.00252693
Iteration 219/1000 | Loss: 0.00148286
Iteration 220/1000 | Loss: 0.00158947
Iteration 221/1000 | Loss: 0.00168853
Iteration 222/1000 | Loss: 0.00176775
Iteration 223/1000 | Loss: 0.00190529
Iteration 224/1000 | Loss: 0.00292708
Iteration 225/1000 | Loss: 0.00186100
Iteration 226/1000 | Loss: 0.00203124
Iteration 227/1000 | Loss: 0.00286715
Iteration 228/1000 | Loss: 0.00241585
Iteration 229/1000 | Loss: 0.00462870
Iteration 230/1000 | Loss: 0.00512928
Iteration 231/1000 | Loss: 0.00500418
Iteration 232/1000 | Loss: 0.00204360
Iteration 233/1000 | Loss: 0.00363195
Iteration 234/1000 | Loss: 0.00200286
Iteration 235/1000 | Loss: 0.00119531
Iteration 236/1000 | Loss: 0.00152898
Iteration 237/1000 | Loss: 0.00157242
Iteration 238/1000 | Loss: 0.00196861
Iteration 239/1000 | Loss: 0.00179948
Iteration 240/1000 | Loss: 0.00167249
Iteration 241/1000 | Loss: 0.00173091
Iteration 242/1000 | Loss: 0.00158501
Iteration 243/1000 | Loss: 0.00161929
Iteration 244/1000 | Loss: 0.00187299
Iteration 245/1000 | Loss: 0.00160799
Iteration 246/1000 | Loss: 0.00162266
Iteration 247/1000 | Loss: 0.00232321
Iteration 248/1000 | Loss: 0.00178939
Iteration 249/1000 | Loss: 0.00169375
Iteration 250/1000 | Loss: 0.00168109
Iteration 251/1000 | Loss: 0.00198816
Iteration 252/1000 | Loss: 0.00189086
Iteration 253/1000 | Loss: 0.00171644
Iteration 254/1000 | Loss: 0.00175184
Iteration 255/1000 | Loss: 0.00183801
Iteration 256/1000 | Loss: 0.00162726
Iteration 257/1000 | Loss: 0.00159819
Iteration 258/1000 | Loss: 0.00171910
Iteration 259/1000 | Loss: 0.00224922
Iteration 260/1000 | Loss: 0.00159722
Iteration 261/1000 | Loss: 0.00177552
Iteration 262/1000 | Loss: 0.00180842
Iteration 263/1000 | Loss: 0.00183649
Iteration 264/1000 | Loss: 0.00178273
Iteration 265/1000 | Loss: 0.00151641
Iteration 266/1000 | Loss: 0.00149158
Iteration 267/1000 | Loss: 0.00184205
Iteration 268/1000 | Loss: 0.00170394
Iteration 269/1000 | Loss: 0.00167465
Iteration 270/1000 | Loss: 0.00166843
Iteration 271/1000 | Loss: 0.00163400
Iteration 272/1000 | Loss: 0.00160120
Iteration 273/1000 | Loss: 0.00166892
Iteration 274/1000 | Loss: 0.00160852
Iteration 275/1000 | Loss: 0.00158737
Iteration 276/1000 | Loss: 0.00170693
Iteration 277/1000 | Loss: 0.00192471
Iteration 278/1000 | Loss: 0.00172986
Iteration 279/1000 | Loss: 0.00151336
Iteration 280/1000 | Loss: 0.00174322
Iteration 281/1000 | Loss: 0.00173243
Iteration 282/1000 | Loss: 0.00163962
Iteration 283/1000 | Loss: 0.00174994
Iteration 284/1000 | Loss: 0.00170724
Iteration 285/1000 | Loss: 0.00181896
Iteration 286/1000 | Loss: 0.00184925
Iteration 287/1000 | Loss: 0.00231775
Iteration 288/1000 | Loss: 0.00208515
Iteration 289/1000 | Loss: 0.00138454
Iteration 290/1000 | Loss: 0.00127262
Iteration 291/1000 | Loss: 0.00181212
Iteration 292/1000 | Loss: 0.00198298
Iteration 293/1000 | Loss: 0.00151856
Iteration 294/1000 | Loss: 0.00139878
Iteration 295/1000 | Loss: 0.00131370
Iteration 296/1000 | Loss: 0.00150127
Iteration 297/1000 | Loss: 0.00137972
Iteration 298/1000 | Loss: 0.00154563
Iteration 299/1000 | Loss: 0.00151704
Iteration 300/1000 | Loss: 0.00153020
Iteration 301/1000 | Loss: 0.00144699
Iteration 302/1000 | Loss: 0.00154246
Iteration 303/1000 | Loss: 0.00156752
Iteration 304/1000 | Loss: 0.00202952
Iteration 305/1000 | Loss: 0.00239890
Iteration 306/1000 | Loss: 0.00129700
Iteration 307/1000 | Loss: 0.00161720
Iteration 308/1000 | Loss: 0.00174986
Iteration 309/1000 | Loss: 0.00174378
Iteration 310/1000 | Loss: 0.00180671
Iteration 311/1000 | Loss: 0.00172276
Iteration 312/1000 | Loss: 0.00211035
Iteration 313/1000 | Loss: 0.00232074
Iteration 314/1000 | Loss: 0.00186883
Iteration 315/1000 | Loss: 0.00201094
Iteration 316/1000 | Loss: 0.00180286
Iteration 317/1000 | Loss: 0.00189260
Iteration 318/1000 | Loss: 0.00185689
Iteration 319/1000 | Loss: 0.00208445
Iteration 320/1000 | Loss: 0.00220585
Iteration 321/1000 | Loss: 0.00165768
Iteration 322/1000 | Loss: 0.00120011
Iteration 323/1000 | Loss: 0.00166564
Iteration 324/1000 | Loss: 0.00170121
Iteration 325/1000 | Loss: 0.00174626
Iteration 326/1000 | Loss: 0.00191845
Iteration 327/1000 | Loss: 0.00197826
Iteration 328/1000 | Loss: 0.00184939
Iteration 329/1000 | Loss: 0.00194034
Iteration 330/1000 | Loss: 0.00180306
Iteration 331/1000 | Loss: 0.00188613
Iteration 332/1000 | Loss: 0.00158758
Iteration 333/1000 | Loss: 0.00248092
Iteration 334/1000 | Loss: 0.00268528
Iteration 335/1000 | Loss: 0.00271086
Iteration 336/1000 | Loss: 0.00200024
Iteration 337/1000 | Loss: 0.00177164
Iteration 338/1000 | Loss: 0.00169634
Iteration 339/1000 | Loss: 0.00188445
Iteration 340/1000 | Loss: 0.00163311
Iteration 341/1000 | Loss: 0.00148490
Iteration 342/1000 | Loss: 0.00151819
Iteration 343/1000 | Loss: 0.00149744
Iteration 344/1000 | Loss: 0.00229827
Iteration 345/1000 | Loss: 0.00193372
Iteration 346/1000 | Loss: 0.00215480
Iteration 347/1000 | Loss: 0.00213209
Iteration 348/1000 | Loss: 0.00201086
Iteration 349/1000 | Loss: 0.00202056
Iteration 350/1000 | Loss: 0.00189464
Iteration 351/1000 | Loss: 0.00194224
Iteration 352/1000 | Loss: 0.00208216
Iteration 353/1000 | Loss: 0.00164035
Iteration 354/1000 | Loss: 0.00167369
Iteration 355/1000 | Loss: 0.00174903
Iteration 356/1000 | Loss: 0.00174292
Iteration 357/1000 | Loss: 0.00189017
Iteration 358/1000 | Loss: 0.00200604
Iteration 359/1000 | Loss: 0.00211876
Iteration 360/1000 | Loss: 0.00178992
Iteration 361/1000 | Loss: 0.00224008
Iteration 362/1000 | Loss: 0.00257816
Iteration 363/1000 | Loss: 0.00274141
Iteration 364/1000 | Loss: 0.00342343
Iteration 365/1000 | Loss: 0.00186539
Iteration 366/1000 | Loss: 0.00123792
Iteration 367/1000 | Loss: 0.00048777
Iteration 368/1000 | Loss: 0.00043508
Iteration 369/1000 | Loss: 0.00022189
Iteration 370/1000 | Loss: 0.00092611
Iteration 371/1000 | Loss: 0.00058652
Iteration 372/1000 | Loss: 0.00049266
Iteration 373/1000 | Loss: 0.00048460
Iteration 374/1000 | Loss: 0.00054239
Iteration 375/1000 | Loss: 0.00046046
Iteration 376/1000 | Loss: 0.00047602
Iteration 377/1000 | Loss: 0.00054629
Iteration 378/1000 | Loss: 0.00040461
Iteration 379/1000 | Loss: 0.00066370
Iteration 380/1000 | Loss: 0.00059153
Iteration 381/1000 | Loss: 0.00055782
Iteration 382/1000 | Loss: 0.00071531
Iteration 383/1000 | Loss: 0.00048649
Iteration 384/1000 | Loss: 0.00104200
Iteration 385/1000 | Loss: 0.00055111
Iteration 386/1000 | Loss: 0.00045101
Iteration 387/1000 | Loss: 0.00042980
Iteration 388/1000 | Loss: 0.00050002
Iteration 389/1000 | Loss: 0.00068577
Iteration 390/1000 | Loss: 0.00062719
Iteration 391/1000 | Loss: 0.00088585
Iteration 392/1000 | Loss: 0.00095371
Iteration 393/1000 | Loss: 0.00063271
Iteration 394/1000 | Loss: 0.00046321
Iteration 395/1000 | Loss: 0.00044680
Iteration 396/1000 | Loss: 0.00049001
Iteration 397/1000 | Loss: 0.00052135
Iteration 398/1000 | Loss: 0.00076213
Iteration 399/1000 | Loss: 0.00072072
Iteration 400/1000 | Loss: 0.00080429
Iteration 401/1000 | Loss: 0.00068142
Iteration 402/1000 | Loss: 0.00097595
Iteration 403/1000 | Loss: 0.00071087
Iteration 404/1000 | Loss: 0.00053762
Iteration 405/1000 | Loss: 0.00034392
Iteration 406/1000 | Loss: 0.00047457
Iteration 407/1000 | Loss: 0.00052885
Iteration 408/1000 | Loss: 0.00038540
Iteration 409/1000 | Loss: 0.00055572
Iteration 410/1000 | Loss: 0.00078411
Iteration 411/1000 | Loss: 0.00069474
Iteration 412/1000 | Loss: 0.00062415
Iteration 413/1000 | Loss: 0.00088069
Iteration 414/1000 | Loss: 0.00067010
Iteration 415/1000 | Loss: 0.00050027
Iteration 416/1000 | Loss: 0.00030200
Iteration 417/1000 | Loss: 0.00043206
Iteration 418/1000 | Loss: 0.00057394
Iteration 419/1000 | Loss: 0.00056989
Iteration 420/1000 | Loss: 0.00047339
Iteration 421/1000 | Loss: 0.00063063
Iteration 422/1000 | Loss: 0.00072191
Iteration 423/1000 | Loss: 0.00065803
Iteration 424/1000 | Loss: 0.00053302
Iteration 425/1000 | Loss: 0.00045314
Iteration 426/1000 | Loss: 0.00044894
Iteration 427/1000 | Loss: 0.00039955
Iteration 428/1000 | Loss: 0.00046348
Iteration 429/1000 | Loss: 0.00042723
Iteration 430/1000 | Loss: 0.00042804
Iteration 431/1000 | Loss: 0.00045895
Iteration 432/1000 | Loss: 0.00059922
Iteration 433/1000 | Loss: 0.00048596
Iteration 434/1000 | Loss: 0.00061309
Iteration 435/1000 | Loss: 0.00040565
Iteration 436/1000 | Loss: 0.00040718
Iteration 437/1000 | Loss: 0.00029909
Iteration 438/1000 | Loss: 0.00035522
Iteration 439/1000 | Loss: 0.00037656
Iteration 440/1000 | Loss: 0.00041393
Iteration 441/1000 | Loss: 0.00033101
Iteration 442/1000 | Loss: 0.00037173
Iteration 443/1000 | Loss: 0.00036572
Iteration 444/1000 | Loss: 0.00033919
Iteration 445/1000 | Loss: 0.00035587
Iteration 446/1000 | Loss: 0.00090792
Iteration 447/1000 | Loss: 0.00073176
Iteration 448/1000 | Loss: 0.00080952
Iteration 449/1000 | Loss: 0.00038127
Iteration 450/1000 | Loss: 0.00030167
Iteration 451/1000 | Loss: 0.00034669
Iteration 452/1000 | Loss: 0.00026281
Iteration 453/1000 | Loss: 0.00025980
Iteration 454/1000 | Loss: 0.00023253
Iteration 455/1000 | Loss: 0.00038377
Iteration 456/1000 | Loss: 0.00025445
Iteration 457/1000 | Loss: 0.00033785
Iteration 458/1000 | Loss: 0.00034895
Iteration 459/1000 | Loss: 0.00035276
Iteration 460/1000 | Loss: 0.00034498
Iteration 461/1000 | Loss: 0.00034290
Iteration 462/1000 | Loss: 0.00027034
Iteration 463/1000 | Loss: 0.00031346
Iteration 464/1000 | Loss: 0.00028788
Iteration 465/1000 | Loss: 0.00033333
Iteration 466/1000 | Loss: 0.00029789
Iteration 467/1000 | Loss: 0.00032838
Iteration 468/1000 | Loss: 0.00028651
Iteration 469/1000 | Loss: 0.00031011
Iteration 470/1000 | Loss: 0.00040370
Iteration 471/1000 | Loss: 0.00029618
Iteration 472/1000 | Loss: 0.00028721
Iteration 473/1000 | Loss: 0.00029106
Iteration 474/1000 | Loss: 0.00030592
Iteration 475/1000 | Loss: 0.00030359
Iteration 476/1000 | Loss: 0.00038945
Iteration 477/1000 | Loss: 0.00031000
Iteration 478/1000 | Loss: 0.00020288
Iteration 479/1000 | Loss: 0.00030206
Iteration 480/1000 | Loss: 0.00013797
Iteration 481/1000 | Loss: 0.00015840
Iteration 482/1000 | Loss: 0.00044799
Iteration 483/1000 | Loss: 0.00024300
Iteration 484/1000 | Loss: 0.00018154
Iteration 485/1000 | Loss: 0.00015860
Iteration 486/1000 | Loss: 0.00017886
Iteration 487/1000 | Loss: 0.00025858
Iteration 488/1000 | Loss: 0.00024696
Iteration 489/1000 | Loss: 0.00030049
Iteration 490/1000 | Loss: 0.00028458
Iteration 491/1000 | Loss: 0.00030667
Iteration 492/1000 | Loss: 0.00030386
Iteration 493/1000 | Loss: 0.00034004
Iteration 494/1000 | Loss: 0.00022343
Iteration 495/1000 | Loss: 0.00025487
Iteration 496/1000 | Loss: 0.00028392
Iteration 497/1000 | Loss: 0.00025664
Iteration 498/1000 | Loss: 0.00027511
Iteration 499/1000 | Loss: 0.00027551
Iteration 500/1000 | Loss: 0.00028748
Iteration 501/1000 | Loss: 0.00018874
Iteration 502/1000 | Loss: 0.00028462
Iteration 503/1000 | Loss: 0.00028346
Iteration 504/1000 | Loss: 0.00027166
Iteration 505/1000 | Loss: 0.00023631
Iteration 506/1000 | Loss: 0.00024441
Iteration 507/1000 | Loss: 0.00031457
Iteration 508/1000 | Loss: 0.00004559
Iteration 509/1000 | Loss: 0.00007210
Iteration 510/1000 | Loss: 0.00012378
Iteration 511/1000 | Loss: 0.00023188
Iteration 512/1000 | Loss: 0.00008273
Iteration 513/1000 | Loss: 0.00003392
Iteration 514/1000 | Loss: 0.00002537
Iteration 515/1000 | Loss: 0.00009187
Iteration 516/1000 | Loss: 0.00002582
Iteration 517/1000 | Loss: 0.00002354
Iteration 518/1000 | Loss: 0.00002240
Iteration 519/1000 | Loss: 0.00002147
Iteration 520/1000 | Loss: 0.00002075
Iteration 521/1000 | Loss: 0.00002028
Iteration 522/1000 | Loss: 0.00001994
Iteration 523/1000 | Loss: 0.00001972
Iteration 524/1000 | Loss: 0.00001968
Iteration 525/1000 | Loss: 0.00001965
Iteration 526/1000 | Loss: 0.00001964
Iteration 527/1000 | Loss: 0.00001956
Iteration 528/1000 | Loss: 0.00001953
Iteration 529/1000 | Loss: 0.00001949
Iteration 530/1000 | Loss: 0.00001948
Iteration 531/1000 | Loss: 0.00001945
Iteration 532/1000 | Loss: 0.00001945
Iteration 533/1000 | Loss: 0.00001944
Iteration 534/1000 | Loss: 0.00001943
Iteration 535/1000 | Loss: 0.00001938
Iteration 536/1000 | Loss: 0.00001930
Iteration 537/1000 | Loss: 0.00001930
Iteration 538/1000 | Loss: 0.00001930
Iteration 539/1000 | Loss: 0.00001927
Iteration 540/1000 | Loss: 0.00001927
Iteration 541/1000 | Loss: 0.00001926
Iteration 542/1000 | Loss: 0.00001926
Iteration 543/1000 | Loss: 0.00001925
Iteration 544/1000 | Loss: 0.00001925
Iteration 545/1000 | Loss: 0.00001925
Iteration 546/1000 | Loss: 0.00001925
Iteration 547/1000 | Loss: 0.00001924
Iteration 548/1000 | Loss: 0.00001924
Iteration 549/1000 | Loss: 0.00001924
Iteration 550/1000 | Loss: 0.00001923
Iteration 551/1000 | Loss: 0.00001923
Iteration 552/1000 | Loss: 0.00001921
Iteration 553/1000 | Loss: 0.00001921
Iteration 554/1000 | Loss: 0.00001921
Iteration 555/1000 | Loss: 0.00001921
Iteration 556/1000 | Loss: 0.00001920
Iteration 557/1000 | Loss: 0.00001920
Iteration 558/1000 | Loss: 0.00001920
Iteration 559/1000 | Loss: 0.00001920
Iteration 560/1000 | Loss: 0.00001918
Iteration 561/1000 | Loss: 0.00001918
Iteration 562/1000 | Loss: 0.00001918
Iteration 563/1000 | Loss: 0.00001917
Iteration 564/1000 | Loss: 0.00001917
Iteration 565/1000 | Loss: 0.00001917
Iteration 566/1000 | Loss: 0.00001917
Iteration 567/1000 | Loss: 0.00001916
Iteration 568/1000 | Loss: 0.00001916
Iteration 569/1000 | Loss: 0.00001916
Iteration 570/1000 | Loss: 0.00001916
Iteration 571/1000 | Loss: 0.00001915
Iteration 572/1000 | Loss: 0.00001915
Iteration 573/1000 | Loss: 0.00001915
Iteration 574/1000 | Loss: 0.00001915
Iteration 575/1000 | Loss: 0.00001915
Iteration 576/1000 | Loss: 0.00001914
Iteration 577/1000 | Loss: 0.00001914
Iteration 578/1000 | Loss: 0.00001914
Iteration 579/1000 | Loss: 0.00001914
Iteration 580/1000 | Loss: 0.00001914
Iteration 581/1000 | Loss: 0.00001914
Iteration 582/1000 | Loss: 0.00001914
Iteration 583/1000 | Loss: 0.00001914
Iteration 584/1000 | Loss: 0.00001914
Iteration 585/1000 | Loss: 0.00001913
Iteration 586/1000 | Loss: 0.00001913
Iteration 587/1000 | Loss: 0.00001913
Iteration 588/1000 | Loss: 0.00001913
Iteration 589/1000 | Loss: 0.00001913
Iteration 590/1000 | Loss: 0.00001913
Iteration 591/1000 | Loss: 0.00001913
Iteration 592/1000 | Loss: 0.00001913
Iteration 593/1000 | Loss: 0.00001912
Iteration 594/1000 | Loss: 0.00001912
Iteration 595/1000 | Loss: 0.00001912
Iteration 596/1000 | Loss: 0.00001912
Iteration 597/1000 | Loss: 0.00001912
Iteration 598/1000 | Loss: 0.00001912
Iteration 599/1000 | Loss: 0.00001912
Iteration 600/1000 | Loss: 0.00001912
Iteration 601/1000 | Loss: 0.00001912
Iteration 602/1000 | Loss: 0.00001911
Iteration 603/1000 | Loss: 0.00001911
Iteration 604/1000 | Loss: 0.00001911
Iteration 605/1000 | Loss: 0.00001911
Iteration 606/1000 | Loss: 0.00001911
Iteration 607/1000 | Loss: 0.00001911
Iteration 608/1000 | Loss: 0.00001911
Iteration 609/1000 | Loss: 0.00001911
Iteration 610/1000 | Loss: 0.00001911
Iteration 611/1000 | Loss: 0.00001910
Iteration 612/1000 | Loss: 0.00001910
Iteration 613/1000 | Loss: 0.00001910
Iteration 614/1000 | Loss: 0.00001910
Iteration 615/1000 | Loss: 0.00001910
Iteration 616/1000 | Loss: 0.00001910
Iteration 617/1000 | Loss: 0.00001910
Iteration 618/1000 | Loss: 0.00001910
Iteration 619/1000 | Loss: 0.00001909
Iteration 620/1000 | Loss: 0.00001909
Iteration 621/1000 | Loss: 0.00001909
Iteration 622/1000 | Loss: 0.00001909
Iteration 623/1000 | Loss: 0.00001909
Iteration 624/1000 | Loss: 0.00001909
Iteration 625/1000 | Loss: 0.00001909
Iteration 626/1000 | Loss: 0.00001909
Iteration 627/1000 | Loss: 0.00001909
Iteration 628/1000 | Loss: 0.00001909
Iteration 629/1000 | Loss: 0.00001909
Iteration 630/1000 | Loss: 0.00001909
Iteration 631/1000 | Loss: 0.00001909
Iteration 632/1000 | Loss: 0.00001909
Iteration 633/1000 | Loss: 0.00001909
Iteration 634/1000 | Loss: 0.00001908
Iteration 635/1000 | Loss: 0.00001908
Iteration 636/1000 | Loss: 0.00001908
Iteration 637/1000 | Loss: 0.00001908
Iteration 638/1000 | Loss: 0.00001908
Iteration 639/1000 | Loss: 0.00001908
Iteration 640/1000 | Loss: 0.00001908
Iteration 641/1000 | Loss: 0.00001908
Iteration 642/1000 | Loss: 0.00001908
Iteration 643/1000 | Loss: 0.00001908
Iteration 644/1000 | Loss: 0.00001908
Iteration 645/1000 | Loss: 0.00001908
Iteration 646/1000 | Loss: 0.00001908
Iteration 647/1000 | Loss: 0.00001908
Iteration 648/1000 | Loss: 0.00001908
Iteration 649/1000 | Loss: 0.00001908
Iteration 650/1000 | Loss: 0.00001908
Iteration 651/1000 | Loss: 0.00001908
Iteration 652/1000 | Loss: 0.00001908
Iteration 653/1000 | Loss: 0.00001908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 653. Stopping optimization.
Last 5 losses: [1.908002741402015e-05, 1.908002741402015e-05, 1.908002741402015e-05, 1.908002741402015e-05, 1.908002741402015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.908002741402015e-05

Optimization complete. Final v2v error: 2.9940061569213867 mm

Highest mean error: 20.41265296936035 mm for frame 29

Lowest mean error: 2.4173495769500732 mm for frame 151

Saving results

Total time: 833.7566120624542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977278
Iteration 2/25 | Loss: 0.00194598
Iteration 3/25 | Loss: 0.00158866
Iteration 4/25 | Loss: 0.00150691
Iteration 5/25 | Loss: 0.00146480
Iteration 6/25 | Loss: 0.00140393
Iteration 7/25 | Loss: 0.00133573
Iteration 8/25 | Loss: 0.00130007
Iteration 9/25 | Loss: 0.00128805
Iteration 10/25 | Loss: 0.00127448
Iteration 11/25 | Loss: 0.00127595
Iteration 12/25 | Loss: 0.00127695
Iteration 13/25 | Loss: 0.00127329
Iteration 14/25 | Loss: 0.00127216
Iteration 15/25 | Loss: 0.00127816
Iteration 16/25 | Loss: 0.00127239
Iteration 17/25 | Loss: 0.00126595
Iteration 18/25 | Loss: 0.00126227
Iteration 19/25 | Loss: 0.00126047
Iteration 20/25 | Loss: 0.00126008
Iteration 21/25 | Loss: 0.00125998
Iteration 22/25 | Loss: 0.00125998
Iteration 23/25 | Loss: 0.00125997
Iteration 24/25 | Loss: 0.00125997
Iteration 25/25 | Loss: 0.00125997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45925796
Iteration 2/25 | Loss: 0.00093028
Iteration 3/25 | Loss: 0.00093027
Iteration 4/25 | Loss: 0.00093027
Iteration 5/25 | Loss: 0.00093027
Iteration 6/25 | Loss: 0.00093027
Iteration 7/25 | Loss: 0.00093027
Iteration 8/25 | Loss: 0.00093027
Iteration 9/25 | Loss: 0.00093027
Iteration 10/25 | Loss: 0.00093027
Iteration 11/25 | Loss: 0.00093027
Iteration 12/25 | Loss: 0.00093027
Iteration 13/25 | Loss: 0.00093027
Iteration 14/25 | Loss: 0.00093027
Iteration 15/25 | Loss: 0.00093027
Iteration 16/25 | Loss: 0.00093027
Iteration 17/25 | Loss: 0.00093027
Iteration 18/25 | Loss: 0.00093027
Iteration 19/25 | Loss: 0.00093027
Iteration 20/25 | Loss: 0.00093027
Iteration 21/25 | Loss: 0.00093027
Iteration 22/25 | Loss: 0.00093027
Iteration 23/25 | Loss: 0.00093027
Iteration 24/25 | Loss: 0.00093027
Iteration 25/25 | Loss: 0.00093027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093027
Iteration 2/1000 | Loss: 0.00013366
Iteration 3/1000 | Loss: 0.00008457
Iteration 4/1000 | Loss: 0.00006842
Iteration 5/1000 | Loss: 0.00025789
Iteration 6/1000 | Loss: 0.00005947
Iteration 7/1000 | Loss: 0.00005059
Iteration 8/1000 | Loss: 0.00156375
Iteration 9/1000 | Loss: 0.00016048
Iteration 10/1000 | Loss: 0.00004193
Iteration 11/1000 | Loss: 0.00003339
Iteration 12/1000 | Loss: 0.00002787
Iteration 13/1000 | Loss: 0.00002448
Iteration 14/1000 | Loss: 0.00002307
Iteration 15/1000 | Loss: 0.00002183
Iteration 16/1000 | Loss: 0.00002125
Iteration 17/1000 | Loss: 0.00002083
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002005
Iteration 20/1000 | Loss: 0.00001975
Iteration 21/1000 | Loss: 0.00001966
Iteration 22/1000 | Loss: 0.00001951
Iteration 23/1000 | Loss: 0.00001950
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001948
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001944
Iteration 29/1000 | Loss: 0.00001943
Iteration 30/1000 | Loss: 0.00001939
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001938
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001936
Iteration 36/1000 | Loss: 0.00001936
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001936
Iteration 39/1000 | Loss: 0.00001936
Iteration 40/1000 | Loss: 0.00001936
Iteration 41/1000 | Loss: 0.00001935
Iteration 42/1000 | Loss: 0.00001935
Iteration 43/1000 | Loss: 0.00001935
Iteration 44/1000 | Loss: 0.00001935
Iteration 45/1000 | Loss: 0.00001935
Iteration 46/1000 | Loss: 0.00001935
Iteration 47/1000 | Loss: 0.00001935
Iteration 48/1000 | Loss: 0.00001935
Iteration 49/1000 | Loss: 0.00001935
Iteration 50/1000 | Loss: 0.00001935
Iteration 51/1000 | Loss: 0.00001934
Iteration 52/1000 | Loss: 0.00001934
Iteration 53/1000 | Loss: 0.00001934
Iteration 54/1000 | Loss: 0.00001934
Iteration 55/1000 | Loss: 0.00001934
Iteration 56/1000 | Loss: 0.00001934
Iteration 57/1000 | Loss: 0.00001934
Iteration 58/1000 | Loss: 0.00001934
Iteration 59/1000 | Loss: 0.00001934
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001934
Iteration 63/1000 | Loss: 0.00001934
Iteration 64/1000 | Loss: 0.00001934
Iteration 65/1000 | Loss: 0.00001934
Iteration 66/1000 | Loss: 0.00001933
Iteration 67/1000 | Loss: 0.00001933
Iteration 68/1000 | Loss: 0.00001933
Iteration 69/1000 | Loss: 0.00001933
Iteration 70/1000 | Loss: 0.00001933
Iteration 71/1000 | Loss: 0.00001933
Iteration 72/1000 | Loss: 0.00001933
Iteration 73/1000 | Loss: 0.00001933
Iteration 74/1000 | Loss: 0.00001933
Iteration 75/1000 | Loss: 0.00001933
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00001933
Iteration 81/1000 | Loss: 0.00001933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.9334487660671584e-05, 1.9334487660671584e-05, 1.9334487660671584e-05, 1.9334487660671584e-05, 1.9334487660671584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9334487660671584e-05

Optimization complete. Final v2v error: 3.721599578857422 mm

Highest mean error: 4.267251014709473 mm for frame 94

Lowest mean error: 3.135662317276001 mm for frame 131

Saving results

Total time: 70.89730787277222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402574
Iteration 2/25 | Loss: 0.00126909
Iteration 3/25 | Loss: 0.00118424
Iteration 4/25 | Loss: 0.00116828
Iteration 5/25 | Loss: 0.00116415
Iteration 6/25 | Loss: 0.00116415
Iteration 7/25 | Loss: 0.00116415
Iteration 8/25 | Loss: 0.00116415
Iteration 9/25 | Loss: 0.00116415
Iteration 10/25 | Loss: 0.00116415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011641480959951878, 0.0011641480959951878, 0.0011641480959951878, 0.0011641480959951878, 0.0011641480959951878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011641480959951878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48910606
Iteration 2/25 | Loss: 0.00047312
Iteration 3/25 | Loss: 0.00047311
Iteration 4/25 | Loss: 0.00047311
Iteration 5/25 | Loss: 0.00047311
Iteration 6/25 | Loss: 0.00047311
Iteration 7/25 | Loss: 0.00047311
Iteration 8/25 | Loss: 0.00047311
Iteration 9/25 | Loss: 0.00047311
Iteration 10/25 | Loss: 0.00047311
Iteration 11/25 | Loss: 0.00047311
Iteration 12/25 | Loss: 0.00047311
Iteration 13/25 | Loss: 0.00047311
Iteration 14/25 | Loss: 0.00047311
Iteration 15/25 | Loss: 0.00047311
Iteration 16/25 | Loss: 0.00047311
Iteration 17/25 | Loss: 0.00047311
Iteration 18/25 | Loss: 0.00047311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00047310686204582453, 0.00047310686204582453, 0.00047310686204582453, 0.00047310686204582453, 0.00047310686204582453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047310686204582453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047311
Iteration 2/1000 | Loss: 0.00002984
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001909
Iteration 5/1000 | Loss: 0.00001755
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00001607
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001537
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001512
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001511
Iteration 22/1000 | Loss: 0.00001508
Iteration 23/1000 | Loss: 0.00001507
Iteration 24/1000 | Loss: 0.00001507
Iteration 25/1000 | Loss: 0.00001507
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001506
Iteration 28/1000 | Loss: 0.00001505
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001503
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001497
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001490
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001487
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001486
Iteration 66/1000 | Loss: 0.00001486
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001486
Iteration 71/1000 | Loss: 0.00001486
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001486
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001486
Iteration 78/1000 | Loss: 0.00001486
Iteration 79/1000 | Loss: 0.00001486
Iteration 80/1000 | Loss: 0.00001486
Iteration 81/1000 | Loss: 0.00001486
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.485526081523858e-05, 1.485526081523858e-05, 1.485526081523858e-05, 1.485526081523858e-05, 1.485526081523858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.485526081523858e-05

Optimization complete. Final v2v error: 3.232851505279541 mm

Highest mean error: 3.6897709369659424 mm for frame 136

Lowest mean error: 2.9090583324432373 mm for frame 58

Saving results

Total time: 34.25269651412964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957522
Iteration 2/25 | Loss: 0.00147464
Iteration 3/25 | Loss: 0.00125691
Iteration 4/25 | Loss: 0.00122524
Iteration 5/25 | Loss: 0.00122682
Iteration 6/25 | Loss: 0.00122110
Iteration 7/25 | Loss: 0.00122076
Iteration 8/25 | Loss: 0.00122069
Iteration 9/25 | Loss: 0.00122069
Iteration 10/25 | Loss: 0.00122069
Iteration 11/25 | Loss: 0.00122069
Iteration 12/25 | Loss: 0.00122069
Iteration 13/25 | Loss: 0.00122069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012206926476210356, 0.0012206926476210356, 0.0012206926476210356, 0.0012206926476210356, 0.0012206926476210356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012206926476210356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30161715
Iteration 2/25 | Loss: 0.00057100
Iteration 3/25 | Loss: 0.00057099
Iteration 4/25 | Loss: 0.00057099
Iteration 5/25 | Loss: 0.00057099
Iteration 6/25 | Loss: 0.00057099
Iteration 7/25 | Loss: 0.00057099
Iteration 8/25 | Loss: 0.00057099
Iteration 9/25 | Loss: 0.00057099
Iteration 10/25 | Loss: 0.00057099
Iteration 11/25 | Loss: 0.00057099
Iteration 12/25 | Loss: 0.00057099
Iteration 13/25 | Loss: 0.00057099
Iteration 14/25 | Loss: 0.00057099
Iteration 15/25 | Loss: 0.00057099
Iteration 16/25 | Loss: 0.00057099
Iteration 17/25 | Loss: 0.00057099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000570987700484693, 0.000570987700484693, 0.000570987700484693, 0.000570987700484693, 0.000570987700484693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000570987700484693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057099
Iteration 2/1000 | Loss: 0.00005259
Iteration 3/1000 | Loss: 0.00002951
Iteration 4/1000 | Loss: 0.00002471
Iteration 5/1000 | Loss: 0.00002297
Iteration 6/1000 | Loss: 0.00002196
Iteration 7/1000 | Loss: 0.00002146
Iteration 8/1000 | Loss: 0.00002099
Iteration 9/1000 | Loss: 0.00002068
Iteration 10/1000 | Loss: 0.00002037
Iteration 11/1000 | Loss: 0.00002014
Iteration 12/1000 | Loss: 0.00001999
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001995
Iteration 15/1000 | Loss: 0.00001995
Iteration 16/1000 | Loss: 0.00001995
Iteration 17/1000 | Loss: 0.00001994
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001993
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001990
Iteration 24/1000 | Loss: 0.00001990
Iteration 25/1000 | Loss: 0.00001985
Iteration 26/1000 | Loss: 0.00001985
Iteration 27/1000 | Loss: 0.00001983
Iteration 28/1000 | Loss: 0.00001983
Iteration 29/1000 | Loss: 0.00001982
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001979
Iteration 33/1000 | Loss: 0.00001979
Iteration 34/1000 | Loss: 0.00001979
Iteration 35/1000 | Loss: 0.00001979
Iteration 36/1000 | Loss: 0.00001979
Iteration 37/1000 | Loss: 0.00001979
Iteration 38/1000 | Loss: 0.00001979
Iteration 39/1000 | Loss: 0.00001979
Iteration 40/1000 | Loss: 0.00001979
Iteration 41/1000 | Loss: 0.00001979
Iteration 42/1000 | Loss: 0.00001978
Iteration 43/1000 | Loss: 0.00001978
Iteration 44/1000 | Loss: 0.00001978
Iteration 45/1000 | Loss: 0.00001978
Iteration 46/1000 | Loss: 0.00001977
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001976
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001975
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001974
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001974
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001973
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001973
Iteration 62/1000 | Loss: 0.00001973
Iteration 63/1000 | Loss: 0.00001973
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001971
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00001971
Iteration 70/1000 | Loss: 0.00001971
Iteration 71/1000 | Loss: 0.00001971
Iteration 72/1000 | Loss: 0.00001971
Iteration 73/1000 | Loss: 0.00001970
Iteration 74/1000 | Loss: 0.00001970
Iteration 75/1000 | Loss: 0.00001970
Iteration 76/1000 | Loss: 0.00001970
Iteration 77/1000 | Loss: 0.00001969
Iteration 78/1000 | Loss: 0.00001969
Iteration 79/1000 | Loss: 0.00001969
Iteration 80/1000 | Loss: 0.00001969
Iteration 81/1000 | Loss: 0.00001968
Iteration 82/1000 | Loss: 0.00001968
Iteration 83/1000 | Loss: 0.00001968
Iteration 84/1000 | Loss: 0.00001967
Iteration 85/1000 | Loss: 0.00001967
Iteration 86/1000 | Loss: 0.00001967
Iteration 87/1000 | Loss: 0.00001967
Iteration 88/1000 | Loss: 0.00001966
Iteration 89/1000 | Loss: 0.00001966
Iteration 90/1000 | Loss: 0.00001966
Iteration 91/1000 | Loss: 0.00001966
Iteration 92/1000 | Loss: 0.00001966
Iteration 93/1000 | Loss: 0.00001966
Iteration 94/1000 | Loss: 0.00001966
Iteration 95/1000 | Loss: 0.00001966
Iteration 96/1000 | Loss: 0.00001966
Iteration 97/1000 | Loss: 0.00001966
Iteration 98/1000 | Loss: 0.00001966
Iteration 99/1000 | Loss: 0.00001965
Iteration 100/1000 | Loss: 0.00001965
Iteration 101/1000 | Loss: 0.00001965
Iteration 102/1000 | Loss: 0.00001965
Iteration 103/1000 | Loss: 0.00001965
Iteration 104/1000 | Loss: 0.00001964
Iteration 105/1000 | Loss: 0.00001964
Iteration 106/1000 | Loss: 0.00001964
Iteration 107/1000 | Loss: 0.00001963
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001963
Iteration 111/1000 | Loss: 0.00001963
Iteration 112/1000 | Loss: 0.00001963
Iteration 113/1000 | Loss: 0.00001962
Iteration 114/1000 | Loss: 0.00001962
Iteration 115/1000 | Loss: 0.00001962
Iteration 116/1000 | Loss: 0.00001962
Iteration 117/1000 | Loss: 0.00001962
Iteration 118/1000 | Loss: 0.00001962
Iteration 119/1000 | Loss: 0.00001962
Iteration 120/1000 | Loss: 0.00001962
Iteration 121/1000 | Loss: 0.00001962
Iteration 122/1000 | Loss: 0.00001962
Iteration 123/1000 | Loss: 0.00001962
Iteration 124/1000 | Loss: 0.00001962
Iteration 125/1000 | Loss: 0.00001962
Iteration 126/1000 | Loss: 0.00001962
Iteration 127/1000 | Loss: 0.00001962
Iteration 128/1000 | Loss: 0.00001962
Iteration 129/1000 | Loss: 0.00001962
Iteration 130/1000 | Loss: 0.00001962
Iteration 131/1000 | Loss: 0.00001962
Iteration 132/1000 | Loss: 0.00001962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.9620509192463942e-05, 1.9620509192463942e-05, 1.9620509192463942e-05, 1.9620509192463942e-05, 1.9620509192463942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9620509192463942e-05

Optimization complete. Final v2v error: 3.7799229621887207 mm

Highest mean error: 4.654944896697998 mm for frame 113

Lowest mean error: 3.041416645050049 mm for frame 7

Saving results

Total time: 43.946964263916016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484189
Iteration 2/25 | Loss: 0.00161297
Iteration 3/25 | Loss: 0.00133713
Iteration 4/25 | Loss: 0.00127801
Iteration 5/25 | Loss: 0.00126590
Iteration 6/25 | Loss: 0.00124325
Iteration 7/25 | Loss: 0.00120688
Iteration 8/25 | Loss: 0.00118497
Iteration 9/25 | Loss: 0.00117473
Iteration 10/25 | Loss: 0.00117113
Iteration 11/25 | Loss: 0.00116714
Iteration 12/25 | Loss: 0.00116805
Iteration 13/25 | Loss: 0.00116247
Iteration 14/25 | Loss: 0.00116190
Iteration 15/25 | Loss: 0.00116167
Iteration 16/25 | Loss: 0.00116156
Iteration 17/25 | Loss: 0.00116140
Iteration 18/25 | Loss: 0.00116421
Iteration 19/25 | Loss: 0.00116205
Iteration 20/25 | Loss: 0.00116163
Iteration 21/25 | Loss: 0.00115827
Iteration 22/25 | Loss: 0.00115763
Iteration 23/25 | Loss: 0.00115731
Iteration 24/25 | Loss: 0.00115729
Iteration 25/25 | Loss: 0.00115729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35339212
Iteration 2/25 | Loss: 0.00055590
Iteration 3/25 | Loss: 0.00055590
Iteration 4/25 | Loss: 0.00055590
Iteration 5/25 | Loss: 0.00055590
Iteration 6/25 | Loss: 0.00055590
Iteration 7/25 | Loss: 0.00055590
Iteration 8/25 | Loss: 0.00055590
Iteration 9/25 | Loss: 0.00055590
Iteration 10/25 | Loss: 0.00055590
Iteration 11/25 | Loss: 0.00055590
Iteration 12/25 | Loss: 0.00055590
Iteration 13/25 | Loss: 0.00055590
Iteration 14/25 | Loss: 0.00055590
Iteration 15/25 | Loss: 0.00055590
Iteration 16/25 | Loss: 0.00055590
Iteration 17/25 | Loss: 0.00055590
Iteration 18/25 | Loss: 0.00055590
Iteration 19/25 | Loss: 0.00055590
Iteration 20/25 | Loss: 0.00055590
Iteration 21/25 | Loss: 0.00055590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000555897771846503, 0.000555897771846503, 0.000555897771846503, 0.000555897771846503, 0.000555897771846503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000555897771846503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055590
Iteration 2/1000 | Loss: 0.00004373
Iteration 3/1000 | Loss: 0.00002752
Iteration 4/1000 | Loss: 0.00002144
Iteration 5/1000 | Loss: 0.00001924
Iteration 6/1000 | Loss: 0.00001778
Iteration 7/1000 | Loss: 0.00001704
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001564
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00001548
Iteration 16/1000 | Loss: 0.00001543
Iteration 17/1000 | Loss: 0.00001543
Iteration 18/1000 | Loss: 0.00001543
Iteration 19/1000 | Loss: 0.00001540
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001538
Iteration 22/1000 | Loss: 0.00001538
Iteration 23/1000 | Loss: 0.00001537
Iteration 24/1000 | Loss: 0.00001537
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001535
Iteration 27/1000 | Loss: 0.00001535
Iteration 28/1000 | Loss: 0.00001534
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001533
Iteration 32/1000 | Loss: 0.00001533
Iteration 33/1000 | Loss: 0.00001533
Iteration 34/1000 | Loss: 0.00001532
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001532
Iteration 37/1000 | Loss: 0.00001532
Iteration 38/1000 | Loss: 0.00001532
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001532
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001532
Iteration 46/1000 | Loss: 0.00001532
Iteration 47/1000 | Loss: 0.00001531
Iteration 48/1000 | Loss: 0.00001531
Iteration 49/1000 | Loss: 0.00001531
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001530
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001529
Iteration 64/1000 | Loss: 0.00001529
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001529
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001528
Iteration 72/1000 | Loss: 0.00001528
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001527
Iteration 84/1000 | Loss: 0.00001527
Iteration 85/1000 | Loss: 0.00001526
Iteration 86/1000 | Loss: 0.00001526
Iteration 87/1000 | Loss: 0.00001526
Iteration 88/1000 | Loss: 0.00001526
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001525
Iteration 91/1000 | Loss: 0.00001525
Iteration 92/1000 | Loss: 0.00001525
Iteration 93/1000 | Loss: 0.00001525
Iteration 94/1000 | Loss: 0.00001525
Iteration 95/1000 | Loss: 0.00001525
Iteration 96/1000 | Loss: 0.00001525
Iteration 97/1000 | Loss: 0.00001525
Iteration 98/1000 | Loss: 0.00001525
Iteration 99/1000 | Loss: 0.00001525
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00001524
Iteration 102/1000 | Loss: 0.00001524
Iteration 103/1000 | Loss: 0.00001524
Iteration 104/1000 | Loss: 0.00001524
Iteration 105/1000 | Loss: 0.00001524
Iteration 106/1000 | Loss: 0.00001524
Iteration 107/1000 | Loss: 0.00001524
Iteration 108/1000 | Loss: 0.00001524
Iteration 109/1000 | Loss: 0.00001524
Iteration 110/1000 | Loss: 0.00001524
Iteration 111/1000 | Loss: 0.00001524
Iteration 112/1000 | Loss: 0.00001524
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001523
Iteration 115/1000 | Loss: 0.00001523
Iteration 116/1000 | Loss: 0.00001523
Iteration 117/1000 | Loss: 0.00001523
Iteration 118/1000 | Loss: 0.00001523
Iteration 119/1000 | Loss: 0.00001523
Iteration 120/1000 | Loss: 0.00001523
Iteration 121/1000 | Loss: 0.00001523
Iteration 122/1000 | Loss: 0.00001523
Iteration 123/1000 | Loss: 0.00001523
Iteration 124/1000 | Loss: 0.00001523
Iteration 125/1000 | Loss: 0.00001523
Iteration 126/1000 | Loss: 0.00001523
Iteration 127/1000 | Loss: 0.00001523
Iteration 128/1000 | Loss: 0.00001523
Iteration 129/1000 | Loss: 0.00001523
Iteration 130/1000 | Loss: 0.00001523
Iteration 131/1000 | Loss: 0.00001523
Iteration 132/1000 | Loss: 0.00001523
Iteration 133/1000 | Loss: 0.00001523
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.5234025340760127e-05, 1.5234025340760127e-05, 1.5234025340760127e-05, 1.5234025340760127e-05, 1.5234025340760127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5234025340760127e-05

Optimization complete. Final v2v error: 3.3391366004943848 mm

Highest mean error: 3.8125057220458984 mm for frame 117

Lowest mean error: 2.9187488555908203 mm for frame 3

Saving results

Total time: 63.035460233688354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01299850
Iteration 2/25 | Loss: 0.01299849
Iteration 3/25 | Loss: 0.00408403
Iteration 4/25 | Loss: 0.00290169
Iteration 5/25 | Loss: 0.00249018
Iteration 6/25 | Loss: 0.00224441
Iteration 7/25 | Loss: 0.00199276
Iteration 8/25 | Loss: 0.00185596
Iteration 9/25 | Loss: 0.00189556
Iteration 10/25 | Loss: 0.00184416
Iteration 11/25 | Loss: 0.00179613
Iteration 12/25 | Loss: 0.00178385
Iteration 13/25 | Loss: 0.00176597
Iteration 14/25 | Loss: 0.00179923
Iteration 15/25 | Loss: 0.00176195
Iteration 16/25 | Loss: 0.00175036
Iteration 17/25 | Loss: 0.00173924
Iteration 18/25 | Loss: 0.00173520
Iteration 19/25 | Loss: 0.00173573
Iteration 20/25 | Loss: 0.00172661
Iteration 21/25 | Loss: 0.00171452
Iteration 22/25 | Loss: 0.00171218
Iteration 23/25 | Loss: 0.00171971
Iteration 24/25 | Loss: 0.00171867
Iteration 25/25 | Loss: 0.00169223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61581409
Iteration 2/25 | Loss: 0.00433699
Iteration 3/25 | Loss: 0.00405583
Iteration 4/25 | Loss: 0.00405583
Iteration 5/25 | Loss: 0.00405583
Iteration 6/25 | Loss: 0.00405583
Iteration 7/25 | Loss: 0.00405583
Iteration 8/25 | Loss: 0.00405583
Iteration 9/25 | Loss: 0.00405583
Iteration 10/25 | Loss: 0.00405583
Iteration 11/25 | Loss: 0.00405583
Iteration 12/25 | Loss: 0.00405583
Iteration 13/25 | Loss: 0.00405583
Iteration 14/25 | Loss: 0.00405583
Iteration 15/25 | Loss: 0.00405583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004055825527757406, 0.004055825527757406, 0.004055825527757406, 0.004055825527757406, 0.004055825527757406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004055825527757406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00405583
Iteration 2/1000 | Loss: 0.00243586
Iteration 3/1000 | Loss: 0.00245554
Iteration 4/1000 | Loss: 0.00272322
Iteration 5/1000 | Loss: 0.00217852
Iteration 6/1000 | Loss: 0.00100238
Iteration 7/1000 | Loss: 0.00070826
Iteration 8/1000 | Loss: 0.00034829
Iteration 9/1000 | Loss: 0.00230528
Iteration 10/1000 | Loss: 0.00037612
Iteration 11/1000 | Loss: 0.00087764
Iteration 12/1000 | Loss: 0.00056207
Iteration 13/1000 | Loss: 0.00090222
Iteration 14/1000 | Loss: 0.00058182
Iteration 15/1000 | Loss: 0.00077235
Iteration 16/1000 | Loss: 0.00043155
Iteration 17/1000 | Loss: 0.00056702
Iteration 18/1000 | Loss: 0.00099486
Iteration 19/1000 | Loss: 0.00060899
Iteration 20/1000 | Loss: 0.00157635
Iteration 21/1000 | Loss: 0.00179124
Iteration 22/1000 | Loss: 0.00034669
Iteration 23/1000 | Loss: 0.00093549
Iteration 24/1000 | Loss: 0.00146252
Iteration 25/1000 | Loss: 0.00093487
Iteration 26/1000 | Loss: 0.00169094
Iteration 27/1000 | Loss: 0.00163723
Iteration 28/1000 | Loss: 0.00280521
Iteration 29/1000 | Loss: 0.00156456
Iteration 30/1000 | Loss: 0.00046626
Iteration 31/1000 | Loss: 0.00111172
Iteration 32/1000 | Loss: 0.00085433
Iteration 33/1000 | Loss: 0.00060549
Iteration 34/1000 | Loss: 0.00068612
Iteration 35/1000 | Loss: 0.00068864
Iteration 36/1000 | Loss: 0.00079168
Iteration 37/1000 | Loss: 0.00061547
Iteration 38/1000 | Loss: 0.00081256
Iteration 39/1000 | Loss: 0.00029036
Iteration 40/1000 | Loss: 0.00066782
Iteration 41/1000 | Loss: 0.00057459
Iteration 42/1000 | Loss: 0.00065794
Iteration 43/1000 | Loss: 0.00038297
Iteration 44/1000 | Loss: 0.00162548
Iteration 45/1000 | Loss: 0.00101549
Iteration 46/1000 | Loss: 0.00127201
Iteration 47/1000 | Loss: 0.00069022
Iteration 48/1000 | Loss: 0.00030555
Iteration 49/1000 | Loss: 0.00016197
Iteration 50/1000 | Loss: 0.00065597
Iteration 51/1000 | Loss: 0.00048088
Iteration 52/1000 | Loss: 0.00132727
Iteration 53/1000 | Loss: 0.00136540
Iteration 54/1000 | Loss: 0.00050317
Iteration 55/1000 | Loss: 0.00104494
Iteration 56/1000 | Loss: 0.00080576
Iteration 57/1000 | Loss: 0.00027038
Iteration 58/1000 | Loss: 0.00042989
Iteration 59/1000 | Loss: 0.00024731
Iteration 60/1000 | Loss: 0.00052041
Iteration 61/1000 | Loss: 0.00088013
Iteration 62/1000 | Loss: 0.00108910
Iteration 63/1000 | Loss: 0.00086449
Iteration 64/1000 | Loss: 0.00065598
Iteration 65/1000 | Loss: 0.00075969
Iteration 66/1000 | Loss: 0.00051052
Iteration 67/1000 | Loss: 0.00028372
Iteration 68/1000 | Loss: 0.00026677
Iteration 69/1000 | Loss: 0.00031700
Iteration 70/1000 | Loss: 0.00042076
Iteration 71/1000 | Loss: 0.00029993
Iteration 72/1000 | Loss: 0.00020232
Iteration 73/1000 | Loss: 0.00019336
Iteration 74/1000 | Loss: 0.00064501
Iteration 75/1000 | Loss: 0.00042074
Iteration 76/1000 | Loss: 0.00013876
Iteration 77/1000 | Loss: 0.00103793
Iteration 78/1000 | Loss: 0.00022167
Iteration 79/1000 | Loss: 0.00012550
Iteration 80/1000 | Loss: 0.00018181
Iteration 81/1000 | Loss: 0.00071266
Iteration 82/1000 | Loss: 0.00037340
Iteration 83/1000 | Loss: 0.00065210
Iteration 84/1000 | Loss: 0.00013558
Iteration 85/1000 | Loss: 0.00030096
Iteration 86/1000 | Loss: 0.00024380
Iteration 87/1000 | Loss: 0.00025763
Iteration 88/1000 | Loss: 0.00015129
Iteration 89/1000 | Loss: 0.00024605
Iteration 90/1000 | Loss: 0.00019113
Iteration 91/1000 | Loss: 0.00018806
Iteration 92/1000 | Loss: 0.00008671
Iteration 93/1000 | Loss: 0.00097877
Iteration 94/1000 | Loss: 0.00082681
Iteration 95/1000 | Loss: 0.00057209
Iteration 96/1000 | Loss: 0.00044607
Iteration 97/1000 | Loss: 0.00058974
Iteration 98/1000 | Loss: 0.00034641
Iteration 99/1000 | Loss: 0.00042627
Iteration 100/1000 | Loss: 0.00008938
Iteration 101/1000 | Loss: 0.00008355
Iteration 102/1000 | Loss: 0.00007959
Iteration 103/1000 | Loss: 0.00007744
Iteration 104/1000 | Loss: 0.00007565
Iteration 105/1000 | Loss: 0.00007452
Iteration 106/1000 | Loss: 0.00007318
Iteration 107/1000 | Loss: 0.00007214
Iteration 108/1000 | Loss: 0.00007163
Iteration 109/1000 | Loss: 0.00082954
Iteration 110/1000 | Loss: 0.00007570
Iteration 111/1000 | Loss: 0.00007204
Iteration 112/1000 | Loss: 0.00006989
Iteration 113/1000 | Loss: 0.00006840
Iteration 114/1000 | Loss: 0.00006740
Iteration 115/1000 | Loss: 0.00006700
Iteration 116/1000 | Loss: 0.00006675
Iteration 117/1000 | Loss: 0.00006651
Iteration 118/1000 | Loss: 0.00099070
Iteration 119/1000 | Loss: 0.00008937
Iteration 120/1000 | Loss: 0.00188283
Iteration 121/1000 | Loss: 0.00250956
Iteration 122/1000 | Loss: 0.00018529
Iteration 123/1000 | Loss: 0.00225269
Iteration 124/1000 | Loss: 0.00013129
Iteration 125/1000 | Loss: 0.00009921
Iteration 126/1000 | Loss: 0.00008162
Iteration 127/1000 | Loss: 0.00007291
Iteration 128/1000 | Loss: 0.00006389
Iteration 129/1000 | Loss: 0.00005931
Iteration 130/1000 | Loss: 0.00005680
Iteration 131/1000 | Loss: 0.00103566
Iteration 132/1000 | Loss: 0.00066081
Iteration 133/1000 | Loss: 0.00007314
Iteration 134/1000 | Loss: 0.00005860
Iteration 135/1000 | Loss: 0.00005129
Iteration 136/1000 | Loss: 0.00004972
Iteration 137/1000 | Loss: 0.00004865
Iteration 138/1000 | Loss: 0.00004791
Iteration 139/1000 | Loss: 0.00004750
Iteration 140/1000 | Loss: 0.00004717
Iteration 141/1000 | Loss: 0.00004695
Iteration 142/1000 | Loss: 0.00004682
Iteration 143/1000 | Loss: 0.00004663
Iteration 144/1000 | Loss: 0.00004645
Iteration 145/1000 | Loss: 0.00004642
Iteration 146/1000 | Loss: 0.00004639
Iteration 147/1000 | Loss: 0.00004638
Iteration 148/1000 | Loss: 0.00004635
Iteration 149/1000 | Loss: 0.00004632
Iteration 150/1000 | Loss: 0.00004631
Iteration 151/1000 | Loss: 0.00004631
Iteration 152/1000 | Loss: 0.00004631
Iteration 153/1000 | Loss: 0.00004631
Iteration 154/1000 | Loss: 0.00004631
Iteration 155/1000 | Loss: 0.00004631
Iteration 156/1000 | Loss: 0.00004631
Iteration 157/1000 | Loss: 0.00004631
Iteration 158/1000 | Loss: 0.00004631
Iteration 159/1000 | Loss: 0.00004631
Iteration 160/1000 | Loss: 0.00004630
Iteration 161/1000 | Loss: 0.00004630
Iteration 162/1000 | Loss: 0.00004630
Iteration 163/1000 | Loss: 0.00004630
Iteration 164/1000 | Loss: 0.00004629
Iteration 165/1000 | Loss: 0.00004629
Iteration 166/1000 | Loss: 0.00004629
Iteration 167/1000 | Loss: 0.00004629
Iteration 168/1000 | Loss: 0.00004629
Iteration 169/1000 | Loss: 0.00004629
Iteration 170/1000 | Loss: 0.00004628
Iteration 171/1000 | Loss: 0.00004628
Iteration 172/1000 | Loss: 0.00004628
Iteration 173/1000 | Loss: 0.00004628
Iteration 174/1000 | Loss: 0.00004628
Iteration 175/1000 | Loss: 0.00004628
Iteration 176/1000 | Loss: 0.00004628
Iteration 177/1000 | Loss: 0.00004628
Iteration 178/1000 | Loss: 0.00004628
Iteration 179/1000 | Loss: 0.00004628
Iteration 180/1000 | Loss: 0.00004628
Iteration 181/1000 | Loss: 0.00004627
Iteration 182/1000 | Loss: 0.00004627
Iteration 183/1000 | Loss: 0.00004627
Iteration 184/1000 | Loss: 0.00004627
Iteration 185/1000 | Loss: 0.00004626
Iteration 186/1000 | Loss: 0.00004626
Iteration 187/1000 | Loss: 0.00004626
Iteration 188/1000 | Loss: 0.00004626
Iteration 189/1000 | Loss: 0.00004626
Iteration 190/1000 | Loss: 0.00004625
Iteration 191/1000 | Loss: 0.00004625
Iteration 192/1000 | Loss: 0.00004625
Iteration 193/1000 | Loss: 0.00004624
Iteration 194/1000 | Loss: 0.00004623
Iteration 195/1000 | Loss: 0.00004622
Iteration 196/1000 | Loss: 0.00004622
Iteration 197/1000 | Loss: 0.00004622
Iteration 198/1000 | Loss: 0.00004622
Iteration 199/1000 | Loss: 0.00004622
Iteration 200/1000 | Loss: 0.00004622
Iteration 201/1000 | Loss: 0.00004622
Iteration 202/1000 | Loss: 0.00004622
Iteration 203/1000 | Loss: 0.00004622
Iteration 204/1000 | Loss: 0.00004622
Iteration 205/1000 | Loss: 0.00004621
Iteration 206/1000 | Loss: 0.00004621
Iteration 207/1000 | Loss: 0.00004621
Iteration 208/1000 | Loss: 0.00004621
Iteration 209/1000 | Loss: 0.00004621
Iteration 210/1000 | Loss: 0.00004621
Iteration 211/1000 | Loss: 0.00004621
Iteration 212/1000 | Loss: 0.00004620
Iteration 213/1000 | Loss: 0.00004620
Iteration 214/1000 | Loss: 0.00004620
Iteration 215/1000 | Loss: 0.00004620
Iteration 216/1000 | Loss: 0.00004620
Iteration 217/1000 | Loss: 0.00004620
Iteration 218/1000 | Loss: 0.00004620
Iteration 219/1000 | Loss: 0.00004620
Iteration 220/1000 | Loss: 0.00004619
Iteration 221/1000 | Loss: 0.00004619
Iteration 222/1000 | Loss: 0.00004619
Iteration 223/1000 | Loss: 0.00004619
Iteration 224/1000 | Loss: 0.00004619
Iteration 225/1000 | Loss: 0.00004619
Iteration 226/1000 | Loss: 0.00004619
Iteration 227/1000 | Loss: 0.00004618
Iteration 228/1000 | Loss: 0.00004618
Iteration 229/1000 | Loss: 0.00004618
Iteration 230/1000 | Loss: 0.00004618
Iteration 231/1000 | Loss: 0.00004618
Iteration 232/1000 | Loss: 0.00004618
Iteration 233/1000 | Loss: 0.00004618
Iteration 234/1000 | Loss: 0.00004618
Iteration 235/1000 | Loss: 0.00004617
Iteration 236/1000 | Loss: 0.00004617
Iteration 237/1000 | Loss: 0.00004617
Iteration 238/1000 | Loss: 0.00004617
Iteration 239/1000 | Loss: 0.00004617
Iteration 240/1000 | Loss: 0.00004617
Iteration 241/1000 | Loss: 0.00004617
Iteration 242/1000 | Loss: 0.00004617
Iteration 243/1000 | Loss: 0.00004617
Iteration 244/1000 | Loss: 0.00004617
Iteration 245/1000 | Loss: 0.00004617
Iteration 246/1000 | Loss: 0.00004616
Iteration 247/1000 | Loss: 0.00004616
Iteration 248/1000 | Loss: 0.00004616
Iteration 249/1000 | Loss: 0.00004616
Iteration 250/1000 | Loss: 0.00004616
Iteration 251/1000 | Loss: 0.00004616
Iteration 252/1000 | Loss: 0.00004616
Iteration 253/1000 | Loss: 0.00004615
Iteration 254/1000 | Loss: 0.00004615
Iteration 255/1000 | Loss: 0.00004615
Iteration 256/1000 | Loss: 0.00004615
Iteration 257/1000 | Loss: 0.00004615
Iteration 258/1000 | Loss: 0.00004615
Iteration 259/1000 | Loss: 0.00004615
Iteration 260/1000 | Loss: 0.00004614
Iteration 261/1000 | Loss: 0.00004614
Iteration 262/1000 | Loss: 0.00004614
Iteration 263/1000 | Loss: 0.00004614
Iteration 264/1000 | Loss: 0.00004614
Iteration 265/1000 | Loss: 0.00004614
Iteration 266/1000 | Loss: 0.00004614
Iteration 267/1000 | Loss: 0.00004614
Iteration 268/1000 | Loss: 0.00004614
Iteration 269/1000 | Loss: 0.00004614
Iteration 270/1000 | Loss: 0.00004614
Iteration 271/1000 | Loss: 0.00004614
Iteration 272/1000 | Loss: 0.00004614
Iteration 273/1000 | Loss: 0.00004614
Iteration 274/1000 | Loss: 0.00004614
Iteration 275/1000 | Loss: 0.00004613
Iteration 276/1000 | Loss: 0.00004613
Iteration 277/1000 | Loss: 0.00004613
Iteration 278/1000 | Loss: 0.00004613
Iteration 279/1000 | Loss: 0.00004613
Iteration 280/1000 | Loss: 0.00004613
Iteration 281/1000 | Loss: 0.00004613
Iteration 282/1000 | Loss: 0.00004613
Iteration 283/1000 | Loss: 0.00004613
Iteration 284/1000 | Loss: 0.00004613
Iteration 285/1000 | Loss: 0.00004613
Iteration 286/1000 | Loss: 0.00004612
Iteration 287/1000 | Loss: 0.00004612
Iteration 288/1000 | Loss: 0.00004612
Iteration 289/1000 | Loss: 0.00004612
Iteration 290/1000 | Loss: 0.00004612
Iteration 291/1000 | Loss: 0.00004612
Iteration 292/1000 | Loss: 0.00004612
Iteration 293/1000 | Loss: 0.00004612
Iteration 294/1000 | Loss: 0.00004612
Iteration 295/1000 | Loss: 0.00004612
Iteration 296/1000 | Loss: 0.00004612
Iteration 297/1000 | Loss: 0.00004612
Iteration 298/1000 | Loss: 0.00004612
Iteration 299/1000 | Loss: 0.00004611
Iteration 300/1000 | Loss: 0.00004611
Iteration 301/1000 | Loss: 0.00004611
Iteration 302/1000 | Loss: 0.00004611
Iteration 303/1000 | Loss: 0.00004611
Iteration 304/1000 | Loss: 0.00004610
Iteration 305/1000 | Loss: 0.00004610
Iteration 306/1000 | Loss: 0.00004610
Iteration 307/1000 | Loss: 0.00004610
Iteration 308/1000 | Loss: 0.00004610
Iteration 309/1000 | Loss: 0.00004610
Iteration 310/1000 | Loss: 0.00004610
Iteration 311/1000 | Loss: 0.00004610
Iteration 312/1000 | Loss: 0.00004610
Iteration 313/1000 | Loss: 0.00004610
Iteration 314/1000 | Loss: 0.00004609
Iteration 315/1000 | Loss: 0.00004609
Iteration 316/1000 | Loss: 0.00004609
Iteration 317/1000 | Loss: 0.00004609
Iteration 318/1000 | Loss: 0.00004609
Iteration 319/1000 | Loss: 0.00004609
Iteration 320/1000 | Loss: 0.00004609
Iteration 321/1000 | Loss: 0.00004609
Iteration 322/1000 | Loss: 0.00004609
Iteration 323/1000 | Loss: 0.00004608
Iteration 324/1000 | Loss: 0.00004608
Iteration 325/1000 | Loss: 0.00004608
Iteration 326/1000 | Loss: 0.00004608
Iteration 327/1000 | Loss: 0.00004608
Iteration 328/1000 | Loss: 0.00004608
Iteration 329/1000 | Loss: 0.00004607
Iteration 330/1000 | Loss: 0.00004607
Iteration 331/1000 | Loss: 0.00004607
Iteration 332/1000 | Loss: 0.00004607
Iteration 333/1000 | Loss: 0.00004607
Iteration 334/1000 | Loss: 0.00004607
Iteration 335/1000 | Loss: 0.00004607
Iteration 336/1000 | Loss: 0.00004607
Iteration 337/1000 | Loss: 0.00004607
Iteration 338/1000 | Loss: 0.00004607
Iteration 339/1000 | Loss: 0.00004607
Iteration 340/1000 | Loss: 0.00004607
Iteration 341/1000 | Loss: 0.00004606
Iteration 342/1000 | Loss: 0.00004606
Iteration 343/1000 | Loss: 0.00004606
Iteration 344/1000 | Loss: 0.00004606
Iteration 345/1000 | Loss: 0.00004606
Iteration 346/1000 | Loss: 0.00004606
Iteration 347/1000 | Loss: 0.00004606
Iteration 348/1000 | Loss: 0.00004606
Iteration 349/1000 | Loss: 0.00004606
Iteration 350/1000 | Loss: 0.00004606
Iteration 351/1000 | Loss: 0.00004606
Iteration 352/1000 | Loss: 0.00004606
Iteration 353/1000 | Loss: 0.00004606
Iteration 354/1000 | Loss: 0.00004606
Iteration 355/1000 | Loss: 0.00004606
Iteration 356/1000 | Loss: 0.00004606
Iteration 357/1000 | Loss: 0.00004606
Iteration 358/1000 | Loss: 0.00004606
Iteration 359/1000 | Loss: 0.00004606
Iteration 360/1000 | Loss: 0.00004606
Iteration 361/1000 | Loss: 0.00004606
Iteration 362/1000 | Loss: 0.00004606
Iteration 363/1000 | Loss: 0.00004606
Iteration 364/1000 | Loss: 0.00004606
Iteration 365/1000 | Loss: 0.00004606
Iteration 366/1000 | Loss: 0.00004606
Iteration 367/1000 | Loss: 0.00004606
Iteration 368/1000 | Loss: 0.00004606
Iteration 369/1000 | Loss: 0.00004606
Iteration 370/1000 | Loss: 0.00004606
Iteration 371/1000 | Loss: 0.00004606
Iteration 372/1000 | Loss: 0.00004606
Iteration 373/1000 | Loss: 0.00004606
Iteration 374/1000 | Loss: 0.00004606
Iteration 375/1000 | Loss: 0.00004606
Iteration 376/1000 | Loss: 0.00004606
Iteration 377/1000 | Loss: 0.00004606
Iteration 378/1000 | Loss: 0.00004606
Iteration 379/1000 | Loss: 0.00004606
Iteration 380/1000 | Loss: 0.00004606
Iteration 381/1000 | Loss: 0.00004606
Iteration 382/1000 | Loss: 0.00004606
Iteration 383/1000 | Loss: 0.00004606
Iteration 384/1000 | Loss: 0.00004606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 384. Stopping optimization.
Last 5 losses: [4.606108268490061e-05, 4.606108268490061e-05, 4.606108268490061e-05, 4.606108268490061e-05, 4.606108268490061e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.606108268490061e-05

Optimization complete. Final v2v error: 5.159819602966309 mm

Highest mean error: 12.638813018798828 mm for frame 82

Lowest mean error: 4.409694671630859 mm for frame 44

Saving results

Total time: 267.1284701824188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991229
Iteration 2/25 | Loss: 0.00140067
Iteration 3/25 | Loss: 0.00121832
Iteration 4/25 | Loss: 0.00119639
Iteration 5/25 | Loss: 0.00119217
Iteration 6/25 | Loss: 0.00119178
Iteration 7/25 | Loss: 0.00119178
Iteration 8/25 | Loss: 0.00119178
Iteration 9/25 | Loss: 0.00119178
Iteration 10/25 | Loss: 0.00119178
Iteration 11/25 | Loss: 0.00119178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011917827650904655, 0.0011917827650904655, 0.0011917827650904655, 0.0011917827650904655, 0.0011917827650904655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011917827650904655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34486842
Iteration 2/25 | Loss: 0.00071164
Iteration 3/25 | Loss: 0.00071162
Iteration 4/25 | Loss: 0.00071161
Iteration 5/25 | Loss: 0.00071161
Iteration 6/25 | Loss: 0.00071161
Iteration 7/25 | Loss: 0.00071161
Iteration 8/25 | Loss: 0.00071161
Iteration 9/25 | Loss: 0.00071161
Iteration 10/25 | Loss: 0.00071161
Iteration 11/25 | Loss: 0.00071161
Iteration 12/25 | Loss: 0.00071161
Iteration 13/25 | Loss: 0.00071161
Iteration 14/25 | Loss: 0.00071161
Iteration 15/25 | Loss: 0.00071161
Iteration 16/25 | Loss: 0.00071161
Iteration 17/25 | Loss: 0.00071161
Iteration 18/25 | Loss: 0.00071161
Iteration 19/25 | Loss: 0.00071161
Iteration 20/25 | Loss: 0.00071161
Iteration 21/25 | Loss: 0.00071161
Iteration 22/25 | Loss: 0.00071161
Iteration 23/25 | Loss: 0.00071161
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007116131018847227, 0.0007116131018847227, 0.0007116131018847227, 0.0007116131018847227, 0.0007116131018847227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007116131018847227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071161
Iteration 2/1000 | Loss: 0.00003289
Iteration 3/1000 | Loss: 0.00002338
Iteration 4/1000 | Loss: 0.00002044
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00001690
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001592
Iteration 10/1000 | Loss: 0.00001570
Iteration 11/1000 | Loss: 0.00001558
Iteration 12/1000 | Loss: 0.00001548
Iteration 13/1000 | Loss: 0.00001547
Iteration 14/1000 | Loss: 0.00001545
Iteration 15/1000 | Loss: 0.00001541
Iteration 16/1000 | Loss: 0.00001539
Iteration 17/1000 | Loss: 0.00001537
Iteration 18/1000 | Loss: 0.00001535
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001534
Iteration 22/1000 | Loss: 0.00001533
Iteration 23/1000 | Loss: 0.00001532
Iteration 24/1000 | Loss: 0.00001529
Iteration 25/1000 | Loss: 0.00001526
Iteration 26/1000 | Loss: 0.00001525
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001525
Iteration 29/1000 | Loss: 0.00001525
Iteration 30/1000 | Loss: 0.00001524
Iteration 31/1000 | Loss: 0.00001524
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001522
Iteration 34/1000 | Loss: 0.00001522
Iteration 35/1000 | Loss: 0.00001522
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001522
Iteration 39/1000 | Loss: 0.00001522
Iteration 40/1000 | Loss: 0.00001521
Iteration 41/1000 | Loss: 0.00001521
Iteration 42/1000 | Loss: 0.00001519
Iteration 43/1000 | Loss: 0.00001519
Iteration 44/1000 | Loss: 0.00001519
Iteration 45/1000 | Loss: 0.00001519
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001519
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001518
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001517
Iteration 55/1000 | Loss: 0.00001517
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001516
Iteration 59/1000 | Loss: 0.00001516
Iteration 60/1000 | Loss: 0.00001516
Iteration 61/1000 | Loss: 0.00001516
Iteration 62/1000 | Loss: 0.00001516
Iteration 63/1000 | Loss: 0.00001516
Iteration 64/1000 | Loss: 0.00001515
Iteration 65/1000 | Loss: 0.00001515
Iteration 66/1000 | Loss: 0.00001515
Iteration 67/1000 | Loss: 0.00001515
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001514
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001514
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001513
Iteration 79/1000 | Loss: 0.00001513
Iteration 80/1000 | Loss: 0.00001513
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001512
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001511
Iteration 86/1000 | Loss: 0.00001511
Iteration 87/1000 | Loss: 0.00001511
Iteration 88/1000 | Loss: 0.00001511
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001511
Iteration 91/1000 | Loss: 0.00001511
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001508
Iteration 114/1000 | Loss: 0.00001508
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001507
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001507
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001507
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001507
Iteration 129/1000 | Loss: 0.00001507
Iteration 130/1000 | Loss: 0.00001507
Iteration 131/1000 | Loss: 0.00001507
Iteration 132/1000 | Loss: 0.00001507
Iteration 133/1000 | Loss: 0.00001506
Iteration 134/1000 | Loss: 0.00001506
Iteration 135/1000 | Loss: 0.00001506
Iteration 136/1000 | Loss: 0.00001506
Iteration 137/1000 | Loss: 0.00001506
Iteration 138/1000 | Loss: 0.00001506
Iteration 139/1000 | Loss: 0.00001506
Iteration 140/1000 | Loss: 0.00001506
Iteration 141/1000 | Loss: 0.00001506
Iteration 142/1000 | Loss: 0.00001506
Iteration 143/1000 | Loss: 0.00001506
Iteration 144/1000 | Loss: 0.00001506
Iteration 145/1000 | Loss: 0.00001506
Iteration 146/1000 | Loss: 0.00001506
Iteration 147/1000 | Loss: 0.00001506
Iteration 148/1000 | Loss: 0.00001506
Iteration 149/1000 | Loss: 0.00001505
Iteration 150/1000 | Loss: 0.00001505
Iteration 151/1000 | Loss: 0.00001505
Iteration 152/1000 | Loss: 0.00001505
Iteration 153/1000 | Loss: 0.00001505
Iteration 154/1000 | Loss: 0.00001505
Iteration 155/1000 | Loss: 0.00001505
Iteration 156/1000 | Loss: 0.00001505
Iteration 157/1000 | Loss: 0.00001505
Iteration 158/1000 | Loss: 0.00001505
Iteration 159/1000 | Loss: 0.00001505
Iteration 160/1000 | Loss: 0.00001505
Iteration 161/1000 | Loss: 0.00001505
Iteration 162/1000 | Loss: 0.00001505
Iteration 163/1000 | Loss: 0.00001505
Iteration 164/1000 | Loss: 0.00001505
Iteration 165/1000 | Loss: 0.00001505
Iteration 166/1000 | Loss: 0.00001505
Iteration 167/1000 | Loss: 0.00001505
Iteration 168/1000 | Loss: 0.00001505
Iteration 169/1000 | Loss: 0.00001504
Iteration 170/1000 | Loss: 0.00001504
Iteration 171/1000 | Loss: 0.00001504
Iteration 172/1000 | Loss: 0.00001504
Iteration 173/1000 | Loss: 0.00001504
Iteration 174/1000 | Loss: 0.00001504
Iteration 175/1000 | Loss: 0.00001504
Iteration 176/1000 | Loss: 0.00001504
Iteration 177/1000 | Loss: 0.00001504
Iteration 178/1000 | Loss: 0.00001504
Iteration 179/1000 | Loss: 0.00001504
Iteration 180/1000 | Loss: 0.00001504
Iteration 181/1000 | Loss: 0.00001504
Iteration 182/1000 | Loss: 0.00001504
Iteration 183/1000 | Loss: 0.00001504
Iteration 184/1000 | Loss: 0.00001504
Iteration 185/1000 | Loss: 0.00001504
Iteration 186/1000 | Loss: 0.00001504
Iteration 187/1000 | Loss: 0.00001504
Iteration 188/1000 | Loss: 0.00001504
Iteration 189/1000 | Loss: 0.00001504
Iteration 190/1000 | Loss: 0.00001504
Iteration 191/1000 | Loss: 0.00001504
Iteration 192/1000 | Loss: 0.00001504
Iteration 193/1000 | Loss: 0.00001504
Iteration 194/1000 | Loss: 0.00001504
Iteration 195/1000 | Loss: 0.00001504
Iteration 196/1000 | Loss: 0.00001504
Iteration 197/1000 | Loss: 0.00001504
Iteration 198/1000 | Loss: 0.00001504
Iteration 199/1000 | Loss: 0.00001504
Iteration 200/1000 | Loss: 0.00001504
Iteration 201/1000 | Loss: 0.00001504
Iteration 202/1000 | Loss: 0.00001504
Iteration 203/1000 | Loss: 0.00001504
Iteration 204/1000 | Loss: 0.00001504
Iteration 205/1000 | Loss: 0.00001504
Iteration 206/1000 | Loss: 0.00001504
Iteration 207/1000 | Loss: 0.00001504
Iteration 208/1000 | Loss: 0.00001504
Iteration 209/1000 | Loss: 0.00001504
Iteration 210/1000 | Loss: 0.00001504
Iteration 211/1000 | Loss: 0.00001504
Iteration 212/1000 | Loss: 0.00001504
Iteration 213/1000 | Loss: 0.00001504
Iteration 214/1000 | Loss: 0.00001504
Iteration 215/1000 | Loss: 0.00001504
Iteration 216/1000 | Loss: 0.00001504
Iteration 217/1000 | Loss: 0.00001504
Iteration 218/1000 | Loss: 0.00001504
Iteration 219/1000 | Loss: 0.00001504
Iteration 220/1000 | Loss: 0.00001504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.504422561993124e-05, 1.504422561993124e-05, 1.504422561993124e-05, 1.504422561993124e-05, 1.504422561993124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.504422561993124e-05

Optimization complete. Final v2v error: 3.2707362174987793 mm

Highest mean error: 3.5056393146514893 mm for frame 72

Lowest mean error: 3.0746419429779053 mm for frame 27

Saving results

Total time: 43.66763615608215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441083
Iteration 2/25 | Loss: 0.00126623
Iteration 3/25 | Loss: 0.00118475
Iteration 4/25 | Loss: 0.00116550
Iteration 5/25 | Loss: 0.00115906
Iteration 6/25 | Loss: 0.00115787
Iteration 7/25 | Loss: 0.00115785
Iteration 8/25 | Loss: 0.00115785
Iteration 9/25 | Loss: 0.00115785
Iteration 10/25 | Loss: 0.00115785
Iteration 11/25 | Loss: 0.00115785
Iteration 12/25 | Loss: 0.00115785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011578539852052927, 0.0011578539852052927, 0.0011578539852052927, 0.0011578539852052927, 0.0011578539852052927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011578539852052927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34824491
Iteration 2/25 | Loss: 0.00056602
Iteration 3/25 | Loss: 0.00056601
Iteration 4/25 | Loss: 0.00056601
Iteration 5/25 | Loss: 0.00056601
Iteration 6/25 | Loss: 0.00056601
Iteration 7/25 | Loss: 0.00056601
Iteration 8/25 | Loss: 0.00056601
Iteration 9/25 | Loss: 0.00056601
Iteration 10/25 | Loss: 0.00056601
Iteration 11/25 | Loss: 0.00056601
Iteration 12/25 | Loss: 0.00056601
Iteration 13/25 | Loss: 0.00056601
Iteration 14/25 | Loss: 0.00056601
Iteration 15/25 | Loss: 0.00056601
Iteration 16/25 | Loss: 0.00056601
Iteration 17/25 | Loss: 0.00056601
Iteration 18/25 | Loss: 0.00056601
Iteration 19/25 | Loss: 0.00056601
Iteration 20/25 | Loss: 0.00056601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005660102469846606, 0.0005660102469846606, 0.0005660102469846606, 0.0005660102469846606, 0.0005660102469846606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005660102469846606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056601
Iteration 2/1000 | Loss: 0.00003763
Iteration 3/1000 | Loss: 0.00002303
Iteration 4/1000 | Loss: 0.00002106
Iteration 5/1000 | Loss: 0.00001976
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001755
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001730
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001729
Iteration 17/1000 | Loss: 0.00001725
Iteration 18/1000 | Loss: 0.00001723
Iteration 19/1000 | Loss: 0.00001723
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001723
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001721
Iteration 24/1000 | Loss: 0.00001718
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001716
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001715
Iteration 33/1000 | Loss: 0.00001714
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001712
Iteration 38/1000 | Loss: 0.00001712
Iteration 39/1000 | Loss: 0.00001711
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001710
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001709
Iteration 44/1000 | Loss: 0.00001709
Iteration 45/1000 | Loss: 0.00001709
Iteration 46/1000 | Loss: 0.00001708
Iteration 47/1000 | Loss: 0.00001708
Iteration 48/1000 | Loss: 0.00001708
Iteration 49/1000 | Loss: 0.00001707
Iteration 50/1000 | Loss: 0.00001707
Iteration 51/1000 | Loss: 0.00001706
Iteration 52/1000 | Loss: 0.00001706
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001705
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001704
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001703
Iteration 63/1000 | Loss: 0.00001703
Iteration 64/1000 | Loss: 0.00001703
Iteration 65/1000 | Loss: 0.00001703
Iteration 66/1000 | Loss: 0.00001703
Iteration 67/1000 | Loss: 0.00001703
Iteration 68/1000 | Loss: 0.00001702
Iteration 69/1000 | Loss: 0.00001702
Iteration 70/1000 | Loss: 0.00001702
Iteration 71/1000 | Loss: 0.00001702
Iteration 72/1000 | Loss: 0.00001702
Iteration 73/1000 | Loss: 0.00001702
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001701
Iteration 78/1000 | Loss: 0.00001701
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001701
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001700
Iteration 92/1000 | Loss: 0.00001700
Iteration 93/1000 | Loss: 0.00001700
Iteration 94/1000 | Loss: 0.00001700
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001700
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001699
Iteration 99/1000 | Loss: 0.00001699
Iteration 100/1000 | Loss: 0.00001699
Iteration 101/1000 | Loss: 0.00001699
Iteration 102/1000 | Loss: 0.00001699
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001698
Iteration 105/1000 | Loss: 0.00001698
Iteration 106/1000 | Loss: 0.00001698
Iteration 107/1000 | Loss: 0.00001698
Iteration 108/1000 | Loss: 0.00001697
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00001697
Iteration 113/1000 | Loss: 0.00001697
Iteration 114/1000 | Loss: 0.00001696
Iteration 115/1000 | Loss: 0.00001696
Iteration 116/1000 | Loss: 0.00001696
Iteration 117/1000 | Loss: 0.00001696
Iteration 118/1000 | Loss: 0.00001696
Iteration 119/1000 | Loss: 0.00001696
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001696
Iteration 122/1000 | Loss: 0.00001695
Iteration 123/1000 | Loss: 0.00001695
Iteration 124/1000 | Loss: 0.00001695
Iteration 125/1000 | Loss: 0.00001695
Iteration 126/1000 | Loss: 0.00001695
Iteration 127/1000 | Loss: 0.00001695
Iteration 128/1000 | Loss: 0.00001695
Iteration 129/1000 | Loss: 0.00001695
Iteration 130/1000 | Loss: 0.00001695
Iteration 131/1000 | Loss: 0.00001695
Iteration 132/1000 | Loss: 0.00001695
Iteration 133/1000 | Loss: 0.00001695
Iteration 134/1000 | Loss: 0.00001695
Iteration 135/1000 | Loss: 0.00001695
Iteration 136/1000 | Loss: 0.00001695
Iteration 137/1000 | Loss: 0.00001695
Iteration 138/1000 | Loss: 0.00001694
Iteration 139/1000 | Loss: 0.00001694
Iteration 140/1000 | Loss: 0.00001694
Iteration 141/1000 | Loss: 0.00001694
Iteration 142/1000 | Loss: 0.00001694
Iteration 143/1000 | Loss: 0.00001694
Iteration 144/1000 | Loss: 0.00001694
Iteration 145/1000 | Loss: 0.00001694
Iteration 146/1000 | Loss: 0.00001694
Iteration 147/1000 | Loss: 0.00001694
Iteration 148/1000 | Loss: 0.00001694
Iteration 149/1000 | Loss: 0.00001694
Iteration 150/1000 | Loss: 0.00001694
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001693
Iteration 154/1000 | Loss: 0.00001693
Iteration 155/1000 | Loss: 0.00001693
Iteration 156/1000 | Loss: 0.00001693
Iteration 157/1000 | Loss: 0.00001693
Iteration 158/1000 | Loss: 0.00001693
Iteration 159/1000 | Loss: 0.00001693
Iteration 160/1000 | Loss: 0.00001693
Iteration 161/1000 | Loss: 0.00001693
Iteration 162/1000 | Loss: 0.00001693
Iteration 163/1000 | Loss: 0.00001693
Iteration 164/1000 | Loss: 0.00001693
Iteration 165/1000 | Loss: 0.00001693
Iteration 166/1000 | Loss: 0.00001693
Iteration 167/1000 | Loss: 0.00001693
Iteration 168/1000 | Loss: 0.00001692
Iteration 169/1000 | Loss: 0.00001692
Iteration 170/1000 | Loss: 0.00001692
Iteration 171/1000 | Loss: 0.00001692
Iteration 172/1000 | Loss: 0.00001692
Iteration 173/1000 | Loss: 0.00001692
Iteration 174/1000 | Loss: 0.00001692
Iteration 175/1000 | Loss: 0.00001692
Iteration 176/1000 | Loss: 0.00001692
Iteration 177/1000 | Loss: 0.00001692
Iteration 178/1000 | Loss: 0.00001692
Iteration 179/1000 | Loss: 0.00001692
Iteration 180/1000 | Loss: 0.00001692
Iteration 181/1000 | Loss: 0.00001692
Iteration 182/1000 | Loss: 0.00001692
Iteration 183/1000 | Loss: 0.00001692
Iteration 184/1000 | Loss: 0.00001692
Iteration 185/1000 | Loss: 0.00001692
Iteration 186/1000 | Loss: 0.00001692
Iteration 187/1000 | Loss: 0.00001692
Iteration 188/1000 | Loss: 0.00001691
Iteration 189/1000 | Loss: 0.00001691
Iteration 190/1000 | Loss: 0.00001691
Iteration 191/1000 | Loss: 0.00001691
Iteration 192/1000 | Loss: 0.00001691
Iteration 193/1000 | Loss: 0.00001691
Iteration 194/1000 | Loss: 0.00001691
Iteration 195/1000 | Loss: 0.00001691
Iteration 196/1000 | Loss: 0.00001691
Iteration 197/1000 | Loss: 0.00001691
Iteration 198/1000 | Loss: 0.00001691
Iteration 199/1000 | Loss: 0.00001691
Iteration 200/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.6912041246541776e-05, 1.6912041246541776e-05, 1.6912041246541776e-05, 1.6912041246541776e-05, 1.6912041246541776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6912041246541776e-05

Optimization complete. Final v2v error: 3.563744306564331 mm

Highest mean error: 3.75022029876709 mm for frame 152

Lowest mean error: 3.381722927093506 mm for frame 118

Saving results

Total time: 36.20282030105591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489603
Iteration 2/25 | Loss: 0.00151139
Iteration 3/25 | Loss: 0.00127592
Iteration 4/25 | Loss: 0.00125135
Iteration 5/25 | Loss: 0.00124249
Iteration 6/25 | Loss: 0.00124020
Iteration 7/25 | Loss: 0.00123961
Iteration 8/25 | Loss: 0.00123945
Iteration 9/25 | Loss: 0.00123945
Iteration 10/25 | Loss: 0.00123945
Iteration 11/25 | Loss: 0.00123945
Iteration 12/25 | Loss: 0.00123945
Iteration 13/25 | Loss: 0.00123945
Iteration 14/25 | Loss: 0.00123945
Iteration 15/25 | Loss: 0.00123945
Iteration 16/25 | Loss: 0.00123945
Iteration 17/25 | Loss: 0.00123943
Iteration 18/25 | Loss: 0.00123943
Iteration 19/25 | Loss: 0.00123943
Iteration 20/25 | Loss: 0.00123943
Iteration 21/25 | Loss: 0.00123943
Iteration 22/25 | Loss: 0.00123943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001239426084794104, 0.001239426084794104, 0.001239426084794104, 0.001239426084794104, 0.001239426084794104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001239426084794104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20246196
Iteration 2/25 | Loss: 0.00059504
Iteration 3/25 | Loss: 0.00059502
Iteration 4/25 | Loss: 0.00059502
Iteration 5/25 | Loss: 0.00059502
Iteration 6/25 | Loss: 0.00059502
Iteration 7/25 | Loss: 0.00059502
Iteration 8/25 | Loss: 0.00059502
Iteration 9/25 | Loss: 0.00059502
Iteration 10/25 | Loss: 0.00059502
Iteration 11/25 | Loss: 0.00059502
Iteration 12/25 | Loss: 0.00059502
Iteration 13/25 | Loss: 0.00059502
Iteration 14/25 | Loss: 0.00059502
Iteration 15/25 | Loss: 0.00059502
Iteration 16/25 | Loss: 0.00059502
Iteration 17/25 | Loss: 0.00059502
Iteration 18/25 | Loss: 0.00059502
Iteration 19/25 | Loss: 0.00059502
Iteration 20/25 | Loss: 0.00059502
Iteration 21/25 | Loss: 0.00059502
Iteration 22/25 | Loss: 0.00059502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005950187332928181, 0.0005950187332928181, 0.0005950187332928181, 0.0005950187332928181, 0.0005950187332928181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005950187332928181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059502
Iteration 2/1000 | Loss: 0.00007166
Iteration 3/1000 | Loss: 0.00004784
Iteration 4/1000 | Loss: 0.00003883
Iteration 5/1000 | Loss: 0.00003666
Iteration 6/1000 | Loss: 0.00003529
Iteration 7/1000 | Loss: 0.00003421
Iteration 8/1000 | Loss: 0.00003334
Iteration 9/1000 | Loss: 0.00003252
Iteration 10/1000 | Loss: 0.00003193
Iteration 11/1000 | Loss: 0.00003149
Iteration 12/1000 | Loss: 0.00003121
Iteration 13/1000 | Loss: 0.00003100
Iteration 14/1000 | Loss: 0.00003084
Iteration 15/1000 | Loss: 0.00003083
Iteration 16/1000 | Loss: 0.00003079
Iteration 17/1000 | Loss: 0.00003077
Iteration 18/1000 | Loss: 0.00003076
Iteration 19/1000 | Loss: 0.00003076
Iteration 20/1000 | Loss: 0.00003072
Iteration 21/1000 | Loss: 0.00003072
Iteration 22/1000 | Loss: 0.00003071
Iteration 23/1000 | Loss: 0.00003071
Iteration 24/1000 | Loss: 0.00003070
Iteration 25/1000 | Loss: 0.00003070
Iteration 26/1000 | Loss: 0.00003070
Iteration 27/1000 | Loss: 0.00003069
Iteration 28/1000 | Loss: 0.00003069
Iteration 29/1000 | Loss: 0.00003069
Iteration 30/1000 | Loss: 0.00003068
Iteration 31/1000 | Loss: 0.00003066
Iteration 32/1000 | Loss: 0.00003064
Iteration 33/1000 | Loss: 0.00003064
Iteration 34/1000 | Loss: 0.00003063
Iteration 35/1000 | Loss: 0.00003063
Iteration 36/1000 | Loss: 0.00003063
Iteration 37/1000 | Loss: 0.00003060
Iteration 38/1000 | Loss: 0.00003059
Iteration 39/1000 | Loss: 0.00003059
Iteration 40/1000 | Loss: 0.00003058
Iteration 41/1000 | Loss: 0.00003058
Iteration 42/1000 | Loss: 0.00003058
Iteration 43/1000 | Loss: 0.00003058
Iteration 44/1000 | Loss: 0.00003057
Iteration 45/1000 | Loss: 0.00003057
Iteration 46/1000 | Loss: 0.00003056
Iteration 47/1000 | Loss: 0.00003056
Iteration 48/1000 | Loss: 0.00003054
Iteration 49/1000 | Loss: 0.00003054
Iteration 50/1000 | Loss: 0.00003054
Iteration 51/1000 | Loss: 0.00003054
Iteration 52/1000 | Loss: 0.00003054
Iteration 53/1000 | Loss: 0.00003054
Iteration 54/1000 | Loss: 0.00003054
Iteration 55/1000 | Loss: 0.00003054
Iteration 56/1000 | Loss: 0.00003054
Iteration 57/1000 | Loss: 0.00003053
Iteration 58/1000 | Loss: 0.00003053
Iteration 59/1000 | Loss: 0.00003053
Iteration 60/1000 | Loss: 0.00003053
Iteration 61/1000 | Loss: 0.00003052
Iteration 62/1000 | Loss: 0.00003051
Iteration 63/1000 | Loss: 0.00003051
Iteration 64/1000 | Loss: 0.00003051
Iteration 65/1000 | Loss: 0.00003050
Iteration 66/1000 | Loss: 0.00003050
Iteration 67/1000 | Loss: 0.00003050
Iteration 68/1000 | Loss: 0.00003049
Iteration 69/1000 | Loss: 0.00003049
Iteration 70/1000 | Loss: 0.00003049
Iteration 71/1000 | Loss: 0.00003049
Iteration 72/1000 | Loss: 0.00003049
Iteration 73/1000 | Loss: 0.00003048
Iteration 74/1000 | Loss: 0.00003048
Iteration 75/1000 | Loss: 0.00003048
Iteration 76/1000 | Loss: 0.00003047
Iteration 77/1000 | Loss: 0.00003047
Iteration 78/1000 | Loss: 0.00003047
Iteration 79/1000 | Loss: 0.00003046
Iteration 80/1000 | Loss: 0.00003046
Iteration 81/1000 | Loss: 0.00003046
Iteration 82/1000 | Loss: 0.00003045
Iteration 83/1000 | Loss: 0.00003045
Iteration 84/1000 | Loss: 0.00003044
Iteration 85/1000 | Loss: 0.00003044
Iteration 86/1000 | Loss: 0.00003044
Iteration 87/1000 | Loss: 0.00003043
Iteration 88/1000 | Loss: 0.00003043
Iteration 89/1000 | Loss: 0.00003042
Iteration 90/1000 | Loss: 0.00003042
Iteration 91/1000 | Loss: 0.00003041
Iteration 92/1000 | Loss: 0.00003041
Iteration 93/1000 | Loss: 0.00003040
Iteration 94/1000 | Loss: 0.00003040
Iteration 95/1000 | Loss: 0.00003040
Iteration 96/1000 | Loss: 0.00003040
Iteration 97/1000 | Loss: 0.00003039
Iteration 98/1000 | Loss: 0.00003039
Iteration 99/1000 | Loss: 0.00003039
Iteration 100/1000 | Loss: 0.00003039
Iteration 101/1000 | Loss: 0.00003038
Iteration 102/1000 | Loss: 0.00003038
Iteration 103/1000 | Loss: 0.00003037
Iteration 104/1000 | Loss: 0.00003037
Iteration 105/1000 | Loss: 0.00003037
Iteration 106/1000 | Loss: 0.00003037
Iteration 107/1000 | Loss: 0.00003037
Iteration 108/1000 | Loss: 0.00003037
Iteration 109/1000 | Loss: 0.00003036
Iteration 110/1000 | Loss: 0.00003036
Iteration 111/1000 | Loss: 0.00003036
Iteration 112/1000 | Loss: 0.00003036
Iteration 113/1000 | Loss: 0.00003036
Iteration 114/1000 | Loss: 0.00003036
Iteration 115/1000 | Loss: 0.00003036
Iteration 116/1000 | Loss: 0.00003036
Iteration 117/1000 | Loss: 0.00003036
Iteration 118/1000 | Loss: 0.00003036
Iteration 119/1000 | Loss: 0.00003036
Iteration 120/1000 | Loss: 0.00003036
Iteration 121/1000 | Loss: 0.00003035
Iteration 122/1000 | Loss: 0.00003035
Iteration 123/1000 | Loss: 0.00003035
Iteration 124/1000 | Loss: 0.00003035
Iteration 125/1000 | Loss: 0.00003035
Iteration 126/1000 | Loss: 0.00003035
Iteration 127/1000 | Loss: 0.00003034
Iteration 128/1000 | Loss: 0.00003034
Iteration 129/1000 | Loss: 0.00003034
Iteration 130/1000 | Loss: 0.00003033
Iteration 131/1000 | Loss: 0.00003033
Iteration 132/1000 | Loss: 0.00003033
Iteration 133/1000 | Loss: 0.00003033
Iteration 134/1000 | Loss: 0.00003033
Iteration 135/1000 | Loss: 0.00003033
Iteration 136/1000 | Loss: 0.00003033
Iteration 137/1000 | Loss: 0.00003033
Iteration 138/1000 | Loss: 0.00003033
Iteration 139/1000 | Loss: 0.00003032
Iteration 140/1000 | Loss: 0.00003032
Iteration 141/1000 | Loss: 0.00003032
Iteration 142/1000 | Loss: 0.00003032
Iteration 143/1000 | Loss: 0.00003032
Iteration 144/1000 | Loss: 0.00003031
Iteration 145/1000 | Loss: 0.00003031
Iteration 146/1000 | Loss: 0.00003031
Iteration 147/1000 | Loss: 0.00003031
Iteration 148/1000 | Loss: 0.00003030
Iteration 149/1000 | Loss: 0.00003030
Iteration 150/1000 | Loss: 0.00003030
Iteration 151/1000 | Loss: 0.00003030
Iteration 152/1000 | Loss: 0.00003030
Iteration 153/1000 | Loss: 0.00003030
Iteration 154/1000 | Loss: 0.00003030
Iteration 155/1000 | Loss: 0.00003030
Iteration 156/1000 | Loss: 0.00003030
Iteration 157/1000 | Loss: 0.00003030
Iteration 158/1000 | Loss: 0.00003030
Iteration 159/1000 | Loss: 0.00003030
Iteration 160/1000 | Loss: 0.00003030
Iteration 161/1000 | Loss: 0.00003030
Iteration 162/1000 | Loss: 0.00003030
Iteration 163/1000 | Loss: 0.00003030
Iteration 164/1000 | Loss: 0.00003030
Iteration 165/1000 | Loss: 0.00003030
Iteration 166/1000 | Loss: 0.00003030
Iteration 167/1000 | Loss: 0.00003030
Iteration 168/1000 | Loss: 0.00003030
Iteration 169/1000 | Loss: 0.00003030
Iteration 170/1000 | Loss: 0.00003030
Iteration 171/1000 | Loss: 0.00003030
Iteration 172/1000 | Loss: 0.00003030
Iteration 173/1000 | Loss: 0.00003030
Iteration 174/1000 | Loss: 0.00003030
Iteration 175/1000 | Loss: 0.00003030
Iteration 176/1000 | Loss: 0.00003030
Iteration 177/1000 | Loss: 0.00003030
Iteration 178/1000 | Loss: 0.00003030
Iteration 179/1000 | Loss: 0.00003030
Iteration 180/1000 | Loss: 0.00003030
Iteration 181/1000 | Loss: 0.00003030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [3.0297431294457056e-05, 3.0297431294457056e-05, 3.0297431294457056e-05, 3.0297431294457056e-05, 3.0297431294457056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0297431294457056e-05

Optimization complete. Final v2v error: 4.435948848724365 mm

Highest mean error: 6.381093502044678 mm for frame 71

Lowest mean error: 3.477363348007202 mm for frame 101

Saving results

Total time: 44.56974196434021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084393
Iteration 2/25 | Loss: 0.00193080
Iteration 3/25 | Loss: 0.00176189
Iteration 4/25 | Loss: 0.00147837
Iteration 5/25 | Loss: 0.00136066
Iteration 6/25 | Loss: 0.00137026
Iteration 7/25 | Loss: 0.00137475
Iteration 8/25 | Loss: 0.00142309
Iteration 9/25 | Loss: 0.00134546
Iteration 10/25 | Loss: 0.00126935
Iteration 11/25 | Loss: 0.00128614
Iteration 12/25 | Loss: 0.00127965
Iteration 13/25 | Loss: 0.00124918
Iteration 14/25 | Loss: 0.00118786
Iteration 15/25 | Loss: 0.00118180
Iteration 16/25 | Loss: 0.00118629
Iteration 17/25 | Loss: 0.00116963
Iteration 18/25 | Loss: 0.00116720
Iteration 19/25 | Loss: 0.00116459
Iteration 20/25 | Loss: 0.00117414
Iteration 21/25 | Loss: 0.00117394
Iteration 22/25 | Loss: 0.00117536
Iteration 23/25 | Loss: 0.00113992
Iteration 24/25 | Loss: 0.00111934
Iteration 25/25 | Loss: 0.00111933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34400618
Iteration 2/25 | Loss: 0.00095479
Iteration 3/25 | Loss: 0.00095479
Iteration 4/25 | Loss: 0.00095479
Iteration 5/25 | Loss: 0.00095479
Iteration 6/25 | Loss: 0.00095479
Iteration 7/25 | Loss: 0.00095479
Iteration 8/25 | Loss: 0.00095479
Iteration 9/25 | Loss: 0.00095479
Iteration 10/25 | Loss: 0.00095479
Iteration 11/25 | Loss: 0.00095479
Iteration 12/25 | Loss: 0.00095479
Iteration 13/25 | Loss: 0.00095479
Iteration 14/25 | Loss: 0.00095479
Iteration 15/25 | Loss: 0.00095479
Iteration 16/25 | Loss: 0.00095479
Iteration 17/25 | Loss: 0.00095479
Iteration 18/25 | Loss: 0.00095479
Iteration 19/25 | Loss: 0.00095479
Iteration 20/25 | Loss: 0.00095479
Iteration 21/25 | Loss: 0.00095479
Iteration 22/25 | Loss: 0.00095479
Iteration 23/25 | Loss: 0.00095479
Iteration 24/25 | Loss: 0.00095479
Iteration 25/25 | Loss: 0.00095479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095479
Iteration 2/1000 | Loss: 0.00020896
Iteration 3/1000 | Loss: 0.00069033
Iteration 4/1000 | Loss: 0.00176491
Iteration 5/1000 | Loss: 0.00122693
Iteration 6/1000 | Loss: 0.00193847
Iteration 7/1000 | Loss: 0.00048644
Iteration 8/1000 | Loss: 0.00042041
Iteration 9/1000 | Loss: 0.00039941
Iteration 10/1000 | Loss: 0.00045200
Iteration 11/1000 | Loss: 0.00038808
Iteration 12/1000 | Loss: 0.00044362
Iteration 13/1000 | Loss: 0.00061509
Iteration 14/1000 | Loss: 0.00053641
Iteration 15/1000 | Loss: 0.00024664
Iteration 16/1000 | Loss: 0.00104751
Iteration 17/1000 | Loss: 0.00093227
Iteration 18/1000 | Loss: 0.00103884
Iteration 19/1000 | Loss: 0.00027842
Iteration 20/1000 | Loss: 0.00028901
Iteration 21/1000 | Loss: 0.00036501
Iteration 22/1000 | Loss: 0.00027408
Iteration 23/1000 | Loss: 0.00026012
Iteration 24/1000 | Loss: 0.00025678
Iteration 25/1000 | Loss: 0.00030372
Iteration 26/1000 | Loss: 0.00068823
Iteration 27/1000 | Loss: 0.00051353
Iteration 28/1000 | Loss: 0.00041492
Iteration 29/1000 | Loss: 0.00015430
Iteration 30/1000 | Loss: 0.00028586
Iteration 31/1000 | Loss: 0.00017839
Iteration 32/1000 | Loss: 0.00023427
Iteration 33/1000 | Loss: 0.00045340
Iteration 34/1000 | Loss: 0.00047903
Iteration 35/1000 | Loss: 0.00027727
Iteration 36/1000 | Loss: 0.00044594
Iteration 37/1000 | Loss: 0.00016133
Iteration 38/1000 | Loss: 0.00030224
Iteration 39/1000 | Loss: 0.00026135
Iteration 40/1000 | Loss: 0.00018639
Iteration 41/1000 | Loss: 0.00017405
Iteration 42/1000 | Loss: 0.00015294
Iteration 43/1000 | Loss: 0.00023987
Iteration 44/1000 | Loss: 0.00018527
Iteration 45/1000 | Loss: 0.00018290
Iteration 46/1000 | Loss: 0.00029570
Iteration 47/1000 | Loss: 0.00032619
Iteration 48/1000 | Loss: 0.00090446
Iteration 49/1000 | Loss: 0.00030160
Iteration 50/1000 | Loss: 0.00031608
Iteration 51/1000 | Loss: 0.00038917
Iteration 52/1000 | Loss: 0.00043497
Iteration 53/1000 | Loss: 0.00039073
Iteration 54/1000 | Loss: 0.00023060
Iteration 55/1000 | Loss: 0.00012021
Iteration 56/1000 | Loss: 0.00022134
Iteration 57/1000 | Loss: 0.00022679
Iteration 58/1000 | Loss: 0.00020518
Iteration 59/1000 | Loss: 0.00019372
Iteration 60/1000 | Loss: 0.00016048
Iteration 61/1000 | Loss: 0.00018975
Iteration 62/1000 | Loss: 0.00029960
Iteration 63/1000 | Loss: 0.00036926
Iteration 64/1000 | Loss: 0.00020719
Iteration 65/1000 | Loss: 0.00021257
Iteration 66/1000 | Loss: 0.00037702
Iteration 67/1000 | Loss: 0.00038835
Iteration 68/1000 | Loss: 0.00031486
Iteration 69/1000 | Loss: 0.00034915
Iteration 70/1000 | Loss: 0.00038912
Iteration 71/1000 | Loss: 0.00027641
Iteration 72/1000 | Loss: 0.00021020
Iteration 73/1000 | Loss: 0.00028600
Iteration 74/1000 | Loss: 0.00012732
Iteration 75/1000 | Loss: 0.00039979
Iteration 76/1000 | Loss: 0.00035007
Iteration 77/1000 | Loss: 0.00035726
Iteration 78/1000 | Loss: 0.00020497
Iteration 79/1000 | Loss: 0.00042713
Iteration 80/1000 | Loss: 0.00076770
Iteration 81/1000 | Loss: 0.00046164
Iteration 82/1000 | Loss: 0.00035067
Iteration 83/1000 | Loss: 0.00023715
Iteration 84/1000 | Loss: 0.00048230
Iteration 85/1000 | Loss: 0.00020713
Iteration 86/1000 | Loss: 0.00057118
Iteration 87/1000 | Loss: 0.00035986
Iteration 88/1000 | Loss: 0.00021961
Iteration 89/1000 | Loss: 0.00035840
Iteration 90/1000 | Loss: 0.00053358
Iteration 91/1000 | Loss: 0.00075268
Iteration 92/1000 | Loss: 0.00058594
Iteration 93/1000 | Loss: 0.00035146
Iteration 94/1000 | Loss: 0.00054882
Iteration 95/1000 | Loss: 0.00015417
Iteration 96/1000 | Loss: 0.00046921
Iteration 97/1000 | Loss: 0.00011936
Iteration 98/1000 | Loss: 0.00014852
Iteration 99/1000 | Loss: 0.00025452
Iteration 100/1000 | Loss: 0.00017833
Iteration 101/1000 | Loss: 0.00019835
Iteration 102/1000 | Loss: 0.00018265
Iteration 103/1000 | Loss: 0.00014945
Iteration 104/1000 | Loss: 0.00021419
Iteration 105/1000 | Loss: 0.00011879
Iteration 106/1000 | Loss: 0.00021031
Iteration 107/1000 | Loss: 0.00030156
Iteration 108/1000 | Loss: 0.00013353
Iteration 109/1000 | Loss: 0.00017076
Iteration 110/1000 | Loss: 0.00015092
Iteration 111/1000 | Loss: 0.00021039
Iteration 112/1000 | Loss: 0.00022983
Iteration 113/1000 | Loss: 0.00020627
Iteration 114/1000 | Loss: 0.00021609
Iteration 115/1000 | Loss: 0.00017003
Iteration 116/1000 | Loss: 0.00022908
Iteration 117/1000 | Loss: 0.00016099
Iteration 118/1000 | Loss: 0.00031015
Iteration 119/1000 | Loss: 0.00034687
Iteration 120/1000 | Loss: 0.00018254
Iteration 121/1000 | Loss: 0.00011533
Iteration 122/1000 | Loss: 0.00011269
Iteration 123/1000 | Loss: 0.00010667
Iteration 124/1000 | Loss: 0.00029552
Iteration 125/1000 | Loss: 0.00027805
Iteration 126/1000 | Loss: 0.00054271
Iteration 127/1000 | Loss: 0.00038936
Iteration 128/1000 | Loss: 0.00054386
Iteration 129/1000 | Loss: 0.00012777
Iteration 130/1000 | Loss: 0.00034416
Iteration 131/1000 | Loss: 0.00018858
Iteration 132/1000 | Loss: 0.00011719
Iteration 133/1000 | Loss: 0.00018953
Iteration 134/1000 | Loss: 0.00008106
Iteration 135/1000 | Loss: 0.00017806
Iteration 136/1000 | Loss: 0.00019957
Iteration 137/1000 | Loss: 0.00012585
Iteration 138/1000 | Loss: 0.00013785
Iteration 139/1000 | Loss: 0.00021634
Iteration 140/1000 | Loss: 0.00023118
Iteration 141/1000 | Loss: 0.00010091
Iteration 142/1000 | Loss: 0.00033934
Iteration 143/1000 | Loss: 0.00005738
Iteration 144/1000 | Loss: 0.00004140
Iteration 145/1000 | Loss: 0.00003222
Iteration 146/1000 | Loss: 0.00003970
Iteration 147/1000 | Loss: 0.00002611
Iteration 148/1000 | Loss: 0.00004700
Iteration 149/1000 | Loss: 0.00003784
Iteration 150/1000 | Loss: 0.00005882
Iteration 151/1000 | Loss: 0.00005494
Iteration 152/1000 | Loss: 0.00004190
Iteration 153/1000 | Loss: 0.00008454
Iteration 154/1000 | Loss: 0.00003241
Iteration 155/1000 | Loss: 0.00005965
Iteration 156/1000 | Loss: 0.00005255
Iteration 157/1000 | Loss: 0.00004402
Iteration 158/1000 | Loss: 0.00004590
Iteration 159/1000 | Loss: 0.00003224
Iteration 160/1000 | Loss: 0.00003760
Iteration 161/1000 | Loss: 0.00003258
Iteration 162/1000 | Loss: 0.00004032
Iteration 163/1000 | Loss: 0.00004749
Iteration 164/1000 | Loss: 0.00004711
Iteration 165/1000 | Loss: 0.00004039
Iteration 166/1000 | Loss: 0.00003481
Iteration 167/1000 | Loss: 0.00004078
Iteration 168/1000 | Loss: 0.00003588
Iteration 169/1000 | Loss: 0.00004746
Iteration 170/1000 | Loss: 0.00004334
Iteration 171/1000 | Loss: 0.00003233
Iteration 172/1000 | Loss: 0.00004373
Iteration 173/1000 | Loss: 0.00004994
Iteration 174/1000 | Loss: 0.00004345
Iteration 175/1000 | Loss: 0.00003720
Iteration 176/1000 | Loss: 0.00005043
Iteration 177/1000 | Loss: 0.00003926
Iteration 178/1000 | Loss: 0.00004281
Iteration 179/1000 | Loss: 0.00003666
Iteration 180/1000 | Loss: 0.00006851
Iteration 181/1000 | Loss: 0.00005222
Iteration 182/1000 | Loss: 0.00006418
Iteration 183/1000 | Loss: 0.00005233
Iteration 184/1000 | Loss: 0.00006211
Iteration 185/1000 | Loss: 0.00004976
Iteration 186/1000 | Loss: 0.00005647
Iteration 187/1000 | Loss: 0.00003917
Iteration 188/1000 | Loss: 0.00004328
Iteration 189/1000 | Loss: 0.00004026
Iteration 190/1000 | Loss: 0.00005688
Iteration 191/1000 | Loss: 0.00004630
Iteration 192/1000 | Loss: 0.00005757
Iteration 193/1000 | Loss: 0.00005025
Iteration 194/1000 | Loss: 0.00005666
Iteration 195/1000 | Loss: 0.00007683
Iteration 196/1000 | Loss: 0.00006840
Iteration 197/1000 | Loss: 0.00004412
Iteration 198/1000 | Loss: 0.00003297
Iteration 199/1000 | Loss: 0.00003903
Iteration 200/1000 | Loss: 0.00004366
Iteration 201/1000 | Loss: 0.00003886
Iteration 202/1000 | Loss: 0.00004232
Iteration 203/1000 | Loss: 0.00003559
Iteration 204/1000 | Loss: 0.00004115
Iteration 205/1000 | Loss: 0.00003518
Iteration 206/1000 | Loss: 0.00003834
Iteration 207/1000 | Loss: 0.00003747
Iteration 208/1000 | Loss: 0.00003254
Iteration 209/1000 | Loss: 0.00004989
Iteration 210/1000 | Loss: 0.00004329
Iteration 211/1000 | Loss: 0.00004166
Iteration 212/1000 | Loss: 0.00004683
Iteration 213/1000 | Loss: 0.00004313
Iteration 214/1000 | Loss: 0.00003750
Iteration 215/1000 | Loss: 0.00004535
Iteration 216/1000 | Loss: 0.00005037
Iteration 217/1000 | Loss: 0.00004632
Iteration 218/1000 | Loss: 0.00004103
Iteration 219/1000 | Loss: 0.00004064
Iteration 220/1000 | Loss: 0.00003623
Iteration 221/1000 | Loss: 0.00004394
Iteration 222/1000 | Loss: 0.00003954
Iteration 223/1000 | Loss: 0.00005243
Iteration 224/1000 | Loss: 0.00003772
Iteration 225/1000 | Loss: 0.00003836
Iteration 226/1000 | Loss: 0.00005016
Iteration 227/1000 | Loss: 0.00003986
Iteration 228/1000 | Loss: 0.00003565
Iteration 229/1000 | Loss: 0.00004180
Iteration 230/1000 | Loss: 0.00004362
Iteration 231/1000 | Loss: 0.00004377
Iteration 232/1000 | Loss: 0.00003762
Iteration 233/1000 | Loss: 0.00003974
Iteration 234/1000 | Loss: 0.00003606
Iteration 235/1000 | Loss: 0.00003942
Iteration 236/1000 | Loss: 0.00003677
Iteration 237/1000 | Loss: 0.00004057
Iteration 238/1000 | Loss: 0.00004159
Iteration 239/1000 | Loss: 0.00004065
Iteration 240/1000 | Loss: 0.00003591
Iteration 241/1000 | Loss: 0.00004106
Iteration 242/1000 | Loss: 0.00003649
Iteration 243/1000 | Loss: 0.00004317
Iteration 244/1000 | Loss: 0.00003608
Iteration 245/1000 | Loss: 0.00004283
Iteration 246/1000 | Loss: 0.00003589
Iteration 247/1000 | Loss: 0.00004027
Iteration 248/1000 | Loss: 0.00003532
Iteration 249/1000 | Loss: 0.00003948
Iteration 250/1000 | Loss: 0.00003548
Iteration 251/1000 | Loss: 0.00003902
Iteration 252/1000 | Loss: 0.00003447
Iteration 253/1000 | Loss: 0.00003848
Iteration 254/1000 | Loss: 0.00003158
Iteration 255/1000 | Loss: 0.00003953
Iteration 256/1000 | Loss: 0.00003324
Iteration 257/1000 | Loss: 0.00003872
Iteration 258/1000 | Loss: 0.00003210
Iteration 259/1000 | Loss: 0.00003532
Iteration 260/1000 | Loss: 0.00003330
Iteration 261/1000 | Loss: 0.00003129
Iteration 262/1000 | Loss: 0.00002651
Iteration 263/1000 | Loss: 0.00003140
Iteration 264/1000 | Loss: 0.00002289
Iteration 265/1000 | Loss: 0.00004696
Iteration 266/1000 | Loss: 0.00003553
Iteration 267/1000 | Loss: 0.00003406
Iteration 268/1000 | Loss: 0.00003952
Iteration 269/1000 | Loss: 0.00002585
Iteration 270/1000 | Loss: 0.00004388
Iteration 271/1000 | Loss: 0.00002254
Iteration 272/1000 | Loss: 0.00003775
Iteration 273/1000 | Loss: 0.00003216
Iteration 274/1000 | Loss: 0.00003685
Iteration 275/1000 | Loss: 0.00003514
Iteration 276/1000 | Loss: 0.00006080
Iteration 277/1000 | Loss: 0.00003625
Iteration 278/1000 | Loss: 0.00005916
Iteration 279/1000 | Loss: 0.00004866
Iteration 280/1000 | Loss: 0.00004775
Iteration 281/1000 | Loss: 0.00004567
Iteration 282/1000 | Loss: 0.00005826
Iteration 283/1000 | Loss: 0.00004002
Iteration 284/1000 | Loss: 0.00003871
Iteration 285/1000 | Loss: 0.00004036
Iteration 286/1000 | Loss: 0.00003686
Iteration 287/1000 | Loss: 0.00004014
Iteration 288/1000 | Loss: 0.00003681
Iteration 289/1000 | Loss: 0.00003831
Iteration 290/1000 | Loss: 0.00005097
Iteration 291/1000 | Loss: 0.00003548
Iteration 292/1000 | Loss: 0.00002634
Iteration 293/1000 | Loss: 0.00004496
Iteration 294/1000 | Loss: 0.00003445
Iteration 295/1000 | Loss: 0.00003961
Iteration 296/1000 | Loss: 0.00003537
Iteration 297/1000 | Loss: 0.00003949
Iteration 298/1000 | Loss: 0.00003534
Iteration 299/1000 | Loss: 0.00004114
Iteration 300/1000 | Loss: 0.00004057
Iteration 301/1000 | Loss: 0.00004596
Iteration 302/1000 | Loss: 0.00003533
Iteration 303/1000 | Loss: 0.00004033
Iteration 304/1000 | Loss: 0.00004755
Iteration 305/1000 | Loss: 0.00006076
Iteration 306/1000 | Loss: 0.00002492
Iteration 307/1000 | Loss: 0.00004348
Iteration 308/1000 | Loss: 0.00004688
Iteration 309/1000 | Loss: 0.00003783
Iteration 310/1000 | Loss: 0.00004291
Iteration 311/1000 | Loss: 0.00003706
Iteration 312/1000 | Loss: 0.00003492
Iteration 313/1000 | Loss: 0.00003721
Iteration 314/1000 | Loss: 0.00003487
Iteration 315/1000 | Loss: 0.00003487
Iteration 316/1000 | Loss: 0.00006085
Iteration 317/1000 | Loss: 0.00004595
Iteration 318/1000 | Loss: 0.00003734
Iteration 319/1000 | Loss: 0.00003606
Iteration 320/1000 | Loss: 0.00003577
Iteration 321/1000 | Loss: 0.00003550
Iteration 322/1000 | Loss: 0.00003612
Iteration 323/1000 | Loss: 0.00003469
Iteration 324/1000 | Loss: 0.00003679
Iteration 325/1000 | Loss: 0.00004191
Iteration 326/1000 | Loss: 0.00006501
Iteration 327/1000 | Loss: 0.00005143
Iteration 328/1000 | Loss: 0.00002207
Iteration 329/1000 | Loss: 0.00001971
Iteration 330/1000 | Loss: 0.00001902
Iteration 331/1000 | Loss: 0.00001859
Iteration 332/1000 | Loss: 0.00001832
Iteration 333/1000 | Loss: 0.00001800
Iteration 334/1000 | Loss: 0.00001784
Iteration 335/1000 | Loss: 0.00001771
Iteration 336/1000 | Loss: 0.00001771
Iteration 337/1000 | Loss: 0.00001769
Iteration 338/1000 | Loss: 0.00001769
Iteration 339/1000 | Loss: 0.00001768
Iteration 340/1000 | Loss: 0.00001767
Iteration 341/1000 | Loss: 0.00001767
Iteration 342/1000 | Loss: 0.00001765
Iteration 343/1000 | Loss: 0.00001764
Iteration 344/1000 | Loss: 0.00001763
Iteration 345/1000 | Loss: 0.00001761
Iteration 346/1000 | Loss: 0.00001761
Iteration 347/1000 | Loss: 0.00001760
Iteration 348/1000 | Loss: 0.00001757
Iteration 349/1000 | Loss: 0.00001753
Iteration 350/1000 | Loss: 0.00001753
Iteration 351/1000 | Loss: 0.00001752
Iteration 352/1000 | Loss: 0.00001751
Iteration 353/1000 | Loss: 0.00001751
Iteration 354/1000 | Loss: 0.00001751
Iteration 355/1000 | Loss: 0.00001751
Iteration 356/1000 | Loss: 0.00001751
Iteration 357/1000 | Loss: 0.00001750
Iteration 358/1000 | Loss: 0.00001750
Iteration 359/1000 | Loss: 0.00001750
Iteration 360/1000 | Loss: 0.00001750
Iteration 361/1000 | Loss: 0.00001750
Iteration 362/1000 | Loss: 0.00001750
Iteration 363/1000 | Loss: 0.00001749
Iteration 364/1000 | Loss: 0.00001749
Iteration 365/1000 | Loss: 0.00001747
Iteration 366/1000 | Loss: 0.00001745
Iteration 367/1000 | Loss: 0.00001745
Iteration 368/1000 | Loss: 0.00001745
Iteration 369/1000 | Loss: 0.00001745
Iteration 370/1000 | Loss: 0.00001745
Iteration 371/1000 | Loss: 0.00001745
Iteration 372/1000 | Loss: 0.00001745
Iteration 373/1000 | Loss: 0.00001744
Iteration 374/1000 | Loss: 0.00001744
Iteration 375/1000 | Loss: 0.00001744
Iteration 376/1000 | Loss: 0.00001742
Iteration 377/1000 | Loss: 0.00001742
Iteration 378/1000 | Loss: 0.00001742
Iteration 379/1000 | Loss: 0.00001742
Iteration 380/1000 | Loss: 0.00001742
Iteration 381/1000 | Loss: 0.00001742
Iteration 382/1000 | Loss: 0.00001742
Iteration 383/1000 | Loss: 0.00001741
Iteration 384/1000 | Loss: 0.00001741
Iteration 385/1000 | Loss: 0.00001741
Iteration 386/1000 | Loss: 0.00001740
Iteration 387/1000 | Loss: 0.00001740
Iteration 388/1000 | Loss: 0.00001740
Iteration 389/1000 | Loss: 0.00001740
Iteration 390/1000 | Loss: 0.00001740
Iteration 391/1000 | Loss: 0.00001740
Iteration 392/1000 | Loss: 0.00001739
Iteration 393/1000 | Loss: 0.00001739
Iteration 394/1000 | Loss: 0.00001739
Iteration 395/1000 | Loss: 0.00001739
Iteration 396/1000 | Loss: 0.00001739
Iteration 397/1000 | Loss: 0.00001739
Iteration 398/1000 | Loss: 0.00001739
Iteration 399/1000 | Loss: 0.00001739
Iteration 400/1000 | Loss: 0.00001739
Iteration 401/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 401. Stopping optimization.
Last 5 losses: [1.7393518646713346e-05, 1.7393518646713346e-05, 1.7393518646713346e-05, 1.7393518646713346e-05, 1.7393518646713346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7393518646713346e-05

Optimization complete. Final v2v error: 3.4399001598358154 mm

Highest mean error: 9.34066104888916 mm for frame 84

Lowest mean error: 2.879739761352539 mm for frame 63

Saving results

Total time: 513.3125324249268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771309
Iteration 2/25 | Loss: 0.00132505
Iteration 3/25 | Loss: 0.00119469
Iteration 4/25 | Loss: 0.00117235
Iteration 5/25 | Loss: 0.00116539
Iteration 6/25 | Loss: 0.00116413
Iteration 7/25 | Loss: 0.00116393
Iteration 8/25 | Loss: 0.00116393
Iteration 9/25 | Loss: 0.00116393
Iteration 10/25 | Loss: 0.00116393
Iteration 11/25 | Loss: 0.00116393
Iteration 12/25 | Loss: 0.00116393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011639324948191643, 0.0011639324948191643, 0.0011639324948191643, 0.0011639324948191643, 0.0011639324948191643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011639324948191643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.19226766
Iteration 2/25 | Loss: 0.00061882
Iteration 3/25 | Loss: 0.00061882
Iteration 4/25 | Loss: 0.00061882
Iteration 5/25 | Loss: 0.00061882
Iteration 6/25 | Loss: 0.00061882
Iteration 7/25 | Loss: 0.00061882
Iteration 8/25 | Loss: 0.00061882
Iteration 9/25 | Loss: 0.00061882
Iteration 10/25 | Loss: 0.00061882
Iteration 11/25 | Loss: 0.00061882
Iteration 12/25 | Loss: 0.00061882
Iteration 13/25 | Loss: 0.00061882
Iteration 14/25 | Loss: 0.00061882
Iteration 15/25 | Loss: 0.00061882
Iteration 16/25 | Loss: 0.00061882
Iteration 17/25 | Loss: 0.00061882
Iteration 18/25 | Loss: 0.00061882
Iteration 19/25 | Loss: 0.00061882
Iteration 20/25 | Loss: 0.00061882
Iteration 21/25 | Loss: 0.00061882
Iteration 22/25 | Loss: 0.00061882
Iteration 23/25 | Loss: 0.00061882
Iteration 24/25 | Loss: 0.00061882
Iteration 25/25 | Loss: 0.00061882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061882
Iteration 2/1000 | Loss: 0.00004367
Iteration 3/1000 | Loss: 0.00002417
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002027
Iteration 6/1000 | Loss: 0.00001951
Iteration 7/1000 | Loss: 0.00001887
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001741
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001720
Iteration 15/1000 | Loss: 0.00001717
Iteration 16/1000 | Loss: 0.00001714
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001705
Iteration 20/1000 | Loss: 0.00001705
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001698
Iteration 24/1000 | Loss: 0.00001694
Iteration 25/1000 | Loss: 0.00001694
Iteration 26/1000 | Loss: 0.00001694
Iteration 27/1000 | Loss: 0.00001693
Iteration 28/1000 | Loss: 0.00001693
Iteration 29/1000 | Loss: 0.00001693
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001693
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001692
Iteration 43/1000 | Loss: 0.00001692
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.69210306921741e-05, 1.69210306921741e-05, 1.69210306921741e-05, 1.69210306921741e-05, 1.69210306921741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.69210306921741e-05

Optimization complete. Final v2v error: 3.557243585586548 mm

Highest mean error: 3.8608975410461426 mm for frame 51

Lowest mean error: 3.3078978061676025 mm for frame 61

Saving results

Total time: 31.426832675933838
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541067
Iteration 2/25 | Loss: 0.00134488
Iteration 3/25 | Loss: 0.00122683
Iteration 4/25 | Loss: 0.00121521
Iteration 5/25 | Loss: 0.00121056
Iteration 6/25 | Loss: 0.00120954
Iteration 7/25 | Loss: 0.00120954
Iteration 8/25 | Loss: 0.00120954
Iteration 9/25 | Loss: 0.00120954
Iteration 10/25 | Loss: 0.00120954
Iteration 11/25 | Loss: 0.00120954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012095398269593716, 0.0012095398269593716, 0.0012095398269593716, 0.0012095398269593716, 0.0012095398269593716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012095398269593716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.36066484
Iteration 2/25 | Loss: 0.00065928
Iteration 3/25 | Loss: 0.00065914
Iteration 4/25 | Loss: 0.00065914
Iteration 5/25 | Loss: 0.00065914
Iteration 6/25 | Loss: 0.00065914
Iteration 7/25 | Loss: 0.00065914
Iteration 8/25 | Loss: 0.00065914
Iteration 9/25 | Loss: 0.00065914
Iteration 10/25 | Loss: 0.00065914
Iteration 11/25 | Loss: 0.00065914
Iteration 12/25 | Loss: 0.00065914
Iteration 13/25 | Loss: 0.00065914
Iteration 14/25 | Loss: 0.00065914
Iteration 15/25 | Loss: 0.00065914
Iteration 16/25 | Loss: 0.00065914
Iteration 17/25 | Loss: 0.00065914
Iteration 18/25 | Loss: 0.00065914
Iteration 19/25 | Loss: 0.00065914
Iteration 20/25 | Loss: 0.00065914
Iteration 21/25 | Loss: 0.00065914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000659136741887778, 0.000659136741887778, 0.000659136741887778, 0.000659136741887778, 0.000659136741887778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000659136741887778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065914
Iteration 2/1000 | Loss: 0.00006268
Iteration 3/1000 | Loss: 0.00002992
Iteration 4/1000 | Loss: 0.00002061
Iteration 5/1000 | Loss: 0.00001871
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001661
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001570
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001559
Iteration 14/1000 | Loss: 0.00001556
Iteration 15/1000 | Loss: 0.00001556
Iteration 16/1000 | Loss: 0.00001556
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001552
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001543
Iteration 23/1000 | Loss: 0.00001538
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001529
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001524
Iteration 30/1000 | Loss: 0.00001523
Iteration 31/1000 | Loss: 0.00001523
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001523
Iteration 34/1000 | Loss: 0.00001522
Iteration 35/1000 | Loss: 0.00001522
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001521
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001521
Iteration 40/1000 | Loss: 0.00001521
Iteration 41/1000 | Loss: 0.00001520
Iteration 42/1000 | Loss: 0.00001520
Iteration 43/1000 | Loss: 0.00001520
Iteration 44/1000 | Loss: 0.00001520
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001517
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001512
Iteration 68/1000 | Loss: 0.00001512
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001511
Iteration 73/1000 | Loss: 0.00001511
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001510
Iteration 79/1000 | Loss: 0.00001510
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001510
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001509
Iteration 85/1000 | Loss: 0.00001509
Iteration 86/1000 | Loss: 0.00001509
Iteration 87/1000 | Loss: 0.00001509
Iteration 88/1000 | Loss: 0.00001509
Iteration 89/1000 | Loss: 0.00001509
Iteration 90/1000 | Loss: 0.00001509
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001509
Iteration 93/1000 | Loss: 0.00001509
Iteration 94/1000 | Loss: 0.00001509
Iteration 95/1000 | Loss: 0.00001509
Iteration 96/1000 | Loss: 0.00001509
Iteration 97/1000 | Loss: 0.00001509
Iteration 98/1000 | Loss: 0.00001509
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001508
Iteration 102/1000 | Loss: 0.00001508
Iteration 103/1000 | Loss: 0.00001508
Iteration 104/1000 | Loss: 0.00001508
Iteration 105/1000 | Loss: 0.00001508
Iteration 106/1000 | Loss: 0.00001508
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001507
Iteration 118/1000 | Loss: 0.00001507
Iteration 119/1000 | Loss: 0.00001507
Iteration 120/1000 | Loss: 0.00001507
Iteration 121/1000 | Loss: 0.00001507
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001507
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001507
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001506
Iteration 131/1000 | Loss: 0.00001506
Iteration 132/1000 | Loss: 0.00001506
Iteration 133/1000 | Loss: 0.00001506
Iteration 134/1000 | Loss: 0.00001506
Iteration 135/1000 | Loss: 0.00001506
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001505
Iteration 138/1000 | Loss: 0.00001505
Iteration 139/1000 | Loss: 0.00001505
Iteration 140/1000 | Loss: 0.00001505
Iteration 141/1000 | Loss: 0.00001505
Iteration 142/1000 | Loss: 0.00001505
Iteration 143/1000 | Loss: 0.00001505
Iteration 144/1000 | Loss: 0.00001505
Iteration 145/1000 | Loss: 0.00001505
Iteration 146/1000 | Loss: 0.00001505
Iteration 147/1000 | Loss: 0.00001504
Iteration 148/1000 | Loss: 0.00001504
Iteration 149/1000 | Loss: 0.00001504
Iteration 150/1000 | Loss: 0.00001504
Iteration 151/1000 | Loss: 0.00001504
Iteration 152/1000 | Loss: 0.00001504
Iteration 153/1000 | Loss: 0.00001503
Iteration 154/1000 | Loss: 0.00001503
Iteration 155/1000 | Loss: 0.00001503
Iteration 156/1000 | Loss: 0.00001503
Iteration 157/1000 | Loss: 0.00001503
Iteration 158/1000 | Loss: 0.00001503
Iteration 159/1000 | Loss: 0.00001503
Iteration 160/1000 | Loss: 0.00001503
Iteration 161/1000 | Loss: 0.00001503
Iteration 162/1000 | Loss: 0.00001503
Iteration 163/1000 | Loss: 0.00001503
Iteration 164/1000 | Loss: 0.00001503
Iteration 165/1000 | Loss: 0.00001503
Iteration 166/1000 | Loss: 0.00001503
Iteration 167/1000 | Loss: 0.00001503
Iteration 168/1000 | Loss: 0.00001502
Iteration 169/1000 | Loss: 0.00001502
Iteration 170/1000 | Loss: 0.00001502
Iteration 171/1000 | Loss: 0.00001502
Iteration 172/1000 | Loss: 0.00001502
Iteration 173/1000 | Loss: 0.00001502
Iteration 174/1000 | Loss: 0.00001502
Iteration 175/1000 | Loss: 0.00001502
Iteration 176/1000 | Loss: 0.00001502
Iteration 177/1000 | Loss: 0.00001502
Iteration 178/1000 | Loss: 0.00001502
Iteration 179/1000 | Loss: 0.00001502
Iteration 180/1000 | Loss: 0.00001502
Iteration 181/1000 | Loss: 0.00001502
Iteration 182/1000 | Loss: 0.00001502
Iteration 183/1000 | Loss: 0.00001502
Iteration 184/1000 | Loss: 0.00001502
Iteration 185/1000 | Loss: 0.00001502
Iteration 186/1000 | Loss: 0.00001502
Iteration 187/1000 | Loss: 0.00001502
Iteration 188/1000 | Loss: 0.00001502
Iteration 189/1000 | Loss: 0.00001502
Iteration 190/1000 | Loss: 0.00001502
Iteration 191/1000 | Loss: 0.00001502
Iteration 192/1000 | Loss: 0.00001502
Iteration 193/1000 | Loss: 0.00001502
Iteration 194/1000 | Loss: 0.00001502
Iteration 195/1000 | Loss: 0.00001502
Iteration 196/1000 | Loss: 0.00001502
Iteration 197/1000 | Loss: 0.00001502
Iteration 198/1000 | Loss: 0.00001502
Iteration 199/1000 | Loss: 0.00001502
Iteration 200/1000 | Loss: 0.00001502
Iteration 201/1000 | Loss: 0.00001502
Iteration 202/1000 | Loss: 0.00001502
Iteration 203/1000 | Loss: 0.00001502
Iteration 204/1000 | Loss: 0.00001502
Iteration 205/1000 | Loss: 0.00001502
Iteration 206/1000 | Loss: 0.00001502
Iteration 207/1000 | Loss: 0.00001502
Iteration 208/1000 | Loss: 0.00001502
Iteration 209/1000 | Loss: 0.00001502
Iteration 210/1000 | Loss: 0.00001502
Iteration 211/1000 | Loss: 0.00001502
Iteration 212/1000 | Loss: 0.00001502
Iteration 213/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.5018733392935246e-05, 1.5018733392935246e-05, 1.5018733392935246e-05, 1.5018733392935246e-05, 1.5018733392935246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5018733392935246e-05

Optimization complete. Final v2v error: 3.398958921432495 mm

Highest mean error: 3.732187271118164 mm for frame 129

Lowest mean error: 3.1115427017211914 mm for frame 0

Saving results

Total time: 40.731576919555664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_nl_1267/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_nl_1267/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459167
Iteration 2/25 | Loss: 0.00124425
Iteration 3/25 | Loss: 0.00116143
Iteration 4/25 | Loss: 0.00114957
Iteration 5/25 | Loss: 0.00114399
Iteration 6/25 | Loss: 0.00114298
Iteration 7/25 | Loss: 0.00114298
Iteration 8/25 | Loss: 0.00114298
Iteration 9/25 | Loss: 0.00114298
Iteration 10/25 | Loss: 0.00114298
Iteration 11/25 | Loss: 0.00114298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011429842561483383, 0.0011429842561483383, 0.0011429842561483383, 0.0011429842561483383, 0.0011429842561483383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011429842561483383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33684707
Iteration 2/25 | Loss: 0.00065569
Iteration 3/25 | Loss: 0.00065569
Iteration 4/25 | Loss: 0.00065569
Iteration 5/25 | Loss: 0.00065569
Iteration 6/25 | Loss: 0.00065568
Iteration 7/25 | Loss: 0.00065568
Iteration 8/25 | Loss: 0.00065568
Iteration 9/25 | Loss: 0.00065568
Iteration 10/25 | Loss: 0.00065568
Iteration 11/25 | Loss: 0.00065568
Iteration 12/25 | Loss: 0.00065568
Iteration 13/25 | Loss: 0.00065568
Iteration 14/25 | Loss: 0.00065568
Iteration 15/25 | Loss: 0.00065568
Iteration 16/25 | Loss: 0.00065568
Iteration 17/25 | Loss: 0.00065568
Iteration 18/25 | Loss: 0.00065568
Iteration 19/25 | Loss: 0.00065568
Iteration 20/25 | Loss: 0.00065568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006556829903274775, 0.0006556829903274775, 0.0006556829903274775, 0.0006556829903274775, 0.0006556829903274775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006556829903274775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065568
Iteration 2/1000 | Loss: 0.00002821
Iteration 3/1000 | Loss: 0.00002001
Iteration 4/1000 | Loss: 0.00001900
Iteration 5/1000 | Loss: 0.00001792
Iteration 6/1000 | Loss: 0.00001737
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001693
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001683
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001671
Iteration 15/1000 | Loss: 0.00001671
Iteration 16/1000 | Loss: 0.00001663
Iteration 17/1000 | Loss: 0.00001658
Iteration 18/1000 | Loss: 0.00001657
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001656
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001655
Iteration 24/1000 | Loss: 0.00001654
Iteration 25/1000 | Loss: 0.00001654
Iteration 26/1000 | Loss: 0.00001654
Iteration 27/1000 | Loss: 0.00001654
Iteration 28/1000 | Loss: 0.00001654
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001654
Iteration 31/1000 | Loss: 0.00001654
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00001649
Iteration 36/1000 | Loss: 0.00001649
Iteration 37/1000 | Loss: 0.00001649
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001647
Iteration 40/1000 | Loss: 0.00001647
Iteration 41/1000 | Loss: 0.00001646
Iteration 42/1000 | Loss: 0.00001646
Iteration 43/1000 | Loss: 0.00001646
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001645
Iteration 46/1000 | Loss: 0.00001645
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001641
Iteration 50/1000 | Loss: 0.00001641
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001636
Iteration 55/1000 | Loss: 0.00001636
Iteration 56/1000 | Loss: 0.00001636
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001636
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001634
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001633
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001633
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001632
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001630
Iteration 79/1000 | Loss: 0.00001630
Iteration 80/1000 | Loss: 0.00001630
Iteration 81/1000 | Loss: 0.00001629
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001629
Iteration 85/1000 | Loss: 0.00001629
Iteration 86/1000 | Loss: 0.00001629
Iteration 87/1000 | Loss: 0.00001629
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001629
Iteration 90/1000 | Loss: 0.00001628
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001628
Iteration 93/1000 | Loss: 0.00001628
Iteration 94/1000 | Loss: 0.00001628
Iteration 95/1000 | Loss: 0.00001627
Iteration 96/1000 | Loss: 0.00001627
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001626
Iteration 99/1000 | Loss: 0.00001626
Iteration 100/1000 | Loss: 0.00001625
Iteration 101/1000 | Loss: 0.00001624
Iteration 102/1000 | Loss: 0.00001624
Iteration 103/1000 | Loss: 0.00001624
Iteration 104/1000 | Loss: 0.00001623
Iteration 105/1000 | Loss: 0.00001623
Iteration 106/1000 | Loss: 0.00001623
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001622
Iteration 110/1000 | Loss: 0.00001622
Iteration 111/1000 | Loss: 0.00001622
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001620
Iteration 117/1000 | Loss: 0.00001620
Iteration 118/1000 | Loss: 0.00001620
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001619
Iteration 126/1000 | Loss: 0.00001619
Iteration 127/1000 | Loss: 0.00001619
Iteration 128/1000 | Loss: 0.00001619
Iteration 129/1000 | Loss: 0.00001619
Iteration 130/1000 | Loss: 0.00001619
Iteration 131/1000 | Loss: 0.00001619
Iteration 132/1000 | Loss: 0.00001619
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001618
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00001618
Iteration 140/1000 | Loss: 0.00001618
Iteration 141/1000 | Loss: 0.00001618
Iteration 142/1000 | Loss: 0.00001618
Iteration 143/1000 | Loss: 0.00001618
Iteration 144/1000 | Loss: 0.00001618
Iteration 145/1000 | Loss: 0.00001618
Iteration 146/1000 | Loss: 0.00001618
Iteration 147/1000 | Loss: 0.00001618
Iteration 148/1000 | Loss: 0.00001618
Iteration 149/1000 | Loss: 0.00001618
Iteration 150/1000 | Loss: 0.00001618
Iteration 151/1000 | Loss: 0.00001618
Iteration 152/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.6177233192138374e-05, 1.6177233192138374e-05, 1.6177233192138374e-05, 1.6177233192138374e-05, 1.6177233192138374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6177233192138374e-05

Optimization complete. Final v2v error: 3.4596259593963623 mm

Highest mean error: 3.6522183418273926 mm for frame 51

Lowest mean error: 3.3143439292907715 mm for frame 0

Saving results

Total time: 34.163642168045044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532961
Iteration 2/25 | Loss: 0.00124923
Iteration 3/25 | Loss: 0.00103738
Iteration 4/25 | Loss: 0.00101977
Iteration 5/25 | Loss: 0.00101518
Iteration 6/25 | Loss: 0.00101551
Iteration 7/25 | Loss: 0.00101302
Iteration 8/25 | Loss: 0.00101124
Iteration 9/25 | Loss: 0.00101282
Iteration 10/25 | Loss: 0.00101040
Iteration 11/25 | Loss: 0.00100869
Iteration 12/25 | Loss: 0.00100797
Iteration 13/25 | Loss: 0.00100753
Iteration 14/25 | Loss: 0.00100736
Iteration 15/25 | Loss: 0.00100726
Iteration 16/25 | Loss: 0.00100724
Iteration 17/25 | Loss: 0.00100724
Iteration 18/25 | Loss: 0.00100724
Iteration 19/25 | Loss: 0.00100724
Iteration 20/25 | Loss: 0.00100723
Iteration 21/25 | Loss: 0.00100723
Iteration 22/25 | Loss: 0.00100723
Iteration 23/25 | Loss: 0.00100723
Iteration 24/25 | Loss: 0.00100723
Iteration 25/25 | Loss: 0.00100723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58655119
Iteration 2/25 | Loss: 0.00106371
Iteration 3/25 | Loss: 0.00106370
Iteration 4/25 | Loss: 0.00106370
Iteration 5/25 | Loss: 0.00106370
Iteration 6/25 | Loss: 0.00106370
Iteration 7/25 | Loss: 0.00106370
Iteration 8/25 | Loss: 0.00106370
Iteration 9/25 | Loss: 0.00106370
Iteration 10/25 | Loss: 0.00106370
Iteration 11/25 | Loss: 0.00106370
Iteration 12/25 | Loss: 0.00106370
Iteration 13/25 | Loss: 0.00106370
Iteration 14/25 | Loss: 0.00106370
Iteration 15/25 | Loss: 0.00106370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010637001832947135, 0.0010637001832947135, 0.0010637001832947135, 0.0010637001832947135, 0.0010637001832947135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010637001832947135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106370
Iteration 2/1000 | Loss: 0.00003969
Iteration 3/1000 | Loss: 0.00036274
Iteration 4/1000 | Loss: 0.00002796
Iteration 5/1000 | Loss: 0.00001638
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001240
Iteration 9/1000 | Loss: 0.00001207
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001158
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001135
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001134
Iteration 17/1000 | Loss: 0.00001134
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001131
Iteration 20/1000 | Loss: 0.00001131
Iteration 21/1000 | Loss: 0.00001130
Iteration 22/1000 | Loss: 0.00001129
Iteration 23/1000 | Loss: 0.00001129
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001128
Iteration 26/1000 | Loss: 0.00001128
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001116
Iteration 33/1000 | Loss: 0.00001116
Iteration 34/1000 | Loss: 0.00001116
Iteration 35/1000 | Loss: 0.00001116
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001116
Iteration 38/1000 | Loss: 0.00001116
Iteration 39/1000 | Loss: 0.00001116
Iteration 40/1000 | Loss: 0.00001115
Iteration 41/1000 | Loss: 0.00001115
Iteration 42/1000 | Loss: 0.00001113
Iteration 43/1000 | Loss: 0.00001113
Iteration 44/1000 | Loss: 0.00001112
Iteration 45/1000 | Loss: 0.00001112
Iteration 46/1000 | Loss: 0.00001112
Iteration 47/1000 | Loss: 0.00001112
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001109
Iteration 59/1000 | Loss: 0.00001109
Iteration 60/1000 | Loss: 0.00001109
Iteration 61/1000 | Loss: 0.00001109
Iteration 62/1000 | Loss: 0.00001109
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001108
Iteration 65/1000 | Loss: 0.00001108
Iteration 66/1000 | Loss: 0.00001108
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001107
Iteration 70/1000 | Loss: 0.00001107
Iteration 71/1000 | Loss: 0.00001107
Iteration 72/1000 | Loss: 0.00001106
Iteration 73/1000 | Loss: 0.00001106
Iteration 74/1000 | Loss: 0.00001106
Iteration 75/1000 | Loss: 0.00001106
Iteration 76/1000 | Loss: 0.00001105
Iteration 77/1000 | Loss: 0.00001105
Iteration 78/1000 | Loss: 0.00001104
Iteration 79/1000 | Loss: 0.00001104
Iteration 80/1000 | Loss: 0.00001103
Iteration 81/1000 | Loss: 0.00001103
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001102
Iteration 86/1000 | Loss: 0.00001102
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001102
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001100
Iteration 97/1000 | Loss: 0.00001100
Iteration 98/1000 | Loss: 0.00001099
Iteration 99/1000 | Loss: 0.00001099
Iteration 100/1000 | Loss: 0.00001099
Iteration 101/1000 | Loss: 0.00001098
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001459
Iteration 104/1000 | Loss: 0.00001228
Iteration 105/1000 | Loss: 0.00001106
Iteration 106/1000 | Loss: 0.00001085
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001066
Iteration 109/1000 | Loss: 0.00001065
Iteration 110/1000 | Loss: 0.00001064
Iteration 111/1000 | Loss: 0.00001064
Iteration 112/1000 | Loss: 0.00001063
Iteration 113/1000 | Loss: 0.00001062
Iteration 114/1000 | Loss: 0.00001062
Iteration 115/1000 | Loss: 0.00001061
Iteration 116/1000 | Loss: 0.00001061
Iteration 117/1000 | Loss: 0.00001061
Iteration 118/1000 | Loss: 0.00001061
Iteration 119/1000 | Loss: 0.00001060
Iteration 120/1000 | Loss: 0.00001060
Iteration 121/1000 | Loss: 0.00001060
Iteration 122/1000 | Loss: 0.00001059
Iteration 123/1000 | Loss: 0.00001059
Iteration 124/1000 | Loss: 0.00001059
Iteration 125/1000 | Loss: 0.00001059
Iteration 126/1000 | Loss: 0.00001059
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001058
Iteration 129/1000 | Loss: 0.00001058
Iteration 130/1000 | Loss: 0.00001058
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001057
Iteration 135/1000 | Loss: 0.00001057
Iteration 136/1000 | Loss: 0.00001057
Iteration 137/1000 | Loss: 0.00001057
Iteration 138/1000 | Loss: 0.00001057
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001056
Iteration 141/1000 | Loss: 0.00001056
Iteration 142/1000 | Loss: 0.00001056
Iteration 143/1000 | Loss: 0.00001055
Iteration 144/1000 | Loss: 0.00001055
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001054
Iteration 147/1000 | Loss: 0.00001054
Iteration 148/1000 | Loss: 0.00001054
Iteration 149/1000 | Loss: 0.00001054
Iteration 150/1000 | Loss: 0.00001054
Iteration 151/1000 | Loss: 0.00001053
Iteration 152/1000 | Loss: 0.00001052
Iteration 153/1000 | Loss: 0.00001052
Iteration 154/1000 | Loss: 0.00001052
Iteration 155/1000 | Loss: 0.00001051
Iteration 156/1000 | Loss: 0.00001051
Iteration 157/1000 | Loss: 0.00001050
Iteration 158/1000 | Loss: 0.00001050
Iteration 159/1000 | Loss: 0.00001050
Iteration 160/1000 | Loss: 0.00001050
Iteration 161/1000 | Loss: 0.00001050
Iteration 162/1000 | Loss: 0.00001050
Iteration 163/1000 | Loss: 0.00001050
Iteration 164/1000 | Loss: 0.00001050
Iteration 165/1000 | Loss: 0.00001050
Iteration 166/1000 | Loss: 0.00001050
Iteration 167/1000 | Loss: 0.00001049
Iteration 168/1000 | Loss: 0.00001049
Iteration 169/1000 | Loss: 0.00001049
Iteration 170/1000 | Loss: 0.00001049
Iteration 171/1000 | Loss: 0.00001049
Iteration 172/1000 | Loss: 0.00001049
Iteration 173/1000 | Loss: 0.00001049
Iteration 174/1000 | Loss: 0.00001048
Iteration 175/1000 | Loss: 0.00001047
Iteration 176/1000 | Loss: 0.00001047
Iteration 177/1000 | Loss: 0.00001047
Iteration 178/1000 | Loss: 0.00001047
Iteration 179/1000 | Loss: 0.00001046
Iteration 180/1000 | Loss: 0.00001045
Iteration 181/1000 | Loss: 0.00001045
Iteration 182/1000 | Loss: 0.00001045
Iteration 183/1000 | Loss: 0.00001044
Iteration 184/1000 | Loss: 0.00001044
Iteration 185/1000 | Loss: 0.00001044
Iteration 186/1000 | Loss: 0.00001043
Iteration 187/1000 | Loss: 0.00001043
Iteration 188/1000 | Loss: 0.00001043
Iteration 189/1000 | Loss: 0.00001042
Iteration 190/1000 | Loss: 0.00001042
Iteration 191/1000 | Loss: 0.00001042
Iteration 192/1000 | Loss: 0.00001042
Iteration 193/1000 | Loss: 0.00001042
Iteration 194/1000 | Loss: 0.00001042
Iteration 195/1000 | Loss: 0.00001042
Iteration 196/1000 | Loss: 0.00001042
Iteration 197/1000 | Loss: 0.00001041
Iteration 198/1000 | Loss: 0.00001041
Iteration 199/1000 | Loss: 0.00001041
Iteration 200/1000 | Loss: 0.00001041
Iteration 201/1000 | Loss: 0.00001041
Iteration 202/1000 | Loss: 0.00001041
Iteration 203/1000 | Loss: 0.00001041
Iteration 204/1000 | Loss: 0.00001041
Iteration 205/1000 | Loss: 0.00001041
Iteration 206/1000 | Loss: 0.00001041
Iteration 207/1000 | Loss: 0.00001041
Iteration 208/1000 | Loss: 0.00001041
Iteration 209/1000 | Loss: 0.00001041
Iteration 210/1000 | Loss: 0.00001040
Iteration 211/1000 | Loss: 0.00001040
Iteration 212/1000 | Loss: 0.00001040
Iteration 213/1000 | Loss: 0.00001040
Iteration 214/1000 | Loss: 0.00001040
Iteration 215/1000 | Loss: 0.00001040
Iteration 216/1000 | Loss: 0.00001040
Iteration 217/1000 | Loss: 0.00001040
Iteration 218/1000 | Loss: 0.00001040
Iteration 219/1000 | Loss: 0.00001040
Iteration 220/1000 | Loss: 0.00001040
Iteration 221/1000 | Loss: 0.00001040
Iteration 222/1000 | Loss: 0.00001040
Iteration 223/1000 | Loss: 0.00001040
Iteration 224/1000 | Loss: 0.00001040
Iteration 225/1000 | Loss: 0.00001039
Iteration 226/1000 | Loss: 0.00001039
Iteration 227/1000 | Loss: 0.00001039
Iteration 228/1000 | Loss: 0.00001039
Iteration 229/1000 | Loss: 0.00001039
Iteration 230/1000 | Loss: 0.00001039
Iteration 231/1000 | Loss: 0.00001039
Iteration 232/1000 | Loss: 0.00001039
Iteration 233/1000 | Loss: 0.00001039
Iteration 234/1000 | Loss: 0.00001039
Iteration 235/1000 | Loss: 0.00001039
Iteration 236/1000 | Loss: 0.00001039
Iteration 237/1000 | Loss: 0.00001039
Iteration 238/1000 | Loss: 0.00001039
Iteration 239/1000 | Loss: 0.00001039
Iteration 240/1000 | Loss: 0.00001039
Iteration 241/1000 | Loss: 0.00001039
Iteration 242/1000 | Loss: 0.00001039
Iteration 243/1000 | Loss: 0.00001039
Iteration 244/1000 | Loss: 0.00001039
Iteration 245/1000 | Loss: 0.00001039
Iteration 246/1000 | Loss: 0.00001039
Iteration 247/1000 | Loss: 0.00001039
Iteration 248/1000 | Loss: 0.00001039
Iteration 249/1000 | Loss: 0.00001039
Iteration 250/1000 | Loss: 0.00001039
Iteration 251/1000 | Loss: 0.00001039
Iteration 252/1000 | Loss: 0.00001039
Iteration 253/1000 | Loss: 0.00001039
Iteration 254/1000 | Loss: 0.00001039
Iteration 255/1000 | Loss: 0.00001039
Iteration 256/1000 | Loss: 0.00001039
Iteration 257/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.0389432645752095e-05, 1.0389432645752095e-05, 1.0389432645752095e-05, 1.0389432645752095e-05, 1.0389432645752095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0389432645752095e-05

Optimization complete. Final v2v error: 2.720468282699585 mm

Highest mean error: 4.110103130340576 mm for frame 57

Lowest mean error: 2.4331650733947754 mm for frame 133

Saving results

Total time: 77.65807843208313
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533047
Iteration 2/25 | Loss: 0.00104589
Iteration 3/25 | Loss: 0.00098332
Iteration 4/25 | Loss: 0.00097483
Iteration 5/25 | Loss: 0.00097258
Iteration 6/25 | Loss: 0.00097221
Iteration 7/25 | Loss: 0.00097221
Iteration 8/25 | Loss: 0.00097221
Iteration 9/25 | Loss: 0.00097221
Iteration 10/25 | Loss: 0.00097221
Iteration 11/25 | Loss: 0.00097221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009722118848003447, 0.0009722118848003447, 0.0009722118848003447, 0.0009722118848003447, 0.0009722118848003447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009722118848003447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.40315723
Iteration 2/25 | Loss: 0.00101679
Iteration 3/25 | Loss: 0.00101677
Iteration 4/25 | Loss: 0.00101677
Iteration 5/25 | Loss: 0.00101677
Iteration 6/25 | Loss: 0.00101677
Iteration 7/25 | Loss: 0.00101677
Iteration 8/25 | Loss: 0.00101677
Iteration 9/25 | Loss: 0.00101677
Iteration 10/25 | Loss: 0.00101677
Iteration 11/25 | Loss: 0.00101677
Iteration 12/25 | Loss: 0.00101677
Iteration 13/25 | Loss: 0.00101677
Iteration 14/25 | Loss: 0.00101677
Iteration 15/25 | Loss: 0.00101677
Iteration 16/25 | Loss: 0.00101677
Iteration 17/25 | Loss: 0.00101677
Iteration 18/25 | Loss: 0.00101677
Iteration 19/25 | Loss: 0.00101677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010167666478082538, 0.0010167666478082538, 0.0010167666478082538, 0.0010167666478082538, 0.0010167666478082538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010167666478082538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101677
Iteration 2/1000 | Loss: 0.00002597
Iteration 3/1000 | Loss: 0.00001479
Iteration 4/1000 | Loss: 0.00001196
Iteration 5/1000 | Loss: 0.00001130
Iteration 6/1000 | Loss: 0.00001088
Iteration 7/1000 | Loss: 0.00001037
Iteration 8/1000 | Loss: 0.00001012
Iteration 9/1000 | Loss: 0.00000984
Iteration 10/1000 | Loss: 0.00000978
Iteration 11/1000 | Loss: 0.00000975
Iteration 12/1000 | Loss: 0.00000967
Iteration 13/1000 | Loss: 0.00000961
Iteration 14/1000 | Loss: 0.00000950
Iteration 15/1000 | Loss: 0.00000944
Iteration 16/1000 | Loss: 0.00000944
Iteration 17/1000 | Loss: 0.00000943
Iteration 18/1000 | Loss: 0.00000942
Iteration 19/1000 | Loss: 0.00000942
Iteration 20/1000 | Loss: 0.00000941
Iteration 21/1000 | Loss: 0.00000941
Iteration 22/1000 | Loss: 0.00000941
Iteration 23/1000 | Loss: 0.00000940
Iteration 24/1000 | Loss: 0.00000940
Iteration 25/1000 | Loss: 0.00000939
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000939
Iteration 28/1000 | Loss: 0.00000939
Iteration 29/1000 | Loss: 0.00000939
Iteration 30/1000 | Loss: 0.00000938
Iteration 31/1000 | Loss: 0.00000938
Iteration 32/1000 | Loss: 0.00000938
Iteration 33/1000 | Loss: 0.00000937
Iteration 34/1000 | Loss: 0.00000937
Iteration 35/1000 | Loss: 0.00000937
Iteration 36/1000 | Loss: 0.00000937
Iteration 37/1000 | Loss: 0.00000937
Iteration 38/1000 | Loss: 0.00000937
Iteration 39/1000 | Loss: 0.00000936
Iteration 40/1000 | Loss: 0.00000936
Iteration 41/1000 | Loss: 0.00000936
Iteration 42/1000 | Loss: 0.00000935
Iteration 43/1000 | Loss: 0.00000935
Iteration 44/1000 | Loss: 0.00000934
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000933
Iteration 47/1000 | Loss: 0.00000933
Iteration 48/1000 | Loss: 0.00000933
Iteration 49/1000 | Loss: 0.00000933
Iteration 50/1000 | Loss: 0.00000933
Iteration 51/1000 | Loss: 0.00000932
Iteration 52/1000 | Loss: 0.00000931
Iteration 53/1000 | Loss: 0.00000931
Iteration 54/1000 | Loss: 0.00000930
Iteration 55/1000 | Loss: 0.00000930
Iteration 56/1000 | Loss: 0.00000930
Iteration 57/1000 | Loss: 0.00000929
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000928
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000928
Iteration 63/1000 | Loss: 0.00000928
Iteration 64/1000 | Loss: 0.00000928
Iteration 65/1000 | Loss: 0.00000927
Iteration 66/1000 | Loss: 0.00000927
Iteration 67/1000 | Loss: 0.00000927
Iteration 68/1000 | Loss: 0.00000927
Iteration 69/1000 | Loss: 0.00000927
Iteration 70/1000 | Loss: 0.00000926
Iteration 71/1000 | Loss: 0.00000926
Iteration 72/1000 | Loss: 0.00000926
Iteration 73/1000 | Loss: 0.00000926
Iteration 74/1000 | Loss: 0.00000926
Iteration 75/1000 | Loss: 0.00000926
Iteration 76/1000 | Loss: 0.00000926
Iteration 77/1000 | Loss: 0.00000926
Iteration 78/1000 | Loss: 0.00000925
Iteration 79/1000 | Loss: 0.00000925
Iteration 80/1000 | Loss: 0.00000925
Iteration 81/1000 | Loss: 0.00000925
Iteration 82/1000 | Loss: 0.00000925
Iteration 83/1000 | Loss: 0.00000925
Iteration 84/1000 | Loss: 0.00000925
Iteration 85/1000 | Loss: 0.00000924
Iteration 86/1000 | Loss: 0.00000924
Iteration 87/1000 | Loss: 0.00000924
Iteration 88/1000 | Loss: 0.00000924
Iteration 89/1000 | Loss: 0.00000923
Iteration 90/1000 | Loss: 0.00000923
Iteration 91/1000 | Loss: 0.00000923
Iteration 92/1000 | Loss: 0.00000923
Iteration 93/1000 | Loss: 0.00000923
Iteration 94/1000 | Loss: 0.00000923
Iteration 95/1000 | Loss: 0.00000922
Iteration 96/1000 | Loss: 0.00000922
Iteration 97/1000 | Loss: 0.00000922
Iteration 98/1000 | Loss: 0.00000922
Iteration 99/1000 | Loss: 0.00000921
Iteration 100/1000 | Loss: 0.00000921
Iteration 101/1000 | Loss: 0.00000921
Iteration 102/1000 | Loss: 0.00000920
Iteration 103/1000 | Loss: 0.00000920
Iteration 104/1000 | Loss: 0.00000920
Iteration 105/1000 | Loss: 0.00000920
Iteration 106/1000 | Loss: 0.00000919
Iteration 107/1000 | Loss: 0.00000919
Iteration 108/1000 | Loss: 0.00000919
Iteration 109/1000 | Loss: 0.00000919
Iteration 110/1000 | Loss: 0.00000918
Iteration 111/1000 | Loss: 0.00000918
Iteration 112/1000 | Loss: 0.00000918
Iteration 113/1000 | Loss: 0.00000918
Iteration 114/1000 | Loss: 0.00000917
Iteration 115/1000 | Loss: 0.00000917
Iteration 116/1000 | Loss: 0.00000917
Iteration 117/1000 | Loss: 0.00000916
Iteration 118/1000 | Loss: 0.00000916
Iteration 119/1000 | Loss: 0.00000916
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000916
Iteration 122/1000 | Loss: 0.00000916
Iteration 123/1000 | Loss: 0.00000916
Iteration 124/1000 | Loss: 0.00000915
Iteration 125/1000 | Loss: 0.00000915
Iteration 126/1000 | Loss: 0.00000914
Iteration 127/1000 | Loss: 0.00000914
Iteration 128/1000 | Loss: 0.00000914
Iteration 129/1000 | Loss: 0.00000914
Iteration 130/1000 | Loss: 0.00000914
Iteration 131/1000 | Loss: 0.00000914
Iteration 132/1000 | Loss: 0.00000913
Iteration 133/1000 | Loss: 0.00000913
Iteration 134/1000 | Loss: 0.00000913
Iteration 135/1000 | Loss: 0.00000913
Iteration 136/1000 | Loss: 0.00000913
Iteration 137/1000 | Loss: 0.00000913
Iteration 138/1000 | Loss: 0.00000913
Iteration 139/1000 | Loss: 0.00000913
Iteration 140/1000 | Loss: 0.00000913
Iteration 141/1000 | Loss: 0.00000913
Iteration 142/1000 | Loss: 0.00000913
Iteration 143/1000 | Loss: 0.00000913
Iteration 144/1000 | Loss: 0.00000912
Iteration 145/1000 | Loss: 0.00000912
Iteration 146/1000 | Loss: 0.00000912
Iteration 147/1000 | Loss: 0.00000912
Iteration 148/1000 | Loss: 0.00000912
Iteration 149/1000 | Loss: 0.00000912
Iteration 150/1000 | Loss: 0.00000912
Iteration 151/1000 | Loss: 0.00000912
Iteration 152/1000 | Loss: 0.00000912
Iteration 153/1000 | Loss: 0.00000912
Iteration 154/1000 | Loss: 0.00000912
Iteration 155/1000 | Loss: 0.00000911
Iteration 156/1000 | Loss: 0.00000911
Iteration 157/1000 | Loss: 0.00000911
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000911
Iteration 164/1000 | Loss: 0.00000910
Iteration 165/1000 | Loss: 0.00000910
Iteration 166/1000 | Loss: 0.00000910
Iteration 167/1000 | Loss: 0.00000910
Iteration 168/1000 | Loss: 0.00000910
Iteration 169/1000 | Loss: 0.00000910
Iteration 170/1000 | Loss: 0.00000910
Iteration 171/1000 | Loss: 0.00000910
Iteration 172/1000 | Loss: 0.00000910
Iteration 173/1000 | Loss: 0.00000910
Iteration 174/1000 | Loss: 0.00000910
Iteration 175/1000 | Loss: 0.00000910
Iteration 176/1000 | Loss: 0.00000910
Iteration 177/1000 | Loss: 0.00000909
Iteration 178/1000 | Loss: 0.00000909
Iteration 179/1000 | Loss: 0.00000909
Iteration 180/1000 | Loss: 0.00000909
Iteration 181/1000 | Loss: 0.00000909
Iteration 182/1000 | Loss: 0.00000909
Iteration 183/1000 | Loss: 0.00000909
Iteration 184/1000 | Loss: 0.00000909
Iteration 185/1000 | Loss: 0.00000909
Iteration 186/1000 | Loss: 0.00000909
Iteration 187/1000 | Loss: 0.00000909
Iteration 188/1000 | Loss: 0.00000909
Iteration 189/1000 | Loss: 0.00000909
Iteration 190/1000 | Loss: 0.00000909
Iteration 191/1000 | Loss: 0.00000909
Iteration 192/1000 | Loss: 0.00000909
Iteration 193/1000 | Loss: 0.00000909
Iteration 194/1000 | Loss: 0.00000909
Iteration 195/1000 | Loss: 0.00000909
Iteration 196/1000 | Loss: 0.00000909
Iteration 197/1000 | Loss: 0.00000909
Iteration 198/1000 | Loss: 0.00000909
Iteration 199/1000 | Loss: 0.00000909
Iteration 200/1000 | Loss: 0.00000909
Iteration 201/1000 | Loss: 0.00000909
Iteration 202/1000 | Loss: 0.00000909
Iteration 203/1000 | Loss: 0.00000909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [9.092565051105339e-06, 9.092565051105339e-06, 9.092565051105339e-06, 9.092565051105339e-06, 9.092565051105339e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.092565051105339e-06

Optimization complete. Final v2v error: 2.5773377418518066 mm

Highest mean error: 3.42057204246521 mm for frame 61

Lowest mean error: 2.278230667114258 mm for frame 163

Saving results

Total time: 38.39995503425598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00355428
Iteration 2/25 | Loss: 0.00101890
Iteration 3/25 | Loss: 0.00093775
Iteration 4/25 | Loss: 0.00092940
Iteration 5/25 | Loss: 0.00092762
Iteration 6/25 | Loss: 0.00092721
Iteration 7/25 | Loss: 0.00092721
Iteration 8/25 | Loss: 0.00092721
Iteration 9/25 | Loss: 0.00092721
Iteration 10/25 | Loss: 0.00092721
Iteration 11/25 | Loss: 0.00092721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009272108436562121, 0.0009272108436562121, 0.0009272108436562121, 0.0009272108436562121, 0.0009272108436562121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009272108436562121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76173460
Iteration 2/25 | Loss: 0.00104450
Iteration 3/25 | Loss: 0.00104449
Iteration 4/25 | Loss: 0.00104449
Iteration 5/25 | Loss: 0.00104449
Iteration 6/25 | Loss: 0.00104449
Iteration 7/25 | Loss: 0.00104449
Iteration 8/25 | Loss: 0.00104449
Iteration 9/25 | Loss: 0.00104449
Iteration 10/25 | Loss: 0.00104449
Iteration 11/25 | Loss: 0.00104449
Iteration 12/25 | Loss: 0.00104449
Iteration 13/25 | Loss: 0.00104449
Iteration 14/25 | Loss: 0.00104449
Iteration 15/25 | Loss: 0.00104449
Iteration 16/25 | Loss: 0.00104449
Iteration 17/25 | Loss: 0.00104449
Iteration 18/25 | Loss: 0.00104449
Iteration 19/25 | Loss: 0.00104449
Iteration 20/25 | Loss: 0.00104449
Iteration 21/25 | Loss: 0.00104449
Iteration 22/25 | Loss: 0.00104449
Iteration 23/25 | Loss: 0.00104449
Iteration 24/25 | Loss: 0.00104449
Iteration 25/25 | Loss: 0.00104449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010444873478263617, 0.0010444873478263617, 0.0010444873478263617, 0.0010444873478263617, 0.0010444873478263617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010444873478263617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104449
Iteration 2/1000 | Loss: 0.00001754
Iteration 3/1000 | Loss: 0.00001066
Iteration 4/1000 | Loss: 0.00000871
Iteration 5/1000 | Loss: 0.00000806
Iteration 6/1000 | Loss: 0.00000774
Iteration 7/1000 | Loss: 0.00000735
Iteration 8/1000 | Loss: 0.00000712
Iteration 9/1000 | Loss: 0.00000711
Iteration 10/1000 | Loss: 0.00000710
Iteration 11/1000 | Loss: 0.00000709
Iteration 12/1000 | Loss: 0.00000694
Iteration 13/1000 | Loss: 0.00000690
Iteration 14/1000 | Loss: 0.00000688
Iteration 15/1000 | Loss: 0.00000687
Iteration 16/1000 | Loss: 0.00000687
Iteration 17/1000 | Loss: 0.00000686
Iteration 18/1000 | Loss: 0.00000684
Iteration 19/1000 | Loss: 0.00000683
Iteration 20/1000 | Loss: 0.00000681
Iteration 21/1000 | Loss: 0.00000681
Iteration 22/1000 | Loss: 0.00000681
Iteration 23/1000 | Loss: 0.00000681
Iteration 24/1000 | Loss: 0.00000681
Iteration 25/1000 | Loss: 0.00000681
Iteration 26/1000 | Loss: 0.00000681
Iteration 27/1000 | Loss: 0.00000681
Iteration 28/1000 | Loss: 0.00000681
Iteration 29/1000 | Loss: 0.00000680
Iteration 30/1000 | Loss: 0.00000679
Iteration 31/1000 | Loss: 0.00000677
Iteration 32/1000 | Loss: 0.00000677
Iteration 33/1000 | Loss: 0.00000676
Iteration 34/1000 | Loss: 0.00000676
Iteration 35/1000 | Loss: 0.00000676
Iteration 36/1000 | Loss: 0.00000675
Iteration 37/1000 | Loss: 0.00000675
Iteration 38/1000 | Loss: 0.00000675
Iteration 39/1000 | Loss: 0.00000674
Iteration 40/1000 | Loss: 0.00000673
Iteration 41/1000 | Loss: 0.00000672
Iteration 42/1000 | Loss: 0.00000672
Iteration 43/1000 | Loss: 0.00000672
Iteration 44/1000 | Loss: 0.00000672
Iteration 45/1000 | Loss: 0.00000671
Iteration 46/1000 | Loss: 0.00000671
Iteration 47/1000 | Loss: 0.00000671
Iteration 48/1000 | Loss: 0.00000671
Iteration 49/1000 | Loss: 0.00000670
Iteration 50/1000 | Loss: 0.00000670
Iteration 51/1000 | Loss: 0.00000670
Iteration 52/1000 | Loss: 0.00000670
Iteration 53/1000 | Loss: 0.00000670
Iteration 54/1000 | Loss: 0.00000669
Iteration 55/1000 | Loss: 0.00000669
Iteration 56/1000 | Loss: 0.00000668
Iteration 57/1000 | Loss: 0.00000668
Iteration 58/1000 | Loss: 0.00000668
Iteration 59/1000 | Loss: 0.00000668
Iteration 60/1000 | Loss: 0.00000667
Iteration 61/1000 | Loss: 0.00000667
Iteration 62/1000 | Loss: 0.00000667
Iteration 63/1000 | Loss: 0.00000666
Iteration 64/1000 | Loss: 0.00000666
Iteration 65/1000 | Loss: 0.00000666
Iteration 66/1000 | Loss: 0.00000666
Iteration 67/1000 | Loss: 0.00000666
Iteration 68/1000 | Loss: 0.00000666
Iteration 69/1000 | Loss: 0.00000666
Iteration 70/1000 | Loss: 0.00000666
Iteration 71/1000 | Loss: 0.00000665
Iteration 72/1000 | Loss: 0.00000665
Iteration 73/1000 | Loss: 0.00000665
Iteration 74/1000 | Loss: 0.00000664
Iteration 75/1000 | Loss: 0.00000664
Iteration 76/1000 | Loss: 0.00000664
Iteration 77/1000 | Loss: 0.00000664
Iteration 78/1000 | Loss: 0.00000663
Iteration 79/1000 | Loss: 0.00000663
Iteration 80/1000 | Loss: 0.00000662
Iteration 81/1000 | Loss: 0.00000662
Iteration 82/1000 | Loss: 0.00000662
Iteration 83/1000 | Loss: 0.00000662
Iteration 84/1000 | Loss: 0.00000662
Iteration 85/1000 | Loss: 0.00000662
Iteration 86/1000 | Loss: 0.00000661
Iteration 87/1000 | Loss: 0.00000661
Iteration 88/1000 | Loss: 0.00000660
Iteration 89/1000 | Loss: 0.00000660
Iteration 90/1000 | Loss: 0.00000660
Iteration 91/1000 | Loss: 0.00000660
Iteration 92/1000 | Loss: 0.00000660
Iteration 93/1000 | Loss: 0.00000659
Iteration 94/1000 | Loss: 0.00000659
Iteration 95/1000 | Loss: 0.00000659
Iteration 96/1000 | Loss: 0.00000659
Iteration 97/1000 | Loss: 0.00000659
Iteration 98/1000 | Loss: 0.00000659
Iteration 99/1000 | Loss: 0.00000659
Iteration 100/1000 | Loss: 0.00000659
Iteration 101/1000 | Loss: 0.00000659
Iteration 102/1000 | Loss: 0.00000658
Iteration 103/1000 | Loss: 0.00000658
Iteration 104/1000 | Loss: 0.00000658
Iteration 105/1000 | Loss: 0.00000658
Iteration 106/1000 | Loss: 0.00000658
Iteration 107/1000 | Loss: 0.00000658
Iteration 108/1000 | Loss: 0.00000658
Iteration 109/1000 | Loss: 0.00000658
Iteration 110/1000 | Loss: 0.00000658
Iteration 111/1000 | Loss: 0.00000657
Iteration 112/1000 | Loss: 0.00000657
Iteration 113/1000 | Loss: 0.00000657
Iteration 114/1000 | Loss: 0.00000657
Iteration 115/1000 | Loss: 0.00000657
Iteration 116/1000 | Loss: 0.00000657
Iteration 117/1000 | Loss: 0.00000657
Iteration 118/1000 | Loss: 0.00000657
Iteration 119/1000 | Loss: 0.00000657
Iteration 120/1000 | Loss: 0.00000657
Iteration 121/1000 | Loss: 0.00000657
Iteration 122/1000 | Loss: 0.00000657
Iteration 123/1000 | Loss: 0.00000657
Iteration 124/1000 | Loss: 0.00000656
Iteration 125/1000 | Loss: 0.00000656
Iteration 126/1000 | Loss: 0.00000656
Iteration 127/1000 | Loss: 0.00000656
Iteration 128/1000 | Loss: 0.00000656
Iteration 129/1000 | Loss: 0.00000656
Iteration 130/1000 | Loss: 0.00000656
Iteration 131/1000 | Loss: 0.00000656
Iteration 132/1000 | Loss: 0.00000655
Iteration 133/1000 | Loss: 0.00000655
Iteration 134/1000 | Loss: 0.00000655
Iteration 135/1000 | Loss: 0.00000655
Iteration 136/1000 | Loss: 0.00000655
Iteration 137/1000 | Loss: 0.00000655
Iteration 138/1000 | Loss: 0.00000655
Iteration 139/1000 | Loss: 0.00000655
Iteration 140/1000 | Loss: 0.00000655
Iteration 141/1000 | Loss: 0.00000655
Iteration 142/1000 | Loss: 0.00000655
Iteration 143/1000 | Loss: 0.00000655
Iteration 144/1000 | Loss: 0.00000655
Iteration 145/1000 | Loss: 0.00000655
Iteration 146/1000 | Loss: 0.00000655
Iteration 147/1000 | Loss: 0.00000655
Iteration 148/1000 | Loss: 0.00000655
Iteration 149/1000 | Loss: 0.00000655
Iteration 150/1000 | Loss: 0.00000655
Iteration 151/1000 | Loss: 0.00000655
Iteration 152/1000 | Loss: 0.00000655
Iteration 153/1000 | Loss: 0.00000655
Iteration 154/1000 | Loss: 0.00000655
Iteration 155/1000 | Loss: 0.00000655
Iteration 156/1000 | Loss: 0.00000655
Iteration 157/1000 | Loss: 0.00000655
Iteration 158/1000 | Loss: 0.00000655
Iteration 159/1000 | Loss: 0.00000655
Iteration 160/1000 | Loss: 0.00000655
Iteration 161/1000 | Loss: 0.00000655
Iteration 162/1000 | Loss: 0.00000655
Iteration 163/1000 | Loss: 0.00000655
Iteration 164/1000 | Loss: 0.00000655
Iteration 165/1000 | Loss: 0.00000655
Iteration 166/1000 | Loss: 0.00000655
Iteration 167/1000 | Loss: 0.00000655
Iteration 168/1000 | Loss: 0.00000655
Iteration 169/1000 | Loss: 0.00000655
Iteration 170/1000 | Loss: 0.00000655
Iteration 171/1000 | Loss: 0.00000655
Iteration 172/1000 | Loss: 0.00000655
Iteration 173/1000 | Loss: 0.00000655
Iteration 174/1000 | Loss: 0.00000655
Iteration 175/1000 | Loss: 0.00000655
Iteration 176/1000 | Loss: 0.00000655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [6.548400051542558e-06, 6.548400051542558e-06, 6.548400051542558e-06, 6.548400051542558e-06, 6.548400051542558e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.548400051542558e-06

Optimization complete. Final v2v error: 2.1971659660339355 mm

Highest mean error: 2.7734525203704834 mm for frame 77

Lowest mean error: 1.9635932445526123 mm for frame 15

Saving results

Total time: 32.62651515007019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481583
Iteration 2/25 | Loss: 0.00115126
Iteration 3/25 | Loss: 0.00103726
Iteration 4/25 | Loss: 0.00101883
Iteration 5/25 | Loss: 0.00101484
Iteration 6/25 | Loss: 0.00101399
Iteration 7/25 | Loss: 0.00101388
Iteration 8/25 | Loss: 0.00101388
Iteration 9/25 | Loss: 0.00101388
Iteration 10/25 | Loss: 0.00101388
Iteration 11/25 | Loss: 0.00101388
Iteration 12/25 | Loss: 0.00101388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001013880711980164, 0.001013880711980164, 0.001013880711980164, 0.001013880711980164, 0.001013880711980164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001013880711980164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38886118
Iteration 2/25 | Loss: 0.00097122
Iteration 3/25 | Loss: 0.00097122
Iteration 4/25 | Loss: 0.00097122
Iteration 5/25 | Loss: 0.00097122
Iteration 6/25 | Loss: 0.00097122
Iteration 7/25 | Loss: 0.00097122
Iteration 8/25 | Loss: 0.00097122
Iteration 9/25 | Loss: 0.00097122
Iteration 10/25 | Loss: 0.00097122
Iteration 11/25 | Loss: 0.00097122
Iteration 12/25 | Loss: 0.00097122
Iteration 13/25 | Loss: 0.00097122
Iteration 14/25 | Loss: 0.00097122
Iteration 15/25 | Loss: 0.00097122
Iteration 16/25 | Loss: 0.00097122
Iteration 17/25 | Loss: 0.00097122
Iteration 18/25 | Loss: 0.00097122
Iteration 19/25 | Loss: 0.00097122
Iteration 20/25 | Loss: 0.00097122
Iteration 21/25 | Loss: 0.00097122
Iteration 22/25 | Loss: 0.00097122
Iteration 23/25 | Loss: 0.00097122
Iteration 24/25 | Loss: 0.00097122
Iteration 25/25 | Loss: 0.00097122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097122
Iteration 2/1000 | Loss: 0.00003637
Iteration 3/1000 | Loss: 0.00002571
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002024
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001848
Iteration 9/1000 | Loss: 0.00001813
Iteration 10/1000 | Loss: 0.00001801
Iteration 11/1000 | Loss: 0.00001785
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001769
Iteration 14/1000 | Loss: 0.00001767
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001763
Iteration 17/1000 | Loss: 0.00001754
Iteration 18/1000 | Loss: 0.00001749
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001738
Iteration 21/1000 | Loss: 0.00001738
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001729
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001715
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001707
Iteration 86/1000 | Loss: 0.00001707
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001706
Iteration 90/1000 | Loss: 0.00001706
Iteration 91/1000 | Loss: 0.00001706
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001705
Iteration 95/1000 | Loss: 0.00001705
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001704
Iteration 98/1000 | Loss: 0.00001704
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001701
Iteration 109/1000 | Loss: 0.00001701
Iteration 110/1000 | Loss: 0.00001701
Iteration 111/1000 | Loss: 0.00001701
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001700
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001699
Iteration 120/1000 | Loss: 0.00001699
Iteration 121/1000 | Loss: 0.00001699
Iteration 122/1000 | Loss: 0.00001699
Iteration 123/1000 | Loss: 0.00001699
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00001698
Iteration 126/1000 | Loss: 0.00001698
Iteration 127/1000 | Loss: 0.00001698
Iteration 128/1000 | Loss: 0.00001698
Iteration 129/1000 | Loss: 0.00001698
Iteration 130/1000 | Loss: 0.00001698
Iteration 131/1000 | Loss: 0.00001697
Iteration 132/1000 | Loss: 0.00001697
Iteration 133/1000 | Loss: 0.00001697
Iteration 134/1000 | Loss: 0.00001697
Iteration 135/1000 | Loss: 0.00001697
Iteration 136/1000 | Loss: 0.00001697
Iteration 137/1000 | Loss: 0.00001697
Iteration 138/1000 | Loss: 0.00001697
Iteration 139/1000 | Loss: 0.00001697
Iteration 140/1000 | Loss: 0.00001696
Iteration 141/1000 | Loss: 0.00001696
Iteration 142/1000 | Loss: 0.00001696
Iteration 143/1000 | Loss: 0.00001696
Iteration 144/1000 | Loss: 0.00001696
Iteration 145/1000 | Loss: 0.00001696
Iteration 146/1000 | Loss: 0.00001695
Iteration 147/1000 | Loss: 0.00001695
Iteration 148/1000 | Loss: 0.00001695
Iteration 149/1000 | Loss: 0.00001695
Iteration 150/1000 | Loss: 0.00001695
Iteration 151/1000 | Loss: 0.00001695
Iteration 152/1000 | Loss: 0.00001695
Iteration 153/1000 | Loss: 0.00001695
Iteration 154/1000 | Loss: 0.00001695
Iteration 155/1000 | Loss: 0.00001694
Iteration 156/1000 | Loss: 0.00001694
Iteration 157/1000 | Loss: 0.00001694
Iteration 158/1000 | Loss: 0.00001694
Iteration 159/1000 | Loss: 0.00001694
Iteration 160/1000 | Loss: 0.00001694
Iteration 161/1000 | Loss: 0.00001694
Iteration 162/1000 | Loss: 0.00001693
Iteration 163/1000 | Loss: 0.00001693
Iteration 164/1000 | Loss: 0.00001693
Iteration 165/1000 | Loss: 0.00001693
Iteration 166/1000 | Loss: 0.00001693
Iteration 167/1000 | Loss: 0.00001693
Iteration 168/1000 | Loss: 0.00001693
Iteration 169/1000 | Loss: 0.00001692
Iteration 170/1000 | Loss: 0.00001692
Iteration 171/1000 | Loss: 0.00001692
Iteration 172/1000 | Loss: 0.00001692
Iteration 173/1000 | Loss: 0.00001692
Iteration 174/1000 | Loss: 0.00001692
Iteration 175/1000 | Loss: 0.00001692
Iteration 176/1000 | Loss: 0.00001691
Iteration 177/1000 | Loss: 0.00001691
Iteration 178/1000 | Loss: 0.00001691
Iteration 179/1000 | Loss: 0.00001691
Iteration 180/1000 | Loss: 0.00001691
Iteration 181/1000 | Loss: 0.00001691
Iteration 182/1000 | Loss: 0.00001691
Iteration 183/1000 | Loss: 0.00001691
Iteration 184/1000 | Loss: 0.00001691
Iteration 185/1000 | Loss: 0.00001690
Iteration 186/1000 | Loss: 0.00001690
Iteration 187/1000 | Loss: 0.00001690
Iteration 188/1000 | Loss: 0.00001690
Iteration 189/1000 | Loss: 0.00001690
Iteration 190/1000 | Loss: 0.00001690
Iteration 191/1000 | Loss: 0.00001690
Iteration 192/1000 | Loss: 0.00001690
Iteration 193/1000 | Loss: 0.00001689
Iteration 194/1000 | Loss: 0.00001689
Iteration 195/1000 | Loss: 0.00001689
Iteration 196/1000 | Loss: 0.00001689
Iteration 197/1000 | Loss: 0.00001689
Iteration 198/1000 | Loss: 0.00001689
Iteration 199/1000 | Loss: 0.00001689
Iteration 200/1000 | Loss: 0.00001689
Iteration 201/1000 | Loss: 0.00001689
Iteration 202/1000 | Loss: 0.00001689
Iteration 203/1000 | Loss: 0.00001689
Iteration 204/1000 | Loss: 0.00001689
Iteration 205/1000 | Loss: 0.00001689
Iteration 206/1000 | Loss: 0.00001689
Iteration 207/1000 | Loss: 0.00001689
Iteration 208/1000 | Loss: 0.00001689
Iteration 209/1000 | Loss: 0.00001689
Iteration 210/1000 | Loss: 0.00001689
Iteration 211/1000 | Loss: 0.00001688
Iteration 212/1000 | Loss: 0.00001688
Iteration 213/1000 | Loss: 0.00001688
Iteration 214/1000 | Loss: 0.00001688
Iteration 215/1000 | Loss: 0.00001688
Iteration 216/1000 | Loss: 0.00001688
Iteration 217/1000 | Loss: 0.00001688
Iteration 218/1000 | Loss: 0.00001688
Iteration 219/1000 | Loss: 0.00001688
Iteration 220/1000 | Loss: 0.00001688
Iteration 221/1000 | Loss: 0.00001688
Iteration 222/1000 | Loss: 0.00001688
Iteration 223/1000 | Loss: 0.00001688
Iteration 224/1000 | Loss: 0.00001688
Iteration 225/1000 | Loss: 0.00001688
Iteration 226/1000 | Loss: 0.00001688
Iteration 227/1000 | Loss: 0.00001688
Iteration 228/1000 | Loss: 0.00001688
Iteration 229/1000 | Loss: 0.00001688
Iteration 230/1000 | Loss: 0.00001688
Iteration 231/1000 | Loss: 0.00001688
Iteration 232/1000 | Loss: 0.00001688
Iteration 233/1000 | Loss: 0.00001688
Iteration 234/1000 | Loss: 0.00001688
Iteration 235/1000 | Loss: 0.00001688
Iteration 236/1000 | Loss: 0.00001688
Iteration 237/1000 | Loss: 0.00001688
Iteration 238/1000 | Loss: 0.00001688
Iteration 239/1000 | Loss: 0.00001688
Iteration 240/1000 | Loss: 0.00001688
Iteration 241/1000 | Loss: 0.00001688
Iteration 242/1000 | Loss: 0.00001688
Iteration 243/1000 | Loss: 0.00001688
Iteration 244/1000 | Loss: 0.00001688
Iteration 245/1000 | Loss: 0.00001688
Iteration 246/1000 | Loss: 0.00001688
Iteration 247/1000 | Loss: 0.00001688
Iteration 248/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.6876807421795093e-05, 1.6876807421795093e-05, 1.6876807421795093e-05, 1.6876807421795093e-05, 1.6876807421795093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6876807421795093e-05

Optimization complete. Final v2v error: 3.276826858520508 mm

Highest mean error: 5.268572807312012 mm for frame 39

Lowest mean error: 2.5071706771850586 mm for frame 21

Saving results

Total time: 45.54540967941284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802614
Iteration 2/25 | Loss: 0.00138044
Iteration 3/25 | Loss: 0.00114048
Iteration 4/25 | Loss: 0.00112624
Iteration 5/25 | Loss: 0.00112281
Iteration 6/25 | Loss: 0.00112281
Iteration 7/25 | Loss: 0.00112281
Iteration 8/25 | Loss: 0.00112281
Iteration 9/25 | Loss: 0.00112281
Iteration 10/25 | Loss: 0.00112281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011228052899241447, 0.0011228052899241447, 0.0011228052899241447, 0.0011228052899241447, 0.0011228052899241447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011228052899241447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23562896
Iteration 2/25 | Loss: 0.00072407
Iteration 3/25 | Loss: 0.00072407
Iteration 4/25 | Loss: 0.00072407
Iteration 5/25 | Loss: 0.00072406
Iteration 6/25 | Loss: 0.00072406
Iteration 7/25 | Loss: 0.00072406
Iteration 8/25 | Loss: 0.00072406
Iteration 9/25 | Loss: 0.00072406
Iteration 10/25 | Loss: 0.00072406
Iteration 11/25 | Loss: 0.00072406
Iteration 12/25 | Loss: 0.00072406
Iteration 13/25 | Loss: 0.00072406
Iteration 14/25 | Loss: 0.00072406
Iteration 15/25 | Loss: 0.00072406
Iteration 16/25 | Loss: 0.00072406
Iteration 17/25 | Loss: 0.00072406
Iteration 18/25 | Loss: 0.00072406
Iteration 19/25 | Loss: 0.00072406
Iteration 20/25 | Loss: 0.00072406
Iteration 21/25 | Loss: 0.00072406
Iteration 22/25 | Loss: 0.00072406
Iteration 23/25 | Loss: 0.00072406
Iteration 24/25 | Loss: 0.00072406
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007240635459311306, 0.0007240635459311306, 0.0007240635459311306, 0.0007240635459311306, 0.0007240635459311306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007240635459311306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072406
Iteration 2/1000 | Loss: 0.00003014
Iteration 3/1000 | Loss: 0.00002073
Iteration 4/1000 | Loss: 0.00001818
Iteration 5/1000 | Loss: 0.00001710
Iteration 6/1000 | Loss: 0.00001664
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001567
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001546
Iteration 11/1000 | Loss: 0.00001531
Iteration 12/1000 | Loss: 0.00001517
Iteration 13/1000 | Loss: 0.00001507
Iteration 14/1000 | Loss: 0.00001502
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001501
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001501
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001501
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001501
Iteration 28/1000 | Loss: 0.00001501
Iteration 29/1000 | Loss: 0.00001501
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001500
Iteration 32/1000 | Loss: 0.00001499
Iteration 33/1000 | Loss: 0.00001499
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001498
Iteration 36/1000 | Loss: 0.00001498
Iteration 37/1000 | Loss: 0.00001498
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001498
Iteration 40/1000 | Loss: 0.00001497
Iteration 41/1000 | Loss: 0.00001497
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001497
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001495
Iteration 49/1000 | Loss: 0.00001495
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001495
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001494
Iteration 61/1000 | Loss: 0.00001494
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001494
Iteration 64/1000 | Loss: 0.00001494
Iteration 65/1000 | Loss: 0.00001494
Iteration 66/1000 | Loss: 0.00001494
Iteration 67/1000 | Loss: 0.00001494
Iteration 68/1000 | Loss: 0.00001494
Iteration 69/1000 | Loss: 0.00001494
Iteration 70/1000 | Loss: 0.00001494
Iteration 71/1000 | Loss: 0.00001494
Iteration 72/1000 | Loss: 0.00001494
Iteration 73/1000 | Loss: 0.00001494
Iteration 74/1000 | Loss: 0.00001494
Iteration 75/1000 | Loss: 0.00001494
Iteration 76/1000 | Loss: 0.00001494
Iteration 77/1000 | Loss: 0.00001494
Iteration 78/1000 | Loss: 0.00001494
Iteration 79/1000 | Loss: 0.00001494
Iteration 80/1000 | Loss: 0.00001494
Iteration 81/1000 | Loss: 0.00001494
Iteration 82/1000 | Loss: 0.00001494
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001494
Iteration 85/1000 | Loss: 0.00001494
Iteration 86/1000 | Loss: 0.00001494
Iteration 87/1000 | Loss: 0.00001494
Iteration 88/1000 | Loss: 0.00001494
Iteration 89/1000 | Loss: 0.00001494
Iteration 90/1000 | Loss: 0.00001494
Iteration 91/1000 | Loss: 0.00001494
Iteration 92/1000 | Loss: 0.00001494
Iteration 93/1000 | Loss: 0.00001494
Iteration 94/1000 | Loss: 0.00001494
Iteration 95/1000 | Loss: 0.00001494
Iteration 96/1000 | Loss: 0.00001494
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001494
Iteration 101/1000 | Loss: 0.00001494
Iteration 102/1000 | Loss: 0.00001494
Iteration 103/1000 | Loss: 0.00001494
Iteration 104/1000 | Loss: 0.00001494
Iteration 105/1000 | Loss: 0.00001494
Iteration 106/1000 | Loss: 0.00001494
Iteration 107/1000 | Loss: 0.00001494
Iteration 108/1000 | Loss: 0.00001494
Iteration 109/1000 | Loss: 0.00001494
Iteration 110/1000 | Loss: 0.00001494
Iteration 111/1000 | Loss: 0.00001494
Iteration 112/1000 | Loss: 0.00001494
Iteration 113/1000 | Loss: 0.00001494
Iteration 114/1000 | Loss: 0.00001494
Iteration 115/1000 | Loss: 0.00001494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.4940302207833156e-05, 1.4940302207833156e-05, 1.4940302207833156e-05, 1.4940302207833156e-05, 1.4940302207833156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4940302207833156e-05

Optimization complete. Final v2v error: 3.2713112831115723 mm

Highest mean error: 3.6751644611358643 mm for frame 10

Lowest mean error: 2.98508620262146 mm for frame 217

Saving results

Total time: 33.681039810180664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998499
Iteration 2/25 | Loss: 0.00167606
Iteration 3/25 | Loss: 0.00127342
Iteration 4/25 | Loss: 0.00120172
Iteration 5/25 | Loss: 0.00119512
Iteration 6/25 | Loss: 0.00117402
Iteration 7/25 | Loss: 0.00114020
Iteration 8/25 | Loss: 0.00109667
Iteration 9/25 | Loss: 0.00107517
Iteration 10/25 | Loss: 0.00105863
Iteration 11/25 | Loss: 0.00105311
Iteration 12/25 | Loss: 0.00105737
Iteration 13/25 | Loss: 0.00104886
Iteration 14/25 | Loss: 0.00104554
Iteration 15/25 | Loss: 0.00104330
Iteration 16/25 | Loss: 0.00103971
Iteration 17/25 | Loss: 0.00104025
Iteration 18/25 | Loss: 0.00104034
Iteration 19/25 | Loss: 0.00104058
Iteration 20/25 | Loss: 0.00104029
Iteration 21/25 | Loss: 0.00104176
Iteration 22/25 | Loss: 0.00103870
Iteration 23/25 | Loss: 0.00104061
Iteration 24/25 | Loss: 0.00103956
Iteration 25/25 | Loss: 0.00103797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35233068
Iteration 2/25 | Loss: 0.00120558
Iteration 3/25 | Loss: 0.00120558
Iteration 4/25 | Loss: 0.00120558
Iteration 5/25 | Loss: 0.00120558
Iteration 6/25 | Loss: 0.00120558
Iteration 7/25 | Loss: 0.00120558
Iteration 8/25 | Loss: 0.00120558
Iteration 9/25 | Loss: 0.00120558
Iteration 10/25 | Loss: 0.00120558
Iteration 11/25 | Loss: 0.00120558
Iteration 12/25 | Loss: 0.00120558
Iteration 13/25 | Loss: 0.00120558
Iteration 14/25 | Loss: 0.00120558
Iteration 15/25 | Loss: 0.00120558
Iteration 16/25 | Loss: 0.00120558
Iteration 17/25 | Loss: 0.00120558
Iteration 18/25 | Loss: 0.00120558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012055750703439116, 0.0012055750703439116, 0.0012055750703439116, 0.0012055750703439116, 0.0012055750703439116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012055750703439116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120558
Iteration 2/1000 | Loss: 0.00015221
Iteration 3/1000 | Loss: 0.00012155
Iteration 4/1000 | Loss: 0.00016245
Iteration 5/1000 | Loss: 0.00020747
Iteration 6/1000 | Loss: 0.00007909
Iteration 7/1000 | Loss: 0.00012363
Iteration 8/1000 | Loss: 0.00009473
Iteration 9/1000 | Loss: 0.00008658
Iteration 10/1000 | Loss: 0.00005076
Iteration 11/1000 | Loss: 0.00017046
Iteration 12/1000 | Loss: 0.00009385
Iteration 13/1000 | Loss: 0.00007681
Iteration 14/1000 | Loss: 0.00013319
Iteration 15/1000 | Loss: 0.00008961
Iteration 16/1000 | Loss: 0.00007042
Iteration 17/1000 | Loss: 0.00012770
Iteration 18/1000 | Loss: 0.00013394
Iteration 19/1000 | Loss: 0.00006127
Iteration 20/1000 | Loss: 0.00003488
Iteration 21/1000 | Loss: 0.00007662
Iteration 22/1000 | Loss: 0.00031224
Iteration 23/1000 | Loss: 0.00006181
Iteration 24/1000 | Loss: 0.00012745
Iteration 25/1000 | Loss: 0.00003086
Iteration 26/1000 | Loss: 0.00002990
Iteration 27/1000 | Loss: 0.00006457
Iteration 28/1000 | Loss: 0.00002968
Iteration 29/1000 | Loss: 0.00002860
Iteration 30/1000 | Loss: 0.00002782
Iteration 31/1000 | Loss: 0.00070745
Iteration 32/1000 | Loss: 0.00031385
Iteration 33/1000 | Loss: 0.00003528
Iteration 34/1000 | Loss: 0.00007525
Iteration 35/1000 | Loss: 0.00002663
Iteration 36/1000 | Loss: 0.00002408
Iteration 37/1000 | Loss: 0.00005862
Iteration 38/1000 | Loss: 0.00002280
Iteration 39/1000 | Loss: 0.00002109
Iteration 40/1000 | Loss: 0.00002035
Iteration 41/1000 | Loss: 0.00002390
Iteration 42/1000 | Loss: 0.00002191
Iteration 43/1000 | Loss: 0.00001948
Iteration 44/1000 | Loss: 0.00006249
Iteration 45/1000 | Loss: 0.00002218
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00002341
Iteration 48/1000 | Loss: 0.00002271
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00002373
Iteration 51/1000 | Loss: 0.00002279
Iteration 52/1000 | Loss: 0.00002367
Iteration 53/1000 | Loss: 0.00002299
Iteration 54/1000 | Loss: 0.00002330
Iteration 55/1000 | Loss: 0.00002295
Iteration 56/1000 | Loss: 0.00002281
Iteration 57/1000 | Loss: 0.00002257
Iteration 58/1000 | Loss: 0.00001934
Iteration 59/1000 | Loss: 0.00002185
Iteration 60/1000 | Loss: 0.00002264
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001854
Iteration 63/1000 | Loss: 0.00002246
Iteration 64/1000 | Loss: 0.00002290
Iteration 65/1000 | Loss: 0.00002236
Iteration 66/1000 | Loss: 0.00002237
Iteration 67/1000 | Loss: 0.00002226
Iteration 68/1000 | Loss: 0.00002239
Iteration 69/1000 | Loss: 0.00002220
Iteration 70/1000 | Loss: 0.00004532
Iteration 71/1000 | Loss: 0.00002424
Iteration 72/1000 | Loss: 0.00002616
Iteration 73/1000 | Loss: 0.00003249
Iteration 74/1000 | Loss: 0.00002012
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00001980
Iteration 77/1000 | Loss: 0.00002044
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00003841
Iteration 80/1000 | Loss: 0.00002349
Iteration 81/1000 | Loss: 0.00004061
Iteration 82/1000 | Loss: 0.00004732
Iteration 83/1000 | Loss: 0.00002817
Iteration 84/1000 | Loss: 0.00002386
Iteration 85/1000 | Loss: 0.00002007
Iteration 86/1000 | Loss: 0.00002085
Iteration 87/1000 | Loss: 0.00001958
Iteration 88/1000 | Loss: 0.00003138
Iteration 89/1000 | Loss: 0.00002375
Iteration 90/1000 | Loss: 0.00010481
Iteration 91/1000 | Loss: 0.00002558
Iteration 92/1000 | Loss: 0.00002014
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00003292
Iteration 95/1000 | Loss: 0.00002004
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002254
Iteration 98/1000 | Loss: 0.00002253
Iteration 99/1000 | Loss: 0.00002438
Iteration 100/1000 | Loss: 0.00007216
Iteration 101/1000 | Loss: 0.00002500
Iteration 102/1000 | Loss: 0.00003685
Iteration 103/1000 | Loss: 0.00001951
Iteration 104/1000 | Loss: 0.00002429
Iteration 105/1000 | Loss: 0.00003010
Iteration 106/1000 | Loss: 0.00002394
Iteration 107/1000 | Loss: 0.00002394
Iteration 108/1000 | Loss: 0.00002332
Iteration 109/1000 | Loss: 0.00002121
Iteration 110/1000 | Loss: 0.00001978
Iteration 111/1000 | Loss: 0.00002044
Iteration 112/1000 | Loss: 0.00003271
Iteration 113/1000 | Loss: 0.00002107
Iteration 114/1000 | Loss: 0.00001975
Iteration 115/1000 | Loss: 0.00002033
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00003482
Iteration 118/1000 | Loss: 0.00002561
Iteration 119/1000 | Loss: 0.00002935
Iteration 120/1000 | Loss: 0.00002480
Iteration 121/1000 | Loss: 0.00013748
Iteration 122/1000 | Loss: 0.00003363
Iteration 123/1000 | Loss: 0.00005842
Iteration 124/1000 | Loss: 0.00002463
Iteration 125/1000 | Loss: 0.00003473
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002224
Iteration 128/1000 | Loss: 0.00002056
Iteration 129/1000 | Loss: 0.00002100
Iteration 130/1000 | Loss: 0.00001889
Iteration 131/1000 | Loss: 0.00001954
Iteration 132/1000 | Loss: 0.00002114
Iteration 133/1000 | Loss: 0.00001942
Iteration 134/1000 | Loss: 0.00002065
Iteration 135/1000 | Loss: 0.00001989
Iteration 136/1000 | Loss: 0.00011929
Iteration 137/1000 | Loss: 0.00001971
Iteration 138/1000 | Loss: 0.00004721
Iteration 139/1000 | Loss: 0.00002263
Iteration 140/1000 | Loss: 0.00006557
Iteration 141/1000 | Loss: 0.00002151
Iteration 142/1000 | Loss: 0.00008251
Iteration 143/1000 | Loss: 0.00002488
Iteration 144/1000 | Loss: 0.00002293
Iteration 145/1000 | Loss: 0.00005025
Iteration 146/1000 | Loss: 0.00022268
Iteration 147/1000 | Loss: 0.00002926
Iteration 148/1000 | Loss: 0.00005131
Iteration 149/1000 | Loss: 0.00002744
Iteration 150/1000 | Loss: 0.00002486
Iteration 151/1000 | Loss: 0.00003303
Iteration 152/1000 | Loss: 0.00002748
Iteration 153/1000 | Loss: 0.00001837
Iteration 154/1000 | Loss: 0.00002687
Iteration 155/1000 | Loss: 0.00001974
Iteration 156/1000 | Loss: 0.00002029
Iteration 157/1000 | Loss: 0.00004469
Iteration 158/1000 | Loss: 0.00002127
Iteration 159/1000 | Loss: 0.00001995
Iteration 160/1000 | Loss: 0.00001980
Iteration 161/1000 | Loss: 0.00001995
Iteration 162/1000 | Loss: 0.00001973
Iteration 163/1000 | Loss: 0.00001997
Iteration 164/1000 | Loss: 0.00001855
Iteration 165/1000 | Loss: 0.00001813
Iteration 166/1000 | Loss: 0.00001808
Iteration 167/1000 | Loss: 0.00001806
Iteration 168/1000 | Loss: 0.00001930
Iteration 169/1000 | Loss: 0.00001970
Iteration 170/1000 | Loss: 0.00001973
Iteration 171/1000 | Loss: 0.00001954
Iteration 172/1000 | Loss: 0.00001923
Iteration 173/1000 | Loss: 0.00001921
Iteration 174/1000 | Loss: 0.00001920
Iteration 175/1000 | Loss: 0.00001904
Iteration 176/1000 | Loss: 0.00002403
Iteration 177/1000 | Loss: 0.00002050
Iteration 178/1000 | Loss: 0.00002182
Iteration 179/1000 | Loss: 0.00003807
Iteration 180/1000 | Loss: 0.00002521
Iteration 181/1000 | Loss: 0.00002396
Iteration 182/1000 | Loss: 0.00002193
Iteration 183/1000 | Loss: 0.00002285
Iteration 184/1000 | Loss: 0.00002175
Iteration 185/1000 | Loss: 0.00003612
Iteration 186/1000 | Loss: 0.00002450
Iteration 187/1000 | Loss: 0.00002960
Iteration 188/1000 | Loss: 0.00002363
Iteration 189/1000 | Loss: 0.00002393
Iteration 190/1000 | Loss: 0.00004994
Iteration 191/1000 | Loss: 0.00002166
Iteration 192/1000 | Loss: 0.00002285
Iteration 193/1000 | Loss: 0.00002163
Iteration 194/1000 | Loss: 0.00002134
Iteration 195/1000 | Loss: 0.00002048
Iteration 196/1000 | Loss: 0.00002457
Iteration 197/1000 | Loss: 0.00002327
Iteration 198/1000 | Loss: 0.00001983
Iteration 199/1000 | Loss: 0.00001916
Iteration 200/1000 | Loss: 0.00002164
Iteration 201/1000 | Loss: 0.00002022
Iteration 202/1000 | Loss: 0.00001998
Iteration 203/1000 | Loss: 0.00001966
Iteration 204/1000 | Loss: 0.00002103
Iteration 205/1000 | Loss: 0.00001985
Iteration 206/1000 | Loss: 0.00002116
Iteration 207/1000 | Loss: 0.00001959
Iteration 208/1000 | Loss: 0.00006162
Iteration 209/1000 | Loss: 0.00003059
Iteration 210/1000 | Loss: 0.00002927
Iteration 211/1000 | Loss: 0.00002409
Iteration 212/1000 | Loss: 0.00002837
Iteration 213/1000 | Loss: 0.00006762
Iteration 214/1000 | Loss: 0.00002567
Iteration 215/1000 | Loss: 0.00002040
Iteration 216/1000 | Loss: 0.00002170
Iteration 217/1000 | Loss: 0.00002623
Iteration 218/1000 | Loss: 0.00001979
Iteration 219/1000 | Loss: 0.00003439
Iteration 220/1000 | Loss: 0.00002390
Iteration 221/1000 | Loss: 0.00002530
Iteration 222/1000 | Loss: 0.00002924
Iteration 223/1000 | Loss: 0.00002414
Iteration 224/1000 | Loss: 0.00001965
Iteration 225/1000 | Loss: 0.00001893
Iteration 226/1000 | Loss: 0.00002562
Iteration 227/1000 | Loss: 0.00002573
Iteration 228/1000 | Loss: 0.00002525
Iteration 229/1000 | Loss: 0.00002731
Iteration 230/1000 | Loss: 0.00002906
Iteration 231/1000 | Loss: 0.00003301
Iteration 232/1000 | Loss: 0.00002835
Iteration 233/1000 | Loss: 0.00002540
Iteration 234/1000 | Loss: 0.00002483
Iteration 235/1000 | Loss: 0.00002525
Iteration 236/1000 | Loss: 0.00002441
Iteration 237/1000 | Loss: 0.00002508
Iteration 238/1000 | Loss: 0.00002445
Iteration 239/1000 | Loss: 0.00010510
Iteration 240/1000 | Loss: 0.00001975
Iteration 241/1000 | Loss: 0.00004714
Iteration 242/1000 | Loss: 0.00001839
Iteration 243/1000 | Loss: 0.00001807
Iteration 244/1000 | Loss: 0.00001804
Iteration 245/1000 | Loss: 0.00001804
Iteration 246/1000 | Loss: 0.00001803
Iteration 247/1000 | Loss: 0.00001803
Iteration 248/1000 | Loss: 0.00001803
Iteration 249/1000 | Loss: 0.00001802
Iteration 250/1000 | Loss: 0.00001802
Iteration 251/1000 | Loss: 0.00001802
Iteration 252/1000 | Loss: 0.00001802
Iteration 253/1000 | Loss: 0.00001802
Iteration 254/1000 | Loss: 0.00001801
Iteration 255/1000 | Loss: 0.00001801
Iteration 256/1000 | Loss: 0.00001801
Iteration 257/1000 | Loss: 0.00001801
Iteration 258/1000 | Loss: 0.00001801
Iteration 259/1000 | Loss: 0.00001800
Iteration 260/1000 | Loss: 0.00001800
Iteration 261/1000 | Loss: 0.00003172
Iteration 262/1000 | Loss: 0.00001979
Iteration 263/1000 | Loss: 0.00001984
Iteration 264/1000 | Loss: 0.00001802
Iteration 265/1000 | Loss: 0.00001800
Iteration 266/1000 | Loss: 0.00001800
Iteration 267/1000 | Loss: 0.00001800
Iteration 268/1000 | Loss: 0.00001800
Iteration 269/1000 | Loss: 0.00001800
Iteration 270/1000 | Loss: 0.00001800
Iteration 271/1000 | Loss: 0.00001798
Iteration 272/1000 | Loss: 0.00001798
Iteration 273/1000 | Loss: 0.00002067
Iteration 274/1000 | Loss: 0.00001799
Iteration 275/1000 | Loss: 0.00001798
Iteration 276/1000 | Loss: 0.00001798
Iteration 277/1000 | Loss: 0.00001797
Iteration 278/1000 | Loss: 0.00001797
Iteration 279/1000 | Loss: 0.00001797
Iteration 280/1000 | Loss: 0.00001797
Iteration 281/1000 | Loss: 0.00001797
Iteration 282/1000 | Loss: 0.00001797
Iteration 283/1000 | Loss: 0.00001797
Iteration 284/1000 | Loss: 0.00001797
Iteration 285/1000 | Loss: 0.00001805
Iteration 286/1000 | Loss: 0.00001798
Iteration 287/1000 | Loss: 0.00001797
Iteration 288/1000 | Loss: 0.00001797
Iteration 289/1000 | Loss: 0.00001797
Iteration 290/1000 | Loss: 0.00001797
Iteration 291/1000 | Loss: 0.00001797
Iteration 292/1000 | Loss: 0.00001797
Iteration 293/1000 | Loss: 0.00001797
Iteration 294/1000 | Loss: 0.00001797
Iteration 295/1000 | Loss: 0.00001797
Iteration 296/1000 | Loss: 0.00001797
Iteration 297/1000 | Loss: 0.00001796
Iteration 298/1000 | Loss: 0.00001796
Iteration 299/1000 | Loss: 0.00001796
Iteration 300/1000 | Loss: 0.00001796
Iteration 301/1000 | Loss: 0.00001795
Iteration 302/1000 | Loss: 0.00005174
Iteration 303/1000 | Loss: 0.00015016
Iteration 304/1000 | Loss: 0.00001806
Iteration 305/1000 | Loss: 0.00001792
Iteration 306/1000 | Loss: 0.00001790
Iteration 307/1000 | Loss: 0.00001790
Iteration 308/1000 | Loss: 0.00001790
Iteration 309/1000 | Loss: 0.00001790
Iteration 310/1000 | Loss: 0.00001789
Iteration 311/1000 | Loss: 0.00001789
Iteration 312/1000 | Loss: 0.00001788
Iteration 313/1000 | Loss: 0.00001788
Iteration 314/1000 | Loss: 0.00001788
Iteration 315/1000 | Loss: 0.00001788
Iteration 316/1000 | Loss: 0.00001788
Iteration 317/1000 | Loss: 0.00001788
Iteration 318/1000 | Loss: 0.00001788
Iteration 319/1000 | Loss: 0.00001787
Iteration 320/1000 | Loss: 0.00001787
Iteration 321/1000 | Loss: 0.00001787
Iteration 322/1000 | Loss: 0.00001786
Iteration 323/1000 | Loss: 0.00001786
Iteration 324/1000 | Loss: 0.00001786
Iteration 325/1000 | Loss: 0.00001785
Iteration 326/1000 | Loss: 0.00001785
Iteration 327/1000 | Loss: 0.00001785
Iteration 328/1000 | Loss: 0.00001785
Iteration 329/1000 | Loss: 0.00001785
Iteration 330/1000 | Loss: 0.00001784
Iteration 331/1000 | Loss: 0.00001784
Iteration 332/1000 | Loss: 0.00001784
Iteration 333/1000 | Loss: 0.00001784
Iteration 334/1000 | Loss: 0.00001784
Iteration 335/1000 | Loss: 0.00001784
Iteration 336/1000 | Loss: 0.00001783
Iteration 337/1000 | Loss: 0.00001783
Iteration 338/1000 | Loss: 0.00001783
Iteration 339/1000 | Loss: 0.00001783
Iteration 340/1000 | Loss: 0.00001783
Iteration 341/1000 | Loss: 0.00001783
Iteration 342/1000 | Loss: 0.00001782
Iteration 343/1000 | Loss: 0.00001782
Iteration 344/1000 | Loss: 0.00001782
Iteration 345/1000 | Loss: 0.00001782
Iteration 346/1000 | Loss: 0.00001782
Iteration 347/1000 | Loss: 0.00001782
Iteration 348/1000 | Loss: 0.00001782
Iteration 349/1000 | Loss: 0.00001782
Iteration 350/1000 | Loss: 0.00001781
Iteration 351/1000 | Loss: 0.00001781
Iteration 352/1000 | Loss: 0.00001781
Iteration 353/1000 | Loss: 0.00001781
Iteration 354/1000 | Loss: 0.00001781
Iteration 355/1000 | Loss: 0.00001781
Iteration 356/1000 | Loss: 0.00001781
Iteration 357/1000 | Loss: 0.00001781
Iteration 358/1000 | Loss: 0.00001781
Iteration 359/1000 | Loss: 0.00001781
Iteration 360/1000 | Loss: 0.00001781
Iteration 361/1000 | Loss: 0.00001781
Iteration 362/1000 | Loss: 0.00001780
Iteration 363/1000 | Loss: 0.00001780
Iteration 364/1000 | Loss: 0.00001780
Iteration 365/1000 | Loss: 0.00001780
Iteration 366/1000 | Loss: 0.00001780
Iteration 367/1000 | Loss: 0.00001780
Iteration 368/1000 | Loss: 0.00001780
Iteration 369/1000 | Loss: 0.00001780
Iteration 370/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 370. Stopping optimization.
Last 5 losses: [1.7804171875468455e-05, 1.7804171875468455e-05, 1.7804171875468455e-05, 1.7804171875468455e-05, 1.7804171875468455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7804171875468455e-05

Optimization complete. Final v2v error: 3.552325963973999 mm

Highest mean error: 4.659093379974365 mm for frame 25

Lowest mean error: 2.95526123046875 mm for frame 24

Saving results

Total time: 434.28510904312134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397890
Iteration 2/25 | Loss: 0.00113502
Iteration 3/25 | Loss: 0.00098902
Iteration 4/25 | Loss: 0.00097191
Iteration 5/25 | Loss: 0.00096702
Iteration 6/25 | Loss: 0.00096530
Iteration 7/25 | Loss: 0.00096513
Iteration 8/25 | Loss: 0.00096513
Iteration 9/25 | Loss: 0.00096513
Iteration 10/25 | Loss: 0.00096513
Iteration 11/25 | Loss: 0.00096513
Iteration 12/25 | Loss: 0.00096513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009651254513300955, 0.0009651254513300955, 0.0009651254513300955, 0.0009651254513300955, 0.0009651254513300955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009651254513300955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36366308
Iteration 2/25 | Loss: 0.00113207
Iteration 3/25 | Loss: 0.00113205
Iteration 4/25 | Loss: 0.00113205
Iteration 5/25 | Loss: 0.00113205
Iteration 6/25 | Loss: 0.00113204
Iteration 7/25 | Loss: 0.00113204
Iteration 8/25 | Loss: 0.00113204
Iteration 9/25 | Loss: 0.00113204
Iteration 10/25 | Loss: 0.00113204
Iteration 11/25 | Loss: 0.00113204
Iteration 12/25 | Loss: 0.00113204
Iteration 13/25 | Loss: 0.00113204
Iteration 14/25 | Loss: 0.00113204
Iteration 15/25 | Loss: 0.00113204
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011320437770336866, 0.0011320437770336866, 0.0011320437770336866, 0.0011320437770336866, 0.0011320437770336866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011320437770336866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113204
Iteration 2/1000 | Loss: 0.00005085
Iteration 3/1000 | Loss: 0.00002501
Iteration 4/1000 | Loss: 0.00001945
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001441
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001300
Iteration 9/1000 | Loss: 0.00001261
Iteration 10/1000 | Loss: 0.00001221
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001180
Iteration 16/1000 | Loss: 0.00001174
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001173
Iteration 20/1000 | Loss: 0.00001167
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001166
Iteration 23/1000 | Loss: 0.00001166
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001165
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001161
Iteration 30/1000 | Loss: 0.00001161
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001160
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001159
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001157
Iteration 38/1000 | Loss: 0.00001157
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001155
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001153
Iteration 48/1000 | Loss: 0.00001152
Iteration 49/1000 | Loss: 0.00001152
Iteration 50/1000 | Loss: 0.00001152
Iteration 51/1000 | Loss: 0.00001151
Iteration 52/1000 | Loss: 0.00001148
Iteration 53/1000 | Loss: 0.00001148
Iteration 54/1000 | Loss: 0.00001147
Iteration 55/1000 | Loss: 0.00001147
Iteration 56/1000 | Loss: 0.00001146
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001145
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001145
Iteration 63/1000 | Loss: 0.00001144
Iteration 64/1000 | Loss: 0.00001142
Iteration 65/1000 | Loss: 0.00001142
Iteration 66/1000 | Loss: 0.00001141
Iteration 67/1000 | Loss: 0.00001141
Iteration 68/1000 | Loss: 0.00001141
Iteration 69/1000 | Loss: 0.00001141
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001140
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001140
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001139
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001138
Iteration 82/1000 | Loss: 0.00001138
Iteration 83/1000 | Loss: 0.00001138
Iteration 84/1000 | Loss: 0.00001138
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001138
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001137
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001132
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001130
Iteration 133/1000 | Loss: 0.00001130
Iteration 134/1000 | Loss: 0.00001130
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001129
Iteration 138/1000 | Loss: 0.00001129
Iteration 139/1000 | Loss: 0.00001129
Iteration 140/1000 | Loss: 0.00001129
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001128
Iteration 145/1000 | Loss: 0.00001128
Iteration 146/1000 | Loss: 0.00001128
Iteration 147/1000 | Loss: 0.00001128
Iteration 148/1000 | Loss: 0.00001128
Iteration 149/1000 | Loss: 0.00001128
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001127
Iteration 152/1000 | Loss: 0.00001127
Iteration 153/1000 | Loss: 0.00001127
Iteration 154/1000 | Loss: 0.00001127
Iteration 155/1000 | Loss: 0.00001127
Iteration 156/1000 | Loss: 0.00001127
Iteration 157/1000 | Loss: 0.00001127
Iteration 158/1000 | Loss: 0.00001127
Iteration 159/1000 | Loss: 0.00001127
Iteration 160/1000 | Loss: 0.00001127
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001125
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001125
Iteration 169/1000 | Loss: 0.00001125
Iteration 170/1000 | Loss: 0.00001124
Iteration 171/1000 | Loss: 0.00001124
Iteration 172/1000 | Loss: 0.00001124
Iteration 173/1000 | Loss: 0.00001124
Iteration 174/1000 | Loss: 0.00001124
Iteration 175/1000 | Loss: 0.00001124
Iteration 176/1000 | Loss: 0.00001123
Iteration 177/1000 | Loss: 0.00001123
Iteration 178/1000 | Loss: 0.00001123
Iteration 179/1000 | Loss: 0.00001123
Iteration 180/1000 | Loss: 0.00001123
Iteration 181/1000 | Loss: 0.00001123
Iteration 182/1000 | Loss: 0.00001123
Iteration 183/1000 | Loss: 0.00001123
Iteration 184/1000 | Loss: 0.00001123
Iteration 185/1000 | Loss: 0.00001123
Iteration 186/1000 | Loss: 0.00001123
Iteration 187/1000 | Loss: 0.00001123
Iteration 188/1000 | Loss: 0.00001123
Iteration 189/1000 | Loss: 0.00001123
Iteration 190/1000 | Loss: 0.00001123
Iteration 191/1000 | Loss: 0.00001123
Iteration 192/1000 | Loss: 0.00001123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.1228996299905702e-05, 1.1228996299905702e-05, 1.1228996299905702e-05, 1.1228996299905702e-05, 1.1228996299905702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1228996299905702e-05

Optimization complete. Final v2v error: 2.650315284729004 mm

Highest mean error: 4.814927577972412 mm for frame 83

Lowest mean error: 1.9963674545288086 mm for frame 169

Saving results

Total time: 42.235872983932495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040436
Iteration 2/25 | Loss: 0.00255316
Iteration 3/25 | Loss: 0.00190248
Iteration 4/25 | Loss: 0.00179076
Iteration 5/25 | Loss: 0.00162067
Iteration 6/25 | Loss: 0.00160379
Iteration 7/25 | Loss: 0.00153150
Iteration 8/25 | Loss: 0.00146521
Iteration 9/25 | Loss: 0.00142756
Iteration 10/25 | Loss: 0.00142332
Iteration 11/25 | Loss: 0.00139557
Iteration 12/25 | Loss: 0.00138500
Iteration 13/25 | Loss: 0.00137529
Iteration 14/25 | Loss: 0.00137342
Iteration 15/25 | Loss: 0.00137402
Iteration 16/25 | Loss: 0.00137189
Iteration 17/25 | Loss: 0.00137263
Iteration 18/25 | Loss: 0.00136813
Iteration 19/25 | Loss: 0.00136848
Iteration 20/25 | Loss: 0.00136581
Iteration 21/25 | Loss: 0.00136479
Iteration 22/25 | Loss: 0.00136666
Iteration 23/25 | Loss: 0.00136497
Iteration 24/25 | Loss: 0.00136112
Iteration 25/25 | Loss: 0.00136048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31582677
Iteration 2/25 | Loss: 0.00663234
Iteration 3/25 | Loss: 0.00393306
Iteration 4/25 | Loss: 0.00393306
Iteration 5/25 | Loss: 0.00393306
Iteration 6/25 | Loss: 0.00393306
Iteration 7/25 | Loss: 0.00393306
Iteration 8/25 | Loss: 0.00393306
Iteration 9/25 | Loss: 0.00393306
Iteration 10/25 | Loss: 0.00393306
Iteration 11/25 | Loss: 0.00393306
Iteration 12/25 | Loss: 0.00393306
Iteration 13/25 | Loss: 0.00393306
Iteration 14/25 | Loss: 0.00393306
Iteration 15/25 | Loss: 0.00393306
Iteration 16/25 | Loss: 0.00393306
Iteration 17/25 | Loss: 0.00393306
Iteration 18/25 | Loss: 0.00393306
Iteration 19/25 | Loss: 0.00393306
Iteration 20/25 | Loss: 0.00393306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003933055326342583, 0.003933055326342583, 0.003933055326342583, 0.003933055326342583, 0.003933055326342583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003933055326342583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00393306
Iteration 2/1000 | Loss: 0.00193142
Iteration 3/1000 | Loss: 0.00039760
Iteration 4/1000 | Loss: 0.00230431
Iteration 5/1000 | Loss: 0.00031656
Iteration 6/1000 | Loss: 0.00080085
Iteration 7/1000 | Loss: 0.00053635
Iteration 8/1000 | Loss: 0.00178879
Iteration 9/1000 | Loss: 0.00062448
Iteration 10/1000 | Loss: 0.00029204
Iteration 11/1000 | Loss: 0.00064456
Iteration 12/1000 | Loss: 0.00053501
Iteration 13/1000 | Loss: 0.00042155
Iteration 14/1000 | Loss: 0.00036637
Iteration 15/1000 | Loss: 0.00102247
Iteration 16/1000 | Loss: 0.00205132
Iteration 17/1000 | Loss: 0.00196742
Iteration 18/1000 | Loss: 0.00221052
Iteration 19/1000 | Loss: 0.00181956
Iteration 20/1000 | Loss: 0.00133345
Iteration 21/1000 | Loss: 0.00067380
Iteration 22/1000 | Loss: 0.00096328
Iteration 23/1000 | Loss: 0.00070907
Iteration 24/1000 | Loss: 0.00195444
Iteration 25/1000 | Loss: 0.00149033
Iteration 26/1000 | Loss: 0.00162478
Iteration 27/1000 | Loss: 0.00052598
Iteration 28/1000 | Loss: 0.00038938
Iteration 29/1000 | Loss: 0.00029217
Iteration 30/1000 | Loss: 0.00091908
Iteration 31/1000 | Loss: 0.00051038
Iteration 32/1000 | Loss: 0.00145989
Iteration 33/1000 | Loss: 0.00087919
Iteration 34/1000 | Loss: 0.00050820
Iteration 35/1000 | Loss: 0.00023694
Iteration 36/1000 | Loss: 0.00188762
Iteration 37/1000 | Loss: 0.00069296
Iteration 38/1000 | Loss: 0.00017367
Iteration 39/1000 | Loss: 0.00018623
Iteration 40/1000 | Loss: 0.00066253
Iteration 41/1000 | Loss: 0.00031543
Iteration 42/1000 | Loss: 0.00038610
Iteration 43/1000 | Loss: 0.00014227
Iteration 44/1000 | Loss: 0.00052218
Iteration 45/1000 | Loss: 0.00021764
Iteration 46/1000 | Loss: 0.00025956
Iteration 47/1000 | Loss: 0.00012737
Iteration 48/1000 | Loss: 0.00012586
Iteration 49/1000 | Loss: 0.00012449
Iteration 50/1000 | Loss: 0.00023977
Iteration 51/1000 | Loss: 0.00099560
Iteration 52/1000 | Loss: 0.00094167
Iteration 53/1000 | Loss: 0.00081008
Iteration 54/1000 | Loss: 0.00036639
Iteration 55/1000 | Loss: 0.00038398
Iteration 56/1000 | Loss: 0.00048703
Iteration 57/1000 | Loss: 0.00128831
Iteration 58/1000 | Loss: 0.00210110
Iteration 59/1000 | Loss: 0.00113874
Iteration 60/1000 | Loss: 0.00013829
Iteration 61/1000 | Loss: 0.00012706
Iteration 62/1000 | Loss: 0.00012320
Iteration 63/1000 | Loss: 0.00012188
Iteration 64/1000 | Loss: 0.00017930
Iteration 65/1000 | Loss: 0.00017464
Iteration 66/1000 | Loss: 0.00011974
Iteration 67/1000 | Loss: 0.00011924
Iteration 68/1000 | Loss: 0.00029655
Iteration 69/1000 | Loss: 0.00029600
Iteration 70/1000 | Loss: 0.00026212
Iteration 71/1000 | Loss: 0.00032899
Iteration 72/1000 | Loss: 0.00013841
Iteration 73/1000 | Loss: 0.00018408
Iteration 74/1000 | Loss: 0.00013892
Iteration 75/1000 | Loss: 0.00024029
Iteration 76/1000 | Loss: 0.00011785
Iteration 77/1000 | Loss: 0.00011727
Iteration 78/1000 | Loss: 0.00014918
Iteration 79/1000 | Loss: 0.00011707
Iteration 80/1000 | Loss: 0.00011685
Iteration 81/1000 | Loss: 0.00011675
Iteration 82/1000 | Loss: 0.00011662
Iteration 83/1000 | Loss: 0.00011657
Iteration 84/1000 | Loss: 0.00011657
Iteration 85/1000 | Loss: 0.00011656
Iteration 86/1000 | Loss: 0.00019712
Iteration 87/1000 | Loss: 0.00011668
Iteration 88/1000 | Loss: 0.00011652
Iteration 89/1000 | Loss: 0.00011652
Iteration 90/1000 | Loss: 0.00011647
Iteration 91/1000 | Loss: 0.00011646
Iteration 92/1000 | Loss: 0.00011645
Iteration 93/1000 | Loss: 0.00011645
Iteration 94/1000 | Loss: 0.00011644
Iteration 95/1000 | Loss: 0.00011644
Iteration 96/1000 | Loss: 0.00011643
Iteration 97/1000 | Loss: 0.00011643
Iteration 98/1000 | Loss: 0.00011642
Iteration 99/1000 | Loss: 0.00011642
Iteration 100/1000 | Loss: 0.00011642
Iteration 101/1000 | Loss: 0.00011642
Iteration 102/1000 | Loss: 0.00011641
Iteration 103/1000 | Loss: 0.00011641
Iteration 104/1000 | Loss: 0.00011641
Iteration 105/1000 | Loss: 0.00011639
Iteration 106/1000 | Loss: 0.00011639
Iteration 107/1000 | Loss: 0.00011639
Iteration 108/1000 | Loss: 0.00011639
Iteration 109/1000 | Loss: 0.00011639
Iteration 110/1000 | Loss: 0.00011639
Iteration 111/1000 | Loss: 0.00011639
Iteration 112/1000 | Loss: 0.00011639
Iteration 113/1000 | Loss: 0.00011639
Iteration 114/1000 | Loss: 0.00011639
Iteration 115/1000 | Loss: 0.00011638
Iteration 116/1000 | Loss: 0.00011638
Iteration 117/1000 | Loss: 0.00011637
Iteration 118/1000 | Loss: 0.00011637
Iteration 119/1000 | Loss: 0.00011635
Iteration 120/1000 | Loss: 0.00011635
Iteration 121/1000 | Loss: 0.00011634
Iteration 122/1000 | Loss: 0.00011633
Iteration 123/1000 | Loss: 0.00011632
Iteration 124/1000 | Loss: 0.00011632
Iteration 125/1000 | Loss: 0.00011632
Iteration 126/1000 | Loss: 0.00011632
Iteration 127/1000 | Loss: 0.00011632
Iteration 128/1000 | Loss: 0.00011632
Iteration 129/1000 | Loss: 0.00011632
Iteration 130/1000 | Loss: 0.00011632
Iteration 131/1000 | Loss: 0.00011632
Iteration 132/1000 | Loss: 0.00011632
Iteration 133/1000 | Loss: 0.00011632
Iteration 134/1000 | Loss: 0.00011631
Iteration 135/1000 | Loss: 0.00011631
Iteration 136/1000 | Loss: 0.00011631
Iteration 137/1000 | Loss: 0.00011630
Iteration 138/1000 | Loss: 0.00011629
Iteration 139/1000 | Loss: 0.00011629
Iteration 140/1000 | Loss: 0.00011628
Iteration 141/1000 | Loss: 0.00011628
Iteration 142/1000 | Loss: 0.00011628
Iteration 143/1000 | Loss: 0.00011628
Iteration 144/1000 | Loss: 0.00011628
Iteration 145/1000 | Loss: 0.00011627
Iteration 146/1000 | Loss: 0.00011627
Iteration 147/1000 | Loss: 0.00011627
Iteration 148/1000 | Loss: 0.00011627
Iteration 149/1000 | Loss: 0.00011627
Iteration 150/1000 | Loss: 0.00011626
Iteration 151/1000 | Loss: 0.00011624
Iteration 152/1000 | Loss: 0.00011624
Iteration 153/1000 | Loss: 0.00011623
Iteration 154/1000 | Loss: 0.00011623
Iteration 155/1000 | Loss: 0.00011622
Iteration 156/1000 | Loss: 0.00011622
Iteration 157/1000 | Loss: 0.00011622
Iteration 158/1000 | Loss: 0.00011622
Iteration 159/1000 | Loss: 0.00011622
Iteration 160/1000 | Loss: 0.00011622
Iteration 161/1000 | Loss: 0.00011621
Iteration 162/1000 | Loss: 0.00011621
Iteration 163/1000 | Loss: 0.00011621
Iteration 164/1000 | Loss: 0.00011621
Iteration 165/1000 | Loss: 0.00011621
Iteration 166/1000 | Loss: 0.00011620
Iteration 167/1000 | Loss: 0.00011620
Iteration 168/1000 | Loss: 0.00011620
Iteration 169/1000 | Loss: 0.00011620
Iteration 170/1000 | Loss: 0.00011620
Iteration 171/1000 | Loss: 0.00011620
Iteration 172/1000 | Loss: 0.00011620
Iteration 173/1000 | Loss: 0.00011620
Iteration 174/1000 | Loss: 0.00011620
Iteration 175/1000 | Loss: 0.00011620
Iteration 176/1000 | Loss: 0.00011620
Iteration 177/1000 | Loss: 0.00011620
Iteration 178/1000 | Loss: 0.00011620
Iteration 179/1000 | Loss: 0.00011620
Iteration 180/1000 | Loss: 0.00011619
Iteration 181/1000 | Loss: 0.00011619
Iteration 182/1000 | Loss: 0.00011619
Iteration 183/1000 | Loss: 0.00011619
Iteration 184/1000 | Loss: 0.00011619
Iteration 185/1000 | Loss: 0.00011619
Iteration 186/1000 | Loss: 0.00011619
Iteration 187/1000 | Loss: 0.00011619
Iteration 188/1000 | Loss: 0.00011619
Iteration 189/1000 | Loss: 0.00011618
Iteration 190/1000 | Loss: 0.00011618
Iteration 191/1000 | Loss: 0.00011618
Iteration 192/1000 | Loss: 0.00011618
Iteration 193/1000 | Loss: 0.00011617
Iteration 194/1000 | Loss: 0.00011617
Iteration 195/1000 | Loss: 0.00011617
Iteration 196/1000 | Loss: 0.00011617
Iteration 197/1000 | Loss: 0.00011616
Iteration 198/1000 | Loss: 0.00011616
Iteration 199/1000 | Loss: 0.00011616
Iteration 200/1000 | Loss: 0.00011616
Iteration 201/1000 | Loss: 0.00011616
Iteration 202/1000 | Loss: 0.00011616
Iteration 203/1000 | Loss: 0.00011615
Iteration 204/1000 | Loss: 0.00011615
Iteration 205/1000 | Loss: 0.00011615
Iteration 206/1000 | Loss: 0.00011615
Iteration 207/1000 | Loss: 0.00011615
Iteration 208/1000 | Loss: 0.00011615
Iteration 209/1000 | Loss: 0.00011614
Iteration 210/1000 | Loss: 0.00011614
Iteration 211/1000 | Loss: 0.00011614
Iteration 212/1000 | Loss: 0.00019432
Iteration 213/1000 | Loss: 0.00011621
Iteration 214/1000 | Loss: 0.00011620
Iteration 215/1000 | Loss: 0.00011612
Iteration 216/1000 | Loss: 0.00011612
Iteration 217/1000 | Loss: 0.00011612
Iteration 218/1000 | Loss: 0.00011612
Iteration 219/1000 | Loss: 0.00011611
Iteration 220/1000 | Loss: 0.00011611
Iteration 221/1000 | Loss: 0.00011610
Iteration 222/1000 | Loss: 0.00011610
Iteration 223/1000 | Loss: 0.00011610
Iteration 224/1000 | Loss: 0.00011610
Iteration 225/1000 | Loss: 0.00011609
Iteration 226/1000 | Loss: 0.00011609
Iteration 227/1000 | Loss: 0.00011609
Iteration 228/1000 | Loss: 0.00011609
Iteration 229/1000 | Loss: 0.00011609
Iteration 230/1000 | Loss: 0.00011609
Iteration 231/1000 | Loss: 0.00011609
Iteration 232/1000 | Loss: 0.00011609
Iteration 233/1000 | Loss: 0.00011609
Iteration 234/1000 | Loss: 0.00011609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [0.00011609307694016024, 0.00011609307694016024, 0.00011609307694016024, 0.00011609307694016024, 0.00011609307694016024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00011609307694016024

Optimization complete. Final v2v error: 5.006242275238037 mm

Highest mean error: 12.509746551513672 mm for frame 8

Lowest mean error: 2.9468822479248047 mm for frame 62

Saving results

Total time: 204.17994260787964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370405
Iteration 2/25 | Loss: 0.00107921
Iteration 3/25 | Loss: 0.00096533
Iteration 4/25 | Loss: 0.00095337
Iteration 5/25 | Loss: 0.00095106
Iteration 6/25 | Loss: 0.00095106
Iteration 7/25 | Loss: 0.00095106
Iteration 8/25 | Loss: 0.00095106
Iteration 9/25 | Loss: 0.00095106
Iteration 10/25 | Loss: 0.00095106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009510559611953795, 0.0009510559611953795, 0.0009510559611953795, 0.0009510559611953795, 0.0009510559611953795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009510559611953795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49974322
Iteration 2/25 | Loss: 0.00087074
Iteration 3/25 | Loss: 0.00087074
Iteration 4/25 | Loss: 0.00087074
Iteration 5/25 | Loss: 0.00087073
Iteration 6/25 | Loss: 0.00087073
Iteration 7/25 | Loss: 0.00087073
Iteration 8/25 | Loss: 0.00087073
Iteration 9/25 | Loss: 0.00087073
Iteration 10/25 | Loss: 0.00087073
Iteration 11/25 | Loss: 0.00087073
Iteration 12/25 | Loss: 0.00087073
Iteration 13/25 | Loss: 0.00087073
Iteration 14/25 | Loss: 0.00087073
Iteration 15/25 | Loss: 0.00087073
Iteration 16/25 | Loss: 0.00087073
Iteration 17/25 | Loss: 0.00087073
Iteration 18/25 | Loss: 0.00087073
Iteration 19/25 | Loss: 0.00087073
Iteration 20/25 | Loss: 0.00087073
Iteration 21/25 | Loss: 0.00087073
Iteration 22/25 | Loss: 0.00087073
Iteration 23/25 | Loss: 0.00087073
Iteration 24/25 | Loss: 0.00087073
Iteration 25/25 | Loss: 0.00087073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087073
Iteration 2/1000 | Loss: 0.00002547
Iteration 3/1000 | Loss: 0.00001451
Iteration 4/1000 | Loss: 0.00001024
Iteration 5/1000 | Loss: 0.00000901
Iteration 6/1000 | Loss: 0.00000820
Iteration 7/1000 | Loss: 0.00000779
Iteration 8/1000 | Loss: 0.00000748
Iteration 9/1000 | Loss: 0.00000723
Iteration 10/1000 | Loss: 0.00000698
Iteration 11/1000 | Loss: 0.00000696
Iteration 12/1000 | Loss: 0.00000689
Iteration 13/1000 | Loss: 0.00000688
Iteration 14/1000 | Loss: 0.00000687
Iteration 15/1000 | Loss: 0.00000686
Iteration 16/1000 | Loss: 0.00000683
Iteration 17/1000 | Loss: 0.00000682
Iteration 18/1000 | Loss: 0.00000681
Iteration 19/1000 | Loss: 0.00000681
Iteration 20/1000 | Loss: 0.00000679
Iteration 21/1000 | Loss: 0.00000678
Iteration 22/1000 | Loss: 0.00000676
Iteration 23/1000 | Loss: 0.00000676
Iteration 24/1000 | Loss: 0.00000675
Iteration 25/1000 | Loss: 0.00000673
Iteration 26/1000 | Loss: 0.00000672
Iteration 27/1000 | Loss: 0.00000672
Iteration 28/1000 | Loss: 0.00000671
Iteration 29/1000 | Loss: 0.00000670
Iteration 30/1000 | Loss: 0.00000670
Iteration 31/1000 | Loss: 0.00000668
Iteration 32/1000 | Loss: 0.00000668
Iteration 33/1000 | Loss: 0.00000667
Iteration 34/1000 | Loss: 0.00000667
Iteration 35/1000 | Loss: 0.00000667
Iteration 36/1000 | Loss: 0.00000666
Iteration 37/1000 | Loss: 0.00000666
Iteration 38/1000 | Loss: 0.00000666
Iteration 39/1000 | Loss: 0.00000666
Iteration 40/1000 | Loss: 0.00000665
Iteration 41/1000 | Loss: 0.00000665
Iteration 42/1000 | Loss: 0.00000665
Iteration 43/1000 | Loss: 0.00000664
Iteration 44/1000 | Loss: 0.00000664
Iteration 45/1000 | Loss: 0.00000664
Iteration 46/1000 | Loss: 0.00000663
Iteration 47/1000 | Loss: 0.00000663
Iteration 48/1000 | Loss: 0.00000662
Iteration 49/1000 | Loss: 0.00000662
Iteration 50/1000 | Loss: 0.00000662
Iteration 51/1000 | Loss: 0.00000661
Iteration 52/1000 | Loss: 0.00000661
Iteration 53/1000 | Loss: 0.00000660
Iteration 54/1000 | Loss: 0.00000660
Iteration 55/1000 | Loss: 0.00000660
Iteration 56/1000 | Loss: 0.00000660
Iteration 57/1000 | Loss: 0.00000660
Iteration 58/1000 | Loss: 0.00000659
Iteration 59/1000 | Loss: 0.00000659
Iteration 60/1000 | Loss: 0.00000659
Iteration 61/1000 | Loss: 0.00000659
Iteration 62/1000 | Loss: 0.00000659
Iteration 63/1000 | Loss: 0.00000659
Iteration 64/1000 | Loss: 0.00000659
Iteration 65/1000 | Loss: 0.00000659
Iteration 66/1000 | Loss: 0.00000659
Iteration 67/1000 | Loss: 0.00000658
Iteration 68/1000 | Loss: 0.00000658
Iteration 69/1000 | Loss: 0.00000658
Iteration 70/1000 | Loss: 0.00000658
Iteration 71/1000 | Loss: 0.00000658
Iteration 72/1000 | Loss: 0.00000658
Iteration 73/1000 | Loss: 0.00000657
Iteration 74/1000 | Loss: 0.00000657
Iteration 75/1000 | Loss: 0.00000657
Iteration 76/1000 | Loss: 0.00000657
Iteration 77/1000 | Loss: 0.00000656
Iteration 78/1000 | Loss: 0.00000656
Iteration 79/1000 | Loss: 0.00000656
Iteration 80/1000 | Loss: 0.00000656
Iteration 81/1000 | Loss: 0.00000656
Iteration 82/1000 | Loss: 0.00000656
Iteration 83/1000 | Loss: 0.00000656
Iteration 84/1000 | Loss: 0.00000656
Iteration 85/1000 | Loss: 0.00000655
Iteration 86/1000 | Loss: 0.00000655
Iteration 87/1000 | Loss: 0.00000655
Iteration 88/1000 | Loss: 0.00000655
Iteration 89/1000 | Loss: 0.00000654
Iteration 90/1000 | Loss: 0.00000654
Iteration 91/1000 | Loss: 0.00000654
Iteration 92/1000 | Loss: 0.00000654
Iteration 93/1000 | Loss: 0.00000654
Iteration 94/1000 | Loss: 0.00000654
Iteration 95/1000 | Loss: 0.00000654
Iteration 96/1000 | Loss: 0.00000653
Iteration 97/1000 | Loss: 0.00000652
Iteration 98/1000 | Loss: 0.00000652
Iteration 99/1000 | Loss: 0.00000652
Iteration 100/1000 | Loss: 0.00000652
Iteration 101/1000 | Loss: 0.00000651
Iteration 102/1000 | Loss: 0.00000651
Iteration 103/1000 | Loss: 0.00000651
Iteration 104/1000 | Loss: 0.00000651
Iteration 105/1000 | Loss: 0.00000651
Iteration 106/1000 | Loss: 0.00000651
Iteration 107/1000 | Loss: 0.00000651
Iteration 108/1000 | Loss: 0.00000651
Iteration 109/1000 | Loss: 0.00000651
Iteration 110/1000 | Loss: 0.00000651
Iteration 111/1000 | Loss: 0.00000651
Iteration 112/1000 | Loss: 0.00000651
Iteration 113/1000 | Loss: 0.00000651
Iteration 114/1000 | Loss: 0.00000651
Iteration 115/1000 | Loss: 0.00000650
Iteration 116/1000 | Loss: 0.00000650
Iteration 117/1000 | Loss: 0.00000650
Iteration 118/1000 | Loss: 0.00000650
Iteration 119/1000 | Loss: 0.00000650
Iteration 120/1000 | Loss: 0.00000650
Iteration 121/1000 | Loss: 0.00000650
Iteration 122/1000 | Loss: 0.00000650
Iteration 123/1000 | Loss: 0.00000650
Iteration 124/1000 | Loss: 0.00000650
Iteration 125/1000 | Loss: 0.00000649
Iteration 126/1000 | Loss: 0.00000649
Iteration 127/1000 | Loss: 0.00000649
Iteration 128/1000 | Loss: 0.00000649
Iteration 129/1000 | Loss: 0.00000649
Iteration 130/1000 | Loss: 0.00000649
Iteration 131/1000 | Loss: 0.00000649
Iteration 132/1000 | Loss: 0.00000649
Iteration 133/1000 | Loss: 0.00000649
Iteration 134/1000 | Loss: 0.00000649
Iteration 135/1000 | Loss: 0.00000649
Iteration 136/1000 | Loss: 0.00000649
Iteration 137/1000 | Loss: 0.00000649
Iteration 138/1000 | Loss: 0.00000649
Iteration 139/1000 | Loss: 0.00000649
Iteration 140/1000 | Loss: 0.00000649
Iteration 141/1000 | Loss: 0.00000648
Iteration 142/1000 | Loss: 0.00000648
Iteration 143/1000 | Loss: 0.00000648
Iteration 144/1000 | Loss: 0.00000648
Iteration 145/1000 | Loss: 0.00000648
Iteration 146/1000 | Loss: 0.00000648
Iteration 147/1000 | Loss: 0.00000648
Iteration 148/1000 | Loss: 0.00000648
Iteration 149/1000 | Loss: 0.00000648
Iteration 150/1000 | Loss: 0.00000648
Iteration 151/1000 | Loss: 0.00000648
Iteration 152/1000 | Loss: 0.00000648
Iteration 153/1000 | Loss: 0.00000648
Iteration 154/1000 | Loss: 0.00000648
Iteration 155/1000 | Loss: 0.00000648
Iteration 156/1000 | Loss: 0.00000647
Iteration 157/1000 | Loss: 0.00000647
Iteration 158/1000 | Loss: 0.00000647
Iteration 159/1000 | Loss: 0.00000647
Iteration 160/1000 | Loss: 0.00000647
Iteration 161/1000 | Loss: 0.00000647
Iteration 162/1000 | Loss: 0.00000647
Iteration 163/1000 | Loss: 0.00000647
Iteration 164/1000 | Loss: 0.00000647
Iteration 165/1000 | Loss: 0.00000647
Iteration 166/1000 | Loss: 0.00000646
Iteration 167/1000 | Loss: 0.00000646
Iteration 168/1000 | Loss: 0.00000646
Iteration 169/1000 | Loss: 0.00000646
Iteration 170/1000 | Loss: 0.00000646
Iteration 171/1000 | Loss: 0.00000646
Iteration 172/1000 | Loss: 0.00000646
Iteration 173/1000 | Loss: 0.00000646
Iteration 174/1000 | Loss: 0.00000646
Iteration 175/1000 | Loss: 0.00000646
Iteration 176/1000 | Loss: 0.00000646
Iteration 177/1000 | Loss: 0.00000646
Iteration 178/1000 | Loss: 0.00000646
Iteration 179/1000 | Loss: 0.00000646
Iteration 180/1000 | Loss: 0.00000646
Iteration 181/1000 | Loss: 0.00000646
Iteration 182/1000 | Loss: 0.00000646
Iteration 183/1000 | Loss: 0.00000646
Iteration 184/1000 | Loss: 0.00000646
Iteration 185/1000 | Loss: 0.00000646
Iteration 186/1000 | Loss: 0.00000646
Iteration 187/1000 | Loss: 0.00000646
Iteration 188/1000 | Loss: 0.00000646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [6.457328709075227e-06, 6.457328709075227e-06, 6.457328709075227e-06, 6.457328709075227e-06, 6.457328709075227e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.457328709075227e-06

Optimization complete. Final v2v error: 2.1898353099823 mm

Highest mean error: 2.6407246589660645 mm for frame 145

Lowest mean error: 1.9283720254898071 mm for frame 65

Saving results

Total time: 39.599717140197754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806169
Iteration 2/25 | Loss: 0.00160860
Iteration 3/25 | Loss: 0.00128395
Iteration 4/25 | Loss: 0.00125358
Iteration 5/25 | Loss: 0.00125034
Iteration 6/25 | Loss: 0.00125014
Iteration 7/25 | Loss: 0.00125014
Iteration 8/25 | Loss: 0.00125014
Iteration 9/25 | Loss: 0.00125014
Iteration 10/25 | Loss: 0.00125014
Iteration 11/25 | Loss: 0.00125014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012501426972448826, 0.0012501426972448826, 0.0012501426972448826, 0.0012501426972448826, 0.0012501426972448826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012501426972448826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.24802426
Iteration 2/25 | Loss: 0.00139840
Iteration 3/25 | Loss: 0.00139840
Iteration 4/25 | Loss: 0.00139840
Iteration 5/25 | Loss: 0.00139840
Iteration 6/25 | Loss: 0.00139840
Iteration 7/25 | Loss: 0.00139840
Iteration 8/25 | Loss: 0.00139840
Iteration 9/25 | Loss: 0.00139840
Iteration 10/25 | Loss: 0.00139840
Iteration 11/25 | Loss: 0.00139840
Iteration 12/25 | Loss: 0.00139840
Iteration 13/25 | Loss: 0.00139840
Iteration 14/25 | Loss: 0.00139840
Iteration 15/25 | Loss: 0.00139840
Iteration 16/25 | Loss: 0.00139840
Iteration 17/25 | Loss: 0.00139840
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013983985409140587, 0.0013983985409140587, 0.0013983985409140587, 0.0013983985409140587, 0.0013983985409140587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013983985409140587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139840
Iteration 2/1000 | Loss: 0.00007660
Iteration 3/1000 | Loss: 0.00005108
Iteration 4/1000 | Loss: 0.00003892
Iteration 5/1000 | Loss: 0.00003500
Iteration 6/1000 | Loss: 0.00003262
Iteration 7/1000 | Loss: 0.00003134
Iteration 8/1000 | Loss: 0.00003032
Iteration 9/1000 | Loss: 0.00002975
Iteration 10/1000 | Loss: 0.00002942
Iteration 11/1000 | Loss: 0.00002915
Iteration 12/1000 | Loss: 0.00002893
Iteration 13/1000 | Loss: 0.00002870
Iteration 14/1000 | Loss: 0.00002855
Iteration 15/1000 | Loss: 0.00002843
Iteration 16/1000 | Loss: 0.00002840
Iteration 17/1000 | Loss: 0.00002838
Iteration 18/1000 | Loss: 0.00002837
Iteration 19/1000 | Loss: 0.00002837
Iteration 20/1000 | Loss: 0.00002836
Iteration 21/1000 | Loss: 0.00002835
Iteration 22/1000 | Loss: 0.00002834
Iteration 23/1000 | Loss: 0.00002834
Iteration 24/1000 | Loss: 0.00002834
Iteration 25/1000 | Loss: 0.00002834
Iteration 26/1000 | Loss: 0.00002834
Iteration 27/1000 | Loss: 0.00002834
Iteration 28/1000 | Loss: 0.00002833
Iteration 29/1000 | Loss: 0.00002833
Iteration 30/1000 | Loss: 0.00002833
Iteration 31/1000 | Loss: 0.00002833
Iteration 32/1000 | Loss: 0.00002833
Iteration 33/1000 | Loss: 0.00002833
Iteration 34/1000 | Loss: 0.00002832
Iteration 35/1000 | Loss: 0.00002832
Iteration 36/1000 | Loss: 0.00002831
Iteration 37/1000 | Loss: 0.00002831
Iteration 38/1000 | Loss: 0.00002830
Iteration 39/1000 | Loss: 0.00002830
Iteration 40/1000 | Loss: 0.00002830
Iteration 41/1000 | Loss: 0.00002830
Iteration 42/1000 | Loss: 0.00002830
Iteration 43/1000 | Loss: 0.00002830
Iteration 44/1000 | Loss: 0.00002829
Iteration 45/1000 | Loss: 0.00002829
Iteration 46/1000 | Loss: 0.00002829
Iteration 47/1000 | Loss: 0.00002829
Iteration 48/1000 | Loss: 0.00002829
Iteration 49/1000 | Loss: 0.00002829
Iteration 50/1000 | Loss: 0.00002828
Iteration 51/1000 | Loss: 0.00002828
Iteration 52/1000 | Loss: 0.00002827
Iteration 53/1000 | Loss: 0.00002827
Iteration 54/1000 | Loss: 0.00002827
Iteration 55/1000 | Loss: 0.00002827
Iteration 56/1000 | Loss: 0.00002826
Iteration 57/1000 | Loss: 0.00002826
Iteration 58/1000 | Loss: 0.00002826
Iteration 59/1000 | Loss: 0.00002826
Iteration 60/1000 | Loss: 0.00002826
Iteration 61/1000 | Loss: 0.00002826
Iteration 62/1000 | Loss: 0.00002826
Iteration 63/1000 | Loss: 0.00002826
Iteration 64/1000 | Loss: 0.00002826
Iteration 65/1000 | Loss: 0.00002825
Iteration 66/1000 | Loss: 0.00002824
Iteration 67/1000 | Loss: 0.00002824
Iteration 68/1000 | Loss: 0.00002824
Iteration 69/1000 | Loss: 0.00002823
Iteration 70/1000 | Loss: 0.00002823
Iteration 71/1000 | Loss: 0.00002822
Iteration 72/1000 | Loss: 0.00002822
Iteration 73/1000 | Loss: 0.00002821
Iteration 74/1000 | Loss: 0.00002821
Iteration 75/1000 | Loss: 0.00002819
Iteration 76/1000 | Loss: 0.00002818
Iteration 77/1000 | Loss: 0.00002818
Iteration 78/1000 | Loss: 0.00002817
Iteration 79/1000 | Loss: 0.00002814
Iteration 80/1000 | Loss: 0.00002814
Iteration 81/1000 | Loss: 0.00002814
Iteration 82/1000 | Loss: 0.00002814
Iteration 83/1000 | Loss: 0.00002814
Iteration 84/1000 | Loss: 0.00002814
Iteration 85/1000 | Loss: 0.00002814
Iteration 86/1000 | Loss: 0.00002813
Iteration 87/1000 | Loss: 0.00002813
Iteration 88/1000 | Loss: 0.00002813
Iteration 89/1000 | Loss: 0.00002813
Iteration 90/1000 | Loss: 0.00002813
Iteration 91/1000 | Loss: 0.00002813
Iteration 92/1000 | Loss: 0.00002812
Iteration 93/1000 | Loss: 0.00002812
Iteration 94/1000 | Loss: 0.00002811
Iteration 95/1000 | Loss: 0.00002811
Iteration 96/1000 | Loss: 0.00002809
Iteration 97/1000 | Loss: 0.00002809
Iteration 98/1000 | Loss: 0.00002808
Iteration 99/1000 | Loss: 0.00002808
Iteration 100/1000 | Loss: 0.00002808
Iteration 101/1000 | Loss: 0.00002808
Iteration 102/1000 | Loss: 0.00002808
Iteration 103/1000 | Loss: 0.00002807
Iteration 104/1000 | Loss: 0.00002807
Iteration 105/1000 | Loss: 0.00002807
Iteration 106/1000 | Loss: 0.00002807
Iteration 107/1000 | Loss: 0.00002806
Iteration 108/1000 | Loss: 0.00002806
Iteration 109/1000 | Loss: 0.00002806
Iteration 110/1000 | Loss: 0.00002802
Iteration 111/1000 | Loss: 0.00002797
Iteration 112/1000 | Loss: 0.00002796
Iteration 113/1000 | Loss: 0.00002796
Iteration 114/1000 | Loss: 0.00002796
Iteration 115/1000 | Loss: 0.00002796
Iteration 116/1000 | Loss: 0.00002795
Iteration 117/1000 | Loss: 0.00002795
Iteration 118/1000 | Loss: 0.00002794
Iteration 119/1000 | Loss: 0.00002794
Iteration 120/1000 | Loss: 0.00002793
Iteration 121/1000 | Loss: 0.00002793
Iteration 122/1000 | Loss: 0.00002792
Iteration 123/1000 | Loss: 0.00002792
Iteration 124/1000 | Loss: 0.00002792
Iteration 125/1000 | Loss: 0.00002791
Iteration 126/1000 | Loss: 0.00002791
Iteration 127/1000 | Loss: 0.00002791
Iteration 128/1000 | Loss: 0.00002790
Iteration 129/1000 | Loss: 0.00002790
Iteration 130/1000 | Loss: 0.00002790
Iteration 131/1000 | Loss: 0.00002790
Iteration 132/1000 | Loss: 0.00002790
Iteration 133/1000 | Loss: 0.00002790
Iteration 134/1000 | Loss: 0.00002790
Iteration 135/1000 | Loss: 0.00002790
Iteration 136/1000 | Loss: 0.00002790
Iteration 137/1000 | Loss: 0.00002790
Iteration 138/1000 | Loss: 0.00002790
Iteration 139/1000 | Loss: 0.00002789
Iteration 140/1000 | Loss: 0.00002789
Iteration 141/1000 | Loss: 0.00002789
Iteration 142/1000 | Loss: 0.00002788
Iteration 143/1000 | Loss: 0.00002788
Iteration 144/1000 | Loss: 0.00002788
Iteration 145/1000 | Loss: 0.00002788
Iteration 146/1000 | Loss: 0.00002788
Iteration 147/1000 | Loss: 0.00002788
Iteration 148/1000 | Loss: 0.00002788
Iteration 149/1000 | Loss: 0.00002788
Iteration 150/1000 | Loss: 0.00002788
Iteration 151/1000 | Loss: 0.00002788
Iteration 152/1000 | Loss: 0.00002788
Iteration 153/1000 | Loss: 0.00002788
Iteration 154/1000 | Loss: 0.00002788
Iteration 155/1000 | Loss: 0.00002788
Iteration 156/1000 | Loss: 0.00002788
Iteration 157/1000 | Loss: 0.00002788
Iteration 158/1000 | Loss: 0.00002787
Iteration 159/1000 | Loss: 0.00002787
Iteration 160/1000 | Loss: 0.00002787
Iteration 161/1000 | Loss: 0.00002787
Iteration 162/1000 | Loss: 0.00002787
Iteration 163/1000 | Loss: 0.00002786
Iteration 164/1000 | Loss: 0.00002786
Iteration 165/1000 | Loss: 0.00002786
Iteration 166/1000 | Loss: 0.00002786
Iteration 167/1000 | Loss: 0.00002786
Iteration 168/1000 | Loss: 0.00002786
Iteration 169/1000 | Loss: 0.00002786
Iteration 170/1000 | Loss: 0.00002786
Iteration 171/1000 | Loss: 0.00002786
Iteration 172/1000 | Loss: 0.00002786
Iteration 173/1000 | Loss: 0.00002786
Iteration 174/1000 | Loss: 0.00002786
Iteration 175/1000 | Loss: 0.00002786
Iteration 176/1000 | Loss: 0.00002786
Iteration 177/1000 | Loss: 0.00002786
Iteration 178/1000 | Loss: 0.00002786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.7862672141054645e-05, 2.7862672141054645e-05, 2.7862672141054645e-05, 2.7862672141054645e-05, 2.7862672141054645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7862672141054645e-05

Optimization complete. Final v2v error: 4.2486090660095215 mm

Highest mean error: 5.617068290710449 mm for frame 5

Lowest mean error: 3.5105888843536377 mm for frame 140

Saving results

Total time: 43.77177333831787
