Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=40, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2240-2295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0018
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00626097
Iteration 2/25 | Loss: 0.00120451
Iteration 3/25 | Loss: 0.00086968
Iteration 4/25 | Loss: 0.00080890
Iteration 5/25 | Loss: 0.00080199
Iteration 6/25 | Loss: 0.00080164
Iteration 7/25 | Loss: 0.00080164
Iteration 8/25 | Loss: 0.00080164
Iteration 9/25 | Loss: 0.00080164
Iteration 10/25 | Loss: 0.00080164
Iteration 11/25 | Loss: 0.00080164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008016428910195827, 0.0008016428910195827, 0.0008016428910195827, 0.0008016428910195827, 0.0008016428910195827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008016428910195827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25784302
Iteration 2/25 | Loss: 0.00034222
Iteration 3/25 | Loss: 0.00034221
Iteration 4/25 | Loss: 0.00034220
Iteration 5/25 | Loss: 0.00034220
Iteration 6/25 | Loss: 0.00034220
Iteration 7/25 | Loss: 0.00034220
Iteration 8/25 | Loss: 0.00034220
Iteration 9/25 | Loss: 0.00034220
Iteration 10/25 | Loss: 0.00034220
Iteration 11/25 | Loss: 0.00034220
Iteration 12/25 | Loss: 0.00034220
Iteration 13/25 | Loss: 0.00034220
Iteration 14/25 | Loss: 0.00034220
Iteration 15/25 | Loss: 0.00034220
Iteration 16/25 | Loss: 0.00034220
Iteration 17/25 | Loss: 0.00034220
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00034220292582176626, 0.00034220292582176626, 0.00034220292582176626, 0.00034220292582176626, 0.00034220292582176626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034220292582176626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034220
Iteration 2/1000 | Loss: 0.00004943
Iteration 3/1000 | Loss: 0.00003579
Iteration 4/1000 | Loss: 0.00003408
Iteration 5/1000 | Loss: 0.00003277
Iteration 6/1000 | Loss: 0.00003162
Iteration 7/1000 | Loss: 0.00003087
Iteration 8/1000 | Loss: 0.00003049
Iteration 9/1000 | Loss: 0.00003005
Iteration 10/1000 | Loss: 0.00002987
Iteration 11/1000 | Loss: 0.00002985
Iteration 12/1000 | Loss: 0.00002984
Iteration 13/1000 | Loss: 0.00002967
Iteration 14/1000 | Loss: 0.00002958
Iteration 15/1000 | Loss: 0.00002958
Iteration 16/1000 | Loss: 0.00002954
Iteration 17/1000 | Loss: 0.00002943
Iteration 18/1000 | Loss: 0.00002938
Iteration 19/1000 | Loss: 0.00002938
Iteration 20/1000 | Loss: 0.00002935
Iteration 21/1000 | Loss: 0.00002935
Iteration 22/1000 | Loss: 0.00002934
Iteration 23/1000 | Loss: 0.00002933
Iteration 24/1000 | Loss: 0.00002932
Iteration 25/1000 | Loss: 0.00002932
Iteration 26/1000 | Loss: 0.00002932
Iteration 27/1000 | Loss: 0.00002932
Iteration 28/1000 | Loss: 0.00002931
Iteration 29/1000 | Loss: 0.00002928
Iteration 30/1000 | Loss: 0.00002927
Iteration 31/1000 | Loss: 0.00002927
Iteration 32/1000 | Loss: 0.00002927
Iteration 33/1000 | Loss: 0.00002926
Iteration 34/1000 | Loss: 0.00002925
Iteration 35/1000 | Loss: 0.00002924
Iteration 36/1000 | Loss: 0.00002924
Iteration 37/1000 | Loss: 0.00002924
Iteration 38/1000 | Loss: 0.00002924
Iteration 39/1000 | Loss: 0.00002924
Iteration 40/1000 | Loss: 0.00002924
Iteration 41/1000 | Loss: 0.00002924
Iteration 42/1000 | Loss: 0.00002924
Iteration 43/1000 | Loss: 0.00002924
Iteration 44/1000 | Loss: 0.00002924
Iteration 45/1000 | Loss: 0.00002924
Iteration 46/1000 | Loss: 0.00002923
Iteration 47/1000 | Loss: 0.00002923
Iteration 48/1000 | Loss: 0.00002923
Iteration 49/1000 | Loss: 0.00002922
Iteration 50/1000 | Loss: 0.00002922
Iteration 51/1000 | Loss: 0.00002922
Iteration 52/1000 | Loss: 0.00002921
Iteration 53/1000 | Loss: 0.00002921
Iteration 54/1000 | Loss: 0.00002921
Iteration 55/1000 | Loss: 0.00002920
Iteration 56/1000 | Loss: 0.00002920
Iteration 57/1000 | Loss: 0.00002920
Iteration 58/1000 | Loss: 0.00002920
Iteration 59/1000 | Loss: 0.00002920
Iteration 60/1000 | Loss: 0.00002920
Iteration 61/1000 | Loss: 0.00002920
Iteration 62/1000 | Loss: 0.00002919
Iteration 63/1000 | Loss: 0.00002919
Iteration 64/1000 | Loss: 0.00002919
Iteration 65/1000 | Loss: 0.00002919
Iteration 66/1000 | Loss: 0.00002919
Iteration 67/1000 | Loss: 0.00002919
Iteration 68/1000 | Loss: 0.00002919
Iteration 69/1000 | Loss: 0.00002919
Iteration 70/1000 | Loss: 0.00002918
Iteration 71/1000 | Loss: 0.00002918
Iteration 72/1000 | Loss: 0.00002918
Iteration 73/1000 | Loss: 0.00002917
Iteration 74/1000 | Loss: 0.00002917
Iteration 75/1000 | Loss: 0.00002917
Iteration 76/1000 | Loss: 0.00002917
Iteration 77/1000 | Loss: 0.00002917
Iteration 78/1000 | Loss: 0.00002917
Iteration 79/1000 | Loss: 0.00002917
Iteration 80/1000 | Loss: 0.00002917
Iteration 81/1000 | Loss: 0.00002916
Iteration 82/1000 | Loss: 0.00002916
Iteration 83/1000 | Loss: 0.00002916
Iteration 84/1000 | Loss: 0.00002915
Iteration 85/1000 | Loss: 0.00002915
Iteration 86/1000 | Loss: 0.00002915
Iteration 87/1000 | Loss: 0.00002914
Iteration 88/1000 | Loss: 0.00002914
Iteration 89/1000 | Loss: 0.00002914
Iteration 90/1000 | Loss: 0.00002914
Iteration 91/1000 | Loss: 0.00002914
Iteration 92/1000 | Loss: 0.00002914
Iteration 93/1000 | Loss: 0.00002914
Iteration 94/1000 | Loss: 0.00002913
Iteration 95/1000 | Loss: 0.00002913
Iteration 96/1000 | Loss: 0.00002913
Iteration 97/1000 | Loss: 0.00002913
Iteration 98/1000 | Loss: 0.00002913
Iteration 99/1000 | Loss: 0.00002913
Iteration 100/1000 | Loss: 0.00002913
Iteration 101/1000 | Loss: 0.00002913
Iteration 102/1000 | Loss: 0.00002912
Iteration 103/1000 | Loss: 0.00002912
Iteration 104/1000 | Loss: 0.00002912
Iteration 105/1000 | Loss: 0.00002912
Iteration 106/1000 | Loss: 0.00002912
Iteration 107/1000 | Loss: 0.00002912
Iteration 108/1000 | Loss: 0.00002912
Iteration 109/1000 | Loss: 0.00002912
Iteration 110/1000 | Loss: 0.00002912
Iteration 111/1000 | Loss: 0.00002911
Iteration 112/1000 | Loss: 0.00002911
Iteration 113/1000 | Loss: 0.00002911
Iteration 114/1000 | Loss: 0.00002911
Iteration 115/1000 | Loss: 0.00002911
Iteration 116/1000 | Loss: 0.00002911
Iteration 117/1000 | Loss: 0.00002911
Iteration 118/1000 | Loss: 0.00002911
Iteration 119/1000 | Loss: 0.00002911
Iteration 120/1000 | Loss: 0.00002911
Iteration 121/1000 | Loss: 0.00002911
Iteration 122/1000 | Loss: 0.00002910
Iteration 123/1000 | Loss: 0.00002910
Iteration 124/1000 | Loss: 0.00002910
Iteration 125/1000 | Loss: 0.00002910
Iteration 126/1000 | Loss: 0.00002910
Iteration 127/1000 | Loss: 0.00002910
Iteration 128/1000 | Loss: 0.00002910
Iteration 129/1000 | Loss: 0.00002910
Iteration 130/1000 | Loss: 0.00002910
Iteration 131/1000 | Loss: 0.00002910
Iteration 132/1000 | Loss: 0.00002910
Iteration 133/1000 | Loss: 0.00002910
Iteration 134/1000 | Loss: 0.00002910
Iteration 135/1000 | Loss: 0.00002910
Iteration 136/1000 | Loss: 0.00002910
Iteration 137/1000 | Loss: 0.00002910
Iteration 138/1000 | Loss: 0.00002910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.9104758141329512e-05, 2.9104758141329512e-05, 2.9104758141329512e-05, 2.9104758141329512e-05, 2.9104758141329512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9104758141329512e-05

Optimization complete. Final v2v error: 4.572453498840332 mm

Highest mean error: 4.92110538482666 mm for frame 39

Lowest mean error: 3.8883893489837646 mm for frame 0

Saving results

Total time: 403.1899833679199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0000
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061105
Iteration 2/25 | Loss: 0.00181647
Iteration 3/25 | Loss: 0.00104909
Iteration 4/25 | Loss: 0.00088909
Iteration 5/25 | Loss: 0.00086840
Iteration 6/25 | Loss: 0.00079636
Iteration 7/25 | Loss: 0.00083173
Iteration 8/25 | Loss: 0.00083922
Iteration 9/25 | Loss: 0.00076685
Iteration 10/25 | Loss: 0.00077222
Iteration 11/25 | Loss: 0.00075521
Iteration 12/25 | Loss: 0.00079767
Iteration 13/25 | Loss: 0.00073178
Iteration 14/25 | Loss: 0.00069390
Iteration 15/25 | Loss: 0.00068577
Iteration 16/25 | Loss: 0.00068757
Iteration 17/25 | Loss: 0.00068290
Iteration 18/25 | Loss: 0.00068364
Iteration 19/25 | Loss: 0.00068643
Iteration 20/25 | Loss: 0.00068588
Iteration 21/25 | Loss: 0.00068493
Iteration 22/25 | Loss: 0.00068516
Iteration 23/25 | Loss: 0.00068447
Iteration 24/25 | Loss: 0.00068835
Iteration 25/25 | Loss: 0.00068481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35938621
Iteration 2/25 | Loss: 0.00043639
Iteration 3/25 | Loss: 0.00043639
Iteration 4/25 | Loss: 0.00043639
Iteration 5/25 | Loss: 0.00043639
Iteration 6/25 | Loss: 0.00043639
Iteration 7/25 | Loss: 0.00043639
Iteration 8/25 | Loss: 0.00043639
Iteration 9/25 | Loss: 0.00043639
Iteration 10/25 | Loss: 0.00043639
Iteration 11/25 | Loss: 0.00043639
Iteration 12/25 | Loss: 0.00043639
Iteration 13/25 | Loss: 0.00043639
Iteration 14/25 | Loss: 0.00043639
Iteration 15/25 | Loss: 0.00043639
Iteration 16/25 | Loss: 0.00043639
Iteration 17/25 | Loss: 0.00043639
Iteration 18/25 | Loss: 0.00043639
Iteration 19/25 | Loss: 0.00043639
Iteration 20/25 | Loss: 0.00043639
Iteration 21/25 | Loss: 0.00043639
Iteration 22/25 | Loss: 0.00043639
Iteration 23/25 | Loss: 0.00043639
Iteration 24/25 | Loss: 0.00043639
Iteration 25/25 | Loss: 0.00043639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043639
Iteration 2/1000 | Loss: 0.00058916
Iteration 3/1000 | Loss: 0.00060192
Iteration 4/1000 | Loss: 0.00034357
Iteration 5/1000 | Loss: 0.00048519
Iteration 6/1000 | Loss: 0.00059394
Iteration 7/1000 | Loss: 0.00042305
Iteration 8/1000 | Loss: 0.00045288
Iteration 9/1000 | Loss: 0.00046717
Iteration 10/1000 | Loss: 0.00025828
Iteration 11/1000 | Loss: 0.00015935
Iteration 12/1000 | Loss: 0.00008565
Iteration 13/1000 | Loss: 0.00036931
Iteration 14/1000 | Loss: 0.00054619
Iteration 15/1000 | Loss: 0.00045031
Iteration 16/1000 | Loss: 0.00093355
Iteration 17/1000 | Loss: 0.00042088
Iteration 18/1000 | Loss: 0.00032512
Iteration 19/1000 | Loss: 0.00045825
Iteration 20/1000 | Loss: 0.00081559
Iteration 21/1000 | Loss: 0.00035101
Iteration 22/1000 | Loss: 0.00042728
Iteration 23/1000 | Loss: 0.00026215
Iteration 24/1000 | Loss: 0.00030655
Iteration 25/1000 | Loss: 0.00056610
Iteration 26/1000 | Loss: 0.00088777
Iteration 27/1000 | Loss: 0.00038096
Iteration 28/1000 | Loss: 0.00030477
Iteration 29/1000 | Loss: 0.00006032
Iteration 30/1000 | Loss: 0.00004220
Iteration 31/1000 | Loss: 0.00047870
Iteration 32/1000 | Loss: 0.00017929
Iteration 33/1000 | Loss: 0.00005291
Iteration 34/1000 | Loss: 0.00003690
Iteration 35/1000 | Loss: 0.00049365
Iteration 36/1000 | Loss: 0.00066209
Iteration 37/1000 | Loss: 0.00047132
Iteration 38/1000 | Loss: 0.00083152
Iteration 39/1000 | Loss: 0.00032609
Iteration 40/1000 | Loss: 0.00021490
Iteration 41/1000 | Loss: 0.00015946
Iteration 42/1000 | Loss: 0.00003345
Iteration 43/1000 | Loss: 0.00016468
Iteration 44/1000 | Loss: 0.00030252
Iteration 45/1000 | Loss: 0.00010281
Iteration 46/1000 | Loss: 0.00016214
Iteration 47/1000 | Loss: 0.00021865
Iteration 48/1000 | Loss: 0.00003367
Iteration 49/1000 | Loss: 0.00011585
Iteration 50/1000 | Loss: 0.00009245
Iteration 51/1000 | Loss: 0.00010673
Iteration 52/1000 | Loss: 0.00008276
Iteration 53/1000 | Loss: 0.00009368
Iteration 54/1000 | Loss: 0.00028220
Iteration 55/1000 | Loss: 0.00047594
Iteration 56/1000 | Loss: 0.00051698
Iteration 57/1000 | Loss: 0.00028178
Iteration 58/1000 | Loss: 0.00003683
Iteration 59/1000 | Loss: 0.00013788
Iteration 60/1000 | Loss: 0.00011309
Iteration 61/1000 | Loss: 0.00012755
Iteration 62/1000 | Loss: 0.00014024
Iteration 63/1000 | Loss: 0.00009269
Iteration 64/1000 | Loss: 0.00018216
Iteration 65/1000 | Loss: 0.00039764
Iteration 66/1000 | Loss: 0.00039246
Iteration 67/1000 | Loss: 0.00033367
Iteration 68/1000 | Loss: 0.00004469
Iteration 69/1000 | Loss: 0.00013933
Iteration 70/1000 | Loss: 0.00012039
Iteration 71/1000 | Loss: 0.00011638
Iteration 72/1000 | Loss: 0.00008463
Iteration 73/1000 | Loss: 0.00010380
Iteration 74/1000 | Loss: 0.00032820
Iteration 75/1000 | Loss: 0.00029447
Iteration 76/1000 | Loss: 0.00007264
Iteration 77/1000 | Loss: 0.00042726
Iteration 78/1000 | Loss: 0.00051280
Iteration 79/1000 | Loss: 0.00015438
Iteration 80/1000 | Loss: 0.00005909
Iteration 81/1000 | Loss: 0.00005588
Iteration 82/1000 | Loss: 0.00002877
Iteration 83/1000 | Loss: 0.00002544
Iteration 84/1000 | Loss: 0.00002313
Iteration 85/1000 | Loss: 0.00002128
Iteration 86/1000 | Loss: 0.00001999
Iteration 87/1000 | Loss: 0.00003779
Iteration 88/1000 | Loss: 0.00003279
Iteration 89/1000 | Loss: 0.00002237
Iteration 90/1000 | Loss: 0.00002001
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00002763
Iteration 93/1000 | Loss: 0.00001960
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001844
Iteration 96/1000 | Loss: 0.00001804
Iteration 97/1000 | Loss: 0.00001768
Iteration 98/1000 | Loss: 0.00001756
Iteration 99/1000 | Loss: 0.00001738
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001713
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001712
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001711
Iteration 117/1000 | Loss: 0.00001711
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001710
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001709
Iteration 122/1000 | Loss: 0.00001709
Iteration 123/1000 | Loss: 0.00001709
Iteration 124/1000 | Loss: 0.00001709
Iteration 125/1000 | Loss: 0.00001709
Iteration 126/1000 | Loss: 0.00001709
Iteration 127/1000 | Loss: 0.00001709
Iteration 128/1000 | Loss: 0.00001709
Iteration 129/1000 | Loss: 0.00001709
Iteration 130/1000 | Loss: 0.00001708
Iteration 131/1000 | Loss: 0.00001708
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001708
Iteration 135/1000 | Loss: 0.00001708
Iteration 136/1000 | Loss: 0.00001708
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001708
Iteration 139/1000 | Loss: 0.00001708
Iteration 140/1000 | Loss: 0.00001708
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001708
Iteration 143/1000 | Loss: 0.00001707
Iteration 144/1000 | Loss: 0.00001707
Iteration 145/1000 | Loss: 0.00001707
Iteration 146/1000 | Loss: 0.00001707
Iteration 147/1000 | Loss: 0.00001707
Iteration 148/1000 | Loss: 0.00001707
Iteration 149/1000 | Loss: 0.00001707
Iteration 150/1000 | Loss: 0.00001707
Iteration 151/1000 | Loss: 0.00001707
Iteration 152/1000 | Loss: 0.00001707
Iteration 153/1000 | Loss: 0.00001707
Iteration 154/1000 | Loss: 0.00001707
Iteration 155/1000 | Loss: 0.00001707
Iteration 156/1000 | Loss: 0.00001706
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001706
Iteration 163/1000 | Loss: 0.00001706
Iteration 164/1000 | Loss: 0.00001706
Iteration 165/1000 | Loss: 0.00001706
Iteration 166/1000 | Loss: 0.00001706
Iteration 167/1000 | Loss: 0.00001706
Iteration 168/1000 | Loss: 0.00001706
Iteration 169/1000 | Loss: 0.00001706
Iteration 170/1000 | Loss: 0.00001706
Iteration 171/1000 | Loss: 0.00001706
Iteration 172/1000 | Loss: 0.00001706
Iteration 173/1000 | Loss: 0.00001706
Iteration 174/1000 | Loss: 0.00001706
Iteration 175/1000 | Loss: 0.00001706
Iteration 176/1000 | Loss: 0.00001706
Iteration 177/1000 | Loss: 0.00001706
Iteration 178/1000 | Loss: 0.00001706
Iteration 179/1000 | Loss: 0.00001706
Iteration 180/1000 | Loss: 0.00001706
Iteration 181/1000 | Loss: 0.00001706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.7064987332560122e-05, 1.7064987332560122e-05, 1.7064987332560122e-05, 1.7064987332560122e-05, 1.7064987332560122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7064987332560122e-05

Optimization complete. Final v2v error: 3.5158650875091553 mm

Highest mean error: 4.570137977600098 mm for frame 64

Lowest mean error: 2.827350378036499 mm for frame 1

Saving results

Total time: 1212.6307606697083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0007
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862531
Iteration 2/25 | Loss: 0.00072697
Iteration 3/25 | Loss: 0.00063597
Iteration 4/25 | Loss: 0.00061055
Iteration 5/25 | Loss: 0.00060303
Iteration 6/25 | Loss: 0.00060149
Iteration 7/25 | Loss: 0.00060129
Iteration 8/25 | Loss: 0.00060129
Iteration 9/25 | Loss: 0.00060129
Iteration 10/25 | Loss: 0.00060129
Iteration 11/25 | Loss: 0.00060129
Iteration 12/25 | Loss: 0.00060129
Iteration 13/25 | Loss: 0.00060129
Iteration 14/25 | Loss: 0.00060129
Iteration 15/25 | Loss: 0.00060129
Iteration 16/25 | Loss: 0.00060129
Iteration 17/25 | Loss: 0.00060129
Iteration 18/25 | Loss: 0.00060129
Iteration 19/25 | Loss: 0.00060129
Iteration 20/25 | Loss: 0.00060129
Iteration 21/25 | Loss: 0.00060129
Iteration 22/25 | Loss: 0.00060129
Iteration 23/25 | Loss: 0.00060129
Iteration 24/25 | Loss: 0.00060129
Iteration 25/25 | Loss: 0.00060129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.39791489
Iteration 2/25 | Loss: 0.00027441
Iteration 3/25 | Loss: 0.00027440
Iteration 4/25 | Loss: 0.00027440
Iteration 5/25 | Loss: 0.00027440
Iteration 6/25 | Loss: 0.00027440
Iteration 7/25 | Loss: 0.00027440
Iteration 8/25 | Loss: 0.00027440
Iteration 9/25 | Loss: 0.00027440
Iteration 10/25 | Loss: 0.00027440
Iteration 11/25 | Loss: 0.00027440
Iteration 12/25 | Loss: 0.00027440
Iteration 13/25 | Loss: 0.00027440
Iteration 14/25 | Loss: 0.00027440
Iteration 15/25 | Loss: 0.00027440
Iteration 16/25 | Loss: 0.00027440
Iteration 17/25 | Loss: 0.00027440
Iteration 18/25 | Loss: 0.00027440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002743992081377655, 0.0002743992081377655, 0.0002743992081377655, 0.0002743992081377655, 0.0002743992081377655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002743992081377655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027440
Iteration 2/1000 | Loss: 0.00003148
Iteration 3/1000 | Loss: 0.00002344
Iteration 4/1000 | Loss: 0.00002186
Iteration 5/1000 | Loss: 0.00001994
Iteration 6/1000 | Loss: 0.00001882
Iteration 7/1000 | Loss: 0.00001810
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001732
Iteration 10/1000 | Loss: 0.00001710
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001688
Iteration 14/1000 | Loss: 0.00001680
Iteration 15/1000 | Loss: 0.00001680
Iteration 16/1000 | Loss: 0.00001679
Iteration 17/1000 | Loss: 0.00001678
Iteration 18/1000 | Loss: 0.00001678
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001676
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001672
Iteration 24/1000 | Loss: 0.00001672
Iteration 25/1000 | Loss: 0.00001671
Iteration 26/1000 | Loss: 0.00001671
Iteration 27/1000 | Loss: 0.00001670
Iteration 28/1000 | Loss: 0.00001670
Iteration 29/1000 | Loss: 0.00001669
Iteration 30/1000 | Loss: 0.00001668
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001667
Iteration 33/1000 | Loss: 0.00001666
Iteration 34/1000 | Loss: 0.00001666
Iteration 35/1000 | Loss: 0.00001666
Iteration 36/1000 | Loss: 0.00001666
Iteration 37/1000 | Loss: 0.00001666
Iteration 38/1000 | Loss: 0.00001666
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001665
Iteration 42/1000 | Loss: 0.00001664
Iteration 43/1000 | Loss: 0.00001664
Iteration 44/1000 | Loss: 0.00001663
Iteration 45/1000 | Loss: 0.00001663
Iteration 46/1000 | Loss: 0.00001662
Iteration 47/1000 | Loss: 0.00001662
Iteration 48/1000 | Loss: 0.00001662
Iteration 49/1000 | Loss: 0.00001662
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001662
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001662
Iteration 54/1000 | Loss: 0.00001662
Iteration 55/1000 | Loss: 0.00001661
Iteration 56/1000 | Loss: 0.00001661
Iteration 57/1000 | Loss: 0.00001661
Iteration 58/1000 | Loss: 0.00001660
Iteration 59/1000 | Loss: 0.00001660
Iteration 60/1000 | Loss: 0.00001660
Iteration 61/1000 | Loss: 0.00001659
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001657
Iteration 64/1000 | Loss: 0.00001657
Iteration 65/1000 | Loss: 0.00001656
Iteration 66/1000 | Loss: 0.00001656
Iteration 67/1000 | Loss: 0.00001656
Iteration 68/1000 | Loss: 0.00001655
Iteration 69/1000 | Loss: 0.00001655
Iteration 70/1000 | Loss: 0.00001655
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001653
Iteration 73/1000 | Loss: 0.00001653
Iteration 74/1000 | Loss: 0.00001652
Iteration 75/1000 | Loss: 0.00001652
Iteration 76/1000 | Loss: 0.00001652
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001652
Iteration 82/1000 | Loss: 0.00001652
Iteration 83/1000 | Loss: 0.00001652
Iteration 84/1000 | Loss: 0.00001652
Iteration 85/1000 | Loss: 0.00001652
Iteration 86/1000 | Loss: 0.00001651
Iteration 87/1000 | Loss: 0.00001651
Iteration 88/1000 | Loss: 0.00001651
Iteration 89/1000 | Loss: 0.00001650
Iteration 90/1000 | Loss: 0.00001650
Iteration 91/1000 | Loss: 0.00001650
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001648
Iteration 97/1000 | Loss: 0.00001648
Iteration 98/1000 | Loss: 0.00001648
Iteration 99/1000 | Loss: 0.00001648
Iteration 100/1000 | Loss: 0.00001648
Iteration 101/1000 | Loss: 0.00001648
Iteration 102/1000 | Loss: 0.00001648
Iteration 103/1000 | Loss: 0.00001648
Iteration 104/1000 | Loss: 0.00001648
Iteration 105/1000 | Loss: 0.00001648
Iteration 106/1000 | Loss: 0.00001648
Iteration 107/1000 | Loss: 0.00001648
Iteration 108/1000 | Loss: 0.00001647
Iteration 109/1000 | Loss: 0.00001647
Iteration 110/1000 | Loss: 0.00001647
Iteration 111/1000 | Loss: 0.00001647
Iteration 112/1000 | Loss: 0.00001647
Iteration 113/1000 | Loss: 0.00001646
Iteration 114/1000 | Loss: 0.00001646
Iteration 115/1000 | Loss: 0.00001646
Iteration 116/1000 | Loss: 0.00001646
Iteration 117/1000 | Loss: 0.00001646
Iteration 118/1000 | Loss: 0.00001646
Iteration 119/1000 | Loss: 0.00001646
Iteration 120/1000 | Loss: 0.00001646
Iteration 121/1000 | Loss: 0.00001646
Iteration 122/1000 | Loss: 0.00001646
Iteration 123/1000 | Loss: 0.00001646
Iteration 124/1000 | Loss: 0.00001646
Iteration 125/1000 | Loss: 0.00001646
Iteration 126/1000 | Loss: 0.00001645
Iteration 127/1000 | Loss: 0.00001645
Iteration 128/1000 | Loss: 0.00001645
Iteration 129/1000 | Loss: 0.00001645
Iteration 130/1000 | Loss: 0.00001645
Iteration 131/1000 | Loss: 0.00001645
Iteration 132/1000 | Loss: 0.00001644
Iteration 133/1000 | Loss: 0.00001644
Iteration 134/1000 | Loss: 0.00001644
Iteration 135/1000 | Loss: 0.00001644
Iteration 136/1000 | Loss: 0.00001644
Iteration 137/1000 | Loss: 0.00001644
Iteration 138/1000 | Loss: 0.00001644
Iteration 139/1000 | Loss: 0.00001644
Iteration 140/1000 | Loss: 0.00001644
Iteration 141/1000 | Loss: 0.00001644
Iteration 142/1000 | Loss: 0.00001644
Iteration 143/1000 | Loss: 0.00001644
Iteration 144/1000 | Loss: 0.00001644
Iteration 145/1000 | Loss: 0.00001644
Iteration 146/1000 | Loss: 0.00001644
Iteration 147/1000 | Loss: 0.00001644
Iteration 148/1000 | Loss: 0.00001644
Iteration 149/1000 | Loss: 0.00001644
Iteration 150/1000 | Loss: 0.00001644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.644047733861953e-05, 1.644047733861953e-05, 1.644047733861953e-05, 1.644047733861953e-05, 1.644047733861953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.644047733861953e-05

Optimization complete. Final v2v error: 3.4320290088653564 mm

Highest mean error: 4.055014610290527 mm for frame 103

Lowest mean error: 2.9999794960021973 mm for frame 43

Saving results

Total time: 197.79597401618958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0004
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744365
Iteration 2/25 | Loss: 0.00149740
Iteration 3/25 | Loss: 0.00091709
Iteration 4/25 | Loss: 0.00085532
Iteration 5/25 | Loss: 0.00084283
Iteration 6/25 | Loss: 0.00084004
Iteration 7/25 | Loss: 0.00083942
Iteration 8/25 | Loss: 0.00083942
Iteration 9/25 | Loss: 0.00083942
Iteration 10/25 | Loss: 0.00083942
Iteration 11/25 | Loss: 0.00083942
Iteration 12/25 | Loss: 0.00083942
Iteration 13/25 | Loss: 0.00083942
Iteration 14/25 | Loss: 0.00083942
Iteration 15/25 | Loss: 0.00083942
Iteration 16/25 | Loss: 0.00083942
Iteration 17/25 | Loss: 0.00083942
Iteration 18/25 | Loss: 0.00083942
Iteration 19/25 | Loss: 0.00083942
Iteration 20/25 | Loss: 0.00083942
Iteration 21/25 | Loss: 0.00083942
Iteration 22/25 | Loss: 0.00083942
Iteration 23/25 | Loss: 0.00083942
Iteration 24/25 | Loss: 0.00083942
Iteration 25/25 | Loss: 0.00083942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.27495462
Iteration 2/25 | Loss: 0.00025842
Iteration 3/25 | Loss: 0.00025842
Iteration 4/25 | Loss: 0.00025842
Iteration 5/25 | Loss: 0.00025842
Iteration 6/25 | Loss: 0.00025842
Iteration 7/25 | Loss: 0.00025842
Iteration 8/25 | Loss: 0.00025842
Iteration 9/25 | Loss: 0.00025842
Iteration 10/25 | Loss: 0.00025842
Iteration 11/25 | Loss: 0.00025842
Iteration 12/25 | Loss: 0.00025842
Iteration 13/25 | Loss: 0.00025842
Iteration 14/25 | Loss: 0.00025842
Iteration 15/25 | Loss: 0.00025842
Iteration 16/25 | Loss: 0.00025842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002584153553470969, 0.0002584153553470969, 0.0002584153553470969, 0.0002584153553470969, 0.0002584153553470969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002584153553470969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025842
Iteration 2/1000 | Loss: 0.00004848
Iteration 3/1000 | Loss: 0.00003740
Iteration 4/1000 | Loss: 0.00003540
Iteration 5/1000 | Loss: 0.00003423
Iteration 6/1000 | Loss: 0.00003359
Iteration 7/1000 | Loss: 0.00003292
Iteration 8/1000 | Loss: 0.00003251
Iteration 9/1000 | Loss: 0.00003216
Iteration 10/1000 | Loss: 0.00003191
Iteration 11/1000 | Loss: 0.00003171
Iteration 12/1000 | Loss: 0.00003156
Iteration 13/1000 | Loss: 0.00003141
Iteration 14/1000 | Loss: 0.00003127
Iteration 15/1000 | Loss: 0.00003122
Iteration 16/1000 | Loss: 0.00003114
Iteration 17/1000 | Loss: 0.00003111
Iteration 18/1000 | Loss: 0.00003110
Iteration 19/1000 | Loss: 0.00003109
Iteration 20/1000 | Loss: 0.00003109
Iteration 21/1000 | Loss: 0.00003108
Iteration 22/1000 | Loss: 0.00003108
Iteration 23/1000 | Loss: 0.00003107
Iteration 24/1000 | Loss: 0.00003104
Iteration 25/1000 | Loss: 0.00003104
Iteration 26/1000 | Loss: 0.00003102
Iteration 27/1000 | Loss: 0.00003102
Iteration 28/1000 | Loss: 0.00003100
Iteration 29/1000 | Loss: 0.00003100
Iteration 30/1000 | Loss: 0.00003097
Iteration 31/1000 | Loss: 0.00003097
Iteration 32/1000 | Loss: 0.00003096
Iteration 33/1000 | Loss: 0.00003096
Iteration 34/1000 | Loss: 0.00003096
Iteration 35/1000 | Loss: 0.00003095
Iteration 36/1000 | Loss: 0.00003095
Iteration 37/1000 | Loss: 0.00003094
Iteration 38/1000 | Loss: 0.00003094
Iteration 39/1000 | Loss: 0.00003094
Iteration 40/1000 | Loss: 0.00003093
Iteration 41/1000 | Loss: 0.00003093
Iteration 42/1000 | Loss: 0.00003093
Iteration 43/1000 | Loss: 0.00003093
Iteration 44/1000 | Loss: 0.00003092
Iteration 45/1000 | Loss: 0.00003092
Iteration 46/1000 | Loss: 0.00003092
Iteration 47/1000 | Loss: 0.00003090
Iteration 48/1000 | Loss: 0.00003090
Iteration 49/1000 | Loss: 0.00003090
Iteration 50/1000 | Loss: 0.00003090
Iteration 51/1000 | Loss: 0.00003090
Iteration 52/1000 | Loss: 0.00003090
Iteration 53/1000 | Loss: 0.00003090
Iteration 54/1000 | Loss: 0.00003089
Iteration 55/1000 | Loss: 0.00003087
Iteration 56/1000 | Loss: 0.00003087
Iteration 57/1000 | Loss: 0.00003087
Iteration 58/1000 | Loss: 0.00003087
Iteration 59/1000 | Loss: 0.00003086
Iteration 60/1000 | Loss: 0.00003086
Iteration 61/1000 | Loss: 0.00003086
Iteration 62/1000 | Loss: 0.00003086
Iteration 63/1000 | Loss: 0.00003086
Iteration 64/1000 | Loss: 0.00003086
Iteration 65/1000 | Loss: 0.00003086
Iteration 66/1000 | Loss: 0.00003086
Iteration 67/1000 | Loss: 0.00003086
Iteration 68/1000 | Loss: 0.00003085
Iteration 69/1000 | Loss: 0.00003085
Iteration 70/1000 | Loss: 0.00003085
Iteration 71/1000 | Loss: 0.00003085
Iteration 72/1000 | Loss: 0.00003085
Iteration 73/1000 | Loss: 0.00003085
Iteration 74/1000 | Loss: 0.00003085
Iteration 75/1000 | Loss: 0.00003085
Iteration 76/1000 | Loss: 0.00003085
Iteration 77/1000 | Loss: 0.00003085
Iteration 78/1000 | Loss: 0.00003084
Iteration 79/1000 | Loss: 0.00003084
Iteration 80/1000 | Loss: 0.00003084
Iteration 81/1000 | Loss: 0.00003083
Iteration 82/1000 | Loss: 0.00003082
Iteration 83/1000 | Loss: 0.00003082
Iteration 84/1000 | Loss: 0.00003082
Iteration 85/1000 | Loss: 0.00003081
Iteration 86/1000 | Loss: 0.00003081
Iteration 87/1000 | Loss: 0.00003081
Iteration 88/1000 | Loss: 0.00003080
Iteration 89/1000 | Loss: 0.00003080
Iteration 90/1000 | Loss: 0.00003080
Iteration 91/1000 | Loss: 0.00003079
Iteration 92/1000 | Loss: 0.00003079
Iteration 93/1000 | Loss: 0.00003079
Iteration 94/1000 | Loss: 0.00003079
Iteration 95/1000 | Loss: 0.00003077
Iteration 96/1000 | Loss: 0.00003077
Iteration 97/1000 | Loss: 0.00003076
Iteration 98/1000 | Loss: 0.00003076
Iteration 99/1000 | Loss: 0.00003076
Iteration 100/1000 | Loss: 0.00003076
Iteration 101/1000 | Loss: 0.00003075
Iteration 102/1000 | Loss: 0.00003075
Iteration 103/1000 | Loss: 0.00003074
Iteration 104/1000 | Loss: 0.00003074
Iteration 105/1000 | Loss: 0.00003074
Iteration 106/1000 | Loss: 0.00003074
Iteration 107/1000 | Loss: 0.00003074
Iteration 108/1000 | Loss: 0.00003074
Iteration 109/1000 | Loss: 0.00003074
Iteration 110/1000 | Loss: 0.00003074
Iteration 111/1000 | Loss: 0.00003073
Iteration 112/1000 | Loss: 0.00003073
Iteration 113/1000 | Loss: 0.00003073
Iteration 114/1000 | Loss: 0.00003073
Iteration 115/1000 | Loss: 0.00003073
Iteration 116/1000 | Loss: 0.00003073
Iteration 117/1000 | Loss: 0.00003073
Iteration 118/1000 | Loss: 0.00003073
Iteration 119/1000 | Loss: 0.00003073
Iteration 120/1000 | Loss: 0.00003072
Iteration 121/1000 | Loss: 0.00003072
Iteration 122/1000 | Loss: 0.00003072
Iteration 123/1000 | Loss: 0.00003072
Iteration 124/1000 | Loss: 0.00003072
Iteration 125/1000 | Loss: 0.00003072
Iteration 126/1000 | Loss: 0.00003072
Iteration 127/1000 | Loss: 0.00003072
Iteration 128/1000 | Loss: 0.00003071
Iteration 129/1000 | Loss: 0.00003071
Iteration 130/1000 | Loss: 0.00003071
Iteration 131/1000 | Loss: 0.00003071
Iteration 132/1000 | Loss: 0.00003071
Iteration 133/1000 | Loss: 0.00003070
Iteration 134/1000 | Loss: 0.00003070
Iteration 135/1000 | Loss: 0.00003070
Iteration 136/1000 | Loss: 0.00003070
Iteration 137/1000 | Loss: 0.00003070
Iteration 138/1000 | Loss: 0.00003070
Iteration 139/1000 | Loss: 0.00003070
Iteration 140/1000 | Loss: 0.00003070
Iteration 141/1000 | Loss: 0.00003070
Iteration 142/1000 | Loss: 0.00003070
Iteration 143/1000 | Loss: 0.00003070
Iteration 144/1000 | Loss: 0.00003070
Iteration 145/1000 | Loss: 0.00003070
Iteration 146/1000 | Loss: 0.00003070
Iteration 147/1000 | Loss: 0.00003070
Iteration 148/1000 | Loss: 0.00003070
Iteration 149/1000 | Loss: 0.00003070
Iteration 150/1000 | Loss: 0.00003070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [3.06985093629919e-05, 3.06985093629919e-05, 3.06985093629919e-05, 3.06985093629919e-05, 3.06985093629919e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.06985093629919e-05

Optimization complete. Final v2v error: 4.448642730712891 mm

Highest mean error: 6.099541664123535 mm for frame 0

Lowest mean error: 3.7585678100585938 mm for frame 209

Saving results

Total time: 464.32875752449036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0009
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100119
Iteration 2/25 | Loss: 0.00193997
Iteration 3/25 | Loss: 0.00116065
Iteration 4/25 | Loss: 0.00097743
Iteration 5/25 | Loss: 0.00101869
Iteration 6/25 | Loss: 0.00111789
Iteration 7/25 | Loss: 0.00094675
Iteration 8/25 | Loss: 0.00083411
Iteration 9/25 | Loss: 0.00079769
Iteration 10/25 | Loss: 0.00077811
Iteration 11/25 | Loss: 0.00073467
Iteration 12/25 | Loss: 0.00073081
Iteration 13/25 | Loss: 0.00073451
Iteration 14/25 | Loss: 0.00072214
Iteration 15/25 | Loss: 0.00071136
Iteration 16/25 | Loss: 0.00071752
Iteration 17/25 | Loss: 0.00070702
Iteration 18/25 | Loss: 0.00069922
Iteration 19/25 | Loss: 0.00069357
Iteration 20/25 | Loss: 0.00069181
Iteration 21/25 | Loss: 0.00069742
Iteration 22/25 | Loss: 0.00069501
Iteration 23/25 | Loss: 0.00069659
Iteration 24/25 | Loss: 0.00069443
Iteration 25/25 | Loss: 0.00069423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44445133
Iteration 2/25 | Loss: 0.00052884
Iteration 3/25 | Loss: 0.00052884
Iteration 4/25 | Loss: 0.00052883
Iteration 5/25 | Loss: 0.00052883
Iteration 6/25 | Loss: 0.00052883
Iteration 7/25 | Loss: 0.00052883
Iteration 8/25 | Loss: 0.00052883
Iteration 9/25 | Loss: 0.00052883
Iteration 10/25 | Loss: 0.00052883
Iteration 11/25 | Loss: 0.00052883
Iteration 12/25 | Loss: 0.00052883
Iteration 13/25 | Loss: 0.00052883
Iteration 14/25 | Loss: 0.00052883
Iteration 15/25 | Loss: 0.00052883
Iteration 16/25 | Loss: 0.00052883
Iteration 17/25 | Loss: 0.00052883
Iteration 18/25 | Loss: 0.00052883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000528833013959229, 0.000528833013959229, 0.000528833013959229, 0.000528833013959229, 0.000528833013959229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000528833013959229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052883
Iteration 2/1000 | Loss: 0.00026489
Iteration 3/1000 | Loss: 0.00023380
Iteration 4/1000 | Loss: 0.00019949
Iteration 5/1000 | Loss: 0.00023278
Iteration 6/1000 | Loss: 0.00024811
Iteration 7/1000 | Loss: 0.00027592
Iteration 8/1000 | Loss: 0.00039085
Iteration 9/1000 | Loss: 0.00019040
Iteration 10/1000 | Loss: 0.00018199
Iteration 11/1000 | Loss: 0.00020660
Iteration 12/1000 | Loss: 0.00019419
Iteration 13/1000 | Loss: 0.00027306
Iteration 14/1000 | Loss: 0.00035972
Iteration 15/1000 | Loss: 0.00010738
Iteration 16/1000 | Loss: 0.00028730
Iteration 17/1000 | Loss: 0.00027847
Iteration 18/1000 | Loss: 0.00012730
Iteration 19/1000 | Loss: 0.00027521
Iteration 20/1000 | Loss: 0.00027812
Iteration 21/1000 | Loss: 0.00029060
Iteration 22/1000 | Loss: 0.00017449
Iteration 23/1000 | Loss: 0.00021752
Iteration 24/1000 | Loss: 0.00020715
Iteration 25/1000 | Loss: 0.00006873
Iteration 26/1000 | Loss: 0.00027889
Iteration 27/1000 | Loss: 0.00023630
Iteration 28/1000 | Loss: 0.00005665
Iteration 29/1000 | Loss: 0.00028940
Iteration 30/1000 | Loss: 0.00026261
Iteration 31/1000 | Loss: 0.00014033
Iteration 32/1000 | Loss: 0.00021189
Iteration 33/1000 | Loss: 0.00014808
Iteration 34/1000 | Loss: 0.00025465
Iteration 35/1000 | Loss: 0.00034511
Iteration 36/1000 | Loss: 0.00015389
Iteration 37/1000 | Loss: 0.00042108
Iteration 38/1000 | Loss: 0.00094903
Iteration 39/1000 | Loss: 0.00019943
Iteration 40/1000 | Loss: 0.00008514
Iteration 41/1000 | Loss: 0.00011820
Iteration 42/1000 | Loss: 0.00016393
Iteration 43/1000 | Loss: 0.00037811
Iteration 44/1000 | Loss: 0.00013726
Iteration 45/1000 | Loss: 0.00046077
Iteration 46/1000 | Loss: 0.00028349
Iteration 47/1000 | Loss: 0.00004578
Iteration 48/1000 | Loss: 0.00026625
Iteration 49/1000 | Loss: 0.00026633
Iteration 50/1000 | Loss: 0.00019939
Iteration 51/1000 | Loss: 0.00012554
Iteration 52/1000 | Loss: 0.00007936
Iteration 53/1000 | Loss: 0.00007750
Iteration 54/1000 | Loss: 0.00006793
Iteration 55/1000 | Loss: 0.00007419
Iteration 56/1000 | Loss: 0.00009723
Iteration 57/1000 | Loss: 0.00024681
Iteration 58/1000 | Loss: 0.00020735
Iteration 59/1000 | Loss: 0.00006980
Iteration 60/1000 | Loss: 0.00035403
Iteration 61/1000 | Loss: 0.00024857
Iteration 62/1000 | Loss: 0.00016666
Iteration 63/1000 | Loss: 0.00007004
Iteration 64/1000 | Loss: 0.00025412
Iteration 65/1000 | Loss: 0.00019336
Iteration 66/1000 | Loss: 0.00009809
Iteration 67/1000 | Loss: 0.00032518
Iteration 68/1000 | Loss: 0.00037425
Iteration 69/1000 | Loss: 0.00016317
Iteration 70/1000 | Loss: 0.00009906
Iteration 71/1000 | Loss: 0.00005747
Iteration 72/1000 | Loss: 0.00004187
Iteration 73/1000 | Loss: 0.00004348
Iteration 74/1000 | Loss: 0.00003836
Iteration 75/1000 | Loss: 0.00003849
Iteration 76/1000 | Loss: 0.00006823
Iteration 77/1000 | Loss: 0.00010140
Iteration 78/1000 | Loss: 0.00008840
Iteration 79/1000 | Loss: 0.00010807
Iteration 80/1000 | Loss: 0.00007410
Iteration 81/1000 | Loss: 0.00006051
Iteration 82/1000 | Loss: 0.00004232
Iteration 83/1000 | Loss: 0.00008423
Iteration 84/1000 | Loss: 0.00028793
Iteration 85/1000 | Loss: 0.00017231
Iteration 86/1000 | Loss: 0.00004896
Iteration 87/1000 | Loss: 0.00019753
Iteration 88/1000 | Loss: 0.00009216
Iteration 89/1000 | Loss: 0.00007491
Iteration 90/1000 | Loss: 0.00019731
Iteration 91/1000 | Loss: 0.00013154
Iteration 92/1000 | Loss: 0.00009838
Iteration 93/1000 | Loss: 0.00010361
Iteration 94/1000 | Loss: 0.00009512
Iteration 95/1000 | Loss: 0.00009781
Iteration 96/1000 | Loss: 0.00006197
Iteration 97/1000 | Loss: 0.00007275
Iteration 98/1000 | Loss: 0.00005020
Iteration 99/1000 | Loss: 0.00008005
Iteration 100/1000 | Loss: 0.00005353
Iteration 101/1000 | Loss: 0.00005459
Iteration 102/1000 | Loss: 0.00004922
Iteration 103/1000 | Loss: 0.00004899
Iteration 104/1000 | Loss: 0.00002597
Iteration 105/1000 | Loss: 0.00004374
Iteration 106/1000 | Loss: 0.00003085
Iteration 107/1000 | Loss: 0.00003188
Iteration 108/1000 | Loss: 0.00002589
Iteration 109/1000 | Loss: 0.00003486
Iteration 110/1000 | Loss: 0.00002976
Iteration 111/1000 | Loss: 0.00003548
Iteration 112/1000 | Loss: 0.00004902
Iteration 113/1000 | Loss: 0.00004507
Iteration 114/1000 | Loss: 0.00003020
Iteration 115/1000 | Loss: 0.00004018
Iteration 116/1000 | Loss: 0.00002830
Iteration 117/1000 | Loss: 0.00003660
Iteration 118/1000 | Loss: 0.00003017
Iteration 119/1000 | Loss: 0.00003514
Iteration 120/1000 | Loss: 0.00004842
Iteration 121/1000 | Loss: 0.00004377
Iteration 122/1000 | Loss: 0.00004629
Iteration 123/1000 | Loss: 0.00004301
Iteration 124/1000 | Loss: 0.00004351
Iteration 125/1000 | Loss: 0.00004909
Iteration 126/1000 | Loss: 0.00004658
Iteration 127/1000 | Loss: 0.00005418
Iteration 128/1000 | Loss: 0.00003027
Iteration 129/1000 | Loss: 0.00006307
Iteration 130/1000 | Loss: 0.00002411
Iteration 131/1000 | Loss: 0.00002189
Iteration 132/1000 | Loss: 0.00002097
Iteration 133/1000 | Loss: 0.00002023
Iteration 134/1000 | Loss: 0.00001986
Iteration 135/1000 | Loss: 0.00001965
Iteration 136/1000 | Loss: 0.00001952
Iteration 137/1000 | Loss: 0.00001948
Iteration 138/1000 | Loss: 0.00001946
Iteration 139/1000 | Loss: 0.00001945
Iteration 140/1000 | Loss: 0.00001944
Iteration 141/1000 | Loss: 0.00001944
Iteration 142/1000 | Loss: 0.00001944
Iteration 143/1000 | Loss: 0.00001943
Iteration 144/1000 | Loss: 0.00001943
Iteration 145/1000 | Loss: 0.00001943
Iteration 146/1000 | Loss: 0.00001942
Iteration 147/1000 | Loss: 0.00001941
Iteration 148/1000 | Loss: 0.00001941
Iteration 149/1000 | Loss: 0.00001933
Iteration 150/1000 | Loss: 0.00001933
Iteration 151/1000 | Loss: 0.00001932
Iteration 152/1000 | Loss: 0.00001932
Iteration 153/1000 | Loss: 0.00001932
Iteration 154/1000 | Loss: 0.00001931
Iteration 155/1000 | Loss: 0.00001931
Iteration 156/1000 | Loss: 0.00001928
Iteration 157/1000 | Loss: 0.00001927
Iteration 158/1000 | Loss: 0.00001927
Iteration 159/1000 | Loss: 0.00001926
Iteration 160/1000 | Loss: 0.00001926
Iteration 161/1000 | Loss: 0.00001922
Iteration 162/1000 | Loss: 0.00001922
Iteration 163/1000 | Loss: 0.00001922
Iteration 164/1000 | Loss: 0.00001920
Iteration 165/1000 | Loss: 0.00001920
Iteration 166/1000 | Loss: 0.00001920
Iteration 167/1000 | Loss: 0.00001920
Iteration 168/1000 | Loss: 0.00001920
Iteration 169/1000 | Loss: 0.00001920
Iteration 170/1000 | Loss: 0.00001920
Iteration 171/1000 | Loss: 0.00001919
Iteration 172/1000 | Loss: 0.00001919
Iteration 173/1000 | Loss: 0.00001919
Iteration 174/1000 | Loss: 0.00001919
Iteration 175/1000 | Loss: 0.00001919
Iteration 176/1000 | Loss: 0.00001919
Iteration 177/1000 | Loss: 0.00001919
Iteration 178/1000 | Loss: 0.00001919
Iteration 179/1000 | Loss: 0.00001918
Iteration 180/1000 | Loss: 0.00001918
Iteration 181/1000 | Loss: 0.00001918
Iteration 182/1000 | Loss: 0.00001918
Iteration 183/1000 | Loss: 0.00001918
Iteration 184/1000 | Loss: 0.00001918
Iteration 185/1000 | Loss: 0.00001918
Iteration 186/1000 | Loss: 0.00001917
Iteration 187/1000 | Loss: 0.00001917
Iteration 188/1000 | Loss: 0.00001917
Iteration 189/1000 | Loss: 0.00001917
Iteration 190/1000 | Loss: 0.00001917
Iteration 191/1000 | Loss: 0.00001916
Iteration 192/1000 | Loss: 0.00001916
Iteration 193/1000 | Loss: 0.00001916
Iteration 194/1000 | Loss: 0.00001916
Iteration 195/1000 | Loss: 0.00001916
Iteration 196/1000 | Loss: 0.00001916
Iteration 197/1000 | Loss: 0.00001916
Iteration 198/1000 | Loss: 0.00001916
Iteration 199/1000 | Loss: 0.00001916
Iteration 200/1000 | Loss: 0.00001916
Iteration 201/1000 | Loss: 0.00001916
Iteration 202/1000 | Loss: 0.00001916
Iteration 203/1000 | Loss: 0.00001916
Iteration 204/1000 | Loss: 0.00001915
Iteration 205/1000 | Loss: 0.00001915
Iteration 206/1000 | Loss: 0.00001915
Iteration 207/1000 | Loss: 0.00001915
Iteration 208/1000 | Loss: 0.00001915
Iteration 209/1000 | Loss: 0.00001915
Iteration 210/1000 | Loss: 0.00001914
Iteration 211/1000 | Loss: 0.00001914
Iteration 212/1000 | Loss: 0.00001913
Iteration 213/1000 | Loss: 0.00001913
Iteration 214/1000 | Loss: 0.00001913
Iteration 215/1000 | Loss: 0.00001913
Iteration 216/1000 | Loss: 0.00001913
Iteration 217/1000 | Loss: 0.00001912
Iteration 218/1000 | Loss: 0.00001912
Iteration 219/1000 | Loss: 0.00001912
Iteration 220/1000 | Loss: 0.00001912
Iteration 221/1000 | Loss: 0.00001912
Iteration 222/1000 | Loss: 0.00001912
Iteration 223/1000 | Loss: 0.00001912
Iteration 224/1000 | Loss: 0.00001912
Iteration 225/1000 | Loss: 0.00001912
Iteration 226/1000 | Loss: 0.00001912
Iteration 227/1000 | Loss: 0.00001912
Iteration 228/1000 | Loss: 0.00001911
Iteration 229/1000 | Loss: 0.00001911
Iteration 230/1000 | Loss: 0.00001911
Iteration 231/1000 | Loss: 0.00001911
Iteration 232/1000 | Loss: 0.00001911
Iteration 233/1000 | Loss: 0.00001911
Iteration 234/1000 | Loss: 0.00001911
Iteration 235/1000 | Loss: 0.00001911
Iteration 236/1000 | Loss: 0.00001911
Iteration 237/1000 | Loss: 0.00001911
Iteration 238/1000 | Loss: 0.00001911
Iteration 239/1000 | Loss: 0.00001911
Iteration 240/1000 | Loss: 0.00001911
Iteration 241/1000 | Loss: 0.00001911
Iteration 242/1000 | Loss: 0.00001911
Iteration 243/1000 | Loss: 0.00001911
Iteration 244/1000 | Loss: 0.00001911
Iteration 245/1000 | Loss: 0.00001911
Iteration 246/1000 | Loss: 0.00001911
Iteration 247/1000 | Loss: 0.00001911
Iteration 248/1000 | Loss: 0.00001911
Iteration 249/1000 | Loss: 0.00001911
Iteration 250/1000 | Loss: 0.00001911
Iteration 251/1000 | Loss: 0.00001911
Iteration 252/1000 | Loss: 0.00001911
Iteration 253/1000 | Loss: 0.00001911
Iteration 254/1000 | Loss: 0.00001911
Iteration 255/1000 | Loss: 0.00001911
Iteration 256/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.9108745618723333e-05, 1.9108745618723333e-05, 1.9108745618723333e-05, 1.9108745618723333e-05, 1.9108745618723333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9108745618723333e-05

Optimization complete. Final v2v error: 3.671420097351074 mm

Highest mean error: 4.342569828033447 mm for frame 8

Lowest mean error: 3.302759885787964 mm for frame 33

Saving results

Total time: 1206.631623506546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0011
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419885
Iteration 2/25 | Loss: 0.00081029
Iteration 3/25 | Loss: 0.00062867
Iteration 4/25 | Loss: 0.00060896
Iteration 5/25 | Loss: 0.00060239
Iteration 6/25 | Loss: 0.00060124
Iteration 7/25 | Loss: 0.00060123
Iteration 8/25 | Loss: 0.00060123
Iteration 9/25 | Loss: 0.00060123
Iteration 10/25 | Loss: 0.00060123
Iteration 11/25 | Loss: 0.00060123
Iteration 12/25 | Loss: 0.00060123
Iteration 13/25 | Loss: 0.00060123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000601229490712285, 0.000601229490712285, 0.000601229490712285, 0.000601229490712285, 0.000601229490712285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000601229490712285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28196228
Iteration 2/25 | Loss: 0.00025363
Iteration 3/25 | Loss: 0.00025363
Iteration 4/25 | Loss: 0.00025363
Iteration 5/25 | Loss: 0.00025363
Iteration 6/25 | Loss: 0.00025363
Iteration 7/25 | Loss: 0.00025363
Iteration 8/25 | Loss: 0.00025363
Iteration 9/25 | Loss: 0.00025363
Iteration 10/25 | Loss: 0.00025363
Iteration 11/25 | Loss: 0.00025363
Iteration 12/25 | Loss: 0.00025363
Iteration 13/25 | Loss: 0.00025363
Iteration 14/25 | Loss: 0.00025363
Iteration 15/25 | Loss: 0.00025363
Iteration 16/25 | Loss: 0.00025363
Iteration 17/25 | Loss: 0.00025363
Iteration 18/25 | Loss: 0.00025363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002536295505706221, 0.0002536295505706221, 0.0002536295505706221, 0.0002536295505706221, 0.0002536295505706221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002536295505706221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025363
Iteration 2/1000 | Loss: 0.00002016
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001452
Iteration 5/1000 | Loss: 0.00001384
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001307
Iteration 11/1000 | Loss: 0.00001303
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001302
Iteration 15/1000 | Loss: 0.00001299
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001298
Iteration 19/1000 | Loss: 0.00001298
Iteration 20/1000 | Loss: 0.00001297
Iteration 21/1000 | Loss: 0.00001296
Iteration 22/1000 | Loss: 0.00001295
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00001295
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001294
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001289
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001286
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001286
Iteration 41/1000 | Loss: 0.00001286
Iteration 42/1000 | Loss: 0.00001286
Iteration 43/1000 | Loss: 0.00001285
Iteration 44/1000 | Loss: 0.00001285
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001285
Iteration 47/1000 | Loss: 0.00001285
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001284
Iteration 52/1000 | Loss: 0.00001283
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001283
Iteration 55/1000 | Loss: 0.00001283
Iteration 56/1000 | Loss: 0.00001283
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001282
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001280
Iteration 66/1000 | Loss: 0.00001280
Iteration 67/1000 | Loss: 0.00001280
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001276
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001275
Iteration 85/1000 | Loss: 0.00001275
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001274
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001273
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001272
Iteration 92/1000 | Loss: 0.00001272
Iteration 93/1000 | Loss: 0.00001272
Iteration 94/1000 | Loss: 0.00001271
Iteration 95/1000 | Loss: 0.00001271
Iteration 96/1000 | Loss: 0.00001271
Iteration 97/1000 | Loss: 0.00001271
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001271
Iteration 100/1000 | Loss: 0.00001271
Iteration 101/1000 | Loss: 0.00001271
Iteration 102/1000 | Loss: 0.00001271
Iteration 103/1000 | Loss: 0.00001270
Iteration 104/1000 | Loss: 0.00001270
Iteration 105/1000 | Loss: 0.00001269
Iteration 106/1000 | Loss: 0.00001269
Iteration 107/1000 | Loss: 0.00001269
Iteration 108/1000 | Loss: 0.00001269
Iteration 109/1000 | Loss: 0.00001268
Iteration 110/1000 | Loss: 0.00001268
Iteration 111/1000 | Loss: 0.00001268
Iteration 112/1000 | Loss: 0.00001268
Iteration 113/1000 | Loss: 0.00001267
Iteration 114/1000 | Loss: 0.00001267
Iteration 115/1000 | Loss: 0.00001267
Iteration 116/1000 | Loss: 0.00001267
Iteration 117/1000 | Loss: 0.00001267
Iteration 118/1000 | Loss: 0.00001267
Iteration 119/1000 | Loss: 0.00001267
Iteration 120/1000 | Loss: 0.00001267
Iteration 121/1000 | Loss: 0.00001267
Iteration 122/1000 | Loss: 0.00001266
Iteration 123/1000 | Loss: 0.00001266
Iteration 124/1000 | Loss: 0.00001266
Iteration 125/1000 | Loss: 0.00001266
Iteration 126/1000 | Loss: 0.00001266
Iteration 127/1000 | Loss: 0.00001266
Iteration 128/1000 | Loss: 0.00001266
Iteration 129/1000 | Loss: 0.00001266
Iteration 130/1000 | Loss: 0.00001266
Iteration 131/1000 | Loss: 0.00001266
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001265
Iteration 134/1000 | Loss: 0.00001265
Iteration 135/1000 | Loss: 0.00001265
Iteration 136/1000 | Loss: 0.00001265
Iteration 137/1000 | Loss: 0.00001265
Iteration 138/1000 | Loss: 0.00001265
Iteration 139/1000 | Loss: 0.00001265
Iteration 140/1000 | Loss: 0.00001265
Iteration 141/1000 | Loss: 0.00001265
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001265
Iteration 145/1000 | Loss: 0.00001265
Iteration 146/1000 | Loss: 0.00001265
Iteration 147/1000 | Loss: 0.00001265
Iteration 148/1000 | Loss: 0.00001265
Iteration 149/1000 | Loss: 0.00001265
Iteration 150/1000 | Loss: 0.00001265
Iteration 151/1000 | Loss: 0.00001265
Iteration 152/1000 | Loss: 0.00001265
Iteration 153/1000 | Loss: 0.00001265
Iteration 154/1000 | Loss: 0.00001265
Iteration 155/1000 | Loss: 0.00001265
Iteration 156/1000 | Loss: 0.00001265
Iteration 157/1000 | Loss: 0.00001265
Iteration 158/1000 | Loss: 0.00001265
Iteration 159/1000 | Loss: 0.00001265
Iteration 160/1000 | Loss: 0.00001265
Iteration 161/1000 | Loss: 0.00001265
Iteration 162/1000 | Loss: 0.00001265
Iteration 163/1000 | Loss: 0.00001265
Iteration 164/1000 | Loss: 0.00001265
Iteration 165/1000 | Loss: 0.00001265
Iteration 166/1000 | Loss: 0.00001265
Iteration 167/1000 | Loss: 0.00001265
Iteration 168/1000 | Loss: 0.00001265
Iteration 169/1000 | Loss: 0.00001265
Iteration 170/1000 | Loss: 0.00001265
Iteration 171/1000 | Loss: 0.00001265
Iteration 172/1000 | Loss: 0.00001265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.265010087081464e-05, 1.265010087081464e-05, 1.265010087081464e-05, 1.265010087081464e-05, 1.265010087081464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.265010087081464e-05

Optimization complete. Final v2v error: 3.0060722827911377 mm

Highest mean error: 3.4352898597717285 mm for frame 8

Lowest mean error: 2.6599676609039307 mm for frame 0

Saving results

Total time: 293.6066586971283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0020
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868687
Iteration 2/25 | Loss: 0.00103035
Iteration 3/25 | Loss: 0.00068785
Iteration 4/25 | Loss: 0.00061814
Iteration 5/25 | Loss: 0.00060723
Iteration 6/25 | Loss: 0.00060468
Iteration 7/25 | Loss: 0.00060414
Iteration 8/25 | Loss: 0.00060414
Iteration 9/25 | Loss: 0.00060414
Iteration 10/25 | Loss: 0.00060414
Iteration 11/25 | Loss: 0.00060414
Iteration 12/25 | Loss: 0.00060414
Iteration 13/25 | Loss: 0.00060414
Iteration 14/25 | Loss: 0.00060414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006041410379111767, 0.0006041410379111767, 0.0006041410379111767, 0.0006041410379111767, 0.0006041410379111767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006041410379111767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29805779
Iteration 2/25 | Loss: 0.00030115
Iteration 3/25 | Loss: 0.00030115
Iteration 4/25 | Loss: 0.00030115
Iteration 5/25 | Loss: 0.00030115
Iteration 6/25 | Loss: 0.00030115
Iteration 7/25 | Loss: 0.00030115
Iteration 8/25 | Loss: 0.00030115
Iteration 9/25 | Loss: 0.00030115
Iteration 10/25 | Loss: 0.00030115
Iteration 11/25 | Loss: 0.00030115
Iteration 12/25 | Loss: 0.00030115
Iteration 13/25 | Loss: 0.00030115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00030114781111478806, 0.00030114781111478806, 0.00030114781111478806, 0.00030114781111478806, 0.00030114781111478806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030114781111478806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030115
Iteration 2/1000 | Loss: 0.00002336
Iteration 3/1000 | Loss: 0.00001735
Iteration 4/1000 | Loss: 0.00001561
Iteration 5/1000 | Loss: 0.00001466
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001337
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001315
Iteration 12/1000 | Loss: 0.00001312
Iteration 13/1000 | Loss: 0.00001310
Iteration 14/1000 | Loss: 0.00001309
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001306
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001303
Iteration 23/1000 | Loss: 0.00001302
Iteration 24/1000 | Loss: 0.00001302
Iteration 25/1000 | Loss: 0.00001298
Iteration 26/1000 | Loss: 0.00001298
Iteration 27/1000 | Loss: 0.00001297
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001296
Iteration 30/1000 | Loss: 0.00001296
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001293
Iteration 34/1000 | Loss: 0.00001291
Iteration 35/1000 | Loss: 0.00001291
Iteration 36/1000 | Loss: 0.00001291
Iteration 37/1000 | Loss: 0.00001290
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001288
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001287
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001287
Iteration 55/1000 | Loss: 0.00001287
Iteration 56/1000 | Loss: 0.00001287
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001287
Iteration 61/1000 | Loss: 0.00001287
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.2865207281720359e-05, 1.2865207281720359e-05, 1.2865207281720359e-05, 1.2865207281720359e-05, 1.2865207281720359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2865207281720359e-05

Optimization complete. Final v2v error: 3.0244455337524414 mm

Highest mean error: 3.562889337539673 mm for frame 84

Lowest mean error: 2.5598244667053223 mm for frame 244

Saving results

Total time: 362.24243664741516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0014
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005841
Iteration 2/25 | Loss: 0.00155887
Iteration 3/25 | Loss: 0.00090364
Iteration 4/25 | Loss: 0.00078357
Iteration 5/25 | Loss: 0.00077361
Iteration 6/25 | Loss: 0.00075960
Iteration 7/25 | Loss: 0.00073408
Iteration 8/25 | Loss: 0.00071509
Iteration 9/25 | Loss: 0.00070080
Iteration 10/25 | Loss: 0.00068880
Iteration 11/25 | Loss: 0.00068357
Iteration 12/25 | Loss: 0.00067127
Iteration 13/25 | Loss: 0.00066261
Iteration 14/25 | Loss: 0.00066338
Iteration 15/25 | Loss: 0.00066583
Iteration 16/25 | Loss: 0.00066200
Iteration 17/25 | Loss: 0.00065577
Iteration 18/25 | Loss: 0.00065428
Iteration 19/25 | Loss: 0.00064800
Iteration 20/25 | Loss: 0.00064549
Iteration 21/25 | Loss: 0.00064666
Iteration 22/25 | Loss: 0.00065457
Iteration 23/25 | Loss: 0.00064847
Iteration 24/25 | Loss: 0.00065002
Iteration 25/25 | Loss: 0.00065376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39783490
Iteration 2/25 | Loss: 0.00052064
Iteration 3/25 | Loss: 0.00052063
Iteration 4/25 | Loss: 0.00052063
Iteration 5/25 | Loss: 0.00050352
Iteration 6/25 | Loss: 0.00050352
Iteration 7/25 | Loss: 0.00050351
Iteration 8/25 | Loss: 0.00050351
Iteration 9/25 | Loss: 0.00050351
Iteration 10/25 | Loss: 0.00050351
Iteration 11/25 | Loss: 0.00050351
Iteration 12/25 | Loss: 0.00050351
Iteration 13/25 | Loss: 0.00050351
Iteration 14/25 | Loss: 0.00050351
Iteration 15/25 | Loss: 0.00050351
Iteration 16/25 | Loss: 0.00050351
Iteration 17/25 | Loss: 0.00050351
Iteration 18/25 | Loss: 0.00050351
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005035140784457326, 0.0005035140784457326, 0.0005035140784457326, 0.0005035140784457326, 0.0005035140784457326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005035140784457326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050351
Iteration 2/1000 | Loss: 0.00025709
Iteration 3/1000 | Loss: 0.00036408
Iteration 4/1000 | Loss: 0.00032182
Iteration 5/1000 | Loss: 0.00040798
Iteration 6/1000 | Loss: 0.00032164
Iteration 7/1000 | Loss: 0.00042953
Iteration 8/1000 | Loss: 0.00037843
Iteration 9/1000 | Loss: 0.00043620
Iteration 10/1000 | Loss: 0.00033159
Iteration 11/1000 | Loss: 0.00023438
Iteration 12/1000 | Loss: 0.00023437
Iteration 13/1000 | Loss: 0.00031139
Iteration 14/1000 | Loss: 0.00032533
Iteration 15/1000 | Loss: 0.00022489
Iteration 16/1000 | Loss: 0.00005397
Iteration 17/1000 | Loss: 0.00005898
Iteration 18/1000 | Loss: 0.00006261
Iteration 19/1000 | Loss: 0.00024673
Iteration 20/1000 | Loss: 0.00092510
Iteration 21/1000 | Loss: 0.00117086
Iteration 22/1000 | Loss: 0.00056652
Iteration 23/1000 | Loss: 0.00025620
Iteration 24/1000 | Loss: 0.00005793
Iteration 25/1000 | Loss: 0.00006980
Iteration 26/1000 | Loss: 0.00003835
Iteration 27/1000 | Loss: 0.00092618
Iteration 28/1000 | Loss: 0.00066160
Iteration 29/1000 | Loss: 0.00072608
Iteration 30/1000 | Loss: 0.00062068
Iteration 31/1000 | Loss: 0.00028006
Iteration 32/1000 | Loss: 0.00061437
Iteration 33/1000 | Loss: 0.00036410
Iteration 34/1000 | Loss: 0.00032988
Iteration 35/1000 | Loss: 0.00016981
Iteration 36/1000 | Loss: 0.00053567
Iteration 37/1000 | Loss: 0.00039788
Iteration 38/1000 | Loss: 0.00080233
Iteration 39/1000 | Loss: 0.00015411
Iteration 40/1000 | Loss: 0.00004501
Iteration 41/1000 | Loss: 0.00007773
Iteration 42/1000 | Loss: 0.00003443
Iteration 43/1000 | Loss: 0.00010795
Iteration 44/1000 | Loss: 0.00003284
Iteration 45/1000 | Loss: 0.00047283
Iteration 46/1000 | Loss: 0.00051712
Iteration 47/1000 | Loss: 0.00034526
Iteration 48/1000 | Loss: 0.00031384
Iteration 49/1000 | Loss: 0.00033127
Iteration 50/1000 | Loss: 0.00029353
Iteration 51/1000 | Loss: 0.00030433
Iteration 52/1000 | Loss: 0.00022976
Iteration 53/1000 | Loss: 0.00026495
Iteration 54/1000 | Loss: 0.00020067
Iteration 55/1000 | Loss: 0.00026153
Iteration 56/1000 | Loss: 0.00027925
Iteration 57/1000 | Loss: 0.00024090
Iteration 58/1000 | Loss: 0.00030758
Iteration 59/1000 | Loss: 0.00167241
Iteration 60/1000 | Loss: 0.00033821
Iteration 61/1000 | Loss: 0.00026744
Iteration 62/1000 | Loss: 0.00009316
Iteration 63/1000 | Loss: 0.00014057
Iteration 64/1000 | Loss: 0.00003220
Iteration 65/1000 | Loss: 0.00003255
Iteration 66/1000 | Loss: 0.00005013
Iteration 67/1000 | Loss: 0.00024827
Iteration 68/1000 | Loss: 0.00005297
Iteration 69/1000 | Loss: 0.00004214
Iteration 70/1000 | Loss: 0.00002585
Iteration 71/1000 | Loss: 0.00004322
Iteration 72/1000 | Loss: 0.00002361
Iteration 73/1000 | Loss: 0.00028563
Iteration 74/1000 | Loss: 0.00022821
Iteration 75/1000 | Loss: 0.00017002
Iteration 76/1000 | Loss: 0.00004377
Iteration 77/1000 | Loss: 0.00018560
Iteration 78/1000 | Loss: 0.00006665
Iteration 79/1000 | Loss: 0.00004068
Iteration 80/1000 | Loss: 0.00004256
Iteration 81/1000 | Loss: 0.00002623
Iteration 82/1000 | Loss: 0.00011543
Iteration 83/1000 | Loss: 0.00008164
Iteration 84/1000 | Loss: 0.00010427
Iteration 85/1000 | Loss: 0.00004881
Iteration 86/1000 | Loss: 0.00004319
Iteration 87/1000 | Loss: 0.00016136
Iteration 88/1000 | Loss: 0.00065203
Iteration 89/1000 | Loss: 0.00026983
Iteration 90/1000 | Loss: 0.00005587
Iteration 91/1000 | Loss: 0.00013950
Iteration 92/1000 | Loss: 0.00016455
Iteration 93/1000 | Loss: 0.00028436
Iteration 94/1000 | Loss: 0.00016824
Iteration 95/1000 | Loss: 0.00020259
Iteration 96/1000 | Loss: 0.00015317
Iteration 97/1000 | Loss: 0.00004150
Iteration 98/1000 | Loss: 0.00012238
Iteration 99/1000 | Loss: 0.00003597
Iteration 100/1000 | Loss: 0.00002953
Iteration 101/1000 | Loss: 0.00003741
Iteration 102/1000 | Loss: 0.00005493
Iteration 103/1000 | Loss: 0.00004907
Iteration 104/1000 | Loss: 0.00009109
Iteration 105/1000 | Loss: 0.00002500
Iteration 106/1000 | Loss: 0.00003070
Iteration 107/1000 | Loss: 0.00003691
Iteration 108/1000 | Loss: 0.00001891
Iteration 109/1000 | Loss: 0.00010387
Iteration 110/1000 | Loss: 0.00003151
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00002476
Iteration 114/1000 | Loss: 0.00002240
Iteration 115/1000 | Loss: 0.00004829
Iteration 116/1000 | Loss: 0.00001887
Iteration 117/1000 | Loss: 0.00001701
Iteration 118/1000 | Loss: 0.00001692
Iteration 119/1000 | Loss: 0.00003545
Iteration 120/1000 | Loss: 0.00001752
Iteration 121/1000 | Loss: 0.00001675
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001631
Iteration 124/1000 | Loss: 0.00001630
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00003864
Iteration 128/1000 | Loss: 0.00002997
Iteration 129/1000 | Loss: 0.00001836
Iteration 130/1000 | Loss: 0.00001612
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001607
Iteration 133/1000 | Loss: 0.00001607
Iteration 134/1000 | Loss: 0.00001607
Iteration 135/1000 | Loss: 0.00001607
Iteration 136/1000 | Loss: 0.00001607
Iteration 137/1000 | Loss: 0.00001607
Iteration 138/1000 | Loss: 0.00001607
Iteration 139/1000 | Loss: 0.00001607
Iteration 140/1000 | Loss: 0.00001607
Iteration 141/1000 | Loss: 0.00001607
Iteration 142/1000 | Loss: 0.00001607
Iteration 143/1000 | Loss: 0.00001606
Iteration 144/1000 | Loss: 0.00001606
Iteration 145/1000 | Loss: 0.00001606
Iteration 146/1000 | Loss: 0.00001606
Iteration 147/1000 | Loss: 0.00001606
Iteration 148/1000 | Loss: 0.00001605
Iteration 149/1000 | Loss: 0.00001605
Iteration 150/1000 | Loss: 0.00001605
Iteration 151/1000 | Loss: 0.00001605
Iteration 152/1000 | Loss: 0.00001604
Iteration 153/1000 | Loss: 0.00001604
Iteration 154/1000 | Loss: 0.00001604
Iteration 155/1000 | Loss: 0.00001604
Iteration 156/1000 | Loss: 0.00001604
Iteration 157/1000 | Loss: 0.00001603
Iteration 158/1000 | Loss: 0.00001603
Iteration 159/1000 | Loss: 0.00001603
Iteration 160/1000 | Loss: 0.00001603
Iteration 161/1000 | Loss: 0.00001602
Iteration 162/1000 | Loss: 0.00001602
Iteration 163/1000 | Loss: 0.00001602
Iteration 164/1000 | Loss: 0.00001601
Iteration 165/1000 | Loss: 0.00001601
Iteration 166/1000 | Loss: 0.00001600
Iteration 167/1000 | Loss: 0.00001599
Iteration 168/1000 | Loss: 0.00001599
Iteration 169/1000 | Loss: 0.00001598
Iteration 170/1000 | Loss: 0.00001598
Iteration 171/1000 | Loss: 0.00001597
Iteration 172/1000 | Loss: 0.00001597
Iteration 173/1000 | Loss: 0.00001596
Iteration 174/1000 | Loss: 0.00001596
Iteration 175/1000 | Loss: 0.00001596
Iteration 176/1000 | Loss: 0.00001596
Iteration 177/1000 | Loss: 0.00001596
Iteration 178/1000 | Loss: 0.00001596
Iteration 179/1000 | Loss: 0.00001596
Iteration 180/1000 | Loss: 0.00001596
Iteration 181/1000 | Loss: 0.00001596
Iteration 182/1000 | Loss: 0.00001595
Iteration 183/1000 | Loss: 0.00001595
Iteration 184/1000 | Loss: 0.00001595
Iteration 185/1000 | Loss: 0.00001595
Iteration 186/1000 | Loss: 0.00001595
Iteration 187/1000 | Loss: 0.00001595
Iteration 188/1000 | Loss: 0.00001595
Iteration 189/1000 | Loss: 0.00001595
Iteration 190/1000 | Loss: 0.00003077
Iteration 191/1000 | Loss: 0.00001594
Iteration 192/1000 | Loss: 0.00001591
Iteration 193/1000 | Loss: 0.00001591
Iteration 194/1000 | Loss: 0.00001591
Iteration 195/1000 | Loss: 0.00001591
Iteration 196/1000 | Loss: 0.00001591
Iteration 197/1000 | Loss: 0.00001591
Iteration 198/1000 | Loss: 0.00001591
Iteration 199/1000 | Loss: 0.00001591
Iteration 200/1000 | Loss: 0.00001591
Iteration 201/1000 | Loss: 0.00001591
Iteration 202/1000 | Loss: 0.00001590
Iteration 203/1000 | Loss: 0.00001590
Iteration 204/1000 | Loss: 0.00001590
Iteration 205/1000 | Loss: 0.00001590
Iteration 206/1000 | Loss: 0.00001590
Iteration 207/1000 | Loss: 0.00001590
Iteration 208/1000 | Loss: 0.00001590
Iteration 209/1000 | Loss: 0.00001590
Iteration 210/1000 | Loss: 0.00001590
Iteration 211/1000 | Loss: 0.00001590
Iteration 212/1000 | Loss: 0.00001590
Iteration 213/1000 | Loss: 0.00001590
Iteration 214/1000 | Loss: 0.00001589
Iteration 215/1000 | Loss: 0.00001589
Iteration 216/1000 | Loss: 0.00001589
Iteration 217/1000 | Loss: 0.00001589
Iteration 218/1000 | Loss: 0.00001589
Iteration 219/1000 | Loss: 0.00001589
Iteration 220/1000 | Loss: 0.00001589
Iteration 221/1000 | Loss: 0.00001589
Iteration 222/1000 | Loss: 0.00001589
Iteration 223/1000 | Loss: 0.00001589
Iteration 224/1000 | Loss: 0.00001589
Iteration 225/1000 | Loss: 0.00001589
Iteration 226/1000 | Loss: 0.00001589
Iteration 227/1000 | Loss: 0.00001589
Iteration 228/1000 | Loss: 0.00001589
Iteration 229/1000 | Loss: 0.00001589
Iteration 230/1000 | Loss: 0.00001589
Iteration 231/1000 | Loss: 0.00001589
Iteration 232/1000 | Loss: 0.00001589
Iteration 233/1000 | Loss: 0.00001589
Iteration 234/1000 | Loss: 0.00001589
Iteration 235/1000 | Loss: 0.00001589
Iteration 236/1000 | Loss: 0.00001589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.5890744180069305e-05, 1.5890744180069305e-05, 1.5890744180069305e-05, 1.5890744180069305e-05, 1.5890744180069305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5890744180069305e-05

Optimization complete. Final v2v error: 3.3521101474761963 mm

Highest mean error: 9.076987266540527 mm for frame 91

Lowest mean error: 2.8915820121765137 mm for frame 94

Saving results

Total time: 1268.0117626190186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0015
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596644
Iteration 2/25 | Loss: 0.00071086
Iteration 3/25 | Loss: 0.00059630
Iteration 4/25 | Loss: 0.00058415
Iteration 5/25 | Loss: 0.00058014
Iteration 6/25 | Loss: 0.00057940
Iteration 7/25 | Loss: 0.00057940
Iteration 8/25 | Loss: 0.00057940
Iteration 9/25 | Loss: 0.00057940
Iteration 10/25 | Loss: 0.00057940
Iteration 11/25 | Loss: 0.00057940
Iteration 12/25 | Loss: 0.00057940
Iteration 13/25 | Loss: 0.00057940
Iteration 14/25 | Loss: 0.00057940
Iteration 15/25 | Loss: 0.00057940
Iteration 16/25 | Loss: 0.00057940
Iteration 17/25 | Loss: 0.00057940
Iteration 18/25 | Loss: 0.00057940
Iteration 19/25 | Loss: 0.00057940
Iteration 20/25 | Loss: 0.00057940
Iteration 21/25 | Loss: 0.00057940
Iteration 22/25 | Loss: 0.00057940
Iteration 23/25 | Loss: 0.00057940
Iteration 24/25 | Loss: 0.00057940
Iteration 25/25 | Loss: 0.00057940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.28860569
Iteration 2/25 | Loss: 0.00023473
Iteration 3/25 | Loss: 0.00023464
Iteration 4/25 | Loss: 0.00023464
Iteration 5/25 | Loss: 0.00023464
Iteration 6/25 | Loss: 0.00023464
Iteration 7/25 | Loss: 0.00023464
Iteration 8/25 | Loss: 0.00023464
Iteration 9/25 | Loss: 0.00023464
Iteration 10/25 | Loss: 0.00023464
Iteration 11/25 | Loss: 0.00023464
Iteration 12/25 | Loss: 0.00023464
Iteration 13/25 | Loss: 0.00023464
Iteration 14/25 | Loss: 0.00023464
Iteration 15/25 | Loss: 0.00023464
Iteration 16/25 | Loss: 0.00023464
Iteration 17/25 | Loss: 0.00023464
Iteration 18/25 | Loss: 0.00023464
Iteration 19/25 | Loss: 0.00023464
Iteration 20/25 | Loss: 0.00023464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00023463704565074295, 0.00023463704565074295, 0.00023463704565074295, 0.00023463704565074295, 0.00023463704565074295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023463704565074295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023464
Iteration 2/1000 | Loss: 0.00002198
Iteration 3/1000 | Loss: 0.00001686
Iteration 4/1000 | Loss: 0.00001536
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001410
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001354
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001310
Iteration 12/1000 | Loss: 0.00001310
Iteration 13/1000 | Loss: 0.00001309
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001302
Iteration 16/1000 | Loss: 0.00001302
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001298
Iteration 23/1000 | Loss: 0.00001298
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001298
Iteration 26/1000 | Loss: 0.00001297
Iteration 27/1000 | Loss: 0.00001297
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001295
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001291
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001288
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001287
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001286
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001284
Iteration 69/1000 | Loss: 0.00001284
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001282
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001280
Iteration 85/1000 | Loss: 0.00001280
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001280
Iteration 88/1000 | Loss: 0.00001280
Iteration 89/1000 | Loss: 0.00001279
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001279
Iteration 93/1000 | Loss: 0.00001278
Iteration 94/1000 | Loss: 0.00001278
Iteration 95/1000 | Loss: 0.00001278
Iteration 96/1000 | Loss: 0.00001277
Iteration 97/1000 | Loss: 0.00001277
Iteration 98/1000 | Loss: 0.00001277
Iteration 99/1000 | Loss: 0.00001277
Iteration 100/1000 | Loss: 0.00001276
Iteration 101/1000 | Loss: 0.00001276
Iteration 102/1000 | Loss: 0.00001276
Iteration 103/1000 | Loss: 0.00001276
Iteration 104/1000 | Loss: 0.00001276
Iteration 105/1000 | Loss: 0.00001276
Iteration 106/1000 | Loss: 0.00001276
Iteration 107/1000 | Loss: 0.00001276
Iteration 108/1000 | Loss: 0.00001276
Iteration 109/1000 | Loss: 0.00001276
Iteration 110/1000 | Loss: 0.00001276
Iteration 111/1000 | Loss: 0.00001276
Iteration 112/1000 | Loss: 0.00001276
Iteration 113/1000 | Loss: 0.00001275
Iteration 114/1000 | Loss: 0.00001275
Iteration 115/1000 | Loss: 0.00001275
Iteration 116/1000 | Loss: 0.00001275
Iteration 117/1000 | Loss: 0.00001275
Iteration 118/1000 | Loss: 0.00001275
Iteration 119/1000 | Loss: 0.00001275
Iteration 120/1000 | Loss: 0.00001275
Iteration 121/1000 | Loss: 0.00001275
Iteration 122/1000 | Loss: 0.00001275
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001274
Iteration 125/1000 | Loss: 0.00001274
Iteration 126/1000 | Loss: 0.00001274
Iteration 127/1000 | Loss: 0.00001274
Iteration 128/1000 | Loss: 0.00001274
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001274
Iteration 131/1000 | Loss: 0.00001274
Iteration 132/1000 | Loss: 0.00001274
Iteration 133/1000 | Loss: 0.00001274
Iteration 134/1000 | Loss: 0.00001274
Iteration 135/1000 | Loss: 0.00001274
Iteration 136/1000 | Loss: 0.00001274
Iteration 137/1000 | Loss: 0.00001274
Iteration 138/1000 | Loss: 0.00001274
Iteration 139/1000 | Loss: 0.00001274
Iteration 140/1000 | Loss: 0.00001274
Iteration 141/1000 | Loss: 0.00001274
Iteration 142/1000 | Loss: 0.00001274
Iteration 143/1000 | Loss: 0.00001274
Iteration 144/1000 | Loss: 0.00001274
Iteration 145/1000 | Loss: 0.00001274
Iteration 146/1000 | Loss: 0.00001274
Iteration 147/1000 | Loss: 0.00001274
Iteration 148/1000 | Loss: 0.00001274
Iteration 149/1000 | Loss: 0.00001274
Iteration 150/1000 | Loss: 0.00001274
Iteration 151/1000 | Loss: 0.00001274
Iteration 152/1000 | Loss: 0.00001274
Iteration 153/1000 | Loss: 0.00001274
Iteration 154/1000 | Loss: 0.00001274
Iteration 155/1000 | Loss: 0.00001274
Iteration 156/1000 | Loss: 0.00001274
Iteration 157/1000 | Loss: 0.00001274
Iteration 158/1000 | Loss: 0.00001274
Iteration 159/1000 | Loss: 0.00001274
Iteration 160/1000 | Loss: 0.00001274
Iteration 161/1000 | Loss: 0.00001274
Iteration 162/1000 | Loss: 0.00001274
Iteration 163/1000 | Loss: 0.00001274
Iteration 164/1000 | Loss: 0.00001274
Iteration 165/1000 | Loss: 0.00001274
Iteration 166/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.2742317267111503e-05, 1.2742317267111503e-05, 1.2742317267111503e-05, 1.2742317267111503e-05, 1.2742317267111503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2742317267111503e-05

Optimization complete. Final v2v error: 3.0566935539245605 mm

Highest mean error: 3.4634532928466797 mm for frame 150

Lowest mean error: 2.711165428161621 mm for frame 82

Saving results

Total time: 309.0408868789673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0010
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407020
Iteration 2/25 | Loss: 0.00089830
Iteration 3/25 | Loss: 0.00068797
Iteration 4/25 | Loss: 0.00064088
Iteration 5/25 | Loss: 0.00063141
Iteration 6/25 | Loss: 0.00062896
Iteration 7/25 | Loss: 0.00062876
Iteration 8/25 | Loss: 0.00062876
Iteration 9/25 | Loss: 0.00062876
Iteration 10/25 | Loss: 0.00062876
Iteration 11/25 | Loss: 0.00062876
Iteration 12/25 | Loss: 0.00062876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006287628784775734, 0.0006287628784775734, 0.0006287628784775734, 0.0006287628784775734, 0.0006287628784775734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006287628784775734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28714907
Iteration 2/25 | Loss: 0.00030818
Iteration 3/25 | Loss: 0.00030818
Iteration 4/25 | Loss: 0.00030818
Iteration 5/25 | Loss: 0.00030818
Iteration 6/25 | Loss: 0.00030818
Iteration 7/25 | Loss: 0.00030818
Iteration 8/25 | Loss: 0.00030818
Iteration 9/25 | Loss: 0.00030818
Iteration 10/25 | Loss: 0.00030818
Iteration 11/25 | Loss: 0.00030818
Iteration 12/25 | Loss: 0.00030818
Iteration 13/25 | Loss: 0.00030818
Iteration 14/25 | Loss: 0.00030818
Iteration 15/25 | Loss: 0.00030818
Iteration 16/25 | Loss: 0.00030818
Iteration 17/25 | Loss: 0.00030818
Iteration 18/25 | Loss: 0.00030818
Iteration 19/25 | Loss: 0.00030818
Iteration 20/25 | Loss: 0.00030818
Iteration 21/25 | Loss: 0.00030818
Iteration 22/25 | Loss: 0.00030818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00030817760853096843, 0.00030817760853096843, 0.00030817760853096843, 0.00030817760853096843, 0.00030817760853096843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030817760853096843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030818
Iteration 2/1000 | Loss: 0.00003165
Iteration 3/1000 | Loss: 0.00002430
Iteration 4/1000 | Loss: 0.00002217
Iteration 5/1000 | Loss: 0.00002052
Iteration 6/1000 | Loss: 0.00001913
Iteration 7/1000 | Loss: 0.00001858
Iteration 8/1000 | Loss: 0.00001808
Iteration 9/1000 | Loss: 0.00001772
Iteration 10/1000 | Loss: 0.00001749
Iteration 11/1000 | Loss: 0.00001726
Iteration 12/1000 | Loss: 0.00001724
Iteration 13/1000 | Loss: 0.00001724
Iteration 14/1000 | Loss: 0.00001721
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001704
Iteration 18/1000 | Loss: 0.00001701
Iteration 19/1000 | Loss: 0.00001700
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001698
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001697
Iteration 25/1000 | Loss: 0.00001697
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001696
Iteration 29/1000 | Loss: 0.00001696
Iteration 30/1000 | Loss: 0.00001696
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001693
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001690
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001689
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001687
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001686
Iteration 51/1000 | Loss: 0.00001685
Iteration 52/1000 | Loss: 0.00001685
Iteration 53/1000 | Loss: 0.00001685
Iteration 54/1000 | Loss: 0.00001685
Iteration 55/1000 | Loss: 0.00001685
Iteration 56/1000 | Loss: 0.00001685
Iteration 57/1000 | Loss: 0.00001685
Iteration 58/1000 | Loss: 0.00001685
Iteration 59/1000 | Loss: 0.00001685
Iteration 60/1000 | Loss: 0.00001685
Iteration 61/1000 | Loss: 0.00001685
Iteration 62/1000 | Loss: 0.00001685
Iteration 63/1000 | Loss: 0.00001685
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.6845326172187924e-05, 1.6845326172187924e-05, 1.6845326172187924e-05, 1.6845326172187924e-05, 1.6845326172187924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6845326172187924e-05

Optimization complete. Final v2v error: 3.4694530963897705 mm

Highest mean error: 3.9886491298675537 mm for frame 184

Lowest mean error: 2.981639862060547 mm for frame 233

Saving results

Total time: 349.72474002838135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1030
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974782
Iteration 2/25 | Loss: 0.00162012
Iteration 3/25 | Loss: 0.00138075
Iteration 4/25 | Loss: 0.00139465
Iteration 5/25 | Loss: 0.00138380
Iteration 6/25 | Loss: 0.00134231
Iteration 7/25 | Loss: 0.00131783
Iteration 8/25 | Loss: 0.00130091
Iteration 9/25 | Loss: 0.00129812
Iteration 10/25 | Loss: 0.00128878
Iteration 11/25 | Loss: 0.00128382
Iteration 12/25 | Loss: 0.00127657
Iteration 13/25 | Loss: 0.00127267
Iteration 14/25 | Loss: 0.00126964
Iteration 15/25 | Loss: 0.00126850
Iteration 16/25 | Loss: 0.00126995
Iteration 17/25 | Loss: 0.00126738
Iteration 18/25 | Loss: 0.00126532
Iteration 19/25 | Loss: 0.00126482
Iteration 20/25 | Loss: 0.00126473
Iteration 21/25 | Loss: 0.00126473
Iteration 22/25 | Loss: 0.00126473
Iteration 23/25 | Loss: 0.00126473
Iteration 24/25 | Loss: 0.00126473
Iteration 25/25 | Loss: 0.00126472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18436873
Iteration 2/25 | Loss: 0.00233949
Iteration 3/25 | Loss: 0.00233945
Iteration 4/25 | Loss: 0.00233945
Iteration 5/25 | Loss: 0.00233945
Iteration 6/25 | Loss: 0.00233945
Iteration 7/25 | Loss: 0.00233945
Iteration 8/25 | Loss: 0.00233945
Iteration 9/25 | Loss: 0.00233945
Iteration 10/25 | Loss: 0.00233945
Iteration 11/25 | Loss: 0.00233945
Iteration 12/25 | Loss: 0.00233945
Iteration 13/25 | Loss: 0.00233945
Iteration 14/25 | Loss: 0.00233945
Iteration 15/25 | Loss: 0.00233945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002339452039450407, 0.002339452039450407, 0.002339452039450407, 0.002339452039450407, 0.002339452039450407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002339452039450407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233945
Iteration 2/1000 | Loss: 0.00006374
Iteration 3/1000 | Loss: 0.00004060
Iteration 4/1000 | Loss: 0.00003268
Iteration 5/1000 | Loss: 0.00002951
Iteration 6/1000 | Loss: 0.00002811
Iteration 7/1000 | Loss: 0.00002681
Iteration 8/1000 | Loss: 0.00002597
Iteration 9/1000 | Loss: 0.00002538
Iteration 10/1000 | Loss: 0.00002484
Iteration 11/1000 | Loss: 0.00002442
Iteration 12/1000 | Loss: 0.00002409
Iteration 13/1000 | Loss: 0.00137395
Iteration 14/1000 | Loss: 0.00002924
Iteration 15/1000 | Loss: 0.00002396
Iteration 16/1000 | Loss: 0.00002165
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002010
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001978
Iteration 21/1000 | Loss: 0.00001969
Iteration 22/1000 | Loss: 0.00001957
Iteration 23/1000 | Loss: 0.00001955
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00001950
Iteration 26/1000 | Loss: 0.00001950
Iteration 27/1000 | Loss: 0.00001949
Iteration 28/1000 | Loss: 0.00001949
Iteration 29/1000 | Loss: 0.00001947
Iteration 30/1000 | Loss: 0.00001947
Iteration 31/1000 | Loss: 0.00001947
Iteration 32/1000 | Loss: 0.00001947
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001946
Iteration 38/1000 | Loss: 0.00001946
Iteration 39/1000 | Loss: 0.00001946
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001944
Iteration 43/1000 | Loss: 0.00001944
Iteration 44/1000 | Loss: 0.00001943
Iteration 45/1000 | Loss: 0.00001943
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00001942
Iteration 48/1000 | Loss: 0.00001940
Iteration 49/1000 | Loss: 0.00001940
Iteration 50/1000 | Loss: 0.00001940
Iteration 51/1000 | Loss: 0.00001940
Iteration 52/1000 | Loss: 0.00001939
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001938
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001935
Iteration 61/1000 | Loss: 0.00001935
Iteration 62/1000 | Loss: 0.00001935
Iteration 63/1000 | Loss: 0.00001933
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001933
Iteration 66/1000 | Loss: 0.00001933
Iteration 67/1000 | Loss: 0.00001933
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001930
Iteration 78/1000 | Loss: 0.00001930
Iteration 79/1000 | Loss: 0.00001930
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001927
Iteration 89/1000 | Loss: 0.00001927
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001926
Iteration 93/1000 | Loss: 0.00001926
Iteration 94/1000 | Loss: 0.00001926
Iteration 95/1000 | Loss: 0.00001925
Iteration 96/1000 | Loss: 0.00001925
Iteration 97/1000 | Loss: 0.00001925
Iteration 98/1000 | Loss: 0.00001925
Iteration 99/1000 | Loss: 0.00001924
Iteration 100/1000 | Loss: 0.00001924
Iteration 101/1000 | Loss: 0.00001924
Iteration 102/1000 | Loss: 0.00001924
Iteration 103/1000 | Loss: 0.00001924
Iteration 104/1000 | Loss: 0.00001923
Iteration 105/1000 | Loss: 0.00001923
Iteration 106/1000 | Loss: 0.00001923
Iteration 107/1000 | Loss: 0.00001922
Iteration 108/1000 | Loss: 0.00001922
Iteration 109/1000 | Loss: 0.00001921
Iteration 110/1000 | Loss: 0.00001921
Iteration 111/1000 | Loss: 0.00001921
Iteration 112/1000 | Loss: 0.00001920
Iteration 113/1000 | Loss: 0.00001920
Iteration 114/1000 | Loss: 0.00001920
Iteration 115/1000 | Loss: 0.00001919
Iteration 116/1000 | Loss: 0.00001919
Iteration 117/1000 | Loss: 0.00001919
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001917
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001916
Iteration 129/1000 | Loss: 0.00001916
Iteration 130/1000 | Loss: 0.00001916
Iteration 131/1000 | Loss: 0.00001916
Iteration 132/1000 | Loss: 0.00001916
Iteration 133/1000 | Loss: 0.00001916
Iteration 134/1000 | Loss: 0.00001916
Iteration 135/1000 | Loss: 0.00001916
Iteration 136/1000 | Loss: 0.00001916
Iteration 137/1000 | Loss: 0.00001916
Iteration 138/1000 | Loss: 0.00001916
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00001915
Iteration 141/1000 | Loss: 0.00001915
Iteration 142/1000 | Loss: 0.00001915
Iteration 143/1000 | Loss: 0.00001915
Iteration 144/1000 | Loss: 0.00001915
Iteration 145/1000 | Loss: 0.00001915
Iteration 146/1000 | Loss: 0.00001914
Iteration 147/1000 | Loss: 0.00001914
Iteration 148/1000 | Loss: 0.00001914
Iteration 149/1000 | Loss: 0.00001914
Iteration 150/1000 | Loss: 0.00001914
Iteration 151/1000 | Loss: 0.00001914
Iteration 152/1000 | Loss: 0.00001914
Iteration 153/1000 | Loss: 0.00001914
Iteration 154/1000 | Loss: 0.00001914
Iteration 155/1000 | Loss: 0.00001914
Iteration 156/1000 | Loss: 0.00001914
Iteration 157/1000 | Loss: 0.00001914
Iteration 158/1000 | Loss: 0.00001913
Iteration 159/1000 | Loss: 0.00001913
Iteration 160/1000 | Loss: 0.00001913
Iteration 161/1000 | Loss: 0.00001913
Iteration 162/1000 | Loss: 0.00001913
Iteration 163/1000 | Loss: 0.00001913
Iteration 164/1000 | Loss: 0.00001913
Iteration 165/1000 | Loss: 0.00001912
Iteration 166/1000 | Loss: 0.00001912
Iteration 167/1000 | Loss: 0.00001912
Iteration 168/1000 | Loss: 0.00001911
Iteration 169/1000 | Loss: 0.00001911
Iteration 170/1000 | Loss: 0.00001911
Iteration 171/1000 | Loss: 0.00001910
Iteration 172/1000 | Loss: 0.00001910
Iteration 173/1000 | Loss: 0.00001910
Iteration 174/1000 | Loss: 0.00001909
Iteration 175/1000 | Loss: 0.00001909
Iteration 176/1000 | Loss: 0.00001909
Iteration 177/1000 | Loss: 0.00001909
Iteration 178/1000 | Loss: 0.00001909
Iteration 179/1000 | Loss: 0.00001909
Iteration 180/1000 | Loss: 0.00001909
Iteration 181/1000 | Loss: 0.00001909
Iteration 182/1000 | Loss: 0.00001909
Iteration 183/1000 | Loss: 0.00001909
Iteration 184/1000 | Loss: 0.00001909
Iteration 185/1000 | Loss: 0.00001908
Iteration 186/1000 | Loss: 0.00001908
Iteration 187/1000 | Loss: 0.00001908
Iteration 188/1000 | Loss: 0.00001908
Iteration 189/1000 | Loss: 0.00001908
Iteration 190/1000 | Loss: 0.00001908
Iteration 191/1000 | Loss: 0.00001908
Iteration 192/1000 | Loss: 0.00001908
Iteration 193/1000 | Loss: 0.00001908
Iteration 194/1000 | Loss: 0.00001908
Iteration 195/1000 | Loss: 0.00001907
Iteration 196/1000 | Loss: 0.00001907
Iteration 197/1000 | Loss: 0.00001907
Iteration 198/1000 | Loss: 0.00001907
Iteration 199/1000 | Loss: 0.00001907
Iteration 200/1000 | Loss: 0.00001906
Iteration 201/1000 | Loss: 0.00001906
Iteration 202/1000 | Loss: 0.00001906
Iteration 203/1000 | Loss: 0.00001906
Iteration 204/1000 | Loss: 0.00001906
Iteration 205/1000 | Loss: 0.00001906
Iteration 206/1000 | Loss: 0.00001905
Iteration 207/1000 | Loss: 0.00001905
Iteration 208/1000 | Loss: 0.00001905
Iteration 209/1000 | Loss: 0.00001905
Iteration 210/1000 | Loss: 0.00001905
Iteration 211/1000 | Loss: 0.00001905
Iteration 212/1000 | Loss: 0.00001904
Iteration 213/1000 | Loss: 0.00001904
Iteration 214/1000 | Loss: 0.00001904
Iteration 215/1000 | Loss: 0.00001903
Iteration 216/1000 | Loss: 0.00001903
Iteration 217/1000 | Loss: 0.00001903
Iteration 218/1000 | Loss: 0.00001903
Iteration 219/1000 | Loss: 0.00001902
Iteration 220/1000 | Loss: 0.00001902
Iteration 221/1000 | Loss: 0.00001902
Iteration 222/1000 | Loss: 0.00001902
Iteration 223/1000 | Loss: 0.00001902
Iteration 224/1000 | Loss: 0.00001902
Iteration 225/1000 | Loss: 0.00001901
Iteration 226/1000 | Loss: 0.00001901
Iteration 227/1000 | Loss: 0.00001901
Iteration 228/1000 | Loss: 0.00001901
Iteration 229/1000 | Loss: 0.00001901
Iteration 230/1000 | Loss: 0.00001901
Iteration 231/1000 | Loss: 0.00001901
Iteration 232/1000 | Loss: 0.00001901
Iteration 233/1000 | Loss: 0.00001901
Iteration 234/1000 | Loss: 0.00001901
Iteration 235/1000 | Loss: 0.00001901
Iteration 236/1000 | Loss: 0.00001900
Iteration 237/1000 | Loss: 0.00001900
Iteration 238/1000 | Loss: 0.00001900
Iteration 239/1000 | Loss: 0.00001900
Iteration 240/1000 | Loss: 0.00001900
Iteration 241/1000 | Loss: 0.00001900
Iteration 242/1000 | Loss: 0.00001900
Iteration 243/1000 | Loss: 0.00001899
Iteration 244/1000 | Loss: 0.00001899
Iteration 245/1000 | Loss: 0.00001899
Iteration 246/1000 | Loss: 0.00001899
Iteration 247/1000 | Loss: 0.00001899
Iteration 248/1000 | Loss: 0.00001899
Iteration 249/1000 | Loss: 0.00001899
Iteration 250/1000 | Loss: 0.00001899
Iteration 251/1000 | Loss: 0.00001899
Iteration 252/1000 | Loss: 0.00001899
Iteration 253/1000 | Loss: 0.00001898
Iteration 254/1000 | Loss: 0.00001898
Iteration 255/1000 | Loss: 0.00001898
Iteration 256/1000 | Loss: 0.00001898
Iteration 257/1000 | Loss: 0.00001898
Iteration 258/1000 | Loss: 0.00001898
Iteration 259/1000 | Loss: 0.00001898
Iteration 260/1000 | Loss: 0.00001898
Iteration 261/1000 | Loss: 0.00001898
Iteration 262/1000 | Loss: 0.00001898
Iteration 263/1000 | Loss: 0.00001897
Iteration 264/1000 | Loss: 0.00001897
Iteration 265/1000 | Loss: 0.00001897
Iteration 266/1000 | Loss: 0.00001897
Iteration 267/1000 | Loss: 0.00001897
Iteration 268/1000 | Loss: 0.00001897
Iteration 269/1000 | Loss: 0.00001897
Iteration 270/1000 | Loss: 0.00001897
Iteration 271/1000 | Loss: 0.00001897
Iteration 272/1000 | Loss: 0.00001897
Iteration 273/1000 | Loss: 0.00001897
Iteration 274/1000 | Loss: 0.00001896
Iteration 275/1000 | Loss: 0.00001896
Iteration 276/1000 | Loss: 0.00001896
Iteration 277/1000 | Loss: 0.00001896
Iteration 278/1000 | Loss: 0.00001896
Iteration 279/1000 | Loss: 0.00001896
Iteration 280/1000 | Loss: 0.00001896
Iteration 281/1000 | Loss: 0.00001896
Iteration 282/1000 | Loss: 0.00001896
Iteration 283/1000 | Loss: 0.00001896
Iteration 284/1000 | Loss: 0.00001896
Iteration 285/1000 | Loss: 0.00001896
Iteration 286/1000 | Loss: 0.00001896
Iteration 287/1000 | Loss: 0.00001896
Iteration 288/1000 | Loss: 0.00001895
Iteration 289/1000 | Loss: 0.00001895
Iteration 290/1000 | Loss: 0.00001895
Iteration 291/1000 | Loss: 0.00001895
Iteration 292/1000 | Loss: 0.00001895
Iteration 293/1000 | Loss: 0.00001895
Iteration 294/1000 | Loss: 0.00001895
Iteration 295/1000 | Loss: 0.00001895
Iteration 296/1000 | Loss: 0.00001895
Iteration 297/1000 | Loss: 0.00001895
Iteration 298/1000 | Loss: 0.00001895
Iteration 299/1000 | Loss: 0.00001895
Iteration 300/1000 | Loss: 0.00001895
Iteration 301/1000 | Loss: 0.00001895
Iteration 302/1000 | Loss: 0.00001895
Iteration 303/1000 | Loss: 0.00001895
Iteration 304/1000 | Loss: 0.00001895
Iteration 305/1000 | Loss: 0.00001895
Iteration 306/1000 | Loss: 0.00001894
Iteration 307/1000 | Loss: 0.00001894
Iteration 308/1000 | Loss: 0.00001894
Iteration 309/1000 | Loss: 0.00001894
Iteration 310/1000 | Loss: 0.00001894
Iteration 311/1000 | Loss: 0.00001894
Iteration 312/1000 | Loss: 0.00001894
Iteration 313/1000 | Loss: 0.00001894
Iteration 314/1000 | Loss: 0.00001894
Iteration 315/1000 | Loss: 0.00001894
Iteration 316/1000 | Loss: 0.00001894
Iteration 317/1000 | Loss: 0.00001894
Iteration 318/1000 | Loss: 0.00001894
Iteration 319/1000 | Loss: 0.00001894
Iteration 320/1000 | Loss: 0.00001894
Iteration 321/1000 | Loss: 0.00001893
Iteration 322/1000 | Loss: 0.00001893
Iteration 323/1000 | Loss: 0.00001893
Iteration 324/1000 | Loss: 0.00001893
Iteration 325/1000 | Loss: 0.00001893
Iteration 326/1000 | Loss: 0.00001893
Iteration 327/1000 | Loss: 0.00001893
Iteration 328/1000 | Loss: 0.00001893
Iteration 329/1000 | Loss: 0.00001893
Iteration 330/1000 | Loss: 0.00001893
Iteration 331/1000 | Loss: 0.00001893
Iteration 332/1000 | Loss: 0.00001893
Iteration 333/1000 | Loss: 0.00001893
Iteration 334/1000 | Loss: 0.00001893
Iteration 335/1000 | Loss: 0.00001892
Iteration 336/1000 | Loss: 0.00001892
Iteration 337/1000 | Loss: 0.00001892
Iteration 338/1000 | Loss: 0.00001892
Iteration 339/1000 | Loss: 0.00001892
Iteration 340/1000 | Loss: 0.00001892
Iteration 341/1000 | Loss: 0.00001892
Iteration 342/1000 | Loss: 0.00001892
Iteration 343/1000 | Loss: 0.00001892
Iteration 344/1000 | Loss: 0.00001892
Iteration 345/1000 | Loss: 0.00001892
Iteration 346/1000 | Loss: 0.00001892
Iteration 347/1000 | Loss: 0.00001892
Iteration 348/1000 | Loss: 0.00001892
Iteration 349/1000 | Loss: 0.00001892
Iteration 350/1000 | Loss: 0.00001892
Iteration 351/1000 | Loss: 0.00001892
Iteration 352/1000 | Loss: 0.00001892
Iteration 353/1000 | Loss: 0.00001891
Iteration 354/1000 | Loss: 0.00001891
Iteration 355/1000 | Loss: 0.00001891
Iteration 356/1000 | Loss: 0.00001891
Iteration 357/1000 | Loss: 0.00001891
Iteration 358/1000 | Loss: 0.00001891
Iteration 359/1000 | Loss: 0.00001891
Iteration 360/1000 | Loss: 0.00001891
Iteration 361/1000 | Loss: 0.00001891
Iteration 362/1000 | Loss: 0.00001891
Iteration 363/1000 | Loss: 0.00001891
Iteration 364/1000 | Loss: 0.00001891
Iteration 365/1000 | Loss: 0.00001891
Iteration 366/1000 | Loss: 0.00001891
Iteration 367/1000 | Loss: 0.00001891
Iteration 368/1000 | Loss: 0.00001891
Iteration 369/1000 | Loss: 0.00001891
Iteration 370/1000 | Loss: 0.00001891
Iteration 371/1000 | Loss: 0.00001891
Iteration 372/1000 | Loss: 0.00001891
Iteration 373/1000 | Loss: 0.00001890
Iteration 374/1000 | Loss: 0.00001890
Iteration 375/1000 | Loss: 0.00001890
Iteration 376/1000 | Loss: 0.00001890
Iteration 377/1000 | Loss: 0.00001890
Iteration 378/1000 | Loss: 0.00001890
Iteration 379/1000 | Loss: 0.00001890
Iteration 380/1000 | Loss: 0.00001890
Iteration 381/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 381. Stopping optimization.
Last 5 losses: [1.8904314856627025e-05, 1.8904314856627025e-05, 1.8904314856627025e-05, 1.8904314856627025e-05, 1.8904314856627025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8904314856627025e-05

Optimization complete. Final v2v error: 3.5374443531036377 mm

Highest mean error: 4.52372932434082 mm for frame 78

Lowest mean error: 2.617041826248169 mm for frame 168

Saving results

Total time: 1032.204787015915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1078
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00625488
Iteration 2/25 | Loss: 0.00122717
Iteration 3/25 | Loss: 0.00115172
Iteration 4/25 | Loss: 0.00114087
Iteration 5/25 | Loss: 0.00113752
Iteration 6/25 | Loss: 0.00113648
Iteration 7/25 | Loss: 0.00113648
Iteration 8/25 | Loss: 0.00113648
Iteration 9/25 | Loss: 0.00113648
Iteration 10/25 | Loss: 0.00113648
Iteration 11/25 | Loss: 0.00113648
Iteration 12/25 | Loss: 0.00113648
Iteration 13/25 | Loss: 0.00113648
Iteration 14/25 | Loss: 0.00113648
Iteration 15/25 | Loss: 0.00113648
Iteration 16/25 | Loss: 0.00113648
Iteration 17/25 | Loss: 0.00113648
Iteration 18/25 | Loss: 0.00113648
Iteration 19/25 | Loss: 0.00113648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00113647838588804, 0.00113647838588804, 0.00113647838588804, 0.00113647838588804, 0.00113647838588804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00113647838588804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28513718
Iteration 2/25 | Loss: 0.00180197
Iteration 3/25 | Loss: 0.00180197
Iteration 4/25 | Loss: 0.00180197
Iteration 5/25 | Loss: 0.00180197
Iteration 6/25 | Loss: 0.00180197
Iteration 7/25 | Loss: 0.00180197
Iteration 8/25 | Loss: 0.00180197
Iteration 9/25 | Loss: 0.00180197
Iteration 10/25 | Loss: 0.00180197
Iteration 11/25 | Loss: 0.00180197
Iteration 12/25 | Loss: 0.00180197
Iteration 13/25 | Loss: 0.00180197
Iteration 14/25 | Loss: 0.00180197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0018019652925431728, 0.0018019652925431728, 0.0018019652925431728, 0.0018019652925431728, 0.0018019652925431728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018019652925431728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180197
Iteration 2/1000 | Loss: 0.00001757
Iteration 3/1000 | Loss: 0.00001184
Iteration 4/1000 | Loss: 0.00001055
Iteration 5/1000 | Loss: 0.00000998
Iteration 6/1000 | Loss: 0.00000957
Iteration 7/1000 | Loss: 0.00000918
Iteration 8/1000 | Loss: 0.00000896
Iteration 9/1000 | Loss: 0.00000878
Iteration 10/1000 | Loss: 0.00000876
Iteration 11/1000 | Loss: 0.00000851
Iteration 12/1000 | Loss: 0.00000850
Iteration 13/1000 | Loss: 0.00000850
Iteration 14/1000 | Loss: 0.00000831
Iteration 15/1000 | Loss: 0.00000830
Iteration 16/1000 | Loss: 0.00000829
Iteration 17/1000 | Loss: 0.00000823
Iteration 18/1000 | Loss: 0.00000819
Iteration 19/1000 | Loss: 0.00000818
Iteration 20/1000 | Loss: 0.00000817
Iteration 21/1000 | Loss: 0.00000817
Iteration 22/1000 | Loss: 0.00000817
Iteration 23/1000 | Loss: 0.00000816
Iteration 24/1000 | Loss: 0.00000815
Iteration 25/1000 | Loss: 0.00000815
Iteration 26/1000 | Loss: 0.00000814
Iteration 27/1000 | Loss: 0.00000813
Iteration 28/1000 | Loss: 0.00000812
Iteration 29/1000 | Loss: 0.00000811
Iteration 30/1000 | Loss: 0.00000810
Iteration 31/1000 | Loss: 0.00000809
Iteration 32/1000 | Loss: 0.00000809
Iteration 33/1000 | Loss: 0.00000808
Iteration 34/1000 | Loss: 0.00000808
Iteration 35/1000 | Loss: 0.00000805
Iteration 36/1000 | Loss: 0.00000804
Iteration 37/1000 | Loss: 0.00000803
Iteration 38/1000 | Loss: 0.00000803
Iteration 39/1000 | Loss: 0.00000802
Iteration 40/1000 | Loss: 0.00000802
Iteration 41/1000 | Loss: 0.00000802
Iteration 42/1000 | Loss: 0.00000802
Iteration 43/1000 | Loss: 0.00000802
Iteration 44/1000 | Loss: 0.00000802
Iteration 45/1000 | Loss: 0.00000802
Iteration 46/1000 | Loss: 0.00000802
Iteration 47/1000 | Loss: 0.00000802
Iteration 48/1000 | Loss: 0.00000800
Iteration 49/1000 | Loss: 0.00000797
Iteration 50/1000 | Loss: 0.00000797
Iteration 51/1000 | Loss: 0.00000797
Iteration 52/1000 | Loss: 0.00000797
Iteration 53/1000 | Loss: 0.00000797
Iteration 54/1000 | Loss: 0.00000797
Iteration 55/1000 | Loss: 0.00000796
Iteration 56/1000 | Loss: 0.00000796
Iteration 57/1000 | Loss: 0.00000796
Iteration 58/1000 | Loss: 0.00000795
Iteration 59/1000 | Loss: 0.00000795
Iteration 60/1000 | Loss: 0.00000794
Iteration 61/1000 | Loss: 0.00000793
Iteration 62/1000 | Loss: 0.00000793
Iteration 63/1000 | Loss: 0.00000792
Iteration 64/1000 | Loss: 0.00000792
Iteration 65/1000 | Loss: 0.00000792
Iteration 66/1000 | Loss: 0.00000792
Iteration 67/1000 | Loss: 0.00000792
Iteration 68/1000 | Loss: 0.00000792
Iteration 69/1000 | Loss: 0.00000792
Iteration 70/1000 | Loss: 0.00000792
Iteration 71/1000 | Loss: 0.00000792
Iteration 72/1000 | Loss: 0.00000791
Iteration 73/1000 | Loss: 0.00000790
Iteration 74/1000 | Loss: 0.00000790
Iteration 75/1000 | Loss: 0.00000790
Iteration 76/1000 | Loss: 0.00000790
Iteration 77/1000 | Loss: 0.00000789
Iteration 78/1000 | Loss: 0.00000789
Iteration 79/1000 | Loss: 0.00000788
Iteration 80/1000 | Loss: 0.00000788
Iteration 81/1000 | Loss: 0.00000788
Iteration 82/1000 | Loss: 0.00000788
Iteration 83/1000 | Loss: 0.00000788
Iteration 84/1000 | Loss: 0.00000788
Iteration 85/1000 | Loss: 0.00000788
Iteration 86/1000 | Loss: 0.00000788
Iteration 87/1000 | Loss: 0.00000788
Iteration 88/1000 | Loss: 0.00000788
Iteration 89/1000 | Loss: 0.00000788
Iteration 90/1000 | Loss: 0.00000788
Iteration 91/1000 | Loss: 0.00000788
Iteration 92/1000 | Loss: 0.00000787
Iteration 93/1000 | Loss: 0.00000786
Iteration 94/1000 | Loss: 0.00000786
Iteration 95/1000 | Loss: 0.00000786
Iteration 96/1000 | Loss: 0.00000786
Iteration 97/1000 | Loss: 0.00000786
Iteration 98/1000 | Loss: 0.00000786
Iteration 99/1000 | Loss: 0.00000786
Iteration 100/1000 | Loss: 0.00000786
Iteration 101/1000 | Loss: 0.00000786
Iteration 102/1000 | Loss: 0.00000786
Iteration 103/1000 | Loss: 0.00000786
Iteration 104/1000 | Loss: 0.00000785
Iteration 105/1000 | Loss: 0.00000785
Iteration 106/1000 | Loss: 0.00000785
Iteration 107/1000 | Loss: 0.00000785
Iteration 108/1000 | Loss: 0.00000785
Iteration 109/1000 | Loss: 0.00000785
Iteration 110/1000 | Loss: 0.00000785
Iteration 111/1000 | Loss: 0.00000785
Iteration 112/1000 | Loss: 0.00000785
Iteration 113/1000 | Loss: 0.00000785
Iteration 114/1000 | Loss: 0.00000785
Iteration 115/1000 | Loss: 0.00000784
Iteration 116/1000 | Loss: 0.00000784
Iteration 117/1000 | Loss: 0.00000784
Iteration 118/1000 | Loss: 0.00000784
Iteration 119/1000 | Loss: 0.00000784
Iteration 120/1000 | Loss: 0.00000784
Iteration 121/1000 | Loss: 0.00000784
Iteration 122/1000 | Loss: 0.00000784
Iteration 123/1000 | Loss: 0.00000784
Iteration 124/1000 | Loss: 0.00000784
Iteration 125/1000 | Loss: 0.00000784
Iteration 126/1000 | Loss: 0.00000784
Iteration 127/1000 | Loss: 0.00000784
Iteration 128/1000 | Loss: 0.00000784
Iteration 129/1000 | Loss: 0.00000784
Iteration 130/1000 | Loss: 0.00000783
Iteration 131/1000 | Loss: 0.00000783
Iteration 132/1000 | Loss: 0.00000783
Iteration 133/1000 | Loss: 0.00000783
Iteration 134/1000 | Loss: 0.00000783
Iteration 135/1000 | Loss: 0.00000783
Iteration 136/1000 | Loss: 0.00000783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [7.834028110664804e-06, 7.834028110664804e-06, 7.834028110664804e-06, 7.834028110664804e-06, 7.834028110664804e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.834028110664804e-06

Optimization complete. Final v2v error: 2.417853832244873 mm

Highest mean error: 2.981039047241211 mm for frame 80

Lowest mean error: 2.2442386150360107 mm for frame 103

Saving results

Total time: 221.21687936782837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1013
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800674
Iteration 2/25 | Loss: 0.00148906
Iteration 3/25 | Loss: 0.00130950
Iteration 4/25 | Loss: 0.00127630
Iteration 5/25 | Loss: 0.00125855
Iteration 6/25 | Loss: 0.00125417
Iteration 7/25 | Loss: 0.00124576
Iteration 8/25 | Loss: 0.00124350
Iteration 9/25 | Loss: 0.00124225
Iteration 10/25 | Loss: 0.00124175
Iteration 11/25 | Loss: 0.00124138
Iteration 12/25 | Loss: 0.00124114
Iteration 13/25 | Loss: 0.00124094
Iteration 14/25 | Loss: 0.00124083
Iteration 15/25 | Loss: 0.00124073
Iteration 16/25 | Loss: 0.00124063
Iteration 17/25 | Loss: 0.00124059
Iteration 18/25 | Loss: 0.00124055
Iteration 19/25 | Loss: 0.00124055
Iteration 20/25 | Loss: 0.00124055
Iteration 21/25 | Loss: 0.00124055
Iteration 22/25 | Loss: 0.00124054
Iteration 23/25 | Loss: 0.00124054
Iteration 24/25 | Loss: 0.00124054
Iteration 25/25 | Loss: 0.00124054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68196142
Iteration 2/25 | Loss: 0.00210554
Iteration 3/25 | Loss: 0.00209150
Iteration 4/25 | Loss: 0.00209149
Iteration 5/25 | Loss: 0.00209149
Iteration 6/25 | Loss: 0.00209150
Iteration 7/25 | Loss: 0.00209149
Iteration 8/25 | Loss: 0.00209149
Iteration 9/25 | Loss: 0.00209149
Iteration 10/25 | Loss: 0.00209149
Iteration 11/25 | Loss: 0.00209149
Iteration 12/25 | Loss: 0.00209149
Iteration 13/25 | Loss: 0.00209149
Iteration 14/25 | Loss: 0.00209149
Iteration 15/25 | Loss: 0.00209149
Iteration 16/25 | Loss: 0.00209149
Iteration 17/25 | Loss: 0.00209149
Iteration 18/25 | Loss: 0.00209149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002091494854539633, 0.002091494854539633, 0.002091494854539633, 0.002091494854539633, 0.002091494854539633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002091494854539633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209149
Iteration 2/1000 | Loss: 0.00006189
Iteration 3/1000 | Loss: 0.00004939
Iteration 4/1000 | Loss: 0.00002885
Iteration 5/1000 | Loss: 0.00004030
Iteration 6/1000 | Loss: 0.00002454
Iteration 7/1000 | Loss: 0.00003867
Iteration 8/1000 | Loss: 0.00002325
Iteration 9/1000 | Loss: 0.00002906
Iteration 10/1000 | Loss: 0.00002239
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002168
Iteration 13/1000 | Loss: 0.00002141
Iteration 14/1000 | Loss: 0.00002126
Iteration 15/1000 | Loss: 0.00004406
Iteration 16/1000 | Loss: 0.00002124
Iteration 17/1000 | Loss: 0.00002607
Iteration 18/1000 | Loss: 0.00002096
Iteration 19/1000 | Loss: 0.00002089
Iteration 20/1000 | Loss: 0.00002086
Iteration 21/1000 | Loss: 0.00002085
Iteration 22/1000 | Loss: 0.00002084
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00003301
Iteration 25/1000 | Loss: 0.00002074
Iteration 26/1000 | Loss: 0.00002074
Iteration 27/1000 | Loss: 0.00002073
Iteration 28/1000 | Loss: 0.00002073
Iteration 29/1000 | Loss: 0.00002072
Iteration 30/1000 | Loss: 0.00002072
Iteration 31/1000 | Loss: 0.00002072
Iteration 32/1000 | Loss: 0.00002072
Iteration 33/1000 | Loss: 0.00002071
Iteration 34/1000 | Loss: 0.00002065
Iteration 35/1000 | Loss: 0.00002063
Iteration 36/1000 | Loss: 0.00002063
Iteration 37/1000 | Loss: 0.00002060
Iteration 38/1000 | Loss: 0.00002059
Iteration 39/1000 | Loss: 0.00002059
Iteration 40/1000 | Loss: 0.00002058
Iteration 41/1000 | Loss: 0.00002058
Iteration 42/1000 | Loss: 0.00002057
Iteration 43/1000 | Loss: 0.00002057
Iteration 44/1000 | Loss: 0.00002055
Iteration 45/1000 | Loss: 0.00002055
Iteration 46/1000 | Loss: 0.00002054
Iteration 47/1000 | Loss: 0.00002053
Iteration 48/1000 | Loss: 0.00002051
Iteration 49/1000 | Loss: 0.00002050
Iteration 50/1000 | Loss: 0.00002049
Iteration 51/1000 | Loss: 0.00002049
Iteration 52/1000 | Loss: 0.00002049
Iteration 53/1000 | Loss: 0.00002048
Iteration 54/1000 | Loss: 0.00002047
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002045
Iteration 57/1000 | Loss: 0.00002045
Iteration 58/1000 | Loss: 0.00002045
Iteration 59/1000 | Loss: 0.00002045
Iteration 60/1000 | Loss: 0.00002045
Iteration 61/1000 | Loss: 0.00006042
Iteration 62/1000 | Loss: 0.00002090
Iteration 63/1000 | Loss: 0.00002043
Iteration 64/1000 | Loss: 0.00002041
Iteration 65/1000 | Loss: 0.00002039
Iteration 66/1000 | Loss: 0.00002038
Iteration 67/1000 | Loss: 0.00002038
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002037
Iteration 70/1000 | Loss: 0.00002037
Iteration 71/1000 | Loss: 0.00002037
Iteration 72/1000 | Loss: 0.00002036
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002035
Iteration 75/1000 | Loss: 0.00002035
Iteration 76/1000 | Loss: 0.00002035
Iteration 77/1000 | Loss: 0.00002035
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002034
Iteration 80/1000 | Loss: 0.00002034
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002033
Iteration 84/1000 | Loss: 0.00002033
Iteration 85/1000 | Loss: 0.00002033
Iteration 86/1000 | Loss: 0.00002033
Iteration 87/1000 | Loss: 0.00002033
Iteration 88/1000 | Loss: 0.00002032
Iteration 89/1000 | Loss: 0.00002032
Iteration 90/1000 | Loss: 0.00002032
Iteration 91/1000 | Loss: 0.00002032
Iteration 92/1000 | Loss: 0.00002032
Iteration 93/1000 | Loss: 0.00002032
Iteration 94/1000 | Loss: 0.00002031
Iteration 95/1000 | Loss: 0.00002031
Iteration 96/1000 | Loss: 0.00002031
Iteration 97/1000 | Loss: 0.00002031
Iteration 98/1000 | Loss: 0.00002031
Iteration 99/1000 | Loss: 0.00002031
Iteration 100/1000 | Loss: 0.00002031
Iteration 101/1000 | Loss: 0.00002031
Iteration 102/1000 | Loss: 0.00002030
Iteration 103/1000 | Loss: 0.00002029
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002028
Iteration 107/1000 | Loss: 0.00002028
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002024
Iteration 110/1000 | Loss: 0.00002018
Iteration 111/1000 | Loss: 0.00002018
Iteration 112/1000 | Loss: 0.00002017
Iteration 113/1000 | Loss: 0.00002017
Iteration 114/1000 | Loss: 0.00002017
Iteration 115/1000 | Loss: 0.00002017
Iteration 116/1000 | Loss: 0.00002017
Iteration 117/1000 | Loss: 0.00002017
Iteration 118/1000 | Loss: 0.00002017
Iteration 119/1000 | Loss: 0.00002013
Iteration 120/1000 | Loss: 0.00002013
Iteration 121/1000 | Loss: 0.00002013
Iteration 122/1000 | Loss: 0.00002013
Iteration 123/1000 | Loss: 0.00002012
Iteration 124/1000 | Loss: 0.00002012
Iteration 125/1000 | Loss: 0.00002012
Iteration 126/1000 | Loss: 0.00002012
Iteration 127/1000 | Loss: 0.00002012
Iteration 128/1000 | Loss: 0.00002012
Iteration 129/1000 | Loss: 0.00002012
Iteration 130/1000 | Loss: 0.00002012
Iteration 131/1000 | Loss: 0.00002012
Iteration 132/1000 | Loss: 0.00002011
Iteration 133/1000 | Loss: 0.00002011
Iteration 134/1000 | Loss: 0.00002011
Iteration 135/1000 | Loss: 0.00002010
Iteration 136/1000 | Loss: 0.00002010
Iteration 137/1000 | Loss: 0.00002010
Iteration 138/1000 | Loss: 0.00002010
Iteration 139/1000 | Loss: 0.00002009
Iteration 140/1000 | Loss: 0.00002009
Iteration 141/1000 | Loss: 0.00002009
Iteration 142/1000 | Loss: 0.00002009
Iteration 143/1000 | Loss: 0.00002009
Iteration 144/1000 | Loss: 0.00002009
Iteration 145/1000 | Loss: 0.00002009
Iteration 146/1000 | Loss: 0.00002008
Iteration 147/1000 | Loss: 0.00002008
Iteration 148/1000 | Loss: 0.00002008
Iteration 149/1000 | Loss: 0.00002008
Iteration 150/1000 | Loss: 0.00002008
Iteration 151/1000 | Loss: 0.00002008
Iteration 152/1000 | Loss: 0.00002008
Iteration 153/1000 | Loss: 0.00002008
Iteration 154/1000 | Loss: 0.00002008
Iteration 155/1000 | Loss: 0.00002008
Iteration 156/1000 | Loss: 0.00002008
Iteration 157/1000 | Loss: 0.00002008
Iteration 158/1000 | Loss: 0.00002008
Iteration 159/1000 | Loss: 0.00002008
Iteration 160/1000 | Loss: 0.00002008
Iteration 161/1000 | Loss: 0.00002007
Iteration 162/1000 | Loss: 0.00002007
Iteration 163/1000 | Loss: 0.00002007
Iteration 164/1000 | Loss: 0.00002006
Iteration 165/1000 | Loss: 0.00002006
Iteration 166/1000 | Loss: 0.00002006
Iteration 167/1000 | Loss: 0.00002005
Iteration 168/1000 | Loss: 0.00002005
Iteration 169/1000 | Loss: 0.00002004
Iteration 170/1000 | Loss: 0.00002000
Iteration 171/1000 | Loss: 0.00002000
Iteration 172/1000 | Loss: 0.00002000
Iteration 173/1000 | Loss: 0.00002000
Iteration 174/1000 | Loss: 0.00002000
Iteration 175/1000 | Loss: 0.00002000
Iteration 176/1000 | Loss: 0.00002000
Iteration 177/1000 | Loss: 0.00001999
Iteration 178/1000 | Loss: 0.00001999
Iteration 179/1000 | Loss: 0.00001999
Iteration 180/1000 | Loss: 0.00001999
Iteration 181/1000 | Loss: 0.00001999
Iteration 182/1000 | Loss: 0.00001999
Iteration 183/1000 | Loss: 0.00001999
Iteration 184/1000 | Loss: 0.00001998
Iteration 185/1000 | Loss: 0.00001998
Iteration 186/1000 | Loss: 0.00001998
Iteration 187/1000 | Loss: 0.00001998
Iteration 188/1000 | Loss: 0.00001998
Iteration 189/1000 | Loss: 0.00001998
Iteration 190/1000 | Loss: 0.00001998
Iteration 191/1000 | Loss: 0.00001997
Iteration 192/1000 | Loss: 0.00001997
Iteration 193/1000 | Loss: 0.00001997
Iteration 194/1000 | Loss: 0.00001997
Iteration 195/1000 | Loss: 0.00001997
Iteration 196/1000 | Loss: 0.00001996
Iteration 197/1000 | Loss: 0.00001996
Iteration 198/1000 | Loss: 0.00001996
Iteration 199/1000 | Loss: 0.00001995
Iteration 200/1000 | Loss: 0.00001995
Iteration 201/1000 | Loss: 0.00001995
Iteration 202/1000 | Loss: 0.00001995
Iteration 203/1000 | Loss: 0.00001995
Iteration 204/1000 | Loss: 0.00001994
Iteration 205/1000 | Loss: 0.00001994
Iteration 206/1000 | Loss: 0.00001994
Iteration 207/1000 | Loss: 0.00001994
Iteration 208/1000 | Loss: 0.00001994
Iteration 209/1000 | Loss: 0.00001994
Iteration 210/1000 | Loss: 0.00001993
Iteration 211/1000 | Loss: 0.00001993
Iteration 212/1000 | Loss: 0.00001993
Iteration 213/1000 | Loss: 0.00001993
Iteration 214/1000 | Loss: 0.00001993
Iteration 215/1000 | Loss: 0.00001993
Iteration 216/1000 | Loss: 0.00001993
Iteration 217/1000 | Loss: 0.00001993
Iteration 218/1000 | Loss: 0.00001993
Iteration 219/1000 | Loss: 0.00001993
Iteration 220/1000 | Loss: 0.00001993
Iteration 221/1000 | Loss: 0.00001993
Iteration 222/1000 | Loss: 0.00001993
Iteration 223/1000 | Loss: 0.00001993
Iteration 224/1000 | Loss: 0.00001993
Iteration 225/1000 | Loss: 0.00001993
Iteration 226/1000 | Loss: 0.00001993
Iteration 227/1000 | Loss: 0.00001993
Iteration 228/1000 | Loss: 0.00001993
Iteration 229/1000 | Loss: 0.00001993
Iteration 230/1000 | Loss: 0.00001993
Iteration 231/1000 | Loss: 0.00001993
Iteration 232/1000 | Loss: 0.00001993
Iteration 233/1000 | Loss: 0.00001993
Iteration 234/1000 | Loss: 0.00001993
Iteration 235/1000 | Loss: 0.00001993
Iteration 236/1000 | Loss: 0.00001993
Iteration 237/1000 | Loss: 0.00001993
Iteration 238/1000 | Loss: 0.00001993
Iteration 239/1000 | Loss: 0.00001993
Iteration 240/1000 | Loss: 0.00001993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.9928385881939903e-05, 1.9928385881939903e-05, 1.9928385881939903e-05, 1.9928385881939903e-05, 1.9928385881939903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9928385881939903e-05

Optimization complete. Final v2v error: 3.3326876163482666 mm

Highest mean error: 12.466939926147461 mm for frame 48

Lowest mean error: 2.7572596073150635 mm for frame 131

Saving results

Total time: 471.0705461502075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1020
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958274
Iteration 2/25 | Loss: 0.00213710
Iteration 3/25 | Loss: 0.00178809
Iteration 4/25 | Loss: 0.00158499
Iteration 5/25 | Loss: 0.00173362
Iteration 6/25 | Loss: 0.00142950
Iteration 7/25 | Loss: 0.00139311
Iteration 8/25 | Loss: 0.00135264
Iteration 9/25 | Loss: 0.00133304
Iteration 10/25 | Loss: 0.00129786
Iteration 11/25 | Loss: 0.00129367
Iteration 12/25 | Loss: 0.00127435
Iteration 13/25 | Loss: 0.00127263
Iteration 14/25 | Loss: 0.00126885
Iteration 15/25 | Loss: 0.00125241
Iteration 16/25 | Loss: 0.00124631
Iteration 17/25 | Loss: 0.00124502
Iteration 18/25 | Loss: 0.00125109
Iteration 19/25 | Loss: 0.00125406
Iteration 20/25 | Loss: 0.00124463
Iteration 21/25 | Loss: 0.00124276
Iteration 22/25 | Loss: 0.00123909
Iteration 23/25 | Loss: 0.00123824
Iteration 24/25 | Loss: 0.00123849
Iteration 25/25 | Loss: 0.00123793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23870409
Iteration 2/25 | Loss: 0.00176140
Iteration 3/25 | Loss: 0.00169625
Iteration 4/25 | Loss: 0.00169625
Iteration 5/25 | Loss: 0.00169625
Iteration 6/25 | Loss: 0.00169625
Iteration 7/25 | Loss: 0.00169625
Iteration 8/25 | Loss: 0.00169625
Iteration 9/25 | Loss: 0.00169625
Iteration 10/25 | Loss: 0.00169625
Iteration 11/25 | Loss: 0.00169625
Iteration 12/25 | Loss: 0.00169625
Iteration 13/25 | Loss: 0.00169625
Iteration 14/25 | Loss: 0.00169625
Iteration 15/25 | Loss: 0.00169625
Iteration 16/25 | Loss: 0.00169625
Iteration 17/25 | Loss: 0.00169625
Iteration 18/25 | Loss: 0.00169625
Iteration 19/25 | Loss: 0.00169625
Iteration 20/25 | Loss: 0.00169625
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016962477238848805, 0.0016962477238848805, 0.0016962477238848805, 0.0016962477238848805, 0.0016962477238848805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016962477238848805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169625
Iteration 2/1000 | Loss: 0.00045550
Iteration 3/1000 | Loss: 0.00028018
Iteration 4/1000 | Loss: 0.00028552
Iteration 5/1000 | Loss: 0.00020999
Iteration 6/1000 | Loss: 0.00020950
Iteration 7/1000 | Loss: 0.00018040
Iteration 8/1000 | Loss: 0.00019112
Iteration 9/1000 | Loss: 0.00019855
Iteration 10/1000 | Loss: 0.00018631
Iteration 11/1000 | Loss: 0.00026654
Iteration 12/1000 | Loss: 0.00017967
Iteration 13/1000 | Loss: 0.00018266
Iteration 14/1000 | Loss: 0.00018141
Iteration 15/1000 | Loss: 0.00021878
Iteration 16/1000 | Loss: 0.00017725
Iteration 17/1000 | Loss: 0.00051480
Iteration 18/1000 | Loss: 0.00010894
Iteration 19/1000 | Loss: 0.00018759
Iteration 20/1000 | Loss: 0.00032407
Iteration 21/1000 | Loss: 0.00019854
Iteration 22/1000 | Loss: 0.00025689
Iteration 23/1000 | Loss: 0.00018980
Iteration 24/1000 | Loss: 0.00014624
Iteration 25/1000 | Loss: 0.00028341
Iteration 26/1000 | Loss: 0.00017840
Iteration 27/1000 | Loss: 0.00016827
Iteration 28/1000 | Loss: 0.00021270
Iteration 29/1000 | Loss: 0.00014209
Iteration 30/1000 | Loss: 0.00020491
Iteration 31/1000 | Loss: 0.00013584
Iteration 32/1000 | Loss: 0.00012099
Iteration 33/1000 | Loss: 0.00023676
Iteration 34/1000 | Loss: 0.00019245
Iteration 35/1000 | Loss: 0.00016564
Iteration 36/1000 | Loss: 0.00018537
Iteration 37/1000 | Loss: 0.00020696
Iteration 38/1000 | Loss: 0.00015491
Iteration 39/1000 | Loss: 0.00009865
Iteration 40/1000 | Loss: 0.00004457
Iteration 41/1000 | Loss: 0.00014735
Iteration 42/1000 | Loss: 0.00017148
Iteration 43/1000 | Loss: 0.00016122
Iteration 44/1000 | Loss: 0.00018004
Iteration 45/1000 | Loss: 0.00020366
Iteration 46/1000 | Loss: 0.00012980
Iteration 47/1000 | Loss: 0.00015622
Iteration 48/1000 | Loss: 0.00019163
Iteration 49/1000 | Loss: 0.00016590
Iteration 50/1000 | Loss: 0.00016430
Iteration 51/1000 | Loss: 0.00014700
Iteration 52/1000 | Loss: 0.00017767
Iteration 53/1000 | Loss: 0.00019338
Iteration 54/1000 | Loss: 0.00016968
Iteration 55/1000 | Loss: 0.00018924
Iteration 56/1000 | Loss: 0.00013096
Iteration 57/1000 | Loss: 0.00009961
Iteration 58/1000 | Loss: 0.00016804
Iteration 59/1000 | Loss: 0.00017609
Iteration 60/1000 | Loss: 0.00013217
Iteration 61/1000 | Loss: 0.00016077
Iteration 62/1000 | Loss: 0.00012222
Iteration 63/1000 | Loss: 0.00014337
Iteration 64/1000 | Loss: 0.00032689
Iteration 65/1000 | Loss: 0.00014429
Iteration 66/1000 | Loss: 0.00027221
Iteration 67/1000 | Loss: 0.00020523
Iteration 68/1000 | Loss: 0.00041460
Iteration 69/1000 | Loss: 0.00038390
Iteration 70/1000 | Loss: 0.00021021
Iteration 71/1000 | Loss: 0.00019371
Iteration 72/1000 | Loss: 0.00010462
Iteration 73/1000 | Loss: 0.00015581
Iteration 74/1000 | Loss: 0.00021859
Iteration 75/1000 | Loss: 0.00008301
Iteration 76/1000 | Loss: 0.00024881
Iteration 77/1000 | Loss: 0.00015734
Iteration 78/1000 | Loss: 0.00014576
Iteration 79/1000 | Loss: 0.00030511
Iteration 80/1000 | Loss: 0.00016716
Iteration 81/1000 | Loss: 0.00023855
Iteration 82/1000 | Loss: 0.00017058
Iteration 83/1000 | Loss: 0.00023199
Iteration 84/1000 | Loss: 0.00020910
Iteration 85/1000 | Loss: 0.00020272
Iteration 86/1000 | Loss: 0.00019738
Iteration 87/1000 | Loss: 0.00026541
Iteration 88/1000 | Loss: 0.00016444
Iteration 89/1000 | Loss: 0.00018255
Iteration 90/1000 | Loss: 0.00023778
Iteration 91/1000 | Loss: 0.00016120
Iteration 92/1000 | Loss: 0.00021471
Iteration 93/1000 | Loss: 0.00019902
Iteration 94/1000 | Loss: 0.00027471
Iteration 95/1000 | Loss: 0.00003006
Iteration 96/1000 | Loss: 0.00002201
Iteration 97/1000 | Loss: 0.00002037
Iteration 98/1000 | Loss: 0.00010310
Iteration 99/1000 | Loss: 0.00002215
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00022921
Iteration 102/1000 | Loss: 0.00021799
Iteration 103/1000 | Loss: 0.00022260
Iteration 104/1000 | Loss: 0.00018624
Iteration 105/1000 | Loss: 0.00004196
Iteration 106/1000 | Loss: 0.00001921
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00043868
Iteration 109/1000 | Loss: 0.00082955
Iteration 110/1000 | Loss: 0.00011829
Iteration 111/1000 | Loss: 0.00011326
Iteration 112/1000 | Loss: 0.00002389
Iteration 113/1000 | Loss: 0.00002074
Iteration 114/1000 | Loss: 0.00034737
Iteration 115/1000 | Loss: 0.00015827
Iteration 116/1000 | Loss: 0.00002742
Iteration 117/1000 | Loss: 0.00005847
Iteration 118/1000 | Loss: 0.00002307
Iteration 119/1000 | Loss: 0.00012616
Iteration 120/1000 | Loss: 0.00002061
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00006613
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00001738
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00009976
Iteration 127/1000 | Loss: 0.00001875
Iteration 128/1000 | Loss: 0.00001511
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001436
Iteration 131/1000 | Loss: 0.00001410
Iteration 132/1000 | Loss: 0.00012412
Iteration 133/1000 | Loss: 0.00016405
Iteration 134/1000 | Loss: 0.00001388
Iteration 135/1000 | Loss: 0.00011366
Iteration 136/1000 | Loss: 0.00034497
Iteration 137/1000 | Loss: 0.00001453
Iteration 138/1000 | Loss: 0.00001371
Iteration 139/1000 | Loss: 0.00001367
Iteration 140/1000 | Loss: 0.00001364
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00001362
Iteration 143/1000 | Loss: 0.00001362
Iteration 144/1000 | Loss: 0.00001362
Iteration 145/1000 | Loss: 0.00001362
Iteration 146/1000 | Loss: 0.00001362
Iteration 147/1000 | Loss: 0.00001362
Iteration 148/1000 | Loss: 0.00001362
Iteration 149/1000 | Loss: 0.00001362
Iteration 150/1000 | Loss: 0.00001362
Iteration 151/1000 | Loss: 0.00001362
Iteration 152/1000 | Loss: 0.00001361
Iteration 153/1000 | Loss: 0.00001361
Iteration 154/1000 | Loss: 0.00001361
Iteration 155/1000 | Loss: 0.00001361
Iteration 156/1000 | Loss: 0.00001361
Iteration 157/1000 | Loss: 0.00001361
Iteration 158/1000 | Loss: 0.00001361
Iteration 159/1000 | Loss: 0.00001361
Iteration 160/1000 | Loss: 0.00001360
Iteration 161/1000 | Loss: 0.00001360
Iteration 162/1000 | Loss: 0.00001360
Iteration 163/1000 | Loss: 0.00001359
Iteration 164/1000 | Loss: 0.00001359
Iteration 165/1000 | Loss: 0.00001359
Iteration 166/1000 | Loss: 0.00010814
Iteration 167/1000 | Loss: 0.00001362
Iteration 168/1000 | Loss: 0.00001358
Iteration 169/1000 | Loss: 0.00001357
Iteration 170/1000 | Loss: 0.00001357
Iteration 171/1000 | Loss: 0.00001356
Iteration 172/1000 | Loss: 0.00001356
Iteration 173/1000 | Loss: 0.00001355
Iteration 174/1000 | Loss: 0.00001355
Iteration 175/1000 | Loss: 0.00001355
Iteration 176/1000 | Loss: 0.00001355
Iteration 177/1000 | Loss: 0.00001355
Iteration 178/1000 | Loss: 0.00001355
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Iteration 181/1000 | Loss: 0.00001354
Iteration 182/1000 | Loss: 0.00001354
Iteration 183/1000 | Loss: 0.00001354
Iteration 184/1000 | Loss: 0.00001354
Iteration 185/1000 | Loss: 0.00001354
Iteration 186/1000 | Loss: 0.00001354
Iteration 187/1000 | Loss: 0.00001354
Iteration 188/1000 | Loss: 0.00001354
Iteration 189/1000 | Loss: 0.00001354
Iteration 190/1000 | Loss: 0.00001353
Iteration 191/1000 | Loss: 0.00001353
Iteration 192/1000 | Loss: 0.00001353
Iteration 193/1000 | Loss: 0.00001353
Iteration 194/1000 | Loss: 0.00001353
Iteration 195/1000 | Loss: 0.00001353
Iteration 196/1000 | Loss: 0.00001353
Iteration 197/1000 | Loss: 0.00001353
Iteration 198/1000 | Loss: 0.00001352
Iteration 199/1000 | Loss: 0.00001352
Iteration 200/1000 | Loss: 0.00001352
Iteration 201/1000 | Loss: 0.00001352
Iteration 202/1000 | Loss: 0.00001352
Iteration 203/1000 | Loss: 0.00001352
Iteration 204/1000 | Loss: 0.00001352
Iteration 205/1000 | Loss: 0.00001352
Iteration 206/1000 | Loss: 0.00001351
Iteration 207/1000 | Loss: 0.00001351
Iteration 208/1000 | Loss: 0.00001351
Iteration 209/1000 | Loss: 0.00001351
Iteration 210/1000 | Loss: 0.00001351
Iteration 211/1000 | Loss: 0.00001351
Iteration 212/1000 | Loss: 0.00001351
Iteration 213/1000 | Loss: 0.00001351
Iteration 214/1000 | Loss: 0.00001350
Iteration 215/1000 | Loss: 0.00001350
Iteration 216/1000 | Loss: 0.00001350
Iteration 217/1000 | Loss: 0.00001350
Iteration 218/1000 | Loss: 0.00001350
Iteration 219/1000 | Loss: 0.00001350
Iteration 220/1000 | Loss: 0.00001349
Iteration 221/1000 | Loss: 0.00001349
Iteration 222/1000 | Loss: 0.00001349
Iteration 223/1000 | Loss: 0.00001349
Iteration 224/1000 | Loss: 0.00001349
Iteration 225/1000 | Loss: 0.00001349
Iteration 226/1000 | Loss: 0.00001349
Iteration 227/1000 | Loss: 0.00001349
Iteration 228/1000 | Loss: 0.00001348
Iteration 229/1000 | Loss: 0.00001348
Iteration 230/1000 | Loss: 0.00001348
Iteration 231/1000 | Loss: 0.00001348
Iteration 232/1000 | Loss: 0.00001348
Iteration 233/1000 | Loss: 0.00001347
Iteration 234/1000 | Loss: 0.00001347
Iteration 235/1000 | Loss: 0.00001347
Iteration 236/1000 | Loss: 0.00001347
Iteration 237/1000 | Loss: 0.00001347
Iteration 238/1000 | Loss: 0.00001347
Iteration 239/1000 | Loss: 0.00001347
Iteration 240/1000 | Loss: 0.00001347
Iteration 241/1000 | Loss: 0.00001346
Iteration 242/1000 | Loss: 0.00001346
Iteration 243/1000 | Loss: 0.00001346
Iteration 244/1000 | Loss: 0.00001345
Iteration 245/1000 | Loss: 0.00001345
Iteration 246/1000 | Loss: 0.00001345
Iteration 247/1000 | Loss: 0.00001345
Iteration 248/1000 | Loss: 0.00001345
Iteration 249/1000 | Loss: 0.00001345
Iteration 250/1000 | Loss: 0.00001345
Iteration 251/1000 | Loss: 0.00001345
Iteration 252/1000 | Loss: 0.00001345
Iteration 253/1000 | Loss: 0.00001344
Iteration 254/1000 | Loss: 0.00001344
Iteration 255/1000 | Loss: 0.00001344
Iteration 256/1000 | Loss: 0.00001344
Iteration 257/1000 | Loss: 0.00001344
Iteration 258/1000 | Loss: 0.00001344
Iteration 259/1000 | Loss: 0.00001343
Iteration 260/1000 | Loss: 0.00001343
Iteration 261/1000 | Loss: 0.00001343
Iteration 262/1000 | Loss: 0.00001343
Iteration 263/1000 | Loss: 0.00001343
Iteration 264/1000 | Loss: 0.00001343
Iteration 265/1000 | Loss: 0.00001343
Iteration 266/1000 | Loss: 0.00001342
Iteration 267/1000 | Loss: 0.00001342
Iteration 268/1000 | Loss: 0.00001342
Iteration 269/1000 | Loss: 0.00001341
Iteration 270/1000 | Loss: 0.00001340
Iteration 271/1000 | Loss: 0.00001340
Iteration 272/1000 | Loss: 0.00001339
Iteration 273/1000 | Loss: 0.00001339
Iteration 274/1000 | Loss: 0.00001338
Iteration 275/1000 | Loss: 0.00001337
Iteration 276/1000 | Loss: 0.00001337
Iteration 277/1000 | Loss: 0.00001337
Iteration 278/1000 | Loss: 0.00001337
Iteration 279/1000 | Loss: 0.00001337
Iteration 280/1000 | Loss: 0.00001337
Iteration 281/1000 | Loss: 0.00001337
Iteration 282/1000 | Loss: 0.00001337
Iteration 283/1000 | Loss: 0.00001337
Iteration 284/1000 | Loss: 0.00001336
Iteration 285/1000 | Loss: 0.00001336
Iteration 286/1000 | Loss: 0.00001336
Iteration 287/1000 | Loss: 0.00001335
Iteration 288/1000 | Loss: 0.00001335
Iteration 289/1000 | Loss: 0.00001335
Iteration 290/1000 | Loss: 0.00001335
Iteration 291/1000 | Loss: 0.00001334
Iteration 292/1000 | Loss: 0.00001334
Iteration 293/1000 | Loss: 0.00001334
Iteration 294/1000 | Loss: 0.00001334
Iteration 295/1000 | Loss: 0.00001334
Iteration 296/1000 | Loss: 0.00001334
Iteration 297/1000 | Loss: 0.00001334
Iteration 298/1000 | Loss: 0.00001334
Iteration 299/1000 | Loss: 0.00001334
Iteration 300/1000 | Loss: 0.00001334
Iteration 301/1000 | Loss: 0.00001334
Iteration 302/1000 | Loss: 0.00001334
Iteration 303/1000 | Loss: 0.00001334
Iteration 304/1000 | Loss: 0.00001334
Iteration 305/1000 | Loss: 0.00001333
Iteration 306/1000 | Loss: 0.00001333
Iteration 307/1000 | Loss: 0.00001333
Iteration 308/1000 | Loss: 0.00001333
Iteration 309/1000 | Loss: 0.00001333
Iteration 310/1000 | Loss: 0.00001333
Iteration 311/1000 | Loss: 0.00001333
Iteration 312/1000 | Loss: 0.00001333
Iteration 313/1000 | Loss: 0.00001333
Iteration 314/1000 | Loss: 0.00001333
Iteration 315/1000 | Loss: 0.00001333
Iteration 316/1000 | Loss: 0.00001333
Iteration 317/1000 | Loss: 0.00001333
Iteration 318/1000 | Loss: 0.00001333
Iteration 319/1000 | Loss: 0.00001333
Iteration 320/1000 | Loss: 0.00001333
Iteration 321/1000 | Loss: 0.00001333
Iteration 322/1000 | Loss: 0.00001333
Iteration 323/1000 | Loss: 0.00001333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [1.3334489267435856e-05, 1.3334489267435856e-05, 1.3334489267435856e-05, 1.3334489267435856e-05, 1.3334489267435856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3334489267435856e-05

Optimization complete. Final v2v error: 3.0267767906188965 mm

Highest mean error: 5.072404861450195 mm for frame 52

Lowest mean error: 2.6630756855010986 mm for frame 30

Saving results

Total time: 1021.9675946235657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1009
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00557374
Iteration 2/25 | Loss: 0.00130934
Iteration 3/25 | Loss: 0.00120395
Iteration 4/25 | Loss: 0.00118674
Iteration 5/25 | Loss: 0.00118280
Iteration 6/25 | Loss: 0.00118273
Iteration 7/25 | Loss: 0.00118273
Iteration 8/25 | Loss: 0.00118273
Iteration 9/25 | Loss: 0.00118273
Iteration 10/25 | Loss: 0.00118273
Iteration 11/25 | Loss: 0.00118273
Iteration 12/25 | Loss: 0.00118273
Iteration 13/25 | Loss: 0.00118273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011827282141894102, 0.0011827282141894102, 0.0011827282141894102, 0.0011827282141894102, 0.0011827282141894102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011827282141894102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.17912245
Iteration 2/25 | Loss: 0.00171056
Iteration 3/25 | Loss: 0.00171055
Iteration 4/25 | Loss: 0.00171055
Iteration 5/25 | Loss: 0.00171055
Iteration 6/25 | Loss: 0.00171055
Iteration 7/25 | Loss: 0.00171055
Iteration 8/25 | Loss: 0.00171055
Iteration 9/25 | Loss: 0.00171055
Iteration 10/25 | Loss: 0.00171055
Iteration 11/25 | Loss: 0.00171055
Iteration 12/25 | Loss: 0.00171055
Iteration 13/25 | Loss: 0.00171055
Iteration 14/25 | Loss: 0.00171055
Iteration 15/25 | Loss: 0.00171055
Iteration 16/25 | Loss: 0.00171055
Iteration 17/25 | Loss: 0.00171055
Iteration 18/25 | Loss: 0.00171055
Iteration 19/25 | Loss: 0.00171055
Iteration 20/25 | Loss: 0.00171055
Iteration 21/25 | Loss: 0.00171055
Iteration 22/25 | Loss: 0.00171055
Iteration 23/25 | Loss: 0.00171055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001710548996925354, 0.001710548996925354, 0.001710548996925354, 0.001710548996925354, 0.001710548996925354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001710548996925354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171055
Iteration 2/1000 | Loss: 0.00002538
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001761
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001587
Iteration 9/1000 | Loss: 0.00001548
Iteration 10/1000 | Loss: 0.00001524
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001477
Iteration 13/1000 | Loss: 0.00001469
Iteration 14/1000 | Loss: 0.00001469
Iteration 15/1000 | Loss: 0.00001467
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001439
Iteration 18/1000 | Loss: 0.00001436
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001431
Iteration 21/1000 | Loss: 0.00001427
Iteration 22/1000 | Loss: 0.00001425
Iteration 23/1000 | Loss: 0.00001424
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001418
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001414
Iteration 36/1000 | Loss: 0.00001414
Iteration 37/1000 | Loss: 0.00001414
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001412
Iteration 40/1000 | Loss: 0.00001412
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001411
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001411
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001411
Iteration 55/1000 | Loss: 0.00001411
Iteration 56/1000 | Loss: 0.00001411
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001411
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001411
Iteration 63/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.4106485650700051e-05, 1.4106485650700051e-05, 1.4106485650700051e-05, 1.4106485650700051e-05, 1.4106485650700051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4106485650700051e-05

Optimization complete. Final v2v error: 3.2098350524902344 mm

Highest mean error: 3.5524282455444336 mm for frame 200

Lowest mean error: 2.8496429920196533 mm for frame 108

Saving results

Total time: 290.69934725761414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1037
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015465
Iteration 2/25 | Loss: 0.00156199
Iteration 3/25 | Loss: 0.00132667
Iteration 4/25 | Loss: 0.00130439
Iteration 5/25 | Loss: 0.00129708
Iteration 6/25 | Loss: 0.00129567
Iteration 7/25 | Loss: 0.00129567
Iteration 8/25 | Loss: 0.00129567
Iteration 9/25 | Loss: 0.00129567
Iteration 10/25 | Loss: 0.00129567
Iteration 11/25 | Loss: 0.00129567
Iteration 12/25 | Loss: 0.00129567
Iteration 13/25 | Loss: 0.00129567
Iteration 14/25 | Loss: 0.00129567
Iteration 15/25 | Loss: 0.00129567
Iteration 16/25 | Loss: 0.00129567
Iteration 17/25 | Loss: 0.00129567
Iteration 18/25 | Loss: 0.00129567
Iteration 19/25 | Loss: 0.00129567
Iteration 20/25 | Loss: 0.00129567
Iteration 21/25 | Loss: 0.00129567
Iteration 22/25 | Loss: 0.00129567
Iteration 23/25 | Loss: 0.00129567
Iteration 24/25 | Loss: 0.00129567
Iteration 25/25 | Loss: 0.00129567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88580698
Iteration 2/25 | Loss: 0.00193636
Iteration 3/25 | Loss: 0.00193636
Iteration 4/25 | Loss: 0.00193636
Iteration 5/25 | Loss: 0.00193636
Iteration 6/25 | Loss: 0.00193636
Iteration 7/25 | Loss: 0.00193636
Iteration 8/25 | Loss: 0.00193636
Iteration 9/25 | Loss: 0.00193636
Iteration 10/25 | Loss: 0.00193636
Iteration 11/25 | Loss: 0.00193636
Iteration 12/25 | Loss: 0.00193636
Iteration 13/25 | Loss: 0.00193636
Iteration 14/25 | Loss: 0.00193636
Iteration 15/25 | Loss: 0.00193636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001936360844410956, 0.001936360844410956, 0.001936360844410956, 0.001936360844410956, 0.001936360844410956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001936360844410956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193636
Iteration 2/1000 | Loss: 0.00007044
Iteration 3/1000 | Loss: 0.00004226
Iteration 4/1000 | Loss: 0.00003232
Iteration 5/1000 | Loss: 0.00002930
Iteration 6/1000 | Loss: 0.00002773
Iteration 7/1000 | Loss: 0.00002698
Iteration 8/1000 | Loss: 0.00002639
Iteration 9/1000 | Loss: 0.00002582
Iteration 10/1000 | Loss: 0.00002539
Iteration 11/1000 | Loss: 0.00002496
Iteration 12/1000 | Loss: 0.00002469
Iteration 13/1000 | Loss: 0.00002454
Iteration 14/1000 | Loss: 0.00002435
Iteration 15/1000 | Loss: 0.00002418
Iteration 16/1000 | Loss: 0.00002409
Iteration 17/1000 | Loss: 0.00002406
Iteration 18/1000 | Loss: 0.00002405
Iteration 19/1000 | Loss: 0.00002400
Iteration 20/1000 | Loss: 0.00002386
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002369
Iteration 23/1000 | Loss: 0.00002367
Iteration 24/1000 | Loss: 0.00002364
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002362
Iteration 28/1000 | Loss: 0.00002358
Iteration 29/1000 | Loss: 0.00002358
Iteration 30/1000 | Loss: 0.00002358
Iteration 31/1000 | Loss: 0.00002357
Iteration 32/1000 | Loss: 0.00002356
Iteration 33/1000 | Loss: 0.00002356
Iteration 34/1000 | Loss: 0.00002356
Iteration 35/1000 | Loss: 0.00002353
Iteration 36/1000 | Loss: 0.00002353
Iteration 37/1000 | Loss: 0.00002353
Iteration 38/1000 | Loss: 0.00002353
Iteration 39/1000 | Loss: 0.00002353
Iteration 40/1000 | Loss: 0.00002353
Iteration 41/1000 | Loss: 0.00002353
Iteration 42/1000 | Loss: 0.00002353
Iteration 43/1000 | Loss: 0.00002352
Iteration 44/1000 | Loss: 0.00002352
Iteration 45/1000 | Loss: 0.00002352
Iteration 46/1000 | Loss: 0.00002352
Iteration 47/1000 | Loss: 0.00002352
Iteration 48/1000 | Loss: 0.00002352
Iteration 49/1000 | Loss: 0.00002352
Iteration 50/1000 | Loss: 0.00002351
Iteration 51/1000 | Loss: 0.00002351
Iteration 52/1000 | Loss: 0.00002351
Iteration 53/1000 | Loss: 0.00002350
Iteration 54/1000 | Loss: 0.00002350
Iteration 55/1000 | Loss: 0.00002350
Iteration 56/1000 | Loss: 0.00002350
Iteration 57/1000 | Loss: 0.00002350
Iteration 58/1000 | Loss: 0.00002349
Iteration 59/1000 | Loss: 0.00002349
Iteration 60/1000 | Loss: 0.00002349
Iteration 61/1000 | Loss: 0.00002349
Iteration 62/1000 | Loss: 0.00002349
Iteration 63/1000 | Loss: 0.00002348
Iteration 64/1000 | Loss: 0.00002348
Iteration 65/1000 | Loss: 0.00002348
Iteration 66/1000 | Loss: 0.00002348
Iteration 67/1000 | Loss: 0.00002348
Iteration 68/1000 | Loss: 0.00002347
Iteration 69/1000 | Loss: 0.00002347
Iteration 70/1000 | Loss: 0.00002347
Iteration 71/1000 | Loss: 0.00002347
Iteration 72/1000 | Loss: 0.00002347
Iteration 73/1000 | Loss: 0.00002347
Iteration 74/1000 | Loss: 0.00002347
Iteration 75/1000 | Loss: 0.00002347
Iteration 76/1000 | Loss: 0.00002347
Iteration 77/1000 | Loss: 0.00002346
Iteration 78/1000 | Loss: 0.00002346
Iteration 79/1000 | Loss: 0.00002345
Iteration 80/1000 | Loss: 0.00002345
Iteration 81/1000 | Loss: 0.00002345
Iteration 82/1000 | Loss: 0.00002345
Iteration 83/1000 | Loss: 0.00002344
Iteration 84/1000 | Loss: 0.00002344
Iteration 85/1000 | Loss: 0.00002344
Iteration 86/1000 | Loss: 0.00002343
Iteration 87/1000 | Loss: 0.00002343
Iteration 88/1000 | Loss: 0.00002343
Iteration 89/1000 | Loss: 0.00002343
Iteration 90/1000 | Loss: 0.00002342
Iteration 91/1000 | Loss: 0.00002342
Iteration 92/1000 | Loss: 0.00002342
Iteration 93/1000 | Loss: 0.00002342
Iteration 94/1000 | Loss: 0.00002342
Iteration 95/1000 | Loss: 0.00002341
Iteration 96/1000 | Loss: 0.00002341
Iteration 97/1000 | Loss: 0.00002341
Iteration 98/1000 | Loss: 0.00002341
Iteration 99/1000 | Loss: 0.00002341
Iteration 100/1000 | Loss: 0.00002341
Iteration 101/1000 | Loss: 0.00002341
Iteration 102/1000 | Loss: 0.00002341
Iteration 103/1000 | Loss: 0.00002341
Iteration 104/1000 | Loss: 0.00002340
Iteration 105/1000 | Loss: 0.00002340
Iteration 106/1000 | Loss: 0.00002340
Iteration 107/1000 | Loss: 0.00002340
Iteration 108/1000 | Loss: 0.00002340
Iteration 109/1000 | Loss: 0.00002339
Iteration 110/1000 | Loss: 0.00002339
Iteration 111/1000 | Loss: 0.00002339
Iteration 112/1000 | Loss: 0.00002339
Iteration 113/1000 | Loss: 0.00002339
Iteration 114/1000 | Loss: 0.00002339
Iteration 115/1000 | Loss: 0.00002339
Iteration 116/1000 | Loss: 0.00002339
Iteration 117/1000 | Loss: 0.00002339
Iteration 118/1000 | Loss: 0.00002338
Iteration 119/1000 | Loss: 0.00002338
Iteration 120/1000 | Loss: 0.00002338
Iteration 121/1000 | Loss: 0.00002338
Iteration 122/1000 | Loss: 0.00002338
Iteration 123/1000 | Loss: 0.00002338
Iteration 124/1000 | Loss: 0.00002338
Iteration 125/1000 | Loss: 0.00002338
Iteration 126/1000 | Loss: 0.00002338
Iteration 127/1000 | Loss: 0.00002338
Iteration 128/1000 | Loss: 0.00002338
Iteration 129/1000 | Loss: 0.00002338
Iteration 130/1000 | Loss: 0.00002338
Iteration 131/1000 | Loss: 0.00002338
Iteration 132/1000 | Loss: 0.00002338
Iteration 133/1000 | Loss: 0.00002338
Iteration 134/1000 | Loss: 0.00002338
Iteration 135/1000 | Loss: 0.00002338
Iteration 136/1000 | Loss: 0.00002338
Iteration 137/1000 | Loss: 0.00002338
Iteration 138/1000 | Loss: 0.00002338
Iteration 139/1000 | Loss: 0.00002338
Iteration 140/1000 | Loss: 0.00002338
Iteration 141/1000 | Loss: 0.00002338
Iteration 142/1000 | Loss: 0.00002338
Iteration 143/1000 | Loss: 0.00002338
Iteration 144/1000 | Loss: 0.00002338
Iteration 145/1000 | Loss: 0.00002338
Iteration 146/1000 | Loss: 0.00002338
Iteration 147/1000 | Loss: 0.00002338
Iteration 148/1000 | Loss: 0.00002338
Iteration 149/1000 | Loss: 0.00002338
Iteration 150/1000 | Loss: 0.00002338
Iteration 151/1000 | Loss: 0.00002338
Iteration 152/1000 | Loss: 0.00002338
Iteration 153/1000 | Loss: 0.00002338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.338152080483269e-05, 2.338152080483269e-05, 2.338152080483269e-05, 2.338152080483269e-05, 2.338152080483269e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.338152080483269e-05

Optimization complete. Final v2v error: 4.006450176239014 mm

Highest mean error: 4.810846328735352 mm for frame 66

Lowest mean error: 3.299980640411377 mm for frame 27

Saving results

Total time: 349.5729093551636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1075
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472644
Iteration 2/25 | Loss: 0.00123148
Iteration 3/25 | Loss: 0.00116333
Iteration 4/25 | Loss: 0.00114953
Iteration 5/25 | Loss: 0.00114497
Iteration 6/25 | Loss: 0.00114371
Iteration 7/25 | Loss: 0.00114371
Iteration 8/25 | Loss: 0.00114371
Iteration 9/25 | Loss: 0.00114371
Iteration 10/25 | Loss: 0.00114371
Iteration 11/25 | Loss: 0.00114371
Iteration 12/25 | Loss: 0.00114371
Iteration 13/25 | Loss: 0.00114371
Iteration 14/25 | Loss: 0.00114371
Iteration 15/25 | Loss: 0.00114371
Iteration 16/25 | Loss: 0.00114371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001143713598139584, 0.001143713598139584, 0.001143713598139584, 0.001143713598139584, 0.001143713598139584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001143713598139584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.19642353
Iteration 2/25 | Loss: 0.00175658
Iteration 3/25 | Loss: 0.00175657
Iteration 4/25 | Loss: 0.00175657
Iteration 5/25 | Loss: 0.00175657
Iteration 6/25 | Loss: 0.00175657
Iteration 7/25 | Loss: 0.00175657
Iteration 8/25 | Loss: 0.00175657
Iteration 9/25 | Loss: 0.00175657
Iteration 10/25 | Loss: 0.00175657
Iteration 11/25 | Loss: 0.00175657
Iteration 12/25 | Loss: 0.00175657
Iteration 13/25 | Loss: 0.00175657
Iteration 14/25 | Loss: 0.00175657
Iteration 15/25 | Loss: 0.00175657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0017565730959177017, 0.0017565730959177017, 0.0017565730959177017, 0.0017565730959177017, 0.0017565730959177017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017565730959177017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175657
Iteration 2/1000 | Loss: 0.00002767
Iteration 3/1000 | Loss: 0.00001753
Iteration 4/1000 | Loss: 0.00001487
Iteration 5/1000 | Loss: 0.00001354
Iteration 6/1000 | Loss: 0.00001285
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001140
Iteration 10/1000 | Loss: 0.00001111
Iteration 11/1000 | Loss: 0.00001084
Iteration 12/1000 | Loss: 0.00001082
Iteration 13/1000 | Loss: 0.00001079
Iteration 14/1000 | Loss: 0.00001060
Iteration 15/1000 | Loss: 0.00001060
Iteration 16/1000 | Loss: 0.00001057
Iteration 17/1000 | Loss: 0.00001055
Iteration 18/1000 | Loss: 0.00001050
Iteration 19/1000 | Loss: 0.00001049
Iteration 20/1000 | Loss: 0.00001048
Iteration 21/1000 | Loss: 0.00001042
Iteration 22/1000 | Loss: 0.00001041
Iteration 23/1000 | Loss: 0.00001040
Iteration 24/1000 | Loss: 0.00001040
Iteration 25/1000 | Loss: 0.00001030
Iteration 26/1000 | Loss: 0.00001029
Iteration 27/1000 | Loss: 0.00001026
Iteration 28/1000 | Loss: 0.00001024
Iteration 29/1000 | Loss: 0.00001018
Iteration 30/1000 | Loss: 0.00001018
Iteration 31/1000 | Loss: 0.00001017
Iteration 32/1000 | Loss: 0.00001017
Iteration 33/1000 | Loss: 0.00001016
Iteration 34/1000 | Loss: 0.00001015
Iteration 35/1000 | Loss: 0.00001013
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001012
Iteration 38/1000 | Loss: 0.00001012
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001010
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001006
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001005
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001005
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001001
Iteration 70/1000 | Loss: 0.00001000
Iteration 71/1000 | Loss: 0.00001000
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00001000
Iteration 74/1000 | Loss: 0.00000999
Iteration 75/1000 | Loss: 0.00000999
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000998
Iteration 82/1000 | Loss: 0.00000998
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000998
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00000998
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000997
Iteration 90/1000 | Loss: 0.00000997
Iteration 91/1000 | Loss: 0.00000997
Iteration 92/1000 | Loss: 0.00000997
Iteration 93/1000 | Loss: 0.00000997
Iteration 94/1000 | Loss: 0.00000996
Iteration 95/1000 | Loss: 0.00000996
Iteration 96/1000 | Loss: 0.00000996
Iteration 97/1000 | Loss: 0.00000996
Iteration 98/1000 | Loss: 0.00000996
Iteration 99/1000 | Loss: 0.00000996
Iteration 100/1000 | Loss: 0.00000996
Iteration 101/1000 | Loss: 0.00000995
Iteration 102/1000 | Loss: 0.00000995
Iteration 103/1000 | Loss: 0.00000995
Iteration 104/1000 | Loss: 0.00000995
Iteration 105/1000 | Loss: 0.00000995
Iteration 106/1000 | Loss: 0.00000995
Iteration 107/1000 | Loss: 0.00000995
Iteration 108/1000 | Loss: 0.00000995
Iteration 109/1000 | Loss: 0.00000995
Iteration 110/1000 | Loss: 0.00000995
Iteration 111/1000 | Loss: 0.00000994
Iteration 112/1000 | Loss: 0.00000994
Iteration 113/1000 | Loss: 0.00000994
Iteration 114/1000 | Loss: 0.00000994
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000994
Iteration 117/1000 | Loss: 0.00000994
Iteration 118/1000 | Loss: 0.00000994
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000993
Iteration 122/1000 | Loss: 0.00000993
Iteration 123/1000 | Loss: 0.00000993
Iteration 124/1000 | Loss: 0.00000992
Iteration 125/1000 | Loss: 0.00000992
Iteration 126/1000 | Loss: 0.00000992
Iteration 127/1000 | Loss: 0.00000991
Iteration 128/1000 | Loss: 0.00000991
Iteration 129/1000 | Loss: 0.00000991
Iteration 130/1000 | Loss: 0.00000991
Iteration 131/1000 | Loss: 0.00000991
Iteration 132/1000 | Loss: 0.00000990
Iteration 133/1000 | Loss: 0.00000990
Iteration 134/1000 | Loss: 0.00000990
Iteration 135/1000 | Loss: 0.00000990
Iteration 136/1000 | Loss: 0.00000990
Iteration 137/1000 | Loss: 0.00000990
Iteration 138/1000 | Loss: 0.00000990
Iteration 139/1000 | Loss: 0.00000990
Iteration 140/1000 | Loss: 0.00000990
Iteration 141/1000 | Loss: 0.00000990
Iteration 142/1000 | Loss: 0.00000990
Iteration 143/1000 | Loss: 0.00000989
Iteration 144/1000 | Loss: 0.00000989
Iteration 145/1000 | Loss: 0.00000989
Iteration 146/1000 | Loss: 0.00000989
Iteration 147/1000 | Loss: 0.00000989
Iteration 148/1000 | Loss: 0.00000989
Iteration 149/1000 | Loss: 0.00000989
Iteration 150/1000 | Loss: 0.00000989
Iteration 151/1000 | Loss: 0.00000989
Iteration 152/1000 | Loss: 0.00000989
Iteration 153/1000 | Loss: 0.00000989
Iteration 154/1000 | Loss: 0.00000988
Iteration 155/1000 | Loss: 0.00000988
Iteration 156/1000 | Loss: 0.00000988
Iteration 157/1000 | Loss: 0.00000988
Iteration 158/1000 | Loss: 0.00000988
Iteration 159/1000 | Loss: 0.00000988
Iteration 160/1000 | Loss: 0.00000988
Iteration 161/1000 | Loss: 0.00000988
Iteration 162/1000 | Loss: 0.00000988
Iteration 163/1000 | Loss: 0.00000988
Iteration 164/1000 | Loss: 0.00000988
Iteration 165/1000 | Loss: 0.00000988
Iteration 166/1000 | Loss: 0.00000988
Iteration 167/1000 | Loss: 0.00000988
Iteration 168/1000 | Loss: 0.00000988
Iteration 169/1000 | Loss: 0.00000988
Iteration 170/1000 | Loss: 0.00000987
Iteration 171/1000 | Loss: 0.00000987
Iteration 172/1000 | Loss: 0.00000987
Iteration 173/1000 | Loss: 0.00000987
Iteration 174/1000 | Loss: 0.00000987
Iteration 175/1000 | Loss: 0.00000987
Iteration 176/1000 | Loss: 0.00000987
Iteration 177/1000 | Loss: 0.00000987
Iteration 178/1000 | Loss: 0.00000987
Iteration 179/1000 | Loss: 0.00000987
Iteration 180/1000 | Loss: 0.00000987
Iteration 181/1000 | Loss: 0.00000987
Iteration 182/1000 | Loss: 0.00000987
Iteration 183/1000 | Loss: 0.00000987
Iteration 184/1000 | Loss: 0.00000987
Iteration 185/1000 | Loss: 0.00000987
Iteration 186/1000 | Loss: 0.00000987
Iteration 187/1000 | Loss: 0.00000987
Iteration 188/1000 | Loss: 0.00000987
Iteration 189/1000 | Loss: 0.00000987
Iteration 190/1000 | Loss: 0.00000987
Iteration 191/1000 | Loss: 0.00000987
Iteration 192/1000 | Loss: 0.00000987
Iteration 193/1000 | Loss: 0.00000987
Iteration 194/1000 | Loss: 0.00000987
Iteration 195/1000 | Loss: 0.00000987
Iteration 196/1000 | Loss: 0.00000987
Iteration 197/1000 | Loss: 0.00000987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [9.87296243692981e-06, 9.87296243692981e-06, 9.87296243692981e-06, 9.87296243692981e-06, 9.87296243692981e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.87296243692981e-06

Optimization complete. Final v2v error: 2.7438759803771973 mm

Highest mean error: 3.016397476196289 mm for frame 109

Lowest mean error: 2.549443483352661 mm for frame 7

Saving results

Total time: 235.75056910514832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1051
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386469
Iteration 2/25 | Loss: 0.00122579
Iteration 3/25 | Loss: 0.00114157
Iteration 4/25 | Loss: 0.00113313
Iteration 5/25 | Loss: 0.00113135
Iteration 6/25 | Loss: 0.00113092
Iteration 7/25 | Loss: 0.00113092
Iteration 8/25 | Loss: 0.00113092
Iteration 9/25 | Loss: 0.00113092
Iteration 10/25 | Loss: 0.00113092
Iteration 11/25 | Loss: 0.00113092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001130924909375608, 0.001130924909375608, 0.001130924909375608, 0.001130924909375608, 0.001130924909375608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001130924909375608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25146198
Iteration 2/25 | Loss: 0.00196315
Iteration 3/25 | Loss: 0.00196315
Iteration 4/25 | Loss: 0.00196315
Iteration 5/25 | Loss: 0.00196315
Iteration 6/25 | Loss: 0.00196315
Iteration 7/25 | Loss: 0.00196315
Iteration 8/25 | Loss: 0.00196315
Iteration 9/25 | Loss: 0.00196315
Iteration 10/25 | Loss: 0.00196315
Iteration 11/25 | Loss: 0.00196315
Iteration 12/25 | Loss: 0.00196315
Iteration 13/25 | Loss: 0.00196315
Iteration 14/25 | Loss: 0.00196315
Iteration 15/25 | Loss: 0.00196315
Iteration 16/25 | Loss: 0.00196315
Iteration 17/25 | Loss: 0.00196315
Iteration 18/25 | Loss: 0.00196315
Iteration 19/25 | Loss: 0.00196315
Iteration 20/25 | Loss: 0.00196315
Iteration 21/25 | Loss: 0.00196315
Iteration 22/25 | Loss: 0.00196315
Iteration 23/25 | Loss: 0.00196315
Iteration 24/25 | Loss: 0.00196315
Iteration 25/25 | Loss: 0.00196315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196315
Iteration 2/1000 | Loss: 0.00002218
Iteration 3/1000 | Loss: 0.00001490
Iteration 4/1000 | Loss: 0.00001232
Iteration 5/1000 | Loss: 0.00001104
Iteration 6/1000 | Loss: 0.00001032
Iteration 7/1000 | Loss: 0.00000987
Iteration 8/1000 | Loss: 0.00000950
Iteration 9/1000 | Loss: 0.00000926
Iteration 10/1000 | Loss: 0.00000899
Iteration 11/1000 | Loss: 0.00000893
Iteration 12/1000 | Loss: 0.00000892
Iteration 13/1000 | Loss: 0.00000891
Iteration 14/1000 | Loss: 0.00000890
Iteration 15/1000 | Loss: 0.00000875
Iteration 16/1000 | Loss: 0.00000872
Iteration 17/1000 | Loss: 0.00000871
Iteration 18/1000 | Loss: 0.00000865
Iteration 19/1000 | Loss: 0.00000865
Iteration 20/1000 | Loss: 0.00000864
Iteration 21/1000 | Loss: 0.00000861
Iteration 22/1000 | Loss: 0.00000861
Iteration 23/1000 | Loss: 0.00000858
Iteration 24/1000 | Loss: 0.00000857
Iteration 25/1000 | Loss: 0.00000855
Iteration 26/1000 | Loss: 0.00000855
Iteration 27/1000 | Loss: 0.00000854
Iteration 28/1000 | Loss: 0.00000854
Iteration 29/1000 | Loss: 0.00000854
Iteration 30/1000 | Loss: 0.00000853
Iteration 31/1000 | Loss: 0.00000853
Iteration 32/1000 | Loss: 0.00000850
Iteration 33/1000 | Loss: 0.00000849
Iteration 34/1000 | Loss: 0.00000848
Iteration 35/1000 | Loss: 0.00000847
Iteration 36/1000 | Loss: 0.00000845
Iteration 37/1000 | Loss: 0.00000845
Iteration 38/1000 | Loss: 0.00000844
Iteration 39/1000 | Loss: 0.00000840
Iteration 40/1000 | Loss: 0.00000838
Iteration 41/1000 | Loss: 0.00000837
Iteration 42/1000 | Loss: 0.00000837
Iteration 43/1000 | Loss: 0.00000837
Iteration 44/1000 | Loss: 0.00000836
Iteration 45/1000 | Loss: 0.00000835
Iteration 46/1000 | Loss: 0.00000832
Iteration 47/1000 | Loss: 0.00000831
Iteration 48/1000 | Loss: 0.00000830
Iteration 49/1000 | Loss: 0.00000830
Iteration 50/1000 | Loss: 0.00000829
Iteration 51/1000 | Loss: 0.00000829
Iteration 52/1000 | Loss: 0.00000828
Iteration 53/1000 | Loss: 0.00000828
Iteration 54/1000 | Loss: 0.00000828
Iteration 55/1000 | Loss: 0.00000827
Iteration 56/1000 | Loss: 0.00000827
Iteration 57/1000 | Loss: 0.00000827
Iteration 58/1000 | Loss: 0.00000827
Iteration 59/1000 | Loss: 0.00000826
Iteration 60/1000 | Loss: 0.00000826
Iteration 61/1000 | Loss: 0.00000826
Iteration 62/1000 | Loss: 0.00000826
Iteration 63/1000 | Loss: 0.00000826
Iteration 64/1000 | Loss: 0.00000826
Iteration 65/1000 | Loss: 0.00000826
Iteration 66/1000 | Loss: 0.00000826
Iteration 67/1000 | Loss: 0.00000825
Iteration 68/1000 | Loss: 0.00000825
Iteration 69/1000 | Loss: 0.00000825
Iteration 70/1000 | Loss: 0.00000825
Iteration 71/1000 | Loss: 0.00000824
Iteration 72/1000 | Loss: 0.00000824
Iteration 73/1000 | Loss: 0.00000824
Iteration 74/1000 | Loss: 0.00000824
Iteration 75/1000 | Loss: 0.00000823
Iteration 76/1000 | Loss: 0.00000823
Iteration 77/1000 | Loss: 0.00000823
Iteration 78/1000 | Loss: 0.00000823
Iteration 79/1000 | Loss: 0.00000823
Iteration 80/1000 | Loss: 0.00000823
Iteration 81/1000 | Loss: 0.00000822
Iteration 82/1000 | Loss: 0.00000822
Iteration 83/1000 | Loss: 0.00000822
Iteration 84/1000 | Loss: 0.00000821
Iteration 85/1000 | Loss: 0.00000821
Iteration 86/1000 | Loss: 0.00000820
Iteration 87/1000 | Loss: 0.00000820
Iteration 88/1000 | Loss: 0.00000820
Iteration 89/1000 | Loss: 0.00000820
Iteration 90/1000 | Loss: 0.00000820
Iteration 91/1000 | Loss: 0.00000820
Iteration 92/1000 | Loss: 0.00000819
Iteration 93/1000 | Loss: 0.00000819
Iteration 94/1000 | Loss: 0.00000819
Iteration 95/1000 | Loss: 0.00000819
Iteration 96/1000 | Loss: 0.00000819
Iteration 97/1000 | Loss: 0.00000819
Iteration 98/1000 | Loss: 0.00000819
Iteration 99/1000 | Loss: 0.00000819
Iteration 100/1000 | Loss: 0.00000819
Iteration 101/1000 | Loss: 0.00000819
Iteration 102/1000 | Loss: 0.00000819
Iteration 103/1000 | Loss: 0.00000819
Iteration 104/1000 | Loss: 0.00000818
Iteration 105/1000 | Loss: 0.00000818
Iteration 106/1000 | Loss: 0.00000818
Iteration 107/1000 | Loss: 0.00000818
Iteration 108/1000 | Loss: 0.00000818
Iteration 109/1000 | Loss: 0.00000818
Iteration 110/1000 | Loss: 0.00000818
Iteration 111/1000 | Loss: 0.00000818
Iteration 112/1000 | Loss: 0.00000818
Iteration 113/1000 | Loss: 0.00000818
Iteration 114/1000 | Loss: 0.00000818
Iteration 115/1000 | Loss: 0.00000818
Iteration 116/1000 | Loss: 0.00000818
Iteration 117/1000 | Loss: 0.00000818
Iteration 118/1000 | Loss: 0.00000818
Iteration 119/1000 | Loss: 0.00000818
Iteration 120/1000 | Loss: 0.00000818
Iteration 121/1000 | Loss: 0.00000818
Iteration 122/1000 | Loss: 0.00000817
Iteration 123/1000 | Loss: 0.00000817
Iteration 124/1000 | Loss: 0.00000817
Iteration 125/1000 | Loss: 0.00000817
Iteration 126/1000 | Loss: 0.00000817
Iteration 127/1000 | Loss: 0.00000817
Iteration 128/1000 | Loss: 0.00000817
Iteration 129/1000 | Loss: 0.00000817
Iteration 130/1000 | Loss: 0.00000817
Iteration 131/1000 | Loss: 0.00000816
Iteration 132/1000 | Loss: 0.00000816
Iteration 133/1000 | Loss: 0.00000816
Iteration 134/1000 | Loss: 0.00000816
Iteration 135/1000 | Loss: 0.00000816
Iteration 136/1000 | Loss: 0.00000816
Iteration 137/1000 | Loss: 0.00000816
Iteration 138/1000 | Loss: 0.00000816
Iteration 139/1000 | Loss: 0.00000816
Iteration 140/1000 | Loss: 0.00000815
Iteration 141/1000 | Loss: 0.00000815
Iteration 142/1000 | Loss: 0.00000815
Iteration 143/1000 | Loss: 0.00000815
Iteration 144/1000 | Loss: 0.00000815
Iteration 145/1000 | Loss: 0.00000815
Iteration 146/1000 | Loss: 0.00000815
Iteration 147/1000 | Loss: 0.00000815
Iteration 148/1000 | Loss: 0.00000815
Iteration 149/1000 | Loss: 0.00000815
Iteration 150/1000 | Loss: 0.00000815
Iteration 151/1000 | Loss: 0.00000814
Iteration 152/1000 | Loss: 0.00000814
Iteration 153/1000 | Loss: 0.00000814
Iteration 154/1000 | Loss: 0.00000814
Iteration 155/1000 | Loss: 0.00000814
Iteration 156/1000 | Loss: 0.00000814
Iteration 157/1000 | Loss: 0.00000814
Iteration 158/1000 | Loss: 0.00000814
Iteration 159/1000 | Loss: 0.00000814
Iteration 160/1000 | Loss: 0.00000814
Iteration 161/1000 | Loss: 0.00000814
Iteration 162/1000 | Loss: 0.00000814
Iteration 163/1000 | Loss: 0.00000814
Iteration 164/1000 | Loss: 0.00000813
Iteration 165/1000 | Loss: 0.00000813
Iteration 166/1000 | Loss: 0.00000813
Iteration 167/1000 | Loss: 0.00000813
Iteration 168/1000 | Loss: 0.00000813
Iteration 169/1000 | Loss: 0.00000813
Iteration 170/1000 | Loss: 0.00000813
Iteration 171/1000 | Loss: 0.00000813
Iteration 172/1000 | Loss: 0.00000813
Iteration 173/1000 | Loss: 0.00000813
Iteration 174/1000 | Loss: 0.00000813
Iteration 175/1000 | Loss: 0.00000813
Iteration 176/1000 | Loss: 0.00000813
Iteration 177/1000 | Loss: 0.00000813
Iteration 178/1000 | Loss: 0.00000813
Iteration 179/1000 | Loss: 0.00000813
Iteration 180/1000 | Loss: 0.00000812
Iteration 181/1000 | Loss: 0.00000812
Iteration 182/1000 | Loss: 0.00000812
Iteration 183/1000 | Loss: 0.00000812
Iteration 184/1000 | Loss: 0.00000812
Iteration 185/1000 | Loss: 0.00000812
Iteration 186/1000 | Loss: 0.00000812
Iteration 187/1000 | Loss: 0.00000811
Iteration 188/1000 | Loss: 0.00000811
Iteration 189/1000 | Loss: 0.00000811
Iteration 190/1000 | Loss: 0.00000811
Iteration 191/1000 | Loss: 0.00000811
Iteration 192/1000 | Loss: 0.00000811
Iteration 193/1000 | Loss: 0.00000811
Iteration 194/1000 | Loss: 0.00000811
Iteration 195/1000 | Loss: 0.00000811
Iteration 196/1000 | Loss: 0.00000811
Iteration 197/1000 | Loss: 0.00000810
Iteration 198/1000 | Loss: 0.00000810
Iteration 199/1000 | Loss: 0.00000810
Iteration 200/1000 | Loss: 0.00000810
Iteration 201/1000 | Loss: 0.00000810
Iteration 202/1000 | Loss: 0.00000810
Iteration 203/1000 | Loss: 0.00000810
Iteration 204/1000 | Loss: 0.00000810
Iteration 205/1000 | Loss: 0.00000810
Iteration 206/1000 | Loss: 0.00000810
Iteration 207/1000 | Loss: 0.00000810
Iteration 208/1000 | Loss: 0.00000810
Iteration 209/1000 | Loss: 0.00000810
Iteration 210/1000 | Loss: 0.00000810
Iteration 211/1000 | Loss: 0.00000810
Iteration 212/1000 | Loss: 0.00000810
Iteration 213/1000 | Loss: 0.00000810
Iteration 214/1000 | Loss: 0.00000810
Iteration 215/1000 | Loss: 0.00000810
Iteration 216/1000 | Loss: 0.00000810
Iteration 217/1000 | Loss: 0.00000810
Iteration 218/1000 | Loss: 0.00000810
Iteration 219/1000 | Loss: 0.00000810
Iteration 220/1000 | Loss: 0.00000810
Iteration 221/1000 | Loss: 0.00000810
Iteration 222/1000 | Loss: 0.00000810
Iteration 223/1000 | Loss: 0.00000810
Iteration 224/1000 | Loss: 0.00000810
Iteration 225/1000 | Loss: 0.00000810
Iteration 226/1000 | Loss: 0.00000810
Iteration 227/1000 | Loss: 0.00000810
Iteration 228/1000 | Loss: 0.00000810
Iteration 229/1000 | Loss: 0.00000810
Iteration 230/1000 | Loss: 0.00000810
Iteration 231/1000 | Loss: 0.00000810
Iteration 232/1000 | Loss: 0.00000810
Iteration 233/1000 | Loss: 0.00000810
Iteration 234/1000 | Loss: 0.00000810
Iteration 235/1000 | Loss: 0.00000810
Iteration 236/1000 | Loss: 0.00000810
Iteration 237/1000 | Loss: 0.00000810
Iteration 238/1000 | Loss: 0.00000810
Iteration 239/1000 | Loss: 0.00000810
Iteration 240/1000 | Loss: 0.00000810
Iteration 241/1000 | Loss: 0.00000810
Iteration 242/1000 | Loss: 0.00000810
Iteration 243/1000 | Loss: 0.00000810
Iteration 244/1000 | Loss: 0.00000810
Iteration 245/1000 | Loss: 0.00000810
Iteration 246/1000 | Loss: 0.00000810
Iteration 247/1000 | Loss: 0.00000810
Iteration 248/1000 | Loss: 0.00000810
Iteration 249/1000 | Loss: 0.00000810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [8.098017133306712e-06, 8.098017133306712e-06, 8.098017133306712e-06, 8.098017133306712e-06, 8.098017133306712e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.098017133306712e-06

Optimization complete. Final v2v error: 2.428781509399414 mm

Highest mean error: 3.4863460063934326 mm for frame 69

Lowest mean error: 2.289654016494751 mm for frame 129

Saving results

Total time: 284.6449863910675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1055
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791199
Iteration 2/25 | Loss: 0.00158373
Iteration 3/25 | Loss: 0.00128114
Iteration 4/25 | Loss: 0.00126107
Iteration 5/25 | Loss: 0.00125898
Iteration 6/25 | Loss: 0.00125898
Iteration 7/25 | Loss: 0.00125898
Iteration 8/25 | Loss: 0.00125898
Iteration 9/25 | Loss: 0.00125898
Iteration 10/25 | Loss: 0.00125898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012589803664013743, 0.0012589803664013743, 0.0012589803664013743, 0.0012589803664013743, 0.0012589803664013743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012589803664013743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27236056
Iteration 2/25 | Loss: 0.00133955
Iteration 3/25 | Loss: 0.00133955
Iteration 4/25 | Loss: 0.00133955
Iteration 5/25 | Loss: 0.00133955
Iteration 6/25 | Loss: 0.00133955
Iteration 7/25 | Loss: 0.00133955
Iteration 8/25 | Loss: 0.00133955
Iteration 9/25 | Loss: 0.00133955
Iteration 10/25 | Loss: 0.00133955
Iteration 11/25 | Loss: 0.00133955
Iteration 12/25 | Loss: 0.00133955
Iteration 13/25 | Loss: 0.00133955
Iteration 14/25 | Loss: 0.00133955
Iteration 15/25 | Loss: 0.00133955
Iteration 16/25 | Loss: 0.00133955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013395451242104173, 0.0013395451242104173, 0.0013395451242104173, 0.0013395451242104173, 0.0013395451242104173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013395451242104173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133955
Iteration 2/1000 | Loss: 0.00004267
Iteration 3/1000 | Loss: 0.00002755
Iteration 4/1000 | Loss: 0.00002442
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002250
Iteration 7/1000 | Loss: 0.00002195
Iteration 8/1000 | Loss: 0.00002145
Iteration 9/1000 | Loss: 0.00002098
Iteration 10/1000 | Loss: 0.00002072
Iteration 11/1000 | Loss: 0.00002049
Iteration 12/1000 | Loss: 0.00002026
Iteration 13/1000 | Loss: 0.00002011
Iteration 14/1000 | Loss: 0.00002009
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00002001
Iteration 17/1000 | Loss: 0.00001999
Iteration 18/1000 | Loss: 0.00001998
Iteration 19/1000 | Loss: 0.00001998
Iteration 20/1000 | Loss: 0.00001998
Iteration 21/1000 | Loss: 0.00001998
Iteration 22/1000 | Loss: 0.00001997
Iteration 23/1000 | Loss: 0.00001997
Iteration 24/1000 | Loss: 0.00001997
Iteration 25/1000 | Loss: 0.00001997
Iteration 26/1000 | Loss: 0.00001997
Iteration 27/1000 | Loss: 0.00001997
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001996
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001996
Iteration 36/1000 | Loss: 0.00001994
Iteration 37/1000 | Loss: 0.00001994
Iteration 38/1000 | Loss: 0.00001994
Iteration 39/1000 | Loss: 0.00001994
Iteration 40/1000 | Loss: 0.00001994
Iteration 41/1000 | Loss: 0.00001994
Iteration 42/1000 | Loss: 0.00001993
Iteration 43/1000 | Loss: 0.00001993
Iteration 44/1000 | Loss: 0.00001993
Iteration 45/1000 | Loss: 0.00001993
Iteration 46/1000 | Loss: 0.00001993
Iteration 47/1000 | Loss: 0.00001993
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00001991
Iteration 54/1000 | Loss: 0.00001991
Iteration 55/1000 | Loss: 0.00001991
Iteration 56/1000 | Loss: 0.00001991
Iteration 57/1000 | Loss: 0.00001991
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00001991
Iteration 60/1000 | Loss: 0.00001991
Iteration 61/1000 | Loss: 0.00001991
Iteration 62/1000 | Loss: 0.00001991
Iteration 63/1000 | Loss: 0.00001991
Iteration 64/1000 | Loss: 0.00001991
Iteration 65/1000 | Loss: 0.00001991
Iteration 66/1000 | Loss: 0.00001991
Iteration 67/1000 | Loss: 0.00001991
Iteration 68/1000 | Loss: 0.00001991
Iteration 69/1000 | Loss: 0.00001991
Iteration 70/1000 | Loss: 0.00001991
Iteration 71/1000 | Loss: 0.00001991
Iteration 72/1000 | Loss: 0.00001991
Iteration 73/1000 | Loss: 0.00001991
Iteration 74/1000 | Loss: 0.00001991
Iteration 75/1000 | Loss: 0.00001991
Iteration 76/1000 | Loss: 0.00001991
Iteration 77/1000 | Loss: 0.00001991
Iteration 78/1000 | Loss: 0.00001991
Iteration 79/1000 | Loss: 0.00001991
Iteration 80/1000 | Loss: 0.00001991
Iteration 81/1000 | Loss: 0.00001991
Iteration 82/1000 | Loss: 0.00001991
Iteration 83/1000 | Loss: 0.00001991
Iteration 84/1000 | Loss: 0.00001991
Iteration 85/1000 | Loss: 0.00001991
Iteration 86/1000 | Loss: 0.00001991
Iteration 87/1000 | Loss: 0.00001991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.99065816559596e-05, 1.99065816559596e-05, 1.99065816559596e-05, 1.99065816559596e-05, 1.99065816559596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.99065816559596e-05

Optimization complete. Final v2v error: 3.8269388675689697 mm

Highest mean error: 4.386743545532227 mm for frame 8

Lowest mean error: 3.197624444961548 mm for frame 237

Saving results

Total time: 294.3366301059723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1043
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458995
Iteration 2/25 | Loss: 0.00137412
Iteration 3/25 | Loss: 0.00123250
Iteration 4/25 | Loss: 0.00121964
Iteration 5/25 | Loss: 0.00121854
Iteration 6/25 | Loss: 0.00121854
Iteration 7/25 | Loss: 0.00121854
Iteration 8/25 | Loss: 0.00121854
Iteration 9/25 | Loss: 0.00121854
Iteration 10/25 | Loss: 0.00121854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012185429222881794, 0.0012185429222881794, 0.0012185429222881794, 0.0012185429222881794, 0.0012185429222881794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012185429222881794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24989450
Iteration 2/25 | Loss: 0.00169233
Iteration 3/25 | Loss: 0.00169231
Iteration 4/25 | Loss: 0.00169231
Iteration 5/25 | Loss: 0.00169231
Iteration 6/25 | Loss: 0.00169231
Iteration 7/25 | Loss: 0.00169231
Iteration 8/25 | Loss: 0.00169231
Iteration 9/25 | Loss: 0.00169231
Iteration 10/25 | Loss: 0.00169231
Iteration 11/25 | Loss: 0.00169231
Iteration 12/25 | Loss: 0.00169231
Iteration 13/25 | Loss: 0.00169231
Iteration 14/25 | Loss: 0.00169231
Iteration 15/25 | Loss: 0.00169231
Iteration 16/25 | Loss: 0.00169231
Iteration 17/25 | Loss: 0.00169231
Iteration 18/25 | Loss: 0.00169231
Iteration 19/25 | Loss: 0.00169231
Iteration 20/25 | Loss: 0.00169231
Iteration 21/25 | Loss: 0.00169231
Iteration 22/25 | Loss: 0.00169231
Iteration 23/25 | Loss: 0.00169231
Iteration 24/25 | Loss: 0.00169231
Iteration 25/25 | Loss: 0.00169231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169231
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00001685
Iteration 4/1000 | Loss: 0.00001504
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001273
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001244
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00001229
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001229
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001229
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001227
Iteration 28/1000 | Loss: 0.00001227
Iteration 29/1000 | Loss: 0.00001226
Iteration 30/1000 | Loss: 0.00001225
Iteration 31/1000 | Loss: 0.00001224
Iteration 32/1000 | Loss: 0.00001224
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001218
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001217
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001211
Iteration 46/1000 | Loss: 0.00001208
Iteration 47/1000 | Loss: 0.00001207
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001184
Iteration 92/1000 | Loss: 0.00001184
Iteration 93/1000 | Loss: 0.00001184
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001184
Iteration 96/1000 | Loss: 0.00001184
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001184
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001183
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001181
Iteration 117/1000 | Loss: 0.00001181
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001180
Iteration 120/1000 | Loss: 0.00001180
Iteration 121/1000 | Loss: 0.00001180
Iteration 122/1000 | Loss: 0.00001180
Iteration 123/1000 | Loss: 0.00001180
Iteration 124/1000 | Loss: 0.00001180
Iteration 125/1000 | Loss: 0.00001180
Iteration 126/1000 | Loss: 0.00001180
Iteration 127/1000 | Loss: 0.00001180
Iteration 128/1000 | Loss: 0.00001179
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Iteration 133/1000 | Loss: 0.00001179
Iteration 134/1000 | Loss: 0.00001179
Iteration 135/1000 | Loss: 0.00001179
Iteration 136/1000 | Loss: 0.00001179
Iteration 137/1000 | Loss: 0.00001179
Iteration 138/1000 | Loss: 0.00001179
Iteration 139/1000 | Loss: 0.00001179
Iteration 140/1000 | Loss: 0.00001179
Iteration 141/1000 | Loss: 0.00001179
Iteration 142/1000 | Loss: 0.00001179
Iteration 143/1000 | Loss: 0.00001179
Iteration 144/1000 | Loss: 0.00001179
Iteration 145/1000 | Loss: 0.00001179
Iteration 146/1000 | Loss: 0.00001179
Iteration 147/1000 | Loss: 0.00001179
Iteration 148/1000 | Loss: 0.00001178
Iteration 149/1000 | Loss: 0.00001178
Iteration 150/1000 | Loss: 0.00001178
Iteration 151/1000 | Loss: 0.00001178
Iteration 152/1000 | Loss: 0.00001178
Iteration 153/1000 | Loss: 0.00001178
Iteration 154/1000 | Loss: 0.00001178
Iteration 155/1000 | Loss: 0.00001178
Iteration 156/1000 | Loss: 0.00001178
Iteration 157/1000 | Loss: 0.00001178
Iteration 158/1000 | Loss: 0.00001178
Iteration 159/1000 | Loss: 0.00001178
Iteration 160/1000 | Loss: 0.00001177
Iteration 161/1000 | Loss: 0.00001177
Iteration 162/1000 | Loss: 0.00001177
Iteration 163/1000 | Loss: 0.00001177
Iteration 164/1000 | Loss: 0.00001177
Iteration 165/1000 | Loss: 0.00001177
Iteration 166/1000 | Loss: 0.00001177
Iteration 167/1000 | Loss: 0.00001177
Iteration 168/1000 | Loss: 0.00001177
Iteration 169/1000 | Loss: 0.00001177
Iteration 170/1000 | Loss: 0.00001177
Iteration 171/1000 | Loss: 0.00001177
Iteration 172/1000 | Loss: 0.00001177
Iteration 173/1000 | Loss: 0.00001177
Iteration 174/1000 | Loss: 0.00001177
Iteration 175/1000 | Loss: 0.00001177
Iteration 176/1000 | Loss: 0.00001177
Iteration 177/1000 | Loss: 0.00001177
Iteration 178/1000 | Loss: 0.00001177
Iteration 179/1000 | Loss: 0.00001177
Iteration 180/1000 | Loss: 0.00001177
Iteration 181/1000 | Loss: 0.00001177
Iteration 182/1000 | Loss: 0.00001177
Iteration 183/1000 | Loss: 0.00001177
Iteration 184/1000 | Loss: 0.00001177
Iteration 185/1000 | Loss: 0.00001177
Iteration 186/1000 | Loss: 0.00001177
Iteration 187/1000 | Loss: 0.00001177
Iteration 188/1000 | Loss: 0.00001177
Iteration 189/1000 | Loss: 0.00001177
Iteration 190/1000 | Loss: 0.00001177
Iteration 191/1000 | Loss: 0.00001177
Iteration 192/1000 | Loss: 0.00001177
Iteration 193/1000 | Loss: 0.00001177
Iteration 194/1000 | Loss: 0.00001177
Iteration 195/1000 | Loss: 0.00001177
Iteration 196/1000 | Loss: 0.00001177
Iteration 197/1000 | Loss: 0.00001177
Iteration 198/1000 | Loss: 0.00001177
Iteration 199/1000 | Loss: 0.00001177
Iteration 200/1000 | Loss: 0.00001177
Iteration 201/1000 | Loss: 0.00001177
Iteration 202/1000 | Loss: 0.00001177
Iteration 203/1000 | Loss: 0.00001177
Iteration 204/1000 | Loss: 0.00001177
Iteration 205/1000 | Loss: 0.00001177
Iteration 206/1000 | Loss: 0.00001177
Iteration 207/1000 | Loss: 0.00001177
Iteration 208/1000 | Loss: 0.00001177
Iteration 209/1000 | Loss: 0.00001177
Iteration 210/1000 | Loss: 0.00001177
Iteration 211/1000 | Loss: 0.00001177
Iteration 212/1000 | Loss: 0.00001177
Iteration 213/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.1770494893426076e-05, 1.1770494893426076e-05, 1.1770494893426076e-05, 1.1770494893426076e-05, 1.1770494893426076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1770494893426076e-05

Optimization complete. Final v2v error: 2.88861346244812 mm

Highest mean error: 3.161802291870117 mm for frame 92

Lowest mean error: 2.6223666667938232 mm for frame 44

Saving results

Total time: 230.08522248268127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1085
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00616936
Iteration 2/25 | Loss: 0.00140028
Iteration 3/25 | Loss: 0.00121886
Iteration 4/25 | Loss: 0.00119964
Iteration 5/25 | Loss: 0.00119822
Iteration 6/25 | Loss: 0.00119822
Iteration 7/25 | Loss: 0.00119822
Iteration 8/25 | Loss: 0.00119822
Iteration 9/25 | Loss: 0.00119822
Iteration 10/25 | Loss: 0.00119822
Iteration 11/25 | Loss: 0.00119822
Iteration 12/25 | Loss: 0.00119822
Iteration 13/25 | Loss: 0.00119822
Iteration 14/25 | Loss: 0.00119822
Iteration 15/25 | Loss: 0.00119822
Iteration 16/25 | Loss: 0.00119822
Iteration 17/25 | Loss: 0.00119822
Iteration 18/25 | Loss: 0.00119822
Iteration 19/25 | Loss: 0.00119822
Iteration 20/25 | Loss: 0.00119822
Iteration 21/25 | Loss: 0.00119822
Iteration 22/25 | Loss: 0.00119822
Iteration 23/25 | Loss: 0.00119822
Iteration 24/25 | Loss: 0.00119822
Iteration 25/25 | Loss: 0.00119822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22382069
Iteration 2/25 | Loss: 0.00157929
Iteration 3/25 | Loss: 0.00157926
Iteration 4/25 | Loss: 0.00157926
Iteration 5/25 | Loss: 0.00157926
Iteration 6/25 | Loss: 0.00157926
Iteration 7/25 | Loss: 0.00157926
Iteration 8/25 | Loss: 0.00157926
Iteration 9/25 | Loss: 0.00157926
Iteration 10/25 | Loss: 0.00157926
Iteration 11/25 | Loss: 0.00157926
Iteration 12/25 | Loss: 0.00157926
Iteration 13/25 | Loss: 0.00157926
Iteration 14/25 | Loss: 0.00157926
Iteration 15/25 | Loss: 0.00157926
Iteration 16/25 | Loss: 0.00157926
Iteration 17/25 | Loss: 0.00157926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015792555641382933, 0.0015792555641382933, 0.0015792555641382933, 0.0015792555641382933, 0.0015792555641382933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015792555641382933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157926
Iteration 2/1000 | Loss: 0.00002346
Iteration 3/1000 | Loss: 0.00001653
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001375
Iteration 6/1000 | Loss: 0.00001324
Iteration 7/1000 | Loss: 0.00001285
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001202
Iteration 10/1000 | Loss: 0.00001169
Iteration 11/1000 | Loss: 0.00001149
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001100
Iteration 15/1000 | Loss: 0.00001093
Iteration 16/1000 | Loss: 0.00001092
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001087
Iteration 20/1000 | Loss: 0.00001086
Iteration 21/1000 | Loss: 0.00001084
Iteration 22/1000 | Loss: 0.00001083
Iteration 23/1000 | Loss: 0.00001082
Iteration 24/1000 | Loss: 0.00001081
Iteration 25/1000 | Loss: 0.00001073
Iteration 26/1000 | Loss: 0.00001066
Iteration 27/1000 | Loss: 0.00001058
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001056
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001051
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001050
Iteration 35/1000 | Loss: 0.00001048
Iteration 36/1000 | Loss: 0.00001048
Iteration 37/1000 | Loss: 0.00001048
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001039
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001038
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001037
Iteration 47/1000 | Loss: 0.00001036
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001034
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001034
Iteration 58/1000 | Loss: 0.00001034
Iteration 59/1000 | Loss: 0.00001033
Iteration 60/1000 | Loss: 0.00001033
Iteration 61/1000 | Loss: 0.00001033
Iteration 62/1000 | Loss: 0.00001033
Iteration 63/1000 | Loss: 0.00001033
Iteration 64/1000 | Loss: 0.00001033
Iteration 65/1000 | Loss: 0.00001032
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001032
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001030
Iteration 71/1000 | Loss: 0.00001030
Iteration 72/1000 | Loss: 0.00001030
Iteration 73/1000 | Loss: 0.00001030
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001030
Iteration 76/1000 | Loss: 0.00001029
Iteration 77/1000 | Loss: 0.00001029
Iteration 78/1000 | Loss: 0.00001028
Iteration 79/1000 | Loss: 0.00001028
Iteration 80/1000 | Loss: 0.00001028
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001026
Iteration 86/1000 | Loss: 0.00001026
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001025
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001024
Iteration 92/1000 | Loss: 0.00001024
Iteration 93/1000 | Loss: 0.00001024
Iteration 94/1000 | Loss: 0.00001024
Iteration 95/1000 | Loss: 0.00001024
Iteration 96/1000 | Loss: 0.00001024
Iteration 97/1000 | Loss: 0.00001024
Iteration 98/1000 | Loss: 0.00001024
Iteration 99/1000 | Loss: 0.00001024
Iteration 100/1000 | Loss: 0.00001024
Iteration 101/1000 | Loss: 0.00001024
Iteration 102/1000 | Loss: 0.00001023
Iteration 103/1000 | Loss: 0.00001023
Iteration 104/1000 | Loss: 0.00001023
Iteration 105/1000 | Loss: 0.00001023
Iteration 106/1000 | Loss: 0.00001023
Iteration 107/1000 | Loss: 0.00001023
Iteration 108/1000 | Loss: 0.00001022
Iteration 109/1000 | Loss: 0.00001022
Iteration 110/1000 | Loss: 0.00001022
Iteration 111/1000 | Loss: 0.00001022
Iteration 112/1000 | Loss: 0.00001022
Iteration 113/1000 | Loss: 0.00001022
Iteration 114/1000 | Loss: 0.00001022
Iteration 115/1000 | Loss: 0.00001022
Iteration 116/1000 | Loss: 0.00001022
Iteration 117/1000 | Loss: 0.00001022
Iteration 118/1000 | Loss: 0.00001022
Iteration 119/1000 | Loss: 0.00001022
Iteration 120/1000 | Loss: 0.00001022
Iteration 121/1000 | Loss: 0.00001022
Iteration 122/1000 | Loss: 0.00001022
Iteration 123/1000 | Loss: 0.00001022
Iteration 124/1000 | Loss: 0.00001022
Iteration 125/1000 | Loss: 0.00001022
Iteration 126/1000 | Loss: 0.00001022
Iteration 127/1000 | Loss: 0.00001021
Iteration 128/1000 | Loss: 0.00001021
Iteration 129/1000 | Loss: 0.00001021
Iteration 130/1000 | Loss: 0.00001021
Iteration 131/1000 | Loss: 0.00001021
Iteration 132/1000 | Loss: 0.00001021
Iteration 133/1000 | Loss: 0.00001021
Iteration 134/1000 | Loss: 0.00001020
Iteration 135/1000 | Loss: 0.00001020
Iteration 136/1000 | Loss: 0.00001020
Iteration 137/1000 | Loss: 0.00001020
Iteration 138/1000 | Loss: 0.00001020
Iteration 139/1000 | Loss: 0.00001020
Iteration 140/1000 | Loss: 0.00001020
Iteration 141/1000 | Loss: 0.00001020
Iteration 142/1000 | Loss: 0.00001020
Iteration 143/1000 | Loss: 0.00001020
Iteration 144/1000 | Loss: 0.00001020
Iteration 145/1000 | Loss: 0.00001020
Iteration 146/1000 | Loss: 0.00001020
Iteration 147/1000 | Loss: 0.00001020
Iteration 148/1000 | Loss: 0.00001020
Iteration 149/1000 | Loss: 0.00001020
Iteration 150/1000 | Loss: 0.00001020
Iteration 151/1000 | Loss: 0.00001020
Iteration 152/1000 | Loss: 0.00001020
Iteration 153/1000 | Loss: 0.00001020
Iteration 154/1000 | Loss: 0.00001020
Iteration 155/1000 | Loss: 0.00001020
Iteration 156/1000 | Loss: 0.00001020
Iteration 157/1000 | Loss: 0.00001020
Iteration 158/1000 | Loss: 0.00001020
Iteration 159/1000 | Loss: 0.00001020
Iteration 160/1000 | Loss: 0.00001020
Iteration 161/1000 | Loss: 0.00001020
Iteration 162/1000 | Loss: 0.00001020
Iteration 163/1000 | Loss: 0.00001020
Iteration 164/1000 | Loss: 0.00001020
Iteration 165/1000 | Loss: 0.00001020
Iteration 166/1000 | Loss: 0.00001020
Iteration 167/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.0197167284786701e-05, 1.0197167284786701e-05, 1.0197167284786701e-05, 1.0197167284786701e-05, 1.0197167284786701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0197167284786701e-05

Optimization complete. Final v2v error: 2.73815655708313 mm

Highest mean error: 3.0335285663604736 mm for frame 154

Lowest mean error: 2.4661786556243896 mm for frame 24

Saving results

Total time: 304.32863306999207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1000
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059555
Iteration 2/25 | Loss: 0.01059554
Iteration 3/25 | Loss: 0.00335434
Iteration 4/25 | Loss: 0.00231818
Iteration 5/25 | Loss: 0.00183441
Iteration 6/25 | Loss: 0.00172016
Iteration 7/25 | Loss: 0.00142903
Iteration 8/25 | Loss: 0.00139094
Iteration 9/25 | Loss: 0.00124226
Iteration 10/25 | Loss: 0.00121115
Iteration 11/25 | Loss: 0.00120305
Iteration 12/25 | Loss: 0.00119824
Iteration 13/25 | Loss: 0.00119831
Iteration 14/25 | Loss: 0.00119370
Iteration 15/25 | Loss: 0.00119572
Iteration 16/25 | Loss: 0.00119348
Iteration 17/25 | Loss: 0.00119343
Iteration 18/25 | Loss: 0.00119342
Iteration 19/25 | Loss: 0.00119342
Iteration 20/25 | Loss: 0.00119342
Iteration 21/25 | Loss: 0.00119342
Iteration 22/25 | Loss: 0.00119342
Iteration 23/25 | Loss: 0.00119342
Iteration 24/25 | Loss: 0.00119342
Iteration 25/25 | Loss: 0.00119342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34274852
Iteration 2/25 | Loss: 0.00292323
Iteration 3/25 | Loss: 0.00198965
Iteration 4/25 | Loss: 0.00198964
Iteration 5/25 | Loss: 0.00198964
Iteration 6/25 | Loss: 0.00198964
Iteration 7/25 | Loss: 0.00198963
Iteration 8/25 | Loss: 0.00198963
Iteration 9/25 | Loss: 0.00198963
Iteration 10/25 | Loss: 0.00198963
Iteration 11/25 | Loss: 0.00198963
Iteration 12/25 | Loss: 0.00198963
Iteration 13/25 | Loss: 0.00198963
Iteration 14/25 | Loss: 0.00198963
Iteration 15/25 | Loss: 0.00198963
Iteration 16/25 | Loss: 0.00198963
Iteration 17/25 | Loss: 0.00198963
Iteration 18/25 | Loss: 0.00198963
Iteration 19/25 | Loss: 0.00198963
Iteration 20/25 | Loss: 0.00198963
Iteration 21/25 | Loss: 0.00198963
Iteration 22/25 | Loss: 0.00198963
Iteration 23/25 | Loss: 0.00198963
Iteration 24/25 | Loss: 0.00198963
Iteration 25/25 | Loss: 0.00198963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198963
Iteration 2/1000 | Loss: 0.00101942
Iteration 3/1000 | Loss: 0.00005004
Iteration 4/1000 | Loss: 0.00005934
Iteration 5/1000 | Loss: 0.00004187
Iteration 6/1000 | Loss: 0.00004653
Iteration 7/1000 | Loss: 0.00003710
Iteration 8/1000 | Loss: 0.00003599
Iteration 9/1000 | Loss: 0.00003502
Iteration 10/1000 | Loss: 0.00051051
Iteration 11/1000 | Loss: 0.00253647
Iteration 12/1000 | Loss: 0.00003356
Iteration 13/1000 | Loss: 0.00002627
Iteration 14/1000 | Loss: 0.00004838
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00001776
Iteration 17/1000 | Loss: 0.00001623
Iteration 18/1000 | Loss: 0.00001521
Iteration 19/1000 | Loss: 0.00002679
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00002622
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001200
Iteration 25/1000 | Loss: 0.00001175
Iteration 26/1000 | Loss: 0.00001159
Iteration 27/1000 | Loss: 0.00001154
Iteration 28/1000 | Loss: 0.00003108
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001144
Iteration 32/1000 | Loss: 0.00001144
Iteration 33/1000 | Loss: 0.00001144
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001143
Iteration 43/1000 | Loss: 0.00001143
Iteration 44/1000 | Loss: 0.00001143
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001141
Iteration 50/1000 | Loss: 0.00001141
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001138
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001137
Iteration 57/1000 | Loss: 0.00001137
Iteration 58/1000 | Loss: 0.00001137
Iteration 59/1000 | Loss: 0.00001136
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001136
Iteration 64/1000 | Loss: 0.00001136
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001135
Iteration 69/1000 | Loss: 0.00002666
Iteration 70/1000 | Loss: 0.00001135
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001129
Iteration 76/1000 | Loss: 0.00001129
Iteration 77/1000 | Loss: 0.00001129
Iteration 78/1000 | Loss: 0.00001129
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001129
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001129
Iteration 85/1000 | Loss: 0.00001129
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001129
Iteration 88/1000 | Loss: 0.00001129
Iteration 89/1000 | Loss: 0.00001129
Iteration 90/1000 | Loss: 0.00001129
Iteration 91/1000 | Loss: 0.00001129
Iteration 92/1000 | Loss: 0.00001129
Iteration 93/1000 | Loss: 0.00001129
Iteration 94/1000 | Loss: 0.00001129
Iteration 95/1000 | Loss: 0.00001129
Iteration 96/1000 | Loss: 0.00001129
Iteration 97/1000 | Loss: 0.00001129
Iteration 98/1000 | Loss: 0.00001129
Iteration 99/1000 | Loss: 0.00001129
Iteration 100/1000 | Loss: 0.00001129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.1286529115750454e-05, 1.1286529115750454e-05, 1.1286529115750454e-05, 1.1286529115750454e-05, 1.1286529115750454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1286529115750454e-05

Optimization complete. Final v2v error: 2.8933675289154053 mm

Highest mean error: 3.1303646564483643 mm for frame 87

Lowest mean error: 2.7383065223693848 mm for frame 65

Saving results

Total time: 531.4710340499878
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1038
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865248
Iteration 2/25 | Loss: 0.00155042
Iteration 3/25 | Loss: 0.00122050
Iteration 4/25 | Loss: 0.00119768
Iteration 5/25 | Loss: 0.00119359
Iteration 6/25 | Loss: 0.00119347
Iteration 7/25 | Loss: 0.00119347
Iteration 8/25 | Loss: 0.00119347
Iteration 9/25 | Loss: 0.00119347
Iteration 10/25 | Loss: 0.00119347
Iteration 11/25 | Loss: 0.00119347
Iteration 12/25 | Loss: 0.00119347
Iteration 13/25 | Loss: 0.00119347
Iteration 14/25 | Loss: 0.00119347
Iteration 15/25 | Loss: 0.00119347
Iteration 16/25 | Loss: 0.00119347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011934671783819795, 0.0011934671783819795, 0.0011934671783819795, 0.0011934671783819795, 0.0011934671783819795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011934671783819795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75866640
Iteration 2/25 | Loss: 0.00148322
Iteration 3/25 | Loss: 0.00148321
Iteration 4/25 | Loss: 0.00148321
Iteration 5/25 | Loss: 0.00148321
Iteration 6/25 | Loss: 0.00148321
Iteration 7/25 | Loss: 0.00148321
Iteration 8/25 | Loss: 0.00148321
Iteration 9/25 | Loss: 0.00148321
Iteration 10/25 | Loss: 0.00148321
Iteration 11/25 | Loss: 0.00148321
Iteration 12/25 | Loss: 0.00148321
Iteration 13/25 | Loss: 0.00148321
Iteration 14/25 | Loss: 0.00148321
Iteration 15/25 | Loss: 0.00148321
Iteration 16/25 | Loss: 0.00148321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014832100132480264, 0.0014832100132480264, 0.0014832100132480264, 0.0014832100132480264, 0.0014832100132480264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014832100132480264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148321
Iteration 2/1000 | Loss: 0.00002599
Iteration 3/1000 | Loss: 0.00001782
Iteration 4/1000 | Loss: 0.00001571
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00001379
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00001283
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001253
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001244
Iteration 17/1000 | Loss: 0.00001242
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001238
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001232
Iteration 25/1000 | Loss: 0.00001228
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001220
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001213
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001211
Iteration 46/1000 | Loss: 0.00001211
Iteration 47/1000 | Loss: 0.00001211
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001207
Iteration 66/1000 | Loss: 0.00001207
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001207
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.2072393474227283e-05, 1.2072393474227283e-05, 1.2072393474227283e-05, 1.2072393474227283e-05, 1.2072393474227283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2072393474227283e-05

Optimization complete. Final v2v error: 2.960951805114746 mm

Highest mean error: 3.288724422454834 mm for frame 7

Lowest mean error: 2.6410975456237793 mm for frame 238

Saving results

Total time: 338.7977864742279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1025
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426996
Iteration 2/25 | Loss: 0.00124961
Iteration 3/25 | Loss: 0.00118916
Iteration 4/25 | Loss: 0.00118010
Iteration 5/25 | Loss: 0.00117695
Iteration 6/25 | Loss: 0.00117637
Iteration 7/25 | Loss: 0.00117637
Iteration 8/25 | Loss: 0.00117637
Iteration 9/25 | Loss: 0.00117637
Iteration 10/25 | Loss: 0.00117637
Iteration 11/25 | Loss: 0.00117637
Iteration 12/25 | Loss: 0.00117637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011763706570491195, 0.0011763706570491195, 0.0011763706570491195, 0.0011763706570491195, 0.0011763706570491195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011763706570491195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67399108
Iteration 2/25 | Loss: 0.00189613
Iteration 3/25 | Loss: 0.00189613
Iteration 4/25 | Loss: 0.00189612
Iteration 5/25 | Loss: 0.00189612
Iteration 6/25 | Loss: 0.00189612
Iteration 7/25 | Loss: 0.00189612
Iteration 8/25 | Loss: 0.00189612
Iteration 9/25 | Loss: 0.00189612
Iteration 10/25 | Loss: 0.00189612
Iteration 11/25 | Loss: 0.00189612
Iteration 12/25 | Loss: 0.00189612
Iteration 13/25 | Loss: 0.00189612
Iteration 14/25 | Loss: 0.00189612
Iteration 15/25 | Loss: 0.00189612
Iteration 16/25 | Loss: 0.00189612
Iteration 17/25 | Loss: 0.00189612
Iteration 18/25 | Loss: 0.00189612
Iteration 19/25 | Loss: 0.00189612
Iteration 20/25 | Loss: 0.00189612
Iteration 21/25 | Loss: 0.00189612
Iteration 22/25 | Loss: 0.00189612
Iteration 23/25 | Loss: 0.00189612
Iteration 24/25 | Loss: 0.00189612
Iteration 25/25 | Loss: 0.00189612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189612
Iteration 2/1000 | Loss: 0.00002854
Iteration 3/1000 | Loss: 0.00002195
Iteration 4/1000 | Loss: 0.00001881
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001670
Iteration 8/1000 | Loss: 0.00001628
Iteration 9/1000 | Loss: 0.00001600
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001549
Iteration 12/1000 | Loss: 0.00001546
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001507
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001500
Iteration 18/1000 | Loss: 0.00001495
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001491
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001490
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001483
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001482
Iteration 31/1000 | Loss: 0.00001482
Iteration 32/1000 | Loss: 0.00001481
Iteration 33/1000 | Loss: 0.00001480
Iteration 34/1000 | Loss: 0.00001480
Iteration 35/1000 | Loss: 0.00001479
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001477
Iteration 40/1000 | Loss: 0.00001477
Iteration 41/1000 | Loss: 0.00001477
Iteration 42/1000 | Loss: 0.00001477
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001477
Iteration 45/1000 | Loss: 0.00001477
Iteration 46/1000 | Loss: 0.00001477
Iteration 47/1000 | Loss: 0.00001476
Iteration 48/1000 | Loss: 0.00001476
Iteration 49/1000 | Loss: 0.00001476
Iteration 50/1000 | Loss: 0.00001475
Iteration 51/1000 | Loss: 0.00001475
Iteration 52/1000 | Loss: 0.00001474
Iteration 53/1000 | Loss: 0.00001474
Iteration 54/1000 | Loss: 0.00001474
Iteration 55/1000 | Loss: 0.00001474
Iteration 56/1000 | Loss: 0.00001474
Iteration 57/1000 | Loss: 0.00001474
Iteration 58/1000 | Loss: 0.00001473
Iteration 59/1000 | Loss: 0.00001473
Iteration 60/1000 | Loss: 0.00001473
Iteration 61/1000 | Loss: 0.00001473
Iteration 62/1000 | Loss: 0.00001473
Iteration 63/1000 | Loss: 0.00001473
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001472
Iteration 66/1000 | Loss: 0.00001472
Iteration 67/1000 | Loss: 0.00001472
Iteration 68/1000 | Loss: 0.00001472
Iteration 69/1000 | Loss: 0.00001471
Iteration 70/1000 | Loss: 0.00001471
Iteration 71/1000 | Loss: 0.00001471
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001470
Iteration 83/1000 | Loss: 0.00001470
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001469
Iteration 86/1000 | Loss: 0.00001469
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001469
Iteration 89/1000 | Loss: 0.00001469
Iteration 90/1000 | Loss: 0.00001469
Iteration 91/1000 | Loss: 0.00001469
Iteration 92/1000 | Loss: 0.00001469
Iteration 93/1000 | Loss: 0.00001469
Iteration 94/1000 | Loss: 0.00001469
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001468
Iteration 97/1000 | Loss: 0.00001468
Iteration 98/1000 | Loss: 0.00001468
Iteration 99/1000 | Loss: 0.00001468
Iteration 100/1000 | Loss: 0.00001468
Iteration 101/1000 | Loss: 0.00001468
Iteration 102/1000 | Loss: 0.00001467
Iteration 103/1000 | Loss: 0.00001467
Iteration 104/1000 | Loss: 0.00001467
Iteration 105/1000 | Loss: 0.00001467
Iteration 106/1000 | Loss: 0.00001467
Iteration 107/1000 | Loss: 0.00001467
Iteration 108/1000 | Loss: 0.00001466
Iteration 109/1000 | Loss: 0.00001466
Iteration 110/1000 | Loss: 0.00001466
Iteration 111/1000 | Loss: 0.00001466
Iteration 112/1000 | Loss: 0.00001466
Iteration 113/1000 | Loss: 0.00001466
Iteration 114/1000 | Loss: 0.00001466
Iteration 115/1000 | Loss: 0.00001466
Iteration 116/1000 | Loss: 0.00001465
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001465
Iteration 120/1000 | Loss: 0.00001465
Iteration 121/1000 | Loss: 0.00001465
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00001464
Iteration 124/1000 | Loss: 0.00001464
Iteration 125/1000 | Loss: 0.00001464
Iteration 126/1000 | Loss: 0.00001464
Iteration 127/1000 | Loss: 0.00001464
Iteration 128/1000 | Loss: 0.00001464
Iteration 129/1000 | Loss: 0.00001464
Iteration 130/1000 | Loss: 0.00001464
Iteration 131/1000 | Loss: 0.00001464
Iteration 132/1000 | Loss: 0.00001464
Iteration 133/1000 | Loss: 0.00001464
Iteration 134/1000 | Loss: 0.00001464
Iteration 135/1000 | Loss: 0.00001464
Iteration 136/1000 | Loss: 0.00001464
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001464
Iteration 142/1000 | Loss: 0.00001464
Iteration 143/1000 | Loss: 0.00001464
Iteration 144/1000 | Loss: 0.00001464
Iteration 145/1000 | Loss: 0.00001464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.463670105295023e-05, 1.463670105295023e-05, 1.463670105295023e-05, 1.463670105295023e-05, 1.463670105295023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.463670105295023e-05

Optimization complete. Final v2v error: 3.286949872970581 mm

Highest mean error: 3.527068614959717 mm for frame 31

Lowest mean error: 3.020049571990967 mm for frame 48

Saving results

Total time: 200.7194311618805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1008
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892833
Iteration 2/25 | Loss: 0.00266911
Iteration 3/25 | Loss: 0.00196530
Iteration 4/25 | Loss: 0.00172830
Iteration 5/25 | Loss: 0.00167792
Iteration 6/25 | Loss: 0.00152647
Iteration 7/25 | Loss: 0.00149479
Iteration 8/25 | Loss: 0.00144502
Iteration 9/25 | Loss: 0.00142178
Iteration 10/25 | Loss: 0.00141470
Iteration 11/25 | Loss: 0.00139390
Iteration 12/25 | Loss: 0.00138527
Iteration 13/25 | Loss: 0.00140967
Iteration 14/25 | Loss: 0.00137551
Iteration 15/25 | Loss: 0.00137776
Iteration 16/25 | Loss: 0.00137091
Iteration 17/25 | Loss: 0.00137023
Iteration 18/25 | Loss: 0.00137223
Iteration 19/25 | Loss: 0.00137050
Iteration 20/25 | Loss: 0.00136994
Iteration 21/25 | Loss: 0.00137058
Iteration 22/25 | Loss: 0.00136820
Iteration 23/25 | Loss: 0.00136982
Iteration 24/25 | Loss: 0.00137001
Iteration 25/25 | Loss: 0.00136980

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.42131710
Iteration 2/25 | Loss: 0.00503515
Iteration 3/25 | Loss: 0.00383099
Iteration 4/25 | Loss: 0.00353642
Iteration 5/25 | Loss: 0.00353642
Iteration 6/25 | Loss: 0.00353642
Iteration 7/25 | Loss: 0.00353642
Iteration 8/25 | Loss: 0.00353642
Iteration 9/25 | Loss: 0.00353642
Iteration 10/25 | Loss: 0.00353642
Iteration 11/25 | Loss: 0.00353642
Iteration 12/25 | Loss: 0.00353642
Iteration 13/25 | Loss: 0.00353642
Iteration 14/25 | Loss: 0.00353642
Iteration 15/25 | Loss: 0.00353642
Iteration 16/25 | Loss: 0.00353642
Iteration 17/25 | Loss: 0.00353642
Iteration 18/25 | Loss: 0.00353642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003536418778821826, 0.003536418778821826, 0.003536418778821826, 0.003536418778821826, 0.003536418778821826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003536418778821826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00353642
Iteration 2/1000 | Loss: 0.00213524
Iteration 3/1000 | Loss: 0.00220838
Iteration 4/1000 | Loss: 0.00110067
Iteration 5/1000 | Loss: 0.00219377
Iteration 6/1000 | Loss: 0.00077521
Iteration 7/1000 | Loss: 0.00128056
Iteration 8/1000 | Loss: 0.00161317
Iteration 9/1000 | Loss: 0.00051075
Iteration 10/1000 | Loss: 0.00048056
Iteration 11/1000 | Loss: 0.00066130
Iteration 12/1000 | Loss: 0.00022368
Iteration 13/1000 | Loss: 0.00020107
Iteration 14/1000 | Loss: 0.00038653
Iteration 15/1000 | Loss: 0.00026607
Iteration 16/1000 | Loss: 0.00020379
Iteration 17/1000 | Loss: 0.00174192
Iteration 18/1000 | Loss: 0.00009236
Iteration 19/1000 | Loss: 0.00029749
Iteration 20/1000 | Loss: 0.00013708
Iteration 21/1000 | Loss: 0.00021061
Iteration 22/1000 | Loss: 0.00031906
Iteration 23/1000 | Loss: 0.00015216
Iteration 24/1000 | Loss: 0.00020088
Iteration 25/1000 | Loss: 0.00043062
Iteration 26/1000 | Loss: 0.00015254
Iteration 27/1000 | Loss: 0.00020123
Iteration 28/1000 | Loss: 0.00015267
Iteration 29/1000 | Loss: 0.00006965
Iteration 30/1000 | Loss: 0.00034216
Iteration 31/1000 | Loss: 0.00018714
Iteration 32/1000 | Loss: 0.00026151
Iteration 33/1000 | Loss: 0.00021179
Iteration 34/1000 | Loss: 0.00007543
Iteration 35/1000 | Loss: 0.00054181
Iteration 36/1000 | Loss: 0.00065266
Iteration 37/1000 | Loss: 0.00143463
Iteration 38/1000 | Loss: 0.00099529
Iteration 39/1000 | Loss: 0.00163444
Iteration 40/1000 | Loss: 0.00130453
Iteration 41/1000 | Loss: 0.00085535
Iteration 42/1000 | Loss: 0.00053328
Iteration 43/1000 | Loss: 0.00109253
Iteration 44/1000 | Loss: 0.00050842
Iteration 45/1000 | Loss: 0.00125018
Iteration 46/1000 | Loss: 0.00081035
Iteration 47/1000 | Loss: 0.00062056
Iteration 48/1000 | Loss: 0.00010936
Iteration 49/1000 | Loss: 0.00012496
Iteration 50/1000 | Loss: 0.00005493
Iteration 51/1000 | Loss: 0.00048387
Iteration 52/1000 | Loss: 0.00095923
Iteration 53/1000 | Loss: 0.00138806
Iteration 54/1000 | Loss: 0.00052875
Iteration 55/1000 | Loss: 0.00024952
Iteration 56/1000 | Loss: 0.00011863
Iteration 57/1000 | Loss: 0.00004668
Iteration 58/1000 | Loss: 0.00018376
Iteration 59/1000 | Loss: 0.00029936
Iteration 60/1000 | Loss: 0.00065138
Iteration 61/1000 | Loss: 0.00031440
Iteration 62/1000 | Loss: 0.00014171
Iteration 63/1000 | Loss: 0.00025396
Iteration 64/1000 | Loss: 0.00006626
Iteration 65/1000 | Loss: 0.00006056
Iteration 66/1000 | Loss: 0.00005661
Iteration 67/1000 | Loss: 0.00003735
Iteration 68/1000 | Loss: 0.00003460
Iteration 69/1000 | Loss: 0.00004071
Iteration 70/1000 | Loss: 0.00003899
Iteration 71/1000 | Loss: 0.00003586
Iteration 72/1000 | Loss: 0.00004938
Iteration 73/1000 | Loss: 0.00005153
Iteration 74/1000 | Loss: 0.00003024
Iteration 75/1000 | Loss: 0.00003408
Iteration 76/1000 | Loss: 0.00003567
Iteration 77/1000 | Loss: 0.00004259
Iteration 78/1000 | Loss: 0.00004039
Iteration 79/1000 | Loss: 0.00004964
Iteration 80/1000 | Loss: 0.00004569
Iteration 81/1000 | Loss: 0.00004424
Iteration 82/1000 | Loss: 0.00003980
Iteration 83/1000 | Loss: 0.00003203
Iteration 84/1000 | Loss: 0.00003154
Iteration 85/1000 | Loss: 0.00004194
Iteration 86/1000 | Loss: 0.00003166
Iteration 87/1000 | Loss: 0.00003008
Iteration 88/1000 | Loss: 0.00004703
Iteration 89/1000 | Loss: 0.00004553
Iteration 90/1000 | Loss: 0.00004480
Iteration 91/1000 | Loss: 0.00004341
Iteration 92/1000 | Loss: 0.00003689
Iteration 93/1000 | Loss: 0.00004904
Iteration 94/1000 | Loss: 0.00003447
Iteration 95/1000 | Loss: 0.00004376
Iteration 96/1000 | Loss: 0.00004618
Iteration 97/1000 | Loss: 0.00004512
Iteration 98/1000 | Loss: 0.00004555
Iteration 99/1000 | Loss: 0.00004444
Iteration 100/1000 | Loss: 0.00004560
Iteration 101/1000 | Loss: 0.00004449
Iteration 102/1000 | Loss: 0.00003883
Iteration 103/1000 | Loss: 0.00002822
Iteration 104/1000 | Loss: 0.00004662
Iteration 105/1000 | Loss: 0.00004365
Iteration 106/1000 | Loss: 0.00004550
Iteration 107/1000 | Loss: 0.00004269
Iteration 108/1000 | Loss: 0.00004568
Iteration 109/1000 | Loss: 0.00005118
Iteration 110/1000 | Loss: 0.00004909
Iteration 111/1000 | Loss: 0.00004958
Iteration 112/1000 | Loss: 0.00056414
Iteration 113/1000 | Loss: 0.00025454
Iteration 114/1000 | Loss: 0.00003935
Iteration 115/1000 | Loss: 0.00004586
Iteration 116/1000 | Loss: 0.00004426
Iteration 117/1000 | Loss: 0.00003193
Iteration 118/1000 | Loss: 0.00003597
Iteration 119/1000 | Loss: 0.00004756
Iteration 120/1000 | Loss: 0.00004450
Iteration 121/1000 | Loss: 0.00004187
Iteration 122/1000 | Loss: 0.00004201
Iteration 123/1000 | Loss: 0.00004040
Iteration 124/1000 | Loss: 0.00004572
Iteration 125/1000 | Loss: 0.00004948
Iteration 126/1000 | Loss: 0.00003501
Iteration 127/1000 | Loss: 0.00004347
Iteration 128/1000 | Loss: 0.00004867
Iteration 129/1000 | Loss: 0.00004154
Iteration 130/1000 | Loss: 0.00004912
Iteration 131/1000 | Loss: 0.00004389
Iteration 132/1000 | Loss: 0.00004774
Iteration 133/1000 | Loss: 0.00004556
Iteration 134/1000 | Loss: 0.00004124
Iteration 135/1000 | Loss: 0.00002868
Iteration 136/1000 | Loss: 0.00003303
Iteration 137/1000 | Loss: 0.00003529
Iteration 138/1000 | Loss: 0.00003502
Iteration 139/1000 | Loss: 0.00004576
Iteration 140/1000 | Loss: 0.00004751
Iteration 141/1000 | Loss: 0.00003776
Iteration 142/1000 | Loss: 0.00003608
Iteration 143/1000 | Loss: 0.00003269
Iteration 144/1000 | Loss: 0.00002892
Iteration 145/1000 | Loss: 0.00003466
Iteration 146/1000 | Loss: 0.00004162
Iteration 147/1000 | Loss: 0.00004689
Iteration 148/1000 | Loss: 0.00003019
Iteration 149/1000 | Loss: 0.00004269
Iteration 150/1000 | Loss: 0.00003566
Iteration 151/1000 | Loss: 0.00003627
Iteration 152/1000 | Loss: 0.00003202
Iteration 153/1000 | Loss: 0.00002675
Iteration 154/1000 | Loss: 0.00002553
Iteration 155/1000 | Loss: 0.00004135
Iteration 156/1000 | Loss: 0.00005044
Iteration 157/1000 | Loss: 0.00003757
Iteration 158/1000 | Loss: 0.00004463
Iteration 159/1000 | Loss: 0.00005027
Iteration 160/1000 | Loss: 0.00003762
Iteration 161/1000 | Loss: 0.00004322
Iteration 162/1000 | Loss: 0.00004104
Iteration 163/1000 | Loss: 0.00004021
Iteration 164/1000 | Loss: 0.00004604
Iteration 165/1000 | Loss: 0.00004366
Iteration 166/1000 | Loss: 0.00003890
Iteration 167/1000 | Loss: 0.00004083
Iteration 168/1000 | Loss: 0.00004367
Iteration 169/1000 | Loss: 0.00004122
Iteration 170/1000 | Loss: 0.00004212
Iteration 171/1000 | Loss: 0.00004365
Iteration 172/1000 | Loss: 0.00004480
Iteration 173/1000 | Loss: 0.00004179
Iteration 174/1000 | Loss: 0.00004102
Iteration 175/1000 | Loss: 0.00004367
Iteration 176/1000 | Loss: 0.00005688
Iteration 177/1000 | Loss: 0.00004419
Iteration 178/1000 | Loss: 0.00004410
Iteration 179/1000 | Loss: 0.00006004
Iteration 180/1000 | Loss: 0.00004222
Iteration 181/1000 | Loss: 0.00004710
Iteration 182/1000 | Loss: 0.00006560
Iteration 183/1000 | Loss: 0.00004877
Iteration 184/1000 | Loss: 0.00003476
Iteration 185/1000 | Loss: 0.00004371
Iteration 186/1000 | Loss: 0.00004014
Iteration 187/1000 | Loss: 0.00003945
Iteration 188/1000 | Loss: 0.00004219
Iteration 189/1000 | Loss: 0.00003866
Iteration 190/1000 | Loss: 0.00004170
Iteration 191/1000 | Loss: 0.00003035
Iteration 192/1000 | Loss: 0.00003169
Iteration 193/1000 | Loss: 0.00004475
Iteration 194/1000 | Loss: 0.00003568
Iteration 195/1000 | Loss: 0.00002761
Iteration 196/1000 | Loss: 0.00003578
Iteration 197/1000 | Loss: 0.00052644
Iteration 198/1000 | Loss: 0.00041782
Iteration 199/1000 | Loss: 0.00003719
Iteration 200/1000 | Loss: 0.00005475
Iteration 201/1000 | Loss: 0.00005286
Iteration 202/1000 | Loss: 0.00002222
Iteration 203/1000 | Loss: 0.00044856
Iteration 204/1000 | Loss: 0.00004780
Iteration 205/1000 | Loss: 0.00003883
Iteration 206/1000 | Loss: 0.00003854
Iteration 207/1000 | Loss: 0.00004492
Iteration 208/1000 | Loss: 0.00003515
Iteration 209/1000 | Loss: 0.00004120
Iteration 210/1000 | Loss: 0.00003494
Iteration 211/1000 | Loss: 0.00004172
Iteration 212/1000 | Loss: 0.00003575
Iteration 213/1000 | Loss: 0.00003835
Iteration 214/1000 | Loss: 0.00003750
Iteration 215/1000 | Loss: 0.00003973
Iteration 216/1000 | Loss: 0.00003355
Iteration 217/1000 | Loss: 0.00002388
Iteration 218/1000 | Loss: 0.00005003
Iteration 219/1000 | Loss: 0.00003328
Iteration 220/1000 | Loss: 0.00003779
Iteration 221/1000 | Loss: 0.00003002
Iteration 222/1000 | Loss: 0.00002951
Iteration 223/1000 | Loss: 0.00004369
Iteration 224/1000 | Loss: 0.00003433
Iteration 225/1000 | Loss: 0.00003694
Iteration 226/1000 | Loss: 0.00003781
Iteration 227/1000 | Loss: 0.00007439
Iteration 228/1000 | Loss: 0.00003543
Iteration 229/1000 | Loss: 0.00004924
Iteration 230/1000 | Loss: 0.00004538
Iteration 231/1000 | Loss: 0.00003838
Iteration 232/1000 | Loss: 0.00003865
Iteration 233/1000 | Loss: 0.00004082
Iteration 234/1000 | Loss: 0.00003811
Iteration 235/1000 | Loss: 0.00003802
Iteration 236/1000 | Loss: 0.00003694
Iteration 237/1000 | Loss: 0.00003081
Iteration 238/1000 | Loss: 0.00003752
Iteration 239/1000 | Loss: 0.00003476
Iteration 240/1000 | Loss: 0.00004052
Iteration 241/1000 | Loss: 0.00004692
Iteration 242/1000 | Loss: 0.00004291
Iteration 243/1000 | Loss: 0.00003901
Iteration 244/1000 | Loss: 0.00003818
Iteration 245/1000 | Loss: 0.00004003
Iteration 246/1000 | Loss: 0.00004711
Iteration 247/1000 | Loss: 0.00004227
Iteration 248/1000 | Loss: 0.00003964
Iteration 249/1000 | Loss: 0.00009927
Iteration 250/1000 | Loss: 0.00002797
Iteration 251/1000 | Loss: 0.00011312
Iteration 252/1000 | Loss: 0.00003065
Iteration 253/1000 | Loss: 0.00003524
Iteration 254/1000 | Loss: 0.00003951
Iteration 255/1000 | Loss: 0.00003749
Iteration 256/1000 | Loss: 0.00003782
Iteration 257/1000 | Loss: 0.00004285
Iteration 258/1000 | Loss: 0.00003840
Iteration 259/1000 | Loss: 0.00003064
Iteration 260/1000 | Loss: 0.00004127
Iteration 261/1000 | Loss: 0.00003577
Iteration 262/1000 | Loss: 0.00004479
Iteration 263/1000 | Loss: 0.00004128
Iteration 264/1000 | Loss: 0.00003611
Iteration 265/1000 | Loss: 0.00004184
Iteration 266/1000 | Loss: 0.00003798
Iteration 267/1000 | Loss: 0.00003655
Iteration 268/1000 | Loss: 0.00003898
Iteration 269/1000 | Loss: 0.00004133
Iteration 270/1000 | Loss: 0.00003763
Iteration 271/1000 | Loss: 0.00003444
Iteration 272/1000 | Loss: 0.00003109
Iteration 273/1000 | Loss: 0.00004390
Iteration 274/1000 | Loss: 0.00004122
Iteration 275/1000 | Loss: 0.00004020
Iteration 276/1000 | Loss: 0.00003886
Iteration 277/1000 | Loss: 0.00004236
Iteration 278/1000 | Loss: 0.00003837
Iteration 279/1000 | Loss: 0.00003887
Iteration 280/1000 | Loss: 0.00004470
Iteration 281/1000 | Loss: 0.00004246
Iteration 282/1000 | Loss: 0.00002988
Iteration 283/1000 | Loss: 0.00003391
Iteration 284/1000 | Loss: 0.00002793
Iteration 285/1000 | Loss: 0.00007078
Iteration 286/1000 | Loss: 0.00001887
Iteration 287/1000 | Loss: 0.00001633
Iteration 288/1000 | Loss: 0.00003538
Iteration 289/1000 | Loss: 0.00001776
Iteration 290/1000 | Loss: 0.00001616
Iteration 291/1000 | Loss: 0.00001553
Iteration 292/1000 | Loss: 0.00001506
Iteration 293/1000 | Loss: 0.00001470
Iteration 294/1000 | Loss: 0.00001452
Iteration 295/1000 | Loss: 0.00001448
Iteration 296/1000 | Loss: 0.00001436
Iteration 297/1000 | Loss: 0.00001421
Iteration 298/1000 | Loss: 0.00001421
Iteration 299/1000 | Loss: 0.00001419
Iteration 300/1000 | Loss: 0.00001417
Iteration 301/1000 | Loss: 0.00001413
Iteration 302/1000 | Loss: 0.00001411
Iteration 303/1000 | Loss: 0.00001410
Iteration 304/1000 | Loss: 0.00001410
Iteration 305/1000 | Loss: 0.00001409
Iteration 306/1000 | Loss: 0.00001409
Iteration 307/1000 | Loss: 0.00001408
Iteration 308/1000 | Loss: 0.00001408
Iteration 309/1000 | Loss: 0.00001407
Iteration 310/1000 | Loss: 0.00001407
Iteration 311/1000 | Loss: 0.00001404
Iteration 312/1000 | Loss: 0.00001401
Iteration 313/1000 | Loss: 0.00001400
Iteration 314/1000 | Loss: 0.00001397
Iteration 315/1000 | Loss: 0.00001397
Iteration 316/1000 | Loss: 0.00001395
Iteration 317/1000 | Loss: 0.00001394
Iteration 318/1000 | Loss: 0.00001394
Iteration 319/1000 | Loss: 0.00001394
Iteration 320/1000 | Loss: 0.00001393
Iteration 321/1000 | Loss: 0.00001392
Iteration 322/1000 | Loss: 0.00001392
Iteration 323/1000 | Loss: 0.00001392
Iteration 324/1000 | Loss: 0.00001392
Iteration 325/1000 | Loss: 0.00001391
Iteration 326/1000 | Loss: 0.00001391
Iteration 327/1000 | Loss: 0.00001390
Iteration 328/1000 | Loss: 0.00001390
Iteration 329/1000 | Loss: 0.00001389
Iteration 330/1000 | Loss: 0.00001389
Iteration 331/1000 | Loss: 0.00001389
Iteration 332/1000 | Loss: 0.00001388
Iteration 333/1000 | Loss: 0.00001388
Iteration 334/1000 | Loss: 0.00001387
Iteration 335/1000 | Loss: 0.00001386
Iteration 336/1000 | Loss: 0.00001386
Iteration 337/1000 | Loss: 0.00001386
Iteration 338/1000 | Loss: 0.00001385
Iteration 339/1000 | Loss: 0.00001385
Iteration 340/1000 | Loss: 0.00002092
Iteration 341/1000 | Loss: 0.00002092
Iteration 342/1000 | Loss: 0.00001498
Iteration 343/1000 | Loss: 0.00001436
Iteration 344/1000 | Loss: 0.00001400
Iteration 345/1000 | Loss: 0.00001377
Iteration 346/1000 | Loss: 0.00001372
Iteration 347/1000 | Loss: 0.00001371
Iteration 348/1000 | Loss: 0.00001358
Iteration 349/1000 | Loss: 0.00001357
Iteration 350/1000 | Loss: 0.00001356
Iteration 351/1000 | Loss: 0.00001354
Iteration 352/1000 | Loss: 0.00001353
Iteration 353/1000 | Loss: 0.00001348
Iteration 354/1000 | Loss: 0.00001348
Iteration 355/1000 | Loss: 0.00001346
Iteration 356/1000 | Loss: 0.00001346
Iteration 357/1000 | Loss: 0.00001345
Iteration 358/1000 | Loss: 0.00001345
Iteration 359/1000 | Loss: 0.00001344
Iteration 360/1000 | Loss: 0.00001343
Iteration 361/1000 | Loss: 0.00001342
Iteration 362/1000 | Loss: 0.00001342
Iteration 363/1000 | Loss: 0.00001341
Iteration 364/1000 | Loss: 0.00001341
Iteration 365/1000 | Loss: 0.00001340
Iteration 366/1000 | Loss: 0.00001340
Iteration 367/1000 | Loss: 0.00001336
Iteration 368/1000 | Loss: 0.00001331
Iteration 369/1000 | Loss: 0.00001331
Iteration 370/1000 | Loss: 0.00001330
Iteration 371/1000 | Loss: 0.00001326
Iteration 372/1000 | Loss: 0.00001326
Iteration 373/1000 | Loss: 0.00001325
Iteration 374/1000 | Loss: 0.00001324
Iteration 375/1000 | Loss: 0.00001324
Iteration 376/1000 | Loss: 0.00001324
Iteration 377/1000 | Loss: 0.00001324
Iteration 378/1000 | Loss: 0.00001323
Iteration 379/1000 | Loss: 0.00001323
Iteration 380/1000 | Loss: 0.00001323
Iteration 381/1000 | Loss: 0.00001323
Iteration 382/1000 | Loss: 0.00001322
Iteration 383/1000 | Loss: 0.00001322
Iteration 384/1000 | Loss: 0.00001322
Iteration 385/1000 | Loss: 0.00001322
Iteration 386/1000 | Loss: 0.00001322
Iteration 387/1000 | Loss: 0.00001322
Iteration 388/1000 | Loss: 0.00001322
Iteration 389/1000 | Loss: 0.00001322
Iteration 390/1000 | Loss: 0.00001322
Iteration 391/1000 | Loss: 0.00001321
Iteration 392/1000 | Loss: 0.00001321
Iteration 393/1000 | Loss: 0.00001321
Iteration 394/1000 | Loss: 0.00001321
Iteration 395/1000 | Loss: 0.00001321
Iteration 396/1000 | Loss: 0.00001321
Iteration 397/1000 | Loss: 0.00001321
Iteration 398/1000 | Loss: 0.00001321
Iteration 399/1000 | Loss: 0.00001321
Iteration 400/1000 | Loss: 0.00001320
Iteration 401/1000 | Loss: 0.00001320
Iteration 402/1000 | Loss: 0.00001320
Iteration 403/1000 | Loss: 0.00001320
Iteration 404/1000 | Loss: 0.00001320
Iteration 405/1000 | Loss: 0.00001320
Iteration 406/1000 | Loss: 0.00001320
Iteration 407/1000 | Loss: 0.00001320
Iteration 408/1000 | Loss: 0.00001319
Iteration 409/1000 | Loss: 0.00001319
Iteration 410/1000 | Loss: 0.00001319
Iteration 411/1000 | Loss: 0.00001319
Iteration 412/1000 | Loss: 0.00001319
Iteration 413/1000 | Loss: 0.00001319
Iteration 414/1000 | Loss: 0.00001319
Iteration 415/1000 | Loss: 0.00001319
Iteration 416/1000 | Loss: 0.00001319
Iteration 417/1000 | Loss: 0.00001319
Iteration 418/1000 | Loss: 0.00001319
Iteration 419/1000 | Loss: 0.00001318
Iteration 420/1000 | Loss: 0.00001318
Iteration 421/1000 | Loss: 0.00001318
Iteration 422/1000 | Loss: 0.00001318
Iteration 423/1000 | Loss: 0.00001318
Iteration 424/1000 | Loss: 0.00001318
Iteration 425/1000 | Loss: 0.00001318
Iteration 426/1000 | Loss: 0.00001318
Iteration 427/1000 | Loss: 0.00001318
Iteration 428/1000 | Loss: 0.00001318
Iteration 429/1000 | Loss: 0.00001318
Iteration 430/1000 | Loss: 0.00001318
Iteration 431/1000 | Loss: 0.00001318
Iteration 432/1000 | Loss: 0.00001318
Iteration 433/1000 | Loss: 0.00001318
Iteration 434/1000 | Loss: 0.00001318
Iteration 435/1000 | Loss: 0.00001318
Iteration 436/1000 | Loss: 0.00001318
Iteration 437/1000 | Loss: 0.00001318
Iteration 438/1000 | Loss: 0.00001318
Iteration 439/1000 | Loss: 0.00001318
Iteration 440/1000 | Loss: 0.00001318
Iteration 441/1000 | Loss: 0.00001318
Iteration 442/1000 | Loss: 0.00001318
Iteration 443/1000 | Loss: 0.00001318
Iteration 444/1000 | Loss: 0.00001318
Iteration 445/1000 | Loss: 0.00001318
Iteration 446/1000 | Loss: 0.00001318
Iteration 447/1000 | Loss: 0.00001318
Iteration 448/1000 | Loss: 0.00001318
Iteration 449/1000 | Loss: 0.00001318
Iteration 450/1000 | Loss: 0.00001318
Iteration 451/1000 | Loss: 0.00001318
Iteration 452/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 452. Stopping optimization.
Last 5 losses: [1.3180620953789912e-05, 1.3180620953789912e-05, 1.3180620953789912e-05, 1.3180620953789912e-05, 1.3180620953789912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3180620953789912e-05

Optimization complete. Final v2v error: 3.0512804985046387 mm

Highest mean error: 4.74398946762085 mm for frame 77

Lowest mean error: 2.5823185443878174 mm for frame 157

Saving results

Total time: 3009.2483417987823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1062
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754930
Iteration 2/25 | Loss: 0.00187257
Iteration 3/25 | Loss: 0.00142085
Iteration 4/25 | Loss: 0.00133311
Iteration 5/25 | Loss: 0.00130765
Iteration 6/25 | Loss: 0.00133235
Iteration 7/25 | Loss: 0.00130975
Iteration 8/25 | Loss: 0.00129114
Iteration 9/25 | Loss: 0.00130042
Iteration 10/25 | Loss: 0.00128347
Iteration 11/25 | Loss: 0.00126426
Iteration 12/25 | Loss: 0.00126574
Iteration 13/25 | Loss: 0.00125914
Iteration 14/25 | Loss: 0.00125285
Iteration 15/25 | Loss: 0.00125155
Iteration 16/25 | Loss: 0.00125182
Iteration 17/25 | Loss: 0.00125328
Iteration 18/25 | Loss: 0.00125106
Iteration 19/25 | Loss: 0.00125114
Iteration 20/25 | Loss: 0.00125077
Iteration 21/25 | Loss: 0.00125073
Iteration 22/25 | Loss: 0.00125120
Iteration 23/25 | Loss: 0.00125335
Iteration 24/25 | Loss: 0.00125048
Iteration 25/25 | Loss: 0.00125117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.02538824
Iteration 2/25 | Loss: 0.00163260
Iteration 3/25 | Loss: 0.00156513
Iteration 4/25 | Loss: 0.00156513
Iteration 5/25 | Loss: 0.00156513
Iteration 6/25 | Loss: 0.00156513
Iteration 7/25 | Loss: 0.00156512
Iteration 8/25 | Loss: 0.00156512
Iteration 9/25 | Loss: 0.00156512
Iteration 10/25 | Loss: 0.00156512
Iteration 11/25 | Loss: 0.00156512
Iteration 12/25 | Loss: 0.00156512
Iteration 13/25 | Loss: 0.00156512
Iteration 14/25 | Loss: 0.00156512
Iteration 15/25 | Loss: 0.00156512
Iteration 16/25 | Loss: 0.00156512
Iteration 17/25 | Loss: 0.00156512
Iteration 18/25 | Loss: 0.00156512
Iteration 19/25 | Loss: 0.00156512
Iteration 20/25 | Loss: 0.00156512
Iteration 21/25 | Loss: 0.00156512
Iteration 22/25 | Loss: 0.00156512
Iteration 23/25 | Loss: 0.00156512
Iteration 24/25 | Loss: 0.00156512
Iteration 25/25 | Loss: 0.00156512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156512
Iteration 2/1000 | Loss: 0.00003917
Iteration 3/1000 | Loss: 0.00010453
Iteration 4/1000 | Loss: 0.00002174
Iteration 5/1000 | Loss: 0.00004592
Iteration 6/1000 | Loss: 0.00005627
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00001666
Iteration 9/1000 | Loss: 0.00001622
Iteration 10/1000 | Loss: 0.00001594
Iteration 11/1000 | Loss: 0.00001527
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001467
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001447
Iteration 16/1000 | Loss: 0.00002997
Iteration 17/1000 | Loss: 0.00001427
Iteration 18/1000 | Loss: 0.00001426
Iteration 19/1000 | Loss: 0.00001414
Iteration 20/1000 | Loss: 0.00001403
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001390
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001388
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00003195
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001377
Iteration 40/1000 | Loss: 0.00001374
Iteration 41/1000 | Loss: 0.00001374
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001374
Iteration 46/1000 | Loss: 0.00001374
Iteration 47/1000 | Loss: 0.00001374
Iteration 48/1000 | Loss: 0.00001374
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001373
Iteration 54/1000 | Loss: 0.00001373
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001370
Iteration 67/1000 | Loss: 0.00001370
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001368
Iteration 74/1000 | Loss: 0.00001368
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001368
Iteration 77/1000 | Loss: 0.00001368
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001367
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001367
Iteration 82/1000 | Loss: 0.00001367
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001367
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001365
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001365
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00003188
Iteration 99/1000 | Loss: 0.00001758
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001365
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001363
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001363
Iteration 107/1000 | Loss: 0.00001363
Iteration 108/1000 | Loss: 0.00001363
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001363
Iteration 111/1000 | Loss: 0.00001363
Iteration 112/1000 | Loss: 0.00001363
Iteration 113/1000 | Loss: 0.00001363
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00002339
Iteration 120/1000 | Loss: 0.00002339
Iteration 121/1000 | Loss: 0.00001363
Iteration 122/1000 | Loss: 0.00001359
Iteration 123/1000 | Loss: 0.00001359
Iteration 124/1000 | Loss: 0.00001359
Iteration 125/1000 | Loss: 0.00001359
Iteration 126/1000 | Loss: 0.00001359
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001358
Iteration 131/1000 | Loss: 0.00001358
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001358
Iteration 136/1000 | Loss: 0.00001358
Iteration 137/1000 | Loss: 0.00001358
Iteration 138/1000 | Loss: 0.00001358
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.3573381693277042e-05, 1.3573381693277042e-05, 1.3573381693277042e-05, 1.3573381693277042e-05, 1.3573381693277042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3573381693277042e-05

Optimization complete. Final v2v error: 3.051809310913086 mm

Highest mean error: 3.6265833377838135 mm for frame 176

Lowest mean error: 2.6817626953125 mm for frame 148

Saving results

Total time: 1028.0021662712097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1032
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467666
Iteration 2/25 | Loss: 0.00137840
Iteration 3/25 | Loss: 0.00122913
Iteration 4/25 | Loss: 0.00120447
Iteration 5/25 | Loss: 0.00119735
Iteration 6/25 | Loss: 0.00119595
Iteration 7/25 | Loss: 0.00119595
Iteration 8/25 | Loss: 0.00119595
Iteration 9/25 | Loss: 0.00119595
Iteration 10/25 | Loss: 0.00119595
Iteration 11/25 | Loss: 0.00119595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011959546245634556, 0.0011959546245634556, 0.0011959546245634556, 0.0011959546245634556, 0.0011959546245634556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011959546245634556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10121465
Iteration 2/25 | Loss: 0.00227265
Iteration 3/25 | Loss: 0.00227264
Iteration 4/25 | Loss: 0.00227264
Iteration 5/25 | Loss: 0.00227264
Iteration 6/25 | Loss: 0.00227264
Iteration 7/25 | Loss: 0.00227264
Iteration 8/25 | Loss: 0.00227264
Iteration 9/25 | Loss: 0.00227264
Iteration 10/25 | Loss: 0.00227264
Iteration 11/25 | Loss: 0.00227264
Iteration 12/25 | Loss: 0.00227264
Iteration 13/25 | Loss: 0.00227264
Iteration 14/25 | Loss: 0.00227264
Iteration 15/25 | Loss: 0.00227264
Iteration 16/25 | Loss: 0.00227264
Iteration 17/25 | Loss: 0.00227264
Iteration 18/25 | Loss: 0.00227264
Iteration 19/25 | Loss: 0.00227264
Iteration 20/25 | Loss: 0.00227264
Iteration 21/25 | Loss: 0.00227264
Iteration 22/25 | Loss: 0.00227264
Iteration 23/25 | Loss: 0.00227264
Iteration 24/25 | Loss: 0.00227264
Iteration 25/25 | Loss: 0.00227264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227264
Iteration 2/1000 | Loss: 0.00003958
Iteration 3/1000 | Loss: 0.00002736
Iteration 4/1000 | Loss: 0.00002158
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001756
Iteration 8/1000 | Loss: 0.00001703
Iteration 9/1000 | Loss: 0.00001672
Iteration 10/1000 | Loss: 0.00001645
Iteration 11/1000 | Loss: 0.00001631
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001594
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001567
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001565
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001563
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001561
Iteration 25/1000 | Loss: 0.00001557
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001553
Iteration 28/1000 | Loss: 0.00001553
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001544
Iteration 31/1000 | Loss: 0.00001544
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001540
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001538
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001534
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001529
Iteration 47/1000 | Loss: 0.00001529
Iteration 48/1000 | Loss: 0.00001528
Iteration 49/1000 | Loss: 0.00001528
Iteration 50/1000 | Loss: 0.00001527
Iteration 51/1000 | Loss: 0.00001527
Iteration 52/1000 | Loss: 0.00001526
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001518
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001516
Iteration 82/1000 | Loss: 0.00001515
Iteration 83/1000 | Loss: 0.00001515
Iteration 84/1000 | Loss: 0.00001515
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001514
Iteration 87/1000 | Loss: 0.00001514
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001513
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001511
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001504
Iteration 124/1000 | Loss: 0.00001504
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001503
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001503
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001501
Iteration 134/1000 | Loss: 0.00001501
Iteration 135/1000 | Loss: 0.00001501
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001500
Iteration 138/1000 | Loss: 0.00001500
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001499
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001497
Iteration 151/1000 | Loss: 0.00001497
Iteration 152/1000 | Loss: 0.00001497
Iteration 153/1000 | Loss: 0.00001497
Iteration 154/1000 | Loss: 0.00001497
Iteration 155/1000 | Loss: 0.00001497
Iteration 156/1000 | Loss: 0.00001496
Iteration 157/1000 | Loss: 0.00001496
Iteration 158/1000 | Loss: 0.00001496
Iteration 159/1000 | Loss: 0.00001496
Iteration 160/1000 | Loss: 0.00001496
Iteration 161/1000 | Loss: 0.00001496
Iteration 162/1000 | Loss: 0.00001496
Iteration 163/1000 | Loss: 0.00001496
Iteration 164/1000 | Loss: 0.00001496
Iteration 165/1000 | Loss: 0.00001495
Iteration 166/1000 | Loss: 0.00001495
Iteration 167/1000 | Loss: 0.00001495
Iteration 168/1000 | Loss: 0.00001495
Iteration 169/1000 | Loss: 0.00001495
Iteration 170/1000 | Loss: 0.00001494
Iteration 171/1000 | Loss: 0.00001494
Iteration 172/1000 | Loss: 0.00001494
Iteration 173/1000 | Loss: 0.00001494
Iteration 174/1000 | Loss: 0.00001493
Iteration 175/1000 | Loss: 0.00001493
Iteration 176/1000 | Loss: 0.00001493
Iteration 177/1000 | Loss: 0.00001493
Iteration 178/1000 | Loss: 0.00001492
Iteration 179/1000 | Loss: 0.00001492
Iteration 180/1000 | Loss: 0.00001492
Iteration 181/1000 | Loss: 0.00001492
Iteration 182/1000 | Loss: 0.00001492
Iteration 183/1000 | Loss: 0.00001492
Iteration 184/1000 | Loss: 0.00001492
Iteration 185/1000 | Loss: 0.00001491
Iteration 186/1000 | Loss: 0.00001491
Iteration 187/1000 | Loss: 0.00001491
Iteration 188/1000 | Loss: 0.00001491
Iteration 189/1000 | Loss: 0.00001491
Iteration 190/1000 | Loss: 0.00001491
Iteration 191/1000 | Loss: 0.00001491
Iteration 192/1000 | Loss: 0.00001491
Iteration 193/1000 | Loss: 0.00001491
Iteration 194/1000 | Loss: 0.00001490
Iteration 195/1000 | Loss: 0.00001490
Iteration 196/1000 | Loss: 0.00001490
Iteration 197/1000 | Loss: 0.00001490
Iteration 198/1000 | Loss: 0.00001489
Iteration 199/1000 | Loss: 0.00001489
Iteration 200/1000 | Loss: 0.00001489
Iteration 201/1000 | Loss: 0.00001489
Iteration 202/1000 | Loss: 0.00001489
Iteration 203/1000 | Loss: 0.00001489
Iteration 204/1000 | Loss: 0.00001489
Iteration 205/1000 | Loss: 0.00001489
Iteration 206/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.4891093996993732e-05, 1.4891093996993732e-05, 1.4891093996993732e-05, 1.4891093996993732e-05, 1.4891093996993732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4891093996993732e-05

Optimization complete. Final v2v error: 3.2238316535949707 mm

Highest mean error: 4.593722343444824 mm for frame 66

Lowest mean error: 2.5141983032226562 mm for frame 224

Saving results

Total time: 401.74762082099915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1040
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515288
Iteration 2/25 | Loss: 0.00140303
Iteration 3/25 | Loss: 0.00122974
Iteration 4/25 | Loss: 0.00121622
Iteration 5/25 | Loss: 0.00121472
Iteration 6/25 | Loss: 0.00121470
Iteration 7/25 | Loss: 0.00121470
Iteration 8/25 | Loss: 0.00121470
Iteration 9/25 | Loss: 0.00121470
Iteration 10/25 | Loss: 0.00121470
Iteration 11/25 | Loss: 0.00121470
Iteration 12/25 | Loss: 0.00121470
Iteration 13/25 | Loss: 0.00121470
Iteration 14/25 | Loss: 0.00121470
Iteration 15/25 | Loss: 0.00121470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012147048255428672, 0.0012147048255428672, 0.0012147048255428672, 0.0012147048255428672, 0.0012147048255428672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012147048255428672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.98548126
Iteration 2/25 | Loss: 0.00110686
Iteration 3/25 | Loss: 0.00110686
Iteration 4/25 | Loss: 0.00110686
Iteration 5/25 | Loss: 0.00110686
Iteration 6/25 | Loss: 0.00110686
Iteration 7/25 | Loss: 0.00110686
Iteration 8/25 | Loss: 0.00110686
Iteration 9/25 | Loss: 0.00110686
Iteration 10/25 | Loss: 0.00110686
Iteration 11/25 | Loss: 0.00110686
Iteration 12/25 | Loss: 0.00110686
Iteration 13/25 | Loss: 0.00110686
Iteration 14/25 | Loss: 0.00110686
Iteration 15/25 | Loss: 0.00110686
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011068617459386587, 0.0011068617459386587, 0.0011068617459386587, 0.0011068617459386587, 0.0011068617459386587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011068617459386587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110686
Iteration 2/1000 | Loss: 0.00003815
Iteration 3/1000 | Loss: 0.00002367
Iteration 4/1000 | Loss: 0.00002055
Iteration 5/1000 | Loss: 0.00001935
Iteration 6/1000 | Loss: 0.00001882
Iteration 7/1000 | Loss: 0.00001833
Iteration 8/1000 | Loss: 0.00001800
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001734
Iteration 11/1000 | Loss: 0.00001712
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001697
Iteration 15/1000 | Loss: 0.00001696
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001681
Iteration 20/1000 | Loss: 0.00001680
Iteration 21/1000 | Loss: 0.00001680
Iteration 22/1000 | Loss: 0.00001679
Iteration 23/1000 | Loss: 0.00001677
Iteration 24/1000 | Loss: 0.00001677
Iteration 25/1000 | Loss: 0.00001676
Iteration 26/1000 | Loss: 0.00001676
Iteration 27/1000 | Loss: 0.00001675
Iteration 28/1000 | Loss: 0.00001673
Iteration 29/1000 | Loss: 0.00001673
Iteration 30/1000 | Loss: 0.00001673
Iteration 31/1000 | Loss: 0.00001673
Iteration 32/1000 | Loss: 0.00001673
Iteration 33/1000 | Loss: 0.00001673
Iteration 34/1000 | Loss: 0.00001673
Iteration 35/1000 | Loss: 0.00001673
Iteration 36/1000 | Loss: 0.00001673
Iteration 37/1000 | Loss: 0.00001673
Iteration 38/1000 | Loss: 0.00001673
Iteration 39/1000 | Loss: 0.00001672
Iteration 40/1000 | Loss: 0.00001672
Iteration 41/1000 | Loss: 0.00001672
Iteration 42/1000 | Loss: 0.00001672
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001671
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001666
Iteration 47/1000 | Loss: 0.00001665
Iteration 48/1000 | Loss: 0.00001665
Iteration 49/1000 | Loss: 0.00001663
Iteration 50/1000 | Loss: 0.00001663
Iteration 51/1000 | Loss: 0.00001661
Iteration 52/1000 | Loss: 0.00001661
Iteration 53/1000 | Loss: 0.00001661
Iteration 54/1000 | Loss: 0.00001661
Iteration 55/1000 | Loss: 0.00001661
Iteration 56/1000 | Loss: 0.00001660
Iteration 57/1000 | Loss: 0.00001660
Iteration 58/1000 | Loss: 0.00001660
Iteration 59/1000 | Loss: 0.00001660
Iteration 60/1000 | Loss: 0.00001659
Iteration 61/1000 | Loss: 0.00001658
Iteration 62/1000 | Loss: 0.00001657
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001655
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001653
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00001649
Iteration 71/1000 | Loss: 0.00001649
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001648
Iteration 74/1000 | Loss: 0.00001648
Iteration 75/1000 | Loss: 0.00001647
Iteration 76/1000 | Loss: 0.00001647
Iteration 77/1000 | Loss: 0.00001643
Iteration 78/1000 | Loss: 0.00001641
Iteration 79/1000 | Loss: 0.00001641
Iteration 80/1000 | Loss: 0.00001641
Iteration 81/1000 | Loss: 0.00001641
Iteration 82/1000 | Loss: 0.00001640
Iteration 83/1000 | Loss: 0.00001640
Iteration 84/1000 | Loss: 0.00001640
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001639
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001638
Iteration 89/1000 | Loss: 0.00001638
Iteration 90/1000 | Loss: 0.00001638
Iteration 91/1000 | Loss: 0.00001638
Iteration 92/1000 | Loss: 0.00001638
Iteration 93/1000 | Loss: 0.00001638
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001638
Iteration 96/1000 | Loss: 0.00001638
Iteration 97/1000 | Loss: 0.00001638
Iteration 98/1000 | Loss: 0.00001637
Iteration 99/1000 | Loss: 0.00001637
Iteration 100/1000 | Loss: 0.00001637
Iteration 101/1000 | Loss: 0.00001637
Iteration 102/1000 | Loss: 0.00001637
Iteration 103/1000 | Loss: 0.00001637
Iteration 104/1000 | Loss: 0.00001637
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Iteration 111/1000 | Loss: 0.00001636
Iteration 112/1000 | Loss: 0.00001636
Iteration 113/1000 | Loss: 0.00001636
Iteration 114/1000 | Loss: 0.00001636
Iteration 115/1000 | Loss: 0.00001636
Iteration 116/1000 | Loss: 0.00001636
Iteration 117/1000 | Loss: 0.00001636
Iteration 118/1000 | Loss: 0.00001636
Iteration 119/1000 | Loss: 0.00001636
Iteration 120/1000 | Loss: 0.00001636
Iteration 121/1000 | Loss: 0.00001636
Iteration 122/1000 | Loss: 0.00001636
Iteration 123/1000 | Loss: 0.00001636
Iteration 124/1000 | Loss: 0.00001636
Iteration 125/1000 | Loss: 0.00001636
Iteration 126/1000 | Loss: 0.00001636
Iteration 127/1000 | Loss: 0.00001636
Iteration 128/1000 | Loss: 0.00001636
Iteration 129/1000 | Loss: 0.00001636
Iteration 130/1000 | Loss: 0.00001636
Iteration 131/1000 | Loss: 0.00001636
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.6358115317416377e-05, 1.6358115317416377e-05, 1.6358115317416377e-05, 1.6358115317416377e-05, 1.6358115317416377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6358115317416377e-05

Optimization complete. Final v2v error: 3.423476457595825 mm

Highest mean error: 3.89886212348938 mm for frame 83

Lowest mean error: 3.0867979526519775 mm for frame 0

Saving results

Total time: 173.18580675125122
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1006
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785984
Iteration 2/25 | Loss: 0.00127737
Iteration 3/25 | Loss: 0.00118413
Iteration 4/25 | Loss: 0.00116687
Iteration 5/25 | Loss: 0.00116201
Iteration 6/25 | Loss: 0.00116124
Iteration 7/25 | Loss: 0.00116124
Iteration 8/25 | Loss: 0.00116124
Iteration 9/25 | Loss: 0.00116124
Iteration 10/25 | Loss: 0.00116124
Iteration 11/25 | Loss: 0.00116124
Iteration 12/25 | Loss: 0.00116124
Iteration 13/25 | Loss: 0.00116124
Iteration 14/25 | Loss: 0.00116124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001161237945780158, 0.001161237945780158, 0.001161237945780158, 0.001161237945780158, 0.001161237945780158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001161237945780158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19563258
Iteration 2/25 | Loss: 0.00257549
Iteration 3/25 | Loss: 0.00257549
Iteration 4/25 | Loss: 0.00257549
Iteration 5/25 | Loss: 0.00257549
Iteration 6/25 | Loss: 0.00257549
Iteration 7/25 | Loss: 0.00257548
Iteration 8/25 | Loss: 0.00257548
Iteration 9/25 | Loss: 0.00257548
Iteration 10/25 | Loss: 0.00257548
Iteration 11/25 | Loss: 0.00257548
Iteration 12/25 | Loss: 0.00257548
Iteration 13/25 | Loss: 0.00257548
Iteration 14/25 | Loss: 0.00257548
Iteration 15/25 | Loss: 0.00257548
Iteration 16/25 | Loss: 0.00257548
Iteration 17/25 | Loss: 0.00257548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0025754838716238737, 0.0025754838716238737, 0.0025754838716238737, 0.0025754838716238737, 0.0025754838716238737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025754838716238737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257548
Iteration 2/1000 | Loss: 0.00003529
Iteration 3/1000 | Loss: 0.00002437
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001848
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001565
Iteration 10/1000 | Loss: 0.00001534
Iteration 11/1000 | Loss: 0.00001499
Iteration 12/1000 | Loss: 0.00001468
Iteration 13/1000 | Loss: 0.00001446
Iteration 14/1000 | Loss: 0.00001439
Iteration 15/1000 | Loss: 0.00001421
Iteration 16/1000 | Loss: 0.00001414
Iteration 17/1000 | Loss: 0.00001407
Iteration 18/1000 | Loss: 0.00001403
Iteration 19/1000 | Loss: 0.00001402
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001399
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001394
Iteration 24/1000 | Loss: 0.00001390
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001382
Iteration 27/1000 | Loss: 0.00001382
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001381
Iteration 30/1000 | Loss: 0.00001380
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001377
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001375
Iteration 38/1000 | Loss: 0.00001368
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001365
Iteration 43/1000 | Loss: 0.00001365
Iteration 44/1000 | Loss: 0.00001365
Iteration 45/1000 | Loss: 0.00001365
Iteration 46/1000 | Loss: 0.00001364
Iteration 47/1000 | Loss: 0.00001364
Iteration 48/1000 | Loss: 0.00001364
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001363
Iteration 52/1000 | Loss: 0.00001362
Iteration 53/1000 | Loss: 0.00001362
Iteration 54/1000 | Loss: 0.00001362
Iteration 55/1000 | Loss: 0.00001361
Iteration 56/1000 | Loss: 0.00001361
Iteration 57/1000 | Loss: 0.00001361
Iteration 58/1000 | Loss: 0.00001361
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001360
Iteration 62/1000 | Loss: 0.00001360
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001359
Iteration 66/1000 | Loss: 0.00001359
Iteration 67/1000 | Loss: 0.00001359
Iteration 68/1000 | Loss: 0.00001359
Iteration 69/1000 | Loss: 0.00001359
Iteration 70/1000 | Loss: 0.00001358
Iteration 71/1000 | Loss: 0.00001358
Iteration 72/1000 | Loss: 0.00001358
Iteration 73/1000 | Loss: 0.00001358
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001356
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001355
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001354
Iteration 108/1000 | Loss: 0.00001354
Iteration 109/1000 | Loss: 0.00001354
Iteration 110/1000 | Loss: 0.00001354
Iteration 111/1000 | Loss: 0.00001354
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001353
Iteration 114/1000 | Loss: 0.00001353
Iteration 115/1000 | Loss: 0.00001353
Iteration 116/1000 | Loss: 0.00001353
Iteration 117/1000 | Loss: 0.00001353
Iteration 118/1000 | Loss: 0.00001353
Iteration 119/1000 | Loss: 0.00001353
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Iteration 125/1000 | Loss: 0.00001352
Iteration 126/1000 | Loss: 0.00001351
Iteration 127/1000 | Loss: 0.00001351
Iteration 128/1000 | Loss: 0.00001351
Iteration 129/1000 | Loss: 0.00001351
Iteration 130/1000 | Loss: 0.00001351
Iteration 131/1000 | Loss: 0.00001351
Iteration 132/1000 | Loss: 0.00001351
Iteration 133/1000 | Loss: 0.00001350
Iteration 134/1000 | Loss: 0.00001350
Iteration 135/1000 | Loss: 0.00001350
Iteration 136/1000 | Loss: 0.00001350
Iteration 137/1000 | Loss: 0.00001350
Iteration 138/1000 | Loss: 0.00001350
Iteration 139/1000 | Loss: 0.00001350
Iteration 140/1000 | Loss: 0.00001350
Iteration 141/1000 | Loss: 0.00001349
Iteration 142/1000 | Loss: 0.00001349
Iteration 143/1000 | Loss: 0.00001349
Iteration 144/1000 | Loss: 0.00001349
Iteration 145/1000 | Loss: 0.00001349
Iteration 146/1000 | Loss: 0.00001349
Iteration 147/1000 | Loss: 0.00001349
Iteration 148/1000 | Loss: 0.00001349
Iteration 149/1000 | Loss: 0.00001349
Iteration 150/1000 | Loss: 0.00001349
Iteration 151/1000 | Loss: 0.00001349
Iteration 152/1000 | Loss: 0.00001349
Iteration 153/1000 | Loss: 0.00001349
Iteration 154/1000 | Loss: 0.00001349
Iteration 155/1000 | Loss: 0.00001349
Iteration 156/1000 | Loss: 0.00001348
Iteration 157/1000 | Loss: 0.00001348
Iteration 158/1000 | Loss: 0.00001348
Iteration 159/1000 | Loss: 0.00001348
Iteration 160/1000 | Loss: 0.00001348
Iteration 161/1000 | Loss: 0.00001348
Iteration 162/1000 | Loss: 0.00001348
Iteration 163/1000 | Loss: 0.00001348
Iteration 164/1000 | Loss: 0.00001348
Iteration 165/1000 | Loss: 0.00001348
Iteration 166/1000 | Loss: 0.00001348
Iteration 167/1000 | Loss: 0.00001348
Iteration 168/1000 | Loss: 0.00001348
Iteration 169/1000 | Loss: 0.00001348
Iteration 170/1000 | Loss: 0.00001348
Iteration 171/1000 | Loss: 0.00001348
Iteration 172/1000 | Loss: 0.00001348
Iteration 173/1000 | Loss: 0.00001348
Iteration 174/1000 | Loss: 0.00001347
Iteration 175/1000 | Loss: 0.00001347
Iteration 176/1000 | Loss: 0.00001347
Iteration 177/1000 | Loss: 0.00001347
Iteration 178/1000 | Loss: 0.00001347
Iteration 179/1000 | Loss: 0.00001347
Iteration 180/1000 | Loss: 0.00001347
Iteration 181/1000 | Loss: 0.00001347
Iteration 182/1000 | Loss: 0.00001347
Iteration 183/1000 | Loss: 0.00001347
Iteration 184/1000 | Loss: 0.00001347
Iteration 185/1000 | Loss: 0.00001347
Iteration 186/1000 | Loss: 0.00001347
Iteration 187/1000 | Loss: 0.00001347
Iteration 188/1000 | Loss: 0.00001347
Iteration 189/1000 | Loss: 0.00001347
Iteration 190/1000 | Loss: 0.00001347
Iteration 191/1000 | Loss: 0.00001347
Iteration 192/1000 | Loss: 0.00001347
Iteration 193/1000 | Loss: 0.00001347
Iteration 194/1000 | Loss: 0.00001347
Iteration 195/1000 | Loss: 0.00001347
Iteration 196/1000 | Loss: 0.00001347
Iteration 197/1000 | Loss: 0.00001347
Iteration 198/1000 | Loss: 0.00001347
Iteration 199/1000 | Loss: 0.00001347
Iteration 200/1000 | Loss: 0.00001347
Iteration 201/1000 | Loss: 0.00001347
Iteration 202/1000 | Loss: 0.00001347
Iteration 203/1000 | Loss: 0.00001347
Iteration 204/1000 | Loss: 0.00001347
Iteration 205/1000 | Loss: 0.00001347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.3467927601595875e-05, 1.3467927601595875e-05, 1.3467927601595875e-05, 1.3467927601595875e-05, 1.3467927601595875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3467927601595875e-05

Optimization complete. Final v2v error: 3.1705265045166016 mm

Highest mean error: 4.048855781555176 mm for frame 239

Lowest mean error: 2.9266695976257324 mm for frame 36

Saving results

Total time: 376.99264121055603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1077
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00716906
Iteration 2/25 | Loss: 0.00137991
Iteration 3/25 | Loss: 0.00125268
Iteration 4/25 | Loss: 0.00124173
Iteration 5/25 | Loss: 0.00123957
Iteration 6/25 | Loss: 0.00123893
Iteration 7/25 | Loss: 0.00123893
Iteration 8/25 | Loss: 0.00123893
Iteration 9/25 | Loss: 0.00123893
Iteration 10/25 | Loss: 0.00123893
Iteration 11/25 | Loss: 0.00123893
Iteration 12/25 | Loss: 0.00123893
Iteration 13/25 | Loss: 0.00123893
Iteration 14/25 | Loss: 0.00123893
Iteration 15/25 | Loss: 0.00123893
Iteration 16/25 | Loss: 0.00123893
Iteration 17/25 | Loss: 0.00123893
Iteration 18/25 | Loss: 0.00123893
Iteration 19/25 | Loss: 0.00123893
Iteration 20/25 | Loss: 0.00123893
Iteration 21/25 | Loss: 0.00123893
Iteration 22/25 | Loss: 0.00123893
Iteration 23/25 | Loss: 0.00123893
Iteration 24/25 | Loss: 0.00123893
Iteration 25/25 | Loss: 0.00123893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17667842
Iteration 2/25 | Loss: 0.00185652
Iteration 3/25 | Loss: 0.00185652
Iteration 4/25 | Loss: 0.00185652
Iteration 5/25 | Loss: 0.00185652
Iteration 6/25 | Loss: 0.00185652
Iteration 7/25 | Loss: 0.00185652
Iteration 8/25 | Loss: 0.00185652
Iteration 9/25 | Loss: 0.00185652
Iteration 10/25 | Loss: 0.00185652
Iteration 11/25 | Loss: 0.00185652
Iteration 12/25 | Loss: 0.00185652
Iteration 13/25 | Loss: 0.00185652
Iteration 14/25 | Loss: 0.00185652
Iteration 15/25 | Loss: 0.00185652
Iteration 16/25 | Loss: 0.00185652
Iteration 17/25 | Loss: 0.00185652
Iteration 18/25 | Loss: 0.00185652
Iteration 19/25 | Loss: 0.00185652
Iteration 20/25 | Loss: 0.00185652
Iteration 21/25 | Loss: 0.00185652
Iteration 22/25 | Loss: 0.00185652
Iteration 23/25 | Loss: 0.00185652
Iteration 24/25 | Loss: 0.00185652
Iteration 25/25 | Loss: 0.00185652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185652
Iteration 2/1000 | Loss: 0.00002982
Iteration 3/1000 | Loss: 0.00002259
Iteration 4/1000 | Loss: 0.00002020
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001818
Iteration 7/1000 | Loss: 0.00001766
Iteration 8/1000 | Loss: 0.00001720
Iteration 9/1000 | Loss: 0.00001696
Iteration 10/1000 | Loss: 0.00001680
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001662
Iteration 13/1000 | Loss: 0.00001659
Iteration 14/1000 | Loss: 0.00001659
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001658
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001623
Iteration 27/1000 | Loss: 0.00001623
Iteration 28/1000 | Loss: 0.00001623
Iteration 29/1000 | Loss: 0.00001623
Iteration 30/1000 | Loss: 0.00001623
Iteration 31/1000 | Loss: 0.00001623
Iteration 32/1000 | Loss: 0.00001623
Iteration 33/1000 | Loss: 0.00001623
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001619
Iteration 38/1000 | Loss: 0.00001619
Iteration 39/1000 | Loss: 0.00001619
Iteration 40/1000 | Loss: 0.00001619
Iteration 41/1000 | Loss: 0.00001619
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001619
Iteration 45/1000 | Loss: 0.00001618
Iteration 46/1000 | Loss: 0.00001618
Iteration 47/1000 | Loss: 0.00001618
Iteration 48/1000 | Loss: 0.00001618
Iteration 49/1000 | Loss: 0.00001618
Iteration 50/1000 | Loss: 0.00001617
Iteration 51/1000 | Loss: 0.00001617
Iteration 52/1000 | Loss: 0.00001617
Iteration 53/1000 | Loss: 0.00001617
Iteration 54/1000 | Loss: 0.00001617
Iteration 55/1000 | Loss: 0.00001617
Iteration 56/1000 | Loss: 0.00001617
Iteration 57/1000 | Loss: 0.00001617
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001616
Iteration 69/1000 | Loss: 0.00001616
Iteration 70/1000 | Loss: 0.00001616
Iteration 71/1000 | Loss: 0.00001616
Iteration 72/1000 | Loss: 0.00001616
Iteration 73/1000 | Loss: 0.00001616
Iteration 74/1000 | Loss: 0.00001616
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001616
Iteration 90/1000 | Loss: 0.00001616
Iteration 91/1000 | Loss: 0.00001616
Iteration 92/1000 | Loss: 0.00001616
Iteration 93/1000 | Loss: 0.00001616
Iteration 94/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.616150257177651e-05, 1.616150257177651e-05, 1.616150257177651e-05, 1.616150257177651e-05, 1.616150257177651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.616150257177651e-05

Optimization complete. Final v2v error: 3.3604557514190674 mm

Highest mean error: 3.592850685119629 mm for frame 29

Lowest mean error: 3.1849570274353027 mm for frame 79

Saving results

Total time: 149.94263362884521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1073
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526508
Iteration 2/25 | Loss: 0.00124373
Iteration 3/25 | Loss: 0.00116415
Iteration 4/25 | Loss: 0.00115183
Iteration 5/25 | Loss: 0.00114880
Iteration 6/25 | Loss: 0.00114828
Iteration 7/25 | Loss: 0.00114828
Iteration 8/25 | Loss: 0.00114828
Iteration 9/25 | Loss: 0.00114828
Iteration 10/25 | Loss: 0.00114828
Iteration 11/25 | Loss: 0.00114828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011482795234769583, 0.0011482795234769583, 0.0011482795234769583, 0.0011482795234769583, 0.0011482795234769583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011482795234769583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.61056232
Iteration 2/25 | Loss: 0.00176244
Iteration 3/25 | Loss: 0.00176243
Iteration 4/25 | Loss: 0.00176243
Iteration 5/25 | Loss: 0.00176243
Iteration 6/25 | Loss: 0.00176243
Iteration 7/25 | Loss: 0.00176243
Iteration 8/25 | Loss: 0.00176243
Iteration 9/25 | Loss: 0.00176243
Iteration 10/25 | Loss: 0.00176243
Iteration 11/25 | Loss: 0.00176243
Iteration 12/25 | Loss: 0.00176243
Iteration 13/25 | Loss: 0.00176243
Iteration 14/25 | Loss: 0.00176243
Iteration 15/25 | Loss: 0.00176243
Iteration 16/25 | Loss: 0.00176243
Iteration 17/25 | Loss: 0.00176243
Iteration 18/25 | Loss: 0.00176243
Iteration 19/25 | Loss: 0.00176243
Iteration 20/25 | Loss: 0.00176243
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001762429135851562, 0.001762429135851562, 0.001762429135851562, 0.001762429135851562, 0.001762429135851562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001762429135851562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176243
Iteration 2/1000 | Loss: 0.00001981
Iteration 3/1000 | Loss: 0.00001497
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001226
Iteration 7/1000 | Loss: 0.00001171
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001109
Iteration 10/1000 | Loss: 0.00001093
Iteration 11/1000 | Loss: 0.00001075
Iteration 12/1000 | Loss: 0.00001069
Iteration 13/1000 | Loss: 0.00001068
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001064
Iteration 16/1000 | Loss: 0.00001063
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001061
Iteration 19/1000 | Loss: 0.00001060
Iteration 20/1000 | Loss: 0.00001057
Iteration 21/1000 | Loss: 0.00001055
Iteration 22/1000 | Loss: 0.00001053
Iteration 23/1000 | Loss: 0.00001048
Iteration 24/1000 | Loss: 0.00001048
Iteration 25/1000 | Loss: 0.00001047
Iteration 26/1000 | Loss: 0.00001042
Iteration 27/1000 | Loss: 0.00001039
Iteration 28/1000 | Loss: 0.00001038
Iteration 29/1000 | Loss: 0.00001038
Iteration 30/1000 | Loss: 0.00001038
Iteration 31/1000 | Loss: 0.00001037
Iteration 32/1000 | Loss: 0.00001037
Iteration 33/1000 | Loss: 0.00001037
Iteration 34/1000 | Loss: 0.00001033
Iteration 35/1000 | Loss: 0.00001033
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001027
Iteration 39/1000 | Loss: 0.00001027
Iteration 40/1000 | Loss: 0.00001026
Iteration 41/1000 | Loss: 0.00001026
Iteration 42/1000 | Loss: 0.00001025
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001020
Iteration 46/1000 | Loss: 0.00001020
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001019
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001018
Iteration 51/1000 | Loss: 0.00001017
Iteration 52/1000 | Loss: 0.00001017
Iteration 53/1000 | Loss: 0.00001016
Iteration 54/1000 | Loss: 0.00001016
Iteration 55/1000 | Loss: 0.00001015
Iteration 56/1000 | Loss: 0.00001015
Iteration 57/1000 | Loss: 0.00001015
Iteration 58/1000 | Loss: 0.00001014
Iteration 59/1000 | Loss: 0.00001014
Iteration 60/1000 | Loss: 0.00001013
Iteration 61/1000 | Loss: 0.00001013
Iteration 62/1000 | Loss: 0.00001013
Iteration 63/1000 | Loss: 0.00001012
Iteration 64/1000 | Loss: 0.00001012
Iteration 65/1000 | Loss: 0.00001011
Iteration 66/1000 | Loss: 0.00001011
Iteration 67/1000 | Loss: 0.00001011
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001009
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001009
Iteration 75/1000 | Loss: 0.00001008
Iteration 76/1000 | Loss: 0.00001008
Iteration 77/1000 | Loss: 0.00001008
Iteration 78/1000 | Loss: 0.00001008
Iteration 79/1000 | Loss: 0.00001007
Iteration 80/1000 | Loss: 0.00001007
Iteration 81/1000 | Loss: 0.00001006
Iteration 82/1000 | Loss: 0.00001005
Iteration 83/1000 | Loss: 0.00001005
Iteration 84/1000 | Loss: 0.00001005
Iteration 85/1000 | Loss: 0.00001004
Iteration 86/1000 | Loss: 0.00001004
Iteration 87/1000 | Loss: 0.00001004
Iteration 88/1000 | Loss: 0.00001004
Iteration 89/1000 | Loss: 0.00001003
Iteration 90/1000 | Loss: 0.00001003
Iteration 91/1000 | Loss: 0.00001002
Iteration 92/1000 | Loss: 0.00001002
Iteration 93/1000 | Loss: 0.00001002
Iteration 94/1000 | Loss: 0.00001002
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001001
Iteration 100/1000 | Loss: 0.00001001
Iteration 101/1000 | Loss: 0.00001001
Iteration 102/1000 | Loss: 0.00001001
Iteration 103/1000 | Loss: 0.00001001
Iteration 104/1000 | Loss: 0.00001001
Iteration 105/1000 | Loss: 0.00001001
Iteration 106/1000 | Loss: 0.00001000
Iteration 107/1000 | Loss: 0.00001000
Iteration 108/1000 | Loss: 0.00001000
Iteration 109/1000 | Loss: 0.00001000
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00000999
Iteration 112/1000 | Loss: 0.00000999
Iteration 113/1000 | Loss: 0.00000999
Iteration 114/1000 | Loss: 0.00000999
Iteration 115/1000 | Loss: 0.00000999
Iteration 116/1000 | Loss: 0.00000999
Iteration 117/1000 | Loss: 0.00000999
Iteration 118/1000 | Loss: 0.00000999
Iteration 119/1000 | Loss: 0.00000999
Iteration 120/1000 | Loss: 0.00000998
Iteration 121/1000 | Loss: 0.00000998
Iteration 122/1000 | Loss: 0.00000998
Iteration 123/1000 | Loss: 0.00000998
Iteration 124/1000 | Loss: 0.00000998
Iteration 125/1000 | Loss: 0.00000998
Iteration 126/1000 | Loss: 0.00000997
Iteration 127/1000 | Loss: 0.00000997
Iteration 128/1000 | Loss: 0.00000997
Iteration 129/1000 | Loss: 0.00000997
Iteration 130/1000 | Loss: 0.00000997
Iteration 131/1000 | Loss: 0.00000997
Iteration 132/1000 | Loss: 0.00000997
Iteration 133/1000 | Loss: 0.00000997
Iteration 134/1000 | Loss: 0.00000997
Iteration 135/1000 | Loss: 0.00000997
Iteration 136/1000 | Loss: 0.00000997
Iteration 137/1000 | Loss: 0.00000997
Iteration 138/1000 | Loss: 0.00000997
Iteration 139/1000 | Loss: 0.00000997
Iteration 140/1000 | Loss: 0.00000996
Iteration 141/1000 | Loss: 0.00000996
Iteration 142/1000 | Loss: 0.00000996
Iteration 143/1000 | Loss: 0.00000996
Iteration 144/1000 | Loss: 0.00000996
Iteration 145/1000 | Loss: 0.00000996
Iteration 146/1000 | Loss: 0.00000996
Iteration 147/1000 | Loss: 0.00000996
Iteration 148/1000 | Loss: 0.00000996
Iteration 149/1000 | Loss: 0.00000995
Iteration 150/1000 | Loss: 0.00000995
Iteration 151/1000 | Loss: 0.00000995
Iteration 152/1000 | Loss: 0.00000995
Iteration 153/1000 | Loss: 0.00000994
Iteration 154/1000 | Loss: 0.00000994
Iteration 155/1000 | Loss: 0.00000994
Iteration 156/1000 | Loss: 0.00000994
Iteration 157/1000 | Loss: 0.00000994
Iteration 158/1000 | Loss: 0.00000994
Iteration 159/1000 | Loss: 0.00000993
Iteration 160/1000 | Loss: 0.00000993
Iteration 161/1000 | Loss: 0.00000993
Iteration 162/1000 | Loss: 0.00000993
Iteration 163/1000 | Loss: 0.00000993
Iteration 164/1000 | Loss: 0.00000993
Iteration 165/1000 | Loss: 0.00000992
Iteration 166/1000 | Loss: 0.00000992
Iteration 167/1000 | Loss: 0.00000992
Iteration 168/1000 | Loss: 0.00000992
Iteration 169/1000 | Loss: 0.00000992
Iteration 170/1000 | Loss: 0.00000992
Iteration 171/1000 | Loss: 0.00000992
Iteration 172/1000 | Loss: 0.00000991
Iteration 173/1000 | Loss: 0.00000991
Iteration 174/1000 | Loss: 0.00000991
Iteration 175/1000 | Loss: 0.00000991
Iteration 176/1000 | Loss: 0.00000991
Iteration 177/1000 | Loss: 0.00000991
Iteration 178/1000 | Loss: 0.00000991
Iteration 179/1000 | Loss: 0.00000991
Iteration 180/1000 | Loss: 0.00000991
Iteration 181/1000 | Loss: 0.00000991
Iteration 182/1000 | Loss: 0.00000991
Iteration 183/1000 | Loss: 0.00000991
Iteration 184/1000 | Loss: 0.00000991
Iteration 185/1000 | Loss: 0.00000990
Iteration 186/1000 | Loss: 0.00000990
Iteration 187/1000 | Loss: 0.00000990
Iteration 188/1000 | Loss: 0.00000990
Iteration 189/1000 | Loss: 0.00000990
Iteration 190/1000 | Loss: 0.00000990
Iteration 191/1000 | Loss: 0.00000990
Iteration 192/1000 | Loss: 0.00000990
Iteration 193/1000 | Loss: 0.00000990
Iteration 194/1000 | Loss: 0.00000990
Iteration 195/1000 | Loss: 0.00000990
Iteration 196/1000 | Loss: 0.00000990
Iteration 197/1000 | Loss: 0.00000990
Iteration 198/1000 | Loss: 0.00000990
Iteration 199/1000 | Loss: 0.00000990
Iteration 200/1000 | Loss: 0.00000990
Iteration 201/1000 | Loss: 0.00000990
Iteration 202/1000 | Loss: 0.00000990
Iteration 203/1000 | Loss: 0.00000990
Iteration 204/1000 | Loss: 0.00000990
Iteration 205/1000 | Loss: 0.00000990
Iteration 206/1000 | Loss: 0.00000990
Iteration 207/1000 | Loss: 0.00000990
Iteration 208/1000 | Loss: 0.00000990
Iteration 209/1000 | Loss: 0.00000990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [9.901111297949683e-06, 9.901111297949683e-06, 9.901111297949683e-06, 9.901111297949683e-06, 9.901111297949683e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.901111297949683e-06

Optimization complete. Final v2v error: 2.70135760307312 mm

Highest mean error: 3.454380989074707 mm for frame 70

Lowest mean error: 2.4070093631744385 mm for frame 30

Saving results

Total time: 234.6337649822235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1094
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788598
Iteration 2/25 | Loss: 0.00190118
Iteration 3/25 | Loss: 0.00133724
Iteration 4/25 | Loss: 0.00129308
Iteration 5/25 | Loss: 0.00126496
Iteration 6/25 | Loss: 0.00127317
Iteration 7/25 | Loss: 0.00125175
Iteration 8/25 | Loss: 0.00124534
Iteration 9/25 | Loss: 0.00124456
Iteration 10/25 | Loss: 0.00124449
Iteration 11/25 | Loss: 0.00124448
Iteration 12/25 | Loss: 0.00124448
Iteration 13/25 | Loss: 0.00124448
Iteration 14/25 | Loss: 0.00124448
Iteration 15/25 | Loss: 0.00124448
Iteration 16/25 | Loss: 0.00124448
Iteration 17/25 | Loss: 0.00124448
Iteration 18/25 | Loss: 0.00124448
Iteration 19/25 | Loss: 0.00124448
Iteration 20/25 | Loss: 0.00124448
Iteration 21/25 | Loss: 0.00124448
Iteration 22/25 | Loss: 0.00124448
Iteration 23/25 | Loss: 0.00124448
Iteration 24/25 | Loss: 0.00124447
Iteration 25/25 | Loss: 0.00124447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.59014845
Iteration 2/25 | Loss: 0.00128270
Iteration 3/25 | Loss: 0.00128267
Iteration 4/25 | Loss: 0.00128267
Iteration 5/25 | Loss: 0.00128267
Iteration 6/25 | Loss: 0.00128267
Iteration 7/25 | Loss: 0.00128267
Iteration 8/25 | Loss: 0.00128267
Iteration 9/25 | Loss: 0.00128267
Iteration 10/25 | Loss: 0.00128267
Iteration 11/25 | Loss: 0.00128267
Iteration 12/25 | Loss: 0.00128267
Iteration 13/25 | Loss: 0.00128267
Iteration 14/25 | Loss: 0.00128266
Iteration 15/25 | Loss: 0.00128266
Iteration 16/25 | Loss: 0.00128267
Iteration 17/25 | Loss: 0.00128267
Iteration 18/25 | Loss: 0.00128267
Iteration 19/25 | Loss: 0.00128266
Iteration 20/25 | Loss: 0.00128266
Iteration 21/25 | Loss: 0.00128267
Iteration 22/25 | Loss: 0.00128267
Iteration 23/25 | Loss: 0.00128267
Iteration 24/25 | Loss: 0.00128267
Iteration 25/25 | Loss: 0.00128267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128267
Iteration 2/1000 | Loss: 0.00003147
Iteration 3/1000 | Loss: 0.00002125
Iteration 4/1000 | Loss: 0.00001863
Iteration 5/1000 | Loss: 0.00010726
Iteration 6/1000 | Loss: 0.00036293
Iteration 7/1000 | Loss: 0.00083741
Iteration 8/1000 | Loss: 0.00010474
Iteration 9/1000 | Loss: 0.00005660
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00005672
Iteration 12/1000 | Loss: 0.00044138
Iteration 13/1000 | Loss: 0.00003681
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00023420
Iteration 17/1000 | Loss: 0.00027239
Iteration 18/1000 | Loss: 0.00004616
Iteration 19/1000 | Loss: 0.00008774
Iteration 20/1000 | Loss: 0.00001657
Iteration 21/1000 | Loss: 0.00004624
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001577
Iteration 24/1000 | Loss: 0.00001573
Iteration 25/1000 | Loss: 0.00001569
Iteration 26/1000 | Loss: 0.00001569
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001556
Iteration 29/1000 | Loss: 0.00001554
Iteration 30/1000 | Loss: 0.00001553
Iteration 31/1000 | Loss: 0.00009176
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001534
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001521
Iteration 40/1000 | Loss: 0.00001520
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001516
Iteration 43/1000 | Loss: 0.00001516
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001516
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001514
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00008494
Iteration 61/1000 | Loss: 0.00015805
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001532
Iteration 64/1000 | Loss: 0.00001509
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001507
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001506
Iteration 75/1000 | Loss: 0.00001506
Iteration 76/1000 | Loss: 0.00001506
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001506
Iteration 80/1000 | Loss: 0.00001506
Iteration 81/1000 | Loss: 0.00001506
Iteration 82/1000 | Loss: 0.00001505
Iteration 83/1000 | Loss: 0.00001505
Iteration 84/1000 | Loss: 0.00001505
Iteration 85/1000 | Loss: 0.00001505
Iteration 86/1000 | Loss: 0.00001505
Iteration 87/1000 | Loss: 0.00001505
Iteration 88/1000 | Loss: 0.00001505
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001505
Iteration 92/1000 | Loss: 0.00001504
Iteration 93/1000 | Loss: 0.00001504
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001503
Iteration 97/1000 | Loss: 0.00001503
Iteration 98/1000 | Loss: 0.00001503
Iteration 99/1000 | Loss: 0.00001503
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001503
Iteration 102/1000 | Loss: 0.00001503
Iteration 103/1000 | Loss: 0.00001503
Iteration 104/1000 | Loss: 0.00001503
Iteration 105/1000 | Loss: 0.00001503
Iteration 106/1000 | Loss: 0.00001503
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001503
Iteration 111/1000 | Loss: 0.00001503
Iteration 112/1000 | Loss: 0.00001503
Iteration 113/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.5026998880784959e-05, 1.5026998880784959e-05, 1.5026998880784959e-05, 1.5026998880784959e-05, 1.5026998880784959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5026998880784959e-05

Optimization complete. Final v2v error: 3.2698190212249756 mm

Highest mean error: 3.6263418197631836 mm for frame 135

Lowest mean error: 2.789214849472046 mm for frame 178

Saving results

Total time: 555.2930495738983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1084
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00723923
Iteration 2/25 | Loss: 0.00126360
Iteration 3/25 | Loss: 0.00116893
Iteration 4/25 | Loss: 0.00116024
Iteration 5/25 | Loss: 0.00115884
Iteration 6/25 | Loss: 0.00115884
Iteration 7/25 | Loss: 0.00115884
Iteration 8/25 | Loss: 0.00115884
Iteration 9/25 | Loss: 0.00115884
Iteration 10/25 | Loss: 0.00115884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011588421184569597, 0.0011588421184569597, 0.0011588421184569597, 0.0011588421184569597, 0.0011588421184569597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011588421184569597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 16.67928314
Iteration 2/25 | Loss: 0.00181855
Iteration 3/25 | Loss: 0.00181849
Iteration 4/25 | Loss: 0.00181849
Iteration 5/25 | Loss: 0.00181848
Iteration 6/25 | Loss: 0.00181848
Iteration 7/25 | Loss: 0.00181848
Iteration 8/25 | Loss: 0.00181848
Iteration 9/25 | Loss: 0.00181848
Iteration 10/25 | Loss: 0.00181848
Iteration 11/25 | Loss: 0.00181848
Iteration 12/25 | Loss: 0.00181848
Iteration 13/25 | Loss: 0.00181848
Iteration 14/25 | Loss: 0.00181848
Iteration 15/25 | Loss: 0.00181848
Iteration 16/25 | Loss: 0.00181848
Iteration 17/25 | Loss: 0.00181848
Iteration 18/25 | Loss: 0.00181848
Iteration 19/25 | Loss: 0.00181848
Iteration 20/25 | Loss: 0.00181848
Iteration 21/25 | Loss: 0.00181848
Iteration 22/25 | Loss: 0.00181848
Iteration 23/25 | Loss: 0.00181848
Iteration 24/25 | Loss: 0.00181848
Iteration 25/25 | Loss: 0.00181848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181848
Iteration 2/1000 | Loss: 0.00001670
Iteration 3/1000 | Loss: 0.00001291
Iteration 4/1000 | Loss: 0.00001167
Iteration 5/1000 | Loss: 0.00001106
Iteration 6/1000 | Loss: 0.00001064
Iteration 7/1000 | Loss: 0.00001034
Iteration 8/1000 | Loss: 0.00001011
Iteration 9/1000 | Loss: 0.00000994
Iteration 10/1000 | Loss: 0.00000976
Iteration 11/1000 | Loss: 0.00000951
Iteration 12/1000 | Loss: 0.00000936
Iteration 13/1000 | Loss: 0.00000926
Iteration 14/1000 | Loss: 0.00000926
Iteration 15/1000 | Loss: 0.00000925
Iteration 16/1000 | Loss: 0.00000918
Iteration 17/1000 | Loss: 0.00000909
Iteration 18/1000 | Loss: 0.00000901
Iteration 19/1000 | Loss: 0.00000894
Iteration 20/1000 | Loss: 0.00000890
Iteration 21/1000 | Loss: 0.00000890
Iteration 22/1000 | Loss: 0.00000890
Iteration 23/1000 | Loss: 0.00000890
Iteration 24/1000 | Loss: 0.00000888
Iteration 25/1000 | Loss: 0.00000887
Iteration 26/1000 | Loss: 0.00000886
Iteration 27/1000 | Loss: 0.00000883
Iteration 28/1000 | Loss: 0.00000882
Iteration 29/1000 | Loss: 0.00000881
Iteration 30/1000 | Loss: 0.00000880
Iteration 31/1000 | Loss: 0.00000878
Iteration 32/1000 | Loss: 0.00000877
Iteration 33/1000 | Loss: 0.00000876
Iteration 34/1000 | Loss: 0.00000875
Iteration 35/1000 | Loss: 0.00000875
Iteration 36/1000 | Loss: 0.00000874
Iteration 37/1000 | Loss: 0.00000874
Iteration 38/1000 | Loss: 0.00000874
Iteration 39/1000 | Loss: 0.00000874
Iteration 40/1000 | Loss: 0.00000874
Iteration 41/1000 | Loss: 0.00000874
Iteration 42/1000 | Loss: 0.00000873
Iteration 43/1000 | Loss: 0.00000873
Iteration 44/1000 | Loss: 0.00000873
Iteration 45/1000 | Loss: 0.00000873
Iteration 46/1000 | Loss: 0.00000873
Iteration 47/1000 | Loss: 0.00000872
Iteration 48/1000 | Loss: 0.00000872
Iteration 49/1000 | Loss: 0.00000872
Iteration 50/1000 | Loss: 0.00000872
Iteration 51/1000 | Loss: 0.00000871
Iteration 52/1000 | Loss: 0.00000871
Iteration 53/1000 | Loss: 0.00000871
Iteration 54/1000 | Loss: 0.00000871
Iteration 55/1000 | Loss: 0.00000871
Iteration 56/1000 | Loss: 0.00000871
Iteration 57/1000 | Loss: 0.00000871
Iteration 58/1000 | Loss: 0.00000871
Iteration 59/1000 | Loss: 0.00000871
Iteration 60/1000 | Loss: 0.00000870
Iteration 61/1000 | Loss: 0.00000870
Iteration 62/1000 | Loss: 0.00000870
Iteration 63/1000 | Loss: 0.00000870
Iteration 64/1000 | Loss: 0.00000870
Iteration 65/1000 | Loss: 0.00000870
Iteration 66/1000 | Loss: 0.00000870
Iteration 67/1000 | Loss: 0.00000869
Iteration 68/1000 | Loss: 0.00000869
Iteration 69/1000 | Loss: 0.00000869
Iteration 70/1000 | Loss: 0.00000868
Iteration 71/1000 | Loss: 0.00000868
Iteration 72/1000 | Loss: 0.00000868
Iteration 73/1000 | Loss: 0.00000868
Iteration 74/1000 | Loss: 0.00000868
Iteration 75/1000 | Loss: 0.00000867
Iteration 76/1000 | Loss: 0.00000867
Iteration 77/1000 | Loss: 0.00000867
Iteration 78/1000 | Loss: 0.00000867
Iteration 79/1000 | Loss: 0.00000867
Iteration 80/1000 | Loss: 0.00000867
Iteration 81/1000 | Loss: 0.00000867
Iteration 82/1000 | Loss: 0.00000867
Iteration 83/1000 | Loss: 0.00000866
Iteration 84/1000 | Loss: 0.00000866
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000865
Iteration 87/1000 | Loss: 0.00000865
Iteration 88/1000 | Loss: 0.00000864
Iteration 89/1000 | Loss: 0.00000864
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000864
Iteration 93/1000 | Loss: 0.00000864
Iteration 94/1000 | Loss: 0.00000864
Iteration 95/1000 | Loss: 0.00000863
Iteration 96/1000 | Loss: 0.00000863
Iteration 97/1000 | Loss: 0.00000863
Iteration 98/1000 | Loss: 0.00000862
Iteration 99/1000 | Loss: 0.00000862
Iteration 100/1000 | Loss: 0.00000862
Iteration 101/1000 | Loss: 0.00000862
Iteration 102/1000 | Loss: 0.00000862
Iteration 103/1000 | Loss: 0.00000862
Iteration 104/1000 | Loss: 0.00000862
Iteration 105/1000 | Loss: 0.00000861
Iteration 106/1000 | Loss: 0.00000861
Iteration 107/1000 | Loss: 0.00000861
Iteration 108/1000 | Loss: 0.00000861
Iteration 109/1000 | Loss: 0.00000860
Iteration 110/1000 | Loss: 0.00000860
Iteration 111/1000 | Loss: 0.00000860
Iteration 112/1000 | Loss: 0.00000859
Iteration 113/1000 | Loss: 0.00000859
Iteration 114/1000 | Loss: 0.00000859
Iteration 115/1000 | Loss: 0.00000859
Iteration 116/1000 | Loss: 0.00000859
Iteration 117/1000 | Loss: 0.00000859
Iteration 118/1000 | Loss: 0.00000859
Iteration 119/1000 | Loss: 0.00000859
Iteration 120/1000 | Loss: 0.00000859
Iteration 121/1000 | Loss: 0.00000859
Iteration 122/1000 | Loss: 0.00000858
Iteration 123/1000 | Loss: 0.00000858
Iteration 124/1000 | Loss: 0.00000858
Iteration 125/1000 | Loss: 0.00000858
Iteration 126/1000 | Loss: 0.00000858
Iteration 127/1000 | Loss: 0.00000858
Iteration 128/1000 | Loss: 0.00000858
Iteration 129/1000 | Loss: 0.00000857
Iteration 130/1000 | Loss: 0.00000857
Iteration 131/1000 | Loss: 0.00000857
Iteration 132/1000 | Loss: 0.00000857
Iteration 133/1000 | Loss: 0.00000857
Iteration 134/1000 | Loss: 0.00000857
Iteration 135/1000 | Loss: 0.00000857
Iteration 136/1000 | Loss: 0.00000857
Iteration 137/1000 | Loss: 0.00000857
Iteration 138/1000 | Loss: 0.00000857
Iteration 139/1000 | Loss: 0.00000857
Iteration 140/1000 | Loss: 0.00000857
Iteration 141/1000 | Loss: 0.00000857
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000857
Iteration 147/1000 | Loss: 0.00000857
Iteration 148/1000 | Loss: 0.00000857
Iteration 149/1000 | Loss: 0.00000857
Iteration 150/1000 | Loss: 0.00000856
Iteration 151/1000 | Loss: 0.00000856
Iteration 152/1000 | Loss: 0.00000856
Iteration 153/1000 | Loss: 0.00000856
Iteration 154/1000 | Loss: 0.00000856
Iteration 155/1000 | Loss: 0.00000856
Iteration 156/1000 | Loss: 0.00000856
Iteration 157/1000 | Loss: 0.00000856
Iteration 158/1000 | Loss: 0.00000856
Iteration 159/1000 | Loss: 0.00000856
Iteration 160/1000 | Loss: 0.00000856
Iteration 161/1000 | Loss: 0.00000856
Iteration 162/1000 | Loss: 0.00000855
Iteration 163/1000 | Loss: 0.00000855
Iteration 164/1000 | Loss: 0.00000855
Iteration 165/1000 | Loss: 0.00000855
Iteration 166/1000 | Loss: 0.00000855
Iteration 167/1000 | Loss: 0.00000855
Iteration 168/1000 | Loss: 0.00000855
Iteration 169/1000 | Loss: 0.00000855
Iteration 170/1000 | Loss: 0.00000855
Iteration 171/1000 | Loss: 0.00000855
Iteration 172/1000 | Loss: 0.00000855
Iteration 173/1000 | Loss: 0.00000855
Iteration 174/1000 | Loss: 0.00000855
Iteration 175/1000 | Loss: 0.00000855
Iteration 176/1000 | Loss: 0.00000855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [8.553590305382386e-06, 8.553590305382386e-06, 8.553590305382386e-06, 8.553590305382386e-06, 8.553590305382386e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.553590305382386e-06

Optimization complete. Final v2v error: 2.5152647495269775 mm

Highest mean error: 2.9897994995117188 mm for frame 149

Lowest mean error: 2.2681875228881836 mm for frame 58

Saving results

Total time: 334.1168293952942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1004
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847138
Iteration 2/25 | Loss: 0.00142404
Iteration 3/25 | Loss: 0.00121324
Iteration 4/25 | Loss: 0.00119280
Iteration 5/25 | Loss: 0.00118930
Iteration 6/25 | Loss: 0.00118930
Iteration 7/25 | Loss: 0.00118930
Iteration 8/25 | Loss: 0.00118930
Iteration 9/25 | Loss: 0.00118930
Iteration 10/25 | Loss: 0.00118930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011893039336428046, 0.0011893039336428046, 0.0011893039336428046, 0.0011893039336428046, 0.0011893039336428046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011893039336428046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84040570
Iteration 2/25 | Loss: 0.00109933
Iteration 3/25 | Loss: 0.00109933
Iteration 4/25 | Loss: 0.00109933
Iteration 5/25 | Loss: 0.00109932
Iteration 6/25 | Loss: 0.00109932
Iteration 7/25 | Loss: 0.00109932
Iteration 8/25 | Loss: 0.00109932
Iteration 9/25 | Loss: 0.00109932
Iteration 10/25 | Loss: 0.00109932
Iteration 11/25 | Loss: 0.00109932
Iteration 12/25 | Loss: 0.00109932
Iteration 13/25 | Loss: 0.00109932
Iteration 14/25 | Loss: 0.00109932
Iteration 15/25 | Loss: 0.00109932
Iteration 16/25 | Loss: 0.00109932
Iteration 17/25 | Loss: 0.00109932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001099322340451181, 0.001099322340451181, 0.001099322340451181, 0.001099322340451181, 0.001099322340451181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001099322340451181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109932
Iteration 2/1000 | Loss: 0.00002974
Iteration 3/1000 | Loss: 0.00002376
Iteration 4/1000 | Loss: 0.00002185
Iteration 5/1000 | Loss: 0.00002087
Iteration 6/1000 | Loss: 0.00002020
Iteration 7/1000 | Loss: 0.00001965
Iteration 8/1000 | Loss: 0.00001915
Iteration 9/1000 | Loss: 0.00001879
Iteration 10/1000 | Loss: 0.00001856
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001808
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001805
Iteration 18/1000 | Loss: 0.00001805
Iteration 19/1000 | Loss: 0.00001804
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001803
Iteration 22/1000 | Loss: 0.00001802
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001801
Iteration 25/1000 | Loss: 0.00001801
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001801
Iteration 28/1000 | Loss: 0.00001801
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001801
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001801
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001799
Iteration 41/1000 | Loss: 0.00001799
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001798
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001797
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001796
Iteration 51/1000 | Loss: 0.00001796
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001796
Iteration 54/1000 | Loss: 0.00001796
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001794
Iteration 60/1000 | Loss: 0.00001794
Iteration 61/1000 | Loss: 0.00001794
Iteration 62/1000 | Loss: 0.00001794
Iteration 63/1000 | Loss: 0.00001794
Iteration 64/1000 | Loss: 0.00001794
Iteration 65/1000 | Loss: 0.00001794
Iteration 66/1000 | Loss: 0.00001794
Iteration 67/1000 | Loss: 0.00001794
Iteration 68/1000 | Loss: 0.00001794
Iteration 69/1000 | Loss: 0.00001794
Iteration 70/1000 | Loss: 0.00001794
Iteration 71/1000 | Loss: 0.00001794
Iteration 72/1000 | Loss: 0.00001794
Iteration 73/1000 | Loss: 0.00001793
Iteration 74/1000 | Loss: 0.00001793
Iteration 75/1000 | Loss: 0.00001793
Iteration 76/1000 | Loss: 0.00001793
Iteration 77/1000 | Loss: 0.00001793
Iteration 78/1000 | Loss: 0.00001793
Iteration 79/1000 | Loss: 0.00001793
Iteration 80/1000 | Loss: 0.00001792
Iteration 81/1000 | Loss: 0.00001792
Iteration 82/1000 | Loss: 0.00001792
Iteration 83/1000 | Loss: 0.00001792
Iteration 84/1000 | Loss: 0.00001792
Iteration 85/1000 | Loss: 0.00001791
Iteration 86/1000 | Loss: 0.00001791
Iteration 87/1000 | Loss: 0.00001791
Iteration 88/1000 | Loss: 0.00001789
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001789
Iteration 92/1000 | Loss: 0.00001789
Iteration 93/1000 | Loss: 0.00001789
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001789
Iteration 96/1000 | Loss: 0.00001789
Iteration 97/1000 | Loss: 0.00001789
Iteration 98/1000 | Loss: 0.00001789
Iteration 99/1000 | Loss: 0.00001789
Iteration 100/1000 | Loss: 0.00001788
Iteration 101/1000 | Loss: 0.00001788
Iteration 102/1000 | Loss: 0.00001788
Iteration 103/1000 | Loss: 0.00001788
Iteration 104/1000 | Loss: 0.00001788
Iteration 105/1000 | Loss: 0.00001788
Iteration 106/1000 | Loss: 0.00001788
Iteration 107/1000 | Loss: 0.00001788
Iteration 108/1000 | Loss: 0.00001788
Iteration 109/1000 | Loss: 0.00001788
Iteration 110/1000 | Loss: 0.00001788
Iteration 111/1000 | Loss: 0.00001788
Iteration 112/1000 | Loss: 0.00001788
Iteration 113/1000 | Loss: 0.00001788
Iteration 114/1000 | Loss: 0.00001788
Iteration 115/1000 | Loss: 0.00001788
Iteration 116/1000 | Loss: 0.00001788
Iteration 117/1000 | Loss: 0.00001788
Iteration 118/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.7884034605231136e-05, 1.7884034605231136e-05, 1.7884034605231136e-05, 1.7884034605231136e-05, 1.7884034605231136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7884034605231136e-05

Optimization complete. Final v2v error: 3.5912516117095947 mm

Highest mean error: 3.799812078475952 mm for frame 0

Lowest mean error: 3.509937286376953 mm for frame 158

Saving results

Total time: 328.5548493862152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1086
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00359702
Iteration 2/25 | Loss: 0.00117530
Iteration 3/25 | Loss: 0.00111804
Iteration 4/25 | Loss: 0.00110873
Iteration 5/25 | Loss: 0.00110579
Iteration 6/25 | Loss: 0.00110509
Iteration 7/25 | Loss: 0.00110509
Iteration 8/25 | Loss: 0.00110509
Iteration 9/25 | Loss: 0.00110509
Iteration 10/25 | Loss: 0.00110509
Iteration 11/25 | Loss: 0.00110509
Iteration 12/25 | Loss: 0.00110509
Iteration 13/25 | Loss: 0.00110509
Iteration 14/25 | Loss: 0.00110509
Iteration 15/25 | Loss: 0.00110509
Iteration 16/25 | Loss: 0.00110509
Iteration 17/25 | Loss: 0.00110509
Iteration 18/25 | Loss: 0.00110509
Iteration 19/25 | Loss: 0.00110509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001105090486817062, 0.001105090486817062, 0.001105090486817062, 0.001105090486817062, 0.001105090486817062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001105090486817062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33955300
Iteration 2/25 | Loss: 0.00188523
Iteration 3/25 | Loss: 0.00188523
Iteration 4/25 | Loss: 0.00188523
Iteration 5/25 | Loss: 0.00188523
Iteration 6/25 | Loss: 0.00188523
Iteration 7/25 | Loss: 0.00188523
Iteration 8/25 | Loss: 0.00188523
Iteration 9/25 | Loss: 0.00188523
Iteration 10/25 | Loss: 0.00188523
Iteration 11/25 | Loss: 0.00188523
Iteration 12/25 | Loss: 0.00188523
Iteration 13/25 | Loss: 0.00188523
Iteration 14/25 | Loss: 0.00188523
Iteration 15/25 | Loss: 0.00188523
Iteration 16/25 | Loss: 0.00188523
Iteration 17/25 | Loss: 0.00188523
Iteration 18/25 | Loss: 0.00188523
Iteration 19/25 | Loss: 0.00188523
Iteration 20/25 | Loss: 0.00188523
Iteration 21/25 | Loss: 0.00188523
Iteration 22/25 | Loss: 0.00188523
Iteration 23/25 | Loss: 0.00188523
Iteration 24/25 | Loss: 0.00188523
Iteration 25/25 | Loss: 0.00188523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0018852303037419915, 0.0018852303037419915, 0.0018852303037419915, 0.0018852303037419915, 0.0018852303037419915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018852303037419915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188523
Iteration 2/1000 | Loss: 0.00001905
Iteration 3/1000 | Loss: 0.00001335
Iteration 4/1000 | Loss: 0.00001070
Iteration 5/1000 | Loss: 0.00000974
Iteration 6/1000 | Loss: 0.00000932
Iteration 7/1000 | Loss: 0.00000886
Iteration 8/1000 | Loss: 0.00000840
Iteration 9/1000 | Loss: 0.00000831
Iteration 10/1000 | Loss: 0.00000830
Iteration 11/1000 | Loss: 0.00000812
Iteration 12/1000 | Loss: 0.00000787
Iteration 13/1000 | Loss: 0.00000773
Iteration 14/1000 | Loss: 0.00000767
Iteration 15/1000 | Loss: 0.00000763
Iteration 16/1000 | Loss: 0.00000763
Iteration 17/1000 | Loss: 0.00000756
Iteration 18/1000 | Loss: 0.00000752
Iteration 19/1000 | Loss: 0.00000752
Iteration 20/1000 | Loss: 0.00000751
Iteration 21/1000 | Loss: 0.00000751
Iteration 22/1000 | Loss: 0.00000751
Iteration 23/1000 | Loss: 0.00000746
Iteration 24/1000 | Loss: 0.00000744
Iteration 25/1000 | Loss: 0.00000744
Iteration 26/1000 | Loss: 0.00000744
Iteration 27/1000 | Loss: 0.00000743
Iteration 28/1000 | Loss: 0.00000742
Iteration 29/1000 | Loss: 0.00000741
Iteration 30/1000 | Loss: 0.00000740
Iteration 31/1000 | Loss: 0.00000740
Iteration 32/1000 | Loss: 0.00000739
Iteration 33/1000 | Loss: 0.00000739
Iteration 34/1000 | Loss: 0.00000739
Iteration 35/1000 | Loss: 0.00000738
Iteration 36/1000 | Loss: 0.00000737
Iteration 37/1000 | Loss: 0.00000737
Iteration 38/1000 | Loss: 0.00000735
Iteration 39/1000 | Loss: 0.00000734
Iteration 40/1000 | Loss: 0.00000734
Iteration 41/1000 | Loss: 0.00000733
Iteration 42/1000 | Loss: 0.00000733
Iteration 43/1000 | Loss: 0.00000732
Iteration 44/1000 | Loss: 0.00000732
Iteration 45/1000 | Loss: 0.00000731
Iteration 46/1000 | Loss: 0.00000730
Iteration 47/1000 | Loss: 0.00000724
Iteration 48/1000 | Loss: 0.00000719
Iteration 49/1000 | Loss: 0.00000718
Iteration 50/1000 | Loss: 0.00000718
Iteration 51/1000 | Loss: 0.00000717
Iteration 52/1000 | Loss: 0.00000716
Iteration 53/1000 | Loss: 0.00000716
Iteration 54/1000 | Loss: 0.00000716
Iteration 55/1000 | Loss: 0.00000716
Iteration 56/1000 | Loss: 0.00000716
Iteration 57/1000 | Loss: 0.00000715
Iteration 58/1000 | Loss: 0.00000715
Iteration 59/1000 | Loss: 0.00000715
Iteration 60/1000 | Loss: 0.00000715
Iteration 61/1000 | Loss: 0.00000715
Iteration 62/1000 | Loss: 0.00000715
Iteration 63/1000 | Loss: 0.00000713
Iteration 64/1000 | Loss: 0.00000713
Iteration 65/1000 | Loss: 0.00000713
Iteration 66/1000 | Loss: 0.00000712
Iteration 67/1000 | Loss: 0.00000712
Iteration 68/1000 | Loss: 0.00000712
Iteration 69/1000 | Loss: 0.00000712
Iteration 70/1000 | Loss: 0.00000712
Iteration 71/1000 | Loss: 0.00000712
Iteration 72/1000 | Loss: 0.00000712
Iteration 73/1000 | Loss: 0.00000712
Iteration 74/1000 | Loss: 0.00000712
Iteration 75/1000 | Loss: 0.00000712
Iteration 76/1000 | Loss: 0.00000711
Iteration 77/1000 | Loss: 0.00000711
Iteration 78/1000 | Loss: 0.00000711
Iteration 79/1000 | Loss: 0.00000711
Iteration 80/1000 | Loss: 0.00000711
Iteration 81/1000 | Loss: 0.00000711
Iteration 82/1000 | Loss: 0.00000711
Iteration 83/1000 | Loss: 0.00000710
Iteration 84/1000 | Loss: 0.00000710
Iteration 85/1000 | Loss: 0.00000710
Iteration 86/1000 | Loss: 0.00000710
Iteration 87/1000 | Loss: 0.00000710
Iteration 88/1000 | Loss: 0.00000709
Iteration 89/1000 | Loss: 0.00000709
Iteration 90/1000 | Loss: 0.00000709
Iteration 91/1000 | Loss: 0.00000709
Iteration 92/1000 | Loss: 0.00000709
Iteration 93/1000 | Loss: 0.00000709
Iteration 94/1000 | Loss: 0.00000709
Iteration 95/1000 | Loss: 0.00000709
Iteration 96/1000 | Loss: 0.00000709
Iteration 97/1000 | Loss: 0.00000708
Iteration 98/1000 | Loss: 0.00000708
Iteration 99/1000 | Loss: 0.00000708
Iteration 100/1000 | Loss: 0.00000708
Iteration 101/1000 | Loss: 0.00000708
Iteration 102/1000 | Loss: 0.00000708
Iteration 103/1000 | Loss: 0.00000708
Iteration 104/1000 | Loss: 0.00000708
Iteration 105/1000 | Loss: 0.00000708
Iteration 106/1000 | Loss: 0.00000708
Iteration 107/1000 | Loss: 0.00000707
Iteration 108/1000 | Loss: 0.00000707
Iteration 109/1000 | Loss: 0.00000707
Iteration 110/1000 | Loss: 0.00000707
Iteration 111/1000 | Loss: 0.00000707
Iteration 112/1000 | Loss: 0.00000707
Iteration 113/1000 | Loss: 0.00000707
Iteration 114/1000 | Loss: 0.00000707
Iteration 115/1000 | Loss: 0.00000706
Iteration 116/1000 | Loss: 0.00000706
Iteration 117/1000 | Loss: 0.00000706
Iteration 118/1000 | Loss: 0.00000706
Iteration 119/1000 | Loss: 0.00000706
Iteration 120/1000 | Loss: 0.00000706
Iteration 121/1000 | Loss: 0.00000706
Iteration 122/1000 | Loss: 0.00000706
Iteration 123/1000 | Loss: 0.00000706
Iteration 124/1000 | Loss: 0.00000706
Iteration 125/1000 | Loss: 0.00000706
Iteration 126/1000 | Loss: 0.00000705
Iteration 127/1000 | Loss: 0.00000705
Iteration 128/1000 | Loss: 0.00000705
Iteration 129/1000 | Loss: 0.00000705
Iteration 130/1000 | Loss: 0.00000705
Iteration 131/1000 | Loss: 0.00000705
Iteration 132/1000 | Loss: 0.00000705
Iteration 133/1000 | Loss: 0.00000705
Iteration 134/1000 | Loss: 0.00000705
Iteration 135/1000 | Loss: 0.00000705
Iteration 136/1000 | Loss: 0.00000705
Iteration 137/1000 | Loss: 0.00000705
Iteration 138/1000 | Loss: 0.00000705
Iteration 139/1000 | Loss: 0.00000705
Iteration 140/1000 | Loss: 0.00000705
Iteration 141/1000 | Loss: 0.00000705
Iteration 142/1000 | Loss: 0.00000705
Iteration 143/1000 | Loss: 0.00000705
Iteration 144/1000 | Loss: 0.00000705
Iteration 145/1000 | Loss: 0.00000705
Iteration 146/1000 | Loss: 0.00000705
Iteration 147/1000 | Loss: 0.00000705
Iteration 148/1000 | Loss: 0.00000705
Iteration 149/1000 | Loss: 0.00000705
Iteration 150/1000 | Loss: 0.00000705
Iteration 151/1000 | Loss: 0.00000705
Iteration 152/1000 | Loss: 0.00000705
Iteration 153/1000 | Loss: 0.00000705
Iteration 154/1000 | Loss: 0.00000705
Iteration 155/1000 | Loss: 0.00000705
Iteration 156/1000 | Loss: 0.00000705
Iteration 157/1000 | Loss: 0.00000705
Iteration 158/1000 | Loss: 0.00000705
Iteration 159/1000 | Loss: 0.00000705
Iteration 160/1000 | Loss: 0.00000705
Iteration 161/1000 | Loss: 0.00000705
Iteration 162/1000 | Loss: 0.00000705
Iteration 163/1000 | Loss: 0.00000705
Iteration 164/1000 | Loss: 0.00000705
Iteration 165/1000 | Loss: 0.00000705
Iteration 166/1000 | Loss: 0.00000705
Iteration 167/1000 | Loss: 0.00000705
Iteration 168/1000 | Loss: 0.00000705
Iteration 169/1000 | Loss: 0.00000705
Iteration 170/1000 | Loss: 0.00000705
Iteration 171/1000 | Loss: 0.00000705
Iteration 172/1000 | Loss: 0.00000705
Iteration 173/1000 | Loss: 0.00000705
Iteration 174/1000 | Loss: 0.00000705
Iteration 175/1000 | Loss: 0.00000705
Iteration 176/1000 | Loss: 0.00000705
Iteration 177/1000 | Loss: 0.00000705
Iteration 178/1000 | Loss: 0.00000705
Iteration 179/1000 | Loss: 0.00000705
Iteration 180/1000 | Loss: 0.00000705
Iteration 181/1000 | Loss: 0.00000705
Iteration 182/1000 | Loss: 0.00000705
Iteration 183/1000 | Loss: 0.00000705
Iteration 184/1000 | Loss: 0.00000705
Iteration 185/1000 | Loss: 0.00000705
Iteration 186/1000 | Loss: 0.00000705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [7.048560746625299e-06, 7.048560746625299e-06, 7.048560746625299e-06, 7.048560746625299e-06, 7.048560746625299e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.048560746625299e-06

Optimization complete. Final v2v error: 2.33013916015625 mm

Highest mean error: 2.5082061290740967 mm for frame 116

Lowest mean error: 2.270284652709961 mm for frame 130

Saving results

Total time: 238.7397177219391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1066
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453231
Iteration 2/25 | Loss: 0.00126493
Iteration 3/25 | Loss: 0.00117050
Iteration 4/25 | Loss: 0.00115832
Iteration 5/25 | Loss: 0.00115592
Iteration 6/25 | Loss: 0.00115547
Iteration 7/25 | Loss: 0.00115547
Iteration 8/25 | Loss: 0.00115547
Iteration 9/25 | Loss: 0.00115547
Iteration 10/25 | Loss: 0.00115547
Iteration 11/25 | Loss: 0.00115547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011554674711078405, 0.0011554674711078405, 0.0011554674711078405, 0.0011554674711078405, 0.0011554674711078405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011554674711078405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26041925
Iteration 2/25 | Loss: 0.00200749
Iteration 3/25 | Loss: 0.00200748
Iteration 4/25 | Loss: 0.00200748
Iteration 5/25 | Loss: 0.00200748
Iteration 6/25 | Loss: 0.00200748
Iteration 7/25 | Loss: 0.00200748
Iteration 8/25 | Loss: 0.00200748
Iteration 9/25 | Loss: 0.00200748
Iteration 10/25 | Loss: 0.00200748
Iteration 11/25 | Loss: 0.00200748
Iteration 12/25 | Loss: 0.00200748
Iteration 13/25 | Loss: 0.00200748
Iteration 14/25 | Loss: 0.00200748
Iteration 15/25 | Loss: 0.00200748
Iteration 16/25 | Loss: 0.00200748
Iteration 17/25 | Loss: 0.00200748
Iteration 18/25 | Loss: 0.00200748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020074762869626284, 0.0020074762869626284, 0.0020074762869626284, 0.0020074762869626284, 0.0020074762869626284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020074762869626284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200748
Iteration 2/1000 | Loss: 0.00002662
Iteration 3/1000 | Loss: 0.00001513
Iteration 4/1000 | Loss: 0.00001221
Iteration 5/1000 | Loss: 0.00001128
Iteration 6/1000 | Loss: 0.00001064
Iteration 7/1000 | Loss: 0.00001015
Iteration 8/1000 | Loss: 0.00000980
Iteration 9/1000 | Loss: 0.00000964
Iteration 10/1000 | Loss: 0.00000956
Iteration 11/1000 | Loss: 0.00000947
Iteration 12/1000 | Loss: 0.00000938
Iteration 13/1000 | Loss: 0.00000937
Iteration 14/1000 | Loss: 0.00000931
Iteration 15/1000 | Loss: 0.00000926
Iteration 16/1000 | Loss: 0.00000916
Iteration 17/1000 | Loss: 0.00000909
Iteration 18/1000 | Loss: 0.00000905
Iteration 19/1000 | Loss: 0.00000905
Iteration 20/1000 | Loss: 0.00000904
Iteration 21/1000 | Loss: 0.00000903
Iteration 22/1000 | Loss: 0.00000902
Iteration 23/1000 | Loss: 0.00000901
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000900
Iteration 26/1000 | Loss: 0.00000900
Iteration 27/1000 | Loss: 0.00000899
Iteration 28/1000 | Loss: 0.00000899
Iteration 29/1000 | Loss: 0.00000898
Iteration 30/1000 | Loss: 0.00000894
Iteration 31/1000 | Loss: 0.00000894
Iteration 32/1000 | Loss: 0.00000893
Iteration 33/1000 | Loss: 0.00000893
Iteration 34/1000 | Loss: 0.00000892
Iteration 35/1000 | Loss: 0.00000891
Iteration 36/1000 | Loss: 0.00000890
Iteration 37/1000 | Loss: 0.00000890
Iteration 38/1000 | Loss: 0.00000890
Iteration 39/1000 | Loss: 0.00000890
Iteration 40/1000 | Loss: 0.00000890
Iteration 41/1000 | Loss: 0.00000890
Iteration 42/1000 | Loss: 0.00000890
Iteration 43/1000 | Loss: 0.00000890
Iteration 44/1000 | Loss: 0.00000889
Iteration 45/1000 | Loss: 0.00000889
Iteration 46/1000 | Loss: 0.00000889
Iteration 47/1000 | Loss: 0.00000889
Iteration 48/1000 | Loss: 0.00000889
Iteration 49/1000 | Loss: 0.00000889
Iteration 50/1000 | Loss: 0.00000889
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000889
Iteration 53/1000 | Loss: 0.00000889
Iteration 54/1000 | Loss: 0.00000889
Iteration 55/1000 | Loss: 0.00000889
Iteration 56/1000 | Loss: 0.00000888
Iteration 57/1000 | Loss: 0.00000888
Iteration 58/1000 | Loss: 0.00000888
Iteration 59/1000 | Loss: 0.00000888
Iteration 60/1000 | Loss: 0.00000888
Iteration 61/1000 | Loss: 0.00000888
Iteration 62/1000 | Loss: 0.00000888
Iteration 63/1000 | Loss: 0.00000888
Iteration 64/1000 | Loss: 0.00000887
Iteration 65/1000 | Loss: 0.00000887
Iteration 66/1000 | Loss: 0.00000886
Iteration 67/1000 | Loss: 0.00000886
Iteration 68/1000 | Loss: 0.00000886
Iteration 69/1000 | Loss: 0.00000886
Iteration 70/1000 | Loss: 0.00000886
Iteration 71/1000 | Loss: 0.00000885
Iteration 72/1000 | Loss: 0.00000885
Iteration 73/1000 | Loss: 0.00000885
Iteration 74/1000 | Loss: 0.00000885
Iteration 75/1000 | Loss: 0.00000885
Iteration 76/1000 | Loss: 0.00000885
Iteration 77/1000 | Loss: 0.00000885
Iteration 78/1000 | Loss: 0.00000885
Iteration 79/1000 | Loss: 0.00000885
Iteration 80/1000 | Loss: 0.00000885
Iteration 81/1000 | Loss: 0.00000885
Iteration 82/1000 | Loss: 0.00000885
Iteration 83/1000 | Loss: 0.00000884
Iteration 84/1000 | Loss: 0.00000884
Iteration 85/1000 | Loss: 0.00000883
Iteration 86/1000 | Loss: 0.00000883
Iteration 87/1000 | Loss: 0.00000883
Iteration 88/1000 | Loss: 0.00000882
Iteration 89/1000 | Loss: 0.00000882
Iteration 90/1000 | Loss: 0.00000882
Iteration 91/1000 | Loss: 0.00000882
Iteration 92/1000 | Loss: 0.00000881
Iteration 93/1000 | Loss: 0.00000881
Iteration 94/1000 | Loss: 0.00000881
Iteration 95/1000 | Loss: 0.00000880
Iteration 96/1000 | Loss: 0.00000880
Iteration 97/1000 | Loss: 0.00000879
Iteration 98/1000 | Loss: 0.00000879
Iteration 99/1000 | Loss: 0.00000879
Iteration 100/1000 | Loss: 0.00000878
Iteration 101/1000 | Loss: 0.00000878
Iteration 102/1000 | Loss: 0.00000878
Iteration 103/1000 | Loss: 0.00000878
Iteration 104/1000 | Loss: 0.00000878
Iteration 105/1000 | Loss: 0.00000877
Iteration 106/1000 | Loss: 0.00000877
Iteration 107/1000 | Loss: 0.00000876
Iteration 108/1000 | Loss: 0.00000876
Iteration 109/1000 | Loss: 0.00000875
Iteration 110/1000 | Loss: 0.00000875
Iteration 111/1000 | Loss: 0.00000875
Iteration 112/1000 | Loss: 0.00000875
Iteration 113/1000 | Loss: 0.00000875
Iteration 114/1000 | Loss: 0.00000874
Iteration 115/1000 | Loss: 0.00000874
Iteration 116/1000 | Loss: 0.00000874
Iteration 117/1000 | Loss: 0.00000874
Iteration 118/1000 | Loss: 0.00000874
Iteration 119/1000 | Loss: 0.00000874
Iteration 120/1000 | Loss: 0.00000874
Iteration 121/1000 | Loss: 0.00000873
Iteration 122/1000 | Loss: 0.00000873
Iteration 123/1000 | Loss: 0.00000873
Iteration 124/1000 | Loss: 0.00000873
Iteration 125/1000 | Loss: 0.00000873
Iteration 126/1000 | Loss: 0.00000872
Iteration 127/1000 | Loss: 0.00000872
Iteration 128/1000 | Loss: 0.00000872
Iteration 129/1000 | Loss: 0.00000871
Iteration 130/1000 | Loss: 0.00000871
Iteration 131/1000 | Loss: 0.00000871
Iteration 132/1000 | Loss: 0.00000870
Iteration 133/1000 | Loss: 0.00000870
Iteration 134/1000 | Loss: 0.00000870
Iteration 135/1000 | Loss: 0.00000870
Iteration 136/1000 | Loss: 0.00000869
Iteration 137/1000 | Loss: 0.00000869
Iteration 138/1000 | Loss: 0.00000869
Iteration 139/1000 | Loss: 0.00000868
Iteration 140/1000 | Loss: 0.00000868
Iteration 141/1000 | Loss: 0.00000868
Iteration 142/1000 | Loss: 0.00000867
Iteration 143/1000 | Loss: 0.00000867
Iteration 144/1000 | Loss: 0.00000866
Iteration 145/1000 | Loss: 0.00000866
Iteration 146/1000 | Loss: 0.00000866
Iteration 147/1000 | Loss: 0.00000865
Iteration 148/1000 | Loss: 0.00000865
Iteration 149/1000 | Loss: 0.00000865
Iteration 150/1000 | Loss: 0.00000865
Iteration 151/1000 | Loss: 0.00000865
Iteration 152/1000 | Loss: 0.00000865
Iteration 153/1000 | Loss: 0.00000865
Iteration 154/1000 | Loss: 0.00000865
Iteration 155/1000 | Loss: 0.00000865
Iteration 156/1000 | Loss: 0.00000865
Iteration 157/1000 | Loss: 0.00000865
Iteration 158/1000 | Loss: 0.00000864
Iteration 159/1000 | Loss: 0.00000864
Iteration 160/1000 | Loss: 0.00000864
Iteration 161/1000 | Loss: 0.00000864
Iteration 162/1000 | Loss: 0.00000864
Iteration 163/1000 | Loss: 0.00000864
Iteration 164/1000 | Loss: 0.00000864
Iteration 165/1000 | Loss: 0.00000863
Iteration 166/1000 | Loss: 0.00000863
Iteration 167/1000 | Loss: 0.00000863
Iteration 168/1000 | Loss: 0.00000863
Iteration 169/1000 | Loss: 0.00000863
Iteration 170/1000 | Loss: 0.00000863
Iteration 171/1000 | Loss: 0.00000863
Iteration 172/1000 | Loss: 0.00000862
Iteration 173/1000 | Loss: 0.00000862
Iteration 174/1000 | Loss: 0.00000862
Iteration 175/1000 | Loss: 0.00000862
Iteration 176/1000 | Loss: 0.00000862
Iteration 177/1000 | Loss: 0.00000862
Iteration 178/1000 | Loss: 0.00000862
Iteration 179/1000 | Loss: 0.00000862
Iteration 180/1000 | Loss: 0.00000862
Iteration 181/1000 | Loss: 0.00000862
Iteration 182/1000 | Loss: 0.00000862
Iteration 183/1000 | Loss: 0.00000862
Iteration 184/1000 | Loss: 0.00000862
Iteration 185/1000 | Loss: 0.00000862
Iteration 186/1000 | Loss: 0.00000861
Iteration 187/1000 | Loss: 0.00000861
Iteration 188/1000 | Loss: 0.00000861
Iteration 189/1000 | Loss: 0.00000861
Iteration 190/1000 | Loss: 0.00000861
Iteration 191/1000 | Loss: 0.00000861
Iteration 192/1000 | Loss: 0.00000861
Iteration 193/1000 | Loss: 0.00000860
Iteration 194/1000 | Loss: 0.00000860
Iteration 195/1000 | Loss: 0.00000860
Iteration 196/1000 | Loss: 0.00000860
Iteration 197/1000 | Loss: 0.00000860
Iteration 198/1000 | Loss: 0.00000860
Iteration 199/1000 | Loss: 0.00000860
Iteration 200/1000 | Loss: 0.00000859
Iteration 201/1000 | Loss: 0.00000859
Iteration 202/1000 | Loss: 0.00000859
Iteration 203/1000 | Loss: 0.00000859
Iteration 204/1000 | Loss: 0.00000859
Iteration 205/1000 | Loss: 0.00000859
Iteration 206/1000 | Loss: 0.00000858
Iteration 207/1000 | Loss: 0.00000858
Iteration 208/1000 | Loss: 0.00000858
Iteration 209/1000 | Loss: 0.00000858
Iteration 210/1000 | Loss: 0.00000858
Iteration 211/1000 | Loss: 0.00000858
Iteration 212/1000 | Loss: 0.00000858
Iteration 213/1000 | Loss: 0.00000858
Iteration 214/1000 | Loss: 0.00000858
Iteration 215/1000 | Loss: 0.00000858
Iteration 216/1000 | Loss: 0.00000858
Iteration 217/1000 | Loss: 0.00000858
Iteration 218/1000 | Loss: 0.00000858
Iteration 219/1000 | Loss: 0.00000857
Iteration 220/1000 | Loss: 0.00000857
Iteration 221/1000 | Loss: 0.00000857
Iteration 222/1000 | Loss: 0.00000857
Iteration 223/1000 | Loss: 0.00000857
Iteration 224/1000 | Loss: 0.00000857
Iteration 225/1000 | Loss: 0.00000857
Iteration 226/1000 | Loss: 0.00000857
Iteration 227/1000 | Loss: 0.00000857
Iteration 228/1000 | Loss: 0.00000857
Iteration 229/1000 | Loss: 0.00000857
Iteration 230/1000 | Loss: 0.00000857
Iteration 231/1000 | Loss: 0.00000857
Iteration 232/1000 | Loss: 0.00000857
Iteration 233/1000 | Loss: 0.00000857
Iteration 234/1000 | Loss: 0.00000857
Iteration 235/1000 | Loss: 0.00000857
Iteration 236/1000 | Loss: 0.00000856
Iteration 237/1000 | Loss: 0.00000856
Iteration 238/1000 | Loss: 0.00000856
Iteration 239/1000 | Loss: 0.00000856
Iteration 240/1000 | Loss: 0.00000856
Iteration 241/1000 | Loss: 0.00000856
Iteration 242/1000 | Loss: 0.00000856
Iteration 243/1000 | Loss: 0.00000856
Iteration 244/1000 | Loss: 0.00000855
Iteration 245/1000 | Loss: 0.00000855
Iteration 246/1000 | Loss: 0.00000855
Iteration 247/1000 | Loss: 0.00000855
Iteration 248/1000 | Loss: 0.00000855
Iteration 249/1000 | Loss: 0.00000855
Iteration 250/1000 | Loss: 0.00000855
Iteration 251/1000 | Loss: 0.00000854
Iteration 252/1000 | Loss: 0.00000854
Iteration 253/1000 | Loss: 0.00000854
Iteration 254/1000 | Loss: 0.00000854
Iteration 255/1000 | Loss: 0.00000854
Iteration 256/1000 | Loss: 0.00000853
Iteration 257/1000 | Loss: 0.00000853
Iteration 258/1000 | Loss: 0.00000853
Iteration 259/1000 | Loss: 0.00000853
Iteration 260/1000 | Loss: 0.00000853
Iteration 261/1000 | Loss: 0.00000853
Iteration 262/1000 | Loss: 0.00000853
Iteration 263/1000 | Loss: 0.00000853
Iteration 264/1000 | Loss: 0.00000853
Iteration 265/1000 | Loss: 0.00000853
Iteration 266/1000 | Loss: 0.00000853
Iteration 267/1000 | Loss: 0.00000853
Iteration 268/1000 | Loss: 0.00000852
Iteration 269/1000 | Loss: 0.00000852
Iteration 270/1000 | Loss: 0.00000852
Iteration 271/1000 | Loss: 0.00000852
Iteration 272/1000 | Loss: 0.00000852
Iteration 273/1000 | Loss: 0.00000852
Iteration 274/1000 | Loss: 0.00000852
Iteration 275/1000 | Loss: 0.00000852
Iteration 276/1000 | Loss: 0.00000852
Iteration 277/1000 | Loss: 0.00000851
Iteration 278/1000 | Loss: 0.00000851
Iteration 279/1000 | Loss: 0.00000851
Iteration 280/1000 | Loss: 0.00000851
Iteration 281/1000 | Loss: 0.00000851
Iteration 282/1000 | Loss: 0.00000851
Iteration 283/1000 | Loss: 0.00000851
Iteration 284/1000 | Loss: 0.00000850
Iteration 285/1000 | Loss: 0.00000850
Iteration 286/1000 | Loss: 0.00000850
Iteration 287/1000 | Loss: 0.00000850
Iteration 288/1000 | Loss: 0.00000850
Iteration 289/1000 | Loss: 0.00000850
Iteration 290/1000 | Loss: 0.00000850
Iteration 291/1000 | Loss: 0.00000850
Iteration 292/1000 | Loss: 0.00000850
Iteration 293/1000 | Loss: 0.00000849
Iteration 294/1000 | Loss: 0.00000849
Iteration 295/1000 | Loss: 0.00000849
Iteration 296/1000 | Loss: 0.00000849
Iteration 297/1000 | Loss: 0.00000849
Iteration 298/1000 | Loss: 0.00000849
Iteration 299/1000 | Loss: 0.00000848
Iteration 300/1000 | Loss: 0.00000848
Iteration 301/1000 | Loss: 0.00000848
Iteration 302/1000 | Loss: 0.00000848
Iteration 303/1000 | Loss: 0.00000848
Iteration 304/1000 | Loss: 0.00000848
Iteration 305/1000 | Loss: 0.00000848
Iteration 306/1000 | Loss: 0.00000848
Iteration 307/1000 | Loss: 0.00000848
Iteration 308/1000 | Loss: 0.00000848
Iteration 309/1000 | Loss: 0.00000847
Iteration 310/1000 | Loss: 0.00000847
Iteration 311/1000 | Loss: 0.00000847
Iteration 312/1000 | Loss: 0.00000847
Iteration 313/1000 | Loss: 0.00000847
Iteration 314/1000 | Loss: 0.00000847
Iteration 315/1000 | Loss: 0.00000847
Iteration 316/1000 | Loss: 0.00000847
Iteration 317/1000 | Loss: 0.00000847
Iteration 318/1000 | Loss: 0.00000847
Iteration 319/1000 | Loss: 0.00000846
Iteration 320/1000 | Loss: 0.00000846
Iteration 321/1000 | Loss: 0.00000846
Iteration 322/1000 | Loss: 0.00000846
Iteration 323/1000 | Loss: 0.00000845
Iteration 324/1000 | Loss: 0.00000845
Iteration 325/1000 | Loss: 0.00000845
Iteration 326/1000 | Loss: 0.00000845
Iteration 327/1000 | Loss: 0.00000845
Iteration 328/1000 | Loss: 0.00000845
Iteration 329/1000 | Loss: 0.00000845
Iteration 330/1000 | Loss: 0.00000845
Iteration 331/1000 | Loss: 0.00000845
Iteration 332/1000 | Loss: 0.00000845
Iteration 333/1000 | Loss: 0.00000845
Iteration 334/1000 | Loss: 0.00000845
Iteration 335/1000 | Loss: 0.00000845
Iteration 336/1000 | Loss: 0.00000845
Iteration 337/1000 | Loss: 0.00000844
Iteration 338/1000 | Loss: 0.00000844
Iteration 339/1000 | Loss: 0.00000844
Iteration 340/1000 | Loss: 0.00000844
Iteration 341/1000 | Loss: 0.00000844
Iteration 342/1000 | Loss: 0.00000844
Iteration 343/1000 | Loss: 0.00000844
Iteration 344/1000 | Loss: 0.00000844
Iteration 345/1000 | Loss: 0.00000844
Iteration 346/1000 | Loss: 0.00000844
Iteration 347/1000 | Loss: 0.00000844
Iteration 348/1000 | Loss: 0.00000844
Iteration 349/1000 | Loss: 0.00000844
Iteration 350/1000 | Loss: 0.00000844
Iteration 351/1000 | Loss: 0.00000843
Iteration 352/1000 | Loss: 0.00000843
Iteration 353/1000 | Loss: 0.00000843
Iteration 354/1000 | Loss: 0.00000843
Iteration 355/1000 | Loss: 0.00000843
Iteration 356/1000 | Loss: 0.00000843
Iteration 357/1000 | Loss: 0.00000843
Iteration 358/1000 | Loss: 0.00000843
Iteration 359/1000 | Loss: 0.00000843
Iteration 360/1000 | Loss: 0.00000843
Iteration 361/1000 | Loss: 0.00000843
Iteration 362/1000 | Loss: 0.00000843
Iteration 363/1000 | Loss: 0.00000843
Iteration 364/1000 | Loss: 0.00000843
Iteration 365/1000 | Loss: 0.00000843
Iteration 366/1000 | Loss: 0.00000843
Iteration 367/1000 | Loss: 0.00000843
Iteration 368/1000 | Loss: 0.00000843
Iteration 369/1000 | Loss: 0.00000843
Iteration 370/1000 | Loss: 0.00000842
Iteration 371/1000 | Loss: 0.00000842
Iteration 372/1000 | Loss: 0.00000842
Iteration 373/1000 | Loss: 0.00000842
Iteration 374/1000 | Loss: 0.00000842
Iteration 375/1000 | Loss: 0.00000842
Iteration 376/1000 | Loss: 0.00000842
Iteration 377/1000 | Loss: 0.00000842
Iteration 378/1000 | Loss: 0.00000842
Iteration 379/1000 | Loss: 0.00000842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 379. Stopping optimization.
Last 5 losses: [8.424916813964956e-06, 8.424916813964956e-06, 8.424916813964956e-06, 8.424916813964956e-06, 8.424916813964956e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.424916813964956e-06

Optimization complete. Final v2v error: 2.3995587825775146 mm

Highest mean error: 2.8314850330352783 mm for frame 60

Lowest mean error: 2.2208752632141113 mm for frame 94

Saving results

Total time: 209.49128532409668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1005
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793264
Iteration 2/25 | Loss: 0.00139591
Iteration 3/25 | Loss: 0.00119824
Iteration 4/25 | Loss: 0.00117263
Iteration 5/25 | Loss: 0.00116791
Iteration 6/25 | Loss: 0.00116697
Iteration 7/25 | Loss: 0.00116697
Iteration 8/25 | Loss: 0.00116697
Iteration 9/25 | Loss: 0.00116697
Iteration 10/25 | Loss: 0.00116697
Iteration 11/25 | Loss: 0.00116697
Iteration 12/25 | Loss: 0.00116697
Iteration 13/25 | Loss: 0.00116697
Iteration 14/25 | Loss: 0.00116697
Iteration 15/25 | Loss: 0.00116697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011669696541503072, 0.0011669696541503072, 0.0011669696541503072, 0.0011669696541503072, 0.0011669696541503072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011669696541503072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56385303
Iteration 2/25 | Loss: 0.00199618
Iteration 3/25 | Loss: 0.00199613
Iteration 4/25 | Loss: 0.00199612
Iteration 5/25 | Loss: 0.00199612
Iteration 6/25 | Loss: 0.00199612
Iteration 7/25 | Loss: 0.00199612
Iteration 8/25 | Loss: 0.00199612
Iteration 9/25 | Loss: 0.00199612
Iteration 10/25 | Loss: 0.00199612
Iteration 11/25 | Loss: 0.00199612
Iteration 12/25 | Loss: 0.00199612
Iteration 13/25 | Loss: 0.00199612
Iteration 14/25 | Loss: 0.00199612
Iteration 15/25 | Loss: 0.00199612
Iteration 16/25 | Loss: 0.00199612
Iteration 17/25 | Loss: 0.00199612
Iteration 18/25 | Loss: 0.00199612
Iteration 19/25 | Loss: 0.00199612
Iteration 20/25 | Loss: 0.00199612
Iteration 21/25 | Loss: 0.00199612
Iteration 22/25 | Loss: 0.00199612
Iteration 23/25 | Loss: 0.00199612
Iteration 24/25 | Loss: 0.00199612
Iteration 25/25 | Loss: 0.00199612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199612
Iteration 2/1000 | Loss: 0.00004216
Iteration 3/1000 | Loss: 0.00002863
Iteration 4/1000 | Loss: 0.00002239
Iteration 5/1000 | Loss: 0.00002076
Iteration 6/1000 | Loss: 0.00001964
Iteration 7/1000 | Loss: 0.00001876
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001799
Iteration 10/1000 | Loss: 0.00001769
Iteration 11/1000 | Loss: 0.00001736
Iteration 12/1000 | Loss: 0.00001714
Iteration 13/1000 | Loss: 0.00001693
Iteration 14/1000 | Loss: 0.00001686
Iteration 15/1000 | Loss: 0.00001678
Iteration 16/1000 | Loss: 0.00001671
Iteration 17/1000 | Loss: 0.00001669
Iteration 18/1000 | Loss: 0.00001668
Iteration 19/1000 | Loss: 0.00001664
Iteration 20/1000 | Loss: 0.00001659
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001635
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001633
Iteration 28/1000 | Loss: 0.00001633
Iteration 29/1000 | Loss: 0.00001632
Iteration 30/1000 | Loss: 0.00001631
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001625
Iteration 46/1000 | Loss: 0.00001625
Iteration 47/1000 | Loss: 0.00001625
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001622
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001621
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001620
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001617
Iteration 69/1000 | Loss: 0.00001617
Iteration 70/1000 | Loss: 0.00001617
Iteration 71/1000 | Loss: 0.00001617
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001617
Iteration 75/1000 | Loss: 0.00001617
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001617
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001616
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001615
Iteration 93/1000 | Loss: 0.00001615
Iteration 94/1000 | Loss: 0.00001615
Iteration 95/1000 | Loss: 0.00001614
Iteration 96/1000 | Loss: 0.00001614
Iteration 97/1000 | Loss: 0.00001614
Iteration 98/1000 | Loss: 0.00001614
Iteration 99/1000 | Loss: 0.00001614
Iteration 100/1000 | Loss: 0.00001614
Iteration 101/1000 | Loss: 0.00001614
Iteration 102/1000 | Loss: 0.00001613
Iteration 103/1000 | Loss: 0.00001613
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001612
Iteration 109/1000 | Loss: 0.00001612
Iteration 110/1000 | Loss: 0.00001612
Iteration 111/1000 | Loss: 0.00001612
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001611
Iteration 122/1000 | Loss: 0.00001611
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001610
Iteration 125/1000 | Loss: 0.00001610
Iteration 126/1000 | Loss: 0.00001610
Iteration 127/1000 | Loss: 0.00001610
Iteration 128/1000 | Loss: 0.00001610
Iteration 129/1000 | Loss: 0.00001610
Iteration 130/1000 | Loss: 0.00001609
Iteration 131/1000 | Loss: 0.00001609
Iteration 132/1000 | Loss: 0.00001609
Iteration 133/1000 | Loss: 0.00001609
Iteration 134/1000 | Loss: 0.00001608
Iteration 135/1000 | Loss: 0.00001608
Iteration 136/1000 | Loss: 0.00001608
Iteration 137/1000 | Loss: 0.00001608
Iteration 138/1000 | Loss: 0.00001608
Iteration 139/1000 | Loss: 0.00001608
Iteration 140/1000 | Loss: 0.00001608
Iteration 141/1000 | Loss: 0.00001607
Iteration 142/1000 | Loss: 0.00001607
Iteration 143/1000 | Loss: 0.00001607
Iteration 144/1000 | Loss: 0.00001607
Iteration 145/1000 | Loss: 0.00001607
Iteration 146/1000 | Loss: 0.00001607
Iteration 147/1000 | Loss: 0.00001607
Iteration 148/1000 | Loss: 0.00001607
Iteration 149/1000 | Loss: 0.00001606
Iteration 150/1000 | Loss: 0.00001606
Iteration 151/1000 | Loss: 0.00001606
Iteration 152/1000 | Loss: 0.00001606
Iteration 153/1000 | Loss: 0.00001606
Iteration 154/1000 | Loss: 0.00001605
Iteration 155/1000 | Loss: 0.00001605
Iteration 156/1000 | Loss: 0.00001605
Iteration 157/1000 | Loss: 0.00001605
Iteration 158/1000 | Loss: 0.00001605
Iteration 159/1000 | Loss: 0.00001605
Iteration 160/1000 | Loss: 0.00001605
Iteration 161/1000 | Loss: 0.00001604
Iteration 162/1000 | Loss: 0.00001604
Iteration 163/1000 | Loss: 0.00001604
Iteration 164/1000 | Loss: 0.00001604
Iteration 165/1000 | Loss: 0.00001604
Iteration 166/1000 | Loss: 0.00001604
Iteration 167/1000 | Loss: 0.00001604
Iteration 168/1000 | Loss: 0.00001604
Iteration 169/1000 | Loss: 0.00001604
Iteration 170/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.6040428818087094e-05, 1.6040428818087094e-05, 1.6040428818087094e-05, 1.6040428818087094e-05, 1.6040428818087094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6040428818087094e-05

Optimization complete. Final v2v error: 3.313579797744751 mm

Highest mean error: 4.492044448852539 mm for frame 130

Lowest mean error: 2.4770305156707764 mm for frame 50

Saving results

Total time: 419.58793568611145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1096
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00733745
Iteration 2/25 | Loss: 0.00153396
Iteration 3/25 | Loss: 0.00140276
Iteration 4/25 | Loss: 0.00137837
Iteration 5/25 | Loss: 0.00137109
Iteration 6/25 | Loss: 0.00138020
Iteration 7/25 | Loss: 0.00127657
Iteration 8/25 | Loss: 0.00126748
Iteration 9/25 | Loss: 0.00126583
Iteration 10/25 | Loss: 0.00126310
Iteration 11/25 | Loss: 0.00128855
Iteration 12/25 | Loss: 0.00125956
Iteration 13/25 | Loss: 0.00125170
Iteration 14/25 | Loss: 0.00124846
Iteration 15/25 | Loss: 0.00124809
Iteration 16/25 | Loss: 0.00124800
Iteration 17/25 | Loss: 0.00127720
Iteration 18/25 | Loss: 0.00126621
Iteration 19/25 | Loss: 0.00123622
Iteration 20/25 | Loss: 0.00122400
Iteration 21/25 | Loss: 0.00121227
Iteration 22/25 | Loss: 0.00119714
Iteration 23/25 | Loss: 0.00118203
Iteration 24/25 | Loss: 0.00117805
Iteration 25/25 | Loss: 0.00117772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27692187
Iteration 2/25 | Loss: 0.00291907
Iteration 3/25 | Loss: 0.00291906
Iteration 4/25 | Loss: 0.00291906
Iteration 5/25 | Loss: 0.00291906
Iteration 6/25 | Loss: 0.00291906
Iteration 7/25 | Loss: 0.00291906
Iteration 8/25 | Loss: 0.00291906
Iteration 9/25 | Loss: 0.00291906
Iteration 10/25 | Loss: 0.00291906
Iteration 11/25 | Loss: 0.00291906
Iteration 12/25 | Loss: 0.00291906
Iteration 13/25 | Loss: 0.00291906
Iteration 14/25 | Loss: 0.00291906
Iteration 15/25 | Loss: 0.00291906
Iteration 16/25 | Loss: 0.00291906
Iteration 17/25 | Loss: 0.00291906
Iteration 18/25 | Loss: 0.00291906
Iteration 19/25 | Loss: 0.00291906
Iteration 20/25 | Loss: 0.00291906
Iteration 21/25 | Loss: 0.00291906
Iteration 22/25 | Loss: 0.00291906
Iteration 23/25 | Loss: 0.00291906
Iteration 24/25 | Loss: 0.00291906
Iteration 25/25 | Loss: 0.00291906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00291906
Iteration 2/1000 | Loss: 0.00005221
Iteration 3/1000 | Loss: 0.00003695
Iteration 4/1000 | Loss: 0.00002875
Iteration 5/1000 | Loss: 0.00002650
Iteration 6/1000 | Loss: 0.00002513
Iteration 7/1000 | Loss: 0.00002377
Iteration 8/1000 | Loss: 0.00002314
Iteration 9/1000 | Loss: 0.00002255
Iteration 10/1000 | Loss: 0.00002214
Iteration 11/1000 | Loss: 0.00002186
Iteration 12/1000 | Loss: 0.00002176
Iteration 13/1000 | Loss: 0.00002164
Iteration 14/1000 | Loss: 0.00002152
Iteration 15/1000 | Loss: 0.00002141
Iteration 16/1000 | Loss: 0.00002133
Iteration 17/1000 | Loss: 0.00002127
Iteration 18/1000 | Loss: 0.00002122
Iteration 19/1000 | Loss: 0.00002118
Iteration 20/1000 | Loss: 0.00002116
Iteration 21/1000 | Loss: 0.00002116
Iteration 22/1000 | Loss: 0.00002116
Iteration 23/1000 | Loss: 0.00002115
Iteration 24/1000 | Loss: 0.00002114
Iteration 25/1000 | Loss: 0.00002113
Iteration 26/1000 | Loss: 0.00002113
Iteration 27/1000 | Loss: 0.00002112
Iteration 28/1000 | Loss: 0.00002112
Iteration 29/1000 | Loss: 0.00002110
Iteration 30/1000 | Loss: 0.00002109
Iteration 31/1000 | Loss: 0.00002108
Iteration 32/1000 | Loss: 0.00002108
Iteration 33/1000 | Loss: 0.00002107
Iteration 34/1000 | Loss: 0.00002107
Iteration 35/1000 | Loss: 0.00002107
Iteration 36/1000 | Loss: 0.00002107
Iteration 37/1000 | Loss: 0.00002107
Iteration 38/1000 | Loss: 0.00002106
Iteration 39/1000 | Loss: 0.00002106
Iteration 40/1000 | Loss: 0.00002106
Iteration 41/1000 | Loss: 0.00002105
Iteration 42/1000 | Loss: 0.00002105
Iteration 43/1000 | Loss: 0.00002104
Iteration 44/1000 | Loss: 0.00002104
Iteration 45/1000 | Loss: 0.00002104
Iteration 46/1000 | Loss: 0.00002104
Iteration 47/1000 | Loss: 0.00002103
Iteration 48/1000 | Loss: 0.00002103
Iteration 49/1000 | Loss: 0.00002103
Iteration 50/1000 | Loss: 0.00002102
Iteration 51/1000 | Loss: 0.00002102
Iteration 52/1000 | Loss: 0.00002101
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002101
Iteration 55/1000 | Loss: 0.00002100
Iteration 56/1000 | Loss: 0.00002100
Iteration 57/1000 | Loss: 0.00002100
Iteration 58/1000 | Loss: 0.00002100
Iteration 59/1000 | Loss: 0.00002100
Iteration 60/1000 | Loss: 0.00002100
Iteration 61/1000 | Loss: 0.00002100
Iteration 62/1000 | Loss: 0.00002100
Iteration 63/1000 | Loss: 0.00002100
Iteration 64/1000 | Loss: 0.00002100
Iteration 65/1000 | Loss: 0.00002099
Iteration 66/1000 | Loss: 0.00002099
Iteration 67/1000 | Loss: 0.00002098
Iteration 68/1000 | Loss: 0.00002098
Iteration 69/1000 | Loss: 0.00002098
Iteration 70/1000 | Loss: 0.00002097
Iteration 71/1000 | Loss: 0.00002097
Iteration 72/1000 | Loss: 0.00002097
Iteration 73/1000 | Loss: 0.00002097
Iteration 74/1000 | Loss: 0.00002097
Iteration 75/1000 | Loss: 0.00002096
Iteration 76/1000 | Loss: 0.00002096
Iteration 77/1000 | Loss: 0.00002096
Iteration 78/1000 | Loss: 0.00002096
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002096
Iteration 83/1000 | Loss: 0.00002095
Iteration 84/1000 | Loss: 0.00002095
Iteration 85/1000 | Loss: 0.00002095
Iteration 86/1000 | Loss: 0.00002095
Iteration 87/1000 | Loss: 0.00002094
Iteration 88/1000 | Loss: 0.00002094
Iteration 89/1000 | Loss: 0.00002094
Iteration 90/1000 | Loss: 0.00002094
Iteration 91/1000 | Loss: 0.00002094
Iteration 92/1000 | Loss: 0.00002094
Iteration 93/1000 | Loss: 0.00002093
Iteration 94/1000 | Loss: 0.00002093
Iteration 95/1000 | Loss: 0.00002093
Iteration 96/1000 | Loss: 0.00002093
Iteration 97/1000 | Loss: 0.00002093
Iteration 98/1000 | Loss: 0.00002093
Iteration 99/1000 | Loss: 0.00002093
Iteration 100/1000 | Loss: 0.00002093
Iteration 101/1000 | Loss: 0.00002092
Iteration 102/1000 | Loss: 0.00002092
Iteration 103/1000 | Loss: 0.00002092
Iteration 104/1000 | Loss: 0.00002092
Iteration 105/1000 | Loss: 0.00002091
Iteration 106/1000 | Loss: 0.00002091
Iteration 107/1000 | Loss: 0.00002091
Iteration 108/1000 | Loss: 0.00002091
Iteration 109/1000 | Loss: 0.00002090
Iteration 110/1000 | Loss: 0.00002090
Iteration 111/1000 | Loss: 0.00002089
Iteration 112/1000 | Loss: 0.00002089
Iteration 113/1000 | Loss: 0.00002089
Iteration 114/1000 | Loss: 0.00002089
Iteration 115/1000 | Loss: 0.00002089
Iteration 116/1000 | Loss: 0.00002089
Iteration 117/1000 | Loss: 0.00002089
Iteration 118/1000 | Loss: 0.00002089
Iteration 119/1000 | Loss: 0.00002089
Iteration 120/1000 | Loss: 0.00002089
Iteration 121/1000 | Loss: 0.00002088
Iteration 122/1000 | Loss: 0.00002088
Iteration 123/1000 | Loss: 0.00002088
Iteration 124/1000 | Loss: 0.00002087
Iteration 125/1000 | Loss: 0.00002087
Iteration 126/1000 | Loss: 0.00002087
Iteration 127/1000 | Loss: 0.00002086
Iteration 128/1000 | Loss: 0.00002086
Iteration 129/1000 | Loss: 0.00002086
Iteration 130/1000 | Loss: 0.00002086
Iteration 131/1000 | Loss: 0.00002086
Iteration 132/1000 | Loss: 0.00002086
Iteration 133/1000 | Loss: 0.00002085
Iteration 134/1000 | Loss: 0.00002085
Iteration 135/1000 | Loss: 0.00002085
Iteration 136/1000 | Loss: 0.00002085
Iteration 137/1000 | Loss: 0.00002085
Iteration 138/1000 | Loss: 0.00002085
Iteration 139/1000 | Loss: 0.00002085
Iteration 140/1000 | Loss: 0.00002085
Iteration 141/1000 | Loss: 0.00002085
Iteration 142/1000 | Loss: 0.00002084
Iteration 143/1000 | Loss: 0.00002084
Iteration 144/1000 | Loss: 0.00002084
Iteration 145/1000 | Loss: 0.00002084
Iteration 146/1000 | Loss: 0.00002084
Iteration 147/1000 | Loss: 0.00002084
Iteration 148/1000 | Loss: 0.00002084
Iteration 149/1000 | Loss: 0.00002084
Iteration 150/1000 | Loss: 0.00002084
Iteration 151/1000 | Loss: 0.00002084
Iteration 152/1000 | Loss: 0.00002083
Iteration 153/1000 | Loss: 0.00002083
Iteration 154/1000 | Loss: 0.00002083
Iteration 155/1000 | Loss: 0.00002083
Iteration 156/1000 | Loss: 0.00002083
Iteration 157/1000 | Loss: 0.00002082
Iteration 158/1000 | Loss: 0.00002082
Iteration 159/1000 | Loss: 0.00002082
Iteration 160/1000 | Loss: 0.00002082
Iteration 161/1000 | Loss: 0.00002082
Iteration 162/1000 | Loss: 0.00002081
Iteration 163/1000 | Loss: 0.00002081
Iteration 164/1000 | Loss: 0.00002081
Iteration 165/1000 | Loss: 0.00002081
Iteration 166/1000 | Loss: 0.00002081
Iteration 167/1000 | Loss: 0.00002081
Iteration 168/1000 | Loss: 0.00002081
Iteration 169/1000 | Loss: 0.00002081
Iteration 170/1000 | Loss: 0.00002081
Iteration 171/1000 | Loss: 0.00002080
Iteration 172/1000 | Loss: 0.00002080
Iteration 173/1000 | Loss: 0.00002080
Iteration 174/1000 | Loss: 0.00002080
Iteration 175/1000 | Loss: 0.00002080
Iteration 176/1000 | Loss: 0.00002080
Iteration 177/1000 | Loss: 0.00002080
Iteration 178/1000 | Loss: 0.00002080
Iteration 179/1000 | Loss: 0.00002080
Iteration 180/1000 | Loss: 0.00002080
Iteration 181/1000 | Loss: 0.00002080
Iteration 182/1000 | Loss: 0.00002079
Iteration 183/1000 | Loss: 0.00002079
Iteration 184/1000 | Loss: 0.00002079
Iteration 185/1000 | Loss: 0.00002079
Iteration 186/1000 | Loss: 0.00002079
Iteration 187/1000 | Loss: 0.00002079
Iteration 188/1000 | Loss: 0.00002079
Iteration 189/1000 | Loss: 0.00002079
Iteration 190/1000 | Loss: 0.00002079
Iteration 191/1000 | Loss: 0.00002079
Iteration 192/1000 | Loss: 0.00002079
Iteration 193/1000 | Loss: 0.00002079
Iteration 194/1000 | Loss: 0.00002078
Iteration 195/1000 | Loss: 0.00002078
Iteration 196/1000 | Loss: 0.00002078
Iteration 197/1000 | Loss: 0.00002078
Iteration 198/1000 | Loss: 0.00002078
Iteration 199/1000 | Loss: 0.00002078
Iteration 200/1000 | Loss: 0.00002078
Iteration 201/1000 | Loss: 0.00002078
Iteration 202/1000 | Loss: 0.00002078
Iteration 203/1000 | Loss: 0.00002078
Iteration 204/1000 | Loss: 0.00002078
Iteration 205/1000 | Loss: 0.00002078
Iteration 206/1000 | Loss: 0.00002077
Iteration 207/1000 | Loss: 0.00002077
Iteration 208/1000 | Loss: 0.00002077
Iteration 209/1000 | Loss: 0.00002077
Iteration 210/1000 | Loss: 0.00002077
Iteration 211/1000 | Loss: 0.00002077
Iteration 212/1000 | Loss: 0.00002077
Iteration 213/1000 | Loss: 0.00002077
Iteration 214/1000 | Loss: 0.00002077
Iteration 215/1000 | Loss: 0.00002077
Iteration 216/1000 | Loss: 0.00002077
Iteration 217/1000 | Loss: 0.00002077
Iteration 218/1000 | Loss: 0.00002076
Iteration 219/1000 | Loss: 0.00002076
Iteration 220/1000 | Loss: 0.00002076
Iteration 221/1000 | Loss: 0.00002076
Iteration 222/1000 | Loss: 0.00002076
Iteration 223/1000 | Loss: 0.00002076
Iteration 224/1000 | Loss: 0.00002076
Iteration 225/1000 | Loss: 0.00002076
Iteration 226/1000 | Loss: 0.00002076
Iteration 227/1000 | Loss: 0.00002076
Iteration 228/1000 | Loss: 0.00002076
Iteration 229/1000 | Loss: 0.00002076
Iteration 230/1000 | Loss: 0.00002076
Iteration 231/1000 | Loss: 0.00002076
Iteration 232/1000 | Loss: 0.00002076
Iteration 233/1000 | Loss: 0.00002076
Iteration 234/1000 | Loss: 0.00002075
Iteration 235/1000 | Loss: 0.00002075
Iteration 236/1000 | Loss: 0.00002075
Iteration 237/1000 | Loss: 0.00002075
Iteration 238/1000 | Loss: 0.00002075
Iteration 239/1000 | Loss: 0.00002075
Iteration 240/1000 | Loss: 0.00002075
Iteration 241/1000 | Loss: 0.00002075
Iteration 242/1000 | Loss: 0.00002075
Iteration 243/1000 | Loss: 0.00002075
Iteration 244/1000 | Loss: 0.00002075
Iteration 245/1000 | Loss: 0.00002075
Iteration 246/1000 | Loss: 0.00002075
Iteration 247/1000 | Loss: 0.00002075
Iteration 248/1000 | Loss: 0.00002075
Iteration 249/1000 | Loss: 0.00002075
Iteration 250/1000 | Loss: 0.00002075
Iteration 251/1000 | Loss: 0.00002075
Iteration 252/1000 | Loss: 0.00002074
Iteration 253/1000 | Loss: 0.00002074
Iteration 254/1000 | Loss: 0.00002074
Iteration 255/1000 | Loss: 0.00002074
Iteration 256/1000 | Loss: 0.00002074
Iteration 257/1000 | Loss: 0.00002074
Iteration 258/1000 | Loss: 0.00002074
Iteration 259/1000 | Loss: 0.00002074
Iteration 260/1000 | Loss: 0.00002074
Iteration 261/1000 | Loss: 0.00002074
Iteration 262/1000 | Loss: 0.00002074
Iteration 263/1000 | Loss: 0.00002073
Iteration 264/1000 | Loss: 0.00002073
Iteration 265/1000 | Loss: 0.00002073
Iteration 266/1000 | Loss: 0.00002073
Iteration 267/1000 | Loss: 0.00002073
Iteration 268/1000 | Loss: 0.00002073
Iteration 269/1000 | Loss: 0.00002073
Iteration 270/1000 | Loss: 0.00002073
Iteration 271/1000 | Loss: 0.00002073
Iteration 272/1000 | Loss: 0.00002073
Iteration 273/1000 | Loss: 0.00002073
Iteration 274/1000 | Loss: 0.00002072
Iteration 275/1000 | Loss: 0.00002072
Iteration 276/1000 | Loss: 0.00002072
Iteration 277/1000 | Loss: 0.00002072
Iteration 278/1000 | Loss: 0.00002072
Iteration 279/1000 | Loss: 0.00002072
Iteration 280/1000 | Loss: 0.00002072
Iteration 281/1000 | Loss: 0.00002072
Iteration 282/1000 | Loss: 0.00002072
Iteration 283/1000 | Loss: 0.00002072
Iteration 284/1000 | Loss: 0.00002072
Iteration 285/1000 | Loss: 0.00002072
Iteration 286/1000 | Loss: 0.00002072
Iteration 287/1000 | Loss: 0.00002072
Iteration 288/1000 | Loss: 0.00002072
Iteration 289/1000 | Loss: 0.00002072
Iteration 290/1000 | Loss: 0.00002072
Iteration 291/1000 | Loss: 0.00002071
Iteration 292/1000 | Loss: 0.00002071
Iteration 293/1000 | Loss: 0.00002071
Iteration 294/1000 | Loss: 0.00002071
Iteration 295/1000 | Loss: 0.00002071
Iteration 296/1000 | Loss: 0.00002071
Iteration 297/1000 | Loss: 0.00002071
Iteration 298/1000 | Loss: 0.00002071
Iteration 299/1000 | Loss: 0.00002071
Iteration 300/1000 | Loss: 0.00002071
Iteration 301/1000 | Loss: 0.00002071
Iteration 302/1000 | Loss: 0.00002071
Iteration 303/1000 | Loss: 0.00002071
Iteration 304/1000 | Loss: 0.00002071
Iteration 305/1000 | Loss: 0.00002071
Iteration 306/1000 | Loss: 0.00002071
Iteration 307/1000 | Loss: 0.00002071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [2.0707726434920914e-05, 2.0707726434920914e-05, 2.0707726434920914e-05, 2.0707726434920914e-05, 2.0707726434920914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0707726434920914e-05

Optimization complete. Final v2v error: 3.788594961166382 mm

Highest mean error: 4.536470413208008 mm for frame 89

Lowest mean error: 2.799959659576416 mm for frame 0

Saving results

Total time: 656.5831007957458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1010
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105013
Iteration 2/25 | Loss: 0.00197992
Iteration 3/25 | Loss: 0.00143935
Iteration 4/25 | Loss: 0.00138877
Iteration 5/25 | Loss: 0.00137783
Iteration 6/25 | Loss: 0.00136953
Iteration 7/25 | Loss: 0.00136837
Iteration 8/25 | Loss: 0.00135842
Iteration 9/25 | Loss: 0.00135747
Iteration 10/25 | Loss: 0.00135574
Iteration 11/25 | Loss: 0.00135371
Iteration 12/25 | Loss: 0.00135058
Iteration 13/25 | Loss: 0.00134437
Iteration 14/25 | Loss: 0.00134152
Iteration 15/25 | Loss: 0.00134364
Iteration 16/25 | Loss: 0.00134569
Iteration 17/25 | Loss: 0.00134987
Iteration 18/25 | Loss: 0.00135385
Iteration 19/25 | Loss: 0.00134860
Iteration 20/25 | Loss: 0.00134911
Iteration 21/25 | Loss: 0.00134735
Iteration 22/25 | Loss: 0.00134656
Iteration 23/25 | Loss: 0.00134245
Iteration 24/25 | Loss: 0.00134231
Iteration 25/25 | Loss: 0.00133999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90157354
Iteration 2/25 | Loss: 0.00238459
Iteration 3/25 | Loss: 0.00238459
Iteration 4/25 | Loss: 0.00238459
Iteration 5/25 | Loss: 0.00238459
Iteration 6/25 | Loss: 0.00238459
Iteration 7/25 | Loss: 0.00238459
Iteration 8/25 | Loss: 0.00238459
Iteration 9/25 | Loss: 0.00238459
Iteration 10/25 | Loss: 0.00238459
Iteration 11/25 | Loss: 0.00238459
Iteration 12/25 | Loss: 0.00238459
Iteration 13/25 | Loss: 0.00238459
Iteration 14/25 | Loss: 0.00238459
Iteration 15/25 | Loss: 0.00238459
Iteration 16/25 | Loss: 0.00238459
Iteration 17/25 | Loss: 0.00238459
Iteration 18/25 | Loss: 0.00238459
Iteration 19/25 | Loss: 0.00238459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023845864925533533, 0.0023845864925533533, 0.0023845864925533533, 0.0023845864925533533, 0.0023845864925533533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023845864925533533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238459
Iteration 2/1000 | Loss: 0.00060397
Iteration 3/1000 | Loss: 0.00063150
Iteration 4/1000 | Loss: 0.00062912
Iteration 5/1000 | Loss: 0.00058933
Iteration 6/1000 | Loss: 0.00048281
Iteration 7/1000 | Loss: 0.00062996
Iteration 8/1000 | Loss: 0.00066245
Iteration 9/1000 | Loss: 0.00063040
Iteration 10/1000 | Loss: 0.00064387
Iteration 11/1000 | Loss: 0.00052223
Iteration 12/1000 | Loss: 0.00065442
Iteration 13/1000 | Loss: 0.00064507
Iteration 14/1000 | Loss: 0.00064705
Iteration 15/1000 | Loss: 0.00071487
Iteration 16/1000 | Loss: 0.00061401
Iteration 17/1000 | Loss: 0.00066173
Iteration 18/1000 | Loss: 0.00059267
Iteration 19/1000 | Loss: 0.00045739
Iteration 20/1000 | Loss: 0.00050364
Iteration 21/1000 | Loss: 0.00051963
Iteration 22/1000 | Loss: 0.00053437
Iteration 23/1000 | Loss: 0.00041614
Iteration 24/1000 | Loss: 0.00057377
Iteration 25/1000 | Loss: 0.00057431
Iteration 26/1000 | Loss: 0.00064913
Iteration 27/1000 | Loss: 0.00073471
Iteration 28/1000 | Loss: 0.00049172
Iteration 29/1000 | Loss: 0.00066633
Iteration 30/1000 | Loss: 0.00054777
Iteration 31/1000 | Loss: 0.00069254
Iteration 32/1000 | Loss: 0.00057141
Iteration 33/1000 | Loss: 0.00059711
Iteration 34/1000 | Loss: 0.00061419
Iteration 35/1000 | Loss: 0.00062093
Iteration 36/1000 | Loss: 0.00065310
Iteration 37/1000 | Loss: 0.00069971
Iteration 38/1000 | Loss: 0.00064900
Iteration 39/1000 | Loss: 0.00048851
Iteration 40/1000 | Loss: 0.00047305
Iteration 41/1000 | Loss: 0.00058565
Iteration 42/1000 | Loss: 0.00067940
Iteration 43/1000 | Loss: 0.00072283
Iteration 44/1000 | Loss: 0.00063325
Iteration 45/1000 | Loss: 0.00066713
Iteration 46/1000 | Loss: 0.00061504
Iteration 47/1000 | Loss: 0.00062606
Iteration 48/1000 | Loss: 0.00059542
Iteration 49/1000 | Loss: 0.00065834
Iteration 50/1000 | Loss: 0.00063741
Iteration 51/1000 | Loss: 0.00055334
Iteration 52/1000 | Loss: 0.00065238
Iteration 53/1000 | Loss: 0.00056226
Iteration 54/1000 | Loss: 0.00062391
Iteration 55/1000 | Loss: 0.00055795
Iteration 56/1000 | Loss: 0.00064124
Iteration 57/1000 | Loss: 0.00057774
Iteration 58/1000 | Loss: 0.00060700
Iteration 59/1000 | Loss: 0.00059346
Iteration 60/1000 | Loss: 0.00070498
Iteration 61/1000 | Loss: 0.00052721
Iteration 62/1000 | Loss: 0.00045065
Iteration 63/1000 | Loss: 0.00069922
Iteration 64/1000 | Loss: 0.00062646
Iteration 65/1000 | Loss: 0.00075206
Iteration 66/1000 | Loss: 0.00077646
Iteration 67/1000 | Loss: 0.00072812
Iteration 68/1000 | Loss: 0.00066402
Iteration 69/1000 | Loss: 0.00064632
Iteration 70/1000 | Loss: 0.00051174
Iteration 71/1000 | Loss: 0.00056032
Iteration 72/1000 | Loss: 0.00074060
Iteration 73/1000 | Loss: 0.00074513
Iteration 74/1000 | Loss: 0.00078269
Iteration 75/1000 | Loss: 0.00087001
Iteration 76/1000 | Loss: 0.00063682
Iteration 77/1000 | Loss: 0.00066782
Iteration 78/1000 | Loss: 0.00061984
Iteration 79/1000 | Loss: 0.00059685
Iteration 80/1000 | Loss: 0.00049513
Iteration 81/1000 | Loss: 0.00079787
Iteration 82/1000 | Loss: 0.00058108
Iteration 83/1000 | Loss: 0.00068398
Iteration 84/1000 | Loss: 0.00066615
Iteration 85/1000 | Loss: 0.00071355
Iteration 86/1000 | Loss: 0.00080295
Iteration 87/1000 | Loss: 0.00043174
Iteration 88/1000 | Loss: 0.00032168
Iteration 89/1000 | Loss: 0.00040395
Iteration 90/1000 | Loss: 0.00030721
Iteration 91/1000 | Loss: 0.00020510
Iteration 92/1000 | Loss: 0.00037518
Iteration 93/1000 | Loss: 0.00024761
Iteration 94/1000 | Loss: 0.00029639
Iteration 95/1000 | Loss: 0.00027276
Iteration 96/1000 | Loss: 0.00027523
Iteration 97/1000 | Loss: 0.00028089
Iteration 98/1000 | Loss: 0.00025758
Iteration 99/1000 | Loss: 0.00033107
Iteration 100/1000 | Loss: 0.00032298
Iteration 101/1000 | Loss: 0.00027439
Iteration 102/1000 | Loss: 0.00028040
Iteration 103/1000 | Loss: 0.00035388
Iteration 104/1000 | Loss: 0.00026839
Iteration 105/1000 | Loss: 0.00019264
Iteration 106/1000 | Loss: 0.00024233
Iteration 107/1000 | Loss: 0.00017768
Iteration 108/1000 | Loss: 0.00024709
Iteration 109/1000 | Loss: 0.00028182
Iteration 110/1000 | Loss: 0.00018178
Iteration 111/1000 | Loss: 0.00025315
Iteration 112/1000 | Loss: 0.00030837
Iteration 113/1000 | Loss: 0.00041033
Iteration 114/1000 | Loss: 0.00027547
Iteration 115/1000 | Loss: 0.00031143
Iteration 116/1000 | Loss: 0.00024202
Iteration 117/1000 | Loss: 0.00028807
Iteration 118/1000 | Loss: 0.00031667
Iteration 119/1000 | Loss: 0.00033457
Iteration 120/1000 | Loss: 0.00026503
Iteration 121/1000 | Loss: 0.00035921
Iteration 122/1000 | Loss: 0.00030544
Iteration 123/1000 | Loss: 0.00030841
Iteration 124/1000 | Loss: 0.00022191
Iteration 125/1000 | Loss: 0.00019217
Iteration 126/1000 | Loss: 0.00014165
Iteration 127/1000 | Loss: 0.00022026
Iteration 128/1000 | Loss: 0.00024008
Iteration 129/1000 | Loss: 0.00031710
Iteration 130/1000 | Loss: 0.00030789
Iteration 131/1000 | Loss: 0.00029010
Iteration 132/1000 | Loss: 0.00028699
Iteration 133/1000 | Loss: 0.00034790
Iteration 134/1000 | Loss: 0.00030427
Iteration 135/1000 | Loss: 0.00036554
Iteration 136/1000 | Loss: 0.00020919
Iteration 137/1000 | Loss: 0.00033963
Iteration 138/1000 | Loss: 0.00031063
Iteration 139/1000 | Loss: 0.00032463
Iteration 140/1000 | Loss: 0.00041219
Iteration 141/1000 | Loss: 0.00036269
Iteration 142/1000 | Loss: 0.00038097
Iteration 143/1000 | Loss: 0.00028381
Iteration 144/1000 | Loss: 0.00013244
Iteration 145/1000 | Loss: 0.00031049
Iteration 146/1000 | Loss: 0.00034754
Iteration 147/1000 | Loss: 0.00033190
Iteration 148/1000 | Loss: 0.00039037
Iteration 149/1000 | Loss: 0.00033600
Iteration 150/1000 | Loss: 0.00037420
Iteration 151/1000 | Loss: 0.00038428
Iteration 152/1000 | Loss: 0.00043368
Iteration 153/1000 | Loss: 0.00040061
Iteration 154/1000 | Loss: 0.00027235
Iteration 155/1000 | Loss: 0.00035175
Iteration 156/1000 | Loss: 0.00024403
Iteration 157/1000 | Loss: 0.00011890
Iteration 158/1000 | Loss: 0.00010543
Iteration 159/1000 | Loss: 0.00009035
Iteration 160/1000 | Loss: 0.00005859
Iteration 161/1000 | Loss: 0.00012168
Iteration 162/1000 | Loss: 0.00007958
Iteration 163/1000 | Loss: 0.00010739
Iteration 164/1000 | Loss: 0.00004754
Iteration 165/1000 | Loss: 0.00019370
Iteration 166/1000 | Loss: 0.00021341
Iteration 167/1000 | Loss: 0.00011348
Iteration 168/1000 | Loss: 0.00010301
Iteration 169/1000 | Loss: 0.00010543
Iteration 170/1000 | Loss: 0.00009514
Iteration 171/1000 | Loss: 0.00016973
Iteration 172/1000 | Loss: 0.00012086
Iteration 173/1000 | Loss: 0.00008175
Iteration 174/1000 | Loss: 0.00016196
Iteration 175/1000 | Loss: 0.00015706
Iteration 176/1000 | Loss: 0.00015068
Iteration 177/1000 | Loss: 0.00014321
Iteration 178/1000 | Loss: 0.00014704
Iteration 179/1000 | Loss: 0.00015488
Iteration 180/1000 | Loss: 0.00026831
Iteration 181/1000 | Loss: 0.00024470
Iteration 182/1000 | Loss: 0.00030857
Iteration 183/1000 | Loss: 0.00017827
Iteration 184/1000 | Loss: 0.00023880
Iteration 185/1000 | Loss: 0.00023497
Iteration 186/1000 | Loss: 0.00018643
Iteration 187/1000 | Loss: 0.00016108
Iteration 188/1000 | Loss: 0.00020213
Iteration 189/1000 | Loss: 0.00012171
Iteration 190/1000 | Loss: 0.00009889
Iteration 191/1000 | Loss: 0.00016025
Iteration 192/1000 | Loss: 0.00014853
Iteration 193/1000 | Loss: 0.00005766
Iteration 194/1000 | Loss: 0.00018551
Iteration 195/1000 | Loss: 0.00019097
Iteration 196/1000 | Loss: 0.00007707
Iteration 197/1000 | Loss: 0.00010006
Iteration 198/1000 | Loss: 0.00009514
Iteration 199/1000 | Loss: 0.00003266
Iteration 200/1000 | Loss: 0.00003088
Iteration 201/1000 | Loss: 0.00003000
Iteration 202/1000 | Loss: 0.00005525
Iteration 203/1000 | Loss: 0.00008730
Iteration 204/1000 | Loss: 0.00008435
Iteration 205/1000 | Loss: 0.00013114
Iteration 206/1000 | Loss: 0.00007488
Iteration 207/1000 | Loss: 0.00003587
Iteration 208/1000 | Loss: 0.00003310
Iteration 209/1000 | Loss: 0.00003212
Iteration 210/1000 | Loss: 0.00052463
Iteration 211/1000 | Loss: 0.00005459
Iteration 212/1000 | Loss: 0.00009804
Iteration 213/1000 | Loss: 0.00008360
Iteration 214/1000 | Loss: 0.00003071
Iteration 215/1000 | Loss: 0.00002894
Iteration 216/1000 | Loss: 0.00011445
Iteration 217/1000 | Loss: 0.00003092
Iteration 218/1000 | Loss: 0.00010272
Iteration 219/1000 | Loss: 0.00013927
Iteration 220/1000 | Loss: 0.00003203
Iteration 221/1000 | Loss: 0.00003016
Iteration 222/1000 | Loss: 0.00011449
Iteration 223/1000 | Loss: 0.00009140
Iteration 224/1000 | Loss: 0.00027876
Iteration 225/1000 | Loss: 0.00008255
Iteration 226/1000 | Loss: 0.00029400
Iteration 227/1000 | Loss: 0.00013411
Iteration 228/1000 | Loss: 0.00004508
Iteration 229/1000 | Loss: 0.00011160
Iteration 230/1000 | Loss: 0.00015223
Iteration 231/1000 | Loss: 0.00009340
Iteration 232/1000 | Loss: 0.00014618
Iteration 233/1000 | Loss: 0.00012790
Iteration 234/1000 | Loss: 0.00014917
Iteration 235/1000 | Loss: 0.00004694
Iteration 236/1000 | Loss: 0.00011566
Iteration 237/1000 | Loss: 0.00005656
Iteration 238/1000 | Loss: 0.00009067
Iteration 239/1000 | Loss: 0.00006934
Iteration 240/1000 | Loss: 0.00007754
Iteration 241/1000 | Loss: 0.00009361
Iteration 242/1000 | Loss: 0.00010995
Iteration 243/1000 | Loss: 0.00009524
Iteration 244/1000 | Loss: 0.00007842
Iteration 245/1000 | Loss: 0.00002874
Iteration 246/1000 | Loss: 0.00007707
Iteration 247/1000 | Loss: 0.00012536
Iteration 248/1000 | Loss: 0.00014951
Iteration 249/1000 | Loss: 0.00017797
Iteration 250/1000 | Loss: 0.00015869
Iteration 251/1000 | Loss: 0.00006979
Iteration 252/1000 | Loss: 0.00008997
Iteration 253/1000 | Loss: 0.00014588
Iteration 254/1000 | Loss: 0.00011615
Iteration 255/1000 | Loss: 0.00010968
Iteration 256/1000 | Loss: 0.00015698
Iteration 257/1000 | Loss: 0.00003000
Iteration 258/1000 | Loss: 0.00002862
Iteration 259/1000 | Loss: 0.00004041
Iteration 260/1000 | Loss: 0.00003802
Iteration 261/1000 | Loss: 0.00003798
Iteration 262/1000 | Loss: 0.00003744
Iteration 263/1000 | Loss: 0.00003839
Iteration 264/1000 | Loss: 0.00002805
Iteration 265/1000 | Loss: 0.00002698
Iteration 266/1000 | Loss: 0.00002650
Iteration 267/1000 | Loss: 0.00002596
Iteration 268/1000 | Loss: 0.00002547
Iteration 269/1000 | Loss: 0.00002527
Iteration 270/1000 | Loss: 0.00014348
Iteration 271/1000 | Loss: 0.00014348
Iteration 272/1000 | Loss: 0.00009219
Iteration 273/1000 | Loss: 0.00002528
Iteration 274/1000 | Loss: 0.00014460
Iteration 275/1000 | Loss: 0.00002924
Iteration 276/1000 | Loss: 0.00005981
Iteration 277/1000 | Loss: 0.00006664
Iteration 278/1000 | Loss: 0.00007265
Iteration 279/1000 | Loss: 0.00006918
Iteration 280/1000 | Loss: 0.00008015
Iteration 281/1000 | Loss: 0.00009703
Iteration 282/1000 | Loss: 0.00012357
Iteration 283/1000 | Loss: 0.00003407
Iteration 284/1000 | Loss: 0.00002627
Iteration 285/1000 | Loss: 0.00003920
Iteration 286/1000 | Loss: 0.00011265
Iteration 287/1000 | Loss: 0.00003262
Iteration 288/1000 | Loss: 0.00003937
Iteration 289/1000 | Loss: 0.00015737
Iteration 290/1000 | Loss: 0.00003024
Iteration 291/1000 | Loss: 0.00004677
Iteration 292/1000 | Loss: 0.00004041
Iteration 293/1000 | Loss: 0.00004165
Iteration 294/1000 | Loss: 0.00007892
Iteration 295/1000 | Loss: 0.00002834
Iteration 296/1000 | Loss: 0.00003818
Iteration 297/1000 | Loss: 0.00016137
Iteration 298/1000 | Loss: 0.00014043
Iteration 299/1000 | Loss: 0.00003597
Iteration 300/1000 | Loss: 0.00007087
Iteration 301/1000 | Loss: 0.00003185
Iteration 302/1000 | Loss: 0.00002891
Iteration 303/1000 | Loss: 0.00002779
Iteration 304/1000 | Loss: 0.00002821
Iteration 305/1000 | Loss: 0.00002743
Iteration 306/1000 | Loss: 0.00002559
Iteration 307/1000 | Loss: 0.00002554
Iteration 308/1000 | Loss: 0.00002515
Iteration 309/1000 | Loss: 0.00002487
Iteration 310/1000 | Loss: 0.00002481
Iteration 311/1000 | Loss: 0.00002477
Iteration 312/1000 | Loss: 0.00002468
Iteration 313/1000 | Loss: 0.00002461
Iteration 314/1000 | Loss: 0.00002459
Iteration 315/1000 | Loss: 0.00002458
Iteration 316/1000 | Loss: 0.00002457
Iteration 317/1000 | Loss: 0.00002457
Iteration 318/1000 | Loss: 0.00002454
Iteration 319/1000 | Loss: 0.00002454
Iteration 320/1000 | Loss: 0.00002448
Iteration 321/1000 | Loss: 0.00002448
Iteration 322/1000 | Loss: 0.00002448
Iteration 323/1000 | Loss: 0.00002448
Iteration 324/1000 | Loss: 0.00002448
Iteration 325/1000 | Loss: 0.00002448
Iteration 326/1000 | Loss: 0.00002448
Iteration 327/1000 | Loss: 0.00002448
Iteration 328/1000 | Loss: 0.00002447
Iteration 329/1000 | Loss: 0.00002447
Iteration 330/1000 | Loss: 0.00002447
Iteration 331/1000 | Loss: 0.00002447
Iteration 332/1000 | Loss: 0.00002447
Iteration 333/1000 | Loss: 0.00002447
Iteration 334/1000 | Loss: 0.00002446
Iteration 335/1000 | Loss: 0.00002446
Iteration 336/1000 | Loss: 0.00002445
Iteration 337/1000 | Loss: 0.00002444
Iteration 338/1000 | Loss: 0.00002443
Iteration 339/1000 | Loss: 0.00002442
Iteration 340/1000 | Loss: 0.00002441
Iteration 341/1000 | Loss: 0.00002441
Iteration 342/1000 | Loss: 0.00002440
Iteration 343/1000 | Loss: 0.00002440
Iteration 344/1000 | Loss: 0.00002440
Iteration 345/1000 | Loss: 0.00002440
Iteration 346/1000 | Loss: 0.00002440
Iteration 347/1000 | Loss: 0.00002440
Iteration 348/1000 | Loss: 0.00002439
Iteration 349/1000 | Loss: 0.00002439
Iteration 350/1000 | Loss: 0.00002439
Iteration 351/1000 | Loss: 0.00002439
Iteration 352/1000 | Loss: 0.00002438
Iteration 353/1000 | Loss: 0.00002437
Iteration 354/1000 | Loss: 0.00002437
Iteration 355/1000 | Loss: 0.00002437
Iteration 356/1000 | Loss: 0.00002436
Iteration 357/1000 | Loss: 0.00002436
Iteration 358/1000 | Loss: 0.00002436
Iteration 359/1000 | Loss: 0.00002436
Iteration 360/1000 | Loss: 0.00002436
Iteration 361/1000 | Loss: 0.00002436
Iteration 362/1000 | Loss: 0.00002436
Iteration 363/1000 | Loss: 0.00002436
Iteration 364/1000 | Loss: 0.00002436
Iteration 365/1000 | Loss: 0.00002436
Iteration 366/1000 | Loss: 0.00002436
Iteration 367/1000 | Loss: 0.00002436
Iteration 368/1000 | Loss: 0.00002436
Iteration 369/1000 | Loss: 0.00002436
Iteration 370/1000 | Loss: 0.00002436
Iteration 371/1000 | Loss: 0.00002436
Iteration 372/1000 | Loss: 0.00002436
Iteration 373/1000 | Loss: 0.00002436
Iteration 374/1000 | Loss: 0.00002436
Iteration 375/1000 | Loss: 0.00002436
Iteration 376/1000 | Loss: 0.00002436
Iteration 377/1000 | Loss: 0.00002436
Iteration 378/1000 | Loss: 0.00002436
Iteration 379/1000 | Loss: 0.00002436
Iteration 380/1000 | Loss: 0.00002436
Iteration 381/1000 | Loss: 0.00002436
Iteration 382/1000 | Loss: 0.00002436
Iteration 383/1000 | Loss: 0.00002436
Iteration 384/1000 | Loss: 0.00002436
Iteration 385/1000 | Loss: 0.00002436
Iteration 386/1000 | Loss: 0.00002436
Iteration 387/1000 | Loss: 0.00002436
Iteration 388/1000 | Loss: 0.00002436
Iteration 389/1000 | Loss: 0.00002436
Iteration 390/1000 | Loss: 0.00002436
Iteration 391/1000 | Loss: 0.00002436
Iteration 392/1000 | Loss: 0.00002436
Iteration 393/1000 | Loss: 0.00002436
Iteration 394/1000 | Loss: 0.00002436
Iteration 395/1000 | Loss: 0.00002436
Iteration 396/1000 | Loss: 0.00002436
Iteration 397/1000 | Loss: 0.00002436
Iteration 398/1000 | Loss: 0.00002436
Iteration 399/1000 | Loss: 0.00002436
Iteration 400/1000 | Loss: 0.00002436
Iteration 401/1000 | Loss: 0.00002436
Iteration 402/1000 | Loss: 0.00002436
Iteration 403/1000 | Loss: 0.00002436
Iteration 404/1000 | Loss: 0.00002436
Iteration 405/1000 | Loss: 0.00002436
Iteration 406/1000 | Loss: 0.00002436
Iteration 407/1000 | Loss: 0.00002436
Iteration 408/1000 | Loss: 0.00002436
Iteration 409/1000 | Loss: 0.00002436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 409. Stopping optimization.
Last 5 losses: [2.4356675567105412e-05, 2.4356675567105412e-05, 2.4356675567105412e-05, 2.4356675567105412e-05, 2.4356675567105412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4356675567105412e-05

Optimization complete. Final v2v error: 4.069063663482666 mm

Highest mean error: 6.162736892700195 mm for frame 97

Lowest mean error: 3.1618740558624268 mm for frame 32

Saving results

Total time: 4133.819377660751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1097
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848148
Iteration 2/25 | Loss: 0.00131819
Iteration 3/25 | Loss: 0.00122768
Iteration 4/25 | Loss: 0.00121366
Iteration 5/25 | Loss: 0.00120875
Iteration 6/25 | Loss: 0.00120875
Iteration 7/25 | Loss: 0.00120875
Iteration 8/25 | Loss: 0.00120875
Iteration 9/25 | Loss: 0.00120875
Iteration 10/25 | Loss: 0.00120875
Iteration 11/25 | Loss: 0.00120875
Iteration 12/25 | Loss: 0.00120875
Iteration 13/25 | Loss: 0.00120875
Iteration 14/25 | Loss: 0.00120875
Iteration 15/25 | Loss: 0.00120875
Iteration 16/25 | Loss: 0.00120875
Iteration 17/25 | Loss: 0.00120875
Iteration 18/25 | Loss: 0.00120875
Iteration 19/25 | Loss: 0.00120875
Iteration 20/25 | Loss: 0.00120875
Iteration 21/25 | Loss: 0.00120875
Iteration 22/25 | Loss: 0.00120875
Iteration 23/25 | Loss: 0.00120875
Iteration 24/25 | Loss: 0.00120875
Iteration 25/25 | Loss: 0.00120875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23492551
Iteration 2/25 | Loss: 0.00217880
Iteration 3/25 | Loss: 0.00217880
Iteration 4/25 | Loss: 0.00217880
Iteration 5/25 | Loss: 0.00217880
Iteration 6/25 | Loss: 0.00217880
Iteration 7/25 | Loss: 0.00217880
Iteration 8/25 | Loss: 0.00217880
Iteration 9/25 | Loss: 0.00217880
Iteration 10/25 | Loss: 0.00217880
Iteration 11/25 | Loss: 0.00217880
Iteration 12/25 | Loss: 0.00217880
Iteration 13/25 | Loss: 0.00217880
Iteration 14/25 | Loss: 0.00217880
Iteration 15/25 | Loss: 0.00217880
Iteration 16/25 | Loss: 0.00217880
Iteration 17/25 | Loss: 0.00217880
Iteration 18/25 | Loss: 0.00217880
Iteration 19/25 | Loss: 0.00217880
Iteration 20/25 | Loss: 0.00217880
Iteration 21/25 | Loss: 0.00217880
Iteration 22/25 | Loss: 0.00217880
Iteration 23/25 | Loss: 0.00217880
Iteration 24/25 | Loss: 0.00217880
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0021787986624985933, 0.0021787986624985933, 0.0021787986624985933, 0.0021787986624985933, 0.0021787986624985933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021787986624985933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217880
Iteration 2/1000 | Loss: 0.00002614
Iteration 3/1000 | Loss: 0.00002019
Iteration 4/1000 | Loss: 0.00001914
Iteration 5/1000 | Loss: 0.00001860
Iteration 6/1000 | Loss: 0.00001855
Iteration 7/1000 | Loss: 0.00001804
Iteration 8/1000 | Loss: 0.00001801
Iteration 9/1000 | Loss: 0.00001766
Iteration 10/1000 | Loss: 0.00001734
Iteration 11/1000 | Loss: 0.00001729
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001718
Iteration 14/1000 | Loss: 0.00001716
Iteration 15/1000 | Loss: 0.00001703
Iteration 16/1000 | Loss: 0.00001702
Iteration 17/1000 | Loss: 0.00001696
Iteration 18/1000 | Loss: 0.00001695
Iteration 19/1000 | Loss: 0.00001695
Iteration 20/1000 | Loss: 0.00001694
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00001692
Iteration 23/1000 | Loss: 0.00001692
Iteration 24/1000 | Loss: 0.00001691
Iteration 25/1000 | Loss: 0.00001691
Iteration 26/1000 | Loss: 0.00001690
Iteration 27/1000 | Loss: 0.00001690
Iteration 28/1000 | Loss: 0.00001682
Iteration 29/1000 | Loss: 0.00001679
Iteration 30/1000 | Loss: 0.00001679
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00001678
Iteration 33/1000 | Loss: 0.00001677
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001675
Iteration 39/1000 | Loss: 0.00001675
Iteration 40/1000 | Loss: 0.00001674
Iteration 41/1000 | Loss: 0.00001674
Iteration 42/1000 | Loss: 0.00001674
Iteration 43/1000 | Loss: 0.00001674
Iteration 44/1000 | Loss: 0.00001673
Iteration 45/1000 | Loss: 0.00001673
Iteration 46/1000 | Loss: 0.00001673
Iteration 47/1000 | Loss: 0.00001671
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001670
Iteration 54/1000 | Loss: 0.00001670
Iteration 55/1000 | Loss: 0.00001669
Iteration 56/1000 | Loss: 0.00001669
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001669
Iteration 60/1000 | Loss: 0.00001669
Iteration 61/1000 | Loss: 0.00001669
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001669
Iteration 64/1000 | Loss: 0.00001669
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001668
Iteration 68/1000 | Loss: 0.00001668
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001667
Iteration 71/1000 | Loss: 0.00001667
Iteration 72/1000 | Loss: 0.00001667
Iteration 73/1000 | Loss: 0.00001667
Iteration 74/1000 | Loss: 0.00001667
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001666
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00001666
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001664
Iteration 82/1000 | Loss: 0.00001664
Iteration 83/1000 | Loss: 0.00001664
Iteration 84/1000 | Loss: 0.00001664
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001664
Iteration 87/1000 | Loss: 0.00001664
Iteration 88/1000 | Loss: 0.00001664
Iteration 89/1000 | Loss: 0.00001664
Iteration 90/1000 | Loss: 0.00001663
Iteration 91/1000 | Loss: 0.00001663
Iteration 92/1000 | Loss: 0.00001663
Iteration 93/1000 | Loss: 0.00001663
Iteration 94/1000 | Loss: 0.00001663
Iteration 95/1000 | Loss: 0.00001663
Iteration 96/1000 | Loss: 0.00001662
Iteration 97/1000 | Loss: 0.00001662
Iteration 98/1000 | Loss: 0.00001662
Iteration 99/1000 | Loss: 0.00001662
Iteration 100/1000 | Loss: 0.00001662
Iteration 101/1000 | Loss: 0.00001662
Iteration 102/1000 | Loss: 0.00001661
Iteration 103/1000 | Loss: 0.00001661
Iteration 104/1000 | Loss: 0.00001661
Iteration 105/1000 | Loss: 0.00001661
Iteration 106/1000 | Loss: 0.00001661
Iteration 107/1000 | Loss: 0.00001661
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001660
Iteration 110/1000 | Loss: 0.00001660
Iteration 111/1000 | Loss: 0.00001660
Iteration 112/1000 | Loss: 0.00001660
Iteration 113/1000 | Loss: 0.00001660
Iteration 114/1000 | Loss: 0.00001660
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001660
Iteration 121/1000 | Loss: 0.00001660
Iteration 122/1000 | Loss: 0.00001659
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001659
Iteration 125/1000 | Loss: 0.00001659
Iteration 126/1000 | Loss: 0.00001659
Iteration 127/1000 | Loss: 0.00001659
Iteration 128/1000 | Loss: 0.00001659
Iteration 129/1000 | Loss: 0.00001659
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00001658
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Iteration 138/1000 | Loss: 0.00001657
Iteration 139/1000 | Loss: 0.00001657
Iteration 140/1000 | Loss: 0.00001657
Iteration 141/1000 | Loss: 0.00001657
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001656
Iteration 144/1000 | Loss: 0.00001656
Iteration 145/1000 | Loss: 0.00001656
Iteration 146/1000 | Loss: 0.00001656
Iteration 147/1000 | Loss: 0.00001656
Iteration 148/1000 | Loss: 0.00001656
Iteration 149/1000 | Loss: 0.00001656
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001656
Iteration 152/1000 | Loss: 0.00001656
Iteration 153/1000 | Loss: 0.00001656
Iteration 154/1000 | Loss: 0.00001656
Iteration 155/1000 | Loss: 0.00001656
Iteration 156/1000 | Loss: 0.00001656
Iteration 157/1000 | Loss: 0.00001656
Iteration 158/1000 | Loss: 0.00001655
Iteration 159/1000 | Loss: 0.00001655
Iteration 160/1000 | Loss: 0.00001655
Iteration 161/1000 | Loss: 0.00001655
Iteration 162/1000 | Loss: 0.00001655
Iteration 163/1000 | Loss: 0.00001655
Iteration 164/1000 | Loss: 0.00001655
Iteration 165/1000 | Loss: 0.00001655
Iteration 166/1000 | Loss: 0.00001655
Iteration 167/1000 | Loss: 0.00001655
Iteration 168/1000 | Loss: 0.00001655
Iteration 169/1000 | Loss: 0.00001655
Iteration 170/1000 | Loss: 0.00001655
Iteration 171/1000 | Loss: 0.00001655
Iteration 172/1000 | Loss: 0.00001655
Iteration 173/1000 | Loss: 0.00001655
Iteration 174/1000 | Loss: 0.00001655
Iteration 175/1000 | Loss: 0.00001655
Iteration 176/1000 | Loss: 0.00001655
Iteration 177/1000 | Loss: 0.00001655
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001655
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001655
Iteration 182/1000 | Loss: 0.00001655
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.654515835980419e-05, 1.654515835980419e-05, 1.654515835980419e-05, 1.654515835980419e-05, 1.654515835980419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.654515835980419e-05

Optimization complete. Final v2v error: 3.4302239418029785 mm

Highest mean error: 3.495738983154297 mm for frame 197

Lowest mean error: 3.3816304206848145 mm for frame 218

Saving results

Total time: 377.75710010528564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1049
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00625496
Iteration 2/25 | Loss: 0.00124006
Iteration 3/25 | Loss: 0.00115398
Iteration 4/25 | Loss: 0.00114194
Iteration 5/25 | Loss: 0.00113836
Iteration 6/25 | Loss: 0.00113716
Iteration 7/25 | Loss: 0.00113716
Iteration 8/25 | Loss: 0.00113716
Iteration 9/25 | Loss: 0.00113716
Iteration 10/25 | Loss: 0.00113716
Iteration 11/25 | Loss: 0.00113716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011371601140126586, 0.0011371601140126586, 0.0011371601140126586, 0.0011371601140126586, 0.0011371601140126586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011371601140126586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28517413
Iteration 2/25 | Loss: 0.00180384
Iteration 3/25 | Loss: 0.00180384
Iteration 4/25 | Loss: 0.00180384
Iteration 5/25 | Loss: 0.00180384
Iteration 6/25 | Loss: 0.00180384
Iteration 7/25 | Loss: 0.00180384
Iteration 8/25 | Loss: 0.00180384
Iteration 9/25 | Loss: 0.00180384
Iteration 10/25 | Loss: 0.00180384
Iteration 11/25 | Loss: 0.00180384
Iteration 12/25 | Loss: 0.00180384
Iteration 13/25 | Loss: 0.00180384
Iteration 14/25 | Loss: 0.00180384
Iteration 15/25 | Loss: 0.00180384
Iteration 16/25 | Loss: 0.00180384
Iteration 17/25 | Loss: 0.00180384
Iteration 18/25 | Loss: 0.00180384
Iteration 19/25 | Loss: 0.00180384
Iteration 20/25 | Loss: 0.00180384
Iteration 21/25 | Loss: 0.00180384
Iteration 22/25 | Loss: 0.00180384
Iteration 23/25 | Loss: 0.00180384
Iteration 24/25 | Loss: 0.00180384
Iteration 25/25 | Loss: 0.00180384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180384
Iteration 2/1000 | Loss: 0.00001768
Iteration 3/1000 | Loss: 0.00001239
Iteration 4/1000 | Loss: 0.00001086
Iteration 5/1000 | Loss: 0.00001015
Iteration 6/1000 | Loss: 0.00000973
Iteration 7/1000 | Loss: 0.00000925
Iteration 8/1000 | Loss: 0.00000908
Iteration 9/1000 | Loss: 0.00000893
Iteration 10/1000 | Loss: 0.00000876
Iteration 11/1000 | Loss: 0.00000855
Iteration 12/1000 | Loss: 0.00000854
Iteration 13/1000 | Loss: 0.00000850
Iteration 14/1000 | Loss: 0.00000845
Iteration 15/1000 | Loss: 0.00000837
Iteration 16/1000 | Loss: 0.00000835
Iteration 17/1000 | Loss: 0.00000828
Iteration 18/1000 | Loss: 0.00000828
Iteration 19/1000 | Loss: 0.00000827
Iteration 20/1000 | Loss: 0.00000827
Iteration 21/1000 | Loss: 0.00000827
Iteration 22/1000 | Loss: 0.00000826
Iteration 23/1000 | Loss: 0.00000825
Iteration 24/1000 | Loss: 0.00000825
Iteration 25/1000 | Loss: 0.00000821
Iteration 26/1000 | Loss: 0.00000819
Iteration 27/1000 | Loss: 0.00000819
Iteration 28/1000 | Loss: 0.00000819
Iteration 29/1000 | Loss: 0.00000819
Iteration 30/1000 | Loss: 0.00000819
Iteration 31/1000 | Loss: 0.00000810
Iteration 32/1000 | Loss: 0.00000802
Iteration 33/1000 | Loss: 0.00000802
Iteration 34/1000 | Loss: 0.00000802
Iteration 35/1000 | Loss: 0.00000802
Iteration 36/1000 | Loss: 0.00000802
Iteration 37/1000 | Loss: 0.00000802
Iteration 38/1000 | Loss: 0.00000802
Iteration 39/1000 | Loss: 0.00000801
Iteration 40/1000 | Loss: 0.00000800
Iteration 41/1000 | Loss: 0.00000800
Iteration 42/1000 | Loss: 0.00000799
Iteration 43/1000 | Loss: 0.00000799
Iteration 44/1000 | Loss: 0.00000797
Iteration 45/1000 | Loss: 0.00000796
Iteration 46/1000 | Loss: 0.00000795
Iteration 47/1000 | Loss: 0.00000795
Iteration 48/1000 | Loss: 0.00000795
Iteration 49/1000 | Loss: 0.00000795
Iteration 50/1000 | Loss: 0.00000795
Iteration 51/1000 | Loss: 0.00000794
Iteration 52/1000 | Loss: 0.00000794
Iteration 53/1000 | Loss: 0.00000794
Iteration 54/1000 | Loss: 0.00000794
Iteration 55/1000 | Loss: 0.00000793
Iteration 56/1000 | Loss: 0.00000793
Iteration 57/1000 | Loss: 0.00000793
Iteration 58/1000 | Loss: 0.00000793
Iteration 59/1000 | Loss: 0.00000793
Iteration 60/1000 | Loss: 0.00000792
Iteration 61/1000 | Loss: 0.00000792
Iteration 62/1000 | Loss: 0.00000792
Iteration 63/1000 | Loss: 0.00000792
Iteration 64/1000 | Loss: 0.00000792
Iteration 65/1000 | Loss: 0.00000791
Iteration 66/1000 | Loss: 0.00000791
Iteration 67/1000 | Loss: 0.00000791
Iteration 68/1000 | Loss: 0.00000791
Iteration 69/1000 | Loss: 0.00000791
Iteration 70/1000 | Loss: 0.00000790
Iteration 71/1000 | Loss: 0.00000789
Iteration 72/1000 | Loss: 0.00000789
Iteration 73/1000 | Loss: 0.00000789
Iteration 74/1000 | Loss: 0.00000788
Iteration 75/1000 | Loss: 0.00000788
Iteration 76/1000 | Loss: 0.00000788
Iteration 77/1000 | Loss: 0.00000788
Iteration 78/1000 | Loss: 0.00000788
Iteration 79/1000 | Loss: 0.00000788
Iteration 80/1000 | Loss: 0.00000788
Iteration 81/1000 | Loss: 0.00000788
Iteration 82/1000 | Loss: 0.00000787
Iteration 83/1000 | Loss: 0.00000787
Iteration 84/1000 | Loss: 0.00000787
Iteration 85/1000 | Loss: 0.00000787
Iteration 86/1000 | Loss: 0.00000787
Iteration 87/1000 | Loss: 0.00000787
Iteration 88/1000 | Loss: 0.00000787
Iteration 89/1000 | Loss: 0.00000787
Iteration 90/1000 | Loss: 0.00000787
Iteration 91/1000 | Loss: 0.00000787
Iteration 92/1000 | Loss: 0.00000787
Iteration 93/1000 | Loss: 0.00000787
Iteration 94/1000 | Loss: 0.00000787
Iteration 95/1000 | Loss: 0.00000787
Iteration 96/1000 | Loss: 0.00000787
Iteration 97/1000 | Loss: 0.00000787
Iteration 98/1000 | Loss: 0.00000786
Iteration 99/1000 | Loss: 0.00000786
Iteration 100/1000 | Loss: 0.00000786
Iteration 101/1000 | Loss: 0.00000786
Iteration 102/1000 | Loss: 0.00000786
Iteration 103/1000 | Loss: 0.00000786
Iteration 104/1000 | Loss: 0.00000786
Iteration 105/1000 | Loss: 0.00000786
Iteration 106/1000 | Loss: 0.00000786
Iteration 107/1000 | Loss: 0.00000785
Iteration 108/1000 | Loss: 0.00000785
Iteration 109/1000 | Loss: 0.00000785
Iteration 110/1000 | Loss: 0.00000785
Iteration 111/1000 | Loss: 0.00000785
Iteration 112/1000 | Loss: 0.00000785
Iteration 113/1000 | Loss: 0.00000785
Iteration 114/1000 | Loss: 0.00000785
Iteration 115/1000 | Loss: 0.00000785
Iteration 116/1000 | Loss: 0.00000785
Iteration 117/1000 | Loss: 0.00000785
Iteration 118/1000 | Loss: 0.00000785
Iteration 119/1000 | Loss: 0.00000785
Iteration 120/1000 | Loss: 0.00000785
Iteration 121/1000 | Loss: 0.00000785
Iteration 122/1000 | Loss: 0.00000785
Iteration 123/1000 | Loss: 0.00000785
Iteration 124/1000 | Loss: 0.00000785
Iteration 125/1000 | Loss: 0.00000785
Iteration 126/1000 | Loss: 0.00000785
Iteration 127/1000 | Loss: 0.00000785
Iteration 128/1000 | Loss: 0.00000785
Iteration 129/1000 | Loss: 0.00000785
Iteration 130/1000 | Loss: 0.00000785
Iteration 131/1000 | Loss: 0.00000785
Iteration 132/1000 | Loss: 0.00000785
Iteration 133/1000 | Loss: 0.00000785
Iteration 134/1000 | Loss: 0.00000785
Iteration 135/1000 | Loss: 0.00000785
Iteration 136/1000 | Loss: 0.00000785
Iteration 137/1000 | Loss: 0.00000785
Iteration 138/1000 | Loss: 0.00000785
Iteration 139/1000 | Loss: 0.00000785
Iteration 140/1000 | Loss: 0.00000785
Iteration 141/1000 | Loss: 0.00000785
Iteration 142/1000 | Loss: 0.00000785
Iteration 143/1000 | Loss: 0.00000785
Iteration 144/1000 | Loss: 0.00000785
Iteration 145/1000 | Loss: 0.00000785
Iteration 146/1000 | Loss: 0.00000785
Iteration 147/1000 | Loss: 0.00000785
Iteration 148/1000 | Loss: 0.00000785
Iteration 149/1000 | Loss: 0.00000785
Iteration 150/1000 | Loss: 0.00000785
Iteration 151/1000 | Loss: 0.00000785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [7.846304470149335e-06, 7.846304470149335e-06, 7.846304470149335e-06, 7.846304470149335e-06, 7.846304470149335e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.846304470149335e-06

Optimization complete. Final v2v error: 2.4189531803131104 mm

Highest mean error: 2.969083547592163 mm for frame 80

Lowest mean error: 2.252943277359009 mm for frame 104

Saving results

Total time: 223.83015894889832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1039
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899747
Iteration 2/25 | Loss: 0.00125588
Iteration 3/25 | Loss: 0.00117664
Iteration 4/25 | Loss: 0.00116266
Iteration 5/25 | Loss: 0.00115955
Iteration 6/25 | Loss: 0.00115955
Iteration 7/25 | Loss: 0.00115955
Iteration 8/25 | Loss: 0.00115955
Iteration 9/25 | Loss: 0.00115955
Iteration 10/25 | Loss: 0.00115955
Iteration 11/25 | Loss: 0.00115955
Iteration 12/25 | Loss: 0.00115955
Iteration 13/25 | Loss: 0.00115955
Iteration 14/25 | Loss: 0.00115955
Iteration 15/25 | Loss: 0.00115955
Iteration 16/25 | Loss: 0.00115955
Iteration 17/25 | Loss: 0.00115955
Iteration 18/25 | Loss: 0.00115955
Iteration 19/25 | Loss: 0.00115955
Iteration 20/25 | Loss: 0.00115955
Iteration 21/25 | Loss: 0.00115955
Iteration 22/25 | Loss: 0.00115955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011595505056902766, 0.0011595505056902766, 0.0011595505056902766, 0.0011595505056902766, 0.0011595505056902766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011595505056902766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33501506
Iteration 2/25 | Loss: 0.00177049
Iteration 3/25 | Loss: 0.00177049
Iteration 4/25 | Loss: 0.00177049
Iteration 5/25 | Loss: 0.00177049
Iteration 6/25 | Loss: 0.00177049
Iteration 7/25 | Loss: 0.00177049
Iteration 8/25 | Loss: 0.00177049
Iteration 9/25 | Loss: 0.00177049
Iteration 10/25 | Loss: 0.00177049
Iteration 11/25 | Loss: 0.00177049
Iteration 12/25 | Loss: 0.00177049
Iteration 13/25 | Loss: 0.00177049
Iteration 14/25 | Loss: 0.00177049
Iteration 15/25 | Loss: 0.00177049
Iteration 16/25 | Loss: 0.00177049
Iteration 17/25 | Loss: 0.00177049
Iteration 18/25 | Loss: 0.00177049
Iteration 19/25 | Loss: 0.00177049
Iteration 20/25 | Loss: 0.00177049
Iteration 21/25 | Loss: 0.00177049
Iteration 22/25 | Loss: 0.00177049
Iteration 23/25 | Loss: 0.00177049
Iteration 24/25 | Loss: 0.00177049
Iteration 25/25 | Loss: 0.00177049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177049
Iteration 2/1000 | Loss: 0.00001904
Iteration 3/1000 | Loss: 0.00001503
Iteration 4/1000 | Loss: 0.00001366
Iteration 5/1000 | Loss: 0.00001277
Iteration 6/1000 | Loss: 0.00001209
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001136
Iteration 9/1000 | Loss: 0.00001110
Iteration 10/1000 | Loss: 0.00001097
Iteration 11/1000 | Loss: 0.00001095
Iteration 12/1000 | Loss: 0.00001094
Iteration 13/1000 | Loss: 0.00001087
Iteration 14/1000 | Loss: 0.00001086
Iteration 15/1000 | Loss: 0.00001083
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001076
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001061
Iteration 21/1000 | Loss: 0.00001059
Iteration 22/1000 | Loss: 0.00001055
Iteration 23/1000 | Loss: 0.00001044
Iteration 24/1000 | Loss: 0.00001041
Iteration 25/1000 | Loss: 0.00001040
Iteration 26/1000 | Loss: 0.00001039
Iteration 27/1000 | Loss: 0.00001038
Iteration 28/1000 | Loss: 0.00001036
Iteration 29/1000 | Loss: 0.00001035
Iteration 30/1000 | Loss: 0.00001035
Iteration 31/1000 | Loss: 0.00001034
Iteration 32/1000 | Loss: 0.00001034
Iteration 33/1000 | Loss: 0.00001033
Iteration 34/1000 | Loss: 0.00001033
Iteration 35/1000 | Loss: 0.00001032
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001031
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001030
Iteration 41/1000 | Loss: 0.00001030
Iteration 42/1000 | Loss: 0.00001030
Iteration 43/1000 | Loss: 0.00001030
Iteration 44/1000 | Loss: 0.00001029
Iteration 45/1000 | Loss: 0.00001029
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001028
Iteration 48/1000 | Loss: 0.00001028
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001026
Iteration 51/1000 | Loss: 0.00001025
Iteration 52/1000 | Loss: 0.00001025
Iteration 53/1000 | Loss: 0.00001025
Iteration 54/1000 | Loss: 0.00001024
Iteration 55/1000 | Loss: 0.00001024
Iteration 56/1000 | Loss: 0.00001024
Iteration 57/1000 | Loss: 0.00001024
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001021
Iteration 61/1000 | Loss: 0.00001020
Iteration 62/1000 | Loss: 0.00001020
Iteration 63/1000 | Loss: 0.00001018
Iteration 64/1000 | Loss: 0.00001018
Iteration 65/1000 | Loss: 0.00001018
Iteration 66/1000 | Loss: 0.00001017
Iteration 67/1000 | Loss: 0.00001017
Iteration 68/1000 | Loss: 0.00001017
Iteration 69/1000 | Loss: 0.00001016
Iteration 70/1000 | Loss: 0.00001016
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001015
Iteration 75/1000 | Loss: 0.00001015
Iteration 76/1000 | Loss: 0.00001015
Iteration 77/1000 | Loss: 0.00001015
Iteration 78/1000 | Loss: 0.00001015
Iteration 79/1000 | Loss: 0.00001015
Iteration 80/1000 | Loss: 0.00001015
Iteration 81/1000 | Loss: 0.00001015
Iteration 82/1000 | Loss: 0.00001015
Iteration 83/1000 | Loss: 0.00001015
Iteration 84/1000 | Loss: 0.00001015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.0146137356059626e-05, 1.0146137356059626e-05, 1.0146137356059626e-05, 1.0146137356059626e-05, 1.0146137356059626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0146137356059626e-05

Optimization complete. Final v2v error: 2.7606070041656494 mm

Highest mean error: 3.208587169647217 mm for frame 142

Lowest mean error: 2.6828696727752686 mm for frame 208

Saving results

Total time: 352.67069578170776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1056
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968652
Iteration 2/25 | Loss: 0.00225071
Iteration 3/25 | Loss: 0.00156591
Iteration 4/25 | Loss: 0.00134588
Iteration 5/25 | Loss: 0.00133466
Iteration 6/25 | Loss: 0.00130428
Iteration 7/25 | Loss: 0.00126513
Iteration 8/25 | Loss: 0.00126374
Iteration 9/25 | Loss: 0.00126889
Iteration 10/25 | Loss: 0.00126212
Iteration 11/25 | Loss: 0.00124468
Iteration 12/25 | Loss: 0.00124646
Iteration 13/25 | Loss: 0.00123614
Iteration 14/25 | Loss: 0.00122242
Iteration 15/25 | Loss: 0.00121732
Iteration 16/25 | Loss: 0.00121872
Iteration 17/25 | Loss: 0.00121768
Iteration 18/25 | Loss: 0.00122111
Iteration 19/25 | Loss: 0.00121968
Iteration 20/25 | Loss: 0.00121739
Iteration 21/25 | Loss: 0.00121616
Iteration 22/25 | Loss: 0.00121510
Iteration 23/25 | Loss: 0.00121425
Iteration 24/25 | Loss: 0.00121681
Iteration 25/25 | Loss: 0.00121453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13522899
Iteration 2/25 | Loss: 0.00198981
Iteration 3/25 | Loss: 0.00193503
Iteration 4/25 | Loss: 0.00193503
Iteration 5/25 | Loss: 0.00193503
Iteration 6/25 | Loss: 0.00193503
Iteration 7/25 | Loss: 0.00193503
Iteration 8/25 | Loss: 0.00193503
Iteration 9/25 | Loss: 0.00193503
Iteration 10/25 | Loss: 0.00193503
Iteration 11/25 | Loss: 0.00193503
Iteration 12/25 | Loss: 0.00193503
Iteration 13/25 | Loss: 0.00193503
Iteration 14/25 | Loss: 0.00193503
Iteration 15/25 | Loss: 0.00193503
Iteration 16/25 | Loss: 0.00193503
Iteration 17/25 | Loss: 0.00193503
Iteration 18/25 | Loss: 0.00193503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019350290531292558, 0.0019350290531292558, 0.0019350290531292558, 0.0019350290531292558, 0.0019350290531292558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019350290531292558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193503
Iteration 2/1000 | Loss: 0.00026421
Iteration 3/1000 | Loss: 0.00004648
Iteration 4/1000 | Loss: 0.00015807
Iteration 5/1000 | Loss: 0.00030462
Iteration 6/1000 | Loss: 0.00024237
Iteration 7/1000 | Loss: 0.00013900
Iteration 8/1000 | Loss: 0.00004417
Iteration 9/1000 | Loss: 0.00004002
Iteration 10/1000 | Loss: 0.00017753
Iteration 11/1000 | Loss: 0.00016917
Iteration 12/1000 | Loss: 0.00030733
Iteration 13/1000 | Loss: 0.00121754
Iteration 14/1000 | Loss: 0.00034069
Iteration 15/1000 | Loss: 0.00005156
Iteration 16/1000 | Loss: 0.00004615
Iteration 17/1000 | Loss: 0.00012848
Iteration 18/1000 | Loss: 0.00024688
Iteration 19/1000 | Loss: 0.00004418
Iteration 20/1000 | Loss: 0.00004947
Iteration 21/1000 | Loss: 0.00003677
Iteration 22/1000 | Loss: 0.00004497
Iteration 23/1000 | Loss: 0.00004537
Iteration 24/1000 | Loss: 0.00003948
Iteration 25/1000 | Loss: 0.00004546
Iteration 26/1000 | Loss: 0.00004497
Iteration 27/1000 | Loss: 0.00005319
Iteration 28/1000 | Loss: 0.00012311
Iteration 29/1000 | Loss: 0.00004558
Iteration 30/1000 | Loss: 0.00003285
Iteration 31/1000 | Loss: 0.00003007
Iteration 32/1000 | Loss: 0.00003297
Iteration 33/1000 | Loss: 0.00004394
Iteration 34/1000 | Loss: 0.00013420
Iteration 35/1000 | Loss: 0.00011819
Iteration 36/1000 | Loss: 0.00006478
Iteration 37/1000 | Loss: 0.00009879
Iteration 38/1000 | Loss: 0.00003662
Iteration 39/1000 | Loss: 0.00003723
Iteration 40/1000 | Loss: 0.00003581
Iteration 41/1000 | Loss: 0.00003736
Iteration 42/1000 | Loss: 0.00003115
Iteration 43/1000 | Loss: 0.00002995
Iteration 44/1000 | Loss: 0.00003632
Iteration 45/1000 | Loss: 0.00003469
Iteration 46/1000 | Loss: 0.00004443
Iteration 47/1000 | Loss: 0.00019048
Iteration 48/1000 | Loss: 0.00002855
Iteration 49/1000 | Loss: 0.00002509
Iteration 50/1000 | Loss: 0.00002390
Iteration 51/1000 | Loss: 0.00002294
Iteration 52/1000 | Loss: 0.00002232
Iteration 53/1000 | Loss: 0.00002196
Iteration 54/1000 | Loss: 0.00002160
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002128
Iteration 57/1000 | Loss: 0.00002124
Iteration 58/1000 | Loss: 0.00002123
Iteration 59/1000 | Loss: 0.00002120
Iteration 60/1000 | Loss: 0.00002117
Iteration 61/1000 | Loss: 0.00002116
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00002114
Iteration 65/1000 | Loss: 0.00002113
Iteration 66/1000 | Loss: 0.00002113
Iteration 67/1000 | Loss: 0.00002112
Iteration 68/1000 | Loss: 0.00002106
Iteration 69/1000 | Loss: 0.00008143
Iteration 70/1000 | Loss: 0.00002091
Iteration 71/1000 | Loss: 0.00002085
Iteration 72/1000 | Loss: 0.00002081
Iteration 73/1000 | Loss: 0.00002077
Iteration 74/1000 | Loss: 0.00002076
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002074
Iteration 77/1000 | Loss: 0.00002074
Iteration 78/1000 | Loss: 0.00002074
Iteration 79/1000 | Loss: 0.00002074
Iteration 80/1000 | Loss: 0.00002074
Iteration 81/1000 | Loss: 0.00002074
Iteration 82/1000 | Loss: 0.00002074
Iteration 83/1000 | Loss: 0.00002074
Iteration 84/1000 | Loss: 0.00002074
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00002072
Iteration 87/1000 | Loss: 0.00002072
Iteration 88/1000 | Loss: 0.00002071
Iteration 89/1000 | Loss: 0.00002071
Iteration 90/1000 | Loss: 0.00002071
Iteration 91/1000 | Loss: 0.00002071
Iteration 92/1000 | Loss: 0.00002071
Iteration 93/1000 | Loss: 0.00002071
Iteration 94/1000 | Loss: 0.00002070
Iteration 95/1000 | Loss: 0.00002070
Iteration 96/1000 | Loss: 0.00002066
Iteration 97/1000 | Loss: 0.00002061
Iteration 98/1000 | Loss: 0.00002061
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002060
Iteration 102/1000 | Loss: 0.00002060
Iteration 103/1000 | Loss: 0.00002060
Iteration 104/1000 | Loss: 0.00002060
Iteration 105/1000 | Loss: 0.00002059
Iteration 106/1000 | Loss: 0.00002059
Iteration 107/1000 | Loss: 0.00002057
Iteration 108/1000 | Loss: 0.00002053
Iteration 109/1000 | Loss: 0.00002053
Iteration 110/1000 | Loss: 0.00002053
Iteration 111/1000 | Loss: 0.00002053
Iteration 112/1000 | Loss: 0.00002052
Iteration 113/1000 | Loss: 0.00002052
Iteration 114/1000 | Loss: 0.00002052
Iteration 115/1000 | Loss: 0.00002052
Iteration 116/1000 | Loss: 0.00002052
Iteration 117/1000 | Loss: 0.00002052
Iteration 118/1000 | Loss: 0.00002052
Iteration 119/1000 | Loss: 0.00002052
Iteration 120/1000 | Loss: 0.00002052
Iteration 121/1000 | Loss: 0.00002052
Iteration 122/1000 | Loss: 0.00002052
Iteration 123/1000 | Loss: 0.00002052
Iteration 124/1000 | Loss: 0.00002051
Iteration 125/1000 | Loss: 0.00002051
Iteration 126/1000 | Loss: 0.00002051
Iteration 127/1000 | Loss: 0.00002051
Iteration 128/1000 | Loss: 0.00002051
Iteration 129/1000 | Loss: 0.00002050
Iteration 130/1000 | Loss: 0.00002050
Iteration 131/1000 | Loss: 0.00002050
Iteration 132/1000 | Loss: 0.00002049
Iteration 133/1000 | Loss: 0.00002049
Iteration 134/1000 | Loss: 0.00002049
Iteration 135/1000 | Loss: 0.00002048
Iteration 136/1000 | Loss: 0.00002048
Iteration 137/1000 | Loss: 0.00002048
Iteration 138/1000 | Loss: 0.00002047
Iteration 139/1000 | Loss: 0.00002047
Iteration 140/1000 | Loss: 0.00002047
Iteration 141/1000 | Loss: 0.00002046
Iteration 142/1000 | Loss: 0.00002046
Iteration 143/1000 | Loss: 0.00002046
Iteration 144/1000 | Loss: 0.00002045
Iteration 145/1000 | Loss: 0.00002045
Iteration 146/1000 | Loss: 0.00002045
Iteration 147/1000 | Loss: 0.00002045
Iteration 148/1000 | Loss: 0.00002044
Iteration 149/1000 | Loss: 0.00002044
Iteration 150/1000 | Loss: 0.00002044
Iteration 151/1000 | Loss: 0.00002043
Iteration 152/1000 | Loss: 0.00002043
Iteration 153/1000 | Loss: 0.00002042
Iteration 154/1000 | Loss: 0.00002042
Iteration 155/1000 | Loss: 0.00002042
Iteration 156/1000 | Loss: 0.00002042
Iteration 157/1000 | Loss: 0.00002041
Iteration 158/1000 | Loss: 0.00002041
Iteration 159/1000 | Loss: 0.00002041
Iteration 160/1000 | Loss: 0.00002040
Iteration 161/1000 | Loss: 0.00002040
Iteration 162/1000 | Loss: 0.00002040
Iteration 163/1000 | Loss: 0.00002040
Iteration 164/1000 | Loss: 0.00002040
Iteration 165/1000 | Loss: 0.00002040
Iteration 166/1000 | Loss: 0.00002039
Iteration 167/1000 | Loss: 0.00002039
Iteration 168/1000 | Loss: 0.00002039
Iteration 169/1000 | Loss: 0.00002039
Iteration 170/1000 | Loss: 0.00002038
Iteration 171/1000 | Loss: 0.00002038
Iteration 172/1000 | Loss: 0.00002038
Iteration 173/1000 | Loss: 0.00002038
Iteration 174/1000 | Loss: 0.00002038
Iteration 175/1000 | Loss: 0.00002038
Iteration 176/1000 | Loss: 0.00002038
Iteration 177/1000 | Loss: 0.00002038
Iteration 178/1000 | Loss: 0.00002038
Iteration 179/1000 | Loss: 0.00002037
Iteration 180/1000 | Loss: 0.00002037
Iteration 181/1000 | Loss: 0.00002037
Iteration 182/1000 | Loss: 0.00002037
Iteration 183/1000 | Loss: 0.00002037
Iteration 184/1000 | Loss: 0.00002037
Iteration 185/1000 | Loss: 0.00002037
Iteration 186/1000 | Loss: 0.00002036
Iteration 187/1000 | Loss: 0.00002036
Iteration 188/1000 | Loss: 0.00002036
Iteration 189/1000 | Loss: 0.00002036
Iteration 190/1000 | Loss: 0.00002036
Iteration 191/1000 | Loss: 0.00002035
Iteration 192/1000 | Loss: 0.00002035
Iteration 193/1000 | Loss: 0.00002035
Iteration 194/1000 | Loss: 0.00002035
Iteration 195/1000 | Loss: 0.00002035
Iteration 196/1000 | Loss: 0.00002035
Iteration 197/1000 | Loss: 0.00002034
Iteration 198/1000 | Loss: 0.00002034
Iteration 199/1000 | Loss: 0.00002034
Iteration 200/1000 | Loss: 0.00002034
Iteration 201/1000 | Loss: 0.00002034
Iteration 202/1000 | Loss: 0.00002034
Iteration 203/1000 | Loss: 0.00002033
Iteration 204/1000 | Loss: 0.00002033
Iteration 205/1000 | Loss: 0.00002033
Iteration 206/1000 | Loss: 0.00002033
Iteration 207/1000 | Loss: 0.00002033
Iteration 208/1000 | Loss: 0.00002033
Iteration 209/1000 | Loss: 0.00002033
Iteration 210/1000 | Loss: 0.00002033
Iteration 211/1000 | Loss: 0.00002033
Iteration 212/1000 | Loss: 0.00002033
Iteration 213/1000 | Loss: 0.00002033
Iteration 214/1000 | Loss: 0.00002033
Iteration 215/1000 | Loss: 0.00002033
Iteration 216/1000 | Loss: 0.00002033
Iteration 217/1000 | Loss: 0.00002033
Iteration 218/1000 | Loss: 0.00002033
Iteration 219/1000 | Loss: 0.00002033
Iteration 220/1000 | Loss: 0.00002033
Iteration 221/1000 | Loss: 0.00002033
Iteration 222/1000 | Loss: 0.00002033
Iteration 223/1000 | Loss: 0.00002033
Iteration 224/1000 | Loss: 0.00002033
Iteration 225/1000 | Loss: 0.00002033
Iteration 226/1000 | Loss: 0.00002033
Iteration 227/1000 | Loss: 0.00002033
Iteration 228/1000 | Loss: 0.00002033
Iteration 229/1000 | Loss: 0.00002033
Iteration 230/1000 | Loss: 0.00002033
Iteration 231/1000 | Loss: 0.00002033
Iteration 232/1000 | Loss: 0.00002033
Iteration 233/1000 | Loss: 0.00002033
Iteration 234/1000 | Loss: 0.00002033
Iteration 235/1000 | Loss: 0.00002033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.0331433915998787e-05, 2.0331433915998787e-05, 2.0331433915998787e-05, 2.0331433915998787e-05, 2.0331433915998787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0331433915998787e-05

Optimization complete. Final v2v error: 3.782792091369629 mm

Highest mean error: 5.711157321929932 mm for frame 8

Lowest mean error: 2.822939157485962 mm for frame 59

Saving results

Total time: 1829.6826441287994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1046
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021410
Iteration 2/25 | Loss: 0.00187779
Iteration 3/25 | Loss: 0.00145966
Iteration 4/25 | Loss: 0.00135374
Iteration 5/25 | Loss: 0.00129455
Iteration 6/25 | Loss: 0.00127733
Iteration 7/25 | Loss: 0.00127932
Iteration 8/25 | Loss: 0.00126810
Iteration 9/25 | Loss: 0.00125261
Iteration 10/25 | Loss: 0.00134199
Iteration 11/25 | Loss: 0.00122704
Iteration 12/25 | Loss: 0.00122054
Iteration 13/25 | Loss: 0.00121968
Iteration 14/25 | Loss: 0.00122135
Iteration 15/25 | Loss: 0.00122087
Iteration 16/25 | Loss: 0.00122138
Iteration 17/25 | Loss: 0.00122137
Iteration 18/25 | Loss: 0.00122098
Iteration 19/25 | Loss: 0.00122062
Iteration 20/25 | Loss: 0.00122079
Iteration 21/25 | Loss: 0.00122080
Iteration 22/25 | Loss: 0.00122064
Iteration 23/25 | Loss: 0.00122091
Iteration 24/25 | Loss: 0.00122103
Iteration 25/25 | Loss: 0.00121875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34553337
Iteration 2/25 | Loss: 0.00267601
Iteration 3/25 | Loss: 0.00267600
Iteration 4/25 | Loss: 0.00267600
Iteration 5/25 | Loss: 0.00267600
Iteration 6/25 | Loss: 0.00267600
Iteration 7/25 | Loss: 0.00267600
Iteration 8/25 | Loss: 0.00267600
Iteration 9/25 | Loss: 0.00267600
Iteration 10/25 | Loss: 0.00267600
Iteration 11/25 | Loss: 0.00267600
Iteration 12/25 | Loss: 0.00267600
Iteration 13/25 | Loss: 0.00267600
Iteration 14/25 | Loss: 0.00267600
Iteration 15/25 | Loss: 0.00267600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002676001749932766, 0.002676001749932766, 0.002676001749932766, 0.002676001749932766, 0.002676001749932766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002676001749932766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267600
Iteration 2/1000 | Loss: 0.00004092
Iteration 3/1000 | Loss: 0.00002757
Iteration 4/1000 | Loss: 0.00002320
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001816
Iteration 9/1000 | Loss: 0.00011597
Iteration 10/1000 | Loss: 0.00002359
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00001888
Iteration 13/1000 | Loss: 0.00013900
Iteration 14/1000 | Loss: 0.00009643
Iteration 15/1000 | Loss: 0.00002460
Iteration 16/1000 | Loss: 0.00002249
Iteration 17/1000 | Loss: 0.00002145
Iteration 18/1000 | Loss: 0.00002010
Iteration 19/1000 | Loss: 0.00001921
Iteration 20/1000 | Loss: 0.00001881
Iteration 21/1000 | Loss: 0.00004184
Iteration 22/1000 | Loss: 0.00002573
Iteration 23/1000 | Loss: 0.00002879
Iteration 24/1000 | Loss: 0.00002545
Iteration 25/1000 | Loss: 0.00004177
Iteration 26/1000 | Loss: 0.00002688
Iteration 27/1000 | Loss: 0.00003512
Iteration 28/1000 | Loss: 0.00002142
Iteration 29/1000 | Loss: 0.00002031
Iteration 30/1000 | Loss: 0.00001917
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001853
Iteration 34/1000 | Loss: 0.00001850
Iteration 35/1000 | Loss: 0.00001820
Iteration 36/1000 | Loss: 0.00001804
Iteration 37/1000 | Loss: 0.00001792
Iteration 38/1000 | Loss: 0.00001791
Iteration 39/1000 | Loss: 0.00001791
Iteration 40/1000 | Loss: 0.00001789
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001769
Iteration 56/1000 | Loss: 0.00001752
Iteration 57/1000 | Loss: 0.00001724
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001693
Iteration 60/1000 | Loss: 0.00001692
Iteration 61/1000 | Loss: 0.00001690
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001683
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001667
Iteration 73/1000 | Loss: 0.00001665
Iteration 74/1000 | Loss: 0.00001665
Iteration 75/1000 | Loss: 0.00001665
Iteration 76/1000 | Loss: 0.00001665
Iteration 77/1000 | Loss: 0.00001664
Iteration 78/1000 | Loss: 0.00001664
Iteration 79/1000 | Loss: 0.00001663
Iteration 80/1000 | Loss: 0.00001663
Iteration 81/1000 | Loss: 0.00001663
Iteration 82/1000 | Loss: 0.00001662
Iteration 83/1000 | Loss: 0.00001662
Iteration 84/1000 | Loss: 0.00001662
Iteration 85/1000 | Loss: 0.00001662
Iteration 86/1000 | Loss: 0.00001661
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001661
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001659
Iteration 93/1000 | Loss: 0.00001658
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001657
Iteration 96/1000 | Loss: 0.00001657
Iteration 97/1000 | Loss: 0.00001657
Iteration 98/1000 | Loss: 0.00001657
Iteration 99/1000 | Loss: 0.00001657
Iteration 100/1000 | Loss: 0.00001657
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001656
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00001656
Iteration 109/1000 | Loss: 0.00001655
Iteration 110/1000 | Loss: 0.00001655
Iteration 111/1000 | Loss: 0.00001655
Iteration 112/1000 | Loss: 0.00001655
Iteration 113/1000 | Loss: 0.00001655
Iteration 114/1000 | Loss: 0.00001654
Iteration 115/1000 | Loss: 0.00001654
Iteration 116/1000 | Loss: 0.00001654
Iteration 117/1000 | Loss: 0.00001654
Iteration 118/1000 | Loss: 0.00001654
Iteration 119/1000 | Loss: 0.00001653
Iteration 120/1000 | Loss: 0.00001653
Iteration 121/1000 | Loss: 0.00001653
Iteration 122/1000 | Loss: 0.00001653
Iteration 123/1000 | Loss: 0.00001653
Iteration 124/1000 | Loss: 0.00001653
Iteration 125/1000 | Loss: 0.00023469
Iteration 126/1000 | Loss: 0.00011883
Iteration 127/1000 | Loss: 0.00003007
Iteration 128/1000 | Loss: 0.00001871
Iteration 129/1000 | Loss: 0.00001754
Iteration 130/1000 | Loss: 0.00001676
Iteration 131/1000 | Loss: 0.00022208
Iteration 132/1000 | Loss: 0.00006809
Iteration 133/1000 | Loss: 0.00002773
Iteration 134/1000 | Loss: 0.00001717
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001672
Iteration 137/1000 | Loss: 0.00020861
Iteration 138/1000 | Loss: 0.00005090
Iteration 139/1000 | Loss: 0.00025684
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00001914
Iteration 142/1000 | Loss: 0.00001737
Iteration 143/1000 | Loss: 0.00014050
Iteration 144/1000 | Loss: 0.00011343
Iteration 145/1000 | Loss: 0.00001706
Iteration 146/1000 | Loss: 0.00012357
Iteration 147/1000 | Loss: 0.00002592
Iteration 148/1000 | Loss: 0.00001857
Iteration 149/1000 | Loss: 0.00001713
Iteration 150/1000 | Loss: 0.00012674
Iteration 151/1000 | Loss: 0.00016852
Iteration 152/1000 | Loss: 0.00013770
Iteration 153/1000 | Loss: 0.00001723
Iteration 154/1000 | Loss: 0.00001644
Iteration 155/1000 | Loss: 0.00001595
Iteration 156/1000 | Loss: 0.00001564
Iteration 157/1000 | Loss: 0.00001560
Iteration 158/1000 | Loss: 0.00001558
Iteration 159/1000 | Loss: 0.00001557
Iteration 160/1000 | Loss: 0.00001557
Iteration 161/1000 | Loss: 0.00001556
Iteration 162/1000 | Loss: 0.00001556
Iteration 163/1000 | Loss: 0.00001555
Iteration 164/1000 | Loss: 0.00001554
Iteration 165/1000 | Loss: 0.00001553
Iteration 166/1000 | Loss: 0.00001553
Iteration 167/1000 | Loss: 0.00001552
Iteration 168/1000 | Loss: 0.00001550
Iteration 169/1000 | Loss: 0.00001546
Iteration 170/1000 | Loss: 0.00001544
Iteration 171/1000 | Loss: 0.00001544
Iteration 172/1000 | Loss: 0.00001544
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001542
Iteration 175/1000 | Loss: 0.00001542
Iteration 176/1000 | Loss: 0.00001541
Iteration 177/1000 | Loss: 0.00001541
Iteration 178/1000 | Loss: 0.00001541
Iteration 179/1000 | Loss: 0.00001540
Iteration 180/1000 | Loss: 0.00001537
Iteration 181/1000 | Loss: 0.00001536
Iteration 182/1000 | Loss: 0.00001531
Iteration 183/1000 | Loss: 0.00003256
Iteration 184/1000 | Loss: 0.00003256
Iteration 185/1000 | Loss: 0.00002181
Iteration 186/1000 | Loss: 0.00002561
Iteration 187/1000 | Loss: 0.00003247
Iteration 188/1000 | Loss: 0.00002469
Iteration 189/1000 | Loss: 0.00001961
Iteration 190/1000 | Loss: 0.00002287
Iteration 191/1000 | Loss: 0.00003228
Iteration 192/1000 | Loss: 0.00002480
Iteration 193/1000 | Loss: 0.00003231
Iteration 194/1000 | Loss: 0.00002119
Iteration 195/1000 | Loss: 0.00003208
Iteration 196/1000 | Loss: 0.00001695
Iteration 197/1000 | Loss: 0.00001642
Iteration 198/1000 | Loss: 0.00002456
Iteration 199/1000 | Loss: 0.00001982
Iteration 200/1000 | Loss: 0.00002308
Iteration 201/1000 | Loss: 0.00002450
Iteration 202/1000 | Loss: 0.00002293
Iteration 203/1000 | Loss: 0.00002381
Iteration 204/1000 | Loss: 0.00002080
Iteration 205/1000 | Loss: 0.00001828
Iteration 206/1000 | Loss: 0.00001992
Iteration 207/1000 | Loss: 0.00002260
Iteration 208/1000 | Loss: 0.00002537
Iteration 209/1000 | Loss: 0.00002582
Iteration 210/1000 | Loss: 0.00001719
Iteration 211/1000 | Loss: 0.00002528
Iteration 212/1000 | Loss: 0.00002024
Iteration 213/1000 | Loss: 0.00002708
Iteration 214/1000 | Loss: 0.00002954
Iteration 215/1000 | Loss: 0.00002502
Iteration 216/1000 | Loss: 0.00002397
Iteration 217/1000 | Loss: 0.00001959
Iteration 218/1000 | Loss: 0.00001859
Iteration 219/1000 | Loss: 0.00002693
Iteration 220/1000 | Loss: 0.00002693
Iteration 221/1000 | Loss: 0.00001837
Iteration 222/1000 | Loss: 0.00001577
Iteration 223/1000 | Loss: 0.00001519
Iteration 224/1000 | Loss: 0.00001491
Iteration 225/1000 | Loss: 0.00001488
Iteration 226/1000 | Loss: 0.00001488
Iteration 227/1000 | Loss: 0.00001488
Iteration 228/1000 | Loss: 0.00001488
Iteration 229/1000 | Loss: 0.00001488
Iteration 230/1000 | Loss: 0.00001488
Iteration 231/1000 | Loss: 0.00001488
Iteration 232/1000 | Loss: 0.00001488
Iteration 233/1000 | Loss: 0.00001488
Iteration 234/1000 | Loss: 0.00001488
Iteration 235/1000 | Loss: 0.00001487
Iteration 236/1000 | Loss: 0.00001487
Iteration 237/1000 | Loss: 0.00001487
Iteration 238/1000 | Loss: 0.00001487
Iteration 239/1000 | Loss: 0.00001487
Iteration 240/1000 | Loss: 0.00001487
Iteration 241/1000 | Loss: 0.00001487
Iteration 242/1000 | Loss: 0.00001487
Iteration 243/1000 | Loss: 0.00001487
Iteration 244/1000 | Loss: 0.00001487
Iteration 245/1000 | Loss: 0.00001486
Iteration 246/1000 | Loss: 0.00001485
Iteration 247/1000 | Loss: 0.00001485
Iteration 248/1000 | Loss: 0.00001485
Iteration 249/1000 | Loss: 0.00001485
Iteration 250/1000 | Loss: 0.00001484
Iteration 251/1000 | Loss: 0.00001484
Iteration 252/1000 | Loss: 0.00001484
Iteration 253/1000 | Loss: 0.00001484
Iteration 254/1000 | Loss: 0.00001484
Iteration 255/1000 | Loss: 0.00001483
Iteration 256/1000 | Loss: 0.00001483
Iteration 257/1000 | Loss: 0.00001483
Iteration 258/1000 | Loss: 0.00001483
Iteration 259/1000 | Loss: 0.00001483
Iteration 260/1000 | Loss: 0.00001483
Iteration 261/1000 | Loss: 0.00001483
Iteration 262/1000 | Loss: 0.00001483
Iteration 263/1000 | Loss: 0.00001482
Iteration 264/1000 | Loss: 0.00001482
Iteration 265/1000 | Loss: 0.00001482
Iteration 266/1000 | Loss: 0.00001482
Iteration 267/1000 | Loss: 0.00001482
Iteration 268/1000 | Loss: 0.00001482
Iteration 269/1000 | Loss: 0.00001482
Iteration 270/1000 | Loss: 0.00001482
Iteration 271/1000 | Loss: 0.00001482
Iteration 272/1000 | Loss: 0.00001481
Iteration 273/1000 | Loss: 0.00001481
Iteration 274/1000 | Loss: 0.00001481
Iteration 275/1000 | Loss: 0.00001481
Iteration 276/1000 | Loss: 0.00001481
Iteration 277/1000 | Loss: 0.00001481
Iteration 278/1000 | Loss: 0.00001481
Iteration 279/1000 | Loss: 0.00001481
Iteration 280/1000 | Loss: 0.00001481
Iteration 281/1000 | Loss: 0.00001480
Iteration 282/1000 | Loss: 0.00001480
Iteration 283/1000 | Loss: 0.00001480
Iteration 284/1000 | Loss: 0.00001480
Iteration 285/1000 | Loss: 0.00001480
Iteration 286/1000 | Loss: 0.00001480
Iteration 287/1000 | Loss: 0.00001480
Iteration 288/1000 | Loss: 0.00001480
Iteration 289/1000 | Loss: 0.00001480
Iteration 290/1000 | Loss: 0.00001480
Iteration 291/1000 | Loss: 0.00001480
Iteration 292/1000 | Loss: 0.00001480
Iteration 293/1000 | Loss: 0.00001480
Iteration 294/1000 | Loss: 0.00001480
Iteration 295/1000 | Loss: 0.00001480
Iteration 296/1000 | Loss: 0.00001479
Iteration 297/1000 | Loss: 0.00001479
Iteration 298/1000 | Loss: 0.00001479
Iteration 299/1000 | Loss: 0.00001479
Iteration 300/1000 | Loss: 0.00001479
Iteration 301/1000 | Loss: 0.00001479
Iteration 302/1000 | Loss: 0.00001479
Iteration 303/1000 | Loss: 0.00001479
Iteration 304/1000 | Loss: 0.00001479
Iteration 305/1000 | Loss: 0.00001479
Iteration 306/1000 | Loss: 0.00001479
Iteration 307/1000 | Loss: 0.00001479
Iteration 308/1000 | Loss: 0.00001479
Iteration 309/1000 | Loss: 0.00001479
Iteration 310/1000 | Loss: 0.00001479
Iteration 311/1000 | Loss: 0.00001479
Iteration 312/1000 | Loss: 0.00001479
Iteration 313/1000 | Loss: 0.00001479
Iteration 314/1000 | Loss: 0.00001479
Iteration 315/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.4787031432206277e-05, 1.4787031432206277e-05, 1.4787031432206277e-05, 1.4787031432206277e-05, 1.4787031432206277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4787031432206277e-05

Optimization complete. Final v2v error: 3.3549351692199707 mm

Highest mean error: 4.57724142074585 mm for frame 13

Lowest mean error: 3.0927634239196777 mm for frame 92

Saving results

Total time: 2106.7349832057953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1001
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769915
Iteration 2/25 | Loss: 0.00167935
Iteration 3/25 | Loss: 0.00136771
Iteration 4/25 | Loss: 0.00132940
Iteration 5/25 | Loss: 0.00130876
Iteration 6/25 | Loss: 0.00130985
Iteration 7/25 | Loss: 0.00130780
Iteration 8/25 | Loss: 0.00129656
Iteration 9/25 | Loss: 0.00128608
Iteration 10/25 | Loss: 0.00128703
Iteration 11/25 | Loss: 0.00128527
Iteration 12/25 | Loss: 0.00128142
Iteration 13/25 | Loss: 0.00127301
Iteration 14/25 | Loss: 0.00126830
Iteration 15/25 | Loss: 0.00126862
Iteration 16/25 | Loss: 0.00127040
Iteration 17/25 | Loss: 0.00126732
Iteration 18/25 | Loss: 0.00126279
Iteration 19/25 | Loss: 0.00126122
Iteration 20/25 | Loss: 0.00126065
Iteration 21/25 | Loss: 0.00126154
Iteration 22/25 | Loss: 0.00126968
Iteration 23/25 | Loss: 0.00126040
Iteration 24/25 | Loss: 0.00125796
Iteration 25/25 | Loss: 0.00125780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22756565
Iteration 2/25 | Loss: 0.00170392
Iteration 3/25 | Loss: 0.00170392
Iteration 4/25 | Loss: 0.00170392
Iteration 5/25 | Loss: 0.00170392
Iteration 6/25 | Loss: 0.00170392
Iteration 7/25 | Loss: 0.00170392
Iteration 8/25 | Loss: 0.00170392
Iteration 9/25 | Loss: 0.00170392
Iteration 10/25 | Loss: 0.00170392
Iteration 11/25 | Loss: 0.00170392
Iteration 12/25 | Loss: 0.00170392
Iteration 13/25 | Loss: 0.00170392
Iteration 14/25 | Loss: 0.00170392
Iteration 15/25 | Loss: 0.00170392
Iteration 16/25 | Loss: 0.00170392
Iteration 17/25 | Loss: 0.00170392
Iteration 18/25 | Loss: 0.00170392
Iteration 19/25 | Loss: 0.00170392
Iteration 20/25 | Loss: 0.00170392
Iteration 21/25 | Loss: 0.00170392
Iteration 22/25 | Loss: 0.00170392
Iteration 23/25 | Loss: 0.00170392
Iteration 24/25 | Loss: 0.00170392
Iteration 25/25 | Loss: 0.00170392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170392
Iteration 2/1000 | Loss: 0.00004420
Iteration 3/1000 | Loss: 0.00003037
Iteration 4/1000 | Loss: 0.00003819
Iteration 5/1000 | Loss: 0.00003079
Iteration 6/1000 | Loss: 0.00002782
Iteration 7/1000 | Loss: 0.00003108
Iteration 8/1000 | Loss: 0.00003563
Iteration 9/1000 | Loss: 0.00003427
Iteration 10/1000 | Loss: 0.00003315
Iteration 11/1000 | Loss: 0.00003218
Iteration 12/1000 | Loss: 0.00003092
Iteration 13/1000 | Loss: 0.00003327
Iteration 14/1000 | Loss: 0.00003428
Iteration 15/1000 | Loss: 0.00003321
Iteration 16/1000 | Loss: 0.00003459
Iteration 17/1000 | Loss: 0.00003261
Iteration 18/1000 | Loss: 0.00003546
Iteration 19/1000 | Loss: 0.00002840
Iteration 20/1000 | Loss: 0.00003670
Iteration 21/1000 | Loss: 0.00003260
Iteration 22/1000 | Loss: 0.00003201
Iteration 23/1000 | Loss: 0.00003280
Iteration 24/1000 | Loss: 0.00003648
Iteration 25/1000 | Loss: 0.00003217
Iteration 26/1000 | Loss: 0.00003472
Iteration 27/1000 | Loss: 0.00003116
Iteration 28/1000 | Loss: 0.00003137
Iteration 29/1000 | Loss: 0.00002397
Iteration 30/1000 | Loss: 0.00003434
Iteration 31/1000 | Loss: 0.00003770
Iteration 32/1000 | Loss: 0.00004167
Iteration 33/1000 | Loss: 0.00002559
Iteration 34/1000 | Loss: 0.00002296
Iteration 35/1000 | Loss: 0.00002236
Iteration 36/1000 | Loss: 0.00002179
Iteration 37/1000 | Loss: 0.00002159
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002059
Iteration 40/1000 | Loss: 0.00002047
Iteration 41/1000 | Loss: 0.00002040
Iteration 42/1000 | Loss: 0.00002038
Iteration 43/1000 | Loss: 0.00002038
Iteration 44/1000 | Loss: 0.00002037
Iteration 45/1000 | Loss: 0.00002032
Iteration 46/1000 | Loss: 0.00002029
Iteration 47/1000 | Loss: 0.00002025
Iteration 48/1000 | Loss: 0.00002024
Iteration 49/1000 | Loss: 0.00002023
Iteration 50/1000 | Loss: 0.00002021
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002020
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002019
Iteration 58/1000 | Loss: 0.00002018
Iteration 59/1000 | Loss: 0.00002018
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00002017
Iteration 62/1000 | Loss: 0.00002017
Iteration 63/1000 | Loss: 0.00002017
Iteration 64/1000 | Loss: 0.00002017
Iteration 65/1000 | Loss: 0.00002016
Iteration 66/1000 | Loss: 0.00002016
Iteration 67/1000 | Loss: 0.00002016
Iteration 68/1000 | Loss: 0.00002016
Iteration 69/1000 | Loss: 0.00002015
Iteration 70/1000 | Loss: 0.00002015
Iteration 71/1000 | Loss: 0.00002015
Iteration 72/1000 | Loss: 0.00002014
Iteration 73/1000 | Loss: 0.00002014
Iteration 74/1000 | Loss: 0.00002014
Iteration 75/1000 | Loss: 0.00002014
Iteration 76/1000 | Loss: 0.00002013
Iteration 77/1000 | Loss: 0.00002013
Iteration 78/1000 | Loss: 0.00002013
Iteration 79/1000 | Loss: 0.00002013
Iteration 80/1000 | Loss: 0.00002013
Iteration 81/1000 | Loss: 0.00002013
Iteration 82/1000 | Loss: 0.00002012
Iteration 83/1000 | Loss: 0.00002012
Iteration 84/1000 | Loss: 0.00002012
Iteration 85/1000 | Loss: 0.00002012
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002012
Iteration 90/1000 | Loss: 0.00002012
Iteration 91/1000 | Loss: 0.00002012
Iteration 92/1000 | Loss: 0.00002012
Iteration 93/1000 | Loss: 0.00002012
Iteration 94/1000 | Loss: 0.00002012
Iteration 95/1000 | Loss: 0.00002012
Iteration 96/1000 | Loss: 0.00002012
Iteration 97/1000 | Loss: 0.00002012
Iteration 98/1000 | Loss: 0.00002012
Iteration 99/1000 | Loss: 0.00002012
Iteration 100/1000 | Loss: 0.00002012
Iteration 101/1000 | Loss: 0.00002012
Iteration 102/1000 | Loss: 0.00002012
Iteration 103/1000 | Loss: 0.00002012
Iteration 104/1000 | Loss: 0.00002012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.011819560721051e-05, 2.011819560721051e-05, 2.011819560721051e-05, 2.011819560721051e-05, 2.011819560721051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.011819560721051e-05

Optimization complete. Final v2v error: 3.693190574645996 mm

Highest mean error: 5.768640041351318 mm for frame 103

Lowest mean error: 2.9912474155426025 mm for frame 189

Saving results

Total time: 1314.594066619873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1057
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005202
Iteration 2/25 | Loss: 0.00192242
Iteration 3/25 | Loss: 0.00151931
Iteration 4/25 | Loss: 0.00130305
Iteration 5/25 | Loss: 0.00125848
Iteration 6/25 | Loss: 0.00124597
Iteration 7/25 | Loss: 0.00123619
Iteration 8/25 | Loss: 0.00124130
Iteration 9/25 | Loss: 0.00123989
Iteration 10/25 | Loss: 0.00123220
Iteration 11/25 | Loss: 0.00122735
Iteration 12/25 | Loss: 0.00122411
Iteration 13/25 | Loss: 0.00121491
Iteration 14/25 | Loss: 0.00121852
Iteration 15/25 | Loss: 0.00121220
Iteration 16/25 | Loss: 0.00121445
Iteration 17/25 | Loss: 0.00119779
Iteration 18/25 | Loss: 0.00119150
Iteration 19/25 | Loss: 0.00118679
Iteration 20/25 | Loss: 0.00118532
Iteration 21/25 | Loss: 0.00118466
Iteration 22/25 | Loss: 0.00118446
Iteration 23/25 | Loss: 0.00118440
Iteration 24/25 | Loss: 0.00118440
Iteration 25/25 | Loss: 0.00118440

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18707788
Iteration 2/25 | Loss: 0.00257467
Iteration 3/25 | Loss: 0.00257467
Iteration 4/25 | Loss: 0.00257467
Iteration 5/25 | Loss: 0.00257467
Iteration 6/25 | Loss: 0.00257467
Iteration 7/25 | Loss: 0.00257467
Iteration 8/25 | Loss: 0.00257467
Iteration 9/25 | Loss: 0.00257467
Iteration 10/25 | Loss: 0.00257467
Iteration 11/25 | Loss: 0.00257467
Iteration 12/25 | Loss: 0.00257467
Iteration 13/25 | Loss: 0.00257467
Iteration 14/25 | Loss: 0.00257467
Iteration 15/25 | Loss: 0.00257467
Iteration 16/25 | Loss: 0.00257467
Iteration 17/25 | Loss: 0.00257467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0025746708270162344, 0.0025746708270162344, 0.0025746708270162344, 0.0025746708270162344, 0.0025746708270162344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025746708270162344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257467
Iteration 2/1000 | Loss: 0.00004560
Iteration 3/1000 | Loss: 0.00003000
Iteration 4/1000 | Loss: 0.00003543
Iteration 5/1000 | Loss: 0.00001850
Iteration 6/1000 | Loss: 0.00002319
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00006009
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00002793
Iteration 12/1000 | Loss: 0.00007914
Iteration 13/1000 | Loss: 0.00006903
Iteration 14/1000 | Loss: 0.00002418
Iteration 15/1000 | Loss: 0.00003025
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001521
Iteration 19/1000 | Loss: 0.00001336
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001335
Iteration 29/1000 | Loss: 0.00001335
Iteration 30/1000 | Loss: 0.00001335
Iteration 31/1000 | Loss: 0.00001335
Iteration 32/1000 | Loss: 0.00001335
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001326
Iteration 37/1000 | Loss: 0.00001326
Iteration 38/1000 | Loss: 0.00001326
Iteration 39/1000 | Loss: 0.00001326
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001325
Iteration 42/1000 | Loss: 0.00001325
Iteration 43/1000 | Loss: 0.00001325
Iteration 44/1000 | Loss: 0.00001325
Iteration 45/1000 | Loss: 0.00001325
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001318
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001315
Iteration 51/1000 | Loss: 0.00001315
Iteration 52/1000 | Loss: 0.00001315
Iteration 53/1000 | Loss: 0.00001313
Iteration 54/1000 | Loss: 0.00001312
Iteration 55/1000 | Loss: 0.00001312
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001369
Iteration 58/1000 | Loss: 0.00001306
Iteration 59/1000 | Loss: 0.00001305
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001305
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001304
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00003348
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001338
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001297
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001297
Iteration 85/1000 | Loss: 0.00001297
Iteration 86/1000 | Loss: 0.00001297
Iteration 87/1000 | Loss: 0.00001297
Iteration 88/1000 | Loss: 0.00001297
Iteration 89/1000 | Loss: 0.00001297
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001297
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001297
Iteration 97/1000 | Loss: 0.00001297
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.2968322153028566e-05, 1.2968322153028566e-05, 1.2968322153028566e-05, 1.2968322153028566e-05, 1.2968322153028566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2968322153028566e-05

Optimization complete. Final v2v error: 3.0881996154785156 mm

Highest mean error: 3.7688677310943604 mm for frame 63

Lowest mean error: 2.6077992916107178 mm for frame 4

Saving results

Total time: 1067.4503421783447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1058
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00345110
Iteration 2/25 | Loss: 0.00127882
Iteration 3/25 | Loss: 0.00117242
Iteration 4/25 | Loss: 0.00115204
Iteration 5/25 | Loss: 0.00114526
Iteration 6/25 | Loss: 0.00114413
Iteration 7/25 | Loss: 0.00114413
Iteration 8/25 | Loss: 0.00114413
Iteration 9/25 | Loss: 0.00114413
Iteration 10/25 | Loss: 0.00114413
Iteration 11/25 | Loss: 0.00114413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011441295500844717, 0.0011441295500844717, 0.0011441295500844717, 0.0011441295500844717, 0.0011441295500844717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011441295500844717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18428874
Iteration 2/25 | Loss: 0.00266906
Iteration 3/25 | Loss: 0.00266906
Iteration 4/25 | Loss: 0.00266906
Iteration 5/25 | Loss: 0.00266906
Iteration 6/25 | Loss: 0.00266906
Iteration 7/25 | Loss: 0.00266906
Iteration 8/25 | Loss: 0.00266906
Iteration 9/25 | Loss: 0.00266906
Iteration 10/25 | Loss: 0.00266906
Iteration 11/25 | Loss: 0.00266906
Iteration 12/25 | Loss: 0.00266906
Iteration 13/25 | Loss: 0.00266906
Iteration 14/25 | Loss: 0.00266906
Iteration 15/25 | Loss: 0.00266906
Iteration 16/25 | Loss: 0.00266906
Iteration 17/25 | Loss: 0.00266906
Iteration 18/25 | Loss: 0.00266906
Iteration 19/25 | Loss: 0.00266906
Iteration 20/25 | Loss: 0.00266906
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0026690552476793528, 0.0026690552476793528, 0.0026690552476793528, 0.0026690552476793528, 0.0026690552476793528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026690552476793528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266906
Iteration 2/1000 | Loss: 0.00004073
Iteration 3/1000 | Loss: 0.00002821
Iteration 4/1000 | Loss: 0.00002260
Iteration 5/1000 | Loss: 0.00002046
Iteration 6/1000 | Loss: 0.00001842
Iteration 7/1000 | Loss: 0.00001728
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001631
Iteration 10/1000 | Loss: 0.00001599
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001545
Iteration 13/1000 | Loss: 0.00001524
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001500
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00001488
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001468
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001468
Iteration 28/1000 | Loss: 0.00001468
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001466
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001462
Iteration 36/1000 | Loss: 0.00001462
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001456
Iteration 46/1000 | Loss: 0.00001456
Iteration 47/1000 | Loss: 0.00001455
Iteration 48/1000 | Loss: 0.00001455
Iteration 49/1000 | Loss: 0.00001455
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001455
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001455
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001454
Iteration 62/1000 | Loss: 0.00001454
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001454
Iteration 65/1000 | Loss: 0.00001454
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001453
Iteration 69/1000 | Loss: 0.00001452
Iteration 70/1000 | Loss: 0.00001452
Iteration 71/1000 | Loss: 0.00001451
Iteration 72/1000 | Loss: 0.00001451
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001450
Iteration 77/1000 | Loss: 0.00001450
Iteration 78/1000 | Loss: 0.00001450
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001449
Iteration 82/1000 | Loss: 0.00001449
Iteration 83/1000 | Loss: 0.00001449
Iteration 84/1000 | Loss: 0.00001449
Iteration 85/1000 | Loss: 0.00001449
Iteration 86/1000 | Loss: 0.00001449
Iteration 87/1000 | Loss: 0.00001448
Iteration 88/1000 | Loss: 0.00001448
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001447
Iteration 91/1000 | Loss: 0.00001447
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001442
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001439
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001438
Iteration 127/1000 | Loss: 0.00001438
Iteration 128/1000 | Loss: 0.00001438
Iteration 129/1000 | Loss: 0.00001438
Iteration 130/1000 | Loss: 0.00001438
Iteration 131/1000 | Loss: 0.00001438
Iteration 132/1000 | Loss: 0.00001438
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001437
Iteration 137/1000 | Loss: 0.00001437
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001435
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001434
Iteration 156/1000 | Loss: 0.00001434
Iteration 157/1000 | Loss: 0.00001434
Iteration 158/1000 | Loss: 0.00001434
Iteration 159/1000 | Loss: 0.00001434
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001434
Iteration 162/1000 | Loss: 0.00001434
Iteration 163/1000 | Loss: 0.00001434
Iteration 164/1000 | Loss: 0.00001434
Iteration 165/1000 | Loss: 0.00001433
Iteration 166/1000 | Loss: 0.00001433
Iteration 167/1000 | Loss: 0.00001433
Iteration 168/1000 | Loss: 0.00001433
Iteration 169/1000 | Loss: 0.00001433
Iteration 170/1000 | Loss: 0.00001433
Iteration 171/1000 | Loss: 0.00001432
Iteration 172/1000 | Loss: 0.00001432
Iteration 173/1000 | Loss: 0.00001432
Iteration 174/1000 | Loss: 0.00001431
Iteration 175/1000 | Loss: 0.00001431
Iteration 176/1000 | Loss: 0.00001431
Iteration 177/1000 | Loss: 0.00001431
Iteration 178/1000 | Loss: 0.00001430
Iteration 179/1000 | Loss: 0.00001430
Iteration 180/1000 | Loss: 0.00001430
Iteration 181/1000 | Loss: 0.00001429
Iteration 182/1000 | Loss: 0.00001429
Iteration 183/1000 | Loss: 0.00001429
Iteration 184/1000 | Loss: 0.00001428
Iteration 185/1000 | Loss: 0.00001428
Iteration 186/1000 | Loss: 0.00001428
Iteration 187/1000 | Loss: 0.00001428
Iteration 188/1000 | Loss: 0.00001428
Iteration 189/1000 | Loss: 0.00001428
Iteration 190/1000 | Loss: 0.00001428
Iteration 191/1000 | Loss: 0.00001428
Iteration 192/1000 | Loss: 0.00001427
Iteration 193/1000 | Loss: 0.00001427
Iteration 194/1000 | Loss: 0.00001427
Iteration 195/1000 | Loss: 0.00001427
Iteration 196/1000 | Loss: 0.00001427
Iteration 197/1000 | Loss: 0.00001427
Iteration 198/1000 | Loss: 0.00001426
Iteration 199/1000 | Loss: 0.00001426
Iteration 200/1000 | Loss: 0.00001426
Iteration 201/1000 | Loss: 0.00001426
Iteration 202/1000 | Loss: 0.00001426
Iteration 203/1000 | Loss: 0.00001426
Iteration 204/1000 | Loss: 0.00001425
Iteration 205/1000 | Loss: 0.00001425
Iteration 206/1000 | Loss: 0.00001425
Iteration 207/1000 | Loss: 0.00001425
Iteration 208/1000 | Loss: 0.00001425
Iteration 209/1000 | Loss: 0.00001425
Iteration 210/1000 | Loss: 0.00001425
Iteration 211/1000 | Loss: 0.00001424
Iteration 212/1000 | Loss: 0.00001424
Iteration 213/1000 | Loss: 0.00001424
Iteration 214/1000 | Loss: 0.00001424
Iteration 215/1000 | Loss: 0.00001424
Iteration 216/1000 | Loss: 0.00001424
Iteration 217/1000 | Loss: 0.00001424
Iteration 218/1000 | Loss: 0.00001423
Iteration 219/1000 | Loss: 0.00001423
Iteration 220/1000 | Loss: 0.00001423
Iteration 221/1000 | Loss: 0.00001423
Iteration 222/1000 | Loss: 0.00001423
Iteration 223/1000 | Loss: 0.00001423
Iteration 224/1000 | Loss: 0.00001423
Iteration 225/1000 | Loss: 0.00001423
Iteration 226/1000 | Loss: 0.00001423
Iteration 227/1000 | Loss: 0.00001423
Iteration 228/1000 | Loss: 0.00001423
Iteration 229/1000 | Loss: 0.00001423
Iteration 230/1000 | Loss: 0.00001423
Iteration 231/1000 | Loss: 0.00001422
Iteration 232/1000 | Loss: 0.00001422
Iteration 233/1000 | Loss: 0.00001422
Iteration 234/1000 | Loss: 0.00001422
Iteration 235/1000 | Loss: 0.00001422
Iteration 236/1000 | Loss: 0.00001422
Iteration 237/1000 | Loss: 0.00001422
Iteration 238/1000 | Loss: 0.00001422
Iteration 239/1000 | Loss: 0.00001422
Iteration 240/1000 | Loss: 0.00001422
Iteration 241/1000 | Loss: 0.00001422
Iteration 242/1000 | Loss: 0.00001422
Iteration 243/1000 | Loss: 0.00001422
Iteration 244/1000 | Loss: 0.00001422
Iteration 245/1000 | Loss: 0.00001421
Iteration 246/1000 | Loss: 0.00001421
Iteration 247/1000 | Loss: 0.00001421
Iteration 248/1000 | Loss: 0.00001421
Iteration 249/1000 | Loss: 0.00001421
Iteration 250/1000 | Loss: 0.00001421
Iteration 251/1000 | Loss: 0.00001421
Iteration 252/1000 | Loss: 0.00001421
Iteration 253/1000 | Loss: 0.00001421
Iteration 254/1000 | Loss: 0.00001421
Iteration 255/1000 | Loss: 0.00001421
Iteration 256/1000 | Loss: 0.00001421
Iteration 257/1000 | Loss: 0.00001421
Iteration 258/1000 | Loss: 0.00001421
Iteration 259/1000 | Loss: 0.00001421
Iteration 260/1000 | Loss: 0.00001421
Iteration 261/1000 | Loss: 0.00001421
Iteration 262/1000 | Loss: 0.00001421
Iteration 263/1000 | Loss: 0.00001421
Iteration 264/1000 | Loss: 0.00001421
Iteration 265/1000 | Loss: 0.00001421
Iteration 266/1000 | Loss: 0.00001421
Iteration 267/1000 | Loss: 0.00001421
Iteration 268/1000 | Loss: 0.00001421
Iteration 269/1000 | Loss: 0.00001421
Iteration 270/1000 | Loss: 0.00001421
Iteration 271/1000 | Loss: 0.00001421
Iteration 272/1000 | Loss: 0.00001421
Iteration 273/1000 | Loss: 0.00001421
Iteration 274/1000 | Loss: 0.00001421
Iteration 275/1000 | Loss: 0.00001421
Iteration 276/1000 | Loss: 0.00001421
Iteration 277/1000 | Loss: 0.00001421
Iteration 278/1000 | Loss: 0.00001421
Iteration 279/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.4206117157300469e-05, 1.4206117157300469e-05, 1.4206117157300469e-05, 1.4206117157300469e-05, 1.4206117157300469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4206117157300469e-05

Optimization complete. Final v2v error: 3.1701385974884033 mm

Highest mean error: 3.9870245456695557 mm for frame 143

Lowest mean error: 2.5355684757232666 mm for frame 179

Saving results

Total time: 478.12968587875366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1091
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505564
Iteration 2/25 | Loss: 0.00130424
Iteration 3/25 | Loss: 0.00118646
Iteration 4/25 | Loss: 0.00116711
Iteration 5/25 | Loss: 0.00116245
Iteration 6/25 | Loss: 0.00116173
Iteration 7/25 | Loss: 0.00116075
Iteration 8/25 | Loss: 0.00115882
Iteration 9/25 | Loss: 0.00115877
Iteration 10/25 | Loss: 0.00115870
Iteration 11/25 | Loss: 0.00115870
Iteration 12/25 | Loss: 0.00115870
Iteration 13/25 | Loss: 0.00115870
Iteration 14/25 | Loss: 0.00115870
Iteration 15/25 | Loss: 0.00115870
Iteration 16/25 | Loss: 0.00115870
Iteration 17/25 | Loss: 0.00115869
Iteration 18/25 | Loss: 0.00115869
Iteration 19/25 | Loss: 0.00115869
Iteration 20/25 | Loss: 0.00115869
Iteration 21/25 | Loss: 0.00115869
Iteration 22/25 | Loss: 0.00115869
Iteration 23/25 | Loss: 0.00115869
Iteration 24/25 | Loss: 0.00115869
Iteration 25/25 | Loss: 0.00115869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.71415138
Iteration 2/25 | Loss: 0.00187873
Iteration 3/25 | Loss: 0.00187872
Iteration 4/25 | Loss: 0.00187872
Iteration 5/25 | Loss: 0.00187872
Iteration 6/25 | Loss: 0.00187872
Iteration 7/25 | Loss: 0.00187872
Iteration 8/25 | Loss: 0.00187872
Iteration 9/25 | Loss: 0.00187872
Iteration 10/25 | Loss: 0.00187872
Iteration 11/25 | Loss: 0.00187872
Iteration 12/25 | Loss: 0.00187872
Iteration 13/25 | Loss: 0.00187872
Iteration 14/25 | Loss: 0.00187872
Iteration 15/25 | Loss: 0.00187872
Iteration 16/25 | Loss: 0.00187872
Iteration 17/25 | Loss: 0.00187872
Iteration 18/25 | Loss: 0.00187872
Iteration 19/25 | Loss: 0.00187872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018787155859172344, 0.0018787155859172344, 0.0018787155859172344, 0.0018787155859172344, 0.0018787155859172344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018787155859172344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187872
Iteration 2/1000 | Loss: 0.00002271
Iteration 3/1000 | Loss: 0.00005726
Iteration 4/1000 | Loss: 0.00001497
Iteration 5/1000 | Loss: 0.00001394
Iteration 6/1000 | Loss: 0.00004654
Iteration 7/1000 | Loss: 0.00009803
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00005004
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001133
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001110
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001103
Iteration 24/1000 | Loss: 0.00006161
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001092
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001087
Iteration 29/1000 | Loss: 0.00001086
Iteration 30/1000 | Loss: 0.00001085
Iteration 31/1000 | Loss: 0.00001084
Iteration 32/1000 | Loss: 0.00001084
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001084
Iteration 36/1000 | Loss: 0.00001084
Iteration 37/1000 | Loss: 0.00001083
Iteration 38/1000 | Loss: 0.00001083
Iteration 39/1000 | Loss: 0.00001083
Iteration 40/1000 | Loss: 0.00001082
Iteration 41/1000 | Loss: 0.00001082
Iteration 42/1000 | Loss: 0.00001080
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001079
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001078
Iteration 47/1000 | Loss: 0.00001078
Iteration 48/1000 | Loss: 0.00001077
Iteration 49/1000 | Loss: 0.00001077
Iteration 50/1000 | Loss: 0.00001076
Iteration 51/1000 | Loss: 0.00001076
Iteration 52/1000 | Loss: 0.00001075
Iteration 53/1000 | Loss: 0.00001075
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001074
Iteration 56/1000 | Loss: 0.00001074
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001073
Iteration 61/1000 | Loss: 0.00001072
Iteration 62/1000 | Loss: 0.00001072
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001071
Iteration 65/1000 | Loss: 0.00001071
Iteration 66/1000 | Loss: 0.00001071
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001071
Iteration 69/1000 | Loss: 0.00001070
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001070
Iteration 73/1000 | Loss: 0.00001070
Iteration 74/1000 | Loss: 0.00001069
Iteration 75/1000 | Loss: 0.00001069
Iteration 76/1000 | Loss: 0.00001068
Iteration 77/1000 | Loss: 0.00001068
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001068
Iteration 80/1000 | Loss: 0.00001067
Iteration 81/1000 | Loss: 0.00001067
Iteration 82/1000 | Loss: 0.00001067
Iteration 83/1000 | Loss: 0.00001067
Iteration 84/1000 | Loss: 0.00001067
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001066
Iteration 87/1000 | Loss: 0.00001066
Iteration 88/1000 | Loss: 0.00001066
Iteration 89/1000 | Loss: 0.00003302
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001132
Iteration 92/1000 | Loss: 0.00001065
Iteration 93/1000 | Loss: 0.00001065
Iteration 94/1000 | Loss: 0.00001065
Iteration 95/1000 | Loss: 0.00001065
Iteration 96/1000 | Loss: 0.00001065
Iteration 97/1000 | Loss: 0.00001065
Iteration 98/1000 | Loss: 0.00001065
Iteration 99/1000 | Loss: 0.00001065
Iteration 100/1000 | Loss: 0.00001065
Iteration 101/1000 | Loss: 0.00001065
Iteration 102/1000 | Loss: 0.00001064
Iteration 103/1000 | Loss: 0.00001064
Iteration 104/1000 | Loss: 0.00001064
Iteration 105/1000 | Loss: 0.00001064
Iteration 106/1000 | Loss: 0.00001064
Iteration 107/1000 | Loss: 0.00001064
Iteration 108/1000 | Loss: 0.00001064
Iteration 109/1000 | Loss: 0.00001064
Iteration 110/1000 | Loss: 0.00001064
Iteration 111/1000 | Loss: 0.00001064
Iteration 112/1000 | Loss: 0.00001064
Iteration 113/1000 | Loss: 0.00001063
Iteration 114/1000 | Loss: 0.00001063
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001062
Iteration 118/1000 | Loss: 0.00001062
Iteration 119/1000 | Loss: 0.00001062
Iteration 120/1000 | Loss: 0.00001061
Iteration 121/1000 | Loss: 0.00001061
Iteration 122/1000 | Loss: 0.00001061
Iteration 123/1000 | Loss: 0.00001061
Iteration 124/1000 | Loss: 0.00001061
Iteration 125/1000 | Loss: 0.00001061
Iteration 126/1000 | Loss: 0.00001061
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001060
Iteration 130/1000 | Loss: 0.00001060
Iteration 131/1000 | Loss: 0.00001060
Iteration 132/1000 | Loss: 0.00001059
Iteration 133/1000 | Loss: 0.00001059
Iteration 134/1000 | Loss: 0.00001059
Iteration 135/1000 | Loss: 0.00001059
Iteration 136/1000 | Loss: 0.00001059
Iteration 137/1000 | Loss: 0.00001059
Iteration 138/1000 | Loss: 0.00001059
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001058
Iteration 141/1000 | Loss: 0.00001058
Iteration 142/1000 | Loss: 0.00001058
Iteration 143/1000 | Loss: 0.00001058
Iteration 144/1000 | Loss: 0.00001058
Iteration 145/1000 | Loss: 0.00001058
Iteration 146/1000 | Loss: 0.00001058
Iteration 147/1000 | Loss: 0.00001058
Iteration 148/1000 | Loss: 0.00001057
Iteration 149/1000 | Loss: 0.00001057
Iteration 150/1000 | Loss: 0.00001057
Iteration 151/1000 | Loss: 0.00001057
Iteration 152/1000 | Loss: 0.00001057
Iteration 153/1000 | Loss: 0.00001057
Iteration 154/1000 | Loss: 0.00001057
Iteration 155/1000 | Loss: 0.00001056
Iteration 156/1000 | Loss: 0.00001056
Iteration 157/1000 | Loss: 0.00001056
Iteration 158/1000 | Loss: 0.00001056
Iteration 159/1000 | Loss: 0.00001056
Iteration 160/1000 | Loss: 0.00001056
Iteration 161/1000 | Loss: 0.00001056
Iteration 162/1000 | Loss: 0.00001056
Iteration 163/1000 | Loss: 0.00001055
Iteration 164/1000 | Loss: 0.00001055
Iteration 165/1000 | Loss: 0.00001055
Iteration 166/1000 | Loss: 0.00001055
Iteration 167/1000 | Loss: 0.00001055
Iteration 168/1000 | Loss: 0.00001054
Iteration 169/1000 | Loss: 0.00001054
Iteration 170/1000 | Loss: 0.00001054
Iteration 171/1000 | Loss: 0.00001054
Iteration 172/1000 | Loss: 0.00001054
Iteration 173/1000 | Loss: 0.00001054
Iteration 174/1000 | Loss: 0.00001054
Iteration 175/1000 | Loss: 0.00001054
Iteration 176/1000 | Loss: 0.00001053
Iteration 177/1000 | Loss: 0.00001053
Iteration 178/1000 | Loss: 0.00001053
Iteration 179/1000 | Loss: 0.00001053
Iteration 180/1000 | Loss: 0.00001053
Iteration 181/1000 | Loss: 0.00001053
Iteration 182/1000 | Loss: 0.00001053
Iteration 183/1000 | Loss: 0.00001053
Iteration 184/1000 | Loss: 0.00001053
Iteration 185/1000 | Loss: 0.00001053
Iteration 186/1000 | Loss: 0.00001053
Iteration 187/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.0526254300202709e-05, 1.0526254300202709e-05, 1.0526254300202709e-05, 1.0526254300202709e-05, 1.0526254300202709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0526254300202709e-05

Optimization complete. Final v2v error: 2.76527738571167 mm

Highest mean error: 3.5145630836486816 mm for frame 200

Lowest mean error: 2.5154900550842285 mm for frame 181

Saving results

Total time: 620.0128374099731
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1007
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809424
Iteration 2/25 | Loss: 0.00157384
Iteration 3/25 | Loss: 0.00128511
Iteration 4/25 | Loss: 0.00123897
Iteration 5/25 | Loss: 0.00122893
Iteration 6/25 | Loss: 0.00122713
Iteration 7/25 | Loss: 0.00122713
Iteration 8/25 | Loss: 0.00122713
Iteration 9/25 | Loss: 0.00122713
Iteration 10/25 | Loss: 0.00122713
Iteration 11/25 | Loss: 0.00122713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012271306477487087, 0.0012271306477487087, 0.0012271306477487087, 0.0012271306477487087, 0.0012271306477487087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012271306477487087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22259104
Iteration 2/25 | Loss: 0.00184654
Iteration 3/25 | Loss: 0.00184654
Iteration 4/25 | Loss: 0.00184654
Iteration 5/25 | Loss: 0.00184654
Iteration 6/25 | Loss: 0.00184654
Iteration 7/25 | Loss: 0.00184654
Iteration 8/25 | Loss: 0.00184654
Iteration 9/25 | Loss: 0.00184654
Iteration 10/25 | Loss: 0.00184654
Iteration 11/25 | Loss: 0.00184654
Iteration 12/25 | Loss: 0.00184654
Iteration 13/25 | Loss: 0.00184654
Iteration 14/25 | Loss: 0.00184654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001846541534177959, 0.001846541534177959, 0.001846541534177959, 0.001846541534177959, 0.001846541534177959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001846541534177959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184654
Iteration 2/1000 | Loss: 0.00003198
Iteration 3/1000 | Loss: 0.00002242
Iteration 4/1000 | Loss: 0.00002088
Iteration 5/1000 | Loss: 0.00001996
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001883
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001807
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001749
Iteration 15/1000 | Loss: 0.00001740
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001725
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001709
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001708
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001705
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001703
Iteration 48/1000 | Loss: 0.00001703
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001702
Iteration 52/1000 | Loss: 0.00001702
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.7018151993397623e-05, 1.7018151993397623e-05, 1.7018151993397623e-05, 1.7018151993397623e-05, 1.7018151993397623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7018151993397623e-05

Optimization complete. Final v2v error: 3.4978854656219482 mm

Highest mean error: 4.125682830810547 mm for frame 217

Lowest mean error: 3.063807487487793 mm for frame 5

Saving results

Total time: 351.58170557022095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1029
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339233
Iteration 2/25 | Loss: 0.00147851
Iteration 3/25 | Loss: 0.00124487
Iteration 4/25 | Loss: 0.00117403
Iteration 5/25 | Loss: 0.00116277
Iteration 6/25 | Loss: 0.00115881
Iteration 7/25 | Loss: 0.00115667
Iteration 8/25 | Loss: 0.00115578
Iteration 9/25 | Loss: 0.00115792
Iteration 10/25 | Loss: 0.00115511
Iteration 11/25 | Loss: 0.00115464
Iteration 12/25 | Loss: 0.00115318
Iteration 13/25 | Loss: 0.00115230
Iteration 14/25 | Loss: 0.00115223
Iteration 15/25 | Loss: 0.00115223
Iteration 16/25 | Loss: 0.00115223
Iteration 17/25 | Loss: 0.00115222
Iteration 18/25 | Loss: 0.00115222
Iteration 19/25 | Loss: 0.00115222
Iteration 20/25 | Loss: 0.00115222
Iteration 21/25 | Loss: 0.00115222
Iteration 22/25 | Loss: 0.00115222
Iteration 23/25 | Loss: 0.00115222
Iteration 24/25 | Loss: 0.00115222
Iteration 25/25 | Loss: 0.00115222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24136198
Iteration 2/25 | Loss: 0.00247447
Iteration 3/25 | Loss: 0.00247447
Iteration 4/25 | Loss: 0.00247447
Iteration 5/25 | Loss: 0.00247447
Iteration 6/25 | Loss: 0.00247447
Iteration 7/25 | Loss: 0.00247447
Iteration 8/25 | Loss: 0.00247447
Iteration 9/25 | Loss: 0.00247447
Iteration 10/25 | Loss: 0.00247447
Iteration 11/25 | Loss: 0.00247447
Iteration 12/25 | Loss: 0.00247447
Iteration 13/25 | Loss: 0.00247447
Iteration 14/25 | Loss: 0.00247447
Iteration 15/25 | Loss: 0.00247447
Iteration 16/25 | Loss: 0.00247447
Iteration 17/25 | Loss: 0.00247447
Iteration 18/25 | Loss: 0.00247447
Iteration 19/25 | Loss: 0.00247447
Iteration 20/25 | Loss: 0.00247447
Iteration 21/25 | Loss: 0.00247447
Iteration 22/25 | Loss: 0.00247447
Iteration 23/25 | Loss: 0.00247447
Iteration 24/25 | Loss: 0.00247447
Iteration 25/25 | Loss: 0.00247447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247447
Iteration 2/1000 | Loss: 0.00004179
Iteration 3/1000 | Loss: 0.00002461
Iteration 4/1000 | Loss: 0.00001834
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001561
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001340
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001286
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001276
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001274
Iteration 25/1000 | Loss: 0.00001273
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001272
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001264
Iteration 34/1000 | Loss: 0.00001264
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001262
Iteration 39/1000 | Loss: 0.00001261
Iteration 40/1000 | Loss: 0.00001261
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001260
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001257
Iteration 50/1000 | Loss: 0.00001257
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001256
Iteration 53/1000 | Loss: 0.00001256
Iteration 54/1000 | Loss: 0.00001256
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001255
Iteration 59/1000 | Loss: 0.00001255
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001254
Iteration 64/1000 | Loss: 0.00001254
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001253
Iteration 67/1000 | Loss: 0.00001253
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001252
Iteration 72/1000 | Loss: 0.00001252
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001250
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001250
Iteration 85/1000 | Loss: 0.00001250
Iteration 86/1000 | Loss: 0.00001249
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001249
Iteration 90/1000 | Loss: 0.00001249
Iteration 91/1000 | Loss: 0.00001249
Iteration 92/1000 | Loss: 0.00001249
Iteration 93/1000 | Loss: 0.00001249
Iteration 94/1000 | Loss: 0.00001248
Iteration 95/1000 | Loss: 0.00001248
Iteration 96/1000 | Loss: 0.00001248
Iteration 97/1000 | Loss: 0.00001247
Iteration 98/1000 | Loss: 0.00001247
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001246
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001245
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001245
Iteration 114/1000 | Loss: 0.00001245
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001243
Iteration 125/1000 | Loss: 0.00001243
Iteration 126/1000 | Loss: 0.00001243
Iteration 127/1000 | Loss: 0.00001243
Iteration 128/1000 | Loss: 0.00001243
Iteration 129/1000 | Loss: 0.00001243
Iteration 130/1000 | Loss: 0.00001243
Iteration 131/1000 | Loss: 0.00001243
Iteration 132/1000 | Loss: 0.00001243
Iteration 133/1000 | Loss: 0.00001243
Iteration 134/1000 | Loss: 0.00001243
Iteration 135/1000 | Loss: 0.00001242
Iteration 136/1000 | Loss: 0.00001242
Iteration 137/1000 | Loss: 0.00001242
Iteration 138/1000 | Loss: 0.00001241
Iteration 139/1000 | Loss: 0.00001241
Iteration 140/1000 | Loss: 0.00001241
Iteration 141/1000 | Loss: 0.00001241
Iteration 142/1000 | Loss: 0.00001241
Iteration 143/1000 | Loss: 0.00001241
Iteration 144/1000 | Loss: 0.00001241
Iteration 145/1000 | Loss: 0.00001241
Iteration 146/1000 | Loss: 0.00001241
Iteration 147/1000 | Loss: 0.00001241
Iteration 148/1000 | Loss: 0.00001241
Iteration 149/1000 | Loss: 0.00001241
Iteration 150/1000 | Loss: 0.00001241
Iteration 151/1000 | Loss: 0.00001240
Iteration 152/1000 | Loss: 0.00001240
Iteration 153/1000 | Loss: 0.00001240
Iteration 154/1000 | Loss: 0.00001240
Iteration 155/1000 | Loss: 0.00001240
Iteration 156/1000 | Loss: 0.00001240
Iteration 157/1000 | Loss: 0.00001239
Iteration 158/1000 | Loss: 0.00001239
Iteration 159/1000 | Loss: 0.00001239
Iteration 160/1000 | Loss: 0.00001239
Iteration 161/1000 | Loss: 0.00001239
Iteration 162/1000 | Loss: 0.00001239
Iteration 163/1000 | Loss: 0.00001239
Iteration 164/1000 | Loss: 0.00001239
Iteration 165/1000 | Loss: 0.00001238
Iteration 166/1000 | Loss: 0.00001238
Iteration 167/1000 | Loss: 0.00001238
Iteration 168/1000 | Loss: 0.00001238
Iteration 169/1000 | Loss: 0.00001238
Iteration 170/1000 | Loss: 0.00001238
Iteration 171/1000 | Loss: 0.00001238
Iteration 172/1000 | Loss: 0.00001238
Iteration 173/1000 | Loss: 0.00001238
Iteration 174/1000 | Loss: 0.00001237
Iteration 175/1000 | Loss: 0.00001237
Iteration 176/1000 | Loss: 0.00001237
Iteration 177/1000 | Loss: 0.00001237
Iteration 178/1000 | Loss: 0.00001237
Iteration 179/1000 | Loss: 0.00001237
Iteration 180/1000 | Loss: 0.00001237
Iteration 181/1000 | Loss: 0.00001237
Iteration 182/1000 | Loss: 0.00001237
Iteration 183/1000 | Loss: 0.00001237
Iteration 184/1000 | Loss: 0.00001237
Iteration 185/1000 | Loss: 0.00001237
Iteration 186/1000 | Loss: 0.00001236
Iteration 187/1000 | Loss: 0.00001236
Iteration 188/1000 | Loss: 0.00001236
Iteration 189/1000 | Loss: 0.00001236
Iteration 190/1000 | Loss: 0.00001236
Iteration 191/1000 | Loss: 0.00001236
Iteration 192/1000 | Loss: 0.00001236
Iteration 193/1000 | Loss: 0.00001236
Iteration 194/1000 | Loss: 0.00001236
Iteration 195/1000 | Loss: 0.00001236
Iteration 196/1000 | Loss: 0.00001236
Iteration 197/1000 | Loss: 0.00001236
Iteration 198/1000 | Loss: 0.00001236
Iteration 199/1000 | Loss: 0.00001235
Iteration 200/1000 | Loss: 0.00001235
Iteration 201/1000 | Loss: 0.00001235
Iteration 202/1000 | Loss: 0.00001235
Iteration 203/1000 | Loss: 0.00001235
Iteration 204/1000 | Loss: 0.00001235
Iteration 205/1000 | Loss: 0.00001235
Iteration 206/1000 | Loss: 0.00001235
Iteration 207/1000 | Loss: 0.00001235
Iteration 208/1000 | Loss: 0.00001235
Iteration 209/1000 | Loss: 0.00001235
Iteration 210/1000 | Loss: 0.00001235
Iteration 211/1000 | Loss: 0.00001234
Iteration 212/1000 | Loss: 0.00001234
Iteration 213/1000 | Loss: 0.00001234
Iteration 214/1000 | Loss: 0.00001234
Iteration 215/1000 | Loss: 0.00001234
Iteration 216/1000 | Loss: 0.00001234
Iteration 217/1000 | Loss: 0.00001234
Iteration 218/1000 | Loss: 0.00001234
Iteration 219/1000 | Loss: 0.00001234
Iteration 220/1000 | Loss: 0.00001234
Iteration 221/1000 | Loss: 0.00001234
Iteration 222/1000 | Loss: 0.00001234
Iteration 223/1000 | Loss: 0.00001234
Iteration 224/1000 | Loss: 0.00001234
Iteration 225/1000 | Loss: 0.00001234
Iteration 226/1000 | Loss: 0.00001233
Iteration 227/1000 | Loss: 0.00001233
Iteration 228/1000 | Loss: 0.00001233
Iteration 229/1000 | Loss: 0.00001233
Iteration 230/1000 | Loss: 0.00001233
Iteration 231/1000 | Loss: 0.00001233
Iteration 232/1000 | Loss: 0.00001233
Iteration 233/1000 | Loss: 0.00001233
Iteration 234/1000 | Loss: 0.00001233
Iteration 235/1000 | Loss: 0.00001233
Iteration 236/1000 | Loss: 0.00001233
Iteration 237/1000 | Loss: 0.00001233
Iteration 238/1000 | Loss: 0.00001233
Iteration 239/1000 | Loss: 0.00001233
Iteration 240/1000 | Loss: 0.00001232
Iteration 241/1000 | Loss: 0.00001232
Iteration 242/1000 | Loss: 0.00001232
Iteration 243/1000 | Loss: 0.00001232
Iteration 244/1000 | Loss: 0.00001232
Iteration 245/1000 | Loss: 0.00001232
Iteration 246/1000 | Loss: 0.00001232
Iteration 247/1000 | Loss: 0.00001232
Iteration 248/1000 | Loss: 0.00001232
Iteration 249/1000 | Loss: 0.00001232
Iteration 250/1000 | Loss: 0.00001232
Iteration 251/1000 | Loss: 0.00001231
Iteration 252/1000 | Loss: 0.00001231
Iteration 253/1000 | Loss: 0.00001231
Iteration 254/1000 | Loss: 0.00001231
Iteration 255/1000 | Loss: 0.00001231
Iteration 256/1000 | Loss: 0.00001231
Iteration 257/1000 | Loss: 0.00001231
Iteration 258/1000 | Loss: 0.00001231
Iteration 259/1000 | Loss: 0.00001231
Iteration 260/1000 | Loss: 0.00001231
Iteration 261/1000 | Loss: 0.00001231
Iteration 262/1000 | Loss: 0.00001230
Iteration 263/1000 | Loss: 0.00001230
Iteration 264/1000 | Loss: 0.00001230
Iteration 265/1000 | Loss: 0.00001230
Iteration 266/1000 | Loss: 0.00001230
Iteration 267/1000 | Loss: 0.00001229
Iteration 268/1000 | Loss: 0.00001229
Iteration 269/1000 | Loss: 0.00001229
Iteration 270/1000 | Loss: 0.00001229
Iteration 271/1000 | Loss: 0.00001229
Iteration 272/1000 | Loss: 0.00001229
Iteration 273/1000 | Loss: 0.00001229
Iteration 274/1000 | Loss: 0.00001229
Iteration 275/1000 | Loss: 0.00001228
Iteration 276/1000 | Loss: 0.00001228
Iteration 277/1000 | Loss: 0.00001228
Iteration 278/1000 | Loss: 0.00001228
Iteration 279/1000 | Loss: 0.00001228
Iteration 280/1000 | Loss: 0.00001228
Iteration 281/1000 | Loss: 0.00001228
Iteration 282/1000 | Loss: 0.00001228
Iteration 283/1000 | Loss: 0.00001228
Iteration 284/1000 | Loss: 0.00001227
Iteration 285/1000 | Loss: 0.00001227
Iteration 286/1000 | Loss: 0.00001227
Iteration 287/1000 | Loss: 0.00001227
Iteration 288/1000 | Loss: 0.00001227
Iteration 289/1000 | Loss: 0.00001227
Iteration 290/1000 | Loss: 0.00001227
Iteration 291/1000 | Loss: 0.00001227
Iteration 292/1000 | Loss: 0.00001226
Iteration 293/1000 | Loss: 0.00001226
Iteration 294/1000 | Loss: 0.00001226
Iteration 295/1000 | Loss: 0.00001226
Iteration 296/1000 | Loss: 0.00001226
Iteration 297/1000 | Loss: 0.00001226
Iteration 298/1000 | Loss: 0.00001225
Iteration 299/1000 | Loss: 0.00001225
Iteration 300/1000 | Loss: 0.00001225
Iteration 301/1000 | Loss: 0.00001225
Iteration 302/1000 | Loss: 0.00001225
Iteration 303/1000 | Loss: 0.00001225
Iteration 304/1000 | Loss: 0.00001225
Iteration 305/1000 | Loss: 0.00001225
Iteration 306/1000 | Loss: 0.00001225
Iteration 307/1000 | Loss: 0.00001225
Iteration 308/1000 | Loss: 0.00001225
Iteration 309/1000 | Loss: 0.00001225
Iteration 310/1000 | Loss: 0.00001225
Iteration 311/1000 | Loss: 0.00001224
Iteration 312/1000 | Loss: 0.00001224
Iteration 313/1000 | Loss: 0.00001224
Iteration 314/1000 | Loss: 0.00001224
Iteration 315/1000 | Loss: 0.00001224
Iteration 316/1000 | Loss: 0.00001224
Iteration 317/1000 | Loss: 0.00001224
Iteration 318/1000 | Loss: 0.00001224
Iteration 319/1000 | Loss: 0.00001224
Iteration 320/1000 | Loss: 0.00001224
Iteration 321/1000 | Loss: 0.00001224
Iteration 322/1000 | Loss: 0.00001224
Iteration 323/1000 | Loss: 0.00001224
Iteration 324/1000 | Loss: 0.00001224
Iteration 325/1000 | Loss: 0.00001224
Iteration 326/1000 | Loss: 0.00001224
Iteration 327/1000 | Loss: 0.00001224
Iteration 328/1000 | Loss: 0.00001224
Iteration 329/1000 | Loss: 0.00001223
Iteration 330/1000 | Loss: 0.00001223
Iteration 331/1000 | Loss: 0.00001223
Iteration 332/1000 | Loss: 0.00001223
Iteration 333/1000 | Loss: 0.00001223
Iteration 334/1000 | Loss: 0.00001223
Iteration 335/1000 | Loss: 0.00001223
Iteration 336/1000 | Loss: 0.00001223
Iteration 337/1000 | Loss: 0.00001223
Iteration 338/1000 | Loss: 0.00001223
Iteration 339/1000 | Loss: 0.00001223
Iteration 340/1000 | Loss: 0.00001223
Iteration 341/1000 | Loss: 0.00001223
Iteration 342/1000 | Loss: 0.00001223
Iteration 343/1000 | Loss: 0.00001223
Iteration 344/1000 | Loss: 0.00001223
Iteration 345/1000 | Loss: 0.00001223
Iteration 346/1000 | Loss: 0.00001223
Iteration 347/1000 | Loss: 0.00001223
Iteration 348/1000 | Loss: 0.00001223
Iteration 349/1000 | Loss: 0.00001223
Iteration 350/1000 | Loss: 0.00001223
Iteration 351/1000 | Loss: 0.00001223
Iteration 352/1000 | Loss: 0.00001222
Iteration 353/1000 | Loss: 0.00001222
Iteration 354/1000 | Loss: 0.00001222
Iteration 355/1000 | Loss: 0.00001222
Iteration 356/1000 | Loss: 0.00001222
Iteration 357/1000 | Loss: 0.00001222
Iteration 358/1000 | Loss: 0.00001222
Iteration 359/1000 | Loss: 0.00001222
Iteration 360/1000 | Loss: 0.00001222
Iteration 361/1000 | Loss: 0.00001222
Iteration 362/1000 | Loss: 0.00001222
Iteration 363/1000 | Loss: 0.00001222
Iteration 364/1000 | Loss: 0.00001222
Iteration 365/1000 | Loss: 0.00001222
Iteration 366/1000 | Loss: 0.00001222
Iteration 367/1000 | Loss: 0.00001222
Iteration 368/1000 | Loss: 0.00001222
Iteration 369/1000 | Loss: 0.00001222
Iteration 370/1000 | Loss: 0.00001222
Iteration 371/1000 | Loss: 0.00001222
Iteration 372/1000 | Loss: 0.00001222
Iteration 373/1000 | Loss: 0.00001222
Iteration 374/1000 | Loss: 0.00001222
Iteration 375/1000 | Loss: 0.00001222
Iteration 376/1000 | Loss: 0.00001222
Iteration 377/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 377. Stopping optimization.
Last 5 losses: [1.2219920790812466e-05, 1.2219920790812466e-05, 1.2219920790812466e-05, 1.2219920790812466e-05, 1.2219920790812466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2219920790812466e-05

Optimization complete. Final v2v error: 2.9718239307403564 mm

Highest mean error: 3.7947640419006348 mm for frame 70

Lowest mean error: 2.4248719215393066 mm for frame 151

Saving results

Total time: 674.4471943378448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1033
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403109
Iteration 2/25 | Loss: 0.00122139
Iteration 3/25 | Loss: 0.00114774
Iteration 4/25 | Loss: 0.00113651
Iteration 5/25 | Loss: 0.00113301
Iteration 6/25 | Loss: 0.00113204
Iteration 7/25 | Loss: 0.00113204
Iteration 8/25 | Loss: 0.00113204
Iteration 9/25 | Loss: 0.00113204
Iteration 10/25 | Loss: 0.00113204
Iteration 11/25 | Loss: 0.00113204
Iteration 12/25 | Loss: 0.00113204
Iteration 13/25 | Loss: 0.00113204
Iteration 14/25 | Loss: 0.00113204
Iteration 15/25 | Loss: 0.00113204
Iteration 16/25 | Loss: 0.00113204
Iteration 17/25 | Loss: 0.00113204
Iteration 18/25 | Loss: 0.00113204
Iteration 19/25 | Loss: 0.00113204
Iteration 20/25 | Loss: 0.00113204
Iteration 21/25 | Loss: 0.00113204
Iteration 22/25 | Loss: 0.00113204
Iteration 23/25 | Loss: 0.00113204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011320417979732156, 0.0011320417979732156, 0.0011320417979732156, 0.0011320417979732156, 0.0011320417979732156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011320417979732156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73374844
Iteration 2/25 | Loss: 0.00196460
Iteration 3/25 | Loss: 0.00196460
Iteration 4/25 | Loss: 0.00196460
Iteration 5/25 | Loss: 0.00196460
Iteration 6/25 | Loss: 0.00196460
Iteration 7/25 | Loss: 0.00196460
Iteration 8/25 | Loss: 0.00196460
Iteration 9/25 | Loss: 0.00196460
Iteration 10/25 | Loss: 0.00196460
Iteration 11/25 | Loss: 0.00196460
Iteration 12/25 | Loss: 0.00196460
Iteration 13/25 | Loss: 0.00196460
Iteration 14/25 | Loss: 0.00196460
Iteration 15/25 | Loss: 0.00196460
Iteration 16/25 | Loss: 0.00196460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001964595867320895, 0.001964595867320895, 0.001964595867320895, 0.001964595867320895, 0.001964595867320895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001964595867320895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196460
Iteration 2/1000 | Loss: 0.00001810
Iteration 3/1000 | Loss: 0.00001338
Iteration 4/1000 | Loss: 0.00001169
Iteration 5/1000 | Loss: 0.00001069
Iteration 6/1000 | Loss: 0.00001012
Iteration 7/1000 | Loss: 0.00000975
Iteration 8/1000 | Loss: 0.00000937
Iteration 9/1000 | Loss: 0.00000922
Iteration 10/1000 | Loss: 0.00000901
Iteration 11/1000 | Loss: 0.00000887
Iteration 12/1000 | Loss: 0.00000886
Iteration 13/1000 | Loss: 0.00000869
Iteration 14/1000 | Loss: 0.00000866
Iteration 15/1000 | Loss: 0.00000863
Iteration 16/1000 | Loss: 0.00000863
Iteration 17/1000 | Loss: 0.00000848
Iteration 18/1000 | Loss: 0.00000847
Iteration 19/1000 | Loss: 0.00000837
Iteration 20/1000 | Loss: 0.00000833
Iteration 21/1000 | Loss: 0.00000833
Iteration 22/1000 | Loss: 0.00000832
Iteration 23/1000 | Loss: 0.00000832
Iteration 24/1000 | Loss: 0.00000831
Iteration 25/1000 | Loss: 0.00000830
Iteration 26/1000 | Loss: 0.00000830
Iteration 27/1000 | Loss: 0.00000829
Iteration 28/1000 | Loss: 0.00000829
Iteration 29/1000 | Loss: 0.00000827
Iteration 30/1000 | Loss: 0.00000827
Iteration 31/1000 | Loss: 0.00000825
Iteration 32/1000 | Loss: 0.00000824
Iteration 33/1000 | Loss: 0.00000823
Iteration 34/1000 | Loss: 0.00000822
Iteration 35/1000 | Loss: 0.00000822
Iteration 36/1000 | Loss: 0.00000822
Iteration 37/1000 | Loss: 0.00000820
Iteration 38/1000 | Loss: 0.00000814
Iteration 39/1000 | Loss: 0.00000806
Iteration 40/1000 | Loss: 0.00000806
Iteration 41/1000 | Loss: 0.00000805
Iteration 42/1000 | Loss: 0.00000805
Iteration 43/1000 | Loss: 0.00000805
Iteration 44/1000 | Loss: 0.00000805
Iteration 45/1000 | Loss: 0.00000805
Iteration 46/1000 | Loss: 0.00000805
Iteration 47/1000 | Loss: 0.00000805
Iteration 48/1000 | Loss: 0.00000804
Iteration 49/1000 | Loss: 0.00000804
Iteration 50/1000 | Loss: 0.00000804
Iteration 51/1000 | Loss: 0.00000804
Iteration 52/1000 | Loss: 0.00000804
Iteration 53/1000 | Loss: 0.00000804
Iteration 54/1000 | Loss: 0.00000803
Iteration 55/1000 | Loss: 0.00000803
Iteration 56/1000 | Loss: 0.00000802
Iteration 57/1000 | Loss: 0.00000802
Iteration 58/1000 | Loss: 0.00000801
Iteration 59/1000 | Loss: 0.00000801
Iteration 60/1000 | Loss: 0.00000801
Iteration 61/1000 | Loss: 0.00000801
Iteration 62/1000 | Loss: 0.00000800
Iteration 63/1000 | Loss: 0.00000800
Iteration 64/1000 | Loss: 0.00000800
Iteration 65/1000 | Loss: 0.00000800
Iteration 66/1000 | Loss: 0.00000799
Iteration 67/1000 | Loss: 0.00000799
Iteration 68/1000 | Loss: 0.00000799
Iteration 69/1000 | Loss: 0.00000798
Iteration 70/1000 | Loss: 0.00000798
Iteration 71/1000 | Loss: 0.00000798
Iteration 72/1000 | Loss: 0.00000798
Iteration 73/1000 | Loss: 0.00000798
Iteration 74/1000 | Loss: 0.00000798
Iteration 75/1000 | Loss: 0.00000798
Iteration 76/1000 | Loss: 0.00000798
Iteration 77/1000 | Loss: 0.00000798
Iteration 78/1000 | Loss: 0.00000798
Iteration 79/1000 | Loss: 0.00000798
Iteration 80/1000 | Loss: 0.00000797
Iteration 81/1000 | Loss: 0.00000797
Iteration 82/1000 | Loss: 0.00000797
Iteration 83/1000 | Loss: 0.00000796
Iteration 84/1000 | Loss: 0.00000796
Iteration 85/1000 | Loss: 0.00000796
Iteration 86/1000 | Loss: 0.00000796
Iteration 87/1000 | Loss: 0.00000796
Iteration 88/1000 | Loss: 0.00000796
Iteration 89/1000 | Loss: 0.00000795
Iteration 90/1000 | Loss: 0.00000795
Iteration 91/1000 | Loss: 0.00000795
Iteration 92/1000 | Loss: 0.00000795
Iteration 93/1000 | Loss: 0.00000795
Iteration 94/1000 | Loss: 0.00000795
Iteration 95/1000 | Loss: 0.00000795
Iteration 96/1000 | Loss: 0.00000795
Iteration 97/1000 | Loss: 0.00000795
Iteration 98/1000 | Loss: 0.00000795
Iteration 99/1000 | Loss: 0.00000795
Iteration 100/1000 | Loss: 0.00000795
Iteration 101/1000 | Loss: 0.00000795
Iteration 102/1000 | Loss: 0.00000795
Iteration 103/1000 | Loss: 0.00000795
Iteration 104/1000 | Loss: 0.00000795
Iteration 105/1000 | Loss: 0.00000795
Iteration 106/1000 | Loss: 0.00000795
Iteration 107/1000 | Loss: 0.00000795
Iteration 108/1000 | Loss: 0.00000795
Iteration 109/1000 | Loss: 0.00000795
Iteration 110/1000 | Loss: 0.00000795
Iteration 111/1000 | Loss: 0.00000795
Iteration 112/1000 | Loss: 0.00000795
Iteration 113/1000 | Loss: 0.00000795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [7.951887710078154e-06, 7.951887710078154e-06, 7.951887710078154e-06, 7.951887710078154e-06, 7.951887710078154e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.951887710078154e-06

Optimization complete. Final v2v error: 2.4705605506896973 mm

Highest mean error: 2.760709047317505 mm for frame 119

Lowest mean error: 2.3040192127227783 mm for frame 33

Saving results

Total time: 339.61540484428406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1036
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815881
Iteration 2/25 | Loss: 0.00149362
Iteration 3/25 | Loss: 0.00130256
Iteration 4/25 | Loss: 0.00129644
Iteration 5/25 | Loss: 0.00122224
Iteration 6/25 | Loss: 0.00120898
Iteration 7/25 | Loss: 0.00119896
Iteration 8/25 | Loss: 0.00119345
Iteration 9/25 | Loss: 0.00119223
Iteration 10/25 | Loss: 0.00119195
Iteration 11/25 | Loss: 0.00119188
Iteration 12/25 | Loss: 0.00119187
Iteration 13/25 | Loss: 0.00119187
Iteration 14/25 | Loss: 0.00119187
Iteration 15/25 | Loss: 0.00119187
Iteration 16/25 | Loss: 0.00119187
Iteration 17/25 | Loss: 0.00119187
Iteration 18/25 | Loss: 0.00119187
Iteration 19/25 | Loss: 0.00119187
Iteration 20/25 | Loss: 0.00119187
Iteration 21/25 | Loss: 0.00119187
Iteration 22/25 | Loss: 0.00119187
Iteration 23/25 | Loss: 0.00119187
Iteration 24/25 | Loss: 0.00119186
Iteration 25/25 | Loss: 0.00119186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15364790
Iteration 2/25 | Loss: 0.00145051
Iteration 3/25 | Loss: 0.00145048
Iteration 4/25 | Loss: 0.00145048
Iteration 5/25 | Loss: 0.00145048
Iteration 6/25 | Loss: 0.00145048
Iteration 7/25 | Loss: 0.00145048
Iteration 8/25 | Loss: 0.00145048
Iteration 9/25 | Loss: 0.00145048
Iteration 10/25 | Loss: 0.00145048
Iteration 11/25 | Loss: 0.00145048
Iteration 12/25 | Loss: 0.00145048
Iteration 13/25 | Loss: 0.00145048
Iteration 14/25 | Loss: 0.00145048
Iteration 15/25 | Loss: 0.00145048
Iteration 16/25 | Loss: 0.00145048
Iteration 17/25 | Loss: 0.00145048
Iteration 18/25 | Loss: 0.00145048
Iteration 19/25 | Loss: 0.00145048
Iteration 20/25 | Loss: 0.00145048
Iteration 21/25 | Loss: 0.00145048
Iteration 22/25 | Loss: 0.00145048
Iteration 23/25 | Loss: 0.00145048
Iteration 24/25 | Loss: 0.00145048
Iteration 25/25 | Loss: 0.00145048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145048
Iteration 2/1000 | Loss: 0.00004044
Iteration 3/1000 | Loss: 0.00002481
Iteration 4/1000 | Loss: 0.00001958
Iteration 5/1000 | Loss: 0.00001752
Iteration 6/1000 | Loss: 0.00001658
Iteration 7/1000 | Loss: 0.00001588
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001516
Iteration 10/1000 | Loss: 0.00001487
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001462
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001451
Iteration 15/1000 | Loss: 0.00001448
Iteration 16/1000 | Loss: 0.00001447
Iteration 17/1000 | Loss: 0.00001433
Iteration 18/1000 | Loss: 0.00001431
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001420
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001409
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001403
Iteration 27/1000 | Loss: 0.00001400
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001398
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001397
Iteration 33/1000 | Loss: 0.00001397
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001396
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001394
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001394
Iteration 42/1000 | Loss: 0.00001394
Iteration 43/1000 | Loss: 0.00001394
Iteration 44/1000 | Loss: 0.00001394
Iteration 45/1000 | Loss: 0.00001393
Iteration 46/1000 | Loss: 0.00001393
Iteration 47/1000 | Loss: 0.00001393
Iteration 48/1000 | Loss: 0.00001393
Iteration 49/1000 | Loss: 0.00001392
Iteration 50/1000 | Loss: 0.00001392
Iteration 51/1000 | Loss: 0.00001392
Iteration 52/1000 | Loss: 0.00001391
Iteration 53/1000 | Loss: 0.00001391
Iteration 54/1000 | Loss: 0.00001391
Iteration 55/1000 | Loss: 0.00001391
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001390
Iteration 58/1000 | Loss: 0.00001390
Iteration 59/1000 | Loss: 0.00001390
Iteration 60/1000 | Loss: 0.00001390
Iteration 61/1000 | Loss: 0.00001390
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001389
Iteration 68/1000 | Loss: 0.00001389
Iteration 69/1000 | Loss: 0.00001389
Iteration 70/1000 | Loss: 0.00001389
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001388
Iteration 74/1000 | Loss: 0.00001388
Iteration 75/1000 | Loss: 0.00001388
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001387
Iteration 83/1000 | Loss: 0.00001387
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001386
Iteration 91/1000 | Loss: 0.00001386
Iteration 92/1000 | Loss: 0.00001386
Iteration 93/1000 | Loss: 0.00001386
Iteration 94/1000 | Loss: 0.00001386
Iteration 95/1000 | Loss: 0.00001386
Iteration 96/1000 | Loss: 0.00001386
Iteration 97/1000 | Loss: 0.00001386
Iteration 98/1000 | Loss: 0.00001386
Iteration 99/1000 | Loss: 0.00001386
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001386
Iteration 102/1000 | Loss: 0.00001386
Iteration 103/1000 | Loss: 0.00001386
Iteration 104/1000 | Loss: 0.00001386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.3864192624168936e-05, 1.3864192624168936e-05, 1.3864192624168936e-05, 1.3864192624168936e-05, 1.3864192624168936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3864192624168936e-05

Optimization complete. Final v2v error: 3.104275703430176 mm

Highest mean error: 3.687739849090576 mm for frame 59

Lowest mean error: 2.586824655532837 mm for frame 20

Saving results

Total time: 333.3823573589325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1090
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722169
Iteration 2/25 | Loss: 0.00171742
Iteration 3/25 | Loss: 0.00134660
Iteration 4/25 | Loss: 0.00128393
Iteration 5/25 | Loss: 0.00126782
Iteration 6/25 | Loss: 0.00126432
Iteration 7/25 | Loss: 0.00126532
Iteration 8/25 | Loss: 0.00126168
Iteration 9/25 | Loss: 0.00128729
Iteration 10/25 | Loss: 0.00125311
Iteration 11/25 | Loss: 0.00125001
Iteration 12/25 | Loss: 0.00124979
Iteration 13/25 | Loss: 0.00124968
Iteration 14/25 | Loss: 0.00124954
Iteration 15/25 | Loss: 0.00125056
Iteration 16/25 | Loss: 0.00124879
Iteration 17/25 | Loss: 0.00124694
Iteration 18/25 | Loss: 0.00125142
Iteration 19/25 | Loss: 0.00125141
Iteration 20/25 | Loss: 0.00124688
Iteration 21/25 | Loss: 0.00124214
Iteration 22/25 | Loss: 0.00124136
Iteration 23/25 | Loss: 0.00124102
Iteration 24/25 | Loss: 0.00124008
Iteration 25/25 | Loss: 0.00124567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.20479822
Iteration 2/25 | Loss: 0.00233571
Iteration 3/25 | Loss: 0.00233556
Iteration 4/25 | Loss: 0.00233556
Iteration 5/25 | Loss: 0.00233556
Iteration 6/25 | Loss: 0.00233556
Iteration 7/25 | Loss: 0.00233556
Iteration 8/25 | Loss: 0.00233556
Iteration 9/25 | Loss: 0.00233556
Iteration 10/25 | Loss: 0.00233556
Iteration 11/25 | Loss: 0.00233556
Iteration 12/25 | Loss: 0.00233556
Iteration 13/25 | Loss: 0.00233556
Iteration 14/25 | Loss: 0.00233556
Iteration 15/25 | Loss: 0.00233556
Iteration 16/25 | Loss: 0.00233556
Iteration 17/25 | Loss: 0.00233556
Iteration 18/25 | Loss: 0.00233556
Iteration 19/25 | Loss: 0.00233556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002335557248443365, 0.002335557248443365, 0.002335557248443365, 0.002335557248443365, 0.002335557248443365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002335557248443365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233556
Iteration 2/1000 | Loss: 0.00009273
Iteration 3/1000 | Loss: 0.00004409
Iteration 4/1000 | Loss: 0.00003303
Iteration 5/1000 | Loss: 0.00002885
Iteration 6/1000 | Loss: 0.00002679
Iteration 7/1000 | Loss: 0.00002554
Iteration 8/1000 | Loss: 0.00002425
Iteration 9/1000 | Loss: 0.00002351
Iteration 10/1000 | Loss: 0.00002304
Iteration 11/1000 | Loss: 0.00002270
Iteration 12/1000 | Loss: 0.00002244
Iteration 13/1000 | Loss: 0.00002238
Iteration 14/1000 | Loss: 0.00002214
Iteration 15/1000 | Loss: 0.00002188
Iteration 16/1000 | Loss: 0.00002171
Iteration 17/1000 | Loss: 0.00002154
Iteration 18/1000 | Loss: 0.00002148
Iteration 19/1000 | Loss: 0.00002143
Iteration 20/1000 | Loss: 0.00002138
Iteration 21/1000 | Loss: 0.00002134
Iteration 22/1000 | Loss: 0.00002131
Iteration 23/1000 | Loss: 0.00002131
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002130
Iteration 26/1000 | Loss: 0.00002127
Iteration 27/1000 | Loss: 0.00002126
Iteration 28/1000 | Loss: 0.00002126
Iteration 29/1000 | Loss: 0.00002123
Iteration 30/1000 | Loss: 0.00002123
Iteration 31/1000 | Loss: 0.00002122
Iteration 32/1000 | Loss: 0.00002121
Iteration 33/1000 | Loss: 0.00002121
Iteration 34/1000 | Loss: 0.00002120
Iteration 35/1000 | Loss: 0.00002120
Iteration 36/1000 | Loss: 0.00002120
Iteration 37/1000 | Loss: 0.00002119
Iteration 38/1000 | Loss: 0.00002117
Iteration 39/1000 | Loss: 0.00002116
Iteration 40/1000 | Loss: 0.00002116
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002115
Iteration 43/1000 | Loss: 0.00002115
Iteration 44/1000 | Loss: 0.00002114
Iteration 45/1000 | Loss: 0.00002114
Iteration 46/1000 | Loss: 0.00002113
Iteration 47/1000 | Loss: 0.00002113
Iteration 48/1000 | Loss: 0.00002113
Iteration 49/1000 | Loss: 0.00002112
Iteration 50/1000 | Loss: 0.00002112
Iteration 51/1000 | Loss: 0.00002112
Iteration 52/1000 | Loss: 0.00002111
Iteration 53/1000 | Loss: 0.00002111
Iteration 54/1000 | Loss: 0.00002111
Iteration 55/1000 | Loss: 0.00002110
Iteration 56/1000 | Loss: 0.00002110
Iteration 57/1000 | Loss: 0.00002109
Iteration 58/1000 | Loss: 0.00002109
Iteration 59/1000 | Loss: 0.00002109
Iteration 60/1000 | Loss: 0.00002108
Iteration 61/1000 | Loss: 0.00002108
Iteration 62/1000 | Loss: 0.00002108
Iteration 63/1000 | Loss: 0.00002106
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002106
Iteration 67/1000 | Loss: 0.00002106
Iteration 68/1000 | Loss: 0.00002106
Iteration 69/1000 | Loss: 0.00002106
Iteration 70/1000 | Loss: 0.00002106
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00002106
Iteration 73/1000 | Loss: 0.00002105
Iteration 74/1000 | Loss: 0.00002105
Iteration 75/1000 | Loss: 0.00002105
Iteration 76/1000 | Loss: 0.00002105
Iteration 77/1000 | Loss: 0.00002104
Iteration 78/1000 | Loss: 0.00002104
Iteration 79/1000 | Loss: 0.00002104
Iteration 80/1000 | Loss: 0.00002103
Iteration 81/1000 | Loss: 0.00002103
Iteration 82/1000 | Loss: 0.00002103
Iteration 83/1000 | Loss: 0.00002103
Iteration 84/1000 | Loss: 0.00002102
Iteration 85/1000 | Loss: 0.00002102
Iteration 86/1000 | Loss: 0.00002102
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002102
Iteration 89/1000 | Loss: 0.00002101
Iteration 90/1000 | Loss: 0.00002101
Iteration 91/1000 | Loss: 0.00002101
Iteration 92/1000 | Loss: 0.00002101
Iteration 93/1000 | Loss: 0.00002101
Iteration 94/1000 | Loss: 0.00002101
Iteration 95/1000 | Loss: 0.00002100
Iteration 96/1000 | Loss: 0.00002100
Iteration 97/1000 | Loss: 0.00002100
Iteration 98/1000 | Loss: 0.00002100
Iteration 99/1000 | Loss: 0.00002100
Iteration 100/1000 | Loss: 0.00002099
Iteration 101/1000 | Loss: 0.00002099
Iteration 102/1000 | Loss: 0.00002098
Iteration 103/1000 | Loss: 0.00002098
Iteration 104/1000 | Loss: 0.00002098
Iteration 105/1000 | Loss: 0.00002098
Iteration 106/1000 | Loss: 0.00002098
Iteration 107/1000 | Loss: 0.00002097
Iteration 108/1000 | Loss: 0.00002097
Iteration 109/1000 | Loss: 0.00002097
Iteration 110/1000 | Loss: 0.00002097
Iteration 111/1000 | Loss: 0.00002097
Iteration 112/1000 | Loss: 0.00002097
Iteration 113/1000 | Loss: 0.00002097
Iteration 114/1000 | Loss: 0.00002097
Iteration 115/1000 | Loss: 0.00002097
Iteration 116/1000 | Loss: 0.00002097
Iteration 117/1000 | Loss: 0.00002096
Iteration 118/1000 | Loss: 0.00002096
Iteration 119/1000 | Loss: 0.00002096
Iteration 120/1000 | Loss: 0.00002096
Iteration 121/1000 | Loss: 0.00002096
Iteration 122/1000 | Loss: 0.00002095
Iteration 123/1000 | Loss: 0.00002095
Iteration 124/1000 | Loss: 0.00002095
Iteration 125/1000 | Loss: 0.00002094
Iteration 126/1000 | Loss: 0.00002094
Iteration 127/1000 | Loss: 0.00002094
Iteration 128/1000 | Loss: 0.00002094
Iteration 129/1000 | Loss: 0.00002093
Iteration 130/1000 | Loss: 0.00002093
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002092
Iteration 133/1000 | Loss: 0.00002092
Iteration 134/1000 | Loss: 0.00002092
Iteration 135/1000 | Loss: 0.00002092
Iteration 136/1000 | Loss: 0.00002091
Iteration 137/1000 | Loss: 0.00002091
Iteration 138/1000 | Loss: 0.00002091
Iteration 139/1000 | Loss: 0.00002090
Iteration 140/1000 | Loss: 0.00002090
Iteration 141/1000 | Loss: 0.00002090
Iteration 142/1000 | Loss: 0.00002090
Iteration 143/1000 | Loss: 0.00002090
Iteration 144/1000 | Loss: 0.00002089
Iteration 145/1000 | Loss: 0.00002089
Iteration 146/1000 | Loss: 0.00002089
Iteration 147/1000 | Loss: 0.00002089
Iteration 148/1000 | Loss: 0.00002088
Iteration 149/1000 | Loss: 0.00002088
Iteration 150/1000 | Loss: 0.00002088
Iteration 151/1000 | Loss: 0.00002088
Iteration 152/1000 | Loss: 0.00002088
Iteration 153/1000 | Loss: 0.00002088
Iteration 154/1000 | Loss: 0.00002088
Iteration 155/1000 | Loss: 0.00002088
Iteration 156/1000 | Loss: 0.00002088
Iteration 157/1000 | Loss: 0.00002088
Iteration 158/1000 | Loss: 0.00002087
Iteration 159/1000 | Loss: 0.00002087
Iteration 160/1000 | Loss: 0.00002087
Iteration 161/1000 | Loss: 0.00002087
Iteration 162/1000 | Loss: 0.00002087
Iteration 163/1000 | Loss: 0.00002087
Iteration 164/1000 | Loss: 0.00002087
Iteration 165/1000 | Loss: 0.00002087
Iteration 166/1000 | Loss: 0.00002087
Iteration 167/1000 | Loss: 0.00002087
Iteration 168/1000 | Loss: 0.00002087
Iteration 169/1000 | Loss: 0.00002087
Iteration 170/1000 | Loss: 0.00002087
Iteration 171/1000 | Loss: 0.00002087
Iteration 172/1000 | Loss: 0.00002087
Iteration 173/1000 | Loss: 0.00002087
Iteration 174/1000 | Loss: 0.00002087
Iteration 175/1000 | Loss: 0.00002087
Iteration 176/1000 | Loss: 0.00002087
Iteration 177/1000 | Loss: 0.00002087
Iteration 178/1000 | Loss: 0.00002087
Iteration 179/1000 | Loss: 0.00002087
Iteration 180/1000 | Loss: 0.00002087
Iteration 181/1000 | Loss: 0.00002087
Iteration 182/1000 | Loss: 0.00002087
Iteration 183/1000 | Loss: 0.00002087
Iteration 184/1000 | Loss: 0.00002087
Iteration 185/1000 | Loss: 0.00002087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.087124448735267e-05, 2.087124448735267e-05, 2.087124448735267e-05, 2.087124448735267e-05, 2.087124448735267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.087124448735267e-05

Optimization complete. Final v2v error: 3.743067741394043 mm

Highest mean error: 5.593532562255859 mm for frame 64

Lowest mean error: 2.6896307468414307 mm for frame 106

Saving results

Total time: 764.0682792663574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1072
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042726
Iteration 2/25 | Loss: 0.00229007
Iteration 3/25 | Loss: 0.00168794
Iteration 4/25 | Loss: 0.00162834
Iteration 5/25 | Loss: 0.00141696
Iteration 6/25 | Loss: 0.00137452
Iteration 7/25 | Loss: 0.00131802
Iteration 8/25 | Loss: 0.00127099
Iteration 9/25 | Loss: 0.00130824
Iteration 10/25 | Loss: 0.00129836
Iteration 11/25 | Loss: 0.00123606
Iteration 12/25 | Loss: 0.00123323
Iteration 13/25 | Loss: 0.00122637
Iteration 14/25 | Loss: 0.00121807
Iteration 15/25 | Loss: 0.00120933
Iteration 16/25 | Loss: 0.00120534
Iteration 17/25 | Loss: 0.00120504
Iteration 18/25 | Loss: 0.00120478
Iteration 19/25 | Loss: 0.00121080
Iteration 20/25 | Loss: 0.00120464
Iteration 21/25 | Loss: 0.00120225
Iteration 22/25 | Loss: 0.00120148
Iteration 23/25 | Loss: 0.00120530
Iteration 24/25 | Loss: 0.00120619
Iteration 25/25 | Loss: 0.00120098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28251076
Iteration 2/25 | Loss: 0.00210879
Iteration 3/25 | Loss: 0.00210879
Iteration 4/25 | Loss: 0.00210879
Iteration 5/25 | Loss: 0.00210879
Iteration 6/25 | Loss: 0.00210879
Iteration 7/25 | Loss: 0.00210878
Iteration 8/25 | Loss: 0.00210878
Iteration 9/25 | Loss: 0.00210878
Iteration 10/25 | Loss: 0.00210878
Iteration 11/25 | Loss: 0.00210878
Iteration 12/25 | Loss: 0.00210878
Iteration 13/25 | Loss: 0.00210878
Iteration 14/25 | Loss: 0.00210878
Iteration 15/25 | Loss: 0.00210878
Iteration 16/25 | Loss: 0.00210878
Iteration 17/25 | Loss: 0.00210878
Iteration 18/25 | Loss: 0.00210878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002108784159645438, 0.002108784159645438, 0.002108784159645438, 0.002108784159645438, 0.002108784159645438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002108784159645438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210878
Iteration 2/1000 | Loss: 0.00143712
Iteration 3/1000 | Loss: 0.00008679
Iteration 4/1000 | Loss: 0.00004867
Iteration 5/1000 | Loss: 0.00003348
Iteration 6/1000 | Loss: 0.00002497
Iteration 7/1000 | Loss: 0.00003145
Iteration 8/1000 | Loss: 0.00002116
Iteration 9/1000 | Loss: 0.00010515
Iteration 10/1000 | Loss: 0.00004433
Iteration 11/1000 | Loss: 0.00007460
Iteration 12/1000 | Loss: 0.00002782
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00004516
Iteration 15/1000 | Loss: 0.00001519
Iteration 16/1000 | Loss: 0.00003516
Iteration 17/1000 | Loss: 0.00001502
Iteration 18/1000 | Loss: 0.00003476
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001323
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001255
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001252
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001249
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001237
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001233
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001228
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00002397
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001222
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001218
Iteration 72/1000 | Loss: 0.00001218
Iteration 73/1000 | Loss: 0.00001218
Iteration 74/1000 | Loss: 0.00001218
Iteration 75/1000 | Loss: 0.00001218
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001217
Iteration 81/1000 | Loss: 0.00001217
Iteration 82/1000 | Loss: 0.00001217
Iteration 83/1000 | Loss: 0.00001217
Iteration 84/1000 | Loss: 0.00001217
Iteration 85/1000 | Loss: 0.00001217
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001216
Iteration 91/1000 | Loss: 0.00001216
Iteration 92/1000 | Loss: 0.00001216
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001213
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001211
Iteration 112/1000 | Loss: 0.00001211
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001209
Iteration 117/1000 | Loss: 0.00001209
Iteration 118/1000 | Loss: 0.00001209
Iteration 119/1000 | Loss: 0.00002951
Iteration 120/1000 | Loss: 0.00001219
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001205
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001205
Iteration 133/1000 | Loss: 0.00001205
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001205
Iteration 137/1000 | Loss: 0.00001205
Iteration 138/1000 | Loss: 0.00001205
Iteration 139/1000 | Loss: 0.00001205
Iteration 140/1000 | Loss: 0.00001205
Iteration 141/1000 | Loss: 0.00001204
Iteration 142/1000 | Loss: 0.00001204
Iteration 143/1000 | Loss: 0.00001204
Iteration 144/1000 | Loss: 0.00001204
Iteration 145/1000 | Loss: 0.00001204
Iteration 146/1000 | Loss: 0.00001204
Iteration 147/1000 | Loss: 0.00001204
Iteration 148/1000 | Loss: 0.00001204
Iteration 149/1000 | Loss: 0.00001204
Iteration 150/1000 | Loss: 0.00001204
Iteration 151/1000 | Loss: 0.00001204
Iteration 152/1000 | Loss: 0.00001204
Iteration 153/1000 | Loss: 0.00001204
Iteration 154/1000 | Loss: 0.00001204
Iteration 155/1000 | Loss: 0.00001204
Iteration 156/1000 | Loss: 0.00001204
Iteration 157/1000 | Loss: 0.00001204
Iteration 158/1000 | Loss: 0.00001204
Iteration 159/1000 | Loss: 0.00001204
Iteration 160/1000 | Loss: 0.00001204
Iteration 161/1000 | Loss: 0.00001204
Iteration 162/1000 | Loss: 0.00001204
Iteration 163/1000 | Loss: 0.00001204
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2044661161780823e-05, 1.2044661161780823e-05, 1.2044661161780823e-05, 1.2044661161780823e-05, 1.2044661161780823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2044661161780823e-05

Optimization complete. Final v2v error: 2.90372633934021 mm

Highest mean error: 4.4742326736450195 mm for frame 68

Lowest mean error: 2.475306749343872 mm for frame 130

Saving results

Total time: 811.2732372283936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1060
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086690
Iteration 2/25 | Loss: 0.00164641
Iteration 3/25 | Loss: 0.00133029
Iteration 4/25 | Loss: 0.00130646
Iteration 5/25 | Loss: 0.00130042
Iteration 6/25 | Loss: 0.00129980
Iteration 7/25 | Loss: 0.00129980
Iteration 8/25 | Loss: 0.00129980
Iteration 9/25 | Loss: 0.00129980
Iteration 10/25 | Loss: 0.00129980
Iteration 11/25 | Loss: 0.00129980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012997989542782307, 0.0012997989542782307, 0.0012997989542782307, 0.0012997989542782307, 0.0012997989542782307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012997989542782307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12030196
Iteration 2/25 | Loss: 0.00188122
Iteration 3/25 | Loss: 0.00188122
Iteration 4/25 | Loss: 0.00188122
Iteration 5/25 | Loss: 0.00188122
Iteration 6/25 | Loss: 0.00188122
Iteration 7/25 | Loss: 0.00188122
Iteration 8/25 | Loss: 0.00188122
Iteration 9/25 | Loss: 0.00188122
Iteration 10/25 | Loss: 0.00188122
Iteration 11/25 | Loss: 0.00188122
Iteration 12/25 | Loss: 0.00188122
Iteration 13/25 | Loss: 0.00188122
Iteration 14/25 | Loss: 0.00188122
Iteration 15/25 | Loss: 0.00188122
Iteration 16/25 | Loss: 0.00188122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018812189809978008, 0.0018812189809978008, 0.0018812189809978008, 0.0018812189809978008, 0.0018812189809978008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018812189809978008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188122
Iteration 2/1000 | Loss: 0.00007064
Iteration 3/1000 | Loss: 0.00004185
Iteration 4/1000 | Loss: 0.00003190
Iteration 5/1000 | Loss: 0.00002994
Iteration 6/1000 | Loss: 0.00002844
Iteration 7/1000 | Loss: 0.00002761
Iteration 8/1000 | Loss: 0.00002697
Iteration 9/1000 | Loss: 0.00002653
Iteration 10/1000 | Loss: 0.00002616
Iteration 11/1000 | Loss: 0.00002583
Iteration 12/1000 | Loss: 0.00002562
Iteration 13/1000 | Loss: 0.00002543
Iteration 14/1000 | Loss: 0.00002523
Iteration 15/1000 | Loss: 0.00002507
Iteration 16/1000 | Loss: 0.00002494
Iteration 17/1000 | Loss: 0.00002480
Iteration 18/1000 | Loss: 0.00002476
Iteration 19/1000 | Loss: 0.00002471
Iteration 20/1000 | Loss: 0.00002465
Iteration 21/1000 | Loss: 0.00002462
Iteration 22/1000 | Loss: 0.00002462
Iteration 23/1000 | Loss: 0.00002462
Iteration 24/1000 | Loss: 0.00002460
Iteration 25/1000 | Loss: 0.00002460
Iteration 26/1000 | Loss: 0.00002460
Iteration 27/1000 | Loss: 0.00002460
Iteration 28/1000 | Loss: 0.00002460
Iteration 29/1000 | Loss: 0.00002460
Iteration 30/1000 | Loss: 0.00002460
Iteration 31/1000 | Loss: 0.00002459
Iteration 32/1000 | Loss: 0.00002459
Iteration 33/1000 | Loss: 0.00002459
Iteration 34/1000 | Loss: 0.00002457
Iteration 35/1000 | Loss: 0.00002457
Iteration 36/1000 | Loss: 0.00002456
Iteration 37/1000 | Loss: 0.00002456
Iteration 38/1000 | Loss: 0.00002453
Iteration 39/1000 | Loss: 0.00002452
Iteration 40/1000 | Loss: 0.00002450
Iteration 41/1000 | Loss: 0.00002449
Iteration 42/1000 | Loss: 0.00002449
Iteration 43/1000 | Loss: 0.00002449
Iteration 44/1000 | Loss: 0.00002448
Iteration 45/1000 | Loss: 0.00002448
Iteration 46/1000 | Loss: 0.00002448
Iteration 47/1000 | Loss: 0.00002447
Iteration 48/1000 | Loss: 0.00002447
Iteration 49/1000 | Loss: 0.00002447
Iteration 50/1000 | Loss: 0.00002446
Iteration 51/1000 | Loss: 0.00002446
Iteration 52/1000 | Loss: 0.00002446
Iteration 53/1000 | Loss: 0.00002445
Iteration 54/1000 | Loss: 0.00002445
Iteration 55/1000 | Loss: 0.00002444
Iteration 56/1000 | Loss: 0.00002444
Iteration 57/1000 | Loss: 0.00002444
Iteration 58/1000 | Loss: 0.00002444
Iteration 59/1000 | Loss: 0.00002444
Iteration 60/1000 | Loss: 0.00002443
Iteration 61/1000 | Loss: 0.00002442
Iteration 62/1000 | Loss: 0.00002442
Iteration 63/1000 | Loss: 0.00002442
Iteration 64/1000 | Loss: 0.00002442
Iteration 65/1000 | Loss: 0.00002441
Iteration 66/1000 | Loss: 0.00002441
Iteration 67/1000 | Loss: 0.00002441
Iteration 68/1000 | Loss: 0.00002441
Iteration 69/1000 | Loss: 0.00002441
Iteration 70/1000 | Loss: 0.00002441
Iteration 71/1000 | Loss: 0.00002441
Iteration 72/1000 | Loss: 0.00002441
Iteration 73/1000 | Loss: 0.00002441
Iteration 74/1000 | Loss: 0.00002441
Iteration 75/1000 | Loss: 0.00002441
Iteration 76/1000 | Loss: 0.00002440
Iteration 77/1000 | Loss: 0.00002440
Iteration 78/1000 | Loss: 0.00002440
Iteration 79/1000 | Loss: 0.00002440
Iteration 80/1000 | Loss: 0.00002440
Iteration 81/1000 | Loss: 0.00002440
Iteration 82/1000 | Loss: 0.00002438
Iteration 83/1000 | Loss: 0.00002438
Iteration 84/1000 | Loss: 0.00002438
Iteration 85/1000 | Loss: 0.00002438
Iteration 86/1000 | Loss: 0.00002438
Iteration 87/1000 | Loss: 0.00002438
Iteration 88/1000 | Loss: 0.00002438
Iteration 89/1000 | Loss: 0.00002438
Iteration 90/1000 | Loss: 0.00002438
Iteration 91/1000 | Loss: 0.00002438
Iteration 92/1000 | Loss: 0.00002438
Iteration 93/1000 | Loss: 0.00002438
Iteration 94/1000 | Loss: 0.00002438
Iteration 95/1000 | Loss: 0.00002437
Iteration 96/1000 | Loss: 0.00002437
Iteration 97/1000 | Loss: 0.00002437
Iteration 98/1000 | Loss: 0.00002436
Iteration 99/1000 | Loss: 0.00002435
Iteration 100/1000 | Loss: 0.00002435
Iteration 101/1000 | Loss: 0.00002435
Iteration 102/1000 | Loss: 0.00002435
Iteration 103/1000 | Loss: 0.00002435
Iteration 104/1000 | Loss: 0.00002435
Iteration 105/1000 | Loss: 0.00002435
Iteration 106/1000 | Loss: 0.00002435
Iteration 107/1000 | Loss: 0.00002434
Iteration 108/1000 | Loss: 0.00002434
Iteration 109/1000 | Loss: 0.00002434
Iteration 110/1000 | Loss: 0.00002434
Iteration 111/1000 | Loss: 0.00002434
Iteration 112/1000 | Loss: 0.00002433
Iteration 113/1000 | Loss: 0.00002433
Iteration 114/1000 | Loss: 0.00002433
Iteration 115/1000 | Loss: 0.00002433
Iteration 116/1000 | Loss: 0.00002433
Iteration 117/1000 | Loss: 0.00002433
Iteration 118/1000 | Loss: 0.00002433
Iteration 119/1000 | Loss: 0.00002433
Iteration 120/1000 | Loss: 0.00002432
Iteration 121/1000 | Loss: 0.00002432
Iteration 122/1000 | Loss: 0.00002432
Iteration 123/1000 | Loss: 0.00002432
Iteration 124/1000 | Loss: 0.00002432
Iteration 125/1000 | Loss: 0.00002432
Iteration 126/1000 | Loss: 0.00002432
Iteration 127/1000 | Loss: 0.00002431
Iteration 128/1000 | Loss: 0.00002431
Iteration 129/1000 | Loss: 0.00002431
Iteration 130/1000 | Loss: 0.00002431
Iteration 131/1000 | Loss: 0.00002431
Iteration 132/1000 | Loss: 0.00002431
Iteration 133/1000 | Loss: 0.00002430
Iteration 134/1000 | Loss: 0.00002430
Iteration 135/1000 | Loss: 0.00002430
Iteration 136/1000 | Loss: 0.00002430
Iteration 137/1000 | Loss: 0.00002430
Iteration 138/1000 | Loss: 0.00002430
Iteration 139/1000 | Loss: 0.00002430
Iteration 140/1000 | Loss: 0.00002430
Iteration 141/1000 | Loss: 0.00002429
Iteration 142/1000 | Loss: 0.00002429
Iteration 143/1000 | Loss: 0.00002429
Iteration 144/1000 | Loss: 0.00002429
Iteration 145/1000 | Loss: 0.00002429
Iteration 146/1000 | Loss: 0.00002429
Iteration 147/1000 | Loss: 0.00002429
Iteration 148/1000 | Loss: 0.00002429
Iteration 149/1000 | Loss: 0.00002429
Iteration 150/1000 | Loss: 0.00002429
Iteration 151/1000 | Loss: 0.00002429
Iteration 152/1000 | Loss: 0.00002428
Iteration 153/1000 | Loss: 0.00002428
Iteration 154/1000 | Loss: 0.00002428
Iteration 155/1000 | Loss: 0.00002428
Iteration 156/1000 | Loss: 0.00002428
Iteration 157/1000 | Loss: 0.00002428
Iteration 158/1000 | Loss: 0.00002428
Iteration 159/1000 | Loss: 0.00002428
Iteration 160/1000 | Loss: 0.00002428
Iteration 161/1000 | Loss: 0.00002428
Iteration 162/1000 | Loss: 0.00002428
Iteration 163/1000 | Loss: 0.00002428
Iteration 164/1000 | Loss: 0.00002428
Iteration 165/1000 | Loss: 0.00002428
Iteration 166/1000 | Loss: 0.00002428
Iteration 167/1000 | Loss: 0.00002428
Iteration 168/1000 | Loss: 0.00002427
Iteration 169/1000 | Loss: 0.00002427
Iteration 170/1000 | Loss: 0.00002427
Iteration 171/1000 | Loss: 0.00002427
Iteration 172/1000 | Loss: 0.00002427
Iteration 173/1000 | Loss: 0.00002427
Iteration 174/1000 | Loss: 0.00002427
Iteration 175/1000 | Loss: 0.00002427
Iteration 176/1000 | Loss: 0.00002427
Iteration 177/1000 | Loss: 0.00002427
Iteration 178/1000 | Loss: 0.00002427
Iteration 179/1000 | Loss: 0.00002427
Iteration 180/1000 | Loss: 0.00002427
Iteration 181/1000 | Loss: 0.00002427
Iteration 182/1000 | Loss: 0.00002427
Iteration 183/1000 | Loss: 0.00002427
Iteration 184/1000 | Loss: 0.00002426
Iteration 185/1000 | Loss: 0.00002426
Iteration 186/1000 | Loss: 0.00002426
Iteration 187/1000 | Loss: 0.00002426
Iteration 188/1000 | Loss: 0.00002426
Iteration 189/1000 | Loss: 0.00002426
Iteration 190/1000 | Loss: 0.00002426
Iteration 191/1000 | Loss: 0.00002426
Iteration 192/1000 | Loss: 0.00002426
Iteration 193/1000 | Loss: 0.00002426
Iteration 194/1000 | Loss: 0.00002426
Iteration 195/1000 | Loss: 0.00002426
Iteration 196/1000 | Loss: 0.00002426
Iteration 197/1000 | Loss: 0.00002426
Iteration 198/1000 | Loss: 0.00002426
Iteration 199/1000 | Loss: 0.00002426
Iteration 200/1000 | Loss: 0.00002426
Iteration 201/1000 | Loss: 0.00002426
Iteration 202/1000 | Loss: 0.00002426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.426014543743804e-05, 2.426014543743804e-05, 2.426014543743804e-05, 2.426014543743804e-05, 2.426014543743804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.426014543743804e-05

Optimization complete. Final v2v error: 3.9974489212036133 mm

Highest mean error: 5.3790483474731445 mm for frame 75

Lowest mean error: 3.1849119663238525 mm for frame 54

Saving results

Total time: 358.5354037284851
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1015
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464664
Iteration 2/25 | Loss: 0.00131413
Iteration 3/25 | Loss: 0.00117557
Iteration 4/25 | Loss: 0.00116649
Iteration 5/25 | Loss: 0.00116503
Iteration 6/25 | Loss: 0.00116498
Iteration 7/25 | Loss: 0.00116498
Iteration 8/25 | Loss: 0.00116498
Iteration 9/25 | Loss: 0.00116498
Iteration 10/25 | Loss: 0.00116498
Iteration 11/25 | Loss: 0.00116498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011649830266833305, 0.0011649830266833305, 0.0011649830266833305, 0.0011649830266833305, 0.0011649830266833305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011649830266833305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25095046
Iteration 2/25 | Loss: 0.00170063
Iteration 3/25 | Loss: 0.00170063
Iteration 4/25 | Loss: 0.00170063
Iteration 5/25 | Loss: 0.00170063
Iteration 6/25 | Loss: 0.00170062
Iteration 7/25 | Loss: 0.00170062
Iteration 8/25 | Loss: 0.00170062
Iteration 9/25 | Loss: 0.00170062
Iteration 10/25 | Loss: 0.00170062
Iteration 11/25 | Loss: 0.00170062
Iteration 12/25 | Loss: 0.00170062
Iteration 13/25 | Loss: 0.00170062
Iteration 14/25 | Loss: 0.00170062
Iteration 15/25 | Loss: 0.00170062
Iteration 16/25 | Loss: 0.00170062
Iteration 17/25 | Loss: 0.00170062
Iteration 18/25 | Loss: 0.00170062
Iteration 19/25 | Loss: 0.00170062
Iteration 20/25 | Loss: 0.00170062
Iteration 21/25 | Loss: 0.00170062
Iteration 22/25 | Loss: 0.00170062
Iteration 23/25 | Loss: 0.00170062
Iteration 24/25 | Loss: 0.00170062
Iteration 25/25 | Loss: 0.00170062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170062
Iteration 2/1000 | Loss: 0.00002433
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001235
Iteration 7/1000 | Loss: 0.00001190
Iteration 8/1000 | Loss: 0.00001149
Iteration 9/1000 | Loss: 0.00001120
Iteration 10/1000 | Loss: 0.00001094
Iteration 11/1000 | Loss: 0.00001079
Iteration 12/1000 | Loss: 0.00001079
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001072
Iteration 15/1000 | Loss: 0.00001068
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001054
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001052
Iteration 20/1000 | Loss: 0.00001050
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001049
Iteration 23/1000 | Loss: 0.00001048
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001047
Iteration 26/1000 | Loss: 0.00001047
Iteration 27/1000 | Loss: 0.00001044
Iteration 28/1000 | Loss: 0.00001044
Iteration 29/1000 | Loss: 0.00001044
Iteration 30/1000 | Loss: 0.00001043
Iteration 31/1000 | Loss: 0.00001043
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001042
Iteration 35/1000 | Loss: 0.00001040
Iteration 36/1000 | Loss: 0.00001039
Iteration 37/1000 | Loss: 0.00001039
Iteration 38/1000 | Loss: 0.00001036
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001027
Iteration 41/1000 | Loss: 0.00001026
Iteration 42/1000 | Loss: 0.00001026
Iteration 43/1000 | Loss: 0.00001026
Iteration 44/1000 | Loss: 0.00001025
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001023
Iteration 47/1000 | Loss: 0.00001023
Iteration 48/1000 | Loss: 0.00001023
Iteration 49/1000 | Loss: 0.00001022
Iteration 50/1000 | Loss: 0.00001022
Iteration 51/1000 | Loss: 0.00001022
Iteration 52/1000 | Loss: 0.00001021
Iteration 53/1000 | Loss: 0.00001020
Iteration 54/1000 | Loss: 0.00001020
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001018
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001018
Iteration 62/1000 | Loss: 0.00001018
Iteration 63/1000 | Loss: 0.00001018
Iteration 64/1000 | Loss: 0.00001018
Iteration 65/1000 | Loss: 0.00001017
Iteration 66/1000 | Loss: 0.00001017
Iteration 67/1000 | Loss: 0.00001017
Iteration 68/1000 | Loss: 0.00001016
Iteration 69/1000 | Loss: 0.00001016
Iteration 70/1000 | Loss: 0.00001015
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001014
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001013
Iteration 76/1000 | Loss: 0.00001013
Iteration 77/1000 | Loss: 0.00001013
Iteration 78/1000 | Loss: 0.00001012
Iteration 79/1000 | Loss: 0.00001012
Iteration 80/1000 | Loss: 0.00001012
Iteration 81/1000 | Loss: 0.00001012
Iteration 82/1000 | Loss: 0.00001011
Iteration 83/1000 | Loss: 0.00001011
Iteration 84/1000 | Loss: 0.00001010
Iteration 85/1000 | Loss: 0.00001010
Iteration 86/1000 | Loss: 0.00001010
Iteration 87/1000 | Loss: 0.00001010
Iteration 88/1000 | Loss: 0.00001010
Iteration 89/1000 | Loss: 0.00001010
Iteration 90/1000 | Loss: 0.00001010
Iteration 91/1000 | Loss: 0.00001010
Iteration 92/1000 | Loss: 0.00001009
Iteration 93/1000 | Loss: 0.00001009
Iteration 94/1000 | Loss: 0.00001008
Iteration 95/1000 | Loss: 0.00001008
Iteration 96/1000 | Loss: 0.00001008
Iteration 97/1000 | Loss: 0.00001008
Iteration 98/1000 | Loss: 0.00001008
Iteration 99/1000 | Loss: 0.00001008
Iteration 100/1000 | Loss: 0.00001008
Iteration 101/1000 | Loss: 0.00001008
Iteration 102/1000 | Loss: 0.00001008
Iteration 103/1000 | Loss: 0.00001008
Iteration 104/1000 | Loss: 0.00001008
Iteration 105/1000 | Loss: 0.00001007
Iteration 106/1000 | Loss: 0.00001007
Iteration 107/1000 | Loss: 0.00001007
Iteration 108/1000 | Loss: 0.00001007
Iteration 109/1000 | Loss: 0.00001007
Iteration 110/1000 | Loss: 0.00001007
Iteration 111/1000 | Loss: 0.00001006
Iteration 112/1000 | Loss: 0.00001006
Iteration 113/1000 | Loss: 0.00001006
Iteration 114/1000 | Loss: 0.00001006
Iteration 115/1000 | Loss: 0.00001006
Iteration 116/1000 | Loss: 0.00001005
Iteration 117/1000 | Loss: 0.00001005
Iteration 118/1000 | Loss: 0.00001005
Iteration 119/1000 | Loss: 0.00001005
Iteration 120/1000 | Loss: 0.00001005
Iteration 121/1000 | Loss: 0.00001005
Iteration 122/1000 | Loss: 0.00001005
Iteration 123/1000 | Loss: 0.00001005
Iteration 124/1000 | Loss: 0.00001004
Iteration 125/1000 | Loss: 0.00001004
Iteration 126/1000 | Loss: 0.00001004
Iteration 127/1000 | Loss: 0.00001003
Iteration 128/1000 | Loss: 0.00001003
Iteration 129/1000 | Loss: 0.00001003
Iteration 130/1000 | Loss: 0.00001003
Iteration 131/1000 | Loss: 0.00001003
Iteration 132/1000 | Loss: 0.00001003
Iteration 133/1000 | Loss: 0.00001003
Iteration 134/1000 | Loss: 0.00001003
Iteration 135/1000 | Loss: 0.00001003
Iteration 136/1000 | Loss: 0.00001003
Iteration 137/1000 | Loss: 0.00001002
Iteration 138/1000 | Loss: 0.00001002
Iteration 139/1000 | Loss: 0.00001002
Iteration 140/1000 | Loss: 0.00001002
Iteration 141/1000 | Loss: 0.00001002
Iteration 142/1000 | Loss: 0.00001002
Iteration 143/1000 | Loss: 0.00001002
Iteration 144/1000 | Loss: 0.00001002
Iteration 145/1000 | Loss: 0.00001001
Iteration 146/1000 | Loss: 0.00001001
Iteration 147/1000 | Loss: 0.00001001
Iteration 148/1000 | Loss: 0.00001001
Iteration 149/1000 | Loss: 0.00001001
Iteration 150/1000 | Loss: 0.00001001
Iteration 151/1000 | Loss: 0.00001001
Iteration 152/1000 | Loss: 0.00001001
Iteration 153/1000 | Loss: 0.00001001
Iteration 154/1000 | Loss: 0.00001000
Iteration 155/1000 | Loss: 0.00001000
Iteration 156/1000 | Loss: 0.00001000
Iteration 157/1000 | Loss: 0.00001000
Iteration 158/1000 | Loss: 0.00001000
Iteration 159/1000 | Loss: 0.00001000
Iteration 160/1000 | Loss: 0.00001000
Iteration 161/1000 | Loss: 0.00000999
Iteration 162/1000 | Loss: 0.00000999
Iteration 163/1000 | Loss: 0.00000999
Iteration 164/1000 | Loss: 0.00000999
Iteration 165/1000 | Loss: 0.00000999
Iteration 166/1000 | Loss: 0.00000998
Iteration 167/1000 | Loss: 0.00000998
Iteration 168/1000 | Loss: 0.00000998
Iteration 169/1000 | Loss: 0.00000998
Iteration 170/1000 | Loss: 0.00000998
Iteration 171/1000 | Loss: 0.00000998
Iteration 172/1000 | Loss: 0.00000997
Iteration 173/1000 | Loss: 0.00000997
Iteration 174/1000 | Loss: 0.00000997
Iteration 175/1000 | Loss: 0.00000997
Iteration 176/1000 | Loss: 0.00000997
Iteration 177/1000 | Loss: 0.00000997
Iteration 178/1000 | Loss: 0.00000997
Iteration 179/1000 | Loss: 0.00000997
Iteration 180/1000 | Loss: 0.00000997
Iteration 181/1000 | Loss: 0.00000997
Iteration 182/1000 | Loss: 0.00000997
Iteration 183/1000 | Loss: 0.00000996
Iteration 184/1000 | Loss: 0.00000996
Iteration 185/1000 | Loss: 0.00000996
Iteration 186/1000 | Loss: 0.00000996
Iteration 187/1000 | Loss: 0.00000995
Iteration 188/1000 | Loss: 0.00000995
Iteration 189/1000 | Loss: 0.00000995
Iteration 190/1000 | Loss: 0.00000995
Iteration 191/1000 | Loss: 0.00000995
Iteration 192/1000 | Loss: 0.00000995
Iteration 193/1000 | Loss: 0.00000995
Iteration 194/1000 | Loss: 0.00000995
Iteration 195/1000 | Loss: 0.00000995
Iteration 196/1000 | Loss: 0.00000995
Iteration 197/1000 | Loss: 0.00000995
Iteration 198/1000 | Loss: 0.00000994
Iteration 199/1000 | Loss: 0.00000994
Iteration 200/1000 | Loss: 0.00000994
Iteration 201/1000 | Loss: 0.00000993
Iteration 202/1000 | Loss: 0.00000993
Iteration 203/1000 | Loss: 0.00000993
Iteration 204/1000 | Loss: 0.00000993
Iteration 205/1000 | Loss: 0.00000993
Iteration 206/1000 | Loss: 0.00000993
Iteration 207/1000 | Loss: 0.00000992
Iteration 208/1000 | Loss: 0.00000992
Iteration 209/1000 | Loss: 0.00000992
Iteration 210/1000 | Loss: 0.00000992
Iteration 211/1000 | Loss: 0.00000992
Iteration 212/1000 | Loss: 0.00000992
Iteration 213/1000 | Loss: 0.00000992
Iteration 214/1000 | Loss: 0.00000992
Iteration 215/1000 | Loss: 0.00000991
Iteration 216/1000 | Loss: 0.00000991
Iteration 217/1000 | Loss: 0.00000991
Iteration 218/1000 | Loss: 0.00000991
Iteration 219/1000 | Loss: 0.00000991
Iteration 220/1000 | Loss: 0.00000991
Iteration 221/1000 | Loss: 0.00000991
Iteration 222/1000 | Loss: 0.00000991
Iteration 223/1000 | Loss: 0.00000991
Iteration 224/1000 | Loss: 0.00000990
Iteration 225/1000 | Loss: 0.00000990
Iteration 226/1000 | Loss: 0.00000990
Iteration 227/1000 | Loss: 0.00000990
Iteration 228/1000 | Loss: 0.00000990
Iteration 229/1000 | Loss: 0.00000990
Iteration 230/1000 | Loss: 0.00000990
Iteration 231/1000 | Loss: 0.00000990
Iteration 232/1000 | Loss: 0.00000990
Iteration 233/1000 | Loss: 0.00000990
Iteration 234/1000 | Loss: 0.00000990
Iteration 235/1000 | Loss: 0.00000990
Iteration 236/1000 | Loss: 0.00000990
Iteration 237/1000 | Loss: 0.00000990
Iteration 238/1000 | Loss: 0.00000990
Iteration 239/1000 | Loss: 0.00000990
Iteration 240/1000 | Loss: 0.00000990
Iteration 241/1000 | Loss: 0.00000989
Iteration 242/1000 | Loss: 0.00000989
Iteration 243/1000 | Loss: 0.00000989
Iteration 244/1000 | Loss: 0.00000989
Iteration 245/1000 | Loss: 0.00000989
Iteration 246/1000 | Loss: 0.00000989
Iteration 247/1000 | Loss: 0.00000989
Iteration 248/1000 | Loss: 0.00000989
Iteration 249/1000 | Loss: 0.00000989
Iteration 250/1000 | Loss: 0.00000989
Iteration 251/1000 | Loss: 0.00000989
Iteration 252/1000 | Loss: 0.00000989
Iteration 253/1000 | Loss: 0.00000989
Iteration 254/1000 | Loss: 0.00000989
Iteration 255/1000 | Loss: 0.00000989
Iteration 256/1000 | Loss: 0.00000989
Iteration 257/1000 | Loss: 0.00000989
Iteration 258/1000 | Loss: 0.00000989
Iteration 259/1000 | Loss: 0.00000989
Iteration 260/1000 | Loss: 0.00000989
Iteration 261/1000 | Loss: 0.00000989
Iteration 262/1000 | Loss: 0.00000989
Iteration 263/1000 | Loss: 0.00000989
Iteration 264/1000 | Loss: 0.00000989
Iteration 265/1000 | Loss: 0.00000989
Iteration 266/1000 | Loss: 0.00000989
Iteration 267/1000 | Loss: 0.00000989
Iteration 268/1000 | Loss: 0.00000989
Iteration 269/1000 | Loss: 0.00000989
Iteration 270/1000 | Loss: 0.00000989
Iteration 271/1000 | Loss: 0.00000989
Iteration 272/1000 | Loss: 0.00000989
Iteration 273/1000 | Loss: 0.00000989
Iteration 274/1000 | Loss: 0.00000989
Iteration 275/1000 | Loss: 0.00000989
Iteration 276/1000 | Loss: 0.00000989
Iteration 277/1000 | Loss: 0.00000989
Iteration 278/1000 | Loss: 0.00000989
Iteration 279/1000 | Loss: 0.00000989
Iteration 280/1000 | Loss: 0.00000989
Iteration 281/1000 | Loss: 0.00000989
Iteration 282/1000 | Loss: 0.00000989
Iteration 283/1000 | Loss: 0.00000989
Iteration 284/1000 | Loss: 0.00000989
Iteration 285/1000 | Loss: 0.00000989
Iteration 286/1000 | Loss: 0.00000989
Iteration 287/1000 | Loss: 0.00000989
Iteration 288/1000 | Loss: 0.00000989
Iteration 289/1000 | Loss: 0.00000989
Iteration 290/1000 | Loss: 0.00000989
Iteration 291/1000 | Loss: 0.00000989
Iteration 292/1000 | Loss: 0.00000989
Iteration 293/1000 | Loss: 0.00000989
Iteration 294/1000 | Loss: 0.00000989
Iteration 295/1000 | Loss: 0.00000989
Iteration 296/1000 | Loss: 0.00000989
Iteration 297/1000 | Loss: 0.00000989
Iteration 298/1000 | Loss: 0.00000989
Iteration 299/1000 | Loss: 0.00000989
Iteration 300/1000 | Loss: 0.00000989
Iteration 301/1000 | Loss: 0.00000989
Iteration 302/1000 | Loss: 0.00000989
Iteration 303/1000 | Loss: 0.00000989
Iteration 304/1000 | Loss: 0.00000989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [9.890746696328279e-06, 9.890746696328279e-06, 9.890746696328279e-06, 9.890746696328279e-06, 9.890746696328279e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.890746696328279e-06

Optimization complete. Final v2v error: 2.6245174407958984 mm

Highest mean error: 3.520136833190918 mm for frame 76

Lowest mean error: 2.3335721492767334 mm for frame 39

Saving results

Total time: 318.4171631336212
