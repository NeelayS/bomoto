Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=9, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 504-559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982261
Iteration 2/25 | Loss: 0.00147399
Iteration 3/25 | Loss: 0.00082001
Iteration 4/25 | Loss: 0.00071008
Iteration 5/25 | Loss: 0.00068649
Iteration 6/25 | Loss: 0.00067890
Iteration 7/25 | Loss: 0.00068380
Iteration 8/25 | Loss: 0.00068598
Iteration 9/25 | Loss: 0.00067474
Iteration 10/25 | Loss: 0.00066871
Iteration 11/25 | Loss: 0.00067979
Iteration 12/25 | Loss: 0.00067080
Iteration 13/25 | Loss: 0.00065799
Iteration 14/25 | Loss: 0.00065252
Iteration 15/25 | Loss: 0.00065935
Iteration 16/25 | Loss: 0.00066237
Iteration 17/25 | Loss: 0.00065904
Iteration 18/25 | Loss: 0.00064938
Iteration 19/25 | Loss: 0.00064845
Iteration 20/25 | Loss: 0.00064811
Iteration 21/25 | Loss: 0.00064744
Iteration 22/25 | Loss: 0.00064691
Iteration 23/25 | Loss: 0.00064679
Iteration 24/25 | Loss: 0.00064673
Iteration 25/25 | Loss: 0.00064673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.99583435
Iteration 2/25 | Loss: 0.00033163
Iteration 3/25 | Loss: 0.00033160
Iteration 4/25 | Loss: 0.00033160
Iteration 5/25 | Loss: 0.00033160
Iteration 6/25 | Loss: 0.00033160
Iteration 7/25 | Loss: 0.00033160
Iteration 8/25 | Loss: 0.00033160
Iteration 9/25 | Loss: 0.00033160
Iteration 10/25 | Loss: 0.00033160
Iteration 11/25 | Loss: 0.00033160
Iteration 12/25 | Loss: 0.00033160
Iteration 13/25 | Loss: 0.00033160
Iteration 14/25 | Loss: 0.00033160
Iteration 15/25 | Loss: 0.00033160
Iteration 16/25 | Loss: 0.00033160
Iteration 17/25 | Loss: 0.00033160
Iteration 18/25 | Loss: 0.00033160
Iteration 19/25 | Loss: 0.00033160
Iteration 20/25 | Loss: 0.00033160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00033160013845190406, 0.00033160013845190406, 0.00033160013845190406, 0.00033160013845190406, 0.00033160013845190406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033160013845190406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033160
Iteration 2/1000 | Loss: 0.00002903
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00002010
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00025681
Iteration 7/1000 | Loss: 0.00001972
Iteration 8/1000 | Loss: 0.00001727
Iteration 9/1000 | Loss: 0.00001604
Iteration 10/1000 | Loss: 0.00001522
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001453
Iteration 14/1000 | Loss: 0.00001449
Iteration 15/1000 | Loss: 0.00001443
Iteration 16/1000 | Loss: 0.00001430
Iteration 17/1000 | Loss: 0.00001426
Iteration 18/1000 | Loss: 0.00001420
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001409
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001406
Iteration 23/1000 | Loss: 0.00001405
Iteration 24/1000 | Loss: 0.00001405
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001403
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00001401
Iteration 34/1000 | Loss: 0.00001401
Iteration 35/1000 | Loss: 0.00001401
Iteration 36/1000 | Loss: 0.00001401
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001399
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001397
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001395
Iteration 67/1000 | Loss: 0.00001395
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001394
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001393
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Iteration 126/1000 | Loss: 0.00001391
Iteration 127/1000 | Loss: 0.00001391
Iteration 128/1000 | Loss: 0.00001391
Iteration 129/1000 | Loss: 0.00001391
Iteration 130/1000 | Loss: 0.00001391
Iteration 131/1000 | Loss: 0.00001391
Iteration 132/1000 | Loss: 0.00001391
Iteration 133/1000 | Loss: 0.00001391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.3908030268794391e-05, 1.3908030268794391e-05, 1.3908030268794391e-05, 1.3908030268794391e-05, 1.3908030268794391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3908030268794391e-05

Optimization complete. Final v2v error: 3.1747403144836426 mm

Highest mean error: 3.9281699657440186 mm for frame 83

Lowest mean error: 2.7507221698760986 mm for frame 28

Saving results

Total time: 68.21147465705872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401062
Iteration 2/25 | Loss: 0.00089309
Iteration 3/25 | Loss: 0.00071454
Iteration 4/25 | Loss: 0.00067210
Iteration 5/25 | Loss: 0.00065442
Iteration 6/25 | Loss: 0.00064857
Iteration 7/25 | Loss: 0.00064600
Iteration 8/25 | Loss: 0.00064506
Iteration 9/25 | Loss: 0.00064500
Iteration 10/25 | Loss: 0.00064500
Iteration 11/25 | Loss: 0.00064500
Iteration 12/25 | Loss: 0.00064500
Iteration 13/25 | Loss: 0.00064500
Iteration 14/25 | Loss: 0.00064500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006450019427575171, 0.0006450019427575171, 0.0006450019427575171, 0.0006450019427575171, 0.0006450019427575171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006450019427575171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44574523
Iteration 2/25 | Loss: 0.00032935
Iteration 3/25 | Loss: 0.00032934
Iteration 4/25 | Loss: 0.00032934
Iteration 5/25 | Loss: 0.00032934
Iteration 6/25 | Loss: 0.00032934
Iteration 7/25 | Loss: 0.00032934
Iteration 8/25 | Loss: 0.00032934
Iteration 9/25 | Loss: 0.00032934
Iteration 10/25 | Loss: 0.00032934
Iteration 11/25 | Loss: 0.00032934
Iteration 12/25 | Loss: 0.00032934
Iteration 13/25 | Loss: 0.00032934
Iteration 14/25 | Loss: 0.00032934
Iteration 15/25 | Loss: 0.00032934
Iteration 16/25 | Loss: 0.00032934
Iteration 17/25 | Loss: 0.00032934
Iteration 18/25 | Loss: 0.00032934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003293413610663265, 0.0003293413610663265, 0.0003293413610663265, 0.0003293413610663265, 0.0003293413610663265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003293413610663265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032934
Iteration 2/1000 | Loss: 0.00004063
Iteration 3/1000 | Loss: 0.00003050
Iteration 4/1000 | Loss: 0.00002323
Iteration 5/1000 | Loss: 0.00002137
Iteration 6/1000 | Loss: 0.00002016
Iteration 7/1000 | Loss: 0.00001930
Iteration 8/1000 | Loss: 0.00001873
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001783
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001735
Iteration 13/1000 | Loss: 0.00001714
Iteration 14/1000 | Loss: 0.00001706
Iteration 15/1000 | Loss: 0.00001698
Iteration 16/1000 | Loss: 0.00001694
Iteration 17/1000 | Loss: 0.00001693
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001693
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001688
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001687
Iteration 28/1000 | Loss: 0.00001686
Iteration 29/1000 | Loss: 0.00001684
Iteration 30/1000 | Loss: 0.00001684
Iteration 31/1000 | Loss: 0.00001684
Iteration 32/1000 | Loss: 0.00001683
Iteration 33/1000 | Loss: 0.00001682
Iteration 34/1000 | Loss: 0.00001681
Iteration 35/1000 | Loss: 0.00001681
Iteration 36/1000 | Loss: 0.00001681
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001679
Iteration 39/1000 | Loss: 0.00001679
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001678
Iteration 42/1000 | Loss: 0.00001678
Iteration 43/1000 | Loss: 0.00001678
Iteration 44/1000 | Loss: 0.00001677
Iteration 45/1000 | Loss: 0.00001677
Iteration 46/1000 | Loss: 0.00001677
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001676
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001672
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001672
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001671
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001671
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001670
Iteration 78/1000 | Loss: 0.00001670
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001669
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001669
Iteration 86/1000 | Loss: 0.00001669
Iteration 87/1000 | Loss: 0.00001669
Iteration 88/1000 | Loss: 0.00001669
Iteration 89/1000 | Loss: 0.00001669
Iteration 90/1000 | Loss: 0.00001669
Iteration 91/1000 | Loss: 0.00001669
Iteration 92/1000 | Loss: 0.00001669
Iteration 93/1000 | Loss: 0.00001669
Iteration 94/1000 | Loss: 0.00001669
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001669
Iteration 97/1000 | Loss: 0.00001669
Iteration 98/1000 | Loss: 0.00001668
Iteration 99/1000 | Loss: 0.00001668
Iteration 100/1000 | Loss: 0.00001668
Iteration 101/1000 | Loss: 0.00001668
Iteration 102/1000 | Loss: 0.00001667
Iteration 103/1000 | Loss: 0.00001667
Iteration 104/1000 | Loss: 0.00001667
Iteration 105/1000 | Loss: 0.00001667
Iteration 106/1000 | Loss: 0.00001667
Iteration 107/1000 | Loss: 0.00001667
Iteration 108/1000 | Loss: 0.00001667
Iteration 109/1000 | Loss: 0.00001667
Iteration 110/1000 | Loss: 0.00001667
Iteration 111/1000 | Loss: 0.00001666
Iteration 112/1000 | Loss: 0.00001666
Iteration 113/1000 | Loss: 0.00001666
Iteration 114/1000 | Loss: 0.00001666
Iteration 115/1000 | Loss: 0.00001665
Iteration 116/1000 | Loss: 0.00001665
Iteration 117/1000 | Loss: 0.00001665
Iteration 118/1000 | Loss: 0.00001665
Iteration 119/1000 | Loss: 0.00001664
Iteration 120/1000 | Loss: 0.00001664
Iteration 121/1000 | Loss: 0.00001664
Iteration 122/1000 | Loss: 0.00001664
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001663
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001663
Iteration 128/1000 | Loss: 0.00001663
Iteration 129/1000 | Loss: 0.00001663
Iteration 130/1000 | Loss: 0.00001663
Iteration 131/1000 | Loss: 0.00001662
Iteration 132/1000 | Loss: 0.00001662
Iteration 133/1000 | Loss: 0.00001662
Iteration 134/1000 | Loss: 0.00001662
Iteration 135/1000 | Loss: 0.00001662
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00001661
Iteration 138/1000 | Loss: 0.00001661
Iteration 139/1000 | Loss: 0.00001661
Iteration 140/1000 | Loss: 0.00001661
Iteration 141/1000 | Loss: 0.00001661
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001660
Iteration 144/1000 | Loss: 0.00001660
Iteration 145/1000 | Loss: 0.00001660
Iteration 146/1000 | Loss: 0.00001660
Iteration 147/1000 | Loss: 0.00001660
Iteration 148/1000 | Loss: 0.00001659
Iteration 149/1000 | Loss: 0.00001659
Iteration 150/1000 | Loss: 0.00001659
Iteration 151/1000 | Loss: 0.00001659
Iteration 152/1000 | Loss: 0.00001659
Iteration 153/1000 | Loss: 0.00001659
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001659
Iteration 156/1000 | Loss: 0.00001659
Iteration 157/1000 | Loss: 0.00001659
Iteration 158/1000 | Loss: 0.00001659
Iteration 159/1000 | Loss: 0.00001659
Iteration 160/1000 | Loss: 0.00001659
Iteration 161/1000 | Loss: 0.00001659
Iteration 162/1000 | Loss: 0.00001659
Iteration 163/1000 | Loss: 0.00001658
Iteration 164/1000 | Loss: 0.00001658
Iteration 165/1000 | Loss: 0.00001658
Iteration 166/1000 | Loss: 0.00001658
Iteration 167/1000 | Loss: 0.00001658
Iteration 168/1000 | Loss: 0.00001658
Iteration 169/1000 | Loss: 0.00001658
Iteration 170/1000 | Loss: 0.00001658
Iteration 171/1000 | Loss: 0.00001658
Iteration 172/1000 | Loss: 0.00001658
Iteration 173/1000 | Loss: 0.00001658
Iteration 174/1000 | Loss: 0.00001658
Iteration 175/1000 | Loss: 0.00001658
Iteration 176/1000 | Loss: 0.00001658
Iteration 177/1000 | Loss: 0.00001658
Iteration 178/1000 | Loss: 0.00001658
Iteration 179/1000 | Loss: 0.00001658
Iteration 180/1000 | Loss: 0.00001658
Iteration 181/1000 | Loss: 0.00001658
Iteration 182/1000 | Loss: 0.00001658
Iteration 183/1000 | Loss: 0.00001658
Iteration 184/1000 | Loss: 0.00001658
Iteration 185/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.6582474927417934e-05, 1.6582474927417934e-05, 1.6582474927417934e-05, 1.6582474927417934e-05, 1.6582474927417934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6582474927417934e-05

Optimization complete. Final v2v error: 3.3435938358306885 mm

Highest mean error: 4.586787223815918 mm for frame 51

Lowest mean error: 2.5840182304382324 mm for frame 81

Saving results

Total time: 43.70847225189209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871606
Iteration 2/25 | Loss: 0.00074641
Iteration 3/25 | Loss: 0.00060099
Iteration 4/25 | Loss: 0.00058243
Iteration 5/25 | Loss: 0.00057958
Iteration 6/25 | Loss: 0.00057928
Iteration 7/25 | Loss: 0.00057928
Iteration 8/25 | Loss: 0.00057928
Iteration 9/25 | Loss: 0.00057928
Iteration 10/25 | Loss: 0.00057928
Iteration 11/25 | Loss: 0.00057928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005792832816950977, 0.0005792832816950977, 0.0005792832816950977, 0.0005792832816950977, 0.0005792832816950977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005792832816950977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.89602470
Iteration 2/25 | Loss: 0.00023350
Iteration 3/25 | Loss: 0.00023350
Iteration 4/25 | Loss: 0.00023349
Iteration 5/25 | Loss: 0.00023349
Iteration 6/25 | Loss: 0.00023349
Iteration 7/25 | Loss: 0.00023349
Iteration 8/25 | Loss: 0.00023349
Iteration 9/25 | Loss: 0.00023349
Iteration 10/25 | Loss: 0.00023349
Iteration 11/25 | Loss: 0.00023349
Iteration 12/25 | Loss: 0.00023349
Iteration 13/25 | Loss: 0.00023349
Iteration 14/25 | Loss: 0.00023349
Iteration 15/25 | Loss: 0.00023349
Iteration 16/25 | Loss: 0.00023349
Iteration 17/25 | Loss: 0.00023349
Iteration 18/25 | Loss: 0.00023349
Iteration 19/25 | Loss: 0.00023349
Iteration 20/25 | Loss: 0.00023349
Iteration 21/25 | Loss: 0.00023349
Iteration 22/25 | Loss: 0.00023349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00023349217372015119, 0.00023349217372015119, 0.00023349217372015119, 0.00023349217372015119, 0.00023349217372015119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023349217372015119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023349
Iteration 2/1000 | Loss: 0.00001845
Iteration 3/1000 | Loss: 0.00001297
Iteration 4/1000 | Loss: 0.00001215
Iteration 5/1000 | Loss: 0.00001130
Iteration 6/1000 | Loss: 0.00001095
Iteration 7/1000 | Loss: 0.00001074
Iteration 8/1000 | Loss: 0.00001063
Iteration 9/1000 | Loss: 0.00001054
Iteration 10/1000 | Loss: 0.00001052
Iteration 11/1000 | Loss: 0.00001051
Iteration 12/1000 | Loss: 0.00001051
Iteration 13/1000 | Loss: 0.00001050
Iteration 14/1000 | Loss: 0.00001049
Iteration 15/1000 | Loss: 0.00001049
Iteration 16/1000 | Loss: 0.00001048
Iteration 17/1000 | Loss: 0.00001047
Iteration 18/1000 | Loss: 0.00001046
Iteration 19/1000 | Loss: 0.00001045
Iteration 20/1000 | Loss: 0.00001045
Iteration 21/1000 | Loss: 0.00001044
Iteration 22/1000 | Loss: 0.00001044
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001043
Iteration 26/1000 | Loss: 0.00001042
Iteration 27/1000 | Loss: 0.00001042
Iteration 28/1000 | Loss: 0.00001041
Iteration 29/1000 | Loss: 0.00001040
Iteration 30/1000 | Loss: 0.00001040
Iteration 31/1000 | Loss: 0.00001039
Iteration 32/1000 | Loss: 0.00001038
Iteration 33/1000 | Loss: 0.00001038
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001038
Iteration 37/1000 | Loss: 0.00001038
Iteration 38/1000 | Loss: 0.00001037
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001037
Iteration 41/1000 | Loss: 0.00001036
Iteration 42/1000 | Loss: 0.00001035
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001033
Iteration 48/1000 | Loss: 0.00001033
Iteration 49/1000 | Loss: 0.00001033
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001032
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001031
Iteration 54/1000 | Loss: 0.00001030
Iteration 55/1000 | Loss: 0.00001030
Iteration 56/1000 | Loss: 0.00001030
Iteration 57/1000 | Loss: 0.00001028
Iteration 58/1000 | Loss: 0.00001028
Iteration 59/1000 | Loss: 0.00001028
Iteration 60/1000 | Loss: 0.00001028
Iteration 61/1000 | Loss: 0.00001028
Iteration 62/1000 | Loss: 0.00001028
Iteration 63/1000 | Loss: 0.00001027
Iteration 64/1000 | Loss: 0.00001027
Iteration 65/1000 | Loss: 0.00001027
Iteration 66/1000 | Loss: 0.00001027
Iteration 67/1000 | Loss: 0.00001027
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001027
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001027
Iteration 73/1000 | Loss: 0.00001026
Iteration 74/1000 | Loss: 0.00001026
Iteration 75/1000 | Loss: 0.00001025
Iteration 76/1000 | Loss: 0.00001025
Iteration 77/1000 | Loss: 0.00001024
Iteration 78/1000 | Loss: 0.00001024
Iteration 79/1000 | Loss: 0.00001024
Iteration 80/1000 | Loss: 0.00001024
Iteration 81/1000 | Loss: 0.00001023
Iteration 82/1000 | Loss: 0.00001023
Iteration 83/1000 | Loss: 0.00001023
Iteration 84/1000 | Loss: 0.00001023
Iteration 85/1000 | Loss: 0.00001023
Iteration 86/1000 | Loss: 0.00001023
Iteration 87/1000 | Loss: 0.00001023
Iteration 88/1000 | Loss: 0.00001023
Iteration 89/1000 | Loss: 0.00001022
Iteration 90/1000 | Loss: 0.00001022
Iteration 91/1000 | Loss: 0.00001022
Iteration 92/1000 | Loss: 0.00001022
Iteration 93/1000 | Loss: 0.00001022
Iteration 94/1000 | Loss: 0.00001022
Iteration 95/1000 | Loss: 0.00001022
Iteration 96/1000 | Loss: 0.00001021
Iteration 97/1000 | Loss: 0.00001021
Iteration 98/1000 | Loss: 0.00001021
Iteration 99/1000 | Loss: 0.00001021
Iteration 100/1000 | Loss: 0.00001021
Iteration 101/1000 | Loss: 0.00001021
Iteration 102/1000 | Loss: 0.00001021
Iteration 103/1000 | Loss: 0.00001021
Iteration 104/1000 | Loss: 0.00001021
Iteration 105/1000 | Loss: 0.00001020
Iteration 106/1000 | Loss: 0.00001020
Iteration 107/1000 | Loss: 0.00001020
Iteration 108/1000 | Loss: 0.00001020
Iteration 109/1000 | Loss: 0.00001020
Iteration 110/1000 | Loss: 0.00001020
Iteration 111/1000 | Loss: 0.00001019
Iteration 112/1000 | Loss: 0.00001019
Iteration 113/1000 | Loss: 0.00001019
Iteration 114/1000 | Loss: 0.00001019
Iteration 115/1000 | Loss: 0.00001019
Iteration 116/1000 | Loss: 0.00001018
Iteration 117/1000 | Loss: 0.00001018
Iteration 118/1000 | Loss: 0.00001018
Iteration 119/1000 | Loss: 0.00001018
Iteration 120/1000 | Loss: 0.00001018
Iteration 121/1000 | Loss: 0.00001018
Iteration 122/1000 | Loss: 0.00001018
Iteration 123/1000 | Loss: 0.00001018
Iteration 124/1000 | Loss: 0.00001018
Iteration 125/1000 | Loss: 0.00001018
Iteration 126/1000 | Loss: 0.00001018
Iteration 127/1000 | Loss: 0.00001017
Iteration 128/1000 | Loss: 0.00001017
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001016
Iteration 133/1000 | Loss: 0.00001016
Iteration 134/1000 | Loss: 0.00001016
Iteration 135/1000 | Loss: 0.00001016
Iteration 136/1000 | Loss: 0.00001016
Iteration 137/1000 | Loss: 0.00001015
Iteration 138/1000 | Loss: 0.00001015
Iteration 139/1000 | Loss: 0.00001015
Iteration 140/1000 | Loss: 0.00001014
Iteration 141/1000 | Loss: 0.00001014
Iteration 142/1000 | Loss: 0.00001014
Iteration 143/1000 | Loss: 0.00001014
Iteration 144/1000 | Loss: 0.00001014
Iteration 145/1000 | Loss: 0.00001014
Iteration 146/1000 | Loss: 0.00001014
Iteration 147/1000 | Loss: 0.00001013
Iteration 148/1000 | Loss: 0.00001013
Iteration 149/1000 | Loss: 0.00001013
Iteration 150/1000 | Loss: 0.00001013
Iteration 151/1000 | Loss: 0.00001013
Iteration 152/1000 | Loss: 0.00001013
Iteration 153/1000 | Loss: 0.00001013
Iteration 154/1000 | Loss: 0.00001013
Iteration 155/1000 | Loss: 0.00001012
Iteration 156/1000 | Loss: 0.00001012
Iteration 157/1000 | Loss: 0.00001012
Iteration 158/1000 | Loss: 0.00001012
Iteration 159/1000 | Loss: 0.00001012
Iteration 160/1000 | Loss: 0.00001012
Iteration 161/1000 | Loss: 0.00001012
Iteration 162/1000 | Loss: 0.00001012
Iteration 163/1000 | Loss: 0.00001012
Iteration 164/1000 | Loss: 0.00001012
Iteration 165/1000 | Loss: 0.00001012
Iteration 166/1000 | Loss: 0.00001012
Iteration 167/1000 | Loss: 0.00001012
Iteration 168/1000 | Loss: 0.00001012
Iteration 169/1000 | Loss: 0.00001011
Iteration 170/1000 | Loss: 0.00001011
Iteration 171/1000 | Loss: 0.00001011
Iteration 172/1000 | Loss: 0.00001011
Iteration 173/1000 | Loss: 0.00001011
Iteration 174/1000 | Loss: 0.00001011
Iteration 175/1000 | Loss: 0.00001011
Iteration 176/1000 | Loss: 0.00001011
Iteration 177/1000 | Loss: 0.00001011
Iteration 178/1000 | Loss: 0.00001011
Iteration 179/1000 | Loss: 0.00001010
Iteration 180/1000 | Loss: 0.00001010
Iteration 181/1000 | Loss: 0.00001010
Iteration 182/1000 | Loss: 0.00001010
Iteration 183/1000 | Loss: 0.00001010
Iteration 184/1000 | Loss: 0.00001010
Iteration 185/1000 | Loss: 0.00001010
Iteration 186/1000 | Loss: 0.00001010
Iteration 187/1000 | Loss: 0.00001010
Iteration 188/1000 | Loss: 0.00001010
Iteration 189/1000 | Loss: 0.00001010
Iteration 190/1000 | Loss: 0.00001010
Iteration 191/1000 | Loss: 0.00001010
Iteration 192/1000 | Loss: 0.00001010
Iteration 193/1000 | Loss: 0.00001010
Iteration 194/1000 | Loss: 0.00001010
Iteration 195/1000 | Loss: 0.00001010
Iteration 196/1000 | Loss: 0.00001010
Iteration 197/1000 | Loss: 0.00001010
Iteration 198/1000 | Loss: 0.00001009
Iteration 199/1000 | Loss: 0.00001009
Iteration 200/1000 | Loss: 0.00001009
Iteration 201/1000 | Loss: 0.00001009
Iteration 202/1000 | Loss: 0.00001009
Iteration 203/1000 | Loss: 0.00001009
Iteration 204/1000 | Loss: 0.00001009
Iteration 205/1000 | Loss: 0.00001009
Iteration 206/1000 | Loss: 0.00001009
Iteration 207/1000 | Loss: 0.00001009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.0094464414578397e-05, 1.0094464414578397e-05, 1.0094464414578397e-05, 1.0094464414578397e-05, 1.0094464414578397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0094464414578397e-05

Optimization complete. Final v2v error: 2.721701145172119 mm

Highest mean error: 2.8888633251190186 mm for frame 237

Lowest mean error: 2.574075937271118 mm for frame 5

Saving results

Total time: 39.11321473121643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119483
Iteration 2/25 | Loss: 0.00191852
Iteration 3/25 | Loss: 0.00138052
Iteration 4/25 | Loss: 0.00131871
Iteration 5/25 | Loss: 0.00088941
Iteration 6/25 | Loss: 0.00086092
Iteration 7/25 | Loss: 0.00085821
Iteration 8/25 | Loss: 0.00085787
Iteration 9/25 | Loss: 0.00085787
Iteration 10/25 | Loss: 0.00085787
Iteration 11/25 | Loss: 0.00085787
Iteration 12/25 | Loss: 0.00085787
Iteration 13/25 | Loss: 0.00085787
Iteration 14/25 | Loss: 0.00085787
Iteration 15/25 | Loss: 0.00085787
Iteration 16/25 | Loss: 0.00085787
Iteration 17/25 | Loss: 0.00085787
Iteration 18/25 | Loss: 0.00085787
Iteration 19/25 | Loss: 0.00085787
Iteration 20/25 | Loss: 0.00085787
Iteration 21/25 | Loss: 0.00085787
Iteration 22/25 | Loss: 0.00085787
Iteration 23/25 | Loss: 0.00085787
Iteration 24/25 | Loss: 0.00085787
Iteration 25/25 | Loss: 0.00085787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29421043
Iteration 2/25 | Loss: 0.00029126
Iteration 3/25 | Loss: 0.00029126
Iteration 4/25 | Loss: 0.00029125
Iteration 5/25 | Loss: 0.00029125
Iteration 6/25 | Loss: 0.00029125
Iteration 7/25 | Loss: 0.00029125
Iteration 8/25 | Loss: 0.00029125
Iteration 9/25 | Loss: 0.00029125
Iteration 10/25 | Loss: 0.00029125
Iteration 11/25 | Loss: 0.00029125
Iteration 12/25 | Loss: 0.00029125
Iteration 13/25 | Loss: 0.00029125
Iteration 14/25 | Loss: 0.00029125
Iteration 15/25 | Loss: 0.00029125
Iteration 16/25 | Loss: 0.00029125
Iteration 17/25 | Loss: 0.00029125
Iteration 18/25 | Loss: 0.00029125
Iteration 19/25 | Loss: 0.00029125
Iteration 20/25 | Loss: 0.00029125
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002912520430982113, 0.0002912520430982113, 0.0002912520430982113, 0.0002912520430982113, 0.0002912520430982113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002912520430982113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029125
Iteration 2/1000 | Loss: 0.00003953
Iteration 3/1000 | Loss: 0.00003080
Iteration 4/1000 | Loss: 0.00002860
Iteration 5/1000 | Loss: 0.00002740
Iteration 6/1000 | Loss: 0.00002692
Iteration 7/1000 | Loss: 0.00002666
Iteration 8/1000 | Loss: 0.00002650
Iteration 9/1000 | Loss: 0.00002644
Iteration 10/1000 | Loss: 0.00002644
Iteration 11/1000 | Loss: 0.00002633
Iteration 12/1000 | Loss: 0.00002633
Iteration 13/1000 | Loss: 0.00002633
Iteration 14/1000 | Loss: 0.00002633
Iteration 15/1000 | Loss: 0.00002633
Iteration 16/1000 | Loss: 0.00002633
Iteration 17/1000 | Loss: 0.00002629
Iteration 18/1000 | Loss: 0.00002627
Iteration 19/1000 | Loss: 0.00002627
Iteration 20/1000 | Loss: 0.00002627
Iteration 21/1000 | Loss: 0.00002627
Iteration 22/1000 | Loss: 0.00002627
Iteration 23/1000 | Loss: 0.00002627
Iteration 24/1000 | Loss: 0.00002626
Iteration 25/1000 | Loss: 0.00002626
Iteration 26/1000 | Loss: 0.00002624
Iteration 27/1000 | Loss: 0.00002623
Iteration 28/1000 | Loss: 0.00002623
Iteration 29/1000 | Loss: 0.00002623
Iteration 30/1000 | Loss: 0.00002622
Iteration 31/1000 | Loss: 0.00002622
Iteration 32/1000 | Loss: 0.00002621
Iteration 33/1000 | Loss: 0.00002621
Iteration 34/1000 | Loss: 0.00002620
Iteration 35/1000 | Loss: 0.00002620
Iteration 36/1000 | Loss: 0.00002620
Iteration 37/1000 | Loss: 0.00002620
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00002619
Iteration 40/1000 | Loss: 0.00002619
Iteration 41/1000 | Loss: 0.00002619
Iteration 42/1000 | Loss: 0.00002619
Iteration 43/1000 | Loss: 0.00002619
Iteration 44/1000 | Loss: 0.00002619
Iteration 45/1000 | Loss: 0.00002619
Iteration 46/1000 | Loss: 0.00002619
Iteration 47/1000 | Loss: 0.00002619
Iteration 48/1000 | Loss: 0.00002618
Iteration 49/1000 | Loss: 0.00002618
Iteration 50/1000 | Loss: 0.00002617
Iteration 51/1000 | Loss: 0.00002617
Iteration 52/1000 | Loss: 0.00002617
Iteration 53/1000 | Loss: 0.00002617
Iteration 54/1000 | Loss: 0.00002617
Iteration 55/1000 | Loss: 0.00002617
Iteration 56/1000 | Loss: 0.00002617
Iteration 57/1000 | Loss: 0.00002617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [2.6169818738708273e-05, 2.6169818738708273e-05, 2.6169818738708273e-05, 2.6169818738708273e-05, 2.6169818738708273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6169818738708273e-05

Optimization complete. Final v2v error: 4.100277423858643 mm

Highest mean error: 4.43522310256958 mm for frame 72

Lowest mean error: 3.7804667949676514 mm for frame 218

Saving results

Total time: 32.038416385650635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460349
Iteration 2/25 | Loss: 0.00089105
Iteration 3/25 | Loss: 0.00068689
Iteration 4/25 | Loss: 0.00065875
Iteration 5/25 | Loss: 0.00065137
Iteration 6/25 | Loss: 0.00064999
Iteration 7/25 | Loss: 0.00064963
Iteration 8/25 | Loss: 0.00064963
Iteration 9/25 | Loss: 0.00064963
Iteration 10/25 | Loss: 0.00064963
Iteration 11/25 | Loss: 0.00064963
Iteration 12/25 | Loss: 0.00064963
Iteration 13/25 | Loss: 0.00064963
Iteration 14/25 | Loss: 0.00064963
Iteration 15/25 | Loss: 0.00064963
Iteration 16/25 | Loss: 0.00064963
Iteration 17/25 | Loss: 0.00064963
Iteration 18/25 | Loss: 0.00064963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006496317218989134, 0.0006496317218989134, 0.0006496317218989134, 0.0006496317218989134, 0.0006496317218989134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006496317218989134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14961386
Iteration 2/25 | Loss: 0.00027389
Iteration 3/25 | Loss: 0.00027389
Iteration 4/25 | Loss: 0.00027389
Iteration 5/25 | Loss: 0.00027389
Iteration 6/25 | Loss: 0.00027389
Iteration 7/25 | Loss: 0.00027389
Iteration 8/25 | Loss: 0.00027389
Iteration 9/25 | Loss: 0.00027389
Iteration 10/25 | Loss: 0.00027389
Iteration 11/25 | Loss: 0.00027389
Iteration 12/25 | Loss: 0.00027389
Iteration 13/25 | Loss: 0.00027389
Iteration 14/25 | Loss: 0.00027389
Iteration 15/25 | Loss: 0.00027389
Iteration 16/25 | Loss: 0.00027389
Iteration 17/25 | Loss: 0.00027389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00027388957096263766, 0.00027388957096263766, 0.00027388957096263766, 0.00027388957096263766, 0.00027388957096263766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027388957096263766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027389
Iteration 2/1000 | Loss: 0.00002823
Iteration 3/1000 | Loss: 0.00002064
Iteration 4/1000 | Loss: 0.00001795
Iteration 5/1000 | Loss: 0.00001711
Iteration 6/1000 | Loss: 0.00001651
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001595
Iteration 9/1000 | Loss: 0.00001572
Iteration 10/1000 | Loss: 0.00001565
Iteration 11/1000 | Loss: 0.00001565
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001551
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001549
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00001533
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001533
Iteration 25/1000 | Loss: 0.00001527
Iteration 26/1000 | Loss: 0.00001522
Iteration 27/1000 | Loss: 0.00001522
Iteration 28/1000 | Loss: 0.00001518
Iteration 29/1000 | Loss: 0.00001518
Iteration 30/1000 | Loss: 0.00001514
Iteration 31/1000 | Loss: 0.00001514
Iteration 32/1000 | Loss: 0.00001514
Iteration 33/1000 | Loss: 0.00001514
Iteration 34/1000 | Loss: 0.00001513
Iteration 35/1000 | Loss: 0.00001513
Iteration 36/1000 | Loss: 0.00001513
Iteration 37/1000 | Loss: 0.00001513
Iteration 38/1000 | Loss: 0.00001513
Iteration 39/1000 | Loss: 0.00001513
Iteration 40/1000 | Loss: 0.00001512
Iteration 41/1000 | Loss: 0.00001511
Iteration 42/1000 | Loss: 0.00001510
Iteration 43/1000 | Loss: 0.00001510
Iteration 44/1000 | Loss: 0.00001510
Iteration 45/1000 | Loss: 0.00001509
Iteration 46/1000 | Loss: 0.00001509
Iteration 47/1000 | Loss: 0.00001507
Iteration 48/1000 | Loss: 0.00001507
Iteration 49/1000 | Loss: 0.00001507
Iteration 50/1000 | Loss: 0.00001507
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001507
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001505
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001504
Iteration 57/1000 | Loss: 0.00001504
Iteration 58/1000 | Loss: 0.00001504
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001502
Iteration 61/1000 | Loss: 0.00001502
Iteration 62/1000 | Loss: 0.00001502
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001501
Iteration 65/1000 | Loss: 0.00001501
Iteration 66/1000 | Loss: 0.00001500
Iteration 67/1000 | Loss: 0.00001500
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001500
Iteration 71/1000 | Loss: 0.00001500
Iteration 72/1000 | Loss: 0.00001500
Iteration 73/1000 | Loss: 0.00001499
Iteration 74/1000 | Loss: 0.00001499
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001499
Iteration 77/1000 | Loss: 0.00001498
Iteration 78/1000 | Loss: 0.00001498
Iteration 79/1000 | Loss: 0.00001498
Iteration 80/1000 | Loss: 0.00001498
Iteration 81/1000 | Loss: 0.00001498
Iteration 82/1000 | Loss: 0.00001498
Iteration 83/1000 | Loss: 0.00001498
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001497
Iteration 87/1000 | Loss: 0.00001497
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001496
Iteration 90/1000 | Loss: 0.00001496
Iteration 91/1000 | Loss: 0.00001496
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001496
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001495
Iteration 101/1000 | Loss: 0.00001495
Iteration 102/1000 | Loss: 0.00001495
Iteration 103/1000 | Loss: 0.00001495
Iteration 104/1000 | Loss: 0.00001495
Iteration 105/1000 | Loss: 0.00001495
Iteration 106/1000 | Loss: 0.00001495
Iteration 107/1000 | Loss: 0.00001495
Iteration 108/1000 | Loss: 0.00001495
Iteration 109/1000 | Loss: 0.00001495
Iteration 110/1000 | Loss: 0.00001495
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001495
Iteration 113/1000 | Loss: 0.00001495
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001494
Iteration 117/1000 | Loss: 0.00001494
Iteration 118/1000 | Loss: 0.00001494
Iteration 119/1000 | Loss: 0.00001494
Iteration 120/1000 | Loss: 0.00001494
Iteration 121/1000 | Loss: 0.00001494
Iteration 122/1000 | Loss: 0.00001494
Iteration 123/1000 | Loss: 0.00001493
Iteration 124/1000 | Loss: 0.00001493
Iteration 125/1000 | Loss: 0.00001493
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001493
Iteration 128/1000 | Loss: 0.00001493
Iteration 129/1000 | Loss: 0.00001493
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001493
Iteration 132/1000 | Loss: 0.00001493
Iteration 133/1000 | Loss: 0.00001493
Iteration 134/1000 | Loss: 0.00001493
Iteration 135/1000 | Loss: 0.00001493
Iteration 136/1000 | Loss: 0.00001493
Iteration 137/1000 | Loss: 0.00001492
Iteration 138/1000 | Loss: 0.00001492
Iteration 139/1000 | Loss: 0.00001492
Iteration 140/1000 | Loss: 0.00001492
Iteration 141/1000 | Loss: 0.00001492
Iteration 142/1000 | Loss: 0.00001492
Iteration 143/1000 | Loss: 0.00001492
Iteration 144/1000 | Loss: 0.00001492
Iteration 145/1000 | Loss: 0.00001492
Iteration 146/1000 | Loss: 0.00001492
Iteration 147/1000 | Loss: 0.00001492
Iteration 148/1000 | Loss: 0.00001492
Iteration 149/1000 | Loss: 0.00001492
Iteration 150/1000 | Loss: 0.00001492
Iteration 151/1000 | Loss: 0.00001491
Iteration 152/1000 | Loss: 0.00001491
Iteration 153/1000 | Loss: 0.00001491
Iteration 154/1000 | Loss: 0.00001491
Iteration 155/1000 | Loss: 0.00001491
Iteration 156/1000 | Loss: 0.00001491
Iteration 157/1000 | Loss: 0.00001491
Iteration 158/1000 | Loss: 0.00001491
Iteration 159/1000 | Loss: 0.00001491
Iteration 160/1000 | Loss: 0.00001491
Iteration 161/1000 | Loss: 0.00001491
Iteration 162/1000 | Loss: 0.00001490
Iteration 163/1000 | Loss: 0.00001490
Iteration 164/1000 | Loss: 0.00001490
Iteration 165/1000 | Loss: 0.00001490
Iteration 166/1000 | Loss: 0.00001490
Iteration 167/1000 | Loss: 0.00001490
Iteration 168/1000 | Loss: 0.00001490
Iteration 169/1000 | Loss: 0.00001490
Iteration 170/1000 | Loss: 0.00001490
Iteration 171/1000 | Loss: 0.00001490
Iteration 172/1000 | Loss: 0.00001490
Iteration 173/1000 | Loss: 0.00001490
Iteration 174/1000 | Loss: 0.00001490
Iteration 175/1000 | Loss: 0.00001490
Iteration 176/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4901769645803142e-05, 1.4901769645803142e-05, 1.4901769645803142e-05, 1.4901769645803142e-05, 1.4901769645803142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4901769645803142e-05

Optimization complete. Final v2v error: 3.254080057144165 mm

Highest mean error: 3.5944337844848633 mm for frame 87

Lowest mean error: 2.980172634124756 mm for frame 15

Saving results

Total time: 38.05069851875305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067514
Iteration 2/25 | Loss: 0.00205363
Iteration 3/25 | Loss: 0.00147870
Iteration 4/25 | Loss: 0.00132223
Iteration 5/25 | Loss: 0.00127382
Iteration 6/25 | Loss: 0.00095021
Iteration 7/25 | Loss: 0.00085355
Iteration 8/25 | Loss: 0.00083506
Iteration 9/25 | Loss: 0.00083609
Iteration 10/25 | Loss: 0.00080386
Iteration 11/25 | Loss: 0.00078910
Iteration 12/25 | Loss: 0.00078471
Iteration 13/25 | Loss: 0.00078363
Iteration 14/25 | Loss: 0.00078348
Iteration 15/25 | Loss: 0.00078348
Iteration 16/25 | Loss: 0.00078348
Iteration 17/25 | Loss: 0.00078348
Iteration 18/25 | Loss: 0.00078348
Iteration 19/25 | Loss: 0.00078348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007834751158952713, 0.0007834751158952713, 0.0007834751158952713, 0.0007834751158952713, 0.0007834751158952713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007834751158952713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44607162
Iteration 2/25 | Loss: 0.00027181
Iteration 3/25 | Loss: 0.00027181
Iteration 4/25 | Loss: 0.00027181
Iteration 5/25 | Loss: 0.00027181
Iteration 6/25 | Loss: 0.00027181
Iteration 7/25 | Loss: 0.00027181
Iteration 8/25 | Loss: 0.00027181
Iteration 9/25 | Loss: 0.00027181
Iteration 10/25 | Loss: 0.00027181
Iteration 11/25 | Loss: 0.00027181
Iteration 12/25 | Loss: 0.00027181
Iteration 13/25 | Loss: 0.00027181
Iteration 14/25 | Loss: 0.00027181
Iteration 15/25 | Loss: 0.00027181
Iteration 16/25 | Loss: 0.00027181
Iteration 17/25 | Loss: 0.00027181
Iteration 18/25 | Loss: 0.00027181
Iteration 19/25 | Loss: 0.00027181
Iteration 20/25 | Loss: 0.00027181
Iteration 21/25 | Loss: 0.00027181
Iteration 22/25 | Loss: 0.00027181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00027180981123819947, 0.00027180981123819947, 0.00027180981123819947, 0.00027180981123819947, 0.00027180981123819947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027180981123819947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027181
Iteration 2/1000 | Loss: 0.00004093
Iteration 3/1000 | Loss: 0.00002997
Iteration 4/1000 | Loss: 0.00002702
Iteration 5/1000 | Loss: 0.00002534
Iteration 6/1000 | Loss: 0.00002449
Iteration 7/1000 | Loss: 0.00002395
Iteration 8/1000 | Loss: 0.00002359
Iteration 9/1000 | Loss: 0.00002331
Iteration 10/1000 | Loss: 0.00002308
Iteration 11/1000 | Loss: 0.00002296
Iteration 12/1000 | Loss: 0.00002295
Iteration 13/1000 | Loss: 0.00002292
Iteration 14/1000 | Loss: 0.00002292
Iteration 15/1000 | Loss: 0.00002290
Iteration 16/1000 | Loss: 0.00002288
Iteration 17/1000 | Loss: 0.00002288
Iteration 18/1000 | Loss: 0.00002288
Iteration 19/1000 | Loss: 0.00002288
Iteration 20/1000 | Loss: 0.00002287
Iteration 21/1000 | Loss: 0.00002287
Iteration 22/1000 | Loss: 0.00002280
Iteration 23/1000 | Loss: 0.00002280
Iteration 24/1000 | Loss: 0.00002280
Iteration 25/1000 | Loss: 0.00002280
Iteration 26/1000 | Loss: 0.00002279
Iteration 27/1000 | Loss: 0.00002279
Iteration 28/1000 | Loss: 0.00002279
Iteration 29/1000 | Loss: 0.00002279
Iteration 30/1000 | Loss: 0.00002279
Iteration 31/1000 | Loss: 0.00002279
Iteration 32/1000 | Loss: 0.00002279
Iteration 33/1000 | Loss: 0.00002279
Iteration 34/1000 | Loss: 0.00002278
Iteration 35/1000 | Loss: 0.00002278
Iteration 36/1000 | Loss: 0.00002277
Iteration 37/1000 | Loss: 0.00002276
Iteration 38/1000 | Loss: 0.00002276
Iteration 39/1000 | Loss: 0.00002276
Iteration 40/1000 | Loss: 0.00002275
Iteration 41/1000 | Loss: 0.00002275
Iteration 42/1000 | Loss: 0.00002274
Iteration 43/1000 | Loss: 0.00002273
Iteration 44/1000 | Loss: 0.00002271
Iteration 45/1000 | Loss: 0.00002271
Iteration 46/1000 | Loss: 0.00002271
Iteration 47/1000 | Loss: 0.00002271
Iteration 48/1000 | Loss: 0.00002271
Iteration 49/1000 | Loss: 0.00002271
Iteration 50/1000 | Loss: 0.00002271
Iteration 51/1000 | Loss: 0.00002270
Iteration 52/1000 | Loss: 0.00002270
Iteration 53/1000 | Loss: 0.00002269
Iteration 54/1000 | Loss: 0.00002269
Iteration 55/1000 | Loss: 0.00002269
Iteration 56/1000 | Loss: 0.00002269
Iteration 57/1000 | Loss: 0.00002269
Iteration 58/1000 | Loss: 0.00002269
Iteration 59/1000 | Loss: 0.00002269
Iteration 60/1000 | Loss: 0.00002268
Iteration 61/1000 | Loss: 0.00002268
Iteration 62/1000 | Loss: 0.00002268
Iteration 63/1000 | Loss: 0.00002268
Iteration 64/1000 | Loss: 0.00002268
Iteration 65/1000 | Loss: 0.00002268
Iteration 66/1000 | Loss: 0.00002268
Iteration 67/1000 | Loss: 0.00002267
Iteration 68/1000 | Loss: 0.00002267
Iteration 69/1000 | Loss: 0.00002267
Iteration 70/1000 | Loss: 0.00002266
Iteration 71/1000 | Loss: 0.00002266
Iteration 72/1000 | Loss: 0.00002266
Iteration 73/1000 | Loss: 0.00002266
Iteration 74/1000 | Loss: 0.00002266
Iteration 75/1000 | Loss: 0.00002266
Iteration 76/1000 | Loss: 0.00002266
Iteration 77/1000 | Loss: 0.00002266
Iteration 78/1000 | Loss: 0.00002266
Iteration 79/1000 | Loss: 0.00002266
Iteration 80/1000 | Loss: 0.00002265
Iteration 81/1000 | Loss: 0.00002265
Iteration 82/1000 | Loss: 0.00002265
Iteration 83/1000 | Loss: 0.00002265
Iteration 84/1000 | Loss: 0.00002265
Iteration 85/1000 | Loss: 0.00002265
Iteration 86/1000 | Loss: 0.00002265
Iteration 87/1000 | Loss: 0.00002265
Iteration 88/1000 | Loss: 0.00002265
Iteration 89/1000 | Loss: 0.00002265
Iteration 90/1000 | Loss: 0.00002265
Iteration 91/1000 | Loss: 0.00002265
Iteration 92/1000 | Loss: 0.00002265
Iteration 93/1000 | Loss: 0.00002265
Iteration 94/1000 | Loss: 0.00002265
Iteration 95/1000 | Loss: 0.00002265
Iteration 96/1000 | Loss: 0.00002265
Iteration 97/1000 | Loss: 0.00002265
Iteration 98/1000 | Loss: 0.00002265
Iteration 99/1000 | Loss: 0.00002265
Iteration 100/1000 | Loss: 0.00002265
Iteration 101/1000 | Loss: 0.00002265
Iteration 102/1000 | Loss: 0.00002265
Iteration 103/1000 | Loss: 0.00002265
Iteration 104/1000 | Loss: 0.00002265
Iteration 105/1000 | Loss: 0.00002265
Iteration 106/1000 | Loss: 0.00002265
Iteration 107/1000 | Loss: 0.00002265
Iteration 108/1000 | Loss: 0.00002265
Iteration 109/1000 | Loss: 0.00002265
Iteration 110/1000 | Loss: 0.00002265
Iteration 111/1000 | Loss: 0.00002265
Iteration 112/1000 | Loss: 0.00002265
Iteration 113/1000 | Loss: 0.00002265
Iteration 114/1000 | Loss: 0.00002265
Iteration 115/1000 | Loss: 0.00002265
Iteration 116/1000 | Loss: 0.00002265
Iteration 117/1000 | Loss: 0.00002265
Iteration 118/1000 | Loss: 0.00002265
Iteration 119/1000 | Loss: 0.00002265
Iteration 120/1000 | Loss: 0.00002265
Iteration 121/1000 | Loss: 0.00002265
Iteration 122/1000 | Loss: 0.00002265
Iteration 123/1000 | Loss: 0.00002265
Iteration 124/1000 | Loss: 0.00002265
Iteration 125/1000 | Loss: 0.00002265
Iteration 126/1000 | Loss: 0.00002265
Iteration 127/1000 | Loss: 0.00002265
Iteration 128/1000 | Loss: 0.00002265
Iteration 129/1000 | Loss: 0.00002265
Iteration 130/1000 | Loss: 0.00002265
Iteration 131/1000 | Loss: 0.00002265
Iteration 132/1000 | Loss: 0.00002265
Iteration 133/1000 | Loss: 0.00002265
Iteration 134/1000 | Loss: 0.00002265
Iteration 135/1000 | Loss: 0.00002265
Iteration 136/1000 | Loss: 0.00002265
Iteration 137/1000 | Loss: 0.00002265
Iteration 138/1000 | Loss: 0.00002265
Iteration 139/1000 | Loss: 0.00002265
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00002265
Iteration 142/1000 | Loss: 0.00002265
Iteration 143/1000 | Loss: 0.00002265
Iteration 144/1000 | Loss: 0.00002265
Iteration 145/1000 | Loss: 0.00002265
Iteration 146/1000 | Loss: 0.00002265
Iteration 147/1000 | Loss: 0.00002265
Iteration 148/1000 | Loss: 0.00002265
Iteration 149/1000 | Loss: 0.00002265
Iteration 150/1000 | Loss: 0.00002265
Iteration 151/1000 | Loss: 0.00002265
Iteration 152/1000 | Loss: 0.00002265
Iteration 153/1000 | Loss: 0.00002265
Iteration 154/1000 | Loss: 0.00002265
Iteration 155/1000 | Loss: 0.00002265
Iteration 156/1000 | Loss: 0.00002265
Iteration 157/1000 | Loss: 0.00002265
Iteration 158/1000 | Loss: 0.00002265
Iteration 159/1000 | Loss: 0.00002265
Iteration 160/1000 | Loss: 0.00002265
Iteration 161/1000 | Loss: 0.00002265
Iteration 162/1000 | Loss: 0.00002265
Iteration 163/1000 | Loss: 0.00002265
Iteration 164/1000 | Loss: 0.00002265
Iteration 165/1000 | Loss: 0.00002265
Iteration 166/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.2646594516118057e-05, 2.2646594516118057e-05, 2.2646594516118057e-05, 2.2646594516118057e-05, 2.2646594516118057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2646594516118057e-05

Optimization complete. Final v2v error: 3.968369245529175 mm

Highest mean error: 4.559144496917725 mm for frame 112

Lowest mean error: 3.4266767501831055 mm for frame 200

Saving results

Total time: 53.957982301712036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390966
Iteration 2/25 | Loss: 0.00074866
Iteration 3/25 | Loss: 0.00063924
Iteration 4/25 | Loss: 0.00061774
Iteration 5/25 | Loss: 0.00060912
Iteration 6/25 | Loss: 0.00060757
Iteration 7/25 | Loss: 0.00060717
Iteration 8/25 | Loss: 0.00060717
Iteration 9/25 | Loss: 0.00060717
Iteration 10/25 | Loss: 0.00060717
Iteration 11/25 | Loss: 0.00060717
Iteration 12/25 | Loss: 0.00060717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006071688258089125, 0.0006071688258089125, 0.0006071688258089125, 0.0006071688258089125, 0.0006071688258089125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006071688258089125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94920039
Iteration 2/25 | Loss: 0.00027449
Iteration 3/25 | Loss: 0.00027449
Iteration 4/25 | Loss: 0.00027449
Iteration 5/25 | Loss: 0.00027449
Iteration 6/25 | Loss: 0.00027449
Iteration 7/25 | Loss: 0.00027449
Iteration 8/25 | Loss: 0.00027449
Iteration 9/25 | Loss: 0.00027449
Iteration 10/25 | Loss: 0.00027449
Iteration 11/25 | Loss: 0.00027449
Iteration 12/25 | Loss: 0.00027449
Iteration 13/25 | Loss: 0.00027449
Iteration 14/25 | Loss: 0.00027449
Iteration 15/25 | Loss: 0.00027449
Iteration 16/25 | Loss: 0.00027449
Iteration 17/25 | Loss: 0.00027449
Iteration 18/25 | Loss: 0.00027449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002744889643508941, 0.0002744889643508941, 0.0002744889643508941, 0.0002744889643508941, 0.0002744889643508941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002744889643508941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027449
Iteration 2/1000 | Loss: 0.00002583
Iteration 3/1000 | Loss: 0.00001816
Iteration 4/1000 | Loss: 0.00001685
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001544
Iteration 7/1000 | Loss: 0.00001498
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001466
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001450
Iteration 12/1000 | Loss: 0.00001449
Iteration 13/1000 | Loss: 0.00001445
Iteration 14/1000 | Loss: 0.00001444
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001436
Iteration 18/1000 | Loss: 0.00001436
Iteration 19/1000 | Loss: 0.00001436
Iteration 20/1000 | Loss: 0.00001435
Iteration 21/1000 | Loss: 0.00001435
Iteration 22/1000 | Loss: 0.00001435
Iteration 23/1000 | Loss: 0.00001435
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00001434
Iteration 28/1000 | Loss: 0.00001431
Iteration 29/1000 | Loss: 0.00001430
Iteration 30/1000 | Loss: 0.00001430
Iteration 31/1000 | Loss: 0.00001430
Iteration 32/1000 | Loss: 0.00001430
Iteration 33/1000 | Loss: 0.00001430
Iteration 34/1000 | Loss: 0.00001429
Iteration 35/1000 | Loss: 0.00001429
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001427
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001426
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001424
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001423
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001422
Iteration 60/1000 | Loss: 0.00001422
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001421
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001420
Iteration 69/1000 | Loss: 0.00001420
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001419
Iteration 72/1000 | Loss: 0.00001419
Iteration 73/1000 | Loss: 0.00001419
Iteration 74/1000 | Loss: 0.00001419
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001416
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001415
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001413
Iteration 87/1000 | Loss: 0.00001413
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001413
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001411
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001410
Iteration 101/1000 | Loss: 0.00001410
Iteration 102/1000 | Loss: 0.00001410
Iteration 103/1000 | Loss: 0.00001410
Iteration 104/1000 | Loss: 0.00001410
Iteration 105/1000 | Loss: 0.00001410
Iteration 106/1000 | Loss: 0.00001409
Iteration 107/1000 | Loss: 0.00001409
Iteration 108/1000 | Loss: 0.00001409
Iteration 109/1000 | Loss: 0.00001409
Iteration 110/1000 | Loss: 0.00001409
Iteration 111/1000 | Loss: 0.00001408
Iteration 112/1000 | Loss: 0.00001408
Iteration 113/1000 | Loss: 0.00001408
Iteration 114/1000 | Loss: 0.00001408
Iteration 115/1000 | Loss: 0.00001408
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001407
Iteration 122/1000 | Loss: 0.00001407
Iteration 123/1000 | Loss: 0.00001407
Iteration 124/1000 | Loss: 0.00001407
Iteration 125/1000 | Loss: 0.00001406
Iteration 126/1000 | Loss: 0.00001406
Iteration 127/1000 | Loss: 0.00001406
Iteration 128/1000 | Loss: 0.00001406
Iteration 129/1000 | Loss: 0.00001406
Iteration 130/1000 | Loss: 0.00001406
Iteration 131/1000 | Loss: 0.00001405
Iteration 132/1000 | Loss: 0.00001405
Iteration 133/1000 | Loss: 0.00001405
Iteration 134/1000 | Loss: 0.00001405
Iteration 135/1000 | Loss: 0.00001405
Iteration 136/1000 | Loss: 0.00001405
Iteration 137/1000 | Loss: 0.00001405
Iteration 138/1000 | Loss: 0.00001405
Iteration 139/1000 | Loss: 0.00001405
Iteration 140/1000 | Loss: 0.00001405
Iteration 141/1000 | Loss: 0.00001405
Iteration 142/1000 | Loss: 0.00001405
Iteration 143/1000 | Loss: 0.00001404
Iteration 144/1000 | Loss: 0.00001404
Iteration 145/1000 | Loss: 0.00001404
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001404
Iteration 149/1000 | Loss: 0.00001404
Iteration 150/1000 | Loss: 0.00001404
Iteration 151/1000 | Loss: 0.00001404
Iteration 152/1000 | Loss: 0.00001404
Iteration 153/1000 | Loss: 0.00001404
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001404
Iteration 156/1000 | Loss: 0.00001404
Iteration 157/1000 | Loss: 0.00001404
Iteration 158/1000 | Loss: 0.00001404
Iteration 159/1000 | Loss: 0.00001404
Iteration 160/1000 | Loss: 0.00001404
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001403
Iteration 168/1000 | Loss: 0.00001403
Iteration 169/1000 | Loss: 0.00001403
Iteration 170/1000 | Loss: 0.00001403
Iteration 171/1000 | Loss: 0.00001403
Iteration 172/1000 | Loss: 0.00001403
Iteration 173/1000 | Loss: 0.00001403
Iteration 174/1000 | Loss: 0.00001403
Iteration 175/1000 | Loss: 0.00001402
Iteration 176/1000 | Loss: 0.00001402
Iteration 177/1000 | Loss: 0.00001402
Iteration 178/1000 | Loss: 0.00001402
Iteration 179/1000 | Loss: 0.00001402
Iteration 180/1000 | Loss: 0.00001402
Iteration 181/1000 | Loss: 0.00001402
Iteration 182/1000 | Loss: 0.00001402
Iteration 183/1000 | Loss: 0.00001402
Iteration 184/1000 | Loss: 0.00001402
Iteration 185/1000 | Loss: 0.00001402
Iteration 186/1000 | Loss: 0.00001402
Iteration 187/1000 | Loss: 0.00001402
Iteration 188/1000 | Loss: 0.00001402
Iteration 189/1000 | Loss: 0.00001402
Iteration 190/1000 | Loss: 0.00001402
Iteration 191/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.4024567462911364e-05, 1.4024567462911364e-05, 1.4024567462911364e-05, 1.4024567462911364e-05, 1.4024567462911364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4024567462911364e-05

Optimization complete. Final v2v error: 3.1671459674835205 mm

Highest mean error: 3.756464719772339 mm for frame 55

Lowest mean error: 2.916533946990967 mm for frame 82

Saving results

Total time: 36.475297689437866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425741
Iteration 2/25 | Loss: 0.00104506
Iteration 3/25 | Loss: 0.00077492
Iteration 4/25 | Loss: 0.00073726
Iteration 5/25 | Loss: 0.00072390
Iteration 6/25 | Loss: 0.00072188
Iteration 7/25 | Loss: 0.00072112
Iteration 8/25 | Loss: 0.00072106
Iteration 9/25 | Loss: 0.00072106
Iteration 10/25 | Loss: 0.00072106
Iteration 11/25 | Loss: 0.00072106
Iteration 12/25 | Loss: 0.00072106
Iteration 13/25 | Loss: 0.00072106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007210623589344323, 0.0007210623589344323, 0.0007210623589344323, 0.0007210623589344323, 0.0007210623589344323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007210623589344323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.04098845
Iteration 2/25 | Loss: 0.00036947
Iteration 3/25 | Loss: 0.00036943
Iteration 4/25 | Loss: 0.00036943
Iteration 5/25 | Loss: 0.00036943
Iteration 6/25 | Loss: 0.00036943
Iteration 7/25 | Loss: 0.00036943
Iteration 8/25 | Loss: 0.00036943
Iteration 9/25 | Loss: 0.00036943
Iteration 10/25 | Loss: 0.00036943
Iteration 11/25 | Loss: 0.00036943
Iteration 12/25 | Loss: 0.00036943
Iteration 13/25 | Loss: 0.00036943
Iteration 14/25 | Loss: 0.00036943
Iteration 15/25 | Loss: 0.00036943
Iteration 16/25 | Loss: 0.00036943
Iteration 17/25 | Loss: 0.00036943
Iteration 18/25 | Loss: 0.00036943
Iteration 19/25 | Loss: 0.00036943
Iteration 20/25 | Loss: 0.00036943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003694313927553594, 0.0003694313927553594, 0.0003694313927553594, 0.0003694313927553594, 0.0003694313927553594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003694313927553594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036943
Iteration 2/1000 | Loss: 0.00004480
Iteration 3/1000 | Loss: 0.00002984
Iteration 4/1000 | Loss: 0.00002764
Iteration 5/1000 | Loss: 0.00002646
Iteration 6/1000 | Loss: 0.00002535
Iteration 7/1000 | Loss: 0.00002459
Iteration 8/1000 | Loss: 0.00002419
Iteration 9/1000 | Loss: 0.00002392
Iteration 10/1000 | Loss: 0.00002371
Iteration 11/1000 | Loss: 0.00002355
Iteration 12/1000 | Loss: 0.00002350
Iteration 13/1000 | Loss: 0.00002347
Iteration 14/1000 | Loss: 0.00002346
Iteration 15/1000 | Loss: 0.00002345
Iteration 16/1000 | Loss: 0.00002341
Iteration 17/1000 | Loss: 0.00002338
Iteration 18/1000 | Loss: 0.00002338
Iteration 19/1000 | Loss: 0.00002336
Iteration 20/1000 | Loss: 0.00002336
Iteration 21/1000 | Loss: 0.00002336
Iteration 22/1000 | Loss: 0.00002336
Iteration 23/1000 | Loss: 0.00002336
Iteration 24/1000 | Loss: 0.00002336
Iteration 25/1000 | Loss: 0.00002336
Iteration 26/1000 | Loss: 0.00002336
Iteration 27/1000 | Loss: 0.00002335
Iteration 28/1000 | Loss: 0.00002335
Iteration 29/1000 | Loss: 0.00002334
Iteration 30/1000 | Loss: 0.00002334
Iteration 31/1000 | Loss: 0.00002334
Iteration 32/1000 | Loss: 0.00002334
Iteration 33/1000 | Loss: 0.00002334
Iteration 34/1000 | Loss: 0.00002333
Iteration 35/1000 | Loss: 0.00002333
Iteration 36/1000 | Loss: 0.00002333
Iteration 37/1000 | Loss: 0.00002333
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002331
Iteration 43/1000 | Loss: 0.00002331
Iteration 44/1000 | Loss: 0.00002330
Iteration 45/1000 | Loss: 0.00002330
Iteration 46/1000 | Loss: 0.00002330
Iteration 47/1000 | Loss: 0.00002328
Iteration 48/1000 | Loss: 0.00002328
Iteration 49/1000 | Loss: 0.00002328
Iteration 50/1000 | Loss: 0.00002328
Iteration 51/1000 | Loss: 0.00002328
Iteration 52/1000 | Loss: 0.00002328
Iteration 53/1000 | Loss: 0.00002328
Iteration 54/1000 | Loss: 0.00002328
Iteration 55/1000 | Loss: 0.00002328
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002328
Iteration 58/1000 | Loss: 0.00002327
Iteration 59/1000 | Loss: 0.00002327
Iteration 60/1000 | Loss: 0.00002327
Iteration 61/1000 | Loss: 0.00002327
Iteration 62/1000 | Loss: 0.00002327
Iteration 63/1000 | Loss: 0.00002327
Iteration 64/1000 | Loss: 0.00002327
Iteration 65/1000 | Loss: 0.00002327
Iteration 66/1000 | Loss: 0.00002327
Iteration 67/1000 | Loss: 0.00002327
Iteration 68/1000 | Loss: 0.00002326
Iteration 69/1000 | Loss: 0.00002326
Iteration 70/1000 | Loss: 0.00002326
Iteration 71/1000 | Loss: 0.00002326
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002326
Iteration 76/1000 | Loss: 0.00002326
Iteration 77/1000 | Loss: 0.00002326
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002325
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002325
Iteration 84/1000 | Loss: 0.00002325
Iteration 85/1000 | Loss: 0.00002324
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002324
Iteration 90/1000 | Loss: 0.00002324
Iteration 91/1000 | Loss: 0.00002324
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002322
Iteration 97/1000 | Loss: 0.00002322
Iteration 98/1000 | Loss: 0.00002322
Iteration 99/1000 | Loss: 0.00002321
Iteration 100/1000 | Loss: 0.00002321
Iteration 101/1000 | Loss: 0.00002321
Iteration 102/1000 | Loss: 0.00002321
Iteration 103/1000 | Loss: 0.00002320
Iteration 104/1000 | Loss: 0.00002320
Iteration 105/1000 | Loss: 0.00002320
Iteration 106/1000 | Loss: 0.00002320
Iteration 107/1000 | Loss: 0.00002320
Iteration 108/1000 | Loss: 0.00002319
Iteration 109/1000 | Loss: 0.00002319
Iteration 110/1000 | Loss: 0.00002319
Iteration 111/1000 | Loss: 0.00002319
Iteration 112/1000 | Loss: 0.00002319
Iteration 113/1000 | Loss: 0.00002319
Iteration 114/1000 | Loss: 0.00002319
Iteration 115/1000 | Loss: 0.00002319
Iteration 116/1000 | Loss: 0.00002318
Iteration 117/1000 | Loss: 0.00002318
Iteration 118/1000 | Loss: 0.00002318
Iteration 119/1000 | Loss: 0.00002318
Iteration 120/1000 | Loss: 0.00002318
Iteration 121/1000 | Loss: 0.00002318
Iteration 122/1000 | Loss: 0.00002318
Iteration 123/1000 | Loss: 0.00002318
Iteration 124/1000 | Loss: 0.00002317
Iteration 125/1000 | Loss: 0.00002317
Iteration 126/1000 | Loss: 0.00002317
Iteration 127/1000 | Loss: 0.00002317
Iteration 128/1000 | Loss: 0.00002317
Iteration 129/1000 | Loss: 0.00002317
Iteration 130/1000 | Loss: 0.00002317
Iteration 131/1000 | Loss: 0.00002317
Iteration 132/1000 | Loss: 0.00002317
Iteration 133/1000 | Loss: 0.00002317
Iteration 134/1000 | Loss: 0.00002316
Iteration 135/1000 | Loss: 0.00002316
Iteration 136/1000 | Loss: 0.00002316
Iteration 137/1000 | Loss: 0.00002316
Iteration 138/1000 | Loss: 0.00002316
Iteration 139/1000 | Loss: 0.00002316
Iteration 140/1000 | Loss: 0.00002316
Iteration 141/1000 | Loss: 0.00002316
Iteration 142/1000 | Loss: 0.00002316
Iteration 143/1000 | Loss: 0.00002315
Iteration 144/1000 | Loss: 0.00002315
Iteration 145/1000 | Loss: 0.00002315
Iteration 146/1000 | Loss: 0.00002315
Iteration 147/1000 | Loss: 0.00002315
Iteration 148/1000 | Loss: 0.00002315
Iteration 149/1000 | Loss: 0.00002315
Iteration 150/1000 | Loss: 0.00002315
Iteration 151/1000 | Loss: 0.00002315
Iteration 152/1000 | Loss: 0.00002315
Iteration 153/1000 | Loss: 0.00002315
Iteration 154/1000 | Loss: 0.00002315
Iteration 155/1000 | Loss: 0.00002315
Iteration 156/1000 | Loss: 0.00002315
Iteration 157/1000 | Loss: 0.00002315
Iteration 158/1000 | Loss: 0.00002315
Iteration 159/1000 | Loss: 0.00002315
Iteration 160/1000 | Loss: 0.00002315
Iteration 161/1000 | Loss: 0.00002315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.3152790163294412e-05, 2.3152790163294412e-05, 2.3152790163294412e-05, 2.3152790163294412e-05, 2.3152790163294412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3152790163294412e-05

Optimization complete. Final v2v error: 4.038858890533447 mm

Highest mean error: 4.56156063079834 mm for frame 59

Lowest mean error: 3.6898863315582275 mm for frame 38

Saving results

Total time: 36.477513790130615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474784
Iteration 2/25 | Loss: 0.00072010
Iteration 3/25 | Loss: 0.00063374
Iteration 4/25 | Loss: 0.00061956
Iteration 5/25 | Loss: 0.00061432
Iteration 6/25 | Loss: 0.00061306
Iteration 7/25 | Loss: 0.00061277
Iteration 8/25 | Loss: 0.00061277
Iteration 9/25 | Loss: 0.00061277
Iteration 10/25 | Loss: 0.00061277
Iteration 11/25 | Loss: 0.00061277
Iteration 12/25 | Loss: 0.00061277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006127716042101383, 0.0006127716042101383, 0.0006127716042101383, 0.0006127716042101383, 0.0006127716042101383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006127716042101383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76326370
Iteration 2/25 | Loss: 0.00033414
Iteration 3/25 | Loss: 0.00033413
Iteration 4/25 | Loss: 0.00033413
Iteration 5/25 | Loss: 0.00033413
Iteration 6/25 | Loss: 0.00033413
Iteration 7/25 | Loss: 0.00033413
Iteration 8/25 | Loss: 0.00033413
Iteration 9/25 | Loss: 0.00033413
Iteration 10/25 | Loss: 0.00033413
Iteration 11/25 | Loss: 0.00033413
Iteration 12/25 | Loss: 0.00033413
Iteration 13/25 | Loss: 0.00033413
Iteration 14/25 | Loss: 0.00033413
Iteration 15/25 | Loss: 0.00033413
Iteration 16/25 | Loss: 0.00033413
Iteration 17/25 | Loss: 0.00033413
Iteration 18/25 | Loss: 0.00033413
Iteration 19/25 | Loss: 0.00033413
Iteration 20/25 | Loss: 0.00033413
Iteration 21/25 | Loss: 0.00033413
Iteration 22/25 | Loss: 0.00033413
Iteration 23/25 | Loss: 0.00033413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00033413025084882975, 0.00033413025084882975, 0.00033413025084882975, 0.00033413025084882975, 0.00033413025084882975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033413025084882975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033413
Iteration 2/1000 | Loss: 0.00002597
Iteration 3/1000 | Loss: 0.00001672
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001474
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001394
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001370
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00001348
Iteration 13/1000 | Loss: 0.00001346
Iteration 14/1000 | Loss: 0.00001345
Iteration 15/1000 | Loss: 0.00001345
Iteration 16/1000 | Loss: 0.00001344
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001343
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001341
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001340
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001337
Iteration 25/1000 | Loss: 0.00001336
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001334
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001332
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001328
Iteration 37/1000 | Loss: 0.00001328
Iteration 38/1000 | Loss: 0.00001327
Iteration 39/1000 | Loss: 0.00001326
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001325
Iteration 42/1000 | Loss: 0.00001325
Iteration 43/1000 | Loss: 0.00001324
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001323
Iteration 46/1000 | Loss: 0.00001323
Iteration 47/1000 | Loss: 0.00001323
Iteration 48/1000 | Loss: 0.00001322
Iteration 49/1000 | Loss: 0.00001322
Iteration 50/1000 | Loss: 0.00001322
Iteration 51/1000 | Loss: 0.00001322
Iteration 52/1000 | Loss: 0.00001322
Iteration 53/1000 | Loss: 0.00001321
Iteration 54/1000 | Loss: 0.00001321
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001319
Iteration 61/1000 | Loss: 0.00001319
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001316
Iteration 80/1000 | Loss: 0.00001316
Iteration 81/1000 | Loss: 0.00001316
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001316
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001316
Iteration 98/1000 | Loss: 0.00001316
Iteration 99/1000 | Loss: 0.00001316
Iteration 100/1000 | Loss: 0.00001316
Iteration 101/1000 | Loss: 0.00001316
Iteration 102/1000 | Loss: 0.00001316
Iteration 103/1000 | Loss: 0.00001316
Iteration 104/1000 | Loss: 0.00001316
Iteration 105/1000 | Loss: 0.00001316
Iteration 106/1000 | Loss: 0.00001316
Iteration 107/1000 | Loss: 0.00001316
Iteration 108/1000 | Loss: 0.00001316
Iteration 109/1000 | Loss: 0.00001316
Iteration 110/1000 | Loss: 0.00001316
Iteration 111/1000 | Loss: 0.00001316
Iteration 112/1000 | Loss: 0.00001316
Iteration 113/1000 | Loss: 0.00001316
Iteration 114/1000 | Loss: 0.00001316
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001316
Iteration 117/1000 | Loss: 0.00001316
Iteration 118/1000 | Loss: 0.00001316
Iteration 119/1000 | Loss: 0.00001316
Iteration 120/1000 | Loss: 0.00001316
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001316
Iteration 123/1000 | Loss: 0.00001316
Iteration 124/1000 | Loss: 0.00001316
Iteration 125/1000 | Loss: 0.00001316
Iteration 126/1000 | Loss: 0.00001316
Iteration 127/1000 | Loss: 0.00001316
Iteration 128/1000 | Loss: 0.00001316
Iteration 129/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.3161718015908264e-05, 1.3161718015908264e-05, 1.3161718015908264e-05, 1.3161718015908264e-05, 1.3161718015908264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3161718015908264e-05

Optimization complete. Final v2v error: 3.0721468925476074 mm

Highest mean error: 3.3701863288879395 mm for frame 115

Lowest mean error: 2.846703052520752 mm for frame 88

Saving results

Total time: 29.74647045135498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358378
Iteration 2/25 | Loss: 0.00076038
Iteration 3/25 | Loss: 0.00067783
Iteration 4/25 | Loss: 0.00065324
Iteration 5/25 | Loss: 0.00064542
Iteration 6/25 | Loss: 0.00064404
Iteration 7/25 | Loss: 0.00064347
Iteration 8/25 | Loss: 0.00064347
Iteration 9/25 | Loss: 0.00064347
Iteration 10/25 | Loss: 0.00064347
Iteration 11/25 | Loss: 0.00064347
Iteration 12/25 | Loss: 0.00064347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006434689857997, 0.0006434689857997, 0.0006434689857997, 0.0006434689857997, 0.0006434689857997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006434689857997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47134495
Iteration 2/25 | Loss: 0.00031566
Iteration 3/25 | Loss: 0.00031566
Iteration 4/25 | Loss: 0.00031566
Iteration 5/25 | Loss: 0.00031566
Iteration 6/25 | Loss: 0.00031565
Iteration 7/25 | Loss: 0.00031565
Iteration 8/25 | Loss: 0.00031565
Iteration 9/25 | Loss: 0.00031565
Iteration 10/25 | Loss: 0.00031565
Iteration 11/25 | Loss: 0.00031565
Iteration 12/25 | Loss: 0.00031565
Iteration 13/25 | Loss: 0.00031565
Iteration 14/25 | Loss: 0.00031565
Iteration 15/25 | Loss: 0.00031565
Iteration 16/25 | Loss: 0.00031565
Iteration 17/25 | Loss: 0.00031565
Iteration 18/25 | Loss: 0.00031565
Iteration 19/25 | Loss: 0.00031565
Iteration 20/25 | Loss: 0.00031565
Iteration 21/25 | Loss: 0.00031565
Iteration 22/25 | Loss: 0.00031565
Iteration 23/25 | Loss: 0.00031565
Iteration 24/25 | Loss: 0.00031565
Iteration 25/25 | Loss: 0.00031565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031565
Iteration 2/1000 | Loss: 0.00002654
Iteration 3/1000 | Loss: 0.00002001
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001875
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001760
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001751
Iteration 16/1000 | Loss: 0.00001751
Iteration 17/1000 | Loss: 0.00001748
Iteration 18/1000 | Loss: 0.00001748
Iteration 19/1000 | Loss: 0.00001748
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001748
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001748
Iteration 26/1000 | Loss: 0.00001748
Iteration 27/1000 | Loss: 0.00001747
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001745
Iteration 36/1000 | Loss: 0.00001745
Iteration 37/1000 | Loss: 0.00001744
Iteration 38/1000 | Loss: 0.00001744
Iteration 39/1000 | Loss: 0.00001743
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001743
Iteration 42/1000 | Loss: 0.00001743
Iteration 43/1000 | Loss: 0.00001743
Iteration 44/1000 | Loss: 0.00001743
Iteration 45/1000 | Loss: 0.00001743
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001742
Iteration 48/1000 | Loss: 0.00001742
Iteration 49/1000 | Loss: 0.00001742
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001741
Iteration 52/1000 | Loss: 0.00001741
Iteration 53/1000 | Loss: 0.00001741
Iteration 54/1000 | Loss: 0.00001741
Iteration 55/1000 | Loss: 0.00001741
Iteration 56/1000 | Loss: 0.00001740
Iteration 57/1000 | Loss: 0.00001740
Iteration 58/1000 | Loss: 0.00001740
Iteration 59/1000 | Loss: 0.00001740
Iteration 60/1000 | Loss: 0.00001740
Iteration 61/1000 | Loss: 0.00001739
Iteration 62/1000 | Loss: 0.00001739
Iteration 63/1000 | Loss: 0.00001739
Iteration 64/1000 | Loss: 0.00001739
Iteration 65/1000 | Loss: 0.00001739
Iteration 66/1000 | Loss: 0.00001739
Iteration 67/1000 | Loss: 0.00001739
Iteration 68/1000 | Loss: 0.00001739
Iteration 69/1000 | Loss: 0.00001738
Iteration 70/1000 | Loss: 0.00001738
Iteration 71/1000 | Loss: 0.00001738
Iteration 72/1000 | Loss: 0.00001738
Iteration 73/1000 | Loss: 0.00001738
Iteration 74/1000 | Loss: 0.00001738
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001738
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001738
Iteration 81/1000 | Loss: 0.00001738
Iteration 82/1000 | Loss: 0.00001738
Iteration 83/1000 | Loss: 0.00001738
Iteration 84/1000 | Loss: 0.00001738
Iteration 85/1000 | Loss: 0.00001738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.7380414647050202e-05, 1.7380414647050202e-05, 1.7380414647050202e-05, 1.7380414647050202e-05, 1.7380414647050202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7380414647050202e-05

Optimization complete. Final v2v error: 3.4925711154937744 mm

Highest mean error: 3.6659278869628906 mm for frame 87

Lowest mean error: 3.3522655963897705 mm for frame 131

Saving results

Total time: 28.92020893096924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834317
Iteration 2/25 | Loss: 0.00093382
Iteration 3/25 | Loss: 0.00068445
Iteration 4/25 | Loss: 0.00065813
Iteration 5/25 | Loss: 0.00062200
Iteration 6/25 | Loss: 0.00061880
Iteration 7/25 | Loss: 0.00061808
Iteration 8/25 | Loss: 0.00061790
Iteration 9/25 | Loss: 0.00061783
Iteration 10/25 | Loss: 0.00061783
Iteration 11/25 | Loss: 0.00061782
Iteration 12/25 | Loss: 0.00061782
Iteration 13/25 | Loss: 0.00061782
Iteration 14/25 | Loss: 0.00061782
Iteration 15/25 | Loss: 0.00061782
Iteration 16/25 | Loss: 0.00061782
Iteration 17/25 | Loss: 0.00061782
Iteration 18/25 | Loss: 0.00061781
Iteration 19/25 | Loss: 0.00061781
Iteration 20/25 | Loss: 0.00061781
Iteration 21/25 | Loss: 0.00061781
Iteration 22/25 | Loss: 0.00061781
Iteration 23/25 | Loss: 0.00061781
Iteration 24/25 | Loss: 0.00061781
Iteration 25/25 | Loss: 0.00061781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53658307
Iteration 2/25 | Loss: 0.00030112
Iteration 3/25 | Loss: 0.00030111
Iteration 4/25 | Loss: 0.00030111
Iteration 5/25 | Loss: 0.00030111
Iteration 6/25 | Loss: 0.00030111
Iteration 7/25 | Loss: 0.00030111
Iteration 8/25 | Loss: 0.00030111
Iteration 9/25 | Loss: 0.00030111
Iteration 10/25 | Loss: 0.00030111
Iteration 11/25 | Loss: 0.00030111
Iteration 12/25 | Loss: 0.00030111
Iteration 13/25 | Loss: 0.00030111
Iteration 14/25 | Loss: 0.00030111
Iteration 15/25 | Loss: 0.00030111
Iteration 16/25 | Loss: 0.00030111
Iteration 17/25 | Loss: 0.00030111
Iteration 18/25 | Loss: 0.00030111
Iteration 19/25 | Loss: 0.00030111
Iteration 20/25 | Loss: 0.00030111
Iteration 21/25 | Loss: 0.00030111
Iteration 22/25 | Loss: 0.00030111
Iteration 23/25 | Loss: 0.00030111
Iteration 24/25 | Loss: 0.00030111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00030111114028841257, 0.00030111114028841257, 0.00030111114028841257, 0.00030111114028841257, 0.00030111114028841257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030111114028841257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030111
Iteration 2/1000 | Loss: 0.00002509
Iteration 3/1000 | Loss: 0.00001711
Iteration 4/1000 | Loss: 0.00001469
Iteration 5/1000 | Loss: 0.00001397
Iteration 6/1000 | Loss: 0.00001339
Iteration 7/1000 | Loss: 0.00001311
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001264
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001257
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001249
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001238
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001228
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001213
Iteration 106/1000 | Loss: 0.00001213
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001213
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001212
Iteration 111/1000 | Loss: 0.00001212
Iteration 112/1000 | Loss: 0.00001211
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001211
Iteration 117/1000 | Loss: 0.00001211
Iteration 118/1000 | Loss: 0.00001211
Iteration 119/1000 | Loss: 0.00001211
Iteration 120/1000 | Loss: 0.00001211
Iteration 121/1000 | Loss: 0.00001211
Iteration 122/1000 | Loss: 0.00001211
Iteration 123/1000 | Loss: 0.00001210
Iteration 124/1000 | Loss: 0.00001210
Iteration 125/1000 | Loss: 0.00001210
Iteration 126/1000 | Loss: 0.00001210
Iteration 127/1000 | Loss: 0.00001210
Iteration 128/1000 | Loss: 0.00001210
Iteration 129/1000 | Loss: 0.00001210
Iteration 130/1000 | Loss: 0.00001209
Iteration 131/1000 | Loss: 0.00001209
Iteration 132/1000 | Loss: 0.00001209
Iteration 133/1000 | Loss: 0.00001209
Iteration 134/1000 | Loss: 0.00001209
Iteration 135/1000 | Loss: 0.00001209
Iteration 136/1000 | Loss: 0.00001209
Iteration 137/1000 | Loss: 0.00001209
Iteration 138/1000 | Loss: 0.00001209
Iteration 139/1000 | Loss: 0.00001209
Iteration 140/1000 | Loss: 0.00001209
Iteration 141/1000 | Loss: 0.00001209
Iteration 142/1000 | Loss: 0.00001209
Iteration 143/1000 | Loss: 0.00001209
Iteration 144/1000 | Loss: 0.00001209
Iteration 145/1000 | Loss: 0.00001209
Iteration 146/1000 | Loss: 0.00001209
Iteration 147/1000 | Loss: 0.00001209
Iteration 148/1000 | Loss: 0.00001209
Iteration 149/1000 | Loss: 0.00001209
Iteration 150/1000 | Loss: 0.00001209
Iteration 151/1000 | Loss: 0.00001209
Iteration 152/1000 | Loss: 0.00001209
Iteration 153/1000 | Loss: 0.00001209
Iteration 154/1000 | Loss: 0.00001209
Iteration 155/1000 | Loss: 0.00001209
Iteration 156/1000 | Loss: 0.00001209
Iteration 157/1000 | Loss: 0.00001209
Iteration 158/1000 | Loss: 0.00001209
Iteration 159/1000 | Loss: 0.00001209
Iteration 160/1000 | Loss: 0.00001209
Iteration 161/1000 | Loss: 0.00001209
Iteration 162/1000 | Loss: 0.00001209
Iteration 163/1000 | Loss: 0.00001209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.2086740753147751e-05, 1.2086740753147751e-05, 1.2086740753147751e-05, 1.2086740753147751e-05, 1.2086740753147751e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2086740753147751e-05

Optimization complete. Final v2v error: 2.9399073123931885 mm

Highest mean error: 3.67366361618042 mm for frame 56

Lowest mean error: 2.67049241065979 mm for frame 125

Saving results

Total time: 38.72173452377319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381754
Iteration 2/25 | Loss: 0.00089557
Iteration 3/25 | Loss: 0.00065145
Iteration 4/25 | Loss: 0.00061266
Iteration 5/25 | Loss: 0.00060320
Iteration 6/25 | Loss: 0.00060138
Iteration 7/25 | Loss: 0.00060091
Iteration 8/25 | Loss: 0.00060089
Iteration 9/25 | Loss: 0.00060089
Iteration 10/25 | Loss: 0.00060089
Iteration 11/25 | Loss: 0.00060089
Iteration 12/25 | Loss: 0.00060089
Iteration 13/25 | Loss: 0.00060089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000600885134190321, 0.000600885134190321, 0.000600885134190321, 0.000600885134190321, 0.000600885134190321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000600885134190321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43186808
Iteration 2/25 | Loss: 0.00024790
Iteration 3/25 | Loss: 0.00024788
Iteration 4/25 | Loss: 0.00024788
Iteration 5/25 | Loss: 0.00024788
Iteration 6/25 | Loss: 0.00024788
Iteration 7/25 | Loss: 0.00024788
Iteration 8/25 | Loss: 0.00024788
Iteration 9/25 | Loss: 0.00024788
Iteration 10/25 | Loss: 0.00024788
Iteration 11/25 | Loss: 0.00024788
Iteration 12/25 | Loss: 0.00024788
Iteration 13/25 | Loss: 0.00024788
Iteration 14/25 | Loss: 0.00024788
Iteration 15/25 | Loss: 0.00024788
Iteration 16/25 | Loss: 0.00024788
Iteration 17/25 | Loss: 0.00024788
Iteration 18/25 | Loss: 0.00024788
Iteration 19/25 | Loss: 0.00024788
Iteration 20/25 | Loss: 0.00024788
Iteration 21/25 | Loss: 0.00024788
Iteration 22/25 | Loss: 0.00024788
Iteration 23/25 | Loss: 0.00024788
Iteration 24/25 | Loss: 0.00024788
Iteration 25/25 | Loss: 0.00024788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024788
Iteration 2/1000 | Loss: 0.00002742
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001542
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00001341
Iteration 7/1000 | Loss: 0.00001289
Iteration 8/1000 | Loss: 0.00001248
Iteration 9/1000 | Loss: 0.00001219
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001210
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001191
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001188
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001171
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001156
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001154
Iteration 36/1000 | Loss: 0.00001154
Iteration 37/1000 | Loss: 0.00001154
Iteration 38/1000 | Loss: 0.00001154
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001153
Iteration 41/1000 | Loss: 0.00001153
Iteration 42/1000 | Loss: 0.00001152
Iteration 43/1000 | Loss: 0.00001152
Iteration 44/1000 | Loss: 0.00001152
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001151
Iteration 47/1000 | Loss: 0.00001151
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001151
Iteration 50/1000 | Loss: 0.00001151
Iteration 51/1000 | Loss: 0.00001151
Iteration 52/1000 | Loss: 0.00001151
Iteration 53/1000 | Loss: 0.00001151
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001149
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001149
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001149
Iteration 77/1000 | Loss: 0.00001149
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001148
Iteration 82/1000 | Loss: 0.00001148
Iteration 83/1000 | Loss: 0.00001148
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001148
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001148
Iteration 91/1000 | Loss: 0.00001148
Iteration 92/1000 | Loss: 0.00001148
Iteration 93/1000 | Loss: 0.00001148
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001147
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001147
Iteration 110/1000 | Loss: 0.00001147
Iteration 111/1000 | Loss: 0.00001147
Iteration 112/1000 | Loss: 0.00001147
Iteration 113/1000 | Loss: 0.00001147
Iteration 114/1000 | Loss: 0.00001147
Iteration 115/1000 | Loss: 0.00001147
Iteration 116/1000 | Loss: 0.00001147
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001146
Iteration 134/1000 | Loss: 0.00001146
Iteration 135/1000 | Loss: 0.00001146
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.1462387192295864e-05, 1.1462387192295864e-05, 1.1462387192295864e-05, 1.1462387192295864e-05, 1.1462387192295864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1462387192295864e-05

Optimization complete. Final v2v error: 2.8725860118865967 mm

Highest mean error: 3.4135491847991943 mm for frame 104

Lowest mean error: 2.555968999862671 mm for frame 7

Saving results

Total time: 35.80515432357788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466341
Iteration 2/25 | Loss: 0.00081390
Iteration 3/25 | Loss: 0.00064464
Iteration 4/25 | Loss: 0.00062259
Iteration 5/25 | Loss: 0.00061587
Iteration 6/25 | Loss: 0.00061435
Iteration 7/25 | Loss: 0.00061404
Iteration 8/25 | Loss: 0.00061404
Iteration 9/25 | Loss: 0.00061404
Iteration 10/25 | Loss: 0.00061404
Iteration 11/25 | Loss: 0.00061404
Iteration 12/25 | Loss: 0.00061404
Iteration 13/25 | Loss: 0.00061404
Iteration 14/25 | Loss: 0.00061404
Iteration 15/25 | Loss: 0.00061404
Iteration 16/25 | Loss: 0.00061404
Iteration 17/25 | Loss: 0.00061404
Iteration 18/25 | Loss: 0.00061404
Iteration 19/25 | Loss: 0.00061404
Iteration 20/25 | Loss: 0.00061404
Iteration 21/25 | Loss: 0.00061404
Iteration 22/25 | Loss: 0.00061404
Iteration 23/25 | Loss: 0.00061404
Iteration 24/25 | Loss: 0.00061404
Iteration 25/25 | Loss: 0.00061404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.20710945
Iteration 2/25 | Loss: 0.00028242
Iteration 3/25 | Loss: 0.00028240
Iteration 4/25 | Loss: 0.00028240
Iteration 5/25 | Loss: 0.00028240
Iteration 6/25 | Loss: 0.00028240
Iteration 7/25 | Loss: 0.00028240
Iteration 8/25 | Loss: 0.00028240
Iteration 9/25 | Loss: 0.00028240
Iteration 10/25 | Loss: 0.00028240
Iteration 11/25 | Loss: 0.00028240
Iteration 12/25 | Loss: 0.00028240
Iteration 13/25 | Loss: 0.00028240
Iteration 14/25 | Loss: 0.00028240
Iteration 15/25 | Loss: 0.00028240
Iteration 16/25 | Loss: 0.00028240
Iteration 17/25 | Loss: 0.00028240
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00028239915263839066, 0.00028239915263839066, 0.00028239915263839066, 0.00028239915263839066, 0.00028239915263839066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028239915263839066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028240
Iteration 2/1000 | Loss: 0.00002478
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00001490
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001284
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001266
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001262
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001261
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001256
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001255
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001254
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001253
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001252
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001251
Iteration 33/1000 | Loss: 0.00001251
Iteration 34/1000 | Loss: 0.00001250
Iteration 35/1000 | Loss: 0.00001250
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001249
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001247
Iteration 45/1000 | Loss: 0.00001247
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001245
Iteration 54/1000 | Loss: 0.00001245
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001245
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00001244
Iteration 64/1000 | Loss: 0.00001243
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001243
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001237
Iteration 101/1000 | Loss: 0.00001237
Iteration 102/1000 | Loss: 0.00001237
Iteration 103/1000 | Loss: 0.00001237
Iteration 104/1000 | Loss: 0.00001236
Iteration 105/1000 | Loss: 0.00001236
Iteration 106/1000 | Loss: 0.00001236
Iteration 107/1000 | Loss: 0.00001236
Iteration 108/1000 | Loss: 0.00001236
Iteration 109/1000 | Loss: 0.00001235
Iteration 110/1000 | Loss: 0.00001235
Iteration 111/1000 | Loss: 0.00001235
Iteration 112/1000 | Loss: 0.00001235
Iteration 113/1000 | Loss: 0.00001235
Iteration 114/1000 | Loss: 0.00001234
Iteration 115/1000 | Loss: 0.00001234
Iteration 116/1000 | Loss: 0.00001234
Iteration 117/1000 | Loss: 0.00001234
Iteration 118/1000 | Loss: 0.00001234
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001234
Iteration 121/1000 | Loss: 0.00001234
Iteration 122/1000 | Loss: 0.00001234
Iteration 123/1000 | Loss: 0.00001234
Iteration 124/1000 | Loss: 0.00001233
Iteration 125/1000 | Loss: 0.00001233
Iteration 126/1000 | Loss: 0.00001233
Iteration 127/1000 | Loss: 0.00001233
Iteration 128/1000 | Loss: 0.00001233
Iteration 129/1000 | Loss: 0.00001233
Iteration 130/1000 | Loss: 0.00001233
Iteration 131/1000 | Loss: 0.00001233
Iteration 132/1000 | Loss: 0.00001233
Iteration 133/1000 | Loss: 0.00001233
Iteration 134/1000 | Loss: 0.00001233
Iteration 135/1000 | Loss: 0.00001233
Iteration 136/1000 | Loss: 0.00001233
Iteration 137/1000 | Loss: 0.00001233
Iteration 138/1000 | Loss: 0.00001233
Iteration 139/1000 | Loss: 0.00001232
Iteration 140/1000 | Loss: 0.00001232
Iteration 141/1000 | Loss: 0.00001232
Iteration 142/1000 | Loss: 0.00001232
Iteration 143/1000 | Loss: 0.00001232
Iteration 144/1000 | Loss: 0.00001232
Iteration 145/1000 | Loss: 0.00001232
Iteration 146/1000 | Loss: 0.00001232
Iteration 147/1000 | Loss: 0.00001232
Iteration 148/1000 | Loss: 0.00001232
Iteration 149/1000 | Loss: 0.00001232
Iteration 150/1000 | Loss: 0.00001232
Iteration 151/1000 | Loss: 0.00001232
Iteration 152/1000 | Loss: 0.00001232
Iteration 153/1000 | Loss: 0.00001232
Iteration 154/1000 | Loss: 0.00001232
Iteration 155/1000 | Loss: 0.00001232
Iteration 156/1000 | Loss: 0.00001232
Iteration 157/1000 | Loss: 0.00001232
Iteration 158/1000 | Loss: 0.00001232
Iteration 159/1000 | Loss: 0.00001232
Iteration 160/1000 | Loss: 0.00001232
Iteration 161/1000 | Loss: 0.00001232
Iteration 162/1000 | Loss: 0.00001232
Iteration 163/1000 | Loss: 0.00001232
Iteration 164/1000 | Loss: 0.00001232
Iteration 165/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2315493222558871e-05, 1.2315493222558871e-05, 1.2315493222558871e-05, 1.2315493222558871e-05, 1.2315493222558871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2315493222558871e-05

Optimization complete. Final v2v error: 2.987429618835449 mm

Highest mean error: 3.3054983615875244 mm for frame 109

Lowest mean error: 2.784313201904297 mm for frame 154

Saving results

Total time: 35.242125272750854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136271
Iteration 2/25 | Loss: 0.00271616
Iteration 3/25 | Loss: 0.00162034
Iteration 4/25 | Loss: 0.00142458
Iteration 5/25 | Loss: 0.00164065
Iteration 6/25 | Loss: 0.00164345
Iteration 7/25 | Loss: 0.00155464
Iteration 8/25 | Loss: 0.00146929
Iteration 9/25 | Loss: 0.00135998
Iteration 10/25 | Loss: 0.00128633
Iteration 11/25 | Loss: 0.00121136
Iteration 12/25 | Loss: 0.00114145
Iteration 13/25 | Loss: 0.00109633
Iteration 14/25 | Loss: 0.00103916
Iteration 15/25 | Loss: 0.00103263
Iteration 16/25 | Loss: 0.00099874
Iteration 17/25 | Loss: 0.00097902
Iteration 18/25 | Loss: 0.00095805
Iteration 19/25 | Loss: 0.00097784
Iteration 20/25 | Loss: 0.00098222
Iteration 21/25 | Loss: 0.00096901
Iteration 22/25 | Loss: 0.00096691
Iteration 23/25 | Loss: 0.00097139
Iteration 24/25 | Loss: 0.00095860
Iteration 25/25 | Loss: 0.00095256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68235332
Iteration 2/25 | Loss: 0.00110625
Iteration 3/25 | Loss: 0.00110625
Iteration 4/25 | Loss: 0.00110625
Iteration 5/25 | Loss: 0.00110625
Iteration 6/25 | Loss: 0.00110625
Iteration 7/25 | Loss: 0.00110625
Iteration 8/25 | Loss: 0.00110625
Iteration 9/25 | Loss: 0.00110625
Iteration 10/25 | Loss: 0.00110625
Iteration 11/25 | Loss: 0.00110625
Iteration 12/25 | Loss: 0.00110625
Iteration 13/25 | Loss: 0.00110625
Iteration 14/25 | Loss: 0.00110625
Iteration 15/25 | Loss: 0.00110625
Iteration 16/25 | Loss: 0.00110625
Iteration 17/25 | Loss: 0.00110625
Iteration 18/25 | Loss: 0.00110625
Iteration 19/25 | Loss: 0.00110625
Iteration 20/25 | Loss: 0.00110625
Iteration 21/25 | Loss: 0.00110625
Iteration 22/25 | Loss: 0.00110625
Iteration 23/25 | Loss: 0.00110625
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011062483536079526, 0.0011062483536079526, 0.0011062483536079526, 0.0011062483536079526, 0.0011062483536079526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011062483536079526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110625
Iteration 2/1000 | Loss: 0.00085461
Iteration 3/1000 | Loss: 0.00105758
Iteration 4/1000 | Loss: 0.00068011
Iteration 5/1000 | Loss: 0.00077626
Iteration 6/1000 | Loss: 0.00091506
Iteration 7/1000 | Loss: 0.00059482
Iteration 8/1000 | Loss: 0.00074915
Iteration 9/1000 | Loss: 0.00087145
Iteration 10/1000 | Loss: 0.00107293
Iteration 11/1000 | Loss: 0.00095522
Iteration 12/1000 | Loss: 0.00084082
Iteration 13/1000 | Loss: 0.00092050
Iteration 14/1000 | Loss: 0.00077527
Iteration 15/1000 | Loss: 0.00091030
Iteration 16/1000 | Loss: 0.00085033
Iteration 17/1000 | Loss: 0.00096211
Iteration 18/1000 | Loss: 0.00076066
Iteration 19/1000 | Loss: 0.00127761
Iteration 20/1000 | Loss: 0.00094747
Iteration 21/1000 | Loss: 0.00079924
Iteration 22/1000 | Loss: 0.00072505
Iteration 23/1000 | Loss: 0.00069747
Iteration 24/1000 | Loss: 0.00121682
Iteration 25/1000 | Loss: 0.00128318
Iteration 26/1000 | Loss: 0.00075914
Iteration 27/1000 | Loss: 0.00310257
Iteration 28/1000 | Loss: 0.00067279
Iteration 29/1000 | Loss: 0.00142743
Iteration 30/1000 | Loss: 0.00200968
Iteration 31/1000 | Loss: 0.00091279
Iteration 32/1000 | Loss: 0.00044395
Iteration 33/1000 | Loss: 0.00066926
Iteration 34/1000 | Loss: 0.00053068
Iteration 35/1000 | Loss: 0.00212065
Iteration 36/1000 | Loss: 0.00130500
Iteration 37/1000 | Loss: 0.00066252
Iteration 38/1000 | Loss: 0.00069198
Iteration 39/1000 | Loss: 0.00030977
Iteration 40/1000 | Loss: 0.00322405
Iteration 41/1000 | Loss: 0.00111334
Iteration 42/1000 | Loss: 0.00201155
Iteration 43/1000 | Loss: 0.00099029
Iteration 44/1000 | Loss: 0.00072229
Iteration 45/1000 | Loss: 0.00060453
Iteration 46/1000 | Loss: 0.00060161
Iteration 47/1000 | Loss: 0.00073188
Iteration 48/1000 | Loss: 0.00048318
Iteration 49/1000 | Loss: 0.00085238
Iteration 50/1000 | Loss: 0.00106760
Iteration 51/1000 | Loss: 0.00106675
Iteration 52/1000 | Loss: 0.00090048
Iteration 53/1000 | Loss: 0.00198941
Iteration 54/1000 | Loss: 0.00176272
Iteration 55/1000 | Loss: 0.00096245
Iteration 56/1000 | Loss: 0.00072040
Iteration 57/1000 | Loss: 0.00321742
Iteration 58/1000 | Loss: 0.00307860
Iteration 59/1000 | Loss: 0.00374496
Iteration 60/1000 | Loss: 0.00473048
Iteration 61/1000 | Loss: 0.00275531
Iteration 62/1000 | Loss: 0.00086522
Iteration 63/1000 | Loss: 0.00161884
Iteration 64/1000 | Loss: 0.00053766
Iteration 65/1000 | Loss: 0.00034469
Iteration 66/1000 | Loss: 0.00037929
Iteration 67/1000 | Loss: 0.00052405
Iteration 68/1000 | Loss: 0.00047823
Iteration 69/1000 | Loss: 0.00081544
Iteration 70/1000 | Loss: 0.00061480
Iteration 71/1000 | Loss: 0.00066335
Iteration 72/1000 | Loss: 0.00081549
Iteration 73/1000 | Loss: 0.00032788
Iteration 74/1000 | Loss: 0.00024512
Iteration 75/1000 | Loss: 0.00029445
Iteration 76/1000 | Loss: 0.00040685
Iteration 77/1000 | Loss: 0.00042225
Iteration 78/1000 | Loss: 0.00051116
Iteration 79/1000 | Loss: 0.00055753
Iteration 80/1000 | Loss: 0.00055205
Iteration 81/1000 | Loss: 0.00063263
Iteration 82/1000 | Loss: 0.00077384
Iteration 83/1000 | Loss: 0.00058336
Iteration 84/1000 | Loss: 0.00089108
Iteration 85/1000 | Loss: 0.00038593
Iteration 86/1000 | Loss: 0.00153244
Iteration 87/1000 | Loss: 0.00106783
Iteration 88/1000 | Loss: 0.00056428
Iteration 89/1000 | Loss: 0.00046496
Iteration 90/1000 | Loss: 0.00090988
Iteration 91/1000 | Loss: 0.00068666
Iteration 92/1000 | Loss: 0.00075537
Iteration 93/1000 | Loss: 0.00034664
Iteration 94/1000 | Loss: 0.00053799
Iteration 95/1000 | Loss: 0.00070652
Iteration 96/1000 | Loss: 0.00069549
Iteration 97/1000 | Loss: 0.00050697
Iteration 98/1000 | Loss: 0.00051257
Iteration 99/1000 | Loss: 0.00035522
Iteration 100/1000 | Loss: 0.00047443
Iteration 101/1000 | Loss: 0.00071135
Iteration 102/1000 | Loss: 0.00054090
Iteration 103/1000 | Loss: 0.00078273
Iteration 104/1000 | Loss: 0.00032645
Iteration 105/1000 | Loss: 0.00034043
Iteration 106/1000 | Loss: 0.00040635
Iteration 107/1000 | Loss: 0.00042314
Iteration 108/1000 | Loss: 0.00041169
Iteration 109/1000 | Loss: 0.00018439
Iteration 110/1000 | Loss: 0.00037409
Iteration 111/1000 | Loss: 0.00037831
Iteration 112/1000 | Loss: 0.00016205
Iteration 113/1000 | Loss: 0.00035999
Iteration 114/1000 | Loss: 0.00044532
Iteration 115/1000 | Loss: 0.00036434
Iteration 116/1000 | Loss: 0.00033987
Iteration 117/1000 | Loss: 0.00027126
Iteration 118/1000 | Loss: 0.00036006
Iteration 119/1000 | Loss: 0.00037150
Iteration 120/1000 | Loss: 0.00028401
Iteration 121/1000 | Loss: 0.00033720
Iteration 122/1000 | Loss: 0.00033509
Iteration 123/1000 | Loss: 0.00025520
Iteration 124/1000 | Loss: 0.00024373
Iteration 125/1000 | Loss: 0.00029785
Iteration 126/1000 | Loss: 0.00027982
Iteration 127/1000 | Loss: 0.00028840
Iteration 128/1000 | Loss: 0.00029532
Iteration 129/1000 | Loss: 0.00035467
Iteration 130/1000 | Loss: 0.00029623
Iteration 131/1000 | Loss: 0.00030430
Iteration 132/1000 | Loss: 0.00041606
Iteration 133/1000 | Loss: 0.00047718
Iteration 134/1000 | Loss: 0.00040742
Iteration 135/1000 | Loss: 0.00079136
Iteration 136/1000 | Loss: 0.00023477
Iteration 137/1000 | Loss: 0.00051155
Iteration 138/1000 | Loss: 0.00036284
Iteration 139/1000 | Loss: 0.00031484
Iteration 140/1000 | Loss: 0.00026518
Iteration 141/1000 | Loss: 0.00021737
Iteration 142/1000 | Loss: 0.00009774
Iteration 143/1000 | Loss: 0.00012210
Iteration 144/1000 | Loss: 0.00036747
Iteration 145/1000 | Loss: 0.00031255
Iteration 146/1000 | Loss: 0.00037935
Iteration 147/1000 | Loss: 0.00015552
Iteration 148/1000 | Loss: 0.00016843
Iteration 149/1000 | Loss: 0.00015470
Iteration 150/1000 | Loss: 0.00013790
Iteration 151/1000 | Loss: 0.00013865
Iteration 152/1000 | Loss: 0.00016131
Iteration 153/1000 | Loss: 0.00013720
Iteration 154/1000 | Loss: 0.00014349
Iteration 155/1000 | Loss: 0.00012952
Iteration 156/1000 | Loss: 0.00014064
Iteration 157/1000 | Loss: 0.00014664
Iteration 158/1000 | Loss: 0.00015643
Iteration 159/1000 | Loss: 0.00042533
Iteration 160/1000 | Loss: 0.00021646
Iteration 161/1000 | Loss: 0.00014695
Iteration 162/1000 | Loss: 0.00014245
Iteration 163/1000 | Loss: 0.00014044
Iteration 164/1000 | Loss: 0.00014647
Iteration 165/1000 | Loss: 0.00028100
Iteration 166/1000 | Loss: 0.00015304
Iteration 167/1000 | Loss: 0.00038497
Iteration 168/1000 | Loss: 0.00064722
Iteration 169/1000 | Loss: 0.00049854
Iteration 170/1000 | Loss: 0.00018432
Iteration 171/1000 | Loss: 0.00064511
Iteration 172/1000 | Loss: 0.00045630
Iteration 173/1000 | Loss: 0.00029290
Iteration 174/1000 | Loss: 0.00012987
Iteration 175/1000 | Loss: 0.00012798
Iteration 176/1000 | Loss: 0.00013635
Iteration 177/1000 | Loss: 0.00006467
Iteration 178/1000 | Loss: 0.00006583
Iteration 179/1000 | Loss: 0.00005974
Iteration 180/1000 | Loss: 0.00005311
Iteration 181/1000 | Loss: 0.00004692
Iteration 182/1000 | Loss: 0.00004024
Iteration 183/1000 | Loss: 0.00003656
Iteration 184/1000 | Loss: 0.00029263
Iteration 185/1000 | Loss: 0.00008850
Iteration 186/1000 | Loss: 0.00029149
Iteration 187/1000 | Loss: 0.00008039
Iteration 188/1000 | Loss: 0.00005329
Iteration 189/1000 | Loss: 0.00005427
Iteration 190/1000 | Loss: 0.00003354
Iteration 191/1000 | Loss: 0.00004830
Iteration 192/1000 | Loss: 0.00005131
Iteration 193/1000 | Loss: 0.00004089
Iteration 194/1000 | Loss: 0.00003800
Iteration 195/1000 | Loss: 0.00003660
Iteration 196/1000 | Loss: 0.00005553
Iteration 197/1000 | Loss: 0.00005063
Iteration 198/1000 | Loss: 0.00004948
Iteration 199/1000 | Loss: 0.00003569
Iteration 200/1000 | Loss: 0.00003698
Iteration 201/1000 | Loss: 0.00004320
Iteration 202/1000 | Loss: 0.00004510
Iteration 203/1000 | Loss: 0.00003548
Iteration 204/1000 | Loss: 0.00003879
Iteration 205/1000 | Loss: 0.00004233
Iteration 206/1000 | Loss: 0.00002978
Iteration 207/1000 | Loss: 0.00004263
Iteration 208/1000 | Loss: 0.00004606
Iteration 209/1000 | Loss: 0.00002945
Iteration 210/1000 | Loss: 0.00003358
Iteration 211/1000 | Loss: 0.00004132
Iteration 212/1000 | Loss: 0.00004824
Iteration 213/1000 | Loss: 0.00004685
Iteration 214/1000 | Loss: 0.00004726
Iteration 215/1000 | Loss: 0.00004566
Iteration 216/1000 | Loss: 0.00004511
Iteration 217/1000 | Loss: 0.00002817
Iteration 218/1000 | Loss: 0.00002991
Iteration 219/1000 | Loss: 0.00004047
Iteration 220/1000 | Loss: 0.00004355
Iteration 221/1000 | Loss: 0.00004812
Iteration 222/1000 | Loss: 0.00004484
Iteration 223/1000 | Loss: 0.00003724
Iteration 224/1000 | Loss: 0.00004724
Iteration 225/1000 | Loss: 0.00003541
Iteration 226/1000 | Loss: 0.00006342
Iteration 227/1000 | Loss: 0.00004462
Iteration 228/1000 | Loss: 0.00004530
Iteration 229/1000 | Loss: 0.00004486
Iteration 230/1000 | Loss: 0.00004493
Iteration 231/1000 | Loss: 0.00003114
Iteration 232/1000 | Loss: 0.00004471
Iteration 233/1000 | Loss: 0.00003066
Iteration 234/1000 | Loss: 0.00003657
Iteration 235/1000 | Loss: 0.00004383
Iteration 236/1000 | Loss: 0.00003536
Iteration 237/1000 | Loss: 0.00004517
Iteration 238/1000 | Loss: 0.00003063
Iteration 239/1000 | Loss: 0.00004467
Iteration 240/1000 | Loss: 0.00004540
Iteration 241/1000 | Loss: 0.00004506
Iteration 242/1000 | Loss: 0.00004520
Iteration 243/1000 | Loss: 0.00004412
Iteration 244/1000 | Loss: 0.00004509
Iteration 245/1000 | Loss: 0.00003135
Iteration 246/1000 | Loss: 0.00004564
Iteration 247/1000 | Loss: 0.00003117
Iteration 248/1000 | Loss: 0.00004270
Iteration 249/1000 | Loss: 0.00004684
Iteration 250/1000 | Loss: 0.00005102
Iteration 251/1000 | Loss: 0.00004876
Iteration 252/1000 | Loss: 0.00006090
Iteration 253/1000 | Loss: 0.00004488
Iteration 254/1000 | Loss: 0.00004735
Iteration 255/1000 | Loss: 0.00006803
Iteration 256/1000 | Loss: 0.00004519
Iteration 257/1000 | Loss: 0.00006473
Iteration 258/1000 | Loss: 0.00004421
Iteration 259/1000 | Loss: 0.00006606
Iteration 260/1000 | Loss: 0.00002971
Iteration 261/1000 | Loss: 0.00002673
Iteration 262/1000 | Loss: 0.00002527
Iteration 263/1000 | Loss: 0.00002502
Iteration 264/1000 | Loss: 0.00002496
Iteration 265/1000 | Loss: 0.00002490
Iteration 266/1000 | Loss: 0.00002483
Iteration 267/1000 | Loss: 0.00002475
Iteration 268/1000 | Loss: 0.00002475
Iteration 269/1000 | Loss: 0.00002475
Iteration 270/1000 | Loss: 0.00002474
Iteration 271/1000 | Loss: 0.00002474
Iteration 272/1000 | Loss: 0.00002474
Iteration 273/1000 | Loss: 0.00002474
Iteration 274/1000 | Loss: 0.00002474
Iteration 275/1000 | Loss: 0.00002474
Iteration 276/1000 | Loss: 0.00002474
Iteration 277/1000 | Loss: 0.00002474
Iteration 278/1000 | Loss: 0.00002474
Iteration 279/1000 | Loss: 0.00002473
Iteration 280/1000 | Loss: 0.00002473
Iteration 281/1000 | Loss: 0.00002473
Iteration 282/1000 | Loss: 0.00002473
Iteration 283/1000 | Loss: 0.00002473
Iteration 284/1000 | Loss: 0.00002473
Iteration 285/1000 | Loss: 0.00002473
Iteration 286/1000 | Loss: 0.00002473
Iteration 287/1000 | Loss: 0.00002473
Iteration 288/1000 | Loss: 0.00002473
Iteration 289/1000 | Loss: 0.00002473
Iteration 290/1000 | Loss: 0.00002473
Iteration 291/1000 | Loss: 0.00002473
Iteration 292/1000 | Loss: 0.00002473
Iteration 293/1000 | Loss: 0.00002473
Iteration 294/1000 | Loss: 0.00002473
Iteration 295/1000 | Loss: 0.00002473
Iteration 296/1000 | Loss: 0.00002473
Iteration 297/1000 | Loss: 0.00002473
Iteration 298/1000 | Loss: 0.00002473
Iteration 299/1000 | Loss: 0.00002473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [2.472755659255199e-05, 2.472755659255199e-05, 2.472755659255199e-05, 2.472755659255199e-05, 2.472755659255199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.472755659255199e-05

Optimization complete. Final v2v error: 4.079590797424316 mm

Highest mean error: 6.857274532318115 mm for frame 136

Lowest mean error: 3.4808928966522217 mm for frame 26

Saving results

Total time: 409.0724411010742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871412
Iteration 2/25 | Loss: 0.00086722
Iteration 3/25 | Loss: 0.00075795
Iteration 4/25 | Loss: 0.00071881
Iteration 5/25 | Loss: 0.00070538
Iteration 6/25 | Loss: 0.00070187
Iteration 7/25 | Loss: 0.00070051
Iteration 8/25 | Loss: 0.00070051
Iteration 9/25 | Loss: 0.00070051
Iteration 10/25 | Loss: 0.00070051
Iteration 11/25 | Loss: 0.00070051
Iteration 12/25 | Loss: 0.00070051
Iteration 13/25 | Loss: 0.00070051
Iteration 14/25 | Loss: 0.00070051
Iteration 15/25 | Loss: 0.00070051
Iteration 16/25 | Loss: 0.00070051
Iteration 17/25 | Loss: 0.00070051
Iteration 18/25 | Loss: 0.00070051
Iteration 19/25 | Loss: 0.00070051
Iteration 20/25 | Loss: 0.00070051
Iteration 21/25 | Loss: 0.00070051
Iteration 22/25 | Loss: 0.00070051
Iteration 23/25 | Loss: 0.00070051
Iteration 24/25 | Loss: 0.00070051
Iteration 25/25 | Loss: 0.00070051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42563808
Iteration 2/25 | Loss: 0.00030709
Iteration 3/25 | Loss: 0.00030709
Iteration 4/25 | Loss: 0.00030709
Iteration 5/25 | Loss: 0.00030709
Iteration 6/25 | Loss: 0.00030709
Iteration 7/25 | Loss: 0.00030709
Iteration 8/25 | Loss: 0.00030709
Iteration 9/25 | Loss: 0.00030709
Iteration 10/25 | Loss: 0.00030709
Iteration 11/25 | Loss: 0.00030709
Iteration 12/25 | Loss: 0.00030709
Iteration 13/25 | Loss: 0.00030709
Iteration 14/25 | Loss: 0.00030709
Iteration 15/25 | Loss: 0.00030709
Iteration 16/25 | Loss: 0.00030709
Iteration 17/25 | Loss: 0.00030709
Iteration 18/25 | Loss: 0.00030709
Iteration 19/25 | Loss: 0.00030709
Iteration 20/25 | Loss: 0.00030709
Iteration 21/25 | Loss: 0.00030709
Iteration 22/25 | Loss: 0.00030709
Iteration 23/25 | Loss: 0.00030709
Iteration 24/25 | Loss: 0.00030709
Iteration 25/25 | Loss: 0.00030709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030709
Iteration 2/1000 | Loss: 0.00003606
Iteration 3/1000 | Loss: 0.00002660
Iteration 4/1000 | Loss: 0.00002470
Iteration 5/1000 | Loss: 0.00002383
Iteration 6/1000 | Loss: 0.00002330
Iteration 7/1000 | Loss: 0.00002287
Iteration 8/1000 | Loss: 0.00002258
Iteration 9/1000 | Loss: 0.00002236
Iteration 10/1000 | Loss: 0.00002225
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002223
Iteration 13/1000 | Loss: 0.00002212
Iteration 14/1000 | Loss: 0.00002209
Iteration 15/1000 | Loss: 0.00002209
Iteration 16/1000 | Loss: 0.00002208
Iteration 17/1000 | Loss: 0.00002208
Iteration 18/1000 | Loss: 0.00002208
Iteration 19/1000 | Loss: 0.00002207
Iteration 20/1000 | Loss: 0.00002204
Iteration 21/1000 | Loss: 0.00002204
Iteration 22/1000 | Loss: 0.00002203
Iteration 23/1000 | Loss: 0.00002203
Iteration 24/1000 | Loss: 0.00002203
Iteration 25/1000 | Loss: 0.00002196
Iteration 26/1000 | Loss: 0.00002195
Iteration 27/1000 | Loss: 0.00002192
Iteration 28/1000 | Loss: 0.00002191
Iteration 29/1000 | Loss: 0.00002191
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002190
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002189
Iteration 34/1000 | Loss: 0.00002189
Iteration 35/1000 | Loss: 0.00002189
Iteration 36/1000 | Loss: 0.00002189
Iteration 37/1000 | Loss: 0.00002189
Iteration 38/1000 | Loss: 0.00002189
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002189
Iteration 41/1000 | Loss: 0.00002189
Iteration 42/1000 | Loss: 0.00002189
Iteration 43/1000 | Loss: 0.00002189
Iteration 44/1000 | Loss: 0.00002189
Iteration 45/1000 | Loss: 0.00002189
Iteration 46/1000 | Loss: 0.00002188
Iteration 47/1000 | Loss: 0.00002188
Iteration 48/1000 | Loss: 0.00002188
Iteration 49/1000 | Loss: 0.00002188
Iteration 50/1000 | Loss: 0.00002188
Iteration 51/1000 | Loss: 0.00002188
Iteration 52/1000 | Loss: 0.00002187
Iteration 53/1000 | Loss: 0.00002187
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002185
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002185
Iteration 58/1000 | Loss: 0.00002185
Iteration 59/1000 | Loss: 0.00002185
Iteration 60/1000 | Loss: 0.00002185
Iteration 61/1000 | Loss: 0.00002185
Iteration 62/1000 | Loss: 0.00002185
Iteration 63/1000 | Loss: 0.00002185
Iteration 64/1000 | Loss: 0.00002184
Iteration 65/1000 | Loss: 0.00002184
Iteration 66/1000 | Loss: 0.00002184
Iteration 67/1000 | Loss: 0.00002184
Iteration 68/1000 | Loss: 0.00002184
Iteration 69/1000 | Loss: 0.00002184
Iteration 70/1000 | Loss: 0.00002184
Iteration 71/1000 | Loss: 0.00002184
Iteration 72/1000 | Loss: 0.00002184
Iteration 73/1000 | Loss: 0.00002184
Iteration 74/1000 | Loss: 0.00002184
Iteration 75/1000 | Loss: 0.00002184
Iteration 76/1000 | Loss: 0.00002184
Iteration 77/1000 | Loss: 0.00002184
Iteration 78/1000 | Loss: 0.00002184
Iteration 79/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.1843025024281815e-05, 2.1843025024281815e-05, 2.1843025024281815e-05, 2.1843025024281815e-05, 2.1843025024281815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1843025024281815e-05

Optimization complete. Final v2v error: 3.9032630920410156 mm

Highest mean error: 4.009659767150879 mm for frame 154

Lowest mean error: 3.762202024459839 mm for frame 0

Saving results

Total time: 34.923935651779175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861904
Iteration 2/25 | Loss: 0.00078633
Iteration 3/25 | Loss: 0.00062813
Iteration 4/25 | Loss: 0.00060406
Iteration 5/25 | Loss: 0.00059507
Iteration 6/25 | Loss: 0.00059366
Iteration 7/25 | Loss: 0.00059348
Iteration 8/25 | Loss: 0.00059348
Iteration 9/25 | Loss: 0.00059348
Iteration 10/25 | Loss: 0.00059348
Iteration 11/25 | Loss: 0.00059348
Iteration 12/25 | Loss: 0.00059348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005934811197221279, 0.0005934811197221279, 0.0005934811197221279, 0.0005934811197221279, 0.0005934811197221279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005934811197221279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67429483
Iteration 2/25 | Loss: 0.00029155
Iteration 3/25 | Loss: 0.00029155
Iteration 4/25 | Loss: 0.00029155
Iteration 5/25 | Loss: 0.00029155
Iteration 6/25 | Loss: 0.00029154
Iteration 7/25 | Loss: 0.00029154
Iteration 8/25 | Loss: 0.00029154
Iteration 9/25 | Loss: 0.00029154
Iteration 10/25 | Loss: 0.00029154
Iteration 11/25 | Loss: 0.00029154
Iteration 12/25 | Loss: 0.00029154
Iteration 13/25 | Loss: 0.00029154
Iteration 14/25 | Loss: 0.00029154
Iteration 15/25 | Loss: 0.00029154
Iteration 16/25 | Loss: 0.00029154
Iteration 17/25 | Loss: 0.00029154
Iteration 18/25 | Loss: 0.00029154
Iteration 19/25 | Loss: 0.00029154
Iteration 20/25 | Loss: 0.00029154
Iteration 21/25 | Loss: 0.00029154
Iteration 22/25 | Loss: 0.00029154
Iteration 23/25 | Loss: 0.00029154
Iteration 24/25 | Loss: 0.00029154
Iteration 25/25 | Loss: 0.00029154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029154
Iteration 2/1000 | Loss: 0.00002134
Iteration 3/1000 | Loss: 0.00001400
Iteration 4/1000 | Loss: 0.00001325
Iteration 5/1000 | Loss: 0.00001269
Iteration 6/1000 | Loss: 0.00001237
Iteration 7/1000 | Loss: 0.00001208
Iteration 8/1000 | Loss: 0.00001188
Iteration 9/1000 | Loss: 0.00001179
Iteration 10/1000 | Loss: 0.00001173
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001167
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001163
Iteration 24/1000 | Loss: 0.00001163
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001162
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001161
Iteration 30/1000 | Loss: 0.00001161
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001158
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001158
Iteration 38/1000 | Loss: 0.00001158
Iteration 39/1000 | Loss: 0.00001158
Iteration 40/1000 | Loss: 0.00001158
Iteration 41/1000 | Loss: 0.00001158
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001149
Iteration 53/1000 | Loss: 0.00001149
Iteration 54/1000 | Loss: 0.00001149
Iteration 55/1000 | Loss: 0.00001149
Iteration 56/1000 | Loss: 0.00001149
Iteration 57/1000 | Loss: 0.00001149
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001149
Iteration 60/1000 | Loss: 0.00001149
Iteration 61/1000 | Loss: 0.00001149
Iteration 62/1000 | Loss: 0.00001149
Iteration 63/1000 | Loss: 0.00001148
Iteration 64/1000 | Loss: 0.00001148
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001144
Iteration 77/1000 | Loss: 0.00001144
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001144
Iteration 80/1000 | Loss: 0.00001143
Iteration 81/1000 | Loss: 0.00001143
Iteration 82/1000 | Loss: 0.00001143
Iteration 83/1000 | Loss: 0.00001143
Iteration 84/1000 | Loss: 0.00001143
Iteration 85/1000 | Loss: 0.00001143
Iteration 86/1000 | Loss: 0.00001143
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001142
Iteration 90/1000 | Loss: 0.00001142
Iteration 91/1000 | Loss: 0.00001142
Iteration 92/1000 | Loss: 0.00001142
Iteration 93/1000 | Loss: 0.00001141
Iteration 94/1000 | Loss: 0.00001141
Iteration 95/1000 | Loss: 0.00001141
Iteration 96/1000 | Loss: 0.00001141
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001140
Iteration 101/1000 | Loss: 0.00001140
Iteration 102/1000 | Loss: 0.00001140
Iteration 103/1000 | Loss: 0.00001140
Iteration 104/1000 | Loss: 0.00001139
Iteration 105/1000 | Loss: 0.00001139
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001138
Iteration 108/1000 | Loss: 0.00001138
Iteration 109/1000 | Loss: 0.00001138
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001136
Iteration 114/1000 | Loss: 0.00001136
Iteration 115/1000 | Loss: 0.00001136
Iteration 116/1000 | Loss: 0.00001136
Iteration 117/1000 | Loss: 0.00001135
Iteration 118/1000 | Loss: 0.00001135
Iteration 119/1000 | Loss: 0.00001135
Iteration 120/1000 | Loss: 0.00001135
Iteration 121/1000 | Loss: 0.00001135
Iteration 122/1000 | Loss: 0.00001134
Iteration 123/1000 | Loss: 0.00001134
Iteration 124/1000 | Loss: 0.00001134
Iteration 125/1000 | Loss: 0.00001134
Iteration 126/1000 | Loss: 0.00001134
Iteration 127/1000 | Loss: 0.00001134
Iteration 128/1000 | Loss: 0.00001134
Iteration 129/1000 | Loss: 0.00001134
Iteration 130/1000 | Loss: 0.00001133
Iteration 131/1000 | Loss: 0.00001133
Iteration 132/1000 | Loss: 0.00001133
Iteration 133/1000 | Loss: 0.00001132
Iteration 134/1000 | Loss: 0.00001132
Iteration 135/1000 | Loss: 0.00001132
Iteration 136/1000 | Loss: 0.00001132
Iteration 137/1000 | Loss: 0.00001132
Iteration 138/1000 | Loss: 0.00001132
Iteration 139/1000 | Loss: 0.00001132
Iteration 140/1000 | Loss: 0.00001132
Iteration 141/1000 | Loss: 0.00001132
Iteration 142/1000 | Loss: 0.00001132
Iteration 143/1000 | Loss: 0.00001132
Iteration 144/1000 | Loss: 0.00001132
Iteration 145/1000 | Loss: 0.00001132
Iteration 146/1000 | Loss: 0.00001132
Iteration 147/1000 | Loss: 0.00001132
Iteration 148/1000 | Loss: 0.00001132
Iteration 149/1000 | Loss: 0.00001132
Iteration 150/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.1316705240460578e-05, 1.1316705240460578e-05, 1.1316705240460578e-05, 1.1316705240460578e-05, 1.1316705240460578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1316705240460578e-05

Optimization complete. Final v2v error: 2.8671164512634277 mm

Highest mean error: 3.0529394149780273 mm for frame 53

Lowest mean error: 2.693660259246826 mm for frame 128

Saving results

Total time: 32.73606872558594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442626
Iteration 2/25 | Loss: 0.00079052
Iteration 3/25 | Loss: 0.00068077
Iteration 4/25 | Loss: 0.00065767
Iteration 5/25 | Loss: 0.00065497
Iteration 6/25 | Loss: 0.00065416
Iteration 7/25 | Loss: 0.00065408
Iteration 8/25 | Loss: 0.00065408
Iteration 9/25 | Loss: 0.00065408
Iteration 10/25 | Loss: 0.00065408
Iteration 11/25 | Loss: 0.00065408
Iteration 12/25 | Loss: 0.00065408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006540842005051672, 0.0006540842005051672, 0.0006540842005051672, 0.0006540842005051672, 0.0006540842005051672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006540842005051672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46009707
Iteration 2/25 | Loss: 0.00029635
Iteration 3/25 | Loss: 0.00029635
Iteration 4/25 | Loss: 0.00029635
Iteration 5/25 | Loss: 0.00029635
Iteration 6/25 | Loss: 0.00029635
Iteration 7/25 | Loss: 0.00029635
Iteration 8/25 | Loss: 0.00029635
Iteration 9/25 | Loss: 0.00029635
Iteration 10/25 | Loss: 0.00029635
Iteration 11/25 | Loss: 0.00029635
Iteration 12/25 | Loss: 0.00029635
Iteration 13/25 | Loss: 0.00029635
Iteration 14/25 | Loss: 0.00029635
Iteration 15/25 | Loss: 0.00029635
Iteration 16/25 | Loss: 0.00029635
Iteration 17/25 | Loss: 0.00029635
Iteration 18/25 | Loss: 0.00029635
Iteration 19/25 | Loss: 0.00029635
Iteration 20/25 | Loss: 0.00029635
Iteration 21/25 | Loss: 0.00029635
Iteration 22/25 | Loss: 0.00029635
Iteration 23/25 | Loss: 0.00029635
Iteration 24/25 | Loss: 0.00029635
Iteration 25/25 | Loss: 0.00029635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029635
Iteration 2/1000 | Loss: 0.00002957
Iteration 3/1000 | Loss: 0.00002011
Iteration 4/1000 | Loss: 0.00001869
Iteration 5/1000 | Loss: 0.00001785
Iteration 6/1000 | Loss: 0.00001735
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001654
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001653
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001652
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001649
Iteration 18/1000 | Loss: 0.00001648
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001639
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001635
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001630
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001629
Iteration 28/1000 | Loss: 0.00001627
Iteration 29/1000 | Loss: 0.00001623
Iteration 30/1000 | Loss: 0.00001621
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001620
Iteration 34/1000 | Loss: 0.00001620
Iteration 35/1000 | Loss: 0.00001620
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001620
Iteration 39/1000 | Loss: 0.00001620
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001619
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001618
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001618
Iteration 46/1000 | Loss: 0.00001618
Iteration 47/1000 | Loss: 0.00001618
Iteration 48/1000 | Loss: 0.00001618
Iteration 49/1000 | Loss: 0.00001617
Iteration 50/1000 | Loss: 0.00001617
Iteration 51/1000 | Loss: 0.00001617
Iteration 52/1000 | Loss: 0.00001616
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001613
Iteration 58/1000 | Loss: 0.00001613
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001612
Iteration 62/1000 | Loss: 0.00001612
Iteration 63/1000 | Loss: 0.00001611
Iteration 64/1000 | Loss: 0.00001610
Iteration 65/1000 | Loss: 0.00001610
Iteration 66/1000 | Loss: 0.00001609
Iteration 67/1000 | Loss: 0.00001609
Iteration 68/1000 | Loss: 0.00001609
Iteration 69/1000 | Loss: 0.00001609
Iteration 70/1000 | Loss: 0.00001609
Iteration 71/1000 | Loss: 0.00001609
Iteration 72/1000 | Loss: 0.00001608
Iteration 73/1000 | Loss: 0.00001608
Iteration 74/1000 | Loss: 0.00001608
Iteration 75/1000 | Loss: 0.00001608
Iteration 76/1000 | Loss: 0.00001608
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001607
Iteration 80/1000 | Loss: 0.00001607
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001606
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001602
Iteration 93/1000 | Loss: 0.00001602
Iteration 94/1000 | Loss: 0.00001601
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001601
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001600
Iteration 99/1000 | Loss: 0.00001600
Iteration 100/1000 | Loss: 0.00001600
Iteration 101/1000 | Loss: 0.00001600
Iteration 102/1000 | Loss: 0.00001600
Iteration 103/1000 | Loss: 0.00001600
Iteration 104/1000 | Loss: 0.00001599
Iteration 105/1000 | Loss: 0.00001599
Iteration 106/1000 | Loss: 0.00001599
Iteration 107/1000 | Loss: 0.00001599
Iteration 108/1000 | Loss: 0.00001599
Iteration 109/1000 | Loss: 0.00001599
Iteration 110/1000 | Loss: 0.00001598
Iteration 111/1000 | Loss: 0.00001598
Iteration 112/1000 | Loss: 0.00001598
Iteration 113/1000 | Loss: 0.00001598
Iteration 114/1000 | Loss: 0.00001598
Iteration 115/1000 | Loss: 0.00001598
Iteration 116/1000 | Loss: 0.00001598
Iteration 117/1000 | Loss: 0.00001598
Iteration 118/1000 | Loss: 0.00001598
Iteration 119/1000 | Loss: 0.00001598
Iteration 120/1000 | Loss: 0.00001598
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001598
Iteration 134/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.5976263966877013e-05, 1.5976263966877013e-05, 1.5976263966877013e-05, 1.5976263966877013e-05, 1.5976263966877013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5976263966877013e-05

Optimization complete. Final v2v error: 3.389706611633301 mm

Highest mean error: 3.685624122619629 mm for frame 133

Lowest mean error: 3.2503981590270996 mm for frame 12

Saving results

Total time: 32.68870735168457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00345638
Iteration 2/25 | Loss: 0.00072332
Iteration 3/25 | Loss: 0.00059989
Iteration 4/25 | Loss: 0.00058252
Iteration 5/25 | Loss: 0.00057735
Iteration 6/25 | Loss: 0.00057574
Iteration 7/25 | Loss: 0.00057531
Iteration 8/25 | Loss: 0.00057531
Iteration 9/25 | Loss: 0.00057531
Iteration 10/25 | Loss: 0.00057531
Iteration 11/25 | Loss: 0.00057531
Iteration 12/25 | Loss: 0.00057531
Iteration 13/25 | Loss: 0.00057531
Iteration 14/25 | Loss: 0.00057531
Iteration 15/25 | Loss: 0.00057531
Iteration 16/25 | Loss: 0.00057531
Iteration 17/25 | Loss: 0.00057531
Iteration 18/25 | Loss: 0.00057531
Iteration 19/25 | Loss: 0.00057531
Iteration 20/25 | Loss: 0.00057531
Iteration 21/25 | Loss: 0.00057531
Iteration 22/25 | Loss: 0.00057531
Iteration 23/25 | Loss: 0.00057531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005753118311986327, 0.0005753118311986327, 0.0005753118311986327, 0.0005753118311986327, 0.0005753118311986327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005753118311986327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75420213
Iteration 2/25 | Loss: 0.00029780
Iteration 3/25 | Loss: 0.00029780
Iteration 4/25 | Loss: 0.00029780
Iteration 5/25 | Loss: 0.00029780
Iteration 6/25 | Loss: 0.00029780
Iteration 7/25 | Loss: 0.00029779
Iteration 8/25 | Loss: 0.00029779
Iteration 9/25 | Loss: 0.00029779
Iteration 10/25 | Loss: 0.00029779
Iteration 11/25 | Loss: 0.00029779
Iteration 12/25 | Loss: 0.00029779
Iteration 13/25 | Loss: 0.00029779
Iteration 14/25 | Loss: 0.00029779
Iteration 15/25 | Loss: 0.00029779
Iteration 16/25 | Loss: 0.00029779
Iteration 17/25 | Loss: 0.00029779
Iteration 18/25 | Loss: 0.00029779
Iteration 19/25 | Loss: 0.00029779
Iteration 20/25 | Loss: 0.00029779
Iteration 21/25 | Loss: 0.00029779
Iteration 22/25 | Loss: 0.00029779
Iteration 23/25 | Loss: 0.00029779
Iteration 24/25 | Loss: 0.00029779
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002977937110699713, 0.0002977937110699713, 0.0002977937110699713, 0.0002977937110699713, 0.0002977937110699713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002977937110699713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029779
Iteration 2/1000 | Loss: 0.00001960
Iteration 3/1000 | Loss: 0.00001124
Iteration 4/1000 | Loss: 0.00001040
Iteration 5/1000 | Loss: 0.00000987
Iteration 6/1000 | Loss: 0.00000961
Iteration 7/1000 | Loss: 0.00000941
Iteration 8/1000 | Loss: 0.00000936
Iteration 9/1000 | Loss: 0.00000932
Iteration 10/1000 | Loss: 0.00000931
Iteration 11/1000 | Loss: 0.00000931
Iteration 12/1000 | Loss: 0.00000930
Iteration 13/1000 | Loss: 0.00000929
Iteration 14/1000 | Loss: 0.00000929
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000927
Iteration 17/1000 | Loss: 0.00000927
Iteration 18/1000 | Loss: 0.00000926
Iteration 19/1000 | Loss: 0.00000923
Iteration 20/1000 | Loss: 0.00000922
Iteration 21/1000 | Loss: 0.00000921
Iteration 22/1000 | Loss: 0.00000921
Iteration 23/1000 | Loss: 0.00000921
Iteration 24/1000 | Loss: 0.00000920
Iteration 25/1000 | Loss: 0.00000918
Iteration 26/1000 | Loss: 0.00000918
Iteration 27/1000 | Loss: 0.00000917
Iteration 28/1000 | Loss: 0.00000917
Iteration 29/1000 | Loss: 0.00000917
Iteration 30/1000 | Loss: 0.00000917
Iteration 31/1000 | Loss: 0.00000917
Iteration 32/1000 | Loss: 0.00000917
Iteration 33/1000 | Loss: 0.00000917
Iteration 34/1000 | Loss: 0.00000917
Iteration 35/1000 | Loss: 0.00000917
Iteration 36/1000 | Loss: 0.00000917
Iteration 37/1000 | Loss: 0.00000916
Iteration 38/1000 | Loss: 0.00000916
Iteration 39/1000 | Loss: 0.00000912
Iteration 40/1000 | Loss: 0.00000911
Iteration 41/1000 | Loss: 0.00000911
Iteration 42/1000 | Loss: 0.00000908
Iteration 43/1000 | Loss: 0.00000908
Iteration 44/1000 | Loss: 0.00000908
Iteration 45/1000 | Loss: 0.00000908
Iteration 46/1000 | Loss: 0.00000908
Iteration 47/1000 | Loss: 0.00000907
Iteration 48/1000 | Loss: 0.00000907
Iteration 49/1000 | Loss: 0.00000906
Iteration 50/1000 | Loss: 0.00000906
Iteration 51/1000 | Loss: 0.00000906
Iteration 52/1000 | Loss: 0.00000906
Iteration 53/1000 | Loss: 0.00000905
Iteration 54/1000 | Loss: 0.00000905
Iteration 55/1000 | Loss: 0.00000905
Iteration 56/1000 | Loss: 0.00000905
Iteration 57/1000 | Loss: 0.00000905
Iteration 58/1000 | Loss: 0.00000905
Iteration 59/1000 | Loss: 0.00000904
Iteration 60/1000 | Loss: 0.00000904
Iteration 61/1000 | Loss: 0.00000904
Iteration 62/1000 | Loss: 0.00000904
Iteration 63/1000 | Loss: 0.00000904
Iteration 64/1000 | Loss: 0.00000904
Iteration 65/1000 | Loss: 0.00000904
Iteration 66/1000 | Loss: 0.00000904
Iteration 67/1000 | Loss: 0.00000904
Iteration 68/1000 | Loss: 0.00000904
Iteration 69/1000 | Loss: 0.00000904
Iteration 70/1000 | Loss: 0.00000904
Iteration 71/1000 | Loss: 0.00000904
Iteration 72/1000 | Loss: 0.00000904
Iteration 73/1000 | Loss: 0.00000904
Iteration 74/1000 | Loss: 0.00000904
Iteration 75/1000 | Loss: 0.00000904
Iteration 76/1000 | Loss: 0.00000904
Iteration 77/1000 | Loss: 0.00000904
Iteration 78/1000 | Loss: 0.00000904
Iteration 79/1000 | Loss: 0.00000904
Iteration 80/1000 | Loss: 0.00000904
Iteration 81/1000 | Loss: 0.00000904
Iteration 82/1000 | Loss: 0.00000904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [9.040690201800317e-06, 9.040690201800317e-06, 9.040690201800317e-06, 9.040690201800317e-06, 9.040690201800317e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.040690201800317e-06

Optimization complete. Final v2v error: 2.567913055419922 mm

Highest mean error: 3.0540518760681152 mm for frame 75

Lowest mean error: 2.4854414463043213 mm for frame 109

Saving results

Total time: 27.209781646728516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886509
Iteration 2/25 | Loss: 0.00100107
Iteration 3/25 | Loss: 0.00071210
Iteration 4/25 | Loss: 0.00066946
Iteration 5/25 | Loss: 0.00065315
Iteration 6/25 | Loss: 0.00064644
Iteration 7/25 | Loss: 0.00064823
Iteration 8/25 | Loss: 0.00065382
Iteration 9/25 | Loss: 0.00063316
Iteration 10/25 | Loss: 0.00062922
Iteration 11/25 | Loss: 0.00062844
Iteration 12/25 | Loss: 0.00062825
Iteration 13/25 | Loss: 0.00062816
Iteration 14/25 | Loss: 0.00062802
Iteration 15/25 | Loss: 0.00062782
Iteration 16/25 | Loss: 0.00062718
Iteration 17/25 | Loss: 0.00063275
Iteration 18/25 | Loss: 0.00062462
Iteration 19/25 | Loss: 0.00062285
Iteration 20/25 | Loss: 0.00062251
Iteration 21/25 | Loss: 0.00062226
Iteration 22/25 | Loss: 0.00062219
Iteration 23/25 | Loss: 0.00062219
Iteration 24/25 | Loss: 0.00062219
Iteration 25/25 | Loss: 0.00062219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58675313
Iteration 2/25 | Loss: 0.00029563
Iteration 3/25 | Loss: 0.00029562
Iteration 4/25 | Loss: 0.00029562
Iteration 5/25 | Loss: 0.00029562
Iteration 6/25 | Loss: 0.00029562
Iteration 7/25 | Loss: 0.00029562
Iteration 8/25 | Loss: 0.00029562
Iteration 9/25 | Loss: 0.00029562
Iteration 10/25 | Loss: 0.00029562
Iteration 11/25 | Loss: 0.00029562
Iteration 12/25 | Loss: 0.00029561
Iteration 13/25 | Loss: 0.00029561
Iteration 14/25 | Loss: 0.00029561
Iteration 15/25 | Loss: 0.00029561
Iteration 16/25 | Loss: 0.00029561
Iteration 17/25 | Loss: 0.00029561
Iteration 18/25 | Loss: 0.00029561
Iteration 19/25 | Loss: 0.00029561
Iteration 20/25 | Loss: 0.00029561
Iteration 21/25 | Loss: 0.00029561
Iteration 22/25 | Loss: 0.00029561
Iteration 23/25 | Loss: 0.00029561
Iteration 24/25 | Loss: 0.00029561
Iteration 25/25 | Loss: 0.00029561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029561
Iteration 2/1000 | Loss: 0.00002772
Iteration 3/1000 | Loss: 0.00001840
Iteration 4/1000 | Loss: 0.00001651
Iteration 5/1000 | Loss: 0.00001579
Iteration 6/1000 | Loss: 0.00001517
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001453
Iteration 9/1000 | Loss: 0.00001438
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001414
Iteration 14/1000 | Loss: 0.00001411
Iteration 15/1000 | Loss: 0.00001410
Iteration 16/1000 | Loss: 0.00001410
Iteration 17/1000 | Loss: 0.00001409
Iteration 18/1000 | Loss: 0.00001408
Iteration 19/1000 | Loss: 0.00001407
Iteration 20/1000 | Loss: 0.00001406
Iteration 21/1000 | Loss: 0.00001406
Iteration 22/1000 | Loss: 0.00001406
Iteration 23/1000 | Loss: 0.00001405
Iteration 24/1000 | Loss: 0.00001405
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001403
Iteration 29/1000 | Loss: 0.00001402
Iteration 30/1000 | Loss: 0.00001402
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00001400
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001397
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00001395
Iteration 41/1000 | Loss: 0.00001395
Iteration 42/1000 | Loss: 0.00001394
Iteration 43/1000 | Loss: 0.00001394
Iteration 44/1000 | Loss: 0.00001394
Iteration 45/1000 | Loss: 0.00001394
Iteration 46/1000 | Loss: 0.00001394
Iteration 47/1000 | Loss: 0.00001393
Iteration 48/1000 | Loss: 0.00001393
Iteration 49/1000 | Loss: 0.00001393
Iteration 50/1000 | Loss: 0.00001393
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001393
Iteration 54/1000 | Loss: 0.00001393
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001391
Iteration 58/1000 | Loss: 0.00001391
Iteration 59/1000 | Loss: 0.00001391
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001390
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001389
Iteration 68/1000 | Loss: 0.00001389
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001384
Iteration 89/1000 | Loss: 0.00001384
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001384
Iteration 92/1000 | Loss: 0.00001383
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Iteration 109/1000 | Loss: 0.00001381
Iteration 110/1000 | Loss: 0.00001380
Iteration 111/1000 | Loss: 0.00001380
Iteration 112/1000 | Loss: 0.00001380
Iteration 113/1000 | Loss: 0.00001380
Iteration 114/1000 | Loss: 0.00001379
Iteration 115/1000 | Loss: 0.00001379
Iteration 116/1000 | Loss: 0.00001379
Iteration 117/1000 | Loss: 0.00001379
Iteration 118/1000 | Loss: 0.00001379
Iteration 119/1000 | Loss: 0.00001378
Iteration 120/1000 | Loss: 0.00001378
Iteration 121/1000 | Loss: 0.00001378
Iteration 122/1000 | Loss: 0.00001377
Iteration 123/1000 | Loss: 0.00001377
Iteration 124/1000 | Loss: 0.00001377
Iteration 125/1000 | Loss: 0.00001377
Iteration 126/1000 | Loss: 0.00001377
Iteration 127/1000 | Loss: 0.00001377
Iteration 128/1000 | Loss: 0.00001376
Iteration 129/1000 | Loss: 0.00001376
Iteration 130/1000 | Loss: 0.00001376
Iteration 131/1000 | Loss: 0.00001376
Iteration 132/1000 | Loss: 0.00001376
Iteration 133/1000 | Loss: 0.00001376
Iteration 134/1000 | Loss: 0.00001376
Iteration 135/1000 | Loss: 0.00001376
Iteration 136/1000 | Loss: 0.00001375
Iteration 137/1000 | Loss: 0.00001375
Iteration 138/1000 | Loss: 0.00001375
Iteration 139/1000 | Loss: 0.00001375
Iteration 140/1000 | Loss: 0.00001375
Iteration 141/1000 | Loss: 0.00001375
Iteration 142/1000 | Loss: 0.00001375
Iteration 143/1000 | Loss: 0.00001375
Iteration 144/1000 | Loss: 0.00001374
Iteration 145/1000 | Loss: 0.00001374
Iteration 146/1000 | Loss: 0.00001374
Iteration 147/1000 | Loss: 0.00001374
Iteration 148/1000 | Loss: 0.00001374
Iteration 149/1000 | Loss: 0.00001374
Iteration 150/1000 | Loss: 0.00001374
Iteration 151/1000 | Loss: 0.00001374
Iteration 152/1000 | Loss: 0.00001374
Iteration 153/1000 | Loss: 0.00001374
Iteration 154/1000 | Loss: 0.00001374
Iteration 155/1000 | Loss: 0.00001374
Iteration 156/1000 | Loss: 0.00001374
Iteration 157/1000 | Loss: 0.00001374
Iteration 158/1000 | Loss: 0.00001374
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001374
Iteration 161/1000 | Loss: 0.00001374
Iteration 162/1000 | Loss: 0.00001374
Iteration 163/1000 | Loss: 0.00001373
Iteration 164/1000 | Loss: 0.00001373
Iteration 165/1000 | Loss: 0.00001373
Iteration 166/1000 | Loss: 0.00001373
Iteration 167/1000 | Loss: 0.00001373
Iteration 168/1000 | Loss: 0.00001373
Iteration 169/1000 | Loss: 0.00001373
Iteration 170/1000 | Loss: 0.00001373
Iteration 171/1000 | Loss: 0.00001373
Iteration 172/1000 | Loss: 0.00001373
Iteration 173/1000 | Loss: 0.00001373
Iteration 174/1000 | Loss: 0.00001373
Iteration 175/1000 | Loss: 0.00001373
Iteration 176/1000 | Loss: 0.00001373
Iteration 177/1000 | Loss: 0.00001373
Iteration 178/1000 | Loss: 0.00001373
Iteration 179/1000 | Loss: 0.00001373
Iteration 180/1000 | Loss: 0.00001373
Iteration 181/1000 | Loss: 0.00001373
Iteration 182/1000 | Loss: 0.00001373
Iteration 183/1000 | Loss: 0.00001373
Iteration 184/1000 | Loss: 0.00001373
Iteration 185/1000 | Loss: 0.00001373
Iteration 186/1000 | Loss: 0.00001373
Iteration 187/1000 | Loss: 0.00001372
Iteration 188/1000 | Loss: 0.00001372
Iteration 189/1000 | Loss: 0.00001372
Iteration 190/1000 | Loss: 0.00001372
Iteration 191/1000 | Loss: 0.00001372
Iteration 192/1000 | Loss: 0.00001372
Iteration 193/1000 | Loss: 0.00001372
Iteration 194/1000 | Loss: 0.00001372
Iteration 195/1000 | Loss: 0.00001372
Iteration 196/1000 | Loss: 0.00001372
Iteration 197/1000 | Loss: 0.00001372
Iteration 198/1000 | Loss: 0.00001372
Iteration 199/1000 | Loss: 0.00001372
Iteration 200/1000 | Loss: 0.00001372
Iteration 201/1000 | Loss: 0.00001372
Iteration 202/1000 | Loss: 0.00001372
Iteration 203/1000 | Loss: 0.00001372
Iteration 204/1000 | Loss: 0.00001372
Iteration 205/1000 | Loss: 0.00001372
Iteration 206/1000 | Loss: 0.00001372
Iteration 207/1000 | Loss: 0.00001372
Iteration 208/1000 | Loss: 0.00001372
Iteration 209/1000 | Loss: 0.00001372
Iteration 210/1000 | Loss: 0.00001372
Iteration 211/1000 | Loss: 0.00001372
Iteration 212/1000 | Loss: 0.00001372
Iteration 213/1000 | Loss: 0.00001372
Iteration 214/1000 | Loss: 0.00001372
Iteration 215/1000 | Loss: 0.00001372
Iteration 216/1000 | Loss: 0.00001372
Iteration 217/1000 | Loss: 0.00001372
Iteration 218/1000 | Loss: 0.00001372
Iteration 219/1000 | Loss: 0.00001372
Iteration 220/1000 | Loss: 0.00001372
Iteration 221/1000 | Loss: 0.00001372
Iteration 222/1000 | Loss: 0.00001372
Iteration 223/1000 | Loss: 0.00001372
Iteration 224/1000 | Loss: 0.00001372
Iteration 225/1000 | Loss: 0.00001372
Iteration 226/1000 | Loss: 0.00001372
Iteration 227/1000 | Loss: 0.00001372
Iteration 228/1000 | Loss: 0.00001372
Iteration 229/1000 | Loss: 0.00001372
Iteration 230/1000 | Loss: 0.00001372
Iteration 231/1000 | Loss: 0.00001372
Iteration 232/1000 | Loss: 0.00001372
Iteration 233/1000 | Loss: 0.00001372
Iteration 234/1000 | Loss: 0.00001372
Iteration 235/1000 | Loss: 0.00001372
Iteration 236/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.3720096831093542e-05, 1.3720096831093542e-05, 1.3720096831093542e-05, 1.3720096831093542e-05, 1.3720096831093542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3720096831093542e-05

Optimization complete. Final v2v error: 3.075183868408203 mm

Highest mean error: 4.2383246421813965 mm for frame 96

Lowest mean error: 2.626082420349121 mm for frame 130

Saving results

Total time: 65.37632822990417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044650
Iteration 2/25 | Loss: 0.01044649
Iteration 3/25 | Loss: 0.01044649
Iteration 4/25 | Loss: 0.00290486
Iteration 5/25 | Loss: 0.00218238
Iteration 6/25 | Loss: 0.00204812
Iteration 7/25 | Loss: 0.00180680
Iteration 8/25 | Loss: 0.00176900
Iteration 9/25 | Loss: 0.00139205
Iteration 10/25 | Loss: 0.00131436
Iteration 11/25 | Loss: 0.00122782
Iteration 12/25 | Loss: 0.00111609
Iteration 13/25 | Loss: 0.00109447
Iteration 14/25 | Loss: 0.00106098
Iteration 15/25 | Loss: 0.00104973
Iteration 16/25 | Loss: 0.00104300
Iteration 17/25 | Loss: 0.00103654
Iteration 18/25 | Loss: 0.00103822
Iteration 19/25 | Loss: 0.00103663
Iteration 20/25 | Loss: 0.00103498
Iteration 21/25 | Loss: 0.00102934
Iteration 22/25 | Loss: 0.00102523
Iteration 23/25 | Loss: 0.00102342
Iteration 24/25 | Loss: 0.00102284
Iteration 25/25 | Loss: 0.00102251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43602157
Iteration 2/25 | Loss: 0.00223725
Iteration 3/25 | Loss: 0.00223725
Iteration 4/25 | Loss: 0.00223725
Iteration 5/25 | Loss: 0.00223725
Iteration 6/25 | Loss: 0.00223724
Iteration 7/25 | Loss: 0.00223724
Iteration 8/25 | Loss: 0.00223724
Iteration 9/25 | Loss: 0.00223724
Iteration 10/25 | Loss: 0.00223724
Iteration 11/25 | Loss: 0.00223724
Iteration 12/25 | Loss: 0.00223724
Iteration 13/25 | Loss: 0.00223724
Iteration 14/25 | Loss: 0.00223724
Iteration 15/25 | Loss: 0.00223724
Iteration 16/25 | Loss: 0.00223724
Iteration 17/25 | Loss: 0.00223724
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002237243577837944, 0.002237243577837944, 0.002237243577837944, 0.002237243577837944, 0.002237243577837944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002237243577837944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223724
Iteration 2/1000 | Loss: 0.00481175
Iteration 3/1000 | Loss: 0.00041563
Iteration 4/1000 | Loss: 0.00021896
Iteration 5/1000 | Loss: 0.00014184
Iteration 6/1000 | Loss: 0.00014897
Iteration 7/1000 | Loss: 0.00006195
Iteration 8/1000 | Loss: 0.00004547
Iteration 9/1000 | Loss: 0.00003732
Iteration 10/1000 | Loss: 0.00003148
Iteration 11/1000 | Loss: 0.00003364
Iteration 12/1000 | Loss: 0.00002666
Iteration 13/1000 | Loss: 0.00002401
Iteration 14/1000 | Loss: 0.00002232
Iteration 15/1000 | Loss: 0.00002145
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002004
Iteration 18/1000 | Loss: 0.00001970
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00001936
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001927
Iteration 23/1000 | Loss: 0.00001919
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001911
Iteration 26/1000 | Loss: 0.00001909
Iteration 27/1000 | Loss: 0.00001904
Iteration 28/1000 | Loss: 0.00001903
Iteration 29/1000 | Loss: 0.00001901
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001900
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00001896
Iteration 36/1000 | Loss: 0.00001895
Iteration 37/1000 | Loss: 0.00001895
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001894
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001891
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001891
Iteration 46/1000 | Loss: 0.00001891
Iteration 47/1000 | Loss: 0.00001891
Iteration 48/1000 | Loss: 0.00001891
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001890
Iteration 57/1000 | Loss: 0.00001890
Iteration 58/1000 | Loss: 0.00001890
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00001889
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001888
Iteration 69/1000 | Loss: 0.00001887
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001885
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001884
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001882
Iteration 79/1000 | Loss: 0.00001882
Iteration 80/1000 | Loss: 0.00001882
Iteration 81/1000 | Loss: 0.00001880
Iteration 82/1000 | Loss: 0.00001880
Iteration 83/1000 | Loss: 0.00001880
Iteration 84/1000 | Loss: 0.00001880
Iteration 85/1000 | Loss: 0.00001880
Iteration 86/1000 | Loss: 0.00001880
Iteration 87/1000 | Loss: 0.00001880
Iteration 88/1000 | Loss: 0.00001879
Iteration 89/1000 | Loss: 0.00001879
Iteration 90/1000 | Loss: 0.00001879
Iteration 91/1000 | Loss: 0.00001879
Iteration 92/1000 | Loss: 0.00001879
Iteration 93/1000 | Loss: 0.00001879
Iteration 94/1000 | Loss: 0.00001879
Iteration 95/1000 | Loss: 0.00001878
Iteration 96/1000 | Loss: 0.00001878
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001878
Iteration 100/1000 | Loss: 0.00001877
Iteration 101/1000 | Loss: 0.00001877
Iteration 102/1000 | Loss: 0.00001876
Iteration 103/1000 | Loss: 0.00001876
Iteration 104/1000 | Loss: 0.00001875
Iteration 105/1000 | Loss: 0.00001875
Iteration 106/1000 | Loss: 0.00001875
Iteration 107/1000 | Loss: 0.00001875
Iteration 108/1000 | Loss: 0.00001875
Iteration 109/1000 | Loss: 0.00001875
Iteration 110/1000 | Loss: 0.00001874
Iteration 111/1000 | Loss: 0.00001874
Iteration 112/1000 | Loss: 0.00001874
Iteration 113/1000 | Loss: 0.00001874
Iteration 114/1000 | Loss: 0.00001874
Iteration 115/1000 | Loss: 0.00001874
Iteration 116/1000 | Loss: 0.00001873
Iteration 117/1000 | Loss: 0.00001873
Iteration 118/1000 | Loss: 0.00001873
Iteration 119/1000 | Loss: 0.00001873
Iteration 120/1000 | Loss: 0.00001873
Iteration 121/1000 | Loss: 0.00001873
Iteration 122/1000 | Loss: 0.00001873
Iteration 123/1000 | Loss: 0.00001873
Iteration 124/1000 | Loss: 0.00001873
Iteration 125/1000 | Loss: 0.00001873
Iteration 126/1000 | Loss: 0.00001873
Iteration 127/1000 | Loss: 0.00001873
Iteration 128/1000 | Loss: 0.00001873
Iteration 129/1000 | Loss: 0.00001873
Iteration 130/1000 | Loss: 0.00001872
Iteration 131/1000 | Loss: 0.00001872
Iteration 132/1000 | Loss: 0.00001872
Iteration 133/1000 | Loss: 0.00001872
Iteration 134/1000 | Loss: 0.00001872
Iteration 135/1000 | Loss: 0.00001872
Iteration 136/1000 | Loss: 0.00001871
Iteration 137/1000 | Loss: 0.00001871
Iteration 138/1000 | Loss: 0.00001871
Iteration 139/1000 | Loss: 0.00001871
Iteration 140/1000 | Loss: 0.00001871
Iteration 141/1000 | Loss: 0.00001871
Iteration 142/1000 | Loss: 0.00001871
Iteration 143/1000 | Loss: 0.00001871
Iteration 144/1000 | Loss: 0.00001871
Iteration 145/1000 | Loss: 0.00001871
Iteration 146/1000 | Loss: 0.00001871
Iteration 147/1000 | Loss: 0.00001871
Iteration 148/1000 | Loss: 0.00001871
Iteration 149/1000 | Loss: 0.00001871
Iteration 150/1000 | Loss: 0.00001871
Iteration 151/1000 | Loss: 0.00001871
Iteration 152/1000 | Loss: 0.00001870
Iteration 153/1000 | Loss: 0.00001870
Iteration 154/1000 | Loss: 0.00001870
Iteration 155/1000 | Loss: 0.00001870
Iteration 156/1000 | Loss: 0.00001869
Iteration 157/1000 | Loss: 0.00001869
Iteration 158/1000 | Loss: 0.00001869
Iteration 159/1000 | Loss: 0.00001869
Iteration 160/1000 | Loss: 0.00001869
Iteration 161/1000 | Loss: 0.00001869
Iteration 162/1000 | Loss: 0.00001869
Iteration 163/1000 | Loss: 0.00001868
Iteration 164/1000 | Loss: 0.00001868
Iteration 165/1000 | Loss: 0.00001868
Iteration 166/1000 | Loss: 0.00001868
Iteration 167/1000 | Loss: 0.00001868
Iteration 168/1000 | Loss: 0.00001868
Iteration 169/1000 | Loss: 0.00001868
Iteration 170/1000 | Loss: 0.00001868
Iteration 171/1000 | Loss: 0.00001868
Iteration 172/1000 | Loss: 0.00001868
Iteration 173/1000 | Loss: 0.00001868
Iteration 174/1000 | Loss: 0.00001868
Iteration 175/1000 | Loss: 0.00001868
Iteration 176/1000 | Loss: 0.00001867
Iteration 177/1000 | Loss: 0.00001867
Iteration 178/1000 | Loss: 0.00001867
Iteration 179/1000 | Loss: 0.00001867
Iteration 180/1000 | Loss: 0.00001867
Iteration 181/1000 | Loss: 0.00001867
Iteration 182/1000 | Loss: 0.00001867
Iteration 183/1000 | Loss: 0.00001867
Iteration 184/1000 | Loss: 0.00001867
Iteration 185/1000 | Loss: 0.00001867
Iteration 186/1000 | Loss: 0.00001867
Iteration 187/1000 | Loss: 0.00001867
Iteration 188/1000 | Loss: 0.00001867
Iteration 189/1000 | Loss: 0.00001867
Iteration 190/1000 | Loss: 0.00001867
Iteration 191/1000 | Loss: 0.00001867
Iteration 192/1000 | Loss: 0.00001867
Iteration 193/1000 | Loss: 0.00001867
Iteration 194/1000 | Loss: 0.00001867
Iteration 195/1000 | Loss: 0.00001867
Iteration 196/1000 | Loss: 0.00001867
Iteration 197/1000 | Loss: 0.00001867
Iteration 198/1000 | Loss: 0.00001867
Iteration 199/1000 | Loss: 0.00001867
Iteration 200/1000 | Loss: 0.00001867
Iteration 201/1000 | Loss: 0.00001867
Iteration 202/1000 | Loss: 0.00001867
Iteration 203/1000 | Loss: 0.00001867
Iteration 204/1000 | Loss: 0.00001867
Iteration 205/1000 | Loss: 0.00001867
Iteration 206/1000 | Loss: 0.00001867
Iteration 207/1000 | Loss: 0.00001867
Iteration 208/1000 | Loss: 0.00001867
Iteration 209/1000 | Loss: 0.00001867
Iteration 210/1000 | Loss: 0.00001867
Iteration 211/1000 | Loss: 0.00001867
Iteration 212/1000 | Loss: 0.00001867
Iteration 213/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.866677303041797e-05, 1.866677303041797e-05, 1.866677303041797e-05, 1.866677303041797e-05, 1.866677303041797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.866677303041797e-05

Optimization complete. Final v2v error: 3.64038348197937 mm

Highest mean error: 3.9018499851226807 mm for frame 32

Lowest mean error: 3.4768571853637695 mm for frame 89

Saving results

Total time: 95.49555563926697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487530
Iteration 2/25 | Loss: 0.00089957
Iteration 3/25 | Loss: 0.00077153
Iteration 4/25 | Loss: 0.00074045
Iteration 5/25 | Loss: 0.00073059
Iteration 6/25 | Loss: 0.00072909
Iteration 7/25 | Loss: 0.00072873
Iteration 8/25 | Loss: 0.00072873
Iteration 9/25 | Loss: 0.00072873
Iteration 10/25 | Loss: 0.00072873
Iteration 11/25 | Loss: 0.00072873
Iteration 12/25 | Loss: 0.00072873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007287338958121836, 0.0007287338958121836, 0.0007287338958121836, 0.0007287338958121836, 0.0007287338958121836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007287338958121836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41301751
Iteration 2/25 | Loss: 0.00026478
Iteration 3/25 | Loss: 0.00026478
Iteration 4/25 | Loss: 0.00026478
Iteration 5/25 | Loss: 0.00026478
Iteration 6/25 | Loss: 0.00026477
Iteration 7/25 | Loss: 0.00026477
Iteration 8/25 | Loss: 0.00026477
Iteration 9/25 | Loss: 0.00026477
Iteration 10/25 | Loss: 0.00026477
Iteration 11/25 | Loss: 0.00026477
Iteration 12/25 | Loss: 0.00026477
Iteration 13/25 | Loss: 0.00026477
Iteration 14/25 | Loss: 0.00026477
Iteration 15/25 | Loss: 0.00026477
Iteration 16/25 | Loss: 0.00026477
Iteration 17/25 | Loss: 0.00026477
Iteration 18/25 | Loss: 0.00026477
Iteration 19/25 | Loss: 0.00026477
Iteration 20/25 | Loss: 0.00026477
Iteration 21/25 | Loss: 0.00026477
Iteration 22/25 | Loss: 0.00026477
Iteration 23/25 | Loss: 0.00026477
Iteration 24/25 | Loss: 0.00026477
Iteration 25/25 | Loss: 0.00026477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026477
Iteration 2/1000 | Loss: 0.00005523
Iteration 3/1000 | Loss: 0.00003946
Iteration 4/1000 | Loss: 0.00003641
Iteration 5/1000 | Loss: 0.00003416
Iteration 6/1000 | Loss: 0.00003326
Iteration 7/1000 | Loss: 0.00003227
Iteration 8/1000 | Loss: 0.00003181
Iteration 9/1000 | Loss: 0.00003142
Iteration 10/1000 | Loss: 0.00003117
Iteration 11/1000 | Loss: 0.00003105
Iteration 12/1000 | Loss: 0.00003086
Iteration 13/1000 | Loss: 0.00003078
Iteration 14/1000 | Loss: 0.00003078
Iteration 15/1000 | Loss: 0.00003078
Iteration 16/1000 | Loss: 0.00003078
Iteration 17/1000 | Loss: 0.00003078
Iteration 18/1000 | Loss: 0.00003078
Iteration 19/1000 | Loss: 0.00003078
Iteration 20/1000 | Loss: 0.00003077
Iteration 21/1000 | Loss: 0.00003076
Iteration 22/1000 | Loss: 0.00003075
Iteration 23/1000 | Loss: 0.00003073
Iteration 24/1000 | Loss: 0.00003072
Iteration 25/1000 | Loss: 0.00003072
Iteration 26/1000 | Loss: 0.00003067
Iteration 27/1000 | Loss: 0.00003067
Iteration 28/1000 | Loss: 0.00003065
Iteration 29/1000 | Loss: 0.00003065
Iteration 30/1000 | Loss: 0.00003063
Iteration 31/1000 | Loss: 0.00003063
Iteration 32/1000 | Loss: 0.00003062
Iteration 33/1000 | Loss: 0.00003060
Iteration 34/1000 | Loss: 0.00003059
Iteration 35/1000 | Loss: 0.00003050
Iteration 36/1000 | Loss: 0.00003049
Iteration 37/1000 | Loss: 0.00003049
Iteration 38/1000 | Loss: 0.00003048
Iteration 39/1000 | Loss: 0.00003047
Iteration 40/1000 | Loss: 0.00003047
Iteration 41/1000 | Loss: 0.00003047
Iteration 42/1000 | Loss: 0.00003046
Iteration 43/1000 | Loss: 0.00003046
Iteration 44/1000 | Loss: 0.00003046
Iteration 45/1000 | Loss: 0.00003046
Iteration 46/1000 | Loss: 0.00003046
Iteration 47/1000 | Loss: 0.00003046
Iteration 48/1000 | Loss: 0.00003045
Iteration 49/1000 | Loss: 0.00003045
Iteration 50/1000 | Loss: 0.00003045
Iteration 51/1000 | Loss: 0.00003045
Iteration 52/1000 | Loss: 0.00003045
Iteration 53/1000 | Loss: 0.00003044
Iteration 54/1000 | Loss: 0.00003044
Iteration 55/1000 | Loss: 0.00003044
Iteration 56/1000 | Loss: 0.00003043
Iteration 57/1000 | Loss: 0.00003043
Iteration 58/1000 | Loss: 0.00003043
Iteration 59/1000 | Loss: 0.00003043
Iteration 60/1000 | Loss: 0.00003043
Iteration 61/1000 | Loss: 0.00003043
Iteration 62/1000 | Loss: 0.00003043
Iteration 63/1000 | Loss: 0.00003043
Iteration 64/1000 | Loss: 0.00003043
Iteration 65/1000 | Loss: 0.00003043
Iteration 66/1000 | Loss: 0.00003043
Iteration 67/1000 | Loss: 0.00003043
Iteration 68/1000 | Loss: 0.00003043
Iteration 69/1000 | Loss: 0.00003043
Iteration 70/1000 | Loss: 0.00003042
Iteration 71/1000 | Loss: 0.00003042
Iteration 72/1000 | Loss: 0.00003042
Iteration 73/1000 | Loss: 0.00003041
Iteration 74/1000 | Loss: 0.00003041
Iteration 75/1000 | Loss: 0.00003041
Iteration 76/1000 | Loss: 0.00003041
Iteration 77/1000 | Loss: 0.00003041
Iteration 78/1000 | Loss: 0.00003041
Iteration 79/1000 | Loss: 0.00003041
Iteration 80/1000 | Loss: 0.00003041
Iteration 81/1000 | Loss: 0.00003041
Iteration 82/1000 | Loss: 0.00003040
Iteration 83/1000 | Loss: 0.00003040
Iteration 84/1000 | Loss: 0.00003040
Iteration 85/1000 | Loss: 0.00003040
Iteration 86/1000 | Loss: 0.00003040
Iteration 87/1000 | Loss: 0.00003039
Iteration 88/1000 | Loss: 0.00003039
Iteration 89/1000 | Loss: 0.00003039
Iteration 90/1000 | Loss: 0.00003039
Iteration 91/1000 | Loss: 0.00003039
Iteration 92/1000 | Loss: 0.00003039
Iteration 93/1000 | Loss: 0.00003039
Iteration 94/1000 | Loss: 0.00003039
Iteration 95/1000 | Loss: 0.00003039
Iteration 96/1000 | Loss: 0.00003038
Iteration 97/1000 | Loss: 0.00003038
Iteration 98/1000 | Loss: 0.00003038
Iteration 99/1000 | Loss: 0.00003038
Iteration 100/1000 | Loss: 0.00003038
Iteration 101/1000 | Loss: 0.00003037
Iteration 102/1000 | Loss: 0.00003037
Iteration 103/1000 | Loss: 0.00003037
Iteration 104/1000 | Loss: 0.00003037
Iteration 105/1000 | Loss: 0.00003037
Iteration 106/1000 | Loss: 0.00003036
Iteration 107/1000 | Loss: 0.00003036
Iteration 108/1000 | Loss: 0.00003036
Iteration 109/1000 | Loss: 0.00003036
Iteration 110/1000 | Loss: 0.00003036
Iteration 111/1000 | Loss: 0.00003036
Iteration 112/1000 | Loss: 0.00003036
Iteration 113/1000 | Loss: 0.00003036
Iteration 114/1000 | Loss: 0.00003036
Iteration 115/1000 | Loss: 0.00003036
Iteration 116/1000 | Loss: 0.00003036
Iteration 117/1000 | Loss: 0.00003035
Iteration 118/1000 | Loss: 0.00003035
Iteration 119/1000 | Loss: 0.00003035
Iteration 120/1000 | Loss: 0.00003035
Iteration 121/1000 | Loss: 0.00003035
Iteration 122/1000 | Loss: 0.00003035
Iteration 123/1000 | Loss: 0.00003035
Iteration 124/1000 | Loss: 0.00003034
Iteration 125/1000 | Loss: 0.00003034
Iteration 126/1000 | Loss: 0.00003034
Iteration 127/1000 | Loss: 0.00003034
Iteration 128/1000 | Loss: 0.00003034
Iteration 129/1000 | Loss: 0.00003034
Iteration 130/1000 | Loss: 0.00003034
Iteration 131/1000 | Loss: 0.00003034
Iteration 132/1000 | Loss: 0.00003034
Iteration 133/1000 | Loss: 0.00003034
Iteration 134/1000 | Loss: 0.00003034
Iteration 135/1000 | Loss: 0.00003034
Iteration 136/1000 | Loss: 0.00003034
Iteration 137/1000 | Loss: 0.00003034
Iteration 138/1000 | Loss: 0.00003033
Iteration 139/1000 | Loss: 0.00003033
Iteration 140/1000 | Loss: 0.00003033
Iteration 141/1000 | Loss: 0.00003033
Iteration 142/1000 | Loss: 0.00003033
Iteration 143/1000 | Loss: 0.00003033
Iteration 144/1000 | Loss: 0.00003033
Iteration 145/1000 | Loss: 0.00003033
Iteration 146/1000 | Loss: 0.00003033
Iteration 147/1000 | Loss: 0.00003033
Iteration 148/1000 | Loss: 0.00003033
Iteration 149/1000 | Loss: 0.00003033
Iteration 150/1000 | Loss: 0.00003033
Iteration 151/1000 | Loss: 0.00003033
Iteration 152/1000 | Loss: 0.00003033
Iteration 153/1000 | Loss: 0.00003032
Iteration 154/1000 | Loss: 0.00003032
Iteration 155/1000 | Loss: 0.00003032
Iteration 156/1000 | Loss: 0.00003032
Iteration 157/1000 | Loss: 0.00003032
Iteration 158/1000 | Loss: 0.00003032
Iteration 159/1000 | Loss: 0.00003032
Iteration 160/1000 | Loss: 0.00003032
Iteration 161/1000 | Loss: 0.00003032
Iteration 162/1000 | Loss: 0.00003032
Iteration 163/1000 | Loss: 0.00003032
Iteration 164/1000 | Loss: 0.00003032
Iteration 165/1000 | Loss: 0.00003032
Iteration 166/1000 | Loss: 0.00003032
Iteration 167/1000 | Loss: 0.00003032
Iteration 168/1000 | Loss: 0.00003032
Iteration 169/1000 | Loss: 0.00003032
Iteration 170/1000 | Loss: 0.00003032
Iteration 171/1000 | Loss: 0.00003031
Iteration 172/1000 | Loss: 0.00003031
Iteration 173/1000 | Loss: 0.00003031
Iteration 174/1000 | Loss: 0.00003031
Iteration 175/1000 | Loss: 0.00003031
Iteration 176/1000 | Loss: 0.00003031
Iteration 177/1000 | Loss: 0.00003031
Iteration 178/1000 | Loss: 0.00003031
Iteration 179/1000 | Loss: 0.00003031
Iteration 180/1000 | Loss: 0.00003031
Iteration 181/1000 | Loss: 0.00003031
Iteration 182/1000 | Loss: 0.00003031
Iteration 183/1000 | Loss: 0.00003031
Iteration 184/1000 | Loss: 0.00003031
Iteration 185/1000 | Loss: 0.00003031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [3.0313949537230656e-05, 3.0313949537230656e-05, 3.0313949537230656e-05, 3.0313949537230656e-05, 3.0313949537230656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0313949537230656e-05

Optimization complete. Final v2v error: 4.339842319488525 mm

Highest mean error: 5.462568759918213 mm for frame 78

Lowest mean error: 3.7057907581329346 mm for frame 111

Saving results

Total time: 41.568538188934326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884829
Iteration 2/25 | Loss: 0.00080892
Iteration 3/25 | Loss: 0.00065397
Iteration 4/25 | Loss: 0.00062585
Iteration 5/25 | Loss: 0.00061738
Iteration 6/25 | Loss: 0.00061605
Iteration 7/25 | Loss: 0.00061605
Iteration 8/25 | Loss: 0.00061605
Iteration 9/25 | Loss: 0.00061605
Iteration 10/25 | Loss: 0.00061605
Iteration 11/25 | Loss: 0.00061605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006160516641102731, 0.0006160516641102731, 0.0006160516641102731, 0.0006160516641102731, 0.0006160516641102731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006160516641102731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.98549318
Iteration 2/25 | Loss: 0.00022796
Iteration 3/25 | Loss: 0.00022796
Iteration 4/25 | Loss: 0.00022796
Iteration 5/25 | Loss: 0.00022796
Iteration 6/25 | Loss: 0.00022796
Iteration 7/25 | Loss: 0.00022796
Iteration 8/25 | Loss: 0.00022796
Iteration 9/25 | Loss: 0.00022796
Iteration 10/25 | Loss: 0.00022796
Iteration 11/25 | Loss: 0.00022796
Iteration 12/25 | Loss: 0.00022796
Iteration 13/25 | Loss: 0.00022796
Iteration 14/25 | Loss: 0.00022796
Iteration 15/25 | Loss: 0.00022796
Iteration 16/25 | Loss: 0.00022796
Iteration 17/25 | Loss: 0.00022796
Iteration 18/25 | Loss: 0.00022796
Iteration 19/25 | Loss: 0.00022796
Iteration 20/25 | Loss: 0.00022796
Iteration 21/25 | Loss: 0.00022796
Iteration 22/25 | Loss: 0.00022796
Iteration 23/25 | Loss: 0.00022796
Iteration 24/25 | Loss: 0.00022796
Iteration 25/25 | Loss: 0.00022796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022796
Iteration 2/1000 | Loss: 0.00002168
Iteration 3/1000 | Loss: 0.00001651
Iteration 4/1000 | Loss: 0.00001491
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001380
Iteration 7/1000 | Loss: 0.00001352
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001329
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001327
Iteration 19/1000 | Loss: 0.00001326
Iteration 20/1000 | Loss: 0.00001326
Iteration 21/1000 | Loss: 0.00001326
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001326
Iteration 26/1000 | Loss: 0.00001326
Iteration 27/1000 | Loss: 0.00001326
Iteration 28/1000 | Loss: 0.00001326
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001322
Iteration 31/1000 | Loss: 0.00001322
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001322
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001321
Iteration 37/1000 | Loss: 0.00001321
Iteration 38/1000 | Loss: 0.00001321
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001317
Iteration 41/1000 | Loss: 0.00001317
Iteration 42/1000 | Loss: 0.00001317
Iteration 43/1000 | Loss: 0.00001317
Iteration 44/1000 | Loss: 0.00001317
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001315
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001314
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001313
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001313
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001312
Iteration 74/1000 | Loss: 0.00001312
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001312
Iteration 77/1000 | Loss: 0.00001312
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001311
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001311
Iteration 84/1000 | Loss: 0.00001311
Iteration 85/1000 | Loss: 0.00001311
Iteration 86/1000 | Loss: 0.00001311
Iteration 87/1000 | Loss: 0.00001311
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001310
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001309
Iteration 100/1000 | Loss: 0.00001309
Iteration 101/1000 | Loss: 0.00001309
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.3083928934065625e-05, 1.3083928934065625e-05, 1.3083928934065625e-05, 1.3083928934065625e-05, 1.3083928934065625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3083928934065625e-05

Optimization complete. Final v2v error: 3.07633638381958 mm

Highest mean error: 3.2842860221862793 mm for frame 237

Lowest mean error: 2.965468406677246 mm for frame 36

Saving results

Total time: 31.921429872512817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795316
Iteration 2/25 | Loss: 0.00094150
Iteration 3/25 | Loss: 0.00064967
Iteration 4/25 | Loss: 0.00061354
Iteration 5/25 | Loss: 0.00060398
Iteration 6/25 | Loss: 0.00060161
Iteration 7/25 | Loss: 0.00060095
Iteration 8/25 | Loss: 0.00060095
Iteration 9/25 | Loss: 0.00060095
Iteration 10/25 | Loss: 0.00060095
Iteration 11/25 | Loss: 0.00060095
Iteration 12/25 | Loss: 0.00060095
Iteration 13/25 | Loss: 0.00060095
Iteration 14/25 | Loss: 0.00060095
Iteration 15/25 | Loss: 0.00060095
Iteration 16/25 | Loss: 0.00060095
Iteration 17/25 | Loss: 0.00060095
Iteration 18/25 | Loss: 0.00060095
Iteration 19/25 | Loss: 0.00060095
Iteration 20/25 | Loss: 0.00060095
Iteration 21/25 | Loss: 0.00060095
Iteration 22/25 | Loss: 0.00060095
Iteration 23/25 | Loss: 0.00060095
Iteration 24/25 | Loss: 0.00060095
Iteration 25/25 | Loss: 0.00060095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01276183
Iteration 2/25 | Loss: 0.00028224
Iteration 3/25 | Loss: 0.00028224
Iteration 4/25 | Loss: 0.00028224
Iteration 5/25 | Loss: 0.00028224
Iteration 6/25 | Loss: 0.00028224
Iteration 7/25 | Loss: 0.00028224
Iteration 8/25 | Loss: 0.00028224
Iteration 9/25 | Loss: 0.00028224
Iteration 10/25 | Loss: 0.00028224
Iteration 11/25 | Loss: 0.00028224
Iteration 12/25 | Loss: 0.00028224
Iteration 13/25 | Loss: 0.00028224
Iteration 14/25 | Loss: 0.00028224
Iteration 15/25 | Loss: 0.00028224
Iteration 16/25 | Loss: 0.00028224
Iteration 17/25 | Loss: 0.00028224
Iteration 18/25 | Loss: 0.00028224
Iteration 19/25 | Loss: 0.00028224
Iteration 20/25 | Loss: 0.00028224
Iteration 21/25 | Loss: 0.00028224
Iteration 22/25 | Loss: 0.00028224
Iteration 23/25 | Loss: 0.00028224
Iteration 24/25 | Loss: 0.00028224
Iteration 25/25 | Loss: 0.00028224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00028223785920999944, 0.00028223785920999944, 0.00028223785920999944, 0.00028223785920999944, 0.00028223785920999944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028223785920999944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028224
Iteration 2/1000 | Loss: 0.00002335
Iteration 3/1000 | Loss: 0.00001595
Iteration 4/1000 | Loss: 0.00001502
Iteration 5/1000 | Loss: 0.00001424
Iteration 6/1000 | Loss: 0.00001399
Iteration 7/1000 | Loss: 0.00001362
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001327
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001319
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001319
Iteration 20/1000 | Loss: 0.00001314
Iteration 21/1000 | Loss: 0.00001313
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001308
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001296
Iteration 29/1000 | Loss: 0.00001296
Iteration 30/1000 | Loss: 0.00001296
Iteration 31/1000 | Loss: 0.00001295
Iteration 32/1000 | Loss: 0.00001295
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001288
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001286
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001285
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001284
Iteration 69/1000 | Loss: 0.00001284
Iteration 70/1000 | Loss: 0.00001284
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001283
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001280
Iteration 81/1000 | Loss: 0.00001280
Iteration 82/1000 | Loss: 0.00001280
Iteration 83/1000 | Loss: 0.00001279
Iteration 84/1000 | Loss: 0.00001278
Iteration 85/1000 | Loss: 0.00001278
Iteration 86/1000 | Loss: 0.00001278
Iteration 87/1000 | Loss: 0.00001278
Iteration 88/1000 | Loss: 0.00001278
Iteration 89/1000 | Loss: 0.00001278
Iteration 90/1000 | Loss: 0.00001278
Iteration 91/1000 | Loss: 0.00001278
Iteration 92/1000 | Loss: 0.00001278
Iteration 93/1000 | Loss: 0.00001278
Iteration 94/1000 | Loss: 0.00001277
Iteration 95/1000 | Loss: 0.00001277
Iteration 96/1000 | Loss: 0.00001277
Iteration 97/1000 | Loss: 0.00001277
Iteration 98/1000 | Loss: 0.00001277
Iteration 99/1000 | Loss: 0.00001277
Iteration 100/1000 | Loss: 0.00001277
Iteration 101/1000 | Loss: 0.00001277
Iteration 102/1000 | Loss: 0.00001277
Iteration 103/1000 | Loss: 0.00001276
Iteration 104/1000 | Loss: 0.00001276
Iteration 105/1000 | Loss: 0.00001276
Iteration 106/1000 | Loss: 0.00001276
Iteration 107/1000 | Loss: 0.00001276
Iteration 108/1000 | Loss: 0.00001276
Iteration 109/1000 | Loss: 0.00001275
Iteration 110/1000 | Loss: 0.00001275
Iteration 111/1000 | Loss: 0.00001275
Iteration 112/1000 | Loss: 0.00001275
Iteration 113/1000 | Loss: 0.00001275
Iteration 114/1000 | Loss: 0.00001275
Iteration 115/1000 | Loss: 0.00001275
Iteration 116/1000 | Loss: 0.00001275
Iteration 117/1000 | Loss: 0.00001275
Iteration 118/1000 | Loss: 0.00001275
Iteration 119/1000 | Loss: 0.00001275
Iteration 120/1000 | Loss: 0.00001275
Iteration 121/1000 | Loss: 0.00001275
Iteration 122/1000 | Loss: 0.00001275
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001274
Iteration 125/1000 | Loss: 0.00001274
Iteration 126/1000 | Loss: 0.00001274
Iteration 127/1000 | Loss: 0.00001274
Iteration 128/1000 | Loss: 0.00001274
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001274
Iteration 131/1000 | Loss: 0.00001274
Iteration 132/1000 | Loss: 0.00001274
Iteration 133/1000 | Loss: 0.00001274
Iteration 134/1000 | Loss: 0.00001274
Iteration 135/1000 | Loss: 0.00001274
Iteration 136/1000 | Loss: 0.00001274
Iteration 137/1000 | Loss: 0.00001274
Iteration 138/1000 | Loss: 0.00001273
Iteration 139/1000 | Loss: 0.00001273
Iteration 140/1000 | Loss: 0.00001273
Iteration 141/1000 | Loss: 0.00001273
Iteration 142/1000 | Loss: 0.00001273
Iteration 143/1000 | Loss: 0.00001273
Iteration 144/1000 | Loss: 0.00001273
Iteration 145/1000 | Loss: 0.00001273
Iteration 146/1000 | Loss: 0.00001273
Iteration 147/1000 | Loss: 0.00001273
Iteration 148/1000 | Loss: 0.00001273
Iteration 149/1000 | Loss: 0.00001272
Iteration 150/1000 | Loss: 0.00001272
Iteration 151/1000 | Loss: 0.00001272
Iteration 152/1000 | Loss: 0.00001272
Iteration 153/1000 | Loss: 0.00001272
Iteration 154/1000 | Loss: 0.00001272
Iteration 155/1000 | Loss: 0.00001272
Iteration 156/1000 | Loss: 0.00001272
Iteration 157/1000 | Loss: 0.00001272
Iteration 158/1000 | Loss: 0.00001272
Iteration 159/1000 | Loss: 0.00001272
Iteration 160/1000 | Loss: 0.00001272
Iteration 161/1000 | Loss: 0.00001272
Iteration 162/1000 | Loss: 0.00001272
Iteration 163/1000 | Loss: 0.00001272
Iteration 164/1000 | Loss: 0.00001272
Iteration 165/1000 | Loss: 0.00001272
Iteration 166/1000 | Loss: 0.00001272
Iteration 167/1000 | Loss: 0.00001272
Iteration 168/1000 | Loss: 0.00001272
Iteration 169/1000 | Loss: 0.00001272
Iteration 170/1000 | Loss: 0.00001272
Iteration 171/1000 | Loss: 0.00001272
Iteration 172/1000 | Loss: 0.00001272
Iteration 173/1000 | Loss: 0.00001272
Iteration 174/1000 | Loss: 0.00001272
Iteration 175/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.2724490261462051e-05, 1.2724490261462051e-05, 1.2724490261462051e-05, 1.2724490261462051e-05, 1.2724490261462051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2724490261462051e-05

Optimization complete. Final v2v error: 3.0576040744781494 mm

Highest mean error: 3.2552030086517334 mm for frame 63

Lowest mean error: 2.924506187438965 mm for frame 44

Saving results

Total time: 36.83131647109985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945067
Iteration 2/25 | Loss: 0.00096090
Iteration 3/25 | Loss: 0.00076435
Iteration 4/25 | Loss: 0.00072330
Iteration 5/25 | Loss: 0.00071282
Iteration 6/25 | Loss: 0.00071105
Iteration 7/25 | Loss: 0.00071074
Iteration 8/25 | Loss: 0.00071074
Iteration 9/25 | Loss: 0.00071074
Iteration 10/25 | Loss: 0.00071074
Iteration 11/25 | Loss: 0.00071074
Iteration 12/25 | Loss: 0.00071074
Iteration 13/25 | Loss: 0.00071074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007107364945113659, 0.0007107364945113659, 0.0007107364945113659, 0.0007107364945113659, 0.0007107364945113659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007107364945113659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.47592688
Iteration 2/25 | Loss: 0.00044359
Iteration 3/25 | Loss: 0.00044355
Iteration 4/25 | Loss: 0.00044355
Iteration 5/25 | Loss: 0.00044355
Iteration 6/25 | Loss: 0.00044355
Iteration 7/25 | Loss: 0.00044355
Iteration 8/25 | Loss: 0.00044355
Iteration 9/25 | Loss: 0.00044355
Iteration 10/25 | Loss: 0.00044355
Iteration 11/25 | Loss: 0.00044355
Iteration 12/25 | Loss: 0.00044355
Iteration 13/25 | Loss: 0.00044355
Iteration 14/25 | Loss: 0.00044355
Iteration 15/25 | Loss: 0.00044355
Iteration 16/25 | Loss: 0.00044355
Iteration 17/25 | Loss: 0.00044355
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00044354997226037085, 0.00044354997226037085, 0.00044354997226037085, 0.00044354997226037085, 0.00044354997226037085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044354997226037085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044355
Iteration 2/1000 | Loss: 0.00003419
Iteration 3/1000 | Loss: 0.00002620
Iteration 4/1000 | Loss: 0.00002454
Iteration 5/1000 | Loss: 0.00002375
Iteration 6/1000 | Loss: 0.00002319
Iteration 7/1000 | Loss: 0.00002278
Iteration 8/1000 | Loss: 0.00002244
Iteration 9/1000 | Loss: 0.00002213
Iteration 10/1000 | Loss: 0.00002182
Iteration 11/1000 | Loss: 0.00002162
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002159
Iteration 14/1000 | Loss: 0.00002158
Iteration 15/1000 | Loss: 0.00002155
Iteration 16/1000 | Loss: 0.00002149
Iteration 17/1000 | Loss: 0.00002148
Iteration 18/1000 | Loss: 0.00002144
Iteration 19/1000 | Loss: 0.00002144
Iteration 20/1000 | Loss: 0.00002143
Iteration 21/1000 | Loss: 0.00002142
Iteration 22/1000 | Loss: 0.00002142
Iteration 23/1000 | Loss: 0.00002141
Iteration 24/1000 | Loss: 0.00002140
Iteration 25/1000 | Loss: 0.00002140
Iteration 26/1000 | Loss: 0.00002140
Iteration 27/1000 | Loss: 0.00002140
Iteration 28/1000 | Loss: 0.00002140
Iteration 29/1000 | Loss: 0.00002140
Iteration 30/1000 | Loss: 0.00002140
Iteration 31/1000 | Loss: 0.00002140
Iteration 32/1000 | Loss: 0.00002140
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002139
Iteration 35/1000 | Loss: 0.00002139
Iteration 36/1000 | Loss: 0.00002139
Iteration 37/1000 | Loss: 0.00002138
Iteration 38/1000 | Loss: 0.00002138
Iteration 39/1000 | Loss: 0.00002137
Iteration 40/1000 | Loss: 0.00002137
Iteration 41/1000 | Loss: 0.00002136
Iteration 42/1000 | Loss: 0.00002136
Iteration 43/1000 | Loss: 0.00002136
Iteration 44/1000 | Loss: 0.00002135
Iteration 45/1000 | Loss: 0.00002135
Iteration 46/1000 | Loss: 0.00002135
Iteration 47/1000 | Loss: 0.00002134
Iteration 48/1000 | Loss: 0.00002134
Iteration 49/1000 | Loss: 0.00002134
Iteration 50/1000 | Loss: 0.00002134
Iteration 51/1000 | Loss: 0.00002134
Iteration 52/1000 | Loss: 0.00002134
Iteration 53/1000 | Loss: 0.00002134
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002133
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002133
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002133
Iteration 63/1000 | Loss: 0.00002133
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002132
Iteration 66/1000 | Loss: 0.00002132
Iteration 67/1000 | Loss: 0.00002132
Iteration 68/1000 | Loss: 0.00002132
Iteration 69/1000 | Loss: 0.00002132
Iteration 70/1000 | Loss: 0.00002132
Iteration 71/1000 | Loss: 0.00002132
Iteration 72/1000 | Loss: 0.00002132
Iteration 73/1000 | Loss: 0.00002132
Iteration 74/1000 | Loss: 0.00002132
Iteration 75/1000 | Loss: 0.00002132
Iteration 76/1000 | Loss: 0.00002132
Iteration 77/1000 | Loss: 0.00002132
Iteration 78/1000 | Loss: 0.00002132
Iteration 79/1000 | Loss: 0.00002131
Iteration 80/1000 | Loss: 0.00002131
Iteration 81/1000 | Loss: 0.00002131
Iteration 82/1000 | Loss: 0.00002131
Iteration 83/1000 | Loss: 0.00002131
Iteration 84/1000 | Loss: 0.00002131
Iteration 85/1000 | Loss: 0.00002131
Iteration 86/1000 | Loss: 0.00002131
Iteration 87/1000 | Loss: 0.00002131
Iteration 88/1000 | Loss: 0.00002131
Iteration 89/1000 | Loss: 0.00002130
Iteration 90/1000 | Loss: 0.00002130
Iteration 91/1000 | Loss: 0.00002130
Iteration 92/1000 | Loss: 0.00002130
Iteration 93/1000 | Loss: 0.00002130
Iteration 94/1000 | Loss: 0.00002130
Iteration 95/1000 | Loss: 0.00002129
Iteration 96/1000 | Loss: 0.00002129
Iteration 97/1000 | Loss: 0.00002129
Iteration 98/1000 | Loss: 0.00002129
Iteration 99/1000 | Loss: 0.00002129
Iteration 100/1000 | Loss: 0.00002129
Iteration 101/1000 | Loss: 0.00002129
Iteration 102/1000 | Loss: 0.00002129
Iteration 103/1000 | Loss: 0.00002129
Iteration 104/1000 | Loss: 0.00002129
Iteration 105/1000 | Loss: 0.00002129
Iteration 106/1000 | Loss: 0.00002129
Iteration 107/1000 | Loss: 0.00002129
Iteration 108/1000 | Loss: 0.00002129
Iteration 109/1000 | Loss: 0.00002129
Iteration 110/1000 | Loss: 0.00002128
Iteration 111/1000 | Loss: 0.00002128
Iteration 112/1000 | Loss: 0.00002128
Iteration 113/1000 | Loss: 0.00002128
Iteration 114/1000 | Loss: 0.00002128
Iteration 115/1000 | Loss: 0.00002128
Iteration 116/1000 | Loss: 0.00002128
Iteration 117/1000 | Loss: 0.00002128
Iteration 118/1000 | Loss: 0.00002128
Iteration 119/1000 | Loss: 0.00002128
Iteration 120/1000 | Loss: 0.00002128
Iteration 121/1000 | Loss: 0.00002128
Iteration 122/1000 | Loss: 0.00002128
Iteration 123/1000 | Loss: 0.00002128
Iteration 124/1000 | Loss: 0.00002128
Iteration 125/1000 | Loss: 0.00002128
Iteration 126/1000 | Loss: 0.00002128
Iteration 127/1000 | Loss: 0.00002128
Iteration 128/1000 | Loss: 0.00002128
Iteration 129/1000 | Loss: 0.00002128
Iteration 130/1000 | Loss: 0.00002127
Iteration 131/1000 | Loss: 0.00002127
Iteration 132/1000 | Loss: 0.00002127
Iteration 133/1000 | Loss: 0.00002127
Iteration 134/1000 | Loss: 0.00002127
Iteration 135/1000 | Loss: 0.00002127
Iteration 136/1000 | Loss: 0.00002127
Iteration 137/1000 | Loss: 0.00002127
Iteration 138/1000 | Loss: 0.00002127
Iteration 139/1000 | Loss: 0.00002127
Iteration 140/1000 | Loss: 0.00002127
Iteration 141/1000 | Loss: 0.00002127
Iteration 142/1000 | Loss: 0.00002127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.1274052414810285e-05, 2.1274052414810285e-05, 2.1274052414810285e-05, 2.1274052414810285e-05, 2.1274052414810285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1274052414810285e-05

Optimization complete. Final v2v error: 3.852346181869507 mm

Highest mean error: 4.219493389129639 mm for frame 164

Lowest mean error: 3.4944944381713867 mm for frame 47

Saving results

Total time: 35.10829448699951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618539
Iteration 2/25 | Loss: 0.00132193
Iteration 3/25 | Loss: 0.00103032
Iteration 4/25 | Loss: 0.00093613
Iteration 5/25 | Loss: 0.00093776
Iteration 6/25 | Loss: 0.00093947
Iteration 7/25 | Loss: 0.00089989
Iteration 8/25 | Loss: 0.00085371
Iteration 9/25 | Loss: 0.00082096
Iteration 10/25 | Loss: 0.00081128
Iteration 11/25 | Loss: 0.00080276
Iteration 12/25 | Loss: 0.00079900
Iteration 13/25 | Loss: 0.00079704
Iteration 14/25 | Loss: 0.00079618
Iteration 15/25 | Loss: 0.00079595
Iteration 16/25 | Loss: 0.00079589
Iteration 17/25 | Loss: 0.00079589
Iteration 18/25 | Loss: 0.00079589
Iteration 19/25 | Loss: 0.00079589
Iteration 20/25 | Loss: 0.00079589
Iteration 21/25 | Loss: 0.00079589
Iteration 22/25 | Loss: 0.00079589
Iteration 23/25 | Loss: 0.00079588
Iteration 24/25 | Loss: 0.00079588
Iteration 25/25 | Loss: 0.00079588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33211267
Iteration 2/25 | Loss: 0.00096296
Iteration 3/25 | Loss: 0.00096288
Iteration 4/25 | Loss: 0.00096288
Iteration 5/25 | Loss: 0.00096288
Iteration 6/25 | Loss: 0.00096288
Iteration 7/25 | Loss: 0.00096288
Iteration 8/25 | Loss: 0.00096288
Iteration 9/25 | Loss: 0.00096288
Iteration 10/25 | Loss: 0.00096288
Iteration 11/25 | Loss: 0.00096288
Iteration 12/25 | Loss: 0.00096288
Iteration 13/25 | Loss: 0.00096288
Iteration 14/25 | Loss: 0.00096288
Iteration 15/25 | Loss: 0.00096288
Iteration 16/25 | Loss: 0.00096288
Iteration 17/25 | Loss: 0.00096288
Iteration 18/25 | Loss: 0.00096288
Iteration 19/25 | Loss: 0.00096288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009628770058043301, 0.0009628770058043301, 0.0009628770058043301, 0.0009628770058043301, 0.0009628770058043301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009628770058043301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096288
Iteration 2/1000 | Loss: 0.00019318
Iteration 3/1000 | Loss: 0.00007261
Iteration 4/1000 | Loss: 0.00004424
Iteration 5/1000 | Loss: 0.00003485
Iteration 6/1000 | Loss: 0.00003104
Iteration 7/1000 | Loss: 0.00002966
Iteration 8/1000 | Loss: 0.00002854
Iteration 9/1000 | Loss: 0.00002738
Iteration 10/1000 | Loss: 0.00002682
Iteration 11/1000 | Loss: 0.00002637
Iteration 12/1000 | Loss: 0.00002606
Iteration 13/1000 | Loss: 0.00002577
Iteration 14/1000 | Loss: 0.00002555
Iteration 15/1000 | Loss: 0.00002534
Iteration 16/1000 | Loss: 0.00002519
Iteration 17/1000 | Loss: 0.00002518
Iteration 18/1000 | Loss: 0.00002515
Iteration 19/1000 | Loss: 0.00002511
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00002508
Iteration 22/1000 | Loss: 0.00002504
Iteration 23/1000 | Loss: 0.00002503
Iteration 24/1000 | Loss: 0.00002502
Iteration 25/1000 | Loss: 0.00002501
Iteration 26/1000 | Loss: 0.00002497
Iteration 27/1000 | Loss: 0.00002492
Iteration 28/1000 | Loss: 0.00002490
Iteration 29/1000 | Loss: 0.00002490
Iteration 30/1000 | Loss: 0.00002487
Iteration 31/1000 | Loss: 0.00002486
Iteration 32/1000 | Loss: 0.00002485
Iteration 33/1000 | Loss: 0.00002485
Iteration 34/1000 | Loss: 0.00002484
Iteration 35/1000 | Loss: 0.00002484
Iteration 36/1000 | Loss: 0.00002483
Iteration 37/1000 | Loss: 0.00002483
Iteration 38/1000 | Loss: 0.00002483
Iteration 39/1000 | Loss: 0.00002483
Iteration 40/1000 | Loss: 0.00002482
Iteration 41/1000 | Loss: 0.00002482
Iteration 42/1000 | Loss: 0.00002481
Iteration 43/1000 | Loss: 0.00002481
Iteration 44/1000 | Loss: 0.00002480
Iteration 45/1000 | Loss: 0.00002480
Iteration 46/1000 | Loss: 0.00002477
Iteration 47/1000 | Loss: 0.00002476
Iteration 48/1000 | Loss: 0.00002475
Iteration 49/1000 | Loss: 0.00002475
Iteration 50/1000 | Loss: 0.00002475
Iteration 51/1000 | Loss: 0.00002474
Iteration 52/1000 | Loss: 0.00002474
Iteration 53/1000 | Loss: 0.00002474
Iteration 54/1000 | Loss: 0.00002473
Iteration 55/1000 | Loss: 0.00002472
Iteration 56/1000 | Loss: 0.00002472
Iteration 57/1000 | Loss: 0.00002470
Iteration 58/1000 | Loss: 0.00002469
Iteration 59/1000 | Loss: 0.00002469
Iteration 60/1000 | Loss: 0.00002468
Iteration 61/1000 | Loss: 0.00002468
Iteration 62/1000 | Loss: 0.00002467
Iteration 63/1000 | Loss: 0.00002467
Iteration 64/1000 | Loss: 0.00002467
Iteration 65/1000 | Loss: 0.00002467
Iteration 66/1000 | Loss: 0.00002467
Iteration 67/1000 | Loss: 0.00002466
Iteration 68/1000 | Loss: 0.00002466
Iteration 69/1000 | Loss: 0.00002466
Iteration 70/1000 | Loss: 0.00002466
Iteration 71/1000 | Loss: 0.00002466
Iteration 72/1000 | Loss: 0.00002465
Iteration 73/1000 | Loss: 0.00002465
Iteration 74/1000 | Loss: 0.00002464
Iteration 75/1000 | Loss: 0.00002464
Iteration 76/1000 | Loss: 0.00002464
Iteration 77/1000 | Loss: 0.00002464
Iteration 78/1000 | Loss: 0.00002464
Iteration 79/1000 | Loss: 0.00002464
Iteration 80/1000 | Loss: 0.00002464
Iteration 81/1000 | Loss: 0.00002463
Iteration 82/1000 | Loss: 0.00002463
Iteration 83/1000 | Loss: 0.00002463
Iteration 84/1000 | Loss: 0.00002463
Iteration 85/1000 | Loss: 0.00002463
Iteration 86/1000 | Loss: 0.00002463
Iteration 87/1000 | Loss: 0.00002463
Iteration 88/1000 | Loss: 0.00002462
Iteration 89/1000 | Loss: 0.00002462
Iteration 90/1000 | Loss: 0.00002462
Iteration 91/1000 | Loss: 0.00002462
Iteration 92/1000 | Loss: 0.00002462
Iteration 93/1000 | Loss: 0.00002462
Iteration 94/1000 | Loss: 0.00002461
Iteration 95/1000 | Loss: 0.00002461
Iteration 96/1000 | Loss: 0.00002461
Iteration 97/1000 | Loss: 0.00002461
Iteration 98/1000 | Loss: 0.00002461
Iteration 99/1000 | Loss: 0.00002461
Iteration 100/1000 | Loss: 0.00002461
Iteration 101/1000 | Loss: 0.00002461
Iteration 102/1000 | Loss: 0.00002461
Iteration 103/1000 | Loss: 0.00002461
Iteration 104/1000 | Loss: 0.00002461
Iteration 105/1000 | Loss: 0.00002461
Iteration 106/1000 | Loss: 0.00002461
Iteration 107/1000 | Loss: 0.00002461
Iteration 108/1000 | Loss: 0.00002461
Iteration 109/1000 | Loss: 0.00002460
Iteration 110/1000 | Loss: 0.00002460
Iteration 111/1000 | Loss: 0.00002460
Iteration 112/1000 | Loss: 0.00002460
Iteration 113/1000 | Loss: 0.00002460
Iteration 114/1000 | Loss: 0.00002460
Iteration 115/1000 | Loss: 0.00002460
Iteration 116/1000 | Loss: 0.00002460
Iteration 117/1000 | Loss: 0.00002460
Iteration 118/1000 | Loss: 0.00002460
Iteration 119/1000 | Loss: 0.00002460
Iteration 120/1000 | Loss: 0.00002460
Iteration 121/1000 | Loss: 0.00002460
Iteration 122/1000 | Loss: 0.00002460
Iteration 123/1000 | Loss: 0.00002460
Iteration 124/1000 | Loss: 0.00002460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.4602028133813292e-05, 2.4602028133813292e-05, 2.4602028133813292e-05, 2.4602028133813292e-05, 2.4602028133813292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4602028133813292e-05

Optimization complete. Final v2v error: 3.873887538909912 mm

Highest mean error: 5.785597324371338 mm for frame 138

Lowest mean error: 3.0325634479522705 mm for frame 76

Saving results

Total time: 60.94359755516052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096073
Iteration 2/25 | Loss: 0.01096073
Iteration 3/25 | Loss: 0.01096073
Iteration 4/25 | Loss: 0.01096073
Iteration 5/25 | Loss: 0.01096073
Iteration 6/25 | Loss: 0.01096073
Iteration 7/25 | Loss: 0.01096073
Iteration 8/25 | Loss: 0.01096073
Iteration 9/25 | Loss: 0.01096073
Iteration 10/25 | Loss: 0.01096073
Iteration 11/25 | Loss: 0.01096073
Iteration 12/25 | Loss: 0.01096073
Iteration 13/25 | Loss: 0.01096072
Iteration 14/25 | Loss: 0.01096072
Iteration 15/25 | Loss: 0.01096072
Iteration 16/25 | Loss: 0.01096072
Iteration 17/25 | Loss: 0.01096072
Iteration 18/25 | Loss: 0.01096072
Iteration 19/25 | Loss: 0.01096072
Iteration 20/25 | Loss: 0.01096072
Iteration 21/25 | Loss: 0.01096072
Iteration 22/25 | Loss: 0.01096072
Iteration 23/25 | Loss: 0.01096072
Iteration 24/25 | Loss: 0.01096072
Iteration 25/25 | Loss: 0.01096071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90363657
Iteration 2/25 | Loss: 0.06305401
Iteration 3/25 | Loss: 0.06304151
Iteration 4/25 | Loss: 0.06304150
Iteration 5/25 | Loss: 0.06304150
Iteration 6/25 | Loss: 0.06304150
Iteration 7/25 | Loss: 0.06304150
Iteration 8/25 | Loss: 0.06304150
Iteration 9/25 | Loss: 0.06304150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.06304150074720383, 0.06304150074720383, 0.06304150074720383, 0.06304150074720383, 0.06304150074720383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06304150074720383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06304149
Iteration 2/1000 | Loss: 0.00594530
Iteration 3/1000 | Loss: 0.00159452
Iteration 4/1000 | Loss: 0.00062944
Iteration 5/1000 | Loss: 0.00110503
Iteration 6/1000 | Loss: 0.01114432
Iteration 7/1000 | Loss: 0.00543282
Iteration 8/1000 | Loss: 0.00825870
Iteration 9/1000 | Loss: 0.00142528
Iteration 10/1000 | Loss: 0.00102327
Iteration 11/1000 | Loss: 0.00011506
Iteration 12/1000 | Loss: 0.00235733
Iteration 13/1000 | Loss: 0.00015470
Iteration 14/1000 | Loss: 0.00073189
Iteration 15/1000 | Loss: 0.00168588
Iteration 16/1000 | Loss: 0.00008362
Iteration 17/1000 | Loss: 0.00019776
Iteration 18/1000 | Loss: 0.00116997
Iteration 19/1000 | Loss: 0.00034978
Iteration 20/1000 | Loss: 0.00049167
Iteration 21/1000 | Loss: 0.00154188
Iteration 22/1000 | Loss: 0.00393414
Iteration 23/1000 | Loss: 0.00021065
Iteration 24/1000 | Loss: 0.00176543
Iteration 25/1000 | Loss: 0.00012849
Iteration 26/1000 | Loss: 0.00029356
Iteration 27/1000 | Loss: 0.00010467
Iteration 28/1000 | Loss: 0.00004176
Iteration 29/1000 | Loss: 0.00041141
Iteration 30/1000 | Loss: 0.00040302
Iteration 31/1000 | Loss: 0.00005914
Iteration 32/1000 | Loss: 0.00004912
Iteration 33/1000 | Loss: 0.00005927
Iteration 34/1000 | Loss: 0.00011940
Iteration 35/1000 | Loss: 0.00003014
Iteration 36/1000 | Loss: 0.00029969
Iteration 37/1000 | Loss: 0.00002851
Iteration 38/1000 | Loss: 0.00038697
Iteration 39/1000 | Loss: 0.00002730
Iteration 40/1000 | Loss: 0.00002616
Iteration 41/1000 | Loss: 0.00008073
Iteration 42/1000 | Loss: 0.00002457
Iteration 43/1000 | Loss: 0.00002392
Iteration 44/1000 | Loss: 0.00002562
Iteration 45/1000 | Loss: 0.00005754
Iteration 46/1000 | Loss: 0.00003083
Iteration 47/1000 | Loss: 0.00002235
Iteration 48/1000 | Loss: 0.00002179
Iteration 49/1000 | Loss: 0.00002632
Iteration 50/1000 | Loss: 0.00015895
Iteration 51/1000 | Loss: 0.00006242
Iteration 52/1000 | Loss: 0.00019632
Iteration 53/1000 | Loss: 0.00002684
Iteration 54/1000 | Loss: 0.00002406
Iteration 55/1000 | Loss: 0.00002084
Iteration 56/1000 | Loss: 0.00002125
Iteration 57/1000 | Loss: 0.00002064
Iteration 58/1000 | Loss: 0.00002046
Iteration 59/1000 | Loss: 0.00002045
Iteration 60/1000 | Loss: 0.00002039
Iteration 61/1000 | Loss: 0.00002039
Iteration 62/1000 | Loss: 0.00002039
Iteration 63/1000 | Loss: 0.00002039
Iteration 64/1000 | Loss: 0.00002039
Iteration 65/1000 | Loss: 0.00002039
Iteration 66/1000 | Loss: 0.00002039
Iteration 67/1000 | Loss: 0.00002039
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002038
Iteration 70/1000 | Loss: 0.00002038
Iteration 71/1000 | Loss: 0.00002035
Iteration 72/1000 | Loss: 0.00002035
Iteration 73/1000 | Loss: 0.00002035
Iteration 74/1000 | Loss: 0.00002035
Iteration 75/1000 | Loss: 0.00002035
Iteration 76/1000 | Loss: 0.00002035
Iteration 77/1000 | Loss: 0.00002035
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002034
Iteration 80/1000 | Loss: 0.00002034
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002034
Iteration 84/1000 | Loss: 0.00002034
Iteration 85/1000 | Loss: 0.00002034
Iteration 86/1000 | Loss: 0.00002034
Iteration 87/1000 | Loss: 0.00002034
Iteration 88/1000 | Loss: 0.00002034
Iteration 89/1000 | Loss: 0.00002661
Iteration 90/1000 | Loss: 0.00013658
Iteration 91/1000 | Loss: 0.00005560
Iteration 92/1000 | Loss: 0.00002390
Iteration 93/1000 | Loss: 0.00003306
Iteration 94/1000 | Loss: 0.00008293
Iteration 95/1000 | Loss: 0.00003410
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002083
Iteration 98/1000 | Loss: 0.00002019
Iteration 99/1000 | Loss: 0.00002019
Iteration 100/1000 | Loss: 0.00002019
Iteration 101/1000 | Loss: 0.00002019
Iteration 102/1000 | Loss: 0.00002019
Iteration 103/1000 | Loss: 0.00002019
Iteration 104/1000 | Loss: 0.00002019
Iteration 105/1000 | Loss: 0.00002019
Iteration 106/1000 | Loss: 0.00002019
Iteration 107/1000 | Loss: 0.00002019
Iteration 108/1000 | Loss: 0.00002019
Iteration 109/1000 | Loss: 0.00002019
Iteration 110/1000 | Loss: 0.00002019
Iteration 111/1000 | Loss: 0.00002019
Iteration 112/1000 | Loss: 0.00002019
Iteration 113/1000 | Loss: 0.00002018
Iteration 114/1000 | Loss: 0.00002018
Iteration 115/1000 | Loss: 0.00002018
Iteration 116/1000 | Loss: 0.00002017
Iteration 117/1000 | Loss: 0.00002017
Iteration 118/1000 | Loss: 0.00002017
Iteration 119/1000 | Loss: 0.00002017
Iteration 120/1000 | Loss: 0.00002017
Iteration 121/1000 | Loss: 0.00002017
Iteration 122/1000 | Loss: 0.00002017
Iteration 123/1000 | Loss: 0.00002017
Iteration 124/1000 | Loss: 0.00002017
Iteration 125/1000 | Loss: 0.00002017
Iteration 126/1000 | Loss: 0.00002017
Iteration 127/1000 | Loss: 0.00002017
Iteration 128/1000 | Loss: 0.00002017
Iteration 129/1000 | Loss: 0.00002017
Iteration 130/1000 | Loss: 0.00002017
Iteration 131/1000 | Loss: 0.00002017
Iteration 132/1000 | Loss: 0.00002017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.017258339037653e-05, 2.017258339037653e-05, 2.017258339037653e-05, 2.017258339037653e-05, 2.017258339037653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.017258339037653e-05

Optimization complete. Final v2v error: 3.5717556476593018 mm

Highest mean error: 4.795055389404297 mm for frame 226

Lowest mean error: 2.8159525394439697 mm for frame 90

Saving results

Total time: 111.83431148529053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786137
Iteration 2/25 | Loss: 0.00077544
Iteration 3/25 | Loss: 0.00065981
Iteration 4/25 | Loss: 0.00063848
Iteration 5/25 | Loss: 0.00063481
Iteration 6/25 | Loss: 0.00063406
Iteration 7/25 | Loss: 0.00063372
Iteration 8/25 | Loss: 0.00063371
Iteration 9/25 | Loss: 0.00063371
Iteration 10/25 | Loss: 0.00063371
Iteration 11/25 | Loss: 0.00063371
Iteration 12/25 | Loss: 0.00063371
Iteration 13/25 | Loss: 0.00063371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006337061058729887, 0.0006337061058729887, 0.0006337061058729887, 0.0006337061058729887, 0.0006337061058729887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006337061058729887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42161131
Iteration 2/25 | Loss: 0.00028496
Iteration 3/25 | Loss: 0.00028496
Iteration 4/25 | Loss: 0.00028495
Iteration 5/25 | Loss: 0.00028495
Iteration 6/25 | Loss: 0.00028495
Iteration 7/25 | Loss: 0.00028495
Iteration 8/25 | Loss: 0.00028495
Iteration 9/25 | Loss: 0.00028495
Iteration 10/25 | Loss: 0.00028495
Iteration 11/25 | Loss: 0.00028495
Iteration 12/25 | Loss: 0.00028495
Iteration 13/25 | Loss: 0.00028495
Iteration 14/25 | Loss: 0.00028495
Iteration 15/25 | Loss: 0.00028495
Iteration 16/25 | Loss: 0.00028495
Iteration 17/25 | Loss: 0.00028495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002849525189958513, 0.0002849525189958513, 0.0002849525189958513, 0.0002849525189958513, 0.0002849525189958513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002849525189958513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028495
Iteration 2/1000 | Loss: 0.00004093
Iteration 3/1000 | Loss: 0.00002451
Iteration 4/1000 | Loss: 0.00002056
Iteration 5/1000 | Loss: 0.00001942
Iteration 6/1000 | Loss: 0.00001872
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001778
Iteration 9/1000 | Loss: 0.00001750
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001706
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001691
Iteration 14/1000 | Loss: 0.00001690
Iteration 15/1000 | Loss: 0.00001683
Iteration 16/1000 | Loss: 0.00001677
Iteration 17/1000 | Loss: 0.00001676
Iteration 18/1000 | Loss: 0.00001676
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001674
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001673
Iteration 25/1000 | Loss: 0.00001673
Iteration 26/1000 | Loss: 0.00001673
Iteration 27/1000 | Loss: 0.00001673
Iteration 28/1000 | Loss: 0.00001672
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001670
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001669
Iteration 35/1000 | Loss: 0.00001669
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001669
Iteration 39/1000 | Loss: 0.00001669
Iteration 40/1000 | Loss: 0.00001669
Iteration 41/1000 | Loss: 0.00001669
Iteration 42/1000 | Loss: 0.00001669
Iteration 43/1000 | Loss: 0.00001668
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001667
Iteration 46/1000 | Loss: 0.00001667
Iteration 47/1000 | Loss: 0.00001667
Iteration 48/1000 | Loss: 0.00001667
Iteration 49/1000 | Loss: 0.00001666
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001665
Iteration 54/1000 | Loss: 0.00001665
Iteration 55/1000 | Loss: 0.00001664
Iteration 56/1000 | Loss: 0.00001664
Iteration 57/1000 | Loss: 0.00001664
Iteration 58/1000 | Loss: 0.00001664
Iteration 59/1000 | Loss: 0.00001664
Iteration 60/1000 | Loss: 0.00001664
Iteration 61/1000 | Loss: 0.00001664
Iteration 62/1000 | Loss: 0.00001664
Iteration 63/1000 | Loss: 0.00001663
Iteration 64/1000 | Loss: 0.00001663
Iteration 65/1000 | Loss: 0.00001663
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001663
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00001663
Iteration 70/1000 | Loss: 0.00001663
Iteration 71/1000 | Loss: 0.00001663
Iteration 72/1000 | Loss: 0.00001663
Iteration 73/1000 | Loss: 0.00001663
Iteration 74/1000 | Loss: 0.00001663
Iteration 75/1000 | Loss: 0.00001663
Iteration 76/1000 | Loss: 0.00001663
Iteration 77/1000 | Loss: 0.00001663
Iteration 78/1000 | Loss: 0.00001663
Iteration 79/1000 | Loss: 0.00001663
Iteration 80/1000 | Loss: 0.00001663
Iteration 81/1000 | Loss: 0.00001663
Iteration 82/1000 | Loss: 0.00001663
Iteration 83/1000 | Loss: 0.00001663
Iteration 84/1000 | Loss: 0.00001663
Iteration 85/1000 | Loss: 0.00001663
Iteration 86/1000 | Loss: 0.00001663
Iteration 87/1000 | Loss: 0.00001663
Iteration 88/1000 | Loss: 0.00001663
Iteration 89/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.6629721358185634e-05, 1.6629721358185634e-05, 1.6629721358185634e-05, 1.6629721358185634e-05, 1.6629721358185634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6629721358185634e-05

Optimization complete. Final v2v error: 3.465808391571045 mm

Highest mean error: 3.7724297046661377 mm for frame 98

Lowest mean error: 3.3091812133789062 mm for frame 48

Saving results

Total time: 32.362831592559814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899078
Iteration 2/25 | Loss: 0.00202811
Iteration 3/25 | Loss: 0.00146324
Iteration 4/25 | Loss: 0.00111728
Iteration 5/25 | Loss: 0.00097185
Iteration 6/25 | Loss: 0.00093369
Iteration 7/25 | Loss: 0.00091222
Iteration 8/25 | Loss: 0.00085975
Iteration 9/25 | Loss: 0.00083210
Iteration 10/25 | Loss: 0.00081443
Iteration 11/25 | Loss: 0.00079896
Iteration 12/25 | Loss: 0.00078608
Iteration 13/25 | Loss: 0.00078185
Iteration 14/25 | Loss: 0.00076142
Iteration 15/25 | Loss: 0.00074885
Iteration 16/25 | Loss: 0.00073832
Iteration 17/25 | Loss: 0.00073351
Iteration 18/25 | Loss: 0.00073257
Iteration 19/25 | Loss: 0.00073520
Iteration 20/25 | Loss: 0.00073428
Iteration 21/25 | Loss: 0.00073281
Iteration 22/25 | Loss: 0.00073442
Iteration 23/25 | Loss: 0.00073266
Iteration 24/25 | Loss: 0.00073091
Iteration 25/25 | Loss: 0.00072884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55808473
Iteration 2/25 | Loss: 0.00105792
Iteration 3/25 | Loss: 0.00105792
Iteration 4/25 | Loss: 0.00105792
Iteration 5/25 | Loss: 0.00105792
Iteration 6/25 | Loss: 0.00105792
Iteration 7/25 | Loss: 0.00105792
Iteration 8/25 | Loss: 0.00105792
Iteration 9/25 | Loss: 0.00105792
Iteration 10/25 | Loss: 0.00105792
Iteration 11/25 | Loss: 0.00105792
Iteration 12/25 | Loss: 0.00105792
Iteration 13/25 | Loss: 0.00105792
Iteration 14/25 | Loss: 0.00105792
Iteration 15/25 | Loss: 0.00105792
Iteration 16/25 | Loss: 0.00105792
Iteration 17/25 | Loss: 0.00105792
Iteration 18/25 | Loss: 0.00105792
Iteration 19/25 | Loss: 0.00105792
Iteration 20/25 | Loss: 0.00105792
Iteration 21/25 | Loss: 0.00105792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010579208610579371, 0.0010579208610579371, 0.0010579208610579371, 0.0010579208610579371, 0.0010579208610579371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010579208610579371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105792
Iteration 2/1000 | Loss: 0.00012971
Iteration 3/1000 | Loss: 0.00009510
Iteration 4/1000 | Loss: 0.00009753
Iteration 5/1000 | Loss: 0.00008003
Iteration 6/1000 | Loss: 0.00009547
Iteration 7/1000 | Loss: 0.00007885
Iteration 8/1000 | Loss: 0.00008056
Iteration 9/1000 | Loss: 0.00006901
Iteration 10/1000 | Loss: 0.00024494
Iteration 11/1000 | Loss: 0.00007207
Iteration 12/1000 | Loss: 0.00006887
Iteration 13/1000 | Loss: 0.00006442
Iteration 14/1000 | Loss: 0.00031440
Iteration 15/1000 | Loss: 0.00028192
Iteration 16/1000 | Loss: 0.00027155
Iteration 17/1000 | Loss: 0.00007185
Iteration 18/1000 | Loss: 0.00005850
Iteration 19/1000 | Loss: 0.00005321
Iteration 20/1000 | Loss: 0.00004901
Iteration 21/1000 | Loss: 0.00004685
Iteration 22/1000 | Loss: 0.00004561
Iteration 23/1000 | Loss: 0.00004447
Iteration 24/1000 | Loss: 0.00004346
Iteration 25/1000 | Loss: 0.00004276
Iteration 26/1000 | Loss: 0.00004228
Iteration 27/1000 | Loss: 0.00004193
Iteration 28/1000 | Loss: 0.00004156
Iteration 29/1000 | Loss: 0.00004115
Iteration 30/1000 | Loss: 0.00004075
Iteration 31/1000 | Loss: 0.00004043
Iteration 32/1000 | Loss: 0.00004007
Iteration 33/1000 | Loss: 0.00003977
Iteration 34/1000 | Loss: 0.00003946
Iteration 35/1000 | Loss: 0.00003922
Iteration 36/1000 | Loss: 0.00003896
Iteration 37/1000 | Loss: 0.00003886
Iteration 38/1000 | Loss: 0.00003885
Iteration 39/1000 | Loss: 0.00019072
Iteration 40/1000 | Loss: 0.00019478
Iteration 41/1000 | Loss: 0.00007575
Iteration 42/1000 | Loss: 0.00004983
Iteration 43/1000 | Loss: 0.00004343
Iteration 44/1000 | Loss: 0.00003966
Iteration 45/1000 | Loss: 0.00003651
Iteration 46/1000 | Loss: 0.00003431
Iteration 47/1000 | Loss: 0.00003293
Iteration 48/1000 | Loss: 0.00003196
Iteration 49/1000 | Loss: 0.00003140
Iteration 50/1000 | Loss: 0.00003086
Iteration 51/1000 | Loss: 0.00003034
Iteration 52/1000 | Loss: 0.00002976
Iteration 53/1000 | Loss: 0.00017433
Iteration 54/1000 | Loss: 0.00012060
Iteration 55/1000 | Loss: 0.00023257
Iteration 56/1000 | Loss: 0.00004098
Iteration 57/1000 | Loss: 0.00003453
Iteration 58/1000 | Loss: 0.00002968
Iteration 59/1000 | Loss: 0.00002576
Iteration 60/1000 | Loss: 0.00002355
Iteration 61/1000 | Loss: 0.00002239
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002128
Iteration 64/1000 | Loss: 0.00002094
Iteration 65/1000 | Loss: 0.00002080
Iteration 66/1000 | Loss: 0.00002060
Iteration 67/1000 | Loss: 0.00014192
Iteration 68/1000 | Loss: 0.00011497
Iteration 69/1000 | Loss: 0.00005647
Iteration 70/1000 | Loss: 0.00003264
Iteration 71/1000 | Loss: 0.00002326
Iteration 72/1000 | Loss: 0.00002034
Iteration 73/1000 | Loss: 0.00001895
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001768
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001707
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001704
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001688
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001686
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00001685
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001683
Iteration 91/1000 | Loss: 0.00001683
Iteration 92/1000 | Loss: 0.00001682
Iteration 93/1000 | Loss: 0.00001682
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001681
Iteration 96/1000 | Loss: 0.00001681
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001676
Iteration 103/1000 | Loss: 0.00001676
Iteration 104/1000 | Loss: 0.00001675
Iteration 105/1000 | Loss: 0.00001675
Iteration 106/1000 | Loss: 0.00001674
Iteration 107/1000 | Loss: 0.00001674
Iteration 108/1000 | Loss: 0.00001674
Iteration 109/1000 | Loss: 0.00001673
Iteration 110/1000 | Loss: 0.00001673
Iteration 111/1000 | Loss: 0.00001673
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001672
Iteration 114/1000 | Loss: 0.00001671
Iteration 115/1000 | Loss: 0.00001671
Iteration 116/1000 | Loss: 0.00001671
Iteration 117/1000 | Loss: 0.00001671
Iteration 118/1000 | Loss: 0.00001671
Iteration 119/1000 | Loss: 0.00001670
Iteration 120/1000 | Loss: 0.00001670
Iteration 121/1000 | Loss: 0.00001670
Iteration 122/1000 | Loss: 0.00001670
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001669
Iteration 132/1000 | Loss: 0.00001669
Iteration 133/1000 | Loss: 0.00001669
Iteration 134/1000 | Loss: 0.00001669
Iteration 135/1000 | Loss: 0.00001669
Iteration 136/1000 | Loss: 0.00001669
Iteration 137/1000 | Loss: 0.00001669
Iteration 138/1000 | Loss: 0.00001669
Iteration 139/1000 | Loss: 0.00001669
Iteration 140/1000 | Loss: 0.00001669
Iteration 141/1000 | Loss: 0.00001669
Iteration 142/1000 | Loss: 0.00001669
Iteration 143/1000 | Loss: 0.00001669
Iteration 144/1000 | Loss: 0.00001669
Iteration 145/1000 | Loss: 0.00001669
Iteration 146/1000 | Loss: 0.00001669
Iteration 147/1000 | Loss: 0.00001669
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001668
Iteration 150/1000 | Loss: 0.00001668
Iteration 151/1000 | Loss: 0.00001668
Iteration 152/1000 | Loss: 0.00001668
Iteration 153/1000 | Loss: 0.00001668
Iteration 154/1000 | Loss: 0.00001668
Iteration 155/1000 | Loss: 0.00001668
Iteration 156/1000 | Loss: 0.00001668
Iteration 157/1000 | Loss: 0.00001668
Iteration 158/1000 | Loss: 0.00001668
Iteration 159/1000 | Loss: 0.00001668
Iteration 160/1000 | Loss: 0.00001668
Iteration 161/1000 | Loss: 0.00001668
Iteration 162/1000 | Loss: 0.00001668
Iteration 163/1000 | Loss: 0.00001668
Iteration 164/1000 | Loss: 0.00001668
Iteration 165/1000 | Loss: 0.00001668
Iteration 166/1000 | Loss: 0.00001668
Iteration 167/1000 | Loss: 0.00001668
Iteration 168/1000 | Loss: 0.00001668
Iteration 169/1000 | Loss: 0.00001668
Iteration 170/1000 | Loss: 0.00001668
Iteration 171/1000 | Loss: 0.00001668
Iteration 172/1000 | Loss: 0.00001668
Iteration 173/1000 | Loss: 0.00001668
Iteration 174/1000 | Loss: 0.00001668
Iteration 175/1000 | Loss: 0.00001668
Iteration 176/1000 | Loss: 0.00001668
Iteration 177/1000 | Loss: 0.00001668
Iteration 178/1000 | Loss: 0.00001668
Iteration 179/1000 | Loss: 0.00001668
Iteration 180/1000 | Loss: 0.00001668
Iteration 181/1000 | Loss: 0.00001668
Iteration 182/1000 | Loss: 0.00001668
Iteration 183/1000 | Loss: 0.00001668
Iteration 184/1000 | Loss: 0.00001668
Iteration 185/1000 | Loss: 0.00001668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.6676060113240965e-05, 1.6676060113240965e-05, 1.6676060113240965e-05, 1.6676060113240965e-05, 1.6676060113240965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6676060113240965e-05

Optimization complete. Final v2v error: 3.247000217437744 mm

Highest mean error: 11.880338668823242 mm for frame 59

Lowest mean error: 2.6480624675750732 mm for frame 24

Saving results

Total time: 178.64637660980225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856510
Iteration 2/25 | Loss: 0.00083407
Iteration 3/25 | Loss: 0.00063105
Iteration 4/25 | Loss: 0.00060037
Iteration 5/25 | Loss: 0.00059300
Iteration 6/25 | Loss: 0.00059047
Iteration 7/25 | Loss: 0.00059005
Iteration 8/25 | Loss: 0.00059005
Iteration 9/25 | Loss: 0.00059005
Iteration 10/25 | Loss: 0.00059005
Iteration 11/25 | Loss: 0.00059005
Iteration 12/25 | Loss: 0.00059005
Iteration 13/25 | Loss: 0.00059005
Iteration 14/25 | Loss: 0.00059005
Iteration 15/25 | Loss: 0.00059005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000590054492931813, 0.000590054492931813, 0.000590054492931813, 0.000590054492931813, 0.000590054492931813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000590054492931813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45733845
Iteration 2/25 | Loss: 0.00025051
Iteration 3/25 | Loss: 0.00025049
Iteration 4/25 | Loss: 0.00025049
Iteration 5/25 | Loss: 0.00025049
Iteration 6/25 | Loss: 0.00025049
Iteration 7/25 | Loss: 0.00025049
Iteration 8/25 | Loss: 0.00025049
Iteration 9/25 | Loss: 0.00025049
Iteration 10/25 | Loss: 0.00025049
Iteration 11/25 | Loss: 0.00025049
Iteration 12/25 | Loss: 0.00025049
Iteration 13/25 | Loss: 0.00025049
Iteration 14/25 | Loss: 0.00025049
Iteration 15/25 | Loss: 0.00025049
Iteration 16/25 | Loss: 0.00025049
Iteration 17/25 | Loss: 0.00025049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002504856965970248, 0.0002504856965970248, 0.0002504856965970248, 0.0002504856965970248, 0.0002504856965970248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002504856965970248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025049
Iteration 2/1000 | Loss: 0.00001941
Iteration 3/1000 | Loss: 0.00001483
Iteration 4/1000 | Loss: 0.00001325
Iteration 5/1000 | Loss: 0.00001248
Iteration 6/1000 | Loss: 0.00001193
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001137
Iteration 9/1000 | Loss: 0.00001133
Iteration 10/1000 | Loss: 0.00001131
Iteration 11/1000 | Loss: 0.00001130
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001125
Iteration 15/1000 | Loss: 0.00001124
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001122
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001117
Iteration 21/1000 | Loss: 0.00001116
Iteration 22/1000 | Loss: 0.00001116
Iteration 23/1000 | Loss: 0.00001115
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001113
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001111
Iteration 29/1000 | Loss: 0.00001110
Iteration 30/1000 | Loss: 0.00001110
Iteration 31/1000 | Loss: 0.00001110
Iteration 32/1000 | Loss: 0.00001109
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001104
Iteration 38/1000 | Loss: 0.00001104
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001101
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001101
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001100
Iteration 50/1000 | Loss: 0.00001100
Iteration 51/1000 | Loss: 0.00001100
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001099
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001098
Iteration 57/1000 | Loss: 0.00001098
Iteration 58/1000 | Loss: 0.00001098
Iteration 59/1000 | Loss: 0.00001097
Iteration 60/1000 | Loss: 0.00001097
Iteration 61/1000 | Loss: 0.00001097
Iteration 62/1000 | Loss: 0.00001097
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001095
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001093
Iteration 85/1000 | Loss: 0.00001093
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001093
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001092
Iteration 92/1000 | Loss: 0.00001092
Iteration 93/1000 | Loss: 0.00001092
Iteration 94/1000 | Loss: 0.00001092
Iteration 95/1000 | Loss: 0.00001092
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.0921886314463336e-05, 1.0921886314463336e-05, 1.0921886314463336e-05, 1.0921886314463336e-05, 1.0921886314463336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0921886314463336e-05

Optimization complete. Final v2v error: 2.8191277980804443 mm

Highest mean error: 2.96683406829834 mm for frame 122

Lowest mean error: 2.7115840911865234 mm for frame 112

Saving results

Total time: 28.816863298416138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481578
Iteration 2/25 | Loss: 0.00077565
Iteration 3/25 | Loss: 0.00066879
Iteration 4/25 | Loss: 0.00064426
Iteration 5/25 | Loss: 0.00063285
Iteration 6/25 | Loss: 0.00063077
Iteration 7/25 | Loss: 0.00063014
Iteration 8/25 | Loss: 0.00063009
Iteration 9/25 | Loss: 0.00063009
Iteration 10/25 | Loss: 0.00063009
Iteration 11/25 | Loss: 0.00063009
Iteration 12/25 | Loss: 0.00063009
Iteration 13/25 | Loss: 0.00063009
Iteration 14/25 | Loss: 0.00063009
Iteration 15/25 | Loss: 0.00063009
Iteration 16/25 | Loss: 0.00063009
Iteration 17/25 | Loss: 0.00063009
Iteration 18/25 | Loss: 0.00063009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006300860550254583, 0.0006300860550254583, 0.0006300860550254583, 0.0006300860550254583, 0.0006300860550254583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006300860550254583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79677010
Iteration 2/25 | Loss: 0.00031590
Iteration 3/25 | Loss: 0.00031590
Iteration 4/25 | Loss: 0.00031590
Iteration 5/25 | Loss: 0.00031590
Iteration 6/25 | Loss: 0.00031590
Iteration 7/25 | Loss: 0.00031590
Iteration 8/25 | Loss: 0.00031589
Iteration 9/25 | Loss: 0.00031589
Iteration 10/25 | Loss: 0.00031589
Iteration 11/25 | Loss: 0.00031589
Iteration 12/25 | Loss: 0.00031589
Iteration 13/25 | Loss: 0.00031589
Iteration 14/25 | Loss: 0.00031589
Iteration 15/25 | Loss: 0.00031589
Iteration 16/25 | Loss: 0.00031589
Iteration 17/25 | Loss: 0.00031589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003158939944114536, 0.0003158939944114536, 0.0003158939944114536, 0.0003158939944114536, 0.0003158939944114536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003158939944114536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031589
Iteration 2/1000 | Loss: 0.00003302
Iteration 3/1000 | Loss: 0.00002134
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001695
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001510
Iteration 9/1000 | Loss: 0.00001504
Iteration 10/1000 | Loss: 0.00001489
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001487
Iteration 13/1000 | Loss: 0.00001481
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001476
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001469
Iteration 26/1000 | Loss: 0.00001469
Iteration 27/1000 | Loss: 0.00001469
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001469
Iteration 31/1000 | Loss: 0.00001468
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001467
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001465
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001464
Iteration 60/1000 | Loss: 0.00001464
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001464
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.4638479115092196e-05, 1.4638479115092196e-05, 1.4638479115092196e-05, 1.4638479115092196e-05, 1.4638479115092196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4638479115092196e-05

Optimization complete. Final v2v error: 3.2455623149871826 mm

Highest mean error: 3.708998203277588 mm for frame 126

Lowest mean error: 3.003272294998169 mm for frame 67

Saving results

Total time: 27.707830667495728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862099
Iteration 2/25 | Loss: 0.00079152
Iteration 3/25 | Loss: 0.00064541
Iteration 4/25 | Loss: 0.00062081
Iteration 5/25 | Loss: 0.00061192
Iteration 6/25 | Loss: 0.00061049
Iteration 7/25 | Loss: 0.00061017
Iteration 8/25 | Loss: 0.00061017
Iteration 9/25 | Loss: 0.00061017
Iteration 10/25 | Loss: 0.00061017
Iteration 11/25 | Loss: 0.00061017
Iteration 12/25 | Loss: 0.00061017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006101655308157206, 0.0006101655308157206, 0.0006101655308157206, 0.0006101655308157206, 0.0006101655308157206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006101655308157206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66637492
Iteration 2/25 | Loss: 0.00029524
Iteration 3/25 | Loss: 0.00029524
Iteration 4/25 | Loss: 0.00029523
Iteration 5/25 | Loss: 0.00029523
Iteration 6/25 | Loss: 0.00029523
Iteration 7/25 | Loss: 0.00029523
Iteration 8/25 | Loss: 0.00029523
Iteration 9/25 | Loss: 0.00029523
Iteration 10/25 | Loss: 0.00029523
Iteration 11/25 | Loss: 0.00029523
Iteration 12/25 | Loss: 0.00029523
Iteration 13/25 | Loss: 0.00029523
Iteration 14/25 | Loss: 0.00029523
Iteration 15/25 | Loss: 0.00029523
Iteration 16/25 | Loss: 0.00029523
Iteration 17/25 | Loss: 0.00029523
Iteration 18/25 | Loss: 0.00029523
Iteration 19/25 | Loss: 0.00029523
Iteration 20/25 | Loss: 0.00029523
Iteration 21/25 | Loss: 0.00029523
Iteration 22/25 | Loss: 0.00029523
Iteration 23/25 | Loss: 0.00029523
Iteration 24/25 | Loss: 0.00029523
Iteration 25/25 | Loss: 0.00029523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029523
Iteration 2/1000 | Loss: 0.00002130
Iteration 3/1000 | Loss: 0.00001488
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001342
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001302
Iteration 8/1000 | Loss: 0.00001276
Iteration 9/1000 | Loss: 0.00001265
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001254
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001247
Iteration 17/1000 | Loss: 0.00001246
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001242
Iteration 22/1000 | Loss: 0.00001242
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001236
Iteration 27/1000 | Loss: 0.00001236
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001236
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001234
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001229
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001228
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001225
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001223
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001222
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001219
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001218
Iteration 66/1000 | Loss: 0.00001218
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001218
Iteration 69/1000 | Loss: 0.00001218
Iteration 70/1000 | Loss: 0.00001218
Iteration 71/1000 | Loss: 0.00001218
Iteration 72/1000 | Loss: 0.00001217
Iteration 73/1000 | Loss: 0.00001217
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001216
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001215
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001213
Iteration 89/1000 | Loss: 0.00001213
Iteration 90/1000 | Loss: 0.00001213
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001212
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001209
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001208
Iteration 110/1000 | Loss: 0.00001208
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00001208
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001208
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001207
Iteration 124/1000 | Loss: 0.00001207
Iteration 125/1000 | Loss: 0.00001207
Iteration 126/1000 | Loss: 0.00001207
Iteration 127/1000 | Loss: 0.00001207
Iteration 128/1000 | Loss: 0.00001207
Iteration 129/1000 | Loss: 0.00001207
Iteration 130/1000 | Loss: 0.00001207
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001206
Iteration 135/1000 | Loss: 0.00001206
Iteration 136/1000 | Loss: 0.00001206
Iteration 137/1000 | Loss: 0.00001206
Iteration 138/1000 | Loss: 0.00001206
Iteration 139/1000 | Loss: 0.00001206
Iteration 140/1000 | Loss: 0.00001206
Iteration 141/1000 | Loss: 0.00001206
Iteration 142/1000 | Loss: 0.00001206
Iteration 143/1000 | Loss: 0.00001206
Iteration 144/1000 | Loss: 0.00001206
Iteration 145/1000 | Loss: 0.00001206
Iteration 146/1000 | Loss: 0.00001206
Iteration 147/1000 | Loss: 0.00001206
Iteration 148/1000 | Loss: 0.00001205
Iteration 149/1000 | Loss: 0.00001205
Iteration 150/1000 | Loss: 0.00001205
Iteration 151/1000 | Loss: 0.00001205
Iteration 152/1000 | Loss: 0.00001205
Iteration 153/1000 | Loss: 0.00001205
Iteration 154/1000 | Loss: 0.00001205
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Iteration 158/1000 | Loss: 0.00001205
Iteration 159/1000 | Loss: 0.00001205
Iteration 160/1000 | Loss: 0.00001205
Iteration 161/1000 | Loss: 0.00001205
Iteration 162/1000 | Loss: 0.00001205
Iteration 163/1000 | Loss: 0.00001205
Iteration 164/1000 | Loss: 0.00001205
Iteration 165/1000 | Loss: 0.00001205
Iteration 166/1000 | Loss: 0.00001205
Iteration 167/1000 | Loss: 0.00001205
Iteration 168/1000 | Loss: 0.00001205
Iteration 169/1000 | Loss: 0.00001205
Iteration 170/1000 | Loss: 0.00001204
Iteration 171/1000 | Loss: 0.00001204
Iteration 172/1000 | Loss: 0.00001204
Iteration 173/1000 | Loss: 0.00001204
Iteration 174/1000 | Loss: 0.00001204
Iteration 175/1000 | Loss: 0.00001204
Iteration 176/1000 | Loss: 0.00001204
Iteration 177/1000 | Loss: 0.00001204
Iteration 178/1000 | Loss: 0.00001204
Iteration 179/1000 | Loss: 0.00001204
Iteration 180/1000 | Loss: 0.00001204
Iteration 181/1000 | Loss: 0.00001204
Iteration 182/1000 | Loss: 0.00001204
Iteration 183/1000 | Loss: 0.00001204
Iteration 184/1000 | Loss: 0.00001204
Iteration 185/1000 | Loss: 0.00001204
Iteration 186/1000 | Loss: 0.00001204
Iteration 187/1000 | Loss: 0.00001204
Iteration 188/1000 | Loss: 0.00001204
Iteration 189/1000 | Loss: 0.00001204
Iteration 190/1000 | Loss: 0.00001204
Iteration 191/1000 | Loss: 0.00001204
Iteration 192/1000 | Loss: 0.00001204
Iteration 193/1000 | Loss: 0.00001204
Iteration 194/1000 | Loss: 0.00001204
Iteration 195/1000 | Loss: 0.00001204
Iteration 196/1000 | Loss: 0.00001204
Iteration 197/1000 | Loss: 0.00001204
Iteration 198/1000 | Loss: 0.00001204
Iteration 199/1000 | Loss: 0.00001204
Iteration 200/1000 | Loss: 0.00001204
Iteration 201/1000 | Loss: 0.00001204
Iteration 202/1000 | Loss: 0.00001204
Iteration 203/1000 | Loss: 0.00001204
Iteration 204/1000 | Loss: 0.00001204
Iteration 205/1000 | Loss: 0.00001204
Iteration 206/1000 | Loss: 0.00001204
Iteration 207/1000 | Loss: 0.00001204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.2038974091410637e-05, 1.2038974091410637e-05, 1.2038974091410637e-05, 1.2038974091410637e-05, 1.2038974091410637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2038974091410637e-05

Optimization complete. Final v2v error: 2.9493660926818848 mm

Highest mean error: 3.318995952606201 mm for frame 68

Lowest mean error: 2.8427786827087402 mm for frame 115

Saving results

Total time: 35.800912618637085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880535
Iteration 2/25 | Loss: 0.00105356
Iteration 3/25 | Loss: 0.00073749
Iteration 4/25 | Loss: 0.00069540
Iteration 5/25 | Loss: 0.00067892
Iteration 6/25 | Loss: 0.00067277
Iteration 7/25 | Loss: 0.00067059
Iteration 8/25 | Loss: 0.00066954
Iteration 9/25 | Loss: 0.00066903
Iteration 10/25 | Loss: 0.00066896
Iteration 11/25 | Loss: 0.00066896
Iteration 12/25 | Loss: 0.00066896
Iteration 13/25 | Loss: 0.00066896
Iteration 14/25 | Loss: 0.00066896
Iteration 15/25 | Loss: 0.00066896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006689629517495632, 0.0006689629517495632, 0.0006689629517495632, 0.0006689629517495632, 0.0006689629517495632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006689629517495632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48979521
Iteration 2/25 | Loss: 0.00028026
Iteration 3/25 | Loss: 0.00028026
Iteration 4/25 | Loss: 0.00028026
Iteration 5/25 | Loss: 0.00028026
Iteration 6/25 | Loss: 0.00028026
Iteration 7/25 | Loss: 0.00028026
Iteration 8/25 | Loss: 0.00028026
Iteration 9/25 | Loss: 0.00028026
Iteration 10/25 | Loss: 0.00028026
Iteration 11/25 | Loss: 0.00028026
Iteration 12/25 | Loss: 0.00028026
Iteration 13/25 | Loss: 0.00028026
Iteration 14/25 | Loss: 0.00028026
Iteration 15/25 | Loss: 0.00028026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0002802575472742319, 0.0002802575472742319, 0.0002802575472742319, 0.0002802575472742319, 0.0002802575472742319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002802575472742319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028026
Iteration 2/1000 | Loss: 0.00004495
Iteration 3/1000 | Loss: 0.00002831
Iteration 4/1000 | Loss: 0.00002311
Iteration 5/1000 | Loss: 0.00002157
Iteration 6/1000 | Loss: 0.00002072
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001932
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001866
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001843
Iteration 14/1000 | Loss: 0.00001829
Iteration 15/1000 | Loss: 0.00001824
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001818
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001804
Iteration 22/1000 | Loss: 0.00001801
Iteration 23/1000 | Loss: 0.00001800
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00001796
Iteration 30/1000 | Loss: 0.00001795
Iteration 31/1000 | Loss: 0.00001795
Iteration 32/1000 | Loss: 0.00001795
Iteration 33/1000 | Loss: 0.00001794
Iteration 34/1000 | Loss: 0.00001794
Iteration 35/1000 | Loss: 0.00001794
Iteration 36/1000 | Loss: 0.00001792
Iteration 37/1000 | Loss: 0.00001792
Iteration 38/1000 | Loss: 0.00001792
Iteration 39/1000 | Loss: 0.00001791
Iteration 40/1000 | Loss: 0.00001791
Iteration 41/1000 | Loss: 0.00001791
Iteration 42/1000 | Loss: 0.00001789
Iteration 43/1000 | Loss: 0.00001789
Iteration 44/1000 | Loss: 0.00001789
Iteration 45/1000 | Loss: 0.00001789
Iteration 46/1000 | Loss: 0.00001789
Iteration 47/1000 | Loss: 0.00001789
Iteration 48/1000 | Loss: 0.00001789
Iteration 49/1000 | Loss: 0.00001789
Iteration 50/1000 | Loss: 0.00001789
Iteration 51/1000 | Loss: 0.00001789
Iteration 52/1000 | Loss: 0.00001788
Iteration 53/1000 | Loss: 0.00001788
Iteration 54/1000 | Loss: 0.00001788
Iteration 55/1000 | Loss: 0.00001787
Iteration 56/1000 | Loss: 0.00001787
Iteration 57/1000 | Loss: 0.00001787
Iteration 58/1000 | Loss: 0.00001786
Iteration 59/1000 | Loss: 0.00001786
Iteration 60/1000 | Loss: 0.00001786
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001785
Iteration 64/1000 | Loss: 0.00001785
Iteration 65/1000 | Loss: 0.00001785
Iteration 66/1000 | Loss: 0.00001785
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001784
Iteration 70/1000 | Loss: 0.00001784
Iteration 71/1000 | Loss: 0.00001784
Iteration 72/1000 | Loss: 0.00001784
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001783
Iteration 75/1000 | Loss: 0.00001783
Iteration 76/1000 | Loss: 0.00001783
Iteration 77/1000 | Loss: 0.00001783
Iteration 78/1000 | Loss: 0.00001783
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001783
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001782
Iteration 92/1000 | Loss: 0.00001782
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001782
Iteration 96/1000 | Loss: 0.00001782
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001781
Iteration 99/1000 | Loss: 0.00001781
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001781
Iteration 103/1000 | Loss: 0.00001781
Iteration 104/1000 | Loss: 0.00001781
Iteration 105/1000 | Loss: 0.00001781
Iteration 106/1000 | Loss: 0.00001781
Iteration 107/1000 | Loss: 0.00001781
Iteration 108/1000 | Loss: 0.00001781
Iteration 109/1000 | Loss: 0.00001780
Iteration 110/1000 | Loss: 0.00001780
Iteration 111/1000 | Loss: 0.00001780
Iteration 112/1000 | Loss: 0.00001780
Iteration 113/1000 | Loss: 0.00001780
Iteration 114/1000 | Loss: 0.00001780
Iteration 115/1000 | Loss: 0.00001780
Iteration 116/1000 | Loss: 0.00001780
Iteration 117/1000 | Loss: 0.00001780
Iteration 118/1000 | Loss: 0.00001780
Iteration 119/1000 | Loss: 0.00001779
Iteration 120/1000 | Loss: 0.00001779
Iteration 121/1000 | Loss: 0.00001779
Iteration 122/1000 | Loss: 0.00001779
Iteration 123/1000 | Loss: 0.00001779
Iteration 124/1000 | Loss: 0.00001779
Iteration 125/1000 | Loss: 0.00001779
Iteration 126/1000 | Loss: 0.00001779
Iteration 127/1000 | Loss: 0.00001779
Iteration 128/1000 | Loss: 0.00001779
Iteration 129/1000 | Loss: 0.00001779
Iteration 130/1000 | Loss: 0.00001779
Iteration 131/1000 | Loss: 0.00001778
Iteration 132/1000 | Loss: 0.00001778
Iteration 133/1000 | Loss: 0.00001778
Iteration 134/1000 | Loss: 0.00001778
Iteration 135/1000 | Loss: 0.00001777
Iteration 136/1000 | Loss: 0.00001777
Iteration 137/1000 | Loss: 0.00001777
Iteration 138/1000 | Loss: 0.00001777
Iteration 139/1000 | Loss: 0.00001777
Iteration 140/1000 | Loss: 0.00001777
Iteration 141/1000 | Loss: 0.00001777
Iteration 142/1000 | Loss: 0.00001777
Iteration 143/1000 | Loss: 0.00001777
Iteration 144/1000 | Loss: 0.00001777
Iteration 145/1000 | Loss: 0.00001777
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001776
Iteration 148/1000 | Loss: 0.00001776
Iteration 149/1000 | Loss: 0.00001776
Iteration 150/1000 | Loss: 0.00001776
Iteration 151/1000 | Loss: 0.00001776
Iteration 152/1000 | Loss: 0.00001776
Iteration 153/1000 | Loss: 0.00001776
Iteration 154/1000 | Loss: 0.00001776
Iteration 155/1000 | Loss: 0.00001776
Iteration 156/1000 | Loss: 0.00001776
Iteration 157/1000 | Loss: 0.00001776
Iteration 158/1000 | Loss: 0.00001776
Iteration 159/1000 | Loss: 0.00001776
Iteration 160/1000 | Loss: 0.00001776
Iteration 161/1000 | Loss: 0.00001775
Iteration 162/1000 | Loss: 0.00001775
Iteration 163/1000 | Loss: 0.00001775
Iteration 164/1000 | Loss: 0.00001775
Iteration 165/1000 | Loss: 0.00001775
Iteration 166/1000 | Loss: 0.00001775
Iteration 167/1000 | Loss: 0.00001775
Iteration 168/1000 | Loss: 0.00001775
Iteration 169/1000 | Loss: 0.00001775
Iteration 170/1000 | Loss: 0.00001775
Iteration 171/1000 | Loss: 0.00001775
Iteration 172/1000 | Loss: 0.00001775
Iteration 173/1000 | Loss: 0.00001775
Iteration 174/1000 | Loss: 0.00001775
Iteration 175/1000 | Loss: 0.00001775
Iteration 176/1000 | Loss: 0.00001775
Iteration 177/1000 | Loss: 0.00001775
Iteration 178/1000 | Loss: 0.00001775
Iteration 179/1000 | Loss: 0.00001775
Iteration 180/1000 | Loss: 0.00001775
Iteration 181/1000 | Loss: 0.00001775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.775448072294239e-05, 1.775448072294239e-05, 1.775448072294239e-05, 1.775448072294239e-05, 1.775448072294239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.775448072294239e-05

Optimization complete. Final v2v error: 3.483003854751587 mm

Highest mean error: 6.09011173248291 mm for frame 70

Lowest mean error: 2.6430869102478027 mm for frame 129

Saving results

Total time: 44.640087366104126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795216
Iteration 2/25 | Loss: 0.00137718
Iteration 3/25 | Loss: 0.00108714
Iteration 4/25 | Loss: 0.00104724
Iteration 5/25 | Loss: 0.00103595
Iteration 6/25 | Loss: 0.00103300
Iteration 7/25 | Loss: 0.00103262
Iteration 8/25 | Loss: 0.00103262
Iteration 9/25 | Loss: 0.00103262
Iteration 10/25 | Loss: 0.00103262
Iteration 11/25 | Loss: 0.00103262
Iteration 12/25 | Loss: 0.00103262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010326225310564041, 0.0010326225310564041, 0.0010326225310564041, 0.0010326225310564041, 0.0010326225310564041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010326225310564041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91528404
Iteration 2/25 | Loss: 0.00049171
Iteration 3/25 | Loss: 0.00049171
Iteration 4/25 | Loss: 0.00049171
Iteration 5/25 | Loss: 0.00049171
Iteration 6/25 | Loss: 0.00049171
Iteration 7/25 | Loss: 0.00049171
Iteration 8/25 | Loss: 0.00049171
Iteration 9/25 | Loss: 0.00049170
Iteration 10/25 | Loss: 0.00049170
Iteration 11/25 | Loss: 0.00049170
Iteration 12/25 | Loss: 0.00049170
Iteration 13/25 | Loss: 0.00049170
Iteration 14/25 | Loss: 0.00049170
Iteration 15/25 | Loss: 0.00049170
Iteration 16/25 | Loss: 0.00049170
Iteration 17/25 | Loss: 0.00049170
Iteration 18/25 | Loss: 0.00049170
Iteration 19/25 | Loss: 0.00049170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004917048499919474, 0.0004917048499919474, 0.0004917048499919474, 0.0004917048499919474, 0.0004917048499919474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004917048499919474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049170
Iteration 2/1000 | Loss: 0.00009021
Iteration 3/1000 | Loss: 0.00007204
Iteration 4/1000 | Loss: 0.00006624
Iteration 5/1000 | Loss: 0.00006305
Iteration 6/1000 | Loss: 0.00006057
Iteration 7/1000 | Loss: 0.00005925
Iteration 8/1000 | Loss: 0.00005814
Iteration 9/1000 | Loss: 0.00005758
Iteration 10/1000 | Loss: 0.00005711
Iteration 11/1000 | Loss: 0.00005679
Iteration 12/1000 | Loss: 0.00005654
Iteration 13/1000 | Loss: 0.00005629
Iteration 14/1000 | Loss: 0.00005609
Iteration 15/1000 | Loss: 0.00005592
Iteration 16/1000 | Loss: 0.00005590
Iteration 17/1000 | Loss: 0.00005586
Iteration 18/1000 | Loss: 0.00005574
Iteration 19/1000 | Loss: 0.00005571
Iteration 20/1000 | Loss: 0.00005551
Iteration 21/1000 | Loss: 0.00005548
Iteration 22/1000 | Loss: 0.00005543
Iteration 23/1000 | Loss: 0.00005543
Iteration 24/1000 | Loss: 0.00005527
Iteration 25/1000 | Loss: 0.00005517
Iteration 26/1000 | Loss: 0.00005501
Iteration 27/1000 | Loss: 0.00005497
Iteration 28/1000 | Loss: 0.00005493
Iteration 29/1000 | Loss: 0.00005490
Iteration 30/1000 | Loss: 0.00005489
Iteration 31/1000 | Loss: 0.00005485
Iteration 32/1000 | Loss: 0.00005485
Iteration 33/1000 | Loss: 0.00005483
Iteration 34/1000 | Loss: 0.00005483
Iteration 35/1000 | Loss: 0.00005482
Iteration 36/1000 | Loss: 0.00005482
Iteration 37/1000 | Loss: 0.00005482
Iteration 38/1000 | Loss: 0.00005481
Iteration 39/1000 | Loss: 0.00005481
Iteration 40/1000 | Loss: 0.00005481
Iteration 41/1000 | Loss: 0.00005481
Iteration 42/1000 | Loss: 0.00005481
Iteration 43/1000 | Loss: 0.00005481
Iteration 44/1000 | Loss: 0.00005481
Iteration 45/1000 | Loss: 0.00005481
Iteration 46/1000 | Loss: 0.00005480
Iteration 47/1000 | Loss: 0.00005480
Iteration 48/1000 | Loss: 0.00005479
Iteration 49/1000 | Loss: 0.00005479
Iteration 50/1000 | Loss: 0.00005479
Iteration 51/1000 | Loss: 0.00005479
Iteration 52/1000 | Loss: 0.00005479
Iteration 53/1000 | Loss: 0.00005479
Iteration 54/1000 | Loss: 0.00005478
Iteration 55/1000 | Loss: 0.00005478
Iteration 56/1000 | Loss: 0.00005478
Iteration 57/1000 | Loss: 0.00005478
Iteration 58/1000 | Loss: 0.00005478
Iteration 59/1000 | Loss: 0.00005478
Iteration 60/1000 | Loss: 0.00005478
Iteration 61/1000 | Loss: 0.00005477
Iteration 62/1000 | Loss: 0.00005477
Iteration 63/1000 | Loss: 0.00005477
Iteration 64/1000 | Loss: 0.00005477
Iteration 65/1000 | Loss: 0.00005477
Iteration 66/1000 | Loss: 0.00005476
Iteration 67/1000 | Loss: 0.00005476
Iteration 68/1000 | Loss: 0.00005476
Iteration 69/1000 | Loss: 0.00005476
Iteration 70/1000 | Loss: 0.00005476
Iteration 71/1000 | Loss: 0.00005476
Iteration 72/1000 | Loss: 0.00005476
Iteration 73/1000 | Loss: 0.00005476
Iteration 74/1000 | Loss: 0.00005475
Iteration 75/1000 | Loss: 0.00005475
Iteration 76/1000 | Loss: 0.00005475
Iteration 77/1000 | Loss: 0.00005474
Iteration 78/1000 | Loss: 0.00005474
Iteration 79/1000 | Loss: 0.00005474
Iteration 80/1000 | Loss: 0.00005474
Iteration 81/1000 | Loss: 0.00005474
Iteration 82/1000 | Loss: 0.00005474
Iteration 83/1000 | Loss: 0.00005474
Iteration 84/1000 | Loss: 0.00005474
Iteration 85/1000 | Loss: 0.00005474
Iteration 86/1000 | Loss: 0.00005474
Iteration 87/1000 | Loss: 0.00005473
Iteration 88/1000 | Loss: 0.00005473
Iteration 89/1000 | Loss: 0.00005473
Iteration 90/1000 | Loss: 0.00005473
Iteration 91/1000 | Loss: 0.00005473
Iteration 92/1000 | Loss: 0.00005473
Iteration 93/1000 | Loss: 0.00005473
Iteration 94/1000 | Loss: 0.00005473
Iteration 95/1000 | Loss: 0.00005472
Iteration 96/1000 | Loss: 0.00005472
Iteration 97/1000 | Loss: 0.00005472
Iteration 98/1000 | Loss: 0.00005472
Iteration 99/1000 | Loss: 0.00005472
Iteration 100/1000 | Loss: 0.00005472
Iteration 101/1000 | Loss: 0.00005472
Iteration 102/1000 | Loss: 0.00005472
Iteration 103/1000 | Loss: 0.00005472
Iteration 104/1000 | Loss: 0.00005471
Iteration 105/1000 | Loss: 0.00005471
Iteration 106/1000 | Loss: 0.00005471
Iteration 107/1000 | Loss: 0.00005471
Iteration 108/1000 | Loss: 0.00005471
Iteration 109/1000 | Loss: 0.00005470
Iteration 110/1000 | Loss: 0.00005470
Iteration 111/1000 | Loss: 0.00005469
Iteration 112/1000 | Loss: 0.00005469
Iteration 113/1000 | Loss: 0.00005469
Iteration 114/1000 | Loss: 0.00005469
Iteration 115/1000 | Loss: 0.00005469
Iteration 116/1000 | Loss: 0.00005469
Iteration 117/1000 | Loss: 0.00005469
Iteration 118/1000 | Loss: 0.00005469
Iteration 119/1000 | Loss: 0.00005469
Iteration 120/1000 | Loss: 0.00005469
Iteration 121/1000 | Loss: 0.00005469
Iteration 122/1000 | Loss: 0.00005469
Iteration 123/1000 | Loss: 0.00005469
Iteration 124/1000 | Loss: 0.00005469
Iteration 125/1000 | Loss: 0.00005469
Iteration 126/1000 | Loss: 0.00005469
Iteration 127/1000 | Loss: 0.00005469
Iteration 128/1000 | Loss: 0.00005469
Iteration 129/1000 | Loss: 0.00005469
Iteration 130/1000 | Loss: 0.00005469
Iteration 131/1000 | Loss: 0.00005469
Iteration 132/1000 | Loss: 0.00005469
Iteration 133/1000 | Loss: 0.00005469
Iteration 134/1000 | Loss: 0.00005469
Iteration 135/1000 | Loss: 0.00005469
Iteration 136/1000 | Loss: 0.00005469
Iteration 137/1000 | Loss: 0.00005469
Iteration 138/1000 | Loss: 0.00005469
Iteration 139/1000 | Loss: 0.00005469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [5.468989911605604e-05, 5.468989911605604e-05, 5.468989911605604e-05, 5.468989911605604e-05, 5.468989911605604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.468989911605604e-05

Optimization complete. Final v2v error: 5.8381218910217285 mm

Highest mean error: 7.986737251281738 mm for frame 120

Lowest mean error: 4.7503557205200195 mm for frame 155

Saving results

Total time: 53.41177272796631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421108
Iteration 2/25 | Loss: 0.00089479
Iteration 3/25 | Loss: 0.00069601
Iteration 4/25 | Loss: 0.00064995
Iteration 5/25 | Loss: 0.00063745
Iteration 6/25 | Loss: 0.00063488
Iteration 7/25 | Loss: 0.00063414
Iteration 8/25 | Loss: 0.00063395
Iteration 9/25 | Loss: 0.00063395
Iteration 10/25 | Loss: 0.00063395
Iteration 11/25 | Loss: 0.00063395
Iteration 12/25 | Loss: 0.00063395
Iteration 13/25 | Loss: 0.00063395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006339523242786527, 0.0006339523242786527, 0.0006339523242786527, 0.0006339523242786527, 0.0006339523242786527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006339523242786527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58307040
Iteration 2/25 | Loss: 0.00027031
Iteration 3/25 | Loss: 0.00027031
Iteration 4/25 | Loss: 0.00027031
Iteration 5/25 | Loss: 0.00027031
Iteration 6/25 | Loss: 0.00027031
Iteration 7/25 | Loss: 0.00027031
Iteration 8/25 | Loss: 0.00027031
Iteration 9/25 | Loss: 0.00027031
Iteration 10/25 | Loss: 0.00027030
Iteration 11/25 | Loss: 0.00027030
Iteration 12/25 | Loss: 0.00027030
Iteration 13/25 | Loss: 0.00027030
Iteration 14/25 | Loss: 0.00027030
Iteration 15/25 | Loss: 0.00027030
Iteration 16/25 | Loss: 0.00027030
Iteration 17/25 | Loss: 0.00027030
Iteration 18/25 | Loss: 0.00027030
Iteration 19/25 | Loss: 0.00027030
Iteration 20/25 | Loss: 0.00027030
Iteration 21/25 | Loss: 0.00027030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002703047648537904, 0.0002703047648537904, 0.0002703047648537904, 0.0002703047648537904, 0.0002703047648537904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002703047648537904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027030
Iteration 2/1000 | Loss: 0.00003647
Iteration 3/1000 | Loss: 0.00002498
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001979
Iteration 6/1000 | Loss: 0.00001909
Iteration 7/1000 | Loss: 0.00001858
Iteration 8/1000 | Loss: 0.00001811
Iteration 9/1000 | Loss: 0.00001777
Iteration 10/1000 | Loss: 0.00001755
Iteration 11/1000 | Loss: 0.00001749
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001707
Iteration 23/1000 | Loss: 0.00001707
Iteration 24/1000 | Loss: 0.00001706
Iteration 25/1000 | Loss: 0.00001705
Iteration 26/1000 | Loss: 0.00001704
Iteration 27/1000 | Loss: 0.00001704
Iteration 28/1000 | Loss: 0.00001703
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001696
Iteration 38/1000 | Loss: 0.00001695
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001694
Iteration 42/1000 | Loss: 0.00001694
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001693
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001690
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001689
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001687
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001686
Iteration 62/1000 | Loss: 0.00001686
Iteration 63/1000 | Loss: 0.00001686
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001684
Iteration 68/1000 | Loss: 0.00001684
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001683
Iteration 73/1000 | Loss: 0.00001683
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001682
Iteration 77/1000 | Loss: 0.00001682
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001681
Iteration 80/1000 | Loss: 0.00001681
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001680
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001679
Iteration 89/1000 | Loss: 0.00001679
Iteration 90/1000 | Loss: 0.00001679
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001677
Iteration 95/1000 | Loss: 0.00001677
Iteration 96/1000 | Loss: 0.00001677
Iteration 97/1000 | Loss: 0.00001676
Iteration 98/1000 | Loss: 0.00001676
Iteration 99/1000 | Loss: 0.00001676
Iteration 100/1000 | Loss: 0.00001676
Iteration 101/1000 | Loss: 0.00001676
Iteration 102/1000 | Loss: 0.00001675
Iteration 103/1000 | Loss: 0.00001675
Iteration 104/1000 | Loss: 0.00001675
Iteration 105/1000 | Loss: 0.00001675
Iteration 106/1000 | Loss: 0.00001675
Iteration 107/1000 | Loss: 0.00001675
Iteration 108/1000 | Loss: 0.00001674
Iteration 109/1000 | Loss: 0.00001674
Iteration 110/1000 | Loss: 0.00001674
Iteration 111/1000 | Loss: 0.00001674
Iteration 112/1000 | Loss: 0.00001674
Iteration 113/1000 | Loss: 0.00001674
Iteration 114/1000 | Loss: 0.00001674
Iteration 115/1000 | Loss: 0.00001674
Iteration 116/1000 | Loss: 0.00001674
Iteration 117/1000 | Loss: 0.00001673
Iteration 118/1000 | Loss: 0.00001673
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001673
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001672
Iteration 126/1000 | Loss: 0.00001672
Iteration 127/1000 | Loss: 0.00001672
Iteration 128/1000 | Loss: 0.00001672
Iteration 129/1000 | Loss: 0.00001672
Iteration 130/1000 | Loss: 0.00001672
Iteration 131/1000 | Loss: 0.00001672
Iteration 132/1000 | Loss: 0.00001672
Iteration 133/1000 | Loss: 0.00001672
Iteration 134/1000 | Loss: 0.00001672
Iteration 135/1000 | Loss: 0.00001672
Iteration 136/1000 | Loss: 0.00001671
Iteration 137/1000 | Loss: 0.00001671
Iteration 138/1000 | Loss: 0.00001671
Iteration 139/1000 | Loss: 0.00001671
Iteration 140/1000 | Loss: 0.00001671
Iteration 141/1000 | Loss: 0.00001671
Iteration 142/1000 | Loss: 0.00001671
Iteration 143/1000 | Loss: 0.00001671
Iteration 144/1000 | Loss: 0.00001671
Iteration 145/1000 | Loss: 0.00001671
Iteration 146/1000 | Loss: 0.00001671
Iteration 147/1000 | Loss: 0.00001671
Iteration 148/1000 | Loss: 0.00001671
Iteration 149/1000 | Loss: 0.00001671
Iteration 150/1000 | Loss: 0.00001670
Iteration 151/1000 | Loss: 0.00001670
Iteration 152/1000 | Loss: 0.00001670
Iteration 153/1000 | Loss: 0.00001670
Iteration 154/1000 | Loss: 0.00001670
Iteration 155/1000 | Loss: 0.00001670
Iteration 156/1000 | Loss: 0.00001670
Iteration 157/1000 | Loss: 0.00001670
Iteration 158/1000 | Loss: 0.00001670
Iteration 159/1000 | Loss: 0.00001670
Iteration 160/1000 | Loss: 0.00001670
Iteration 161/1000 | Loss: 0.00001670
Iteration 162/1000 | Loss: 0.00001670
Iteration 163/1000 | Loss: 0.00001670
Iteration 164/1000 | Loss: 0.00001670
Iteration 165/1000 | Loss: 0.00001670
Iteration 166/1000 | Loss: 0.00001669
Iteration 167/1000 | Loss: 0.00001669
Iteration 168/1000 | Loss: 0.00001669
Iteration 169/1000 | Loss: 0.00001669
Iteration 170/1000 | Loss: 0.00001669
Iteration 171/1000 | Loss: 0.00001669
Iteration 172/1000 | Loss: 0.00001669
Iteration 173/1000 | Loss: 0.00001669
Iteration 174/1000 | Loss: 0.00001669
Iteration 175/1000 | Loss: 0.00001669
Iteration 176/1000 | Loss: 0.00001669
Iteration 177/1000 | Loss: 0.00001669
Iteration 178/1000 | Loss: 0.00001669
Iteration 179/1000 | Loss: 0.00001668
Iteration 180/1000 | Loss: 0.00001668
Iteration 181/1000 | Loss: 0.00001668
Iteration 182/1000 | Loss: 0.00001668
Iteration 183/1000 | Loss: 0.00001668
Iteration 184/1000 | Loss: 0.00001668
Iteration 185/1000 | Loss: 0.00001668
Iteration 186/1000 | Loss: 0.00001668
Iteration 187/1000 | Loss: 0.00001668
Iteration 188/1000 | Loss: 0.00001668
Iteration 189/1000 | Loss: 0.00001668
Iteration 190/1000 | Loss: 0.00001668
Iteration 191/1000 | Loss: 0.00001668
Iteration 192/1000 | Loss: 0.00001668
Iteration 193/1000 | Loss: 0.00001668
Iteration 194/1000 | Loss: 0.00001668
Iteration 195/1000 | Loss: 0.00001668
Iteration 196/1000 | Loss: 0.00001668
Iteration 197/1000 | Loss: 0.00001667
Iteration 198/1000 | Loss: 0.00001667
Iteration 199/1000 | Loss: 0.00001667
Iteration 200/1000 | Loss: 0.00001667
Iteration 201/1000 | Loss: 0.00001667
Iteration 202/1000 | Loss: 0.00001667
Iteration 203/1000 | Loss: 0.00001667
Iteration 204/1000 | Loss: 0.00001667
Iteration 205/1000 | Loss: 0.00001667
Iteration 206/1000 | Loss: 0.00001667
Iteration 207/1000 | Loss: 0.00001667
Iteration 208/1000 | Loss: 0.00001667
Iteration 209/1000 | Loss: 0.00001667
Iteration 210/1000 | Loss: 0.00001667
Iteration 211/1000 | Loss: 0.00001667
Iteration 212/1000 | Loss: 0.00001667
Iteration 213/1000 | Loss: 0.00001667
Iteration 214/1000 | Loss: 0.00001667
Iteration 215/1000 | Loss: 0.00001667
Iteration 216/1000 | Loss: 0.00001666
Iteration 217/1000 | Loss: 0.00001666
Iteration 218/1000 | Loss: 0.00001666
Iteration 219/1000 | Loss: 0.00001666
Iteration 220/1000 | Loss: 0.00001666
Iteration 221/1000 | Loss: 0.00001666
Iteration 222/1000 | Loss: 0.00001666
Iteration 223/1000 | Loss: 0.00001666
Iteration 224/1000 | Loss: 0.00001666
Iteration 225/1000 | Loss: 0.00001666
Iteration 226/1000 | Loss: 0.00001666
Iteration 227/1000 | Loss: 0.00001666
Iteration 228/1000 | Loss: 0.00001666
Iteration 229/1000 | Loss: 0.00001666
Iteration 230/1000 | Loss: 0.00001666
Iteration 231/1000 | Loss: 0.00001666
Iteration 232/1000 | Loss: 0.00001666
Iteration 233/1000 | Loss: 0.00001666
Iteration 234/1000 | Loss: 0.00001666
Iteration 235/1000 | Loss: 0.00001666
Iteration 236/1000 | Loss: 0.00001666
Iteration 237/1000 | Loss: 0.00001666
Iteration 238/1000 | Loss: 0.00001666
Iteration 239/1000 | Loss: 0.00001666
Iteration 240/1000 | Loss: 0.00001666
Iteration 241/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.6661777408444323e-05, 1.6661777408444323e-05, 1.6661777408444323e-05, 1.6661777408444323e-05, 1.6661777408444323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6661777408444323e-05

Optimization complete. Final v2v error: 3.4591660499572754 mm

Highest mean error: 4.818059921264648 mm for frame 46

Lowest mean error: 2.9663889408111572 mm for frame 23

Saving results

Total time: 42.571465253829956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863211
Iteration 2/25 | Loss: 0.00081777
Iteration 3/25 | Loss: 0.00064059
Iteration 4/25 | Loss: 0.00061110
Iteration 5/25 | Loss: 0.00060006
Iteration 6/25 | Loss: 0.00059779
Iteration 7/25 | Loss: 0.00059718
Iteration 8/25 | Loss: 0.00059718
Iteration 9/25 | Loss: 0.00059718
Iteration 10/25 | Loss: 0.00059718
Iteration 11/25 | Loss: 0.00059718
Iteration 12/25 | Loss: 0.00059718
Iteration 13/25 | Loss: 0.00059718
Iteration 14/25 | Loss: 0.00059718
Iteration 15/25 | Loss: 0.00059718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005971765494905412, 0.0005971765494905412, 0.0005971765494905412, 0.0005971765494905412, 0.0005971765494905412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005971765494905412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01436114
Iteration 2/25 | Loss: 0.00026823
Iteration 3/25 | Loss: 0.00026822
Iteration 4/25 | Loss: 0.00026822
Iteration 5/25 | Loss: 0.00026822
Iteration 6/25 | Loss: 0.00026822
Iteration 7/25 | Loss: 0.00026822
Iteration 8/25 | Loss: 0.00026822
Iteration 9/25 | Loss: 0.00026822
Iteration 10/25 | Loss: 0.00026822
Iteration 11/25 | Loss: 0.00026822
Iteration 12/25 | Loss: 0.00026822
Iteration 13/25 | Loss: 0.00026822
Iteration 14/25 | Loss: 0.00026822
Iteration 15/25 | Loss: 0.00026822
Iteration 16/25 | Loss: 0.00026822
Iteration 17/25 | Loss: 0.00026822
Iteration 18/25 | Loss: 0.00026822
Iteration 19/25 | Loss: 0.00026822
Iteration 20/25 | Loss: 0.00026822
Iteration 21/25 | Loss: 0.00026822
Iteration 22/25 | Loss: 0.00026822
Iteration 23/25 | Loss: 0.00026822
Iteration 24/25 | Loss: 0.00026822
Iteration 25/25 | Loss: 0.00026822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026822
Iteration 2/1000 | Loss: 0.00002441
Iteration 3/1000 | Loss: 0.00001599
Iteration 4/1000 | Loss: 0.00001490
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001376
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001315
Iteration 12/1000 | Loss: 0.00001314
Iteration 13/1000 | Loss: 0.00001314
Iteration 14/1000 | Loss: 0.00001313
Iteration 15/1000 | Loss: 0.00001311
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001301
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001300
Iteration 24/1000 | Loss: 0.00001300
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001298
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001296
Iteration 30/1000 | Loss: 0.00001296
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001290
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00001288
Iteration 55/1000 | Loss: 0.00001287
Iteration 56/1000 | Loss: 0.00001287
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001283
Iteration 65/1000 | Loss: 0.00001283
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001283
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001282
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001282
Iteration 73/1000 | Loss: 0.00001282
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001280
Iteration 78/1000 | Loss: 0.00001280
Iteration 79/1000 | Loss: 0.00001280
Iteration 80/1000 | Loss: 0.00001280
Iteration 81/1000 | Loss: 0.00001280
Iteration 82/1000 | Loss: 0.00001280
Iteration 83/1000 | Loss: 0.00001280
Iteration 84/1000 | Loss: 0.00001280
Iteration 85/1000 | Loss: 0.00001279
Iteration 86/1000 | Loss: 0.00001279
Iteration 87/1000 | Loss: 0.00001279
Iteration 88/1000 | Loss: 0.00001279
Iteration 89/1000 | Loss: 0.00001278
Iteration 90/1000 | Loss: 0.00001278
Iteration 91/1000 | Loss: 0.00001278
Iteration 92/1000 | Loss: 0.00001277
Iteration 93/1000 | Loss: 0.00001277
Iteration 94/1000 | Loss: 0.00001277
Iteration 95/1000 | Loss: 0.00001277
Iteration 96/1000 | Loss: 0.00001276
Iteration 97/1000 | Loss: 0.00001276
Iteration 98/1000 | Loss: 0.00001276
Iteration 99/1000 | Loss: 0.00001276
Iteration 100/1000 | Loss: 0.00001276
Iteration 101/1000 | Loss: 0.00001276
Iteration 102/1000 | Loss: 0.00001276
Iteration 103/1000 | Loss: 0.00001276
Iteration 104/1000 | Loss: 0.00001275
Iteration 105/1000 | Loss: 0.00001275
Iteration 106/1000 | Loss: 0.00001275
Iteration 107/1000 | Loss: 0.00001275
Iteration 108/1000 | Loss: 0.00001274
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001273
Iteration 113/1000 | Loss: 0.00001273
Iteration 114/1000 | Loss: 0.00001273
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001271
Iteration 120/1000 | Loss: 0.00001271
Iteration 121/1000 | Loss: 0.00001271
Iteration 122/1000 | Loss: 0.00001270
Iteration 123/1000 | Loss: 0.00001270
Iteration 124/1000 | Loss: 0.00001270
Iteration 125/1000 | Loss: 0.00001270
Iteration 126/1000 | Loss: 0.00001269
Iteration 127/1000 | Loss: 0.00001269
Iteration 128/1000 | Loss: 0.00001269
Iteration 129/1000 | Loss: 0.00001269
Iteration 130/1000 | Loss: 0.00001268
Iteration 131/1000 | Loss: 0.00001268
Iteration 132/1000 | Loss: 0.00001268
Iteration 133/1000 | Loss: 0.00001268
Iteration 134/1000 | Loss: 0.00001268
Iteration 135/1000 | Loss: 0.00001268
Iteration 136/1000 | Loss: 0.00001268
Iteration 137/1000 | Loss: 0.00001267
Iteration 138/1000 | Loss: 0.00001267
Iteration 139/1000 | Loss: 0.00001267
Iteration 140/1000 | Loss: 0.00001267
Iteration 141/1000 | Loss: 0.00001267
Iteration 142/1000 | Loss: 0.00001267
Iteration 143/1000 | Loss: 0.00001266
Iteration 144/1000 | Loss: 0.00001266
Iteration 145/1000 | Loss: 0.00001266
Iteration 146/1000 | Loss: 0.00001266
Iteration 147/1000 | Loss: 0.00001266
Iteration 148/1000 | Loss: 0.00001266
Iteration 149/1000 | Loss: 0.00001266
Iteration 150/1000 | Loss: 0.00001266
Iteration 151/1000 | Loss: 0.00001266
Iteration 152/1000 | Loss: 0.00001266
Iteration 153/1000 | Loss: 0.00001266
Iteration 154/1000 | Loss: 0.00001266
Iteration 155/1000 | Loss: 0.00001266
Iteration 156/1000 | Loss: 0.00001266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2658299965551123e-05, 1.2658299965551123e-05, 1.2658299965551123e-05, 1.2658299965551123e-05, 1.2658299965551123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2658299965551123e-05

Optimization complete. Final v2v error: 2.9946188926696777 mm

Highest mean error: 3.7890801429748535 mm for frame 92

Lowest mean error: 2.6333847045898438 mm for frame 128

Saving results

Total time: 34.87330746650696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094931
Iteration 2/25 | Loss: 0.00232194
Iteration 3/25 | Loss: 0.00153412
Iteration 4/25 | Loss: 0.00127499
Iteration 5/25 | Loss: 0.00117477
Iteration 6/25 | Loss: 0.00105962
Iteration 7/25 | Loss: 0.00103339
Iteration 8/25 | Loss: 0.00100541
Iteration 9/25 | Loss: 0.00095911
Iteration 10/25 | Loss: 0.00088726
Iteration 11/25 | Loss: 0.00085322
Iteration 12/25 | Loss: 0.00083752
Iteration 13/25 | Loss: 0.00082534
Iteration 14/25 | Loss: 0.00081861
Iteration 15/25 | Loss: 0.00081781
Iteration 16/25 | Loss: 0.00081248
Iteration 17/25 | Loss: 0.00081307
Iteration 18/25 | Loss: 0.00081136
Iteration 19/25 | Loss: 0.00081084
Iteration 20/25 | Loss: 0.00080581
Iteration 21/25 | Loss: 0.00080326
Iteration 22/25 | Loss: 0.00080242
Iteration 23/25 | Loss: 0.00080036
Iteration 24/25 | Loss: 0.00080243
Iteration 25/25 | Loss: 0.00080078

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48792386
Iteration 2/25 | Loss: 0.00109005
Iteration 3/25 | Loss: 0.00090778
Iteration 4/25 | Loss: 0.00090778
Iteration 5/25 | Loss: 0.00090778
Iteration 6/25 | Loss: 0.00090778
Iteration 7/25 | Loss: 0.00090778
Iteration 8/25 | Loss: 0.00090778
Iteration 9/25 | Loss: 0.00090778
Iteration 10/25 | Loss: 0.00090778
Iteration 11/25 | Loss: 0.00090778
Iteration 12/25 | Loss: 0.00090778
Iteration 13/25 | Loss: 0.00090778
Iteration 14/25 | Loss: 0.00090778
Iteration 15/25 | Loss: 0.00090778
Iteration 16/25 | Loss: 0.00090778
Iteration 17/25 | Loss: 0.00090778
Iteration 18/25 | Loss: 0.00090778
Iteration 19/25 | Loss: 0.00090778
Iteration 20/25 | Loss: 0.00090778
Iteration 21/25 | Loss: 0.00090778
Iteration 22/25 | Loss: 0.00090778
Iteration 23/25 | Loss: 0.00090778
Iteration 24/25 | Loss: 0.00090778
Iteration 25/25 | Loss: 0.00090778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090778
Iteration 2/1000 | Loss: 0.00063571
Iteration 3/1000 | Loss: 0.00023752
Iteration 4/1000 | Loss: 0.00032843
Iteration 5/1000 | Loss: 0.00019909
Iteration 6/1000 | Loss: 0.00018181
Iteration 7/1000 | Loss: 0.00037154
Iteration 8/1000 | Loss: 0.00190270
Iteration 9/1000 | Loss: 0.00124693
Iteration 10/1000 | Loss: 0.00123401
Iteration 11/1000 | Loss: 0.00138034
Iteration 12/1000 | Loss: 0.00154428
Iteration 13/1000 | Loss: 0.00102756
Iteration 14/1000 | Loss: 0.00061973
Iteration 15/1000 | Loss: 0.00059032
Iteration 16/1000 | Loss: 0.00016824
Iteration 17/1000 | Loss: 0.00013257
Iteration 18/1000 | Loss: 0.00010796
Iteration 19/1000 | Loss: 0.00006540
Iteration 20/1000 | Loss: 0.00013209
Iteration 21/1000 | Loss: 0.00006425
Iteration 22/1000 | Loss: 0.00042799
Iteration 23/1000 | Loss: 0.00113467
Iteration 24/1000 | Loss: 0.00028242
Iteration 25/1000 | Loss: 0.00052125
Iteration 26/1000 | Loss: 0.00018979
Iteration 27/1000 | Loss: 0.00025586
Iteration 28/1000 | Loss: 0.00005805
Iteration 29/1000 | Loss: 0.00017483
Iteration 30/1000 | Loss: 0.00008414
Iteration 31/1000 | Loss: 0.00009767
Iteration 32/1000 | Loss: 0.00004428
Iteration 33/1000 | Loss: 0.00014425
Iteration 34/1000 | Loss: 0.00013227
Iteration 35/1000 | Loss: 0.00020521
Iteration 36/1000 | Loss: 0.00014759
Iteration 37/1000 | Loss: 0.00012156
Iteration 38/1000 | Loss: 0.00015808
Iteration 39/1000 | Loss: 0.00013378
Iteration 40/1000 | Loss: 0.00005773
Iteration 41/1000 | Loss: 0.00011388
Iteration 42/1000 | Loss: 0.00062471
Iteration 43/1000 | Loss: 0.00010753
Iteration 44/1000 | Loss: 0.00009099
Iteration 45/1000 | Loss: 0.00008509
Iteration 46/1000 | Loss: 0.00008771
Iteration 47/1000 | Loss: 0.00005489
Iteration 48/1000 | Loss: 0.00014360
Iteration 49/1000 | Loss: 0.00014724
Iteration 50/1000 | Loss: 0.00014654
Iteration 51/1000 | Loss: 0.00005235
Iteration 52/1000 | Loss: 0.00003491
Iteration 53/1000 | Loss: 0.00153829
Iteration 54/1000 | Loss: 0.00049634
Iteration 55/1000 | Loss: 0.00078967
Iteration 56/1000 | Loss: 0.00034455
Iteration 57/1000 | Loss: 0.00017905
Iteration 58/1000 | Loss: 0.00011412
Iteration 59/1000 | Loss: 0.00014065
Iteration 60/1000 | Loss: 0.00004821
Iteration 61/1000 | Loss: 0.00004271
Iteration 62/1000 | Loss: 0.00010930
Iteration 63/1000 | Loss: 0.00055757
Iteration 64/1000 | Loss: 0.00022177
Iteration 65/1000 | Loss: 0.00013341
Iteration 66/1000 | Loss: 0.00026909
Iteration 67/1000 | Loss: 0.00016181
Iteration 68/1000 | Loss: 0.00021559
Iteration 69/1000 | Loss: 0.00010605
Iteration 70/1000 | Loss: 0.00009315
Iteration 71/1000 | Loss: 0.00006231
Iteration 72/1000 | Loss: 0.00015069
Iteration 73/1000 | Loss: 0.00010755
Iteration 74/1000 | Loss: 0.00010941
Iteration 75/1000 | Loss: 0.00019991
Iteration 76/1000 | Loss: 0.00028568
Iteration 77/1000 | Loss: 0.00115577
Iteration 78/1000 | Loss: 0.00019671
Iteration 79/1000 | Loss: 0.00011560
Iteration 80/1000 | Loss: 0.00003866
Iteration 81/1000 | Loss: 0.00010241
Iteration 82/1000 | Loss: 0.00007732
Iteration 83/1000 | Loss: 0.00010211
Iteration 84/1000 | Loss: 0.00047688
Iteration 85/1000 | Loss: 0.00040062
Iteration 86/1000 | Loss: 0.00016358
Iteration 87/1000 | Loss: 0.00016623
Iteration 88/1000 | Loss: 0.00013129
Iteration 89/1000 | Loss: 0.00017462
Iteration 90/1000 | Loss: 0.00003681
Iteration 91/1000 | Loss: 0.00003212
Iteration 92/1000 | Loss: 0.00003009
Iteration 93/1000 | Loss: 0.00015039
Iteration 94/1000 | Loss: 0.00003787
Iteration 95/1000 | Loss: 0.00022003
Iteration 96/1000 | Loss: 0.00021525
Iteration 97/1000 | Loss: 0.00003271
Iteration 98/1000 | Loss: 0.00003069
Iteration 99/1000 | Loss: 0.00024167
Iteration 100/1000 | Loss: 0.00004077
Iteration 101/1000 | Loss: 0.00003407
Iteration 102/1000 | Loss: 0.00003098
Iteration 103/1000 | Loss: 0.00002924
Iteration 104/1000 | Loss: 0.00002770
Iteration 105/1000 | Loss: 0.00002704
Iteration 106/1000 | Loss: 0.00002645
Iteration 107/1000 | Loss: 0.00002590
Iteration 108/1000 | Loss: 0.00059139
Iteration 109/1000 | Loss: 0.00003046
Iteration 110/1000 | Loss: 0.00002630
Iteration 111/1000 | Loss: 0.00002504
Iteration 112/1000 | Loss: 0.00002418
Iteration 113/1000 | Loss: 0.00002358
Iteration 114/1000 | Loss: 0.00002314
Iteration 115/1000 | Loss: 0.00002289
Iteration 116/1000 | Loss: 0.00002271
Iteration 117/1000 | Loss: 0.00002252
Iteration 118/1000 | Loss: 0.00002246
Iteration 119/1000 | Loss: 0.00002242
Iteration 120/1000 | Loss: 0.00002241
Iteration 121/1000 | Loss: 0.00002238
Iteration 122/1000 | Loss: 0.00002237
Iteration 123/1000 | Loss: 0.00002237
Iteration 124/1000 | Loss: 0.00002237
Iteration 125/1000 | Loss: 0.00002236
Iteration 126/1000 | Loss: 0.00002235
Iteration 127/1000 | Loss: 0.00002234
Iteration 128/1000 | Loss: 0.00002233
Iteration 129/1000 | Loss: 0.00002232
Iteration 130/1000 | Loss: 0.00002232
Iteration 131/1000 | Loss: 0.00002231
Iteration 132/1000 | Loss: 0.00002231
Iteration 133/1000 | Loss: 0.00002230
Iteration 134/1000 | Loss: 0.00002230
Iteration 135/1000 | Loss: 0.00002230
Iteration 136/1000 | Loss: 0.00002230
Iteration 137/1000 | Loss: 0.00002230
Iteration 138/1000 | Loss: 0.00002230
Iteration 139/1000 | Loss: 0.00002230
Iteration 140/1000 | Loss: 0.00002230
Iteration 141/1000 | Loss: 0.00002227
Iteration 142/1000 | Loss: 0.00002227
Iteration 143/1000 | Loss: 0.00002226
Iteration 144/1000 | Loss: 0.00002226
Iteration 145/1000 | Loss: 0.00002225
Iteration 146/1000 | Loss: 0.00002225
Iteration 147/1000 | Loss: 0.00002225
Iteration 148/1000 | Loss: 0.00002224
Iteration 149/1000 | Loss: 0.00002224
Iteration 150/1000 | Loss: 0.00002224
Iteration 151/1000 | Loss: 0.00002223
Iteration 152/1000 | Loss: 0.00002223
Iteration 153/1000 | Loss: 0.00002223
Iteration 154/1000 | Loss: 0.00002223
Iteration 155/1000 | Loss: 0.00002222
Iteration 156/1000 | Loss: 0.00002222
Iteration 157/1000 | Loss: 0.00002222
Iteration 158/1000 | Loss: 0.00002222
Iteration 159/1000 | Loss: 0.00002221
Iteration 160/1000 | Loss: 0.00002221
Iteration 161/1000 | Loss: 0.00002221
Iteration 162/1000 | Loss: 0.00002221
Iteration 163/1000 | Loss: 0.00002221
Iteration 164/1000 | Loss: 0.00002221
Iteration 165/1000 | Loss: 0.00002221
Iteration 166/1000 | Loss: 0.00002221
Iteration 167/1000 | Loss: 0.00002220
Iteration 168/1000 | Loss: 0.00002220
Iteration 169/1000 | Loss: 0.00002220
Iteration 170/1000 | Loss: 0.00002220
Iteration 171/1000 | Loss: 0.00002220
Iteration 172/1000 | Loss: 0.00002220
Iteration 173/1000 | Loss: 0.00002220
Iteration 174/1000 | Loss: 0.00002220
Iteration 175/1000 | Loss: 0.00002219
Iteration 176/1000 | Loss: 0.00002219
Iteration 177/1000 | Loss: 0.00002219
Iteration 178/1000 | Loss: 0.00002219
Iteration 179/1000 | Loss: 0.00002219
Iteration 180/1000 | Loss: 0.00002219
Iteration 181/1000 | Loss: 0.00002219
Iteration 182/1000 | Loss: 0.00002219
Iteration 183/1000 | Loss: 0.00002219
Iteration 184/1000 | Loss: 0.00002218
Iteration 185/1000 | Loss: 0.00002218
Iteration 186/1000 | Loss: 0.00002218
Iteration 187/1000 | Loss: 0.00002218
Iteration 188/1000 | Loss: 0.00002218
Iteration 189/1000 | Loss: 0.00002218
Iteration 190/1000 | Loss: 0.00002218
Iteration 191/1000 | Loss: 0.00002218
Iteration 192/1000 | Loss: 0.00002218
Iteration 193/1000 | Loss: 0.00002218
Iteration 194/1000 | Loss: 0.00002218
Iteration 195/1000 | Loss: 0.00002218
Iteration 196/1000 | Loss: 0.00002218
Iteration 197/1000 | Loss: 0.00002218
Iteration 198/1000 | Loss: 0.00002218
Iteration 199/1000 | Loss: 0.00002217
Iteration 200/1000 | Loss: 0.00002217
Iteration 201/1000 | Loss: 0.00002217
Iteration 202/1000 | Loss: 0.00002217
Iteration 203/1000 | Loss: 0.00002217
Iteration 204/1000 | Loss: 0.00002217
Iteration 205/1000 | Loss: 0.00002217
Iteration 206/1000 | Loss: 0.00002217
Iteration 207/1000 | Loss: 0.00002217
Iteration 208/1000 | Loss: 0.00002217
Iteration 209/1000 | Loss: 0.00002217
Iteration 210/1000 | Loss: 0.00002217
Iteration 211/1000 | Loss: 0.00002217
Iteration 212/1000 | Loss: 0.00002217
Iteration 213/1000 | Loss: 0.00002217
Iteration 214/1000 | Loss: 0.00002217
Iteration 215/1000 | Loss: 0.00002217
Iteration 216/1000 | Loss: 0.00002217
Iteration 217/1000 | Loss: 0.00002216
Iteration 218/1000 | Loss: 0.00002216
Iteration 219/1000 | Loss: 0.00002216
Iteration 220/1000 | Loss: 0.00002216
Iteration 221/1000 | Loss: 0.00002216
Iteration 222/1000 | Loss: 0.00002216
Iteration 223/1000 | Loss: 0.00002216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.2164993424667045e-05, 2.2164993424667045e-05, 2.2164993424667045e-05, 2.2164993424667045e-05, 2.2164993424667045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2164993424667045e-05

Optimization complete. Final v2v error: 3.898357391357422 mm

Highest mean error: 8.76207160949707 mm for frame 119

Lowest mean error: 3.309459686279297 mm for frame 170

Saving results

Total time: 244.3212113380432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904374
Iteration 2/25 | Loss: 0.00130991
Iteration 3/25 | Loss: 0.00100711
Iteration 4/25 | Loss: 0.00092449
Iteration 5/25 | Loss: 0.00088965
Iteration 6/25 | Loss: 0.00088048
Iteration 7/25 | Loss: 0.00088770
Iteration 8/25 | Loss: 0.00088326
Iteration 9/25 | Loss: 0.00086753
Iteration 10/25 | Loss: 0.00084895
Iteration 11/25 | Loss: 0.00087088
Iteration 12/25 | Loss: 0.00085777
Iteration 13/25 | Loss: 0.00082816
Iteration 14/25 | Loss: 0.00082256
Iteration 15/25 | Loss: 0.00081675
Iteration 16/25 | Loss: 0.00082465
Iteration 17/25 | Loss: 0.00081078
Iteration 18/25 | Loss: 0.00082118
Iteration 19/25 | Loss: 0.00081066
Iteration 20/25 | Loss: 0.00081316
Iteration 21/25 | Loss: 0.00081436
Iteration 22/25 | Loss: 0.00080573
Iteration 23/25 | Loss: 0.00080234
Iteration 24/25 | Loss: 0.00081177
Iteration 25/25 | Loss: 0.00080651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47065353
Iteration 2/25 | Loss: 0.00132226
Iteration 3/25 | Loss: 0.00132226
Iteration 4/25 | Loss: 0.00132226
Iteration 5/25 | Loss: 0.00132226
Iteration 6/25 | Loss: 0.00132225
Iteration 7/25 | Loss: 0.00132225
Iteration 8/25 | Loss: 0.00132225
Iteration 9/25 | Loss: 0.00132225
Iteration 10/25 | Loss: 0.00132225
Iteration 11/25 | Loss: 0.00132225
Iteration 12/25 | Loss: 0.00132225
Iteration 13/25 | Loss: 0.00132225
Iteration 14/25 | Loss: 0.00132225
Iteration 15/25 | Loss: 0.00132225
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013222536072134972, 0.0013222536072134972, 0.0013222536072134972, 0.0013222536072134972, 0.0013222536072134972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013222536072134972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132225
Iteration 2/1000 | Loss: 0.00145773
Iteration 3/1000 | Loss: 0.00058593
Iteration 4/1000 | Loss: 0.00043851
Iteration 5/1000 | Loss: 0.00048414
Iteration 6/1000 | Loss: 0.00056263
Iteration 7/1000 | Loss: 0.00062607
Iteration 8/1000 | Loss: 0.00062135
Iteration 9/1000 | Loss: 0.00076506
Iteration 10/1000 | Loss: 0.00060346
Iteration 11/1000 | Loss: 0.00064785
Iteration 12/1000 | Loss: 0.00069817
Iteration 13/1000 | Loss: 0.00086980
Iteration 14/1000 | Loss: 0.00048496
Iteration 15/1000 | Loss: 0.00059413
Iteration 16/1000 | Loss: 0.00038810
Iteration 17/1000 | Loss: 0.00029959
Iteration 18/1000 | Loss: 0.00091178
Iteration 19/1000 | Loss: 0.00063812
Iteration 20/1000 | Loss: 0.00108525
Iteration 21/1000 | Loss: 0.00063498
Iteration 22/1000 | Loss: 0.00074013
Iteration 23/1000 | Loss: 0.00059464
Iteration 24/1000 | Loss: 0.00072767
Iteration 25/1000 | Loss: 0.00064364
Iteration 26/1000 | Loss: 0.00067237
Iteration 27/1000 | Loss: 0.00065551
Iteration 28/1000 | Loss: 0.00123696
Iteration 29/1000 | Loss: 0.00157966
Iteration 30/1000 | Loss: 0.00066760
Iteration 31/1000 | Loss: 0.00074024
Iteration 32/1000 | Loss: 0.00154905
Iteration 33/1000 | Loss: 0.00101043
Iteration 34/1000 | Loss: 0.00064825
Iteration 35/1000 | Loss: 0.00064685
Iteration 36/1000 | Loss: 0.00173966
Iteration 37/1000 | Loss: 0.00100757
Iteration 38/1000 | Loss: 0.00069744
Iteration 39/1000 | Loss: 0.00015774
Iteration 40/1000 | Loss: 0.00020064
Iteration 41/1000 | Loss: 0.00038390
Iteration 42/1000 | Loss: 0.00019778
Iteration 43/1000 | Loss: 0.00015151
Iteration 44/1000 | Loss: 0.00019306
Iteration 45/1000 | Loss: 0.00036539
Iteration 46/1000 | Loss: 0.00040357
Iteration 47/1000 | Loss: 0.00095662
Iteration 48/1000 | Loss: 0.00017124
Iteration 49/1000 | Loss: 0.00007691
Iteration 50/1000 | Loss: 0.00085537
Iteration 51/1000 | Loss: 0.00080349
Iteration 52/1000 | Loss: 0.00053402
Iteration 53/1000 | Loss: 0.00034692
Iteration 54/1000 | Loss: 0.00129960
Iteration 55/1000 | Loss: 0.00096275
Iteration 56/1000 | Loss: 0.00058489
Iteration 57/1000 | Loss: 0.00080194
Iteration 58/1000 | Loss: 0.00079956
Iteration 59/1000 | Loss: 0.00088076
Iteration 60/1000 | Loss: 0.00035483
Iteration 61/1000 | Loss: 0.00069813
Iteration 62/1000 | Loss: 0.00107558
Iteration 63/1000 | Loss: 0.00037262
Iteration 64/1000 | Loss: 0.00021567
Iteration 65/1000 | Loss: 0.00032120
Iteration 66/1000 | Loss: 0.00028184
Iteration 67/1000 | Loss: 0.00034173
Iteration 68/1000 | Loss: 0.00023030
Iteration 69/1000 | Loss: 0.00022666
Iteration 70/1000 | Loss: 0.00024095
Iteration 71/1000 | Loss: 0.00044921
Iteration 72/1000 | Loss: 0.00047695
Iteration 73/1000 | Loss: 0.00069202
Iteration 74/1000 | Loss: 0.00019687
Iteration 75/1000 | Loss: 0.00008080
Iteration 76/1000 | Loss: 0.00038528
Iteration 77/1000 | Loss: 0.00048343
Iteration 78/1000 | Loss: 0.00008301
Iteration 79/1000 | Loss: 0.00092985
Iteration 80/1000 | Loss: 0.00040623
Iteration 81/1000 | Loss: 0.00051622
Iteration 82/1000 | Loss: 0.00054275
Iteration 83/1000 | Loss: 0.00028912
Iteration 84/1000 | Loss: 0.00026620
Iteration 85/1000 | Loss: 0.00056310
Iteration 86/1000 | Loss: 0.00055321
Iteration 87/1000 | Loss: 0.00042339
Iteration 88/1000 | Loss: 0.00119622
Iteration 89/1000 | Loss: 0.00145945
Iteration 90/1000 | Loss: 0.00091372
Iteration 91/1000 | Loss: 0.00037803
Iteration 92/1000 | Loss: 0.00130553
Iteration 93/1000 | Loss: 0.00055479
Iteration 94/1000 | Loss: 0.00101942
Iteration 95/1000 | Loss: 0.00084198
Iteration 96/1000 | Loss: 0.00025742
Iteration 97/1000 | Loss: 0.00012909
Iteration 98/1000 | Loss: 0.00038280
Iteration 99/1000 | Loss: 0.00123425
Iteration 100/1000 | Loss: 0.00089372
Iteration 101/1000 | Loss: 0.00047229
Iteration 102/1000 | Loss: 0.00016962
Iteration 103/1000 | Loss: 0.00008754
Iteration 104/1000 | Loss: 0.00025983
Iteration 105/1000 | Loss: 0.00164625
Iteration 106/1000 | Loss: 0.00122186
Iteration 107/1000 | Loss: 0.00077929
Iteration 108/1000 | Loss: 0.00032443
Iteration 109/1000 | Loss: 0.00035969
Iteration 110/1000 | Loss: 0.00018538
Iteration 111/1000 | Loss: 0.00030866
Iteration 112/1000 | Loss: 0.00030375
Iteration 113/1000 | Loss: 0.00022562
Iteration 114/1000 | Loss: 0.00025452
Iteration 115/1000 | Loss: 0.00033984
Iteration 116/1000 | Loss: 0.00032859
Iteration 117/1000 | Loss: 0.00018961
Iteration 118/1000 | Loss: 0.00014461
Iteration 119/1000 | Loss: 0.00017622
Iteration 120/1000 | Loss: 0.00023179
Iteration 121/1000 | Loss: 0.00019249
Iteration 122/1000 | Loss: 0.00020779
Iteration 123/1000 | Loss: 0.00006035
Iteration 124/1000 | Loss: 0.00021455
Iteration 125/1000 | Loss: 0.00005098
Iteration 126/1000 | Loss: 0.00004749
Iteration 127/1000 | Loss: 0.00004429
Iteration 128/1000 | Loss: 0.00004598
Iteration 129/1000 | Loss: 0.00039235
Iteration 130/1000 | Loss: 0.00022135
Iteration 131/1000 | Loss: 0.00022280
Iteration 132/1000 | Loss: 0.00009241
Iteration 133/1000 | Loss: 0.00018525
Iteration 134/1000 | Loss: 0.00007848
Iteration 135/1000 | Loss: 0.00020040
Iteration 136/1000 | Loss: 0.00008332
Iteration 137/1000 | Loss: 0.00019150
Iteration 138/1000 | Loss: 0.00008628
Iteration 139/1000 | Loss: 0.00016359
Iteration 140/1000 | Loss: 0.00008058
Iteration 141/1000 | Loss: 0.00016774
Iteration 142/1000 | Loss: 0.00007995
Iteration 143/1000 | Loss: 0.00003853
Iteration 144/1000 | Loss: 0.00016509
Iteration 145/1000 | Loss: 0.00009788
Iteration 146/1000 | Loss: 0.00018449
Iteration 147/1000 | Loss: 0.00005077
Iteration 148/1000 | Loss: 0.00011442
Iteration 149/1000 | Loss: 0.00011883
Iteration 150/1000 | Loss: 0.00010825
Iteration 151/1000 | Loss: 0.00008084
Iteration 152/1000 | Loss: 0.00004133
Iteration 153/1000 | Loss: 0.00003935
Iteration 154/1000 | Loss: 0.00016893
Iteration 155/1000 | Loss: 0.00019283
Iteration 156/1000 | Loss: 0.00100785
Iteration 157/1000 | Loss: 0.00036938
Iteration 158/1000 | Loss: 0.00029919
Iteration 159/1000 | Loss: 0.00023147
Iteration 160/1000 | Loss: 0.00044090
Iteration 161/1000 | Loss: 0.00032108
Iteration 162/1000 | Loss: 0.00024347
Iteration 163/1000 | Loss: 0.00018029
Iteration 164/1000 | Loss: 0.00007698
Iteration 165/1000 | Loss: 0.00024558
Iteration 166/1000 | Loss: 0.00018820
Iteration 167/1000 | Loss: 0.00014637
Iteration 168/1000 | Loss: 0.00018515
Iteration 169/1000 | Loss: 0.00017581
Iteration 170/1000 | Loss: 0.00016660
Iteration 171/1000 | Loss: 0.00008776
Iteration 172/1000 | Loss: 0.00020055
Iteration 173/1000 | Loss: 0.00019761
Iteration 174/1000 | Loss: 0.00021206
Iteration 175/1000 | Loss: 0.00020137
Iteration 176/1000 | Loss: 0.00015739
Iteration 177/1000 | Loss: 0.00019808
Iteration 178/1000 | Loss: 0.00015748
Iteration 179/1000 | Loss: 0.00034382
Iteration 180/1000 | Loss: 0.00033050
Iteration 181/1000 | Loss: 0.00004065
Iteration 182/1000 | Loss: 0.00061405
Iteration 183/1000 | Loss: 0.00003887
Iteration 184/1000 | Loss: 0.00004901
Iteration 185/1000 | Loss: 0.00003642
Iteration 186/1000 | Loss: 0.00021220
Iteration 187/1000 | Loss: 0.00018863
Iteration 188/1000 | Loss: 0.00017581
Iteration 189/1000 | Loss: 0.00021805
Iteration 190/1000 | Loss: 0.00017086
Iteration 191/1000 | Loss: 0.00019056
Iteration 192/1000 | Loss: 0.00015599
Iteration 193/1000 | Loss: 0.00003475
Iteration 194/1000 | Loss: 0.00003316
Iteration 195/1000 | Loss: 0.00003133
Iteration 196/1000 | Loss: 0.00002933
Iteration 197/1000 | Loss: 0.00002809
Iteration 198/1000 | Loss: 0.00002738
Iteration 199/1000 | Loss: 0.00002713
Iteration 200/1000 | Loss: 0.00002686
Iteration 201/1000 | Loss: 0.00002648
Iteration 202/1000 | Loss: 0.00002646
Iteration 203/1000 | Loss: 0.00017472
Iteration 204/1000 | Loss: 0.00083456
Iteration 205/1000 | Loss: 0.00097257
Iteration 206/1000 | Loss: 0.00055111
Iteration 207/1000 | Loss: 0.00059225
Iteration 208/1000 | Loss: 0.00063315
Iteration 209/1000 | Loss: 0.00052664
Iteration 210/1000 | Loss: 0.00039417
Iteration 211/1000 | Loss: 0.00038629
Iteration 212/1000 | Loss: 0.00013201
Iteration 213/1000 | Loss: 0.00021695
Iteration 214/1000 | Loss: 0.00012476
Iteration 215/1000 | Loss: 0.00013070
Iteration 216/1000 | Loss: 0.00007188
Iteration 217/1000 | Loss: 0.00020412
Iteration 218/1000 | Loss: 0.00019531
Iteration 219/1000 | Loss: 0.00012650
Iteration 220/1000 | Loss: 0.00013396
Iteration 221/1000 | Loss: 0.00014252
Iteration 222/1000 | Loss: 0.00017502
Iteration 223/1000 | Loss: 0.00020267
Iteration 224/1000 | Loss: 0.00010903
Iteration 225/1000 | Loss: 0.00006122
Iteration 226/1000 | Loss: 0.00022852
Iteration 227/1000 | Loss: 0.00019824
Iteration 228/1000 | Loss: 0.00018616
Iteration 229/1000 | Loss: 0.00017658
Iteration 230/1000 | Loss: 0.00024217
Iteration 231/1000 | Loss: 0.00017687
Iteration 232/1000 | Loss: 0.00013512
Iteration 233/1000 | Loss: 0.00009414
Iteration 234/1000 | Loss: 0.00011585
Iteration 235/1000 | Loss: 0.00021154
Iteration 236/1000 | Loss: 0.00014882
Iteration 237/1000 | Loss: 0.00012706
Iteration 238/1000 | Loss: 0.00012661
Iteration 239/1000 | Loss: 0.00005466
Iteration 240/1000 | Loss: 0.00022221
Iteration 241/1000 | Loss: 0.00016262
Iteration 242/1000 | Loss: 0.00019219
Iteration 243/1000 | Loss: 0.00035184
Iteration 244/1000 | Loss: 0.00031502
Iteration 245/1000 | Loss: 0.00022727
Iteration 246/1000 | Loss: 0.00021328
Iteration 247/1000 | Loss: 0.00011707
Iteration 248/1000 | Loss: 0.00021154
Iteration 249/1000 | Loss: 0.00031254
Iteration 250/1000 | Loss: 0.00004001
Iteration 251/1000 | Loss: 0.00035196
Iteration 252/1000 | Loss: 0.00061990
Iteration 253/1000 | Loss: 0.00056607
Iteration 254/1000 | Loss: 0.00033750
Iteration 255/1000 | Loss: 0.00126594
Iteration 256/1000 | Loss: 0.00107361
Iteration 257/1000 | Loss: 0.00015531
Iteration 258/1000 | Loss: 0.00012245
Iteration 259/1000 | Loss: 0.00015660
Iteration 260/1000 | Loss: 0.00018496
Iteration 261/1000 | Loss: 0.00004463
Iteration 262/1000 | Loss: 0.00004021
Iteration 263/1000 | Loss: 0.00003785
Iteration 264/1000 | Loss: 0.00025875
Iteration 265/1000 | Loss: 0.00029814
Iteration 266/1000 | Loss: 0.00023562
Iteration 267/1000 | Loss: 0.00005344
Iteration 268/1000 | Loss: 0.00003881
Iteration 269/1000 | Loss: 0.00003425
Iteration 270/1000 | Loss: 0.00003051
Iteration 271/1000 | Loss: 0.00002798
Iteration 272/1000 | Loss: 0.00002664
Iteration 273/1000 | Loss: 0.00002557
Iteration 274/1000 | Loss: 0.00002481
Iteration 275/1000 | Loss: 0.00002429
Iteration 276/1000 | Loss: 0.00002391
Iteration 277/1000 | Loss: 0.00002372
Iteration 278/1000 | Loss: 0.00002361
Iteration 279/1000 | Loss: 0.00002360
Iteration 280/1000 | Loss: 0.00002358
Iteration 281/1000 | Loss: 0.00002358
Iteration 282/1000 | Loss: 0.00002357
Iteration 283/1000 | Loss: 0.00002357
Iteration 284/1000 | Loss: 0.00002357
Iteration 285/1000 | Loss: 0.00002356
Iteration 286/1000 | Loss: 0.00002355
Iteration 287/1000 | Loss: 0.00002355
Iteration 288/1000 | Loss: 0.00002355
Iteration 289/1000 | Loss: 0.00002354
Iteration 290/1000 | Loss: 0.00002354
Iteration 291/1000 | Loss: 0.00002353
Iteration 292/1000 | Loss: 0.00002352
Iteration 293/1000 | Loss: 0.00002351
Iteration 294/1000 | Loss: 0.00002351
Iteration 295/1000 | Loss: 0.00002351
Iteration 296/1000 | Loss: 0.00002350
Iteration 297/1000 | Loss: 0.00002350
Iteration 298/1000 | Loss: 0.00002349
Iteration 299/1000 | Loss: 0.00002349
Iteration 300/1000 | Loss: 0.00002349
Iteration 301/1000 | Loss: 0.00002349
Iteration 302/1000 | Loss: 0.00002348
Iteration 303/1000 | Loss: 0.00002348
Iteration 304/1000 | Loss: 0.00002348
Iteration 305/1000 | Loss: 0.00002347
Iteration 306/1000 | Loss: 0.00002347
Iteration 307/1000 | Loss: 0.00002347
Iteration 308/1000 | Loss: 0.00002347
Iteration 309/1000 | Loss: 0.00002347
Iteration 310/1000 | Loss: 0.00002347
Iteration 311/1000 | Loss: 0.00002347
Iteration 312/1000 | Loss: 0.00002347
Iteration 313/1000 | Loss: 0.00002346
Iteration 314/1000 | Loss: 0.00002346
Iteration 315/1000 | Loss: 0.00002346
Iteration 316/1000 | Loss: 0.00002346
Iteration 317/1000 | Loss: 0.00002346
Iteration 318/1000 | Loss: 0.00002346
Iteration 319/1000 | Loss: 0.00002345
Iteration 320/1000 | Loss: 0.00002345
Iteration 321/1000 | Loss: 0.00002344
Iteration 322/1000 | Loss: 0.00002343
Iteration 323/1000 | Loss: 0.00002343
Iteration 324/1000 | Loss: 0.00002343
Iteration 325/1000 | Loss: 0.00002343
Iteration 326/1000 | Loss: 0.00002342
Iteration 327/1000 | Loss: 0.00002342
Iteration 328/1000 | Loss: 0.00002342
Iteration 329/1000 | Loss: 0.00002341
Iteration 330/1000 | Loss: 0.00002341
Iteration 331/1000 | Loss: 0.00002341
Iteration 332/1000 | Loss: 0.00002338
Iteration 333/1000 | Loss: 0.00002337
Iteration 334/1000 | Loss: 0.00002336
Iteration 335/1000 | Loss: 0.00002336
Iteration 336/1000 | Loss: 0.00017462
Iteration 337/1000 | Loss: 0.00011450
Iteration 338/1000 | Loss: 0.00002339
Iteration 339/1000 | Loss: 0.00002332
Iteration 340/1000 | Loss: 0.00002332
Iteration 341/1000 | Loss: 0.00002332
Iteration 342/1000 | Loss: 0.00002332
Iteration 343/1000 | Loss: 0.00002332
Iteration 344/1000 | Loss: 0.00002332
Iteration 345/1000 | Loss: 0.00002332
Iteration 346/1000 | Loss: 0.00002332
Iteration 347/1000 | Loss: 0.00002331
Iteration 348/1000 | Loss: 0.00002331
Iteration 349/1000 | Loss: 0.00002331
Iteration 350/1000 | Loss: 0.00002331
Iteration 351/1000 | Loss: 0.00002331
Iteration 352/1000 | Loss: 0.00002331
Iteration 353/1000 | Loss: 0.00017171
Iteration 354/1000 | Loss: 0.00010593
Iteration 355/1000 | Loss: 0.00002330
Iteration 356/1000 | Loss: 0.00002329
Iteration 357/1000 | Loss: 0.00002329
Iteration 358/1000 | Loss: 0.00002329
Iteration 359/1000 | Loss: 0.00002328
Iteration 360/1000 | Loss: 0.00002328
Iteration 361/1000 | Loss: 0.00002328
Iteration 362/1000 | Loss: 0.00002327
Iteration 363/1000 | Loss: 0.00002327
Iteration 364/1000 | Loss: 0.00002327
Iteration 365/1000 | Loss: 0.00002327
Iteration 366/1000 | Loss: 0.00002326
Iteration 367/1000 | Loss: 0.00002326
Iteration 368/1000 | Loss: 0.00002326
Iteration 369/1000 | Loss: 0.00002325
Iteration 370/1000 | Loss: 0.00002325
Iteration 371/1000 | Loss: 0.00002325
Iteration 372/1000 | Loss: 0.00002325
Iteration 373/1000 | Loss: 0.00002325
Iteration 374/1000 | Loss: 0.00002324
Iteration 375/1000 | Loss: 0.00002324
Iteration 376/1000 | Loss: 0.00002324
Iteration 377/1000 | Loss: 0.00002324
Iteration 378/1000 | Loss: 0.00002324
Iteration 379/1000 | Loss: 0.00002324
Iteration 380/1000 | Loss: 0.00002324
Iteration 381/1000 | Loss: 0.00002324
Iteration 382/1000 | Loss: 0.00002324
Iteration 383/1000 | Loss: 0.00002324
Iteration 384/1000 | Loss: 0.00002323
Iteration 385/1000 | Loss: 0.00002323
Iteration 386/1000 | Loss: 0.00002322
Iteration 387/1000 | Loss: 0.00002322
Iteration 388/1000 | Loss: 0.00002322
Iteration 389/1000 | Loss: 0.00017741
Iteration 390/1000 | Loss: 0.00017741
Iteration 391/1000 | Loss: 0.00009504
Iteration 392/1000 | Loss: 0.00002322
Iteration 393/1000 | Loss: 0.00002322
Iteration 394/1000 | Loss: 0.00002322
Iteration 395/1000 | Loss: 0.00002322
Iteration 396/1000 | Loss: 0.00002322
Iteration 397/1000 | Loss: 0.00002322
Iteration 398/1000 | Loss: 0.00002321
Iteration 399/1000 | Loss: 0.00002321
Iteration 400/1000 | Loss: 0.00002321
Iteration 401/1000 | Loss: 0.00002321
Iteration 402/1000 | Loss: 0.00002321
Iteration 403/1000 | Loss: 0.00002321
Iteration 404/1000 | Loss: 0.00002321
Iteration 405/1000 | Loss: 0.00002321
Iteration 406/1000 | Loss: 0.00002320
Iteration 407/1000 | Loss: 0.00002320
Iteration 408/1000 | Loss: 0.00002320
Iteration 409/1000 | Loss: 0.00002320
Iteration 410/1000 | Loss: 0.00002320
Iteration 411/1000 | Loss: 0.00002320
Iteration 412/1000 | Loss: 0.00002320
Iteration 413/1000 | Loss: 0.00002320
Iteration 414/1000 | Loss: 0.00002320
Iteration 415/1000 | Loss: 0.00002320
Iteration 416/1000 | Loss: 0.00002320
Iteration 417/1000 | Loss: 0.00002320
Iteration 418/1000 | Loss: 0.00002320
Iteration 419/1000 | Loss: 0.00002320
Iteration 420/1000 | Loss: 0.00002320
Iteration 421/1000 | Loss: 0.00002319
Iteration 422/1000 | Loss: 0.00002319
Iteration 423/1000 | Loss: 0.00002319
Iteration 424/1000 | Loss: 0.00002319
Iteration 425/1000 | Loss: 0.00002319
Iteration 426/1000 | Loss: 0.00002319
Iteration 427/1000 | Loss: 0.00002319
Iteration 428/1000 | Loss: 0.00017571
Iteration 429/1000 | Loss: 0.00011458
Iteration 430/1000 | Loss: 0.00002442
Iteration 431/1000 | Loss: 0.00002340
Iteration 432/1000 | Loss: 0.00019510
Iteration 433/1000 | Loss: 0.00011159
Iteration 434/1000 | Loss: 0.00002546
Iteration 435/1000 | Loss: 0.00002362
Iteration 436/1000 | Loss: 0.00002322
Iteration 437/1000 | Loss: 0.00002320
Iteration 438/1000 | Loss: 0.00002319
Iteration 439/1000 | Loss: 0.00002319
Iteration 440/1000 | Loss: 0.00002318
Iteration 441/1000 | Loss: 0.00002318
Iteration 442/1000 | Loss: 0.00002317
Iteration 443/1000 | Loss: 0.00002317
Iteration 444/1000 | Loss: 0.00002317
Iteration 445/1000 | Loss: 0.00002316
Iteration 446/1000 | Loss: 0.00002316
Iteration 447/1000 | Loss: 0.00002316
Iteration 448/1000 | Loss: 0.00002316
Iteration 449/1000 | Loss: 0.00002315
Iteration 450/1000 | Loss: 0.00002315
Iteration 451/1000 | Loss: 0.00002315
Iteration 452/1000 | Loss: 0.00002315
Iteration 453/1000 | Loss: 0.00002315
Iteration 454/1000 | Loss: 0.00002315
Iteration 455/1000 | Loss: 0.00002315
Iteration 456/1000 | Loss: 0.00002315
Iteration 457/1000 | Loss: 0.00002315
Iteration 458/1000 | Loss: 0.00002315
Iteration 459/1000 | Loss: 0.00002314
Iteration 460/1000 | Loss: 0.00002314
Iteration 461/1000 | Loss: 0.00002314
Iteration 462/1000 | Loss: 0.00002314
Iteration 463/1000 | Loss: 0.00017634
Iteration 464/1000 | Loss: 0.00009009
Iteration 465/1000 | Loss: 0.00002937
Iteration 466/1000 | Loss: 0.00002614
Iteration 467/1000 | Loss: 0.00002348
Iteration 468/1000 | Loss: 0.00002322
Iteration 469/1000 | Loss: 0.00002321
Iteration 470/1000 | Loss: 0.00002319
Iteration 471/1000 | Loss: 0.00002318
Iteration 472/1000 | Loss: 0.00002316
Iteration 473/1000 | Loss: 0.00002316
Iteration 474/1000 | Loss: 0.00002316
Iteration 475/1000 | Loss: 0.00002316
Iteration 476/1000 | Loss: 0.00018290
Iteration 477/1000 | Loss: 0.00012800
Iteration 478/1000 | Loss: 0.00019361
Iteration 479/1000 | Loss: 0.00012895
Iteration 480/1000 | Loss: 0.00013648
Iteration 481/1000 | Loss: 0.00015739
Iteration 482/1000 | Loss: 0.00002364
Iteration 483/1000 | Loss: 0.00002324
Iteration 484/1000 | Loss: 0.00002317
Iteration 485/1000 | Loss: 0.00002317
Iteration 486/1000 | Loss: 0.00002317
Iteration 487/1000 | Loss: 0.00002317
Iteration 488/1000 | Loss: 0.00002317
Iteration 489/1000 | Loss: 0.00002317
Iteration 490/1000 | Loss: 0.00002316
Iteration 491/1000 | Loss: 0.00002316
Iteration 492/1000 | Loss: 0.00002316
Iteration 493/1000 | Loss: 0.00002316
Iteration 494/1000 | Loss: 0.00002315
Iteration 495/1000 | Loss: 0.00002315
Iteration 496/1000 | Loss: 0.00002315
Iteration 497/1000 | Loss: 0.00002315
Iteration 498/1000 | Loss: 0.00002315
Iteration 499/1000 | Loss: 0.00002315
Iteration 500/1000 | Loss: 0.00002315
Iteration 501/1000 | Loss: 0.00002315
Iteration 502/1000 | Loss: 0.00002315
Iteration 503/1000 | Loss: 0.00002315
Iteration 504/1000 | Loss: 0.00002315
Iteration 505/1000 | Loss: 0.00002315
Iteration 506/1000 | Loss: 0.00002315
Iteration 507/1000 | Loss: 0.00002315
Iteration 508/1000 | Loss: 0.00002314
Iteration 509/1000 | Loss: 0.00002314
Iteration 510/1000 | Loss: 0.00002314
Iteration 511/1000 | Loss: 0.00002314
Iteration 512/1000 | Loss: 0.00002314
Iteration 513/1000 | Loss: 0.00002314
Iteration 514/1000 | Loss: 0.00002314
Iteration 515/1000 | Loss: 0.00002314
Iteration 516/1000 | Loss: 0.00002314
Iteration 517/1000 | Loss: 0.00002314
Iteration 518/1000 | Loss: 0.00002314
Iteration 519/1000 | Loss: 0.00002314
Iteration 520/1000 | Loss: 0.00002314
Iteration 521/1000 | Loss: 0.00002314
Iteration 522/1000 | Loss: 0.00002314
Iteration 523/1000 | Loss: 0.00002314
Iteration 524/1000 | Loss: 0.00002314
Iteration 525/1000 | Loss: 0.00002314
Iteration 526/1000 | Loss: 0.00002313
Iteration 527/1000 | Loss: 0.00002313
Iteration 528/1000 | Loss: 0.00002313
Iteration 529/1000 | Loss: 0.00002313
Iteration 530/1000 | Loss: 0.00002313
Iteration 531/1000 | Loss: 0.00002313
Iteration 532/1000 | Loss: 0.00002313
Iteration 533/1000 | Loss: 0.00002313
Iteration 534/1000 | Loss: 0.00002313
Iteration 535/1000 | Loss: 0.00002313
Iteration 536/1000 | Loss: 0.00002313
Iteration 537/1000 | Loss: 0.00002313
Iteration 538/1000 | Loss: 0.00002313
Iteration 539/1000 | Loss: 0.00002313
Iteration 540/1000 | Loss: 0.00002312
Iteration 541/1000 | Loss: 0.00002312
Iteration 542/1000 | Loss: 0.00002312
Iteration 543/1000 | Loss: 0.00002312
Iteration 544/1000 | Loss: 0.00002312
Iteration 545/1000 | Loss: 0.00002312
Iteration 546/1000 | Loss: 0.00002312
Iteration 547/1000 | Loss: 0.00002312
Iteration 548/1000 | Loss: 0.00002312
Iteration 549/1000 | Loss: 0.00002312
Iteration 550/1000 | Loss: 0.00002312
Iteration 551/1000 | Loss: 0.00002312
Iteration 552/1000 | Loss: 0.00002311
Iteration 553/1000 | Loss: 0.00002311
Iteration 554/1000 | Loss: 0.00002311
Iteration 555/1000 | Loss: 0.00002311
Iteration 556/1000 | Loss: 0.00002311
Iteration 557/1000 | Loss: 0.00002311
Iteration 558/1000 | Loss: 0.00002311
Iteration 559/1000 | Loss: 0.00002311
Iteration 560/1000 | Loss: 0.00002311
Iteration 561/1000 | Loss: 0.00002311
Iteration 562/1000 | Loss: 0.00002311
Iteration 563/1000 | Loss: 0.00002311
Iteration 564/1000 | Loss: 0.00002311
Iteration 565/1000 | Loss: 0.00002311
Iteration 566/1000 | Loss: 0.00002311
Iteration 567/1000 | Loss: 0.00002311
Iteration 568/1000 | Loss: 0.00002311
Iteration 569/1000 | Loss: 0.00002311
Iteration 570/1000 | Loss: 0.00002311
Iteration 571/1000 | Loss: 0.00002311
Iteration 572/1000 | Loss: 0.00002311
Iteration 573/1000 | Loss: 0.00002311
Iteration 574/1000 | Loss: 0.00002311
Iteration 575/1000 | Loss: 0.00002311
Iteration 576/1000 | Loss: 0.00002310
Iteration 577/1000 | Loss: 0.00002310
Iteration 578/1000 | Loss: 0.00002310
Iteration 579/1000 | Loss: 0.00002310
Iteration 580/1000 | Loss: 0.00002310
Iteration 581/1000 | Loss: 0.00002310
Iteration 582/1000 | Loss: 0.00002310
Iteration 583/1000 | Loss: 0.00002310
Iteration 584/1000 | Loss: 0.00002310
Iteration 585/1000 | Loss: 0.00002310
Iteration 586/1000 | Loss: 0.00002310
Iteration 587/1000 | Loss: 0.00002310
Iteration 588/1000 | Loss: 0.00002310
Iteration 589/1000 | Loss: 0.00002310
Iteration 590/1000 | Loss: 0.00002310
Iteration 591/1000 | Loss: 0.00002310
Iteration 592/1000 | Loss: 0.00002310
Iteration 593/1000 | Loss: 0.00002310
Iteration 594/1000 | Loss: 0.00002310
Iteration 595/1000 | Loss: 0.00002310
Iteration 596/1000 | Loss: 0.00002310
Iteration 597/1000 | Loss: 0.00002310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 597. Stopping optimization.
Last 5 losses: [2.310061790922191e-05, 2.310061790922191e-05, 2.310061790922191e-05, 2.310061790922191e-05, 2.310061790922191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.310061790922191e-05

Optimization complete. Final v2v error: 3.7845911979675293 mm

Highest mean error: 6.759904384613037 mm for frame 90

Lowest mean error: 2.9584333896636963 mm for frame 149

Saving results

Total time: 470.90172600746155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834078
Iteration 2/25 | Loss: 0.00096874
Iteration 3/25 | Loss: 0.00070840
Iteration 4/25 | Loss: 0.00064144
Iteration 5/25 | Loss: 0.00062367
Iteration 6/25 | Loss: 0.00061810
Iteration 7/25 | Loss: 0.00061668
Iteration 8/25 | Loss: 0.00061640
Iteration 9/25 | Loss: 0.00061640
Iteration 10/25 | Loss: 0.00061640
Iteration 11/25 | Loss: 0.00061640
Iteration 12/25 | Loss: 0.00061640
Iteration 13/25 | Loss: 0.00061640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006164011429063976, 0.0006164011429063976, 0.0006164011429063976, 0.0006164011429063976, 0.0006164011429063976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006164011429063976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51311874
Iteration 2/25 | Loss: 0.00038576
Iteration 3/25 | Loss: 0.00038575
Iteration 4/25 | Loss: 0.00038575
Iteration 5/25 | Loss: 0.00038575
Iteration 6/25 | Loss: 0.00038575
Iteration 7/25 | Loss: 0.00038575
Iteration 8/25 | Loss: 0.00038575
Iteration 9/25 | Loss: 0.00038575
Iteration 10/25 | Loss: 0.00038575
Iteration 11/25 | Loss: 0.00038575
Iteration 12/25 | Loss: 0.00038575
Iteration 13/25 | Loss: 0.00038575
Iteration 14/25 | Loss: 0.00038575
Iteration 15/25 | Loss: 0.00038575
Iteration 16/25 | Loss: 0.00038575
Iteration 17/25 | Loss: 0.00038575
Iteration 18/25 | Loss: 0.00038575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003857497649732977, 0.0003857497649732977, 0.0003857497649732977, 0.0003857497649732977, 0.0003857497649732977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003857497649732977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038575
Iteration 2/1000 | Loss: 0.00003121
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001507
Iteration 5/1000 | Loss: 0.00001402
Iteration 6/1000 | Loss: 0.00001334
Iteration 7/1000 | Loss: 0.00001289
Iteration 8/1000 | Loss: 0.00001256
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001199
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001180
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001174
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001168
Iteration 19/1000 | Loss: 0.00001167
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001163
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00001159
Iteration 25/1000 | Loss: 0.00001158
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001157
Iteration 28/1000 | Loss: 0.00001156
Iteration 29/1000 | Loss: 0.00001156
Iteration 30/1000 | Loss: 0.00001155
Iteration 31/1000 | Loss: 0.00001155
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001153
Iteration 36/1000 | Loss: 0.00001153
Iteration 37/1000 | Loss: 0.00001152
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001152
Iteration 40/1000 | Loss: 0.00001151
Iteration 41/1000 | Loss: 0.00001151
Iteration 42/1000 | Loss: 0.00001150
Iteration 43/1000 | Loss: 0.00001149
Iteration 44/1000 | Loss: 0.00001149
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001147
Iteration 50/1000 | Loss: 0.00001147
Iteration 51/1000 | Loss: 0.00001147
Iteration 52/1000 | Loss: 0.00001146
Iteration 53/1000 | Loss: 0.00001146
Iteration 54/1000 | Loss: 0.00001146
Iteration 55/1000 | Loss: 0.00001146
Iteration 56/1000 | Loss: 0.00001146
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001145
Iteration 59/1000 | Loss: 0.00001145
Iteration 60/1000 | Loss: 0.00001145
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001144
Iteration 63/1000 | Loss: 0.00001144
Iteration 64/1000 | Loss: 0.00001144
Iteration 65/1000 | Loss: 0.00001144
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001144
Iteration 68/1000 | Loss: 0.00001144
Iteration 69/1000 | Loss: 0.00001144
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001143
Iteration 73/1000 | Loss: 0.00001143
Iteration 74/1000 | Loss: 0.00001143
Iteration 75/1000 | Loss: 0.00001143
Iteration 76/1000 | Loss: 0.00001143
Iteration 77/1000 | Loss: 0.00001142
Iteration 78/1000 | Loss: 0.00001142
Iteration 79/1000 | Loss: 0.00001142
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001142
Iteration 84/1000 | Loss: 0.00001142
Iteration 85/1000 | Loss: 0.00001141
Iteration 86/1000 | Loss: 0.00001141
Iteration 87/1000 | Loss: 0.00001141
Iteration 88/1000 | Loss: 0.00001141
Iteration 89/1000 | Loss: 0.00001141
Iteration 90/1000 | Loss: 0.00001141
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001141
Iteration 93/1000 | Loss: 0.00001141
Iteration 94/1000 | Loss: 0.00001140
Iteration 95/1000 | Loss: 0.00001140
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001140
Iteration 101/1000 | Loss: 0.00001140
Iteration 102/1000 | Loss: 0.00001140
Iteration 103/1000 | Loss: 0.00001140
Iteration 104/1000 | Loss: 0.00001140
Iteration 105/1000 | Loss: 0.00001140
Iteration 106/1000 | Loss: 0.00001140
Iteration 107/1000 | Loss: 0.00001140
Iteration 108/1000 | Loss: 0.00001140
Iteration 109/1000 | Loss: 0.00001139
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001139
Iteration 114/1000 | Loss: 0.00001139
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001138
Iteration 124/1000 | Loss: 0.00001138
Iteration 125/1000 | Loss: 0.00001138
Iteration 126/1000 | Loss: 0.00001138
Iteration 127/1000 | Loss: 0.00001138
Iteration 128/1000 | Loss: 0.00001138
Iteration 129/1000 | Loss: 0.00001138
Iteration 130/1000 | Loss: 0.00001138
Iteration 131/1000 | Loss: 0.00001138
Iteration 132/1000 | Loss: 0.00001138
Iteration 133/1000 | Loss: 0.00001138
Iteration 134/1000 | Loss: 0.00001138
Iteration 135/1000 | Loss: 0.00001138
Iteration 136/1000 | Loss: 0.00001138
Iteration 137/1000 | Loss: 0.00001138
Iteration 138/1000 | Loss: 0.00001138
Iteration 139/1000 | Loss: 0.00001137
Iteration 140/1000 | Loss: 0.00001137
Iteration 141/1000 | Loss: 0.00001137
Iteration 142/1000 | Loss: 0.00001137
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Iteration 146/1000 | Loss: 0.00001137
Iteration 147/1000 | Loss: 0.00001137
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001136
Iteration 150/1000 | Loss: 0.00001136
Iteration 151/1000 | Loss: 0.00001136
Iteration 152/1000 | Loss: 0.00001136
Iteration 153/1000 | Loss: 0.00001136
Iteration 154/1000 | Loss: 0.00001136
Iteration 155/1000 | Loss: 0.00001136
Iteration 156/1000 | Loss: 0.00001136
Iteration 157/1000 | Loss: 0.00001135
Iteration 158/1000 | Loss: 0.00001135
Iteration 159/1000 | Loss: 0.00001135
Iteration 160/1000 | Loss: 0.00001135
Iteration 161/1000 | Loss: 0.00001134
Iteration 162/1000 | Loss: 0.00001134
Iteration 163/1000 | Loss: 0.00001134
Iteration 164/1000 | Loss: 0.00001134
Iteration 165/1000 | Loss: 0.00001134
Iteration 166/1000 | Loss: 0.00001133
Iteration 167/1000 | Loss: 0.00001133
Iteration 168/1000 | Loss: 0.00001133
Iteration 169/1000 | Loss: 0.00001133
Iteration 170/1000 | Loss: 0.00001133
Iteration 171/1000 | Loss: 0.00001132
Iteration 172/1000 | Loss: 0.00001132
Iteration 173/1000 | Loss: 0.00001132
Iteration 174/1000 | Loss: 0.00001132
Iteration 175/1000 | Loss: 0.00001132
Iteration 176/1000 | Loss: 0.00001132
Iteration 177/1000 | Loss: 0.00001132
Iteration 178/1000 | Loss: 0.00001132
Iteration 179/1000 | Loss: 0.00001132
Iteration 180/1000 | Loss: 0.00001132
Iteration 181/1000 | Loss: 0.00001132
Iteration 182/1000 | Loss: 0.00001132
Iteration 183/1000 | Loss: 0.00001131
Iteration 184/1000 | Loss: 0.00001131
Iteration 185/1000 | Loss: 0.00001131
Iteration 186/1000 | Loss: 0.00001131
Iteration 187/1000 | Loss: 0.00001131
Iteration 188/1000 | Loss: 0.00001131
Iteration 189/1000 | Loss: 0.00001131
Iteration 190/1000 | Loss: 0.00001131
Iteration 191/1000 | Loss: 0.00001131
Iteration 192/1000 | Loss: 0.00001131
Iteration 193/1000 | Loss: 0.00001131
Iteration 194/1000 | Loss: 0.00001131
Iteration 195/1000 | Loss: 0.00001131
Iteration 196/1000 | Loss: 0.00001131
Iteration 197/1000 | Loss: 0.00001131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.1314718904031906e-05, 1.1314718904031906e-05, 1.1314718904031906e-05, 1.1314718904031906e-05, 1.1314718904031906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1314718904031906e-05

Optimization complete. Final v2v error: 2.8715877532958984 mm

Highest mean error: 3.452626943588257 mm for frame 47

Lowest mean error: 2.6155850887298584 mm for frame 176

Saving results

Total time: 41.094274282455444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930965
Iteration 2/25 | Loss: 0.00136024
Iteration 3/25 | Loss: 0.00087485
Iteration 4/25 | Loss: 0.00076109
Iteration 5/25 | Loss: 0.00075554
Iteration 6/25 | Loss: 0.00075466
Iteration 7/25 | Loss: 0.00075462
Iteration 8/25 | Loss: 0.00075462
Iteration 9/25 | Loss: 0.00075462
Iteration 10/25 | Loss: 0.00075462
Iteration 11/25 | Loss: 0.00075462
Iteration 12/25 | Loss: 0.00075462
Iteration 13/25 | Loss: 0.00075462
Iteration 14/25 | Loss: 0.00075462
Iteration 15/25 | Loss: 0.00075462
Iteration 16/25 | Loss: 0.00075462
Iteration 17/25 | Loss: 0.00075462
Iteration 18/25 | Loss: 0.00075462
Iteration 19/25 | Loss: 0.00075462
Iteration 20/25 | Loss: 0.00075462
Iteration 21/25 | Loss: 0.00075462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007546215201728046, 0.0007546215201728046, 0.0007546215201728046, 0.0007546215201728046, 0.0007546215201728046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007546215201728046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45314598
Iteration 2/25 | Loss: 0.00034691
Iteration 3/25 | Loss: 0.00034686
Iteration 4/25 | Loss: 0.00034686
Iteration 5/25 | Loss: 0.00034686
Iteration 6/25 | Loss: 0.00034686
Iteration 7/25 | Loss: 0.00034686
Iteration 8/25 | Loss: 0.00034686
Iteration 9/25 | Loss: 0.00034686
Iteration 10/25 | Loss: 0.00034686
Iteration 11/25 | Loss: 0.00034686
Iteration 12/25 | Loss: 0.00034686
Iteration 13/25 | Loss: 0.00034686
Iteration 14/25 | Loss: 0.00034686
Iteration 15/25 | Loss: 0.00034686
Iteration 16/25 | Loss: 0.00034686
Iteration 17/25 | Loss: 0.00034686
Iteration 18/25 | Loss: 0.00034686
Iteration 19/25 | Loss: 0.00034686
Iteration 20/25 | Loss: 0.00034686
Iteration 21/25 | Loss: 0.00034686
Iteration 22/25 | Loss: 0.00034686
Iteration 23/25 | Loss: 0.00034686
Iteration 24/25 | Loss: 0.00034686
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003468572976998985, 0.0003468572976998985, 0.0003468572976998985, 0.0003468572976998985, 0.0003468572976998985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003468572976998985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034686
Iteration 2/1000 | Loss: 0.00004562
Iteration 3/1000 | Loss: 0.00003417
Iteration 4/1000 | Loss: 0.00002606
Iteration 5/1000 | Loss: 0.00002360
Iteration 6/1000 | Loss: 0.00002240
Iteration 7/1000 | Loss: 0.00002190
Iteration 8/1000 | Loss: 0.00002144
Iteration 9/1000 | Loss: 0.00002097
Iteration 10/1000 | Loss: 0.00002067
Iteration 11/1000 | Loss: 0.00002027
Iteration 12/1000 | Loss: 0.00002006
Iteration 13/1000 | Loss: 0.00001976
Iteration 14/1000 | Loss: 0.00001974
Iteration 15/1000 | Loss: 0.00001961
Iteration 16/1000 | Loss: 0.00001953
Iteration 17/1000 | Loss: 0.00001951
Iteration 18/1000 | Loss: 0.00001951
Iteration 19/1000 | Loss: 0.00001950
Iteration 20/1000 | Loss: 0.00001949
Iteration 21/1000 | Loss: 0.00001945
Iteration 22/1000 | Loss: 0.00001944
Iteration 23/1000 | Loss: 0.00001944
Iteration 24/1000 | Loss: 0.00001943
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001940
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001937
Iteration 32/1000 | Loss: 0.00001933
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001930
Iteration 35/1000 | Loss: 0.00001929
Iteration 36/1000 | Loss: 0.00001928
Iteration 37/1000 | Loss: 0.00001928
Iteration 38/1000 | Loss: 0.00001928
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001927
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001925
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001923
Iteration 58/1000 | Loss: 0.00001923
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001923
Iteration 61/1000 | Loss: 0.00001923
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001922
Iteration 64/1000 | Loss: 0.00001921
Iteration 65/1000 | Loss: 0.00001921
Iteration 66/1000 | Loss: 0.00001921
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001921
Iteration 69/1000 | Loss: 0.00001921
Iteration 70/1000 | Loss: 0.00001921
Iteration 71/1000 | Loss: 0.00001921
Iteration 72/1000 | Loss: 0.00001921
Iteration 73/1000 | Loss: 0.00001920
Iteration 74/1000 | Loss: 0.00001920
Iteration 75/1000 | Loss: 0.00001920
Iteration 76/1000 | Loss: 0.00001920
Iteration 77/1000 | Loss: 0.00001920
Iteration 78/1000 | Loss: 0.00001920
Iteration 79/1000 | Loss: 0.00001920
Iteration 80/1000 | Loss: 0.00001920
Iteration 81/1000 | Loss: 0.00001920
Iteration 82/1000 | Loss: 0.00001920
Iteration 83/1000 | Loss: 0.00001920
Iteration 84/1000 | Loss: 0.00001920
Iteration 85/1000 | Loss: 0.00001919
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00001919
Iteration 88/1000 | Loss: 0.00001919
Iteration 89/1000 | Loss: 0.00001919
Iteration 90/1000 | Loss: 0.00001919
Iteration 91/1000 | Loss: 0.00001919
Iteration 92/1000 | Loss: 0.00001918
Iteration 93/1000 | Loss: 0.00001918
Iteration 94/1000 | Loss: 0.00001918
Iteration 95/1000 | Loss: 0.00001918
Iteration 96/1000 | Loss: 0.00001918
Iteration 97/1000 | Loss: 0.00001918
Iteration 98/1000 | Loss: 0.00001918
Iteration 99/1000 | Loss: 0.00001918
Iteration 100/1000 | Loss: 0.00001918
Iteration 101/1000 | Loss: 0.00001918
Iteration 102/1000 | Loss: 0.00001918
Iteration 103/1000 | Loss: 0.00001918
Iteration 104/1000 | Loss: 0.00001918
Iteration 105/1000 | Loss: 0.00001918
Iteration 106/1000 | Loss: 0.00001918
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00001918
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001918
Iteration 112/1000 | Loss: 0.00001918
Iteration 113/1000 | Loss: 0.00001918
Iteration 114/1000 | Loss: 0.00001918
Iteration 115/1000 | Loss: 0.00001918
Iteration 116/1000 | Loss: 0.00001918
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001918
Iteration 124/1000 | Loss: 0.00001918
Iteration 125/1000 | Loss: 0.00001918
Iteration 126/1000 | Loss: 0.00001918
Iteration 127/1000 | Loss: 0.00001918
Iteration 128/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.9179791706847027e-05, 1.9179791706847027e-05, 1.9179791706847027e-05, 1.9179791706847027e-05, 1.9179791706847027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9179791706847027e-05

Optimization complete. Final v2v error: 3.6699562072753906 mm

Highest mean error: 3.8577463626861572 mm for frame 112

Lowest mean error: 3.526399612426758 mm for frame 71

Saving results

Total time: 36.4102885723114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817965
Iteration 2/25 | Loss: 0.00080489
Iteration 3/25 | Loss: 0.00062180
Iteration 4/25 | Loss: 0.00059676
Iteration 5/25 | Loss: 0.00059167
Iteration 6/25 | Loss: 0.00059041
Iteration 7/25 | Loss: 0.00059018
Iteration 8/25 | Loss: 0.00059018
Iteration 9/25 | Loss: 0.00059018
Iteration 10/25 | Loss: 0.00059018
Iteration 11/25 | Loss: 0.00059018
Iteration 12/25 | Loss: 0.00059018
Iteration 13/25 | Loss: 0.00059018
Iteration 14/25 | Loss: 0.00059018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005901846452616155, 0.0005901846452616155, 0.0005901846452616155, 0.0005901846452616155, 0.0005901846452616155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005901846452616155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46831191
Iteration 2/25 | Loss: 0.00030177
Iteration 3/25 | Loss: 0.00030177
Iteration 4/25 | Loss: 0.00030177
Iteration 5/25 | Loss: 0.00030177
Iteration 6/25 | Loss: 0.00030177
Iteration 7/25 | Loss: 0.00030177
Iteration 8/25 | Loss: 0.00030177
Iteration 9/25 | Loss: 0.00030177
Iteration 10/25 | Loss: 0.00030177
Iteration 11/25 | Loss: 0.00030177
Iteration 12/25 | Loss: 0.00030177
Iteration 13/25 | Loss: 0.00030177
Iteration 14/25 | Loss: 0.00030177
Iteration 15/25 | Loss: 0.00030177
Iteration 16/25 | Loss: 0.00030177
Iteration 17/25 | Loss: 0.00030177
Iteration 18/25 | Loss: 0.00030177
Iteration 19/25 | Loss: 0.00030177
Iteration 20/25 | Loss: 0.00030177
Iteration 21/25 | Loss: 0.00030177
Iteration 22/25 | Loss: 0.00030177
Iteration 23/25 | Loss: 0.00030177
Iteration 24/25 | Loss: 0.00030177
Iteration 25/25 | Loss: 0.00030177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030177
Iteration 2/1000 | Loss: 0.00001930
Iteration 3/1000 | Loss: 0.00001247
Iteration 4/1000 | Loss: 0.00001150
Iteration 5/1000 | Loss: 0.00001093
Iteration 6/1000 | Loss: 0.00001056
Iteration 7/1000 | Loss: 0.00001028
Iteration 8/1000 | Loss: 0.00001027
Iteration 9/1000 | Loss: 0.00001024
Iteration 10/1000 | Loss: 0.00001021
Iteration 11/1000 | Loss: 0.00001020
Iteration 12/1000 | Loss: 0.00001020
Iteration 13/1000 | Loss: 0.00001015
Iteration 14/1000 | Loss: 0.00001014
Iteration 15/1000 | Loss: 0.00001009
Iteration 16/1000 | Loss: 0.00001009
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001005
Iteration 20/1000 | Loss: 0.00001005
Iteration 21/1000 | Loss: 0.00001005
Iteration 22/1000 | Loss: 0.00001004
Iteration 23/1000 | Loss: 0.00001004
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00001003
Iteration 26/1000 | Loss: 0.00001002
Iteration 27/1000 | Loss: 0.00001002
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000997
Iteration 31/1000 | Loss: 0.00000997
Iteration 32/1000 | Loss: 0.00000996
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000994
Iteration 38/1000 | Loss: 0.00000994
Iteration 39/1000 | Loss: 0.00000994
Iteration 40/1000 | Loss: 0.00000994
Iteration 41/1000 | Loss: 0.00000994
Iteration 42/1000 | Loss: 0.00000994
Iteration 43/1000 | Loss: 0.00000993
Iteration 44/1000 | Loss: 0.00000993
Iteration 45/1000 | Loss: 0.00000993
Iteration 46/1000 | Loss: 0.00000993
Iteration 47/1000 | Loss: 0.00000993
Iteration 48/1000 | Loss: 0.00000992
Iteration 49/1000 | Loss: 0.00000992
Iteration 50/1000 | Loss: 0.00000992
Iteration 51/1000 | Loss: 0.00000991
Iteration 52/1000 | Loss: 0.00000991
Iteration 53/1000 | Loss: 0.00000990
Iteration 54/1000 | Loss: 0.00000990
Iteration 55/1000 | Loss: 0.00000990
Iteration 56/1000 | Loss: 0.00000990
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000990
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000990
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000989
Iteration 65/1000 | Loss: 0.00000989
Iteration 66/1000 | Loss: 0.00000988
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000987
Iteration 70/1000 | Loss: 0.00000987
Iteration 71/1000 | Loss: 0.00000987
Iteration 72/1000 | Loss: 0.00000987
Iteration 73/1000 | Loss: 0.00000986
Iteration 74/1000 | Loss: 0.00000986
Iteration 75/1000 | Loss: 0.00000986
Iteration 76/1000 | Loss: 0.00000985
Iteration 77/1000 | Loss: 0.00000984
Iteration 78/1000 | Loss: 0.00000984
Iteration 79/1000 | Loss: 0.00000983
Iteration 80/1000 | Loss: 0.00000983
Iteration 81/1000 | Loss: 0.00000983
Iteration 82/1000 | Loss: 0.00000983
Iteration 83/1000 | Loss: 0.00000983
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000982
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000981
Iteration 98/1000 | Loss: 0.00000981
Iteration 99/1000 | Loss: 0.00000981
Iteration 100/1000 | Loss: 0.00000981
Iteration 101/1000 | Loss: 0.00000981
Iteration 102/1000 | Loss: 0.00000981
Iteration 103/1000 | Loss: 0.00000981
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000980
Iteration 106/1000 | Loss: 0.00000980
Iteration 107/1000 | Loss: 0.00000980
Iteration 108/1000 | Loss: 0.00000980
Iteration 109/1000 | Loss: 0.00000980
Iteration 110/1000 | Loss: 0.00000980
Iteration 111/1000 | Loss: 0.00000980
Iteration 112/1000 | Loss: 0.00000980
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000980
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000980
Iteration 118/1000 | Loss: 0.00000980
Iteration 119/1000 | Loss: 0.00000980
Iteration 120/1000 | Loss: 0.00000979
Iteration 121/1000 | Loss: 0.00000979
Iteration 122/1000 | Loss: 0.00000979
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000979
Iteration 125/1000 | Loss: 0.00000979
Iteration 126/1000 | Loss: 0.00000979
Iteration 127/1000 | Loss: 0.00000979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [9.794774996407796e-06, 9.794774996407796e-06, 9.794774996407796e-06, 9.794774996407796e-06, 9.794774996407796e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.794774996407796e-06

Optimization complete. Final v2v error: 2.649487257003784 mm

Highest mean error: 2.8162167072296143 mm for frame 45

Lowest mean error: 2.5292234420776367 mm for frame 160

Saving results

Total time: 30.334646463394165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428140
Iteration 2/25 | Loss: 0.00084231
Iteration 3/25 | Loss: 0.00072891
Iteration 4/25 | Loss: 0.00070049
Iteration 5/25 | Loss: 0.00068869
Iteration 6/25 | Loss: 0.00068598
Iteration 7/25 | Loss: 0.00068477
Iteration 8/25 | Loss: 0.00068462
Iteration 9/25 | Loss: 0.00068462
Iteration 10/25 | Loss: 0.00068462
Iteration 11/25 | Loss: 0.00068462
Iteration 12/25 | Loss: 0.00068462
Iteration 13/25 | Loss: 0.00068462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006846162141300738, 0.0006846162141300738, 0.0006846162141300738, 0.0006846162141300738, 0.0006846162141300738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006846162141300738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85680783
Iteration 2/25 | Loss: 0.00031640
Iteration 3/25 | Loss: 0.00031640
Iteration 4/25 | Loss: 0.00031640
Iteration 5/25 | Loss: 0.00031640
Iteration 6/25 | Loss: 0.00031640
Iteration 7/25 | Loss: 0.00031640
Iteration 8/25 | Loss: 0.00031640
Iteration 9/25 | Loss: 0.00031640
Iteration 10/25 | Loss: 0.00031640
Iteration 11/25 | Loss: 0.00031640
Iteration 12/25 | Loss: 0.00031640
Iteration 13/25 | Loss: 0.00031639
Iteration 14/25 | Loss: 0.00031639
Iteration 15/25 | Loss: 0.00031639
Iteration 16/25 | Loss: 0.00031639
Iteration 17/25 | Loss: 0.00031639
Iteration 18/25 | Loss: 0.00031640
Iteration 19/25 | Loss: 0.00031639
Iteration 20/25 | Loss: 0.00031639
Iteration 21/25 | Loss: 0.00031639
Iteration 22/25 | Loss: 0.00031639
Iteration 23/25 | Loss: 0.00031639
Iteration 24/25 | Loss: 0.00031639
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003163949877489358, 0.0003163949877489358, 0.0003163949877489358, 0.0003163949877489358, 0.0003163949877489358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003163949877489358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031639
Iteration 2/1000 | Loss: 0.00002873
Iteration 3/1000 | Loss: 0.00002197
Iteration 4/1000 | Loss: 0.00002095
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001953
Iteration 7/1000 | Loss: 0.00001911
Iteration 8/1000 | Loss: 0.00001884
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001870
Iteration 11/1000 | Loss: 0.00001856
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001851
Iteration 14/1000 | Loss: 0.00001850
Iteration 15/1000 | Loss: 0.00001843
Iteration 16/1000 | Loss: 0.00001843
Iteration 17/1000 | Loss: 0.00001843
Iteration 18/1000 | Loss: 0.00001843
Iteration 19/1000 | Loss: 0.00001840
Iteration 20/1000 | Loss: 0.00001840
Iteration 21/1000 | Loss: 0.00001839
Iteration 22/1000 | Loss: 0.00001838
Iteration 23/1000 | Loss: 0.00001838
Iteration 24/1000 | Loss: 0.00001838
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001829
Iteration 27/1000 | Loss: 0.00001828
Iteration 28/1000 | Loss: 0.00001828
Iteration 29/1000 | Loss: 0.00001826
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001826
Iteration 32/1000 | Loss: 0.00001826
Iteration 33/1000 | Loss: 0.00001826
Iteration 34/1000 | Loss: 0.00001826
Iteration 35/1000 | Loss: 0.00001826
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001825
Iteration 38/1000 | Loss: 0.00001825
Iteration 39/1000 | Loss: 0.00001825
Iteration 40/1000 | Loss: 0.00001825
Iteration 41/1000 | Loss: 0.00001825
Iteration 42/1000 | Loss: 0.00001825
Iteration 43/1000 | Loss: 0.00001824
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001823
Iteration 46/1000 | Loss: 0.00001823
Iteration 47/1000 | Loss: 0.00001823
Iteration 48/1000 | Loss: 0.00001823
Iteration 49/1000 | Loss: 0.00001823
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001823
Iteration 53/1000 | Loss: 0.00001823
Iteration 54/1000 | Loss: 0.00001822
Iteration 55/1000 | Loss: 0.00001822
Iteration 56/1000 | Loss: 0.00001821
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001821
Iteration 59/1000 | Loss: 0.00001820
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001817
Iteration 64/1000 | Loss: 0.00001817
Iteration 65/1000 | Loss: 0.00001816
Iteration 66/1000 | Loss: 0.00001816
Iteration 67/1000 | Loss: 0.00001816
Iteration 68/1000 | Loss: 0.00001815
Iteration 69/1000 | Loss: 0.00001815
Iteration 70/1000 | Loss: 0.00001815
Iteration 71/1000 | Loss: 0.00001815
Iteration 72/1000 | Loss: 0.00001815
Iteration 73/1000 | Loss: 0.00001814
Iteration 74/1000 | Loss: 0.00001814
Iteration 75/1000 | Loss: 0.00001814
Iteration 76/1000 | Loss: 0.00001814
Iteration 77/1000 | Loss: 0.00001814
Iteration 78/1000 | Loss: 0.00001813
Iteration 79/1000 | Loss: 0.00001812
Iteration 80/1000 | Loss: 0.00001812
Iteration 81/1000 | Loss: 0.00001812
Iteration 82/1000 | Loss: 0.00001811
Iteration 83/1000 | Loss: 0.00001811
Iteration 84/1000 | Loss: 0.00001811
Iteration 85/1000 | Loss: 0.00001811
Iteration 86/1000 | Loss: 0.00001811
Iteration 87/1000 | Loss: 0.00001810
Iteration 88/1000 | Loss: 0.00001809
Iteration 89/1000 | Loss: 0.00001809
Iteration 90/1000 | Loss: 0.00001809
Iteration 91/1000 | Loss: 0.00001808
Iteration 92/1000 | Loss: 0.00001808
Iteration 93/1000 | Loss: 0.00001808
Iteration 94/1000 | Loss: 0.00001807
Iteration 95/1000 | Loss: 0.00001807
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001805
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001805
Iteration 102/1000 | Loss: 0.00001805
Iteration 103/1000 | Loss: 0.00001805
Iteration 104/1000 | Loss: 0.00001805
Iteration 105/1000 | Loss: 0.00001805
Iteration 106/1000 | Loss: 0.00001805
Iteration 107/1000 | Loss: 0.00001805
Iteration 108/1000 | Loss: 0.00001805
Iteration 109/1000 | Loss: 0.00001805
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001804
Iteration 112/1000 | Loss: 0.00001804
Iteration 113/1000 | Loss: 0.00001804
Iteration 114/1000 | Loss: 0.00001804
Iteration 115/1000 | Loss: 0.00001804
Iteration 116/1000 | Loss: 0.00001804
Iteration 117/1000 | Loss: 0.00001804
Iteration 118/1000 | Loss: 0.00001804
Iteration 119/1000 | Loss: 0.00001803
Iteration 120/1000 | Loss: 0.00001803
Iteration 121/1000 | Loss: 0.00001803
Iteration 122/1000 | Loss: 0.00001803
Iteration 123/1000 | Loss: 0.00001802
Iteration 124/1000 | Loss: 0.00001802
Iteration 125/1000 | Loss: 0.00001802
Iteration 126/1000 | Loss: 0.00001802
Iteration 127/1000 | Loss: 0.00001802
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001801
Iteration 132/1000 | Loss: 0.00001800
Iteration 133/1000 | Loss: 0.00001800
Iteration 134/1000 | Loss: 0.00001800
Iteration 135/1000 | Loss: 0.00001800
Iteration 136/1000 | Loss: 0.00001800
Iteration 137/1000 | Loss: 0.00001800
Iteration 138/1000 | Loss: 0.00001800
Iteration 139/1000 | Loss: 0.00001800
Iteration 140/1000 | Loss: 0.00001799
Iteration 141/1000 | Loss: 0.00001799
Iteration 142/1000 | Loss: 0.00001799
Iteration 143/1000 | Loss: 0.00001799
Iteration 144/1000 | Loss: 0.00001799
Iteration 145/1000 | Loss: 0.00001799
Iteration 146/1000 | Loss: 0.00001799
Iteration 147/1000 | Loss: 0.00001799
Iteration 148/1000 | Loss: 0.00001799
Iteration 149/1000 | Loss: 0.00001799
Iteration 150/1000 | Loss: 0.00001799
Iteration 151/1000 | Loss: 0.00001799
Iteration 152/1000 | Loss: 0.00001798
Iteration 153/1000 | Loss: 0.00001798
Iteration 154/1000 | Loss: 0.00001798
Iteration 155/1000 | Loss: 0.00001798
Iteration 156/1000 | Loss: 0.00001798
Iteration 157/1000 | Loss: 0.00001798
Iteration 158/1000 | Loss: 0.00001798
Iteration 159/1000 | Loss: 0.00001798
Iteration 160/1000 | Loss: 0.00001798
Iteration 161/1000 | Loss: 0.00001798
Iteration 162/1000 | Loss: 0.00001798
Iteration 163/1000 | Loss: 0.00001798
Iteration 164/1000 | Loss: 0.00001798
Iteration 165/1000 | Loss: 0.00001798
Iteration 166/1000 | Loss: 0.00001798
Iteration 167/1000 | Loss: 0.00001798
Iteration 168/1000 | Loss: 0.00001798
Iteration 169/1000 | Loss: 0.00001798
Iteration 170/1000 | Loss: 0.00001798
Iteration 171/1000 | Loss: 0.00001798
Iteration 172/1000 | Loss: 0.00001797
Iteration 173/1000 | Loss: 0.00001797
Iteration 174/1000 | Loss: 0.00001797
Iteration 175/1000 | Loss: 0.00001797
Iteration 176/1000 | Loss: 0.00001797
Iteration 177/1000 | Loss: 0.00001797
Iteration 178/1000 | Loss: 0.00001797
Iteration 179/1000 | Loss: 0.00001797
Iteration 180/1000 | Loss: 0.00001797
Iteration 181/1000 | Loss: 0.00001797
Iteration 182/1000 | Loss: 0.00001797
Iteration 183/1000 | Loss: 0.00001797
Iteration 184/1000 | Loss: 0.00001797
Iteration 185/1000 | Loss: 0.00001797
Iteration 186/1000 | Loss: 0.00001797
Iteration 187/1000 | Loss: 0.00001797
Iteration 188/1000 | Loss: 0.00001797
Iteration 189/1000 | Loss: 0.00001797
Iteration 190/1000 | Loss: 0.00001797
Iteration 191/1000 | Loss: 0.00001796
Iteration 192/1000 | Loss: 0.00001796
Iteration 193/1000 | Loss: 0.00001796
Iteration 194/1000 | Loss: 0.00001796
Iteration 195/1000 | Loss: 0.00001796
Iteration 196/1000 | Loss: 0.00001796
Iteration 197/1000 | Loss: 0.00001796
Iteration 198/1000 | Loss: 0.00001796
Iteration 199/1000 | Loss: 0.00001796
Iteration 200/1000 | Loss: 0.00001796
Iteration 201/1000 | Loss: 0.00001796
Iteration 202/1000 | Loss: 0.00001796
Iteration 203/1000 | Loss: 0.00001796
Iteration 204/1000 | Loss: 0.00001796
Iteration 205/1000 | Loss: 0.00001796
Iteration 206/1000 | Loss: 0.00001796
Iteration 207/1000 | Loss: 0.00001796
Iteration 208/1000 | Loss: 0.00001796
Iteration 209/1000 | Loss: 0.00001796
Iteration 210/1000 | Loss: 0.00001796
Iteration 211/1000 | Loss: 0.00001796
Iteration 212/1000 | Loss: 0.00001796
Iteration 213/1000 | Loss: 0.00001796
Iteration 214/1000 | Loss: 0.00001796
Iteration 215/1000 | Loss: 0.00001796
Iteration 216/1000 | Loss: 0.00001796
Iteration 217/1000 | Loss: 0.00001796
Iteration 218/1000 | Loss: 0.00001796
Iteration 219/1000 | Loss: 0.00001796
Iteration 220/1000 | Loss: 0.00001796
Iteration 221/1000 | Loss: 0.00001796
Iteration 222/1000 | Loss: 0.00001796
Iteration 223/1000 | Loss: 0.00001796
Iteration 224/1000 | Loss: 0.00001796
Iteration 225/1000 | Loss: 0.00001796
Iteration 226/1000 | Loss: 0.00001796
Iteration 227/1000 | Loss: 0.00001796
Iteration 228/1000 | Loss: 0.00001796
Iteration 229/1000 | Loss: 0.00001796
Iteration 230/1000 | Loss: 0.00001796
Iteration 231/1000 | Loss: 0.00001796
Iteration 232/1000 | Loss: 0.00001796
Iteration 233/1000 | Loss: 0.00001796
Iteration 234/1000 | Loss: 0.00001796
Iteration 235/1000 | Loss: 0.00001796
Iteration 236/1000 | Loss: 0.00001796
Iteration 237/1000 | Loss: 0.00001796
Iteration 238/1000 | Loss: 0.00001796
Iteration 239/1000 | Loss: 0.00001796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.796214019122999e-05, 1.796214019122999e-05, 1.796214019122999e-05, 1.796214019122999e-05, 1.796214019122999e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.796214019122999e-05

Optimization complete. Final v2v error: 3.600131034851074 mm

Highest mean error: 3.8964767456054688 mm for frame 158

Lowest mean error: 3.3909337520599365 mm for frame 191

Saving results

Total time: 45.67561721801758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085380
Iteration 2/25 | Loss: 0.00376271
Iteration 3/25 | Loss: 0.00237582
Iteration 4/25 | Loss: 0.00220039
Iteration 5/25 | Loss: 0.00183741
Iteration 6/25 | Loss: 0.00187820
Iteration 7/25 | Loss: 0.00155944
Iteration 8/25 | Loss: 0.00152781
Iteration 9/25 | Loss: 0.00140859
Iteration 10/25 | Loss: 0.00135343
Iteration 11/25 | Loss: 0.00125431
Iteration 12/25 | Loss: 0.00121850
Iteration 13/25 | Loss: 0.00119313
Iteration 14/25 | Loss: 0.00118106
Iteration 15/25 | Loss: 0.00117744
Iteration 16/25 | Loss: 0.00115786
Iteration 17/25 | Loss: 0.00115652
Iteration 18/25 | Loss: 0.00113409
Iteration 19/25 | Loss: 0.00113801
Iteration 20/25 | Loss: 0.00112058
Iteration 21/25 | Loss: 0.00111792
Iteration 22/25 | Loss: 0.00111608
Iteration 23/25 | Loss: 0.00110554
Iteration 24/25 | Loss: 0.00110732
Iteration 25/25 | Loss: 0.00111008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39996910
Iteration 2/25 | Loss: 0.00456030
Iteration 3/25 | Loss: 0.00433908
Iteration 4/25 | Loss: 0.00433908
Iteration 5/25 | Loss: 0.00433907
Iteration 6/25 | Loss: 0.00433907
Iteration 7/25 | Loss: 0.00433907
Iteration 8/25 | Loss: 0.00433907
Iteration 9/25 | Loss: 0.00433907
Iteration 10/25 | Loss: 0.00433907
Iteration 11/25 | Loss: 0.00433907
Iteration 12/25 | Loss: 0.00433907
Iteration 13/25 | Loss: 0.00433907
Iteration 14/25 | Loss: 0.00433907
Iteration 15/25 | Loss: 0.00433907
Iteration 16/25 | Loss: 0.00433907
Iteration 17/25 | Loss: 0.00433907
Iteration 18/25 | Loss: 0.00433907
Iteration 19/25 | Loss: 0.00433907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0043390714563429356, 0.0043390714563429356, 0.0043390714563429356, 0.0043390714563429356, 0.0043390714563429356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0043390714563429356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00433907
Iteration 2/1000 | Loss: 0.00391237
Iteration 3/1000 | Loss: 0.00448759
Iteration 4/1000 | Loss: 0.00301772
Iteration 5/1000 | Loss: 0.00220033
Iteration 6/1000 | Loss: 0.00530631
Iteration 7/1000 | Loss: 0.00688790
Iteration 8/1000 | Loss: 0.00437187
Iteration 9/1000 | Loss: 0.00365962
Iteration 10/1000 | Loss: 0.00125871
Iteration 11/1000 | Loss: 0.00110557
Iteration 12/1000 | Loss: 0.00204179
Iteration 13/1000 | Loss: 0.00341705
Iteration 14/1000 | Loss: 0.00109158
Iteration 15/1000 | Loss: 0.00256498
Iteration 16/1000 | Loss: 0.00354890
Iteration 17/1000 | Loss: 0.00174448
Iteration 18/1000 | Loss: 0.00394873
Iteration 19/1000 | Loss: 0.00113037
Iteration 20/1000 | Loss: 0.00130765
Iteration 21/1000 | Loss: 0.00662242
Iteration 22/1000 | Loss: 0.00114725
Iteration 23/1000 | Loss: 0.00322934
Iteration 24/1000 | Loss: 0.00220620
Iteration 25/1000 | Loss: 0.00080474
Iteration 26/1000 | Loss: 0.00188882
Iteration 27/1000 | Loss: 0.00109669
Iteration 28/1000 | Loss: 0.00412000
Iteration 29/1000 | Loss: 0.00294723
Iteration 30/1000 | Loss: 0.00480297
Iteration 31/1000 | Loss: 0.00353777
Iteration 32/1000 | Loss: 0.00352047
Iteration 33/1000 | Loss: 0.00320163
Iteration 34/1000 | Loss: 0.00213321
Iteration 35/1000 | Loss: 0.00176946
Iteration 36/1000 | Loss: 0.00150716
Iteration 37/1000 | Loss: 0.00123834
Iteration 38/1000 | Loss: 0.00117657
Iteration 39/1000 | Loss: 0.00059181
Iteration 40/1000 | Loss: 0.00053687
Iteration 41/1000 | Loss: 0.00012676
Iteration 42/1000 | Loss: 0.00075716
Iteration 43/1000 | Loss: 0.00157565
Iteration 44/1000 | Loss: 0.00203054
Iteration 45/1000 | Loss: 0.00082457
Iteration 46/1000 | Loss: 0.00125437
Iteration 47/1000 | Loss: 0.00089052
Iteration 48/1000 | Loss: 0.00131868
Iteration 49/1000 | Loss: 0.00072830
Iteration 50/1000 | Loss: 0.00057580
Iteration 51/1000 | Loss: 0.00024698
Iteration 52/1000 | Loss: 0.00015745
Iteration 53/1000 | Loss: 0.00008619
Iteration 54/1000 | Loss: 0.00021376
Iteration 55/1000 | Loss: 0.00066040
Iteration 56/1000 | Loss: 0.00054227
Iteration 57/1000 | Loss: 0.00033962
Iteration 58/1000 | Loss: 0.00112483
Iteration 59/1000 | Loss: 0.00101529
Iteration 60/1000 | Loss: 0.00087356
Iteration 61/1000 | Loss: 0.00090156
Iteration 62/1000 | Loss: 0.00226327
Iteration 63/1000 | Loss: 0.00090548
Iteration 64/1000 | Loss: 0.00062338
Iteration 65/1000 | Loss: 0.00068012
Iteration 66/1000 | Loss: 0.00087952
Iteration 67/1000 | Loss: 0.00046231
Iteration 68/1000 | Loss: 0.00065397
Iteration 69/1000 | Loss: 0.00043991
Iteration 70/1000 | Loss: 0.00126067
Iteration 71/1000 | Loss: 0.00044951
Iteration 72/1000 | Loss: 0.00071572
Iteration 73/1000 | Loss: 0.00028668
Iteration 74/1000 | Loss: 0.00013729
Iteration 75/1000 | Loss: 0.00044782
Iteration 76/1000 | Loss: 0.00058873
Iteration 77/1000 | Loss: 0.00063786
Iteration 78/1000 | Loss: 0.00023105
Iteration 79/1000 | Loss: 0.00073616
Iteration 80/1000 | Loss: 0.00049777
Iteration 81/1000 | Loss: 0.00036768
Iteration 82/1000 | Loss: 0.00021608
Iteration 83/1000 | Loss: 0.00109666
Iteration 84/1000 | Loss: 0.00078385
Iteration 85/1000 | Loss: 0.00099457
Iteration 86/1000 | Loss: 0.00089218
Iteration 87/1000 | Loss: 0.00083164
Iteration 88/1000 | Loss: 0.00070020
Iteration 89/1000 | Loss: 0.00137489
Iteration 90/1000 | Loss: 0.00074619
Iteration 91/1000 | Loss: 0.00045511
Iteration 92/1000 | Loss: 0.00042566
Iteration 93/1000 | Loss: 0.00014755
Iteration 94/1000 | Loss: 0.00055915
Iteration 95/1000 | Loss: 0.00081687
Iteration 96/1000 | Loss: 0.00122685
Iteration 97/1000 | Loss: 0.00078644
Iteration 98/1000 | Loss: 0.00009509
Iteration 99/1000 | Loss: 0.00007753
Iteration 100/1000 | Loss: 0.00010578
Iteration 101/1000 | Loss: 0.00031436
Iteration 102/1000 | Loss: 0.00013672
Iteration 103/1000 | Loss: 0.00014009
Iteration 104/1000 | Loss: 0.00049321
Iteration 105/1000 | Loss: 0.00023426
Iteration 106/1000 | Loss: 0.00034148
Iteration 107/1000 | Loss: 0.00022163
Iteration 108/1000 | Loss: 0.00007485
Iteration 109/1000 | Loss: 0.00006264
Iteration 110/1000 | Loss: 0.00007587
Iteration 111/1000 | Loss: 0.00005628
Iteration 112/1000 | Loss: 0.00006498
Iteration 113/1000 | Loss: 0.00019595
Iteration 114/1000 | Loss: 0.00013561
Iteration 115/1000 | Loss: 0.00016314
Iteration 116/1000 | Loss: 0.00015420
Iteration 117/1000 | Loss: 0.00015907
Iteration 118/1000 | Loss: 0.00017031
Iteration 119/1000 | Loss: 0.00005152
Iteration 120/1000 | Loss: 0.00034626
Iteration 121/1000 | Loss: 0.00015121
Iteration 122/1000 | Loss: 0.00009860
Iteration 123/1000 | Loss: 0.00005321
Iteration 124/1000 | Loss: 0.00078874
Iteration 125/1000 | Loss: 0.00127285
Iteration 126/1000 | Loss: 0.00115580
Iteration 127/1000 | Loss: 0.00107507
Iteration 128/1000 | Loss: 0.00111103
Iteration 129/1000 | Loss: 0.00070275
Iteration 130/1000 | Loss: 0.00100753
Iteration 131/1000 | Loss: 0.00041615
Iteration 132/1000 | Loss: 0.00053839
Iteration 133/1000 | Loss: 0.00113756
Iteration 134/1000 | Loss: 0.00077001
Iteration 135/1000 | Loss: 0.00130180
Iteration 136/1000 | Loss: 0.00145279
Iteration 137/1000 | Loss: 0.00051038
Iteration 138/1000 | Loss: 0.00066421
Iteration 139/1000 | Loss: 0.00042052
Iteration 140/1000 | Loss: 0.00126805
Iteration 141/1000 | Loss: 0.00092465
Iteration 142/1000 | Loss: 0.00044727
Iteration 143/1000 | Loss: 0.00038833
Iteration 144/1000 | Loss: 0.00041309
Iteration 145/1000 | Loss: 0.00061351
Iteration 146/1000 | Loss: 0.00049029
Iteration 147/1000 | Loss: 0.00037625
Iteration 148/1000 | Loss: 0.00010933
Iteration 149/1000 | Loss: 0.00053122
Iteration 150/1000 | Loss: 0.00165881
Iteration 151/1000 | Loss: 0.00067988
Iteration 152/1000 | Loss: 0.00034382
Iteration 153/1000 | Loss: 0.00118346
Iteration 154/1000 | Loss: 0.00085303
Iteration 155/1000 | Loss: 0.00008202
Iteration 156/1000 | Loss: 0.00005593
Iteration 157/1000 | Loss: 0.00004238
Iteration 158/1000 | Loss: 0.00003920
Iteration 159/1000 | Loss: 0.00049883
Iteration 160/1000 | Loss: 0.00152618
Iteration 161/1000 | Loss: 0.00039162
Iteration 162/1000 | Loss: 0.00089449
Iteration 163/1000 | Loss: 0.00054756
Iteration 164/1000 | Loss: 0.00025826
Iteration 165/1000 | Loss: 0.00004487
Iteration 166/1000 | Loss: 0.00004516
Iteration 167/1000 | Loss: 0.00003322
Iteration 168/1000 | Loss: 0.00008219
Iteration 169/1000 | Loss: 0.00003031
Iteration 170/1000 | Loss: 0.00002933
Iteration 171/1000 | Loss: 0.00002876
Iteration 172/1000 | Loss: 0.00002831
Iteration 173/1000 | Loss: 0.00007581
Iteration 174/1000 | Loss: 0.00002901
Iteration 175/1000 | Loss: 0.00002831
Iteration 176/1000 | Loss: 0.00002782
Iteration 177/1000 | Loss: 0.00002741
Iteration 178/1000 | Loss: 0.00002736
Iteration 179/1000 | Loss: 0.00002734
Iteration 180/1000 | Loss: 0.00002718
Iteration 181/1000 | Loss: 0.00043360
Iteration 182/1000 | Loss: 0.00016743
Iteration 183/1000 | Loss: 0.00004198
Iteration 184/1000 | Loss: 0.00011067
Iteration 185/1000 | Loss: 0.00023491
Iteration 186/1000 | Loss: 0.00012850
Iteration 187/1000 | Loss: 0.00003436
Iteration 188/1000 | Loss: 0.00003084
Iteration 189/1000 | Loss: 0.00003552
Iteration 190/1000 | Loss: 0.00002905
Iteration 191/1000 | Loss: 0.00023225
Iteration 192/1000 | Loss: 0.00027325
Iteration 193/1000 | Loss: 0.00052535
Iteration 194/1000 | Loss: 0.00003777
Iteration 195/1000 | Loss: 0.00007770
Iteration 196/1000 | Loss: 0.00002774
Iteration 197/1000 | Loss: 0.00002644
Iteration 198/1000 | Loss: 0.00002603
Iteration 199/1000 | Loss: 0.00002531
Iteration 200/1000 | Loss: 0.00002420
Iteration 201/1000 | Loss: 0.00002366
Iteration 202/1000 | Loss: 0.00002350
Iteration 203/1000 | Loss: 0.00002346
Iteration 204/1000 | Loss: 0.00002345
Iteration 205/1000 | Loss: 0.00002329
Iteration 206/1000 | Loss: 0.00002316
Iteration 207/1000 | Loss: 0.00002315
Iteration 208/1000 | Loss: 0.00002310
Iteration 209/1000 | Loss: 0.00002308
Iteration 210/1000 | Loss: 0.00002308
Iteration 211/1000 | Loss: 0.00002308
Iteration 212/1000 | Loss: 0.00002306
Iteration 213/1000 | Loss: 0.00002303
Iteration 214/1000 | Loss: 0.00002302
Iteration 215/1000 | Loss: 0.00002300
Iteration 216/1000 | Loss: 0.00002299
Iteration 217/1000 | Loss: 0.00002299
Iteration 218/1000 | Loss: 0.00002298
Iteration 219/1000 | Loss: 0.00002298
Iteration 220/1000 | Loss: 0.00002297
Iteration 221/1000 | Loss: 0.00002297
Iteration 222/1000 | Loss: 0.00002296
Iteration 223/1000 | Loss: 0.00002296
Iteration 224/1000 | Loss: 0.00002295
Iteration 225/1000 | Loss: 0.00002294
Iteration 226/1000 | Loss: 0.00002294
Iteration 227/1000 | Loss: 0.00002294
Iteration 228/1000 | Loss: 0.00002293
Iteration 229/1000 | Loss: 0.00002293
Iteration 230/1000 | Loss: 0.00002292
Iteration 231/1000 | Loss: 0.00002292
Iteration 232/1000 | Loss: 0.00002292
Iteration 233/1000 | Loss: 0.00002291
Iteration 234/1000 | Loss: 0.00002291
Iteration 235/1000 | Loss: 0.00002291
Iteration 236/1000 | Loss: 0.00002290
Iteration 237/1000 | Loss: 0.00002290
Iteration 238/1000 | Loss: 0.00002290
Iteration 239/1000 | Loss: 0.00002289
Iteration 240/1000 | Loss: 0.00002285
Iteration 241/1000 | Loss: 0.00002285
Iteration 242/1000 | Loss: 0.00002285
Iteration 243/1000 | Loss: 0.00002285
Iteration 244/1000 | Loss: 0.00002285
Iteration 245/1000 | Loss: 0.00002285
Iteration 246/1000 | Loss: 0.00002285
Iteration 247/1000 | Loss: 0.00002284
Iteration 248/1000 | Loss: 0.00002284
Iteration 249/1000 | Loss: 0.00002284
Iteration 250/1000 | Loss: 0.00002283
Iteration 251/1000 | Loss: 0.00002283
Iteration 252/1000 | Loss: 0.00002283
Iteration 253/1000 | Loss: 0.00002282
Iteration 254/1000 | Loss: 0.00002282
Iteration 255/1000 | Loss: 0.00002282
Iteration 256/1000 | Loss: 0.00002282
Iteration 257/1000 | Loss: 0.00002282
Iteration 258/1000 | Loss: 0.00002282
Iteration 259/1000 | Loss: 0.00002281
Iteration 260/1000 | Loss: 0.00002281
Iteration 261/1000 | Loss: 0.00002281
Iteration 262/1000 | Loss: 0.00002281
Iteration 263/1000 | Loss: 0.00002281
Iteration 264/1000 | Loss: 0.00002280
Iteration 265/1000 | Loss: 0.00002280
Iteration 266/1000 | Loss: 0.00002280
Iteration 267/1000 | Loss: 0.00002280
Iteration 268/1000 | Loss: 0.00002280
Iteration 269/1000 | Loss: 0.00002280
Iteration 270/1000 | Loss: 0.00002280
Iteration 271/1000 | Loss: 0.00002280
Iteration 272/1000 | Loss: 0.00002280
Iteration 273/1000 | Loss: 0.00002280
Iteration 274/1000 | Loss: 0.00002280
Iteration 275/1000 | Loss: 0.00002280
Iteration 276/1000 | Loss: 0.00002280
Iteration 277/1000 | Loss: 0.00002280
Iteration 278/1000 | Loss: 0.00002280
Iteration 279/1000 | Loss: 0.00002280
Iteration 280/1000 | Loss: 0.00002280
Iteration 281/1000 | Loss: 0.00002280
Iteration 282/1000 | Loss: 0.00002280
Iteration 283/1000 | Loss: 0.00002280
Iteration 284/1000 | Loss: 0.00002280
Iteration 285/1000 | Loss: 0.00002280
Iteration 286/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [2.2795256882091053e-05, 2.2795256882091053e-05, 2.2795256882091053e-05, 2.2795256882091053e-05, 2.2795256882091053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2795256882091053e-05

Optimization complete. Final v2v error: 3.91741943359375 mm

Highest mean error: 5.85501766204834 mm for frame 66

Lowest mean error: 3.3287594318389893 mm for frame 136

Saving results

Total time: 373.2972049713135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408993
Iteration 2/25 | Loss: 0.00089227
Iteration 3/25 | Loss: 0.00073612
Iteration 4/25 | Loss: 0.00071345
Iteration 5/25 | Loss: 0.00070718
Iteration 6/25 | Loss: 0.00070616
Iteration 7/25 | Loss: 0.00070579
Iteration 8/25 | Loss: 0.00070575
Iteration 9/25 | Loss: 0.00070575
Iteration 10/25 | Loss: 0.00070575
Iteration 11/25 | Loss: 0.00070575
Iteration 12/25 | Loss: 0.00070575
Iteration 13/25 | Loss: 0.00070575
Iteration 14/25 | Loss: 0.00070575
Iteration 15/25 | Loss: 0.00070575
Iteration 16/25 | Loss: 0.00070575
Iteration 17/25 | Loss: 0.00070575
Iteration 18/25 | Loss: 0.00070575
Iteration 19/25 | Loss: 0.00070575
Iteration 20/25 | Loss: 0.00070575
Iteration 21/25 | Loss: 0.00070575
Iteration 22/25 | Loss: 0.00070575
Iteration 23/25 | Loss: 0.00070575
Iteration 24/25 | Loss: 0.00070575
Iteration 25/25 | Loss: 0.00070575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45566320
Iteration 2/25 | Loss: 0.00031708
Iteration 3/25 | Loss: 0.00031708
Iteration 4/25 | Loss: 0.00031708
Iteration 5/25 | Loss: 0.00031708
Iteration 6/25 | Loss: 0.00031708
Iteration 7/25 | Loss: 0.00031708
Iteration 8/25 | Loss: 0.00031708
Iteration 9/25 | Loss: 0.00031708
Iteration 10/25 | Loss: 0.00031708
Iteration 11/25 | Loss: 0.00031708
Iteration 12/25 | Loss: 0.00031708
Iteration 13/25 | Loss: 0.00031708
Iteration 14/25 | Loss: 0.00031708
Iteration 15/25 | Loss: 0.00031708
Iteration 16/25 | Loss: 0.00031708
Iteration 17/25 | Loss: 0.00031708
Iteration 18/25 | Loss: 0.00031708
Iteration 19/25 | Loss: 0.00031708
Iteration 20/25 | Loss: 0.00031708
Iteration 21/25 | Loss: 0.00031708
Iteration 22/25 | Loss: 0.00031708
Iteration 23/25 | Loss: 0.00031708
Iteration 24/25 | Loss: 0.00031708
Iteration 25/25 | Loss: 0.00031708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031708
Iteration 2/1000 | Loss: 0.00005149
Iteration 3/1000 | Loss: 0.00003721
Iteration 4/1000 | Loss: 0.00003112
Iteration 5/1000 | Loss: 0.00002967
Iteration 6/1000 | Loss: 0.00002840
Iteration 7/1000 | Loss: 0.00002771
Iteration 8/1000 | Loss: 0.00002707
Iteration 9/1000 | Loss: 0.00002675
Iteration 10/1000 | Loss: 0.00002651
Iteration 11/1000 | Loss: 0.00002632
Iteration 12/1000 | Loss: 0.00002629
Iteration 13/1000 | Loss: 0.00002619
Iteration 14/1000 | Loss: 0.00002611
Iteration 15/1000 | Loss: 0.00002611
Iteration 16/1000 | Loss: 0.00002608
Iteration 17/1000 | Loss: 0.00002608
Iteration 18/1000 | Loss: 0.00002607
Iteration 19/1000 | Loss: 0.00002607
Iteration 20/1000 | Loss: 0.00002607
Iteration 21/1000 | Loss: 0.00002606
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00002605
Iteration 24/1000 | Loss: 0.00002603
Iteration 25/1000 | Loss: 0.00002601
Iteration 26/1000 | Loss: 0.00002597
Iteration 27/1000 | Loss: 0.00002595
Iteration 28/1000 | Loss: 0.00002593
Iteration 29/1000 | Loss: 0.00002593
Iteration 30/1000 | Loss: 0.00002592
Iteration 31/1000 | Loss: 0.00002587
Iteration 32/1000 | Loss: 0.00002585
Iteration 33/1000 | Loss: 0.00002585
Iteration 34/1000 | Loss: 0.00002584
Iteration 35/1000 | Loss: 0.00002584
Iteration 36/1000 | Loss: 0.00002583
Iteration 37/1000 | Loss: 0.00002583
Iteration 38/1000 | Loss: 0.00002583
Iteration 39/1000 | Loss: 0.00002582
Iteration 40/1000 | Loss: 0.00002582
Iteration 41/1000 | Loss: 0.00002582
Iteration 42/1000 | Loss: 0.00002582
Iteration 43/1000 | Loss: 0.00002582
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002582
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002581
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002581
Iteration 50/1000 | Loss: 0.00002581
Iteration 51/1000 | Loss: 0.00002581
Iteration 52/1000 | Loss: 0.00002581
Iteration 53/1000 | Loss: 0.00002580
Iteration 54/1000 | Loss: 0.00002580
Iteration 55/1000 | Loss: 0.00002580
Iteration 56/1000 | Loss: 0.00002580
Iteration 57/1000 | Loss: 0.00002580
Iteration 58/1000 | Loss: 0.00002580
Iteration 59/1000 | Loss: 0.00002580
Iteration 60/1000 | Loss: 0.00002580
Iteration 61/1000 | Loss: 0.00002580
Iteration 62/1000 | Loss: 0.00002580
Iteration 63/1000 | Loss: 0.00002579
Iteration 64/1000 | Loss: 0.00002579
Iteration 65/1000 | Loss: 0.00002579
Iteration 66/1000 | Loss: 0.00002579
Iteration 67/1000 | Loss: 0.00002579
Iteration 68/1000 | Loss: 0.00002579
Iteration 69/1000 | Loss: 0.00002579
Iteration 70/1000 | Loss: 0.00002579
Iteration 71/1000 | Loss: 0.00002579
Iteration 72/1000 | Loss: 0.00002579
Iteration 73/1000 | Loss: 0.00002579
Iteration 74/1000 | Loss: 0.00002579
Iteration 75/1000 | Loss: 0.00002579
Iteration 76/1000 | Loss: 0.00002579
Iteration 77/1000 | Loss: 0.00002578
Iteration 78/1000 | Loss: 0.00002578
Iteration 79/1000 | Loss: 0.00002578
Iteration 80/1000 | Loss: 0.00002578
Iteration 81/1000 | Loss: 0.00002578
Iteration 82/1000 | Loss: 0.00002578
Iteration 83/1000 | Loss: 0.00002578
Iteration 84/1000 | Loss: 0.00002578
Iteration 85/1000 | Loss: 0.00002578
Iteration 86/1000 | Loss: 0.00002578
Iteration 87/1000 | Loss: 0.00002578
Iteration 88/1000 | Loss: 0.00002578
Iteration 89/1000 | Loss: 0.00002577
Iteration 90/1000 | Loss: 0.00002577
Iteration 91/1000 | Loss: 0.00002577
Iteration 92/1000 | Loss: 0.00002577
Iteration 93/1000 | Loss: 0.00002577
Iteration 94/1000 | Loss: 0.00002577
Iteration 95/1000 | Loss: 0.00002577
Iteration 96/1000 | Loss: 0.00002577
Iteration 97/1000 | Loss: 0.00002577
Iteration 98/1000 | Loss: 0.00002577
Iteration 99/1000 | Loss: 0.00002577
Iteration 100/1000 | Loss: 0.00002577
Iteration 101/1000 | Loss: 0.00002577
Iteration 102/1000 | Loss: 0.00002577
Iteration 103/1000 | Loss: 0.00002577
Iteration 104/1000 | Loss: 0.00002576
Iteration 105/1000 | Loss: 0.00002576
Iteration 106/1000 | Loss: 0.00002576
Iteration 107/1000 | Loss: 0.00002576
Iteration 108/1000 | Loss: 0.00002576
Iteration 109/1000 | Loss: 0.00002576
Iteration 110/1000 | Loss: 0.00002576
Iteration 111/1000 | Loss: 0.00002576
Iteration 112/1000 | Loss: 0.00002576
Iteration 113/1000 | Loss: 0.00002576
Iteration 114/1000 | Loss: 0.00002575
Iteration 115/1000 | Loss: 0.00002575
Iteration 116/1000 | Loss: 0.00002575
Iteration 117/1000 | Loss: 0.00002575
Iteration 118/1000 | Loss: 0.00002575
Iteration 119/1000 | Loss: 0.00002575
Iteration 120/1000 | Loss: 0.00002575
Iteration 121/1000 | Loss: 0.00002575
Iteration 122/1000 | Loss: 0.00002575
Iteration 123/1000 | Loss: 0.00002575
Iteration 124/1000 | Loss: 0.00002575
Iteration 125/1000 | Loss: 0.00002575
Iteration 126/1000 | Loss: 0.00002575
Iteration 127/1000 | Loss: 0.00002575
Iteration 128/1000 | Loss: 0.00002575
Iteration 129/1000 | Loss: 0.00002575
Iteration 130/1000 | Loss: 0.00002575
Iteration 131/1000 | Loss: 0.00002575
Iteration 132/1000 | Loss: 0.00002575
Iteration 133/1000 | Loss: 0.00002575
Iteration 134/1000 | Loss: 0.00002575
Iteration 135/1000 | Loss: 0.00002575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.5753994123078883e-05, 2.5753994123078883e-05, 2.5753994123078883e-05, 2.5753994123078883e-05, 2.5753994123078883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5753994123078883e-05

Optimization complete. Final v2v error: 4.200998783111572 mm

Highest mean error: 4.5852742195129395 mm for frame 14

Lowest mean error: 3.800898313522339 mm for frame 33

Saving results

Total time: 36.494014739990234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806713
Iteration 2/25 | Loss: 0.00135999
Iteration 3/25 | Loss: 0.00093893
Iteration 4/25 | Loss: 0.00086315
Iteration 5/25 | Loss: 0.00085472
Iteration 6/25 | Loss: 0.00083387
Iteration 7/25 | Loss: 0.00083574
Iteration 8/25 | Loss: 0.00082408
Iteration 9/25 | Loss: 0.00082656
Iteration 10/25 | Loss: 0.00081654
Iteration 11/25 | Loss: 0.00081848
Iteration 12/25 | Loss: 0.00081435
Iteration 13/25 | Loss: 0.00083959
Iteration 14/25 | Loss: 0.00082341
Iteration 15/25 | Loss: 0.00081165
Iteration 16/25 | Loss: 0.00079708
Iteration 17/25 | Loss: 0.00078643
Iteration 18/25 | Loss: 0.00078560
Iteration 19/25 | Loss: 0.00078615
Iteration 20/25 | Loss: 0.00078087
Iteration 21/25 | Loss: 0.00078920
Iteration 22/25 | Loss: 0.00079360
Iteration 23/25 | Loss: 0.00079172
Iteration 24/25 | Loss: 0.00079641
Iteration 25/25 | Loss: 0.00078523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.95249939
Iteration 2/25 | Loss: 0.00105653
Iteration 3/25 | Loss: 0.00105557
Iteration 4/25 | Loss: 0.00105557
Iteration 5/25 | Loss: 0.00105557
Iteration 6/25 | Loss: 0.00105557
Iteration 7/25 | Loss: 0.00105557
Iteration 8/25 | Loss: 0.00105557
Iteration 9/25 | Loss: 0.00105557
Iteration 10/25 | Loss: 0.00105557
Iteration 11/25 | Loss: 0.00105557
Iteration 12/25 | Loss: 0.00105557
Iteration 13/25 | Loss: 0.00105557
Iteration 14/25 | Loss: 0.00105557
Iteration 15/25 | Loss: 0.00105557
Iteration 16/25 | Loss: 0.00105557
Iteration 17/25 | Loss: 0.00105557
Iteration 18/25 | Loss: 0.00105557
Iteration 19/25 | Loss: 0.00105557
Iteration 20/25 | Loss: 0.00105557
Iteration 21/25 | Loss: 0.00105557
Iteration 22/25 | Loss: 0.00105557
Iteration 23/25 | Loss: 0.00105557
Iteration 24/25 | Loss: 0.00105557
Iteration 25/25 | Loss: 0.00105557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105557
Iteration 2/1000 | Loss: 0.00725438
Iteration 3/1000 | Loss: 0.01726436
Iteration 4/1000 | Loss: 0.01214045
Iteration 5/1000 | Loss: 0.01141429
Iteration 6/1000 | Loss: 0.00053877
Iteration 7/1000 | Loss: 0.00691127
Iteration 8/1000 | Loss: 0.00091999
Iteration 9/1000 | Loss: 0.00052168
Iteration 10/1000 | Loss: 0.00014906
Iteration 11/1000 | Loss: 0.00066319
Iteration 12/1000 | Loss: 0.00017267
Iteration 13/1000 | Loss: 0.00105826
Iteration 14/1000 | Loss: 0.00112834
Iteration 15/1000 | Loss: 0.00060715
Iteration 16/1000 | Loss: 0.00036279
Iteration 17/1000 | Loss: 0.00028472
Iteration 18/1000 | Loss: 0.00016869
Iteration 19/1000 | Loss: 0.00024941
Iteration 20/1000 | Loss: 0.00013211
Iteration 21/1000 | Loss: 0.00013919
Iteration 22/1000 | Loss: 0.00036861
Iteration 23/1000 | Loss: 0.00021152
Iteration 24/1000 | Loss: 0.00057123
Iteration 25/1000 | Loss: 0.00006363
Iteration 26/1000 | Loss: 0.00021829
Iteration 27/1000 | Loss: 0.00025961
Iteration 28/1000 | Loss: 0.00031415
Iteration 29/1000 | Loss: 0.00041813
Iteration 30/1000 | Loss: 0.00005391
Iteration 31/1000 | Loss: 0.00024453
Iteration 32/1000 | Loss: 0.00025022
Iteration 33/1000 | Loss: 0.00030414
Iteration 34/1000 | Loss: 0.00004467
Iteration 35/1000 | Loss: 0.00038474
Iteration 36/1000 | Loss: 0.00029640
Iteration 37/1000 | Loss: 0.00039054
Iteration 38/1000 | Loss: 0.00004032
Iteration 39/1000 | Loss: 0.00003423
Iteration 40/1000 | Loss: 0.00003238
Iteration 41/1000 | Loss: 0.00003031
Iteration 42/1000 | Loss: 0.00002856
Iteration 43/1000 | Loss: 0.00002758
Iteration 44/1000 | Loss: 0.00002700
Iteration 45/1000 | Loss: 0.00002666
Iteration 46/1000 | Loss: 0.00002645
Iteration 47/1000 | Loss: 0.00002643
Iteration 48/1000 | Loss: 0.00039250
Iteration 49/1000 | Loss: 0.00004605
Iteration 50/1000 | Loss: 0.00003635
Iteration 51/1000 | Loss: 0.00003170
Iteration 52/1000 | Loss: 0.00002888
Iteration 53/1000 | Loss: 0.00002776
Iteration 54/1000 | Loss: 0.00002681
Iteration 55/1000 | Loss: 0.00002579
Iteration 56/1000 | Loss: 0.00002515
Iteration 57/1000 | Loss: 0.00002478
Iteration 58/1000 | Loss: 0.00002460
Iteration 59/1000 | Loss: 0.00002450
Iteration 60/1000 | Loss: 0.00002449
Iteration 61/1000 | Loss: 0.00002448
Iteration 62/1000 | Loss: 0.00002447
Iteration 63/1000 | Loss: 0.00002446
Iteration 64/1000 | Loss: 0.00002444
Iteration 65/1000 | Loss: 0.00002444
Iteration 66/1000 | Loss: 0.00002437
Iteration 67/1000 | Loss: 0.00002435
Iteration 68/1000 | Loss: 0.00002435
Iteration 69/1000 | Loss: 0.00002434
Iteration 70/1000 | Loss: 0.00002434
Iteration 71/1000 | Loss: 0.00002434
Iteration 72/1000 | Loss: 0.00002433
Iteration 73/1000 | Loss: 0.00002432
Iteration 74/1000 | Loss: 0.00002431
Iteration 75/1000 | Loss: 0.00002431
Iteration 76/1000 | Loss: 0.00002431
Iteration 77/1000 | Loss: 0.00002431
Iteration 78/1000 | Loss: 0.00002431
Iteration 79/1000 | Loss: 0.00002431
Iteration 80/1000 | Loss: 0.00002431
Iteration 81/1000 | Loss: 0.00002431
Iteration 82/1000 | Loss: 0.00002431
Iteration 83/1000 | Loss: 0.00002431
Iteration 84/1000 | Loss: 0.00002430
Iteration 85/1000 | Loss: 0.00002430
Iteration 86/1000 | Loss: 0.00002430
Iteration 87/1000 | Loss: 0.00002430
Iteration 88/1000 | Loss: 0.00002427
Iteration 89/1000 | Loss: 0.00002426
Iteration 90/1000 | Loss: 0.00002426
Iteration 91/1000 | Loss: 0.00002426
Iteration 92/1000 | Loss: 0.00002425
Iteration 93/1000 | Loss: 0.00002425
Iteration 94/1000 | Loss: 0.00002424
Iteration 95/1000 | Loss: 0.00002423
Iteration 96/1000 | Loss: 0.00002423
Iteration 97/1000 | Loss: 0.00002423
Iteration 98/1000 | Loss: 0.00002423
Iteration 99/1000 | Loss: 0.00002422
Iteration 100/1000 | Loss: 0.00002422
Iteration 101/1000 | Loss: 0.00002422
Iteration 102/1000 | Loss: 0.00002421
Iteration 103/1000 | Loss: 0.00002421
Iteration 104/1000 | Loss: 0.00002421
Iteration 105/1000 | Loss: 0.00002421
Iteration 106/1000 | Loss: 0.00002420
Iteration 107/1000 | Loss: 0.00002420
Iteration 108/1000 | Loss: 0.00002419
Iteration 109/1000 | Loss: 0.00002419
Iteration 110/1000 | Loss: 0.00002419
Iteration 111/1000 | Loss: 0.00002419
Iteration 112/1000 | Loss: 0.00002419
Iteration 113/1000 | Loss: 0.00002418
Iteration 114/1000 | Loss: 0.00002418
Iteration 115/1000 | Loss: 0.00002418
Iteration 116/1000 | Loss: 0.00002418
Iteration 117/1000 | Loss: 0.00002418
Iteration 118/1000 | Loss: 0.00002418
Iteration 119/1000 | Loss: 0.00002417
Iteration 120/1000 | Loss: 0.00002417
Iteration 121/1000 | Loss: 0.00002417
Iteration 122/1000 | Loss: 0.00002417
Iteration 123/1000 | Loss: 0.00002417
Iteration 124/1000 | Loss: 0.00002417
Iteration 125/1000 | Loss: 0.00002417
Iteration 126/1000 | Loss: 0.00002417
Iteration 127/1000 | Loss: 0.00002417
Iteration 128/1000 | Loss: 0.00002417
Iteration 129/1000 | Loss: 0.00002417
Iteration 130/1000 | Loss: 0.00002417
Iteration 131/1000 | Loss: 0.00002417
Iteration 132/1000 | Loss: 0.00002417
Iteration 133/1000 | Loss: 0.00002417
Iteration 134/1000 | Loss: 0.00002417
Iteration 135/1000 | Loss: 0.00002417
Iteration 136/1000 | Loss: 0.00002417
Iteration 137/1000 | Loss: 0.00002417
Iteration 138/1000 | Loss: 0.00002417
Iteration 139/1000 | Loss: 0.00002417
Iteration 140/1000 | Loss: 0.00002417
Iteration 141/1000 | Loss: 0.00002417
Iteration 142/1000 | Loss: 0.00002417
Iteration 143/1000 | Loss: 0.00002417
Iteration 144/1000 | Loss: 0.00002417
Iteration 145/1000 | Loss: 0.00002417
Iteration 146/1000 | Loss: 0.00002417
Iteration 147/1000 | Loss: 0.00002417
Iteration 148/1000 | Loss: 0.00002417
Iteration 149/1000 | Loss: 0.00002417
Iteration 150/1000 | Loss: 0.00002417
Iteration 151/1000 | Loss: 0.00002417
Iteration 152/1000 | Loss: 0.00002417
Iteration 153/1000 | Loss: 0.00002417
Iteration 154/1000 | Loss: 0.00002417
Iteration 155/1000 | Loss: 0.00002417
Iteration 156/1000 | Loss: 0.00002417
Iteration 157/1000 | Loss: 0.00002417
Iteration 158/1000 | Loss: 0.00002417
Iteration 159/1000 | Loss: 0.00002417
Iteration 160/1000 | Loss: 0.00002417
Iteration 161/1000 | Loss: 0.00002417
Iteration 162/1000 | Loss: 0.00002417
Iteration 163/1000 | Loss: 0.00002417
Iteration 164/1000 | Loss: 0.00002417
Iteration 165/1000 | Loss: 0.00002417
Iteration 166/1000 | Loss: 0.00002417
Iteration 167/1000 | Loss: 0.00002417
Iteration 168/1000 | Loss: 0.00002417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.416642746538855e-05, 2.416642746538855e-05, 2.416642746538855e-05, 2.416642746538855e-05, 2.416642746538855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.416642746538855e-05

Optimization complete. Final v2v error: 3.9237258434295654 mm

Highest mean error: 6.332918167114258 mm for frame 34

Lowest mean error: 2.8198702335357666 mm for frame 71

Saving results

Total time: 132.22576546669006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621887
Iteration 2/25 | Loss: 0.00097792
Iteration 3/25 | Loss: 0.00081776
Iteration 4/25 | Loss: 0.00077904
Iteration 5/25 | Loss: 0.00077509
Iteration 6/25 | Loss: 0.00077371
Iteration 7/25 | Loss: 0.00077358
Iteration 8/25 | Loss: 0.00077358
Iteration 9/25 | Loss: 0.00077358
Iteration 10/25 | Loss: 0.00077358
Iteration 11/25 | Loss: 0.00077358
Iteration 12/25 | Loss: 0.00077358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007735780091024935, 0.0007735780091024935, 0.0007735780091024935, 0.0007735780091024935, 0.0007735780091024935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007735780091024935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64996004
Iteration 2/25 | Loss: 0.00046288
Iteration 3/25 | Loss: 0.00046288
Iteration 4/25 | Loss: 0.00046288
Iteration 5/25 | Loss: 0.00046288
Iteration 6/25 | Loss: 0.00046288
Iteration 7/25 | Loss: 0.00046288
Iteration 8/25 | Loss: 0.00046288
Iteration 9/25 | Loss: 0.00046288
Iteration 10/25 | Loss: 0.00046288
Iteration 11/25 | Loss: 0.00046288
Iteration 12/25 | Loss: 0.00046288
Iteration 13/25 | Loss: 0.00046288
Iteration 14/25 | Loss: 0.00046288
Iteration 15/25 | Loss: 0.00046288
Iteration 16/25 | Loss: 0.00046288
Iteration 17/25 | Loss: 0.00046288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004628787864930928, 0.0004628787864930928, 0.0004628787864930928, 0.0004628787864930928, 0.0004628787864930928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004628787864930928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046288
Iteration 2/1000 | Loss: 0.00006143
Iteration 3/1000 | Loss: 0.00003321
Iteration 4/1000 | Loss: 0.00003073
Iteration 5/1000 | Loss: 0.00002906
Iteration 6/1000 | Loss: 0.00002844
Iteration 7/1000 | Loss: 0.00002800
Iteration 8/1000 | Loss: 0.00002763
Iteration 9/1000 | Loss: 0.00002742
Iteration 10/1000 | Loss: 0.00002722
Iteration 11/1000 | Loss: 0.00002710
Iteration 12/1000 | Loss: 0.00002706
Iteration 13/1000 | Loss: 0.00002705
Iteration 14/1000 | Loss: 0.00002705
Iteration 15/1000 | Loss: 0.00002703
Iteration 16/1000 | Loss: 0.00002702
Iteration 17/1000 | Loss: 0.00002701
Iteration 18/1000 | Loss: 0.00002701
Iteration 19/1000 | Loss: 0.00002701
Iteration 20/1000 | Loss: 0.00002701
Iteration 21/1000 | Loss: 0.00002701
Iteration 22/1000 | Loss: 0.00002700
Iteration 23/1000 | Loss: 0.00002700
Iteration 24/1000 | Loss: 0.00002700
Iteration 25/1000 | Loss: 0.00002700
Iteration 26/1000 | Loss: 0.00002699
Iteration 27/1000 | Loss: 0.00002699
Iteration 28/1000 | Loss: 0.00002698
Iteration 29/1000 | Loss: 0.00002698
Iteration 30/1000 | Loss: 0.00002697
Iteration 31/1000 | Loss: 0.00002697
Iteration 32/1000 | Loss: 0.00002697
Iteration 33/1000 | Loss: 0.00002697
Iteration 34/1000 | Loss: 0.00002697
Iteration 35/1000 | Loss: 0.00002697
Iteration 36/1000 | Loss: 0.00002696
Iteration 37/1000 | Loss: 0.00002696
Iteration 38/1000 | Loss: 0.00002696
Iteration 39/1000 | Loss: 0.00002696
Iteration 40/1000 | Loss: 0.00002696
Iteration 41/1000 | Loss: 0.00002696
Iteration 42/1000 | Loss: 0.00002696
Iteration 43/1000 | Loss: 0.00002696
Iteration 44/1000 | Loss: 0.00002695
Iteration 45/1000 | Loss: 0.00002695
Iteration 46/1000 | Loss: 0.00002695
Iteration 47/1000 | Loss: 0.00002695
Iteration 48/1000 | Loss: 0.00002695
Iteration 49/1000 | Loss: 0.00002695
Iteration 50/1000 | Loss: 0.00002695
Iteration 51/1000 | Loss: 0.00002695
Iteration 52/1000 | Loss: 0.00002694
Iteration 53/1000 | Loss: 0.00002694
Iteration 54/1000 | Loss: 0.00002694
Iteration 55/1000 | Loss: 0.00002694
Iteration 56/1000 | Loss: 0.00002693
Iteration 57/1000 | Loss: 0.00002693
Iteration 58/1000 | Loss: 0.00002692
Iteration 59/1000 | Loss: 0.00002692
Iteration 60/1000 | Loss: 0.00002692
Iteration 61/1000 | Loss: 0.00002692
Iteration 62/1000 | Loss: 0.00002691
Iteration 63/1000 | Loss: 0.00002691
Iteration 64/1000 | Loss: 0.00002691
Iteration 65/1000 | Loss: 0.00002691
Iteration 66/1000 | Loss: 0.00002691
Iteration 67/1000 | Loss: 0.00002691
Iteration 68/1000 | Loss: 0.00002691
Iteration 69/1000 | Loss: 0.00002690
Iteration 70/1000 | Loss: 0.00002690
Iteration 71/1000 | Loss: 0.00002690
Iteration 72/1000 | Loss: 0.00002690
Iteration 73/1000 | Loss: 0.00002690
Iteration 74/1000 | Loss: 0.00002688
Iteration 75/1000 | Loss: 0.00002688
Iteration 76/1000 | Loss: 0.00002688
Iteration 77/1000 | Loss: 0.00002688
Iteration 78/1000 | Loss: 0.00002687
Iteration 79/1000 | Loss: 0.00002687
Iteration 80/1000 | Loss: 0.00002687
Iteration 81/1000 | Loss: 0.00002686
Iteration 82/1000 | Loss: 0.00002684
Iteration 83/1000 | Loss: 0.00002684
Iteration 84/1000 | Loss: 0.00002684
Iteration 85/1000 | Loss: 0.00002684
Iteration 86/1000 | Loss: 0.00002684
Iteration 87/1000 | Loss: 0.00002684
Iteration 88/1000 | Loss: 0.00002684
Iteration 89/1000 | Loss: 0.00002684
Iteration 90/1000 | Loss: 0.00002684
Iteration 91/1000 | Loss: 0.00002684
Iteration 92/1000 | Loss: 0.00002684
Iteration 93/1000 | Loss: 0.00002684
Iteration 94/1000 | Loss: 0.00002684
Iteration 95/1000 | Loss: 0.00002683
Iteration 96/1000 | Loss: 0.00002683
Iteration 97/1000 | Loss: 0.00002683
Iteration 98/1000 | Loss: 0.00002682
Iteration 99/1000 | Loss: 0.00002682
Iteration 100/1000 | Loss: 0.00002682
Iteration 101/1000 | Loss: 0.00002681
Iteration 102/1000 | Loss: 0.00002681
Iteration 103/1000 | Loss: 0.00002681
Iteration 104/1000 | Loss: 0.00002681
Iteration 105/1000 | Loss: 0.00002681
Iteration 106/1000 | Loss: 0.00002681
Iteration 107/1000 | Loss: 0.00002681
Iteration 108/1000 | Loss: 0.00002681
Iteration 109/1000 | Loss: 0.00002681
Iteration 110/1000 | Loss: 0.00002681
Iteration 111/1000 | Loss: 0.00002680
Iteration 112/1000 | Loss: 0.00002680
Iteration 113/1000 | Loss: 0.00002680
Iteration 114/1000 | Loss: 0.00002680
Iteration 115/1000 | Loss: 0.00002680
Iteration 116/1000 | Loss: 0.00002680
Iteration 117/1000 | Loss: 0.00002680
Iteration 118/1000 | Loss: 0.00002680
Iteration 119/1000 | Loss: 0.00002680
Iteration 120/1000 | Loss: 0.00002680
Iteration 121/1000 | Loss: 0.00002680
Iteration 122/1000 | Loss: 0.00002680
Iteration 123/1000 | Loss: 0.00002679
Iteration 124/1000 | Loss: 0.00002679
Iteration 125/1000 | Loss: 0.00002679
Iteration 126/1000 | Loss: 0.00002679
Iteration 127/1000 | Loss: 0.00002678
Iteration 128/1000 | Loss: 0.00002678
Iteration 129/1000 | Loss: 0.00002678
Iteration 130/1000 | Loss: 0.00002678
Iteration 131/1000 | Loss: 0.00002678
Iteration 132/1000 | Loss: 0.00002678
Iteration 133/1000 | Loss: 0.00002678
Iteration 134/1000 | Loss: 0.00002678
Iteration 135/1000 | Loss: 0.00002678
Iteration 136/1000 | Loss: 0.00002678
Iteration 137/1000 | Loss: 0.00002677
Iteration 138/1000 | Loss: 0.00002677
Iteration 139/1000 | Loss: 0.00002677
Iteration 140/1000 | Loss: 0.00002677
Iteration 141/1000 | Loss: 0.00002677
Iteration 142/1000 | Loss: 0.00002677
Iteration 143/1000 | Loss: 0.00002677
Iteration 144/1000 | Loss: 0.00002677
Iteration 145/1000 | Loss: 0.00002677
Iteration 146/1000 | Loss: 0.00002677
Iteration 147/1000 | Loss: 0.00002677
Iteration 148/1000 | Loss: 0.00002677
Iteration 149/1000 | Loss: 0.00002677
Iteration 150/1000 | Loss: 0.00002677
Iteration 151/1000 | Loss: 0.00002677
Iteration 152/1000 | Loss: 0.00002677
Iteration 153/1000 | Loss: 0.00002677
Iteration 154/1000 | Loss: 0.00002677
Iteration 155/1000 | Loss: 0.00002677
Iteration 156/1000 | Loss: 0.00002677
Iteration 157/1000 | Loss: 0.00002677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.676979056559503e-05, 2.676979056559503e-05, 2.676979056559503e-05, 2.676979056559503e-05, 2.676979056559503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.676979056559503e-05

Optimization complete. Final v2v error: 4.365089416503906 mm

Highest mean error: 4.526134490966797 mm for frame 65

Lowest mean error: 4.2625813484191895 mm for frame 117

Saving results

Total time: 34.03210806846619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829541
Iteration 2/25 | Loss: 0.00076755
Iteration 3/25 | Loss: 0.00061980
Iteration 4/25 | Loss: 0.00059557
Iteration 5/25 | Loss: 0.00059154
Iteration 6/25 | Loss: 0.00059078
Iteration 7/25 | Loss: 0.00059078
Iteration 8/25 | Loss: 0.00059078
Iteration 9/25 | Loss: 0.00059078
Iteration 10/25 | Loss: 0.00059078
Iteration 11/25 | Loss: 0.00059078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005907779559493065, 0.0005907779559493065, 0.0005907779559493065, 0.0005907779559493065, 0.0005907779559493065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005907779559493065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46333408
Iteration 2/25 | Loss: 0.00028226
Iteration 3/25 | Loss: 0.00028226
Iteration 4/25 | Loss: 0.00028226
Iteration 5/25 | Loss: 0.00028226
Iteration 6/25 | Loss: 0.00028226
Iteration 7/25 | Loss: 0.00028226
Iteration 8/25 | Loss: 0.00028226
Iteration 9/25 | Loss: 0.00028226
Iteration 10/25 | Loss: 0.00028226
Iteration 11/25 | Loss: 0.00028226
Iteration 12/25 | Loss: 0.00028226
Iteration 13/25 | Loss: 0.00028226
Iteration 14/25 | Loss: 0.00028226
Iteration 15/25 | Loss: 0.00028226
Iteration 16/25 | Loss: 0.00028226
Iteration 17/25 | Loss: 0.00028226
Iteration 18/25 | Loss: 0.00028226
Iteration 19/25 | Loss: 0.00028226
Iteration 20/25 | Loss: 0.00028226
Iteration 21/25 | Loss: 0.00028226
Iteration 22/25 | Loss: 0.00028226
Iteration 23/25 | Loss: 0.00028226
Iteration 24/25 | Loss: 0.00028226
Iteration 25/25 | Loss: 0.00028226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028226
Iteration 2/1000 | Loss: 0.00001910
Iteration 3/1000 | Loss: 0.00001231
Iteration 4/1000 | Loss: 0.00001147
Iteration 5/1000 | Loss: 0.00001084
Iteration 6/1000 | Loss: 0.00001057
Iteration 7/1000 | Loss: 0.00001038
Iteration 8/1000 | Loss: 0.00001034
Iteration 9/1000 | Loss: 0.00001029
Iteration 10/1000 | Loss: 0.00001028
Iteration 11/1000 | Loss: 0.00001022
Iteration 12/1000 | Loss: 0.00001019
Iteration 13/1000 | Loss: 0.00001019
Iteration 14/1000 | Loss: 0.00001019
Iteration 15/1000 | Loss: 0.00001018
Iteration 16/1000 | Loss: 0.00001018
Iteration 17/1000 | Loss: 0.00001018
Iteration 18/1000 | Loss: 0.00001018
Iteration 19/1000 | Loss: 0.00001018
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001017
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00001016
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001014
Iteration 27/1000 | Loss: 0.00001014
Iteration 28/1000 | Loss: 0.00001013
Iteration 29/1000 | Loss: 0.00001013
Iteration 30/1000 | Loss: 0.00001012
Iteration 31/1000 | Loss: 0.00001008
Iteration 32/1000 | Loss: 0.00001008
Iteration 33/1000 | Loss: 0.00001008
Iteration 34/1000 | Loss: 0.00001007
Iteration 35/1000 | Loss: 0.00001007
Iteration 36/1000 | Loss: 0.00001006
Iteration 37/1000 | Loss: 0.00001005
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00001003
Iteration 41/1000 | Loss: 0.00001002
Iteration 42/1000 | Loss: 0.00001000
Iteration 43/1000 | Loss: 0.00001000
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00000999
Iteration 46/1000 | Loss: 0.00000999
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000998
Iteration 49/1000 | Loss: 0.00000998
Iteration 50/1000 | Loss: 0.00000998
Iteration 51/1000 | Loss: 0.00000998
Iteration 52/1000 | Loss: 0.00000998
Iteration 53/1000 | Loss: 0.00000997
Iteration 54/1000 | Loss: 0.00000997
Iteration 55/1000 | Loss: 0.00000996
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000995
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000994
Iteration 60/1000 | Loss: 0.00000994
Iteration 61/1000 | Loss: 0.00000993
Iteration 62/1000 | Loss: 0.00000993
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000992
Iteration 67/1000 | Loss: 0.00000992
Iteration 68/1000 | Loss: 0.00000991
Iteration 69/1000 | Loss: 0.00000991
Iteration 70/1000 | Loss: 0.00000991
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000990
Iteration 76/1000 | Loss: 0.00000990
Iteration 77/1000 | Loss: 0.00000990
Iteration 78/1000 | Loss: 0.00000990
Iteration 79/1000 | Loss: 0.00000989
Iteration 80/1000 | Loss: 0.00000989
Iteration 81/1000 | Loss: 0.00000989
Iteration 82/1000 | Loss: 0.00000989
Iteration 83/1000 | Loss: 0.00000989
Iteration 84/1000 | Loss: 0.00000989
Iteration 85/1000 | Loss: 0.00000989
Iteration 86/1000 | Loss: 0.00000989
Iteration 87/1000 | Loss: 0.00000988
Iteration 88/1000 | Loss: 0.00000988
Iteration 89/1000 | Loss: 0.00000988
Iteration 90/1000 | Loss: 0.00000988
Iteration 91/1000 | Loss: 0.00000988
Iteration 92/1000 | Loss: 0.00000988
Iteration 93/1000 | Loss: 0.00000988
Iteration 94/1000 | Loss: 0.00000988
Iteration 95/1000 | Loss: 0.00000987
Iteration 96/1000 | Loss: 0.00000987
Iteration 97/1000 | Loss: 0.00000987
Iteration 98/1000 | Loss: 0.00000987
Iteration 99/1000 | Loss: 0.00000987
Iteration 100/1000 | Loss: 0.00000987
Iteration 101/1000 | Loss: 0.00000987
Iteration 102/1000 | Loss: 0.00000987
Iteration 103/1000 | Loss: 0.00000987
Iteration 104/1000 | Loss: 0.00000987
Iteration 105/1000 | Loss: 0.00000987
Iteration 106/1000 | Loss: 0.00000987
Iteration 107/1000 | Loss: 0.00000987
Iteration 108/1000 | Loss: 0.00000987
Iteration 109/1000 | Loss: 0.00000987
Iteration 110/1000 | Loss: 0.00000987
Iteration 111/1000 | Loss: 0.00000987
Iteration 112/1000 | Loss: 0.00000987
Iteration 113/1000 | Loss: 0.00000987
Iteration 114/1000 | Loss: 0.00000986
Iteration 115/1000 | Loss: 0.00000986
Iteration 116/1000 | Loss: 0.00000986
Iteration 117/1000 | Loss: 0.00000986
Iteration 118/1000 | Loss: 0.00000986
Iteration 119/1000 | Loss: 0.00000985
Iteration 120/1000 | Loss: 0.00000985
Iteration 121/1000 | Loss: 0.00000984
Iteration 122/1000 | Loss: 0.00000984
Iteration 123/1000 | Loss: 0.00000984
Iteration 124/1000 | Loss: 0.00000983
Iteration 125/1000 | Loss: 0.00000983
Iteration 126/1000 | Loss: 0.00000983
Iteration 127/1000 | Loss: 0.00000983
Iteration 128/1000 | Loss: 0.00000983
Iteration 129/1000 | Loss: 0.00000983
Iteration 130/1000 | Loss: 0.00000983
Iteration 131/1000 | Loss: 0.00000983
Iteration 132/1000 | Loss: 0.00000983
Iteration 133/1000 | Loss: 0.00000982
Iteration 134/1000 | Loss: 0.00000982
Iteration 135/1000 | Loss: 0.00000982
Iteration 136/1000 | Loss: 0.00000982
Iteration 137/1000 | Loss: 0.00000981
Iteration 138/1000 | Loss: 0.00000981
Iteration 139/1000 | Loss: 0.00000981
Iteration 140/1000 | Loss: 0.00000981
Iteration 141/1000 | Loss: 0.00000981
Iteration 142/1000 | Loss: 0.00000981
Iteration 143/1000 | Loss: 0.00000980
Iteration 144/1000 | Loss: 0.00000980
Iteration 145/1000 | Loss: 0.00000980
Iteration 146/1000 | Loss: 0.00000979
Iteration 147/1000 | Loss: 0.00000979
Iteration 148/1000 | Loss: 0.00000979
Iteration 149/1000 | Loss: 0.00000979
Iteration 150/1000 | Loss: 0.00000979
Iteration 151/1000 | Loss: 0.00000979
Iteration 152/1000 | Loss: 0.00000979
Iteration 153/1000 | Loss: 0.00000978
Iteration 154/1000 | Loss: 0.00000978
Iteration 155/1000 | Loss: 0.00000978
Iteration 156/1000 | Loss: 0.00000978
Iteration 157/1000 | Loss: 0.00000978
Iteration 158/1000 | Loss: 0.00000978
Iteration 159/1000 | Loss: 0.00000978
Iteration 160/1000 | Loss: 0.00000978
Iteration 161/1000 | Loss: 0.00000978
Iteration 162/1000 | Loss: 0.00000978
Iteration 163/1000 | Loss: 0.00000978
Iteration 164/1000 | Loss: 0.00000978
Iteration 165/1000 | Loss: 0.00000977
Iteration 166/1000 | Loss: 0.00000977
Iteration 167/1000 | Loss: 0.00000977
Iteration 168/1000 | Loss: 0.00000977
Iteration 169/1000 | Loss: 0.00000977
Iteration 170/1000 | Loss: 0.00000977
Iteration 171/1000 | Loss: 0.00000977
Iteration 172/1000 | Loss: 0.00000977
Iteration 173/1000 | Loss: 0.00000976
Iteration 174/1000 | Loss: 0.00000976
Iteration 175/1000 | Loss: 0.00000976
Iteration 176/1000 | Loss: 0.00000976
Iteration 177/1000 | Loss: 0.00000976
Iteration 178/1000 | Loss: 0.00000976
Iteration 179/1000 | Loss: 0.00000976
Iteration 180/1000 | Loss: 0.00000976
Iteration 181/1000 | Loss: 0.00000976
Iteration 182/1000 | Loss: 0.00000976
Iteration 183/1000 | Loss: 0.00000976
Iteration 184/1000 | Loss: 0.00000976
Iteration 185/1000 | Loss: 0.00000976
Iteration 186/1000 | Loss: 0.00000976
Iteration 187/1000 | Loss: 0.00000976
Iteration 188/1000 | Loss: 0.00000976
Iteration 189/1000 | Loss: 0.00000976
Iteration 190/1000 | Loss: 0.00000976
Iteration 191/1000 | Loss: 0.00000976
Iteration 192/1000 | Loss: 0.00000976
Iteration 193/1000 | Loss: 0.00000976
Iteration 194/1000 | Loss: 0.00000976
Iteration 195/1000 | Loss: 0.00000975
Iteration 196/1000 | Loss: 0.00000975
Iteration 197/1000 | Loss: 0.00000975
Iteration 198/1000 | Loss: 0.00000975
Iteration 199/1000 | Loss: 0.00000975
Iteration 200/1000 | Loss: 0.00000975
Iteration 201/1000 | Loss: 0.00000975
Iteration 202/1000 | Loss: 0.00000975
Iteration 203/1000 | Loss: 0.00000975
Iteration 204/1000 | Loss: 0.00000975
Iteration 205/1000 | Loss: 0.00000975
Iteration 206/1000 | Loss: 0.00000975
Iteration 207/1000 | Loss: 0.00000975
Iteration 208/1000 | Loss: 0.00000975
Iteration 209/1000 | Loss: 0.00000975
Iteration 210/1000 | Loss: 0.00000975
Iteration 211/1000 | Loss: 0.00000975
Iteration 212/1000 | Loss: 0.00000975
Iteration 213/1000 | Loss: 0.00000975
Iteration 214/1000 | Loss: 0.00000975
Iteration 215/1000 | Loss: 0.00000975
Iteration 216/1000 | Loss: 0.00000975
Iteration 217/1000 | Loss: 0.00000975
Iteration 218/1000 | Loss: 0.00000975
Iteration 219/1000 | Loss: 0.00000975
Iteration 220/1000 | Loss: 0.00000975
Iteration 221/1000 | Loss: 0.00000975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [9.751574907568283e-06, 9.751574907568283e-06, 9.751574907568283e-06, 9.751574907568283e-06, 9.751574907568283e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.751574907568283e-06

Optimization complete. Final v2v error: 2.6301026344299316 mm

Highest mean error: 2.7947402000427246 mm for frame 30

Lowest mean error: 2.536454916000366 mm for frame 4

Saving results

Total time: 35.21204924583435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389911
Iteration 2/25 | Loss: 0.00082565
Iteration 3/25 | Loss: 0.00065283
Iteration 4/25 | Loss: 0.00062281
Iteration 5/25 | Loss: 0.00061423
Iteration 6/25 | Loss: 0.00061103
Iteration 7/25 | Loss: 0.00061036
Iteration 8/25 | Loss: 0.00061036
Iteration 9/25 | Loss: 0.00061036
Iteration 10/25 | Loss: 0.00061036
Iteration 11/25 | Loss: 0.00061036
Iteration 12/25 | Loss: 0.00061036
Iteration 13/25 | Loss: 0.00061036
Iteration 14/25 | Loss: 0.00061036
Iteration 15/25 | Loss: 0.00061036
Iteration 16/25 | Loss: 0.00061036
Iteration 17/25 | Loss: 0.00061036
Iteration 18/25 | Loss: 0.00061036
Iteration 19/25 | Loss: 0.00061036
Iteration 20/25 | Loss: 0.00061036
Iteration 21/25 | Loss: 0.00061036
Iteration 22/25 | Loss: 0.00061036
Iteration 23/25 | Loss: 0.00061036
Iteration 24/25 | Loss: 0.00061036
Iteration 25/25 | Loss: 0.00061036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44728994
Iteration 2/25 | Loss: 0.00023812
Iteration 3/25 | Loss: 0.00023812
Iteration 4/25 | Loss: 0.00023812
Iteration 5/25 | Loss: 0.00023812
Iteration 6/25 | Loss: 0.00023812
Iteration 7/25 | Loss: 0.00023812
Iteration 8/25 | Loss: 0.00023812
Iteration 9/25 | Loss: 0.00023812
Iteration 10/25 | Loss: 0.00023812
Iteration 11/25 | Loss: 0.00023812
Iteration 12/25 | Loss: 0.00023812
Iteration 13/25 | Loss: 0.00023812
Iteration 14/25 | Loss: 0.00023812
Iteration 15/25 | Loss: 0.00023812
Iteration 16/25 | Loss: 0.00023812
Iteration 17/25 | Loss: 0.00023812
Iteration 18/25 | Loss: 0.00023812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00023811767459847033, 0.00023811767459847033, 0.00023811767459847033, 0.00023811767459847033, 0.00023811767459847033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023811767459847033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023812
Iteration 2/1000 | Loss: 0.00002482
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001663
Iteration 6/1000 | Loss: 0.00001604
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001542
Iteration 10/1000 | Loss: 0.00001525
Iteration 11/1000 | Loss: 0.00001521
Iteration 12/1000 | Loss: 0.00001514
Iteration 13/1000 | Loss: 0.00001514
Iteration 14/1000 | Loss: 0.00001514
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001514
Iteration 20/1000 | Loss: 0.00001514
Iteration 21/1000 | Loss: 0.00001514
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001513
Iteration 24/1000 | Loss: 0.00001513
Iteration 25/1000 | Loss: 0.00001507
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001494
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001490
Iteration 49/1000 | Loss: 0.00001490
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001490
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001487
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001486
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001483
Iteration 66/1000 | Loss: 0.00001483
Iteration 67/1000 | Loss: 0.00001482
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001477
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001477
Iteration 82/1000 | Loss: 0.00001477
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001474
Iteration 87/1000 | Loss: 0.00001474
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001473
Iteration 92/1000 | Loss: 0.00001473
Iteration 93/1000 | Loss: 0.00001473
Iteration 94/1000 | Loss: 0.00001473
Iteration 95/1000 | Loss: 0.00001473
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001472
Iteration 104/1000 | Loss: 0.00001472
Iteration 105/1000 | Loss: 0.00001472
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001471
Iteration 111/1000 | Loss: 0.00001471
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001471
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001471
Iteration 116/1000 | Loss: 0.00001471
Iteration 117/1000 | Loss: 0.00001470
Iteration 118/1000 | Loss: 0.00001470
Iteration 119/1000 | Loss: 0.00001470
Iteration 120/1000 | Loss: 0.00001470
Iteration 121/1000 | Loss: 0.00001470
Iteration 122/1000 | Loss: 0.00001470
Iteration 123/1000 | Loss: 0.00001470
Iteration 124/1000 | Loss: 0.00001470
Iteration 125/1000 | Loss: 0.00001470
Iteration 126/1000 | Loss: 0.00001470
Iteration 127/1000 | Loss: 0.00001469
Iteration 128/1000 | Loss: 0.00001469
Iteration 129/1000 | Loss: 0.00001469
Iteration 130/1000 | Loss: 0.00001469
Iteration 131/1000 | Loss: 0.00001468
Iteration 132/1000 | Loss: 0.00001468
Iteration 133/1000 | Loss: 0.00001468
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001468
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001467
Iteration 143/1000 | Loss: 0.00001467
Iteration 144/1000 | Loss: 0.00001467
Iteration 145/1000 | Loss: 0.00001467
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001467
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001466
Iteration 154/1000 | Loss: 0.00001466
Iteration 155/1000 | Loss: 0.00001466
Iteration 156/1000 | Loss: 0.00001466
Iteration 157/1000 | Loss: 0.00001466
Iteration 158/1000 | Loss: 0.00001466
Iteration 159/1000 | Loss: 0.00001466
Iteration 160/1000 | Loss: 0.00001466
Iteration 161/1000 | Loss: 0.00001466
Iteration 162/1000 | Loss: 0.00001466
Iteration 163/1000 | Loss: 0.00001465
Iteration 164/1000 | Loss: 0.00001465
Iteration 165/1000 | Loss: 0.00001465
Iteration 166/1000 | Loss: 0.00001465
Iteration 167/1000 | Loss: 0.00001465
Iteration 168/1000 | Loss: 0.00001465
Iteration 169/1000 | Loss: 0.00001465
Iteration 170/1000 | Loss: 0.00001465
Iteration 171/1000 | Loss: 0.00001465
Iteration 172/1000 | Loss: 0.00001465
Iteration 173/1000 | Loss: 0.00001465
Iteration 174/1000 | Loss: 0.00001465
Iteration 175/1000 | Loss: 0.00001465
Iteration 176/1000 | Loss: 0.00001465
Iteration 177/1000 | Loss: 0.00001465
Iteration 178/1000 | Loss: 0.00001465
Iteration 179/1000 | Loss: 0.00001465
Iteration 180/1000 | Loss: 0.00001465
Iteration 181/1000 | Loss: 0.00001465
Iteration 182/1000 | Loss: 0.00001465
Iteration 183/1000 | Loss: 0.00001465
Iteration 184/1000 | Loss: 0.00001465
Iteration 185/1000 | Loss: 0.00001465
Iteration 186/1000 | Loss: 0.00001465
Iteration 187/1000 | Loss: 0.00001465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.4649072909378447e-05, 1.4649072909378447e-05, 1.4649072909378447e-05, 1.4649072909378447e-05, 1.4649072909378447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4649072909378447e-05

Optimization complete. Final v2v error: 3.2132070064544678 mm

Highest mean error: 3.5254878997802734 mm for frame 117

Lowest mean error: 2.9272358417510986 mm for frame 1

Saving results

Total time: 38.52915954589844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721996
Iteration 2/25 | Loss: 0.00103267
Iteration 3/25 | Loss: 0.00070656
Iteration 4/25 | Loss: 0.00068352
Iteration 5/25 | Loss: 0.00065411
Iteration 6/25 | Loss: 0.00065064
Iteration 7/25 | Loss: 0.00063488
Iteration 8/25 | Loss: 0.00063381
Iteration 9/25 | Loss: 0.00063356
Iteration 10/25 | Loss: 0.00063835
Iteration 11/25 | Loss: 0.00063835
Iteration 12/25 | Loss: 0.00063750
Iteration 13/25 | Loss: 0.00063471
Iteration 14/25 | Loss: 0.00063889
Iteration 15/25 | Loss: 0.00063380
Iteration 16/25 | Loss: 0.00063319
Iteration 17/25 | Loss: 0.00063546
Iteration 18/25 | Loss: 0.00063243
Iteration 19/25 | Loss: 0.00063199
Iteration 20/25 | Loss: 0.00063185
Iteration 21/25 | Loss: 0.00063185
Iteration 22/25 | Loss: 0.00063185
Iteration 23/25 | Loss: 0.00063185
Iteration 24/25 | Loss: 0.00063185
Iteration 25/25 | Loss: 0.00063185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97124946
Iteration 2/25 | Loss: 0.00029534
Iteration 3/25 | Loss: 0.00029534
Iteration 4/25 | Loss: 0.00029534
Iteration 5/25 | Loss: 0.00029534
Iteration 6/25 | Loss: 0.00029534
Iteration 7/25 | Loss: 0.00029533
Iteration 8/25 | Loss: 0.00029533
Iteration 9/25 | Loss: 0.00029533
Iteration 10/25 | Loss: 0.00029533
Iteration 11/25 | Loss: 0.00029533
Iteration 12/25 | Loss: 0.00029533
Iteration 13/25 | Loss: 0.00029533
Iteration 14/25 | Loss: 0.00029533
Iteration 15/25 | Loss: 0.00029533
Iteration 16/25 | Loss: 0.00029533
Iteration 17/25 | Loss: 0.00029533
Iteration 18/25 | Loss: 0.00029533
Iteration 19/25 | Loss: 0.00029533
Iteration 20/25 | Loss: 0.00029533
Iteration 21/25 | Loss: 0.00029533
Iteration 22/25 | Loss: 0.00029533
Iteration 23/25 | Loss: 0.00029533
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00029533397173509, 0.00029533397173509, 0.00029533397173509, 0.00029533397173509, 0.00029533397173509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029533397173509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029533
Iteration 2/1000 | Loss: 0.00002871
Iteration 3/1000 | Loss: 0.00001936
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001643
Iteration 6/1000 | Loss: 0.00001636
Iteration 7/1000 | Loss: 0.00001586
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001536
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001502
Iteration 12/1000 | Loss: 0.00001497
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001476
Iteration 15/1000 | Loss: 0.00001472
Iteration 16/1000 | Loss: 0.00001472
Iteration 17/1000 | Loss: 0.00001469
Iteration 18/1000 | Loss: 0.00001466
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001461
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001459
Iteration 24/1000 | Loss: 0.00001457
Iteration 25/1000 | Loss: 0.00001456
Iteration 26/1000 | Loss: 0.00001456
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001455
Iteration 29/1000 | Loss: 0.00001453
Iteration 30/1000 | Loss: 0.00001451
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001444
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001442
Iteration 53/1000 | Loss: 0.00001442
Iteration 54/1000 | Loss: 0.00001441
Iteration 55/1000 | Loss: 0.00001441
Iteration 56/1000 | Loss: 0.00001441
Iteration 57/1000 | Loss: 0.00001441
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001440
Iteration 73/1000 | Loss: 0.00001440
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001440
Iteration 80/1000 | Loss: 0.00001440
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001440
Iteration 84/1000 | Loss: 0.00001440
Iteration 85/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.4397394807019737e-05, 1.4397394807019737e-05, 1.4397394807019737e-05, 1.4397394807019737e-05, 1.4397394807019737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4397394807019737e-05

Optimization complete. Final v2v error: 3.2304575443267822 mm

Highest mean error: 3.5339763164520264 mm for frame 127

Lowest mean error: 3.0561182498931885 mm for frame 262

Saving results

Total time: 64.12156009674072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068498
Iteration 2/25 | Loss: 0.00210092
Iteration 3/25 | Loss: 0.00132176
Iteration 4/25 | Loss: 0.00115071
Iteration 5/25 | Loss: 0.00112540
Iteration 6/25 | Loss: 0.00108935
Iteration 7/25 | Loss: 0.00108317
Iteration 8/25 | Loss: 0.00093094
Iteration 9/25 | Loss: 0.00095924
Iteration 10/25 | Loss: 0.00086699
Iteration 11/25 | Loss: 0.00080133
Iteration 12/25 | Loss: 0.00072565
Iteration 13/25 | Loss: 0.00076006
Iteration 14/25 | Loss: 0.00069725
Iteration 15/25 | Loss: 0.00067587
Iteration 16/25 | Loss: 0.00066177
Iteration 17/25 | Loss: 0.00066599
Iteration 18/25 | Loss: 0.00066342
Iteration 19/25 | Loss: 0.00071643
Iteration 20/25 | Loss: 0.00066509
Iteration 21/25 | Loss: 0.00065684
Iteration 22/25 | Loss: 0.00065448
Iteration 23/25 | Loss: 0.00064888
Iteration 24/25 | Loss: 0.00065222
Iteration 25/25 | Loss: 0.00064867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54469752
Iteration 2/25 | Loss: 0.00028868
Iteration 3/25 | Loss: 0.00028868
Iteration 4/25 | Loss: 0.00028867
Iteration 5/25 | Loss: 0.00028867
Iteration 6/25 | Loss: 0.00028867
Iteration 7/25 | Loss: 0.00028867
Iteration 8/25 | Loss: 0.00028867
Iteration 9/25 | Loss: 0.00028867
Iteration 10/25 | Loss: 0.00028867
Iteration 11/25 | Loss: 0.00028867
Iteration 12/25 | Loss: 0.00028867
Iteration 13/25 | Loss: 0.00028867
Iteration 14/25 | Loss: 0.00028867
Iteration 15/25 | Loss: 0.00028867
Iteration 16/25 | Loss: 0.00028867
Iteration 17/25 | Loss: 0.00028867
Iteration 18/25 | Loss: 0.00028867
Iteration 19/25 | Loss: 0.00028867
Iteration 20/25 | Loss: 0.00028867
Iteration 21/25 | Loss: 0.00028867
Iteration 22/25 | Loss: 0.00028867
Iteration 23/25 | Loss: 0.00028867
Iteration 24/25 | Loss: 0.00028867
Iteration 25/25 | Loss: 0.00028867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028867
Iteration 2/1000 | Loss: 0.00004130
Iteration 3/1000 | Loss: 0.00003774
Iteration 4/1000 | Loss: 0.00005792
Iteration 5/1000 | Loss: 0.00003078
Iteration 6/1000 | Loss: 0.00004236
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002042
Iteration 9/1000 | Loss: 0.00003792
Iteration 10/1000 | Loss: 0.00038803
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00003100
Iteration 13/1000 | Loss: 0.00008981
Iteration 14/1000 | Loss: 0.00002010
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00001894
Iteration 17/1000 | Loss: 0.00002878
Iteration 18/1000 | Loss: 0.00005830
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00002029
Iteration 21/1000 | Loss: 0.00001869
Iteration 22/1000 | Loss: 0.00001869
Iteration 23/1000 | Loss: 0.00001868
Iteration 24/1000 | Loss: 0.00001868
Iteration 25/1000 | Loss: 0.00001868
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001867
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001866
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00002651
Iteration 38/1000 | Loss: 0.00003095
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00001860
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001858
Iteration 45/1000 | Loss: 0.00001858
Iteration 46/1000 | Loss: 0.00001858
Iteration 47/1000 | Loss: 0.00001858
Iteration 48/1000 | Loss: 0.00001858
Iteration 49/1000 | Loss: 0.00001857
Iteration 50/1000 | Loss: 0.00001857
Iteration 51/1000 | Loss: 0.00001857
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001855
Iteration 56/1000 | Loss: 0.00001854
Iteration 57/1000 | Loss: 0.00001854
Iteration 58/1000 | Loss: 0.00001854
Iteration 59/1000 | Loss: 0.00001854
Iteration 60/1000 | Loss: 0.00001854
Iteration 61/1000 | Loss: 0.00001854
Iteration 62/1000 | Loss: 0.00001853
Iteration 63/1000 | Loss: 0.00001853
Iteration 64/1000 | Loss: 0.00001853
Iteration 65/1000 | Loss: 0.00001852
Iteration 66/1000 | Loss: 0.00003163
Iteration 67/1000 | Loss: 0.00003163
Iteration 68/1000 | Loss: 0.00002322
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001842
Iteration 72/1000 | Loss: 0.00001842
Iteration 73/1000 | Loss: 0.00002340
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00002197
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001859
Iteration 78/1000 | Loss: 0.00001850
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001848
Iteration 82/1000 | Loss: 0.00001848
Iteration 83/1000 | Loss: 0.00004358
Iteration 84/1000 | Loss: 0.00005168
Iteration 85/1000 | Loss: 0.00007641
Iteration 86/1000 | Loss: 0.00004282
Iteration 87/1000 | Loss: 0.00004848
Iteration 88/1000 | Loss: 0.00001926
Iteration 89/1000 | Loss: 0.00002103
Iteration 90/1000 | Loss: 0.00001844
Iteration 91/1000 | Loss: 0.00001896
Iteration 92/1000 | Loss: 0.00001836
Iteration 93/1000 | Loss: 0.00001836
Iteration 94/1000 | Loss: 0.00001836
Iteration 95/1000 | Loss: 0.00001836
Iteration 96/1000 | Loss: 0.00001836
Iteration 97/1000 | Loss: 0.00001836
Iteration 98/1000 | Loss: 0.00001836
Iteration 99/1000 | Loss: 0.00001836
Iteration 100/1000 | Loss: 0.00001836
Iteration 101/1000 | Loss: 0.00001836
Iteration 102/1000 | Loss: 0.00001836
Iteration 103/1000 | Loss: 0.00001836
Iteration 104/1000 | Loss: 0.00001836
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001836
Iteration 110/1000 | Loss: 0.00001836
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001836
Iteration 113/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.835525836213492e-05, 1.835525836213492e-05, 1.835525836213492e-05, 1.835525836213492e-05, 1.835525836213492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.835525836213492e-05

Optimization complete. Final v2v error: 3.586324453353882 mm

Highest mean error: 4.9163994789123535 mm for frame 66

Lowest mean error: 2.9852142333984375 mm for frame 111

Saving results

Total time: 99.21899342536926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880520
Iteration 2/25 | Loss: 0.00114882
Iteration 3/25 | Loss: 0.00075691
Iteration 4/25 | Loss: 0.00070177
Iteration 5/25 | Loss: 0.00068157
Iteration 6/25 | Loss: 0.00067384
Iteration 7/25 | Loss: 0.00067130
Iteration 8/25 | Loss: 0.00067014
Iteration 9/25 | Loss: 0.00066952
Iteration 10/25 | Loss: 0.00066940
Iteration 11/25 | Loss: 0.00066940
Iteration 12/25 | Loss: 0.00066940
Iteration 13/25 | Loss: 0.00066940
Iteration 14/25 | Loss: 0.00066940
Iteration 15/25 | Loss: 0.00066940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006693964242003858, 0.0006693964242003858, 0.0006693964242003858, 0.0006693964242003858, 0.0006693964242003858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006693964242003858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48985672
Iteration 2/25 | Loss: 0.00028063
Iteration 3/25 | Loss: 0.00028063
Iteration 4/25 | Loss: 0.00028063
Iteration 5/25 | Loss: 0.00028063
Iteration 6/25 | Loss: 0.00028063
Iteration 7/25 | Loss: 0.00028063
Iteration 8/25 | Loss: 0.00028063
Iteration 9/25 | Loss: 0.00028063
Iteration 10/25 | Loss: 0.00028063
Iteration 11/25 | Loss: 0.00028063
Iteration 12/25 | Loss: 0.00028063
Iteration 13/25 | Loss: 0.00028063
Iteration 14/25 | Loss: 0.00028063
Iteration 15/25 | Loss: 0.00028063
Iteration 16/25 | Loss: 0.00028063
Iteration 17/25 | Loss: 0.00028063
Iteration 18/25 | Loss: 0.00028063
Iteration 19/25 | Loss: 0.00028063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002806261763907969, 0.0002806261763907969, 0.0002806261763907969, 0.0002806261763907969, 0.0002806261763907969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002806261763907969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028063
Iteration 2/1000 | Loss: 0.00004516
Iteration 3/1000 | Loss: 0.00002827
Iteration 4/1000 | Loss: 0.00002274
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002044
Iteration 7/1000 | Loss: 0.00001985
Iteration 8/1000 | Loss: 0.00001950
Iteration 9/1000 | Loss: 0.00001919
Iteration 10/1000 | Loss: 0.00001889
Iteration 11/1000 | Loss: 0.00001863
Iteration 12/1000 | Loss: 0.00001845
Iteration 13/1000 | Loss: 0.00001841
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001811
Iteration 19/1000 | Loss: 0.00001805
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001801
Iteration 22/1000 | Loss: 0.00001801
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001800
Iteration 25/1000 | Loss: 0.00001800
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001799
Iteration 28/1000 | Loss: 0.00001799
Iteration 29/1000 | Loss: 0.00001799
Iteration 30/1000 | Loss: 0.00001797
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001796
Iteration 33/1000 | Loss: 0.00001796
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001795
Iteration 37/1000 | Loss: 0.00001795
Iteration 38/1000 | Loss: 0.00001794
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001794
Iteration 42/1000 | Loss: 0.00001794
Iteration 43/1000 | Loss: 0.00001794
Iteration 44/1000 | Loss: 0.00001794
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001793
Iteration 47/1000 | Loss: 0.00001793
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001793
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00001791
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001790
Iteration 56/1000 | Loss: 0.00001790
Iteration 57/1000 | Loss: 0.00001790
Iteration 58/1000 | Loss: 0.00001790
Iteration 59/1000 | Loss: 0.00001789
Iteration 60/1000 | Loss: 0.00001789
Iteration 61/1000 | Loss: 0.00001789
Iteration 62/1000 | Loss: 0.00001789
Iteration 63/1000 | Loss: 0.00001788
Iteration 64/1000 | Loss: 0.00001788
Iteration 65/1000 | Loss: 0.00001788
Iteration 66/1000 | Loss: 0.00001788
Iteration 67/1000 | Loss: 0.00001788
Iteration 68/1000 | Loss: 0.00001787
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001787
Iteration 72/1000 | Loss: 0.00001787
Iteration 73/1000 | Loss: 0.00001787
Iteration 74/1000 | Loss: 0.00001787
Iteration 75/1000 | Loss: 0.00001787
Iteration 76/1000 | Loss: 0.00001787
Iteration 77/1000 | Loss: 0.00001786
Iteration 78/1000 | Loss: 0.00001786
Iteration 79/1000 | Loss: 0.00001786
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001786
Iteration 82/1000 | Loss: 0.00001786
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001785
Iteration 85/1000 | Loss: 0.00001785
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001784
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001784
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001784
Iteration 109/1000 | Loss: 0.00001784
Iteration 110/1000 | Loss: 0.00001784
Iteration 111/1000 | Loss: 0.00001784
Iteration 112/1000 | Loss: 0.00001784
Iteration 113/1000 | Loss: 0.00001784
Iteration 114/1000 | Loss: 0.00001784
Iteration 115/1000 | Loss: 0.00001784
Iteration 116/1000 | Loss: 0.00001784
Iteration 117/1000 | Loss: 0.00001784
Iteration 118/1000 | Loss: 0.00001784
Iteration 119/1000 | Loss: 0.00001784
Iteration 120/1000 | Loss: 0.00001784
Iteration 121/1000 | Loss: 0.00001784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.7839320207713172e-05, 1.7839320207713172e-05, 1.7839320207713172e-05, 1.7839320207713172e-05, 1.7839320207713172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7839320207713172e-05

Optimization complete. Final v2v error: 3.4908227920532227 mm

Highest mean error: 6.079352855682373 mm for frame 70

Lowest mean error: 2.642939567565918 mm for frame 128

Saving results

Total time: 40.85448145866394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058227
Iteration 2/25 | Loss: 0.00181000
Iteration 3/25 | Loss: 0.00116943
Iteration 4/25 | Loss: 0.00105051
Iteration 5/25 | Loss: 0.00092842
Iteration 6/25 | Loss: 0.00092931
Iteration 7/25 | Loss: 0.00093876
Iteration 8/25 | Loss: 0.00082753
Iteration 9/25 | Loss: 0.00074568
Iteration 10/25 | Loss: 0.00073447
Iteration 11/25 | Loss: 0.00071219
Iteration 12/25 | Loss: 0.00070500
Iteration 13/25 | Loss: 0.00069685
Iteration 14/25 | Loss: 0.00068996
Iteration 15/25 | Loss: 0.00069431
Iteration 16/25 | Loss: 0.00069062
Iteration 17/25 | Loss: 0.00068187
Iteration 18/25 | Loss: 0.00067659
Iteration 19/25 | Loss: 0.00066958
Iteration 20/25 | Loss: 0.00066622
Iteration 21/25 | Loss: 0.00066432
Iteration 22/25 | Loss: 0.00066265
Iteration 23/25 | Loss: 0.00066208
Iteration 24/25 | Loss: 0.00066186
Iteration 25/25 | Loss: 0.00066167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56906402
Iteration 2/25 | Loss: 0.00050051
Iteration 3/25 | Loss: 0.00050051
Iteration 4/25 | Loss: 0.00050051
Iteration 5/25 | Loss: 0.00050051
Iteration 6/25 | Loss: 0.00050051
Iteration 7/25 | Loss: 0.00050051
Iteration 8/25 | Loss: 0.00050051
Iteration 9/25 | Loss: 0.00050051
Iteration 10/25 | Loss: 0.00050051
Iteration 11/25 | Loss: 0.00050051
Iteration 12/25 | Loss: 0.00050051
Iteration 13/25 | Loss: 0.00050051
Iteration 14/25 | Loss: 0.00050051
Iteration 15/25 | Loss: 0.00050051
Iteration 16/25 | Loss: 0.00050051
Iteration 17/25 | Loss: 0.00050051
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005005072453059256, 0.0005005072453059256, 0.0005005072453059256, 0.0005005072453059256, 0.0005005072453059256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005005072453059256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050051
Iteration 2/1000 | Loss: 0.00004424
Iteration 3/1000 | Loss: 0.00003248
Iteration 4/1000 | Loss: 0.00046019
Iteration 5/1000 | Loss: 0.00122693
Iteration 6/1000 | Loss: 0.00077491
Iteration 7/1000 | Loss: 0.00125639
Iteration 8/1000 | Loss: 0.00126481
Iteration 9/1000 | Loss: 0.00110708
Iteration 10/1000 | Loss: 0.00082145
Iteration 11/1000 | Loss: 0.00095470
Iteration 12/1000 | Loss: 0.00015551
Iteration 13/1000 | Loss: 0.00005081
Iteration 14/1000 | Loss: 0.00120635
Iteration 15/1000 | Loss: 0.00015849
Iteration 16/1000 | Loss: 0.00004482
Iteration 17/1000 | Loss: 0.00033127
Iteration 18/1000 | Loss: 0.00025311
Iteration 19/1000 | Loss: 0.00015607
Iteration 20/1000 | Loss: 0.00021098
Iteration 21/1000 | Loss: 0.00012450
Iteration 22/1000 | Loss: 0.00016901
Iteration 23/1000 | Loss: 0.00017245
Iteration 24/1000 | Loss: 0.00003254
Iteration 25/1000 | Loss: 0.00061890
Iteration 26/1000 | Loss: 0.00056625
Iteration 27/1000 | Loss: 0.00049105
Iteration 28/1000 | Loss: 0.00036100
Iteration 29/1000 | Loss: 0.00029272
Iteration 30/1000 | Loss: 0.00051794
Iteration 31/1000 | Loss: 0.00051295
Iteration 32/1000 | Loss: 0.00038849
Iteration 33/1000 | Loss: 0.00068542
Iteration 34/1000 | Loss: 0.00033113
Iteration 35/1000 | Loss: 0.00161150
Iteration 36/1000 | Loss: 0.00074106
Iteration 37/1000 | Loss: 0.00022373
Iteration 38/1000 | Loss: 0.00049319
Iteration 39/1000 | Loss: 0.00011583
Iteration 40/1000 | Loss: 0.00049662
Iteration 41/1000 | Loss: 0.00061753
Iteration 42/1000 | Loss: 0.00053261
Iteration 43/1000 | Loss: 0.00003533
Iteration 44/1000 | Loss: 0.00019582
Iteration 45/1000 | Loss: 0.00016387
Iteration 46/1000 | Loss: 0.00002510
Iteration 47/1000 | Loss: 0.00002290
Iteration 48/1000 | Loss: 0.00018020
Iteration 49/1000 | Loss: 0.00001958
Iteration 50/1000 | Loss: 0.00001745
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001496
Iteration 55/1000 | Loss: 0.00001470
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001439
Iteration 58/1000 | Loss: 0.00001435
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001428
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001418
Iteration 71/1000 | Loss: 0.00001417
Iteration 72/1000 | Loss: 0.00001417
Iteration 73/1000 | Loss: 0.00001417
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001416
Iteration 76/1000 | Loss: 0.00001414
Iteration 77/1000 | Loss: 0.00001414
Iteration 78/1000 | Loss: 0.00001414
Iteration 79/1000 | Loss: 0.00001413
Iteration 80/1000 | Loss: 0.00001413
Iteration 81/1000 | Loss: 0.00001413
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001412
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001410
Iteration 89/1000 | Loss: 0.00001410
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001410
Iteration 95/1000 | Loss: 0.00001410
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001410
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001409
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001407
Iteration 110/1000 | Loss: 0.00001407
Iteration 111/1000 | Loss: 0.00001407
Iteration 112/1000 | Loss: 0.00024583
Iteration 113/1000 | Loss: 0.00002053
Iteration 114/1000 | Loss: 0.00001664
Iteration 115/1000 | Loss: 0.00001502
Iteration 116/1000 | Loss: 0.00001376
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001276
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001260
Iteration 122/1000 | Loss: 0.00001259
Iteration 123/1000 | Loss: 0.00001258
Iteration 124/1000 | Loss: 0.00001257
Iteration 125/1000 | Loss: 0.00001255
Iteration 126/1000 | Loss: 0.00001255
Iteration 127/1000 | Loss: 0.00001252
Iteration 128/1000 | Loss: 0.00001250
Iteration 129/1000 | Loss: 0.00001249
Iteration 130/1000 | Loss: 0.00001249
Iteration 131/1000 | Loss: 0.00001238
Iteration 132/1000 | Loss: 0.00001238
Iteration 133/1000 | Loss: 0.00001526
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001189
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001171
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001165
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001164
Iteration 144/1000 | Loss: 0.00001164
Iteration 145/1000 | Loss: 0.00001164
Iteration 146/1000 | Loss: 0.00001164
Iteration 147/1000 | Loss: 0.00001164
Iteration 148/1000 | Loss: 0.00001164
Iteration 149/1000 | Loss: 0.00001164
Iteration 150/1000 | Loss: 0.00001164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.164040804724209e-05, 1.164040804724209e-05, 1.164040804724209e-05, 1.164040804724209e-05, 1.164040804724209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.164040804724209e-05

Optimization complete. Final v2v error: 2.919492244720459 mm

Highest mean error: 4.631054878234863 mm for frame 49

Lowest mean error: 2.612572431564331 mm for frame 94

Saving results

Total time: 159.18852162361145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568367
Iteration 2/25 | Loss: 0.00098468
Iteration 3/25 | Loss: 0.00072831
Iteration 4/25 | Loss: 0.00069977
Iteration 5/25 | Loss: 0.00069339
Iteration 6/25 | Loss: 0.00069226
Iteration 7/25 | Loss: 0.00069226
Iteration 8/25 | Loss: 0.00069226
Iteration 9/25 | Loss: 0.00069226
Iteration 10/25 | Loss: 0.00069226
Iteration 11/25 | Loss: 0.00069226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006922637112438679, 0.0006922637112438679, 0.0006922637112438679, 0.0006922637112438679, 0.0006922637112438679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006922637112438679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65836966
Iteration 2/25 | Loss: 0.00021626
Iteration 3/25 | Loss: 0.00021625
Iteration 4/25 | Loss: 0.00021625
Iteration 5/25 | Loss: 0.00021625
Iteration 6/25 | Loss: 0.00021625
Iteration 7/25 | Loss: 0.00021625
Iteration 8/25 | Loss: 0.00021625
Iteration 9/25 | Loss: 0.00021625
Iteration 10/25 | Loss: 0.00021625
Iteration 11/25 | Loss: 0.00021625
Iteration 12/25 | Loss: 0.00021625
Iteration 13/25 | Loss: 0.00021625
Iteration 14/25 | Loss: 0.00021625
Iteration 15/25 | Loss: 0.00021625
Iteration 16/25 | Loss: 0.00021625
Iteration 17/25 | Loss: 0.00021625
Iteration 18/25 | Loss: 0.00021625
Iteration 19/25 | Loss: 0.00021625
Iteration 20/25 | Loss: 0.00021625
Iteration 21/25 | Loss: 0.00021625
Iteration 22/25 | Loss: 0.00021625
Iteration 23/25 | Loss: 0.00021625
Iteration 24/25 | Loss: 0.00021625
Iteration 25/25 | Loss: 0.00021625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021625
Iteration 2/1000 | Loss: 0.00002679
Iteration 3/1000 | Loss: 0.00002289
Iteration 4/1000 | Loss: 0.00002183
Iteration 5/1000 | Loss: 0.00002084
Iteration 6/1000 | Loss: 0.00002022
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001949
Iteration 9/1000 | Loss: 0.00001931
Iteration 10/1000 | Loss: 0.00001924
Iteration 11/1000 | Loss: 0.00001924
Iteration 12/1000 | Loss: 0.00001924
Iteration 13/1000 | Loss: 0.00001923
Iteration 14/1000 | Loss: 0.00001922
Iteration 15/1000 | Loss: 0.00001907
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001890
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00001881
Iteration 33/1000 | Loss: 0.00001879
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001879
Iteration 36/1000 | Loss: 0.00001879
Iteration 37/1000 | Loss: 0.00001879
Iteration 38/1000 | Loss: 0.00001878
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001878
Iteration 41/1000 | Loss: 0.00001878
Iteration 42/1000 | Loss: 0.00001878
Iteration 43/1000 | Loss: 0.00001878
Iteration 44/1000 | Loss: 0.00001878
Iteration 45/1000 | Loss: 0.00001878
Iteration 46/1000 | Loss: 0.00001878
Iteration 47/1000 | Loss: 0.00001878
Iteration 48/1000 | Loss: 0.00001878
Iteration 49/1000 | Loss: 0.00001878
Iteration 50/1000 | Loss: 0.00001878
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001877
Iteration 53/1000 | Loss: 0.00001876
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001872
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001872
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001870
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001870
Iteration 68/1000 | Loss: 0.00001870
Iteration 69/1000 | Loss: 0.00001870
Iteration 70/1000 | Loss: 0.00001870
Iteration 71/1000 | Loss: 0.00001869
Iteration 72/1000 | Loss: 0.00001869
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001866
Iteration 81/1000 | Loss: 0.00001866
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001863
Iteration 87/1000 | Loss: 0.00001863
Iteration 88/1000 | Loss: 0.00001863
Iteration 89/1000 | Loss: 0.00001863
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001862
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001861
Iteration 96/1000 | Loss: 0.00001861
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001860
Iteration 107/1000 | Loss: 0.00001860
Iteration 108/1000 | Loss: 0.00001860
Iteration 109/1000 | Loss: 0.00001860
Iteration 110/1000 | Loss: 0.00001860
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001859
Iteration 116/1000 | Loss: 0.00001859
Iteration 117/1000 | Loss: 0.00001859
Iteration 118/1000 | Loss: 0.00001858
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001853
Iteration 126/1000 | Loss: 0.00001853
Iteration 127/1000 | Loss: 0.00001853
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001852
Iteration 134/1000 | Loss: 0.00001852
Iteration 135/1000 | Loss: 0.00001851
Iteration 136/1000 | Loss: 0.00001851
Iteration 137/1000 | Loss: 0.00001851
Iteration 138/1000 | Loss: 0.00001851
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001849
Iteration 146/1000 | Loss: 0.00001849
Iteration 147/1000 | Loss: 0.00001849
Iteration 148/1000 | Loss: 0.00001849
Iteration 149/1000 | Loss: 0.00001849
Iteration 150/1000 | Loss: 0.00001849
Iteration 151/1000 | Loss: 0.00001849
Iteration 152/1000 | Loss: 0.00001849
Iteration 153/1000 | Loss: 0.00001849
Iteration 154/1000 | Loss: 0.00001849
Iteration 155/1000 | Loss: 0.00001849
Iteration 156/1000 | Loss: 0.00001849
Iteration 157/1000 | Loss: 0.00001849
Iteration 158/1000 | Loss: 0.00001849
Iteration 159/1000 | Loss: 0.00001849
Iteration 160/1000 | Loss: 0.00001849
Iteration 161/1000 | Loss: 0.00001849
Iteration 162/1000 | Loss: 0.00001849
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001849
Iteration 169/1000 | Loss: 0.00001849
Iteration 170/1000 | Loss: 0.00001849
Iteration 171/1000 | Loss: 0.00001849
Iteration 172/1000 | Loss: 0.00001849
Iteration 173/1000 | Loss: 0.00001849
Iteration 174/1000 | Loss: 0.00001849
Iteration 175/1000 | Loss: 0.00001849
Iteration 176/1000 | Loss: 0.00001849
Iteration 177/1000 | Loss: 0.00001849
Iteration 178/1000 | Loss: 0.00001849
Iteration 179/1000 | Loss: 0.00001849
Iteration 180/1000 | Loss: 0.00001849
Iteration 181/1000 | Loss: 0.00001849
Iteration 182/1000 | Loss: 0.00001849
Iteration 183/1000 | Loss: 0.00001849
Iteration 184/1000 | Loss: 0.00001849
Iteration 185/1000 | Loss: 0.00001849
Iteration 186/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.848760621214751e-05, 1.848760621214751e-05, 1.848760621214751e-05, 1.848760621214751e-05, 1.848760621214751e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.848760621214751e-05

Optimization complete. Final v2v error: 3.603147506713867 mm

Highest mean error: 3.67868709564209 mm for frame 25

Lowest mean error: 3.5206167697906494 mm for frame 209

Saving results

Total time: 43.64948630332947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804357
Iteration 2/25 | Loss: 0.00108833
Iteration 3/25 | Loss: 0.00081234
Iteration 4/25 | Loss: 0.00075025
Iteration 5/25 | Loss: 0.00073148
Iteration 6/25 | Loss: 0.00072588
Iteration 7/25 | Loss: 0.00072412
Iteration 8/25 | Loss: 0.00072379
Iteration 9/25 | Loss: 0.00072379
Iteration 10/25 | Loss: 0.00072379
Iteration 11/25 | Loss: 0.00072379
Iteration 12/25 | Loss: 0.00072379
Iteration 13/25 | Loss: 0.00072379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007237924146465957, 0.0007237924146465957, 0.0007237924146465957, 0.0007237924146465957, 0.0007237924146465957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007237924146465957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65261805
Iteration 2/25 | Loss: 0.00046681
Iteration 3/25 | Loss: 0.00046680
Iteration 4/25 | Loss: 0.00046680
Iteration 5/25 | Loss: 0.00046680
Iteration 6/25 | Loss: 0.00046680
Iteration 7/25 | Loss: 0.00046680
Iteration 8/25 | Loss: 0.00046680
Iteration 9/25 | Loss: 0.00046680
Iteration 10/25 | Loss: 0.00046680
Iteration 11/25 | Loss: 0.00046680
Iteration 12/25 | Loss: 0.00046680
Iteration 13/25 | Loss: 0.00046680
Iteration 14/25 | Loss: 0.00046680
Iteration 15/25 | Loss: 0.00046680
Iteration 16/25 | Loss: 0.00046680
Iteration 17/25 | Loss: 0.00046680
Iteration 18/25 | Loss: 0.00046680
Iteration 19/25 | Loss: 0.00046680
Iteration 20/25 | Loss: 0.00046680
Iteration 21/25 | Loss: 0.00046680
Iteration 22/25 | Loss: 0.00046680
Iteration 23/25 | Loss: 0.00046680
Iteration 24/25 | Loss: 0.00046680
Iteration 25/25 | Loss: 0.00046680

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046680
Iteration 2/1000 | Loss: 0.00005731
Iteration 3/1000 | Loss: 0.00003454
Iteration 4/1000 | Loss: 0.00002790
Iteration 5/1000 | Loss: 0.00002561
Iteration 6/1000 | Loss: 0.00002395
Iteration 7/1000 | Loss: 0.00002312
Iteration 8/1000 | Loss: 0.00002252
Iteration 9/1000 | Loss: 0.00002197
Iteration 10/1000 | Loss: 0.00002155
Iteration 11/1000 | Loss: 0.00002126
Iteration 12/1000 | Loss: 0.00002100
Iteration 13/1000 | Loss: 0.00002077
Iteration 14/1000 | Loss: 0.00002073
Iteration 15/1000 | Loss: 0.00002066
Iteration 16/1000 | Loss: 0.00002066
Iteration 17/1000 | Loss: 0.00002061
Iteration 18/1000 | Loss: 0.00002058
Iteration 19/1000 | Loss: 0.00002057
Iteration 20/1000 | Loss: 0.00002056
Iteration 21/1000 | Loss: 0.00002056
Iteration 22/1000 | Loss: 0.00002055
Iteration 23/1000 | Loss: 0.00002054
Iteration 24/1000 | Loss: 0.00002053
Iteration 25/1000 | Loss: 0.00002052
Iteration 26/1000 | Loss: 0.00002048
Iteration 27/1000 | Loss: 0.00002044
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00002038
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00002035
Iteration 32/1000 | Loss: 0.00002035
Iteration 33/1000 | Loss: 0.00002034
Iteration 34/1000 | Loss: 0.00002034
Iteration 35/1000 | Loss: 0.00002034
Iteration 36/1000 | Loss: 0.00002034
Iteration 37/1000 | Loss: 0.00002033
Iteration 38/1000 | Loss: 0.00002033
Iteration 39/1000 | Loss: 0.00002032
Iteration 40/1000 | Loss: 0.00002032
Iteration 41/1000 | Loss: 0.00002032
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002031
Iteration 44/1000 | Loss: 0.00002031
Iteration 45/1000 | Loss: 0.00002030
Iteration 46/1000 | Loss: 0.00002030
Iteration 47/1000 | Loss: 0.00002030
Iteration 48/1000 | Loss: 0.00002030
Iteration 49/1000 | Loss: 0.00002030
Iteration 50/1000 | Loss: 0.00002030
Iteration 51/1000 | Loss: 0.00002029
Iteration 52/1000 | Loss: 0.00002029
Iteration 53/1000 | Loss: 0.00002029
Iteration 54/1000 | Loss: 0.00002029
Iteration 55/1000 | Loss: 0.00002029
Iteration 56/1000 | Loss: 0.00002029
Iteration 57/1000 | Loss: 0.00002029
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00002028
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002027
Iteration 63/1000 | Loss: 0.00002027
Iteration 64/1000 | Loss: 0.00002026
Iteration 65/1000 | Loss: 0.00002026
Iteration 66/1000 | Loss: 0.00002026
Iteration 67/1000 | Loss: 0.00002026
Iteration 68/1000 | Loss: 0.00002026
Iteration 69/1000 | Loss: 0.00002026
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00002025
Iteration 72/1000 | Loss: 0.00002025
Iteration 73/1000 | Loss: 0.00002025
Iteration 74/1000 | Loss: 0.00002025
Iteration 75/1000 | Loss: 0.00002025
Iteration 76/1000 | Loss: 0.00002025
Iteration 77/1000 | Loss: 0.00002025
Iteration 78/1000 | Loss: 0.00002024
Iteration 79/1000 | Loss: 0.00002024
Iteration 80/1000 | Loss: 0.00002024
Iteration 81/1000 | Loss: 0.00002023
Iteration 82/1000 | Loss: 0.00002023
Iteration 83/1000 | Loss: 0.00002023
Iteration 84/1000 | Loss: 0.00002023
Iteration 85/1000 | Loss: 0.00002022
Iteration 86/1000 | Loss: 0.00002022
Iteration 87/1000 | Loss: 0.00002022
Iteration 88/1000 | Loss: 0.00002022
Iteration 89/1000 | Loss: 0.00002022
Iteration 90/1000 | Loss: 0.00002022
Iteration 91/1000 | Loss: 0.00002022
Iteration 92/1000 | Loss: 0.00002022
Iteration 93/1000 | Loss: 0.00002021
Iteration 94/1000 | Loss: 0.00002021
Iteration 95/1000 | Loss: 0.00002021
Iteration 96/1000 | Loss: 0.00002021
Iteration 97/1000 | Loss: 0.00002021
Iteration 98/1000 | Loss: 0.00002021
Iteration 99/1000 | Loss: 0.00002021
Iteration 100/1000 | Loss: 0.00002021
Iteration 101/1000 | Loss: 0.00002021
Iteration 102/1000 | Loss: 0.00002020
Iteration 103/1000 | Loss: 0.00002020
Iteration 104/1000 | Loss: 0.00002020
Iteration 105/1000 | Loss: 0.00002020
Iteration 106/1000 | Loss: 0.00002020
Iteration 107/1000 | Loss: 0.00002020
Iteration 108/1000 | Loss: 0.00002020
Iteration 109/1000 | Loss: 0.00002019
Iteration 110/1000 | Loss: 0.00002019
Iteration 111/1000 | Loss: 0.00002019
Iteration 112/1000 | Loss: 0.00002019
Iteration 113/1000 | Loss: 0.00002019
Iteration 114/1000 | Loss: 0.00002019
Iteration 115/1000 | Loss: 0.00002019
Iteration 116/1000 | Loss: 0.00002019
Iteration 117/1000 | Loss: 0.00002019
Iteration 118/1000 | Loss: 0.00002018
Iteration 119/1000 | Loss: 0.00002018
Iteration 120/1000 | Loss: 0.00002018
Iteration 121/1000 | Loss: 0.00002018
Iteration 122/1000 | Loss: 0.00002018
Iteration 123/1000 | Loss: 0.00002018
Iteration 124/1000 | Loss: 0.00002018
Iteration 125/1000 | Loss: 0.00002018
Iteration 126/1000 | Loss: 0.00002017
Iteration 127/1000 | Loss: 0.00002017
Iteration 128/1000 | Loss: 0.00002017
Iteration 129/1000 | Loss: 0.00002017
Iteration 130/1000 | Loss: 0.00002017
Iteration 131/1000 | Loss: 0.00002017
Iteration 132/1000 | Loss: 0.00002016
Iteration 133/1000 | Loss: 0.00002016
Iteration 134/1000 | Loss: 0.00002016
Iteration 135/1000 | Loss: 0.00002016
Iteration 136/1000 | Loss: 0.00002016
Iteration 137/1000 | Loss: 0.00002016
Iteration 138/1000 | Loss: 0.00002016
Iteration 139/1000 | Loss: 0.00002016
Iteration 140/1000 | Loss: 0.00002016
Iteration 141/1000 | Loss: 0.00002016
Iteration 142/1000 | Loss: 0.00002016
Iteration 143/1000 | Loss: 0.00002016
Iteration 144/1000 | Loss: 0.00002015
Iteration 145/1000 | Loss: 0.00002015
Iteration 146/1000 | Loss: 0.00002015
Iteration 147/1000 | Loss: 0.00002015
Iteration 148/1000 | Loss: 0.00002015
Iteration 149/1000 | Loss: 0.00002015
Iteration 150/1000 | Loss: 0.00002015
Iteration 151/1000 | Loss: 0.00002015
Iteration 152/1000 | Loss: 0.00002015
Iteration 153/1000 | Loss: 0.00002015
Iteration 154/1000 | Loss: 0.00002015
Iteration 155/1000 | Loss: 0.00002015
Iteration 156/1000 | Loss: 0.00002015
Iteration 157/1000 | Loss: 0.00002015
Iteration 158/1000 | Loss: 0.00002015
Iteration 159/1000 | Loss: 0.00002015
Iteration 160/1000 | Loss: 0.00002015
Iteration 161/1000 | Loss: 0.00002015
Iteration 162/1000 | Loss: 0.00002015
Iteration 163/1000 | Loss: 0.00002015
Iteration 164/1000 | Loss: 0.00002015
Iteration 165/1000 | Loss: 0.00002015
Iteration 166/1000 | Loss: 0.00002015
Iteration 167/1000 | Loss: 0.00002015
Iteration 168/1000 | Loss: 0.00002015
Iteration 169/1000 | Loss: 0.00002015
Iteration 170/1000 | Loss: 0.00002015
Iteration 171/1000 | Loss: 0.00002015
Iteration 172/1000 | Loss: 0.00002015
Iteration 173/1000 | Loss: 0.00002015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.0151577700744383e-05, 2.0151577700744383e-05, 2.0151577700744383e-05, 2.0151577700744383e-05, 2.0151577700744383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0151577700744383e-05

Optimization complete. Final v2v error: 3.761288642883301 mm

Highest mean error: 5.371342658996582 mm for frame 176

Lowest mean error: 2.836351156234741 mm for frame 129

Saving results

Total time: 48.76665902137756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00606628
Iteration 2/25 | Loss: 0.00077867
Iteration 3/25 | Loss: 0.00064174
Iteration 4/25 | Loss: 0.00061819
Iteration 5/25 | Loss: 0.00061387
Iteration 6/25 | Loss: 0.00061297
Iteration 7/25 | Loss: 0.00061285
Iteration 8/25 | Loss: 0.00061285
Iteration 9/25 | Loss: 0.00061285
Iteration 10/25 | Loss: 0.00061285
Iteration 11/25 | Loss: 0.00061285
Iteration 12/25 | Loss: 0.00061285
Iteration 13/25 | Loss: 0.00061285
Iteration 14/25 | Loss: 0.00061285
Iteration 15/25 | Loss: 0.00061285
Iteration 16/25 | Loss: 0.00061285
Iteration 17/25 | Loss: 0.00061285
Iteration 18/25 | Loss: 0.00061285
Iteration 19/25 | Loss: 0.00061285
Iteration 20/25 | Loss: 0.00061285
Iteration 21/25 | Loss: 0.00061285
Iteration 22/25 | Loss: 0.00061285
Iteration 23/25 | Loss: 0.00061285
Iteration 24/25 | Loss: 0.00061285
Iteration 25/25 | Loss: 0.00061285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.18367529
Iteration 2/25 | Loss: 0.00028916
Iteration 3/25 | Loss: 0.00028916
Iteration 4/25 | Loss: 0.00028916
Iteration 5/25 | Loss: 0.00028916
Iteration 6/25 | Loss: 0.00028916
Iteration 7/25 | Loss: 0.00028916
Iteration 8/25 | Loss: 0.00028916
Iteration 9/25 | Loss: 0.00028916
Iteration 10/25 | Loss: 0.00028916
Iteration 11/25 | Loss: 0.00028916
Iteration 12/25 | Loss: 0.00028916
Iteration 13/25 | Loss: 0.00028916
Iteration 14/25 | Loss: 0.00028916
Iteration 15/25 | Loss: 0.00028916
Iteration 16/25 | Loss: 0.00028916
Iteration 17/25 | Loss: 0.00028916
Iteration 18/25 | Loss: 0.00028916
Iteration 19/25 | Loss: 0.00028916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000289159914245829, 0.000289159914245829, 0.000289159914245829, 0.000289159914245829, 0.000289159914245829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000289159914245829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028916
Iteration 2/1000 | Loss: 0.00002521
Iteration 3/1000 | Loss: 0.00001697
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001457
Iteration 7/1000 | Loss: 0.00001424
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001397
Iteration 10/1000 | Loss: 0.00001394
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001384
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001373
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001369
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001368
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001363
Iteration 31/1000 | Loss: 0.00001363
Iteration 32/1000 | Loss: 0.00001362
Iteration 33/1000 | Loss: 0.00001362
Iteration 34/1000 | Loss: 0.00001361
Iteration 35/1000 | Loss: 0.00001358
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001357
Iteration 38/1000 | Loss: 0.00001357
Iteration 39/1000 | Loss: 0.00001356
Iteration 40/1000 | Loss: 0.00001355
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001351
Iteration 48/1000 | Loss: 0.00001351
Iteration 49/1000 | Loss: 0.00001351
Iteration 50/1000 | Loss: 0.00001351
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001348
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001348
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001346
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001345
Iteration 76/1000 | Loss: 0.00001345
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001342
Iteration 91/1000 | Loss: 0.00001341
Iteration 92/1000 | Loss: 0.00001341
Iteration 93/1000 | Loss: 0.00001341
Iteration 94/1000 | Loss: 0.00001340
Iteration 95/1000 | Loss: 0.00001340
Iteration 96/1000 | Loss: 0.00001340
Iteration 97/1000 | Loss: 0.00001340
Iteration 98/1000 | Loss: 0.00001340
Iteration 99/1000 | Loss: 0.00001340
Iteration 100/1000 | Loss: 0.00001340
Iteration 101/1000 | Loss: 0.00001340
Iteration 102/1000 | Loss: 0.00001340
Iteration 103/1000 | Loss: 0.00001339
Iteration 104/1000 | Loss: 0.00001339
Iteration 105/1000 | Loss: 0.00001338
Iteration 106/1000 | Loss: 0.00001337
Iteration 107/1000 | Loss: 0.00001337
Iteration 108/1000 | Loss: 0.00001337
Iteration 109/1000 | Loss: 0.00001337
Iteration 110/1000 | Loss: 0.00001337
Iteration 111/1000 | Loss: 0.00001337
Iteration 112/1000 | Loss: 0.00001337
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001336
Iteration 115/1000 | Loss: 0.00001336
Iteration 116/1000 | Loss: 0.00001336
Iteration 117/1000 | Loss: 0.00001336
Iteration 118/1000 | Loss: 0.00001336
Iteration 119/1000 | Loss: 0.00001336
Iteration 120/1000 | Loss: 0.00001336
Iteration 121/1000 | Loss: 0.00001336
Iteration 122/1000 | Loss: 0.00001336
Iteration 123/1000 | Loss: 0.00001336
Iteration 124/1000 | Loss: 0.00001336
Iteration 125/1000 | Loss: 0.00001335
Iteration 126/1000 | Loss: 0.00001335
Iteration 127/1000 | Loss: 0.00001335
Iteration 128/1000 | Loss: 0.00001335
Iteration 129/1000 | Loss: 0.00001335
Iteration 130/1000 | Loss: 0.00001335
Iteration 131/1000 | Loss: 0.00001335
Iteration 132/1000 | Loss: 0.00001335
Iteration 133/1000 | Loss: 0.00001335
Iteration 134/1000 | Loss: 0.00001335
Iteration 135/1000 | Loss: 0.00001335
Iteration 136/1000 | Loss: 0.00001335
Iteration 137/1000 | Loss: 0.00001335
Iteration 138/1000 | Loss: 0.00001335
Iteration 139/1000 | Loss: 0.00001335
Iteration 140/1000 | Loss: 0.00001334
Iteration 141/1000 | Loss: 0.00001334
Iteration 142/1000 | Loss: 0.00001334
Iteration 143/1000 | Loss: 0.00001334
Iteration 144/1000 | Loss: 0.00001334
Iteration 145/1000 | Loss: 0.00001334
Iteration 146/1000 | Loss: 0.00001334
Iteration 147/1000 | Loss: 0.00001334
Iteration 148/1000 | Loss: 0.00001334
Iteration 149/1000 | Loss: 0.00001334
Iteration 150/1000 | Loss: 0.00001334
Iteration 151/1000 | Loss: 0.00001334
Iteration 152/1000 | Loss: 0.00001333
Iteration 153/1000 | Loss: 0.00001333
Iteration 154/1000 | Loss: 0.00001333
Iteration 155/1000 | Loss: 0.00001333
Iteration 156/1000 | Loss: 0.00001333
Iteration 157/1000 | Loss: 0.00001333
Iteration 158/1000 | Loss: 0.00001333
Iteration 159/1000 | Loss: 0.00001333
Iteration 160/1000 | Loss: 0.00001333
Iteration 161/1000 | Loss: 0.00001333
Iteration 162/1000 | Loss: 0.00001333
Iteration 163/1000 | Loss: 0.00001333
Iteration 164/1000 | Loss: 0.00001333
Iteration 165/1000 | Loss: 0.00001333
Iteration 166/1000 | Loss: 0.00001333
Iteration 167/1000 | Loss: 0.00001333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.3332487469597254e-05, 1.3332487469597254e-05, 1.3332487469597254e-05, 1.3332487469597254e-05, 1.3332487469597254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3332487469597254e-05

Optimization complete. Final v2v error: 3.129211902618408 mm

Highest mean error: 3.391289710998535 mm for frame 71

Lowest mean error: 2.9004814624786377 mm for frame 116

Saving results

Total time: 33.70847773551941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390805
Iteration 2/25 | Loss: 0.00086570
Iteration 3/25 | Loss: 0.00072918
Iteration 4/25 | Loss: 0.00070467
Iteration 5/25 | Loss: 0.00069705
Iteration 6/25 | Loss: 0.00069542
Iteration 7/25 | Loss: 0.00069542
Iteration 8/25 | Loss: 0.00069542
Iteration 9/25 | Loss: 0.00069542
Iteration 10/25 | Loss: 0.00069542
Iteration 11/25 | Loss: 0.00069542
Iteration 12/25 | Loss: 0.00069542
Iteration 13/25 | Loss: 0.00069542
Iteration 14/25 | Loss: 0.00069542
Iteration 15/25 | Loss: 0.00069542
Iteration 16/25 | Loss: 0.00069542
Iteration 17/25 | Loss: 0.00069542
Iteration 18/25 | Loss: 0.00069542
Iteration 19/25 | Loss: 0.00069542
Iteration 20/25 | Loss: 0.00069542
Iteration 21/25 | Loss: 0.00069542
Iteration 22/25 | Loss: 0.00069542
Iteration 23/25 | Loss: 0.00069542
Iteration 24/25 | Loss: 0.00069542
Iteration 25/25 | Loss: 0.00069542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41844630
Iteration 2/25 | Loss: 0.00029211
Iteration 3/25 | Loss: 0.00029211
Iteration 4/25 | Loss: 0.00029211
Iteration 5/25 | Loss: 0.00029211
Iteration 6/25 | Loss: 0.00029211
Iteration 7/25 | Loss: 0.00029211
Iteration 8/25 | Loss: 0.00029211
Iteration 9/25 | Loss: 0.00029211
Iteration 10/25 | Loss: 0.00029211
Iteration 11/25 | Loss: 0.00029211
Iteration 12/25 | Loss: 0.00029211
Iteration 13/25 | Loss: 0.00029211
Iteration 14/25 | Loss: 0.00029211
Iteration 15/25 | Loss: 0.00029211
Iteration 16/25 | Loss: 0.00029211
Iteration 17/25 | Loss: 0.00029211
Iteration 18/25 | Loss: 0.00029211
Iteration 19/25 | Loss: 0.00029211
Iteration 20/25 | Loss: 0.00029211
Iteration 21/25 | Loss: 0.00029211
Iteration 22/25 | Loss: 0.00029211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00029211005312390625, 0.00029211005312390625, 0.00029211005312390625, 0.00029211005312390625, 0.00029211005312390625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029211005312390625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029211
Iteration 2/1000 | Loss: 0.00006179
Iteration 3/1000 | Loss: 0.00003870
Iteration 4/1000 | Loss: 0.00003473
Iteration 5/1000 | Loss: 0.00003244
Iteration 6/1000 | Loss: 0.00003095
Iteration 7/1000 | Loss: 0.00002995
Iteration 8/1000 | Loss: 0.00002901
Iteration 9/1000 | Loss: 0.00002844
Iteration 10/1000 | Loss: 0.00002807
Iteration 11/1000 | Loss: 0.00002784
Iteration 12/1000 | Loss: 0.00002758
Iteration 13/1000 | Loss: 0.00002734
Iteration 14/1000 | Loss: 0.00002731
Iteration 15/1000 | Loss: 0.00002714
Iteration 16/1000 | Loss: 0.00002713
Iteration 17/1000 | Loss: 0.00002713
Iteration 18/1000 | Loss: 0.00002713
Iteration 19/1000 | Loss: 0.00002712
Iteration 20/1000 | Loss: 0.00002711
Iteration 21/1000 | Loss: 0.00002710
Iteration 22/1000 | Loss: 0.00002709
Iteration 23/1000 | Loss: 0.00002709
Iteration 24/1000 | Loss: 0.00002708
Iteration 25/1000 | Loss: 0.00002707
Iteration 26/1000 | Loss: 0.00002706
Iteration 27/1000 | Loss: 0.00002706
Iteration 28/1000 | Loss: 0.00002706
Iteration 29/1000 | Loss: 0.00002705
Iteration 30/1000 | Loss: 0.00002704
Iteration 31/1000 | Loss: 0.00002704
Iteration 32/1000 | Loss: 0.00002702
Iteration 33/1000 | Loss: 0.00002702
Iteration 34/1000 | Loss: 0.00002701
Iteration 35/1000 | Loss: 0.00002701
Iteration 36/1000 | Loss: 0.00002700
Iteration 37/1000 | Loss: 0.00002700
Iteration 38/1000 | Loss: 0.00002700
Iteration 39/1000 | Loss: 0.00002699
Iteration 40/1000 | Loss: 0.00002696
Iteration 41/1000 | Loss: 0.00002695
Iteration 42/1000 | Loss: 0.00002692
Iteration 43/1000 | Loss: 0.00002691
Iteration 44/1000 | Loss: 0.00002690
Iteration 45/1000 | Loss: 0.00002689
Iteration 46/1000 | Loss: 0.00002689
Iteration 47/1000 | Loss: 0.00002688
Iteration 48/1000 | Loss: 0.00002688
Iteration 49/1000 | Loss: 0.00002688
Iteration 50/1000 | Loss: 0.00002687
Iteration 51/1000 | Loss: 0.00002687
Iteration 52/1000 | Loss: 0.00002687
Iteration 53/1000 | Loss: 0.00002687
Iteration 54/1000 | Loss: 0.00002687
Iteration 55/1000 | Loss: 0.00002686
Iteration 56/1000 | Loss: 0.00002686
Iteration 57/1000 | Loss: 0.00002686
Iteration 58/1000 | Loss: 0.00002685
Iteration 59/1000 | Loss: 0.00002685
Iteration 60/1000 | Loss: 0.00002685
Iteration 61/1000 | Loss: 0.00002685
Iteration 62/1000 | Loss: 0.00002685
Iteration 63/1000 | Loss: 0.00002685
Iteration 64/1000 | Loss: 0.00002685
Iteration 65/1000 | Loss: 0.00002685
Iteration 66/1000 | Loss: 0.00002685
Iteration 67/1000 | Loss: 0.00002685
Iteration 68/1000 | Loss: 0.00002684
Iteration 69/1000 | Loss: 0.00002684
Iteration 70/1000 | Loss: 0.00002684
Iteration 71/1000 | Loss: 0.00002684
Iteration 72/1000 | Loss: 0.00002684
Iteration 73/1000 | Loss: 0.00002684
Iteration 74/1000 | Loss: 0.00002684
Iteration 75/1000 | Loss: 0.00002684
Iteration 76/1000 | Loss: 0.00002683
Iteration 77/1000 | Loss: 0.00002683
Iteration 78/1000 | Loss: 0.00002683
Iteration 79/1000 | Loss: 0.00002683
Iteration 80/1000 | Loss: 0.00002683
Iteration 81/1000 | Loss: 0.00002683
Iteration 82/1000 | Loss: 0.00002683
Iteration 83/1000 | Loss: 0.00002683
Iteration 84/1000 | Loss: 0.00002683
Iteration 85/1000 | Loss: 0.00002682
Iteration 86/1000 | Loss: 0.00002682
Iteration 87/1000 | Loss: 0.00002682
Iteration 88/1000 | Loss: 0.00002682
Iteration 89/1000 | Loss: 0.00002682
Iteration 90/1000 | Loss: 0.00002682
Iteration 91/1000 | Loss: 0.00002682
Iteration 92/1000 | Loss: 0.00002682
Iteration 93/1000 | Loss: 0.00002682
Iteration 94/1000 | Loss: 0.00002682
Iteration 95/1000 | Loss: 0.00002681
Iteration 96/1000 | Loss: 0.00002681
Iteration 97/1000 | Loss: 0.00002681
Iteration 98/1000 | Loss: 0.00002681
Iteration 99/1000 | Loss: 0.00002681
Iteration 100/1000 | Loss: 0.00002681
Iteration 101/1000 | Loss: 0.00002681
Iteration 102/1000 | Loss: 0.00002681
Iteration 103/1000 | Loss: 0.00002681
Iteration 104/1000 | Loss: 0.00002681
Iteration 105/1000 | Loss: 0.00002681
Iteration 106/1000 | Loss: 0.00002681
Iteration 107/1000 | Loss: 0.00002681
Iteration 108/1000 | Loss: 0.00002681
Iteration 109/1000 | Loss: 0.00002681
Iteration 110/1000 | Loss: 0.00002681
Iteration 111/1000 | Loss: 0.00002681
Iteration 112/1000 | Loss: 0.00002681
Iteration 113/1000 | Loss: 0.00002681
Iteration 114/1000 | Loss: 0.00002681
Iteration 115/1000 | Loss: 0.00002681
Iteration 116/1000 | Loss: 0.00002681
Iteration 117/1000 | Loss: 0.00002681
Iteration 118/1000 | Loss: 0.00002681
Iteration 119/1000 | Loss: 0.00002681
Iteration 120/1000 | Loss: 0.00002681
Iteration 121/1000 | Loss: 0.00002681
Iteration 122/1000 | Loss: 0.00002681
Iteration 123/1000 | Loss: 0.00002681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.6809217160916887e-05, 2.6809217160916887e-05, 2.6809217160916887e-05, 2.6809217160916887e-05, 2.6809217160916887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6809217160916887e-05

Optimization complete. Final v2v error: 4.149620532989502 mm

Highest mean error: 4.285933971405029 mm for frame 83

Lowest mean error: 3.9817779064178467 mm for frame 210

Saving results

Total time: 42.20824646949768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393405
Iteration 2/25 | Loss: 0.00088598
Iteration 3/25 | Loss: 0.00063450
Iteration 4/25 | Loss: 0.00059973
Iteration 5/25 | Loss: 0.00059322
Iteration 6/25 | Loss: 0.00059186
Iteration 7/25 | Loss: 0.00059147
Iteration 8/25 | Loss: 0.00059147
Iteration 9/25 | Loss: 0.00059147
Iteration 10/25 | Loss: 0.00059147
Iteration 11/25 | Loss: 0.00059147
Iteration 12/25 | Loss: 0.00059147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005914662615396082, 0.0005914662615396082, 0.0005914662615396082, 0.0005914662615396082, 0.0005914662615396082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005914662615396082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45237899
Iteration 2/25 | Loss: 0.00024800
Iteration 3/25 | Loss: 0.00024800
Iteration 4/25 | Loss: 0.00024800
Iteration 5/25 | Loss: 0.00024800
Iteration 6/25 | Loss: 0.00024800
Iteration 7/25 | Loss: 0.00024800
Iteration 8/25 | Loss: 0.00024800
Iteration 9/25 | Loss: 0.00024799
Iteration 10/25 | Loss: 0.00024799
Iteration 11/25 | Loss: 0.00024799
Iteration 12/25 | Loss: 0.00024799
Iteration 13/25 | Loss: 0.00024799
Iteration 14/25 | Loss: 0.00024799
Iteration 15/25 | Loss: 0.00024799
Iteration 16/25 | Loss: 0.00024799
Iteration 17/25 | Loss: 0.00024799
Iteration 18/25 | Loss: 0.00024799
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00024799437960609794, 0.00024799437960609794, 0.00024799437960609794, 0.00024799437960609794, 0.00024799437960609794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024799437960609794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024799
Iteration 2/1000 | Loss: 0.00001950
Iteration 3/1000 | Loss: 0.00001489
Iteration 4/1000 | Loss: 0.00001382
Iteration 5/1000 | Loss: 0.00001298
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001210
Iteration 8/1000 | Loss: 0.00001185
Iteration 9/1000 | Loss: 0.00001183
Iteration 10/1000 | Loss: 0.00001182
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001176
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001163
Iteration 19/1000 | Loss: 0.00001162
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001160
Iteration 22/1000 | Loss: 0.00001159
Iteration 23/1000 | Loss: 0.00001158
Iteration 24/1000 | Loss: 0.00001157
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001145
Iteration 45/1000 | Loss: 0.00001144
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001142
Iteration 50/1000 | Loss: 0.00001142
Iteration 51/1000 | Loss: 0.00001141
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001138
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001137
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001136
Iteration 68/1000 | Loss: 0.00001136
Iteration 69/1000 | Loss: 0.00001135
Iteration 70/1000 | Loss: 0.00001135
Iteration 71/1000 | Loss: 0.00001135
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001135
Iteration 74/1000 | Loss: 0.00001134
Iteration 75/1000 | Loss: 0.00001134
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001134
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001133
Iteration 80/1000 | Loss: 0.00001133
Iteration 81/1000 | Loss: 0.00001133
Iteration 82/1000 | Loss: 0.00001132
Iteration 83/1000 | Loss: 0.00001132
Iteration 84/1000 | Loss: 0.00001132
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001132
Iteration 88/1000 | Loss: 0.00001132
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001132
Iteration 91/1000 | Loss: 0.00001132
Iteration 92/1000 | Loss: 0.00001132
Iteration 93/1000 | Loss: 0.00001132
Iteration 94/1000 | Loss: 0.00001132
Iteration 95/1000 | Loss: 0.00001132
Iteration 96/1000 | Loss: 0.00001132
Iteration 97/1000 | Loss: 0.00001132
Iteration 98/1000 | Loss: 0.00001132
Iteration 99/1000 | Loss: 0.00001132
Iteration 100/1000 | Loss: 0.00001132
Iteration 101/1000 | Loss: 0.00001132
Iteration 102/1000 | Loss: 0.00001132
Iteration 103/1000 | Loss: 0.00001132
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001132
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001132
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001132
Iteration 122/1000 | Loss: 0.00001132
Iteration 123/1000 | Loss: 0.00001132
Iteration 124/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.131554199673701e-05, 1.131554199673701e-05, 1.131554199673701e-05, 1.131554199673701e-05, 1.131554199673701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.131554199673701e-05

Optimization complete. Final v2v error: 2.8877217769622803 mm

Highest mean error: 2.959148645401001 mm for frame 118

Lowest mean error: 2.7952606678009033 mm for frame 98

Saving results

Total time: 30.712836980819702
